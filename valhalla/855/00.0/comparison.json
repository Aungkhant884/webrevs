{"files":[{"patch":"@@ -122,1 +122,0 @@\n-    --enable-preview -source $(JDK_SOURCE_TARGET_VERSION) \\\n","filename":"make\/Docs.gmk","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -590,1 +590,1 @@\n-            dependencies: [\"devkit\", \"gtest\"],\n+            dependencies: [\"devkit\", \"gtest\", \"libffi\"],\n@@ -594,1 +594,2 @@\n-                \"--enable-libffi-bundling\"\n+                \"--with-libffi=\" + input.get(\"libffi\", \"home_path\"),\n+                \"--enable-libffi-bundling\",\n@@ -601,1 +602,1 @@\n-            dependencies: [\"devkit\", \"gtest\"],\n+            dependencies: [\"devkit\", \"gtest\", \"libffi\"],\n@@ -605,0 +606,1 @@\n+                \"--with-libffi=\" + input.get(\"libffi\", \"home_path\"),\n@@ -613,1 +615,1 @@\n-            dependencies: [\"devkit\", \"gtest\"],\n+            dependencies: [\"devkit\", \"gtest\", \"libffi\"],\n@@ -617,0 +619,1 @@\n+                \"--with-libffi=\" + input.get(\"libffi\", \"home_path\"),\n@@ -747,0 +750,34 @@\n+    \/\/ Define artifact just for linux-x64-zero, which is the only one we test on\n+    [\"linux-x64\"].forEach(function (name) {\n+        var o = artifactData[name]\n+        var pf = o.platform\n+        var jdk_subdir = (o.jdk_subdir != null ? o.jdk_subdir : \"jdk-\" + data.version);\n+        var jdk_suffix = (o.jdk_suffix != null ? o.jdk_suffix : \"tar.gz\");\n+        var zeroName = name + \"-zero\";\n+        profiles[zeroName].artifacts = {\n+            jdk: {\n+                local: \"bundles\/\\\\(jdk.*bin.\" + jdk_suffix + \"\\\\)\",\n+                remote: [\n+                    \"bundles\/\" + pf + \"\/jdk-\" + data.version + \"_\" + pf + \"_bin-zero.\" + jdk_suffix,\n+                ],\n+                subdir: jdk_subdir,\n+                exploded: \"images\/jdk\",\n+            },\n+            test: {\n+                    local: \"bundles\/\\\\(jdk.*bin-tests.tar.gz\\\\)\",\n+                    remote: [\n+                        \"bundles\/\" + pf + \"\/jdk-\" + data.version + \"_\" + pf + \"_bin-zero-tests.tar.gz\",\n+                    ],\n+                    exploded: \"images\/test\"\n+            },\n+            jdk_symbols: {\n+                    local: \"bundles\/\\\\(jdk.*bin-symbols.tar.gz\\\\)\",\n+                    remote: [\n+                        \"bundles\/\" + pf + \"\/jdk-\" + data.version + \"_\" + pf + \"_bin-zero-symbols.tar.gz\",\n+                    ],\n+                    subdir: jdk_subdir,\n+                    exploded: \"images\/jdk\"\n+                },\n+            };\n+    });\n+\n@@ -1237,0 +1274,7 @@\n+\n+        libffi: {\n+            organization: common.organization,\n+            module: \"libffi-\" + input.target_platform,\n+            ext: \"tar.gz\",\n+            revision: \"3.4.2+1.0\"\n+        },\n","filename":"make\/conf\/jib-profiles.js","additions":48,"deletions":4,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -222,0 +222,2 @@\n+JVM_VirtualThreadStart\n+JVM_VirtualThreadEnd\n","filename":"make\/data\/hotspot-symbols\/symbols-unix","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -152,0 +152,1 @@\n+  JVM_EXCLUDE_PATTERNS += gc\/x\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2006, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2006, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -2388,1 +2388,2 @@\n-                    String name = cf.constant_pool.getUTF8Value(e.name_index);\n+                    String name = e.name_index == 0 ? null\n+                            : cf.constant_pool.getUTF8Value(e.name_index);\n","filename":"make\/langtools\/src\/classes\/build\/tools\/symbolgenerator\/CreateSymbols.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -103,1 +103,0 @@\n-        --add-exports java.base\/jdk.internal.classfile.java.lang.constant=ALL-UNNAMED \\\n","filename":"make\/test\/BuildMicrobenchmark.gmk","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3859,1 +3859,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3876,1 +3876,4 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      __ b(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -3909,0 +3912,1 @@\n+      __ b(cont);\n@@ -3910,1 +3914,3 @@\n-      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count);\n+      __ b(count);\n@@ -3912,1 +3918,0 @@\n-    __ b(cont);\n@@ -3921,1 +3926,1 @@\n-    __ add(tmp, disp_hdr, (ObjectMonitor::owner_offset_in_bytes()-markWord::monitor_value));\n+    __ add(tmp, disp_hdr, (in_bytes(ObjectMonitor::owner_offset())-markWord::monitor_value));\n@@ -3925,7 +3930,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3938,1 +3944,1 @@\n-    __ increment(Address(disp_hdr, ObjectMonitor::recursions_offset_in_bytes() - markWord::monitor_value), 1);\n+    __ increment(Address(disp_hdr, in_bytes(ObjectMonitor::recursions_offset()) - markWord::monitor_value), 1);\n@@ -3946,0 +3952,1 @@\n+    __ bind(count);\n@@ -3959,1 +3966,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3963,1 +3970,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -3976,1 +3983,4 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      __ b(cont);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -3983,0 +3993,1 @@\n+      __ b(cont);\n@@ -3984,1 +3995,3 @@\n-      __ tst(oop, oop); \/\/ Set NE to indicate 'failure' -> take slow-path. We know that oop != 0.\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n+      __ b(count);\n@@ -3986,1 +3999,0 @@\n-    __ b(cont);\n@@ -3994,1 +4006,15 @@\n-    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset_in_bytes()));\n+\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset()));\n+      \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+      \/\/ be encoded.\n+      __ tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ br(Assembler::NE, stub->entry());\n+      __ bind(stub->continuation());\n+    }\n+\n+    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n@@ -4001,1 +4027,1 @@\n-    __ str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset_in_bytes()));\n+    __ str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n@@ -4006,2 +4032,2 @@\n-    __ ldr(rscratch1, Address(tmp, ObjectMonitor::EntryList_offset_in_bytes()));\n-    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::cxq_offset_in_bytes()));\n+    __ ldr(rscratch1, Address(tmp, ObjectMonitor::EntryList_offset()));\n+    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::cxq_offset()));\n@@ -4012,1 +4038,1 @@\n-    __ lea(tmp, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+    __ lea(tmp, Address(tmp, ObjectMonitor::owner_offset()));\n@@ -4020,0 +4046,1 @@\n+    __ bind(count);\n@@ -4360,1 +4387,12 @@\n-  predicate(n->get_int() < (int)(BoolTest::unsigned_compare));\n+  predicate(!Matcher::is_unsigned_booltest_pred(n->get_int()));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ BoolTest condition for unsigned compare\n+operand immI_cmpU_cond()\n+%{\n+  predicate(Matcher::is_unsigned_booltest_pred(n->get_int()));\n@@ -4467,0 +4505,22 @@\n+\/\/ 5 bit signed integer\n+operand immI5()\n+%{\n+  predicate(Assembler::is_simm(n->get_int(), 5));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 7 bit unsigned integer\n+operand immIU7()\n+%{\n+  predicate(Assembler::is_uimm(n->get_int(), 7));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4609,0 +4669,22 @@\n+\/\/ 5 bit signed long integer\n+operand immL5()\n+%{\n+  predicate(Assembler::is_simm(n->get_long(), 5));\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 7 bit unsigned long integer\n+operand immLU7()\n+%{\n+  predicate(Assembler::is_uimm(n->get_long(), 7));\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -17117,2 +17199,3 @@\n-       iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n-       iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+                          iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n+                          iRegINoSp tmp3, iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6,\n+                          vRegD_V0 vtmp0, vRegD_V1 vtmp1, rFlagsReg cr)\n@@ -17123,2 +17206,4 @@\n-         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UU)\" %}\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6,\n+         TEMP vtmp0, TEMP vtmp1, KILL cr);\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UU) \"\n+            \"# KILL $str1 $cnt1 $str2 $cnt2 $tmp1 $tmp2 $tmp3 $tmp4 $tmp5 $tmp6 V0-V1 cr\" %}\n@@ -17138,2 +17223,3 @@\n-       iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n-       iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+                          iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n+                          iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6,\n+                          vRegD_V0 vtmp0, vRegD_V1 vtmp1, rFlagsReg cr)\n@@ -17144,2 +17230,4 @@\n-         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (LL)\" %}\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6,\n+         TEMP vtmp0, TEMP vtmp1, KILL cr);\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (LL) \"\n+            \"# KILL $str1 $cnt1 $str2 $cnt2 $tmp1 $tmp2 $tmp3 $tmp4 $tmp5 $tmp6 V0-V1 cr\" %}\n@@ -17159,2 +17247,3 @@\n-       iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n-       iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+                          iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,iRegINoSp tmp3,\n+                          iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6,\n+                          vRegD_V0 vtmp0, vRegD_V1 vtmp1, rFlagsReg cr)\n@@ -17165,2 +17254,4 @@\n-         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UL)\" %}\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5,\n+         TEMP tmp6, TEMP vtmp0, TEMP vtmp1, KILL cr);\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UL) \"\n+            \"# KILL $str1 cnt1 $str2 $cnt2 $tmp1 $tmp2 $tmp3 $tmp4 $tmp5 $tmp6 V0-V1 cr\" %}\n@@ -17180,2 +17271,2 @@\n-                 immI_le_4 int_cnt2, iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n-                 iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+                              immI_le_4 int_cnt2, iRegI_R0 result, iRegINoSp tmp1,\n+                              iRegINoSp tmp2, iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n@@ -17187,1 +17278,2 @@\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UU)\" %}\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UU) \"\n+            \"# KILL $str1 $cnt1 $str2 $tmp1 $tmp2 $tmp3 $tmp4 cr\" %}\n@@ -17201,2 +17293,2 @@\n-                 immI_le_4 int_cnt2, iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n-                 iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+                              immI_le_4 int_cnt2, iRegI_R0 result, iRegINoSp tmp1,\n+                              iRegINoSp tmp2, iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n@@ -17208,1 +17300,2 @@\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (LL)\" %}\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (LL) \"\n+            \"# KILL $str1 $cnt1 $str2 $tmp1 $tmp2 $tmp3 $tmp4 cr\" %}\n@@ -17222,2 +17315,2 @@\n-                 immI_1 int_cnt2, iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n-                 iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+                              immI_1 int_cnt2, iRegI_R0 result, iRegINoSp tmp1,\n+                              iRegINoSp tmp2, iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n@@ -17229,1 +17322,2 @@\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UL)\" %}\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UL) \"\n+            \"# KILL $str1 $cnt1 $str2 $tmp1 $tmp2 $tmp3 $tmp4 cr\" %}\n@@ -17346,0 +17440,2 @@\n+                       vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                       vRegD_V4 vtmp4, vRegD_V5 vtmp5, vRegD_V6 vtmp6, vRegD_V7 vtmp7,\n@@ -17350,1 +17446,3 @@\n-  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3,\n+         TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP vtmp5,\n+         TEMP vtmp6, TEMP vtmp7, KILL cr);\n@@ -17352,1 +17450,1 @@\n-  format %{ \"Array Equals $ary1,ary2 -> $result    \/\/ KILL $tmp\" %}\n+  format %{ \"Array Equals $ary1,ary2 -> $result # KILL $ary1 $ary2 $tmp $tmp1 $tmp2 $tmp3 V0-V7 cr\" %}\n@@ -17367,0 +17465,2 @@\n+                       vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                       vRegD_V4 vtmp4, vRegD_V5 vtmp5, vRegD_V6 vtmp6, vRegD_V7 vtmp7,\n@@ -17371,1 +17471,3 @@\n-  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3,\n+         TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP vtmp5,\n+         TEMP vtmp6, TEMP vtmp7, KILL cr);\n@@ -17373,1 +17475,1 @@\n-  format %{ \"Array Equals $ary1,ary2 -> $result    \/\/ KILL $tmp\" %}\n+  format %{ \"Array Equals $ary1,ary2 -> $result # KILL $ary1 $ary2 $tmp $tmp1 $tmp2 $tmp3 V0-V7 cr\" %}\n@@ -17403,2 +17505,2 @@\n-                         vRegD_V0 tmp1, vRegD_V1 tmp2,\n-                         vRegD_V2 tmp3, vRegD_V3 tmp4,\n+                         vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2,\n+                         vRegD_V3 vtmp3, vRegD_V4 vtmp4, vRegD_V5 vtmp5,\n@@ -17408,1 +17510,1 @@\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4,\n+  effect(TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP vtmp5,\n@@ -17411,1 +17513,1 @@\n-  format %{ \"String Compress $src,$dst,$len -> $result  \/\/ KILL $src,$dst\" %}\n+  format %{ \"String Compress $src,$dst,$len -> $result # KILL $src $dst V0-V5 cr\" %}\n@@ -17414,3 +17516,3 @@\n-                           $result$$Register,\n-                           $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n-                           $tmp3$$FloatRegister, $tmp4$$FloatRegister);\n+                           $result$$Register, $vtmp0$$FloatRegister, $vtmp1$$FloatRegister,\n+                           $vtmp2$$FloatRegister, $vtmp3$$FloatRegister,\n+                           $vtmp4$$FloatRegister, $vtmp5$$FloatRegister);\n@@ -17422,2 +17524,3 @@\n-instruct string_inflate(Universe dummy, iRegP_R0 src, iRegP_R1 dst, iRegI_R2 len,\n-                        vRegD_V0 tmp1, vRegD_V1 tmp2, vRegD_V2 tmp3, iRegP_R3 tmp4, rFlagsReg cr)\n+instruct string_inflate(Universe dummy, iRegP_R0 src, iRegP_R1 dst, iRegI_R2 len, iRegP_R3 tmp,\n+                        vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                        vRegD_V4 vtmp4, vRegD_V5 vtmp5, vRegD_V6 vtmp6, rFlagsReg cr)\n@@ -17426,1 +17529,3 @@\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+  effect(TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3,\n+         TEMP vtmp4, TEMP vtmp5, TEMP vtmp6, TEMP tmp,\n+         USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n@@ -17428,1 +17533,1 @@\n-  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  format %{ \"String Inflate $src,$dst # KILL $tmp $src $dst $len V0-V6 cr\" %}\n@@ -17431,2 +17536,2 @@\n-                                        $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n-                                        $tmp3$$FloatRegister, $tmp4$$Register);\n+                                        $vtmp0$$FloatRegister, $vtmp1$$FloatRegister,\n+                                        $vtmp2$$FloatRegister, $tmp$$Register);\n@@ -17443,2 +17548,2 @@\n-                          vRegD_V0 vtmp0, vRegD_V1 vtmp1,\n-                          vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                          vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2,\n+                          vRegD_V3 vtmp3, vRegD_V4 vtmp4, vRegD_V5 vtmp5,\n@@ -17449,2 +17554,2 @@\n-  effect(USE_KILL src, USE_KILL dst, USE len,\n-         KILL vtmp0, KILL vtmp1, KILL vtmp2, KILL vtmp3, KILL cr);\n+  effect(USE_KILL src, USE_KILL dst, USE len, KILL vtmp0, KILL vtmp1,\n+         KILL vtmp2, KILL vtmp3, KILL vtmp4, KILL vtmp5, KILL cr);\n@@ -17452,1 +17557,1 @@\n-  format %{ \"Encode ISO array $src,$dst,$len -> $result\" %}\n+  format %{ \"Encode ISO array $src,$dst,$len -> $result # KILL $src $dst V0-V5 cr\" %}\n@@ -17457,1 +17562,2 @@\n-                        $vtmp2$$FloatRegister, $vtmp3$$FloatRegister);\n+                        $vtmp2$$FloatRegister, $vtmp3$$FloatRegister,\n+                        $vtmp4$$FloatRegister, $vtmp5$$FloatRegister);\n@@ -17463,2 +17569,2 @@\n-                            vRegD_V0 vtmp0, vRegD_V1 vtmp1,\n-                            vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                            vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2,\n+                            vRegD_V3 vtmp3, vRegD_V4 vtmp4, vRegD_V5 vtmp5,\n@@ -17469,2 +17575,2 @@\n-  effect(USE_KILL src, USE_KILL dst, USE len,\n-         KILL vtmp0, KILL vtmp1, KILL vtmp2, KILL vtmp3, KILL cr);\n+  effect(USE_KILL src, USE_KILL dst, USE len, KILL vtmp0, KILL vtmp1,\n+         KILL vtmp2, KILL vtmp3, KILL vtmp4, KILL vtmp5, KILL cr);\n@@ -17472,1 +17578,1 @@\n-  format %{ \"Encode ASCII array $src,$dst,$len -> $result\" %}\n+  format %{ \"Encode ASCII array $src,$dst,$len -> $result # KILL $src $dst V0-V5 cr\" %}\n@@ -17477,1 +17583,2 @@\n-                        $vtmp2$$FloatRegister, $vtmp3$$FloatRegister);\n+                        $vtmp2$$FloatRegister, $vtmp3$$FloatRegister,\n+                        $vtmp4$$FloatRegister, $vtmp5$$FloatRegister);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":181,"deletions":74,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,1 +47,1 @@\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+  assert(SharedRuntime::polling_page_return_handler_blob() != nullptr,\n@@ -276,1 +276,1 @@\n-  if (_throw_imse_stub != NULL) {\n+  if (_throw_imse_stub != nullptr) {\n@@ -420,1 +420,1 @@\n-  if (call == NULL) {\n+  if (call == nullptr) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -127,1 +127,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -137,1 +137,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -283,1 +283,1 @@\n-        __ stop(\"locked object is NULL\");\n+        __ stop(\"locked object is null\");\n@@ -333,1 +333,1 @@\n-  if (o == NULL) {\n+  if (o == nullptr) {\n@@ -341,1 +341,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n@@ -384,1 +384,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -432,1 +432,1 @@\n-  MonitorExitStub* stub = NULL;\n+  MonitorExitStub* stub = nullptr;\n@@ -436,1 +436,1 @@\n-    if (UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n@@ -460,1 +460,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -471,1 +471,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -492,1 +492,1 @@\n-  if (info->exception_handlers() != NULL) {\n+  if (info->exception_handlers() != nullptr) {\n@@ -507,1 +507,1 @@\n-        assert(unpack_handler != NULL, \"must be\");\n+        assert(unpack_handler != nullptr, \"must be\");\n@@ -546,1 +546,1 @@\n-  guarantee(info != NULL, \"Shouldn't be NULL\");\n+  guarantee(info != nullptr, \"Shouldn't be null\");\n@@ -643,1 +643,1 @@\n-        const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);\n+        const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, nullptr);\n@@ -650,1 +650,1 @@\n-      const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);\n+      const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, nullptr);\n@@ -817,1 +817,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -890,1 +890,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -933,1 +933,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n@@ -986,1 +986,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -1054,1 +1054,1 @@\n-    if (!UseZGC) {\n+    if (!(UseZGC && !ZGenerational)) {\n@@ -1070,1 +1070,1 @@\n-    const2reg(src, dst, lir_patch_none, NULL);\n+    const2reg(src, dst, lir_patch_none, nullptr);\n@@ -1111,3 +1111,3 @@\n-  assert(op->block() == NULL || op->block()->label() == op->label(), \"wrong label\");\n-  if (op->block() != NULL)  _branch_target_blocks.append(op->block());\n-  if (op->ublock() != NULL) _branch_target_blocks.append(op->ublock());\n+  assert(op->block() == nullptr || op->block()->label() == op->label(), \"wrong label\");\n+  if (op->block() != nullptr)  _branch_target_blocks.append(op->block());\n+  if (op->ublock() != nullptr) _branch_target_blocks.append(op->ublock());\n@@ -1117,1 +1117,1 @@\n-    if (op->info() != NULL) add_debug_info_for_branch(op->info());\n+    if (op->info() != nullptr) add_debug_info_for_branch(op->info());\n@@ -1346,1 +1346,1 @@\n-    assert(method != NULL, \"Should have method\");\n+    assert(method != nullptr, \"Should have method\");\n@@ -1349,1 +1349,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -1351,1 +1351,1 @@\n-    assert(data != NULL,                \"need data for type check\");\n+    assert(data != nullptr,                \"need data for type check\");\n@@ -1435,1 +1435,1 @@\n-      __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+      __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -1488,1 +1488,1 @@\n-      assert(method != NULL, \"Should have method\");\n+      assert(method != nullptr, \"Should have method\");\n@@ -1491,1 +1491,1 @@\n-      assert(md != NULL, \"Sanity\");\n+      assert(md != nullptr, \"Sanity\");\n@@ -1493,1 +1493,1 @@\n-      assert(data != NULL,                \"need data for type check\");\n+      assert(data != nullptr,                \"need data for type check\");\n@@ -1526,1 +1526,1 @@\n-    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -1654,1 +1654,1 @@\n-  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n@@ -1667,1 +1667,1 @@\n-  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n@@ -1807,1 +1807,1 @@\n-    const2reg(opr1, tmp, lir_patch_none, NULL);\n+    const2reg(opr1, tmp, lir_patch_none, nullptr);\n@@ -1817,1 +1817,1 @@\n-    const2reg(opr2, tmp, lir_patch_none, NULL);\n+    const2reg(opr2, tmp, lir_patch_none, nullptr);\n@@ -1828,1 +1828,1 @@\n-  assert(info == NULL, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n+  assert(info == nullptr, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n@@ -2221,1 +2221,1 @@\n-  if (call == NULL) {\n+  if (call == nullptr) {\n@@ -2232,1 +2232,1 @@\n-  if (call == NULL) {\n+  if (call == nullptr) {\n@@ -2243,1 +2243,1 @@\n-  if (stub == NULL) {\n+  if (stub == nullptr) {\n@@ -2415,1 +2415,1 @@\n-      \/\/ Take the slow path if it's a null_free destination array, in case the source array contains NULLs.\n+      \/\/ Take the slow path if it's a null_free destination array, in case the source array contains nulls.\n@@ -2438,1 +2438,1 @@\n-  BasicType basic_type = default_type != NULL ? default_type->element_type()->basic_type() : T_ILLEGAL;\n+  BasicType basic_type = default_type != nullptr ? default_type->element_type()->basic_type() : T_ILLEGAL;\n@@ -2448,1 +2448,1 @@\n-  if (default_type == NULL \/\/ || basic_type == T_OBJECT\n+  if (default_type == nullptr \/\/ || basic_type == T_OBJECT\n@@ -2460,1 +2460,1 @@\n-    assert(copyfunc_addr != NULL, \"generic arraycopy stub required\");\n+    assert(copyfunc_addr != nullptr, \"generic arraycopy stub required\");\n@@ -2509,1 +2509,1 @@\n-  assert(default_type != NULL && default_type->is_array_klass() && default_type->is_loaded(), \"must be true at this point\");\n+  assert(default_type != nullptr && default_type->is_array_klass() && default_type->is_loaded(), \"must be true at this point\");\n@@ -2519,1 +2519,1 @@\n-  \/\/ test for NULL\n+  \/\/ test for null\n@@ -2604,1 +2604,1 @@\n-      __ check_klass_subtype_fast_path(src, dst, tmp, &cont, &slow, NULL);\n+      __ check_klass_subtype_fast_path(src, dst, tmp, &cont, &slow, nullptr);\n@@ -2616,1 +2616,1 @@\n-      if (copyfunc_addr != NULL) { \/\/ use stub if available\n+      if (copyfunc_addr != nullptr) { \/\/ use stub if available\n@@ -2785,2 +2785,2 @@\n-  if (UseHeavyMonitors) {\n-    if (op->info() != NULL) {\n+  if (LockingMode == LM_MONITOR) {\n+    if (op->info() != nullptr) {\n@@ -2795,1 +2795,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -2813,1 +2813,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -2832,1 +2832,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -2834,1 +2834,1 @@\n-  assert(data != NULL && data->is_CounterData(), \"need CounterData for calls\");\n+  assert(data != nullptr && data->is_CounterData(), \"need CounterData for calls\");\n@@ -2847,1 +2847,1 @@\n-    if (C1OptimizeVirtualCallProfiling && known_klass != NULL) {\n+    if (C1OptimizeVirtualCallProfiling && known_klass != nullptr) {\n@@ -2872,1 +2872,1 @@\n-        if (receiver == NULL) {\n+        if (receiver == nullptr) {\n@@ -2939,1 +2939,1 @@\n-  bool exact_klass_set = exact_klass != NULL && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n+  bool exact_klass_set = exact_klass != nullptr && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n@@ -2975,1 +2975,1 @@\n-    if (exact_klass != NULL) {\n+    if (exact_klass != nullptr) {\n@@ -2986,2 +2986,2 @@\n-      if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {\n-        if (exact_klass != NULL) {\n+      if (exact_klass == nullptr || TypeEntries::is_type_none(current_klass)) {\n+        if (exact_klass != nullptr) {\n@@ -3016,1 +3016,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -3037,1 +3037,1 @@\n-      assert(exact_klass != NULL, \"should be\");\n+      assert(exact_klass != nullptr, \"should be\");\n@@ -3066,1 +3066,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -3150,1 +3150,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":66,"deletions":66,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -63,1 +63,1 @@\n-  void poll_for_safepoint(relocInfo::relocType rtype, CodeEmitInfo* info = NULL);\n+  void poll_for_safepoint(relocInfo::relocType rtype, CodeEmitInfo* info = nullptr);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -117,1 +117,1 @@\n-  if (v->type()->as_IntConstant() != NULL) {\n+  if (v->type()->as_IntConstant() != nullptr) {\n@@ -119,1 +119,1 @@\n-  } else if (v->type()->as_LongConstant() != NULL) {\n+  } else if (v->type()->as_LongConstant() != nullptr) {\n@@ -121,1 +121,1 @@\n-  } else if (v->type()->as_ObjectConstant() != NULL) {\n+  } else if (v->type()->as_ObjectConstant() != nullptr) {\n@@ -130,1 +130,1 @@\n-  if (v->type()->as_IntConstant() != NULL) {\n+  if (v->type()->as_IntConstant() != nullptr) {\n@@ -132,1 +132,1 @@\n-  } else if (v->type()->as_LongConstant() != NULL) {\n+  } else if (v->type()->as_LongConstant() != nullptr) {\n@@ -134,1 +134,1 @@\n-  } else if (v->type()->as_ObjectConstant() != NULL) {\n+  } else if (v->type()->as_ObjectConstant() != nullptr) {\n@@ -330,1 +330,1 @@\n-  CodeEmitInfo* info_for_exception = NULL;\n+  CodeEmitInfo* info_for_exception = nullptr;\n@@ -487,1 +487,1 @@\n-    arithmetic_op_long(x->op(), x->operand(), left.result(), right.result(), NULL);\n+    arithmetic_op_long(x->op(), x->operand(), left.result(), right.result(), nullptr);\n@@ -533,1 +533,1 @@\n-      __ irem(left_arg->result(), right_arg->result(), x->operand(), ill, NULL);\n+      __ irem(left_arg->result(), right_arg->result(), x->operand(), ill, nullptr);\n@@ -535,1 +535,1 @@\n-      __ idiv(left_arg->result(), right_arg->result(), x->operand(), ill, NULL);\n+      __ idiv(left_arg->result(), right_arg->result(), x->operand(), ill, nullptr);\n@@ -568,1 +568,1 @@\n-  if (x->is_commutative() && x->y()->as_Constant() == NULL && x->x()->use_count() > x->y()->use_count()) {\n+  if (x->is_commutative() && x->y()->as_Constant() == nullptr && x->x()->use_count() > x->y()->use_count()) {\n@@ -821,1 +821,1 @@\n-  CallingConvention* cc = NULL;\n+  CallingConvention* cc = nullptr;\n@@ -843,1 +843,1 @@\n-      if (StubRoutines::dexp() != NULL) {\n+      if (StubRoutines::dexp() != nullptr) {\n@@ -850,1 +850,1 @@\n-      if (StubRoutines::dlog() != NULL) {\n+      if (StubRoutines::dlog() != nullptr) {\n@@ -857,1 +857,1 @@\n-      if (StubRoutines::dlog10() != NULL) {\n+      if (StubRoutines::dlog10() != nullptr) {\n@@ -864,1 +864,1 @@\n-      if (StubRoutines::dpow() != NULL) {\n+      if (StubRoutines::dpow() != nullptr) {\n@@ -871,1 +871,1 @@\n-      if (StubRoutines::dsin() != NULL) {\n+      if (StubRoutines::dsin() != nullptr) {\n@@ -878,1 +878,1 @@\n-      if (StubRoutines::dcos() != NULL) {\n+      if (StubRoutines::dcos() != nullptr) {\n@@ -885,1 +885,1 @@\n-      if (StubRoutines::dtan() != NULL) {\n+      if (StubRoutines::dtan() != nullptr) {\n@@ -1200,1 +1200,1 @@\n-  CodeEmitInfo* patching_info = NULL;\n+  CodeEmitInfo* patching_info = nullptr;\n@@ -1238,1 +1238,1 @@\n-  LIRItemList* items = new LIRItemList(i, i, NULL);\n+  LIRItemList* items = new LIRItemList(i, i, nullptr);\n@@ -1245,1 +1245,1 @@\n-  CodeEmitInfo* patching_info = NULL;\n+  CodeEmitInfo* patching_info = nullptr;\n@@ -1292,1 +1292,1 @@\n-  CodeEmitInfo* patching_info = NULL;\n+  CodeEmitInfo* patching_info = nullptr;\n@@ -1310,1 +1310,1 @@\n-    assert(patching_info == NULL, \"can't patch this\");\n+    assert(patching_info == nullptr, \"can't patch this\");\n@@ -1313,1 +1313,1 @@\n-    assert(patching_info == NULL, \"can't patch this\");\n+    assert(patching_info == nullptr, \"can't patch this\");\n@@ -1339,1 +1339,1 @@\n-  CodeEmitInfo* patching_info = NULL;\n+  CodeEmitInfo* patching_info = nullptr;\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":26,"deletions":26,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -68,2 +68,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr);\n@@ -75,1 +74,1 @@\n-  str(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+  str(obj, Address(disp_hdr, BasicObjectLock::obj_offset()));\n@@ -88,6 +87,43 @@\n-  \/\/ and mark it as unlocked\n-  orr(hdr, hdr, markWord::unlocked_value);\n-\n-  if (EnableValhalla) {\n-    \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n-    andr(hdr, hdr, ~markWord::inline_type_bit_in_place);\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    orr(hdr, hdr, markWord::unlocked_value);\n+\n+    if (EnableValhalla) {\n+      \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+      andr(hdr, hdr, ~markWord::inline_type_bit_in_place);\n+    }\n+\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    lea(rscratch2, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/nullptr);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    mov(rscratch1, sp);\n+    sub(hdr, hdr, rscratch1);\n+    ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    str(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    cbnz(hdr, slow_case);\n+    \/\/ done\n+    bind(done);\n@@ -95,32 +131,0 @@\n-\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  lea(rscratch2, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  mov(rscratch1, sp);\n-  sub(hdr, hdr, rscratch1);\n-  ands(hdr, hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n-  str(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  cbnz(hdr, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -138,5 +142,8 @@\n-  \/\/ load displaced header\n-  ldr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is NULL we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  cbz(hdr, done);\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    ldr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    cbz(hdr, done);\n+  }\n+\n@@ -144,1 +151,1 @@\n-  ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+  ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset()));\n@@ -146,10 +153,22 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    lea(rscratch1, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n-  } else {\n-    cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+    \/\/ be encoded.\n+    tst(hdr, markWord::monitor_value);\n+    br(Assembler::NE, slow_case);\n+    fast_unlock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      lea(rscratch1, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, rscratch1, rscratch2, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, rscratch2, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -157,2 +176,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -312,1 +329,1 @@\n-  \/\/ explicit NULL check not needed since load from [klass_offset] causes a trap\n+  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n@@ -341,2 +358,2 @@\n-  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n-  if (verified_inline_entry_label != NULL) {\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n+  if (verified_inline_entry_label != nullptr) {\n@@ -382,1 +399,1 @@\n-  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":80,"deletions":63,"binary":false,"changes":143,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -372,1 +372,1 @@\n-  OopMap* oop_map = NULL;\n+  OopMap* oop_map = nullptr;\n@@ -545,1 +545,1 @@\n-  assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+  assert(deopt_blob != nullptr, \"deoptimization blob must have been created\");\n@@ -619,2 +619,2 @@\n-  OopMapSet* oop_maps = NULL;\n-  OopMap* oop_map = NULL;\n+  OopMapSet* oop_maps = nullptr;\n+  OopMap* oop_map = nullptr;\n@@ -955,1 +955,1 @@\n-        __ check_klass_subtype_slow_path(r4, r0, r2, r5, NULL, &miss);\n+        __ check_klass_subtype_slow_path(r4, r0, r2, r5, nullptr, &miss);\n@@ -1025,1 +1025,1 @@\n-        assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+        assert(deopt_blob != nullptr, \"deoptimization blob must have been created\");\n@@ -1112,1 +1112,1 @@\n-        assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+        assert(deopt_blob != nullptr, \"deoptimization blob must have been created\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+\/\/ Clobbers: rscratch1, rscratch2, rflags. May also clobber v0-v1, when icnt1==-1.\n@@ -313,1 +314,1 @@\n-    RuntimeAddress stub = NULL;\n+    RuntimeAddress stub = nullptr;\n@@ -316,1 +317,1 @@\n-      assert(stub.target() != NULL, \"string_indexof_linear_ll stub has not been generated\");\n+      assert(stub.target() != nullptr, \"string_indexof_linear_ll stub has not been generated\");\n@@ -319,1 +320,1 @@\n-       assert(stub.target() != NULL, \"string_indexof_linear_ul stub has not been generated\");\n+       assert(stub.target() != nullptr, \"string_indexof_linear_ul stub has not been generated\");\n@@ -322,1 +323,1 @@\n-      assert(stub.target() != NULL, \"string_indexof_linear_uu stub has not been generated\");\n+      assert(stub.target() != nullptr, \"string_indexof_linear_uu stub has not been generated\");\n@@ -870,1 +871,1 @@\n-    RuntimeAddress stub = NULL;\n+    RuntimeAddress stub = nullptr;\n@@ -887,1 +888,1 @@\n-    assert(stub.target() != NULL, \"compare_long_string stub has not been generated\");\n+    assert(stub.target() != nullptr, \"compare_long_string stub has not been generated\");\n@@ -2075,1 +2076,1 @@\n-  if (ciEnv::current()->task() != NULL) {\n+  if (ciEnv::current()->task() != nullptr) {\n@@ -2077,1 +2078,1 @@\n-    if (phase_output != NULL && phase_output->in_scratch_emit_size()) {\n+    if (phase_output != nullptr && phase_output->in_scratch_emit_size()) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_MacroAssembler_aarch64.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-  if (_cb != NULL ) {\n+  if (_cb != nullptr ) {\n@@ -125,4 +125,4 @@\n-    intptr_t* sender_sp = NULL;\n-    intptr_t* sender_unextended_sp = NULL;\n-    address   sender_pc = NULL;\n-    intptr_t* saved_fp =  NULL;\n+    intptr_t* sender_sp = nullptr;\n+    intptr_t* sender_unextended_sp = nullptr;\n+    address   sender_pc = nullptr;\n+    intptr_t* saved_fp =  nullptr;\n@@ -195,1 +195,1 @@\n-    if (sender_pc == NULL ||  sender_blob == NULL) {\n+    if (sender_pc == nullptr ||  sender_blob == nullptr) {\n@@ -228,1 +228,1 @@\n-    if (nm != NULL) {\n+    if (nm != nullptr) {\n@@ -270,1 +270,1 @@\n-  if ( (address) this->fp()[return_addr_offset] == NULL) return false;\n+  if ( (address) this->fp()[return_addr_offset] == nullptr) return false;\n@@ -304,1 +304,1 @@\n-  if (original_pc != NULL) {\n+  if (original_pc != nullptr) {\n@@ -370,1 +370,1 @@\n-  assert(map != NULL, \"map must be set\");\n+  assert(map != nullptr, \"map must be set\");\n@@ -398,1 +398,1 @@\n-  return jfa->last_Java_sp() == NULL;\n+  return jfa->last_Java_sp() == nullptr;\n@@ -402,1 +402,1 @@\n-  assert(map != NULL, \"map must be set\");\n+  assert(map != nullptr, \"map must be set\");\n@@ -447,1 +447,1 @@\n-  if (_cb != NULL) {\n+  if (_cb != nullptr) {\n@@ -449,1 +449,1 @@\n-    if (sender_cm != NULL) {\n+    if (sender_cm != nullptr) {\n@@ -575,1 +575,1 @@\n-        obj = (obj_p == NULL) ? (oop)NULL : *obj_p;\n+        obj = (obj_p == nullptr) ? (oop)nullptr : *obj_p;\n@@ -643,1 +643,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -723,1 +723,1 @@\n-    if (cb != NULL) {\n+    if (cb != nullptr) {\n@@ -805,1 +805,1 @@\n-  if (last_Java_sp() == NULL) return;\n+  if (last_Java_sp() == nullptr) return;\n@@ -808,2 +808,2 @@\n-  vmassert(last_Java_sp() != NULL, \"not called from Java code?\");\n-  vmassert(last_Java_pc() == NULL, \"already walkable\");\n+  vmassert(last_Java_sp() != nullptr, \"not called from Java code?\");\n+  vmassert(last_Java_pc() == nullptr, \"already walkable\");\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":20,"deletions":20,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -45,5 +45,5 @@\n-  _pc = NULL;\n-  _sp = NULL;\n-  _unextended_sp = NULL;\n-  _fp = NULL;\n-  _cb = NULL;\n+  _pc = nullptr;\n+  _sp = nullptr;\n+  _unextended_sp = nullptr;\n+  _fp = nullptr;\n+  _cb = nullptr;\n@@ -66,1 +66,1 @@\n-  _oop_map = NULL;\n+  _oop_map = nullptr;\n@@ -70,1 +70,1 @@\n-  assert(pc != NULL, \"no pc?\");\n+  assert(pc != nullptr, \"no pc?\");\n@@ -79,1 +79,1 @@\n-  if (original_pc != NULL) {\n+  if (original_pc != nullptr) {\n@@ -82,1 +82,1 @@\n-    assert(_cb == NULL || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+    assert(_cb == nullptr || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n@@ -106,1 +106,1 @@\n-  assert(pc != NULL, \"no pc?\");\n+  assert(pc != nullptr, \"no pc?\");\n@@ -108,2 +108,2 @@\n-  _oop_map = NULL;\n-  assert(_cb != NULL, \"pc: \" INTPTR_FORMAT, p2i(pc));\n+  _oop_map = nullptr;\n+  assert(_cb != nullptr, \"pc: \" INTPTR_FORMAT, p2i(pc));\n@@ -130,1 +130,1 @@\n-  if (cb != NULL) {\n+  if (cb != nullptr) {\n@@ -150,2 +150,2 @@\n-  _oop_map = NULL;\n-  assert(_cb != NULL, \"pc: \" INTPTR_FORMAT \" sp: \" INTPTR_FORMAT \" unextended_sp: \" INTPTR_FORMAT \" fp: \" INTPTR_FORMAT, p2i(pc), p2i(sp), p2i(unextended_sp), p2i(fp));\n+  _oop_map = nullptr;\n+  assert(_cb != nullptr, \"pc: \" INTPTR_FORMAT \" sp: \" INTPTR_FORMAT \" unextended_sp: \" INTPTR_FORMAT \" fp: \" INTPTR_FORMAT, p2i(pc), p2i(sp), p2i(unextended_sp), p2i(fp));\n@@ -177,1 +177,1 @@\n-  \/\/ assert(_pc != NULL, \"no pc?\");\n+  \/\/ assert(_pc != nullptr, \"no pc?\");\n@@ -183,1 +183,1 @@\n-  if (original_pc != NULL) {\n+  if (original_pc != nullptr) {\n@@ -204,1 +204,1 @@\n-\/\/ identity and younger\/older relationship. NULL represents an invalid (incomparable)\n+\/\/ identity and younger\/older relationship. null represents an invalid (incomparable)\n@@ -209,1 +209,1 @@\n-inline bool frame::is_older(intptr_t* id) const   { assert(this->id() != NULL && id != NULL, \"NULL frame id\");\n+inline bool frame::is_older(intptr_t* id) const   { assert(this->id() != nullptr && id != nullptr, \"null frame id\");\n@@ -216,1 +216,1 @@\n-  return os::is_readable_pointer(ptr) ? *ptr : NULL;\n+  return os::is_readable_pointer(ptr) ? *ptr : nullptr;\n@@ -225,1 +225,1 @@\n-  if (_cb != NULL) {\n+  if (_cb != nullptr) {\n@@ -249,1 +249,1 @@\n-  assert(mask != NULL, \"\");\n+  assert(mask != nullptr, \"\");\n@@ -302,1 +302,1 @@\n-  if (last_sp == NULL) {\n+  if (last_sp == nullptr) {\n@@ -342,1 +342,1 @@\n-  guarantee(result_adr != NULL, \"bad register save location\");\n+  guarantee(result_adr != nullptr, \"bad register save location\");\n@@ -348,1 +348,1 @@\n-  guarantee(result_adr != NULL, \"bad register save location\");\n+  guarantee(result_adr != nullptr, \"bad register save location\");\n@@ -362,2 +362,2 @@\n-  if (_cb == NULL) return NULL;\n-  if (_cb->oop_maps() != NULL) {\n+  if (_cb == nullptr) return nullptr;\n+  if (_cb->oop_maps() != nullptr) {\n@@ -365,1 +365,1 @@\n-    if (nop != NULL && nop->displacement() != 0) {\n+    if (nop != nullptr && nop->displacement() != 0) {\n@@ -372,1 +372,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -401,1 +401,1 @@\n-  if (_cb != NULL) return sender_for_compiled_frame(map);\n+  if (_cb != nullptr) return sender_for_compiled_frame(map);\n@@ -452,1 +452,1 @@\n-    if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+    if (nm != nullptr && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n@@ -463,1 +463,1 @@\n-      if (oop_map() != NULL) {\n+      if (oop_map() != nullptr) {\n@@ -469,1 +469,1 @@\n-      assert(oop_map() == NULL || !oop_map()->has_any(OopMapValue::callee_saved_value), \"callee-saved value in compiled frame\");\n+      assert(oop_map() == nullptr || !oop_map()->has_any(OopMapValue::callee_saved_value), \"callee-saved value in compiled frame\");\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.inline.hpp","additions":33,"deletions":33,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -182,1 +182,1 @@\n-  \/\/ that checks that the *(rfp+frame::interpreter_frame_last_sp) == NULL.\n+  \/\/ that checks that the *(rfp+frame::interpreter_frame_last_sp) == nullptr.\n@@ -238,1 +238,1 @@\n-  \/\/ crosses regions, storing NULL?\n+  \/\/ crosses regions, storing null?\n@@ -242,1 +242,1 @@\n-  \/\/ storing region crossing non-NULL, is card already dirty?\n+  \/\/ storing region crossing non-null, is card already dirty?\n@@ -264,1 +264,1 @@\n-  \/\/ storing a region crossing, non-NULL oop, card is clean.\n+  \/\/ storing a region crossing, non-null oop, card is clean.\n@@ -490,1 +490,1 @@\n-  \/\/ At this point we know new_value is non-NULL and the new_value crosses regions.\n+  \/\/ At this point we know new_value is non-null and the new_value crosses regions.\n@@ -517,1 +517,1 @@\n-  \/\/ storing region crossing non-NULL, card is clean.\n+  \/\/ storing region crossing non-null, card is clean.\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -330,1 +330,1 @@\n-  if (bs_nm == NULL) {\n+  if (bs_nm == nullptr) {\n@@ -338,1 +338,1 @@\n-  if (slow_path == NULL) {\n+  if (slow_path == nullptr) {\n@@ -343,2 +343,2 @@\n-  Assembler::Condition condition = slow_path == NULL ? Assembler::EQ : Assembler::NE;\n-  Label& barrier_target = slow_path == NULL ? skip_barrier : *slow_path;\n+  Assembler::Condition condition = slow_path == nullptr ? Assembler::EQ : Assembler::NE;\n+  Label& barrier_target = slow_path == nullptr ? skip_barrier : *slow_path;\n@@ -390,1 +390,1 @@\n-  if (slow_path == NULL) {\n+  if (slow_path == nullptr) {\n@@ -407,1 +407,1 @@\n-  if (bs == NULL) {\n+  if (bs == nullptr) {\n@@ -450,1 +450,1 @@\n-  __ cbz(obj, error);      \/\/ if klass is NULL it is broken\n+  __ cbz(obj, error);      \/\/ if klass is null it is broken\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -39,0 +39,4 @@\n+#include \"utilities\/formatBuffer.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmciRuntime.hpp\"\n+#endif\n@@ -45,1 +49,1 @@\n-\/\/ This is the offset of the entry barrier from where the frame is completed.\n+\/\/ This is the offset of the entry barrier relative to where the frame is completed.\n@@ -65,2 +69,11 @@\n-class NativeNMethodBarrier: public NativeInstruction {\n-  address instruction_address() const { return addr_at(0); }\n+class NativeNMethodBarrier {\n+  address  _instruction_address;\n+  int*     _guard_addr;\n+  nmethod* _nm;\n+\n+public:\n+  address instruction_address() const { return _instruction_address; }\n+\n+  int *guard_addr() {\n+    return _guard_addr;\n+  }\n@@ -73,9 +86,28 @@\n-  int *guard_addr(nmethod* nm) {\n-    if (nm->is_compiled_by_c2()) {\n-      \/\/ With c2 compiled code, the guard is out-of-line in a stub\n-      \/\/ We find it using the RelocIterator.\n-      RelocIterator iter(nm);\n-      while (iter.next()) {\n-        if (iter.type() == relocInfo::entry_guard_type) {\n-          entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n-          return reinterpret_cast<int*>(reloc->addr());\n+  NativeNMethodBarrier(nmethod* nm, address alt_entry_instruction_address = 0): _nm(nm) {\n+#if INCLUDE_JVMCI\n+    if (nm->is_compiled_by_jvmci()) {\n+      assert(alt_entry_instruction_address == 0, \"invariant\");\n+      address pc = nm->code_begin() + nm->jvmci_nmethod_data()->nmethod_entry_patch_offset();\n+      RelocIterator iter(nm, pc, pc + 4);\n+      guarantee(iter.next(), \"missing relocs\");\n+      guarantee(iter.type() == relocInfo::section_word_type, \"unexpected reloc\");\n+\n+      _guard_addr = (int*) iter.section_word_reloc()->target();\n+      _instruction_address = pc;\n+    } else\n+#endif\n+      {\n+        _instruction_address = (alt_entry_instruction_address != 0) ? alt_entry_instruction_address :\n+          nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n+        if (nm->is_compiled_by_c2()) {\n+          \/\/ With c2 compiled code, the guard is out-of-line in a stub\n+          \/\/ We find it using the RelocIterator.\n+          RelocIterator iter(nm);\n+          while (iter.next()) {\n+            if (iter.type() == relocInfo::entry_guard_type) {\n+              entry_guard_Relocation* const reloc = iter.entry_guard_reloc();\n+              _guard_addr = reinterpret_cast<int*>(reloc->addr());\n+              return;\n+            }\n+          }\n+          ShouldNotReachHere();\n@@ -83,0 +115,1 @@\n+        _guard_addr =  reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -84,3 +117,0 @@\n-      ShouldNotReachHere();\n-    }\n-    return reinterpret_cast<int*>(instruction_address() + local_guard_offset(nm));\n@@ -89,3 +119,2 @@\n-public:\n-  int get_value(nmethod* nm) {\n-    return Atomic::load_acquire(guard_addr(nm));\n+  int get_value() {\n+    return Atomic::load_acquire(guard_addr());\n@@ -94,2 +123,2 @@\n-  void set_value(nmethod* nm, int value) {\n-    Atomic::release_store(guard_addr(nm), value);\n+  void set_value(int value) {\n+    Atomic::release_store(guard_addr(), value);\n@@ -98,8 +127,5 @@\n-  void verify() const;\n-};\n-\n-\/\/ Store the instruction bitmask, bits and name for checking the barrier.\n-struct CheckInsn {\n-  uint32_t mask;\n-  uint32_t bits;\n-  const char *name;\n+  bool check_barrier(err_msg& msg) const;\n+  void verify() const {\n+    err_msg msg(\"%s\", \"\");\n+    assert(check_barrier(msg), \"%s\", msg.buffer());\n+  }\n@@ -110,1 +136,1 @@\n-void NativeNMethodBarrier::verify() const {\n+bool NativeNMethodBarrier::check_barrier(err_msg& msg) const {\n@@ -114,2 +140,3 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", (intptr_t)addr, inst);\n-    fatal(\"not an ldr (literal) instruction.\");\n+    msg.print(\"Nmethod entry barrier did not start with ldr (literal) as expected. \"\n+              \"Addr: \" PTR_FORMAT \" Code: \" UINT32_FORMAT, p2i(addr), inst);\n+    return false;\n@@ -117,0 +144,1 @@\n+  return true;\n@@ -159,7 +187,0 @@\n-static NativeNMethodBarrier* native_nmethod_barrier(nmethod* nm) {\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n-  NativeNMethodBarrier* barrier = reinterpret_cast<NativeNMethodBarrier*>(barrier_address);\n-  debug_only(barrier->verify());\n-  return barrier;\n-}\n-\n@@ -167,2 +188,2 @@\n-  NativeNMethodBarrier* cmp1 = native_nmethod_barrier(nm);\n-  cmp1->set_value(nm, val);\n+  NativeNMethodBarrier cmp1 = NativeNMethodBarrier(nm);\n+  cmp1.set_value(val);\n@@ -176,5 +197,5 @@\n-    int barrier_offset = reinterpret_cast<address>(cmp1) - method_body;\n-    NativeNMethodBarrier* cmp2 = reinterpret_cast<NativeNMethodBarrier*>(entry_point2 + barrier_offset);\n-    assert(cmp1 != cmp2, \"sanity\");\n-    debug_only(cmp2->verify());\n-    cmp2->set_value(nm, val);\n+    int barrier_offset = cmp1.instruction_address() - method_body;\n+    NativeNMethodBarrier cmp2 = NativeNMethodBarrier(nm, entry_point2 + barrier_offset);\n+    assert(cmp1.instruction_address() != cmp2.instruction_address(), \"sanity\");\n+    debug_only(cmp2.verify());\n+    cmp2.set_value(val);\n@@ -183,4 +204,4 @@\n-      NativeNMethodBarrier* cmp3 = reinterpret_cast<NativeNMethodBarrier*>(nm->verified_inline_ro_entry_point() + barrier_offset);\n-      assert(cmp1 != cmp3 && cmp2 != cmp3, \"sanity\");\n-      debug_only(cmp3->verify());\n-      cmp3->set_value(nm, val);\n+      NativeNMethodBarrier cmp3 = NativeNMethodBarrier(nm, nm->verified_inline_ro_entry_point() + barrier_offset);\n+      assert(cmp1.instruction_address() != cmp3.instruction_address() && cmp2.instruction_address() != cmp3.instruction_address(), \"sanity\");\n+      debug_only(cmp3.verify());\n+      cmp3.set_value(val);\n@@ -215,2 +236,8 @@\n-  NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);\n-  return barrier->get_value(nm);\n+  NativeNMethodBarrier barrier(nm);\n+  return barrier.get_value();\n+}\n+\n+#if INCLUDE_JVMCI\n+bool BarrierSetNMethod::verify_barrier(nmethod* nm, err_msg& msg) {\n+  NativeNMethodBarrier barrier(nm);\n+  return barrier.check_barrier(msg);\n@@ -218,0 +245,1 @@\n+#endif\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":81,"deletions":53,"binary":false,"changes":134,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-define_pd_global(bool, UncommonNullCast,         true);  \/\/ Uncommon-trap NULLs past to check cast\n+define_pd_global(bool, UncommonNullCast,         true);  \/\/ Uncommon-trap nulls past to check cast\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -144,1 +145,1 @@\n-    cbz(rscratch1, L); \/\/ if (thread->jvmti_thread_state() == NULL) exit;\n+    cbz(rscratch1, L); \/\/ if (thread->jvmti_thread_state() == nullptr) exit;\n@@ -343,2 +344,2 @@\n-  ldr(result, Address(result, ConstantPool::cache_offset_in_bytes()));\n-  ldr(result, Address(result, ConstantPoolCache::resolved_references_offset_in_bytes()));\n+  ldr(result, Address(result, ConstantPool::cache_offset()));\n+  ldr(result, Address(result, ConstantPoolCache::resolved_references_offset()));\n@@ -355,1 +356,1 @@\n-  ldr(klass, Address(cpool,  ConstantPool::resolved_klasses_offset_in_bytes())); \/\/ klass = cpool->_resolved_klasses\n+  ldr(klass, Address(cpool,  ConstantPool::resolved_klasses_offset())); \/\/ klass = cpool->_resolved_klasses\n@@ -587,1 +588,1 @@\n-  dispatch_base(state, Interpreter::dispatch_table(state), generate_poll);\n+  dispatch_base(state, Interpreter::dispatch_table(state), \/*verifyoop*\/true, generate_poll);\n@@ -661,1 +662,1 @@\n-  ldr(r0, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+  ldr(r0, Address(c_rarg1, BasicObjectLock::obj_offset()));\n@@ -738,1 +739,1 @@\n-    ldr(rscratch1, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+    ldr(rscratch1, Address(c_rarg1, BasicObjectLock::obj_offset()));\n@@ -799,1 +800,1 @@\n-      tstw(rscratch1, Method::scalarized_return_flag());\n+      tstw(rscratch1, ConstMethodFlags::has_scalarized_return_flag());\n@@ -834,1 +835,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -845,2 +846,2 @@\n-    const int obj_offset = BasicObjectLock::obj_offset_in_bytes();\n-    const int lock_offset = BasicObjectLock::lock_offset_in_bytes ();\n+    const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n+    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n@@ -862,53 +863,12 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-    if (EnableValhalla) {\n-      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-      andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n-    }\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-    br(Assembler::EQ, count);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n+      b(count);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -916,0 +876,46 @@\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+      br(Assembler::EQ, count);\n+    }\n@@ -919,3 +925,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -947,1 +959,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -958,3 +970,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset()));\n+    }\n@@ -963,1 +977,1 @@\n-    ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+    ldr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -966,11 +980,17 @@\n-    str(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n-\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Test for recursion\n-    cbz(header_reg, count);\n-\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+    str(zr, Address(lock_reg, BasicObjectLock::obj_offset()));\n+\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      Label slow_case;\n+\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = rscratch1;\n+      \/\/ First check for lock-stack underflow.\n+      ldrw(tmp, Address(rthread, JavaThread::lock_stack_top_offset()));\n+      cmpw(tmp, (unsigned)LockStack::start_offset());\n+      br(Assembler::LE, slow_case);\n+      \/\/ Then check if the top of the lock-stack matches the unlocked object.\n+      subw(tmp, tmp, oopSize);\n+      ldr(tmp, Address(rthread, tmp));\n+      cmpoop(tmp, obj_reg);\n+      br(Assembler::NE, slow_case);\n@@ -978,0 +998,16 @@\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      tbnz(header_reg, exact_log2(markWord::monitor_value), slow_case);\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(count);\n+      bind(slow_case);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      cbz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n+    }\n@@ -979,1 +1015,1 @@\n-    str(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes())); \/\/ restore obj\n+    str(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset())); \/\/ restore obj\n@@ -1004,1 +1040,1 @@\n-  \/\/ Test MDO to avoid the call if it is NULL.\n+  \/\/ Test MDO to avoid the call if it is null.\n@@ -1395,1 +1431,1 @@\n-  \/\/ observed the item[start_row] is NULL.\n+  \/\/ observed the item[start_row] is null.\n@@ -1411,1 +1447,1 @@\n-\/\/   if (row[0].rec != NULL) {\n+\/\/   if (row[0].rec != nullptr) {\n@@ -1414,1 +1450,1 @@\n-\/\/     if (row[1].rec != NULL) {\n+\/\/     if (row[1].rec != nullptr) {\n@@ -1417,1 +1453,1 @@\n-\/\/       if (row[2].rec != NULL) { count.incr(); goto done; } \/\/ overflow\n+\/\/       if (row[2].rec != nullptr) { count.incr(); goto done; } \/\/ overflow\n@@ -1770,1 +1806,1 @@\n-         \" last_sp != NULL\");\n+         \" last_sp != nullptr\");\n@@ -1798,1 +1834,1 @@\n-         \" last_sp != NULL\");\n+         \" last_sp != nullptr\");\n@@ -1957,1 +1993,1 @@\n-      ldrh(rscratch1, Address(tmp, Method::intrinsic_id_offset_in_bytes()));\n+      ldrh(rscratch1, Address(tmp, Method::intrinsic_id_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":128,"deletions":92,"binary":false,"changes":220,"status":"modified"},{"patch":"@@ -132,1 +132,1 @@\n-    ldr(reg, Address(reg, ConstantPool::cache_offset_in_bytes()));\n+    ldr(reg, Address(reg, ConstantPool::cache_offset()));\n@@ -137,1 +137,1 @@\n-    ldr(tags, Address(cpool, ConstantPool::tags_offset_in_bytes()));\n+    ldr(tags, Address(cpool, ConstantPool::tags_offset()));\n@@ -200,1 +200,1 @@\n-    \/\/ NULL last_sp until next java call\n+    \/\/ null last_sp until next java call\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -279,1 +279,1 @@\n-    intptr_t value = *addr == 0 ? NULL : (intptr_t)addr;\n+    intptr_t value = *addr == 0 ? (intptr_t)0 : (intptr_t)addr;\n","filename":"src\/hotspot\/cpu\/aarch64\/interpreterRT_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2004, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n@@ -112,1 +112,1 @@\n-      name = NULL;  \/\/ unreachable\n+      name = nullptr;  \/\/ unreachable\n@@ -200,1 +200,1 @@\n-      slow_case_addr = NULL;  \/\/ unreachable\n+      slow_case_addr = nullptr;  \/\/ unreachable\n","filename":"src\/hotspot\/cpu\/aarch64\/jniFastGetField_aarch64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -654,1 +654,1 @@\n-  assert(last_java_pc != NULL, \"must provide a valid PC\");\n+  assert(last_java_pc != nullptr, \"must provide a valid PC\");\n@@ -692,1 +692,1 @@\n-  assert(CodeCache::find_blob(entry.target()) != NULL,\n+  assert(CodeCache::find_blob(entry.target()) != nullptr,\n@@ -711,1 +711,1 @@\n-  assert(CodeCache::find_blob(entry.target()) != NULL,\n+  assert(CodeCache::find_blob(entry.target()) != nullptr,\n@@ -869,1 +869,1 @@\n-    assert(CodeCache::find_blob(target) != NULL &&\n+    assert(CodeCache::find_blob(target) != nullptr &&\n@@ -901,1 +901,1 @@\n-        if (stub == NULL) {\n+        if (stub == nullptr) {\n@@ -903,1 +903,1 @@\n-          return NULL; \/\/ CodeCache is full\n+          return nullptr; \/\/ CodeCache is full\n@@ -933,2 +933,2 @@\n-  if (stub == NULL) {\n-    return NULL;  \/\/ CodeBuffer::expand failed\n+  if (stub == nullptr) {\n+    return nullptr;  \/\/ CodeBuffer::expand failed\n@@ -974,1 +974,1 @@\n-  mov_metadata(rmethod, (Metadata*)NULL);\n+  mov_metadata(rmethod, nullptr);\n@@ -1184,1 +1184,1 @@\n-  int itentry_off = itableMethodEntry::method_offset_in_bytes();\n+  int itentry_off = in_bytes(itableMethodEntry::method_offset());\n@@ -1205,1 +1205,1 @@\n-  \/\/ for (scan = klass->itable(); scan->interface() != NULL; scan += scan_step) {\n+  \/\/ for (scan = klass->itable(); scan->interface() != nullptr; scan += scan_step) {\n@@ -1212,1 +1212,1 @@\n-  ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));\n+  ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset()));\n@@ -1220,1 +1220,1 @@\n-  if (itableOffsetEntry::interface_offset_in_bytes() != 0) {\n+  if (itableOffsetEntry::interface_offset() != 0) {\n@@ -1222,1 +1222,1 @@\n-    ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));\n+    ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset()));\n@@ -1233,1 +1233,1 @@\n-    ldrw(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));\n+    ldrw(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset()));\n@@ -1242,1 +1242,0 @@\n-  const int base = in_bytes(Klass::vtable_start_offset());\n@@ -1245,1 +1244,1 @@\n-  int vtable_offset_in_bytes = base + vtableEntry::method_offset_in_bytes();\n+  int64_t vtable_offset_in_bytes = in_bytes(Klass::vtable_start_offset() + vtableEntry::method_offset());\n@@ -1264,2 +1263,2 @@\n-  check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &L_success, &L_failure, NULL);\n-  check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &L_success, NULL);\n+  check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &L_success, &L_failure, nullptr);\n+  check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &L_success, nullptr);\n@@ -1288,4 +1287,4 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  if (L_slow_path == NULL) { L_slow_path = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_slow_path == nullptr) { L_slow_path = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -1410,3 +1409,3 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -1483,1 +1482,1 @@\n-  assert(L_fast_path != NULL || L_slow_path != NULL, \"at least one is required\");\n+  assert(L_fast_path != nullptr || L_slow_path != nullptr, \"at least one is required\");\n@@ -1487,1 +1486,1 @@\n-  if (L_fast_path == NULL) {\n+  if (L_fast_path == nullptr) {\n@@ -1489,1 +1488,1 @@\n-  } else if (L_slow_path == NULL) {\n+  } else if (L_slow_path == nullptr) {\n@@ -1520,1 +1519,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -1556,1 +1555,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -1695,1 +1694,1 @@\n-    \/\/ provoke OS NULL exception if reg = NULL by\n+    \/\/ provoke OS null exception if reg is null by\n@@ -1701,1 +1700,1 @@\n-    \/\/ will provoke OS NULL exception if reg = NULL\n+    \/\/ will provoke OS null exception if reg is null\n@@ -2129,1 +2128,1 @@\n-  if (last != NULL && nativeInstruction_at(last)->is_Membar() && prev == last) {\n+  if (last != nullptr && nativeInstruction_at(last)->is_Membar() && prev == last) {\n@@ -2614,2 +2613,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  if (!UseCompressedOops || Universe::ptr_base() == NULL) {\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  if (!UseCompressedOops || Universe::ptr_base() == nullptr) {\n@@ -2636,1 +2635,1 @@\n-  cbz(value, done);           \/\/ Use NULL as-is.\n+  cbz(value, done);           \/\/ Use null as-is.\n@@ -2667,1 +2666,1 @@\n-  cbz(value, done);           \/\/ Use NULL as-is.\n+  cbz(value, done);           \/\/ Use null as-is.\n@@ -2693,1 +2692,1 @@\n-  const char* buf = NULL;\n+  const char* buf = nullptr;\n@@ -3262,1 +3261,1 @@\n-  if (last == NULL || !nativeInstruction_at(last)->is_Imm_LdSt()) {\n+  if (last == nullptr || !nativeInstruction_at(last)->is_Imm_LdSt()) {\n@@ -4477,1 +4476,1 @@\n-  ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); \/\/ InstanceKlass*\n+  ldr(holder, Address(holder, ConstantPool::pool_holder_offset()));          \/\/ InstanceKlass*\n@@ -4522,1 +4521,1 @@\n-  ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));\n+  ldr(dst, Address(dst, ConstantPool::pool_holder_offset()));\n@@ -4530,1 +4529,1 @@\n-    if (CompressedKlassPointers::base() == NULL) {\n+    if (CompressedKlassPointers::base() == nullptr) {\n@@ -4575,1 +4574,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -4608,1 +4607,1 @@\n-  if (CompressedOops::base() != NULL) {\n+  if (CompressedOops::base() != nullptr) {\n@@ -4630,1 +4629,1 @@\n-  if (CompressedOops::base() != NULL) {\n+  if (CompressedOops::base() != nullptr) {\n@@ -4647,1 +4646,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -4664,1 +4663,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -4670,1 +4669,1 @@\n-    if (CompressedOops::base() != NULL) {\n+    if (CompressedOops::base() != nullptr) {\n@@ -4676,1 +4675,1 @@\n-    assert (CompressedOops::base() == NULL, \"sanity\");\n+    assert (CompressedOops::base() == nullptr, \"sanity\");\n@@ -4682,1 +4681,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -4688,1 +4687,1 @@\n-    if (CompressedOops::base() != NULL) {\n+    if (CompressedOops::base() != nullptr) {\n@@ -4694,1 +4693,1 @@\n-    assert (CompressedOops::base() == NULL, \"sanity\");\n+    assert (CompressedOops::base() == nullptr, \"sanity\");\n@@ -4714,1 +4713,1 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n+  if (CompressedKlassPointers::base() == nullptr) {\n@@ -4822,2 +4821,2 @@\n-    assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-    assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+    assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+    assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -4837,1 +4836,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -4930,1 +4929,1 @@\n-\/\/ Used for storing NULLs.\n+\/\/ Used for storing nulls.\n@@ -4936,1 +4935,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs a Recorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs a Recorder\");\n@@ -4945,1 +4944,1 @@\n-  if (obj == NULL) {\n+  if (obj == nullptr) {\n@@ -4970,1 +4969,1 @@\n-  if (obj == NULL) {\n+  if (obj == nullptr) {\n@@ -4983,1 +4982,1 @@\n-    assert(oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+    assert(oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5357,1 +5356,1 @@\n-    assert(count_pos.target() != NULL, \"count_positives stub has not been generated\");\n+    assert(count_pos.target() != nullptr, \"count_positives stub has not been generated\");\n@@ -5359,1 +5358,1 @@\n-    if (tpc1 == NULL) {\n+    if (tpc1 == nullptr) {\n@@ -5362,1 +5361,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -5368,1 +5367,1 @@\n-    assert(count_pos_long.target() != NULL, \"count_positives_long stub has not been generated\");\n+    assert(count_pos_long.target() != nullptr, \"count_positives_long stub has not been generated\");\n@@ -5370,1 +5369,1 @@\n-    if (tpc2 == NULL) {\n+    if (tpc2 == nullptr) {\n@@ -5373,1 +5372,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -5387,0 +5386,2 @@\n+\/\/ Clobbers: rscratch1, rscratch2, rflags\n+\/\/ May also clobber v0-v7 when (!UseSimpleArrayEquals && UseSIMDForArrayEquals)\n@@ -5420,1 +5421,1 @@\n-    \/\/ if (a1 == null || a2 == null)\n+    \/\/ if (a1 == nullptr || a2 == nullptr)\n@@ -5551,1 +5552,1 @@\n-    assert(stub.target() != NULL, \"array_equals_long stub has not been generated\");\n+    assert(stub.target() != nullptr, \"array_equals_long stub has not been generated\");\n@@ -5553,1 +5554,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -5556,1 +5557,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -5703,1 +5704,1 @@\n-  assert(zero_blocks.target() != NULL, \"zero_blocks stub has not been generated\");\n+  assert(zero_blocks.target() != nullptr, \"zero_blocks stub has not been generated\");\n@@ -5710,1 +5711,1 @@\n-    assert(zero_blocks.target() != NULL, \"zero_blocks stub has not been generated\");\n+    assert(zero_blocks.target() != nullptr, \"zero_blocks stub has not been generated\");\n@@ -5721,1 +5722,1 @@\n-      if (tpc == NULL) {\n+      if (tpc == nullptr) {\n@@ -5723,1 +5724,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -5936,0 +5937,1 @@\n+\/\/ Clobbers: src, dst, res, rscratch1, rscratch2, rflags\n@@ -5939,1 +5941,2 @@\n-                                      FloatRegister vtmp2, FloatRegister vtmp3)\n+                                      FloatRegister vtmp2, FloatRegister vtmp3,\n+                                      FloatRegister vtmp4, FloatRegister vtmp5)\n@@ -5958,2 +5961,2 @@\n-    FloatRegister vlo0 = v4;\n-    FloatRegister vlo1 = v5;\n+    FloatRegister vlo0 = vtmp4;\n+    FloatRegister vlo1 = vtmp5;\n@@ -6032,0 +6035,1 @@\n+\/\/ Clobbers: src, dst, len, rflags, rscratch1, v0-v6\n@@ -6072,1 +6076,1 @@\n-      assert(stub.target() != NULL, \"large_byte_array_inflate stub has not been generated\");\n+      assert(stub.target() != nullptr, \"large_byte_array_inflate stub has not been generated\");\n@@ -6074,1 +6078,1 @@\n-      if (tpc == NULL) {\n+      if (tpc == nullptr) {\n@@ -6077,1 +6081,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -6140,2 +6144,3 @@\n-                                         FloatRegister tmp2, FloatRegister tmp3) {\n-  encode_iso_array(src, dst, len, res, false, tmp0, tmp1, tmp2, tmp3);\n+                                         FloatRegister tmp2, FloatRegister tmp3,\n+                                         FloatRegister tmp4, FloatRegister tmp5) {\n+  encode_iso_array(src, dst, len, res, false, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5);\n@@ -6905,1 +6910,1 @@\n-  \/\/ See if oop is NULL if it is we need no handle\n+  \/\/ See if oop is null if it is we need no handle\n@@ -6918,1 +6923,1 @@\n-    \/\/ conditionally move a NULL\n+    \/\/ conditionally move a null\n@@ -6924,1 +6929,1 @@\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-null\n@@ -6951,1 +6956,1 @@\n-    \/\/ Store oop in handle area, may be NULL\n+    \/\/ Store oop in handle area, may be null\n@@ -6959,1 +6964,1 @@\n-    \/\/ conditionally move a NULL\n+    \/\/ conditionally move a null\n@@ -7026,0 +7031,94 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with ZF set.\n+\/\/\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n+  br(Assembler::GT, slow);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(rthread, t1));\n+  addw(t1, t1, oopSize);\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with ZF set.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object\n+\/\/ - t1, t2: temporary registers\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t1, (unsigned)LockStack::start_offset());\n+    br(Assembler::GT, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    subw(t1, t1, oopSize);\n+    ldr(t1, Address(rthread, t1));\n+    cmpoop(t1, obj);\n+    br(Assembler::EQ, tos_ok);\n+    STOP(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+    Label hdr_ok;\n+    tst(hdr, markWord::lock_mask_in_place);\n+    br(Assembler::EQ, hdr_ok);\n+    STOP(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+#endif\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(t1, t1, oopSize);\n+#ifdef ASSERT\n+  str(zr, Address(rthread, t1));\n+#endif\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":188,"deletions":89,"binary":false,"changes":277,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-    Label *retaddr = NULL\n+    Label *retaddr = nullptr\n@@ -610,1 +610,1 @@\n-  \/\/ Support for NULL-checks\n+  \/\/ Support for null-checks\n@@ -612,1 +612,1 @@\n-  \/\/ Generates code that causes a NULL OS exception if the content of reg is NULL.\n+  \/\/ Generates code that causes a null OS exception if the content of reg is null.\n@@ -666,1 +666,1 @@\n-  static void pd_patch_instruction(address branch, address target, const char* file = NULL, int line = 0) {\n+  static void pd_patch_instruction(address branch, address target, const char* file = nullptr, int line = 0) {\n@@ -922,1 +922,1 @@\n-  \/\/ Used for storing NULL. All other oop constants should be\n+  \/\/ Used for storing null. All other oop constants should be\n@@ -931,1 +931,1 @@\n-  \/\/ converting a zero (like NULL) into a Register by giving\n+  \/\/ converting a zero (like null) into a Register by giving\n@@ -1015,1 +1015,1 @@\n-  \/\/ One of the three labels can be NULL, meaning take the fall-through.\n+  \/\/ One of the three labels can be null, meaning take the fall-through.\n@@ -1048,2 +1048,2 @@\n-                      Label* L_fast_path = NULL,\n-                      Label* L_slow_path = NULL);\n+                      Label* L_fast_path = nullptr,\n+                      Label* L_slow_path = nullptr);\n@@ -1252,1 +1252,1 @@\n-  \/\/ Return: the call PC or NULL if CodeCache is full.\n+  \/\/ Return: the call PC or null if CodeCache is full.\n@@ -1477,1 +1477,2 @@\n-                           FloatRegister vtmp2, FloatRegister vtmp3);\n+                           FloatRegister vtmp2, FloatRegister vtmp3,\n+                           FloatRegister vtmp4, FloatRegister vtmp5);\n@@ -1482,1 +1483,2 @@\n-                        FloatRegister vtmp2, FloatRegister vtmp3);\n+                        FloatRegister vtmp2, FloatRegister vtmp3,\n+                        FloatRegister vtmp4, FloatRegister vtmp5);\n@@ -1664,0 +1666,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":17,"deletions":12,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -182,1 +182,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -189,1 +189,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -210,1 +210,1 @@\n-    __ ldrh(rscratch1, Address(rmethod, Method::intrinsic_id_offset_in_bytes()));\n+    __ ldrh(rscratch1, Address(rmethod, Method::intrinsic_id_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -834,1 +834,1 @@\n-      (Interpreter::code() != NULL || StubRoutines::final_stubs_code() != NULL)) {\n+      (Interpreter::code() != nullptr || StubRoutines::final_stubs_code() != nullptr)) {\n@@ -842,1 +842,1 @@\n-    if (Interpreter::code() != NULL) {\n+    if (Interpreter::code() != nullptr) {\n@@ -847,1 +847,1 @@\n-    if (StubRoutines::initial_stubs_code() != NULL) {\n+    if (StubRoutines::initial_stubs_code() != nullptr) {\n@@ -853,1 +853,1 @@\n-    if (StubRoutines::final_stubs_code() != NULL) {\n+    if (StubRoutines::final_stubs_code() != nullptr) {\n@@ -1054,1 +1054,1 @@\n-  address c2i_no_clinit_check_entry = NULL;\n+  address c2i_no_clinit_check_entry = nullptr;\n@@ -1095,1 +1095,1 @@\n-  assert(regs2 == NULL, \"not needed on AArch64\");\n+  assert(regs2 == nullptr, \"not needed on AArch64\");\n@@ -1714,1 +1714,1 @@\n-                                       (OopMapSet*)NULL);\n+                                       nullptr);\n@@ -1717,1 +1717,1 @@\n-  assert(native_func != NULL, \"must have function\");\n+  assert(native_func != nullptr, \"must have function\");\n@@ -1734,1 +1734,1 @@\n-  BasicType* in_elem_bt = NULL;\n+  BasicType* in_elem_bt = nullptr;\n@@ -1749,1 +1749,1 @@\n-  out_arg_slots = c_calling_convention_priv(out_sig_bt, out_regs, NULL, total_c_args);\n+  out_arg_slots = c_calling_convention_priv(out_sig_bt, out_regs, nullptr, total_c_args);\n@@ -1752,1 +1752,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -1874,1 +1874,1 @@\n-  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n+  bs->nmethod_entry_barrier(masm, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n@@ -2072,1 +2072,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ b(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -2085,1 +2087,1 @@\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/nullptr);\n@@ -2106,1 +2108,3 @@\n-      __ b(slow_path_lock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n@@ -2216,1 +2220,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -2231,1 +2235,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ b(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -2243,1 +2249,5 @@\n-      __ b(slow_path_unlock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ tbnz(old_hdr, exact_log2(markWord::monitor_value), slow_path_unlock);\n+      __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+      __ decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -2278,1 +2288,1 @@\n-  __ str(zr, Address(r2, JNIHandleBlock::top_offset_in_bytes()));\n+  __ str(zr, Address(r2, JNIHandleBlock::top_offset()));\n@@ -2477,1 +2487,1 @@\n-  OopMap* map = NULL;\n+  OopMap* map = nullptr;\n@@ -2678,1 +2688,1 @@\n-  __ ldrw(rcpool, Address(r5, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));\n+  __ ldrw(rcpool, Address(r5, Deoptimization::UnrollBlock::unpack_kind_offset()));\n@@ -2683,1 +2693,1 @@\n-  \/\/ QQQ this is useless it was NULL above\n+  \/\/ QQQ this is useless it was null above\n@@ -2724,1 +2734,1 @@\n-  __ ldrw(r2, Address(r5, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));\n+  __ ldrw(r2, Address(r5, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset()));\n@@ -2735,1 +2745,1 @@\n-  __ ldrw(r19, Address(r5, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));\n+  __ ldrw(r19, Address(r5, Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n@@ -2739,1 +2749,1 @@\n-  __ ldr(r2, Address(r5, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+  __ ldr(r2, Address(r5, Deoptimization::UnrollBlock::frame_pcs_offset()));\n@@ -2745,1 +2755,1 @@\n-  __ ldr(r4, Address(r5, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));\n+  __ ldr(r4, Address(r5, Deoptimization::UnrollBlock::frame_sizes_offset()));\n@@ -2748,1 +2758,1 @@\n-  __ ldrw(r3, Address(r5, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));\n+  __ ldrw(r3, Address(r5, Deoptimization::UnrollBlock::number_of_frames_offset()));\n@@ -2760,1 +2770,1 @@\n-                       caller_adjustment_offset_in_bytes()));\n+                       caller_adjustment_offset()));\n@@ -2920,1 +2930,1 @@\n-    __ ldrw(rscratch1, Address(r4, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));\n+    __ ldrw(rscratch1, Address(r4, Deoptimization::UnrollBlock::unpack_kind_offset()));\n@@ -2941,1 +2951,1 @@\n-                      size_of_deoptimized_frame_offset_in_bytes()));\n+                      size_of_deoptimized_frame_offset()));\n@@ -2954,1 +2964,1 @@\n-                      total_frame_sizes_offset_in_bytes()));\n+                      total_frame_sizes_offset()));\n@@ -2960,1 +2970,1 @@\n-                     Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+                     Deoptimization::UnrollBlock::frame_pcs_offset()));\n@@ -2965,1 +2975,1 @@\n-                     frame_sizes_offset_in_bytes()));\n+                     frame_sizes_offset()));\n@@ -2970,1 +2980,1 @@\n-                      number_of_frames_offset_in_bytes())); \/\/ (int)\n+                      number_of_frames_offset())); \/\/ (int)\n@@ -2982,1 +2992,1 @@\n-                      caller_adjustment_offset_in_bytes())); \/\/ (int)\n+                      caller_adjustment_offset())); \/\/ (int)\n@@ -3063,1 +3073,1 @@\n-  address call_pc = NULL;\n+  address call_pc = nullptr;\n@@ -3179,1 +3189,1 @@\n-  assert (StubRoutines::forward_exception_entry() != NULL, \"must be generated before\");\n+  assert (StubRoutines::forward_exception_entry() != nullptr, \"must be generated before\");\n@@ -3191,1 +3201,1 @@\n-  OopMap* map = NULL;\n+  OopMap* map = nullptr;\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":48,"deletions":38,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -459,1 +459,1 @@\n-    assert(StubRoutines::_call_stub_return_address != NULL,\n+    assert(StubRoutines::_call_stub_return_address != nullptr,\n@@ -587,1 +587,1 @@\n-    __ cbz(r0, exit); \/\/ if obj is NULL it is OK\n+    __ cbz(r0, exit); \/\/ if obj is null it is OK\n@@ -1501,1 +1501,1 @@\n-    if (entry != NULL) {\n+    if (entry != nullptr) {\n@@ -1567,1 +1567,1 @@\n-    if (entry != NULL) {\n+    if (entry != nullptr) {\n@@ -1845,1 +1845,1 @@\n-    __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n+    __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, nullptr,\n@@ -1847,1 +1847,1 @@\n-    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n+    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, nullptr);\n@@ -1918,1 +1918,1 @@\n-    if (entry != NULL) {\n+    if (entry != nullptr) {\n@@ -2155,1 +2155,1 @@\n-    \/\/ (5) src klass and dst klass should be the same and not NULL.\n+    \/\/ (5) src klass and dst klass should be the same and not null.\n@@ -2161,1 +2161,1 @@\n-    \/\/  if (src == NULL) return -1;\n+    \/\/  if (src == nullptr) return -1;\n@@ -2167,1 +2167,1 @@\n-    \/\/  if (dst == NULL) return -1;\n+    \/\/  if (dst == nullptr) return -1;\n@@ -2184,1 +2184,1 @@\n-    \/\/  assert(src->klass() != NULL);\n+    \/\/  assert(src->klass() != nullptr);\n@@ -2188,1 +2188,1 @@\n-      __ cbnz(scratch_src_klass, L2);   \/\/ it is broken if klass is NULL\n+      __ cbnz(scratch_src_klass, L2);   \/\/ it is broken if klass is null\n@@ -2604,1 +2604,1 @@\n-    StubRoutines::_arrayof_jbyte_arraycopy          = generate_conjoint_byte_copy(true, entry, NULL,\n+    StubRoutines::_arrayof_jbyte_arraycopy          = generate_conjoint_byte_copy(true, entry, nullptr,\n@@ -2616,1 +2616,1 @@\n-    StubRoutines::_arrayof_jshort_arraycopy          = generate_conjoint_short_copy(true, entry, NULL,\n+    StubRoutines::_arrayof_jshort_arraycopy          = generate_conjoint_short_copy(true, entry, nullptr,\n@@ -2659,1 +2659,1 @@\n-        = generate_conjoint_oop_copy(aligned, entry, NULL, \"arrayof_oop_arraycopy_uninit\",\n+        = generate_conjoint_oop_copy(aligned, entry, nullptr, \"arrayof_oop_arraycopy_uninit\",\n@@ -2669,1 +2669,1 @@\n-    StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n+    StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", nullptr,\n@@ -3361,0 +3361,27 @@\n+  class Cached64Bytes {\n+  private:\n+    MacroAssembler *_masm;\n+    Register _regs[8];\n+\n+  public:\n+    Cached64Bytes(MacroAssembler *masm, RegSet rs): _masm(masm) {\n+      assert(rs.size() == 8, \"%u registers are used to cache 16 4-byte data\", rs.size());\n+      auto it = rs.begin();\n+      for (auto &r: _regs) {\n+        r = *it;\n+        ++it;\n+      }\n+    }\n+\n+    void gen_loads(Register base) {\n+      for (int i = 0; i < 8; i += 2) {\n+        __ ldp(_regs[i], _regs[i + 1], Address(base, 8 * i));\n+      }\n+    }\n+\n+    \/\/ Generate code extracting i-th unsigned word (4 bytes) from cached 64 bytes.\n+    void extract_u32(Register dest, int i) {\n+      __ ubfx(dest, _regs[i \/ 2], 32 * (i % 2), 32);\n+    }\n+  };\n+\n@@ -3363,1 +3390,1 @@\n-  void md5_FF(Register buf, Register r1, Register r2, Register r3, Register r4,\n+  void md5_FF(Cached64Bytes& reg_cache, Register r1, Register r2, Register r3, Register r4,\n@@ -3372,1 +3399,1 @@\n-    __ ldrw(rscratch1, Address(buf, k*4));\n+    reg_cache.extract_u32(rscratch1, k);\n@@ -3380,1 +3407,1 @@\n-  void md5_GG(Register buf, Register r1, Register r2, Register r3, Register r4,\n+  void md5_GG(Cached64Bytes& reg_cache, Register r1, Register r2, Register r3, Register r4,\n@@ -3387,1 +3414,1 @@\n-    __ ldrw(rscratch1, Address(buf, k*4));\n+    reg_cache.extract_u32(rscratch1, k);\n@@ -3397,1 +3424,1 @@\n-  void md5_HH(Register buf, Register r1, Register r2, Register r3, Register r4,\n+  void md5_HH(Cached64Bytes& reg_cache, Register r1, Register r2, Register r3, Register r4,\n@@ -3405,1 +3432,1 @@\n-    __ ldrw(rscratch1, Address(buf, k*4));\n+    reg_cache.extract_u32(rscratch1, k);\n@@ -3413,1 +3440,1 @@\n-  void md5_II(Register buf, Register r1, Register r2, Register r3, Register r4,\n+  void md5_II(Cached64Bytes& reg_cache, Register r1, Register r2, Register r3, Register r4,\n@@ -3421,1 +3448,1 @@\n-    __ ldrw(rscratch1, Address(buf, k*4));\n+    reg_cache.extract_u32(rscratch1, k);\n@@ -3453,0 +3480,12 @@\n+    Register state_regs[2] = { r12, r13 };\n+    RegSet saved_regs = RegSet::range(r16, r22) - r18_tls;\n+    Cached64Bytes reg_cache(_masm, RegSet::of(r14, r15) + saved_regs);  \/\/ using 8 registers\n+\n+    __ push(saved_regs, sp);\n+\n+    __ ldp(state_regs[0], state_regs[1], Address(state));\n+    __ ubfx(a, state_regs[0],  0, 32);\n+    __ ubfx(b, state_regs[0], 32, 32);\n+    __ ubfx(c, state_regs[1],  0, 32);\n+    __ ubfx(d, state_regs[1], 32, 32);\n+\n@@ -3456,5 +3495,1 @@\n-    \/\/ Save hash values for addition after rounds\n-    __ ldrw(a, Address(state,  0));\n-    __ ldrw(b, Address(state,  4));\n-    __ ldrw(c, Address(state,  8));\n-    __ ldrw(d, Address(state, 12));\n+    reg_cache.gen_loads(buf);\n@@ -3463,16 +3498,16 @@\n-    md5_FF(buf, a, b, c, d,  0,  7, 0xd76aa478);\n-    md5_FF(buf, d, a, b, c,  1, 12, 0xe8c7b756);\n-    md5_FF(buf, c, d, a, b,  2, 17, 0x242070db);\n-    md5_FF(buf, b, c, d, a,  3, 22, 0xc1bdceee);\n-    md5_FF(buf, a, b, c, d,  4,  7, 0xf57c0faf);\n-    md5_FF(buf, d, a, b, c,  5, 12, 0x4787c62a);\n-    md5_FF(buf, c, d, a, b,  6, 17, 0xa8304613);\n-    md5_FF(buf, b, c, d, a,  7, 22, 0xfd469501);\n-    md5_FF(buf, a, b, c, d,  8,  7, 0x698098d8);\n-    md5_FF(buf, d, a, b, c,  9, 12, 0x8b44f7af);\n-    md5_FF(buf, c, d, a, b, 10, 17, 0xffff5bb1);\n-    md5_FF(buf, b, c, d, a, 11, 22, 0x895cd7be);\n-    md5_FF(buf, a, b, c, d, 12,  7, 0x6b901122);\n-    md5_FF(buf, d, a, b, c, 13, 12, 0xfd987193);\n-    md5_FF(buf, c, d, a, b, 14, 17, 0xa679438e);\n-    md5_FF(buf, b, c, d, a, 15, 22, 0x49b40821);\n+    md5_FF(reg_cache, a, b, c, d,  0,  7, 0xd76aa478);\n+    md5_FF(reg_cache, d, a, b, c,  1, 12, 0xe8c7b756);\n+    md5_FF(reg_cache, c, d, a, b,  2, 17, 0x242070db);\n+    md5_FF(reg_cache, b, c, d, a,  3, 22, 0xc1bdceee);\n+    md5_FF(reg_cache, a, b, c, d,  4,  7, 0xf57c0faf);\n+    md5_FF(reg_cache, d, a, b, c,  5, 12, 0x4787c62a);\n+    md5_FF(reg_cache, c, d, a, b,  6, 17, 0xa8304613);\n+    md5_FF(reg_cache, b, c, d, a,  7, 22, 0xfd469501);\n+    md5_FF(reg_cache, a, b, c, d,  8,  7, 0x698098d8);\n+    md5_FF(reg_cache, d, a, b, c,  9, 12, 0x8b44f7af);\n+    md5_FF(reg_cache, c, d, a, b, 10, 17, 0xffff5bb1);\n+    md5_FF(reg_cache, b, c, d, a, 11, 22, 0x895cd7be);\n+    md5_FF(reg_cache, a, b, c, d, 12,  7, 0x6b901122);\n+    md5_FF(reg_cache, d, a, b, c, 13, 12, 0xfd987193);\n+    md5_FF(reg_cache, c, d, a, b, 14, 17, 0xa679438e);\n+    md5_FF(reg_cache, b, c, d, a, 15, 22, 0x49b40821);\n@@ -3481,16 +3516,16 @@\n-    md5_GG(buf, a, b, c, d,  1,  5, 0xf61e2562);\n-    md5_GG(buf, d, a, b, c,  6,  9, 0xc040b340);\n-    md5_GG(buf, c, d, a, b, 11, 14, 0x265e5a51);\n-    md5_GG(buf, b, c, d, a,  0, 20, 0xe9b6c7aa);\n-    md5_GG(buf, a, b, c, d,  5,  5, 0xd62f105d);\n-    md5_GG(buf, d, a, b, c, 10,  9, 0x02441453);\n-    md5_GG(buf, c, d, a, b, 15, 14, 0xd8a1e681);\n-    md5_GG(buf, b, c, d, a,  4, 20, 0xe7d3fbc8);\n-    md5_GG(buf, a, b, c, d,  9,  5, 0x21e1cde6);\n-    md5_GG(buf, d, a, b, c, 14,  9, 0xc33707d6);\n-    md5_GG(buf, c, d, a, b,  3, 14, 0xf4d50d87);\n-    md5_GG(buf, b, c, d, a,  8, 20, 0x455a14ed);\n-    md5_GG(buf, a, b, c, d, 13,  5, 0xa9e3e905);\n-    md5_GG(buf, d, a, b, c,  2,  9, 0xfcefa3f8);\n-    md5_GG(buf, c, d, a, b,  7, 14, 0x676f02d9);\n-    md5_GG(buf, b, c, d, a, 12, 20, 0x8d2a4c8a);\n+    md5_GG(reg_cache, a, b, c, d,  1,  5, 0xf61e2562);\n+    md5_GG(reg_cache, d, a, b, c,  6,  9, 0xc040b340);\n+    md5_GG(reg_cache, c, d, a, b, 11, 14, 0x265e5a51);\n+    md5_GG(reg_cache, b, c, d, a,  0, 20, 0xe9b6c7aa);\n+    md5_GG(reg_cache, a, b, c, d,  5,  5, 0xd62f105d);\n+    md5_GG(reg_cache, d, a, b, c, 10,  9, 0x02441453);\n+    md5_GG(reg_cache, c, d, a, b, 15, 14, 0xd8a1e681);\n+    md5_GG(reg_cache, b, c, d, a,  4, 20, 0xe7d3fbc8);\n+    md5_GG(reg_cache, a, b, c, d,  9,  5, 0x21e1cde6);\n+    md5_GG(reg_cache, d, a, b, c, 14,  9, 0xc33707d6);\n+    md5_GG(reg_cache, c, d, a, b,  3, 14, 0xf4d50d87);\n+    md5_GG(reg_cache, b, c, d, a,  8, 20, 0x455a14ed);\n+    md5_GG(reg_cache, a, b, c, d, 13,  5, 0xa9e3e905);\n+    md5_GG(reg_cache, d, a, b, c,  2,  9, 0xfcefa3f8);\n+    md5_GG(reg_cache, c, d, a, b,  7, 14, 0x676f02d9);\n+    md5_GG(reg_cache, b, c, d, a, 12, 20, 0x8d2a4c8a);\n@@ -3499,16 +3534,16 @@\n-    md5_HH(buf, a, b, c, d,  5,  4, 0xfffa3942);\n-    md5_HH(buf, d, a, b, c,  8, 11, 0x8771f681);\n-    md5_HH(buf, c, d, a, b, 11, 16, 0x6d9d6122);\n-    md5_HH(buf, b, c, d, a, 14, 23, 0xfde5380c);\n-    md5_HH(buf, a, b, c, d,  1,  4, 0xa4beea44);\n-    md5_HH(buf, d, a, b, c,  4, 11, 0x4bdecfa9);\n-    md5_HH(buf, c, d, a, b,  7, 16, 0xf6bb4b60);\n-    md5_HH(buf, b, c, d, a, 10, 23, 0xbebfbc70);\n-    md5_HH(buf, a, b, c, d, 13,  4, 0x289b7ec6);\n-    md5_HH(buf, d, a, b, c,  0, 11, 0xeaa127fa);\n-    md5_HH(buf, c, d, a, b,  3, 16, 0xd4ef3085);\n-    md5_HH(buf, b, c, d, a,  6, 23, 0x04881d05);\n-    md5_HH(buf, a, b, c, d,  9,  4, 0xd9d4d039);\n-    md5_HH(buf, d, a, b, c, 12, 11, 0xe6db99e5);\n-    md5_HH(buf, c, d, a, b, 15, 16, 0x1fa27cf8);\n-    md5_HH(buf, b, c, d, a,  2, 23, 0xc4ac5665);\n+    md5_HH(reg_cache, a, b, c, d,  5,  4, 0xfffa3942);\n+    md5_HH(reg_cache, d, a, b, c,  8, 11, 0x8771f681);\n+    md5_HH(reg_cache, c, d, a, b, 11, 16, 0x6d9d6122);\n+    md5_HH(reg_cache, b, c, d, a, 14, 23, 0xfde5380c);\n+    md5_HH(reg_cache, a, b, c, d,  1,  4, 0xa4beea44);\n+    md5_HH(reg_cache, d, a, b, c,  4, 11, 0x4bdecfa9);\n+    md5_HH(reg_cache, c, d, a, b,  7, 16, 0xf6bb4b60);\n+    md5_HH(reg_cache, b, c, d, a, 10, 23, 0xbebfbc70);\n+    md5_HH(reg_cache, a, b, c, d, 13,  4, 0x289b7ec6);\n+    md5_HH(reg_cache, d, a, b, c,  0, 11, 0xeaa127fa);\n+    md5_HH(reg_cache, c, d, a, b,  3, 16, 0xd4ef3085);\n+    md5_HH(reg_cache, b, c, d, a,  6, 23, 0x04881d05);\n+    md5_HH(reg_cache, a, b, c, d,  9,  4, 0xd9d4d039);\n+    md5_HH(reg_cache, d, a, b, c, 12, 11, 0xe6db99e5);\n+    md5_HH(reg_cache, c, d, a, b, 15, 16, 0x1fa27cf8);\n+    md5_HH(reg_cache, b, c, d, a,  2, 23, 0xc4ac5665);\n@@ -3517,33 +3552,26 @@\n-    md5_II(buf, a, b, c, d,  0,  6, 0xf4292244);\n-    md5_II(buf, d, a, b, c,  7, 10, 0x432aff97);\n-    md5_II(buf, c, d, a, b, 14, 15, 0xab9423a7);\n-    md5_II(buf, b, c, d, a,  5, 21, 0xfc93a039);\n-    md5_II(buf, a, b, c, d, 12,  6, 0x655b59c3);\n-    md5_II(buf, d, a, b, c,  3, 10, 0x8f0ccc92);\n-    md5_II(buf, c, d, a, b, 10, 15, 0xffeff47d);\n-    md5_II(buf, b, c, d, a,  1, 21, 0x85845dd1);\n-    md5_II(buf, a, b, c, d,  8,  6, 0x6fa87e4f);\n-    md5_II(buf, d, a, b, c, 15, 10, 0xfe2ce6e0);\n-    md5_II(buf, c, d, a, b,  6, 15, 0xa3014314);\n-    md5_II(buf, b, c, d, a, 13, 21, 0x4e0811a1);\n-    md5_II(buf, a, b, c, d,  4,  6, 0xf7537e82);\n-    md5_II(buf, d, a, b, c, 11, 10, 0xbd3af235);\n-    md5_II(buf, c, d, a, b,  2, 15, 0x2ad7d2bb);\n-    md5_II(buf, b, c, d, a,  9, 21, 0xeb86d391);\n-\n-    \/\/ write hash values back in the correct order\n-    __ ldrw(rscratch1, Address(state,  0));\n-    __ addw(rscratch1, rscratch1, a);\n-    __ strw(rscratch1, Address(state,  0));\n-\n-    __ ldrw(rscratch2, Address(state,  4));\n-    __ addw(rscratch2, rscratch2, b);\n-    __ strw(rscratch2, Address(state,  4));\n-\n-    __ ldrw(rscratch3, Address(state,  8));\n-    __ addw(rscratch3, rscratch3, c);\n-    __ strw(rscratch3, Address(state,  8));\n-\n-    __ ldrw(rscratch4, Address(state, 12));\n-    __ addw(rscratch4, rscratch4, d);\n-    __ strw(rscratch4, Address(state, 12));\n+    md5_II(reg_cache, a, b, c, d,  0,  6, 0xf4292244);\n+    md5_II(reg_cache, d, a, b, c,  7, 10, 0x432aff97);\n+    md5_II(reg_cache, c, d, a, b, 14, 15, 0xab9423a7);\n+    md5_II(reg_cache, b, c, d, a,  5, 21, 0xfc93a039);\n+    md5_II(reg_cache, a, b, c, d, 12,  6, 0x655b59c3);\n+    md5_II(reg_cache, d, a, b, c,  3, 10, 0x8f0ccc92);\n+    md5_II(reg_cache, c, d, a, b, 10, 15, 0xffeff47d);\n+    md5_II(reg_cache, b, c, d, a,  1, 21, 0x85845dd1);\n+    md5_II(reg_cache, a, b, c, d,  8,  6, 0x6fa87e4f);\n+    md5_II(reg_cache, d, a, b, c, 15, 10, 0xfe2ce6e0);\n+    md5_II(reg_cache, c, d, a, b,  6, 15, 0xa3014314);\n+    md5_II(reg_cache, b, c, d, a, 13, 21, 0x4e0811a1);\n+    md5_II(reg_cache, a, b, c, d,  4,  6, 0xf7537e82);\n+    md5_II(reg_cache, d, a, b, c, 11, 10, 0xbd3af235);\n+    md5_II(reg_cache, c, d, a, b,  2, 15, 0x2ad7d2bb);\n+    md5_II(reg_cache, b, c, d, a,  9, 21, 0xeb86d391);\n+\n+    __ addw(a, state_regs[0], a);\n+    __ ubfx(rscratch2, state_regs[0], 32, 32);\n+    __ addw(b, rscratch2, b);\n+    __ addw(c, state_regs[1], c);\n+    __ ubfx(rscratch4, state_regs[1], 32, 32);\n+    __ addw(d, rscratch4, d);\n+\n+    __ orr(state_regs[0], a, b, Assembler::LSL, 32);\n+    __ orr(state_regs[1], c, d, Assembler::LSL, 32);\n@@ -3559,0 +3587,5 @@\n+    \/\/ write hash values back in the correct order\n+    __ stp(state_regs[0], state_regs[1], Address(state));\n+\n+    __ pop(saved_regs, sp);\n+\n@@ -5180,0 +5213,1 @@\n+  \/\/ Clobbers: v0-v7 when UseSIMDForArrayEquals, rscratch1, rscratch2\n@@ -5763,0 +5797,2 @@\n+  \/\/ Clobbers: rscratch1, rscratch2, v0, v1, rflags\n+  \/\/\n@@ -6077,0 +6113,1 @@\n+  \/\/ Clobbers: r0, r1, r3, rscratch1, rflags, v0-v6\n@@ -8254,1 +8291,1 @@\n-    if (bs_nm != NULL) {\n+    if (bs_nm != nullptr) {\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":148,"deletions":111,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -103,1 +103,1 @@\n-  \/\/        bcp (NULL)\n+  \/\/        bcp (null)\n@@ -166,1 +166,1 @@\n-  address entry_point = NULL;\n+  address entry_point = nullptr;\n@@ -246,1 +246,1 @@\n-    if (StubRoutines::dsin() == NULL) {\n+    if (StubRoutines::dsin() == nullptr) {\n@@ -253,1 +253,1 @@\n-    if (StubRoutines::dcos() == NULL) {\n+    if (StubRoutines::dcos() == nullptr) {\n@@ -260,1 +260,1 @@\n-    if (StubRoutines::dtan() == NULL) {\n+    if (StubRoutines::dtan() == nullptr) {\n@@ -267,1 +267,1 @@\n-    if (StubRoutines::dlog() == NULL) {\n+    if (StubRoutines::dlog() == nullptr) {\n@@ -274,1 +274,1 @@\n-    if (StubRoutines::dlog10() == NULL) {\n+    if (StubRoutines::dlog10() == nullptr) {\n@@ -281,1 +281,1 @@\n-    if (StubRoutines::dexp() == NULL) {\n+    if (StubRoutines::dexp() == nullptr) {\n@@ -288,1 +288,1 @@\n-    if (StubRoutines::dpow() == NULL) {\n+    if (StubRoutines::dpow() == nullptr) {\n@@ -296,1 +296,1 @@\n-    fn = NULL;  \/\/ unreachable\n+    fn = nullptr;  \/\/ unreachable\n@@ -346,1 +346,1 @@\n-  \/\/  pop return address, reset last_sp to NULL\n+  \/\/  pop return address, reset last_sp to null\n@@ -431,1 +431,1 @@\n-  assert(!pass_oop || message == NULL, \"either oop or message but not both\");\n+  assert(!pass_oop || message == nullptr, \"either oop or message but not both\");\n@@ -448,1 +448,1 @@\n-    \/\/ kind of lame ExternalAddress can't take NULL because\n+    \/\/ kind of lame ExternalAddress can't take null because\n@@ -450,1 +450,1 @@\n-    if (message != NULL) {\n+    if (message != nullptr) {\n@@ -469,1 +469,1 @@\n-  \/\/ and NULL it as marker that esp is now tos until next java call\n+  \/\/ and null it as marker that esp is now tos until next java call\n@@ -530,1 +530,1 @@\n-  \/\/ NULL last_sp until next java call\n+  \/\/ null last_sp until next java call\n@@ -569,1 +569,1 @@\n-  if (continuation == NULL) {\n+  if (continuation == nullptr) {\n@@ -663,2 +663,2 @@\n-  \/\/ (NULL bcp).  We pass zero for it.  The call returns the address\n-  \/\/ of the verified entry point for the method or NULL if the\n+  \/\/ (null bcp).  We pass zero for it.  The call returns the address\n+  \/\/ of the verified entry point for the method or null if the\n@@ -756,1 +756,1 @@\n-  assert(StubRoutines::throw_StackOverflowError_entry() != NULL, \"stub not yet generated\");\n+  assert(StubRoutines::throw_StackOverflowError_entry() != nullptr, \"stub not yet generated\");\n@@ -806,1 +806,1 @@\n-      __ stop(\"synchronization object is NULL\");\n+      __ stop(\"synchronization object is null\");\n@@ -822,1 +822,1 @@\n-  __ str(r0, Address(esp, BasicObjectLock::obj_offset_in_bytes()));\n+  __ str(r0, Address(esp, BasicObjectLock::obj_offset()));\n@@ -868,1 +868,1 @@\n-  __ ldr(rcpool, Address(rcpool, ConstantPool::cache_offset_in_bytes()));\n+  __ ldr(rcpool, Address(rcpool, ConstantPool::cache_offset()));\n@@ -894,1 +894,1 @@\n-    __ stp(zr, rscratch1, Address(sp, 4 * wordSize));\n+    __ stp(r10, rscratch1, Address(sp, 4 * wordSize));\n@@ -944,1 +944,1 @@\n-  \/\/ Check if local 0 != NULL\n+  \/\/ Check if local 0 != null\n@@ -1444,1 +1444,1 @@\n-  __ str(zr, Address(t, JNIHandleBlock::top_offset_in_bytes()));\n+  __ str(zr, Address(t, JNIHandleBlock::top_offset()));\n@@ -1522,1 +1522,1 @@\n-      __ ldr(t, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+      __ ldr(t, Address(c_rarg1, BasicObjectLock::obj_offset()));\n@@ -1904,1 +1904,1 @@\n-    \/\/ Detect such a case in the InterpreterRuntime function and return the member name argument, or NULL.\n+    \/\/ Detect such a case in the InterpreterRuntime function and return the member name argument, or null.\n@@ -2071,1 +2071,1 @@\n-  assert(Interpreter::trace_code(t->tos_in()) != NULL,\n+  assert(Interpreter::trace_code(t->tos_in()) != nullptr,\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":29,"deletions":29,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -142,2 +142,2 @@\n-\/\/ Store an oop (or NULL) at the Address described by obj.\n-\/\/ If val == noreg this means store a NULL\n+\/\/ Store an oop (or null) at the Address described by obj.\n+\/\/ If val == noreg this means store a null\n@@ -419,1 +419,1 @@\n-    __ mov(result, 0);  \/\/ NULL object reference\n+    __ mov(result, 0);  \/\/ null object reference\n@@ -1130,1 +1130,1 @@\n-  \/\/ do array store check - check for NULL value first\n+  \/\/ do array store check - check for null value first\n@@ -1167,1 +1167,1 @@\n-  \/\/ Have a NULL in r0, r3=array, r2=index.  Store NULL at ary[idx]\n+  \/\/ Have a null in r0, r3=array, r2=index.  Store null at ary[idx]\n@@ -1182,1 +1182,1 @@\n-  \/\/ Store a NULL\n+  \/\/ Store a null\n@@ -1946,1 +1946,1 @@\n-    \/\/ r0: osr nmethod (osr ok) or NULL (osr not possible)\n+    \/\/ r0: osr nmethod (osr ok) or null (osr not possible)\n@@ -2404,1 +2404,1 @@\n-    __ clinit_barrier(temp, rscratch1, NULL, &clinit_barrier_slow);\n+    __ clinit_barrier(temp, rscratch1, nullptr, &clinit_barrier_slow);\n@@ -2563,1 +2563,1 @@\n-      __ mov(c_rarg1, zr); \/\/ NULL object reference\n+      __ mov(c_rarg1, zr); \/\/ null object reference\n@@ -2568,1 +2568,1 @@\n-    \/\/ c_rarg1: object pointer or NULL\n+    \/\/ c_rarg1: object pointer or null\n@@ -2887,1 +2887,1 @@\n-    \/\/ c_rarg1: object pointer set up above (NULL if static)\n+    \/\/ c_rarg1: object pointer set up above (null if static)\n@@ -3966,1 +3966,1 @@\n-  \/\/ Collect counts on whether this test sees NULLs a lot or not.\n+  \/\/ Collect counts on whether this test sees nulls a lot or not.\n@@ -4032,1 +4032,1 @@\n-  \/\/ Collect counts on whether this test sees NULLs a lot or not.\n+  \/\/ Collect counts on whether this test sees nulls a lot or not.\n@@ -4041,2 +4041,2 @@\n-  \/\/ r0 = 0: obj == NULL or  obj is not an instanceof the specified klass\n-  \/\/ r0 = 1: obj != NULL and obj is     an instanceof the specified klass\n+  \/\/ r0 = 0: obj == nullptr or  obj is not an instanceof the specified klass\n+  \/\/ r0 = 1: obj != nullptr and obj is     an instanceof the specified klass\n@@ -4102,1 +4102,1 @@\n-  \/\/ check for NULL object\n+  \/\/ check for null object\n@@ -4118,1 +4118,1 @@\n-  __ mov(c_rarg1, zr); \/\/ points to free slot or NULL\n+  __ mov(c_rarg1, zr); \/\/ points to free slot or null\n@@ -4132,1 +4132,1 @@\n-    __ ldr(rscratch1, Address(c_rarg3, BasicObjectLock::obj_offset_in_bytes()));\n+    __ ldr(rscratch1, Address(c_rarg3, BasicObjectLock::obj_offset()));\n@@ -4192,1 +4192,1 @@\n-  __ str(r0, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+  __ str(r0, Address(c_rarg1, BasicObjectLock::obj_offset()));\n@@ -4214,1 +4214,1 @@\n-  \/\/ check for NULL object\n+  \/\/ check for null object\n@@ -4248,1 +4248,1 @@\n-    __ ldr(rscratch1, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+    __ ldr(rscratch1, Address(c_rarg1, BasicObjectLock::obj_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":21,"deletions":21,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,3 +54,3 @@\n-  \/\/ Can be NULL if there is no free space in the code cache.\n-  if (s == NULL) {\n-    return NULL;\n+  \/\/ Can be null if there is no free space in the code cache.\n+  if (s == nullptr) {\n+    return nullptr;\n@@ -125,1 +125,1 @@\n-    __ stop(\"Vtable entry is NULL\");\n+    __ stop(\"Vtable entry is null\");\n@@ -149,3 +149,3 @@\n-  \/\/ Can be NULL if there is no free space in the code cache.\n-  if (s == NULL) {\n-    return NULL;\n+  \/\/ Can be null if there is no free space in the code cache.\n+  if (s == nullptr) {\n+    return nullptr;\n@@ -253,1 +253,1 @@\n-  assert(SharedRuntime::get_handle_wrong_method_stub() != NULL, \"check initialization order\");\n+  assert(SharedRuntime::get_handle_wrong_method_stub() != nullptr, \"check initialization order\");\n","filename":"src\/hotspot\/cpu\/aarch64\/vtableStubs_aarch64.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2434,1 +2434,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2689,1 +2689,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode != LM_MONITOR) {\n@@ -2715,1 +2715,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode != LM_MONITOR) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -508,2 +508,2 @@\n-  ld(result, ConstantPool::cache_offset_in_bytes(), result);\n-  ld(result, ConstantPoolCache::resolved_references_offset_in_bytes(), result);\n+  ld(result, ConstantPool::cache_offset(), result);\n+  ld(result, ConstantPoolCache::resolved_references_offset(), result);\n@@ -539,1 +539,1 @@\n-  ld(Rklass, ConstantPool::resolved_klasses_offset_in_bytes(), Rcpool); \/\/ Rklass = Rcpool->_resolved_klasses\n+  ld(Rklass, ConstantPool::resolved_klasses_offset(), Rcpool); \/\/ Rklass = Rcpool->_resolved_klasses\n@@ -649,1 +649,1 @@\n-  ld(Rdst, ConstantPool::cache_offset_in_bytes(), Rdst);\n+  ld(Rdst, ConstantPool::cache_offset(), Rdst);\n@@ -654,1 +654,1 @@\n-  ld(Rtags, ConstantPool::tags_offset_in_bytes(), Rcpool);\n+  ld(Rtags, ConstantPool::tags_offset(), Rcpool);\n@@ -704,1 +704,1 @@\n-    ld(R0, BasicObjectLock::obj_offset_in_bytes(), Rmonitor_base);\n+    ld(R0, BasicObjectLock::obj_offset(), Rmonitor_base);\n@@ -743,1 +743,1 @@\n-           BasicObjectLock::obj_offset_in_bytes() - frame::interpreter_frame_monitor_size_in_bytes());\n+           in_bytes(BasicObjectLock::obj_offset()) - frame::interpreter_frame_monitor_size_in_bytes());\n@@ -778,1 +778,1 @@\n-      addi(Rmonitor_addr, Rcurrent_obj_addr, -BasicObjectLock::obj_offset_in_bytes() + delta);\n+      addi(Rmonitor_addr, Rcurrent_obj_addr, -in_bytes(BasicObjectLock::obj_offset()) + delta);\n@@ -924,1 +924,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -967,0 +967,3 @@\n+    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n+    const int mark_offset = lock_offset +\n+                            BasicLock::displaced_header_offset_in_bytes();\n@@ -969,2 +972,1 @@\n-    std(displaced_header, BasicObjectLock::lock_offset_in_bytes() +\n-        BasicLock::displaced_header_offset_in_bytes(), monitor);\n+    std(displaced_header, mark_offset, monitor);\n@@ -1011,2 +1013,1 @@\n-    std(R0\/*==0!*\/, BasicObjectLock::lock_offset_in_bytes() +\n-        BasicLock::displaced_header_offset_in_bytes(), monitor);\n+    std(R0\/*==0!*\/, mark_offset, monitor);\n@@ -1040,1 +1041,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1068,2 +1069,2 @@\n-    ld(displaced_header, BasicObjectLock::lock_offset_in_bytes() +\n-           BasicLock::displaced_header_offset_in_bytes(), monitor);\n+    ld(displaced_header, in_bytes(BasicObjectLock::lock_offset()) +\n+                         BasicLock::displaced_header_offset_in_bytes(), monitor);\n@@ -1082,1 +1083,1 @@\n-    ld(object, BasicObjectLock::obj_offset_in_bytes(), monitor);\n+    ld(object, in_bytes(BasicObjectLock::obj_offset()), monitor);\n@@ -1116,1 +1117,1 @@\n-    std(R0, BasicObjectLock::obj_offset_in_bytes(), monitor);\n+    std(R0, in_bytes(BasicObjectLock::obj_offset()), monitor);\n@@ -1864,1 +1865,1 @@\n-      lbz(tmp2, Method::intrinsic_id_offset_in_bytes(), R19_method);\n+      lbz(tmp2, in_bytes(Method::intrinsic_id_offset()), R19_method);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":20,"deletions":19,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -859,1 +859,1 @@\n-  return 0; \/\/ unused\n+  return Address(); \/\/ unused\n@@ -865,1 +865,1 @@\n-  return 0; \/\/ unused\n+  return Address(); \/\/ unused\n@@ -2725,1 +2725,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -394,2 +394,2 @@\n-  z_lg(result, ConstantPool::cache_offset_in_bytes(), result);\n-  z_lg(result, ConstantPoolCache::resolved_references_offset_in_bytes(), result);\n+  z_lg(result, in_bytes(ConstantPool::cache_offset()), result);\n+  z_lg(result, in_bytes(ConstantPoolCache::resolved_references_offset()), result);\n@@ -415,1 +415,1 @@\n-  z_lg(iklass, Address(cpool, ConstantPool::resolved_klasses_offset_in_bytes())); \/\/ iklass = cpool->_resolved_klasses\n+  z_lg(iklass, Address(cpool, ConstantPool::resolved_klasses_offset())); \/\/ iklass = cpool->_resolved_klasses\n@@ -757,1 +757,1 @@\n-  mem2reg_opt(Rdst, Address(Rdst, ConstantPool::cache_offset_in_bytes()));\n+  mem2reg_opt(Rdst, Address(Rdst, ConstantPool::cache_offset()));\n@@ -762,1 +762,1 @@\n-  mem2reg_opt(Rtags, Address(Rcpool, ConstantPool::tags_offset_in_bytes()));\n+  mem2reg_opt(Rtags, Address(Rcpool, ConstantPool::tags_offset()));\n@@ -813,1 +813,1 @@\n-  z_lg(Z_ARG3, Address(Z_ARG2, BasicObjectLock::obj_offset_in_bytes()));\n+  z_lg(Z_ARG3, Address(Z_ARG2, BasicObjectLock::obj_offset()));\n@@ -880,1 +880,1 @@\n-    load_and_test_long(Z_R0_scratch, Address(R_current_monitor, BasicObjectLock::obj_offset_in_bytes()));\n+    load_and_test_long(Z_R0_scratch, Address(R_current_monitor, BasicObjectLock::obj_offset()));\n@@ -985,1 +985,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1028,1 +1028,1 @@\n-  z_stg(displaced_header, BasicObjectLock::lock_offset_in_bytes() +\n+  z_stg(displaced_header, in_bytes(BasicObjectLock::lock_offset()) +\n@@ -1062,1 +1062,1 @@\n-  z_stg(Z_R0\/*==0!*\/, BasicObjectLock::lock_offset_in_bytes() +\n+  z_stg(Z_R0\/*==0!*\/, in_bytes(BasicObjectLock::lock_offset()) +\n@@ -1089,1 +1089,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1110,1 +1110,1 @@\n-  Address obj_entry(monitor, BasicObjectLock::obj_offset_in_bytes());\n+  Address obj_entry(monitor, BasicObjectLock::obj_offset());\n@@ -1131,1 +1131,1 @@\n-                                     Address(monitor, BasicObjectLock::lock_offset_in_bytes() +\n+                                     Address(monitor, in_bytes(BasicObjectLock::lock_offset()) +\n@@ -1813,1 +1813,1 @@\n-        z_cli(Method::intrinsic_id_offset_in_bytes(), tmp, static_cast<int>(vmIntrinsics::_compiledLambdaForm));\n+        z_cli(in_bytes(Method::intrinsic_id_offset()), tmp, static_cast<int>(vmIntrinsics::_compiledLambdaForm));\n@@ -1816,1 +1816,1 @@\n-        z_lh(tmp, Method::intrinsic_id_offset_in_bytes(), Z_R0, tmp);\n+        z_lh(tmp, in_bytes(Method::intrinsic_id_offset()), Z_R0, tmp);\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -459,1 +459,1 @@\n-    if (UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n@@ -1395,2 +1395,2 @@\n-    \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n-    if (!UseZGC) {\n+    if (!(UseZGC && !ZGenerational)) {\n+      \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n@@ -3726,1 +3726,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -3734,0 +3734,1 @@\n+    Register tmp = LockingMode == LM_LIGHTWEIGHT ? op->scratch_opr()->as_register() : noreg;\n@@ -3735,1 +3736,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op->stub()->entry());\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -332,0 +332,1 @@\n+    assert(LockingMode != LM_LIGHTWEIGHT, \"LM_LIGHTWEIGHT not yet compatible with EnableValhalla\");\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register tmp, Label& slow_case) {\n@@ -46,2 +46,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, tmp);\n@@ -53,1 +52,1 @@\n-  movptr(Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()), obj);\n+  movptr(Address(disp_hdr, BasicObjectLock::obj_offset()), obj);\n@@ -66,5 +65,49 @@\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  if (EnableValhalla) {\n-    \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-    andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    assert(!EnableValhalla, \"LM_LIGHTWEIGHT not yet compatible with EnableValhalla\");\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    fast_lock_impl(obj, hdr, thread, tmp, slow_case);\n+  } else  if (LockingMode == LM_LEGACY) {\n+    Label done;\n+    \/\/ and mark it as unlocked\n+    orptr(hdr, markWord::unlocked_value);\n+    if (EnableValhalla) {\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andptr(hdr, ~((int) markWord::inline_type_bit_in_place));\n+    }\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n@@ -72,31 +115,0 @@\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (null in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n@@ -116,8 +128,8 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is null we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n-  \/\/ load object\n-  movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+  }\n@@ -125,0 +137,2 @@\n+  \/\/ load object\n+  movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset()));\n@@ -126,10 +140,16 @@\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+  }\n+  bind(done);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":78,"deletions":58,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -260,1 +260,1 @@\n-    orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), NoRTM);\n+    orl(Address(tmpReg, MethodData::rtm_state_offset()), NoRTM);\n@@ -274,1 +274,1 @@\n-    orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), UseRTM);\n+    orl(Address(tmpReg, MethodData::rtm_state_offset()), UseRTM);\n@@ -576,1 +576,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -618,1 +618,1 @@\n-    assert(!UseHeavyMonitors, \"+UseHeavyMonitors and +UseRTMForStackLocks are mutually exclusive\");\n+    assert(LockingMode != LM_MONITOR, \"LockingMode == 0 (LM_MONITOR) and +UseRTMForStackLocks are mutually exclusive\");\n@@ -627,1 +627,1 @@\n-  jccb(Assembler::notZero, IsInflated);\n+  jcc(Assembler::notZero, IsInflated);\n@@ -629,1 +629,4 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n+    \/\/ Clear ZF so that we take the slow path at the DONE label. objReg is known to be not 0.\n+    testptr(objReg, objReg);\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -649,2 +652,3 @@\n-    \/\/ Clear ZF so that we take the slow path at the DONE label. objReg is known to be not 0.\n-    testptr(objReg, objReg);\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+    fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT);\n+    jmp(COUNT);\n@@ -713,1 +717,1 @@\n-  cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -720,1 +724,1 @@\n-  cmpptr(r15_thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n+  cmpptr(thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n@@ -736,6 +740,1 @@\n-#ifndef _LP64\n-  get_thread(tmpReg);\n-  incrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n-#else \/\/ _LP64\n-  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-#endif\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n@@ -793,1 +792,1 @@\n-    assert(!UseHeavyMonitors, \"+UseHeavyMonitors and +UseRTMForStackLocks are mutually exclusive\");\n+    assert(LockingMode != LM_MONITOR, \"LockingMode == 0 (LM_MONITOR) and +UseRTMForStackLocks are mutually exclusive\");\n@@ -805,1 +804,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -810,1 +809,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode != LM_MONITOR) {\n@@ -812,1 +811,1 @@\n-    jccb   (Assembler::zero, Stacked);\n+    jcc(Assembler::zero, Stacked);\n@@ -816,0 +815,18 @@\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n+    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n+#ifdef _LP64\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n+      Compile::current()->output()->add_stub(stub);\n+      jcc(Assembler::notEqual, stub->entry());\n+      bind(stub->continuation());\n+    } else\n+#endif\n+    {\n+      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n+      \/\/ Call the slow-path instead.\n+      jcc(Assembler::notEqual, NO_COUNT);\n+    }\n+  }\n+\n@@ -824,1 +841,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -936,1 +953,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode != LM_MONITOR) {\n@@ -938,3 +955,9 @@\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else if (LockingMode == LM_LEGACY) {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n@@ -4302,0 +4325,50 @@\n+#ifdef _LP64\n+\n+static void convertF2I_slowpath(C2_MacroAssembler& masm, C2GeneralStub<Register, XMMRegister, address>& stub) {\n+#define __ masm.\n+  Register dst = stub.data<0>();\n+  XMMRegister src = stub.data<1>();\n+  address target = stub.data<2>();\n+  __ bind(stub.entry());\n+  __ subptr(rsp, 8);\n+  __ movdbl(Address(rsp), src);\n+  __ call(RuntimeAddress(target));\n+  __ pop(dst);\n+  __ jmp(stub.continuation());\n+#undef __\n+}\n+\n+void C2_MacroAssembler::convertF2I(BasicType dst_bt, BasicType src_bt, Register dst, XMMRegister src) {\n+  assert(dst_bt == T_INT || dst_bt == T_LONG, \"\");\n+  assert(src_bt == T_FLOAT || src_bt == T_DOUBLE, \"\");\n+\n+  address slowpath_target;\n+  if (dst_bt == T_INT) {\n+    if (src_bt == T_FLOAT) {\n+      cvttss2sil(dst, src);\n+      cmpl(dst, 0x80000000);\n+      slowpath_target = StubRoutines::x86::f2i_fixup();\n+    } else {\n+      cvttsd2sil(dst, src);\n+      cmpl(dst, 0x80000000);\n+      slowpath_target = StubRoutines::x86::d2i_fixup();\n+    }\n+  } else {\n+    if (src_bt == T_FLOAT) {\n+      cvttss2siq(dst, src);\n+      cmp64(dst, ExternalAddress(StubRoutines::x86::double_sign_flip()));\n+      slowpath_target = StubRoutines::x86::f2l_fixup();\n+    } else {\n+      cvttsd2siq(dst, src);\n+      cmp64(dst, ExternalAddress(StubRoutines::x86::double_sign_flip()));\n+      slowpath_target = StubRoutines::x86::d2l_fixup();\n+    }\n+  }\n+\n+  auto stub = C2CodeStub::make<Register, XMMRegister, address>(dst, src, slowpath_target, 23, convertF2I_slowpath);\n+  jcc(Assembler::equal, stub->entry());\n+  bind(stub->continuation());\n+}\n+\n+#endif \/\/ _LP64\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":98,"deletions":25,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n@@ -308,0 +308,4 @@\n+#ifdef _LP64\n+  void convertF2I(BasicType dst_bt, BasicType src_bt, Register dst, XMMRegister src);\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -36,0 +37,3 @@\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmciRuntime.hpp\"\n+#endif\n@@ -61,1 +65,7 @@\n-  void verify() const;\n+  bool check_barrier(err_msg& msg) const;\n+  void verify() const {\n+#ifdef ASSERT\n+    err_msg msg(\"%s\", \"\");\n+    assert(check_barrier(msg), \"%s\", msg.buffer());\n+#endif\n+  }\n@@ -65,1 +75,2 @@\n-void NativeNMethodCmpBarrier::verify() const {\n+bool NativeNMethodCmpBarrier::check_barrier(err_msg& msg) const {\n+  \/\/ Only require 4 byte alignment\n@@ -67,1 +78,2 @@\n-    fatal(\"Not properly aligned\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" not properly aligned\", p2i(instruction_address()));\n+    return false;\n@@ -72,3 +84,2 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Prefix: 0x%x\", p2i(instruction_address()),\n-        prefix);\n-    fatal(\"not a cmp barrier\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x expected 0x%x\", p2i(instruction_address()), prefix, instruction_rex_prefix);\n+    return false;\n@@ -79,3 +90,2 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", p2i(instruction_address()),\n-        inst);\n-    fatal(\"not a cmp barrier\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x expected 0x%x\", p2i(instruction_address()), inst, instruction_code);\n+    return false;\n@@ -86,3 +96,2 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" mod\/rm: 0x%x\", p2i(instruction_address()),\n-        modrm);\n-    fatal(\"not a cmp barrier\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x expected mod\/rm 0x%x\", p2i(instruction_address()), modrm, instruction_modrm);\n+    return false;\n@@ -90,0 +99,1 @@\n+  return true;\n@@ -92,1 +102,1 @@\n-void NativeNMethodCmpBarrier::verify() const {\n+bool NativeNMethodCmpBarrier::check_barrier(err_msg& msg) const {\n@@ -94,1 +104,2 @@\n-    fatal(\"Not properly aligned\");\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" not properly aligned\", p2i(instruction_address()));\n+    return false;\n@@ -99,1 +110,1 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", p2i(instruction_address()),\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", p2i(instruction_address()),\n@@ -101,1 +112,1 @@\n-    fatal(\"not a cmp barrier\");\n+    return false;\n@@ -106,1 +117,1 @@\n-    tty->print_cr(\"Addr: \" INTPTR_FORMAT \" mod\/rm: 0x%x\", p2i(instruction_address()),\n+    msg.print(\"Addr: \" INTPTR_FORMAT \" mod\/rm: 0x%x\", p2i(instruction_address()),\n@@ -108,1 +119,1 @@\n-    fatal(\"not a cmp barrier\");\n+    return false;\n@@ -110,0 +121,1 @@\n+  return true;\n@@ -173,1 +185,10 @@\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n+  address barrier_address;\n+#if INCLUDE_JVMCI\n+  if (nm->is_compiled_by_jvmci()) {\n+    barrier_address = nm->code_begin() + nm->jvmci_nmethod_data()->nmethod_entry_patch_offset();\n+  } else\n+#endif\n+    {\n+      barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset(nm);\n+    }\n+\n@@ -175,1 +196,1 @@\n-  debug_only(barrier->verify());\n+  barrier->verify();\n@@ -220,0 +241,8 @@\n+\n+\n+#if INCLUDE_JVMCI\n+bool BarrierSetNMethod::verify_barrier(nmethod* nm, err_msg& msg) {\n+  NativeNMethodCmpBarrier* barrier = native_nmethod_barrier(nm);\n+  return barrier->check_barrier(msg);\n+}\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetNMethod_x86.cpp","additions":49,"deletions":20,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"compiler\/compileTask.hpp\"\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -34,0 +36,1 @@\n+#include \"runtime\/jniHandles.hpp\"\n@@ -42,0 +45,1 @@\n+#include \"c2_intelJccErratum_x86.hpp\"\n@@ -43,0 +47,1 @@\n+#include \"opto\/output.hpp\"\n@@ -54,0 +59,135 @@\n+ZBarrierSetAssembler::ZBarrierSetAssembler()\n+  : _load_bad_relocations(),\n+    _store_bad_relocations(),\n+    _store_good_relocations() {}\n+\n+enum class ZXMMSpillMode {\n+  none,\n+  avx128,\n+  avx256\n+};\n+\n+\/\/ Helper for saving and restoring registers across a runtime call that does\n+\/\/ not have any live vector registers.\n+class ZRuntimeCallSpill {\n+private:\n+  const ZXMMSpillMode _xmm_spill_mode;\n+  const int _xmm_size;\n+  const int _xmm_spill_size;\n+  MacroAssembler* _masm;\n+  Register _result;\n+\n+  void save() {\n+    MacroAssembler* masm = _masm;\n+    __ push(rax);\n+    __ push(rcx);\n+    __ push(rdx);\n+    __ push(rdi);\n+    __ push(rsi);\n+    __ push(r8);\n+    __ push(r9);\n+    __ push(r10);\n+    __ push(r11);\n+\n+    if (_xmm_spill_size != 0) {\n+      __ subptr(rsp, _xmm_spill_size);\n+      if (_xmm_spill_mode == ZXMMSpillMode::avx128) {\n+        __ movdqu(Address(rsp, _xmm_size * 7), xmm7);\n+        __ movdqu(Address(rsp, _xmm_size * 6), xmm6);\n+        __ movdqu(Address(rsp, _xmm_size * 5), xmm5);\n+        __ movdqu(Address(rsp, _xmm_size * 4), xmm4);\n+        __ movdqu(Address(rsp, _xmm_size * 3), xmm3);\n+        __ movdqu(Address(rsp, _xmm_size * 2), xmm2);\n+        __ movdqu(Address(rsp, _xmm_size * 1), xmm1);\n+        __ movdqu(Address(rsp, _xmm_size * 0), xmm0);\n+      } else {\n+        assert(_xmm_spill_mode == ZXMMSpillMode::avx256, \"AVX support ends at avx256\");\n+        __ vmovdqu(Address(rsp, _xmm_size * 7), xmm7);\n+        __ vmovdqu(Address(rsp, _xmm_size * 6), xmm6);\n+        __ vmovdqu(Address(rsp, _xmm_size * 5), xmm5);\n+        __ vmovdqu(Address(rsp, _xmm_size * 4), xmm4);\n+        __ vmovdqu(Address(rsp, _xmm_size * 3), xmm3);\n+        __ vmovdqu(Address(rsp, _xmm_size * 2), xmm2);\n+        __ vmovdqu(Address(rsp, _xmm_size * 1), xmm1);\n+        __ vmovdqu(Address(rsp, _xmm_size * 0), xmm0);\n+      }\n+    }\n+  }\n+\n+  void restore() {\n+    MacroAssembler* masm = _masm;\n+    if (_xmm_spill_size != 0) {\n+      if (_xmm_spill_mode == ZXMMSpillMode::avx128) {\n+        __ movdqu(xmm0, Address(rsp, _xmm_size * 0));\n+        __ movdqu(xmm1, Address(rsp, _xmm_size * 1));\n+        __ movdqu(xmm2, Address(rsp, _xmm_size * 2));\n+        __ movdqu(xmm3, Address(rsp, _xmm_size * 3));\n+        __ movdqu(xmm4, Address(rsp, _xmm_size * 4));\n+        __ movdqu(xmm5, Address(rsp, _xmm_size * 5));\n+        __ movdqu(xmm6, Address(rsp, _xmm_size * 6));\n+        __ movdqu(xmm7, Address(rsp, _xmm_size * 7));\n+      } else {\n+        assert(_xmm_spill_mode == ZXMMSpillMode::avx256, \"AVX support ends at avx256\");\n+        __ vmovdqu(xmm0, Address(rsp, _xmm_size * 0));\n+        __ vmovdqu(xmm1, Address(rsp, _xmm_size * 1));\n+        __ vmovdqu(xmm2, Address(rsp, _xmm_size * 2));\n+        __ vmovdqu(xmm3, Address(rsp, _xmm_size * 3));\n+        __ vmovdqu(xmm4, Address(rsp, _xmm_size * 4));\n+        __ vmovdqu(xmm5, Address(rsp, _xmm_size * 5));\n+        __ vmovdqu(xmm6, Address(rsp, _xmm_size * 6));\n+        __ vmovdqu(xmm7, Address(rsp, _xmm_size * 7));\n+      }\n+      __ addptr(rsp, _xmm_spill_size);\n+    }\n+\n+    __ pop(r11);\n+    __ pop(r10);\n+    __ pop(r9);\n+    __ pop(r8);\n+    __ pop(rsi);\n+    __ pop(rdi);\n+    __ pop(rdx);\n+    __ pop(rcx);\n+    if (_result == noreg) {\n+      __ pop(rax);\n+    } else if (_result == rax) {\n+      __ addptr(rsp, wordSize);\n+    } else {\n+      __ movptr(_result, rax);\n+      __ pop(rax);\n+    }\n+  }\n+\n+  static int compute_xmm_size(ZXMMSpillMode spill_mode) {\n+    switch (spill_mode) {\n+      case ZXMMSpillMode::none:\n+        return 0;\n+      case ZXMMSpillMode::avx128:\n+        return wordSize * 2;\n+      case ZXMMSpillMode::avx256:\n+        return wordSize * 4;\n+      default:\n+        ShouldNotReachHere();\n+        return 0;\n+    }\n+  }\n+\n+public:\n+  ZRuntimeCallSpill(MacroAssembler* masm, Register result, ZXMMSpillMode spill_mode)\n+    : _xmm_spill_mode(spill_mode),\n+      _xmm_size(compute_xmm_size(spill_mode)),\n+      _xmm_spill_size(_xmm_size * Argument::n_float_register_parameters_j),\n+      _masm(masm),\n+      _result(result) {\n+    \/\/ We may end up here from generate_native_wrapper, then the method may have\n+    \/\/ floats as arguments, and we must spill them before calling the VM runtime\n+    \/\/ leaf. From the interpreter all floats are passed on the stack.\n+    assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+    save();\n+  }\n+\n+  ~ZRuntimeCallSpill() {\n+    restore();\n+  }\n+};\n+\n@@ -104,0 +244,1 @@\n+  Label uncolor;\n@@ -115,0 +256,4 @@\n+  const bool on_non_strong =\n+      (decorators & ON_WEAK_OOP_REF) != 0 ||\n+      (decorators & ON_PHANTOM_OOP_REF) != 0;\n+\n@@ -116,2 +261,7 @@\n-  __ testptr(dst, address_bad_mask_from_thread(r15_thread));\n-  __ jcc(Assembler::zero, done);\n+  if (on_non_strong) {\n+    __ testptr(dst, mark_bad_mask_from_thread(r15_thread));\n+  } else {\n+    __ testptr(dst, load_bad_mask_from_thread(r15_thread));\n+  }\n+\n+  __ jcc(Assembler::zero, uncolor);\n@@ -123,26 +273,5 @@\n-  \/\/ Save registers\n-  __ push(rax);\n-  __ push(rcx);\n-  __ push(rdx);\n-  __ push(rdi);\n-  __ push(rsi);\n-  __ push(r8);\n-  __ push(r9);\n-  __ push(r10);\n-  __ push(r11);\n-\n-  \/\/ We may end up here from generate_native_wrapper, then the method may have\n-  \/\/ floats as arguments, and we must spill them before calling the VM runtime\n-  \/\/ leaf. From the interpreter all floats are passed on the stack.\n-  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n-  const int xmm_size = wordSize * 2;\n-  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n-  __ subptr(rsp, xmm_spill_size);\n-  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n-  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n-  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n-  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n-  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n-  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n-  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n-  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+  {\n+    \/\/ Call VM\n+    ZRuntimeCallSpill rcs(masm, dst, ZXMMSpillMode::avx128);\n+    call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), dst, scratch);\n+  }\n@@ -150,24 +279,10 @@\n-  \/\/ Call VM\n-  call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators), dst, scratch);\n-\n-  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n-  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n-  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n-  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n-  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n-  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n-  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n-  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n-  __ addptr(rsp, xmm_spill_size);\n-\n-  __ pop(r11);\n-  __ pop(r10);\n-  __ pop(r9);\n-  __ pop(r8);\n-  __ pop(rsi);\n-  __ pop(rdi);\n-  __ pop(rdx);\n-  __ pop(rcx);\n-\n-  if (dst == rax) {\n-    __ addptr(rsp, wordSize);\n+  \/\/ Slow-path has already uncolored\n+  __ jmp(done);\n+\n+  __ bind(uncolor);\n+\n+  __ movptr(scratch, rcx); \/\/ Save rcx because shrq needs shift in rcx\n+  __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+  if (dst == rcx) {\n+    \/\/ Dst was rcx which is saved in scratch because shrq needs rcx for shift\n+    __ shrq(scratch);\n@@ -175,2 +290,1 @@\n-    __ movptr(dst, rax);\n-    __ pop(rax);\n+    __ shrq(dst);\n@@ -178,0 +292,1 @@\n+  __ movptr(rcx, scratch); \/\/ restore rcx\n@@ -189,1 +304,202 @@\n-#ifdef ASSERT\n+static void emit_store_fast_path_check(MacroAssembler* masm, Address ref_addr, bool is_atomic, Label& medium_path) {\n+  if (is_atomic) {\n+    \/\/ Atomic operations must ensure that the contents of memory are store-good before\n+    \/\/ an atomic operation can execute.\n+    \/\/ A not relocatable object could have spurious raw null pointers in its fields after\n+    \/\/ getting promoted to the old generation.\n+    __ cmpw(ref_addr, barrier_Relocation::unpatched);\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodAfterCmp);\n+  } else {\n+    \/\/ Stores on relocatable objects never need to deal with raw null pointers in fields.\n+    \/\/ Raw null pointers may only exist in the young generation, as they get pruned when\n+    \/\/ the object is relocated to old. And no pre-write barrier needs to perform any action\n+    \/\/ in the young generation.\n+    __ Assembler::testl(ref_addr, barrier_Relocation::unpatched);\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreBadAfterTest);\n+  }\n+  __ jcc(Assembler::notEqual, medium_path);\n+}\n+\n+#ifdef COMPILER2\n+static int store_fast_path_check_size(MacroAssembler* masm, Address ref_addr, bool is_atomic, Label& medium_path) {\n+  if (!VM_Version::has_intel_jcc_erratum()) {\n+    return 0;\n+  }\n+  int size = 0;\n+  bool in_scratch_emit_size = masm->code_section()->scratch_emit();\n+  if (!in_scratch_emit_size) {\n+    \/\/ Temporarily register as scratch buffer so that relocations don't register\n+    masm->code_section()->set_scratch_emit();\n+  }\n+  \/\/ First emit the code, to measure its size\n+  address insts_end = masm->code_section()->end();\n+  \/\/ The dummy medium path label is bound after the code emission. This ensures\n+  \/\/ full size of the generated jcc, which is what the real barrier will have\n+  \/\/ as well, as it also binds after the emission of the barrier.\n+  Label dummy_medium_path;\n+  emit_store_fast_path_check(masm, ref_addr, is_atomic, dummy_medium_path);\n+  address emitted_end = masm->code_section()->end();\n+  size = (int)(intptr_t)(emitted_end - insts_end);\n+  __ bind(dummy_medium_path);\n+  if (!in_scratch_emit_size) {\n+    \/\/ Potentially restore scratchyness\n+    masm->code_section()->clear_scratch_emit();\n+  }\n+  \/\/ Roll back code, now that we know the size\n+  masm->code_section()->set_end(insts_end);\n+  return size;\n+}\n+#endif\n+\n+static void emit_store_fast_path_check_c2(MacroAssembler* masm, Address ref_addr, bool is_atomic, Label& medium_path) {\n+#ifdef COMPILER2\n+  \/\/ This is a JCC erratum mitigation wrapper for calling the inner check\n+  int size = store_fast_path_check_size(masm, ref_addr, is_atomic, medium_path);\n+  \/\/ Emit JCC erratum mitigation nops with the right size\n+  IntelJccErratumAlignment(*masm, size);\n+  \/\/ Emit the JCC erratum mitigation guarded code\n+  emit_store_fast_path_check(masm, ref_addr, is_atomic, medium_path);\n+#endif\n+}\n+\n+static bool is_c2_compilation() {\n+  CompileTask* task = ciEnv::current()->task();\n+  return task != nullptr && is_c2_compile(task->comp_level());\n+}\n+\n+void ZBarrierSetAssembler::store_barrier_fast(MacroAssembler* masm,\n+                                              Address ref_addr,\n+                                              Register rnew_zaddress,\n+                                              Register rnew_zpointer,\n+                                              bool in_nmethod,\n+                                              bool is_atomic,\n+                                              Label& medium_path,\n+                                              Label& medium_path_continuation) const {\n+  assert_different_registers(ref_addr.base(), rnew_zpointer);\n+  assert_different_registers(ref_addr.index(), rnew_zpointer);\n+  assert_different_registers(rnew_zaddress, rnew_zpointer);\n+\n+  if (in_nmethod) {\n+    if (is_c2_compilation()) {\n+      emit_store_fast_path_check_c2(masm, ref_addr, is_atomic, medium_path);\n+    } else {\n+      emit_store_fast_path_check(masm, ref_addr, is_atomic, medium_path);\n+    }\n+    __ bind(medium_path_continuation);\n+    if (rnew_zaddress != noreg) {\n+      \/\/ noreg means null; no need to color\n+      __ movptr(rnew_zpointer, rnew_zaddress);\n+      __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeShl);\n+      __ shlq(rnew_zpointer, barrier_Relocation::unpatched);\n+      __ orq_imm32(rnew_zpointer, barrier_Relocation::unpatched);\n+      __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodAfterOr);\n+    }\n+  } else {\n+    __ movzwq(rnew_zpointer, ref_addr);\n+    __ testq(rnew_zpointer, Address(r15_thread, ZThreadLocalData::store_bad_mask_offset()));\n+    __ jcc(Assembler::notEqual, medium_path);\n+    __ bind(medium_path_continuation);\n+    if (rnew_zaddress == noreg) {\n+      __ xorptr(rnew_zpointer, rnew_zpointer);\n+    } else {\n+      __ movptr(rnew_zpointer, rnew_zaddress);\n+    }\n+    assert_different_registers(rcx, rnew_zpointer);\n+    __ push(rcx);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shlq(rnew_zpointer);\n+    __ pop(rcx);\n+    __ orq(rnew_zpointer, Address(r15_thread, ZThreadLocalData::store_good_mask_offset()));\n+  }\n+}\n+\n+static void store_barrier_buffer_add(MacroAssembler* masm,\n+                                     Address ref_addr,\n+                                     Register tmp1,\n+                                     Label& slow_path) {\n+  Address buffer(r15_thread, ZThreadLocalData::store_barrier_buffer_offset());\n+\n+  __ movptr(tmp1, buffer);\n+\n+  \/\/ Combined pointer bump and check if the buffer is disabled or full\n+  __ cmpptr(Address(tmp1, ZStoreBarrierBuffer::current_offset()), 0);\n+  __ jcc(Assembler::equal, slow_path);\n+\n+  Register tmp2 = r15_thread;\n+  __ push(tmp2);\n+\n+  \/\/ Bump the pointer\n+  __ movq(tmp2, Address(tmp1, ZStoreBarrierBuffer::current_offset()));\n+  __ subq(tmp2, sizeof(ZStoreBarrierEntry));\n+  __ movq(Address(tmp1, ZStoreBarrierBuffer::current_offset()), tmp2);\n+\n+  \/\/ Compute the buffer entry address\n+  __ lea(tmp2, Address(tmp1, tmp2, Address::times_1, ZStoreBarrierBuffer::buffer_offset()));\n+\n+  \/\/ Compute and log the store address\n+  __ lea(tmp1, ref_addr);\n+  __ movptr(Address(tmp2, in_bytes(ZStoreBarrierEntry::p_offset())), tmp1);\n+\n+  \/\/ Load and log the prev value\n+  __ movptr(tmp1, Address(tmp1, 0));\n+  __ movptr(Address(tmp2, in_bytes(ZStoreBarrierEntry::prev_offset())), tmp1);\n+\n+  __ pop(tmp2);\n+}\n+\n+void ZBarrierSetAssembler::store_barrier_medium(MacroAssembler* masm,\n+                                                Address ref_addr,\n+                                                Register tmp,\n+                                                bool is_native,\n+                                                bool is_atomic,\n+                                                Label& medium_path_continuation,\n+                                                Label& slow_path,\n+                                                Label& slow_path_continuation) const {\n+  assert_different_registers(ref_addr.base(), tmp);\n+\n+  \/\/ The reason to end up in the medium path is that the pre-value was not 'good'.\n+\n+  if (is_native) {\n+    __ jmp(slow_path);\n+    __ bind(slow_path_continuation);\n+    __ jmp(medium_path_continuation);\n+  } else if (is_atomic) {\n+    \/\/ Atomic accesses can get to the medium fast path because the value was a\n+    \/\/ raw null value. If it was not null, then there is no doubt we need to take a slow path.\n+    __ cmpptr(ref_addr, 0);\n+    __ jcc(Assembler::notEqual, slow_path);\n+\n+    \/\/ If we get this far, we know there is a young raw null value in the field.\n+    \/\/ Try to self-heal null values for atomic accesses\n+    __ push(rax);\n+    __ push(rbx);\n+    __ push(rcx);\n+\n+    __ lea(rcx, ref_addr);\n+    __ xorq(rax, rax);\n+    __ movptr(rbx, Address(r15, ZThreadLocalData::store_good_mask_offset()));\n+\n+    __ lock();\n+    __ cmpxchgq(rbx, Address(rcx, 0));\n+\n+    __ pop(rcx);\n+    __ pop(rbx);\n+    __ pop(rax);\n+\n+    __ jcc(Assembler::notEqual, slow_path);\n+\n+    __ bind(slow_path_continuation);\n+    __ jmp(medium_path_continuation);\n+  } else {\n+    \/\/ A non-atomic relocatable object won't get to the medium fast path due to a\n+    \/\/ raw null in the young generation. We only get here because the field is bad.\n+    \/\/ In this path we don't need any self healing, so we can avoid a runtime call\n+    \/\/ most of the time by buffering the store barrier to be applied lazily.\n+    store_barrier_buffer_add(masm,\n+                             ref_addr,\n+                             tmp,\n+                             slow_path);\n+    __ bind(slow_path_continuation);\n+    __ jmp(medium_path_continuation);\n+  }\n+}\n@@ -202,1 +518,2 @@\n-  \/\/ Verify oop store\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n@@ -204,3 +521,15 @@\n-    \/\/ Note that src could be noreg, which means we\n-    \/\/ are storing null and can skip verification.\n-    if (src != noreg) {\n+    assert_different_registers(src, tmp1, dst.base(), dst.index());\n+\n+    if (dest_uninitialized) {\n+      assert_different_registers(rcx, tmp1);\n+      if (src == noreg) {\n+        __ xorq(tmp1, tmp1);\n+      } else {\n+        __ movptr(tmp1, src);\n+      }\n+      __ push(rcx);\n+      __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+      __ shlq(tmp1);\n+      __ pop(rcx);\n+      __ orq(tmp1, Address(r15_thread, ZThreadLocalData::store_good_mask_offset()));\n+    } else {\n@@ -208,4 +537,25 @@\n-      __ testptr(src, address_bad_mask_from_thread(r15_thread));\n-      __ jcc(Assembler::zero, done);\n-      __ stop(\"Verify oop store failed\");\n-      __ should_not_reach_here();\n+      Label medium;\n+      Label medium_continuation;\n+      Label slow;\n+      Label slow_continuation;\n+      store_barrier_fast(masm, dst, src, tmp1, false, false, medium, medium_continuation);\n+      __ jmp(done);\n+      __ bind(medium);\n+      store_barrier_medium(masm,\n+                           dst,\n+                           tmp1,\n+                           false \/* is_native *\/,\n+                           false \/* is_atomic *\/,\n+                           medium_continuation,\n+                           slow,\n+                           slow_continuation);\n+\n+      __ bind(slow);\n+      {\n+        \/\/ Call VM\n+        ZRuntimeCallSpill rcs(masm, noreg, ZXMMSpillMode::avx128);\n+        __ leaq(c_rarg0, dst);\n+        __ MacroAssembler::call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), c_rarg0);\n+      }\n+\n+      __ jmp(slow_continuation);\n@@ -214,0 +564,121 @@\n+\n+    \/\/ Store value\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, tmp1, noreg, noreg, noreg);\n+  } else {\n+    BarrierSetAssembler::store_at(masm, decorators, type, dst, src, noreg, noreg, noreg);\n+  }\n+\n+  BLOCK_COMMENT(\"} ZBarrierSetAssembler::store_at\");\n+}\n+\n+bool ZBarrierSetAssembler::supports_avx3_masked_arraycopy() {\n+  return false;\n+}\n+\n+static void load_arraycopy_masks(MacroAssembler* masm) {\n+  \/\/ xmm2: load_bad_mask\n+  \/\/ xmm3: store_bad_mask\n+  \/\/ xmm4: store_good_mask\n+  if (UseAVX >= 2) {\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorLoadBadMask));\n+    __ vmovdqu(xmm2, Address(r10, 0));\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorStoreBadMask));\n+    __ vmovdqu(xmm3, Address(r10, 0));\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorStoreGoodMask));\n+    __ vmovdqu(xmm4, Address(r10, 0));\n+  } else {\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorLoadBadMask));\n+    __ movdqu(xmm2, Address(r10, 0));\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorStoreBadMask));\n+    __ movdqu(xmm3, Address(r10, 0));\n+    __ lea(r10, ExternalAddress((address)&ZPointerVectorStoreGoodMask));\n+    __ movdqu(xmm4, Address(r10, 0));\n+  }\n+}\n+\n+static ZXMMSpillMode compute_arraycopy_spill_mode() {\n+  if (UseAVX >= 2) {\n+    return ZXMMSpillMode::avx256;\n+  } else {\n+    return ZXMMSpillMode::avx128;\n+  }\n+}\n+\n+void ZBarrierSetAssembler::copy_load_at(MacroAssembler* masm,\n+                                        DecoratorSet decorators,\n+                                        BasicType type,\n+                                        size_t bytes,\n+                                        Register dst,\n+                                        Address src,\n+                                        Register tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst, src, tmp);\n+    return;\n+  }\n+\n+  Label load_done;\n+\n+  \/\/ Load oop at address\n+  __ movptr(dst, src);\n+\n+  \/\/ Test address bad mask\n+  __ Assembler::testl(dst, (int32_t)(uint32_t)ZPointerLoadBadMask);\n+  _load_bad_relocations.append(__ code_section()->end());\n+  __ jcc(Assembler::zero, load_done);\n+\n+  {\n+    \/\/ Call VM\n+    ZRuntimeCallSpill rcs(masm, dst, compute_arraycopy_spill_mode());\n+    __ leaq(c_rarg1, src);\n+    call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_field_preloaded_store_good_addr(), dst, c_rarg1);\n+  }\n+\n+  __ bind(load_done);\n+\n+  \/\/ Remove metadata bits so that the store side (vectorized or non-vectorized) can\n+  \/\/ inject the store-good color with an or instruction.\n+  __ andq(dst, _zpointer_address_mask);\n+\n+  if ((decorators & ARRAYCOPY_CHECKCAST) != 0) {\n+    \/\/ The checkcast arraycopy needs to be able to dereference the oops in order to perform a typechecks.\n+    assert(tmp != rcx, \"Surprising choice of temp register\");\n+    __ movptr(tmp, rcx);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shrq(dst);\n+    __ movptr(rcx, tmp);\n+  }\n+}\n+\n+void ZBarrierSetAssembler::copy_store_at(MacroAssembler* masm,\n+                                         DecoratorSet decorators,\n+                                         BasicType type,\n+                                         size_t bytes,\n+                                         Address dst,\n+                                         Register src,\n+                                         Register tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src, tmp);\n+    return;\n+  }\n+\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  if (!dest_uninitialized) {\n+    Label store;\n+    Label store_bad;\n+    __ Assembler::testl(dst, (int32_t)(uint32_t)ZPointerStoreBadMask);\n+    _store_bad_relocations.append(__ code_section()->end());\n+    __ jcc(Assembler::zero, store);\n+\n+    store_barrier_buffer_add(masm, dst, tmp, store_bad);\n+    __ jmp(store);\n+\n+    __ bind(store_bad);\n+    {\n+      \/\/ Call VM\n+      ZRuntimeCallSpill rcs(masm, noreg, compute_arraycopy_spill_mode());\n+      __ leaq(c_rarg0, dst);\n+      __ MacroAssembler::call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), c_rarg0);\n+    }\n+\n+    __ bind(store);\n@@ -216,0 +687,12 @@\n+  if ((decorators & ARRAYCOPY_CHECKCAST) != 0) {\n+    assert(tmp != rcx, \"Surprising choice of temp register\");\n+    __ movptr(tmp, rcx);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shlq(src);\n+    __ movptr(rcx, tmp);\n+  }\n+\n+  \/\/ Color\n+  __ orq_imm32(src, (int32_t)(uint32_t)ZPointerStoreGoodMask);\n+  _store_good_relocations.append(__ code_section()->end());\n+\n@@ -217,1 +700,2 @@\n-  BarrierSetAssembler::store_at(masm, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  __ movptr(dst, src);\n+}\n@@ -219,1 +703,88 @@\n-  BLOCK_COMMENT(\"} ZBarrierSetAssembler::store_at\");\n+void ZBarrierSetAssembler::copy_load_at(MacroAssembler* masm,\n+                                        DecoratorSet decorators,\n+                                        BasicType type,\n+                                        size_t bytes,\n+                                        XMMRegister dst,\n+                                        Address src,\n+                                        Register tmp,\n+                                        XMMRegister xmm_tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_load_at(masm, decorators, type, bytes, dst, src, tmp, xmm_tmp);\n+    return;\n+  }\n+  Address src0(src.base(), src.index(), src.scale(), src.disp() + 0);\n+  Address src1(src.base(), src.index(), src.scale(), src.disp() + 8);\n+  Address src2(src.base(), src.index(), src.scale(), src.disp() + 16);\n+  Address src3(src.base(), src.index(), src.scale(), src.disp() + 24);\n+\n+  \/\/ Registers set up in the prologue:\n+  \/\/ xmm2: load_bad_mask\n+  \/\/ xmm3: store_bad_mask\n+  \/\/ xmm4: store_good_mask\n+\n+  if (bytes == 16) {\n+    Label done;\n+    Label fallback;\n+\n+    if (UseAVX >= 1) {\n+      \/\/ Load source vector\n+      __ movdqu(dst, src);\n+      \/\/ Check source load-good\n+      __ movdqu(xmm_tmp, dst);\n+      __ ptest(xmm_tmp, xmm2);\n+      __ jcc(Assembler::notZero, fallback);\n+\n+      \/\/ Remove bad metadata bits\n+      __ vpandn(dst, xmm3, dst, Assembler::AVX_128bit);\n+      __ jmp(done);\n+    }\n+\n+    __ bind(fallback);\n+\n+    __ subptr(rsp, wordSize * 2);\n+\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src0, noreg);\n+    __ movq(Address(rsp, 0), tmp);\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src1, noreg);\n+    __ movq(Address(rsp, 8), tmp);\n+\n+    __ movdqu(dst, Address(rsp, 0));\n+    __ addptr(rsp, wordSize * 2);\n+\n+    __ bind(done);\n+  } else if (bytes == 32) {\n+    Label done;\n+    Label fallback;\n+    assert(UseAVX >= 2, \"Assume that UseAVX >= 2\");\n+\n+    \/\/ Load source vector\n+    __ vmovdqu(dst, src);\n+    \/\/ Check source load-good\n+    __ vmovdqu(xmm_tmp, dst);\n+    __ vptest(xmm_tmp, xmm2, Assembler::AVX_256bit);\n+    __ jcc(Assembler::notZero, fallback);\n+\n+    \/\/ Remove bad metadata bits so that the store can colour the pointers with an or instruction.\n+    \/\/ This makes the fast path and slow path formats look the same, in the sense that they don't\n+    \/\/ have any of the store bad bits.\n+    __ vpandn(dst, xmm3, dst, Assembler::AVX_256bit);\n+    __ jmp(done);\n+\n+    __ bind(fallback);\n+\n+    __ subptr(rsp, wordSize * 4);\n+\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src0, noreg);\n+    __ movq(Address(rsp, 0), tmp);\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src1, noreg);\n+    __ movq(Address(rsp, 8), tmp);\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src2, noreg);\n+    __ movq(Address(rsp, 16), tmp);\n+    ZBarrierSetAssembler::copy_load_at(masm, decorators, type, 8, tmp, src3, noreg);\n+    __ movq(Address(rsp, 24), tmp);\n+\n+    __ vmovdqu(dst, Address(rsp, 0));\n+    __ addptr(rsp, wordSize * 4);\n+\n+    __ bind(done);\n+  }\n@@ -222,1 +793,97 @@\n-#endif \/\/ ASSERT\n+void ZBarrierSetAssembler::copy_store_at(MacroAssembler* masm,\n+                                         DecoratorSet decorators,\n+                                         BasicType type,\n+                                         size_t bytes,\n+                                         Address dst,\n+                                         XMMRegister src,\n+                                         Register tmp1,\n+                                         Register tmp2,\n+                                         XMMRegister xmm_tmp) {\n+  if (!is_reference_type(type)) {\n+    BarrierSetAssembler::copy_store_at(masm, decorators, type, bytes, dst, src, tmp1, tmp2, xmm_tmp);\n+    return;\n+  }\n+  Address dst0(dst.base(), dst.index(), dst.scale(), dst.disp() + 0);\n+  Address dst1(dst.base(), dst.index(), dst.scale(), dst.disp() + 8);\n+  Address dst2(dst.base(), dst.index(), dst.scale(), dst.disp() + 16);\n+  Address dst3(dst.base(), dst.index(), dst.scale(), dst.disp() + 24);\n+\n+  bool dest_uninitialized = (decorators & IS_DEST_UNINITIALIZED) != 0;\n+\n+  \/\/ Registers set up in the prologue:\n+  \/\/ xmm2: load_bad_mask\n+  \/\/ xmm3: store_bad_mask\n+  \/\/ xmm4: store_good_mask\n+\n+  if (bytes == 16) {\n+    Label done;\n+    Label fallback;\n+\n+    if (UseAVX >= 1) {\n+      if (!dest_uninitialized) {\n+        \/\/ Load destination vector\n+        __ movdqu(xmm_tmp, dst);\n+        \/\/ Check destination store-good\n+        __ ptest(xmm_tmp, xmm3);\n+        __ jcc(Assembler::notZero, fallback);\n+      }\n+\n+      \/\/ Color source\n+      __ por(src, xmm4);\n+      \/\/ Store source in destination\n+      __ movdqu(dst, src);\n+      __ jmp(done);\n+    }\n+\n+    __ bind(fallback);\n+\n+    __ subptr(rsp, wordSize * 2);\n+    __ movdqu(Address(rsp, 0), src);\n+\n+    __ movq(tmp1, Address(rsp, 0));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst0, tmp1, tmp2);\n+    __ movq(tmp1, Address(rsp, 8));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst1, tmp1, tmp2);\n+\n+    __ addptr(rsp, wordSize * 2);\n+\n+    __ bind(done);\n+  } else if (bytes == 32) {\n+    Label done;\n+    Label fallback;\n+    assert(UseAVX >= 2, \"Assume UseAVX >= 2\");\n+\n+    if (!dest_uninitialized) {\n+      \/\/ Load destination vector\n+      __ vmovdqu(xmm_tmp, dst);\n+      \/\/ Check destination store-good\n+      __ vptest(xmm_tmp, xmm3, Assembler::AVX_256bit);\n+      __ jcc(Assembler::notZero, fallback);\n+    }\n+\n+    \/\/ Color source\n+    __ vpor(src, src, xmm4, Assembler::AVX_256bit);\n+\n+    \/\/ Store colored source in destination\n+    __ vmovdqu(dst, src);\n+    __ jmp(done);\n+\n+    __ bind(fallback);\n+\n+    __ subptr(rsp, wordSize * 4);\n+    __ vmovdqu(Address(rsp, 0), src);\n+\n+    __ movq(tmp1, Address(rsp, 0));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst0, tmp1, tmp2);\n+    __ movq(tmp1, Address(rsp, 8));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst1, tmp1, tmp2);\n+    __ movq(tmp1, Address(rsp, 16));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst2, tmp1, tmp2);\n+    __ movq(tmp1, Address(rsp, 24));\n+    ZBarrierSetAssembler::copy_store_at(masm, decorators, type, 8, dst3, tmp1, tmp2);\n+\n+    __ addptr(rsp, wordSize * 4);\n+\n+    __ bind(done);\n+  }\n+}\n@@ -237,8 +904,1 @@\n-  \/\/ Save registers\n-  __ pusha();\n-\n-  \/\/ Call VM\n-  call_vm(masm, ZBarrierSetRuntime::load_barrier_on_oop_array_addr(), src, count);\n-\n-  \/\/ Restore registers\n-  __ popa();\n+  load_arraycopy_masks(masm);\n@@ -256,2 +916,1 @@\n-  \/\/ Resolve jobject\n-  BarrierSetAssembler::try_resolve_jobject_in_native(masm, jni_env, obj, tmp, slowpath);\n+  Label done, tagged, weak_tagged, uncolor;\n@@ -259,2 +918,25 @@\n-  \/\/ Test address bad mask\n-  __ testptr(obj, address_bad_mask_from_jni_env(jni_env));\n+  \/\/ Test for tag\n+  __ testptr(obj, JNIHandles::tag_mask);\n+  __ jcc(Assembler::notZero, tagged);\n+\n+  \/\/ Resolve local handle\n+  __ movptr(obj, Address(obj, 0));\n+  __ jmp(done);\n+\n+  __ bind(tagged);\n+\n+  \/\/ Test for weak tag\n+  __ testptr(obj, JNIHandles::TypeTag::weak_global);\n+  __ jcc(Assembler::notZero, weak_tagged);\n+\n+  \/\/ Resolve global handle\n+  __ movptr(obj, Address(obj, -JNIHandles::TypeTag::global));\n+  __ testptr(obj, load_bad_mask_from_jni_env(jni_env));\n+  __ jcc(Assembler::notZero, slowpath);\n+  __ jmp(uncolor);\n+\n+  __ bind(weak_tagged);\n+\n+  \/\/ Resolve weak handle\n+  __ movptr(obj, Address(obj, -JNIHandles::TypeTag::weak_global));\n+  __ testptr(obj, mark_bad_mask_from_jni_env(jni_env));\n@@ -263,0 +945,17 @@\n+  __ bind(uncolor);\n+\n+  \/\/ Uncolor\n+  if (obj == rcx) {\n+    __ movptr(tmp, obj);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shrq(tmp);\n+    __ movptr(obj, tmp);\n+  } else {\n+    __ push(rcx);\n+    __ movptr(rcx, ExternalAddress((address)&ZPointerLoadShift));\n+    __ shrq(obj);\n+    __ pop(rcx);\n+  }\n+\n+  __ bind(done);\n+\n@@ -271,3 +970,39 @@\n-void ZBarrierSetAssembler::generate_c1_load_barrier_test(LIR_Assembler* ce,\n-                                                         LIR_Opr ref) const {\n-  __ testptr(ref->as_register(), address_bad_mask_from_thread(r15_thread));\n+static void z_uncolor(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeShl);\n+  __ shrq(ref->as_register(), barrier_Relocation::unpatched);\n+}\n+\n+static void z_color(LIR_Assembler* ce, LIR_Opr ref) {\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatLoadGoodBeforeShl);\n+  __ shlq(ref->as_register(), barrier_Relocation::unpatched);\n+  __ orq_imm32(ref->as_register(), barrier_Relocation::unpatched);\n+  __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatStoreGoodAfterOr);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_uncolor(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_uncolor(ce, ref);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_color(LIR_Assembler* ce, LIR_Opr ref) const {\n+  z_color(ce, ref);\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_load_barrier(LIR_Assembler* ce,\n+                                                    LIR_Opr ref,\n+                                                    ZLoadBarrierStubC1* stub,\n+                                                    bool on_non_strong) const {\n+  if (on_non_strong) {\n+    \/\/ Test against MarkBad mask\n+    __ Assembler::testl(ref->as_register(), barrier_Relocation::unpatched);\n+    __ relocate(barrier_Relocation::spec(), ZBarrierRelocationFormatMarkBadAfterTest);\n+\n+    \/\/ Slow path if not zero\n+    __ jcc(Assembler::notZero, *stub->entry());\n+    \/\/ Fast path: convert to colorless\n+    z_uncolor(ce, ref);\n+  } else {\n+    \/\/ Convert to colorless and fast path test\n+    z_uncolor(ce, ref);\n+    __ jcc(Assembler::above, *stub->entry());\n+  }\n+  __ bind(*stub->continuation());\n@@ -285,0 +1020,3 @@\n+  \/\/ The fast-path shift destroyed the oop - need to re-read it\n+  __ movptr(ref, ce->as_Address(stub->ref_addr()->as_address_ptr()));\n+\n@@ -325,0 +1063,49 @@\n+void ZBarrierSetAssembler::generate_c1_store_barrier(LIR_Assembler* ce,\n+                                                     LIR_Address* addr,\n+                                                     LIR_Opr new_zaddress,\n+                                                     LIR_Opr new_zpointer,\n+                                                     ZStoreBarrierStubC1* stub) const {\n+  Register rnew_zaddress = new_zaddress->as_register();\n+  Register rnew_zpointer = new_zpointer->as_register();\n+\n+  Register rbase = addr->base()->as_pointer_register();\n+  store_barrier_fast(ce->masm(),\n+                     ce->as_Address(addr),\n+                     rnew_zaddress,\n+                     rnew_zpointer,\n+                     true,\n+                     stub->is_atomic(),\n+                     *stub->entry(),\n+                     *stub->continuation());\n+}\n+\n+void ZBarrierSetAssembler::generate_c1_store_barrier_stub(LIR_Assembler* ce,\n+                                                          ZStoreBarrierStubC1* stub) const {\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Label slow;\n+  Label slow_continuation;\n+  store_barrier_medium(ce->masm(),\n+                       ce->as_Address(stub->ref_addr()->as_address_ptr()),\n+                       rscratch1,\n+                       false \/* is_native *\/,\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow,\n+                       slow_continuation);\n+\n+  __ bind(slow);\n+\n+  ce->leal(stub->ref_addr(), stub->new_zpointer());\n+\n+  \/\/ Setup arguments and call runtime stub\n+  __ subptr(rsp, 2 * BytesPerWord);\n+  ce->store_parameter(stub->new_zpointer()->as_pointer_register(), 0);\n+  __ call(RuntimeAddress(stub->runtime_stub()));\n+  __ addptr(rsp, 2 * BytesPerWord);\n+\n+  \/\/ Stub exit\n+  __ jmp(slow_continuation);\n+}\n+\n@@ -347,0 +1134,22 @@\n+void ZBarrierSetAssembler::generate_c1_store_barrier_runtime_stub(StubAssembler* sasm,\n+                                                                  bool self_healing) const {\n+  \/\/ Enter and save registers\n+  __ enter();\n+  __ save_live_registers_no_oop_map(true \/* save_fpu_registers *\/);\n+\n+  \/\/ Setup arguments\n+  __ load_parameter(0, c_rarg0);\n+\n+  \/\/ Call VM\n+  if (self_healing) {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr(), c_rarg0);\n+  } else {\n+    __ call_VM_leaf(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr(), c_rarg0);\n+  }\n+\n+  \/\/ Restore registers and return\n+  __ restore_live_registers(true \/* restore_fpu_registers *\/);\n+  __ leave();\n+  __ ret(0);\n+}\n+\n@@ -471,1 +1280,1 @@\n-  void initialize(ZLoadBarrierStubC2* stub) {\n+  void initialize(ZBarrierStubC2* stub) {\n@@ -484,1 +1293,4 @@\n-    caller_saved.Remove(OptoReg::as_OptoReg(stub->ref()->as_VMReg()));\n+\n+    if (stub->result() != noreg) {\n+      caller_saved.Remove(OptoReg::as_OptoReg(stub->result()->as_VMReg()));\n+    }\n@@ -488,3 +1300,0 @@\n-    if (stub->tmp() != noreg) {\n-      live.Insert(OptoReg::as_OptoReg(stub->tmp()->as_VMReg()));\n-    }\n@@ -548,2 +1357,2 @@\n-  ZSaveLiveRegisters(MacroAssembler* masm, ZLoadBarrierStubC2* stub) :\n-      _masm(masm),\n+  ZSaveLiveRegisters(MacroAssembler* masm, ZBarrierStubC2* stub)\n+    : _masm(masm),\n@@ -640,2 +1449,2 @@\n-  ZSetupArguments(MacroAssembler* masm, ZLoadBarrierStubC2* stub) :\n-      _masm(masm),\n+  ZSetupArguments(MacroAssembler* masm, ZLoadBarrierStubC2* stub)\n+    : _masm(masm),\n@@ -687,0 +1496,1 @@\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);\n@@ -692,0 +1502,3 @@\n+  \/\/ The fast-path shift destroyed the oop - need to re-read it\n+  __ movptr(stub->ref(), stub->ref_addr());\n+\n@@ -702,0 +1515,38 @@\n+void ZBarrierSetAssembler::generate_c2_store_barrier_stub(MacroAssembler* masm, ZStoreBarrierStubC2* stub) const {\n+  Assembler::InlineSkippedInstructionsCounter skipped_counter(masm);\n+  BLOCK_COMMENT(\"ZStoreBarrierStubC2\");\n+\n+  \/\/ Stub entry\n+  __ bind(*stub->entry());\n+\n+  Label slow;\n+  Label slow_continuation;\n+  store_barrier_medium(masm,\n+                       stub->ref_addr(),\n+                       stub->new_zpointer(),\n+                       stub->is_native(),\n+                       stub->is_atomic(),\n+                       *stub->continuation(),\n+                       slow,\n+                       slow_continuation);\n+\n+  __ bind(slow);\n+\n+  {\n+    ZSaveLiveRegisters save_live_registers(masm, stub);\n+    __ lea(c_rarg0, stub->ref_addr());\n+\n+    if (stub->is_native()) {\n+      __ call(RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_native_oop_field_without_healing_addr()));\n+    } else if (stub->is_atomic()) {\n+      __ call(RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_oop_field_with_healing_addr()));\n+    } else {\n+      __ call(RuntimeAddress(ZBarrierSetRuntime::store_barrier_on_oop_field_without_healing_addr()));\n+    }\n+  }\n+\n+  \/\/ Stub exit\n+  __ jmp(slow_continuation);\n+}\n+\n+#undef __\n@@ -704,0 +1555,74 @@\n+static int patch_barrier_relocation_offset(int format) {\n+  switch (format) {\n+  case ZBarrierRelocationFormatLoadGoodBeforeShl:\n+    return 3;\n+\n+  case ZBarrierRelocationFormatStoreGoodAfterCmp:\n+    return -2;\n+\n+  case ZBarrierRelocationFormatLoadBadAfterTest:\n+  case ZBarrierRelocationFormatMarkBadAfterTest:\n+  case ZBarrierRelocationFormatStoreBadAfterTest:\n+  case ZBarrierRelocationFormatStoreGoodAfterOr:\n+    return -4;\n+  case ZBarrierRelocationFormatStoreGoodAfterMov:\n+    return -3;\n+\n+  default:\n+    ShouldNotReachHere();\n+    return 0;\n+  }\n+}\n+\n+static uint16_t patch_barrier_relocation_value(int format) {\n+  switch (format) {\n+  case ZBarrierRelocationFormatLoadGoodBeforeShl:\n+    return (uint16_t)ZPointerLoadShift;\n+\n+  case ZBarrierRelocationFormatMarkBadAfterTest:\n+    return (uint16_t)ZPointerMarkBadMask;\n+\n+  case ZBarrierRelocationFormatLoadBadAfterTest:\n+    return (uint16_t)ZPointerLoadBadMask;\n+\n+  case ZBarrierRelocationFormatStoreGoodAfterCmp:\n+  case ZBarrierRelocationFormatStoreGoodAfterOr:\n+  case ZBarrierRelocationFormatStoreGoodAfterMov:\n+    return (uint16_t)ZPointerStoreGoodMask;\n+\n+  case ZBarrierRelocationFormatStoreBadAfterTest:\n+    return (uint16_t)ZPointerStoreBadMask;\n+\n+  default:\n+    ShouldNotReachHere();\n+    return 0;\n+  }\n+}\n+\n+void ZBarrierSetAssembler::patch_barrier_relocation(address addr, int format) {\n+  const int offset = patch_barrier_relocation_offset(format);\n+  const uint16_t value = patch_barrier_relocation_value(format);\n+  uint8_t* const patch_addr = (uint8_t*)addr + offset;\n+  if (format == ZBarrierRelocationFormatLoadGoodBeforeShl) {\n+    *patch_addr = (uint8_t)value;\n+  } else {\n+    *(uint16_t*)patch_addr = value;\n+  }\n+}\n+\n+void ZBarrierSetAssembler::patch_barriers() {\n+  for (int i = 0; i < _load_bad_relocations.length(); ++i) {\n+    address addr = _load_bad_relocations.at(i);\n+    patch_barrier_relocation(addr, ZBarrierRelocationFormatLoadBadAfterTest);\n+  }\n+  for (int i = 0; i < _store_bad_relocations.length(); ++i) {\n+    address addr = _store_bad_relocations.at(i);\n+    patch_barrier_relocation(addr, ZBarrierRelocationFormatStoreBadAfterTest);\n+  }\n+  for (int i = 0; i < _store_good_relocations.length(); ++i) {\n+    address addr = _store_good_relocations.at(i);\n+    patch_barrier_relocation(addr, ZBarrierRelocationFormatStoreGoodAfterOr);\n+  }\n+}\n+\n+\n@@ -707,0 +1632,1 @@\n+\n@@ -708,2 +1634,59 @@\n-  \/\/ Check if metadata bits indicate a bad oop\n-  __ testptr(obj, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n+  \/\/ C1 calls verfy_oop in the middle of barriers, before they have been uncolored\n+  \/\/ and after being colored. Therefore, we must deal with colored oops as well.\n+  Label done;\n+  Label check_oop;\n+  Label check_zaddress;\n+  int color_bits = ZPointerRemappedShift + ZPointerRemappedBits;\n+\n+  uintptr_t shifted_base_start_mask = (UCONST64(1) << (ZAddressHeapBaseShift + color_bits + 1)) - 1;\n+  uintptr_t shifted_base_end_mask = (UCONST64(1) << (ZAddressHeapBaseShift + 1)) - 1;\n+  uintptr_t shifted_base_mask = shifted_base_start_mask ^ shifted_base_end_mask;\n+\n+  uintptr_t shifted_address_end_mask = (UCONST64(1) << (color_bits + 1)) - 1;\n+  uintptr_t shifted_address_mask = shifted_address_end_mask ^ (uintptr_t)CONST64(-1);\n+\n+  \/\/ Check colored null\n+  __ mov64(tmp1, shifted_address_mask);\n+  __ testptr(tmp1, obj);\n+  __ jcc(Assembler::zero, done);\n+\n+  \/\/ Check for zpointer\n+  __ mov64(tmp1, shifted_base_mask);\n+  __ testptr(tmp1, obj);\n+  __ jcc(Assembler::zero, check_oop);\n+\n+  \/\/ Lookup shift\n+  __ movq(tmp1, obj);\n+  __ mov64(tmp2, shifted_address_end_mask);\n+  __ andq(tmp1, tmp2);\n+  __ shrq(tmp1, ZPointerRemappedShift);\n+  __ andq(tmp1, (1 << ZPointerRemappedBits) - 1);\n+  __ lea(tmp2, ExternalAddress((address)&ZPointerLoadShiftTable));\n+\n+  \/\/ Uncolor presumed zpointer\n+  assert(obj != rcx, \"bad choice of register\");\n+  if (rcx != tmp1 && rcx != tmp2) {\n+    __ push(rcx);\n+  }\n+  __ movl(rcx, Address(tmp2, tmp1, Address::times_4, 0));\n+  __ shrq(obj);\n+  if (rcx != tmp1 && rcx != tmp2) {\n+    __ pop(rcx);\n+  }\n+\n+  __ jmp(check_zaddress);\n+\n+  __ bind(check_oop);\n+\n+  \/\/ make sure klass is 'reasonable', which is not zero.\n+  __ load_klass(tmp1, obj, tmp2);  \/\/ get klass\n+  __ testptr(tmp1, tmp1);\n+  __ jcc(Assembler::zero, error); \/\/ if klass is null it is broken\n+\n+  __ bind(check_zaddress);\n+  \/\/ Check if the oop is in the right area of memory\n+  __ movptr(tmp1, obj);\n+  __ movptr(tmp2, (intptr_t) Universe::verify_oop_mask());\n+  __ andptr(tmp1, tmp2);\n+  __ movptr(tmp2, (intptr_t) Universe::verify_oop_bits());\n+  __ cmpptr(tmp1, tmp2);\n@@ -711,1 +1694,2 @@\n-  BarrierSetAssembler::check_oop(masm, obj, tmp1, tmp2, error);\n+\n+  __ bind(done);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/z\/zBarrierSetAssembler_x86.cpp","additions":1077,"deletions":93,"binary":false,"changes":1170,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/constMethodFlags.hpp\"\n@@ -193,1 +194,1 @@\n-      cmpw(Address(tmp, Method::intrinsic_id_offset_in_bytes()), static_cast<int>(vmIntrinsics::_compiledLambdaForm));\n+      cmpw(Address(tmp, Method::intrinsic_id_offset()), static_cast<int>(vmIntrinsics::_compiledLambdaForm));\n@@ -514,2 +515,2 @@\n-  movptr(result, Address(result, ConstantPool::cache_offset_in_bytes()));\n-  movptr(result, Address(result, ConstantPoolCache::resolved_references_offset_in_bytes()));\n+  movptr(result, Address(result, ConstantPool::cache_offset()));\n+  movptr(result, Address(result, ConstantPoolCache::resolved_references_offset()));\n@@ -530,1 +531,1 @@\n-  movptr(resolved_klasses, Address(cpool, ConstantPool::resolved_klasses_offset_in_bytes()));\n+  movptr(resolved_klasses, Address(cpool, ConstantPool::resolved_klasses_offset()));\n@@ -1047,1 +1048,1 @@\n-  movptr(rax, Address(robj, BasicObjectLock::obj_offset_in_bytes()));\n+  movptr(rax, Address(robj, BasicObjectLock::obj_offset()));\n@@ -1130,1 +1131,1 @@\n-    cmpptr(Address(rmon, BasicObjectLock::obj_offset_in_bytes()), NULL_WORD);\n+    cmpptr(Address(rmon, BasicObjectLock::obj_offset()), NULL_WORD);\n@@ -1200,1 +1201,1 @@\n-      testl(rcx, Method::scalarized_return_flag());\n+      testl(rcx, ConstMethodFlags::has_scalarized_return_flag());\n@@ -1349,1 +1350,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1361,2 +1362,2 @@\n-    const int obj_offset = BasicObjectLock::obj_offset_in_bytes();\n-    const int lock_offset = BasicObjectLock::lock_offset_in_bytes ();\n+    const int obj_offset = in_bytes(BasicObjectLock::obj_offset());\n+    const int lock_offset = in_bytes(BasicObjectLock::lock_offset());\n@@ -1376,2 +1377,20 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, 1);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, 1);\n+\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      if (EnableValhalla) {\n+        \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+        andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+      }\n@@ -1379,6 +1398,45 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    if (EnableValhalla) {\n-      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n-      andptr(swap_reg, ~((int) markWord::inline_type_bit_in_place));\n-    }\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n+\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      jcc(Assembler::zero, count_locking);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+      jcc(Assembler::notZero, slow_case);\n@@ -1386,47 +1444,2 @@\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n-\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    jcc(Assembler::zero, count_locking);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-    jcc(Assembler::notZero, slow_case);\n-\n-    bind(count_locking);\n+      bind(count_locking);\n+    }\n@@ -1439,4 +1452,9 @@\n-    call_VM(noreg,\n-            CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n-            lock_reg);\n-\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter_obj),\n+              obj_reg);\n+    } else {\n+      call_VM(noreg,\n+              CAST_FROM_FN_PTR(address, InterpreterRuntime::monitorenter),\n+              lock_reg);\n+    }\n@@ -1464,1 +1482,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -1475,3 +1493,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset()));\n+    }\n@@ -1480,1 +1500,1 @@\n-    movptr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+    movptr(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset()));\n@@ -1483,15 +1503,1 @@\n-    movptr(Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()), NULL_WORD);\n-\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n-\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n-\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, count_locking);\n-\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+    movptr(Address(lock_reg, BasicObjectLock::obj_offset()), NULL_WORD);\n@@ -1499,2 +1505,33 @@\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::notZero, slow_case);\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      Register tmp = swap_reg;\n+      movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+      cmpptr(obj_reg, Address(thread, tmp, Address::times_1,  -oopSize));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlocked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n+\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, count_locking);\n+\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::notZero, slow_case);\n@@ -1502,1 +1539,2 @@\n-    bind(count_locking);\n+      bind(count_locking);\n+    }\n@@ -1508,1 +1546,1 @@\n-    movptr(Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()), obj_reg); \/\/ restore obj\n+    movptr(Address(lock_reg, BasicObjectLock::obj_offset()), obj_reg); \/\/ restore obj\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":131,"deletions":93,"binary":false,"changes":224,"status":"modified"},{"patch":"@@ -98,1 +98,1 @@\n-    movptr(reg, Address(reg, ConstantPool::cache_offset_in_bytes()));\n+    movptr(reg, Address(reg, ConstantPool::cache_offset()));\n@@ -103,1 +103,1 @@\n-    movptr(tags, Address(cpool, ConstantPool::tags_offset_in_bytes()));\n+    movptr(tags, Address(cpool, ConstantPool::tags_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4561,1 +4561,1 @@\n-  int itentry_off = itableMethodEntry::method_offset_in_bytes();\n+  int itentry_off = in_bytes(itableMethodEntry::method_offset());\n@@ -4586,1 +4586,1 @@\n-    movptr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));\n+    movptr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset()));\n@@ -4612,1 +4612,1 @@\n-    movl(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));\n+    movl(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset()));\n@@ -4622,1 +4622,1 @@\n-  const int base = in_bytes(Klass::vtable_start_offset());\n+  const ByteSize base = Klass::vtable_start_offset();\n@@ -4626,1 +4626,1 @@\n-                            base + vtableEntry::method_offset_in_bytes());\n+                            base + vtableEntry::method_offset());\n@@ -5441,1 +5441,1 @@\n-  movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); \/\/ InstanceKlass*\n+  movptr(holder, Address(holder, ConstantPool::pool_holder_offset()));          \/\/ InstanceKlass*\n@@ -10446,0 +10446,67 @@\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be locked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ thread: the thread which attempts to lock obj\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n+  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n+  \/\/ avoids one branch.\n+  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n+  jcc(Assembler::greater, slow);\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lock_mask bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set unlocked_value bit.\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), obj);\n+  incrementl(tmp, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be unlocked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(thread);\n+#endif\n+  subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), 0);\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":73,"deletions":6,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -186,0 +186,2 @@\n+  void increment(Address dst, int value = 1)  { LP64_ONLY(incrementq(dst, value)) NOT_LP64(incrementl(dst, value)) ; }\n+  void decrement(Address dst, int value = 1)  { LP64_ONLY(decrementq(dst, value)) NOT_LP64(decrementl(dst, value)) ; }\n@@ -2085,0 +2087,2 @@\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -267,1 +267,1 @@\n-    __ cmpw(Address(rbx_method, Method::intrinsic_id_offset_in_bytes()), (int) iid);\n+    __ cmpw(Address(rbx_method, Method::intrinsic_id_offset()), (int) iid);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1701,1 +1701,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1733,1 +1735,4 @@\n-      __ jmp(slow_path_lock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      \/\/ Load object header\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n@@ -1858,1 +1863,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -1873,1 +1878,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -1888,1 +1895,5 @@\n-      __ jmp(slow_path_unlock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ dec_held_monitor_count();\n@@ -1929,1 +1940,1 @@\n-  __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);\n+  __ movl(Address(rcx, JNIHandleBlock::top_offset()), NULL_WORD);\n@@ -2253,1 +2264,1 @@\n-  Address unpack_kind(rdi, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes());\n+  Address unpack_kind(rdi, Deoptimization::UnrollBlock::unpack_kind_offset());\n@@ -2299,1 +2310,1 @@\n-  __ addptr(rsp, Address(rdi,Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));\n+  __ addptr(rsp, Address(rdi,Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset()));\n@@ -2305,1 +2316,1 @@\n-  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));\n+  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset()));\n@@ -2311,1 +2322,1 @@\n-  __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));\n+  __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n@@ -2316,1 +2327,1 @@\n-  __ movptr(rcx,Address(rdi,Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+  __ movptr(rcx,Address(rdi,Deoptimization::UnrollBlock::frame_pcs_offset()));\n@@ -2321,1 +2332,1 @@\n-  __ movptr(rsi,Address(rdi,Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));\n+  __ movptr(rsi,Address(rdi,Deoptimization::UnrollBlock::frame_sizes_offset()));\n@@ -2323,1 +2334,1 @@\n-  Address counter(rdi, Deoptimization::UnrollBlock::counter_temp_offset_in_bytes());\n+  Address counter(rdi, Deoptimization::UnrollBlock::counter_temp_offset());\n@@ -2325,1 +2336,1 @@\n-  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));\n+  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset()));\n@@ -2333,1 +2344,1 @@\n-  Address sp_temp(rdi, Deoptimization::UnrollBlock::sender_sp_temp_offset_in_bytes());\n+  Address sp_temp(rdi, Deoptimization::UnrollBlock::sender_sp_temp_offset());\n@@ -2335,1 +2346,1 @@\n-  __ movl2ptr(rbx, Address(rdi, Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes()));\n+  __ movl2ptr(rbx, Address(rdi, Deoptimization::UnrollBlock::caller_adjustment_offset()));\n@@ -2503,1 +2514,1 @@\n-    __ cmpptr(Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()),\n+    __ cmpptr(Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset()),\n@@ -2522,1 +2533,1 @@\n-  __ movl2ptr(rcx, Address(rdi,Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));\n+  __ movl2ptr(rcx, Address(rdi,Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset()));\n@@ -2529,1 +2540,1 @@\n-  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));\n+  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset()));\n@@ -2535,1 +2546,1 @@\n-  __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));\n+  __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n@@ -2540,1 +2551,1 @@\n-  __ movl(rcx,Address(rdi,Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+  __ movl(rcx,Address(rdi,Deoptimization::UnrollBlock::frame_pcs_offset()));\n@@ -2545,1 +2556,1 @@\n-  __ movptr(rsi,Address(rdi,Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));\n+  __ movptr(rsi,Address(rdi,Deoptimization::UnrollBlock::frame_sizes_offset()));\n@@ -2547,1 +2558,1 @@\n-  Address counter(rdi, Deoptimization::UnrollBlock::counter_temp_offset_in_bytes());\n+  Address counter(rdi, Deoptimization::UnrollBlock::counter_temp_offset());\n@@ -2549,1 +2560,1 @@\n-  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));\n+  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset()));\n@@ -2557,1 +2568,1 @@\n-  Address sp_temp(rdi, Deoptimization::UnrollBlock::sender_sp_temp_offset_in_bytes());\n+  Address sp_temp(rdi, Deoptimization::UnrollBlock::sender_sp_temp_offset());\n@@ -2559,1 +2570,1 @@\n-  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes()));\n+  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::caller_adjustment_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":37,"deletions":26,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -2432,2 +2432,3 @@\n-    if (!UseHeavyMonitors) {\n-\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_lock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -2470,1 +2471,4 @@\n-      __ jmp(slow_path_lock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      \/\/ Load object header\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n@@ -2583,1 +2587,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -2598,1 +2602,3 @@\n-    if (!UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n+      __ jmp(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -2610,1 +2616,5 @@\n-      __ jmp(slow_path_unlock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      __ dec_held_monitor_count();\n@@ -2647,1 +2657,1 @@\n-  __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);\n+  __ movl(Address(rcx, JNIHandleBlock::top_offset()), NULL_WORD);\n@@ -3006,1 +3016,1 @@\n-  __ movl(r14, Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));\n+  __ movl(r14, Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset()));\n@@ -3045,1 +3055,1 @@\n-  __ movl(rcx, Address(rdi, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));\n+  __ movl(rcx, Address(rdi, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset()));\n@@ -3052,1 +3062,1 @@\n-  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));\n+  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset()));\n@@ -3058,1 +3068,1 @@\n-  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));\n+  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n@@ -3063,1 +3073,1 @@\n-  __ movptr(rcx, Address(rdi, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+  __ movptr(rcx, Address(rdi, Deoptimization::UnrollBlock::frame_pcs_offset()));\n@@ -3069,1 +3079,1 @@\n-  __ movptr(rsi, Address(rdi, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));\n+  __ movptr(rsi, Address(rdi, Deoptimization::UnrollBlock::frame_sizes_offset()));\n@@ -3072,1 +3082,1 @@\n-  __ movl(rdx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));\n+  __ movl(rdx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset()));\n@@ -3084,1 +3094,1 @@\n-                       caller_adjustment_offset_in_bytes()));\n+                       caller_adjustment_offset()));\n@@ -3225,1 +3235,1 @@\n-    __ cmpptr(Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()),\n+    __ cmpptr(Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset()),\n@@ -3246,1 +3256,1 @@\n-                       size_of_deoptimized_frame_offset_in_bytes()));\n+                       size_of_deoptimized_frame_offset()));\n@@ -3253,1 +3263,1 @@\n-  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));\n+  __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset()));\n@@ -3259,1 +3269,1 @@\n-  __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));\n+  __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n@@ -3264,1 +3274,1 @@\n-  __ movptr(rcx, Address(rdi, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+  __ movptr(rcx, Address(rdi, Deoptimization::UnrollBlock::frame_pcs_offset()));\n@@ -3270,1 +3280,1 @@\n-  __ movptr(rsi, Address(rdi, Deoptimization::UnrollBlock:: frame_sizes_offset_in_bytes()));\n+  __ movptr(rsi, Address(rdi, Deoptimization::UnrollBlock:: frame_sizes_offset()));\n@@ -3273,1 +3283,1 @@\n-  __ movl(rdx, Address(rdi, Deoptimization::UnrollBlock:: number_of_frames_offset_in_bytes())); \/\/ (int)\n+  __ movl(rdx, Address(rdi, Deoptimization::UnrollBlock:: number_of_frames_offset())); \/\/ (int)\n@@ -3283,1 +3293,1 @@\n-  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock:: caller_adjustment_offset_in_bytes())); \/\/ (int)\n+  __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock:: caller_adjustment_offset())); \/\/ (int)\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":33,"deletions":23,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -50,3 +50,0 @@\n-#if INCLUDE_ZGC\n-#include \"gc\/z\/zThreadLocalData.hpp\"\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -615,1 +615,1 @@\n-  __ movptr(Address(rsp, BasicObjectLock::obj_offset_in_bytes()), rax);\n+  __ movptr(Address(rsp, BasicObjectLock::obj_offset()), rax);\n@@ -656,1 +656,1 @@\n-  __ movptr(rdx, Address(rdx, ConstantPool::cache_offset_in_bytes()));\n+  __ movptr(rdx, Address(rdx, ConstantPool::cache_offset()));\n@@ -1163,1 +1163,1 @@\n-  __ movl(Address(t, JNIHandleBlock::top_offset_in_bytes()), NULL_WORD);\n+  __ movl(Address(t, JNIHandleBlock::top_offset()), NULL_WORD);\n@@ -1257,1 +1257,1 @@\n-      __ movptr(t, Address(regmon, BasicObjectLock::obj_offset_in_bytes()));\n+      __ movptr(t, Address(regmon, BasicObjectLock::obj_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -4693,1 +4694,1 @@\n-    __ cmpptr(Address(rtop, BasicObjectLock::obj_offset_in_bytes()), NULL_WORD);\n+    __ cmpptr(Address(rtop, BasicObjectLock::obj_offset()), NULL_WORD);\n@@ -4697,1 +4698,1 @@\n-    __ cmpptr(rax, Address(rtop, BasicObjectLock::obj_offset_in_bytes()));\n+    __ cmpptr(rax, Address(rtop, BasicObjectLock::obj_offset()));\n@@ -4746,1 +4747,1 @@\n-  __ movptr(Address(rmon, BasicObjectLock::obj_offset_in_bytes()), rax);\n+  __ movptr(Address(rmon, BasicObjectLock::obj_offset()), rax);\n@@ -4802,1 +4803,1 @@\n-    __ cmpptr(rax, Address(rtop, BasicObjectLock::obj_offset_in_bytes()));\n+    __ cmpptr(rax, Address(rtop, BasicObjectLock::obj_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1249,4 +1249,0 @@\n-static inline bool is_unsigned_booltest_pred(int bt) {\n-  return  ((bt & BoolTest::unsigned_compare) == BoolTest::unsigned_compare);\n-}\n-\n@@ -7648,1 +7644,1 @@\n-            !is_unsigned_booltest_pred(n->in(2)->get_int()) &&\n+            !Matcher::is_unsigned_booltest_pred(n->in(2)->get_int()) &&\n@@ -7668,1 +7664,1 @@\n-            !is_unsigned_booltest_pred(n->in(2)->get_int()) &&\n+            !Matcher::is_unsigned_booltest_pred(n->in(2)->get_int()) &&\n@@ -7689,1 +7685,1 @@\n-            is_unsigned_booltest_pred(n->in(2)->get_int()) &&\n+            Matcher::is_unsigned_booltest_pred(n->in(2)->get_int()) &&\n@@ -7726,1 +7722,1 @@\n-    bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+    bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);\n@@ -7760,1 +7756,1 @@\n-    bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+    bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);\n@@ -9975,1 +9971,1 @@\n-        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);\n@@ -9981,1 +9977,1 @@\n-        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);\n@@ -9987,1 +9983,1 @@\n-        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);\n@@ -9993,1 +9989,1 @@\n-        bool is_unsigned = is_unsigned_booltest_pred($cond$$constant);\n+        bool is_unsigned = Matcher::is_unsigned_booltest_pred($cond$$constant);\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":9,"deletions":13,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -13760,1 +13760,1 @@\n-instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2) %{\n+instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2, eRegP thread) %{\n@@ -13763,1 +13763,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box, TEMP thread);\n@@ -13767,0 +13767,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13768,1 +13769,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, $thread$$Register,\n@@ -13776,1 +13777,1 @@\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr) %{\n+instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n@@ -13779,1 +13780,1 @@\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n@@ -13783,0 +13784,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13784,1 +13786,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, $thread$$Register, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -3243,1 +3243,1 @@\n-            && is_power_of_2(n->get_int() + 1));\n+            && is_power_of_2((juint)n->get_int() + 1));\n@@ -5398,1 +5398,1 @@\n-  predicate(UseAVX > 0 && !n->is_reduction());\n+  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n@@ -5420,1 +5420,1 @@\n-  predicate(UseAVX > 0 && n->is_reduction());\n+  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n@@ -5434,1 +5434,1 @@\n-  predicate(UseAVX > 0 && !n->is_reduction());\n+  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n@@ -5456,1 +5456,1 @@\n-  predicate(UseAVX > 0 && n->is_reduction());\n+  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n@@ -5470,1 +5470,1 @@\n-  predicate(UseAVX > 0 && !n->is_reduction());\n+  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n@@ -5492,1 +5492,1 @@\n-  predicate(UseAVX > 0 && n->is_reduction());\n+  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n@@ -5506,1 +5506,1 @@\n-  predicate(UseAVX > 0 && !n->is_reduction());\n+  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n@@ -5528,1 +5528,1 @@\n-  predicate(UseAVX > 0 && n->is_reduction());\n+  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n@@ -11057,1 +11057,1 @@\n-  format %{ \"convert_f2i $dst,$src\" %}\n+  format %{ \"convert_f2i $dst, $src\" %}\n@@ -11059,1 +11059,1 @@\n-    __ convert_f2i($dst$$Register, $src$$XMMRegister);\n+    __ convertF2I(T_INT, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n@@ -11068,1 +11068,1 @@\n-  format %{ \"convert_f2l $dst,$src\"%}\n+  format %{ \"convert_f2l $dst, $src\"%}\n@@ -11070,1 +11070,1 @@\n-    __ convert_f2l($dst$$Register, $src$$XMMRegister);\n+    __ convertF2I(T_LONG, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n@@ -11079,1 +11079,1 @@\n-  format %{ \"convert_d2i $dst,$src\"%}\n+  format %{ \"convert_d2i $dst, $src\"%}\n@@ -11081,1 +11081,1 @@\n-    __ convert_d2i($dst$$Register, $src$$XMMRegister);\n+    __ convertF2I(T_INT, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n@@ -11090,1 +11090,1 @@\n-  format %{ \"convert_d2l $dst,$src\"%}\n+  format %{ \"convert_d2l $dst, $src\"%}\n@@ -11092,1 +11092,1 @@\n-    __ convert_d2l($dst$$Register, $src$$XMMRegister);\n+    __ convertF2I(T_LONG, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n@@ -12698,0 +12698,11 @@\n+instruct testI_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src1 src2) zero));\n+\n+  format %{ \"testl   $src1, $src2\" %}\n+  ins_encode %{\n+    __ testl($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n@@ -13014,0 +13025,11 @@\n+instruct testL_reg_reg(rFlagsReg cr, rRegL src1, rRegL src2, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src1 src2) zero));\n+\n+  format %{ \"testq   $src1, $src2\\t# long\" %}\n+  ins_encode %{\n+    __ testq($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n@@ -13540,1 +13562,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -13556,1 +13578,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":41,"deletions":19,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -275,0 +275,1 @@\n+  AD.addInclude(AD._DFA_file, \"opto\/superword.hpp\");\n","filename":"src\/hotspot\/share\/adlc\/main.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -120,8 +120,8 @@\n-    _start         = NULL;\n-    _mark          = NULL;\n-    _end           = NULL;\n-    _limit         = NULL;\n-    _locs_start    = NULL;\n-    _locs_end      = NULL;\n-    _locs_limit    = NULL;\n-    _locs_point    = NULL;\n+    _start         = nullptr;\n+    _mark          = nullptr;\n+    _end           = nullptr;\n+    _limit         = nullptr;\n+    _locs_start    = nullptr;\n+    _locs_end      = nullptr;\n+    _locs_limit    = nullptr;\n+    _locs_point    = nullptr;\n@@ -141,1 +141,1 @@\n-    assert(_start == NULL, \"only one init step, please\");\n+    assert(_start == nullptr, \"only one init step, please\");\n@@ -143,1 +143,1 @@\n-    _mark          = NULL;\n+    _mark          = nullptr;\n@@ -170,1 +170,1 @@\n-  csize_t     mark_off() const      { assert(_mark != NULL, \"not an offset\");\n+  csize_t     mark_off() const      { assert(_mark != nullptr, \"not an offset\");\n@@ -184,1 +184,1 @@\n-  bool        is_allocated() const  { return _start != NULL; }\n+  bool        is_allocated() const  { return _start != nullptr; }\n@@ -186,1 +186,1 @@\n-  bool        has_locs() const      { return _locs_end != NULL; }\n+  bool        has_locs() const      { return _locs_end != nullptr; }\n@@ -190,0 +190,1 @@\n+  void        clear_scratch_emit()  { _scratch_emit = false; }\n@@ -210,1 +211,1 @@\n-  void    clear_mark()              { _mark = NULL; }\n+  void    clear_mark()              { _mark = nullptr; }\n@@ -459,1 +460,1 @@\n-    assert(name != NULL, \"must have a name\");\n+    assert(name != nullptr, \"must have a name\");\n@@ -461,5 +462,5 @@\n-    _before_expand   = NULL;\n-    _blob            = NULL;\n-    _oop_recorder    = NULL;\n-    _overflow_arena  = NULL;\n-    _last_insn       = NULL;\n+    _before_expand   = nullptr;\n+    _blob            = nullptr;\n+    _oop_recorder    = nullptr;\n+    _overflow_arena  = nullptr;\n+    _last_insn       = nullptr;\n@@ -467,2 +468,2 @@\n-    _shared_stub_to_interp_requests = NULL;\n-    _shared_trampoline_requests = NULL;\n+    _shared_stub_to_interp_requests = nullptr;\n+    _shared_trampoline_requests = nullptr;\n@@ -479,1 +480,1 @@\n-    _decode_begin    = NULL;\n+    _decode_begin    = nullptr;\n@@ -534,1 +535,1 @@\n-    assert(code_start != NULL, \"sanity\");\n+    assert(code_start != nullptr, \"sanity\");\n@@ -576,1 +577,1 @@\n-  \/\/ present sections in order; return NULL at end; consts is #0, etc.\n+  \/\/ present sections in order; return null at end; consts is #0, etc.\n@@ -601,1 +602,1 @@\n-    if (locator < 0)  return NULL;\n+    if (locator < 0)  return nullptr;\n@@ -665,1 +666,1 @@\n-    return (recorder == NULL)? 0: recorder->oop_size();\n+    return (recorder == nullptr)? 0: recorder->oop_size();\n@@ -671,1 +672,1 @@\n-    return (recorder == NULL)? 0: recorder->metadata_size();\n+    return (recorder == nullptr)? 0: recorder->metadata_size();\n@@ -686,1 +687,1 @@\n-  void clear_last_insn() { set_last_insn(NULL); }\n+  void clear_last_insn() { set_last_insn(nullptr); }\n@@ -711,1 +712,1 @@\n-    assert(blob != NULL, \"sane\");\n+    assert(blob != nullptr, \"sane\");\n@@ -722,1 +723,1 @@\n-  const char* code_string(const char* str) PRODUCT_RETURN_(return NULL;);\n+  const char* code_string(const char* str) PRODUCT_RETURN_(return nullptr;);\n@@ -762,1 +763,1 @@\n-  SharedStubToInterpRequest(ciMethod* method = NULL, CodeBuffer::csize_t call_offset = -1) : _shared_method(method),\n+  SharedStubToInterpRequest(ciMethod* method = nullptr, CodeBuffer::csize_t call_offset = -1) : _shared_method(method),\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":33,"deletions":32,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,1 @@\n-  assert(x != NULL, \"value must exist\");\n+  assert(x != nullptr, \"value must exist\");\n@@ -92,3 +92,3 @@\n-            case Bytecodes::_iadd: set_constant(a + b); return;\n-            case Bytecodes::_isub: set_constant(a - b); return;\n-            case Bytecodes::_imul: set_constant(a * b); return;\n+            case Bytecodes::_iadd: set_constant(java_add(a, b)); return;\n+            case Bytecodes::_isub: set_constant(java_subtract(a, b)); return;\n+            case Bytecodes::_imul: set_constant(java_multiply(a, b)); return;\n@@ -203,1 +203,1 @@\n-  while (max_distance > 0 && v != NULL && v->as_BlockEnd() == NULL) {\n+  while (max_distance > 0 && v != nullptr && v->as_BlockEnd() == nullptr) {\n@@ -207,1 +207,1 @@\n-  return v == NULL;\n+  return v == nullptr;\n@@ -216,1 +216,1 @@\n-    Value value = NULL;\n+    Value value = nullptr;\n@@ -225,1 +225,1 @@\n-    if (value != NULL && in_current_block(conv)) {\n+    if (value != nullptr && in_current_block(conv)) {\n@@ -239,1 +239,1 @@\n-  if ((na = x->array()->as_NewArray()) != NULL) {\n+  if ((na = x->array()->as_NewArray()) != nullptr) {\n@@ -246,3 +246,3 @@\n-    if (na->length() != NULL &&\n-        (length = na->length()->as_Constant()) != NULL) {\n-      assert(length->type()->as_IntConstant() != NULL, \"array length must be integer\");\n+    if (na->length() != nullptr &&\n+        (length = na->length()->as_Constant()) != nullptr) {\n+      assert(length->type()->as_IntConstant() != nullptr, \"array length must be integer\");\n@@ -250,3 +250,3 @@\n-    } else if ((nma = x->array()->as_NewMultiArray()) != NULL &&\n-               (length = nma->dims()->at(0)->as_Constant()) != NULL) {\n-      assert(length->type()->as_IntConstant() != NULL, \"array length must be integer\");\n+    } else if ((nma = x->array()->as_NewMultiArray()) != nullptr &&\n+               (length = nma->dims()->at(0)->as_Constant()) != nullptr) {\n+      assert(length->type()->as_IntConstant() != nullptr, \"array length must be integer\");\n@@ -256,1 +256,1 @@\n-  } else if ((ct = x->array()->as_Constant()) != NULL) {\n+  } else if ((ct = x->array()->as_Constant()) != nullptr) {\n@@ -259,1 +259,1 @@\n-    if (cnst != NULL) {\n+    if (cnst != nullptr) {\n@@ -263,1 +263,1 @@\n-  } else if ((lf = x->array()->as_LoadField()) != NULL) {\n+  } else if ((lf = x->array()->as_LoadField()) != nullptr) {\n@@ -281,1 +281,1 @@\n-  assert(array == NULL || FoldStableValues, \"not enabled\");\n+  assert(array == nullptr || FoldStableValues, \"not enabled\");\n@@ -284,1 +284,1 @@\n-  if (!x->should_profile() && !x->mismatched() && array != NULL && index != NULL) {\n+  if (!x->should_profile() && !x->mismatched() && array != nullptr && index != nullptr) {\n@@ -295,1 +295,1 @@\n-      ValueType* value = NULL;\n+      ValueType* value = nullptr;\n@@ -315,1 +315,1 @@\n-    Value value = NULL;\n+    Value value = nullptr;\n@@ -324,1 +324,1 @@\n-    if (value != NULL && in_current_block(conv)) {\n+    if (value != nullptr && in_current_block(conv)) {\n@@ -338,3 +338,3 @@\n-      case intTag   : set_constant(-t->as_IntConstant   ()->value()); return;\n-      case longTag  : set_constant(-t->as_LongConstant  ()->value()); return;\n-      case floatTag : set_constant(-t->as_FloatConstant ()->value()); return;\n+      case intTag   : set_constant(java_negate(t->as_IntConstant()->value())); return;\n+      case longTag  : set_constant(java_negate(t->as_LongConstant()->value())); return;\n+      case floatTag : set_constant(-t->as_FloatConstant()->value()); return;\n@@ -482,1 +482,1 @@\n-    if (c != NULL) {\n+    if (c != nullptr) {\n@@ -491,1 +491,1 @@\n-    if (c != NULL) {\n+    if (c != nullptr) {\n@@ -500,1 +500,1 @@\n-    if (c != NULL) {\n+    if (c != nullptr) {\n@@ -509,1 +509,1 @@\n-    if (c != NULL) {\n+    if (c != nullptr) {\n@@ -520,2 +520,2 @@\n-    if (c != NULL && !c->value()->is_null_object()) {\n-      \/\/ ciInstance::java_mirror_type() returns non-NULL only for Java mirrors\n+    if (c != nullptr && !c->value()->is_null_object()) {\n+      \/\/ ciInstance::java_mirror_type() returns non-null only for Java mirrors\n@@ -543,1 +543,1 @@\n-    if (c != NULL && !c->value()->is_null_object()) {\n+    if (c != nullptr && !c->value()->is_null_object()) {\n@@ -554,1 +554,1 @@\n-    if (c != NULL && !c->value()->is_null_object()) {\n+    if (c != nullptr && !c->value()->is_null_object()) {\n@@ -643,1 +643,1 @@\n-  if (x->obj()->as_NewArray() != NULL || x->obj()->as_NewInstance() != NULL || x->obj()->as_NewInlineTypeInstance()) {\n+  if (x->obj()->as_NewArray() != nullptr || x->obj()->as_NewInstance() != nullptr || x->obj()->as_NewInlineTypeInstance()) {\n@@ -671,1 +671,1 @@\n-    if (klass == NULL) {\n+    if (klass == nullptr) {\n@@ -674,1 +674,1 @@\n-    if (klass != NULL && klass->is_loaded()) {\n+    if (klass != nullptr && klass->is_loaded()) {\n@@ -695,1 +695,1 @@\n-    if (exact != NULL && exact->is_loaded() && (obj->as_NewInstance() || obj->as_NewArray() || obj->as_NewInlineTypeInstance())) {\n+    if (exact != nullptr && exact->is_loaded() && (obj->as_NewInstance() || obj->as_NewArray() || obj->as_NewInlineTypeInstance())) {\n@@ -743,1 +743,1 @@\n-    BlockBegin* sux = NULL;\n+    BlockBegin* sux = nullptr;\n@@ -759,1 +759,1 @@\n-    if (x->x()->as_Constant() != NULL) {\n+    if (x->x()->as_Constant() != nullptr) {\n@@ -764,1 +764,1 @@\n-      if (sux != NULL) {\n+      if (sux != nullptr) {\n@@ -769,1 +769,1 @@\n-  } else if (rt->as_IntConstant() != NULL) {\n+  } else if (rt->as_IntConstant() != nullptr) {\n@@ -772,1 +772,1 @@\n-    if (l->as_CompareOp() != NULL) {\n+    if (l->as_CompareOp() != nullptr) {\n@@ -790,2 +790,2 @@\n-        BlockBegin* tsux = NULL;\n-        BlockBegin* fsux = NULL;\n+        BlockBegin* tsux = nullptr;\n+        BlockBegin* fsux = nullptr;\n","filename":"src\/hotspot\/share\/c1\/c1_Canonicalizer.cpp","additions":46,"deletions":46,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-  virtual CodeEmitInfo* info() const             { return NULL; }\n+  virtual CodeEmitInfo* info() const             { return nullptr; }\n@@ -173,1 +173,1 @@\n-    assert(info != NULL, \"must have info\");\n+    assert(info != nullptr, \"must have info\");\n@@ -181,1 +181,1 @@\n-    assert(info != NULL, \"must have info\");\n+    assert(info != nullptr, \"must have info\");\n@@ -514,1 +514,1 @@\n-    , _info(NULL)\n+    , _info(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -86,1 +86,1 @@\n-    _log(NULL), _timer(timer)\n+    _log(nullptr), _timer(timer)\n@@ -88,1 +88,1 @@\n-    if (Compilation::current() != NULL) {\n+    if (Compilation::current() != nullptr) {\n@@ -92,1 +92,1 @@\n-    if (_log != NULL) {\n+    if (_log != nullptr) {\n@@ -100,1 +100,1 @@\n-    if (_log != NULL)\n+    if (_log != nullptr)\n@@ -111,1 +111,1 @@\n-  if (_current_instruction != NULL && _last_instruction_printed != _current_instruction) {\n+  if (_current_instruction != nullptr && _last_instruction_printed != _current_instruction) {\n@@ -145,1 +145,1 @@\n-  if (log != NULL) {\n+  if (log != nullptr) {\n@@ -215,1 +215,1 @@\n-    if (_hir->osr_entry() == NULL) {\n+    if (_hir->osr_entry() == nullptr) {\n@@ -485,1 +485,1 @@\n-  if (log() != NULL) \/\/ Print code cache state into compiler log\n+  if (log() != nullptr) \/\/ Print code cache state into compiler log\n@@ -564,1 +564,1 @@\n-, _hir(NULL)\n+, _hir(nullptr)\n@@ -566,2 +566,2 @@\n-, _frame_map(NULL)\n-, _masm(NULL)\n+, _frame_map(nullptr)\n+, _masm(nullptr)\n@@ -577,3 +577,3 @@\n-, _bailout_msg(NULL)\n-, _exception_info_list(NULL)\n-, _allocator(NULL)\n+, _bailout_msg(nullptr)\n+, _exception_info_list(nullptr)\n+, _allocator(nullptr)\n@@ -585,1 +585,1 @@\n-, _current_instruction(NULL)\n+, _current_instruction(nullptr)\n@@ -587,2 +587,2 @@\n-, _last_instruction_printed(NULL)\n-, _cfg_printer_output(NULL)\n+, _last_instruction_printed(nullptr)\n+, _cfg_printer_output(nullptr)\n@@ -616,1 +616,1 @@\n-    if (md != NULL) {\n+    if (md != nullptr) {\n@@ -626,1 +626,1 @@\n-  _env->set_compiler_data(NULL);\n+  _env->set_compiler_data(nullptr);\n@@ -646,1 +646,1 @@\n-  assert(msg != NULL, \"bailout message must exist\");\n+  assert(msg != nullptr, \"bailout message must exist\");\n@@ -655,1 +655,1 @@\n-  if (type != NULL && type->is_loaded() && type->is_instance_klass()) {\n+  if (type != nullptr && type->is_loaded() && type->is_instance_klass()) {\n@@ -657,1 +657,1 @@\n-    assert(ik->exact_klass() == NULL, \"no cha for final klass\");\n+    assert(ik->exact_klass() == nullptr, \"no cha for final klass\");\n@@ -663,1 +663,1 @@\n-  return NULL;\n+  return nullptr;\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":23,"deletions":23,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -203,1 +203,1 @@\n-    guarantee(_cfg_printer_output != NULL, \"CFG printer output not initialized\");\n+    guarantee(_cfg_printer_output != nullptr, \"CFG printer output not initialized\");\n@@ -210,1 +210,1 @@\n-  bool bailed_out() const                        { return _bailout_msg != NULL; }\n+  bool bailed_out() const                        { return _bailout_msg != nullptr; }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -121,1 +121,1 @@\n-  intptr_t out_preserve = SharedRuntime::c_calling_convention(sig_bt, regs, NULL, sizeargs);\n+  intptr_t out_preserve = SharedRuntime::c_calling_convention(sig_bt, regs, nullptr, sizeargs);\n@@ -242,1 +242,1 @@\n-  if (second != NULL) {\n+  if (second != nullptr) {\n@@ -293,1 +293,1 @@\n-  return sp_offset_for_monitor_base(index) + in_ByteSize(BasicObjectLock::lock_offset_in_bytes());;\n+  return sp_offset_for_monitor_base(index) + BasicObjectLock::lock_offset();\n@@ -298,1 +298,1 @@\n-  return sp_offset_for_monitor_base(index) + in_ByteSize(BasicObjectLock::obj_offset_in_bytes());\n+  return sp_offset_for_monitor_base(index) + BasicObjectLock::obj_offset();\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -232,1 +232,1 @@\n-                            Location* loc, Location* second = NULL) const;\n+                            Location* loc, Location* second = nullptr) const;\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n- , _bci2block(new BlockList(scope->method()->code_size(), NULL))\n+ , _bci2block(new BlockList(scope->method()->code_size(), nullptr))\n@@ -153,2 +153,2 @@\n-  BlockBegin* std_entry = make_block_at(0, NULL);\n-  if (scope()->caller() == NULL) {\n+  BlockBegin* std_entry = make_block_at(0, nullptr);\n+  if (scope()->caller() == nullptr) {\n@@ -158,1 +158,1 @@\n-    BlockBegin* osr_entry = make_block_at(osr_bci, NULL);\n+    BlockBegin* osr_entry = make_block_at(osr_bci, nullptr);\n@@ -167,1 +167,1 @@\n-    BlockBegin* entry = make_block_at(h->handler_bci(), NULL);\n+    BlockBegin* entry = make_block_at(h->handler_bci(), nullptr);\n@@ -178,1 +178,1 @@\n-  if (block == NULL) {\n+  if (block == nullptr) {\n@@ -185,1 +185,1 @@\n-    assert(predecessor == NULL || predecessor->bci() < cur_bci, \"targets for backward branches must already exist\");\n+    assert(predecessor == nullptr || predecessor->bci() < cur_bci, \"targets for backward branches must already exist\");\n@@ -188,1 +188,1 @@\n-  if (predecessor != NULL) {\n+  if (predecessor != nullptr) {\n@@ -220,1 +220,1 @@\n-      assert(entry != NULL && entry == _bci2block->at(h->handler_bci()), \"entry must be set\");\n+      assert(entry != nullptr && entry == _bci2block->at(h->handler_bci()), \"entry must be set\");\n@@ -251,1 +251,1 @@\n-  BlockBegin* current = NULL;\n+  BlockBegin* current = nullptr;\n@@ -268,1 +268,1 @@\n-    assert(current != NULL, \"must have current block\");\n+    assert(current != nullptr, \"must have current block\");\n@@ -312,1 +312,1 @@\n-        current = NULL;\n+        current = nullptr;\n@@ -335,1 +335,1 @@\n-        current = NULL;\n+        current = nullptr;\n@@ -340,1 +340,1 @@\n-        current = NULL;\n+        current = nullptr;\n@@ -345,1 +345,1 @@\n-        current = NULL;\n+        current = nullptr;\n@@ -350,1 +350,1 @@\n-        current = NULL;\n+        current = nullptr;\n@@ -355,1 +355,1 @@\n-        current = NULL;\n+        current = nullptr;\n@@ -366,1 +366,1 @@\n-        current = NULL;\n+        current = nullptr;\n@@ -378,1 +378,1 @@\n-        current = NULL;\n+        current = nullptr;\n@@ -581,1 +581,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -588,1 +588,1 @@\n-    _values.at_put_grow(offset, value, NULL);\n+    _values.at_put_grow(offset, value, nullptr);\n@@ -633,1 +633,1 @@\n-        if (buf->at(field) == NULL && is_default_value(value)) {\n+        if (buf->at(field) == nullptr && is_default_value(value)) {\n@@ -640,1 +640,1 @@\n-          return NULL;\n+          return nullptr;\n@@ -645,1 +645,1 @@\n-        _objects.at_put_grow(offset, object, NULL);\n+        _objects.at_put_grow(offset, object, nullptr);\n@@ -697,1 +697,1 @@\n-      Value result = NULL;\n+      Value result = nullptr;\n@@ -701,1 +701,1 @@\n-      } else if (_objects.at_grow(offset, NULL) == object) {\n+      } else if (_objects.at_grow(offset, nullptr) == object) {\n@@ -704,1 +704,1 @@\n-      if (result != NULL) {\n+      if (result != nullptr) {\n@@ -722,1 +722,1 @@\n-    if (_fields.at_grow(index, NULL) == NULL) {\n+    if (_fields.at_grow(index, nullptr) == nullptr) {\n@@ -733,1 +733,1 @@\n-    if (_fields.at_grow(index, NULL) == NULL) {\n+    if (_fields.at_grow(index, nullptr) == nullptr) {\n@@ -767,2 +767,2 @@\n-  , _bci2block(NULL)\n-  , _scope(NULL)\n+  , _bci2block(nullptr)\n+  , _scope(nullptr)\n@@ -770,2 +770,2 @@\n-  , _stream(NULL)\n-  , _work_list(NULL)\n+  , _stream(nullptr)\n+  , _work_list(nullptr)\n@@ -773,1 +773,1 @@\n-  , _continuation(NULL)\n+  , _continuation(nullptr)\n@@ -775,1 +775,1 @@\n-  , _jsr_xhandlers(NULL)\n+  , _jsr_xhandlers(nullptr)\n@@ -777,3 +777,3 @@\n-  , _cleanup_block(NULL)\n-  , _cleanup_return_prev(NULL)\n-  , _cleanup_state(NULL)\n+  , _cleanup_block(nullptr)\n+  , _cleanup_return_prev(nullptr)\n+  , _cleanup_state(nullptr)\n@@ -782,1 +782,1 @@\n-  if (parent != NULL) {\n+  if (parent != nullptr) {\n@@ -808,1 +808,1 @@\n-    if (block != NULL && block == parent()->bci2block()->at(bci)) {\n+    if (block != nullptr && block == parent()->bci2block()->at(bci)) {\n@@ -839,1 +839,1 @@\n-  if (_jsr_xhandlers == NULL) {\n+  if (_jsr_xhandlers == nullptr) {\n@@ -851,1 +851,1 @@\n-  if (parent() != NULL) {\n+  if (parent() != nullptr) {\n@@ -868,1 +868,1 @@\n-  if (_work_list == NULL) {\n+  if (_work_list == nullptr) {\n@@ -913,1 +913,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -920,1 +920,1 @@\n-  return (_work_list == NULL || _work_list->length() == 0);\n+  return (_work_list == nullptr || _work_list->length() == 0);\n@@ -969,1 +969,1 @@\n-    ValueStack* patch_state = NULL;\n+    ValueStack* patch_state = nullptr;\n@@ -1002,1 +1002,1 @@\n-    if (patch_state != NULL) {\n+    if (patch_state != nullptr) {\n@@ -1013,1 +1013,1 @@\n-    if (patch_state != NULL) {\n+    if (patch_state != nullptr) {\n@@ -1037,1 +1037,1 @@\n-  assert(x != NULL && !x->type()->is_illegal(), \"access of illegal local variable\");\n+  assert(x != nullptr && !x->type()->is_illegal(), \"access of illegal local variable\");\n@@ -1039,1 +1039,1 @@\n-  if (x->as_NewInlineTypeInstance() != NULL && x->as_NewInlineTypeInstance()->in_larval_state()) {\n+  if (x->as_NewInlineTypeInstance() != nullptr && x->as_NewInlineTypeInstance()->in_larval_state()) {\n@@ -1052,1 +1052,1 @@\n-  if (x->as_NewInlineTypeInstance() != NULL) {\n+  if (x->as_NewInlineTypeInstance() != nullptr) {\n@@ -1071,1 +1071,1 @@\n-           cur_scope_data != NULL && cur_scope_data->parsing_jsr() && cur_scope_data->scope() == scope();\n+           cur_scope_data != nullptr && cur_scope_data->parsing_jsr() && cur_scope_data->scope() == scope();\n@@ -1083,1 +1083,1 @@\n-  if (x->as_NewInlineTypeInstance() != NULL) {\n+  if (x->as_NewInlineTypeInstance() != nullptr) {\n@@ -1091,1 +1091,1 @@\n-  ValueStack* state_before = NULL;\n+  ValueStack* state_before = nullptr;\n@@ -1103,1 +1103,1 @@\n-  Value length = NULL;\n+  Value length = nullptr;\n@@ -1105,1 +1105,1 @@\n-      (array->as_Constant() != NULL) ||\n+      (array->as_Constant() != nullptr) ||\n@@ -1113,2 +1113,2 @@\n-  LoadIndexed* load_indexed = NULL;\n-  Instruction* result = NULL;\n+  LoadIndexed* load_indexed = nullptr;\n+  Instruction* result = nullptr;\n@@ -1177,1 +1177,1 @@\n-  ValueStack* state_before = NULL;\n+  ValueStack* state_before = nullptr;\n@@ -1190,1 +1190,1 @@\n-  Value length = NULL;\n+  Value length = nullptr;\n@@ -1192,1 +1192,1 @@\n-      (array->as_Constant() != NULL) ||\n+      (array->as_Constant() != nullptr) ||\n@@ -1200,1 +1200,1 @@\n-  if (array_type != NULL) {\n+  if (array_type != nullptr) {\n@@ -1258,1 +1258,1 @@\n-        if (w1 != NULL && w1->as_NewInlineTypeInstance() != NULL) {\n+        if (w1 != nullptr && w1->as_NewInlineTypeInstance() != nullptr) {\n@@ -1352,1 +1352,1 @@\n-    if (s1 != NULL) {\n+    if (s1 != nullptr) {\n@@ -1355,1 +1355,1 @@\n-      if (l != NULL && l->op() == Bytecodes::_ishl) {\n+      if (l != nullptr && l->op() == Bytecodes::_ishl) {\n@@ -1358,1 +1358,1 @@\n-        if (s0 != NULL) {\n+        if (s0 != nullptr) {\n@@ -1442,1 +1442,1 @@\n-      if (left_klass == NULL || right_klass == NULL) {\n+      if (left_klass == nullptr || right_klass == nullptr) {\n@@ -1461,1 +1461,1 @@\n-  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : NULL, is_bb, subst_check));\n+  Instruction *i = append(new If(x, cond, false, y, tsux, fsux, (is_bb || compilation()->is_optimistic() || subst_check) ? state_before : nullptr, is_bb, subst_check));\n@@ -1463,1 +1463,1 @@\n-  assert(i->as_Goto() == NULL ||\n+  assert(i->as_Goto() == nullptr ||\n@@ -1470,1 +1470,1 @@\n-    if (if_node != NULL) {\n+    if (if_node != nullptr) {\n@@ -1488,1 +1488,1 @@\n-    if (goto_node != NULL) {\n+    if (goto_node != nullptr) {\n@@ -1539,1 +1539,1 @@\n-       cur_scope_data != NULL && cur_scope_data->parsing_jsr() && cur_scope_data->scope() == scope();\n+       cur_scope_data != nullptr && cur_scope_data->parsing_jsr() && cur_scope_data->scope() == scope();\n@@ -1582,1 +1582,1 @@\n-    BlockList* sux = new BlockList(l + 1, NULL);\n+    BlockList* sux = new BlockList(l + 1, nullptr);\n@@ -1628,1 +1628,1 @@\n-    BlockList* sux = new BlockList(l + 1, NULL);\n+    BlockList* sux = new BlockList(l + 1, nullptr);\n@@ -1663,1 +1663,1 @@\n-  assert(receiver != NULL, \"must have a receiver\");\n+  assert(receiver != nullptr, \"must have a receiver\");\n@@ -1666,1 +1666,1 @@\n-  if (exact_type == NULL &&\n+  if (exact_type == nullptr &&\n@@ -1683,1 +1683,1 @@\n-  if (exact_type != NULL) {\n+  if (exact_type != nullptr) {\n@@ -1685,1 +1685,1 @@\n-  } else if (declared_type != NULL) {\n+  } else if (declared_type != nullptr) {\n@@ -1753,1 +1753,1 @@\n-  if (continuation() != NULL) {\n+  if (continuation() != nullptr) {\n@@ -1757,1 +1757,1 @@\n-    if (x != NULL  && !ignore_return) {\n+    if (x != nullptr  && !ignore_return) {\n@@ -1762,1 +1762,1 @@\n-        if (declared_ret_type->is_klass() && x->exact_type() == NULL &&\n+        if (declared_ret_type->is_klass() && x->exact_type() == nullptr &&\n@@ -1793,1 +1793,1 @@\n-    if (x != NULL) {\n+    if (x != nullptr) {\n@@ -1838,1 +1838,1 @@\n-  if (!field_value.is_valid())  return NULL;\n+  if (!field_value.is_valid())  return nullptr;\n@@ -1857,1 +1857,1 @@\n-      return NULL; \/\/ Not a constant.\n+      return nullptr; \/\/ Not a constant.\n@@ -1888,1 +1888,1 @@\n-  ValueStack* state_before = NULL;\n+  ValueStack* state_before = nullptr;\n@@ -1895,1 +1895,1 @@\n-  Value obj = NULL;\n+  Value obj = nullptr;\n@@ -1897,1 +1897,1 @@\n-    if (state_before != NULL) {\n+    if (state_before != nullptr) {\n@@ -1920,1 +1920,1 @@\n-      Value constant = NULL;\n+      Value constant = nullptr;\n@@ -1931,1 +1931,1 @@\n-      if (constant != NULL) {\n+      if (constant != nullptr) {\n@@ -1934,1 +1934,1 @@\n-        if (state_before == NULL) {\n+        if (state_before == nullptr) {\n@@ -1945,1 +1945,1 @@\n-      if (state_before == NULL) {\n+      if (state_before == nullptr) {\n@@ -1961,2 +1961,2 @@\n-      Value constant = NULL;\n-      if (state_before == NULL && field->is_flattened()) {\n+      Value constant = nullptr;\n+      if (state_before == nullptr && field->is_flattened()) {\n@@ -1998,1 +1998,1 @@\n-      if (constant != NULL) {\n+      if (constant != nullptr) {\n@@ -2001,1 +2001,1 @@\n-        if (state_before == NULL) {\n+        if (state_before == nullptr) {\n@@ -2010,2 +2010,2 @@\n-            assert(field != NULL, \"field not found\");\n-            set_pending_field_access(NULL);\n+            assert(field != nullptr, \"field not found\");\n+            set_pending_field_access(nullptr);\n@@ -2018,1 +2018,1 @@\n-            set_pending_load_indexed(NULL);\n+            set_pending_load_indexed(nullptr);\n@@ -2076,1 +2076,1 @@\n-                set_pending_field_access(NULL);\n+                set_pending_field_access(nullptr);\n@@ -2078,1 +2078,1 @@\n-                set_pending_load_indexed(NULL);\n+                set_pending_load_indexed(nullptr);\n@@ -2088,1 +2088,1 @@\n-              set_pending_load_indexed(NULL);\n+              set_pending_load_indexed(nullptr);\n@@ -2099,1 +2099,1 @@\n-                set_pending_field_access(NULL);\n+                set_pending_field_access(nullptr);\n@@ -2119,1 +2119,1 @@\n-      if (state_before == NULL) {\n+      if (state_before == nullptr) {\n@@ -2132,1 +2132,1 @@\n-        if (store != NULL) {\n+        if (store != nullptr) {\n@@ -2177,1 +2177,1 @@\n-  if (obj->as_NewInlineTypeInstance() != NULL && obj->as_NewInlineTypeInstance()->in_larval_state()) {\n+  if (obj->as_NewInlineTypeInstance() != nullptr && obj->as_NewInlineTypeInstance()->in_larval_state()) {\n@@ -2235,1 +2235,1 @@\n-    if (data != NULL && (data->is_CallTypeData() || data->is_VirtualCallTypeData())) {\n+    if (data != nullptr && (data->is_CallTypeData() || data->is_VirtualCallTypeData())) {\n@@ -2240,2 +2240,2 @@\n-  if (profile_parameters() && target != NULL) {\n-    if (target->method_data() != NULL && target->method_data()->parameters_type_data() != NULL) {\n+  if (profile_parameters() && target != nullptr) {\n+    if (target->method_data() != nullptr && target->method_data()->parameters_type_data() != nullptr) {\n@@ -2251,1 +2251,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -2257,1 +2257,1 @@\n-  ciSignature* declared_signature = NULL;\n+  ciSignature* declared_signature = nullptr;\n@@ -2267,2 +2267,2 @@\n-  if (obj_args == NULL) {\n-    return NULL;\n+  if (obj_args == nullptr) {\n+    return nullptr;\n@@ -2284,1 +2284,1 @@\n-  ciSignature* declared_signature = NULL;\n+  ciSignature* declared_signature = nullptr;\n@@ -2288,1 +2288,1 @@\n-  assert(declared_signature != NULL, \"cannot be null\");\n+  assert(declared_signature != nullptr, \"cannot be null\");\n@@ -2301,1 +2301,1 @@\n-  if (log != NULL)\n+  if (log != nullptr)\n@@ -2367,3 +2367,3 @@\n-  ciMethod* cha_monomorphic_target = NULL;\n-  ciMethod* exact_target = NULL;\n-  Value better_receiver = NULL;\n+  ciMethod* cha_monomorphic_target = nullptr;\n+  ciMethod* exact_target = nullptr;\n+  Value better_receiver = nullptr;\n@@ -2375,2 +2375,2 @@\n-    Value receiver = NULL;\n-    ciInstanceKlass* receiver_klass = NULL;\n+    Value receiver = nullptr;\n+    ciInstanceKlass* receiver_klass = nullptr;\n@@ -2383,1 +2383,1 @@\n-      if (type != NULL && type->is_loaded() &&\n+      if (type != nullptr && type->is_loaded() &&\n@@ -2388,1 +2388,1 @@\n-      if (type == NULL) {\n+      if (type == nullptr) {\n@@ -2390,1 +2390,1 @@\n-        if (type != NULL && type->is_loaded() &&\n+        if (type != nullptr && type->is_loaded() &&\n@@ -2402,1 +2402,1 @@\n-    if (receiver_klass != NULL && type_is_exact &&\n+    if (receiver_klass != nullptr && type_is_exact &&\n@@ -2407,1 +2407,1 @@\n-      if (exact_target != NULL) {\n+      if (exact_target != nullptr) {\n@@ -2412,1 +2412,1 @@\n-    if (receiver_klass != NULL &&\n+    if (receiver_klass != nullptr &&\n@@ -2422,1 +2422,1 @@\n-    } else if (code == Bytecodes::_invokeinterface && callee_holder->is_loaded() && receiver != NULL) {\n+    } else if (code == Bytecodes::_invokeinterface && callee_holder->is_loaded() && receiver != nullptr) {\n@@ -2440,1 +2440,1 @@\n-      if (singleton != NULL) {\n+      if (singleton != nullptr) {\n@@ -2443,1 +2443,1 @@\n-        if (cha_monomorphic_target != NULL) {\n+        if (cha_monomorphic_target != nullptr) {\n@@ -2459,1 +2459,1 @@\n-            cha_monomorphic_target = NULL; \/\/ subtype check against Object is useless\n+            cha_monomorphic_target = nullptr; \/\/ subtype check against Object is useless\n@@ -2466,1 +2466,1 @@\n-  if (cha_monomorphic_target != NULL) {\n+  if (cha_monomorphic_target != nullptr) {\n@@ -2490,2 +2490,2 @@\n-      ciMethod* inline_target = (cha_monomorphic_target != NULL) ? cha_monomorphic_target : target;\n-      bool holder_known = (cha_monomorphic_target != NULL) || (exact_target != NULL);\n+      ciMethod* inline_target = (cha_monomorphic_target != nullptr) ? cha_monomorphic_target : target;\n+      bool holder_known = (cha_monomorphic_target != nullptr) || (exact_target != nullptr);\n@@ -2534,1 +2534,1 @@\n-  Value recv = has_receiver ? apop() : NULL;\n+  Value recv = has_receiver ? apop() : nullptr;\n@@ -2553,1 +2553,1 @@\n-    if (recv != NULL) {\n+    if (recv != nullptr) {\n@@ -2562,3 +2562,3 @@\n-        assert(cha_monomorphic_target == NULL || exact_target == NULL, \"both can not be set\");\n-        ciKlass* target_klass = NULL;\n-        if (cha_monomorphic_target != NULL) {\n+        assert(cha_monomorphic_target == nullptr || exact_target == nullptr, \"both can not be set\");\n+        ciKlass* target_klass = nullptr;\n+        if (cha_monomorphic_target != nullptr) {\n@@ -2566,1 +2566,1 @@\n-        } else if (exact_target != NULL) {\n+        } else if (exact_target != nullptr) {\n@@ -2569,1 +2569,1 @@\n-        profile_call(target, recv, target_klass, collect_args_for_profiling(args, NULL, false), false);\n+        profile_call(target, recv, target_klass, collect_args_for_profiling(args, nullptr, false), false);\n@@ -2688,1 +2688,1 @@\n-    assert(obj_type == NULL || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n+    assert(obj_type == nullptr || !obj_type->is_inlinetype(), \"inline types cannot have synchronized methods\");\n@@ -2694,1 +2694,1 @@\n-      if (obj_type == NULL || obj_type->as_klass()->can_be_inline_klass()) {\n+      if (obj_type == nullptr || obj_type->as_klass()->can_be_inline_klass()) {\n@@ -2720,1 +2720,1 @@\n-  Values* dims = new Values(dimensions, dimensions, NULL);\n+  Values* dims = new Values(dimensions, dimensions, nullptr);\n@@ -2750,3 +2750,3 @@\n-          fp_value->as_Constant() == NULL &&\n-          fp_value->as_Local() == NULL &&       \/\/ method parameters need no rounding\n-          fp_value->as_RoundFP() == NULL) {\n+          fp_value->as_Constant() == nullptr &&\n+          fp_value->as_Local() == nullptr &&       \/\/ method parameters need no rounding\n+          fp_value->as_RoundFP() == nullptr) {\n@@ -2787,1 +2787,1 @@\n-  assert(i1->next() == NULL, \"shouldn't already be linked\");\n+  assert(i1->next() == nullptr, \"shouldn't already be linked\");\n@@ -2809,1 +2809,1 @@\n-  if (s != NULL) {\n+  if (s != nullptr) {\n@@ -2812,1 +2812,1 @@\n-      if (s->as_Invoke() != NULL || (intrinsic && !intrinsic->preserves_state())) {\n+      if (s->as_Invoke() != nullptr || (intrinsic && !intrinsic->preserves_state())) {\n@@ -2822,1 +2822,1 @@\n-    assert(i1->exception_state() != NULL || !i1->needs_exception_state() || bailed_out(), \"handle_exception must set exception state\");\n+    assert(i1->exception_state() != nullptr || !i1->needs_exception_state() || bailed_out(), \"handle_exception must set exception state\");\n@@ -2829,1 +2829,1 @@\n-  assert(instr->as_StateSplit() == NULL || instr->as_BlockEnd() != NULL, \"wrong append used\");\n+  assert(instr->as_StateSplit() == nullptr || instr->as_BlockEnd() != nullptr, \"wrong append used\");\n@@ -2840,1 +2840,1 @@\n-  if (value->as_NewArray() != NULL || value->as_NewInstance() != NULL || value->as_NewInlineTypeInstance() != NULL) {\n+  if (value->as_NewArray() != nullptr || value->as_NewInstance() != nullptr || value->as_NewInlineTypeInstance() != nullptr) {\n@@ -2861,2 +2861,2 @@\n-  if (!has_handler() && (!instruction->needs_exception_state() || instruction->exception_state() != NULL)) {\n-    assert(instruction->exception_state() == NULL\n+  if (!has_handler() && (!instruction->needs_exception_state() || instruction->exception_state() != nullptr)) {\n+    assert(instruction->exception_state() == nullptr\n@@ -2872,1 +2872,1 @@\n-  ValueStack* prev_state = NULL;\n+  ValueStack* prev_state = nullptr;\n@@ -2875,1 +2875,1 @@\n-  assert(cur_state != NULL, \"state_before must be set\");\n+  assert(cur_state != nullptr, \"state_before must be set\");\n@@ -2904,1 +2904,1 @@\n-        assert(entry->state() == NULL || cur_state->total_locks_size() == entry->state()->total_locks_size(), \"locks do not match\");\n+        assert(entry->state() == nullptr || cur_state->total_locks_size() == entry->state()->total_locks_size(), \"locks do not match\");\n@@ -2910,1 +2910,1 @@\n-        if (instruction->exception_state() == NULL) {\n+        if (instruction->exception_state() == nullptr) {\n@@ -2962,1 +2962,1 @@\n-      if (prev_state != NULL) {\n+      if (prev_state != nullptr) {\n@@ -2965,1 +2965,1 @@\n-      if (instruction->exception_state() == NULL) {\n+      if (instruction->exception_state() == nullptr) {\n@@ -2986,1 +2986,1 @@\n-  } while (cur_scope_data != NULL);\n+  } while (cur_scope_data != nullptr);\n@@ -3013,1 +3013,1 @@\n-  if (phi == NULL) {\n+  if (phi == nullptr) {\n@@ -3034,1 +3034,1 @@\n-    Value subst = NULL;\n+    Value subst = nullptr;\n@@ -3038,1 +3038,1 @@\n-      assert(opd != NULL, \"Operand must exist!\");\n+      assert(opd != nullptr, \"Operand must exist!\");\n@@ -3048,1 +3048,1 @@\n-      assert(new_opd != NULL, \"Simplified operand must exist!\");\n+      assert(new_opd != nullptr, \"Simplified operand must exist!\");\n@@ -3051,1 +3051,1 @@\n-        if (subst == NULL) {\n+        if (subst == nullptr) {\n@@ -3063,1 +3063,1 @@\n-    assert(subst != NULL, \"illegal phi function\");\n+    assert(subst != nullptr, \"illegal phi function\");\n@@ -3092,1 +3092,1 @@\n-    assert(phi == NULL || phi->block() != b, \"must not have phi function to simplify in caller state\");\n+    assert(phi == nullptr || phi->block() != b, \"must not have phi function to simplify in caller state\");\n@@ -3127,1 +3127,1 @@\n-  assert(state() != NULL, \"ValueStack missing!\");\n+  assert(state() != nullptr, \"ValueStack missing!\");\n@@ -3137,1 +3137,1 @@\n-  if (block()->is_set(BlockBegin::exception_entry_flag) && block()->next() == NULL) {\n+  if (block()->is_set(BlockBegin::exception_entry_flag) && block()->next() == nullptr) {\n@@ -3144,1 +3144,1 @@\n-  while (!bailed_out() && last()->as_BlockEnd() == NULL &&\n+  while (!bailed_out() && last()->as_BlockEnd() == nullptr &&\n@@ -3146,1 +3146,1 @@\n-         (block_at(s.cur_bci()) == NULL || block_at(s.cur_bci()) == block())) {\n+         (block_at(s.cur_bci()) == nullptr || block_at(s.cur_bci()) == block())) {\n@@ -3149,1 +3149,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -3344,1 +3344,1 @@\n-      case Bytecodes::_return         : method_return(NULL  , ignore_return); break;\n+      case Bytecodes::_return         : method_return(nullptr, ignore_return); break;\n@@ -3371,1 +3371,1 @@\n-      case Bytecodes::_breakpoint     : BAILOUT_(\"concurrent setting of breakpoint\", NULL);\n+      case Bytecodes::_breakpoint     : BAILOUT_(\"concurrent setting of breakpoint\", nullptr);\n@@ -3375,1 +3375,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -3382,1 +3382,1 @@\n-  CHECK_BAILOUT_(NULL);\n+  CHECK_BAILOUT_(nullptr);\n@@ -3391,1 +3391,1 @@\n-  if (end == NULL) {\n+  if (end == nullptr) {\n@@ -3398,2 +3398,2 @@\n-  assert(end->state() != NULL, \"state must already be present\");\n-  assert(end->as_Return() == NULL || end->as_Throw() == NULL || end->state()->stack_size() == 0, \"stack not needed for return and throw\");\n+  assert(end->state() != nullptr, \"state must already be present\");\n+  assert(end->as_Return() == nullptr || end->as_Throw() == nullptr || end->state()->stack_size() == 0, \"stack not needed for return and throw\");\n@@ -3409,1 +3409,1 @@\n-    if (!sux->try_merge(end->state(), compilation()->has_irreducible_loops())) BAILOUT_(\"block join failed\", NULL);\n+    if (!sux->try_merge(end->state(), compilation()->has_irreducible_loops())) BAILOUT_(\"block join failed\", nullptr);\n@@ -3413,1 +3413,1 @@\n-  scope_data()->set_stream(NULL);\n+  scope_data()->set_stream(nullptr);\n@@ -3427,1 +3427,1 @@\n-      while ((b = scope_data()->remove_from_work_list()) != NULL) {\n+      while ((b = scope_data()->remove_from_work_list()) != nullptr) {\n@@ -3561,1 +3561,1 @@\n-  if (base->std_entry()->state() == NULL) {\n+  if (base->std_entry()->state() == nullptr) {\n@@ -3566,1 +3566,1 @@\n-  assert(base->std_entry()->state() != NULL, \"\");\n+  assert(base->std_entry()->state() != nullptr, \"\");\n@@ -3585,1 +3585,1 @@\n-  assert(target != NULL && target->is_set(BlockBegin::osr_entry_flag), \"must be there\");\n+  assert(target != nullptr && target->is_set(BlockBegin::osr_entry_flag), \"must be there\");\n@@ -3641,1 +3641,1 @@\n-  assert(state->caller_state() == NULL, \"should be top scope\");\n+  assert(state->caller_state() == nullptr, \"should be top scope\");\n@@ -3648,1 +3648,1 @@\n-  scope_data()->set_stream(NULL);\n+  scope_data()->set_stream(nullptr);\n@@ -3653,1 +3653,1 @@\n-  ValueStack* state = new ValueStack(scope(), NULL);\n+  ValueStack* state = new ValueStack(scope(), nullptr);\n@@ -3678,1 +3678,1 @@\n-    state->lock(NULL);\n+    state->lock(nullptr);\n@@ -3686,1 +3686,1 @@\n-  : _scope_data(NULL)\n+  : _scope_data(nullptr)\n@@ -3689,1 +3689,1 @@\n-  , _inline_bailout_msg(NULL)\n+  , _inline_bailout_msg(nullptr)\n@@ -3691,3 +3691,3 @@\n-  , _osr_entry(NULL)\n-  , _pending_field_access(NULL)\n-  , _pending_load_indexed(NULL)\n+  , _osr_entry(nullptr)\n+  , _pending_field_access(nullptr)\n+  , _pending_load_indexed(nullptr)\n@@ -3816,1 +3816,1 @@\n-  \/\/All blocks reachable from start_block have _end != NULL\n+  \/\/ For all blocks reachable from start_block: _end must be non-null\n@@ -3823,2 +3823,2 @@\n-      assert(current != NULL, \"Should not happen.\");\n-      assert(current->end() != NULL, \"All blocks reachable from start_block should have end() != NULL.\");\n+      assert(current != nullptr, \"Should not happen.\");\n+      assert(current->end() != nullptr, \"All blocks reachable from start_block should have end() != nullptr.\");\n@@ -3876,1 +3876,1 @@\n-  if (!has_handler()) return NULL;\n+  if (!has_handler()) return nullptr;\n@@ -3882,1 +3882,1 @@\n-  if (s == NULL) {\n+  if (s == nullptr) {\n@@ -3894,1 +3894,1 @@\n-  for (IRScope* s = scope(); s != NULL; s = s->caller()) {\n+  for (IRScope* s = scope(); s != nullptr; s = s->caller()) {\n@@ -3904,1 +3904,1 @@\n-  const char* msg = NULL;\n+  const char* msg = nullptr;\n@@ -3911,1 +3911,1 @@\n-  if (msg != NULL) {\n+  if (msg != nullptr) {\n@@ -3942,1 +3942,1 @@\n-  if (msg != NULL) {\n+  if (msg != nullptr) {\n@@ -3972,1 +3972,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -3975,1 +3975,1 @@\n-\/\/ negative filter: should callee NOT be inlined?  returns NULL, ok to inline, or rejection msg\n+\/\/ negative filter: should callee NOT be inlined?  returns null, ok to inline, or rejection msg\n@@ -3979,1 +3979,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -4060,1 +4060,1 @@\n-        Value recv = NULL;\n+        Value recv = nullptr;\n@@ -4065,1 +4065,1 @@\n-        profile_call(callee, recv, NULL, collect_args_for_profiling(args, callee, true), true);\n+        profile_call(callee, recv, nullptr, collect_args_for_profiling(args, callee, true), true);\n@@ -4114,1 +4114,1 @@\n-  assert(cont != NULL, \"continuation must exist (BlockListBuilder starts a new block after a jsr\");\n+  assert(cont != nullptr, \"continuation must exist (BlockListBuilder starts a new block after a jsr\");\n@@ -4127,1 +4127,1 @@\n-  assert(jsr_start_block != NULL, \"jsr start block must exist\");\n+  assert(jsr_start_block != nullptr, \"jsr start block must exist\");\n@@ -4131,1 +4131,1 @@\n-  assert(jsr_start_block->state() == NULL, \"should have fresh jsr starting block\");\n+  assert(jsr_start_block->state() == nullptr, \"should have fresh jsr starting block\");\n@@ -4138,1 +4138,1 @@\n-  scope_data()->set_stream(NULL);\n+  scope_data()->set_stream(nullptr);\n@@ -4153,1 +4153,1 @@\n-  if (cont->state() != NULL) {\n+  if (cont->state() != nullptr) {\n@@ -4181,1 +4181,1 @@\n-  assert(lock != NULL && sync_handler != NULL, \"lock or handler missing\");\n+  assert(lock != nullptr && sync_handler != nullptr, \"lock or handler missing\");\n@@ -4184,1 +4184,1 @@\n-  assert(_last->as_MonitorEnter() != NULL, \"monitor enter expected\");\n+  assert(_last->as_MonitorEnter() != nullptr, \"monitor enter expected\");\n@@ -4209,1 +4209,1 @@\n-  assert(sync_handler != NULL, \"handler missing\");\n+  assert(sync_handler != nullptr, \"handler missing\");\n@@ -4212,1 +4212,1 @@\n-  assert(lock != NULL || default_handler, \"lock or handler missing\");\n+  assert(lock != nullptr || default_handler, \"lock or handler missing\");\n@@ -4288,1 +4288,1 @@\n-  Value recv = NULL;\n+  Value recv = nullptr;\n@@ -4326,1 +4326,1 @@\n-      while (top->caller() != NULL) {\n+      while (top->caller() != nullptr) {\n@@ -4365,1 +4365,1 @@\n-      if (obj_args != NULL) {\n+      if (obj_args != nullptr) {\n@@ -4377,1 +4377,1 @@\n-      profile_call(callee, recv, holder_known ? callee->holder() : NULL, obj_args, true);\n+      profile_call(callee, recv, holder_known ? callee->holder() : nullptr, obj_args, true);\n@@ -4388,1 +4388,1 @@\n-  if (cont == NULL) {\n+  if (cont == nullptr) {\n@@ -4430,2 +4430,2 @@\n-  Value lock = NULL;\n-  BlockBegin* sync_handler = NULL;\n+  Value lock = nullptr;\n+  BlockBegin* sync_handler = nullptr;\n@@ -4452,1 +4452,1 @@\n-  if (callee_start_block != NULL) {\n+  if (callee_start_block != nullptr) {\n@@ -4467,1 +4467,1 @@\n-  scope_data()->set_stream(NULL);\n+  scope_data()->set_stream(nullptr);\n@@ -4471,1 +4471,1 @@\n-  if (log != NULL) log->head(\"parse method='%d'\", log->identify(callee));\n+  if (log != nullptr) log->head(\"parse method='%d'\", log->identify(callee));\n@@ -4475,1 +4475,1 @@\n-  iterate_all_blocks(callee_start_block == NULL);\n+  iterate_all_blocks(callee_start_block == nullptr);\n@@ -4477,1 +4477,1 @@\n-  if (log != NULL) log->done(\"parse\");\n+  if (log != nullptr) log->done(\"parse\");\n@@ -4526,1 +4526,1 @@\n-  if (callee->is_synchronized() && sync_handler->state() != NULL) {\n+  if (callee->is_synchronized() && sync_handler->state() != nullptr) {\n@@ -4603,1 +4603,1 @@\n-            if (obj->exact_type() == NULL &&\n+            if (obj->exact_type() == nullptr &&\n@@ -4616,1 +4616,1 @@\n-              if (obj->exact_type() == NULL &&\n+              if (obj->exact_type() == nullptr &&\n@@ -4655,1 +4655,1 @@\n-  assert(msg != NULL, \"inline bailout msg must exist\");\n+  assert(msg != nullptr, \"inline bailout msg must exist\");\n@@ -4661,1 +4661,1 @@\n-  _inline_bailout_msg = NULL;\n+  _inline_bailout_msg = nullptr;\n@@ -4666,1 +4666,1 @@\n-  ScopeData* data = new ScopeData(NULL);\n+  ScopeData* data = new ScopeData(nullptr);\n@@ -4684,1 +4684,1 @@\n-    blb.bci2block()->at_put(0, NULL);\n+    blb.bci2block()->at_put(0, nullptr);\n@@ -4809,1 +4809,1 @@\n-    Instruction* store = append(new StoreIndexed(array, index, NULL, T_CHAR, value, state_before, false, true));\n+    Instruction* store = append(new StoreIndexed(array, index, nullptr, T_CHAR, value, state_before, false, true));\n@@ -4813,1 +4813,1 @@\n-    Instruction* load = append(new LoadIndexed(array, index, NULL, T_CHAR, state_before, true));\n+    Instruction* load = append(new LoadIndexed(array, index, nullptr, T_CHAR, state_before, true));\n@@ -4821,2 +4821,2 @@\n-  if (log != NULL) {\n-    assert(msg != NULL, \"inlining msg should not be null!\");\n+  if (log != nullptr) {\n+    assert(msg != nullptr, \"inlining msg should not be null!\");\n@@ -4866,1 +4866,1 @@\n-  assert(known_holder == NULL || (known_holder->is_instance_klass() &&\n+  assert(known_holder == nullptr || (known_holder->is_instance_klass() &&\n@@ -4869,2 +4869,2 @@\n-  if (known_holder != NULL) {\n-    if (known_holder->exact_klass() == NULL) {\n+  if (known_holder != nullptr) {\n+    if (known_holder->exact_klass() == nullptr) {\n@@ -4879,2 +4879,2 @@\n-  assert((m == NULL) == (invoke_bci < 0), \"invalid method and invalid bci together\");\n-  if (m == NULL) {\n+  assert((m == nullptr) == (invoke_bci < 0), \"invalid method and invalid bci together\");\n+  if (m == nullptr) {\n@@ -4888,1 +4888,1 @@\n-  if (data != NULL && (data->is_CallTypeData() || data->is_VirtualCallTypeData())) {\n+  if (data != nullptr && (data->is_CallTypeData() || data->is_VirtualCallTypeData())) {\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":247,"deletions":247,"binary":false,"changes":494,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -146,1 +146,1 @@\n-    \/\/ How to remove the next block to be parsed; returns NULL if none left\n+    \/\/ How to remove the next block to be parsed; returns null if none left\n@@ -265,1 +265,1 @@\n-  void arithmetic_op(ValueType* type, Bytecodes::Code code, ValueStack* state_before = NULL);\n+  void arithmetic_op(ValueType* type, Bytecodes::Code code, ValueStack* state_before = nullptr);\n@@ -342,1 +342,1 @@\n-  ValueStack* copy_state_if_bb(bool is_bb) { return (is_bb || compilation()->is_optimistic()) ? copy_state_before() : NULL; }\n+  ValueStack* copy_state_if_bb(bool is_bb) { return (is_bb || compilation()->is_optimistic()) ? copy_state_before() : nullptr; }\n@@ -383,1 +383,1 @@\n-  bool try_inline(           ciMethod* callee, bool holder_known, bool ignore_return, Bytecodes::Code bc = Bytecodes::_illegal, Value receiver = NULL);\n+  bool try_inline(           ciMethod* callee, bool holder_known, bool ignore_return, Bytecodes::Code bc = Bytecodes::_illegal, Value receiver = nullptr);\n@@ -385,1 +385,1 @@\n-  bool try_inline_full(      ciMethod* callee, bool holder_known, bool ignore_return, Bytecodes::Code bc = Bytecodes::_illegal, Value receiver = NULL);\n+  bool try_inline_full(      ciMethod* callee, bool holder_known, bool ignore_return, Bytecodes::Code bc = Bytecodes::_illegal, Value receiver = nullptr);\n@@ -429,1 +429,1 @@\n-  void profile_return_type(Value ret, ciMethod* callee, ciMethod* m = NULL, int bci = -1);\n+  void profile_return_type(Value ret, ciMethod* callee, ciMethod* m = nullptr, int bci = -1);\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.hpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -105,1 +105,1 @@\n-  if (others == NULL) return false;\n+  if (others == nullptr) return false;\n@@ -130,1 +130,1 @@\n-  if (compilation->bailed_out()) return NULL;\n+  if (compilation->bailed_out()) return nullptr;\n@@ -141,1 +141,1 @@\n-  _level              = caller == NULL ?  0 : caller->level() + 1;\n+  _level              = caller == nullptr ?  0 : caller->level() + 1;\n@@ -149,1 +149,1 @@\n-  _start              = NULL;\n+  _start              = nullptr;\n@@ -179,1 +179,1 @@\n-  if (cur_method != NULL && cur_bci != SynchronizationEntryBCI) {\n+  if (cur_method != nullptr && cur_bci != SynchronizationEntryBCI) {\n@@ -190,1 +190,1 @@\n-  : _scope_debug_info(NULL)\n+  : _scope_debug_info(nullptr)\n@@ -193,1 +193,1 @@\n-  , _oop_map(NULL)\n+  , _oop_map(nullptr)\n@@ -198,1 +198,1 @@\n-  assert(_stack != NULL, \"must be non null\");\n+  assert(_stack != nullptr, \"must be non null\");\n@@ -203,1 +203,1 @@\n-  : _scope_debug_info(NULL)\n+  : _scope_debug_info(nullptr)\n@@ -205,3 +205,3 @@\n-  , _exception_handlers(NULL)\n-  , _oop_map(NULL)\n-  , _stack(stack == NULL ? info->_stack : stack)\n+  , _exception_handlers(nullptr)\n+  , _oop_map(nullptr)\n+  , _stack(stack == nullptr ? info->_stack : stack)\n@@ -213,1 +213,1 @@\n-  if (info->_exception_handlers != NULL) {\n+  if (info->_exception_handlers != nullptr) {\n@@ -229,1 +229,1 @@\n-  assert(_oop_map != NULL, \"oop map must already exist\");\n+  assert(_oop_map != nullptr, \"oop map must already exist\");\n@@ -246,1 +246,1 @@\n-  while (state != NULL) {\n+  while (state != nullptr) {\n@@ -275,2 +275,2 @@\n-  _top_scope   = new IRScope(compilation, NULL, -1, method, osr_bci, true);\n-  _code        = NULL;\n+  _top_scope   = new IRScope(compilation, nullptr, -1, method, osr_bci, true);\n+  _code        = nullptr;\n@@ -341,1 +341,1 @@\n-    BlockPair* last_pair = NULL;\n+    BlockPair* last_pair = nullptr;\n@@ -345,1 +345,1 @@\n-      if (last_pair != NULL && pair->is_same(last_pair)) continue;\n+      if (last_pair != nullptr && pair->is_same(last_pair)) continue;\n@@ -402,1 +402,1 @@\n-      if ((*n)->as_BlockEnd() != NULL) {\n+      if ((*n)->as_BlockEnd() != nullptr) {\n@@ -415,1 +415,1 @@\n-    for (Instruction* n = b; n != NULL; n = n->next()) {\n+    for (Instruction* n = b; n != nullptr; n = n->next()) {\n@@ -536,1 +536,1 @@\n-  _linear_scan_order(NULL), \/\/ initialized later with correct size\n+  _linear_scan_order(nullptr), \/\/ initialized later with correct size\n@@ -548,1 +548,1 @@\n-  count_edges(start_block, NULL);\n+  count_edges(start_block, nullptr);\n@@ -554,1 +554,1 @@\n-      assert(md != NULL, \"Sanity\");\n+      assert(md != nullptr, \"Sanity\");\n@@ -579,2 +579,2 @@\n-  TRACE_LINEAR_SCAN(3, tty->print_cr(\"Enter count_edges for block B%d coming from B%d\", cur->block_id(), parent != NULL ? parent->block_id() : -1));\n-  assert(cur->dominator() == NULL, \"dominator already initialized\");\n+  TRACE_LINEAR_SCAN(3, tty->print_cr(\"Enter count_edges for block B%d coming from B%d\", cur->block_id(), parent != nullptr ? parent->block_id() : -1));\n+  assert(cur->dominator() == nullptr, \"dominator already initialized\");\n@@ -585,1 +585,1 @@\n-    assert(parent != NULL, \"must have parent\");\n+    assert(parent != nullptr, \"must have parent\");\n@@ -762,1 +762,1 @@\n-  assert(a != NULL && b != NULL, \"must have input blocks\");\n+  assert(a != nullptr && b != nullptr, \"must have input blocks\");\n@@ -765,1 +765,1 @@\n-  while (a != NULL) {\n+  while (a != nullptr) {\n@@ -767,1 +767,1 @@\n-    assert(a->dominator() != NULL || a == _linear_scan_order->at(0), \"dominator must be initialized\");\n+    assert(a->dominator() != nullptr || a == _linear_scan_order->at(0), \"dominator must be initialized\");\n@@ -770,2 +770,2 @@\n-  while (b != NULL && !_dominator_blocks.at(b->block_id())) {\n-    assert(b->dominator() != NULL || b == _linear_scan_order->at(0), \"dominator must be initialized\");\n+  while (b != nullptr && !_dominator_blocks.at(b->block_id())) {\n+    assert(b->dominator() != nullptr || b == _linear_scan_order->at(0), \"dominator must be initialized\");\n@@ -775,1 +775,1 @@\n-  assert(b != NULL, \"could not find dominator\");\n+  assert(b != nullptr, \"could not find dominator\");\n@@ -788,1 +788,1 @@\n-  if (cur->dominator() == NULL) {\n+  if (cur->dominator() == nullptr) {\n@@ -814,1 +814,1 @@\n-  BlockBegin* single_sux = NULL;\n+  BlockBegin* single_sux = nullptr;\n@@ -842,2 +842,2 @@\n-  INC_WEIGHT_IF(cur->end()->as_Throw() == NULL  && (single_sux == NULL || single_sux->end()->as_Throw()  == NULL));\n-  INC_WEIGHT_IF(cur->end()->as_Return() == NULL && (single_sux == NULL || single_sux->end()->as_Return() == NULL));\n+  INC_WEIGHT_IF(cur->end()->as_Throw() == nullptr  && (single_sux == nullptr || single_sux->end()->as_Throw()  == nullptr));\n+  INC_WEIGHT_IF(cur->end()->as_Return() == nullptr && (single_sux == nullptr || single_sux->end()->as_Return() == nullptr));\n@@ -885,1 +885,1 @@\n-  _work_list.append(NULL); \/\/ provide space for new element\n+  _work_list.append(nullptr); \/\/ provide space for new element\n@@ -923,1 +923,1 @@\n-  assert(start_block->end()->as_Base() != NULL, \"start block must end with Base-instruction\");\n+  assert(start_block->end()->as_Base() != nullptr, \"start block must end with Base-instruction\");\n@@ -927,2 +927,2 @@\n-  BlockBegin* sux_of_osr_entry = NULL;\n-  if (osr_entry != NULL) {\n+  BlockBegin* sux_of_osr_entry = nullptr;\n+  if (osr_entry != nullptr) {\n@@ -988,1 +988,1 @@\n-  assert(_linear_scan_order->at(0)->dominator() == NULL, \"must not have dominator\");\n+  assert(_linear_scan_order->at(0)->dominator() == nullptr, \"must not have dominator\");\n@@ -1082,1 +1082,1 @@\n-      if (cur->dominator() != NULL) {\n+      if (cur->dominator() != nullptr) {\n@@ -1085,1 +1085,1 @@\n-        tty->print(\"    dom: NULL \");\n+        tty->print(\"    dom: null \");\n@@ -1161,1 +1161,1 @@\n-      assert(cur->dominator() == NULL, \"first block has no dominator\");\n+      assert(cur->dominator() == nullptr, \"first block has no dominator\");\n@@ -1163,1 +1163,1 @@\n-      assert(cur->dominator() != NULL, \"all but first block must have dominator\");\n+      assert(cur->dominator() != nullptr, \"all but first block must have dominator\");\n@@ -1274,1 +1274,1 @@\n-    assert(block->end() != NULL, \"Expect block end to exist.\");\n+    assert(block->end() != nullptr, \"Expect block end to exist.\");\n@@ -1309,1 +1309,1 @@\n-    _predecessors = new BlockListList(BlockBegin::number_of_blocks(), BlockBegin::number_of_blocks(), NULL);\n+    _predecessors = new BlockListList(BlockBegin::number_of_blocks(), BlockBegin::number_of_blocks(), nullptr);\n@@ -1313,1 +1313,1 @@\n-    if (hir->code() != NULL) {\n+    if (hir->code() != nullptr) {\n@@ -1342,2 +1342,2 @@\n-    BlockList* preds = _predecessors->at_grow(sux->block_id(), NULL);\n-    if (preds == NULL) {\n+    BlockList* preds = _predecessors->at_grow(sux->block_id(), nullptr);\n+    if (preds == nullptr) {\n@@ -1352,1 +1352,1 @@\n-    if (preds == NULL) {\n+    if (preds == nullptr) {\n@@ -1375,1 +1375,1 @@\n-    for (Instruction* cur = block; cur != NULL; cur = cur->next()) {\n+    for (Instruction* cur = block; cur != nullptr; cur = cur->next()) {\n@@ -1393,2 +1393,2 @@\n-      assert(block->pred_at(i) != NULL, \"Predecessor must exist\");\n-      assert(block->pred_at(i)->end() != NULL, \"Predecessor end must exist\");\n+      assert(block->pred_at(i) != nullptr, \"Predecessor must exist\");\n+      assert(block->pred_at(i)->end() != nullptr, \"Predecessor end must exist\");\n@@ -1479,2 +1479,2 @@\n-  Instruction* last = NULL;\n-  for (Instruction* n = block; n != NULL;) {\n+  Instruction* last = nullptr;\n+  for (Instruction* n = block; n != nullptr;) {\n@@ -1484,1 +1484,1 @@\n-      guarantee(last != NULL, \"must have last\");\n+      guarantee(last != nullptr, \"must have last\");\n","filename":"src\/hotspot\/share\/c1\/c1_IR.cpp","additions":59,"deletions":59,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,2 +54,2 @@\n-    , _entry_block(NULL)\n-    , _entry_code(NULL)\n+    , _entry_block(nullptr)\n+    , _entry_code(nullptr)\n@@ -140,1 +140,1 @@\n-  IRScope*      _caller;                         \/\/ the caller scope, or NULL\n+  IRScope*      _caller;                         \/\/ the caller scope, or null\n@@ -172,1 +172,1 @@\n-  bool          is_top_scope() const             { return _caller == NULL; }\n+  bool          is_top_scope() const             { return _caller == nullptr; }\n@@ -178,1 +178,1 @@\n-  bool          is_valid() const                 { return start() != NULL; }\n+  bool          is_valid() const                 { return start() != nullptr; }\n@@ -239,1 +239,1 @@\n-    if (caller() != NULL) {\n+    if (caller() != nullptr) {\n@@ -284,1 +284,1 @@\n-  CodeEmitInfo(CodeEmitInfo* info, ValueStack* stack = NULL);\n+  CodeEmitInfo(CodeEmitInfo* info, ValueStack* stack = nullptr);\n@@ -342,1 +342,1 @@\n-  BlockList* linear_scan_order() {  assert(_code != NULL, \"not computed\"); return _code; }\n+  BlockList* linear_scan_order() {  assert(_code != nullptr, \"not computed\"); return _code; }\n","filename":"src\/hotspot\/share\/c1\/c1_IR.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,1 +81,1 @@\n-  if (state != NULL && (state->kind() == ValueStack::EmptyExceptionState || state->kind() == ValueStack::ExceptionState)) {\n+  if (state != nullptr && (state->kind() == ValueStack::EmptyExceptionState || state->kind() == ValueStack::ExceptionState)) {\n@@ -85,1 +85,1 @@\n-    _exception_state = NULL;\n+    _exception_state = nullptr;\n@@ -91,1 +91,1 @@\n-  Instruction* p = NULL;\n+  Instruction* p = nullptr;\n@@ -94,1 +94,1 @@\n-    assert(q != NULL, \"this is not in the block's instruction list\");\n+    assert(q != nullptr, \"this is not in the block's instruction list\");\n@@ -102,1 +102,1 @@\n-  if (state_before() != NULL) {\n+  if (state_before() != nullptr) {\n@@ -105,1 +105,1 @@\n-  if (exception_state() != NULL){\n+  if (exception_state() != nullptr) {\n@@ -112,1 +112,1 @@\n-  if (t != NULL && t->is_klass()) {\n+  if (t != nullptr && t->is_klass()) {\n@@ -115,1 +115,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -120,1 +120,1 @@\n-  if (type != NULL && type->is_klass()) {\n+  if (type != nullptr && type->is_klass()) {\n@@ -126,1 +126,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -132,1 +132,1 @@\n-    return type != NULL && type->is_flat_array_klass();\n+    return type != nullptr && type->is_flat_array_klass();\n@@ -140,1 +140,1 @@\n-    if (type != NULL) {\n+    if (type != nullptr) {\n@@ -164,1 +164,1 @@\n-  if (type != NULL) {\n+  if (type != nullptr) {\n@@ -181,1 +181,1 @@\n-  if (state != NULL) {\n+  if (state != nullptr) {\n@@ -233,1 +233,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -238,1 +238,1 @@\n-  if (delayed() == NULL && array_type != NULL) {\n+  if (delayed() == nullptr && array_type != nullptr) {\n@@ -252,1 +252,1 @@\n-  if (delayed() != NULL) {\n+  if (delayed() != nullptr) {\n@@ -257,2 +257,2 @@\n-  if (array_type == NULL || !array_type->is_loaded()) {\n-    return NULL;\n+  if (array_type == nullptr || !array_type->is_loaded()) {\n+    return nullptr;\n@@ -266,1 +266,1 @@\n-  if (array()->is_loaded_flattened_array() && value()->as_Constant() == NULL && value()->declared_type() != NULL) {\n+  if (array()->is_loaded_flattened_array() && value()->as_Constant() == nullptr && value()->declared_type() != nullptr) {\n@@ -397,1 +397,1 @@\n-  if (state() != NULL) state()->values_do(f);\n+  if (state() != nullptr) state()->values_do(f);\n@@ -416,1 +416,1 @@\n-  , _enclosing_field(NULL)\n+  , _enclosing_field(nullptr)\n@@ -424,1 +424,1 @@\n-  if (value->as_NewInlineTypeInstance() != NULL) {\n+  if (value->as_NewInlineTypeInstance() != nullptr) {\n@@ -441,1 +441,1 @@\n-  if (value->as_NewInlineTypeInstance() != NULL) {\n+  if (value->as_NewInlineTypeInstance() != nullptr) {\n@@ -462,1 +462,1 @@\n-  assert(args != NULL, \"args must exist\");\n+  assert(args != nullptr, \"args must exist\");\n@@ -472,1 +472,1 @@\n-    if (receiver()->as_NewInlineTypeInstance() != NULL) {\n+    if (receiver()->as_NewInlineTypeInstance() != nullptr) {\n@@ -481,1 +481,1 @@\n-    if (v->as_NewInlineTypeInstance() != NULL) {\n+    if (v->as_NewInlineTypeInstance() != nullptr) {\n@@ -490,2 +490,2 @@\n-  if (state_before() != NULL) state_before()->values_do(f);\n-  if (state()        != NULL) state()->values_do(f);\n+  if (state_before() != nullptr) state_before()->values_do(f);\n+  if (state()        != nullptr) state()->values_do(f);\n@@ -503,1 +503,1 @@\n-  if (state_before() == NULL) {\n+  if (state_before() == nullptr) {\n@@ -535,1 +535,1 @@\n-  if (v->as_Constant() == NULL) return false;\n+  if (v->as_Constant() == nullptr) return false;\n@@ -542,1 +542,1 @@\n-        return (t1 != NULL && t2 != NULL &&\n+        return (t1 != nullptr && t2 != nullptr &&\n@@ -549,1 +549,1 @@\n-        return (t1 != NULL && t2 != NULL &&\n+        return (t1 != nullptr && t2 != nullptr &&\n@@ -556,1 +556,1 @@\n-        return (t1 != NULL && t2 != NULL &&\n+        return (t1 != nullptr && t2 != nullptr &&\n@@ -563,1 +563,1 @@\n-        return (t1 != NULL && t2 != NULL &&\n+        return (t1 != nullptr && t2 != nullptr &&\n@@ -570,1 +570,1 @@\n-        return (t1 != NULL && t2 != NULL &&\n+        return (t1 != nullptr && t2 != nullptr &&\n@@ -578,1 +578,1 @@\n-        return (t1 != NULL && t2 != NULL &&\n+        return (t1 != nullptr && t2 != nullptr &&\n@@ -590,1 +590,1 @@\n-  if (rc == NULL) return not_comparable;\n+  if (rc == nullptr) return not_comparable;\n@@ -628,1 +628,1 @@\n-    assert(xvalue != NULL && yvalue != NULL, \"not constants\");\n+    assert(xvalue != nullptr && yvalue != nullptr, \"not constants\");\n@@ -641,1 +641,1 @@\n-    assert(xvalue != NULL && yvalue != NULL, \"not constants\");\n+    assert(xvalue != nullptr && yvalue != nullptr, \"not constants\");\n@@ -661,1 +661,1 @@\n-  assert(new_end != NULL, \"Should not reset block new_end to NULL\");\n+  assert(new_end != nullptr, \"Should not reset block new_end to null\");\n@@ -665,1 +665,1 @@\n-  if (_end != NULL) {\n+  if (_end != nullptr) {\n@@ -792,1 +792,1 @@\n-  assert(b != NULL && (b->is_set(exception_entry_flag)), \"exception handler must exist\");\n+  assert(b != nullptr && (b->is_set(exception_entry_flag)), \"exception handler must exist\");\n@@ -799,1 +799,1 @@\n-  if (_exception_states == NULL) {\n+  if (_exception_states == nullptr) {\n@@ -844,1 +844,1 @@\n-  for (Instruction* n = this; n != NULL; n = n->next()) n->values_do(f);\n+  for (Instruction* n = this; n != nullptr; n = n->next()) n->values_do(f);\n@@ -863,1 +863,1 @@\n-  if (existing_state == NULL) {\n+  if (existing_state == nullptr) {\n@@ -925,1 +925,1 @@\n-        if (new_value == NULL || new_value->type()->tag() != existing_value->type()->tag()) {\n+        if (new_value == nullptr || new_value->type()->tag() != existing_value->type()->tag()) {\n@@ -927,1 +927,1 @@\n-          if (existing_phi == NULL) {\n+          if (existing_phi == nullptr) {\n@@ -938,1 +938,1 @@\n-        if (existing_value != new_state->local_at(index) && existing_value->as_Phi() == NULL) {\n+        if (existing_value != new_state->local_at(index) && existing_value->as_Phi() == nullptr) {\n@@ -947,1 +947,1 @@\n-        assert(existing_value->as_Phi() != NULL && existing_value->as_Phi()->block() == this, \"phi function required\");\n+        assert(existing_value->as_Phi() != nullptr && existing_value->as_Phi()->block() == this, \"phi function required\");\n@@ -950,1 +950,1 @@\n-        assert(existing_value == new_state->local_at(index) || (existing_value->as_Phi() != NULL && existing_value->as_Phi()->as_Phi()->block() == this), \"phi function required\");\n+        assert(existing_value == new_state->local_at(index) || (existing_value->as_Phi() != nullptr && existing_value->as_Phi()->as_Phi()->block() == this), \"phi function required\");\n@@ -962,1 +962,1 @@\n-        if (new_value != existing_value && (existing_phi == NULL || existing_phi->block() != this)) {\n+        if (new_value != existing_value && (existing_phi == nullptr || existing_phi->block() != this)) {\n@@ -965,2 +965,2 @@\n-          if (new_value->as_NewInlineTypeInstance() != NULL) {new_value->as_NewInlineTypeInstance()->set_not_larva_anymore(); }\n-          if (existing_value->as_NewInlineTypeInstance() != NULL) {existing_value->as_NewInlineTypeInstance()->set_not_larva_anymore(); }\n+          if (new_value->as_NewInlineTypeInstance() != nullptr) {new_value->as_NewInlineTypeInstance()->set_not_larva_anymore(); }\n+          if (existing_value->as_NewInlineTypeInstance() != nullptr) {existing_value->as_NewInlineTypeInstance()->set_not_larva_anymore(); }\n@@ -975,1 +975,1 @@\n-        if (new_value == NULL || new_value->type()->tag() != existing_value->type()->tag()) {\n+        if (new_value == nullptr || new_value->type()->tag() != existing_value->type()->tag()) {\n@@ -978,1 +978,1 @@\n-        } else if (new_value != existing_value && (existing_phi == NULL || existing_phi->block() != this)) {\n+        } else if (new_value != existing_value && (existing_phi == nullptr || existing_phi->block() != this)) {\n@@ -981,2 +981,2 @@\n-          if (new_value->as_NewInlineTypeInstance() != NULL) {new_value->as_NewInlineTypeInstance()->set_not_larva_anymore(); }\n-          if (existing_value->as_NewInlineTypeInstance() != NULL) {existing_value->as_NewInlineTypeInstance()->set_not_larva_anymore(); }\n+          if (new_value->as_NewInlineTypeInstance() != nullptr) {new_value->as_NewInlineTypeInstance()->set_not_larva_anymore(); }\n+          if (existing_value->as_NewInlineTypeInstance() != nullptr) {existing_value->as_NewInlineTypeInstance()->set_not_larva_anymore(); }\n@@ -1012,1 +1012,1 @@\n-  for (Instruction* n = next(); n != NULL; n = n->next()) {\n+  for (Instruction* n = next(); n != nullptr; n = n->next()) {\n@@ -1074,1 +1074,1 @@\n-  assert(state != NULL, \"\");\n+  assert(state != nullptr, \"\");\n@@ -1126,1 +1126,1 @@\n-  if (state() != NULL) state()->values_do(f);\n+  if (state() != nullptr) state()->values_do(f);\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.cpp","additions":62,"deletions":62,"binary":false,"changes":124,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -246,1 +246,1 @@\n-    if (_v == NULL  ) return false;                   \\\n+    if (_v == nullptr) return false;                  \\\n@@ -259,1 +259,1 @@\n-    if (_v == NULL  ) return false;                   \\\n+    if (_v == nullptr) return false;                  \\\n@@ -273,1 +273,1 @@\n-    if (_v == NULL  ) return false;                   \\\n+    if (_v == nullptr) return false;                  \\\n@@ -287,1 +287,1 @@\n-    if (_v == NULL  ) return false;                   \\\n+    if (_v == nullptr  ) return false;                   \\\n@@ -307,1 +307,1 @@\n-  Instruction* _next;                            \/\/ the next instruction if any (NULL for BlockEnd instructions)\n+  Instruction* _next;                            \/\/ the next instruction if any (null for BlockEnd instructions)\n@@ -312,1 +312,1 @@\n-  ValueStack*  _state_before;                    \/\/ Copy of state with input operands still on stack (or NULL)\n+  ValueStack*  _state_before;                    \/\/ Copy of state with input operands still on stack (or null)\n@@ -325,1 +325,1 @@\n-    assert(type != NULL, \"type must exist\");\n+    assert(type != nullptr, \"type must exist\");\n@@ -422,1 +422,1 @@\n-  Instruction(ValueType* type, ValueStack* state_before = NULL, bool type_is_constant = false)\n+  Instruction(ValueType* type, ValueStack* state_before = nullptr, bool type_is_constant = false)\n@@ -430,2 +430,2 @@\n-  , _next(NULL)\n-  , _subst(NULL)\n+  , _next(nullptr)\n+  , _subst(nullptr)\n@@ -435,2 +435,2 @@\n-  , _exception_handlers(NULL)\n-  , _block(NULL)\n+  , _exception_handlers(nullptr)\n+  , _block(nullptr)\n@@ -439,1 +439,1 @@\n-    assert(type != NULL && (!type->is_constant() || type_is_constant), \"type must exist\");\n+    assert(type != nullptr && (!type->is_constant() || type_is_constant), \"type must exist\");\n@@ -458,2 +458,2 @@\n-  bool has_subst() const                         { return _subst != NULL; }\n-  Instruction* subst()                           { return _subst == NULL ? this : _subst->subst(); }\n+  bool has_subst() const                         { return _subst != nullptr; }\n+  Instruction* subst()                           { return _subst == nullptr ? this : _subst->subst(); }\n@@ -467,1 +467,1 @@\n-  bool can_be_linked()                           { return as_Local() == NULL && as_Phi() == NULL; }\n+  bool can_be_linked()                           { return as_Local() == nullptr && as_Phi() == nullptr; }\n@@ -469,1 +469,1 @@\n-  bool is_null_obj()                             { return as_Constant() != NULL && type()->as_ObjectType()->constant_value()->is_null_object(); }\n+  bool is_null_obj()                             { return as_Constant() != nullptr && type()->as_ObjectType()->constant_value()->is_null_object(); }\n@@ -486,2 +486,2 @@\n-    assert(next != NULL, \"must not be NULL\");\n-    assert(as_BlockEnd() == NULL, \"BlockEnd instructions must have no next\");\n+    assert(next != nullptr, \"must not be null\");\n+    assert(as_BlockEnd() == nullptr, \"BlockEnd instructions must have no next\");\n@@ -533,1 +533,1 @@\n-    assert(subst == NULL ||\n+    assert(subst == nullptr ||\n@@ -548,52 +548,52 @@\n-  virtual Phi*              as_Phi()             { return NULL; }\n-  virtual Local*            as_Local()           { return NULL; }\n-  virtual Constant*         as_Constant()        { return NULL; }\n-  virtual AccessField*      as_AccessField()     { return NULL; }\n-  virtual LoadField*        as_LoadField()       { return NULL; }\n-  virtual StoreField*       as_StoreField()      { return NULL; }\n-  virtual AccessArray*      as_AccessArray()     { return NULL; }\n-  virtual ArrayLength*      as_ArrayLength()     { return NULL; }\n-  virtual AccessIndexed*    as_AccessIndexed()   { return NULL; }\n-  virtual LoadIndexed*      as_LoadIndexed()     { return NULL; }\n-  virtual StoreIndexed*     as_StoreIndexed()    { return NULL; }\n-  virtual NegateOp*         as_NegateOp()        { return NULL; }\n-  virtual Op2*              as_Op2()             { return NULL; }\n-  virtual ArithmeticOp*     as_ArithmeticOp()    { return NULL; }\n-  virtual ShiftOp*          as_ShiftOp()         { return NULL; }\n-  virtual LogicOp*          as_LogicOp()         { return NULL; }\n-  virtual CompareOp*        as_CompareOp()       { return NULL; }\n-  virtual IfOp*             as_IfOp()            { return NULL; }\n-  virtual Convert*          as_Convert()         { return NULL; }\n-  virtual NullCheck*        as_NullCheck()       { return NULL; }\n-  virtual OsrEntry*         as_OsrEntry()        { return NULL; }\n-  virtual StateSplit*       as_StateSplit()      { return NULL; }\n-  virtual Invoke*           as_Invoke()          { return NULL; }\n-  virtual NewInstance*      as_NewInstance()     { return NULL; }\n-  virtual NewInlineTypeInstance* as_NewInlineTypeInstance() { return NULL; }\n-  virtual NewArray*         as_NewArray()        { return NULL; }\n-  virtual NewTypeArray*     as_NewTypeArray()    { return NULL; }\n-  virtual NewObjectArray*   as_NewObjectArray()  { return NULL; }\n-  virtual NewMultiArray*    as_NewMultiArray()   { return NULL; }\n-  virtual Deoptimize*       as_Deoptimize()      { return NULL; }\n-  virtual TypeCheck*        as_TypeCheck()       { return NULL; }\n-  virtual CheckCast*        as_CheckCast()       { return NULL; }\n-  virtual InstanceOf*       as_InstanceOf()      { return NULL; }\n-  virtual TypeCast*         as_TypeCast()        { return NULL; }\n-  virtual AccessMonitor*    as_AccessMonitor()   { return NULL; }\n-  virtual MonitorEnter*     as_MonitorEnter()    { return NULL; }\n-  virtual MonitorExit*      as_MonitorExit()     { return NULL; }\n-  virtual Intrinsic*        as_Intrinsic()       { return NULL; }\n-  virtual BlockBegin*       as_BlockBegin()      { return NULL; }\n-  virtual BlockEnd*         as_BlockEnd()        { return NULL; }\n-  virtual Goto*             as_Goto()            { return NULL; }\n-  virtual If*               as_If()              { return NULL; }\n-  virtual TableSwitch*      as_TableSwitch()     { return NULL; }\n-  virtual LookupSwitch*     as_LookupSwitch()    { return NULL; }\n-  virtual Return*           as_Return()          { return NULL; }\n-  virtual Throw*            as_Throw()           { return NULL; }\n-  virtual Base*             as_Base()            { return NULL; }\n-  virtual RoundFP*          as_RoundFP()         { return NULL; }\n-  virtual ExceptionObject*  as_ExceptionObject() { return NULL; }\n-  virtual UnsafeOp*         as_UnsafeOp()        { return NULL; }\n-  virtual ProfileInvoke*    as_ProfileInvoke()   { return NULL; }\n-  virtual RangeCheckPredicate* as_RangeCheckPredicate() { return NULL; }\n+  virtual Phi*              as_Phi()             { return nullptr; }\n+  virtual Local*            as_Local()           { return nullptr; }\n+  virtual Constant*         as_Constant()        { return nullptr; }\n+  virtual AccessField*      as_AccessField()     { return nullptr; }\n+  virtual LoadField*        as_LoadField()       { return nullptr; }\n+  virtual StoreField*       as_StoreField()      { return nullptr; }\n+  virtual AccessArray*      as_AccessArray()     { return nullptr; }\n+  virtual ArrayLength*      as_ArrayLength()     { return nullptr; }\n+  virtual AccessIndexed*    as_AccessIndexed()   { return nullptr; }\n+  virtual LoadIndexed*      as_LoadIndexed()     { return nullptr; }\n+  virtual StoreIndexed*     as_StoreIndexed()    { return nullptr; }\n+  virtual NegateOp*         as_NegateOp()        { return nullptr; }\n+  virtual Op2*              as_Op2()             { return nullptr; }\n+  virtual ArithmeticOp*     as_ArithmeticOp()    { return nullptr; }\n+  virtual ShiftOp*          as_ShiftOp()         { return nullptr; }\n+  virtual LogicOp*          as_LogicOp()         { return nullptr; }\n+  virtual CompareOp*        as_CompareOp()       { return nullptr; }\n+  virtual IfOp*             as_IfOp()            { return nullptr; }\n+  virtual Convert*          as_Convert()         { return nullptr; }\n+  virtual NullCheck*        as_NullCheck()       { return nullptr; }\n+  virtual OsrEntry*         as_OsrEntry()        { return nullptr; }\n+  virtual StateSplit*       as_StateSplit()      { return nullptr; }\n+  virtual Invoke*           as_Invoke()          { return nullptr; }\n+  virtual NewInstance*      as_NewInstance()     { return nullptr; }\n+  virtual NewInlineTypeInstance* as_NewInlineTypeInstance() { return nullptr; }\n+  virtual NewArray*         as_NewArray()        { return nullptr; }\n+  virtual NewTypeArray*     as_NewTypeArray()    { return nullptr; }\n+  virtual NewObjectArray*   as_NewObjectArray()  { return nullptr; }\n+  virtual NewMultiArray*    as_NewMultiArray()   { return nullptr; }\n+  virtual Deoptimize*       as_Deoptimize()      { return nullptr; }\n+  virtual TypeCheck*        as_TypeCheck()       { return nullptr; }\n+  virtual CheckCast*        as_CheckCast()       { return nullptr; }\n+  virtual InstanceOf*       as_InstanceOf()      { return nullptr; }\n+  virtual TypeCast*         as_TypeCast()        { return nullptr; }\n+  virtual AccessMonitor*    as_AccessMonitor()   { return nullptr; }\n+  virtual MonitorEnter*     as_MonitorEnter()    { return nullptr; }\n+  virtual MonitorExit*      as_MonitorExit()     { return nullptr; }\n+  virtual Intrinsic*        as_Intrinsic()       { return nullptr; }\n+  virtual BlockBegin*       as_BlockBegin()      { return nullptr; }\n+  virtual BlockEnd*         as_BlockEnd()        { return nullptr; }\n+  virtual Goto*             as_Goto()            { return nullptr; }\n+  virtual If*               as_If()              { return nullptr; }\n+  virtual TableSwitch*      as_TableSwitch()     { return nullptr; }\n+  virtual LookupSwitch*     as_LookupSwitch()    { return nullptr; }\n+  virtual Return*           as_Return()          { return nullptr; }\n+  virtual Throw*            as_Throw()           { return nullptr; }\n+  virtual Base*             as_Base()            { return nullptr; }\n+  virtual RoundFP*          as_RoundFP()         { return nullptr; }\n+  virtual ExceptionObject*  as_ExceptionObject() { return nullptr; }\n+  virtual UnsafeOp*         as_UnsafeOp()        { return nullptr; }\n+  virtual ProfileInvoke*    as_ProfileInvoke()   { return nullptr; }\n+  virtual RangeCheckPredicate* as_RangeCheckPredicate() { return nullptr; }\n@@ -602,1 +602,1 @@\n-  virtual Assert*           as_Assert()          { return NULL; }\n+  virtual Assert*           as_Assert()          { return nullptr; }\n@@ -615,1 +615,1 @@\n-  virtual ciType* declared_type() const          { return NULL; }\n+  virtual ciType* declared_type() const          { return nullptr; }\n@@ -651,1 +651,1 @@\n-  void visit(Value* x)             { assert((*x) != NULL, \"value must exist\"); }\n+  void visit(Value* x)             { assert((*x) != nullptr, \"value must exist\"); }\n@@ -751,1 +751,1 @@\n-      Instruction(type, NULL, \/*type_is_constant*\/ true)\n+      Instruction(type, nullptr, \/*type_is_constant*\/ true)\n@@ -759,1 +759,1 @@\n-    assert(state_before != NULL, \"only used for constants which need patching\");\n+    assert(state_before != nullptr, \"only used for constants which need patching\");\n@@ -766,1 +766,1 @@\n-  virtual bool can_trap() const                  { return state_before() != NULL; }\n+  virtual bool can_trap() const                  { return state_before() != nullptr; }\n@@ -783,1 +783,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -790,1 +790,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -811,1 +811,1 @@\n-  , _explicit_null_check(NULL)\n+  , _explicit_null_check(nullptr)\n@@ -855,1 +855,1 @@\n-            ciInlineKlass* inline_klass = NULL, Value default_value = NULL )\n+            ciInlineKlass* inline_klass = nullptr, Value default_value = nullptr )\n@@ -920,1 +920,1 @@\n-  , _explicit_null_check(NULL) {}\n+  , _explicit_null_check(nullptr) {}\n@@ -951,1 +951,1 @@\n-  , _profiled_method(NULL), _profiled_bci(0)\n+  , _profiled_method(nullptr), _profiled_bci(0)\n@@ -963,1 +963,1 @@\n-  void clear_length()                            { _length = NULL; }\n+  void clear_length()                            { _length = nullptr; }\n@@ -976,2 +976,2 @@\n-\/\/ generic\n-  virtual void input_values_do(ValueVisitor* f)   { AccessArray::input_values_do(f); f->visit(&_index); if (_length != NULL) f->visit(&_length); }\n+  \/\/ generic\n+  virtual void input_values_do(ValueVisitor* f)   { AccessArray::input_values_do(f); f->visit(&_index); if (_length != nullptr) f->visit(&_length); }\n@@ -992,1 +992,1 @@\n-  , _explicit_null_check(NULL), _vt(NULL), _delayed(NULL) {}\n+  , _explicit_null_check(nullptr), _vt(nullptr), _delayed(nullptr) {}\n@@ -1011,1 +1011,1 @@\n-  HASHING4(LoadIndexed, delayed() == NULL && !should_profile(), type()->tag(), array()->subst(), index()->subst(), vt())\n+  HASHING4(LoadIndexed, delayed() == nullptr && !should_profile(), type()->tag(), array()->subst(), index()->subst(), vt())\n@@ -1024,1 +1024,1 @@\n-  , _field(NULL)\n+  , _field(nullptr)\n@@ -1088,1 +1088,1 @@\n-  Op2(ValueType* type, Bytecodes::Code op, Value x, Value y, ValueStack* state_before = NULL)\n+  Op2(ValueType* type, Bytecodes::Code op, Value x, Value y, ValueStack* state_before = nullptr)\n@@ -1276,1 +1276,1 @@\n-  StateSplit(ValueType* type, ValueStack* state_before = NULL)\n+  StateSplit(ValueType* type, ValueStack* state_before = nullptr)\n@@ -1278,1 +1278,1 @@\n-  , _state(NULL)\n+  , _state(nullptr)\n@@ -1288,1 +1288,1 @@\n-  void set_state(ValueStack* state)              { assert(_state == NULL, \"overwriting existing state\"); check_state(state); _state = state; }\n+  void set_state(ValueStack* state)              { assert(_state == nullptr, \"overwriting existing state\"); check_state(state); _state = state; }\n@@ -1312,1 +1312,1 @@\n-  bool has_receiver() const                      { return receiver() != NULL; }\n+  bool has_receiver() const                      { return receiver() != nullptr; }\n@@ -1424,1 +1424,1 @@\n-    \/\/ Do not ASSERT_VALUES since length is NULL for NewMultiArray\n+    \/\/ Do not ASSERT_VALUES since length is null for NewMultiArray\n@@ -1432,1 +1432,1 @@\n-  ciType* exact_type() const                     { return NULL; }\n+  ciType* exact_type() const                     { return nullptr; }\n@@ -1482,1 +1482,1 @@\n-  NewMultiArray(ciKlass* klass, Values* dims, ValueStack* state_before) : NewArray(NULL, state_before), _klass(klass), _dims(dims) {\n+  NewMultiArray(ciKlass* klass, Values* dims, ValueStack* state_before) : NewArray(nullptr, state_before), _klass(klass), _dims(dims) {\n@@ -1530,1 +1530,1 @@\n-    _profiled_method(NULL), _profiled_bci(0) {\n+    _profiled_method(nullptr), _profiled_bci(0) {\n@@ -1538,1 +1538,1 @@\n-  bool is_loaded() const                         { return klass() != NULL; }\n+  bool is_loaded() const                         { return klass() != nullptr; }\n@@ -1603,1 +1603,1 @@\n-  AccessMonitor(Value obj, int monitor_no, ValueStack* state_before = NULL)\n+  AccessMonitor(Value obj, int monitor_no, ValueStack* state_before = nullptr)\n@@ -1644,1 +1644,1 @@\n-  : AccessMonitor(obj, monitor_no, NULL)\n+  : AccessMonitor(obj, monitor_no, nullptr)\n@@ -1676,1 +1676,1 @@\n-  , _recv(NULL)\n+  , _recv(nullptr)\n@@ -1678,1 +1678,1 @@\n-    assert(args != NULL, \"args must exist\");\n+    assert(args != nullptr, \"args must exist\");\n@@ -1698,1 +1698,1 @@\n-  bool has_receiver() const                      { return (_recv != NULL); }\n+  bool has_receiver() const                      { return (_recv != nullptr); }\n@@ -1792,2 +1792,2 @@\n-  , _dominator(NULL)\n-  , _end(NULL)\n+  , _dominator(nullptr)\n+  , _end(nullptr)\n@@ -1795,1 +1795,1 @@\n-  , _exception_states(NULL)\n+  , _exception_states(nullptr)\n@@ -1797,1 +1797,1 @@\n-  , _lir(NULL)\n+  , _lir(nullptr)\n@@ -1803,1 +1803,1 @@\n-  , _fpu_stack_state(NULL)\n+  , _fpu_stack_state(nullptr)\n@@ -1879,1 +1879,1 @@\n-  int number_of_exception_states()               { assert(is_set(exception_entry_flag), \"only for xhandlers\"); return _exception_states == NULL ? 0 : _exception_states->length(); }\n+  int number_of_exception_states()               { assert(is_set(exception_entry_flag), \"only for xhandlers\"); return _exception_states == nullptr ? 0 : _exception_states->length(); }\n@@ -1941,2 +1941,2 @@\n-    assert(sux != NULL, \"sux must exist\");\n-    for (int i = sux->length() - 1; i >= 0; i--) assert(sux->at(i) != NULL, \"sux must exist\");\n+    assert(sux != nullptr, \"sux must exist\");\n+    for (int i = sux->length() - 1; i >= 0; i--) assert(sux->at(i) != nullptr, \"sux must exist\");\n@@ -1951,1 +1951,1 @@\n-  , _sux(NULL)\n+  , _sux(nullptr)\n@@ -1966,1 +1966,1 @@\n-  int number_of_sux() const                      { return _sux != NULL ? _sux->length() : 0; }\n+  int number_of_sux() const                      { return _sux != nullptr ? _sux->length() : 0; }\n@@ -1968,1 +1968,1 @@\n-  bool is_sux(BlockBegin* sux) const             { return _sux == NULL ? false : _sux->contains(sux); }\n+  bool is_sux(BlockBegin* sux) const             { return _sux == nullptr ? false : _sux->contains(sux); }\n@@ -1988,1 +1988,1 @@\n-    , _profiled_method(NULL)\n+    , _profiled_method(nullptr)\n@@ -1996,2 +1996,2 @@\n-  Goto(BlockBegin* sux, bool is_safepoint) : BlockEnd(illegalType, NULL, is_safepoint)\n-                                           , _profiled_method(NULL)\n+  Goto(BlockBegin* sux, bool is_safepoint) : BlockEnd(illegalType, nullptr, is_safepoint)\n+                                           , _profiled_method(nullptr)\n@@ -2068,1 +2068,1 @@\n-    _x = _y = NULL;\n+    _x = _y = nullptr;\n@@ -2078,1 +2078,1 @@\n-  void always_fail()                             { _x = _y = NULL; }\n+  void always_fail()                             { _x = _y = nullptr; }\n@@ -2103,1 +2103,1 @@\n-  , _profiled_method(NULL)\n+  , _profiled_method(nullptr)\n@@ -2116,2 +2116,2 @@\n-      assert(x->as_NewInlineTypeInstance() == NULL || y->type() == objectNull, \"Sanity check\");\n-      assert(y->as_NewInlineTypeInstance() == NULL || x->type() == objectNull, \"Sanity check\");\n+      assert(x->as_NewInlineTypeInstance() == nullptr || y->type() == objectNull, \"Sanity check\");\n+      assert(y->as_NewInlineTypeInstance() == nullptr || x->type() == objectNull, \"Sanity check\");\n@@ -2200,1 +2200,1 @@\n-    assert(keys != NULL, \"keys must exist\");\n+    assert(keys != nullptr, \"keys must exist\");\n@@ -2216,1 +2216,1 @@\n-    BlockEnd(result == NULL ? voidType : result->type()->base(), NULL, true),\n+    BlockEnd(result == nullptr ? voidType : result->type()->base(), nullptr, true),\n@@ -2221,1 +2221,1 @@\n-  bool has_result() const                        { return result() != NULL; }\n+  bool has_result() const                        { return result() != nullptr; }\n@@ -2253,1 +2253,1 @@\n-  Base(BlockBegin* std_entry, BlockBegin* osr_entry) : BlockEnd(illegalType, NULL, false) {\n+  Base(BlockBegin* std_entry, BlockBegin* osr_entry) : BlockEnd(illegalType, nullptr, false) {\n@@ -2255,1 +2255,1 @@\n-    assert(osr_entry == NULL || osr_entry->is_set(BlockBegin::osr_entry_flag), \"osr entry must be flagged\");\n+    assert(osr_entry == nullptr || osr_entry->is_set(BlockBegin::osr_entry_flag), \"osr entry must be flagged\");\n@@ -2257,1 +2257,1 @@\n-    if (osr_entry != NULL) s->append(osr_entry);\n+    if (osr_entry != nullptr) s->append(osr_entry);\n@@ -2264,1 +2264,1 @@\n-  BlockBegin* osr_entry() const                  { return number_of_sux() < 2 ? NULL : sux_at(0); }\n+  BlockBegin* osr_entry() const                  { return number_of_sux() < 2 ? nullptr : sux_at(0); }\n@@ -2443,1 +2443,1 @@\n-  int nb_profiled_args()         const { return _obj_args == NULL ? 0 : _obj_args->length(); }\n+  int nb_profiled_args()         const { return _obj_args == nullptr ? 0 : _obj_args->length(); }\n@@ -2455,1 +2455,1 @@\n-    if (_recv != NULL) {\n+    if (_recv != nullptr) {\n@@ -2490,1 +2490,1 @@\n-    if (_ret != NULL) {\n+    if (_ret != nullptr) {\n@@ -2529,1 +2529,1 @@\n-    if (_left != NULL) {\n+    if (_left != nullptr) {\n@@ -2532,1 +2532,1 @@\n-    if (_right != NULL) {\n+    if (_right != nullptr) {\n@@ -2625,2 +2625,2 @@\n-inline int         BlockBegin::number_of_sux() const            { assert(_end != NULL, \"need end\"); return _end->number_of_sux(); }\n-inline BlockBegin* BlockBegin::sux_at(int i) const              { assert(_end != NULL , \"need end\"); return _end->sux_at(i); }\n+inline int         BlockBegin::number_of_sux() const            { assert(_end != nullptr, \"need end\"); return _end->number_of_sux(); }\n+inline BlockBegin* BlockBegin::sux_at(int i) const              { assert(_end != nullptr , \"need end\"); return _end->sux_at(i); }\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":139,"deletions":139,"binary":false,"changes":278,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-  if (n == NULL || type > T_VOID) {\n+  if (n == nullptr || type > T_VOID) {\n@@ -58,1 +58,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -106,1 +106,1 @@\n-  Phi* phi = v ? v->as_Phi() : NULL;\n+  Phi* phi = v ? v->as_Phi() : nullptr;\n@@ -115,1 +115,1 @@\n-  Phi* phi = v ? v->as_Phi() : NULL;\n+  Phi* phi = v ? v->as_Phi() : nullptr;\n@@ -127,1 +127,1 @@\n-  if (type->as_ObjectConstant() != NULL) {\n+  if (type->as_ObjectConstant() != nullptr) {\n@@ -138,1 +138,1 @@\n-  } else if (type->as_InstanceConstant() != NULL) {\n+  } else if (type->as_InstanceConstant() != nullptr) {\n@@ -147,1 +147,1 @@\n-  } else if (type->as_ArrayConstant() != NULL) {\n+  } else if (type->as_ArrayConstant() != nullptr) {\n@@ -149,1 +149,1 @@\n-  } else if (type->as_ClassConstant() != NULL) {\n+  } else if (type->as_ClassConstant() != nullptr) {\n@@ -156,1 +156,1 @@\n-  } else if (type->as_MethodConstant() != NULL) {\n+  } else if (type->as_MethodConstant() != nullptr) {\n@@ -181,1 +181,1 @@\n-  if (indexed->length() != NULL) {\n+  if (indexed->length() != nullptr) {\n@@ -204,2 +204,2 @@\n-  if (value == NULL) {\n-    output()->print(\"NULL\");\n+  if (value == nullptr) {\n+    output()->print(\"null\");\n@@ -229,1 +229,1 @@\n-      if (phi != NULL) {\n+      if (phi != nullptr) {\n@@ -248,1 +248,1 @@\n-      if (t == NULL) {\n+      if (t == nullptr) {\n@@ -284,1 +284,1 @@\n-      else output()->print(\"NULL\");\n+      else output()->print(\"null\");\n@@ -328,1 +328,1 @@\n-  if (split != NULL && split->state() != NULL && !split->state()->stack_is_empty()) {\n+  if (split != nullptr && split->state() != nullptr && !split->state()->stack_is_empty()) {\n@@ -473,1 +473,1 @@\n-  if (x->receiver() != NULL) {\n+  if (x->receiver() != nullptr) {\n@@ -550,2 +550,2 @@\n-  if (strchr(name, '_') == NULL) {\n-    kname = NULL;\n+  if (strchr(name, '_') == nullptr) {\n+    kname = nullptr;\n@@ -554,1 +554,1 @@\n-    if (kptr != NULL)  kname = kptr + 1;\n+    if (kptr != nullptr)  kname = kptr + 1;\n@@ -556,1 +556,1 @@\n-  if (kname == NULL)\n+  if (kname == nullptr)\n@@ -606,1 +606,1 @@\n-  output()->print(\"[%d, %d]\", x->bci(), (end == NULL ? -1 : end->printable_bci()));\n+  output()->print(\"[%d, %d]\", x->bci(), (end == nullptr ? -1 : end->printable_bci()));\n@@ -609,1 +609,1 @@\n-  if (end != NULL && end->number_of_sux() > 0) {\n+  if (end != nullptr && end->number_of_sux() > 0) {\n@@ -626,1 +626,1 @@\n-  if (x->dominator() != NULL) {\n+  if (x->dominator() != nullptr) {\n@@ -663,1 +663,1 @@\n-    } while (state != NULL);\n+    } while (state != nullptr);\n@@ -685,1 +685,1 @@\n-    } while (state != NULL);\n+    } while (state != nullptr);\n@@ -766,1 +766,1 @@\n-  if (x->result() == NULL) {\n+  if (x->result() == nullptr) {\n@@ -825,1 +825,1 @@\n-  if (x->x() != NULL && x->y() != NULL) {\n+  if (x->x() != nullptr && x->y() != nullptr) {\n@@ -849,1 +849,1 @@\n-  if (x->known_holder() != NULL) {\n+  if (x->known_holder() != nullptr) {\n","filename":"src\/hotspot\/share\/c1\/c1_InstructionPrinter.cpp","additions":30,"deletions":30,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -57,3 +57,3 @@\n-    if (c != NULL && !c->value()->is_loaded()) {\n-      return LIR_OprFact::metadataConst(NULL);\n-    } else if (c != NULL) {\n+    if (c != nullptr && !c->value()->is_loaded()) {\n+      return LIR_OprFact::metadataConst(nullptr);\n+    } else if (c != nullptr) {\n@@ -63,1 +63,1 @@\n-      assert (m != NULL, \"not a class or a method?\");\n+      assert (m != nullptr, \"not a class or a method?\");\n@@ -242,1 +242,1 @@\n-  : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n+  : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)nullptr)\n@@ -245,2 +245,2 @@\n-  , _ublock(NULL)\n-  , _stub(NULL) {\n+  , _ublock(nullptr)\n+  , _stub(nullptr) {\n@@ -250,1 +250,1 @@\n-  LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n+  LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)nullptr)\n@@ -252,2 +252,2 @@\n-  , _block(NULL)\n-  , _ublock(NULL)\n+  , _block(nullptr)\n+  , _ublock(nullptr)\n@@ -258,1 +258,1 @@\n-  : LIR_Op2(lir_cond_float_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n+  : LIR_Op2(lir_cond_float_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)nullptr)\n@@ -262,1 +262,1 @@\n-  , _stub(NULL)\n+  , _stub(nullptr)\n@@ -267,1 +267,1 @@\n-  assert(_block != NULL, \"must have old block\");\n+  assert(_block != nullptr, \"must have old block\");\n@@ -275,1 +275,1 @@\n-  assert(_ublock != NULL, \"must have old block\");\n+  assert(_ublock != nullptr, \"must have old block\");\n@@ -297,1 +297,1 @@\n-  : LIR_Op(code, result, NULL)\n+  : LIR_Op(code, result, nullptr)\n@@ -308,1 +308,1 @@\n-  , _profiled_method(NULL)\n+  , _profiled_method(nullptr)\n@@ -314,1 +314,1 @@\n-    assert(info_for_exception != NULL, \"checkcast throws exceptions\");\n+    assert(info_for_exception != nullptr, \"checkcast throws exceptions\");\n@@ -316,1 +316,1 @@\n-    assert(info_for_exception == NULL, \"instanceof throws no exceptions\");\n+    assert(info_for_exception == nullptr, \"instanceof throws no exceptions\");\n@@ -325,1 +325,1 @@\n-  : LIR_Op(code, LIR_OprFact::illegalOpr, NULL)\n+  : LIR_Op(code, LIR_OprFact::illegalOpr, nullptr)\n@@ -328,1 +328,1 @@\n-  , _klass(NULL)\n+  , _klass(nullptr)\n@@ -333,1 +333,1 @@\n-  , _info_for_patch(NULL)\n+  , _info_for_patch(nullptr)\n@@ -335,2 +335,2 @@\n-  , _stub(NULL)\n-  , _profiled_method(NULL)\n+  , _stub(nullptr)\n+  , _profiled_method(nullptr)\n@@ -343,1 +343,1 @@\n-    assert(info_for_exception != NULL, \"store_check throws exceptions\");\n+    assert(info_for_exception != nullptr, \"store_check throws exceptions\");\n@@ -350,1 +350,1 @@\n-  : LIR_Op(lir_flattened_array_check, LIR_OprFact::illegalOpr, NULL)\n+  : LIR_Op(lir_flattened_array_check, LIR_OprFact::illegalOpr, nullptr)\n@@ -358,1 +358,1 @@\n-  : LIR_Op(lir_null_free_array_check, LIR_OprFact::illegalOpr, NULL)\n+  : LIR_Op(lir_null_free_array_check, LIR_OprFact::illegalOpr, nullptr)\n@@ -396,1 +396,1 @@\n-  : LIR_Op(lir_updatecrc32, res, NULL)\n+  : LIR_Op(lir_updatecrc32, res, nullptr)\n@@ -449,2 +449,2 @@\n-      assert(op->as_Op0() != NULL, \"must be\");\n-      assert(op->_info == NULL, \"info not used by this instruction\");\n+      assert(op->as_Op0() != nullptr, \"must be\");\n+      assert(op->_info == nullptr, \"info not used by this instruction\");\n@@ -460,2 +460,2 @@\n-      assert(op->as_Op0() != NULL, \"must be\");\n-      if (op->_info != NULL)           do_info(op->_info);\n+      assert(op->as_Op0() != nullptr, \"must be\");\n+      if (op->_info != nullptr)           do_info(op->_info);\n@@ -470,2 +470,2 @@\n-      assert(op->as_OpLabel() != NULL, \"must be\");\n-      assert(op->_info == NULL, \"info not used by this instruction\");\n+      assert(op->as_OpLabel() != nullptr, \"must be\");\n+      assert(op->_info == nullptr, \"info not used by this instruction\");\n@@ -487,1 +487,1 @@\n-      assert(op->as_Op1() != NULL, \"must be\");\n+      assert(op->as_Op1() != nullptr, \"must be\");\n@@ -499,1 +499,1 @@\n-      assert(op->as_OpReturn() != NULL, \"must be\");\n+      assert(op->as_OpReturn() != nullptr, \"must be\");\n@@ -505,1 +505,1 @@\n-      if (op_ret->stub() != NULL)      do_stub(op_ret->stub());\n+      if (op_ret->stub() != nullptr)      do_stub(op_ret->stub());\n@@ -512,1 +512,1 @@\n-      assert(op->as_Op1() != NULL, \"must be\");\n+      assert(op->as_Op1() != nullptr, \"must be\");\n@@ -515,1 +515,1 @@\n-      assert(op1->_info != NULL, \"\");  do_info(op1->_info);\n+      assert(op1->_info != nullptr, \"\");  do_info(op1->_info);\n@@ -525,1 +525,1 @@\n-      assert(op->as_OpConvert() != NULL, \"must be\");\n+      assert(op->as_OpConvert() != nullptr, \"must be\");\n@@ -528,1 +528,1 @@\n-      assert(opConvert->_info == NULL, \"must be\");\n+      assert(opConvert->_info == nullptr, \"must be\");\n@@ -540,1 +540,1 @@\n-      assert(op->as_OpBranch() != NULL, \"must be\");\n+      assert(op->as_OpBranch() != nullptr, \"must be\");\n@@ -550,1 +550,1 @@\n-      if (opBranch->_info != NULL)     do_info(opBranch->_info);\n+      if (opBranch->_info != nullptr)  do_info(opBranch->_info);\n@@ -552,1 +552,1 @@\n-      if (opBranch->_stub != NULL)     opBranch->stub()->visit(this);\n+      if (opBranch->_stub != nullptr)  opBranch->stub()->visit(this);\n@@ -561,1 +561,1 @@\n-      assert(op->as_OpAllocObj() != NULL, \"must be\");\n+      assert(op->as_OpAllocObj() != nullptr, \"must be\");\n@@ -580,1 +580,1 @@\n-      assert(op->as_OpRoundFP() != NULL, \"must be\");\n+      assert(op->as_OpRoundFP() != nullptr, \"must be\");\n@@ -583,1 +583,1 @@\n-      assert(op->_info == NULL, \"info not used by this instruction\");\n+      assert(op->_info == nullptr, \"info not used by this instruction\");\n@@ -615,1 +615,1 @@\n-      assert(op->as_Op2() != NULL, \"must be\");\n+      assert(op->as_Op2() != nullptr, \"must be\");\n@@ -640,1 +640,1 @@\n-      assert(op->as_Op4() != NULL, \"must be\");\n+      assert(op->as_Op4() != nullptr, \"must be\");\n@@ -643,1 +643,1 @@\n-      assert(op4->_info == NULL && op4->_tmp1->is_illegal() && op4->_tmp2->is_illegal() &&\n+      assert(op4->_info == nullptr && op4->_tmp1->is_illegal() && op4->_tmp2->is_illegal() &&\n@@ -663,1 +663,1 @@\n-      assert(op->as_Op2() != NULL, \"must be\");\n+      assert(op->as_Op2() != nullptr, \"must be\");\n@@ -666,1 +666,1 @@\n-      assert(op2->_info == NULL, \"not used\");\n+      assert(op2->_info == nullptr, \"not used\");\n@@ -682,1 +682,1 @@\n-      assert(op->as_Op2() != NULL, \"must be\");\n+      assert(op->as_Op2() != nullptr, \"must be\");\n@@ -696,1 +696,1 @@\n-      assert(op->as_Op1() != NULL, \"must be\");\n+      assert(op->as_Op1() != nullptr, \"must be\");\n@@ -699,1 +699,1 @@\n-      assert(op1->_info == NULL, \"no info\");\n+      assert(op1->_info == nullptr, \"no info\");\n@@ -709,1 +709,1 @@\n-      assert(op->as_Op3() != NULL, \"must be\");\n+      assert(op->as_Op3() != nullptr, \"must be\");\n@@ -728,1 +728,1 @@\n-      assert(op->as_Op3() != NULL, \"must be\");\n+      assert(op->as_Op3() != nullptr, \"must be\");\n@@ -730,1 +730,1 @@\n-      assert(op3->_info == NULL, \"no info\");\n+      assert(op3->_info == nullptr, \"no info\");\n@@ -744,1 +744,1 @@\n-      assert(opJavaCall != NULL, \"must be\");\n+      assert(opJavaCall != nullptr, \"must be\");\n@@ -771,1 +771,1 @@\n-      assert(op->as_OpRTCall() != NULL, \"must be\");\n+      assert(op->as_OpRTCall() != nullptr, \"must be\");\n@@ -792,1 +792,1 @@\n-      assert(op->as_OpArrayCopy() != NULL, \"must be\");\n+      assert(op->as_OpArrayCopy() != nullptr, \"must be\");\n@@ -813,1 +813,1 @@\n-      assert(op->as_OpUpdateCRC32() != NULL, \"must be\");\n+      assert(op->as_OpUpdateCRC32() != nullptr, \"must be\");\n@@ -819,1 +819,1 @@\n-      assert(opUp->_info == NULL, \"no info for LIR_OpUpdateCRC32\");\n+      assert(opUp->_info == nullptr, \"no info for LIR_OpUpdateCRC32\");\n@@ -828,1 +828,1 @@\n-      assert(op->as_OpLock() != NULL, \"must be\");\n+      assert(op->as_OpLock() != nullptr, \"must be\");\n@@ -851,1 +851,1 @@\n-      assert(op->as_OpDelay() != NULL, \"must be\");\n+      assert(op->as_OpDelay() != nullptr, \"must be\");\n@@ -862,1 +862,1 @@\n-      assert(op->as_OpTypeCheck() != NULL, \"must be\");\n+      assert(op->as_OpTypeCheck() != nullptr, \"must be\");\n@@ -882,1 +882,1 @@\n-      assert(op->as_OpFlattenedArrayCheck() != NULL, \"must be\");\n+      assert(op->as_OpFlattenedArrayCheck() != nullptr, \"must be\");\n@@ -896,1 +896,1 @@\n-      assert(op->as_OpNullFreeArrayCheck() != NULL, \"must be\");\n+      assert(op->as_OpNullFreeArrayCheck() != nullptr, \"must be\");\n@@ -906,1 +906,1 @@\n-      assert(op->as_OpSubstitutabilityCheck() != NULL, \"must be\");\n+      assert(op->as_OpSubstitutabilityCheck() != nullptr, \"must be\");\n@@ -931,1 +931,1 @@\n-      assert(op->as_OpCompareAndSwap() != NULL, \"must be\");\n+      assert(op->as_OpCompareAndSwap() != nullptr, \"must be\");\n@@ -951,1 +951,1 @@\n-      assert(op->as_OpAllocArray() != NULL, \"must be\");\n+      assert(op->as_OpAllocArray() != nullptr, \"must be\");\n@@ -974,1 +974,1 @@\n-      assert(opLoadKlass != NULL, \"must be\");\n+      assert(opLoadKlass != nullptr, \"must be\");\n@@ -985,1 +985,1 @@\n-      assert(op->as_OpProfileCall() != NULL, \"must be\");\n+      assert(op->as_OpProfileCall() != nullptr, \"must be\");\n@@ -996,1 +996,1 @@\n-      assert(op->as_OpProfileType() != NULL, \"must be\");\n+      assert(op->as_OpProfileType() != nullptr, \"must be\");\n@@ -1007,1 +1007,1 @@\n-      assert(op->as_OpProfileInlineType() != NULL, \"must be\");\n+      assert(op->as_OpProfileInlineType() != nullptr, \"must be\");\n@@ -1025,1 +1025,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -1031,1 +1031,1 @@\n-  XHandlers* result = NULL;\n+  XHandlers* result = nullptr;\n@@ -1035,1 +1035,1 @@\n-    if (info_at(i)->exception_handlers() != NULL) {\n+    if (info_at(i)->exception_handlers() != nullptr) {\n@@ -1043,1 +1043,1 @@\n-    assert(info_at(i)->exception_handlers() == NULL ||\n+    assert(info_at(i)->exception_handlers() == nullptr ||\n@@ -1049,1 +1049,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -1074,2 +1074,2 @@\n-    LIR_Op1(lir_return, opr, (CodeEmitInfo*)NULL \/* info *\/),\n-    _stub(NULL) {\n+    LIR_Op1(lir_return, opr, (CodeEmitInfo*)nullptr \/* info *\/),\n+    _stub(nullptr) {\n@@ -1094,1 +1094,1 @@\n-        if (vk_ret != NULL) {\n+        if (vk_ret != nullptr) {\n@@ -1155,1 +1155,1 @@\n-  if (stub() != NULL) {\n+  if (stub() != nullptr) {\n@@ -1178,1 +1178,1 @@\n-  if (stub() != NULL) {\n+  if (stub() != nullptr) {\n@@ -1189,1 +1189,1 @@\n-  if (stub() != NULL) {\n+  if (stub() != nullptr) {\n@@ -1250,1 +1250,1 @@\n-  , _file(NULL)\n+  , _file(nullptr)\n@@ -1263,2 +1263,2 @@\n-  if (f == NULL) f = strrchr(file, '\\\\');\n-  if (f == NULL) {\n+  if (f == nullptr) f = strrchr(file, '\\\\');\n+  if (f == nullptr) {\n@@ -1295,0 +1295,6 @@\n+    case lir_cas_long:\n+    case lir_cas_obj:\n+    case lir_cas_int:\n+      _cmp_opr1 = op->as_OpCompareAndSwap()->result_opr();\n+      _cmp_opr2 = LIR_OprFact::intConst(0);\n+      break;\n@@ -1296,1 +1302,1 @@\n-    case lir_zloadbarrier_test:\n+    case lir_xloadbarrier_test:\n@@ -1313,1 +1319,1 @@\n-    _operations.at_grow(n + buffer->number_of_ops() - 1, NULL);\n+    _operations.at_grow(n + buffer->number_of_ops() - 1, nullptr);\n@@ -1579,1 +1585,1 @@\n-                    NULL));\n+                    nullptr));\n@@ -1600,1 +1606,1 @@\n-  if (profiled_method != NULL) {\n+  if (profiled_method != nullptr) {\n@@ -1609,2 +1615,2 @@\n-  LIR_OpTypeCheck* c = new LIR_OpTypeCheck(lir_instanceof, result, object, klass, tmp1, tmp2, tmp3, fast_check, NULL, info_for_patch, NULL);\n-  if (profiled_method != NULL) {\n+  LIR_OpTypeCheck* c = new LIR_OpTypeCheck(lir_instanceof, result, object, klass, tmp1, tmp2, tmp3, fast_check, nullptr, info_for_patch, nullptr);\n+  if (profiled_method != nullptr) {\n@@ -1623,1 +1629,1 @@\n-  if (profiled_method != NULL) {\n+  if (profiled_method != nullptr) {\n@@ -1635,1 +1641,1 @@\n-    cmp(lir_cond_equal, opr, LIR_OprFact::oopConst(NULL));\n+    cmp(lir_cond_equal, opr, LIR_OprFact::oopConst(nullptr));\n@@ -1797,1 +1803,1 @@\n-  tty->print(\"[%d, %d] \", x->bci(), (end == NULL ? -1 : end->printable_bci()));\n+  tty->print(\"[%d, %d] \", x->bci(), (end == nullptr ? -1 : end->printable_bci()));\n@@ -1807,1 +1813,1 @@\n-  if (end != NULL && x->number_of_sux() > 0) {\n+  if (end != nullptr && x->number_of_sux() > 0) {\n@@ -1853,1 +1859,1 @@\n-  if (info() != NULL) out->print(\" [bci:%d]\", info()->stack()->bci());\n+  if (info() != nullptr) out->print(\" [bci:%d]\", info()->stack()->bci());\n@@ -1855,1 +1861,1 @@\n-  if (Verbose && _file != NULL) {\n+  if (Verbose && _file != nullptr) {\n@@ -1862,1 +1868,1 @@\n-  const char* s = NULL;\n+  const char* s = nullptr;\n@@ -2074,1 +2080,1 @@\n-  if (block() != NULL) {\n+  if (block() != nullptr) {\n@@ -2076,1 +2082,1 @@\n-  } else if (stub() != NULL) {\n+  } else if (stub() != nullptr) {\n@@ -2080,1 +2086,1 @@\n-    if (stub()->info() != NULL) out->print(\" [bci:%d]\", stub()->info()->stack()->bci());\n+    if (stub()->info() != nullptr) out->print(\" [bci:%d]\", stub()->info()->stack()->bci());\n@@ -2084,1 +2090,1 @@\n-  if (ublock() != NULL) {\n+  if (ublock() != nullptr) {\n@@ -2193,1 +2199,1 @@\n-  if (info_for_exception() != NULL) out->print(\" [bci:%d]\", info_for_exception()->stack()->bci());\n+  if (info_for_exception() != nullptr) out->print(\" [bci:%d]\", info_for_exception()->stack()->bci());\n@@ -2200,1 +2206,1 @@\n-  if (stub() != NULL) {\n+  if (stub() != nullptr) {\n@@ -2218,1 +2224,1 @@\n-  if (left_klass() == NULL) {\n+  if (left_klass() == nullptr) {\n@@ -2223,1 +2229,1 @@\n-  if (right_klass() == NULL) {\n+  if (right_klass() == nullptr) {\n@@ -2230,1 +2236,1 @@\n-  if (stub() != NULL) {\n+  if (stub() != nullptr) {\n@@ -2297,1 +2303,1 @@\n-  if  (exact_klass() == NULL) {\n+  if (exact_klass() == nullptr) {\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":117,"deletions":111,"binary":false,"changes":228,"status":"modified"},{"patch":"@@ -71,2 +71,2 @@\n-  virtual LIR_Const*  as_constant()              { return NULL; }\n-  virtual LIR_Address* as_address()              { return NULL; }\n+  virtual LIR_Const*  as_constant()              { return nullptr; }\n+  virtual LIR_Address* as_address()              { return nullptr; }\n@@ -388,2 +388,2 @@\n-  bool is_constant() const     { return is_pointer() && pointer()->as_constant() != NULL; }\n-  bool is_address() const      { return is_pointer() && pointer()->as_address() != NULL; }\n+  bool is_constant() const     { return is_pointer() && pointer()->as_constant() != nullptr; }\n+  bool is_address() const      { return is_pointer() && pointer()->as_address() != nullptr; }\n@@ -1040,3 +1040,3 @@\n-  , begin_opZLoadBarrierTest\n-    , lir_zloadbarrier_test\n-  , end_opZLoadBarrierTest\n+  , begin_opXLoadBarrierTest\n+    , lir_xloadbarrier_test\n+  , end_opXLoadBarrierTest\n@@ -1107,1 +1107,1 @@\n-      _file(NULL)\n+      _file(nullptr)\n@@ -1113,1 +1113,1 @@\n-    , _info(NULL)\n+    , _info(nullptr)\n@@ -1116,1 +1116,1 @@\n-    , _source(NULL) {}\n+    , _source(nullptr) {}\n@@ -1121,1 +1121,1 @@\n-      _file(NULL)\n+      _file(nullptr)\n@@ -1130,1 +1130,1 @@\n-    , _source(NULL) {}\n+    , _source(nullptr) {}\n@@ -1163,28 +1163,28 @@\n-  virtual LIR_OpCall* as_OpCall() { return NULL; }\n-  virtual LIR_OpJavaCall* as_OpJavaCall() { return NULL; }\n-  virtual LIR_OpLabel* as_OpLabel() { return NULL; }\n-  virtual LIR_OpDelay* as_OpDelay() { return NULL; }\n-  virtual LIR_OpLock* as_OpLock() { return NULL; }\n-  virtual LIR_OpAllocArray* as_OpAllocArray() { return NULL; }\n-  virtual LIR_OpAllocObj* as_OpAllocObj() { return NULL; }\n-  virtual LIR_OpRoundFP* as_OpRoundFP() { return NULL; }\n-  virtual LIR_OpBranch* as_OpBranch() { return NULL; }\n-  virtual LIR_OpReturn* as_OpReturn() { return NULL; }\n-  virtual LIR_OpRTCall* as_OpRTCall() { return NULL; }\n-  virtual LIR_OpConvert* as_OpConvert() { return NULL; }\n-  virtual LIR_Op0* as_Op0() { return NULL; }\n-  virtual LIR_Op1* as_Op1() { return NULL; }\n-  virtual LIR_Op2* as_Op2() { return NULL; }\n-  virtual LIR_Op3* as_Op3() { return NULL; }\n-  virtual LIR_Op4* as_Op4() { return NULL; }\n-  virtual LIR_OpArrayCopy* as_OpArrayCopy() { return NULL; }\n-  virtual LIR_OpUpdateCRC32* as_OpUpdateCRC32() { return NULL; }\n-  virtual LIR_OpTypeCheck* as_OpTypeCheck() { return NULL; }\n-  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return NULL; }\n-  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return NULL; }\n-  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return NULL; }\n-  virtual LIR_OpCompareAndSwap* as_OpCompareAndSwap() { return NULL; }\n-  virtual LIR_OpLoadKlass* as_OpLoadKlass() { return NULL; }\n-  virtual LIR_OpProfileCall* as_OpProfileCall() { return NULL; }\n-  virtual LIR_OpProfileType* as_OpProfileType() { return NULL; }\n-  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return NULL; }\n+  virtual LIR_OpCall* as_OpCall() { return nullptr; }\n+  virtual LIR_OpJavaCall* as_OpJavaCall() { return nullptr; }\n+  virtual LIR_OpLabel* as_OpLabel() { return nullptr; }\n+  virtual LIR_OpDelay* as_OpDelay() { return nullptr; }\n+  virtual LIR_OpLock* as_OpLock() { return nullptr; }\n+  virtual LIR_OpAllocArray* as_OpAllocArray() { return nullptr; }\n+  virtual LIR_OpAllocObj* as_OpAllocObj() { return nullptr; }\n+  virtual LIR_OpRoundFP* as_OpRoundFP() { return nullptr; }\n+  virtual LIR_OpBranch* as_OpBranch() { return nullptr; }\n+  virtual LIR_OpReturn* as_OpReturn() { return nullptr; }\n+  virtual LIR_OpRTCall* as_OpRTCall() { return nullptr; }\n+  virtual LIR_OpConvert* as_OpConvert() { return nullptr; }\n+  virtual LIR_Op0* as_Op0() { return nullptr; }\n+  virtual LIR_Op1* as_Op1() { return nullptr; }\n+  virtual LIR_Op2* as_Op2() { return nullptr; }\n+  virtual LIR_Op3* as_Op3() { return nullptr; }\n+  virtual LIR_Op4* as_Op4() { return nullptr; }\n+  virtual LIR_OpArrayCopy* as_OpArrayCopy() { return nullptr; }\n+  virtual LIR_OpUpdateCRC32* as_OpUpdateCRC32() { return nullptr; }\n+  virtual LIR_OpTypeCheck* as_OpTypeCheck() { return nullptr; }\n+  virtual LIR_OpFlattenedArrayCheck* as_OpFlattenedArrayCheck() { return nullptr; }\n+  virtual LIR_OpNullFreeArrayCheck* as_OpNullFreeArrayCheck() { return nullptr; }\n+  virtual LIR_OpSubstitutabilityCheck* as_OpSubstitutabilityCheck() { return nullptr; }\n+  virtual LIR_OpCompareAndSwap* as_OpCompareAndSwap() { return nullptr; }\n+  virtual LIR_OpLoadKlass* as_OpLoadKlass() { return nullptr; }\n+  virtual LIR_OpProfileCall* as_OpProfileCall() { return nullptr; }\n+  virtual LIR_OpProfileType* as_OpProfileType() { return nullptr; }\n+  virtual LIR_OpProfileInlineType* as_OpProfileInlineType() { return nullptr; }\n@@ -1192,1 +1192,1 @@\n-  virtual LIR_OpAssert* as_OpAssert() { return NULL; }\n+  virtual LIR_OpAssert* as_OpAssert() { return nullptr; }\n@@ -1207,1 +1207,1 @@\n-             LIR_OprList* arguments, CodeEmitInfo* info = NULL)\n+             LIR_OprList* arguments, CodeEmitInfo* info = nullptr)\n@@ -1264,1 +1264,1 @@\n-  bool maybe_return_as_fields(ciInlineKlass** vk = NULL) const;\n+  bool maybe_return_as_fields(ciInlineKlass** vk = nullptr) const;\n@@ -1278,1 +1278,1 @@\n-   : LIR_Op(lir_label, LIR_OprFact::illegalOpr, NULL)\n+   : LIR_Op(lir_label, LIR_OprFact::illegalOpr, nullptr)\n@@ -1368,2 +1368,2 @@\n-   : LIR_Op(code, LIR_OprFact::illegalOpr, NULL)  { assert(is_in_range(code, begin_op0, end_op0), \"code check\"); }\n-  LIR_Op0(LIR_Code code, LIR_Opr result, CodeEmitInfo* info = NULL)\n+   : LIR_Op(code, LIR_OprFact::illegalOpr, nullptr)  { assert(is_in_range(code, begin_op0, end_op0), \"code check\"); }\n+  LIR_Op0(LIR_Code code, LIR_Opr result, CodeEmitInfo* info = nullptr)\n@@ -1398,1 +1398,1 @@\n-  LIR_Op1(LIR_Code code, LIR_Opr opr, LIR_Opr result = LIR_OprFact::illegalOpr, BasicType type = T_ILLEGAL, LIR_PatchCode patch = lir_patch_none, CodeEmitInfo* info = NULL)\n+  LIR_Op1(LIR_Code code, LIR_Opr opr, LIR_Opr result = LIR_OprFact::illegalOpr, BasicType type = T_ILLEGAL, LIR_PatchCode patch = lir_patch_none, CodeEmitInfo* info = nullptr)\n@@ -1448,1 +1448,1 @@\n-               LIR_Opr result, LIR_OprList* arguments, CodeEmitInfo* info = NULL)\n+               LIR_Opr result, LIR_OprList* arguments, CodeEmitInfo* info = nullptr)\n@@ -1610,1 +1610,1 @@\n-  virtual bool is_patching() { return _info_for_patch != NULL; }\n+  virtual bool is_patching() { return _info_for_patch != nullptr; }\n@@ -1712,1 +1712,1 @@\n-  LIR_Op2(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, CodeEmitInfo* info = NULL, BasicType type = T_ILLEGAL)\n+  LIR_Op2(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, CodeEmitInfo* info = nullptr, BasicType type = T_ILLEGAL)\n@@ -1728,1 +1728,1 @@\n-    : LIR_Op(code, result, NULL)\n+    : LIR_Op(code, result, nullptr)\n@@ -1744,1 +1744,1 @@\n-          CodeEmitInfo* info = NULL, BasicType type = T_ILLEGAL)\n+          CodeEmitInfo* info = nullptr, BasicType type = T_ILLEGAL)\n@@ -1761,1 +1761,1 @@\n-    : LIR_Op(code, result, NULL)\n+    : LIR_Op(code, result, nullptr)\n@@ -1812,1 +1812,1 @@\n-    : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*) NULL)\n+    : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*) nullptr)\n@@ -1814,3 +1814,3 @@\n-    , _block(NULL)\n-    , _ublock(NULL)\n-    , _stub(NULL) { }\n+    , _block(nullptr)\n+    , _ublock(nullptr)\n+    , _stub(nullptr) { }\n@@ -1861,1 +1861,1 @@\n-    : LIR_Op(lir_alloc_array, result, NULL)\n+    : LIR_Op(lir_alloc_array, result, nullptr)\n@@ -1895,1 +1895,1 @@\n-  LIR_Op3(LIR_Code code, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr opr3, LIR_Opr result, CodeEmitInfo* info = NULL)\n+  LIR_Op3(LIR_Code code, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr opr3, LIR_Opr result, CodeEmitInfo* info = nullptr)\n@@ -1927,1 +1927,1 @@\n-    : LIR_Op(code, result, NULL)\n+    : LIR_Op(code, result, nullptr)\n@@ -1988,1 +1988,1 @@\n-  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_imse_stub=NULL)\n+  LIR_OpLock(LIR_Code code, LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_imse_stub=nullptr)\n@@ -2085,1 +2085,1 @@\n-    : LIR_Op(code, result, NULL)  \/\/ no result, no info\n+    : LIR_Op(code, result, nullptr)  \/\/ no result, no info\n@@ -2119,1 +2119,1 @@\n-    : LIR_Op(lir_profile_call, LIR_OprFact::illegalOpr, NULL)  \/\/ no result, no info\n+    : LIR_Op(lir_profile_call, LIR_OprFact::illegalOpr, nullptr)  \/\/ no result, no info\n@@ -2155,1 +2155,1 @@\n-  ciKlass*     _exact_klass;   \/\/ non NULL if we know the klass statically (no need to load it from _obj)\n+  ciKlass*     _exact_klass;   \/\/ non null if we know the klass statically (no need to load it from _obj)\n@@ -2158,1 +2158,1 @@\n-  bool         _no_conflict;   \/\/ true if we're profling parameters, _exact_klass is not NULL and we know\n+  bool         _no_conflict;   \/\/ true if we're profling parameters, _exact_klass is not null and we know\n@@ -2164,1 +2164,1 @@\n-    : LIR_Op(lir_profile_type, LIR_OprFact::illegalOpr, NULL)  \/\/ no result, no info\n+    : LIR_Op(lir_profile_type, LIR_OprFact::illegalOpr, nullptr)  \/\/ no result, no info\n@@ -2200,1 +2200,1 @@\n-    : LIR_Op(lir_profile_inline_type, LIR_OprFact::illegalOpr, NULL)  \/\/ no result, no info\n+    : LIR_Op(lir_profile_inline_type, LIR_OprFact::illegalOpr, nullptr)  \/\/ no result, no info\n@@ -2247,1 +2247,1 @@\n-    if (op->source() == NULL)\n+    if (op->source() == nullptr)\n@@ -2267,1 +2267,1 @@\n-    _file = NULL;\n+    _file = nullptr;\n@@ -2272,1 +2272,1 @@\n-  LIR_List(Compilation* compilation, BlockBegin* block = NULL);\n+  LIR_List(Compilation* compilation, BlockBegin* block = nullptr);\n@@ -2338,1 +2338,1 @@\n-  void leal(LIR_Opr from, LIR_Opr result_reg, LIR_PatchCode patch_code = lir_patch_none, CodeEmitInfo* info = NULL) { append(new LIR_Op1(lir_leal, from, result_reg, T_ILLEGAL, patch_code, info)); }\n+  void leal(LIR_Opr from, LIR_Opr result_reg, LIR_PatchCode patch_code = lir_patch_none, CodeEmitInfo* info = nullptr) { append(new LIR_Op1(lir_leal, from, result_reg, T_ILLEGAL, patch_code, info)); }\n@@ -2343,4 +2343,4 @@\n-  void move(LIR_Opr src, LIR_Opr dst, CodeEmitInfo* info = NULL) { append(new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, info)); }\n-  void move(LIR_Address* src, LIR_Opr dst, CodeEmitInfo* info = NULL) { append(new LIR_Op1(lir_move, LIR_OprFact::address(src), dst, src->type(), lir_patch_none, info)); }\n-  void move(LIR_Opr src, LIR_Address* dst, CodeEmitInfo* info = NULL) { append(new LIR_Op1(lir_move, src, LIR_OprFact::address(dst), dst->type(), lir_patch_none, info)); }\n-  void move_wide(LIR_Address* src, LIR_Opr dst, CodeEmitInfo* info = NULL) {\n+  void move(LIR_Opr src, LIR_Opr dst, CodeEmitInfo* info = nullptr) { append(new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, info)); }\n+  void move(LIR_Address* src, LIR_Opr dst, CodeEmitInfo* info = nullptr) { append(new LIR_Op1(lir_move, LIR_OprFact::address(src), dst, src->type(), lir_patch_none, info)); }\n+  void move(LIR_Opr src, LIR_Address* dst, CodeEmitInfo* info = nullptr) { append(new LIR_Op1(lir_move, src, LIR_OprFact::address(dst), dst->type(), lir_patch_none, info)); }\n+  void move_wide(LIR_Address* src, LIR_Opr dst, CodeEmitInfo* info = nullptr) {\n@@ -2353,1 +2353,1 @@\n-  void move_wide(LIR_Opr src, LIR_Address* dst, CodeEmitInfo* info = NULL) {\n+  void move_wide(LIR_Opr src, LIR_Address* dst, CodeEmitInfo* info = nullptr) {\n@@ -2360,1 +2360,1 @@\n-  void volatile_move(LIR_Opr src, LIR_Opr dst, BasicType type, CodeEmitInfo* info = NULL, LIR_PatchCode patch_code = lir_patch_none) { append(new LIR_Op1(lir_move, src, dst, type, patch_code, info, lir_move_volatile)); }\n+  void volatile_move(LIR_Opr src, LIR_Opr dst, BasicType type, CodeEmitInfo* info = nullptr, LIR_PatchCode patch_code = lir_patch_none) { append(new LIR_Op1(lir_move, src, dst, type, patch_code, info, lir_move_volatile)); }\n@@ -2371,1 +2371,1 @@\n-  void convert(Bytecodes::Code code, LIR_Opr left, LIR_Opr dst, ConversionStub* stub = NULL\/*, bool is_32bit = false*\/) { append(new LIR_OpConvert(code, left, dst, stub)); }\n+  void convert(Bytecodes::Code code, LIR_Opr left, LIR_Opr dst, ConversionStub* stub = nullptr\/*, bool is_32bit = false*\/) { append(new LIR_OpConvert(code, left, dst, stub)); }\n@@ -2388,1 +2388,1 @@\n-  void cmp(LIR_Condition condition, LIR_Opr left, LIR_Opr right, CodeEmitInfo* info = NULL) {\n+  void cmp(LIR_Condition condition, LIR_Opr left, LIR_Opr right, CodeEmitInfo* info = nullptr) {\n@@ -2391,1 +2391,1 @@\n-  void cmp(LIR_Condition condition, LIR_Opr left, int right, CodeEmitInfo* info = NULL) {\n+  void cmp(LIR_Condition condition, LIR_Opr left, int right, CodeEmitInfo* info = nullptr) {\n@@ -2421,1 +2421,1 @@\n-  void sub (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = NULL) { append(new LIR_Op2(lir_sub, left, right, res, info)); }\n+  void sub (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = nullptr) { append(new LIR_Op2(lir_sub, left, right, res, info)); }\n@@ -2424,1 +2424,1 @@\n-  void div (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = NULL)      { append(new LIR_Op2(lir_div, left, right, res, info)); }\n+  void div (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = nullptr)      { append(new LIR_Op2(lir_div, left, right, res, info)); }\n@@ -2426,1 +2426,1 @@\n-  void rem (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = NULL)      { append(new LIR_Op2(lir_rem, left, right, res, info)); }\n+  void rem (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = nullptr)      { append(new LIR_Op2(lir_rem, left, right, res, info)); }\n@@ -2431,1 +2431,1 @@\n-  void load(LIR_Address* addr, LIR_Opr src, CodeEmitInfo* info = NULL, LIR_PatchCode patch_code = lir_patch_none);\n+  void load(LIR_Address* addr, LIR_Opr src, CodeEmitInfo* info = nullptr, LIR_PatchCode patch_code = lir_patch_none);\n@@ -2435,1 +2435,1 @@\n-  void store(LIR_Opr src, LIR_Address* addr, CodeEmitInfo* info = NULL, LIR_PatchCode patch_code = lir_patch_none);\n+  void store(LIR_Opr src, LIR_Address* addr, CodeEmitInfo* info = nullptr, LIR_PatchCode patch_code = lir_patch_none);\n@@ -2492,1 +2492,1 @@\n-  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_imse_stub=NULL);\n+  void lock_object(LIR_Opr hdr, LIR_Opr obj, LIR_Opr lock, LIR_Opr scratch, CodeStub* stub, CodeEmitInfo* info, CodeStub* throw_imse_stub=nullptr);\n@@ -2538,1 +2538,1 @@\n-  LIR_List*   _lir;   \/\/ the lir list where ops of this buffer should be inserted later (NULL when uninitialized)\n+  LIR_List*   _lir;   \/\/ the lir list where ops of this buffer should be inserted later (null when uninitialized)\n@@ -2556,1 +2556,1 @@\n-  LIR_InsertionBuffer() : _lir(NULL), _index_and_count(8), _ops(8) { }\n+  LIR_InsertionBuffer() : _lir(nullptr), _index_and_count(8), _ops(8) { }\n@@ -2560,1 +2560,1 @@\n-  bool initialized() const  { return _lir != NULL; }\n+  bool initialized() const  { return _lir != nullptr; }\n@@ -2562,1 +2562,1 @@\n-  void finish()             { _lir = NULL; }\n+  void finish()             { _lir = nullptr; }\n@@ -2577,1 +2577,1 @@\n-  void move(int index, LIR_Opr src, LIR_Opr dst, CodeEmitInfo* info = NULL) { append(index, new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, info)); }\n+  void move(int index, LIR_Opr src, LIR_Opr dst, CodeEmitInfo* info = nullptr) { append(index, new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, info)); }\n@@ -2626,1 +2626,1 @@\n-      if (address != NULL) {\n+      if (address != nullptr) {\n@@ -2654,1 +2654,1 @@\n-    assert(info != NULL, \"should not call this otherwise\");\n+    assert(info != nullptr, \"should not call this otherwise\");\n@@ -2669,1 +2669,1 @@\n-    _op = NULL;\n+    _op = nullptr;\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":94,"deletions":94,"binary":false,"changes":188,"status":"modified"},{"patch":"@@ -113,2 +113,2 @@\n- , _current_block(NULL)\n- , _pending_non_safepoint(NULL)\n+ , _current_block(nullptr)\n+ , _pending_non_safepoint(nullptr)\n@@ -203,1 +203,1 @@\n-      assert(handler->entry_code() == NULL ||\n+      assert(handler->entry_code() == nullptr ||\n@@ -209,1 +209,1 @@\n-        if (handler->entry_code() != NULL && handler->entry_code()->instructions_list()->length() > 1) {\n+        if (handler->entry_code() != nullptr && handler->entry_code()->instructions_list()->length() > 1) {\n@@ -263,1 +263,1 @@\n-  assert(block->lir() != NULL, \"must have LIR\");\n+  assert(block->lir() != nullptr, \"must have LIR\");\n@@ -342,1 +342,1 @@\n-  if (info->exception_handlers() != NULL) {\n+  if (info->exception_handlers() != nullptr) {\n@@ -350,1 +350,1 @@\n-  if (cinfo->exception_handlers() != NULL) {\n+  if (cinfo->exception_handlers() != nullptr) {\n@@ -357,1 +357,1 @@\n-  if (ss != NULL) return ss->state();\n+  if (ss != nullptr) return ss->state();\n@@ -363,1 +363,1 @@\n-  if (src == NULL)  return;\n+  if (src == nullptr)  return;\n@@ -370,2 +370,2 @@\n-  if (vstack == NULL)  return;\n-  if (_pending_non_safepoint != NULL) {\n+  if (vstack == nullptr)  return;\n+  if (_pending_non_safepoint != nullptr) {\n@@ -380,1 +380,1 @@\n-    _pending_non_safepoint = NULL;\n+    _pending_non_safepoint = nullptr;\n@@ -390,1 +390,1 @@\n-\/\/ Return NULL if n is too large.\n+\/\/ Return null if n is too large.\n@@ -395,1 +395,1 @@\n-    if (t == NULL)  break;\n+    if (t == nullptr)  break;\n@@ -398,1 +398,1 @@\n-  if (t == NULL)  return NULL;\n+  if (t == nullptr)  return nullptr;\n@@ -401,1 +401,1 @@\n-    if (tc == NULL)  return s;\n+    if (tc == nullptr)  return s;\n@@ -422,1 +422,1 @@\n-    if (s == NULL)  break;\n+    if (s == nullptr)  break;\n@@ -494,1 +494,1 @@\n-  ciInlineKlass* vk = NULL;\n+  ciInlineKlass* vk = nullptr;\n@@ -540,1 +540,1 @@\n-      assert(op->as_OpReturn() != NULL, \"sanity\");\n+      assert(op->as_OpReturn() != nullptr, \"sanity\");\n@@ -543,1 +543,1 @@\n-      if (ret_op->stub() != NULL) {\n+      if (ret_op->stub() != nullptr) {\n@@ -616,3 +616,3 @@\n-  DebugToken* locvals = debug_info->create_scope_values(NULL); \/\/ FIXME is this needed (for Java debugging to work properly??)\n-  DebugToken* expvals = debug_info->create_scope_values(NULL); \/\/ FIXME is this needed (for Java debugging to work properly??)\n-  DebugToken* monvals = debug_info->create_monitor_values(NULL); \/\/ FIXME: need testing with synchronized method\n+  DebugToken* locvals = debug_info->create_scope_values(nullptr); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* expvals = debug_info->create_scope_values(nullptr); \/\/ FIXME is this needed (for Java debugging to work properly??)\n+  DebugToken* monvals = debug_info->create_monitor_values(nullptr); \/\/ FIXME: need testing with synchronized method\n@@ -688,1 +688,1 @@\n-    emit_std_entry(CodeOffsets::Verified_Inline_Entry, NULL);\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, nullptr);\n@@ -704,1 +704,1 @@\n-    emit_std_entry(CodeOffsets::Verified_Inline_Entry, NULL);\n+    emit_std_entry(CodeOffsets::Verified_Inline_Entry, nullptr);\n@@ -745,1 +745,1 @@\n-      assert(op->info() == NULL, \"not supported\");\n+      assert(op->info() == nullptr, \"not supported\");\n@@ -822,1 +822,1 @@\n-      if (op->info() != NULL) {\n+      if (op->info() != nullptr) {\n@@ -930,1 +930,1 @@\n-      assert(patch_code == lir_patch_none && info == NULL, \"no patching and info allowed here\");\n+      assert(patch_code == lir_patch_none && info == nullptr, \"no patching and info allowed here\");\n@@ -933,1 +933,1 @@\n-      assert(patch_code == lir_patch_none && info == NULL, \"no patching and info allowed here\");\n+      assert(patch_code == lir_patch_none && info == nullptr, \"no patching and info allowed here\");\n@@ -942,1 +942,1 @@\n-    assert(patch_code == lir_patch_none && info == NULL, \"no patching and info allowed here\");\n+    assert(patch_code == lir_patch_none && info == nullptr, \"no patching and info allowed here\");\n@@ -955,1 +955,1 @@\n-      assert(patch_code == lir_patch_none && info == NULL, \"no patching and info allowed here\");\n+      assert(patch_code == lir_patch_none && info == nullptr, \"no patching and info allowed here\");\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":31,"deletions":31,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -65,1 +65,1 @@\n-    if (_pending_non_safepoint != NULL) {\n+    if (_pending_non_safepoint != nullptr) {\n@@ -68,1 +68,1 @@\n-      _pending_non_safepoint = NULL;\n+      _pending_non_safepoint = nullptr;\n@@ -100,0 +100,1 @@\n+ public:\n@@ -113,0 +114,1 @@\n+ private:\n@@ -254,1 +256,1 @@\n-  void leal(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code = lir_patch_none, CodeEmitInfo* info = NULL);\n+  void leal(LIR_Opr src, LIR_Opr dest, LIR_PatchCode patch_code = lir_patch_none, CodeEmitInfo* info = nullptr);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -113,1 +113,1 @@\n-\/\/ Call graph: move(NULL, c) -> move(c, b) -> move(b, a)\n+\/\/ Call graph: move(null, c) -> move(c, b) -> move(b, a)\n@@ -116,1 +116,1 @@\n-\/\/ Call graph: move(NULL, a) -> move(a, b) -> move(b, a)\n+\/\/ Call graph: move(null, a) -> move(a, b) -> move(b, a)\n@@ -126,1 +126,1 @@\n-    assert(_loop == NULL, \"only one loop valid!\");\n+    assert(_loop == nullptr, \"only one loop valid!\");\n@@ -136,1 +136,1 @@\n-    } else if (src != NULL) {\n+    } else if (src != nullptr) {\n@@ -150,2 +150,2 @@\n-      _loop = NULL;\n-      move(NULL, node);\n+      _loop = nullptr;\n+      move(nullptr, node);\n@@ -171,3 +171,3 @@\n-    node = vreg_table().at_grow(vreg_num, NULL);\n-    assert(node == NULL || node->operand() == opr, \"\");\n-    if (node == NULL) {\n+    node = vreg_table().at_grow(vreg_num, nullptr);\n+    assert(node == nullptr || node->operand() == opr, \"\");\n+    if (node == nullptr) {\n@@ -209,1 +209,1 @@\n-    _gen->_instruction_for_operand.at_put_grow(opr->vreg_number(), value(), NULL);\n+    _gen->_instruction_for_operand.at_put_grow(opr->vreg_number(), value(), nullptr);\n@@ -266,1 +266,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -271,2 +271,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_IntConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_IntConstant() != nullptr, \"type check\");\n@@ -278,2 +278,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_AddressConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_AddressConstant() != nullptr, \"type check\");\n@@ -285,2 +285,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_FloatConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_FloatConstant() != nullptr, \"type check\");\n@@ -292,2 +292,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_DoubleConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_DoubleConstant() != nullptr, \"type check\");\n@@ -299,2 +299,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_LongConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_LongConstant() != nullptr, \"type check\");\n@@ -317,1 +317,1 @@\n-  assert(block->lir() == NULL, \"LIR list already computed for this block\");\n+  assert(block->lir() == nullptr, \"LIR list already computed for this block\");\n@@ -358,1 +358,1 @@\n-  for (Instruction* instr = block; instr != NULL; instr = instr->next()) {\n+  for (Instruction* instr = block; instr != nullptr; instr = instr->next()) {\n@@ -362,1 +362,1 @@\n-  set_block(NULL);\n+  set_block(nullptr);\n@@ -381,1 +381,1 @@\n-         instr->as_Constant() != NULL || bailed_out(), \"invalid item set\");\n+         instr->as_Constant() != nullptr || bailed_out(), \"invalid item set\");\n@@ -389,2 +389,2 @@\n-  if ((instr->is_pinned() && instr->as_Phi() == NULL) || instr->operand()->is_valid()) {\n-    assert(instr->operand() != LIR_OprFact::illegalOpr || instr->as_Constant() != NULL, \"this root has not yet been visited\");\n+  if ((instr->is_pinned() && instr->as_Phi() == nullptr) || instr->operand()->is_valid()) {\n+    assert(instr->operand() != LIR_OprFact::illegalOpr || instr->as_Constant() != nullptr, \"this root has not yet been visited\");\n@@ -394,1 +394,1 @@\n-    \/\/ assert(instr->use_count() > 0 || instr->as_Phi() != NULL, \"leaf instruction must have a use\");\n+    \/\/ assert(instr->use_count() > 0 || instr->as_Phi() != nullptr, \"leaf instruction must have a use\");\n@@ -400,1 +400,1 @@\n-  assert(state != NULL, \"state must be defined\");\n+  assert(state != nullptr, \"state must be defined\");\n@@ -417,1 +417,1 @@\n-      if (!value->is_pinned() && value->as_Constant() == NULL && value->as_Local() == NULL) {\n+      if (!value->is_pinned() && value->as_Constant() == nullptr && value->as_Local() == nullptr) {\n@@ -444,1 +444,1 @@\n-          if (!value->is_pinned() && value->as_Constant() == NULL && value->as_Local() == NULL) {\n+          if (!value->is_pinned() && value->as_Constant() == nullptr && value->as_Local() == nullptr) {\n@@ -449,1 +449,1 @@\n-          \/\/ NULL out this local so that linear scan can assume that all non-NULL values are live.\n+          \/\/ null out this local so that linear scan can assume that all non-null values are live.\n@@ -456,1 +456,1 @@\n-  return new CodeEmitInfo(state, ignore_xhandler ? NULL : x->exception_handlers(), x->check_flag(Instruction::DeoptimizeOnException));\n+  return new CodeEmitInfo(state, ignore_xhandler ? nullptr : x->exception_handlers(), x->check_flag(Instruction::DeoptimizeOnException));\n@@ -470,2 +470,2 @@\n-    assert(info != NULL, \"info must be set if class is not loaded\");\n-    __ klass2reg_patch(NULL, r, info);\n+    assert(info != nullptr, \"info must be set if class is not loaded\");\n+    __ klass2reg_patch(nullptr, r, info);\n@@ -629,1 +629,1 @@\n-  CodeStub* slow_path = new MonitorExitStub(lock, !UseHeavyMonitors, monitor_no);\n+  CodeStub* slow_path = new MonitorExitStub(lock, LockingMode != LM_MONITOR, monitor_no);\n@@ -692,1 +692,1 @@\n-  if (type != NULL && type->is_array_klass() && type->is_loaded()) {\n+  if (type != nullptr && type->is_array_klass() && type->is_loaded()) {\n@@ -695,1 +695,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -701,2 +701,2 @@\n-  if (t == NULL) {\n-    return NULL;\n+  if (t == nullptr) {\n+    return nullptr;\n@@ -706,1 +706,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -720,1 +720,1 @@\n-  ciArrayKlass* expected_type = NULL;\n+  ciArrayKlass* expected_type = nullptr;\n@@ -726,1 +726,1 @@\n-    if (src_declared_type == NULL && (phi = src->as_Phi()) != NULL) {\n+    if (src_declared_type == nullptr && (phi = src->as_Phi()) != nullptr) {\n@@ -731,1 +731,1 @@\n-    if (dst_declared_type == NULL && (phi = dst->as_Phi()) != NULL) {\n+    if (dst_declared_type == nullptr && (phi = dst->as_Phi()) != nullptr) {\n@@ -735,1 +735,1 @@\n-    if (src_exact_type != NULL && src_exact_type == dst_exact_type) {\n+    if (src_exact_type != nullptr && src_exact_type == dst_exact_type) {\n@@ -739,1 +739,1 @@\n-    } else if (dst_exact_type != NULL && dst_exact_type->is_obj_array_klass()) {\n+    } else if (dst_exact_type != nullptr && dst_exact_type->is_obj_array_klass()) {\n@@ -741,2 +741,2 @@\n-      ciArrayKlass* src_type = NULL;\n-      if (src_exact_type != NULL && src_exact_type->is_obj_array_klass()) {\n+      ciArrayKlass* src_type = nullptr;\n+      if (src_exact_type != nullptr && src_exact_type->is_obj_array_klass()) {\n@@ -744,1 +744,1 @@\n-      } else if (src_declared_type != NULL && src_declared_type->is_obj_array_klass()) {\n+      } else if (src_declared_type != nullptr && src_declared_type->is_obj_array_klass()) {\n@@ -747,1 +747,1 @@\n-      if (src_type != NULL) {\n+      if (src_type != nullptr) {\n@@ -755,3 +755,3 @@\n-    if (expected_type == NULL) expected_type = dst_exact_type;\n-    if (expected_type == NULL) expected_type = src_declared_type;\n-    if (expected_type == NULL) expected_type = dst_declared_type;\n+    if (expected_type == nullptr) expected_type = dst_exact_type;\n+    if (expected_type == nullptr) expected_type = src_declared_type;\n+    if (expected_type == nullptr) expected_type = dst_declared_type;\n@@ -788,2 +788,2 @@\n-  if (expected_type != NULL) {\n-    Value length_limit = NULL;\n+  if (expected_type != nullptr) {\n+    Value length_limit = nullptr;\n@@ -792,1 +792,1 @@\n-    if (ifop != NULL) {\n+    if (ifop != nullptr) {\n@@ -804,1 +804,1 @@\n-    if (src_array != NULL) {\n+    if (src_array != nullptr) {\n@@ -806,1 +806,1 @@\n-      if (length_limit != NULL &&\n+      if (length_limit != nullptr &&\n@@ -814,1 +814,1 @@\n-    if (dst_array != NULL) {\n+    if (dst_array != nullptr) {\n@@ -816,1 +816,1 @@\n-      if (length_limit != NULL &&\n+      if (length_limit != nullptr &&\n@@ -834,1 +834,1 @@\n-    if (al != NULL) {\n+    if (al != nullptr) {\n@@ -863,1 +863,1 @@\n-    if (expected_type != NULL) {\n+    if (expected_type != nullptr) {\n@@ -866,2 +866,2 @@\n-      if (((arrayOopDesc::base_offset_in_bytes(t) + s_offs * element_size) % HeapWordSize == 0) &&\n-          ((arrayOopDesc::base_offset_in_bytes(t) + d_offs * element_size) % HeapWordSize == 0)) {\n+      if (((arrayOopDesc::base_offset_in_bytes(t) + (uint)s_offs * element_size) % HeapWordSize == 0) &&\n+          ((arrayOopDesc::base_offset_in_bytes(t) + (uint)d_offs * element_size) % HeapWordSize == 0)) {\n@@ -931,1 +931,1 @@\n-    assert(method != NULL, \"method should be set if branch is profiled\");\n+    assert(method != nullptr, \"method should be set if branch is profiled\");\n@@ -933,1 +933,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -935,1 +935,1 @@\n-    assert(data != NULL, \"must have profiling data\");\n+    assert(data != nullptr, \"must have profiling data\");\n@@ -986,1 +986,1 @@\n-  if (phi != NULL && cur_val != NULL && cur_val != phi && !phi->is_illegal()) {\n+  if (phi != nullptr && cur_val != nullptr && cur_val != phi && !phi->is_illegal()) {\n@@ -990,1 +990,1 @@\n-        if (op != NULL && op->type()->is_illegal()) {\n+        if (op != nullptr && op->type()->is_illegal()) {\n@@ -996,1 +996,1 @@\n-    if (cur_phi != NULL && cur_phi->is_illegal()) {\n+    if (cur_phi != nullptr && cur_phi->is_illegal()) {\n@@ -1005,1 +1005,1 @@\n-      assert(cur_val->as_Constant() != NULL || cur_val->as_Local() != NULL,\n+      assert(cur_val->as_Constant() != nullptr || cur_val->as_Local() != nullptr,\n@@ -1104,1 +1104,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1120,1 +1120,1 @@\n-  __ move_wide(LIR_OprFact::oopConst(NULL),\n+  __ move_wide(LIR_OprFact::oopConst(nullptr),\n@@ -1122,1 +1122,1 @@\n-  __ move_wide(LIR_OprFact::oopConst(NULL),\n+  __ move_wide(LIR_OprFact::oopConst(nullptr),\n@@ -1149,1 +1149,1 @@\n-  if (x->state_before() != NULL) {\n+  if (x->state_before() != nullptr) {\n@@ -1153,1 +1153,1 @@\n-    __ oop2reg_patch(NULL, reg, info);\n+    __ oop2reg_patch(nullptr, reg, info);\n@@ -1196,1 +1196,1 @@\n-    call_runtime(&signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), voidType, NULL);\n+    call_runtime(&signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), voidType, nullptr);\n@@ -1223,1 +1223,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1259,1 +1259,1 @@\n-                                     NULL); \/\/ NULL CodeEmitInfo results in a leaf call\n+                                     nullptr); \/\/ null CodeEmitInfo results in a leaf call\n@@ -1277,1 +1277,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1299,1 +1299,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1317,1 +1317,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1353,1 +1353,1 @@\n-  load_klass(value.result(), klass, NULL);\n+  load_klass(value.result(), klass, nullptr);\n@@ -1496,1 +1496,1 @@\n-    if (c != NULL) {\n+    if (c != nullptr) {\n@@ -1499,1 +1499,1 @@\n-      assert(x->as_Phi() || x->as_Local() != NULL, \"only for Phi and Local\");\n+      assert(x->as_Phi() || x->as_Local() != nullptr, \"only for Phi and Local\");\n@@ -1502,1 +1502,1 @@\n-      _instruction_for_operand.at_put_grow(x->operand()->vreg_number(), x, NULL);\n+      _instruction_for_operand.at_put_grow(x->operand()->vreg_number(), x, nullptr);\n@@ -1513,1 +1513,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1521,1 +1521,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1644,1 +1644,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1646,1 +1646,1 @@\n-    assert(x->explicit_null_check() == NULL, \"can't fold null check into patching field access\");\n+    assert(x->explicit_null_check() == nullptr, \"can't fold null check into patching field access\");\n@@ -1650,1 +1650,1 @@\n-    if (nc == NULL) {\n+    if (nc == nullptr) {\n@@ -1694,1 +1694,1 @@\n-    \/\/ If the class is not loaded and the object is NULL, we need to deoptimize to throw a\n+    \/\/ If the class is not loaded and the object is null, we need to deoptimize to throw a\n@@ -1708,1 +1708,1 @@\n-                  value.result(), info != NULL ? new CodeEmitInfo(info) : NULL, info);\n+                  value.result(), info != nullptr ? new CodeEmitInfo(info) : nullptr, info);\n@@ -1760,1 +1760,1 @@\n-  assert(field != NULL, \"Need a subelement type specified\");\n+  assert(field != nullptr, \"Need a subelement type specified\");\n@@ -1772,1 +1772,1 @@\n-                     NULL, NULL);\n+                     nullptr, nullptr);\n@@ -1779,1 +1779,1 @@\n-    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));\n+    __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(nullptr));\n@@ -1795,1 +1795,1 @@\n-  assert(sub_offset == 0 || field != NULL, \"Sanity check\");\n+  assert(sub_offset == 0 || field != nullptr, \"Sanity check\");\n@@ -1800,2 +1800,2 @@\n-  ciInlineKlass* elem_klass = NULL;\n-  if (field != NULL) {\n+  ciInlineKlass* elem_klass = nullptr;\n+  if (field != nullptr) {\n@@ -1834,1 +1834,1 @@\n-                     NULL, NULL);\n+                     nullptr, nullptr);\n@@ -1837,1 +1837,1 @@\n-                      NULL, NULL);\n+                      nullptr, nullptr);\n@@ -1841,1 +1841,1 @@\n-                     NULL, NULL);\n+                     nullptr, nullptr);\n@@ -1844,1 +1844,1 @@\n-                      NULL, NULL);\n+                      nullptr, nullptr);\n@@ -1866,1 +1866,1 @@\n-    if (type != NULL && type->is_klass()) {\n+    if (type != nullptr && type->is_klass()) {\n@@ -1889,1 +1889,1 @@\n-  bool use_length = x->length() != NULL;\n+  bool use_length = x->length() != nullptr;\n@@ -1892,1 +1892,1 @@\n-                                        (x->value()->as_Constant() == NULL ||\n+                                        (x->value()->as_Constant() == nullptr ||\n@@ -1921,1 +1921,1 @@\n-  CodeEmitInfo* null_check_info = NULL;\n+  CodeEmitInfo* null_check_info = nullptr;\n@@ -1933,1 +1933,1 @@\n-      null_check_info = NULL;\n+      null_check_info = nullptr;\n@@ -1943,2 +1943,2 @@\n-      ciMethodData* md = NULL;\n-      ciArrayLoadStoreData* load_store = NULL;\n+      ciMethodData* md = nullptr;\n+      ciArrayLoadStoreData* load_store = nullptr;\n@@ -1955,1 +1955,1 @@\n-    array_store_check(value.result(), array.result(), store_check_info, NULL, -1);\n+    array_store_check(value.result(), array.result(), store_check_info, nullptr, -1);\n@@ -1967,1 +1967,1 @@\n-    StoreFlattenedArrayStub* slow_path = NULL;\n+    StoreFlattenedArrayStub* slow_path = nullptr;\n@@ -1986,2 +1986,2 @@\n-                    NULL, null_check_info);\n-    if (slow_path != NULL) {\n+                    nullptr, null_check_info);\n+    if (slow_path != nullptr) {\n@@ -2084,1 +2084,1 @@\n-  bool not_initialized = x->is_static() && x->as_LoadField() != NULL &&\n+  bool not_initialized = x->is_static() && x->as_LoadField() != nullptr &&\n@@ -2102,1 +2102,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -2104,1 +2104,1 @@\n-    assert(x->explicit_null_check() == NULL, \"can't fold null check into patching field access\");\n+    assert(x->explicit_null_check() == nullptr, \"can't fold null check into patching field access\");\n@@ -2108,1 +2108,1 @@\n-    if (nc == NULL) {\n+    if (nc == nullptr) {\n@@ -2129,1 +2129,1 @@\n-    __ move(LIR_OprFact::oopConst(NULL), result);\n+    __ move(LIR_OprFact::oopConst(nullptr), result);\n@@ -2141,1 +2141,1 @@\n-      __ move(LIR_OprFact::oopConst(NULL), obj);\n+      __ move(LIR_OprFact::oopConst(nullptr), obj);\n@@ -2144,1 +2144,1 @@\n-    \/\/ If the class is not loaded and the object is NULL, we need to deoptimize to throw a\n+    \/\/ If the class is not loaded and the object is null, we need to deoptimize to throw a\n@@ -2160,1 +2160,1 @@\n-                 info ? new CodeEmitInfo(info) : NULL, info);\n+                 info ? new CodeEmitInfo(info) : nullptr, info);\n@@ -2177,1 +2177,1 @@\n-      __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(NULL));\n+      __ cmp(lir_cond_notEqual, result, LIR_OprFact::oopConst(nullptr));\n@@ -2190,1 +2190,1 @@\n-      __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(NULL));\n+      __ cmp(lir_cond_equal, result, LIR_OprFact::oopConst(nullptr));\n@@ -2270,1 +2270,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -2273,1 +2273,1 @@\n-    if (nc == NULL) {\n+    if (nc == nullptr) {\n@@ -2280,1 +2280,1 @@\n-      __ move(LIR_OprFact::oopConst(NULL), obj);\n+      __ move(LIR_OprFact::oopConst(nullptr), obj);\n@@ -2289,1 +2289,1 @@\n-  bool use_length = x->length() != NULL;\n+  bool use_length = x->length() != nullptr;\n@@ -2309,1 +2309,1 @@\n-  CodeEmitInfo* null_check_info = NULL;\n+  CodeEmitInfo* null_check_info = nullptr;\n@@ -2312,1 +2312,1 @@\n-    if (nc != NULL) {\n+    if (nc != nullptr) {\n@@ -2319,1 +2319,1 @@\n-      __ move(LIR_OprFact::oopConst(NULL), obj);\n+      __ move(LIR_OprFact::oopConst(nullptr), obj);\n@@ -2335,1 +2335,1 @@\n-      null_check_info = NULL;\n+      null_check_info = nullptr;\n@@ -2339,2 +2339,2 @@\n-  ciMethodData* md = NULL;\n-  ciArrayLoadStoreData* load_store = NULL;\n+  ciMethodData* md = nullptr;\n+  ciArrayLoadStoreData* load_store = nullptr;\n@@ -2352,1 +2352,1 @@\n-  if (x->vt() != NULL) {\n+  if (x->vt() != nullptr) {\n@@ -2358,2 +2358,2 @@\n-                           x->delayed() == NULL ? 0 : x->delayed()->field(),\n-                           x->delayed() == NULL ? 0 : x->delayed()->offset());\n+                           x->delayed() == nullptr ? 0 : x->delayed()->field(),\n+                           x->delayed() == nullptr ? 0 : x->delayed()->offset());\n@@ -2361,1 +2361,1 @@\n-  } else if (x->delayed() != NULL) {\n+  } else if (x->delayed() != nullptr) {\n@@ -2365,1 +2365,1 @@\n-  } else if (x->array() != NULL && x->array()->is_loaded_flattened_array() &&\n+  } else if (x->array() != nullptr && x->array()->is_loaded_flattened_array() &&\n@@ -2380,1 +2380,1 @@\n-    LoadFlattenedArrayStub* slow_path = NULL;\n+    LoadFlattenedArrayStub* slow_path = nullptr;\n@@ -2387,1 +2387,1 @@\n-      assert(x->delayed() == NULL, \"Delayed LoadIndexed only apply to loaded_flattened_arrays\");\n+      assert(x->delayed() == nullptr, \"Delayed LoadIndexed only apply to loaded_flattened_arrays\");\n@@ -2398,1 +2398,1 @@\n-                   NULL, null_check_info);\n+                   nullptr, null_check_info);\n@@ -2400,1 +2400,1 @@\n-    if (slow_path != NULL) {\n+    if (slow_path != nullptr) {\n@@ -2423,1 +2423,1 @@\n-  __ move(LIR_OprFact::oopConst(NULL), reg);\n+  __ move(LIR_OprFact::oopConst(nullptr), reg);\n@@ -2466,1 +2466,1 @@\n-    if (throw_type == NULL) {\n+    if (throw_type == nullptr) {\n@@ -2470,1 +2470,1 @@\n-    if (throw_type != NULL && throw_type->is_instance_klass()) {\n+    if (throw_type != nullptr && throw_type->is_instance_klass()) {\n@@ -2480,1 +2480,1 @@\n-  if (x->exception()->as_NewInstance() == NULL && x->exception()->as_ExceptionObject() == NULL) {\n+  if (x->exception()->as_NewInstance() == nullptr && x->exception()->as_ExceptionObject() == nullptr) {\n@@ -2718,1 +2718,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2720,1 +2720,1 @@\n-    assert(data != NULL, \"must have profiling data\");\n+    assert(data != nullptr, \"must have profiling data\");\n@@ -2776,1 +2776,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2778,1 +2778,1 @@\n-    assert(data != NULL, \"must have profiling data\");\n+    assert(data != nullptr, \"must have profiling data\");\n@@ -2844,1 +2844,1 @@\n-    assert(method != NULL, \"method should be set if branch is profiled\");\n+    assert(method != nullptr, \"method should be set if branch is profiled\");\n@@ -2846,1 +2846,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2848,1 +2848,1 @@\n-    assert(data != NULL, \"must have profiling data\");\n+    assert(data != nullptr, \"must have profiling data\");\n@@ -2893,1 +2893,1 @@\n-  ciKlass* result = NULL;\n+  ciKlass* result = nullptr;\n@@ -2902,1 +2902,1 @@\n-  ciKlass* exact_klass = NULL;\n+  ciKlass* exact_klass = nullptr;\n@@ -2908,1 +2908,1 @@\n-    if (type == NULL) {\n+    if (type == nullptr) {\n@@ -2912,2 +2912,2 @@\n-    assert(type == NULL || type->is_klass(), \"type should be class\");\n-    exact_klass = (type != NULL && type->is_loaded()) ? (ciKlass*)type : NULL;\n+    assert(type == nullptr || type->is_klass(), \"type should be class\");\n+    exact_klass = (type != nullptr && type->is_loaded()) ? (ciKlass*)type : nullptr;\n@@ -2915,1 +2915,1 @@\n-    do_update = exact_klass == NULL || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n+    do_update = exact_klass == nullptr || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n@@ -2922,2 +2922,2 @@\n-  ciKlass* exact_signature_k = NULL;\n-  if (do_update && signature_at_call_k != NULL) {\n+  ciKlass* exact_signature_k = nullptr;\n+  if (do_update && signature_at_call_k != nullptr) {\n@@ -2926,1 +2926,1 @@\n-    if (exact_signature_k == NULL) {\n+    if (exact_signature_k == nullptr) {\n@@ -2934,1 +2934,1 @@\n-    \/\/ exact_klass and exact_signature_k can be both non NULL but\n+    \/\/ exact_klass and exact_signature_k can be both non null but\n@@ -2937,1 +2937,1 @@\n-    if (exact_klass == NULL && exact_signature_k != NULL && exact_klass != exact_signature_k) {\n+    if (exact_klass == nullptr && exact_signature_k != nullptr && exact_klass != exact_signature_k) {\n@@ -2942,1 +2942,1 @@\n-    if (callee_signature_k != NULL &&\n+    if (callee_signature_k != nullptr &&\n@@ -2945,1 +2945,1 @@\n-      if (improved_klass == NULL) {\n+      if (improved_klass == nullptr) {\n@@ -2948,1 +2948,1 @@\n-      if (exact_klass == NULL && improved_klass != NULL && exact_klass != improved_klass) {\n+      if (exact_klass == nullptr && improved_klass != nullptr && exact_klass != improved_klass) {\n@@ -2952,1 +2952,1 @@\n-    do_update = exact_klass == NULL || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n+    do_update = exact_klass == nullptr || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n@@ -2971,1 +2971,1 @@\n-                  value.result(), exact_klass, profiled_k, new_pointer_register(), not_null, exact_signature_k != NULL);\n+                  value.result(), exact_klass, profiled_k, new_pointer_register(), not_null, exact_signature_k != nullptr);\n@@ -2980,1 +2980,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2982,1 +2982,1 @@\n-    if (md->parameters_type_data() != NULL) {\n+    if (md->parameters_type_data() != nullptr) {\n@@ -2995,1 +2995,1 @@\n-                                        profiled_k, local, mdp, false, local->declared_type()->as_klass(), NULL);\n+                                        profiled_k, local, mdp, false, local->declared_type()->as_klass(), nullptr);\n@@ -2997,1 +2997,1 @@\n-          if (exact != NULL) {\n+          if (exact != nullptr) {\n@@ -3009,1 +3009,1 @@\n-  assert(md != NULL && data != NULL, \"should have been initialized\");\n+  assert(md != nullptr && data != nullptr, \"should have been initialized\");\n@@ -3037,1 +3037,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -3039,1 +3039,1 @@\n-  assert(data != NULL && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n+  assert(data != nullptr && data->is_ArrayLoadStoreData(), \"incorrect profiling entry\");\n@@ -3043,1 +3043,1 @@\n-               load_store->array()->type(), x->array(), mdp, true, NULL, NULL);\n+               load_store->array()->type(), x->array(), mdp, true, nullptr, nullptr);\n@@ -3048,1 +3048,1 @@\n-  assert(md != NULL && load_store != NULL, \"should have been initialized\");\n+  assert(md != nullptr && load_store != nullptr, \"should have been initialized\");\n@@ -3051,1 +3051,1 @@\n-               load_store->element()->type(), element, mdp, false, NULL, NULL);\n+               load_store->element()->type(), element, mdp, false, nullptr, nullptr);\n@@ -3083,1 +3083,1 @@\n-    assert(local != NULL, \"Locals for incoming arguments must have been created\");\n+    assert(local != nullptr, \"Locals for incoming arguments must have been created\");\n@@ -3089,1 +3089,1 @@\n-    _instruction_for_operand.at_put_grow(dest->vreg_number(), local, NULL);\n+    _instruction_for_operand.at_put_grow(dest->vreg_number(), local, nullptr);\n@@ -3102,1 +3102,1 @@\n-    call_runtime(&signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry), voidType, NULL);\n+    call_runtime(&signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry), voidType, nullptr);\n@@ -3112,1 +3112,1 @@\n-      assert(receiver != NULL, \"must already exist\");\n+      assert(receiver != nullptr, \"must already exist\");\n@@ -3121,1 +3121,1 @@\n-      CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, x->check_flag(Instruction::DeoptimizeOnException));\n+      CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, SynchronizationEntryBCI), nullptr, x->check_flag(Instruction::DeoptimizeOnException));\n@@ -3124,2 +3124,2 @@\n-      \/\/ receiver is guaranteed non-NULL so don't need CodeEmitInfo\n-      __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, NULL);\n+      \/\/ receiver is guaranteed non-null so don't need CodeEmitInfo\n+      __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, nullptr);\n@@ -3131,1 +3131,1 @@\n-    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, false);\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, SynchronizationEntryBCI), nullptr, false);\n@@ -3137,1 +3137,1 @@\n-    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), NULL, false);\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, 0), nullptr, false);\n@@ -3398,1 +3398,1 @@\n-  if ((left_klass == NULL || right_klass == NULL) ||\/\/ The klass is still unloaded, or came from a Phi node.\n+  if ((left_klass == nullptr || right_klass == nullptr) ||\/\/ The klass is still unloaded, or came from a Phi node.\n@@ -3403,1 +3403,1 @@\n-  if (left_klass != NULL && left_klass->is_inlinetype() && left_klass == right_klass) {\n+  if (left_klass != nullptr && left_klass->is_inlinetype() && left_klass == right_klass) {\n@@ -3548,1 +3548,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -3550,1 +3550,1 @@\n-    if (data != NULL) {\n+    if (data != nullptr) {\n@@ -3569,1 +3569,1 @@\n-        ciSignatureStream callee_signature_stream(callee_signature, has_receiver ? x->callee()->holder() : NULL);\n+        ciSignatureStream callee_signature_stream(callee_signature, has_receiver ? x->callee()->holder() : nullptr);\n@@ -3572,1 +3572,1 @@\n-        ciSignature* signature_at_call = NULL;\n+        ciSignature* signature_at_call = nullptr;\n@@ -3583,1 +3583,1 @@\n-          if (exact != NULL) {\n+          if (exact != nullptr) {\n@@ -3604,1 +3604,1 @@\n-    if (md != NULL) {\n+    if (md != nullptr) {\n@@ -3606,1 +3606,1 @@\n-      if (parameters_type_data != NULL) {\n+      if (parameters_type_data != nullptr) {\n@@ -3611,1 +3611,1 @@\n-        ciSignatureStream sig_stream(sig, has_receiver ? x->callee()->holder() : NULL);\n+        ciSignatureStream sig_stream(sig, has_receiver ? x->callee()->holder() : nullptr);\n@@ -3620,1 +3620,1 @@\n-        if (arg == NULL || !Bytecodes::has_receiver(bc)) {\n+        if (arg == nullptr || !Bytecodes::has_receiver(bc)) {\n@@ -3630,1 +3630,1 @@\n-                                        profiled_k, arg, mdp, not_null, sig_stream.next_klass(), NULL);\n+                                        profiled_k, arg, mdp, not_null, sig_stream.next_klass(), nullptr);\n@@ -3632,1 +3632,1 @@\n-          if (exact != NULL) {\n+          if (exact != nullptr) {\n@@ -3641,1 +3641,1 @@\n-                x->recv() != NULL && Bytecodes::has_receiver(bc)) {\n+                x->recv() != nullptr && Bytecodes::has_receiver(bc)) {\n@@ -3669,1 +3669,1 @@\n-  if (x->recv() != NULL || x->nb_profiled_args() > 0) {\n+  if (x->recv() != nullptr || x->nb_profiled_args() > 0) {\n@@ -3673,1 +3673,1 @@\n-  if (x->recv() != NULL) {\n+  if (x->recv() != nullptr) {\n@@ -3685,1 +3685,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -3687,1 +3687,1 @@\n-  if (data != NULL) {\n+  if (data != nullptr) {\n@@ -3693,1 +3693,1 @@\n-    ciSignature* signature_at_call = NULL;\n+    ciSignature* signature_at_call = nullptr;\n@@ -3704,1 +3704,1 @@\n-    if (exact != NULL) {\n+    if (exact != nullptr) {\n@@ -3712,1 +3712,1 @@\n-  if (klass != NULL) {\n+  if (klass != nullptr) {\n@@ -3727,1 +3727,1 @@\n-  assert(method != NULL, \"method should be set if branch is profiled\");\n+  assert(method != nullptr, \"method should be set if branch is profiled\");\n@@ -3729,1 +3729,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -3731,1 +3731,1 @@\n-  assert(data != NULL, \"must have profiling data\");\n+  assert(data != nullptr, \"must have profiling data\");\n@@ -3736,1 +3736,1 @@\n-               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), NULL, NULL);\n+               acmp->left()->type(), x->left(), mdp, !x->left_maybe_null(), nullptr, nullptr);\n@@ -3747,1 +3747,1 @@\n-               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), NULL, NULL);\n+               acmp->right()->type(), x->right(), mdp, !x->right_maybe_null(), nullptr, nullptr);\n@@ -3823,1 +3823,1 @@\n-    if (counters_adr == NULL) {\n+    if (counters_adr == nullptr) {\n@@ -3836,1 +3836,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -3890,1 +3890,1 @@\n-  LIR_Opr result = call_runtime(signature, args, x->entry(), x->type(), NULL);\n+  LIR_Opr result = call_runtime(signature, args, x->entry(), x->type(), nullptr);\n@@ -4122,1 +4122,1 @@\n-  null_check_info = NULL;\n+  null_check_info = nullptr;\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":219,"deletions":219,"binary":false,"changes":438,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -230,1 +230,1 @@\n-      _instruction_for_operand.at_put_grow(opr->vreg_number(), x, NULL);\n+      _instruction_for_operand.at_put_grow(opr->vreg_number(), x, nullptr);\n@@ -309,1 +309,1 @@\n-                       CodeEmitInfo* patch_info = NULL, CodeEmitInfo* store_emit_info = NULL);\n+                       CodeEmitInfo* patch_info = nullptr, CodeEmitInfo* store_emit_info = nullptr);\n@@ -313,1 +313,1 @@\n-                      CodeEmitInfo* patch_info = NULL, CodeEmitInfo* load_emit_info = NULL);\n+                      CodeEmitInfo* patch_info = nullptr, CodeEmitInfo* load_emit_info = nullptr);\n@@ -363,1 +363,1 @@\n-  void arithmetic_op(Bytecodes::Code code, LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr tmp, CodeEmitInfo* info = NULL);\n+  void arithmetic_op(Bytecodes::Code code, LIR_Opr result, LIR_Opr left, LIR_Opr right, LIR_Opr tmp, CodeEmitInfo* info = nullptr);\n@@ -375,1 +375,1 @@\n-  void arithmetic_op_long (Bytecodes::Code code, LIR_Opr result, LIR_Opr left, LIR_Opr right, CodeEmitInfo* info = NULL);\n+  void arithmetic_op_long (Bytecodes::Code code, LIR_Opr result, LIR_Opr left, LIR_Opr right, CodeEmitInfo* info = nullptr);\n@@ -649,1 +649,1 @@\n-    set_instruction(NULL);\n+    set_instruction(nullptr);\n@@ -655,1 +655,1 @@\n-    if (_value != NULL) {\n+    if (_value != nullptr) {\n@@ -695,1 +695,1 @@\n-  bool is_constant() const { return value()->as_Constant() != NULL; }\n+  bool is_constant() const { return value()->as_Constant() != nullptr; }\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -86,2 +86,2 @@\n- , _new_intervals_from_allocation(NULL)\n- , _sorted_intervals(NULL)\n+ , _new_intervals_from_allocation(nullptr)\n+ , _sorted_intervals(nullptr)\n@@ -96,1 +96,1 @@\n- , _fpu_stack_allocator(NULL)\n+ , _fpu_stack_allocator(nullptr)\n@@ -99,4 +99,4 @@\n-  assert(this->ir() != NULL,          \"check if valid\");\n-  assert(this->compilation() != NULL, \"check if valid\");\n-  assert(this->gen() != NULL,         \"check if valid\");\n-  assert(this->frame_map() != NULL,   \"check if valid\");\n+  assert(this->ir() != nullptr,          \"check if valid\");\n+  assert(this->compilation() != nullptr, \"check if valid\");\n+  assert(this->gen() != nullptr,         \"check if valid\");\n+  assert(this->frame_map() != nullptr,   \"check if valid\");\n@@ -271,1 +271,1 @@\n-  assert(_intervals.at(reg_num) == NULL, \"overwriting existing interval\");\n+  assert(_intervals.at(reg_num) == nullptr, \"overwriting existing interval\");\n@@ -289,1 +289,1 @@\n-  if (new_intervals == NULL) {\n+  if (new_intervals == nullptr) {\n@@ -396,1 +396,1 @@\n-  create_unhandled_lists(&interval, &temp_list, must_store_at_definition, NULL);\n+  create_unhandled_lists(&interval, &temp_list, must_store_at_definition, nullptr);\n@@ -399,1 +399,1 @@\n-  Interval* prev = NULL;\n+  Interval* prev = nullptr;\n@@ -403,1 +403,1 @@\n-    if (prev != NULL) {\n+    if (prev != nullptr) {\n@@ -435,1 +435,1 @@\n-        assert(op->as_Op1() != NULL, \"move must be LIR_Op1\");\n+        assert(op->as_Op1() != nullptr, \"move must be LIR_Op1\");\n@@ -444,1 +444,1 @@\n-          instructions->at_put(j, NULL); \/\/ NULL-instructions are deleted by assign_reg_num\n+          instructions->at_put(j, nullptr); \/\/ null-instructions are deleted by assign_reg_num\n@@ -501,2 +501,2 @@\n-  _lir_ops = LIR_OpArray(num_instructions, num_instructions, NULL);\n-  _block_of_op = BlockBeginArray(num_instructions, num_instructions, NULL);\n+  _lir_ops = LIR_OpArray(num_instructions, num_instructions, nullptr);\n+  _block_of_op = BlockBeginArray(num_instructions, num_instructions, nullptr);\n@@ -543,2 +543,2 @@\n-  assert(con == NULL || opr->is_virtual() || opr->is_constant() || opr->is_illegal(), \"assumption: Constant instructions have only constant operands\");\n-  assert(con != NULL || opr->is_virtual(), \"assumption: non-Constant instructions have only virtual operands\");\n+  assert(con == nullptr || opr->is_virtual() || opr->is_constant() || opr->is_illegal(), \"assumption: Constant instructions have only constant operands\");\n+  assert(con != nullptr || opr->is_virtual(), \"assumption: non-Constant instructions have only virtual operands\");\n@@ -546,1 +546,1 @@\n-  if ((con == NULL || con->is_pinned()) && opr->is_register()) {\n+  if ((con == nullptr || con->is_pinned()) && opr->is_register()) {\n@@ -842,1 +842,1 @@\n-        tty->print_cr(\"* vreg %d (HIR instruction %c%d)\", i, instr == NULL ? ' ' : instr->type()->tchar(), instr == NULL ? 0 : instr->id());\n+        tty->print_cr(\"* vreg %d (HIR instruction %c%d)\", i, instr == nullptr ? ' ' : instr->type()->tchar(), instr == nullptr ? 0 : instr->id());\n@@ -873,1 +873,1 @@\n-  if ((con == NULL || con->is_pinned()) && opr->is_register()) {\n+  if ((con == nullptr || con->is_pinned()) && opr->is_register()) {\n@@ -943,1 +943,1 @@\n-  if (interval != NULL) {\n+  if (interval != nullptr) {\n@@ -988,1 +988,1 @@\n-  if (interval == NULL) {\n+  if (interval == nullptr) {\n@@ -1003,1 +1003,1 @@\n-  if (interval == NULL) {\n+  if (interval == nullptr) {\n@@ -1022,1 +1022,1 @@\n-    assert(op->as_Op1() != NULL, \"lir_move must be LIR_Op1\");\n+    assert(op->as_Op1() != nullptr, \"lir_move must be LIR_Op1\");\n@@ -1058,1 +1058,1 @@\n-    assert(op->as_Op1() != NULL, \"lir_move must be LIR_Op1\");\n+    assert(op->as_Op1() != nullptr, \"lir_move must be LIR_Op1\");\n@@ -1103,1 +1103,1 @@\n-          assert(op->as_Op2() != NULL, \"must be LIR_Op2\");\n+          assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n@@ -1121,1 +1121,1 @@\n-          assert(op->as_Op2() != NULL, \"must be LIR_Op2\");\n+          assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n@@ -1145,1 +1145,1 @@\n-        assert(op->as_Op2() != NULL, \"must be LIR_Op2\");\n+        assert(op->as_Op2() != nullptr, \"must be LIR_Op2\");\n@@ -1169,1 +1169,1 @@\n-    assert(op->as_Op1() != NULL, \"must be LIR_Op1\");\n+    assert(op->as_Op1() != nullptr, \"must be LIR_Op1\");\n@@ -1205,1 +1205,1 @@\n-    assert(op->as_Op1() != NULL, \"must be LIR_Op1\");\n+    assert(op->as_Op1() != nullptr, \"must be LIR_Op1\");\n@@ -1210,1 +1210,1 @@\n-      if (address != NULL) {\n+      if (address != nullptr) {\n@@ -1226,1 +1226,1 @@\n-      assert(op->as_Op1() != NULL, \"lir_move, lir_convert must be LIR_Op1\");\n+      assert(op->as_Op1() != nullptr, \"lir_move, lir_convert must be LIR_Op1\");\n@@ -1235,1 +1235,1 @@\n-        if (from != NULL && to != NULL) {\n+        if (from != nullptr && to != nullptr) {\n@@ -1243,1 +1243,1 @@\n-      assert(op->as_Op4() != NULL, \"lir_cmove must be LIR_Op4\");\n+      assert(op->as_Op4() != nullptr, \"lir_cmove must be LIR_Op4\");\n@@ -1252,1 +1252,1 @@\n-        if (from != NULL && to != NULL) {\n+        if (from != nullptr && to != nullptr) {\n@@ -1272,1 +1272,1 @@\n-  _intervals.at_put_grow(num_virtual_regs() - 1, NULL, NULL);\n+  _intervals.at_put_grow(num_virtual_regs() - 1, nullptr, nullptr);\n@@ -1424,1 +1424,1 @@\n-    if (interval != NULL) {\n+    if (interval != nullptr) {\n@@ -1434,2 +1434,2 @@\n-  if (*a != NULL) {\n-    if (*b != NULL) {\n+  if (*a != nullptr) {\n+    if (*b != nullptr) {\n@@ -1441,1 +1441,1 @@\n-    if (*b != NULL) {\n+    if (*b != nullptr) {\n@@ -1494,1 +1494,1 @@\n-    if (it != NULL) {\n+    if (it != nullptr) {\n@@ -1508,1 +1508,1 @@\n-    if (interval != NULL) {\n+    if (interval != nullptr) {\n@@ -1516,1 +1516,1 @@\n-      \"Sorted list should contain the same amount of non-NULL intervals as unsorted list\");\n+      \"Sorted list should contain the same amount of non-null intervals as unsorted list\");\n@@ -1523,1 +1523,1 @@\n-  if (*prev != NULL) {\n+  if (*prev != nullptr) {\n@@ -1536,2 +1536,2 @@\n-  Interval* list1_prev = NULL;\n-  Interval* list2_prev = NULL;\n+  Interval* list1_prev = nullptr;\n+  Interval* list2_prev = nullptr;\n@@ -1543,1 +1543,1 @@\n-    if (v == NULL) continue;\n+    if (v == nullptr) continue;\n@@ -1547,1 +1547,1 @@\n-    } else if (is_list2 == NULL || is_list2(v)) {\n+    } else if (is_list2 == nullptr || is_list2(v)) {\n@@ -1552,2 +1552,2 @@\n-  if (list1_prev != NULL) list1_prev->set_next(Interval::end());\n-  if (list2_prev != NULL) list2_prev->set_next(Interval::end());\n+  if (list1_prev != nullptr) list1_prev->set_next(Interval::end());\n+  if (list2_prev != nullptr) list2_prev->set_next(Interval::end());\n@@ -1555,2 +1555,2 @@\n-  assert(list1_prev == NULL || list1_prev->next() == Interval::end(), \"linear list ends not with sentinel\");\n-  assert(list2_prev == NULL || list2_prev->next() == Interval::end(), \"linear list ends not with sentinel\");\n+  assert(list1_prev == nullptr || list1_prev->next() == Interval::end(), \"linear list ends not with sentinel\");\n+  assert(list2_prev == nullptr || list2_prev->next() == Interval::end(), \"linear list ends not with sentinel\");\n@@ -1578,1 +1578,1 @@\n-  \/\/ calc number of items for sorted list (sorted list must not contain NULL values)\n+  \/\/ calc number of items for sorted list (sorted list must not contain null values)\n@@ -1580,1 +1580,1 @@\n-    if (unsorted_list->at(unsorted_idx) != NULL) {\n+    if (unsorted_list->at(unsorted_idx) != nullptr) {\n@@ -1584,1 +1584,1 @@\n-  IntervalArray* sorted_list = new IntervalArray(sorted_len, sorted_len, NULL);\n+  IntervalArray* sorted_list = new IntervalArray(sorted_len, sorted_len, nullptr);\n@@ -1591,1 +1591,1 @@\n-    if (cur_interval != NULL) {\n+    if (cur_interval != nullptr) {\n@@ -1625,1 +1625,1 @@\n-  int new_len = new_list == NULL ? 0 : new_list->length();\n+  int new_len = new_list == nullptr ? 0 : new_list->length();\n@@ -1638,1 +1638,1 @@\n-  IntervalArray* combined_list = new IntervalArray(combined_list_len, combined_list_len, NULL);\n+  IntervalArray* combined_list = new IntervalArray(combined_list_len, combined_list_len, nullptr);\n@@ -1701,1 +1701,1 @@\n-\/\/ instead of returning NULL\n+\/\/ instead of returning null\n@@ -1704,1 +1704,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -1712,1 +1712,1 @@\n-  BAILOUT_(\"LinearScan: interval is NULL\", result);\n+  BAILOUT_(\"LinearScan: interval is null\", result);\n@@ -1718,1 +1718,1 @@\n-  assert(interval_at(reg_num) != NULL, \"no interval found\");\n+  assert(interval_at(reg_num) != nullptr, \"no interval found\");\n@@ -1725,1 +1725,1 @@\n-  assert(interval_at(reg_num) != NULL, \"no interval found\");\n+  assert(interval_at(reg_num) != nullptr, \"no interval found\");\n@@ -1732,1 +1732,1 @@\n-  assert(interval_at(reg_num) != NULL, \"no interval found\");\n+  assert(interval_at(reg_num) != nullptr, \"no interval found\");\n@@ -1766,1 +1766,1 @@\n-    if (branch != NULL) {\n+    if (branch != nullptr) {\n@@ -1777,1 +1777,1 @@\n-    assert(from_block->lir()->instructions_list()->at(0)->as_OpLabel() != NULL, \"block does not start with a label\");\n+    assert(from_block->lir()->instructions_list()->at(0)->as_OpLabel() != nullptr, \"block does not start with a label\");\n@@ -1863,1 +1863,1 @@\n-  if (interval_at(reg_num) == NULL) {\n+  if (interval_at(reg_num) == nullptr) {\n@@ -1936,1 +1936,1 @@\n-  if (interval_at(reg_num) == NULL) {\n+  if (interval_at(reg_num) == nullptr) {\n@@ -1946,1 +1946,1 @@\n-  if (phi != NULL) {\n+  if (phi != nullptr) {\n@@ -1958,1 +1958,1 @@\n-    if (con != NULL && (!con->is_pinned() || con->operand()->is_constant())) {\n+    if (con != nullptr && (!con->is_pinned() || con->operand()->is_constant())) {\n@@ -1986,1 +1986,1 @@\n-  assert(handler->entry_code() == NULL, \"code already present\");\n+  assert(handler->entry_code() == nullptr, \"code already present\");\n@@ -1992,1 +1992,1 @@\n-    resolve_exception_edge(handler, throwing_op_id, r, NULL, move_resolver);\n+    resolve_exception_edge(handler, throwing_op_id, r, nullptr, move_resolver);\n@@ -2218,1 +2218,1 @@\n-  assert(interval != NULL, \"interval must exist\");\n+  assert(interval != nullptr, \"interval must exist\");\n@@ -2228,1 +2228,1 @@\n-      if (branch != NULL) {\n+      if (branch != nullptr) {\n@@ -2268,1 +2268,1 @@\n-  if (values == NULL) {\n+  if (values == nullptr) {\n@@ -2283,1 +2283,1 @@\n-  if (values == NULL) {\n+  if (values == nullptr) {\n@@ -2332,2 +2332,2 @@\n-  if (d1->locals() != NULL) {\n-    assert(d1->locals() != NULL && d2->locals() != NULL, \"not equal\");\n+  if (d1->locals() != nullptr) {\n+    assert(d1->locals() != nullptr && d2->locals() != nullptr, \"not equal\");\n@@ -2339,1 +2339,1 @@\n-    assert(d1->locals() == NULL && d2->locals() == NULL, \"not equal\");\n+    assert(d1->locals() == nullptr && d2->locals() == nullptr, \"not equal\");\n@@ -2342,2 +2342,2 @@\n-  if (d1->expressions() != NULL) {\n-    assert(d1->expressions() != NULL && d2->expressions() != NULL, \"not equal\");\n+  if (d1->expressions() != nullptr) {\n+    assert(d1->expressions() != nullptr && d2->expressions() != nullptr, \"not equal\");\n@@ -2349,1 +2349,1 @@\n-    assert(d1->expressions() == NULL && d2->expressions() == NULL, \"not equal\");\n+    assert(d1->expressions() == nullptr && d2->expressions() == nullptr, \"not equal\");\n@@ -2352,2 +2352,2 @@\n-  if (d1->monitors() != NULL) {\n-    assert(d1->monitors() != NULL && d2->monitors() != NULL, \"not equal\");\n+  if (d1->monitors() != nullptr) {\n+    assert(d1->monitors() != nullptr && d2->monitors() != nullptr, \"not equal\");\n@@ -2359,1 +2359,1 @@\n-    assert(d1->monitors() == NULL && d2->monitors() == NULL, \"not equal\");\n+    assert(d1->monitors() == nullptr && d2->monitors() == nullptr, \"not equal\");\n@@ -2362,2 +2362,2 @@\n-  if (d1->caller() != NULL) {\n-    assert(d1->caller() != NULL && d2->caller() != NULL, \"not equal\");\n+  if (d1->caller() != nullptr) {\n+    assert(d1->caller() != nullptr && d2->caller() != nullptr, \"not equal\");\n@@ -2366,1 +2366,1 @@\n-    assert(d1->caller() == NULL && d2->caller() == NULL, \"not equal\");\n+    assert(d1->caller() == nullptr && d2->caller() == nullptr, \"not equal\");\n@@ -2406,1 +2406,1 @@\n-  create_unhandled_lists(&oop_intervals, &non_oop_intervals, is_oop_interval, NULL);\n+  create_unhandled_lists(&oop_intervals, &non_oop_intervals, is_oop_interval, nullptr);\n@@ -2467,1 +2467,1 @@\n-  assert(info->stack() != NULL, \"CodeEmitInfo must always have a stack\");\n+  assert(info->stack() != nullptr, \"CodeEmitInfo must always have a stack\");\n@@ -2499,1 +2499,1 @@\n-    if (info->_oop_map == NULL) {\n+    if (info->_oop_map == nullptr) {\n@@ -2516,1 +2516,1 @@\n-ConstantOopWriteValue* LinearScan::_oop_null_scope_value = new (mtCompiler) ConstantOopWriteValue(NULL);\n+ConstantOopWriteValue* LinearScan::_oop_null_scope_value = new (mtCompiler) ConstantOopWriteValue(nullptr);\n@@ -2527,1 +2527,1 @@\n-  _scope_value_cache = ScopeValueArray(cache_size, cache_size, NULL);\n+  _scope_value_cache = ScopeValueArray(cache_size, cache_size, nullptr);\n@@ -2560,1 +2560,1 @@\n-      if (value == NULL) {\n+      if (value == nullptr) {\n@@ -2620,1 +2620,1 @@\n-    if (sv == NULL) {\n+    if (sv == nullptr) {\n@@ -2638,1 +2638,1 @@\n-    if (sv == NULL) {\n+    if (sv == nullptr) {\n@@ -2666,1 +2666,1 @@\n-    assert(_fpu_stack_allocator != NULL, \"must be present\");\n+    assert(_fpu_stack_allocator != nullptr, \"must be present\");\n@@ -2713,1 +2713,1 @@\n-      if (!frame_map()->locations_for_slot(opr->double_stack_ix(), loc_type, &loc1, NULL)) {\n+      if (!frame_map()->locations_for_slot(opr->double_stack_ix(), loc_type, &loc1, nullptr)) {\n@@ -2779,1 +2779,1 @@\n-      assert(_fpu_stack_allocator != NULL, \"must be present\");\n+      assert(_fpu_stack_allocator != nullptr, \"must be present\");\n@@ -2811,2 +2811,2 @@\n-      first = NULL;\n-      second = NULL;\n+      first = nullptr;\n+      second = nullptr;\n@@ -2815,1 +2815,1 @@\n-    assert(first != NULL && second != NULL, \"must be set\");\n+    assert(first != nullptr && second != nullptr, \"must be set\");\n@@ -2830,1 +2830,1 @@\n-  if (value != NULL) {\n+  if (value != nullptr) {\n@@ -2834,2 +2834,2 @@\n-    assert(con == NULL || opr->is_virtual() || opr->is_constant() || opr->is_illegal(), \"assumption: Constant instructions have only constant operands (or illegal if constant is optimized away)\");\n-    assert(con != NULL || opr->is_virtual(), \"assumption: non-Constant instructions have only virtual operands\");\n+    assert(con == nullptr || opr->is_virtual() || opr->is_constant() || opr->is_illegal(), \"assumption: Constant instructions have only constant operands (or illegal if constant is optimized away)\");\n+    assert(con != nullptr || opr->is_virtual(), \"assumption: non-Constant instructions have only virtual operands\");\n@@ -2837,1 +2837,1 @@\n-    if (con != NULL && !con->is_pinned() && !opr->is_constant()) {\n+    if (con != nullptr && !con->is_pinned() && !opr->is_constant()) {\n@@ -2855,1 +2855,1 @@\n-        if (block->lir()->instructions_list()->last()->as_OpBranch() != NULL) {\n+        if (block->lir()->instructions_list()->last()->as_OpBranch() != nullptr) {\n@@ -2873,1 +2873,1 @@\n-      assert(value->as_Constant() != NULL, \"all other instructions have only virtual operands\");\n+      assert(value->as_Constant() != nullptr, \"all other instructions have only virtual operands\");\n@@ -2887,1 +2887,1 @@\n-  IRScopeDebugInfo* caller_debug_info = NULL;\n+  IRScopeDebugInfo* caller_debug_info = nullptr;\n@@ -2890,1 +2890,1 @@\n-  if (caller_state != NULL) {\n+  if (caller_state != nullptr) {\n@@ -2898,3 +2898,3 @@\n-  GrowableArray<ScopeValue*>*   locals      = NULL;\n-  GrowableArray<ScopeValue*>*   expressions = NULL;\n-  GrowableArray<MonitorValue*>* monitors    = NULL;\n+  GrowableArray<ScopeValue*>*   locals      = nullptr;\n+  GrowableArray<ScopeValue*>*   expressions = nullptr;\n+  GrowableArray<MonitorValue*>* monitors    = nullptr;\n@@ -2945,1 +2945,1 @@\n-    int lock_offset = cur_state->caller_state() != NULL ? cur_state->caller_state()->total_locks_size() : 0;\n+    int lock_offset = cur_state->caller_state() != nullptr ? cur_state->caller_state()->total_locks_size() : 0;\n@@ -2962,1 +2962,1 @@\n-  assert(innermost_scope != NULL && innermost_state != NULL, \"why is it missing?\");\n+  assert(innermost_scope != nullptr && innermost_state != nullptr, \"why is it missing?\");\n@@ -2966,1 +2966,1 @@\n-  if (info->_scope_debug_info == NULL) {\n+  if (info->_scope_debug_info == nullptr) {\n@@ -2983,1 +2983,1 @@\n-    if (op == NULL) {  \/\/ this can happen when spill-moves are removed in eliminate_spill_moves\n+    if (op == nullptr) { \/\/ this can happen when spill-moves are removed in eliminate_spill_moves\n@@ -3010,2 +3010,2 @@\n-          if (handler->entry_code() != NULL) {\n-            assign_reg_num(handler->entry_code()->instructions_list(), NULL);\n+          if (handler->entry_code() != nullptr) {\n+            assign_reg_num(handler->entry_code()->instructions_list(), nullptr);\n@@ -3019,1 +3019,1 @@\n-      assert(iw != NULL, \"needed for compute_oop_map\");\n+      assert(iw != nullptr, \"needed for compute_oop_map\");\n@@ -3042,1 +3042,1 @@\n-      assert(op->as_Op1() != NULL, \"move must be LIR_Op1\");\n+      assert(op->as_Op1() != nullptr, \"move must be LIR_Op1\");\n@@ -3049,1 +3049,1 @@\n-        instructions->at_put(j, NULL);\n+        instructions->at_put(j, nullptr);\n@@ -3060,1 +3060,1 @@\n-      if (op != NULL) {\n+      if (op != nullptr) {\n@@ -3183,1 +3183,1 @@\n-      if (interval != NULL) {\n+      if (interval != nullptr) {\n@@ -3257,1 +3257,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -3291,1 +3291,1 @@\n-    if (i1 == NULL) continue;\n+    if (i1 == nullptr) continue;\n@@ -3338,1 +3338,1 @@\n-      if (i2 == NULL || (i2->from() == 1 && i2->to() == 2)) continue;\n+      if (i2 == nullptr || (i2->from() == 1 && i2->to() == 2)) continue;\n@@ -3360,1 +3360,1 @@\n-  create_unhandled_lists(&fixed_intervals, &other_intervals, is_precolored_cpu_interval, NULL);\n+  create_unhandled_lists(&fixed_intervals, &other_intervals, is_precolored_cpu_interval, nullptr);\n@@ -3388,1 +3388,1 @@\n-        if (branch != NULL && branch->stub() != NULL && branch->stub()->is_exception_throw_stub()) {\n+        if (branch != nullptr && branch->stub() != nullptr && branch->stub()->is_exception_throw_stub()) {\n@@ -3441,1 +3441,1 @@\n-              assert(interval != NULL, \"no interval\");\n+              assert(interval != nullptr, \"no interval\");\n@@ -3480,1 +3480,1 @@\n-      assert(value != NULL, \"all intervals live across block boundaries must have Value\");\n+      assert(value != nullptr, \"all intervals live across block boundaries must have Value\");\n@@ -3483,1 +3483,1 @@\n-      \/\/ TKR assert(value->as_Constant() == NULL || value->is_pinned(), \"only pinned constants can be alive across block boundaries\");\n+      \/\/ TKR assert(value->as_Constant() == nullptr || value->is_pinned(), \"only pinned constants can be alive across block boundaries\");\n@@ -3523,1 +3523,1 @@\n-    , _saved_states(BlockBegin::number_of_blocks(), BlockBegin::number_of_blocks(), NULL)\n+    , _saved_states(BlockBegin::number_of_blocks(), BlockBegin::number_of_blocks(), nullptr)\n@@ -3540,1 +3540,1 @@\n-  IntervalList* input_state = new IntervalList(input_state_len, input_state_len, NULL);\n+  IntervalList* input_state = new IntervalList(input_state_len, input_state_len, nullptr);\n@@ -3578,1 +3578,1 @@\n-      if (input_state->at(i) != NULL) {\n+      if (input_state->at(i) != nullptr) {\n@@ -3603,1 +3603,1 @@\n-  if (xhandler->entry_code() != NULL) {\n+  if (xhandler->entry_code() != nullptr) {\n@@ -3612,1 +3612,1 @@\n-  if (saved_state != NULL) {\n+  if (saved_state != nullptr) {\n@@ -3621,1 +3621,1 @@\n-        if (saved_state->at(i) != NULL) {\n+        if (saved_state->at(i) != nullptr) {\n@@ -3626,1 +3626,1 @@\n-          saved_state->at_put(i, NULL);\n+          saved_state->at_put(i, nullptr);\n@@ -3660,1 +3660,1 @@\n-    if (interval != NULL) {\n+    if (interval != nullptr) {\n@@ -3662,2 +3662,2 @@\n-    } else if (input_state->at(reg) != NULL) {\n-      TRACE_LINEAR_SCAN(4, tty->print_cr(\"        reg[%d] = NULL\", reg));\n+    } else if (input_state->at(reg) != nullptr) {\n+      TRACE_LINEAR_SCAN(4, tty->print_cr(\"        reg[%d] = null\", reg));\n@@ -3708,2 +3708,2 @@\n-          state_put(input_state, interval->assigned_reg(),   NULL);\n-          state_put(input_state, interval->assigned_regHi(), NULL);\n+          state_put(input_state, interval->assigned_reg(),   nullptr);\n+          state_put(input_state, interval->assigned_regHi(), nullptr);\n@@ -3717,1 +3717,1 @@\n-        state_put(input_state, reg_num(FrameMap::caller_save_cpu_reg_at(j)), NULL);\n+        state_put(input_state, reg_num(FrameMap::caller_save_cpu_reg_at(j)), nullptr);\n@@ -3720,1 +3720,1 @@\n-        state_put(input_state, reg_num(FrameMap::caller_save_fpu_reg_at(j)), NULL);\n+        state_put(input_state, reg_num(FrameMap::caller_save_fpu_reg_at(j)), nullptr);\n@@ -3726,1 +3726,1 @@\n-        state_put(input_state, reg_num(FrameMap::caller_save_xmm_reg_at(j)), NULL);\n+        state_put(input_state, reg_num(FrameMap::caller_save_xmm_reg_at(j)), nullptr);\n@@ -3738,1 +3738,1 @@\n-    \/\/ set temp operands (some operations use temp operands also as output operands, so can't set them NULL)\n+    \/\/ set temp operands (some operations use temp operands also as output operands, so can't set them null)\n@@ -3779,1 +3779,1 @@\n-  _insert_list(NULL),\n+  _insert_list(nullptr),\n@@ -3807,1 +3807,1 @@\n-  assert(_insert_list != NULL && _insert_idx != -1, \"insert position not set\");\n+  assert(_insert_list != nullptr && _insert_idx != -1, \"insert position not set\");\n@@ -3813,1 +3813,1 @@\n-        assert(_mapping_from.at(i) == NULL || _mapping_from.at(i) != _mapping_from.at(j), \"cannot read from same interval twice\");\n+        assert(_mapping_from.at(i) == nullptr || _mapping_from.at(i) != _mapping_from.at(j), \"cannot read from same interval twice\");\n@@ -3829,1 +3829,1 @@\n-      if (it != NULL) {\n+      if (it != nullptr) {\n@@ -3856,1 +3856,1 @@\n-    if (it != NULL && it->assigned_reg() >= LinearScan::nof_regs) {\n+    if (it != nullptr && it->assigned_reg() >= LinearScan::nof_regs) {\n@@ -3901,1 +3901,1 @@\n-  if (from != NULL) {\n+  if (from != nullptr) {\n@@ -3934,1 +3934,1 @@\n-  _insert_list = NULL;\n+  _insert_list = nullptr;\n@@ -3941,1 +3941,1 @@\n-  assert(_insert_list != NULL && _insert_idx != -1, \"must setup insert position first\");\n+  assert(_insert_list != nullptr && _insert_idx != -1, \"must setup insert position first\");\n@@ -3960,1 +3960,1 @@\n-  assert(_insert_list != NULL && _insert_idx != -1, \"must setup insert position first\");\n+  assert(_insert_list != nullptr && _insert_idx != -1, \"must setup insert position first\");\n@@ -3986,1 +3986,1 @@\n-  TRACE_LINEAR_SCAN(4, tty->print_cr(\"MoveResolver: resolving mappings for Block B%d, index %d\", _insert_list->block() != NULL ? _insert_list->block()->block_id() : -1, _insert_idx));\n+  TRACE_LINEAR_SCAN(4, tty->print_cr(\"MoveResolver: resolving mappings for Block B%d, index %d\", _insert_list->block() != nullptr ? _insert_list->block()->block_id() : -1, _insert_idx));\n@@ -3995,1 +3995,1 @@\n-    if (from_interval != NULL) {\n+    if (from_interval != nullptr) {\n@@ -4010,1 +4010,1 @@\n-        if (from_interval != NULL) {\n+        if (from_interval != nullptr) {\n@@ -4021,1 +4021,1 @@\n-      } else if (from_interval != NULL && from_interval->assigned_reg() < LinearScan::nof_regs) {\n+      } else if (from_interval != nullptr && from_interval->assigned_reg() < LinearScan::nof_regs) {\n@@ -4071,2 +4071,2 @@\n-  TRACE_LINEAR_SCAN(4, tty->print_cr(\"MoveResolver: setting insert position to Block B%d, index %d\", insert_list->block() != NULL ? insert_list->block()->block_id() : -1, insert_idx));\n-  assert(_insert_list == NULL && _insert_idx == -1, \"use move_insert_position instead of set_insert_position when data already set\");\n+  TRACE_LINEAR_SCAN(4, tty->print_cr(\"MoveResolver: setting insert position to Block B%d, index %d\", insert_list->block() != nullptr ? insert_list->block()->block_id() : -1, insert_idx));\n+  assert(_insert_list == nullptr && _insert_idx == -1, \"use move_insert_position instead of set_insert_position when data already set\");\n@@ -4080,1 +4080,1 @@\n-  TRACE_LINEAR_SCAN(4, tty->print_cr(\"MoveResolver: moving insert position to Block B%d, index %d\", insert_list->block() != NULL ? insert_list->block()->block_id() : -1, insert_idx));\n+  TRACE_LINEAR_SCAN(4, tty->print_cr(\"MoveResolver: moving insert position to Block B%d, index %d\", insert_list->block() != nullptr ? insert_list->block()->block_id() : -1, insert_idx));\n@@ -4082,1 +4082,1 @@\n-  if (_insert_list != NULL && (insert_list != _insert_list || insert_idx != _insert_idx)) {\n+  if (_insert_list != nullptr && (insert_list != _insert_list || insert_idx != _insert_idx)) {\n@@ -4111,1 +4111,1 @@\n-  _mapping_from.append(NULL);\n+  _mapping_from.append(nullptr);\n@@ -4135,1 +4135,1 @@\n-Range* Range::_end = NULL;\n+Range* Range::_end = nullptr;\n@@ -4139,1 +4139,1 @@\n-  _end = ::new(static_cast<void*>(end_storage)) Range(max_jint, max_jint, NULL);\n+  _end = ::new(static_cast<void*>(end_storage)) Range(max_jint, max_jint, nullptr);\n@@ -4145,1 +4145,1 @@\n-  assert(r1 != NULL && r2 != NULL, \"null ranges not allowed\");\n+  assert(r1 != nullptr && r2 != nullptr, \"null ranges not allowed\");\n@@ -4184,1 +4184,1 @@\n-Interval* Interval::_end = NULL;\n+Interval* Interval::_end = nullptr;\n@@ -4205,1 +4205,1 @@\n-  _split_children(NULL),\n+  _split_children(nullptr),\n@@ -4210,1 +4210,1 @@\n-  _register_hint(NULL)\n+  _register_hint(nullptr)\n@@ -4230,1 +4230,1 @@\n-  if (_split_children != NULL && _split_children->length() > 0) {\n+  if (_split_children != nullptr && _split_children->length() > 0) {\n@@ -4262,1 +4262,1 @@\n-  if (_register_hint != NULL) {\n+  if (_register_hint != nullptr) {\n@@ -4268,1 +4268,1 @@\n-    } else if (_register_hint->_split_children != NULL && _register_hint->_split_children->length() > 0) {\n+    } else if (_register_hint->_split_children != nullptr && _register_hint->_split_children->length() > 0) {\n@@ -4282,1 +4282,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -4291,1 +4291,1 @@\n-  if (_split_children == NULL || _split_children->length() == 0) {\n+  if (_split_children == nullptr || _split_children->length() == 0) {\n@@ -4294,1 +4294,1 @@\n-    result = NULL;\n+    result = nullptr;\n@@ -4329,1 +4329,1 @@\n-  assert(result != NULL, \"no matching interval found\");\n+  assert(result != nullptr, \"no matching interval found\");\n@@ -4341,1 +4341,1 @@\n-  Interval* result = NULL;\n+  Interval* result = nullptr;\n@@ -4343,1 +4343,1 @@\n-  assert(parent->_split_children != NULL, \"no split children available\");\n+  assert(parent->_split_children != nullptr, \"no split children available\");\n@@ -4349,1 +4349,1 @@\n-    if (cur->to() <= op_id && (result == NULL || result->to() < cur->to())) {\n+    if (cur->to() <= op_id && (result == nullptr || result->to() < cur->to())) {\n@@ -4354,1 +4354,1 @@\n-  assert(result != NULL, \"no split child found\");\n+  assert(result != nullptr, \"no split child found\");\n@@ -4463,1 +4463,1 @@\n-  if (parent->_split_children == NULL) {\n+  if (parent->_split_children == nullptr) {\n@@ -4491,1 +4491,1 @@\n-  Range* prev = NULL;\n+  Range* prev = nullptr;\n@@ -4505,1 +4505,1 @@\n-    assert(prev != NULL, \"split before start of first range\");\n+    assert(prev != nullptr, \"split before start of first range\");\n@@ -4626,1 +4626,1 @@\n-  if (interval->_split_children != NULL) {\n+  if (interval->_split_children != nullptr) {\n@@ -4670,1 +4670,1 @@\n-  out->print(\"%d %d \", split_parent()->reg_num(), (register_hint(false) != NULL ? register_hint(false)->reg_num() : -1));\n+  out->print(\"%d %d \", split_parent()->reg_num(), (register_hint(false) != nullptr ? register_hint(false)->reg_num() : -1));\n@@ -4677,1 +4677,1 @@\n-    assert(cur != NULL, \"range list not closed with range sentinel\");\n+    assert(cur != nullptr, \"range list not closed with range sentinel\");\n@@ -4704,1 +4704,1 @@\n-  if (_split_children == NULL) {\n+  if (_split_children == nullptr) {\n@@ -4732,1 +4732,1 @@\n-  _current = NULL;\n+  _current = nullptr;\n@@ -4739,1 +4739,1 @@\n-  Interval* prev = NULL;\n+  Interval* prev = nullptr;\n@@ -4744,1 +4744,1 @@\n-  if (prev == NULL) {\n+  if (prev == nullptr) {\n@@ -4755,1 +4755,1 @@\n-  Interval* prev = NULL;\n+  Interval* prev = nullptr;\n@@ -4760,1 +4760,1 @@\n-  if (prev == NULL) {\n+  if (prev == nullptr) {\n@@ -4865,1 +4865,1 @@\n-    _current = NULL; return;\n+    _current = nullptr; return;\n@@ -4877,1 +4877,1 @@\n-  while (current() != NULL) {\n+  while (current() != nullptr) {\n@@ -5303,1 +5303,1 @@\n-    while (parent != NULL && parent->is_split_child()) {\n+    while (parent != nullptr && parent->is_split_child()) {\n@@ -5313,1 +5313,1 @@\n-          parent = NULL;\n+          parent = nullptr;\n@@ -5483,1 +5483,1 @@\n-  if (register_hint != NULL) {\n+  if (register_hint != nullptr) {\n@@ -5799,1 +5799,1 @@\n-  assert(op->as_Op1() != NULL, \"move must be LIR_Op1\");\n+  assert(op->as_Op1() != nullptr, \"move must be LIR_Op1\");\n@@ -5815,1 +5815,1 @@\n-  if (register_hint == NULL) {\n+  if (register_hint == nullptr) {\n@@ -5922,1 +5922,1 @@\n-    assert(cur->current_split_child() != NULL, \"must be\");\n+    assert(cur->current_split_child() != nullptr, \"must be\");\n@@ -5979,1 +5979,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -5996,1 +5996,1 @@\n-  if (op1 == NULL || op2 == NULL) {\n+  if (op1 == nullptr || op2 == nullptr) {\n@@ -6002,2 +6002,2 @@\n-    assert(op1->as_Op1() != NULL, \"move must be LIR_Op1\");\n-    assert(op2->as_Op1() != NULL, \"move must be LIR_Op1\");\n+    assert(op1->as_Op1() != nullptr, \"move must be LIR_Op1\");\n+    assert(op2->as_Op1() != nullptr, \"move must be LIR_Op1\");\n@@ -6012,2 +6012,2 @@\n-    assert(op1->as_Op1() != NULL, \"fxch must be LIR_Op1\");\n-    assert(op2->as_Op1() != NULL, \"fxch must be LIR_Op1\");\n+    assert(op1->as_Op1() != nullptr, \"fxch must be LIR_Op1\");\n+    assert(op2->as_Op1() != nullptr, \"fxch must be LIR_Op1\");\n@@ -6058,1 +6058,1 @@\n-    assert(pred_instructions->last()->as_OpBranch() != NULL, \"branch must be LIR_OpBranch\");\n+    assert(pred_instructions->last()->as_OpBranch() != nullptr, \"branch must be LIR_OpBranch\");\n@@ -6061,1 +6061,1 @@\n-    if (pred_instructions->last()->info() != NULL) {\n+    if (pred_instructions->last()->info() != nullptr) {\n@@ -6105,1 +6105,1 @@\n-  assert(cur_instructions->last()->as_OpBranch() != NULL, \"branch must be LIR_OpBranch\");\n+  assert(cur_instructions->last()->as_OpBranch() != nullptr, \"branch must be LIR_OpBranch\");\n@@ -6108,1 +6108,1 @@\n-  if (cur_instructions->last()->info() != NULL) {\n+  if (cur_instructions->last()->info() != nullptr) {\n@@ -6114,1 +6114,1 @@\n-  if (branch->info() != NULL || (branch->code() != lir_branch && branch->code() != lir_cond_float_branch)) {\n+  if (branch->info() != nullptr || (branch->code() != lir_branch && branch->code() != lir_cond_float_branch)) {\n@@ -6129,1 +6129,1 @@\n-    if ((op->code() == lir_branch || op->code() == lir_cond_float_branch) && ((LIR_OpBranch*)op)->block() != NULL) {\n+    if ((op->code() == lir_branch || op->code() == lir_cond_float_branch) && ((LIR_OpBranch*)op)->block() != nullptr) {\n@@ -6258,1 +6258,1 @@\n-  assert(instructions->last()->as_OpBranch() != NULL, \"last instruction must always be a branch\");\n+  assert(instructions->last()->as_OpBranch() != nullptr, \"last instruction must always be a branch\");\n@@ -6264,1 +6264,1 @@\n-  if (instructions->length() == 2 && instructions->last()->info() == NULL) {\n+  if (instructions->length() == 2 && instructions->last()->info() == nullptr) {\n@@ -6281,1 +6281,1 @@\n-      assert(op->as_OpBranch() != NULL, \"branch must be of type LIR_OpBranch\");\n+      assert(op->as_OpBranch() != nullptr, \"branch must be of type LIR_OpBranch\");\n@@ -6350,1 +6350,1 @@\n-      assert(last_op->as_OpBranch() != NULL, \"branch must be of type LIR_OpBranch\");\n+      assert(last_op->as_OpBranch() != nullptr, \"branch must be of type LIR_OpBranch\");\n@@ -6353,1 +6353,1 @@\n-      assert(last_branch->block() != NULL, \"last branch must always have a block as target\");\n+      assert(last_branch->block() != nullptr, \"last branch must always have a block as target\");\n@@ -6356,1 +6356,1 @@\n-      if (last_branch->info() == NULL) {\n+      if (last_branch->info() == nullptr) {\n@@ -6367,1 +6367,1 @@\n-            assert(prev_op->as_OpBranch() != NULL, \"branch must be of type LIR_OpBranch\");\n+            assert(prev_op->as_OpBranch() != nullptr, \"branch must be of type LIR_OpBranch\");\n@@ -6370,1 +6370,1 @@\n-            if (prev_branch->stub() == NULL) {\n+            if (prev_branch->stub() == nullptr) {\n@@ -6372,1 +6372,1 @@\n-              LIR_Op2* prev_cmp = NULL;\n+              LIR_Op2* prev_cmp = nullptr;\n@@ -6376,1 +6376,1 @@\n-              LIR_Op4* prev_cmove = NULL;\n+              LIR_Op4* prev_cmove = nullptr;\n@@ -6378,1 +6378,1 @@\n-              for(int j = instructions->length() - 3; j >= 0 && prev_cmp == NULL; j--) {\n+              for(int j = instructions->length() - 3; j >= 0 && prev_cmp == nullptr; j--) {\n@@ -6382,1 +6382,1 @@\n-                  assert(prev_op->as_Op4() != NULL, \"cmove must be of type LIR_Op4\");\n+                  assert(prev_op->as_Op4() != nullptr, \"cmove must be of type LIR_Op4\");\n@@ -6387,1 +6387,1 @@\n-                  assert(prev_op->as_Op2() != NULL, \"branch must be of type LIR_Op2\");\n+                  assert(prev_op->as_Op2() != nullptr, \"branch must be of type LIR_Op2\");\n@@ -6393,2 +6393,2 @@\n-              guarantee(prev_cmp != NULL, \"should have found comp instruction for branch\");\n-              if (prev_branch->block() == code->at(i + 1) && prev_branch->info() == NULL) {\n+              guarantee(prev_cmp != nullptr, \"should have found comp instruction for branch\");\n+              if (prev_branch->block() == code->at(i + 1) && prev_branch->info() == nullptr) {\n@@ -6404,1 +6404,1 @@\n-                if (prev_cmove != NULL) {\n+                if (prev_cmove != nullptr) {\n@@ -6441,1 +6441,1 @@\n-      assert(cur_last_op->info() == NULL, \"return instructions do not have debug information\");\n+      assert(cur_last_op->info() == nullptr, \"return instructions do not have debug information\");\n@@ -6446,1 +6446,1 @@\n-      assert(cur_last_op->as_Op1() != NULL, \"return must be LIR_Op1\");\n+      assert(cur_last_op->as_Op1() != nullptr, \"return must be LIR_Op1\");\n@@ -6455,1 +6455,1 @@\n-          assert(pred_last_op->as_OpBranch() != NULL, \"branch must be LIR_OpBranch\");\n+          assert(pred_last_op->as_OpBranch() != nullptr, \"branch must be LIR_OpBranch\");\n@@ -6458,1 +6458,1 @@\n-          if (pred_last_branch->block() == block && pred_last_branch->cond() == lir_cond_always && pred_last_branch->info() == NULL) {\n+          if (pred_last_branch->block() == block && pred_last_branch->cond() == lir_cond_always && pred_last_branch->info() == nullptr) {\n@@ -6483,3 +6483,3 @@\n-      if (op_branch != NULL) {\n-        assert(op_branch->block() == NULL || code->find(op_branch->block()) != -1, \"branch target not valid\");\n-        assert(op_branch->ublock() == NULL || code->find(op_branch->ublock()) != -1, \"branch target not valid\");\n+      if (op_branch != nullptr) {\n+        assert(op_branch->block() == nullptr || code->find(op_branch->block()) != -1, \"branch target not valid\");\n+        assert(op_branch->ublock() == nullptr || code->find(op_branch->ublock()) != -1, \"branch target not valid\");\n@@ -6633,1 +6633,1 @@\n-    if (cur != NULL) {\n+    if (cur != nullptr) {\n@@ -6720,1 +6720,1 @@\n-          if (branch->block() == NULL) {\n+          if (branch->block() == nullptr) {\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":241,"deletions":241,"binary":false,"changes":482,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,1 +59,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -65,1 +65,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -100,1 +100,1 @@\n-  if (if_ == NULL) return;\n+  if (if_ == nullptr) return;\n@@ -115,3 +115,3 @@\n-  Value t_const = NULL;\n-  Value f_const = NULL;\n-  if (t_cur->as_Constant() != NULL && !t_cur->can_trap()) {\n+  Value t_const = nullptr;\n+  Value f_const = nullptr;\n+  if (t_cur->as_Constant() != nullptr && !t_cur->can_trap()) {\n@@ -121,1 +121,1 @@\n-  if (f_cur->as_Constant() != NULL && !f_cur->can_trap()) {\n+  if (f_cur->as_Constant() != nullptr && !f_cur->can_trap()) {\n@@ -128,1 +128,1 @@\n-  if (t_goto == NULL) return;\n+  if (t_goto == nullptr) return;\n@@ -130,1 +130,1 @@\n-  if (f_goto == NULL) return;\n+  if (f_goto == nullptr) return;\n@@ -143,1 +143,1 @@\n-      assert(if_state != NULL, \"states do not match up\");\n+      assert(if_state != nullptr, \"states do not match up\");\n@@ -148,1 +148,1 @@\n-      assert(sux_state != NULL, \"states do not match up\");\n+      assert(sux_state != nullptr, \"states do not match up\");\n@@ -157,1 +157,1 @@\n-  if (sux_phi == NULL || sux_phi->as_Phi() == NULL || sux_phi->as_Phi()->block() != sux) return;\n+  if (sux_phi == nullptr || sux_phi->as_Phi() == nullptr || sux_phi->as_Phi()->block() != sux) return;\n@@ -224,1 +224,1 @@\n-  assert(result != NULL, \"make_ifop must return a non-null instruction\");\n+  assert(result != nullptr, \"make_ifop must return a non-null instruction\");\n@@ -295,1 +295,1 @@\n-  if (y_const != NULL) {\n+  if (y_const != nullptr) {\n@@ -297,1 +297,1 @@\n-    if (x_ifop != NULL) {                 \/\/ x is an ifop, y is a constant\n+    if (x_ifop != nullptr) {                 \/\/ x is an ifop, y is a constant\n@@ -301,1 +301,1 @@\n-      if (x_tval_const != NULL && x_fval_const != NULL) {\n+      if (x_tval_const != nullptr && x_fval_const != nullptr) {\n@@ -322,1 +322,1 @@\n-      if (x_const != NULL) {         \/\/ x and y are constants\n+      if (x_const != nullptr) { \/\/ x and y are constants\n@@ -366,1 +366,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -372,1 +372,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -378,1 +378,1 @@\n-    if (end->as_Goto() == NULL) return false;\n+    if (end->as_Goto() == nullptr) return false;\n@@ -410,1 +410,1 @@\n-      if (sux_phi != NULL && sux_phi->is_illegal()) continue;\n+      if (sux_phi != nullptr && sux_phi->is_illegal()) continue;\n@@ -425,1 +425,1 @@\n-    assert(prev->as_BlockEnd() == NULL, \"must not be a BlockEnd\");\n+    assert(prev->as_BlockEnd() == nullptr, \"must not be a BlockEnd\");\n@@ -478,1 +478,1 @@\n-          \/\/ becomes NULL In such (rare) cases it is not\n+          \/\/ becomes null In such (rare) cases it is not\n@@ -481,1 +481,1 @@\n-          while (prev != NULL && prev->next() != if_) {\n+          while (prev != nullptr && prev->next() != if_) {\n@@ -485,1 +485,1 @@\n-          if (prev != NULL) {\n+          if (prev != nullptr) {\n@@ -616,1 +616,1 @@\n-    assert(_visitable_instructions != NULL, \"check\");\n+    assert(_visitable_instructions != nullptr, \"check\");\n@@ -620,1 +620,1 @@\n-    assert(_visitable_instructions != NULL, \"check\");\n+    assert(_visitable_instructions != nullptr, \"check\");\n@@ -624,1 +624,1 @@\n-    assert(_visitable_instructions != NULL, \"check\");\n+    assert(_visitable_instructions != nullptr, \"check\");\n@@ -628,1 +628,1 @@\n-    assert(_visitable_instructions != NULL, \"check\");\n+    assert(_visitable_instructions != nullptr, \"check\");\n@@ -637,3 +637,3 @@\n-  bool set_contains(Value x)                      { assert(_set != NULL, \"check\"); return _set->contains(x); }\n-  void set_put     (Value x)                      { assert(_set != NULL, \"check\"); _set->put(x); }\n-  void set_remove  (Value x)                      { assert(_set != NULL, \"check\"); _set->remove(x); }\n+  bool set_contains(Value x)                      { assert(_set != nullptr, \"check\"); return _set->contains(x); }\n+  void set_put     (Value x)                      { assert(_set != nullptr, \"check\"); _set->put(x); }\n+  void set_remove  (Value x)                      { assert(_set != nullptr, \"check\"); _set->remove(x); }\n@@ -660,2 +660,2 @@\n-    , _block_states(BlockBegin::number_of_blocks(), BlockBegin::number_of_blocks(), NULL)\n-    , _last_explicit_null_check(NULL) {\n+    , _block_states(BlockBegin::number_of_blocks(), BlockBegin::number_of_blocks(), nullptr)\n+    , _last_explicit_null_check(nullptr) {\n@@ -665,1 +665,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -671,1 +671,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -694,1 +694,1 @@\n-                                                                         : NULL); }\n+                                                                         : nullptr); }\n@@ -700,1 +700,1 @@\n-  void        clear_last_explicit_null_check()               { _last_explicit_null_check = NULL; }\n+  void        clear_last_explicit_null_check()               { _last_explicit_null_check = nullptr; }\n@@ -793,1 +793,1 @@\n-  assert(*p != NULL, \"should not find NULL instructions\");\n+  assert(*p != nullptr, \"should not find null instructions\");\n@@ -802,1 +802,1 @@\n-  if (state == NULL) {\n+  if (state == nullptr) {\n@@ -826,1 +826,1 @@\n-  set_last_explicit_null_check(NULL);\n+  set_last_explicit_null_check(nullptr);\n@@ -837,1 +837,1 @@\n-  if (state_for(block) == NULL) {\n+  if (state_for(block) == nullptr) {\n@@ -847,1 +847,1 @@\n-      assert(local0 != NULL, \"must be\");\n+      assert(local0 != nullptr, \"must be\");\n@@ -850,1 +850,1 @@\n-      if (local0 != NULL) {\n+      if (local0 != nullptr) {\n@@ -872,1 +872,1 @@\n-  assert(e != NULL, \"incomplete graph\");\n+  assert(e != nullptr, \"incomplete graph\");\n@@ -890,1 +890,1 @@\n-  for (Instruction* instr = block; instr != NULL; instr = instr->next()) {\n+  for (Instruction* instr = block; instr != nullptr; instr = instr->next()) {\n@@ -896,1 +896,1 @@\n-    if (instr->is_pinned() || instr->can_trap() || (instr->as_NullCheck() != NULL)) {\n+    if (instr->is_pinned() || instr->can_trap() || (instr->as_NullCheck() != nullptr)) {\n@@ -922,1 +922,1 @@\n-    if (x->as_LoadField() != NULL) {\n+    if (x->as_LoadField() != nullptr) {\n@@ -958,1 +958,1 @@\n-      x->set_explicit_null_check(NULL);\n+      x->set_explicit_null_check(nullptr);\n@@ -971,1 +971,1 @@\n-    x->set_explicit_null_check(NULL);\n+    x->set_explicit_null_check(nullptr);\n@@ -989,1 +989,1 @@\n-      x->set_explicit_null_check(NULL);\n+      x->set_explicit_null_check(nullptr);\n@@ -1002,1 +1002,1 @@\n-    x->set_explicit_null_check(NULL);\n+    x->set_explicit_null_check(nullptr);\n@@ -1020,1 +1020,1 @@\n-      x->set_explicit_null_check(NULL);\n+      x->set_explicit_null_check(nullptr);\n@@ -1033,1 +1033,1 @@\n-    x->set_explicit_null_check(NULL);\n+    x->set_explicit_null_check(nullptr);\n","filename":"src\/hotspot\/share\/c1\/c1_Optimizer.cpp","additions":55,"deletions":55,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,1 +55,1 @@\n-    bool can_reach(BlockBegin *start, BlockBegin *end, BlockBegin *dont_use = NULL);\n+    bool can_reach(BlockBegin *start, BlockBegin *end, BlockBegin *dont_use = nullptr);\n@@ -118,1 +118,1 @@\n-    void clear_bound() { _bound = NULL; }\n+    void clear_bound() { _bound = nullptr; }\n","filename":"src\/hotspot\/share\/c1\/c1_RangeCheckElimination.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -167,1 +167,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -227,1 +227,1 @@\n-  assert(oop_maps == NULL || sasm->frame_size() != no_frame_size,\n+  assert(oop_maps == nullptr || sasm->frame_size() != no_frame_size,\n@@ -229,1 +229,1 @@\n-  assert(!expect_oop_map || oop_maps != NULL, \"must have an oopmap\");\n+  assert(!expect_oop_map || oop_maps != nullptr, \"must have an oopmap\");\n@@ -245,1 +245,1 @@\n-  assert(blob != NULL, \"blob must exist\");\n+  assert(blob != nullptr, \"blob must exist\");\n@@ -284,1 +284,1 @@\n-      if (_blobs[id]->oop_maps() != NULL) {\n+      if (_blobs[id]->oop_maps() != nullptr) {\n@@ -568,1 +568,1 @@\n-  nmethod* osr_nm = NULL;\n+  nmethod* osr_nm = nullptr;\n@@ -577,1 +577,1 @@\n-  assert(nm!= NULL && nm->is_nmethod(), \"Sanity check\");\n+  assert(nm!= nullptr && nm->is_nmethod(), \"Sanity check\");\n@@ -612,1 +612,1 @@\n-    if (osr_nm != NULL) {\n+    if (osr_nm != nullptr) {\n@@ -621,1 +621,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -659,1 +659,1 @@\n-  assert(nm != NULL, \"this is not an nmethod\");\n+  assert(nm != nullptr, \"this is not an nmethod\");\n@@ -671,1 +671,1 @@\n-  assert(exception.not_null(), \"NULL exceptions should be handled by throw_exception\");\n+  assert(exception.not_null(), \"null exceptions should be handled by throw_exception\");\n@@ -681,1 +681,1 @@\n-    assert(nm->method() != NULL, \"Unexpected NULL method()\");\n+    assert(nm->method() != nullptr, \"Unexpected null method()\");\n@@ -723,1 +723,1 @@\n-    if (fast_continuation != NULL) {\n+    if (fast_continuation != nullptr) {\n@@ -732,2 +732,2 @@\n-  \/\/ skip the exception cache update (i.e., just leave continuation==NULL).\n-  address continuation = NULL;\n+  \/\/ skip the exception cache update (i.e., just leave continuation as null).\n+  address continuation = nullptr;\n@@ -756,1 +756,1 @@\n-    if (continuation != NULL && !recursive_exception) {\n+    if (continuation != nullptr && !recursive_exception) {\n@@ -785,2 +785,2 @@\n-  nmethod* nm = NULL;\n-  address continuation = NULL;\n+  nmethod* nm = nullptr;\n+  address continuation = nullptr;\n@@ -796,1 +796,1 @@\n-  if (nm != NULL && caller_is_deopted(current)) {\n+  if (nm != nullptr && caller_is_deopted(current)) {\n@@ -800,1 +800,1 @@\n-  assert(continuation != NULL, \"no handler found\");\n+  assert(continuation != nullptr, \"no handler found\");\n@@ -886,1 +886,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -889,2 +889,2 @@\n-  assert(obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n+  assert(LockingMode == LM_LIGHTWEIGHT || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, LockingMode == LM_LIGHTWEIGHT ? nullptr : lock->lock(), current);\n@@ -903,1 +903,1 @@\n-  assert(oopDesc::is_oop(obj), \"must be NULL or an object\");\n+  assert(oopDesc::is_oop(obj), \"must be null or an object\");\n@@ -918,1 +918,1 @@\n-  assert(nm != NULL, \"Sanity check\");\n+  assert(nm != nullptr, \"Sanity check\");\n@@ -928,1 +928,1 @@\n-        if (trap_mdo != NULL) {\n+        if (trap_mdo != nullptr) {\n@@ -1023,1 +1023,1 @@\n-\/\/ NULL, so the next thread to execute this code will fail the test,\n+\/\/ null, so the next thread to execute this code will fail the test,\n@@ -1086,4 +1086,4 @@\n-  Klass* init_klass = NULL; \/\/ klass needed by load_klass_patching code\n-  Klass* load_klass = NULL; \/\/ klass needed by load_klass_patching code\n-  Handle mirror(current, NULL);                    \/\/ oop needed by load_mirror_patching code\n-  Handle appendix(current, NULL);                  \/\/ oop needed by appendix_patching code\n+  Klass* init_klass = nullptr; \/\/ klass needed by load_klass_patching code\n+  Klass* load_klass = nullptr; \/\/ klass needed by load_klass_patching code\n+  Handle mirror(current, nullptr); \/\/ oop needed by load_mirror_patching code\n+  Handle appendix(current, nullptr); \/\/ oop needed by appendix_patching code\n@@ -1130,1 +1130,1 @@\n-    Klass* k = NULL;\n+    Klass* k = nullptr;\n@@ -1230,1 +1230,1 @@\n-    if (nm != NULL) {\n+    if (nm != nullptr) {\n@@ -1276,1 +1276,1 @@\n-          assert(caller_code != NULL, \"nmethod not found\");\n+          assert(caller_code != nullptr, \"nmethod not found\");\n@@ -1282,1 +1282,1 @@\n-          assert(map != NULL, \"null check\");\n+          assert(map != nullptr, \"null check\");\n@@ -1317,1 +1317,1 @@\n-              assert(load_klass != NULL, \"klass not set\");\n+              assert(load_klass != nullptr, \"klass not set\");\n@@ -1350,2 +1350,2 @@\n-            address addr = NULL;\n-            assert(nm != NULL, \"invalid nmethod_pc\");\n+            address addr = nullptr;\n+            assert(nm != nullptr, \"invalid nmethod_pc\");\n@@ -1367,1 +1367,1 @@\n-            assert(addr != NULL, \"metadata relocation must exist\");\n+            assert(addr != nullptr, \"metadata relocation must exist\");\n@@ -1391,1 +1391,1 @@\n-            assert(nm != NULL, \"invalid nmethod_pc\");\n+            assert(nm != nullptr, \"invalid nmethod_pc\");\n@@ -1414,1 +1414,1 @@\n-    guarantee(nm != NULL, \"only nmethods can contain non-perm oops\");\n+    guarantee(nm != nullptr, \"only nmethods can contain non-perm oops\");\n@@ -1482,1 +1482,1 @@\n-    if (nm != NULL) {\n+    if (nm != nullptr) {\n@@ -1594,1 +1594,1 @@\n-  assert(mirror != NULL, \"should null-check on mirror before calling\");\n+  assert(mirror != nullptr, \"should null-check on mirror before calling\");\n@@ -1596,1 +1596,1 @@\n-  return (k != NULL && obj != NULL && obj->is_a(k)) ? 1 : 0;\n+  return (k != nullptr && obj != nullptr && obj->is_a(k)) ? 1 : 0;\n@@ -1610,1 +1610,1 @@\n-  assert (nm != NULL, \"no more nmethod?\");\n+  assert (nm != nullptr, \"no more nmethod?\");\n@@ -1616,1 +1616,1 @@\n-  if (mdo == NULL && !HAS_PENDING_EXCEPTION) {\n+  if (mdo == nullptr && !HAS_PENDING_EXCEPTION) {\n@@ -1628,1 +1628,1 @@\n-  if (mdo != NULL) {\n+  if (mdo != nullptr) {\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":47,"deletions":47,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-  , _entries(ValueMapInitialSize, ValueMapInitialSize, NULL)\n+  , _entries(ValueMapInitialSize, ValueMapInitialSize, nullptr)\n@@ -59,1 +59,1 @@\n-  , _entries(old->_entries.length(), old->_entries.length(), NULL)\n+  , _entries(old->_entries.length(), old->_entries.length(), nullptr)\n@@ -75,1 +75,1 @@\n-  ValueMapEntryArray new_entries(new_size, new_size, NULL);\n+  ValueMapEntryArray new_entries(new_size, new_size, nullptr);\n@@ -82,1 +82,1 @@\n-    for (entry = entry_at(i); entry != NULL; entry = entry->next()) {\n+    for (entry = entry_at(i); entry != nullptr; entry = entry->next()) {\n@@ -96,1 +96,1 @@\n-        entry = new ValueMapEntry(entry->hash(), entry->value(), entry->nesting(), NULL);\n+        entry = new ValueMapEntry(entry->hash(), entry->value(), entry->nesting(), nullptr);\n@@ -115,1 +115,1 @@\n-    for (ValueMapEntry* entry = entry_at(entry_index(hash, size())); entry != NULL; entry = entry->next()) {\n+    for (ValueMapEntry* entry = entry_at(entry_index(hash, size())); entry != nullptr; entry = entry->next()) {\n@@ -123,1 +123,1 @@\n-          if (entry->nesting() != nesting() && f->as_Constant() == NULL) {\n+          if (entry->nesting() != nesting() && f->as_Constant() == nullptr) {\n@@ -155,2 +155,2 @@\n-    ValueMapEntry* prev_entry = NULL;                                                    \\\n-    for (ValueMapEntry* entry = entry_at(i); entry != NULL; entry = entry->next()) {     \\\n+    ValueMapEntry* prev_entry = nullptr;                                                 \\\n+    for (ValueMapEntry* entry = entry_at(i); entry != nullptr; entry = entry->next()) {  \\\n@@ -164,1 +164,1 @@\n-        if (prev_entry == NULL) {                                                        \\\n+        if (prev_entry == nullptr) {                                                     \\\n@@ -182,1 +182,1 @@\n-  bool must_kill = value->as_LoadField() != NULL || value->as_LoadIndexed() != NULL;\n+  bool must_kill = value->as_LoadField() != nullptr || value->as_LoadIndexed() != nullptr;\n@@ -185,1 +185,1 @@\n-  bool must_kill = value->as_LoadIndexed() != NULL                                       \\\n+  bool must_kill = value->as_LoadIndexed() != nullptr                                    \\\n@@ -191,1 +191,1 @@\n-  bool must_kill = lf != NULL                                                            \\\n+  bool must_kill = lf != nullptr                                                         \\\n@@ -216,1 +216,1 @@\n-    _entries.at_put(i, NULL);\n+    _entries.at_put(i, nullptr);\n@@ -229,1 +229,1 @@\n-    if (entry_at(i) != NULL) {\n+    if (entry_at(i) != nullptr) {\n@@ -231,1 +231,1 @@\n-      for (ValueMapEntry* entry = entry_at(i); entry != NULL; entry = entry->next()) {\n+      for (ValueMapEntry* entry = entry_at(i); entry != nullptr; entry = entry->next()) {\n@@ -236,1 +236,1 @@\n-      tty->print_cr(\"NULL\");\n+      tty->print_cr(\"null\");\n@@ -330,1 +330,1 @@\n-  : _gvn(gvn), _short_loop_optimizer(slo), _insertion_point(NULL), _state(NULL), _insert_is_pred(false) {\n+  : _gvn(gvn), _short_loop_optimizer(slo), _insertion_point(nullptr), _state(nullptr), _insert_is_pred(false) {\n@@ -340,1 +340,1 @@\n-  assert(insertion_block->end()->as_Base() == NULL, \"cannot insert into entry block\");\n+  assert(insertion_block->end()->as_Base() == nullptr, \"cannot insert into entry block\");\n@@ -368,1 +368,1 @@\n-  while (cur != NULL) {\n+  while (cur != nullptr) {\n@@ -373,1 +373,1 @@\n-    if (cur->as_Constant() != NULL) {\n+    if (cur->as_Constant() != nullptr) {\n@@ -375,2 +375,2 @@\n-    } else if (cur->as_ArithmeticOp() != NULL || cur->as_LogicOp() != NULL || cur->as_ShiftOp() != NULL) {\n-      assert(cur->as_Op2() != NULL, \"must be Op2\");\n+    } else if (cur->as_ArithmeticOp() != nullptr || cur->as_LogicOp() != nullptr || cur->as_ShiftOp() != nullptr) {\n+      assert(cur->as_Op2() != nullptr, \"must be Op2\");\n@@ -379,1 +379,1 @@\n-    } else if (cur->as_LoadField() != NULL) {\n+    } else if (cur->as_LoadField() != nullptr) {\n@@ -383,1 +383,1 @@\n-    } else if (cur->as_ArrayLength() != NULL) {\n+    } else if (cur->as_ArrayLength() != nullptr) {\n@@ -386,1 +386,1 @@\n-    } else if (cur->as_LoadIndexed() != NULL) {\n+    } else if (cur->as_LoadIndexed() != nullptr) {\n@@ -389,1 +389,1 @@\n-    } else if (cur->as_NegateOp() != NULL) {\n+    } else if (cur->as_NegateOp() != nullptr) {\n@@ -392,1 +392,1 @@\n-    } else if (cur->as_Convert() != NULL) {\n+    } else if (cur->as_Convert() != nullptr) {\n@@ -401,1 +401,1 @@\n-      if (cur->as_Constant() == NULL) {\n+      if (cur->as_Constant() == nullptr) {\n@@ -416,1 +416,1 @@\n-      cur->set_exception_handlers(NULL);\n+      cur->set_exception_handlers(nullptr);\n@@ -421,1 +421,1 @@\n-      if (cur->state_before() != NULL) {\n+      if (cur->state_before() != nullptr) {\n@@ -424,1 +424,1 @@\n-      if (cur->exception_state() != NULL) {\n+      if (cur->exception_state() != nullptr) {\n@@ -461,1 +461,1 @@\n-      if (pred_map != NULL) {\n+      if (pred_map != nullptr) {\n@@ -472,1 +472,1 @@\n-    for (Value instr = block->next(); instr != NULL; instr = instr->next()) {\n+    for (Value instr = block->next(); instr != nullptr; instr = instr->next()) {\n@@ -493,2 +493,2 @@\n-  , _current_map(NULL)\n-  , _value_maps(ir->linear_scan_order()->length(), ir->linear_scan_order()->length(), NULL)\n+  , _current_map(nullptr)\n+  , _value_maps(ir->linear_scan_order()->length(), ir->linear_scan_order()->length(), nullptr)\n@@ -505,2 +505,2 @@\n-  assert(start_block == ir->start() && start_block->number_of_preds() == 0 && start_block->dominator() == NULL, \"must be start block\");\n-  assert(start_block->next()->as_Base() != NULL && start_block->next()->next() == NULL, \"start block must not have instructions\");\n+  assert(start_block == ir->start() && start_block->number_of_preds() == 0 && start_block->dominator() == nullptr, \"must be start block\");\n+  assert(start_block->next()->as_Base() != nullptr && start_block->next()->next() == nullptr, \"start block must not have instructions\");\n@@ -510,1 +510,1 @@\n-     assert(value->as_Local() != NULL, \"only method parameters allowed\");\n+     assert(value->as_Local() != nullptr, \"only method parameters allowed\");\n@@ -525,2 +525,2 @@\n-    assert(dominator != NULL, \"dominator must exist\");\n-    assert(value_map_of(dominator) != NULL, \"value map of dominator must exist\");\n+    assert(dominator != nullptr, \"dominator must exist\");\n+    assert(value_map_of(dominator) != nullptr, \"value map of dominator must exist\");\n@@ -549,1 +549,1 @@\n-        if (pred_map != NULL) {\n+        if (pred_map != nullptr) {\n@@ -568,1 +568,1 @@\n-    for (Value instr = block->next(); instr != NULL; instr = instr->next()) {\n+    for (Value instr = block->next(); instr != nullptr; instr = instr->next()) {\n","filename":"src\/hotspot\/share\/c1\/c1_ValueMap.cpp","additions":43,"deletions":43,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -252,1 +252,1 @@\n-  void          set_value_map_of(BlockBegin* block, ValueMap* map)   { assert(value_map_of(block) == NULL, \"\"); _value_maps.at_put(block->linear_scan_number(), map); }\n+  void          set_value_map_of(BlockBegin* block, ValueMap* map) { assert(value_map_of(block) == nullptr, \"\"); _value_maps.at_put(block->linear_scan_number(), map); }\n","filename":"src\/hotspot\/share\/c1\/c1_ValueMap.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-, _locals(scope->method()->max_locals(), scope->method()->max_locals(), NULL)\n+, _locals(scope->method()->max_locals(), scope->method()->max_locals(), nullptr)\n@@ -41,1 +41,1 @@\n-, _locks(NULL)\n+, _locks(nullptr)\n@@ -54,1 +54,1 @@\n-  , _locks(copy_from->locks_size() == 0 ? NULL : new Values(copy_from->locks_size()))\n+  , _locks(copy_from->locks_size() == 0 ? nullptr : new Values(copy_from->locks_size()))\n@@ -108,1 +108,1 @@\n-    if (value != NULL && value != s->lock_at(i)) {\n+    if (value != nullptr && value != s->lock_at(i)) {\n@@ -117,1 +117,1 @@\n-    _locals.at_put(i, NULL);\n+    _locals.at_put(i, nullptr);\n@@ -124,1 +124,1 @@\n-    if (v->as_Constant() == NULL && v->as_Local() == NULL) {\n+    if (v->as_Constant() == nullptr && v->as_Local() == nullptr) {\n@@ -136,1 +136,1 @@\n-    if (v0 != NULL && !v0->type()->is_illegal()) {\n+    if (v0 != nullptr && !v0->type()->is_illegal()) {\n@@ -141,1 +141,1 @@\n-      assert(!v1->type()->is_double_word() || list.at(i + 1) == NULL, \"hi-word of doubleword value must be NULL\");\n+      assert(!v1->type()->is_double_word() || list.at(i + 1) == nullptr, \"hi-word of doubleword value must be null\");\n@@ -154,1 +154,1 @@\n-    if (state->_locks != NULL) {\n+    if (state->_locks != nullptr) {\n@@ -181,1 +181,1 @@\n-  if (_locks == NULL) {\n+  if (_locks == nullptr) {\n@@ -197,1 +197,1 @@\n-  assert(stack_at(index)->as_Phi() == NULL || stack_at(index)->as_Phi()->block() != b, \"phi function already created\");\n+  assert(stack_at(index)->as_Phi() == nullptr || stack_at(index)->as_Phi()->block() != b, \"phi function already created\");\n@@ -204,1 +204,1 @@\n-  assert(!t->is_double_word() || _stack.at(index + 1) == NULL, \"hi-word of doubleword value must be NULL\");\n+  assert(!t->is_double_word() || _stack.at(index + 1) == nullptr, \"hi-word of doubleword value must be null\");\n@@ -208,1 +208,1 @@\n-  assert(local_at(index)->as_Phi() == NULL || local_at(index)->as_Phi()->block() != b, \"phi function already created\");\n+  assert(local_at(index)->as_Phi() == nullptr || local_at(index)->as_Phi()->block() != b, \"phi function already created\");\n@@ -237,1 +237,1 @@\n-      if (t == NULL) {\n+      if (t == nullptr) {\n@@ -251,1 +251,1 @@\n-      if (l == NULL) {\n+      if (l == nullptr) {\n@@ -263,1 +263,1 @@\n-  if (caller_state() != NULL) {\n+  if (caller_state() != nullptr) {\n@@ -270,2 +270,2 @@\n-  assert(scope() != NULL, \"scope must exist\");\n-  if (caller_state() != NULL) {\n+  assert(scope() != nullptr, \"scope must exist\");\n+  if (caller_state() != nullptr) {\n@@ -288,2 +288,2 @@\n-    if (v == NULL) {\n-      assert(_stack.at(i - 1)->type()->is_double_word(), \"only hi-words are NULL on stack\");\n+    if (v == nullptr) {\n+      assert(_stack.at(i - 1)->type()->is_double_word(), \"only hi-words are null on stack\");\n@@ -291,1 +291,1 @@\n-      assert(_stack.at(i + 1) == NULL, \"hi-word must be NULL\");\n+      assert(_stack.at(i + 1) == nullptr, \"hi-word must be null\");\n@@ -297,2 +297,2 @@\n-    if (v != NULL && v->type()->is_double_word()) {\n-      assert(_locals.at(i + 1) == NULL, \"hi-word must be NULL\");\n+    if (v != nullptr && v->type()->is_double_word()) {\n+      assert(_locals.at(i + 1) == nullptr, \"hi-word must be null\");\n@@ -303,1 +303,1 @@\n-    assert(v != NULL, \"just test if state-iteration succeeds\");\n+    assert(v != nullptr, \"just test if state-iteration succeeds\");\n","filename":"src\/hotspot\/share\/c1\/c1_ValueStack.cpp","additions":25,"deletions":25,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,1 +59,1 @@\n-    assert(h == NULL, \"hi-word of doubleword value must be NULL\");\n+    assert(h == nullptr, \"hi-word of doubleword value must be null\");\n@@ -98,1 +98,1 @@\n-  int locks_size() const                         { return _locks == NULL ? 0 : _locks->length(); }\n+  int locks_size() const                         { return _locks == nullptr ? 0 : _locks->length(); }\n@@ -100,1 +100,1 @@\n-  bool no_active_locks() const                   { return _locks == NULL || _locks->is_empty(); }\n+  bool no_active_locks() const                   { return _locks == nullptr || _locks->is_empty(); }\n@@ -104,1 +104,1 @@\n-  void clear_locals();                           \/\/ sets all locals to NULL;\n+  void clear_locals();                           \/\/ sets all locals to null;\n@@ -108,2 +108,2 @@\n-           _locals.at(i + 1) == NULL, \"hi-word of doubleword value must be NULL\");\n-    _locals.at_put(i, NULL);\n+           _locals.at(i + 1) == nullptr, \"hi-word of doubleword value must be null\");\n+    _locals.at_put(i, nullptr);\n@@ -114,2 +114,2 @@\n-    assert(x == NULL || !x->type()->is_double_word() ||\n-           _locals.at(i + 1) == NULL, \"hi-word of doubleword value must be NULL\");\n+    assert(x == nullptr || !x->type()->is_double_word() ||\n+           _locals.at(i + 1) == nullptr, \"hi-word of doubleword value must be null\");\n@@ -124,2 +124,2 @@\n-      if (prev != NULL && prev->type()->is_double_word()) {\n-        _locals.at_put(i - 1, NULL);\n+      if (prev != nullptr && prev->type()->is_double_word()) {\n+        _locals.at_put(i - 1, nullptr);\n@@ -131,2 +131,2 @@\n-      \/\/ hi-word of doubleword value is always NULL\n-      _locals.at_put(i + 1, NULL);\n+      \/\/ hi-word of doubleword value is always null\n+      _locals.at_put(i + 1, nullptr);\n@@ -140,1 +140,1 @@\n-           _stack.at(i + 1) == NULL, \"hi-word of doubleword value must be NULL\");\n+           _stack.at(i + 1) == nullptr, \"hi-word of doubleword value must be null\");\n@@ -170,2 +170,2 @@\n-  void lpush(Value t)                            { _stack.push(check(longTag   , t)); _stack.push(NULL); }\n-  void dpush(Value t)                            { _stack.push(check(doubleTag , t)); _stack.push(NULL); }\n+  void lpush(Value t)                            { _stack.push(check(longTag   , t)); _stack.push(nullptr); }\n+  void dpush(Value t)                            { _stack.push(check(doubleTag , t)); _stack.push(nullptr); }\n@@ -200,1 +200,1 @@\n-      default        : ShouldNotReachHere(); return NULL;\n+      default        : ShouldNotReachHere(); return nullptr;\n@@ -243,1 +243,1 @@\n-\/\/ as an invariant, state is NULL now\n+\/\/ as an invariant, state is null now\n@@ -252,1 +252,1 @@\n-  for (; state != NULL; state = state->caller_state())\n+  for (; state != nullptr; state = state->caller_state())\n@@ -258,2 +258,2 @@\n-       index += (value == NULL || value->type()->is_illegal() ? 1 : value->type()->size()))    \\\n-    if (value != NULL)\n+       index += (value == nullptr || value->type()->is_illegal() ? 1 : value->type()->size())) \\\n+    if (value != nullptr)\n@@ -274,1 +274,1 @@\n-    if (value != NULL)\n+    if (value != nullptr)\n@@ -324,1 +324,1 @@\n-      if (v_phi != NULL && v_phi->block() == v_block) {                                        \\\n+      if (v_phi != nullptr && v_phi->block() == v_block) {                                     \\\n@@ -332,1 +332,1 @@\n-      if (v_phi != NULL && v_phi->block() == v_block) {                                        \\\n+      if (v_phi != nullptr && v_phi->block() == v_block) {                                     \\\n","filename":"src\/hotspot\/share\/c1\/c1_ValueStack.hpp","additions":24,"deletions":24,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,11 +34,11 @@\n-VoidType*       voidType     = NULL;\n-IntType*        intType      = NULL;\n-LongType*       longType     = NULL;\n-FloatType*      floatType    = NULL;\n-DoubleType*     doubleType   = NULL;\n-ObjectType*     objectType   = NULL;\n-ArrayType*      arrayType    = NULL;\n-InstanceType*   instanceType = NULL;\n-ClassType*      classType    = NULL;\n-AddressType*    addressType  = NULL;\n-IllegalType*    illegalType  = NULL;\n+VoidType*       voidType     = nullptr;\n+IntType*        intType      = nullptr;\n+LongType*       longType     = nullptr;\n+FloatType*      floatType    = nullptr;\n+DoubleType*     doubleType   = nullptr;\n+ObjectType*     objectType   = nullptr;\n+ArrayType*      arrayType    = nullptr;\n+InstanceType*   instanceType = nullptr;\n+ClassType*      classType    = nullptr;\n+AddressType*    addressType  = nullptr;\n+IllegalType*    illegalType  = nullptr;\n@@ -48,3 +48,3 @@\n-IntConstant*    intZero      = NULL;\n-IntConstant*    intOne       = NULL;\n-ObjectConstant* objectNull   = NULL;\n+IntConstant*    intZero      = nullptr;\n+IntConstant*    intOne       = nullptr;\n+ObjectConstant* objectNull   = nullptr;\n@@ -92,1 +92,1 @@\n-  return (c != NULL && !c->is_null_object()) ? c->klass() : NULL;\n+  return (c != nullptr && !c->is_null_object()) ? c->klass() : nullptr;\n@@ -96,1 +96,1 @@\n-  return (c != NULL && !c->is_null_object()) ? c->klass() : NULL;\n+  return (c != nullptr && !c->is_null_object()) ? c->klass() : nullptr;\n@@ -100,1 +100,1 @@\n-  return (c != NULL && !c->is_null_object()) ? c->klass() : NULL;\n+  return (c != nullptr && !c->is_null_object()) ? c->klass() : nullptr;\n","filename":"src\/hotspot\/share\/c1\/c1_ValueType.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -237,1 +237,1 @@\n-  soc->do_ptr((void**)&_index);\n+  soc->do_ptr(&_index);\n@@ -262,0 +262,1 @@\n+  case MetaspaceObj::SharedClassPathEntryType:\n@@ -277,1 +278,1 @@\n-            \" a new subtype of Klass or MetaData without updating CPP_VTABLE_TYPES_DO\",\n+            \" a new subtype of Klass or MetaData without updating CPP_VTABLE_TYPES_DO or the cases in this 'switch' statement\",\n","filename":"src\/hotspot\/share\/cds\/cppVtables.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -361,1 +361,0 @@\n-  st->print_cr(\"- shared_path_table_size:         %d\", _shared_path_table_size);\n@@ -431,0 +430,1 @@\n+  assert(ent != NULL, \"sanity\");\n@@ -522,1 +522,0 @@\n-\n@@ -529,4 +528,1 @@\n-  it->push(&_table);\n-  for (int i=0; i<_size; i++) {\n-    path_at(i)->metaspace_pointers_do(it);\n-  }\n+  it->push(&_entries);\n@@ -536,41 +532,10 @@\n-  size_t entry_size = sizeof(SharedClassPathEntry);\n-  int num_entries = 0;\n-  num_entries += ClassLoader::num_boot_classpath_entries();\n-  num_entries += ClassLoader::num_app_classpath_entries();\n-  num_entries += ClassLoader::num_module_path_entries();\n-  num_entries += FileMapInfo::num_non_existent_class_paths();\n-  size_t bytes = entry_size * num_entries;\n-\n-  _table = MetadataFactory::new_array<u8>(loader_data, (int)bytes, CHECK);\n-  _size = num_entries;\n-}\n-\n-\/\/ Make a copy of the _shared_path_table for use during dynamic CDS dump.\n-\/\/ It is needed because some Java code continues to execute after dynamic dump has finished.\n-\/\/ However, during dynamic dump, we have modified FileMapInfo::_shared_path_table so\n-\/\/ FileMapInfo::shared_path(i) returns incorrect information in ClassLoader::record_result().\n-void FileMapInfo::copy_shared_path_table(ClassLoaderData* loader_data, TRAPS) {\n-  size_t entry_size = sizeof(SharedClassPathEntry);\n-  size_t bytes = entry_size * _shared_path_table.size();\n-\n-  Array<u8>* array = MetadataFactory::new_array<u8>(loader_data, (int)bytes, CHECK);\n-  _saved_shared_path_table = SharedPathTable(array, _shared_path_table.size());\n-\n-  for (int i = 0; i < _shared_path_table.size(); i++) {\n-    _saved_shared_path_table.path_at(i)->copy_from(shared_path(i), loader_data, CHECK);\n-  }\n-  _saved_shared_path_table_array = array;\n-}\n-\n-void FileMapInfo::clone_shared_path_table(TRAPS) {\n-  Arguments::assert_is_dumping_archive();\n-\n-  ClassLoaderData* loader_data = ClassLoaderData::the_null_class_loader_data();\n-  ClassPathEntry* jrt = ClassLoader::get_jrt_entry();\n-\n-  assert(jrt != nullptr,\n-         \"No modular java runtime image present when allocating the CDS classpath entry table\");\n-\n-  if (_saved_shared_path_table_array != nullptr) {\n-    MetadataFactory::free_array<u8>(loader_data, _saved_shared_path_table_array);\n-    _saved_shared_path_table_array = nullptr;\n+  const int num_entries =\n+    ClassLoader::num_boot_classpath_entries() +\n+    ClassLoader::num_app_classpath_entries() +\n+    ClassLoader::num_module_path_entries() +\n+    FileMapInfo::num_non_existent_class_paths();\n+  _entries = MetadataFactory::new_array<SharedClassPathEntry*>(loader_data, num_entries, CHECK);\n+  for (int i = 0; i < num_entries; i++) {\n+    SharedClassPathEntry* ent =\n+      new (loader_data, SharedClassPathEntry::size(), MetaspaceObj::SharedClassPathEntryType, THREAD) SharedClassPathEntry;\n+    _entries->at_put(i, ent);\n@@ -578,2 +543,0 @@\n-\n-  copy_shared_path_table(loader_data, CHECK);\n@@ -605,1 +568,0 @@\n-  clone_shared_path_table(CHECK);\n@@ -1758,2 +1720,1 @@\n-  ssize_t n = os::write(_fd, buffer, (unsigned int)nbytes);\n-  if (n < 0 || (size_t)n != nbytes) {\n+  if (!os::write(_fd, buffer, nbytes)) {\n@@ -2401,8 +2362,0 @@\n-void FileMapInfo::metaspace_pointers_do(MetaspaceClosure* it, bool use_copy) {\n-  if (use_copy) {\n-    _saved_shared_path_table.metaspace_pointers_do(it);\n-  } else {\n-    _shared_path_table.metaspace_pointers_do(it);\n-  }\n-}\n-\n@@ -2413,2 +2366,0 @@\n-SharedPathTable FileMapInfo::_saved_shared_path_table;\n-Array<u8>*      FileMapInfo::_saved_shared_path_table_array = nullptr;\n@@ -2467,0 +2418,7 @@\n+void FileMapInfo::print(outputStream* st) const {\n+  header()->print(st);\n+  if (!is_static()) {\n+    dynamic_header()->print(st);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":20,"deletions":62,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/allocation.hpp\"\n@@ -52,1 +53,1 @@\n-class SharedClassPathEntry {\n+class SharedClassPathEntry : public MetaspaceObj {\n@@ -73,0 +74,7 @@\n+  SharedClassPathEntry() : _type(0), _is_module_path(false),\n+                           _from_class_path_attr(false), _timestamp(0),\n+                           _filesize(0), _name(nullptr), _manifest(nullptr) {}\n+  static int size() {\n+    static_assert(is_aligned(sizeof(SharedClassPathEntry), wordSize), \"must be\");\n+    return (int)(sizeof(SharedClassPathEntry) \/ wordSize);\n+  }\n@@ -76,0 +84,1 @@\n+  MetaspaceObj::Type type() const { return SharedClassPathEntryType; }\n@@ -110,2 +119,1 @@\n-  Array<u8>* _table;\n-  int _size;\n+  Array<SharedClassPathEntry*>* _entries;\n@@ -113,2 +121,2 @@\n-  SharedPathTable() : _table(nullptr), _size(0) {}\n-  SharedPathTable(Array<u8>* table, int size) : _table(table), _size(size) {}\n+  SharedPathTable() : _entries(nullptr) {}\n+  SharedPathTable(Array<SharedClassPathEntry*>* entries) : _entries(entries) {}\n@@ -120,1 +128,1 @@\n-    return _size;\n+    return _entries == nullptr ? 0 : _entries->length();\n@@ -123,10 +131,4 @@\n-    if (index < 0) {\n-      return nullptr;\n-    }\n-    assert(index < _size, \"sanity\");\n-    char* p = (char*)_table->data();\n-    p += sizeof(SharedClassPathEntry) * index;\n-    return (SharedClassPathEntry*)p;\n-  }\n-  Array<u8>* table() {return _table;}\n-  void set_table(Array<u8>* table) {_table = table;}\n+    return _entries->at(index);\n+  }\n+  Array<SharedClassPathEntry*>* table() {return _entries;}\n+  void set_table(Array<SharedClassPathEntry*>* table) {_entries = table;}\n@@ -247,1 +249,0 @@\n-  int    _shared_path_table_size;\n@@ -323,1 +324,0 @@\n-    _shared_path_table_size = table.size();\n@@ -332,2 +332,2 @@\n-    return SharedPathTable((Array<u8>*)from_mapped_offset(_shared_path_table_offset),\n-                           _shared_path_table_size);\n+    return SharedPathTable((Array<SharedClassPathEntry*>*)\n+                           from_mapped_offset(_shared_path_table_offset));\n@@ -375,3 +375,0 @@\n-  \/\/ TODO: Probably change the following to be non-static\n-  static SharedPathTable       _saved_shared_path_table;\n-  static Array<u8>*            _saved_shared_path_table_array;  \/\/ remember the table array for cleanup\n@@ -398,4 +395,0 @@\n-  static SharedPathTable saved_shared_path_table() {\n-    assert(_saved_shared_path_table.size() >= 0, \"Sanity check\");\n-    return _saved_shared_path_table;\n-  }\n@@ -404,1 +397,3 @@\n-  static void metaspace_pointers_do(MetaspaceClosure* it, bool use_copy = true);\n+  static void metaspace_pointers_do(MetaspaceClosure* it) {\n+    _shared_path_table.metaspace_pointers_do(it);\n+  }\n@@ -514,2 +509,0 @@\n-  static void copy_shared_path_table(ClassLoaderData* loader_data, TRAPS);\n-  static void clone_shared_path_table(TRAPS);\n@@ -529,0 +522,1 @@\n+  \/\/ Caller needs a ResourceMark because parts of the returned cfs are resource-allocated.\n@@ -567,3 +561,1 @@\n-  void print(outputStream* st) {\n-    header()->print(st);\n-  }\n+  void print(outputStream* st) const;\n","filename":"src\/hotspot\/share\/cds\/filemap.hpp","additions":25,"deletions":33,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -456,2 +456,2 @@\n-  virtual void iterate_roots(MetaspaceClosure* it, bool is_relocating_pointers) {\n-    FileMapInfo::metaspace_pointers_do(it, false);\n+  virtual void iterate_roots(MetaspaceClosure* it) {\n+    FileMapInfo::metaspace_pointers_do(it);\n@@ -512,3 +512,0 @@\n-  \/\/ Dump supported java heap objects\n-\n-  builder.relocate_roots();\n@@ -518,2 +515,0 @@\n-  builder.relocate_vm_classes();\n-\n@@ -593,2 +588,1 @@\n-    \/\/ tolerate this. (Note that unregistered classes are loaded by the null\n-    \/\/ loader during DumpSharedSpaces).\n+    \/\/ tolerate this.\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -828,1 +828,1 @@\n-                                        int index) {\n+                                        int index, Bytecodes::Code bc) {\n@@ -831,1 +831,1 @@\n-    ciField* field = new (arena()) ciField(accessor, index);\n+    ciField* field = new (arena()) ciField(accessor, index, bc);\n@@ -836,1 +836,1 @@\n-      field = new (arena()) ciField(accessor, index);\n+      field = new (arena()) ciField(accessor, index, bc);\n@@ -848,2 +848,2 @@\n-                                   int index) {\n-  GUARDED_VM_ENTRY(return get_field_by_index_impl(accessor, index);)\n+                                   int index, Bytecodes::Code bc) {\n+  GUARDED_VM_ENTRY(return get_field_by_index_impl(accessor, index, bc);)\n@@ -916,1 +916,1 @@\n-    ciSymbol*        signature = get_symbol(cpool->signature_ref_at(index));\n+    ciSymbol*        signature = get_symbol(cpool->signature_ref_at(index, bc));\n@@ -919,1 +919,1 @@\n-    const int holder_index = cpool->klass_ref_index_at(index);\n+    const int holder_index = cpool->klass_ref_index_at(index, bc);\n@@ -924,2 +924,2 @@\n-    Symbol* name_sym = cpool->name_ref_at(index);\n-    Symbol* sig_sym  = cpool->signature_ref_at(index);\n+    Symbol* name_sym = cpool->name_ref_at(index, bc);\n+    Symbol* sig_sym  = cpool->signature_ref_at(index, bc);\n@@ -951,1 +951,1 @@\n-      constantTag tag = cpool->tag_ref_at(index);\n+      constantTag tag = cpool->tag_ref_at(index, bc);\n@@ -1567,1 +1567,1 @@\n-  const int holder_index = cp->klass_ref_index_at(index);\n+  const int holder_index = cp->klass_ref_index_at(index, Bytecodes::_invokehandle);\n@@ -1572,1 +1572,1 @@\n-  Symbol* name = cp->name_ref_at(index);\n+  Symbol* name = cp->name_ref_at(index, Bytecodes::_invokehandle);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -134,1 +134,1 @@\n-                                int field_index);\n+                                int field_index, Bytecodes::Code bc);\n@@ -154,1 +154,1 @@\n-                                     int field_index);\n+                                     int field_index, Bytecodes::Code bc);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-ciField::ciField(ciInstanceKlass* klass, int index) :\n+ciField::ciField(ciInstanceKlass* klass, int index, Bytecodes::Code bc) :\n@@ -85,1 +85,1 @@\n-  Symbol* name  = cpool->name_ref_at(index);\n+  Symbol* name  = cpool->name_ref_at(index, bc);\n@@ -88,1 +88,1 @@\n-  int nt_index = cpool->name_and_type_ref_index_at(index);\n+  int nt_index = cpool->name_and_type_ref_index_at(index, bc);\n@@ -115,1 +115,1 @@\n-  int holder_index = cpool->klass_ref_index_at(index);\n+  int holder_index = cpool->klass_ref_index_at(index, bc);\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -62,1 +62,1 @@\n-  ciField(ciInstanceKlass* klass, int index);\n+  ciField(ciInstanceKlass* klass, int index, Bytecodes::Code bc);\n","filename":"src\/hotspot\/share\/ci\/ciField.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -88,2 +88,2 @@\n-  _uses_monitors      = h_m->access_flags().has_monitor_bytecodes();\n-  _balanced_monitors  = !_uses_monitors || h_m->access_flags().is_monitor_matching();\n+  _uses_monitors      = h_m->has_monitor_bytecodes();\n+  _balanced_monitors  = !_uses_monitors || h_m->guaranteed_monitor_matching();\n@@ -1228,1 +1228,1 @@\n-bool ciMethod::is_klass_loaded(int refinfo_index, bool must_be_resolved) const {\n+bool ciMethod::is_klass_loaded(int refinfo_index, Bytecodes::Code bc, bool must_be_resolved) const {\n@@ -1230,1 +1230,1 @@\n-  return get_Method()->is_klass_loaded(refinfo_index, must_be_resolved);\n+  return get_Method()->is_klass_loaded(refinfo_index, bc, must_be_resolved);\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -318,1 +318,1 @@\n-  bool is_klass_loaded(int refinfo_index, bool must_be_resolved) const;\n+  bool is_klass_loaded(int refinfo_index, Bytecodes::Code bc, bool must_be_resolved) const;\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -432,2 +432,2 @@\n-        Klass* holder = cp->klass_ref_at(index, CHECK_NULL);\n-        Symbol* name = cp->name_ref_at(index);\n+        Klass* holder = cp->klass_ref_at(index, bytecode.code(), CHECK_NULL);\n+        Symbol* name = cp->name_ref_at(index, bytecode.code());\n@@ -1158,0 +1158,1 @@\n+          parse_klass(CHECK_(true)); \/\/ eat up the array class name\n@@ -1162,0 +1163,1 @@\n+          parse_klass(CHECK_(true)); \/\/ eat up the array class name\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -323,1 +323,1 @@\n-  ciField* f = CURRENT_ENV->get_field_by_index(_holder, get_field_index());\n+  ciField* f = CURRENT_ENV->get_field_by_index(_holder, get_field_index(), _bc);\n@@ -358,1 +358,1 @@\n-    return cpool->klass_ref_index_at(get_field_index());\n+    return cpool->klass_ref_index_at(get_field_index(), _bc);\n@@ -542,1 +542,1 @@\n-  return cpool->klass_ref_index_at(get_method_index());\n+  return cpool->klass_ref_index_at(get_method_index(), _bc);\n@@ -554,1 +554,1 @@\n-    const int name_and_type_index = cpool->name_and_type_ref_index_at(method_index);\n+    const int name_and_type_index = cpool->name_and_type_ref_index_at(method_index, _bc);\n","filename":"src\/hotspot\/share\/ci\/ciStreams.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -461,2 +461,2 @@\n-        const int klass_ref_index = cp->klass_ref_index_at(index);\n-        const int name_and_type_ref_index = cp->name_and_type_ref_index_at(index);\n+        const int klass_ref_index = cp->uncached_klass_ref_index_at(index);\n+        const int name_and_type_ref_index = cp->uncached_name_and_type_ref_index_at(index);\n@@ -671,1 +671,1 @@\n-          cp->name_and_type_ref_index_at(index);\n+          cp->uncached_name_and_type_ref_index_at(index);\n@@ -694,1 +694,1 @@\n-          cp->name_and_type_ref_index_at(index);\n+          cp->uncached_name_and_type_ref_index_at(index);\n@@ -752,1 +752,1 @@\n-              cp->name_and_type_ref_index_at(ref_index);\n+              cp->uncached_name_and_type_ref_index_at(ref_index);\n@@ -2084,1 +2084,1 @@\n-    m->set_caller_sensitive(true);\n+    m->set_caller_sensitive();\n@@ -2086,1 +2086,1 @@\n-    m->set_force_inline(true);\n+    m->set_force_inline();\n@@ -2088,1 +2088,1 @@\n-    m->set_dont_inline(true);\n+    m->set_dont_inline();\n@@ -2090,1 +2090,1 @@\n-    m->set_changes_current_thread(true);\n+    m->set_changes_current_thread();\n@@ -2092,1 +2092,1 @@\n-    m->set_jvmti_mount_transition(true);\n+    m->set_jvmti_mount_transition();\n@@ -2094,1 +2094,1 @@\n-    m->set_has_injected_profile(true);\n+    m->set_has_injected_profile();\n@@ -2098,1 +2098,1 @@\n-    m->set_hidden(true);\n+    m->set_is_hidden();\n@@ -2100,1 +2100,1 @@\n-    m->set_scoped(true);\n+    m->set_scoped();\n@@ -2102,1 +2102,1 @@\n-    m->set_intrinsic_candidate(true);\n+    m->set_intrinsic_candidate();\n@@ -2104,1 +2104,1 @@\n-    m->set_has_reserved_stack_access(true);\n+    m->set_has_reserved_stack_access();\n@@ -2901,1 +2901,1 @@\n-    m->set_hidden(true);\n+    m->set_is_hidden();\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":16,"deletions":16,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -163,1 +163,1 @@\n-  void set_next(ClassLoaderData* next) { _next = next; }\n+  void set_next(ClassLoaderData* next) { Atomic::store(&_next, next); }\n@@ -307,2 +307,2 @@\n-  static ByteSize holder_offset()     { return in_ByteSize(offset_of(ClassLoaderData, _holder)); }\n-  static ByteSize keep_alive_offset() { return in_ByteSize(offset_of(ClassLoaderData, _keep_alive)); }\n+  static ByteSize holder_offset()     { return byte_offset_of(ClassLoaderData, _holder); }\n+  static ByteSize keep_alive_offset() { return byte_offset_of(ClassLoaderData, _keep_alive); }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1439,1 +1439,9 @@\n-  assert(Universe::java_mirror(type) == java_class, \"must be consistent\");\n+#ifdef ASSERT\n+  if (DumpSharedSpaces) {\n+    oop mirror = Universe::java_mirror(type);\n+    oop scratch_mirror = HeapShared::scratch_java_mirror(type);\n+    assert(java_class == mirror || java_class == scratch_mirror, \"must be consistent\");\n+  } else {\n+    assert(Universe::java_mirror(type) == java_class, \"must be consistent\");\n+  }\n+#endif\n@@ -1647,0 +1655,4 @@\n+JavaThread* java_lang_Thread::thread_acquire(oop java_thread) {\n+  return reinterpret_cast<JavaThread*>(java_thread->address_field_acquire(_eetop_offset));\n+}\n+\n@@ -1651,0 +1663,4 @@\n+void java_lang_Thread::release_set_thread(oop java_thread, JavaThread* thread) {\n+  java_thread->release_address_field_put(_eetop_offset, (address)thread);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":17,"deletions":1,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -382,0 +382,1 @@\n+  static JavaThread* thread_acquire(oop java_thread);\n@@ -384,0 +385,1 @@\n+  static void release_set_thread(oop java_thread, JavaThread* thread);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -1185,1 +1186,2 @@\n-  assert(!ik->is_unshareable_info_restored(), \"shared class can be loaded only once\");\n+  assert(!ik->is_unshareable_info_restored(), \"shared class can be restored only once\");\n+  assert(Atomic::add(&ik->_shared_class_load_count, 1) == 1, \"shared class loaded more than once\");\n@@ -1663,1 +1665,3 @@\n-    f(method);\n+    if (method != nullptr) {\n+      f(method);\n+    }\n@@ -1667,1 +1671,1 @@\n-    MutexLocker ml(InvokeMethodTable_lock);\n+    MutexLocker ml(InvokeMethodIntrinsicTable_lock);\n@@ -2023,0 +2027,7 @@\n+  InvokeMethodKey key(signature, iid_as_int);\n+  Method** met = nullptr;\n+\n+  \/\/ We only want one entry in the table for this (signature\/id, method) pair but the code\n+  \/\/ to create the intrinsic method needs to be outside the lock.\n+  \/\/ The first thread claims the entry by adding the key and the other threads wait, until the\n+  \/\/ Method has been added as the value.\n@@ -2024,5 +2035,17 @@\n-    MutexLocker ml(THREAD, InvokeMethodTable_lock);\n-    InvokeMethodKey key(signature, iid_as_int);\n-    Method** met = _invoke_method_intrinsic_table.get(key);\n-    if (met != nullptr) {\n-      return *met;\n+    MonitorLocker ml(THREAD, InvokeMethodIntrinsicTable_lock);\n+    while (true) {\n+      bool created;\n+      met = _invoke_method_intrinsic_table.put_if_absent(key, &created);\n+      assert(met != nullptr, \"either created or found\");\n+      if (*met != nullptr) {\n+        return *met;\n+      } else if (created) {\n+        \/\/ The current thread won the race and will try to create the full entry.\n+        break;\n+      } else {\n+        \/\/ Another thread beat us to it, so wait for them to complete\n+        \/\/ and return *met; or if they hit an error we get another try.\n+        ml.wait();\n+        \/\/ Note it is not safe to read *met here as that entry could have\n+        \/\/ been deleted, so we must loop and try put_if_absent again.\n+      }\n@@ -2030,0 +2053,1 @@\n+  }\n@@ -2031,11 +2055,9 @@\n-    bool throw_error = false;\n-    \/\/ This function could get an OOM but it is safe to call inside of a lock because\n-    \/\/ throwing OutOfMemoryError doesn't call Java code.\n-    methodHandle m = Method::make_method_handle_intrinsic(iid, signature, CHECK_NULL);\n-    if (!Arguments::is_interpreter_only() || iid == vmIntrinsics::_linkToNative) {\n-        \/\/ Generate a compiled form of the MH intrinsic\n-        \/\/ linkToNative doesn't have interpreter-specific implementation, so always has to go through compiled version.\n-        AdapterHandlerLibrary::create_native_wrapper(m);\n-        \/\/ Check if have the compiled code.\n-        throw_error = (!m->has_compiled_code());\n-    }\n+  methodHandle m = Method::make_method_handle_intrinsic(iid, signature, THREAD);\n+  bool throw_error = HAS_PENDING_EXCEPTION;\n+  if (!throw_error && (!Arguments::is_interpreter_only() || iid == vmIntrinsics::_linkToNative)) {\n+    \/\/ Generate a compiled form of the MH intrinsic\n+    \/\/ linkToNative doesn't have interpreter-specific implementation, so always has to go through compiled version.\n+    AdapterHandlerLibrary::create_native_wrapper(m);\n+    \/\/ Check if have the compiled code.\n+    throw_error = (!m->has_compiled_code());\n+  }\n@@ -2043,1 +2065,8 @@\n-    if (!throw_error) {\n+  {\n+    MonitorLocker ml(THREAD, InvokeMethodIntrinsicTable_lock);\n+    if (throw_error) {\n+      \/\/ Remove the entry and let another thread try, or get the same exception.\n+      bool removed = _invoke_method_intrinsic_table.remove(key);\n+      assert(removed, \"must be the owner\");\n+      ml.notify_all();\n+    } else {\n@@ -2045,2 +2074,0 @@\n-      bool created = _invoke_method_intrinsic_table.put(key, m());\n-      assert(created, \"must be since we still hold the lock\");\n@@ -2050,0 +2077,2 @@\n+      *met = m(); \/\/ insert the element\n+      ml.notify_all();\n@@ -2054,3 +2083,6 @@\n-  \/\/ Throw error outside of the lock.\n-  THROW_MSG_NULL(vmSymbols::java_lang_VirtualMachineError(),\n-                 \"Out of space in CodeCache for method handle intrinsic\");\n+  \/\/ Throw VirtualMachineError or the pending exception in the JavaThread\n+  if (throw_error && !HAS_PENDING_EXCEPTION) {\n+    THROW_MSG_NULL(vmSymbols::java_lang_VirtualMachineError(),\n+                   \"Out of space in CodeCache for method handle intrinsic\");\n+  }\n+  return nullptr;\n@@ -2199,1 +2231,1 @@\n-    MutexLocker ml(THREAD, InvokeMethodTable_lock);\n+    MutexLocker ml(THREAD, InvokeMethodTypeTable_lock);\n@@ -2268,1 +2300,1 @@\n-    MutexLocker ml(THREAD, InvokeMethodTable_lock);\n+    MutexLocker ml(THREAD, InvokeMethodTypeTable_lock);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":59,"deletions":27,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -2360,2 +2360,2 @@\n-  Symbol* field_name = cp->name_ref_at(index);\n-  Symbol* field_sig = cp->signature_ref_at(index);\n+  Symbol* field_name = cp->uncached_name_ref_at(index);\n+  Symbol* field_sig = cp->uncached_signature_ref_at(index);\n@@ -2463,1 +2463,1 @@\n-        cp->klass_name_at(cp->klass_ref_index_at(index));\n+        cp->klass_name_at(cp->uncached_klass_ref_index_at(index));\n@@ -2787,1 +2787,1 @@\n-        cp->signature_ref_at(bcs->get_index_u2()),\n+        cp->uncached_signature_ref_at(bcs->get_index_u2()),\n@@ -2866,2 +2866,2 @@\n-  Symbol* method_name = cp->name_ref_at(index);\n-  Symbol* method_sig = cp->signature_ref_at(index);\n+  Symbol* method_name = cp->uncached_name_ref_at(index);\n+  Symbol* method_sig = cp->uncached_signature_ref_at(index);\n@@ -2890,1 +2890,1 @@\n-  int sig_index = cp->signature_ref_index_at(cp->name_and_type_ref_index_at(index));\n+  int sig_index = cp->signature_ref_index_at(cp->uncached_name_and_type_ref_index_at(index));\n@@ -3000,1 +3000,1 @@\n-            cp->klass_name_at(cp->klass_ref_index_at(index));\n+            cp->klass_name_at(cp->uncached_klass_ref_index_at(index));\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -304,1 +304,1 @@\n-    return cp_index_to_type(cp->klass_ref_index_at(index), cp, THREAD);\n+    return cp_index_to_type(cp->uncached_klass_ref_index_at(index), cp, THREAD);\n","filename":"src\/hotspot\/share\/classfile\/verifier.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -593,3 +593,5 @@\n-  do_intrinsic(_notifyJvmtiMount, java_lang_VirtualThread, notifyJvmtiMount_name, bool_bool_void_signature, F_RN)       \\\n-  do_intrinsic(_notifyJvmtiUnmount, java_lang_VirtualThread, notifyJvmtiUnmount_name, bool_bool_void_signature, F_RN)   \\\n-  do_intrinsic(_notifyJvmtiHideFrames, java_lang_VirtualThread, notifyJvmtiHideFrames_name, bool_void_signature, F_RN)  \\\n+  do_intrinsic(_notifyJvmtiVThreadStart, java_lang_VirtualThread, notifyJvmtiStart_name, void_method_signature, F_RN)   \\\n+  do_intrinsic(_notifyJvmtiVThreadEnd, java_lang_VirtualThread, notifyJvmtiEnd_name, void_method_signature, F_RN)       \\\n+  do_intrinsic(_notifyJvmtiVThreadMount, java_lang_VirtualThread, notifyJvmtiMount_name, bool_void_signature, F_RN)     \\\n+  do_intrinsic(_notifyJvmtiVThreadUnmount, java_lang_VirtualThread, notifyJvmtiUnmount_name, bool_void_signature, F_RN) \\\n+  do_intrinsic(_notifyJvmtiVThreadHideFrames, java_lang_VirtualThread, notifyJvmtiHideFrames_name, bool_void_signature, F_RN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -417,0 +417,2 @@\n+  template(notifyJvmtiStart_name,                     \"notifyJvmtiStart\")                         \\\n+  template(notifyJvmtiEnd_name,                       \"notifyJvmtiEnd\")                           \\\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -553,3 +553,1 @@\n-  int nmethod_mirror_index,\n-  const char* nmethod_mirror_name,\n-  FailedSpeculation** failed_speculations\n+  JVMCINMethodData* jvmci_data\n@@ -564,1 +562,1 @@\n-  int jvmci_data_size = !compiler->is_jvmci() ? 0 : JVMCINMethodData::compute_size(nmethod_mirror_name);\n+  int jvmci_data_size = compiler->is_jvmci() ? jvmci_data->size() : 0;\n@@ -591,1 +589,1 @@\n-            jvmci_data_size\n+            jvmci_data\n@@ -596,6 +594,0 @@\n-#if INCLUDE_JVMCI\n-      if (compiler->is_jvmci()) {\n-        \/\/ Initialize the JVMCINMethodData object inlined into nm\n-        nm->jvmci_nmethod_data()->initialize(nmethod_mirror_index, nmethod_mirror_name, failed_speculations);\n-      }\n-#endif\n@@ -795,1 +787,1 @@\n-  int jvmci_data_size\n+  JVMCINMethodData* jvmci_data\n@@ -875,0 +867,1 @@\n+    int jvmci_data_size      = compiler->is_jvmci() ? jvmci_data->size() : 0;\n@@ -897,0 +890,7 @@\n+#if INCLUDE_JVMCI\n+    if (compiler->is_jvmci()) {\n+      \/\/ Initialize the JVMCINMethodData object inlined into nm\n+      jvmci_nmethod_data()->copy(jvmci_data);\n+    }\n+#endif\n+\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -299,3 +299,3 @@\n-          , char* speculations,\n-          int speculations_len,\n-          int jvmci_data_size\n+          , char* speculations = nullptr,\n+          int speculations_len = 0,\n+          JVMCINMethodData* jvmci_data = nullptr\n@@ -352,3 +352,1 @@\n-                              int nmethod_mirror_index = -1,\n-                              const char* nmethod_mirror_name = nullptr,\n-                              FailedSpeculation** failed_speculations = nullptr\n+                              JVMCINMethodData* jvmci_data = nullptr\n@@ -711,3 +709,3 @@\n-  static int verified_entry_point_offset()        { return offset_of(nmethod, _verified_entry_point); }\n-  static int osr_entry_point_offset()             { return offset_of(nmethod, _osr_entry_point); }\n-  static int state_offset()                       { return offset_of(nmethod, _state); }\n+  static ByteSize verified_entry_point_offset() { return byte_offset_of(nmethod, _verified_entry_point); }\n+  static ByteSize osr_entry_point_offset()      { return byte_offset_of(nmethod, _osr_entry_point); }\n+  static ByteSize state_offset()                { return byte_offset_of(nmethod, _state); }\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":7,"deletions":9,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1014,1 +1014,1 @@\n-  julong available_memory = os::available_memory();\n+  julong free_memory = os::free_memory();\n@@ -1026,1 +1026,1 @@\n-        (int)(available_memory \/ (200*M)),\n+        (int)(free_memory \/ (200*M)),\n@@ -1073,2 +1073,2 @@\n-        msg.print(\"Added compiler thread %s (available memory: %dMB, available non-profiled code cache: %dMB)\",\n-                  ct->name(), (int)(available_memory\/M), (int)(available_cc_np\/M));\n+        msg.print(\"Added compiler thread %s (free memory: %dMB, available non-profiled code cache: %dMB)\",\n+                  ct->name(), (int)(free_memory\/M), (int)(available_cc_np\/M));\n@@ -1084,1 +1084,1 @@\n-        (int)(available_memory \/ (100*M)),\n+        (int)(free_memory \/ (100*M)),\n@@ -1096,2 +1096,2 @@\n-        msg.print(\"Added compiler thread %s (available memory: %dMB, available profiled code cache: %dMB)\",\n-                  ct->name(), (int)(available_memory\/M), (int)(available_cc_p\/M));\n+        msg.print(\"Added compiler thread %s (free memory: %dMB, available profiled code cache: %dMB)\",\n+                  ct->name(), (int)(free_memory\/M), (int)(available_cc_p\/M));\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,2 +76,2 @@\n- * If the previous value is NULL there is no need to save the old value.\n- * References that are NULL are filtered during runtime by the barrier\n+ * If the previous value is null there is no need to save the old value.\n+ * References that are null are filtered during runtime by the barrier\n@@ -81,1 +81,1 @@\n- * prove that the reference about to be overwritten is NULL during compile\n+ * prove that the reference about to be overwritten is null during compile\n@@ -103,1 +103,1 @@\n-  if (alloc == NULL) {\n+  if (alloc == nullptr) {\n@@ -119,1 +119,1 @@\n-      if (st_base == NULL) {\n+      if (st_base == nullptr) {\n@@ -159,1 +159,1 @@\n-        \/\/ Check that the initialization is storing NULL so that no previous store\n+        \/\/ Check that the initialization is storing null so that no previous store\n@@ -164,1 +164,1 @@\n-        if (captured_store == NULL || captured_store == st_init->zero_memory()) {\n+        if (captured_store == nullptr || captured_store == st_init->zero_memory()) {\n@@ -194,4 +194,4 @@\n-    assert(obj != NULL, \"must have a base\");\n-    assert(adr != NULL, \"where are loading from?\");\n-    assert(pre_val == NULL, \"loaded already?\");\n-    assert(val_type != NULL, \"need a type\");\n+    assert(obj != nullptr, \"must have a base\");\n+    assert(adr != nullptr, \"where are loading from?\");\n+    assert(pre_val == nullptr, \"loaded already?\");\n+    assert(val_type != nullptr, \"need a type\");\n@@ -206,1 +206,1 @@\n-    assert(pre_val != NULL, \"must be loaded already\");\n+    assert(pre_val != nullptr, \"must be loaded already\");\n@@ -251,1 +251,1 @@\n-    \/\/ if (pre_val != NULL)\n+    \/\/ if (pre_val != nullptr)\n@@ -273,1 +273,1 @@\n-    } __ end_if();  \/\/ (pre_val != NULL)\n+    } __ end_if();  \/\/ (pre_val != nullptr)\n@@ -291,1 +291,1 @@\n- * the same region as the reference, when the NULL is being written or\n+ * the same region as the reference, when the null is being written or\n@@ -316,1 +316,1 @@\n-  if (alloc == NULL) {\n+  if (alloc == nullptr) {\n@@ -380,1 +380,1 @@\n-  \/\/ If we are writing a NULL then we need no post barrier\n+  \/\/ If we are writing a null then we need no post barrier\n@@ -382,2 +382,2 @@\n-  if (val != NULL && val->is_Con() && val->bottom_type() == TypePtr::NULL_PTR) {\n-    \/\/ Must be NULL\n+  if (val != nullptr && val->is_Con() && val->bottom_type() == TypePtr::NULL_PTR) {\n+    \/\/ Must be null\n@@ -385,2 +385,2 @@\n-    assert(t == Type::TOP || t == TypePtr::NULL_PTR, \"must be NULL\");\n-    \/\/ No post barrier if writing NULLx\n+    assert(t == Type::TOP || t == TypePtr::NULL_PTR, \"must be null\");\n+    \/\/ No post barrier if writing null\n@@ -409,1 +409,1 @@\n-  assert(adr != NULL, \"\");\n+  assert(adr != nullptr, \"\");\n@@ -451,1 +451,1 @@\n-  if (val != NULL) {\n+  if (val != nullptr) {\n@@ -462,1 +462,1 @@\n-      \/\/ No barrier if we are storing a NULL\n+      \/\/ No barrier if we are storing a null.\n@@ -512,1 +512,1 @@\n-  if (otype != NULL && otype->is_con() &&\n+  if (otype != nullptr && otype->is_con() &&\n@@ -520,1 +520,1 @@\n-  if (btype != NULL) {\n+  if (btype != nullptr) {\n@@ -527,1 +527,1 @@\n-    if (itype != NULL) {\n+    if (itype != nullptr) {\n@@ -566,1 +566,1 @@\n-      \/\/ is_instof == 0 if base_oop == NULL\n+      \/\/ is_instof == 0 if base_oop == nullptr\n@@ -575,1 +575,1 @@\n-                    NULL \/* obj *\/, NULL \/* adr *\/, max_juint \/* alias_idx *\/, NULL \/* val *\/, NULL \/* val_type *\/,\n+                    nullptr \/* obj *\/, nullptr \/* adr *\/, max_juint \/* alias_idx *\/, nullptr \/* val *\/, nullptr \/* val_type *\/,\n@@ -650,1 +650,1 @@\n-                NULL \/* obj *\/, NULL \/* adr *\/, max_juint \/* alias_idx *\/, NULL \/* val *\/, NULL \/* val_type *\/,\n+                nullptr \/* obj *\/, nullptr \/* adr *\/, max_juint \/* alias_idx *\/, nullptr \/* val *\/, nullptr \/* val_type *\/,\n@@ -672,1 +672,1 @@\n-  if (call->_name == NULL) {\n+  if (call->_name == nullptr) {\n@@ -723,1 +723,1 @@\n-    \/\/ An other case of only one user (Xor) is when the value check for NULL\n+    \/\/ An other case of only one user (Xor) is when the value check for null\n@@ -730,1 +730,1 @@\n-    assert(this_region != NULL, \"\");\n+    assert(this_region != nullptr, \"\");\n@@ -738,1 +738,1 @@\n-    if (xorx != NULL) {\n+    if (xorx != nullptr) {\n@@ -749,1 +749,1 @@\n-      \/\/ There is no G1 pre barrier if previous stored value is NULL\n+      \/\/ There is no G1 pre barrier if previous stored value is null\n@@ -778,1 +778,1 @@\n-      assert(shift != NULL, \"missing G1 post barrier\");\n+      assert(shift != nullptr, \"missing G1 post barrier\");\n@@ -781,1 +781,1 @@\n-      assert(load != NULL, \"missing G1 post barrier\");\n+      assert(load != nullptr, \"missing G1 post barrier\");\n@@ -798,1 +798,1 @@\n-      c != NULL && c->is_Region() && c->req() == 3) {\n+      c != nullptr && c->is_Region() && c->req() == 3) {\n@@ -800,1 +800,1 @@\n-      if (c->in(i) != NULL && c->in(i)->is_Region() &&\n+      if (c->in(i) != nullptr && c->in(i)->is_Region() &&\n@@ -804,2 +804,2 @@\n-          if (r->in(j) != NULL && r->in(j)->is_Proj() &&\n-              r->in(j)->in(0) != NULL &&\n+          if (r->in(j) != nullptr && r->in(j)->is_Proj() &&\n+              r->in(j)->in(0) != nullptr &&\n@@ -810,1 +810,1 @@\n-            if (c != NULL && c->Opcode() != Op_Parm) {\n+            if (c != nullptr && c->Opcode() != Op_Parm) {\n@@ -812,1 +812,1 @@\n-              if (c != NULL) {\n+              if (c != nullptr) {\n@@ -814,5 +814,5 @@\n-                assert(call->in(0) == NULL ||\n-                       call->in(0)->in(0) == NULL ||\n-                       call->in(0)->in(0)->in(0) == NULL ||\n-                       call->in(0)->in(0)->in(0)->in(0) == NULL ||\n-                       call->in(0)->in(0)->in(0)->in(0)->in(0) == NULL ||\n+                assert(call->in(0) == nullptr ||\n+                       call->in(0)->in(0) == nullptr ||\n+                       call->in(0)->in(0)->in(0) == nullptr ||\n+                       call->in(0)->in(0)->in(0)->in(0) == nullptr ||\n+                       call->in(0)->in(0)->in(0)->in(0)->in(0) == nullptr ||\n@@ -865,1 +865,1 @@\n-  if (pre_val_if != NULL) {\n+  if (pre_val_if != nullptr) {\n@@ -892,1 +892,1 @@\n-        assert(x->in(0) != NULL, \"Pre-val load has to have a control\");\n+        assert(x->in(0) != nullptr, \"Pre-val load has to have a control\");\n@@ -936,1 +936,1 @@\n-    if (x == NULL || x == compile->top()) continue;\n+    if (x == nullptr || x == compile->top()) continue;\n@@ -974,1 +974,1 @@\n-    if (x == NULL || x == compile->top()) continue;\n+    if (x == nullptr || x == compile->top()) continue;\n@@ -1014,1 +1014,1 @@\n-              assert(load_ctrl != NULL && if_ctrl == load_ctrl, \"controls must match\");\n+              assert(load_ctrl != nullptr && if_ctrl == load_ctrl, \"controls must match\");\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":54,"deletions":54,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -62,2 +62,1 @@\n-                                           size_t young_cset_length,\n-                                           size_t optional_cset_length,\n+                                           G1CollectionSet* collection_set,\n@@ -69,2 +68,2 @@\n-    _closures(NULL),\n-    _plab_allocator(NULL),\n+    _closures(nullptr),\n+    _plab_allocator(nullptr),\n@@ -79,3 +78,3 @@\n-    _surviving_young_words_base(NULL),\n-    _surviving_young_words(NULL),\n-    _surviving_words_length(young_cset_length + 1),\n+    _surviving_young_words_base(nullptr),\n+    _surviving_young_words(nullptr),\n+    _surviving_words_length(collection_set->young_region_length() + 1),\n@@ -86,1 +85,1 @@\n-    _max_num_optional_regions(optional_cset_length),\n+    _max_num_optional_regions(collection_set->optional_region_length()),\n@@ -88,1 +87,1 @@\n-    _obj_alloc_stat(NULL),\n+    _obj_alloc_stat(nullptr),\n@@ -107,1 +106,3 @@\n-  _closures = G1EvacuationRootClosures::create_root_closures(this, _g1h);\n+  _closures = G1EvacuationRootClosures::create_root_closures(_g1h,\n+                                                             this,\n+                                                             collection_set->only_contains_young_regions());\n@@ -151,1 +152,1 @@\n-  assert(task != NULL, \"invariant\");\n+  assert(task != nullptr, \"invariant\");\n@@ -159,1 +160,1 @@\n-  assert(task != NULL, \"invariant\");\n+  assert(task != nullptr, \"invariant\");\n@@ -187,1 +188,1 @@\n-  \/\/ Reference should not be NULL here as such are never pushed to the task queue.\n+  \/\/ Reference should not be null here as such are never pushed to the task queue.\n@@ -349,1 +350,1 @@\n-    if (obj_ptr != NULL) {\n+    if (obj_ptr != nullptr) {\n@@ -361,1 +362,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -399,1 +400,1 @@\n-  HeapWord* obj_ptr = NULL;\n+  HeapWord* obj_ptr = nullptr;\n@@ -407,1 +408,1 @@\n-    if (obj_ptr == NULL) {\n+    if (obj_ptr == nullptr) {\n@@ -414,1 +415,1 @@\n-  if (obj_ptr != NULL) {\n+  if (obj_ptr != nullptr) {\n@@ -466,2 +467,2 @@\n-  \/\/ normally check against NULL once and that's it.\n-  if (obj_ptr == NULL) {\n+  \/\/ normally check against null once and that's it.\n+  if (obj_ptr == nullptr) {\n@@ -469,1 +470,1 @@\n-    if (obj_ptr == NULL) {\n+    if (obj_ptr == nullptr) {\n@@ -476,1 +477,1 @@\n-  assert(obj_ptr != NULL, \"when we get here, allocation should have succeeded\");\n+  assert(obj_ptr != nullptr, \"when we get here, allocation should have succeeded\");\n@@ -497,1 +498,1 @@\n-  if (forward_ptr == NULL) {\n+  if (forward_ptr == nullptr) {\n@@ -567,1 +568,1 @@\n-  if (_states[worker_id] == NULL) {\n+  if (_states[worker_id] == nullptr) {\n@@ -573,2 +574,1 @@\n-                               _young_cset_length,\n-                               _optional_cset_length,\n+                               _collection_set,\n@@ -625,1 +625,1 @@\n-  if (forward_ptr == NULL) {\n+  if (forward_ptr == nullptr) {\n@@ -680,1 +680,1 @@\n-  if (_obj_alloc_stat != NULL) {\n+  if (_obj_alloc_stat != nullptr) {\n@@ -687,1 +687,1 @@\n-  if (_obj_alloc_stat != NULL) {\n+  if (_obj_alloc_stat != nullptr) {\n@@ -694,2 +694,1 @@\n-                                                 size_t young_cset_length,\n-                                                 size_t optional_cset_length,\n+                                                 G1CollectionSet* collection_set,\n@@ -698,0 +697,1 @@\n+    _collection_set(collection_set),\n@@ -701,3 +701,1 @@\n-    _surviving_young_words_total(NEW_C_HEAP_ARRAY(size_t, young_cset_length + 1, mtGC)),\n-    _young_cset_length(young_cset_length),\n-    _optional_cset_length(optional_cset_length),\n+    _surviving_young_words_total(NEW_C_HEAP_ARRAY(size_t, collection_set->young_region_length() + 1, mtGC)),\n@@ -709,1 +707,1 @@\n-    _states[i] = NULL;\n+    _states[i] = nullptr;\n@@ -711,1 +709,1 @@\n-  memset(_surviving_young_words_total, 0, (young_cset_length + 1) * sizeof(size_t));\n+  memset(_surviving_young_words_total, 0, (collection_set->young_region_length() + 1) * sizeof(size_t));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":35,"deletions":37,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -1753,1 +1753,1 @@\n-    TraceMemoryManagerStats tms(heap->old_gc_manager(), gc_cause);\n+    TraceMemoryManagerStats tms(heap->old_gc_manager(), gc_cause, \"end of major GC\");\n@@ -2071,1 +2071,4 @@\n-  _gc_tracer.report_object_count_after_gc(is_alive_closure());\n+  {\n+    GCTraceTime(Debug, gc, phases) tm(\"Report Object Count\", &_gc_timer);\n+    _gc_tracer.report_object_count_after_gc(is_alive_closure(), &ParallelScavengeHeap::heap()->workers());\n+  }\n@@ -2239,1 +2242,1 @@\n-    uint claimed = Atomic::fetch_and_add(&_counter, 1u);\n+    uint claimed = Atomic::fetch_then_add(&_counter, 1u);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  friend class DisableIsGCActiveMark; \/\/ Disable current IsGCActiveMark\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,111 @@\n+\/*\n+ * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBARRIERSET_HPP\n+#define SHARE_GC_X_XBARRIERSET_HPP\n+\n+#include \"gc\/shared\/barrierSet.hpp\"\n+\n+class XBarrierSetAssembler;\n+\n+class XBarrierSet : public BarrierSet {\n+public:\n+  XBarrierSet();\n+\n+  static XBarrierSetAssembler* assembler();\n+  static bool barrier_needed(DecoratorSet decorators, BasicType type);\n+\n+  virtual void on_thread_create(Thread* thread);\n+  virtual void on_thread_destroy(Thread* thread);\n+  virtual void on_thread_attach(Thread* thread);\n+  virtual void on_thread_detach(Thread* thread);\n+\n+  virtual void print_on(outputStream* st) const;\n+\n+  template <DecoratorSet decorators, typename BarrierSetT = XBarrierSet>\n+  class AccessBarrier : public BarrierSet::AccessBarrier<decorators, BarrierSetT> {\n+  private:\n+    typedef BarrierSet::AccessBarrier<decorators, BarrierSetT> Raw;\n+\n+    template <DecoratorSet expected>\n+    static void verify_decorators_present();\n+\n+    template <DecoratorSet expected>\n+    static void verify_decorators_absent();\n+\n+    static oop* field_addr(oop base, ptrdiff_t offset);\n+\n+    template <typename T>\n+    static oop load_barrier_on_oop_field_preloaded(T* addr, oop o);\n+\n+    template <typename T>\n+    static oop load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o);\n+\n+  public:\n+    \/\/\n+    \/\/ In heap\n+    \/\/\n+    template <typename T>\n+    static oop oop_load_in_heap(T* addr);\n+    static oop oop_load_in_heap_at(oop base, ptrdiff_t offset);\n+\n+    template <typename T>\n+    static oop oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value);\n+    static oop oop_atomic_cmpxchg_in_heap_at(oop base, ptrdiff_t offset, oop compare_value, oop new_value);\n+\n+    template <typename T>\n+    static oop oop_atomic_xchg_in_heap(T* addr, oop new_value);\n+    static oop oop_atomic_xchg_in_heap_at(oop base, ptrdiff_t offset, oop new_value);\n+\n+    template <typename T>\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n+                                      size_t length);\n+\n+    static void clone_in_heap(oop src, oop dst, size_t size);\n+\n+    static void value_copy_in_heap(void* src, void* dst, InlineKlass* md);\n+\n+    \/\/\n+    \/\/ Not in heap\n+    \/\/\n+    template <typename T>\n+    static oop oop_load_not_in_heap(T* addr);\n+\n+    template <typename T>\n+    static oop oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value);\n+\n+    template <typename T>\n+    static oop oop_atomic_xchg_not_in_heap(T* addr, oop new_value);\n+  };\n+};\n+\n+template<> struct BarrierSet::GetName<XBarrierSet> {\n+  static const BarrierSet::Name value = BarrierSet::XBarrierSet;\n+};\n+\n+template<> struct BarrierSet::GetType<BarrierSet::XBarrierSet> {\n+  typedef ::XBarrierSet type;\n+};\n+\n+#endif \/\/ SHARE_GC_X_XBARRIERSET_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSet.hpp","additions":111,"deletions":0,"binary":false,"changes":111,"status":"added"},{"patch":"@@ -0,0 +1,266 @@\n+\/*\n+ * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_X_XBARRIERSET_INLINE_HPP\n+#define SHARE_GC_X_XBARRIERSET_INLINE_HPP\n+\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+\n+#include \"gc\/shared\/accessBarrierSupport.inline.hpp\"\n+#include \"gc\/x\/xBarrier.inline.hpp\"\n+#include \"oops\/inlineKlass.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <DecoratorSet expected>\n+inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::verify_decorators_present() {\n+  if ((decorators & expected) == 0) {\n+    fatal(\"Using unsupported access decorators\");\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <DecoratorSet expected>\n+inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::verify_decorators_absent() {\n+  if ((decorators & expected) != 0) {\n+    fatal(\"Using unsupported access decorators\");\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop* XBarrierSet::AccessBarrier<decorators, BarrierSetT>::field_addr(oop base, ptrdiff_t offset) {\n+  assert(base != nullptr, \"Invalid base\");\n+  return reinterpret_cast<oop*>(reinterpret_cast<intptr_t>((void*)base) + offset);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_oop_field_preloaded(T* addr, oop o) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  if (HasDecorator<decorators, AS_NO_KEEPALIVE>::value) {\n+    if (HasDecorator<decorators, ON_STRONG_OOP_REF>::value) {\n+      return XBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n+    } else if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value) {\n+      return XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n+    } else {\n+      assert((HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value), \"Must be\");\n+      return XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+    }\n+  } else {\n+    if (HasDecorator<decorators, ON_STRONG_OOP_REF>::value) {\n+      return XBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n+    } else if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value) {\n+      return XBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n+    } else {\n+      assert((HasDecorator<decorators, ON_PHANTOM_OOP_REF>::value), \"Must be\");\n+      return XBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+    }\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o) {\n+  verify_decorators_present<ON_UNKNOWN_OOP_REF>();\n+\n+  const DecoratorSet decorators_known_strength =\n+    AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n+\n+  if (HasDecorator<decorators, AS_NO_KEEPALIVE>::value) {\n+    if (decorators_known_strength & ON_STRONG_OOP_REF) {\n+      return XBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n+    } else if (decorators_known_strength & ON_WEAK_OOP_REF) {\n+      return XBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n+    } else {\n+      assert(decorators_known_strength & ON_PHANTOM_OOP_REF, \"Must be\");\n+      return XBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+    }\n+  } else {\n+    if (decorators_known_strength & ON_STRONG_OOP_REF) {\n+      return XBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n+    } else if (decorators_known_strength & ON_WEAK_OOP_REF) {\n+      return XBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n+    } else {\n+      assert(decorators_known_strength & ON_PHANTOM_OOP_REF, \"Must be\");\n+      return XBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+    }\n+  }\n+}\n+\n+\/\/\n+\/\/ In heap\n+\/\/\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap(T* addr) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  const oop o = Raw::oop_load_in_heap(addr);\n+  return load_barrier_on_oop_field_preloaded(addr, o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap_at(oop base, ptrdiff_t offset) {\n+  oop* const addr = field_addr(base, offset);\n+  const oop o = Raw::oop_load_in_heap(addr);\n+\n+  if (HasDecorator<decorators, ON_UNKNOWN_OOP_REF>::value) {\n+    return load_barrier_on_unknown_oop_field_preloaded(base, offset, addr, o);\n+  }\n+\n+  return load_barrier_on_oop_field_preloaded(addr, o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  XBarrier::load_barrier_on_oop_field(addr);\n+  return Raw::oop_atomic_cmpxchg_in_heap(addr, compare_value, new_value);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap_at(oop base, ptrdiff_t offset, oop compare_value, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF | ON_UNKNOWN_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  \/\/ Through Unsafe.CompareAndExchangeObject()\/CompareAndSetObject() we can receive\n+  \/\/ calls with ON_UNKNOWN_OOP_REF set. However, we treat these as ON_STRONG_OOP_REF,\n+  \/\/ with the motivation that if you're doing Unsafe operations on a Reference.referent\n+  \/\/ field, then you're on your own anyway.\n+  XBarrier::load_barrier_on_oop_field(field_addr(base, offset));\n+  return Raw::oop_atomic_cmpxchg_in_heap_at(base, offset, compare_value, new_value);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap(T* addr, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  const oop o = Raw::oop_atomic_xchg_in_heap(addr, new_value);\n+  return XBarrier::load_barrier_on_oop(o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap_at(oop base, ptrdiff_t offset, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  const oop o = Raw::oop_atomic_xchg_in_heap_at(base, offset, new_value);\n+  return XBarrier::load_barrier_on_oop(o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n+                                                                                       arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n+                                                                                       size_t length) {\n+  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+\n+  if ((!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) &&\n+      (!HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value)) {\n+    \/\/ No check cast, bulk barrier and bulk copy\n+    XBarrier::load_barrier_on_oop_array(src, length);\n+    Raw::oop_arraycopy_in_heap(nullptr, 0, src, nullptr, 0, dst, length);\n+    return;\n+  }\n+\n+  \/\/ Check cast and copy each elements\n+  Klass* const dst_klass = objArrayOop(dst_obj)->element_klass();\n+  for (const T* const end = src + length; src < end; src++, dst++) {\n+    const oop elem = XBarrier::load_barrier_on_oop_field(src);\n+    if (HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value && elem == nullptr) {\n+      throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n+      return;\n+    }\n+    if (HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value &&\n+        (!oopDesc::is_instanceof_or_null(elem, dst_klass))) {\n+      \/\/ Check cast failed\n+      throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n+      return;\n+    }\n+\n+    \/\/ Cast is safe, since we know it's never a narrowOop\n+    *(oop*)dst = elem;\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::clone_in_heap(oop src, oop dst, size_t size) {\n+  XBarrier::load_barrier_on_oop_fields(src);\n+  Raw::clone_in_heap(src, dst, size);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void XBarrierSet::AccessBarrier<decorators, BarrierSetT>::value_copy_in_heap(void* src, void* dst, InlineKlass* md) {\n+  if (md->contains_oops()) {\n+    \/\/ src\/dst aren't oops, need offset to adjust oop map offset\n+    const address src_oop_addr_offset = ((address) src) - md->first_field_offset();\n+\n+    OopMapBlock* map = md->start_of_nonstatic_oop_maps();\n+    OopMapBlock* const end = map + md->nonstatic_oop_map_count();\n+    while (map != end) {\n+      address soop_address = src_oop_addr_offset + map->offset();\n+      XBarrier::load_barrier_on_oop_array((oop*) soop_address, map->count());\n+      map++;\n+    }\n+  }\n+  Raw::value_copy_in_heap(src, dst, md);\n+}\n+\n+\/\/\n+\/\/ Not in heap\n+\/\/\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(T* addr) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  const oop o = Raw::oop_load_not_in_heap(addr);\n+  return load_barrier_on_oop_field_preloaded(addr, o);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  return Raw::oop_atomic_cmpxchg_not_in_heap(addr, compare_value, new_value);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+template <typename T>\n+inline oop XBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_not_in_heap(T* addr, oop new_value) {\n+  verify_decorators_present<ON_STRONG_OOP_REF>();\n+  verify_decorators_absent<AS_NO_KEEPALIVE>();\n+\n+  return Raw::oop_atomic_xchg_not_in_heap(addr, new_value);\n+}\n+\n+#endif \/\/ SHARE_GC_X_XBARRIERSET_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/x\/xBarrierSet.inline.hpp","additions":266,"deletions":0,"binary":false,"changes":266,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"gc\/x\/xObjArrayAllocator.hpp\"\n+#include \"gc\/x\/xUtils.inline.hpp\"\n+#include \"oops\/arrayKlass.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+XObjArrayAllocator::XObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread) :\n+    ObjArrayAllocator(klass, word_size, length, do_zero, thread) {}\n+\n+void XObjArrayAllocator::yield_for_safepoint() const {\n+  ThreadBlockInVM tbivm(JavaThread::cast(_thread));\n+}\n+\n+oop XObjArrayAllocator::initialize(HeapWord* mem) const {\n+  \/\/ ZGC specializes the initialization by performing segmented clearing\n+  \/\/ to allow shorter time-to-safepoints.\n+\n+  if (!_do_zero) {\n+    \/\/ No need for ZGC specialization\n+    return ObjArrayAllocator::initialize(mem);\n+  }\n+\n+  \/\/ A max segment size of 64K was chosen because microbenchmarking\n+  \/\/ suggested that it offered a good trade-off between allocation\n+  \/\/ time and time-to-safepoint\n+  const size_t segment_max = XUtils::bytes_to_words(64 * K);\n+  const BasicType element_type = ArrayKlass::cast(_klass)->element_type();\n+  const size_t header = arrayOopDesc::header_size(element_type);\n+  const size_t payload_size = _word_size - header;\n+\n+  if (payload_size <= segment_max) {\n+    \/\/ To small to use segmented clearing\n+    return ObjArrayAllocator::initialize(mem);\n+  }\n+\n+  \/\/ Segmented clearing\n+\n+  \/\/ The array is going to be exposed before it has been completely\n+  \/\/ cleared, therefore we can't expose the header at the end of this\n+  \/\/ function. Instead explicitly initialize it according to our needs.\n+  arrayOopDesc::set_mark(mem, Klass::default_prototype_header(_klass));\n+  arrayOopDesc::release_set_klass(mem, _klass);\n+  assert(_length >= 0, \"length should be non-negative\");\n+  arrayOopDesc::set_length(mem, _length);\n+\n+  \/\/ Keep the array alive across safepoints through an invisible\n+  \/\/ root. Invisible roots are not visited by the heap itarator\n+  \/\/ and the marking logic will not attempt to follow its elements.\n+  \/\/ Relocation knows how to dodge iterating over such objects.\n+  XThreadLocalData::set_invisible_root(_thread, (oop*)&mem);\n+\n+  for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n+    \/\/ Calculate segment\n+    HeapWord* const start = (HeapWord*)(mem + header + processed);\n+    const size_t remaining = payload_size - processed;\n+    const size_t segment_size = MIN2(remaining, segment_max);\n+\n+    \/\/ Clear segment\n+    Copy::zero_to_words(start, segment_size);\n+\n+    \/\/ Safepoint\n+    yield_for_safepoint();\n+  }\n+\n+  XThreadLocalData::clear_invisible_root(_thread);\n+\n+  return cast_to_oop(mem);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"asm\/macroAssembler.hpp\"\n@@ -47,0 +48,74 @@\n+template<typename K, typename V, size_t _table_size>\n+class ZArenaHashtable : public ResourceObj {\n+  class ZArenaHashtableEntry : public ResourceObj {\n+  public:\n+    ZArenaHashtableEntry* _next;\n+    K _key;\n+    V _value;\n+  };\n+\n+  static const size_t _table_mask = _table_size - 1;\n+\n+  Arena* _arena;\n+  ZArenaHashtableEntry* _table[_table_size];\n+\n+public:\n+  class Iterator {\n+    ZArenaHashtable* _table;\n+    ZArenaHashtableEntry* _current_entry;\n+    size_t _current_index;\n+\n+  public:\n+    Iterator(ZArenaHashtable* table)\n+      : _table(table),\n+        _current_entry(table->_table[0]),\n+        _current_index(0) {\n+      if (_current_entry == nullptr) {\n+        next();\n+      }\n+    }\n+\n+    bool has_next() { return _current_entry != nullptr; }\n+    K key()         { return _current_entry->_key; }\n+    V value()       { return _current_entry->_value; }\n+\n+    void next() {\n+      if (_current_entry != nullptr) {\n+        _current_entry = _current_entry->_next;\n+      }\n+      while (_current_entry == nullptr && ++_current_index < _table_size) {\n+        _current_entry = _table->_table[_current_index];\n+      }\n+    }\n+  };\n+\n+  ZArenaHashtable(Arena* arena)\n+    : _arena(arena),\n+      _table() {\n+    Copy::zero_to_bytes(&_table, sizeof(_table));\n+  }\n+\n+  void add(K key, V value) {\n+    ZArenaHashtableEntry* entry = new (_arena) ZArenaHashtableEntry();\n+    entry->_key = key;\n+    entry->_value = value;\n+    entry->_next = _table[key & _table_mask];\n+    _table[key & _table_mask] = entry;\n+  }\n+\n+  V* get(K key) const {\n+    for (ZArenaHashtableEntry* e = _table[key & _table_mask]; e != nullptr; e = e->_next) {\n+      if (e->_key == key) {\n+        return &(e->_value);\n+      }\n+    }\n+    return nullptr;\n+  }\n+\n+  Iterator iterator() {\n+    return Iterator(this);\n+  }\n+};\n+\n+typedef ZArenaHashtable<intptr_t, bool, 4> ZOffsetTable;\n+\n@@ -49,2 +124,4 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* _stubs;\n-  Node_Array                          _live;\n+  GrowableArray<ZBarrierStubC2*>* _stubs;\n+  Node_Array                      _live;\n+  int                             _trampoline_stubs_count;\n+  int                             _stubs_start_offset;\n@@ -53,3 +130,5 @@\n-  ZBarrierSetC2State(Arena* arena) :\n-    _stubs(new (arena) GrowableArray<ZLoadBarrierStubC2*>(arena, 8,  0, NULL)),\n-    _live(arena) {}\n+  ZBarrierSetC2State(Arena* arena)\n+    : _stubs(new (arena) GrowableArray<ZBarrierStubC2*>(arena, 8,  0, nullptr)),\n+      _live(arena),\n+      _trampoline_stubs_count(0),\n+      _stubs_start_offset(0) {}\n@@ -57,1 +136,1 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* stubs() {\n+  GrowableArray<ZBarrierStubC2*>* stubs() {\n@@ -64,1 +143,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -68,1 +147,1 @@\n-    if (mach->barrier_data() == ZLoadBarrierElided) {\n+    if (mach->barrier_data() == ZBarrierElided) {\n@@ -70,1 +149,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -74,1 +153,1 @@\n-    if (live == NULL) {\n+    if (live == nullptr) {\n@@ -81,0 +160,17 @@\n+\n+  void inc_trampoline_stubs_count() {\n+    assert(_trampoline_stubs_count != INT_MAX, \"Overflow\");\n+    ++_trampoline_stubs_count;\n+  }\n+\n+  int trampoline_stubs_count() {\n+    return _trampoline_stubs_count;\n+  }\n+\n+  void set_stubs_start_offset(int offset) {\n+    _stubs_start_offset = offset;\n+  }\n+\n+  int stubs_start_offset() {\n+    return _stubs_start_offset;\n+  }\n@@ -87,2 +183,1 @@\n-ZLoadBarrierStubC2* ZLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n-  ZLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZLoadBarrierStubC2(node, ref_addr, ref, tmp, barrier_data);\n+void ZBarrierStubC2::register_stub(ZBarrierStubC2* stub) {\n@@ -92,0 +187,44 @@\n+}\n+\n+void ZBarrierStubC2::inc_trampoline_stubs_count() {\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->inc_trampoline_stubs_count();\n+  }\n+}\n+\n+int ZBarrierStubC2::trampoline_stubs_count() {\n+  return barrier_set_state()->trampoline_stubs_count();\n+}\n+\n+int ZBarrierStubC2::stubs_start_offset() {\n+  return barrier_set_state()->stubs_start_offset();\n+}\n+\n+ZBarrierStubC2::ZBarrierStubC2(const MachNode* node)\n+  : _node(node),\n+    _entry(),\n+    _continuation() {}\n+\n+Register ZBarrierStubC2::result() const {\n+  return noreg;\n+}\n+\n+RegMask& ZBarrierStubC2::live() const {\n+  return *barrier_set_state()->live(_node);\n+}\n+\n+Label* ZBarrierStubC2::entry() {\n+  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n+  \/\/ However, we still need to return a label that is not bound now, but\n+  \/\/ will eventually be bound. Any eventually bound label will do, as it\n+  \/\/ will only act as a placeholder, so we return the _continuation label.\n+  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n+}\n+\n+Label* ZBarrierStubC2::continuation() {\n+  return &_continuation;\n+}\n+\n+ZLoadBarrierStubC2* ZLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref) {\n+  ZLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZLoadBarrierStubC2(node, ref_addr, ref);\n+  register_stub(stub);\n@@ -96,2 +235,2 @@\n-ZLoadBarrierStubC2::ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) :\n-    _node(node),\n+ZLoadBarrierStubC2::ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref)\n+  : ZBarrierStubC2(node),\n@@ -99,5 +238,1 @@\n-    _ref(ref),\n-    _tmp(tmp),\n-    _barrier_data(barrier_data),\n-    _entry(),\n-    _continuation() {\n+    _ref(ref) {\n@@ -116,2 +251,2 @@\n-Register ZLoadBarrierStubC2::tmp() const {\n-  return _tmp;\n+Register ZLoadBarrierStubC2::result() const {\n+  return ref();\n@@ -121,0 +256,1 @@\n+  const uint8_t barrier_data = _node->barrier_data();\n@@ -122,1 +258,1 @@\n-  if (_barrier_data & ZLoadBarrierStrong) {\n+  if (barrier_data & ZBarrierStrong) {\n@@ -125,1 +261,1 @@\n-  if (_barrier_data & ZLoadBarrierWeak) {\n+  if (barrier_data & ZBarrierWeak) {\n@@ -128,1 +264,1 @@\n-  if (_barrier_data & ZLoadBarrierPhantom) {\n+  if (barrier_data & ZBarrierPhantom) {\n@@ -131,1 +267,1 @@\n-  if (_barrier_data & ZLoadBarrierNoKeepalive) {\n+  if (barrier_data & ZBarrierNoKeepalive) {\n@@ -137,4 +273,2 @@\n-RegMask& ZLoadBarrierStubC2::live() const {\n-  RegMask* mask = barrier_set_state()->live(_node);\n-  assert(mask != NULL, \"must be mach-node with barrier\");\n-  return *mask;\n+void ZLoadBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, static_cast<ZLoadBarrierStubC2*>(this));\n@@ -143,6 +277,5 @@\n-Label* ZLoadBarrierStubC2::entry() {\n-  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n-  \/\/ However, we still need to return a label that is not bound now, but\n-  \/\/ will eventually be bound. Any label will do, as it will only act as\n-  \/\/ a placeholder, so we return the _continuation label.\n-  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n+ZStoreBarrierStubC2* ZStoreBarrierStubC2::create(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic) {\n+  ZStoreBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZStoreBarrierStubC2(node, ref_addr, new_zaddress, new_zpointer, is_native, is_atomic);\n+  register_stub(stub);\n+\n+  return stub;\n@@ -151,2 +284,34 @@\n-Label* ZLoadBarrierStubC2::continuation() {\n-  return &_continuation;\n+ZStoreBarrierStubC2::ZStoreBarrierStubC2(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic)\n+  : ZBarrierStubC2(node),\n+    _ref_addr(ref_addr),\n+    _new_zaddress(new_zaddress),\n+    _new_zpointer(new_zpointer),\n+    _is_native(is_native),\n+    _is_atomic(is_atomic) {}\n+\n+Address ZStoreBarrierStubC2::ref_addr() const {\n+  return _ref_addr;\n+}\n+\n+Register ZStoreBarrierStubC2::new_zaddress() const {\n+  return _new_zaddress;\n+}\n+\n+Register ZStoreBarrierStubC2::new_zpointer() const {\n+  return _new_zpointer;\n+}\n+\n+bool ZStoreBarrierStubC2::is_native() const {\n+  return _is_native;\n+}\n+\n+bool ZStoreBarrierStubC2::is_atomic() const {\n+  return _is_atomic;\n+}\n+\n+Register ZStoreBarrierStubC2::result() const {\n+  return noreg;\n+}\n+\n+void ZStoreBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  ZBarrierSet::assembler()->generate_c2_store_barrier_stub(&masm, static_cast<ZStoreBarrierStubC2*>(this));\n@@ -160,1 +325,1 @@\n-  analyze_dominating_barriers();\n+  analyze_dominating_barriers();\n@@ -166,1 +331,2 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  GrowableArray<ZBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  barrier_set_state()->set_stubs_start_offset(masm.offset());\n@@ -170,1 +336,1 @@\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == nullptr) {\n@@ -175,1 +341,1 @@\n-    ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+    stubs->at(i)->emit_code(masm);\n@@ -184,1 +350,1 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  GrowableArray<ZBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n@@ -190,1 +356,1 @@\n-    ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+    stubs->at(i)->emit_code(masm);\n@@ -198,2 +364,3 @@\n-  if (ZBarrierSet::barrier_needed(access.decorators(), access.type())) {\n-    uint8_t barrier_data = 0;\n+  if (!ZBarrierSet::barrier_needed(access.decorators(), access.type())) {\n+    return;\n+  }\n@@ -201,7 +368,4 @@\n-    if (access.decorators() & ON_PHANTOM_OOP_REF) {\n-      barrier_data |= ZLoadBarrierPhantom;\n-    } else if (access.decorators() & ON_WEAK_OOP_REF) {\n-      barrier_data |= ZLoadBarrierWeak;\n-    } else {\n-      barrier_data |= ZLoadBarrierStrong;\n-    }\n+  if (access.decorators() & C2_TIGHTLY_COUPLED_ALLOC) {\n+    access.set_barrier_data(ZBarrierElided);\n+    return;\n+  }\n@@ -209,3 +373,9 @@\n-    if (access.decorators() & AS_NO_KEEPALIVE) {\n-      barrier_data |= ZLoadBarrierNoKeepalive;\n-    }\n+  uint8_t barrier_data = 0;\n+\n+  if (access.decorators() & ON_PHANTOM_OOP_REF) {\n+    barrier_data |= ZBarrierPhantom;\n+  } else if (access.decorators() & ON_WEAK_OOP_REF) {\n+    barrier_data |= ZBarrierWeak;\n+  } else {\n+    barrier_data |= ZBarrierStrong;\n+  }\n@@ -213,1 +383,2 @@\n-    access.set_barrier_data(barrier_data);\n+  if (access.decorators() & IN_NATIVE) {\n+    barrier_data |= ZBarrierNative;\n@@ -215,0 +386,11 @@\n+\n+  if (access.decorators() & AS_NO_KEEPALIVE) {\n+    barrier_data |= ZBarrierNoKeepalive;\n+  }\n+\n+  access.set_barrier_data(barrier_data);\n+}\n+\n+Node* ZBarrierSetC2::store_at_resolved(C2Access& access, C2AccessValue& val) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::store_at_resolved(access, val);\n@@ -255,1 +437,1 @@\n-  const Type** domain_fields = TypeTuple::fields(4);\n+  const Type** const domain_fields = TypeTuple::fields(4);\n@@ -260,1 +442,1 @@\n-  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n+  const TypeTuple* const domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n@@ -263,2 +445,2 @@\n-  const Type** range_fields = TypeTuple::fields(0);\n-  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n+  const Type** const range_fields = TypeTuple::fields(0);\n+  const TypeTuple* const range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n@@ -273,1 +455,1 @@\n-  const TypeAryPtr* ary_ptr = src->get_ptr_type()->isa_aryptr();\n+  const TypeAryPtr* const ary_ptr = src->get_ptr_type()->isa_aryptr();\n@@ -275,1 +457,1 @@\n-  if (ac->is_clone_array() && ary_ptr != NULL) {\n+  if (ac->is_clone_array() && ary_ptr != nullptr) {\n@@ -285,3 +467,3 @@\n-    Node* ctrl = ac->in(TypeFunc::Control);\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    Node* src = ac->in(ArrayCopyNode::Src);\n+    Node* const ctrl = ac->in(TypeFunc::Control);\n+    Node* const mem = ac->in(TypeFunc::Memory);\n+    Node* const src = ac->in(ArrayCopyNode::Src);\n@@ -289,1 +471,1 @@\n-    Node* dest = ac->in(ArrayCopyNode::Dest);\n+    Node* const dest = ac->in(ArrayCopyNode::Dest);\n@@ -299,1 +481,1 @@\n-      jlong offset = src_offset->get_long();\n+      const jlong offset = src_offset->get_long();\n@@ -308,2 +490,2 @@\n-    Node* payload_src = phase->basic_plus_adr(src, src_offset);\n-    Node* payload_dst = phase->basic_plus_adr(dest, dest_offset);\n+    Node* const payload_src = phase->basic_plus_adr(src, src_offset);\n+    Node* const payload_dst = phase->basic_plus_adr(dest, dest_offset);\n@@ -311,2 +493,2 @@\n-    const char* copyfunc_name = \"arraycopy\";\n-    address     copyfunc_addr = phase->basictype2arraycopy(bt, NULL, NULL, true, copyfunc_name, true);\n+    const char*   copyfunc_name = \"arraycopy\";\n+    const address copyfunc_addr = phase->basictype2arraycopy(bt, nullptr, nullptr, true, copyfunc_name, true);\n@@ -314,2 +496,2 @@\n-    const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n-    const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n+    const TypePtr* const raw_adr_type = TypeRawPtr::BOTTOM;\n+    const TypeFunc* const call_type = OptoRuntime::fast_arraycopy_Type();\n@@ -317,1 +499,1 @@\n-    Node* call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n+    Node* const call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n@@ -381,7 +563,19 @@\n-void ZBarrierSetC2::analyze_dominating_barriers() const {\n-  ResourceMark rm;\n-  Compile* const C = Compile::current();\n-  PhaseCFG* const cfg = C->cfg();\n-  Block_List worklist;\n-  Node_List mem_ops;\n-  Node_List barrier_loads;\n+\/\/ Look through various node aliases\n+static const Node* look_through_node(const Node* node) {\n+  while (node != nullptr) {\n+    const Node* new_node = node;\n+    if (node->is_Mach()) {\n+      const MachNode* const node_mach = node->as_Mach();\n+      if (node_mach->ideal_Opcode() == Op_CheckCastPP) {\n+        new_node = node->in(1);\n+      }\n+      if (node_mach->is_SpillCopy()) {\n+        new_node = node->in(1);\n+      }\n+    }\n+    if (new_node == node || new_node == nullptr) {\n+      break;\n+    } else {\n+      node = new_node;\n+    }\n+  }\n@@ -389,6 +583,62 @@\n-  \/\/ Step 1 - Find accesses, and track them in lists\n-  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n-    const Block* const block = cfg->get_block(i);\n-    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n-      const Node* const node = block->get_node(j);\n-      if (!node->is_Mach()) {\n+  return node;\n+}\n+\n+\/\/ Whether the given offset is undefined.\n+static bool is_undefined(intptr_t offset) {\n+  return offset == Type::OffsetTop;\n+}\n+\n+\/\/ Whether the given offset is unknown.\n+static bool is_unknown(intptr_t offset) {\n+  return offset == Type::OffsetBot;\n+}\n+\n+\/\/ Whether the given offset is concrete (defined and compile-time known).\n+static bool is_concrete(intptr_t offset) {\n+  return !is_undefined(offset) && !is_unknown(offset);\n+}\n+\n+\/\/ Compute base + offset components of the memory address accessed by mach.\n+\/\/ Return a node representing the base address, or null if the base cannot be\n+\/\/ found or the offset is undefined or a concrete negative value. If a non-null\n+\/\/ base is returned, the offset is a concrete, nonnegative value or unknown.\n+static const Node* get_base_and_offset(const MachNode* mach, intptr_t& offset) {\n+  const TypePtr* adr_type = nullptr;\n+  offset = 0;\n+  const Node* base = mach->get_base_and_disp(offset, adr_type);\n+\n+  if (base == nullptr || base == NodeSentinel) {\n+    return nullptr;\n+  }\n+\n+  if (offset == 0 && base->is_Mach() && base->as_Mach()->ideal_Opcode() == Op_AddP) {\n+    \/\/ The memory address is computed by 'base' and fed to 'mach' via an\n+    \/\/ indirect memory operand (indicated by offset == 0). The ultimate base and\n+    \/\/ offset can be fetched directly from the inputs and Ideal type of 'base'.\n+    offset = base->bottom_type()->isa_oopptr()->offset();\n+    \/\/ Even if 'base' is not an Ideal AddP node anymore, Matcher::ReduceInst()\n+    \/\/ guarantees that the base address is still available at the same slot.\n+    base = base->in(AddPNode::Base);\n+    assert(base != nullptr, \"\");\n+  }\n+\n+  if (is_undefined(offset) || (is_concrete(offset) && offset < 0)) {\n+    return nullptr;\n+  }\n+\n+  return look_through_node(base);\n+}\n+\n+\/\/ Whether a phi node corresponds to an array allocation.\n+\/\/ This test is incomplete: in some edge cases, it might return false even\n+\/\/ though the node does correspond to an array allocation.\n+static bool is_array_allocation(const Node* phi) {\n+  precond(phi->is_Phi());\n+  \/\/ Check whether phi has a successor cast (CheckCastPP) to Java array pointer,\n+  \/\/ possibly below spill copies and other cast nodes. Limit the exploration to\n+  \/\/ a single path from the phi node consisting of these node types.\n+  const Node* current = phi;\n+  while (true) {\n+    const Node* next = nullptr;\n+    for (DUIterator_Fast imax, i = current->fast_outs(imax); i < imax; i++) {\n+      if (!current->fast_out(i)->isa_Mach()) {\n@@ -397,10 +647,5 @@\n-\n-      MachNode* const mach = node->as_Mach();\n-      switch (mach->ideal_Opcode()) {\n-      case Op_LoadP:\n-        if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n-          barrier_loads.push(mach);\n-        }\n-        if ((mach->barrier_data() & (ZLoadBarrierStrong | ZLoadBarrierNoKeepalive)) ==\n-            ZLoadBarrierStrong) {\n-          mem_ops.push(mach);\n+      const MachNode* succ = current->fast_out(i)->as_Mach();\n+      if (succ->ideal_Opcode() == Op_CheckCastPP) {\n+        if (succ->get_ptr_type()->isa_aryptr()) {\n+          \/\/ Cast to Java array pointer: phi corresponds to an array allocation.\n+          return true;\n@@ -408,13 +653,5 @@\n-        break;\n-      case Op_CompareAndExchangeP:\n-      case Op_CompareAndSwapP:\n-      case Op_GetAndSetP:\n-        if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n-          barrier_loads.push(mach);\n-        }\n-      case Op_StoreP:\n-        mem_ops.push(mach);\n-        break;\n-\n-      default:\n-        break;\n+        \/\/ Other cast: record as candidate for further exploration.\n+        next = succ;\n+      } else if (succ->is_SpillCopy() && next == nullptr) {\n+        \/\/ Spill copy, and no better candidate found: record as candidate.\n+        next = succ;\n@@ -423,0 +660,7 @@\n+    if (next == nullptr) {\n+      \/\/ No evidence found that phi corresponds to an array allocation, and no\n+      \/\/ candidates available to continue exploring.\n+      return false;\n+    }\n+    \/\/ Continue exploring from the best candidate found.\n+    current = next;\n@@ -424,0 +668,2 @@\n+  ShouldNotReachHere();\n+}\n@@ -425,16 +671,25 @@\n-  \/\/ Step 2 - Find dominating accesses for each load\n-  for (uint i = 0; i < barrier_loads.size(); i++) {\n-    MachNode* const load = barrier_loads.at(i)->as_Mach();\n-    const TypePtr* load_adr_type = NULL;\n-    intptr_t load_offset = 0;\n-    const Node* const load_obj = load->get_base_and_disp(load_offset, load_adr_type);\n-    Block* const load_block = cfg->get_block_for_node(load);\n-    const uint load_index = block_index(load_block, load);\n-\n-    for (uint j = 0; j < mem_ops.size(); j++) {\n-      MachNode* mem = mem_ops.at(j)->as_Mach();\n-      const TypePtr* mem_adr_type = NULL;\n-      intptr_t mem_offset = 0;\n-      const Node* mem_obj = mem->get_base_and_disp(mem_offset, mem_adr_type);\n-      Block* mem_block = cfg->get_block_for_node(mem);\n-      uint mem_index = block_index(mem_block, mem);\n+\/\/ Match the phi node that connects a TLAB allocation fast path with its slowpath\n+static bool is_allocation(const Node* node) {\n+  if (node->req() != 3) {\n+    return false;\n+  }\n+  const Node* const fast_node = node->in(2);\n+  if (!fast_node->is_Mach()) {\n+    return false;\n+  }\n+  const MachNode* const fast_mach = fast_node->as_Mach();\n+  if (fast_mach->ideal_Opcode() != Op_LoadP) {\n+    return false;\n+  }\n+  const TypePtr* const adr_type = nullptr;\n+  intptr_t offset;\n+  const Node* const base = get_base_and_offset(fast_mach, offset);\n+  if (base == nullptr || !base->is_Mach() || !is_concrete(offset)) {\n+    return false;\n+  }\n+  const MachNode* const base_mach = base->as_Mach();\n+  if (base_mach->ideal_Opcode() != Op_ThreadLocal) {\n+    return false;\n+  }\n+  return offset == in_bytes(Thread::tlab_top_offset());\n+}\n@@ -442,5 +697,3 @@\n-      if (load_obj == NodeSentinel || mem_obj == NodeSentinel ||\n-          load_obj == NULL || mem_obj == NULL ||\n-          load_offset < 0 || mem_offset < 0) {\n-        continue;\n-      }\n+static void elide_mach_barrier(MachNode* mach) {\n+  mach->set_barrier_data(ZBarrierElided);\n+}\n@@ -448,3 +701,50 @@\n-      if (mem_obj != load_obj || mem_offset != load_offset) {\n-        \/\/ Not the same addresses, not a candidate\n-        continue;\n+void ZBarrierSetC2::analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const {\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+\n+  for (uint i = 0; i < accesses.size(); i++) {\n+    MachNode* const access = accesses.at(i)->as_Mach();\n+    intptr_t access_offset;\n+    const Node* const access_obj = get_base_and_offset(access, access_offset);\n+    Block* const access_block = cfg->get_block_for_node(access);\n+    const uint access_index = block_index(access_block, access);\n+\n+    if (access_obj == nullptr) {\n+      \/\/ No information available\n+      continue;\n+    }\n+\n+    for (uint j = 0; j < access_dominators.size(); j++) {\n+     const  Node* const mem = access_dominators.at(j);\n+      if (mem->is_Phi()) {\n+        \/\/ Allocation node\n+        if (mem != access_obj) {\n+          continue;\n+        }\n+        if (is_unknown(access_offset) && !is_array_allocation(mem)) {\n+          \/\/ The accessed address has an unknown offset, but the allocated\n+          \/\/ object cannot be determined to be an array. Avoid eliding in this\n+          \/\/ case, to be on the safe side.\n+          continue;\n+        }\n+        assert((is_concrete(access_offset) && access_offset >= 0) || (is_unknown(access_offset) && is_array_allocation(mem)),\n+               \"candidate allocation-dominated access offsets must be either concrete and nonnegative, or unknown (for array allocations only)\");\n+      } else {\n+        \/\/ Access node\n+        const MachNode* const mem_mach = mem->as_Mach();\n+        intptr_t mem_offset;\n+        const Node* const mem_obj = get_base_and_offset(mem_mach, mem_offset);\n+\n+        if (mem_obj == nullptr ||\n+            !is_concrete(access_offset) ||\n+            !is_concrete(mem_offset)) {\n+          \/\/ No information available\n+          continue;\n+        }\n+\n+        if (mem_obj != access_obj || mem_offset != access_offset) {\n+          \/\/ Not the same addresses, not a candidate\n+          continue;\n+        }\n+        assert(is_concrete(access_offset) && access_offset >= 0,\n+               \"candidate non-allocation-dominated access offsets must be concrete and nonnegative\");\n@@ -453,1 +753,4 @@\n-      if (load_block == mem_block) {\n+      Block* mem_block = cfg->get_block_for_node(mem);\n+      const uint mem_index = block_index(mem_block, mem);\n+\n+      if (access_block == mem_block) {\n@@ -455,2 +758,2 @@\n-        if (mem_index < load_index && !block_has_safepoint(mem_block, mem_index + 1, load_index)) {\n-          load->set_barrier_data(ZLoadBarrierElided);\n+        if (mem_index < access_index && !block_has_safepoint(mem_block, mem_index + 1, access_index)) {\n+          elide_mach_barrier(access);\n@@ -458,1 +761,1 @@\n-      } else if (mem_block->dominates(load_block)) {\n+      } else if (mem_block->dominates(access_block)) {\n@@ -463,2 +766,2 @@\n-        stack.push(load_block);\n-        bool safepoint_found = block_has_safepoint(load_block);\n+        stack.push(access_block);\n+        bool safepoint_found = block_has_safepoint(access_block);\n@@ -466,1 +769,1 @@\n-          Block* block = stack.pop();\n+          const Block* const block = stack.pop();\n@@ -480,1 +783,1 @@\n-            Block* pred = cfg->get_block_for_node(block->pred(p));\n+            Block* const pred = cfg->get_block_for_node(block->pred(p));\n@@ -486,1 +789,48 @@\n-          load->set_barrier_data(ZLoadBarrierElided);\n+          elide_mach_barrier(access);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void ZBarrierSetC2::analyze_dominating_barriers() const {\n+  ResourceMark rm;\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+\n+  Node_List loads;\n+  Node_List load_dominators;\n+\n+  Node_List stores;\n+  Node_List store_dominators;\n+\n+  Node_List atomics;\n+  Node_List atomic_dominators;\n+\n+  \/\/ Step 1 - Find accesses and allocations, and track them in lists\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    const Block* const block = cfg->get_block(i);\n+    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+      Node* const node = block->get_node(j);\n+      if (node->is_Phi()) {\n+        if (is_allocation(node)) {\n+          load_dominators.push(node);\n+          store_dominators.push(node);\n+          \/\/ An allocation can't be considered to \"dominate\" an atomic operation.\n+          \/\/ For example a CAS requires the memory location to be store-good.\n+          \/\/ When you have a dominating store or atomic instruction, that is\n+          \/\/ indeed ensured to be the case. However, as for allocations, the\n+          \/\/ initialized memory location could be raw null, which isn't store-good.\n+        }\n+        continue;\n+      } else if (!node->is_Mach()) {\n+        continue;\n+      }\n+\n+      MachNode* const mach = node->as_Mach();\n+      switch (mach->ideal_Opcode()) {\n+      case Op_LoadP:\n+        if ((mach->barrier_data() & ZBarrierStrong) != 0 &&\n+            (mach->barrier_data() & ZBarrierNoKeepalive) == 0) {\n+          loads.push(mach);\n+          load_dominators.push(mach);\n@@ -488,0 +838,22 @@\n+        break;\n+      case Op_StoreP:\n+        if (mach->barrier_data() != 0) {\n+          stores.push(mach);\n+          load_dominators.push(mach);\n+          store_dominators.push(mach);\n+          atomic_dominators.push(mach);\n+        }\n+        break;\n+      case Op_CompareAndExchangeP:\n+      case Op_CompareAndSwapP:\n+      case Op_GetAndSetP:\n+        if (mach->barrier_data() != 0) {\n+          atomics.push(mach);\n+          load_dominators.push(mach);\n+          store_dominators.push(mach);\n+          atomic_dominators.push(mach);\n+        }\n+        break;\n+\n+      default:\n+        break;\n@@ -491,0 +863,5 @@\n+\n+  \/\/ Step 2 - Find dominating accesses or allocations for each access\n+  analyze_dominating_barriers_impl(loads, load_dominators);\n+  analyze_dominating_barriers_impl(stores, store_dominators);\n+  analyze_dominating_barriers_impl(atomics, atomic_dominators);\n@@ -550,1 +927,1 @@\n-      if (regs != NULL) {\n+      if (regs != nullptr) {\n@@ -568,0 +945,14 @@\n+void ZBarrierSetC2::eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const {\n+  eliminate_gc_barrier_data(node);\n+}\n+\n+void ZBarrierSetC2::eliminate_gc_barrier_data(Node* node) const {\n+  if (node->is_LoadStore()) {\n+    LoadStoreNode* loadstore = node->as_LoadStore();\n+    loadstore->set_barrier_data(ZBarrierElided);\n+  } else if (node->is_Mem()) {\n+    MemNode* mem = node->as_Mem();\n+    mem->set_barrier_data(ZBarrierElided);\n+  }\n+}\n+\n@@ -570,1 +961,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n+  if ((mach->barrier_data() & ZBarrierStrong) != 0) {\n@@ -573,1 +964,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierWeak) != 0) {\n+  if ((mach->barrier_data() & ZBarrierWeak) != 0) {\n@@ -576,1 +967,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierPhantom) != 0) {\n+  if ((mach->barrier_data() & ZBarrierPhantom) != 0) {\n@@ -579,1 +970,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierNoKeepalive) != 0) {\n+  if ((mach->barrier_data() & ZBarrierNoKeepalive) != 0) {\n@@ -582,0 +973,6 @@\n+  if ((mach->barrier_data() & ZBarrierNative) != 0) {\n+    st->print(\"native \");\n+  }\n+  if ((mach->barrier_data() & ZBarrierElided) != 0) {\n+    st->print(\"elided \");\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":545,"deletions":148,"binary":false,"changes":693,"status":"modified"},{"patch":"@@ -32,5 +32,6 @@\n-const uint8_t ZLoadBarrierElided      = 0;\n-const uint8_t ZLoadBarrierStrong      = 1;\n-const uint8_t ZLoadBarrierWeak        = 2;\n-const uint8_t ZLoadBarrierPhantom     = 4;\n-const uint8_t ZLoadBarrierNoKeepalive = 8;\n+const uint8_t ZBarrierStrong      =  1;\n+const uint8_t ZBarrierWeak        =  2;\n+const uint8_t ZBarrierPhantom     =  4;\n+const uint8_t ZBarrierNoKeepalive =  8;\n+const uint8_t ZBarrierNative      = 16;\n+const uint8_t ZBarrierElided      = 32;\n@@ -38,2 +39,7 @@\n-class ZLoadBarrierStubC2 : public ArenaObj {\n-private:\n+class Block;\n+class MachNode;\n+\n+class MacroAssembler;\n+\n+class ZBarrierStubC2 : public ArenaObj {\n+protected:\n@@ -41,4 +47,0 @@\n-  const Address   _ref_addr;\n-  const Register  _ref;\n-  const Register  _tmp;\n-  const uint8_t   _barrier_data;\n@@ -48,1 +50,4 @@\n-  ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data);\n+static void register_stub(ZBarrierStubC2* stub);\n+static void inc_trampoline_stubs_count();\n+static int trampoline_stubs_count();\n+static int stubs_start_offset();\n@@ -51,1 +56,1 @@\n-  static ZLoadBarrierStubC2* create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data);\n+  ZBarrierStubC2(const MachNode* node);\n@@ -53,4 +58,0 @@\n-  Address ref_addr() const;\n-  Register ref() const;\n-  Register tmp() const;\n-  address slow_path() const;\n@@ -60,0 +61,46 @@\n+\n+  virtual Register result() const = 0;\n+  virtual void emit_code(MacroAssembler& masm) = 0;\n+};\n+\n+class ZLoadBarrierStubC2 : public ZBarrierStubC2 {\n+private:\n+  const Address  _ref_addr;\n+  const Register _ref;\n+\n+protected:\n+  ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref);\n+\n+public:\n+  static ZLoadBarrierStubC2* create(const MachNode* node, Address ref_addr, Register ref);\n+\n+  Address ref_addr() const;\n+  Register ref() const;\n+  address slow_path() const;\n+\n+  virtual Register result() const;\n+  virtual void emit_code(MacroAssembler& masm);\n+};\n+\n+class ZStoreBarrierStubC2 : public ZBarrierStubC2 {\n+private:\n+  const Address  _ref_addr;\n+  const Register _new_zaddress;\n+  const Register _new_zpointer;\n+  const bool     _is_native;\n+  const bool     _is_atomic;\n+\n+protected:\n+  ZStoreBarrierStubC2(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic);\n+\n+public:\n+  static ZStoreBarrierStubC2* create(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic);\n+\n+  Address ref_addr() const;\n+  Register new_zaddress() const;\n+  Register new_zpointer() const;\n+  bool is_native() const;\n+  bool is_atomic() const;\n+\n+  virtual Register result() const;\n+  virtual void emit_code(MacroAssembler& masm);\n@@ -65,0 +112,1 @@\n+  void analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const;\n@@ -68,0 +116,1 @@\n+  virtual Node* store_at_resolved(C2Access& access, C2AccessValue& val) const;\n@@ -94,0 +143,2 @@\n+  virtual void eliminate_gc_barrier(PhaseIterGVN* igvn, Node* node) const;\n+  virtual void eliminate_gc_barrier_data(Node* node) const;\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.hpp","additions":68,"deletions":17,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -29,1 +30,0 @@\n-#include \"oops\/oop.hpp\"\n@@ -32,2 +32,44 @@\n-typedef bool (*ZBarrierFastPath)(uintptr_t);\n-typedef uintptr_t (*ZBarrierSlowPath)(uintptr_t);\n+\/\/ == Shift based load barrier ==\n+\/\/\n+\/\/ The load barriers of ZGC check if a loaded value is safe to expose or not, and\n+\/\/ then shifts the pointer to remove metadata bits, such that it points to mapped\n+\/\/ memory.\n+\/\/\n+\/\/ A pointer is safe to expose if it does not have any load-bad bits set in its\n+\/\/ metadata bits. In the C++ code and non-nmethod generated code, that is checked\n+\/\/ by testing the pointer value against a load-bad mask, checking that no bad bit\n+\/\/ is set, followed by a shift, removing the metadata bits if they were good.\n+\/\/ However, for nmethod code, the test + shift sequence is optimized in such\n+\/\/ a way that the shift both tests if the pointer is exposable or not, and removes\n+\/\/ the metadata bits, with the same instruction. This is a speculative optimization\n+\/\/ that assumes that the loaded pointer is frequently going to be load-good or null\n+\/\/ when checked. Therefore, the nmethod load barriers just apply the shift with the\n+\/\/ current \"good\" shift (which is patched with nmethod entry barriers for each GC\n+\/\/ phase). If the result of that shift was a raw null value, then the ZF flag is set.\n+\/\/ If the result is a good pointer, then the very last bit that was removed by the\n+\/\/ shift, must have been a 1, which would have set the CF flag. Therefore, the \"above\"\n+\/\/ branch condition code is used to take a slowpath only iff CF == 0 and ZF == 0.\n+\/\/ CF == 0 implies it was not a good pointer, and ZF == 0 implies the resulting address\n+\/\/ was not a null value. Then we decide that the pointer is bad. This optimization\n+\/\/ is necessary to get satisfactory performance, but does come with a few constraints:\n+\/\/\n+\/\/ 1) The load barrier can only recognize 4 different good patterns across all GC phases.\n+\/\/    The reason is that when a load barrier applies the currently good shift, then\n+\/\/    the value of said shift may differ only by 3, until we risk shifting away more\n+\/\/    than the low order three zeroes of an address, given a bad pointer, which would\n+\/\/    yield spurious false positives.\n+\/\/\n+\/\/ 2) Those bit patterns must have only a single bit set. We achieve that by moving\n+\/\/    non-relocation work to store barriers.\n+\/\/\n+\/\/ Another consequence of this speculative optimization, is that when the compiled code\n+\/\/ takes a slow path, it needs to reload the oop, because the shifted oop is now\n+\/\/ broken after being shifted with a different shift to what was used when the oop\n+\/\/ was stored.\n+\n+typedef bool (*ZBarrierFastPath)(zpointer);\n+typedef zpointer (*ZBarrierColor)(zaddress, zpointer);\n+\n+class ZGeneration;\n+\n+void z_assert_is_barrier_safe();\n@@ -36,9 +78,3 @@\n-private:\n-  static const bool GCThread    = true;\n-  static const bool AnyThread   = false;\n-\n-  static const bool Follow      = true;\n-  static const bool DontFollow  = false;\n-\n-  static const bool Strong      = false;\n-  static const bool Finalizable = true;\n+  friend class ZContinuation;\n+  friend class ZStoreBarrierBuffer;\n+  friend class ZUncoloredRoot;\n@@ -46,4 +82,54 @@\n-  static const bool Publish     = true;\n-  static const bool Overflow    = false;\n-\n-  template <ZBarrierFastPath fast_path> static void self_heal(volatile oop* p, uintptr_t addr, uintptr_t heal_addr);\n+private:\n+  static void assert_transition_monotonicity(zpointer ptr, zpointer heal_ptr);\n+  static void self_heal(ZBarrierFastPath fast_path, volatile zpointer* p, zpointer ptr, zpointer heal_ptr, bool allow_null);\n+\n+  template <typename ZBarrierSlowPath>\n+  static zaddress barrier(ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path, ZBarrierColor color, volatile zpointer* p, zpointer o, bool allow_null = false);\n+\n+  static zaddress make_load_good(zpointer ptr);\n+  static zaddress make_load_good_no_relocate(zpointer ptr);\n+  static zaddress relocate_or_remap(zaddress_unsafe addr, ZGeneration* generation);\n+  static zaddress remap(zaddress_unsafe addr, ZGeneration* generation);\n+  static void remember(volatile zpointer* p);\n+  static void mark_and_remember(volatile zpointer* p, zaddress addr);\n+\n+  \/\/ Fast paths in increasing strength level\n+  static bool is_load_good_or_null_fast_path(zpointer ptr);\n+  static bool is_mark_good_fast_path(zpointer ptr);\n+  static bool is_store_good_fast_path(zpointer ptr);\n+  static bool is_store_good_or_null_fast_path(zpointer ptr);\n+  static bool is_store_good_or_null_any_fast_path(zpointer ptr);\n+\n+  static bool is_mark_young_good_fast_path(zpointer ptr);\n+  static bool is_finalizable_good_fast_path(zpointer ptr);\n+\n+  \/\/ Slow paths\n+  static zaddress blocking_keep_alive_on_weak_slow_path(volatile zpointer* p, zaddress addr);\n+  static zaddress blocking_keep_alive_on_phantom_slow_path(volatile zpointer* p, zaddress addr);\n+  static zaddress blocking_load_barrier_on_weak_slow_path(volatile zpointer* p, zaddress addr);\n+  static zaddress blocking_load_barrier_on_phantom_slow_path(volatile zpointer* p, zaddress addr);\n+\n+  static zaddress verify_old_object_live_slow_path(zaddress addr);\n+\n+  static zaddress mark_slow_path(zaddress addr);\n+  static zaddress mark_young_slow_path(zaddress addr);\n+  static zaddress mark_from_young_slow_path(zaddress addr);\n+  static zaddress mark_from_old_slow_path(zaddress addr);\n+  static zaddress mark_finalizable_slow_path(zaddress addr);\n+  static zaddress mark_finalizable_from_old_slow_path(zaddress addr);\n+\n+  static zaddress keep_alive_slow_path(zaddress addr);\n+  static zaddress heap_store_slow_path(volatile zpointer* p, zaddress addr, zpointer prev, bool heal);\n+  static zaddress native_store_slow_path(zaddress addr);\n+  static zaddress no_keep_alive_heap_store_slow_path(volatile zpointer* p, zaddress addr);\n+\n+  static zaddress promote_slow_path(zaddress addr);\n+\n+  \/\/ Helpers for non-strong oop refs barriers\n+  static zaddress blocking_keep_alive_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress blocking_keep_alive_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress blocking_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress blocking_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+\n+  \/\/ Verification\n+  static void verify_on_weak(volatile zpointer* referent_addr) NOT_DEBUG_RETURN;\n@@ -51,3 +137,1 @@\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static oop barrier(volatile oop* p, oop o);\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static oop weak_barrier(volatile oop* p, oop o);\n-  template <ZBarrierFastPath fast_path, ZBarrierSlowPath slow_path> static void root_barrier(oop* p, oop o);\n+public:\n@@ -55,3 +139,1 @@\n-  static bool is_good_or_null_fast_path(uintptr_t addr);\n-  static bool is_weak_good_or_null_fast_path(uintptr_t addr);\n-  static bool is_marked_or_null_fast_path(uintptr_t addr);\n+  static zpointer load_atomic(volatile zpointer* p);\n@@ -59,9 +141,3 @@\n-  static bool during_mark();\n-  static bool during_relocate();\n-  template <bool finalizable> static bool should_mark_through(uintptr_t addr);\n-  template <bool gc_thread, bool follow, bool finalizable, bool publish> static uintptr_t mark(uintptr_t addr);\n-  static uintptr_t remap(uintptr_t addr);\n-  static uintptr_t relocate(uintptr_t addr);\n-  static uintptr_t relocate_or_mark(uintptr_t addr);\n-  static uintptr_t relocate_or_mark_no_follow(uintptr_t addr);\n-  static uintptr_t relocate_or_remap(uintptr_t addr);\n+  \/\/ Helpers for relocation\n+  static ZGeneration* remap_generation(zpointer ptr);\n+  static void remap_young_relocated(volatile zpointer* p, zpointer o);\n@@ -69,2 +145,7 @@\n-  static uintptr_t load_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t load_barrier_on_invisible_root_oop_slow_path(uintptr_t addr);\n+  \/\/ Helpers for marking\n+  template <bool resurrect, bool gc_thread, bool follow, bool finalizable>\n+  static void mark(zaddress addr);\n+  template <bool resurrect, bool gc_thread, bool follow>\n+  static void mark_young(zaddress addr);\n+  template <bool resurrect, bool gc_thread, bool follow>\n+  static void mark_if_young(zaddress addr);\n@@ -72,3 +153,3 @@\n-  static uintptr_t weak_load_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t weak_load_barrier_on_weak_oop_slow_path(uintptr_t addr);\n-  static uintptr_t weak_load_barrier_on_phantom_oop_slow_path(uintptr_t addr);\n+  \/\/ Load barrier\n+  static zaddress load_barrier_on_oop_field(volatile zpointer* p);\n+  static zaddress load_barrier_on_oop_field_preloaded(volatile zpointer* p, zpointer o);\n@@ -76,3 +157,1 @@\n-  static uintptr_t keep_alive_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t keep_alive_barrier_on_weak_oop_slow_path(uintptr_t addr);\n-  static uintptr_t keep_alive_barrier_on_phantom_oop_slow_path(uintptr_t addr);\n+  static zaddress keep_alive_load_barrier_on_oop_field_preloaded(volatile zpointer* p, zpointer o);\n@@ -80,2 +159,3 @@\n-  static uintptr_t mark_barrier_on_oop_slow_path(uintptr_t addr);\n-  static uintptr_t mark_barrier_on_finalizable_oop_slow_path(uintptr_t addr);\n+  \/\/ Load barriers on non-strong oop refs\n+  static zaddress load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o);\n@@ -83,1 +163,2 @@\n-  static void verify_on_weak(volatile oop* referent_addr) NOT_DEBUG_RETURN;\n+  static zaddress no_keep_alive_load_barrier_on_weak_oop_field_preloaded(volatile zpointer* p, zpointer o);\n+  static zaddress no_keep_alive_load_barrier_on_phantom_oop_field_preloaded(volatile zpointer* p, zpointer o);\n@@ -85,29 +166,4 @@\n-public:\n-  \/\/ Load barrier\n-  static  oop load_barrier_on_oop(oop o);\n-  static  oop load_barrier_on_oop_field(volatile oop* p);\n-  static  oop load_barrier_on_oop_field_preloaded(volatile oop* p, oop o);\n-  static void load_barrier_on_oop_array(volatile oop* p, size_t length);\n-  static void load_barrier_on_oop_fields(oop o);\n-  static  oop load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o);\n-  static  oop load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o);\n-  static void load_barrier_on_root_oop_field(oop* p);\n-  static void load_barrier_on_invisible_root_oop_field(oop* p);\n-\n-  \/\/ Weak load barrier\n-  static oop weak_load_barrier_on_oop_field(volatile oop* p);\n-  static oop weak_load_barrier_on_oop_field_preloaded(volatile oop* p, oop o);\n-  static oop weak_load_barrier_on_weak_oop(oop o);\n-  static oop weak_load_barrier_on_weak_oop_field_preloaded(volatile oop* p, oop o);\n-  static oop weak_load_barrier_on_phantom_oop(oop o);\n-  static oop weak_load_barrier_on_phantom_oop_field_preloaded(volatile oop* p, oop o);\n-\n-  \/\/ Is alive barrier\n-  static bool is_alive_barrier_on_weak_oop(oop o);\n-  static bool is_alive_barrier_on_phantom_oop(oop o);\n-\n-  \/\/ Keep alive barrier\n-  static void keep_alive_barrier_on_oop(oop o);\n-  static void keep_alive_barrier_on_weak_oop_field(volatile oop* p);\n-  static void keep_alive_barrier_on_phantom_oop_field(volatile oop* p);\n-  static void keep_alive_barrier_on_phantom_root_oop_field(oop* p);\n+  \/\/ Reference processor \/ weak cleaning barriers\n+  static bool clean_barrier_on_weak_oop_field(volatile zpointer* p);\n+  static bool clean_barrier_on_phantom_oop_field(volatile zpointer* p);\n+  static bool clean_barrier_on_final_oop_field(volatile zpointer* p);\n@@ -116,18 +172,12 @@\n-  static void mark_barrier_on_oop_field(volatile oop* p, bool finalizable);\n-  static void mark_barrier_on_oop_array(volatile oop* p, size_t length, bool finalizable);\n-\n-  \/\/ Narrow oop variants, never used.\n-  static oop  load_barrier_on_oop_field(volatile narrowOop* p);\n-  static oop  load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static void load_barrier_on_oop_array(volatile narrowOop* p, size_t length);\n-  static oop  load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_weak_oop_field_preloaded(volatile narrowOop* p, oop o);\n-  static oop  weak_load_barrier_on_phantom_oop_field_preloaded(volatile narrowOop* p, oop o);\n-};\n-\n-class ZLoadBarrierOopClosure : public BasicOopIterateClosure {\n-public:\n-  virtual void do_oop(oop* p);\n-  virtual void do_oop(narrowOop* p);\n+  static void mark_barrier_on_young_oop_field(volatile zpointer* p);\n+  static void mark_barrier_on_old_oop_field(volatile zpointer* p, bool finalizable);\n+  static void mark_barrier_on_oop_field(volatile zpointer* p, bool finalizable);\n+  static void mark_young_good_barrier_on_oop_field(volatile zpointer* p);\n+  static zaddress remset_barrier_on_oop_field(volatile zpointer* p);\n+  static void promote_barrier_on_young_oop_field(volatile zpointer* p);\n+\n+  \/\/ Store barrier\n+  static void store_barrier_on_heap_oop_field(volatile zpointer* p, bool heal);\n+  static void store_barrier_on_native_oop_field(volatile zpointer* p, bool heal);\n+\n+  static void no_keep_alive_store_barrier_on_heap_oop_field(volatile zpointer* p);\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrier.hpp","additions":140,"deletions":90,"binary":false,"changes":230,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/z\/zAddress.hpp\"\n@@ -32,0 +33,3 @@\n+private:\n+  static zpointer store_good(oop obj);\n+\n@@ -43,0 +47,2 @@\n+  virtual void on_slowpath_allocation_exit(JavaThread* thread, oop new_obj);\n+\n@@ -45,0 +51,6 @@\n+  enum OopCopyCheckStatus {\n+    oop_copy_check_ok = 0,         \/\/ oop array copy sucessful\n+    oop_copy_check_class_cast = 1, \/\/ oop array copy failed subtype check (ARRAYCOPY_CHECKCAST)\n+    oop_copy_check_null = 2        \/\/ oop array copy failed null check (ARRAYCOPY_NOTNULL)\n+  };\n+\n@@ -56,1 +68,11 @@\n-    static oop* field_addr(oop base, ptrdiff_t offset);\n+    static zpointer* field_addr(oop base, ptrdiff_t offset);\n+\n+    static zaddress load_barrier(zpointer* p, zpointer o);\n+    static zaddress load_barrier_on_unknown_oop_ref(oop base, ptrdiff_t offset, zpointer* p, zpointer o);\n+\n+    static void store_barrier_heap_with_healing(zpointer* p);\n+    static void store_barrier_heap_without_healing(zpointer* p);\n+    static void no_keep_alive_store_barrier_heap(zpointer* p);\n+\n+    static void store_barrier_native_with_healing(zpointer* p);\n+    static void store_barrier_native_without_healing(zpointer* p);\n@@ -58,2 +80,8 @@\n-    template <typename T>\n-    static oop load_barrier_on_oop_field_preloaded(T* addr, oop o);\n+    static void unsupported();\n+    static zaddress load_barrier(narrowOop* p, zpointer o) { unsupported(); return zaddress::null; }\n+    static zaddress load_barrier_on_unknown_oop_ref(oop base, ptrdiff_t offset, narrowOop* p, zpointer o) { unsupported(); return zaddress::null; }\n+    static void store_barrier_heap_with_healing(narrowOop* p) { unsupported(); }\n+    static void store_barrier_heap_without_healing(narrowOop* p)  { unsupported(); }\n+    static void no_keep_alive_store_barrier_heap(narrowOop* p)  { unsupported(); }\n+    static void store_barrier_native_with_healing(narrowOop* p)  { unsupported(); }\n+    static void store_barrier_native_without_healing(narrowOop* p)  { unsupported(); }\n@@ -61,2 +89,6 @@\n-    template <typename T>\n-    static oop load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o);\n+    static zaddress oop_copy_one_barriers(zpointer* dst, zpointer* src);\n+    static OopCopyCheckStatus oop_copy_one_check_cast(zpointer* dst, zpointer* src, Klass* dst_klass);\n+    static OopCopyCheckStatus oop_copy_one(zpointer* dst, zpointer* src);\n+\n+    static OopCopyCheckStatus oop_arraycopy_in_heap_check_cast(zpointer* dst, zpointer* src, size_t length, Klass* dst_klass);\n+    static OopCopyCheckStatus oop_arraycopy_in_heap_no_check_cast(zpointer* dst, zpointer* src, size_t length);\n@@ -68,2 +100,4 @@\n-    template <typename T>\n-    static oop oop_load_in_heap(T* addr);\n+    static oop oop_load_in_heap(zpointer* p);\n+    static oop oop_load_in_heap(oop* p)       { return oop_load_in_heap((zpointer*)p); };\n+    static oop oop_load_in_heap(narrowOop* p) { unsupported(); return nullptr; }\n+\n@@ -72,2 +106,13 @@\n-    template <typename T>\n-    static oop oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value);\n+    static void oop_store_in_heap(zpointer* p, oop value);\n+    static void oop_store_in_heap(oop* p, oop value)       { oop_store_in_heap((zpointer*)p, value); }\n+    static void oop_store_in_heap(narrowOop* p, oop value) { unsupported(); }\n+    static void oop_store_in_heap_at(oop base, ptrdiff_t offset, oop value);\n+\n+    static void oop_store_not_in_heap(zpointer* p, oop value);\n+    static void oop_store_not_in_heap(oop* p, oop value)       { oop_store_not_in_heap((zpointer*)p, value); }\n+    static void oop_store_not_in_heap(narrowOop* p, oop value) { unsupported(); }\n+    static void oop_store_not_in_heap_at(oop base, ptrdiff_t offset, oop value);\n+\n+    static oop oop_atomic_cmpxchg_in_heap(zpointer* p, oop compare_value, oop new_value);\n+    static oop oop_atomic_cmpxchg_in_heap(oop* p, oop compare_value, oop new_value)       { return oop_atomic_cmpxchg_in_heap((zpointer*)p, compare_value, new_value); }\n+    static oop oop_atomic_cmpxchg_in_heap(narrowOop* p, oop compare_value, oop new_value) { unsupported(); return nullptr; }\n@@ -76,2 +121,3 @@\n-    template <typename T>\n-    static oop oop_atomic_xchg_in_heap(T* addr, oop new_value);\n+    static oop oop_atomic_xchg_in_heap(zpointer* p, oop new_value);\n+    static oop oop_atomic_xchg_in_heap(oop* p, oop new_value)       { return oop_atomic_xchg_in_heap((zpointer*)p, new_value); }\n+    static oop oop_atomic_xchg_in_heap(narrowOop* p, oop new_value) { unsupported(); return nullptr; }\n@@ -80,3 +126,2 @@\n-    template <typename T>\n-    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n-                                      arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, zpointer* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, zpointer* dst_raw,\n@@ -84,0 +129,10 @@\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, oop* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, oop* dst_raw,\n+                                      size_t length) {\n+      oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, (zpointer*)src_raw,\n+                            dst_obj, dst_offset_in_bytes, (zpointer*)dst_raw,\n+                            length);\n+    }\n+    static void oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, narrowOop* src_raw,\n+                                      arrayOop dst_obj, size_t dst_offset_in_bytes, narrowOop* dst_raw,\n+                                      size_t length) { unsupported(); }\n@@ -92,8 +147,13 @@\n-    template <typename T>\n-    static oop oop_load_not_in_heap(T* addr);\n-\n-    template <typename T>\n-    static oop oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value);\n-\n-    template <typename T>\n-    static oop oop_atomic_xchg_not_in_heap(T* addr, oop new_value);\n+    static oop oop_load_not_in_heap(zpointer* p);\n+    static oop oop_load_not_in_heap(oop* p);\n+    static oop oop_load_not_in_heap(narrowOop* p) { unsupported(); return nullptr; }\n+\n+    static oop oop_atomic_cmpxchg_not_in_heap(zpointer* p, oop compare_value, oop new_value);\n+    static oop oop_atomic_cmpxchg_not_in_heap(oop* p, oop compare_value, oop new_value) {\n+      return oop_atomic_cmpxchg_not_in_heap((zpointer*)p, compare_value, new_value);\n+    }\n+    static oop oop_atomic_cmpxchg_not_in_heap(narrowOop* addr, oop compare_value, oop new_value) { unsupported(); return nullptr; }\n+\n+    static oop oop_atomic_xchg_not_in_heap(zpointer* p, oop new_value);\n+    static oop oop_atomic_xchg_not_in_heap(oop* p, oop new_value)       { return oop_atomic_xchg_not_in_heap((zpointer*)p, new_value); }\n+    static oop oop_atomic_xchg_not_in_heap(narrowOop* p, oop new_value) { unsupported(); return nullptr; }\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.hpp","additions":83,"deletions":23,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"gc\/z\/zAddress.inline.hpp\"\n@@ -31,0 +32,3 @@\n+#include \"gc\/z\/zIterator.inline.hpp\"\n+#include \"gc\/z\/zNMethod.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -51,3 +55,2 @@\n-inline oop* ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::field_addr(oop base, ptrdiff_t offset) {\n-  assert(base != NULL, \"Invalid base\");\n-  return reinterpret_cast<oop*>(reinterpret_cast<intptr_t>((void*)base) + offset);\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::unsupported() {\n+  ShouldNotReachHere();\n@@ -57,2 +60,7 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_oop_field_preloaded(T* addr, oop o) {\n+inline zpointer* ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::field_addr(oop base, ptrdiff_t offset) {\n+  assert(base != nullptr, \"Invalid base\");\n+  return reinterpret_cast<zpointer*>(reinterpret_cast<intptr_t>((void*)base) + offset);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline zaddress ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier(zpointer* p, zpointer o) {\n@@ -63,1 +71,2 @@\n-      return ZBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n+      \/\/ Load barriers on strong oop refs don't keep objects alive\n+      return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n@@ -65,1 +74,1 @@\n-      return ZBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n+      return ZBarrier::no_keep_alive_load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -68,1 +77,1 @@\n-      return ZBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+      return ZBarrier::no_keep_alive_load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -72,1 +81,1 @@\n-      return ZBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n@@ -74,1 +83,1 @@\n-      return ZBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -77,1 +86,1 @@\n-      return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -83,2 +92,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_unknown_oop_field_preloaded(oop base, ptrdiff_t offset, T* addr, oop o) {\n+inline zaddress ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::load_barrier_on_unknown_oop_ref(oop base, ptrdiff_t offset, zpointer* p, zpointer o) {\n@@ -92,1 +100,2 @@\n-      return ZBarrier::weak_load_barrier_on_oop_field_preloaded(addr, o);\n+      \/\/ Load barriers on strong oop refs don't keep objects alive\n+      return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n@@ -94,1 +103,1 @@\n-      return ZBarrier::weak_load_barrier_on_weak_oop_field_preloaded(addr, o);\n+      return ZBarrier::no_keep_alive_load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -97,1 +106,1 @@\n-      return ZBarrier::weak_load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+      return ZBarrier::no_keep_alive_load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -101,1 +110,1 @@\n-      return ZBarrier::load_barrier_on_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_oop_field_preloaded(p, o);\n@@ -103,1 +112,1 @@\n-      return ZBarrier::load_barrier_on_weak_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_weak_oop_field_preloaded(p, o);\n@@ -106,1 +115,1 @@\n-      return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(addr, o);\n+      return ZBarrier::load_barrier_on_phantom_oop_field_preloaded(p, o);\n@@ -111,0 +120,46 @@\n+inline zpointer ZBarrierSet::store_good(oop obj) {\n+  assert(ZPointerStoreGoodMask != 0, \"sanity\");\n+\n+  const zaddress addr = to_zaddress(obj);\n+  return ZAddress::store_good(addr);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::store_barrier_heap_with_healing(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::store_barrier_on_heap_oop_field(p, true \/* heal *\/);\n+  } else {\n+    assert(false, \"Should not be used on uninitialized memory\");\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::store_barrier_heap_without_healing(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::store_barrier_on_heap_oop_field(p, false \/* heal *\/);\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::no_keep_alive_store_barrier_heap(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::no_keep_alive_store_barrier_on_heap_oop_field(p);\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::store_barrier_native_with_healing(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::store_barrier_on_native_oop_field(p, true \/* heal *\/);\n+  } else {\n+    assert(false, \"Should not be used on uninitialized memory\");\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::store_barrier_native_without_healing(zpointer* p) {\n+  if (!HasDecorator<decorators, IS_DEST_UNINITIALIZED>::value) {\n+    ZBarrier::store_barrier_on_native_oop_field(p, false \/* heal *\/);\n+  }\n+}\n+\n@@ -115,2 +170,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap(T* addr) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_in_heap(zpointer* p) {\n@@ -119,2 +173,4 @@\n-  const oop o = Raw::oop_load_in_heap(addr);\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n+  const zpointer o = Raw::load_in_heap(p);\n+  assert_is_valid(o);\n+\n+  return to_oop(load_barrier(p, o));\n@@ -125,2 +181,4 @@\n-  oop* const addr = field_addr(base, offset);\n-  const oop o = Raw::oop_load_in_heap(addr);\n+  zpointer* const p = field_addr(base, offset);\n+\n+  const zpointer o = Raw::load_in_heap(p);\n+  assert_is_valid(o);\n@@ -129,1 +187,1 @@\n-    return load_barrier_on_unknown_oop_field_preloaded(base, offset, addr, o);\n+    return to_oop(load_barrier_on_unknown_oop_ref(base, offset, p, o));\n@@ -132,1 +190,62 @@\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n+  return to_oop(load_barrier(p, o));\n+}\n+\n+template <DecoratorSet decorators>\n+bool is_store_barrier_no_keep_alive() {\n+  if (HasDecorator<decorators, ON_STRONG_OOP_REF>::value) {\n+    return HasDecorator<decorators, AS_NO_KEEPALIVE>::value;\n+  }\n+\n+  if (HasDecorator<decorators, ON_WEAK_OOP_REF>::value) {\n+    return true;\n+  }\n+\n+  assert((decorators & ON_PHANTOM_OOP_REF) != 0, \"Must be\");\n+  return true;\n+}\n+\n+template <DecoratorSet decorators>\n+inline bool is_store_barrier_no_keep_alive(oop base, ptrdiff_t offset) {\n+  if (!HasDecorator<decorators, ON_UNKNOWN_OOP_REF>::value) {\n+    return is_store_barrier_no_keep_alive<decorators>();\n+  }\n+\n+  const DecoratorSet decorators_known_strength =\n+      AccessBarrierSupport::resolve_possibly_unknown_oop_ref_strength<decorators>(base, offset);\n+\n+  if ((decorators_known_strength & ON_STRONG_OOP_REF) != 0) {\n+    return (decorators & AS_NO_KEEPALIVE) != 0;\n+  }\n+\n+  if ((decorators_known_strength & ON_WEAK_OOP_REF) != 0) {\n+    return true;\n+  }\n+\n+  assert((decorators_known_strength & ON_PHANTOM_OOP_REF) != 0, \"Must be\");\n+  return true;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_store_in_heap(zpointer* p, oop value) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  if (is_store_barrier_no_keep_alive<decorators>()) {\n+    no_keep_alive_store_barrier_heap(p);\n+  } else {\n+    store_barrier_heap_without_healing(p);\n+  }\n+\n+  Raw::store_in_heap(p, store_good(value));\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_store_in_heap_at(oop base, ptrdiff_t offset, oop value) {\n+  zpointer* const p = field_addr(base, offset);\n+\n+  if (is_store_barrier_no_keep_alive<decorators>(base, offset)) {\n+    no_keep_alive_store_barrier_heap(p);\n+  } else {\n+    store_barrier_heap_without_healing(p);\n+  }\n+\n+  Raw::store_in_heap(p, store_good(value));\n@@ -136,2 +255,12 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap(T* addr, oop compare_value, oop new_value) {\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_store_not_in_heap(zpointer* p, oop value) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  if (!is_store_barrier_no_keep_alive<decorators>()) {\n+    store_barrier_native_without_healing(p);\n+  }\n+\n+  Raw::store(p, store_good(value));\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_in_heap(zpointer* p, oop compare_value, oop new_value) {\n@@ -141,2 +270,6 @@\n-  ZBarrier::load_barrier_on_oop_field(addr);\n-  return Raw::oop_atomic_cmpxchg_in_heap(addr, compare_value, new_value);\n+  store_barrier_heap_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_cmpxchg_in_heap(p, store_good(compare_value), store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -154,2 +287,8 @@\n-  ZBarrier::load_barrier_on_oop_field(field_addr(base, offset));\n-  return Raw::oop_atomic_cmpxchg_in_heap_at(base, offset, compare_value, new_value);\n+  zpointer* const p = field_addr(base, offset);\n+\n+  store_barrier_heap_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_cmpxchg_in_heap(p, store_good(compare_value), store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -159,2 +298,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap(T* addr, oop new_value) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_in_heap(zpointer* p, oop new_value) {\n@@ -164,2 +302,6 @@\n-  const oop o = Raw::oop_atomic_xchg_in_heap(addr, new_value);\n-  return ZBarrier::load_barrier_on_oop(o);\n+  store_barrier_heap_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_xchg_in_heap(p, store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -173,2 +315,8 @@\n-  const oop o = Raw::oop_atomic_xchg_in_heap_at(base, offset, new_value);\n-  return ZBarrier::load_barrier_on_oop(o);\n+  zpointer* const p = field_addr(base, offset);\n+\n+  store_barrier_heap_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_xchg_in_heap(p, store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -178,13 +326,25 @@\n-template <typename T>\n-inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, T* src_raw,\n-                                                                                       arrayOop dst_obj, size_t dst_offset_in_bytes, T* dst_raw,\n-                                                                                       size_t length) {\n-  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n-  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n-\n-  if ((!HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) &&\n-      (!HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value)) {\n-    \/\/ No check cast, bulk barrier and bulk copy\n-    ZBarrier::load_barrier_on_oop_array(src, length);\n-    Raw::oop_arraycopy_in_heap(NULL, 0, src, NULL, 0, dst, length);\n-    return;\n+inline zaddress ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one_barriers(zpointer* dst, zpointer* src) {\n+  store_barrier_heap_without_healing(dst);\n+\n+  return ZBarrier::load_barrier_on_oop_field(src);\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline ZBarrierSet::OopCopyCheckStatus ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one(zpointer* dst, zpointer* src) {\n+  const zaddress obj = oop_copy_one_barriers(dst, src);\n+\n+  if (HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value && is_null(obj)) {\n+    return oop_copy_check_null;\n+  }\n+\n+  Atomic::store(dst, ZAddress::store_good(obj));\n+  return oop_copy_check_ok;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline ZBarrierSet::OopCopyCheckStatus ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_copy_one_check_cast(zpointer* dst, zpointer* src, Klass* dst_klass) {\n+  const zaddress obj = oop_copy_one_barriers(dst, src);\n+  const bool null_check = HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value;\n+\n+  if (null_check && is_null(obj)) {\n+    return oop_copy_check_null;\n@@ -192,0 +352,9 @@\n+  else if (!oopDesc::is_instanceof_or_null(to_oop(obj), dst_klass)) {\n+    \/\/ Check cast failed\n+    return oop_copy_check_class_cast;\n+  }\n+\n+  Atomic::store(dst, ZAddress::store_good(obj));\n+\n+  return oop_copy_check_ok;\n+}\n@@ -193,0 +362,3 @@\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline ZBarrierSet::OopCopyCheckStatus ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap_check_cast(zpointer* dst, zpointer* src, size_t length, Klass* dst_klass) {\n@@ -194,6 +366,14 @@\n-  Klass* const dst_klass = objArrayOop(dst_obj)->element_klass();\n-  for (const T* const end = src + length; src < end; src++, dst++) {\n-    const oop elem = ZBarrier::load_barrier_on_oop_field(src);\n-    if (HasDecorator<decorators, ARRAYCOPY_NOTNULL>::value && elem == NULL) {\n-      throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n-      return;\n+  OopCopyCheckStatus check_status = oop_copy_check_ok;\n+  for (const zpointer* const end = src + length; (check_status = oop_copy_check_ok) && (src < end); src++, dst++) {\n+    check_status = oop_copy_one_check_cast(dst, src, dst_klass);\n+  }\n+  return check_status;\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline ZBarrierSet::OopCopyCheckStatus ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap_no_check_cast(zpointer* dst, zpointer* src, size_t length) {\n+  const bool is_disjoint = HasDecorator<decorators, ARRAYCOPY_DISJOINT>::value;\n+  OopCopyCheckStatus check_status = oop_copy_check_ok;\n+  if (is_disjoint || src > dst) {\n+    for (const zpointer* const end = src + length; (check_status == oop_copy_check_ok) && (src < end); src++, dst++) {\n+      check_status = oop_copy_one(dst, src);\n@@ -201,5 +381,9 @@\n-    if (HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value &&\n-        (!oopDesc::is_instanceof_or_null(elem, dst_klass))) {\n-      \/\/ Check cast failed\n-      throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n-      return;\n+    return check_status;\n+  }\n+\n+  if (src < dst) {\n+    const zpointer* const end = src;\n+    src += length - 1;\n+    dst += length - 1;\n+    for ( ; (check_status == oop_copy_check_ok) && (src >= end); src--, dst--) {\n+      check_status = oop_copy_one(dst, src);\n@@ -207,0 +391,6 @@\n+    return check_status;\n+  }\n+\n+  \/\/ src and dst are the same; nothing to do\n+  return check_status;\n+}\n@@ -208,2 +398,27 @@\n-    \/\/ Cast is safe, since we know it's never a narrowOop\n-    *(oop*)dst = elem;\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline void ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_arraycopy_in_heap(arrayOop src_obj, size_t src_offset_in_bytes, zpointer* src_raw,\n+                                                                                       arrayOop dst_obj, size_t dst_offset_in_bytes, zpointer* dst_raw,\n+                                                                                       size_t length) {\n+  zpointer* const src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  zpointer* const dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+  OopCopyCheckStatus check_status;\n+\n+  if (HasDecorator<decorators, ARRAYCOPY_CHECKCAST>::value) {\n+    Klass* const dst_klass = objArrayOop(dst_obj)->element_klass();\n+    check_status = oop_arraycopy_in_heap_check_cast(dst, src, length, dst_klass);\n+  } else {\n+    check_status = oop_arraycopy_in_heap_no_check_cast(dst, src, length);\n+  }\n+\n+  switch (check_status) {\n+  case oop_copy_check_ok:\n+    return;\n+  case oop_copy_check_class_cast:\n+    throw_array_store_exception(src_obj, dst_obj, JavaThread::current());\n+    break;\n+  case oop_copy_check_null:\n+    throw_array_null_pointer_store_exception(src_obj, dst_obj, JavaThread::current());\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+    return;\n@@ -213,0 +428,26 @@\n+class ZStoreBarrierOopClosure : public BasicOopIterateClosure {\n+public:\n+  virtual void do_oop(oop* p_) {\n+    volatile zpointer* const p = (volatile zpointer*)p_;\n+    const zpointer ptr = ZBarrier::load_atomic(p);\n+    const zaddress addr = ZPointer::uncolor(ptr);\n+    ZBarrier::store_barrier_on_heap_oop_field(p, false \/* heal *\/);\n+    *p = ZAddress::store_good(addr);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n+class ZLoadBarrierOopClosure : public BasicOopIterateClosure {\n+public:\n+  virtual void do_oop(oop* p) {\n+    ZBarrier::load_barrier_on_oop_field((zpointer*)p);\n+  }\n+\n+  virtual void do_oop(narrowOop* p) {\n+    ShouldNotReachHere();\n+  }\n+};\n+\n@@ -215,1 +456,7 @@\n-  ZBarrier::load_barrier_on_oop_fields(src);\n+  assert_is_valid(to_zaddress(src));\n+\n+  \/\/ Fix the oops\n+  ZLoadBarrierOopClosure cl;\n+  ZIterator::oop_iterate(src, &cl);\n+\n+  \/\/ Clone the object\n@@ -217,0 +464,6 @@\n+\n+  assert(ZHeap::heap()->is_young(to_zaddress(dst)), \"ZColorStoreGoodOopClosure is only valid for young objects\");\n+\n+  \/\/ Color store good before handing out\n+  ZStoreBarrierOopClosure cl_sg;\n+  ZIterator::oop_iterate(dst, &cl_sg);\n@@ -226,1 +479,1 @@\n-    OopMapBlock* const end = map + md->nonstatic_oop_map_count();\n+    const OopMapBlock* const end = map + md->nonstatic_oop_map_count();\n@@ -228,2 +481,5 @@\n-      address soop_address = src_oop_addr_offset + map->offset();\n-      ZBarrier::load_barrier_on_oop_array((oop*) soop_address, map->count());\n+      const address soop_address = src_oop_addr_offset + map->offset();\n+      zpointer *p = (zpointer*) soop_address;\n+      for (const zpointer* const end = p + map->count(); p < end; p++) {\n+        ZBarrier::load_barrier_on_oop_field(p);\n+      }\n@@ -240,2 +496,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(T* addr) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(zpointer* p) {\n@@ -244,2 +499,3 @@\n-  const oop o = Raw::oop_load_not_in_heap(addr);\n-  return load_barrier_on_oop_field_preloaded(addr, o);\n+  const zpointer o = Raw::template load<zpointer>(p);\n+  assert_is_valid(o);\n+  return to_oop(load_barrier(p, o));\n@@ -249,2 +505,12 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_not_in_heap(T* addr, oop compare_value, oop new_value) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_load_not_in_heap(oop* p) {\n+  verify_decorators_absent<ON_UNKNOWN_OOP_REF>();\n+\n+  if (HasDecorator<decorators, IN_NMETHOD>::value) {\n+    return ZNMethod::load_oop(p, decorators);\n+  } else {\n+    return oop_load_not_in_heap((zpointer*)p);\n+  }\n+}\n+\n+template <DecoratorSet decorators, typename BarrierSetT>\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_cmpxchg_not_in_heap(zpointer* p, oop compare_value, oop new_value) {\n@@ -254,1 +520,6 @@\n-  return Raw::oop_atomic_cmpxchg_not_in_heap(addr, compare_value, new_value);\n+  store_barrier_native_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_cmpxchg(p, store_good(compare_value), store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n@@ -258,2 +529,1 @@\n-template <typename T>\n-inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_not_in_heap(T* addr, oop new_value) {\n+inline oop ZBarrierSet::AccessBarrier<decorators, BarrierSetT>::oop_atomic_xchg_not_in_heap(zpointer* p, oop new_value) {\n@@ -263,1 +533,6 @@\n-  return Raw::oop_atomic_xchg_not_in_heap(addr, new_value);\n+  store_barrier_native_with_healing(p);\n+\n+  const zpointer o = Raw::atomic_xchg(p, store_good(new_value));\n+  assert_is_valid(o);\n+\n+  return to_oop(ZPointer::uncolor_store_good(o));\n","filename":"src\/hotspot\/share\/gc\/z\/zBarrierSet.inline.hpp","additions":355,"deletions":80,"binary":false,"changes":435,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,2 +32,2 @@\n-ZObjArrayAllocator::ZObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread) :\n-    ObjArrayAllocator(klass, word_size, length, do_zero, thread) {}\n+ZObjArrayAllocator::ZObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread)\n+  : ObjArrayAllocator(klass, word_size, length, do_zero, thread) {}\n@@ -66,1 +66,4 @@\n-  arrayOopDesc::set_mark(mem, Klass::default_prototype_header(_klass));\n+\n+  \/\/ Signal to the ZIterator that this is an invisible root, by setting\n+  \/\/ the mark word to \"marked\". Reset to prototype() after the clearing.\n+  arrayOopDesc::set_mark(mem, Klass::default_prototype_header(_klass).set_marked());\n@@ -72,1 +75,1 @@\n-  \/\/ root. Invisible roots are not visited by the heap itarator\n+  \/\/ root. Invisible roots are not visited by the heap iterator\n@@ -74,2 +77,14 @@\n-  \/\/ Relocation knows how to dodge iterating over such objects.\n-  ZThreadLocalData::set_invisible_root(_thread, (oop*)&mem);\n+  \/\/ Relocation and remembered set code know how to dodge iterating\n+  \/\/ over such objects.\n+  ZThreadLocalData::set_invisible_root(_thread, (zaddress_unsafe*)&mem);\n+\n+  uint32_t old_seqnum_before = ZGeneration::old()->seqnum();\n+  uint32_t young_seqnum_before = ZGeneration::young()->seqnum();\n+  uintptr_t color_before = ZPointerStoreGoodMask;\n+  auto gc_safepoint_happened = [&]() {\n+    return old_seqnum_before != ZGeneration::old()->seqnum() ||\n+           young_seqnum_before != ZGeneration::young()->seqnum() ||\n+           color_before != ZPointerStoreGoodMask;\n+  };\n+\n+  bool seen_gc_safepoint = false;\n@@ -77,5 +92,22 @@\n-  for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n-    \/\/ Calculate segment\n-    HeapWord* const start = (HeapWord*)(mem + header + processed);\n-    const size_t remaining = payload_size - processed;\n-    const size_t segment_size = MIN2(remaining, segment_max);\n+  auto initialize_memory = [&]() {\n+    for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n+      \/\/ Clear segment\n+      uintptr_t* const start = (uintptr_t*)(mem + header + processed);\n+      const size_t remaining = payload_size - processed;\n+      const size_t segment = MIN2(remaining, segment_max);\n+      \/\/ Usually, the young marking code has the responsibility to color\n+      \/\/ raw nulls, before they end up in the old generation. However, the\n+      \/\/ invisible roots are hidden from the marking code, and therefore\n+      \/\/ we must color the nulls already here in the initialization. The\n+      \/\/ color we choose must be store bad for any subsequent stores, regardless\n+      \/\/ of how many GC flips later it will arrive. That's why we OR in 11\n+      \/\/ (ZPointerRememberedMask) in the remembered bits, similar to how\n+      \/\/ forgotten old oops also have 11, for the very same reason.\n+      \/\/ However, we opportunistically try to color without the 11 remembered\n+      \/\/ bits, hoping to not get interrupted in the middle of a GC safepoint.\n+      \/\/ Most of the time, we manage to do that, and can the avoid having GC\n+      \/\/ barriers trigger slow paths for this.\n+      const uintptr_t colored_null = seen_gc_safepoint ? (ZPointerStoreGoodMask | ZPointerRememberedMask)\n+                                                       : ZPointerStoreGoodMask;\n+      const uintptr_t fill_value = is_reference_type(element_type) ? colored_null : 0;\n+      ZUtils::fill(start, segment, fill_value);\n@@ -83,2 +115,2 @@\n-    \/\/ Clear segment\n-    Copy::zero_to_words(start, segment_size);\n+      \/\/ Safepoint\n+      yield_for_safepoint();\n@@ -86,2 +118,15 @@\n-    \/\/ Safepoint\n-    yield_for_safepoint();\n+      \/\/ Deal with safepoints\n+      if (!seen_gc_safepoint && gc_safepoint_happened()) {\n+        \/\/ The first time we observe a GC safepoint in the yield point,\n+        \/\/ we have to restart processing with 11 remembered bits.\n+        seen_gc_safepoint = true;\n+        return false;\n+      }\n+    }\n+    return true;\n+  };\n+\n+  if (!initialize_memory()) {\n+    \/\/ Re-color with 11 remset bits if we got intercepted by a GC safepoint\n+    const bool result = initialize_memory();\n+    assert(result, \"Array initialization should always succeed the second time\");\n@@ -92,0 +137,3 @@\n+  \/\/ Signal to the ZIterator that this is no longer an invisible root\n+  oopDesc::release_set_mark(mem, markWord::prototype());\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":64,"deletions":16,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -180,0 +180,3 @@\n+JNIEXPORT jboolean JNICALL\n+JVM_IsForeignLinkerSupported(void);\n+\n@@ -279,1 +282,1 @@\n-JVM_Sleep(JNIEnv *env, jclass threadClass, jlong millis);\n+JVM_Sleep(JNIEnv *env, jclass threadClass, jlong nanos);\n@@ -1150,1 +1153,7 @@\n-JVM_VirtualThreadMount(JNIEnv* env, jobject vthread, jboolean hide, jboolean first_mount);\n+JVM_VirtualThreadStart(JNIEnv* env, jobject vthread);\n+\n+JNIEXPORT void JNICALL\n+JVM_VirtualThreadEnd(JNIEnv* env, jobject vthread);\n+\n+JNIEXPORT void JNICALL\n+JVM_VirtualThreadMount(JNIEnv* env, jobject vthread, jboolean hide);\n@@ -1153,1 +1162,1 @@\n-JVM_VirtualThreadUnmount(JNIEnv* env, jobject vthread, jboolean hide, jboolean last_unmount);\n+JVM_VirtualThreadUnmount(JNIEnv* env, jobject vthread, jboolean hide);\n","filename":"src\/hotspot\/share\/include\/jvm.h","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -274,1 +274,1 @@\n-static void print_method_name(outputStream *os, Method* method, int cp_index) {\n+static void print_method_name(outputStream *os, Method* method, int cp_index, Bytecodes::Code bc) {\n@@ -277,3 +277,3 @@\n-  Symbol* klass     = cp->klass_ref_at_noresolve(cp_index);\n-  Symbol* name      = cp->name_ref_at(cp_index);\n-  Symbol* signature = cp->signature_ref_at(cp_index);\n+  Symbol* klass     = cp->klass_ref_at_noresolve(cp_index, bc);\n+  Symbol* name      = cp->name_ref_at(cp_index, bc);\n+  Symbol* signature = cp->signature_ref_at(cp_index, bc);\n@@ -290,1 +290,1 @@\n-static void print_field_and_class(outputStream *os, Method* method, int cp_index) {\n+static void print_field_and_class(outputStream *os, Method* method, int cp_index, Bytecodes::Code bc) {\n@@ -293,2 +293,2 @@\n-  Symbol* klass    = cp->klass_ref_at_noresolve(cp_index);\n-  Symbol *name     = cp->name_ref_at(cp_index);\n+  Symbol* klass    = cp->klass_ref_at_noresolve(cp_index, bc);\n+  Symbol *name     = cp->name_ref_at(cp_index, bc);\n@@ -301,2 +301,2 @@\n-static char const* get_field_name(Method* method, int cp_index) {\n-  Symbol* name = method->constants()->name_ref_at(cp_index);\n+static char const* get_field_name(Method* method, int cp_index, Bytecodes::Code bc) {\n+  Symbol* name = method->constants()->name_ref_at(cp_index, bc);\n@@ -479,1 +479,1 @@\n-  if (const_method->has_exception_handler()) {\n+  if (const_method->has_exception_table()) {\n@@ -973,1 +973,1 @@\n-      int name_and_type_index = cp->name_and_type_ref_index_at(cp_index);\n+      int name_and_type_index = cp->name_and_type_ref_index_at(cp_index, code);\n@@ -988,1 +988,1 @@\n-      int name_and_type_index = cp->name_and_type_ref_index_at(cp_index);\n+      int name_and_type_index = cp->name_and_type_ref_index_at(cp_index, code);\n@@ -1010,1 +1010,1 @@\n-      int name_and_type_index = cp->name_and_type_ref_index_at(cp_index);\n+      int name_and_type_index = cp->name_and_type_ref_index_at(cp_index, code);\n@@ -1140,1 +1140,1 @@\n-        int name_and_type_index = cp->name_and_type_ref_index_at(cp_index);\n+        int name_and_type_index = cp->name_and_type_ref_index_at(cp_index, code);\n@@ -1151,1 +1151,1 @@\n-        int name_and_type_index = cp->name_and_type_ref_index_at(cp_index);\n+        int name_and_type_index = cp->name_and_type_ref_index_at(cp_index, code);\n@@ -1339,1 +1339,1 @@\n-      print_field_and_class(os, _method, cp_index);\n+      print_field_and_class(os, _method, cp_index, code);\n@@ -1350,1 +1350,1 @@\n-      os->print(\"%s\", get_field_name(_method, cp_index));\n+      os->print(\"%s\", get_field_name(_method, cp_index, code));\n@@ -1362,1 +1362,1 @@\n-      print_method_name(os, _method, cp_index);\n+      print_method_name(os, _method, cp_index, code);\n@@ -1428,1 +1428,1 @@\n-        int name_and_type_index = cp->name_and_type_ref_index_at(cp_index);\n+        int name_and_type_index = cp->name_and_type_ref_index_at(cp_index, code);\n@@ -1436,1 +1436,1 @@\n-        os->print(\"Cannot assign field \\\"%s\\\"\", get_field_name(_method, cp_index));\n+        os->print(\"Cannot assign field \\\"%s\\\"\", get_field_name(_method, cp_index, code));\n@@ -1443,1 +1443,1 @@\n-        print_method_name(os, _method, cp_index);\n+        print_method_name(os, _method, cp_index, code);\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeUtils.cpp","additions":21,"deletions":21,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -1001,0 +1001,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"Should call monitorenter_obj() when using the new lightweight locking\");\n@@ -1015,0 +1016,16 @@\n+\/\/ NOTE: We provide a separate implementation for the new lightweight locking to workaround a limitation\n+\/\/ of registers in x86_32. This entry point accepts an oop instead of a BasicObjectLock*.\n+\/\/ The problem is that we would need to preserve the register that holds the BasicObjectLock,\n+\/\/ but we are using that register to hold the thread. We don't have enough registers to\n+\/\/ also keep the BasicObjectLock, but we don't really need it anyway, we only need\n+\/\/ the object. See also InterpreterMacroAssembler::lock_object().\n+\/\/ As soon as legacy stack-locking goes away we could remove the other monitorenter() entry\n+\/\/ point, and only use oop-accepting entries (same for monitorexit() below).\n+JRT_ENTRY_NO_ASYNC(void, InterpreterRuntime::monitorenter_obj(JavaThread* current, oopDesc* obj))\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"Should call monitorenter() when not using the new lightweight locking\");\n+  Handle h_obj(current, cast_to_oop(obj));\n+  assert(Universe::heap()->is_in_or_null(h_obj()),\n+         \"must be null or an object\");\n+  ObjectSynchronizer::enter(h_obj, nullptr, current);\n+  return;\n+JRT_END\n@@ -1763,2 +1780,2 @@\n-  Symbol* cname = cpool->klass_name_at(cpool->klass_ref_index_at(cp_index));\n-  Symbol* mname = cpool->name_ref_at(cp_index);\n+  Symbol* cname = cpool->klass_name_at(cpool->klass_ref_index_at(cp_index, code));\n+  Symbol* mname = cpool->name_ref_at(cp_index, code);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":19,"deletions":2,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -117,0 +117,1 @@\n+  static void    monitorenter_obj(JavaThread* current, oopDesc* obj);\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-LinkInfo::LinkInfo(const constantPoolHandle& pool, int index, const methodHandle& current_method, TRAPS) {\n+LinkInfo::LinkInfo(const constantPoolHandle& pool, int index, const methodHandle& current_method, Bytecodes::Code code, TRAPS) {\n@@ -233,1 +233,1 @@\n-  _resolved_klass = pool->klass_ref_at(index, CHECK);\n+  _resolved_klass = pool->klass_ref_at(index, code, CHECK);\n@@ -236,3 +236,3 @@\n-  _name          = pool->name_ref_at(index);\n-  _signature     = pool->signature_ref_at(index);\n-  _tag           = pool->tag_ref_at(index);\n+  _name          = pool->name_ref_at(index, code);\n+  _signature     = pool->signature_ref_at(index, code);\n+  _tag           = pool->tag_ref_at(index, code);\n@@ -247,1 +247,1 @@\n-LinkInfo::LinkInfo(const constantPoolHandle& pool, int index, TRAPS) {\n+LinkInfo::LinkInfo(const constantPoolHandle& pool, int index, Bytecodes::Code code, TRAPS) {\n@@ -249,1 +249,1 @@\n-  _resolved_klass = pool->klass_ref_at(index, CHECK);\n+  _resolved_klass = pool->klass_ref_at(index, code, CHECK);\n@@ -252,3 +252,3 @@\n-  _name          = pool->name_ref_at(index);\n-  _signature     = pool->signature_ref_at(index);\n-  _tag           = pool->tag_ref_at(index);\n+  _name          = pool->name_ref_at(index, code);\n+  _signature     = pool->signature_ref_at(index, code);\n+  _tag           = pool->tag_ref_at(index, code);\n@@ -621,1 +621,1 @@\n-    Symbol* method_signature = pool->signature_ref_at(index);\n+    Symbol* method_signature = pool->signature_ref_at(index, code);\n@@ -627,1 +627,1 @@\n-  LinkInfo link_info(pool, index, methodHandle(), CHECK_NULL);\n+  LinkInfo link_info(pool, index, methodHandle(), code, CHECK_NULL);\n@@ -953,1 +953,1 @@\n-  LinkInfo link_info(pool, index, method, CHECK);\n+  LinkInfo link_info(pool, index, method, byte, CHECK);\n@@ -1702,1 +1702,1 @@\n-  LinkInfo link_info(pool, index, CHECK);\n+  LinkInfo link_info(pool, index, Bytecodes::_invokestatic, CHECK);\n@@ -1709,1 +1709,1 @@\n-  LinkInfo link_info(pool, index, CHECK);\n+  LinkInfo link_info(pool, index, Bytecodes::_invokespecial, CHECK);\n@@ -1718,1 +1718,1 @@\n-  LinkInfo link_info(pool, index, CHECK);\n+  LinkInfo link_info(pool, index, Bytecodes::_invokevirtual, CHECK);\n@@ -1725,1 +1725,1 @@\n-  LinkInfo link_info(pool, index, CHECK);\n+  LinkInfo link_info(pool, index, Bytecodes::_invokeinterface, CHECK);\n@@ -1746,1 +1746,1 @@\n-  LinkInfo link_info(pool, index, CHECK);\n+  LinkInfo link_info(pool, index, Bytecodes::_invokehandle, CHECK);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -149,2 +149,2 @@\n-  LinkInfo(const constantPoolHandle& pool, int index, const methodHandle& current_method, TRAPS);\n-  LinkInfo(const constantPoolHandle& pool, int index, TRAPS);\n+  LinkInfo(const constantPoolHandle& pool, int index, const methodHandle& current_method, Bytecodes::Code code, TRAPS);\n+  LinkInfo(const constantPoolHandle& pool, int index, Bytecodes::Code code, TRAPS);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -228,1 +228,1 @@\n-        if (_pool->klass_ref_at_noresolve(cp_index) == vmSymbols::java_lang_invoke_MethodHandle() &&\n+        if (_pool->uncached_klass_ref_at_noresolve(cp_index) == vmSymbols::java_lang_invoke_MethodHandle() &&\n@@ -230,1 +230,1 @@\n-                                                         _pool->name_ref_at(cp_index))) {\n+                                                         _pool->uncached_name_ref_at(cp_index))) {\n@@ -234,1 +234,1 @@\n-        } else if (_pool->klass_ref_at_noresolve(cp_index) == vmSymbols::java_lang_invoke_VarHandle() &&\n+        } else if (_pool->uncached_klass_ref_at_noresolve(cp_index) == vmSymbols::java_lang_invoke_VarHandle() &&\n@@ -236,1 +236,1 @@\n-                                                                _pool->name_ref_at(cp_index))) {\n+                                                                _pool->uncached_name_ref_at(cp_index))) {\n@@ -423,1 +423,1 @@\n-          Symbol* ref_class_name = cp->klass_name_at(cp->klass_ref_index_at(bc_index));\n+          Symbol* ref_class_name = cp->klass_name_at(cp->uncached_klass_ref_index_at(bc_index));\n@@ -426,2 +426,2 @@\n-            Symbol* field_name = cp->name_ref_at(bc_index);\n-            Symbol* field_sig = cp->signature_ref_at(bc_index);\n+            Symbol* field_name = cp->uncached_name_ref_at(bc_index);\n+            Symbol* field_sig = cp->uncached_signature_ref_at(bc_index);\n@@ -476,1 +476,1 @@\n-  \/\/ Update access flags\n+  \/\/ Update flags\n@@ -485,2 +485,0 @@\n-    \/\/ Second pass will revisit this method.\n-    assert(method->has_jsrs(), \"didn't we just set this?\");\n","filename":"src\/hotspot\/share\/interpreter\/rewriter.cpp","additions":8,"deletions":10,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -382,1 +383,1 @@\n-    oopDesc::verify(obj);\n+    guarantee(oopDesc::is_oop_or_null(obj), \"invalid oop: \" INTPTR_FORMAT, p2i((oopDesc*) obj));\n@@ -730,0 +731,8 @@\n+    \/\/ We would like to be strict about the nmethod entry barrier but there are various test\n+    \/\/ configurations which generate assembly without being a full compiler. So for now we enforce\n+    \/\/ that JIT compiled methods must have an nmethod barrier.\n+    bool install_default = JVMCIENV->get_HotSpotNmethod_isDefault(installed_code) != 0;\n+    if (_nmethod_entry_patch_offset == -1 && install_default) {\n+      JVMCI_THROW_MSG_(IllegalArgumentException, \"nmethod entry barrier is missing\", JVMCI::ok);\n+    }\n+\n@@ -754,1 +763,2 @@\n-                                        speculations_len);\n+                                        speculations_len,\n+                                        _nmethod_entry_patch_offset);\n@@ -763,0 +773,11 @@\n+\n+      if (nm != nullptr) {\n+        if (_nmethod_entry_patch_offset != -1) {\n+          err_msg msg(\"\");\n+          BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+\n+          if (!bs_nm->verify_barrier(nm, msg)) {\n+            JVMCI_THROW_MSG_(IllegalArgumentException, err_msg(\"nmethod entry barrier is malformed: %s\", msg.buffer()), JVMCI::ok);\n+          }\n+        }\n+      }\n@@ -807,0 +828,1 @@\n+  _has_monitors = false;\n@@ -808,0 +830,1 @@\n+  _nmethod_entry_patch_offset = -1;\n@@ -1262,0 +1285,3 @@\n+    case ENTRY_BARRIER_PATCH:\n+      _nmethod_entry_patch_offset = pc_offset;\n+      break;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":28,"deletions":2,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -42,0 +42,3 @@\n+  _ok_upcalls = 0;\n+  _err_upcalls = 0;\n+  _disabled = false;\n@@ -124,0 +127,3 @@\n+  if (_disabled) {\n+    return true;\n+  }\n@@ -217,0 +223,33 @@\n+void JVMCICompiler::on_upcall(const char* error, JVMCICompileState* compile_state) {\n+  if (error != nullptr) {\n+\n+    Atomic::inc(&_err_upcalls);\n+    int ok = _ok_upcalls;\n+    int err = _err_upcalls;\n+    \/\/ If there have been at least 10 upcalls with an error\n+    \/\/ and the number of error upcalls is 10% or more of the\n+    \/\/ number of non-error upcalls, disable JVMCI compilation.\n+    if (err > 10 && err * 10 > ok && !_disabled) {\n+      _disabled = true;\n+      int total = err + ok;\n+      const char* disable_msg = err_msg(\"JVMCI compiler disabled \"\n+      \"after %d of %d upcalls had errors (Last error: \\\"%s\\\"). \"\n+      \"Use -Xlog:jit+compilation for more detail.\", err, total, error);\n+      log_warning(jit,compilation)(\"%s\", disable_msg);\n+      if (compile_state != nullptr) {\n+        const char* disable_error = os::strdup(disable_msg);\n+        if (disable_error != nullptr) {\n+          compile_state->set_failure(true, disable_error, true);\n+          JVMCI_event_1(\"%s\", disable_error);\n+          return;\n+        } else {\n+          \/\/ Leave failure reason as set by caller when strdup fails\n+        }\n+      }\n+    }\n+    JVMCI_event_1(\"JVMCI upcall had an error: %s\", error);\n+  } else {\n+    Atomic::inc(&_ok_upcalls);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"interpreter\/linkResolver.hpp\"\n+#include \"interpreter\/linkResolver.hpp\"\n@@ -39,1 +39,1 @@\n-#include \"jvmci\/jvmciCompilerToVM.hpp\"\n+#include \"jvmci\/jvmciCompilerToVM.hpp\"\n@@ -47,1 +47,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.hpp\"\n@@ -50,0 +50,1 @@\n+#include \"oops\/objArrayKlass.inline.hpp\"\n@@ -64,1 +65,1 @@\n-#include \"runtime\/vframe_hp.hpp\"\n+#include \"runtime\/vframe_hp.hpp\"\n@@ -104,0 +105,48 @@\n+class JavaArgumentUnboxer : public SignatureIterator {\n+ protected:\n+  JavaCallArguments*  _jca;\n+  arrayOop _args;\n+  int _index;\n+\n+  Handle next_arg(BasicType expectedType);\n+\n+ public:\n+  JavaArgumentUnboxer(Symbol* signature,\n+                      JavaCallArguments* jca,\n+                      arrayOop args,\n+                      bool is_static)\n+    : SignatureIterator(signature)\n+  {\n+    this->_return_type = T_ILLEGAL;\n+    _jca = jca;\n+    _index = 0;\n+    _args = args;\n+    if (!is_static) {\n+      _jca->push_oop(next_arg(T_OBJECT));\n+    }\n+    do_parameters_on(this);\n+    assert(_index == args->length(), \"arg count mismatch with signature\");\n+  }\n+\n+ private:\n+  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n+  void do_type(BasicType type) {\n+    if (is_reference_type(type)) {\n+      (type == T_PRIMITIVE_OBJECT) ? _jca->push_oop(next_arg(T_PRIMITIVE_OBJECT)) : _jca->push_oop(next_arg(T_OBJECT));\n+      return;\n+    }\n+    Handle arg = next_arg(type);\n+    int box_offset = java_lang_boxing_object::value_offset(type);\n+    switch (type) {\n+    case T_BOOLEAN:     _jca->push_int(arg->bool_field(box_offset));    break;\n+    case T_CHAR:        _jca->push_int(arg->char_field(box_offset));    break;\n+    case T_SHORT:       _jca->push_int(arg->short_field(box_offset));   break;\n+    case T_BYTE:        _jca->push_int(arg->byte_field(box_offset));    break;\n+    case T_INT:         _jca->push_int(arg->int_field(box_offset));     break;\n+    case T_LONG:        _jca->push_long(arg->long_field(box_offset));   break;\n+    case T_FLOAT:       _jca->push_float(arg->float_field(box_offset));    break;\n+    case T_DOUBLE:      _jca->push_double(arg->double_field(box_offset));  break;\n+    default:            ShouldNotReachHere();\n+    }\n+  }\n+};\n@@ -386,3 +435,7 @@\n-  JVMCIKlassHandle klass(THREAD);\n-  jlong base_address = 0;\n-  if (base_object.is_non_null() && offset == oopDesc::klass_offset_in_bytes()) {\n+  if (base_object.is_null()) {\n+    JVMCI_THROW_MSG_NULL(NullPointerException, \"base object is null\");\n+  }\n+\n+  const char* base_desc = nullptr;\n+  JVMCIKlassHandle klass(THREAD);\n+  if (offset == oopDesc::klass_offset_in_bytes()) {\n@@ -394,1 +447,1 @@\n-      assert(false, \"What types are we actually expecting here?\");\n+      goto unexpected;\n@@ -397,11 +450,42 @@\n-    if (base_object.is_non_null()) {\n-      if (JVMCIENV->isa_HotSpotResolvedJavaMethodImpl(base_object)) {\n-        base_address = (intptr_t) JVMCIENV->asMethod(base_object);\n-      } else if (JVMCIENV->isa_HotSpotConstantPool(base_object)) {\n-        base_address = (intptr_t) JVMCIENV->asConstantPool(base_object);\n-      } else if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(base_object)) {\n-        base_address = (intptr_t) JVMCIENV->asKlass(base_object);\n-      } else if (JVMCIENV->isa_HotSpotObjectConstantImpl(base_object)) {\n-        Handle base_oop = JVMCIENV->asConstant(base_object, JVMCI_CHECK_NULL);\n-        if (base_oop->is_a(vmClasses::Class_klass())) {\n-          base_address = cast_from_oop<jlong>(base_oop());\n+    if (JVMCIENV->isa_HotSpotConstantPool(base_object)) {\n+      ConstantPool* cp = JVMCIENV->asConstantPool(base_object);\n+      if (offset == in_bytes(ConstantPool::pool_holder_offset())) {\n+        klass = cp->pool_holder();\n+      } else {\n+        base_desc = FormatBufferResource(\"[constant pool for %s]\", cp->pool_holder()->signature_name());\n+        goto unexpected;\n+      }\n+    } else if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(base_object)) {\n+      Klass* base_klass = JVMCIENV->asKlass(base_object);\n+      if (offset == in_bytes(Klass::subklass_offset())) {\n+        klass = base_klass->subklass();\n+      } else if (offset == in_bytes(Klass::super_offset())) {\n+        klass = base_klass->super();\n+      } else if (offset == in_bytes(Klass::next_sibling_offset())) {\n+        klass = base_klass->next_sibling();\n+      } else if (offset == in_bytes(ObjArrayKlass::element_klass_offset()) && base_klass->is_objArray_klass()) {\n+        klass = ObjArrayKlass::cast(base_klass)->element_klass();\n+      } else if (offset >= in_bytes(Klass::primary_supers_offset()) &&\n+                 offset < in_bytes(Klass::primary_supers_offset()) + (int) (sizeof(Klass*) * Klass::primary_super_limit()) &&\n+                 offset % sizeof(Klass*) == 0) {\n+        \/\/ Offset is within the primary supers array\n+        int index = (int) ((offset - in_bytes(Klass::primary_supers_offset())) \/ sizeof(Klass*));\n+        klass = base_klass->primary_super_of_depth(index);\n+      } else {\n+        base_desc = FormatBufferResource(\"[%s]\", base_klass->signature_name());\n+        goto unexpected;\n+      }\n+    } else if (JVMCIENV->isa_HotSpotObjectConstantImpl(base_object)) {\n+      Handle base_oop = JVMCIENV->asConstant(base_object, JVMCI_CHECK_NULL);\n+      if (base_oop->is_a(vmClasses::Class_klass())) {\n+        if (offset == java_lang_Class::klass_offset()) {\n+          klass = java_lang_Class::as_Klass(base_oop());\n+        } else if (offset == java_lang_Class::array_klass_offset()) {\n+          klass = java_lang_Class::array_klass_acquire(base_oop());\n+        } else {\n+          base_desc = FormatBufferResource(\"[Class=%s]\", java_lang_Class::as_Klass(base_oop())->signature_name());\n+          goto unexpected;\n+        }\n+      } else {\n+        if (!base_oop.is_null()) {\n+          base_desc = FormatBufferResource(\"[%s]\", base_oop()->klass()->signature_name());\n@@ -409,0 +493,1 @@\n+        goto unexpected;\n@@ -410,3 +495,6 @@\n-      if (base_address == 0) {\n-        JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                    err_msg(\"Unexpected arguments: %s \" JLONG_FORMAT \" %s\", JVMCIENV->klass_name(base_object), offset, compressed ? \"true\" : \"false\"));\n+    } else if (JVMCIENV->isa_HotSpotMethodData(base_object)) {\n+      jlong base_address = (intptr_t) JVMCIENV->asMethodData(base_object);\n+      klass = *((Klass**) (intptr_t) (base_address + offset));\n+      if (klass == nullptr || !klass->is_loader_alive()) {\n+        \/\/ Klasses in methodData might be concurrently unloading so return null in that case.\n+        return nullptr;\n@@ -414,0 +502,2 @@\n+    } else {\n+      goto unexpected;\n@@ -415,5 +505,1 @@\n-    klass = *((Klass**) (intptr_t) (base_address + offset));\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                err_msg(\"Unexpected arguments: %s \" JLONG_FORMAT \" %s\",\n-                        base_object.is_non_null() ? JVMCIENV->klass_name(base_object) : \"null\",\n-                        offset, compressed ? \"true\" : \"false\"));\n+    goto unexpected;\n@@ -422,3 +508,14 @@\n-  assert (klass == nullptr || klass->is_klass(), \"invalid read\");\n-  JVMCIObject result = JVMCIENV->get_jvmci_type(klass, JVMCI_CHECK_NULL);\n-  return JVMCIENV->get_jobject(result);\n+\n+  {\n+    if (klass == nullptr) {\n+      return nullptr;\n+    }\n+    JVMCIObject result = JVMCIENV->get_jvmci_type(klass, JVMCI_CHECK_NULL);\n+    return JVMCIENV->get_jobject(result);\n+  }\n+\n+unexpected:\n+  JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n+                       err_msg(\"Unexpected arguments: %s%s \" JLONG_FORMAT \" %s\",\n+                               JVMCIENV->klass_name(base_object), base_desc == nullptr ? \"\" : base_desc,\n+                               offset, compressed ? \"true\" : \"false\"));\n@@ -720,1 +817,1 @@\n-C2V_VMENTRY_0(jint, lookupNameAndTypeRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+C2V_VMENTRY_0(jint, lookupNameAndTypeRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jint opcode))\n@@ -722,1 +819,1 @@\n-  return cp->name_and_type_ref_index_at(index);\n+  return cp->name_and_type_ref_index_at(index, (Bytecodes::Code)opcode);\n@@ -725,1 +822,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupNameInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+C2V_VMENTRY_NULL(jobject, lookupNameInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which, jint opcode))\n@@ -727,1 +824,1 @@\n-  JVMCIObject sym = JVMCIENV->create_string(cp->name_ref_at(which), JVMCI_CHECK_NULL);\n+  JVMCIObject sym = JVMCIENV->create_string(cp->name_ref_at(which, (Bytecodes::Code)opcode), JVMCI_CHECK_NULL);\n@@ -731,1 +828,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupSignatureInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+C2V_VMENTRY_NULL(jobject, lookupSignatureInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which, jint opcode))\n@@ -733,1 +830,1 @@\n-  JVMCIObject sym = JVMCIENV->create_string(cp->signature_ref_at(which), JVMCI_CHECK_NULL);\n+  JVMCIObject sym = JVMCIENV->create_string(cp->signature_ref_at(which, (Bytecodes::Code)opcode), JVMCI_CHECK_NULL);\n@@ -737,1 +834,1 @@\n-C2V_VMENTRY_0(jint, lookupKlassRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+C2V_VMENTRY_0(jint, lookupKlassRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jint opcode))\n@@ -739,1 +836,1 @@\n-  return cp->klass_ref_index_at(index);\n+  return cp->klass_ref_index_at(index, (Bytecodes::Code)opcode);\n@@ -812,1 +909,1 @@\n-  LinkInfo link_info(cp, index, mh, CHECK_NULL);\n+  LinkInfo link_info(cp, index, mh, code, CHECK_NULL);\n@@ -929,2 +1026,2 @@\n-  method->set_not_c1_compilable();\n-  method->set_not_c2_compilable();\n+  method->set_is_not_c1_compilable();\n+  method->set_is_not_c2_compilable();\n@@ -1498,2 +1595,2 @@\n-  Klass* holder = cp->klass_ref_at(index, CHECK);\n-  Symbol* name = cp->name_ref_at(index);\n+  Klass* holder = cp->klass_ref_at(index, Bytecodes::_invokehandle, CHECK);\n+  Symbol* name = cp->name_ref_at(index, Bytecodes::_invokehandle);\n@@ -1515,1 +1612,1 @@\n-    LinkInfo link_info(cp, index, CATCH);\n+    LinkInfo link_info(cp, index, Bytecodes::_invokehandle, CATCH);\n@@ -1519,1 +1616,1 @@\n-    Symbol* name_sym = cp->name_ref_at(index);\n+    Symbol* name_sym = cp->name_ref_at(index, Bytecodes::_invokehandle);\n@@ -1715,0 +1812,7 @@\n+  \/\/ Java code should never directly access the extra data section\n+  JVMCI_THROW_MSG_0(IllegalArgumentException, err_msg(\"Invalid profile data position %d\", position));\n+C2V_END\n+\n+C2V_VMENTRY_0(jint, methodDataExceptionSeen, (JNIEnv* env, jobject, jlong method_data_pointer, jint bci))\n+  MethodData* mdo = (MethodData*) method_data_pointer;\n+  MutexLocker mu(mdo->extra_data_lock());\n@@ -1716,1 +1820,1 @@\n-  DataLayout* end   = mdo->extra_data_limit();\n+  DataLayout* end   = mdo->args_data_limit();\n@@ -1719,3 +1823,17 @@\n-    profile_data = data->data_in();\n-    if (mdo->dp_to_di(profile_data->dp()) == position) {\n-      return profile_data->size_in_bytes();\n+    int tag = data->tag();\n+    switch(tag) {\n+      case DataLayout::bit_data_tag: {\n+        BitData* bit_data = (BitData*) data->data_in();\n+        if (bit_data->bci() == bci) {\n+          return bit_data->exception_seen() ? 1 : 0;\n+        }\n+        break;\n+      }\n+    case DataLayout::no_tag:\n+      \/\/ There is a free slot so return false since a BitData would have been allocated to record\n+      \/\/ true if it had been seen.\n+      return 0;\n+    case DataLayout::arg_info_data_tag:\n+      \/\/ The bci wasn't found and there are no free slots to record a trap for this location, so always\n+      \/\/ return unknown.\n+      return -1;\n@@ -1724,1 +1842,2 @@\n-  JVMCI_THROW_MSG_0(IllegalArgumentException, err_msg(\"Invalid profile data position %d\", position));\n+  ShouldNotReachHere();\n+  return -1;\n@@ -2283,1 +2402,2 @@\n-    JVMCIEnv __peer_jvmci_env__(thread, false, __FILE__, __LINE__);\n+    bool jni_enomem_is_fatal = false;\n+    JVMCIEnv __peer_jvmci_env__(thread, false, jni_enomem_is_fatal, __FILE__, __LINE__);\n@@ -2285,0 +2405,3 @@\n+    if (peerEnv->has_jni_enomem()) {\n+      JVMCI_THROW_MSG_0(OutOfMemoryError, \"JNI_ENOMEM creating or attaching to libjvmci\");\n+    }\n@@ -2447,1 +2570,2 @@\n-      JVMCIEnv __peer_jvmci_env__(thread, false, __FILE__, __LINE__);\n+      bool jni_enomem_is_fatal = false;\n+      JVMCIEnv __peer_jvmci_env__(thread, false, jni_enomem_is_fatal, __FILE__, __LINE__);\n@@ -2449,0 +2573,4 @@\n+      if (peerJVMCIEnv->has_jni_enomem()) {\n+        JVMCI_THROW_MSG_0(OutOfMemoryError, \"JNI_ENOMEM creating or attaching to libjvmci\");\n+      }\n+\n@@ -2542,1 +2670,2 @@\n-  JVMCIEnv __peer_jvmci_env__(thread, !JVMCIENV->is_hotspot(), __FILE__, __LINE__);\n+  bool jni_enomem_is_fatal = false;\n+  JVMCIEnv __peer_jvmci_env__(thread, !JVMCIENV->is_hotspot(), jni_enomem_is_fatal, __FILE__, __LINE__);\n@@ -2545,0 +2674,3 @@\n+  if (peerEnv->has_jni_enomem()) {\n+      JVMCI_THROW_MSG_0(OutOfMemoryError, \"JNI_ENOMEM creating or attaching to libjvmci\");\n+  }\n@@ -2969,4 +3101,4 @@\n-  {CC \"lookupNameInPool\",                             CC \"(\" HS_CONSTANT_POOL2 \"I)\" STRING,                                                 FN_PTR(lookupNameInPool)},\n-  {CC \"lookupNameAndTypeRefIndexInPool\",              CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(lookupNameAndTypeRefIndexInPool)},\n-  {CC \"lookupSignatureInPool\",                        CC \"(\" HS_CONSTANT_POOL2 \"I)\" STRING,                                                 FN_PTR(lookupSignatureInPool)},\n-  {CC \"lookupKlassRefIndexInPool\",                    CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(lookupKlassRefIndexInPool)},\n+  {CC \"lookupNameInPool\",                             CC \"(\" HS_CONSTANT_POOL2 \"II)\" STRING,                                                FN_PTR(lookupNameInPool)},\n+  {CC \"lookupNameAndTypeRefIndexInPool\",              CC \"(\" HS_CONSTANT_POOL2 \"II)I\",                                                      FN_PTR(lookupNameAndTypeRefIndexInPool)},\n+  {CC \"lookupSignatureInPool\",                        CC \"(\" HS_CONSTANT_POOL2 \"II)\" STRING,                                                FN_PTR(lookupSignatureInPool)},\n+  {CC \"lookupKlassRefIndexInPool\",                    CC \"(\" HS_CONSTANT_POOL2 \"II)I\",                                                      FN_PTR(lookupKlassRefIndexInPool)},\n@@ -3020,0 +3152,1 @@\n+  {CC \"methodDataExceptionSeen\",                      CC \"(JI)I\",                                                                           FN_PTR(methodDataExceptionSeen)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":190,"deletions":57,"binary":false,"changes":247,"status":"modified"},{"patch":"@@ -59,0 +59,18 @@\n+  static_field(CompilerToVM::Data,             SharedRuntime_polling_page_return_handler,                                            \\\n+                                                                                       address)                                      \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             nmethod_entry_barrier, address)                                                       \\\n+  static_field(CompilerToVM::Data,             thread_disarmed_guard_value_offset, int)                                              \\\n+  static_field(CompilerToVM::Data,             thread_address_bad_mask_offset, int)                                                  \\\n+  AARCH64_ONLY(static_field(CompilerToVM::Data, BarrierSetAssembler_nmethod_patching_type, int))                                     \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_oop_field_preloaded, address)                      \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_weak_oop_field_preloaded, address)                 \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_phantom_oop_field_preloaded, address)              \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_oop_field_preloaded, address)                 \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_weak_oop_field_preloaded, address)            \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_phantom_oop_field_preloaded, address)         \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_oop_array, address)                                \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_clone, address)                                                    \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             continuations_enabled, bool)                                                          \\\n@@ -132,1 +150,1 @@\n-  nonstatic_field(ConstMethod,                 _flags,                                 u2)                                           \\\n+  nonstatic_field(ConstMethod,                 _flags._flags,                          u4)                                           \\\n@@ -187,0 +205,1 @@\n+  nonstatic_field(JavaThread,                  _saved_exception_pc,                           address)                               \\\n@@ -231,1 +250,1 @@\n-  nonstatic_field(Method,                      _flags,                                        u2)                                    \\\n+  nonstatic_field(Method,                      _flags._status,                                u4)                                    \\\n@@ -419,2 +438,0 @@\n-  declare_constant(JVM_ACC_MONITOR_MATCH)                                 \\\n-  declare_constant(JVM_ACC_HAS_MONITOR_BYTECODES)                         \\\n@@ -480,0 +497,1 @@\n+  declare_constant(CodeInstaller::ENTRY_BARRIER_PATCH)                    \\\n@@ -585,5 +603,10 @@\n-  declare_constant(ConstMethod::_has_linenumber_table)                    \\\n-  declare_constant(ConstMethod::_has_localvariable_table)                 \\\n-  declare_constant(ConstMethod::_has_exception_table)                     \\\n-  declare_constant(ConstMethod::_has_method_annotations)                  \\\n-  declare_constant(ConstMethod::_has_parameter_annotations)               \\\n+  declare_constant(ConstMethodFlags::_misc_has_linenumber_table)          \\\n+  declare_constant(ConstMethodFlags::_misc_has_localvariable_table)       \\\n+  declare_constant(ConstMethodFlags::_misc_has_exception_table)           \\\n+  declare_constant(ConstMethodFlags::_misc_has_method_annotations)        \\\n+  declare_constant(ConstMethodFlags::_misc_has_parameter_annotations)     \\\n+  declare_constant(ConstMethodFlags::_misc_caller_sensitive)              \\\n+  declare_constant(ConstMethodFlags::_misc_is_hidden)                     \\\n+  declare_constant(ConstMethodFlags::_misc_intrinsic_candidate)           \\\n+  declare_constant(ConstMethodFlags::_misc_reserved_stack_access)         \\\n+  declare_constant(ConstMethodFlags::_misc_changes_current_thread)        \\\n@@ -688,7 +711,2 @@\n-  declare_constant(Method::_caller_sensitive)                             \\\n-  declare_constant(Method::_force_inline)                                 \\\n-  declare_constant(Method::_dont_inline)                                  \\\n-  declare_constant(Method::_hidden)                                       \\\n-  declare_constant(Method::_intrinsic_candidate)                          \\\n-  declare_constant(Method::_reserved_stack_access)                        \\\n-  declare_constant(Method::_changes_current_thread)                       \\\n+  declare_constant(MethodFlags::_misc_force_inline)                       \\\n+  declare_constant(MethodFlags::_misc_dont_inline)                        \\\n@@ -701,0 +719,4 @@\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::stw_instruction_and_data_patch))  \\\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::conc_instruction_and_data_patch)) \\\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::conc_data_patch))                 \\\n+                                                                          \\\n@@ -722,0 +744,1 @@\n+  declare_constant(markWord::lock_mask_in_place)                          \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":39,"deletions":16,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -80,0 +80,1 @@\n+  LOG_TAG(fastlock) \\\n@@ -140,0 +141,1 @@\n+  LOG_TAG(page) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -702,2 +702,1 @@\n-uintx HeapInspection::populate_table(KlassInfoTable* cit, BoolObjectClosure *filter, uint parallel_thread_num) {\n-\n+uintx HeapInspection::populate_table(KlassInfoTable* cit, BoolObjectClosure *filter, WorkerThreads* workers) {\n@@ -705,1 +704,1 @@\n-  if (parallel_thread_num > 1) {\n+  if (workers != nullptr) {\n@@ -707,16 +706,6 @@\n-\n-    WorkerThreads* workers = Universe::heap()->safepoint_workers();\n-    if (workers != nullptr) {\n-      \/\/ The GC provided a WorkerThreads to be used during a safepoint.\n-\n-      \/\/ Can't run with more threads than provided by the WorkerThreads.\n-      const uint capped_parallel_thread_num = MIN2(parallel_thread_num, workers->max_workers());\n-      WithActiveWorkers with_active_workers(workers, capped_parallel_thread_num);\n-\n-      ParallelObjectIterator poi(workers->active_workers());\n-      ParHeapInspectTask task(&poi, cit, filter);\n-      \/\/ Run task with the active workers.\n-      workers->run_task(&task);\n-      if (task.success()) {\n-        return task.missed_count();\n-      }\n+    ParallelObjectIterator poi(workers->active_workers());\n+    ParHeapInspectTask task(&poi, cit, filter);\n+    \/\/ Run task with the active workers.\n+    workers->run_task(&task);\n+    if (task.success()) {\n+      return task.missed_count();\n@@ -733,1 +722,1 @@\n-void HeapInspection::heap_inspection(outputStream* st, uint parallel_thread_num) {\n+void HeapInspection::heap_inspection(outputStream* st, WorkerThreads* workers) {\n@@ -739,1 +728,1 @@\n-    uintx missed_count = populate_table(&cit, nullptr, parallel_thread_num);\n+    uintx missed_count = populate_table(&cit, nullptr, workers);\n","filename":"src\/hotspot\/share\/memory\/heapInspection.cpp","additions":10,"deletions":21,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -208,2 +208,2 @@\n-  void heap_inspection(outputStream* st, uint parallel_thread_num = 1) NOT_SERVICES_RETURN;\n-  uintx populate_table(KlassInfoTable* cit, BoolObjectClosure* filter = nullptr, uint parallel_thread_num = 1) NOT_SERVICES_RETURN_(0);\n+  void heap_inspection(outputStream* st, WorkerThreads* workers) NOT_SERVICES_RETURN;\n+  uintx populate_table(KlassInfoTable* cit, BoolObjectClosure* filter, WorkerThreads* workers) NOT_SERVICES_RETURN_(0);\n","filename":"src\/hotspot\/share\/memory\/heapInspection.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -214,1 +214,0 @@\n-\n@@ -220,0 +219,5 @@\n+class OopFieldClosure {\n+public:\n+  virtual void do_field(oop base, oop* p) = 0;\n+};\n+\n@@ -347,0 +351,3 @@\n+  \/\/ Read\/write the int pointed to by p.\n+  virtual void do_int(int* p) = 0;\n+\n@@ -365,0 +372,3 @@\n+\n+  \/\/ Useful alias\n+  template <typename T> void do_ptr(T** p) { do_ptr((void**)p); }\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -268,1 +268,1 @@\n-    f->do_u4((u4*)&_archived_basic_type_mirror_indices[i]);\n+    f->do_int(&_archived_basic_type_mirror_indices[i]);\n@@ -275,1 +275,1 @@\n-  f->do_ptr((void**)&_fillerArrayKlassObj);\n+  f->do_ptr(&_fillerArrayKlassObj);\n@@ -277,1 +277,1 @@\n-    f->do_ptr((void**)&_typeArrayKlassObjs[i]);\n+    f->do_ptr(&_typeArrayKlassObjs[i]);\n@@ -280,7 +280,7 @@\n-  f->do_ptr((void**)&_objectArrayKlassObj);\n-  f->do_ptr((void**)&_the_array_interfaces_array);\n-  f->do_ptr((void**)&_the_empty_int_array);\n-  f->do_ptr((void**)&_the_empty_short_array);\n-  f->do_ptr((void**)&_the_empty_method_array);\n-  f->do_ptr((void**)&_the_empty_klass_array);\n-  f->do_ptr((void**)&_the_empty_instance_klass_array);\n+  f->do_ptr(&_objectArrayKlassObj);\n+  f->do_ptr(&_the_array_interfaces_array);\n+  f->do_ptr(&_the_empty_int_array);\n+  f->do_ptr(&_the_empty_short_array);\n+  f->do_ptr(&_the_empty_method_array);\n+  f->do_ptr(&_the_empty_klass_array);\n+  f->do_ptr(&_the_empty_instance_klass_array);\n@@ -1238,0 +1238,5 @@\n+void Universe::set_verify_data(uintptr_t mask, uintptr_t bits) {\n+  _verify_oop_mask = mask;\n+  _verify_oop_bits = bits;\n+}\n+\n@@ -1285,0 +1290,4 @@\n+void LatestMethodCache::serialize(SerializeClosure* f) {\n+  f->do_ptr(&_klass);\n+}\n+\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":19,"deletions":10,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+class SerializeClosure;\n@@ -70,3 +71,1 @@\n-  void serialize(SerializeClosure* f) {\n-    f->do_ptr((void**)&_klass);\n-  }\n+  void serialize(SerializeClosure* f);\n@@ -214,0 +213,1 @@\n+  static void set_verify_data(uintptr_t mask, uintptr_t bits) PRODUCT_RETURN;\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -0,0 +1,98 @@\n+\/*\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_OOPS_CONSTMETHODFLAGS_HPP\n+#define SHARE_OOPS_CONSTMETHODFLAGS_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class outputStream;\n+\n+\/\/ The ConstMethodFlags class contains the parse-time flags associated with\n+\/\/ a Method, and its associated accessors.\n+\/\/ These flags are JVM internal and not part of the AccessFlags classfile specification.\n+\n+class ConstMethodFlags {\n+  friend class VMStructs;\n+  friend class JVMCIVMStructs;\n+\n+#define CM_FLAGS_DO(flag)  \\\n+   flag(has_linenumber_table      , 1 << 0) \\\n+   flag(has_checked_exceptions    , 1 << 1) \\\n+   flag(has_localvariable_table   , 1 << 2) \\\n+   flag(has_exception_table       , 1 << 3) \\\n+   flag(has_generic_signature     , 1 << 4) \\\n+   flag(has_method_parameters     , 1 << 5) \\\n+   flag(is_overpass               , 1 << 6) \\\n+   flag(has_method_annotations    , 1 << 7) \\\n+   flag(has_parameter_annotations , 1 << 8) \\\n+   flag(has_type_annotations      , 1 << 9) \\\n+   flag(has_default_annotations   , 1 << 10) \\\n+   flag(caller_sensitive          , 1 << 11) \\\n+   flag(is_hidden                 , 1 << 12) \\\n+   flag(has_injected_profile      , 1 << 13) \\\n+   flag(intrinsic_candidate       , 1 << 14) \\\n+   flag(reserved_stack_access     , 1 << 15) \\\n+   flag(is_scoped                 , 1 << 16) \\\n+   flag(changes_current_thread    , 1 << 17) \\\n+   flag(jvmti_mount_transition    , 1 << 18) \\\n+   flag(has_scalarized_args       , 1 << 19) \\\n+   flag(has_scalarized_return     , 1 << 20) \\\n+   flag(c1_needs_stack_repair     , 1 << 21) \\\n+   flag(c2_needs_stack_repair     , 1 << 22) \\\n+   flag(mismatch                  , 1 << 23) \\\n+   \/* end of list *\/\n+\n+#define CM_FLAGS_ENUM_NAME(name, value)    _misc_##name = value,\n+  enum {\n+    CM_FLAGS_DO(CM_FLAGS_ENUM_NAME)\n+  };\n+#undef CM_FLAGS_ENUM_NAME\n+\n+  \/\/ These flags are write-once before the class is published and then read-only so don't require atomic updates.\n+  u4 _flags;\n+\n+ public:\n+\n+  ConstMethodFlags() : _flags(0) {}\n+\n+  \/\/ Create getters and setters for the flag values.\n+#define CM_FLAGS_GET_SET(name, ignore)          \\\n+  bool name() const { return (_flags & _misc_##name) != 0; } \\\n+  void set_##name() {         \\\n+    _flags |= _misc_##name;  \\\n+  }\n+  CM_FLAGS_DO(CM_FLAGS_GET_SET)\n+#undef CM_FLAGS_GET_SET\n+\n+  static u4 has_scalarized_return_flag() {\n+    return _misc_has_scalarized_return;\n+  }\n+\n+  int as_int() const { return _flags; }\n+  void print_on(outputStream* st) const;\n+};\n+\n+#endif \/\/ SHARE_OOPS_CONSTMETHODFLAGS_HPP\n","filename":"src\/hotspot\/share\/oops\/constMethodFlags.hpp","additions":98,"deletions":0,"binary":false,"changes":98,"status":"added"},{"patch":"@@ -722,9 +722,22 @@\n-Symbol* ConstantPool::impl_name_ref_at(int which, bool uncached) {\n-  int name_index = name_ref_index_at(impl_name_and_type_ref_index_at(which, uncached));\n-  return symbol_at(name_index);\n-}\n-\n-\n-Symbol* ConstantPool::impl_signature_ref_at(int which, bool uncached) {\n-  int signature_index = signature_ref_index_at(impl_name_and_type_ref_index_at(which, uncached));\n-  return symbol_at(signature_index);\n+\/\/ Translate index, which could be CPCache index or Indy index, to a constant pool index\n+int ConstantPool::to_cp_index(int index, Bytecodes::Code code) {\n+  assert(cache() != nullptr, \"'index' is a rewritten index so this class must have been rewritten\");\n+  switch(code) {\n+    case Bytecodes::_invokedynamic:\n+      return invokedynamic_bootstrap_ref_index_at(index);\n+    case Bytecodes::_getfield:\n+    case Bytecodes::_getstatic:\n+    case Bytecodes::_putfield:\n+    case Bytecodes::_putstatic:\n+      \/\/ TODO: handle resolved field entries with new structure\n+      \/\/ i = ....\n+    case Bytecodes::_invokeinterface:\n+    case Bytecodes::_invokehandle:\n+    case Bytecodes::_invokespecial:\n+    case Bytecodes::_invokestatic:\n+    case Bytecodes::_invokevirtual:\n+      \/\/ TODO: handle resolved method entries with new structure\n+    default:\n+      \/\/ change byte-ordering and go via cache\n+      return remap_instruction_operand_from_cache(index);\n+  }\n@@ -733,18 +746,5 @@\n-int ConstantPool::impl_name_and_type_ref_index_at(int which, bool uncached) {\n-  int i = which;\n-  if (!uncached && cache() != nullptr) {\n-    if (ConstantPool::is_invokedynamic_index(which)) {\n-      \/\/ Invokedynamic index is index into the resolved indy array in the constant pool cache\n-      int pool_index = invokedynamic_bootstrap_ref_index_at(which);\n-      pool_index = bootstrap_name_and_type_ref_index_at(pool_index);\n-      assert(tag_at(pool_index).is_name_and_type(), \"\");\n-      return pool_index;\n-    }\n-    \/\/ change byte-ordering and go via cache\n-    i = remap_instruction_operand_from_cache(which);\n-  } else {\n-    if (tag_at(which).has_bootstrap()) {\n-      int pool_index = bootstrap_name_and_type_ref_index_at(which);\n-      assert(tag_at(pool_index).is_name_and_type(), \"\");\n-      return pool_index;\n-    }\n+int ConstantPool::uncached_name_and_type_ref_index_at(int cp_index)  {\n+  if (tag_at(cp_index).has_bootstrap()) {\n+    int pool_index = bootstrap_name_and_type_ref_index_at(cp_index);\n+    assert(tag_at(pool_index).is_name_and_type(), \"\");\n+    return pool_index;\n@@ -752,3 +752,3 @@\n-  assert(tag_at(i).is_field_or_method(), \"Corrupted constant pool\");\n-  assert(!tag_at(i).has_bootstrap(), \"Must be handled above\");\n-  jint ref_index = *int_at_addr(i);\n+  assert(tag_at(cp_index).is_field_or_method(), \"Corrupted constant pool\");\n+  assert(!tag_at(cp_index).has_bootstrap(), \"Must be handled above\");\n+  jint ref_index = *int_at_addr(cp_index);\n@@ -758,1 +758,6 @@\n-constantTag ConstantPool::impl_tag_ref_at(int which, bool uncached) {\n+int ConstantPool::name_and_type_ref_index_at(int index, Bytecodes::Code code) {\n+  return uncached_name_and_type_ref_index_at(to_cp_index(index, code));\n+}\n+\n+constantTag ConstantPool::tag_ref_at(int which, Bytecodes::Code code) {\n+  \/\/ which may be either a Constant Pool index or a rewritten index\n@@ -760,9 +765,2 @@\n-  if (!uncached && cache() != nullptr) {\n-    if (ConstantPool::is_invokedynamic_index(which)) {\n-      \/\/ Invokedynamic index is index into resolved_references\n-      pool_index = invokedynamic_bootstrap_ref_index_at(which);\n-    } else {\n-      \/\/ change byte-ordering and go via cache\n-      pool_index = remap_instruction_operand_from_cache(which);\n-    }\n-  }\n+  assert(cache() != nullptr, \"'index' is a rewritten index so this class must have been rewritten\");\n+  pool_index = to_cp_index(which, code);\n@@ -772,10 +770,3 @@\n-int ConstantPool::impl_klass_ref_index_at(int which, bool uncached) {\n-  guarantee(!ConstantPool::is_invokedynamic_index(which),\n-            \"an invokedynamic instruction does not have a klass\");\n-  int i = which;\n-  if (!uncached && cache() != nullptr) {\n-    \/\/ change byte-ordering and go via cache\n-    i = remap_instruction_operand_from_cache(which);\n-  }\n-  assert(tag_at(i).is_field_or_method(), \"Corrupted constant pool\");\n-  jint ref_index = *int_at_addr(i);\n+int ConstantPool::uncached_klass_ref_index_at(int cp_index) {\n+  assert(tag_at(cp_index).is_field_or_method(), \"Corrupted constant pool\");\n+  jint ref_index = *int_at_addr(cp_index);\n@@ -785,0 +776,7 @@\n+int ConstantPool::klass_ref_index_at(int index, Bytecodes::Code code) {\n+  guarantee(!ConstantPool::is_invokedynamic_index(index),\n+            \"an invokedynamic instruction does not have a klass\");\n+  assert(code != Bytecodes::_invokedynamic,\n+            \"an invokedynamic instruction does not have a klass\");\n+  return uncached_klass_ref_index_at(to_cp_index(index, code));\n+}\n@@ -817,2 +815,2 @@\n-Klass* ConstantPool::klass_ref_at(int which, TRAPS) {\n-  return klass_at(klass_ref_index_at(which), THREAD);\n+Klass* ConstantPool::klass_ref_at(int which, Bytecodes::Code code, TRAPS) {\n+  return klass_at(klass_ref_index_at(which, code), THREAD);\n@@ -825,2 +823,2 @@\n-Symbol* ConstantPool::klass_ref_at_noresolve(int which) {\n-  jint ref_index = klass_ref_index_at(which);\n+Symbol* ConstantPool::klass_ref_at_noresolve(int which, Bytecodes::Code code) {\n+  jint ref_index = klass_ref_index_at(which, code);\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":52,"deletions":54,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -267,4 +267,4 @@\n-  static int tags_offset_in_bytes()         { return offset_of(ConstantPool, _tags); }\n-  static int cache_offset_in_bytes()        { return offset_of(ConstantPool, _cache); }\n-  static int pool_holder_offset_in_bytes()  { return offset_of(ConstantPool, _pool_holder); }\n-  static int resolved_klasses_offset_in_bytes()    { return offset_of(ConstantPool, _resolved_klasses); }\n+  static ByteSize tags_offset()         { return byte_offset_of(ConstantPool, _tags); }\n+  static ByteSize cache_offset()        { return byte_offset_of(ConstantPool, _cache); }\n+  static ByteSize pool_holder_offset()  { return byte_offset_of(ConstantPool, _pool_holder); }\n+  static ByteSize resolved_klasses_offset()    { return byte_offset_of(ConstantPool, _resolved_klasses); }\n@@ -513,1 +513,1 @@\n-    return impl_name_ref_at(member, true);\n+    return uncached_name_ref_at(member);\n@@ -517,1 +517,1 @@\n-    return impl_signature_ref_at(member, true);\n+    return uncached_signature_ref_at(member);\n@@ -521,1 +521,1 @@\n-    return impl_klass_ref_index_at(member, true);\n+    return uncached_klass_ref_index_at(member);\n@@ -669,4 +669,10 @@\n-  Klass* klass_ref_at(int which, TRAPS);\n-  Symbol* klass_ref_at_noresolve(int which);\n-  Symbol* name_ref_at(int which)                { return impl_name_ref_at(which, false); }\n-  Symbol* signature_ref_at(int which)           { return impl_signature_ref_at(which, false); }\n+  Klass* klass_ref_at(int which, Bytecodes::Code code, TRAPS);\n+  Symbol* klass_ref_at_noresolve(int which, Bytecodes::Code code);\n+  Symbol* name_ref_at(int which, Bytecodes::Code code) {\n+    int name_index = name_ref_index_at(name_and_type_ref_index_at(which, code));\n+    return symbol_at(name_index);\n+  }\n+  Symbol* signature_ref_at(int which, Bytecodes::Code code) {\n+    int signature_index = signature_ref_index_at(name_and_type_ref_index_at(which, code));\n+    return symbol_at(signature_index);\n+  }\n@@ -674,2 +680,2 @@\n-  int klass_ref_index_at(int which)               { return impl_klass_ref_index_at(which, false); }\n-  int name_and_type_ref_index_at(int which)       { return impl_name_and_type_ref_index_at(which, false); }\n+  int klass_ref_index_at(int which, Bytecodes::Code code);\n+  int name_and_type_ref_index_at(int which, Bytecodes::Code code);\n@@ -679,1 +685,3 @@\n-  constantTag tag_ref_at(int cp_cache_index)      { return impl_tag_ref_at(cp_cache_index, false); }\n+  constantTag tag_ref_at(int cp_cache_index, Bytecodes::Code code);\n+\n+  int to_cp_index(int which, Bytecodes::Code code);\n@@ -781,5 +789,11 @@\n-  Symbol* uncached_klass_ref_at_noresolve(int which);\n-  Symbol* uncached_name_ref_at(int which)                 { return impl_name_ref_at(which, true); }\n-  Symbol* uncached_signature_ref_at(int which)            { return impl_signature_ref_at(which, true); }\n-  int       uncached_klass_ref_index_at(int which)          { return impl_klass_ref_index_at(which, true); }\n-  int       uncached_name_and_type_ref_index_at(int which)  { return impl_name_and_type_ref_index_at(which, true); }\n+  Symbol* uncached_klass_ref_at_noresolve(int cp_index);\n+  Symbol* uncached_name_ref_at(int cp_index) {\n+    int name_index = name_ref_index_at(uncached_name_and_type_ref_index_at(cp_index));\n+    return symbol_at(name_index);\n+  }\n+  Symbol* uncached_signature_ref_at(int cp_index) {\n+    int signature_index = signature_ref_index_at(uncached_name_and_type_ref_index_at(cp_index));\n+    return symbol_at(signature_index);\n+  }\n+  int       uncached_klass_ref_index_at(int cp_index);\n+  int       uncached_name_and_type_ref_index_at(int cp_index);\n@@ -812,7 +826,0 @@\n-  Symbol* impl_name_ref_at(int which, bool uncached);\n-  Symbol* impl_signature_ref_at(int which, bool uncached);\n-\n-  int       impl_klass_ref_index_at(int which, bool uncached);\n-  int       impl_name_and_type_ref_index_at(int which, bool uncached);\n-  constantTag impl_tag_ref_at(int which, bool uncached);\n-\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":33,"deletions":26,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -465,2 +465,2 @@\n-  static int resolved_references_offset_in_bytes() { return offset_of(ConstantPoolCache, _resolved_references); }\n-  static ByteSize invokedynamic_entries_offset()   { return byte_offset_of(ConstantPoolCache, _resolved_indy_entries); }\n+  static ByteSize resolved_references_offset()   { return byte_offset_of(ConstantPoolCache, _resolved_references); }\n+  static ByteSize invokedynamic_entries_offset() { return byte_offset_of(ConstantPoolCache, _resolved_indy_entries); }\n","filename":"src\/hotspot\/share\/oops\/cpCache.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1324,1 +1324,1 @@\n-      int nameAndTypeIdx    = cp->name_and_type_ref_index_at(idx);\n+      int nameAndTypeIdx    = cp->name_and_type_ref_index_at(idx, currentBC->code());\n@@ -1389,1 +1389,1 @@\n-    case Bytecodes::_withfield:         do_withfield(itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_withfield:        do_withfield(itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n@@ -1604,4 +1604,4 @@\n-    case Bytecodes::_getstatic:         do_field(true,  true, itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_putstatic:         do_field(false, true, itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_getfield:          do_field(true,  false, itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_putfield:          do_field(false, false, itr->get_index_u2_cpcache(), itr->bci()); break;\n+    case Bytecodes::_getstatic:         do_field(true,  true,  itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_putstatic:         do_field(false, true,  itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_getfield:          do_field(true,  false, itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_putfield:          do_field(false, false, itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n@@ -1611,3 +1611,3 @@\n-    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2_cpcache(), itr->bci()); break;\n-    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(),         itr->bci()); break;\n+    case Bytecodes::_invokespecial:     do_method(false, itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokestatic:      do_method(true , itr->get_index_u2_cpcache(), itr->bci(), itr->code()); break;\n+    case Bytecodes::_invokedynamic:     do_method(true , itr->get_index_u4(),         itr->bci(), itr->code()); break;\n@@ -1939,1 +1939,1 @@\n-void GenerateOopMap::do_field(int is_get, int is_static, int idx, int bci) {\n+void GenerateOopMap::do_field(int is_get, int is_static, int idx, int bci, Bytecodes::Code bc) {\n@@ -1942,1 +1942,1 @@\n-  int nameAndTypeIdx     = cp->name_and_type_ref_index_at(idx);\n+  int nameAndTypeIdx     = cp->name_and_type_ref_index_at(idx, bc);\n@@ -1967,1 +1967,1 @@\n-void GenerateOopMap::do_method(int is_static, int idx, int bci) {\n+void GenerateOopMap::do_method(int is_static, int idx, int bci, Bytecodes::Code bc) {\n@@ -1970,1 +1970,1 @@\n-  Symbol* signature   = cp->signature_ref_at(idx);\n+  Symbol* signature   = cp->signature_ref_at(idx, bc);\n@@ -2004,1 +2004,1 @@\n-void GenerateOopMap::do_withfield(int idx, int bci) {\n+void GenerateOopMap::do_withfield(int idx, int bci, Bytecodes::Code bc) {\n@@ -2007,1 +2007,1 @@\n-  int nameAndTypeIdx = cp->name_and_type_ref_index_at(idx);\n+  int nameAndTypeIdx = cp->name_and_type_ref_index_at(idx, bc);\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -404,3 +404,3 @@\n-  void  do_field                            (int is_get, int is_static, int idx, int bci);\n-  void  do_method                           (int is_static, int idx, int bci);\n-  void  do_withfield                       (int idx, int bci);\n+  void  do_field                            (int is_get, int is_static, int idx, int bci, Bytecodes::Code bc);\n+  void  do_method                           (int is_static, int idx, int bci, Bytecodes::Code bc);\n+  void  do_withfield                        (int idx, int bci, Bytecodes::Code bc);\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -511,1 +511,1 @@\n-  it->push_internal_pointer(&this_ptr, (intptr_t*)&_adr_inlineklass_fixed_block);\n+  \/\/ TODO: _adr_inlineklass_fixed_block ?\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2812,0 +2812,12 @@\n+\n+  remove_unshareable_flags();\n+}\n+\n+void InstanceKlass::remove_unshareable_flags() {\n+  \/\/ clear all the flags\/stats that shouldn't be in the archived version\n+  assert(!is_scratch_class(), \"must be\");\n+  assert(!has_been_redefined(), \"must be\");\n+#if INCLUDE_JVMTI\n+  set_is_being_redefined(false);\n+#endif\n+  set_has_resolved_methods(false);\n@@ -3738,1 +3750,1 @@\n-  st->print(BULLET\"misc flags:        0x%x\", _misc_flags.flags());               st->cr();\n+  st->print(BULLET\"flags:             \"); _misc_flags.print_on(st);               st->cr();\n@@ -4272,1 +4284,1 @@\n-bool InstanceKlass::_has_previous_versions = false;\n+bool InstanceKlass::_should_clean_previous_versions = false;\n@@ -4279,3 +4291,3 @@\n-bool InstanceKlass::has_previous_versions_and_reset() {\n-  bool ret = _has_previous_versions;\n-  log_trace(redefine, class, iklass, purge)(\"Class unloading: has_previous_versions = %s\",\n+bool InstanceKlass::should_clean_previous_versions_and_reset() {\n+  bool ret = _should_clean_previous_versions;\n+  log_trace(redefine, class, iklass, purge)(\"Class unloading: should_clean_previous_versions = %s\",\n@@ -4283,1 +4295,1 @@\n-  _has_previous_versions = false;\n+  _should_clean_previous_versions = false;\n@@ -4340,1 +4352,0 @@\n-      log_trace(redefine, class, iklass, purge)(\"previous version \" PTR_FORMAT \" is alive\", p2i(pv_node));\n@@ -4344,2 +4355,8 @@\n-      \/\/ found a previous version for next time we do class unloading\n-      _has_previous_versions = true;\n+      if (pvcp->is_shared()) {\n+        \/\/ Shared previous versions can never be removed so no cleaning is needed.\n+        log_trace(redefine, class, iklass, purge)(\"previous version \" PTR_FORMAT \" is shared\", p2i(pv_node));\n+      } else {\n+        \/\/ Previous version alive, set that clean is needed for next time.\n+        _should_clean_previous_versions = true;\n+        log_trace(redefine, class, iklass, purge)(\"previous version \" PTR_FORMAT \" is alive\", p2i(pv_node));\n+      }\n@@ -4445,4 +4462,2 @@\n-  \/\/ Add previous version if any methods are still running.\n-  \/\/ Set has_previous_version flag for processing during class unloading.\n-  _has_previous_versions = true;\n-  log_trace(redefine, class, iklass, add) (\"scratch class added; one of its methods is on_stack.\");\n+  \/\/ Add previous version if any methods are still running or if this is\n+  \/\/ a shared class which should never be removed.\n@@ -4452,0 +4467,8 @@\n+  if (cp_ref->is_shared()) {\n+    log_trace(redefine, class, iklass, add) (\"scratch class added; class is shared\");\n+  } else {\n+    \/\/  We only set clean_previous_versions flag for processing during class\n+    \/\/ unloading for non-shared classes.\n+    _should_clean_previous_versions = true;\n+    log_trace(redefine, class, iklass, add) (\"scratch class added; one of its methods is on_stack.\");\n+  }\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":36,"deletions":13,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -278,0 +278,1 @@\n+  NOT_PRODUCT(volatile int _shared_class_load_count;) \/\/ ensure a shared class is loaded only once\n@@ -611,1 +612,1 @@\n-  static ByteSize reference_type_offset() { return in_ByteSize(offset_of(InstanceKlass, _reference_type)); }\n+  static ByteSize reference_type_offset() { return byte_offset_of(InstanceKlass, _reference_type); }\n@@ -771,0 +772,1 @@\n+  void set_has_resolved_methods(bool value)   { _misc_flags.set_has_resolved_methods(value); }\n@@ -780,1 +782,1 @@\n-  static bool  _has_previous_versions;\n+  static bool  _should_clean_previous_versions;\n@@ -788,2 +790,2 @@\n-  static bool has_previous_versions_and_reset();\n-  static bool has_previous_versions() { return _has_previous_versions; }\n+  static bool should_clean_previous_versions_and_reset();\n+  static bool should_clean_previous_versions() { return _should_clean_previous_versions; }\n@@ -809,1 +811,1 @@\n-  static bool has_previous_versions_and_reset() { return false; }\n+  static bool should_clean_previous_versions_and_reset() { return false; }\n@@ -929,1 +931,1 @@\n-  static ByteSize init_state_offset()  { return in_ByteSize(offset_of(InstanceKlass, _init_state)); }\n+  static ByteSize init_state_offset()  { return byte_offset_of(InstanceKlass, _init_state); }\n@@ -931,1 +933,1 @@\n-  static ByteSize init_thread_offset() { return in_ByteSize(offset_of(InstanceKlass, _init_thread)); }\n+  static ByteSize init_thread_offset() { return byte_offset_of(InstanceKlass, _init_thread); }\n@@ -1207,0 +1209,1 @@\n+  void remove_unshareable_flags();\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,2 @@\n+#include \"runtime\/atomic.hpp\"\n+\n@@ -80,1 +82,2 @@\n-    status(is_marked_dependent               , 1 << 4) \/* class is the redefined scratch class *\/\n+    status(is_marked_dependent               , 1 << 4) \/* class is the redefined scratch class *\/ \\\n+    \/* end of list *\/\n@@ -103,6 +106,2 @@\n-#define IK_FLAGS_GET(name, ignore)          \\\n-  bool name() const { return (_flags & _misc_##name) != 0; }\n-  IK_FLAGS_DO(IK_FLAGS_GET)\n-#undef IK_FLAGS_GET\n-\n-#define IK_FLAGS_SET(name, ignore)   \\\n+#define IK_FLAGS_GET_SET(name, ignore)          \\\n+  bool name() const { return (_flags & _misc_##name) != 0; } \\\n@@ -113,2 +112,2 @@\n-  IK_FLAGS_DO(IK_FLAGS_SET)\n-#undef IK_FLAGS_SET\n+  IK_FLAGS_DO(IK_FLAGS_GET_SET)\n+#undef IK_FLAGS_GET_SET\n@@ -133,6 +132,2 @@\n-#define IK_STATUS_GET(name, ignore)          \\\n-  bool name() const { return (_status & _misc_##name) != 0; }\n-  IK_STATUS_DO(IK_STATUS_GET)\n-#undef IK_STATUS_GET\n-\n-#define IK_STATUS_SET(name, ignore)   \\\n+#define IK_STATUS_GET_SET(name, ignore)          \\\n+  bool name() const { return (_status & _misc_##name) != 0; } \\\n@@ -146,2 +141,2 @@\n-  IK_STATUS_DO(IK_STATUS_SET)\n-#undef IK_STATUS_SET\n+  IK_STATUS_DO(IK_STATUS_GET_SET)\n+#undef IK_STATUS_GET_SET\n@@ -149,2 +144,3 @@\n-  void atomic_set_bits(u1 bits);\n-  void atomic_clear_bits(u1 bits);\n+  void atomic_set_bits(u1 bits)   { Atomic::fetch_then_or(&_status, bits); }\n+  void atomic_clear_bits(u1 bits) { Atomic::fetch_then_and(&_status, (u1)(~bits)); }\n+  void print_on(outputStream* st) const;\n","filename":"src\/hotspot\/share\/oops\/instanceKlassFlags.hpp","additions":16,"deletions":20,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -390,10 +390,14 @@\n-  static ByteSize super_offset()                 { return in_ByteSize(offset_of(Klass, _super)); }\n-  static ByteSize super_check_offset_offset()    { return in_ByteSize(offset_of(Klass, _super_check_offset)); }\n-  static ByteSize primary_supers_offset()        { return in_ByteSize(offset_of(Klass, _primary_supers)); }\n-  static ByteSize secondary_super_cache_offset() { return in_ByteSize(offset_of(Klass, _secondary_super_cache)); }\n-  static ByteSize secondary_supers_offset()      { return in_ByteSize(offset_of(Klass, _secondary_supers)); }\n-  static ByteSize java_mirror_offset()           { return in_ByteSize(offset_of(Klass, _java_mirror)); }\n-  static ByteSize class_loader_data_offset()     { return in_ByteSize(offset_of(Klass, _class_loader_data)); }\n-  static ByteSize modifier_flags_offset()        { return in_ByteSize(offset_of(Klass, _modifier_flags)); }\n-  static ByteSize layout_helper_offset()         { return in_ByteSize(offset_of(Klass, _layout_helper)); }\n-  static ByteSize access_flags_offset()          { return in_ByteSize(offset_of(Klass, _access_flags)); }\n+  static ByteSize super_offset()                 { return byte_offset_of(Klass, _super); }\n+  static ByteSize super_check_offset_offset()    { return byte_offset_of(Klass, _super_check_offset); }\n+  static ByteSize primary_supers_offset()        { return byte_offset_of(Klass, _primary_supers); }\n+  static ByteSize secondary_super_cache_offset() { return byte_offset_of(Klass, _secondary_super_cache); }\n+  static ByteSize secondary_supers_offset()      { return byte_offset_of(Klass, _secondary_supers); }\n+  static ByteSize java_mirror_offset()           { return byte_offset_of(Klass, _java_mirror); }\n+  static ByteSize class_loader_data_offset()     { return byte_offset_of(Klass, _class_loader_data); }\n+  static ByteSize modifier_flags_offset()        { return byte_offset_of(Klass, _modifier_flags); }\n+  static ByteSize layout_helper_offset()         { return byte_offset_of(Klass, _layout_helper); }\n+  static ByteSize access_flags_offset()          { return byte_offset_of(Klass, _access_flags); }\n+#if INCLUDE_JVMCI\n+  static ByteSize subklass_offset()              { return byte_offset_of(Klass, _subklass); }\n+  static ByteSize next_sibling_offset()          { return byte_offset_of(Klass, _next_sibling); }\n+#endif\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":14,"deletions":10,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -187,1 +187,1 @@\n-  static int method_offset_in_bytes() { return offset_of(vtableEntry, _method); }\n+  static ByteSize method_offset() { return byte_offset_of(vtableEntry, _method); }\n@@ -233,3 +233,3 @@\n-  static int size()                       { return sizeof(itableOffsetEntry) \/ wordSize; }    \/\/ size in words\n-  static int interface_offset_in_bytes()  { return offset_of(itableOffsetEntry, _interface); }\n-  static int offset_offset_in_bytes()     { return offset_of(itableOffsetEntry, _offset); }\n+  static int size()                            { return sizeof(itableOffsetEntry) \/ wordSize; }    \/\/ size in words\n+  static ByteSize interface_offset()  { return byte_offset_of(itableOffsetEntry, _interface); }\n+  static ByteSize offset_offset()     { return byte_offset_of(itableOffsetEntry, _offset); }\n@@ -255,1 +255,1 @@\n-  static int method_offset_in_bytes()       { return offset_of(itableMethodEntry, _method); }\n+  static ByteSize method_offset()  { return byte_offset_of(itableMethodEntry, _method); }\n","filename":"src\/hotspot\/share\/oops\/klassVtable.hpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-\/\/    [ptr             | 00]  locked             ptr points to real header on stack\n+\/\/    [ptr             | 00]  locked             ptr points to real header on stack (stack-locking in use)\n+\/\/    [header          | 00]  locked             locked regular object header (fast-locking in use)\n@@ -55,1 +56,1 @@\n-\/\/    [ptr             | 10]  monitor            inflated lock (header is wapped out)\n+\/\/    [ptr             | 10]  monitor            inflated lock (header is swapped out)\n@@ -57,1 +58,1 @@\n-\/\/    [0 ............ 0| 00]  inflating          inflation in progress\n+\/\/    [0 ............ 0| 00]  inflating          inflation in progress (stack-locking in use)\n@@ -267,0 +268,1 @@\n+  \/\/ Fast-locking does not use INFLATING.\n@@ -281,1 +283,2 @@\n-    return ((value() & lock_mask_in_place) == locked_value);\n+    assert(LockingMode == LM_LEGACY, \"should only be called with legacy stack locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n@@ -287,0 +290,10 @@\n+\n+  bool is_fast_locked() const {\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"should only be called with new lightweight locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n+  }\n+  markWord set_fast_locked() const {\n+    \/\/ Clear the lock_mask_in_place bits to set locked_value:\n+    return markWord(value() & ~lock_mask_in_place);\n+  }\n+\n@@ -296,1 +309,3 @@\n-    return ((value() & unlocked_value) == 0);\n+    intptr_t lockbits = value() & lock_mask_in_place;\n+    return LockingMode == LM_LIGHTWEIGHT  ? lockbits == monitor_value   \/\/ monitor?\n+                                          : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":20,"deletions":5,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -106,5 +106,0 @@\n-  set_force_inline(false);\n-  set_hidden(false);\n-  set_dont_inline(false);\n-  set_changes_current_thread(false);\n-  set_has_injected_profile(false);\n@@ -426,0 +421,1 @@\n+  assert(!queued_for_compilation(), \"method's queued_for_compilation flag should not be set\");\n@@ -763,1 +759,3 @@\n-        if (bcs.dest() < bcs.next_bci()) _access_flags.set_has_loops();\n+        if (bcs.dest() < bcs.next_bci()) {\n+          return set_has_loops();\n+        }\n@@ -768,1 +766,3 @@\n-        if (bcs.dest_w() < bcs.next_bci()) _access_flags.set_has_loops();\n+        if (bcs.dest_w() < bcs.next_bci()) {\n+          return set_has_loops();\n+        }\n@@ -774,1 +774,1 @@\n-          _access_flags.set_has_loops();\n+          return set_has_loops();\n@@ -779,2 +779,1 @@\n-              _access_flags.set_has_loops();\n-              break;\n+              return set_has_loops();\n@@ -789,1 +788,1 @@\n-          _access_flags.set_has_loops();\n+          return set_has_loops();\n@@ -793,1 +792,1 @@\n-              _access_flags.set_has_loops();\n+              return set_has_loops();\n@@ -803,2 +802,3 @@\n-  _access_flags.set_loops_flag_init();\n-  return _access_flags.has_loops();\n+\n+  _flags.set_has_loops_flag_init(true);\n+  return false;\n@@ -1018,2 +1018,2 @@\n-bool Method::is_klass_loaded(int refinfo_index, bool must_be_resolved) const {\n-  int klass_index = constants()->klass_ref_index_at(refinfo_index);\n+bool Method::is_klass_loaded(int refinfo_index, Bytecodes::Code bc, bool must_be_resolved) const {\n+  int klass_index = constants()->klass_ref_index_at(refinfo_index, bc);\n@@ -1150,2 +1150,2 @@\n-    set_not_c1_compilable();\n-    set_not_c2_compilable();\n+    set_is_not_c1_compilable();\n+    set_is_not_c2_compilable();\n@@ -1154,1 +1154,1 @@\n-      set_not_c1_compilable();\n+      set_is_not_c1_compilable();\n@@ -1156,1 +1156,1 @@\n-      set_not_c2_compilable();\n+      set_is_not_c2_compilable();\n@@ -1176,2 +1176,2 @@\n-    set_not_c1_osr_compilable();\n-    set_not_c2_osr_compilable();\n+    set_is_not_c1_osr_compilable();\n+    set_is_not_c2_osr_compilable();\n@@ -1180,1 +1180,1 @@\n-      set_not_c1_osr_compilable();\n+      set_is_not_c1_osr_compilable();\n@@ -1182,1 +1182,1 @@\n-      set_not_c2_osr_compilable();\n+      set_is_not_c2_osr_compilable();\n@@ -1242,0 +1242,15 @@\n+  remove_unshareable_flags();\n+}\n+\n+void Method::remove_unshareable_flags() {\n+  \/\/ clear all the flags that shouldn't be in the archived version\n+  assert(!is_old(), \"must be\");\n+  assert(!is_obsolete(), \"must be\");\n+  assert(!is_deleted(), \"must be\");\n+\n+  set_is_prefixed_native(false);\n+  set_queued_for_compilation(false);\n+  set_is_not_c2_compilable(false);\n+  set_is_not_c1_compilable(false);\n+  set_is_not_c2_osr_compilable(false);\n+  set_on_stack_flag(false);\n@@ -1271,1 +1286,1 @@\n-    set_has_scalarized_return(true);\n+    set_has_scalarized_return();\n@@ -1730,1 +1745,1 @@\n-      set_force_inline(true);\n+      set_force_inline();\n@@ -2308,2 +2323,2 @@\n-  bool already_set = on_stack();\n-  _access_flags.set_on_stack(value);\n+  bool already_set = on_stack_flag();\n+  set_on_stack_flag(value);\n@@ -2396,0 +2411,1 @@\n+  st->print   (\" - flags:             0x%x  \", _flags.as_int()); _flags.print_on(st); st->cr();\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":44,"deletions":28,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/methodFlags.hpp\"\n@@ -82,2 +83,1 @@\n-                                                 \/\/ note: can have vtables with >2**16 elements (because of inheritance)\n-  u2                _intrinsic_id;               \/\/ vmSymbols::intrinsic_id (0 == _none)\n+  MethodFlags       _flags;\n@@ -85,19 +85,1 @@\n-  \/\/ Flags\n-  enum Flags {\n-    _caller_sensitive       = 1 << 0,\n-    _force_inline           = 1 << 1,\n-    _dont_inline            = 1 << 2,\n-    _hidden                 = 1 << 3,\n-    _has_injected_profile   = 1 << 4,\n-    _intrinsic_candidate    = 1 << 5,\n-    _reserved_stack_access  = 1 << 6,\n-    _scalarized_args        = 1 << 7,\n-    _scalarized_return      = 1 << 8,\n-    _c1_needs_stack_repair  = 1 << 9,\n-    _c2_needs_stack_repair  = 1 << 10,\n-    _scoped                 = 1 << 11,\n-    _changes_current_thread = 1 << 12,\n-    _jvmti_mount_transition = 1 << 13,\n-    _mismatch               = 1 << 14\n-  };\n-  mutable u2 _flags;\n+  u2                _intrinsic_id;               \/\/ vmSymbols::intrinsic_id (0 == _none)\n@@ -344,1 +326,1 @@\n-                             { return constMethod()->has_exception_handler(); }\n+                             { return constMethod()->has_exception_table(); }\n@@ -482,0 +464,1 @@\n+  void remove_unshareable_flags() NOT_CDS_RETURN;\n@@ -624,0 +607,8 @@\n+  \/\/ Flags getting and setting.\n+#define M_STATUS_GET_SET(name, ignore)          \\\n+  bool name() const { return _flags.name(); }   \\\n+  void set_##name(bool x) { _flags.set_##name(x); } \\\n+  void set_##name() { _flags.set_##name(true); }\n+  M_STATUS_DO(M_STATUS_GET_SET)\n+#undef M_STATUS_GET_SET\n+\n@@ -626,1 +617,1 @@\n-    return access_flags().loops_flag_init() ? access_flags().has_loops() : compute_has_loops_flag();\n+    return has_loops_flag_init() ? has_loops_flag() : compute_has_loops_flag();\n@@ -630,6 +621,5 @@\n-\n-  bool has_jsrs() {\n-    return access_flags().has_jsrs();\n-  };\n-  void set_has_jsrs() {\n-    _access_flags.set_has_jsrs();\n+  bool set_has_loops() {\n+    \/\/ set both the flags and that it's been initialized.\n+    set_has_loops_flag();\n+    set_has_loops_flag_init();\n+    return true;\n@@ -639,4 +629,1 @@\n-  bool has_monitors() const                      { return is_synchronized() || access_flags().has_monitor_bytecodes(); }\n-  bool has_monitor_bytecodes() const             { return access_flags().has_monitor_bytecodes(); }\n-\n-  void set_has_monitor_bytecodes()               { _access_flags.set_has_monitor_bytecodes(); }\n+  bool has_monitors() const                      { return is_synchronized() || has_monitor_bytecodes(); }\n@@ -645,1 +632,1 @@\n-  \/\/ propererly nest in the method. It might return false, even though they actually nest properly, since the info.\n+  \/\/ properly nest in the method. It might return false, even though they actually nest properly, since the info.\n@@ -647,2 +634,2 @@\n-  bool guaranteed_monitor_matching() const       { return access_flags().is_monitor_matching(); }\n-  void set_guaranteed_monitor_matching()         { _access_flags.set_monitor_matching(); }\n+  bool guaranteed_monitor_matching() const       { return monitor_matching(); }\n+  void set_guaranteed_monitor_matching()         { set_monitor_matching(); }\n@@ -700,3 +687,1 @@\n-  static ByteSize method_data_offset()           {\n-    return byte_offset_of(Method, _method_data);\n-  }\n+\n@@ -716,2 +701,2 @@\n-  static int method_data_offset_in_bytes()       { return offset_of(Method, _method_data); }\n-  static int intrinsic_id_offset_in_bytes()      { return offset_of(Method, _intrinsic_id); }\n+  static ByteSize method_data_offset()  { return byte_offset_of(Method, _method_data); }\n+  static ByteSize intrinsic_id_offset() { return byte_offset_of(Method, _intrinsic_id); }\n@@ -771,8 +756,1 @@\n-  bool is_old() const                               { return access_flags().is_old(); }\n-  void set_is_old()                                 { _access_flags.set_is_old(); }\n-  bool is_obsolete() const                          { return access_flags().is_obsolete(); }\n-  void set_is_obsolete()                            { _access_flags.set_is_obsolete(); }\n-  bool is_deleted() const                           { return access_flags().is_deleted(); }\n-  void set_is_deleted()                             { _access_flags.set_is_deleted(); }\n-\n-  bool on_stack() const                             { return access_flags().on_stack(); }\n+  bool on_stack() const                             { return on_stack_flag(); }\n@@ -786,4 +764,0 @@\n-  \/\/ JVMTI Native method prefixing support:\n-  bool is_prefixed_native() const                   { return access_flags().is_prefixed_native(); }\n-  void set_is_prefixed_native()                     { _access_flags.set_is_prefixed_native(); }\n-\n@@ -846,42 +820,2 @@\n-  bool caller_sensitive() {\n-    return (_flags & _caller_sensitive) != 0;\n-  }\n-  void set_caller_sensitive(bool x) {\n-    _flags = x ? (_flags | _caller_sensitive) : (_flags & ~_caller_sensitive);\n-  }\n-\n-  bool force_inline() {\n-    return (_flags & _force_inline) != 0;\n-  }\n-  void set_force_inline(bool x) {\n-    _flags = x ? (_flags | _force_inline) : (_flags & ~_force_inline);\n-  }\n-\n-  bool dont_inline() {\n-    return (_flags & _dont_inline) != 0;\n-  }\n-  void set_dont_inline(bool x) {\n-    _flags = x ? (_flags | _dont_inline) : (_flags & ~_dont_inline);\n-  }\n-\n-  bool changes_current_thread() {\n-    return (_flags & _changes_current_thread) != 0;\n-  }\n-  void set_changes_current_thread(bool x) {\n-    _flags = x ? (_flags | _changes_current_thread) : (_flags & ~_changes_current_thread);\n-  }\n-\n-  bool jvmti_mount_transition() {\n-    return (_flags & _jvmti_mount_transition) != 0;\n-  }\n-  void set_jvmti_mount_transition(bool x) {\n-    _flags = x ? (_flags | _jvmti_mount_transition) : (_flags & ~_jvmti_mount_transition);\n-  }\n-\n-  bool is_hidden() const {\n-    return (_flags & _hidden) != 0;\n-  }\n-\n-  void set_hidden(bool x) {\n-    _flags = x ? (_flags | _hidden) : (_flags & ~_hidden);\n-  }\n+  bool caller_sensitive() const     { return constMethod()->caller_sensitive(); }\n+  void set_caller_sensitive() { constMethod()->set_caller_sensitive(); }\n@@ -889,3 +823,2 @@\n-  bool is_scoped() const {\n-    return (_flags & _scoped) != 0;\n-  }\n+  bool changes_current_thread() const { return constMethod()->changes_current_thread(); }\n+  void set_changes_current_thread() { constMethod()->set_changes_current_thread(); }\n@@ -893,3 +826,2 @@\n-  void set_scoped(bool x) {\n-    _flags = x ? (_flags | _scoped) : (_flags & ~_scoped);\n-  }\n+  bool jvmti_mount_transition() const { return constMethod()->jvmti_mount_transition(); }\n+  void set_jvmti_mount_transition() { constMethod()->set_jvmti_mount_transition(); }\n@@ -897,6 +829,2 @@\n-  bool intrinsic_candidate() {\n-    return (_flags & _intrinsic_candidate) != 0;\n-  }\n-  void set_intrinsic_candidate(bool x) {\n-    _flags = x ? (_flags | _intrinsic_candidate) : (_flags & ~_intrinsic_candidate);\n-  }\n+  bool is_hidden() const { return constMethod()->is_hidden(); }\n+  void set_is_hidden() { constMethod()->set_is_hidden(); }\n@@ -904,10 +832,2 @@\n-  bool has_injected_profile() {\n-    return (_flags & _has_injected_profile) != 0;\n-  }\n-  void set_has_injected_profile(bool x) {\n-    _flags = x ? (_flags | _has_injected_profile) : (_flags & ~_has_injected_profile);\n-  }\n-\n-  bool has_reserved_stack_access() {\n-    return (_flags & _reserved_stack_access) != 0;\n-  }\n+  bool is_scoped() const { return constMethod()->is_scoped(); }\n+  void set_scoped() { constMethod()->set_is_scoped(); }\n@@ -915,7 +835,2 @@\n-  void set_has_reserved_stack_access(bool x) {\n-    _flags = x ? (_flags | _reserved_stack_access) : (_flags & ~_reserved_stack_access);\n-  }\n-\n-  bool has_scalarized_args() const {\n-    return (_flags & _scalarized_args) != 0;\n-  }\n+  bool intrinsic_candidate() const { return constMethod()->intrinsic_candidate(); }\n+  void set_intrinsic_candidate() { constMethod()->set_intrinsic_candidate(); }\n@@ -923,3 +838,2 @@\n-  void set_has_scalarized_args(bool x) {\n-    _flags = x ? (_flags | _scalarized_args) : (_flags & ~_scalarized_args);\n-  }\n+  bool has_injected_profile() const { return constMethod()->has_injected_profile(); }\n+  void set_has_injected_profile() { constMethod()->set_has_injected_profile(); }\n@@ -927,3 +841,2 @@\n-  bool has_scalarized_return() const {\n-    return (_flags & _scalarized_return) != 0;\n-  }\n+  bool has_reserved_stack_access() const { return constMethod()->reserved_stack_access(); }\n+  void set_has_reserved_stack_access() { constMethod()->set_reserved_stack_access(); }\n@@ -931,3 +844,2 @@\n-  void set_has_scalarized_return(bool x) {\n-    _flags = x ? (_flags | _scalarized_return) : (_flags & ~_scalarized_return);\n-  }\n+  bool has_scalarized_args() const { return constMethod()->has_scalarized_args(); }\n+  void set_has_scalarized_args() { constMethod()->set_has_scalarized_args(); }\n@@ -935,3 +847,2 @@\n-  static u2 scalarized_return_flag() {\n-    return _scalarized_return;\n-  }\n+  bool has_scalarized_return() const { return constMethod()->has_scalarized_return(); }\n+  void set_has_scalarized_return() { constMethod()->set_has_scalarized_return(); }\n@@ -941,3 +852,2 @@\n-  bool c1_needs_stack_repair() const {\n-    return (_flags & _c1_needs_stack_repair) != 0;\n-  }\n+  bool c1_needs_stack_repair() const { return constMethod()->c1_needs_stack_repair(); }\n+  void set_c1_needs_stack_repair() { constMethod()->set_c1_needs_stack_repair(); }\n@@ -945,3 +855,2 @@\n-  bool c2_needs_stack_repair() const {\n-    return (_flags & _c2_needs_stack_repair) != 0;\n-  }\n+  bool c2_needs_stack_repair() const { return constMethod()->c2_needs_stack_repair(); }\n+  void set_c2_needs_stack_repair() { constMethod()->set_c2_needs_stack_repair(); }\n@@ -949,15 +858,2 @@\n-  void set_c1_needs_stack_repair(bool x) {\n-    _flags = x ? (_flags | _c1_needs_stack_repair) : (_flags & ~_c1_needs_stack_repair);\n-  }\n-\n-  void set_c2_needs_stack_repair(bool x) {\n-    _flags = x ? (_flags | _c2_needs_stack_repair) : (_flags & ~_c2_needs_stack_repair);\n-  }\n-\n-  bool mismatch() const {\n-    return (_flags & _mismatch) != 0;\n-  }\n-\n-  void set_mismatch(bool x) {\n-    _flags = x ? (_flags | _mismatch) : (_flags & ~_mismatch);\n-  }\n+  bool mismatch() const { return constMethod()->mismatch(); }\n+  void set_mismatch() { constMethod()->set_mismatch(); }\n@@ -983,1 +879,1 @@\n-  bool is_klass_loaded(int refinfo_index, bool must_be_resolved = false) const;\n+  bool is_klass_loaded(int refinfo_index, Bytecodes::Code bc, bool must_be_resolved = false) const;\n@@ -1011,6 +907,3 @@\n-  bool   is_not_c1_compilable() const         { return access_flags().is_not_c1_compilable();  }\n-  void  set_not_c1_compilable()               {       _access_flags.set_not_c1_compilable();   }\n-  void clear_not_c1_compilable()              {       _access_flags.clear_not_c1_compilable(); }\n-  bool   is_not_c2_compilable() const         { return access_flags().is_not_c2_compilable();  }\n-  void  set_not_c2_compilable()               {       _access_flags.set_not_c2_compilable();   }\n-  void clear_not_c2_compilable()              {       _access_flags.clear_not_c2_compilable(); }\n+  void clear_is_not_c1_compilable()           { set_is_not_c1_compilable(false); }\n+  void clear_is_not_c2_compilable()           { set_is_not_c2_compilable(false); }\n+  void clear_is_not_c2_osr_compilable()       { set_is_not_c2_osr_compilable(false); }\n@@ -1018,6 +911,4 @@\n-  bool    is_not_c1_osr_compilable() const    { return is_not_c1_compilable(); }  \/\/ don't waste an accessFlags bit\n-  void   set_not_c1_osr_compilable()          {       set_not_c1_compilable(); }  \/\/ don't waste an accessFlags bit\n-  void clear_not_c1_osr_compilable()          {     clear_not_c1_compilable(); }  \/\/ don't waste an accessFlags bit\n-  bool   is_not_c2_osr_compilable() const     { return access_flags().is_not_c2_osr_compilable();  }\n-  void  set_not_c2_osr_compilable()           {       _access_flags.set_not_c2_osr_compilable();   }\n-  void clear_not_c2_osr_compilable()          {       _access_flags.clear_not_c2_osr_compilable(); }\n+  \/\/ not_c1_osr_compilable == not_c1_compilable\n+  bool is_not_c1_osr_compilable() const       { return is_not_c1_compilable(); }\n+  void set_is_not_c1_osr_compilable()         { set_is_not_c1_compilable(); }\n+  void clear_is_not_c1_osr_compilable()       { clear_is_not_c1_compilable(); }\n@@ -1026,3 +917,1 @@\n-  bool queued_for_compilation() const  { return access_flags().queued_for_compilation(); }\n-  void set_queued_for_compilation()    { _access_flags.set_queued_for_compilation();     }\n-  void clear_queued_for_compilation()  { _access_flags.clear_queued_for_compilation();   }\n+  void clear_queued_for_compilation()  { set_queued_for_compilation(false);   }\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":62,"deletions":173,"binary":false,"changes":235,"status":"modified"},{"patch":"@@ -2417,2 +2417,2 @@\n-  static int rtm_state_offset_in_bytes() {\n-    return offset_of(MethodData, _rtm_state);\n+  static ByteSize rtm_state_offset() {\n+    return byte_offset_of(MethodData, _rtm_state);\n","filename":"src\/hotspot\/share\/oops\/methodData.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -122,1 +122,1 @@\n-  \/\/ at a safepoint, it must not be zero.\n+  \/\/ at a safepoint, it must not be zero, except when using the new lightweight locking.\n@@ -131,1 +131,1 @@\n-  return !SafepointSynchronize::is_at_safepoint() ;\n+  return LockingMode == LM_LIGHTWEIGHT || !SafepointSynchronize::is_at_safepoint();\n@@ -195,1 +195,2 @@\n-void oopDesc::obj_field_put_raw(int offset, oop value)                { RawAccess<>::oop_store_at(as_oop(), offset, value); }\n+void oopDesc::obj_field_put_raw(int offset, oop value)                { assert(!(UseZGC && ZGenerational), \"Generational ZGC must use store barriers\");\n+                                                                        RawAccess<>::oop_store_at(as_oop(), offset, value); }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -326,2 +327,2 @@\n-  static int mark_offset_in_bytes()      { return offset_of(oopDesc, _mark); }\n-  static int klass_offset_in_bytes()     { return offset_of(oopDesc, _metadata._klass); }\n+  static int mark_offset_in_bytes()      { return (int)offset_of(oopDesc, _mark); }\n+  static int klass_offset_in_bytes()     { return (int)offset_of(oopDesc, _metadata._klass); }\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -69,0 +69,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -81,0 +81,5 @@\n+\/\/ Extra verification when creating and using oops.\n+\/\/ Used to catch broken oops as soon as possible.\n+using CheckOopFunctionPointer = void(*)(oopDesc*);\n+extern CheckOopFunctionPointer check_oop_function;\n+\n@@ -87,3 +92,6 @@\n-  void register_if_checking() {\n-    if (CheckUnhandledOops) register_oop();\n-  }\n+  \/\/ Extra verification of the oop\n+  void check_oop() const { if (check_oop_function != nullptr && _o != nullptr) check_oop_function(_o); }\n+\n+  void on_usage() const  { check_oop(); }\n+  void on_construction() { check_oop(); if (CheckUnhandledOops)   register_oop(); }\n+  void on_destruction()  {              if (CheckUnhandledOops) unregister_oop(); }\n@@ -92,3 +100,3 @@\n-  oop()             : _o(nullptr) { register_if_checking(); }\n-  oop(const oop& o) : _o(o._o)    { register_if_checking(); }\n-  oop(oopDesc* o)   : _o(o)       { register_if_checking(); }\n+  oop()             : _o(nullptr) { on_construction(); }\n+  oop(const oop& o) : _o(o._o)    { on_construction(); }\n+  oop(oopDesc* o)   : _o(o)       { on_construction(); }\n@@ -96,1 +104,1 @@\n-    if (CheckUnhandledOops) unregister_oop();\n+    on_destruction();\n@@ -99,3 +107,1 @@\n-  oopDesc* obj() const                 { return _o; }\n-  oopDesc* operator->() const          { return _o; }\n-  operator oopDesc* () const           { return _o; }\n+  oopDesc* obj() const                  { on_usage(); return _o; }\n@@ -103,2 +109,2 @@\n-  bool operator==(const oop& o) const  { return _o == o._o; }\n-  bool operator!=(const oop& o) const  { return _o != o._o; }\n+  oopDesc* operator->() const           { return obj(); }\n+  operator oopDesc* () const            { return obj(); }\n@@ -106,2 +112,2 @@\n-  bool operator==(std::nullptr_t) const     { return _o == nullptr; }\n-  bool operator!=(std::nullptr_t) const     { return _o != nullptr; }\n+  bool operator==(const oop& o) const   { return obj() == o.obj(); }\n+  bool operator!=(const oop& o) const   { return obj() != o.obj(); }\n@@ -109,1 +115,4 @@\n-  oop& operator=(const oop& o)         { _o = o._o; return *this; }\n+  bool operator==(std::nullptr_t) const { return obj() == nullptr; }\n+  bool operator!=(std::nullptr_t) const { return obj() != nullptr; }\n+\n+  oop& operator=(const oop& o)          { _o = o.obj(); return *this; }\n@@ -165,0 +174,4 @@\n+inline intptr_t p2i(narrowOop o) {\n+  return static_cast<intptr_t>(o);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":28,"deletions":15,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -512,1 +512,1 @@\n-      if (!caller_method->is_klass_loaded(index, true)) {\n+      if (!caller_method->is_klass_loaded(index, call_bc, true)) {\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -136,1 +136,1 @@\n-  notproduct(intx, BreakAtNode, 0,                                          \\\n+  notproduct(uint64_t, BreakAtNode, 0,                                      \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -786,3 +786,5 @@\n-  case vmIntrinsics::_notifyJvmtiMount:\n-  case vmIntrinsics::_notifyJvmtiUnmount:\n-  case vmIntrinsics::_notifyJvmtiHideFrames:\n+  case vmIntrinsics::_notifyJvmtiVThreadStart:\n+  case vmIntrinsics::_notifyJvmtiVThreadEnd:\n+  case vmIntrinsics::_notifyJvmtiVThreadMount:\n+  case vmIntrinsics::_notifyJvmtiVThreadUnmount:\n+  case vmIntrinsics::_notifyJvmtiVThreadHideFrames:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -72,0 +72,22 @@\n+\n+  \/\/ Visit all non-cast uses of the node, bypassing ConstraintCasts.\n+  \/\/ Pattern: this (-> ConstraintCast)* -> non_cast\n+  \/\/ In other words: find all non_cast nodes such that\n+  \/\/ non_cast->uncast() == this.\n+  template <typename Callback>\n+  static void visit_uncasted_uses(const Node* n, Callback callback) {\n+    ResourceMark rm;\n+    Unique_Node_List internals;\n+    internals.push((Node*)n); \/\/ start traversal\n+    for (uint j = 0; j < internals.size(); ++j) {\n+      Node* internal = internals.at(j); \/\/ for every internal\n+      for (DUIterator_Fast kmax, k = internal->fast_outs(kmax); k < kmax; k++) {\n+        Node* internal_use = internal->fast_out(k);\n+        if (internal_use->is_ConstraintCast()) {\n+          internals.push(internal_use); \/\/ traverse this cast also\n+        } else {\n+          callback(internal_use);\n+        }\n+      }\n+    }\n+  }\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -1823,1 +1823,1 @@\n-      if (PhaseIdealLoop::find_predicate(r->in(i)) != nullptr) {\n+      if (PhaseIdealLoop::find_parse_predicate(r->in(i)) != nullptr) {\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -61,0 +61,4 @@\n+\/\/ The success projection of a Parse Predicate is always an IfTrueNode and the uncommon projection an IfFalseNode\n+typedef IfTrueNode ParsePredicateSuccessProj;\n+typedef IfFalseNode ParsePredicateUncommonProj;\n+\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -391,1 +391,1 @@\n-    remove_skeleton_predicate_opaq(dead);\n+    remove_template_assertion_predicate_opaq(dead);\n@@ -445,2 +445,2 @@\n-  remove_useless_nodes(_predicate_opaqs,    useful); \/\/ remove useless predicate opaque nodes\n-  remove_useless_nodes(_skeleton_predicate_opaqs, useful);\n+  remove_useless_nodes(_parse_predicate_opaqs, useful); \/\/ remove useless Parse Predicate opaque nodes\n+  remove_useless_nodes(_template_assertion_predicate_opaqs, useful); \/\/ remove useless Assertion Predicate opaque nodes\n@@ -594,1 +594,0 @@\n-debug_only( int Compile::_debug_idx = 100000; )\n@@ -633,2 +632,2 @@\n-                  _predicate_opaqs   (comp_arena(), 8, 0, nullptr),\n-                  _skeleton_predicate_opaqs (comp_arena(), 8, 0, nullptr),\n+                  _parse_predicate_opaqs (comp_arena(), 8, 0, nullptr),\n+                  _template_assertion_predicate_opaqs (comp_arena(), 8, 0, nullptr),\n@@ -1866,7 +1865,8 @@\n-\/\/---------------------cleanup_loop_predicates-----------------------\n-\/\/ Remove the opaque nodes that protect the predicates so that all unused\n-\/\/ checks and uncommon_traps will be eliminated from the ideal graph\n-void Compile::cleanup_loop_predicates(PhaseIterGVN &igvn) {\n-  if (predicate_count()==0) return;\n-  for (int i = predicate_count(); i > 0; i--) {\n-    Node * n = predicate_opaque1_node(i-1);\n+\/\/ Remove the opaque nodes that protect the Parse Predicates so that all unused\n+\/\/ checks and uncommon_traps will be eliminated from the ideal graph.\n+void Compile::cleanup_parse_predicates(PhaseIterGVN& igvn) const {\n+  if (parse_predicate_count() == 0) {\n+    return;\n+  }\n+  for (int i = parse_predicate_count(); i > 0; i--) {\n+    Node* n = parse_predicate_opaque1_node(i - 1);\n@@ -1876,1 +1876,1 @@\n-  assert(predicate_count()==0, \"should be clean!\");\n+  assert(parse_predicate_count() == 0, \"should be clean!\");\n@@ -3349,6 +3349,1 @@\n-#ifdef ASSERT\n-      if (TraceNewVectors) {\n-        tty->print(\"new Vector node: \");\n-        macro_logic->dump();\n-      }\n-#endif\n+      VectorNode::trace_new_vector(macro_logic, \"MacroLogic\");\n@@ -4547,3 +4542,0 @@\n-        DEBUG_ONLY( n->fast_out(j)->dump(); );\n-        DEBUG_ONLY( n->dump_bfs(1, 0, \"-\"); );\n-        assert(false, \"infinite loop\");\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":15,"deletions":23,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -360,2 +360,2 @@\n-  GrowableArray<Node*>  _predicate_opaqs;       \/\/ List of Opaque1 nodes for the loop predicates.\n-  GrowableArray<Node*>  _skeleton_predicate_opaqs; \/\/ List of Opaque4 nodes for the loop skeleton predicates.\n+  GrowableArray<Node*>  _parse_predicate_opaqs; \/\/ List of Opaque1 nodes for the Parse Predicates.\n+  GrowableArray<Node*>  _template_assertion_predicate_opaqs; \/\/ List of Opaque4 nodes for Template Assertion Predicates.\n@@ -383,1 +383,0 @@\n-  debug_only(static int _debug_idx;)            \/\/ Monotonic counter (not reset), use -XX:BreakAtNode=<idx>\n@@ -691,2 +690,2 @@\n-  int           predicate_count()         const { return _predicate_opaqs.length(); }\n-  int           skeleton_predicate_count() const { return _skeleton_predicate_opaqs.length(); }\n+  int           parse_predicate_count()   const { return _parse_predicate_opaqs.length(); }\n+  int           template_assertion_predicate_count() const { return _template_assertion_predicate_opaqs.length(); }\n@@ -697,2 +696,6 @@\n-  Node*         predicate_opaque1_node(int idx) const { return _predicate_opaqs.at(idx); }\n-  Node*         skeleton_predicate_opaque4_node(int idx) const { return _skeleton_predicate_opaqs.at(idx); }\n+  Node*         parse_predicate_opaque1_node(int idx) const { return _parse_predicate_opaqs.at(idx); }\n+\n+  Node* template_assertion_predicate_opaq_node(int idx) const {\n+    return _template_assertion_predicate_opaqs.at(idx);\n+  }\n+\n@@ -712,3 +715,3 @@\n-    \/\/ remove from _predicate_opaqs list also if it is there\n-    if (predicate_count() > 0) {\n-      _predicate_opaqs.remove_if_existing(n);\n+    \/\/ remove from _parse_predicate_opaqs list also if it is there\n+    if (parse_predicate_count() > 0) {\n+      _parse_predicate_opaqs.remove_if_existing(n);\n@@ -725,2 +728,2 @@\n-  void add_predicate_opaq(Node* n) {\n-    assert(!_predicate_opaqs.contains(n), \"duplicate entry in predicate opaque1\");\n+  void add_parse_predicate_opaq(Node* n) {\n+    assert(!_parse_predicate_opaqs.contains(n), \"duplicate entry in Parse Predicate opaque1 list\");\n@@ -728,1 +731,1 @@\n-    _predicate_opaqs.append(n);\n+    _parse_predicate_opaqs.append(n);\n@@ -730,3 +733,4 @@\n-  void add_skeleton_predicate_opaq(Node* n) {\n-    assert(!_skeleton_predicate_opaqs.contains(n), \"duplicate entry in skeleton predicate opaque4 list\");\n-    _skeleton_predicate_opaqs.append(n);\n+  void add_template_assertion_predicate_opaq(Node* n) {\n+    assert(!_template_assertion_predicate_opaqs.contains(n),\n+           \"duplicate entry in template assertion predicate opaque4 list\");\n+    _template_assertion_predicate_opaqs.append(n);\n@@ -734,3 +738,3 @@\n-  void remove_skeleton_predicate_opaq(Node* n) {\n-    if (skeleton_predicate_count() > 0) {\n-      _skeleton_predicate_opaqs.remove_if_existing(n);\n+  void remove_template_assertion_predicate_opaq(Node* n) {\n+    if (template_assertion_predicate_count() > 0) {\n+      _template_assertion_predicate_opaqs.remove_if_existing(n);\n@@ -765,1 +769,1 @@\n-  \/\/ remove the opaque nodes that protect the predicates so that the unused checks and\n+  \/\/ Remove the opaque nodes that protect the Parse Predicates so that the unused checks and\n@@ -767,3 +771,3 @@\n-  void cleanup_loop_predicates(PhaseIterGVN &igvn);\n-  bool is_predicate_opaq(Node* n) {\n-    return _predicate_opaqs.contains(n);\n+  void cleanup_parse_predicates(PhaseIterGVN &igvn) const;\n+  bool is_predicate_opaq(Node* n) const {\n+    return _parse_predicate_opaqs.contains(n);\n@@ -809,2 +813,0 @@\n-  static int   debug_idx()                 { return debug_only(_debug_idx)+0; }\n-  static void  set_debug_idx(int i)        { debug_only(_debug_idx = i); }\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":27,"deletions":25,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -1060,9 +1060,5 @@\n-      if (strncmp(name, \"_notify_jvmti_object_alloc\", 26) == 0) { \/\/ Object escapes to JVMTI\n-        add_java_object(call, PointsToNode::GlobalEscape);\n-      } else {\n-        assert(strncmp(name, \"_multianewarray\", 15) == 0 ||\n-               strncmp(name, \"_load_unknown_inline\", 20) == 0, \"TODO: add failed case check\");\n-        \/\/ Returns a newly allocated non-escaped object.\n-        add_java_object(call, PointsToNode::NoEscape);\n-        set_not_scalar_replaceable(ptnode_adr(call_idx) NOT_PRODUCT(COMMA \"is result of multinewarray\"));\n-      }\n+      assert(strncmp(name, \"_multianewarray\", 15) == 0 ||\n+             strncmp(name, \"_load_unknown_inline\", 20) == 0, \"TODO: add failed case check\");\n+      \/\/ Returns a newly allocated non-escaped object.\n+      add_java_object(call, PointsToNode::NoEscape);\n+      set_not_scalar_replaceable(ptnode_adr(call_idx) NOT_PRODUCT(COMMA \"is result of multinewarray\"));\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -4417,4 +4417,2 @@\n-\/\/----------------------------- loop predicates ---------------------------\n-\n-\/\/------------------------------add_predicate_impl----------------------------\n-void GraphKit::add_empty_predicate_impl(Deoptimization::DeoptReason reason, int nargs) {\n+\/\/ Add a Parse Predicate with an uncommon trap on the failing\/false path. Normal control will continue on the true path.\n+void GraphKit::add_parse_predicate(Deoptimization::DeoptReason reason, const int nargs) {\n@@ -4433,1 +4431,1 @@\n-    \/\/ do not generate predicate.\n+    \/\/ do not generate Parse Predicate.\n@@ -4437,1 +4435,1 @@\n-  Node *cont    = _gvn.intcon(1);\n+  Node* cont    = _gvn.intcon(1);\n@@ -4439,1 +4437,1 @@\n-  Node *bol     = _gvn.transform(new Conv2BNode(opq));\n+  Node* bol     = _gvn.transform(new Conv2BNode(opq));\n@@ -4442,1 +4440,1 @@\n-  C->add_predicate_opaq(opq);\n+  C->add_parse_predicate_opaq(opq);\n@@ -4453,5 +4451,3 @@\n-\/\/------------------------------add_predicate---------------------------------\n-void GraphKit::add_empty_predicates(int nargs) {\n-  \/\/ These loop predicates remain empty. All concrete loop predicates are inserted above the corresponding\n-  \/\/ empty loop predicate later by 'PhaseIdealLoop::create_new_if_for_predicate'. All concrete loop predicates of\n-  \/\/ a specific kind (normal, profile or limit check) share the same uncommon trap as the empty loop predicate.\n+\/\/ Add Parse Predicates which serve as placeholders to create new Runtime Predicates above them. All\n+\/\/ Runtime Predicates inside a Runtime Predicate block share the same uncommon trap as the Parse Predicate.\n+void GraphKit::add_parse_predicates(int nargs) {\n@@ -4459,1 +4455,1 @@\n-    add_empty_predicate_impl(Deoptimization::Reason_predicate, nargs);\n+    add_parse_predicate(Deoptimization::Reason_predicate, nargs);\n@@ -4462,1 +4458,1 @@\n-    add_empty_predicate_impl(Deoptimization::Reason_profile_predicate, nargs);\n+    add_parse_predicate(Deoptimization::Reason_profile_predicate, nargs);\n@@ -4464,2 +4460,2 @@\n-  \/\/ loop's limit check predicate should be near the loop.\n-  add_empty_predicate_impl(Deoptimization::Reason_loop_limit_check, nargs);\n+  \/\/ Loop Limit Check Predicate should be near the loop.\n+  add_parse_predicate(Deoptimization::Reason_loop_limit_check, nargs);\n@@ -4589,1 +4585,1 @@\n-  add_empty_predicates();\n+  add_parse_predicates();\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":14,"deletions":18,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -920,2 +920,2 @@\n-  void add_empty_predicates(int nargs = 0);\n-  void add_empty_predicate_impl(Deoptimization::DeoptReason reason, int nargs);\n+  void add_parse_predicates(int nargs = 0);\n+  void add_parse_predicate(Deoptimization::DeoptReason reason, int nargs);\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -172,2 +172,2 @@\n-    \/\/ Add loop predicate.\n-    gkit->add_empty_predicates(nargs);\n+    \/\/ Add Parse Predicates.\n+    gkit->add_parse_predicates(nargs);\n","filename":"src\/hotspot\/share\/opto\/idealKit.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-    Node* proj = PhaseIdealLoop::find_predicate(r->in(ii));\n+    Node* proj = PhaseIdealLoop::find_parse_predicate(r->in(ii));\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -54,1 +54,0 @@\n-#include \"prims\/jvmtiExport.hpp\"\n@@ -489,5 +488,9 @@\n-  case vmIntrinsics::_notifyJvmtiMount:         return inline_native_notify_jvmti_funcs(CAST_FROM_FN_PTR(address, OptoRuntime::notify_jvmti_mount()),\n-                                                                                        \"notifyJvmtiMount\");\n-  case vmIntrinsics::_notifyJvmtiUnmount:       return inline_native_notify_jvmti_funcs(CAST_FROM_FN_PTR(address, OptoRuntime::notify_jvmti_unmount()),\n-                                                                                        \"notifyJvmtiUnmount\");\n-  case vmIntrinsics::_notifyJvmtiHideFrames:    return inline_native_notify_jvmti_hide();\n+  case vmIntrinsics::_notifyJvmtiVThreadStart:   return inline_native_notify_jvmti_funcs(CAST_FROM_FN_PTR(address, OptoRuntime::notify_jvmti_vthread_start()),\n+                                                                                         \"notifyJvmtiStart\", true, false);\n+  case vmIntrinsics::_notifyJvmtiVThreadEnd:     return inline_native_notify_jvmti_funcs(CAST_FROM_FN_PTR(address, OptoRuntime::notify_jvmti_vthread_end()),\n+                                                                                         \"notifyJvmtiEnd\", false, true);\n+  case vmIntrinsics::_notifyJvmtiVThreadMount:   return inline_native_notify_jvmti_funcs(CAST_FROM_FN_PTR(address, OptoRuntime::notify_jvmti_vthread_mount()),\n+                                                                                         \"notifyJvmtiMount\", false, false);\n+  case vmIntrinsics::_notifyJvmtiVThreadUnmount: return inline_native_notify_jvmti_funcs(CAST_FROM_FN_PTR(address, OptoRuntime::notify_jvmti_vthread_unmount()),\n+                                                                                         \"notifyJvmtiUnmount\", false, false);\n+  case vmIntrinsics::_notifyJvmtiVThreadHideFrames: return inline_native_notify_jvmti_hide();\n@@ -3044,22 +3047,0 @@\n-#if INCLUDE_JVMTI\n-  \/\/ Check if JvmtiExport::_should_notify_object_alloc is enabled and post notifications\n-  IdealKit ideal(this);\n-  IdealVariable result(ideal); ideal.declarations_done();\n-  Node* ONE = ideal.ConI(1);\n-  Node* addr = makecon(TypeRawPtr::make((address) &JvmtiExport::_should_notify_object_alloc));\n-  Node* should_post_vm_object_alloc = ideal.load(ideal.ctrl(), addr, TypeInt::BOOL, T_BOOLEAN, Compile::AliasIdxRaw);\n-\n-  ideal.sync_kit(this);\n-  ideal.if_then(should_post_vm_object_alloc, BoolTest::eq, ONE); {\n-    const TypeFunc *tf = OptoRuntime::notify_jvmti_object_alloc_Type();\n-    address funcAddr = OptoRuntime::notify_jvmti_object_alloc();\n-    sync_kit(ideal);\n-    Node* call = make_runtime_call(RC_NO_LEAF, tf, funcAddr, \"_notify_jvmti_object_alloc\", TypePtr::BOTTOM, obj);\n-    ideal.sync_kit(this);\n-    ideal.set(result,_gvn.transform(new ProjNode(call, TypeFunc::Parms+0)));\n-  } ideal.else_(); {\n-    ideal.set(result,obj);\n-  } ideal.end_if();\n-  final_sync(ideal);\n-  obj = ideal.value(result);\n-#endif \/\/INCLUDE_JVMTI\n@@ -3091,1 +3072,1 @@\n-bool LibraryCallKit::inline_native_notify_jvmti_funcs(address funcAddr, const char* funcName) {\n+bool LibraryCallKit::inline_native_notify_jvmti_funcs(address funcAddr, const char* funcName, bool is_start, bool is_end) {\n@@ -3098,1 +3079,1 @@\n-  Node* hide = _gvn.transform(argument(1)); \/\/ hide argument: true for begin and false for end of VTMS transition\n+  Node* hide = is_start ? ideal.ConI(0) : (is_end ? ideal.ConI(1) : _gvn.transform(argument(1)));\n@@ -3104,1 +3085,1 @@\n-    const TypeFunc* tf = OptoRuntime::notify_jvmti_Type();\n+    const TypeFunc* tf = OptoRuntime::notify_jvmti_vthread_Type();\n@@ -3106,1 +3087,0 @@\n-    Node* cond   = _gvn.transform(argument(2)); \/\/ firstMount or lastUnmount argument\n@@ -3109,1 +3089,1 @@\n-    make_runtime_call(RC_NO_LEAF, tf, funcAddr, funcName, TypePtr::BOTTOM, vt_oop, hide, cond);\n+    make_runtime_call(RC_NO_LEAF, tf, funcAddr, funcName, TypePtr::BOTTOM, vt_oop, hide);\n@@ -4596,1 +4576,1 @@\n-                     vtableEntry::method_offset_in_bytes();\n+                     in_bytes(vtableEntry::method_offset());\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":14,"deletions":34,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -271,1 +271,1 @@\n-  bool inline_native_notify_jvmti_funcs(address funcAddr, const char* funcName);\n+  bool inline_native_notify_jvmti_funcs(address funcAddr, const char* funcName, bool is_start, bool is_end);\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-\/\/  predicate                      predicate\n+\/\/  predicates                     predicates\n@@ -46,1 +46,1 @@\n-\/\/    endif                        predicate [clone]\n+\/\/    endif                        predicates [clone]\n@@ -155,3 +155,3 @@\n-      \/\/ Bailout if there are loop predicates from which there are additional control dependencies (i.e. from\n-      \/\/ loop entry 'entry') to previously partially peeled statements since this case is not handled and can lead\n-      \/\/ to wrong execution. Remove this bailout, once this is fixed.\n+      \/\/ Bailout if there are predicates from which there are additional control dependencies (i.e. from loop\n+      \/\/ entry 'entry') to previously partially peeled statements since this case is not handled and can lead\n+      \/\/ to a wrong execution. Remove this bailout, once this is fixed.\n@@ -190,3 +190,3 @@\n-  Node* predicate = find_predicate(entry);\n-  if (predicate == nullptr) {\n-    \/\/ No empty predicate\n+  Node* parse_predicate = find_parse_predicate(entry);\n+  if (parse_predicate == nullptr) {\n+    \/\/ No Parse Predicate.\n@@ -197,1 +197,1 @@\n-    \/\/ There is at least one empty predicate. When calling 'skip_loop_predicates' on each found empty predicate,\n+    \/\/ There is at least one Parse Predicate. When calling 'skip_related_predicates' on each found Parse Predicate,\n@@ -199,1 +199,1 @@\n-    Node* proj_before_first_empty_predicate = skip_loop_predicates(entry);\n+    Node* proj_before_first_parse_predicate = skip_related_predicates(entry);\n@@ -201,3 +201,3 @@\n-      predicate = find_predicate(proj_before_first_empty_predicate);\n-      if (predicate != nullptr) {\n-        proj_before_first_empty_predicate = skip_loop_predicates(predicate);\n+      parse_predicate = find_parse_predicate(proj_before_first_parse_predicate);\n+      if (parse_predicate != nullptr) {\n+        proj_before_first_parse_predicate = skip_related_predicates(parse_predicate);\n@@ -207,3 +207,3 @@\n-      predicate = find_predicate(proj_before_first_empty_predicate);\n-      if (predicate != nullptr) {\n-        proj_before_first_empty_predicate = skip_loop_predicates(predicate);\n+      parse_predicate = find_parse_predicate(proj_before_first_parse_predicate);\n+      if (parse_predicate != nullptr) {\n+        proj_before_first_parse_predicate = skip_related_predicates(parse_predicate);\n@@ -212,1 +212,1 @@\n-    assert(proj_true == proj_before_first_empty_predicate, \"must hold by construction if at least one predicate\");\n+    assert(proj_true == proj_before_first_parse_predicate, \"must hold by construction if at least one predicate\");\n@@ -334,1 +334,1 @@\n-  ProjNode* iffast = new IfTrueNode(iff);\n+  IfProjNode* iffast = new IfTrueNode(iff);\n@@ -336,1 +336,1 @@\n-  ProjNode* ifslow = new IfFalseNode(iff);\n+  IfProjNode* ifslow = new IfFalseNode(iff);\n@@ -346,3 +346,3 @@\n-  ProjNode* iffast_pred = iffast;\n-  ProjNode* ifslow_pred = ifslow;\n-  clone_predicates_to_unswitched_loop(loop, old_new, iffast_pred, ifslow_pred);\n+  IfProjNode* iffast_pred = iffast;\n+  IfProjNode* ifslow_pred = ifslow;\n+  clone_parse_and_assertion_predicates_to_unswitched_loop(loop, old_new, iffast_pred, ifslow_pred);\n","filename":"src\/hotspot\/share\/opto\/loopUnswitch.cpp","additions":22,"deletions":22,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -64,19 +64,17 @@\n-         MainHasNoPreLoop    = 1<<2,\n-         HasExactTripCount   = 1<<3,\n-         InnerLoop           = 1<<4,\n-         PartialPeelLoop     = 1<<5,\n-         PartialPeelFailed   = 1<<6,\n-         HasReductions       = 1<<7,\n-         WasSlpAnalyzed      = 1<<8,\n-         PassedSlpAnalysis   = 1<<9,\n-         DoUnrollOnly        = 1<<10,\n-         VectorizedLoop      = 1<<11,\n-         HasAtomicPostLoop   = 1<<12,\n-         IsMultiversioned    = 1<<13,\n-         StripMined          = 1<<14,\n-         SubwordLoop         = 1<<15,\n-         ProfileTripFailed   = 1<<16,\n-         LoopNestInnerLoop   = 1<< 17,\n-         LoopNestLongOuterLoop = 1<< 18,\n-         FlattenedArrays     = 1<<19};\n-\n+         MainHasNoPreLoop      = 1<<2,\n+         HasExactTripCount     = 1<<3,\n+         InnerLoop             = 1<<4,\n+         PartialPeelLoop       = 1<<5,\n+         PartialPeelFailed     = 1<<6,\n+         WasSlpAnalyzed        = 1<<7,\n+         PassedSlpAnalysis     = 1<<8,\n+         DoUnrollOnly          = 1<<9,\n+         VectorizedLoop        = 1<<10,\n+         HasAtomicPostLoop     = 1<<11,\n+         IsMultiversioned      = 1<<12,\n+         StripMined            = 1<<13,\n+         SubwordLoop           = 1<<14,\n+         ProfileTripFailed     = 1<<15,\n+         LoopNestInnerLoop     = 1<<16,\n+         LoopNestLongOuterLoop = 1<<17,\n+         FlattenedArrays       = 1<<18};\n@@ -111,1 +109,0 @@\n-  void mark_has_reductions() { _loop_flags |= HasReductions; }\n@@ -293,1 +290,0 @@\n-  bool is_reduction_loop() const { return (_loop_flags&HasReductions) == HasReductions; }\n@@ -950,10 +946,12 @@\n-  void copy_skeleton_predicates_to_main_loop_helper(Node* predicate, Node* init, Node* stride, IdealLoopTree* outer_loop, LoopNode* outer_main_head,\n-                                                    uint dd_main_head, const uint idx_before_pre_post, const uint idx_after_post_before_pre,\n-                                                    Node* zero_trip_guard_proj_main, Node* zero_trip_guard_proj_post, const Node_List &old_new);\n-  void copy_skeleton_predicates_to_main_loop(CountedLoopNode* pre_head, Node* init, Node* stride, IdealLoopTree* outer_loop, LoopNode* outer_main_head,\n-                                             uint dd_main_head, const uint idx_before_pre_post, const uint idx_after_post_before_pre,\n-                                             Node* zero_trip_guard_proj_main, Node* zero_trip_guard_proj_post, const Node_List &old_new);\n-  Node* clone_skeleton_predicate_and_initialize(Node* iff, Node* new_init, Node* new_stride, Node* predicate, Node* uncommon_proj, Node* control,\n-                                                IdealLoopTree* outer_loop, Node* input_proj);\n-  Node* clone_skeleton_predicate_bool(Node* iff, Node* new_init, Node* new_stride, Node* control);\n-  static bool skeleton_predicate_has_opaque(IfNode* iff);\n+  void copy_assertion_predicates_to_main_loop_helper(Node* predicate, Node* init, Node* stride, IdealLoopTree* outer_loop,\n+                                                     LoopNode* outer_main_head, uint dd_main_head,\n+                                                     uint idx_before_pre_post, uint idx_after_post_before_pre,\n+                                                     Node* zero_trip_guard_proj_main, Node* zero_trip_guard_proj_post,\n+                                                     const Node_List &old_new);\n+  void copy_assertion_predicates_to_main_loop(CountedLoopNode* pre_head, Node* init, Node* stride, IdealLoopTree* outer_loop,\n+                                              LoopNode* outer_main_head, uint dd_main_head, uint idx_before_pre_post,\n+                                              uint idx_after_post_before_pre, Node* zero_trip_guard_proj_main,\n+                                              Node* zero_trip_guard_proj_post, const Node_List& old_new);\n+  Node* clone_assertion_predicate_and_initialize(Node* iff, Node* new_init, Node* new_stride, Node* predicate,\n+                                                 Node* uncommon_proj, Node* control, IdealLoopTree* outer_loop,\n+                                                 Node* input_proj);\n@@ -962,7 +960,13 @@\n-  static void get_skeleton_predicates(Node* predicate, Unique_Node_List& list, bool get_opaque = false);\n-  void update_main_loop_skeleton_predicates(Node* ctrl, CountedLoopNode* loop_head, Node* init, int stride_con);\n-  void copy_skeleton_predicates_to_post_loop(LoopNode* main_loop_head, CountedLoopNode* post_loop_head, Node* init, Node* stride);\n-  void initialize_skeleton_predicates_for_peeled_loop(ProjNode* predicate, LoopNode* outer_loop_head, int dd_outer_loop_head,\n-                                                      Node* init, Node* stride, IdealLoopTree* outer_loop,\n-                                                      const uint idx_before_clone, const Node_List& old_new);\n-  void insert_loop_limit_check(ProjNode* limit_check_proj, Node* cmp_limit, Node* bol);\n+  Node* create_bool_from_template_assertion_predicate(Node* template_assertion_predicate, Node* new_init, Node* new_stride,\n+                                                      Node* control);\n+  static bool assertion_predicate_has_loop_opaque_node(IfNode* iff);\n+  static void get_assertion_predicates(Node* predicate, Unique_Node_List& list, bool get_opaque = false);\n+  void update_main_loop_assertion_predicates(Node* ctrl, CountedLoopNode* loop_head, Node* init, int stride_con);\n+  void copy_assertion_predicates_to_post_loop(LoopNode* main_loop_head, CountedLoopNode* post_loop_head, Node* init,\n+                                              Node* stride);\n+  void initialize_assertion_predicates_for_peeled_loop(IfProjNode* predicate_proj, LoopNode* outer_loop_head,\n+                                                       const int dd_outer_loop_head, Node* init, Node* stride,\n+                                                       IdealLoopTree* outer_loop, const uint idx_before_clone,\n+                                                       const Node_List& old_new);\n+  void insert_loop_limit_check_predicate(ParsePredicateSuccessProj* loop_limit_check_parse_proj, Node* cmp_limit,\n+                                         Node* bol);\n@@ -1227,1 +1231,1 @@\n-  void add_empty_predicate(Deoptimization::DeoptReason reason, Node* inner_head, IdealLoopTree* loop, SafePointNode* sfpt);\n+  void add_parse_predicate(Deoptimization::DeoptReason reason, Node* inner_head, IdealLoopTree* loop, SafePointNode* sfpt);\n@@ -1320,3 +1324,0 @@\n-  \/\/ Mark vector reduction candidates before loop unrolling\n-  void mark_reductions( IdealLoopTree *loop );\n-\n@@ -1348,3 +1349,3 @@\n-  ProjNode* create_new_if_for_predicate(ProjNode* cont_proj, Node* new_entry, Deoptimization::DeoptReason reason,\n-                                        int opcode, bool rewire_uncommon_proj_phi_inputs = false,\n-                                        bool if_cont_is_true_proj = true);\n+  IfProjNode* create_new_if_for_predicate(IfProjNode* cont_proj, Node* new_entry, Deoptimization::DeoptReason reason,\n+                                          int opcode, bool rewire_uncommon_proj_phi_inputs = false,\n+                                          bool if_cont_is_true_proj = true);\n@@ -1365,3 +1366,3 @@\n-  static Node* skip_all_loop_predicates(Node* entry);\n-  static Node* skip_loop_predicates(Node* entry);\n-  static ProjNode* next_predicate(ProjNode* predicate);\n+  static Node* skip_all_predicates(Node* entry);\n+  static Node* skip_related_predicates(Node* entry);\n+  static IfProjNode* next_predicate(IfProjNode* predicate_proj);\n@@ -1370,4 +1371,9 @@\n-  static ProjNode* find_predicate_insertion_point(Node* start_c, Deoptimization::DeoptReason reason);\n-\n-  class Predicates {\n-  public:\n+  static ParsePredicateSuccessProj* find_predicate_insertion_point(Node* start_c, Deoptimization::DeoptReason reason);\n+\n+  class ParsePredicates {\n+   private:\n+    ParsePredicateSuccessProj* _loop_predicate = nullptr;\n+    ParsePredicateSuccessProj* _profiled_loop_predicate = nullptr;\n+    ParsePredicateSuccessProj* _loop_limit_check_predicate = nullptr;\n+    Node* _first_predicate = nullptr;\n+   public:\n@@ -1375,1 +1381,1 @@\n-    Predicates(Node* entry);\n+    ParsePredicates(Node* entry);\n@@ -1377,3 +1383,3 @@\n-    \/\/ Proj of empty loop limit check predicate\n-    ProjNode* loop_limit_check() {\n-      return _loop_limit_check;\n+    \/\/ Proj of Loop Limit Check Parse Predicate.\n+    ParsePredicateSuccessProj* loop_limit_check_predicate() {\n+      return _loop_limit_check_predicate;\n@@ -1382,3 +1388,3 @@\n-    \/\/ Proj of empty profile predicate\n-    ProjNode* profile_predicate() {\n-      return _profile_predicate;\n+    \/\/ Proj of Profile Loop Parse Predicate.\n+    ParsePredicateSuccessProj* profiled_loop_predicate() {\n+      return _profiled_loop_predicate;\n@@ -1387,3 +1393,3 @@\n-    \/\/ Proj of empty predicate\n-    ProjNode* predicate() {\n-      return _predicate;\n+    \/\/ Proj of Loop Parse Predicate.\n+    ParsePredicateSuccessProj* loop_predicate() {\n+      return _loop_predicate;\n@@ -1392,3 +1398,3 @@\n-    \/\/ First control node above all predicates\n-    Node* skip_all() {\n-      return _entry_to_all_predicates;\n+    \/\/ Proj of first Parse Predicate when walking the graph down from root.\n+    Node* get_first_predicate() {\n+      return _first_predicate;\n@@ -1396,6 +1402,0 @@\n-\n-  private:\n-    ProjNode*_loop_limit_check = nullptr;\n-    ProjNode* _profile_predicate = nullptr;\n-    ProjNode* _predicate = nullptr;\n-    Node* _entry_to_all_predicates = nullptr;\n@@ -1405,1 +1405,1 @@\n-  static Node* find_predicate(Node* entry);\n+  static Node* find_parse_predicate(Node* entry);\n@@ -1415,4 +1415,4 @@\n-  bool loop_predication_impl_helper(IdealLoopTree *loop, ProjNode* proj, ProjNode *predicate_proj,\n-                                    CountedLoopNode *cl, ConNode* zero, Invariance& invar,\n-                                    Deoptimization::DeoptReason reason);\n-  bool loop_predication_should_follow_branches(IdealLoopTree *loop, ProjNode *predicate_proj, float& loop_trip_cnt);\n+  bool loop_predication_impl_helper(IdealLoopTree* loop, IfProjNode* if_proj,\n+                                    ParsePredicateSuccessProj* parse_predicate_proj, CountedLoopNode* cl, ConNode* zero,\n+                                    Invariance& invar, Deoptimization::DeoptReason reason);\n+  bool loop_predication_should_follow_branches(IdealLoopTree* loop, IfProjNode* predicate_proj, float& loop_trip_cnt);\n@@ -1422,10 +1422,5 @@\n-  ProjNode* insert_initial_skeleton_predicate(IfNode* iff, IdealLoopTree *loop,\n-                                              ProjNode* proj, ProjNode *predicate_proj,\n-                                              ProjNode* upper_bound_proj,\n-                                              int scale, Node* offset,\n-                                              Node* init, Node* limit, jint stride,\n-                                              Node* rng, bool& overflow,\n-                                              Deoptimization::DeoptReason reason);\n-  Node* add_range_check_predicate(IdealLoopTree* loop, CountedLoopNode* cl,\n-                                  Node* predicate_proj, int scale_con, Node* offset,\n-                                  Node* limit, jint stride_con, Node* value);\n+  IfProjNode* add_template_assertion_predicate(IfNode* iff, IdealLoopTree* loop, IfProjNode* if_proj, IfProjNode* predicate_proj,\n+                                               IfProjNode* upper_bound_proj, int scale, Node* offset, Node* init, Node* limit,\n+                                               jint stride, Node* rng, bool& overflow, Deoptimization::DeoptReason reason);\n+  Node* add_range_check_elimination_assertion_predicate(IdealLoopTree* loop, Node* predicate_proj, int scale_con,\n+                                                        Node* offset, Node* limit, jint stride_con, Node* value);\n@@ -1497,0 +1492,3 @@\n+  \/\/ Move UnorderedReduction out of loop if possible\n+  void move_unordered_reduction_out_of_loop(IdealLoopTree* loop);\n+\n@@ -1679,10 +1677,12 @@\n-  \/\/ Clone loop predicates to slow and fast loop when unswitching a loop\n-  void clone_predicates_to_unswitched_loop(IdealLoopTree* loop, Node_List& old_new, ProjNode*& iffast_pred, ProjNode*& ifslow_pred);\n-  ProjNode* clone_predicate_to_unswitched_loop(ProjNode* predicate_proj, Node* new_entry,\n-                                               Deoptimization::DeoptReason reason, bool slow_loop);\n-  void clone_skeleton_predicates_to_unswitched_loop(IdealLoopTree* loop, const Node_List& old_new, Deoptimization::DeoptReason reason,\n-                                                    ProjNode* old_predicate_proj, ProjNode* iffast_pred, ProjNode* ifslow_pred);\n-  ProjNode* clone_skeleton_predicate_for_unswitched_loops(Node* iff, ProjNode* predicate,\n-                                                          Deoptimization::DeoptReason reason,\n-                                                          ProjNode* output_proj);\n-  static void check_created_predicate_for_unswitching(const Node* new_entry) PRODUCT_RETURN;\n+  \/\/ Clone Parse Predicates to slow and fast loop when unswitching a loop\n+  void clone_parse_and_assertion_predicates_to_unswitched_loop(IdealLoopTree* loop, Node_List& old_new,\n+                                                               IfProjNode*& iffast_pred, IfProjNode*& ifslow_pred);\n+  IfProjNode* clone_parse_predicate_to_unswitched_loop(ParsePredicateSuccessProj* predicate_proj, Node* new_entry,\n+                                                       Deoptimization::DeoptReason reason, bool slow_loop);\n+  void clone_assertion_predicates_to_unswitched_loop(IdealLoopTree* loop, const Node_List& old_new,\n+                                                     Deoptimization::DeoptReason reason, IfProjNode* old_predicate_proj,\n+                                                     IfProjNode* iffast_pred, IfProjNode* ifslow_pred);\n+  IfProjNode* clone_assertion_predicate_for_unswitched_loops(Node* iff, IfProjNode* predicate,\n+                                                             Deoptimization::DeoptReason reason,\n+                                                             IfProjNode* output_proj);\n+  static void check_cloned_parse_predicate_for_unswitching(const Node* new_entry) PRODUCT_RETURN;\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":96,"deletions":96,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"opto\/vectornode.hpp\"\n@@ -2806,4 +2807,0 @@\n-    if (old->is_reduction()) {\n-      \/\/ Reduction flag is not copied by default. Copy it here when cloning the entire loop body.\n-      nnn->add_flag(Node::Flag_is_reduction);\n-    }\n@@ -3475,1 +3472,1 @@\n-\/\/               loop predicate\n+\/\/                 predicates\n@@ -3510,1 +3507,1 @@\n-\/\/               loop predicate\n+\/\/                predicates\n@@ -3551,1 +3548,1 @@\n-\/\/               loop predicate\n+\/\/                predicates\n@@ -3595,1 +3592,1 @@\n-\/\/               loop predicate\n+\/\/                predicates\n@@ -4308,0 +4305,185 @@\n+\n+\/\/ Having ReductionNodes in the loop is expensive. They need to recursively\n+\/\/ fold together the vector values, for every vectorized loop iteration. If\n+\/\/ we encounter the following pattern, we can vector accumulate the values\n+\/\/ inside the loop, and only have a single UnorderedReduction after the loop.\n+\/\/\n+\/\/ CountedLoop     init\n+\/\/          |        |\n+\/\/          +------+ | +-----------------------+\n+\/\/                 | | |                       |\n+\/\/                PhiNode (s)                  |\n+\/\/                  |                          |\n+\/\/                  |          Vector          |\n+\/\/                  |            |             |\n+\/\/               UnorderedReduction (first_ur) |\n+\/\/                  |                          |\n+\/\/                 ...         Vector          |\n+\/\/                  |            |             |\n+\/\/               UnorderedReduction (last_ur)  |\n+\/\/                       |                     |\n+\/\/                       +---------------------+\n+\/\/\n+\/\/ We patch the graph to look like this:\n+\/\/\n+\/\/ CountedLoop   identity_vector\n+\/\/         |         |\n+\/\/         +-------+ | +---------------+\n+\/\/                 | | |               |\n+\/\/                PhiNode (v)          |\n+\/\/                   |                 |\n+\/\/                   |         Vector  |\n+\/\/                   |           |     |\n+\/\/                 VectorAccumulator   |\n+\/\/                   |                 |\n+\/\/                  ...        Vector  |\n+\/\/                   |           |     |\n+\/\/      init       VectorAccumulator   |\n+\/\/        |          |     |           |\n+\/\/     UnorderedReduction  +-----------+\n+\/\/\n+\/\/ We turned the scalar (s) Phi into a vectorized one (v). In the loop, we\n+\/\/ use vector_accumulators, which do the same reductions, but only element\n+\/\/ wise. This is a single operation per vector_accumulator, rather than many\n+\/\/ for a UnorderedReduction. We can then reduce the last vector_accumulator\n+\/\/ after the loop, and also reduce the init value into it.\n+\/\/ We can not do this with all reductions. Some reductions do not allow the\n+\/\/ reordering of operations (for example float addition).\n+void PhaseIdealLoop::move_unordered_reduction_out_of_loop(IdealLoopTree* loop) {\n+  assert(!C->major_progress() && loop->is_counted() && loop->is_innermost(), \"sanity\");\n+\n+  \/\/ Find all Phi nodes with UnorderedReduction on backedge.\n+  CountedLoopNode* cl = loop->_head->as_CountedLoop();\n+  for (DUIterator_Fast jmax, j = cl->fast_outs(jmax); j < jmax; j++) {\n+    Node* phi = cl->fast_out(j);\n+    \/\/ We have a phi with a single use, and a UnorderedReduction on the backedge.\n+    if (!phi->is_Phi() || phi->outcnt() != 1 || !phi->in(2)->is_UnorderedReduction()) {\n+      continue;\n+    }\n+\n+    UnorderedReductionNode* last_ur = phi->in(2)->as_UnorderedReduction();\n+\n+    \/\/ Determine types\n+    const TypeVect* vec_t = last_ur->vect_type();\n+    uint vector_length    = vec_t->length();\n+    BasicType bt          = vec_t->element_basic_type();\n+    const Type* bt_t      = Type::get_const_basic_type(bt);\n+\n+    \/\/ Convert opcode from vector-reduction -> scalar -> normal-vector-op\n+    const int sopc        = VectorNode::scalar_opcode(last_ur->Opcode(), bt);\n+    const int vopc        = VectorNode::opcode(sopc, bt);\n+    if (!Matcher::match_rule_supported_vector(vopc, vector_length, bt)) {\n+        DEBUG_ONLY( last_ur->dump(); )\n+        assert(false, \"do not have normal vector op for this reduction\");\n+        continue; \/\/ not implemented -> fails\n+    }\n+\n+    \/\/ Traverse up the chain of UnorderedReductions, checking that it loops back to\n+    \/\/ the phi. Check that all UnorderedReductions only have a single use, except for\n+    \/\/ the last (last_ur), which only has phi as a use in the loop, and all other uses\n+    \/\/ are outside the loop.\n+    UnorderedReductionNode* current = last_ur;\n+    UnorderedReductionNode* first_ur = nullptr;\n+    while (true) {\n+      assert(current->is_UnorderedReduction(), \"sanity\");\n+\n+      \/\/ Expect no ctrl and a vector_input from within the loop.\n+      Node* ctrl = current->in(0);\n+      Node* vector_input = current->in(2);\n+      if (ctrl != nullptr || get_ctrl(vector_input) != cl) {\n+        DEBUG_ONLY( current->dump(1); )\n+        assert(false, \"reduction has ctrl or bad vector_input\");\n+        break; \/\/ Chain traversal fails.\n+      }\n+\n+      \/\/ Expect single use of UnorderedReduction, except for last_ur.\n+      if (current == last_ur) {\n+        \/\/ Expect all uses to be outside the loop, except phi.\n+        for (DUIterator_Fast kmax, k = current->fast_outs(kmax); k < kmax; k++) {\n+          Node* use = current->fast_out(k);\n+          if (use != phi && ctrl_or_self(use) == cl) {\n+            DEBUG_ONLY( current->dump(-1); )\n+            assert(false, \"reduction has use inside loop\");\n+            break; \/\/ Chain traversal fails.\n+          }\n+        }\n+      } else {\n+        if (current->outcnt() != 1) {\n+          break; \/\/ Chain traversal fails.\n+        }\n+      }\n+\n+      \/\/ Expect another UnorderedReduction or phi as the scalar input.\n+      Node* scalar_input = current->in(1);\n+      if (scalar_input->is_UnorderedReduction() &&\n+          scalar_input->Opcode() == current->Opcode()) {\n+        \/\/ Move up the UnorderedReduction chain.\n+        current = scalar_input->as_UnorderedReduction();\n+      } else if (scalar_input == phi) {\n+        \/\/ Chain terminates at phi.\n+        first_ur = current;\n+        current = nullptr;\n+        break; \/\/ Success.\n+      } else {\n+        DEBUG_ONLY( current->dump(1); )\n+        assert(false, \"scalar_input is neither phi nor a matchin reduction\");\n+        break; \/\/ Chain traversal fails.\n+      }\n+    }\n+    if (current != nullptr) {\n+      \/\/ Chain traversal was not successful.\n+      continue;\n+    }\n+    assert(first_ur != nullptr, \"must have successfully terminated chain traversal\");\n+\n+    Node* identity_scalar = ReductionNode::make_identity_con_scalar(_igvn, sopc, bt);\n+    set_ctrl(identity_scalar, C->root());\n+    VectorNode* identity_vector = VectorNode::scalar2vector(identity_scalar, vector_length, bt_t);\n+    register_new_node(identity_vector, C->root());\n+    assert(vec_t == identity_vector->vect_type(), \"matching vector type\");\n+    VectorNode::trace_new_vector(identity_vector, \"UnorderedReduction\");\n+\n+    \/\/ Turn the scalar phi into a vector phi.\n+    _igvn.rehash_node_delayed(phi);\n+    Node* init = phi->in(1); \/\/ Remember init before replacing it.\n+    phi->set_req_X(1, identity_vector, &_igvn);\n+    phi->as_Type()->set_type(vec_t);\n+    _igvn.set_type(phi, vec_t);\n+\n+    \/\/ Traverse down the chain of UnorderedReductions, and replace them with vector_accumulators.\n+    current = first_ur;\n+    while (true) {\n+      \/\/ Create vector_accumulator to replace current.\n+      Node* last_vector_accumulator = current->in(1);\n+      Node* vector_input            = current->in(2);\n+      VectorNode* vector_accumulator = VectorNode::make(vopc, last_vector_accumulator, vector_input, vec_t);\n+      register_new_node(vector_accumulator, cl);\n+      _igvn.replace_node(current, vector_accumulator);\n+      VectorNode::trace_new_vector(vector_accumulator, \"UnorderedReduction\");\n+      if (current == last_ur) {\n+        break;\n+      }\n+      current = vector_accumulator->unique_out()->as_UnorderedReduction();\n+    }\n+\n+    \/\/ Create post-loop reduction.\n+    Node* last_accumulator = phi->in(2);\n+    Node* post_loop_reduction = ReductionNode::make(sopc, nullptr, init, last_accumulator, bt);\n+\n+    \/\/ Take over uses of last_accumulator that are not in the loop.\n+    for (DUIterator i = last_accumulator->outs(); last_accumulator->has_out(i); i++) {\n+      Node* use = last_accumulator->out(i);\n+      if (use != phi && use != post_loop_reduction) {\n+        assert(ctrl_or_self(use) != cl, \"use must be outside loop\");\n+        use->replace_edge(last_accumulator, post_loop_reduction,  &_igvn);\n+        --i;\n+      }\n+    }\n+    register_new_node(post_loop_reduction, get_late_ctrl(post_loop_reduction, cl));\n+    VectorNode::trace_new_vector(post_loop_reduction, \"UnorderedReduction\");\n+\n+    assert(last_accumulator->outcnt() == 2, \"last_accumulator has 2 uses: phi and post_loop_reduction\");\n+    assert(post_loop_reduction->outcnt() > 0, \"should have taken over all non loop uses of last_accumulator\");\n+    assert(phi->outcnt() == 1, \"accumulator is the only use of phi\");\n+  }\n+}\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":190,"deletions":8,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -352,2 +352,2 @@\n-    aligned = ((arrayOopDesc::base_offset_in_bytes(t) + s_offs * element_size) % HeapWordSize == 0) &&\n-              ((arrayOopDesc::base_offset_in_bytes(t) + d_offs * element_size) % HeapWordSize == 0);\n+    aligned = ((arrayOopDesc::base_offset_in_bytes(t) + (uint)s_offs * element_size) % HeapWordSize == 0) &&\n+              ((arrayOopDesc::base_offset_in_bytes(t) + (uint)d_offs * element_size) % HeapWordSize == 0);\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -381,0 +381,5 @@\n+  \/\/ Check if given booltest condition is unsigned or not\n+  static inline bool is_unsigned_booltest_pred(int bt) {\n+    return ((bt & BoolTest::unsigned_compare) == BoolTest::unsigned_compare);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -798,1 +798,1 @@\n-      const jlong sign_bits_mask = ~(((jlong)CONST64(1) << (jlong)(BitsPerJavaLong - shift)) -1);\n+      const julong sign_bits_mask = ~(((julong)CONST64(1) << (julong)(BitsPerJavaLong - shift)) -1);\n@@ -1946,0 +1946,4 @@\n+  const TypeInteger* mask_t = phase->type(mask)->isa_integer(bt);\n+  if (mask_t == nullptr || phase->type(shift)->isa_integer(bt) == nullptr) {\n+    return false;\n+  }\n@@ -1950,3 +1954,1 @@\n-  const TypeInteger* mask_t = phase->type(mask)->isa_integer(bt);\n-  const TypeInteger* shift_t = phase->type(shift)->isa_integer(bt);\n-  if (mask_t == nullptr || shift_t == nullptr) {\n+  if (phase->type(shift)->isa_integer(bt) == nullptr) {\n@@ -1969,0 +1971,3 @@\n+      if (phase->type(shift)->isa_integer(bt) == nullptr) {\n+        return false;\n+      }\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -72,16 +72,1 @@\n-  int old_debug_idx = Compile::debug_idx();\n-  int new_debug_idx = old_debug_idx + 1;\n-  if (new_debug_idx > 0) {\n-    \/\/ Arrange that the lowest five decimal digits of _debug_idx\n-    \/\/ will repeat those of _idx. In case this is somehow pathological,\n-    \/\/ we continue to assign negative numbers (!) consecutively.\n-    const int mod = 100000;\n-    int bump = (int)(_idx - new_debug_idx) % mod;\n-    if (bump < 0) {\n-      bump += mod;\n-    }\n-    assert(bump >= 0 && bump < mod, \"\");\n-    new_debug_idx += bump;\n-  }\n-  Compile::set_debug_idx(new_debug_idx);\n-  set_debug_idx(new_debug_idx);\n+  \/\/ The decimal digits of _debug_idx are <compile_id> followed by 10 digits of <_idx>\n@@ -90,0 +75,2 @@\n+  uint64_t new_debug_idx = (uint64_t)C->compile_id() * 10000000000 + _idx;\n+  set_debug_idx(new_debug_idx);\n@@ -94,2 +81,2 @@\n-  if (BreakAtNode != 0 && (_debug_idx == BreakAtNode || (int)_idx == BreakAtNode)) {\n-    tty->print_cr(\"BreakAtNode: _idx=%d _debug_idx=%d\", _idx, _debug_idx);\n+  if (BreakAtNode != 0 && (_debug_idx == BreakAtNode || (uint64_t)_idx == BreakAtNode)) {\n+    tty->print_cr(\"BreakAtNode: _idx=%d _debug_idx=\" UINT64_FORMAT, _idx, _debug_idx);\n@@ -525,4 +512,0 @@\n-  if (n->is_reduction()) {\n-    \/\/ Do not copy reduction information. This must be explicitly set by the calling code.\n-    n->remove_flag(Node::Flag_is_reduction);\n-  }\n@@ -631,1 +614,1 @@\n-    compile->remove_skeleton_predicate_opaq(this);\n+    compile->remove_template_assertion_predicate_opaq(this);\n@@ -2507,2 +2490,2 @@\n-    if (orig->debug_idx() == BreakAtNode || (int)orig->_idx == BreakAtNode) {\n-      tty->print_cr(\"BreakAtNode: _idx=%d _debug_idx=%d orig._idx=%d orig._debug_idx=%d\",\n+    if (orig->debug_idx() == BreakAtNode || (uintx)orig->_idx == BreakAtNode) {\n+      tty->print_cr(\"BreakAtNode: _idx=%d _debug_idx=\" UINT64_FORMAT \" orig._idx=%d orig._debug_idx=\" UINT64_FORMAT,\n@@ -2540,1 +2523,1 @@\n-    st->print(\"  [%d]\",debug_idx());\n+    st->print(\"  [\" UINT64_FORMAT \"]\", debug_idx());\n@@ -2556,1 +2539,1 @@\n-    st->print(\"  [%d]\",debug_idx());\n+    st->print(\"  [\" UINT64_FORMAT \"]\", debug_idx());\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":10,"deletions":27,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -157,0 +157,1 @@\n+class ReductionNode;\n@@ -171,0 +172,1 @@\n+class UnorderedReductionNode;\n@@ -585,0 +587,6 @@\n+    \/\/ Flip swapped edges flag.\n+    if (has_swapped_edges()) {\n+      remove_flag(Node::Flag_has_swapped_edges);\n+    } else {\n+      add_flag(Node::Flag_has_swapped_edges);\n+    }\n@@ -723,1 +731,3 @@\n-      DEFINE_CLASS_ID(Con, Type, 9)\n+        DEFINE_CLASS_ID(Reduction, Vector, 9)\n+          DEFINE_CLASS_ID(UnorderedReduction, Reduction, 0)\n+      DEFINE_CLASS_ID(Con, Type, 10)\n@@ -796,1 +806,1 @@\n-    Flag_is_reduction                = 1 << 11,\n+    Flag_has_swapped_edges           = 1 << 11,\n@@ -951,0 +961,1 @@\n+  DEFINE_CLASS_QUERY(Reduction)\n@@ -961,0 +972,1 @@\n+  DEFINE_CLASS_QUERY(UnorderedReduction)\n@@ -1018,4 +1030,2 @@\n-\n-  \/\/ An arithmetic node which accumulates a data in a loop.\n-  \/\/ It must have the loop's phi as input and provide a def to the phi.\n-  bool is_reduction() const { return (_flags & Flag_is_reduction) != 0; }\n+  \/\/ The node's original edge position is swapped.\n+  bool has_swapped_edges() const { return (_flags & Flag_has_swapped_edges) != 0; }\n@@ -1257,3 +1267,3 @@\n-  int  _debug_idx;                     \/\/ Unique value assigned to every node.\n-  int   debug_idx() const              { return _debug_idx; }\n-  void  set_debug_idx( int debug_idx ) { _debug_idx = debug_idx; }\n+  uint64_t _debug_idx;                 \/\/ Unique value assigned to every node.\n+  uint64_t debug_idx() const           { return _debug_idx; }\n+  void set_debug_idx(uint64_t debug_idx) { _debug_idx = debug_idx; }\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":19,"deletions":9,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -704,1 +704,1 @@\n-            add_empty_predicates();\n+            add_parse_predicates();\n@@ -1804,1 +1804,1 @@\n-          \/\/ Add loop predicate for the special case when\n+          \/\/ Add Parse Predicates for the special case when\n@@ -1806,1 +1806,1 @@\n-          add_empty_predicates();\n+          add_parse_predicates();\n@@ -2322,1 +2322,1 @@\n-    int offset = MethodData::rtm_state_offset_in_bytes();\n+    int offset = in_bytes(MethodData::rtm_state_offset());\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -773,1 +773,1 @@\n-  bool makes_backward_branch = false;\n+  bool makes_backward_branch = (default_dest <= bci());\n@@ -864,1 +864,1 @@\n-  bool makes_backward_branch = false;\n+  bool makes_backward_branch = (default_dest <= bci());\n@@ -1072,1 +1072,1 @@\n-    Node* cmp = _gvn.transform(new CmpUNode(val, _gvn.intcon(most_freq.hi() - most_freq.lo())));\n+    Node* cmp = _gvn.transform(new CmpUNode(val, _gvn.intcon(java_subtract(most_freq.hi(), most_freq.lo()))));\n@@ -1530,0 +1530,19 @@\n+\n+\/\/ Give up if too few (or too many, in which case the sum will overflow) counts to be meaningful.\n+\/\/ We also check that individual counters are positive first, otherwise the sum can become positive.\n+\/\/ (check for saturation, integer overflow, and immature counts)\n+static bool counters_are_meaningful(int counter1, int counter2, int min) {\n+  \/\/ check for saturation, including \"uint\" values too big to fit in \"int\"\n+  if (counter1 < 0 || counter2 < 0) {\n+    return false;\n+  }\n+  \/\/ check for integer overflow of the sum\n+  int64_t sum = (int64_t)counter1 + (int64_t)counter2;\n+  STATIC_ASSERT(sizeof(counter1) < sizeof(sum));\n+  if (sum > INT_MAX) {\n+    return false;\n+  }\n+  \/\/ check if mature\n+  return (counter1 + counter2) >= min;\n+}\n+\n@@ -1556,0 +1575,2 @@\n+    \/\/ NOTE: saturated UINT_MAX values become negative,\n+    \/\/ as do counts above INT_MAX.\n@@ -1563,0 +1584,1 @@\n+    \/\/ NOTE: overflow for positive values is clamped at INT_MAX\n@@ -1566,0 +1588,2 @@\n+  \/\/ At this point, saturation or overflow is indicated by INT_MAX\n+  \/\/ or a negative value.\n@@ -1569,1 +1593,1 @@\n-  if (taken < 0 || not_taken < 0 || taken + not_taken < 40) {\n+  if (!counters_are_meaningful(taken, not_taken, 40)) {\n@@ -1598,1 +1622,1 @@\n-         \"Bad frequency assignment in if\");\n+         \"Bad frequency assignment in if cnt=%g prob=%g taken=%d not_taken=%d\", cnt, prob, taken, not_taken);\n@@ -2294,1 +2318,1 @@\n-    add_empty_predicates();\n+    add_parse_predicates();\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":30,"deletions":6,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -1785,0 +1785,3 @@\n+    \/\/ Patterns:\n+    \/\/ ConstraintCast+ -> Sub\n+    \/\/ ConstraintCast+ -> Phi\n@@ -1786,22 +1789,3 @@\n-      for (DUIterator_Fast i2max, i2 = use->fast_outs(i2max); i2 < i2max; i2++) {\n-        Node* u = use->fast_out(i2);\n-        if (u->is_Phi() || u->is_Sub()) {\n-          \/\/ Phi (.., CastII, ..) or Sub(Cast(x), x)\n-          _worklist.push(u);\n-        } else if (u->is_ConstraintCast()) {\n-          \/\/ Follow cast-chains down to Sub: Sub( CastII(CastII(x)), x)\n-          \/\/ This case is quite rare. Let's BFS-traverse casts, to find Subs:\n-          ResourceMark rm;\n-          Unique_Node_List casts;\n-          casts.push(u); \/\/ start traversal\n-          for (uint j = 0; j < casts.size(); ++j) {\n-            Node* cast = casts.at(j); \/\/ for every cast\n-            for (DUIterator_Fast kmax, k = cast->fast_outs(kmax); k < kmax; k++) {\n-              Node* cast_use = cast->fast_out(k);\n-              if (cast_use->is_ConstraintCast()) {\n-                casts.push(cast_use); \/\/ traverse this cast also\n-              } else if (cast_use->is_Sub()) {\n-                _worklist.push(cast_use); \/\/ found Sub\n-              }\n-            }\n-          }\n+      auto push_phi_or_sub_uses_to_worklist = [&](Node* n){\n+        if (n->is_Phi() || n->is_Sub()) {\n+          _worklist.push(n);\n@@ -1809,1 +1793,2 @@\n-      }\n+      };\n+      ConstraintCastNode::visit_uncasted_uses(use, push_phi_or_sub_uses_to_worklist);\n@@ -2009,0 +1994,7 @@\n+  \/\/ CCP worklist is placed on a local arena, so that we can allow ResourceMarks on \"Compile::current()->resource_arena()\".\n+  \/\/ We also do not want to put the worklist on \"Compile::current()->comp_arena()\", as that one only gets de-allocated after\n+  \/\/ Compile is over. The local arena gets de-allocated at the end of its scope.\n+  ResourceArea local_arena(mtCompiler);\n+  Unique_Node_List worklist(&local_arena);\n+  DEBUG_ONLY(Unique_Node_List worklist_verify(&local_arena);)\n+\n@@ -2010,2 +2002,0 @@\n-  Unique_Node_List worklist;\n-  DEBUG_ONLY(Unique_Node_List worklist_verify;)\n@@ -2206,0 +2196,1 @@\n+\/\/ Pattern: parent -> LShift (use) -> ConstraintCast* -> And\n@@ -2210,5 +2201,4 @@\n-    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n-      Node* and_node = use->fast_out(i);\n-      uint and_node_op = and_node->Opcode();\n-      if (and_node_op == Op_AndI || and_node_op == Op_AndL) {\n-        push_if_not_bottom_type(worklist, and_node);\n+    auto push_and_uses_to_worklist = [&](Node* n){\n+      uint opc = n->Opcode();\n+      if (opc == Op_AndI || opc == Op_AndL) {\n+        push_if_not_bottom_type(worklist, n);\n@@ -2216,1 +2206,2 @@\n-    }\n+    };\n+    ConstraintCastNode::visit_uncasted_uses(use, push_and_uses_to_worklist);\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":22,"deletions":31,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -115,3 +115,4 @@\n-address OptoRuntime::_notify_jvmti_object_alloc                   = nullptr;\n-address OptoRuntime::_notify_jvmti_mount                          = nullptr;\n-address OptoRuntime::_notify_jvmti_unmount                        = nullptr;\n+address OptoRuntime::_notify_jvmti_vthread_start                  = nullptr;\n+address OptoRuntime::_notify_jvmti_vthread_end                    = nullptr;\n+address OptoRuntime::_notify_jvmti_vthread_mount                  = nullptr;\n+address OptoRuntime::_notify_jvmti_vthread_unmount                = nullptr;\n@@ -159,3 +160,4 @@\n-  gen(env, _notify_jvmti_object_alloc      , notify_jvmti_object_alloc_Type, SharedRuntime::notify_jvmti_object_alloc, 0, true, false);\n-  gen(env, _notify_jvmti_mount             , notify_jvmti_Type            , SharedRuntime::notify_jvmti_mount,   0 , true, false);\n-  gen(env, _notify_jvmti_unmount           , notify_jvmti_Type            , SharedRuntime::notify_jvmti_unmount, 0 , true, false);\n+  gen(env, _notify_jvmti_vthread_start     , notify_jvmti_vthread_Type    , SharedRuntime::notify_jvmti_vthread_start, 0, true, false);\n+  gen(env, _notify_jvmti_vthread_end       , notify_jvmti_vthread_Type    , SharedRuntime::notify_jvmti_vthread_end,   0, true, false);\n+  gen(env, _notify_jvmti_vthread_mount     , notify_jvmti_vthread_Type    , SharedRuntime::notify_jvmti_vthread_mount, 0, true, false);\n+  gen(env, _notify_jvmti_vthread_unmount   , notify_jvmti_vthread_Type    , SharedRuntime::notify_jvmti_vthread_unmount, 0, true, false);\n@@ -486,1 +488,1 @@\n-const TypeFunc *OptoRuntime::notify_jvmti_object_alloc_Type() {\n+const TypeFunc *OptoRuntime::notify_jvmti_vthread_Type() {\n@@ -488,7 +490,4 @@\n-  const Type **fields = TypeTuple::fields(1);\n-  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n-\n-   \/\/ create result type (range)\n-   fields = TypeTuple::fields(1);\n-   fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ Returned oop\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ VirtualThread oop\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ jboolean\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2,fields);\n@@ -496,1 +495,4 @@\n-   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  \/\/ no result type needed\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n@@ -498,1 +500,1 @@\n-   return TypeFunc::make(domain, range);\n+  return TypeFunc::make(domain,range);\n@@ -1679,18 +1681,0 @@\n-#if INCLUDE_JVMTI\n-const TypeFunc *OptoRuntime::notify_jvmti_Type() {\n-  \/\/ create input type (domain)\n-  const Type **fields = TypeTuple::fields(3);\n-  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ VirtualThread oop\n-  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ jboolean\n-  fields[TypeFunc::Parms+2] = TypeInt::BOOL;        \/\/ jboolean\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+3,fields);\n-\n-  \/\/ no result type needed\n-  fields = TypeTuple::fields(1);\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n-  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n-\n-  return TypeFunc::make(domain,range);\n-}\n-#endif\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":18,"deletions":34,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -140,3 +140,4 @@\n-  static address _notify_jvmti_object_alloc;\n-  static address _notify_jvmti_mount;\n-  static address _notify_jvmti_unmount;\n+  static address _notify_jvmti_vthread_start;\n+  static address _notify_jvmti_vthread_end;\n+  static address _notify_jvmti_vthread_mount;\n+  static address _notify_jvmti_vthread_unmount;\n@@ -219,3 +220,4 @@\n-  static address notify_jvmti_object_alloc()             { return _notify_jvmti_object_alloc; }\n-  static address notify_jvmti_mount()                    { return _notify_jvmti_mount; }\n-  static address notify_jvmti_unmount()                  { return _notify_jvmti_unmount; }\n+  static address notify_jvmti_vthread_start()            { return _notify_jvmti_vthread_start; }\n+  static address notify_jvmti_vthread_end()              { return _notify_jvmti_vthread_end; }\n+  static address notify_jvmti_vthread_mount()            { return _notify_jvmti_vthread_mount; }\n+  static address notify_jvmti_vthread_unmount()          { return _notify_jvmti_vthread_unmount; }\n@@ -310,2 +312,1 @@\n-  static const TypeFunc* notify_jvmti_object_alloc_Type();\n-  static const TypeFunc* notify_jvmti_Type();\n+  static const TypeFunc* notify_jvmti_vthread_Type();\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -105,2 +105,2 @@\n-        assert(skeleton_predicate_has_opaque(m->as_If()), \"opaque node not reachable from if?\");\n-        Node* bol = clone_skeleton_predicate_bool(m, nullptr, nullptr, m->in(0));\n+        assert(assertion_predicate_has_loop_opaque_node(m->as_If()), \"opaque node not reachable from if?\");\n+        Node* bol = create_bool_from_template_assertion_predicate(m, nullptr, nullptr, m->in(0));\n","filename":"src\/hotspot\/share\/opto\/split_if.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1178,1 +1178,1 @@\n-      p = 10 * p;\n+      p = java_multiply(10, p);\n@@ -1217,1 +1217,1 @@\n-  kit.add_empty_predicates();\n+  kit.add_parse_predicates();\n@@ -1302,2 +1302,2 @@\n-  \/\/ Add loop predicate first.\n-  kit.add_empty_predicates();\n+  \/\/ Add Parse Predicates first.\n+  kit.add_parse_predicates();\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -179,1 +179,1 @@\n-      return new AddINode(in1, phase->intcon(-i->get_con()));\n+      return new AddINode(in1, phase->intcon(java_negate(i->get_con())));\n@@ -209,1 +209,1 @@\n-      Node* neg_c0 = phase->intcon(-c0);\n+      Node* neg_c0 = phase->intcon(java_negate(c0));\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1751,1 +1751,1 @@\n-            (juint)(_lo - min) >= (juint)(max - _hi)) {\n+            ((juint)_lo - min) >= ((juint)max - _hi)) {\n@@ -2061,2 +2061,2 @@\n-  julong nrange = _hi - _lo;\n-  julong orange = ohi - olo;\n+  julong nrange = (julong)_hi - _lo;\n+  julong orange = (julong)ohi - olo;\n@@ -3376,1 +3376,2 @@\n-          _hash_computed(0), _exact_klass_computed(0), _is_loaded_computed(0) {\n+          _hash(0), _exact_klass(nullptr) {\n+  DEBUG_ONLY(_initialized = true);\n@@ -3381,1 +3382,1 @@\n-          _hash_computed(0), _exact_klass_computed(0), _is_loaded_computed(0) {\n+          _hash(0), _exact_klass(nullptr) {\n@@ -3385,0 +3386,1 @@\n+  initialize();\n@@ -3387,0 +3389,5 @@\n+void TypePtr::InterfaceSet::initialize() {\n+  compute_hash();\n+  compute_exact_klass();\n+  DEBUG_ONLY(_initialized = true;)\n+}\n@@ -3422,3 +3429,12 @@\n-int TypePtr::InterfaceSet::hash() const {\n-  if (_hash_computed) {\n-    return _hash;\n+bool TypePtr::InterfaceSet::eq(ciInstanceKlass* k) const {\n+  assert(k->is_loaded(), \"should be loaded\");\n+  GrowableArray<ciInstanceKlass *>* interfaces = k->as_instance_klass()->transitive_interfaces();\n+  if (_list.length() != interfaces->length()) {\n+    return false;\n+  }\n+  for (int i = 0; i < interfaces->length(); i++) {\n+    bool found = false;\n+    _list.find_sorted<ciKlass*, compare>(interfaces->at(i), found);\n+    if (!found) {\n+      return false;\n+    }\n@@ -3426,2 +3442,6 @@\n-  const_cast<InterfaceSet*>(this)->compute_hash();\n-  assert(_hash_computed, \"should be computed now\");\n+  return true;\n+}\n+\n+\n+int TypePtr::InterfaceSet::hash() const {\n+  assert(_initialized, \"must be\");\n@@ -3437,1 +3457,0 @@\n-  _hash_computed = 1;\n@@ -3445,1 +3464,1 @@\n-void TypePtr::InterfaceSet::dump(outputStream *st) const {\n+void TypePtr::InterfaceSet::dump(outputStream* st) const {\n@@ -3465,0 +3484,1 @@\n+#ifdef ASSERT\n@@ -3466,1 +3486,0 @@\n-#ifdef DEBUG\n@@ -3473,1 +3492,1 @@\n-#endif\n+#endif\n@@ -3501,0 +3520,2 @@\n+  result.initialize();\n+#ifdef ASSERT\n@@ -3502,2 +3523,1 @@\n-#ifdef DEBUG\n-    assert(result.contains(_list.at(i)), \"missing\");\n+    assert(result._list.contains(_list.at(i)), \"missing\");\n@@ -3507,1 +3527,1 @@\n-    assert(result.contains(other._list.at(i)), \"missing\");\n+    assert(result._list.contains(other._list.at(i)), \"missing\");\n@@ -3539,0 +3559,2 @@\n+  result.initialize();\n+#ifdef ASSERT\n@@ -3540,2 +3562,1 @@\n-#ifdef DEBUG\n-    assert(!other._list.contains(_list.at(i)) || result.contains(_list.at(i)), \"missing\");\n+    assert(!other._list.contains(_list.at(i)) || result._list.contains(_list.at(i)), \"missing\");\n@@ -3545,1 +3566,1 @@\n-    assert(!_list.contains(other._list.at(i)) || result.contains(other._list.at(i)), \"missing\");\n+    assert(!_list.contains(other._list.at(i)) || result._list.contains(other._list.at(i)), \"missing\");\n@@ -3556,5 +3577,1 @@\n-  if (_exact_klass_computed) {\n-    return _exact_klass;\n-  }\n-  const_cast<InterfaceSet*>(this)->compute_exact_klass();\n-  assert(_exact_klass_computed, \"should be computed now\");\n+  assert(_initialized, \"must be\");\n@@ -3566,1 +3583,0 @@\n-    _exact_klass_computed = 1;\n@@ -3572,2 +3588,2 @@\n-    ciKlass* interface = _list.at(i);\n-    if (eq(interfaces(interface, false, true, false, trust_interfaces))) {\n+    ciInstanceKlass* interface = _list.at(i)->as_instance_klass();\n+    if (eq(interface)) {\n@@ -3575,1 +3591,1 @@\n-      res = _list.at(i);\n+      res = interface;\n@@ -3578,1 +3594,0 @@\n-  _exact_klass_computed = 1;\n@@ -3582,11 +3597,2 @@\n-bool TypePtr::InterfaceSet::is_loaded() const {\n-  if (_is_loaded_computed) {\n-    return _is_loaded;\n-  }\n-  const_cast<InterfaceSet*>(this)->compute_is_loaded();\n-  assert(_is_loaded_computed, \"should be computed now\");\n-  return _is_loaded;\n-}\n-\n-void TypePtr::InterfaceSet::compute_is_loaded() {\n-  _is_loaded_computed = 1;\n+#ifdef ASSERT\n+void TypePtr::InterfaceSet::verify_is_loaded() const {\n@@ -3595,4 +3601,1 @@\n-    if (!interface->is_loaded()) {\n-      _is_loaded = false;\n-      return;\n-    }\n+    assert(interface->is_loaded(), \"Interface not loaded\");\n@@ -3600,1 +3603,1 @@\n-  _is_loaded = true;\n+#endif\n@@ -3614,0 +3617,5 @@\n+#ifdef ASSERT\n+  if (klass() != nullptr && klass()->is_loaded()) {\n+    interfaces.verify_is_loaded();\n+  }\n+#endif\n@@ -4151,3 +4159,1 @@\n-    ciKlass* k = _klass;\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n-    if (_interfaces.eq(interfaces)) {\n+    if (_interfaces.eq(_klass->as_instance_klass())) {\n@@ -4222,1 +4228,1 @@\n-      GrowableArray<ciInstanceKlass *> *k_interfaces = k->as_instance_klass()->transitive_interfaces();\n+      GrowableArray<ciInstanceKlass *>* k_interfaces = k->as_instance_klass()->transitive_interfaces();\n@@ -4573,1 +4579,0 @@\n-  InterfaceSet subtype_interfaces;\n@@ -4781,5 +4786,2 @@\n-    ciKlass* k = ik;\n-    TypePtr::InterfaceSet interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n-    assert(k == ik, \"\");\n-    if (interfaces.eq(_interfaces)) {\n-      Compile *C = Compile::current();\n+    if (_interfaces.eq(ik)) {\n+      Compile* C = Compile::current();\n@@ -5971,2 +5973,1 @@\n-    ciKlass* k = _klass;\n-    if (_interfaces.eq(TypePtr::interfaces(k, true, false, true, ignore_interfaces))) {\n+    if (_interfaces.eq(_klass->as_instance_klass())) {\n@@ -6169,4 +6170,1 @@\n-        ciKlass* sub_k = sub;\n-        TypePtr::InterfaceSet sub_interfaces = TypePtr::interfaces(sub_k, true, false, false, ignore_interfaces);\n-        assert(sub_k == sub, \"\");\n-        if (sub_interfaces.eq(_interfaces)) {\n+        if (_interfaces.eq(sub)) {\n@@ -6419,4 +6417,1 @@\n-        ciKlass *sub_k = sub;\n-        TypePtr::InterfaceSet sub_interfaces = TypePtr::interfaces(sub_k, true, false, false, ignore_interfaces);\n-        assert(sub_k == sub, \"\");\n-        if (sub_interfaces.eq(_interfaces)) {\n+        if (_interfaces.eq(sub)) {\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":61,"deletions":66,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -923,6 +923,0 @@\n-    void raw_add(ciKlass* interface);\n-    void add(ciKlass* interface);\n-    void verify() const;\n-    uint _hash_computed:1;\n-    uint _exact_klass_computed:1;\n-    uint _is_loaded_computed:1;\n@@ -931,1 +925,6 @@\n-    bool _is_loaded;\n+    DEBUG_ONLY(bool _initialized;)\n+\n+    void initialize();\n+    void raw_add(ciKlass* interface);\n+    void add(ciKlass* interface);\n+    void verify() const NOT_DEBUG_RETURN;\n@@ -938,0 +937,1 @@\n+    bool eq(ciInstanceKlass* k) const;\n@@ -939,1 +939,1 @@\n-    void dump(outputStream *st) const;\n+    void dump(outputStream* st) const;\n@@ -951,1 +951,1 @@\n-    inline void operator delete( void* ptr ) {\n+    inline void operator delete(void* ptr) {\n@@ -955,3 +955,1 @@\n-    bool is_loaded() const;\n-\n-    static int compare(ciKlass* const &, ciKlass* const & k2);\n+    void verify_is_loaded() const NOT_DEBUG_RETURN;\n@@ -959,1 +957,1 @@\n-    void compute_is_loaded();\n+    static int compare(ciKlass* const& k1, ciKlass* const& k2);\n@@ -1250,1 +1248,1 @@\n-  virtual bool  is_loaded() const { return klass()->is_loaded() && _interfaces.is_loaded(); }\n+  virtual bool  is_loaded() const { return klass()->is_loaded(); }\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":12,"deletions":14,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -302,0 +302,1 @@\n+    VectorSet visited;\n@@ -304,1 +305,2 @@\n-    Node* result = expand_vbox_node_helper(vbox, vect, vec_box->box_type(), vec_box->vec_type());\n+    Node* result = expand_vbox_node_helper(vbox, vect, vec_box->box_type(),\n+                                           vec_box->vec_type(), visited);\n@@ -314,1 +316,23 @@\n-                                           const TypeVect* vect_type) {\n+                                           const TypeVect* vect_type,\n+                                           VectorSet &visited) {\n+  \/\/ JDK-8304948 shows an example that there may be a cycle in the graph.\n+  if (visited.test_set(vbox->_idx)) {\n+    assert(vbox->is_Phi(), \"should be phi\");\n+    return vbox; \/\/ already visited\n+  }\n+\n+  \/\/ Handle the case when the allocation input to VectorBoxNode is a Proj.\n+  \/\/ This is the normal case before expanding.\n+  if (vbox->is_Proj() && vbox->in(0)->Opcode() == Op_VectorBoxAllocate) {\n+    VectorBoxAllocateNode* vbox_alloc = static_cast<VectorBoxAllocateNode*>(vbox->in(0));\n+    return expand_vbox_alloc_node(vbox_alloc, vect, box_type, vect_type);\n+  }\n+\n+  \/\/ Handle the case when both the allocation input and vector input to\n+  \/\/ VectorBoxNode are Phi. This case is generated after the transformation of\n+  \/\/ Phi: Phi (VectorBox1 VectorBox2) => VectorBox (Phi1 Phi2).\n+  \/\/ With this optimization, the relative two allocation inputs of VectorBox1 and\n+  \/\/ VectorBox2 are gathered into Phi1 now. Similarly, the original vector\n+  \/\/ inputs of two VectorBox nodes are in Phi2.\n+  \/\/\n+  \/\/ See PhiNode::merge_through_phi in cfg.cpp for more details.\n@@ -317,3 +341,6 @@\n-    Node* new_phi = new PhiNode(vbox->as_Phi()->region(), box_type);\n-      Node* new_box = expand_vbox_node_helper(vbox->in(i), vect->in(i), box_type, vect_type);\n-      new_phi->set_req(i, new_box);\n+      Node* new_box = expand_vbox_node_helper(vbox->in(i), vect->in(i),\n+                                              box_type, vect_type, visited);\n+      if (!new_box->is_Phi()) {\n+        C->initial_gvn()->hash_delete(vbox);\n+        vbox->set_req(i, new_box);\n+      }\n@@ -322,11 +349,11 @@\n-    new_phi = C->initial_gvn()->transform(new_phi);\n-    return new_phi;\n-  } else if (vbox->is_Phi() && (vect->is_Vector() || vect->is_LoadVector())) {\n-    \/\/ Handle the case when the allocation input to VectorBoxNode is a phi\n-    \/\/ but the vector input is not, which can definitely be the case if the\n-    \/\/ vector input has been value-numbered. It seems to be safe to do by\n-    \/\/ construction because VectorBoxNode and VectorBoxAllocate come in a\n-    \/\/ specific order as a result of expanding an intrinsic call. After that, if\n-    \/\/ any of the inputs to VectorBoxNode are value-numbered they can only\n-    \/\/ move up and are guaranteed to dominate.\n-    Node* new_phi = new PhiNode(vbox->as_Phi()->region(), box_type);\n+    return C->initial_gvn()->transform(vbox);\n+  }\n+\n+  \/\/ Handle the case when the allocation input to VectorBoxNode is a phi\n+  \/\/ but the vector input is not, which can definitely be the case if the\n+  \/\/ vector input has been value-numbered. It seems to be safe to do by\n+  \/\/ construction because VectorBoxNode and VectorBoxAllocate come in a\n+  \/\/ specific order as a result of expanding an intrinsic call. After that, if\n+  \/\/ any of the inputs to VectorBoxNode are value-numbered they can only\n+  \/\/ move up and are guaranteed to dominate.\n+  if (vbox->is_Phi() && (vect->is_Vector() || vect->is_LoadVector())) {\n@@ -334,2 +361,6 @@\n-      Node* new_box = expand_vbox_node_helper(vbox->in(i), vect, box_type, vect_type);\n-      new_phi->set_req(i, new_box);\n+      Node* new_box = expand_vbox_node_helper(vbox->in(i), vect,\n+                                              box_type, vect_type, visited);\n+      if (!new_box->is_Phi()) {\n+        C->initial_gvn()->hash_delete(vbox);\n+        vbox->set_req(i, new_box);\n+      }\n@@ -337,9 +368,1 @@\n-    new_phi = C->initial_gvn()->transform(new_phi);\n-    return new_phi;\n-  } else if (vbox->is_Proj() && vbox->in(0)->Opcode() == Op_VectorBoxAllocate) {\n-    VectorBoxAllocateNode* vbox_alloc = static_cast<VectorBoxAllocateNode*>(vbox->in(0));\n-    return expand_vbox_alloc_node(vbox_alloc, vect, box_type, vect_type);\n-  } else {\n-    assert(!vbox->is_Phi(), \"\");\n-    \/\/ TODO: assert that expanded vbox is initialized with the same value (vect).\n-    return vbox; \/\/ already expanded\n+    return C->initial_gvn()->transform(vbox);\n@@ -347,0 +370,4 @@\n+\n+  assert(!vbox->is_Phi(), \"should be expanded\");\n+  \/\/ TODO: assert that expanded vbox is initialized with the same value (vect).\n+  return vbox; \/\/ already expanded\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":54,"deletions":27,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+#include \"prims\/foreignGlobals.hpp\"\n@@ -2980,1 +2981,1 @@\n-    ssize_t count = os::write(defaultStream::output_fd(), s, (int)len);\n+    bool dummy = os::write(defaultStream::output_fd(), s, len);\n@@ -3127,3 +3128,3 @@\n-JVM_ENTRY(void, JVM_Sleep(JNIEnv* env, jclass threadClass, jlong millis))\n-  if (millis < 0) {\n-    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"timeout value is negative\");\n+JVM_ENTRY(void, JVM_Sleep(JNIEnv* env, jclass threadClass, jlong nanos))\n+  if (nanos < 0) {\n+    THROW_MSG(vmSymbols::java_lang_IllegalArgumentException(), \"nanosecond timeout value out of range\");\n@@ -3140,1 +3141,1 @@\n-  HOTSPOT_THREAD_SLEEP_BEGIN(millis);\n+  HOTSPOT_THREAD_SLEEP_BEGIN(nanos \/ NANOSECS_PER_MILLISEC);\n@@ -3142,1 +3143,1 @@\n-  if (millis == 0) {\n+  if (nanos == 0) {\n@@ -3147,1 +3148,1 @@\n-    if (!thread->sleep(millis)) { \/\/ interrupted\n+    if (!thread->sleep_nanos(nanos)) { \/\/ interrupted\n@@ -3550,0 +3551,4 @@\n+JVM_LEAF(jboolean, JVM_IsForeignLinkerSupported(void))\n+  return ForeignGlobals::is_foreign_linker_supported() ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3993,3 +3998,1 @@\n-\/\/ If notifications are disabled then just update the VTMS transition bit and return.\n-\/\/ Otherwise, the bit is updated in the given jvmtiVTMSTransitionDisabler function call.\n-JVM_ENTRY(void, JVM_VirtualThreadMount(JNIEnv* env, jobject vthread, jboolean hide, jboolean first_mount))\n+JVM_ENTRY(void, JVM_VirtualThreadStart(JNIEnv* env, jobject vthread))\n@@ -4001,4 +4004,15 @@\n-  if (!JvmtiVTMSTransitionDisabler::VTMS_notify_jvmti_events()) {\n-    thread->set_is_in_VTMS_transition(hide);\n-    oop vt = JNIHandles::resolve_external_guard(vthread);\n-    java_lang_Thread::set_is_in_VTMS_transition(vt, hide);\n+  if (JvmtiVTMSTransitionDisabler::VTMS_notify_jvmti_events()) {\n+    JvmtiVTMSTransitionDisabler::VTMS_vthread_start(vthread);\n+  } else {\n+    \/\/ set VTMS transition bit value in JavaThread and java.lang.VirtualThread object\n+    JvmtiVTMSTransitionDisabler::set_is_in_VTMS_transition(thread, vthread, false);\n+  }\n+#else\n+  fatal(\"Should only be called with JVMTI enabled\");\n+#endif\n+JVM_END\n+\n+JVM_ENTRY(void, JVM_VirtualThreadEnd(JNIEnv* env, jobject vthread))\n+#if INCLUDE_JVMTI\n+  if (!DoJVMTIVirtualThreadTransitions) {\n+    assert(!JvmtiExport::can_support_virtual_threads(), \"sanity check\");\n@@ -4007,2 +4021,2 @@\n-  if (hide) {\n-   JvmtiVTMSTransitionDisabler::VTMS_mount_begin(vthread, first_mount);\n+  if (JvmtiVTMSTransitionDisabler::VTMS_notify_jvmti_events()) {\n+    JvmtiVTMSTransitionDisabler::VTMS_vthread_end(vthread);\n@@ -4010,1 +4024,2 @@\n-   JvmtiVTMSTransitionDisabler::VTMS_mount_end(vthread, first_mount);\n+    \/\/ set VTMS transition bit value in JavaThread and java.lang.VirtualThread object\n+    JvmtiVTMSTransitionDisabler::set_is_in_VTMS_transition(thread, vthread, true);\n@@ -4018,2 +4033,2 @@\n-\/\/ Otherwise, the bit is updated in the given jvmtiVTMSTransitionDisabler function call below.\n-JVM_ENTRY(void, JVM_VirtualThreadUnmount(JNIEnv* env, jobject vthread, jboolean hide, jboolean last_unmount))\n+\/\/ Otherwise, the bit is updated in the given jvmtiVTMSTransitionDisabler function call.\n+JVM_ENTRY(void, JVM_VirtualThreadMount(JNIEnv* env, jobject vthread, jboolean hide))\n@@ -4025,4 +4040,17 @@\n-  if (!JvmtiVTMSTransitionDisabler::VTMS_notify_jvmti_events()) {\n-    thread->set_is_in_VTMS_transition(hide);\n-    oop vt = JNIHandles::resolve_external_guard(vthread);\n-    java_lang_Thread::set_is_in_VTMS_transition(vt, hide);\n+  if (JvmtiVTMSTransitionDisabler::VTMS_notify_jvmti_events()) {\n+    JvmtiVTMSTransitionDisabler::VTMS_vthread_mount(vthread, hide);\n+  } else {\n+    \/\/ set VTMS transition bit value in JavaThread and java.lang.VirtualThread object\n+    JvmtiVTMSTransitionDisabler::set_is_in_VTMS_transition(thread, vthread, hide);\n+  }\n+#else\n+  fatal(\"Should only be called with JVMTI enabled\");\n+#endif\n+JVM_END\n+\n+\/\/ If notifications are disabled then just update the VTMS transition bit and return.\n+\/\/ Otherwise, the bit is updated in the given jvmtiVTMSTransitionDisabler function call below.\n+JVM_ENTRY(void, JVM_VirtualThreadUnmount(JNIEnv* env, jobject vthread, jboolean hide))\n+#if INCLUDE_JVMTI\n+  if (!DoJVMTIVirtualThreadTransitions) {\n+    assert(!JvmtiExport::can_support_virtual_threads(), \"sanity check\");\n@@ -4031,2 +4059,2 @@\n-  if (hide) {\n-   JvmtiVTMSTransitionDisabler::VTMS_unmount_begin(vthread, last_unmount);\n+  if (JvmtiVTMSTransitionDisabler::VTMS_notify_jvmti_events()) {\n+    JvmtiVTMSTransitionDisabler::VTMS_vthread_unmount(vthread, hide);\n@@ -4034,1 +4062,2 @@\n-   JvmtiVTMSTransitionDisabler::VTMS_unmount_end(vthread, last_unmount);\n+    \/\/ set VTMS transition bit value in JavaThread and java.lang.VirtualThread object\n+    JvmtiVTMSTransitionDisabler::set_is_in_VTMS_transition(thread, vthread, hide);\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":55,"deletions":26,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -1193,3 +1193,9 @@\n-  if (thread_oop != nullptr && thread_oop->is_a(vmClasses::BaseVirtualThread_klass())) {\n-    \/\/ No support for virtual threads (yet).\n-    return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n+  bool is_virtual = thread_oop != nullptr && thread_oop->is_a(vmClasses::BaseVirtualThread_klass());\n+\n+  if (is_virtual && !is_JavaThread_current(java_thread, thread_oop)) {\n+    if (!is_vthread_suspended(thread_oop, java_thread)) {\n+      return JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n+    }\n+    if (java_thread == nullptr) { \/\/ unmounted virtual thread\n+      return JVMTI_ERROR_OPAQUE_FRAME;\n+    }\n@@ -1880,4 +1886,0 @@\n-  if (thread_obj != nullptr && thread_obj->is_a(vmClasses::BaseVirtualThread_klass())) {\n-    \/\/ No support for virtual threads (yet).\n-    return JVMTI_ERROR_OPAQUE_FRAME;\n-  }\n@@ -1887,0 +1889,17 @@\n+  bool is_virtual = thread_obj != nullptr && thread_obj->is_a(vmClasses::BaseVirtualThread_klass());\n+\n+  if (is_virtual) {\n+    if (!is_JavaThread_current(java_thread, thread_obj)) {\n+      if (!is_vthread_suspended(thread_obj, java_thread)) {\n+        return JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n+      }\n+      if (java_thread == nullptr) { \/\/ unmounted virtual thread\n+        return JVMTI_ERROR_OPAQUE_FRAME;\n+      }\n+    }\n+  } else { \/\/ platform thread\n+    if (java_thread != current_thread && !java_thread->is_suspended() &&\n+        !java_thread->is_carrier_thread_suspended()) {\n+      return JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n+    }\n+  }\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":26,"deletions":7,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -832,2 +832,0 @@\n-  \/\/ We do not check the EnableThreadSMRExtraValidityChecks option\n-  \/\/ for this includes() call because JVM\/TI's spec is tighter.\n@@ -875,2 +873,0 @@\n-  \/\/ We do not check the EnableThreadSMRExtraValidityChecks option\n-  \/\/ for this includes() call because JVM\/TI's spec is tighter.\n@@ -1070,4 +1066,0 @@\n-\/\/ This flag is read by C2 during VM internal objects allocation\n-bool JvmtiExport::_should_notify_object_alloc = false;\n-\n-\n@@ -1483,5 +1475,7 @@\n-  if (JvmtiExport::can_support_virtual_threads() && thread->threadObj()->is_a(vmClasses::BoundVirtualThread_klass())) {\n-    \/\/ Check for VirtualThreadStart event instead.\n-    HandleMark hm(thread);\n-    Handle vthread(thread, thread->threadObj());\n-    JvmtiExport::post_vthread_start((jthread)vthread.raw_value());\n+  if (thread->threadObj()->is_a(vmClasses::BoundVirtualThread_klass())) {\n+    if (JvmtiExport::can_support_virtual_threads()) {\n+      \/\/ Check for VirtualThreadStart event instead.\n+      HandleMark hm(thread);\n+      Handle vthread(thread, thread->threadObj());\n+      JvmtiExport::post_vthread_start((jthread)vthread.raw_value());\n+    }\n@@ -1527,5 +1521,7 @@\n-  if (JvmtiExport::can_support_virtual_threads() && thread->threadObj()->is_a(vmClasses::BoundVirtualThread_klass())) {\n-    \/\/ Check for VirtualThreadEnd event instead.\n-    HandleMark hm(thread);\n-    Handle vthread(thread, thread->threadObj());\n-    JvmtiExport::post_vthread_end((jthread)vthread.raw_value());\n+  if (thread->threadObj()->is_a(vmClasses::BoundVirtualThread_klass())) {\n+    if (JvmtiExport::can_support_virtual_threads()) {\n+      \/\/ Check for VirtualThreadEnd event instead.\n+      HandleMark hm(thread);\n+      Handle vthread(thread, thread->threadObj());\n+      JvmtiExport::post_vthread_end((jthread)vthread.raw_value());\n+    }\n@@ -2599,2 +2595,0 @@\n-  assert(!thread->is_in_any_VTMS_transition(), \"dynamic code generated events are not allowed in any VTMS transition\");\n-\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":14,"deletions":20,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -1290,1 +1290,1 @@\n-            Symbol* name = caller->constants()->name_ref_at(bss_index_in_pool);\n+            Symbol* name = caller->constants()->name_ref_at(bss_index_in_pool, Bytecodes::_invokedynamic);\n@@ -1297,1 +1297,1 @@\n-            Symbol* type = caller->constants()->signature_ref_at(bss_index_in_pool);\n+            Symbol* type = caller->constants()->signature_ref_at(bss_index_in_pool, Bytecodes::_invokedynamic);\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -930,12 +930,14 @@\n-    ThreadsListHandle tlh;\n-    JavaThread* thr = nullptr;\n-    oop java_thread = nullptr;\n-    (void) tlh.cv_internal_thread_to_JavaThread(jthread, &thr, &java_thread);\n-    if (java_thread != nullptr) {\n-      \/\/ This is a valid oop.\n-      if (thr != nullptr) {\n-        \/\/ The JavaThread is alive.\n-        Parker* p = thr->parker();\n-        HOTSPOT_THREAD_UNPARK((uintptr_t) p);\n-        p->unpark();\n-      }\n+    oop thread_oop = JNIHandles::resolve_non_null(jthread);\n+    \/\/ Get the JavaThread* stored in the java.lang.Thread object _before_\n+    \/\/ the embedded ThreadsListHandle is constructed so we know if the\n+    \/\/ early life stage of the JavaThread* is protected. We use acquire\n+    \/\/ here to ensure that if we see a non-nullptr value, then we also\n+    \/\/ see the main ThreadsList updates from the JavaThread* being added.\n+    FastThreadsListHandle ftlh(thread_oop, java_lang_Thread::thread_acquire(thread_oop));\n+    JavaThread* thr = ftlh.protected_java_thread();\n+    if (thr != nullptr) {\n+      \/\/ The still live JavaThread* is protected by the FastThreadsListHandle\n+      \/\/ so it is safe to access.\n+      Parker* p = thr->parker();\n+      HOTSPOT_THREAD_UNPARK((uintptr_t) p);\n+      p->unpark();\n@@ -943,2 +945,1 @@\n-  } \/\/ ThreadsListHandle is destroyed here.\n-\n+  } \/\/ FastThreadsListHandle is destroyed here.\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":15,"deletions":14,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -115,0 +115,3 @@\n+#if INCLUDE_ZGC\n+#include \"gc\/z\/zAddress.inline.hpp\"\n+#endif \/\/ INCLUDE_ZGC\n@@ -371,1 +374,2 @@\n-    JVMCIEnv jvmciEnv(thread, env, __FILE__, __LINE__);\n+    \/\/ Enter the JVMCI env that will be used by the CompileBroker.\n+    JVMCIEnv jvmciEnv(thread, __FILE__, __LINE__);\n@@ -403,1 +407,5 @@\n-    return Universe::heap()->is_in(p);\n+    if (ZGenerational) {\n+      return ZHeap::heap()->is_old(to_zaddress(p));\n+    } else {\n+      return Universe::heap()->is_in(p);\n+    }\n@@ -1212,3 +1220,3 @@\n-  mh->clear_not_c1_compilable();\n-  mh->clear_not_c2_compilable();\n-  mh->clear_not_c2_osr_compilable();\n+  mh->clear_is_not_c1_compilable();\n+  mh->clear_is_not_c2_compilable();\n+  mh->clear_is_not_c2_osr_compilable();\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -527,0 +527,2 @@\n+  { \"RefDiscoveryPolicy\",           JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n+  { \"MetaspaceReclaimPolicy\",       JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n@@ -1479,3 +1481,0 @@\n-      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n@@ -1487,3 +1486,0 @@\n-\n-\/\/ NOTE: set_use_compressed_klass_ptrs() must be called after calling\n-\/\/ set_use_compressed_oops().\n@@ -1492,25 +1488,2 @@\n-  \/\/ On some architectures, the use of UseCompressedClassPointers implies the use of\n-  \/\/ UseCompressedOops. The reason is that the rheap_base register of said platforms\n-  \/\/ is reused to perform some optimized spilling, in order to use rheap_base as a\n-  \/\/ temp register. But by treating it as any other temp register, spilling can typically\n-  \/\/ be completely avoided instead. So it is better not to perform this trick. And by\n-  \/\/ not having that reliance, large heaps, or heaps not supporting compressed oops,\n-  \/\/ can still use compressed class pointers.\n-  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {\n-    if (UseCompressedClassPointers) {\n-      warning(\"UseCompressedClassPointers requires UseCompressedOops\");\n-    }\n-    FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-  } else {\n-    \/\/ Turn on UseCompressedClassPointers too\n-    if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-      FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-    }\n-    \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-    if (UseCompressedClassPointers) {\n-      if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-        warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n-    }\n-  }\n+  assert(!UseCompressedClassPointers || CompressedClassSpaceSize <= KlassEncodingMetaspaceMax,\n+         \"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n@@ -1538,3 +1511,0 @@\n-\n-  \/\/ set_use_compressed_klass_ptrs() must be called after calling\n-  \/\/ set_use_compressed_oops().\n@@ -1679,3 +1649,0 @@\n-          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-            FLAG_SET_ERGO(UseCompressedClassPointers, false);\n-          }\n@@ -1950,1 +1917,8 @@\n-#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64)\n+\n+#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM)\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    FLAG_SET_CMDLINE(LockingMode, LM_LEGACY);\n+    warning(\"New lightweight locking not supported on this platform\");\n+  }\n+#endif\n+\n@@ -1952,0 +1926,10 @@\n+    if (FLAG_IS_CMDLINE(LockingMode) && LockingMode != LM_MONITOR) {\n+      jio_fprintf(defaultStream::error_stream(),\n+                  \"Conflicting -XX:+UseHeavyMonitors and -XX:LockingMode=%d flags\", LockingMode);\n+      return false;\n+    }\n+    FLAG_SET_CMDLINE(LockingMode, LM_MONITOR);\n+  }\n+\n+#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64) && !defined(S390)\n+  if (LockingMode == LM_MONITOR) {\n@@ -1953,1 +1937,1 @@\n-                \"UseHeavyMonitors is not fully implemented on this architecture\");\n+                \"LockingMode == 0 (LM_MONITOR) is not fully implemented on this architecture\");\n@@ -1958,1 +1942,1 @@\n-  if (UseHeavyMonitors && UseRTMForStackLocks) {\n+  if (LockingMode == LM_MONITOR && UseRTMForStackLocks) {\n@@ -1960,1 +1944,1 @@\n-                \"-XX:+UseHeavyMonitors and -XX:+UseRTMForStackLocks are mutually exclusive\");\n+                \"LockingMode == 0 (LM_MONITOR) and -XX:+UseRTMForStackLocks are mutually exclusive\");\n@@ -1965,1 +1949,1 @@\n-  if (VerifyHeavyMonitors && !UseHeavyMonitors) {\n+  if (VerifyHeavyMonitors && LockingMode != LM_MONITOR) {\n@@ -1967,1 +1951,1 @@\n-                \"-XX:+VerifyHeavyMonitors requires -XX:+UseHeavyMonitors\");\n+                \"-XX:+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\");\n@@ -1970,1 +1954,0 @@\n-\n@@ -3141,0 +3124,5 @@\n+\n+    \/\/ String deduplication may cause CDS to iterate the strings in different order from one\n+    \/\/ run to another which resulting in non-determinstic CDS archives.\n+    \/\/ Disable UseStringDeduplication while dumping CDS archive.\n+    UseStringDeduplication = false;\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":32,"deletions":44,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -1730,1 +1730,1 @@\n-          if (mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n+          if (LockingMode == LM_LEGACY && mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -269,10 +269,10 @@\n-    static int size_of_deoptimized_frame_offset_in_bytes() { return offset_of(UnrollBlock, _size_of_deoptimized_frame); }\n-    static int caller_adjustment_offset_in_bytes()         { return offset_of(UnrollBlock, _caller_adjustment);         }\n-    static int number_of_frames_offset_in_bytes()          { return offset_of(UnrollBlock, _number_of_frames);          }\n-    static int frame_sizes_offset_in_bytes()               { return offset_of(UnrollBlock, _frame_sizes);               }\n-    static int total_frame_sizes_offset_in_bytes()         { return offset_of(UnrollBlock, _total_frame_sizes);         }\n-    static int frame_pcs_offset_in_bytes()                 { return offset_of(UnrollBlock, _frame_pcs);                 }\n-    static int counter_temp_offset_in_bytes()              { return offset_of(UnrollBlock, _counter_temp);              }\n-    static int initial_info_offset_in_bytes()              { return offset_of(UnrollBlock, _initial_info);              }\n-    static int unpack_kind_offset_in_bytes()               { return offset_of(UnrollBlock, _unpack_kind);               }\n-    static int sender_sp_temp_offset_in_bytes()            { return offset_of(UnrollBlock, _sender_sp_temp);            }\n+    static ByteSize size_of_deoptimized_frame_offset() { return byte_offset_of(UnrollBlock, _size_of_deoptimized_frame); }\n+    static ByteSize caller_adjustment_offset()         { return byte_offset_of(UnrollBlock, _caller_adjustment);         }\n+    static ByteSize number_of_frames_offset()          { return byte_offset_of(UnrollBlock, _number_of_frames);          }\n+    static ByteSize frame_sizes_offset()               { return byte_offset_of(UnrollBlock, _frame_sizes);               }\n+    static ByteSize total_frame_sizes_offset()         { return byte_offset_of(UnrollBlock, _total_frame_sizes);         }\n+    static ByteSize frame_pcs_offset()                 { return byte_offset_of(UnrollBlock, _frame_pcs);                 }\n+    static ByteSize counter_temp_offset()              { return byte_offset_of(UnrollBlock, _counter_temp);              }\n+    static ByteSize initial_info_offset()              { return byte_offset_of(UnrollBlock, _initial_info);              }\n+    static ByteSize unpack_kind_offset()               { return byte_offset_of(UnrollBlock, _unpack_kind);               }\n+    static ByteSize sender_sp_temp_offset()            { return byte_offset_of(UnrollBlock, _sender_sp_temp);            }\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -701,4 +701,0 @@\n-  product(bool, PostVirtualThreadCompatibleLifecycleEvents, true, EXPERIMENTAL, \\\n-               \"Post virtual thread ThreadStart and ThreadEnd events for \"  \\\n-               \"virtual thread unaware agents\")                             \\\n-                                                                            \\\n@@ -737,2 +733,3 @@\n-          \"off). The check is performed on GuaranteedSafepointInterval \"    \\\n-          \"or AsyncDeflationInterval.\")                                     \\\n+          \"off). The check is performed on GuaranteedSafepointInterval, \"   \\\n+          \"AsyncDeflationInterval or GuaranteedAsyncDeflationInterval, \"    \\\n+          \"whichever is lower.\")                                            \\\n@@ -977,3 +974,0 @@\n-  product(bool, EnableThreadSMRExtraValidityChecks, true, DIAGNOSTIC,       \\\n-             \"Enable Thread SMR extra validity checks\")                     \\\n-                                                                            \\\n@@ -1443,3 +1437,0 @@\n-  product(ccstr, MetaspaceReclaimPolicy, \"balanced\", DIAGNOSTIC,            \\\n-          \"options: balanced, aggressive\")                                  \\\n-                                                                            \\\n@@ -2025,0 +2016,7 @@\n+                                                                            \\\n+  product(int, LockingMode, LM_LEGACY, EXPERIMENTAL,                        \\\n+          \"Select locking mode: \"                                           \\\n+          \"0: monitors only (LM_MONITOR), \"                                 \\\n+          \"1: monitors & legacy stack-locking (LM_LEGACY, default), \"       \\\n+          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT)\")         \\\n+          range(0, 2)                                                       \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":11,"deletions":13,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -74,0 +74,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -494,2 +495,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack(this) {\n@@ -745,2 +747,1 @@\n-  OrderAccess::release();\n-  java_lang_Thread::set_thread(threadObj(), nullptr);\n+  java_lang_Thread::release_set_thread(threadObj(), nullptr);\n@@ -998,0 +999,1 @@\n+  assert(LockingMode != LM_LIGHTWEIGHT, \"should not be called with new lightweight locking\");\n@@ -1101,3 +1103,6 @@\n-  \/\/ Interrupt thread so it will wake up from a potential wait()\/sleep()\/park()\n-  java_lang_Thread::set_interrupted(threadObj(), true);\n-  this->interrupt();\n+  oop vt_oop = vthread();\n+  if (vt_oop == nullptr || !vt_oop->is_a(vmClasses::BaseVirtualThread_klass())) {\n+    \/\/ Interrupt thread so it will wake up from a potential wait()\/sleep()\/park()\n+    java_lang_Thread::set_interrupted(threadObj(), true);\n+    this->interrupt();\n+  }\n@@ -1391,0 +1396,4 @@\n+\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    lock_stack().oops_do(f);\n+  }\n@@ -1519,0 +1528,1 @@\n+  st->fill_to(60);\n@@ -1524,1 +1534,1 @@\n-      if (java_lang_Thread::is_daemon(thread_obj)) st->print(\" daemon\");\n+      st->print(java_lang_Thread::is_daemon(thread_obj) ? \" daemon\" : \"       \");\n@@ -1532,2 +1542,3 @@\n-  st->print(\", stack(\" PTR_FORMAT \",\" PTR_FORMAT \")\",\n-            p2i(stack_end()), p2i(stack_base()));\n+  st->print(\", stack(\" PTR_FORMAT \",\" PTR_FORMAT \") (\" PROPERFMT \")\",\n+            p2i(stack_end()), p2i(stack_base()),\n+            PROPERFMTARGS(stack_size()));\n@@ -1660,1 +1671,0 @@\n-  java_lang_Thread::set_thread(thread_oop(), this);\n@@ -1676,0 +1686,5 @@\n+  \/\/ Publish the JavaThread* in java.lang.Thread after the JavaThread* is\n+  \/\/ on a ThreadsList. We don't want to wait for the release when the\n+  \/\/ Theads_lock is dropped somewhere in the caller since the JavaThread*\n+  \/\/ is already visible to JVM\/TI via the ThreadsList.\n+  java_lang_Thread::release_set_thread(thread_oop(), this);\n@@ -1984,0 +1999,12 @@\n+\/\/ Internal convenience function for millisecond resolution sleeps.\n+bool JavaThread::sleep(jlong millis) {\n+  jlong nanos;\n+  if (millis > max_jlong \/ NANOUNITS_PER_MILLIUNIT) {\n+    \/\/ Conversion to nanos would overflow, saturate at max\n+    nanos = max_jlong;\n+  } else {\n+    nanos = millis * NANOUNITS_PER_MILLIUNIT;\n+  }\n+  return sleep_nanos(nanos);\n+}\n+\n@@ -1987,1 +2014,1 @@\n-bool JavaThread::sleep(jlong millis) {\n+bool JavaThread::sleep_nanos(jlong nanos) {\n@@ -1989,0 +2016,1 @@\n+  assert(nanos >= 0, \"nanos are in range\");\n@@ -2002,0 +2030,2 @@\n+  jlong nanos_remaining = nanos;\n+\n@@ -2008,1 +2038,1 @@\n-    if (millis <= 0) {\n+    if (nanos_remaining <= 0) {\n@@ -2015,1 +2045,1 @@\n-      slp->park(millis);\n+      slp->park_nanos(nanos_remaining);\n@@ -2026,1 +2056,1 @@\n-      millis -= (newtime - prevtime) \/ NANOSECS_PER_MILLISEC;\n+      nanos_remaining -= (newtime - prevtime);\n@@ -2098,3 +2128,0 @@\n-\n-  java_lang_Thread::set_thread(thread_oop(), target); \/\/ isAlive == true now\n-\n@@ -2113,0 +2140,5 @@\n+  \/\/ Publish the JavaThread* in java.lang.Thread after the JavaThread* is\n+  \/\/ on a ThreadsList. We don't want to wait for the release when the\n+  \/\/ Theads_lock is dropped when the 'mu' destructor is run since the\n+  \/\/ JavaThread* is already visible to JVM\/TI via the ThreadsList.\n+  java_lang_Thread::release_set_thread(thread_oop(), target); \/\/ isAlive == true now\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":50,"deletions":18,"binary":false,"changes":68,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -516,3 +517,1 @@\n-  virtual bool is_active_Java_thread() const {\n-    return on_thread_list() && !is_terminated();\n-  }\n+  virtual bool is_active_Java_thread() const;\n@@ -1150,0 +1149,1 @@\n+  bool sleep_nanos(jlong nanos);\n@@ -1155,0 +1155,13 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_offset()      { return byte_offset_of(JavaThread, _lock_stack); }\n+  \/\/ Those offsets are used in code generators to access the LockStack that is embedded in this\n+  \/\/ JavaThread structure. Those accesses are relative to the current thread, which\n+  \/\/ is typically in a dedicated register.\n+  static ByteSize lock_stack_top_offset()  { return lock_stack_offset() + LockStack::top_offset(); }\n+  static ByteSize lock_stack_base_offset() { return lock_stack_offset() + LockStack::base_offset(); }\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":16,"deletions":3,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -551,0 +551,22 @@\n+  uint expected_num_workers() const {\n+    uint workers = 0;\n+\n+    if (SymbolTable::rehash_table_expects_safepoint_rehashing()) {\n+      workers++;\n+    }\n+\n+    if (StringTable::rehash_table_expects_safepoint_rehashing()) {\n+      workers++;\n+    }\n+\n+    if (InlineCacheBuffer::needs_update_inline_caches()) {\n+      workers++;\n+    }\n+\n+    if (_do_lazy_roots) {\n+      workers++;\n+    }\n+\n+    return MAX2<uint>(1, workers);\n+  }\n+\n@@ -605,1 +627,2 @@\n-  if (cleanup_workers != nullptr) {\n+  const uint expected_num_workers = cleanup.expected_num_workers();\n+  if (cleanup_workers != nullptr && expected_num_workers > 1) {\n@@ -607,1 +630,2 @@\n-    cleanup_workers->run_task(&cleanup);\n+    const uint num_workers = MIN2(expected_num_workers, cleanup_workers->active_workers());\n+    cleanup_workers->run_task(&cleanup, num_workers);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":26,"deletions":2,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -634,6 +634,5 @@\n-JRT_ENTRY(void, SharedRuntime::notify_jvmti_object_alloc(oopDesc* o, JavaThread* current))\n-  Handle h = Handle(current, o);\n-  if (JvmtiExport::should_post_vm_object_alloc()) {\n-    JvmtiExport::post_vm_object_alloc(current, o);\n-  }\n-  current->set_vm_result(h());\n+JRT_ENTRY(void, SharedRuntime::notify_jvmti_vthread_start(oopDesc* vt, jboolean hide, JavaThread* current))\n+  assert(hide == JNI_FALSE, \"must be VTMS transition finish\");\n+  jobject vthread = JNIHandles::make_local(const_cast<oopDesc*>(vt));\n+  JvmtiVTMSTransitionDisabler::VTMS_vthread_start(vthread);\n+  JNIHandles::destroy_local(vthread);\n@@ -642,1 +641,2 @@\n-JRT_ENTRY(void, SharedRuntime::notify_jvmti_mount(oopDesc* vt, jboolean hide, jboolean first_mount, JavaThread* current))\n+JRT_ENTRY(void, SharedRuntime::notify_jvmti_vthread_end(oopDesc* vt, jboolean hide, JavaThread* current))\n+  assert(hide == JNI_TRUE, \"must be VTMS transition start\");\n@@ -644,7 +644,1 @@\n-\n-  if (hide) {\n-    JvmtiVTMSTransitionDisabler::VTMS_mount_begin(vthread, first_mount);\n-  } else {\n-    JvmtiVTMSTransitionDisabler::VTMS_mount_end(vthread, first_mount);\n-  }\n-\n+  JvmtiVTMSTransitionDisabler::VTMS_vthread_end(vthread);\n@@ -654,1 +648,1 @@\n-JRT_ENTRY(void, SharedRuntime::notify_jvmti_unmount(oopDesc* vt, jboolean hide, jboolean last_unmount, JavaThread* current))\n+JRT_ENTRY(void, SharedRuntime::notify_jvmti_vthread_mount(oopDesc* vt, jboolean hide, JavaThread* current))\n@@ -656,0 +650,3 @@\n+  JvmtiVTMSTransitionDisabler::VTMS_vthread_mount(vthread, hide);\n+  JNIHandles::destroy_local(vthread);\n+JRT_END\n@@ -657,6 +654,3 @@\n-  if (hide) {\n-    JvmtiVTMSTransitionDisabler::VTMS_unmount_begin(vthread, last_unmount);\n-  } else {\n-    JvmtiVTMSTransitionDisabler::VTMS_unmount_end(vthread, last_unmount);\n-  }\n-\n+JRT_ENTRY(void, SharedRuntime::notify_jvmti_vthread_unmount(oopDesc* vt, jboolean hide, JavaThread* current))\n+  jobject vthread = JNIHandles::make_local(const_cast<oopDesc*>(vt));\n+  JvmtiVTMSTransitionDisabler::VTMS_vthread_unmount(vthread, hide);\n@@ -1267,1 +1261,1 @@\n-      rk = constants->klass_ref_at(bytecode_index, CHECK_NH);\n+      rk = constants->klass_ref_at(bytecode_index, bc, CHECK_NH);\n@@ -3082,1 +3076,1 @@\n-                  super_method->set_mismatch(true);\n+                  super_method->set_mismatch();\n@@ -3176,3 +3170,7 @@\n-    method->set_has_scalarized_args(true);\n-    method->set_c1_needs_stack_repair(ces.c1_needs_stack_repair());\n-    method->set_c2_needs_stack_repair(ces.c2_needs_stack_repair());\n+    method->set_has_scalarized_args();\n+    if (ces.c1_needs_stack_repair()) {\n+      method->set_c1_needs_stack_repair();\n+    }\n+    if (ces.c2_needs_stack_repair()) {\n+      method->set_c2_needs_stack_repair();\n+    }\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":24,"deletions":26,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -269,3 +269,4 @@\n-  static void notify_jvmti_object_alloc(oopDesc* o, JavaThread* current);\n-  static void notify_jvmti_mount(oopDesc* vt, jboolean hide, jboolean first_mount, JavaThread* current);\n-  static void notify_jvmti_unmount(oopDesc* vt, jboolean hide, jboolean last_unmount, JavaThread* current);\n+  static void notify_jvmti_vthread_start(oopDesc* vt, jboolean hide, JavaThread* current);\n+  static void notify_jvmti_vthread_end(oopDesc* vt, jboolean hide, JavaThread* current);\n+  static void notify_jvmti_vthread_mount(oopDesc* vt, jboolean hide, JavaThread* current);\n+  static void notify_jvmti_vthread_unmount(oopDesc* vt, jboolean hide, JavaThread* current);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -341,4 +343,12 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Degenerate notify\n-    \/\/ stack-locked by caller so by definition the implied waitset is empty.\n-    return true;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n+      \/\/ Degenerate notify\n+      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Degenerate notify\n+      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n@@ -416,10 +426,12 @@\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-null so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ stack-locking in the object's header, the second check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ This Java Monitor is inflated so obj's header will never be\n+      \/\/ displaced to this thread's BasicLock. Make the displaced header\n+      \/\/ non-null so this BasicLock is not seen as recursive nor as\n+      \/\/ being locked. We do this unconditionally so that this thread's\n+      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+      \/\/ performance reasons, stack walkers generally first check for\n+      \/\/ stack-locking in the object's header, the second check is for\n+      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -493,2 +505,2 @@\n-#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64)\n-  return UseHeavyMonitors;\n+#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64) || defined(S390)\n+  return LockingMode == LM_MONITOR;\n@@ -515,6 +527,33 @@\n-    markWord mark = obj->mark();\n-    if (mark.is_neutral()) {\n-      \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-      \/\/ be visible <= the ST performed by the CAS.\n-      lock->set_displaced_header(mark);\n-      if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      LockStack& lock_stack = current->lock_stack();\n+      if (lock_stack.can_push()) {\n+        markWord mark = obj()->mark_acquire();\n+        if (mark.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state.\n+          markWord locked_mark = mark.set_fast_locked();\n+          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+          if (old_mark == mark) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+        }\n+      }\n+      \/\/ All other paths fall-through to inflate-enter.\n+    } else if (LockingMode == LM_LEGACY) {\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address) mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(nullptr));\n@@ -523,13 +562,6 @@\n-      \/\/ Fall through to inflate() ...\n-    } else if (mark.has_locker() &&\n-               current->is_lock_owned((address)mark.locker())) {\n-      assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-      assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-      lock->set_displaced_header(markWord::from_pointer(nullptr));\n-      return;\n-    }\n-    \/\/ The object header will never be displaced to this lock,\n-    \/\/ so it does not matter what the value is, except that it\n-    \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-    \/\/ and must not look locked either.\n-    lock->set_displaced_header(markWord::unused_mark());\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -538,1 +570,1 @@\n-    guarantee(!obj->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -560,26 +592,15 @@\n-    assert(!EnableValhalla || !object->klass()->is_inline_klass(), \"monitor op on inline type\");\n-\n-    markWord dhw = lock->displaced_header();\n-    if (dhw.value() == 0) {\n-      \/\/ If the displaced header is null, then this exit matches up with\n-      \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-      if (mark != markWord::INFLATING()) {\n-        \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-        \/\/ exiting a recursive enter of a Java Monitor that is being\n-        \/\/ inflated is safe; see the has_monitor() comment below.\n-        assert(!mark.is_neutral(), \"invariant\");\n-        assert(!mark.has_locker() ||\n-        current->is_lock_owned((address)mark.locker()), \"invariant\");\n-        if (mark.has_monitor()) {\n-          \/\/ The BasicLock's displaced_header is marked as a recursive\n-          \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-          \/\/ This is a special case where the Java Monitor was inflated\n-          \/\/ after this thread entered the stack-lock recursively. When a\n-          \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-          \/\/ Monitor owner's stack and update the BasicLocks because a\n-          \/\/ Java Monitor can be asynchronously inflated by a thread that\n-          \/\/ does not own the Java Monitor.\n-          ObjectMonitor* m = mark.monitor();\n-          assert(m->object()->mark() == mark, \"invariant\");\n-          assert(m->is_entered(current), \"invariant\");\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_mark = mark.set_unlocked();\n+        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark != mark) {\n+          \/\/ Another thread won the CAS, it must have inflated the monitor.\n+          \/\/ It can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and\n+          \/\/ exit it (allowing waiting threads to enter).\n+          assert(old_mark.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = old_mark.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n@@ -587,0 +608,3 @@\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -588,0 +612,27 @@\n+    } else if (LockingMode == LM_LEGACY) {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is null, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -589,8 +640,0 @@\n-      return;\n-    }\n-\n-    if (mark == markWord::from_pointer(lock)) {\n-      \/\/ If the object is stack-locked by the current thread, try to\n-      \/\/ swing the displaced header from the BasicLock back to the mark.\n-      assert(dhw.is_neutral(), \"invariant\");\n-      if (object->cas_set_mark(dhw, mark) == mark) {\n@@ -599,0 +642,9 @@\n+\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -601,1 +653,1 @@\n-    guarantee(!object->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee((object->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -608,0 +660,7 @@\n+  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n+    \/\/ It must be owned by us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -702,3 +761,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -718,3 +784,10 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -746,1 +819,2 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n@@ -861,0 +935,7 @@\n+\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n+\/\/ calculations as part of JVM\/TI tagging.\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n+  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -873,2 +954,2 @@\n-      assert(UseHeavyMonitors, \"+VerifyHeavyMonitors requires +UseHeavyMonitors\");\n-      guarantee(!mark.has_locker(), \"must not be stack locked\");\n+      assert(LockingMode == LM_MONITOR, \"+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\");\n+      guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -919,2 +1000,9 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n-      \/\/ This is a stack lock owned by the calling thread so fetch the\n+    } else if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast-lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (LockingMode == LM_LEGACY && mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ This is a stack-lock owned by the calling thread so fetch the\n@@ -931,1 +1019,1 @@\n-      \/\/ So we have to inflate the stack lock into an ObjectMonitor\n+      \/\/ So we have to inflate the stack-lock into an ObjectMonitor\n@@ -987,2 +1075,2 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked case, header points into owner's stack\n@@ -991,1 +1079,6 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locking case, see if lock is in current's lock stack\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -993,0 +1086,1 @@\n+    \/\/ Inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -1005,2 +1099,0 @@\n-  address owner = nullptr;\n-\n@@ -1009,3 +1101,10 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n-    owner = (address) mark.locker();\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked so header points into owner's stack.\n+    \/\/ owning_thread_from_monitor_owner() may also return null here:\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locked so get owner from the object.\n+    \/\/ owning_thread_from_object() may also return null here:\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -1014,2 +1113,2 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n+    \/\/ Inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -1020,6 +1119,2 @@\n-    owner = (address) monitor->owner();\n-  }\n-\n-  if (owner != nullptr) {\n-    \/\/ owning_thread_from_monitor_owner() may also return null here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n+    \/\/ owning_thread_from_monitor() may also return null here:\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -1039,1 +1134,1 @@\n-\/\/ ObjectMonitors where owner is set to a stack lock address in thread.\n+\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n@@ -1049,1 +1144,1 @@\n-      \/\/ is set to a stack lock address in the target thread.\n+      \/\/ is set to a stack-lock address in the target thread.\n@@ -1075,1 +1170,1 @@\n-    \/\/ Owner set to a stack lock address in thread should never be seen here:\n+    \/\/ Owner set to a stack-lock address in thread should never be seen here:\n@@ -1263,4 +1358,11 @@\n-    \/\/ *  Inflated     - just return\n-    \/\/ *  Stack-locked - coerce it to inflated\n-    \/\/ *  INFLATING    - busy wait for conversion to complete\n-    \/\/ *  Neutral      - aggressively inflate the object.\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the current thread owns the\n+    \/\/                   object lock, then we make the current thread the\n+    \/\/                   ObjectMonitor owner and remove the lock from the\n+    \/\/                   current thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n+    \/\/ *  INFLATING    - Busy wait for conversion from stack-locked to\n+    \/\/                   inflated.\n+    \/\/ *  neutral      - Aggressively inflate the object.\n@@ -1273,0 +1375,4 @@\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        JavaThread::cast(current)->lock_stack().remove(object);\n+      }\n@@ -1276,9 +1382,64 @@\n-    \/\/ CASE: inflation in progress - inflating over a stack-lock.\n-    \/\/ Some other thread is converting from stack-locked to inflated.\n-    \/\/ Only that thread can complete inflation -- other threads must wait.\n-    \/\/ The INFLATING value is transient.\n-    \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n-    \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n-    if (mark == markWord::INFLATING()) {\n-      read_stable_mark(object);\n-      continue;\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ New lightweight locking does not use INFLATING.\n+      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+      \/\/ Some other thread is converting from stack-locked to inflated.\n+      \/\/ Only that thread can complete inflation -- other threads must wait.\n+      \/\/ The INFLATING value is transient.\n+      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+      if (mark == markWord::INFLATING()) {\n+        read_stable_mark(object);\n+        continue;\n+      }\n+    }\n+\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n+    \/\/\n+    LogStreamHandle(Trace, monitorinflation) lsh;\n+    if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(nullptr, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          JavaThread::cast(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;  \/\/ Interference -- just retry\n+      }\n@@ -1288,1 +1449,1 @@\n-    \/\/ Could be stack-locked either by this thread or by some other thread.\n+    \/\/ Could be stack-locked either by current or by some other thread.\n@@ -1295,5 +1456,5 @@\n-    \/\/ the odds of inflation contention.\n-\n-    LogStreamHandle(Trace, monitorinflation) lsh;\n-\n-    if (mark.has_locker()) {\n+    \/\/ the odds of inflation contention. If we lose the race to set INFLATING,\n+    \/\/ then we just delete the ObjectMonitor and loop around again.\n+    \/\/\n+    if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+      assert(LockingMode != LM_LIGHTWEIGHT, \"cannot happen with new lightweight locking\");\n@@ -1461,2 +1622,2 @@\n-\/\/ is set to a stack lock address are NOT associated with the JavaThread\n-\/\/ that holds that stack lock. All of the current consumers of\n+\/\/ is set to a stack-lock address are NOT associated with the JavaThread\n+\/\/ that holds that stack-lock. All of the current consumers of\n@@ -1464,1 +1625,1 @@\n-\/\/ those do not have the owner set to a stack lock address.\n+\/\/ those do not have the owner set to a stack-lock address.\n@@ -1481,1 +1642,1 @@\n-      \/\/ not include when owner is set to a stack lock address in thread.\n+      \/\/ not include when owner is set to a stack-lock address in thread.\n@@ -1513,0 +1674,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1554,1 +1725,1 @@\n-    if (current->is_Java_thread()) {\n+    if (current->is_monitor_deflation_thread()) {\n@@ -1568,0 +1739,5 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n@@ -1575,0 +1751,4 @@\n+    } else {\n+      \/\/ This is not a monitor deflation thread.\n+      \/\/ No handshake or rendezvous is needed when we are already at safepoint.\n+      assert_at_safepoint();\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":306,"deletions":126,"binary":false,"changes":432,"status":"modified"},{"patch":"@@ -63,4 +63,13 @@\n-  template(ZMarkStart)                            \\\n-  template(ZMarkEnd)                              \\\n-  template(ZRelocateStart)                        \\\n-  template(ZVerify)                               \\\n+  template(ZMarkEndOld)                           \\\n+  template(ZMarkEndYoung)                         \\\n+  template(ZMarkFlushOperation)                   \\\n+  template(ZMarkStartYoung)                       \\\n+  template(ZMarkStartYoungAndOld)                 \\\n+  template(ZRelocateStartOld)                     \\\n+  template(ZRelocateStartYoung)                   \\\n+  template(ZRendezvousGCThreads)                  \\\n+  template(ZVerifyOld)                            \\\n+  template(XMarkStart)                            \\\n+  template(XMarkEnd)                              \\\n+  template(XRelocateStart)                        \\\n+  template(XVerify)                               \\\n@@ -110,1 +119,2 @@\n-  template(JvmtiPostObjectFree)\n+  template(JvmtiPostObjectFree)                   \\\n+  template(RendezvousGCThreads)\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":15,"deletions":5,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"gc\/shared\/stringdedup\/stringDedupThread.hpp\"\n@@ -303,1 +304,0 @@\n-  nonstatic_field(Method,                      _flags,                                        u2)                                    \\\n@@ -312,1 +312,1 @@\n-  nonstatic_field(ConstMethod,                 _flags,                                        u2)                                    \\\n+  nonstatic_field(ConstMethod,                 _flags._flags,                                 u4)                                    \\\n@@ -707,0 +707,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _top,                                          uint32_t)                              \\\n+  nonstatic_field(LockStack,                   _base[0],                                      oop)                                   \\\n@@ -1318,0 +1321,1 @@\n+        declare_type(StringDedupThread, JavaThread)                       \\\n@@ -1323,0 +1327,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n@@ -2091,10 +2096,0 @@\n-  declare_constant(JVM_ACC_MONITOR_MATCH)                                 \\\n-  declare_constant(JVM_ACC_HAS_MONITOR_BYTECODES)                         \\\n-  declare_constant(JVM_ACC_HAS_LOOPS)                                     \\\n-  declare_constant(JVM_ACC_LOOPS_FLAG_INIT)                               \\\n-  declare_constant(JVM_ACC_QUEUED)                                        \\\n-  declare_constant(JVM_ACC_NOT_C2_OSR_COMPILABLE)                         \\\n-  declare_constant(JVM_ACC_HAS_JSRS)                                      \\\n-  declare_constant(JVM_ACC_IS_OLD)                                        \\\n-  declare_constant(JVM_ACC_IS_OBSOLETE)                                   \\\n-  declare_constant(JVM_ACC_IS_PREFIXED_NATIVE)                            \\\n@@ -2184,0 +2179,3 @@\n+  declare_constant(Method::nonvirtual_vtable_index)                       \\\n+  declare_constant(Method::extra_stack_entries_for_jsr292)                \\\n+                                                                          \\\n@@ -2188,20 +2186,10 @@\n-  declare_constant(Method::_caller_sensitive)                             \\\n-  declare_constant(Method::_force_inline)                                 \\\n-  declare_constant(Method::_dont_inline)                                  \\\n-  declare_constant(Method::_hidden)                                       \\\n-  declare_constant(Method::_changes_current_thread)                       \\\n-                                                                          \\\n-  declare_constant(Method::nonvirtual_vtable_index)                       \\\n-                                                                          \\\n-  declare_constant(Method::extra_stack_entries_for_jsr292)                \\\n-                                                                          \\\n-  declare_constant(ConstMethod::_has_linenumber_table)                    \\\n-  declare_constant(ConstMethod::_has_checked_exceptions)                  \\\n-  declare_constant(ConstMethod::_has_localvariable_table)                 \\\n-  declare_constant(ConstMethod::_has_exception_table)                     \\\n-  declare_constant(ConstMethod::_has_generic_signature)                   \\\n-  declare_constant(ConstMethod::_has_method_parameters)                   \\\n-  declare_constant(ConstMethod::_has_method_annotations)                  \\\n-  declare_constant(ConstMethod::_has_parameter_annotations)               \\\n-  declare_constant(ConstMethod::_has_default_annotations)                 \\\n-  declare_constant(ConstMethod::_has_type_annotations)                    \\\n+  declare_constant(ConstMethodFlags::_misc_has_linenumber_table)          \\\n+  declare_constant(ConstMethodFlags::_misc_has_checked_exceptions)        \\\n+  declare_constant(ConstMethodFlags::_misc_has_localvariable_table)       \\\n+  declare_constant(ConstMethodFlags::_misc_has_exception_table)           \\\n+  declare_constant(ConstMethodFlags::_misc_has_generic_signature)         \\\n+  declare_constant(ConstMethodFlags::_misc_has_method_parameters)         \\\n+  declare_constant(ConstMethodFlags::_misc_has_method_annotations)        \\\n+  declare_constant(ConstMethodFlags::_misc_has_parameter_annotations)     \\\n+  declare_constant(ConstMethodFlags::_misc_has_default_annotations)       \\\n+  declare_constant(ConstMethodFlags::_misc_has_type_annotations)          \\\n@@ -2437,0 +2425,8 @@\n+  \/**********************************************\/                        \\\n+  \/* LockingMode enum (globalDefinitions.hpp) *\/                          \\\n+  \/**********************************************\/                        \\\n+                                                                          \\\n+  declare_constant(LM_MONITOR)                                            \\\n+  declare_constant(LM_LEGACY)                                             \\\n+  declare_constant(LM_LIGHTWEIGHT)                                        \\\n+                                                                          \\\n@@ -2621,2 +2617,4 @@\n-  declare_constant(InvocationCounter::count_shift)\n-\n+  declare_constant(InvocationCounter::count_shift)                        \\\n+                                                                          \\\n+  \/* ObjectMonitor constants *\/                                           \\\n+  declare_constant(ObjectMonitor::ANONYMOUS_OWNER)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":32,"deletions":34,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -2461,0 +2461,1 @@\n+  virtual bool doit_prologue();\n@@ -2638,0 +2639,11 @@\n+bool VM_HeapDumper::doit_prologue() {\n+  if (_gc_before_heap_dump && UseZGC) {\n+    \/\/ ZGC cannot perform a synchronous GC cycle from within the VM thread.\n+    \/\/ So ZCollectedHeap::collect_as_vm_thread() is a noop. To respect the\n+    \/\/ _gc_before_heap_dump flag a synchronous GC cycle is performed from\n+    \/\/ the caller thread in the prologue.\n+    Universe::heap()->collect(GCCause::_heap_dump);\n+  }\n+  return VM_GC_Operation::doit_prologue();\n+}\n+\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,21 +30,0 @@\n-void AccessFlags::atomic_set_bits(jint bits) {\n-  \/\/ Atomically update the flags with the bits given\n-  jint old_flags, new_flags, f;\n-  do {\n-    old_flags = _flags;\n-    new_flags = old_flags | bits;\n-    f = Atomic::cmpxchg(&_flags, old_flags, new_flags);\n-  } while(f != old_flags);\n-}\n-\n-void AccessFlags::atomic_clear_bits(jint bits) {\n-  \/\/ Atomically update the flags with the bits given\n-  jint old_flags, new_flags, f;\n-  do {\n-    old_flags = _flags;\n-    new_flags = old_flags & ~bits;\n-    f = Atomic::cmpxchg(&_flags, old_flags, new_flags);\n-  } while(f != old_flags);\n-}\n-\n-\n@@ -69,3 +48,0 @@\n-  if (is_old         ()) st->print(\"{old} \"       );\n-  if (is_obsolete    ()) st->print(\"{obsolete} \"  );\n-  if (on_stack       ()) st->print(\"{on_stack} \"  );\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.cpp","additions":1,"deletions":25,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -40,2 +40,0 @@\n-  \/\/ HotSpot-specific access flags\n-\n@@ -45,17 +43,4 @@\n-  \/\/ Method* flags\n-  JVM_ACC_MONITOR_MATCH           = 0x10000000,     \/\/ True if we know that monitorenter\/monitorexit bytecodes match\n-  JVM_ACC_HAS_MONITOR_BYTECODES   = 0x20000000,     \/\/ Method contains monitorenter\/monitorexit bytecodes\n-  JVM_ACC_HAS_LOOPS               = 0x40000000,     \/\/ Method has loops\n-  JVM_ACC_LOOPS_FLAG_INIT         = (int)0x80000000,\/\/ The loop flag has been initialized\n-  JVM_ACC_QUEUED                  = 0x01000000,     \/\/ Queued for compilation\n-  JVM_ACC_NOT_C2_COMPILABLE       = 0x02000000,\n-  JVM_ACC_NOT_C1_COMPILABLE       = 0x04000000,\n-  JVM_ACC_NOT_C2_OSR_COMPILABLE   = 0x08000000,\n-  JVM_ACC_HAS_JSRS                = 0x00800000,\n-  JVM_ACC_IS_OLD                  = 0x00010000,     \/\/ RedefineClasses() has replaced this method\n-  JVM_ACC_IS_OBSOLETE             = 0x00020000,     \/\/ RedefineClasses() has made method obsolete\n-  JVM_ACC_IS_PREFIXED_NATIVE      = 0x00040000,     \/\/ JVMTI has prefixed this native method\n-  JVM_ACC_ON_STACK                = 0x00080000,     \/\/ RedefineClasses() was used on the stack\n-  JVM_ACC_IS_DELETED              = 0x00008000,     \/\/ RedefineClasses() has deleted this method\n-\n-  \/\/ Klass* flags\n+  \/\/ HotSpot-specific access flags\n+  \/\/ These Klass flags should be migrated, to a field such as InstanceKlass::_misc_flags,\n+  \/\/ or to a similar flags field in Klass itself.\n+  \/\/ Do not add new ACC flags here.\n@@ -72,1 +57,1 @@\n-  jint _flags;\n+  jint _flags;  \/\/ TODO: move 4 access flags above to Klass and change to u2\n@@ -97,15 +82,0 @@\n-  \/\/ Method* flags\n-  bool is_monitor_matching     () const { return (_flags & JVM_ACC_MONITOR_MATCH          ) != 0; }\n-  bool has_monitor_bytecodes   () const { return (_flags & JVM_ACC_HAS_MONITOR_BYTECODES  ) != 0; }\n-  bool has_loops               () const { return (_flags & JVM_ACC_HAS_LOOPS              ) != 0; }\n-  bool loops_flag_init         () const { return (_flags & JVM_ACC_LOOPS_FLAG_INIT        ) != 0; }\n-  bool queued_for_compilation  () const { return (_flags & JVM_ACC_QUEUED                 ) != 0; }\n-  bool is_not_c1_compilable    () const { return (_flags & JVM_ACC_NOT_C1_COMPILABLE      ) != 0; }\n-  bool is_not_c2_compilable    () const { return (_flags & JVM_ACC_NOT_C2_COMPILABLE      ) != 0; }\n-  bool is_not_c2_osr_compilable() const { return (_flags & JVM_ACC_NOT_C2_OSR_COMPILABLE  ) != 0; }\n-  bool has_jsrs                () const { return (_flags & JVM_ACC_HAS_JSRS               ) != 0; }\n-  bool is_old                  () const { return (_flags & JVM_ACC_IS_OLD                 ) != 0; }\n-  bool is_obsolete             () const { return (_flags & JVM_ACC_IS_OBSOLETE            ) != 0; }\n-  bool is_deleted              () const { return (_flags & JVM_ACC_IS_DELETED             ) != 0; }\n-  bool is_prefixed_native      () const { return (_flags & JVM_ACC_IS_PREFIXED_NATIVE     ) != 0; }\n-\n@@ -118,2 +88,0 @@\n-  bool on_stack() const                 { return (_flags & JVM_ACC_ON_STACK) != 0; }\n-\n@@ -130,8 +98,0 @@\n-  void set_queued_for_compilation()    { atomic_set_bits(JVM_ACC_QUEUED); }\n-  void clear_queued_for_compilation()  { atomic_clear_bits(JVM_ACC_QUEUED); }\n-\n-  \/\/ Atomic update of flags\n-  void atomic_set_bits(jint bits);\n-  void atomic_clear_bits(jint bits);\n-\n-  friend class Method;\n@@ -145,19 +105,2 @@\n-  void set_is_synthetic()              { atomic_set_bits(JVM_ACC_SYNTHETIC);               }\n-\n-  \/\/ Method* flags\n-  void set_monitor_matching()          { atomic_set_bits(JVM_ACC_MONITOR_MATCH);           }\n-  void set_has_monitor_bytecodes()     { atomic_set_bits(JVM_ACC_HAS_MONITOR_BYTECODES);   }\n-  void set_has_loops()                 { atomic_set_bits(JVM_ACC_HAS_LOOPS);               }\n-  void set_loops_flag_init()           { atomic_set_bits(JVM_ACC_LOOPS_FLAG_INIT);         }\n-  void set_not_c1_compilable()         { atomic_set_bits(JVM_ACC_NOT_C1_COMPILABLE);       }\n-  void set_not_c2_compilable()         { atomic_set_bits(JVM_ACC_NOT_C2_COMPILABLE);       }\n-  void set_not_c2_osr_compilable()     { atomic_set_bits(JVM_ACC_NOT_C2_OSR_COMPILABLE);   }\n-  void set_has_jsrs()                  { atomic_set_bits(JVM_ACC_HAS_JSRS);                }\n-  void set_is_old()                    { atomic_set_bits(JVM_ACC_IS_OLD);                  }\n-  void set_is_obsolete()               { atomic_set_bits(JVM_ACC_IS_OBSOLETE);             }\n-  void set_is_deleted()                { atomic_set_bits(JVM_ACC_IS_DELETED);              }\n-  void set_is_prefixed_native()        { atomic_set_bits(JVM_ACC_IS_PREFIXED_NATIVE);      }\n-\n-  void clear_not_c1_compilable()       { atomic_clear_bits(JVM_ACC_NOT_C1_COMPILABLE);       }\n-  void clear_not_c2_compilable()       { atomic_clear_bits(JVM_ACC_NOT_C2_COMPILABLE);       }\n-  void clear_not_c2_osr_compilable()   { atomic_clear_bits(JVM_ACC_NOT_C2_OSR_COMPILABLE);   }\n+  void set_is_synthetic()              { _flags |= JVM_ACC_SYNTHETIC; }\n+\n@@ -165,4 +108,5 @@\n-  void set_has_finalizer()             { atomic_set_bits(JVM_ACC_HAS_FINALIZER);           }\n-  void set_is_cloneable_fast()         { atomic_set_bits(JVM_ACC_IS_CLONEABLE_FAST);       }\n-  void set_is_hidden_class()           { atomic_set_bits(JVM_ACC_IS_HIDDEN_CLASS);         }\n-  void set_is_value_based_class()      { atomic_set_bits(JVM_ACC_IS_VALUE_BASED_CLASS);    }\n+  \/\/ These are set at classfile parsing time so do not require atomic access.\n+  void set_has_finalizer()             { _flags |= JVM_ACC_HAS_FINALIZER; }\n+  void set_is_cloneable_fast()         { _flags |= JVM_ACC_IS_CLONEABLE_FAST; }\n+  void set_is_hidden_class()           { _flags |= JVM_ACC_IS_HIDDEN_CLASS; }\n+  void set_is_value_based_class()      { _flags |= JVM_ACC_IS_VALUE_BASED_CLASS; }\n@@ -171,8 +115,0 @@\n-  void set_on_stack(const bool value)\n-                                       {\n-                                         if (value) {\n-                                           atomic_set_bits(JVM_ACC_ON_STACK);\n-                                         } else {\n-                                           atomic_clear_bits(JVM_ACC_ON_STACK);\n-                                         }\n-                                       }\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.hpp","additions":12,"deletions":76,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -1060,0 +1060,9 @@\n+enum LockingMode {\n+  \/\/ Use only heavy monitors for locking\n+  LM_MONITOR     = 0,\n+  \/\/ Legacy stack-locking, with monitors as 2nd tier\n+  LM_LEGACY      = 1,\n+  \/\/ New lightweight locking, with monitors as 2nd tier\n+  LM_LIGHTWEIGHT = 2\n+};\n+\n@@ -1234,0 +1243,3 @@\n+inline jint  java_negate(jint  v) { return java_subtract((jint) 0, v); }\n+inline jlong java_negate(jlong v) { return java_subtract((jlong)0, v); }\n+\n@@ -1323,1 +1335,1 @@\n-  const uint64_t z2 = x2 * y2;\n+  const uint64_t z2 = (uint64_t)x2 * y2;\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -290,1 +290,1 @@\n-  template <typename K, int compare(const K&, const E&)> int find_sorted(const K& key, bool& found) {\n+  template <typename K, int compare(const K&, const E&)> int find_sorted(const K& key, bool& found) const {\n@@ -346,0 +346,8 @@\n+template <typename E>\n+class GrowableArrayFromArray : public GrowableArrayView<E> {\n+public:\n+\n+  GrowableArrayFromArray<E>(E* data, int len) :\n+    GrowableArrayView<E>(data, len, len) {}\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1461,1 +1461,1 @@\n-     * @throws NegativeArraySizeException if arrayLength is negative\n+     * @throws StreamCorruptedException if arrayLength is negative\n@@ -1464,1 +1464,1 @@\n-    private void checkArray(Class<?> arrayType, int arrayLength) throws InvalidClassException {\n+    private void checkArray(Class<?> arrayType, int arrayLength) throws ObjectStreamException {\n@@ -1470,1 +1470,1 @@\n-            throw new NegativeArraySizeException();\n+            throw new StreamCorruptedException(\"Array length is negative\");\n@@ -2148,1 +2148,3 @@\n-\n+        if (len < 0) {\n+            throw new StreamCorruptedException(\"Array length is negative\");\n+        }\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectInputStream.java","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -81,1 +81,1 @@\n-import jdk.internal.misc.VM;\n+import jdk.internal.javac.PreviewFeature;\n@@ -85,0 +85,1 @@\n+import jdk.internal.misc.VM;\n@@ -1890,4 +1891,4 @@\n-     * Initiates the <a href=\"Runtime.html#shutdown\">shutdown sequence<\/a> of the\n-     * Java Virtual Machine. This method always blocks indefinitely. The argument\n-     * serves as a status code; by convention, a nonzero status code indicates\n-     * abnormal termination.\n+     * Initiates the {@linkplain Runtime##shutdown shutdown sequence} of the Java Virtual Machine.\n+     * Unless the security manager denies exiting, this method initiates the shutdown sequence\n+     * (if it is not already initiated) and then blocks indefinitely. This method neither returns\n+     * nor throws an exception; that is, it does not complete either normally or abruptly.\n@@ -1895,2 +1896,2 @@\n-     * This method calls the {@code exit} method in class {@code Runtime}. This\n-     * method never returns normally.\n+     * The argument serves as a status code. By convention, a nonzero status code\n+     * indicates abnormal termination.\n@@ -1899,3 +1900,3 @@\n-     * <blockquote><pre>\n-     * Runtime.getRuntime().exit(n)\n-     * <\/pre><\/blockquote>\n+     * {@snippet :\n+     *     Runtime.getRuntime().exit(n)\n+     * }\n@@ -2526,0 +2527,17 @@\n+            @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES)\n+            public long stringConcatCoder(char value) {\n+                return StringConcatHelper.coder(value);\n+            }\n+\n+            @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES)\n+            public long stringBuilderConcatMix(long lengthCoder,\n+                                               StringBuilder sb) {\n+                return sb.mix(lengthCoder);\n+            }\n+\n+            @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES)\n+            public long stringBuilderConcatPrepend(long lengthCoder, byte[] buf,\n+                                                   StringBuilder sb) {\n+                return sb.prepend(lengthCoder, buf);\n+            }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/System.java","additions":28,"deletions":10,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+    static final ClassDesc[] EMPTY_CLASSDESC = new ClassDesc[0];\n@@ -78,0 +79,60 @@\n+    \/**\n+     * Validates the correctness of a binary package name.\n+     * In particular checks for the presence of invalid characters in the name.\n+     * Empty package name is allowed.\n+     *\n+     * @param name the package name\n+     * @return the package name passed if valid\n+     * @throws IllegalArgumentException if the package name is invalid\n+     * @throws NullPointerException if the package name is {@code null}\n+     *\/\n+    public static String validateBinaryPackageName(String name) {\n+        for (int i=0; i<name.length(); i++) {\n+            char ch = name.charAt(i);\n+            if (ch == ';' || ch == '[' || ch == '\/')\n+                throw new IllegalArgumentException(\"Invalid package name: \" + name);\n+        }\n+        return name;\n+    }\n+\n+    \/**\n+     * Validates the correctness of an internal package name.\n+     * In particular checks for the presence of invalid characters in the name.\n+     * Empty package name is allowed.\n+     *\n+     * @param name the package name\n+     * @return the package name passed if valid\n+     * @throws IllegalArgumentException if the package name is invalid\n+     * @throws NullPointerException if the package name is {@code null}\n+     *\/\n+    public static String validateInternalPackageName(String name) {\n+        for (int i=0; i<name.length(); i++) {\n+            char ch = name.charAt(i);\n+            if (ch == ';' || ch == '[' || ch == '.')\n+                throw new IllegalArgumentException(\"Invalid package name: \" + name);\n+        }\n+        return name;\n+    }\n+\n+    \/**\n+     * Validates the correctness of a module name.\n+     * In particular checks for the presence of invalid characters in the name.\n+     * Empty module name is allowed.\n+     *\n+     * {@jvms 4.2.3} Module and Package Names\n+     *\n+     * @param name the module name\n+     * @return the module name passed if valid\n+     * @throws IllegalArgumentException if the module name is invalid\n+     * @throws NullPointerException if the module name is {@code null}\n+     *\/\n+    public static String validateModuleName(String name) {\n+        for (int i=name.length() - 1; i >= 0; i--) {\n+            char ch = name.charAt(i);\n+            if ((ch >= '\\u0000' && ch <= '\\u001F')\n+            || ((ch == '\\\\' || ch == ':' || ch =='@') && (i == 0 || name.charAt(--i) != '\\\\')))\n+                throw new IllegalArgumentException(\"Invalid module name: \" + name);\n+        }\n+        return name;\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/constant\/ConstantUtils.java","additions":61,"deletions":0,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -8004,1 +8004,1 @@\n-     *             MemoryLayout.paddingLayout(32),\n+     *             MemoryLayout.paddingLayout(4),\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+import jdk.internal.javac.PreviewFeature;\n@@ -424,0 +425,18 @@\n+   \/**\n+    * Get the coder for the supplied character.\n+    *\/\n+   @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES)\n+   long stringConcatCoder(char value);\n+\n+   \/**\n+    * Update lengthCoder for StringBuilder.\n+    *\/\n+   @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES)\n+   long stringBuilderConcatMix(long lengthCoder, StringBuilder sb);\n+\n+    \/**\n+     * Prepend StringBuilder content.\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES)\n+   long stringBuilderConcatPrepend(long lengthCoder, byte[] buf, StringBuilder sb);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangAccess.java","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -67,4 +67,0 @@\n-        @JEP(number=433, title=\"Pattern Matching for switch\", status=\"Fourth Preview\")\n-        SWITCH_PATTERN_MATCHING(),\n-        @JEP(number=432, title=\"Record Patterns\", status=\"Second Preview\")\n-        RECORD_PATTERNS,\n@@ -73,1 +69,1 @@\n-        @JEP(number=434, title=\"Foreign Function & Memory API\", status=\"Second Preview\")\n+        @JEP(number=442, title=\"Foreign Function & Memory API\", status=\"Third Preview\")\n@@ -77,0 +73,4 @@\n+        @JEP(number=430, title=\"String Templates\", status=\"First Preview\")\n+        STRING_TEMPLATES,\n+        @JEP(number=443, title=\"Unnamed Patterns and Variables\")\n+        UNNAMED,\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/javac\/PreviewFeature.java","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -152,1 +152,0 @@\n-        jdk.jartool, \/\/ participates in preview features\n@@ -248,1 +247,2 @@\n-        java.net.http;\n+        java.net.http,\n+        jdk.naming.dns;\n@@ -286,0 +286,1 @@\n+        jdk.jlink,\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -178,0 +178,6 @@\n+        \/**\n+         * Used for instances of {@link StringTemplateTree}.\n+         *\/\n+        @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES, reflective=true)\n+        TEMPLATE(StringTemplateTree.class),\n+\n@@ -227,1 +233,1 @@\n-         * @since 16\n+         * @since 21\n@@ -229,1 +235,2 @@\n-        BINDING_PATTERN(BindingPatternTree.class),\n+        @PreviewFeature(feature=PreviewFeature.Feature.UNNAMED)\n+        ANY_PATTERN(AnyPatternTree.class),\n@@ -232,1 +239,1 @@\n-         * Used for instances of {@link ParenthesizedPatternTree}.\n+         * Used for instances of {@link BindingPatternTree}.\n@@ -234,1 +241,1 @@\n-         * @since 17\n+         * @since 16\n@@ -236,2 +243,1 @@\n-        @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-        PARENTHESIZED_PATTERN(ParenthesizedPatternTree.class),\n+        BINDING_PATTERN(BindingPatternTree.class),\n@@ -242,1 +248,1 @@\n-         * @since 17\n+         * @since 21\n@@ -244,1 +250,0 @@\n-        @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n@@ -250,1 +255,1 @@\n-         * @since 19\n+         * @since 21\n@@ -252,1 +257,0 @@\n-        @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n@@ -258,1 +262,1 @@\n-         * @since 19\n+         * @since 21\n@@ -260,1 +264,0 @@\n-        @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n@@ -266,1 +269,1 @@\n-         * @since 19\n+         * @since 21\n@@ -268,1 +271,0 @@\n-        @PreviewFeature(feature=PreviewFeature.Feature.RECORD_PATTERNS, reflective=true)\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/Tree.java","additions":16,"deletions":14,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -270,0 +270,19 @@\n+    \/**\n+     * Visits a StringTemplateTree node.\n+     * @param node the node being visited\n+     * @param p a parameter value\n+     * @return a result value\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES, reflective=true)\n+    R visitStringTemplate(StringTemplateTree node, P p);\n+\n+    \/**\n+     * Visits a {@code AnyPatternTree} node.\n+     * @param node the node being visited\n+     * @param p a parameter value\n+     * @return a result value\n+     * @since 21\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.UNNAMED)\n+    R visitAnyPattern(AnyPatternTree node, P p);\n+\n@@ -284,1 +303,1 @@\n-     * @since 17\n+     * @since 21\n@@ -286,1 +305,0 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n@@ -294,1 +312,1 @@\n-     * @since 19\n+     * @since 21\n@@ -296,1 +314,0 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n@@ -304,1 +321,1 @@\n-     * @since 19\n+     * @since 21\n@@ -306,1 +323,0 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n@@ -314,1 +330,1 @@\n-     * @since 19\n+     * @since 21\n@@ -316,1 +332,0 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.RECORD_PATTERNS, reflective=true)\n@@ -343,10 +358,0 @@\n-    \/**\n-     * Visits a {@code ParenthesizedPatternTree} node.\n-     * @param node the node being visited\n-     * @param p a parameter value\n-     * @return a result value\n-     * @since 17\n-     *\/\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-    R visitParenthesizedPattern(ParenthesizedPatternTree node, P p);\n-\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/TreeVisitor.java","additions":23,"deletions":18,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -655,0 +655,13 @@\n+    \/**\n+     * {@inheritDoc} This implementation calls {@code defaultAction}.\n+     *\n+     * @param node {@inheritDoc}\n+     * @param p {@inheritDoc}\n+     * @return  the result of {@code defaultAction}\n+     *\/\n+    @Override\n+    @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES, reflective=true)\n+    public R visitStringTemplate(StringTemplateTree node, P p) {\n+        return defaultAction(node, p);\n+    }\n+\n@@ -663,1 +676,1 @@\n-     * @since 14\n+     * @since 21\n@@ -666,1 +679,2 @@\n-    public R visitBindingPattern(BindingPatternTree node, P p) {\n+    @PreviewFeature(feature=PreviewFeature.Feature.UNNAMED)\n+    public R visitAnyPattern(AnyPatternTree node, P p) {\n@@ -678,1 +692,1 @@\n-     * @since 17\n+     * @since 14\n@@ -681,2 +695,1 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-    public R visitDefaultCaseLabel(DefaultCaseLabelTree node, P p) {\n+    public R visitBindingPattern(BindingPatternTree node, P p) {\n@@ -694,1 +707,1 @@\n-     * @since 19\n+     * @since 21\n@@ -697,2 +710,1 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-    public R visitConstantCaseLabel(ConstantCaseLabelTree node, P p) {\n+    public R visitDefaultCaseLabel(DefaultCaseLabelTree node, P p) {\n@@ -710,1 +722,1 @@\n-     * @since 19\n+     * @since 21\n@@ -713,2 +725,1 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.RECORD_PATTERNS, reflective=true)\n-    public R visitDeconstructionPattern(DeconstructionPatternTree node, P p) {\n+    public R visitConstantCaseLabel(ConstantCaseLabelTree node, P p) {\n@@ -726,1 +737,1 @@\n-     * @since 19\n+     * @since 21\n@@ -729,2 +740,1 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-    public R visitPatternCaseLabel(PatternCaseLabelTree node, P p) {\n+    public R visitDeconstructionPattern(DeconstructionPatternTree node, P p) {\n@@ -742,0 +752,1 @@\n+     * @since 21\n@@ -744,1 +755,1 @@\n-    public R visitArrayAccess(ArrayAccessTree node, P p) {\n+    public R visitPatternCaseLabel(PatternCaseLabelTree node, P p) {\n@@ -758,1 +769,1 @@\n-    public R visitMemberSelect(MemberSelectTree node, P p) {\n+    public R visitArrayAccess(ArrayAccessTree node, P p) {\n@@ -770,1 +781,0 @@\n-     * @since 17\n@@ -773,2 +783,1 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-    public R visitParenthesizedPattern(ParenthesizedPatternTree node, P p) {\n+    public R visitMemberSelect(MemberSelectTree node, P p) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/util\/SimpleTreeVisitor.java","additions":28,"deletions":19,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -350,1 +350,1 @@\n-        R r = scan(node.getVariableOrRecordPattern(), p);\n+        R r = scan(node.getVariable(), p);\n@@ -414,0 +414,1 @@\n+        r = scanAndReduce(node.getGuard(), p, r);\n@@ -788,0 +789,32 @@\n+    \/**\n+     * {@inheritDoc}\n+     *\n+     * @implSpec This implementation returns {@code null}.\n+     *\n+     * @param node  {@inheritDoc}\n+     * @param p  {@inheritDoc}\n+     * @return the result of scanning\n+     * @since 21\n+     *\/\n+    @Override\n+    public R visitAnyPattern(AnyPatternTree node, P p) {\n+        return null;\n+    }\n+\n+    \/**\n+     * {@inheritDoc}\n+     *\n+     * @implSpec This implementation scans the children in left to right order.\n+     *\n+     * @param node  {@inheritDoc}\n+     * @param p  {@inheritDoc}\n+     * @return the result of scanning\n+     *\/\n+    @Override\n+    @PreviewFeature(feature=PreviewFeature.Feature.STRING_TEMPLATES, reflective=true)\n+    public R visitStringTemplate(StringTemplateTree node, P p) {\n+        R r = scan(node.getProcessor(), p);\n+        r = scanAndReduce(node.getExpressions(), p, r);\n+        return r;\n+    }\n+\n@@ -811,1 +844,1 @@\n-     * @since 17\n+     * @since 21\n@@ -814,1 +847,0 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n@@ -827,1 +859,1 @@\n-     * @since 19\n+     * @since 21\n@@ -830,1 +862,0 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n@@ -843,1 +874,1 @@\n-     * @since 19\n+     * @since 21\n@@ -846,4 +877,1 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-        R r = scan(node.getPattern(), p);\n-        r = scanAndReduce(node.getGuard(), p, r);\n-        return r;\n+        return scan(node.getPattern(), p);\n@@ -861,1 +889,1 @@\n-     * @since 19\n+     * @since 21\n@@ -864,1 +892,0 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.RECORD_PATTERNS, reflective=true)\n@@ -905,16 +932,0 @@\n-     * @param p  {@inheritDoc}\n-     * @return the result of scanning\n-     * @since 17\n-     *\/\n-    @Override\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-    public R visitParenthesizedPattern(ParenthesizedPatternTree node, P p) {\n-        return scan(node.getPattern(), p);\n-    }\n-\n-    \/**\n-     * {@inheritDoc}\n-     *\n-     * @implSpec This implementation scans the children in left to right order.\n-     *\n-     * @param node  {@inheritDoc}\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/util\/TreeScanner.java","additions":39,"deletions":28,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -158,0 +158,16 @@\n+    private final Modules modules;\n+    private final Resolve resolve;\n+    private final Enter enter;\n+    private final Log log;\n+    private final MemberEnter memberEnter;\n+    private final Attr attr;\n+    private final Check chk;\n+    private final TreeMaker treeMaker;\n+    private final JavacElements elements;\n+    private final JavacTaskImpl javacTaskImpl;\n+    private final Names names;\n+    private final Types types;\n+    private final DocTreeMaker docTreeMaker;\n+    private final JavaFileManager fileManager;\n+    private final ParserFactory parser;\n+    private final Symtab syms;\n@@ -159,17 +175,0 @@\n-    \/\/ in a world of a single context per compilation, these would all be final\n-    private Modules modules;\n-    private Resolve resolve;\n-    private Enter enter;\n-    private Log log;\n-    private MemberEnter memberEnter;\n-    private Attr attr;\n-    private Check chk;\n-    private TreeMaker treeMaker;\n-    private JavacElements elements;\n-    private JavacTaskImpl javacTaskImpl;\n-    private Names names;\n-    private Types types;\n-    private DocTreeMaker docTreeMaker;\n-    private JavaFileManager fileManager;\n-    private ParserFactory parser;\n-    private Symtab syms;\n@@ -205,7 +204,0 @@\n-        init(context);\n-    }\n-\n-    public void updateContext(Context context) {\n-        init(context);\n-    }\n-    private void init(Context context) {\n@@ -228,3 +220,2 @@\n-        JavacTask t = context.get(JavacTask.class);\n-        if (t instanceof JavacTaskImpl taskImpl)\n-            javacTaskImpl = taskImpl;\n+        var task = context.get(JavacTask.class);\n+        javacTaskImpl = (task instanceof JavacTaskImpl taskImpl) ? taskImpl : null;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/api\/JavacTrees.java","additions":18,"deletions":27,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -229,0 +229,1 @@\n+        NO_TARGET_ANNOTATION_APPLICABILITY(JDK14),\n@@ -234,2 +235,2 @@\n-        CASE_NULL(JDK17, Fragments.FeatureCaseNull, DiagKind.NORMAL),\n-        PATTERN_SWITCH(JDK17, Fragments.FeaturePatternSwitch, DiagKind.PLURAL),\n+        CASE_NULL(JDK21, Fragments.FeatureCaseNull, DiagKind.NORMAL),\n+        PATTERN_SWITCH(JDK21, Fragments.FeaturePatternSwitch, DiagKind.PLURAL),\n@@ -237,4 +238,5 @@\n-        UNCONDITIONAL_PATTERN_IN_INSTANCEOF(JDK19, Fragments.FeatureUnconditionalPatternsInInstanceof, DiagKind.PLURAL),\n-        RECORD_PATTERNS(JDK19, Fragments.FeatureDeconstructionPatterns, DiagKind.PLURAL),\n-        PRIMITIVE_CLASSES(JDK19, Fragments.FeaturePrimitiveClasses, DiagKind.PLURAL),\n-        VALUE_CLASSES(JDK19, Fragments.FeatureValueClasses, DiagKind.PLURAL),\n+        UNCONDITIONAL_PATTERN_IN_INSTANCEOF(JDK21, Fragments.FeatureUnconditionalPatternsInInstanceof, DiagKind.PLURAL),\n+        RECORD_PATTERNS(JDK21, Fragments.FeatureDeconstructionPatterns, DiagKind.PLURAL),\n+        STRING_TEMPLATES(JDK21, Fragments.FeatureStringTemplates, DiagKind.PLURAL),\n+        PRIMITIVE_CLASSES(JDK21, Fragments.FeaturePrimitiveClasses, DiagKind.PLURAL),\n+        VALUE_CLASSES(JDK21, Fragments.FeatureValueClasses, DiagKind.PLURAL),\n@@ -242,0 +244,1 @@\n+        UNNAMED_VARIABLES(JDK21, Fragments.FeatureUnnamedVariables, DiagKind.PLURAL),\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Source.java","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -1691,0 +1691,1 @@\n+    @SuppressWarnings(\"preview\")\n@@ -1833,0 +1834,4 @@\n+\n+        public boolean isUnnamedVariable() {\n+            return name.isEmpty();\n+        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symbol.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+    public final Type methodHandlesType;\n@@ -197,1 +198,0 @@\n-    public final Type nullPointerExceptionType;\n@@ -230,0 +230,1 @@\n+    public final Type constantBootstrapsType;\n@@ -232,0 +233,2 @@\n+    public final Type classDescType;\n+    public final Type enumDescType;\n@@ -241,0 +244,6 @@\n+    \/\/ For string templates\n+    public final Type stringTemplateType;\n+    public final Type templateRuntimeType;\n+    public final Type processorType;\n+    public final Type linkageType;\n+\n@@ -559,0 +568,1 @@\n+        methodHandlesType = enterClass(\"java.lang.invoke.MethodHandles\");\n@@ -573,1 +583,0 @@\n-        nullPointerExceptionType = enterClass(\"java.lang.NullPointerException\");\n@@ -617,0 +626,1 @@\n+        constantBootstrapsType = enterClass(\"java.lang.invoke.ConstantBootstraps\");\n@@ -619,0 +629,2 @@\n+        classDescType = enterClass(\"java.lang.constant.ClassDesc\");\n+        enumDescType = enterClass(\"java.lang.Enum$EnumDesc\");\n@@ -626,1 +638,0 @@\n-\n@@ -637,0 +648,6 @@\n+        \/\/ For string templates\n+        stringTemplateType = enterClass(\"java.lang.StringTemplate\");\n+        templateRuntimeType = enterClass(\"java.lang.runtime.TemplateRuntime\");\n+        processorType = enterClass(\"java.lang.StringTemplate$Processor\");\n+        linkageType = enterClass(\"java.lang.StringTemplate$Processor$Linkage\");\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symtab.java","additions":20,"deletions":3,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-import com.sun.source.tree.EnhancedForLoopTree;\n@@ -176,2 +175,2 @@\n-        allowUnconditionalPatternsInstanceOf = (preview.isEnabled() || !preview.isPreview(Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF)) &&\n-                                     Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF.allowedInSource(source);\n+        allowUnconditionalPatternsInstanceOf =\n+                             Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF.allowedInSource(source);\n@@ -1533,1 +1532,0 @@\n-\n@@ -1540,2 +1538,2 @@\n-            tree.elementType = types.elemtype(exprType); \/\/ perhaps expr is an array?\n-            if (tree.elementType == null) {\n+            Type elemtype = types.elemtype(exprType); \/\/ perhaps expr is an array?\n+            if (elemtype == null) {\n@@ -1548,1 +1546,1 @@\n-                    tree.elementType = types.createErrorType(exprType);\n+                    elemtype = types.createErrorType(exprType);\n@@ -1551,1 +1549,1 @@\n-                    tree.elementType = iterableParams.isEmpty()\n+                    elemtype = iterableParams.isEmpty()\n@@ -1565,33 +1563,3 @@\n-            if (tree.varOrRecordPattern instanceof JCVariableDecl jcVariableDecl) {\n-                if (jcVariableDecl.isImplicitlyTyped()) {\n-                    Type inferredType = chk.checkLocalVarType(jcVariableDecl, tree.elementType, jcVariableDecl.name);\n-                    setSyntheticVariableType(jcVariableDecl, inferredType);\n-                }\n-                attribStat(jcVariableDecl, loopEnv);\n-                chk.checkType(tree.expr.pos(), tree.elementType, jcVariableDecl.sym.type);\n-\n-                loopEnv.tree = tree; \/\/ before, we were not in loop!\n-                attribStat(tree.body, loopEnv);\n-            } else {\n-                Assert.check(tree.getDeclarationKind() == EnhancedForLoopTree.DeclarationKind.PATTERN);\n-                JCRecordPattern jcRecordPattern = (JCRecordPattern) tree.varOrRecordPattern;\n-\n-                attribExpr(jcRecordPattern, loopEnv, tree.elementType);\n-\n-                \/\/ for(<pattern> x : xs) { y }\n-                \/\/ we include x's bindings when true in y\n-                \/\/ we don't do anything with x's bindings when false\n-\n-                MatchBindings forWithRecordPatternBindings = matchBindings;\n-                Env<AttrContext> recordPatternEnv = bindingEnv(loopEnv, forWithRecordPatternBindings.bindingsWhenTrue);\n-\n-                Type clazztype = jcRecordPattern.type;\n-\n-                checkCastablePattern(tree.expr.pos(), tree.elementType, clazztype);\n-\n-                recordPatternEnv.tree = tree; \/\/ before, we were not in loop!\n-                try {\n-                    attribStat(tree.body, recordPatternEnv);\n-                } finally {\n-                    recordPatternEnv.info.scope.leave();\n-                }\n+            if (tree.var.isImplicitlyTyped()) {\n+                Type inferredType = chk.checkLocalVarType(tree.var, elemtype, tree.var.name);\n+                setSyntheticVariableType(tree.var, inferredType);\n@@ -1599,0 +1567,4 @@\n+            attribStat(tree.var, loopEnv);\n+            chk.checkType(tree.expr.pos(), elemtype, tree.var.sym.type);\n+            loopEnv.tree = tree; \/\/ before, we were not in loop!\n+            attribStat(tree.body, loopEnv);\n@@ -1735,2 +1707,3 @@\n-                boolean wasUnconditionalPattern = hasUnconditionalPattern;\n-                for (JCCaseLabel label : c.labels) {\n+                MatchBindings guardBindings = null;\n+                for (List<JCCaseLabel> labels = c.labels; labels.nonEmpty(); labels = labels.tail) {\n+                    JCCaseLabel label = labels.head;\n@@ -1750,1 +1723,5 @@\n-                                log.error(expr.pos(), Errors.EnumLabelMustBeUnqualifiedEnum);\n+                                if (allowPatternSwitch) {\n+                                    attribTree(expr, switchEnv, caseLabelResultInfo(seltype));\n+                                } else {\n+                                    log.error(expr.pos(), Errors.EnumLabelMustBeUnqualifiedEnum);\n+                                }\n@@ -1766,4 +1743,1 @@\n-                            ResultInfo valTypInfo = new ResultInfo(KindSelector.VAL_TYP,\n-                                                                   !seltype.hasTag(ERROR) ? seltype\n-                                                                                          : Type.noType);\n-                            Type pattype = attribTree(expr, switchEnv, valTypInfo);\n+                            Type pattype = attribTree(expr, switchEnv, caseLabelResultInfo(seltype));\n@@ -1776,1 +1750,1 @@\n-                                    } else {\n+                                    } else if ((s != null && !s.isEnum()) || !allowPatternSwitch) {\n@@ -1805,2 +1779,2 @@\n-                        JCExpression guard = patternlabel.guard;\n-                        if (guard != null) {\n+                        JCExpression guard = c.guard;\n+                        if (guardBindings == null && guard != null) {\n@@ -1814,1 +1788,3 @@\n-                            matchBindings = matchBindingsComputer.caseGuard(c, afterPattern, matchBindings);\n+\n+                            guardBindings = matchBindings;\n+                            matchBindings = afterPattern;\n@@ -1820,1 +1796,1 @@\n-                        boolean unguarded = TreeInfo.unguardedCaseLabel(label) && !pat.hasTag(RECORDPATTERN);\n+                        boolean unguarded = TreeInfo.unguardedCase(c) && !pat.hasTag(RECORDPATTERN);\n@@ -1841,0 +1817,4 @@\n+                if (guardBindings != null) {\n+                    currentBindings = matchBindingsComputer.caseGuard(c, currentBindings, guardBindings);\n+                }\n+\n@@ -1873,0 +1853,5 @@\n+        private ResultInfo caseLabelResultInfo(Type seltype) {\n+            return new ResultInfo(KindSelector.VAL_TYP,\n+                                  !seltype.hasTag(ERROR) ? seltype\n+                                                         : Type.noType);\n+        }\n@@ -4149,1 +4134,0 @@\n-            tree.pattern.getTag() == PARENTHESIZEDPATTERN ||\n@@ -4157,3 +4141,2 @@\n-                    log.error(tree.pos(), Errors.InstanceofPatternNoSubtype(exprtype, clazztype));\n-                } else if (preview.isPreview(Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF)) {\n-                    preview.warnPreview(tree.pattern.pos(), Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF);\n+                    log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(),\n+                              Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF.error(this.sourceName));\n@@ -4216,0 +4199,5 @@\n+    @Override\n+    public void visitAnyPattern(JCAnyPattern tree) {\n+        result = tree.type = resultInfo.pt;\n+    }\n+\n@@ -4237,1 +4225,5 @@\n-        matchBindings = new MatchBindings(List.of(v), List.nil());\n+        if (v.isUnnamedVariable()) {\n+            matchBindings = MatchBindingsComputer.EMPTY;\n+        } else {\n+            matchBindings = new MatchBindings(List.of(v), List.nil());\n+        }\n@@ -4242,7 +4234,15 @@\n-        Type type = attribType(tree.deconstructor, env);\n-        if (type.isRaw() && type.tsym.getTypeParameters().nonEmpty()) {\n-            Type inferred = infer.instantiatePatternType(resultInfo.pt, type.tsym);\n-            if (inferred == null) {\n-                log.error(tree.pos(), Errors.PatternTypeCannotInfer);\n-            } else {\n-                type = inferred;\n+        Type site;\n+\n+        if (tree.deconstructor == null) {\n+            log.error(tree.pos(), Errors.DeconstructionPatternVarNotAllowed);\n+            tree.record = syms.errSymbol;\n+            site = tree.type = types.createErrorType(tree.record.type);\n+        } else {\n+            Type type = attribType(tree.deconstructor, env);\n+            if (type.isRaw() && type.tsym.getTypeParameters().nonEmpty()) {\n+                Type inferred = infer.instantiatePatternType(resultInfo.pt, type.tsym);\n+                if (inferred == null) {\n+                    log.error(tree.pos(), Errors.PatternTypeCannotInfer);\n+                } else {\n+                    type = inferred;\n+                }\n@@ -4250,0 +4250,2 @@\n+            tree.type = tree.deconstructor.type = type;\n+            site = types.capture(tree.type);\n@@ -4251,2 +4253,1 @@\n-        tree.type = tree.deconstructor.type = type;\n-        Type site = types.capture(tree.type);\n+\n@@ -4301,5 +4302,0 @@\n-    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n-        attribExpr(tree.pattern, env);\n-        result = tree.type = tree.pattern.type;\n-    }\n-\n@@ -5085,0 +5081,21 @@\n+    public void visitStringTemplate(JCStringTemplate tree) {\n+        JCExpression processor = tree.processor;\n+        Type resultType = syms.stringTemplateType;\n+\n+        if (processor != null) {\n+            resultType = attribTree(processor, env, new ResultInfo(KindSelector.VAL, Type.noType));\n+            resultType = chk.checkProcessorType(processor, resultType, env);\n+        }\n+\n+        Env<AttrContext> localEnv = env.dup(tree, env.info.dup());\n+\n+        for (JCExpression arg : tree.expressions) {\n+            chk.checkNonVoid(arg.pos(), attribExpr(arg, localEnv));\n+        }\n+\n+        tree.type = resultType;\n+        result = resultType;\n+\n+        check(tree, resultType, KindSelector.VAL, resultInfo);\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":89,"deletions":72,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -120,0 +120,2 @@\n+    public boolean disablePreviewCheck;\n+\n@@ -158,0 +160,2 @@\n+        disablePreviewCheck = false;\n+\n@@ -3770,1 +3774,1 @@\n-                false :\n+                (Feature.NO_TARGET_ANNOTATION_APPLICABILITY.allowedInSource(source) && isTypeParameter) :\n@@ -4050,1 +4054,1 @@\n-        if ((s.flags() & PREVIEW_API) != 0 && !preview.participatesInPreview(syms, other, s)) {\n+        if ((s.flags() & PREVIEW_API) != 0 && !preview.participatesInPreview(syms, other, s) && !disablePreviewCheck) {\n@@ -4568,0 +4572,21 @@\n+    public Type checkProcessorType(JCExpression processor, Type resultType, Env<AttrContext> env) {\n+        Type processorType = processor.type;\n+        Type interfaceType = types.asSuper(processorType, syms.processorType.tsym);\n+\n+        if (interfaceType != null) {\n+            List<Type> typeArguments = interfaceType.getTypeArguments();\n+\n+            if (typeArguments.size() == 2) {\n+                resultType = typeArguments.head;\n+            } else {\n+                log.error(DiagnosticFlag.RESOLVE_ERROR, processor.pos,\n+                        Errors.ProcessorTypeCannotBeARawType(processorType.tsym));\n+            }\n+        } else {\n+            log.error(DiagnosticFlag.RESOLVE_ERROR, processor.pos,\n+                    Errors.NotAProcessorType(processorType.tsym));\n+        }\n+\n+        return resultType;\n+    }\n+\n@@ -4804,2 +4829,5 @@\n-            } else {\n-                if (c.labels.tail.nonEmpty()) {\n+            } else if (c.labels.tail.nonEmpty()) {\n+                var patterCaseLabels = c.labels.stream().filter(ll -> ll instanceof JCPatternCaseLabel).map(cl -> (JCPatternCaseLabel)cl);\n+                var allUnderscore = patterCaseLabels.allMatch(pcl -> !hasBindings(pcl.getPattern()));\n+\n+                if (!allUnderscore) {\n@@ -4840,1 +4868,1 @@\n-                bindings[0] = true;\n+                bindings[0] = !tree.var.sym.isUnnamedVariable();\n@@ -4858,1 +4886,1 @@\n-        List<JCCaseLabel> caseLabels = List.nil();\n+        List<Pair<JCCase, JCCaseLabel>> caseLabels = List.nil();\n@@ -4885,1 +4913,3 @@\n-                for (JCCaseLabel testCaseLabel : caseLabels) {\n+                for (Pair<JCCase, JCCaseLabel> caseAndLabel : caseLabels) {\n+                    JCCase testCase = caseAndLabel.fst;\n+                    JCCaseLabel testCaseLabel = caseAndLabel.snd;\n@@ -4895,1 +4925,1 @@\n-                                   TreeInfo.unguardedCaseLabel(testCaseLabel)) {\n+                                   TreeInfo.unguardedCase(testCase)) {\n@@ -4904,1 +4934,1 @@\n-                caseLabels = caseLabels.prepend(label);\n+                caseLabels = caseLabels.prepend(Pair.of(c, label));\n@@ -4929,6 +4959,0 @@\n-            while (existingPattern instanceof JCParenthesizedPattern parenthesized) {\n-                existingPattern = parenthesized.pattern;\n-            }\n-            while (currentPattern instanceof JCParenthesizedPattern parenthesized) {\n-                currentPattern = parenthesized.pattern;\n-            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":39,"deletions":15,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -35,2 +35,1 @@\n-import java.util.stream.Collectors;\n-import java.util.stream.StreamSupport;\n+import com.sun.source.tree.CaseTree;\n@@ -41,1 +40,0 @@\n-import com.sun.tools.javac.code.Source.Feature;\n@@ -55,0 +53,1 @@\n+import com.sun.tools.javac.code.Kinds.Kind;\n@@ -58,1 +57,0 @@\n-import static com.sun.tools.javac.code.TypeTag.NONE;\n@@ -61,1 +59,0 @@\n-import com.sun.tools.javac.code.Types.UniqueType;\n@@ -66,0 +63,8 @@\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.IdentityHashMap;\n+import java.util.Iterator;\n+import java.util.function.Predicate;\n+import java.util.stream.Collectors;\n+\n+import static java.util.stream.Collectors.groupingBy;\n@@ -216,0 +221,1 @@\n+    private final Infer infer;\n@@ -339,0 +345,1 @@\n+        infer = Infer.instance(context);\n@@ -652,15 +659,1 @@\n-            if(tree.varOrRecordPattern instanceof JCVariableDecl jcVariableDecl) {\n-                visitVarDef(jcVariableDecl);\n-            } else if (tree.varOrRecordPattern instanceof JCRecordPattern jcRecordPattern) {\n-                visitRecordPattern(jcRecordPattern);\n-\n-                Set<Symbol> coveredSymbols =\n-                        coveredSymbols(jcRecordPattern.pos(), List.of(jcRecordPattern));\n-\n-                boolean isExhaustive =\n-                        isExhaustive(jcRecordPattern.pos(), tree.elementType, coveredSymbols);\n-\n-                if (!isExhaustive) {\n-                    log.error(tree, Errors.ForeachNotExhaustiveOnType(jcRecordPattern.type, tree.elementType));\n-                }\n-            }\n+            visitVarDef(tree.var);\n@@ -710,2 +703,1 @@\n-                Set<Symbol> coveredSymbols = coveredSymbolsForCases(tree.pos(), tree.cases);\n-                tree.isExhaustive |= isExhaustive(tree.selector.pos(), tree.selector.type, coveredSymbols);\n+                tree.isExhaustive |= exhausts(tree.selector, tree.cases);\n@@ -745,1 +737,0 @@\n-            Set<Symbol> coveredSymbols = coveredSymbolsForCases(tree.pos(), tree.cases);\n@@ -748,1 +739,1 @@\n-                                isExhaustive(tree.selector.pos(), tree.selector.type, coveredSymbols);\n+                                exhausts(tree.selector, tree.cases);\n@@ -756,23 +747,22 @@\n-        private Set<Symbol> coveredSymbolsForCases(DiagnosticPosition pos,\n-                                                   List<JCCase> cases) {\n-            HashSet<JCTree> labelValues = cases.stream()\n-                                               .flatMap(c -> c.labels.stream())\n-                                               .filter(TreeInfo::unguardedCaseLabel)\n-                                               .filter(l -> !l.hasTag(DEFAULTCASELABEL))\n-                                               .map(l -> l.hasTag(CONSTANTCASELABEL) ? ((JCConstantCaseLabel) l).expr\n-                                                                                     : ((JCPatternCaseLabel) l).pat)\n-                                               .collect(Collectors.toCollection(HashSet::new));\n-            return coveredSymbols(pos, labelValues);\n-        }\n-\n-        private Set<Symbol> coveredSymbols(DiagnosticPosition pos,\n-                                           Iterable<? extends JCTree> labels) {\n-            Set<Symbol> coveredSymbols = new HashSet<>();\n-            Map<UniqueType, List<JCRecordPattern>> deconstructionPatternsByType = new HashMap<>();\n-\n-            for (JCTree labelValue : labels) {\n-                switch (labelValue.getTag()) {\n-                    case BINDINGPATTERN, PARENTHESIZEDPATTERN -> {\n-                        Type primaryPatternType = TreeInfo.primaryPatternType((JCPattern) labelValue);\n-                        if (!primaryPatternType.hasTag(NONE)) {\n-                            coveredSymbols.add(primaryPatternType.tsym);\n+        private boolean exhausts(JCExpression selector, List<JCCase> cases) {\n+            Set<PatternDescription> patternSet = new HashSet<>();\n+            Map<Symbol, Set<Symbol>> enum2Constants = new HashMap<>();\n+            for (JCCase c : cases) {\n+                if (!TreeInfo.unguardedCase(c))\n+                    continue;\n+\n+                for (var l : c.labels) {\n+                    if (l instanceof JCPatternCaseLabel patternLabel) {\n+                        for (Type component : components(selector.type)) {\n+                            patternSet.add(makePatternDescription(component, patternLabel.pat));\n+                        }\n+                    } else if (l instanceof JCConstantCaseLabel constantLabel) {\n+                        Symbol s = TreeInfo.symbol(constantLabel.expr);\n+                        if (s != null && s.isEnum()) {\n+                            enum2Constants.computeIfAbsent(s.owner, x -> {\n+                                Set<Symbol> result = new HashSet<>();\n+                                s.owner.members()\n+                                       .getSymbols(sym -> sym.kind == Kind.VAR && sym.isEnum())\n+                                       .forEach(result::add);\n+                                return result;\n+                            }).remove(s);\n@@ -781,8 +771,19 @@\n-                    case RECORDPATTERN -> {\n-                        JCRecordPattern dpat = (JCRecordPattern) labelValue;\n-                        UniqueType type = new UniqueType(dpat.type, types);\n-                        List<JCRecordPattern> augmentedPatterns =\n-                                deconstructionPatternsByType.getOrDefault(type, List.nil())\n-                                                                 .prepend(dpat);\n-\n-                        deconstructionPatternsByType.put(type, augmentedPatterns);\n+                }\n+            }\n+            for (Entry<Symbol, Set<Symbol>> e : enum2Constants.entrySet()) {\n+                if (e.getValue().isEmpty()) {\n+                    patternSet.add(new BindingPattern(e.getKey().type));\n+                }\n+            }\n+            List<PatternDescription> patterns = List.from(patternSet);\n+            try {\n+                boolean repeat = true;\n+                while (repeat) {\n+                    List<PatternDescription> updatedPatterns;\n+                    updatedPatterns = reduceBindingPatterns(selector.type, patterns);\n+                    updatedPatterns = reduceNestedPatterns(updatedPatterns);\n+                    updatedPatterns = reduceRecordPatterns(updatedPatterns);\n+                    repeat = updatedPatterns != patterns;\n+                    patterns = updatedPatterns;\n+                    if (checkCovered(selector.type, patterns)) {\n+                        return true;\n@@ -790,0 +791,7 @@\n+                }\n+                return checkCovered(selector.type, patterns);\n+            } catch (CompletionFailure cf) {\n+                chk.completionError(selector.pos(), cf);\n+                return true; \/\/error recovery\n+            }\n+        }\n@@ -791,5 +799,6 @@\n-                    default -> {\n-                        Assert.check(labelValue instanceof JCExpression, labelValue.getTag().name());\n-                        JCExpression expr = (JCExpression) labelValue;\n-                        if (expr.hasTag(IDENT) && ((JCIdent) expr).sym.isEnum())\n-                            coveredSymbols.add(((JCIdent) expr).sym);\n+        private boolean checkCovered(Type seltype, List<PatternDescription> patterns) {\n+            for (Type seltypeComponent : components(seltype)) {\n+                for (PatternDescription pd : patterns) {\n+                    if (pd instanceof BindingPattern bp &&\n+                        types.isSubtype(seltypeComponent, types.erasure(bp.type))) {\n+                        return true;\n@@ -799,57 +808,14 @@\n-            for (Entry<UniqueType, List<JCRecordPattern>> e : deconstructionPatternsByType.entrySet()) {\n-                if (e.getValue().stream().anyMatch(r -> r.nested.size() != r.record.getRecordComponents().size())) {\n-                    coveredSymbols.add(syms.errSymbol);\n-                } else if (coversDeconstructionFromComponent(pos, e.getKey().type, e.getValue(), 0)) {\n-                    coveredSymbols.add(e.getKey().type.tsym);\n-                }\n-            }\n-            return coveredSymbols;\n-        }\n-\n-        private boolean coversDeconstructionFromComponent(DiagnosticPosition pos,\n-                                                          Type recordType,\n-                                                          List<JCRecordPattern> deconstructionPatterns,\n-                                                          int component) {\n-            \/\/Given a set of record patterns for the same record, and a starting component,\n-            \/\/this method checks, whether the nested patterns for the components are exhaustive,\n-            \/\/i.e. represent all possible combinations.\n-            \/\/This is done by categorizing the patterns based on the type covered by the given\n-            \/\/starting component.\n-            \/\/For each such category, it is then checked if the nested patterns starting at the next\n-            \/\/component are exhaustive, by recursivelly invoking this method. If these nested patterns\n-            \/\/are exhaustive, the given covered type is accepted.\n-            \/\/All such covered types are then checked whether they cover the declared type of\n-            \/\/the starting component's declaration. If yes, the given set of patterns starting at\n-            \/\/the given component cover the given record exhaustivelly, and true is returned.\n-            List<? extends RecordComponent> components =\n-                    deconstructionPatterns.head.record.getRecordComponents();\n-\n-            if (components.size() == component) {\n-                \/\/no components remain to be checked:\n-                return true;\n-            }\n-\n-            \/\/for the first tested component, gather symbols covered by the nested patterns:\n-            Type instantiatedComponentType = types.memberType(recordType, components.get(component));\n-            List<JCPattern> nestedComponentPatterns = deconstructionPatterns.map(d -> d.nested.get(component));\n-            Set<Symbol> coveredSymbolsForComponent = coveredSymbols(pos,\n-                                                                    nestedComponentPatterns);\n-\n-            \/\/for each of the symbols covered by the starting component, find all deconstruction patterns\n-            \/\/that have the given type, or its supertype, as a type of the starting nested pattern:\n-            Map<Symbol, List<JCRecordPattern>> coveredSymbol2Patterns = new HashMap<>();\n-\n-            for (JCRecordPattern deconstructionPattern : deconstructionPatterns) {\n-                JCPattern nestedPattern = deconstructionPattern.nested.get(component);\n-                Symbol componentPatternType;\n-                switch (nestedPattern.getTag()) {\n-                    case BINDINGPATTERN, PARENTHESIZEDPATTERN -> {\n-                        Type primaryPatternType =\n-                                TreeInfo.primaryPatternType(nestedPattern);\n-                        componentPatternType = primaryPatternType.tsym;\n-                    }\n-                    case RECORDPATTERN -> {\n-                        componentPatternType = ((JCRecordPattern) nestedPattern).record;\n-                    }\n-                    default -> {\n-                        throw Assert.error(\"Unexpected tree kind: \" + nestedPattern.getTag());\n+            return false;\n+        }\n+\n+        private List<Type> components(Type seltype) {\n+            return switch (seltype.getTag()) {\n+                case CLASS -> {\n+                    if (seltype.isCompound()) {\n+                        if (seltype.isIntersection()) {\n+                            yield ((Type.IntersectionClassType) seltype).getComponents()\n+                                                                        .stream()\n+                                                                        .flatMap(t -> components(t).stream())\n+                                                                        .collect(List.collector());\n+                        }\n+                        yield List.nil();\n@@ -857,0 +823,1 @@\n+                    yield List.of(types.erasure(seltype));\n@@ -858,7 +825,91 @@\n-                for (Symbol currentType : coveredSymbolsForComponent) {\n-                    if (types.isSubtype(types.erasure(currentType.type),\n-                                        types.erasure(componentPatternType.type))) {\n-                        coveredSymbol2Patterns.put(currentType,\n-                                                   coveredSymbol2Patterns.getOrDefault(currentType,\n-                                                                                       List.nil())\n-                                              .prepend(deconstructionPattern));\n+                case TYPEVAR -> components(((TypeVar) seltype).getUpperBound());\n+                default -> List.of(types.erasure(seltype));\n+            };\n+        }\n+\n+        \/* In a set of patterns, search for a sub-set of binding patterns that\n+         * in combination exhaust their sealed supertype. If such a sub-set\n+         * is found, it is removed, and replaced with a binding pattern\n+         * for the sealed supertype.\n+         *\/\n+        private List<PatternDescription> reduceBindingPatterns(Type selectorType, List<PatternDescription> patterns) {\n+            Set<Symbol> existingBindings = patterns.stream()\n+                                                   .filter(pd -> pd instanceof BindingPattern)\n+                                                   .map(pd -> ((BindingPattern) pd).type.tsym)\n+                                                   .collect(Collectors.toSet());\n+\n+            for (PatternDescription pdOne : patterns) {\n+                if (pdOne instanceof BindingPattern bpOne) {\n+                    Set<PatternDescription> toRemove = new HashSet<>();\n+                    Set<PatternDescription> toAdd = new HashSet<>();\n+\n+                    for (Type sup : types.directSupertypes(bpOne.type)) {\n+                        ClassSymbol clazz = (ClassSymbol) sup.tsym;\n+\n+                        if (clazz.isSealed() && clazz.isAbstract() &&\n+                            \/\/if a binding pattern for clazz already exists, no need to analyze it again:\n+                            !existingBindings.contains(clazz)) {\n+                            ListBuffer<PatternDescription> bindings = new ListBuffer<>();\n+                            \/\/do not reduce to types unrelated to the selector type:\n+                            Type clazzErasure = types.erasure(clazz.type);\n+                            if (components(selectorType).stream()\n+                                                        .map(types::erasure)\n+                                                        .noneMatch(c -> types.isSubtype(clazzErasure, c))) {\n+                                continue;\n+                            }\n+\n+                            Set<Symbol> permitted = allPermittedSubTypes(clazz, csym -> {\n+                                Type instantiated;\n+                                if (csym.type.allparams().isEmpty()) {\n+                                    instantiated = csym.type;\n+                                } else {\n+                                    instantiated = infer.instantiatePatternType(selectorType, csym);\n+                                }\n+\n+                                return instantiated != null && types.isCastable(selectorType, instantiated);\n+                            });\n+\n+                            for (PatternDescription pdOther : patterns) {\n+                                if (pdOther instanceof BindingPattern bpOther) {\n+                                    boolean reduces = false;\n+                                    Set<Symbol> currentPermittedSubTypes =\n+                                            allPermittedSubTypes((ClassSymbol) bpOther.type.tsym, s -> true);\n+\n+                                    PERMITTED: for (Iterator<Symbol> it = permitted.iterator(); it.hasNext();) {\n+                                        Symbol perm = it.next();\n+\n+                                        for (Symbol currentPermitted : currentPermittedSubTypes) {\n+                                            if (types.isSubtype(types.erasure(currentPermitted.type),\n+                                                                types.erasure(perm.type))) {\n+                                                it.remove();\n+                                                continue PERMITTED;\n+                                            }\n+                                        }\n+                                        if (types.isSubtype(types.erasure(perm.type),\n+                                                            types.erasure(bpOther.type))) {\n+                                            it.remove();\n+                                            reduces = true;\n+                                        }\n+                                    }\n+\n+                                    if (reduces) {\n+                                        bindings.append(pdOther);\n+                                    }\n+                                }\n+                            }\n+\n+                            if (permitted.isEmpty()) {\n+                                toRemove.addAll(bindings);\n+                                toAdd.add(new BindingPattern(clazz.type));\n+                            }\n+                        }\n+                    }\n+\n+                    if (!toAdd.isEmpty() || !toRemove.isEmpty()) {\n+                        for (PatternDescription pd : toRemove) {\n+                            patterns = List.filter(patterns, pd);\n+                        }\n+                        for (PatternDescription pd : toAdd) {\n+                            patterns = patterns.prepend(pd);\n+                        }\n+                        return patterns;\n@@ -868,0 +919,11 @@\n+            return patterns;\n+        }\n+\n+        private Set<Symbol> allPermittedSubTypes(ClassSymbol root, Predicate<ClassSymbol> accept) {\n+            Set<Symbol> permitted = new HashSet<>();\n+            List<ClassSymbol> permittedSubtypesClosure = List.of(root);\n+\n+            while (permittedSubtypesClosure.nonEmpty()) {\n+                ClassSymbol current = permittedSubtypesClosure.head;\n+\n+                permittedSubtypesClosure = permittedSubtypesClosure.tail;\n@@ -869,4 +931,3 @@\n-            \/\/Check the components following the starting component, for each of the covered symbol,\n-            \/\/if they are exhaustive. If yes, the given covered symbol should be part of the following\n-            \/\/exhaustiveness check:\n-            Set<Symbol> covered = new HashSet<>();\n+                if (current.isSealed() && current.isAbstract()) {\n+                    for (Symbol sym : current.permitted) {\n+                        ClassSymbol csym = (ClassSymbol) sym;\n@@ -874,3 +935,5 @@\n-            for (Entry<Symbol, List<JCRecordPattern>> e : coveredSymbol2Patterns.entrySet()) {\n-                if (coversDeconstructionFromComponent(pos, recordType, e.getValue(), component + 1)) {\n-                    covered.add(e.getKey());\n+                        if (accept.test(csym)) {\n+                            permittedSubtypesClosure = permittedSubtypesClosure.prepend(csym);\n+                            permitted.add(csym);\n+                        }\n+                    }\n@@ -880,2 +943,1 @@\n-            \/\/verify whether the filtered symbols cover the given record's declared type:\n-            return isExhaustive(pos, instantiatedComponentType, covered);\n+            return permitted;\n@@ -884,13 +946,70 @@\n-        private void transitiveCovers(DiagnosticPosition pos, Type seltype, Set<Symbol> covered) {\n-            List<Symbol> todo = List.from(covered);\n-            while (todo.nonEmpty()) {\n-                Symbol sym = todo.head;\n-                todo = todo.tail;\n-                switch (sym.kind) {\n-                    case VAR -> {\n-                        Iterable<Symbol> constants = sym.owner\n-                                                        .members()\n-                                                        .getSymbols(s -> s.isEnum() &&\n-                                                                         s.kind == VAR);\n-                        boolean hasAll = StreamSupport.stream(constants.spliterator(), false)\n-                                                      .allMatch(covered::contains);\n+        \/* Among the set of patterns, find sub-set of patterns such:\n+         * $record($prefix$, $nested, $suffix$)\n+         * Where $record, $prefix$ and $suffix$ is the same for each pattern\n+         * in the set, and the patterns only differ in one \"column\" in\n+         * the $nested pattern.\n+         * Then, the set of $nested patterns is taken, and passed recursively\n+         * to reduceNestedPatterns and to reduceBindingPatterns, to\n+         * simplify the pattern. If that succeeds, the original found sub-set\n+         * of patterns is replaced with a new set of patterns of the form:\n+         * $record($prefix$, $resultOfReduction, $suffix$)\n+         *\/\n+        private List<PatternDescription> reduceNestedPatterns(List<PatternDescription> patterns) {\n+            \/* implementation note:\n+             * finding a sub-set of patterns that only differ in a single\n+             * column is time-consuming task, so this method speeds it up by:\n+             * - group the patterns by their record class\n+             * - for each column (nested pattern) do:\n+             * -- group patterns by their hash\n+             * -- in each such by-hash group, find sub-sets that only differ in\n+             *    the chosen column, and then call reduceBindingPatterns and reduceNestedPatterns\n+             *    on patterns in the chosen column, as described above\n+             *\/\n+            var groupByRecordClass =\n+                    patterns.stream()\n+                            .filter(pd -> pd instanceof RecordPattern)\n+                            .map(pd -> (RecordPattern) pd)\n+                            .collect(groupingBy(pd -> (ClassSymbol) pd.recordType.tsym));\n+\n+            for (var e : groupByRecordClass.entrySet()) {\n+                int nestedPatternsCount = e.getKey().getRecordComponents().size();\n+\n+                for (int mismatchingCandidate = 0;\n+                     mismatchingCandidate < nestedPatternsCount;\n+                     mismatchingCandidate++) {\n+                    int mismatchingCandidateFin = mismatchingCandidate;\n+                    var groupByHashes =\n+                            e.getValue()\n+                             .stream()\n+                             \/\/error recovery, ignore patterns with incorrect number of nested patterns:\n+                             .filter(pd -> pd.nested.length == nestedPatternsCount)\n+                             .collect(groupingBy(pd -> pd.hashCode(mismatchingCandidateFin)));\n+                    for (var candidates : groupByHashes.values()) {\n+                        var candidatesArr = candidates.toArray(RecordPattern[]::new);\n+\n+                        for (int firstCandidate = 0;\n+                             firstCandidate < candidatesArr.length;\n+                             firstCandidate++) {\n+                            RecordPattern rpOne = candidatesArr[firstCandidate];\n+                            ListBuffer<RecordPattern> join = new ListBuffer<>();\n+\n+                            join.append(rpOne);\n+\n+                            NEXT_PATTERN: for (int nextCandidate = 0;\n+                                               nextCandidate < candidatesArr.length;\n+                                               nextCandidate++) {\n+                                if (firstCandidate == nextCandidate) {\n+                                    continue;\n+                                }\n+\n+                                RecordPattern rpOther = candidatesArr[nextCandidate];\n+                                if (rpOne.recordType.tsym == rpOther.recordType.tsym) {\n+                                    for (int i = 0; i < rpOne.nested.length; i++) {\n+                                        if (i != mismatchingCandidate &&\n+                                            !rpOne.nested[i].equals(rpOther.nested[i])) {\n+                                            continue NEXT_PATTERN;\n+                                        }\n+                                    }\n+                                    join.append(rpOther);\n+                                }\n+                            }\n@@ -898,4 +1017,17 @@\n-                        if (hasAll && covered.add(sym.owner)) {\n-                            todo = todo.prepend(sym.owner);\n-                        }\n-                    }\n+                            var nestedPatterns = join.stream().map(rp -> rp.nested[mismatchingCandidateFin]).collect(List.collector());\n+                            var updatedPatterns = reduceNestedPatterns(nestedPatterns);\n+\n+                            updatedPatterns = reduceRecordPatterns(updatedPatterns);\n+                            updatedPatterns = reduceBindingPatterns(rpOne.fullComponentTypes()[mismatchingCandidateFin], updatedPatterns);\n+\n+                            if (nestedPatterns != updatedPatterns) {\n+                                ListBuffer<PatternDescription> result = new ListBuffer<>();\n+                                Set<PatternDescription> toRemove = Collections.newSetFromMap(new IdentityHashMap<>());\n+\n+                                toRemove.addAll(join);\n+\n+                                for (PatternDescription p : patterns) {\n+                                    if (!toRemove.contains(p)) {\n+                                        result.append(p);\n+                                    }\n+                                }\n@@ -903,6 +1035,7 @@\n-                    case TYP -> {\n-                        for (Type sup : types.directSupertypes(sym.type)) {\n-                            if (sup.tsym.kind == TYP) {\n-                                if (isTransitivelyCovered(pos, seltype, sup.tsym, covered) &&\n-                                    covered.add(sup.tsym)) {\n-                                    todo = todo.prepend(sup.tsym);\n+                                for (PatternDescription nested : updatedPatterns) {\n+                                    PatternDescription[] newNested =\n+                                            Arrays.copyOf(rpOne.nested, rpOne.nested.length);\n+                                    newNested[mismatchingCandidateFin] = nested;\n+                                    result.append(new RecordPattern(rpOne.recordType(),\n+                                                                    rpOne.fullComponentTypes(),\n+                                                                    newNested));\n@@ -910,0 +1043,1 @@\n+                                return result.toList();\n@@ -915,0 +1049,1 @@\n+            return patterns;\n@@ -917,17 +1052,18 @@\n-        private boolean isTransitivelyCovered(DiagnosticPosition pos, Type seltype,\n-                                              Symbol sealed, Set<Symbol> covered) {\n-            try {\n-                if (covered.stream().anyMatch(c -> sealed.isSubClass(c, types)))\n-                    return true;\n-                if (sealed.kind == TYP && sealed.isAbstract() && sealed.isSealed()) {\n-                    return ((ClassSymbol) sealed).permitted\n-                                                 .stream()\n-                                                 .filter(s -> {\n-                                                     return types.isCastable(seltype, s.type\/*, types.noWarnings*\/);\n-                                                 })\n-                                                 .allMatch(s -> isTransitivelyCovered(pos, seltype, s, covered));\n-                }\n-                return false;\n-            } catch (CompletionFailure cf) {\n-                chk.completionError(pos, cf);\n-                return true;\n+        \/* In the set of patterns, find those for which, given:\n+         * $record($nested1, $nested2, ...)\n+         * all the $nestedX pattern cover the given record component,\n+         * and replace those with a simple binding pattern over $record.\n+         *\/\n+        private List<PatternDescription> reduceRecordPatterns(List<PatternDescription> patterns) {\n+            var newPatterns = new ListBuffer<PatternDescription>();\n+            boolean modified = false;\n+            for (PatternDescription pd : patterns) {\n+                if (pd instanceof RecordPattern rpOne) {\n+                    PatternDescription reducedPattern = reduceRecordPattern(rpOne);\n+                    if (reducedPattern != rpOne) {\n+                        newPatterns.append(reducedPattern);\n+                        modified = true;\n+                        continue;\n+                    }\n+                }\n+                newPatterns.append(pd);\n@@ -935,1 +1071,2 @@\n-        }\n+            return modified ? newPatterns.toList() : patterns;\n+                }\n@@ -937,9 +1074,14 @@\n-        private boolean isExhaustive(DiagnosticPosition pos, Type seltype, Set<Symbol> covered) {\n-            transitiveCovers(pos, seltype, covered);\n-            return switch (seltype.getTag()) {\n-                case CLASS -> {\n-                    if (seltype.isCompound()) {\n-                        if (seltype.isIntersection()) {\n-                            yield ((Type.IntersectionClassType) seltype).getComponents()\n-                                                                        .stream()\n-                                                                        .anyMatch(t -> isExhaustive(pos, t, covered));\n+        private PatternDescription reduceRecordPattern(PatternDescription pattern) {\n+            if (pattern instanceof RecordPattern rpOne) {\n+                Type[] componentType = rpOne.fullComponentTypes();\n+                \/\/error recovery, ignore patterns with incorrect number of nested patterns:\n+                if (componentType.length != rpOne.nested.length) {\n+                    return pattern;\n+                }\n+                PatternDescription[] reducedNestedPatterns = null;\n+                boolean covered = true;\n+                for (int i = 0; i < componentType.length; i++) {\n+                    PatternDescription newNested = reduceRecordPattern(rpOne.nested[i]);\n+                    if (newNested != rpOne.nested[i]) {\n+                        if (reducedNestedPatterns == null) {\n+                            reducedNestedPatterns = Arrays.copyOf(rpOne.nested, rpOne.nested.length);\n@@ -947,1 +1089,1 @@\n-                        yield false;\n+                        reducedNestedPatterns[i] = newNested;\n@@ -949,4 +1091,3 @@\n-                    yield covered.stream()\n-                                 .filter(coveredSym -> coveredSym.kind == TYP)\n-                                 .anyMatch(coveredSym -> types.isSubtype(types.erasure(seltype),\n-                                                                         types.erasure(coveredSym.type)));\n+\n+                    covered &= newNested instanceof BindingPattern bp &&\n+                               types.isSubtype(types.erasure(componentType[i]), types.erasure(bp.type));\n@@ -954,3 +1095,4 @@\n-                case TYPEVAR -> isExhaustive(pos, ((TypeVar) seltype).getUpperBound(), covered);\n-                default -> {\n-                    yield covered.contains(types.erasure(seltype).tsym);\n+                if (covered) {\n+                    return new BindingPattern(rpOne.recordType);\n+                } else if (reducedNestedPatterns != null) {\n+                    return new RecordPattern(rpOne.recordType, rpOne.fullComponentTypes(), reducedNestedPatterns);\n@@ -958,1 +1100,2 @@\n-            };\n+            }\n+            return pattern;\n@@ -1379,5 +1522,1 @@\n-            if(tree.varOrRecordPattern instanceof JCVariableDecl jcVariableDecl) {\n-                visitVarDef(jcVariableDecl);\n-            } else if (tree.varOrRecordPattern instanceof JCRecordPattern jcRecordPattern) {\n-                visitRecordPattern(jcRecordPattern);\n-            }\n+            visitVarDef(tree.var);\n@@ -1533,0 +1672,24 @@\n+        @Override\n+        public void visitStringTemplate(JCStringTemplate tree) {\n+            JCExpression processor = tree.processor;\n+\n+            if (processor != null) {\n+                scan(processor);\n+                Type interfaceType = types.asSuper(processor.type, syms.processorType.tsym);\n+\n+                if (interfaceType != null) {\n+                    List<Type> typeArguments = interfaceType.getTypeArguments();\n+\n+                    if (typeArguments.size() == 2) {\n+                        Type throwType = typeArguments.tail.head;\n+\n+                        if (throwType != null) {\n+                            markThrown(tree, throwType);\n+                        }\n+                    }\n+                }\n+            }\n+\n+            scan(tree.expressions);\n+        }\n+\n@@ -2191,4 +2354,0 @@\n-            if (inits.isReset()) {\n-                inits.assign(initsWhenTrue);\n-                uninits.assign(uninitsWhenTrue);\n-            }\n@@ -2581,0 +2740,2 @@\n+            visitVarDef(tree.var);\n+\n@@ -2589,7 +2750,1 @@\n-            if(tree.varOrRecordPattern instanceof JCVariableDecl jcVariableDecl) {\n-                visitVarDef(jcVariableDecl);\n-                letInit(tree.pos(), jcVariableDecl.sym);\n-            } else if (tree.varOrRecordPattern instanceof JCRecordPattern jcRecordPattern) {\n-                visitRecordPattern(jcRecordPattern);\n-            }\n-\n+            letInit(tree.pos(), tree.var.sym);\n@@ -2647,11 +2802,4 @@\n-                if (l.head.stats.isEmpty() &&\n-                    l.tail.nonEmpty() &&\n-                    l.tail.head.labels.size() == 1 &&\n-                    TreeInfo.isNullCaseLabel(l.tail.head.labels.head)) {\n-                    \/\/handling:\n-                    \/\/case Integer i:\n-                    \/\/case null:\n-                    \/\/joining these two cases together - processing Integer i pattern,\n-                    \/\/but statements from case null:\n-                    l = l.tail;\n-                    c = l.head;\n+                scan(c.guard);\n+                if (inits.isReset()) {\n+                    inits.assign(initsWhenTrue);\n+                    uninits.assign(uninitsWhenTrue);\n@@ -2722,1 +2870,1 @@\n-                    if (unrefdResources.includes(resVar.sym)) {\n+                    if (unrefdResources.includes(resVar.sym) && !resVar.sym.isUnnamedVariable()) {\n@@ -2916,0 +3064,1 @@\n+            final Bits prevUninitsTry = new Bits(uninitsTry);\n@@ -2943,0 +3092,1 @@\n+                uninitsTry.assign(prevUninitsTry);\n@@ -3072,6 +3222,0 @@\n-        @Override\n-        public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n-            scan(tree.pat);\n-            scan(tree.guard);\n-        }\n-\n@@ -3148,0 +3292,1 @@\n+        WriteableScope declaredInsideGuard;\n@@ -3161,1 +3306,1 @@\n-                    case PATTERNCASELABEL:\n+                    case CASE:\n@@ -3175,4 +3320,1 @@\n-                if (currentTree != null &&\n-                        sym.kind == VAR &&\n-                        sym.owner.kind == MTH &&\n-                        ((VarSymbol)sym).pos < currentTree.getStartPosition()) {\n+                if (currentTree != null) {\n@@ -3180,4 +3322,12 @@\n-                        case CLASSDEF:\n-                        case CASE:\n-                        case LAMBDA:\n-                            reportEffectivelyFinalError(tree, sym);\n+                        case CLASSDEF, LAMBDA -> {\n+                            if (sym.kind == VAR &&\n+                                sym.owner.kind == MTH &&\n+                                ((VarSymbol)sym).pos < currentTree.getStartPosition()) {\n+                                reportEffectivelyFinalError(tree, sym);\n+                            }\n+                        }\n+                        case CASE -> {\n+                            if (!declaredInsideGuard.includes(sym)) {\n+                                log.error(tree.pos(), Errors.CannotAssignNotDeclaredGuard(sym));\n+                            }\n+                        }\n@@ -3192,1 +3342,1 @@\n-                case PATTERNCASELABEL -> Fragments.Guard;\n+                case CASE -> Fragments.Guard;\n@@ -3232,13 +3382,13 @@\n-        public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n-            scan(tree.pattern);\n-        }\n-\n-        @Override\n-        public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n-             scan(tree.pat);\n-            JCTree prevTree = currentTree;\n-            try {\n-                currentTree = tree;\n-                scan(tree.guard);\n-            } finally {\n-                currentTree = prevTree;\n+        public void visitCase(JCCase tree) {\n+            scan(tree.labels);\n+            if (tree.guard != null) {\n+                JCTree prevTree = currentTree;\n+                WriteableScope prevDeclaredInsideGuard = declaredInsideGuard;\n+                try {\n+                    currentTree = tree;\n+                    declaredInsideGuard = WriteableScope.create(attrEnv.enclClass.sym);\n+                    scan(tree.guard);\n+                } finally {\n+                    currentTree = prevTree;\n+                    declaredInsideGuard = prevDeclaredInsideGuard;\n+                }\n@@ -3246,0 +3396,1 @@\n+            scan(tree.stats);\n@@ -3300,0 +3451,8 @@\n+        @Override\n+        public void visitVarDef(JCVariableDecl tree) {\n+            if (declaredInsideGuard != null) {\n+                declaredInsideGuard.enter(tree.sym);\n+            }\n+            super.visitVarDef(tree);\n+        }\n+\n@@ -3384,0 +3543,89 @@\n+    sealed interface PatternDescription { }\n+    public PatternDescription makePatternDescription(Type selectorType, JCPattern pattern) {\n+        if (pattern instanceof JCBindingPattern binding) {\n+            Type type = types.isSubtype(selectorType, binding.type)\n+                    ? selectorType : binding.type;\n+            return new BindingPattern(type);\n+        } else if (pattern instanceof JCRecordPattern record) {\n+            Type[] componentTypes;\n+\n+            if (!record.type.isErroneous()) {\n+                componentTypes = ((ClassSymbol) record.type.tsym).getRecordComponents()\n+                        .map(r -> types.memberType(record.type, r))\n+                        .toArray(s -> new Type[s]);\n+            }\n+            else {\n+                componentTypes = record.nested.map(t -> types.createErrorType(t.type)).toArray(s -> new Type[s]);;\n+            }\n+\n+            PatternDescription[] nestedDescriptions =\n+                    new PatternDescription[record.nested.size()];\n+            int i = 0;\n+            for (List<JCPattern> it = record.nested;\n+                 it.nonEmpty();\n+                 it = it.tail, i++) {\n+                nestedDescriptions[i] = makePatternDescription(types.erasure(componentTypes[i]), it.head);\n+            }\n+            return new RecordPattern(record.type, componentTypes, nestedDescriptions);\n+        } else if (pattern instanceof JCAnyPattern) {\n+            Type type = types.isSubtype(selectorType, syms.objectType)\n+                    ? selectorType : syms.objectType;\n+            return new BindingPattern(type);\n+        } else {\n+            throw Assert.error();\n+        }\n+    }\n+    record BindingPattern(Type type) implements PatternDescription {\n+        @Override\n+        public int hashCode() {\n+            return type.tsym.hashCode();\n+        }\n+        @Override\n+        public boolean equals(Object o) {\n+            return o instanceof BindingPattern other &&\n+                    type.tsym == other.type.tsym;\n+        }\n+        @Override\n+        public String toString() {\n+            return type.tsym + \" _\";\n+        }\n+    }\n+    record RecordPattern(Type recordType, int _hashCode, Type[] fullComponentTypes, PatternDescription... nested) implements PatternDescription {\n+\n+        public RecordPattern(Type recordType, Type[] fullComponentTypes, PatternDescription[] nested) {\n+            this(recordType, hashCode(-1, recordType, nested), fullComponentTypes, nested);\n+        }\n+\n+        @Override\n+        public int hashCode() {\n+            return _hashCode;\n+        }\n+\n+        @Override\n+        public boolean equals(Object o) {\n+            return o instanceof RecordPattern other &&\n+                    recordType.tsym == other.recordType.tsym &&\n+                    Arrays.equals(nested, other.nested);\n+        }\n+\n+        public int hashCode(int excludeComponent) {\n+            return hashCode(excludeComponent, recordType, nested);\n+        }\n+\n+        public static int hashCode(int excludeComponent, Type recordType, PatternDescription... nested) {\n+            int hash = 5;\n+            hash =  41 * hash + recordType.tsym.hashCode();\n+            for (int  i = 0; i < nested.length; i++) {\n+                if (i != excludeComponent) {\n+                    hash = 41 * hash + nested[i].hashCode();\n+                }\n+            }\n+            return hash;\n+        }\n+        @Override\n+        public String toString() {\n+            return recordType.tsym + \"(\" + Arrays.stream(nested)\n+                    .map(pd -> pd.toString())\n+                    .collect(Collectors.joining(\", \")) + \")\";\n+        }\n+    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":497,"deletions":249,"binary":false,"changes":746,"status":"modified"},{"patch":"@@ -707,1 +707,1 @@\n-            cases.add(make.Case(JCCase.STATEMENT, List.of(make.ConstantCaseLabel(make.Literal(entry.getKey()))), stmts, null));\n+            cases.add(make.Case(JCCase.STATEMENT, List.of(make.ConstantCaseLabel(make.Literal(entry.getKey()))), null, stmts, null));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/LambdaToMethod.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,0 @@\n-import com.sun.source.tree.EnhancedForLoopTree;\n@@ -3587,9 +3586,5 @@\n-\n-            Assert.check(tree.getDeclarationKind() == EnhancedForLoopTree.DeclarationKind.VARIABLE);\n-            JCVariableDecl jcVariableDecl = (JCVariableDecl) tree.varOrRecordPattern;\n-\n-            JCVariableDecl loopvardef = (JCVariableDecl)make.VarDef(jcVariableDecl.mods,\n-                    jcVariableDecl.name,\n-                    jcVariableDecl.vartype,\n-                    loopvarinit).setType(jcVariableDecl.type);\n-            loopvardef.sym = jcVariableDecl.sym;\n+            JCVariableDecl loopvardef = (JCVariableDecl)make.VarDef(tree.var.mods,\n+                                                  tree.var.name,\n+                                                  tree.var.vartype,\n+                                                  loopvarinit).setType(tree.var.type);\n+            loopvardef.sym = tree.var.sym;\n@@ -3597,2 +3592,1 @@\n-                    Block(0, List.of(loopvardef, tree.body));\n-\n+                Block(0, List.of(loopvardef, tree.body));\n@@ -3677,5 +3671,1 @@\n-\n-            Assert.check(tree.getDeclarationKind() == EnhancedForLoopTree.DeclarationKind.VARIABLE);\n-\n-            JCVariableDecl var = (JCVariableDecl) tree.varOrRecordPattern;\n-            if (var.type.isPrimitive())\n+            if (tree.var.type.isPrimitive())\n@@ -3684,6 +3674,6 @@\n-                vardefinit = make.TypeCast(var.type, vardefinit);\n-            JCVariableDecl indexDef = (JCVariableDecl) make.VarDef(var.mods,\n-                    var.name,\n-                    var.vartype,\n-                    vardefinit).setType(var.type);\n-            indexDef.sym = var.sym;\n+                vardefinit = make.TypeCast(tree.var.type, vardefinit);\n+            JCVariableDecl indexDef = (JCVariableDecl)make.VarDef(tree.var.mods,\n+                                                  tree.var.name,\n+                                                  tree.var.vartype,\n+                                                  vardefinit).setType(tree.var.type);\n+            indexDef.sym = tree.var.sym;\n@@ -3693,4 +3683,4 @@\n-                    ForLoop(List.of(init),\n-                            cond,\n-                            List.nil(),\n-                            body));\n+                ForLoop(List.of(init),\n+                        cond,\n+                        List.nil(),\n+                        body));\n@@ -3799,1 +3789,1 @@\n-            JCCase c = make.Case(JCCase.STATEMENT, List.of(make.DefaultCaseLabel()), List.of(thr), null);\n+            JCCase c = make.Case(JCCase.STATEMENT, List.of(make.DefaultCaseLabel()), null, List.of(thr), null);\n@@ -3826,0 +3816,1 @@\n+                                                           null,\n@@ -3920,1 +3911,1 @@\n-                newCases.append(make.Case(JCCase.STATEMENT, List.of(make.ConstantCaseLabel(pat)), c.stats, null));\n+                newCases.append(make.Case(JCCase.STATEMENT, List.of(make.ConstantCaseLabel(pat)), null, c.stats, null));\n@@ -4092,0 +4083,1 @@\n+                                            null,\n@@ -4127,0 +4119,1 @@\n+                                    null,\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Lower.java","additions":22,"deletions":29,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+import com.sun.tools.javac.code.Source.Feature;\n@@ -58,0 +59,2 @@\n+    \/** The Source language setting. *\/\n+    private final Source source;\n@@ -65,0 +68,1 @@\n+    private final Names names;\n@@ -85,0 +89,2 @@\n+        source = Source.instance(context);\n+        names = Names.instance(context);\n@@ -315,1 +321,2 @@\n-        VarSymbol v = new VarSymbol(0, tree.name, vartype, enclScope.owner);\n+        Name name = tree.name;\n+        VarSymbol v = new VarSymbol(0, name, vartype, enclScope.owner);\n@@ -330,6 +337,9 @@\n-        if (chk.checkUnique(tree.pos(), v, enclScope)) {\n-            chk.checkTransparentVar(tree.pos(), v, enclScope);\n-            enclScope.enter(v);\n-        } else if (v.owner.kind == MTH || (v.flags_field & (Flags.PRIVATE | Flags.FINAL | Flags.GENERATED_MEMBER | Flags.RECORD)) != 0) {\n-            \/\/ if this is a parameter or a field obtained from a record component, enter it\n-            enclScope.enter(v);\n+\n+        if(!(Feature.UNNAMED_VARIABLES.allowedInSource(source) && tree.sym.isUnnamedVariable())) {\n+            if (chk.checkUnique(tree.pos(), v, enclScope)) {\n+                chk.checkTransparentVar(tree.pos(), v, enclScope);\n+                enclScope.enter(v);\n+            } else if (v.owner.kind == MTH || (v.flags_field & (Flags.PRIVATE | Flags.FINAL | Flags.GENERATED_MEMBER | Flags.RECORD)) != 0) {\n+                \/\/ if this is a parameter or a field obtained from a record component, enter it\n+                enclScope.enter(v);\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/MemberEnter.java","additions":17,"deletions":7,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -522,1 +522,1 @@\n-        tree.varOrRecordPattern = translate(tree.varOrRecordPattern, null);\n+        tree.var = translate(tree.var, null);\n@@ -561,0 +561,1 @@\n+        tree.guard = translate(tree.guard, syms.booleanType);\n@@ -565,0 +566,5 @@\n+    @Override\n+    public void visitAnyPattern(JCAnyPattern tree) {\n+        result = tree;\n+    }\n+\n@@ -579,1 +585,0 @@\n-        tree.guard = translate(tree.guard, syms.booleanType);\n@@ -594,6 +599,0 @@\n-    @Override\n-    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n-        tree.pattern = translate(tree.pattern, null);\n-        result = tree;\n-    }\n-\n@@ -848,0 +847,7 @@\n+\n+    public void visitStringTemplate(JCStringTemplate tree) {\n+        tree.expressions = tree.expressions.stream()\n+                .map(e -> translate(e, erasure(e.type))).collect(List.collector());\n+        tree.type = erasure(tree.type);\n+        result = tree;\n+    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TransTypes.java","additions":14,"deletions":8,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -28,1 +28,0 @@\n-import java.util.ArrayList;\n@@ -32,1 +31,0 @@\n-import java.util.stream.Collectors;\n@@ -39,0 +37,1 @@\n+import com.sun.tools.javac.code.Scope.ImportScope;\n@@ -45,0 +44,2 @@\n+import com.sun.tools.javac.parser.Parser;\n+import com.sun.tools.javac.parser.ParserFactory;\n@@ -118,0 +119,1 @@\n+    private final ParserFactory parserFactory;\n@@ -146,0 +148,1 @@\n+        parserFactory = ParserFactory.instance(context);\n@@ -331,0 +334,34 @@\n+        private void importJavaLang(JCCompilationUnit tree, Env<AttrContext> env, ImportFilter typeImportFilter) {\n+            \/\/ Import-on-demand java.lang.\n+            PackageSymbol javaLang = syms.enterPackage(syms.java_base, names.java_lang);\n+            if (javaLang.members().isEmpty() && !javaLang.exists()) {\n+                log.error(Errors.NoJavaLang);\n+                throw new Abort();\n+            }\n+            importAll(make.at(tree.pos()).Import(make.Select(make.QualIdent(javaLang.owner), javaLang), false),\n+                javaLang, env);\n+        }\n+\n+        private void staticImports(JCCompilationUnit tree, Env<AttrContext> env, ImportFilter staticImportFilter) {\n+             if (preview.isEnabled() && preview.isPreview(Feature.STRING_TEMPLATES)) {\n+                Lint prevLint = chk.setLint(lint.suppress(LintCategory.DEPRECATION, LintCategory.REMOVAL, LintCategory.PREVIEW));\n+                boolean prevPreviewCheck = chk.disablePreviewCheck;\n+\n+                try {\n+                    chk.disablePreviewCheck = true;\n+                    String autoImports = \"\"\"\n+                            import static java.lang.StringTemplate.STR;\n+                            \"\"\";\n+                    Parser parser = parserFactory.newParser(autoImports, false, false, false, false);\n+                    JCCompilationUnit importTree = parser.parseCompilationUnit();\n+\n+                    for (JCImport imp : importTree.getImports()) {\n+                        doImport(imp);\n+                    }\n+                } finally {\n+                    chk.setLint(prevLint);\n+                    chk.disablePreviewCheck = prevPreviewCheck;\n+                }\n+            }\n+        }\n+\n@@ -353,8 +390,2 @@\n-                \/\/ Import-on-demand java.lang.\n-                PackageSymbol javaLang = syms.enterPackage(syms.java_base, names.java_lang);\n-                if (javaLang.members().isEmpty() && !javaLang.exists()) {\n-                    log.error(Errors.NoJavaLang);\n-                    throw new Abort();\n-                }\n-                importAll(make.at(tree.pos()).Import(make.Select(make.QualIdent(javaLang.owner), javaLang), false),\n-                    javaLang, env);\n+                importJavaLang(tree, env, typeImportFilter);\n+                staticImports(tree, env, staticImportFilter);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TypeEnter.java","additions":41,"deletions":10,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -302,1 +302,1 @@\n-            sr.mergeWith(csp(tree.varOrRecordPattern));\n+            sr.mergeWith(csp(tree.var));\n@@ -332,0 +332,1 @@\n+            sr.mergeWith(csp(tree.guard));\n@@ -352,1 +353,0 @@\n-            sr.mergeWith(csp(tree.guard));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/CRTable.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -214,1 +214,7 @@\n-    int[] parameterNameIndices;\n+    int[] parameterNameIndicesLvt;\n+\n+    \/**\n+     * A table to hold the constant pool indices for method parameter\n+     * names, as given in the MethodParameters attribute.\n+     *\/\n+    int[] parameterNameIndicesMp;\n@@ -241,12 +247,0 @@\n-    \/**\n-     * Whether or not any parameter names have been found.\n-     *\/\n-    boolean haveParameterNameIndices;\n-\n-    \/** Set this to false every time we start reading a method\n-     * and are saving parameter names.  Set it to true when we see\n-     * MethodParameters, if it's set when we see a LocalVariableTable,\n-     * then we ignore the parameter names from the LVT.\n-     *\/\n-    boolean sawMethodParameters;\n-\n@@ -983,1 +977,1 @@\n-                    if (saveParameterNames && !sawMethodParameters) {\n+                    if (saveParameterNames) {\n@@ -1002,1 +996,1 @@\n-                                if (register >= parameterNameIndices.length) {\n+                                if (register >= parameterNameIndicesLvt.length) {\n@@ -1004,3 +998,3 @@\n-                                            Math.max(register + 1, parameterNameIndices.length + 8);\n-                                    parameterNameIndices =\n-                                            Arrays.copyOf(parameterNameIndices, newSize);\n+                                            Math.max(register + 1, parameterNameIndicesLvt.length + 8);\n+                                    parameterNameIndicesLvt =\n+                                            Arrays.copyOf(parameterNameIndicesLvt, newSize);\n@@ -1008,2 +1002,1 @@\n-                                parameterNameIndices[register] = nameIndex;\n-                                haveParameterNameIndices = true;\n+                                parameterNameIndicesLvt[register] = nameIndex;\n@@ -1167,2 +1160,1 @@\n-                        sawMethodParameters = true;\n-                        parameterNameIndices = new int[numEntries];\n+                        parameterNameIndicesMp = new int[numEntries];\n@@ -1171,1 +1163,0 @@\n-                        haveParameterNameIndices = true;\n@@ -1179,1 +1170,1 @@\n-                            parameterNameIndices[index] = nameIndex;\n+                            parameterNameIndicesMp[index] = nameIndex;\n@@ -2454,3 +2445,3 @@\n-        if (parameterNameIndices == null\n-                || parameterNameIndices.length < expectedParameterSlots) {\n-            parameterNameIndices = new int[expectedParameterSlots];\n+        if (parameterNameIndicesLvt == null\n+                || parameterNameIndicesLvt.length < expectedParameterSlots) {\n+            parameterNameIndicesLvt = new int[expectedParameterSlots];\n@@ -2458,3 +2449,1 @@\n-            Arrays.fill(parameterNameIndices, 0);\n-        haveParameterNameIndices = false;\n-        sawMethodParameters = false;\n+            Arrays.fill(parameterNameIndicesLvt, 0);\n@@ -2475,33 +2464,28 @@\n-        \/\/ If we get parameter names from MethodParameters, then we\n-        \/\/ don't need to skip.\n-        int firstParam = 0;\n-        if (!sawMethodParameters) {\n-            firstParam = ((sym.flags() & STATIC) == 0) ? 1 : 0;\n-            \/\/ the code in readMethod may have skipped the first\n-            \/\/ parameter when setting up the MethodType. If so, we\n-            \/\/ make a corresponding allowance here for the position of\n-            \/\/ the first parameter.  Note that this assumes the\n-            \/\/ skipped parameter has a width of 1 -- i.e. it is not\n-            \/\/ a double width type (long or double.)\n-            if (names.isInitOrVNew(sym.name) && currentOwner.hasOuterInstance()) {\n-                \/\/ Sometimes anonymous classes don't have an outer\n-                \/\/ instance, however, there is no reliable way to tell so\n-                \/\/ we never strip this$n\n-                if (!currentOwner.name.isEmpty())\n-                    firstParam += 1;\n-            }\n-\n-            if (sym.type != jvmType) {\n-                \/\/ reading the method attributes has caused the\n-                \/\/ symbol's type to be changed. (i.e. the Signature\n-                \/\/ attribute.)  This may happen if there are hidden\n-                \/\/ (synthetic) parameters in the descriptor, but not\n-                \/\/ in the Signature.  The position of these hidden\n-                \/\/ parameters is unspecified; for now, assume they are\n-                \/\/ at the beginning, and so skip over them. The\n-                \/\/ primary case for this is two hidden parameters\n-                \/\/ passed into Enum constructors.\n-                int skip = Code.width(jvmType.getParameterTypes())\n-                        - Code.width(sym.type.getParameterTypes());\n-                firstParam += skip;\n-            }\n+        int firstParamLvt = ((sym.flags() & STATIC) == 0) ? 1 : 0;\n+        \/\/ the code in readMethod may have skipped the first\n+        \/\/ parameter when setting up the MethodType. If so, we\n+        \/\/ make a corresponding allowance here for the position of\n+        \/\/ the first parameter.  Note that this assumes the\n+        \/\/ skipped parameter has a width of 1 -- i.e. it is not\n+        \/\/ a double width type (long or double.)\n+        if (names.isInitOrVNew(sym.name) && currentOwner.hasOuterInstance()) {\n+            \/\/ Sometimes anonymous classes don't have an outer\n+            \/\/ instance, however, there is no reliable way to tell so\n+            \/\/ we never strip this$n\n+            if (!currentOwner.name.isEmpty())\n+                firstParamLvt += 1;\n+        }\n+\n+        if (sym.type != jvmType) {\n+            \/\/ reading the method attributes has caused the\n+            \/\/ symbol's type to be changed. (i.e. the Signature\n+            \/\/ attribute.)  This may happen if there are hidden\n+            \/\/ (synthetic) parameters in the descriptor, but not\n+            \/\/ in the Signature.  The position of these hidden\n+            \/\/ parameters is unspecified; for now, assume they are\n+            \/\/ at the beginning, and so skip over them. The\n+            \/\/ primary case for this is two hidden parameters\n+            \/\/ passed into Enum constructors.\n+            int skip = Code.width(jvmType.getParameterTypes())\n+                    - Code.width(sym.type.getParameterTypes());\n+            firstParamLvt += skip;\n@@ -2511,1 +2495,7 @@\n-        int nameIndex = firstParam;\n+        \/\/ we maintain two index pointers, one for the LocalVariableTable attribute\n+        \/\/ and the other for the MethodParameters attribute.\n+        \/\/ This is needed as the MethodParameters attribute may contain\n+        \/\/ name_index = 0 in which case we want to fall back to the LocalVariableTable.\n+        \/\/ In such case, we still want to read the flags from the MethodParameters with that index.\n+        int nameIndexLvt = firstParamLvt;\n+        int nameIndexMp = 0;\n@@ -2514,1 +2504,1 @@\n-            VarSymbol param = parameter(nameIndex, t, sym, paramNames);\n+            VarSymbol param = parameter(nameIndexMp, nameIndexLvt, t, sym, paramNames);\n@@ -2523,1 +2513,2 @@\n-            nameIndex += sawMethodParameters ? 1 : Code.width(t);\n+            nameIndexLvt += Code.width(t);\n+            nameIndexMp++;\n@@ -2532,1 +2523,2 @@\n-        parameterNameIndices = null;\n+        parameterNameIndicesLvt = null;\n+        parameterNameIndicesMp = null;\n@@ -2536,5 +2528,9 @@\n-\n-    \/\/ Returns the name for the parameter at position 'index', either using\n-    \/\/ names read from the MethodParameters, or by synthesizing a name that\n-    \/\/ is not on the 'exclude' list.\n-    private VarSymbol parameter(int index, Type t, MethodSymbol owner, Set<Name> exclude) {\n+    \/**\n+     * Creates the parameter at the position {@code mpIndex} in the parameter list of the owning method.\n+     * Flags are optionally read from the MethodParameters attribute.\n+     * Names are optionally read from the MethodParameters attribute. If the constant pool index\n+     * of the name is 0, then the name is optionally read from the LocalVariableTable attribute.\n+     * @param mpIndex the index of the parameter in the MethodParameters attribute\n+     * @param lvtIndex the index of the parameter in the LocalVariableTable attribute\n+     *\/\n+    private VarSymbol parameter(int mpIndex, int lvtIndex, Type t, MethodSymbol owner, Set<Name> exclude) {\n@@ -2543,7 +2539,12 @@\n-        if (parameterAccessFlags != null && index < parameterAccessFlags.length\n-                && parameterAccessFlags[index] != 0) {\n-            flags |= parameterAccessFlags[index];\n-        }\n-        if (parameterNameIndices != null && index < parameterNameIndices.length\n-                && parameterNameIndices[index] != 0) {\n-            argName = optPoolEntry(parameterNameIndices[index], poolReader::getName, names.empty);\n+        if (parameterAccessFlags != null && mpIndex < parameterAccessFlags.length\n+                && parameterAccessFlags[mpIndex] != 0) {\n+            flags |= parameterAccessFlags[mpIndex];\n+        }\n+        if (parameterNameIndicesMp != null\n+                \/\/ if name_index is 0, then we might still get a name from the LocalVariableTable\n+                && parameterNameIndicesMp[mpIndex] != 0) {\n+            argName = optPoolEntry(parameterNameIndicesMp[mpIndex], poolReader::getName, names.empty);\n+            flags |= NAME_FILLED;\n+        } else if (parameterNameIndicesLvt != null && lvtIndex < parameterNameIndicesLvt.length\n+                && parameterNameIndicesLvt[lvtIndex] != 0) {\n+            argName = optPoolEntry(parameterNameIndicesLvt[lvtIndex], poolReader::getName, names.empty);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassReader.java","additions":80,"deletions":79,"binary":false,"changes":159,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -391,1 +391,1 @@\n-    int writeMethodParametersAttr(MethodSymbol m) {\n+    int writeMethodParametersAttr(MethodSymbol m, boolean writeParamNames) {\n@@ -402,1 +402,4 @@\n-                databuf.appendChar(poolWriter.putName(s.name));\n+                if (writeParamNames)\n+                    databuf.appendChar(poolWriter.putName(s.name));\n+                else\n+                    databuf.appendChar(0);\n@@ -410,1 +413,4 @@\n-                databuf.appendChar(poolWriter.putName(s.name));\n+                if (writeParamNames)\n+                    databuf.appendChar(poolWriter.putName(s.name));\n+                else\n+                    databuf.appendChar(0);\n@@ -418,1 +424,4 @@\n-                databuf.appendChar(poolWriter.putName(s.name));\n+                if (writeParamNames)\n+                    databuf.appendChar(poolWriter.putName(s.name));\n+                else\n+                    databuf.appendChar(0);\n@@ -951,0 +960,9 @@\n+        int lastBootstrapMethods;\n+        do {\n+            lastBootstrapMethods = poolWriter.bootstrapMethods.size();\n+            for (BsmKey bsmKey : java.util.List.copyOf(poolWriter.bootstrapMethods.keySet())) {\n+                for (LoadableConstant arg : bsmKey.staticArgs) {\n+                    poolWriter.putConstant(arg);\n+                }\n+            }\n+        } while (lastBootstrapMethods < poolWriter.bootstrapMethods.size());\n@@ -1041,5 +1059,6 @@\n-        if (target.hasMethodParameters() && (\n-                options.isSet(PARAMETERS)\n-                || ((m.flags_field & RECORD) != 0 && (m.isInitOrVNew() || m.isValueObjectFactory())))) {\n-            if (!m.isLambdaMethod()) \/\/ Per JDK-8138729, do not emit parameters table for lambda bodies.\n-                acount += writeMethodParametersAttr(m);\n+        if (target.hasMethodParameters()) {\n+            if (!m.isLambdaMethod()) { \/\/ Per JDK-8138729, do not emit parameters table for lambda bodies.\n+                boolean requiresParamNames = requiresParamNames(m);\n+                if (requiresParamNames || requiresParamFlags(m))\n+                    acount += writeMethodParametersAttr(m, requiresParamNames);\n+            }\n@@ -1054,0 +1073,19 @@\n+    private boolean requiresParamNames(MethodSymbol m) {\n+        if (options.isSet(PARAMETERS))\n+            return true;\n+        if ((m.isInitOrVNew() || m.isValueObjectFactory()) && (m.flags_field & RECORD) != 0)\n+            return true;\n+        return false;\n+    }\n+\n+    private boolean requiresParamFlags(MethodSymbol m) {\n+        if (!m.extraParams.isEmpty()) {\n+            return m.extraParams.stream().anyMatch(p -> (p.flags_field & (SYNTHETIC | MANDATED)) != 0);\n+        }\n+        if (m.params != null) {\n+            \/\/ parameter is stored in params for Enum#valueOf(name)\n+            return m.params.stream().anyMatch(p -> (p.flags_field & (SYNTHETIC | MANDATED)) != 0);\n+        }\n+        return false;\n+    }\n+\n@@ -1674,3 +1712,1 @@\n-        if (preview.isEnabled() && preview.usesPreview(c.sourcefile)\n-                \/\/ do not write PREVIEW_MINOR_VERSION for classes participating in preview\n-                && !preview.participatesInPreview(syms, c, syms.java_base.unnamedPackage)) {\n+        if (preview.isEnabled() && preview.usesPreview(c.sourcefile)) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassWriter.java","additions":49,"deletions":13,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -1538,5 +1538,0 @@\n-            public void visitParenthesizedPattern(JCTree.JCParenthesizedPattern tree) {\n-                hasPatterns = true;\n-                super.visitParenthesizedPattern(tree);\n-            }\n-            @Override\n@@ -1597,0 +1592,6 @@\n+            if (shouldStop(CompileState.TRANSLITERALS))\n+                return;\n+\n+            env.tree = TransLiterals.instance(context).translateTopLevelClass(env, env.tree, localMake);\n+            compileStates.put(env, CompileState.TRANSLITERALS);\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/main\/JavaCompiler.java","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -68,1 +68,0 @@\n-import java.util.function.BiFunction;\n@@ -622,0 +621,8 @@\n+        return ident(allowClass, false);\n+    }\n+\n+    public Name identOrUnderscore() {\n+        return ident(false, true);\n+    }\n+\n+    protected Name ident(boolean allowClass, boolean asVariable) {\n@@ -647,0 +654,2 @@\n+            } else if (asVariable) {\n+                checkSourceLevel(Feature.UNNAMED_VARIABLES);\n@@ -648,1 +657,5 @@\n-                log.error(DiagnosticFlag.SYNTAX, token.pos, Errors.UnderscoreAsIdentifier);\n+                if (preview.isEnabled() && Feature.UNNAMED_VARIABLES.allowedInSource(source)) {\n+                    log.error(DiagnosticFlag.SYNTAX, token.pos, Errors.UseOfUnderscoreNotAllowed);\n+                } else {\n+                    log.error(DiagnosticFlag.SYNTAX, token.pos, Errors.UnderscoreAsIdentifier);\n+                }\n@@ -683,0 +696,53 @@\n+    \/**\n+     * StringTemplate =\n+     *    [STRINGFRAGMENT] [EmbeddedExpression]\n+     *  | STRINGLITERAL\n+     *\n+     * EmbeddedExpression =\n+     *  LBRACE term RBRACE\n+     *\/\n+    JCExpression stringTemplate(JCExpression processor) {\n+        checkSourceLevel(Feature.STRING_TEMPLATES);\n+        \/\/ Disable standalone string templates\n+        if (processor == null) {\n+            log.error(DiagnosticFlag.SYNTAX, token.pos,\n+                    Errors.ProcessorMissingFromStringTemplateExpression);\n+        }\n+        int oldmode = mode;\n+        selectExprMode();\n+        Token stringToken = token;\n+        int pos = stringToken.pos;\n+        int endPos = stringToken.endPos;\n+        TokenKind kind = stringToken.kind;\n+        String string = token.stringVal();\n+        List<String> fragments = List.of(string);\n+        List<JCExpression> expressions = List.nil();\n+        nextToken();\n+        if (kind != STRINGLITERAL) {\n+            while (token.kind == STRINGFRAGMENT) {\n+                stringToken = token;\n+                endPos = stringToken.endPos;\n+                string = stringToken.stringVal();\n+                fragments = fragments.append(string);\n+                nextToken();\n+             }\n+            while (token.pos < endPos && token.kind != DEFAULT && token.kind != ERROR) {\n+                accept(LBRACE);\n+                JCExpression expression = token.kind == RBRACE ? F.at(pos).Literal(TypeTag.BOT, null)\n+                                                               : term(EXPR);\n+                expressions = expressions.append(expression);\n+                if (token.kind != ERROR) {\n+                    accept(RBRACE);\n+                }\n+            }\n+            \/\/ clean up remaining expression tokens if error\n+            while (token.pos < endPos && token.kind != DEFAULT) {\n+                nextToken();\n+            }\n+            S.setPrevToken(stringToken);\n+        }\n+        JCExpression t = toP(F.at(pos).StringTemplate(processor, fragments, expressions));\n+        setMode(oldmode);\n+        return t;\n+    }\n+\n@@ -812,10 +878,7 @@\n-        if (token.kind == LPAREN && parsedType == null) {\n-            \/\/parenthesized pattern:\n-            int startPos = token.pos;\n-            accept(LPAREN);\n-            JCPattern p = parsePattern(token.pos, null, null, true, false);\n-            accept(RPAREN);\n-            pattern = toP(F.at(startPos).ParenthesizedPattern(p));\n-        } else {\n-            mods = mods != null ? mods : optFinal(0);\n-            JCExpression e;\n+        mods = mods != null ? mods : optFinal(0);\n+        JCExpression e;\n+        if (token.kind == UNDERSCORE && parsedType == null) {\n+            nextToken();\n+            pattern = toP(F.at(token.pos).AnyPattern());\n+        }\n+        else {\n@@ -846,0 +909,9 @@\n+                if (mods.annotations.nonEmpty()) {\n+                    log.error(mods.annotations.head.pos(), Errors.RecordPatternsAnnotationsNotAllowed);\n+                }\n+                new TreeScanner() {\n+                    @Override\n+                    public void visitAnnotatedType(JCAnnotatedType tree) {\n+                        log.error(tree.pos(), Errors.RecordPatternsAnnotationsNotAllowed);\n+                    }\n+                }.scan(e);\n@@ -848,1 +920,8 @@\n-                JCVariableDecl var = toP(F.at(token.pos).VarDef(mods, ident(), e, null));\n+                int varPos = token.pos;\n+                JCVariableDecl var = variableDeclaratorRest(varPos, mods, e, identOrUnderscore(), false, null, false, false, true);\n+                if (e == null) {\n+                    var.startPos = pos;\n+                    if (var.name == names.underscore && !allowVar) {\n+                        log.error(DiagnosticFlag.SYNTAX, varPos, Errors.UseOfUnderscoreNotAllowed);\n+                    }\n+                }\n@@ -855,1 +934,0 @@\n-\n@@ -1052,0 +1130,3 @@\n+                    } else if (token.kind == UNDERSCORE) {\n+                        checkSourceLevel(token.pos, Feature.UNNAMED_VARIABLES);\n+                        pattern = parsePattern(patternPos, mods, type, false, false);\n@@ -1311,0 +1392,8 @@\n+         case STRINGFRAGMENT:\n+             if (typeArgs == null && isMode(EXPR)) {\n+                 selectExprMode();\n+                 t = stringTemplate(null);\n+             } else {\n+                 return illegal();\n+             }\n+             break;\n@@ -1447,0 +1536,6 @@\n+                            case STRINGFRAGMENT:\n+                            case STRINGLITERAL:\n+                                if (typeArgs != null) return illegal();\n+                                t = stringTemplate(t);\n+                                typeArgs = null;\n+                                break loop;\n@@ -1598,0 +1693,1 @@\n+        JCExpression guard = parseGuard(pats.last());\n@@ -1623,1 +1719,1 @@\n-        caseExprs.append(toP(F.at(casePos).Case(kind, pats.toList(), stats, body)));\n+        caseExprs.append(toP(F.at(casePos).Case(kind, pats.toList(), guard, stats, body)));\n@@ -1676,0 +1772,6 @@\n+                } else if (token.kind == TokenKind.STRINGFRAGMENT ||\n+                           token.kind == TokenKind.STRINGLITERAL) {\n+                    if (typeArgs != null) {\n+                        return illegal();\n+                    }\n+                    t = stringTemplate(t);\n@@ -1836,0 +1938,1 @@\n+                        case STRINGFRAGMENT:\n@@ -2019,1 +2122,0 @@\n-\n@@ -2022,1 +2124,11 @@\n-        List<JCVariableDecl> params;\n+        \/**\n+         * analyzeParens() has already classified the lambda as EXPLICIT_LAMBDA, due to\n+         * two consecutive identifiers. Because of that {@code (<explicit lambda>)}, the\n+         * parser will always attempt to parse a type, followed by a name. If the lambda\n+         * contains an illegal mix of implicit and explicit parameters, it is possible\n+         * for the parser to see a {@code ,} when expecting a name, in which case the\n+         * variable is created with an erroneous name. The logic below makes sure that\n+         * the lambda parameters are all declared with either an explicit type (e.g.\n+         * {@code String x}), or with an inferred type (using {@code var x}). Any other\n+         * combination is rejected.\n+         * *\/\n@@ -2025,9 +2137,3 @@\n-            if (param.vartype != null && param.name != names.empty) {\n-                if (restrictedTypeName(param.vartype, false) != null) {\n-                    reduce(LambdaParameterKind.VAR);\n-                } else {\n-                    reduce(LambdaParameterKind.EXPLICIT);\n-                }\n-            }\n-            if (param.vartype == null && param.name != names.empty ||\n-                param.vartype != null && param.name == names.empty) {\n+            Assert.check(param.vartype != null);\n+\n+            if (param.name == names.error) {\n@@ -2036,0 +2142,5 @@\n+            else if (restrictedTypeName(param.vartype, false) != null) {\n+                reduce(LambdaParameterKind.VAR);\n+            } else {\n+                reduce(LambdaParameterKind.EXPLICIT);\n+            }\n@@ -2770,0 +2881,1 @@\n+                    case STRINGFRAGMENT:\n@@ -2902,15 +3014,6 @@\n-            JCTree pattern;\n-\n-            ForInitResult initResult = analyzeForInit();\n-\n-            if (initResult == ForInitResult.RecordPattern) {\n-                int patternPos = token.pos;\n-                JCModifiers mods = optFinal(0);\n-                int typePos = token.pos;\n-                JCExpression type = unannotatedType(false);\n-\n-                pattern = parsePattern(patternPos, mods, type, false, false);\n-\n-                if (pattern != null) {\n-                    checkSourceLevel(token.pos, Feature.PATTERN_SWITCH);\n-                }\n+            List<JCStatement> inits = token.kind == SEMI ? List.nil() : forInit();\n+            if (inits.length() == 1 &&\n+                inits.head.hasTag(VARDEF) &&\n+                ((JCVariableDecl) inits.head).init == null &&\n+                token.kind == COLON) {\n+                JCVariableDecl var = (JCVariableDecl)inits.head;\n@@ -2921,1 +3024,1 @@\n-                return F.at(pos).ForeachLoop(pattern, expr, body);\n+                return F.at(pos).ForeachLoop(var, expr, body);\n@@ -2923,20 +3026,7 @@\n-                List<JCStatement> inits = token.kind == SEMI ? List.nil() : forInit();\n-                if (inits.length() == 1 &&\n-                        inits.head.hasTag(VARDEF) &&\n-                        ((JCVariableDecl) inits.head).init == null &&\n-                        token.kind == COLON) {\n-                    JCVariableDecl var = (JCVariableDecl) inits.head;\n-                    accept(COLON);\n-                    JCExpression expr = parseExpression();\n-                    accept(RPAREN);\n-                    JCStatement body = parseStatementAsBlock();\n-                    return F.at(pos).ForeachLoop(var, expr, body);\n-                } else {\n-                    accept(SEMI);\n-                    JCExpression cond = token.kind == SEMI ? null : parseExpression();\n-                    accept(SEMI);\n-                    List<JCExpressionStatement> steps = token.kind == RPAREN ? List.nil() : forUpdate();\n-                    accept(RPAREN);\n-                    JCStatement body = parseStatementAsBlock();\n-                    return F.at(pos).ForLoop(inits, cond, steps, body);\n-                }\n+                accept(SEMI);\n+                JCExpression cond = token.kind == SEMI ? null : parseExpression();\n+                accept(SEMI);\n+                List<JCExpressionStatement> steps = token.kind == RPAREN ? List.nil() : forUpdate();\n+                accept(RPAREN);\n+                JCStatement body = parseStatementAsBlock();\n+                return F.at(pos).ForLoop(inits, cond, steps, body);\n@@ -3059,85 +3149,0 @@\n-    private enum ForInitResult {\n-        LocalVarDecl,\n-        RecordPattern\n-    }\n-\n-    @SuppressWarnings(\"fallthrough\")\n-    ForInitResult analyzeForInit() {\n-        boolean inType = false;\n-        boolean inSelectionAndParenthesis = false;\n-        int typeParameterPossibleStart = -1;\n-        outer: for (int lookahead = 0; ; lookahead++) {\n-            TokenKind tk = S.token(lookahead).kind;\n-            switch (tk) {\n-                case DOT:\n-                    if (inType) break; \/\/ in qualified type\n-                case COMMA:\n-                    typeParameterPossibleStart = lookahead;\n-                    break;\n-                case QUES:\n-                    \/\/ \"?\" only allowed in a type parameter position - otherwise it's an expression\n-                    if (typeParameterPossibleStart == lookahead - 1) break;\n-                    else return ForInitResult.LocalVarDecl;\n-                case EXTENDS: case SUPER: case AMP:\n-                case GTGTGT: case GTGT: case GT:\n-                case FINAL: case ELLIPSIS:\n-                    break;\n-                case BYTE: case SHORT: case INT: case LONG: case FLOAT:\n-                case DOUBLE: case BOOLEAN: case CHAR: case VOID:\n-                    if (peekToken(lookahead, IDENTIFIER)) {\n-                        return inSelectionAndParenthesis ? ForInitResult.RecordPattern\n-                                                         : ForInitResult.LocalVarDecl;\n-                    }\n-                    break;\n-                case LPAREN:\n-                    if (lookahead != 0 && inType) {\n-                        inSelectionAndParenthesis = true;\n-                        inType = false;\n-                    }\n-                    break;\n-                case RPAREN:\n-                    \/\/ a method call in the init part or a record pattern?\n-                    if (inSelectionAndParenthesis) {\n-                        if (peekToken(lookahead, DOT)  ||\n-                                peekToken(lookahead, SEMI) ||\n-                                peekToken(lookahead, ARROW)) {\n-                            return ForInitResult.LocalVarDecl;\n-                        }\n-                        else if(peekToken(lookahead, COLON)) {\n-                            return ForInitResult.RecordPattern;\n-                        }\n-                        break;\n-                    }\n-                case UNDERSCORE:\n-                case ASSERT:\n-                case ENUM:\n-                case IDENTIFIER:\n-                    if (lookahead == 0) {\n-                        inType = true;\n-                    }\n-                    break;\n-                case MONKEYS_AT: {\n-                    int prevLookahead = lookahead;\n-                    lookahead = skipAnnotation(lookahead);\n-                    if (typeParameterPossibleStart == prevLookahead - 1) {\n-                        \/\/ move possible start of type param after the anno\n-                        typeParameterPossibleStart = lookahead;\n-                    }\n-                    break;\n-                }\n-                case LBRACKET:\n-                    if (peekToken(lookahead, RBRACKET)) {\n-                        return inSelectionAndParenthesis ? ForInitResult.RecordPattern\n-                                                         : ForInitResult.LocalVarDecl;\n-                    }\n-                    return ForInitResult.LocalVarDecl;\n-                case LT:\n-                    typeParameterPossibleStart = lookahead;\n-                    break;\n-                default:\n-                    \/\/this includes EOF\n-                    return ForInitResult.LocalVarDecl;\n-            }\n-        }\n-    }\n-\n@@ -3168,1 +3173,1 @@\n-        JCVariableDecl formal = variableDeclaratorId(mods, paramType);\n+        JCVariableDecl formal = variableDeclaratorId(mods, paramType, true, false, false);\n@@ -3226,0 +3231,1 @@\n+            JCExpression guard = parseGuard(pats.last());\n@@ -3243,1 +3249,1 @@\n-            c = F.at(pos).Case(caseKind, pats.toList(), stats, body);\n+            c = F.at(pos).Case(caseKind, pats.toList(), guard, stats, body);\n@@ -3251,0 +3257,1 @@\n+            JCExpression guard = parseGuard(defaultPattern);\n@@ -3268,1 +3275,1 @@\n-            c = F.at(pos).Case(caseKind, List.of(defaultPattern), stats, body);\n+            c = F.at(pos).Case(caseKind, List.of(defaultPattern), guard, stats, body);\n@@ -3296,6 +3303,1 @@\n-                JCExpression guard = null;\n-                if (token.kind == IDENTIFIER && token.name() == names.when) {\n-                    nextToken();\n-                    guard = term(EXPR | NOLAMBDA);\n-                }\n-                return toP(F.at(patternPos).PatternCaseLabel(p, guard));\n+                return toP(F.at(patternPos).PatternCaseLabel(p));\n@@ -3311,0 +3313,16 @@\n+    private JCExpression parseGuard(JCCaseLabel label) {\n+        JCExpression guard = null;\n+\n+        if (token.kind == IDENTIFIER && token.name() == names.when) {\n+            int pos = token.pos;\n+\n+            nextToken();\n+            guard = term(EXPR | NOLAMBDA);\n+\n+            if (!(label instanceof JCPatternCaseLabel)) {\n+                guard = syntaxError(pos, List.of(guard), Errors.GuardNotAllowed);\n+            }\n+        }\n+\n+        return guard;\n+    }\n@@ -3321,1 +3339,1 @@\n-                case ASSERT, ENUM, IDENTIFIER, UNDERSCORE:\n+                case ASSERT, ENUM, IDENTIFIER:\n@@ -3330,0 +3348,12 @@\n+                case UNDERSCORE:\n+                    \/\/ TODO: REFACTOR to remove the code duplication\n+                    if (typeDepth == 0 && peekToken(lookahead, tk -> tk == RPAREN || tk == COMMA)) {\n+                        return PatternResult.PATTERN;\n+                    } else if (typeDepth == 0 && peekToken(lookahead, LAX_IDENTIFIER)) {\n+                        if (parenDepth == 0) {\n+                            return PatternResult.PATTERN;\n+                        } else {\n+                            pendingResult = PatternResult.PATTERN;\n+                        }\n+                    }\n+                    break;\n@@ -3652,1 +3682,1 @@\n-        return variableDeclaratorsRest(token.pos, mods, type, ident(), false, null, vdefs, localDecl);\n+        return variableDeclaratorsRest(token.pos, mods, type, identOrUnderscore(), false, null, vdefs, localDecl);\n@@ -3670,1 +3700,1 @@\n-        JCVariableDecl head = variableDeclaratorRest(pos, mods, type, name, reqInit, dc, localDecl, false);\n+        JCVariableDecl head = variableDeclaratorRest(pos, mods, type, name, reqInit, dc, localDecl, false, false);\n@@ -3685,1 +3715,1 @@\n-        return variableDeclaratorRest(token.pos, mods, type, ident(), reqInit, dc, localDecl, true);\n+        return variableDeclaratorRest(token.pos, mods, type, identOrUnderscore(), reqInit, dc, localDecl, true, false);\n@@ -3695,1 +3725,1 @@\n-                                  boolean reqInit, Comment dc, boolean localDecl, boolean compound) {\n+                                  boolean reqInit, Comment dc, boolean localDecl, boolean compound, boolean isTypePattern) {\n@@ -3699,0 +3729,8 @@\n+\n+        if (Feature.UNNAMED_VARIABLES.allowedInSource(source) && name == names.underscore) {\n+            if (!localDecl && !isTypePattern) {\n+                log.error(DiagnosticFlag.SYNTAX, pos, Errors.UseOfUnderscoreNotAllowed);\n+            }\n+            name = names.empty;\n+        }\n+\n@@ -3704,20 +3742,31 @@\n-        JCTree elemType = TreeInfo.innermostType(type, true);\n-        int startPos = Position.NOPOS;\n-        if (elemType.hasTag(IDENT)) {\n-            Name typeName = ((JCIdent)elemType).name;\n-            if (restrictedTypeNameStartingAtSource(typeName, pos, !compound && localDecl) != null) {\n-                if (typeName != names.var) {\n-                    reportSyntaxError(elemType.pos, Errors.RestrictedTypeNotAllowedHere(typeName));\n-                } else if (type.hasTag(TYPEARRAY) && !compound) {\n-                    \/\/error - 'var' and arrays\n-                    reportSyntaxError(elemType.pos, Errors.RestrictedTypeNotAllowedArray(typeName));\n-                } else {\n-                    declaredUsingVar = true;\n-                    if(compound)\n-                        \/\/error - 'var' in compound local var decl\n-                        reportSyntaxError(elemType.pos, Errors.RestrictedTypeNotAllowedCompound(typeName));\n-                    startPos = TreeInfo.getStartPos(mods);\n-                    if (startPos == Position.NOPOS)\n-                        startPos = TreeInfo.getStartPos(type);\n-                    \/\/implicit type\n-                    type = null;\n+\n+        if (Feature.UNNAMED_VARIABLES.allowedInSource(source) && name == names.empty\n+                && localDecl\n+                && init == null\n+                && token.kind != COLON) { \/\/ if its unnamed local variable, it needs to have an init unless in enhanced-for\n+            syntaxError(token.pos, Errors.Expected(EQ));\n+        }\n+\n+        JCVariableDecl result;\n+        if (!isTypePattern) {\n+            int startPos = Position.NOPOS;\n+            JCTree elemType = TreeInfo.innermostType(type, true);\n+            if (elemType.hasTag(IDENT)) {\n+                Name typeName = ((JCIdent) elemType).name;\n+                if (restrictedTypeNameStartingAtSource(typeName, pos, !compound && localDecl) != null) {\n+                    if (typeName != names.var) {\n+                        reportSyntaxError(elemType.pos, Errors.RestrictedTypeNotAllowedHere(typeName));\n+                    } else if (type.hasTag(TYPEARRAY) && !compound) {\n+                        \/\/error - 'var' and arrays\n+                        reportSyntaxError(elemType.pos, Errors.RestrictedTypeNotAllowedArray(typeName));\n+                    } else {\n+                        declaredUsingVar = true;\n+                        if (compound)\n+                            \/\/error - 'var' in compound local var decl\n+                            reportSyntaxError(elemType.pos, Errors.RestrictedTypeNotAllowedCompound(typeName));\n+                        startPos = TreeInfo.getStartPos(mods);\n+                        if (startPos == Position.NOPOS)\n+                            startPos = TreeInfo.getStartPos(type);\n+                        \/\/implicit type\n+                        type = null;\n+                    }\n@@ -3726,0 +3775,5 @@\n+            result = toP(F.at(pos).VarDef(mods, name, type, init, declaredUsingVar));\n+            attach(result, dc);\n+            result.startPos = startPos;\n+        } else {\n+            result = toP(F.at(pos).VarDef(mods, name, type, null));\n@@ -3727,4 +3781,1 @@\n-        JCVariableDecl result =\n-            toP(F.at(pos).VarDef(mods, name, type, init, declaredUsingVar));\n-        attach(result, dc);\n-        result.startPos = startPos;\n+\n@@ -3805,5 +3856,1 @@\n-    JCVariableDecl variableDeclaratorId(JCModifiers mods, JCExpression type) {\n-        return variableDeclaratorId(mods, type, false, false);\n-    }\n-    \/\/where\n-    JCVariableDecl variableDeclaratorId(JCModifiers mods, JCExpression type, boolean lambdaParameter, boolean recordComponent) {\n+    JCVariableDecl variableDeclaratorId(JCModifiers mods, JCExpression type, boolean catchParameter, boolean lambdaParameter, boolean recordComponent) {\n@@ -3812,32 +3859,8 @@\n-        if (lambdaParameter && token.kind == UNDERSCORE) {\n-            log.error(pos, Errors.UnderscoreAsIdentifierInLambda);\n-            name = token.name();\n-            nextToken();\n-        } else {\n-            if (allowThisIdent ||\n-                !lambdaParameter ||\n-                LAX_IDENTIFIER.test(token.kind) ||\n-                mods.flags != Flags.PARAMETER ||\n-                mods.annotations.nonEmpty()) {\n-                JCExpression pn = qualident(false);\n-                if (pn.hasTag(Tag.IDENT) && ((JCIdent)pn).name != names._this) {\n-                    name = ((JCIdent)pn).name;\n-                } else if (lambdaParameter && type == null) {\n-                    \/\/ we have a lambda parameter that is not an identifier this is a syntax error\n-                    type = pn;\n-                    name = names.empty;\n-                    reportSyntaxError(pos, Errors.Expected(IDENTIFIER));\n-                } else {\n-                    if (allowThisIdent) {\n-                        if ((mods.flags & Flags.VARARGS) != 0) {\n-                            log.error(token.pos, Errors.VarargsAndReceiver);\n-                        }\n-                        if (token.kind == LBRACKET) {\n-                            log.error(token.pos, Errors.ArrayAndReceiver);\n-                        }\n-                        if (pn.hasTag(Tag.SELECT) && ((JCFieldAccess)pn).name != names._this) {\n-                            log.error(token.pos, Errors.WrongReceiver);\n-                        }\n-                    }\n-                    return toP(F.at(pos).ReceiverVarDef(mods, pn, type));\n-                }\n+        if (allowThisIdent ||\n+            !lambdaParameter ||\n+            LAX_IDENTIFIER.test(token.kind) ||\n+            mods.flags != Flags.PARAMETER ||\n+            mods.annotations.nonEmpty()) {\n+            JCExpression pn;\n+            if (token.kind == UNDERSCORE && (catchParameter || lambdaParameter)) {\n+                pn = toP(F.at(token.pos).Ident(identOrUnderscore()));\n@@ -3845,7 +3868,7 @@\n-                \/** if it is a lambda parameter and the token kind is not an identifier,\n-                 *  and there are no modifiers or annotations, then this means that the compiler\n-                 *  supposed the lambda to be explicit but it can contain a mix of implicit,\n-                 *  var or explicit parameters. So we assign the error name to the parameter name\n-                 *  instead of issuing an error and analyze the lambda parameters as a whole at\n-                 *  a higher level.\n-                 *\/\n+                pn = qualident(false);\n+            }\n+            if (pn.hasTag(Tag.IDENT) && ((JCIdent)pn).name != names._this) {\n+                name = ((JCIdent)pn).name;\n+            } else if (lambdaParameter && type == null) {\n+                \/\/ we have a lambda parameter that is not an identifier this is a syntax error\n+                type = pn;\n@@ -3853,0 +3876,14 @@\n+                reportSyntaxError(pos, Errors.Expected(IDENTIFIER));\n+            } else {\n+                if (allowThisIdent) {\n+                    if ((mods.flags & Flags.VARARGS) != 0) {\n+                        log.error(token.pos, Errors.VarargsAndReceiver);\n+                    }\n+                    if (token.kind == LBRACKET) {\n+                        log.error(token.pos, Errors.ArrayAndReceiver);\n+                    }\n+                    if (pn.hasTag(Tag.SELECT) && ((JCFieldAccess)pn).name != names._this) {\n+                        log.error(token.pos, Errors.WrongReceiver);\n+                    }\n+                }\n+                return toP(F.at(pos).ReceiverVarDef(mods, pn, type));\n@@ -3854,0 +3891,9 @@\n+        } else {\n+            \/** if it is a lambda parameter and the token kind is not an identifier,\n+             *  and there are no modifiers or annotations, then this means that the compiler\n+             *  supposed the lambda to be explicit but it can contain a mix of implicit,\n+             *  var or explicit parameters. So we assign the error name to the parameter name\n+             *  instead of issuing an error and analyze the lambda parameters as a whole at\n+             *  a higher level.\n+             *\/\n+            name = names.error;\n@@ -3864,0 +3910,4 @@\n+        if (Feature.UNNAMED_VARIABLES.allowedInSource(source) && name == names.underscore) {\n+            name = names.empty;\n+        }\n+\n@@ -3894,1 +3944,1 @@\n-            return variableDeclaratorRest(token.pos, mods, t, ident(), true, null, true, false);\n+            return variableDeclaratorRest(token.pos, mods, t, ident(), true, null, true, false, false);\n@@ -3899,1 +3949,1 @@\n-            return variableDeclaratorRest(token.pos, mods, t, ident(), true, null, true, false);\n+            return variableDeclaratorRest(token.pos, mods, t, identOrUnderscore(), true, null, true, false, false);\n@@ -4255,1 +4305,1 @@\n-                                .VarDef(F.Modifiers(Flags.PARAMETER | Flags.GENERATED_MEMBER | param.mods.flags & Flags.VARARGS,\n+                                .VarDef(F.Modifiers(Flags.PARAMETER | Flags.GENERATED_MEMBER | Flags.MANDATED | param.mods.flags & Flags.VARARGS,\n@@ -5133,1 +5183,1 @@\n-        return variableDeclaratorId(mods, type, lambdaParameter, recordComponent);\n+        return variableDeclaratorId(mods, type, false, lambdaParameter, recordComponent);\n@@ -5138,1 +5188,1 @@\n-        return variableDeclaratorId(mods, null, true, false);\n+        return variableDeclaratorId(mods, null, false, true, false);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":284,"deletions":234,"binary":false,"changes":518,"status":"modified"},{"patch":"@@ -537,0 +537,3 @@\n+compiler.err.guard.not.allowed=\\\n+    guards are only allowed for case with a pattern\n+\n@@ -540,0 +543,4 @@\n+# 0: symbol\n+compiler.err.cannot.assign.not.declared.guard=\\\n+    cannot assign to {0}, as it was not declared inside the guard\n+\n@@ -628,4 +635,0 @@\n-# 0: type, 1: type\n-compiler.err.foreach.not.exhaustive.on.type=\\\n-    Pattern {0} is not exhaustive on {1}\n-\n@@ -1331,0 +1334,17 @@\n+compiler.err.string.template.is.not.well.formed=\\\n+    string template is not well formed\n+\n+compiler.err.text.block.template.is.not.well.formed=\\\n+    text block template is not well formed\n+\n+compiler.err.processor.missing.from.string.template.expression=\\\n+    processor missing from string template expression\n+\n+# 0: symbol\n+compiler.err.processor.type.cannot.be.a.raw.type=\\\n+    processor type cannot be a raw type: {0}\n+\n+# 0: symbol\n+compiler.err.not.a.processor.type=\\\n+    not a processor type: {0}\n+\n@@ -1492,4 +1512,0 @@\n-# 0: type, 1: type\n-compiler.err.instanceof.pattern.no.subtype=\\\n-    expression type {0} is a subtype of pattern type {1}\n-\n@@ -3127,0 +3143,3 @@\n+compiler.misc.feature.unnamed.variables=\\\n+    unnamed variables\n+\n@@ -3139,0 +3158,3 @@\n+compiler.misc.feature.string.templates=\\\n+    string templates\n+\n@@ -3148,3 +3170,3 @@\n-compiler.err.underscore.as.identifier.in.lambda=\\\n-    ''_'' used as an identifier\\n\\\n-    (use of ''_'' as an identifier is forbidden for lambda parameters)\n+compiler.err.use.of.underscore.not.allowed=\\\n+    as of release 21, the underscore keyword ''_'' is only allowed to declare\\n\\\n+    unnamed patterns, local variables, exception parameters or lambda parameters\n@@ -3849,0 +3871,3 @@\n+compiler.err.record.patterns.annotations.not.allowed=\\\n+    annotations not allowed on record patterns\n+\n@@ -4067,0 +4092,3 @@\n+compiler.err.deconstruction.pattern.var.not.allowed=\\\n+    deconstruction patterns can only be applied to records, var is not allowed\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":39,"deletions":11,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-    Supported releases: {0}\n+    Supported releases: \\n    {0}\n@@ -93,1 +93,1 @@\n-    Supported releases: {0}\n+    Supported releases: \\n    {0}\n@@ -96,1 +96,1 @@\n-    Supported releases: {0}\n+    Supported releases: \\n    {0}\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/javac.properties","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -246,0 +246,1 @@\n+        ANYPATTERN,\n@@ -247,1 +248,0 @@\n-        PARENTHESIZEDPATTERN,\n@@ -280,0 +280,4 @@\n+        \/** String template expression.\n+         *\/\n+        STRING_TEMPLATE,\n+\n@@ -1266,1 +1270,1 @@\n-        public JCTree varOrRecordPattern;\n+        public JCVariableDecl var;\n@@ -1269,4 +1273,2 @@\n-        public Type elementType;\n-\n-        protected JCEnhancedForLoop(JCTree varOrRecordPattern, JCExpression expr, JCStatement body) {\n-            this.varOrRecordPattern = varOrRecordPattern;\n+        protected JCEnhancedForLoop(JCVariableDecl var, JCExpression expr, JCStatement body) {\n+            this.var = var;\n@@ -1282,5 +1284,1 @@\n-        public JCVariableDecl getVariable() {\n-            return varOrRecordPattern instanceof JCVariableDecl var ? var : null;\n-        }\n-        @DefinedBy(Api.COMPILER_TREE)\n-        public JCTree getVariableOrRecordPattern() { return varOrRecordPattern; }\n+        public JCVariableDecl getVariable() { return var; }\n@@ -1299,4 +1297,0 @@\n-        @Override @DefinedBy(Api.COMPILER_TREE)\n-        public EnhancedForLoopTree.DeclarationKind getDeclarationKind() {\n-            return varOrRecordPattern.hasTag(VARDEF) ? DeclarationKind.VARIABLE : DeclarationKind.PATTERN;\n-        }\n@@ -1378,0 +1372,1 @@\n+        public JCExpression guard;\n@@ -1382,0 +1377,1 @@\n+                         JCExpression guard,\n@@ -1387,0 +1383,1 @@\n+            this.guard = guard;\n@@ -1409,0 +1406,2 @@\n+        public JCExpression getGuard() { return guard; }\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n@@ -2322,4 +2321,0 @@\n-        @DefinedBy(Api.COMPILER_TREE)\n-        public TestKind getTestKind() {\n-            return pattern instanceof JCPatternCaseLabel ? TestKind.PATTERN : TestKind.TYPE;\n-        }\n@@ -2343,0 +2338,28 @@\n+    public static class JCAnyPattern extends JCPattern\n+            implements AnyPatternTree {\n+\n+        protected JCAnyPattern() {\n+        }\n+\n+        @Override\n+        public void accept(Visitor v) {\n+            v.visitAnyPattern(this);\n+        }\n+\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public Kind getKind() {\n+            return Kind.ANY_PATTERN;\n+        }\n+\n+        @Override\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public <R, D> R accept(TreeVisitor<R, D> v, D d) {\n+            return v.visitAnyPattern(this, d);\n+        }\n+\n+        @Override\n+        public Tag getTag() {\n+            return ANYPATTERN;\n+        }\n+    }\n+\n@@ -2448,1 +2471,1 @@\n-        public JCExpression guard;\n+        public JCExpression syntheticGuard;\n@@ -2450,1 +2473,1 @@\n-        protected JCPatternCaseLabel(JCPattern pat, JCExpression guard) {\n+        protected JCPatternCaseLabel(JCPattern pat) {\n@@ -2452,1 +2475,0 @@\n-            this.guard = guard;\n@@ -2460,5 +2482,0 @@\n-        @Override @DefinedBy(Api.COMPILER_TREE)\n-        public JCExpression getGuard() {\n-            return guard;\n-        }\n-\n@@ -2488,35 +2505,0 @@\n-    public static class JCParenthesizedPattern extends JCPattern\n-            implements ParenthesizedPatternTree {\n-        public JCPattern pattern;\n-\n-        public JCParenthesizedPattern(JCPattern pattern) {\n-            this.pattern = pattern;\n-        }\n-\n-        @Override @DefinedBy(Api.COMPILER_TREE)\n-        public PatternTree getPattern() {\n-            return pattern;\n-        }\n-\n-        @Override\n-        public void accept(Visitor v) {\n-            v.visitParenthesizedPattern(this);\n-        }\n-\n-        @DefinedBy(Api.COMPILER_TREE)\n-        public Kind getKind() {\n-            return Kind.PARENTHESIZED_PATTERN;\n-        }\n-\n-        @Override\n-        @DefinedBy(Api.COMPILER_TREE)\n-        public <R, D> R accept(TreeVisitor<R, D> v, D d) {\n-            return v.visitParenthesizedPattern(this, d);\n-        }\n-\n-        @Override\n-        public Tag getTag() {\n-            return PARENTHESIZEDPATTERN;\n-        }\n-    }\n-\n@@ -2573,0 +2555,52 @@\n+    \/**\n+     * String template expression.\n+     *\/\n+    public static class JCStringTemplate extends JCExpression implements StringTemplateTree {\n+        public JCExpression processor;\n+        public List<String> fragments;\n+        public List<JCExpression> expressions;\n+\n+        protected JCStringTemplate(JCExpression processor,\n+                                   List<String> fragments,\n+                                   List<JCExpression> expressions) {\n+            this.processor = processor;\n+            this.fragments = fragments;\n+            this.expressions = expressions;\n+        }\n+\n+        @Override\n+        public ExpressionTree getProcessor() {\n+            return processor;\n+        }\n+\n+        @Override\n+        public List<String> getFragments() {\n+            return fragments;\n+        }\n+\n+        @Override\n+        public List<? extends ExpressionTree> getExpressions() {\n+            return expressions;\n+        }\n+\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public Kind getKind() {\n+            return Kind.TEMPLATE;\n+        }\n+\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public Tag getTag() {\n+            return STRING_TEMPLATE;\n+        }\n+\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public void accept(Visitor v) {\n+            v.visitStringTemplate(this);\n+        }\n+\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public <R, D> R accept(TreeVisitor<R, D> v, D d) {\n+            return v.visitStringTemplate(this, d);\n+        }\n+    }\n+\n@@ -3505,1 +3539,1 @@\n-        JCEnhancedForLoop ForeachLoop(JCTree var, JCExpression expr, JCStatement body);\n+        JCEnhancedForLoop ForeachLoop(JCVariableDecl var, JCExpression expr, JCStatement body);\n@@ -3509,1 +3543,1 @@\n-        JCCase Case(CaseTree.CaseKind caseKind, List<JCCaseLabel> labels,\n+        JCCase Case(CaseTree.CaseKind caseKind, List<JCCaseLabel> labels, JCExpression guard,\n@@ -3553,0 +3587,3 @@\n+        JCStringTemplate StringTemplate(JCExpression processor,\n+                                        List<String> fragments,\n+                                        List<JCExpression> expressions);\n@@ -3615,0 +3652,1 @@\n+        public void visitAnyPattern(JCAnyPattern that)       { visitTree(that); }\n@@ -3619,1 +3657,0 @@\n-        public void visitParenthesizedPattern(JCParenthesizedPattern that) { visitTree(that); }\n@@ -3626,0 +3663,1 @@\n+        public void visitStringTemplate(JCStringTemplate that) { visitTree(that); }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/JCTree.java","additions":103,"deletions":65,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+import java.util.stream.Collectors;\n@@ -717,1 +718,5 @@\n-                    print(tree.name);\n+                    if (tree.name.isEmpty()) {\n+                        print('_');\n+                    } else {\n+                        print(tree.name);\n+                    }\n@@ -837,1 +842,1 @@\n-            printExpr(tree.varOrRecordPattern);\n+            printExpr(tree.var);\n@@ -885,0 +890,4 @@\n+            if (tree.guard != null) {\n+                print(\" when \");\n+                print(tree.guard);\n+            }\n@@ -927,4 +936,0 @@\n-            if (tree.guard != null) {\n-                print(\" when \");\n-                print(tree.guard);\n-            }\n@@ -964,2 +969,1 @@\n-    @Override\n-    public void visitParenthesizedPattern(JCParenthesizedPattern patt) {\n+    public void visitAnyPattern(JCAnyPattern patt) {\n@@ -967,3 +971,1 @@\n-            print('(');\n-            printExpr(patt.pattern);\n-            print(')');\n+            print('_');\n@@ -1498,0 +1500,17 @@\n+    public void visitStringTemplate(JCStringTemplate tree) {\n+        try {\n+            JCExpression processor = tree.processor;\n+            print(\"[\");\n+            if (processor != null) {\n+                printExpr(processor);\n+            }\n+            print(\"]\");\n+            print(\"\\\"\" + tree.fragments.stream().collect(Collectors.joining(\"\\\\{}\")) + \"\\\"\");\n+            print(\"(\");\n+            printExprs(tree.expressions);\n+            print(\")\");\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/Pretty.java","additions":30,"deletions":11,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -156,0 +156,1 @@\n+        JCExpression guard = copy(t.guard, p);\n@@ -164,1 +165,1 @@\n-        return M.at(t.pos).Case(t.caseKind, labels, stats, body);\n+        return M.at(t.pos).Case(t.caseKind, labels, guard, stats, body);\n@@ -233,1 +234,1 @@\n-        JCTree varOrRecordPattern = copy(t.varOrRecordPattern, p);\n+        JCVariableDecl var = copy(t.var, p);\n@@ -236,1 +237,1 @@\n-        return M.at(t.pos).ForeachLoop(varOrRecordPattern, expr, body);\n+        return M.at(t.pos).ForeachLoop(var, expr, body);\n@@ -293,0 +294,8 @@\n+    @DefinedBy(Api.COMPILER_TREE)\n+    public JCTree visitStringTemplate(StringTemplateTree node, P p) {\n+        JCStringTemplate t = (JCStringTemplate) node;\n+        JCExpression processor = copy(t.processor, p);\n+        List<JCExpression> expressions = copy(t.expressions, p);\n+        return M.at(t.pos).StringTemplate(processor, t.fragments, expressions);\n+    }\n+\n@@ -502,0 +511,6 @@\n+    @DefinedBy(Api.COMPILER_TREE)\n+    public JCTree visitAnyPattern(AnyPatternTree node, P p) {\n+        JCAnyPattern t = (JCAnyPattern) node;\n+        return M.at(t.pos).AnyPattern();\n+    }\n+\n@@ -509,7 +524,0 @@\n-    @DefinedBy(Api.COMPILER_TREE)\n-    public JCTree visitParenthesizedPattern(ParenthesizedPatternTree node, P p) {\n-        JCParenthesizedPattern t = (JCParenthesizedPattern) node;\n-        JCPattern pattern = copy(t.pattern, p);\n-        return M.at(t.pos).ParenthesizedPattern(pattern);\n-    }\n-\n@@ -533,2 +541,1 @@\n-        JCExpression guard = copy(t.guard, p);\n-        return M.at(t.pos).PatternCaseLabel(pat, guard);\n+        return M.at(t.pos).PatternCaseLabel(pat);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeCopier.java","additions":19,"deletions":12,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -346,0 +346,1 @@\n+            case STRING_TEMPLATE:\n@@ -563,0 +564,8 @@\n+            case STRING_TEMPLATE: {\n+                JCStringTemplate node = (JCStringTemplate) tree;\n+                if (node.processor == null) {\n+                    return node.pos;\n+                } else {\n+                    return getStartPos(node.processor);\n+                }\n+            }\n@@ -659,4 +668,0 @@\n-            case PARENTHESIZEDPATTERN: {\n-                JCParenthesizedPattern node = (JCParenthesizedPattern) tree;\n-                return getEndPos(node.pattern, endPosTable);\n-            }\n@@ -866,9 +871,0 @@\n-    \/** Skip parens and return the enclosed expression\n-     *\/\n-    public static JCPattern skipParens(JCPattern tree) {\n-        while (tree.hasTag(PARENTHESIZEDPATTERN)) {\n-            tree = ((JCParenthesizedPattern) tree).pattern;\n-        }\n-        return tree;\n-    }\n-\n@@ -993,0 +989,2 @@\n+        case CLASSDEF:\n+            return ((JCClassDecl) tree).sym;\n@@ -1367,1 +1365,1 @@\n-            case PARENTHESIZEDPATTERN -> primaryPatternType(((JCParenthesizedPattern) pat).pattern);\n+            case ANYPATTERN -> ((JCAnyPattern) pat).type;\n@@ -1376,1 +1374,0 @@\n-            case PARENTHESIZEDPATTERN -> primaryPatternTypeTree(((JCParenthesizedPattern) pat).pattern);\n@@ -1389,5 +1386,2 @@\n-    public static boolean unguardedCaseLabel(JCCaseLabel cse) {\n-        if (!cse.hasTag(PATTERNCASELABEL)) {\n-            return true;\n-        }\n-        JCExpression guard = ((JCPatternCaseLabel) cse).guard;\n+    public static boolean unguardedCase(JCCase cse) {\n+        JCExpression guard = cse.guard;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":14,"deletions":20,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -283,2 +283,2 @@\n-    public JCEnhancedForLoop ForeachLoop(JCTree varOrRecordPattern, JCExpression expr, JCStatement body) {\n-        JCEnhancedForLoop tree = new JCEnhancedForLoop(varOrRecordPattern, expr, body);\n+    public JCEnhancedForLoop ForeachLoop(JCVariableDecl var, JCExpression expr, JCStatement body) {\n+        JCEnhancedForLoop tree = new JCEnhancedForLoop(var, expr, body);\n@@ -302,2 +302,2 @@\n-                       List<JCStatement> stats, JCTree body) {\n-        JCCase tree = new JCCase(caseKind, labels, stats, body);\n+                       JCExpression guard, List<JCStatement> stats, JCTree body) {\n+        JCCase tree = new JCCase(caseKind, labels, guard, stats, body);\n@@ -498,0 +498,6 @@\n+    public JCAnyPattern AnyPattern() {\n+        JCAnyPattern tree = new JCAnyPattern();\n+        tree.pos = pos;\n+        return tree;\n+    }\n+\n@@ -516,8 +522,2 @@\n-    public JCPatternCaseLabel PatternCaseLabel(JCPattern pat, JCExpression guard) {\n-        JCPatternCaseLabel tree = new JCPatternCaseLabel(pat, guard);\n-        tree.pos = pos;\n-        return tree;\n-    }\n-\n-    public JCParenthesizedPattern ParenthesizedPattern(JCPattern pattern) {\n-        JCParenthesizedPattern tree = new JCParenthesizedPattern(pattern);\n+    public JCPatternCaseLabel PatternCaseLabel(JCPattern pat) {\n+        JCPatternCaseLabel tree = new JCPatternCaseLabel(pat);\n@@ -565,0 +565,8 @@\n+    public JCStringTemplate StringTemplate(JCExpression processor,\n+                                           List<String> fragments,\n+                                           List<JCExpression> expressions) {\n+        JCStringTemplate tree = new JCStringTemplate(processor, fragments, expressions);\n+        tree.pos = pos;\n+        return tree;\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeMaker.java","additions":20,"deletions":12,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+import jdk.internal.javac.PreviewFeature;\n@@ -170,1 +171,1 @@\n-        scan(tree.varOrRecordPattern);\n+        scan(tree.var);\n@@ -186,0 +187,1 @@\n+        scan(tree.guard);\n@@ -331,1 +333,0 @@\n-        scan(tree.guard);\n@@ -335,2 +336,2 @@\n-    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n-        scan(tree.pattern);\n+    @PreviewFeature(feature=PreviewFeature.Feature.UNNAMED)\n+    public void visitAnyPattern(JCAnyPattern that) {\n@@ -365,0 +366,5 @@\n+    public void visitStringTemplate(JCStringTemplate tree) {\n+        scan(tree.processor);\n+        scan(tree.expressions);\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeScanner.java","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -198,1 +198,1 @@\n-        tree.varOrRecordPattern = translate(tree.varOrRecordPattern);\n+        tree.var = translate(tree.var);\n@@ -217,0 +217,1 @@\n+        tree.guard = translate(tree.guard);\n@@ -372,0 +373,4 @@\n+    public void visitAnyPattern(JCAnyPattern tree) {\n+        result = tree;\n+    }\n+\n@@ -386,7 +391,0 @@\n-        tree.guard = translate(tree.guard);\n-        result = tree;\n-    }\n-\n-    @Override\n-    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n-        tree.pattern = translate(tree.pattern);\n@@ -425,0 +423,7 @@\n+    public void visitStringTemplate(JCStringTemplate tree) {\n+        tree.processor = translate(tree.processor);\n+        tree.expressions = translate(tree.expressions);\n+\n+        result = tree;\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeTranslator.java","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+    public final Name underscore;\n@@ -97,0 +98,1 @@\n+    public final Name invoke;\n@@ -100,0 +102,1 @@\n+    public final Name of;\n@@ -237,0 +240,9 @@\n+    public final Name enumConstant;\n+\n+    \/\/ templated string\n+    public final Name process;\n+    public final Name STR;\n+    public final Name RAW;\n+    public final Name newStringTemplate;\n+    public final Name newLargeStringTemplate;\n+    public final Name processStringTemplate;\n@@ -268,0 +280,1 @@\n+        underscore = fromString(\"_\");\n@@ -292,0 +305,1 @@\n+        invoke = fromString(\"invoke\");\n@@ -295,0 +309,1 @@\n+        of = fromString(\"of\");\n@@ -424,0 +439,8 @@\n+        \/\/ templated string\n+        process = fromString(\"process\");\n+        STR = fromString(\"STR\");\n+        RAW = fromString(\"RAW\");\n+        newStringTemplate = fromString(\"newStringTemplate\");\n+        newLargeStringTemplate = fromString(\"newLargeStringTemplate\");\n+        processStringTemplate = fromString(\"processStringTemplate\");\n+\n@@ -427,0 +450,1 @@\n+        enumConstant = fromString(\"enumConstant\");\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/util\/Names.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -263,4 +263,0 @@\n-  public boolean isObsolete() {\n-     return getAccessFlagsObj().isObsolete();\n-  }\n-\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Method.java","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -47,1 +47,1 @@\n-final class HotSpotMethodData {\n+final class HotSpotMethodData implements MetaspaceObject {\n@@ -179,0 +179,5 @@\n+    @Override\n+    public long getMetaspacePointer() {\n+        return methodDataPointer;\n+    }\n+\n@@ -202,0 +207,3 @@\n+    \/**\n+     * Return true if there is an extra data section and the first tag is non-zero.\n+     *\/\n@@ -203,1 +211,1 @@\n-        return extraDataSize() > 0;\n+        return extraDataSize() > 0 && HotSpotMethodDataAccessor.readTag(state.config, this, getExtraDataBeginOffset()) != 0;\n@@ -206,1 +214,1 @@\n-    public int getExtraDataBeginOffset() {\n+    private int getExtraDataBeginOffset() {\n@@ -211,1 +219,1 @@\n-        return position >= 0 && position < normalDataSize() + extraDataSize();\n+        return position >= 0 && position < normalDataSize();\n@@ -246,11 +254,0 @@\n-    public HotSpotMethodDataAccessor getExtraData(int position) {\n-        if (position >= normalDataSize() + extraDataSize()) {\n-            return null;\n-        }\n-        HotSpotMethodDataAccessor data = getData(position);\n-        if (data != null) {\n-            return data;\n-        }\n-        return data;\n-    }\n-\n@@ -313,1 +310,1 @@\n-        return compilerToVM().getResolvedJavaType(methodDataPointer + fullOffsetInBytes);\n+        return compilerToVM().getResolvedJavaType(this, fullOffsetInBytes);\n@@ -349,14 +346,0 @@\n-        if (hasExtraData()) {\n-            int pos = getExtraDataBeginOffset();\n-            HotSpotMethodDataAccessor data;\n-            while ((data = getExtraData(pos)) != null) {\n-                if (pos == getExtraDataBeginOffset()) {\n-                    sb.append(nl).append(\"--- Extra data:\");\n-                }\n-                int bci = data.getBCI(this, pos);\n-                sb.append(String.format(\"%n%-6d bci: %-6d%-20s\", pos, bci, data.getClass().getSimpleName()));\n-                sb.append(data.appendTo(new StringBuilder(), this, pos).toString().replace(nl, nlIndent));\n-                pos = pos + data.getSize(this, pos);\n-            }\n-\n-        }\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotMethodData.java","additions":13,"deletions":30,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -152,1 +152,1 @@\n-    final int methodFlagsOffset = getFieldOffset(\"Method::_flags\", Integer.class, \"u2\");\n+    final int methodFlagsOffset = getFieldOffset(\"Method::_flags._status\", Integer.class, \"u4\");\n@@ -158,5 +158,2 @@\n-    final int methodFlagsCallerSensitive = getConstant(\"Method::_caller_sensitive\", Integer.class);\n-    final int methodFlagsForceInline = getConstant(\"Method::_force_inline\", Integer.class);\n-    final int methodFlagsIntrinsicCandidate = getConstant(\"Method::_intrinsic_candidate\", Integer.class);\n-    final int methodFlagsDontInline = getConstant(\"Method::_dont_inline\", Integer.class);\n-    final int methodFlagsReservedStackAccess = getConstant(\"Method::_reserved_stack_access\", Integer.class);\n+    final int methodFlagsForceInline = getConstant(\"MethodFlags::_misc_force_inline\", Integer.class);\n+    final int methodFlagsDontInline = getConstant(\"MethodFlags::_misc_dont_inline\", Integer.class);\n@@ -193,1 +190,1 @@\n-    final int constMethodFlagsOffset = getFieldOffset(\"ConstMethod::_flags\", Integer.class, \"u2\");\n+    final int constMethodFlagsOffset = getFieldOffset(\"ConstMethod::_flags._flags\", Integer.class, \"u4\");\n@@ -201,5 +198,8 @@\n-    final int constMethodHasLineNumberTable = getConstant(\"ConstMethod::_has_linenumber_table\", Integer.class);\n-    final int constMethodHasLocalVariableTable = getConstant(\"ConstMethod::_has_localvariable_table\", Integer.class);\n-    final int constMethodHasMethodAnnotations = getConstant(\"ConstMethod::_has_method_annotations\", Integer.class);\n-    final int constMethodHasParameterAnnotations = getConstant(\"ConstMethod::_has_parameter_annotations\", Integer.class);\n-    final int constMethodHasExceptionTable = getConstant(\"ConstMethod::_has_exception_table\", Integer.class);\n+    final int constMethodFlagsReservedStackAccess = getConstant(\"ConstMethodFlags::_misc_reserved_stack_access\", Integer.class);\n+    final int constMethodFlagsCallerSensitive = getConstant(\"ConstMethodFlags::_misc_caller_sensitive\", Integer.class);\n+    final int constMethodFlagsIntrinsicCandidate = getConstant(\"ConstMethodFlags::_misc_intrinsic_candidate\", Integer.class);\n+    final int constMethodHasLineNumberTable = getConstant(\"ConstMethodFlags::_misc_has_linenumber_table\", Integer.class);\n+    final int constMethodHasLocalVariableTable = getConstant(\"ConstMethodFlags::_misc_has_localvariable_table\", Integer.class);\n+    final int constMethodHasMethodAnnotations = getConstant(\"ConstMethodFlags::_misc_has_method_annotations\", Integer.class);\n+    final int constMethodHasParameterAnnotations = getConstant(\"ConstMethodFlags::_misc_has_parameter_annotations\", Integer.class);\n+    final int constMethodHasExceptionTable = getConstant(\"ConstMethodFlags::_misc_has_exception_table\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n@@ -60,1 +60,1 @@\n-compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n@@ -63,2 +63,2 @@\n-compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263 generic-x64,generic-i586\n-compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n@@ -103,1 +103,0 @@\n-runtime\/Thread\/TestAlwaysPreTouchStacks.java 8305416 generic-all\n@@ -109,0 +108,1 @@\n+containers\/docker\/TestMemoryAwareness.java 8303470 linux-x64\n@@ -167,0 +167,2 @@\n+gtest\/NMTGtests.java#nmt-detail 8306561 aix-ppc64\n+gtest\/NMTGtests.java#nmt-summary 8306561 aix-ppc64\n@@ -174,2 +176,0 @@\n-vmTestbase\/nsk\/jdi\/ThreadReference\/stop\/stop001\/TestDescription.java 7034630 generic-all\n-\n@@ -181,0 +181,1 @@\n+vmTestbase\/nsk\/jvmti\/AttachOnDemand\/attach002a\/TestDescription.java 8307462 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -136,0 +136,7 @@\n+serviceability_ttf_virtual = \\\n+  serviceability\/ \\\n+  -serviceability\/jvmti\/vthread \\\n+  -serviceability\/jvmti\/thread  \\\n+  -serviceability\/jvmti\/events  \\\n+  -serviceability\/jvmti\/negative\n+\n@@ -389,0 +396,1 @@\n+ -runtime\/ErrorHandling\/ReattemptErrorTest.java \\\n@@ -708,2 +716,1 @@\n-  vmTestbase\/gc\/ArrayJuggle\/ \\\n-  vmTestbase\/gc\/memory\/Array\/ArrayJuggle\n+  vmTestbase\/gc\/ArrayJuggle\/\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -652,0 +652,20 @@\n+    public static final String MAX_D_REDUCTION_REG = PREFIX + \"MAX_D_REDUCTION_REG\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MAX_D_REDUCTION_REG, \"maxD_reduction_reg\");\n+    }\n+\n+    public static final String MAX_D_REG = PREFIX + \"MAX_D_REG\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MAX_D_REG, \"maxD_reg\");\n+    }\n+\n+    public static final String MAX_F_REDUCTION_REG = PREFIX + \"MAX_F_REDUCTION_REG\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MAX_F_REDUCTION_REG, \"maxF_reduction_reg\");\n+    }\n+\n+    public static final String MAX_F_REG = PREFIX + \"MAX_F_REG\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MAX_F_REG, \"maxF_reg\");\n+    }\n+\n@@ -682,0 +702,20 @@\n+    public static final String MIN_D_REDUCTION_REG = PREFIX + \"MIN_D_REDUCTION_REG\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MIN_D_REDUCTION_REG, \"minD_reduction_reg\");\n+    }\n+\n+    public static final String MIN_D_REG = PREFIX + \"MIN_D_REG\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MIN_D_REG, \"minD_reg\");\n+    }\n+\n+    public static final String MIN_F_REDUCTION_REG = PREFIX + \"MIN_F_REDUCTION_REG\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MIN_F_REDUCTION_REG, \"minF_reduction_reg\");\n+    }\n+\n+    public static final String MIN_F_REG = PREFIX + \"MIN_F_REG\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(MIN_F_REG, \"minF_reg\");\n+    }\n+\n@@ -1381,0 +1421,40 @@\n+    public static final String VMASK_CMP_IMM_B_SVE = PREFIX + \"VMASK_CMP_IMM_B_SVE\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VMASK_CMP_IMM_B_SVE, \"vmaskcmp_immB_sve\");\n+    }\n+\n+    public static final String VMASK_CMPU_IMM_B_SVE = PREFIX + \"VMASK_CMPU_IMM_B_SVE\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VMASK_CMPU_IMM_B_SVE, \"vmaskcmpU_immB_sve\");\n+    }\n+\n+    public static final String VMASK_CMP_IMM_S_SVE = PREFIX + \"VMASK_CMP_IMM_S_SVE\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VMASK_CMP_IMM_S_SVE, \"vmaskcmp_immS_sve\");\n+    }\n+\n+    public static final String VMASK_CMPU_IMM_S_SVE = PREFIX + \"VMASK_CMPU_IMM_S_SVE\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VMASK_CMPU_IMM_S_SVE, \"vmaskcmpU_immS_sve\");\n+    }\n+\n+    public static final String VMASK_CMP_IMM_I_SVE = PREFIX + \"VMASK_CMP_IMM_I_SVE\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VMASK_CMP_IMM_I_SVE, \"vmaskcmp_immI_sve\");\n+    }\n+\n+    public static final String VMASK_CMPU_IMM_I_SVE = PREFIX + \"VMASK_CMPU_IMM_I_SVE\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VMASK_CMPU_IMM_I_SVE, \"vmaskcmpU_immI_sve\");\n+    }\n+\n+    public static final String VMASK_CMP_IMM_L_SVE = PREFIX + \"VMASK_CMP_IMM_L_SVE\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VMASK_CMP_IMM_L_SVE, \"vmaskcmp_immL_sve\");\n+    }\n+\n+    public static final String VMASK_CMPU_IMM_L_SVE = PREFIX + \"VMASK_CMPU_IMM_L_SVE\" + POSTFIX;\n+    static {\n+        machOnlyNameRegex(VMASK_CMPU_IMM_L_SVE, \"vmaskcmpU_immL_sve\");\n+    }\n+\n@@ -1441,0 +1521,18 @@\n+    public static final String Z_LOAD_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"Z_LOAD_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"zLoadP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(Z_LOAD_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String Z_STORE_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"Z_STORE_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"zStoreP\\\\S*\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(Z_STORE_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n+    public static final String Z_GET_AND_SET_P_WITH_BARRIER_FLAG = COMPOSITE_PREFIX + \"Z_GET_AND_SET_P_WITH_BARRIER_FLAG\" + POSTFIX;\n+    static {\n+        String regex = START + \"(zXChgP)|(zGetAndSetP\\\\S*)\" + MID + \"barrier\\\\(\\\\s*\" + IS_REPLACED + \"\\\\s*\\\\)\" + END;\n+        machOnly(Z_GET_AND_SET_P_WITH_BARRIER_FLAG, regex);\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/IRNode.java","additions":98,"deletions":0,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+import jdk.test.lib.SA.SATestUtils;\n@@ -50,0 +51,4 @@\n+        if (SATestUtils.needsPrivileges()) {\n+            \/\/ This test will create a file as root that cannot be easily deleted, so don't run it.\n+            throw new SkippedException(\"Cannot run this test on OSX if adding privileges is required.\");\n+        }\n","filename":"test\/hotspot\/jtreg\/serviceability\/sa\/ClhsdbDumpclass.java","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -461,0 +461,1 @@\n+java\/awt\/event\/SequencedEvent\/MultipleContextsFunctionalTest.java 8305061 macosx-x64\n@@ -527,0 +528,2 @@\n+javax\/management\/remote\/mandatory\/connection\/BrokenConnectionTest.java 8262312 linux-all\n+\n@@ -608,0 +611,1 @@\n+sun\/security\/pkcs11\/Cipher\/TestKATForGCM.java                   8240611 linux-x64,macosx-x64\n@@ -669,0 +673,1 @@\n+sanity\/client\/SwingSet\/src\/EditorPaneDemoTest.java 8212240 linux-x64\n@@ -700,1 +705,0 @@\n-com\/sun\/jdi\/JdbLastErrorTest.java                               8305913 windows-x64\n@@ -744,0 +748,1 @@\n+jdk\/incubator\/vector\/LoadJsvmlTest.java                         8305390 windows-x64\n","filename":"test\/jdk\/ProblemList.txt","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -225,0 +225,1 @@\n+    com\/sun\/org\/apache\/xml\/internal\/security \\\n@@ -232,1 +233,0 @@\n-    com\/sun\/org\/apache\/xml\/internal\/security \\\n@@ -373,0 +373,1 @@\n+    jdk\/internal\/reflect\/CallerSensitive\/CheckCSMs.java \\\n","filename":"test\/jdk\/TEST.groups","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -50,0 +50,2 @@\n+import jdk.internal.foreign.CABI;\n+\n@@ -129,0 +131,1 @@\n+        map.put(\"jdk.foreign.linker\", this::jdkForeignLinker);\n@@ -349,2 +352,1 @@\n-        vmOptFinalFlag(map, \"UseCompressedOops\");\n-        vmOptFinalFlag(map, \"UseVectorizedMismatchIntrinsic\");\n+        vmOptFinalFlag(map, \"CriticalJNINatives\");\n@@ -354,0 +356,2 @@\n+        vmOptFinalFlag(map, \"UseCompressedOops\");\n+        vmOptFinalFlag(map, \"UseVectorizedMismatchIntrinsic\");\n@@ -355,0 +359,1 @@\n+        vmOptFinalFlag(map, \"ZGenerational\");\n@@ -476,1 +481,1 @@\n-   \/**\n+    \/**\n@@ -656,0 +661,11 @@\n+    \/*\n+     * A string indicating the foreign linker that is currently being used. See jdk.internal.foreign.CABI\n+     * for valid values.\n+     *\n+     * \"FALLBACK\" and \"UNSUPPORTED\" are special values. The former indicates the fallback linker is\n+     * being used. The latter indicates an unsupported platform.\n+     *\/\n+    private String jdkForeignLinker() {\n+        return String.valueOf(CABI.current());\n+    }\n+\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":19,"deletions":3,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -145,0 +145,1 @@\n+compiler.warn.preview.feature.use                       # preview feature support: not generated currently\n","filename":"test\/langtools\/tools\/javac\/diags\/examples.not-yet.txt","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}