{"files":[{"patch":"@@ -1901,6 +1901,0 @@\n-  __ verified_entry(C, 0);\n-  __ bind(*_verified_entry);\n-  \/\/ n.b. frame size includes space for return pc and rfp\n-  const int framesize = C->output()->frame_size_in_bytes();\n-  assert(framesize%(2*wordSize) == 0, \"must preserve 2*wordSize alignment\");\n-\n@@ -1926,5 +1920,2 @@\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  if (C->output()->need_stack_bang(bangsize))\n-    __ generate_stack_overflow_check(bangsize);\n-\n-  __ build_frame(framesize);\n+  __ verified_entry(C, 0);\n+  __ bind(*_verified_entry);\n@@ -3941,0 +3932,6 @@\n+    if (EnableValhalla) {\n+      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      __ andr(tmp, tmp, ~((int) markWord::inline_type_bit_in_place));\n+    }\n+\n@@ -14982,1 +14979,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg_immL0(iRegL_R11 cnt, iRegP_R10 base, immL0 zero, Universe dummy, rFlagsReg cr)\n@@ -14984,1 +14981,1 @@\n-  match(Set dummy (ClearArray (Binary cnt base) val));\n+  match(Set dummy (ClearArray (Binary cnt base) zero));\n@@ -15001,0 +14998,16 @@\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n+%{\n+  predicate(((ClearArrayNode*)n)->word_copy_only());\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n+  effect(USE_KILL cnt, USE_KILL base, KILL cr);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ClearArray $cnt, $base, $val\" %}\n+\n+  ins_encode %{\n+    __ fill_words($base$$Register, $cnt$$Register, $val$$Register);\n+  %}\n+\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -15004,1 +15017,2 @@\n-            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord));\n+            < (uint64_t)(BlockZeroingLowLimit >> LogBytesPerWord)\n+            && !((ClearArrayNode*)n)->word_copy_only());\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":28,"deletions":14,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -211,0 +211,1 @@\n+         stub_id == Runtime1::new_instance_no_inline_id       ||\n@@ -256,1 +257,2 @@\n-NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result, CodeEmitInfo* info, bool is_inline_type) {\n+NewObjectArrayStub::NewObjectArrayStub(LIR_Opr klass_reg, LIR_Opr length, LIR_Opr result,\n+                                       CodeEmitInfo* info, bool is_inline_type) {\n@@ -302,1 +304,1 @@\n-    __ mov(rscratch2, markWord::always_locked_pattern);\n+    __ mov(rscratch2, markWord::inline_type_pattern);\n@@ -306,1 +308,1 @@\n-    __ br(Assembler::NE, *_throw_imse_stub->entry());\n+    __ br(Assembler::EQ, *_throw_imse_stub->entry());\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -248,1 +248,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), needs_stack_repair(), NULL);\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n@@ -463,1 +463,2 @@\n-  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n+  int initial_framesize = initial_frame_size_in_bytes();\n+  __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);\n@@ -528,1 +529,0 @@\n-\n@@ -530,1 +530,2 @@\n-  __ remove_frame(initial_frame_size_in_bytes(), needs_stack_repair());\n+  int initial_framesize = initial_frame_size_in_bytes();\n+  __ remove_frame(initial_framesize, needs_stack_repair(), initial_framesize - wordSize);\n@@ -1068,1 +1069,0 @@\n-      __ andr(dest->as_register(), dest->as_register(), oopDesc::compressed_klass_mask());\n@@ -1070,2 +1070,0 @@\n-    } else {\n-      __   ubfm(dest->as_register(), dest->as_register(), 0, 63 - oopDesc::storage_props_nof_bits);\n@@ -1387,0 +1385,1 @@\n+  if (op->need_null_check()) {\n@@ -1405,0 +1404,1 @@\n+  }\n@@ -1594,2 +1594,3 @@\n-  \/\/ We are loading\/storing an array that *may* be a flattened array (the declared type\n-  \/\/ Object[], interface[], or VT?[]). If this array is flattened, take slow path.\n+  \/\/ We are loading\/storing from\/to an array that *may* be flattened (the\n+  \/\/ declared type is Object[], abstract[], interface[] or VT.ref[]).\n+  \/\/ If this array is flattened, take the slow path.\n@@ -1597,3 +1598,9 @@\n-  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());\n-  __ tst(op->tmp()->as_register(), ArrayStorageProperties::flattened_value);\n-  __ br(Assembler::NE, *op->stub()->entry());\n+  Register klass = op->tmp()->as_register();\n+  if (UseArrayMarkWordCheck) {\n+    __ test_flattened_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+  } else {\n+    __ load_klass(klass, op->array()->as_register());\n+    __ ldrw(klass, Address(klass, Klass::layout_helper_offset()));\n+    __ tst(klass, Klass::_lh_array_tag_vt_value_bit_inplace);\n+    __ br(Assembler::NE, *op->stub()->entry());\n+  }\n@@ -1601,1 +1608,2 @@\n-    \/\/ We are storing into the array.\n+    \/\/ The array is not flattened, but it might be null-free. If we are storing\n+    \/\/ a null into a null-free array, take the slow path (which will throw NPE).\n@@ -1603,5 +1611,7 @@\n-    __ tst(op->tmp()->as_register(), ArrayStorageProperties::null_free_value);\n-    __ br(Assembler::EQ, skip);\n-    \/\/ The array is not flattened, but it is null_free. If we are storing\n-    \/\/ a null, take the slow path (which will throw NPE).\n-    __ cbz(op->value()->as_register(), *op->stub()->entry());\n+    __ cbnz(op->value()->as_register(), skip);\n+    if (UseArrayMarkWordCheck) {\n+      __ test_null_free_array_oop(op->array()->as_register(), op->tmp()->as_register(), *op->stub()->entry());\n+    } else {\n+      __ tst(klass, Klass::_lh_null_free_bit_inplace);\n+      __ br(Assembler::NE, *op->stub()->entry());\n+    }\n@@ -1610,1 +1620,0 @@\n-\n@@ -1614,7 +1623,17 @@\n-  \/\/ This is called when we use aastore into a an array declared as \"[LVT;\",\n-  \/\/ where we know VT is not flattened (due to FlatArrayElementMaxSize, etc).\n-  \/\/ However, we need to do a NULL check if the actual array is a \"[QVT;\".\n-\n-  __ load_storage_props(op->tmp()->as_register(), op->array()->as_register());\n-  __ mov(rscratch1, (uint64_t) ArrayStorageProperties::null_free_value);\n-  __ cmp(op->tmp()->as_register(), rscratch1);\n+  \/\/ We are storing into an array that *may* be null-free (the declared type is\n+  \/\/ Object[], abstract[], interface[] or VT.ref[]).\n+  if (UseArrayMarkWordCheck) {\n+    Label test_mark_word;\n+    Register tmp = op->tmp()->as_register();\n+    __ ldr(tmp, Address(op->array()->as_register(), oopDesc::mark_offset_in_bytes()));\n+    __ tst(tmp, markWord::unlocked_value);\n+    __ br(Assembler::NE, test_mark_word);\n+    __ load_prototype_header(tmp, op->array()->as_register());\n+    __ bind(test_mark_word);\n+    __ tst(tmp, markWord::nullfree_array_bit_in_place);\n+  } else {\n+    Register klass = op->tmp()->as_register();\n+    __ load_klass(klass, op->array()->as_register());\n+    __ ldr(klass, Address(klass, Klass::layout_helper_offset()));\n+    __ tst(klass, Klass::_lh_null_free_bit_inplace);\n+  }\n@@ -1642,1 +1661,0 @@\n-\n@@ -1646,1 +1664,1 @@\n-  \/\/ (2) Value object check -- if either of the operands is not a value object,\n+  \/\/ (2) Inline type check -- if either of the operands is not a inline type,\n@@ -1648,1 +1666,1 @@\n-  \/\/     operands are value objects\n+  \/\/     operands are inline type\n@@ -1651,13 +1669,7 @@\n-    Register tmp1  = rscratch1; \/* op->tmp1()->as_register(); *\/\n-    Register tmp2  = rscratch2; \/* op->tmp2()->as_register(); *\/\n-\n-    __ mov(tmp1, (intptr_t)markWord::always_locked_pattern);\n-\n-    __ ldr(tmp2, Address(left, oopDesc::mark_offset_in_bytes()));\n-    __ andr(tmp1, tmp1, tmp2);\n-\n-    __ ldr(tmp2, Address(right, oopDesc::mark_offset_in_bytes()));\n-    __ andr(tmp1, tmp1, tmp2);\n-\n-    __ mov(tmp2, (intptr_t)markWord::always_locked_pattern);\n-    __ cmp(tmp1, tmp2);\n+    Register tmp1  = op->tmp1()->as_register();\n+    __ mov(tmp1, markWord::inline_type_pattern);\n+    __ ldr(rscratch1, Address(left, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ ldr(rscratch1, Address(right, oopDesc::mark_offset_in_bytes()));\n+    __ andr(tmp1, tmp1, rscratch1);\n+    __ cmp(tmp1, (u1)markWord::inline_type_pattern);\n@@ -1675,1 +1687,1 @@\n-    if (UseCompressedOops) {\n+    if (UseCompressedClassPointers) {\n@@ -1697,1 +1709,1 @@\n-  \/\/ We've returned from the stub. op->result_opr() contains 0x0 IFF the two\n+  \/\/ We've returned from the stub. R0 contains 0x0 IFF the two\n@@ -1701,7 +1713,1 @@\n-\n-  if (op->result_opr()->type() == T_LONG) {\n-    __ cbzw(op->result_opr()->as_register(), L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n-  } else {\n-    __ cbz(op->result_opr()->as_register(), L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n-  }\n-\n+  __ cbz(r0, L_oops_not_equal); \/\/ (call_stub() == 0x0) -> not_equal\n@@ -1711,1 +1717,0 @@\n-\n@@ -2233,1 +2238,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2243,1 +2248,1 @@\n-  add_call_info(code_offset(), op->info());\n+  add_call_info(code_offset(), op->info(), op->maybe_return_as_fields());\n@@ -2414,6 +2419,10 @@\n-void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest) {\n-  __ load_storage_props(tmp, obj);\n-  if (is_dest) {\n-    \/\/ We also take slow path if it's a null_free destination array, just in case the source array\n-    \/\/ contains NULLs.\n-    __ tst(tmp, ArrayStorageProperties::flattened_value | ArrayStorageProperties::null_free_value);\n+void LIR_Assembler::arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check) {\n+  if (null_check) {\n+    __ cbz(obj, *slow_path->entry());\n+  }\n+  if (UseArrayMarkWordCheck) {\n+    if (is_dest) {\n+      __ test_null_free_array_oop(obj, tmp, *slow_path->entry());\n+    } else {\n+      __ test_flattened_array_oop(obj, tmp, *slow_path->entry());\n+    }\n@@ -2421,1 +2430,9 @@\n-    __ tst(tmp, ArrayStorageProperties::flattened_value);\n+    __ load_klass(tmp, obj);\n+    __ ldr(tmp, Address(tmp, Klass::layout_helper_offset()));\n+    if (is_dest) {\n+      \/\/ Take the slow path if it's a null_free destination array, in case the source array contains NULLs.\n+      __ tst(tmp, Klass::_lh_null_free_bit_inplace);\n+    } else {\n+      __ tst(tmp, Klass::_lh_array_tag_vt_value_bit_inplace);\n+    }\n+    __ br(Assembler::NE, *slow_path->entry());\n@@ -2423,1 +2440,0 @@\n-  __ br(Assembler::NE, *slow_path->entry());\n@@ -2426,2 +2442,0 @@\n-\n-\n@@ -2454,10 +2468,0 @@\n-  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n-    arraycopy_inlinetype_check(src, tmp, stub, false);\n-  }\n-\n-  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n-    arraycopy_inlinetype_check(dst, tmp, stub, true);\n-  }\n-\n-\n-\n@@ -2517,0 +2521,9 @@\n+  \/\/ Handle inline type arrays\n+  if (flags & LIR_OpArrayCopy::src_inlinetype_check) {\n+    arraycopy_inlinetype_check(src, tmp, stub, false, (flags & LIR_OpArrayCopy::src_null_check));\n+  }\n+\n+  if (flags & LIR_OpArrayCopy::dst_inlinetype_check) {\n+    arraycopy_inlinetype_check(dst, tmp, stub, true, (flags & LIR_OpArrayCopy::dst_null_check));\n+  }\n+\n@@ -3077,1 +3090,18 @@\n-  Unimplemented();\n+  Register obj = op->obj()->as_register();\n+  Register tmp = op->tmp()->as_pointer_register();\n+  Address mdo_addr = as_Address(op->mdp()->as_address_ptr());\n+  bool not_null = op->not_null();\n+  int flag = op->flag();\n+\n+  Label not_inline_type;\n+  if (!not_null) {\n+    __ cbz(obj, not_inline_type);\n+  }\n+\n+  __ test_oop_is_not_inline_type(obj, tmp, not_inline_type);\n+\n+  __ ldrb(rscratch1, mdo_addr);\n+  __ orr(rscratch1, rscratch1, flag);\n+  __ strb(rscratch1, mdo_addr);\n+\n+  __ bind(not_inline_type);\n@@ -3218,0 +3248,4 @@\n+void LIR_Assembler::check_orig_pc() {\n+  __ ldr(rscratch2, frame_map()->address_for_orig_pc_addr());\n+  __ cmp(rscratch2, (u1)NULL_WORD);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":109,"deletions":75,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -82,0 +82,3 @@\n+  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest, bool null_check);\n+  void move(LIR_Opr src, LIR_Opr dst);\n+\n@@ -87,2 +90,0 @@\n-  void arraycopy_inlinetype_check(Register obj, Register tmp, CodeStub* slow_path, bool is_dest);\n-  void move(LIR_Opr src, LIR_Opr dst);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -184,1 +184,1 @@\n-        __ add(tmp, tmp, LIR_OprFact::intptrConst(large_disp));\n+        __ add(index, LIR_OprFact::intptrConst(large_disp), tmp);\n@@ -201,1 +201,1 @@\n-  if (large_disp == 0) {\n+  if (large_disp == 0 && index->is_register()) {\n@@ -1160,5 +1160,6 @@\n-                       FrameMap::r2_oop_opr,\n-                       FrameMap::r5_oop_opr,\n-                       FrameMap::r4_oop_opr,\n-                       LIR_OprFact::illegalOpr,\n-                       FrameMap::r3_metadata_opr, info);\n+               \/* allow_inline *\/ false,\n+               FrameMap::r2_oop_opr,\n+               FrameMap::r5_oop_opr,\n+               FrameMap::r4_oop_opr,\n+               LIR_OprFact::illegalOpr,\n+               FrameMap::r3_metadata_opr, info);\n@@ -1170,2 +1171,2 @@\n-  \/\/ Mapping to do_NewInstance (same code)\n-  CodeEmitInfo* info = state_for(x, x->state());\n+  \/\/ Mapping to do_NewInstance (same code) but use state_before for reexecution.\n+  CodeEmitInfo* info = state_for(x, x->state_before());\n@@ -1174,6 +1175,7 @@\n-  new_instance(reg, x->klass(), x->is_unresolved(),\n-             FrameMap::r2_oop_opr,\n-             FrameMap::r5_oop_opr,\n-             FrameMap::r4_oop_opr,\n-             LIR_OprFact::illegalOpr,\n-             FrameMap::r3_metadata_opr, info);\n+  new_instance(reg, x->klass(), false,\n+               \/* allow_inline *\/ true,\n+               FrameMap::r2_oop_opr,\n+               FrameMap::r5_oop_opr,\n+               FrameMap::r4_oop_opr,\n+               LIR_OprFact::illegalOpr,\n+               FrameMap::r3_metadata_opr, info);\n@@ -1422,1 +1424,6 @@\n-  __ cmp(lir_cond(cond), left, right);\n+  if (x->substitutability_check()) {\n+    substitutability_check(x, *xin, *yin);\n+  } else {\n+    __ cmp(lir_cond(cond), left, right);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":23,"deletions":16,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -53,0 +53,1 @@\n+  assert(index()->is_illegal() || disp() == 0, \"cannot set both index and displacement\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIR_aarch64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -97,1 +97,2 @@\n-  if (EnableValhalla && !UseBiasedLocking) {\n+  if (EnableValhalla) {\n+    assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n@@ -99,1 +100,1 @@\n-    andr(hdr, hdr, ~markWord::biased_lock_bit_in_place);\n+    andr(hdr, hdr, ~markWord::inline_type_bit_in_place);\n@@ -190,1 +191,2 @@\n-  if (UseBiasedLocking && !len->is_valid()) {\n+  if (EnableValhalla) {\n+    \/\/ Need to copy markWord::prototype header for klass\n@@ -303,0 +305,1 @@\n+\n@@ -348,0 +351,7 @@\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_inc, bool needs_stack_repair) {\n+  MacroAssembler::build_frame(frame_size_in_bytes + 2 * wordSize);\n+\n+  if (needs_stack_repair) {\n+    Unimplemented();\n+  }\n+}\n@@ -349,2 +359,2 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, bool needs_stack_repair, Label* verified_inline_entry_label) {\n-  assert(bang_size_in_bytes >= framesize, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -360,1 +370,1 @@\n-  MacroAssembler::build_frame(framesize + 2 * wordSize);\n+  build_frame_helper(frame_size_in_bytes, 0, needs_stack_repair);\n@@ -367,5 +377,4 @@\n-void C1_MacroAssembler::remove_frame(int framesize, bool needs_stack_repair) {\n-\n-  guarantee(needs_stack_repair == false, \"Stack repair should not be true\");\n-\n-  MacroAssembler::remove_frame(framesize + 2 * wordSize);\n+void C1_MacroAssembler::remove_frame(int frame_size_in_bytes, bool needs_stack_repair,\n+                                     int sp_inc_offset) {\n+  MacroAssembler::remove_frame(frame_size_in_bytes + 2 * wordSize,\n+                               needs_stack_repair, sp_inc_offset);\n@@ -374,12 +383,5 @@\n-void C1_MacroAssembler::verified_inline_entry() {\n-  if (C1Breakpoint || VerifyFPU || !UseStackBanging) {\n-    \/\/ Verified Entry first instruction should be 5 bytes long for correct\n-    \/\/ patching by patch_verified_entry().\n-    \/\/\n-    \/\/ C1Breakpoint and VerifyFPU have one byte first instruction.\n-    \/\/ Also first instruction will be one byte \"push(rbp)\" if stack banging\n-    \/\/ code is not generated (see build_frame() above).\n-    \/\/ For all these cases generate long instruction first.\n-    nop();\n-  }\n-\n+void C1_MacroAssembler::verified_entry() {\n+  \/\/ If we have to make this method not-entrant we'll overwrite its\n+  \/\/ first instruction with a jump.  For this action to be legal we\n+  \/\/ must ensure that this first instruction is a B, BL, NOP, BKPT,\n+  \/\/ SVC, HVC, or SMC.  Make it a NOP.\n@@ -387,2 +389,1 @@\n-  \/\/ build frame\n-  \/\/ verify_FPU(0, \"method_entry\");\n+  if (C1Breakpoint) brk(1);\n@@ -391,16 +392,1 @@\n-int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n-  \/\/ This function required to support for InlineTypePassFieldsAsArgs\n-  if (C1Breakpoint || VerifyFPU || !UseStackBanging) {\n-    \/\/ Verified Entry first instruction should be 5 bytes long for correct\n-    \/\/ patching by patch_verified_entry().\n-    \/\/\n-    \/\/ C1Breakpoint and VerifyFPU have one byte first instruction.\n-    \/\/ Also first instruction will be one byte \"push(rbp)\" if stack banging\n-    \/\/ code is not generated (see build_frame() above).\n-    \/\/ For all these cases generate long instruction first.\n-    nop();\n-  }\n-\n-  nop();\n-  \/\/ verify_FPU(0, \"method_entry\");\n-\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n@@ -408,0 +394,3 @@\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n@@ -409,1 +398,1 @@\n-  GrowableArray<SigEntry>* sig   = &ces->sig();\n+  GrowableArray<SigEntry>* sig    = &ces->sig();\n@@ -421,7 +410,4 @@\n-  \/\/ Create a temp frame so we can call into runtime. It must be properly set up to accomodate GC.\n-  int sp_inc = (args_on_stack - args_on_stack_cc) * VMRegImpl::stack_slot_size;\n-  if (sp_inc > 0) {\n-    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n-    sub(sp, sp, sp_inc);\n-  } else {\n-    sp_inc = 0;\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    Unimplemented();\n@@ -430,9 +416,9 @@\n-  sub(sp, sp, frame_size_in_bytes);\n-  if (sp_inc > 0) {\n-    int real_frame_size = frame_size_in_bytes +\n-           + wordSize  \/\/ pushed rbp\n-           + wordSize  \/\/ returned address pushed by the stack extension code\n-           + sp_inc;   \/\/ stack extension\n-    mov(rscratch1, real_frame_size);\n-    str(rscratch1, Address(sp, frame_size_in_bytes - wordSize));\n-  }\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_inc, ces->c1_needs_stack_repair());\n+\n+  \/\/ Initialize orig_pc to detect deoptimization during buffering in below runtime call\n+  str(zr, Address(sp, sp_offset_for_orig_pc));\n+\n+  \/\/ The runtime call might safepoint, make sure nmethod entry barrier is executed\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->nmethod_entry_barrier(this);\n@@ -452,11 +438,7 @@\n-  int n = shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n-                              args_passed_cc, regs_cc,            \/\/ from\n-                              args_passed, args_on_stack, regs);  \/\/ to\n-  assert(sp_inc == n, \"must be\");\n-\n-  if (sp_inc != 0) {\n-    \/\/ Do the stack banging here, and skip over the stack repair code in the\n-    \/\/ verified_inline_entry (which has a different real_frame_size).\n-    assert(sp_inc > 0, \"stack should not shrink\");\n-    generate_stack_overflow_check(bang_size_in_bytes);\n-    decrement(sp, frame_size_in_bytes);\n+  shuffle_inline_args(true, is_inline_ro_entry, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs,          \/\/ to\n+                      sp_inc);\n+\n+  if (ces->c1_needs_stack_repair()) {\n+    Unimplemented();\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":51,"deletions":69,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -114,0 +114,2 @@\n+  void remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -634,0 +634,1 @@\n+    case new_instance_no_inline_id:\n@@ -642,0 +643,2 @@\n+        } else if (id == new_instance_no_inline_id) {\n+          __ set_info(\"new_instance_no_inline\", dont_gc_arguments);\n@@ -701,1 +704,6 @@\n-        int call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        int call_offset;\n+        if (id == new_instance_no_inline_id) {\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance_no_inline), klass);\n+        } else {\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_instance), klass);\n+        }\n@@ -742,2 +750,1 @@\n-        }\n-        else if (id == new_object_array_id) {\n+        } else if (id == new_object_array_id) {\n@@ -745,2 +752,1 @@\n-        }\n-        else {\n+        } else {\n@@ -757,2 +763,0 @@\n-\n-          int tag = 0;\n@@ -760,4 +764,21 @@\n-           case new_type_array_id: tag = Klass::_lh_array_tag_type_value; break;\n-           case new_object_array_id: tag = Klass::_lh_array_tag_obj_value; break;\n-           case new_flat_array_id: tag = Klass::_lh_array_tag_vt_value; break;\n-           default:  ShouldNotReachHere();\n+          case new_type_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_type_value);\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is a type array klass)\");\n+            break;\n+          case new_object_array_id:\n+            __ cmpw(t0, Klass::_lh_array_tag_obj_value); \/\/ new \"[Ljava\/lang\/Object;\"\n+            __ br(Assembler::EQ, ok);\n+            __ cmpw(t0, Klass::_lh_array_tag_vt_value);  \/\/ new \"[LVT;\"\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          case new_flat_array_id:\n+            \/\/ new \"[QVT;\"\n+            __ cmpw(t0, Klass::_lh_array_tag_vt_value);  \/\/ the array can be flattened.\n+            __ br(Assembler::EQ, ok);\n+            __ cmpw(t0, Klass::_lh_array_tag_obj_value); \/\/ the array cannot be flattened (due to InlineArrayElementMaxFlatSize, etc)\n+            __ br(Assembler::EQ, ok);\n+            __ stop(\"assert(is an object or inline type array klass)\");\n+            break;\n+          default:  ShouldNotReachHere();\n@@ -765,4 +786,0 @@\n-          __ mov(rscratch1, tag);\n-          __ cmpw(t0, rscratch1);\n-          __ br(Assembler::EQ, ok);\n-          __ stop(\"assert(is an array klass)\");\n@@ -824,2 +841,1 @@\n-        } else {\n-          \/\/ Runtime1::new_object_array handles both object and flat arrays\n+        } else if (id == new_object_array_id) {\n@@ -827,0 +843,3 @@\n+        } else {\n+          assert(id == new_flat_array_id, \"must be\");\n+          call_offset = __ call_RT(obj, noreg, CAST_FROM_FN_PTR(address, new_flat_array), klass, length);\n@@ -863,1 +882,1 @@\n-    {\n+      {\n@@ -867,1 +886,1 @@\n-        OopMap* map = save_live_registers(sasm, 2);\n+        OopMap* map = save_live_registers(sasm);\n@@ -883,1 +902,1 @@\n-        OopMap* map = save_live_registers(sasm, 3);\n+        OopMap* map = save_live_registers(sasm);\n@@ -918,1 +937,1 @@\n-      case substitutability_check_id:\n+    case substitutability_check_id:\n@@ -921,1 +940,1 @@\n-        OopMap* map = save_live_registers(sasm, 3);\n+        OopMap* map = save_live_registers(sasm);\n@@ -925,3 +944,3 @@\n-        f.load_argument(1, r0); \/\/ r0,: left\n-        f.load_argument(0, r1); \/\/ r1,: right\n-        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), r0, r1);\n+        f.load_argument(1, r1); \/\/ r1,: left\n+        f.load_argument(0, r2); \/\/ r2,: right\n+        int call_offset = __ call_RT(noreg, noreg, CAST_FROM_FN_PTR(address, substitutability_check), r1, r2);\n@@ -937,2 +956,0 @@\n-\n-\n@@ -978,1 +995,1 @@\n-      { StubFrame f(sasm, \"throw_incompatible_class_change_exception\", dont_gc_arguments);\n+      { StubFrame f(sasm, \"throw_incompatible_class_change_error\", dont_gc_arguments);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":45,"deletions":28,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/barrierSetRuntime.hpp\"\n@@ -49,0 +50,2 @@\n+\n+  assert(type != T_INLINE_TYPE, \"Not supported yet\");\n@@ -88,0 +91,1 @@\n+  assert(type != T_INLINE_TYPE, \"Not supported yet\");\n@@ -91,1 +95,1 @@\n-   if (in_heap) {\n+    if (in_heap) {\n@@ -135,0 +139,13 @@\n+void BarrierSetAssembler::value_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                                     Register src, Register dst, Register value_klass) {\n+  \/\/ value_copy implementation is fairly complex, and there are not any\n+  \/\/ \"short-cuts\" to be made from asm. What there is, appears to have the same\n+  \/\/ cost in C++, so just \"call_VM_leaf\" for now rather than maintain hundreds\n+  \/\/ of hand-rolled instructions...\n+  if (decorators & IS_DEST_UNINITIALIZED) {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy_is_dest_uninitialized), src, dst, value_klass);\n+  } else {\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, BarrierSetRuntime::value_copy), src, dst, value_klass);\n+  }\n+}\n+\n@@ -315,1 +332,0 @@\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":18,"deletions":2,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,0 +50,3 @@\n+  virtual void value_copy(MacroAssembler* masm, DecoratorSet decorators,\n+                          Register src, Register dst, Register value_klass);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -269,0 +269,59 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::read_inlined_field(Register holder_klass,\n+                                                   Register field_index, Register field_offset,\n+                                                   Register temp, Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = rscratch1;\n+  const Register dst_temp   = temp;\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grab the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  b(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  b(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_inlined_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+}\n+\n@@ -315,1 +374,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -321,1 +381,3 @@\n-  profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  if (profile) {\n+    profile_typecheck(r2, Rsub_klass, r5); \/\/ blows r2, reloads r5\n+  }\n@@ -327,1 +389,3 @@\n-  profile_typecheck_failed(r2); \/\/ blows r2\n+  if (profile) {\n+    profile_typecheck_failed(r2); \/\/ blows r2\n+  }\n@@ -793,0 +857,5 @@\n+    if (EnableValhalla) {\n+      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -797,5 +866,0 @@\n-    if (EnableValhalla && !UseBiasedLocking) {\n-      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n-      andr(swap_reg, swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n-    }\n-\n@@ -1155,1 +1219,1 @@\n-void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp) {\n+void InterpreterMacroAssembler::profile_not_taken_branch(Register mdp, bool acmp) {\n@@ -1167,1 +1231,1 @@\n-    update_mdp_by_constant(mdp, in_bytes(BranchData::branch_data_size()));\n+    update_mdp_by_constant(mdp, acmp ? in_bytes(ACmpData::acmp_data_size()) : in_bytes(BranchData::branch_data_size()));\n@@ -1531,0 +1595,79 @@\n+void InterpreterMacroAssembler::profile_array(Register mdp,\n+                                              Register array,\n+                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flattened_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element(Register mdp,\n+                                                Register element,\n+                                                Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_acmp(Register mdp,\n+                                             Register left,\n+                                             Register right,\n+                                             Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, left);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::left_offset())));\n+\n+    Label left_not_inline_type;\n+    test_oop_is_not_inline_type(left, tmp, left_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::left_inline_type_byte_constant());\n+    bind(left_not_inline_type);\n+\n+    mov(tmp, right);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ACmpData::right_offset())));\n+\n+    Label right_not_inline_type;\n+    test_oop_is_not_inline_type(right, tmp, right_not_inline_type);\n+    set_mdp_flag_at(mdp, ACmpData::right_inline_type_byte_constant());\n+    bind(right_not_inline_type);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":154,"deletions":11,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -121,0 +121,22 @@\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n+  \/\/ Allocate instance in \"obj\" and read in the content of the inline field\n+  \/\/ NOTES:\n+  \/\/   - input holder object via \"obj\", which must be r0,\n+  \/\/     will return new instance via the same reg\n+  \/\/   - assumes holder_klass and valueKlass field klass have both been resolved\n+  void read_inlined_field(Register holder_klass,\n+                          Register field_index, Register field_offset,\n+                          Register temp,  Register obj = r0);\n+\n+  \/\/ Allocate value buffer in \"obj\" and read in flattened element at the given index\n+  \/\/ NOTES:\n+  \/\/   - Return via \"obj\" must be r0\n+  \/\/   - kills all given regs\n+  void read_flattened_element(Register array, Register index,\n+                              Register t1, Register t2,\n+                              Register obj = r0);\n+\n@@ -166,1 +188,1 @@\n-  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype );\n+  void gen_subtype_check( Register sub_klass, Label &ok_is_subtype, bool profile = true);\n@@ -259,1 +281,1 @@\n-  void profile_not_taken_branch(Register mdp);\n+  void profile_not_taken_branch(Register mdp, bool acmp = false);\n@@ -272,0 +294,3 @@\n+  void profile_array(Register mdp, Register array, Register tmp);\n+  void profile_element(Register mdp, Register element, Register tmp);\n+  void profile_acmp(Register mdp, Register left, Register right, Register tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":28,"deletions":3,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2004, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2004, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2021, Red Hat Inc. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"runtime\/jfieldIDWorkaround.hpp\"\n@@ -112,1 +113,1 @@\n-  __ lsr(roffset, c_rarg2, 2);                \/\/ offset\n+  __ lsr(roffset, c_rarg2, jfieldIDWorkaround::offset_shift);       \/\/ offset\n","filename":"src\/hotspot\/cpu\/aarch64\/jniFastGetField_aarch64.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"ci\/ciInlineKlass.hpp\"\n@@ -55,0 +56,1 @@\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -941,0 +943,35 @@\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  ldr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  ldr(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  ldr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n@@ -1408,0 +1445,1 @@\n+  assert_different_registers(arg_1, c_rarg0);\n@@ -1415,0 +1453,2 @@\n+  assert_different_registers(arg_1, c_rarg0);\n+  assert_different_registers(arg_2, c_rarg0, c_rarg1);\n@@ -1474,1 +1514,9 @@\n-void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {\n+void MacroAssembler::test_markword_is_inline_type(Register markword, Label& is_inline_type) {\n+  assert_different_registers(markword, rscratch2);\n+  andr(markword, markword, markWord::inline_type_mask_in_place);\n+  mov(rscratch2, markWord::inline_type_pattern);\n+  cmp(markword, rscratch2);\n+  br(Assembler::EQ, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n@@ -1477,1 +1525,25 @@\n-  cbnz(temp_reg, is_value);\n+  cbnz(temp_reg, is_inline_type);\n+}\n+\n+void MacroAssembler::test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type) {\n+  cbz(object, not_inline_type);\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  ldr(tmp, Address(object, oopDesc::mark_offset_in_bytes()));\n+  mov(rscratch1, is_inline_type_mask);\n+  andr(tmp, tmp, rscratch1);\n+  cmp(tmp, rscratch1);\n+  br(Assembler::NE, not_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  ldrw(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  andr(temp_reg, temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  cbnz(temp_reg, is_empty_inline_type);\n@@ -1481,2 +1553,2 @@\n-  (void) temp_reg; \/\/ keep signature uniform with x86\n-  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inline_type_shift, is_inline);\n@@ -1486,2 +1558,2 @@\n-  (void) temp_reg; \/\/ keep signature uniform with x86\n-  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_inline_type_shift, not_inline);\n@@ -1491,2 +1563,21 @@\n-  (void) temp_reg; \/\/ keep signature uniform with x86\n-  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);\n+  assert(temp_reg == noreg, \"not needed\"); \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inlined_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label) {\n+  Label test_mark_word;\n+  \/\/ load mark word\n+  ldr(temp_reg, Address(oop, oopDesc::mark_offset_in_bytes()));\n+  \/\/ check displaced\n+  tst(temp_reg, markWord::unlocked_value);\n+  br(Assembler::NE, test_mark_word);\n+  \/\/ slow path use klass prototype\n+  load_prototype_header(temp_reg, oop);\n+\n+  bind(test_mark_word);\n+  andr(temp_reg, temp_reg, test_bit);\n+  if (jmp_set) {\n+    cbnz(temp_reg, jmp_label);\n+  } else {\n+    cbz(temp_reg, jmp_label);\n+  }\n@@ -1496,3 +1587,6 @@\n-  load_storage_props(temp_reg, oop);\n-  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);\n-  cbnz(temp_reg, is_flattened_array);\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, true, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::flat_array_bit_in_place, false, is_non_flattened_array);\n@@ -1502,3 +1596,25 @@\n-  load_storage_props(temp_reg, oop);\n-  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);\n-  cbnz(temp_reg, is_null_free_array);\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, true, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  test_oop_prototype_bit(oop, temp_reg, markWord::nullfree_array_bit_in_place, false, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  tst(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  br(Assembler::NE, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  tst(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  br(Assembler::EQ, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_bit_inplace);\n+  br(Assembler::NE, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  tst(lh, Klass::_lh_null_free_bit_inplace);\n+  br(Assembler::EQ, is_non_null_free_array);\n@@ -3829,1 +3945,0 @@\n-  load_metadata(dst, src);\n@@ -3831,1 +3946,1 @@\n-    andr(dst, dst, oopDesc::compressed_klass_mask());\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -3834,1 +3949,1 @@\n-    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);\n+    ldr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -3869,9 +3984,0 @@\n-void MacroAssembler::load_storage_props(Register dst, Register src) {\n-  load_metadata(dst, src);\n-  if (UseCompressedClassPointers) {\n-    asrw(dst, dst, oopDesc::narrow_storage_props_shift);\n-  } else {\n-    asr(dst, dst, oopDesc::wide_storage_props_shift);\n-  }\n-}\n-\n@@ -4227,0 +4333,40 @@\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  ldr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  ldrw(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    add(data, data, offset);\n+  } else {\n+    lea(data, Address(oop, offset));\n+  }\n+}\n+\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rscratch1, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  ldrw(rscratch1, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  lsr(rscratch1, rscratch1, Klass::_lh_log2_element_size_shift);\n+  andr(rscratch1, rscratch1, Klass::_lh_log2_element_size_mask);\n+  lslv(index, index, rscratch1);\n+\n+  add(data, array, index);\n+  add(data, data, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE));\n+}\n+\n@@ -4317,0 +4463,119 @@\n+\/\/ Object \/ value buffer allocation...\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == r0, \"needs to be r0, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));\n+    cmpw(rscratch1, InstanceKlass::fully_initialized);\n+    br(Assembler::EQ, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  ldrw(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  tst(layout_size, Klass::_lh_instance_slow_path_bit);\n+  br(Assembler::NE, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+\n+  if (UseTLAB) {\n+    tlab_allocate(new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      b(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      b(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      subs(layout_size, layout_size, sizeof(oopDesc));\n+      br(Assembler::EQ, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      tst(layout_size, 7);\n+      br(Assembler::EQ, L);\n+      stop(\"object size is not multiple of 8 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      lsr(layout_size, layout_size, LogBytesPerLong);\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        Register base = t2;\n+\n+        bind(loop);\n+        add(rscratch1, new_obj, layout_size, Assembler::LSL, LogBytesPerLong);\n+        str(zr, Address(rscratch1, sizeof(oopDesc) - 1*oopSize));\n+        subs(layout_size, layout_size, 1);\n+        br(Assembler::NE, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    ldr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    str(mark_word, Address(new_obj, oopDesc::mark_offset_in_bytes ()));\n+    store_klass_gap(new_obj, zr);  \/\/ zero klass gap for compressed oops\n+    mov(t2, klass);         \/\/ preserve klass\n+    store_klass(new_obj, t2);  \/\/ src klass reg is potentially compressed\n+\n+    b(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  b(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -4428,0 +4693,13 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  ldr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cbnz(inline_klass, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  ldr(inline_klass, Address(inline_klass, index, Address::lsl(3)));\n+}\n+\n@@ -5353,0 +5631,1 @@\n+#ifdef COMPILER2\n@@ -5358,1 +5637,1 @@\n-  const long framesize = C->frame_size_in_bytes();\n+  const long framesize = C->output()->frame_size_in_bytes();\n@@ -5365,2 +5644,2 @@\n-  int bangsize = C->bang_size_in_bytes();\n-  if (C->need_stack_bang(bangsize) && UseStackBanging)\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  if (C->output()->need_stack_bang(bangsize))\n@@ -5371,0 +5650,4 @@\n+  if (C->needs_stack_repair()) {\n+    Unimplemented();\n+  }\n+\n@@ -5375,0 +5658,1 @@\n+#endif \/\/ COMPILER2\n@@ -5404,23 +5688,8 @@\n-     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n-\n-     \/\/ check whether we have space in TLAB,\n-     \/\/ rscratch1 contains pointer to just allocated obj\n-      lea(r14, Address(r13, r14));\n-      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n-\n-      cmp(r14, rscratch1);\n-      br(Assembler::GT, slow_case);\n-\n-      \/\/ OK we have room in TLAB,\n-      \/\/ Set new TLAB top\n-      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n-\n-      \/\/ Set new class always locked\n-      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());\n-      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n-\n-      store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n-      if (vk == NULL) {\n-        \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n-         mov(r0, r1);\n-      }\n+    ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+    lea(r14, Address(r13, r14));\n+    ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n+    cmp(r14, rscratch1);\n+    br(Assembler::GT, slow_case);\n+    str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+    mov(rscratch1, (intptr_t)markWord::inline_type_prototype().value());\n+    str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n@@ -5428,1 +5697,1 @@\n-      store_klass(r13, r1);  \/\/ klass\n+    store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n@@ -5430,9 +5699,8 @@\n-      if (vk != NULL) {\n-        \/\/ FIXME -- do the packing in-line to avoid the runtime call\n-        mov(r0, r13);\n-        far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n-      } else {\n-\n-        \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n-        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n-        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+    if (vk != NULL) {\n+      \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+      mov(r0, r13);\n+      far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+      ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n@@ -5440,5 +5708,5 @@\n-        \/\/ Mov new class to r0 and call pack_handler\n-        mov(r0, r13);\n-        blr(r1);\n-      }\n-      b(skip);\n+      \/\/ Mov new class to r0 and call pack_handler\n+      mov(r0, r13);\n+      blr(r1);\n+    }\n+    b(skip);\n@@ -5453,1 +5721,0 @@\n-\n@@ -5521,3 +5788,3 @@\n-bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n-                                          int& to_index, RegState reg_state[]) {\n-  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                                          VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n+                                          RegState reg_state[]) {\n@@ -5525,0 +5792,9 @@\n+  assert(from->is_valid(), \"source must bevalid\");\n+  Register fromReg;\n+  if (from->is_reg()) {\n+    fromReg = from->as_Register();\n+  } else {\n+    int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+    ldr(r10, Address(sp, st_off));\n+    fromReg = r10;\n+  }\n@@ -5526,2 +5802,1 @@\n-\n-  int vt = 1;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, to, to_count, to_index, -1);\n@@ -5530,9 +5805,16 @@\n-  do {\n-    sig_index--;\n-    BasicType bt = sig->at(sig_index)._bt;\n-    if (bt == T_INLINE_TYPE) {\n-      vt--;\n-    } else if (bt == T_VOID &&\n-               sig->at(sig_index-1)._bt != T_LONG &&\n-               sig->at(sig_index-1)._bt != T_DOUBLE) {\n-      vt++;\n+  VMReg toReg;\n+  BasicType bt;\n+  while (stream.next(toReg, bt)) {\n+    int off = sig->at(stream.sig_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    Address fromAddr = Address(fromReg, off);\n+\n+    int idx = (int)toReg->value();\n+    if (reg_state[idx] == reg_readonly) {\n+     if (idx != from->value()) {\n+       mark_done = false;\n+     }\n+     done = false;\n+     continue;\n+    } else if (reg_state[idx] == reg_written) {\n+      continue;\n@@ -5540,15 +5822,8 @@\n-      assert(to_index >= 0, \"invalid to_index\");\n-      VMRegPair pair_to = regs_to[to_index--];\n-      VMReg to = pair_to.first();\n-\n-      if (bt == T_VOID) continue;\n-\n-      int idx = (int) to->value();\n-      if (reg_state[idx] == reg_readonly) {\n-         if (idx != from->value()) {\n-           mark_done = false;\n-         }\n-         done = false;\n-         continue;\n-      } else if (reg_state[idx] == reg_written) {\n-        continue;\n+      assert(reg_state[idx] == reg_writable, \"must be writable\");\n+      reg_state[idx] = reg_written;\n+    }\n+\n+    if (!toReg->is_FloatRegister()) {\n+      Register dst = toReg->is_stack() ? r13 : toReg->as_Register();\n+      if (is_reference_type(bt)) {\n+        load_heap_oop(dst, fromAddr);\n@@ -5556,2 +5831,2 @@\n-        assert(reg_state[idx] == reg_writable, \"must be writable\");\n-        reg_state[idx] = reg_written;\n+        bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+        load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n@@ -5559,5 +5834,3 @@\n-\n-      if (fromReg == noreg) {\n-        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n-        ldr(rscratch2, Address(sp, st_off));\n-        fromReg = rscratch2;\n+      if (toReg->is_stack()) {\n+        int st_off = toReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        str(dst, Address(sp, st_off));\n@@ -5565,30 +5838,5 @@\n-\n-      int off = sig->at(sig_index)._offset;\n-      assert(off > 0, \"offset in object should be positive\");\n-      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n-\n-      Address fromAddr = Address(fromReg, off);\n-      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n-\n-      if (!to->is_FloatRegister()) {\n-\n-        Register dst = to->is_stack() ? rscratch1 : to->as_Register();\n-\n-        if (is_oop) {\n-          load_heap_oop(dst, fromAddr);\n-        } else {\n-          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n-        }\n-        if (to->is_stack()) {\n-          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n-          str(dst, Address(sp, st_off));\n-        }\n-      } else {\n-        if (bt == T_DOUBLE) {\n-          ldrd(to->as_FloatRegister(), fromAddr);\n-        } else {\n-          assert(bt == T_FLOAT, \"must be float\");\n-          ldrs(to->as_FloatRegister(), fromAddr);\n-        }\n-     }\n-\n+    } else if (bt == T_DOUBLE) {\n+      ldrd(toReg->as_FloatRegister(), fromAddr);\n+    } else {\n+      assert(bt == T_FLOAT, \"must be float\");\n+      ldrs(toReg->as_FloatRegister(), fromAddr);\n@@ -5596,2 +5844,3 @@\n-\n-  } while (vt != 0);\n+  }\n+  sig_index = stream.sig_index();\n+  to_index = stream.regs_index();\n@@ -5603,0 +5852,1 @@\n+  from_index--;\n@@ -5608,1 +5858,2 @@\n-                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[]) {\n+                                        VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                                        RegState reg_state[]) {\n@@ -5610,1 +5861,1 @@\n-  assert(to->is_valid(), \"must be\");\n+  assert(to->is_valid(), \"destination must be valid\");\n@@ -5613,1 +5864,1 @@\n-    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n@@ -5626,2 +5877,2 @@\n-    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n-      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, from, from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, from, from_count, from_index);\n@@ -5636,2 +5887,2 @@\n-  ScalarizedInlineArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n-  VMRegPair from_pair;\n+  ScalarizedInlineArgsStream stream(sig, sig_index, from, from_count, from_index);\n+  VMReg fromReg;\n@@ -5639,2 +5890,1 @@\n-\n-  while (stream.next(from_pair, bt)) {\n+  while (stream.next(fromReg, bt)) {\n@@ -5643,1 +5893,0 @@\n-    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n@@ -5646,3 +5895,0 @@\n-    VMReg from_r1 = from_pair.first();\n-    VMReg from_r2 = from_pair.second();\n-\n@@ -5652,6 +5898,6 @@\n-    if (!from_r1->is_FloatRegister()) {\n-      Register from_reg;\n-      if (from_r1->is_stack()) {\n-        from_reg = from_reg_tmp;\n-        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n-        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+    if (!fromReg->is_FloatRegister()) {\n+      Register src;\n+      if (fromReg->is_stack()) {\n+        src = from_reg_tmp;\n+        int ld_off = fromReg->reg2stack() * VMRegImpl::stack_slot_size + wordSize;\n+        load_sized_value(src, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n@@ -5659,1 +5905,1 @@\n-        from_reg = from_r1->as_Register();\n+        src = fromReg->as_Register();\n@@ -5661,4 +5907,3 @@\n-\n-      if (is_oop) {\n-        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;\n-        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);\n+      assert_different_registers(dst.base(), src, tmp1, tmp2, tmp3, val_array);\n+      if (is_reference_type(bt)) {\n+        store_heap_oop(dst, src, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n@@ -5666,1 +5911,1 @@\n-        store_sized_value(dst, from_reg, size_in_bytes);\n+        store_sized_value(dst, src, size_in_bytes);\n@@ -5668,0 +5913,2 @@\n+    } else if (bt == T_DOUBLE) {\n+      strd(fromReg->as_FloatRegister(), dst);\n@@ -5669,5 +5916,2 @@\n-      if (from_r2->is_valid()) {\n-        strd(from_r1->as_FloatRegister(), dst);\n-      } else {\n-        strs(from_r1->as_FloatRegister(), dst);\n-      }\n+      assert(bt == T_FLOAT, \"must be float\");\n+      strs(fromReg->as_FloatRegister(), dst);\n@@ -5675,2 +5919,1 @@\n-\n-    reg_state[from_r1->value()] = reg_writable;\n+    reg_state[fromReg->value()] = reg_writable;\n@@ -5692,0 +5935,9 @@\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    Unimplemented();\n+  } else {\n+    remove_frame(initial_framesize);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":420,"deletions":168,"binary":false,"changes":588,"status":"modified"},{"patch":"@@ -619,1 +619,13 @@\n-  void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);\n+  \/\/ markWord tests, kills markWord reg\n+  void test_markword_is_inline_type(Register markword, Label& is_inline_type);\n+\n+  \/\/ inlineKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+  void test_oop_is_not_inline_type(Register object, Register tmp, Label& not_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n@@ -625,1 +637,2 @@\n-  \/\/ Check klass\/oops is flat inline type array (oop->_klass->_layout_helper & vt_bit)\n+  \/\/ Check oops for special arrays, i.e. flattened and\/or null-free\n+  void test_oop_prototype_bit(Register oop, Register temp_reg, int32_t test_bit, bool jmp_set, Label& jmp_label);\n@@ -627,0 +640,1 @@\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n@@ -628,0 +642,7 @@\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n@@ -837,1 +858,0 @@\n-  void load_storage_props(Register dst, Register src);\n@@ -853,0 +873,9 @@\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -912,0 +941,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -930,0 +968,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -1224,1 +1265,2 @@\n-  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index,\n+                            VMReg from, int& from_index, VMRegPair* to, int to_count, int& to_index,\n@@ -1227,2 +1269,3 @@\n-                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[]);\n-  void restore_stack(Compile* C);\n+                          VMRegPair* from, int from_count, int& from_index, VMReg to,\n+                          RegState reg_state[]);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":49,"deletions":6,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -611,1 +611,2 @@\n-      OopMap* map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words);\n+      RegisterSaver reg_save(false \/* save_vectors *\/);\n+      OopMap* map = reg_save.save_live_registers(masm, 0, &frame_size_in_words);\n@@ -628,1 +629,1 @@\n-      RegisterSaver::restore_live_registers(masm);\n+      reg_save.restore_live_registers(masm);\n@@ -990,1 +991,0 @@\n-    Label unused;\n@@ -992,1 +992,1 @@\n-    skip_fixup = unused;\n+    skip_fixup.reset();\n@@ -1024,1 +1024,1 @@\n-  gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);\n+  gen_c2i_adapter(masm, sig_cc, regs_cc, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, true);\n@@ -1028,1 +1028,1 @@\n- \/\/ Non-scalarized c2i adapter\n+  \/\/ Non-scalarized c2i adapter\n@@ -1036,1 +1036,0 @@\n-    Label unused;\n@@ -2029,0 +2028,5 @@\n+    if (EnableValhalla) {\n+      assert(!UseBiasedLocking, \"Not compatible with biased-locking\");\n+      \/\/ Mask inline_type bit such that we go to the slow path if object is an inline type\n+      __ andr(swap_reg, swap_reg, ~((int) markWord::inline_type_bit_in_place));\n+    }\n@@ -2094,1 +2098,1 @@\n-  case T_INLINE_TYPE:\n+  case T_INLINE_TYPE:           \/\/ Really a handle\n@@ -3344,0 +3348,1 @@\n+#endif \/\/ COMPILER2\n@@ -3358,0 +3363,9 @@\n+  int pack_fields_jobject_off = __ offset();\n+  \/\/ Resolve pre-allocated buffer from JNI handle.\n+  \/\/ We cannot do this in generate_call_stub() because it requires GC code to be initialized.\n+  __ ldr(r0, Address(r13, 0));\n+  __ resolve_jobject(r0 \/* value *\/,\n+                     rthread \/* thread *\/,\n+                     r12 \/* tmp *\/);\n+  __ str(r0, Address(r13, 0));\n+\n@@ -3447,1 +3461,1 @@\n-  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, unpack_fields_off);\n+  return BufferedInlineTypeBlob::create(&buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);\n@@ -3701,1 +3715,0 @@\n-#endif \/\/ COMPILER2\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":23,"deletions":10,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -2107,0 +2107,8 @@\n+    \/\/ Check for flat inline type array -> return -1\n+    __ tst(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+    __ br(Assembler::NE, L_failed);\n+\n+    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+    __ tst(lh, Klass::_lh_null_free_bit_inplace);\n+    __ br(Assembler::NE, L_failed);\n+\n@@ -6821,1 +6829,0 @@\n-    __ maybe_isb();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -331,0 +331,1 @@\n+  __ andr(r3, r3, ~JVM_CONSTANT_QDescBit);\n@@ -811,0 +812,1 @@\n+  __ profile_array(r2, r0, r4);\n@@ -826,0 +828,1 @@\n+  __ profile_element(r2, r0, r4);\n@@ -1112,1 +1115,1 @@\n-  Label is_null, ok_is_subtype, done;\n+  Label is_null, is_flat_array, ok_is_subtype, done;\n@@ -1119,2 +1122,0 @@\n-  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n-\n@@ -1123,1 +1124,3 @@\n-  \/\/ FIXME: Could we remove the line below?\n+  __ profile_array(r4, r3, r5);\n+  __ profile_element(r4, r0, r5);\n+\n@@ -1125,0 +1128,2 @@\n+  Address element_address(r3, r4, Address::uxtw(LogBytesPerHeapOop));\n+  \/\/ Be careful not to clobber r4 below\n@@ -1129,1 +1134,3 @@\n-  Label  is_flat_array;\n+  \/\/ Move array class to r5\n+  __ load_klass(r5, r3);\n+\n@@ -1131,1 +1138,2 @@\n-    __ test_flattened_array_oop(r3, r8 \/*temp*\/, is_flat_array);\n+    __ ldrw(r6, Address(r5, Klass::layout_helper_offset()));\n+    __ test_flattened_array_layout(r6, is_flat_array);\n@@ -1137,3 +1145,2 @@\n-  \/\/ Move superklass into r0\n-  __ load_klass(r0, r3);\n-  __ ldr(r0, Address(r0, ObjArrayKlass::element_klass_offset()));\n+  \/\/ Move array element superklass into r0\n+  __ ldr(r0, Address(r5, ObjArrayKlass::element_klass_offset()));\n@@ -1145,1 +1152,2 @@\n-  __ gen_subtype_check(r1, ok_is_subtype);\n+  \/\/ is \"r1 <: r0\" ? (value subclass <: array element superclass)\n+  __ gen_subtype_check(r1, ok_is_subtype, false);\n@@ -1151,1 +1159,0 @@\n-\n@@ -1155,1 +1162,0 @@\n-\n@@ -1164,2 +1170,0 @@\n-  __ profile_null_seen(r2);\n-\n@@ -1169,1 +1173,1 @@\n-    \/\/ No way to store null in flat array\n+    \/\/ No way to store null in flat null-free array\n@@ -1185,3 +1189,1 @@\n-\n-    \/\/ store non-null value\n-    __ bind(is_flat_array);\n+    __ bind(is_flat_array); \/\/ Store non-null value to flat\n@@ -1195,1 +1197,0 @@\n-     __ profile_typecheck(r2, r1, r0); \/\/ blows r2, and r0\n@@ -1197,2 +1198,2 @@\n-    \/\/ flat value array needs exact type match\n-    \/\/ is \"r8 == r0\" (value subclass == array element superclass)\n+    \/\/ Move element klass into r7\n+     __ ldr(r7, Address(r5, ArrayKlass::element_klass_offset()));\n@@ -1200,3 +1201,2 @@\n-    \/\/ Move element klass into r0\n-\n-     __ load_klass(r0, r3);\n+    \/\/ flat value array needs exact type match\n+    \/\/ is \"r1 == r7\" (value subclass == array element superclass)\n@@ -1204,2 +1204,1 @@\n-     __ ldr(r0, Address(r0, ArrayKlass::element_klass_offset()));\n-     __ cmp(r0, r1);\n+     __ cmp(r7, r1);\n@@ -1208,1 +1207,0 @@\n-     __ profile_typecheck_failed(r2);\n@@ -1212,0 +1210,4 @@\n+    \/\/ r1: value's klass\n+    \/\/ r3: array\n+    \/\/ r5: array klass\n+    __ test_klass_is_empty_inline_type(r1, r7, done);\n@@ -1213,7 +1215,3 @@\n-    \/\/ Reload from TOS to be safe, because of profile_typecheck that blows r2 and r0.\n-    \/\/ FIXME: Should we really do it?\n-     __ ldr(r1, at_tos());  \/\/ value\n-     __ mov(r2, r3); \/\/ array, ldr(r2, at_tos_p2());\n-     __ ldr(r3, at_tos_p1()); \/\/ index\n-     __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_store), r1, r2, r3);\n-  }\n+    \/\/ calc dst for copy\n+    __ ldrw(r7, at_tos_p1()); \/\/ index\n+    __ data_for_value_array_index(r3, r5, r7, r7);\n@@ -1221,0 +1219,7 @@\n+    \/\/ ...and src for copy\n+    __ ldr(r6, at_tos());  \/\/ value\n+    __ data_for_oop(r6, r6, r1);\n+\n+    __ mov(r4, r1);  \/\/ Shuffle arguments to avoid conflict with c_rarg1\n+    __ access_value_copy(IN_HEAP, r6, r7, r4);\n+  }\n@@ -2037,2 +2042,4 @@\n-  Register is_value_mask = rscratch1;\n-  __ mov(is_value_mask, markWord::always_locked_pattern);\n+  __ profile_acmp(r2, r1, r0, r4);\n+\n+  Register is_inline_type_mask = rscratch1;\n+  __ mov(is_inline_type_mask, markWord::inline_type_pattern);\n@@ -2050,1 +2057,1 @@\n-    __ andr(r2, r2, is_value_mask);\n+    __ andr(r2, r2, is_inline_type_mask);\n@@ -2052,1 +2059,1 @@\n-    __ andr(r4, r4, is_value_mask);\n+    __ andr(r4, r4, is_inline_type_mask);\n@@ -2054,1 +2061,1 @@\n-    __ cmp(r2,  is_value_mask);\n+    __ cmp(r2,  is_inline_type_mask);\n@@ -2077,1 +2084,1 @@\n-  __ profile_not_taken_branch(r0);\n+  __ profile_not_taken_branch(r0, true);\n@@ -2427,1 +2434,1 @@\n-                                          ConstantPoolCacheEntry::f2_offset())));\n+                                      ConstantPoolCacheEntry::f2_offset())));\n@@ -2430,1 +2437,1 @@\n-                                           ConstantPoolCacheEntry::flags_offset())));\n+                                         ConstantPoolCacheEntry::flags_offset())));\n@@ -2528,0 +2535,2 @@\n+  const Register klass = r5;\n+  const Register inline_klass = r7;\n@@ -2560,0 +2569,5 @@\n+  if (!is_static) {\n+    __ ldr(klass, Address(cache, in_bytes(ConstantPoolCache::base_offset() +\n+                                          ConstantPoolCacheEntry::f1_offset())));\n+  }\n+\n@@ -2605,1 +2619,0 @@\n-\n@@ -2608,1 +2621,1 @@\n-      Label is_inline, isUninitialized;\n+      Label is_inline_type, uninitialized;\n@@ -2610,2 +2623,2 @@\n-      __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n-        \/\/ Not inline case\n+      __ test_field_is_inline_type(raw_flags, noreg \/*temp*\/, is_inline_type);\n+        \/\/ field is not an inline type\n@@ -2614,3 +2627,3 @@\n-      \/\/ Inline case, must not return null even if uninitialized\n-      __ bind(is_inline);\n-        __ cbz(r0, isUninitialized);\n+      \/\/ field is an inline type, must not return null even if uninitialized\n+      __ bind(is_inline_type);\n+        __ cbz(r0, uninitialized);\n@@ -2619,1 +2632,1 @@\n-        __ bind(isUninitialized);\n+        __ bind(uninitialized);\n@@ -2621,0 +2634,7 @@\n+          Label slow_case, finish;\n+          __ ldrb(rscratch1, Address(cache, InstanceKlass::init_state_offset()));\n+          __ cmp(rscratch1, (u1)InstanceKlass::fully_initialized);\n+          __ br(Assembler::NE, slow_case);\n+          __ get_default_value_oop(klass, off \/* temp *\/, r0);\n+        __ b(finish);\n+        __ bind(slow_case);\n@@ -2622,0 +2642,1 @@\n+          __ bind(finish);\n@@ -2626,2 +2647,2 @@\n-      Label isFlattened, isInitialized, is_inline, rewrite_inline;\n-        __ test_field_is_inline_type(raw_flags, r8 \/*temp*\/, is_inline);\n+      Label is_inlined, nonnull, is_inline_type, rewrite_inline;\n+      __ test_field_is_inline_type(raw_flags, noreg \/*temp*\/, is_inline_type);\n@@ -2635,3 +2656,3 @@\n-      __ bind(is_inline);\n-        __ test_field_is_inlined(raw_flags, r8 \/* temp *\/, isFlattened);\n-         \/\/ Non-inline field case\n+      __ bind(is_inline_type);\n+        __ test_field_is_inlined(raw_flags, noreg \/* temp *\/, is_inlined);\n+         \/\/ field is not inlined\n@@ -2639,1 +2660,1 @@\n-          __ cbnz(r0, isInitialized);\n+          __ cbnz(r0, nonnull);\n@@ -2641,2 +2662,3 @@\n-            __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), obj, raw_flags);\n-          __ bind(isInitialized);\n+            __ get_inline_type_field_klass(klass, raw_flags, inline_klass);\n+            __ get_default_value_oop(inline_klass, klass \/* temp *\/, r0);\n+          __ bind(nonnull);\n@@ -2646,2 +2668,2 @@\n-        __ bind(isFlattened);\n-          __ ldr(r10, Address(cache, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n@@ -2649,1 +2671,2 @@\n-          call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), obj, raw_flags, r10);\n+          __ mov(r0, obj);\n+          __ read_inlined_field(klass, raw_flags, off, inline_klass \/* temp *\/, r0);\n@@ -2654,1 +2677,1 @@\n-         patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);\n+        patch_bytecode(Bytecodes::_fast_qgetfield, bc, r1);\n@@ -2830,0 +2853,1 @@\n+  const Register inline_klass = r5;\n@@ -2905,1 +2929,0 @@\n-\n@@ -2908,2 +2931,2 @@\n-        Label not_inline;\n-         __ test_field_is_not_inline_type(flags2, r8 \/* temp *\/, not_inline);\n+        Label is_inline_type;\n+         __ test_field_is_not_inline_type(flags2, noreg \/* temp *\/, is_inline_type);\n@@ -2911,1 +2934,1 @@\n-         __ bind(not_inline);\n+         __ bind(is_inline_type);\n@@ -2915,3 +2938,3 @@\n-        Label is_inline, isFlattened, rewrite_not_inline, rewrite_inline;\n-        __ test_field_is_inline_type(flags2, r8 \/*temp*\/, is_inline);\n-        \/\/ Not inline case\n+        Label is_inline_type, is_inlined, rewrite_not_inline, rewrite_inline;\n+        __ test_field_is_inline_type(flags2, noreg \/*temp*\/, is_inline_type);\n+        \/\/ Not an inline type\n@@ -2926,2 +2949,2 @@\n-        \/\/ Implementation of the inline semantic\n-        __ bind(is_inline);\n+        \/\/ Implementation of the inline type semantic\n+        __ bind(is_inline_type);\n@@ -2929,2 +2952,2 @@\n-        __ test_field_is_inlined(flags2, r8 \/*temp*\/, isFlattened);\n-        \/\/ Not inline case\n+        __ test_field_is_inlined(flags2, noreg \/*temp*\/, is_inlined);\n+        \/\/ field is not inlined\n@@ -2935,1 +2958,2 @@\n-        __ bind(isFlattened);\n+        __ bind(is_inlined);\n+        \/\/ field is inlined\n@@ -2937,1 +2961,5 @@\n-        call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, off, obj);\n+        assert_different_registers(r0, inline_klass, obj, off);\n+        __ load_klass(inline_klass, r0);\n+        __ data_for_oop(r0, r0, inline_klass);\n+        __ add(obj, obj, off);\n+        __ access_value_copy(IN_HEAP, r0, obj, inline_klass);\n@@ -3167,1 +3195,1 @@\n-      Label isFlattened, done;\n+      Label is_inlined, done;\n@@ -3169,2 +3197,2 @@\n-      __ test_field_is_flattened(r3, r8 \/* temp *\/, isFlattened);\n-      \/\/ No Flattened case\n+      __ test_field_is_inlined(r3, noreg \/* temp *\/, is_inlined);\n+      \/\/ field is not inlined\n@@ -3173,2 +3201,6 @@\n-      __ bind(isFlattened);\n-      call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::write_flattened_value), r0, r1, r2);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+      __ load_klass(r4, r0);\n+      __ data_for_oop(r0, r0, r4);\n+      __ lea(rscratch1, field);\n+      __ access_value_copy(IN_HEAP, r0, rscratch1, r4);\n@@ -3277,6 +3309,4 @@\n-       Label isFlattened, isInitialized, Done;\n-       \/\/ FIXME: We don't need to reload registers multiple times, but stay close to x86 code\n-       __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n-       __ test_field_is_inlined(r9, r8 \/* temp *\/, isFlattened);\n-        \/\/ Non-flattened field case\n-        __ mov(r9, r0);\n+      Register index = r4, klass = r5, inline_klass = r6;\n+      Label is_inlined, nonnull, Done;\n+      __ test_field_is_inlined(r3, noreg \/* temp *\/, is_inlined);\n+        \/\/ field is not inlined\n@@ -3284,6 +3314,7 @@\n-        __ cbnz(r0, isInitialized);\n-          __ mov(r0, r9);\n-          __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n-          __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n-          __ call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::uninitialized_instance_inline_type_field), r0, r9);\n-        __ bind(isInitialized);\n+        __ cbnz(r0, nonnull);\n+          __ andw(index, r3, ConstantPoolCacheEntry::field_index_mask);\n+          __ ldr(klass, Address(r2, in_bytes(ConstantPoolCache::base_offset() +\n+                                             ConstantPoolCacheEntry::f1_offset())));\n+          __ get_inline_type_field_klass(klass, index, inline_klass);\n+          __ get_default_value_oop(inline_klass, rscratch1 \/* temp *\/, r0);\n+        __ bind(nonnull);\n@@ -3292,5 +3323,6 @@\n-      __ bind(isFlattened);\n-        __ ldrw(r9, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::flags_offset())));\n-        __ andw(r9, r9, ConstantPoolCacheEntry::field_index_mask);\n-        __ ldr(r3, Address(r2, in_bytes(ConstantPoolCache::base_offset() + ConstantPoolCacheEntry::f1_offset())));\n-        call_VM(r0, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_flattened_field), r0, r9, r3);\n+      __ bind(is_inlined);\n+      \/\/ field is inlined\n+        __ andw(index, r3, ConstantPoolCacheEntry::field_index_mask);\n+        __ ldr(klass, Address(r2, in_bytes(ConstantPoolCache::base_offset() +\n+                                           ConstantPoolCacheEntry::f1_offset())));\n+        __ read_inlined_field(klass, index, r1, inline_klass \/* temp *\/, r0);\n@@ -3731,0 +3763,1 @@\n+  Label is_not_value;\n@@ -3748,0 +3781,8 @@\n+  __ ldrb(rscratch1, Address(r4, InstanceKlass::kind_offset()));\n+  __ cmp(rscratch1, (u1)InstanceKlass::_kind_inline_type);\n+  __ br(Assembler::NE, is_not_value);\n+\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address, InterpreterRuntime::throw_InstantiationError));\n+\n+  __ bind(is_not_value);\n+\n@@ -3754,83 +3795,2 @@\n-  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n-  __ ldrw(r3,\n-          Address(r4,\n-                  Klass::layout_helper_offset()));\n-  \/\/ test to see if it has a finalizer or is malformed in some way\n-  __ tbnz(r3, exact_log2(Klass::_lh_instance_slow_path_bit), slow_case);\n-\n-  \/\/ Allocate the instance:\n-  \/\/  If TLAB is enabled:\n-  \/\/    Try to allocate in the TLAB.\n-  \/\/    If fails, go to the slow path.\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n-  \/\/    Initialize the allocation.\n-  \/\/    Exit.\n-  \/\/\n-  \/\/  Go to slow path.\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n-\n-  if (UseTLAB) {\n-    __ tlab_allocate(r0, r3, 0, noreg, r1, slow_case);\n-\n-    if (ZeroTLAB) {\n-      \/\/ the fields have been already cleared\n-      __ b(initialize_header);\n-    } else {\n-      \/\/ initialize both the header and fields\n-      __ b(initialize_object);\n-    }\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    \/\/ r3: instance size in bytes\n-    if (allow_shared_alloc) {\n-      __ eden_allocate(r0, r3, 0, r10, slow_case);\n-    }\n-  }\n-\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n-    \/\/ The object is initialized before the header.  If the object size is\n-    \/\/ zero, go directly to the header initialization.\n-    __ bind(initialize_object);\n-    __ sub(r3, r3, sizeof(oopDesc));\n-    __ cbz(r3, initialize_header);\n-\n-    \/\/ Initialize object fields\n-    {\n-      __ add(r2, r0, sizeof(oopDesc));\n-      Label loop;\n-      __ bind(loop);\n-      __ str(zr, Address(__ post(r2, BytesPerLong)));\n-      __ sub(r3, r3, BytesPerLong);\n-      __ cbnz(r3, loop);\n-    }\n-\n-    \/\/ initialize object header only.\n-    __ bind(initialize_header);\n-    if (UseBiasedLocking) {\n-      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n-    } else {\n-      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    }\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n-    {\n-      SkipIfEqual skip(_masm, &DTraceAllocProbes, false);\n-      \/\/ Trigger dtrace event for fastpath\n-      __ push(atos); \/\/ save the return value\n-      __ call_VM_leaf(\n-           CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), r0);\n-      __ pop(atos); \/\/ restore the return value\n-\n-    }\n-    __ b(done);\n-  }\n+  __ allocate_instance(r4, r0, r3, r1, true, slow_case);\n+  __ b(done);\n@@ -3915,0 +3875,1 @@\n+  __ andr(r1, r1, ~JVM_CONSTANT_QDescBit);\n@@ -3984,0 +3945,1 @@\n+  __ andr(r1, r1, ~JVM_CONSTANT_QDescBit);\n@@ -4089,0 +4051,4 @@\n+  Label is_inline_type;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ test_markword_is_inline_type(rscratch1, is_inline_type);\n+\n@@ -4178,0 +4144,5 @@\n+\n+  __ bind(is_inline_type);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                    InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n@@ -4190,0 +4161,12 @@\n+  const int is_inline_type_mask = markWord::inline_type_pattern;\n+  Label has_identity;\n+  __ ldr(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+  __ mov(rscratch2, is_inline_type_mask);\n+  __ andr(rscratch1, rscratch1, rscratch2);\n+  __ cmp(rscratch1, rscratch2);\n+  __ br(Assembler::NE, has_identity);\n+  __ call_VM(noreg, CAST_FROM_FN_PTR(address,\n+                     InterpreterRuntime::throw_illegal_monitor_state_exception));\n+  __ should_not_reach_here();\n+  __ bind(has_identity);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":163,"deletions":180,"binary":false,"changes":343,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -142,0 +142,1 @@\n+#ifdef X86\n@@ -145,0 +146,3 @@\n+#else\n+    Unimplemented();\n+#endif\n","filename":"src\/hotspot\/share\/asm\/macroAssembler_common.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1731,1 +1731,1 @@\n-  LIR_Address* elm_address = new LIR_Address(array.result(), index_op, array_header_size, T_ADDRESS);\n+  LIR_Address* elm_address = generate_address(array.result(), index_op, 0, array_header_size, T_ADDRESS);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}