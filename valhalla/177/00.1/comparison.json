{"files":[{"patch":"@@ -492,0 +492,1 @@\n+d2aa5d494481a1039a092d70efa1f5c9826c5b77 lw1_0\n@@ -514,0 +515,1 @@\n+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable\n@@ -570,0 +572,4 @@\n+6132641c6ff61b7b8f3f10b9cd385aafadbd72ef lworld_stable\n+2b098533f1e52d7d541121409b745d9420886945 lworld_stable\n+2b098533f1e52d7d541121409b745d9420886945 lworld_stable\n+7c637fd25e7d6fccdab1098bedd48ed195a86cc7 lworld_stable\n","filename":".hgtags","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1628,0 +1628,2 @@\n+  __ verified_entry(C, 0);\n+  __ bind(*_verified_entry);\n@@ -1974,1 +1976,31 @@\n-\/\/=============================================================================\n+\/\/\/=============================================================================\n+#ifndef PRODUCT\n+void MachVEPNode::format(PhaseRegAlloc* ra_, outputStream* st) const\n+{\n+  st->print_cr(\"# MachVEPNode\");\n+  if (!_verified) {\n+    st->print_cr(\"\\t load_class\");\n+  } else {\n+    st->print_cr(\"\\t unpack_inline_arg\");\n+  }\n+}\n+#endif\n+\n+void MachVEPNode::emit(CodeBuffer& cbuf, PhaseRegAlloc* ra_) const\n+{\n+  MacroAssembler _masm(&cbuf);\n+\n+  if (!_verified) {\n+    Label skip;\n+    __ cmp_klass(j_rarg0, rscratch2, rscratch1);\n+    __ br(Assembler::EQ, skip);\n+      __ far_jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));\n+    __ bind(skip);\n+\n+  } else {\n+    \/\/ Unpack inline type args passed as oop and then jump to\n+    \/\/ the verified entry point (skipping the unverified entry).\n+    __ unpack_inline_args(ra_->C, _receiver_only);\n+    __ b(*_verified_entry);\n+  }\n+}\n@@ -1976,0 +2008,8 @@\n+\n+uint MachVEPNode::size(PhaseRegAlloc* ra_) const\n+{\n+  return MachNode::size(ra_); \/\/ too many variables; just compute it the hard way\n+}\n+\n+\n+\/\/=============================================================================\n@@ -1997,0 +2037,1 @@\n+  Label skip;\n@@ -1998,0 +2039,1 @@\n+  \/\/ UseCompressedClassPointers logic are inside cmp_klass\n@@ -1999,1 +2041,1 @@\n-  Label skip;\n+\n@@ -2406,1 +2448,0 @@\n-\n@@ -8285,0 +8326,15 @@\n+instruct castN2X(iRegLNoSp dst, iRegN src) %{\n+  match(Set dst (CastP2X src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"mov $dst, $src\\t# ptr -> long\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ mov(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -8300,0 +8356,31 @@\n+instruct castN2I(iRegINoSp dst, iRegN src) %{\n+  match(Set dst (CastN2I src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"movw $dst, $src\\t# compressed ptr -> int\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct castI2N(iRegNNoSp dst, iRegI src) %{\n+  match(Set dst (CastI2N src));\n+\n+  ins_cost(INSN_COST);\n+  format %{ \"movw $dst, $src\\t# int -> compressed ptr\" %}\n+\n+  ins_encode %{\n+    if ($dst$$reg != $src$$reg) {\n+      __ movw(as_Register($dst$$reg), as_Register($src$$reg));\n+    }\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\n@@ -14213,1 +14300,1 @@\n-instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, Universe dummy, rFlagsReg cr)\n+instruct clearArray_reg_reg(iRegL_R11 cnt, iRegP_R10 base, iRegL val, Universe dummy, rFlagsReg cr)\n@@ -14215,1 +14302,1 @@\n-  match(Set dummy (ClearArray cnt base));\n+  match(Set dummy (ClearArray (Binary cnt base) val));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":92,"deletions":5,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -94,0 +96,6 @@\n+\n+  if (EnableValhalla && !UseBiasedLocking) {\n+    \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+    andr(hdr, hdr, ~markWord::biased_lock_bit_in_place);\n+  }\n+\n@@ -341,1 +349,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, bool needs_stack_repair, Label* verified_inline_entry_label) {\n@@ -346,0 +354,6 @@\n+\n+  guarantee(needs_stack_repair == false, \"Stack repair should not be true\");\n+  if (verified_inline_entry_label != NULL) {\n+    bind(*verified_inline_entry_label);\n+  }\n+\n@@ -353,1 +367,4 @@\n-void C1_MacroAssembler::remove_frame(int framesize) {\n+void C1_MacroAssembler::remove_frame(int framesize, bool needs_stack_repair) {\n+\n+  guarantee(needs_stack_repair == false, \"Stack repair should not be true\");\n+\n@@ -357,0 +374,29 @@\n+void C1_MacroAssembler::verified_inline_entry() {\n+  if (C1Breakpoint || VerifyFPU || !UseStackBanging) {\n+    \/\/ Verified Entry first instruction should be 5 bytes long for correct\n+    \/\/ patching by patch_verified_entry().\n+    \/\/\n+    \/\/ C1Breakpoint and VerifyFPU have one byte first instruction.\n+    \/\/ Also first instruction will be one byte \"push(rbp)\" if stack banging\n+    \/\/ code is not generated (see build_frame() above).\n+    \/\/ For all these cases generate long instruction first.\n+    nop();\n+  }\n+\n+  nop();\n+  \/\/ build frame\n+  \/\/ verify_FPU(0, \"method_entry\");\n+}\n+\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  \/\/ This function required to support for InlineTypePassFieldsAsArgs\n+  if (C1Breakpoint || VerifyFPU || !UseStackBanging) {\n+    \/\/ Verified Entry first instruction should be 5 bytes long for correct\n+    \/\/ patching by patch_verified_entry().\n+    \/\/\n+    \/\/ C1Breakpoint and VerifyFPU have one byte first instruction.\n+    \/\/ Also first instruction will be one byte \"push(rbp)\" if stack banging\n+    \/\/ code is not generated (see build_frame() above).\n+    \/\/ For all these cases generate long instruction first.\n+    nop();\n+  }\n@@ -358,5 +404,64 @@\n-void C1_MacroAssembler::verified_entry() {\n-  \/\/ If we have to make this method not-entrant we'll overwrite its\n-  \/\/ first instruction with a jump.  For this action to be legal we\n-  \/\/ must ensure that this first instruction is a B, BL, NOP, BKPT,\n-  \/\/ SVC, HVC, or SMC.  Make it a NOP.\n+  \/\/ verify_FPU(0, \"method_entry\");\n+\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+\n+  GrowableArray<SigEntry>* sig   = &ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? &ces->sig_cc_ro() : &ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+\n+  int extra_stack_offset = wordSize; \/\/ tos is return address.\n+\n+  \/\/ Create a temp frame so we can call into runtime. It must be properly set up to accomodate GC.\n+  int sp_inc = (args_on_stack - args_on_stack_cc) * VMRegImpl::stack_slot_size;\n+  if (sp_inc > 0) {\n+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+    sub(sp, sp, sp_inc);\n+  } else {\n+    sp_inc = 0;\n+  }\n+\n+  sub(sp, sp, frame_size_in_bytes);\n+  if (sp_inc > 0) {\n+    int real_frame_size = frame_size_in_bytes +\n+           + wordSize  \/\/ pushed rbp\n+           + wordSize  \/\/ returned address pushed by the stack extension code\n+           + sp_inc;   \/\/ stack extension\n+    mov(rscratch1, real_frame_size);\n+    str(rscratch1, Address(sp, frame_size_in_bytes - wordSize));\n+  }\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  mov(r1, (intptr_t) ces->method());\n+  if (is_inline_ro_entry) {\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  add(sp, sp, frame_size_in_bytes);\n+\n+  int n = shuffle_inline_args(true, is_inline_ro_entry, extra_stack_offset, sig_bt, sig_cc,\n+                              args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                              args_passed, args_on_stack, regs);         \/\/ to\n+  assert(sp_inc == n, \"must be\");\n+\n+  if (sp_inc != 0) {\n+    \/\/ Do the stack banging here, and skip over the stack repair code in the\n+    \/\/ verified_inline_entry (which has a different real_frame_size).\n+    assert(sp_inc > 0, \"stack should not shrink\");\n+    generate_stack_overflow_check(bang_size_in_bytes);\n+    decrement(sp, frame_size_in_bytes);\n+  }\n+\n+  b(verified_inline_entry_label);\n+  return rt_call_offset;\n@@ -366,0 +471,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":113,"deletions":7,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -670,0 +671,1 @@\n+\n@@ -686,0 +688,27 @@\n+\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    \/\/ Test if the return type is an inline type\n+    ldr(rscratch1, Address(rfp, frame::interpreter_frame_method_offset * wordSize));\n+    ldr(rscratch1, Address(rscratch1, Method::const_offset()));\n+    ldrb(rscratch1, Address(rscratch1, ConstMethod::result_type_offset()));\n+    cmpw(rscratch1, (u1) T_INLINE_TYPE);\n+    br(Assembler::NE, skip);\n+\n+    \/\/ We are returning an inline type, load its fields into registers\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+\n+    load_klass(rscratch1 \/*dst*\/, r0 \/*src*\/);\n+    ldr(rscratch1, Address(rscratch1, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    ldr(rscratch1, Address(rscratch1, InlineKlass::unpack_handler_offset()));\n+    cbz(rscratch1, skip);\n+\n+    blr(rscratch1);\n+\n+    \/\/ call above kills the value in r1. Reload it.\n+    ldr(r1, Address(rfp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n+\n+\n@@ -746,0 +775,5 @@\n+    if (EnableValhalla && !UseBiasedLocking) {\n+      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n+      andr(swap_reg, swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n+    }\n+\n@@ -1711,1 +1745,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1757,1 +1791,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -1313,1 +1314,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1343,1 +1348,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -1436,0 +1445,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -1485,0 +1498,33 @@\n+void MacroAssembler::test_klass_is_value(Register klass, Register temp_reg, Label& is_value) {\n+  ldrw(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  andr(temp_reg, temp_reg, JVM_ACC_INLINE);\n+  cbnz(temp_reg, is_value);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_inline_field_shift, is_inline);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbz(flags, ConstantPoolCacheEntry::is_inline_field_shift, not_inline);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened) {\n+  (void) temp_reg; \/\/ keep signature uniform with x86\n+  tbnz(flags, ConstantPoolCacheEntry::is_flattened_field_shift, is_flattened);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg, Label& is_flattened_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::flattened_value);\n+  cbnz(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array) {\n+  load_storage_props(temp_reg, oop);\n+  andr(temp_reg, temp_reg, ArrayStorageProperties::null_free_value);\n+  cbnz(temp_reg, is_null_free_array);\n+}\n+\n@@ -3716,1 +3762,1 @@\n-void MacroAssembler::load_klass(Register dst, Register src) {\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n@@ -3719,1 +3765,0 @@\n-    decode_klass_not_null(dst);\n@@ -3725,0 +3770,10 @@\n+void MacroAssembler::load_klass(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    andr(dst, dst, oopDesc::compressed_klass_mask());\n+    decode_klass_not_null(dst);\n+  } else {\n+    ubfm(dst, dst, 0, 63 - oopDesc::storage_props_nof_bits);\n+  }\n+}\n+\n@@ -3756,0 +3811,9 @@\n+void MacroAssembler::load_storage_props(Register dst, Register src) {\n+  load_metadata(dst, src);\n+  if (UseCompressedClassPointers) {\n+    asrw(dst, dst, oopDesc::narrow_storage_props_shift);\n+  } else {\n+    asr(dst, dst, oopDesc::wide_storage_props_shift);\n+  }\n+}\n+\n@@ -4093,1 +4157,2 @@\n-                                     Register tmp1, Register thread_tmp) {\n+                                     Register tmp1, Register thread_tmp, Register tmp3) {\n+\n@@ -4098,1 +4163,1 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4100,1 +4165,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp);\n+    bs->store_at(this, decorators, type, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4124,2 +4189,2 @@\n-                                    Register thread_tmp, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp);\n+                                    Register thread_tmp, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, thread_tmp, tmp3);\n@@ -4130,1 +4195,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -5203,0 +5268,389 @@\n+\/\/ C2 compiled method's prolog code\n+\/\/ Moved here from aarch64.ad to support Valhalla code belows\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+\n+\/\/ n.b. frame size includes space for return pc and rfp\n+  const long framesize = C->frame_size_in_bytes();\n+  assert(framesize % (2 * wordSize) == 0, \"must preserve 2 * wordSize alignment\");\n+\n+  \/\/ insert a nop at the start of the prolog so we can patch in a\n+  \/\/ branch if we need to invalidate the method later\n+  nop();\n+\n+  int bangsize = C->bang_size_in_bytes();\n+  if (C->need_stack_bang(bangsize) && UseStackBanging)\n+     generate_stack_overflow_check(bangsize);\n+\n+  build_frame(framesize);\n+\n+  if (VerifyStackAtCalls) {\n+    Unimplemented();\n+  }\n+}\n+\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  cmp(r0, (u1) 1);\n+  br(Assembler::EQ, skip);\n+  int call_offset = -1;\n+\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      mov(r1, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      mov(r14, lh);\n+    } else {\n+       \/\/ Call from interpreter. R0 contains ((the InlineKlass* of the return type) | 0x01)\n+       andr(r1, r0, -2);\n+       \/\/ get obj size\n+       ldrw(r14, Address(rscratch1 \/*klass*\/, Klass::layout_helper_offset()));\n+    }\n+\n+     ldr(r13, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+     \/\/ check whether we have space in TLAB,\n+     \/\/ rscratch1 contains pointer to just allocated obj\n+      lea(r14, Address(r13, r14));\n+      ldr(rscratch1, Address(rthread, in_bytes(JavaThread::tlab_end_offset())));\n+\n+      cmp(r14, rscratch1);\n+      br(Assembler::GT, slow_case);\n+\n+      \/\/ OK we have room in TLAB,\n+      \/\/ Set new TLAB top\n+      str(r14, Address(rthread, in_bytes(JavaThread::tlab_top_offset())));\n+\n+      \/\/ Set new class always locked\n+      mov(rscratch1, (uint64_t) markWord::always_locked_prototype().value());\n+      str(rscratch1, Address(r13, oopDesc::mark_offset_in_bytes()));\n+\n+      store_klass_gap(r13, zr);  \/\/ zero klass gap for compressed oops\n+      if (vk == NULL) {\n+        \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+         mov(r0, r1);\n+      }\n+\n+      store_klass(r13, r1);  \/\/ klass\n+\n+      if (vk != NULL) {\n+        \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+        mov(r0, r13);\n+        far_call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+      } else {\n+\n+        \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+        ldr(r1, Address(r0, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+        ldr(r1, Address(r1, InlineKlass::pack_handler_offset()));\n+\n+        \/\/ Mov new class to r0 and call pack_handler\n+        mov(r0, r13);\n+        blr(r1);\n+      }\n+      b(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    ldr(rscratch1, RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    blr(rscratch1);\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        mov(to->as_Register(), from->as_Register());\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        Address to_addr = Address(sp, st_off);\n+        if (from->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             strd(from->as_FloatRegister(), to_addr);\n+          } else {\n+             assert(bt == T_FLOAT, \"must be float\");\n+             strs(from->as_FloatRegister(), to_addr);\n+          }\n+        } else {\n+          str(from->as_Register(), to_addr);\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(sp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);\n+      if (to->is_reg()) {\n+        if (to->is_FloatRegister()) {\n+          if (bt == T_DOUBLE) {\n+             ldrd(to->as_FloatRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            ldrs(to->as_FloatRegister(), from_addr);\n+          }\n+        } else {\n+          ldr(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        ldr(rscratch1, from_addr);\n+        str(rscratch1, Address(sp, st_off));\n+      }\n+    }\n+  }\n+\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {\n+      to_index--; \/\/ Ignore this\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int) to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+      }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        ldr(rscratch2, Address(sp, st_off));\n+        fromReg = rscratch2;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+\n+      if (!to->is_FloatRegister()) {\n+\n+        Register dst = to->is_stack() ? rscratch1 : to->as_Register();\n+\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+          str(dst, Address(sp, st_off));\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          ldrd(to->as_FloatRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          ldrs(to->as_FloatRegister(), fromAddr);\n+        }\n+     }\n+\n+    }\n+\n+  } while (vt != 0);\n+\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                                        int ret_off, int extra_stack_offset) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = r0;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r10;\n+  Register tmp1 = r14;\n+  Register tmp2 = r13;\n+  Register tmp3 = r1;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_cc_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+\n+    if (!from_r1->is_FloatRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        load_sized_value(from_reg, Address(sp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+\n+      if (is_oop) {\n+        DecoratorSet decorators = IN_HEAP | ACCESS_WRITE;\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, decorators);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        strd(from_r1->as_FloatRegister(), dst);\n+      } else {\n+        strs(from_r1->as_FloatRegister(), dst);\n+      }\n+    }\n+\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_cc_index();\n+  from_index = stream.regs_cc_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+\/\/ Unpack all inline type arguments passed as oops\n+void MacroAssembler::unpack_inline_args(Compile* C, bool receiver_only) {\n+  int sp_inc = unpack_inline_args_common(C, receiver_only);\n+  \/\/ Emit code for verified entry and save increment for stack repair on return\n+  verified_entry(C, sp_inc);\n+}\n+\n+int MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                                        BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                                        int args_passed, int args_on_stack, VMRegPair* regs,            \/\/ from\n+                                        int args_passed_to, int args_on_stack_to, VMRegPair* regs_to) { \/\/ to\n+  \/\/ Check if we need to extend the stack for packing\/unpacking\n+  int sp_inc = (args_on_stack_to - args_on_stack) * VMRegImpl::stack_slot_size;\n+  if (sp_inc > 0) {\n+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+    if (!is_packing) {\n+      \/\/ Save the return address, adjust the stack (make sure it is properly\n+      \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+      \/\/ (Note: C1 does this in C1_MacroAssembler::scalarized_entry).\n+      \/\/ FIXME: We need not to preserve return address on aarch64\n+      pop(rscratch1);\n+      sub(sp, sp, sp_inc);\n+      push(rscratch1);\n+    }\n+  } else {\n+    \/\/ The scalarized calling convention needs less stack space than the unscalarized one.\n+    \/\/ No need to extend the stack, the caller will take care of these adjustments.\n+    sp_inc = 0;\n+  }\n+\n+  int ret_off; \/\/ make sure we don't overwrite the return address\n+  if (is_packing) {\n+    \/\/ For C1 code, the VIEP doesn't have reserved slots, so we store the returned address at\n+    \/\/ rsp[0] during shuffling.\n+    ret_off = 0;\n+  } else {\n+    \/\/ C2 code ensures that sp_inc is a reserved slot.\n+    ret_off = sp_inc;\n+  }\n+\n+  return shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,\n+                                    sig_bt, sig_cc,\n+                                    args_passed, args_on_stack, regs,\n+                                    args_passed_to, args_on_stack_to, regs_to,\n+                                    sp_inc, ret_off);\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return (reg->is_FloatRegister()) ? v0->as_VMReg() : r14->as_VMReg();\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":464,"deletions":10,"binary":false,"changes":474,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -32,0 +33,4 @@\n+#include \"runtime\/signature.hpp\"\n+\n+\n+class ciInlineKlass;\n@@ -605,0 +610,10 @@\n+  void test_klass_is_value(Register klass, Register temp_reg, Label& is_value);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_flattened);\n+\n+  \/\/ Check klass\/oops is flat inline type array (oop->_klass->_layout_helper & vt_bit)\n+  void test_flattened_array_oop(Register klass, Register temp_reg, Label& is_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label& is_null_free_array);\n+\n@@ -812,0 +827,3 @@\n+  void load_metadata(Register dst, Register src);\n+  void load_storage_props(Register dst, Register src);\n+\n@@ -824,1 +842,1 @@\n-                       Register tmp1, Register tmp_thread);\n+                       Register tmp1, Register tmp_thread, Register tmp3 = noreg);\n@@ -836,1 +854,1 @@\n-                      Register tmp_thread = noreg, DecoratorSet decorators = 0);\n+                      Register tmp_thread = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -1169,0 +1187,31 @@\n+\n+  enum RegState {\n+     reg_readonly,\n+     reg_writable,\n+     reg_written\n+  };\n+\n+  void verified_entry(Compile* C, int sp_inc);\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+\n+\/\/ Unpack all inline type arguments passed as oops\n+  void unpack_inline_args(Compile* C, bool receiver_only);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,\n+                            RegState reg_state[], int ret_off, int extra_stack_offset);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                          int ret_off, int extra_stack_offset);\n+  void restore_stack(Compile* C);\n+\n+  int shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                          BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                          int args_passed, int args_on_stack, VMRegPair* regs,\n+                          int args_passed_to, int args_on_stack_to, VMRegPair* regs_to);\n+  bool shuffle_inline_args_spill(bool is_packing,  const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,\n+                                 VMRegPair* regs_from, int from_index, int regs_from_count,\n+                                 RegState* reg_state, int sp_inc, int extra_stack_offset);\n+  VMReg spill_reg_for(VMReg reg);\n+\n+\n@@ -1234,0 +1283,2 @@\n+  void fill_words(Register base, uint64_t cnt, Register value);\n+\n@@ -1355,0 +1406,3 @@\n+\n+  #include \"asm\/macroAssembler_common.hpp\"\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":56,"deletions":2,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -70,0 +70,3 @@\n+define_pd_global(bool, InlineTypePassFieldsAsArgs, false);\n+define_pd_global(bool, InlineTypeReturnedAsFields, false);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/globals_ppc.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1889,1 +1889,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(),\n@@ -1930,1 +1930,1 @@\n-    profile_obj_type(ret, R28_mdx, -in_bytes(ReturnTypeEntry::size()), tmp1, tmp2);\n+    profile_obj_type(ret, R28_mdx, -in_bytes(SingleTypeEntry::size()), tmp1, tmp2);\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1774,1 +1774,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -1823,1 +1823,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -73,0 +74,4 @@\n+  if (EnableValhalla && !UseBiasedLocking) {\n+    \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+    andptr(hdr, ~((int) markWord::biased_lock_bit_in_place));\n+  }\n@@ -162,1 +167,2 @@\n-  if (UseBiasedLocking && !len->is_valid()) {\n+  if ((UseBiasedLocking || EnableValhalla) && !len->is_valid()) {\n+    \/\/ Need to copy markWord::always_locked_pattern for values.\n@@ -324,0 +330,20 @@\n+void C1_MacroAssembler::build_frame_helper(int frame_size_in_bytes, int sp_inc, bool needs_stack_repair) {\n+  push(rbp);\n+  if (PreserveFramePointer) {\n+    mov(rbp, rsp);\n+  }\n+  #if !defined(_LP64) && defined(TIERED)\n+    if (UseSSE < 2 ) {\n+      \/\/ c2 leaves fpu stack dirty. Clean it on entry\n+      empty_FPU_stack();\n+    }\n+  #endif \/\/ !_LP64 && TIERED\n+  decrement(rsp, frame_size_in_bytes);\n+\n+  if (needs_stack_repair) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    int real_frame_size = sp_inc + frame_size_in_bytes + wordSize;\n+    movptr(Address(rsp, frame_size_in_bytes - wordSize), real_frame_size);\n+  }\n+}\n@@ -325,2 +351,8 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n-  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, bool needs_stack_repair, bool has_scalarized_args, Label* verified_inline_entry_label) {\n+  if (has_scalarized_args) {\n+    \/\/ Initialize orig_pc to detect deoptimization during buffering in the entry points\n+    movptr(Address(rsp, sp_offset_for_orig_pc - frame_size_in_bytes - wordSize), 0);\n+  }\n+  if (!needs_stack_repair && verified_inline_entry_label != NULL) {\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -332,0 +364,1 @@\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n@@ -334,11 +367,1 @@\n-  push(rbp);\n-  if (PreserveFramePointer) {\n-    mov(rbp, rsp);\n-  }\n-#if !defined(_LP64) && defined(TIERED)\n-  if (UseSSE < 2 ) {\n-    \/\/ c2 leaves fpu stack dirty. Clean it on entry\n-    empty_FPU_stack();\n-  }\n-#endif \/\/ !_LP64 && TIERED\n-  decrement(rsp, frame_size_in_bytes); \/\/ does not emit code for frame_size == 0\n+  build_frame_helper(frame_size_in_bytes, 0, needs_stack_repair);\n@@ -346,0 +369,5 @@\n+  if (needs_stack_repair && verified_inline_entry_label != NULL) {\n+    \/\/ Jump here from the scalarized entry points that require additional stack space\n+    \/\/ for packing scalarized arguments and therefore already created the frame.\n+    bind(*verified_inline_entry_label);\n+  }\n@@ -350,7 +378,0 @@\n-\n-void C1_MacroAssembler::remove_frame(int frame_size_in_bytes) {\n-  increment(rsp, frame_size_in_bytes);  \/\/ Does not emit code for frame_size == 0\n-  pop(rbp);\n-}\n-\n-\n@@ -373,0 +394,63 @@\n+int C1_MacroAssembler::scalarized_entry(const CompiledEntrySignature* ces, int frame_size_in_bytes, int bang_size_in_bytes, int sp_offset_for_orig_pc, Label& verified_inline_entry_label, bool is_inline_ro_entry) {\n+  assert(InlineTypePassFieldsAsArgs, \"sanity\");\n+  \/\/ Make sure there is enough stack space for this method's activation.\n+  assert(bang_size_in_bytes >= frame_size_in_bytes, \"stack bang size incorrect\");\n+  generate_stack_overflow_check(bang_size_in_bytes);\n+\n+  GrowableArray<SigEntry>* sig    = &ces->sig();\n+  GrowableArray<SigEntry>* sig_cc = is_inline_ro_entry ? &ces->sig_cc_ro() : &ces->sig_cc();\n+  VMRegPair* regs      = ces->regs();\n+  VMRegPair* regs_cc   = is_inline_ro_entry ? ces->regs_cc_ro() : ces->regs_cc();\n+  int args_on_stack    = ces->args_on_stack();\n+  int args_on_stack_cc = is_inline_ro_entry ? ces->args_on_stack_cc_ro() : ces->args_on_stack_cc();\n+\n+  assert(sig->length() <= sig_cc->length(), \"Zero-sized inline class not allowed!\");\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sig_cc->length());\n+  int args_passed = sig->length();\n+  int args_passed_cc = SigEntry::fill_sig_bt(sig_cc, sig_bt);\n+  int extra_stack_offset = wordSize; \/\/ tos is return address.\n+\n+  \/\/ Check if we need to extend the stack for packing\n+  int sp_inc = 0;\n+  if (args_on_stack > args_on_stack_cc) {\n+    \/\/ Two additional slots to account for return address\n+    sp_inc = (args_on_stack + 2) * VMRegImpl::stack_slot_size;\n+    sp_inc = align_up(sp_inc, StackAlignmentInBytes);\n+    pop(r13); \/\/ Copy return address\n+    subptr(rsp, sp_inc);\n+    push(r13);\n+  }\n+\n+  \/\/ Create a temp frame so we can call into the runtime. It must be properly set up to accommodate GC.\n+  build_frame_helper(frame_size_in_bytes, sp_inc, ces->c1_needs_stack_repair());\n+\n+  \/\/ Initialize orig_pc to detect deoptimization during buffering in below runtime call\n+  movptr(Address(rsp, sp_offset_for_orig_pc), 0);\n+\n+  \/\/ FIXME -- call runtime only if we cannot in-line allocate all the incoming inline type args.\n+  movptr(rbx, (intptr_t)(ces->method()));\n+  if (is_inline_ro_entry) {\n+    call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id)));\n+  } else {\n+    call(RuntimeAddress(Runtime1::entry_for(Runtime1::buffer_inline_args_id)));\n+  }\n+  int rt_call_offset = offset();\n+\n+  \/\/ Remove the temp frame\n+  addptr(rsp, frame_size_in_bytes);\n+  pop(rbp);\n+\n+  shuffle_inline_args(true, is_inline_ro_entry, extra_stack_offset, sig_bt, sig_cc,\n+                      args_passed_cc, args_on_stack_cc, regs_cc, \/\/ from\n+                      args_passed, args_on_stack, regs, sp_inc); \/\/ to\n+\n+  if (ces->c1_needs_stack_repair()) {\n+    \/\/ Create the real frame. Below jump will then skip over the stack banging and frame\n+    \/\/ setup code in the verified_inline_entry (which has a different real_frame_size).\n+    build_frame_helper(frame_size_in_bytes, sp_inc, true);\n+  }\n+\n+  jmp(verified_inline_entry_label);\n+  return rt_call_offset;\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":105,"deletions":21,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -504,0 +504,4 @@\n+  if (EnableValhalla && !UseBiasedLocking) {\n+    \/\/ Mask always_locked bit such that we go to the slow path if object is an inline type\n+    andptr(tmpReg, ~((int) markWord::biased_lock_bit_in_place));\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -154,1 +155,1 @@\n-        assert(ReturnTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n+        assert(SingleTypeEntry::static_cell_count() < TypeStackSlotEntries::per_arg_count(), \"can't move past ret type\");\n@@ -199,1 +200,1 @@\n-    Address mdo_ret_addr(mdp, -in_bytes(ReturnTypeEntry::size()));\n+    Address mdo_ret_addr(mdp, -in_bytes(SingleTypeEntry::size()));\n@@ -559,1 +560,2 @@\n-                                                  Label& ok_is_subtype) {\n+                                                  Label& ok_is_subtype,\n+                                                  bool profile) {\n@@ -567,1 +569,3 @@\n-  profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  if (profile) {\n+    profile_typecheck(rcx, Rsub_klass, rdi); \/\/ blows rcx, reloads rdi\n+  }\n@@ -573,1 +577,3 @@\n-  profile_typecheck_failed(rcx); \/\/ blows rcx\n+  if (profile) {\n+    profile_typecheck_failed(rcx); \/\/ blows rcx\n+  }\n@@ -998,1 +1004,1 @@\n- \/\/ get method access flags\n+  \/\/ get method access flags\n@@ -1122,4 +1128,2 @@\n-  \/\/ remove activation\n-  \/\/ get sender sp\n-  movptr(rbx,\n-         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    movptr(rbx,\n+               Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n@@ -1147,0 +1151,34 @@\n+\n+  \/\/ remove activation\n+  \/\/ get sender sp\n+  movptr(rbx,\n+         Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+\n+  if (state == atos && InlineTypeReturnedAsFields) {\n+    Label skip;\n+    \/\/ Test if the return type is an inline type\n+    movptr(rdi, Address(rbp, frame::interpreter_frame_method_offset * wordSize));\n+    movptr(rdi, Address(rdi, Method::const_offset()));\n+    load_unsigned_byte(rdi, Address(rdi, ConstMethod::result_type_offset()));\n+    cmpl(rdi, T_INLINE_TYPE);\n+    jcc(Assembler::notEqual, skip);\n+\n+    \/\/ We are returning an inline type, load its fields into registers\n+#ifndef _LP64\n+    super_call_VM_leaf(StubRoutines::load_inline_type_fields_in_regs());\n+#else\n+    \/\/ Load fields from a buffered value with an inline class specific handler\n+    Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    load_klass(rdi, rax, tmp_load_klass);\n+    movptr(rdi, Address(rdi, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    movptr(rdi, Address(rdi, InlineKlass::unpack_handler_offset()));\n+\n+    testptr(rdi, rdi);\n+    jcc(Assembler::equal, skip);\n+\n+    call(rdi);\n+#endif\n+    \/\/ call above kills the value in rbx. Reload it.\n+    movptr(rbx, Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize));\n+    bind(skip);\n+  }\n@@ -1166,0 +1204,106 @@\n+void InterpreterMacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                                  Register t1, Register t2,\n+                                                  bool clear_fields, Label& alloc_failed) {\n+  MacroAssembler::allocate_instance(klass, new_obj, t1, t2, clear_fields, alloc_failed);\n+  {\n+    SkipIfEqual skip_if(this, &DTraceAllocProbes, 0);\n+    \/\/ Trigger dtrace event for fastpath\n+    push(atos);\n+    call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_object_alloc), new_obj);\n+    pop(atos);\n+  }\n+}\n+\n+\n+void InterpreterMacroAssembler::read_inlined_field(Register holder_klass,\n+                                                     Register field_index, Register field_offset,\n+                                                     Register obj) {\n+  Label alloc_failed, empty_value, done;\n+  const Register src = field_offset;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+  assert_different_registers(obj, holder_klass, field_index, field_offset, dst_temp);\n+\n+  \/\/ Grap the inline field klass\n+  push(holder_klass);\n+  const Register field_klass = holder_klass;\n+  get_inline_type_field_klass(holder_klass, field_index, field_klass);\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(field_klass, dst_temp, empty_value);\n+\n+  \/\/ allocate buffer\n+  push(obj); \/\/ save holder\n+  allocate_instance(field_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+\n+  \/\/ Have an oop instance buffer, copy into it\n+  data_for_oop(obj, dst_temp, field_klass);\n+  pop(alloc_temp);             \/\/ restore holder\n+  lea(src, Address(alloc_temp, field_offset));\n+  \/\/ call_VM_leaf, clobbers a few regs, save restore new obj\n+  push(obj);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, field_klass);\n+  pop(obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(field_klass, dst_temp, obj);\n+  pop(holder_klass);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(obj);\n+  pop(holder_klass);\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::read_inlined_field),\n+          obj, field_index, holder_klass);\n+\n+  bind(done);\n+}\n+\n+void InterpreterMacroAssembler::read_flattened_element(Register array, Register index,\n+                                                       Register t1, Register t2,\n+                                                       Register obj) {\n+  assert_different_registers(array, index, t1, t2);\n+  Label alloc_failed, empty_value, done;\n+  const Register array_klass = t2;\n+  const Register elem_klass = t1;\n+  const Register alloc_temp = LP64_ONLY(rscratch1) NOT_LP64(rsi);\n+  const Register dst_temp   = LP64_ONLY(rscratch2) NOT_LP64(rdi);\n+\n+  \/\/ load in array->klass()->element_klass()\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(array_klass, array, tmp_load_klass);\n+  movptr(elem_klass, Address(array_klass, ArrayKlass::element_klass_offset()));\n+\n+  \/\/check for empty value klass\n+  test_klass_is_empty_inline_type(elem_klass, dst_temp, empty_value);\n+\n+  \/\/ calc source into \"array_klass\" and free up some regs\n+  const Register src = array_klass;\n+  push(index); \/\/ preserve index reg in case alloc_failed\n+  data_for_value_array_index(array, array_klass, index, src);\n+\n+  allocate_instance(elem_klass, obj, alloc_temp, dst_temp, false, alloc_failed);\n+  \/\/ Have an oop instance buffer, copy into it\n+  store_ptr(0, obj); \/\/ preserve obj (overwrite index, no longer needed)\n+  data_for_oop(obj, dst_temp, elem_klass);\n+  access_value_copy(IS_DEST_UNINITIALIZED, src, dst_temp, elem_klass);\n+  pop(obj);\n+  jmp(done);\n+\n+  bind(empty_value);\n+  get_empty_inline_type_oop(elem_klass, dst_temp, obj);\n+  jmp(done);\n+\n+  bind(alloc_failed);\n+  pop(index);\n+  if (array == c_rarg2) {\n+    mov(elem_klass, array);\n+    array = elem_klass;\n+  }\n+  call_VM(obj, CAST_FROM_FN_PTR(address, InterpreterRuntime::value_array_load), array, index);\n+\n+  bind(done);\n+}\n+\n@@ -1217,0 +1361,4 @@\n+    if (EnableValhalla && !UseBiasedLocking) {\n+      \/\/ For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking\n+      andptr(swap_reg, ~((int) markWord::biased_lock_bit_in_place));\n+    }\n@@ -1941,0 +2089,38 @@\n+void InterpreterMacroAssembler::profile_array(Register mdp,\n+                                              Register array,\n+                                              Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n+\n+    mov(tmp, array);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::array_offset())));\n+\n+    Label not_flat;\n+    test_non_flattened_array_oop(array, tmp, not_flat);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::flat_array_byte_constant());\n+\n+    bind(not_flat);\n+\n+    Label not_null_free;\n+    test_non_null_free_array_oop(array, tmp, not_null_free);\n+\n+    set_mdp_flag_at(mdp, ArrayLoadStoreData::null_free_array_byte_constant());\n+\n+    bind(not_null_free);\n+\n+    bind(profile_continue);\n+  }\n+}\n+\n+void InterpreterMacroAssembler::profile_element(Register mdp,\n+                                                Register element,\n+                                                Register tmp) {\n+  if (ProfileInterpreter) {\n+    Label profile_continue;\n+\n+    \/\/ If no method data exists, go to profile_continue.\n+    test_method_data_pointer(mdp, profile_continue);\n@@ -1942,0 +2128,9 @@\n+    mov(tmp, element);\n+    profile_obj_type(tmp, Address(mdp, in_bytes(ArrayLoadStoreData::element_offset())));\n+\n+    \/\/ The method data pointer needs to be updated.\n+    update_mdp_by_constant(mdp, in_bytes(ArrayLoadStoreData::array_load_store_data_size()));\n+\n+    bind(profile_continue);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":205,"deletions":10,"binary":false,"changes":215,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"runtime\/signature_cc.hpp\"\n@@ -51,0 +52,1 @@\n+#include \"vmreg_x86.inline.hpp\"\n@@ -52,0 +54,3 @@\n+#ifdef COMPILER2\n+#include \"opto\/output.hpp\"\n+#endif\n@@ -1636,0 +1641,4 @@\n+void MacroAssembler::super_call_VM_leaf(address entry_point) {\n+  MacroAssembler::call_VM_leaf_base(entry_point, 1);\n+}\n+\n@@ -2604,0 +2613,94 @@\n+void MacroAssembler::test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, Address(klass, Klass::access_flags_offset()));\n+  testl(temp_reg, JVM_ACC_INLINE);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(klass, temp_reg, done_check);\n+    stop(\"test_klass_is_empty_inline_type with non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  movl(temp_reg, Address(klass, InstanceKlass::misc_flags_offset()));\n+  testl(temp_reg, InstanceKlass::misc_flags_is_empty_inline_type());\n+  jcc(Assembler::notZero, is_empty_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline_type) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inline_type_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::zero, not_inline_type);\n+}\n+\n+void MacroAssembler::test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined) {\n+  movl(temp_reg, flags);\n+  shrl(temp_reg, ConstantPoolCacheEntry::is_inlined_shift);\n+  andl(temp_reg, 0x1);\n+  testl(temp_reg, temp_reg);\n+  jcc(Assembler::notZero, is_inlined);\n+}\n+\n+void MacroAssembler::test_flattened_array_oop(Register oop, Register temp_reg,\n+                                              Label&is_flattened_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_flattened_array_layout(temp_reg, is_flattened_array);\n+}\n+\n+void MacroAssembler::test_non_flattened_array_oop(Register oop, Register temp_reg,\n+                                                  Label&is_non_flattened_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_flattened_array_layout(temp_reg, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_null_free_array_layout(temp_reg, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array) {\n+  Register tmp_load_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+  load_klass(temp_reg, oop, tmp_load_klass);\n+  movl(temp_reg, Address(temp_reg, Klass::layout_helper_offset()));\n+  test_non_null_free_array_layout(temp_reg, is_non_null_free_array);\n+}\n+\n+void MacroAssembler::test_flattened_array_layout(Register lh, Label& is_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::notZero, is_flattened_array);\n+}\n+void MacroAssembler::test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array) {\n+  testl(lh, Klass::_lh_array_tag_vt_value_bit_inplace);\n+  jcc(Assembler::zero, is_non_flattened_array);\n+}\n+\n+void MacroAssembler::test_null_free_array_layout(Register lh, Label& is_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::notZero, is_null_free_array);\n+}\n+\n+void MacroAssembler::test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array) {\n+  testl(lh, Klass::_lh_null_free_bit_inplace);\n+  jcc(Assembler::zero, is_non_null_free_array);\n+}\n+\n+\n@@ -3302,0 +3405,129 @@\n+\/\/ Object \/ value buffer allocation...\n+\/\/\n+\/\/ Kills klass and rsi on LP64\n+void MacroAssembler::allocate_instance(Register klass, Register new_obj,\n+                                       Register t1, Register t2,\n+                                       bool clear_fields, Label& alloc_failed)\n+{\n+  Label done, initialize_header, initialize_object, slow_case, slow_case_no_pop;\n+  Register layout_size = t1;\n+  assert(new_obj == rax, \"needs to be rax, according to barrier asm eden_allocate\");\n+  assert_different_registers(klass, new_obj, t1, t2);\n+\n+#ifdef ASSERT\n+  {\n+    Label L;\n+    cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n+    jcc(Assembler::equal, L);\n+    stop(\"klass not initialized\");\n+    bind(L);\n+  }\n+#endif\n+\n+  \/\/ get instance_size in InstanceKlass (scaled to a count of bytes)\n+  movl(layout_size, Address(klass, Klass::layout_helper_offset()));\n+  \/\/ test to see if it has a finalizer or is malformed in some way\n+  testl(layout_size, Klass::_lh_instance_slow_path_bit);\n+  jcc(Assembler::notZero, slow_case_no_pop);\n+\n+  \/\/ Allocate the instance:\n+  \/\/  If TLAB is enabled:\n+  \/\/    Try to allocate in the TLAB.\n+  \/\/    If fails, go to the slow path.\n+  \/\/  Else If inline contiguous allocations are enabled:\n+  \/\/    Try to allocate in eden.\n+  \/\/    If fails due to heap end, go to slow path.\n+  \/\/\n+  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n+  \/\/    Initialize the allocation.\n+  \/\/    Exit.\n+  \/\/\n+  \/\/  Go to slow path.\n+  const bool allow_shared_alloc =\n+    Universe::heap()->supports_inline_contig_alloc();\n+\n+  push(klass);\n+  const Register thread = LP64_ONLY(r15_thread) NOT_LP64(klass);\n+#ifndef _LP64\n+  if (UseTLAB || allow_shared_alloc) {\n+    get_thread(thread);\n+  }\n+#endif \/\/ _LP64\n+\n+  if (UseTLAB) {\n+    tlab_allocate(thread, new_obj, layout_size, 0, klass, t2, slow_case);\n+    if (ZeroTLAB || (!clear_fields)) {\n+      \/\/ the fields have been already cleared\n+      jmp(initialize_header);\n+    } else {\n+      \/\/ initialize both the header and fields\n+      jmp(initialize_object);\n+    }\n+  } else {\n+    \/\/ Allocation in the shared Eden, if allowed.\n+    \/\/\n+    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);\n+  }\n+\n+  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n+  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n+  if (UseTLAB || allow_shared_alloc) {\n+    if (clear_fields) {\n+      \/\/ The object is initialized before the header.  If the object size is\n+      \/\/ zero, go directly to the header initialization.\n+      bind(initialize_object);\n+      decrement(layout_size, sizeof(oopDesc));\n+      jcc(Assembler::zero, initialize_header);\n+\n+      \/\/ Initialize topmost object field, divide size by 8, check if odd and\n+      \/\/ test if zero.\n+      Register zero = klass;\n+      xorl(zero, zero);    \/\/ use zero reg to clear memory (shorter code)\n+      shrl(layout_size, LogBytesPerLong); \/\/ divide by 2*oopSize and set carry flag if odd\n+\n+  #ifdef ASSERT\n+      \/\/ make sure instance_size was multiple of 8\n+      Label L;\n+      \/\/ Ignore partial flag stall after shrl() since it is debug VM\n+      jcc(Assembler::carryClear, L);\n+      stop(\"object size is not multiple of 2 - adjust this code\");\n+      bind(L);\n+      \/\/ must be > 0, no extra check needed here\n+  #endif\n+\n+      \/\/ initialize remaining object fields: instance_size was a multiple of 8\n+      {\n+        Label loop;\n+        bind(loop);\n+        movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 1*oopSize), zero);\n+        NOT_LP64(movptr(Address(new_obj, layout_size, Address::times_8, sizeof(oopDesc) - 2*oopSize), zero));\n+        decrement(layout_size);\n+        jcc(Assembler::notZero, loop);\n+      }\n+    } \/\/ clear_fields\n+\n+    \/\/ initialize object header only.\n+    bind(initialize_header);\n+    pop(klass);\n+    Register mark_word = t2;\n+    movptr(mark_word, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(new_obj, oopDesc::mark_offset_in_bytes ()), mark_word);\n+#ifdef _LP64\n+    xorl(rsi, rsi);                 \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(new_obj, rsi);  \/\/ zero klass gap for compressed oops\n+#endif\n+    movptr(t2, klass);         \/\/ preserve klass\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(new_obj, t2, tmp_store_klass);  \/\/ src klass reg is potentially compressed\n+\n+    jmp(done);\n+  }\n+\n+  bind(slow_case);\n+  pop(klass);\n+  bind(slow_case_no_pop);\n+  jmp(alloc_failed);\n+\n+  bind(done);\n+}\n+\n@@ -3379,0 +3611,50 @@\n+void MacroAssembler::get_inline_type_field_klass(Register klass, Register index, Register inline_klass) {\n+  movptr(inline_klass, Address(klass, InstanceKlass::inline_type_field_klasses_offset()));\n+#ifdef ASSERT\n+  {\n+    Label done;\n+    cmpptr(inline_klass, 0);\n+    jcc(Assembler::notEqual, done);\n+    stop(\"get_inline_type_field_klass contains no inline klass\");\n+    bind(done);\n+  }\n+#endif\n+  movptr(inline_klass, Address(inline_klass, index, Address::times_ptr));\n+}\n+\n+void MacroAssembler::get_default_value_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_default_value_oop from non inline type klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  Register offset = temp_reg;\n+  \/\/ Getting the offset of the pre-allocated default value\n+  movptr(offset, Address(inline_klass, in_bytes(InstanceKlass::adr_inlineklass_fixed_block_offset())));\n+  movl(offset, Address(offset, in_bytes(InlineKlass::default_value_offset_offset())));\n+\n+  \/\/ Getting the mirror\n+  movptr(obj, Address(inline_klass, in_bytes(Klass::java_mirror_offset())));\n+  resolve_oop_handle(obj, inline_klass);\n+\n+  \/\/ Getting the pre-allocated default value from the mirror\n+  Address field(obj, offset, Address::times_1);\n+  load_heap_oop(obj, field);\n+}\n+\n+void MacroAssembler::get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj) {\n+#ifdef ASSERT\n+  {\n+    Label done_check;\n+    test_klass_is_empty_inline_type(inline_klass, temp_reg, done_check);\n+    stop(\"get_empty_value from non-empty inline klass\");\n+    bind(done_check);\n+  }\n+#endif\n+  get_default_value_oop(inline_klass, temp_reg, obj);\n+}\n+\n+\n@@ -3727,1 +4009,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -3825,1 +4111,5 @@\n-  if (!VerifyOops) return;\n+  if (!VerifyOops || VerifyAdapterSharing) {\n+    \/\/ Below address of the code string confuses VerifyAdapterSharing\n+    \/\/ because it may differ between otherwise equivalent adapters.\n+    return;\n+  }\n@@ -4321,0 +4611,8 @@\n+void MacroAssembler::load_metadata(Register dst, Register src) {\n+  if (UseCompressedClassPointers) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  } else {\n+    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n@@ -4330,1 +4628,1 @@\n-    movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+  movptr(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n@@ -4363,1 +4661,1 @@\n-                                     Register tmp1, Register tmp2) {\n+                                     Register tmp1, Register tmp2, Register tmp3) {\n@@ -4368,1 +4666,23 @@\n-    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    bs->BarrierSetAssembler::store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  } else {\n+    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2, tmp3);\n+  }\n+}\n+\n+void MacroAssembler::access_value_copy(DecoratorSet decorators, Register src, Register dst,\n+                                       Register inline_klass) {\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->value_copy(this, decorators, src, dst, inline_klass);\n+}\n+\n+void MacroAssembler::first_field_offset(Register inline_klass, Register offset) {\n+  movptr(offset, Address(inline_klass, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+  movl(offset, Address(offset, InlineKlass::first_field_offset_offset()));\n+}\n+\n+void MacroAssembler::data_for_oop(Register oop, Register data, Register inline_klass) {\n+  \/\/ ((address) (void*) o) + vk->first_field_offset();\n+  Register offset = (data == oop) ? rscratch1 : data;\n+  first_field_offset(inline_klass, offset);\n+  if (data == oop) {\n+    addptr(data, offset);\n@@ -4370,1 +4690,1 @@\n-    bs->store_at(this, decorators, type, dst, src, tmp1, tmp2);\n+    lea(data, Address(oop, offset));\n@@ -4374,0 +4694,18 @@\n+void MacroAssembler::data_for_value_array_index(Register array, Register array_klass,\n+                                                Register index, Register data) {\n+  assert(index != rcx, \"index needs to shift by rcx\");\n+  assert_different_registers(array, array_klass, index);\n+  assert_different_registers(rcx, array, index);\n+\n+  \/\/ array->base() + (index << Klass::layout_helper_log2_element_size(lh));\n+  movl(rcx, Address(array_klass, Klass::layout_helper_offset()));\n+\n+  \/\/ Klass::layout_helper_log2_element_size(lh)\n+  \/\/ (lh >> _lh_log2_element_size_shift) & _lh_log2_element_size_mask;\n+  shrl(rcx, Klass::_lh_log2_element_size_shift);\n+  andl(rcx, Klass::_lh_log2_element_size_mask);\n+  shlptr(index); \/\/ index << rcx\n+\n+  lea(data, Address(array, index, Address::times_1, arrayOopDesc::base_offset_in_bytes(T_INLINE_TYPE)));\n+}\n+\n@@ -4395,2 +4733,2 @@\n-                                    Register tmp2, DecoratorSet decorators) {\n-  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2);\n+                                    Register tmp2, Register tmp3, DecoratorSet decorators) {\n+  access_store_at(T_OBJECT, IN_HEAP | decorators, dst, src, tmp1, tmp2, tmp3);\n@@ -4401,1 +4739,1 @@\n-  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg);\n+  access_store_at(T_OBJECT, IN_HEAP, dst, noreg, noreg, noreg, noreg);\n@@ -4716,1 +5054,5 @@\n-void MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n@@ -4769,0 +5111,6 @@\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, C->output()->sp_inc_offset()), sp_inc + framesize + wordSize);\n+  }\n+\n@@ -4797,5 +5145,0 @@\n-\n-  if (!is_stub) {\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->nmethod_entry_barrier(this);\n-  }\n@@ -4805,1 +5148,1 @@\n-void MacroAssembler::xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp) {\n+void MacroAssembler::xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp) {\n@@ -4809,0 +5152,1 @@\n+  movdq(xtmp, val);\n@@ -4810,1 +5154,2 @@\n-    vpxor(xtmp, xtmp, xtmp, AVX_256bit);\n+    punpcklqdq(xtmp, xtmp);\n+    vinserti128_high(xtmp, xtmp);\n@@ -4812,1 +5157,1 @@\n-    pxor(xtmp, xtmp);\n+    punpcklqdq(xtmp, xtmp);\n@@ -4856,1 +5201,359 @@\n-void MacroAssembler::clear_mem(Register base, Register cnt, Register tmp, XMMRegister xtmp, bool is_large) {\n+int MacroAssembler::store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter) {\n+  \/\/ An inline type might be returned. If fields are in registers we\n+  \/\/ need to allocate an inline type instance and initialize it with\n+  \/\/ the value of the fields.\n+  Label skip;\n+  \/\/ We only need a new buffered inline type if a new one is not returned\n+  testptr(rax, 1);\n+  jcc(Assembler::zero, skip);\n+  int call_offset = -1;\n+\n+#ifdef _LP64\n+  Label slow_case;\n+\n+  \/\/ Try to allocate a new buffered inline type (from the heap)\n+  if (UseTLAB) {\n+    \/\/ FIXME -- for smaller code, the inline allocation (and the slow case) should be moved inside the pack handler.\n+    if (vk != NULL) {\n+      \/\/ Called from C1, where the return type is statically known.\n+      movptr(rbx, (intptr_t)vk->get_InlineKlass());\n+      jint lh = vk->layout_helper();\n+      assert(lh != Klass::_lh_neutral_value, \"inline class in return type must have been resolved\");\n+      movl(r14, lh);\n+    } else {\n+      \/\/ Call from interpreter. RAX contains ((the InlineKlass* of the return type) | 0x01)\n+      mov(rbx, rax);\n+      andptr(rbx, -2);\n+      movl(r14, Address(rbx, Klass::layout_helper_offset()));\n+    }\n+\n+    movptr(r13, Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())));\n+    lea(r14, Address(r13, r14, Address::times_1));\n+    cmpptr(r14, Address(r15_thread, in_bytes(JavaThread::tlab_end_offset())));\n+    jcc(Assembler::above, slow_case);\n+    movptr(Address(r15_thread, in_bytes(JavaThread::tlab_top_offset())), r14);\n+    movptr(Address(r13, oopDesc::mark_offset_in_bytes()), (intptr_t)markWord::always_locked_prototype().value());\n+\n+    xorl(rax, rax); \/\/ use zero reg to clear memory (shorter code)\n+    store_klass_gap(r13, rax);  \/\/ zero klass gap for compressed oops\n+\n+    if (vk == NULL) {\n+      \/\/ store_klass corrupts rbx, so save it in rax for later use (interpreter case only).\n+      mov(rax, rbx);\n+    }\n+    Register tmp_store_klass = LP64_ONLY(rscratch1) NOT_LP64(noreg);\n+    store_klass(r13, rbx, tmp_store_klass);  \/\/ klass\n+\n+    \/\/ We have our new buffered inline type, initialize its fields with an inline class specific handler\n+    if (vk != NULL) {\n+      \/\/ FIXME -- do the packing in-line to avoid the runtime call\n+      mov(rax, r13);\n+      call(RuntimeAddress(vk->pack_handler())); \/\/ no need for call info as this will not safepoint.\n+    } else {\n+      movptr(rbx, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+      movptr(rbx, Address(rbx, InlineKlass::pack_handler_offset()));\n+      mov(rax, r13);\n+      call(rbx);\n+    }\n+    jmp(skip);\n+  }\n+\n+  bind(slow_case);\n+  \/\/ We failed to allocate a new inline type, fall back to a runtime\n+  \/\/ call. Some oop field may be live in some registers but we can't\n+  \/\/ tell. That runtime call will take care of preserving them\n+  \/\/ across a GC if there's one.\n+#endif\n+\n+  if (from_interpreter) {\n+    super_call_VM_leaf(StubRoutines::store_inline_type_fields_to_buf());\n+  } else {\n+    call(RuntimeAddress(StubRoutines::store_inline_type_fields_to_buf()));\n+    call_offset = offset();\n+  }\n+\n+  bind(skip);\n+  return call_offset;\n+}\n+\n+\n+\/\/ Move a value between registers\/stack slots and update the reg_state\n+bool MacroAssembler::move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  if (reg_state[to->value()] == reg_written) {\n+    return true; \/\/ Already written\n+  }\n+  if (from != to && bt != T_VOID) {\n+    if (reg_state[to->value()] == reg_readonly) {\n+      return false; \/\/ Not yet writable\n+    }\n+    if (from->is_reg()) {\n+      if (to->is_reg()) {\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to->as_Register(), from->as_Register());\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        assert(st_off != ret_off, \"overwriting return address at %d\", st_off);\n+        Address to_addr = Address(rsp, st_off);\n+        if (from->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to_addr, from->as_XMMRegister());\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to_addr, from->as_XMMRegister());\n+          }\n+        } else {\n+          movq(to_addr, from->as_Register());\n+        }\n+      }\n+    } else {\n+      Address from_addr = Address(rsp, from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset);\n+      if (to->is_reg()) {\n+        if (to->is_XMMRegister()) {\n+          if (bt == T_DOUBLE) {\n+            movdbl(to->as_XMMRegister(), from_addr);\n+          } else {\n+            assert(bt == T_FLOAT, \"must be float\");\n+            movflt(to->as_XMMRegister(), from_addr);\n+          }\n+        } else {\n+          movq(to->as_Register(), from_addr);\n+        }\n+      } else {\n+        int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        assert(st_off != ret_off, \"overwriting return address at %d\", st_off);\n+        movq(r13, from_addr);\n+        movq(Address(rsp, st_off), r13);\n+      }\n+    }\n+  }\n+  \/\/ Update register states\n+  reg_state[from->value()] = reg_writable;\n+  reg_state[to->value()] = reg_written;\n+  return true;\n+}\n+\n+\/\/ Read all fields from an inline type oop and store the values in registers\/stack slots\n+bool MacroAssembler::unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to,\n+                                          int& to_index, RegState reg_state[], int ret_off, int extra_stack_offset) {\n+  Register fromReg = from->is_reg() ? from->as_Register() : noreg;\n+  assert(sig->at(sig_index)._bt == T_VOID, \"should be at end delimiter\");\n+\n+  int vt = 1;\n+  bool done = true;\n+  bool mark_done = true;\n+  do {\n+    sig_index--;\n+    BasicType bt = sig->at(sig_index)._bt;\n+    if (bt == T_INLINE_TYPE) {\n+      vt--;\n+    } else if (bt == T_VOID &&\n+               sig->at(sig_index-1)._bt != T_LONG &&\n+               sig->at(sig_index-1)._bt != T_DOUBLE) {\n+      vt++;\n+    } else if (SigEntry::is_reserved_entry(sig, sig_index)) {\n+      to_index--; \/\/ Ignore this\n+    } else {\n+      assert(to_index >= 0, \"invalid to_index\");\n+      VMRegPair pair_to = regs_to[to_index--];\n+      VMReg to = pair_to.first();\n+\n+      if (bt == T_VOID) continue;\n+\n+      int idx = (int)to->value();\n+      if (reg_state[idx] == reg_readonly) {\n+         if (idx != from->value()) {\n+           mark_done = false;\n+         }\n+         done = false;\n+         continue;\n+      } else if (reg_state[idx] == reg_written) {\n+        continue;\n+      } else {\n+        assert(reg_state[idx] == reg_writable, \"must be writable\");\n+        reg_state[idx] = reg_written;\n+       }\n+\n+      if (fromReg == noreg) {\n+        int st_off = from->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        movq(r10, Address(rsp, st_off));\n+        fromReg = r10;\n+      }\n+\n+      int off = sig->at(sig_index)._offset;\n+      assert(off > 0, \"offset in object should be positive\");\n+      bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+\n+      Address fromAddr = Address(fromReg, off);\n+      bool is_signed = (bt != T_CHAR) && (bt != T_BOOLEAN);\n+      if (!to->is_XMMRegister()) {\n+        Register dst = to->is_stack() ? r13 : to->as_Register();\n+        if (is_oop) {\n+          load_heap_oop(dst, fromAddr);\n+        } else {\n+          load_sized_value(dst, fromAddr, type2aelembytes(bt), is_signed);\n+        }\n+        if (to->is_stack()) {\n+          int st_off = to->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+          assert(st_off != ret_off, \"overwriting return address at %d\", st_off);\n+          movq(Address(rsp, st_off), dst);\n+        }\n+      } else {\n+        if (bt == T_DOUBLE) {\n+          movdbl(to->as_XMMRegister(), fromAddr);\n+        } else {\n+          assert(bt == T_FLOAT, \"must be float\");\n+          movflt(to->as_XMMRegister(), fromAddr);\n+        }\n+      }\n+    }\n+  } while (vt != 0);\n+  if (mark_done && reg_state[from->value()] != reg_written) {\n+    \/\/ This is okay because no one else will write to that slot\n+    reg_state[from->value()] = reg_writable;\n+  }\n+  return done;\n+}\n+\n+\/\/ Pack fields back into an inline type oop\n+bool MacroAssembler::pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                                        VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                                        int ret_off, int extra_stack_offset) {\n+  assert(sig->at(sig_index)._bt == T_INLINE_TYPE, \"should be at end delimiter\");\n+  assert(to->is_valid(), \"must be\");\n+\n+  if (reg_state[to->value()] == reg_written) {\n+    skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+    return true; \/\/ Already written\n+  }\n+\n+  Register val_array = rax;\n+  Register val_obj_tmp = r11;\n+  Register from_reg_tmp = r14; \/\/ Be careful with r14 because it's used for spilling\n+  Register tmp1 = r10;\n+  Register tmp2 = r13;\n+  Register tmp3 = rbx;\n+  Register val_obj = to->is_stack() ? val_obj_tmp : to->as_Register();\n+\n+  if (reg_state[to->value()] == reg_readonly) {\n+    if (!is_reg_in_unpacked_fields(sig, sig_index, to, regs_from, regs_from_count, from_index)) {\n+      skip_unpacked_fields(sig, sig_index, regs_from, regs_from_count, from_index);\n+      return false; \/\/ Not yet writable\n+    }\n+    val_obj = val_obj_tmp;\n+  }\n+\n+  int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + vtarg_index * type2aelembytes(T_INLINE_TYPE);\n+  load_heap_oop(val_obj, Address(val_array, index));\n+\n+  ScalarizedValueArgsStream stream(sig, sig_index, regs_from, regs_from_count, from_index);\n+  VMRegPair from_pair;\n+  BasicType bt;\n+  while (stream.next(from_pair, bt)) {\n+    int off = sig->at(stream.sig_cc_index())._offset;\n+    assert(off > 0, \"offset in object should be positive\");\n+    bool is_oop = (bt == T_OBJECT || bt == T_ARRAY);\n+    size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;\n+\n+    VMReg from_r1 = from_pair.first();\n+    VMReg from_r2 = from_pair.second();\n+\n+    \/\/ Pack the scalarized field into the value object.\n+    Address dst(val_obj, off);\n+    if (!from_r1->is_XMMRegister()) {\n+      Register from_reg;\n+      if (from_r1->is_stack()) {\n+        from_reg = from_reg_tmp;\n+        int ld_off = from_r1->reg2stack() * VMRegImpl::stack_slot_size + extra_stack_offset;\n+        load_sized_value(from_reg, Address(rsp, ld_off), size_in_bytes, \/* is_signed *\/ false);\n+      } else {\n+        from_reg = from_r1->as_Register();\n+      }\n+      assert_different_registers(dst.base(), from_reg, tmp1, tmp2, tmp3, val_array);\n+      if (is_oop) {\n+        store_heap_oop(dst, from_reg, tmp1, tmp2, tmp3, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);\n+      } else {\n+        store_sized_value(dst, from_reg, size_in_bytes);\n+      }\n+    } else {\n+      if (from_r2->is_valid()) {\n+        movdbl(dst, from_r1->as_XMMRegister());\n+      } else {\n+        movflt(dst, from_r1->as_XMMRegister());\n+      }\n+    }\n+    reg_state[from_r1->value()] = reg_writable;\n+  }\n+  sig_index = stream.sig_cc_index();\n+  from_index = stream.regs_cc_index();\n+\n+  assert(reg_state[to->value()] == reg_writable, \"must have already been read\");\n+  bool success = move_helper(val_obj->as_VMReg(), to, T_OBJECT, reg_state, ret_off, extra_stack_offset);\n+  assert(success, \"to register must be writeable\");\n+\n+  return true;\n+}\n+\n+\/\/ Unpack all inline type arguments passed as oops\n+void MacroAssembler::unpack_inline_args(Compile* C, bool receiver_only) {\n+  int sp_inc = unpack_inline_args_common(C, receiver_only);\n+  \/\/ Emit code for verified entry and save increment for stack repair on return\n+  verified_entry(C, sp_inc);\n+}\n+\n+void MacroAssembler::shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                                         BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                                         int args_passed, int args_on_stack, VMRegPair* regs,\n+                                         int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc) {\n+  \/\/ Check if we need to extend the stack for packing\/unpacking\n+  if (sp_inc > 0 && !is_packing) {\n+    \/\/ Save the return address, adjust the stack (make sure it is properly\n+    \/\/ 16-byte aligned) and copy the return address to the new top of the stack.\n+    \/\/ (Note: C1 does this in C1_MacroAssembler::scalarized_entry).\n+    pop(r13);\n+    subptr(rsp, sp_inc);\n+    push(r13);\n+  }\n+\n+  int ret_off; \/\/ make sure we don't overwrite the return address\n+  if (is_packing) {\n+    \/\/ For C1 code, the VIEP doesn't have reserved slots, so we store the returned address at\n+    \/\/ rsp[0] during shuffling.\n+    ret_off = 0;\n+  } else {\n+    \/\/ C2 code ensures that sp_inc is a reserved slot.\n+    ret_off = sp_inc;\n+  }\n+\n+  shuffle_inline_args_common(is_packing, receiver_only, extra_stack_offset,\n+                             sig_bt, sig_cc,\n+                             args_passed, args_on_stack, regs,\n+                             args_passed_to, args_on_stack_to, regs_to,\n+                             sp_inc, ret_off);\n+}\n+\n+VMReg MacroAssembler::spill_reg_for(VMReg reg) {\n+  return reg->is_XMMRegister() ? xmm8->as_VMReg() : r14->as_VMReg();\n+}\n+\n+void MacroAssembler::remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset) {\n+  assert((initial_framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  if (needs_stack_repair) {\n+    movq(rbp, Address(rsp, initial_framesize));\n+    addq(rsp, Address(rsp, sp_inc_offset));\n+  } else {\n+    if (initial_framesize > 0) {\n+      addq(rsp, initial_framesize);\n+    }\n+    pop(rbp);\n+  }\n+}\n+\n+void MacroAssembler::clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only) {\n@@ -4861,1 +5564,1 @@\n-  assert(tmp==rax,   \"tmp register must be eax for rep stos\");\n+  assert(val==rax,   \"tmp register must be eax for rep stos\");\n@@ -4868,4 +5571,0 @@\n-  if (!is_large || !UseXMMForObjInit) {\n-    xorptr(tmp, tmp);\n-  }\n-\n@@ -4884,1 +5583,1 @@\n-    movptr(Address(base, cnt, Address::times_ptr), tmp);\n+    movptr(Address(base, cnt, Address::times_ptr), val);\n@@ -4893,1 +5592,1 @@\n-  if (UseFastStosb) {\n+  if (UseFastStosb && !word_copy_only) {\n@@ -4897,2 +5596,1 @@\n-    movptr(tmp, base);\n-    xmm_clear_mem(tmp, cnt, xtmp);\n+    xmm_clear_mem(base, cnt, val, xtmp);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":726,"deletions":28,"binary":false,"changes":754,"status":"modified"},{"patch":"@@ -31,0 +31,3 @@\n+#include \"runtime\/signature.hpp\"\n+\n+class ciInlineKlass;\n@@ -101,0 +104,26 @@\n+  \/\/ valueKlass queries, kills temp_reg\n+  void test_klass_is_inline_type(Register klass, Register temp_reg, Label& is_inline_type);\n+  void test_klass_is_empty_inline_type(Register klass, Register temp_reg, Label& is_empty_inline_type);\n+\n+  \/\/ Get the default value oop for the given InlineKlass\n+  void get_default_value_oop(Register inline_klass, Register temp_reg, Register obj);\n+  \/\/ The empty value oop, for the given InlineKlass (\"empty\" as in no instance fields)\n+  \/\/ get_default_value_oop with extra assertion for empty inline klass\n+  void get_empty_inline_type_oop(Register inline_klass, Register temp_reg, Register obj);\n+\n+  void test_field_is_inline_type(Register flags, Register temp_reg, Label& is_inline);\n+  void test_field_is_not_inline_type(Register flags, Register temp_reg, Label& not_inline);\n+  void test_field_is_inlined(Register flags, Register temp_reg, Label& is_inlined);\n+\n+  \/\/ Check oops array storage properties, i.e. flattened and\/or null-free\n+  void test_flattened_array_oop(Register oop, Register temp_reg, Label&is_flattened_array);\n+  void test_non_flattened_array_oop(Register oop, Register temp_reg, Label&is_non_flattened_array);\n+  void test_null_free_array_oop(Register oop, Register temp_reg, Label&is_null_free_array);\n+  void test_non_null_free_array_oop(Register oop, Register temp_reg, Label&is_non_null_free_array);\n+\n+  \/\/ Check array klass layout helper for flatten or null-free arrays...\n+  void test_flattened_array_layout(Register lh, Label& is_flattened_array);\n+  void test_non_flattened_array_layout(Register lh, Label& is_non_flattened_array);\n+  void test_null_free_array_layout(Register lh, Label& is_null_free_array);\n+  void test_non_null_free_array_layout(Register lh, Label& is_non_null_free_array);\n+\n@@ -318,0 +347,1 @@\n+  void load_metadata(Register dst, Register src);\n@@ -324,1 +354,11 @@\n-                       Register tmp1, Register tmp2);\n+                       Register tmp1, Register tmp2, Register tmp3 = noreg);\n+\n+  void access_value_copy(DecoratorSet decorators, Register src, Register dst, Register inline_klass);\n+\n+  \/\/ inline type data payload offsets...\n+  void first_field_offset(Register inline_klass, Register offset);\n+  void data_for_oop(Register oop, Register data, Register inline_klass);\n+  \/\/ get data payload ptr a flat value array at index, kills rcx and index\n+  void data_for_value_array_index(Register array, Register array_klass,\n+                                  Register index, Register data);\n+\n@@ -335,1 +375,1 @@\n-                      Register tmp2 = noreg, DecoratorSet decorators = 0);\n+                      Register tmp2 = noreg, Register tmp3 = noreg, DecoratorSet decorators = 0);\n@@ -511,0 +551,9 @@\n+\n+  \/\/ Object \/ value buffer allocation...\n+  \/\/ Allocate instance of klass, assumes klass initialized by caller\n+  \/\/ new_obj prefers to be rax\n+  \/\/ Kills t1 and t2, perserves klass, return allocation in new_obj (rsi on LP64)\n+  void allocate_instance(Register klass, Register new_obj,\n+                         Register t1, Register t2,\n+                         bool clear_fields, Label& alloc_failed);\n+\n@@ -530,0 +579,3 @@\n+  \/\/ For field \"index\" within \"klass\", return inline_klass ...\n+  void get_inline_type_field_klass(Register klass, Register index, Register inline_klass);\n+\n@@ -1596,1 +1648,28 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(Compile* C, int sp_inc = 0);\n+\n+  enum RegState {\n+    reg_readonly,\n+    reg_writable,\n+    reg_written\n+  };\n+\n+  int store_inline_type_fields_to_buf(ciInlineKlass* vk, bool from_interpreter = true);\n+\n+  \/\/ Unpack all inline type arguments passed as oops\n+  void unpack_inline_args(Compile* C, bool receiver_only);\n+  bool move_helper(VMReg from, VMReg to, BasicType bt, RegState reg_state[], int ret_off, int extra_stack_offset);\n+  bool unpack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, VMReg from, VMRegPair* regs_to, int& to_index,\n+                            RegState reg_state[], int ret_off, int extra_stack_offset);\n+  bool pack_inline_helper(const GrowableArray<SigEntry>* sig, int& sig_index, int vtarg_index,\n+                          VMReg to, VMRegPair* regs_from, int regs_from_count, int& from_index, RegState reg_state[],\n+                          int ret_off, int extra_stack_offset);\n+  void remove_frame(int initial_framesize, bool needs_stack_repair, int sp_inc_offset);\n+\n+  void shuffle_inline_args(bool is_packing, bool receiver_only, int extra_stack_offset,\n+                           BasicType* sig_bt, const GrowableArray<SigEntry>* sig_cc,\n+                           int args_passed, int args_on_stack, VMRegPair* regs,\n+                           int args_passed_to, int args_on_stack_to, VMRegPair* regs_to, int sp_inc);\n+  bool shuffle_inline_args_spill(bool is_packing,  const GrowableArray<SigEntry>* sig_cc, int sig_cc_index,\n+                                 VMRegPair* regs_from, int from_index, int regs_from_count,\n+                                 RegState* reg_state, int sp_inc, int extra_stack_offset);\n+  VMReg spill_reg_for(VMReg reg);\n@@ -1600,1 +1679,1 @@\n-  void clear_mem(Register base, Register cnt, Register rtmp, XMMRegister xtmp, bool is_large);\n+  void clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp, bool is_large, bool word_copy_only);\n@@ -1603,1 +1682,1 @@\n-  void xmm_clear_mem(Register base, Register cnt, XMMRegister xtmp);\n+  void xmm_clear_mem(Register base, Register cnt, Register val, XMMRegister xtmp);\n@@ -1733,0 +1812,2 @@\n+\n+  #include \"asm\/macroAssembler_common.hpp\"\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":86,"deletions":5,"binary":false,"changes":91,"status":"modified"},{"patch":"@@ -1539,1 +1539,1 @@\n-  if (!UseFastStosb && UseSSE >= 2 && UseUnalignedLoadStores) {\n+  if (UseSSE >= 2 && UseUnalignedLoadStores) {\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -568,0 +568,1 @@\n+, _compiled_entry_signature(method->get_Method())\n@@ -584,0 +585,4 @@\n+  {\n+    ResetNoHandleMark rnhm; \/\/ Huh? Required when doing class lookup of the Q-types\n+    _compiled_entry_signature.compute_calling_conventions();\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -94,0 +95,1 @@\n+  CompiledEntrySignature _compiled_entry_signature;\n@@ -261,0 +263,4 @@\n+  bool profile_array_accesses() {\n+    return env()->comp_level() == CompLevel_full_profile &&\n+      C1UpdateMethodData;\n+  }\n@@ -289,0 +295,7 @@\n+\n+  const CompiledEntrySignature* compiled_entry_signature() const {\n+    return &_compiled_entry_signature;\n+  }\n+  bool needs_stack_repair() const {\n+    return compiled_entry_signature()->c1_needs_stack_repair();\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -173,0 +173,3 @@\n+  if (_should_reexecute) {\n+    return true;\n+  }\n@@ -182,0 +185,19 @@\n+void IRScopeDebugInfo::record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool topmost, bool is_method_handle_invoke, bool maybe_return_as_fields) {\n+  if (caller() != NULL) {\n+    \/\/ Order is significant:  Must record caller first.\n+    caller()->record_debug_info(recorder, pc_offset, false\/*topmost*\/);\n+  }\n+  DebugToken* locvals = recorder->create_scope_values(locals());\n+  DebugToken* expvals = recorder->create_scope_values(expressions());\n+  DebugToken* monvals = recorder->create_monitor_values(monitors());\n+  \/\/ reexecute allowed only for the topmost frame\n+  bool reexecute = topmost ? should_reexecute() : false;\n+  bool return_oop = false; \/\/ This flag will be ignored since it used only for C2 with escape analysis.\n+  bool rethrow_exception = false;\n+  bool return_vt = false;\n+  if (maybe_return_as_fields) {\n+    return_oop = true;\n+    return_vt = true;\n+  }\n+  recorder->describe_scope(pc_offset, methodHandle(), scope()->method(), bci(), reexecute, rethrow_exception, is_method_handle_invoke, return_oop, return_vt, locvals, expvals, monvals);\n+}\n@@ -214,1 +236,1 @@\n-void CodeEmitInfo::record_debug_info(DebugInformationRecorder* recorder, int pc_offset) {\n+void CodeEmitInfo::record_debug_info(DebugInformationRecorder* recorder, int pc_offset, bool maybe_return_as_fields) {\n@@ -217,1 +239,1 @@\n-  _scope_debug_info->record_debug_info(recorder, pc_offset, true\/*topmost*\/, _is_method_handle_invoke);\n+  _scope_debug_info->record_debug_info(recorder, pc_offset, true\/*topmost*\/, _is_method_handle_invoke, maybe_return_as_fields);\n","filename":"src\/hotspot\/share\/c1\/c1_IR.cpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2,  1, 2, 1, -1};\n+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 2, 2, 0, 2,  1, 2, 1, 2, -1};\n@@ -68,1 +68,1 @@\n-static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, -1, 1, 1, -1};\n+static int type2spill_size[T_CONFLICT+1]={ -1, 0, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 0, 1, -1, 1, 1, 1, -1};\n@@ -263,1 +263,1 @@\n-  if (!frame_map()->finalize_frame(max_spills())) {\n+  if (!frame_map()->finalize_frame(max_spills(), compilation()->needs_stack_repair())) {\n@@ -2946,1 +2946,1 @@\n-  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info);\n+  return new IRScopeDebugInfo(cur_scope, cur_state->bci(), locals, expressions, monitors, caller_debug_info, cur_state->should_reexecute());\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -24,0 +24,1 @@\n+\n@@ -52,0 +53,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -84,0 +86,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -137,0 +140,2 @@\n+#define CONSTANT_CLASS_DESCRIPTORS        60\n+\n@@ -176,1 +181,1 @@\n-      case JVM_CONSTANT_Class : {\n+      case JVM_CONSTANT_Class: {\n@@ -506,1 +511,8 @@\n-        cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+\n+        Symbol* const name = cp->symbol_at(class_index);\n+        const unsigned int name_len = name->utf8_length();\n+        if (name->is_Q_signature()) {\n+          cp->unresolved_qdescriptor_at_put(index, class_index, num_klasses++);\n+        } else {\n+          cp->unresolved_klass_at_put(index, class_index, num_klasses++);\n+        }\n@@ -760,2 +772,2 @@\n-            if (ref_kind == JVM_REF_newInvokeSpecial) {\n-              if (name != vmSymbols::object_initializer_name()) {\n+            if (name != vmSymbols::object_initializer_name()) {\n+              if (ref_kind == JVM_REF_newInvokeSpecial) {\n@@ -767,1 +779,12 @@\n-              if (name == vmSymbols::object_initializer_name()) {\n+              \/\/ The allowed invocation mode of <init> depends on its signature.\n+              \/\/ This test corresponds to verify_invoke_instructions in the verifier.\n+              const int signature_ref_index =\n+                cp->signature_ref_index_at(name_and_type_ref_index);\n+              const Symbol* const signature = cp->symbol_at(signature_ref_index);\n+              if (signature->is_void_method_signature()\n+                  && ref_kind == JVM_REF_newInvokeSpecial) {\n+                \/\/ OK, could be a constructor call\n+              } else if (!signature->is_void_method_signature()\n+                         && ref_kind == JVM_REF_invokeStatic) {\n+                \/\/ also OK, could be a static factory call\n+              } else {\n@@ -926,3 +949,4 @@\n-void ClassFileParser::parse_interfaces(const ClassFileStream* const stream,\n-                                       const int itfs_len,\n-                                       ConstantPool* const cp,\n+void ClassFileParser::parse_interfaces(const ClassFileStream* stream,\n+                                       int itfs_len,\n+                                       ConstantPool* cp,\n+                                       bool is_inline_type,\n@@ -930,0 +954,7 @@\n+                                       \/\/ FIXME: lots of these functions\n+                                       \/\/ declare their parameters as const,\n+                                       \/\/ which adds only noise to the code.\n+                                       \/\/ Remove the spurious const modifiers.\n+                                       \/\/ Many are of the form \"const int x\"\n+                                       \/\/ or \"T* const x\".\n+                                       bool* const is_declared_atomic,\n@@ -936,1 +967,1 @@\n-    _local_interfaces = Universe::the_empty_instance_klass_array();\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(0);\n@@ -939,3 +970,2 @@\n-    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n-\n-    int index;\n+    _temp_local_interfaces = new GrowableArray<InstanceKlass*>(itfs_len);\n+    int index = 0;\n@@ -959,1 +989,1 @@\n-        \/\/ Call resolve_super so classcircularity is checked\n+        \/\/ Call resolve_super so class circularity is checked\n@@ -977,1 +1007,14 @@\n-      if (InstanceKlass::cast(interf)->has_nonstatic_concrete_methods()) {\n+      InstanceKlass* ik = InstanceKlass::cast(interf);\n+      if (is_inline_type && ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"Inline type %s attempts to implement interface java.lang.IdentityObject\",\n+          _class_name->as_klass_external_name());\n+        return;\n+      }\n+      if (ik->invalid_inline_super()) {\n+        set_invalid_inline_super();\n+      }\n+      if (ik->has_nonstatic_concrete_methods()) {\n@@ -980,1 +1023,7 @@\n-      _local_interfaces->at_put(index, InstanceKlass::cast(interf));\n+      if (ik->is_declared_atomic()) {\n+        *is_declared_atomic = true;\n+      }\n+      if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+        _implements_identityObject = true;\n+      }\n+      _temp_local_interfaces->append(ik);\n@@ -998,1 +1047,1 @@\n-        const InstanceKlass* const k = _local_interfaces->at(index);\n+        const InstanceKlass* const k = _temp_local_interfaces->at(index);\n@@ -1474,0 +1523,1 @@\n+  STATIC_INLINE,        \/\/ inline type field\n@@ -1479,0 +1529,1 @@\n+  NONSTATIC_INLINE,\n@@ -1498,6 +1549,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  NONSTATIC_OOP,       \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20,\n@@ -1518,6 +1570,7 @@\n-  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 14,\n-  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 15,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 16,\n-  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 17,\n-  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 18,\n-  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 19,\n+  STATIC_OOP,          \/\/ T_INLINE_TYPE = 14,\n+  BAD_ALLOCATION_TYPE, \/\/ T_VOID        = 15,\n+  BAD_ALLOCATION_TYPE, \/\/ T_ADDRESS     = 16,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWOOP   = 17,\n+  BAD_ALLOCATION_TYPE, \/\/ T_METADATA    = 18,\n+  BAD_ALLOCATION_TYPE, \/\/ T_NARROWKLASS = 19,\n+  BAD_ALLOCATION_TYPE, \/\/ T_CONFLICT    = 20\n@@ -1526,1 +1579,1 @@\n-static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type) {\n+static FieldAllocationType basic_type_to_atype(bool is_static, BasicType type, bool is_inline_type) {\n@@ -1530,0 +1583,3 @@\n+  if (is_inline_type) {\n+    result = is_static ? STATIC_INLINE : NONSTATIC_INLINE;\n+  }\n@@ -1543,2 +1599,2 @@\n-  FieldAllocationType update(bool is_static, BasicType type) {\n-    FieldAllocationType atype = basic_type_to_atype(is_static, type);\n+  FieldAllocationType update(bool is_static, BasicType type, bool is_inline_type) {\n+    FieldAllocationType atype = basic_type_to_atype(is_static, type, is_inline_type);\n@@ -1558,0 +1614,1 @@\n+                                   bool is_inline_type,\n@@ -1580,1 +1637,5 @@\n-  const int total_fields = length + num_injected;\n+\n+  \/\/ two more slots are required for inline classes:\n+  \/\/ one for the static field with a reference to the pre-allocated default value\n+  \/\/ one for the field the JVM injects when detecting an empty inline class\n+  const int total_fields = length + num_injected + (is_inline_type ? 2 : 0);\n@@ -1610,0 +1671,1 @@\n+  int instance_fields_count = 0;\n@@ -1614,0 +1676,4 @@\n+    jint recognized_modifiers = JVM_RECOGNIZED_FIELD_MODIFIERS;\n+\n+    const jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+    verify_legal_field_modifiers(flags, is_interface, is_inline_type, CHECK);\n@@ -1615,2 +1681,0 @@\n-    const jint flags = cfs->get_u2_fast() & JVM_RECOGNIZED_FIELD_MODIFIERS;\n-    verify_legal_field_modifiers(flags, is_interface, CHECK);\n@@ -1632,0 +1696,1 @@\n+    if (!access_flags.is_static()) instance_fields_count++;\n@@ -1691,1 +1756,1 @@\n-    const FieldAllocationType atype = fac->update(is_static, type);\n+    const FieldAllocationType atype = fac->update(is_static, type, type == T_INLINE_TYPE);\n@@ -1736,1 +1801,1 @@\n-      const FieldAllocationType atype = fac->update(false, type);\n+      const FieldAllocationType atype = fac->update(false, type, false);\n@@ -1742,0 +1807,29 @@\n+  if (is_inline_type) {\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL | JVM_ACC_STATIC,\n+                      vmSymbols::default_value_name_enum,\n+                      vmSymbols::object_signature_enum,\n+                      0);\n+    const BasicType type = Signature::basic_type(vmSymbols::object_signature());\n+    const FieldAllocationType atype = fac->update(true, type, false);\n+    field->set_allocation_type(atype);\n+    index++;\n+  }\n+\n+  if (is_inline_type && instance_fields_count == 0) {\n+    _is_empty_inline_type = true;\n+    FieldInfo* const field = FieldInfo::from_field_array(fa, index);\n+    field->initialize(JVM_ACC_FIELD_INTERNAL,\n+        vmSymbols::empty_marker_name_enum,\n+        vmSymbols::byte_signature_enum,\n+        0);\n+    const BasicType type = Signature::basic_type(vmSymbols::byte_signature());\n+    const FieldAllocationType atype = fac->update(false, type, false);\n+    field->set_allocation_type(atype);\n+    index++;\n+  }\n+\n+  if (instance_fields_count > 0) {\n+    _has_nonstatic_fields = true;\n+  }\n+\n@@ -2057,0 +2151,5 @@\n+  const char* class_note = \"\";\n+  if (is_inline_type() && name == vmSymbols::object_initializer_name()) {\n+    class_note = \" (an inline class)\";\n+  }\n+\n@@ -2060,2 +2159,2 @@\n-      \"%s \\\"%s\\\" in class %s has illegal signature \\\"%s\\\"\", type,\n-      name->as_C_string(), _class_name->as_C_string(), sig->as_C_string());\n+      \"%s \\\"%s\\\" in class %s%s has illegal signature \\\"%s\\\"\", type,\n+      name->as_C_string(), _class_name->as_C_string(), class_note, sig->as_C_string());\n@@ -2326,0 +2425,1 @@\n+                                      bool is_inline_type,\n@@ -2366,5 +2466,50 @@\n-    verify_legal_method_modifiers(flags, is_interface, name, CHECK_NULL);\n-  }\n-\n-  if (name == vmSymbols::object_initializer_name() && is_interface) {\n-    classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", CHECK_NULL);\n+    verify_legal_method_modifiers(flags, is_interface, is_inline_type, name, CHECK_NULL);\n+  }\n+\n+  if (name == vmSymbols::object_initializer_name()) {\n+    if (is_interface) {\n+      classfile_parse_error(\"Interface cannot have a method named <init>, class file %s\", CHECK_NULL);\n+    } else if (!is_inline_type && signature->is_void_method_signature()) {\n+      \/\/ OK, a constructor\n+    } else if (is_inline_type && !signature->is_void_method_signature()) {\n+      \/\/ also OK, a static factory, as long as the return value is good\n+      bool ok = false;\n+      SignatureStream ss((Symbol*) signature, true);\n+      while (!ss.at_return_type())  ss.next();\n+      if (ss.is_reference()) {\n+        Symbol* ret = ss.as_symbol();\n+        const Symbol* required = class_name();\n+        if (is_hidden()) {\n+          \/\/ The original class name in hidden classes gets changed.  So using\n+          \/\/ the original name in the return type is no longer valid.\n+          \/\/ Note that expecting the return type for inline hidden class factory\n+          \/\/ methods to be java.lang.Object works around a JVM Spec issue for\n+          \/\/ hidden classes.\n+          required = vmSymbols::java_lang_Object();\n+        }\n+        ok = (ret == required);\n+      }\n+      if (!ok) {\n+        throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+      }\n+    } else {\n+      \/\/ not OK, so throw the same error as in verify_legal_method_signature.\n+      throwIllegalSignature(\"Method\", name, signature, CHECK_0);\n+    }\n+    \/\/ A declared <init> method must always be either a non-static\n+    \/\/ object constructor, with a void return, or else it must be a\n+    \/\/ static factory method, with a non-void return.  No other\n+    \/\/ definition of <init> is possible.\n+    \/\/\n+    \/\/ The verifier (in verify_invoke_instructions) will inspect the\n+    \/\/ signature of any attempt to invoke <init>, and ensures that it\n+    \/\/ returns non-void if and only if it is being invoked by\n+    \/\/ invokestatic, and void if and only if it is being invoked by\n+    \/\/ invokespecial.\n+    \/\/\n+    \/\/ When a symbolic reference to <init> is resolved for a\n+    \/\/ particular invocation mode (special or static), the mode is\n+    \/\/ matched to the JVM_ACC_STATIC modifier of the <init> method.\n+    \/\/ Thus, it is impossible to statically invoke a constructor, and\n+    \/\/ impossible to \"new + invokespecial\" a static factory, either\n+    \/\/ through bytecode or through reflection.\n@@ -2921,0 +3066,1 @@\n+                                    bool is_inline_type,\n@@ -2945,0 +3091,1 @@\n+                                    is_inline_type,\n@@ -3137,2 +3284,2 @@\n-    \/\/ Access flags\n-    jint flags;\n+\n+    jint recognized_modifiers = RECOGNIZED_INNER_CLASS_MODIFIERS;\n@@ -3141,3 +3288,5 @@\n-      flags = cfs->get_u2_fast() & (RECOGNIZED_INNER_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-    } else {\n-      flags = cfs->get_u2_fast() & RECOGNIZED_INNER_CLASS_MODIFIERS;\n+      recognized_modifiers |= JVM_ACC_MODULE;\n+    }\n+    \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+    if (supports_inline_types()) {\n+      recognized_modifiers |= JVM_ACC_INLINE;\n@@ -3145,0 +3294,4 @@\n+\n+    \/\/ Access flags\n+    jint flags = cfs->get_u2_fast() & recognized_modifiers;\n+\n@@ -3517,3 +3670,2 @@\n-  return _major_version == JVM_CLASSFILE_MAJOR_VERSION &&\n-         _minor_version == JAVA_PREVIEW_MINOR_VERSION &&\n-         Arguments::enable_preview();\n+  \/\/ temporarily disable the sealed type preview feature check\n+  return _major_version == JVM_CLASSFILE_MAJOR_VERSION;\n@@ -4004,1 +4156,2 @@\n-    check_property(_class_name == vmSymbols::java_lang_Object(),\n+    check_property(_class_name == vmSymbols::java_lang_Object()\n+                   || (_access_flags.get_flags() & JVM_ACC_INLINE),\n@@ -4147,0 +4300,19 @@\n+void ClassFileParser::throwInlineTypeLimitation(THREAD_AND_LOCATION_DECL,\n+                                                const char* msg,\n+                                                const Symbol* name,\n+                                                const Symbol* sig) const {\n+\n+  ResourceMark rm(THREAD);\n+  if (name == NULL || sig == NULL) {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"class: %s - %s\", _class_name->as_C_string(), msg);\n+  }\n+  else {\n+    Exceptions::fthrow(THREAD_AND_LOCATION_ARGS,\n+        vmSymbols::java_lang_ClassFormatError(),\n+        \"\\\"%s\\\" sig: \\\"%s\\\" class: %s - %s\", name->as_C_string(), sig->as_C_string(),\n+        _class_name->as_C_string(), msg);\n+  }\n+}\n+\n@@ -4180,0 +4352,5 @@\n+      if (ik->is_inline_klass()) {\n+        Thread *THREAD = Thread::current();\n+        throwInlineTypeLimitation(THREAD_AND_LOCATION, \"Inline Types do not support Cloneable\");\n+        return;\n+      }\n@@ -4220,0 +4397,5 @@\n+bool ClassFileParser::supports_inline_types() const {\n+  \/\/ Inline types are only supported by class file version 55 and later\n+  return _major_version >= JAVA_11_VERSION;\n+}\n+\n@@ -4263,3 +4445,4 @@\n-  } else if (max_transitive_size == local_size) {\n-    \/\/ only local interfaces added, share local interface array\n-    return local_ifs;\n+    \/\/ The three lines below are commented to work around bug JDK-8245487\n+\/\/  } else if (max_transitive_size == local_size) {\n+\/\/    \/\/ only local interfaces added, share local interface array\n+\/\/    return local_ifs;\n@@ -4286,0 +4469,5 @@\n+\n+    if (length == 1 && result->at(0) == SystemDictionary::IdentityObject_klass()) {\n+      return Universe::the_single_IdentityObject_klass_array();\n+    }\n+\n@@ -4518,0 +4706,1 @@\n+  const bool is_inline_type = (flags & JVM_ACC_INLINE) != 0;\n@@ -4519,0 +4708,1 @@\n+  assert(supports_inline_types() || !is_inline_type, \"JVM_ACC_INLINE should not be set\");\n@@ -4529,0 +4719,10 @@\n+  if (is_inline_type && !EnableValhalla) {\n+    ResourceMark rm(THREAD);\n+    Exceptions::fthrow(\n+      THREAD_AND_LOCATION,\n+      vmSymbols::java_lang_ClassFormatError(),\n+      \"Class modifier ACC_INLINE in class %s requires option -XX:+EnableValhalla\",\n+      _class_name->as_C_string()\n+    );\n+  }\n+\n@@ -4543,1 +4743,2 @@\n-      (!is_interface && major_gte_1_5 && is_annotation)) {\n+      (!is_interface && major_gte_1_5 && is_annotation) ||\n+      (is_inline_type && (is_interface || is_abstract || is_enum || !is_final))) {\n@@ -4545,0 +4746,2 @@\n+    const char* class_note = \"\";\n+    if (is_inline_type)  class_note = \" (an inline class)\";\n@@ -4548,2 +4751,2 @@\n-      \"Illegal class modifiers in class %s: 0x%X\",\n-      _class_name->as_C_string(), flags\n+      \"Illegal class modifiers in class %s%s: 0x%X\",\n+      _class_name->as_C_string(), class_note, flags\n@@ -4628,0 +4831,1 @@\n+                                                   bool is_inline_type,\n@@ -4652,0 +4856,4 @@\n+    } else {\n+      if (is_inline_type && !is_static && !is_final) {\n+        is_illegal = true;\n+      }\n@@ -4668,0 +4876,1 @@\n+                                                    bool is_inline_type,\n@@ -4688,0 +4897,2 @@\n+  const char* class_note = \"\";\n+\n@@ -4722,1 +4933,1 @@\n-        if (is_static || is_final || is_synchronized || is_native ||\n+        if (is_final || is_synchronized || is_native ||\n@@ -4726,0 +4937,9 @@\n+        if (!is_static && !is_inline_type) {\n+          \/\/ OK, an object constructor in a regular class\n+        } else if (is_static && is_inline_type) {\n+          \/\/ OK, a static init factory in an inline class\n+        } else {\n+          \/\/ but no other combinations are allowed\n+          is_illegal = true;\n+          class_note = (is_inline_type ? \" (an inline class)\" : \" (not an inline class)\");\n+        }\n@@ -4727,4 +4947,9 @@\n-        if (is_abstract) {\n-          if ((is_final || is_native || is_private || is_static ||\n-              (major_gte_1_5 && (is_synchronized || is_strict)))) {\n-            is_illegal = true;\n+        if (is_inline_type && is_synchronized && !is_static) {\n+          is_illegal = true;\n+          class_note = \" (an inline class)\";\n+        } else {\n+          if (is_abstract) {\n+            if ((is_final || is_native || is_private || is_static ||\n+                (major_gte_1_5 && (is_synchronized || is_strict)))) {\n+              is_illegal = true;\n+            }\n@@ -4742,2 +4967,2 @@\n-      \"Method %s in class %s has illegal modifiers: 0x%X\",\n-      name->as_C_string(), _class_name->as_C_string(), flags);\n+      \"Method %s in class %s%s has illegal modifiers: 0x%X\",\n+      name->as_C_string(), _class_name->as_C_string(), class_note, flags);\n@@ -4901,1 +5126,10 @@\n-    case JVM_SIGNATURE_CLASS: {\n+    case JVM_SIGNATURE_INLINE_TYPE:\n+      \/\/ Can't enable this check until JDK upgrades the bytecode generators\n+      \/\/ if (_major_version < CONSTANT_CLASS_DESCRIPTORS ) {\n+      \/\/   classfile_parse_error(\"Class name contains illegal Q-signature \"\n+      \/\/                                    \"in descriptor in class file %s\",\n+      \/\/                                    CHECK_0);\n+      \/\/ }\n+      \/\/ fall through\n+    case JVM_SIGNATURE_CLASS:\n+    {\n@@ -4912,1 +5146,1 @@\n-        \/\/ Skip leading 'L' and ignore first appearance of ';'\n+        \/\/ Skip leading 'L' or 'Q' and ignore first appearance of ';'\n@@ -4967,0 +5201,3 @@\n+    } else if (_major_version >= CONSTANT_CLASS_DESCRIPTORS && bytes[length - 1] == ';' ) {\n+      \/\/ Support for L...; and Q...; descriptors\n+      legal = verify_unqualified_name(bytes + 1, length - 2, LegalClass);\n@@ -5116,1 +5353,1 @@\n-        \/\/ All internal methods must return void\n+        \/\/ All constructor methods must return void\n@@ -5120,0 +5357,16 @@\n+        \/\/ All static init methods must return the current class\n+        if ((length >= 3) && (p[length-1] == JVM_SIGNATURE_ENDCLASS)\n+            && name == vmSymbols::object_initializer_name()) {\n+          nextp = skip_over_field_signature(p, true, length, CHECK_0);\n+          if (nextp && ((int)length == (nextp - p))) {\n+            \/\/ The actual class will be checked against current class\n+            \/\/ when the method is defined (see parse_method).\n+            \/\/ A reference to a static init with a bad return type\n+            \/\/ will load and verify OK, but will fail to link.\n+            return args_size;\n+          }\n+        }\n+        \/\/ The distinction between static factory methods and\n+        \/\/ constructors depends on the JVM_ACC_STATIC modifier.\n+        \/\/ This distinction must be reflected in a void or non-void\n+        \/\/ return. For declared methods, the check is in parse_method.\n@@ -5277,0 +5530,6 @@\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -5280,0 +5539,34 @@\n+\/\/ Return true if the specified class is not a valid super class for an inline type.\n+\/\/ A valid super class for an inline type is abstract, has no instance fields,\n+\/\/ does not implement interface java.lang.IdentityObject (checked elsewhere), has\n+\/\/ an empty body-less no-arg constructor, and no synchronized instance methods.\n+\/\/ This function doesn't check if the class's super types are invalid.  Those checks\n+\/\/ are done elsewhere.  The final determination of whether or not a class is an\n+\/\/ invalid super type for an inline class is done in fill_instance_klass().\n+bool ClassFileParser::is_invalid_super_for_inline_type() {\n+  if (class_name() == vmSymbols::java_lang_IdentityObject()) {\n+    return true;\n+  }\n+  if (is_interface() || class_name() == vmSymbols::java_lang_Object()) {\n+    return false;\n+  }\n+  if (!access_flags().is_abstract() || _has_nonstatic_fields) {\n+    return true;\n+  } else {\n+    \/\/ Look at each method\n+    for (int x = 0; x < _methods->length(); x++) {\n+      const Method* const method = _methods->at(x);\n+      if (method->is_synchronized() && !method->is_static()) {\n+        return true;\n+\n+      } else if (method->name() == vmSymbols::object_initializer_name()) {\n+        if (method->signature() != vmSymbols::void_method_signature() ||\n+            !method->is_vanilla_constructor()) {\n+          return true;\n+        }\n+      }\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -5312,0 +5605,15 @@\n+  if (_field_info->_is_naturally_atomic && ik->is_inline_klass()) {\n+    ik->set_is_naturally_atomic();\n+  }\n+  if (_is_empty_inline_type) {\n+    ik->set_is_empty_inline_type();\n+  }\n+\n+  if (this->_invalid_inline_super) {\n+    ik->set_invalid_inline_super();\n+  }\n+\n+  if (_has_injected_identityObject) {\n+    ik->set_has_injected_identityObject();\n+  }\n+\n@@ -5313,1 +5621,1 @@\n-  ik->set_static_oop_field_count(_fac->count[STATIC_OOP]);\n+  ik->set_static_oop_field_count(_fac->count[STATIC_OOP] + _fac->count[STATIC_INLINE]);\n@@ -5363,0 +5671,3 @@\n+  if (_is_declared_atomic) {\n+    ik->set_is_declared_atomic();\n+  }\n@@ -5474,0 +5785,30 @@\n+  int nfields = ik->java_fields_count();\n+  if (ik->is_inline_klass()) nfields++;\n+  for (int i = 0; i < nfields; i++) {\n+    if (ik->field_is_inline_type(i) && ((ik->field_access_flags(i) & JVM_ACC_STATIC) == 0)) {\n+      Symbol* klass_name = ik->field_signature(i)->fundamental_name(CHECK);\n+      \/\/ Inline classes for instance fields must have been pre-loaded\n+      \/\/ Inline classes for static fields might not have been loaded yet\n+      Klass* klass = SystemDictionary::find(klass_name,\n+          Handle(THREAD, ik->class_loader()),\n+          Handle(THREAD, ik->protection_domain()), CHECK);\n+      if (klass != NULL) {\n+        assert(klass->access_flags().is_inline_type(), \"Inline type expected\");\n+        ik->set_inline_type_field_klass(i, klass);\n+      }\n+      klass_name->decrement_refcount();\n+    } else\n+      if (is_inline_type() && ((ik->field_access_flags(i) & JVM_ACC_FIELD_INTERNAL) != 0)\n+        && ((ik->field_access_flags(i) & JVM_ACC_STATIC) != 0)) {\n+      InlineKlass::cast(ik)->set_default_value_offset(ik->field_offset(i));\n+    }\n+  }\n+\n+  if (is_inline_type()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    vk->set_alignment(_alignment);\n+    vk->set_first_field_offset(_first_field_offset);\n+    vk->set_exact_size_in_bytes(_exact_size_in_bytes);\n+    InlineKlass::cast(ik)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -5519,0 +5860,4 @@\n+  if (ik->name() == vmSymbols::java_lang_IdentityObject()) {\n+    Universe::initialize_the_single_IdentityObject_klass_array(ik, CHECK);\n+  }\n+\n@@ -5621,0 +5966,1 @@\n+  _temp_local_interfaces(NULL),\n@@ -5660,0 +6006,9 @@\n+  _has_inline_type_fields(false),\n+  _has_nonstatic_fields(false),\n+  _is_empty_inline_type(false),\n+  _is_naturally_atomic(false),\n+  _is_declared_atomic(false),\n+  _invalid_inline_super(false),\n+  _invalid_identity_super(false),\n+  _implements_identityObject(false),\n+  _has_injected_identityObject(false),\n@@ -5870,2 +6225,1 @@\n-  \/\/ Access flags\n-  jint flags;\n+  jint recognized_modifiers = JVM_RECOGNIZED_CLASS_MODIFIERS;\n@@ -5874,3 +6228,5 @@\n-    flags = stream->get_u2_fast() & (JVM_RECOGNIZED_CLASS_MODIFIERS | JVM_ACC_MODULE);\n-  } else {\n-    flags = stream->get_u2_fast() & JVM_RECOGNIZED_CLASS_MODIFIERS;\n+    recognized_modifiers |= JVM_ACC_MODULE;\n+  }\n+  \/\/ JVM_ACC_INLINE is defined for class file version 55 and later\n+  if (supports_inline_types()) {\n+    recognized_modifiers |= JVM_ACC_INLINE;\n@@ -5879,0 +6235,3 @@\n+  \/\/ Access flags\n+  jint flags = stream->get_u2_fast() & recognized_modifiers;\n+\n@@ -5998,0 +6357,1 @@\n+                   is_inline_type(),\n@@ -5999,0 +6359,1 @@\n+                   &_is_declared_atomic,\n@@ -6001,1 +6362,1 @@\n-  assert(_local_interfaces != NULL, \"invariant\");\n+  assert(_temp_local_interfaces != NULL, \"invariant\");\n@@ -6006,1 +6367,2 @@\n-               _access_flags.is_interface(),\n+               is_interface(),\n+               is_inline_type(),\n@@ -6018,1 +6380,2 @@\n-                _access_flags.is_interface(),\n+                is_interface(),\n+                is_inline_type(),\n@@ -6089,3 +6452,3 @@\n-    check_property(_local_interfaces == Universe::the_empty_instance_klass_array(),\n-                   \"java.lang.Object cannot implement an interface in class file %s\",\n-                   CHECK);\n+    check_property(_temp_local_interfaces->length() == 0,\n+        \"java.lang.Object cannot implement an interface in class file %s\",\n+        CHECK);\n@@ -6096,1 +6459,1 @@\n-    if (_access_flags.is_interface()) {\n+    if (is_interface()) {\n@@ -6117,0 +6480,3 @@\n+    if (_super_klass->is_declared_atomic()) {\n+      _is_declared_atomic = true;\n+    }\n@@ -6129,0 +6495,28 @@\n+\n+    \/\/ For an inline class, only java\/lang\/Object or special abstract classes\n+    \/\/ are acceptable super classes.\n+    if (is_inline_type()) {\n+      const InstanceKlass* super_ik = _super_klass;\n+      if (super_ik->invalid_inline_super()) {\n+        ResourceMark rm(THREAD);\n+        Exceptions::fthrow(\n+          THREAD_AND_LOCATION,\n+          vmSymbols::java_lang_IncompatibleClassChangeError(),\n+          \"inline class %s has an invalid super class %s\",\n+          _class_name->as_klass_external_name(),\n+          _super_klass->external_name());\n+        return;\n+      }\n+    }\n+  }\n+\n+  if (_class_name == vmSymbols::java_lang_NonTearable() && _loader_data->class_loader() == NULL) {\n+    \/\/ This is the original source of this condition.\n+    \/\/ It propagates by inheritance, as if testing \"instanceof NonTearable\".\n+    _is_declared_atomic = true;\n+  } else if (*ForceNonTearable != '\\0') {\n+    \/\/ Allow a command line switch to force the same atomicity property:\n+    const char* class_name_str = _class_name->as_C_string();\n+    if (StringUtils::class_list_match(ForceNonTearable, class_name_str)) {\n+      _is_declared_atomic = true;\n+    }\n@@ -6131,0 +6525,28 @@\n+  \/\/ Set ik->invalid_inline_super field to TRUE if already marked as invalid,\n+  \/\/ if super is marked invalid, or if is_invalid_super_for_inline_type()\n+  \/\/ returns true\n+  if (invalid_inline_super() ||\n+      (_super_klass != NULL && _super_klass->invalid_inline_super()) ||\n+      is_invalid_super_for_inline_type()) {\n+    set_invalid_inline_super();\n+  }\n+\n+  if (!is_inline_type() && invalid_inline_super() && (_super_klass == NULL || !_super_klass->invalid_inline_super())\n+      && !_implements_identityObject && class_name() != vmSymbols::java_lang_IdentityObject()) {\n+    _temp_local_interfaces->append(SystemDictionary::IdentityObject_klass());\n+    _has_injected_identityObject = true;\n+  }\n+  int itfs_len = _temp_local_interfaces->length();\n+  if (itfs_len == 0) {\n+    _local_interfaces = Universe::the_empty_instance_klass_array();\n+  } else if (itfs_len == 1 && _temp_local_interfaces->at(0) == SystemDictionary::IdentityObject_klass()) {\n+    _local_interfaces = Universe::the_single_IdentityObject_klass_array();\n+  } else {\n+    _local_interfaces = MetadataFactory::new_array<InstanceKlass*>(_loader_data, itfs_len, NULL, CHECK);\n+    for (int i = 0; i < itfs_len; i++) {\n+      _local_interfaces->at_put(i, _temp_local_interfaces->at(i));\n+    }\n+  }\n+  _temp_local_interfaces = NULL;\n+  assert(_local_interfaces != NULL, \"invariant\");\n+\n@@ -6159,1 +6581,1 @@\n-  _itable_size = _access_flags.is_interface() ? 0 :\n+  _itable_size = is_interface() ? 0 :\n@@ -6165,0 +6587,12 @@\n+\n+  for (AllFieldStream fs(_fields, cp); !fs.done(); fs.next()) {\n+    if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE  && !fs.access_flags().is_static()) {\n+      \/\/ Pre-load inline class\n+      Klass* klass = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+          Handle(THREAD, _loader_data->class_loader()),\n+          _protection_domain, true, CHECK);\n+      assert(klass != NULL, \"Sanity check\");\n+      assert(klass->access_flags().is_inline_type(), \"Value type expected\");\n+    }\n+  }\n+\n@@ -6167,2 +6601,9 @@\n-                        _parsed_annotations->is_contended(), _field_info);\n-  lb.build_layout();\n+      _parsed_annotations->is_contended(), is_inline_type(),\n+      loader_data(), _protection_domain, _field_info);\n+  lb.build_layout(CHECK);\n+  if (is_inline_type()) {\n+    _alignment = lb.get_alignment();\n+    _first_field_offset = lb.get_first_field_offset();\n+    _exact_size_in_bytes = lb.get_exact_size_in_byte();\n+  }\n+  _has_inline_type_fields = _field_info->_has_inline_fields;\n@@ -6170,1 +6611,1 @@\n-  \/\/ Compute reference typ\n+  \/\/ Compute reference type\n@@ -6172,1 +6613,0 @@\n-\n@@ -6204,0 +6644,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":527,"deletions":86,"binary":false,"changes":613,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -72,0 +73,1 @@\n+#include \"oops\/inlineKlass.inline.hpp\"\n@@ -80,0 +82,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -327,1 +330,1 @@\n-    \/\/ Ignore wrapping L and ;.\n+    \/\/ Ignore wrapping L and ;. (and Q and ; for value types);\n@@ -359,0 +362,4 @@\n+      if ((class_name->is_Q_array_signature() && !k->is_inline_klass()) ||\n+          (!class_name->is_Q_array_signature() && k->is_inline_klass())) {\n+            THROW_MSG_NULL(vmSymbols::java_lang_IncompatibleClassChangeError(), \"L\/Q mismatch on bottom type\");\n+          }\n@@ -368,1 +375,0 @@\n-\n@@ -512,0 +518,45 @@\n+Klass* SystemDictionary::resolve_inline_type_field_or_fail(AllFieldStream* fs,\n+                                                           Handle class_loader,\n+                                                           Handle protection_domain,\n+                                                           bool throw_error,\n+                                                           TRAPS) {\n+  Symbol* class_name = fs->signature()->fundamental_name(THREAD);\n+  class_loader = Handle(THREAD, java_lang_ClassLoader::non_reflection_class_loader(class_loader()));\n+  ClassLoaderData* loader_data = class_loader_data(class_loader);\n+  unsigned int p_hash = placeholders()->compute_hash(class_name);\n+  int p_index = placeholders()->hash_to_index(p_hash);\n+  bool throw_circularity_error = false;\n+  PlaceholderEntry* oldprobe;\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    oldprobe = placeholders()->get_entry(p_index, p_hash, class_name, loader_data);\n+    if (oldprobe != NULL &&\n+      oldprobe->check_seen_thread(THREAD, PlaceholderTable::INLINE_TYPE_FIELD)) {\n+      throw_circularity_error = true;\n+\n+    } else {\n+      placeholders()->find_and_add(p_index, p_hash, class_name, loader_data,\n+                                   PlaceholderTable::INLINE_TYPE_FIELD, NULL, THREAD);\n+    }\n+  }\n+\n+  Klass* klass = NULL;\n+  if (!throw_circularity_error) {\n+    klass = SystemDictionary::resolve_or_fail(class_name, class_loader,\n+                                               protection_domain, true, THREAD);\n+  } else {\n+    ResourceMark rm(THREAD);\n+    THROW_MSG_NULL(vmSymbols::java_lang_ClassCircularityError(), class_name->as_C_string());\n+  }\n+\n+  {\n+    MutexLocker mu(THREAD, SystemDictionary_lock);\n+    placeholders()->find_and_remove(p_index, p_hash, class_name, loader_data,\n+                                    PlaceholderTable::INLINE_TYPE_FIELD, THREAD);\n+  }\n+\n+  class_name->decrement_refcount();\n+  return klass;\n+}\n+\n@@ -1040,1 +1091,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n@@ -1424,0 +1475,18 @@\n+\n+  if (ik->has_inline_type_fields()) {\n+    for (AllFieldStream fs(ik->fields(), ik->constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        if (!fs.access_flags().is_static()) {\n+          \/\/ Pre-load inline class\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            class_loader, protection_domain, true, CHECK_NULL);\n+          Klass* k = ik->get_inline_type_field_klass_or_null(fs.index());\n+          if (real_k != k) {\n+            \/\/ oops, the app has substituted a different version of k!\n+            return NULL;\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1460,0 +1529,7 @@\n+\n+  if (ik->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(ik);\n+    oop val = ik->allocate_instance(CHECK_NULL);\n+    vk->set_default_value(val);\n+  }\n+\n@@ -1514,0 +1590,15 @@\n+  if (klass->has_inline_type_fields()) {\n+    for (AllFieldStream fs(klass->fields(), klass->constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        if (!fs.access_flags().is_static()) {\n+          Klass* real_k = SystemDictionary::resolve_inline_type_field_or_fail(&fs,\n+            Handle(THREAD, loader_data->class_loader()), domain, true, CHECK);\n+          Klass* k = klass->get_inline_type_field_klass_or_null(fs.index());\n+          assert(real_k == k, \"oops, the app has substituted a different version of k!\");\n+        } else {\n+          klass->reset_inline_type_field_klass(fs.index());\n+        }\n+      }\n+    }\n+  }\n+\n@@ -2337,1 +2428,1 @@\n-    if (t != T_OBJECT) {\n+    if (t != T_OBJECT && t != T_INLINE_TYPE) {\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":95,"deletions":4,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -205,0 +205,10 @@\n+  bool  needs_stack_repair() const {\n+    if (is_compiled_by_c1()) {\n+      return method()->c1_needs_stack_repair();\n+    } else if (is_compiled_by_c2()) {\n+      return method()->c2_needs_stack_repair();\n+    } else {\n+      return false;\n+    }\n+  }\n+\n@@ -221,0 +231,2 @@\n+  virtual address verified_inline_entry_point() const = 0;\n+  virtual address verified_inline_ro_entry_point() const = 0;\n@@ -227,0 +239,1 @@\n+  virtual address inline_entry_point() const = 0;\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-ScopeDesc::ScopeDesc(const CompiledMethod* code, int decode_offset, int obj_decode_offset, bool reexecute, bool rethrow_exception, bool return_oop) {\n+ScopeDesc::ScopeDesc(const CompiledMethod* code, int decode_offset, int obj_decode_offset, bool reexecute, bool rethrow_exception, bool return_oop, bool return_vt) {\n@@ -41,0 +41,1 @@\n+  _return_vt     = return_vt;\n@@ -44,1 +45,1 @@\n-ScopeDesc::ScopeDesc(const CompiledMethod* code, int decode_offset, bool reexecute, bool rethrow_exception, bool return_oop) {\n+ScopeDesc::ScopeDesc(const CompiledMethod* code, int decode_offset, bool reexecute, bool rethrow_exception, bool return_oop, bool return_vt) {\n@@ -51,0 +52,1 @@\n+  _return_vt     = return_vt;\n@@ -62,0 +64,1 @@\n+  _return_vt     = false;\n","filename":"src\/hotspot\/share\/code\/scopeDesc.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+#include \"oops\/flatArrayKlass.inline.hpp\"\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1184,1 +1184,1 @@\n-  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, return_oop,\n+  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, return_oop, false,\n@@ -1350,0 +1350,2 @@\n+        _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n+        _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -162,1 +162,1 @@\n-  nonstatic_field(InstanceKlass,               _misc_flags,                                   u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _misc_flags,                                   u4)                                    \\\n@@ -515,0 +515,1 @@\n+  declare_constant(DataLayout::array_load_store_data_tag)                 \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+  LOG_TAG(valuetypes) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -52,0 +52,2 @@\n+#include \"oops\/flatArrayKlass.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n@@ -625,1 +627,1 @@\n-  f(InstanceKlass) \\\n+  f(InstanceKlass) \\\n@@ -631,1 +633,3 @@\n-  f(TypeArrayKlass)\n+  f(TypeArrayKlass) \\\n+  f(FlatArrayKlass) \\\n+  f(InlineKlass)\n","filename":"src\/hotspot\/share\/memory\/metaspaceShared.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -158,0 +159,2 @@\n+bool InstanceKlass::field_is_inline_type(int index) const { return Signature::basic_type(field(index)->signature(constants())) == T_INLINE_TYPE; }\n+\n@@ -476,1 +479,3 @@\n-                                       should_store_fingerprint(is_hidden_or_anonymous));\n+                                       should_store_fingerprint(is_hidden_or_anonymous),\n+                                       parser.has_inline_fields() ? parser.java_fields_count() : 0,\n+                                       parser.is_inline_type());\n@@ -490,2 +495,1 @@\n-    }\n-    else if (is_class_loader(class_name, parser)) {\n+    } else if (is_class_loader(class_name, parser)) {\n@@ -494,0 +498,3 @@\n+    } else if (parser.is_inline_type()) {\n+      \/\/ inline type\n+      ik = new (loader_data, size, THREAD) InlineKlass(parser);\n@@ -509,0 +516,7 @@\n+#ifdef ASSERT\n+  assert(ik->size() == size, \"\");\n+  ik->bounds_check((address) ik->start_of_vtable(), false, size);\n+  ik->bounds_check((address) ik->start_of_itable(), false, size);\n+  ik->bounds_check((address) ik->end_of_itable(), true, size);\n+  ik->bounds_check((address) ik->end_of_nonstatic_oop_maps(), true, size);\n+#endif \/\/ASSERT\n@@ -512,0 +526,23 @@\n+#ifndef PRODUCT\n+bool InstanceKlass::bounds_check(address addr, bool edge_ok, intptr_t size_in_bytes) const {\n+  const char* bad = NULL;\n+  address end = NULL;\n+  if (addr < (address)this) {\n+    bad = \"before\";\n+  } else if (addr == (address)this) {\n+    if (edge_ok)  return true;\n+    bad = \"just before\";\n+  } else if (addr == (end = (address)this + sizeof(intptr_t) * (size_in_bytes < 0 ? size() : size_in_bytes))) {\n+    if (edge_ok)  return true;\n+    bad = \"just after\";\n+  } else if (addr > end) {\n+    bad = \"after\";\n+  } else {\n+    return true;\n+  }\n+  tty->print_cr(\"%s object bounds: \" INTPTR_FORMAT \" [\" INTPTR_FORMAT \"..\" INTPTR_FORMAT \"]\",\n+      bad, (intptr_t)addr, (intptr_t)this, (intptr_t)end);\n+  Verbose = WizardMode = true; this->print(); \/\/@@\n+  return false;\n+}\n+#endif \/\/PRODUCT\n@@ -546,1 +583,3 @@\n-  _init_thread(NULL)\n+  _init_thread(NULL),\n+  _inline_type_field_klasses(NULL),\n+  _adr_inlineklass_fixed_block(NULL)\n@@ -555,0 +594,4 @@\n+    if (parser.has_inline_fields()) {\n+      set_has_inline_type_fields();\n+    }\n+    _java_fields_count = parser.java_fields_count();\n@@ -556,3 +599,3 @@\n-  assert(NULL == _methods, \"underlying memory not zeroed?\");\n-  assert(is_instance_klass(), \"is layout incorrect?\");\n-  assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n+    assert(NULL == _methods, \"underlying memory not zeroed?\");\n+    assert(is_instance_klass(), \"is layout incorrect?\");\n+    assert(size_helper() == parser.layout_size(), \"incorrect size_helper?\");\n@@ -565,0 +608,3 @@\n+  if (has_inline_type_fields()) {\n+    _inline_type_field_klasses = (const Klass**) adr_inline_type_field_klasses();\n+  }\n@@ -594,1 +640,2 @@\n-    if (ti != sti && ti != NULL && !ti->is_shared()) {\n+    if (ti != sti && ti != NULL && !ti->is_shared() &&\n+        ti != Universe::the_single_IdentityObject_klass_array()) {\n@@ -601,1 +648,2 @@\n-      local_interfaces != NULL && !local_interfaces->is_shared()) {\n+      local_interfaces != NULL && !local_interfaces->is_shared() &&\n+      local_interfaces != Universe::the_single_IdentityObject_klass_array()) {\n@@ -929,0 +977,56 @@\n+\n+  \/\/ If a class declares a method that uses an inline class as an argument\n+  \/\/ type or return inline type, this inline class must be loaded during the\n+  \/\/ linking of this class because size and properties of the inline class\n+  \/\/ must be known in order to be able to perform inline type optimizations.\n+  \/\/ The implementation below is an approximation of this rule, the code\n+  \/\/ iterates over all methods of the current class (including overridden\n+  \/\/ methods), not only the methods declared by this class. This\n+  \/\/ approximation makes the code simpler, and doesn't change the semantic\n+  \/\/ because classes declaring methods overridden by the current class are\n+  \/\/ linked (and have performed their own pre-loading) before the linking\n+  \/\/ of the current class.\n+\n+\n+  \/\/ Note:\n+  \/\/ Inline class types are loaded during\n+  \/\/ the loading phase (see ClassFileParser::post_process_parsed_stream()).\n+  \/\/ Inline class types used as element types for array creation\n+  \/\/ are not pre-loaded. Their loading is triggered by either anewarray\n+  \/\/ or multianewarray bytecodes.\n+\n+  \/\/ Could it be possible to do the following processing only if the\n+  \/\/ class uses inline types?\n+  {\n+    ResourceMark rm(THREAD);\n+    for (int i = 0; i < methods()->length(); i++) {\n+      Method* m = methods()->at(i);\n+      for (SignatureStream ss(m->signature()); !ss.is_done(); ss.next()) {\n+        if (ss.is_reference()) {\n+          if (ss.is_array()) {\n+            ss.skip_array_prefix();\n+          }\n+          if (ss.type() == T_INLINE_TYPE) {\n+            Symbol* symb = ss.as_symbol();\n+\n+            oop loader = class_loader();\n+            oop protection_domain = this->protection_domain();\n+            Klass* klass = SystemDictionary::resolve_or_fail(symb,\n+                                                             Handle(THREAD, loader), Handle(THREAD, protection_domain), true,\n+                                                             CHECK_false);\n+            if (klass == NULL) {\n+              THROW_(vmSymbols::java_lang_LinkageError(), false);\n+            }\n+            if (!klass->is_inline_klass()) {\n+              Exceptions::fthrow(\n+                THREAD_AND_LOCATION,\n+                vmSymbols::java_lang_IncompatibleClassChangeError(),\n+                \"class %s is not an inline type\",\n+                klass->external_name());\n+            }\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1000,0 +1104,1 @@\n+\n@@ -1153,0 +1258,29 @@\n+  \/\/ Step 8\n+  \/\/ Initialize classes of inline fields\n+  {\n+    for (AllFieldStream fs(this); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        Klass* klass = get_inline_type_field_klass_or_null(fs.index());\n+        if (fs.access_flags().is_static() && klass == NULL) {\n+          klass = SystemDictionary::resolve_or_fail(field_signature(fs.index())->fundamental_name(THREAD),\n+              Handle(THREAD, class_loader()),\n+              Handle(THREAD, protection_domain()),\n+              true, CHECK);\n+          if (klass == NULL) {\n+            THROW(vmSymbols::java_lang_NoClassDefFoundError());\n+          }\n+          if (!klass->is_inline_klass()) {\n+            THROW(vmSymbols::java_lang_IncompatibleClassChangeError());\n+          }\n+          set_inline_type_field_klass(fs.index(), klass);\n+        }\n+        InstanceKlass::cast(klass)->initialize(CHECK);\n+        if (fs.access_flags().is_static()) {\n+          if (java_mirror()->obj_field(fs.offset()) == NULL) {\n+            java_mirror()->obj_field_put(fs.offset(), InlineKlass::cast(klass)->default_value());\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n@@ -1157,1 +1291,1 @@\n-  \/\/ Step 8\n+  \/\/ Step 9\n@@ -1179,1 +1313,1 @@\n-  \/\/ Step 9\n+  \/\/ Step 10\n@@ -1187,1 +1321,1 @@\n-    \/\/ Step 10 and 11\n+    \/\/ Step 11 and 12\n@@ -1475,1 +1609,1 @@\n-  if (clinit != NULL && clinit->has_valid_initializer_flags()) {\n+  if (clinit != NULL && clinit->is_class_initializer()) {\n@@ -1513,1 +1647,1 @@\n-    MutexLocker x(OopMapCacheAlloc_lock);\n+    MutexLocker x(OopMapCacheAlloc_lock,  Mutex::_no_safepoint_check_flag);\n@@ -1525,5 +1659,0 @@\n-bool InstanceKlass::contains_field_offset(int offset) {\n-  fieldDescriptor fd;\n-  return find_field_from_offset(offset, false, &fd);\n-}\n-\n@@ -1600,0 +1729,9 @@\n+bool InstanceKlass::contains_field_offset(int offset) {\n+  if (this->is_inline_klass()) {\n+    InlineKlass* vk = InlineKlass::cast(this);\n+    return offset >= vk->first_field_offset() && offset < (vk->first_field_offset() + vk->get_exact_size_in_bytes());\n+  } else {\n+    fieldDescriptor fd;\n+    return find_field_from_offset(offset, false, &fd);\n+  }\n+}\n@@ -1987,0 +2125,3 @@\n+    if (name == vmSymbols::object_initializer_name()) {\n+      break;  \/\/ <init> is never inherited, not even as a static factory\n+    }\n@@ -2495,0 +2636,6 @@\n+\n+  if (has_inline_type_fields()) {\n+    for (int i = 0; i < java_fields_count(); i++) {\n+      it->push(&((Klass**)adr_inline_type_field_klasses())[i]);\n+    }\n+  }\n@@ -2530,0 +2677,8 @@\n+  if (has_inline_type_fields()) {\n+    for (AllFieldStream fs(fields(), constants()); !fs.done(); fs.next()) {\n+      if (Signature::basic_type(fs.signature()) == T_INLINE_TYPE) {\n+        reset_inline_type_field_klass(fs.index());\n+      }\n+    }\n+  }\n+\n@@ -2569,0 +2724,4 @@\n+  if (is_inline_klass()) {\n+    InlineKlass::cast(this)->initialize_calling_convention(CHECK);\n+  }\n+\n@@ -2594,1 +2753,1 @@\n-  if (UseBiasedLocking && BiasedLocking::enabled()) {\n+  if (UseBiasedLocking && BiasedLocking::enabled() && !is_inline_klass()) {\n@@ -2759,1 +2918,1 @@\n-  \/\/ Add L as type indicator\n+  \/\/ Add L or Q as type indicator\n@@ -2761,1 +2920,1 @@\n-  dest[dest_index++] = JVM_SIGNATURE_CLASS;\n+  dest[dest_index++] = is_inline_klass() ? JVM_SIGNATURE_INLINE_TYPE : JVM_SIGNATURE_CLASS;\n@@ -3331,1 +3490,4 @@\n-static void print_vtable(intptr_t* start, int len, outputStream* st) {\n+static void print_vtable(address self, intptr_t* start, int len, outputStream* st) {\n+  ResourceMark rm;\n+  int* forward_refs = NEW_RESOURCE_ARRAY(int, len);\n+  for (int i = 0; i < len; i++)  forward_refs[i] = 0;\n@@ -3335,0 +3497,5 @@\n+    if (forward_refs[i] != 0) {\n+      int from = forward_refs[i];\n+      int off = (int) start[from];\n+      st->print(\" (offset %d <= [%d])\", off, from);\n+    }\n@@ -3338,0 +3505,6 @@\n+    } else if (self != NULL && e > 0 && e < 0x10000) {\n+      address location = self + e;\n+      int index = (int)((intptr_t*)location - start);\n+      st->print(\" (offset %d => [%d])\", (int)e, index);\n+      if (index >= 0 && index < len)\n+        forward_refs[index] = i;\n@@ -3344,1 +3517,22 @@\n-  return print_vtable(reinterpret_cast<intptr_t*>(start), len, st);\n+  return print_vtable(NULL, reinterpret_cast<intptr_t*>(start), len, st);\n+}\n+\n+template<typename T>\n+ static void print_array_on(outputStream* st, Array<T>* array) {\n+   if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+   array->print_value_on(st); st->cr();\n+   if (Verbose || WizardMode) {\n+     for (int i = 0; i < array->length(); i++) {\n+       st->print(\"%d : \", i); array->at(i)->print_value_on(st); st->cr();\n+     }\n+   }\n+ }\n+\n+static void print_array_on(outputStream* st, Array<int>* array) {\n+  if (array == NULL) { st->print_cr(\"NULL\"); return; }\n+  array->print_value_on(st); st->cr();\n+  if (Verbose || WizardMode) {\n+    for (int i = 0; i < array->length(); i++) {\n+      st->print(\"%d : %d\", i, array->at(i)); st->cr();\n+    }\n+  }\n@@ -3354,0 +3548,1 @@\n+  st->print(BULLET\"misc flags:        0x%x\", _misc_flags);                        st->cr();\n@@ -3380,15 +3575,3 @@\n-  st->print(BULLET\"methods:           \"); methods()->print_value_on(st);                  st->cr();\n-  if (Verbose || WizardMode) {\n-    Array<Method*>* method_array = methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n-  st->print(BULLET\"method ordering:   \"); method_ordering()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"default_methods:   \"); default_methods()->print_value_on(st);      st->cr();\n-  if (Verbose && default_methods() != NULL) {\n-    Array<Method*>* method_array = default_methods();\n-    for (int i = 0; i < method_array->length(); i++) {\n-      st->print(\"%d : \", i); method_array->at(i)->print_value(); st->cr();\n-    }\n-  }\n+  st->print(BULLET\"methods:           \"); print_array_on(st, methods());\n+  st->print(BULLET\"method ordering:   \"); print_array_on(st, method_ordering());\n+  st->print(BULLET\"default_methods:   \"); print_array_on(st, default_methods());\n@@ -3396,1 +3579,1 @@\n-    st->print(BULLET\"default vtable indices:   \"); default_vtable_indices()->print_value_on(st);       st->cr();\n+    st->print(BULLET\"default vtable indices:   \"); print_array_on(st, default_vtable_indices());\n@@ -3398,2 +3581,2 @@\n-  st->print(BULLET\"local interfaces:  \"); local_interfaces()->print_value_on(st);      st->cr();\n-  st->print(BULLET\"trans. interfaces: \"); transitive_interfaces()->print_value_on(st); st->cr();\n+  st->print(BULLET\"local interfaces:  \"); print_array_on(st, local_interfaces());\n+  st->print(BULLET\"trans. interfaces: \"); print_array_on(st, transitive_interfaces());\n@@ -3456,1 +3639,1 @@\n-  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(start_of_itable(), itable_length(), st);\n+  if (itable_length() > 0 && (Verbose || WizardMode))  print_vtable(NULL, start_of_itable(), itable_length(), st);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":226,"deletions":43,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -57,0 +58,2 @@\n+\/\/    [EMBEDDED inline_type_field_klasses] only if has_inline_fields() == true\n+\/\/    [EMBEDDED InlineKlassFixedBlock] only if is an InlineKlass instance\n@@ -73,0 +76,1 @@\n+class BufferedInlineTypeBlob;\n@@ -135,0 +139,17 @@\n+class SigEntry;\n+\n+class InlineKlassFixedBlock {\n+  Array<SigEntry>** _extended_sig;\n+  Array<VMRegPair>** _return_regs;\n+  address* _pack_handler;\n+  address* _pack_handler_jobject;\n+  address* _unpack_handler;\n+  int* _default_value_offset;\n+  Klass** _flat_array_klass;\n+  int _alignment;\n+  int _first_field_offset;\n+  int _exact_size_in_bytes;\n+\n+  friend class InlineKlass;\n+};\n+\n@@ -140,0 +161,1 @@\n+  friend class TemplateTable;\n@@ -157,1 +179,1 @@\n-    fully_initialized,                  \/\/ initialized (successfull final state)\n+    fully_initialized,                  \/\/ initialized (successful final state)\n@@ -240,1 +262,1 @@\n-  \/\/ This can be used to quickly discriminate among the four kinds of\n+  \/\/ This can be used to quickly discriminate among the five kinds of\n@@ -246,0 +268,1 @@\n+  static const unsigned _kind_inline_type  = 4; \/\/ InlineKlass\n@@ -267,1 +290,8 @@\n-    _misc_has_contended_annotations           = 1 << 15  \/\/ has @Contended annotation\n+    _misc_has_contended_annotations           = 1 << 15,  \/\/ has @Contended annotation\n+    _misc_has_inline_type_fields              = 1 << 16, \/\/ has inline fields and related embedded section is not empty\n+    _misc_is_empty_inline_type                = 1 << 17, \/\/ empty inline type\n+    _misc_is_naturally_atomic                 = 1 << 18, \/\/ loaded\/stored in one instruction\n+    _misc_is_declared_atomic                  = 1 << 19, \/\/ implements jl.NonTearable\n+    _misc_invalid_inline_super                = 1 << 20, \/\/ invalid super type for an inline type\n+    _misc_invalid_identity_super              = 1 << 21, \/\/ invalid super type for an identity type\n+    _misc_has_injected_identityObject         = 1 << 22  \/\/ IdentityObject has been injected by the JVM\n@@ -272,1 +302,1 @@\n-  u2              _misc_flags;           \/\/ There is more space in access_flags for more flags.\n+  u4              _misc_flags;           \/\/ There is more space in access_flags for more flags.\n@@ -324,0 +354,3 @@\n+  const Klass**   _inline_type_field_klasses; \/\/ For \"inline class\" fields, NULL if none present\n+\n+  const InlineKlassFixedBlock* _adr_inlineklass_fixed_block;\n@@ -384,0 +417,65 @@\n+  bool has_inline_type_fields() const          {\n+    return (_misc_flags & _misc_has_inline_type_fields) != 0;\n+  }\n+  void set_has_inline_type_fields()  {\n+    _misc_flags |= _misc_has_inline_type_fields;\n+  }\n+\n+  bool is_empty_inline_type() const {\n+    return (_misc_flags & _misc_is_empty_inline_type) != 0;\n+  }\n+  void set_is_empty_inline_type() {\n+    _misc_flags |= _misc_is_empty_inline_type;\n+  }\n+\n+  \/\/ Note:  The naturally_atomic property only applies to\n+  \/\/ inline classes; it is never true on identity classes.\n+  \/\/ The bit is placed on instanceKlass for convenience.\n+\n+  \/\/ Query if h\/w provides atomic load\/store for instances.\n+  bool is_naturally_atomic() const {\n+    return (_misc_flags & _misc_is_naturally_atomic) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_is_naturally_atomic() {\n+    _misc_flags |= _misc_is_naturally_atomic;\n+  }\n+\n+  \/\/ Query if this class implements jl.NonTearable or was\n+  \/\/ mentioned in the JVM option ForceNonTearable.\n+  \/\/ This bit can occur anywhere, but is only significant\n+  \/\/ for inline classes *and* their super types.\n+  \/\/ It inherits from supers along with NonTearable.\n+  bool is_declared_atomic() const {\n+    return (_misc_flags & _misc_is_declared_atomic) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_is_declared_atomic() {\n+    _misc_flags |= _misc_is_declared_atomic;\n+  }\n+\n+  \/\/ Query if class is an invalid super class for an inline type.\n+  bool invalid_inline_super() const {\n+    return (_misc_flags & _misc_invalid_inline_super) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_invalid_inline_super() {\n+    _misc_flags |= _misc_invalid_inline_super;\n+  }\n+  \/\/ Query if class is an invalid super class for an identity type.\n+  bool invalid_identity_super() const {\n+    return (_misc_flags & _misc_invalid_identity_super) != 0;\n+  }\n+  \/\/ Initialized in the class file parser, not changed later.\n+  void set_invalid_identity_super() {\n+    _misc_flags |= _misc_invalid_identity_super;\n+  }\n+\n+  bool has_injected_identityObject() const {\n+    return (_misc_flags & _misc_has_injected_identityObject);\n+  }\n+\n+  void set_has_injected_identityObject() {\n+    _misc_flags |= _misc_has_injected_identityObject;\n+  }\n+\n@@ -446,0 +544,2 @@\n+  bool    field_is_inlined(int index) const { return field(index)->is_inlined(); }\n+  bool    field_is_inline_type(int index) const;\n@@ -576,0 +676,4 @@\n+  static ByteSize kind_offset() { return in_ByteSize(offset_of(InstanceKlass, _kind)); }\n+  static ByteSize misc_flags_offset() { return in_ByteSize(offset_of(InstanceKlass, _misc_flags)); }\n+  static u4 misc_flags_is_empty_inline_type() { return _misc_is_empty_inline_type; }\n+\n@@ -775,0 +879,1 @@\n+\n@@ -776,1 +881,1 @@\n-    return ((_misc_flags & _misc_is_being_redefined) != 0);\n+    return (_misc_flags & _misc_is_being_redefined);\n@@ -861,0 +966,1 @@\n+  bool is_inline_type_klass()           const { return is_kind(_kind_inline_type); }\n@@ -1030,0 +1136,3 @@\n+  static ByteSize inline_type_field_klasses_offset() { return in_ByteSize(offset_of(InstanceKlass, _inline_type_field_klasses)); }\n+  static ByteSize adr_inlineklass_fixed_block_offset() { return in_ByteSize(offset_of(InstanceKlass, _adr_inlineklass_fixed_block)); }\n+\n@@ -1064,2 +1173,2 @@\n-  void array_klasses_do(void f(Klass* k));\n-  void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);\n+  virtual void array_klasses_do(void f(Klass* k));\n+  virtual void array_klasses_do(void f(Klass* k, TRAPS), TRAPS);\n@@ -1086,1 +1195,2 @@\n-                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint) {\n+                  bool is_interface, bool is_unsafe_anonymous, bool has_stored_fingerprint,\n+                  int java_fields, bool is_inline_type) {\n@@ -1093,1 +1203,3 @@\n-           (has_stored_fingerprint ? (int)sizeof(uint64_t*)\/wordSize : 0));\n+           (has_stored_fingerprint ? (int)sizeof(uint64_t*)\/wordSize : 0) +\n+           (java_fields * (int)sizeof(Klass*)\/wordSize) +\n+           (is_inline_type ? (int)sizeof(InlineKlassFixedBlock) : 0));\n@@ -1100,1 +1212,3 @@\n-                                               has_stored_fingerprint());\n+                                               has_stored_fingerprint(),\n+                                               has_inline_type_fields() ? java_fields_count() : 0,\n+                                               is_inline_klass());\n@@ -1110,0 +1224,2 @@\n+  bool bounds_check(address addr, bool edge_ok = false, intptr_t size_in_bytes = -1) const PRODUCT_RETURN0;\n+\n@@ -1158,0 +1274,54 @@\n+  address adr_inline_type_field_klasses() const {\n+    if (has_inline_type_fields()) {\n+      address adr_fing = adr_fingerprint();\n+      if (adr_fing != NULL) {\n+        return adr_fingerprint() + sizeof(u8);\n+      }\n+\n+      InstanceKlass** adr_host = adr_unsafe_anonymous_host();\n+      if (adr_host != NULL) {\n+        return (address)(adr_host + 1);\n+      }\n+\n+      Klass* volatile* adr_impl = adr_implementor();\n+      if (adr_impl != NULL) {\n+        return (address)(adr_impl + 1);\n+      }\n+\n+      return (address)end_of_nonstatic_oop_maps();\n+    } else {\n+      return NULL;\n+    }\n+  }\n+\n+  Klass* get_inline_type_field_klass(int idx) const {\n+    assert(has_inline_type_fields(), \"Sanity checking\");\n+    assert(idx < java_fields_count(), \"IOOB\");\n+    Klass* k = ((Klass**)adr_inline_type_field_klasses())[idx];\n+    assert(k != NULL, \"Should always be set before being read\");\n+    assert(k->is_inline_klass(), \"Must be an inline type\");\n+    return k;\n+  }\n+\n+  Klass* get_inline_type_field_klass_or_null(int idx) const {\n+    assert(has_inline_type_fields(), \"Sanity checking\");\n+    assert(idx < java_fields_count(), \"IOOB\");\n+    Klass* k = ((Klass**)adr_inline_type_field_klasses())[idx];\n+    assert(k == NULL || k->is_inline_klass(), \"Must be an inline type\");\n+    return k;\n+  }\n+\n+  void set_inline_type_field_klass(int idx, Klass* k) {\n+    assert(has_inline_type_fields(), \"Sanity checking\");\n+    assert(idx < java_fields_count(), \"IOOB\");\n+    assert(k != NULL, \"Should not be set to NULL\");\n+    assert(((Klass**)adr_inline_type_field_klasses())[idx] == NULL, \"Should not be set twice\");\n+    ((Klass**)adr_inline_type_field_klasses())[idx] = k;\n+  }\n+\n+  void reset_inline_type_field_klass(int idx) {\n+    assert(has_inline_type_fields(), \"Sanity checking\");\n+    assert(idx < java_fields_count(), \"IOOB\");\n+    ((Klass**)adr_inline_type_field_klasses())[idx] = NULL;\n+  }\n+\n@@ -1159,1 +1329,1 @@\n-  int size_helper() const {\n+  virtual int size_helper() const {\n@@ -1296,1 +1466,1 @@\n-\n+protected:\n@@ -1298,1 +1468,1 @@\n-  Klass* array_klass_impl(bool or_null, int n, TRAPS);\n+  virtual Klass* array_klass_impl(bool or_null, int n, TRAPS);\n@@ -1301,1 +1471,3 @@\n-  Klass* array_klass_impl(bool or_null, TRAPS);\n+  virtual Klass* array_klass_impl(bool or_null, TRAPS);\n+\n+private:\n@@ -1331,1 +1503,1 @@\n-  void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n+  virtual void restore_unshareable_info(ClassLoaderData* loader_data, Handle protection_domain, PackageEntry* pkg_entry, TRAPS);\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":187,"deletions":15,"binary":false,"changes":202,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+  FlatArrayKlassID,\n@@ -51,1 +52,1 @@\n-const uint KLASS_ID_COUNT = 6;\n+const uint KLASS_ID_COUNT = 7;\n@@ -101,1 +102,1 @@\n-  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops\n+  \/\/    tag is 0x80 if the elements are oops, 0xC0 if non-oops, 0xA0 if value types\n@@ -363,1 +364,1 @@\n-  static const int _lh_array_tag_bits          = 2;\n+  static const int _lh_array_tag_bits          = 3;\n@@ -365,2 +366,10 @@\n-  static const int _lh_array_tag_obj_value     = ~0x01;   \/\/ 0x80000000 >> 30\n-  static const unsigned int _lh_array_tag_type_value = 0Xffffffff; \/\/ ~0x00,  \/\/ 0xC0000000 >> 30\n+  static const unsigned int _lh_array_tag_type_value = 0Xfffffffc;\n+  static const unsigned int _lh_array_tag_vt_value   = 0Xfffffffd;\n+  static const unsigned int _lh_array_tag_obj_value  = 0Xfffffffe;\n+\n+  \/\/ null-free array flag bit under the array tag bits, shift one more to get array tag value\n+  static const int _lh_null_free_shift = _lh_array_tag_shift - 1;\n+  static const int _lh_null_free_mask  = 1;\n+\n+  static const jint _lh_array_tag_vt_value_bit_inplace = (jint) (1 << _lh_array_tag_shift);\n+  static const jint _lh_null_free_bit_inplace = (jint) (_lh_null_free_mask << _lh_null_free_shift);\n@@ -384,2 +393,1 @@\n-    \/\/ _lh_array_tag_type_value == (lh >> _lh_array_tag_shift);\n-    return (juint)lh >= (juint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint) _lh_array_tag_type_value == (juint)(lh >> _lh_array_tag_shift);\n@@ -388,2 +396,13 @@\n-    \/\/ _lh_array_tag_obj_value == (lh >> _lh_array_tag_shift);\n-    return (jint)lh < (jint)(_lh_array_tag_type_value << _lh_array_tag_shift);\n+    return (juint)_lh_array_tag_obj_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_flatArray(jint lh) {\n+    return (juint)_lh_array_tag_vt_value == (juint)(lh >> _lh_array_tag_shift);\n+  }\n+  static bool layout_helper_is_null_free(jint lh) {\n+    assert(layout_helper_is_flatArray(lh) || layout_helper_is_objArray(lh), \"must be array of inline types\");\n+    return ((lh >> _lh_null_free_shift) & _lh_null_free_mask);\n+  }\n+  static jint layout_helper_set_null_free(jint lh) {\n+    lh |= (_lh_null_free_mask << _lh_null_free_shift);\n+    assert(layout_helper_is_null_free(lh), \"Bad encoding\");\n+    return lh;\n@@ -400,1 +419,1 @@\n-    assert(btvalue >= T_BOOLEAN && btvalue <= T_OBJECT, \"sanity\");\n+    assert((btvalue >= T_BOOLEAN && btvalue <= T_OBJECT) || btvalue == T_INLINE_TYPE, \"sanity\");\n@@ -421,1 +440,1 @@\n-    assert(l2esz <= LogBytesPerLong,\n+    assert(layout_helper_element_type(lh) == T_INLINE_TYPE || l2esz <= LogBytesPerLong,\n@@ -425,1 +444,1 @@\n-  static jint array_layout_helper(jint tag, int hsize, BasicType etype, int log2_esize) {\n+  static jint array_layout_helper(jint tag, bool null_free, int hsize, BasicType etype, int log2_esize) {\n@@ -427,0 +446,1 @@\n+      |    ((null_free ? 1 : 0) <<  _lh_null_free_shift)\n@@ -567,0 +587,2 @@\n+  \/\/ For value classes, this returns the name with a leading 'Q' and a trailing ';'\n+  \/\/     and the package separators as '\/'.\n@@ -582,0 +604,1 @@\n+  virtual bool is_flatArray_klass_slow()    const { return false; }\n@@ -583,0 +606,2 @@\n+  \/\/ current implementation uses this method even in non debug builds\n+  virtual bool is_inline_klass_slow()       const { return false; }\n@@ -608,0 +633,5 @@\n+  inline  bool is_inline_klass()              const { return is_inline_klass_slow(); } \/\/temporary hack\n+  inline  bool is_flatArray_klass()           const { return assert_same_query(\n+                                                    layout_helper_is_flatArray(layout_helper()),\n+                                                    is_flatArray_klass_slow()); }\n+\n@@ -610,0 +640,2 @@\n+  inline bool is_null_free_array_klass()      const { return layout_helper_is_null_free(layout_helper()); }\n+\n@@ -645,1 +677,4 @@\n-  markWord prototype_header() const      { return _prototype_header; }\n+  markWord prototype_header() const     { return _prototype_header; }\n+  static inline markWord default_prototype_header(Klass* k) {\n+    return (k == NULL) ? markWord::prototype() : k->prototype_header();\n+  }\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":48,"deletions":13,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+\/\/             \"1\"        :23 epoch:2 age:4    biased_lock:1 lock:2 (biased always locked object)\n@@ -46,0 +47,1 @@\n+\/\/  \"1\"        :54 epoch:2 unused:1   age:4    biased_lock:1 lock:2 (biased always locked object)\n@@ -91,0 +93,12 @@\n+\/\/    Always locked: since displaced and monitor references require memory at a\n+\/\/    fixed address, and hash code can be displaced, an efficiently providing a\n+\/\/    *permanent lock* leaves us with specializing the biased pattern (even when\n+\/\/    biased locking isn't enabled). Since biased_lock_alignment for the thread\n+\/\/    reference doesn't use the lowest bit (\"2 << thread_shift\"), we can use\n+\/\/    this illegal thread pointer alignment to denote \"always locked\" pattern.\n+\/\/\n+\/\/    [ <unused> | larval |1| epoch | age | 1 | 01]       permanently locked\n+\/\/\n+\/\/    A private buffered value is always locked and can be in a larval state.\n+\/\/\n+\/\/\n@@ -136,0 +150,3 @@\n+  static const int always_locked_bits             = 1;\n+  static const int larval_bits                    = 1;\n+\n@@ -145,0 +162,2 @@\n+  static const int thread_shift                   = epoch_shift + epoch_bits;\n+  static const int larval_shift                   = thread_shift + always_locked_bits;\n@@ -155,0 +174,2 @@\n+  static const uintptr_t larval_mask              = right_n_bits(larval_bits);\n+  static const uintptr_t larval_mask_in_place     = larval_mask << larval_shift;\n@@ -160,1 +181,1 @@\n-  static const size_t biased_lock_alignment       = 2 << (epoch_shift + epoch_bits);\n+  static const size_t biased_lock_alignment       = 2 << thread_shift;\n@@ -167,0 +188,1 @@\n+  static const uintptr_t always_locked_pattern    = 1 << thread_shift | biased_lock_pattern;\n@@ -179,0 +201,8 @@\n+  enum { larval_state_pattern     = (1 << larval_shift) };\n+\n+  static markWord always_locked_prototype() {\n+    return markWord(always_locked_pattern);\n+  }\n+\n+  bool is_always_locked() const { return mask_bits(value(), always_locked_pattern) == always_locked_pattern; }\n+\n@@ -190,0 +220,1 @@\n+    assert(!is_always_locked(), \"invariant\");\n@@ -207,0 +238,1 @@\n+    assert(!is_always_locked(), \"Rebias needs to fail\");\n@@ -349,0 +381,11 @@\n+  \/\/ private buffered value operations\n+  markWord enter_larval_state() const {\n+    return markWord((value() & ~larval_mask_in_place) | larval_state_pattern);\n+  }\n+  markWord exit_larval_state() const {\n+    return markWord(value() & ~larval_mask_in_place);\n+  }\n+  bool is_larval_state() const {\n+    return (value() & larval_mask_in_place) == larval_state_pattern;\n+  }\n+\n@@ -364,1 +407,1 @@\n-  inline void* decode_pointer() { if (UseBiasedLocking && has_bias_pattern()) return NULL; return (void*)clear_lock_bits().value(); }\n+  inline void* decode_pointer() { if (has_bias_pattern()) return NULL; return (void*)clear_lock_bits().value(); }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":45,"deletions":2,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -84,0 +84,2 @@\n+  virtual CallGenerator* inline_cg() { ShouldNotReachHere(); return NULL; }\n+\n","filename":"src\/hotspot\/share\/opto\/callGenerator.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"ci\/ciFlatArrayKlass.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"opto\/inlinetypenode.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -79,1 +82,1 @@\n-Node *StartNode::match( const ProjNode *proj, const Matcher *match ) {\n+Node *StartNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n@@ -103,11 +106,0 @@\n-\/\/------------------------------StartOSRNode----------------------------------\n-\/\/ The method start node for an on stack replacement adapter\n-\n-\/\/------------------------------osr_domain-----------------------------\n-const TypeTuple *StartOSRNode::osr_domain() {\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeRawPtr::BOTTOM;  \/\/ address of osr buffer\n-\n-  return TypeTuple::make(TypeFunc::Parms+1, fields);\n-}\n-\n@@ -483,0 +475,8 @@\n+      } else if (cik->is_flat_array_klass()) {\n+        ciKlass* cie = cik->as_flat_array_klass()->base_element_klass();\n+        cie->print_name_on(st);\n+        st->print(\"[%d]\", spobj->n_fields());\n+        int ndim = cik->as_array_klass()->dimension() - 1;\n+        while (ndim-- > 0) {\n+          st->print(\"[]\");\n+        }\n@@ -692,1 +692,1 @@\n-const Type *CallNode::bottom_type() const { return tf()->range(); }\n+const Type *CallNode::bottom_type() const { return tf()->range_cc(); }\n@@ -694,2 +694,4 @@\n-  if (phase->type(in(0)) == Type::TOP)  return Type::TOP;\n-  return tf()->range();\n+  if (!in(0) || phase->type(in(0)) == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  return tf()->range_cc();\n@@ -699,1 +701,8 @@\n-void CallNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n+void CallNode::calling_convention(BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt) const {\n+  if (_entry_point == StubRoutines::store_inline_type_fields_to_buf()) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -708,2 +717,28 @@\n-Node *CallNode::match( const ProjNode *proj, const Matcher *match ) {\n-  switch (proj->_con) {\n+Node *CallNode::match(const ProjNode *proj, const Matcher *match, const RegMask* mask) {\n+  uint con = proj->_con;\n+  const TypeTuple *range_cc = tf()->range_cc();\n+  if (con >= TypeFunc::Parms) {\n+    if (is_CallRuntime()) {\n+      if (con == TypeFunc::Parms) {\n+        uint ideal_reg = range_cc->field_at(TypeFunc::Parms)->ideal_reg();\n+        OptoRegPair regs = match->c_return_value(ideal_reg,true);\n+        RegMask rm = RegMask(regs.first());\n+        if (OptoReg::is_valid(regs.second())) {\n+          rm.Insert(regs.second());\n+        }\n+        return new MachProjNode(this,con,rm,ideal_reg);\n+      } else {\n+        assert(con == TypeFunc::Parms+1, \"only one return value\");\n+        assert(range_cc->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n+        return new MachProjNode(this,con, RegMask::Empty, (uint)OptoReg::Bad);\n+      }\n+    } else {\n+      \/\/ The Call may return multiple values (inline type fields): we\n+      \/\/ create one projection per returned value.\n+      assert(con <= TypeFunc::Parms+1 || InlineTypeReturnedAsFields, \"only for multi value return\");\n+      uint ideal_reg = range_cc->field_at(con)->ideal_reg();\n+      return new MachProjNode(this, con, mask[con-TypeFunc::Parms], ideal_reg);\n+    }\n+  }\n+\n+  switch (con) {\n@@ -715,16 +750,0 @@\n-  case TypeFunc::Parms+1:       \/\/ For LONG & DOUBLE returns\n-    assert(tf()->range()->field_at(TypeFunc::Parms+1) == Type::HALF, \"\");\n-    \/\/ 2nd half of doubles and longs\n-    return new MachProjNode(this,proj->_con, RegMask::Empty, (uint)OptoReg::Bad);\n-\n-  case TypeFunc::Parms: {       \/\/ Normal returns\n-    uint ideal_reg = tf()->range()->field_at(TypeFunc::Parms)->ideal_reg();\n-    OptoRegPair regs = is_CallRuntime()\n-      ? match->c_return_value(ideal_reg,true)  \/\/ Calls into C runtime\n-      : match->  return_value(ideal_reg,true); \/\/ Calls into compiled Java code\n-    RegMask rm = RegMask(regs.first());\n-    if( OptoReg::is_valid(regs.second()) )\n-      rm.Insert( regs.second() );\n-    return new MachProjNode(this,proj->_con,rm,ideal_reg);\n-  }\n-\n@@ -751,1 +770,1 @@\n-    const TypeTuple* args = _tf->domain();\n+    const TypeTuple* args = _tf->domain_sig();\n@@ -800,1 +819,1 @@\n-      const TypeTuple* d = tf()->domain();\n+      const TypeTuple* d = tf()->domain_cc();\n@@ -816,1 +835,1 @@\n-  const TypeTuple * d = tf()->domain();\n+  const TypeTuple * d = tf()->domain_cc();\n@@ -826,0 +845,11 @@\n+bool CallNode::has_debug_use(Node *n) {\n+  assert(jvms() != NULL, \"jvms should not be null\");\n+  for (uint i = jvms()->debug_start(); i < jvms()->debug_end(); i++) {\n+    Node *arg = in(i);\n+    if (arg == n) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n@@ -857,10 +887,15 @@\n-void CallNode::extract_projections(CallProjections* projs, bool separate_io_proj, bool do_asserts) {\n-  projs->fallthrough_proj      = NULL;\n-  projs->fallthrough_catchproj = NULL;\n-  projs->fallthrough_ioproj    = NULL;\n-  projs->catchall_ioproj       = NULL;\n-  projs->catchall_catchproj    = NULL;\n-  projs->fallthrough_memproj   = NULL;\n-  projs->catchall_memproj      = NULL;\n-  projs->resproj               = NULL;\n-  projs->exobj                 = NULL;\n+CallProjections* CallNode::extract_projections(bool separate_io_proj, bool do_asserts) {\n+  uint max_res = TypeFunc::Parms-1;\n+  for (DUIterator_Fast imax, i = fast_outs(imax); i < imax; i++) {\n+    ProjNode *pn = fast_out(i)->as_Proj();\n+    max_res = MAX2(max_res, pn->_con);\n+  }\n+\n+  assert(max_res < _tf->range_cc()->cnt(), \"result out of bounds\");\n+\n+  uint projs_size = sizeof(CallProjections);\n+  if (max_res > TypeFunc::Parms) {\n+    projs_size += (max_res-TypeFunc::Parms)*sizeof(Node*);\n+  }\n+  char* projs_storage = resource_allocate_bytes(projs_size);\n+  CallProjections* projs = new(projs_storage)CallProjections(max_res - TypeFunc::Parms + 1);\n@@ -912,1 +947,1 @@\n-      projs->resproj = pn;\n+      projs->resproj[0] = pn;\n@@ -915,1 +950,3 @@\n-      assert(false, \"unexpected projection from allocation node.\");\n+      assert(pn->_con <= max_res, \"unexpected projection from allocation node.\");\n+      projs->resproj[pn->_con-TypeFunc::Parms] = pn;\n+      break;\n@@ -922,1 +959,1 @@\n-  assert(projs->fallthrough_proj      != NULL, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_proj      != NULL, \"must be found\");\n@@ -932,0 +969,1 @@\n+  return projs;\n@@ -970,0 +1008,40 @@\n+\n+void CallJavaNode::copy_call_debug_info(PhaseIterGVN* phase, CallNode *oldcall) {\n+  \/\/ Copy debug information and adjust JVMState information\n+  uint old_dbg_start = oldcall->tf()->domain_sig()->cnt();\n+  uint new_dbg_start = tf()->domain_sig()->cnt();\n+  int jvms_adj  = new_dbg_start - old_dbg_start;\n+  assert (new_dbg_start == req(), \"argument count mismatch\");\n+  Compile* C = phase->C;\n+\n+  \/\/ SafePointScalarObject node could be referenced several times in debug info.\n+  \/\/ Use Dict to record cloned nodes.\n+  Dict* sosn_map = new Dict(cmpkey,hashkey);\n+  for (uint i = old_dbg_start; i < oldcall->req(); i++) {\n+    Node* old_in = oldcall->in(i);\n+    \/\/ Clone old SafePointScalarObjectNodes, adjusting their field contents.\n+    if (old_in != NULL && old_in->is_SafePointScalarObject()) {\n+      SafePointScalarObjectNode* old_sosn = old_in->as_SafePointScalarObject();\n+      uint old_unique = C->unique();\n+      Node* new_in = old_sosn->clone(sosn_map);\n+      if (old_unique != C->unique()) { \/\/ New node?\n+        new_in->set_req(0, C->root()); \/\/ reset control edge\n+        new_in = phase->transform(new_in); \/\/ Register new node.\n+      }\n+      old_in = new_in;\n+    }\n+    add_req(old_in);\n+  }\n+\n+  \/\/ JVMS may be shared so clone it before we modify it\n+  set_jvms(oldcall->jvms() != NULL ? oldcall->jvms()->clone_deep(C) : NULL);\n+  for (JVMState *jvms = this->jvms(); jvms != NULL; jvms = jvms->caller()) {\n+    jvms->set_map(this);\n+    jvms->set_locoff(jvms->locoff()+jvms_adj);\n+    jvms->set_stkoff(jvms->stkoff()+jvms_adj);\n+    jvms->set_monoff(jvms->monoff()+jvms_adj);\n+    jvms->set_scloff(jvms->scloff()+jvms_adj);\n+    jvms->set_endoff(jvms->endoff()+jvms_adj);\n+  }\n+}\n+\n@@ -975,0 +1053,4 @@\n+  Bytecodes::Code bc = jvms()->method()->java_code_at_bci(_bci);\n+  if (EnableValhalla && (bc == Bytecodes::_if_acmpeq || bc == Bytecodes::_if_acmpne)) {\n+    return true;\n+  }\n@@ -1029,0 +1111,151 @@\n+bool CallStaticJavaNode::remove_useless_allocation(PhaseGVN *phase, Node* ctl, Node* mem, Node* unc_arg) {\n+  \/\/ Split if can cause the flattened array branch of an array load to\n+  \/\/ end in an uncommon trap. In that case, the allocation of the\n+  \/\/ loaded value and its initialization is useless. Eliminate it. use\n+  \/\/ the jvm state of the allocation to create a new uncommon trap\n+  \/\/ call at the load.\n+  if (ctl == NULL || ctl->is_top() || mem == NULL || mem->is_top() || !mem->is_MergeMem()) {\n+    return false;\n+  }\n+  PhaseIterGVN* igvn = phase->is_IterGVN();\n+  if (ctl->is_Region()) {\n+    bool res = false;\n+    for (uint i = 1; i < ctl->req(); i++) {\n+      MergeMemNode* mm = mem->clone()->as_MergeMem();\n+      for (MergeMemStream mms(mm); mms.next_non_empty(); ) {\n+        Node* m = mms.memory();\n+        if (m->is_Phi() && m->in(0) == ctl) {\n+          mms.set_memory(m->in(i));\n+        }\n+      }\n+      if (remove_useless_allocation(phase, ctl->in(i), mm, unc_arg)) {\n+        res = true;\n+        if (!ctl->in(i)->is_Region()) {\n+          igvn->replace_input_of(ctl, i, phase->C->top());\n+        }\n+      }\n+      igvn->remove_dead_node(mm);\n+    }\n+    return res;\n+  }\n+  \/\/ verify the control flow is ok\n+  Node* c = ctl;\n+  Node* copy = NULL;\n+  Node* alloc = NULL;\n+  for (;;) {\n+    if (c == NULL || c->is_top()) {\n+      return false;\n+    }\n+    if (c->is_Proj() || c->is_Catch() || c->is_MemBar()) {\n+      c = c->in(0);\n+    } else if (c->Opcode() == Op_CallLeaf &&\n+               c->as_Call()->entry_point() == CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline)) {\n+      copy = c;\n+      c = c->in(0);\n+    } else if (c->is_Allocate()) {\n+      Node* new_obj = c->as_Allocate()->result_cast();\n+      if (copy == NULL || new_obj == NULL) {\n+        return false;\n+      }\n+      Node* copy_dest = copy->in(TypeFunc::Parms + 2);\n+      if (copy_dest != new_obj) {\n+        return false;\n+      }\n+      alloc = c;\n+      break;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  JVMState* jvms = alloc->jvms();\n+  if (phase->C->too_many_traps(jvms->method(), jvms->bci(), Deoptimization::trap_request_reason(uncommon_trap_request()))) {\n+    return false;\n+  }\n+\n+  Node* alloc_mem = alloc->in(TypeFunc::Memory);\n+  if (alloc_mem == NULL || alloc_mem->is_top()) {\n+    return false;\n+  }\n+  if (!alloc_mem->is_MergeMem()) {\n+    alloc_mem = MergeMemNode::make(alloc_mem);\n+  }\n+\n+  \/\/ and that there's no unexpected side effect\n+  for (MergeMemStream mms2(mem->as_MergeMem(), alloc_mem->as_MergeMem()); mms2.next_non_empty2(); ) {\n+    Node* m1 = mms2.is_empty() ? mms2.base_memory() : mms2.memory();\n+    Node* m2 = mms2.memory2();\n+\n+    for (uint i = 0; i < 100; i++) {\n+      if (m1 == m2) {\n+        break;\n+      } else if (m1->is_Proj()) {\n+        m1 = m1->in(0);\n+      } else if (m1->is_MemBar()) {\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->Opcode() == Op_CallLeaf &&\n+                 m1->as_Call()->entry_point() == CAST_FROM_FN_PTR(address, OptoRuntime::load_unknown_inline)) {\n+        if (m1 != copy) {\n+          return false;\n+        }\n+        m1 = m1->in(TypeFunc::Memory);\n+      } else if (m1->is_Allocate()) {\n+        if (m1 != alloc) {\n+          return false;\n+        }\n+        break;\n+      } else if (m1->is_MergeMem()) {\n+        MergeMemNode* mm = m1->as_MergeMem();\n+        int idx = mms2.alias_idx();\n+        if (idx == Compile::AliasIdxBot) {\n+          m1 = mm->base_memory();\n+        } else {\n+          m1 = mm->memory_at(idx);\n+        }\n+      } else {\n+        return false;\n+      }\n+    }\n+  }\n+  if (alloc_mem->outcnt() == 0) {\n+    igvn->remove_dead_node(alloc_mem);\n+  }\n+\n+  address call_addr = SharedRuntime::uncommon_trap_blob()->entry_point();\n+  CallNode* unc = new CallStaticJavaNode(OptoRuntime::uncommon_trap_Type(), call_addr, \"uncommon_trap\",\n+                                         jvms->bci(), NULL);\n+  unc->init_req(TypeFunc::Control, alloc->in(0));\n+  unc->init_req(TypeFunc::I_O, alloc->in(TypeFunc::I_O));\n+  unc->init_req(TypeFunc::Memory, alloc->in(TypeFunc::Memory));\n+  unc->init_req(TypeFunc::FramePtr,  alloc->in(TypeFunc::FramePtr));\n+  unc->init_req(TypeFunc::ReturnAdr, alloc->in(TypeFunc::ReturnAdr));\n+  unc->init_req(TypeFunc::Parms+0, unc_arg);\n+  unc->set_cnt(PROB_UNLIKELY_MAG(4));\n+  unc->copy_call_debug_info(igvn, alloc->as_Allocate());\n+\n+  igvn->replace_input_of(alloc, 0, phase->C->top());\n+\n+  igvn->register_new_node_with_optimizer(unc);\n+\n+  Node* ctrl = phase->transform(new ProjNode(unc, TypeFunc::Control));\n+  Node* halt = phase->transform(new HaltNode(ctrl, alloc->in(TypeFunc::FramePtr), \"uncommon trap returned which should never happen\"));\n+  phase->C->root()->add_req(halt);\n+\n+  return true;\n+}\n+\n+\n+Node* CallStaticJavaNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  if (can_reshape && uncommon_trap_request() != 0) {\n+    if (remove_useless_allocation(phase, in(0), in(TypeFunc::Memory), in(TypeFunc::Parms))) {\n+      if (!in(0)->is_Region()) {\n+        PhaseIterGVN* igvn = phase->is_IterGVN();\n+        igvn->replace_input_of(this, 0, phase->C->top());\n+      }\n+      return this;\n+    }\n+  }\n+  return CallNode::Ideal(phase, can_reshape);\n+}\n+\n+\n@@ -1086,0 +1319,7 @@\n+  if (_entry_point == NULL) {\n+    \/\/ The call to that stub is a special case: its inputs are\n+    \/\/ multiple values returned from a call and so it should follow\n+    \/\/ the return convention.\n+    SharedRuntime::java_return_convention(sig_bt, parm_regs, argcnt);\n+    return;\n+  }\n@@ -1102,0 +1342,6 @@\n+uint CallLeafNoFPNode::match_edge(uint idx) const {\n+  \/\/ Null entry point is a special case for which the target is in a\n+  \/\/ register. Need to match that edge.\n+  return entry_point() == NULL && idx == TypeFunc::Parms;\n+}\n+\n@@ -1359,1 +1605,3 @@\n-                           Node *size, Node *klass_node, Node *initial_test)\n+                           Node *size, Node *klass_node,\n+                           Node* initial_test,\n+                           InlineTypeBaseNode* inline_type_node)\n@@ -1367,0 +1615,1 @@\n+  _larval = false;\n@@ -1378,0 +1627,3 @@\n+  init_req( InlineTypeNode     , inline_type_node);\n+  \/\/ DefaultValue defaults to NULL\n+  \/\/ RawDefaultValue defaults to NULL\n@@ -1384,3 +1636,2 @@\n-         initializer->is_initializer() &&\n-         !initializer->is_static(),\n-             \"unexpected initializer method\");\n+         initializer->is_object_constructor_or_class_initializer(),\n+         \"unexpected initializer method\");\n@@ -1397,1 +1648,2 @@\n-Node *AllocateNode::make_ideal_mark(PhaseGVN *phase, Node* obj, Node* control, Node* mem) {\n+\n+Node* AllocateNode::make_ideal_mark(PhaseGVN* phase, Node* control, Node* mem) {\n@@ -1400,1 +1652,1 @@\n-  if (UseBiasedLocking && Opcode() == Op_Allocate) {\n+  if ((EnableValhalla || UseBiasedLocking) && Opcode() == Op_Allocate) {\n@@ -1407,1 +1659,3 @@\n-  return mark_node;\n+  mark_node = phase->transform(mark_node);\n+  \/\/ Avoid returning a constant (old node) here because this method is used by LoadNode::Ideal\n+  return new OrXNode(mark_node, phase->MakeConX(_larval ? markWord::larval_state_pattern : 0));\n@@ -1410,0 +1664,1 @@\n+\n@@ -1412,1 +1667,4 @@\n-  if (remove_dead_region(phase, can_reshape))  return this;\n+  Node* res = SafePointNode::Ideal(phase, can_reshape);\n+  if (res != NULL) {\n+    return res;\n+  }\n@@ -1837,1 +2095,3 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() &&\n+      !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2005,1 +2265,3 @@\n-  if (can_reshape && EliminateLocks && !is_non_esc_obj()) {\n+  const Type* obj_type = phase->type(obj_node());\n+  if (can_reshape && EliminateLocks && !is_non_esc_obj() &&\n+      !obj_type->isa_inlinetype() && !obj_type->is_inlinetypeptr()) {\n@@ -2087,1 +2349,2 @@\n-    dest_t = dest_t->add_offset(Type::OffsetBot)->is_oopptr();\n+    dest_t = dest_t->is_aryptr()->with_field_offset(Type::OffsetBot)->add_offset(Type::OffsetBot)->is_oopptr();\n+    t_oop = t_oop->is_aryptr()->with_field_offset(Type::OffsetBot);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":324,"deletions":61,"binary":false,"changes":385,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"opto\/mulnode.hpp\"\n+#include \"opto\/addnode.hpp\"\n@@ -54,0 +56,1 @@\n+\n@@ -77,1 +80,7 @@\n-  if (phase->find_unswitching_candidate(this) == NULL) {\n+\n+  if (head->is_flattened_arrays()) {\n+    return false;\n+  }\n+\n+  Node_List unswitch_iffs;\n+  if (phase->find_unswitching_candidate(this, unswitch_iffs) == NULL) {\n@@ -87,1 +96,1 @@\n-IfNode* PhaseIdealLoop::find_unswitching_candidate(const IdealLoopTree *loop) const {\n+IfNode* PhaseIdealLoop::find_unswitching_candidate(const IdealLoopTree *loop, Node_List& unswitch_iffs) const {\n@@ -112,0 +121,18 @@\n+  if (unswitch_iff != NULL) {\n+    unswitch_iffs.push(unswitch_iff);\n+  }\n+\n+  \/\/ Collect all non-flattened array checks for unswitching to create a fast loop\n+  \/\/ without checks (only non-flattened array accesses) and a slow loop with checks.\n+  if (unswitch_iff == NULL || unswitch_iff->is_non_flattened_array_check(&_igvn)) {\n+    for (uint i = 0; i < loop->_body.size(); i++) {\n+      IfNode* n = loop->_body.at(i)->isa_If();\n+      if (n != NULL && n != unswitch_iff && n->is_non_flattened_array_check(&_igvn) &&\n+          loop->is_invariant(n->in(1)) && !loop->is_loop_exit(n)) {\n+        unswitch_iffs.push(n);\n+        if (unswitch_iff == NULL) {\n+          unswitch_iff = n;\n+        }\n+      }\n+    }\n+  }\n@@ -135,2 +162,3 @@\n-  IfNode* unswitch_iff = find_unswitching_candidate((const IdealLoopTree *)loop);\n-  assert(unswitch_iff != NULL, \"should be at least one\");\n+  Node_List unswitch_iffs;\n+  IfNode* unswitch_iff = find_unswitching_candidate((const IdealLoopTree *)loop, unswitch_iffs);\n+  assert(unswitch_iff != NULL && unswitch_iffs.size() > 0, \"should be at least one\");\n@@ -142,0 +170,4 @@\n+    for (uint i = 0; i < unswitch_iffs.size(); i++) {\n+      unswitch_iffs.at(i)->dump(3);\n+      tty->cr();\n+    }\n@@ -189,0 +221,1 @@\n+  invar_iff->_prob    = unswitch_iff->_prob;\n@@ -190,0 +223,29 @@\n+  if (unswitch_iffs.size() > 1) {\n+    \/\/ Flattened array checks are used on array access to switch between\n+    \/\/ a legacy object array access and a flattened inline type array\n+    \/\/ access. We want the performance impact on legacy accesses to be\n+    \/\/ as small as possible so we make two copies of the loop: a fast\n+    \/\/ one where all accesses are known to be legacy, a slow one where\n+    \/\/ some accesses are to flattened arrays. Flattened array checks\n+    \/\/ can be removed from the fast loop but not from the slow loop\n+    \/\/ as it can have a mix of flattened\/legacy accesses.\n+    bol = bol->clone()->as_Bool();\n+    register_new_node(bol, invar_iff->in(0));\n+    Node* cmp = bol->in(1)->clone();\n+    register_new_node(cmp, invar_iff->in(0));\n+    bol->set_req(1, cmp);\n+    \/\/ Combine all checks into a single one that fails if one array is flattened\n+    Node* in1 = NULL;\n+    for (uint i = 0; i < unswitch_iffs.size(); i++) {\n+      Node* array_tag = unswitch_iffs.at(i)->in(1)->in(1)->in(1);\n+      array_tag = new AndINode(array_tag, _igvn.intcon(Klass::_lh_array_tag_vt_value));\n+      register_new_node(array_tag, invar_iff->in(0));\n+      if (in1 == NULL) {\n+        in1 = array_tag;\n+      } else {\n+        in1 = new OrINode(in1, array_tag);\n+        register_new_node(in1, invar_iff->in(0));\n+      }\n+    }\n+    cmp->set_req(1, in1);\n+  }\n@@ -191,6 +253,1 @@\n-  invar_iff->_prob    = unswitch_iff->_prob;\n-\n-  ProjNode* proj_false = invar_iff->proj_out(0)->as_Proj();\n-\n-  \/\/ Hoist invariant casts out of each loop to the appropriate\n-  \/\/ control projection.\n+  \/\/ Hoist invariant casts out of each loop to the appropriate control projection.\n@@ -199,8 +256,23 @@\n-\n-  for (DUIterator_Fast imax, i = unswitch_iff->fast_outs(imax); i < imax; i++) {\n-    ProjNode* proj= unswitch_iff->fast_out(i)->as_Proj();\n-    \/\/ Copy to a worklist for easier manipulation\n-    for (DUIterator_Fast jmax, j = proj->fast_outs(jmax); j < jmax; j++) {\n-      Node* use = proj->fast_out(j);\n-      if (use->Opcode() == Op_CheckCastPP && loop->is_invariant(use->in(1))) {\n-        worklist.push(use);\n+  for (uint i = 0; i < unswitch_iffs.size(); i++) {\n+    IfNode* iff = unswitch_iffs.at(i)->as_If();\n+    for (DUIterator_Fast imax, i = iff->fast_outs(imax); i < imax; i++) {\n+      ProjNode* proj = iff->fast_out(i)->as_Proj();\n+      \/\/ Copy to a worklist for easier manipulation\n+      for (DUIterator_Fast jmax, j = proj->fast_outs(jmax); j < jmax; j++) {\n+        Node* use = proj->fast_out(j);\n+        if (use->Opcode() == Op_CheckCastPP && loop->is_invariant(use->in(1))) {\n+          worklist.push(use);\n+        }\n+      }\n+      ProjNode* invar_proj = invar_iff->proj_out(proj->_con)->as_Proj();\n+      while (worklist.size() > 0) {\n+        Node* use = worklist.pop();\n+        Node* nuse = use->clone();\n+        nuse->set_req(0, invar_proj);\n+        _igvn.replace_input_of(use, 1, nuse);\n+        register_new_node(nuse, invar_proj);\n+        \/\/ Same for the clone if we are removing checks from the slow loop\n+        if (unswitch_iffs.size() == 1) {\n+          Node* use_clone = old_new[use->_idx];\n+          _igvn.replace_input_of(use_clone, 1, nuse);\n+        }\n@@ -208,11 +280,0 @@\n-    }\n-    ProjNode* invar_proj = invar_iff->proj_out(proj->_con)->as_Proj();\n-    while (worklist.size() > 0) {\n-      Node* use = worklist.pop();\n-      Node* nuse = use->clone();\n-      nuse->set_req(0, invar_proj);\n-      _igvn.replace_input_of(use, 1, nuse);\n-      register_new_node(nuse, invar_proj);\n-      \/\/ Same for the clone\n-      Node* use_clone = old_new[use->_idx];\n-      _igvn.replace_input_of(use_clone, 1, nuse);\n@@ -223,3 +284,5 @@\n-  _igvn.rehash_node_delayed(unswitch_iff);\n-  dominated_by(proj_true, unswitch_iff, false, false);\n-\n+  for (uint i = 0; i < unswitch_iffs.size(); i++) {\n+    IfNode* iff = unswitch_iffs.at(i)->as_If();\n+    _igvn.rehash_node_delayed(iff);\n+    dominated_by(proj_true, iff, false, false);\n+  }\n@@ -227,2 +290,9 @@\n-  _igvn.rehash_node_delayed(unswitch_iff_clone);\n-  dominated_by(proj_false, unswitch_iff_clone, false, false);\n+  if (unswitch_iffs.size() == 1) {\n+    ProjNode* proj_false = invar_iff->proj_out(0)->as_Proj();\n+    _igvn.rehash_node_delayed(unswitch_iff_clone);\n+    dominated_by(proj_false, unswitch_iff_clone, false, false);\n+  } else {\n+    \/\/ Leave the flattened array checks in the slow loop and\n+    \/\/ prevent it from being unswitched again based on these checks.\n+    head_clone->mark_flattened_arrays();\n+  }\n@@ -240,3 +310,5 @@\n-    tty->print_cr(\"Loop unswitching orig: %d @ %d  new: %d @ %d\",\n-                  head->_idx,                unswitch_iff->_idx,\n-                  old_new[head->_idx]->_idx, unswitch_iff_clone->_idx);\n+    for (uint i = 0; i < unswitch_iffs.size(); i++) {\n+      tty->print_cr(\"Loop unswitching orig: %d @ %d  new: %d @ %d\",\n+                    head->_idx,                unswitch_iffs.at(i)->_idx,\n+                    old_new[head->_idx]->_idx, old_new[unswitch_iffs.at(i)->_idx]->_idx);\n+    }\n","filename":"src\/hotspot\/share\/opto\/loopUnswitch.cpp","additions":109,"deletions":37,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -501,1 +501,2 @@\n-  if (ty_sign[0] == JVM_SIGNATURE_CLASS &&\n+  if ((ty_sign[0] == JVM_SIGNATURE_CLASS ||\n+       ty_sign[0] == JVM_SIGNATURE_INLINE_TYPE) &&\n@@ -569,0 +570,1 @@\n+  case T_INLINE_TYPE:\n@@ -684,1 +686,1 @@\n-      if (_type == T_OBJECT) {\n+      if (_type == T_OBJECT || _type == T_INLINE_TYPE) {\n@@ -702,1 +704,2 @@\n-      case T_OBJECT: {\n+      case T_OBJECT:\n+      case T_INLINE_TYPE: {\n@@ -723,1 +726,2 @@\n-        case T_OBJECT: {\n+        case T_OBJECT:\n+        case T_INLINE_TYPE: {\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":8,"deletions":4,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-#include \"memory\/iterator.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -55,0 +55,1 @@\n+#include \"oops\/compressedOops.inline.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -1822,0 +1824,92 @@\n+WB_ENTRY(jobjectArray, WB_getObjectsViaKlassOopMaps(JNIEnv* env, jobject wb, jobject thing))\n+  oop aoop = JNIHandles::resolve(thing);\n+  if (!aoop->is_instance()) {\n+    return NULL;\n+  }\n+  instanceHandle ih(THREAD, (instanceOop) aoop);\n+  InstanceKlass* klass = InstanceKlass::cast(aoop->klass());\n+  if (klass->nonstatic_oop_map_count() == 0) {\n+    return NULL;\n+  }\n+  const OopMapBlock* map = klass->start_of_nonstatic_oop_maps();\n+  const OopMapBlock* const end = map + klass->nonstatic_oop_map_count();\n+  int oop_count = 0;\n+  while (map < end) {\n+    oop_count += map->count();\n+    map++;\n+  }\n+\n+  objArrayOop result_array =\n+      oopFactory::new_objArray(SystemDictionary::Object_klass(), oop_count, CHECK_NULL);\n+  map = klass->start_of_nonstatic_oop_maps();\n+  instanceOop ioop = ih();\n+  int index = 0;\n+  while (map < end) {\n+    int offset = map->offset();\n+    for (unsigned int j = 0; j < map->count(); j++) {\n+      result_array->obj_at_put(index++, ioop->obj_field(offset));\n+      offset += heapOopSize;\n+    }\n+    map++;\n+  }\n+  return (jobjectArray)JNIHandles::make_local(THREAD, result_array);\n+WB_END\n+\n+class CollectOops : public BasicOopIterateClosure {\n+ public:\n+  GrowableArray<Handle>* array;\n+\n+  objArrayOop create_results(TRAPS) {\n+    objArrayOop result_array =\n+        oopFactory::new_objArray(SystemDictionary::Object_klass(), array->length(), CHECK_NULL);\n+    for (int i = 0 ; i < array->length(); i++) {\n+      result_array->obj_at_put(i, array->at(i)());\n+    }\n+    return result_array;\n+  }\n+\n+  jobjectArray create_jni_result(JNIEnv* env, TRAPS) {\n+    return (jobjectArray)JNIHandles::make_local(THREAD, create_results(THREAD));\n+  }\n+\n+  void add_oop(oop o) {\n+    \/\/ Value might be oop, but JLS can't see as Object, just iterate through it...\n+    if (o != NULL && o->is_inline_type()) {\n+      o->oop_iterate(this);\n+    } else {\n+      array->append(Handle(Thread::current(), o));\n+    }\n+  }\n+\n+  void do_oop(oop* o) { add_oop(*o); }\n+  void do_oop(narrowOop* v) { add_oop(CompressedOops::decode(*v)); }\n+};\n+\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaOopIterator(JNIEnv* env, jobject wb, jobject thing))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+\n+  JNIHandles::resolve(thing)->oop_iterate(&collectOops);\n+\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+WB_ENTRY(jobjectArray, WB_getObjectsViaFrameOopIterator(JNIEnv* env, jobject wb, jint depth))\n+  ResourceMark rm(THREAD);\n+  GrowableArray<Handle>* array = new GrowableArray<Handle>(128);\n+  CollectOops collectOops;\n+  collectOops.array = array;\n+  StackFrameStream sfs(thread);\n+  while (depth > 0) { \/\/ Skip the native WB API frame\n+    sfs.next();\n+    frame* f = sfs.current();\n+    f->oops_do(&collectOops, NULL, sfs.register_map());\n+    depth--;\n+  }\n+  return collectOops.create_jni_result(env, THREAD);\n+WB_END\n+\n+\n@@ -2456,0 +2550,6 @@\n+  {CC\"getObjectsViaKlassOopMaps0\",\n+      CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",    (void*)&WB_getObjectsViaKlassOopMaps},\n+  {CC\"getObjectsViaOopIterator0\",\n+          CC\"(Ljava\/lang\/Object;)[Ljava\/lang\/Object;\",(void*)&WB_getObjectsViaOopIterator},\n+  {CC\"getObjectsViaFrameOopIterator\",\n+      CC\"(I)[Ljava\/lang\/Object;\",                     (void*)&WB_getObjectsViaFrameOopIterator},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":101,"deletions":1,"binary":false,"changes":102,"status":"modified"},{"patch":"@@ -2159,0 +2159,10 @@\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypePassFieldsAsArgs)) {\n+    FLAG_SET_CMDLINE(InlineTypePassFieldsAsArgs, false);\n+    warning(\"InlineTypePassFieldsAsArgs is not supported on this platform\");\n+  }\n+\n+  if (AMD64_ONLY(false &&) !FLAG_IS_DEFAULT(InlineTypeReturnedAsFields)) {\n+    FLAG_SET_CMDLINE(InlineTypeReturnedAsFields, false);\n+    warning(\"InlineTypeReturnedAsFields is not supported on this platform\");\n+  }\n+\n@@ -3081,0 +3091,18 @@\n+  if (EnableValhalla) {\n+    \/\/ create_property(\"valhalla.enableValhalla\", \"true\", InternalProperty)\n+    const char* prop_name = \"valhalla.enableValhalla\";\n+    const char* prop_value = \"true\";\n+    const size_t prop_len = strlen(prop_name) + strlen(prop_value) + 2;\n+    char* property = AllocateHeap(prop_len, mtArguments);\n+    int ret = jio_snprintf(property, prop_len, \"%s=%s\", prop_name, prop_value);\n+    if (ret < 0 || ret >= (int)prop_len) {\n+      FreeHeap(property);\n+      return JNI_ENOMEM;\n+    }\n+    bool added = add_property(property, UnwriteableProperty, InternalProperty);\n+    FreeHeap(property);\n+    if (!added) {\n+      return JNI_ENOMEM;\n+    }\n+  }\n+\n@@ -4170,0 +4198,5 @@\n+  if (!EnableValhalla || (is_interpreter_only() && !is_dumping_archive())) {\n+    \/\/ Disable calling convention optimizations if inline types are not supported\n+    InlineTypePassFieldsAsArgs = false;\n+    InlineTypeReturnedAsFields = false;\n+  }\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -765,0 +765,18 @@\n+  notproduct(bool, PrintInlineLayout, false,                                \\\n+          \"Print field layout for each inline type\")                        \\\n+                                                                            \\\n+  notproduct(bool, PrintFlatArrayLayout, false,                             \\\n+          \"Print array layout for each inline type array\")                  \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxSize, -1,                                \\\n+          \"Max size for flattening inline array elements, <0 no limit\")     \\\n+                                                                            \\\n+  product(intx, InlineFieldMaxFlatSize, 128,                                \\\n+          \"Max size for flattening inline type fields, <0 no limit\")        \\\n+                                                                            \\\n+  product(intx, FlatArrayElementMaxOops, 4,                                 \\\n+          \"Max nof embedded object references in an inline type to flatten, <0 no limit\")  \\\n+                                                                            \\\n+  product(bool, InlineArrayAtomicAccess, false,                             \\\n+          \"Atomic inline array accesses by-default, for all inline arrays\") \\\n+                                                                            \\\n@@ -780,1 +798,1 @@\n-  product(bool, UseBiasedLocking, false,                                    \\\n+  product(bool, UseBiasedLocking, true,                                     \\\n@@ -2465,0 +2483,20 @@\n+  product(bool, EnableValhalla, true,                                       \\\n+          \"Enable experimental Valhalla features\")                          \\\n+                                                                            \\\n+  product_pd(bool, InlineTypePassFieldsAsArgs,                              \\\n+          \"Pass each inline type field as an argument at calls\")            \\\n+                                                                            \\\n+  product_pd(bool, InlineTypeReturnedAsFields,                              \\\n+          \"Return fields instead of an inline type reference\")              \\\n+                                                                            \\\n+  develop(bool, StressInlineTypeReturnedAsFields, false,                    \\\n+          \"Stress return of fields instead of an inline type reference\")    \\\n+                                                                            \\\n+  develop(bool, ScalarizeInlineTypes, true,                                 \\\n+          \"Scalarize inline types in compiled code\")                        \\\n+                                                                            \\\n+  diagnostic(ccstrlist, ForceNonTearable, \"\",                               \\\n+          \"List of inline classes which are forced to be atomic \"           \\\n+          \"(whitespace and commas separate names, \"                         \\\n+          \"and leading and trailing stars '*' are wildcards)\")              \\\n+                                                                            \\\n@@ -2471,0 +2509,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":40,"deletions":1,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -50,0 +50,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -988,0 +989,1 @@\n+    ResourceMark rm;\n@@ -989,2 +991,25 @@\n-    bool return_oop = nm->method()->is_returning_oop();\n-    Handle return_value;\n+    Method* method = nm->method();\n+    bool return_oop = method->is_returning_oop();\n+\n+    GrowableArray<Handle> return_values;\n+    InlineKlass* vk = NULL;\n+\n+    if (return_oop && InlineTypeReturnedAsFields) {\n+      SignatureStream ss(method->signature());\n+      while (!ss.at_return_type()) {\n+        ss.next();\n+      }\n+      if (ss.type() == T_INLINE_TYPE) {\n+        \/\/ Check if inline type is returned as fields\n+        vk = InlineKlass::returned_inline_klass(map);\n+        if (vk != NULL) {\n+          \/\/ We're at a safepoint at the return of a method that returns\n+          \/\/ multiple values. We must make sure we preserve the oop values\n+          \/\/ across the safepoint.\n+          assert(vk == method->returned_inline_type(thread()), \"bad inline klass\");\n+          vk->save_oop_fields(map, return_values);\n+          return_oop = false;\n+        }\n+      }\n+    }\n+\n@@ -997,1 +1022,1 @@\n-      return_value = Handle(thread(), result);\n+      return_values.push(Handle(thread(), result));\n@@ -1006,1 +1031,4 @@\n-      caller_fr.set_saved_oop_result(&map, return_value());\n+      assert(return_values.length() == 1, \"only one return value\");\n+      caller_fr.set_saved_oop_result(&map, return_values.pop()());\n+    } else if (vk != NULL) {\n+      vk->restore_oop_results(map, return_values);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":32,"deletions":4,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -162,0 +162,13 @@\n+#define CHECK_THROW_NOSYNC_IMSE(obj)  \\\n+  if ((obj)->mark().is_always_locked()) {  \\\n+    ResourceMark rm(THREAD);                \\\n+    THROW_MSG(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+#define CHECK_THROW_NOSYNC_IMSE_0(obj)  \\\n+    if ((obj)->mark().is_always_locked()) {  \\\n+    ResourceMark rm(THREAD);                  \\\n+    THROW_MSG_0(vmSymbols::java_lang_IllegalMonitorStateException(), obj->klass()->external_name()); \\\n+  }\n+\n+\n@@ -456,0 +469,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -506,0 +520,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -614,0 +629,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -663,0 +679,4 @@\n+  if (EnableValhalla && mark.is_always_locked()) {\n+    return;\n+  }\n+  assert(!EnableValhalla || !object->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -726,0 +746,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -740,0 +761,1 @@\n+  assert(!EnableValhalla || !obj->klass()->is_inline_klass(), \"monitor op on inline type\");\n@@ -766,0 +788,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -785,0 +808,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -828,0 +852,1 @@\n+  CHECK_THROW_NOSYNC_IMSE_0(obj);\n@@ -852,0 +877,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -867,0 +893,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -884,0 +911,1 @@\n+  CHECK_THROW_NOSYNC_IMSE(obj);\n@@ -1058,0 +1086,4 @@\n+  if (EnableValhalla && obj->klass()->is_inline_klass()) {\n+    \/\/ VM should be calling bootstrap method\n+    ShouldNotReachHere();\n+  }\n@@ -1190,6 +1222,0 @@\n-\/\/ Deprecated -- use FastHashCode() instead.\n-\n-intptr_t ObjectSynchronizer::identity_hash_value_for(Handle obj) {\n-  return FastHashCode(Thread::current(), obj());\n-}\n-\n@@ -1199,0 +1225,3 @@\n+  if (EnableValhalla && h_obj->mark().is_always_locked()) {\n+    return false;\n+  }\n@@ -1859,0 +1888,4 @@\n+  if (EnableValhalla) {\n+    guarantee(!object->klass()->is_inline_klass(), \"Attempt to inflate inline type\");\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":39,"deletions":6,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-  static intptr_t identity_hash_value_for(Handle obj);\n+  static intptr_t identity_hash_value_for(Handle obj);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+#include \"oops\/inlineKlass.hpp\"\n@@ -1685,0 +1686,1 @@\n+  set_return_buffered_value(NULL);\n@@ -2894,0 +2896,3 @@\n+  \/\/ Because this method is used to verify oops, it must support\n+  \/\/ oops in buffered values\n+\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -510,0 +510,4 @@\n+\n+void VM_PrintClassLayout::doit() {\n+  PrintClassLayout::print_class_layout(_out, _class_name);\n+}\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -117,0 +117,1 @@\n+  template(ClassPrintLayout)                      \\\n@@ -425,0 +426,10 @@\n+class VM_PrintClassLayout: public VM_Operation {\n+ private:\n+  outputStream* _out;\n+  char* _class_name;\n+ public:\n+  VM_PrintClassLayout(outputStream* st, char* class_name): _out(st), _class_name(class_name) {}\n+  VMOp_Type type() const { return VMOp_PrintClassHierarchy; }\n+  void doit();\n+};\n+\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-  JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE  = 0x00000100, \/\/ (static) final field updated outside (class) initializer, same as JVM_ACC_NATIVE\n+  JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE  = 0x00000200, \/\/ (static) final field updated outside (class) initializer, same as JVM_ACC_NATIVE\n@@ -90,0 +90,1 @@\n+  JVM_ACC_FIELD_INLINED                   = 0x00008000, \/\/ field is inlined\n@@ -95,1 +96,2 @@\n-                                       JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE,\n+                                       JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE |\n+                                       JVM_ACC_FIELD_INLINED,\n@@ -126,0 +128,1 @@\n+  bool is_inline_type () const         { return (_flags & JVM_ACC_INLINE      ) != 0; }\n","filename":"src\/hotspot\/share\/utilities\/accessFlags.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+import java.lang.reflect.InaccessibleObjectException;\n@@ -506,0 +507,1 @@\n+        boolean isInlineClass = cl.isInlineClass();\n@@ -577,0 +579,2 @@\n+            } else if (isInlineClass && writeReplaceMethod == null) {\n+                deserializeEx = new ExceptionInfo(name, \"inline class\");\n@@ -1574,1 +1578,1 @@\n-        } catch (NoSuchMethodException ex) {\n+        } catch (NoSuchMethodException | InaccessibleObjectException ex) {\n@@ -1905,0 +1909,1 @@\n+                \/\/ Skip IdentityObject to keep the computed SVUID the same.\n@@ -1906,1 +1911,2 @@\n-                    dout.writeUTF(ifaceNames[i]);\n+                    if (!\"java.lang.IdentityObject\".equals(ifaceNames[i]))\n+                        dout.writeUTF(ifaceNames[i]);\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectStreamClass.java","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -369,2 +369,3 @@\n-                \/\/ both are reference types: fromType should be a superclass of toType.\n-                return !strict || toType.isAssignableFrom(fromType);\n+                \/\/ inline types: fromType and toType are projection types of the same inline class\n+                \/\/ identity types: fromType should be a superclass of toType.\n+                return !strict || canConvert(fromType, toType);\n@@ -375,0 +376,34 @@\n+    \/**\n+     * Tests if {@code fromType} can be converted to {@code toType}\n+     * via an identity conversion, via a widening reference conversion or\n+     * via inline narrowing and widening conversions.\n+     * <p>\n+     * If {@code fromType} represents a class or interface, this method\n+     * returns {@code true} if {@code toType} is the same as,\n+     * or is a superclass or superinterface of, {@code fromType}.\n+     * <p>\n+     * If {@code fromType} is an inline class, this method returns {@code true}\n+     * if {@code toType} is the {@linkplain Class#referenceType() reference projection type}\n+     * of {@code fromType}.\n+     * If {@code toType} is an inline class, this method returns {@code true}\n+     * if {@code toType} is the {@linkplain Class#valueType() value projection type}\n+     * of {@code fromType}.\n+     * <p>\n+     * Otherwise, this method returns {@code false}.\n+     *\n+     * @param     fromType the {@code Class} object to be converted from\n+     * @param     toType the {@code Class} object to be converted to\n+     * @return    {@code true} if {@code fromType} can be converted to {@code toType}\n+     *\/\n+    private boolean canConvert(Class<?> fromType, Class<?> toType) {\n+        if (toType.isAssignableFrom(fromType)) {\n+            return true;\n+        }\n+\n+        if (!fromType.isInlineClass() && !toType.isInlineClass()) {\n+            return false;\n+        }\n+\n+        return fromType.valueType().equals(toType.valueType());\n+    }\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/AbstractValidatingLambdaMetafactory.java","additions":37,"deletions":2,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+import jdk.internal.access.JavaLangAccess;\n+import jdk.internal.access.SharedSecrets;\n@@ -56,1 +58,1 @@\n-    private static final int CLASSFILE_VERSION = 52;\n+    private static final int CLASSFILE_VERSION = V16;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/InnerClassLambdaMetafactory.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1017,1 +1017,2 @@\n-        if (member.isConstructor())  return false;\n+        if (member.isObjectConstructorOrStaticInitMethod())  return false;\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/InvokerBytecodeGenerator.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2015, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -87,1 +87,1 @@\n-        if (c == 'L') {\n+        if (c == 'L' || c == 'Q') {\n","filename":"src\/java.base\/share\/classes\/sun\/invoke\/util\/BytecodeDescriptor.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+import com.sun.tools.javac.jvm.Target;\n@@ -96,0 +97,1 @@\n+    final boolean allowValueBasedClasses;\n@@ -125,0 +127,2 @@\n+        Options options = Options.instance(context);\n+        allowValueBasedClasses = options.isSet(\"allowValueBasedClasses\");\n@@ -604,0 +608,9 @@\n+\n+        boolean tValue = t.isValue();\n+        boolean sValue = s.isValue();\n+        if (tValue != sValue) {\n+            return tValue ?\n+                    isSubtype(t.referenceProjection(), s) :\n+                    (!t.hasTag(BOT) || isValueBased(s)) && isSubtype(t, s.referenceProjection());\n+        }\n+\n@@ -997,0 +1010,8 @@\n+    public boolean isValue(Type t) {\n+        return t != null && t.tsym != null && (t.tsym.flags_field & Flags.VALUE) != 0;\n+    }\n+\n+    public boolean isValueBased(Type t) {\n+        return allowValueBasedClasses && t != null && t.tsym != null && (t.tsym.flags() & Flags.VALUEBASED) != 0;\n+    }\n+\n@@ -1020,1 +1041,11 @@\n-                    return isSubtypeUncheckedInternal(elemtype(t), elemtype(s), false, warn);\n+                    \/\/ if T.ref <: S, then T[] <: S[]\n+                    Type es = elemtype(s);\n+                    Type et = elemtype(t);\n+                    if (isValue(et)) {\n+                        et = et.referenceProjection();\n+                        if (isValue(es))\n+                            es = es.referenceProjection();  \/\/ V <: V, surely\n+                    }\n+                    if (!isSubtypeUncheckedInternal(et, es, false, warn))\n+                        return false;\n+                    return true;\n@@ -1117,1 +1148,1 @@\n-                         s.hasTag(BOT) || s.hasTag(CLASS) ||\n+                         s.hasTag(BOT) || (s.hasTag(CLASS) && (!isValue(s) || isValueBased(s))) ||\n@@ -1195,2 +1226,11 @@\n-                    else\n-                        return isSubtypeNoCapture(t.elemtype, elemtype(s));\n+                    else {\n+                        \/\/ if T.ref <: S, then T[] <: S[]\n+                        Type es = elemtype(s);\n+                        Type et = elemtype(t);\n+                        if (isValue(et)) {\n+                            et = et.referenceProjection();\n+                            if (isValue(es))\n+                                es = es.referenceProjection();  \/\/ V <: V, surely\n+                        }\n+                        return isSubtypeNoCapture(et, es);\n+                    }\n@@ -1203,1 +1243,2 @@\n-                        || sname == names.java_io_Serializable;\n+                        || sname == names.java_io_Serializable\n+                        || sname == names.java_lang_IdentityObject;\n@@ -1581,0 +1622,9 @@\n+\n+                    \/\/ -----------------------------------  Unspecified behavior ----------------\n+\n+                    \/* If a value class V implements an interface I, then does \"? extends I\" contain V?\n+                       It seems widening must be applied here to answer yes to compile some common code\n+                       patterns.\n+                    *\/\n+\n+                    \/\/ ---------------------------------------------------------------------------\n@@ -1686,1 +1736,1 @@\n-                if (s.hasTag(ERROR) || s.hasTag(BOT))\n+                if (s.hasTag(ERROR) || (s.hasTag(BOT) && !isValue(t)))\n@@ -1705,0 +1755,8 @@\n+                    if (isValue(t)) {\n+                        \/\/ (s) Value ? == (s) Value.ref\n+                        t = t.referenceProjection();\n+                    }\n+                    if (isValue(s)) {\n+                        \/\/ (Value) t ? == (Value.ref) t\n+                        s = s.referenceProjection();\n+                    }\n@@ -1763,1 +1821,1 @@\n-                            return ((t.tsym.flags() & FINAL) == 0)\n+                            return (dynamicTypeMayImplementAdditionalInterfaces(t.tsym))\n@@ -1810,1 +1868,5 @@\n-                        return visit(elemtype(t), elemtype(s));\n+                        Type et = elemtype(t);\n+                        Type es = elemtype(s);\n+                        if (!visit(et, es))\n+                            return false;\n+                        return true;\n@@ -2102,0 +2164,17 @@\n+        return asSuper(t, sym, false);\n+    }\n+\n+    \/**\n+     * Return the (most specific) base type of t that starts with the\n+     * given symbol.  If none exists, return null.\n+     *\n+     * Caveat Emptor: Since javac represents the class of all arrays with a singleton\n+     * symbol Symtab.arrayClass, which by being a singleton cannot hold any discriminant,\n+     * this method could yield surprising answers when invoked on arrays. For example when\n+     * invoked with t being byte [] and sym being t.sym itself, asSuper would answer null.\n+     *\n+     * @param t a type\n+     * @param sym a symbol\n+     * @param checkReferenceProjection if true, first compute reference projection of t\n+     *\/\n+    public Type asSuper(Type t, Symbol sym, boolean checkReferenceProjection) {\n@@ -2110,0 +2189,8 @@\n+\n+        \/* For a (value or identity) class V, whether it implements an interface I, boils down to whether\n+           V.ref is a subtype of I. OIOW, whether asSuper(V.ref, sym) != null. (Likewise for an abstract\n+           superclass)\n+        *\/\n+        if (checkReferenceProjection)\n+            t = t.isValue() ? t.referenceProjection() : t;\n+\n@@ -2111,1 +2198,12 @@\n-            return syms.objectType;\n+            if (!isValue(t))\n+                return syms.objectType;\n+        }\n+        if (sym.type == syms.identityObjectType) {\n+            \/\/ IdentityObject is super interface of every concrete identity class other than jlO\n+            if (t.isValue() || t.tsym == syms.objectType.tsym)\n+                return null;\n+            if (t.hasTag(ARRAY))\n+                return syms.identityObjectType;\n+            if (t.hasTag(CLASS) && !t.isReferenceProjection() && !t.tsym.isInterface() && !t.tsym.isAbstract()) {\n+                return syms.identityObjectType;\n+            } \/\/ else fall through and look for explicit coded super interface\n@@ -2129,0 +2227,4 @@\n+                \/\/ No man may be an island, but the bell tolls for a value.\n+                if (isValue(t))\n+                    return null;\n+\n@@ -2240,3 +2342,15 @@\n-        return (sym.flags() & STATIC) != 0\n-            ? sym.type\n-            : memberType.visit(t, sym);\n+\n+        if ((sym.flags() & STATIC) != 0)\n+            return sym.type;\n+\n+        \/* If any inline types are involved, switch over to the reference universe,\n+           where the hierarchy is navigable. V and V.ref have identical membership\n+           with no bridging needs.\n+        *\/\n+        if (t.isValue())\n+            t = t.referenceProjection();\n+\n+        if (sym.owner.isValue())\n+            sym = sym.referenceProjection();\n+\n+        return memberType.visit(t, sym);\n@@ -2452,0 +2566,3 @@\n+        long flags = ABSTRACT | PUBLIC | SYNTHETIC | COMPOUND | ACYCLIC;\n+        if (isValue(bounds.head))\n+            flags |= VALUE;\n@@ -2453,1 +2570,1 @@\n-            new ClassSymbol(ABSTRACT|PUBLIC|SYNTHETIC|COMPOUND|ACYCLIC,\n+            new ClassSymbol(flags,\n@@ -3926,1 +4043,0 @@\n-\n@@ -4047,2 +4163,3 @@\n-                        arraySuperType = makeIntersectionType(List.of(syms.serializableType,\n-                                syms.cloneableType), true);\n+                        List<Type> ifaces =\n+                                List.of(syms.serializableType, syms.cloneableType, syms.identityObjectType);\n+                        arraySuperType = makeIntersectionType(ifaces, true);\n@@ -4486,1 +4603,1 @@\n-        Assert.check((from.tsym.flags() & FINAL) != 0);\n+        Assert.check(!dynamicTypeMayImplementAdditionalInterfaces(from.tsym));\n@@ -4498,0 +4615,4 @@\n+    private boolean dynamicTypeMayImplementAdditionalInterfaces(TypeSymbol tsym) {\n+        return (tsym.flags() & FINAL) == 0 && !tsym.isReferenceProjection();\n+    }\n+\n@@ -4827,0 +4948,1 @@\n+        private boolean encodeTypeSig;\n@@ -4828,1 +4950,1 @@\n-        public UniqueType(Type type, Types types) {\n+        public UniqueType(Type type, Types types, boolean encodeTypeSig) {\n@@ -4831,0 +4953,5 @@\n+            this.encodeTypeSig = encodeTypeSig;\n+        }\n+\n+        public UniqueType(Type type, Types types) {\n+            this(type, types, true);\n@@ -4842,0 +4969,4 @@\n+        public boolean encodeTypeSig() {\n+            return encodeTypeSig;\n+        }\n+\n@@ -5074,1 +5205,4 @@\n-                    append('L');\n+                    if (types.isValue(type))\n+                        append('Q');\n+                    else\n+                        append('L');\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Types.java","additions":153,"deletions":19,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -106,1 +106,8 @@\n-    jdk\/modules\n+    jdk\/modules \\\n+    valhalla\n+\n+# valhalla lworld tests\n+jdk_valhalla = \\\n+    java\/lang\/invoke \\\n+    valhalla \\\n+    com\/sun\/jdi\/JdbInlineTypesTest.java\n","filename":"test\/jdk\/TEST.groups","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -329,0 +329,1 @@\n+        vmOptFinalFlag(map, \"TieredCompilation\");\n@@ -377,0 +378,3 @@\n+        if (WB.getBooleanVMFlag(\"EnableValhalla\").booleanValue()) {\n+            return \"false\";\n+        }\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -151,0 +151,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n","filename":"test\/lib\/sun\/hotspot\/WhiteBox.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"}]}