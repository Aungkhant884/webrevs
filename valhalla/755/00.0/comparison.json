{"files":[{"patch":"@@ -1134,1 +1134,1 @@\n-            version: \"6.1\",\n+            version: \"7\",\n@@ -1136,1 +1136,1 @@\n-            file: \"bundles\/jtreg-6.1+1.zip\",\n+            file: \"bundles\/jtreg-7+1.zip\",\n","filename":"make\/conf\/jib-profiles.js","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -123,1 +123,0 @@\n-      cdsProtectionDomain.cpp \\\n@@ -126,3 +125,0 @@\n-      dumpTimeSharedClassInfo.cpp \\\n-      lambdaProxyClassDictionary.cpp \\\n-      runTimeSharedClassInfo.cpp \\\n","filename":"make\/hotspot\/lib\/JvmFeatures.gmk","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -360,12 +360,0 @@\n-void LIRGenerator::do_continuation_doYield(Intrinsic* x) {\n-  BasicTypeList signature(0);\n-  CallingConvention* cc = frame_map()->java_calling_convention(&signature, true);\n-\n-  const LIR_Opr result_reg = result_register_for(x->type());\n-  address entry = StubRoutines::cont_doYield();\n-  LIR_Opr result = rlock_result(x);\n-  CodeEmitInfo* info = state_for(x, x->state());\n-  __ call_runtime(entry, LIR_OprFact::illegalOpr, result_reg, cc->args(), info);\n-  __ move(result_reg, result);\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -443,0 +443,9 @@\n+  \/\/ Is marking still active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ ldrw(tmp, in_progress);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ ldrb(tmp, in_progress);\n+  }\n+  __ cbzw(tmp, done);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -855,0 +855,31 @@\n+\/\/ Check the entry target is always reachable from any branch.\n+static bool is_always_within_branch_range(Address entry) {\n+  const address target = entry.target();\n+\n+  if (!CodeCache::contains(target)) {\n+    \/\/ We always use trampolines for callees outside CodeCache.\n+    assert(entry.rspec().type() == relocInfo::runtime_call_type, \"non-runtime call of an external target\");\n+    return false;\n+  }\n+\n+  if (!MacroAssembler::far_branches()) {\n+    return true;\n+  }\n+\n+  if (entry.rspec().type() == relocInfo::runtime_call_type) {\n+    \/\/ Runtime calls are calls of a non-compiled method (stubs, adapters).\n+    \/\/ Non-compiled methods stay forever in CodeCache.\n+    \/\/ We check whether the longest possible branch is within the branch range.\n+    assert(CodeCache::find_blob(target) != NULL &&\n+          !CodeCache::find_blob(target)->is_compiled(),\n+          \"runtime call of compiled method\");\n+    const address right_longest_branch_start = CodeCache::high_bound() - NativeInstruction::instruction_size;\n+    const address left_longest_branch_start = CodeCache::low_bound();\n+    const bool is_reachable = Assembler::reachable_from_branch_at(left_longest_branch_start, target) &&\n+                              Assembler::reachable_from_branch_at(right_longest_branch_start, target);\n+    return is_reachable;\n+  }\n+\n+  return false;\n+}\n+\n@@ -865,16 +896,1 @@\n-  \/\/ We might need a trampoline if branches are far.\n-  bool need_trampoline = far_branches();\n-  if (!need_trampoline && entry.rspec().type() == relocInfo::runtime_call_type && !CodeCache::contains(target)) {\n-    \/\/ If it is a runtime call of an address outside small CodeCache,\n-    \/\/ we need to check whether it is in range.\n-    assert(target < CodeCache::low_bound() || target >= CodeCache::high_bound(), \"target is inside CodeCache\");\n-    \/\/ Case 1: -------T-------L====CodeCache====H-------\n-    \/\/                ^-------longest branch---|\n-    \/\/ Case 2: -------L====CodeCache====H-------T-------\n-    \/\/                |-------longest branch ---^\n-    address longest_branch_start = (target < CodeCache::low_bound()) ? CodeCache::high_bound() - NativeInstruction::instruction_size\n-                                                                     : CodeCache::low_bound();\n-    need_trampoline = !reachable_from_branch_at(longest_branch_start, target);\n-  }\n-\n-  if (need_trampoline) {\n+  if (!is_always_within_branch_range(entry)) {\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":32,"deletions":16,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -1414,0 +1414,56 @@\n+static void gen_continuation_yield(MacroAssembler* masm,\n+                                   const methodHandle& method,\n+                                   const BasicType* sig_bt,\n+                                   const VMRegPair* regs,\n+                                   int& exception_offset,\n+                                   OopMapSet* oop_maps,\n+                                   int& frame_complete,\n+                                   int& stack_slots,\n+                                   int& interpreted_entry_offset,\n+                                   int& compiled_entry_offset) {\n+    enum layout {\n+      rfp_off1,\n+      rfp_off2,\n+      lr_off,\n+      lr_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+    \/\/ assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+    stack_slots = framesize \/  VMRegImpl::slots_per_word;\n+    assert(stack_slots == 2, \"recheck layout\");\n+\n+    address start = __ pc();\n+\n+    compiled_entry_offset = __ pc() - start;\n+    __ enter();\n+\n+    __ mov(c_rarg1, sp);\n+\n+    frame_complete = __ pc() - start;\n+    address the_pc = __ pc();\n+\n+    __ post_call_nop(); \/\/ this must be exactly after the pc value that is pushed into the frame info, we use this nop for fast CodeBlob lookup\n+\n+    __ mov(c_rarg0, rthread);\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n+    __ reset_last_Java_frame(true);\n+\n+    Label pinned;\n+\n+    __ cbnz(r0, pinned);\n+\n+    \/\/ We've succeeded, set sp to the ContinuationEntry\n+    __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+    __ mov(sp, rscratch1);\n+    continuation_enter_cleanup(masm);\n+\n+    __ bind(pinned); \/\/ pinned -- return to caller\n+\n+    __ leave();\n+    __ ret(lr);\n+\n+    OopMap* map = new OopMap(framesize, 1);\n+    oop_maps->add_gc_map(the_pc - start, map);\n+}\n+\n@@ -1498,3 +1554,1 @@\n-  if (method->is_continuation_enter_intrinsic()) {\n-    vmIntrinsics::ID iid = method->intrinsic_id();\n-    intptr_t start = (intptr_t)__ pc();\n+  if (method->is_continuation_native_intrinsic()) {\n@@ -1507,10 +1561,25 @@\n-    gen_continuation_enter(masm,\n-                         method,\n-                         in_sig_bt,\n-                         in_regs,\n-                         exception_offset,\n-                         oop_maps,\n-                         frame_complete,\n-                         stack_slots,\n-                         interpreted_entry_offset,\n-                         vep_offset);\n+    if (method->is_continuation_enter_intrinsic()) {\n+      gen_continuation_enter(masm,\n+                             method,\n+                             in_sig_bt,\n+                             in_regs,\n+                             exception_offset,\n+                             oop_maps,\n+                             frame_complete,\n+                             stack_slots,\n+                             interpreted_entry_offset,\n+                             vep_offset);\n+    } else if (method->is_continuation_yield_intrinsic()) {\n+      gen_continuation_yield(masm,\n+                             method,\n+                             in_sig_bt,\n+                             in_regs,\n+                             exception_offset,\n+                             oop_maps,\n+                             frame_complete,\n+                             stack_slots,\n+                             interpreted_entry_offset,\n+                             vep_offset);\n+    } else {\n+      guarantee(false, \"Unknown Continuation native intrinsic\");\n+    }\n@@ -1528,1 +1597,7 @@\n-    ContinuationEntry::set_enter_code(nm, interpreted_entry_offset);\n+    if (method->is_continuation_enter_intrinsic()) {\n+      ContinuationEntry::set_enter_code(nm, interpreted_entry_offset);\n+    } else if (method->is_continuation_yield_intrinsic()) {\n+      _cont_doYield_stub = nm;\n+    } else {\n+      guarantee(false, \"Unknown Continuation native intrinsic\");\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":89,"deletions":14,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -3253,0 +3253,68 @@\n+  \/\/ Utility routines for md5.\n+  \/\/ Clobbers r10 and r11.\n+  void md5_FF(Register buf, Register r1, Register r2, Register r3, Register r4,\n+              int k, int s, int t) {\n+    Register rscratch3 = r10;\n+    Register rscratch4 = r11;\n+\n+    __ eorw(rscratch3, r3, r4);\n+    __ movw(rscratch2, t);\n+    __ andw(rscratch3, rscratch3, r2);\n+    __ addw(rscratch4, r1, rscratch2);\n+    __ ldrw(rscratch1, Address(buf, k*4));\n+    __ eorw(rscratch3, rscratch3, r4);\n+    __ addw(rscratch3, rscratch3, rscratch1);\n+    __ addw(rscratch3, rscratch3, rscratch4);\n+    __ rorw(rscratch2, rscratch3, 32 - s);\n+    __ addw(r1, rscratch2, r2);\n+  }\n+\n+  void md5_GG(Register buf, Register r1, Register r2, Register r3, Register r4,\n+              int k, int s, int t) {\n+    Register rscratch3 = r10;\n+    Register rscratch4 = r11;\n+\n+    __ eorw(rscratch2, r2, r3);\n+    __ ldrw(rscratch1, Address(buf, k*4));\n+    __ andw(rscratch3, rscratch2, r4);\n+    __ movw(rscratch2, t);\n+    __ eorw(rscratch3, rscratch3, r3);\n+    __ addw(rscratch4, r1, rscratch2);\n+    __ addw(rscratch3, rscratch3, rscratch1);\n+    __ addw(rscratch3, rscratch3, rscratch4);\n+    __ rorw(rscratch2, rscratch3, 32 - s);\n+    __ addw(r1, rscratch2, r2);\n+  }\n+\n+  void md5_HH(Register buf, Register r1, Register r2, Register r3, Register r4,\n+              int k, int s, int t) {\n+    Register rscratch3 = r10;\n+    Register rscratch4 = r11;\n+\n+    __ eorw(rscratch3, r3, r4);\n+    __ movw(rscratch2, t);\n+    __ addw(rscratch4, r1, rscratch2);\n+    __ ldrw(rscratch1, Address(buf, k*4));\n+    __ eorw(rscratch3, rscratch3, r2);\n+    __ addw(rscratch3, rscratch3, rscratch1);\n+    __ addw(rscratch3, rscratch3, rscratch4);\n+    __ rorw(rscratch2, rscratch3, 32 - s);\n+    __ addw(r1, rscratch2, r2);\n+  }\n+\n+  void md5_II(Register buf, Register r1, Register r2, Register r3, Register r4,\n+              int k, int s, int t) {\n+    Register rscratch3 = r10;\n+    Register rscratch4 = r11;\n+\n+    __ movw(rscratch3, t);\n+    __ ornw(rscratch2, r2, r4);\n+    __ addw(rscratch4, r1, rscratch3);\n+    __ ldrw(rscratch1, Address(buf, k*4));\n+    __ eorw(rscratch3, rscratch2, r3);\n+    __ addw(rscratch3, rscratch3, rscratch1);\n+    __ addw(rscratch3, rscratch3, rscratch4);\n+    __ rorw(rscratch2, rscratch3, 32 - s);\n+    __ addw(r1, rscratch2, r2);\n+  }\n+\n@@ -3277,2 +3345,0 @@\n-    Label keys;\n-\n@@ -3288,62 +3354,16 @@\n-#define FF(r1, r2, r3, r4, k, s, t)              \\\n-    __ eorw(rscratch3, r3, r4);                  \\\n-    __ movw(rscratch2, t);                       \\\n-    __ andw(rscratch3, rscratch3, r2);           \\\n-    __ addw(rscratch4, r1, rscratch2);           \\\n-    __ ldrw(rscratch1, Address(buf, k*4));       \\\n-    __ eorw(rscratch3, rscratch3, r4);           \\\n-    __ addw(rscratch3, rscratch3, rscratch1);    \\\n-    __ addw(rscratch3, rscratch3, rscratch4);    \\\n-    __ rorw(rscratch2, rscratch3, 32 - s);       \\\n-    __ addw(r1, rscratch2, r2);\n-\n-#define GG(r1, r2, r3, r4, k, s, t)              \\\n-    __ eorw(rscratch2, r2, r3);                  \\\n-    __ ldrw(rscratch1, Address(buf, k*4));       \\\n-    __ andw(rscratch3, rscratch2, r4);           \\\n-    __ movw(rscratch2, t);                       \\\n-    __ eorw(rscratch3, rscratch3, r3);           \\\n-    __ addw(rscratch4, r1, rscratch2);           \\\n-    __ addw(rscratch3, rscratch3, rscratch1);    \\\n-    __ addw(rscratch3, rscratch3, rscratch4);    \\\n-    __ rorw(rscratch2, rscratch3, 32 - s);       \\\n-    __ addw(r1, rscratch2, r2);\n-\n-#define HH(r1, r2, r3, r4, k, s, t)              \\\n-    __ eorw(rscratch3, r3, r4);                  \\\n-    __ movw(rscratch2, t);                       \\\n-    __ addw(rscratch4, r1, rscratch2);           \\\n-    __ ldrw(rscratch1, Address(buf, k*4));       \\\n-    __ eorw(rscratch3, rscratch3, r2);           \\\n-    __ addw(rscratch3, rscratch3, rscratch1);    \\\n-    __ addw(rscratch3, rscratch3, rscratch4);    \\\n-    __ rorw(rscratch2, rscratch3, 32 - s);       \\\n-    __ addw(r1, rscratch2, r2);\n-\n-#define II(r1, r2, r3, r4, k, s, t)              \\\n-    __ movw(rscratch3, t);                       \\\n-    __ ornw(rscratch2, r2, r4);                  \\\n-    __ addw(rscratch4, r1, rscratch3);           \\\n-    __ ldrw(rscratch1, Address(buf, k*4));       \\\n-    __ eorw(rscratch3, rscratch2, r3);           \\\n-    __ addw(rscratch3, rscratch3, rscratch1);    \\\n-    __ addw(rscratch3, rscratch3, rscratch4);    \\\n-    __ rorw(rscratch2, rscratch3, 32 - s);       \\\n-    __ addw(r1, rscratch2, r2);\n-\n-    FF(a, b, c, d,  0,  7, 0xd76aa478)\n-    FF(d, a, b, c,  1, 12, 0xe8c7b756)\n-    FF(c, d, a, b,  2, 17, 0x242070db)\n-    FF(b, c, d, a,  3, 22, 0xc1bdceee)\n-    FF(a, b, c, d,  4,  7, 0xf57c0faf)\n-    FF(d, a, b, c,  5, 12, 0x4787c62a)\n-    FF(c, d, a, b,  6, 17, 0xa8304613)\n-    FF(b, c, d, a,  7, 22, 0xfd469501)\n-    FF(a, b, c, d,  8,  7, 0x698098d8)\n-    FF(d, a, b, c,  9, 12, 0x8b44f7af)\n-    FF(c, d, a, b, 10, 17, 0xffff5bb1)\n-    FF(b, c, d, a, 11, 22, 0x895cd7be)\n-    FF(a, b, c, d, 12,  7, 0x6b901122)\n-    FF(d, a, b, c, 13, 12, 0xfd987193)\n-    FF(c, d, a, b, 14, 17, 0xa679438e)\n-    FF(b, c, d, a, 15, 22, 0x49b40821)\n+    md5_FF(buf, a, b, c, d,  0,  7, 0xd76aa478);\n+    md5_FF(buf, d, a, b, c,  1, 12, 0xe8c7b756);\n+    md5_FF(buf, c, d, a, b,  2, 17, 0x242070db);\n+    md5_FF(buf, b, c, d, a,  3, 22, 0xc1bdceee);\n+    md5_FF(buf, a, b, c, d,  4,  7, 0xf57c0faf);\n+    md5_FF(buf, d, a, b, c,  5, 12, 0x4787c62a);\n+    md5_FF(buf, c, d, a, b,  6, 17, 0xa8304613);\n+    md5_FF(buf, b, c, d, a,  7, 22, 0xfd469501);\n+    md5_FF(buf, a, b, c, d,  8,  7, 0x698098d8);\n+    md5_FF(buf, d, a, b, c,  9, 12, 0x8b44f7af);\n+    md5_FF(buf, c, d, a, b, 10, 17, 0xffff5bb1);\n+    md5_FF(buf, b, c, d, a, 11, 22, 0x895cd7be);\n+    md5_FF(buf, a, b, c, d, 12,  7, 0x6b901122);\n+    md5_FF(buf, d, a, b, c, 13, 12, 0xfd987193);\n+    md5_FF(buf, c, d, a, b, 14, 17, 0xa679438e);\n+    md5_FF(buf, b, c, d, a, 15, 22, 0x49b40821);\n@@ -3353,16 +3373,16 @@\n-    GG(a, b, c, d,  1,  5, 0xf61e2562)\n-    GG(d, a, b, c,  6,  9, 0xc040b340)\n-    GG(c, d, a, b, 11, 14, 0x265e5a51)\n-    GG(b, c, d, a,  0, 20, 0xe9b6c7aa)\n-    GG(a, b, c, d,  5,  5, 0xd62f105d)\n-    GG(d, a, b, c, 10,  9, 0x02441453)\n-    GG(c, d, a, b, 15, 14, 0xd8a1e681)\n-    GG(b, c, d, a,  4, 20, 0xe7d3fbc8)\n-    GG(a, b, c, d,  9,  5, 0x21e1cde6)\n-    GG(d, a, b, c, 14,  9, 0xc33707d6)\n-    GG(c, d, a, b,  3, 14, 0xf4d50d87)\n-    GG(b, c, d, a,  8, 20, 0x455a14ed)\n-    GG(a, b, c, d, 13,  5, 0xa9e3e905)\n-    GG(d, a, b, c,  2,  9, 0xfcefa3f8)\n-    GG(c, d, a, b,  7, 14, 0x676f02d9)\n-    GG(b, c, d, a, 12, 20, 0x8d2a4c8a)\n+    md5_GG(buf, a, b, c, d,  1,  5, 0xf61e2562);\n+    md5_GG(buf, d, a, b, c,  6,  9, 0xc040b340);\n+    md5_GG(buf, c, d, a, b, 11, 14, 0x265e5a51);\n+    md5_GG(buf, b, c, d, a,  0, 20, 0xe9b6c7aa);\n+    md5_GG(buf, a, b, c, d,  5,  5, 0xd62f105d);\n+    md5_GG(buf, d, a, b, c, 10,  9, 0x02441453);\n+    md5_GG(buf, c, d, a, b, 15, 14, 0xd8a1e681);\n+    md5_GG(buf, b, c, d, a,  4, 20, 0xe7d3fbc8);\n+    md5_GG(buf, a, b, c, d,  9,  5, 0x21e1cde6);\n+    md5_GG(buf, d, a, b, c, 14,  9, 0xc33707d6);\n+    md5_GG(buf, c, d, a, b,  3, 14, 0xf4d50d87);\n+    md5_GG(buf, b, c, d, a,  8, 20, 0x455a14ed);\n+    md5_GG(buf, a, b, c, d, 13,  5, 0xa9e3e905);\n+    md5_GG(buf, d, a, b, c,  2,  9, 0xfcefa3f8);\n+    md5_GG(buf, c, d, a, b,  7, 14, 0x676f02d9);\n+    md5_GG(buf, b, c, d, a, 12, 20, 0x8d2a4c8a);\n@@ -3371,16 +3391,16 @@\n-    HH(a, b, c, d,  5,  4, 0xfffa3942)\n-    HH(d, a, b, c,  8, 11, 0x8771f681)\n-    HH(c, d, a, b, 11, 16, 0x6d9d6122)\n-    HH(b, c, d, a, 14, 23, 0xfde5380c)\n-    HH(a, b, c, d,  1,  4, 0xa4beea44)\n-    HH(d, a, b, c,  4, 11, 0x4bdecfa9)\n-    HH(c, d, a, b,  7, 16, 0xf6bb4b60)\n-    HH(b, c, d, a, 10, 23, 0xbebfbc70)\n-    HH(a, b, c, d, 13,  4, 0x289b7ec6)\n-    HH(d, a, b, c,  0, 11, 0xeaa127fa)\n-    HH(c, d, a, b,  3, 16, 0xd4ef3085)\n-    HH(b, c, d, a,  6, 23, 0x04881d05)\n-    HH(a, b, c, d,  9,  4, 0xd9d4d039)\n-    HH(d, a, b, c, 12, 11, 0xe6db99e5)\n-    HH(c, d, a, b, 15, 16, 0x1fa27cf8)\n-    HH(b, c, d, a,  2, 23, 0xc4ac5665)\n+    md5_HH(buf, a, b, c, d,  5,  4, 0xfffa3942);\n+    md5_HH(buf, d, a, b, c,  8, 11, 0x8771f681);\n+    md5_HH(buf, c, d, a, b, 11, 16, 0x6d9d6122);\n+    md5_HH(buf, b, c, d, a, 14, 23, 0xfde5380c);\n+    md5_HH(buf, a, b, c, d,  1,  4, 0xa4beea44);\n+    md5_HH(buf, d, a, b, c,  4, 11, 0x4bdecfa9);\n+    md5_HH(buf, c, d, a, b,  7, 16, 0xf6bb4b60);\n+    md5_HH(buf, b, c, d, a, 10, 23, 0xbebfbc70);\n+    md5_HH(buf, a, b, c, d, 13,  4, 0x289b7ec6);\n+    md5_HH(buf, d, a, b, c,  0, 11, 0xeaa127fa);\n+    md5_HH(buf, c, d, a, b,  3, 16, 0xd4ef3085);\n+    md5_HH(buf, b, c, d, a,  6, 23, 0x04881d05);\n+    md5_HH(buf, a, b, c, d,  9,  4, 0xd9d4d039);\n+    md5_HH(buf, d, a, b, c, 12, 11, 0xe6db99e5);\n+    md5_HH(buf, c, d, a, b, 15, 16, 0x1fa27cf8);\n+    md5_HH(buf, b, c, d, a,  2, 23, 0xc4ac5665);\n@@ -3389,21 +3409,16 @@\n-    II(a, b, c, d,  0,  6, 0xf4292244)\n-    II(d, a, b, c,  7, 10, 0x432aff97)\n-    II(c, d, a, b, 14, 15, 0xab9423a7)\n-    II(b, c, d, a,  5, 21, 0xfc93a039)\n-    II(a, b, c, d, 12,  6, 0x655b59c3)\n-    II(d, a, b, c,  3, 10, 0x8f0ccc92)\n-    II(c, d, a, b, 10, 15, 0xffeff47d)\n-    II(b, c, d, a,  1, 21, 0x85845dd1)\n-    II(a, b, c, d,  8,  6, 0x6fa87e4f)\n-    II(d, a, b, c, 15, 10, 0xfe2ce6e0)\n-    II(c, d, a, b,  6, 15, 0xa3014314)\n-    II(b, c, d, a, 13, 21, 0x4e0811a1)\n-    II(a, b, c, d,  4,  6, 0xf7537e82)\n-    II(d, a, b, c, 11, 10, 0xbd3af235)\n-    II(c, d, a, b,  2, 15, 0x2ad7d2bb)\n-    II(b, c, d, a,  9, 21, 0xeb86d391)\n-\n-#undef FF\n-#undef GG\n-#undef HH\n-#undef II\n+    md5_II(buf, a, b, c, d,  0,  6, 0xf4292244);\n+    md5_II(buf, d, a, b, c,  7, 10, 0x432aff97);\n+    md5_II(buf, c, d, a, b, 14, 15, 0xab9423a7);\n+    md5_II(buf, b, c, d, a,  5, 21, 0xfc93a039);\n+    md5_II(buf, a, b, c, d, 12,  6, 0x655b59c3);\n+    md5_II(buf, d, a, b, c,  3, 10, 0x8f0ccc92);\n+    md5_II(buf, c, d, a, b, 10, 15, 0xffeff47d);\n+    md5_II(buf, b, c, d, a,  1, 21, 0x85845dd1);\n+    md5_II(buf, a, b, c, d,  8,  6, 0x6fa87e4f);\n+    md5_II(buf, d, a, b, c, 15, 10, 0xfe2ce6e0);\n+    md5_II(buf, c, d, a, b,  6, 15, 0xa3014314);\n+    md5_II(buf, b, c, d, a, 13, 21, 0x4e0811a1);\n+    md5_II(buf, a, b, c, d,  4,  6, 0xf7537e82);\n+    md5_II(buf, d, a, b, c, 11, 10, 0xbd3af235);\n+    md5_II(buf, c, d, a, b,  2, 15, 0x2ad7d2bb);\n+    md5_II(buf, b, c, d, a,  9, 21, 0xeb86d391);\n@@ -3647,0 +3662,28 @@\n+  \/\/ Double rounds for sha512.\n+  void sha512_dround(int dr,\n+                     FloatRegister vi0, FloatRegister vi1,\n+                     FloatRegister vi2, FloatRegister vi3,\n+                     FloatRegister vi4, FloatRegister vrc0,\n+                     FloatRegister vrc1, FloatRegister vin0,\n+                     FloatRegister vin1, FloatRegister vin2,\n+                     FloatRegister vin3, FloatRegister vin4) {\n+      if (dr < 36) {\n+        __ ld1(vrc1, __ T2D, __ post(rscratch2, 16));\n+      }\n+      __ addv(v5, __ T2D, vrc0, vin0);\n+      __ ext(v6, __ T16B, vi2, vi3, 8);\n+      __ ext(v5, __ T16B, v5, v5, 8);\n+      __ ext(v7, __ T16B, vi1, vi2, 8);\n+      __ addv(vi3, __ T2D, vi3, v5);\n+      if (dr < 32) {\n+        __ ext(v5, __ T16B, vin3, vin4, 8);\n+        __ sha512su0(vin0, __ T2D, vin1);\n+      }\n+      __ sha512h(vi3, __ T2D, v6, v7);\n+      if (dr < 32) {\n+        __ sha512su1(vin0, __ T2D, vin2, v5);\n+      }\n+      __ addv(vi4, __ T2D, vi1, vi3);\n+      __ sha512h2(vi3, __ T2D, vi1, vi0);\n+  }\n+\n@@ -3686,19 +3729,0 @@\n-    \/\/ Double rounds for sha512.\n-    #define sha512_dround(dr, i0, i1, i2, i3, i4, rc0, rc1, in0, in1, in2, in3, in4) \\\n-      if (dr < 36)                                                                   \\\n-        __ ld1(v##rc1, __ T2D, __ post(rscratch2, 16));                              \\\n-      __ addv(v5, __ T2D, v##rc0, v##in0);                                           \\\n-      __ ext(v6, __ T16B, v##i2, v##i3, 8);                                          \\\n-      __ ext(v5, __ T16B, v5, v5, 8);                                                \\\n-      __ ext(v7, __ T16B, v##i1, v##i2, 8);                                          \\\n-      __ addv(v##i3, __ T2D, v##i3, v5);                                             \\\n-      if (dr < 32) {                                                                 \\\n-        __ ext(v5, __ T16B, v##in3, v##in4, 8);                                      \\\n-        __ sha512su0(v##in0, __ T2D, v##in1);                                        \\\n-      }                                                                              \\\n-      __ sha512h(v##i3, __ T2D, v6, v7);                                             \\\n-      if (dr < 32)                                                                   \\\n-        __ sha512su1(v##in0, __ T2D, v##in2, v5);                                    \\\n-      __ addv(v##i4, __ T2D, v##i1, v##i3);                                          \\\n-      __ sha512h2(v##i3, __ T2D, v##i1, v##i0);                                      \\\n-\n@@ -3748,40 +3772,40 @@\n-    sha512_dround( 0, 0, 1, 2, 3, 4, 20, 24, 12, 13, 19, 16, 17);\n-    sha512_dround( 1, 3, 0, 4, 2, 1, 21, 25, 13, 14, 12, 17, 18);\n-    sha512_dround( 2, 2, 3, 1, 4, 0, 22, 26, 14, 15, 13, 18, 19);\n-    sha512_dround( 3, 4, 2, 0, 1, 3, 23, 27, 15, 16, 14, 19, 12);\n-    sha512_dround( 4, 1, 4, 3, 0, 2, 24, 28, 16, 17, 15, 12, 13);\n-    sha512_dround( 5, 0, 1, 2, 3, 4, 25, 29, 17, 18, 16, 13, 14);\n-    sha512_dround( 6, 3, 0, 4, 2, 1, 26, 30, 18, 19, 17, 14, 15);\n-    sha512_dround( 7, 2, 3, 1, 4, 0, 27, 31, 19, 12, 18, 15, 16);\n-    sha512_dround( 8, 4, 2, 0, 1, 3, 28, 24, 12, 13, 19, 16, 17);\n-    sha512_dround( 9, 1, 4, 3, 0, 2, 29, 25, 13, 14, 12, 17, 18);\n-    sha512_dround(10, 0, 1, 2, 3, 4, 30, 26, 14, 15, 13, 18, 19);\n-    sha512_dround(11, 3, 0, 4, 2, 1, 31, 27, 15, 16, 14, 19, 12);\n-    sha512_dround(12, 2, 3, 1, 4, 0, 24, 28, 16, 17, 15, 12, 13);\n-    sha512_dround(13, 4, 2, 0, 1, 3, 25, 29, 17, 18, 16, 13, 14);\n-    sha512_dround(14, 1, 4, 3, 0, 2, 26, 30, 18, 19, 17, 14, 15);\n-    sha512_dround(15, 0, 1, 2, 3, 4, 27, 31, 19, 12, 18, 15, 16);\n-    sha512_dround(16, 3, 0, 4, 2, 1, 28, 24, 12, 13, 19, 16, 17);\n-    sha512_dround(17, 2, 3, 1, 4, 0, 29, 25, 13, 14, 12, 17, 18);\n-    sha512_dround(18, 4, 2, 0, 1, 3, 30, 26, 14, 15, 13, 18, 19);\n-    sha512_dround(19, 1, 4, 3, 0, 2, 31, 27, 15, 16, 14, 19, 12);\n-    sha512_dround(20, 0, 1, 2, 3, 4, 24, 28, 16, 17, 15, 12, 13);\n-    sha512_dround(21, 3, 0, 4, 2, 1, 25, 29, 17, 18, 16, 13, 14);\n-    sha512_dround(22, 2, 3, 1, 4, 0, 26, 30, 18, 19, 17, 14, 15);\n-    sha512_dround(23, 4, 2, 0, 1, 3, 27, 31, 19, 12, 18, 15, 16);\n-    sha512_dround(24, 1, 4, 3, 0, 2, 28, 24, 12, 13, 19, 16, 17);\n-    sha512_dround(25, 0, 1, 2, 3, 4, 29, 25, 13, 14, 12, 17, 18);\n-    sha512_dround(26, 3, 0, 4, 2, 1, 30, 26, 14, 15, 13, 18, 19);\n-    sha512_dround(27, 2, 3, 1, 4, 0, 31, 27, 15, 16, 14, 19, 12);\n-    sha512_dround(28, 4, 2, 0, 1, 3, 24, 28, 16, 17, 15, 12, 13);\n-    sha512_dround(29, 1, 4, 3, 0, 2, 25, 29, 17, 18, 16, 13, 14);\n-    sha512_dround(30, 0, 1, 2, 3, 4, 26, 30, 18, 19, 17, 14, 15);\n-    sha512_dround(31, 3, 0, 4, 2, 1, 27, 31, 19, 12, 18, 15, 16);\n-    sha512_dround(32, 2, 3, 1, 4, 0, 28, 24, 12,  0,  0,  0,  0);\n-    sha512_dround(33, 4, 2, 0, 1, 3, 29, 25, 13,  0,  0,  0,  0);\n-    sha512_dround(34, 1, 4, 3, 0, 2, 30, 26, 14,  0,  0,  0,  0);\n-    sha512_dround(35, 0, 1, 2, 3, 4, 31, 27, 15,  0,  0,  0,  0);\n-    sha512_dround(36, 3, 0, 4, 2, 1, 24,  0, 16,  0,  0,  0,  0);\n-    sha512_dround(37, 2, 3, 1, 4, 0, 25,  0, 17,  0,  0,  0,  0);\n-    sha512_dround(38, 4, 2, 0, 1, 3, 26,  0, 18,  0,  0,  0,  0);\n-    sha512_dround(39, 1, 4, 3, 0, 2, 27,  0, 19,  0,  0,  0,  0);\n+    sha512_dround( 0, v0, v1, v2, v3, v4, v20, v24, v12, v13, v19, v16, v17);\n+    sha512_dround( 1, v3, v0, v4, v2, v1, v21, v25, v13, v14, v12, v17, v18);\n+    sha512_dround( 2, v2, v3, v1, v4, v0, v22, v26, v14, v15, v13, v18, v19);\n+    sha512_dround( 3, v4, v2, v0, v1, v3, v23, v27, v15, v16, v14, v19, v12);\n+    sha512_dround( 4, v1, v4, v3, v0, v2, v24, v28, v16, v17, v15, v12, v13);\n+    sha512_dround( 5, v0, v1, v2, v3, v4, v25, v29, v17, v18, v16, v13, v14);\n+    sha512_dround( 6, v3, v0, v4, v2, v1, v26, v30, v18, v19, v17, v14, v15);\n+    sha512_dround( 7, v2, v3, v1, v4, v0, v27, v31, v19, v12, v18, v15, v16);\n+    sha512_dround( 8, v4, v2, v0, v1, v3, v28, v24, v12, v13, v19, v16, v17);\n+    sha512_dround( 9, v1, v4, v3, v0, v2, v29, v25, v13, v14, v12, v17, v18);\n+    sha512_dround(10, v0, v1, v2, v3, v4, v30, v26, v14, v15, v13, v18, v19);\n+    sha512_dround(11, v3, v0, v4, v2, v1, v31, v27, v15, v16, v14, v19, v12);\n+    sha512_dround(12, v2, v3, v1, v4, v0, v24, v28, v16, v17, v15, v12, v13);\n+    sha512_dround(13, v4, v2, v0, v1, v3, v25, v29, v17, v18, v16, v13, v14);\n+    sha512_dround(14, v1, v4, v3, v0, v2, v26, v30, v18, v19, v17, v14, v15);\n+    sha512_dround(15, v0, v1, v2, v3, v4, v27, v31, v19, v12, v18, v15, v16);\n+    sha512_dround(16, v3, v0, v4, v2, v1, v28, v24, v12, v13, v19, v16, v17);\n+    sha512_dround(17, v2, v3, v1, v4, v0, v29, v25, v13, v14, v12, v17, v18);\n+    sha512_dround(18, v4, v2, v0, v1, v3, v30, v26, v14, v15, v13, v18, v19);\n+    sha512_dround(19, v1, v4, v3, v0, v2, v31, v27, v15, v16, v14, v19, v12);\n+    sha512_dround(20, v0, v1, v2, v3, v4, v24, v28, v16, v17, v15, v12, v13);\n+    sha512_dround(21, v3, v0, v4, v2, v1, v25, v29, v17, v18, v16, v13, v14);\n+    sha512_dround(22, v2, v3, v1, v4, v0, v26, v30, v18, v19, v17, v14, v15);\n+    sha512_dround(23, v4, v2, v0, v1, v3, v27, v31, v19, v12, v18, v15, v16);\n+    sha512_dround(24, v1, v4, v3, v0, v2, v28, v24, v12, v13, v19, v16, v17);\n+    sha512_dround(25, v0, v1, v2, v3, v4, v29, v25, v13, v14, v12, v17, v18);\n+    sha512_dround(26, v3, v0, v4, v2, v1, v30, v26, v14, v15, v13, v18, v19);\n+    sha512_dround(27, v2, v3, v1, v4, v0, v31, v27, v15, v16, v14, v19, v12);\n+    sha512_dround(28, v4, v2, v0, v1, v3, v24, v28, v16, v17, v15, v12, v13);\n+    sha512_dround(29, v1, v4, v3, v0, v2, v25, v29, v17, v18, v16, v13, v14);\n+    sha512_dround(30, v0, v1, v2, v3, v4, v26, v30, v18, v19, v17, v14, v15);\n+    sha512_dround(31, v3, v0, v4, v2, v1, v27, v31, v19, v12, v18, v15, v16);\n+    sha512_dround(32, v2, v3, v1, v4, v0, v28, v24, v12,  v0,  v0,  v0,  v0);\n+    sha512_dround(33, v4, v2, v0, v1, v3, v29, v25, v13,  v0,  v0,  v0,  v0);\n+    sha512_dround(34, v1, v4, v3, v0, v2, v30, v26, v14,  v0,  v0,  v0,  v0);\n+    sha512_dround(35, v0, v1, v2, v3, v4, v31, v27, v15,  v0,  v0,  v0,  v0);\n+    sha512_dround(36, v3, v0, v4, v2, v1, v24,  v0, v16,  v0,  v0,  v0,  v0);\n+    sha512_dround(37, v2, v3, v1, v4, v0, v25,  v0, v17,  v0,  v0,  v0,  v0);\n+    sha512_dround(38, v4, v2, v0, v1, v3, v26,  v0, v18,  v0,  v0,  v0,  v0);\n+    sha512_dround(39, v1, v4, v3, v0, v2, v27,  v0, v19,  v0,  v0,  v0,  v0);\n@@ -6649,63 +6673,0 @@\n-  RuntimeStub* generate_cont_doYield() {\n-    if (!Continuations::enabled()) return nullptr;\n-\n-    const char *name = \"cont_doYield\";\n-\n-    enum layout {\n-      rfp_off1,\n-      rfp_off2,\n-      lr_off,\n-      lr_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n-    \/\/ assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n-\n-    int insts_size = 512;\n-    int locs_size  = 64;\n-    CodeBuffer code(name, insts_size, locs_size);\n-    OopMapSet* oop_maps  = new OopMapSet();\n-    MacroAssembler* masm = new MacroAssembler(&code);\n-    MacroAssembler* _masm = masm;\n-\n-    address start = __ pc();\n-\n-    __ enter();\n-\n-    __ mov(c_rarg1, sp);\n-\n-    int frame_complete = __ pc() - start;\n-    address the_pc = __ pc();\n-\n-    __ post_call_nop(); \/\/ this must be exactly after the pc value that is pushed into the frame info, we use this nop for fast CodeBlob lookup\n-\n-    __ mov(c_rarg0, rthread);\n-    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n-    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n-    __ reset_last_Java_frame(true);\n-\n-    Label pinned;\n-\n-    __ cbnz(r0, pinned);\n-\n-    \/\/ We've succeeded, set sp to the ContinuationEntry\n-    __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n-    __ mov(sp, rscratch1);\n-    continuation_enter_cleanup(masm);\n-\n-    __ bind(pinned); \/\/ pinned -- return to caller\n-\n-    __ leave();\n-    __ ret(lr);\n-\n-    OopMap* map = new OopMap(framesize, 1);\n-    oop_maps->add_gc_map(the_pc - start, map);\n-\n-    RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub::new_runtime_stub(name,\n-                                  &code,\n-                                  frame_complete,\n-                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                  oop_maps, false);\n-    return stub;\n-  }\n-\n@@ -8021,3 +7982,0 @@\n-    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n-    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n-                                        : StubRoutines::_cont_doYield_stub->entry_point();\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":200,"deletions":242,"binary":false,"changes":442,"status":"modified"},{"patch":"@@ -853,12 +853,0 @@\n-address TemplateInterpreterGenerator::generate_Continuation_doYield_entry(void) {\n-  if (!Continuations::enabled()) return nullptr;\n-\n-  address entry = __ pc();\n-  assert(StubRoutines::cont_doYield() != NULL, \"stub not yet generated\");\n-\n-  __ push_cont_fastpath(rthread);\n-  __ far_jump(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::cont_doYield())));\n-\n-  return entry;\n-}\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -364,12 +364,0 @@\n-void LIRGenerator::do_continuation_doYield(Intrinsic* x) {\n-  BasicTypeList signature(0);\n-  CallingConvention* cc = frame_map()->java_calling_convention(&signature, true);\n-\n-  const LIR_Opr result_reg = result_register_for(x->type());\n-  address entry = StubRoutines::cont_doYield();\n-  LIR_Opr result = rlock_result(x);\n-  CodeEmitInfo* info = state_for(x, x->state());\n-  __ call_runtime(entry, LIR_OprFact::illegalOpr, result_reg, cc->args(), info);\n-  __ move(result_reg, result);\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -529,0 +529,9 @@\n+  \/\/ Is marking still active?\n+  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n+    __ cmpl(queue_active, 0);\n+  } else {\n+    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n+    __ cmpb(queue_active, 0);\n+  }\n+  __ jcc(Assembler::equal, done);\n+\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1090,51 +1090,1 @@\n-#ifdef _LP64\n- private:\n-  \/\/ Initialized in macroAssembler_x86_constants.cpp\n-  static address ONE;\n-  static address ONEHALF;\n-  static address SIGN_MASK;\n-  static address TWO_POW_55;\n-  static address TWO_POW_M55;\n-  static address SHIFTER;\n-  static address ZERO;\n-  static address NEG_ZERO;\n-  static address PI32INV;\n-  static address PI_INV_TABLE;\n-  static address Ctable;\n-  static address SC_1;\n-  static address SC_2;\n-  static address SC_3;\n-  static address SC_4;\n-  static address PI_4;\n-  static address P_1;\n-  static address P_3;\n-  static address P_2;\n-\n- public:\n-  void fast_log(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register tmp1, Register tmp2);\n-\n-  void fast_log10(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                  XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                  Register rax, Register rcx, Register rdx, Register r11, Register tmp);\n-\n-  void fast_pow(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3, XMMRegister xmm4,\n-                XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7, Register rax, Register rcx,\n-                Register rdx, Register tmp1, Register tmp2, Register tmp3, Register tmp4);\n-\n-  void fast_sin(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rbx, Register rcx, Register rdx, Register tmp1);\n-\n-  void fast_cos(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register r8,\n-                Register  r9, Register r10, Register r11, Register tmp);\n-\n-  void fast_tan(XMMRegister xmm0, XMMRegister xmm1, XMMRegister xmm2, XMMRegister xmm3,\n-                XMMRegister xmm4, XMMRegister xmm5, XMMRegister xmm6, XMMRegister xmm7,\n-                Register rax, Register rcx, Register rdx, Register r8,\n-                Register  r9, Register r10, Register r11, Register tmp);\n-\n-#else\n+#ifndef _LP64\n@@ -1184,1 +1134,1 @@\n-#endif\n+#endif \/\/ !_LP64\n@@ -2039,1 +1989,0 @@\n-  void updateBytesAdler32(Register adler32, Register buf, Register length, XMMRegister shuf0, XMMRegister shuf1, ExternalAddress scale);\n@@ -2126,27 +2075,1 @@\n-#if COMPILER2_OR_JVMCI\n-  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n-                                    Register to, Register count, int shift,\n-                                    Register index, Register temp,\n-                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n-\n-  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n-                                             Register to, Register start_index, Register end_index,\n-                                             Register count, int shift, Register temp,\n-                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n-\n-  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register index,\n-                         Register temp, int shift = Address::times_1, int offset = 0,\n-                         bool use64byteVector = false);\n-\n-  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n-                         KRegister mask, Register length, Register index,\n-                         Register temp, int shift = Address::times_1, int offset = 0);\n-\n-  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                  int shift = Address::times_1, int offset = 0);\n-\n-  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n-                  bool conjoint, int shift = Address::times_1, int offset = 0,\n-                  bool use64byteVector = false);\n-\n+#ifdef COMPILER2_OR_JVMCI\n@@ -2155,1 +2078,0 @@\n-\n@@ -2161,1 +2083,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":3,"deletions":82,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -1550,7 +1550,7 @@\n-                                 const VMRegPair* regs,\n-                                 int& exception_offset,\n-                                 OopMapSet* oop_maps,\n-                                 int& frame_complete,\n-                                 int& stack_slots,\n-                                 int& interpreted_entry_offset,\n-                                 int& compiled_entry_offset) {\n+                                   const VMRegPair* regs,\n+                                   int& exception_offset,\n+                                   OopMapSet* oop_maps,\n+                                   int& frame_complete,\n+                                   int& stack_slots,\n+                                   int& interpreted_entry_offset,\n+                                   int& compiled_entry_offset) {\n@@ -1723,0 +1723,55 @@\n+static void gen_continuation_yield(MacroAssembler* masm,\n+                                   const VMRegPair* regs,\n+                                   int& exception_offset,\n+                                   OopMapSet* oop_maps,\n+                                   int& frame_complete,\n+                                   int& stack_slots,\n+                                   int& interpreted_entry_offset,\n+                                   int& compiled_entry_offset) {\n+  enum layout {\n+    rbp_off,\n+    rbpH_off,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n+  stack_slots = framesize \/  VMRegImpl::slots_per_word;\n+  assert(stack_slots == 2, \"recheck layout\");\n+\n+  address start = __ pc();\n+  compiled_entry_offset = __ pc() - start;\n+  __ enter();\n+  address the_pc = __ pc();\n+\n+  frame_complete = the_pc - start;\n+\n+  \/\/ This nop must be exactly at the PC we push into the frame info.\n+  \/\/ We use this nop for fast CodeBlob lookup, associate the OopMap\n+  \/\/ with it right away.\n+  __ post_call_nop();\n+  OopMap* map = new OopMap(framesize, 1);\n+  oop_maps->add_gc_map(frame_complete, map);\n+\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, rsp);\n+  __ call_VM_leaf(Continuation::freeze_entry(), 2);\n+  __ reset_last_Java_frame(true);\n+\n+  Label L_pinned;\n+\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::notZero, L_pinned);\n+\n+  __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  __ continuation_enter_cleanup();\n+  __ pop(rbp);\n+  __ ret(0);\n+\n+  __ bind(L_pinned);\n+\n+  \/\/ Pinned, return to caller\n+  __ leave();\n+  __ ret(0);\n+}\n+\n@@ -1807,3 +1862,1 @@\n-  if (method->is_continuation_enter_intrinsic()) {\n-    vmIntrinsics::ID iid = method->intrinsic_id();\n-    intptr_t start = (intptr_t)__ pc();\n+  if (method->is_continuation_native_intrinsic()) {\n@@ -1814,1 +1867,1 @@\n-    OopMapSet* oop_maps =  new OopMapSet();\n+    OopMapSet* oop_maps = new OopMapSet();\n@@ -1816,8 +1869,21 @@\n-    gen_continuation_enter(masm,\n-                         in_regs,\n-                         exception_offset,\n-                         oop_maps,\n-                         frame_complete,\n-                         stack_slots,\n-                         interpreted_entry_offset,\n-                         vep_offset);\n+    if (method->is_continuation_enter_intrinsic()) {\n+      gen_continuation_enter(masm,\n+                             in_regs,\n+                             exception_offset,\n+                             oop_maps,\n+                             frame_complete,\n+                             stack_slots,\n+                             interpreted_entry_offset,\n+                             vep_offset);\n+    } else if (method->is_continuation_yield_intrinsic()) {\n+      gen_continuation_yield(masm,\n+                             in_regs,\n+                             exception_offset,\n+                             oop_maps,\n+                             frame_complete,\n+                             stack_slots,\n+                             interpreted_entry_offset,\n+                             vep_offset);\n+    } else {\n+      guarantee(false, \"Unknown Continuation native intrinsic\");\n+    }\n@@ -1835,1 +1901,5 @@\n-    ContinuationEntry::set_enter_code(nm, interpreted_entry_offset);\n+    if (method->is_continuation_enter_intrinsic()) {\n+      ContinuationEntry::set_enter_code(nm, interpreted_entry_offset);\n+    } else if (method->is_continuation_yield_intrinsic()) {\n+      _cont_doYield_stub = nm;\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":90,"deletions":20,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -28,2 +28,1 @@\n-#include \"asm\/macroAssembler.inline.hpp\"\n-#include \"ci\/ciUtilities.hpp\"\n+#include \"classfile\/vmIntrinsics.hpp\"\n@@ -35,12 +34,0 @@\n-#include \"interpreter\/interpreter.hpp\"\n-#include \"nativeInst_x86.hpp\"\n-#include \"oops\/instanceOop.hpp\"\n-#include \"oops\/method.hpp\"\n-#include \"oops\/objArrayKlass.hpp\"\n-#include \"oops\/oop.inline.hpp\"\n-#include \"prims\/methodHandles.hpp\"\n-#include \"register_x86.hpp\"\n-#include \"runtime\/continuation.hpp\"\n-#include \"runtime\/continuationEntry.inline.hpp\"\n-#include \"runtime\/frame.inline.hpp\"\n-#include \"runtime\/handles.inline.hpp\"\n@@ -52,1 +39,0 @@\n-#include \"runtime\/stubCodeGenerator.hpp\"\n@@ -55,0 +41,1 @@\n+#include \"stubGenerator_x86_64.hpp\"\n@@ -58,0 +45,1 @@\n+#include \"opto\/c2_globals.hpp\"\n@@ -69,2 +57,0 @@\n-\n-\/\/ Declaration and definition of StubGenerator (no .hpp file).\n@@ -76,1 +62,0 @@\n-#define a__ ((Assembler*)_masm)->\n@@ -82,1 +67,1 @@\n-#endif\n+#endif \/\/ PRODUCT\n@@ -85,133 +70,70 @@\n-const int MXCSR_MASK = 0xFFC0;  \/\/ Mask out any pending exceptions\n-\n-\/\/ Stub Code definitions\n-\n-class StubGenerator: public StubCodeGenerator {\n- private:\n-\n-#ifdef PRODUCT\n-#define INC_COUNTER_NP(counter, rscratch) ((void)0)\n-#else\n-  void inc_counter_np(int& counter, Register rscratch) {\n-    __ incrementl(ExternalAddress((address)&counter), rscratch);\n-  }\n-#define INC_COUNTER_NP(counter, rscratch) \\\n-  BLOCK_COMMENT(\"inc_counter \" #counter); \\\n-  inc_counter_np(counter, rscratch);\n-#endif\n-\n-  \/\/ Call stubs are used to call Java from C\n-  \/\/\n-  \/\/ Linux Arguments:\n-  \/\/    c_rarg0:   call wrapper address                   address\n-  \/\/    c_rarg1:   result                                 address\n-  \/\/    c_rarg2:   result type                            BasicType\n-  \/\/    c_rarg3:   method                                 Method*\n-  \/\/    c_rarg4:   (interpreter) entry point              address\n-  \/\/    c_rarg5:   parameters                             intptr_t*\n-  \/\/    16(rbp): parameter size (in words)              int\n-  \/\/    24(rbp): thread                                 Thread*\n-  \/\/\n-  \/\/     [ return_from_Java     ] <--- rsp\n-  \/\/     [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -12 [ argument word 1      ]\n-  \/\/ -11 [ saved r15            ] <--- rsp_after_call\n-  \/\/ -10 [ saved r14            ]\n-  \/\/  -9 [ saved r13            ]\n-  \/\/  -8 [ saved r12            ]\n-  \/\/  -7 [ saved rbx            ]\n-  \/\/  -6 [ call wrapper         ]\n-  \/\/  -5 [ result               ]\n-  \/\/  -4 [ result type          ]\n-  \/\/  -3 [ method               ]\n-  \/\/  -2 [ entry point          ]\n-  \/\/  -1 [ parameters           ]\n-  \/\/   0 [ saved rbp            ] <--- rbp\n-  \/\/   1 [ return address       ]\n-  \/\/   2 [ parameter size       ]\n-  \/\/   3 [ thread               ]\n-  \/\/\n-  \/\/ Windows Arguments:\n-  \/\/    c_rarg0:   call wrapper address                   address\n-  \/\/    c_rarg1:   result                                 address\n-  \/\/    c_rarg2:   result type                            BasicType\n-  \/\/    c_rarg3:   method                                 Method*\n-  \/\/    48(rbp): (interpreter) entry point              address\n-  \/\/    56(rbp): parameters                             intptr_t*\n-  \/\/    64(rbp): parameter size (in words)              int\n-  \/\/    72(rbp): thread                                 Thread*\n-  \/\/\n-  \/\/     [ return_from_Java     ] <--- rsp\n-  \/\/     [ argument word n      ]\n-  \/\/      ...\n-  \/\/ -60 [ argument word 1      ]\n-  \/\/ -59 [ saved xmm31          ] <--- rsp after_call\n-  \/\/     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)\n-  \/\/ -27 [ saved xmm15          ]\n-  \/\/     [ saved xmm7-xmm14     ]\n-  \/\/  -9 [ saved xmm6           ] (each xmm register takes 2 slots)\n-  \/\/  -7 [ saved r15            ]\n-  \/\/  -6 [ saved r14            ]\n-  \/\/  -5 [ saved r13            ]\n-  \/\/  -4 [ saved r12            ]\n-  \/\/  -3 [ saved rdi            ]\n-  \/\/  -2 [ saved rsi            ]\n-  \/\/  -1 [ saved rbx            ]\n-  \/\/   0 [ saved rbp            ] <--- rbp\n-  \/\/   1 [ return address       ]\n-  \/\/   2 [ call wrapper         ]\n-  \/\/   3 [ result               ]\n-  \/\/   4 [ result type          ]\n-  \/\/   5 [ method               ]\n-  \/\/   6 [ entry point          ]\n-  \/\/   7 [ parameters           ]\n-  \/\/   8 [ parameter size       ]\n-  \/\/   9 [ thread               ]\n-  \/\/\n-  \/\/    Windows reserves the callers stack space for arguments 1-4.\n-  \/\/    We spill c_rarg0-c_rarg3 to this space.\n-  \/\/ Call stub stack layout word offsets from rbp\n-  enum call_stub_layout {\n-#ifdef _WIN64\n-    xmm_save_first     = 6,  \/\/ save from xmm6\n-    xmm_save_last      = 31, \/\/ to xmm31\n-    xmm_save_base      = -9,\n-    rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), \/\/ -27\n-    r15_off            = -7,\n-    r14_off            = -6,\n-    r13_off            = -5,\n-    r12_off            = -4,\n-    rdi_off            = -3,\n-    rsi_off            = -2,\n-    rbx_off            = -1,\n-    rbp_off            =  0,\n-    retaddr_off        =  1,\n-    call_wrapper_off   =  2,\n-    result_off         =  3,\n-    result_type_off    =  4,\n-    method_off         =  5,\n-    entry_point_off    =  6,\n-    parameters_off     =  7,\n-    parameter_size_off =  8,\n-    thread_off         =  9\n-#else\n-    rsp_after_call_off = -12,\n-    mxcsr_off          = rsp_after_call_off,\n-    r15_off            = -11,\n-    r14_off            = -10,\n-    r13_off            = -9,\n-    r12_off            = -8,\n-    rbx_off            = -7,\n-    call_wrapper_off   = -6,\n-    result_off         = -5,\n-    result_type_off    = -4,\n-    method_off         = -3,\n-    entry_point_off    = -2,\n-    parameters_off     = -1,\n-    rbp_off            =  0,\n-    retaddr_off        =  1,\n-    parameter_size_off =  2,\n-    thread_off         =  3\n-#endif\n-  };\n+\/\/\n+\/\/ Linux Arguments:\n+\/\/    c_rarg0:   call wrapper address                   address\n+\/\/    c_rarg1:   result                                 address\n+\/\/    c_rarg2:   result type                            BasicType\n+\/\/    c_rarg3:   method                                 Method*\n+\/\/    c_rarg4:   (interpreter) entry point              address\n+\/\/    c_rarg5:   parameters                             intptr_t*\n+\/\/    16(rbp): parameter size (in words)              int\n+\/\/    24(rbp): thread                                 Thread*\n+\/\/\n+\/\/     [ return_from_Java     ] <--- rsp\n+\/\/     [ argument word n      ]\n+\/\/      ...\n+\/\/ -12 [ argument word 1      ]\n+\/\/ -11 [ saved r15            ] <--- rsp_after_call\n+\/\/ -10 [ saved r14            ]\n+\/\/  -9 [ saved r13            ]\n+\/\/  -8 [ saved r12            ]\n+\/\/  -7 [ saved rbx            ]\n+\/\/  -6 [ call wrapper         ]\n+\/\/  -5 [ result               ]\n+\/\/  -4 [ result type          ]\n+\/\/  -3 [ method               ]\n+\/\/  -2 [ entry point          ]\n+\/\/  -1 [ parameters           ]\n+\/\/   0 [ saved rbp            ] <--- rbp\n+\/\/   1 [ return address       ]\n+\/\/   2 [ parameter size       ]\n+\/\/   3 [ thread               ]\n+\/\/\n+\/\/ Windows Arguments:\n+\/\/    c_rarg0:   call wrapper address                   address\n+\/\/    c_rarg1:   result                                 address\n+\/\/    c_rarg2:   result type                            BasicType\n+\/\/    c_rarg3:   method                                 Method*\n+\/\/    48(rbp): (interpreter) entry point              address\n+\/\/    56(rbp): parameters                             intptr_t*\n+\/\/    64(rbp): parameter size (in words)              int\n+\/\/    72(rbp): thread                                 Thread*\n+\/\/\n+\/\/     [ return_from_Java     ] <--- rsp\n+\/\/     [ argument word n      ]\n+\/\/      ...\n+\/\/ -60 [ argument word 1      ]\n+\/\/ -59 [ saved xmm31          ] <--- rsp after_call\n+\/\/     [ saved xmm16-xmm30    ] (EVEX enabled, else the space is blank)\n+\/\/ -27 [ saved xmm15          ]\n+\/\/     [ saved xmm7-xmm14     ]\n+\/\/  -9 [ saved xmm6           ] (each xmm register takes 2 slots)\n+\/\/  -7 [ saved r15            ]\n+\/\/  -6 [ saved r14            ]\n+\/\/  -5 [ saved r13            ]\n+\/\/  -4 [ saved r12            ]\n+\/\/  -3 [ saved rdi            ]\n+\/\/  -2 [ saved rsi            ]\n+\/\/  -1 [ saved rbx            ]\n+\/\/   0 [ saved rbp            ] <--- rbp\n+\/\/   1 [ return address       ]\n+\/\/   2 [ call wrapper         ]\n+\/\/   3 [ result               ]\n+\/\/   4 [ result type          ]\n+\/\/   5 [ method               ]\n+\/\/   6 [ entry point          ]\n+\/\/   7 [ parameters           ]\n+\/\/   8 [ parameter size       ]\n+\/\/   9 [ thread               ]\n+\/\/\n+\/\/    Windows reserves the callers stack space for arguments 1-4.\n+\/\/    We spill c_rarg0-c_rarg3 to this space.\n@@ -220,0 +142,1 @@\n+\/\/ Call stub stack layout word offsets from rbp\n@@ -221,38 +144,83 @@\n-  Address xmm_save(int reg) {\n-    assert(reg >= xmm_save_first && reg <= xmm_save_last, \"XMM register number out of range\");\n-    return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);\n-  }\n-#endif\n-\n-  address generate_call_stub(address& return_address) {\n-    assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &&\n-           (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n-           \"adjust this code\");\n-    StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n-    address start = __ pc();\n-\n-    \/\/ same as in generate_catch_exception()!\n-    const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n-\n-    const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);\n-    const Address result        (rbp, result_off         * wordSize);\n-    const Address result_type   (rbp, result_type_off    * wordSize);\n-    const Address method        (rbp, method_off         * wordSize);\n-    const Address entry_point   (rbp, entry_point_off    * wordSize);\n-    const Address parameters    (rbp, parameters_off     * wordSize);\n-    const Address parameter_size(rbp, parameter_size_off * wordSize);\n-\n-    \/\/ same as in generate_catch_exception()!\n-    const Address thread        (rbp, thread_off         * wordSize);\n-\n-    const Address r15_save(rbp, r15_off * wordSize);\n-    const Address r14_save(rbp, r14_off * wordSize);\n-    const Address r13_save(rbp, r13_off * wordSize);\n-    const Address r12_save(rbp, r12_off * wordSize);\n-    const Address rbx_save(rbp, rbx_off * wordSize);\n-\n-    \/\/ stub code\n-    __ enter();\n-    __ subptr(rsp, -rsp_after_call_off * wordSize);\n-\n-    \/\/ save register parameters\n+enum call_stub_layout {\n+  xmm_save_first     = 6,  \/\/ save from xmm6\n+  xmm_save_last      = 31, \/\/ to xmm31\n+  xmm_save_base      = -9,\n+  rsp_after_call_off = xmm_save_base - 2 * (xmm_save_last - xmm_save_first), \/\/ -27\n+  r15_off            = -7,\n+  r14_off            = -6,\n+  r13_off            = -5,\n+  r12_off            = -4,\n+  rdi_off            = -3,\n+  rsi_off            = -2,\n+  rbx_off            = -1,\n+  rbp_off            =  0,\n+  retaddr_off        =  1,\n+  call_wrapper_off   =  2,\n+  result_off         =  3,\n+  result_type_off    =  4,\n+  method_off         =  5,\n+  entry_point_off    =  6,\n+  parameters_off     =  7,\n+  parameter_size_off =  8,\n+  thread_off         =  9\n+};\n+\n+static Address xmm_save(int reg) {\n+  assert(reg >= xmm_save_first && reg <= xmm_save_last, \"XMM register number out of range\");\n+  return Address(rbp, (xmm_save_base - (reg - xmm_save_first) * 2) * wordSize);\n+}\n+#else \/\/ !_WIN64\n+enum call_stub_layout {\n+  rsp_after_call_off = -12,\n+  mxcsr_off          = rsp_after_call_off,\n+  r15_off            = -11,\n+  r14_off            = -10,\n+  r13_off            = -9,\n+  r12_off            = -8,\n+  rbx_off            = -7,\n+  call_wrapper_off   = -6,\n+  result_off         = -5,\n+  result_type_off    = -4,\n+  method_off         = -3,\n+  entry_point_off    = -2,\n+  parameters_off     = -1,\n+  rbp_off            =  0,\n+  retaddr_off        =  1,\n+  parameter_size_off =  2,\n+  thread_off         =  3\n+};\n+#endif \/\/ _WIN64\n+\n+address StubGenerator::generate_call_stub(address& return_address) {\n+\n+  assert((int)frame::entry_frame_after_call_words == -(int)rsp_after_call_off + 1 &&\n+         (int)frame::entry_frame_call_wrapper_offset == (int)call_wrapper_off,\n+         \"adjust this code\");\n+  StubCodeMark mark(this, \"StubRoutines\", \"call_stub\");\n+  address start = __ pc();\n+\n+  \/\/ same as in generate_catch_exception()!\n+  const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n+\n+  const Address call_wrapper  (rbp, call_wrapper_off   * wordSize);\n+  const Address result        (rbp, result_off         * wordSize);\n+  const Address result_type   (rbp, result_type_off    * wordSize);\n+  const Address method        (rbp, method_off         * wordSize);\n+  const Address entry_point   (rbp, entry_point_off    * wordSize);\n+  const Address parameters    (rbp, parameters_off     * wordSize);\n+  const Address parameter_size(rbp, parameter_size_off * wordSize);\n+\n+  \/\/ same as in generate_catch_exception()!\n+  const Address thread        (rbp, thread_off         * wordSize);\n+\n+  const Address r15_save(rbp, r15_off * wordSize);\n+  const Address r14_save(rbp, r14_off * wordSize);\n+  const Address r13_save(rbp, r13_off * wordSize);\n+  const Address r12_save(rbp, r12_off * wordSize);\n+  const Address rbx_save(rbp, rbx_off * wordSize);\n+\n+  \/\/ stub code\n+  __ enter();\n+  __ subptr(rsp, -rsp_after_call_off * wordSize);\n+\n+  \/\/ save register parameters\n@@ -260,2 +228,2 @@\n-    __ movptr(parameters,   c_rarg5); \/\/ parameters\n-    __ movptr(entry_point,  c_rarg4); \/\/ entry_point\n+  __ movptr(parameters,   c_rarg5); \/\/ parameters\n+  __ movptr(entry_point,  c_rarg4); \/\/ entry_point\n@@ -264,4 +232,4 @@\n-    __ movptr(method,       c_rarg3); \/\/ method\n-    __ movl(result_type,  c_rarg2);   \/\/ result type\n-    __ movptr(result,       c_rarg1); \/\/ result\n-    __ movptr(call_wrapper, c_rarg0); \/\/ call wrapper\n+  __ movptr(method,       c_rarg3); \/\/ method\n+  __ movl(result_type,  c_rarg2);   \/\/ result type\n+  __ movptr(result,       c_rarg1); \/\/ result\n+  __ movptr(call_wrapper, c_rarg0); \/\/ call wrapper\n@@ -269,6 +237,6 @@\n-    \/\/ save regs belonging to calling function\n-    __ movptr(rbx_save, rbx);\n-    __ movptr(r12_save, r12);\n-    __ movptr(r13_save, r13);\n-    __ movptr(r14_save, r14);\n-    __ movptr(r15_save, r15);\n+  \/\/ save regs belonging to calling function\n+  __ movptr(rbx_save, rbx);\n+  __ movptr(r12_save, r12);\n+  __ movptr(r13_save, r13);\n+  __ movptr(r14_save, r14);\n+  __ movptr(r15_save, r15);\n@@ -277,3 +245,7 @@\n-    int last_reg = 15;\n-    if (UseAVX > 2) {\n-      last_reg = 31;\n+  int last_reg = 15;\n+  if (UseAVX > 2) {\n+    last_reg = 31;\n+  }\n+  if (VM_Version::supports_evex()) {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);\n@@ -281,8 +253,3 @@\n-    if (VM_Version::supports_evex()) {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ vextractf32x4(xmm_save(i), as_XMMRegister(i), 0);\n-      }\n-    } else {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ movdqu(xmm_save(i), as_XMMRegister(i));\n-      }\n+  } else {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ movdqu(xmm_save(i), as_XMMRegister(i));\n@@ -290,0 +257,1 @@\n+  }\n@@ -291,2 +259,2 @@\n-    const Address rdi_save(rbp, rdi_off * wordSize);\n-    const Address rsi_save(rbp, rsi_off * wordSize);\n+  const Address rdi_save(rbp, rdi_off * wordSize);\n+  const Address rsi_save(rbp, rsi_off * wordSize);\n@@ -294,2 +262,2 @@\n-    __ movptr(rsi_save, rsi);\n-    __ movptr(rdi_save, rdi);\n+  __ movptr(rsi_save, rsi);\n+  __ movptr(rdi_save, rdi);\n@@ -297,12 +265,12 @@\n-    const Address mxcsr_save(rbp, mxcsr_off * wordSize);\n-    {\n-      Label skip_ldmx;\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ cmp32(rax, mxcsr_std, rscratch1);\n-      __ jcc(Assembler::equal, skip_ldmx);\n-      __ ldmxcsr(mxcsr_std, rscratch1);\n-      __ bind(skip_ldmx);\n-    }\n+  const Address mxcsr_save(rbp, mxcsr_off * wordSize);\n+  {\n+    Label skip_ldmx;\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, 0xFFC0); \/\/ Mask out any pending exceptions (only check control and mask bits)\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+    __ cmp32(rax, mxcsr_std, rscratch1);\n+    __ jcc(Assembler::equal, skip_ldmx);\n+    __ ldmxcsr(mxcsr_std, rscratch1);\n+    __ bind(skip_ldmx);\n+  }\n@@ -311,3 +279,3 @@\n-    \/\/ Load up thread register\n-    __ movptr(r15_thread, thread);\n-    __ reinit_heapbase();\n+  \/\/ Load up thread register\n+  __ movptr(r15_thread, thread);\n+  __ reinit_heapbase();\n@@ -316,8 +284,8 @@\n-    \/\/ make sure we have no pending exceptions\n-    {\n-      Label L;\n-      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure we have no pending exceptions\n+  {\n+    Label L;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"StubRoutines::call_stub: entered with pending exception\");\n+    __ bind(L);\n+  }\n@@ -326,51 +294,51 @@\n-    \/\/ pass parameters if any\n-    BLOCK_COMMENT(\"pass parameters if any\");\n-    Label parameters_done;\n-    __ movl(c_rarg3, parameter_size);\n-    __ testl(c_rarg3, c_rarg3);\n-    __ jcc(Assembler::zero, parameters_done);\n-\n-    Label loop;\n-    __ movptr(c_rarg2, parameters);       \/\/ parameter pointer\n-    __ movl(c_rarg1, c_rarg3);            \/\/ parameter counter is in c_rarg1\n-    __ BIND(loop);\n-    __ movptr(rax, Address(c_rarg2, 0));\/\/ get parameter\n-    __ addptr(c_rarg2, wordSize);       \/\/ advance to next parameter\n-    __ decrementl(c_rarg1);             \/\/ decrement counter\n-    __ push(rax);                       \/\/ pass parameter\n-    __ jcc(Assembler::notZero, loop);\n-\n-    \/\/ call Java function\n-    __ BIND(parameters_done);\n-    __ movptr(rbx, method);             \/\/ get Method*\n-    __ movptr(c_rarg1, entry_point);    \/\/ get entry_point\n-    __ mov(r13, rsp);                   \/\/ set sender sp\n-    BLOCK_COMMENT(\"call Java function\");\n-    __ call(c_rarg1);\n-\n-    BLOCK_COMMENT(\"call_stub_return_address:\");\n-    return_address = __ pc();\n-\n-    \/\/ store result depending on type (everything that is not\n-    \/\/ T_OBJECT, T_PRIMITIVE_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n-    __ movptr(r13, result);\n-    Label is_long, is_float, is_double, check_prim, exit;\n-    __ movl(rbx, result_type);\n-    __ cmpl(rbx, T_OBJECT);\n-    __ jcc(Assembler::equal, check_prim);\n-    __ cmpl(rbx, T_PRIMITIVE_OBJECT);\n-    __ jcc(Assembler::equal, check_prim);\n-    __ cmpl(rbx, T_LONG);\n-    __ jcc(Assembler::equal, is_long);\n-    __ cmpl(rbx, T_FLOAT);\n-    __ jcc(Assembler::equal, is_float);\n-    __ cmpl(rbx, T_DOUBLE);\n-    __ jcc(Assembler::equal, is_double);\n-\n-    \/\/ handle T_INT case\n-    __ movl(Address(r13, 0), rax);\n-\n-    __ BIND(exit);\n-\n-    \/\/ pop parameters\n-    __ lea(rsp, rsp_after_call);\n+  \/\/ pass parameters if any\n+  BLOCK_COMMENT(\"pass parameters if any\");\n+  Label parameters_done;\n+  __ movl(c_rarg3, parameter_size);\n+  __ testl(c_rarg3, c_rarg3);\n+  __ jcc(Assembler::zero, parameters_done);\n+\n+  Label loop;\n+  __ movptr(c_rarg2, parameters);       \/\/ parameter pointer\n+  __ movl(c_rarg1, c_rarg3);            \/\/ parameter counter is in c_rarg1\n+  __ BIND(loop);\n+  __ movptr(rax, Address(c_rarg2, 0));\/\/ get parameter\n+  __ addptr(c_rarg2, wordSize);       \/\/ advance to next parameter\n+  __ decrementl(c_rarg1);             \/\/ decrement counter\n+  __ push(rax);                       \/\/ pass parameter\n+  __ jcc(Assembler::notZero, loop);\n+\n+  \/\/ call Java function\n+  __ BIND(parameters_done);\n+  __ movptr(rbx, method);             \/\/ get Method*\n+  __ movptr(c_rarg1, entry_point);    \/\/ get entry_point\n+  __ mov(r13, rsp);                   \/\/ set sender sp\n+  BLOCK_COMMENT(\"call Java function\");\n+  __ call(c_rarg1);\n+\n+  BLOCK_COMMENT(\"call_stub_return_address:\");\n+  return_address = __ pc();\n+\n+  \/\/ store result depending on type (everything that is not\n+  \/\/ T_OBJECT, T_PRIMITIVE_OBJECT, T_LONG, T_FLOAT or T_DOUBLE is treated as T_INT)\n+  __ movptr(r13, result);\n+  Label is_long, is_float, is_double, check_prim, exit;\n+  __ movl(rbx, result_type);\n+  __ cmpl(rbx, T_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_PRIMITIVE_OBJECT);\n+  __ jcc(Assembler::equal, check_prim);\n+  __ cmpl(rbx, T_LONG);\n+  __ jcc(Assembler::equal, is_long);\n+  __ cmpl(rbx, T_FLOAT);\n+  __ jcc(Assembler::equal, is_float);\n+  __ cmpl(rbx, T_DOUBLE);\n+  __ jcc(Assembler::equal, is_double);\n+\n+  \/\/ handle T_INT case\n+  __ movl(Address(r13, 0), rax);\n+\n+  __ BIND(exit);\n+\n+  \/\/ pop parameters\n+  __ lea(rsp, rsp_after_call);\n@@ -379,17 +347,17 @@\n-    \/\/ verify that threads correspond\n-    {\n-     Label L1, L2, L3;\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L1);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is corrupted\");\n-      __ bind(L1);\n-      __ get_thread(rbx);\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-      __ bind(L2);\n-      __ cmpptr(r15_thread, rbx);\n-      __ jcc(Assembler::equal, L3);\n-      __ stop(\"StubRoutines::call_stub: threads must correspond\");\n-      __ bind(L3);\n-    }\n+  \/\/ verify that threads correspond\n+  {\n+   Label L1, L2, L3;\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L1);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is corrupted\");\n+    __ bind(L1);\n+    __ get_thread(rbx);\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n+    __ bind(L2);\n+    __ cmpptr(r15_thread, rbx);\n+    __ jcc(Assembler::equal, L3);\n+    __ stop(\"StubRoutines::call_stub: threads must correspond\");\n+    __ bind(L3);\n+  }\n@@ -398,1 +366,1 @@\n-    __ pop_cont_fastpath();\n+  __ pop_cont_fastpath();\n@@ -400,1 +368,1 @@\n-    \/\/ restore regs belonging to calling function\n+  \/\/ restore regs belonging to calling function\n@@ -402,9 +370,8 @@\n-    \/\/ emit the restores for xmm regs\n-    if (VM_Version::supports_evex()) {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);\n-      }\n-    } else {\n-      for (int i = xmm_save_first; i <= last_reg; i++) {\n-        __ movdqu(as_XMMRegister(i), xmm_save(i));\n-      }\n+  \/\/ emit the restores for xmm regs\n+  if (VM_Version::supports_evex()) {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ vinsertf32x4(as_XMMRegister(i), as_XMMRegister(i), xmm_save(i), 0);\n+    }\n+  } else {\n+    for (int i = xmm_save_first; i <= last_reg; i++) {\n+      __ movdqu(as_XMMRegister(i), xmm_save(i));\n@@ -412,0 +379,1 @@\n+  }\n@@ -413,5 +381,5 @@\n-    __ movptr(r15, r15_save);\n-    __ movptr(r14, r14_save);\n-    __ movptr(r13, r13_save);\n-    __ movptr(r12, r12_save);\n-    __ movptr(rbx, rbx_save);\n+  __ movptr(r15, r15_save);\n+  __ movptr(r14, r14_save);\n+  __ movptr(r13, r13_save);\n+  __ movptr(r12, r12_save);\n+  __ movptr(rbx, rbx_save);\n@@ -420,2 +388,2 @@\n-    __ movptr(rdi, rdi_save);\n-    __ movptr(rsi, rsi_save);\n+  __ movptr(rdi, rdi_save);\n+  __ movptr(rsi, rsi_save);\n@@ -423,1 +391,1 @@\n-    __ ldmxcsr(mxcsr_save);\n+  __ ldmxcsr(mxcsr_save);\n@@ -426,24 +394,20 @@\n-    \/\/ restore rsp\n-    __ addptr(rsp, -rsp_after_call_off * wordSize);\n-\n-    \/\/ return\n-    __ vzeroupper();\n-    __ pop(rbp);\n-    __ ret(0);\n-\n-    \/\/ handle return types different from T_INT\n-    __ BIND(check_prim);\n-    if (InlineTypeReturnedAsFields) {\n-      \/\/ Check for scalarized return value\n-      __ testptr(rax, 1);\n-      __ jcc(Assembler::zero, is_long);\n-      \/\/ Load pack handler address\n-      __ andptr(rax, -2);\n-      __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n-      __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n-      \/\/ Call pack handler to initialize the buffer\n-      __ call(rbx);\n-      __ jmp(exit);\n-    }\n-    __ BIND(is_long);\n-    __ movq(Address(r13, 0), rax);\n+  \/\/ restore rsp\n+  __ addptr(rsp, -rsp_after_call_off * wordSize);\n+\n+  \/\/ return\n+  __ vzeroupper();\n+  __ pop(rbp);\n+  __ ret(0);\n+\n+  \/\/ handle return types different from T_INT\n+  __ BIND(check_prim);\n+  if (InlineTypeReturnedAsFields) {\n+    \/\/ Check for scalarized return value\n+    __ testptr(rax, 1);\n+    __ jcc(Assembler::zero, is_long);\n+    \/\/ Load pack handler address\n+    __ andptr(rax, -2);\n+    __ movptr(rax, Address(rax, InstanceKlass::adr_inlineklass_fixed_block_offset()));\n+    __ movptr(rbx, Address(rax, InlineKlass::pack_handler_jobject_offset()));\n+    \/\/ Call pack handler to initialize the buffer\n+    __ call(rbx);\n@@ -451,0 +415,4 @@\n+  }\n+  __ BIND(is_long);\n+  __ movq(Address(r13, 0), rax);\n+  __ jmp(exit);\n@@ -452,3 +420,3 @@\n-    __ BIND(is_float);\n-    __ movflt(Address(r13, 0), xmm0);\n-    __ jmp(exit);\n+  __ BIND(is_float);\n+  __ movflt(Address(r13, 0), xmm0);\n+  __ jmp(exit);\n@@ -456,3 +424,3 @@\n-    __ BIND(is_double);\n-    __ movdbl(Address(r13, 0), xmm0);\n-    __ jmp(exit);\n+  __ BIND(is_double);\n+  __ movdbl(Address(r13, 0), xmm0);\n+  __ jmp(exit);\n@@ -460,2 +428,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -463,11 +431,11 @@\n-  \/\/ Return point for a Java call if there's an exception thrown in\n-  \/\/ Java code.  The exception is caught and transformed into a\n-  \/\/ pending exception stored in JavaThread that can be tested from\n-  \/\/ within the VM.\n-  \/\/\n-  \/\/ Note: Usually the parameters are removed by the callee. In case\n-  \/\/ of an exception crossing an activation frame boundary, that is\n-  \/\/ not the case if the callee is compiled code => need to setup the\n-  \/\/ rsp.\n-  \/\/\n-  \/\/ rax: exception oop\n+\/\/ Return point for a Java call if there's an exception thrown in\n+\/\/ Java code.  The exception is caught and transformed into a\n+\/\/ pending exception stored in JavaThread that can be tested from\n+\/\/ within the VM.\n+\/\/\n+\/\/ Note: Usually the parameters are removed by the callee. In case\n+\/\/ of an exception crossing an activation frame boundary, that is\n+\/\/ not the case if the callee is compiled code => need to setup the\n+\/\/ rsp.\n+\/\/\n+\/\/ rax: exception oop\n@@ -475,3 +443,3 @@\n-  address generate_catch_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n-    address start = __ pc();\n+address StubGenerator::generate_catch_exception() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"catch_exception\");\n+  address start = __ pc();\n@@ -479,3 +447,3 @@\n-    \/\/ same as in generate_call_stub():\n-    const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n-    const Address thread        (rbp, thread_off         * wordSize);\n+  \/\/ same as in generate_call_stub():\n+  const Address rsp_after_call(rbp, rsp_after_call_off * wordSize);\n+  const Address thread        (rbp, thread_off         * wordSize);\n@@ -484,17 +452,17 @@\n-    \/\/ verify that threads correspond\n-    {\n-      Label L1, L2, L3;\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L1);\n-      __ stop(\"StubRoutines::catch_exception: r15_thread is corrupted\");\n-      __ bind(L1);\n-      __ get_thread(rbx);\n-      __ cmpptr(r15_thread, thread);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::catch_exception: r15_thread is modified by call\");\n-      __ bind(L2);\n-      __ cmpptr(r15_thread, rbx);\n-      __ jcc(Assembler::equal, L3);\n-      __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n-      __ bind(L3);\n-    }\n+  \/\/ verify that threads correspond\n+  {\n+    Label L1, L2, L3;\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L1);\n+    __ stop(\"StubRoutines::catch_exception: r15_thread is corrupted\");\n+    __ bind(L1);\n+    __ get_thread(rbx);\n+    __ cmpptr(r15_thread, thread);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::catch_exception: r15_thread is modified by call\");\n+    __ bind(L2);\n+    __ cmpptr(r15_thread, rbx);\n+    __ jcc(Assembler::equal, L3);\n+    __ stop(\"StubRoutines::catch_exception: threads must correspond\");\n+    __ bind(L3);\n+  }\n@@ -503,2 +471,2 @@\n-    \/\/ set pending exception\n-    __ verify_oop(rax);\n+  \/\/ set pending exception\n+  __ verify_oop(rax);\n@@ -506,4 +474,4 @@\n-    __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);\n-    __ lea(rscratch1, ExternalAddress((address)__FILE__));\n-    __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);\n-    __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);\n+  __ movptr(Address(r15_thread, Thread::pending_exception_offset()), rax);\n+  __ lea(rscratch1, ExternalAddress((address)__FILE__));\n+  __ movptr(Address(r15_thread, Thread::exception_file_offset()), rscratch1);\n+  __ movl(Address(r15_thread, Thread::exception_line_offset()), (int)  __LINE__);\n@@ -511,4 +479,4 @@\n-    \/\/ complete return to VM\n-    assert(StubRoutines::_call_stub_return_address != NULL,\n-           \"_call_stub_return_address must have been generated before\");\n-    __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n+  \/\/ complete return to VM\n+  assert(StubRoutines::_call_stub_return_address != NULL,\n+         \"_call_stub_return_address must have been generated before\");\n+  __ jump(RuntimeAddress(StubRoutines::_call_stub_return_address));\n@@ -516,2 +484,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -519,10 +487,10 @@\n-  \/\/ Continuation point for runtime calls returning with a pending\n-  \/\/ exception.  The pending exception check happened in the runtime\n-  \/\/ or native call stub.  The pending exception in Thread is\n-  \/\/ converted into a Java-level exception.\n-  \/\/\n-  \/\/ Contract with Java-level exception handlers:\n-  \/\/ rax: exception\n-  \/\/ rdx: throwing pc\n-  \/\/\n-  \/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n+\/\/ Continuation point for runtime calls returning with a pending\n+\/\/ exception.  The pending exception check happened in the runtime\n+\/\/ or native call stub.  The pending exception in Thread is\n+\/\/ converted into a Java-level exception.\n+\/\/\n+\/\/ Contract with Java-level exception handlers:\n+\/\/ rax: exception\n+\/\/ rdx: throwing pc\n+\/\/\n+\/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n@@ -530,3 +498,3 @@\n-  address generate_forward_exception() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n-    address start = __ pc();\n+address StubGenerator::generate_forward_exception() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"forward exception\");\n+  address start = __ pc();\n@@ -534,8 +502,8 @@\n-    \/\/ Upon entry, the sp points to the return address returning into\n-    \/\/ Java (interpreted or compiled) code; i.e., the return address\n-    \/\/ becomes the throwing pc.\n-    \/\/\n-    \/\/ Arguments pushed before the runtime call are still on the stack\n-    \/\/ but the exception handler will reset the stack pointer ->\n-    \/\/ ignore them.  A potential result in registers can be ignored as\n-    \/\/ well.\n+  \/\/ Upon entry, the sp points to the return address returning into\n+  \/\/ Java (interpreted or compiled) code; i.e., the return address\n+  \/\/ becomes the throwing pc.\n+  \/\/\n+  \/\/ Arguments pushed before the runtime call are still on the stack\n+  \/\/ but the exception handler will reset the stack pointer ->\n+  \/\/ ignore them.  A potential result in registers can be ignored as\n+  \/\/ well.\n@@ -544,8 +512,8 @@\n-    \/\/ make sure this code is only executed if there is a pending exception\n-    {\n-      Label L;\n-      __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure this code is only executed if there is a pending exception\n+  {\n+    Label L;\n+    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+    __ jcc(Assembler::notEqual, L);\n+    __ stop(\"StubRoutines::forward exception: no pending exception (1)\");\n+    __ bind(L);\n+  }\n@@ -554,7 +522,7 @@\n-    \/\/ compute exception handler into rbx\n-    __ movptr(c_rarg0, Address(rsp, 0));\n-    BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n-                         SharedRuntime::exception_handler_for_return_address),\n-                    r15_thread, c_rarg0);\n-    __ mov(rbx, rax);\n+  \/\/ compute exception handler into rbx\n+  __ movptr(c_rarg0, Address(rsp, 0));\n+  BLOCK_COMMENT(\"call exception_handler_for_return_address\");\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address,\n+                       SharedRuntime::exception_handler_for_return_address),\n+                  r15_thread, c_rarg0);\n+  __ mov(rbx, rax);\n@@ -562,4 +530,4 @@\n-    \/\/ setup rax & rdx, remove return address & clear pending exception\n-    __ pop(rdx);\n-    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n-    __ movptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+  \/\/ setup rax & rdx, remove return address & clear pending exception\n+  __ pop(rdx);\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ movptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n@@ -568,8 +536,8 @@\n-    \/\/ make sure exception is set\n-    {\n-      Label L;\n-      __ testptr(rax, rax);\n-      __ jcc(Assembler::notEqual, L);\n-      __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n-      __ bind(L);\n-    }\n+  \/\/ make sure exception is set\n+  {\n+    Label L;\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::notEqual, L);\n+    __ stop(\"StubRoutines::forward exception: no pending exception (2)\");\n+    __ bind(L);\n+  }\n@@ -578,9 +546,6 @@\n-    \/\/ continue at exception handler (return address removed)\n-    \/\/ rax: exception\n-    \/\/ rbx: exception handler\n-    \/\/ rdx: throwing pc\n-    __ verify_oop(rax);\n-    __ jmp(rbx);\n-\n-    return start;\n-  }\n+  \/\/ continue at exception handler (return address removed)\n+  \/\/ rax: exception\n+  \/\/ rbx: exception handler\n+  \/\/ rdx: throwing pc\n+  __ verify_oop(rax);\n+  __ jmp(rbx);\n@@ -588,10 +553,2 @@\n-  \/\/ Support for intptr_t OrderAccess::fence()\n-  \/\/\n-  \/\/ Arguments :\n-  \/\/\n-  \/\/ Result:\n-  address generate_orderaccess_fence() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"orderaccess_fence\");\n-    address start = __ pc();\n-    __ membar(Assembler::StoreLoad);\n-    __ ret(0);\n+  return start;\n+}\n@@ -599,2 +556,8 @@\n-    return start;\n-  }\n+\/\/ Support for intptr_t OrderAccess::fence()\n+\/\/\n+\/\/ Arguments :\n+\/\/\n+\/\/ Result:\n+address StubGenerator::generate_orderaccess_fence() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"orderaccess_fence\");\n+  address start = __ pc();\n@@ -602,0 +565,2 @@\n+  __ membar(Assembler::StoreLoad);\n+  __ ret(0);\n@@ -603,11 +568,2 @@\n-  \/\/ Support for intptr_t get_previous_sp()\n-  \/\/\n-  \/\/ This routine is used to find the previous stack pointer for the\n-  \/\/ caller.\n-  address generate_get_previous_sp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"get_previous_sp\");\n-    address start = __ pc();\n-\n-    __ movptr(rax, rsp);\n-    __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n-    __ ret(0);\n+  return start;\n+}\n@@ -615,34 +571,7 @@\n-    return start;\n-  }\n-  \/\/----------------------------------------------------------------------------------------------------\n-  \/\/ Support for void verify_mxcsr()\n-  \/\/\n-  \/\/ This routine is used with -Xcheck:jni to verify that native\n-  \/\/ JNI code does not return to Java code without restoring the\n-  \/\/ MXCSR register to our expected state.\n-\n-  address generate_verify_mxcsr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n-    address start = __ pc();\n-\n-    const Address mxcsr_save(rsp, 0);\n-\n-    if (CheckJNICalls) {\n-      Label ok_ret;\n-      ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n-      __ push(rax);\n-      __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n-      __ stmxcsr(mxcsr_save);\n-      __ movl(rax, mxcsr_save);\n-      __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n-      __ cmp32(rax, mxcsr_std, rscratch1);\n-      __ jcc(Assembler::equal, ok_ret);\n-\n-      __ warn(\"MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall\");\n-\n-      __ ldmxcsr(mxcsr_std, rscratch1);\n-\n-      __ bind(ok_ret);\n-      __ addptr(rsp, wordSize);\n-      __ pop(rax);\n-    }\n+\/\/ Support for intptr_t get_previous_sp()\n+\/\/\n+\/\/ This routine is used to find the previous stack pointer for the\n+\/\/ caller.\n+address StubGenerator::generate_get_previous_sp() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"get_previous_sp\");\n+  address start = __ pc();\n@@ -651,1 +580,3 @@\n-    __ ret(0);\n+  __ movptr(rax, rsp);\n+  __ addptr(rax, 8); \/\/ return address is at the top of the stack.\n+  __ ret(0);\n@@ -653,2 +584,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -656,3 +587,6 @@\n-  address generate_f2i_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"f2i_fixup\");\n-    Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Support for void verify_mxcsr()\n+\/\/\n+\/\/ This routine is used with -Xcheck:jni to verify that native\n+\/\/ JNI code does not return to Java code without restoring the\n+\/\/ MXCSR register to our expected state.\n@@ -660,1 +594,3 @@\n-    address start = __ pc();\n+address StubGenerator::generate_verify_mxcsr() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"verify_mxcsr\");\n+  address start = __ pc();\n@@ -662,1 +598,1 @@\n-    Label L;\n+  const Address mxcsr_save(rsp, 0);\n@@ -664,0 +600,3 @@\n+  if (CheckJNICalls) {\n+    Label ok_ret;\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n@@ -665,15 +604,6 @@\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-\n-    __ movl(rax, 0x7f800000);\n-    __ xorl(c_rarg3, c_rarg3);\n-    __ movl(c_rarg2, inout);\n-    __ movl(c_rarg1, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n-    __ jcc(Assembler::negative, L);\n-    __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jint : max_jint\n-    __ movl(c_rarg3, 0x80000000);\n-    __ movl(rax, 0x7fffffff);\n-    __ cmovl(Assembler::positive, c_rarg3, rax);\n+    __ subptr(rsp, wordSize);      \/\/ allocate a temp location\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, 0xFFC0); \/\/ Mask out any pending exceptions (only check control and mask bits)\n+    __ cmp32(rax, mxcsr_std, rscratch1);\n+    __ jcc(Assembler::equal, ok_ret);\n@@ -681,2 +611,1 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg3);\n+    __ warn(\"MXCSR changed by native JNI code, use -XX:+RestoreMXCSROnJNICall\");\n@@ -684,3 +613,4 @@\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n+    __ ldmxcsr(mxcsr_std, rscratch1);\n+\n+    __ bind(ok_ret);\n+    __ addptr(rsp, wordSize);\n@@ -688,0 +618,1 @@\n+  }\n@@ -689,1 +620,1 @@\n-    __ ret(0);\n+  __ ret(0);\n@@ -691,2 +622,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -694,4 +625,3 @@\n-  address generate_f2l_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"f2l_fixup\");\n-    Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n-    address start = __ pc();\n+address StubGenerator::generate_f2i_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"f2i_fixup\");\n+  Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n@@ -699,1 +629,1 @@\n-    Label L;\n+  address start = __ pc();\n@@ -701,16 +631,1 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-\n-    __ movl(rax, 0x7f800000);\n-    __ xorl(c_rarg3, c_rarg3);\n-    __ movl(c_rarg2, inout);\n-    __ movl(c_rarg1, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n-    __ jcc(Assembler::negative, L);\n-    __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jlong : max_jlong\n-    __ mov64(c_rarg3, 0x8000000000000000);\n-    __ mov64(rax, 0x7fffffffffffffff);\n-    __ cmov(Assembler::positive, c_rarg3, rax);\n+  Label L;\n@@ -718,2 +633,4 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg3);\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n@@ -721,4 +638,11 @@\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  __ movl(rax, 0x7f800000);\n+  __ xorl(c_rarg3, c_rarg3);\n+  __ movl(c_rarg2, inout);\n+  __ movl(c_rarg1, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n+  __ jcc(Assembler::negative, L);\n+  __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jint : max_jint\n+  __ movl(c_rarg3, 0x80000000);\n+  __ movl(rax, 0x7fffffff);\n+  __ cmovl(Assembler::positive, c_rarg3, rax);\n@@ -726,1 +650,2 @@\n-    __ ret(0);\n+  __ bind(L);\n+  __ movptr(inout, c_rarg3);\n@@ -728,2 +653,4 @@\n-    return start;\n-  }\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n@@ -731,3 +658,1 @@\n-  address generate_d2i_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2i_fixup\");\n-    Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+  __ ret(0);\n@@ -735,1 +660,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -737,1 +663,4 @@\n-    Label L;\n+address StubGenerator::generate_f2l_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"f2l_fixup\");\n+  Address inout(rsp, 5 * wordSize); \/\/ return address + 4 saves\n+  address start = __ pc();\n@@ -739,24 +668,1 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-    __ push(c_rarg0);\n-\n-    __ movl(rax, 0x7ff00000);\n-    __ movq(c_rarg2, inout);\n-    __ movl(c_rarg3, c_rarg2);\n-    __ mov(c_rarg1, c_rarg2);\n-    __ mov(c_rarg0, c_rarg2);\n-    __ negl(c_rarg3);\n-    __ shrptr(c_rarg1, 0x20);\n-    __ orl(c_rarg3, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ xorl(c_rarg2, c_rarg2);\n-    __ shrl(c_rarg3, 0x1f);\n-    __ orl(c_rarg1, c_rarg3);\n-    __ cmpl(rax, c_rarg1);\n-    __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n-    __ testptr(c_rarg0, c_rarg0); \/\/ signed ? min_jint : max_jint\n-    __ movl(c_rarg2, 0x80000000);\n-    __ movl(rax, 0x7fffffff);\n-    __ cmov(Assembler::positive, c_rarg2, rax);\n+  Label L;\n@@ -764,2 +670,4 @@\n-    __ bind(L);\n-    __ movptr(inout, c_rarg2);\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n@@ -767,5 +675,11 @@\n-    __ pop(c_rarg0);\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  __ movl(rax, 0x7f800000);\n+  __ xorl(c_rarg3, c_rarg3);\n+  __ movl(c_rarg2, inout);\n+  __ movl(c_rarg1, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ cmpl(rax, c_rarg1); \/\/ NaN? -> 0\n+  __ jcc(Assembler::negative, L);\n+  __ testl(c_rarg2, c_rarg2); \/\/ signed ? min_jlong : max_jlong\n+  __ mov64(c_rarg3, 0x8000000000000000);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ cmov(Assembler::positive, c_rarg3, rax);\n@@ -773,1 +687,2 @@\n-    __ ret(0);\n+  __ bind(L);\n+  __ movptr(inout, c_rarg3);\n@@ -775,2 +690,4 @@\n-    return start;\n-  }\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n@@ -778,3 +695,1 @@\n-  address generate_d2l_fixup() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"d2l_fixup\");\n-    Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+  __ ret(0);\n@@ -782,1 +697,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -784,1 +700,43 @@\n-    Label L;\n+address StubGenerator::generate_d2i_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"d2i_fixup\");\n+  Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+\n+  address start = __ pc();\n+\n+  Label L;\n+\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n+  __ push(c_rarg0);\n+\n+  __ movl(rax, 0x7ff00000);\n+  __ movq(c_rarg2, inout);\n+  __ movl(c_rarg3, c_rarg2);\n+  __ mov(c_rarg1, c_rarg2);\n+  __ mov(c_rarg0, c_rarg2);\n+  __ negl(c_rarg3);\n+  __ shrptr(c_rarg1, 0x20);\n+  __ orl(c_rarg3, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ xorl(c_rarg2, c_rarg2);\n+  __ shrl(c_rarg3, 0x1f);\n+  __ orl(c_rarg1, c_rarg3);\n+  __ cmpl(rax, c_rarg1);\n+  __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n+  __ testptr(c_rarg0, c_rarg0); \/\/ signed ? min_jint : max_jint\n+  __ movl(c_rarg2, 0x80000000);\n+  __ movl(rax, 0x7fffffff);\n+  __ cmov(Assembler::positive, c_rarg2, rax);\n+\n+  __ bind(L);\n+  __ movptr(inout, c_rarg2);\n+\n+  __ pop(c_rarg0);\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n+\n+  __ ret(0);\n@@ -786,24 +744,2 @@\n-    __ push(rax);\n-    __ push(c_rarg3);\n-    __ push(c_rarg2);\n-    __ push(c_rarg1);\n-    __ push(c_rarg0);\n-\n-    __ movl(rax, 0x7ff00000);\n-    __ movq(c_rarg2, inout);\n-    __ movl(c_rarg3, c_rarg2);\n-    __ mov(c_rarg1, c_rarg2);\n-    __ mov(c_rarg0, c_rarg2);\n-    __ negl(c_rarg3);\n-    __ shrptr(c_rarg1, 0x20);\n-    __ orl(c_rarg3, c_rarg2);\n-    __ andl(c_rarg1, 0x7fffffff);\n-    __ xorl(c_rarg2, c_rarg2);\n-    __ shrl(c_rarg3, 0x1f);\n-    __ orl(c_rarg1, c_rarg3);\n-    __ cmpl(rax, c_rarg1);\n-    __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n-    __ testq(c_rarg0, c_rarg0); \/\/ signed ? min_jlong : max_jlong\n-    __ mov64(c_rarg2, 0x8000000000000000);\n-    __ mov64(rax, 0x7fffffffffffffff);\n-    __ cmovq(Assembler::positive, c_rarg2, rax);\n+  return start;\n+}\n@@ -811,2 +747,43 @@\n-    __ bind(L);\n-    __ movq(inout, c_rarg2);\n+address StubGenerator::generate_d2l_fixup() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"d2l_fixup\");\n+  Address inout(rsp, 6 * wordSize); \/\/ return address + 5 saves\n+\n+  address start = __ pc();\n+\n+  Label L;\n+\n+  __ push(rax);\n+  __ push(c_rarg3);\n+  __ push(c_rarg2);\n+  __ push(c_rarg1);\n+  __ push(c_rarg0);\n+\n+  __ movl(rax, 0x7ff00000);\n+  __ movq(c_rarg2, inout);\n+  __ movl(c_rarg3, c_rarg2);\n+  __ mov(c_rarg1, c_rarg2);\n+  __ mov(c_rarg0, c_rarg2);\n+  __ negl(c_rarg3);\n+  __ shrptr(c_rarg1, 0x20);\n+  __ orl(c_rarg3, c_rarg2);\n+  __ andl(c_rarg1, 0x7fffffff);\n+  __ xorl(c_rarg2, c_rarg2);\n+  __ shrl(c_rarg3, 0x1f);\n+  __ orl(c_rarg1, c_rarg3);\n+  __ cmpl(rax, c_rarg1);\n+  __ jcc(Assembler::negative, L); \/\/ NaN -> 0\n+  __ testq(c_rarg0, c_rarg0); \/\/ signed ? min_jlong : max_jlong\n+  __ mov64(c_rarg2, 0x8000000000000000);\n+  __ mov64(rax, 0x7fffffffffffffff);\n+  __ cmovq(Assembler::positive, c_rarg2, rax);\n+\n+  __ bind(L);\n+  __ movq(inout, c_rarg2);\n+\n+  __ pop(c_rarg0);\n+  __ pop(c_rarg1);\n+  __ pop(c_rarg2);\n+  __ pop(c_rarg3);\n+  __ pop(rax);\n+\n+  __ ret(0);\n@@ -814,5 +791,2 @@\n-    __ pop(c_rarg0);\n-    __ pop(c_rarg1);\n-    __ pop(c_rarg2);\n-    __ pop(c_rarg3);\n-    __ pop(rax);\n+  return start;\n+}\n@@ -820,1 +794,4 @@\n-    __ ret(0);\n+address StubGenerator::generate_count_leading_zeros_lut(const char *stub_name) {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -822,2 +799,8 @@\n-    return start;\n-  }\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0101010102020304, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n@@ -825,14 +808,2 @@\n-  address generate_count_leading_zeros_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0101010102020304, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -840,14 +811,4 @@\n-  address generate_popcount_avx_lut(const char *stub_name) {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    __ emit_data64(0x0302020102010100, relocInfo::none);\n-    __ emit_data64(0x0403030203020201, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_popcount_avx_lut(const char *stub_name) {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -855,14 +816,8 @@\n-  address generate_iota_indices(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0706050403020100, relocInfo::none);\n-    __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n-    __ emit_data64(0x1716151413121110, relocInfo::none);\n-    __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n-    __ emit_data64(0x2726252423222120, relocInfo::none);\n-    __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n-    __ emit_data64(0x3736353433323130, relocInfo::none);\n-    __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n-    return start;\n-  }\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n+  __ emit_data64(0x0302020102010100, relocInfo::none);\n+  __ emit_data64(0x0403030203020201, relocInfo::none);\n@@ -870,14 +825,2 @@\n-  address generate_vector_reverse_bit_lut(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n-    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -885,14 +828,4 @@\n-  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_iota_indices(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -900,14 +833,8 @@\n-  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n-    return start;\n-  }\n+  __ emit_data64(0x0706050403020100, relocInfo::none);\n+  __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n+  __ emit_data64(0x1716151413121110, relocInfo::none);\n+  __ emit_data64(0x1F1E1D1C1B1A1918, relocInfo::none);\n+  __ emit_data64(0x2726252423222120, relocInfo::none);\n+  __ emit_data64(0x2F2E2D2C2B2A2928, relocInfo::none);\n+  __ emit_data64(0x3736353433323130, relocInfo::none);\n+  __ emit_data64(0x3F3E3D3C3B3A3938, relocInfo::none);\n@@ -915,14 +842,2 @@\n-  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    __ emit_data64(0x0607040502030001, relocInfo::none);\n-    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -930,10 +845,4 @@\n-  address generate_vector_byte_shuffle_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-    __ emit_data64(0x7070707070707070, relocInfo::none);\n-    __ emit_data64(0x7070707070707070, relocInfo::none);\n-    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n-    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_vector_reverse_bit_lut(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -941,4 +850,8 @@\n-  address generate_fp_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+  __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+  __ emit_data64(0x0F070B030D050901, relocInfo::none);\n@@ -946,2 +859,2 @@\n-    __ emit_data64( mask, relocInfo::none );\n-    __ emit_data64( mask, relocInfo::none );\n+  return start;\n+}\n@@ -949,2 +862,4 @@\n-    return start;\n-  }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -952,16 +867,8 @@\n-  address generate_vector_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-\n-    return start;\n-  }\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n@@ -969,4 +876,2 @@\n-  address generate_vector_byte_perm_mask(const char *stub_name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n+  return start;\n+}\n@@ -974,8 +879,4 @@\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\n-    __ emit_data64(0x0000000000000003, relocInfo::none);\n-    __ emit_data64(0x0000000000000005, relocInfo::none);\n-    __ emit_data64(0x0000000000000007, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000002, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000006, relocInfo::none);\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -983,2 +884,8 @@\n-    return start;\n-  }\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n@@ -986,16 +893,2 @@\n-  address generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-    __ emit_data64(mask, relocInfo::none);\n-\n-    return start;\n-  }\n+  return start;\n+}\n@@ -1003,30 +896,4 @@\n-  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n-                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n-                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n-                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n-                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n-    address start = __ pc();\n-\n-    assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n-    __ emit_data(val0, relocInfo::none, 0);\n-    __ emit_data(val1, relocInfo::none, 0);\n-    __ emit_data(val2, relocInfo::none, 0);\n-    __ emit_data(val3, relocInfo::none, 0);\n-    if (len >= Assembler::AVX_256bit) {\n-      __ emit_data(val4, relocInfo::none, 0);\n-      __ emit_data(val5, relocInfo::none, 0);\n-      __ emit_data(val6, relocInfo::none, 0);\n-      __ emit_data(val7, relocInfo::none, 0);\n-      if (len >= Assembler::AVX_512bit) {\n-        __ emit_data(val8, relocInfo::none, 0);\n-        __ emit_data(val9, relocInfo::none, 0);\n-        __ emit_data(val10, relocInfo::none, 0);\n-        __ emit_data(val11, relocInfo::none, 0);\n-        __ emit_data(val12, relocInfo::none, 0);\n-        __ emit_data(val13, relocInfo::none, 0);\n-        __ emit_data(val14, relocInfo::none, 0);\n-        __ emit_data(val15, relocInfo::none, 0);\n-      }\n-    }\n+address StubGenerator::generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1034,2 +901,8 @@\n-    return start;\n-  }\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+  __ emit_data64(0x0607040502030001, relocInfo::none);\n+  __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n@@ -1037,48 +910,2 @@\n-  \/\/ Non-destructive plausibility checks for oops\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/    all args on stack!\n-  \/\/\n-  \/\/ Stack after saving c_rarg3:\n-  \/\/    [tos + 0]: saved c_rarg3\n-  \/\/    [tos + 1]: saved c_rarg2\n-  \/\/    [tos + 2]: saved r12 (several TemplateTable methods use it)\n-  \/\/    [tos + 3]: saved flags\n-  \/\/    [tos + 4]: return address\n-  \/\/  * [tos + 5]: error message (char*)\n-  \/\/  * [tos + 6]: object to verify (oop)\n-  \/\/  * [tos + 7]: saved rax - saved by caller and bashed\n-  \/\/  * [tos + 8]: saved r10 (rscratch1) - saved by caller\n-  \/\/  * = popped on exit\n-  address generate_verify_oop() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n-    address start = __ pc();\n-\n-    Label exit, error;\n-\n-    __ pushf();\n-    __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()), rscratch1);\n-\n-    __ push(r12);\n-\n-    \/\/ save c_rarg2 and c_rarg3\n-    __ push(c_rarg2);\n-    __ push(c_rarg3);\n-\n-    enum {\n-           \/\/ After previous pushes.\n-           oop_to_verify = 6 * wordSize,\n-           saved_rax     = 7 * wordSize,\n-           saved_r10     = 8 * wordSize,\n-\n-           \/\/ Before the call to MacroAssembler::debug(), see below.\n-           return_addr   = 16 * wordSize,\n-           error_msg     = 17 * wordSize\n-    };\n-\n-    \/\/ get object\n-    __ movptr(rax, Address(rsp, oop_to_verify));\n-\n-    \/\/ make sure object is 'reasonable'\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, exit); \/\/ if obj is NULL it is OK\n+  return start;\n+}\n@@ -1086,7 +913,4 @@\n-#if INCLUDE_ZGC\n-    if (UseZGC) {\n-      \/\/ Check if metadata bits indicate a bad oop\n-      __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n-      __ jcc(Assembler::notZero, error);\n-    }\n-#endif\n+address StubGenerator::generate_vector_byte_shuffle_mask(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1094,7 +918,4 @@\n-    \/\/ Check if the oop is in the right area of memory\n-    __ movptr(c_rarg2, rax);\n-    __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n-    __ andptr(c_rarg2, c_rarg3);\n-    __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n-    __ cmpptr(c_rarg2, c_rarg3);\n-    __ jcc(Assembler::notZero, error);\n+  __ emit_data64(0x7070707070707070, relocInfo::none);\n+  __ emit_data64(0x7070707070707070, relocInfo::none);\n+  __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+  __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n@@ -1102,50 +923,2 @@\n-    \/\/ make sure klass is 'reasonable', which is not zero.\n-    __ load_klass(rax, rax, rscratch1);  \/\/ get klass\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, error); \/\/ if klass is NULL it is broken\n-\n-    \/\/ return if everything seems ok\n-    __ bind(exit);\n-    __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n-    __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n-    __ pop(c_rarg3);                             \/\/ restore c_rarg3\n-    __ pop(c_rarg2);                             \/\/ restore c_rarg2\n-    __ pop(r12);                                 \/\/ restore r12\n-    __ popf();                                   \/\/ restore flags\n-    __ ret(4 * wordSize);                        \/\/ pop caller saved stuff\n-\n-    \/\/ handle errors\n-    __ bind(error);\n-    __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n-    __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n-    __ pop(c_rarg3);                             \/\/ get saved c_rarg3 back\n-    __ pop(c_rarg2);                             \/\/ get saved c_rarg2 back\n-    __ pop(r12);                                 \/\/ get saved r12 back\n-    __ popf();                                   \/\/ get saved flags off stack --\n-                                                 \/\/ will be ignored\n-\n-    __ pusha();                                  \/\/ push registers\n-                                                 \/\/ (rip is already\n-                                                 \/\/ already pushed)\n-    \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n-    \/\/ We've popped the registers we'd saved (c_rarg3, c_rarg2 and flags), and\n-    \/\/ pushed all the registers, so now the stack looks like:\n-    \/\/     [tos +  0] 16 saved registers\n-    \/\/     [tos + 16] return address\n-    \/\/   * [tos + 17] error message (char*)\n-    \/\/   * [tos + 18] object to verify (oop)\n-    \/\/   * [tos + 19] saved rax - saved by caller and bashed\n-    \/\/   * [tos + 20] saved r10 (rscratch1) - saved by caller\n-    \/\/   * = popped on exit\n-\n-    __ movptr(c_rarg0, Address(rsp, error_msg));    \/\/ pass address of error message\n-    __ movptr(c_rarg1, Address(rsp, return_addr));  \/\/ pass return address\n-    __ movq(c_rarg2, rsp);                          \/\/ pass address of regs on stack\n-    __ mov(r12, rsp);                               \/\/ remember rsp\n-    __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-    __ andptr(rsp, -16);                            \/\/ align stack as required by ABI\n-    BLOCK_COMMENT(\"call MacroAssembler::debug\");\n-    __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));\n-    __ hlt();\n-    return start;\n-  }\n+  return start;\n+}\n@@ -1153,19 +926,4 @@\n-  \/\/\n-  \/\/ Verify that a register contains clean 32-bits positive value\n-  \/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    Rint  -  32-bits value\n-  \/\/    Rtmp  -  scratch\n-  \/\/\n-  void assert_clean_int(Register Rint, Register Rtmp) {\n-#ifdef ASSERT\n-    Label L;\n-    assert_different_registers(Rtmp, Rint);\n-    __ movslq(Rtmp, Rint);\n-    __ cmpq(Rtmp, Rint);\n-    __ jcc(Assembler::equal, L);\n-    __ stop(\"high 32-bits of int value are not 0\");\n-    __ bind(L);\n-#endif\n-  }\n+address StubGenerator::generate_fp_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1173,36 +931,2 @@\n-  \/\/  Generate overlap test for array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/     c_rarg0 - from\n-  \/\/     c_rarg1 - to\n-  \/\/     c_rarg2 - element count\n-  \/\/\n-  \/\/  Output:\n-  \/\/     rax   - &from[element count - 1]\n-  \/\/\n-  void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {\n-    assert(no_overlap_target != NULL, \"must be generated\");\n-    array_overlap_test(no_overlap_target, NULL, sf);\n-  }\n-  void array_overlap_test(Label& L_no_overlap, Address::ScaleFactor sf) {\n-    array_overlap_test(NULL, &L_no_overlap, sf);\n-  }\n-  void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {\n-    const Register from     = c_rarg0;\n-    const Register to       = c_rarg1;\n-    const Register count    = c_rarg2;\n-    const Register end_from = rax;\n-\n-    __ cmpptr(to, from);\n-    __ lea(end_from, Address(from, count, sf, 0));\n-    if (NOLp == NULL) {\n-      ExternalAddress no_overlap(no_overlap_target);\n-      __ jump_cc(Assembler::belowEqual, no_overlap);\n-      __ cmpptr(to, end_from);\n-      __ jump_cc(Assembler::aboveEqual, no_overlap);\n-    } else {\n-      __ jcc(Assembler::belowEqual, (*NOLp));\n-      __ cmpptr(to, end_from);\n-      __ jcc(Assembler::aboveEqual, (*NOLp));\n-    }\n-  }\n+  __ emit_data64( mask, relocInfo::none );\n+  __ emit_data64( mask, relocInfo::none );\n@@ -1210,12 +934,2 @@\n-  \/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n-  \/\/\n-  \/\/ Outputs:\n-  \/\/    rdi - rcx\n-  \/\/    rsi - rdx\n-  \/\/    rdx - r8\n-  \/\/    rcx - r9\n-  \/\/\n-  \/\/ Registers r9 and r10 are used to save rdi and rsi on Windows, which latter\n-  \/\/ are non-volatile.  r9 and r10 should not be used by the caller.\n-  \/\/\n-  DEBUG_ONLY(bool regs_in_thread;)\n+  return start;\n+}\n@@ -1223,22 +937,4 @@\n-  void setup_arg_regs(int nargs = 3) {\n-    const Register saved_rdi = r9;\n-    const Register saved_rsi = r10;\n-    assert(nargs == 3 || nargs == 4, \"else fix\");\n-#ifdef _WIN64\n-    assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n-           \"unexpected argument registers\");\n-    if (nargs >= 4)\n-      __ mov(rax, r9);  \/\/ r9 is also saved_rdi\n-    __ movptr(saved_rdi, rdi);\n-    __ movptr(saved_rsi, rsi);\n-    __ mov(rdi, rcx); \/\/ c_rarg0\n-    __ mov(rsi, rdx); \/\/ c_rarg1\n-    __ mov(rdx, r8);  \/\/ c_rarg2\n-    if (nargs >= 4)\n-      __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n-#else\n-    assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n-           \"unexpected argument registers\");\n-#endif\n-    DEBUG_ONLY(regs_in_thread = false;)\n-  }\n+address StubGenerator::generate_vector_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1246,9 +942,8 @@\n-  void restore_arg_regs() {\n-    assert(!regs_in_thread, \"wrong call to restore_arg_regs\");\n-    const Register saved_rdi = r9;\n-    const Register saved_rsi = r10;\n-#ifdef _WIN64\n-    __ movptr(rdi, saved_rdi);\n-    __ movptr(rsi, saved_rsi);\n-#endif\n-  }\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n@@ -1256,21 +951,2 @@\n-  \/\/ This is used in places where r10 is a scratch register, and can\n-  \/\/ be adapted if r9 is needed also.\n-  void setup_arg_regs_using_thread() {\n-    const Register saved_r15 = r9;\n-#ifdef _WIN64\n-    __ mov(saved_r15, r15);  \/\/ r15 is callee saved and needs to be restored\n-    __ get_thread(r15_thread);\n-    assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n-           \"unexpected argument registers\");\n-    __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);\n-    __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);\n-\n-    __ mov(rdi, rcx); \/\/ c_rarg0\n-    __ mov(rsi, rdx); \/\/ c_rarg1\n-    __ mov(rdx, r8);  \/\/ c_rarg2\n-#else\n-    assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n-           \"unexpected argument registers\");\n-#endif\n-    DEBUG_ONLY(regs_in_thread = true;)\n-  }\n+  return start;\n+}\n@@ -1278,10 +954,4 @@\n-  void restore_arg_regs_using_thread() {\n-    assert(regs_in_thread, \"wrong call to restore_arg_regs\");\n-    const Register saved_r15 = r9;\n-#ifdef _WIN64\n-    __ get_thread(r15_thread);\n-    __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));\n-    __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));\n-    __ mov(r15, saved_r15);  \/\/ r15 is callee saved and needs to be restored\n-#endif\n-  }\n+address StubGenerator::generate_vector_byte_perm_mask(const char *stub_name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1289,34 +959,8 @@\n-  \/\/ Copy big chunks forward\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   end_from     - source arrays end address\n-  \/\/   end_to       - destination array end address\n-  \/\/   qword_count  - 64-bits element count, negative\n-  \/\/   to           - scratch\n-  \/\/   L_copy_bytes - entry label\n-  \/\/   L_copy_8_bytes  - exit  label\n-  \/\/\n-  void copy_bytes_forward(Register end_from, Register end_to,\n-                             Register qword_count, Register to,\n-                             Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-    DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-    Label L_loop;\n-    __ align(OptoLoopAlignment);\n-    if (UseUnalignedLoadStores) {\n-      Label L_end;\n-      __ BIND(L_loop);\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-        __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n-        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n-        __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n-        __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n-      }\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\n+  __ emit_data64(0x0000000000000003, relocInfo::none);\n+  __ emit_data64(0x0000000000000005, relocInfo::none);\n+  __ emit_data64(0x0000000000000007, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000002, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000006, relocInfo::none);\n@@ -1324,497 +968,2 @@\n-      __ BIND(L_copy_bytes);\n-      __ addptr(qword_count, 8);\n-      __ jcc(Assembler::lessEqual, L_loop);\n-      __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n-      __ jccb(Assembler::greater, L_end);\n-      \/\/ Copy trailing 32 bytes\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-        __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-      } else {\n-        __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n-        __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));\n-        __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);\n-      }\n-      __ addptr(qword_count, 4);\n-      __ BIND(L_end);\n-    } else {\n-      \/\/ Copy 32-bytes per iteration\n-      __ BIND(L_loop);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, -24));\n-      __ movq(Address(end_to, qword_count, Address::times_8, -24), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, -16));\n-      __ movq(Address(end_to, qword_count, Address::times_8, -16), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);\n-      __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));\n-      __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);\n-\n-      __ BIND(L_copy_bytes);\n-      __ addptr(qword_count, 4);\n-      __ jcc(Assembler::lessEqual, L_loop);\n-    }\n-    __ subptr(qword_count, 4);\n-    __ jcc(Assembler::less, L_copy_8_bytes); \/\/ Copy trailing qwords\n-  }\n-\n-  \/\/ Copy big chunks backward\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   from         - source arrays address\n-  \/\/   dest         - destination array address\n-  \/\/   qword_count  - 64-bits element count\n-  \/\/   to           - scratch\n-  \/\/   L_copy_bytes - entry label\n-  \/\/   L_copy_8_bytes  - exit  label\n-  \/\/\n-  void copy_bytes_backward(Register from, Register dest,\n-                              Register qword_count, Register to,\n-                              Label& L_copy_bytes, Label& L_copy_8_bytes) {\n-    DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n-    Label L_loop;\n-    __ align(OptoLoopAlignment);\n-    if (UseUnalignedLoadStores) {\n-      Label L_end;\n-      __ BIND(L_loop);\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n-        __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-      } else {\n-        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n-        __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n-        __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n-        __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n-        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n-      }\n-\n-      __ BIND(L_copy_bytes);\n-      __ subptr(qword_count, 8);\n-      __ jcc(Assembler::greaterEqual, L_loop);\n-\n-      __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n-      __ jccb(Assembler::less, L_end);\n-      \/\/ Copy trailing 32 bytes\n-      if (UseAVX >= 2) {\n-        __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));\n-        __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);\n-      } else {\n-        __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));\n-        __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);\n-        __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n-        __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n-      }\n-      __ subptr(qword_count, 4);\n-      __ BIND(L_end);\n-    } else {\n-      \/\/ Copy 32-bytes per iteration\n-      __ BIND(L_loop);\n-      __ movq(to, Address(from, qword_count, Address::times_8, 24));\n-      __ movq(Address(dest, qword_count, Address::times_8, 24), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8, 16));\n-      __ movq(Address(dest, qword_count, Address::times_8, 16), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8,  8));\n-      __ movq(Address(dest, qword_count, Address::times_8,  8), to);\n-      __ movq(to, Address(from, qword_count, Address::times_8,  0));\n-      __ movq(Address(dest, qword_count, Address::times_8,  0), to);\n-\n-      __ BIND(L_copy_bytes);\n-      __ subptr(qword_count, 4);\n-      __ jcc(Assembler::greaterEqual, L_loop);\n-    }\n-    __ addptr(qword_count, 4);\n-    __ jcc(Assembler::greater, L_copy_8_bytes); \/\/ Copy trailing qwords\n-  }\n-\n-#ifndef PRODUCT\n-    int& get_profile_ctr(int shift) {\n-      if ( 0 == shift)\n-        return SharedRuntime::_jbyte_array_copy_ctr;\n-      else if(1 == shift)\n-        return SharedRuntime::_jshort_array_copy_ctr;\n-      else if(2 == shift)\n-        return SharedRuntime::_jint_array_copy_ctr;\n-      else\n-        return SharedRuntime::_jlong_array_copy_ctr;\n-    }\n-#endif\n-\n-  void setup_argument_regs(BasicType type) {\n-    if (type == T_BYTE || type == T_SHORT) {\n-      setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                        \/\/ r9 and r10 may be used to save non-volatile registers\n-    } else {\n-      setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                     \/\/ r9 is used to save r15_thread\n-    }\n-  }\n-\n-  void restore_argument_regs(BasicType type) {\n-    if (type == T_BYTE || type == T_SHORT) {\n-      restore_arg_regs();\n-    } else {\n-      restore_arg_regs_using_thread();\n-    }\n-  }\n-\n-#if COMPILER2_OR_JVMCI\n-  \/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n-  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n-  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n-  \/\/   default configuration.\n-  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n-  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n-  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n-  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n-  \/\/   copy performs better.\n-  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n-  \/\/   64 byte vector registers (ZMMs).\n-\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n-  \/\/\n-\n-  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             bool aligned, bool is_oop, bool dest_uninitialized) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-    int avx3threshold = VM_Version::avx3_threshold();\n-    bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-    Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register temp1       = r8;\n-    const Register temp2       = r11;\n-    const Register temp3       = rax;\n-    const Register temp4       = rcx;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n-\n-    setup_argument_regs(type);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    {\n-      \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n-      int loop_size[]        = { 192,     96,       48,      24};\n-      int threshold[]        = { 4096,    2048,     1024,    512};\n-\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-\n-      \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n-      \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n-      __ mov64(temp4, 0);\n-      __ movq(temp1, count);\n-\n-      \/\/ Zero length check.\n-      __ BIND(L_tail);\n-      __ cmpq(temp1, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      \/\/ Special cases using 32 byte [masked] vector copy operations.\n-      __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                      temp4, temp3, use64byteVector, L_entry, L_exit);\n-\n-      \/\/ PRE-MAIN-POST loop for aligned copy.\n-      __ BIND(L_entry);\n-\n-      if (avx3threshold != 0) {\n-        __ cmpq(count, threshold[shift]);\n-        if (MaxVectorSize == 64) {\n-          \/\/ Copy using 64 byte vectors.\n-          __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-        } else {\n-          assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n-          \/\/ REP MOVS offer a faster copy path.\n-          __ jcc(Assembler::greaterEqual, L_repmovs);\n-        }\n-      }\n-\n-      if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-        \/\/ Partial copy to make dst address 32 byte aligned.\n-        __ movq(temp2, to);\n-        __ andq(temp2, 31);\n-        __ jcc(Assembler::equal, L_main_pre_loop);\n-\n-        __ negptr(temp2);\n-        __ addq(temp2, 32);\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ movq(temp3, temp2);\n-        __ copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail);\n-\n-        __ BIND(L_main_pre_loop);\n-        __ subq(temp1, loop_size[shift]);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-        __ align32();\n-        __ BIND(L_main_loop);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n-           __ addptr(temp4, loop_size[shift]);\n-           __ subq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop);\n-\n-        __ addq(temp1, loop_size[shift]);\n-\n-        \/\/ Tail loop.\n-        __ jmp(L_tail);\n-\n-        __ BIND(L_repmovs);\n-          __ movq(temp2, temp1);\n-          \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n-          __ movq(temp3, to);\n-          __ movq(to,  from);\n-          __ movq(from, temp3);\n-          \/\/ Save to\/from for restoration post rep_mov.\n-          __ movq(temp1, to);\n-          __ movq(temp3, from);\n-          if(shift < 3) {\n-            __ shrq(temp2, 3-shift);     \/\/ quad word count\n-          }\n-          __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n-          __ rep_mov();\n-          __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n-          if(shift) {\n-            __ shrq(temp2, shift);       \/\/ type specific count.\n-          }\n-          \/\/ Restore original addresses in to\/from.\n-          __ movq(to, temp3);\n-          __ movq(from, temp1);\n-          __ movq(temp4, temp2);\n-          __ movq(temp1, count);\n-          __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n-          __ jmp(L_tail);\n-      }\n-\n-      if (MaxVectorSize > 32) {\n-        __ BIND(L_pre_main_post_64);\n-        \/\/ Partial copy to make dst address 64 byte aligned.\n-        __ movq(temp2, to);\n-        __ andq(temp2, 63);\n-        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n-\n-        __ negptr(temp2);\n-        __ addq(temp2, 64);\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ movq(temp3, temp2);\n-        __ copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n-        __ movq(temp4, temp2);\n-        __ movq(temp1, count);\n-        __ subq(temp1, temp2);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail64);\n-\n-        __ BIND(L_main_pre_loop_64bytes);\n-        __ subq(temp1, loop_size[shift]);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at\n-        \/\/ 64 byte copy granularity.\n-        __ align32();\n-        __ BIND(L_main_loop_64bytes);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n-           __ copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n-           __ addptr(temp4, loop_size[shift]);\n-           __ subq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-        __ addq(temp1, loop_size[shift]);\n-        \/\/ Zero length check.\n-        __ jcc(Assembler::lessEqual, L_exit);\n-\n-        __ BIND(L_tail64);\n-\n-        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-        use64byteVector = true;\n-        __ arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n-                                        temp4, temp3, use64byteVector, L_entry, L_exit);\n-      }\n-      __ BIND(L_exit);\n-    }\n-\n-    address ucme_exit_pc = __ pc();\n-    \/\/ When called from generic_arraycopy r11 contains specific values\n-    \/\/ used during arraycopy epilogue, re-initializing r11.\n-    if (is_oop) {\n-      __ movq(r11, shift == 3 ? count : to);\n-    }\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-    restore_argument_regs(type);\n-    INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/\n-  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n-                                             address nooverlap_target, bool aligned, bool is_oop,\n-                                             bool dest_uninitialized) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    int avx3threshold = VM_Version::avx3_threshold();\n-    bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n-\n-    Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n-    Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register temp1       = r8;\n-    const Register temp2       = rcx;\n-    const Register temp3       = r11;\n-    const Register temp4       = rax;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n-\n-    BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n-\n-    setup_argument_regs(type);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-    {\n-      \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n-      int loop_size[]   = { 192,     96,       48,      24};\n-      int threshold[]   = { 4096,    2048,     1024,    512};\n-\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-\n-      \/\/ temp1 holds remaining count.\n-      __ movq(temp1, count);\n-\n-      \/\/ Zero length check.\n-      __ BIND(L_tail);\n-      __ cmpq(temp1, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      __ mov64(temp2, 0);\n-      __ movq(temp3, temp1);\n-      \/\/ Special cases using 32 byte [masked] vector copy operations.\n-      __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                               temp4, use64byteVector, L_entry, L_exit);\n-\n-      \/\/ PRE-MAIN-POST loop for aligned copy.\n-      __ BIND(L_entry);\n-\n-      if ((MaxVectorSize > 32) && (avx3threshold != 0)) {\n-        __ cmpq(temp1, threshold[shift]);\n-        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n-      }\n-\n-      if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n-        \/\/ Partial copy to make dst address 32 byte aligned.\n-        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-        __ andq(temp2, 31);\n-        __ jcc(Assembler::equal, L_main_pre_loop);\n-\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ subq(temp1, temp2);\n-        __ copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail);\n-\n-        __ BIND(L_main_pre_loop);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n-        __ align32();\n-        __ BIND(L_main_loop);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n-           __ subptr(temp1, loop_size[shift]);\n-           __ cmpq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop);\n-\n-        \/\/ Tail loop.\n-        __ jmp(L_tail);\n-      }\n-\n-      if (MaxVectorSize > 32) {\n-        __ BIND(L_pre_main_post_64);\n-        \/\/ Partial copy to make dst address 64 byte aligned.\n-        __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n-        __ andq(temp2, 63);\n-        __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+  return start;\n+}\n@@ -1822,53 +971,4 @@\n-        if (shift) {\n-          __ shrq(temp2, shift);\n-        }\n-        __ subq(temp1, temp2);\n-        __ copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n-\n-        __ cmpq(temp1, loop_size[shift]);\n-        __ jcc(Assembler::less, L_tail64);\n-\n-        __ BIND(L_main_pre_loop_64bytes);\n-\n-        \/\/ Main loop with aligned copy block size of 192 bytes at\n-        \/\/ 64 byte copy granularity.\n-        __ align32();\n-        __ BIND(L_main_loop_64bytes);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n-           __ copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n-           __ subq(temp1, loop_size[shift]);\n-           __ cmpq(temp1, loop_size[shift]);\n-           __ jcc(Assembler::greater, L_main_loop_64bytes);\n-\n-        \/\/ Zero length check.\n-        __ cmpq(temp1, 0);\n-        __ jcc(Assembler::lessEqual, L_exit);\n-\n-        __ BIND(L_tail64);\n-\n-        \/\/ Tail handling using 64 byte [masked] vector copy operations.\n-        use64byteVector = true;\n-        __ mov64(temp2, 0);\n-        __ movq(temp3, temp1);\n-        __ arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n-                                                 temp4, use64byteVector, L_entry, L_exit);\n-      }\n-      __ BIND(L_exit);\n-    }\n-    address ucme_exit_pc = __ pc();\n-    \/\/ When called from generic_arraycopy r11 contains specific values\n-    \/\/ used during arraycopy epilogue, re-initializing r11.\n-    if(is_oop) {\n-      __ movq(r11, count);\n-    }\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n-    restore_argument_regs(type);\n-    INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-#endif \/\/ COMPILER2_OR_JVMCI\n+address StubGenerator::generate_vector_fp_mask(const char *stub_name, int64_t mask) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n@@ -1876,0 +976,8 @@\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n+  __ emit_data64(mask, relocInfo::none);\n@@ -1877,109 +985,2 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n-  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_byte_copy().\n-  \/\/\n-  address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n-                                                 aligned, false, false);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-    Label L_copy_byte, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register byte_count  = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(byte_count, count);\n-      __ shrptr(count, 3); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count); \/\/ make the count negative\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(byte_count, 4);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 4);\n-      __ addptr(end_to, 4);\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(byte_count, 2);\n-      __ jccb(Assembler::zero, L_copy_byte);\n-      __ movw(rax, Address(end_from, 8));\n-      __ movw(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 2);\n-      __ addptr(end_to, 2);\n-\n-      \/\/ Check for and copy trailing byte\n-    __ BIND(L_copy_byte);\n-      __ testl(byte_count, 1);\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movb(rax, Address(end_from, 8));\n-      __ movb(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  return start;\n+}\n@@ -1987,5 +988,28 @@\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n+address StubGenerator::generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n+                                   int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n+                                   int32_t val4, int32_t val5, int32_t val6, int32_t val7,\n+                                   int32_t val8, int32_t val9, int32_t val10, int32_t val11,\n+                                   int32_t val12, int32_t val13, int32_t val14, int32_t val15) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+  address start = __ pc();\n+\n+  assert(len != Assembler::AVX_NoVec, \"vector len must be specified\");\n+  __ emit_data(val0, relocInfo::none, 0);\n+  __ emit_data(val1, relocInfo::none, 0);\n+  __ emit_data(val2, relocInfo::none, 0);\n+  __ emit_data(val3, relocInfo::none, 0);\n+  if (len >= Assembler::AVX_256bit) {\n+    __ emit_data(val4, relocInfo::none, 0);\n+    __ emit_data(val5, relocInfo::none, 0);\n+    __ emit_data(val6, relocInfo::none, 0);\n+    __ emit_data(val7, relocInfo::none, 0);\n+    if (len >= Assembler::AVX_512bit) {\n+      __ emit_data(val8, relocInfo::none, 0);\n+      __ emit_data(val9, relocInfo::none, 0);\n+      __ emit_data(val10, relocInfo::none, 0);\n+      __ emit_data(val11, relocInfo::none, 0);\n+      __ emit_data(val12, relocInfo::none, 0);\n+      __ emit_data(val13, relocInfo::none, 0);\n+      __ emit_data(val14, relocInfo::none, 0);\n+      __ emit_data(val15, relocInfo::none, 0);\n@@ -1993,1 +1017,2 @@\n-    return start;\n+  return start;\n+}\n@@ -1996,46 +1021,41 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n-  \/\/ we let the hardware handle it.  The one to eight bytes within words,\n-  \/\/ dwords or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n-                                      address* entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n-                                                 nooverlap_target, aligned, false, false);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register byte_count  = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_1);\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n+\/\/ Non-destructive plausibility checks for oops\n+\/\/\n+\/\/ Arguments:\n+\/\/    all args on stack!\n+\/\/\n+\/\/ Stack after saving c_rarg3:\n+\/\/    [tos + 0]: saved c_rarg3\n+\/\/    [tos + 1]: saved c_rarg2\n+\/\/    [tos + 2]: saved r12 (several TemplateTable methods use it)\n+\/\/    [tos + 3]: saved flags\n+\/\/    [tos + 4]: return address\n+\/\/  * [tos + 5]: error message (char*)\n+\/\/  * [tos + 6]: object to verify (oop)\n+\/\/  * [tos + 7]: saved rax - saved by caller and bashed\n+\/\/  * [tos + 8]: saved r10 (rscratch1) - saved by caller\n+\/\/  * = popped on exit\n+address StubGenerator::generate_verify_oop() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"verify_oop\");\n+  address start = __ pc();\n+\n+  Label exit, error;\n+\n+  __ pushf();\n+  __ incrementl(ExternalAddress((address) StubRoutines::verify_oop_count_addr()), rscratch1);\n+\n+  __ push(r12);\n+\n+  \/\/ save c_rarg2 and c_rarg3\n+  __ push(c_rarg2);\n+  __ push(c_rarg3);\n+\n+  enum {\n+    \/\/ After previous pushes.\n+    oop_to_verify = 6 * wordSize,\n+    saved_rax     = 7 * wordSize,\n+    saved_r10     = 8 * wordSize,\n+\n+    \/\/ Before the call to MacroAssembler::debug(), see below.\n+    return_addr   = 16 * wordSize,\n+    error_msg     = 17 * wordSize\n+  };\n@@ -2043,44 +1063,2 @@\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(byte_count, count);\n-      __ shrptr(count, 3);   \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.\n-\n-      \/\/ Check for and copy trailing byte\n-      __ testl(byte_count, 1);\n-      __ jcc(Assembler::zero, L_copy_2_bytes);\n-      __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n-      __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n-      __ decrement(byte_count); \/\/ Adjust for possible trailing word\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(byte_count, 2);\n-      __ jcc(Assembler::zero, L_copy_4_bytes);\n-      __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n-      __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(byte_count, 4);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, qword_count, Address::times_8));\n-      __ movl(Address(to, qword_count, Address::times_8), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  \/\/ get object\n+  __ movptr(rax, Address(rsp, oop_to_verify));\n@@ -2088,12 +1066,3 @@\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  \/\/ make sure object is 'reasonable'\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, exit); \/\/ if obj is NULL it is OK\n@@ -2101,1 +1070,5 @@\n-    return start;\n+#if INCLUDE_ZGC\n+  if (UseZGC) {\n+    \/\/ Check if metadata bits indicate a bad oop\n+    __ testptr(rax, Address(r15_thread, ZThreadLocalData::address_bad_mask_offset()));\n+    __ jcc(Assembler::notZero, error);\n@@ -2103,26 +1076,0 @@\n-\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-  \/\/ let the hardware handle it.  The two or four words within dwords\n-  \/\/ or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_short_copy().\n-  \/\/\n-  address generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n-                                                 aligned, false, false);\n-    }\n@@ -2131,99 +1078,56 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register word_count  = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(word_count, count);\n-      __ shrptr(count, 2); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Original 'dest' is trashed, so we can't use it as a\n-      \/\/ base register for a possible trailing word copy\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(word_count, 2);\n-      __ jccb(Assembler::zero, L_copy_2_bytes);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-\n-      __ addptr(end_from, 4);\n-      __ addptr(end_to, 4);\n-\n-      \/\/ Check for and copy trailing word\n-    __ BIND(L_copy_2_bytes);\n-      __ testl(word_count, 1);\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movw(rax, Address(end_from, 8));\n-      __ movw(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-\n-    return start;\n-  }\n-\n-  address generate_fill(BasicType t, bool aligned, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-\n-    const Register to       = c_rarg0;  \/\/ destination array address\n-    const Register value    = c_rarg1;  \/\/ value\n-    const Register count    = c_rarg2;  \/\/ elements count\n-    __ mov(r11, count);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  \/\/ Check if the oop is in the right area of memory\n+  __ movptr(c_rarg2, rax);\n+  __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_mask());\n+  __ andptr(c_rarg2, c_rarg3);\n+  __ movptr(c_rarg3, (intptr_t) Universe::verify_oop_bits());\n+  __ cmpptr(c_rarg2, c_rarg3);\n+  __ jcc(Assembler::notZero, error);\n+\n+  \/\/ make sure klass is 'reasonable', which is not zero.\n+  __ load_klass(rax, rax, rscratch1);  \/\/ get klass\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, error); \/\/ if klass is NULL it is broken\n+\n+  \/\/ return if everything seems ok\n+  __ bind(exit);\n+  __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n+  __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n+  __ pop(c_rarg3);                             \/\/ restore c_rarg3\n+  __ pop(c_rarg2);                             \/\/ restore c_rarg2\n+  __ pop(r12);                                 \/\/ restore r12\n+  __ popf();                                   \/\/ restore flags\n+  __ ret(4 * wordSize);                        \/\/ pop caller saved stuff\n+\n+  \/\/ handle errors\n+  __ bind(error);\n+  __ movptr(rax, Address(rsp, saved_rax));     \/\/ get saved rax back\n+  __ movptr(rscratch1, Address(rsp, saved_r10)); \/\/ get saved r10 back\n+  __ pop(c_rarg3);                             \/\/ get saved c_rarg3 back\n+  __ pop(c_rarg2);                             \/\/ get saved c_rarg2 back\n+  __ pop(r12);                                 \/\/ get saved r12 back\n+  __ popf();                                   \/\/ get saved flags off stack --\n+                                               \/\/ will be ignored\n+\n+  __ pusha();                                  \/\/ push registers\n+                                               \/\/ (rip is already\n+                                               \/\/ already pushed)\n+  \/\/ debug(char* msg, int64_t pc, int64_t regs[])\n+  \/\/ We've popped the registers we'd saved (c_rarg3, c_rarg2 and flags), and\n+  \/\/ pushed all the registers, so now the stack looks like:\n+  \/\/     [tos +  0] 16 saved registers\n+  \/\/     [tos + 16] return address\n+  \/\/   * [tos + 17] error message (char*)\n+  \/\/   * [tos + 18] object to verify (oop)\n+  \/\/   * [tos + 19] saved rax - saved by caller and bashed\n+  \/\/   * [tos + 20] saved r10 (rscratch1) - saved by caller\n+  \/\/   * = popped on exit\n+\n+  __ movptr(c_rarg0, Address(rsp, error_msg));    \/\/ pass address of error message\n+  __ movptr(c_rarg1, Address(rsp, return_addr));  \/\/ pass return address\n+  __ movq(c_rarg2, rsp);                          \/\/ pass address of regs on stack\n+  __ mov(r12, rsp);                               \/\/ remember rsp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16);                            \/\/ align stack as required by ABI\n+  BLOCK_COMMENT(\"call MacroAssembler::debug\");\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, MacroAssembler::debug64)));\n+  __ hlt();\n@@ -2231,1 +1135,2 @@\n-    __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n+  return start;\n+}\n@@ -2233,27 +1138,30 @@\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n-  \/\/ let the hardware handle it.  The two or four words within dwords\n-  \/\/ or qwords that span cache line boundaries will still be loaded\n-  \/\/ and stored atomically.\n-  \/\/\n-  address generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n-                                       address *entry, const char *name) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n-                                                 nooverlap_target, aligned, false, false);\n-    }\n+\/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n+\/\/\n+\/\/ Outputs:\n+\/\/    rdi - rcx\n+\/\/    rsi - rdx\n+\/\/    rdx - r8\n+\/\/    rcx - r9\n+\/\/\n+\/\/ Registers r9 and r10 are used to save rdi and rsi on Windows, which latter\n+\/\/ are non-volatile.  r9 and r10 should not be used by the caller.\n+\/\/\n+void StubGenerator::setup_arg_regs(int nargs) {\n+  const Register saved_rdi = r9;\n+  const Register saved_rsi = r10;\n+  assert(nargs == 3 || nargs == 4, \"else fix\");\n+#ifdef _WIN64\n+  assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n+         \"unexpected argument registers\");\n+  if (nargs >= 4)\n+    __ mov(rax, r9);  \/\/ r9 is also saved_rdi\n+  __ movptr(saved_rdi, rdi);\n+  __ movptr(saved_rsi, rsi);\n+  __ mov(rdi, rcx); \/\/ c_rarg0\n+  __ mov(rsi, rdx); \/\/ c_rarg1\n+  __ mov(rdx, r8);  \/\/ c_rarg2\n+  if (nargs >= 4)\n+    __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n+#else\n+  assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n+         \"unexpected argument registers\");\n@@ -2262,73 +1170,2 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register word_count  = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_2);\n-    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n-                      \/\/ r9 and r10 may be used to save non-volatile registers\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(word_count, count);\n-      __ shrptr(count, 2); \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-      \/\/ Check for and copy trailing word\n-      __ testl(word_count, 1);\n-      __ jccb(Assembler::zero, L_copy_4_bytes);\n-      __ movw(rax, Address(from, word_count, Address::times_2, -2));\n-      __ movw(Address(to, word_count, Address::times_2, -2), rax);\n-\n-     \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(word_count, 2);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, qword_count, Address::times_8));\n-      __ movl(Address(to, qword_count, Address::times_8), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    restore_arg_regs();\n-    INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  DEBUG_ONLY(_regs_in_thread = false;)\n+}\n@@ -2336,28 +1173,7 @@\n-    return start;\n-  }\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-  \/\/ the hardware handle it.  The two dwords within qwords that span\n-  \/\/ cache line boundaries will still be loaded and stored atomically.\n-  \/\/\n-  \/\/ Side Effects:\n-  \/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n-  \/\/   used by generate_conjoint_int_oop_copy().\n-  \/\/\n-  address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n-                                         const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n-                                                 aligned, is_oop, dest_uninitialized);\n-    }\n+void StubGenerator::restore_arg_regs() {\n+  assert(!_regs_in_thread, \"wrong call to restore_arg_regs\");\n+  const Register saved_rdi = r9;\n+  const Register saved_rsi = r10;\n+#ifdef _WIN64\n+  __ movptr(rdi, saved_rdi);\n+  __ movptr(rsi, saved_rsi);\n@@ -2366,0 +1182,1 @@\n+}\n@@ -2367,108 +1184,18 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register dword_count = rcx;\n-    const Register qword_count = count;\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = to;   \/\/ destination array end address\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_INT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(dword_count, count);\n-      __ shrptr(count, 1); \/\/ count => qword_count\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-      \/\/ Check for and copy trailing dword\n-    __ BIND(L_copy_4_bytes);\n-      __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n-      __ jccb(Assembler::zero, L_exit);\n-      __ movl(rax, Address(end_from, 8));\n-      __ movl(Address(end_to, 8), rax);\n-    }\n-  __ BIND(L_exit);\n-    address ucme_exit_pc = __ pc();\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-      __ jmp(L_copy_4_bytes);\n-    }\n-\n-    return start;\n-  }\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n-  \/\/ the hardware handle it.  The two dwords within qwords that span\n-  \/\/ cache line boundaries will still be loaded and stored atomically.\n-  \/\/\n-  address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n-                                         address *entry, const char *name,\n-                                         bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n-                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n-    }\n+\/\/ This is used in places where r10 is a scratch register, and can\n+\/\/ be adapted if r9 is needed also.\n+void StubGenerator::setup_arg_regs_using_thread() {\n+  const Register saved_r15 = r9;\n+#ifdef _WIN64\n+  __ mov(saved_r15, r15);  \/\/ r15 is callee saved and needs to be restored\n+  __ get_thread(r15_thread);\n+  assert(c_rarg0 == rcx && c_rarg1 == rdx && c_rarg2 == r8 && c_rarg3 == r9,\n+         \"unexpected argument registers\");\n+  __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())), rdi);\n+  __ movptr(Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())), rsi);\n+\n+  __ mov(rdi, rcx); \/\/ c_rarg0\n+  __ mov(rsi, rdx); \/\/ c_rarg1\n+  __ mov(rdx, r8);  \/\/ c_rarg2\n+#else\n+  assert(c_rarg0 == rdi && c_rarg1 == rsi && c_rarg2 == rdx && c_rarg3 == rcx,\n+         \"unexpected argument registers\");\n@@ -2477,86 +1204,2 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register count       = rdx;  \/\/ elements count\n-    const Register dword_count = rcx;\n-    const Register qword_count = count;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-       \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    array_overlap_test(nooverlap_target, Address::times_4);\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                   \/\/ r9 is used to save r15_thread\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_INT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    \/\/ no registers are destroyed by this call\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    assert_clean_int(count, rax); \/\/ Make sure 'count' is clean int.\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ 'from', 'to' and 'count' are now valid\n-      __ movptr(dword_count, count);\n-      __ shrptr(count, 1); \/\/ count => qword_count\n-\n-      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-      \/\/ Check for and copy trailing dword\n-      __ testl(dword_count, 1);\n-      __ jcc(Assembler::zero, L_copy_bytes);\n-      __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n-      __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    }\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-\n-  __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  DEBUG_ONLY(_regs_in_thread = true;)\n+}\n@@ -2564,24 +1207,8 @@\n-    return start;\n-  }\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n- \/\/ Side Effects:\n-  \/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n-  \/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n-  \/\/\n-  address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n-                                          const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n-                                                 aligned, is_oop, dest_uninitialized);\n-    }\n+void StubGenerator::restore_arg_regs_using_thread() {\n+  assert(_regs_in_thread, \"wrong call to restore_arg_regs\");\n+  const Register saved_r15 = r9;\n+#ifdef _WIN64\n+  __ get_thread(r15_thread);\n+  __ movptr(rsi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rsi_offset())));\n+  __ movptr(rdi, Address(r15_thread, in_bytes(JavaThread::windows_saved_rdi_offset())));\n+  __ mov(r15, saved_r15);  \/\/ r15 is callee saved and needs to be restored\n@@ -2590,87 +1217,1 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register qword_count = rdx;  \/\/ elements count\n-    const Register end_from    = from; \/\/ source array end address\n-    const Register end_to      = rcx;  \/\/ destination array end address\n-    const Register saved_count = r11;\n-    \/\/ End pointers are inclusive, and if count is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    \/\/ Save no-overlap entry point for generate_conjoint_long_oop_copy()\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n-                                     \/\/ r9 is used to save r15_thread\n-    \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_LONG;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-      __ negptr(qword_count);\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-      __ increment(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    } else {\n-      restore_arg_regs_using_thread();\n-      INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-      __ xorptr(rax, rax); \/\/ return 0\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-    }\n-\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-\n-    __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n-                            SharedRuntime::_jlong_array_copy_ctr,\n-                   rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n+}\n@@ -2678,39 +1219,5 @@\n-  \/\/ Arguments:\n-  \/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n-  \/\/             ignored\n-  \/\/   is_oop  - true => oop array, so generate store check code\n-  \/\/   name    - stub name string\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source array address\n-  \/\/   c_rarg1   - destination array address\n-  \/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/\n-  address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,\n-                                          address nooverlap_target, address *entry,\n-                                          const char *name, bool dest_uninitialized = false) {\n-#if COMPILER2_OR_JVMCI\n-    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n-       return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n-                                                 nooverlap_target, aligned, is_oop, dest_uninitialized);\n-    }\n-#endif\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Label L_copy_bytes, L_copy_8_bytes, L_exit;\n-    const Register from        = rdi;  \/\/ source array address\n-    const Register to          = rsi;  \/\/ destination array address\n-    const Register qword_count = rdx;  \/\/ elements count\n-    const Register saved_count = rcx;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n-\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-    array_overlap_test(nooverlap_target, Address::times_8);\n+void StubGenerator::setup_argument_regs(BasicType type) {\n+  if (type == T_BYTE || type == T_SHORT) {\n+    setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                      \/\/ r9 and r10 may be used to save non-volatile registers\n+  } else {\n@@ -2720,55 +1227,1 @@\n-    \/\/ 'from', 'to' and 'qword_count' are now valid\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-    if (aligned) {\n-      decorators |= ARRAYCOPY_ALIGNED;\n-    }\n-\n-    BasicType type = is_oop ? T_OBJECT : T_LONG;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      __ jmp(L_copy_bytes);\n-\n-      \/\/ Copy trailing qwords\n-    __ BIND(L_copy_8_bytes);\n-      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-      __ decrement(qword_count);\n-      __ jcc(Assembler::notZero, L_copy_8_bytes);\n-    }\n-    if (is_oop) {\n-      __ jmp(L_exit);\n-    } else {\n-      restore_arg_regs_using_thread();\n-      INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-      __ xorptr(rax, rax); \/\/ return 0\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n-    }\n-    {\n-      \/\/ UnsafeCopyMemory page error: continue after ucm\n-      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n-\n-      \/\/ Copy in multi-bytes chunks\n-      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    }\n-    __ BIND(L_exit);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n-    restore_arg_regs_using_thread();\n-    INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n-                            SharedRuntime::_jlong_array_copy_ctr,\n-                   rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ vzeroupper();\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n+}\n@@ -2778,209 +1231,2 @@\n-  \/\/ Helper for generating a dynamic type check.\n-  \/\/ Smashes no registers.\n-  void generate_type_check(Register sub_klass,\n-                           Register super_check_offset,\n-                           Register super_klass,\n-                           Label& L_success) {\n-    assert_different_registers(sub_klass, super_check_offset, super_klass);\n-\n-    BLOCK_COMMENT(\"type_check:\");\n-\n-    Label L_miss;\n-\n-    __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n-                                     super_check_offset);\n-    __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n-\n-    \/\/ Fall through on failure!\n-    __ BIND(L_miss);\n-  }\n-\n-  \/\/\n-  \/\/  Generate checkcasting array copy stub\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0   - source array address\n-  \/\/    c_rarg1   - destination array address\n-  \/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n-  \/\/    c_rarg3   - size_t ckoff (super_check_offset)\n-  \/\/ not Win64\n-  \/\/    c_rarg4   - oop ckval (super_klass)\n-  \/\/ Win64\n-  \/\/    rsp+40    - oop ckval (super_klass)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax ==  0  -  success\n-  \/\/    rax == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_checkcast_copy(const char *name, address *entry,\n-                                  bool dest_uninitialized = false) {\n-\n-    Label L_load_element, L_store_element, L_do_card_marks, L_done;\n-\n-    \/\/ Input registers (after setup_arg_regs)\n-    const Register from        = rdi;   \/\/ source array address\n-    const Register to          = rsi;   \/\/ destination array address\n-    const Register length      = rdx;   \/\/ elements count\n-    const Register ckoff       = rcx;   \/\/ super_check_offset\n-    const Register ckval       = r8;    \/\/ super_klass\n-\n-    \/\/ Registers used as temps (r13, r14 are save-on-entry)\n-    const Register end_from    = from;  \/\/ source array end address\n-    const Register end_to      = r13;   \/\/ destination array end address\n-    const Register count       = rdx;   \/\/ -(count_remaining)\n-    const Register r14_length  = r14;   \/\/ saved copy of length\n-    \/\/ End pointers are inclusive, and if length is not zero they point\n-    \/\/ to the last unit copied:  end_to[0] := end_from[0]\n-\n-    const Register rax_oop    = rax;    \/\/ actual oop copied\n-    const Register r11_klass  = r11;    \/\/ oop._klass\n-\n-    \/\/---------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the two arrays are subtypes of Object[] but the\n-    \/\/ destination array type is not equal to or a supertype\n-    \/\/ of the source type.  Each element must be separately\n-    \/\/ checked.\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef ASSERT\n-    \/\/ caller guarantees that the arrays really are different\n-    \/\/ otherwise, we would have to make conjoint checks\n-    { Label L;\n-      array_overlap_test(L, TIMES_OOP);\n-      __ stop(\"checkcast_copy within a single array\");\n-      __ bind(L);\n-    }\n-#endif \/\/ASSERT\n-\n-    setup_arg_regs(4); \/\/ from => rdi, to => rsi, length => rdx\n-                       \/\/ ckoff => rcx, ckval => r8\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-#ifdef _WIN64\n-    \/\/ last argument (#4) is on stack on Win64\n-    __ movptr(ckval, Address(rsp, 6 * wordSize));\n-#endif\n-\n-    \/\/ Caller of this entry point must set up the argument registers.\n-    if (entry != NULL) {\n-      *entry = __ pc();\n-      BLOCK_COMMENT(\"Entry:\");\n-    }\n-\n-    \/\/ allocate spill slots for r13, r14\n-    enum {\n-      saved_r13_offset,\n-      saved_r14_offset,\n-      saved_r10_offset,\n-      saved_rbp_offset\n-    };\n-    __ subptr(rsp, saved_rbp_offset * wordSize);\n-    __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-    __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-    __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);\n-\n-#ifdef ASSERT\n-      Label L2;\n-      __ get_thread(r14);\n-      __ cmpptr(r15_thread, r14);\n-      __ jcc(Assembler::equal, L2);\n-      __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n-      __ bind(L2);\n-#endif \/\/ ASSERT\n-\n-    \/\/ check that int operands are properly extended to size_t\n-    assert_clean_int(length, rax);\n-    assert_clean_int(ckoff, rax);\n-\n-#ifdef ASSERT\n-    BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n-    \/\/ The ckoff and ckval must be mutually consistent,\n-    \/\/ even though caller generates both.\n-    { Label L;\n-      int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ cmpl(ckoff, Address(ckval, sco_offset));\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"super_check_offset inconsistent\");\n-      __ bind(L);\n-    }\n-#endif \/\/ASSERT\n-\n-    \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n-    Address end_from_addr(from, length, TIMES_OOP, 0);\n-    Address   end_to_addr(to,   length, TIMES_OOP, 0);\n-    \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n-    Address from_element_addr(end_from, count, TIMES_OOP, 0);\n-    Address   to_element_addr(end_to,   count, TIMES_OOP, 0);\n-\n-    DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n-    if (dest_uninitialized) {\n-      decorators |= IS_DEST_UNINITIALIZED;\n-    }\n-\n-    BasicType type = T_OBJECT;\n-    BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n-\n-    \/\/ Copy from low to high addresses, indexed from the end of each array.\n-    __ lea(end_from, end_from_addr);\n-    __ lea(end_to,   end_to_addr);\n-    __ movptr(r14_length, length);        \/\/ save a copy of the length\n-    assert(length == count, \"\");          \/\/ else fix next line:\n-    __ negptr(count);                     \/\/ negate and test the length\n-    __ jcc(Assembler::notZero, L_load_element);\n-\n-    \/\/ Empty array:  Nothing to do.\n-    __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n-    __ jmp(L_done);\n-\n-    \/\/ ======== begin loop ========\n-    \/\/ (Loop is rotated; its entry is L_load_element.)\n-    \/\/ Loop control:\n-    \/\/   for (count = -count; count != 0; count++)\n-    \/\/ Base pointers src, dst are biased by 8*(count-1),to last element.\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_store_element);\n-    __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n-    __ increment(count);               \/\/ increment the count toward zero\n-    __ jcc(Assembler::zero, L_do_card_marks);\n-\n-    \/\/ ======== loop entry is here ========\n-    __ BIND(L_load_element);\n-    __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); \/\/ load the oop\n-    __ testptr(rax_oop, rax_oop);\n-    __ jcc(Assembler::zero, L_store_element);\n-\n-    __ load_klass(r11_klass, rax_oop, rscratch1);\/\/ query the object klass\n-    generate_type_check(r11_klass, ckoff, ckval, L_store_element);\n-    \/\/ ======== end loop ========\n-\n-    \/\/ It was a real error; we must depend on the caller to finish the job.\n-    \/\/ Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.\n-    \/\/ Emit GC store barriers for the oops we have copied (r14 + rdx),\n-    \/\/ and report their number to the caller.\n-    assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);\n-    Label L_post_barrier;\n-    __ addptr(r14_length, count);     \/\/ K = (original - remaining) oops\n-    __ movptr(rax, r14_length);       \/\/ save the value\n-    __ notptr(rax);                   \/\/ report (-1^K) to caller (does not affect flags)\n-    __ jccb(Assembler::notZero, L_post_barrier);\n-    __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n-\n-    \/\/ Come here on success only.\n-    __ BIND(L_do_card_marks);\n-    __ xorptr(rax, rax);              \/\/ return 0 on success\n-\n-    __ BIND(L_post_barrier);\n-    bs->arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);\n-\n-    \/\/ Common exit point (success or failure).\n-    __ BIND(L_done);\n-    __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-    __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-    __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));\n+void StubGenerator::restore_argument_regs(BasicType type) {\n+  if (type == T_BYTE || type == T_SHORT) {\n@@ -2988,101 +1234,2 @@\n-    INC_COUNTER_NP(SharedRuntime::_checkcast_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n-  }\n-\n-  \/\/\n-  \/\/  Generate 'unsafe' array copy stub\n-  \/\/  Though just as safe as the other stubs, it takes an unscaled\n-  \/\/  size_t argument instead of an element count.\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0   - source array address\n-  \/\/    c_rarg1   - destination array address\n-  \/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n-  \/\/\n-  \/\/ Examines the alignment of the operands and dispatches\n-  \/\/ to a long, int, short, or byte copy loop.\n-  \/\/\n-  address generate_unsafe_copy(const char *name,\n-                               address byte_copy_entry, address short_copy_entry,\n-                               address int_copy_entry, address long_copy_entry) {\n-\n-    Label L_long_aligned, L_int_aligned, L_short_aligned;\n-\n-    \/\/ Input registers (before setup_arg_regs)\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register size        = c_rarg2;  \/\/ byte count (size_t)\n-\n-    \/\/ Register used as a temp\n-    const Register bits        = rax;      \/\/ test copy of low bits\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ bump this on entry, not on exit:\n-    INC_COUNTER_NP(SharedRuntime::_unsafe_array_copy_ctr, rscratch1);\n-\n-    __ mov(bits, from);\n-    __ orptr(bits, to);\n-    __ orptr(bits, size);\n-\n-    __ testb(bits, BytesPerLong-1);\n-    __ jccb(Assembler::zero, L_long_aligned);\n-\n-    __ testb(bits, BytesPerInt-1);\n-    __ jccb(Assembler::zero, L_int_aligned);\n-\n-    __ testb(bits, BytesPerShort-1);\n-    __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n-\n-    __ BIND(L_short_aligned);\n-    __ shrptr(size, LogBytesPerShort); \/\/ size => short_count\n-    __ jump(RuntimeAddress(short_copy_entry));\n-\n-    __ BIND(L_int_aligned);\n-    __ shrptr(size, LogBytesPerInt); \/\/ size => int_count\n-    __ jump(RuntimeAddress(int_copy_entry));\n-\n-    __ BIND(L_long_aligned);\n-    __ shrptr(size, LogBytesPerLong); \/\/ size => qword_count\n-    __ jump(RuntimeAddress(long_copy_entry));\n-\n-    return start;\n-  }\n-\n-  \/\/ Perform range checks on the proposed arraycopy.\n-  \/\/ Kills temp, but nothing else.\n-  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n-  void arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n-                              Register src_pos, \/\/ source position (c_rarg1)\n-                              Register dst,     \/\/ destination array oo (c_rarg2)\n-                              Register dst_pos, \/\/ destination position (c_rarg3)\n-                              Register length,\n-                              Register temp,\n-                              Label& L_failed) {\n-    BLOCK_COMMENT(\"arraycopy_range_checks:\");\n-\n-    \/\/  if (src_pos + length > arrayOop(src)->length())  FAIL;\n-    __ movl(temp, length);\n-    __ addl(temp, src_pos);             \/\/ src_pos + length\n-    __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n-\n-    \/\/  if (dst_pos + length > arrayOop(dst)->length())  FAIL;\n-    __ movl(temp, length);\n-    __ addl(temp, dst_pos);             \/\/ dst_pos + length\n-    __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n-    __ jcc(Assembler::above, L_failed);\n-\n-    \/\/ Have to clean up high 32-bits of 'src_pos' and 'dst_pos'.\n-    \/\/ Move with sign extension can be used since they are positive.\n-    __ movslq(src_pos, src_pos);\n-    __ movslq(dst_pos, dst_pos);\n-\n-    BLOCK_COMMENT(\"arraycopy_range_checks done\");\n+  } else {\n+    restore_arg_regs_using_thread();\n@@ -3090,0 +1237,1 @@\n+}\n@@ -3091,331 +1239,2 @@\n-  \/\/\n-  \/\/  Generate generic array copy stubs\n-  \/\/\n-  \/\/  Input:\n-  \/\/    c_rarg0    -  src oop\n-  \/\/    c_rarg1    -  src_pos (32-bits)\n-  \/\/    c_rarg2    -  dst oop\n-  \/\/    c_rarg3    -  dst_pos (32-bits)\n-  \/\/ not Win64\n-  \/\/    c_rarg4    -  element count (32-bits)\n-  \/\/ Win64\n-  \/\/    rsp+40     -  element count (32-bits)\n-  \/\/\n-  \/\/  Output:\n-  \/\/    rax ==  0  -  success\n-  \/\/    rax == -1^K - failure, where K is partial transfer count\n-  \/\/\n-  address generate_generic_copy(const char *name,\n-                                address byte_copy_entry, address short_copy_entry,\n-                                address int_copy_entry, address oop_copy_entry,\n-                                address long_copy_entry, address checkcast_copy_entry) {\n-\n-    Label L_failed, L_failed_0, L_objArray;\n-    Label L_copy_shorts, L_copy_ints, L_copy_longs;\n-\n-    \/\/ Input registers\n-    const Register src        = c_rarg0;  \/\/ source array oop\n-    const Register src_pos    = c_rarg1;  \/\/ source position\n-    const Register dst        = c_rarg2;  \/\/ destination array oop\n-    const Register dst_pos    = c_rarg3;  \/\/ destination position\n-#ifndef _WIN64\n-    const Register length     = c_rarg4;\n-    const Register rklass_tmp = r9;  \/\/ load_klass\n-#else\n-    const Address  length(rsp, 7 * wordSize);  \/\/ elements count is on stack on Win64\n-    const Register rklass_tmp = rdi;  \/\/ load_klass\n-#endif\n-\n-    { int modulus = CodeEntryAlignment;\n-      int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n-      int advance = target - (__ offset() % modulus);\n-      if (advance < 0)  advance += modulus;\n-      if (advance > 0)  __ nop(advance);\n-    }\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n-    __ BIND(L_failed_0);\n-    __ jmp(L_failed);\n-    assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n-\n-    __ align(CodeEntryAlignment);\n-    address start = __ pc();\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WIN64\n-    __ push(rklass_tmp); \/\/ rdi is callee-save on Windows\n-#endif\n-\n-    \/\/ bump this on entry, not on exit:\n-    INC_COUNTER_NP(SharedRuntime::_generic_array_copy_ctr, rscratch1);\n-\n-    \/\/-----------------------------------------------------------------------\n-    \/\/ Assembler stub will be used for this call to arraycopy\n-    \/\/ if the following conditions are met:\n-    \/\/\n-    \/\/ (1) src and dst must not be null.\n-    \/\/ (2) src_pos must not be negative.\n-    \/\/ (3) dst_pos must not be negative.\n-    \/\/ (4) length  must not be negative.\n-    \/\/ (5) src klass and dst klass should be the same and not NULL.\n-    \/\/ (6) src and dst should be arrays.\n-    \/\/ (7) src_pos + length must not exceed length of src.\n-    \/\/ (8) dst_pos + length must not exceed length of dst.\n-    \/\/\n-\n-    \/\/  if (src == NULL) return -1;\n-    __ testptr(src, src);         \/\/ src oop\n-    size_t j1off = __ offset();\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (src_pos < 0) return -1;\n-    __ testl(src_pos, src_pos); \/\/ src_pos (32-bits)\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/  if (dst == NULL) return -1;\n-    __ testptr(dst, dst);         \/\/ dst oop\n-    __ jccb(Assembler::zero, L_failed_0);\n-\n-    \/\/  if (dst_pos < 0) return -1;\n-    __ testl(dst_pos, dst_pos); \/\/ dst_pos (32-bits)\n-    size_t j4off = __ offset();\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    \/\/ The first four tests are very dense code,\n-    \/\/ but not quite dense enough to put four\n-    \/\/ jumps in a 16-byte instruction fetch buffer.\n-    \/\/ That's good, because some branch predicters\n-    \/\/ do not like jumps so close together.\n-    \/\/ Make sure of this.\n-    guarantee(((j1off ^ j4off) & ~15) != 0, \"I$ line of 1st & 4th jumps\");\n-\n-    \/\/ registers used as temp\n-    const Register r11_length    = r11; \/\/ elements count to copy\n-    const Register r10_src_klass = r10; \/\/ array klass\n-\n-    \/\/  if (length < 0) return -1;\n-    __ movl(r11_length, length);        \/\/ length (elements count, 32-bits value)\n-    __ testl(r11_length, r11_length);\n-    __ jccb(Assembler::negative, L_failed_0);\n-\n-    __ load_klass(r10_src_klass, src, rklass_tmp);\n-#ifdef ASSERT\n-    \/\/  assert(src->klass() != NULL);\n-    {\n-      BLOCK_COMMENT(\"assert klasses not null {\");\n-      Label L1, L2;\n-      __ testptr(r10_src_klass, r10_src_klass);\n-      __ jcc(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n-      __ bind(L1);\n-      __ stop(\"broken null klass\");\n-      __ bind(L2);\n-      __ load_klass(rax, dst, rklass_tmp);\n-      __ cmpq(rax, 0);\n-      __ jcc(Assembler::equal, L1);     \/\/ this would be broken also\n-      BLOCK_COMMENT(\"} assert klasses not null done\");\n-    }\n-#endif\n-\n-    \/\/ Load layout helper (32-bits)\n-    \/\/\n-    \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n-    \/\/ 32        30    24            16              8     2                 0\n-    \/\/\n-    \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n-    \/\/\n-\n-    const int lh_offset = in_bytes(Klass::layout_helper_offset());\n-\n-    \/\/ Handle objArrays completely differently...\n-    const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n-    __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);\n-    __ jcc(Assembler::equal, L_objArray);\n-\n-    \/\/  if (src->klass() != dst->klass()) return -1;\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(r10_src_klass, rax);\n-    __ jcc(Assembler::notEqual, L_failed);\n-\n-    const Register rax_lh = rax;  \/\/ layout helper\n-    __ movl(rax_lh, Address(r10_src_klass, lh_offset));\n-\n-    \/\/ Check for flat inline type array -> return -1\n-    __ testl(rax_lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n-    __ jcc(Assembler::notZero, L_failed);\n-\n-    \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n-    __ testl(rax_lh, Klass::_lh_null_free_array_bit_inplace);\n-    __ jcc(Assembler::notZero, L_objArray);\n-\n-    \/\/  if (!src->is_Array()) return -1;\n-    __ cmpl(rax_lh, Klass::_lh_neutral_value);\n-    __ jcc(Assembler::greaterEqual, L_failed);\n-\n-    \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n-#ifdef ASSERT\n-    {\n-      BLOCK_COMMENT(\"assert primitive array {\");\n-      Label L;\n-      __ movl(rklass_tmp, rax_lh);\n-      __ sarl(rklass_tmp, Klass::_lh_array_tag_shift);\n-      __ cmpl(rklass_tmp, Klass::_lh_array_tag_type_value);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"must be a primitive array\");\n-      __ bind(L);\n-      BLOCK_COMMENT(\"} assert primitive array done\");\n-    }\n-#endif\n-\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                           r10, L_failed);\n-\n-    \/\/ TypeArrayKlass\n-    \/\/\n-    \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n-    \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n-    \/\/\n-\n-    const Register r10_offset = r10;    \/\/ array offset\n-    const Register rax_elsize = rax_lh; \/\/ element size\n-\n-    __ movl(r10_offset, rax_lh);\n-    __ shrl(r10_offset, Klass::_lh_header_size_shift);\n-    __ andptr(r10_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n-    __ addptr(src, r10_offset);           \/\/ src array offset\n-    __ addptr(dst, r10_offset);           \/\/ dst array offset\n-    BLOCK_COMMENT(\"choose copy loop based on element size\");\n-    __ andl(rax_lh, Klass::_lh_log2_element_size_mask); \/\/ rax_lh -> rax_elsize\n-\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-\n-    \/\/ next registers should be set before the jump to corresponding stub\n-    const Register from     = c_rarg0;  \/\/ source array address\n-    const Register to       = c_rarg1;  \/\/ destination array address\n-    const Register count    = c_rarg2;  \/\/ elements count\n-\n-    \/\/ 'from', 'to', 'count' registers should be set in such order\n-    \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n-\n-    __ cmpl(rax_elsize, 0);\n-    __ jccb(Assembler::notEqual, L_copy_shorts);\n-    __ lea(from, Address(src, src_pos, Address::times_1, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(byte_copy_entry));\n-\n-  __ BIND(L_copy_shorts);\n-    __ cmpl(rax_elsize, LogBytesPerShort);\n-    __ jccb(Assembler::notEqual, L_copy_ints);\n-    __ lea(from, Address(src, src_pos, Address::times_2, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(short_copy_entry));\n-\n-  __ BIND(L_copy_ints);\n-    __ cmpl(rax_elsize, LogBytesPerInt);\n-    __ jccb(Assembler::notEqual, L_copy_longs);\n-    __ lea(from, Address(src, src_pos, Address::times_4, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(int_copy_entry));\n-\n-  __ BIND(L_copy_longs);\n-#ifdef ASSERT\n-    {\n-      BLOCK_COMMENT(\"assert long copy {\");\n-      Label L;\n-      __ cmpl(rax_elsize, LogBytesPerLong);\n-      __ jcc(Assembler::equal, L);\n-      __ stop(\"must be long copy, but elsize is wrong\");\n-      __ bind(L);\n-      BLOCK_COMMENT(\"} assert long copy done\");\n-    }\n-#endif\n-    __ lea(from, Address(src, src_pos, Address::times_8, 0));\/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));\/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-    __ jump(RuntimeAddress(long_copy_entry));\n-\n-    \/\/ ObjArrayKlass\n-  __ BIND(L_objArray);\n-    \/\/ live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]\n-\n-    Label L_plain_copy, L_checkcast_copy;\n-    \/\/  test array classes for subtyping\n-    __ load_klass(rax, dst, rklass_tmp);\n-    __ cmpq(r10_src_klass, rax); \/\/ usual case is exact equality\n-    __ jcc(Assembler::notEqual, L_checkcast_copy);\n-\n-    \/\/ Identically typed arrays can be copied without element-wise checks.\n-    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                           r10, L_failed);\n-\n-    __ lea(from, Address(src, src_pos, TIMES_OOP,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n-    __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-                 arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n-    __ movl2ptr(count, r11_length); \/\/ length\n-  __ BIND(L_plain_copy);\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-    __ jump(RuntimeAddress(oop_copy_entry));\n-\n-  __ BIND(L_checkcast_copy);\n-    \/\/ live at this point:  r10_src_klass, r11_length, rax (dst_klass)\n-    {\n-      \/\/ Before looking at dst.length, make sure dst is also an objArray.\n-      \/\/ This check also fails for flat\/null-free arrays which are not supported.\n-      __ cmpl(Address(rax, lh_offset), objArray_lh);\n-      __ jcc(Assembler::notEqual, L_failed);\n-\n-#ifdef ASSERT\n-      {\n-        BLOCK_COMMENT(\"assert not null-free array {\");\n-        Label L;\n-        __ movl(rklass_tmp, Address(rax, lh_offset));\n-        __ testl(rklass_tmp, Klass::_lh_null_free_array_bit_inplace);\n-        __ jcc(Assembler::zero, L);\n-        __ stop(\"unexpected null-free array\");\n-        __ bind(L);\n-        BLOCK_COMMENT(\"} assert not null-free array\");\n-      }\n-#endif\n-\n-      \/\/ It is safe to examine both src.length and dst.length.\n-      arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n-                             rax, L_failed);\n-\n-      const Register r11_dst_klass = r11;\n-      __ load_klass(r11_dst_klass, dst, rklass_tmp); \/\/ reload\n-\n-      \/\/ Marshal the base address arguments now, freeing registers.\n-      __ lea(from, Address(src, src_pos, TIMES_OOP,\n-                   arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n-                   arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n-      __ movl(count, length);           \/\/ length (reloaded)\n-      Register sco_temp = c_rarg3;      \/\/ this register is free now\n-      assert_different_registers(from, to, count, sco_temp,\n-                                 r11_dst_klass, r10_src_klass);\n-      assert_clean_int(count, sco_temp);\n-\n-      \/\/ Generate the type check.\n-      const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n-      __ movl(sco_temp, Address(r11_dst_klass, sco_offset));\n-      assert_clean_int(sco_temp, rax);\n-      generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);\n-\n-      \/\/ Fetch destination element klass from the ObjArrayKlass header.\n-      int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n-      __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));\n-      __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));\n-      assert_clean_int(sco_temp, rax);\n-\n-#ifdef _WIN64\n-      __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n+address StubGenerator::generate_data_cache_writeback() {\n+  const Register src        = c_rarg0;  \/\/ source address\n@@ -3423,7 +1242,1 @@\n-      \/\/ the checkcast_copy loop needs two extra arguments:\n-      assert(c_rarg3 == sco_temp, \"#3 already in place\");\n-      \/\/ Set up arguments for checkcast_copy_entry.\n-      setup_arg_regs(4);\n-      __ movptr(r8, r11_dst_klass);  \/\/ dst.klass.element_klass, r8 is c_rarg4 on Linux\/Solaris\n-      __ jump(RuntimeAddress(checkcast_copy_entry));\n-    }\n+  __ align(CodeEntryAlignment);\n@@ -3431,8 +1244,1 @@\n-  __ BIND(L_failed);\n-#ifdef _WIN64\n-    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n-#endif\n-    __ xorptr(rax, rax);\n-    __ notptr(rax); \/\/ return -1\n-    __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n@@ -3440,2 +1246,1 @@\n-    return start;\n-  }\n+  address start = __ pc();\n@@ -3443,2 +1248,4 @@\n-  address generate_data_cache_writeback() {\n-    const Register src        = c_rarg0;  \/\/ source address\n+  __ enter();\n+  __ cache_wb(Address(src, 0));\n+  __ leave();\n+  __ ret(0);\n@@ -3446,1 +1253,2 @@\n-    __ align(CodeEntryAlignment);\n+  return start;\n+}\n@@ -3448,1 +1256,2 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback\");\n+address StubGenerator::generate_data_cache_writeback_sync() {\n+  const Register is_pre    = c_rarg0;  \/\/ pre or post sync\n@@ -3450,5 +1259,1 @@\n-    address start = __ pc();\n-    __ enter();\n-    __ cache_wb(Address(src, 0));\n-    __ leave();\n-    __ ret(0);\n+  __ align(CodeEntryAlignment);\n@@ -3456,2 +1261,1 @@\n-    return start;\n-  }\n+  StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n@@ -3459,2 +1263,2 @@\n-  address generate_data_cache_writeback_sync() {\n-    const Register is_pre    = c_rarg0;  \/\/ pre or post sync\n+  \/\/ pre wbsync is a no-op\n+  \/\/ post wbsync translates to an sfence\n@@ -3462,1 +1266,2 @@\n-    __ align(CodeEntryAlignment);\n+  Label skip;\n+  address start = __ pc();\n@@ -3464,1 +1269,7 @@\n-    StubCodeMark mark(this, \"StubRoutines\", \"_data_cache_writeback_sync\");\n+  __ enter();\n+  __ cmpl(is_pre, 0);\n+  __ jcc(Assembler::notEqual, skip);\n+  __ cache_wbsync(false);\n+  __ bind(skip);\n+  __ leave();\n+  __ ret(0);\n@@ -3466,2 +1277,2 @@\n-    \/\/ pre wbsync is a no-op\n-    \/\/ post wbsync translates to an sfence\n+  return start;\n+}\n@@ -3469,9 +1280,1 @@\n-    Label skip;\n-    address start = __ pc();\n-    __ enter();\n-    __ cmpl(is_pre, 0);\n-    __ jcc(Assembler::notEqual, skip);\n-    __ cache_wbsync(false);\n-    __ bind(skip);\n-    __ leave();\n-    __ ret(0);\n+\/\/ AES intrinsic stubs\n@@ -3479,2 +1282,4 @@\n-    return start;\n-  }\n+address StubGenerator::generate_key_shuffle_mask() {\n+  __ align(16);\n+  StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n+  address start = __ pc();\n@@ -3482,53 +1287,2 @@\n-  void generate_arraycopy_stubs() {\n-    address entry;\n-    address entry_jbyte_arraycopy;\n-    address entry_jshort_arraycopy;\n-    address entry_jint_arraycopy;\n-    address entry_oop_arraycopy;\n-    address entry_jlong_arraycopy;\n-    address entry_checkcast_arraycopy;\n-\n-    StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &entry,\n-                                                                           \"jbyte_disjoint_arraycopy\");\n-    StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &entry_jbyte_arraycopy,\n-                                                                           \"jbyte_arraycopy\");\n-\n-    StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &entry,\n-                                                                            \"jshort_disjoint_arraycopy\");\n-    StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &entry_jshort_arraycopy,\n-                                                                            \"jshort_arraycopy\");\n-\n-    StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &entry,\n-                                                                              \"jint_disjoint_arraycopy\");\n-    StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,\n-                                                                              &entry_jint_arraycopy, \"jint_arraycopy\");\n-\n-    StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &entry,\n-                                                                               \"jlong_disjoint_arraycopy\");\n-    StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,\n-                                                                               &entry_jlong_arraycopy, \"jlong_arraycopy\");\n-\n-\n-    if (UseCompressedOops) {\n-      StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                              \"oop_disjoint_arraycopy\");\n-      StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                              &entry_oop_arraycopy, \"oop_arraycopy\");\n-      StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &entry,\n-                                                                                     \"oop_disjoint_arraycopy_uninit\",\n-                                                                                     \/*dest_uninitialized*\/true);\n-      StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,\n-                                                                                     NULL, \"oop_arraycopy_uninit\",\n-                                                                                     \/*dest_uninitialized*\/true);\n-    } else {\n-      StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                               \"oop_disjoint_arraycopy\");\n-      StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                               &entry_oop_arraycopy, \"oop_arraycopy\");\n-      StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &entry,\n-                                                                                      \"oop_disjoint_arraycopy_uninit\",\n-                                                                                      \/*dest_uninitialized*\/true);\n-      StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,\n-                                                                                      NULL, \"oop_arraycopy_uninit\",\n-                                                                                      \/*dest_uninitialized*\/true);\n-    }\n+  __ emit_data64( 0x0405060700010203, relocInfo::none );\n+  __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );\n@@ -3536,44 +1290,2 @@\n-    StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n-    StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n-                                                                        \/*dest_uninitialized*\/true);\n-\n-    StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n-                                                              entry_jbyte_arraycopy,\n-                                                              entry_jshort_arraycopy,\n-                                                              entry_jint_arraycopy,\n-                                                              entry_jlong_arraycopy);\n-    StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n-                                                               entry_jbyte_arraycopy,\n-                                                               entry_jshort_arraycopy,\n-                                                               entry_jint_arraycopy,\n-                                                               entry_oop_arraycopy,\n-                                                               entry_jlong_arraycopy,\n-                                                               entry_checkcast_arraycopy);\n-\n-    StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n-    StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n-    StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n-    StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n-    StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n-    StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n-\n-    \/\/ We don't generate specialized code for HeapWord-aligned source\n-    \/\/ arrays, so just use the code we've already generated\n-    StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;\n-\n-    StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;\n-\n-    StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;\n-\n-    StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;\n-    StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;\n-\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;\n-    StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;\n-\n-    StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;\n-    StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;\n-  }\n+  return start;\n+}\n@@ -3581,2 +1293,4 @@\n-  \/\/ AES intrinsic stubs\n-  enum {AESBlockSize = 16};\n+address StubGenerator::generate_counter_shuffle_mask() {\n+  __ align(16);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n+  address start = __ pc();\n@@ -3584,8 +1298,2 @@\n-  address generate_key_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"key_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data64( 0x0405060700010203, relocInfo::none );\n-    __ emit_data64( 0x0c0d0e0f08090a0b, relocInfo::none );\n-    return start;\n-  }\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n@@ -3593,8 +1301,2 @@\n-  address generate_counter_shuffle_mask() {\n-    __ align(16);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_shuffle_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -3602,9 +1304,8 @@\n-  \/\/ Utility routine for loading a 128-bit key word in little endian format\n-  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n-  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n-    __ movdqu(xmmdst, Address(key, offset));\n-    if (xmm_shuf_mask != xnoreg) {\n-      __ pshufb(xmmdst, xmm_shuf_mask);\n-    } else {\n-      __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    }\n+\/\/ Utility routine for loading a 128-bit key word in little endian format\n+\/\/ can optionally specify that the shuffle mask is already in an xmmregister\n+void StubGenerator::load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n+  __ movdqu(xmmdst, Address(key, offset));\n+  if (xmm_shuf_mask != xnoreg) {\n+    __ pshufb(xmmdst, xmm_shuf_mask);\n+  } else {\n+    __ pshufb(xmmdst, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n@@ -3612,0 +1313,1 @@\n+}\n@@ -3613,11 +1315,11 @@\n-  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n-  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n-    __ pextrq(reg, xmmdst, 0x0);\n-    __ addq(reg, inc_delta);\n-    __ pinsrq(xmmdst, reg, 0x0);\n-    __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n-    __ pextrq(reg, xmmdst, 0x01); \/\/ Carry\n-    __ addq(reg, 0x01);\n-    __ pinsrq(xmmdst, reg, 0x01); \/\/Carry end\n-    __ BIND(next_block);          \/\/ next instruction\n-  }\n+\/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n+void StubGenerator::inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block) {\n+  __ pextrq(reg, xmmdst, 0x0);\n+  __ addq(reg, inc_delta);\n+  __ pinsrq(xmmdst, reg, 0x0);\n+  __ jcc(Assembler::carryClear, next_block); \/\/ jump if no carry\n+  __ pextrq(reg, xmmdst, 0x01); \/\/ Carry\n+  __ addq(reg, 0x01);\n+  __ pinsrq(xmmdst, reg, 0x01); \/\/Carry end\n+  __ BIND(next_block);          \/\/ next instruction\n+}\n@@ -3625,18 +1327,13 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_encryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register keylen      = rax;\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/\n+address StubGenerator::generate_aescrypt_encryptBlock() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_encryptBlock\");\n+  Label L_doLast;\n+  address start = __ pc();\n@@ -3644,7 +1341,4 @@\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register keylen      = rax;\n@@ -3652,1 +1346,7 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n+  const XMMRegister xmm_temp1  = xmm2;\n+  const XMMRegister xmm_temp2  = xmm3;\n+  const XMMRegister xmm_temp3  = xmm4;\n+  const XMMRegister xmm_temp4  = xmm5;\n@@ -3654,2 +1354,1 @@\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -3657,2 +1356,2 @@\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n+  \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n@@ -3660,2 +1359,2 @@\n-    \/\/ For encryption, the java expanded key ordering is just what we need\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  __ movdqu(xmm_result, Address(from, 0));  \/\/ get 16 bytes of input\n@@ -3663,2 +1362,2 @@\n-    load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n-    __ pxor(xmm_result, xmm_temp1);\n+  \/\/ For encryption, the java expanded key ordering is just what we need\n+  \/\/ we don't know if the key is aligned, hence not using load-execute form\n@@ -3666,4 +1365,2 @@\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n+  load_key(xmm_temp1, key, 0x00, xmm_key_shuf_mask);\n+  __ pxor(xmm_result, xmm_temp1);\n@@ -3671,4 +1368,4 @@\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n+  load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n@@ -3676,4 +1373,4 @@\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n+  __ aesenc(xmm_result, xmm_temp3);\n+  __ aesenc(xmm_result, xmm_temp4);\n@@ -3681,4 +1378,4 @@\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n-    __ aesenc(xmm_result, xmm_temp3);\n-    __ aesenc(xmm_result, xmm_temp4);\n+  load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n@@ -3686,2 +1383,4 @@\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n+  __ aesenc(xmm_result, xmm_temp3);\n+  __ aesenc(xmm_result, xmm_temp4);\n@@ -3689,2 +1388,2 @@\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n+  load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n@@ -3692,2 +1391,2 @@\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n+  __ cmpl(keylen, 44);\n+  __ jccb(Assembler::equal, L_doLast);\n@@ -3695,2 +1394,2 @@\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n@@ -3698,2 +1397,2 @@\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n+  load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n@@ -3701,2 +1400,2 @@\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenc(xmm_result, xmm_temp2);\n+  __ cmpl(keylen, 52);\n+  __ jccb(Assembler::equal, L_doLast);\n@@ -3704,2 +1403,2 @@\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenc(xmm_result, xmm_temp2);\n@@ -3707,7 +1406,2 @@\n-    __ BIND(L_doLast);\n-    __ aesenc(xmm_result, xmm_temp1);\n-    __ aesenclast(xmm_result, xmm_temp2);\n-    __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n@@ -3715,2 +1409,7 @@\n-    return start;\n-  }\n+  __ BIND(L_doLast);\n+  __ aesenc(xmm_result, xmm_temp1);\n+  __ aesenclast(xmm_result, xmm_temp2);\n+  __ movdqu(Address(to, 0), xmm_result);        \/\/ store the result\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -3718,0 +1417,2 @@\n+  return start;\n+}\n@@ -3719,89 +1420,90 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/\n-  address generate_aescrypt_decryptBlock() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n-    Label L_doLast;\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register keylen      = rax;\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n-    const XMMRegister xmm_temp1  = xmm2;\n-    const XMMRegister xmm_temp2  = xmm3;\n-    const XMMRegister xmm_temp3  = xmm4;\n-    const XMMRegister xmm_temp4  = xmm5;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    __ movdqu(xmm_result, Address(from, 0));\n-\n-    \/\/ for decryption java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    \/\/ we don't know if the key is aligned, hence not using load-execute form\n-    load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n-\n-    __ pxor  (xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n-    load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-    __ aesdec(xmm_result, xmm_temp3);\n-    __ aesdec(xmm_result, xmm_temp4);\n-\n-    load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n-    load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 44);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n-\n-    __ cmpl(keylen, 52);\n-    __ jccb(Assembler::equal, L_doLast);\n-\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n-    load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n-\n-    __ BIND(L_doLast);\n-    __ aesdec(xmm_result, xmm_temp1);\n-    __ aesdec(xmm_result, xmm_temp2);\n-\n-    \/\/ for decryption the aesdeclast operation is always on key+0x00\n-    __ aesdeclast(xmm_result, xmm_temp3);\n-    __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n-    __ xorptr(rax, rax); \/\/ return 0\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/\n+address StubGenerator::generate_aescrypt_decryptBlock() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"aescrypt_decryptBlock\");\n+  Label L_doLast;\n+  address start = __ pc();\n+\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register keylen      = rax;\n+\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  \/\/ On win64 xmm6-xmm15 must be preserved so don't use them.\n+  const XMMRegister xmm_temp1  = xmm2;\n+  const XMMRegister xmm_temp2  = xmm3;\n+  const XMMRegister xmm_temp3  = xmm4;\n+  const XMMRegister xmm_temp4  = xmm5;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  \/\/ keylen could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movl(keylen, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  __ movdqu(xmm_result, Address(from, 0));\n+\n+  \/\/ for decryption java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 0x10 here and hit 0x00 last\n+  \/\/ we don't know if the key is aligned, hence not using load-execute form\n+  load_key(xmm_temp1, key, 0x10, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x20, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x30, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x40, xmm_key_shuf_mask);\n+\n+  __ pxor  (xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+  __ aesdec(xmm_result, xmm_temp3);\n+  __ aesdec(xmm_result, xmm_temp4);\n+\n+  load_key(xmm_temp1, key, 0x50, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0x60, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x70, xmm_key_shuf_mask);\n+  load_key(xmm_temp4, key, 0x80, xmm_key_shuf_mask);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+  __ aesdec(xmm_result, xmm_temp3);\n+  __ aesdec(xmm_result, xmm_temp4);\n+\n+  load_key(xmm_temp1, key, 0x90, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xa0, xmm_key_shuf_mask);\n+  load_key(xmm_temp3, key, 0x00, xmm_key_shuf_mask);\n+\n+  __ cmpl(keylen, 44);\n+  __ jccb(Assembler::equal, L_doLast);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  load_key(xmm_temp1, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xc0, xmm_key_shuf_mask);\n+\n+  __ cmpl(keylen, 52);\n+  __ jccb(Assembler::equal, L_doLast);\n+\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  load_key(xmm_temp1, key, 0xd0, xmm_key_shuf_mask);\n+  load_key(xmm_temp2, key, 0xe0, xmm_key_shuf_mask);\n+\n+  __ BIND(L_doLast);\n+  __ aesdec(xmm_result, xmm_temp1);\n+  __ aesdec(xmm_result, xmm_temp2);\n+\n+  \/\/ for decryption the aesdeclast operation is always on key+0x00\n+  __ aesdeclast(xmm_result, xmm_temp3);\n+  __ movdqu(Address(to, 0), xmm_result);  \/\/ store the result\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -3810,2 +1512,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -3814,24 +1516,24 @@\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_encryptAESCrypt() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n-    address start = __ pc();\n-\n-    Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - r vector byte array address\n+\/\/   c_rarg4   - input length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_cipherBlockChaining_encryptAESCrypt() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_encryptAESCrypt\");\n+  address start = __ pc();\n+\n+  Label L_exit, L_key_192_256, L_key_256, L_loopTop_128, L_loopTop_192, L_loopTop_256;\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+                                         \/\/ and left with the results of the last encryption block\n@@ -3839,1 +1541,1 @@\n-    const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+  const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n@@ -3841,2 +1543,2 @@\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg     = r11;      \/\/ pick the volatile windows register\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg     = r11;      \/\/ pick the volatile windows register\n@@ -3844,15 +1546,15 @@\n-    const Register pos         = rax;\n-\n-    \/\/ xmm register assignments for the loops below\n-    const XMMRegister xmm_result = xmm0;\n-    const XMMRegister xmm_temp   = xmm1;\n-    \/\/ keys 0-10 preloaded into xmm2-xmm12\n-    const int XMM_REG_NUM_KEY_FIRST = 2;\n-    const int XMM_REG_NUM_KEY_LAST  = 15;\n-    const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-    const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);\n-    const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);\n-    const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);\n-    const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  const Register pos         = rax;\n+\n+  \/\/ xmm register assignments for the loops below\n+  const XMMRegister xmm_result = xmm0;\n+  const XMMRegister xmm_temp   = xmm1;\n+  \/\/ keys 0-10 preloaded into xmm2-xmm12\n+  const int XMM_REG_NUM_KEY_FIRST = 2;\n+  const int XMM_REG_NUM_KEY_LAST  = 15;\n+  const XMMRegister xmm_key0   = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n+  const XMMRegister xmm_key10  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+10);\n+  const XMMRegister xmm_key11  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+11);\n+  const XMMRegister xmm_key12  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+12);\n+  const XMMRegister xmm_key13  = as_XMMRegister(XMM_REG_NUM_KEY_FIRST+13);\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -3861,2 +1563,2 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n@@ -3864,1 +1566,1 @@\n-    __ push(len_reg); \/\/ Save\n+  __ push(len_reg); \/\/ Save\n@@ -3867,31 +1569,31 @@\n-    const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum <= XMM_REG_NUM_KEY_FIRST+10; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-    __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n-\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rax, 44);\n-    __ jcc(Assembler::notEqual, L_key_192_256);\n-\n-    \/\/ 128 bit code follows here\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_128);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum <= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    __ aesenclast(xmm_result, xmm_key10);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_128);\n+  const XMMRegister xmm_key_shuf_mask = xmm_temp;  \/\/ used temporarily to swap key bytes up front\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  \/\/ load up xmm regs xmm2 thru xmm12 with key 0x00 - 0xa0\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x00; rnum <= XMM_REG_NUM_KEY_FIRST+10; rnum++) {\n+    load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n+    offset += 0x10;\n+  }\n+  __ movdqu(xmm_result, Address(rvec, 0x00));   \/\/ initialize xmm_result with r vec\n+\n+  \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n+  __ movl(rax, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rax, 44);\n+  __ jcc(Assembler::notEqual, L_key_192_256);\n+\n+  \/\/ 128 bit code follows here\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_128);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum <= XMM_REG_NUM_KEY_FIRST + 9; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n+  }\n+  __ aesenclast(xmm_result, xmm_key10);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_128);\n@@ -3899,2 +1601,2 @@\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n+  __ BIND(L_exit);\n+  __ movdqu(Address(rvec, 0), xmm_result);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n@@ -3903,1 +1605,1 @@\n-    __ movl(rax, len_mem);\n+  __ movl(rax, len_mem);\n@@ -3905,1 +1607,1 @@\n-    __ pop(rax); \/\/ return length\n+  __ pop(rax); \/\/ return length\n@@ -3907,13 +1609,50 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    __ BIND(L_key_192_256);\n-    \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n-    load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);\n-    load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);\n-    __ cmpl(rax, 52);\n-    __ jcc(Assembler::notEqual, L_key_256);\n-\n-    \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  __ BIND(L_key_192_256);\n+  \/\/ here rax = len in ints of AESCrypt.KLE array (52=192, or 60=256)\n+  load_key(xmm_key11, key, 0xb0, xmm_key_shuf_mask);\n+  load_key(xmm_key12, key, 0xc0, xmm_key_shuf_mask);\n+  __ cmpl(rax, 52);\n+  __ jcc(Assembler::notEqual, L_key_256);\n+\n+  \/\/ 192-bit code follows here (could be changed to use more xmm registers)\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_192);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n+  }\n+  __ aesenclast(xmm_result, xmm_key12);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_192);\n+  __ jmp(L_exit);\n+\n+  __ BIND(L_key_256);\n+  \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n+  load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);\n+  __ movptr(pos, 0);\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_loopTop_256);\n+  __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n+  __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n+  __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {\n+    __ aesenc(xmm_result, as_XMMRegister(rnum));\n+  }\n+  load_key(xmm_temp, key, 0xe0);\n+  __ aesenclast(xmm_result, xmm_temp);\n+  __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n+  \/\/ no need to store r to memory until we exit\n+  __ addptr(pos, AESBlockSize);\n+  __ subptr(len_reg, AESBlockSize);\n+  __ jcc(Assembler::notEqual, L_loopTop_256);\n+  __ jmp(L_exit);\n@@ -3921,39 +1660,2 @@\n-    __ BIND(L_loopTop_192);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 11; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    __ aesenclast(xmm_result, xmm_key12);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_192);\n-    __ jmp(L_exit);\n-\n-    __ BIND(L_key_256);\n-    \/\/ 256-bit code follows here (could be changed to use more xmm registers)\n-    load_key(xmm_key13, key, 0xd0, xmm_key_shuf_mask);\n-    __ movptr(pos, 0);\n-    __ align(OptoLoopAlignment);\n-\n-    __ BIND(L_loopTop_256);\n-    __ movdqu(xmm_temp, Address(from, pos, Address::times_1, 0));   \/\/ get next 16 bytes of input\n-    __ pxor  (xmm_result, xmm_temp);               \/\/ xor with the current r vector\n-    __ pxor  (xmm_result, xmm_key0);               \/\/ do the aes rounds\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST + 1; rnum  <= XMM_REG_NUM_KEY_FIRST + 13; rnum++) {\n-      __ aesenc(xmm_result, as_XMMRegister(rnum));\n-    }\n-    load_key(xmm_temp, key, 0xe0);\n-    __ aesenclast(xmm_result, xmm_temp);\n-    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result);     \/\/ store into the next 16 bytes of output\n-    \/\/ no need to store r to memory until we exit\n-    __ addptr(pos, AESBlockSize);\n-    __ subptr(len_reg, AESBlockSize);\n-    __ jcc(Assembler::notEqual, L_loopTop_256);\n-    __ jmp(L_exit);\n-\n-    return start;\n-  }\n+  return start;\n+}\n@@ -3961,26 +1663,26 @@\n-  \/\/ This is a version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n-  \/\/ to hide instruction latency\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - r vector byte array address\n-  \/\/   c_rarg4   - input length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n-\n-    const Register from        = c_rarg0;  \/\/ source array address\n-    const Register to          = c_rarg1;  \/\/ destination array address\n-    const Register key         = c_rarg2;  \/\/ key array address\n-    const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-                                           \/\/ and left with the results of the last encryption block\n+\/\/ This is a version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n+\/\/ to hide instruction latency\n+\/\/\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - r vector byte array address\n+\/\/   c_rarg4   - input length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_cipherBlockChaining_decryptAESCrypt_Parallel() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register key         = c_rarg2;  \/\/ key array address\n+  const Register rvec        = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+                                         \/\/ and left with the results of the last encryption block\n@@ -3988,1 +1690,1 @@\n-    const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+  const Register len_reg     = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n@@ -3990,2 +1692,2 @@\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg     = r11;      \/\/ pick the volatile windows register\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg     = r11;      \/\/ pick the volatile windows register\n@@ -3993,1 +1695,1 @@\n-    const Register pos         = rax;\n+  const Register pos         = rax;\n@@ -3995,2 +1697,2 @@\n-    const int PARALLEL_FACTOR = 4;\n-    const int ROUNDS[3] = { 10, 12, 14 }; \/\/ aes rounds for key128, key192, key256\n+  const int PARALLEL_FACTOR = 4;\n+  const int ROUNDS[3] = { 10, 12, 14 }; \/\/ aes rounds for key128, key192, key256\n@@ -3998,6 +1700,6 @@\n-    Label L_exit;\n-    Label L_singleBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-    Label L_singleBlock_loopTopHead2[3]; \/\/ 128, 192, 256\n-    Label L_singleBlock_loopTop[3]; \/\/ 128, 192, 256\n-    Label L_multiBlock_loopTopHead[3]; \/\/ 128, 192, 256\n-    Label L_multiBlock_loopTop[3]; \/\/ 128, 192, 256\n+  Label L_exit;\n+  Label L_singleBlock_loopTopHead[3]; \/\/ 128, 192, 256\n+  Label L_singleBlock_loopTopHead2[3]; \/\/ 128, 192, 256\n+  Label L_singleBlock_loopTop[3]; \/\/ 128, 192, 256\n+  Label L_multiBlock_loopTopHead[3]; \/\/ 128, 192, 256\n+  Label L_multiBlock_loopTop[3]; \/\/ 128, 192, 256\n@@ -4005,5 +1707,5 @@\n-    \/\/ keys 0-10 preloaded into xmm5-xmm15\n-    const int XMM_REG_NUM_KEY_FIRST = 5;\n-    const int XMM_REG_NUM_KEY_LAST  = 15;\n-    const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n-    const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);\n+  \/\/ keys 0-10 preloaded into xmm5-xmm15\n+  const int XMM_REG_NUM_KEY_FIRST = 5;\n+  const int XMM_REG_NUM_KEY_LAST  = 15;\n+  const XMMRegister xmm_key_first = as_XMMRegister(XMM_REG_NUM_KEY_FIRST);\n+  const XMMRegister xmm_key_last  = as_XMMRegister(XMM_REG_NUM_KEY_LAST);\n@@ -4011,1 +1713,1 @@\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -4014,2 +1716,2 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n@@ -4017,1 +1719,1 @@\n-    __ push(len_reg); \/\/ Save\n+  __ push(len_reg); \/\/ Save\n@@ -4019,11 +1721,11 @@\n-    __ push(rbx);\n-    \/\/ the java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 0x10 here and hit 0x00 last\n-    const XMMRegister xmm_key_shuf_mask = xmm1;  \/\/ used temporarily to swap key bytes up front\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-    \/\/ load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00\n-    for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum < XMM_REG_NUM_KEY_LAST; rnum++) {\n-      load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n-      offset += 0x10;\n-    }\n-    load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);\n+  __ push(rbx);\n+  \/\/ the java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 0x10 here and hit 0x00 last\n+  const XMMRegister xmm_key_shuf_mask = xmm1;  \/\/ used temporarily to swap key bytes up front\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+  \/\/ load up xmm regs 5 thru 15 with key 0x10 - 0xa0 - 0x00\n+  for (int rnum = XMM_REG_NUM_KEY_FIRST, offset = 0x10; rnum < XMM_REG_NUM_KEY_LAST; rnum++) {\n+    load_key(as_XMMRegister(rnum), key, offset, xmm_key_shuf_mask);\n+    offset += 0x10;\n+  }\n+  load_key(xmm_key_last, key, 0x00, xmm_key_shuf_mask);\n@@ -4031,1 +1733,1 @@\n-    const XMMRegister xmm_prev_block_cipher = xmm1;  \/\/ holds cipher of previous block\n+  const XMMRegister xmm_prev_block_cipher = xmm1;  \/\/ holds cipher of previous block\n@@ -4033,5 +1735,5 @@\n-    \/\/ registers holding the four results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm0;\n-    const XMMRegister xmm_result1 = xmm2;\n-    const XMMRegister xmm_result2 = xmm3;\n-    const XMMRegister xmm_result3 = xmm4;\n+  \/\/ registers holding the four results in the parallelized loop\n+  const XMMRegister xmm_result0 = xmm0;\n+  const XMMRegister xmm_result1 = xmm2;\n+  const XMMRegister xmm_result2 = xmm3;\n+  const XMMRegister xmm_result3 = xmm4;\n@@ -4039,1 +1741,1 @@\n-    __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   \/\/ initialize with initial rvec\n+  __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));   \/\/ initialize with initial rvec\n@@ -4041,1 +1743,1 @@\n-    __ xorptr(pos, pos);\n+  __ xorptr(pos, pos);\n@@ -4043,6 +1745,6 @@\n-    \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n-    __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rbx, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);\n-    __ cmpl(rbx, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);\n+  \/\/ now split to different paths depending on the keylen (len in ints of AESCrypt.KLE array (52=192, or 60=256))\n+  __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rbx, 52);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTopHead[1]);\n+  __ cmpl(rbx, 60);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTopHead[2]);\n@@ -4051,32 +1753,8 @@\n-  __ opc(xmm_result0, src_reg);         \\\n-  __ opc(xmm_result1, src_reg);         \\\n-  __ opc(xmm_result2, src_reg);         \\\n-  __ opc(xmm_result3, src_reg);         \\\n-\n-    for (int k = 0; k < 3; ++k) {\n-      __ BIND(L_multiBlock_loopTopHead[k]);\n-      if (k != 0) {\n-        __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n-        __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);\n-      }\n-      if (k == 1) {\n-        __ subptr(rsp, 6 * wordSize);\n-        __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-        load_key(xmm15, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-        __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-        __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-      } else if (k == 2) {\n-        __ subptr(rsp, 10 * wordSize);\n-        __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n-        load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n-        __ movdqu(Address(rsp, 6 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xe0);  \/\/ 0xe0;\n-        __ movdqu(Address(rsp, 8 * wordSize), xmm1);\n-        load_key(xmm15, key, 0xb0); \/\/ 0xb0;\n-        __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n-        load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n-        __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n-      }\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n+__ opc(xmm_result0, src_reg);         \\\n+__ opc(xmm_result1, src_reg);         \\\n+__ opc(xmm_result2, src_reg);         \\\n+__ opc(xmm_result3, src_reg);         \\\n+\n+  for (int k = 0; k < 3; ++k) {\n+    __ BIND(L_multiBlock_loopTopHead[k]);\n+    if (k != 0) {\n@@ -4084,11 +1762,25 @@\n-      __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);\n-\n-      if  (k != 0) {\n-        __ movdqu(xmm15, Address(rsp, 2 * wordSize));\n-        __ movdqu(xmm1, Address(rsp, 4 * wordSize));\n-      }\n-\n-      __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n-      __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n+      __ jcc(Assembler::less, L_singleBlock_loopTopHead2[k]);\n+    }\n+    if (k == 1) {\n+      __ subptr(rsp, 6 * wordSize);\n+      __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n+      load_key(xmm15, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n+      __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n+      __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n+    } else if (k == 2) {\n+      __ subptr(rsp, 10 * wordSize);\n+      __ movdqu(Address(rsp, 0), xmm15); \/\/save last_key from xmm15\n+      load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n+      __ movdqu(Address(rsp, 6 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xe0);  \/\/ 0xe0;\n+      __ movdqu(Address(rsp, 8 * wordSize), xmm1);\n+      load_key(xmm15, key, 0xb0); \/\/ 0xb0;\n+      __ movdqu(Address(rsp, 2 * wordSize), xmm15);\n+      load_key(xmm1, key, 0xc0);  \/\/ 0xc0;\n+      __ movdqu(Address(rsp, 4 * wordSize), xmm1);\n+    }\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_multiBlock_loopTop[k]);\n+    __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least 4 blocks left\n+    __ jcc(Assembler::less, L_singleBlock_loopTopHead[k]);\n@@ -4096,27 +1788,4 @@\n-      DoFour(pxor, xmm_key_first);\n-      if (k == 0) {\n-        for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        DoFour(aesdeclast, xmm_key_last);\n-      } else if (k == 1) {\n-        for (int rnum = 1; rnum <= ROUNDS[k]-2; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-        __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-        DoFour(aesdeclast, xmm_key_last);\n-      } else if (k == 2) {\n-        for (int rnum = 1; rnum <= ROUNDS[k] - 4; rnum++) {\n-          DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n-        }\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n-        __ movdqu(xmm15, Address(rsp, 6 * wordSize));\n-        __ movdqu(xmm1, Address(rsp, 8 * wordSize));\n-        DoFour(aesdec, xmm15);  \/\/ key : 0xd0\n-        __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n-        DoFour(aesdec, xmm1);  \/\/ key : 0xe0\n-        __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n-        DoFour(aesdeclast, xmm_key_last);\n-      }\n+    if  (k != 0) {\n+      __ movdqu(xmm15, Address(rsp, 2 * wordSize));\n+      __ movdqu(xmm1, Address(rsp, 4 * wordSize));\n+    }\n@@ -4124,12 +1793,4 @@\n-      \/\/ for each result, xor with the r vector of previous cipher block\n-      __ pxor(xmm_result0, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-      __ pxor(xmm_result1, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ pxor(xmm_result2, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ pxor(xmm_result3, xmm_prev_block_cipher);\n-      __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   \/\/ this will carry over to next set of blocks\n-      if (k != 0) {\n-        __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);\n-      }\n+    __ movdqu(xmm_result0, Address(from, pos, Address::times_1, 0 * AESBlockSize)); \/\/ get next 4 blocks into xmmresult registers\n+    __ movdqu(xmm_result1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ movdqu(xmm_result2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ movdqu(xmm_result3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n@@ -4137,29 +1798,4 @@\n-      __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     \/\/ store 4 results into the next 64 bytes of output\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ registers used in the non-parallelized loops\n-      \/\/ xmm register assignments for the loops below\n-      const XMMRegister xmm_result = xmm0;\n-      const XMMRegister xmm_prev_block_cipher_save = xmm2;\n-      const XMMRegister xmm_key11 = xmm3;\n-      const XMMRegister xmm_key12 = xmm4;\n-      const XMMRegister key_tmp = xmm4;\n-\n-      __ BIND(L_singleBlock_loopTopHead[k]);\n-      if (k == 1) {\n-        __ addptr(rsp, 6 * wordSize);\n-      } else if (k == 2) {\n-        __ addptr(rsp, 10 * wordSize);\n-      }\n-      __ cmpptr(len_reg, 0); \/\/ any blocks left??\n-      __ jcc(Assembler::equal, L_exit);\n-      __ BIND(L_singleBlock_loopTopHead2[k]);\n-      if (k == 1) {\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n-        load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n+    DoFour(pxor, xmm_key_first);\n+    if (k == 0) {\n+      for (int rnum = 1; rnum < ROUNDS[k]; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n@@ -4167,2 +1803,4 @@\n-      if (k == 2) {\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n+      DoFour(aesdeclast, xmm_key_last);\n+    } else if (k == 1) {\n+      for (int rnum = 1; rnum <= ROUNDS[k]-2; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n@@ -4170,7 +1808,7 @@\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlock_loopTop[k]);\n-      __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n-      __ movdqa(xmm_prev_block_cipher_save, xmm_result); \/\/ save for next r vector\n-      __ pxor(xmm_result, xmm_key_first); \/\/ do the aes dec rounds\n-      for (int rnum = 1; rnum <= 9 ; rnum++) {\n-          __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n+      __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n+      __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n+      DoFour(aesdeclast, xmm_key_last);\n+    } else if (k == 2) {\n+      for (int rnum = 1; rnum <= ROUNDS[k] - 4; rnum++) {\n+        DoFour(aesdec, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n@@ -4178,26 +1816,90 @@\n-      if (k == 1) {\n-        __ aesdec(xmm_result, xmm_key11);\n-        __ aesdec(xmm_result, xmm_key12);\n-      }\n-      if (k == 2) {\n-        __ aesdec(xmm_result, xmm_key11);\n-        load_key(key_tmp, key, 0xc0);\n-        __ aesdec(xmm_result, key_tmp);\n-        load_key(key_tmp, key, 0xd0);\n-        __ aesdec(xmm_result, key_tmp);\n-        load_key(key_tmp, key, 0xe0);\n-        __ aesdec(xmm_result, key_tmp);\n-      }\n-\n-      __ aesdeclast(xmm_result, xmm_key_last); \/\/ xmm15 always came from key+0\n-      __ pxor(xmm_result, xmm_prev_block_cipher); \/\/ xor with the current r vector\n-      __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); \/\/ store into the next 16 bytes of output\n-      \/\/ no need to store r to memory until we exit\n-      __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); \/\/ set up next r vector with cipher input from this block\n-      __ addptr(pos, AESBlockSize);\n-      __ subptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);\n-      if (k != 2) {\n-        __ jmp(L_exit);\n-      }\n-    } \/\/for 128\/192\/256\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xc0\n+      __ movdqu(xmm15, Address(rsp, 6 * wordSize));\n+      __ movdqu(xmm1, Address(rsp, 8 * wordSize));\n+      DoFour(aesdec, xmm15);  \/\/ key : 0xd0\n+      __ movdqu(xmm_key_last, Address(rsp, 0)); \/\/ xmm15 needs to be loaded again.\n+      DoFour(aesdec, xmm1);  \/\/ key : 0xe0\n+      __ movdqu(xmm_prev_block_cipher, Address(rvec, 0x00));  \/\/ xmm1 needs to be loaded again\n+      DoFour(aesdeclast, xmm_key_last);\n+    }\n+\n+    \/\/ for each result, xor with the r vector of previous cipher block\n+    __ pxor(xmm_result0, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n+    __ pxor(xmm_result1, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ pxor(xmm_result2, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ pxor(xmm_result3, xmm_prev_block_cipher);\n+    __ movdqu(xmm_prev_block_cipher, Address(from, pos, Address::times_1, 3 * AESBlockSize));   \/\/ this will carry over to next set of blocks\n+    if (k != 0) {\n+      __ movdqu(Address(rvec, 0x00), xmm_prev_block_cipher);\n+    }\n+\n+    __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);     \/\/ store 4 results into the next 64 bytes of output\n+    __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n+    __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n+    __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n+\n+    __ addptr(pos, PARALLEL_FACTOR * AESBlockSize);\n+    __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize);\n+    __ jmp(L_multiBlock_loopTop[k]);\n+\n+    \/\/ registers used in the non-parallelized loops\n+    \/\/ xmm register assignments for the loops below\n+    const XMMRegister xmm_result = xmm0;\n+    const XMMRegister xmm_prev_block_cipher_save = xmm2;\n+    const XMMRegister xmm_key11 = xmm3;\n+    const XMMRegister xmm_key12 = xmm4;\n+    const XMMRegister key_tmp = xmm4;\n+\n+    __ BIND(L_singleBlock_loopTopHead[k]);\n+    if (k == 1) {\n+      __ addptr(rsp, 6 * wordSize);\n+    } else if (k == 2) {\n+      __ addptr(rsp, 10 * wordSize);\n+    }\n+    __ cmpptr(len_reg, 0); \/\/ any blocks left??\n+    __ jcc(Assembler::equal, L_exit);\n+    __ BIND(L_singleBlock_loopTopHead2[k]);\n+    if (k == 1) {\n+      load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n+      load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n+    }\n+    if (k == 2) {\n+      load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n+    }\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_singleBlock_loopTop[k]);\n+    __ movdqu(xmm_result, Address(from, pos, Address::times_1, 0)); \/\/ get next 16 bytes of cipher input\n+    __ movdqa(xmm_prev_block_cipher_save, xmm_result); \/\/ save for next r vector\n+    __ pxor(xmm_result, xmm_key_first); \/\/ do the aes dec rounds\n+    for (int rnum = 1; rnum <= 9 ; rnum++) {\n+        __ aesdec(xmm_result, as_XMMRegister(rnum + XMM_REG_NUM_KEY_FIRST));\n+    }\n+    if (k == 1) {\n+      __ aesdec(xmm_result, xmm_key11);\n+      __ aesdec(xmm_result, xmm_key12);\n+    }\n+    if (k == 2) {\n+      __ aesdec(xmm_result, xmm_key11);\n+      load_key(key_tmp, key, 0xc0);\n+      __ aesdec(xmm_result, key_tmp);\n+      load_key(key_tmp, key, 0xd0);\n+      __ aesdec(xmm_result, key_tmp);\n+      load_key(key_tmp, key, 0xe0);\n+      __ aesdec(xmm_result, key_tmp);\n+    }\n+\n+    __ aesdeclast(xmm_result, xmm_key_last); \/\/ xmm15 always came from key+0\n+    __ pxor(xmm_result, xmm_prev_block_cipher); \/\/ xor with the current r vector\n+    __ movdqu(Address(to, pos, Address::times_1, 0), xmm_result); \/\/ store into the next 16 bytes of output\n+    \/\/ no need to store r to memory until we exit\n+    __ movdqa(xmm_prev_block_cipher, xmm_prev_block_cipher_save); \/\/ set up next r vector with cipher input from this block\n+    __ addptr(pos, AESBlockSize);\n+    __ subptr(len_reg, AESBlockSize);\n+    __ jcc(Assembler::notEqual, L_singleBlock_loopTop[k]);\n+    if (k != 2) {\n+      __ jmp(L_exit);\n+    }\n+  } \/\/for 128\/192\/256\n@@ -4205,3 +1907,3 @@\n-    __ BIND(L_exit);\n-    __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n-    __ pop(rbx);\n+  __ BIND(L_exit);\n+  __ movdqu(Address(rvec, 0), xmm_prev_block_cipher);     \/\/ final value of r stored in rvec of CipherBlockChaining object\n+  __ pop(rbx);\n@@ -4209,1 +1911,1 @@\n-    __ movl(rax, len_mem);\n+  __ movl(rax, len_mem);\n@@ -4211,1 +1913,1 @@\n-    __ pop(rax); \/\/ return length\n+  __ pop(rax); \/\/ return length\n@@ -4213,3 +1915,4 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n@@ -4218,14 +1921,16 @@\n-  address generate_electronicCodeBook_encryptAESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_encryptAESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ aesecb_encrypt(from, to, key, len);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+address StubGenerator::generate_electronicCodeBook_encryptAESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_encryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ aesecb_encrypt(from, to, key, len);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n@@ -4234,15 +1939,14 @@\n-  address generate_electronicCodeBook_decryptAESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_decryptAESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ aesecb_decrypt(from, to, key, len);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+address StubGenerator::generate_electronicCodeBook_decryptAESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"electronicCodeBook_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register len = c_rarg3;  \/\/ src len (must be multiple of blocksize 16)\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ aesecb_decrypt(from, to, key, len);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -4250,26 +1954,2 @@\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n-  address generate_md5_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    const Register buf_param = r15;\n-    const Address state_param(rsp, 0 * wordSize);\n-    const Address ofs_param  (rsp, 1 * wordSize    );\n-    const Address limit_param(rsp, 1 * wordSize + 4);\n-\n-    __ enter();\n-    __ push(rbx);\n-    __ push(rdi);\n-    __ push(rsi);\n-    __ push(r15);\n-    __ subptr(rsp, 2 * wordSize);\n-\n-    __ movptr(buf_param, c_rarg0);\n-    __ movptr(state_param, c_rarg1);\n-    if (multi_block) {\n-      __ movl(ofs_param, c_rarg2);\n-      __ movl(limit_param, c_rarg3);\n-    }\n-    __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n+  return start;\n+}\n@@ -4277,9 +1957,34 @@\n-    __ addptr(rsp, 2 * wordSize);\n-    __ pop(r15);\n-    __ pop(rsi);\n-    __ pop(rdi);\n-    __ pop(rbx);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n+address StubGenerator::generate_md5_implCompress(bool multi_block, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  const Register buf_param = r15;\n+  const Address state_param(rsp, 0 * wordSize);\n+  const Address ofs_param  (rsp, 1 * wordSize    );\n+  const Address limit_param(rsp, 1 * wordSize + 4);\n+\n+  __ enter();\n+  __ push(rbx);\n+  __ push(rdi);\n+  __ push(rsi);\n+  __ push(r15);\n+  __ subptr(rsp, 2 * wordSize);\n+\n+  __ movptr(buf_param, c_rarg0);\n+  __ movptr(state_param, c_rarg1);\n+  if (multi_block) {\n+    __ movl(ofs_param, c_rarg2);\n+    __ movl(limit_param, c_rarg3);\n+  }\n+  __ fast_md5(buf_param, state_param, ofs_param, limit_param, multi_block);\n+\n+  __ addptr(rsp, 2 * wordSize);\n+  __ pop(r15);\n+  __ pop(rsi);\n+  __ pop(rdi);\n+  __ pop(rbx);\n+  __ leave();\n+  __ ret(0);\n@@ -4287,8 +1992,2 @@\n-  address generate_upper_word_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n@@ -4296,8 +1995,7 @@\n-  address generate_shuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    return start;\n-  }\n+address StubGenerator::generate_upper_word_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"upper_word_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0xFFFFFFFF00000000, relocInfo::none);\n@@ -4305,6 +2003,2 @@\n-  \/\/ ofs and limit are use for multi-block byte array.\n-  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n-  address generate_sha1_implCompress(bool multi_block, const char *name) {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n+  return start;\n+}\n@@ -4312,4 +2006,4 @@\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n+address StubGenerator::generate_shuffle_byte_flip_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"shuffle_byte_flip_mask\");\n+  address start = __ pc();\n@@ -4317,4 +2011,5 @@\n-    const XMMRegister abcd = xmm0;\n-    const XMMRegister e0 = xmm1;\n-    const XMMRegister e1 = xmm2;\n-    const XMMRegister msg0 = xmm3;\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+\n+  return start;\n+}\n@@ -4322,4 +2017,6 @@\n-    const XMMRegister msg1 = xmm4;\n-    const XMMRegister msg2 = xmm5;\n-    const XMMRegister msg3 = xmm6;\n-    const XMMRegister shuf_mask = xmm7;\n+\/\/ ofs and limit are use for multi-block byte array.\n+\/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+address StubGenerator::generate_sha1_implCompress(bool multi_block, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -4327,1 +2024,4 @@\n-    __ enter();\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -4329,1 +2029,4 @@\n-    __ subptr(rsp, 4 * wordSize);\n+  const XMMRegister abcd = xmm0;\n+  const XMMRegister e0 = xmm1;\n+  const XMMRegister e1 = xmm2;\n+  const XMMRegister msg0 = xmm3;\n@@ -4331,2 +2034,4 @@\n-    __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n-      buf, state, ofs, limit, rsp, multi_block);\n+  const XMMRegister msg1 = xmm4;\n+  const XMMRegister msg2 = xmm5;\n+  const XMMRegister msg3 = xmm6;\n+  const XMMRegister shuf_mask = xmm7;\n@@ -4334,1 +2039,1 @@\n-    __ addptr(rsp, 4 * wordSize);\n+  __ enter();\n@@ -4336,4 +2041,1 @@\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  __ subptr(rsp, 4 * wordSize);\n@@ -4341,6 +2043,2 @@\n-  address generate_pshuffle_byte_flip_mask() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0405060700010203, relocInfo::none);\n-    __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n+  __ fast_sha1(abcd, e0, e1, msg0, msg1, msg2, msg3, shuf_mask,\n+    buf, state, ofs, limit, rsp, multi_block);\n@@ -4348,14 +2046,1 @@\n-    if (VM_Version::supports_avx2()) {\n-      __ emit_data64(0x0405060700010203, relocInfo::none); \/\/ second copy\n-      __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n-      \/\/ _SHUF_00BA\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      \/\/ _SHUF_DC00\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0x0b0a090803020100, relocInfo::none);\n-    }\n+  __ addptr(rsp, 4 * wordSize);\n@@ -4363,1 +2048,27 @@\n-    return start;\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_pshuffle_byte_flip_mask() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0405060700010203, relocInfo::none);\n+  __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n+\n+  if (VM_Version::supports_avx2()) {\n+    __ emit_data64(0x0405060700010203, relocInfo::none); \/\/ second copy\n+    __ emit_data64(0x0c0d0e0f08090a0b, relocInfo::none);\n+    \/\/ _SHUF_00BA\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    \/\/ _SHUF_DC00\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0x0b0a090803020100, relocInfo::none);\n@@ -4366,15 +2077,8 @@\n-  \/\/Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n-  address generate_pshuffle_byte_flip_mask_sha512() {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask_sha512\");\n-    address start = __ pc();\n-    if (VM_Version::supports_avx2()) {\n-      __ emit_data64(0x0001020304050607, relocInfo::none); \/\/ PSHUFFLE_BYTE_FLIP_MASK\n-      __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-      __ emit_data64(0x1011121314151617, relocInfo::none);\n-      __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);\n-      __ emit_data64(0x0000000000000000, relocInfo::none); \/\/MASK_YMM_LO\n-      __ emit_data64(0x0000000000000000, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-      __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n-    }\n+  return start;\n+}\n+\n+\/\/Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n+address StubGenerator::generate_pshuffle_byte_flip_mask_sha512() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pshuffle_byte_flip_mask_sha512\");\n+  address start = __ pc();\n@@ -4382,1 +2086,9 @@\n-    return start;\n+  if (VM_Version::supports_avx2()) {\n+    __ emit_data64(0x0001020304050607, relocInfo::none); \/\/ PSHUFFLE_BYTE_FLIP_MASK\n+    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+    __ emit_data64(0x1011121314151617, relocInfo::none);\n+    __ emit_data64(0x18191a1b1c1d1e1f, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none); \/\/MASK_YMM_LO\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n+    __ emit_data64(0xFFFFFFFFFFFFFFFF, relocInfo::none);\n@@ -4385,0 +2097,3 @@\n+  return start;\n+}\n+\n@@ -4387,40 +2102,5 @@\n-  address generate_sha256_implCompress(bool multi_block, const char *name) {\n-    assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), \"\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n-\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n-\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n-\n-    const XMMRegister shuf_mask = xmm8;\n-\n-    __ enter();\n-\n-    __ subptr(rsp, 4 * wordSize);\n-\n-    if (VM_Version::supports_sha()) {\n-      __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-        buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-    } else if (VM_Version::supports_avx2()) {\n-      __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-        buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n-    }\n-    __ addptr(rsp, 4 * wordSize);\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+address StubGenerator::generate_sha256_implCompress(bool multi_block, const char *name) {\n+  assert(VM_Version::supports_sha() || VM_Version::supports_avx2(), \"\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n@@ -4428,6 +2108,4 @@\n-  address generate_sha512_implCompress(bool multi_block, const char *name) {\n-    assert(VM_Version::supports_avx2(), \"\");\n-    assert(VM_Version::supports_bmi2(), \"\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-    address start = __ pc();\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n@@ -4435,4 +2113,4 @@\n-    Register buf = c_rarg0;\n-    Register state = c_rarg1;\n-    Register ofs = c_rarg2;\n-    Register limit = c_rarg3;\n+  const XMMRegister msg = xmm0;\n+  const XMMRegister state0 = xmm1;\n+  const XMMRegister state1 = xmm2;\n+  const XMMRegister msgtmp0 = xmm3;\n@@ -4440,8 +2118,4 @@\n-    const XMMRegister msg = xmm0;\n-    const XMMRegister state0 = xmm1;\n-    const XMMRegister state1 = xmm2;\n-    const XMMRegister msgtmp0 = xmm3;\n-    const XMMRegister msgtmp1 = xmm4;\n-    const XMMRegister msgtmp2 = xmm5;\n-    const XMMRegister msgtmp3 = xmm6;\n-    const XMMRegister msgtmp4 = xmm7;\n+  const XMMRegister msgtmp1 = xmm4;\n+  const XMMRegister msgtmp2 = xmm5;\n+  const XMMRegister msgtmp3 = xmm6;\n+  const XMMRegister msgtmp4 = xmm7;\n@@ -4449,1 +2123,1 @@\n-    const XMMRegister shuf_mask = xmm8;\n+  const XMMRegister shuf_mask = xmm8;\n@@ -4451,1 +2125,1 @@\n-    __ enter();\n+  __ enter();\n@@ -4453,2 +2127,1 @@\n-    __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n-    buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+  __ subptr(rsp, 4 * wordSize);\n@@ -4456,4 +2129,6 @@\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n-    return start;\n+  if (VM_Version::supports_sha()) {\n+    __ fast_sha256(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+  } else if (VM_Version::supports_avx2()) {\n+    __ sha256_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+      buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n@@ -4461,0 +2136,30 @@\n+  __ addptr(rsp, 4 * wordSize);\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::generate_sha512_implCompress(bool multi_block, const char *name) {\n+  assert(VM_Version::supports_avx2(), \"\");\n+  assert(VM_Version::supports_bmi2(), \"\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Register buf = c_rarg0;\n+  Register state = c_rarg1;\n+  Register ofs = c_rarg2;\n+  Register limit = c_rarg3;\n+\n+  const XMMRegister msg = xmm0;\n+  const XMMRegister state0 = xmm1;\n+  const XMMRegister state1 = xmm2;\n+  const XMMRegister msgtmp0 = xmm3;\n+  const XMMRegister msgtmp1 = xmm4;\n+  const XMMRegister msgtmp2 = xmm5;\n+  const XMMRegister msgtmp3 = xmm6;\n+  const XMMRegister msgtmp4 = xmm7;\n+\n+  const XMMRegister shuf_mask = xmm8;\n@@ -4462,17 +2167,31 @@\n-  address ghash_polynomial512_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x00000001C2000000, relocInfo::none);\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n-    __ emit_data64(0xC200000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n-    __ emit_data64(0x0000000100000000, relocInfo::none);\n-    return start;\n+  __ enter();\n+\n+  __ sha512_AVX2(msg, state0, state1, msgtmp0, msgtmp1, msgtmp2, msgtmp3, msgtmp4,\n+  buf, state, ofs, limit, rsp, multi_block, shuf_mask);\n+\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+address StubGenerator::ghash_polynomial512_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly512_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x00000001C2000000, relocInfo::none); \/\/ POLY for reduction\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x00000001C2000000, relocInfo::none);\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ POLY\n+  __ emit_data64(0xC200000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none); \/\/ TWOONE\n+  __ emit_data64(0x0000000100000000, relocInfo::none);\n+\n+  return start;\n@@ -4481,20 +2200,24 @@\n-  \/\/ Vector AES Galois Counter Mode implementation. Parameters:\n-  \/\/ Windows regs            |  Linux regs\n-  \/\/ in = c_rarg0 (rcx)      |  c_rarg0 (rsi)\n-  \/\/ len = c_rarg1 (rdx)     |  c_rarg1 (rdi)\n-  \/\/ ct = c_rarg2 (r8)       |  c_rarg2 (rdx)\n-  \/\/ out = c_rarg3 (r9)      |  c_rarg3 (rcx)\n-  \/\/ key = r10               |  c_rarg4 (r8)\n-  \/\/ state = r13             |  c_rarg5 (r9)\n-  \/\/ subkeyHtbl = r14        |  r11\n-  \/\/ counter = rsi           |  r12\n-  \/\/ return - number of processed bytes\n-  address generate_galoisCounterMode_AESCrypt() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register in = c_rarg0;\n-    const Register len = c_rarg1;\n-    const Register ct = c_rarg2;\n-    const Register out = c_rarg3;\n-    \/\/ and updated with the incremented counter in the end\n+\/\/ Vector AES Galois Counter Mode implementation.\n+\/\/\n+\/\/ Inputs:           Windows    |   Linux\n+\/\/   in         = rcx (c_rarg0) | rsi (c_rarg0)\n+\/\/   len        = rdx (c_rarg1) | rdi (c_rarg1)\n+\/\/   ct         = r8  (c_rarg2) | rdx (c_rarg2)\n+\/\/   out        = r9  (c_rarg3) | rcx (c_rarg3)\n+\/\/   key        = r10           | r8  (c_rarg4)\n+\/\/   state      = r13           | r9  (c_rarg5)\n+\/\/   subkeyHtbl = r14           | r11\n+\/\/   counter    = rsi           | r12\n+\/\/\n+\/\/ Output:\n+\/\/   rax - number of processed bytes\n+address StubGenerator::generate_galoisCounterMode_AESCrypt() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"galoisCounterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register in = c_rarg0;\n+  const Register len = c_rarg1;\n+  const Register ct = c_rarg2;\n+  const Register out = c_rarg3;\n+  \/\/ and updated with the incremented counter in the end\n@@ -4502,7 +2225,7 @@\n-    const Register key = c_rarg4;\n-    const Register state = c_rarg5;\n-    const Address subkeyH_mem(rbp, 2 * wordSize);\n-    const Register subkeyHtbl = r11;\n-    const Register avx512_subkeyHtbl = r13;\n-    const Address counter_mem(rbp, 3 * wordSize);\n-    const Register counter = r12;\n+  const Register key = c_rarg4;\n+  const Register state = c_rarg5;\n+  const Address subkeyH_mem(rbp, 2 * wordSize);\n+  const Register subkeyHtbl = r11;\n+  const Register avx512_subkeyHtbl = r13;\n+  const Address counter_mem(rbp, 3 * wordSize);\n+  const Register counter = r12;\n@@ -4510,9 +2233,9 @@\n-    const Address key_mem(rbp, 6 * wordSize);\n-    const Register key = r10;\n-    const Address state_mem(rbp, 7 * wordSize);\n-    const Register state = r13;\n-    const Address subkeyH_mem(rbp, 8 * wordSize);\n-    const Register subkeyHtbl = r14;\n-    const Register avx512_subkeyHtbl = r12;\n-    const Address counter_mem(rbp, 9 * wordSize);\n-    const Register counter = rsi;\n+  const Address key_mem(rbp, 6 * wordSize);\n+  const Register key = r10;\n+  const Address state_mem(rbp, 7 * wordSize);\n+  const Register state = r13;\n+  const Address subkeyH_mem(rbp, 8 * wordSize);\n+  const Register subkeyHtbl = r14;\n+  const Register avx512_subkeyHtbl = r12;\n+  const Address counter_mem(rbp, 9 * wordSize);\n+  const Register counter = rsi;\n@@ -4520,7 +2243,7 @@\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-    __ push(rbx);\n+  __ enter();\n+ \/\/ Save state before entering routine\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n@@ -4528,4 +2251,4 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ push(rsi);\n-    __ movptr(key, key_mem);\n-    __ movptr(state, state_mem);\n+  \/\/ on win64, fill len_reg from stack position\n+  __ push(rsi);\n+  __ movptr(key, key_mem);\n+  __ movptr(state, state_mem);\n@@ -4533,2 +2256,2 @@\n-    __ movptr(subkeyHtbl, subkeyH_mem);\n-    __ movptr(counter, counter_mem);\n+  __ movptr(subkeyHtbl, subkeyH_mem);\n+  __ movptr(counter, counter_mem);\n@@ -4536,2 +2259,2 @@\n-    __ push(rbp);\n-    __ movq(rbp, rsp);\n+  __ push(rbp);\n+  __ movq(rbp, rsp);\n@@ -4539,3 +2262,3 @@\n-    __ andq(rsp, -64);\n-    __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n-    __ movptr(avx512_subkeyHtbl, rsp);\n+  __ andq(rsp, -64);\n+  __ subptr(rsp, 96 * longSize); \/\/ Create space on the stack for htbl entries\n+  __ movptr(avx512_subkeyHtbl, rsp);\n@@ -4543,2 +2266,2 @@\n-    __ aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n-    __ vzeroupper();\n+  __ aesgcm_encrypt(in, len, ct, out, key, state, subkeyHtbl, avx512_subkeyHtbl, counter);\n+  __ vzeroupper();\n@@ -4546,2 +2269,2 @@\n-    __ movq(rsp, rbp);\n-    __ pop(rbp);\n+  __ movq(rsp, rbp);\n+  __ pop(rbp);\n@@ -4549,1 +2272,1 @@\n-    \/\/ Restore state before leaving routine\n+  \/\/ Restore state before leaving routine\n@@ -4551,1 +2274,1 @@\n-    __ pop(rsi);\n+  __ pop(rsi);\n@@ -4553,5 +2276,5 @@\n-    __ pop(rbx);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n@@ -4559,4 +2282,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-     return start;\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -4564,55 +2285,60 @@\n-  \/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n-  address counter_mask_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\/\/lbswapmask\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n-    __ emit_data64(0x0001020304050607, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\/\/linc0 = counter_mask_addr+64\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\/\/counter_mask_addr() + 80\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000002, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000003, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\/\/linc4 = counter_mask_addr() + 128\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000004, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\/\/linc8 = counter_mask_addr() + 192\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000008, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\/\/linc32 = counter_mask_addr() + 256\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000020, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\/\/linc16 = counter_mask_addr() + 320\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000010, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n+  return start;\n+}\n+\n+\/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n+address StubGenerator::counter_mask_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"counter_mask_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\/\/lbswapmask\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none);\n+  __ emit_data64(0x0001020304050607, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\/\/linc0 = counter_mask_addr+64\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\/\/counter_mask_addr() + 80\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000002, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000003, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\/\/linc4 = counter_mask_addr() + 128\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000004, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\/\/linc8 = counter_mask_addr() + 192\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000008, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\/\/linc32 = counter_mask_addr() + 256\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000020, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\/\/linc16 = counter_mask_addr() + 320\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000010, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+\n+  return start;\n+}\n@@ -4621,9 +2347,10 @@\n-  address generate_counterMode_VectorAESCrypt()  {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0; \/\/ source array address\n-    const Register to = c_rarg1; \/\/ destination array address\n-    const Register key = c_rarg2; \/\/ key array address r8\n-    const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-    \/\/ and updated with the incremented counter in the end\n+address StubGenerator::generate_counterMode_VectorAESCrypt()  {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0; \/\/ source array address\n+  const Register to = c_rarg1; \/\/ destination array address\n+  const Register key = c_rarg2; \/\/ key array address r8\n+  const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n+  \/\/ and updated with the incremented counter in the end\n@@ -4631,5 +2358,5 @@\n-    const Register len_reg = c_rarg4;\n-    const Register saved_encCounter_start = c_rarg5;\n-    const Register used_addr = r10;\n-    const Address  used_mem(rbp, 2 * wordSize);\n-    const Register used = r11;\n+  const Register len_reg = c_rarg4;\n+  const Register saved_encCounter_start = c_rarg5;\n+  const Register used_addr = r10;\n+  const Address  used_mem(rbp, 2 * wordSize);\n+  const Register used = r11;\n@@ -4637,7 +2364,7 @@\n-    const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ saved encrypted counter is on stack on Win64\n-    const Address used_mem(rbp, 8 * wordSize); \/\/ used length is on stack on Win64\n-    const Register len_reg = r10; \/\/ pick the first volatile windows register\n-    const Register saved_encCounter_start = r11;\n-    const Register used_addr = r13;\n-    const Register used = r14;\n+  const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ saved encrypted counter is on stack on Win64\n+  const Address used_mem(rbp, 8 * wordSize); \/\/ used length is on stack on Win64\n+  const Register len_reg = r10; \/\/ pick the first volatile windows register\n+  const Register saved_encCounter_start = r11;\n+  const Register used_addr = r13;\n+  const Register used = r14;\n@@ -4645,6 +2372,6 @@\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n+  __ enter();\n+ \/\/ Save state before entering routine\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n@@ -4652,5 +2379,5 @@\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-    __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+  __ movptr(saved_encCounter_start, saved_encCounter_mem);\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n@@ -4658,3 +2385,3 @@\n-    __ push(len_reg); \/\/ Save\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n+  __ push(len_reg); \/\/ Save\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n@@ -4662,5 +2389,5 @@\n-    __ push(rbx);\n-    __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);\n-    __ vzeroupper();\n-    \/\/ Restore state before leaving routine\n-    __ pop(rbx);\n+  __ push(rbx);\n+  __ aesctr_encrypt(from, to, key, counter, len_reg, used, used_addr, saved_encCounter_start);\n+  __ vzeroupper();\n+  \/\/ Restore state before leaving routine\n+  __ pop(rbx);\n@@ -4668,1 +2395,1 @@\n-    __ movl(rax, len_mem); \/\/ return length\n+  __ movl(rax, len_mem); \/\/ return length\n@@ -4670,1 +2397,1 @@\n-    __ pop(rax); \/\/ return length\n+  __ pop(rax); \/\/ return length\n@@ -4672,4 +2399,4 @@\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n@@ -4677,4 +2404,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -4682,32 +2407,36 @@\n-  \/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n-  \/\/ to hide instruction latency\n-  \/\/\n-  \/\/ Arguments:\n-  \/\/\n-  \/\/ Inputs:\n-  \/\/   c_rarg0   - source byte array address\n-  \/\/   c_rarg1   - destination byte array address\n-  \/\/   c_rarg2   - K (key) in little endian int array\n-  \/\/   c_rarg3   - counter vector byte array address\n-  \/\/   Linux\n-  \/\/     c_rarg4   -          input length\n-  \/\/     c_rarg5   -          saved encryptedCounter start\n-  \/\/     rbp + 6 * wordSize - saved used length\n-  \/\/   Windows\n-  \/\/     rbp + 6 * wordSize - input length\n-  \/\/     rbp + 7 * wordSize - saved encryptedCounter start\n-  \/\/     rbp + 8 * wordSize - saved used length\n-  \/\/\n-  \/\/ Output:\n-  \/\/   rax       - input length\n-  \/\/\n-  address generate_counterMode_AESCrypt_Parallel() {\n-    assert(UseAES, \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n-    address start = __ pc();\n-    const Register from = c_rarg0; \/\/ source array address\n-    const Register to = c_rarg1; \/\/ destination array address\n-    const Register key = c_rarg2; \/\/ key array address\n-    const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n-                                      \/\/ and updated with the incremented counter in the end\n+  return start;\n+}\n+\n+\/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n+\/\/ to hide instruction latency\n+\/\/\n+\/\/ Arguments:\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source byte array address\n+\/\/   c_rarg1   - destination byte array address\n+\/\/   c_rarg2   - K (key) in little endian int array\n+\/\/   c_rarg3   - counter vector byte array address\n+\/\/   Linux\n+\/\/     c_rarg4   -          input length\n+\/\/     c_rarg5   -          saved encryptedCounter start\n+\/\/     rbp + 6 * wordSize - saved used length\n+\/\/   Windows\n+\/\/     rbp + 6 * wordSize - input length\n+\/\/     rbp + 7 * wordSize - saved encryptedCounter start\n+\/\/     rbp + 8 * wordSize - saved used length\n+\/\/\n+\/\/ Output:\n+\/\/   rax       - input length\n+\/\/\n+address StubGenerator::generate_counterMode_AESCrypt_Parallel() {\n+  assert(UseAES, \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"counterMode_AESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0; \/\/ source array address\n+  const Register to = c_rarg1; \/\/ destination array address\n+  const Register key = c_rarg2; \/\/ key array address\n+  const Register counter = c_rarg3; \/\/ counter byte array initialized from counter array address\n+                                    \/\/ and updated with the incremented counter in the end\n@@ -4715,5 +2444,5 @@\n-    const Register len_reg = c_rarg4;\n-    const Register saved_encCounter_start = c_rarg5;\n-    const Register used_addr = r10;\n-    const Address  used_mem(rbp, 2 * wordSize);\n-    const Register used = r11;\n+  const Register len_reg = c_rarg4;\n+  const Register saved_encCounter_start = c_rarg5;\n+  const Register used_addr = r10;\n+  const Address  used_mem(rbp, 2 * wordSize);\n+  const Register used = r11;\n@@ -4721,7 +2450,7 @@\n-    const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ length is on stack on Win64\n-    const Address used_mem(rbp, 8 * wordSize); \/\/ length is on stack on Win64\n-    const Register len_reg = r10; \/\/ pick the first volatile windows register\n-    const Register saved_encCounter_start = r11;\n-    const Register used_addr = r13;\n-    const Register used = r14;\n+  const Address len_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address saved_encCounter_mem(rbp, 7 * wordSize); \/\/ length is on stack on Win64\n+  const Address used_mem(rbp, 8 * wordSize); \/\/ length is on stack on Win64\n+  const Register len_reg = r10; \/\/ pick the first volatile windows register\n+  const Register saved_encCounter_start = r11;\n+  const Register used_addr = r13;\n+  const Register used = r14;\n@@ -4729,38 +2458,38 @@\n-    const Register pos = rax;\n-\n-    const int PARALLEL_FACTOR = 6;\n-    const XMMRegister xmm_counter_shuf_mask = xmm0;\n-    const XMMRegister xmm_key_shuf_mask = xmm1; \/\/ used temporarily to swap key bytes up front\n-    const XMMRegister xmm_curr_counter = xmm2;\n-\n-    const XMMRegister xmm_key_tmp0 = xmm3;\n-    const XMMRegister xmm_key_tmp1 = xmm4;\n-\n-    \/\/ registers holding the four results in the parallelized loop\n-    const XMMRegister xmm_result0 = xmm5;\n-    const XMMRegister xmm_result1 = xmm6;\n-    const XMMRegister xmm_result2 = xmm7;\n-    const XMMRegister xmm_result3 = xmm8;\n-    const XMMRegister xmm_result4 = xmm9;\n-    const XMMRegister xmm_result5 = xmm10;\n-\n-    const XMMRegister xmm_from0 = xmm11;\n-    const XMMRegister xmm_from1 = xmm12;\n-    const XMMRegister xmm_from2 = xmm13;\n-    const XMMRegister xmm_from3 = xmm14; \/\/the last one is xmm14. we have to preserve it on WIN64.\n-    const XMMRegister xmm_from4 = xmm3; \/\/reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text\n-    const XMMRegister xmm_from5 = xmm4;\n-\n-    \/\/for key_128, key_192, key_256\n-    const int rounds[3] = {10, 12, 14};\n-    Label L_exit_preLoop, L_preLoop_start;\n-    Label L_multiBlock_loopTop[3];\n-    Label L_singleBlockLoopTop[3];\n-    Label L__incCounter[3][6]; \/\/for 6 blocks\n-    Label L__incCounter_single[3]; \/\/for single block, key128, key192, key256\n-    Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n-    Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n-\n-    Label L_exit;\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  const Register pos = rax;\n+\n+  const int PARALLEL_FACTOR = 6;\n+  const XMMRegister xmm_counter_shuf_mask = xmm0;\n+  const XMMRegister xmm_key_shuf_mask = xmm1; \/\/ used temporarily to swap key bytes up front\n+  const XMMRegister xmm_curr_counter = xmm2;\n+\n+  const XMMRegister xmm_key_tmp0 = xmm3;\n+  const XMMRegister xmm_key_tmp1 = xmm4;\n+\n+  \/\/ registers holding the four results in the parallelized loop\n+  const XMMRegister xmm_result0 = xmm5;\n+  const XMMRegister xmm_result1 = xmm6;\n+  const XMMRegister xmm_result2 = xmm7;\n+  const XMMRegister xmm_result3 = xmm8;\n+  const XMMRegister xmm_result4 = xmm9;\n+  const XMMRegister xmm_result5 = xmm10;\n+\n+  const XMMRegister xmm_from0 = xmm11;\n+  const XMMRegister xmm_from1 = xmm12;\n+  const XMMRegister xmm_from2 = xmm13;\n+  const XMMRegister xmm_from3 = xmm14; \/\/the last one is xmm14. we have to preserve it on WIN64.\n+  const XMMRegister xmm_from4 = xmm3; \/\/reuse xmm3~4. Because xmm_key_tmp0~1 are useless when loading input text\n+  const XMMRegister xmm_from5 = xmm4;\n+\n+  \/\/for key_128, key_192, key_256\n+  const int rounds[3] = {10, 12, 14};\n+  Label L_exit_preLoop, L_preLoop_start;\n+  Label L_multiBlock_loopTop[3];\n+  Label L_singleBlockLoopTop[3];\n+  Label L__incCounter[3][6]; \/\/for 6 blocks\n+  Label L__incCounter_single[3]; \/\/for single block, key128, key192, key256\n+  Label L_processTail_insr[3], L_processTail_4_insr[3], L_processTail_2_insr[3], L_processTail_1_insr[3], L_processTail_exit_insr[3];\n+  Label L_processTail_4_extr[3], L_processTail_2_extr[3], L_processTail_1_extr[3], L_processTail_exit_extr[3];\n+\n+  Label L_exit;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -4769,14 +2498,14 @@\n-    \/\/ allocate spill slots for r13, r14\n-    enum {\n-        saved_r13_offset,\n-        saved_r14_offset\n-    };\n-    __ subptr(rsp, 2 * wordSize);\n-    __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n-    __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n-\n-    \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n-    __ movptr(saved_encCounter_start, saved_encCounter_mem);\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n+  \/\/ allocate spill slots for r13, r14\n+  enum {\n+      saved_r13_offset,\n+      saved_r14_offset\n+  };\n+  __ subptr(rsp, 2 * wordSize);\n+  __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n+  __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n+\n+  \/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n+  __ movptr(saved_encCounter_start, saved_encCounter_mem);\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n@@ -4784,3 +2513,3 @@\n-    __ push(len_reg); \/\/ Save\n-    __ movptr(used_addr, used_mem);\n-    __ movl(used, Address(used_addr, 0));\n+  __ push(len_reg); \/\/ Save\n+  __ movptr(used_addr, used_mem);\n+  __ movl(used, Address(used_addr, 0));\n@@ -4789,31 +2518,31 @@\n-    __ push(rbx); \/\/ Save RBX\n-    __ movdqu(xmm_curr_counter, Address(counter, 0x00)); \/\/ initialize counter with initial counter\n-    __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); \/\/ pos as scratch\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled\n-    __ movptr(pos, 0);\n-\n-    \/\/ Use the partially used encrpyted counter from last invocation\n-    __ BIND(L_preLoop_start);\n-    __ cmpptr(used, 16);\n-    __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::lessEqual, L_exit_preLoop);\n-      __ movb(rbx, Address(saved_encCounter_start, used));\n-      __ xorb(rbx, Address(from, pos));\n-      __ movb(Address(to, pos), rbx);\n-      __ addptr(pos, 1);\n-      __ addptr(used, 1);\n-      __ subptr(len_reg, 1);\n-\n-    __ jmp(L_preLoop_start);\n-\n-    __ BIND(L_exit_preLoop);\n-    __ movl(Address(used_addr, 0), used);\n-\n-    \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); \/\/ rbx as scratch\n-    __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-    __ cmpl(rbx, 52);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n-    __ cmpl(rbx, 60);\n-    __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n+  __ push(rbx); \/\/ Save RBX\n+  __ movdqu(xmm_curr_counter, Address(counter, 0x00)); \/\/ initialize counter with initial counter\n+  __ movdqu(xmm_counter_shuf_mask, ExternalAddress(StubRoutines::x86::counter_shuffle_mask_addr()), pos); \/\/ pos as scratch\n+  __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled\n+  __ movptr(pos, 0);\n+\n+  \/\/ Use the partially used encrpyted counter from last invocation\n+  __ BIND(L_preLoop_start);\n+  __ cmpptr(used, 16);\n+  __ jcc(Assembler::aboveEqual, L_exit_preLoop);\n+    __ cmpptr(len_reg, 0);\n+    __ jcc(Assembler::lessEqual, L_exit_preLoop);\n+    __ movb(rbx, Address(saved_encCounter_start, used));\n+    __ xorb(rbx, Address(from, pos));\n+    __ movb(Address(to, pos), rbx);\n+    __ addptr(pos, 1);\n+    __ addptr(used, 1);\n+    __ subptr(len_reg, 1);\n+\n+  __ jmp(L_preLoop_start);\n+\n+  __ BIND(L_exit_preLoop);\n+  __ movl(Address(used_addr, 0), used);\n+\n+  \/\/ key length could be only {11, 13, 15} * 4 = {44, 52, 60}\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()), rbx); \/\/ rbx as scratch\n+  __ movl(rbx, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+  __ cmpl(rbx, 52);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTop[1]);\n+  __ cmpl(rbx, 60);\n+  __ jcc(Assembler::equal, L_multiBlock_loopTop[2]);\n@@ -4822,41 +2551,39 @@\n-    __ opc(xmm_result0, src_reg);              \\\n-    __ opc(xmm_result1, src_reg);              \\\n-    __ opc(xmm_result2, src_reg);              \\\n-    __ opc(xmm_result3, src_reg);              \\\n-    __ opc(xmm_result4, src_reg);              \\\n-    __ opc(xmm_result5, src_reg);\n-\n-    \/\/ k == 0 :  generate code for key_128\n-    \/\/ k == 1 :  generate code for key_192\n-    \/\/ k == 2 :  generate code for key_256\n-    for (int k = 0; k < 3; ++k) {\n-      \/\/multi blocks starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_multiBlock_loopTop[k]);\n-      __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n-      __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n-      load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-\n-      \/\/load, then increase counters\n-      CTR_DoSix(movdqa, xmm_curr_counter);\n-      inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);\n-      inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);\n-      inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);\n-      inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);\n-      inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);\n-      inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);\n-      CTR_DoSix(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n-      CTR_DoSix(pxor, xmm_key_tmp0);   \/\/PXOR with Round 0 key\n-\n-      \/\/load two ROUND_KEYs at a time\n-      for (int i = 1; i < rounds[k]; ) {\n-        load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);\n-        load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);\n-        CTR_DoSix(aesenc, xmm_key_tmp1);\n-        i++;\n-        if (i != rounds[k]) {\n-          CTR_DoSix(aesenc, xmm_key_tmp0);\n-        } else {\n-          CTR_DoSix(aesenclast, xmm_key_tmp0);\n-        }\n-        i++;\n+  __ opc(xmm_result0, src_reg);              \\\n+  __ opc(xmm_result1, src_reg);              \\\n+  __ opc(xmm_result2, src_reg);              \\\n+  __ opc(xmm_result3, src_reg);              \\\n+  __ opc(xmm_result4, src_reg);              \\\n+  __ opc(xmm_result5, src_reg);\n+\n+  \/\/ k == 0 :  generate code for key_128\n+  \/\/ k == 1 :  generate code for key_192\n+  \/\/ k == 2 :  generate code for key_256\n+  for (int k = 0; k < 3; ++k) {\n+    \/\/multi blocks starts here\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_multiBlock_loopTop[k]);\n+    __ cmpptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ see if at least PARALLEL_FACTOR blocks left\n+    __ jcc(Assembler::less, L_singleBlockLoopTop[k]);\n+    load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n+\n+    \/\/load, then increase counters\n+    CTR_DoSix(movdqa, xmm_curr_counter);\n+    inc_counter(rbx, xmm_result1, 0x01, L__incCounter[k][0]);\n+    inc_counter(rbx, xmm_result2, 0x02, L__incCounter[k][1]);\n+    inc_counter(rbx, xmm_result3, 0x03, L__incCounter[k][2]);\n+    inc_counter(rbx, xmm_result4, 0x04, L__incCounter[k][3]);\n+    inc_counter(rbx, xmm_result5,  0x05, L__incCounter[k][4]);\n+    inc_counter(rbx, xmm_curr_counter, 0x06, L__incCounter[k][5]);\n+    CTR_DoSix(pshufb, xmm_counter_shuf_mask); \/\/ after increased, shuffled counters back for PXOR\n+    CTR_DoSix(pxor, xmm_key_tmp0);   \/\/PXOR with Round 0 key\n+\n+    \/\/load two ROUND_KEYs at a time\n+    for (int i = 1; i < rounds[k]; ) {\n+      load_key(xmm_key_tmp1, key, (0x10 * i), xmm_key_shuf_mask);\n+      load_key(xmm_key_tmp0, key, (0x10 * (i+1)), xmm_key_shuf_mask);\n+      CTR_DoSix(aesenc, xmm_key_tmp1);\n+      i++;\n+      if (i != rounds[k]) {\n+        CTR_DoSix(aesenc, xmm_key_tmp0);\n+      } else {\n+        CTR_DoSix(aesenclast, xmm_key_tmp0);\n@@ -4864,2 +2591,48 @@\n-\n-      \/\/ get next PARALLEL_FACTOR blocks into xmm_result registers\n+      i++;\n+    }\n+\n+    \/\/ get next PARALLEL_FACTOR blocks into xmm_result registers\n+    __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n+    __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n+    __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n+    __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n+    __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));\n+    __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));\n+\n+    __ pxor(xmm_result0, xmm_from0);\n+    __ pxor(xmm_result1, xmm_from1);\n+    __ pxor(xmm_result2, xmm_from2);\n+    __ pxor(xmm_result3, xmm_from3);\n+    __ pxor(xmm_result4, xmm_from4);\n+    __ pxor(xmm_result5, xmm_from5);\n+\n+    \/\/ store 6 results into the next 64 bytes of output\n+    __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n+    __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n+    __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n+    __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n+    __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);\n+    __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);\n+\n+    __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n+    __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n+    __ jmp(L_multiBlock_loopTop[k]);\n+\n+    \/\/ singleBlock starts here\n+    __ align(OptoLoopAlignment);\n+    __ BIND(L_singleBlockLoopTop[k]);\n+    __ cmpptr(len_reg, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+    load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n+    __ movdqa(xmm_result0, xmm_curr_counter);\n+    inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);\n+    __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n+    __ pxor(xmm_result0, xmm_key_tmp0);\n+    for (int i = 1; i < rounds[k]; i++) {\n+      load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);\n+      __ aesenc(xmm_result0, xmm_key_tmp0);\n+    }\n+    load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);\n+    __ aesenclast(xmm_result0, xmm_key_tmp0);\n+    __ cmpptr(len_reg, AESBlockSize);\n+    __ jcc(Assembler::less, L_processTail_insr[k]);\n@@ -4867,103 +2640,58 @@\n-      __ movdqu(xmm_from1, Address(from, pos, Address::times_1, 1 * AESBlockSize));\n-      __ movdqu(xmm_from2, Address(from, pos, Address::times_1, 2 * AESBlockSize));\n-      __ movdqu(xmm_from3, Address(from, pos, Address::times_1, 3 * AESBlockSize));\n-      __ movdqu(xmm_from4, Address(from, pos, Address::times_1, 4 * AESBlockSize));\n-      __ movdqu(xmm_from5, Address(from, pos, Address::times_1, 5 * AESBlockSize));\n-\n-      __ pxor(xmm_result1, xmm_from1);\n-      __ pxor(xmm_result2, xmm_from2);\n-      __ pxor(xmm_result3, xmm_from3);\n-      __ pxor(xmm_result4, xmm_from4);\n-      __ pxor(xmm_result5, xmm_from5);\n-\n-      \/\/ store 6 results into the next 64 bytes of output\n-      __ movdqu(Address(to, pos, Address::times_1, 1 * AESBlockSize), xmm_result1);\n-      __ movdqu(Address(to, pos, Address::times_1, 2 * AESBlockSize), xmm_result2);\n-      __ movdqu(Address(to, pos, Address::times_1, 3 * AESBlockSize), xmm_result3);\n-      __ movdqu(Address(to, pos, Address::times_1, 4 * AESBlockSize), xmm_result4);\n-      __ movdqu(Address(to, pos, Address::times_1, 5 * AESBlockSize), xmm_result5);\n-\n-      __ addptr(pos, PARALLEL_FACTOR * AESBlockSize); \/\/ increase the length of crypt text\n-      __ subptr(len_reg, PARALLEL_FACTOR * AESBlockSize); \/\/ decrease the remaining length\n-      __ jmp(L_multiBlock_loopTop[k]);\n-\n-      \/\/ singleBlock starts here\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_singleBlockLoopTop[k]);\n-      __ cmpptr(len_reg, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-      load_key(xmm_key_tmp0, key, 0x00, xmm_key_shuf_mask);\n-      __ movdqa(xmm_result0, xmm_curr_counter);\n-      inc_counter(rbx, xmm_curr_counter, 0x01, L__incCounter_single[k]);\n-      __ pshufb(xmm_result0, xmm_counter_shuf_mask);\n-      __ pxor(xmm_result0, xmm_key_tmp0);\n-      for (int i = 1; i < rounds[k]; i++) {\n-        load_key(xmm_key_tmp0, key, (0x10 * i), xmm_key_shuf_mask);\n-        __ aesenc(xmm_result0, xmm_key_tmp0);\n-      }\n-      load_key(xmm_key_tmp0, key, (rounds[k] * 0x10), xmm_key_shuf_mask);\n-      __ aesenclast(xmm_result0, xmm_key_tmp0);\n-      __ cmpptr(len_reg, AESBlockSize);\n-      __ jcc(Assembler::less, L_processTail_insr[k]);\n-        __ movdqu(xmm_from0, Address(from, pos, Address::times_1, 0 * AESBlockSize));\n-        __ pxor(xmm_result0, xmm_from0);\n-        __ movdqu(Address(to, pos, Address::times_1, 0 * AESBlockSize), xmm_result0);\n-        __ addptr(pos, AESBlockSize);\n-        __ subptr(len_reg, AESBlockSize);\n-        __ jmp(L_singleBlockLoopTop[k]);\n-      __ BIND(L_processTail_insr[k]);                               \/\/ Process the tail part of the input array\n-        __ addptr(pos, len_reg);                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n-          __ subptr(pos,8);\n-          __ pinsrq(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_4_insr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n-          __ subptr(pos,4);\n-          __ pslldq(xmm_from0, 4);\n-          __ pinsrd(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_2_insr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n-          __ subptr(pos, 2);\n-          __ pslldq(xmm_from0, 2);\n-          __ pinsrw(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_1_insr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n-          __ subptr(pos, 1);\n-          __ pslldq(xmm_from0, 1);\n-          __ pinsrb(xmm_from0, Address(from, pos), 0);\n-        __ BIND(L_processTail_exit_insr[k]);\n-\n-        __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n-        __ pxor(xmm_result0, xmm_from0);                             \/\/    Also the encrypted counter is saved for next invocation.\n-\n-        __ testptr(len_reg, 8);\n-        __ jcc(Assembler::zero, L_processTail_4_extr[k]);            \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n-          __ pextrq(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 8);\n-          __ addptr(pos, 8);\n-        __ BIND(L_processTail_4_extr[k]);\n-        __ testptr(len_reg, 4);\n-        __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n-          __ pextrd(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 4);\n-          __ addptr(pos, 4);\n-        __ BIND(L_processTail_2_extr[k]);\n-        __ testptr(len_reg, 2);\n-        __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n-          __ pextrw(Address(to, pos), xmm_result0, 0);\n-          __ psrldq(xmm_result0, 2);\n-          __ addptr(pos, 2);\n-        __ BIND(L_processTail_1_extr[k]);\n-        __ testptr(len_reg, 1);\n-        __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n-          __ pextrb(Address(to, pos), xmm_result0, 0);\n-\n-        __ BIND(L_processTail_exit_extr[k]);\n-        __ movl(Address(used_addr, 0), len_reg);\n-        __ jmp(L_exit);\n-\n-    }\n+      __ addptr(pos, AESBlockSize);\n+      __ subptr(len_reg, AESBlockSize);\n+      __ jmp(L_singleBlockLoopTop[k]);\n+    __ BIND(L_processTail_insr[k]);                               \/\/ Process the tail part of the input array\n+      __ addptr(pos, len_reg);                                    \/\/ 1. Insert bytes from src array into xmm_from0 register\n+      __ testptr(len_reg, 8);\n+      __ jcc(Assembler::zero, L_processTail_4_insr[k]);\n+        __ subptr(pos,8);\n+        __ pinsrq(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_4_insr[k]);\n+      __ testptr(len_reg, 4);\n+      __ jcc(Assembler::zero, L_processTail_2_insr[k]);\n+        __ subptr(pos,4);\n+        __ pslldq(xmm_from0, 4);\n+        __ pinsrd(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_2_insr[k]);\n+      __ testptr(len_reg, 2);\n+      __ jcc(Assembler::zero, L_processTail_1_insr[k]);\n+        __ subptr(pos, 2);\n+        __ pslldq(xmm_from0, 2);\n+        __ pinsrw(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_1_insr[k]);\n+      __ testptr(len_reg, 1);\n+      __ jcc(Assembler::zero, L_processTail_exit_insr[k]);\n+        __ subptr(pos, 1);\n+        __ pslldq(xmm_from0, 1);\n+        __ pinsrb(xmm_from0, Address(from, pos), 0);\n+      __ BIND(L_processTail_exit_insr[k]);\n+\n+      __ movdqu(Address(saved_encCounter_start, 0), xmm_result0);  \/\/ 2. Perform pxor of the encrypted counter and plaintext Bytes.\n+      __ pxor(xmm_result0, xmm_from0);                             \/\/    Also the encrypted counter is saved for next invocation.\n+\n+      __ testptr(len_reg, 8);\n+      __ jcc(Assembler::zero, L_processTail_4_extr[k]);            \/\/ 3. Extract bytes from xmm_result0 into the dest. array\n+        __ pextrq(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 8);\n+        __ addptr(pos, 8);\n+      __ BIND(L_processTail_4_extr[k]);\n+      __ testptr(len_reg, 4);\n+      __ jcc(Assembler::zero, L_processTail_2_extr[k]);\n+        __ pextrd(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 4);\n+        __ addptr(pos, 4);\n+      __ BIND(L_processTail_2_extr[k]);\n+      __ testptr(len_reg, 2);\n+      __ jcc(Assembler::zero, L_processTail_1_extr[k]);\n+        __ pextrw(Address(to, pos), xmm_result0, 0);\n+        __ psrldq(xmm_result0, 2);\n+        __ addptr(pos, 2);\n+      __ BIND(L_processTail_1_extr[k]);\n+      __ testptr(len_reg, 1);\n+      __ jcc(Assembler::zero, L_processTail_exit_extr[k]);\n+        __ pextrb(Address(to, pos), xmm_result0, 0);\n+\n+      __ BIND(L_processTail_exit_extr[k]);\n+      __ movl(Address(used_addr, 0), len_reg);\n+      __ jmp(L_exit);\n+  }\n@@ -4973,4 +2701,4 @@\n-    __ BIND(L_exit);\n-    __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n-    __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n-    __ pop(rbx); \/\/ pop the saved RBX.\n+  __ BIND(L_exit);\n+  __ pshufb(xmm_curr_counter, xmm_counter_shuf_mask); \/\/counter is shuffled back.\n+  __ movdqu(Address(counter, 0), xmm_curr_counter); \/\/save counter back\n+  __ pop(rbx); \/\/ pop the saved RBX.\n@@ -4978,4 +2706,4 @@\n-    __ movl(rax, len_mem);\n-    __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n-    __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n-    __ addptr(rsp, 2 * wordSize);\n+  __ movl(rax, len_mem);\n+  __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n+  __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n+  __ addptr(rsp, 2 * wordSize);\n@@ -4983,1 +2711,1 @@\n-    __ pop(rax); \/\/ return 'len'\n+  __ pop(rax); \/\/ return 'len'\n@@ -4985,4 +2713,5 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n@@ -4990,1 +2719,1 @@\n-void roundDec(XMMRegister xmm_reg) {\n+void StubGenerator::roundDec(XMMRegister xmm_reg) {\n@@ -5001,1 +2730,1 @@\n-void roundDeclast(XMMRegister xmm_reg) {\n+void StubGenerator::roundDeclast(XMMRegister xmm_reg) {\n@@ -5012,1 +2741,1 @@\n-void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg) {\n+void StubGenerator::ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask) {\n@@ -5020,1 +2749,0 @@\n-\n@@ -5023,11 +2751,11 @@\n-address generate_cipherBlockChaining_decryptVectorAESCrypt() {\n-    assert(VM_Version::supports_avx512_vaes(), \"need AES instructions and misaligned SSE support\");\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n-    address start = __ pc();\n-\n-    const Register from = c_rarg0;  \/\/ source array address\n-    const Register to = c_rarg1;  \/\/ destination array address\n-    const Register key = c_rarg2;  \/\/ key array address\n-    const Register rvec = c_rarg3;  \/\/ r byte array initialized from initvector array address\n-    \/\/ and left with the results of the last encryption block\n+address StubGenerator::generate_cipherBlockChaining_decryptVectorAESCrypt() {\n+  assert(VM_Version::supports_avx512_vaes(), \"need AES instructions and misaligned SSE support\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"cipherBlockChaining_decryptAESCrypt\");\n+  address start = __ pc();\n+\n+  const Register from = c_rarg0;  \/\/ source array address\n+  const Register to = c_rarg1;  \/\/ destination array address\n+  const Register key = c_rarg2;  \/\/ key array address\n+  const Register rvec = c_rarg3;  \/\/ r byte array initialized from initvector array address\n+  \/\/ and left with the results of the last encryption block\n@@ -5035,1 +2763,1 @@\n-    const Register len_reg = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n+  const Register len_reg = c_rarg4;  \/\/ src len (must be multiple of blocksize 16)\n@@ -5037,2 +2765,2 @@\n-    const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Register len_reg = r11;      \/\/ pick the volatile windows register\n+  const Address  len_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Register len_reg = r11;      \/\/ pick the volatile windows register\n@@ -5041,2 +2769,2 @@\n-    Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,\n-          Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;\n+  Label Loop, Loop1, L_128, L_256, L_192, KEY_192, KEY_256, Loop2, Lcbc_dec_rem_loop,\n+        Lcbc_dec_rem_last, Lcbc_dec_ret, Lcbc_dec_rem, Lcbc_exit;\n@@ -5044,1 +2772,1 @@\n-    __ enter();\n+  __ enter();\n@@ -5047,2 +2775,2 @@\n-  \/\/ on win64, fill len_reg from stack position\n-    __ movl(len_reg, len_mem);\n+\/\/ on win64, fill len_reg from stack position\n+  __ movl(len_reg, len_mem);\n@@ -5050,1 +2778,1 @@\n-    __ push(len_reg); \/\/ Save\n+  __ push(len_reg); \/\/ Save\n@@ -5052,235 +2780,235 @@\n-    __ push(rbx);\n-    __ vzeroupper();\n-\n-    \/\/ Temporary variable declaration for swapping key bytes\n-    const XMMRegister xmm_key_shuf_mask = xmm1;\n-    __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n-\n-    \/\/ Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n-    const Register rounds = rbx;\n-    __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n-\n-    const XMMRegister IV = xmm0;\n-    \/\/ Load IV and broadcast value to 512-bits\n-    __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);\n-\n-    \/\/ Temporary variables for storing round keys\n-    const XMMRegister RK0 = xmm30;\n-    const XMMRegister RK1 = xmm9;\n-    const XMMRegister RK2 = xmm18;\n-    const XMMRegister RK3 = xmm19;\n-    const XMMRegister RK4 = xmm20;\n-    const XMMRegister RK5 = xmm21;\n-    const XMMRegister RK6 = xmm22;\n-    const XMMRegister RK7 = xmm23;\n-    const XMMRegister RK8 = xmm24;\n-    const XMMRegister RK9 = xmm25;\n-    const XMMRegister RK10 = xmm26;\n-\n-     \/\/ Load and shuffle key\n-    \/\/ the java expanded key ordering is rotated one position from what we want\n-    \/\/ so we start from 1*16 here and hit 0*16 last\n-    ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);\n-    ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);\n-\n-    \/\/ Variables for storing source cipher text\n-    const XMMRegister S0 = xmm10;\n-    const XMMRegister S1 = xmm11;\n-    const XMMRegister S2 = xmm12;\n-    const XMMRegister S3 = xmm13;\n-    const XMMRegister S4 = xmm14;\n-    const XMMRegister S5 = xmm15;\n-    const XMMRegister S6 = xmm16;\n-    const XMMRegister S7 = xmm17;\n-\n-    \/\/ Variables for storing decrypted text\n-    const XMMRegister B0 = xmm1;\n-    const XMMRegister B1 = xmm2;\n-    const XMMRegister B2 = xmm3;\n-    const XMMRegister B3 = xmm4;\n-    const XMMRegister B4 = xmm5;\n-    const XMMRegister B5 = xmm6;\n-    const XMMRegister B6 = xmm7;\n-    const XMMRegister B7 = xmm8;\n-\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::greater, KEY_192);\n-    __ jmp(Loop);\n-\n-    __ BIND(KEY_192);\n-    const XMMRegister RK11 = xmm27;\n-    const XMMRegister RK12 = xmm28;\n-    ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);\n-    ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);\n-\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::greater, KEY_256);\n-    __ jmp(Loop);\n-\n-    __ BIND(KEY_256);\n-    const XMMRegister RK13 = xmm29;\n-    const XMMRegister RK14 = xmm31;\n-    ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);\n-    ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);\n-\n-    __ BIND(Loop);\n-    __ cmpl(len_reg, 512);\n-    __ jcc(Assembler::below, Lcbc_dec_rem);\n-    __ BIND(Loop1);\n-    __ subl(len_reg, 512);\n-    __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);\n-    __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);\n-    __ leaq(from, Address(from, 8 * 64));\n-\n-    __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);\n-\n-    __ evalignq(IV, S0, IV, 0x06);\n-    __ evalignq(S0, S1, S0, 0x06);\n-    __ evalignq(S1, S2, S1, 0x06);\n-    __ evalignq(S2, S3, S2, 0x06);\n-    __ evalignq(S3, S4, S3, 0x06);\n-    __ evalignq(S4, S5, S4, 0x06);\n-    __ evalignq(S5, S6, S5, 0x06);\n-    __ evalignq(S6, S7, S6, 0x06);\n-\n-    roundDec(RK2);\n-    roundDec(RK3);\n-    roundDec(RK4);\n-    roundDec(RK5);\n-    roundDec(RK6);\n-    roundDec(RK7);\n-    roundDec(RK8);\n-    roundDec(RK9);\n-    roundDec(RK10);\n-\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, L_128);\n-    roundDec(RK11);\n-    roundDec(RK12);\n-\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, L_192);\n-    roundDec(RK13);\n-    roundDec(RK14);\n-\n-    __ BIND(L_256);\n-    roundDeclast(RK0);\n-    __ jmp(Loop2);\n-\n-    __ BIND(L_128);\n-    roundDeclast(RK0);\n-    __ jmp(Loop2);\n-\n-    __ BIND(L_192);\n-    roundDeclast(RK0);\n-\n-    __ BIND(Loop2);\n-    __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-    __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);\n-    __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);\n-    __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);\n-    __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);\n-    __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);\n-    __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);\n-    __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);\n-    __ evmovdquq(IV, S7, Assembler::AVX_512bit);\n-\n-    __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);\n-    __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);\n-    __ leaq(to, Address(to, 8 * 64));\n-    __ jmp(Loop);\n-\n-    __ BIND(Lcbc_dec_rem);\n-    __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_dec_rem_loop);\n-    __ subl(len_reg, 16);\n-    __ jcc(Assembler::carrySet, Lcbc_dec_ret);\n-\n-    __ movdqu(S0, Address(from, 0));\n-    __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-    __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n-\n-    __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);\n-    __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_dec_rem_last);\n-    __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);\n-\n-    __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n-    __ evmovdquq(IV, S0, Assembler::AVX_512bit);\n-    __ movdqu(Address(to, 0), B0);\n-    __ leaq(from, Address(from, 16));\n-    __ leaq(to, Address(to, 16));\n-    __ jmp(Lcbc_dec_rem_loop);\n-\n-    __ BIND(Lcbc_dec_ret);\n-    __ movdqu(Address(rvec, 0), IV);\n-\n-    \/\/ Zero out the round keys\n-    __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);\n-    __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);\n-    __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);\n-    __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);\n-    __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);\n-    __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);\n-    __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);\n-    __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);\n-    __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);\n-    __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);\n-    __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 44);\n-    __ jcc(Assembler::belowEqual, Lcbc_exit);\n-    __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);\n-    __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);\n-    __ cmpl(rounds, 52);\n-    __ jcc(Assembler::belowEqual, Lcbc_exit);\n-    __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);\n-    __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);\n-\n-    __ BIND(Lcbc_exit);\n-    __ vzeroupper();\n-    __ pop(rbx);\n+  __ push(rbx);\n+  __ vzeroupper();\n+\n+  \/\/ Temporary variable declaration for swapping key bytes\n+  const XMMRegister xmm_key_shuf_mask = xmm1;\n+  __ movdqu(xmm_key_shuf_mask, ExternalAddress(StubRoutines::x86::key_shuffle_mask_addr()));\n+\n+  \/\/ Calculate number of rounds from key size: 44 for 10-rounds, 52 for 12-rounds, 60 for 14-rounds\n+  const Register rounds = rbx;\n+  __ movl(rounds, Address(key, arrayOopDesc::length_offset_in_bytes() - arrayOopDesc::base_offset_in_bytes(T_INT)));\n+\n+  const XMMRegister IV = xmm0;\n+  \/\/ Load IV and broadcast value to 512-bits\n+  __ evbroadcasti64x2(IV, Address(rvec, 0), Assembler::AVX_512bit);\n+\n+  \/\/ Temporary variables for storing round keys\n+  const XMMRegister RK0 = xmm30;\n+  const XMMRegister RK1 = xmm9;\n+  const XMMRegister RK2 = xmm18;\n+  const XMMRegister RK3 = xmm19;\n+  const XMMRegister RK4 = xmm20;\n+  const XMMRegister RK5 = xmm21;\n+  const XMMRegister RK6 = xmm22;\n+  const XMMRegister RK7 = xmm23;\n+  const XMMRegister RK8 = xmm24;\n+  const XMMRegister RK9 = xmm25;\n+  const XMMRegister RK10 = xmm26;\n+\n+  \/\/ Load and shuffle key\n+  \/\/ the java expanded key ordering is rotated one position from what we want\n+  \/\/ so we start from 1*16 here and hit 0*16 last\n+  ev_load_key(RK1, key, 1 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK2, key, 2 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK3, key, 3 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK4, key, 4 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK5, key, 5 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK6, key, 6 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK7, key, 7 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK8, key, 8 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK9, key, 9 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK10, key, 10 * 16, xmm_key_shuf_mask);\n+  ev_load_key(RK0, key, 0*16, xmm_key_shuf_mask);\n+\n+  \/\/ Variables for storing source cipher text\n+  const XMMRegister S0 = xmm10;\n+  const XMMRegister S1 = xmm11;\n+  const XMMRegister S2 = xmm12;\n+  const XMMRegister S3 = xmm13;\n+  const XMMRegister S4 = xmm14;\n+  const XMMRegister S5 = xmm15;\n+  const XMMRegister S6 = xmm16;\n+  const XMMRegister S7 = xmm17;\n+\n+  \/\/ Variables for storing decrypted text\n+  const XMMRegister B0 = xmm1;\n+  const XMMRegister B1 = xmm2;\n+  const XMMRegister B2 = xmm3;\n+  const XMMRegister B3 = xmm4;\n+  const XMMRegister B4 = xmm5;\n+  const XMMRegister B5 = xmm6;\n+  const XMMRegister B6 = xmm7;\n+  const XMMRegister B7 = xmm8;\n+\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::greater, KEY_192);\n+  __ jmp(Loop);\n+\n+  __ BIND(KEY_192);\n+  const XMMRegister RK11 = xmm27;\n+  const XMMRegister RK12 = xmm28;\n+  ev_load_key(RK11, key, 11*16, xmm_key_shuf_mask);\n+  ev_load_key(RK12, key, 12*16, xmm_key_shuf_mask);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::greater, KEY_256);\n+  __ jmp(Loop);\n+\n+  __ BIND(KEY_256);\n+  const XMMRegister RK13 = xmm29;\n+  const XMMRegister RK14 = xmm31;\n+  ev_load_key(RK13, key, 13*16, xmm_key_shuf_mask);\n+  ev_load_key(RK14, key, 14*16, xmm_key_shuf_mask);\n+\n+  __ BIND(Loop);\n+  __ cmpl(len_reg, 512);\n+  __ jcc(Assembler::below, Lcbc_dec_rem);\n+  __ BIND(Loop1);\n+  __ subl(len_reg, 512);\n+  __ evmovdquq(S0, Address(from, 0 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S1, Address(from, 1 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S2, Address(from, 2 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S3, Address(from, 3 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S4, Address(from, 4 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S5, Address(from, 5 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S6, Address(from, 6 * 64), Assembler::AVX_512bit);\n+  __ evmovdquq(S7, Address(from, 7 * 64), Assembler::AVX_512bit);\n+  __ leaq(from, Address(from, 8 * 64));\n+\n+  __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B1, S1, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B2, S2, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B3, S3, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B4, S4, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B5, S5, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B6, S6, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(B7, S7, RK1, Assembler::AVX_512bit);\n+\n+  __ evalignq(IV, S0, IV, 0x06);\n+  __ evalignq(S0, S1, S0, 0x06);\n+  __ evalignq(S1, S2, S1, 0x06);\n+  __ evalignq(S2, S3, S2, 0x06);\n+  __ evalignq(S3, S4, S3, 0x06);\n+  __ evalignq(S4, S5, S4, 0x06);\n+  __ evalignq(S5, S6, S5, 0x06);\n+  __ evalignq(S6, S7, S6, 0x06);\n+\n+  roundDec(RK2);\n+  roundDec(RK3);\n+  roundDec(RK4);\n+  roundDec(RK5);\n+  roundDec(RK6);\n+  roundDec(RK7);\n+  roundDec(RK8);\n+  roundDec(RK9);\n+  roundDec(RK10);\n+\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, L_128);\n+  roundDec(RK11);\n+  roundDec(RK12);\n+\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, L_192);\n+  roundDec(RK13);\n+  roundDec(RK14);\n+\n+  __ BIND(L_256);\n+  roundDeclast(RK0);\n+  __ jmp(Loop2);\n+\n+  __ BIND(L_128);\n+  roundDeclast(RK0);\n+  __ jmp(Loop2);\n+\n+  __ BIND(L_192);\n+  roundDeclast(RK0);\n+\n+  __ BIND(Loop2);\n+  __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n+  __ evpxorq(B1, B1, S0, Assembler::AVX_512bit);\n+  __ evpxorq(B2, B2, S1, Assembler::AVX_512bit);\n+  __ evpxorq(B3, B3, S2, Assembler::AVX_512bit);\n+  __ evpxorq(B4, B4, S3, Assembler::AVX_512bit);\n+  __ evpxorq(B5, B5, S4, Assembler::AVX_512bit);\n+  __ evpxorq(B6, B6, S5, Assembler::AVX_512bit);\n+  __ evpxorq(B7, B7, S6, Assembler::AVX_512bit);\n+  __ evmovdquq(IV, S7, Assembler::AVX_512bit);\n+\n+  __ evmovdquq(Address(to, 0 * 64), B0, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 1 * 64), B1, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 2 * 64), B2, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 3 * 64), B3, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 4 * 64), B4, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 5 * 64), B5, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 6 * 64), B6, Assembler::AVX_512bit);\n+  __ evmovdquq(Address(to, 7 * 64), B7, Assembler::AVX_512bit);\n+  __ leaq(to, Address(to, 8 * 64));\n+  __ jmp(Loop);\n+\n+  __ BIND(Lcbc_dec_rem);\n+  __ evshufi64x2(IV, IV, IV, 0x03, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_dec_rem_loop);\n+  __ subl(len_reg, 16);\n+  __ jcc(Assembler::carrySet, Lcbc_dec_ret);\n+\n+  __ movdqu(S0, Address(from, 0));\n+  __ evpxorq(B0, S0, RK1, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK2, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK3, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK4, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK5, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK6, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK7, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK8, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK9, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK10, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n+\n+  __ vaesdec(B0, B0, RK11, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK12, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, Lcbc_dec_rem_last);\n+\n+  __ vaesdec(B0, B0, RK13, Assembler::AVX_512bit);\n+  __ vaesdec(B0, B0, RK14, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_dec_rem_last);\n+  __ vaesdeclast(B0, B0, RK0, Assembler::AVX_512bit);\n+\n+  __ evpxorq(B0, B0, IV, Assembler::AVX_512bit);\n+  __ evmovdquq(IV, S0, Assembler::AVX_512bit);\n+  __ movdqu(Address(to, 0), B0);\n+  __ leaq(from, Address(from, 16));\n+  __ leaq(to, Address(to, 16));\n+  __ jmp(Lcbc_dec_rem_loop);\n+\n+  __ BIND(Lcbc_dec_ret);\n+  __ movdqu(Address(rvec, 0), IV);\n+\n+  \/\/ Zero out the round keys\n+  __ evpxorq(RK0, RK0, RK0, Assembler::AVX_512bit);\n+  __ evpxorq(RK1, RK1, RK1, Assembler::AVX_512bit);\n+  __ evpxorq(RK2, RK2, RK2, Assembler::AVX_512bit);\n+  __ evpxorq(RK3, RK3, RK3, Assembler::AVX_512bit);\n+  __ evpxorq(RK4, RK4, RK4, Assembler::AVX_512bit);\n+  __ evpxorq(RK5, RK5, RK5, Assembler::AVX_512bit);\n+  __ evpxorq(RK6, RK6, RK6, Assembler::AVX_512bit);\n+  __ evpxorq(RK7, RK7, RK7, Assembler::AVX_512bit);\n+  __ evpxorq(RK8, RK8, RK8, Assembler::AVX_512bit);\n+  __ evpxorq(RK9, RK9, RK9, Assembler::AVX_512bit);\n+  __ evpxorq(RK10, RK10, RK10, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 44);\n+  __ jcc(Assembler::belowEqual, Lcbc_exit);\n+  __ evpxorq(RK11, RK11, RK11, Assembler::AVX_512bit);\n+  __ evpxorq(RK12, RK12, RK12, Assembler::AVX_512bit);\n+  __ cmpl(rounds, 52);\n+  __ jcc(Assembler::belowEqual, Lcbc_exit);\n+  __ evpxorq(RK13, RK13, RK13, Assembler::AVX_512bit);\n+  __ evpxorq(RK14, RK14, RK14, Assembler::AVX_512bit);\n+\n+  __ BIND(Lcbc_exit);\n+  __ vzeroupper();\n+  __ pop(rbx);\n@@ -5288,1 +3016,1 @@\n-    __ movl(rax, len_mem);\n+  __ movl(rax, len_mem);\n@@ -5290,1 +3018,1 @@\n-    __ pop(rax); \/\/ return length\n+  __ pop(rax); \/\/ return length\n@@ -5292,3 +3020,4 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n@@ -5298,7 +3027,9 @@\n-address ghash_polynomial_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x0000000000000001, relocInfo::none);\n-    __ emit_data64(0xc200000000000000, relocInfo::none);\n-    return start;\n+address StubGenerator::ghash_polynomial_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_poly_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0000000000000001, relocInfo::none);\n+  __ emit_data64(0xc200000000000000, relocInfo::none);\n+\n+  return start;\n@@ -5307,7 +3038,9 @@\n-address ghash_shufflemask_addr() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"_ghash_shuffmask_addr\");\n-    address start = __ pc();\n-    __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-    __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n-    return start;\n+address StubGenerator::ghash_shufflemask_addr() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"_ghash_shuffmask_addr\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n+  __ emit_data64(0x0f0f0f0f0f0f0f0f, relocInfo::none);\n+\n+  return start;\n@@ -5317,18 +3050,16 @@\n-address generate_avx_ghash_processBlocks() {\n-    __ align(CodeEntryAlignment);\n-\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n-\n-    \/\/ arguments\n-    const Register state = c_rarg0;\n-    const Register htbl = c_rarg1;\n-    const Register data = c_rarg2;\n-    const Register blocks = c_rarg3;\n-    __ enter();\n-   \/\/ Save state before entering routine\n-    __ avx_ghash(state, htbl, data, blocks);\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-}\n+address StubGenerator::generate_avx_ghash_processBlocks() {\n+  __ align(CodeEntryAlignment);\n+\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n+  address start = __ pc();\n+\n+  \/\/ arguments\n+  const Register state = c_rarg0;\n+  const Register htbl = c_rarg1;\n+  const Register data = c_rarg2;\n+  const Register blocks = c_rarg3;\n+  __ enter();\n+ \/\/ Save state before entering routine\n+  __ avx_ghash(state, htbl, data, blocks);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -5336,8 +3067,1 @@\n-  \/\/ byte swap x86 long\n-  address generate_ghash_long_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );\n-    __ emit_data64(0x0706050403020100, relocInfo::none );\n-  }\n+}\n@@ -5346,9 +3070,5 @@\n-  \/\/ byte swap x86 byte array\n-  address generate_ghash_byte_swap_mask() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n-    address start = __ pc();\n-    __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );\n-    __ emit_data64(0x0001020304050607, relocInfo::none );\n-  return start;\n-  }\n+\/\/ byte swap x86 long\n+address StubGenerator::generate_ghash_long_swap_mask() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_long_swap_mask\");\n+  address start = __ pc();\n@@ -5356,6 +3076,2 @@\n-  \/* Single and multi-block ghash operations *\/\n-  address generate_ghash_processBlocks() {\n-    __ align(CodeEntryAlignment);\n-    Label L_ghash_loop, L_exit;\n-    StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n-    address start = __ pc();\n+  __ emit_data64(0x0f0e0d0c0b0a0908, relocInfo::none );\n+  __ emit_data64(0x0706050403020100, relocInfo::none );\n@@ -5363,4 +3079,2 @@\n-    const Register state        = c_rarg0;\n-    const Register subkeyH      = c_rarg1;\n-    const Register data         = c_rarg2;\n-    const Register blocks       = c_rarg3;\n+return start;\n+}\n@@ -5368,11 +3082,5 @@\n-    const XMMRegister xmm_temp0 = xmm0;\n-    const XMMRegister xmm_temp1 = xmm1;\n-    const XMMRegister xmm_temp2 = xmm2;\n-    const XMMRegister xmm_temp3 = xmm3;\n-    const XMMRegister xmm_temp4 = xmm4;\n-    const XMMRegister xmm_temp5 = xmm5;\n-    const XMMRegister xmm_temp6 = xmm6;\n-    const XMMRegister xmm_temp7 = xmm7;\n-    const XMMRegister xmm_temp8 = xmm8;\n-    const XMMRegister xmm_temp9 = xmm9;\n-    const XMMRegister xmm_temp10 = xmm10;\n+\/\/ byte swap x86 byte array\n+address StubGenerator::generate_ghash_byte_swap_mask() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_byte_swap_mask\");\n+  address start = __ pc();\n@@ -5380,1 +3088,2 @@\n-    __ enter();\n+  __ emit_data64(0x08090a0b0c0d0e0f, relocInfo::none );\n+  __ emit_data64(0x0001020304050607, relocInfo::none );\n@@ -5382,1 +3091,2 @@\n-    __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n+return start;\n+}\n@@ -5384,2 +3094,6 @@\n-    __ movdqu(xmm_temp0, Address(state, 0));\n-    __ pshufb(xmm_temp0, xmm_temp10);\n+\/* Single and multi-block ghash operations *\/\n+address StubGenerator::generate_ghash_processBlocks() {\n+  __ align(CodeEntryAlignment);\n+  Label L_ghash_loop, L_exit;\n+  StubCodeMark mark(this, \"StubRoutines\", \"ghash_processBlocks\");\n+  address start = __ pc();\n@@ -5387,0 +3101,4 @@\n+  const Register state        = c_rarg0;\n+  const Register subkeyH      = c_rarg1;\n+  const Register data         = c_rarg2;\n+  const Register blocks       = c_rarg3;\n@@ -5388,3 +3106,11 @@\n-    __ BIND(L_ghash_loop);\n-    __ movdqu(xmm_temp2, Address(data, 0));\n-    __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n+  const XMMRegister xmm_temp0 = xmm0;\n+  const XMMRegister xmm_temp1 = xmm1;\n+  const XMMRegister xmm_temp2 = xmm2;\n+  const XMMRegister xmm_temp3 = xmm3;\n+  const XMMRegister xmm_temp4 = xmm4;\n+  const XMMRegister xmm_temp5 = xmm5;\n+  const XMMRegister xmm_temp6 = xmm6;\n+  const XMMRegister xmm_temp7 = xmm7;\n+  const XMMRegister xmm_temp8 = xmm8;\n+  const XMMRegister xmm_temp9 = xmm9;\n+  const XMMRegister xmm_temp10 = xmm10;\n@@ -5392,2 +3118,1 @@\n-    __ movdqu(xmm_temp1, Address(subkeyH, 0));\n-    __ pshufb(xmm_temp1, xmm_temp10);\n+  __ enter();\n@@ -5395,1 +3120,1 @@\n-    __ pxor(xmm_temp0, xmm_temp2);\n+  __ movdqu(xmm_temp10, ExternalAddress(StubRoutines::x86::ghash_long_swap_mask_addr()));\n@@ -5397,38 +3122,2 @@\n-    \/\/\n-    \/\/ Multiply with the hash key\n-    \/\/\n-    __ movdqu(xmm_temp3, xmm_temp0);\n-    __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n-    __ movdqu(xmm_temp4, xmm_temp0);\n-    __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n-\n-    __ movdqu(xmm_temp5, xmm_temp0);\n-    __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n-    __ movdqu(xmm_temp6, xmm_temp0);\n-    __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n-\n-    __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n-\n-    __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n-    __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n-    __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n-    __ pxor(xmm_temp3, xmm_temp5);\n-    __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n-                                        \/\/ of the carry-less multiplication of\n-                                        \/\/ xmm0 by xmm1.\n-\n-    \/\/ We shift the result of the multiplication by one bit position\n-    \/\/ to the left to cope for the fact that the bits are reversed.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp8, xmm_temp6);\n-    __ pslld(xmm_temp3, 1);\n-    __ pslld(xmm_temp6, 1);\n-    __ psrld(xmm_temp7, 31);\n-    __ psrld(xmm_temp8, 31);\n-    __ movdqu(xmm_temp9, xmm_temp7);\n-    __ pslldq(xmm_temp8, 4);\n-    __ pslldq(xmm_temp7, 4);\n-    __ psrldq(xmm_temp9, 12);\n-    __ por(xmm_temp3, xmm_temp7);\n-    __ por(xmm_temp6, xmm_temp8);\n-    __ por(xmm_temp6, xmm_temp9);\n+  __ movdqu(xmm_temp0, Address(state, 0));\n+  __ pshufb(xmm_temp0, xmm_temp10);\n@@ -5436,39 +3125,3 @@\n-    \/\/\n-    \/\/ First phase of the reduction\n-    \/\/\n-    \/\/ Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts\n-    \/\/ independently.\n-    __ movdqu(xmm_temp7, xmm_temp3);\n-    __ movdqu(xmm_temp8, xmm_temp3);\n-    __ movdqu(xmm_temp9, xmm_temp3);\n-    __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n-    __ pslld(xmm_temp8, 30);    \/\/ packed right shift shifting << 30\n-    __ pslld(xmm_temp9, 25);    \/\/ packed right shift shifting << 25\n-    __ pxor(xmm_temp7, xmm_temp8);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp7, xmm_temp9);\n-    __ movdqu(xmm_temp8, xmm_temp7);\n-    __ pslldq(xmm_temp7, 12);\n-    __ psrldq(xmm_temp8, 4);\n-    __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n-    \/\/\n-    \/\/ Second phase of the reduction\n-    \/\/\n-    \/\/ Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these\n-    \/\/ shift operations.\n-    __ movdqu(xmm_temp2, xmm_temp3);\n-    __ movdqu(xmm_temp4, xmm_temp3);\n-    __ movdqu(xmm_temp5, xmm_temp3);\n-    __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n-    __ psrld(xmm_temp4, 2);     \/\/ packed left shifting >> 2\n-    __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n-    __ pxor(xmm_temp2, xmm_temp4);      \/\/ xor the shifted versions\n-    __ pxor(xmm_temp2, xmm_temp5);\n-    __ pxor(xmm_temp2, xmm_temp8);\n-    __ pxor(xmm_temp3, xmm_temp2);\n-    __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n-\n-    __ decrement(blocks);\n-    __ jcc(Assembler::zero, L_exit);\n-    __ movdqu(xmm_temp0, xmm_temp6);\n-    __ addptr(data, 16);\n-    __ jmp(L_ghash_loop);\n+  __ BIND(L_ghash_loop);\n+  __ movdqu(xmm_temp2, Address(data, 0));\n+  __ pshufb(xmm_temp2, ExternalAddress(StubRoutines::x86::ghash_byte_swap_mask_addr()));\n@@ -5477,7 +3130,2 @@\n-    __ BIND(L_exit);\n-    __ pshufb(xmm_temp6, xmm_temp10);          \/\/ Byte swap 16-byte result\n-    __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  __ movdqu(xmm_temp1, Address(subkeyH, 0));\n+  __ pshufb(xmm_temp1, xmm_temp10);\n@@ -5485,17 +3133,1 @@\n-  address base64_shuffle_addr()\n-  {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0405030401020001, relocInfo::none);\n-    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n-    __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n-    __ emit_data64(0x1617151613141213, relocInfo::none);\n-    __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n-    __ emit_data64(0x222321221f201e1f, relocInfo::none);\n-    __ emit_data64(0x2829272825262425, relocInfo::none);\n-    __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n-    return start;\n-  }\n+  __ pxor(xmm_temp0, xmm_temp2);\n@@ -5503,11 +3135,38 @@\n-  address base64_avx2_shuffle_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x0809070805060405, relocInfo::none);\n-    __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n-    __ emit_data64(0x0405030401020001, relocInfo::none);\n-    __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n-    return start;\n-  }\n+  \/\/\n+  \/\/ Multiply with the hash key\n+  \/\/\n+  __ movdqu(xmm_temp3, xmm_temp0);\n+  __ pclmulqdq(xmm_temp3, xmm_temp1, 0);      \/\/ xmm3 holds a0*b0\n+  __ movdqu(xmm_temp4, xmm_temp0);\n+  __ pclmulqdq(xmm_temp4, xmm_temp1, 16);     \/\/ xmm4 holds a0*b1\n+\n+  __ movdqu(xmm_temp5, xmm_temp0);\n+  __ pclmulqdq(xmm_temp5, xmm_temp1, 1);      \/\/ xmm5 holds a1*b0\n+  __ movdqu(xmm_temp6, xmm_temp0);\n+  __ pclmulqdq(xmm_temp6, xmm_temp1, 17);     \/\/ xmm6 holds a1*b1\n+\n+  __ pxor(xmm_temp4, xmm_temp5);      \/\/ xmm4 holds a0*b1 + a1*b0\n+\n+  __ movdqu(xmm_temp5, xmm_temp4);    \/\/ move the contents of xmm4 to xmm5\n+  __ psrldq(xmm_temp4, 8);    \/\/ shift by xmm4 64 bits to the right\n+  __ pslldq(xmm_temp5, 8);    \/\/ shift by xmm5 64 bits to the left\n+  __ pxor(xmm_temp3, xmm_temp5);\n+  __ pxor(xmm_temp6, xmm_temp4);      \/\/ Register pair <xmm6:xmm3> holds the result\n+                                      \/\/ of the carry-less multiplication of\n+                                      \/\/ xmm0 by xmm1.\n+\n+  \/\/ We shift the result of the multiplication by one bit position\n+  \/\/ to the left to cope for the fact that the bits are reversed.\n+  __ movdqu(xmm_temp7, xmm_temp3);\n+  __ movdqu(xmm_temp8, xmm_temp6);\n+  __ pslld(xmm_temp3, 1);\n+  __ pslld(xmm_temp6, 1);\n+  __ psrld(xmm_temp7, 31);\n+  __ psrld(xmm_temp8, 31);\n+  __ movdqu(xmm_temp9, xmm_temp7);\n+  __ pslldq(xmm_temp8, 4);\n+  __ pslldq(xmm_temp7, 4);\n+  __ psrldq(xmm_temp9, 12);\n+  __ por(xmm_temp3, xmm_temp7);\n+  __ por(xmm_temp6, xmm_temp8);\n+  __ por(xmm_temp6, xmm_temp9);\n@@ -5515,11 +3174,17 @@\n-  address base64_avx2_input_mask_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0x8000000000000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    __ emit_data64(0x8000000080000000, relocInfo::none);\n-    return start;\n-  }\n+  \/\/\n+  \/\/ First phase of the reduction\n+  \/\/\n+  \/\/ Move xmm3 into xmm7, xmm8, xmm9 in order to perform the shifts\n+  \/\/ independently.\n+  __ movdqu(xmm_temp7, xmm_temp3);\n+  __ movdqu(xmm_temp8, xmm_temp3);\n+  __ movdqu(xmm_temp9, xmm_temp3);\n+  __ pslld(xmm_temp7, 31);    \/\/ packed right shift shifting << 31\n+  __ pslld(xmm_temp8, 30);    \/\/ packed right shift shifting << 30\n+  __ pslld(xmm_temp9, 25);    \/\/ packed right shift shifting << 25\n+  __ pxor(xmm_temp7, xmm_temp8);      \/\/ xor the shifted versions\n+  __ pxor(xmm_temp7, xmm_temp9);\n+  __ movdqu(xmm_temp8, xmm_temp7);\n+  __ pslldq(xmm_temp7, 12);\n+  __ psrldq(xmm_temp8, 4);\n+  __ pxor(xmm_temp3, xmm_temp7);      \/\/ first phase of the reduction complete\n@@ -5527,17 +3192,22 @@\n-  address base64_avx2_lut_addr()\n-  {\n-    __ align32();\n-    StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n-\n-    \/\/ URL LUT\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n-    __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n-    __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n-    return start;\n-  }\n+  \/\/\n+  \/\/ Second phase of the reduction\n+  \/\/\n+  \/\/ Make 3 copies of xmm3 in xmm2, xmm4, xmm5 for doing these\n+  \/\/ shift operations.\n+  __ movdqu(xmm_temp2, xmm_temp3);\n+  __ movdqu(xmm_temp4, xmm_temp3);\n+  __ movdqu(xmm_temp5, xmm_temp3);\n+  __ psrld(xmm_temp2, 1);     \/\/ packed left shifting >> 1\n+  __ psrld(xmm_temp4, 2);     \/\/ packed left shifting >> 2\n+  __ psrld(xmm_temp5, 7);     \/\/ packed left shifting >> 7\n+  __ pxor(xmm_temp2, xmm_temp4);      \/\/ xor the shifted versions\n+  __ pxor(xmm_temp2, xmm_temp5);\n+  __ pxor(xmm_temp2, xmm_temp8);\n+  __ pxor(xmm_temp3, xmm_temp2);\n+  __ pxor(xmm_temp6, xmm_temp3);      \/\/ the result is in xmm6\n+\n+  __ decrement(blocks);\n+  __ jcc(Assembler::zero, L_exit);\n+  __ movdqu(xmm_temp0, xmm_temp6);\n+  __ addptr(data, 16);\n+  __ jmp(L_ghash_loop);\n@@ -5545,26 +3215,5 @@\n-  address base64_encoding_table_addr()\n-  {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x4847464544434241, relocInfo::none);\n-    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n-    __ emit_data64(0x5857565554535251, relocInfo::none);\n-    __ emit_data64(0x6665646362615a59, relocInfo::none);\n-    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n-    __ emit_data64(0x767574737271706f, relocInfo::none);\n-    __ emit_data64(0x333231307a797877, relocInfo::none);\n-    __ emit_data64(0x2f2b393837363534, relocInfo::none);\n-\n-    \/\/ URL table\n-    __ emit_data64(0x4847464544434241, relocInfo::none);\n-    __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n-    __ emit_data64(0x5857565554535251, relocInfo::none);\n-    __ emit_data64(0x6665646362615a59, relocInfo::none);\n-    __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n-    __ emit_data64(0x767574737271706f, relocInfo::none);\n-    __ emit_data64(0x333231307a797877, relocInfo::none);\n-    __ emit_data64(0x5f2d393837363534, relocInfo::none);\n-    return start;\n-  }\n+  __ BIND(L_exit);\n+  __ pshufb(xmm_temp6, xmm_temp10);          \/\/ Byte swap 16-byte result\n+  __ movdqu(Address(state, 0), xmm_temp6);   \/\/ store the result\n+  __ leave();\n+  __ ret(0);\n@@ -5572,22 +3221,2 @@\n-  \/\/ Code for generating Base64 encoding.\n-  \/\/ Intrinsic function prototype in Base64.java:\n-  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n-  \/\/ boolean isURL) {\n-  address generate_base64_encodeBlock()\n-  {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n-    address start = __ pc();\n-    __ enter();\n-\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-\n-    \/\/ arguments\n-    const Register source = c_rarg0;       \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2;   \/\/ end offset\n-    const Register dest = c_rarg3;   \/\/ destination array\n+  return start;\n+}\n@@ -5595,11 +3224,15 @@\n-#ifndef _WIN64\n-    const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n-#else\n-    const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10; \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-#endif\n+address StubGenerator::base64_shuffle_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"shuffle_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0405030401020001, relocInfo::none);\n+  __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n+  __ emit_data64(0x10110f100d0e0c0d, relocInfo::none);\n+  __ emit_data64(0x1617151613141213, relocInfo::none);\n+  __ emit_data64(0x1c1d1b1c191a1819, relocInfo::none);\n+  __ emit_data64(0x222321221f201e1f, relocInfo::none);\n+  __ emit_data64(0x2829272825262425, relocInfo::none);\n+  __ emit_data64(0x2e2f2d2e2b2c2a2b, relocInfo::none);\n@@ -5607,3 +3240,2 @@\n-    const Register length = r14;\n-    const Register encode_table = r13;\n-    Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+  return start;\n+}\n@@ -5611,5 +3243,4 @@\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n-    __ cmpl(length, 0);\n-    __ jcc(Assembler::lessEqual, L_exit);\n+address StubGenerator::base64_avx2_shuffle_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_shuffle_base64\");\n+  address start = __ pc();\n@@ -5617,6 +3248,4 @@\n-    \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n-    \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n-    \/\/ sure not to read past the end of the input buffer.\n-    if (VM_Version::supports_avx512_vbmi()) {\n-      __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n-      __ jcc(Assembler::below, L_not512);\n+  __ emit_data64(0x0809070805060405, relocInfo::none);\n+  __ emit_data64(0x0e0f0d0e0b0c0a0b, relocInfo::none);\n+  __ emit_data64(0x0405030401020001, relocInfo::none);\n+  __ emit_data64(0x0a0b090a07080607, relocInfo::none);\n@@ -5624,4 +3253,2 @@\n-      __ shll(isURL, 6); \/\/ index into decode table based on isURL\n-      __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n-      __ addptr(encode_table, isURL);\n-      __ shrl(isURL, 6); \/\/ restore isURL\n+  return start;\n+}\n@@ -5629,4 +3256,4 @@\n-      __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n-      __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n-      __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n-      __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n+address StubGenerator::base64_avx2_input_mask_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_input_mask_base64\");\n+  address start = __ pc();\n@@ -5634,2 +3261,4 @@\n-      __ align32();\n-      __ BIND(L_vbmiLoop);\n+  __ emit_data64(0x8000000000000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n+  __ emit_data64(0x8000000080000000, relocInfo::none);\n@@ -5637,2 +3266,2 @@\n-      __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n-      __ subl(length, 48);\n+  return start;\n+}\n@@ -5640,4 +3269,4 @@\n-      \/\/ Put the input bytes into the proper lanes for writing, then\n-      \/\/ encode them.\n-      __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n-      __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n+address StubGenerator::base64_avx2_lut_addr() {\n+  __ align32();\n+  StubCodeMark mark(this, \"StubRoutines\", \"avx2_lut_base64\");\n+  address start = __ pc();\n@@ -5645,2 +3274,4 @@\n-      \/\/ Write to destination\n-      __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x0000f0edfcfcfcfc, relocInfo::none);\n@@ -5648,4 +3279,5 @@\n-      __ addptr(dest, 64);\n-      __ addptr(source, 48);\n-      __ cmpl(length, 64);\n-      __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n+  \/\/ URL LUT\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n+  __ emit_data64(0xfcfcfcfcfcfc4741, relocInfo::none);\n+  __ emit_data64(0x000020effcfcfcfc, relocInfo::none);\n@@ -5653,2 +3285,2 @@\n-      __ vzeroupper();\n-    }\n+  return start;\n+}\n@@ -5656,216 +3288,24 @@\n-    __ BIND(L_not512);\n-    if (VM_Version::supports_avx2()\n-        && VM_Version::supports_avx512vlbw()) {\n-      \/*\n-      ** This AVX2 encoder is based off the paper at:\n-      **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n-      **\n-      ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n-      ** output bytes.\n-      **\n-      *\/\n-      \/\/ Lengths under 32 bytes are done with scalar routine\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::belowEqual, L_process3);\n-\n-      \/\/ Set up supporting constant table data\n-      __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n-      \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n-      __ movl(rax, 0x0fc0fc00);\n-      __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n-      __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n-      \/\/ bits\n-      __ movl(rax, 0x04000040);\n-\n-      __ subl(length, 24);\n-      __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n-\n-      \/\/ For the first load, we mask off reading of the first 4\n-      \/\/ bytes into the register. This is so we can get 4 3-byte\n-      \/\/ chunks into each lane of the register, avoiding having to\n-      \/\/ handle end conditions.  We then shuffle these bytes into a\n-      \/\/ specific order so that manipulation is easier.\n-      \/\/\n-      \/\/ The initial read loads the XMM register like this:\n-      \/\/\n-      \/\/ Lower 128-bit lane:\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n-      \/\/ | C2 | D0 | D1 | D2 |\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/\n-      \/\/ Upper 128-bit lane:\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n-      \/\/ | XX | XX | XX | XX |\n-      \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n-      \/\/\n-      \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n-      \/\/ The alphabetical significance denotes the 3 bytes to be\n-      \/\/ consumed and encoded into 4 bytes.\n-      \/\/\n-      \/\/ We then shuffle the register so each 32-bit word contains\n-      \/\/ the sequence:\n-      \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n-      \/\/ Each of these byte sequences are then manipulated into 4\n-      \/\/ 6-bit values ready for encoding.\n-      \/\/\n-      \/\/ If we focus on one set of 3-byte chunks, changing the\n-      \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n-      \/\/ shuffle such that each 24-bit chunk contains:\n-      \/\/\n-      \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n-      \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n-      \/\/ Explain this step.\n-      \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n-      \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n-      \/\/\n-      \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n-      \/\/ a5..a0) and shift them using a vector multiplication\n-      \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n-      \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n-      \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n-      \/\/ bits respectively.  This is done using vpmullw.  We end up\n-      \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n-      \/\/ ready for encoding:\n-      \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n-      \/\/\n-      \/\/ For translation, we recognize that there are 5 distinct\n-      \/\/ ranges of legal Base64 characters as below:\n-      \/\/\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   | 6-bit value | ASCII range |   offset   |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   |    0..25    |    A..Z     |     65     |\n-      \/\/   |   26..51    |    a..z     |     71     |\n-      \/\/   |   52..61    |    0..9     |     -4     |\n-      \/\/   |     62      |   + or -    | -19 or -17 |\n-      \/\/   |     63      |   \/ or _    | -16 or 32  |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/\n-      \/\/ We note that vpshufb does a parallel lookup in a\n-      \/\/ destination register using the lower 4 bits of bytes from a\n-      \/\/ source register.  If we use a saturated subtraction and\n-      \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n-      \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n-      \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n-      \/\/ value of 13 for all 6-bit values less than 26.  We end up\n-      \/\/ with:\n-      \/\/\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   | 6-bit value |   Reduced   |   offset   |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/   |    0..25    |     13      |     65     |\n-      \/\/   |   26..51    |      0      |     71     |\n-      \/\/   |   52..61    |    0..9     |     -4     |\n-      \/\/   |     62      |     11      | -19 or -17 |\n-      \/\/   |     63      |     12      | -16 or 32  |\n-      \/\/   +-------------+-------------+------------+\n-      \/\/\n-      \/\/ We then use a final vpshufb to add the appropriate offset,\n-      \/\/ translating the bytes.\n-      \/\/\n-      \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n-      \/\/ not load into the full register.\n-      __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n-\n-      \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n-      \/\/ ordering by:\n-      \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n-      \/\/   for easy masking\n-      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n-\n-      __ addl(start_offset, 24);\n-\n-      \/\/ Load masking register for first and third (and multiples)\n-      \/\/ 6-bit values.\n-      __ movl(rax, 0x003f03f0);\n-      __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n-      \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n-      __ movl(rax, 0x01000010);\n-      __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Isolate 6-bit chunks of interest\n-      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n-\n-      \/\/ Load constants for encoding\n-      __ movl(rax, 0x19191919);\n-      __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n-      __ movl(rax, 0x33333333);\n-      __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n-\n-      \/\/ Shift output bytes 0 and 2 into proper lanes\n-      __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n-\n-      \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n-      \/\/ combine\n-      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n-      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n-      __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n-\n-      \/\/ Find out which are 0..25.  This indicates which input\n-      \/\/ values fall in the range of 'A'-'Z', which require an\n-      \/\/ additional offset (see comments above)\n-      __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n-      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n-      __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n-\n-      \/\/ Load the proper lookup table\n-      __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n-      __ movl(r15, isURL);\n-      __ shll(r15, 5);\n-      __ vmovdqu(xmm2, Address(r11, r15));\n-\n-      \/\/ Shuffle the offsets based on the range calculation done\n-      \/\/ above. This allows us to add the correct offset to the\n-      \/\/ 6-bit value corresponding to the range documented above.\n-      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n-      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n-\n-      \/\/ Store the encoded bytes\n-      __ vmovdqu(Address(dest, dp), xmm0);\n-      __ addl(dp, 32);\n-\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::belowEqual, L_process3);\n-\n-      __ align32();\n-      __ BIND(L_32byteLoop);\n-\n-      \/\/ Get next 32 bytes\n-      __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n-\n-      __ subl(length, 24);\n-      __ addl(start_offset, 24);\n-\n-      \/\/ This logic is identical to the above, with only constant\n-      \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n-      \/\/ chunks, shift them into place, then add the offset to\n-      \/\/ encode.\n-      __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n-\n-      __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n-      __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n-      __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n-      __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n-      __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n-      __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n-      __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n-      __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n-      __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n-      __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n-\n-      \/\/ Store the encoded bytes\n-      __ vmovdqu(Address(dest, dp), xmm0);\n-      __ addl(dp, 32);\n-\n-      __ cmpl(length, 31);\n-      __ jcc(Assembler::above, L_32byteLoop);\n-\n-      __ BIND(L_process3);\n-      __ vzeroupper();\n-    } else {\n-      __ BIND(L_process3);\n-    }\n+address StubGenerator::base64_encoding_table_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"encoding_table_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0, \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x4847464544434241, relocInfo::none);\n+  __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+  __ emit_data64(0x5857565554535251, relocInfo::none);\n+  __ emit_data64(0x6665646362615a59, relocInfo::none);\n+  __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+  __ emit_data64(0x767574737271706f, relocInfo::none);\n+  __ emit_data64(0x333231307a797877, relocInfo::none);\n+  __ emit_data64(0x2f2b393837363534, relocInfo::none);\n+\n+  \/\/ URL table\n+  __ emit_data64(0x4847464544434241, relocInfo::none);\n+  __ emit_data64(0x504f4e4d4c4b4a49, relocInfo::none);\n+  __ emit_data64(0x5857565554535251, relocInfo::none);\n+  __ emit_data64(0x6665646362615a59, relocInfo::none);\n+  __ emit_data64(0x6e6d6c6b6a696867, relocInfo::none);\n+  __ emit_data64(0x767574737271706f, relocInfo::none);\n+  __ emit_data64(0x333231307a797877, relocInfo::none);\n+  __ emit_data64(0x5f2d393837363534, relocInfo::none);\n@@ -5873,2 +3313,2 @@\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::below, L_exit);\n+  return start;\n+}\n@@ -5876,5 +3316,23 @@\n-    \/\/ Load the encoding table based on isURL\n-    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n-    __ movl(r15, isURL);\n-    __ shll(r15, 6);\n-    __ addptr(r11, r15);\n+\/\/ Code for generating Base64 encoding.\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp,\n+\/\/ boolean isURL) {\n+address StubGenerator::generate_base64_encodeBlock()\n+{\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"implEncode\");\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ Save callee-saved registers before using them\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+\n+  \/\/ arguments\n+  const Register source = c_rarg0;       \/\/ Source Array\n+  const Register start_offset = c_rarg1; \/\/ start offset\n+  const Register end_offset = c_rarg2;   \/\/ end offset\n+  const Register dest = c_rarg3;   \/\/ destination array\n@@ -5882,1 +3340,11 @@\n-    __ BIND(L_processdata);\n+#ifndef _WIN64\n+  const Register dp = c_rarg4;    \/\/ Position for writing to dest array\n+  const Register isURL = c_rarg5; \/\/ Base64 or URL character set\n+#else\n+  const Address dp_mem(rbp, 6 * wordSize); \/\/ length is on stack on Win64\n+  const Address isURL_mem(rbp, 7 * wordSize);\n+  const Register isURL = r10; \/\/ pick the volatile windows register\n+  const Register dp = r12;\n+  __ movl(dp, dp_mem);\n+  __ movl(isURL, isURL_mem);\n+#endif\n@@ -5884,4 +3352,26 @@\n-    \/\/ Load 3 bytes\n-    __ load_unsigned_byte(r15, Address(source, start_offset));\n-    __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n-    __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n+  const Register length = r14;\n+  const Register encode_table = r13;\n+  Label L_process3, L_exit, L_processdata, L_vbmiLoop, L_not512, L_32byteLoop;\n+\n+  \/\/ calculate length from offsets\n+  __ movl(length, end_offset);\n+  __ subl(length, start_offset);\n+  __ cmpl(length, 0);\n+  __ jcc(Assembler::lessEqual, L_exit);\n+\n+  \/\/ Code for 512-bit VBMI encoding.  Encodes 48 input bytes into 64\n+  \/\/ output bytes. We read 64 input bytes and ignore the last 16, so be\n+  \/\/ sure not to read past the end of the input buffer.\n+  if (VM_Version::supports_avx512_vbmi()) {\n+    __ cmpl(length, 64); \/\/ Do not overrun input buffer.\n+    __ jcc(Assembler::below, L_not512);\n+\n+    __ shll(isURL, 6); \/\/ index into decode table based on isURL\n+    __ lea(encode_table, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+    __ addptr(encode_table, isURL);\n+    __ shrl(isURL, 6); \/\/ restore isURL\n+\n+    __ mov64(rax, 0x3036242a1016040aull); \/\/ Shifts\n+    __ evmovdquq(xmm3, ExternalAddress(StubRoutines::x86::base64_shuffle_addr()), Assembler::AVX_512bit, r15);\n+    __ evmovdquq(xmm2, Address(encode_table, 0), Assembler::AVX_512bit);\n+    __ evpbroadcastq(xmm1, rax, Assembler::AVX_512bit);\n@@ -5889,4 +3379,2 @@\n-    \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n-    __ movl(rax, r10);\n-    __ shll(r10, 24);\n-    __ orl(rax, r10);\n+    __ align32();\n+    __ BIND(L_vbmiLoop);\n@@ -5894,1 +3382,2 @@\n-    __ subl(length, 3);\n+    __ vpermb(xmm0, xmm3, Address(source, start_offset), Assembler::AVX_512bit);\n+    __ subl(length, 48);\n@@ -5896,3 +3385,4 @@\n-    __ shll(r15, 8);\n-    __ shll(r13, 16);\n-    __ orl(rax, r15);\n+    \/\/ Put the input bytes into the proper lanes for writing, then\n+    \/\/ encode them.\n+    __ evpmultishiftqb(xmm0, xmm1, xmm0, Assembler::AVX_512bit);\n+    __ vpermb(xmm0, xmm0, xmm2, Assembler::AVX_512bit);\n@@ -5900,1 +3390,2 @@\n-    __ addl(start_offset, 3);\n+    \/\/ Write to destination\n+    __ evmovdquq(Address(dest, dp), xmm0, Assembler::AVX_512bit);\n@@ -5902,6 +3393,4 @@\n-    __ orl(rax, r13);\n-    \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n-    \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n-    \/\/ This translated byte is the fourth output byte.\n-    __ shrl(r13, 16);\n-    __ andl(r13, 0x3f);\n+    __ addptr(dest, 64);\n+    __ addptr(source, 48);\n+    __ cmpl(length, 64);\n+    __ jcc(Assembler::aboveEqual, L_vbmiLoop);\n@@ -5909,3 +3398,2 @@\n-    \/\/ The high-order 6 bits of r15 (byte0) is translated.\n-    \/\/ The translated byte is the first output byte.\n-    __ shrl(r15, 10);\n+    __ vzeroupper();\n+  }\n@@ -5913,2 +3401,164 @@\n-    __ load_unsigned_byte(r13, Address(r11, r13));\n-    __ load_unsigned_byte(r15, Address(r11, r15));\n+  __ BIND(L_not512);\n+  if (VM_Version::supports_avx2()\n+      && VM_Version::supports_avx512vlbw()) {\n+    \/*\n+    ** This AVX2 encoder is based off the paper at:\n+    **      https:\/\/dl.acm.org\/doi\/10.1145\/3132709\n+    **\n+    ** We use AVX2 SIMD instructions to encode 24 bytes into 32\n+    ** output bytes.\n+    **\n+    *\/\n+    \/\/ Lengths under 32 bytes are done with scalar routine\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::belowEqual, L_process3);\n+\n+    \/\/ Set up supporting constant table data\n+    __ vmovdqu(xmm9, ExternalAddress(StubRoutines::x86::base64_avx2_shuffle_addr()), rax);\n+    \/\/ 6-bit mask for 2nd and 4th (and multiples) 6-bit values\n+    __ movl(rax, 0x0fc0fc00);\n+    __ vmovdqu(xmm1, ExternalAddress(StubRoutines::x86::base64_avx2_input_mask_addr()), rax);\n+    __ evpbroadcastd(xmm8, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Multiplication constant for \"shifting\" right by 6 and 10\n+    \/\/ bits\n+    __ movl(rax, 0x04000040);\n+\n+    __ subl(length, 24);\n+    __ evpbroadcastd(xmm7, rax, Assembler::AVX_256bit);\n+\n+    \/\/ For the first load, we mask off reading of the first 4\n+    \/\/ bytes into the register. This is so we can get 4 3-byte\n+    \/\/ chunks into each lane of the register, avoiding having to\n+    \/\/ handle end conditions.  We then shuffle these bytes into a\n+    \/\/ specific order so that manipulation is easier.\n+    \/\/\n+    \/\/ The initial read loads the XMM register like this:\n+    \/\/\n+    \/\/ Lower 128-bit lane:\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/ | XX | XX | XX | XX | A0 | A1 | A2 | B0 | B1 | B2 | C0 | C1\n+    \/\/ | C2 | D0 | D1 | D2 |\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/\n+    \/\/ Upper 128-bit lane:\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/ | E0 | E1 | E2 | F0 | F1 | F2 | G0 | G1 | G2 | H0 | H1 | H2\n+    \/\/ | XX | XX | XX | XX |\n+    \/\/ +----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n+    \/\/\n+    \/\/ Where A0 is the first input byte, B0 is the fourth, etc.\n+    \/\/ The alphabetical significance denotes the 3 bytes to be\n+    \/\/ consumed and encoded into 4 bytes.\n+    \/\/\n+    \/\/ We then shuffle the register so each 32-bit word contains\n+    \/\/ the sequence:\n+    \/\/    A1 A0 A2 A1, B1, B0, B2, B1, etc.\n+    \/\/ Each of these byte sequences are then manipulated into 4\n+    \/\/ 6-bit values ready for encoding.\n+    \/\/\n+    \/\/ If we focus on one set of 3-byte chunks, changing the\n+    \/\/ nomenclature such that A0 => a, A1 => b, and A2 => c, we\n+    \/\/ shuffle such that each 24-bit chunk contains:\n+    \/\/\n+    \/\/ b7 b6 b5 b4 b3 b2 b1 b0 | a7 a6 a5 a4 a3 a2 a1 a0 | c7 c6\n+    \/\/ c5 c4 c3 c2 c1 c0 | b7 b6 b5 b4 b3 b2 b1 b0\n+    \/\/ Explain this step.\n+    \/\/ b3 b2 b1 b0 c5 c4 c3 c2 | c1 c0 d5 d4 d3 d2 d1 d0 | a5 a4\n+    \/\/ a3 a2 a1 a0 b5 b4 | b3 b2 b1 b0 c5 c4 c3 c2\n+    \/\/\n+    \/\/ W first and off all but bits 4-9 and 16-21 (c5..c0 and\n+    \/\/ a5..a0) and shift them using a vector multiplication\n+    \/\/ operation (vpmulhuw) which effectively shifts c right by 6\n+    \/\/ bits and a right by 10 bits.  We similarly mask bits 10-15\n+    \/\/ (d5..d0) and 22-27 (b5..b0) and shift them left by 8 and 4\n+    \/\/ bits respectively.  This is done using vpmullw.  We end up\n+    \/\/ with 4 6-bit values, thus splitting the 3 input bytes,\n+    \/\/ ready for encoding:\n+    \/\/    0 0 d5..d0 0 0 c5..c0 0 0 b5..b0 0 0 a5..a0\n+    \/\/\n+    \/\/ For translation, we recognize that there are 5 distinct\n+    \/\/ ranges of legal Base64 characters as below:\n+    \/\/\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   | 6-bit value | ASCII range |   offset   |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   |    0..25    |    A..Z     |     65     |\n+    \/\/   |   26..51    |    a..z     |     71     |\n+    \/\/   |   52..61    |    0..9     |     -4     |\n+    \/\/   |     62      |   + or -    | -19 or -17 |\n+    \/\/   |     63      |   \/ or _    | -16 or 32  |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/\n+    \/\/ We note that vpshufb does a parallel lookup in a\n+    \/\/ destination register using the lower 4 bits of bytes from a\n+    \/\/ source register.  If we use a saturated subtraction and\n+    \/\/ subtract 51 from each 6-bit value, bytes from [0,51]\n+    \/\/ saturate to 0, and [52,63] map to a range of [1,12].  We\n+    \/\/ distinguish the [0,25] and [26,51] ranges by assigning a\n+    \/\/ value of 13 for all 6-bit values less than 26.  We end up\n+    \/\/ with:\n+    \/\/\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   | 6-bit value |   Reduced   |   offset   |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/   |    0..25    |     13      |     65     |\n+    \/\/   |   26..51    |      0      |     71     |\n+    \/\/   |   52..61    |    0..9     |     -4     |\n+    \/\/   |     62      |     11      | -19 or -17 |\n+    \/\/   |     63      |     12      | -16 or 32  |\n+    \/\/   +-------------+-------------+------------+\n+    \/\/\n+    \/\/ We then use a final vpshufb to add the appropriate offset,\n+    \/\/ translating the bytes.\n+    \/\/\n+    \/\/ Load input bytes - only 28 bytes.  Mask the first load to\n+    \/\/ not load into the full register.\n+    __ vpmaskmovd(xmm1, xmm1, Address(source, start_offset, Address::times_1, -4), Assembler::AVX_256bit);\n+\n+    \/\/ Move 3-byte chunks of input (12 bytes) into 16 bytes,\n+    \/\/ ordering by:\n+    \/\/   1, 0, 2, 1; 4, 3, 5, 4; etc.  This groups 6-bit chunks\n+    \/\/   for easy masking\n+    __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+    __ addl(start_offset, 24);\n+\n+    \/\/ Load masking register for first and third (and multiples)\n+    \/\/ 6-bit values.\n+    __ movl(rax, 0x003f03f0);\n+    __ evpbroadcastd(xmm6, rax, Assembler::AVX_256bit);\n+    \/\/ Multiplication constant for \"shifting\" left by 4 and 8 bits\n+    __ movl(rax, 0x01000010);\n+    __ evpbroadcastd(xmm5, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Isolate 6-bit chunks of interest\n+    __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+\n+    \/\/ Load constants for encoding\n+    __ movl(rax, 0x19191919);\n+    __ evpbroadcastd(xmm3, rax, Assembler::AVX_256bit);\n+    __ movl(rax, 0x33333333);\n+    __ evpbroadcastd(xmm4, rax, Assembler::AVX_256bit);\n+\n+    \/\/ Shift output bytes 0 and 2 into proper lanes\n+    __ vpmulhuw(xmm2, xmm0, xmm7, Assembler::AVX_256bit);\n+\n+    \/\/ Mask and shift output bytes 1 and 3 into proper lanes and\n+    \/\/ combine\n+    __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+    __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+    __ vpor(xmm0, xmm0, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ Find out which are 0..25.  This indicates which input\n+    \/\/ values fall in the range of 'A'-'Z', which require an\n+    \/\/ additional offset (see comments above)\n+    __ vpcmpgtb(xmm2, xmm0, xmm3, Assembler::AVX_256bit);\n+    __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+    __ vpsubb(xmm1, xmm1, xmm2, Assembler::AVX_256bit);\n+\n+    \/\/ Load the proper lookup table\n+    __ lea(r11, ExternalAddress(StubRoutines::x86::base64_avx2_lut_addr()));\n+    __ movl(r15, isURL);\n+    __ shll(r15, 5);\n+    __ vmovdqu(xmm2, Address(r11, r15));\n@@ -5916,1 +3566,5 @@\n-    __ movb(Address(dest, dp, Address::times_1, 3), r13);\n+    \/\/ Shuffle the offsets based on the range calculation done\n+    \/\/ above. This allows us to add the correct offset to the\n+    \/\/ 6-bit value corresponding to the range documented above.\n+    __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+    __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n@@ -5918,5 +3572,3 @@\n-    \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n-    \/\/ This translated byte is the second output byte.\n-    __ shrl(rax, 4);\n-    __ movl(r10, rax);\n-    __ andl(rax, 0x3f);\n+    \/\/ Store the encoded bytes\n+    __ vmovdqu(Address(dest, dp), xmm0);\n+    __ addl(dp, 32);\n@@ -5924,1 +3576,2 @@\n-    __ movb(Address(dest, dp, Address::times_1, 0), r15);\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::belowEqual, L_process3);\n@@ -5926,1 +3579,38 @@\n-    __ load_unsigned_byte(rax, Address(r11, rax));\n+    __ align32();\n+    __ BIND(L_32byteLoop);\n+\n+    \/\/ Get next 32 bytes\n+    __ vmovdqu(xmm1, Address(source, start_offset, Address::times_1, -4));\n+\n+    __ subl(length, 24);\n+    __ addl(start_offset, 24);\n+\n+    \/\/ This logic is identical to the above, with only constant\n+    \/\/ register loads removed.  Shuffle the input, mask off 6-bit\n+    \/\/ chunks, shift them into place, then add the offset to\n+    \/\/ encode.\n+    __ vpshufb(xmm1, xmm1, xmm9, Assembler::AVX_256bit);\n+\n+    __ vpand(xmm0, xmm8, xmm1, Assembler::AVX_256bit);\n+    __ vpmulhuw(xmm10, xmm0, xmm7, Assembler::AVX_256bit);\n+    __ vpand(xmm0, xmm6, xmm1, Assembler::AVX_256bit);\n+    __ vpmullw(xmm0, xmm5, xmm0, Assembler::AVX_256bit);\n+    __ vpor(xmm0, xmm0, xmm10, Assembler::AVX_256bit);\n+    __ vpcmpgtb(xmm10, xmm0, xmm3, Assembler::AVX_256bit);\n+    __ vpsubusb(xmm1, xmm0, xmm4, Assembler::AVX_256bit);\n+    __ vpsubb(xmm1, xmm1, xmm10, Assembler::AVX_256bit);\n+    __ vpshufb(xmm1, xmm2, xmm1, Assembler::AVX_256bit);\n+    __ vpaddb(xmm0, xmm1, xmm0, Assembler::AVX_256bit);\n+\n+    \/\/ Store the encoded bytes\n+    __ vmovdqu(Address(dest, dp), xmm0);\n+    __ addl(dp, 32);\n+\n+    __ cmpl(length, 31);\n+    __ jcc(Assembler::above, L_32byteLoop);\n+\n+    __ BIND(L_process3);\n+    __ vzeroupper();\n+  } else {\n+    __ BIND(L_process3);\n+  }\n@@ -5928,4 +3618,2 @@\n-    \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n-    \/\/ This translated byte is the third output byte.\n-    __ shrl(r10, 18);\n-    __ andl(r10, 0x3f);\n+  __ cmpl(length, 3);\n+  __ jcc(Assembler::below, L_exit);\n@@ -5933,1 +3621,5 @@\n-    __ load_unsigned_byte(r10, Address(r11, r10));\n+  \/\/ Load the encoding table based on isURL\n+  __ lea(r11, ExternalAddress(StubRoutines::x86::base64_encoding_table_addr()));\n+  __ movl(r15, isURL);\n+  __ shll(r15, 6);\n+  __ addptr(r11, r15);\n@@ -5935,2 +3627,1 @@\n-    __ movb(Address(dest, dp, Address::times_1, 1), rax);\n-    __ movb(Address(dest, dp, Address::times_1, 2), r10);\n+  __ BIND(L_processdata);\n@@ -5938,3 +3629,4 @@\n-    __ addl(dp, 4);\n-    __ cmpl(length, 3);\n-    __ jcc(Assembler::aboveEqual, L_processdata);\n+  \/\/ Load 3 bytes\n+  __ load_unsigned_byte(r15, Address(source, start_offset));\n+  __ load_unsigned_byte(r10, Address(source, start_offset, Address::times_1, 1));\n+  __ load_unsigned_byte(r13, Address(source, start_offset, Address::times_1, 2));\n@@ -5942,9 +3634,4 @@\n-    __ BIND(L_exit);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n-    __ leave();\n-    __ ret(0);\n-    return start;\n-  }\n+  \/\/ Build a 32-bit word with bytes 1, 2, 0, 1\n+  __ movl(rax, r10);\n+  __ shll(r10, 24);\n+  __ orl(rax, r10);\n@@ -5952,17 +3639,1 @@\n-  \/\/ base64 AVX512vbmi tables\n-  address base64_vbmi_lookup_lo_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x3f8080803e808080, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n-    return start;\n-  }\n+  __ subl(length, 3);\n@@ -5970,32 +3641,3 @@\n-  address base64_vbmi_lookup_hi_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0605040302010080, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x8080808080191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0x8080808080333231, relocInfo::none);\n-    return start;\n-  }\n-  address base64_vbmi_lookup_lo_url_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x8080808080808080, relocInfo::none);\n-    __ emit_data64(0x80803e8080808080, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0x8080808080803d3c, relocInfo::none);\n-    return start;\n-  }\n+  __ shll(r15, 8);\n+  __ shll(r13, 16);\n+  __ orl(rax, r15);\n@@ -6003,16 +3645,1 @@\n-  address base64_vbmi_lookup_hi_url_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x0605040302010080, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x3f80808080191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0x8080808080333231, relocInfo::none);\n-    return start;\n-  }\n+  __ addl(start_offset, 3);\n@@ -6020,16 +3647,6 @@\n-  address base64_vbmi_pack_vec_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x090a040506000102, relocInfo::none);\n-    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    __ emit_data64(0x0000000000000000, relocInfo::none);\n-    return start;\n-  }\n+  __ orl(rax, r13);\n+  \/\/ At this point, rax contains | byte1 | byte2 | byte0 | byte1\n+  \/\/ r13 has byte2 << 16 - need low-order 6 bits to translate.\n+  \/\/ This translated byte is the fourth output byte.\n+  __ shrl(r13, 16);\n+  __ andl(r13, 0x3f);\n@@ -6037,16 +3654,3 @@\n-  address base64_vbmi_join_0_1_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x090a040506000102, relocInfo::none);\n-    __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ The high-order 6 bits of r15 (byte0) is translated.\n+  \/\/ The translated byte is the first output byte.\n+  __ shrl(r15, 10);\n@@ -6054,16 +3658,2 @@\n-  address base64_vbmi_join_1_2_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n-    __ emit_data64(0x292a242526202122, relocInfo::none);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n-    __ emit_data64(0x696a646566606162, relocInfo::none);\n-    return start;\n-  }\n+  __ load_unsigned_byte(r13, Address(r11, r13));\n+  __ load_unsigned_byte(r15, Address(r11, r15));\n@@ -6071,16 +3661,1 @@\n-  address base64_vbmi_join_2_3_addr() {\n-    __ align64();\n-    StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n-    address start = __ pc();\n-    assert(((unsigned long long)start & 0x3f) == 0,\n-           \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n-    __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n-    __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n-    __ emit_data64(0x494a444546404142, relocInfo::none);\n-    __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n-    __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n-    __ emit_data64(0x696a646566606162, relocInfo::none);\n-    __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n-    __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n-    return start;\n-  }\n+  __ movb(Address(dest, dp, Address::times_1, 3), r13);\n@@ -6088,71 +3663,5 @@\n-  address base64_decoding_table_addr() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n-    address start = __ pc();\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n-    __ emit_data64(0x06050403020100ff, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0xffffffffff191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0xffffffffff333231, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-\n-    \/\/ URL table\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffff3effffffffff, relocInfo::none);\n-    __ emit_data64(0x3b3a393837363534, relocInfo::none);\n-    __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n-    __ emit_data64(0x06050403020100ff, relocInfo::none);\n-    __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n-    __ emit_data64(0x161514131211100f, relocInfo::none);\n-    __ emit_data64(0x3fffffffff191817, relocInfo::none);\n-    __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n-    __ emit_data64(0x2827262524232221, relocInfo::none);\n-    __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n-    __ emit_data64(0xffffffffff333231, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    __ emit_data64(0xffffffffffffffff, relocInfo::none);\n-    return start;\n-  }\n+  \/\/ Extract high-order 4 bits of byte1 and low-order 2 bits of byte0.\n+  \/\/ This translated byte is the second output byte.\n+  __ shrl(rax, 4);\n+  __ movl(r10, rax);\n+  __ andl(rax, 0x3f);\n@@ -6160,0 +3669,1 @@\n+  __ movb(Address(dest, dp, Address::times_1, 0), r15);\n@@ -6161,25 +3671,1 @@\n-\/\/ Code for generating Base64 decoding.\n-\/\/\n-\/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n-\/\/\n-\/\/ Intrinsic function prototype in Base64.java:\n-\/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME) {\n-  address generate_base64_decodeBlock() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n-    address start = __ pc();\n-    __ enter();\n-\n-    \/\/ Save callee-saved registers before using them\n-    __ push(r12);\n-    __ push(r13);\n-    __ push(r14);\n-    __ push(r15);\n-    __ push(rbx);\n-\n-    \/\/ arguments\n-    const Register source = c_rarg0; \/\/ Source Array\n-    const Register start_offset = c_rarg1; \/\/ start offset\n-    const Register end_offset = c_rarg2; \/\/ end offset\n-    const Register dest = c_rarg3; \/\/ destination array\n-    const Register isMIME = rbx;\n+  __ load_unsigned_byte(rax, Address(r11, rax));\n@@ -6187,13 +3673,4 @@\n-#ifndef _WIN64\n-    const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n-    const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n-    __ movl(isMIME, Address(rbp, 2 * wordSize));\n-#else\n-    const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n-    const Address isURL_mem(rbp, 7 * wordSize);\n-    const Register isURL = r10;      \/\/ pick the volatile windows register\n-    const Register dp = r12;\n-    __ movl(dp, dp_mem);\n-    __ movl(isURL, isURL_mem);\n-    __ movl(isMIME, Address(rbp, 8 * wordSize));\n-#endif\n+  \/\/ Extract low-order 2 bits of byte1 and high-order 4 bits of byte2.\n+  \/\/ This translated byte is the third output byte.\n+  __ shrl(r10, 18);\n+  __ andl(r10, 0x3f);\n@@ -6201,338 +3678,1 @@\n-    const XMMRegister lookup_lo = xmm5;\n-    const XMMRegister lookup_hi = xmm6;\n-    const XMMRegister errorvec = xmm7;\n-    const XMMRegister pack16_op = xmm9;\n-    const XMMRegister pack32_op = xmm8;\n-    const XMMRegister input0 = xmm3;\n-    const XMMRegister input1 = xmm20;\n-    const XMMRegister input2 = xmm21;\n-    const XMMRegister input3 = xmm19;\n-    const XMMRegister join01 = xmm12;\n-    const XMMRegister join12 = xmm11;\n-    const XMMRegister join23 = xmm10;\n-    const XMMRegister translated0 = xmm2;\n-    const XMMRegister translated1 = xmm1;\n-    const XMMRegister translated2 = xmm0;\n-    const XMMRegister translated3 = xmm4;\n-\n-    const XMMRegister merged0 = xmm2;\n-    const XMMRegister merged1 = xmm1;\n-    const XMMRegister merged2 = xmm0;\n-    const XMMRegister merged3 = xmm4;\n-    const XMMRegister merge_ab_bc0 = xmm2;\n-    const XMMRegister merge_ab_bc1 = xmm1;\n-    const XMMRegister merge_ab_bc2 = xmm0;\n-    const XMMRegister merge_ab_bc3 = xmm4;\n-\n-    const XMMRegister pack24bits = xmm4;\n-\n-    const Register length = r14;\n-    const Register output_size = r13;\n-    const Register output_mask = r15;\n-    const KRegister input_mask = k1;\n-\n-    const XMMRegister input_initial_valid_b64 = xmm0;\n-    const XMMRegister tmp = xmm10;\n-    const XMMRegister mask = xmm0;\n-    const XMMRegister invalid_b64 = xmm1;\n-\n-    Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n-    Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n-    Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n-\n-    \/\/ calculate length from offsets\n-    __ movl(length, end_offset);\n-    __ subl(length, start_offset);\n-    __ push(dest);          \/\/ Save for return value calc\n-\n-    \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n-    if(VM_Version::supports_avx512_vbmi() &&\n-       VM_Version::supports_avx512bw()) {\n-      __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n-      __ jcc(Assembler::lessEqual, L_bruteForce);\n-\n-      __ cmpl(isMIME, 0);\n-      __ jcc(Assembler::notEqual, L_bruteForce);\n-\n-      \/\/ Load lookup tables based on isURL\n-      __ cmpl(isURL, 0);\n-      __ jcc(Assembler::notZero, L_loadURL);\n-\n-      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ BIND(L_continue);\n-\n-      __ movl(r15, 0x01400140);\n-      __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n-\n-      __ movl(r15, 0x00011000);\n-      __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n-\n-      __ cmpl(length, 0xff);\n-      __ jcc(Assembler::lessEqual, L_process64);\n-\n-      \/\/ load masks required for decoding data\n-      __ BIND(L_processdata);\n-      __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n-      __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ align32();\n-      __ BIND(L_process256);\n-      \/\/ Grab input data\n-      __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n-      __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n-      __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n-      __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n-\n-      \/\/ Copy the low part of the lookup table into the destination of the permutation\n-      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n-      __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n-\n-      \/\/ Translate the base64 input into \"decoded\" bytes\n-      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n-      __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n-\n-      \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n-      __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n-\n-      __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n-      __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n-      __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n-\n-      \/\/ Check if there was an error - if so, try 64-byte chunks\n-      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n-      __ kortestql(k3, k3);\n-      __ jcc(Assembler::notZero, L_process64);\n-\n-      \/\/ The merging and shuffling happens here\n-      \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n-      \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n-      \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n-      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n-\n-      \/\/ Now do the same with packed 16-bit values.\n-      \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n-      \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n-      \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n-      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n-\n-      \/\/ The join vectors specify which byte from which vector goes into the outputs\n-      \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n-      \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n-      __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n-      __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n-      __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n-\n-      \/\/ Store result\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n-      __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n-\n-      __ addptr(source, 0x100);\n-      __ addptr(dest, 0xc0);\n-      __ subl(length, 0x100);\n-      __ cmpl(length, 64 * 4);\n-      __ jcc(Assembler::greaterEqual, L_process256);\n-\n-      \/\/ At this point, we've decoded 64 * 4 * n bytes.\n-      \/\/ The remaining length will be <= 64 * 4 - 1.\n-      \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n-      \/\/ case, the length will be arbitrarily long.\n-      \/\/\n-      \/\/ Note that this will be the path for MIME-encoded strings.\n-\n-      __ BIND(L_process64);\n-\n-      __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n-\n-      __ cmpl(length, 63);\n-      __ jcc(Assembler::lessEqual, L_finalBit);\n-\n-      __ mov64(rax, 0x0000ffffffffffff);\n-      __ kmovql(k2, rax);\n-\n-      __ align32();\n-      __ BIND(L_process64Loop);\n-\n-      \/\/ Handle first 64-byte block\n-\n-      __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n-      __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n-      __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n-\n-      __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n-\n-      \/\/ Check for error and bomb out before updating dest\n-      __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n-      __ kortestql(k3, k3);\n-      __ jcc(Assembler::notZero, L_exit);\n-\n-      \/\/ Pack output register, selecting correct byte ordering\n-      __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n-      __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n-\n-      __ evmovdqub(Address(dest, dp), k2, merged0, true, Assembler::AVX_512bit);\n-\n-      __ subl(length, 64);\n-      __ addptr(source, 64);\n-      __ addptr(dest, 48);\n-\n-      __ cmpl(length, 64);\n-      __ jcc(Assembler::greaterEqual, L_process64Loop);\n-\n-      __ cmpl(length, 0);\n-      __ jcc(Assembler::lessEqual, L_exit);\n-\n-      __ BIND(L_finalBit);\n-      \/\/ Now have 1 to 63 bytes left to decode\n-\n-      \/\/ I was going to let Java take care of the final fragment\n-      \/\/ however it will repeatedly call this routine for every 4 bytes\n-      \/\/ of input data, so handle the rest here.\n-      __ movq(rax, -1);\n-      __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n-\n-      __ movl(output_size, length);\n-      __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n-      __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n-      \/\/ output_size in r13\n-\n-      \/\/ Strip pad characters, if any, and adjust length and mask\n-      __ cmpb(Address(source, length, Address::times_1, -1), '=');\n-      __ jcc(Assembler::equal, L_padding);\n-\n-      __ BIND(L_donePadding);\n-\n-      \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n-      __ kmovql(input_mask, rax);\n-      __ movq(output_mask, -1);\n-      __ bzhiq(output_mask, output_mask, output_size);\n-\n-      \/\/ Load initial input with all valid base64 characters.  Will be used\n-      \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n-      __ movl(rax, 0x61616161);\n-      __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n-\n-      \/\/ A register containing all invalid base64 decoded values\n-      __ movl(rax, 0x80808080);\n-      __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n-\n-      \/\/ input_mask is in k1\n-      \/\/ output_size is in r13\n-      \/\/ output_mask is in r15\n-      \/\/ zmm0 - free\n-      \/\/ zmm1 - 0x00011000\n-      \/\/ zmm2 - 0x01400140\n-      \/\/ zmm3 - errorvec\n-      \/\/ zmm4 - pack vector\n-      \/\/ zmm5 - lookup_lo\n-      \/\/ zmm6 - lookup_hi\n-      \/\/ zmm7 - errorvec\n-      \/\/ zmm8 - 0x61616161\n-      \/\/ zmm9 - 0x80808080\n-\n-      \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n-      __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n-\n-      \/\/ Decode all bytes within our merged input\n-      __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n-      __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n-      __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n-\n-      \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n-      \/\/ If any bytes have their high-order bit set, then we have an error.\n-      __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n-      __ kortestql(k2, k2);\n-\n-      \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n-      __ jcc(Assembler::notZero, L_bruteForce);\n-\n-      \/\/ Shuffle output bytes\n-      __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n-      __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n-\n-      __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n-      __ kmovql(k1, output_mask);\n-      __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n-\n-      __ addptr(dest, output_size);\n-\n-      __ BIND(L_exit);\n-      __ vzeroupper();\n-      __ pop(rax);             \/\/ Get original dest value\n-      __ subptr(dest, rax);      \/\/ Number of bytes converted\n-      __ movptr(rax, dest);\n-      __ pop(rbx);\n-      __ pop(r15);\n-      __ pop(r14);\n-      __ pop(r13);\n-      __ pop(r12);\n-      __ leave();\n-      __ ret(0);\n-\n-      __ BIND(L_loadURL);\n-      __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n-      __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n-      __ jmp(L_continue);\n-\n-      __ BIND(L_padding);\n-      __ decrementq(output_size, 1);\n-      __ shrq(rax, 1);\n-\n-      __ cmpb(Address(source, length, Address::times_1, -2), '=');\n-      __ jcc(Assembler::notEqual, L_donePadding);\n-\n-      __ decrementq(output_size, 1);\n-      __ shrq(rax, 1);\n-      __ jmp(L_donePadding);\n-\n-      __ align32();\n-      __ BIND(L_bruteForce);\n-    }   \/\/ End of if(avx512_vbmi)\n-\n-    \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n-\n-    \/\/ Register state (Linux):\n-    \/\/ r12-15 - saved on stack\n-    \/\/ rdi - src\n-    \/\/ rsi - sp\n-    \/\/ rdx - sl\n-    \/\/ rcx - dst\n-    \/\/ r8 - dp\n-    \/\/ r9 - isURL\n-\n-    \/\/ Register state (Windows):\n-    \/\/ r12-15 - saved on stack\n-    \/\/ rcx - src\n-    \/\/ rdx - sp\n-    \/\/ r8 - sl\n-    \/\/ r9 - dst\n-    \/\/ r12 - dp\n-    \/\/ r10 - isURL\n-\n-    \/\/ Registers (common):\n-    \/\/ length (r14) - bytes in src\n-\n-    const Register decode_table = r11;\n-    const Register out_byte_count = rbx;\n-    const Register byte1 = r13;\n-    const Register byte2 = r15;\n-    const Register byte3 = WINDOWS_ONLY(r8) NOT_WINDOWS(rdx);\n-    const Register byte4 = WINDOWS_ONLY(r10) NOT_WINDOWS(r9);\n-\n-    __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n-    __ cmpl(length, 0);\n-    __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n+  __ load_unsigned_byte(r10, Address(r11, r10));\n@@ -6540,3 +3680,2 @@\n-    __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n-    __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n-    __ addptr(decode_table, isURL);\n+  __ movb(Address(dest, dp, Address::times_1, 1), rax);\n+  __ movb(Address(dest, dp, Address::times_1, 2), r10);\n@@ -6544,1 +3683,3 @@\n-    __ jmp(L_bottomLoop);\n+  __ addl(dp, 4);\n+  __ cmpl(length, 3);\n+  __ jcc(Assembler::aboveEqual, L_processdata);\n@@ -6546,48 +3687,7 @@\n-    __ align32();\n-    __ BIND(L_forceLoop);\n-    __ shll(byte1, 18);\n-    __ shll(byte2, 12);\n-    __ shll(byte3, 6);\n-    __ orl(byte1, byte2);\n-    __ orl(byte1, byte3);\n-    __ orl(byte1, byte4);\n-\n-    __ addptr(source, 4);\n-\n-    __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n-    __ shrl(byte1, 8);\n-    __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n-    __ shrl(byte1, 8);\n-    __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n-\n-    __ addptr(dest, 3);\n-    __ decrementl(length, 1);\n-    __ jcc(Assembler::zero, L_exit_no_vzero);\n-\n-    __ BIND(L_bottomLoop);\n-    __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n-    __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n-    __ load_signed_byte(byte1, Address(decode_table, byte1));\n-    __ load_signed_byte(byte2, Address(decode_table, byte2));\n-    __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n-    __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n-    __ load_signed_byte(byte3, Address(decode_table, byte3));\n-    __ load_signed_byte(byte4, Address(decode_table, byte4));\n-\n-    __ mov(rax, byte1);\n-    __ orl(rax, byte2);\n-    __ orl(rax, byte3);\n-    __ orl(rax, byte4);\n-    __ jcc(Assembler::positive, L_forceLoop);\n-\n-    __ BIND(L_exit_no_vzero);\n-    __ pop(rax);             \/\/ Get original dest value\n-    __ subptr(dest, rax);      \/\/ Number of bytes converted\n-    __ movptr(rax, dest);\n-    __ pop(rbx);\n-    __ pop(r15);\n-    __ pop(r14);\n-    __ pop(r13);\n-    __ pop(r12);\n-    __ leave();\n-    __ ret(0);\n+  __ BIND(L_exit);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+  __ leave();\n+  __ ret(0);\n@@ -6595,2 +3695,2 @@\n-    return start;\n-  }\n+  return start;\n+}\n@@ -6598,0 +3698,16 @@\n+\/\/ base64 AVX512vbmi tables\n+address StubGenerator::base64_vbmi_lookup_lo_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x3f8080803e808080, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -6599,45 +3715,2 @@\n-  \/**\n-   *  Arguments:\n-   *\n-   * Inputs:\n-   *   c_rarg0   - int crc\n-   *   c_rarg1   - byte* buf\n-   *   c_rarg2   - int length\n-   *\n-   * Output:\n-   *       rax   - int crc result\n-   *\/\n-  address generate_updateBytesCRC32() {\n-    assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ rscratch1: r10\n-    const Register crc   = c_rarg0;  \/\/ crc\n-    const Register buf   = c_rarg1;  \/\/ source java byte array address\n-    const Register len   = c_rarg2;  \/\/ length\n-    const Register table = c_rarg3;  \/\/ crc_table address (reuse register)\n-    const Register tmp1   = r11;\n-    const Register tmp2   = r10;\n-    assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n-        VM_Version::supports_avx512bw() &&\n-        VM_Version::supports_avx512vl()) {\n-        \/\/ The constants used in the CRC32 algorithm requires the 1's compliment of the initial crc value.\n-        \/\/ However, the constant table for CRC32-C assumes the original crc value.  Account for this\n-        \/\/ difference before calling and after returning.\n-      __ lea(table, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));\n-      __ notl(crc);\n-      __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);\n-      __ notl(crc);\n-    } else {\n-      __ kernel_crc32(crc, buf, len, table, tmp1);\n-    }\n+  return start;\n+}\n@@ -6645,4 +3718,15 @@\n-    __ movl(rax, crc);\n-    __ vzeroupper();\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+address StubGenerator::base64_vbmi_lookup_hi_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0605040302010080, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x8080808080191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -6650,2 +3734,17 @@\n-    return start;\n-  }\n+  return start;\n+}\n+address StubGenerator::base64_vbmi_lookup_lo_url_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_lo_base64url\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x8080808080808080, relocInfo::none);\n+  __ emit_data64(0x80803e8080808080, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0x8080808080803d3c, relocInfo::none);\n@@ -6653,63 +3752,2 @@\n-  \/**\n-  *  Arguments:\n-  *\n-  * Inputs:\n-  *   c_rarg0   - int crc\n-  *   c_rarg1   - byte* buf\n-  *   c_rarg2   - long length\n-  *   c_rarg3   - table_start - optional (present only when doing a library_call,\n-  *              not used by x86 algorithm)\n-  *\n-  * Output:\n-  *       rax   - int crc result\n-  *\/\n-  address generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n-      assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n-      __ align(CodeEntryAlignment);\n-      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n-      address start = __ pc();\n-      \/\/reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs\n-      \/\/Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3\n-      \/\/Lin \/ Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7\n-      const Register crc = c_rarg0;  \/\/ crc\n-      const Register buf = c_rarg1;  \/\/ source java byte array address\n-      const Register len = c_rarg2;  \/\/ length\n-      const Register a = rax;\n-      const Register j = r9;\n-      const Register k = r10;\n-      const Register l = r11;\n-#ifdef _WIN64\n-      const Register y = rdi;\n-      const Register z = rsi;\n-#else\n-      const Register y = rcx;\n-      const Register z = r8;\n-#endif\n-      assert_different_registers(crc, buf, len, a, j, k, l, y, z);\n-\n-      BLOCK_COMMENT(\"Entry:\");\n-      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n-          VM_Version::supports_avx512bw() &&\n-          VM_Version::supports_avx512vl()) {\n-        __ lea(j, ExternalAddress(StubRoutines::x86::crc32c_table_avx512_addr()));\n-        __ kernel_crc32_avx512(crc, buf, len, j, l, k);\n-      } else {\n-#ifdef _WIN64\n-        __ push(y);\n-        __ push(z);\n-#endif\n-        __ crc32c_ipl_alg2_alt2(crc, buf, len,\n-                                a, j, k,\n-                                l, y, z,\n-                                c_farg0, c_farg1, c_farg2,\n-                                is_pclmulqdq_supported);\n-#ifdef _WIN64\n-        __ pop(z);\n-        __ pop(y);\n-#endif\n-      }\n-      __ movl(rax, crc);\n-      __ vzeroupper();\n-      __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-      __ ret(0);\n+  return start;\n+}\n@@ -6717,2 +3755,15 @@\n-      return start;\n-  }\n+address StubGenerator::base64_vbmi_lookup_hi_url_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"lookup_hi_base64url\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x0605040302010080, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x3f80808080191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1a80, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0x8080808080333231, relocInfo::none);\n@@ -6720,0 +3771,2 @@\n+  return start;\n+}\n@@ -6721,39 +3774,15 @@\n-  \/***\n-   *  Arguments:\n-   *\n-   *  Inputs:\n-   *   c_rarg0   - int   adler\n-   *   c_rarg1   - byte* buff\n-   *   c_rarg2   - int   len\n-   *\n-   * Output:\n-   *   rax   - int adler result\n-   *\/\n-\n-  address generate_updateBytesAdler32() {\n-      assert(UseAdler32Intrinsics, \"need AVX2\");\n-\n-      __ align(CodeEntryAlignment);\n-      StubCodeMark mark(this, \"StubRoutines\", \"updateBytesAdler32\");\n-\n-      address start = __ pc();\n-\n-      const Register data = r9;\n-      const Register size = r10;\n-\n-      const XMMRegister yshuf0 = xmm6;\n-      const XMMRegister yshuf1 = xmm7;\n-      assert_different_registers(c_rarg0, c_rarg1, c_rarg2, data, size);\n-\n-      BLOCK_COMMENT(\"Entry:\");\n-      __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-      __ vmovdqu(yshuf0, ExternalAddress((address) StubRoutines::x86::_adler32_shuf0_table), r9);\n-      __ vmovdqu(yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_shuf1_table), r9);\n-      __ movptr(data, c_rarg1); \/\/data\n-      __ movl(size, c_rarg2); \/\/length\n-      __ updateBytesAdler32(c_rarg0, data, size, yshuf0, yshuf1, ExternalAddress((address) StubRoutines::x86::_adler32_ascale_table));\n-      __ leave();\n-      __ ret(0);\n-      return start;\n-  }\n+address StubGenerator::base64_vbmi_pack_vec_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"pack_vec_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n+  __ emit_data64(0x0000000000000000, relocInfo::none);\n@@ -6761,38 +3790,2 @@\n-  \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - x address\n-   *    c_rarg1   - x length\n-   *    c_rarg2   - y address\n-   *    c_rarg3   - y length\n-   * not Win64\n-   *    c_rarg4   - z address\n-   *    c_rarg5   - z length\n-   * Win64\n-   *    rsp+40    - z address\n-   *    rsp+48    - z length\n-   *\/\n-  address generate_multiplyToLen() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register x     = rdi;\n-    const Register xlen  = rax;\n-    const Register y     = rsi;\n-    const Register ylen  = rcx;\n-    const Register z     = r8;\n-    const Register zlen  = r11;\n-\n-    \/\/ Next registers will be saved on stack in multiply_to_len().\n-    const Register tmp1  = r12;\n-    const Register tmp2  = r13;\n-    const Register tmp3  = r14;\n-    const Register tmp4  = r15;\n-    const Register tmp5  = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  return start;\n+}\n@@ -6800,11 +3793,15 @@\n-#ifndef _WIN64\n-    __ movptr(zlen, r9); \/\/ Save r9 in r11 - zlen\n-#endif\n-    setup_arg_regs(4); \/\/ x => rdi, xlen => rsi, y => rdx\n-                       \/\/ ylen => rcx, z => r8, zlen => r11\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-#ifdef _WIN64\n-    \/\/ last 2 arguments (#4, #5) are on stack on Win64\n-    __ movptr(z, Address(rsp, 6 * wordSize));\n-    __ movptr(zlen, Address(rsp, 7 * wordSize));\n-#endif\n+address StubGenerator::base64_vbmi_join_0_1_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_0_1_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x090a040506000102, relocInfo::none);\n+  __ emit_data64(0x161011120c0d0e08, relocInfo::none);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n@@ -6812,3 +3809,2 @@\n-    __ movptr(xlen, rsi);\n-    __ movptr(y,    rdx);\n-    __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);\n+  return start;\n+}\n@@ -6816,1 +3812,15 @@\n-    restore_arg_regs();\n+address StubGenerator::base64_vbmi_join_1_2_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_1_2_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x1c1d1e18191a1415, relocInfo::none);\n+  __ emit_data64(0x292a242526202122, relocInfo::none);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+  __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+  __ emit_data64(0x696a646566606162, relocInfo::none);\n@@ -6818,2 +3828,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  return start;\n+}\n@@ -6821,2 +3831,15 @@\n-    return start;\n-  }\n+address StubGenerator::base64_vbmi_join_2_3_addr() {\n+  __ align64();\n+  StubCodeMark mark(this, \"StubRoutines\", \"join_2_3_base64\");\n+  address start = __ pc();\n+\n+  assert(((unsigned long long)start & 0x3f) == 0,\n+         \"Alignment problem (0x%08llx)\", (unsigned long long)start);\n+  __ emit_data64(0x363031322c2d2e28, relocInfo::none);\n+  __ emit_data64(0x3c3d3e38393a3435, relocInfo::none);\n+  __ emit_data64(0x494a444546404142, relocInfo::none);\n+  __ emit_data64(0x565051524c4d4e48, relocInfo::none);\n+  __ emit_data64(0x5c5d5e58595a5455, relocInfo::none);\n+  __ emit_data64(0x696a646566606162, relocInfo::none);\n+  __ emit_data64(0x767071726c6d6e68, relocInfo::none);\n+  __ emit_data64(0x7c7d7e78797a7475, relocInfo::none);\n@@ -6824,19 +3847,2 @@\n-  \/**\n-  *  Arguments:\n-  *\n-  *  Input:\n-  *    c_rarg0   - obja     address\n-  *    c_rarg1   - objb     address\n-  *    c_rarg3   - length   length\n-  *    c_rarg4   - scale    log2_array_indxscale\n-  *\n-  *  Output:\n-  *        rax   - int >= mismatched index, < 0 bitwise complement of tail\n-  *\/\n-  address generate_vectorizedMismatch() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"vectorizedMismatch\");\n-    address start = __ pc();\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter();\n+  return start;\n+}\n@@ -6844,22 +3850,73 @@\n-#ifdef _WIN64  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register scale = c_rarg0;  \/\/rcx, will exchange with r9\n-    const Register objb = c_rarg1;   \/\/rdx\n-    const Register length = c_rarg2; \/\/r8\n-    const Register obja = c_rarg3;   \/\/r9\n-    __ xchgq(obja, scale);  \/\/now obja and scale contains the correct contents\n-\n-    const Register tmp1 = r10;\n-    const Register tmp2 = r11;\n-#endif\n-#ifndef _WIN64 \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register obja = c_rarg0;   \/\/U:rdi\n-    const Register objb = c_rarg1;   \/\/U:rsi\n-    const Register length = c_rarg2; \/\/U:rdx\n-    const Register scale = c_rarg3;  \/\/U:rcx\n-    const Register tmp1 = r8;\n-    const Register tmp2 = r9;\n-#endif\n-    const Register result = rax; \/\/return value\n-    const XMMRegister vec0 = xmm0;\n-    const XMMRegister vec1 = xmm1;\n-    const XMMRegister vec2 = xmm2;\n+address StubGenerator::base64_decoding_table_addr() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"decoding_table_base64\");\n+  address start = __ pc();\n+\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0x3fffffff3effffff, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+  __ emit_data64(0x06050403020100ff, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0xffffffffff191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0xffffffffff333231, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+  \/\/ URL table\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffff3effffffffff, relocInfo::none);\n+  __ emit_data64(0x3b3a393837363534, relocInfo::none);\n+  __ emit_data64(0xffffffffffff3d3c, relocInfo::none);\n+  __ emit_data64(0x06050403020100ff, relocInfo::none);\n+  __ emit_data64(0x0e0d0c0b0a090807, relocInfo::none);\n+  __ emit_data64(0x161514131211100f, relocInfo::none);\n+  __ emit_data64(0x3fffffffff191817, relocInfo::none);\n+  __ emit_data64(0x201f1e1d1c1b1aff, relocInfo::none);\n+  __ emit_data64(0x2827262524232221, relocInfo::none);\n+  __ emit_data64(0x302f2e2d2c2b2a29, relocInfo::none);\n+  __ emit_data64(0xffffffffff333231, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+  __ emit_data64(0xffffffffffffffff, relocInfo::none);\n+\n+  return start;\n+}\n@@ -6867,4 +3924,26 @@\n-    __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);\n-    __ vzeroupper();\n-    __ leave();\n-    __ ret(0);\n+\/\/ Code for generating Base64 decoding.\n+\/\/\n+\/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n+\/\/\n+\/\/ Intrinsic function prototype in Base64.java:\n+\/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME) {\n+address StubGenerator::generate_base64_decodeBlock() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"implDecode\");\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ Save callee-saved registers before using them\n+  __ push(r12);\n+  __ push(r13);\n+  __ push(r14);\n+  __ push(r15);\n+  __ push(rbx);\n+\n+  \/\/ arguments\n+  const Register source = c_rarg0; \/\/ Source Array\n+  const Register start_offset = c_rarg1; \/\/ start offset\n+  const Register end_offset = c_rarg2; \/\/ end offset\n+  const Register dest = c_rarg3; \/\/ destination array\n+  const Register isMIME = rbx;\n@@ -6873,2 +3952,13 @@\n-    return start;\n-  }\n+#ifndef _WIN64\n+  const Register dp = c_rarg4;  \/\/ Position for writing to dest array\n+  const Register isURL = c_rarg5;\/\/ Base64 or URL character set\n+  __ movl(isMIME, Address(rbp, 2 * wordSize));\n+#else\n+  const Address  dp_mem(rbp, 6 * wordSize);  \/\/ length is on stack on Win64\n+  const Address isURL_mem(rbp, 7 * wordSize);\n+  const Register isURL = r10;      \/\/ pick the volatile windows register\n+  const Register dp = r12;\n+  __ movl(dp, dp_mem);\n+  __ movl(isURL, isURL_mem);\n+  __ movl(isMIME, Address(rbp, 8 * wordSize));\n+#endif\n@@ -6876,37 +3966,79 @@\n-\/**\n-   *  Arguments:\n-   *\n-  \/\/  Input:\n-  \/\/    c_rarg0   - x address\n-  \/\/    c_rarg1   - x length\n-  \/\/    c_rarg2   - z address\n-  \/\/    c_rarg3   - z length\n-   *\n-   *\/\n-  address generate_squareToLen() {\n-\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)\n-    const Register x      = rdi;\n-    const Register len    = rsi;\n-    const Register z      = r8;\n-    const Register zlen   = rcx;\n-\n-   const Register tmp1      = r12;\n-   const Register tmp2      = r13;\n-   const Register tmp3      = r14;\n-   const Register tmp4      = r15;\n-   const Register tmp5      = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    setup_arg_regs(4); \/\/ x => rdi, len => rsi, z => rdx\n-                       \/\/ zlen => rcx\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n-    __ movptr(r8, rdx);\n-    __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n+  const XMMRegister lookup_lo = xmm5;\n+  const XMMRegister lookup_hi = xmm6;\n+  const XMMRegister errorvec = xmm7;\n+  const XMMRegister pack16_op = xmm9;\n+  const XMMRegister pack32_op = xmm8;\n+  const XMMRegister input0 = xmm3;\n+  const XMMRegister input1 = xmm20;\n+  const XMMRegister input2 = xmm21;\n+  const XMMRegister input3 = xmm19;\n+  const XMMRegister join01 = xmm12;\n+  const XMMRegister join12 = xmm11;\n+  const XMMRegister join23 = xmm10;\n+  const XMMRegister translated0 = xmm2;\n+  const XMMRegister translated1 = xmm1;\n+  const XMMRegister translated2 = xmm0;\n+  const XMMRegister translated3 = xmm4;\n+\n+  const XMMRegister merged0 = xmm2;\n+  const XMMRegister merged1 = xmm1;\n+  const XMMRegister merged2 = xmm0;\n+  const XMMRegister merged3 = xmm4;\n+  const XMMRegister merge_ab_bc0 = xmm2;\n+  const XMMRegister merge_ab_bc1 = xmm1;\n+  const XMMRegister merge_ab_bc2 = xmm0;\n+  const XMMRegister merge_ab_bc3 = xmm4;\n+\n+  const XMMRegister pack24bits = xmm4;\n+\n+  const Register length = r14;\n+  const Register output_size = r13;\n+  const Register output_mask = r15;\n+  const KRegister input_mask = k1;\n+\n+  const XMMRegister input_initial_valid_b64 = xmm0;\n+  const XMMRegister tmp = xmm10;\n+  const XMMRegister mask = xmm0;\n+  const XMMRegister invalid_b64 = xmm1;\n+\n+  Label L_process256, L_process64, L_process64Loop, L_exit, L_processdata, L_loadURL;\n+  Label L_continue, L_finalBit, L_padding, L_donePadding, L_bruteForce;\n+  Label L_forceLoop, L_bottomLoop, L_checkMIME, L_exit_no_vzero;\n+\n+  \/\/ calculate length from offsets\n+  __ movl(length, end_offset);\n+  __ subl(length, start_offset);\n+  __ push(dest);          \/\/ Save for return value calc\n+\n+  \/\/ If AVX512 VBMI not supported, just compile non-AVX code\n+  if(VM_Version::supports_avx512_vbmi() &&\n+     VM_Version::supports_avx512bw()) {\n+    __ cmpl(length, 128);     \/\/ 128-bytes is break-even for AVX-512\n+    __ jcc(Assembler::lessEqual, L_bruteForce);\n+\n+    __ cmpl(isMIME, 0);\n+    __ jcc(Assembler::notEqual, L_bruteForce);\n+\n+    \/\/ Load lookup tables based on isURL\n+    __ cmpl(isURL, 0);\n+    __ jcc(Assembler::notZero, L_loadURL);\n+\n+    __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_addr()), Assembler::AVX_512bit, r13);\n+\n+    __ BIND(L_continue);\n+\n+    __ movl(r15, 0x01400140);\n+    __ evpbroadcastd(pack16_op, r15, Assembler::AVX_512bit);\n+\n+    __ movl(r15, 0x00011000);\n+    __ evpbroadcastd(pack32_op, r15, Assembler::AVX_512bit);\n+\n+    __ cmpl(length, 0xff);\n+    __ jcc(Assembler::lessEqual, L_process64);\n+\n+    \/\/ load masks required for decoding data\n+    __ BIND(L_processdata);\n+    __ evmovdquq(join01, ExternalAddress(StubRoutines::x86::base64_vbmi_join_0_1_addr()), Assembler::AVX_512bit,r13);\n+    __ evmovdquq(join12, ExternalAddress(StubRoutines::x86::base64_vbmi_join_1_2_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(join23, ExternalAddress(StubRoutines::x86::base64_vbmi_join_2_3_addr()), Assembler::AVX_512bit, r13);\n@@ -6914,1 +4046,74 @@\n-    restore_arg_regs();\n+    __ align32();\n+    __ BIND(L_process256);\n+    \/\/ Grab input data\n+    __ evmovdquq(input0, Address(source, start_offset, Address::times_1, 0x00), Assembler::AVX_512bit);\n+    __ evmovdquq(input1, Address(source, start_offset, Address::times_1, 0x40), Assembler::AVX_512bit);\n+    __ evmovdquq(input2, Address(source, start_offset, Address::times_1, 0x80), Assembler::AVX_512bit);\n+    __ evmovdquq(input3, Address(source, start_offset, Address::times_1, 0xc0), Assembler::AVX_512bit);\n+\n+    \/\/ Copy the low part of the lookup table into the destination of the permutation\n+    __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated1, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated2, lookup_lo, Assembler::AVX_512bit);\n+    __ evmovdquq(translated3, lookup_lo, Assembler::AVX_512bit);\n+\n+    \/\/ Translate the base64 input into \"decoded\" bytes\n+    __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated1, input1, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated2, input2, lookup_hi, Assembler::AVX_512bit);\n+    __ evpermt2b(translated3, input3, lookup_hi, Assembler::AVX_512bit);\n+\n+    \/\/ OR all of the translations together to check for errors (high-order bit of byte set)\n+    __ vpternlogd(input0, 0xfe, input1, input2, Assembler::AVX_512bit);\n+\n+    __ vpternlogd(input3, 0xfe, translated0, translated1, Assembler::AVX_512bit);\n+    __ vpternlogd(input0, 0xfe, translated2, translated3, Assembler::AVX_512bit);\n+    __ vpor(errorvec, input3, input0, Assembler::AVX_512bit);\n+\n+    \/\/ Check if there was an error - if so, try 64-byte chunks\n+    __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+    __ kortestql(k3, k3);\n+    __ jcc(Assembler::notZero, L_process64);\n+\n+    \/\/ The merging and shuffling happens here\n+    \/\/ We multiply each byte pair [00dddddd | 00cccccc | 00bbbbbb | 00aaaaaa]\n+    \/\/ Multiply [00cccccc] by 2^6 added to [00dddddd] to get [0000cccc | ccdddddd]\n+    \/\/ The pack16_op is a vector of 0x01400140, so multiply D by 1 and C by 0x40\n+    __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc1, translated1, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc2, translated2, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddubsw(merge_ab_bc3, translated3, pack16_op, Assembler::AVX_512bit);\n+\n+    \/\/ Now do the same with packed 16-bit values.\n+    \/\/ We start with [0000cccc | ccdddddd | 0000aaaa | aabbbbbb]\n+    \/\/ pack32_op is 0x00011000 (2^12, 1), so this multiplies [0000aaaa | aabbbbbb] by 2^12\n+    \/\/ and adds [0000cccc | ccdddddd] to yield [00000000 | aaaaaabb | bbbbcccc | ccdddddd]\n+    __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged1, merge_ab_bc1, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged2, merge_ab_bc2, pack32_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged3, merge_ab_bc3, pack32_op, Assembler::AVX_512bit);\n+\n+    \/\/ The join vectors specify which byte from which vector goes into the outputs\n+    \/\/ One of every 4 bytes in the extended vector is zero, so we pack them into their\n+    \/\/ final positions in the register for storing (256 bytes in, 192 bytes out)\n+    __ evpermt2b(merged0, join01, merged1, Assembler::AVX_512bit);\n+    __ evpermt2b(merged1, join12, merged2, Assembler::AVX_512bit);\n+    __ evpermt2b(merged2, join23, merged3, Assembler::AVX_512bit);\n+\n+    \/\/ Store result\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x00), merged0, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x40), merged1, Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dest, dp, Address::times_1, 0x80), merged2, Assembler::AVX_512bit);\n+\n+    __ addptr(source, 0x100);\n+    __ addptr(dest, 0xc0);\n+    __ subl(length, 0x100);\n+    __ cmpl(length, 64 * 4);\n+    __ jcc(Assembler::greaterEqual, L_process256);\n+\n+    \/\/ At this point, we've decoded 64 * 4 * n bytes.\n+    \/\/ The remaining length will be <= 64 * 4 - 1.\n+    \/\/ UNLESS there was an error decoding the first 256-byte chunk.  In this\n+    \/\/ case, the length will be arbitrarily long.\n+    \/\/\n+    \/\/ Note that this will be the path for MIME-encoded strings.\n@@ -6916,2 +4121,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+    __ BIND(L_process64);\n@@ -6919,2 +4123,1 @@\n-    return start;\n-  }\n+    __ evmovdquq(pack24bits, ExternalAddress(StubRoutines::x86::base64_vbmi_pack_vec_addr()), Assembler::AVX_512bit, r13);\n@@ -6922,3 +4125,2 @@\n-  address generate_method_entry_barrier() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+    __ cmpl(length, 63);\n+    __ jcc(Assembler::lessEqual, L_finalBit);\n@@ -6926,1 +4128,2 @@\n-    Label deoptimize_label;\n+    __ mov64(rax, 0x0000ffffffffffff);\n+    __ kmovql(k2, rax);\n@@ -6928,1 +4131,2 @@\n-    address start = __ pc();\n+    __ align32();\n+    __ BIND(L_process64Loop);\n@@ -6930,1 +4134,1 @@\n-    __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n+    \/\/ Handle first 64-byte block\n@@ -6932,2 +4136,3 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ save rbp\n+    __ evmovdquq(input0, Address(source, start_offset), Assembler::AVX_512bit);\n+    __ evmovdquq(translated0, lookup_lo, Assembler::AVX_512bit);\n+    __ evpermt2b(translated0, input0, lookup_hi, Assembler::AVX_512bit);\n@@ -6935,3 +4140,1 @@\n-    \/\/ save c_rarg0, because we want to use that value.\n-    \/\/ We could do without it but then we depend on the number of slots used by pusha\n-    __ push(c_rarg0);\n+    __ vpor(errorvec, translated0, input0, Assembler::AVX_512bit);\n@@ -6939,1 +4142,4 @@\n-    __ lea(c_rarg0, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address\n+    \/\/ Check for error and bomb out before updating dest\n+    __ evpmovb2m(k3, errorvec, Assembler::AVX_512bit);\n+    __ kortestql(k3, k3);\n+    __ jcc(Assembler::notZero, L_exit);\n@@ -6941,1 +4147,4 @@\n-    __ pusha();\n+    \/\/ Pack output register, selecting correct byte ordering\n+    __ vpmaddubsw(merge_ab_bc0, translated0, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(merged0, merge_ab_bc0, pack32_op, Assembler::AVX_512bit);\n+    __ vpermb(merged0, pack24bits, merged0, Assembler::AVX_512bit);\n@@ -6943,14 +4152,1 @@\n-    \/\/ The method may have floats as arguments, and we must spill them before calling\n-    \/\/ the VM runtime.\n-    assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n-    const int xmm_size = wordSize * 2;\n-    const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n-    __ subptr(rsp, xmm_spill_size);\n-    __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n-    __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n-    __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n-    __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n-    __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n-    __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n-    __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n-    __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+    __ evmovdqub(Address(dest, dp), k2, merged0, true, Assembler::AVX_512bit);\n@@ -6958,1 +4154,3 @@\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);\n+    __ subl(length, 64);\n+    __ addptr(source, 64);\n+    __ addptr(dest, 48);\n@@ -6960,9 +4158,2 @@\n-    __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n-    __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n-    __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n-    __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n-    __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n-    __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n-    __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n-    __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n-    __ addptr(rsp, xmm_spill_size);\n+    __ cmpl(length, 64);\n+    __ jcc(Assembler::greaterEqual, L_process64Loop);\n@@ -6970,2 +4161,2 @@\n-    __ cmpl(rax, 1); \/\/ 1 means deoptimize\n-    __ jcc(Assembler::equal, deoptimize_label);\n+    __ cmpl(length, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n@@ -6973,2 +4164,73 @@\n-    __ popa();\n-    __ pop(c_rarg0);\n+    __ BIND(L_finalBit);\n+    \/\/ Now have 1 to 63 bytes left to decode\n+\n+    \/\/ I was going to let Java take care of the final fragment\n+    \/\/ however it will repeatedly call this routine for every 4 bytes\n+    \/\/ of input data, so handle the rest here.\n+    __ movq(rax, -1);\n+    __ bzhiq(rax, rax, length);    \/\/ Input mask in rax\n+\n+    __ movl(output_size, length);\n+    __ shrl(output_size, 2);   \/\/ Find (len \/ 4) * 3 (output length)\n+    __ lea(output_size, Address(output_size, output_size, Address::times_2, 0));\n+    \/\/ output_size in r13\n+\n+    \/\/ Strip pad characters, if any, and adjust length and mask\n+    __ cmpb(Address(source, length, Address::times_1, -1), '=');\n+    __ jcc(Assembler::equal, L_padding);\n+\n+    __ BIND(L_donePadding);\n+\n+    \/\/ Output size is (64 - output_size), output mask is (all 1s >> output_size).\n+    __ kmovql(input_mask, rax);\n+    __ movq(output_mask, -1);\n+    __ bzhiq(output_mask, output_mask, output_size);\n+\n+    \/\/ Load initial input with all valid base64 characters.  Will be used\n+    \/\/ in merging source bytes to avoid masking when determining if an error occurred.\n+    __ movl(rax, 0x61616161);\n+    __ evpbroadcastd(input_initial_valid_b64, rax, Assembler::AVX_512bit);\n+\n+    \/\/ A register containing all invalid base64 decoded values\n+    __ movl(rax, 0x80808080);\n+    __ evpbroadcastd(invalid_b64, rax, Assembler::AVX_512bit);\n+\n+    \/\/ input_mask is in k1\n+    \/\/ output_size is in r13\n+    \/\/ output_mask is in r15\n+    \/\/ zmm0 - free\n+    \/\/ zmm1 - 0x00011000\n+    \/\/ zmm2 - 0x01400140\n+    \/\/ zmm3 - errorvec\n+    \/\/ zmm4 - pack vector\n+    \/\/ zmm5 - lookup_lo\n+    \/\/ zmm6 - lookup_hi\n+    \/\/ zmm7 - errorvec\n+    \/\/ zmm8 - 0x61616161\n+    \/\/ zmm9 - 0x80808080\n+\n+    \/\/ Load only the bytes from source, merging into our \"fully-valid\" register\n+    __ evmovdqub(input_initial_valid_b64, input_mask, Address(source, start_offset, Address::times_1, 0x0), true, Assembler::AVX_512bit);\n+\n+    \/\/ Decode all bytes within our merged input\n+    __ evmovdquq(tmp, lookup_lo, Assembler::AVX_512bit);\n+    __ evpermt2b(tmp, input_initial_valid_b64, lookup_hi, Assembler::AVX_512bit);\n+    __ vporq(mask, tmp, input_initial_valid_b64, Assembler::AVX_512bit);\n+\n+    \/\/ Check for error.  Compare (decoded | initial) to all invalid.\n+    \/\/ If any bytes have their high-order bit set, then we have an error.\n+    __ evptestmb(k2, mask, invalid_b64, Assembler::AVX_512bit);\n+    __ kortestql(k2, k2);\n+\n+    \/\/ If we have an error, use the brute force loop to decode what we can (4-byte chunks).\n+    __ jcc(Assembler::notZero, L_bruteForce);\n+\n+    \/\/ Shuffle output bytes\n+    __ vpmaddubsw(tmp, tmp, pack16_op, Assembler::AVX_512bit);\n+    __ vpmaddwd(tmp, tmp, pack32_op, Assembler::AVX_512bit);\n+\n+    __ vpermb(tmp, pack24bits, tmp, Assembler::AVX_512bit);\n+    __ kmovql(k1, output_mask);\n+    __ evmovdqub(Address(dest, dp), k1, tmp, true, Assembler::AVX_512bit);\n+\n+    __ addptr(dest, output_size);\n@@ -6976,0 +4238,10 @@\n+    __ BIND(L_exit);\n+    __ vzeroupper();\n+    __ pop(rax);             \/\/ Get original dest value\n+    __ subptr(dest, rax);      \/\/ Number of bytes converted\n+    __ movptr(rax, dest);\n+    __ pop(rbx);\n+    __ pop(r15);\n+    __ pop(r14);\n+    __ pop(r13);\n+    __ pop(r12);\n@@ -6977,2 +4249,0 @@\n-\n-    __ addptr(rsp, 1 * wordSize); \/\/ cookie\n@@ -6981,0 +4251,4 @@\n+    __ BIND(L_loadURL);\n+    __ evmovdquq(lookup_lo, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_lo_url_addr()), Assembler::AVX_512bit, r13);\n+    __ evmovdquq(lookup_hi, ExternalAddress(StubRoutines::x86::base64_vbmi_lookup_hi_url_addr()), Assembler::AVX_512bit, r13);\n+    __ jmp(L_continue);\n@@ -6982,1 +4256,3 @@\n-    __ BIND(deoptimize_label);\n+    __ BIND(L_padding);\n+    __ decrementq(output_size, 1);\n+    __ shrq(rax, 1);\n@@ -6984,2 +4260,2 @@\n-    __ popa();\n-    __ pop(c_rarg0);\n+    __ cmpb(Address(source, length, Address::times_1, -2), '=');\n+    __ jcc(Assembler::notEqual, L_donePadding);\n@@ -6987,1 +4263,220 @@\n-    __ leave();\n+    __ decrementq(output_size, 1);\n+    __ shrq(rax, 1);\n+    __ jmp(L_donePadding);\n+\n+    __ align32();\n+    __ BIND(L_bruteForce);\n+  }   \/\/ End of if(avx512_vbmi)\n+\n+  \/\/ Use non-AVX code to decode 4-byte chunks into 3 bytes of output\n+\n+  \/\/ Register state (Linux):\n+  \/\/ r12-15 - saved on stack\n+  \/\/ rdi - src\n+  \/\/ rsi - sp\n+  \/\/ rdx - sl\n+  \/\/ rcx - dst\n+  \/\/ r8 - dp\n+  \/\/ r9 - isURL\n+\n+  \/\/ Register state (Windows):\n+  \/\/ r12-15 - saved on stack\n+  \/\/ rcx - src\n+  \/\/ rdx - sp\n+  \/\/ r8 - sl\n+  \/\/ r9 - dst\n+  \/\/ r12 - dp\n+  \/\/ r10 - isURL\n+\n+  \/\/ Registers (common):\n+  \/\/ length (r14) - bytes in src\n+\n+  const Register decode_table = r11;\n+  const Register out_byte_count = rbx;\n+  const Register byte1 = r13;\n+  const Register byte2 = r15;\n+  const Register byte3 = WIN64_ONLY(r8) NOT_WIN64(rdx);\n+  const Register byte4 = WIN64_ONLY(r10) NOT_WIN64(r9);\n+\n+  __ shrl(length, 2);    \/\/ Multiple of 4 bytes only - length is # 4-byte chunks\n+  __ cmpl(length, 0);\n+  __ jcc(Assembler::lessEqual, L_exit_no_vzero);\n+\n+  __ shll(isURL, 8);    \/\/ index into decode table based on isURL\n+  __ lea(decode_table, ExternalAddress(StubRoutines::x86::base64_decoding_table_addr()));\n+  __ addptr(decode_table, isURL);\n+\n+  __ jmp(L_bottomLoop);\n+\n+  __ align32();\n+  __ BIND(L_forceLoop);\n+  __ shll(byte1, 18);\n+  __ shll(byte2, 12);\n+  __ shll(byte3, 6);\n+  __ orl(byte1, byte2);\n+  __ orl(byte1, byte3);\n+  __ orl(byte1, byte4);\n+\n+  __ addptr(source, 4);\n+\n+  __ movb(Address(dest, dp, Address::times_1, 2), byte1);\n+  __ shrl(byte1, 8);\n+  __ movb(Address(dest, dp, Address::times_1, 1), byte1);\n+  __ shrl(byte1, 8);\n+  __ movb(Address(dest, dp, Address::times_1, 0), byte1);\n+\n+  __ addptr(dest, 3);\n+  __ decrementl(length, 1);\n+  __ jcc(Assembler::zero, L_exit_no_vzero);\n+\n+  __ BIND(L_bottomLoop);\n+  __ load_unsigned_byte(byte1, Address(source, start_offset, Address::times_1, 0x00));\n+  __ load_unsigned_byte(byte2, Address(source, start_offset, Address::times_1, 0x01));\n+  __ load_signed_byte(byte1, Address(decode_table, byte1));\n+  __ load_signed_byte(byte2, Address(decode_table, byte2));\n+  __ load_unsigned_byte(byte3, Address(source, start_offset, Address::times_1, 0x02));\n+  __ load_unsigned_byte(byte4, Address(source, start_offset, Address::times_1, 0x03));\n+  __ load_signed_byte(byte3, Address(decode_table, byte3));\n+  __ load_signed_byte(byte4, Address(decode_table, byte4));\n+\n+  __ mov(rax, byte1);\n+  __ orl(rax, byte2);\n+  __ orl(rax, byte3);\n+  __ orl(rax, byte4);\n+  __ jcc(Assembler::positive, L_forceLoop);\n+\n+  __ BIND(L_exit_no_vzero);\n+  __ pop(rax);             \/\/ Get original dest value\n+  __ subptr(dest, rax);      \/\/ Number of bytes converted\n+  __ movptr(rax, dest);\n+  __ pop(rbx);\n+  __ pop(r15);\n+  __ pop(r14);\n+  __ pop(r13);\n+  __ pop(r12);\n+  __ leave();\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/**\n+ *  Arguments:\n+ *\n+ * Inputs:\n+ *   c_rarg0   - int crc\n+ *   c_rarg1   - byte* buf\n+ *   c_rarg2   - int length\n+ *\n+ * Output:\n+ *       rax   - int crc result\n+ *\/\n+address StubGenerator::generate_updateBytesCRC32() {\n+  assert(UseCRC32Intrinsics, \"need AVX and CLMUL instructions\");\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32\");\n+\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ rscratch1: r10\n+  const Register crc   = c_rarg0;  \/\/ crc\n+  const Register buf   = c_rarg1;  \/\/ source java byte array address\n+  const Register len   = c_rarg2;  \/\/ length\n+  const Register table = c_rarg3;  \/\/ crc_table address (reuse register)\n+  const Register tmp1   = r11;\n+  const Register tmp2   = r10;\n+  assert_different_registers(crc, buf, len, table, tmp1, tmp2, rax);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n+      VM_Version::supports_avx512bw() &&\n+      VM_Version::supports_avx512vl()) {\n+      \/\/ The constants used in the CRC32 algorithm requires the 1's compliment of the initial crc value.\n+      \/\/ However, the constant table for CRC32-C assumes the original crc value.  Account for this\n+      \/\/ difference before calling and after returning.\n+    __ lea(table, ExternalAddress(StubRoutines::x86::crc_table_avx512_addr()));\n+    __ notl(crc);\n+    __ kernel_crc32_avx512(crc, buf, len, table, tmp1, tmp2);\n+    __ notl(crc);\n+  } else {\n+    __ kernel_crc32(crc, buf, len, table, tmp1);\n+  }\n+\n+  __ movl(rax, crc);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/**\n+*  Arguments:\n+*\n+* Inputs:\n+*   c_rarg0   - int crc\n+*   c_rarg1   - byte* buf\n+*   c_rarg2   - long length\n+*   c_rarg3   - table_start - optional (present only when doing a library_call,\n+*              not used by x86 algorithm)\n+*\n+* Output:\n+*       rax   - int crc result\n+*\/\n+address StubGenerator::generate_updateBytesCRC32C(bool is_pclmulqdq_supported) {\n+  assert(UseCRC32CIntrinsics, \"need SSE4_2\");\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"updateBytesCRC32C\");\n+  address start = __ pc();\n+\n+  \/\/reg.arg        int#0        int#1        int#2        int#3        int#4        int#5        float regs\n+  \/\/Windows        RCX          RDX          R8           R9           none         none         XMM0..XMM3\n+  \/\/Lin \/ Sol      RDI          RSI          RDX          RCX          R8           R9           XMM0..XMM7\n+  const Register crc = c_rarg0;  \/\/ crc\n+  const Register buf = c_rarg1;  \/\/ source java byte array address\n+  const Register len = c_rarg2;  \/\/ length\n+  const Register a = rax;\n+  const Register j = r9;\n+  const Register k = r10;\n+  const Register l = r11;\n+#ifdef _WIN64\n+  const Register y = rdi;\n+  const Register z = rsi;\n+#else\n+  const Register y = rcx;\n+  const Register z = r8;\n+#endif\n+  assert_different_registers(crc, buf, len, a, j, k, l, y, z);\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  if (VM_Version::supports_sse4_1() && VM_Version::supports_avx512_vpclmulqdq() &&\n+      VM_Version::supports_avx512bw() &&\n+      VM_Version::supports_avx512vl()) {\n+    __ lea(j, ExternalAddress(StubRoutines::x86::crc32c_table_avx512_addr()));\n+    __ kernel_crc32_avx512(crc, buf, len, j, l, k);\n+  } else {\n+#ifdef _WIN64\n+    __ push(y);\n+    __ push(z);\n+#endif\n+    __ crc32c_ipl_alg2_alt2(crc, buf, len,\n+                            a, j, k,\n+                            l, y, z,\n+                            c_farg0, c_farg1, c_farg2,\n+                            is_pclmulqdq_supported);\n+#ifdef _WIN64\n+    __ pop(z);\n+    __ pop(y);\n+#endif\n+  }\n+  __ movl(rax, crc);\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -6989,3 +4484,2 @@\n-    \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n-    \/\/ here while still having a correct stack is valuable\n-    __ testptr(rsp, Address(rsp, 0));\n+  return start;\n+}\n@@ -6993,4 +4487,38 @@\n-    __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n-    __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n-    return start;\n-  }\n+\/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - x address\n+ *    c_rarg1   - x length\n+ *    c_rarg2   - y address\n+ *    c_rarg3   - y length\n+ * not Win64\n+ *    c_rarg4   - z address\n+ *    c_rarg5   - z length\n+ * Win64\n+ *    rsp+40    - z address\n+ *    rsp+48    - z length\n+ *\/\n+address StubGenerator::generate_multiplyToLen() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"multiplyToLen\");\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register x     = rdi;\n+  const Register xlen  = rax;\n+  const Register y     = rsi;\n+  const Register ylen  = rcx;\n+  const Register z     = r8;\n+  const Register zlen  = r11;\n+\n+  \/\/ Next registers will be saved on stack in multiply_to_len().\n+  const Register tmp1  = r12;\n+  const Register tmp2  = r13;\n+  const Register tmp3  = r14;\n+  const Register tmp4  = r15;\n+  const Register tmp5  = rbx;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -6999,39 +4527,6 @@\n-   \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - out address\n-   *    c_rarg1   - in address\n-   *    c_rarg2   - offset\n-   *    c_rarg3   - len\n-   * not Win64\n-   *    c_rarg4   - k\n-   * Win64\n-   *    rsp+40    - k\n-   *\/\n-  address generate_mulAdd() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n-\n-    address start = __ pc();\n-    \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n-    const Register out     = rdi;\n-    const Register in      = rsi;\n-    const Register offset  = r11;\n-    const Register len     = rcx;\n-    const Register k       = r8;\n-\n-    \/\/ Next registers will be saved on stack in mul_add().\n-    const Register tmp1  = r12;\n-    const Register tmp2  = r13;\n-    const Register tmp3  = r14;\n-    const Register tmp4  = r15;\n-    const Register tmp5  = rbx;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    setup_arg_regs(4); \/\/ out => rdi, in => rsi, offset => rdx\n-                       \/\/ len => rcx, k => r8\n-                       \/\/ r9 and r10 may be used to save non-volatile registers\n+#ifndef _WIN64\n+  __ movptr(zlen, r9); \/\/ Save r9 in r11 - zlen\n+#endif\n+  setup_arg_regs(4); \/\/ x => rdi, xlen => rsi, y => rdx\n+                     \/\/ ylen => rcx, z => r8, zlen => r11\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -7039,2 +4534,3 @@\n-    \/\/ last argument is on stack on Win64\n-    __ movl(k, Address(rsp, 6 * wordSize));\n+  \/\/ last 2 arguments (#4, #5) are on stack on Win64\n+  __ movptr(z, Address(rsp, 6 * wordSize));\n+  __ movptr(zlen, Address(rsp, 7 * wordSize));\n@@ -7042,3 +4538,3 @@\n-    __ movptr(r11, rdx);  \/\/ move offset in rdx to offset(r11)\n-    __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n-    restore_arg_regs();\n+  __ movptr(xlen, rsi);\n+  __ movptr(y,    rdx);\n+  __ multiply_to_len(x, xlen, y, ylen, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5);\n@@ -7047,2 +4543,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  restore_arg_regs();\n@@ -7050,2 +4545,2 @@\n-    return start;\n-  }\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7053,59 +4548,2 @@\n-  address generate_bigIntegerRightShift() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n-\n-    address start = __ pc();\n-    Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n-    \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n-    const Register newArr = rdi;\n-    const Register oldArr = rsi;\n-    const Register newIdx = rdx;\n-    const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n-    const Register totalNumIter = r8;\n-\n-    \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n-    \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n-    const Register tmp1 = r11;                    \/\/ Caller save.\n-    const Register tmp2 = rax;                    \/\/ Caller save.\n-    const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp5 = r14;                    \/\/ Callee save.\n-    const Register tmp6 = r15;\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WINDOWS\n-    setup_arg_regs(4);\n-    \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n-    __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n-    \/\/ Save callee save registers.\n-    __ push(tmp3);\n-    __ push(tmp4);\n-#endif\n-    __ push(tmp5);\n-\n-    \/\/ Rename temps used throughout the code.\n-    const Register idx = tmp1;\n-    const Register nIdx = tmp2;\n-\n-    __ xorl(idx, idx);\n-\n-    \/\/ Start right shift from end of the array.\n-    \/\/ For example, if #iteration = 4 and newIdx = 1\n-    \/\/ then dest[4] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n-    \/\/ if #iteration = 4 and newIdx = 0\n-    \/\/ then dest[3] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n-    __ movl(idx, totalNumIter);\n-    __ movl(nIdx, idx);\n-    __ addl(nIdx, newIdx);\n-\n-    \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n-    \/\/ If not, then go to ShifTwo processing 2 iterations\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      __ cmpptr(totalNumIter, (AVX3Threshold\/64));\n-      __ jcc(Assembler::less, ShiftTwo);\n+  return start;\n+}\n@@ -7113,60 +4551,19 @@\n-      if (AVX3Threshold < 16 * 64) {\n-        __ cmpl(totalNumIter, 16);\n-        __ jcc(Assembler::less, ShiftTwo);\n-      }\n-      __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n-      __ subl(idx, 16);\n-      __ subl(nIdx, 16);\n-      __ BIND(Shift512Loop);\n-      __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);\n-      __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n-      __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);\n-      __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);\n-      __ subl(nIdx, 16);\n-      __ subl(idx, 16);\n-      __ jcc(Assembler::greaterEqual, Shift512Loop);\n-      __ addl(idx, 16);\n-      __ addl(nIdx, 16);\n-    }\n-    __ BIND(ShiftTwo);\n-    __ cmpl(idx, 2);\n-    __ jcc(Assembler::less, ShiftOne);\n-    __ subl(idx, 2);\n-    __ subl(nIdx, 2);\n-    __ BIND(ShiftTwoLoop);\n-    __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ shrdl(tmp5, tmp4);\n-    __ shrdl(tmp4, tmp3);\n-    __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);\n-    __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n-    __ subl(nIdx, 2);\n-    __ subl(idx, 2);\n-    __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n-    __ addl(idx, 2);\n-    __ addl(nIdx, 2);\n-\n-    \/\/ Do the last iteration\n-    __ BIND(ShiftOne);\n-    __ cmpl(idx, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ subl(idx, 1);\n-    __ subl(nIdx, 1);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ shrdl(tmp4, tmp3);\n-    __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n-    __ BIND(Exit);\n-    __ vzeroupper();\n-    \/\/ Restore callee save registers.\n-    __ pop(tmp5);\n-#ifdef _WINDOWS\n-    __ pop(tmp4);\n-    __ pop(tmp3);\n-    restore_arg_regs();\n-#endif\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+\/**\n+*  Arguments:\n+*\n+*  Input:\n+*    c_rarg0   - obja     address\n+*    c_rarg1   - objb     address\n+*    c_rarg3   - length   length\n+*    c_rarg4   - scale    log2_array_indxscale\n+*\n+*  Output:\n+*        rax   - int >= mismatched index, < 0 bitwise complement of tail\n+*\/\n+address StubGenerator::generate_vectorizedMismatch() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"vectorizedMismatch\");\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter();\n@@ -7174,45 +4571,9 @@\n-   \/**\n-   *  Arguments:\n-   *\n-   *  Input:\n-   *    c_rarg0   - newArr address\n-   *    c_rarg1   - oldArr address\n-   *    c_rarg2   - newIdx\n-   *    c_rarg3   - shiftCount\n-   * not Win64\n-   *    c_rarg4   - numIter\n-   * Win64\n-   *    rsp40    - numIter\n-   *\/\n-  address generate_bigIntegerLeftShift() {\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n-    address start = __ pc();\n-    Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n-    \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n-    const Register newArr = rdi;\n-    const Register oldArr = rsi;\n-    const Register newIdx = rdx;\n-    const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n-    const Register totalNumIter = r8;\n-    \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n-    \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n-    const Register tmp1 = r11;                    \/\/ Caller save.\n-    const Register tmp2 = rax;                    \/\/ Caller save.\n-    const Register tmp3 = WINDOWS_ONLY(r12) NOT_WINDOWS(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp4 = WINDOWS_ONLY(r13) NOT_WINDOWS(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n-    const Register tmp5 = r14;                    \/\/ Callee save.\n-\n-    const XMMRegister x0 = xmm0;\n-    const XMMRegister x1 = xmm1;\n-    const XMMRegister x2 = xmm2;\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-#ifdef _WINDOWS\n-    setup_arg_regs(4);\n-    \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n-    __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n-    \/\/ Save callee save registers.\n-    __ push(tmp3);\n-    __ push(tmp4);\n+#ifdef _WIN64  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register scale = c_rarg0;  \/\/rcx, will exchange with r9\n+  const Register objb = c_rarg1;   \/\/rdx\n+  const Register length = c_rarg2; \/\/r8\n+  const Register obja = c_rarg3;   \/\/r9\n+  __ xchgq(obja, scale);  \/\/now obja and scale contains the correct contents\n+\n+  const Register tmp1 = r10;\n+  const Register tmp2 = r11;\n@@ -7220,70 +4581,7 @@\n-    __ push(tmp5);\n-\n-    \/\/ Rename temps used throughout the code\n-    const Register idx = tmp1;\n-    const Register numIterTmp = tmp2;\n-\n-    \/\/ Start idx from zero.\n-    __ xorl(idx, idx);\n-    \/\/ Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.\n-    __ lea(newArr, Address(newArr, newIdx, Address::times_4));\n-    __ movl(numIterTmp, totalNumIter);\n-\n-    \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n-    \/\/ If not, then go to ShiftTwo shifting two numbers at a time\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      __ cmpl(totalNumIter, (AVX3Threshold\/64));\n-      __ jcc(Assembler::less, ShiftTwo);\n-\n-      if (AVX3Threshold < 16 * 64) {\n-        __ cmpl(totalNumIter, 16);\n-        __ jcc(Assembler::less, ShiftTwo);\n-      }\n-      __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n-      __ subl(numIterTmp, 16);\n-      __ BIND(Shift512Loop);\n-      __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n-      __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);\n-      __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);\n-      __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);\n-      __ addl(idx, 16);\n-      __ subl(numIterTmp, 16);\n-      __ jcc(Assembler::greaterEqual, Shift512Loop);\n-      __ addl(numIterTmp, 16);\n-    }\n-    __ BIND(ShiftTwo);\n-    __ cmpl(totalNumIter, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n-    __ subl(numIterTmp, 2);\n-    __ jcc(Assembler::less, ShiftOne);\n-\n-    __ BIND(ShiftTwoLoop);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n-    __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));\n-    __ shldl(tmp3, tmp4);\n-    __ shldl(tmp4, tmp5);\n-    __ movl(Address(newArr, idx, Address::times_4), tmp3);\n-    __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);\n-    __ movl(tmp3, tmp5);\n-    __ addl(idx, 2);\n-    __ subl(numIterTmp, 2);\n-    __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n-\n-    \/\/ Do the last iteration\n-    __ BIND(ShiftOne);\n-    __ addl(numIterTmp, 2);\n-    __ cmpl(numIterTmp, 1);\n-    __ jcc(Assembler::less, Exit);\n-    __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n-    __ shldl(tmp3, tmp4);\n-    __ movl(Address(newArr, idx, Address::times_4), tmp3);\n-\n-    __ BIND(Exit);\n-    __ vzeroupper();\n-    \/\/ Restore callee save registers.\n-    __ pop(tmp5);\n-#ifdef _WINDOWS\n-    __ pop(tmp4);\n-    __ pop(tmp3);\n-    restore_arg_regs();\n+#ifndef _WIN64 \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register obja = c_rarg0;   \/\/U:rdi\n+  const Register objb = c_rarg1;   \/\/U:rsi\n+  const Register length = c_rarg2; \/\/U:rdx\n+  const Register scale = c_rarg3;  \/\/U:rcx\n+  const Register tmp1 = r8;\n+  const Register tmp2 = r9;\n@@ -7291,4 +4589,4 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-    return start;\n-  }\n+  const Register result = rax; \/\/return value\n+  const XMMRegister vec0 = xmm0;\n+  const XMMRegister vec1 = xmm1;\n+  const XMMRegister vec2 = xmm2;\n@@ -7296,2 +4594,1 @@\n-  address generate_libmExp() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmExp\");\n+  __ vectorized_mismatch(obja, objb, length, scale, result, tmp1, tmp2, vec0, vec1, vec2);\n@@ -7299,1 +4596,3 @@\n-    address start = __ pc();\n+  __ vzeroupper();\n+  __ leave();\n+  __ ret(0);\n@@ -7301,2 +4600,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  return start;\n+}\n@@ -7304,2 +4603,11 @@\n-    __ fast_exp(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r11);\n+\/**\n+ *  Arguments:\n+ *\n+\/\/  Input:\n+\/\/    c_rarg0   - x address\n+\/\/    c_rarg1   - x length\n+\/\/    c_rarg2   - z address\n+\/\/    c_rarg3   - z length\n+ *\n+ *\/\n+address StubGenerator::generate_squareToLen() {\n@@ -7307,2 +4615,3 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"squareToLen\");\n+  address start = __ pc();\n@@ -7310,1 +4619,6 @@\n-    return start;\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx (c_rarg0, c_rarg1, ...)\n+  const Register x      = rdi;\n+  const Register len    = rsi;\n+  const Register z      = r8;\n+  const Register zlen   = rcx;\n@@ -7312,1 +4626,5 @@\n-  }\n+ const Register tmp1      = r12;\n+ const Register tmp2      = r13;\n+ const Register tmp3      = r14;\n+ const Register tmp4      = r15;\n+ const Register tmp5      = rbx;\n@@ -7314,2 +4632,2 @@\n-  address generate_libmLog() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmLog\");\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7317,1 +4635,5 @@\n-    address start = __ pc();\n+  setup_arg_regs(4); \/\/ x => rdi, len => rsi, z => rdx\n+                     \/\/ zlen => rcx\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n+  __ movptr(r8, rdx);\n+  __ square_to_len(x, len, z, zlen, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n@@ -7319,2 +4641,1 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  restore_arg_regs();\n@@ -7322,2 +4643,2 @@\n-    __ fast_log(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r11, r8);\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7325,2 +4646,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  return start;\n+}\n@@ -7328,1 +4649,4 @@\n-    return start;\n+address StubGenerator::generate_method_entry_barrier() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+  address start = __ pc();\n@@ -7330,1 +4654,1 @@\n-  }\n+  Label deoptimize_label;\n@@ -7332,2 +4656,1 @@\n-  address generate_libmLog10() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmLog10\");\n+  __ push(-1); \/\/ cookie, this is used for writing the new rsp when deoptimizing\n@@ -7335,1 +4658,2 @@\n-    address start = __ pc();\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ save rbp\n@@ -7337,2 +4661,3 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  \/\/ save c_rarg0, because we want to use that value.\n+  \/\/ We could do without it but then we depend on the number of slots used by pusha\n+  __ push(c_rarg0);\n@@ -7340,2 +4665,1 @@\n-    __ fast_log10(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                  rax, rcx, rdx, r11, r8);\n+  __ lea(c_rarg0, Address(rsp, wordSize * 3)); \/\/ 1 for cookie, 1 for rbp, 1 for c_rarg0 - this should be the return address\n@@ -7343,2 +4667,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ pusha();\n@@ -7346,1 +4669,14 @@\n-    return start;\n+  \/\/ The method may have floats as arguments, and we must spill them before calling\n+  \/\/ the VM runtime.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n@@ -7348,1 +4684,1 @@\n-  }\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<int (*)(address*)>(BarrierSetNMethod::nmethod_stub_entry_barrier)), 1);\n@@ -7350,2 +4686,9 @@\n-  address generate_libmPow() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmPow\");\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n@@ -7353,1 +4696,2 @@\n-    address start = __ pc();\n+  __ cmpl(rax, 1); \/\/ 1 means deoptimize\n+  __ jcc(Assembler::equal, deoptimize_label);\n@@ -7355,2 +4699,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ popa();\n+  __ pop(c_rarg0);\n@@ -7358,2 +4702,1 @@\n-    __ fast_pow(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r8, r9, r10, r11);\n+  __ leave();\n@@ -7361,2 +4704,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  __ addptr(rsp, 1 * wordSize); \/\/ cookie\n+  __ ret(0);\n@@ -7364,2 +4707,1 @@\n-    return start;\n-  }\n+  __ BIND(deoptimize_label);\n@@ -7368,2 +4710,2 @@\n-  address generate_libmSin() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmSin\");\n+  __ popa();\n+  __ pop(c_rarg0);\n@@ -7371,1 +4713,1 @@\n-    address start = __ pc();\n+  __ leave();\n@@ -7373,2 +4715,3 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  \/\/ this can be taken out, but is good for verification purposes. getting a SIGSEGV\n+  \/\/ here while still having a correct stack is valuable\n+  __ testptr(rsp, Address(rsp, 0));\n@@ -7376,0 +4719,45 @@\n+  __ movptr(rsp, Address(rsp, 0)); \/\/ new rsp was written in the barrier\n+  __ jmp(Address(rsp, -1 * wordSize)); \/\/ jmp target should be callers verified_entry_point\n+\n+  return start;\n+}\n+\n+ \/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - out address\n+ *    c_rarg1   - in address\n+ *    c_rarg2   - offset\n+ *    c_rarg3   - len\n+ * not Win64\n+ *    c_rarg4   - k\n+ * Win64\n+ *    rsp+40    - k\n+ *\/\n+address StubGenerator::generate_mulAdd() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"mulAdd\");\n+  address start = __ pc();\n+\n+  \/\/ Win64: rcx, rdx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  \/\/ Unix:  rdi, rsi, rdx, rcx, r8, r9 (c_rarg0, c_rarg1, ...)\n+  const Register out     = rdi;\n+  const Register in      = rsi;\n+  const Register offset  = r11;\n+  const Register len     = rcx;\n+  const Register k       = r8;\n+\n+  \/\/ Next registers will be saved on stack in mul_add().\n+  const Register tmp1  = r12;\n+  const Register tmp2  = r13;\n+  const Register tmp3  = r14;\n+  const Register tmp4  = r15;\n+  const Register tmp5  = rbx;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  setup_arg_regs(4); \/\/ out => rdi, in => rsi, offset => rdx\n+                     \/\/ len => rcx, k => r8\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n@@ -7377,8 +4765,2 @@\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_sin(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rbx, rcx, rdx, r8);\n-#ifdef _WIN64\n-    __ pop(rdi);\n-    __ pop(rsi);\n+  \/\/ last argument is on stack on Win64\n+  __ movl(k, Address(rsp, 6 * wordSize));\n@@ -7386,0 +4768,2 @@\n+  __ movptr(r11, rdx);  \/\/ move offset in rdx to offset(r11)\n+  __ mul_add(out, in, offset, len, k, tmp1, tmp2, tmp3, tmp4, tmp5, rdx, rax);\n@@ -7387,2 +4771,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  restore_arg_regs();\n@@ -7390,1 +4773,2 @@\n-    return start;\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7392,1 +4776,31 @@\n-  }\n+  return start;\n+}\n+\n+address StubGenerator::generate_bigIntegerRightShift() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n+  address start = __ pc();\n+\n+  Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n+  \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n+  const Register newArr = rdi;\n+  const Register oldArr = rsi;\n+  const Register newIdx = rdx;\n+  const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n+  const Register totalNumIter = r8;\n+\n+  \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n+  \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n+  const Register tmp1 = r11;                    \/\/ Caller save.\n+  const Register tmp2 = rax;                    \/\/ Caller save.\n+  const Register tmp3 = WIN64_ONLY(r12) NOT_WIN64(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp4 = WIN64_ONLY(r13) NOT_WIN64(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp5 = r14;                    \/\/ Callee save.\n+  const Register tmp6 = r15;\n+\n+  const XMMRegister x0 = xmm0;\n+  const XMMRegister x1 = xmm1;\n+  const XMMRegister x2 = xmm2;\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7394,2 +4808,89 @@\n-  address generate_libmCos() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmCos\");\n+#ifdef _WIN64\n+  setup_arg_regs(4);\n+  \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n+  __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n+  \/\/ Save callee save registers.\n+  __ push(tmp3);\n+  __ push(tmp4);\n+#endif\n+  __ push(tmp5);\n+\n+  \/\/ Rename temps used throughout the code.\n+  const Register idx = tmp1;\n+  const Register nIdx = tmp2;\n+\n+  __ xorl(idx, idx);\n+\n+  \/\/ Start right shift from end of the array.\n+  \/\/ For example, if #iteration = 4 and newIdx = 1\n+  \/\/ then dest[4] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n+  \/\/ if #iteration = 4 and newIdx = 0\n+  \/\/ then dest[3] = src[4] >> shiftCount  | src[3] <<< (shiftCount - 32)\n+  __ movl(idx, totalNumIter);\n+  __ movl(nIdx, idx);\n+  __ addl(nIdx, newIdx);\n+\n+  \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n+  \/\/ If not, then go to ShifTwo processing 2 iterations\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    __ cmpptr(totalNumIter, (AVX3Threshold\/64));\n+    __ jcc(Assembler::less, ShiftTwo);\n+\n+    if (AVX3Threshold < 16 * 64) {\n+      __ cmpl(totalNumIter, 16);\n+      __ jcc(Assembler::less, ShiftTwo);\n+    }\n+    __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n+    __ subl(idx, 16);\n+    __ subl(nIdx, 16);\n+    __ BIND(Shift512Loop);\n+    __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 4), Assembler::AVX_512bit);\n+    __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n+    __ vpshrdvd(x2, x1, x0, Assembler::AVX_512bit);\n+    __ evmovdqul(Address(newArr, nIdx, Address::times_4), x2, Assembler::AVX_512bit);\n+    __ subl(nIdx, 16);\n+    __ subl(idx, 16);\n+    __ jcc(Assembler::greaterEqual, Shift512Loop);\n+    __ addl(idx, 16);\n+    __ addl(nIdx, 16);\n+  }\n+  __ BIND(ShiftTwo);\n+  __ cmpl(idx, 2);\n+  __ jcc(Assembler::less, ShiftOne);\n+  __ subl(idx, 2);\n+  __ subl(nIdx, 2);\n+  __ BIND(ShiftTwoLoop);\n+  __ movl(tmp5, Address(oldArr, idx, Address::times_4, 8));\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ shrdl(tmp5, tmp4);\n+  __ shrdl(tmp4, tmp3);\n+  __ movl(Address(newArr, nIdx, Address::times_4, 4), tmp5);\n+  __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n+  __ subl(nIdx, 2);\n+  __ subl(idx, 2);\n+  __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n+  __ addl(idx, 2);\n+  __ addl(nIdx, 2);\n+\n+  \/\/ Do the last iteration\n+  __ BIND(ShiftOne);\n+  __ cmpl(idx, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ subl(idx, 1);\n+  __ subl(nIdx, 1);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 4));\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ shrdl(tmp4, tmp3);\n+  __ movl(Address(newArr, nIdx, Address::times_4), tmp4);\n+  __ BIND(Exit);\n+  __ vzeroupper();\n+  \/\/ Restore callee save registers.\n+  __ pop(tmp5);\n+#ifdef _WIN64\n+  __ pop(tmp4);\n+  __ pop(tmp3);\n+  restore_arg_regs();\n+#endif\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7397,1 +4898,2 @@\n-    address start = __ pc();\n+  return start;\n+}\n@@ -7399,2 +4901,38 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+ \/**\n+ *  Arguments:\n+ *\n+ *  Input:\n+ *    c_rarg0   - newArr address\n+ *    c_rarg1   - oldArr address\n+ *    c_rarg2   - newIdx\n+ *    c_rarg3   - shiftCount\n+ * not Win64\n+ *    c_rarg4   - numIter\n+ * Win64\n+ *    rsp40    - numIter\n+ *\/\n+address StubGenerator::generate_bigIntegerLeftShift() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n+  address start = __ pc();\n+\n+  Label Shift512Loop, ShiftTwo, ShiftTwoLoop, ShiftOne, Exit;\n+  \/\/ For Unix, the arguments are as follows: rdi, rsi, rdx, rcx, r8.\n+  const Register newArr = rdi;\n+  const Register oldArr = rsi;\n+  const Register newIdx = rdx;\n+  const Register shiftCount = rcx;  \/\/ It was intentional to have shiftCount in rcx since it is used implicitly for shift.\n+  const Register totalNumIter = r8;\n+  \/\/ For windows, we use r9 and r10 as temps to save rdi and rsi. Thus we cannot allocate them for our temps.\n+  \/\/ For everything else, we prefer using r9 and r10 since we do not have to save them before use.\n+  const Register tmp1 = r11;                    \/\/ Caller save.\n+  const Register tmp2 = rax;                    \/\/ Caller save.\n+  const Register tmp3 = WIN64_ONLY(r12) NOT_WIN64(r9);   \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp4 = WIN64_ONLY(r13) NOT_WIN64(r10);  \/\/ Windows: Callee save. Linux: Caller save.\n+  const Register tmp5 = r14;                    \/\/ Callee save.\n+\n+  const XMMRegister x0 = xmm0;\n+  const XMMRegister x1 = xmm1;\n+  const XMMRegister x2 = xmm2;\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7403,2 +4941,6 @@\n-    __ push(rsi);\n-    __ push(rdi);\n+  setup_arg_regs(4);\n+  \/\/ For windows, since last argument is on stack, we need to move it to the appropriate register.\n+  __ movl(totalNumIter, Address(rsp, 6 * wordSize));\n+  \/\/ Save callee save registers.\n+  __ push(tmp3);\n+  __ push(tmp4);\n@@ -7406,3 +4948,66 @@\n-    __ fast_cos(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r8, r9, r10, r11, rbx);\n-\n+  __ push(tmp5);\n+\n+  \/\/ Rename temps used throughout the code\n+  const Register idx = tmp1;\n+  const Register numIterTmp = tmp2;\n+\n+  \/\/ Start idx from zero.\n+  __ xorl(idx, idx);\n+  \/\/ Compute interior pointer for new array. We do this so that we can use same index for both old and new arrays.\n+  __ lea(newArr, Address(newArr, newIdx, Address::times_4));\n+  __ movl(numIterTmp, totalNumIter);\n+\n+  \/\/ If vectorization is enabled, check if the number of iterations is at least 64\n+  \/\/ If not, then go to ShiftTwo shifting two numbers at a time\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    __ cmpl(totalNumIter, (AVX3Threshold\/64));\n+    __ jcc(Assembler::less, ShiftTwo);\n+\n+    if (AVX3Threshold < 16 * 64) {\n+      __ cmpl(totalNumIter, 16);\n+      __ jcc(Assembler::less, ShiftTwo);\n+    }\n+    __ evpbroadcastd(x0, shiftCount, Assembler::AVX_512bit);\n+    __ subl(numIterTmp, 16);\n+    __ BIND(Shift512Loop);\n+    __ evmovdqul(x1, Address(oldArr, idx, Address::times_4), Assembler::AVX_512bit);\n+    __ evmovdqul(x2, Address(oldArr, idx, Address::times_4, 0x4), Assembler::AVX_512bit);\n+    __ vpshldvd(x1, x2, x0, Assembler::AVX_512bit);\n+    __ evmovdqul(Address(newArr, idx, Address::times_4), x1, Assembler::AVX_512bit);\n+    __ addl(idx, 16);\n+    __ subl(numIterTmp, 16);\n+    __ jcc(Assembler::greaterEqual, Shift512Loop);\n+    __ addl(numIterTmp, 16);\n+  }\n+  __ BIND(ShiftTwo);\n+  __ cmpl(totalNumIter, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ movl(tmp3, Address(oldArr, idx, Address::times_4));\n+  __ subl(numIterTmp, 2);\n+  __ jcc(Assembler::less, ShiftOne);\n+\n+  __ BIND(ShiftTwoLoop);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n+  __ movl(tmp5, Address(oldArr, idx, Address::times_4, 0x8));\n+  __ shldl(tmp3, tmp4);\n+  __ shldl(tmp4, tmp5);\n+  __ movl(Address(newArr, idx, Address::times_4), tmp3);\n+  __ movl(Address(newArr, idx, Address::times_4, 0x4), tmp4);\n+  __ movl(tmp3, tmp5);\n+  __ addl(idx, 2);\n+  __ subl(numIterTmp, 2);\n+  __ jcc(Assembler::greaterEqual, ShiftTwoLoop);\n+\n+  \/\/ Do the last iteration\n+  __ BIND(ShiftOne);\n+  __ addl(numIterTmp, 2);\n+  __ cmpl(numIterTmp, 1);\n+  __ jcc(Assembler::less, Exit);\n+  __ movl(tmp4, Address(oldArr, idx, Address::times_4, 0x4));\n+  __ shldl(tmp3, tmp4);\n+  __ movl(Address(newArr, idx, Address::times_4), tmp3);\n+\n+  __ BIND(Exit);\n+  __ vzeroupper();\n+  \/\/ Restore callee save registers.\n+  __ pop(tmp5);\n@@ -7410,2 +5015,3 @@\n-    __ pop(rdi);\n-    __ pop(rsi);\n+  __ pop(tmp4);\n+  __ pop(tmp3);\n+  restore_arg_regs();\n@@ -7413,0 +5019,2 @@\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n@@ -7414,4 +5022,2 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n-\n-    return start;\n+  return start;\n+}\n@@ -7419,0 +5025,23 @@\n+void StubGenerator::generate_libm_stubs() {\n+  if (UseLibmIntrinsic && InlineIntrinsics) {\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n+      StubRoutines::_dsin = generate_libmSin(); \/\/ from stubGenerator_x86_64_sin.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n+      StubRoutines::_dcos = generate_libmCos(); \/\/ from stubGenerator_x86_64_cos.cpp\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n+      StubRoutines::_dtan = generate_libmTan();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n+      StubRoutines::_dexp = generate_libmExp();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n+      StubRoutines::_dpow = generate_libmPow();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n+      StubRoutines::_dlog = generate_libmLog();\n+    }\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n+      StubRoutines::_dlog10 = generate_libmLog10();\n+    }\n@@ -7420,0 +5049,1 @@\n+}\n@@ -7421,3 +5051,2 @@\n-  address generate_libmTan() {\n-    StubCodeMark mark(this, \"StubRoutines\", \"libmTan\");\n-    address start = __ pc();\n+address StubGenerator::generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n+  if (!Continuations::enabled()) return nullptr;\n@@ -7426,2 +5055,2 @@\n-    BLOCK_COMMENT(\"Entry:\");\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+  bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n@@ -7429,6 +5058,2 @@\n-#ifdef _WIN64\n-    __ push(rsi);\n-    __ push(rdi);\n-#endif\n-    __ fast_tan(xmm0, xmm1, xmm2, xmm3, xmm4, xmm5, xmm6, xmm7,\n-                rax, rcx, rdx, r8, r9, r10, r11, rbx);\n+  StubCodeMark mark(this, \"StubRoutines\", label);\n+  address start = __ pc();\n@@ -7436,4 +5061,1 @@\n-#ifdef _WIN64\n-    __ pop(rdi);\n-    __ pop(rsi);\n-#endif\n+  \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n@@ -7441,2 +5063,7 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-    __ ret(0);\n+  if (!return_barrier) {\n+    \/\/ Pop return address. If we don't do this, we get a drift,\n+    \/\/ where the bottom-most frozen frame continuously grows.\n+    __ pop(c_rarg3);\n+  } else {\n+    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  }\n@@ -7444,1 +5071,9 @@\n-    return start;\n+#ifdef ASSERT\n+  {\n+    Label L_good_sp;\n+    __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    __ jcc(Assembler::equal, L_good_sp);\n+    __ stop(\"Incorrect rsp at thaw entry\");\n+    __ BIND(L_good_sp);\n+  }\n+#endif \/\/ ASSERT\n@@ -7446,0 +5081,4 @@\n+  if (return_barrier) {\n+    \/\/ Preserve possible return value from a method returning to the return barrier.\n+    __ push(rax);\n+    __ push_d(xmm0);\n@@ -7448,2 +5087,4 @@\n-  RuntimeStub* generate_cont_doYield() {\n-    if (!Continuations::enabled()) return nullptr;\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n+  __ movptr(rbx, rax);\n@@ -7451,7 +5092,6 @@\n-    enum layout {\n-      rbp_off,\n-      rbpH_off,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n+  if (return_barrier) {\n+    \/\/ Restore return value from a method returning to the return barrier.\n+    \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+    __ pop_d(xmm0);\n+    __ pop(rax);\n+  }\n@@ -7459,2 +5099,9 @@\n-    CodeBuffer code(\"cont_doYield\", 512, 64);\n-    MacroAssembler* _masm = new MacroAssembler(&code);\n+#ifdef ASSERT\n+  {\n+    Label L_good_sp;\n+    __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    __ jcc(Assembler::equal, L_good_sp);\n+    __ stop(\"Incorrect rsp after prepare thaw\");\n+    __ BIND(L_good_sp);\n+  }\n+#endif \/\/ ASSERT\n@@ -7462,3 +5109,6 @@\n-    address start = __ pc();\n-    __ enter();\n-    address the_pc = __ pc();\n+  \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n+  Label L_thaw_success;\n+  __ testptr(rbx, rbx);\n+  __ jccb(Assembler::notZero, L_thaw_success);\n+  __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+  __ bind(L_thaw_success);\n@@ -7466,1 +5116,3 @@\n-    int frame_complete = the_pc - start;\n+  \/\/ Make room for the thawed frames and align the stack.\n+  __ subptr(rsp, rbx);\n+  __ andptr(rsp, -StackAlignmentInBytes);\n@@ -7468,7 +5120,5 @@\n-    \/\/ This nop must be exactly at the PC we push into the frame info.\n-    \/\/ We use this nop for fast CodeBlob lookup, associate the OopMap\n-    \/\/ with it right away.\n-    __ post_call_nop();\n-    OopMapSet* oop_maps = new OopMapSet();\n-    OopMap* map = new OopMap(framesize, 1);\n-    oop_maps->add_gc_map(frame_complete, map);\n+  if (return_barrier) {\n+    \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n+    __ push(rax);\n+    __ push_d(xmm0);\n+  }\n@@ -7476,5 +5126,5 @@\n-    __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n-    __ movptr(c_rarg0, r15_thread);\n-    __ movptr(c_rarg1, rsp);\n-    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n-    __ reset_last_Java_frame(true);\n+  \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, kind);\n+  __ call_VM_leaf(Continuation::thaw_entry(), 2);\n+  __ movptr(rbx, rax);\n@@ -7482,1 +5132,9 @@\n-    Label L_pinned;\n+  if (return_barrier) {\n+    \/\/ Restore return value from a method returning to the return barrier. (Again.)\n+    \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+    __ pop_d(xmm0);\n+    __ pop(rax);\n+  } else {\n+    \/\/ Return 0 (success) from doYield.\n+    __ xorptr(rax, rax);\n+  }\n@@ -7484,2 +5142,4 @@\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::notZero, L_pinned);\n+  \/\/ After thawing, rbx is the SP of the yielding frame.\n+  \/\/ Move there, and then to saved RBP slot.\n+  __ movptr(rsp, rbx);\n+  __ subptr(rsp, 2*wordSize);\n@@ -7487,4 +5147,3 @@\n-    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-    __ continuation_enter_cleanup();\n-    __ pop(rbp);\n-    __ ret(0);\n+  if (return_barrier_exception) {\n+    __ movptr(c_rarg0, r15_thread);\n+    __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n@@ -7492,1 +5151,2 @@\n-    __ bind(L_pinned);\n+    \/\/ rax still holds the original exception oop, save it before the call\n+    __ push(rax);\n@@ -7494,3 +5154,2 @@\n-    \/\/ Pinned, return to caller\n-    __ leave();\n-    __ ret(0);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n+    __ movptr(rbx, rax);\n@@ -7498,8 +5157,13 @@\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(code.name(),\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps,\n-                                    false);\n-    return stub;\n+    \/\/ Continue at exception handler:\n+    \/\/   rax: exception oop\n+    \/\/   rbx: exception handler\n+    \/\/   rdx: exception pc\n+    __ pop(rax);\n+    __ verify_oop(rax);\n+    __ pop(rbp); \/\/ pop out RBP here too\n+    __ pop(rdx);\n+    __ jmp(rbx);\n+  } else {\n+    \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n+    __ pop(rbp);\n+    __ ret(0);\n@@ -7508,5 +5172,2 @@\n-  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n-    if (!Continuations::enabled()) return nullptr;\n-\n-    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n-    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n+  return start;\n+}\n@@ -7514,2 +5175,3 @@\n-    StubCodeMark mark(this, \"StubRoutines\", label);\n-    address start = __ pc();\n+address StubGenerator::generate_cont_thaw() {\n+  return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n+}\n@@ -7517,1 +5179,1 @@\n-    \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n+\/\/ TODO: will probably need multiple return barriers depending on return type\n@@ -7519,7 +5181,3 @@\n-    if (!return_barrier) {\n-      \/\/ Pop return address. If we don't do this, we get a drift,\n-      \/\/ where the bottom-most frozen frame continuously grows.\n-      __ pop(c_rarg3);\n-    } else {\n-      __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-    }\n+address StubGenerator::generate_cont_returnBarrier() {\n+  return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n+}\n@@ -7527,9 +5185,3 @@\n-#ifdef ASSERT\n-    {\n-      Label L_good_sp;\n-      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-      __ jcc(Assembler::equal, L_good_sp);\n-      __ stop(\"Incorrect rsp at thaw entry\");\n-      __ BIND(L_good_sp);\n-    }\n-#endif\n+address StubGenerator::generate_cont_returnBarrier_exception() {\n+  return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n+}\n@@ -7537,5 +5189,1 @@\n-    if (return_barrier) {\n-      \/\/ Preserve possible return value from a method returning to the return barrier.\n-      __ push(rax);\n-      __ push_d(xmm0);\n-    }\n+#if INCLUDE_JFR\n@@ -7543,4 +5191,11 @@\n-    __ movptr(c_rarg0, r15_thread);\n-    __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n-    __ movptr(rbx, rax);\n+\/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+\/\/ It returns a jobject handle to the event writer.\n+\/\/ The handle is dereferenced and the return value is the event writer oop.\n+RuntimeStub* StubGenerator::generate_jfr_write_checkpoint() {\n+  enum layout {\n+    rbp_off,\n+    rbpH_off,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7548,6 +5203,3 @@\n-    if (return_barrier) {\n-      \/\/ Restore return value from a method returning to the return barrier.\n-      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n-      __ pop_d(xmm0);\n-      __ pop(rax);\n-    }\n+  CodeBuffer code(\"jfr_write_checkpoint\", 512, 64);\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n+  address start = __ pc();\n@@ -7555,9 +5207,2 @@\n-#ifdef ASSERT\n-    {\n-      Label L_good_sp;\n-      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n-      __ jcc(Assembler::equal, L_good_sp);\n-      __ stop(\"Incorrect rsp after prepare thaw\");\n-      __ BIND(L_good_sp);\n-    }\n-#endif\n+  __ enter();\n+  address the_pc = __ pc();\n@@ -7565,16 +5210,1 @@\n-    \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n-    Label L_thaw_success;\n-    __ testptr(rbx, rbx);\n-    __ jccb(Assembler::notZero, L_thaw_success);\n-    __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n-    __ bind(L_thaw_success);\n-\n-    \/\/ Make room for the thawed frames and align the stack.\n-    __ subptr(rsp, rbx);\n-    __ andptr(rsp, -StackAlignmentInBytes);\n-\n-    if (return_barrier) {\n-      \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n-      __ push(rax);\n-      __ push_d(xmm0);\n-    }\n+  int frame_complete = the_pc - start;\n@@ -7582,5 +5212,4 @@\n-    \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n-    __ movptr(c_rarg0, r15_thread);\n-    __ movptr(c_rarg1, kind);\n-    __ call_VM_leaf(Continuation::thaw_entry(), 2);\n-    __ movptr(rbx, rax);\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ movptr(c_rarg0, r15_thread);\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+  __ reset_last_Java_frame(true);\n@@ -7588,9 +5217,4 @@\n-    if (return_barrier) {\n-      \/\/ Restore return value from a method returning to the return barrier. (Again.)\n-      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n-      __ pop_d(xmm0);\n-      __ pop(rax);\n-    } else {\n-      \/\/ Return 0 (success) from doYield.\n-      __ xorptr(rax, rax);\n-    }\n+  \/\/ rax is jobject handle result, unpack and process it through a barrier.\n+  Label L_null_jobject;\n+  __ testptr(rax, rax);\n+  __ jcc(Assembler::zero, L_null_jobject);\n@@ -7598,29 +5222,2 @@\n-    \/\/ After thawing, rbx is the SP of the yielding frame.\n-    \/\/ Move there, and then to saved RBP slot.\n-    __ movptr(rsp, rbx);\n-    __ subptr(rsp, 2*wordSize);\n-\n-    if (return_barrier_exception) {\n-      __ movptr(c_rarg0, r15_thread);\n-      __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n-\n-      \/\/ rax still holds the original exception oop, save it before the call\n-      __ push(rax);\n-\n-      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n-      __ movptr(rbx, rax);\n-\n-      \/\/ Continue at exception handler:\n-      \/\/   rax: exception oop\n-      \/\/   rbx: exception handler\n-      \/\/   rdx: exception pc\n-      __ pop(rax);\n-      __ verify_oop(rax);\n-      __ pop(rbp); \/\/ pop out RBP here too\n-      __ pop(rdx);\n-      __ jmp(rbx);\n-    } else {\n-      \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n-      __ pop(rbp);\n-      __ ret(0);\n-    }\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->load_at(_masm, ACCESS_READ | IN_NATIVE, T_OBJECT, rax, Address(rax, 0), c_rarg0, r15_thread);\n@@ -7628,2 +5225,1 @@\n-    return start;\n-  }\n+  __ bind(L_null_jobject);\n@@ -7631,3 +5227,2 @@\n-  address generate_cont_thaw() {\n-    return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n-  }\n+  __ leave();\n+  __ ret(0);\n@@ -7635,1 +5230,3 @@\n-  \/\/ TODO: will probably need multiple return barriers depending on return type\n+  OopMapSet* oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(framesize, 1);\n+  oop_maps->add_gc_map(frame_complete, map);\n@@ -7637,3 +5234,9 @@\n-  address generate_cont_returnBarrier() {\n-    return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n-  }\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(code.name(),\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps,\n+                                  false);\n+  return stub;\n+}\n@@ -7641,3 +5244,1 @@\n-  address generate_cont_returnBarrier_exception() {\n-    return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n-  }\n+#endif \/\/ INCLUDE_JFR\n@@ -7645,1 +5246,30 @@\n-#if INCLUDE_JFR\n+\/\/ Continuation point for throwing of implicit exceptions that are\n+\/\/ not handled in the current activation. Fabricates an exception\n+\/\/ oop and initiates normal exception dispatching in this\n+\/\/ frame. Since we need to preserve callee-saved values (currently\n+\/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+\/\/ map and therefore have to make these stubs into RuntimeStubs\n+\/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+\/\/ be preserved between the fault point and the exception handler\n+\/\/ then it must assume responsibility for that in\n+\/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+\/\/ continuation_for_implicit_division_by_zero_exception. All other\n+\/\/ implicit exceptions (e.g., NullPointerException or\n+\/\/ AbstractMethodError on entry) are either at call sites or\n+\/\/ otherwise assume that stack unwinding will be initiated, so\n+\/\/ caller saved registers were assumed volatile in the compiler.\n+address StubGenerator::generate_throw_exception(const char* name,\n+                                                address runtime_entry,\n+                                                Register arg1,\n+                                                Register arg2) {\n+  \/\/ Information about frame layout at time of blocking runtime call.\n+  \/\/ Note that we only have to preserve callee-saved registers since\n+  \/\/ the compilers are responsible for supplying a continuation point\n+  \/\/ if they expect all registers to be preserved.\n+  enum layout {\n+    rbp_off = frame::arg_reg_save_area_bytes\/BytesPerInt,\n+    rbp_off2,\n+    return_off,\n+    return_off2,\n+    framesize \/\/ inclusive of return address\n+  };\n@@ -7647,11 +5277,2 @@\n-  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n-  \/\/ It returns a jobject handle to the event writer.\n-  \/\/ The handle is dereferenced and the return value is the event writer oop.\n-  RuntimeStub* generate_jfr_write_checkpoint() {\n-    enum layout {\n-      rbp_off,\n-      rbpH_off,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n+  int insts_size = 512;\n+  int locs_size  = 64;\n@@ -7659,2 +5280,3 @@\n-    CodeBuffer code(\"jfr_write_checkpoint\", 512, 64);\n-    MacroAssembler* _masm = new MacroAssembler(&code);\n+  CodeBuffer code(name, insts_size, locs_size);\n+  OopMapSet* oop_maps  = new OopMapSet();\n+  MacroAssembler* _masm = new MacroAssembler(&code);\n@@ -7662,3 +5284,1 @@\n-    address start = __ pc();\n-    __ enter();\n-    address the_pc = __ pc();\n+  address start = __ pc();\n@@ -7666,1 +5286,4 @@\n-    int frame_complete = the_pc - start;\n+  \/\/ This is an inlined and slightly modified version of call_VM\n+  \/\/ which has the ability to fetch the return PC out of\n+  \/\/ thread-local storage and also sets up last_Java_sp slightly\n+  \/\/ differently than the real call_VM\n@@ -7668,4 +5291,1 @@\n-    __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n-    __ movptr(c_rarg0, r15_thread);\n-    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n-    __ reset_last_Java_frame(true);\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7673,4 +5293,1 @@\n-    \/\/ rax is jobject handle result, unpack and process it through a barrier.\n-    Label L_null_jobject;\n-    __ testptr(rax, rax);\n-    __ jcc(Assembler::zero, L_null_jobject);\n+  assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n@@ -7678,2 +5295,2 @@\n-    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-    bs->load_at(_masm, ACCESS_READ | IN_NATIVE, T_OBJECT, rax, Address(rax, 0), c_rarg0, r15_thread);\n+  \/\/ return address and rbp are already in place\n+  __ subptr(rsp, (framesize-4) << LogBytesPerInt); \/\/ prolog\n@@ -7681,1 +5298,1 @@\n-    __ bind(L_null_jobject);\n+  int frame_complete = __ pc() - start;\n@@ -7683,2 +5300,4 @@\n-    __ leave();\n-    __ ret(0);\n+  \/\/ Set up last_Java_sp and last_Java_fp\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n+  __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n@@ -7686,12 +5305,4 @@\n-    OopMapSet* oop_maps = new OopMapSet();\n-    OopMap* map = new OopMap(framesize, 1);\n-    oop_maps->add_gc_map(frame_complete, map);\n-\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(code.name(),\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps,\n-                                    false);\n-    return stub;\n+  \/\/ Call runtime\n+  if (arg1 != noreg) {\n+    assert(arg2 != c_rarg1, \"clobbered\");\n+    __ movptr(c_rarg1, arg1);\n@@ -7699,0 +5310,6 @@\n+  if (arg2 != noreg) {\n+    __ movptr(c_rarg2, arg2);\n+  }\n+  __ movptr(c_rarg0, r15_thread);\n+  BLOCK_COMMENT(\"call runtime_entry\");\n+  __ call(RuntimeAddress(runtime_entry));\n@@ -7700,78 +5317,2 @@\n-#endif \/\/ INCLUDE_JFR\n-\n-#undef __\n-#define __ masm->\n-\n-  \/\/ Continuation point for throwing of implicit exceptions that are\n-  \/\/ not handled in the current activation. Fabricates an exception\n-  \/\/ oop and initiates normal exception dispatching in this\n-  \/\/ frame. Since we need to preserve callee-saved values (currently\n-  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n-  \/\/ map and therefore have to make these stubs into RuntimeStubs\n-  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n-  \/\/ be preserved between the fault point and the exception handler\n-  \/\/ then it must assume responsibility for that in\n-  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n-  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n-  \/\/ implicit exceptions (e.g., NullPointerException or\n-  \/\/ AbstractMethodError on entry) are either at call sites or\n-  \/\/ otherwise assume that stack unwinding will be initiated, so\n-  \/\/ caller saved registers were assumed volatile in the compiler.\n-  address generate_throw_exception(const char* name,\n-                                   address runtime_entry,\n-                                   Register arg1 = noreg,\n-                                   Register arg2 = noreg) {\n-    \/\/ Information about frame layout at time of blocking runtime call.\n-    \/\/ Note that we only have to preserve callee-saved registers since\n-    \/\/ the compilers are responsible for supplying a continuation point\n-    \/\/ if they expect all registers to be preserved.\n-    enum layout {\n-      rbp_off = frame::arg_reg_save_area_bytes\/BytesPerInt,\n-      rbp_off2,\n-      return_off,\n-      return_off2,\n-      framesize \/\/ inclusive of return address\n-    };\n-\n-    int insts_size = 512;\n-    int locs_size  = 64;\n-\n-    CodeBuffer code(name, insts_size, locs_size);\n-    OopMapSet* oop_maps  = new OopMapSet();\n-    MacroAssembler* masm = new MacroAssembler(&code);\n-\n-    address start = __ pc();\n-\n-    \/\/ This is an inlined and slightly modified version of call_VM\n-    \/\/ which has the ability to fetch the return PC out of\n-    \/\/ thread-local storage and also sets up last_Java_sp slightly\n-    \/\/ differently than the real call_VM\n-\n-    __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n-\n-    assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n-\n-    \/\/ return address and rbp are already in place\n-    __ subptr(rsp, (framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-    int frame_complete = __ pc() - start;\n-\n-    \/\/ Set up last_Java_sp and last_Java_fp\n-    address the_pc = __ pc();\n-    __ set_last_Java_frame(rsp, rbp, the_pc, rscratch1);\n-    __ andptr(rsp, -(StackAlignmentInBytes));    \/\/ Align stack\n-\n-    \/\/ Call runtime\n-    if (arg1 != noreg) {\n-      assert(arg2 != c_rarg1, \"clobbered\");\n-      __ movptr(c_rarg1, arg1);\n-    }\n-    if (arg2 != noreg) {\n-      __ movptr(c_rarg2, arg2);\n-    }\n-    __ movptr(c_rarg0, r15_thread);\n-    BLOCK_COMMENT(\"call runtime_entry\");\n-    __ call(RuntimeAddress(runtime_entry));\n-\n-    \/\/ Generate oop map\n-    OopMap* map = new OopMap(framesize, 0);\n+  \/\/ Generate oop map\n+  OopMap* map = new OopMap(framesize, 0);\n@@ -7779,1 +5320,1 @@\n-    oop_maps->add_gc_map(the_pc - start, map);\n+  oop_maps->add_gc_map(the_pc - start, map);\n@@ -7781,1 +5322,1 @@\n-    __ reset_last_Java_frame(true);\n+  __ reset_last_Java_frame(true);\n@@ -7783,1 +5324,1 @@\n-    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n@@ -7785,1 +5326,1 @@\n-    \/\/ check for pending exceptions\n+  \/\/ check for pending exceptions\n@@ -7787,5 +5328,5 @@\n-    Label L;\n-    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n-    __ jcc(Assembler::notEqual, L);\n-    __ should_not_reach_here();\n-    __ bind(L);\n+  Label L;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), NULL_WORD);\n+  __ jcc(Assembler::notEqual, L);\n+  __ should_not_reach_here();\n+  __ bind(L);\n@@ -7793,2 +5334,1 @@\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n-\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n@@ -7796,13 +5336,9 @@\n-    \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n-    RuntimeStub* stub =\n-      RuntimeStub::new_runtime_stub(name,\n-                                    &code,\n-                                    frame_complete,\n-                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n-                                    oop_maps, false);\n-    return stub->entry_point();\n-  }\n-  void create_control_words() {\n-    \/\/ Round to nearest, 64-bit mode, exceptions masked\n-    StubRoutines::x86::_mxcsr_std = 0x1F80;\n-  }\n+  \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(name,\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps, false);\n+  return stub->entry_point();\n+}\n@@ -7811,128 +5347,4 @@\n-  \/\/ Call here from the interpreter or compiled code to either load\n-  \/\/ multiple returned values from the inline type instance being\n-  \/\/ returned to registers or to store returned values to a newly\n-  \/\/ allocated inline type instance.\n-  \/\/ Register is a class, but it would be assigned numerical value.\n-  \/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n-  PRAGMA_DIAG_PUSH\n-  PRAGMA_NONNULL_IGNORED\n-  address generate_return_value_stub(address destination, const char* name, bool has_res) {\n-    \/\/ We need to save all registers the calling convention may use so\n-    \/\/ the runtime calls read or update those registers. This needs to\n-    \/\/ be in sync with SharedRuntime::java_return_convention().\n-    enum layout {\n-      pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n-      rax_off, rax_off_2,\n-      j_rarg5_off, j_rarg5_2,\n-      j_rarg4_off, j_rarg4_2,\n-      j_rarg3_off, j_rarg3_2,\n-      j_rarg2_off, j_rarg2_2,\n-      j_rarg1_off, j_rarg1_2,\n-      j_rarg0_off, j_rarg0_2,\n-      j_farg0_off, j_farg0_2,\n-      j_farg1_off, j_farg1_2,\n-      j_farg2_off, j_farg2_2,\n-      j_farg3_off, j_farg3_2,\n-      j_farg4_off, j_farg4_2,\n-      j_farg5_off, j_farg5_2,\n-      j_farg6_off, j_farg6_2,\n-      j_farg7_off, j_farg7_2,\n-      rbp_off, rbp_off_2,\n-      return_off, return_off_2,\n-\n-      framesize\n-    };\n-\n-    CodeBuffer buffer(name, 1000, 512);\n-    MacroAssembler* masm = new MacroAssembler(&buffer);\n-\n-    int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n-    assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n-    int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n-    int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n-\n-    OopMapSet *oop_maps = new OopMapSet();\n-    OopMap* map = new OopMap(frame_size_in_slots, 0);\n-\n-    map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n-    map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n-\n-    int start = __ offset();\n-\n-    __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n-\n-    __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n-    __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n-    __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n-    __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n-    __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n-    __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n-    __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n-    __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n-    __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n-\n-    __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n-    __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n-    __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n-    __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n-    __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n-    __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n-    __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n-\n-    int frame_complete = __ offset();\n-\n-    __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);\n-\n-    __ mov(c_rarg0, r15_thread);\n-    __ mov(c_rarg1, rax);\n-\n-    __ call(RuntimeAddress(destination));\n-\n-    \/\/ Set an oopmap for the call site.\n-\n-    oop_maps->add_gc_map( __ offset() - start, map);\n-\n-    \/\/ clear last_Java_sp\n-    __ reset_last_Java_frame(false);\n-\n-    __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n-    __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n-    __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n-    __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n-    __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n-    __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n-    __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n-    __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n-    __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n-\n-    __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n-    __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n-    __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n-    __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n-    __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n-    __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n-    __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n-\n-    __ addptr(rsp, frame_size_in_bytes-8);\n-\n-    \/\/ check for pending exceptions\n-    Label pending;\n-    __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n-    __ jcc(Assembler::notEqual, pending);\n-\n-    if (has_res) {\n-      __ get_vm_result(rax, r15_thread);\n-    }\n+void StubGenerator::create_control_words() {\n+  \/\/ Round to nearest, 64-bit mode, exceptions masked\n+  StubRoutines::x86::_mxcsr_std = 0x1F80;\n+}\n@@ -7940,1 +5352,3 @@\n-    __ ret(0);\n+\/\/ Initialization\n+void StubGenerator::generate_initial() {\n+  \/\/ Generates all stubs and initializes the entry points\n@@ -7942,1 +5356,2 @@\n-    __ bind(pending);\n+  \/\/ This platform-specific settings are needed by generate_call_stub()\n+  create_control_words();\n@@ -7944,2 +5359,5 @@\n-    __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n-    __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+  \/\/ entry points that exist in all platforms Note: This is code\n+  \/\/ that could be shared among different platforms - however the\n+  \/\/ benefit seems to be smaller than the disadvantage of having a\n+  \/\/ much more complicated generator structure. See also comment in\n+  \/\/ stubRoutines.hpp.\n@@ -7947,3 +5365,1 @@\n-    \/\/ -------------\n-    \/\/ make sure all code is generated\n-    masm->flush();\n+  StubRoutines::_forward_exception_entry = generate_forward_exception();\n@@ -7951,2 +5367,8 @@\n-    RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n-    return stub->entry_point();\n+  \/\/ Generate these first because they are called from other stubs\n+  if (InlineTypeReturnedAsFields) {\n+    StubRoutines::_load_inline_type_fields_in_regs =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs),\n+                                 \"load_inline_type_fields_in_regs\", false);\n+    StubRoutines::_store_inline_type_fields_to_buf =\n+      generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf),\n+                                 \"store_inline_type_fields_to_buf\", true);\n@@ -7955,3 +5377,2 @@\n-  \/\/ Initialization\n-  void generate_initial() {\n-    \/\/ Generates all stubs and initializes the entry points\n+  StubRoutines::_call_stub_entry =\n+    generate_call_stub(StubRoutines::_call_stub_return_address);\n@@ -7959,2 +5380,2 @@\n-    \/\/ This platform-specific settings are needed by generate_call_stub()\n-    create_control_words();\n+  \/\/ is referenced by megamorphic call\n+  StubRoutines::_catch_exception_entry = generate_catch_exception();\n@@ -7962,5 +5383,2 @@\n-    \/\/ entry points that exist in all platforms Note: This is code\n-    \/\/ that could be shared among different platforms - however the\n-    \/\/ benefit seems to be smaller than the disadvantage of having a\n-    \/\/ much more complicated generator structure. See also comment in\n-    \/\/ stubRoutines.hpp.\n+  \/\/ atomic calls\n+  StubRoutines::_fence_entry                = generate_orderaccess_fence();\n@@ -7968,1 +5386,2 @@\n-    StubRoutines::_forward_exception_entry = generate_forward_exception();\n+  \/\/ platform dependent\n+  StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n@@ -7970,46 +5389,1 @@\n-    \/\/ Generate these first because they are called from other stubs\n-    if (InlineTypeReturnedAsFields) {\n-      StubRoutines::_load_inline_type_fields_in_regs =\n-        generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::load_inline_type_fields_in_regs), \"load_inline_type_fields_in_regs\", false);\n-      StubRoutines::_store_inline_type_fields_to_buf =\n-        generate_return_value_stub(CAST_FROM_FN_PTR(address, SharedRuntime::store_inline_type_fields_to_buf), \"store_inline_type_fields_to_buf\", true);\n-    }\n-    StubRoutines::_call_stub_entry = generate_call_stub(StubRoutines::_call_stub_return_address);\n-\n-    \/\/ is referenced by megamorphic call\n-    StubRoutines::_catch_exception_entry = generate_catch_exception();\n-\n-    \/\/ atomic calls\n-    StubRoutines::_fence_entry                = generate_orderaccess_fence();\n-\n-    \/\/ platform dependent\n-    StubRoutines::x86::_get_previous_sp_entry = generate_get_previous_sp();\n-\n-    StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();\n-\n-    StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();\n-    StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();\n-    StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();\n-    StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();\n-\n-    StubRoutines::x86::_float_sign_mask       = generate_fp_mask(\"float_sign_mask\",  0x7FFFFFFF7FFFFFFF);\n-    StubRoutines::x86::_float_sign_flip       = generate_fp_mask(\"float_sign_flip\",  0x8000000080000000);\n-    StubRoutines::x86::_double_sign_mask      = generate_fp_mask(\"double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_double_sign_flip      = generate_fp_mask(\"double_sign_flip\", 0x8000000000000000);\n-\n-    \/\/ Build this early so it's available for the interpreter.\n-    StubRoutines::_throw_StackOverflowError_entry =\n-      generate_throw_exception(\"StackOverflowError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_StackOverflowError));\n-    StubRoutines::_throw_delayed_StackOverflowError_entry =\n-      generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_delayed_StackOverflowError));\n-    if (UseCRC32Intrinsics) {\n-      \/\/ set table address before stub generation which use it\n-      StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n-      StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n-    }\n+  StubRoutines::x86::_verify_mxcsr_entry    = generate_verify_mxcsr();\n@@ -8017,6 +5391,4 @@\n-    if (UseCRC32CIntrinsics) {\n-      bool supports_clmul = VM_Version::supports_clmul();\n-      StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n-      StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n-      StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n-    }\n+  StubRoutines::x86::_f2i_fixup             = generate_f2i_fixup();\n+  StubRoutines::x86::_f2l_fixup             = generate_f2l_fixup();\n+  StubRoutines::x86::_d2i_fixup             = generate_d2i_fixup();\n+  StubRoutines::x86::_d2l_fixup             = generate_d2l_fixup();\n@@ -8024,3 +5396,4 @@\n-    if (UseAdler32Intrinsics) {\n-       StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n-    }\n+  StubRoutines::x86::_float_sign_mask       = generate_fp_mask(\"float_sign_mask\",  0x7FFFFFFF7FFFFFFF);\n+  StubRoutines::x86::_float_sign_flip       = generate_fp_mask(\"float_sign_flip\",  0x8000000080000000);\n+  StubRoutines::x86::_double_sign_mask      = generate_fp_mask(\"double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_double_sign_flip      = generate_fp_mask(\"double_sign_flip\", 0x8000000000000000);\n@@ -8028,23 +5401,15 @@\n-    if (UseLibmIntrinsic && InlineIntrinsics) {\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dexp)) {\n-        StubRoutines::_dexp = generate_libmExp();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog)) {\n-        StubRoutines::_dlog = generate_libmLog();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dlog10)) {\n-        StubRoutines::_dlog10 = generate_libmLog10();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dpow)) {\n-        StubRoutines::_dpow = generate_libmPow();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dsin)) {\n-        StubRoutines::_dsin = generate_libmSin();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dcos)) {\n-        StubRoutines::_dcos = generate_libmCos();\n-      }\n-      if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_dtan)) {\n-        StubRoutines::_dtan = generate_libmTan();\n-      }\n-    }\n+  \/\/ Build this early so it's available for the interpreter.\n+  StubRoutines::_throw_StackOverflowError_entry =\n+    generate_throw_exception(\"StackOverflowError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_StackOverflowError));\n+  StubRoutines::_throw_delayed_StackOverflowError_entry =\n+    generate_throw_exception(\"delayed StackOverflowError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_delayed_StackOverflowError));\n+  if (UseCRC32Intrinsics) {\n+    \/\/ set table address before stub generation which use it\n+    StubRoutines::_crc_table_adr = (address)StubRoutines::x86::_crc_table;\n+    StubRoutines::_updateBytesCRC32 = generate_updateBytesCRC32();\n@@ -8053,11 +5418,5 @@\n-  void generate_phase1() {\n-    \/\/ Continuation stubs:\n-    StubRoutines::_cont_thaw          = generate_cont_thaw();\n-    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n-    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n-    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n-    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n-                                        : StubRoutines::_cont_doYield_stub->entry_point();\n-\n-    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n-    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+  if (UseCRC32CIntrinsics) {\n+    bool supports_clmul = VM_Version::supports_clmul();\n+    StubRoutines::x86::generate_CRC32C_table(supports_clmul);\n+    StubRoutines::_crc32c_table_addr = (address)StubRoutines::x86::_crc32c_table;\n+    StubRoutines::_updateBytesCRC32C = generate_updateBytesCRC32C(supports_clmul);\n@@ -8066,55 +5425,4 @@\n-  void generate_all() {\n-    \/\/ Generates all stubs and initializes the entry points\n-\n-    \/\/ These entry points require SharedInfo::stack0 to be set up in\n-    \/\/ non-core builds and need to be relocatable, so they each\n-    \/\/ fabricate a RuntimeStub internally.\n-    StubRoutines::_throw_AbstractMethodError_entry =\n-      generate_throw_exception(\"AbstractMethodError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_AbstractMethodError));\n-\n-    StubRoutines::_throw_IncompatibleClassChangeError_entry =\n-      generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_IncompatibleClassChangeError));\n-\n-    StubRoutines::_throw_NullPointerException_at_call_entry =\n-      generate_throw_exception(\"NullPointerException at call throw_exception\",\n-                               CAST_FROM_FN_PTR(address,\n-                                                SharedRuntime::\n-                                                throw_NullPointerException_at_call));\n-\n-    \/\/ entry points that are platform specific\n-    StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF7FFFFFFF);\n-    StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x8000000080000000);\n-    StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(\"vector_double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(\"vector_double_sign_flip\", 0x8000000000000000);\n-    StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n-    StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n-    StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff00ff00ff);\n-    StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n-    StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n-    StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n-    StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0, 0, 0);\n-    StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n-                                                                        0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n-    StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n-    StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n-    StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n-    StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n-    StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(\"vector_long_sign_mask\", 0x8000000000000000);\n-    StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n-    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n-    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n-    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n-\n-    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n-      \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n-      StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n-    }\n+  if (UseAdler32Intrinsics) {\n+     StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+  }\n+}\n@@ -8122,4 +5430,34 @@\n-    \/\/ support for verify_oop (must happen after universe_init)\n-    if (VerifyOops) {\n-      StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n-    }\n+\/\/ Call here from the interpreter or compiled code to either load\n+\/\/ multiple returned values from the inline type instance being\n+\/\/ returned to registers or to store returned values to a newly\n+\/\/ allocated inline type instance.\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for xmm0. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n+address StubGenerator::generate_return_value_stub(address destination, const char* name, bool has_res) {\n+  \/\/ We need to save all registers the calling convention may use so\n+  \/\/ the runtime calls read or update those registers. This needs to\n+  \/\/ be in sync with SharedRuntime::java_return_convention().\n+  enum layout {\n+    pad_off = frame::arg_reg_save_area_bytes\/BytesPerInt, pad_off_2,\n+    rax_off, rax_off_2,\n+    j_rarg5_off, j_rarg5_2,\n+    j_rarg4_off, j_rarg4_2,\n+    j_rarg3_off, j_rarg3_2,\n+    j_rarg2_off, j_rarg2_2,\n+    j_rarg1_off, j_rarg1_2,\n+    j_rarg0_off, j_rarg0_2,\n+    j_farg0_off, j_farg0_2,\n+    j_farg1_off, j_farg1_2,\n+    j_farg2_off, j_farg2_2,\n+    j_farg3_off, j_farg3_2,\n+    j_farg4_off, j_farg4_2,\n+    j_farg5_off, j_farg5_2,\n+    j_farg6_off, j_farg6_2,\n+    j_farg7_off, j_farg7_2,\n+    rbp_off, rbp_off_2,\n+    return_off, return_off_2,\n+\n+    framesize\n+  };\n@@ -8127,25 +5465,108 @@\n-    \/\/ data cache line writeback\n-    StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n-    StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n-\n-    \/\/ arraycopy stubs used by compilers\n-    generate_arraycopy_stubs();\n-\n-    \/\/ don't bother generating these AES intrinsic stubs unless global flag is set\n-    if (UseAESIntrinsics) {\n-      StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ needed by the others\n-      StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n-      StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n-      StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n-      if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq() ) {\n-        StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();\n-        StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();\n-        StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();\n-        StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-        StubRoutines::x86::_ghash_poly512_addr = ghash_polynomial512_addr();\n-        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-        StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n-      } else {\n-        StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n-      }\n-    }\n+  CodeBuffer buffer(name, 1000, 512);\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+\n+  int frame_size_in_bytes = align_up(framesize*BytesPerInt, 16);\n+  assert(frame_size_in_bytes == framesize*BytesPerInt, \"misaligned\");\n+  int frame_size_in_slots = frame_size_in_bytes \/ BytesPerInt;\n+  int frame_size_in_words = frame_size_in_bytes \/ wordSize;\n+\n+  OopMapSet *oop_maps = new OopMapSet();\n+  OopMap* map = new OopMap(frame_size_in_slots, 0);\n+\n+  map->set_callee_saved(VMRegImpl::stack2reg(rax_off), rax->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg5_off), j_rarg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg4_off), j_rarg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg3_off), j_rarg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg2_off), j_rarg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg1_off), j_rarg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_rarg0_off), j_rarg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg0_off), j_farg0->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg1_off), j_farg1->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg2_off), j_farg2->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg3_off), j_farg3->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg4_off), j_farg4->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg5_off), j_farg5->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg6_off), j_farg6->as_VMReg());\n+  map->set_callee_saved(VMRegImpl::stack2reg(j_farg7_off), j_farg7->as_VMReg());\n+\n+  int start = __ offset();\n+\n+  __ subptr(rsp, frame_size_in_bytes - 8 \/* return address*\/);\n+\n+  __ movptr(Address(rsp, rbp_off * BytesPerInt), rbp);\n+  __ movdbl(Address(rsp, j_farg7_off * BytesPerInt), j_farg7);\n+  __ movdbl(Address(rsp, j_farg6_off * BytesPerInt), j_farg6);\n+  __ movdbl(Address(rsp, j_farg5_off * BytesPerInt), j_farg5);\n+  __ movdbl(Address(rsp, j_farg4_off * BytesPerInt), j_farg4);\n+  __ movdbl(Address(rsp, j_farg3_off * BytesPerInt), j_farg3);\n+  __ movdbl(Address(rsp, j_farg2_off * BytesPerInt), j_farg2);\n+  __ movdbl(Address(rsp, j_farg1_off * BytesPerInt), j_farg1);\n+  __ movdbl(Address(rsp, j_farg0_off * BytesPerInt), j_farg0);\n+\n+  __ movptr(Address(rsp, j_rarg0_off * BytesPerInt), j_rarg0);\n+  __ movptr(Address(rsp, j_rarg1_off * BytesPerInt), j_rarg1);\n+  __ movptr(Address(rsp, j_rarg2_off * BytesPerInt), j_rarg2);\n+  __ movptr(Address(rsp, j_rarg3_off * BytesPerInt), j_rarg3);\n+  __ movptr(Address(rsp, j_rarg4_off * BytesPerInt), j_rarg4);\n+  __ movptr(Address(rsp, j_rarg5_off * BytesPerInt), j_rarg5);\n+  __ movptr(Address(rsp, rax_off * BytesPerInt), rax);\n+\n+  int frame_complete = __ offset();\n+\n+  __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(c_rarg1, rax);\n+\n+  __ call(RuntimeAddress(destination));\n+\n+  \/\/ Set an oopmap for the call site.\n+\n+  oop_maps->add_gc_map( __ offset() - start, map);\n+\n+  \/\/ clear last_Java_sp\n+  __ reset_last_Java_frame(false);\n+\n+  __ movptr(rbp, Address(rsp, rbp_off * BytesPerInt));\n+  __ movdbl(j_farg7, Address(rsp, j_farg7_off * BytesPerInt));\n+  __ movdbl(j_farg6, Address(rsp, j_farg6_off * BytesPerInt));\n+  __ movdbl(j_farg5, Address(rsp, j_farg5_off * BytesPerInt));\n+  __ movdbl(j_farg4, Address(rsp, j_farg4_off * BytesPerInt));\n+  __ movdbl(j_farg3, Address(rsp, j_farg3_off * BytesPerInt));\n+  __ movdbl(j_farg2, Address(rsp, j_farg2_off * BytesPerInt));\n+  __ movdbl(j_farg1, Address(rsp, j_farg1_off * BytesPerInt));\n+  __ movdbl(j_farg0, Address(rsp, j_farg0_off * BytesPerInt));\n+\n+  __ movptr(j_rarg0, Address(rsp, j_rarg0_off * BytesPerInt));\n+  __ movptr(j_rarg1, Address(rsp, j_rarg1_off * BytesPerInt));\n+  __ movptr(j_rarg2, Address(rsp, j_rarg2_off * BytesPerInt));\n+  __ movptr(j_rarg3, Address(rsp, j_rarg3_off * BytesPerInt));\n+  __ movptr(j_rarg4, Address(rsp, j_rarg4_off * BytesPerInt));\n+  __ movptr(j_rarg5, Address(rsp, j_rarg5_off * BytesPerInt));\n+  __ movptr(rax, Address(rsp, rax_off * BytesPerInt));\n+\n+  __ addptr(rsp, frame_size_in_bytes-8);\n+\n+  \/\/ check for pending exceptions\n+  Label pending;\n+  __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);\n+  __ jcc(Assembler::notEqual, pending);\n+\n+  if (has_res) {\n+    __ get_vm_result(rax, r15_thread);\n+  }\n+\n+  __ ret(0);\n+\n+  __ bind(pending);\n+\n+  __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));\n+  __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));\n+\n+  \/\/ -------------\n+  \/\/ make sure all code is generated\n+  _masm->flush();\n+\n+  RuntimeStub* stub = RuntimeStub::new_runtime_stub(name, &buffer, frame_complete, frame_size_in_words, oop_maps, false);\n+  return stub->entry_point();\n+}\n@@ -8153,11 +5574,5 @@\n-    if (UseAESCTRIntrinsics) {\n-      if (VM_Version::supports_avx512_vaes() && VM_Version::supports_avx512bw() && VM_Version::supports_avx512vl()) {\n-        if (StubRoutines::x86::_counter_mask_addr == NULL) {\n-          StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n-        }\n-        StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();\n-      } else {\n-        StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n-        StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n-      }\n-    }\n+void StubGenerator::generate_phase1() {\n+  \/\/ Continuation stubs:\n+  StubRoutines::_cont_thaw          = generate_cont_thaw();\n+  StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+  StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n@@ -8165,29 +5580,3 @@\n-    if (UseMD5Intrinsics) {\n-      StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n-      StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n-    }\n-    if (UseSHA1Intrinsics) {\n-      StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n-      StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n-      StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n-      StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n-    }\n-    if (UseSHA256Intrinsics) {\n-      StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n-      char* dst = (char*)StubRoutines::x86::_k256_W;\n-      char* src = (char*)StubRoutines::x86::_k256;\n-      for (int ii = 0; ii < 16; ++ii) {\n-        memcpy(dst + 32 * ii,      src + 16 * ii, 16);\n-        memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);\n-      }\n-      StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n-      StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n-      StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n-    }\n-    if (UseSHA512Intrinsics) {\n-      StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;\n-      StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();\n-      StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, \"sha512_implCompress\");\n-      StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n-    }\n+  JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+  JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n+}\n@@ -8195,13 +5584,84 @@\n-    \/\/ Generate GHASH intrinsics code\n-    if (UseGHASHIntrinsics) {\n-      if (StubRoutines::x86::_ghash_long_swap_mask_addr == NULL) {\n-        StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n-      }\n-    StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n-      if (VM_Version::supports_avx()) {\n-        StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();\n-        StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();\n-        StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();\n-      } else {\n-        StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n-      }\n+void StubGenerator::generate_all() {\n+  \/\/ Generates all stubs and initializes the entry points\n+\n+  \/\/ These entry points require SharedInfo::stack0 to be set up in\n+  \/\/ non-core builds and need to be relocatable, so they each\n+  \/\/ fabricate a RuntimeStub internally.\n+  StubRoutines::_throw_AbstractMethodError_entry =\n+    generate_throw_exception(\"AbstractMethodError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_AbstractMethodError));\n+\n+  StubRoutines::_throw_IncompatibleClassChangeError_entry =\n+    generate_throw_exception(\"IncompatibleClassChangeError throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_IncompatibleClassChangeError));\n+\n+  StubRoutines::_throw_NullPointerException_at_call_entry =\n+    generate_throw_exception(\"NullPointerException at call throw_exception\",\n+                             CAST_FROM_FN_PTR(address,\n+                                              SharedRuntime::\n+                                              throw_NullPointerException_at_call));\n+\n+  \/\/ entry points that are platform specific\n+  StubRoutines::x86::_vector_float_sign_mask = generate_vector_mask(\"vector_float_sign_mask\", 0x7FFFFFFF7FFFFFFF);\n+  StubRoutines::x86::_vector_float_sign_flip = generate_vector_mask(\"vector_float_sign_flip\", 0x8000000080000000);\n+  StubRoutines::x86::_vector_double_sign_mask = generate_vector_mask(\"vector_double_sign_mask\", 0x7FFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_vector_double_sign_flip = generate_vector_mask(\"vector_double_sign_flip\", 0x8000000000000000);\n+  StubRoutines::x86::_vector_all_bits_set = generate_vector_mask(\"vector_all_bits_set\", 0xFFFFFFFFFFFFFFFF);\n+  StubRoutines::x86::_vector_int_mask_cmp_bits = generate_vector_mask(\"vector_int_mask_cmp_bits\", 0x0000000100000001);\n+  StubRoutines::x86::_vector_short_to_byte_mask = generate_vector_mask(\"vector_short_to_byte_mask\", 0x00ff00ff00ff00ff);\n+  StubRoutines::x86::_vector_byte_perm_mask = generate_vector_byte_perm_mask(\"vector_byte_perm_mask\");\n+  StubRoutines::x86::_vector_int_to_byte_mask = generate_vector_mask(\"vector_int_to_byte_mask\", 0x000000ff000000ff);\n+  StubRoutines::x86::_vector_int_to_short_mask = generate_vector_mask(\"vector_int_to_short_mask\", 0x0000ffff0000ffff);\n+  StubRoutines::x86::_vector_32_bit_mask = generate_vector_custom_i32(\"vector_32_bit_mask\", Assembler::AVX_512bit,\n+                                                                      0xFFFFFFFF, 0, 0, 0);\n+  StubRoutines::x86::_vector_64_bit_mask = generate_vector_custom_i32(\"vector_64_bit_mask\", Assembler::AVX_512bit,\n+                                                                      0xFFFFFFFF, 0xFFFFFFFF, 0, 0);\n+  StubRoutines::x86::_vector_int_shuffle_mask = generate_vector_mask(\"vector_int_shuffle_mask\", 0x0302010003020100);\n+  StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n+  StubRoutines::x86::_vector_short_shuffle_mask = generate_vector_mask(\"vector_short_shuffle_mask\", 0x0100010001000100);\n+  StubRoutines::x86::_vector_long_shuffle_mask = generate_vector_mask(\"vector_long_shuffle_mask\", 0x0000000100000000);\n+  StubRoutines::x86::_vector_long_sign_mask = generate_vector_mask(\"vector_long_sign_mask\", 0x8000000000000000);\n+  StubRoutines::x86::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n+  StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n+  StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n+  StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n+\n+  if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n+    \/\/ lut implementation influenced by counting 1s algorithm from section 5-1 of Hackers' Delight.\n+    StubRoutines::x86::_vector_popcount_lut = generate_popcount_avx_lut(\"popcount_lut\");\n+  }\n+\n+  \/\/ support for verify_oop (must happen after universe_init)\n+  if (VerifyOops) {\n+    StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n+  }\n+\n+  \/\/ data cache line writeback\n+  StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n+  StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n+\n+  \/\/ arraycopy stubs used by compilers\n+  generate_arraycopy_stubs();\n+\n+  \/\/ don't bother generating these AES intrinsic stubs unless global flag is set\n+  if (UseAESIntrinsics) {\n+    StubRoutines::x86::_key_shuffle_mask_addr = generate_key_shuffle_mask();  \/\/ needed by the others\n+    StubRoutines::_aescrypt_encryptBlock = generate_aescrypt_encryptBlock();\n+    StubRoutines::_aescrypt_decryptBlock = generate_aescrypt_decryptBlock();\n+    StubRoutines::_cipherBlockChaining_encryptAESCrypt = generate_cipherBlockChaining_encryptAESCrypt();\n+    if (VM_Version::supports_avx512_vaes() &&  VM_Version::supports_avx512vl() && VM_Version::supports_avx512dq() ) {\n+      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptVectorAESCrypt();\n+      StubRoutines::_electronicCodeBook_encryptAESCrypt = generate_electronicCodeBook_encryptAESCrypt();\n+      StubRoutines::_electronicCodeBook_decryptAESCrypt = generate_electronicCodeBook_decryptAESCrypt();\n+      StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n+      StubRoutines::x86::_ghash_poly512_addr = ghash_polynomial512_addr();\n+      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n+      StubRoutines::_galoisCounterMode_AESCrypt = generate_galoisCounterMode_AESCrypt();\n+    } else {\n+      StubRoutines::_cipherBlockChaining_decryptAESCrypt = generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n@@ -8209,0 +5669,1 @@\n+  }\n@@ -8210,20 +5671,4 @@\n-\n-    if (UseBASE64Intrinsics) {\n-      if(VM_Version::supports_avx2() &&\n-         VM_Version::supports_avx512bw() &&\n-         VM_Version::supports_avx512vl()) {\n-        StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n-        StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n-        StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n-      }\n-      StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n-      if (VM_Version::supports_avx512_vbmi()) {\n-        StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n-        StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n-        StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n-        StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n-        StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n-        StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n-        StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n-        StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n-        StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n+  if (UseAESCTRIntrinsics) {\n+    if (VM_Version::supports_avx512_vaes() && VM_Version::supports_avx512bw() && VM_Version::supports_avx512vl()) {\n+      if (StubRoutines::x86::_counter_mask_addr == NULL) {\n+        StubRoutines::x86::_counter_mask_addr = counter_mask_addr();\n@@ -8231,3 +5676,49 @@\n-      StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n-      StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n-      StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_VectorAESCrypt();\n+    } else {\n+      StubRoutines::x86::_counter_shuffle_mask_addr = generate_counter_shuffle_mask();\n+      StubRoutines::_counterMode_AESCrypt = generate_counterMode_AESCrypt_Parallel();\n+    }\n+  }\n+\n+  if (UseMD5Intrinsics) {\n+    StubRoutines::_md5_implCompress = generate_md5_implCompress(false, \"md5_implCompress\");\n+    StubRoutines::_md5_implCompressMB = generate_md5_implCompress(true, \"md5_implCompressMB\");\n+  }\n+  if (UseSHA1Intrinsics) {\n+    StubRoutines::x86::_upper_word_mask_addr = generate_upper_word_mask();\n+    StubRoutines::x86::_shuffle_byte_flip_mask_addr = generate_shuffle_byte_flip_mask();\n+    StubRoutines::_sha1_implCompress = generate_sha1_implCompress(false, \"sha1_implCompress\");\n+    StubRoutines::_sha1_implCompressMB = generate_sha1_implCompress(true, \"sha1_implCompressMB\");\n+  }\n+  if (UseSHA256Intrinsics) {\n+    StubRoutines::x86::_k256_adr = (address)StubRoutines::x86::_k256;\n+    char* dst = (char*)StubRoutines::x86::_k256_W;\n+    char* src = (char*)StubRoutines::x86::_k256;\n+    for (int ii = 0; ii < 16; ++ii) {\n+      memcpy(dst + 32 * ii,      src + 16 * ii, 16);\n+      memcpy(dst + 32 * ii + 16, src + 16 * ii, 16);\n+    }\n+    StubRoutines::x86::_k256_W_adr = (address)StubRoutines::x86::_k256_W;\n+    StubRoutines::x86::_pshuffle_byte_flip_mask_addr = generate_pshuffle_byte_flip_mask();\n+    StubRoutines::_sha256_implCompress = generate_sha256_implCompress(false, \"sha256_implCompress\");\n+    StubRoutines::_sha256_implCompressMB = generate_sha256_implCompress(true, \"sha256_implCompressMB\");\n+  }\n+  if (UseSHA512Intrinsics) {\n+    StubRoutines::x86::_k512_W_addr = (address)StubRoutines::x86::_k512_W;\n+    StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = generate_pshuffle_byte_flip_mask_sha512();\n+    StubRoutines::_sha512_implCompress = generate_sha512_implCompress(false, \"sha512_implCompress\");\n+    StubRoutines::_sha512_implCompressMB = generate_sha512_implCompress(true, \"sha512_implCompressMB\");\n+  }\n+\n+  \/\/ Generate GHASH intrinsics code\n+  if (UseGHASHIntrinsics) {\n+    if (StubRoutines::x86::_ghash_long_swap_mask_addr == NULL) {\n+      StubRoutines::x86::_ghash_long_swap_mask_addr = generate_ghash_long_swap_mask();\n+    }\n+  StubRoutines::x86::_ghash_byte_swap_mask_addr = generate_ghash_byte_swap_mask();\n+    if (VM_Version::supports_avx()) {\n+      StubRoutines::x86::_ghash_shuffmask_addr = ghash_shufflemask_addr();\n+      StubRoutines::x86::_ghash_poly_addr = ghash_polynomial_addr();\n+      StubRoutines::_ghash_processBlocks = generate_avx_ghash_processBlocks();\n+    } else {\n+      StubRoutines::_ghash_processBlocks = generate_ghash_processBlocks();\n@@ -8235,0 +5726,1 @@\n+  }\n@@ -8236,21 +5728,8 @@\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != NULL) {\n-      StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n-#ifdef COMPILER2\n-    if (UseMultiplyToLenIntrinsic) {\n-      StubRoutines::_multiplyToLen = generate_multiplyToLen();\n-    }\n-    if (UseSquareToLenIntrinsic) {\n-      StubRoutines::_squareToLen = generate_squareToLen();\n-    }\n-    if (UseMulAddIntrinsic) {\n-      StubRoutines::_mulAdd = generate_mulAdd();\n-    }\n-    if (VM_Version::supports_avx512_vbmi2()) {\n-      StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n-      StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();\n-    }\n-    if (UseMontgomeryMultiplyIntrinsic) {\n-      StubRoutines::_montgomeryMultiply\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n+\n+  if (UseBASE64Intrinsics) {\n+    if(VM_Version::supports_avx2() &&\n+       VM_Version::supports_avx512bw() &&\n+       VM_Version::supports_avx512vl()) {\n+      StubRoutines::x86::_avx2_shuffle_base64 = base64_avx2_shuffle_addr();\n+      StubRoutines::x86::_avx2_input_mask_base64 = base64_avx2_input_mask_addr();\n+      StubRoutines::x86::_avx2_lut_base64 = base64_avx2_lut_addr();\n@@ -8258,3 +5737,11 @@\n-    if (UseMontgomerySquareIntrinsic) {\n-      StubRoutines::_montgomerySquare\n-        = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n+    StubRoutines::x86::_encoding_table_base64 = base64_encoding_table_addr();\n+    if (VM_Version::supports_avx512_vbmi()) {\n+      StubRoutines::x86::_shuffle_base64 = base64_shuffle_addr();\n+      StubRoutines::x86::_lookup_lo_base64 = base64_vbmi_lookup_lo_addr();\n+      StubRoutines::x86::_lookup_hi_base64 = base64_vbmi_lookup_hi_addr();\n+      StubRoutines::x86::_lookup_lo_base64url = base64_vbmi_lookup_lo_url_addr();\n+      StubRoutines::x86::_lookup_hi_base64url = base64_vbmi_lookup_hi_url_addr();\n+      StubRoutines::x86::_pack_vec_base64 = base64_vbmi_pack_vec_addr();\n+      StubRoutines::x86::_join_0_1_base64 = base64_vbmi_join_0_1_addr();\n+      StubRoutines::x86::_join_1_2_base64 = base64_vbmi_join_1_2_addr();\n+      StubRoutines::x86::_join_2_3_base64 = base64_vbmi_join_2_3_addr();\n@@ -8262,0 +5749,4 @@\n+    StubRoutines::x86::_decoding_table_base64 = base64_decoding_table_addr();\n+    StubRoutines::_base64_encodeBlock = generate_base64_encodeBlock();\n+    StubRoutines::_base64_decodeBlock = generate_base64_decodeBlock();\n+  }\n@@ -8263,39 +5754,52 @@\n-    \/\/ Get svml stub routine addresses\n-    void *libjsvml = NULL;\n-    char ebuf[1024];\n-    char dll_name[JVM_MAXPATHLEN];\n-    if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"jsvml\")) {\n-      libjsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n-    }\n-    if (libjsvml != NULL) {\n-      \/\/ SVML method naming convention\n-      \/\/   All the methods are named as __jsvml_op<T><N>_ha_<VV>\n-      \/\/   Where:\n-      \/\/      ha stands for high accuracy\n-      \/\/      <T> is optional to indicate float\/double\n-      \/\/              Set to f for vector float operation\n-      \/\/              Omitted for vector double operation\n-      \/\/      <N> is the number of elements in the vector\n-      \/\/              1, 2, 4, 8, 16\n-      \/\/              e.g. 128 bit float vector has 4 float elements\n-      \/\/      <VV> indicates the avx\/sse level:\n-      \/\/              z0 is AVX512, l9 is AVX2, e9 is AVX1 and ex is for SSE2\n-      \/\/      e.g. __jsvml_expf16_ha_z0 is the method for computing 16 element vector float exp using AVX 512 insns\n-      \/\/           __jsvml_exp8_ha_z0 is the method for computing 8 element vector double exp using AVX 512 insns\n-\n-      log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"jsvml\" JNI_LIB_SUFFIX, p2i(libjsvml));\n-      if (UseAVX > 2) {\n-        for (int op = 0; op < VectorSupport::NUM_SVML_OP; op++) {\n-          int vop = VectorSupport::VECTOR_OP_SVML_START + op;\n-          if ((!VM_Version::supports_avx512dq()) &&\n-              (vop == VectorSupport::VECTOR_OP_LOG || vop == VectorSupport::VECTOR_OP_LOG10 || vop == VectorSupport::VECTOR_OP_POW)) {\n-            continue;\n-          }\n-          snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf16_ha_z0\", VectorSupport::svmlname[op]);\n-          StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-\n-          snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s8_ha_z0\", VectorSupport::svmlname[op]);\n-          StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-        }\n-      }\n-      const char* avx_sse_str = (UseAVX >= 2) ? \"l9\" : ((UseAVX == 1) ? \"e9\" : \"ex\");\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != NULL) {\n+    StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n+  }\n+#ifdef COMPILER2\n+  if (UseMultiplyToLenIntrinsic) {\n+    StubRoutines::_multiplyToLen = generate_multiplyToLen();\n+  }\n+  if (UseSquareToLenIntrinsic) {\n+    StubRoutines::_squareToLen = generate_squareToLen();\n+  }\n+  if (UseMulAddIntrinsic) {\n+    StubRoutines::_mulAdd = generate_mulAdd();\n+  }\n+  if (VM_Version::supports_avx512_vbmi2()) {\n+    StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n+    StubRoutines::_bigIntegerLeftShiftWorker = generate_bigIntegerLeftShift();\n+  }\n+  if (UseMontgomeryMultiplyIntrinsic) {\n+    StubRoutines::_montgomeryMultiply\n+      = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_multiply);\n+  }\n+  if (UseMontgomerySquareIntrinsic) {\n+    StubRoutines::_montgomerySquare\n+      = CAST_FROM_FN_PTR(address, SharedRuntime::montgomery_square);\n+  }\n+\n+  \/\/ Get svml stub routine addresses\n+  void *libjsvml = NULL;\n+  char ebuf[1024];\n+  char dll_name[JVM_MAXPATHLEN];\n+  if (os::dll_locate_lib(dll_name, sizeof(dll_name), Arguments::get_dll_dir(), \"jsvml\")) {\n+    libjsvml = os::dll_load(dll_name, ebuf, sizeof ebuf);\n+  }\n+  if (libjsvml != NULL) {\n+    \/\/ SVML method naming convention\n+    \/\/   All the methods are named as __jsvml_op<T><N>_ha_<VV>\n+    \/\/   Where:\n+    \/\/      ha stands for high accuracy\n+    \/\/      <T> is optional to indicate float\/double\n+    \/\/              Set to f for vector float operation\n+    \/\/              Omitted for vector double operation\n+    \/\/      <N> is the number of elements in the vector\n+    \/\/              1, 2, 4, 8, 16\n+    \/\/              e.g. 128 bit float vector has 4 float elements\n+    \/\/      <VV> indicates the avx\/sse level:\n+    \/\/              z0 is AVX512, l9 is AVX2, e9 is AVX1 and ex is for SSE2\n+    \/\/      e.g. __jsvml_expf16_ha_z0 is the method for computing 16 element vector float exp using AVX 512 insns\n+    \/\/           __jsvml_exp8_ha_z0 is the method for computing 8 element vector double exp using AVX 512 insns\n+\n+    log_info(library)(\"Loaded library %s, handle \" INTPTR_FORMAT, JNI_LIB_PREFIX \"jsvml\" JNI_LIB_SUFFIX, p2i(libjsvml));\n+    if (UseAVX > 2) {\n@@ -8304,1 +5808,2 @@\n-        if (vop == VectorSupport::VECTOR_OP_POW) {\n+        if ((!VM_Version::supports_avx512dq()) &&\n+            (vop == VectorSupport::VECTOR_OP_LOG || vop == VectorSupport::VECTOR_OP_LOG10 || vop == VectorSupport::VECTOR_OP_POW)) {\n@@ -8307,2 +5812,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf16_ha_z0\", VectorSupport::svmlname[op]);\n+        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8310,2 +5815,12 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s8_ha_z0\", VectorSupport::svmlname[op]);\n+        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_512][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      }\n+    }\n+    const char* avx_sse_str = (UseAVX >= 2) ? \"l9\" : ((UseAVX == 1) ? \"e9\" : \"ex\");\n+    for (int op = 0; op < VectorSupport::NUM_SVML_OP; op++) {\n+      int vop = VectorSupport::VECTOR_OP_SVML_START + op;\n+      if (vop == VectorSupport::VECTOR_OP_POW) {\n+        continue;\n+      }\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8313,2 +5828,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf8_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8316,2 +5831,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s1_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%sf8_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_f_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8319,2 +5834,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s2_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s1_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_64][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8322,5 +5837,2 @@\n-        snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n-        StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n-      }\n-    }\n-#endif \/\/ COMPILER2\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s2_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_128][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8328,2 +5840,2 @@\n-    if (UseVectorizedMismatchIntrinsic) {\n-      StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n+      snprintf(ebuf, sizeof(ebuf), \"__jsvml_%s4_ha_%s\", VectorSupport::svmlname[op], avx_sse_str);\n+      StubRoutines::_vector_d_math[VectorSupport::VEC_SIZE_256][op] = (address)os::dll_lookup(libjsvml, ebuf);\n@@ -8332,0 +5844,1 @@\n+#endif \/\/ COMPILER2\n@@ -8333,9 +5846,2 @@\n- public:\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  if (UseVectorizedMismatchIntrinsic) {\n+    StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n@@ -8343,1 +5849,1 @@\n-}; \/\/ end class declaration\n+}\n@@ -8345,1 +5851,0 @@\n-#define UCM_TABLE_MAX_ENTRIES 16\n@@ -8348,1 +5853,1 @@\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+    UnsafeCopyMemory::create_table(16);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":5108,"deletions":7603,"binary":false,"changes":12711,"status":"modified"},{"patch":"@@ -0,0 +1,509 @@\n+\/*\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_X86_STUBGENERATOR_X86_64_HPP\n+#define CPU_X86_STUBGENERATOR_X86_64_HPP\n+\n+#include \"code\/codeBlob.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+\n+\/\/ Stub Code definitions\n+\n+class StubGenerator: public StubCodeGenerator {\n+ private:\n+\n+  \/\/ Call stubs are used to call Java from C.\n+  address generate_call_stub(address& return_address);\n+\n+  \/\/ Return point for a Java call if there's an exception thrown in\n+  \/\/ Java code.  The exception is caught and transformed into a\n+  \/\/ pending exception stored in JavaThread that can be tested from\n+  \/\/ within the VM.\n+  \/\/\n+  \/\/ Note: Usually the parameters are removed by the callee. In case\n+  \/\/ of an exception crossing an activation frame boundary, that is\n+  \/\/ not the case if the callee is compiled code => need to setup the\n+  \/\/ rsp.\n+  \/\/\n+  \/\/ rax: exception oop\n+\n+  address generate_catch_exception();\n+\n+  \/\/ Continuation point for runtime calls returning with a pending\n+  \/\/ exception.  The pending exception check happened in the runtime\n+  \/\/ or native call stub.  The pending exception in Thread is\n+  \/\/ converted into a Java-level exception.\n+  \/\/\n+  \/\/ Contract with Java-level exception handlers:\n+  \/\/ rax: exception\n+  \/\/ rdx: throwing pc\n+  \/\/\n+  \/\/ NOTE: At entry of this stub, exception-pc must be on stack !!\n+\n+  address generate_forward_exception();\n+\n+  \/\/ Support for intptr_t OrderAccess::fence()\n+  address generate_orderaccess_fence();\n+\n+  \/\/ Support for intptr_t get_previous_sp()\n+  \/\/\n+  \/\/ This routine is used to find the previous stack pointer for the\n+  \/\/ caller.\n+  address generate_get_previous_sp();\n+\n+  \/\/----------------------------------------------------------------------------------------------------\n+  \/\/ Support for void verify_mxcsr()\n+  \/\/\n+  \/\/ This routine is used with -Xcheck:jni to verify that native\n+  \/\/ JNI code does not return to Java code without restoring the\n+  \/\/ MXCSR register to our expected state.\n+\n+  address generate_verify_mxcsr();\n+\n+  address generate_f2i_fixup();\n+  address generate_f2l_fixup();\n+  address generate_d2i_fixup();\n+  address generate_d2l_fixup();\n+\n+  address generate_count_leading_zeros_lut(const char *stub_name);\n+  address generate_popcount_avx_lut(const char *stub_name);\n+  address generate_iota_indices(const char *stub_name);\n+  address generate_vector_reverse_bit_lut(const char *stub_name);\n+\n+  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name);\n+  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name);\n+  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name);\n+  address generate_vector_byte_shuffle_mask(const char *stub_name);\n+\n+  address generate_fp_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_byte_perm_mask(const char *stub_name);\n+\n+  address generate_vector_fp_mask(const char *stub_name, int64_t mask);\n+\n+  address generate_vector_custom_i32(const char *stub_name, Assembler::AvxVectorLen len,\n+                                     int32_t val0, int32_t val1, int32_t val2, int32_t val3,\n+                                     int32_t val4 = 0, int32_t val5 = 0, int32_t val6 = 0, int32_t val7 = 0,\n+                                     int32_t val8 = 0, int32_t val9 = 0, int32_t val10 = 0, int32_t val11 = 0,\n+                                     int32_t val12 = 0, int32_t val13 = 0, int32_t val14 = 0, int32_t val15 = 0);\n+\n+  \/\/ Non-destructive plausibility checks for oops\n+  address generate_verify_oop();\n+\n+  \/\/ Verify that a register contains clean 32-bits positive value\n+  \/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n+  void assert_clean_int(Register Rint, Register Rtmp);\n+\n+  \/\/  Generate overlap test for array copy stubs\n+  void array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf);\n+\n+  void array_overlap_test(address no_overlap_target, Address::ScaleFactor sf) {\n+    assert(no_overlap_target != NULL, \"must be generated\");\n+    array_overlap_test(no_overlap_target, NULL, sf);\n+  }\n+  void array_overlap_test(Label& L_no_overlap, Address::ScaleFactor sf) {\n+    array_overlap_test(NULL, &L_no_overlap, sf);\n+  }\n+\n+\n+  \/\/ Shuffle first three arg regs on Windows into Linux\/Solaris locations.\n+  void setup_arg_regs(int nargs = 3);\n+  void restore_arg_regs();\n+\n+#ifdef ASSERT\n+  bool _regs_in_thread;\n+#endif\n+\n+  \/\/ This is used in places where r10 is a scratch register, and can\n+  \/\/ be adapted if r9 is needed also.\n+  void setup_arg_regs_using_thread();\n+\n+  void restore_arg_regs_using_thread();\n+\n+  \/\/ Copy big chunks forward\n+  void copy_bytes_forward(Register end_from, Register end_to,\n+                          Register qword_count, Register to,\n+                          Label& L_copy_bytes, Label& L_copy_8_bytes);\n+\n+  \/\/ Copy big chunks backward\n+  void copy_bytes_backward(Register from, Register dest,\n+                           Register qword_count, Register to,\n+                           Label& L_copy_bytes, Label& L_copy_8_bytes);\n+\n+  void setup_argument_regs(BasicType type);\n+\n+  void restore_argument_regs(BasicType type);\n+\n+#if COMPILER2_OR_JVMCI\n+  \/\/ Following rules apply to AVX3 optimized arraycopy stubs:\n+  \/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n+  \/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n+  \/\/   default configuration.\n+  \/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n+  \/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n+  \/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n+  \/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n+  \/\/   copy performs better.\n+  \/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n+  \/\/   64 byte vector registers (ZMMs).\n+\n+  address generate_disjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             bool aligned, bool is_oop, bool dest_uninitialized);\n+\n+  address generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                             address nooverlap_target, bool aligned, bool is_oop,\n+                                             bool dest_uninitialized);\n+\n+  void arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                    Register to, Register count, int shift,\n+                                    Register index, Register temp,\n+                                    bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                             Register to, Register start_index, Register end_index,\n+                                             Register count, int shift, Register temp,\n+                                             bool use64byteVector, Label& L_entry, Label& L_exit);\n+\n+  void copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  int shift = Address::times_1, int offset = 0);\n+\n+  void copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                  bool conjoint, int shift = Address::times_1, int offset = 0,\n+                  bool use64byteVector = false);\n+\n+  void copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0,\n+                         bool use64byteVector = false);\n+\n+  void copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                         KRegister mask, Register length, Register index,\n+                         Register temp, int shift = Address::times_1, int offset = 0);\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n+  address generate_disjoint_byte_copy(bool aligned, address* entry, const char *name);\n+\n+  address generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n+                                      address* entry, const char *name);\n+\n+  address generate_disjoint_short_copy(bool aligned, address *entry, const char *name);\n+\n+  address generate_fill(BasicType t, bool aligned, const char *name);\n+\n+  address generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n+                                       address *entry, const char *name);\n+  address generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n+                                         const char *name, bool dest_uninitialized = false);\n+  address generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                         address *entry, const char *name,\n+                                         bool dest_uninitialized = false);\n+  address generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n+                                          const char *name, bool dest_uninitialized = false);\n+  address generate_conjoint_long_oop_copy(bool aligned, bool is_oop,\n+                                          address nooverlap_target, address *entry,\n+                                          const char *name, bool dest_uninitialized = false);\n+\n+  \/\/ Helper for generating a dynamic type check.\n+  \/\/ Smashes no registers.\n+  void generate_type_check(Register sub_klass,\n+                           Register super_check_offset,\n+                           Register super_klass,\n+                           Label& L_success);\n+\n+  \/\/ Generate checkcasting array copy stub\n+  address generate_checkcast_copy(const char *name, address *entry,\n+                                  bool dest_uninitialized = false);\n+\n+  \/\/ Generate 'unsafe' array copy stub\n+  \/\/ Though just as safe as the other stubs, it takes an unscaled\n+  \/\/ size_t argument instead of an element count.\n+  \/\/\n+  \/\/ Examines the alignment of the operands and dispatches\n+  \/\/ to a long, int, short, or byte copy loop.\n+  address generate_unsafe_copy(const char *name,\n+                               address byte_copy_entry, address short_copy_entry,\n+                               address int_copy_entry, address long_copy_entry);\n+\n+  \/\/ Perform range checks on the proposed arraycopy.\n+  \/\/ Kills temp, but nothing else.\n+  \/\/ Also, clean the sign bits of src_pos and dst_pos.\n+  void arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n+                              Register src_pos, \/\/ source position (c_rarg1)\n+                              Register dst,     \/\/ destination array oo (c_rarg2)\n+                              Register dst_pos, \/\/ destination position (c_rarg3)\n+                              Register length,\n+                              Register temp,\n+                              Label& L_failed);\n+\n+  \/\/ Generate generic array copy stubs\n+  address generate_generic_copy(const char *name,\n+                                address byte_copy_entry, address short_copy_entry,\n+                                address int_copy_entry, address oop_copy_entry,\n+                                address long_copy_entry, address checkcast_copy_entry);\n+\n+  address generate_data_cache_writeback();\n+\n+  address generate_data_cache_writeback_sync();\n+\n+  void generate_arraycopy_stubs();\n+\n+  \/\/ AES intrinsic stubs\n+\n+  enum {\n+    AESBlockSize = 16\n+  };\n+\n+  address generate_key_shuffle_mask();\n+\n+  address generate_counter_shuffle_mask();\n+\n+  \/\/ Utility routine for loading a 128-bit key word in little endian format\n+  \/\/ can optionally specify that the shuffle mask is already in an xmmregister\n+  void load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg);\n+\n+  \/\/ Utility routine for increase 128bit counter (iv in CTR mode)\n+  void inc_counter(Register reg, XMMRegister xmmdst, int inc_delta, Label& next_block);\n+\n+  address generate_aescrypt_encryptBlock();\n+\n+  address generate_aescrypt_decryptBlock();\n+\n+  address generate_cipherBlockChaining_encryptAESCrypt();\n+\n+  \/\/ A version of CBC\/AES Decrypt which does 4 blocks in a loop at a time\n+  \/\/ to hide instruction latency\n+  address generate_cipherBlockChaining_decryptAESCrypt_Parallel();\n+\n+  address generate_electronicCodeBook_encryptAESCrypt();\n+\n+  address generate_electronicCodeBook_decryptAESCrypt();\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.MD5.implCompress(byte[] b, int ofs)\n+  address generate_md5_implCompress(bool multi_block, const char *name);\n+\n+  address generate_upper_word_mask();\n+\n+  address generate_shuffle_byte_flip_mask();\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+  address generate_sha1_implCompress(bool multi_block, const char *name);\n+\n+  address generate_pshuffle_byte_flip_mask();\n+\n+  \/\/ Mask for byte-swapping a couple of qwords in an XMM register using (v)pshufb.\n+  address generate_pshuffle_byte_flip_mask_sha512();\n+\n+  \/\/ ofs and limit are use for multi-block byte array.\n+  \/\/ int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)\n+  address generate_sha256_implCompress(bool multi_block, const char *name);\n+  address generate_sha512_implCompress(bool multi_block, const char *name);\n+\n+  address ghash_polynomial512_addr();\n+\n+  \/\/ Vector AES Galois Counter Mode implementation\n+  address generate_galoisCounterMode_AESCrypt();\n+\n+  \/\/ This mask is used for incrementing counter value(linc0, linc4, etc.)\n+  address counter_mask_addr();\n+\n+ \/\/ Vector AES Counter implementation\n+  address generate_counterMode_VectorAESCrypt();\n+\n+  \/\/ This is a version of CTR\/AES crypt which does 6 blocks in a loop at a time\n+  \/\/ to hide instruction latency\n+  address generate_counterMode_AESCrypt_Parallel();\n+\n+  void roundDec(XMMRegister xmm_reg);\n+\n+  void roundDeclast(XMMRegister xmm_reg);\n+\n+  void ev_load_key(XMMRegister xmmdst, Register key, int offset, XMMRegister xmm_shuf_mask = xnoreg);\n+\n+  address generate_cipherBlockChaining_decryptVectorAESCrypt();\n+\n+  \/\/ Polynomial x^128+x^127+x^126+x^121+1\n+  address ghash_polynomial_addr();\n+\n+  address ghash_shufflemask_addr();\n+\n+  \/\/ Ghash single and multi block operations using AVX instructions\n+  address generate_avx_ghash_processBlocks();\n+\n+  \/\/ byte swap x86 long\n+  address generate_ghash_long_swap_mask();\n+\n+  \/\/ byte swap x86 byte array\n+  address generate_ghash_byte_swap_mask();\n+\n+  \/\/ Single and multi-block ghash operations\n+  address generate_ghash_processBlocks();\n+\n+  address base64_shuffle_addr();\n+  address base64_avx2_shuffle_addr();\n+  address base64_avx2_input_mask_addr();\n+  address base64_avx2_lut_addr();\n+  address base64_encoding_table_addr();\n+\n+  \/\/ Code for generating Base64 encoding.\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void encodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL)\n+  address generate_base64_encodeBlock();\n+\n+  \/\/ base64 AVX512vbmi tables\n+  address base64_vbmi_lookup_lo_addr();\n+  address base64_vbmi_lookup_hi_addr();\n+  address base64_vbmi_lookup_lo_url_addr();\n+  address base64_vbmi_lookup_hi_url_addr();\n+  address base64_vbmi_pack_vec_addr();\n+  address base64_vbmi_join_0_1_addr();\n+  address base64_vbmi_join_1_2_addr();\n+  address base64_vbmi_join_2_3_addr();\n+  address base64_decoding_table_addr();\n+\n+  \/\/ Code for generating Base64 decoding.\n+  \/\/\n+  \/\/ Based on the article (and associated code) from https:\/\/arxiv.org\/abs\/1910.05109.\n+  \/\/\n+  \/\/ Intrinsic function prototype in Base64.java:\n+  \/\/ private void decodeBlock(byte[] src, int sp, int sl, byte[] dst, int dp, boolean isURL, isMIME);\n+  address generate_base64_decodeBlock();\n+\n+  address generate_updateBytesCRC32();\n+  address generate_updateBytesCRC32C(bool is_pclmulqdq_supported);\n+\n+  address generate_updateBytesAdler32();\n+\n+  address generate_multiplyToLen();\n+\n+  address generate_vectorizedMismatch();\n+\n+  address generate_squareToLen();\n+\n+  address generate_method_entry_barrier();\n+\n+  address generate_mulAdd();\n+\n+  address generate_bigIntegerRightShift();\n+  address generate_bigIntegerLeftShift();\n+\n+\n+  \/\/ Libm trigonometric stubs\n+\n+  address generate_libmSin();\n+  address generate_libmCos();\n+  address generate_libmTan();\n+  address generate_libmExp();\n+  address generate_libmPow();\n+  address generate_libmLog();\n+  address generate_libmLog10();\n+\n+  \/\/ Shared constants\n+  static address ZERO;\n+  static address NEG_ZERO;\n+  static address ONE;\n+  static address ONEHALF;\n+  static address SIGN_MASK;\n+  static address TWO_POW_55;\n+  static address TWO_POW_M55;\n+  static address SHIFTER;\n+  static address PI32INV;\n+  static address PI_INV_TABLE;\n+  static address Ctable;\n+  static address SC_1;\n+  static address SC_2;\n+  static address SC_3;\n+  static address SC_4;\n+  static address PI_4;\n+  static address P_1;\n+  static address P_3;\n+  static address P_2;\n+\n+  void generate_libm_stubs();\n+\n+\n+  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind);\n+  address generate_cont_thaw();\n+\n+  \/\/ TODO: will probably need multiple return barriers depending on return type\n+  address generate_cont_returnBarrier();\n+  address generate_cont_returnBarrier_exception();\n+\n+#if INCLUDE_JFR\n+\n+  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+  \/\/ It returns a jobject handle to the event writer.\n+  \/\/ The handle is dereferenced and the return value is the event writer oop.\n+  RuntimeStub* generate_jfr_write_checkpoint();\n+\n+#endif \/\/ INCLUDE_JFR\n+\n+  \/\/ Continuation point for throwing of implicit exceptions that are\n+  \/\/ not handled in the current activation. Fabricates an exception\n+  \/\/ oop and initiates normal exception dispatching in this\n+  \/\/ frame. Since we need to preserve callee-saved values (currently\n+  \/\/ only for C2, but done for C1 as well) we need a callee-saved oop\n+  \/\/ map and therefore have to make these stubs into RuntimeStubs\n+  \/\/ rather than BufferBlobs.  If the compiler needs all registers to\n+  \/\/ be preserved between the fault point and the exception handler\n+  \/\/ then it must assume responsibility for that in\n+  \/\/ AbstractCompiler::continuation_for_implicit_null_exception or\n+  \/\/ continuation_for_implicit_division_by_zero_exception. All other\n+  \/\/ implicit exceptions (e.g., NullPointerException or\n+  \/\/ AbstractMethodError on entry) are either at call sites or\n+  \/\/ otherwise assume that stack unwinding will be initiated, so\n+  \/\/ caller saved registers were assumed volatile in the compiler.\n+  address generate_throw_exception(const char* name,\n+                                   address runtime_entry,\n+                                   Register arg1 = noreg,\n+                                   Register arg2 = noreg);\n+\n+  \/\/ interpreter or compiled code marshalling registers to\/from inline type instance\n+  address generate_return_value_stub(address destination, const char* name, bool has_res);\n+\n+  void create_control_words();\n+\n+  \/\/ Initialization\n+  void generate_initial();\n+  void generate_phase1();\n+  void generate_all();\n+\n+ public:\n+  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n+    DEBUG_ONLY( _regs_in_thread = false; )\n+    if (phase == 0) {\n+      generate_initial();\n+    } else if (phase == 1) {\n+      generate_phase1(); \/\/ stubs that must be available for the interpreter\n+    } else {\n+      generate_all();\n+    }\n+  }\n+};\n+\n+#endif \/\/ CPU_X86_STUBGENERATOR_X86_64_HPP\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":509,"deletions":0,"binary":false,"changes":509,"status":"added"},{"patch":"@@ -0,0 +1,2570 @@\n+\/*\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n+#include \"oops\/objArrayKlass.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"stubGenerator_x86_64.hpp\"\n+#ifdef COMPILER2\n+#include \"opto\/c2_globals.hpp\"\n+#endif\n+\n+#define __ _masm->\n+\n+#define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)\n+\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif \/\/ PRODUCT\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+#ifdef PRODUCT\n+#define INC_COUNTER_NP(counter, rscratch) ((void)0)\n+#else\n+#define INC_COUNTER_NP(counter, rscratch) \\\n+BLOCK_COMMENT(\"inc_counter \" #counter); \\\n+inc_counter_np(_masm, counter, rscratch);\n+\n+static void inc_counter_np(MacroAssembler* _masm, int& counter, Register rscratch) {\n+  __ incrementl(ExternalAddress((address)&counter), rscratch);\n+}\n+\n+#if COMPILER2_OR_JVMCI\n+static int& get_profile_ctr(int shift) {\n+  if (shift == 0) {\n+    return SharedRuntime::_jbyte_array_copy_ctr;\n+  } else if (shift == 1) {\n+    return SharedRuntime::_jshort_array_copy_ctr;\n+  } else if (shift == 2) {\n+    return SharedRuntime::_jint_array_copy_ctr;\n+  } else {\n+    assert(shift == 3, \"\");\n+    return SharedRuntime::_jlong_array_copy_ctr;\n+  }\n+}\n+#endif \/\/ COMPILER2_OR_JVMCI\n+#endif \/\/ !PRODUCT\n+\n+void StubGenerator::generate_arraycopy_stubs() {\n+  address entry;\n+  address entry_jbyte_arraycopy;\n+  address entry_jshort_arraycopy;\n+  address entry_jint_arraycopy;\n+  address entry_oop_arraycopy;\n+  address entry_jlong_arraycopy;\n+  address entry_checkcast_arraycopy;\n+\n+  StubRoutines::_jbyte_disjoint_arraycopy  = generate_disjoint_byte_copy(false, &entry,\n+                                                                         \"jbyte_disjoint_arraycopy\");\n+  StubRoutines::_jbyte_arraycopy           = generate_conjoint_byte_copy(false, entry, &entry_jbyte_arraycopy,\n+                                                                         \"jbyte_arraycopy\");\n+\n+  StubRoutines::_jshort_disjoint_arraycopy = generate_disjoint_short_copy(false, &entry,\n+                                                                          \"jshort_disjoint_arraycopy\");\n+  StubRoutines::_jshort_arraycopy          = generate_conjoint_short_copy(false, entry, &entry_jshort_arraycopy,\n+                                                                          \"jshort_arraycopy\");\n+\n+  StubRoutines::_jint_disjoint_arraycopy   = generate_disjoint_int_oop_copy(false, false, &entry,\n+                                                                            \"jint_disjoint_arraycopy\");\n+  StubRoutines::_jint_arraycopy            = generate_conjoint_int_oop_copy(false, false, entry,\n+                                                                            &entry_jint_arraycopy, \"jint_arraycopy\");\n+\n+  StubRoutines::_jlong_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, false, &entry,\n+                                                                             \"jlong_disjoint_arraycopy\");\n+  StubRoutines::_jlong_arraycopy           = generate_conjoint_long_oop_copy(false, false, entry,\n+                                                                             &entry_jlong_arraycopy, \"jlong_arraycopy\");\n+  if (UseCompressedOops) {\n+    StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_int_oop_copy(false, true, &entry,\n+                                                                            \"oop_disjoint_arraycopy\");\n+    StubRoutines::_oop_arraycopy           = generate_conjoint_int_oop_copy(false, true, entry,\n+                                                                            &entry_oop_arraycopy, \"oop_arraycopy\");\n+    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_int_oop_copy(false, true, &entry,\n+                                                                                   \"oop_disjoint_arraycopy_uninit\",\n+                                                                                   \/*dest_uninitialized*\/true);\n+    StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_int_oop_copy(false, true, entry,\n+                                                                                   NULL, \"oop_arraycopy_uninit\",\n+                                                                                   \/*dest_uninitialized*\/true);\n+  } else {\n+    StubRoutines::_oop_disjoint_arraycopy  = generate_disjoint_long_oop_copy(false, true, &entry,\n+                                                                             \"oop_disjoint_arraycopy\");\n+    StubRoutines::_oop_arraycopy           = generate_conjoint_long_oop_copy(false, true, entry,\n+                                                                             &entry_oop_arraycopy, \"oop_arraycopy\");\n+    StubRoutines::_oop_disjoint_arraycopy_uninit  = generate_disjoint_long_oop_copy(false, true, &entry,\n+                                                                                    \"oop_disjoint_arraycopy_uninit\",\n+                                                                                    \/*dest_uninitialized*\/true);\n+    StubRoutines::_oop_arraycopy_uninit           = generate_conjoint_long_oop_copy(false, true, entry,\n+                                                                                    NULL, \"oop_arraycopy_uninit\",\n+                                                                                    \/*dest_uninitialized*\/true);\n+  }\n+\n+  StubRoutines::_checkcast_arraycopy        = generate_checkcast_copy(\"checkcast_arraycopy\", &entry_checkcast_arraycopy);\n+  StubRoutines::_checkcast_arraycopy_uninit = generate_checkcast_copy(\"checkcast_arraycopy_uninit\", NULL,\n+                                                                      \/*dest_uninitialized*\/true);\n+\n+  StubRoutines::_unsafe_arraycopy    = generate_unsafe_copy(\"unsafe_arraycopy\",\n+                                                            entry_jbyte_arraycopy,\n+                                                            entry_jshort_arraycopy,\n+                                                            entry_jint_arraycopy,\n+                                                            entry_jlong_arraycopy);\n+  StubRoutines::_generic_arraycopy   = generate_generic_copy(\"generic_arraycopy\",\n+                                                             entry_jbyte_arraycopy,\n+                                                             entry_jshort_arraycopy,\n+                                                             entry_jint_arraycopy,\n+                                                             entry_oop_arraycopy,\n+                                                             entry_jlong_arraycopy,\n+                                                             entry_checkcast_arraycopy);\n+\n+  StubRoutines::_jbyte_fill = generate_fill(T_BYTE, false, \"jbyte_fill\");\n+  StubRoutines::_jshort_fill = generate_fill(T_SHORT, false, \"jshort_fill\");\n+  StubRoutines::_jint_fill = generate_fill(T_INT, false, \"jint_fill\");\n+  StubRoutines::_arrayof_jbyte_fill = generate_fill(T_BYTE, true, \"arrayof_jbyte_fill\");\n+  StubRoutines::_arrayof_jshort_fill = generate_fill(T_SHORT, true, \"arrayof_jshort_fill\");\n+  StubRoutines::_arrayof_jint_fill = generate_fill(T_INT, true, \"arrayof_jint_fill\");\n+\n+  \/\/ We don't generate specialized code for HeapWord-aligned source\n+  \/\/ arrays, so just use the code we've already generated\n+  StubRoutines::_arrayof_jbyte_disjoint_arraycopy  = StubRoutines::_jbyte_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jbyte_arraycopy           = StubRoutines::_jbyte_arraycopy;\n+\n+  StubRoutines::_arrayof_jshort_disjoint_arraycopy = StubRoutines::_jshort_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jshort_arraycopy          = StubRoutines::_jshort_arraycopy;\n+\n+  StubRoutines::_arrayof_jint_disjoint_arraycopy   = StubRoutines::_jint_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jint_arraycopy            = StubRoutines::_jint_arraycopy;\n+\n+  StubRoutines::_arrayof_jlong_disjoint_arraycopy  = StubRoutines::_jlong_disjoint_arraycopy;\n+  StubRoutines::_arrayof_jlong_arraycopy           = StubRoutines::_jlong_arraycopy;\n+\n+  StubRoutines::_arrayof_oop_disjoint_arraycopy    = StubRoutines::_oop_disjoint_arraycopy;\n+  StubRoutines::_arrayof_oop_arraycopy             = StubRoutines::_oop_arraycopy;\n+\n+  StubRoutines::_arrayof_oop_disjoint_arraycopy_uninit    = StubRoutines::_oop_disjoint_arraycopy_uninit;\n+  StubRoutines::_arrayof_oop_arraycopy_uninit             = StubRoutines::_oop_arraycopy_uninit;\n+}\n+\n+\n+\/\/ Verify that a register contains clean 32-bits positive value\n+\/\/ (high 32-bits are 0) so it could be used in 64-bits shifts.\n+\/\/\n+\/\/  Input:\n+\/\/    Rint  -  32-bits value\n+\/\/    Rtmp  -  scratch\n+\/\/\n+void StubGenerator::assert_clean_int(Register Rint, Register Rtmp) {\n+#ifdef ASSERT\n+  Label L;\n+  assert_different_registers(Rtmp, Rint);\n+  __ movslq(Rtmp, Rint);\n+  __ cmpq(Rtmp, Rint);\n+  __ jcc(Assembler::equal, L);\n+  __ stop(\"high 32-bits of int value are not 0\");\n+  __ bind(L);\n+#endif\n+}\n+\n+\n+\/\/  Generate overlap test for array copy stubs\n+\/\/\n+\/\/  Input:\n+\/\/     c_rarg0 - from\n+\/\/     c_rarg1 - to\n+\/\/     c_rarg2 - element count\n+\/\/\n+\/\/  Output:\n+\/\/     rax   - &from[element count - 1]\n+\/\/\n+void StubGenerator::array_overlap_test(address no_overlap_target, Label* NOLp, Address::ScaleFactor sf) {\n+  const Register from     = c_rarg0;\n+  const Register to       = c_rarg1;\n+  const Register count    = c_rarg2;\n+  const Register end_from = rax;\n+\n+  __ cmpptr(to, from);\n+  __ lea(end_from, Address(from, count, sf, 0));\n+  if (NOLp == NULL) {\n+    ExternalAddress no_overlap(no_overlap_target);\n+    __ jump_cc(Assembler::belowEqual, no_overlap);\n+    __ cmpptr(to, end_from);\n+    __ jump_cc(Assembler::aboveEqual, no_overlap);\n+  } else {\n+    __ jcc(Assembler::belowEqual, (*NOLp));\n+    __ cmpptr(to, end_from);\n+    __ jcc(Assembler::aboveEqual, (*NOLp));\n+  }\n+}\n+\n+\n+\/\/ Copy big chunks forward\n+\/\/\n+\/\/ Inputs:\n+\/\/   end_from     - source arrays end address\n+\/\/   end_to       - destination array end address\n+\/\/   qword_count  - 64-bits element count, negative\n+\/\/   to           - scratch\n+\/\/   L_copy_bytes - entry label\n+\/\/   L_copy_8_bytes  - exit  label\n+\/\/\n+void StubGenerator::copy_bytes_forward(Register end_from, Register end_to,\n+                                       Register qword_count, Register to,\n+                                       Label& L_copy_bytes, Label& L_copy_8_bytes) {\n+  DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n+  Label L_loop;\n+  __ align(OptoLoopAlignment);\n+  if (UseUnalignedLoadStores) {\n+    Label L_end;\n+    __ BIND(L_loop);\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n+      __ vmovdqu(xmm1, Address(end_from, qword_count, Address::times_8, -24));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm1);\n+    } else {\n+      __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -56));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -56), xmm0);\n+      __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, -40));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -40), xmm1);\n+      __ movdqu(xmm2, Address(end_from, qword_count, Address::times_8, -24));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm2);\n+      __ movdqu(xmm3, Address(end_from, qword_count, Address::times_8, - 8));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm3);\n+    }\n+\n+    __ BIND(L_copy_bytes);\n+    __ addptr(qword_count, 8);\n+    __ jcc(Assembler::lessEqual, L_loop);\n+    __ subptr(qword_count, 4);  \/\/ sub(8) and add(4)\n+    __ jccb(Assembler::greater, L_end);\n+    \/\/ Copy trailing 32 bytes\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n+      __ vmovdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n+    } else {\n+      __ movdqu(xmm0, Address(end_from, qword_count, Address::times_8, -24));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, -24), xmm0);\n+      __ movdqu(xmm1, Address(end_from, qword_count, Address::times_8, - 8));\n+      __ movdqu(Address(end_to, qword_count, Address::times_8, - 8), xmm1);\n+    }\n+    __ addptr(qword_count, 4);\n+    __ BIND(L_end);\n+  } else {\n+    \/\/ Copy 32-bytes per iteration\n+    __ BIND(L_loop);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, -24));\n+    __ movq(Address(end_to, qword_count, Address::times_8, -24), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, -16));\n+    __ movq(Address(end_to, qword_count, Address::times_8, -16), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, - 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, - 8), to);\n+    __ movq(to, Address(end_from, qword_count, Address::times_8, - 0));\n+    __ movq(Address(end_to, qword_count, Address::times_8, - 0), to);\n+\n+    __ BIND(L_copy_bytes);\n+    __ addptr(qword_count, 4);\n+    __ jcc(Assembler::lessEqual, L_loop);\n+  }\n+  __ subptr(qword_count, 4);\n+  __ jcc(Assembler::less, L_copy_8_bytes); \/\/ Copy trailing qwords\n+}\n+\n+\n+\/\/ Copy big chunks backward\n+\/\/\n+\/\/ Inputs:\n+\/\/   from         - source arrays address\n+\/\/   dest         - destination array address\n+\/\/   qword_count  - 64-bits element count\n+\/\/   to           - scratch\n+\/\/   L_copy_bytes - entry label\n+\/\/   L_copy_8_bytes  - exit  label\n+\/\/\n+void StubGenerator::copy_bytes_backward(Register from, Register dest,\n+                                        Register qword_count, Register to,\n+                                        Label& L_copy_bytes, Label& L_copy_8_bytes) {\n+  DEBUG_ONLY(__ stop(\"enter at entry label, not here\"));\n+  Label L_loop;\n+  __ align(OptoLoopAlignment);\n+  if (UseUnalignedLoadStores) {\n+    Label L_end;\n+    __ BIND(L_loop);\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 32));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8, 32), xmm0);\n+      __ vmovdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n+    } else {\n+      __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 48));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 48), xmm0);\n+      __ movdqu(xmm1, Address(from, qword_count, Address::times_8, 32));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 32), xmm1);\n+      __ movdqu(xmm2, Address(from, qword_count, Address::times_8, 16));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm2);\n+      __ movdqu(xmm3, Address(from, qword_count, Address::times_8,  0));\n+      __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm3);\n+    }\n+\n+    __ BIND(L_copy_bytes);\n+    __ subptr(qword_count, 8);\n+    __ jcc(Assembler::greaterEqual, L_loop);\n+\n+    __ addptr(qword_count, 4);  \/\/ add(8) and sub(4)\n+    __ jccb(Assembler::less, L_end);\n+    \/\/ Copy trailing 32 bytes\n+    if (UseAVX >= 2) {\n+      __ vmovdqu(xmm0, Address(from, qword_count, Address::times_8, 0));\n+      __ vmovdqu(Address(dest, qword_count, Address::times_8, 0), xmm0);\n+    } else {\n+      __ movdqu(xmm0, Address(from, qword_count, Address::times_8, 16));\n+      __ movdqu(Address(dest, qword_count, Address::times_8, 16), xmm0);\n+      __ movdqu(xmm1, Address(from, qword_count, Address::times_8,  0));\n+      __ movdqu(Address(dest, qword_count, Address::times_8,  0), xmm1);\n+    }\n+    __ subptr(qword_count, 4);\n+    __ BIND(L_end);\n+  } else {\n+    \/\/ Copy 32-bytes per iteration\n+    __ BIND(L_loop);\n+    __ movq(to, Address(from, qword_count, Address::times_8, 24));\n+    __ movq(Address(dest, qword_count, Address::times_8, 24), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8, 16));\n+    __ movq(Address(dest, qword_count, Address::times_8, 16), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8,  8));\n+    __ movq(Address(dest, qword_count, Address::times_8,  8), to);\n+    __ movq(to, Address(from, qword_count, Address::times_8,  0));\n+    __ movq(Address(dest, qword_count, Address::times_8,  0), to);\n+\n+    __ BIND(L_copy_bytes);\n+    __ subptr(qword_count, 4);\n+    __ jcc(Assembler::greaterEqual, L_loop);\n+  }\n+  __ addptr(qword_count, 4);\n+  __ jcc(Assembler::greater, L_copy_8_bytes); \/\/ Copy trailing qwords\n+}\n+\n+#if COMPILER2_OR_JVMCI\n+\n+\/\/ Note: Following rules apply to AVX3 optimized arraycopy stubs:-\n+\/\/ - If target supports AVX3 features (BW+VL+F) then implementation uses 32 byte vectors (YMMs)\n+\/\/   for both special cases (various small block sizes) and aligned copy loop. This is the\n+\/\/   default configuration.\n+\/\/ - If copy length is above AVX3Threshold, then implementation use 64 byte vectors (ZMMs)\n+\/\/   for main copy loop (and subsequent tail) since bulk of the cycles will be consumed in it.\n+\/\/ - If user forces MaxVectorSize=32 then above 4096 bytes its seen that REP MOVs shows a\n+\/\/   better performance for disjoint copies. For conjoint\/backward copy vector based\n+\/\/   copy performs better.\n+\/\/ - If user sets AVX3Threshold=0, then special cases for small blocks sizes operate over\n+\/\/   64 byte vector registers (ZMMs).\n+\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_copy_avx3_masked is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_[byte\/int\/short\/long]_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_copy_avx3_masked(address* entry, const char *name,\n+                                                          int shift, bool aligned, bool is_oop,\n+                                                          bool dest_uninitialized) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  int avx3threshold = VM_Version::avx3_threshold();\n+  bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n+  Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+  Label L_repmovs, L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register temp1       = r8;\n+  const Register temp2       = r11;\n+  const Register temp3       = rax;\n+  const Register temp4       = rcx;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+  setup_argument_regs(type);\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  {\n+    \/\/ Type(shift)           byte(0), short(1), int(2),   long(3)\n+    int loop_size[]        = { 192,     96,       48,      24};\n+    int threshold[]        = { 4096,    2048,     1024,    512};\n+\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+\n+    \/\/ temp1 holds remaining count and temp4 holds running count used to compute\n+    \/\/ next address offset for start of to\/from addresses (temp4 * scale).\n+    __ mov64(temp4, 0);\n+    __ movq(temp1, count);\n+\n+    \/\/ Zero length check.\n+    __ BIND(L_tail);\n+    __ cmpq(temp1, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    \/\/ Special cases using 32 byte [masked] vector copy operations.\n+    arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                 temp4, temp3, use64byteVector, L_entry, L_exit);\n+\n+    \/\/ PRE-MAIN-POST loop for aligned copy.\n+    __ BIND(L_entry);\n+\n+    if (avx3threshold != 0) {\n+      __ cmpq(count, threshold[shift]);\n+      if (MaxVectorSize == 64) {\n+        \/\/ Copy using 64 byte vectors.\n+        __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+      } else {\n+        assert(MaxVectorSize < 64, \"vector size should be < 64 bytes\");\n+        \/\/ REP MOVS offer a faster copy path.\n+        __ jcc(Assembler::greaterEqual, L_repmovs);\n+      }\n+    }\n+\n+    if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n+      \/\/ Partial copy to make dst address 32 byte aligned.\n+      __ movq(temp2, to);\n+      __ andq(temp2, 31);\n+      __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+      __ negptr(temp2);\n+      __ addq(temp2, 32);\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ movq(temp3, temp2);\n+      copy32_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift);\n+      __ movq(temp4, temp2);\n+      __ movq(temp1, count);\n+      __ subq(temp1, temp2);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail);\n+\n+      __ BIND(L_main_pre_loop);\n+      __ subq(temp1, loop_size[shift]);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+      __ align32();\n+      __ BIND(L_main_loop);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 0);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 64);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 128);\n+         __ addptr(temp4, loop_size[shift]);\n+         __ subq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop);\n+\n+      __ addq(temp1, loop_size[shift]);\n+\n+      \/\/ Tail loop.\n+      __ jmp(L_tail);\n+\n+      __ BIND(L_repmovs);\n+        __ movq(temp2, temp1);\n+        \/\/ Swap to(RSI) and from(RDI) addresses to comply with REP MOVs semantics.\n+        __ movq(temp3, to);\n+        __ movq(to,  from);\n+        __ movq(from, temp3);\n+        \/\/ Save to\/from for restoration post rep_mov.\n+        __ movq(temp1, to);\n+        __ movq(temp3, from);\n+        if(shift < 3) {\n+          __ shrq(temp2, 3-shift);     \/\/ quad word count\n+        }\n+        __ movq(temp4 , temp2);        \/\/ move quad ward count into temp4(RCX).\n+        __ rep_mov();\n+        __ shlq(temp2, 3);             \/\/ convert quad words into byte count.\n+        if(shift) {\n+          __ shrq(temp2, shift);       \/\/ type specific count.\n+        }\n+        \/\/ Restore original addresses in to\/from.\n+        __ movq(to, temp3);\n+        __ movq(from, temp1);\n+        __ movq(temp4, temp2);\n+        __ movq(temp1, count);\n+        __ subq(temp1, temp2);         \/\/ tailing part (less than a quad ward size).\n+        __ jmp(L_tail);\n+    }\n+\n+    if (MaxVectorSize > 32) {\n+      __ BIND(L_pre_main_post_64);\n+      \/\/ Partial copy to make dst address 64 byte aligned.\n+      __ movq(temp2, to);\n+      __ andq(temp2, 63);\n+      __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+      __ negptr(temp2);\n+      __ addq(temp2, 64);\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ movq(temp3, temp2);\n+      copy64_masked_avx(to, from, xmm1, k2, temp3, temp4, temp1, shift, 0 , true);\n+      __ movq(temp4, temp2);\n+      __ movq(temp1, count);\n+      __ subq(temp1, temp2);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail64);\n+\n+      __ BIND(L_main_pre_loop_64bytes);\n+      __ subq(temp1, loop_size[shift]);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at\n+      \/\/ 64 byte copy granularity.\n+      __ align32();\n+      __ BIND(L_main_loop_64bytes);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 0 , true);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 64, true);\n+         copy64_avx(to, from, temp4, xmm1, false, shift, 128, true);\n+         __ addptr(temp4, loop_size[shift]);\n+         __ subq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+      __ addq(temp1, loop_size[shift]);\n+      \/\/ Zero length check.\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ BIND(L_tail64);\n+\n+      \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+      use64byteVector = true;\n+      arraycopy_avx3_special_cases(xmm1, k2, from, to, temp1, shift,\n+                                   temp4, temp3, use64byteVector, L_entry, L_exit);\n+    }\n+    __ BIND(L_exit);\n+  }\n+\n+  address ucme_exit_pc = __ pc();\n+  \/\/ When called from generic_arraycopy r11 contains specific values\n+  \/\/ used during arraycopy epilogue, re-initializing r11.\n+  if (is_oop) {\n+    __ movq(r11, shift == 3 ? count : to);\n+  }\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+  restore_argument_regs(type);\n+  INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/\n+address StubGenerator::generate_conjoint_copy_avx3_masked(address* entry, const char *name, int shift,\n+                                                          address nooverlap_target, bool aligned,\n+                                                          bool is_oop, bool dest_uninitialized) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  int avx3threshold = VM_Version::avx3_threshold();\n+  bool use64byteVector = (MaxVectorSize > 32) && (avx3threshold == 0);\n+\n+  Label L_main_pre_loop, L_main_pre_loop_64bytes, L_pre_main_post_64;\n+  Label L_main_loop, L_main_loop_64bytes, L_tail, L_tail64, L_exit, L_entry;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register temp1       = r8;\n+  const Register temp2       = rcx;\n+  const Register temp3       = r11;\n+  const Register temp4       = rax;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, (Address::ScaleFactor)(shift));\n+\n+  BasicType type_vec[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  BasicType type = is_oop ? T_OBJECT : type_vec[shift];\n+\n+  setup_argument_regs(type);\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+  {\n+    \/\/ Type(shift)       byte(0), short(1), int(2),   long(3)\n+    int loop_size[]   = { 192,     96,       48,      24};\n+    int threshold[]   = { 4096,    2048,     1024,    512};\n+\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+\n+    \/\/ temp1 holds remaining count.\n+    __ movq(temp1, count);\n+\n+    \/\/ Zero length check.\n+    __ BIND(L_tail);\n+    __ cmpq(temp1, 0);\n+    __ jcc(Assembler::lessEqual, L_exit);\n+\n+    __ mov64(temp2, 0);\n+    __ movq(temp3, temp1);\n+    \/\/ Special cases using 32 byte [masked] vector copy operations.\n+    arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                          temp4, use64byteVector, L_entry, L_exit);\n+\n+    \/\/ PRE-MAIN-POST loop for aligned copy.\n+    __ BIND(L_entry);\n+\n+    if ((MaxVectorSize > 32) && (avx3threshold != 0)) {\n+      __ cmpq(temp1, threshold[shift]);\n+      __ jcc(Assembler::greaterEqual, L_pre_main_post_64);\n+    }\n+\n+    if ((MaxVectorSize < 64)  || (avx3threshold != 0)) {\n+      \/\/ Partial copy to make dst address 32 byte aligned.\n+      __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+      __ andq(temp2, 31);\n+      __ jcc(Assembler::equal, L_main_pre_loop);\n+\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ subq(temp1, temp2);\n+      copy32_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail);\n+\n+      __ BIND(L_main_pre_loop);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at 32 byte granularity.\n+      __ align32();\n+      __ BIND(L_main_loop);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -64);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -128);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -192);\n+         __ subptr(temp1, loop_size[shift]);\n+         __ cmpq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop);\n+\n+      \/\/ Tail loop.\n+      __ jmp(L_tail);\n+    }\n+\n+    if (MaxVectorSize > 32) {\n+      __ BIND(L_pre_main_post_64);\n+      \/\/ Partial copy to make dst address 64 byte aligned.\n+      __ leaq(temp2, Address(to, temp1, (Address::ScaleFactor)(shift), 0));\n+      __ andq(temp2, 63);\n+      __ jcc(Assembler::equal, L_main_pre_loop_64bytes);\n+\n+      if (shift) {\n+        __ shrq(temp2, shift);\n+      }\n+      __ subq(temp1, temp2);\n+      copy64_masked_avx(to, from, xmm1, k2, temp2, temp1, temp3, shift, 0 , true);\n+\n+      __ cmpq(temp1, loop_size[shift]);\n+      __ jcc(Assembler::less, L_tail64);\n+\n+      __ BIND(L_main_pre_loop_64bytes);\n+\n+      \/\/ Main loop with aligned copy block size of 192 bytes at\n+      \/\/ 64 byte copy granularity.\n+      __ align32();\n+      __ BIND(L_main_loop_64bytes);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -64 , true);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -128, true);\n+         copy64_avx(to, from, temp1, xmm1, true, shift, -192, true);\n+         __ subq(temp1, loop_size[shift]);\n+         __ cmpq(temp1, loop_size[shift]);\n+         __ jcc(Assembler::greater, L_main_loop_64bytes);\n+\n+      \/\/ Zero length check.\n+      __ cmpq(temp1, 0);\n+      __ jcc(Assembler::lessEqual, L_exit);\n+\n+      __ BIND(L_tail64);\n+\n+      \/\/ Tail handling using 64 byte [masked] vector copy operations.\n+      use64byteVector = true;\n+      __ mov64(temp2, 0);\n+      __ movq(temp3, temp1);\n+      arraycopy_avx3_special_cases_conjoint(xmm1, k2, from, to, temp2, temp3, temp1, shift,\n+                                            temp4, use64byteVector, L_entry, L_exit);\n+    }\n+    __ BIND(L_exit);\n+  }\n+  address ucme_exit_pc = __ pc();\n+  \/\/ When called from generic_arraycopy r11 contains specific values\n+  \/\/ used during arraycopy epilogue, re-initializing r11.\n+  if(is_oop) {\n+    __ movq(r11, count);\n+  }\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, count);\n+  restore_argument_regs(type);\n+  INC_COUNTER_NP(get_profile_ctr(shift), rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+void StubGenerator::arraycopy_avx3_special_cases(XMMRegister xmm, KRegister mask, Register from,\n+                                                 Register to, Register count, int shift,\n+                                                 Register index, Register temp,\n+                                                 bool use64byteVector, Label& L_entry, Label& L_exit) {\n+  Label L_entry_64, L_entry_96, L_entry_128;\n+  Label L_entry_160, L_entry_192;\n+\n+  int size_mat[][6] = {\n+  \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n+  \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n+  \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n+  \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n+  };\n+\n+  \/\/ Case A) Special case for length less than equal to 32 bytes.\n+  __ cmpq(count, size_mat[shift][0]);\n+  __ jccb(Assembler::greater, L_entry_64);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case B) Special case for length less than equal to 64 bytes.\n+  __ BIND(L_entry_64);\n+  __ cmpq(count, size_mat[shift][1]);\n+  __ jccb(Assembler::greater, L_entry_96);\n+  copy64_masked_avx(to, from, xmm, mask, count, index, temp, shift, 0, use64byteVector);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case C) Special case for length less than equal to 96 bytes.\n+  __ BIND(L_entry_96);\n+  __ cmpq(count, size_mat[shift][2]);\n+  __ jccb(Assembler::greater, L_entry_128);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  __ subq(count, 64 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 64);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case D) Special case for length less than equal to 128 bytes.\n+  __ BIND(L_entry_128);\n+  __ cmpq(count, size_mat[shift][3]);\n+  __ jccb(Assembler::greater, L_entry_160);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy32_avx(to, from, index, xmm, shift, 64);\n+  __ subq(count, 96 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 96);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case E) Special case for length less than equal to 160 bytes.\n+  __ BIND(L_entry_160);\n+  __ cmpq(count, size_mat[shift][4]);\n+  __ jccb(Assembler::greater, L_entry_192);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n+  __ subq(count, 128 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 128);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case F) Special case for length less than equal to 192 bytes.\n+  __ BIND(L_entry_192);\n+  __ cmpq(count, size_mat[shift][5]);\n+  __ jcc(Assembler::greater, L_entry);\n+  copy64_avx(to, from, index, xmm, false, shift, 0, use64byteVector);\n+  copy64_avx(to, from, index, xmm, false, shift, 64, use64byteVector);\n+  copy32_avx(to, from, index, xmm, shift, 128);\n+  __ subq(count, 160 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, index, temp, shift, 160);\n+  __ jmp(L_exit);\n+}\n+\n+void StubGenerator::arraycopy_avx3_special_cases_conjoint(XMMRegister xmm, KRegister mask, Register from,\n+                                                           Register to, Register start_index, Register end_index,\n+                                                           Register count, int shift, Register temp,\n+                                                           bool use64byteVector, Label& L_entry, Label& L_exit) {\n+  Label L_entry_64, L_entry_96, L_entry_128;\n+  Label L_entry_160, L_entry_192;\n+  bool avx3 = (MaxVectorSize > 32) && (VM_Version::avx3_threshold() == 0);\n+\n+  int size_mat[][6] = {\n+  \/* T_BYTE *\/ {32 , 64,  96 , 128 , 160 , 192 },\n+  \/* T_SHORT*\/ {16 , 32,  48 , 64  , 80  , 96  },\n+  \/* T_INT  *\/ {8  , 16,  24 , 32  , 40  , 48  },\n+  \/* T_LONG *\/ {4  ,  8,  12 , 16  , 20  , 24  }\n+  };\n+\n+  \/\/ Case A) Special case for length less than equal to 32 bytes.\n+  __ cmpq(count, size_mat[shift][0]);\n+  __ jccb(Assembler::greater, L_entry_64);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case B) Special case for length less than equal to 64 bytes.\n+  __ BIND(L_entry_64);\n+  __ cmpq(count, size_mat[shift][1]);\n+  __ jccb(Assembler::greater, L_entry_96);\n+  if (avx3) {\n+     copy64_masked_avx(to, from, xmm, mask, count, start_index, temp, shift, 0, true);\n+  } else {\n+     copy32_avx(to, from, end_index, xmm, shift, -32);\n+     __ subq(count, 32 >> shift);\n+     copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  }\n+  __ jmp(L_exit);\n+\n+  \/\/ Case C) Special case for length less than equal to 96 bytes.\n+  __ BIND(L_entry_96);\n+  __ cmpq(count, size_mat[shift][2]);\n+  __ jccb(Assembler::greater, L_entry_128);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  __ subq(count, 64 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case D) Special case for length less than equal to 128 bytes.\n+  __ BIND(L_entry_128);\n+  __ cmpq(count, size_mat[shift][3]);\n+  __ jccb(Assembler::greater, L_entry_160);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy32_avx(to, from, end_index, xmm, shift, -96);\n+  __ subq(count, 96 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case E) Special case for length less than equal to 160 bytes.\n+  __ BIND(L_entry_160);\n+  __ cmpq(count, size_mat[shift][4]);\n+  __ jccb(Assembler::greater, L_entry_192);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n+  __ subq(count, 128 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+\n+  \/\/ Case F) Special case for length less than equal to 192 bytes.\n+  __ BIND(L_entry_192);\n+  __ cmpq(count, size_mat[shift][5]);\n+  __ jcc(Assembler::greater, L_entry);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -64, use64byteVector);\n+  copy64_avx(to, from, end_index, xmm, true, shift, -128, use64byteVector);\n+  copy32_avx(to, from, end_index, xmm, shift, -160);\n+  __ subq(count, 160 >> shift);\n+  copy32_masked_avx(to, from, xmm, mask, count, start_index, temp, shift);\n+  __ jmp(L_exit);\n+}\n+\n+void StubGenerator::copy64_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                                       KRegister mask, Register length, Register index,\n+                                       Register temp, int shift, int offset,\n+                                       bool use64byteVector) {\n+  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  if (!use64byteVector) {\n+    copy32_avx(dst, src, index, xmm, shift, offset);\n+    __ subptr(length, 32 >> shift);\n+    copy32_masked_avx(dst, src, xmm, mask, length, index, temp, shift, offset+32);\n+  } else {\n+    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+    assert(MaxVectorSize == 64, \"vector length != 64\");\n+    __ mov64(temp, -1L);\n+    __ bzhiq(temp, temp, length);\n+    __ kmovql(mask, temp);\n+    __ evmovdqu(type[shift], mask, xmm, Address(src, index, scale, offset), false, Assembler::AVX_512bit);\n+    __ evmovdqu(type[shift], mask, Address(dst, index, scale, offset), xmm, true, Assembler::AVX_512bit);\n+  }\n+}\n+\n+\n+void StubGenerator::copy32_masked_avx(Register dst, Register src, XMMRegister xmm,\n+                                       KRegister mask, Register length, Register index,\n+                                       Register temp, int shift, int offset) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  BasicType type[] = { T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n+  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+  __ mov64(temp, -1L);\n+  __ bzhiq(temp, temp, length);\n+  __ kmovql(mask, temp);\n+  __ evmovdqu(type[shift], mask, xmm, Address(src, index, scale, offset), false, Assembler::AVX_256bit);\n+  __ evmovdqu(type[shift], mask, Address(dst, index, scale, offset), xmm, true, Assembler::AVX_256bit);\n+}\n+\n+\n+void StubGenerator::copy32_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                                int shift, int offset) {\n+  assert(MaxVectorSize >= 32, \"vector length should be >= 32\");\n+  Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+  __ vmovdqu(xmm, Address(src, index, scale, offset));\n+  __ vmovdqu(Address(dst, index, scale, offset), xmm);\n+}\n+\n+\n+void StubGenerator::copy64_avx(Register dst, Register src, Register index, XMMRegister xmm,\n+                                bool conjoint, int shift, int offset, bool use64byteVector) {\n+  assert(MaxVectorSize == 64 || MaxVectorSize == 32, \"vector length mismatch\");\n+  if (!use64byteVector) {\n+    if (conjoint) {\n+      copy32_avx(dst, src, index, xmm, shift, offset+32);\n+      copy32_avx(dst, src, index, xmm, shift, offset);\n+    } else {\n+      copy32_avx(dst, src, index, xmm, shift, offset);\n+      copy32_avx(dst, src, index, xmm, shift, offset+32);\n+    }\n+  } else {\n+    Address::ScaleFactor scale = (Address::ScaleFactor)(shift);\n+    __ evmovdquq(xmm, Address(src, index, scale, offset), Assembler::AVX_512bit);\n+    __ evmovdquq(Address(dst, index, scale, offset), xmm, Assembler::AVX_512bit);\n+  }\n+}\n+\n+#endif \/\/ COMPILER2_OR_JVMCI\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+\/\/ we let the hardware handle it.  The one to eight bytes within words,\n+\/\/ dwords or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_byte_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_byte_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_byte_copy(bool aligned, address* entry, const char *name) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jbyte_disjoint_arraycopy_avx3\", 0,\n+                                               aligned, false, false);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n+  Label L_copy_byte, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register byte_count  = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(byte_count, count);\n+    __ shrptr(count, 3); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count); \/\/ make the count negative\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+    \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(byte_count, 4);\n+    __ jccb(Assembler::zero, L_copy_2_bytes);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n+\n+    __ addptr(end_from, 4);\n+    __ addptr(end_to, 4);\n+\n+    \/\/ Check for and copy trailing word\n+  __ BIND(L_copy_2_bytes);\n+    __ testl(byte_count, 2);\n+    __ jccb(Assembler::zero, L_copy_byte);\n+    __ movw(rax, Address(end_from, 8));\n+    __ movw(Address(end_to, 8), rax);\n+\n+    __ addptr(end_from, 2);\n+    __ addptr(end_to, 2);\n+\n+    \/\/ Check for and copy trailing byte\n+  __ BIND(L_copy_byte);\n+    __ testl(byte_count, 1);\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movb(rax, Address(end_from, 8));\n+    __ movb(Address(end_to, 8), rax);\n+  }\n+__ BIND(L_exit);\n+  address ucme_exit_pc = __ pc();\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n+  }\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-, 2-, or 1-byte boundaries,\n+\/\/ we let the hardware handle it.  The one to eight bytes within words,\n+\/\/ dwords or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_byte_copy(bool aligned, address nooverlap_target,\n+                                                   address* entry, const char *name) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jbyte_conjoint_arraycopy_avx3\", 0,\n+                                               nooverlap_target, aligned, false, false);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_copy_2_bytes;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register byte_count  = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, Address::times_1);\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(byte_count, count);\n+    __ shrptr(count, 3);   \/\/ count => qword_count\n+\n+    \/\/ Copy from high to low addresses.\n+\n+    \/\/ Check for and copy trailing byte\n+    __ testl(byte_count, 1);\n+    __ jcc(Assembler::zero, L_copy_2_bytes);\n+    __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n+    __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n+    __ decrement(byte_count); \/\/ Adjust for possible trailing word\n+\n+    \/\/ Check for and copy trailing word\n+  __ BIND(L_copy_2_bytes);\n+    __ testl(byte_count, 2);\n+    __ jcc(Assembler::zero, L_copy_4_bytes);\n+    __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n+    __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n+\n+    \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(byte_count, 4);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, qword_count, Address::times_8));\n+    __ movl(Address(to, qword_count, Address::times_8), rax);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jbyte_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n+\/\/ let the hardware handle it.  The two or four words within dwords\n+\/\/ or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_short_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_short_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_short_copy(bool aligned, address *entry, const char *name) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jshort_disjoint_arraycopy_avx3\", 1,\n+                                               aligned, false, false);\n+  }\n+#endif\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes,L_copy_2_bytes,L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register word_count  = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(word_count, count);\n+    __ shrptr(count, 2); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+    \/\/ Original 'dest' is trashed, so we can't use it as a\n+    \/\/ base register for a possible trailing word copy\n+\n+    \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(word_count, 2);\n+    __ jccb(Assembler::zero, L_copy_2_bytes);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n+\n+    __ addptr(end_from, 4);\n+    __ addptr(end_to, 4);\n+\n+    \/\/ Check for and copy trailing word\n+  __ BIND(L_copy_2_bytes);\n+    __ testl(word_count, 1);\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movw(rax, Address(end_from, 8));\n+    __ movw(Address(end_to, 8), rax);\n+  }\n+__ BIND(L_exit);\n+  address ucme_exit_pc = __ pc();\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n+  }\n+\n+  return start;\n+}\n+\n+\n+address StubGenerator::generate_fill(BasicType t, bool aligned, const char *name) {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+\n+  const Register to       = c_rarg0;  \/\/ destination array address\n+  const Register value    = c_rarg1;  \/\/ value\n+  const Register count    = c_rarg2;  \/\/ elements count\n+  __ mov(r11, count);\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  __ generate_fill(t, aligned, to, value, r11, rax, xmm0);\n+\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4- or 2-byte boundaries, we\n+\/\/ let the hardware handle it.  The two or four words within dwords\n+\/\/ or qwords that span cache line boundaries will still be loaded\n+\/\/ and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_short_copy(bool aligned, address nooverlap_target,\n+                                                    address *entry, const char *name) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jshort_conjoint_arraycopy_avx3\", 1,\n+                                               nooverlap_target, aligned, false, false);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register word_count  = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, Address::times_2);\n+  setup_arg_regs(); \/\/ from => rdi, to => rsi, count => rdx\n+                    \/\/ r9 and r10 may be used to save non-volatile registers\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(word_count, count);\n+    __ shrptr(count, 2); \/\/ count => qword_count\n+\n+    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+    \/\/ Check for and copy trailing word\n+    __ testl(word_count, 1);\n+    __ jccb(Assembler::zero, L_copy_4_bytes);\n+    __ movw(rax, Address(from, word_count, Address::times_2, -2));\n+    __ movw(Address(to, word_count, Address::times_2, -2), rax);\n+\n+   \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(word_count, 2);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, qword_count, Address::times_8));\n+    __ movl(Address(to, qword_count, Address::times_8), rax);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_jshort_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+\/\/ the hardware handle it.  The two dwords within qwords that span\n+\/\/ cache line boundaries will still be loaded and stored atomically.\n+\/\/\n+\/\/ Side Effects:\n+\/\/   disjoint_int_copy_entry is set to the no-overlap entry point\n+\/\/   used by generate_conjoint_int_oop_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_int_oop_copy(bool aligned, bool is_oop, address* entry,\n+                                                      const char *name, bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jint_disjoint_arraycopy_avx3\", 2,\n+                                               aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_copy_4_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register dword_count = rcx;\n+  const Register qword_count = count;\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = to;   \/\/ destination array end address\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_INT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(dword_count, count);\n+    __ shrptr(count, 1); \/\/ count => qword_count\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+    \/\/ Check for and copy trailing dword\n+  __ BIND(L_copy_4_bytes);\n+    __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n+    __ jccb(Assembler::zero, L_exit);\n+    __ movl(rax, Address(end_from, 8));\n+    __ movl(Address(end_to, 8), rax);\n+  }\n+__ BIND(L_exit);\n+  address ucme_exit_pc = __ pc();\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    __ jmp(L_copy_4_bytes);\n+  }\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord == 8-byte boundary\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ If 'from' and\/or 'to' are aligned on 4-byte boundaries, we let\n+\/\/ the hardware handle it.  The two dwords within qwords that span\n+\/\/ cache line boundaries will still be loaded and stored atomically.\n+\/\/\n+address StubGenerator::generate_conjoint_int_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                                      address *entry, const char *name,\n+                                                      bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jint_conjoint_arraycopy_avx3\", 2,\n+                                               nooverlap_target, aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register count       = rdx;  \/\/ elements count\n+  const Register dword_count = rcx;\n+  const Register qword_count = count;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+     \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, Address::times_4);\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_INT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  \/\/ no registers are destroyed by this call\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  assert_clean_int(count, rax); \/\/ Make sure 'count' is clean int.\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ 'from', 'to' and 'count' are now valid\n+    __ movptr(dword_count, count);\n+    __ shrptr(count, 1); \/\/ count => qword_count\n+\n+    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+    \/\/ Check for and copy trailing dword\n+    __ testl(dword_count, 1);\n+    __ jcc(Assembler::zero, L_copy_bytes);\n+    __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n+    __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  }\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+\n+__ BIND(L_exit);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, dword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(SharedRuntime::_jint_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ vzeroupper();\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+ \/\/ Side Effects:\n+\/\/   disjoint_oop_copy_entry or disjoint_long_copy_entry is set to the\n+\/\/   no-overlap entry point used by generate_conjoint_long_oop_copy().\n+\/\/\n+address StubGenerator::generate_disjoint_long_oop_copy(bool aligned, bool is_oop, address *entry,\n+                                                       const char *name, bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_disjoint_copy_avx3_masked(entry, \"jlong_disjoint_arraycopy_avx3\", 3,\n+                                               aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register qword_count = rdx;  \/\/ elements count\n+  const Register end_from    = from; \/\/ source array end address\n+  const Register end_to      = rcx;  \/\/ destination array end address\n+  const Register saved_count = r11;\n+  \/\/ End pointers are inclusive, and if count is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  \/\/ Save no-overlap entry point for generate_conjoint_long_oop_copy()\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                   \/\/ r9 is used to save r15_thread\n+  \/\/ 'from', 'to' and 'qword_count' are now valid\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_LONG;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+    __ negptr(qword_count);\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+    __ increment(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  } else {\n+    restore_arg_regs_using_thread();\n+    INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+    __ xorptr(rax, rax); \/\/ return 0\n+    __ vzeroupper();\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+  }\n+\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+\n+  __ BIND(L_exit);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n+                          SharedRuntime::_jlong_array_copy_ctr,\n+                 rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Arguments:\n+\/\/   aligned - true => Input and output aligned on a HeapWord boundary == 8 bytes\n+\/\/             ignored\n+\/\/   is_oop  - true => oop array, so generate store check code\n+\/\/   name    - stub name string\n+\/\/\n+\/\/ Inputs:\n+\/\/   c_rarg0   - source array address\n+\/\/   c_rarg1   - destination array address\n+\/\/   c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/\n+address StubGenerator::generate_conjoint_long_oop_copy(bool aligned, bool is_oop, address nooverlap_target,\n+                                                       address *entry, const char *name,\n+                                                       bool dest_uninitialized) {\n+#if COMPILER2_OR_JVMCI\n+  if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n+     return generate_conjoint_copy_avx3_masked(entry, \"jlong_conjoint_arraycopy_avx3\", 3,\n+                                               nooverlap_target, aligned, is_oop, dest_uninitialized);\n+  }\n+#endif\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  Label L_copy_bytes, L_copy_8_bytes, L_exit;\n+  const Register from        = rdi;  \/\/ source array address\n+  const Register to          = rsi;  \/\/ destination array address\n+  const Register qword_count = rdx;  \/\/ elements count\n+  const Register saved_count = rcx;\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  assert_clean_int(c_rarg2, rax);    \/\/ Make sure 'count' is clean int.\n+\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    \/\/ caller can pass a 64-bit byte count here (from Unsafe.copyMemory)\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  array_overlap_test(nooverlap_target, Address::times_8);\n+  setup_arg_regs_using_thread(); \/\/ from => rdi, to => rsi, count => rdx\n+                                 \/\/ r9 is used to save r15_thread\n+  \/\/ 'from', 'to' and 'qword_count' are now valid\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+  if (aligned) {\n+    decorators |= ARRAYCOPY_ALIGNED;\n+  }\n+\n+  BasicType type = is_oop ? T_OBJECT : T_LONG;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, qword_count);\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+    __ jmp(L_copy_bytes);\n+\n+    \/\/ Copy trailing qwords\n+  __ BIND(L_copy_8_bytes);\n+    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+    __ decrement(qword_count);\n+    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+  }\n+  if (is_oop) {\n+    __ jmp(L_exit);\n+  } else {\n+    restore_arg_regs_using_thread();\n+    INC_COUNTER_NP(SharedRuntime::_jlong_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+    __ xorptr(rax, rax); \/\/ return 0\n+    __ vzeroupper();\n+    __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+    __ ret(0);\n+  }\n+  {\n+    \/\/ UnsafeCopyMemory page error: continue after ucm\n+    UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+    \/\/ Copy in multi-bytes chunks\n+    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+  }\n+  __ BIND(L_exit);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, qword_count);\n+  restore_arg_regs_using_thread();\n+  INC_COUNTER_NP(is_oop ? SharedRuntime::_oop_array_copy_ctr :\n+                          SharedRuntime::_jlong_array_copy_ctr,\n+                 rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ vzeroupper();\n+  __ xorptr(rax, rax); \/\/ return 0\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/ Helper for generating a dynamic type check.\n+\/\/ Smashes no registers.\n+void StubGenerator::generate_type_check(Register sub_klass,\n+                                        Register super_check_offset,\n+                                        Register super_klass,\n+                                        Label& L_success) {\n+  assert_different_registers(sub_klass, super_check_offset, super_klass);\n+\n+  BLOCK_COMMENT(\"type_check:\");\n+\n+  Label L_miss;\n+\n+  __ check_klass_subtype_fast_path(sub_klass, super_klass, noreg,        &L_success, &L_miss, NULL,\n+                                   super_check_offset);\n+  __ check_klass_subtype_slow_path(sub_klass, super_klass, noreg, noreg, &L_success, NULL);\n+\n+  \/\/ Fall through on failure!\n+  __ BIND(L_miss);\n+}\n+\n+\/\/\n+\/\/  Generate checkcasting array copy stub\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0   - source array address\n+\/\/    c_rarg1   - destination array address\n+\/\/    c_rarg2   - element count, treated as ssize_t, can be zero\n+\/\/    c_rarg3   - size_t ckoff (super_check_offset)\n+\/\/ not Win64\n+\/\/    c_rarg4   - oop ckval (super_klass)\n+\/\/ Win64\n+\/\/    rsp+40    - oop ckval (super_klass)\n+\/\/\n+\/\/  Output:\n+\/\/    rax ==  0  -  success\n+\/\/    rax == -1^K - failure, where K is partial transfer count\n+\/\/\n+address StubGenerator::generate_checkcast_copy(const char *name, address *entry, bool dest_uninitialized) {\n+\n+  Label L_load_element, L_store_element, L_do_card_marks, L_done;\n+\n+  \/\/ Input registers (after setup_arg_regs)\n+  const Register from        = rdi;   \/\/ source array address\n+  const Register to          = rsi;   \/\/ destination array address\n+  const Register length      = rdx;   \/\/ elements count\n+  const Register ckoff       = rcx;   \/\/ super_check_offset\n+  const Register ckval       = r8;    \/\/ super_klass\n+\n+  \/\/ Registers used as temps (r13, r14 are save-on-entry)\n+  const Register end_from    = from;  \/\/ source array end address\n+  const Register end_to      = r13;   \/\/ destination array end address\n+  const Register count       = rdx;   \/\/ -(count_remaining)\n+  const Register r14_length  = r14;   \/\/ saved copy of length\n+  \/\/ End pointers are inclusive, and if length is not zero they point\n+  \/\/ to the last unit copied:  end_to[0] := end_from[0]\n+\n+  const Register rax_oop    = rax;    \/\/ actual oop copied\n+  const Register r11_klass  = r11;    \/\/ oop._klass\n+\n+  \/\/---------------------------------------------------------------\n+  \/\/ Assembler stub will be used for this call to arraycopy\n+  \/\/ if the two arrays are subtypes of Object[] but the\n+  \/\/ destination array type is not equal to or a supertype\n+  \/\/ of the source type.  Each element must be separately\n+  \/\/ checked.\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef ASSERT\n+  \/\/ caller guarantees that the arrays really are different\n+  \/\/ otherwise, we would have to make conjoint checks\n+  { Label L;\n+    array_overlap_test(L, TIMES_OOP);\n+    __ stop(\"checkcast_copy within a single array\");\n+    __ bind(L);\n+  }\n+#endif \/\/ASSERT\n+\n+  setup_arg_regs(4); \/\/ from => rdi, to => rsi, length => rdx\n+                     \/\/ ckoff => rcx, ckval => r8\n+                     \/\/ r9 and r10 may be used to save non-volatile registers\n+#ifdef _WIN64\n+  \/\/ last argument (#4) is on stack on Win64\n+  __ movptr(ckval, Address(rsp, 6 * wordSize));\n+#endif\n+\n+  \/\/ Caller of this entry point must set up the argument registers.\n+  if (entry != NULL) {\n+    *entry = __ pc();\n+    BLOCK_COMMENT(\"Entry:\");\n+  }\n+\n+  \/\/ allocate spill slots for r13, r14\n+  enum {\n+    saved_r13_offset,\n+    saved_r14_offset,\n+    saved_r10_offset,\n+    saved_rbp_offset\n+  };\n+  __ subptr(rsp, saved_rbp_offset * wordSize);\n+  __ movptr(Address(rsp, saved_r13_offset * wordSize), r13);\n+  __ movptr(Address(rsp, saved_r14_offset * wordSize), r14);\n+  __ movptr(Address(rsp, saved_r10_offset * wordSize), r10);\n+\n+#ifdef ASSERT\n+    Label L2;\n+    __ get_thread(r14);\n+    __ cmpptr(r15_thread, r14);\n+    __ jcc(Assembler::equal, L2);\n+    __ stop(\"StubRoutines::call_stub: r15_thread is modified by call\");\n+    __ bind(L2);\n+#endif \/\/ ASSERT\n+\n+  \/\/ check that int operands are properly extended to size_t\n+  assert_clean_int(length, rax);\n+  assert_clean_int(ckoff, rax);\n+\n+#ifdef ASSERT\n+  BLOCK_COMMENT(\"assert consistent ckoff\/ckval\");\n+  \/\/ The ckoff and ckval must be mutually consistent,\n+  \/\/ even though caller generates both.\n+  { Label L;\n+    int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+    __ cmpl(ckoff, Address(ckval, sco_offset));\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"super_check_offset inconsistent\");\n+    __ bind(L);\n+  }\n+#endif \/\/ASSERT\n+\n+  \/\/ Loop-invariant addresses.  They are exclusive end pointers.\n+  Address end_from_addr(from, length, TIMES_OOP, 0);\n+  Address   end_to_addr(to,   length, TIMES_OOP, 0);\n+  \/\/ Loop-variant addresses.  They assume post-incremented count < 0.\n+  Address from_element_addr(end_from, count, TIMES_OOP, 0);\n+  Address   to_element_addr(end_to,   count, TIMES_OOP, 0);\n+\n+  DecoratorSet decorators = IN_HEAP | IS_ARRAY | ARRAYCOPY_CHECKCAST | ARRAYCOPY_DISJOINT;\n+  if (dest_uninitialized) {\n+    decorators |= IS_DEST_UNINITIALIZED;\n+  }\n+\n+  BasicType type = T_OBJECT;\n+  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->arraycopy_prologue(_masm, decorators, type, from, to, count);\n+\n+  \/\/ Copy from low to high addresses, indexed from the end of each array.\n+  __ lea(end_from, end_from_addr);\n+  __ lea(end_to,   end_to_addr);\n+  __ movptr(r14_length, length);        \/\/ save a copy of the length\n+  assert(length == count, \"\");          \/\/ else fix next line:\n+  __ negptr(count);                     \/\/ negate and test the length\n+  __ jcc(Assembler::notZero, L_load_element);\n+\n+  \/\/ Empty array:  Nothing to do.\n+  __ xorptr(rax, rax);                  \/\/ return 0 on (trivial) success\n+  __ jmp(L_done);\n+\n+  \/\/ ======== begin loop ========\n+  \/\/ (Loop is rotated; its entry is L_load_element.)\n+  \/\/ Loop control:\n+  \/\/   for (count = -count; count != 0; count++)\n+  \/\/ Base pointers src, dst are biased by 8*(count-1),to last element.\n+  __ align(OptoLoopAlignment);\n+\n+  __ BIND(L_store_element);\n+  __ store_heap_oop(to_element_addr, rax_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n+  __ increment(count);               \/\/ increment the count toward zero\n+  __ jcc(Assembler::zero, L_do_card_marks);\n+\n+  \/\/ ======== loop entry is here ========\n+  __ BIND(L_load_element);\n+  __ load_heap_oop(rax_oop, from_element_addr, noreg, noreg, AS_RAW); \/\/ load the oop\n+  __ testptr(rax_oop, rax_oop);\n+  __ jcc(Assembler::zero, L_store_element);\n+\n+  __ load_klass(r11_klass, rax_oop, rscratch1);\/\/ query the object klass\n+  generate_type_check(r11_klass, ckoff, ckval, L_store_element);\n+  \/\/ ======== end loop ========\n+\n+  \/\/ It was a real error; we must depend on the caller to finish the job.\n+  \/\/ Register rdx = -1 * number of *remaining* oops, r14 = *total* oops.\n+  \/\/ Emit GC store barriers for the oops we have copied (r14 + rdx),\n+  \/\/ and report their number to the caller.\n+  assert_different_registers(rax, r14_length, count, to, end_to, rcx, rscratch1);\n+  Label L_post_barrier;\n+  __ addptr(r14_length, count);     \/\/ K = (original - remaining) oops\n+  __ movptr(rax, r14_length);       \/\/ save the value\n+  __ notptr(rax);                   \/\/ report (-1^K) to caller (does not affect flags)\n+  __ jccb(Assembler::notZero, L_post_barrier);\n+  __ jmp(L_done); \/\/ K == 0, nothing was copied, skip post barrier\n+\n+  \/\/ Come here on success only.\n+  __ BIND(L_do_card_marks);\n+  __ xorptr(rax, rax);              \/\/ return 0 on success\n+\n+  __ BIND(L_post_barrier);\n+  bs->arraycopy_epilogue(_masm, decorators, type, from, to, r14_length);\n+\n+  \/\/ Common exit point (success or failure).\n+  __ BIND(L_done);\n+  __ movptr(r13, Address(rsp, saved_r13_offset * wordSize));\n+  __ movptr(r14, Address(rsp, saved_r14_offset * wordSize));\n+  __ movptr(r10, Address(rsp, saved_r10_offset * wordSize));\n+  restore_arg_regs();\n+  INC_COUNTER_NP(SharedRuntime::_checkcast_array_copy_ctr, rscratch1); \/\/ Update counter after rscratch1 is free\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\n+\/\/  Generate 'unsafe' array copy stub\n+\/\/  Though just as safe as the other stubs, it takes an unscaled\n+\/\/  size_t argument instead of an element count.\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0   - source array address\n+\/\/    c_rarg1   - destination array address\n+\/\/    c_rarg2   - byte count, treated as ssize_t, can be zero\n+\/\/\n+\/\/ Examines the alignment of the operands and dispatches\n+\/\/ to a long, int, short, or byte copy loop.\n+\/\/\n+address StubGenerator::generate_unsafe_copy(const char *name,\n+                                            address byte_copy_entry, address short_copy_entry,\n+                                            address int_copy_entry, address long_copy_entry) {\n+\n+  Label L_long_aligned, L_int_aligned, L_short_aligned;\n+\n+  \/\/ Input registers (before setup_arg_regs)\n+  const Register from        = c_rarg0;  \/\/ source array address\n+  const Register to          = c_rarg1;  \/\/ destination array address\n+  const Register size        = c_rarg2;  \/\/ byte count (size_t)\n+\n+  \/\/ Register used as a temp\n+  const Register bits        = rax;      \/\/ test copy of low bits\n+\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+  address start = __ pc();\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+  \/\/ bump this on entry, not on exit:\n+  INC_COUNTER_NP(SharedRuntime::_unsafe_array_copy_ctr, rscratch1);\n+\n+  __ mov(bits, from);\n+  __ orptr(bits, to);\n+  __ orptr(bits, size);\n+\n+  __ testb(bits, BytesPerLong-1);\n+  __ jccb(Assembler::zero, L_long_aligned);\n+\n+  __ testb(bits, BytesPerInt-1);\n+  __ jccb(Assembler::zero, L_int_aligned);\n+\n+  __ testb(bits, BytesPerShort-1);\n+  __ jump_cc(Assembler::notZero, RuntimeAddress(byte_copy_entry));\n+\n+  __ BIND(L_short_aligned);\n+  __ shrptr(size, LogBytesPerShort); \/\/ size => short_count\n+  __ jump(RuntimeAddress(short_copy_entry));\n+\n+  __ BIND(L_int_aligned);\n+  __ shrptr(size, LogBytesPerInt); \/\/ size => int_count\n+  __ jump(RuntimeAddress(int_copy_entry));\n+\n+  __ BIND(L_long_aligned);\n+  __ shrptr(size, LogBytesPerLong); \/\/ size => qword_count\n+  __ jump(RuntimeAddress(long_copy_entry));\n+\n+  return start;\n+}\n+\n+\n+\/\/ Perform range checks on the proposed arraycopy.\n+\/\/ Kills temp, but nothing else.\n+\/\/ Also, clean the sign bits of src_pos and dst_pos.\n+void StubGenerator::arraycopy_range_checks(Register src,     \/\/ source array oop (c_rarg0)\n+                                           Register src_pos, \/\/ source position (c_rarg1)\n+                                           Register dst,     \/\/ destination array oo (c_rarg2)\n+                                           Register dst_pos, \/\/ destination position (c_rarg3)\n+                                           Register length,\n+                                           Register temp,\n+                                           Label& L_failed) {\n+  BLOCK_COMMENT(\"arraycopy_range_checks:\");\n+\n+  \/\/  if (src_pos + length > arrayOop(src)->length())  FAIL;\n+  __ movl(temp, length);\n+  __ addl(temp, src_pos);             \/\/ src_pos + length\n+  __ cmpl(temp, Address(src, arrayOopDesc::length_offset_in_bytes()));\n+  __ jcc(Assembler::above, L_failed);\n+\n+  \/\/  if (dst_pos + length > arrayOop(dst)->length())  FAIL;\n+  __ movl(temp, length);\n+  __ addl(temp, dst_pos);             \/\/ dst_pos + length\n+  __ cmpl(temp, Address(dst, arrayOopDesc::length_offset_in_bytes()));\n+  __ jcc(Assembler::above, L_failed);\n+\n+  \/\/ Have to clean up high 32-bits of 'src_pos' and 'dst_pos'.\n+  \/\/ Move with sign extension can be used since they are positive.\n+  __ movslq(src_pos, src_pos);\n+  __ movslq(dst_pos, dst_pos);\n+\n+  BLOCK_COMMENT(\"arraycopy_range_checks done\");\n+}\n+\n+\n+\/\/  Generate generic array copy stubs\n+\/\/\n+\/\/  Input:\n+\/\/    c_rarg0    -  src oop\n+\/\/    c_rarg1    -  src_pos (32-bits)\n+\/\/    c_rarg2    -  dst oop\n+\/\/    c_rarg3    -  dst_pos (32-bits)\n+\/\/ not Win64\n+\/\/    c_rarg4    -  element count (32-bits)\n+\/\/ Win64\n+\/\/    rsp+40     -  element count (32-bits)\n+\/\/\n+\/\/  Output:\n+\/\/    rax ==  0  -  success\n+\/\/    rax == -1^K - failure, where K is partial transfer count\n+\/\/\n+address StubGenerator::generate_generic_copy(const char *name,\n+                                             address byte_copy_entry, address short_copy_entry,\n+                                             address int_copy_entry, address oop_copy_entry,\n+                                             address long_copy_entry, address checkcast_copy_entry) {\n+\n+  Label L_failed, L_failed_0, L_objArray;\n+  Label L_copy_shorts, L_copy_ints, L_copy_longs;\n+\n+  \/\/ Input registers\n+  const Register src        = c_rarg0;  \/\/ source array oop\n+  const Register src_pos    = c_rarg1;  \/\/ source position\n+  const Register dst        = c_rarg2;  \/\/ destination array oop\n+  const Register dst_pos    = c_rarg3;  \/\/ destination position\n+#ifndef _WIN64\n+  const Register length     = c_rarg4;\n+  const Register rklass_tmp = r9;  \/\/ load_klass\n+#else\n+  const Address  length(rsp, 7 * wordSize);  \/\/ elements count is on stack on Win64\n+  const Register rklass_tmp = rdi;  \/\/ load_klass\n+#endif\n+\n+  { int modulus = CodeEntryAlignment;\n+    int target  = modulus - 5; \/\/ 5 = sizeof jmp(L_failed)\n+    int advance = target - (__ offset() % modulus);\n+    if (advance < 0)  advance += modulus;\n+    if (advance > 0)  __ nop(advance);\n+  }\n+  StubCodeMark mark(this, \"StubRoutines\", name);\n+\n+  \/\/ Short-hop target to L_failed.  Makes for denser prologue code.\n+  __ BIND(L_failed_0);\n+  __ jmp(L_failed);\n+  assert(__ offset() % CodeEntryAlignment == 0, \"no further alignment needed\");\n+\n+  __ align(CodeEntryAlignment);\n+  address start = __ pc();\n+\n+  __ enter(); \/\/ required for proper stackwalking of RuntimeStub frame\n+\n+#ifdef _WIN64\n+  __ push(rklass_tmp); \/\/ rdi is callee-save on Windows\n+#endif\n+\n+  \/\/ bump this on entry, not on exit:\n+  INC_COUNTER_NP(SharedRuntime::_generic_array_copy_ctr, rscratch1);\n+\n+  \/\/-----------------------------------------------------------------------\n+  \/\/ Assembler stub will be used for this call to arraycopy\n+  \/\/ if the following conditions are met:\n+  \/\/\n+  \/\/ (1) src and dst must not be null.\n+  \/\/ (2) src_pos must not be negative.\n+  \/\/ (3) dst_pos must not be negative.\n+  \/\/ (4) length  must not be negative.\n+  \/\/ (5) src klass and dst klass should be the same and not NULL.\n+  \/\/ (6) src and dst should be arrays.\n+  \/\/ (7) src_pos + length must not exceed length of src.\n+  \/\/ (8) dst_pos + length must not exceed length of dst.\n+  \/\/\n+\n+  \/\/  if (src == NULL) return -1;\n+  __ testptr(src, src);         \/\/ src oop\n+  size_t j1off = __ offset();\n+  __ jccb(Assembler::zero, L_failed_0);\n+\n+  \/\/  if (src_pos < 0) return -1;\n+  __ testl(src_pos, src_pos); \/\/ src_pos (32-bits)\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  \/\/  if (dst == NULL) return -1;\n+  __ testptr(dst, dst);         \/\/ dst oop\n+  __ jccb(Assembler::zero, L_failed_0);\n+\n+  \/\/  if (dst_pos < 0) return -1;\n+  __ testl(dst_pos, dst_pos); \/\/ dst_pos (32-bits)\n+  size_t j4off = __ offset();\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  \/\/ The first four tests are very dense code,\n+  \/\/ but not quite dense enough to put four\n+  \/\/ jumps in a 16-byte instruction fetch buffer.\n+  \/\/ That's good, because some branch predicters\n+  \/\/ do not like jumps so close together.\n+  \/\/ Make sure of this.\n+  guarantee(((j1off ^ j4off) & ~15) != 0, \"I$ line of 1st & 4th jumps\");\n+\n+  \/\/ registers used as temp\n+  const Register r11_length    = r11; \/\/ elements count to copy\n+  const Register r10_src_klass = r10; \/\/ array klass\n+\n+  \/\/  if (length < 0) return -1;\n+  __ movl(r11_length, length);        \/\/ length (elements count, 32-bits value)\n+  __ testl(r11_length, r11_length);\n+  __ jccb(Assembler::negative, L_failed_0);\n+\n+  __ load_klass(r10_src_klass, src, rklass_tmp);\n+#ifdef ASSERT\n+  \/\/  assert(src->klass() != NULL);\n+  {\n+    BLOCK_COMMENT(\"assert klasses not null {\");\n+    Label L1, L2;\n+    __ testptr(r10_src_klass, r10_src_klass);\n+    __ jcc(Assembler::notZero, L2);   \/\/ it is broken if klass is NULL\n+    __ bind(L1);\n+    __ stop(\"broken null klass\");\n+    __ bind(L2);\n+    __ load_klass(rax, dst, rklass_tmp);\n+    __ cmpq(rax, 0);\n+    __ jcc(Assembler::equal, L1);     \/\/ this would be broken also\n+    BLOCK_COMMENT(\"} assert klasses not null done\");\n+  }\n+#endif\n+\n+  \/\/ Load layout helper (32-bits)\n+  \/\/\n+  \/\/  |array_tag|     | header_size | element_type |     |log2_element_size|\n+  \/\/ 32        30    24            16              8     2                 0\n+  \/\/\n+  \/\/   array_tag: typeArray = 0x3, objArray = 0x2, non-array = 0x0\n+  \/\/\n+\n+  const int lh_offset = in_bytes(Klass::layout_helper_offset());\n+\n+  \/\/ Handle objArrays completely differently...\n+  const jint objArray_lh = Klass::array_layout_helper(T_OBJECT);\n+  __ cmpl(Address(r10_src_klass, lh_offset), objArray_lh);\n+  __ jcc(Assembler::equal, L_objArray);\n+\n+  \/\/  if (src->klass() != dst->klass()) return -1;\n+  __ load_klass(rax, dst, rklass_tmp);\n+  __ cmpq(r10_src_klass, rax);\n+  __ jcc(Assembler::notEqual, L_failed);\n+\n+  const Register rax_lh = rax;  \/\/ layout helper\n+  __ movl(rax_lh, Address(r10_src_klass, lh_offset));\n+\n+  \/\/ Check for flat inline type array -> return -1\n+  __ testl(rax_lh, Klass::_lh_array_tag_flat_value_bit_inplace);\n+  __ jcc(Assembler::notZero, L_failed);\n+\n+  \/\/ Check for null-free (non-flat) inline type array -> handle as object array\n+  __ testl(rax_lh, Klass::_lh_null_free_array_bit_inplace);\n+  __ jcc(Assembler::notZero, L_objArray);\n+\n+  \/\/  if (!src->is_Array()) return -1;\n+  __ cmpl(rax_lh, Klass::_lh_neutral_value);\n+  __ jcc(Assembler::greaterEqual, L_failed);\n+\n+  \/\/ At this point, it is known to be a typeArray (array_tag 0x3).\n+#ifdef ASSERT\n+  {\n+    BLOCK_COMMENT(\"assert primitive array {\");\n+    Label L;\n+    __ movl(rklass_tmp, rax_lh);\n+    __ sarl(rklass_tmp, Klass::_lh_array_tag_shift);\n+    __ cmpl(rklass_tmp, Klass::_lh_array_tag_type_value);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"must be a primitive array\");\n+    __ bind(L);\n+    BLOCK_COMMENT(\"} assert primitive array done\");\n+  }\n+#endif\n+\n+  arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n+                         r10, L_failed);\n+\n+  \/\/ TypeArrayKlass\n+  \/\/\n+  \/\/ src_addr = (src + array_header_in_bytes()) + (src_pos << log2elemsize);\n+  \/\/ dst_addr = (dst + array_header_in_bytes()) + (dst_pos << log2elemsize);\n+  \/\/\n+\n+  const Register r10_offset = r10;    \/\/ array offset\n+  const Register rax_elsize = rax_lh; \/\/ element size\n+\n+  __ movl(r10_offset, rax_lh);\n+  __ shrl(r10_offset, Klass::_lh_header_size_shift);\n+  __ andptr(r10_offset, Klass::_lh_header_size_mask);   \/\/ array_offset\n+  __ addptr(src, r10_offset);           \/\/ src array offset\n+  __ addptr(dst, r10_offset);           \/\/ dst array offset\n+  BLOCK_COMMENT(\"choose copy loop based on element size\");\n+  __ andl(rax_lh, Klass::_lh_log2_element_size_mask); \/\/ rax_lh -> rax_elsize\n+\n+#ifdef _WIN64\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+\n+  \/\/ next registers should be set before the jump to corresponding stub\n+  const Register from     = c_rarg0;  \/\/ source array address\n+  const Register to       = c_rarg1;  \/\/ destination array address\n+  const Register count    = c_rarg2;  \/\/ elements count\n+\n+  \/\/ 'from', 'to', 'count' registers should be set in such order\n+  \/\/ since they are the same as 'src', 'src_pos', 'dst'.\n+\n+  __ cmpl(rax_elsize, 0);\n+  __ jccb(Assembler::notEqual, L_copy_shorts);\n+  __ lea(from, Address(src, src_pos, Address::times_1, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_1, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(byte_copy_entry));\n+\n+__ BIND(L_copy_shorts);\n+  __ cmpl(rax_elsize, LogBytesPerShort);\n+  __ jccb(Assembler::notEqual, L_copy_ints);\n+  __ lea(from, Address(src, src_pos, Address::times_2, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_2, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(short_copy_entry));\n+\n+__ BIND(L_copy_ints);\n+  __ cmpl(rax_elsize, LogBytesPerInt);\n+  __ jccb(Assembler::notEqual, L_copy_longs);\n+  __ lea(from, Address(src, src_pos, Address::times_4, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_4, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(int_copy_entry));\n+\n+__ BIND(L_copy_longs);\n+#ifdef ASSERT\n+  {\n+    BLOCK_COMMENT(\"assert long copy {\");\n+    Label L;\n+    __ cmpl(rax_elsize, LogBytesPerLong);\n+    __ jcc(Assembler::equal, L);\n+    __ stop(\"must be long copy, but elsize is wrong\");\n+    __ bind(L);\n+    BLOCK_COMMENT(\"} assert long copy done\");\n+  }\n+#endif\n+  __ lea(from, Address(src, src_pos, Address::times_8, 0));\/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, Address::times_8, 0));\/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+  __ jump(RuntimeAddress(long_copy_entry));\n+\n+  \/\/ ObjArrayKlass\n+__ BIND(L_objArray);\n+  \/\/ live at this point:  r10_src_klass, r11_length, src[_pos], dst[_pos]\n+\n+  Label L_plain_copy, L_checkcast_copy;\n+  \/\/  test array classes for subtyping\n+  __ load_klass(rax, dst, rklass_tmp);\n+  __ cmpq(r10_src_klass, rax); \/\/ usual case is exact equality\n+  __ jcc(Assembler::notEqual, L_checkcast_copy);\n+\n+  \/\/ Identically typed arrays can be copied without element-wise checks.\n+  arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n+                         r10, L_failed);\n+\n+  __ lea(from, Address(src, src_pos, TIMES_OOP,\n+               arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ src_addr\n+  __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n+               arrayOopDesc::base_offset_in_bytes(T_OBJECT))); \/\/ dst_addr\n+  __ movl2ptr(count, r11_length); \/\/ length\n+__ BIND(L_plain_copy);\n+#ifdef _WIN64\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+  __ jump(RuntimeAddress(oop_copy_entry));\n+\n+__ BIND(L_checkcast_copy);\n+  \/\/ live at this point:  r10_src_klass, r11_length, rax (dst_klass)\n+  {\n+    \/\/ Before looking at dst.length, make sure dst is also an objArray.\n+    \/\/ This check also fails for flat\/null-free arrays which are not supported.\n+    __ cmpl(Address(rax, lh_offset), objArray_lh);\n+    __ jcc(Assembler::notEqual, L_failed);\n+\n+#ifdef ASSERT\n+    {\n+      BLOCK_COMMENT(\"assert not null-free array {\");\n+      Label L;\n+      __ movl(rklass_tmp, Address(rax, lh_offset));\n+      __ testl(rklass_tmp, Klass::_lh_null_free_array_bit_inplace);\n+      __ jcc(Assembler::zero, L);\n+      __ stop(\"unexpected null-free array\");\n+      __ bind(L);\n+      BLOCK_COMMENT(\"} assert not null-free array\");\n+    }\n+#endif\n+\n+    \/\/ It is safe to examine both src.length and dst.length.\n+    arraycopy_range_checks(src, src_pos, dst, dst_pos, r11_length,\n+                           rax, L_failed);\n+\n+    const Register r11_dst_klass = r11;\n+    __ load_klass(r11_dst_klass, dst, rklass_tmp); \/\/ reload\n+\n+    \/\/ Marshal the base address arguments now, freeing registers.\n+    __ lea(from, Address(src, src_pos, TIMES_OOP,\n+                 arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n+    __ lea(to,   Address(dst, dst_pos, TIMES_OOP,\n+                 arrayOopDesc::base_offset_in_bytes(T_OBJECT)));\n+    __ movl(count, length);           \/\/ length (reloaded)\n+    Register sco_temp = c_rarg3;      \/\/ this register is free now\n+    assert_different_registers(from, to, count, sco_temp,\n+                               r11_dst_klass, r10_src_klass);\n+    assert_clean_int(count, sco_temp);\n+\n+    \/\/ Generate the type check.\n+    const int sco_offset = in_bytes(Klass::super_check_offset_offset());\n+    __ movl(sco_temp, Address(r11_dst_klass, sco_offset));\n+    assert_clean_int(sco_temp, rax);\n+    generate_type_check(r10_src_klass, sco_temp, r11_dst_klass, L_plain_copy);\n+\n+    \/\/ Fetch destination element klass from the ObjArrayKlass header.\n+    int ek_offset = in_bytes(ObjArrayKlass::element_klass_offset());\n+    __ movptr(r11_dst_klass, Address(r11_dst_klass, ek_offset));\n+    __ movl(  sco_temp,      Address(r11_dst_klass, sco_offset));\n+    assert_clean_int(sco_temp, rax);\n+\n+#ifdef _WIN64\n+    __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+\n+    \/\/ the checkcast_copy loop needs two extra arguments:\n+    assert(c_rarg3 == sco_temp, \"#3 already in place\");\n+    \/\/ Set up arguments for checkcast_copy_entry.\n+    setup_arg_regs(4);\n+    __ movptr(r8, r11_dst_klass);  \/\/ dst.klass.element_klass, r8 is c_rarg4 on Linux\/Solaris\n+    __ jump(RuntimeAddress(checkcast_copy_entry));\n+  }\n+\n+__ BIND(L_failed);\n+#ifdef _WIN64\n+  __ pop(rklass_tmp); \/\/ Restore callee-save rdi\n+#endif\n+  __ xorptr(rax, rax);\n+  __ notptr(rax); \/\/ return -1\n+  __ leave();   \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64_arraycopy.cpp","additions":2570,"deletions":0,"binary":false,"changes":2570,"status":"added"},{"patch":"@@ -664,14 +664,0 @@\n-address TemplateInterpreterGenerator::generate_Continuation_doYield_entry(void) {\n-  if (!Continuations::enabled()) return nullptr;\n-\n-  address entry = __ pc();\n-  assert(StubRoutines::cont_doYield() != NULL, \"stub not yet generated\");\n-\n-  __ push_cont_fastpath();\n-\n-  __ jump(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::cont_doYield())));\n-  \/\/ return value is in rax\n-\n-  return entry;\n-}\n-\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1756,1 +1756,1 @@\n-    default:       out->print(\"%3d:0x\" UINT64_FORMAT_X, type(), (uint64_t)as_jlong()); break;\n+    default:       out->print(\"%3d:\" UINT64_FORMAT_X, type(), (uint64_t)as_jlong()); break;\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3531,4 +3531,0 @@\n-  case vmIntrinsics::_Continuation_doYield:\n-    do_continuation_doYield(x);\n-    break;\n-\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -278,1 +278,0 @@\n-  void do_continuation_doYield(Intrinsic* x);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -353,1 +353,0 @@\n-  FUNCTION_CASE(entry, StubRoutines::cont_doYield());\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/archiveHeapLoader.inline.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"cds\/cds_globals.hpp\"\n@@ -31,1 +33,1 @@\n-#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/heapShared.hpp\"\n@@ -398,2 +400,2 @@\n-  st->print_cr(\"- cloned_vtables_offset:          \" SIZE_FORMAT_HEX, _cloned_vtables_offset);\n-  st->print_cr(\"- serialized_data_offset:         \" SIZE_FORMAT_HEX, _serialized_data_offset);\n+  st->print_cr(\"- cloned_vtables_offset:          \" SIZE_FORMAT_X, _cloned_vtables_offset);\n+  st->print_cr(\"- serialized_data_offset:         \" SIZE_FORMAT_X, _serialized_data_offset);\n@@ -403,1 +405,1 @@\n-  st->print_cr(\"- shared_path_table_offset:       \" SIZE_FORMAT_HEX, _shared_path_table_offset);\n+  st->print_cr(\"- shared_path_table_offset:       \" SIZE_FORMAT_X, _shared_path_table_offset);\n@@ -1583,2 +1585,2 @@\n-  st->print_cr(\"- file_offset:                    \" SIZE_FORMAT_HEX, _file_offset);\n-  st->print_cr(\"- mapping_offset:                 \" SIZE_FORMAT_HEX, _mapping_offset);\n+  st->print_cr(\"- file_offset:                    \" SIZE_FORMAT_X, _file_offset);\n+  st->print_cr(\"- mapping_offset:                 \" SIZE_FORMAT_X, _mapping_offset);\n@@ -1586,1 +1588,1 @@\n-  st->print_cr(\"- oopmap_offset:                  \" SIZE_FORMAT_HEX, _oopmap_offset);\n+  st->print_cr(\"- oopmap_offset:                  \" SIZE_FORMAT_X, _oopmap_offset);\n@@ -1626,1 +1628,1 @@\n-                   \" bytes, addr \" INTPTR_FORMAT \" file offset \" SIZE_FORMAT_HEX_W(08)\n+                   \" bytes, addr \" INTPTR_FORMAT \" file offset 0x%08\" PRIxPTR\n@@ -1630,0 +1632,1 @@\n+\n@@ -2042,1 +2045,1 @@\n-    return cast_from_oop<address>(HeapShared::decode_from_archive(n));\n+    return cast_from_oop<address>(ArchiveHeapLoader::decode_from_archive(n));\n@@ -2088,1 +2091,1 @@\n-    if (HeapShared::can_map()) {\n+    if (ArchiveHeapLoader::can_map()) {\n@@ -2090,2 +2093,2 @@\n-    } else if (HeapShared::can_load()) {\n-      success = HeapShared::load_heap_regions(this);\n+    } else if (ArchiveHeapLoader::can_load()) {\n+      success = ArchiveHeapLoader::load_heap_regions(this);\n@@ -2157,1 +2160,1 @@\n-    return header()->heap_begin() + spc->mapping_offset() + HeapShared::runtime_delta();\n+    return header()->heap_begin() + spc->mapping_offset() + ArchiveHeapLoader::runtime_delta();\n@@ -2163,1 +2166,1 @@\n-    HeapShared::init_narrow_oop_decoding(narrow_oop_base() + delta, narrow_oop_shift());\n+    ArchiveHeapLoader::init_narrow_oop_decoding(narrow_oop_base() + delta, narrow_oop_shift());\n@@ -2165,1 +2168,1 @@\n-    HeapShared::set_runtime_delta(delta);\n+    ArchiveHeapLoader::set_runtime_delta(delta);\n@@ -2281,1 +2284,1 @@\n-    HeapShared::set_closed_regions_mapped();\n+    ArchiveHeapLoader::set_closed_regions_mapped();\n@@ -2288,1 +2291,1 @@\n-      HeapShared::set_open_regions_mapped();\n+      ArchiveHeapLoader::set_open_regions_mapped();\n@@ -2296,1 +2299,1 @@\n-  if (!HeapShared::closed_regions_mapped()) {\n+  if (!ArchiveHeapLoader::closed_regions_mapped()) {\n@@ -2301,1 +2304,1 @@\n-  if (!HeapShared::open_regions_mapped()) {\n+  if (!ArchiveHeapLoader::open_regions_mapped()) {\n@@ -2408,1 +2411,1 @@\n-    HeapShared::patch_embedded_pointers(\n+    ArchiveHeapLoader::patch_embedded_pointers(\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":23,"deletions":20,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n@@ -29,2 +30,1 @@\n-#include \"cds\/filemap.hpp\"\n-#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/heapShared.hpp\"\n@@ -48,2 +48,0 @@\n-#include \"memory\/metadataFactory.hpp\"\n-#include \"memory\/metaspaceClosure.hpp\"\n@@ -59,2 +57,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"runtime\/java.hpp\"\n@@ -72,5 +68,0 @@\n-bool HeapShared::_closed_regions_mapped = false;\n-bool HeapShared::_open_regions_mapped = false;\n-bool HeapShared::_is_loaded = false;\n-address   HeapShared::_narrow_oop_base;\n-int       HeapShared::_narrow_oop_shift;\n@@ -80,17 +71,0 @@\n-\/\/ Support for loaded heap.\n-uintptr_t HeapShared::_loaded_heap_bottom = 0;\n-uintptr_t HeapShared::_loaded_heap_top = 0;\n-uintptr_t HeapShared::_dumptime_base_0 = UINTPTR_MAX;\n-uintptr_t HeapShared::_dumptime_base_1 = UINTPTR_MAX;\n-uintptr_t HeapShared::_dumptime_base_2 = UINTPTR_MAX;\n-uintptr_t HeapShared::_dumptime_base_3 = UINTPTR_MAX;\n-uintptr_t HeapShared::_dumptime_top    = 0;\n-intx HeapShared::_runtime_offset_0 = 0;\n-intx HeapShared::_runtime_offset_1 = 0;\n-intx HeapShared::_runtime_offset_2 = 0;\n-intx HeapShared::_runtime_offset_3 = 0;\n-bool HeapShared::_loading_failed = false;\n-\n-\/\/ Support for mapped heap (!UseCompressedOops only)\n-ptrdiff_t HeapShared::_runtime_delta = 0;\n-\n@@ -165,16 +139,0 @@\n-void HeapShared::fixup_regions() {\n-  FileMapInfo* mapinfo = FileMapInfo::current_info();\n-  if (is_mapped()) {\n-    mapinfo->fixup_mapped_heap_regions();\n-  } else if (_loading_failed) {\n-    fill_failed_loaded_region();\n-  }\n-  if (is_fully_available()) {\n-    if (!MetaspaceShared::use_full_module_graph()) {\n-      \/\/ Need to remove all the archived java.lang.Module objects from HeapShared::roots().\n-      ClassLoaderDataShared::clear_archived_oops();\n-    }\n-  }\n-  SystemDictionaryShared::update_archived_mirror_native_pointers();\n-}\n-\n@@ -296,1 +254,1 @@\n-  if (is_fully_available()) {\n+  if (ArchiveHeapLoader::is_fully_available()) {\n@@ -443,1 +401,1 @@\n-  if (!is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_fully_available()) {\n@@ -590,5 +548,0 @@\n-void HeapShared::init_narrow_oop_decoding(address base, int shift) {\n-  _narrow_oop_base = base;\n-  _narrow_oop_shift = shift;\n-}\n-\n@@ -815,1 +768,1 @@\n-      assert(HeapShared::is_fully_available(), \"must be\");\n+      assert(ArchiveHeapLoader::is_fully_available(), \"must be\");\n@@ -861,1 +814,1 @@\n-  if (!is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_fully_available()) {\n@@ -899,1 +852,1 @@\n-  if (!is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_fully_available()) {\n@@ -1668,347 +1621,0 @@\n-\/\/ Patch all the embedded oop pointers inside an archived heap region,\n-\/\/ to be consistent with the runtime oop encoding.\n-class PatchCompressedEmbeddedPointers: public BitMapClosure {\n-  narrowOop* _start;\n-\n- public:\n-  PatchCompressedEmbeddedPointers(narrowOop* start) : _start(start) {}\n-\n-  bool do_bit(size_t offset) {\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    oop o = HeapShared::decode_from_archive(v);\n-    RawAccess<IS_NOT_NULL>::oop_store(p, o);\n-    return true;\n-  }\n-};\n-\n-class PatchUncompressedEmbeddedPointers: public BitMapClosure {\n-  oop* _start;\n-\n- public:\n-  PatchUncompressedEmbeddedPointers(oop* start) : _start(start) {}\n-\n-  bool do_bit(size_t offset) {\n-    oop* p = _start + offset;\n-    intptr_t dumptime_oop = (intptr_t)((void*)*p);\n-    assert(dumptime_oop != 0, \"null oops should have been filtered out at dump time\");\n-    intptr_t runtime_oop = dumptime_oop + HeapShared::runtime_delta();\n-    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(runtime_oop));\n-    return true;\n-  }\n-};\n-\n-\/\/ Patch all the non-null pointers that are embedded in the archived heap objects\n-\/\/ in this region\n-void HeapShared::patch_embedded_pointers(MemRegion region, address oopmap,\n-                                         size_t oopmap_size_in_bits) {\n-  BitMapView bm((BitMap::bm_word_t*)oopmap, oopmap_size_in_bits);\n-\n-#ifndef PRODUCT\n-  ResourceMark rm;\n-  ResourceBitMap checkBm = calculate_oopmap(region);\n-  assert(bm.is_same(checkBm), \"sanity\");\n-#endif\n-\n-  if (UseCompressedOops) {\n-    PatchCompressedEmbeddedPointers patcher((narrowOop*)region.start());\n-    bm.iterate(&patcher);\n-  } else {\n-    PatchUncompressedEmbeddedPointers patcher((oop*)region.start());\n-    bm.iterate(&patcher);\n-  }\n-}\n-\n-\/\/ The CDS archive remembers each heap object by its address at dump time, but\n-\/\/ the heap object may be loaded at a different address at run time. This structure is used\n-\/\/ to translate the dump time addresses for all objects in FileMapInfo::space_at(region_index)\n-\/\/ to their runtime addresses.\n-struct LoadedArchiveHeapRegion {\n-  int       _region_index;   \/\/ index for FileMapInfo::space_at(index)\n-  size_t    _region_size;    \/\/ number of bytes in this region\n-  uintptr_t _dumptime_base;  \/\/ The dump-time (decoded) address of the first object in this region\n-  intx      _runtime_offset; \/\/ If an object's dump time address P is within in this region, its\n-                             \/\/ runtime address is P + _runtime_offset\n-\n-  static int comparator(const void* a, const void* b) {\n-    LoadedArchiveHeapRegion* reg_a = (LoadedArchiveHeapRegion*)a;\n-    LoadedArchiveHeapRegion* reg_b = (LoadedArchiveHeapRegion*)b;\n-    if (reg_a->_dumptime_base < reg_b->_dumptime_base) {\n-      return -1;\n-    } else if (reg_a->_dumptime_base == reg_b->_dumptime_base) {\n-      return 0;\n-    } else {\n-      return 1;\n-    }\n-  }\n-\n-  uintptr_t top() {\n-    return _dumptime_base + _region_size;\n-  }\n-};\n-\n-void HeapShared::init_loaded_heap_relocation(LoadedArchiveHeapRegion* loaded_regions,\n-                                             int num_loaded_regions) {\n-  _dumptime_base_0 = loaded_regions[0]._dumptime_base;\n-  _dumptime_base_1 = loaded_regions[1]._dumptime_base;\n-  _dumptime_base_2 = loaded_regions[2]._dumptime_base;\n-  _dumptime_base_3 = loaded_regions[3]._dumptime_base;\n-  _dumptime_top = loaded_regions[num_loaded_regions-1].top();\n-\n-  _runtime_offset_0 = loaded_regions[0]._runtime_offset;\n-  _runtime_offset_1 = loaded_regions[1]._runtime_offset;\n-  _runtime_offset_2 = loaded_regions[2]._runtime_offset;\n-  _runtime_offset_3 = loaded_regions[3]._runtime_offset;\n-\n-  assert(2 <= num_loaded_regions && num_loaded_regions <= 4, \"must be\");\n-  if (num_loaded_regions < 4) {\n-    _dumptime_base_3 = UINTPTR_MAX;\n-  }\n-  if (num_loaded_regions < 3) {\n-    _dumptime_base_2 = UINTPTR_MAX;\n-  }\n-}\n-\n-bool HeapShared::can_load() {\n-  return Universe::heap()->can_load_archived_objects();\n-}\n-\n-template <int NUM_LOADED_REGIONS>\n-class PatchLoadedRegionPointers: public BitMapClosure {\n-  narrowOop* _start;\n-  intx _offset_0;\n-  intx _offset_1;\n-  intx _offset_2;\n-  intx _offset_3;\n-  uintptr_t _base_0;\n-  uintptr_t _base_1;\n-  uintptr_t _base_2;\n-  uintptr_t _base_3;\n-  uintptr_t _top;\n-\n-  static_assert(MetaspaceShared::max_num_heap_regions == 4, \"can't handle more than 4 regions\");\n-  static_assert(NUM_LOADED_REGIONS >= 2, \"we have at least 2 loaded regions\");\n-  static_assert(NUM_LOADED_REGIONS <= 4, \"we have at most 4 loaded regions\");\n-\n- public:\n-  PatchLoadedRegionPointers(narrowOop* start, LoadedArchiveHeapRegion* loaded_regions)\n-    : _start(start),\n-      _offset_0(loaded_regions[0]._runtime_offset),\n-      _offset_1(loaded_regions[1]._runtime_offset),\n-      _offset_2(loaded_regions[2]._runtime_offset),\n-      _offset_3(loaded_regions[3]._runtime_offset),\n-      _base_0(loaded_regions[0]._dumptime_base),\n-      _base_1(loaded_regions[1]._dumptime_base),\n-      _base_2(loaded_regions[2]._dumptime_base),\n-      _base_3(loaded_regions[3]._dumptime_base) {\n-    _top = loaded_regions[NUM_LOADED_REGIONS-1].top();\n-  }\n-\n-  bool do_bit(size_t offset) {\n-    narrowOop* p = _start + offset;\n-    narrowOop v = *p;\n-    assert(!CompressedOops::is_null(v), \"null oops should have been filtered out at dump time\");\n-    uintptr_t o = cast_from_oop<uintptr_t>(HeapShared::decode_from_archive(v));\n-    assert(_base_0 <= o && o < _top, \"must be\");\n-\n-\n-    \/\/ We usually have only 2 regions for the default archive. Use template to avoid unnecessary comparisons.\n-    if (NUM_LOADED_REGIONS > 3 && o >= _base_3) {\n-      o += _offset_3;\n-    } else if (NUM_LOADED_REGIONS > 2 && o >= _base_2) {\n-      o += _offset_2;\n-    } else if (o >= _base_1) {\n-      o += _offset_1;\n-    } else {\n-      o += _offset_0;\n-    }\n-    HeapShared::assert_in_loaded_heap(o);\n-    RawAccess<IS_NOT_NULL>::oop_store(p, cast_to_oop(o));\n-    return true;\n-  }\n-};\n-\n-int HeapShared::init_loaded_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                                    MemRegion& archive_space) {\n-  size_t total_bytes = 0;\n-  int num_loaded_regions = 0;\n-  for (int i = MetaspaceShared::first_archive_heap_region;\n-       i <= MetaspaceShared::last_archive_heap_region; i++) {\n-    FileMapRegion* r = mapinfo->space_at(i);\n-    r->assert_is_heap_region();\n-    if (r->used() > 0) {\n-      assert(is_aligned(r->used(), HeapWordSize), \"must be\");\n-      total_bytes += r->used();\n-      LoadedArchiveHeapRegion* ri = &loaded_regions[num_loaded_regions++];\n-      ri->_region_index = i;\n-      ri->_region_size = r->used();\n-      ri->_dumptime_base = (uintptr_t)mapinfo->start_address_as_decoded_from_archive(r);\n-    }\n-  }\n-\n-  assert(is_aligned(total_bytes, HeapWordSize), \"must be\");\n-  size_t word_size = total_bytes \/ HeapWordSize;\n-  HeapWord* buffer = Universe::heap()->allocate_loaded_archive_space(word_size);\n-  if (buffer == nullptr) {\n-    return 0;\n-  }\n-\n-  archive_space = MemRegion(buffer, word_size);\n-  _loaded_heap_bottom = (uintptr_t)archive_space.start();\n-  _loaded_heap_top    = _loaded_heap_bottom + total_bytes;\n-\n-  return num_loaded_regions;\n-}\n-\n-void HeapShared::sort_loaded_regions(LoadedArchiveHeapRegion* loaded_regions, int num_loaded_regions,\n-                                     uintptr_t buffer) {\n-  \/\/ Find the relocation offset of the pointers in each region\n-  qsort(loaded_regions, num_loaded_regions, sizeof(LoadedArchiveHeapRegion),\n-        LoadedArchiveHeapRegion::comparator);\n-\n-  uintptr_t p = buffer;\n-  for (int i = 0; i < num_loaded_regions; i++) {\n-    \/\/ This region will be loaded at p, so all objects inside this\n-    \/\/ region will be shifted by ri->offset\n-    LoadedArchiveHeapRegion* ri = &loaded_regions[i];\n-    ri->_runtime_offset = p - ri->_dumptime_base;\n-    p += ri->_region_size;\n-  }\n-  assert(p == _loaded_heap_top, \"must be\");\n-}\n-\n-bool HeapShared::load_regions(FileMapInfo* mapinfo, LoadedArchiveHeapRegion* loaded_regions,\n-                              int num_loaded_regions, uintptr_t buffer) {\n-  uintptr_t bitmap_base = (uintptr_t)mapinfo->map_bitmap_region();\n-  if (bitmap_base == 0) {\n-    _loading_failed = true;\n-    return false; \/\/ OOM or CRC error\n-  }\n-  uintptr_t load_address = buffer;\n-  for (int i = 0; i < num_loaded_regions; i++) {\n-    LoadedArchiveHeapRegion* ri = &loaded_regions[i];\n-    FileMapRegion* r = mapinfo->space_at(ri->_region_index);\n-\n-    if (!mapinfo->read_region(ri->_region_index, (char*)load_address, r->used(), \/* do_commit = *\/ false)) {\n-      \/\/ There's no easy way to free the buffer, so we will fill it with zero later\n-      \/\/ in fill_failed_loaded_region(), and it will eventually be GC'ed.\n-      log_warning(cds)(\"Loading of heap region %d has failed. Archived objects are disabled\", i);\n-      _loading_failed = true;\n-      return false;\n-    }\n-    log_info(cds)(\"Loaded heap    region #%d at base \" INTPTR_FORMAT \" top \" INTPTR_FORMAT\n-                  \" size \" SIZE_FORMAT_W(6) \" delta \" INTX_FORMAT,\n-                  ri->_region_index, load_address, load_address + ri->_region_size,\n-                  ri->_region_size, ri->_runtime_offset);\n-\n-    uintptr_t oopmap = bitmap_base + r->oopmap_offset();\n-    BitMapView bm((BitMap::bm_word_t*)oopmap, r->oopmap_size_in_bits());\n-\n-    if (num_loaded_regions == 4) {\n-      PatchLoadedRegionPointers<4> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n-    } else if (num_loaded_regions == 3) {\n-      PatchLoadedRegionPointers<3> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n-    } else {\n-      assert(num_loaded_regions == 2, \"must be\");\n-      PatchLoadedRegionPointers<2> patcher((narrowOop*)load_address, loaded_regions);\n-      bm.iterate(&patcher);\n-    }\n-\n-    load_address += r->used();\n-  }\n-\n-  return true;\n-}\n-\n-bool HeapShared::load_heap_regions(FileMapInfo* mapinfo) {\n-  init_narrow_oop_decoding(mapinfo->narrow_oop_base(), mapinfo->narrow_oop_shift());\n-\n-  LoadedArchiveHeapRegion loaded_regions[MetaspaceShared::max_num_heap_regions];\n-  memset(loaded_regions, 0, sizeof(loaded_regions));\n-\n-  MemRegion archive_space;\n-  int num_loaded_regions = init_loaded_regions(mapinfo, loaded_regions, archive_space);\n-  if (num_loaded_regions <= 0) {\n-    return false;\n-  }\n-  sort_loaded_regions(loaded_regions, num_loaded_regions, (uintptr_t)archive_space.start());\n-  if (!load_regions(mapinfo, loaded_regions, num_loaded_regions, (uintptr_t)archive_space.start())) {\n-    assert(_loading_failed, \"must be\");\n-    return false;\n-  }\n-\n-  init_loaded_heap_relocation(loaded_regions, num_loaded_regions);\n-  _is_loaded = true;\n-\n-  return true;\n-}\n-\n-class VerifyLoadedHeapEmbeddedPointers: public BasicOopIterateClosure {\n-  ResourceHashtable<uintptr_t, bool>* _table;\n-\n- public:\n-  VerifyLoadedHeapEmbeddedPointers(ResourceHashtable<uintptr_t, bool>* table) : _table(table) {}\n-\n-  virtual void do_oop(narrowOop* p) {\n-    \/\/ This should be called before the loaded regions are modified, so all the embedded pointers\n-    \/\/ must be NULL, or must point to a valid object in the loaded regions.\n-    narrowOop v = *p;\n-    if (!CompressedOops::is_null(v)) {\n-      oop o = CompressedOops::decode_not_null(v);\n-      uintptr_t u = cast_from_oop<uintptr_t>(o);\n-      HeapShared::assert_in_loaded_heap(u);\n-      guarantee(_table->contains(u), \"must point to beginning of object in loaded archived regions\");\n-    }\n-  }\n-  virtual void do_oop(oop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-void HeapShared::finish_initialization() {\n-  if (is_loaded()) {\n-    HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-    HeapWord* top    = (HeapWord*)_loaded_heap_top;\n-\n-    MemRegion archive_space = MemRegion(bottom, top);\n-    Universe::heap()->complete_loaded_archive_space(archive_space);\n-  }\n-\n-  if (VerifyArchivedFields <= 0 || !is_loaded()) {\n-    return;\n-  }\n-\n-  log_info(cds, heap)(\"Verify all oops and pointers in loaded heap\");\n-\n-  ResourceMark rm;\n-  ResourceHashtable<uintptr_t, bool> table;\n-  VerifyLoadedHeapEmbeddedPointers verifier(&table);\n-  HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-  HeapWord* top    = (HeapWord*)_loaded_heap_top;\n-\n-  for (HeapWord* p = bottom; p < top; ) {\n-    oop o = cast_to_oop(p);\n-    table.put(cast_from_oop<uintptr_t>(o), true);\n-    p += o->size();\n-  }\n-\n-  for (HeapWord* p = bottom; p < top; ) {\n-    oop o = cast_to_oop(p);\n-    o->oop_iterate(&verifier);\n-    p += o->size();\n-  }\n-}\n-\n-void HeapShared::fill_failed_loaded_region() {\n-  assert(_loading_failed, \"must be\");\n-  if (_loaded_heap_bottom != 0) {\n-    assert(_loaded_heap_top != 0, \"must be\");\n-    HeapWord* bottom = (HeapWord*)_loaded_heap_bottom;\n-    HeapWord* top = (HeapWord*)_loaded_heap_top;\n-    Universe::heap()->fill_with_objects(bottom, top - bottom);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":7,"deletions":401,"binary":false,"changes":408,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n+#include \"cds\/cds_globals.hpp\"\n@@ -1474,1 +1476,1 @@\n-  HeapShared::finish_initialization();\n+  ArchiveHeapLoader::finish_initialization();\n@@ -1561,1 +1563,1 @@\n-    result &= HeapShared::can_use();\n+    result &= ArchiveHeapLoader::can_use();\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -40,1 +40,0 @@\n-#include \"classfile\/symbolTable.hpp\"\n@@ -69,0 +68,1 @@\n+#include \"oops\/symbolHandle.hpp\"\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -729,1 +729,1 @@\n-    out->print(\" \" INTPTRNZ_FORMAT, data()[i]);\n+    out->print(\" \" INTX_FORMAT_X, data()[i]);\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -6378,1 +6378,1 @@\n-    jio_snprintf(addr_buf, 20, SIZE_FORMAT_HEX, new_id);\n+    jio_snprintf(addr_buf, 20, SIZE_FORMAT_X, new_id);\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -50,1 +50,0 @@\n-class TempNewSymbol;\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/cds_globals.hpp\"\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,2 @@\n-#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/archiveHeapLoader.hpp\"\n+#include \"cds\/heapShared.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -901,1 +903,1 @@\n-    if (HeapShared::are_archived_mirrors_available()) {\n+    if (ArchiveHeapLoader::are_archived_mirrors_available()) {\n@@ -1370,1 +1372,1 @@\n-  if (HeapShared::is_mapped()) {\n+  if (ArchiveHeapLoader::is_mapped()) {\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1004,0 +1004,2 @@\n+  static inline bool is_weak(oop ref);\n+  static inline bool is_soft(oop ref);\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"memory\/referenceType.hpp\"\n@@ -135,0 +136,1 @@\n+  assert(java_lang_ref_Reference::is_weak(ref) || java_lang_ref_Reference::is_soft(ref), \"must be Weak or Soft Reference\");\n@@ -139,0 +141,1 @@\n+  assert(java_lang_ref_Reference::is_weak(ref) || java_lang_ref_Reference::is_soft(ref), \"must be Weak or Soft Reference\");\n@@ -143,0 +146,1 @@\n+  assert(java_lang_ref_Reference::is_phantom(ref), \"must be Phantom Reference\");\n@@ -198,0 +202,8 @@\n+bool java_lang_ref_Reference::is_weak(oop ref) {\n+  return InstanceKlass::cast(ref->klass())->reference_type() == REF_WEAK;\n+}\n+\n+bool java_lang_ref_Reference::is_soft(oop ref) {\n+  return InstanceKlass::cast(ref->klass())->reference_type() == REF_SOFT;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"oops\/symbolHandle.hpp\"\n@@ -37,1 +38,1 @@\n-  Symbol* _name;\n+  SymbolHandle _name;\n@@ -215,2 +216,0 @@\n-  \/\/ Since we're storing this key in the hashtable, we need to increment the refcount.\n-  class_name->increment_refcount();\n@@ -229,2 +228,0 @@\n-  \/\/ Decrement the refcount in key, since it's no longer in the table.\n-  class_name->decrement_refcount();\n@@ -250,1 +247,1 @@\n-inline void log(PlaceholderEntry* entry, const char* function, PlaceholderTable::classloadAction action) {\n+inline void log(Symbol* name, PlaceholderEntry* entry, const char* function, PlaceholderTable::classloadAction action) {\n@@ -255,1 +252,1 @@\n-    ls.print(\"%s %s \", function, action_to_string(action));\n+    ls.print(\"entry %s : %s %s \", name->as_C_string(), function, action_to_string(action));\n@@ -281,1 +278,1 @@\n-  log(probe, \"find_and_add\", action);\n+  log(name, probe, \"find_and_add\", action);\n@@ -305,1 +302,1 @@\n-    log(probe, \"find_and_remove\", action);\n+    log(name, probe, \"find_and_remove\", action);\n","filename":"src\/hotspot\/share\/classfile\/placeholders.cpp","additions":6,"deletions":9,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -121,2 +121,1 @@\n-ResourceHashtable<Symbol*, OopHandle, 139, ResourceObj::C_HEAP, mtClass> _invoke_method_type_table;\n-ProtectionDomainCacheTable*   SystemDictionary::_pd_cache_table = NULL;\n+ResourceHashtable<SymbolHandle, OopHandle, 139, ResourceObj::C_HEAP, mtClass, SymbolHandle::compute_hash> _invoke_method_type_table;\n@@ -127,6 +126,0 @@\n-\/\/ Default ProtectionDomainCacheSize value\n-const int defaultProtectionDomainCacheSize = 1009;\n-\n-const int _resolution_error_size  = 107;                     \/\/ number of entries in resolution error table\n-const int _invoke_method_size     = 139;                     \/\/ number of entries in invoke method table\n-\n@@ -385,1 +378,1 @@\n-static inline void log_circularity_error(Thread* thread, PlaceholderEntry* probe) {\n+static inline void log_circularity_error(Symbol* name, PlaceholderEntry* probe) {\n@@ -388,1 +381,1 @@\n-    ResourceMark rm(thread);\n+    ResourceMark rm;\n@@ -390,1 +383,1 @@\n-    ls.print(\"ClassCircularityError detected for placeholder \");\n+    ls.print(\"ClassCircularityError detected for placeholder entry %s\", name->as_C_string());\n@@ -462,1 +455,1 @@\n-          log_circularity_error(THREAD, probe);\n+          log_circularity_error(class_name, probe);\n@@ -625,1 +618,1 @@\n-      log_circularity_error(current, oldprobe);\n+      log_circularity_error(name, oldprobe);\n@@ -1744,1 +1737,1 @@\n-      _pd_cache_table->trigger_cleanup();\n+      ProtectionDomainCacheTable::trigger_cleanup();\n@@ -1746,1 +1739,1 @@\n-      assert(_pd_cache_table->number_of_entries() == 0, \"should be empty\");\n+      assert(ProtectionDomainCacheTable::number_of_entries() == 0, \"should be empty\");\n@@ -1779,3 +1772,0 @@\n-  \/\/ Allocate arrays\n-  _pd_cache_table = new ProtectionDomainCacheTable(defaultProtectionDomainCacheSize);\n-\n@@ -2129,1 +2119,0 @@\n-  methodHandle empty;\n@@ -2136,2 +2125,0 @@\n-  Method** met;\n-  InvokeMethodKey key(signature, iid_as_int);\n@@ -2140,1 +2127,2 @@\n-    met = _invoke_method_intrinsic_table.get(key);\n+    InvokeMethodKey key(signature, iid_as_int);\n+    Method** met = _invoke_method_intrinsic_table.get(key);\n@@ -2144,24 +2132,21 @@\n-  }\n-  methodHandle m = Method::make_method_handle_intrinsic(iid, signature, CHECK_NULL);\n-  if (!Arguments::is_interpreter_only() || iid == vmIntrinsics::_linkToNative) {\n-      \/\/ Generate a compiled form of the MH intrinsic\n-      \/\/ linkToNative doesn't have interpreter-specific implementation, so always has to go through compiled version.\n-      AdapterHandlerLibrary::create_native_wrapper(m);\n-      \/\/ Check if have the compiled code.\n-      if (!m->has_compiled_code()) {\n-        THROW_MSG_NULL(vmSymbols::java_lang_VirtualMachineError(),\n-                       \"Out of space in CodeCache for method handle intrinsic\");\n-      }\n-  }\n-  \/\/ Now grab the lock.  We might have to throw away the new method,\n-  \/\/ if a racing thread has managed to install one at the same time.\n-  {\n-    MutexLocker ml(THREAD, InvokeMethodTable_lock);\n-    signature->make_permanent(); \/\/ The signature is never unloaded.\n-    bool created;\n-    met = _invoke_method_intrinsic_table.put_if_absent(key, m(), &created);\n-    Method* saved_method = *met;\n-    assert(Arguments::is_interpreter_only() || (saved_method->has_compiled_code() &&\n-         saved_method->code()->entry_point() == saved_method->from_compiled_entry()),\n-         \"MH intrinsic invariant\");\n-    return saved_method;\n+    bool throw_error = false;\n+    \/\/ This function could get an OOM but it is safe to call inside of a lock because\n+    \/\/ throwing OutOfMemoryError doesn't call Java code.\n+    methodHandle m = Method::make_method_handle_intrinsic(iid, signature, CHECK_NULL);\n+    if (!Arguments::is_interpreter_only() || iid == vmIntrinsics::_linkToNative) {\n+        \/\/ Generate a compiled form of the MH intrinsic\n+        \/\/ linkToNative doesn't have interpreter-specific implementation, so always has to go through compiled version.\n+        AdapterHandlerLibrary::create_native_wrapper(m);\n+        \/\/ Check if have the compiled code.\n+        throw_error = (!m->has_compiled_code());\n+    }\n+\n+    if (!throw_error) {\n+      signature->make_permanent(); \/\/ The signature is never unloaded.\n+      bool created = _invoke_method_intrinsic_table.put(key, m());\n+      assert(created, \"must be since we still hold the lock\");\n+      assert(Arguments::is_interpreter_only() || (m->has_compiled_code() &&\n+             m->code()->entry_point() == m->from_compiled_entry()),\n+             \"MH intrinsic invariant\");\n+      return m();\n+    }\n@@ -2170,0 +2155,4 @@\n+\n+  \/\/ Throw error outside of the lock.\n+  THROW_MSG_NULL(vmSymbols::java_lang_VirtualMachineError(),\n+                 \"Out of space in CodeCache for method handle intrinsic\");\n@@ -2551,1 +2540,1 @@\n-  _pd_cache_table->print_on(st);\n+  ProtectionDomainCacheTable::print_on(st);\n@@ -2567,1 +2556,2 @@\n-  _pd_cache_table->verify();\n+  \/\/ Verify protection domain table\n+  ProtectionDomainCacheTable::verify();\n@@ -2578,1 +2568,1 @@\n-    pd_cache_table()->print_table_statistics(st, \"ProtectionDomainCache Table\");\n+    ProtectionDomainCacheTable::print_table_statistics(st);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":39,"deletions":49,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -74,4 +74,0 @@\n-template <MEMFLAGS F> class HashtableBucket;\n-class SymbolPropertyTable;\n-class ProtectionDomainCacheTable;\n-class ProtectionDomainCacheEntry;\n@@ -200,3 +196,0 @@\n-  \/\/ Protection Domain Table\n-  static ProtectionDomainCacheTable* pd_cache_table() { return _pd_cache_table; }\n-\n@@ -305,9 +298,0 @@\n- private:\n-  \/\/ Static tables owned by the SystemDictionary\n-\n-  \/\/ Invoke methods (JSR 292)\n-  static SymbolPropertyTable*    _invoke_method_table;\n-\n-  \/\/ ProtectionDomain cache\n-  static ProtectionDomainCacheTable*   _pd_cache_table;\n-\n@@ -324,2 +308,0 @@\n-  static SymbolPropertyTable* invoke_method_table() { return _invoke_method_table; }\n-\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":0,"deletions":18,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -241,0 +241,1 @@\n+    case vmIntrinsics::_Continuation_enterSpecial:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -557,1 +557,1 @@\n-  do_intrinsic(_Continuation_doYield,      jdk_internal_vm_Continuation, doYield_name,      continuationDoYield_signature, F_S) \\\n+  do_intrinsic(_Continuation_doYield,      jdk_internal_vm_Continuation, doYield_name,      continuationDoYield_signature, F_SN) \\\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -433,1 +433,1 @@\n-    if (method()->is_continuation_enter_intrinsic()) {\n+    if (method()->is_continuation_native_intrinsic()) {\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -198,1 +198,1 @@\n-  assert(!region_attr.is_humongous(),\n+  assert(!region_attr.is_humongous_candidate(),\n@@ -330,1 +330,1 @@\n-  assert(dest->is_in_cset_or_humongous(), \"Unexpected dest: %s region attr\", dest->get_type_str());\n+  assert(dest->is_in_cset_or_humongous_candidate(), \"Unexpected dest: %s region attr\", dest->get_type_str());\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1687,1 +1687,0 @@\n-  GCCause::Cause gc_cause = heap->gc_cause();\n@@ -1690,1 +1689,0 @@\n-  PSAdaptiveSizePolicy* policy = heap->size_policy();\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -84,1 +84,0 @@\n-    java_lang_continuation_doYield,                             \/\/ implementation of jdk.internal.vm.Continuation.doYield()\n@@ -299,1 +298,1 @@\n-  AbstractInterpreterGenerator(StubQueue* _code);\n+  AbstractInterpreterGenerator();\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"classfile\/symbolTable.hpp\"\n@@ -52,0 +51,1 @@\n+#include \"oops\/symbolHandle.hpp\"\n@@ -1115,1 +1115,1 @@\n-  if (resolved_method->is_continuation_enter_intrinsic()\n+  if (resolved_method->is_continuation_native_intrinsic()\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -66,1 +66,1 @@\n-    TemplateInterpreterGenerator g(_code);\n+    TemplateInterpreterGenerator g;\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreter.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-TemplateInterpreterGenerator::TemplateInterpreterGenerator(StubQueue* _code): AbstractInterpreterGenerator(_code) {\n+TemplateInterpreterGenerator::TemplateInterpreterGenerator(): AbstractInterpreterGenerator() {\n@@ -227,2 +227,0 @@\n-  method_entry(java_lang_continuation_doYield)\n-\n@@ -429,2 +427,0 @@\n-  case Interpreter::java_lang_continuation_doYield\n-                                           : entry_point = generate_Continuation_doYield_entry(); break;\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -339,1 +339,0 @@\n-  static_field(StubRoutines,                _cont_doYield,                                    address)                               \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n@@ -461,1 +462,1 @@\n-        HeapShared::are_archived_mirrors_available() &&\n+        ArchiveHeapLoader::are_archived_mirrors_available() &&\n@@ -463,1 +464,1 @@\n-      assert(HeapShared::can_use(), \"Sanity\");\n+      assert(ArchiveHeapLoader::can_use(), \"Sanity\");\n@@ -817,1 +818,1 @@\n-    if (HeapShared::is_loaded()) {\n+    if (ArchiveHeapLoader::is_loaded()) {\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n@@ -351,1 +352,1 @@\n-    if (HeapShared::is_fully_available() &&\n+    if (ArchiveHeapLoader::is_fully_available() &&\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"classfile\/symbolTable.hpp\"\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"oops\/objArrayKlass.hpp\"\n@@ -609,1 +611,1 @@\n-    if (HeapShared::are_archived_mirrors_available()) {\n+    if (ArchiveHeapLoader::are_archived_mirrors_available()) {\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1288,1 +1288,1 @@\n-  if (h_method->is_continuation_enter_intrinsic()) {\n+  if (h_method->is_continuation_native_intrinsic()) {\n@@ -1292,0 +1292,1 @@\n+    _i2i_entry = NULL;\n@@ -1397,1 +1398,1 @@\n-  if (mh->is_continuation_enter_intrinsic()) {\n+  if (mh->is_continuation_native_intrinsic()) {\n@@ -1400,2 +1401,8 @@\n-    \/\/ This is the entry used when we're in interpreter-only mode; see InterpreterMacroAssembler::jump_from_interpreted\n-    mh->_i2i_entry = ContinuationEntry::interpreted_entry();\n+    if (mh->is_continuation_enter_intrinsic()) {\n+      \/\/ This is the entry used when we're in interpreter-only mode; see InterpreterMacroAssembler::jump_from_interpreted\n+      mh->_i2i_entry = ContinuationEntry::interpreted_entry();\n+    } else if (mh->is_continuation_yield_intrinsic()) {\n+      mh->_i2i_entry = mh->get_i2c_entry();\n+    } else {\n+      guarantee(false, \"Unknown Continuation native intrinsic\");\n+    }\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -752,0 +752,2 @@\n+  inline bool is_continuation_yield_intrinsic() const;\n+  inline bool is_continuation_native_intrinsic() const;\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -104,0 +104,10 @@\n+\n+inline bool Method::is_continuation_yield_intrinsic() const {\n+  return intrinsic_id() == vmIntrinsics::_Continuation_doYield;\n+}\n+\n+inline bool Method::is_continuation_native_intrinsic() const {\n+  return intrinsic_id() == vmIntrinsics::_Continuation_enterSpecial ||\n+         intrinsic_id() == vmIntrinsics::_Continuation_doYield;\n+}\n+\n@@ -105,1 +115,1 @@\n-  return is_method_handle_intrinsic() || is_continuation_enter_intrinsic();\n+  return is_method_handle_intrinsic() || is_continuation_native_intrinsic();\n","filename":"src\/hotspot\/share\/oops\/method.inline.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -120,4 +120,0 @@\n-  notproduct(uintx, PrintIdealIndentThreshold, 0,                           \\\n-          \"A depth threshold of ideal graph. Indentation is disabled \"      \\\n-          \"when users attempt to dump an ideal graph deeper than it.\")      \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -748,1 +748,0 @@\n-  case vmIntrinsics::_Continuation_doYield:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -133,7 +133,0 @@\n-\n-\/\/ For a ParmNode, all immediate inputs and outputs are considered relevant\n-\/\/ both in compact and standard representation.\n-void ParmNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  this->collect_nodes(in_rel, 1, false, false);\n-  this->collect_nodes(out_rel, -1, false, false);\n-}\n@@ -1584,13 +1577,0 @@\n-\n-\/\/ The related nodes of a SafepointNode are all data inputs, excluding the\n-\/\/ control boundary, as well as all outputs till level 2 (to include projection\n-\/\/ nodes and targets). In compact mode, just include inputs till level 1 and\n-\/\/ outputs as before.\n-void SafePointNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  if (compact) {\n-    this->collect_nodes(in_rel, 1, false, false);\n-  } else {\n-    this->collect_nodes_in_all_data(in_rel, false);\n-  }\n-  this->collect_nodes(out_rel, -2, false, false);\n-}\n@@ -2229,10 +2209,0 @@\n-\n-\/\/ The related set of lock nodes includes the control boundary.\n-void AbstractLockNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  if (compact) {\n-      this->collect_nodes(in_rel, 1, false, false);\n-    } else {\n-      this->collect_nodes_in_all_data(in_rel, true);\n-    }\n-    this->collect_nodes(out_rel, -2, false, false);\n-}\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -110,1 +110,0 @@\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n@@ -502,1 +501,0 @@\n-  virtual void           related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n@@ -1128,1 +1126,0 @@\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2738,8 +2738,0 @@\n-void PhiNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  \/\/ For a PhiNode, the set of related nodes includes all inputs till level 2,\n-  \/\/ and all outputs till level 1. In compact mode, inputs till level 1 are\n-  \/\/ collected.\n-  this->collect_nodes(in_rel, compact ? 1 : 2, false, false);\n-  this->collect_nodes(out_rel, -1, false, false);\n-}\n-\n@@ -2770,11 +2762,0 @@\n-#ifndef PRODUCT\n-\/\/-----------------------------related-----------------------------------------\n-\/\/ The related nodes of a GotoNode are all inputs at level 1, as well as the\n-\/\/ outputs at level 1. This is regardless of compact mode.\n-void GotoNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  this->collect_nodes(in_rel, 1, false, false);\n-  this->collect_nodes(out_rel, -1, false, false);\n-}\n-#endif\n-\n-\n@@ -2786,11 +2767,0 @@\n-#ifndef PRODUCT\n-\/\/-----------------------------related-----------------------------------------\n-\/\/ The related nodes of a JumpNode are all inputs at level 1, as well as the\n-\/\/ outputs at level 2 (to include actual jump targets beyond projection nodes).\n-\/\/ This is regardless of compact mode.\n-void JumpNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  this->collect_nodes(in_rel, 1, false, false);\n-  this->collect_nodes(out_rel, -2, false, false);\n-}\n-#endif\n-\n@@ -2857,6 +2827,0 @@\n-\n-void JumpProjNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  \/\/ The related nodes of a JumpProjNode are its inputs and outputs at level 1.\n-  this->collect_nodes(in_rel, 1, false, false);\n-  this->collect_nodes(out_rel, -1, false, false);\n-}\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":0,"deletions":36,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -229,1 +229,0 @@\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n@@ -255,4 +254,0 @@\n-\n-#ifndef PRODUCT\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n-#endif\n@@ -413,1 +408,0 @@\n-  virtual void related(GrowableArray <Node *> *in_rel, GrowableArray <Node *> *out_rel, bool compact) const;\n@@ -439,5 +433,0 @@\n-\n-#ifndef PRODUCT\n-public:\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n-#endif\n@@ -511,3 +500,0 @@\n-#ifndef PRODUCT\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n-#endif\n@@ -539,1 +525,0 @@\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -3460,0 +3460,1 @@\n+    cfg.remove_unreachable_blocks();\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1760,15 +1760,0 @@\n-\/\/-------------------------------related---------------------------------------\n-\/\/ An IfProjNode's related node set consists of its input (an IfNode) including\n-\/\/ the IfNode's condition, plus all of its outputs at level 1. In compact mode,\n-\/\/ the restrictions for IfNode apply (see IfNode::rel).\n-void IfProjNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  Node* ifNode = this->in(0);\n-  in_rel->append(ifNode);\n-  if (compact) {\n-    ifNode->collect_nodes(in_rel, 3, false, true);\n-  } else {\n-    ifNode->collect_nodes_in_all_data(in_rel, false);\n-  }\n-  this->collect_nodes(out_rel, -1, false, false);\n-}\n-\n@@ -1779,15 +1764,0 @@\n-\n-\/\/-------------------------------related---------------------------------------\n-\/\/ For an IfNode, the set of related output nodes is just the output nodes till\n-\/\/ depth 2, i.e, the IfTrue\/IfFalse projection nodes plus the nodes they refer.\n-\/\/ The related input nodes contain no control nodes, but all data nodes\n-\/\/ pertaining to the condition. In compact mode, the input nodes are collected\n-\/\/ up to a depth of 3.\n-void IfNode::related(GrowableArray <Node *> *in_rel, GrowableArray <Node *> *out_rel, bool compact) const {\n-  if (compact) {\n-    this->collect_nodes(in_rel, 3, false, true);\n-  } else {\n-    this->collect_nodes_in_all_data(in_rel, false);\n-  }\n-  this->collect_nodes(out_rel, -2, false, false);\n-}\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":0,"deletions":30,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -972,1 +972,1 @@\n-        block->get_node(i)->fast_dump();\n+        block->get_node(i)->dump();\n@@ -1240,1 +1240,1 @@\n-      block->get_node(i)->fast_dump();\n+      block->get_node(i)->dump();\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -655,3 +655,0 @@\n-  case vmIntrinsics::_Continuation_doYield:\n-    return inline_continuation_do_yield();\n-\n@@ -1629,0 +1626,4 @@\n+  \/\/ Save state and restore on bailout\n+  uint old_sp = sp();\n+  SafePointNode* old_map = clone_map();\n+\n@@ -1633,0 +1634,2 @@\n+    set_map(old_map);\n+    set_sp(old_sp);\n@@ -1635,0 +1638,1 @@\n+  old_map->destruct(&_gvn);\n@@ -7791,9 +7795,0 @@\n-bool LibraryCallKit::inline_continuation_do_yield() {\n-  address call_addr = StubRoutines::cont_doYield();\n-  const TypeFunc* tf = OptoRuntime::continuation_doYield_Type();\n-  Node* call = make_runtime_call(RC_NO_LEAF, tf, call_addr, \"doYield\", TypeRawPtr::BOTTOM);\n-  Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n-  set_result(result);\n-  return true;\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":7,"deletions":12,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -529,1 +529,1 @@\n-  Node* find_old_node(Node* new_node) {\n+  Node* find_old_node(const Node* new_node) {\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -335,1 +335,0 @@\n-  , _indent(0)\n@@ -356,1 +355,0 @@\n-  , _indent(0)\n@@ -370,1 +368,0 @@\n-  , _indent(0)\n@@ -386,1 +383,0 @@\n-  , _indent(0)\n@@ -404,1 +400,0 @@\n-  , _indent(0)\n@@ -424,1 +419,0 @@\n-  , _indent(0)\n@@ -447,1 +441,0 @@\n-  , _indent(0)\n@@ -472,1 +465,0 @@\n-  , _indent(0)\n@@ -1657,1 +1649,1 @@\n-int node_idx_cmp(Node** n1, Node** n2) {\n+int node_idx_cmp(const Node** n1, const Node** n2) {\n@@ -1661,1 +1653,1 @@\n-Node* find_node_by_name(Node* start, const char* name) {\n+void find_nodes_by_name(Node* start, const char* name) {\n@@ -1663,3 +1655,2 @@\n-  Node* result = nullptr;\n-  GrowableArray<Node*> ns;\n-  auto callback = [&] (Node* n) {\n+  GrowableArray<const Node*> ns;\n+  auto callback = [&] (const Node* n) {\n@@ -1668,1 +1659,0 @@\n-      result = n;\n@@ -1676,1 +1666,0 @@\n-  return result;\n@@ -1679,1 +1668,1 @@\n-Node* find_node_by_dump(Node* start, const char* pattern) {\n+void find_nodes_by_dump(Node* start, const char* pattern) {\n@@ -1681,3 +1670,2 @@\n-  Node* result = nullptr;\n-  GrowableArray<Node*> ns;\n-  auto callback = [&] (Node* n) {\n+  GrowableArray<const Node*> ns;\n+  auto callback = [&] (const Node* n) {\n@@ -1688,1 +1676,0 @@\n-      result = n;\n@@ -1696,1 +1683,0 @@\n-  return result;\n@@ -1702,1 +1688,1 @@\n-Node* find_node_by_name(const char* name) {\n+void find_nodes_by_name(const char* name) {\n@@ -1704,1 +1690,1 @@\n-  return find_node_by_name(root, name);\n+  find_nodes_by_name(root, name);\n@@ -1710,1 +1696,1 @@\n-Node* find_old_node_by_name(const char* name) {\n+void find_old_nodes_by_name(const char* name) {\n@@ -1712,1 +1698,1 @@\n-  return find_node_by_name(root, name);\n+  find_nodes_by_name(root, name);\n@@ -1718,1 +1704,1 @@\n-Node* find_node_by_dump(const char* pattern) {\n+void find_nodes_by_dump(const char* pattern) {\n@@ -1720,1 +1706,1 @@\n-  return find_node_by_dump(root, pattern);\n+  find_nodes_by_dump(root, pattern);\n@@ -1726,1 +1712,1 @@\n-Node* find_old_node_by_dump(const char* pattern) {\n+void find_old_nodes_by_dump(const char* pattern) {\n@@ -1728,1 +1714,1 @@\n-  return find_node_by_dump(root, pattern);\n+  find_nodes_by_dump(root, pattern);\n@@ -1781,1 +1767,1 @@\n-  PrintBFS(Node* start, const int max_distance, Node* target, const char* options)\n+  PrintBFS(const Node* start, const int max_distance, const Node* target, const char* options)\n@@ -1798,1 +1784,1 @@\n-  Node* _start;\n+  const Node* _start;\n@@ -1800,1 +1786,1 @@\n-  Node* _target;\n+  const Node* _target;\n@@ -1822,0 +1808,10 @@\n+    \/\/ Check if the filter accepts the node. Go by the type categories, but also all CFG nodes\n+    \/\/ are considered to have control.\n+    bool accepts(const Node* n) {\n+      const Type* t = n->bottom_type();\n+      return ( _data    &&  t->has_category(Type::Category::Data)                    ) ||\n+             ( _memory  &&  t->has_category(Type::Category::Memory)                  ) ||\n+             ( _mixed   &&  t->has_category(Type::Category::Mixed)                   ) ||\n+             ( _control && (t->has_category(Type::Category::Control) || n->is_CFG()) ) ||\n+             ( _other   &&  t->has_category(Type::Category::Other)                   );\n+    }\n@@ -1830,0 +1826,1 @@\n+  bool _dump_only = false;\n@@ -1833,2 +1830,0 @@\n-  \/\/ node category (filter \/ color)\n-  static bool filter_category(Node* n, Filter& filter); \/\/ filter node category against options\n@@ -1848,4 +1843,4 @@\n-  static Node* old_node(Node* n); \/\/ mach node -> prior IR node\n-  static void print_node_idx(Node* n); \/\/ to tty\n-  static void print_block_id(Block* b); \/\/ to tty\n-  static void print_node_block(Node* n); \/\/ to tty: _pre_order, head idx, _idom, _dom_depth\n+  static Node* old_node(const Node* n); \/\/ mach node -> prior IR node\n+  static void print_node_idx(const Node* n); \/\/ to tty\n+  static void print_block_id(const Block* b); \/\/ to tty\n+  static void print_node_block(const Node* n); \/\/ to tty: _pre_order, head idx, _idom, _dom_depth\n@@ -1854,2 +1849,2 @@\n-  Node_List _worklist; \/\/ BFS queue\n-  void maybe_traverse(Node* src, Node* dst);\n+  GrowableArray<const Node*> _worklist; \/\/ BFS queue\n+  void maybe_traverse(const Node* src, const Node* dst);\n@@ -1861,1 +1856,1 @@\n-    Info(Node* node, int distance)\n+    Info(const Node* node, int distance)\n@@ -1863,1 +1858,1 @@\n-    Node* node() { return _node; };\n+    const Node* node() const { return _node; };\n@@ -1867,1 +1862,1 @@\n-    Node_List edge_bwd; \/\/ pointing toward _start\n+    GrowableArray<const Node*> edge_bwd; \/\/ pointing toward _start\n@@ -1871,1 +1866,1 @@\n-    Node* _node;\n+    const Node* _node;\n@@ -1887,1 +1882,1 @@\n-  void make_info(Node* node, const int distance) {\n+  void make_info(const Node* node, const int distance) {\n@@ -1890,1 +1885,1 @@\n-    _info_uid.Insert(node, (void*)uid);\n+    _info_uid.Insert((void*)node, (void*)uid);\n@@ -1896,1 +1891,1 @@\n-  GrowableArray<Node*> _print_list;\n+  GrowableArray<const Node*> _print_list;\n@@ -1900,1 +1895,1 @@\n-  void print_node(Node* n);\n+  void print_node(const Node* n);\n@@ -1925,3 +1920,3 @@\n-  uint pos = 0;\n-  while (pos < _worklist.size()) {\n-    Node* n = _worklist.at(pos++); \/\/ next node to traverse\n+  int pos = 0;\n+  while (pos < _worklist.length()) {\n+    const Node* n = _worklist.at(pos++); \/\/ next node to traverse\n@@ -1929,1 +1924,1 @@\n-    if (!filter_category(n, _filter_visit) && n != _start) {\n+    if (!_filter_visit.accepts(n) && n != _start) {\n@@ -1967,2 +1962,2 @@\n-  for (uint i = 0; i < _worklist.size(); i++) {\n-    Node* n = _worklist.at(i);\n+  for (int i = 0; i < _worklist.length(); i++) {\n+    const Node* n = _worklist.at(i);\n@@ -1976,2 +1971,2 @@\n-  uint pos = 0;\n-  Node_List backtrace;\n+  int pos = 0;\n+  GrowableArray<const Node*> backtrace;\n@@ -1982,2 +1977,2 @@\n-  while (pos < backtrace.size()) {\n-    Node* n = backtrace.at(pos++);\n+  while (pos < backtrace.length()) {\n+    const Node* n = backtrace.at(pos++);\n@@ -1985,1 +1980,1 @@\n-    for (uint i = 0; i < info->edge_bwd.size(); i++) {\n+    for (int i = 0; i < info->edge_bwd.length(); i++) {\n@@ -1987,1 +1982,1 @@\n-      Node* back = info->edge_bwd.at(i);\n+      const Node* back = info->edge_bwd.at(i);\n@@ -2003,1 +1998,1 @@\n-  Node* current = _target;\n+  const Node* current = _target;\n@@ -2019,2 +2014,2 @@\n-    for (int i = _worklist.size() - 1; i >= 0; i--) {\n-      Node* n = _worklist.at(i);\n+    for (int i = _worklist.length() - 1; i >= 0; i--) {\n+      const Node* n = _worklist.at(i);\n@@ -2028,2 +2023,2 @@\n-    for (uint i = 0; i < _worklist.size(); i++) {\n-      Node* n = _worklist.at(i);\n+    for (int i = 0; i < _worklist.length(); i++) {\n+      const Node* n = _worklist.at(i);\n@@ -2046,1 +2041,1 @@\n-      Node* n = _print_list.at(i);\n+      const Node* n = _print_list.at(i);\n@@ -2069,1 +2064,1 @@\n-  tty->print(\"    if nullptr: same as \\\"cdmxo@B\\\"\\n\");\n+  tty->print(\"    if nullptr: same as \\\"cdmox@B\\\"\\n\");\n@@ -2076,2 +2071,1 @@\n-  tty->print(\"      m: visit memory nodes\\n\");\n-  tty->print(\"      x: visit mixed nodes\\n\");\n+  tty->print(\"      m: visit memory nodes\\n\");\n@@ -2080,0 +2074,1 @@\n+  tty->print(\"      x: visit mixed nodes\\n\");\n@@ -2081,2 +2076,1 @@\n-  tty->print(\"      M: boundary memory nodes\\n\");\n-  tty->print(\"      X: boundary mixed nodes\\n\");\n+  tty->print(\"      M: boundary memory nodes\\n\");\n@@ -2085,0 +2079,2 @@\n+  tty->print(\"      X: boundary mixed nodes\\n\");\n+  tty->print(\"      #: display node category in color (not supported in all terminals)\\n\");\n@@ -2087,1 +2083,0 @@\n-  tty->print(\"      #: display node category in color (not supported in all terminals)\\n\");\n@@ -2090,0 +2085,1 @@\n+  tty->print(\"      $: dump only, no header, no other columns\\n\");\n@@ -2093,0 +2089,3 @@\n+  tty->print(\"Note: the categories can be overlapping. For example a mixed node\\n\");\n+  tty->print(\"      can contain control and memory output. Some from the other\\n\");\n+  tty->print(\"      category are also control (Halt, Return, etc).\\n\");\n@@ -2132,1 +2131,1 @@\n-    tty->print(\"  find_node(741)->dump_bfs(8, find_node(746), \\\"cdmxo+A\\\")\\n\");\n+    tty->print(\"  find_node(741)->dump_bfs(8, find_node(746), \\\"cdmox+A\\\")\\n\");\n@@ -2141,1 +2140,1 @@\n-    _options = \"cmdxo@B\"; \/\/ default options\n+    _options = \"cdmox@B\"; \/\/ default options\n@@ -2197,0 +2196,3 @@\n+      case '$':\n+        _dump_only = true;\n+        break;\n@@ -2221,23 +2223,0 @@\n-bool PrintBFS::filter_category(Node* n, Filter& filter) {\n-  const Type* t = n->bottom_type();\n-  switch (t->category()) {\n-    case Type::Category::Data:\n-      return filter._data;\n-    case Type::Category::Memory:\n-      return filter._memory;\n-    case Type::Category::Mixed:\n-      return filter._mixed;\n-    case Type::Category::Control:\n-      return filter._control;\n-    case Type::Category::Other:\n-      return filter._other;\n-    case Type::Category::Undef:\n-      n->dump();\n-      assert(false, \"category undef ??\");\n-    default:\n-      n->dump();\n-      assert(false, \"not covered\");\n-  }\n-  return false;\n-}\n-\n@@ -2288,1 +2267,1 @@\n-Node* PrintBFS::old_node(Node* n) {\n+Node* PrintBFS::old_node(const Node* n) {\n@@ -2297,1 +2276,1 @@\n-void PrintBFS::print_node_idx(Node* n) {\n+void PrintBFS::print_node_idx(const Node* n) {\n@@ -2310,1 +2289,1 @@\n-void PrintBFS::print_block_id(Block* b) {\n+void PrintBFS::print_block_id(const Block* b) {\n@@ -2317,1 +2296,1 @@\n-void PrintBFS::print_node_block(Node* n) {\n+void PrintBFS::print_node_block(const Node* n) {\n@@ -2340,1 +2319,1 @@\n-void PrintBFS::maybe_traverse(Node* src, Node* dst) {\n+void PrintBFS::maybe_traverse(const Node* src, const Node* dst) {\n@@ -2342,2 +2321,2 @@\n-     (filter_category(dst, _filter_visit) ||\n-      filter_category(dst, _filter_boundary) ||\n+     (_filter_visit.accepts(dst) ||\n+      _filter_boundary.accepts(dst) ||\n@@ -2362,0 +2341,3 @@\n+  if (_dump_only) {\n+    return; \/\/ no header in dump only mode\n+  }\n@@ -2376,1 +2358,5 @@\n-void PrintBFS::print_node(Node* n) {\n+void PrintBFS::print_node(const Node* n) {\n+  if (_dump_only) {\n+    n->dump(\"\\n\", false, tty, &_dcc);\n+    return;\n+  }\n@@ -2399,1 +2385,1 @@\n-void Node::dump_bfs(const int max_distance, Node* target, const char* options) {\n+void Node::dump_bfs(const int max_distance, Node* target, const char* options) const {\n@@ -2405,1 +2391,1 @@\n-void Node::dump_bfs(const int max_distance) {\n+void Node::dump_bfs(const int max_distance) const {\n@@ -2532,4 +2518,0 @@\n-  if (_indent > 0) {\n-    st->print(\"%*s\", (_indent << 1), \"  \");\n-  }\n-\n@@ -2612,0 +2594,5 @@\n+\/\/ call from debugger: dump node to tty with newline\n+void Node::dump() const {\n+  dump(\"\\n\");\n+}\n+\n@@ -2663,67 +2650,1 @@\n-\/\/----------------------------collect_nodes_i----------------------------------\n-\/\/ Collects nodes from an Ideal graph, starting from a given start node and\n-\/\/ moving in a given direction until a certain depth (distance from the start\n-\/\/ node) is reached. Duplicates are ignored.\n-\/\/ Arguments:\n-\/\/   queue:         the nodes are collected into this array.\n-\/\/   start:         the node at which to start collecting.\n-\/\/   direction:     if this is a positive number, collect input nodes; if it is\n-\/\/                  a negative number, collect output nodes.\n-\/\/   depth:         collect nodes up to this distance from the start node.\n-\/\/   include_start: whether to include the start node in the result collection.\n-\/\/   only_ctrl:     whether to regard control edges only during traversal.\n-\/\/   only_data:     whether to regard data edges only during traversal.\n-static void collect_nodes_i(GrowableArray<Node*>* queue, const Node* start, int direction, uint depth, bool include_start, bool only_ctrl, bool only_data) {\n-  bool indent = depth <= PrintIdealIndentThreshold;\n-  Node* s = (Node*) start; \/\/ remove const\n-  queue->append(s);\n-  int begin = 0;\n-  int end = 0;\n-\n-  s->set_indent(0);\n-  for(uint i = 0; i < depth; i++) {\n-    end = queue->length();\n-    for(int j = begin; j < end; j++) {\n-      Node* tp  = queue->at(j);\n-      uint limit = direction > 0 ? tp->len() : tp->outcnt();\n-      for(uint k = 0; k < limit; k++) {\n-        Node* n = direction > 0 ? tp->in(k) : tp->raw_out(k);\n-\n-        if (not_a_node(n))  continue;\n-        \/\/ do not recurse through top or the root (would reach unrelated stuff)\n-        if (n->is_Root() || n->is_top()) continue;\n-        if (only_ctrl && !n->is_CFG()) continue;\n-        if (only_data && n->is_CFG()) continue;\n-        bool in_queue = queue->contains(n);\n-        if (!in_queue) {\n-          queue->append(n);\n-          n->set_indent(indent ? (i + 1) : 0);\n-        }\n-      }\n-    }\n-    begin = end;\n-  }\n-  if (!include_start) {\n-    queue->remove(s);\n-  }\n-}\n-\n-\/\/------------------------------dump_nodes-------------------------------------\n-static void dump_nodes(const Node* start, int d, bool only_ctrl) {\n-  if (not_a_node(start)) return;\n-\n-  GrowableArray <Node *> queue(Compile::current()->live_nodes());\n-  collect_nodes_i(&queue, start, d, (uint) ABS(d), true, only_ctrl, false);\n-\n-  int end = queue.length();\n-  if (d > 0) {\n-    for(int j = end-1; j >= 0; j--) {\n-      queue.at(j)->dump();\n-    }\n-  } else {\n-    for(int j = 0; j < end; j++) {\n-      queue.at(j)->dump();\n-    }\n-  }\n-}\n-\n+\/\/ call from debugger: dump Node's inputs (or outputs if d negative)\n@@ -2732,1 +2653,1 @@\n-  dump_nodes(this, d, false);\n+  dump_bfs(abs(d), nullptr, (d > 0) ? \"+$\" : \"-$\");\n@@ -2736,1 +2657,1 @@\n-\/\/ Dump a Node's control history to depth\n+\/\/ call from debugger: dump Node's control inputs (or outputs if d negative)\n@@ -2738,1 +2659,1 @@\n-  dump_nodes(this, d, true);\n+  dump_bfs(abs(d), nullptr, (d > 0) ? \"+$c\" : \"-$c\");\n@@ -2761,195 +2682,0 @@\n-\/\/----------------------------dump_related-------------------------------------\n-\/\/ Dump a Node's related nodes - the notion of \"related\" depends on the Node at\n-\/\/ hand and is determined by the implementation of the virtual method rel.\n-void Node::dump_related() const {\n-  Compile* C = Compile::current();\n-  GrowableArray <Node *> in_rel(C->unique());\n-  GrowableArray <Node *> out_rel(C->unique());\n-  this->related(&in_rel, &out_rel, false);\n-  for (int i = in_rel.length() - 1; i >= 0; i--) {\n-    in_rel.at(i)->dump();\n-  }\n-  this->dump(\"\\n\", true);\n-  for (int i = 0; i < out_rel.length(); i++) {\n-    out_rel.at(i)->dump();\n-  }\n-}\n-\n-\/\/----------------------------dump_related-------------------------------------\n-\/\/ Dump a Node's related nodes up to a given depth (distance from the start\n-\/\/ node).\n-\/\/ Arguments:\n-\/\/   d_in:  depth for input nodes.\n-\/\/   d_out: depth for output nodes (note: this also is a positive number).\n-void Node::dump_related(uint d_in, uint d_out) const {\n-  Compile* C = Compile::current();\n-  GrowableArray <Node *> in_rel(C->unique());\n-  GrowableArray <Node *> out_rel(C->unique());\n-\n-  \/\/ call collect_nodes_i directly\n-  collect_nodes_i(&in_rel, this, 1, d_in, false, false, false);\n-  collect_nodes_i(&out_rel, this, -1, d_out, false, false, false);\n-\n-  for (int i = in_rel.length() - 1; i >= 0; i--) {\n-    in_rel.at(i)->dump();\n-  }\n-  this->dump(\"\\n\", true);\n-  for (int i = 0; i < out_rel.length(); i++) {\n-    out_rel.at(i)->dump();\n-  }\n-}\n-\n-\/\/------------------------dump_related_compact---------------------------------\n-\/\/ Dump a Node's related nodes in compact representation. The notion of\n-\/\/ \"related\" depends on the Node at hand and is determined by the implementation\n-\/\/ of the virtual method rel.\n-void Node::dump_related_compact() const {\n-  Compile* C = Compile::current();\n-  GrowableArray <Node *> in_rel(C->unique());\n-  GrowableArray <Node *> out_rel(C->unique());\n-  this->related(&in_rel, &out_rel, true);\n-  int n_in = in_rel.length();\n-  int n_out = out_rel.length();\n-\n-  this->dump_comp(n_in == 0 ? \"\\n\" : \"  \");\n-  for (int i = 0; i < n_in; i++) {\n-    in_rel.at(i)->dump_comp(i == n_in - 1 ? \"\\n\" : \"  \");\n-  }\n-  for (int i = 0; i < n_out; i++) {\n-    out_rel.at(i)->dump_comp(i == n_out - 1 ? \"\\n\" : \"  \");\n-  }\n-}\n-\n-\/\/------------------------------related----------------------------------------\n-\/\/ Collect a Node's related nodes. The default behaviour just collects the\n-\/\/ inputs and outputs at depth 1, including both control and data flow edges,\n-\/\/ regardless of whether the presentation is compact or not. For data nodes,\n-\/\/ the default is to collect all data inputs (till level 1 if compact), and\n-\/\/ outputs till level 1.\n-void Node::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  if (this->is_CFG()) {\n-    collect_nodes_i(in_rel, this, 1, 1, false, false, false);\n-    collect_nodes_i(out_rel, this, -1, 1, false, false, false);\n-  } else {\n-    if (compact) {\n-      this->collect_nodes(in_rel, 1, false, true);\n-    } else {\n-      this->collect_nodes_in_all_data(in_rel, false);\n-    }\n-    this->collect_nodes(out_rel, -1, false, false);\n-  }\n-}\n-\n-\/\/---------------------------collect_nodes-------------------------------------\n-\/\/ An entry point to the low-level node collection facility, to start from a\n-\/\/ given node in the graph. The start node is by default not included in the\n-\/\/ result.\n-\/\/ Arguments:\n-\/\/   ns:   collect the nodes into this data structure.\n-\/\/   d:    the depth (distance from start node) to which nodes should be\n-\/\/         collected. A value >0 indicates input nodes, a value <0, output\n-\/\/         nodes.\n-\/\/   ctrl: include only control nodes.\n-\/\/   data: include only data nodes.\n-void Node::collect_nodes(GrowableArray<Node*> *ns, int d, bool ctrl, bool data) const {\n-  if (ctrl && data) {\n-    \/\/ ignore nonsensical combination\n-    return;\n-  }\n-  collect_nodes_i(ns, this, d, (uint) ABS(d), false, ctrl, data);\n-}\n-\n-\/\/--------------------------collect_nodes_in-----------------------------------\n-static void collect_nodes_in(Node* start, GrowableArray<Node*> *ns, bool primary_is_data, bool collect_secondary) {\n-  \/\/ The maximum depth is determined using a BFS that visits all primary (data\n-  \/\/ or control) inputs and increments the depth at each level.\n-  uint d_in = 0;\n-  GrowableArray<Node*> nodes(Compile::current()->unique());\n-  nodes.push(start);\n-  int nodes_at_current_level = 1;\n-  int n_idx = 0;\n-  while (nodes_at_current_level > 0) {\n-    \/\/ Add all primary inputs reachable from the current level to the list, and\n-    \/\/ increase the depth if there were any.\n-    int nodes_at_next_level = 0;\n-    bool nodes_added = false;\n-    while (nodes_at_current_level > 0) {\n-      nodes_at_current_level--;\n-      Node* current = nodes.at(n_idx++);\n-      for (uint i = 0; i < current->len(); i++) {\n-        Node* n = current->in(i);\n-        if (not_a_node(n)) {\n-          continue;\n-        }\n-        if ((primary_is_data && n->is_CFG()) || (!primary_is_data && !n->is_CFG())) {\n-          continue;\n-        }\n-        if (!nodes.contains(n)) {\n-          nodes.push(n);\n-          nodes_added = true;\n-          nodes_at_next_level++;\n-        }\n-      }\n-    }\n-    if (nodes_added) {\n-      d_in++;\n-    }\n-    nodes_at_current_level = nodes_at_next_level;\n-  }\n-  start->collect_nodes(ns, d_in, !primary_is_data, primary_is_data);\n-  if (collect_secondary) {\n-    \/\/ Now, iterate over the secondary nodes in ns and add the respective\n-    \/\/ boundary reachable from them.\n-    GrowableArray<Node*> sns(Compile::current()->unique());\n-    for (GrowableArrayIterator<Node*> it = ns->begin(); it != ns->end(); ++it) {\n-      Node* n = *it;\n-      n->collect_nodes(&sns, 1, primary_is_data, !primary_is_data);\n-      for (GrowableArrayIterator<Node*> d = sns.begin(); d != sns.end(); ++d) {\n-        ns->append_if_missing(*d);\n-      }\n-      sns.clear();\n-    }\n-  }\n-}\n-\n-\/\/---------------------collect_nodes_in_all_data-------------------------------\n-\/\/ Collect the entire data input graph. Include the control boundary if\n-\/\/ requested.\n-\/\/ Arguments:\n-\/\/   ns:   collect the nodes into this data structure.\n-\/\/   ctrl: if true, include the control boundary.\n-void Node::collect_nodes_in_all_data(GrowableArray<Node*> *ns, bool ctrl) const {\n-  collect_nodes_in((Node*) this, ns, true, ctrl);\n-}\n-\n-\/\/--------------------------collect_nodes_in_all_ctrl--------------------------\n-\/\/ Collect the entire control input graph. Include the data boundary if\n-\/\/ requested.\n-\/\/   ns:   collect the nodes into this data structure.\n-\/\/   data: if true, include the control boundary.\n-void Node::collect_nodes_in_all_ctrl(GrowableArray<Node*> *ns, bool data) const {\n-  collect_nodes_in((Node*) this, ns, false, data);\n-}\n-\n-\/\/------------------collect_nodes_out_all_ctrl_boundary------------------------\n-\/\/ Collect the entire output graph until hitting control node boundaries, and\n-\/\/ include those.\n-void Node::collect_nodes_out_all_ctrl_boundary(GrowableArray<Node*> *ns) const {\n-  \/\/ Perform a BFS and stop at control nodes.\n-  GrowableArray<Node*> nodes(Compile::current()->unique());\n-  nodes.push((Node*) this);\n-  while (nodes.length() > 0) {\n-    Node* current = nodes.pop();\n-    if (not_a_node(current)) {\n-      continue;\n-    }\n-    ns->append_if_missing(current);\n-    if (!current->is_CFG()) {\n-      for (DUIterator i = current->outs(); current->has_out(i); i++) {\n-        nodes.push(current->out(i));\n-      }\n-    }\n-  }\n-  ns->remove((Node*) this);\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":102,"deletions":376,"binary":false,"changes":478,"status":"modified"},{"patch":"@@ -1210,8 +1210,0 @@\n- private:\n-  int _indent;\n-\n-  void set_indent(int indent) { _indent = indent; }\n-\n- private:\n-  static bool add_to_worklist(Node* n, Node_List* worklist, Arena* old_arena, VectorSet* old_space, VectorSet* new_space);\n-public:\n@@ -1221,2 +1213,2 @@\n-  void dump_bfs(const int max_distance, Node* target, const char* options); \/\/ Print BFS traversal\n-  void dump_bfs(const int max_distance); \/\/ dump_bfs(max_distance, nullptr, nullptr)\n+  void dump_bfs(const int max_distance, Node* target, const char* options) const; \/\/ Print BFS traversal\n+  void dump_bfs(const int max_distance) const; \/\/ dump_bfs(max_distance, nullptr, nullptr)\n@@ -1224,1 +1216,1 @@\n-  public:\n+   public:\n@@ -1231,1 +1223,1 @@\n-  void dump() const { dump(\"\\n\"); }  \/\/ Print this node.\n+  void dump() const; \/\/ print node with newline\n@@ -1238,0 +1230,1 @@\n+ private:\n@@ -1241,0 +1234,1 @@\n+ public:\n@@ -1244,16 +1238,0 @@\n-  void dump_related() const;             \/\/ Print related nodes (depends on node at hand).\n-  \/\/ Print related nodes up to given depths for input and output nodes.\n-  void dump_related(uint d_in, uint d_out) const;\n-  void dump_related_compact() const;     \/\/ Print related nodes in compact representation.\n-  \/\/ Collect related nodes.\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n-  \/\/ Collect nodes starting from this node, explicitly including\/excluding control and data links.\n-  void collect_nodes(GrowableArray<Node*> *ns, int d, bool ctrl, bool data) const;\n-\n-  \/\/ Node collectors, to be used in implementations of Node::rel().\n-  \/\/ Collect the entire data input graph. Include control inputs if requested.\n-  void collect_nodes_in_all_data(GrowableArray<Node*> *ns, bool ctrl) const;\n-  \/\/ Collect the entire control input graph. Include data inputs if requested.\n-  void collect_nodes_in_all_ctrl(GrowableArray<Node*> *ns, bool data) const;\n-  \/\/ Collect the entire output graph until hitting and including control nodes.\n-  void collect_nodes_out_all_ctrl_boundary(GrowableArray<Node*> *ns) const;\n@@ -1268,13 +1246,1 @@\n-  \/\/ RegMask Print Functions\n-  void dump_in_regmask(int idx) { in_RegMask(idx).dump(); }\n-  void dump_out_regmask() { out_RegMask().dump(); }\n-  static bool in_dump() { return Compile::current()->_in_dump_cnt > 0; }\n-  void fast_dump() const {\n-    tty->print(\"%4d: %-17s\", _idx, Name());\n-    for (uint i = 0; i < len(); i++)\n-      if (in(i))\n-        tty->print(\" %4d\", in(i)->_idx);\n-      else\n-        tty->print(\" NULL\");\n-    tty->print(\"\\n\");\n-  }\n+  static bool in_dump() { return Compile::current()->_in_dump_cnt > 0; } \/\/ check if we are in a dump call\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":7,"deletions":41,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/hotspot\/share\/opto\/parse.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -740,31 +740,0 @@\n- const TypeFunc* OptoRuntime::continuation_doYield_Type() {\n-   \/\/ create input type (domain)\n-   const Type **fields = TypeTuple::fields(0);\n-   const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+0, fields);\n-\n-   \/\/ create result type (range)\n-   fields = TypeTuple::fields(1);\n-   fields[TypeFunc::Parms+0] = TypeInt::INT;\n-   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n-\n-   return TypeFunc::make(domain, range);\n- }\n-\n- const TypeFunc* OptoRuntime::continuation_jump_Type() {\n-  \/\/ create input type (domain)\n-  const Type **fields = TypeTuple::fields(6);\n-  fields[TypeFunc::Parms+0] = TypeLong::LONG;\n-  fields[TypeFunc::Parms+1] = Type::HALF;\n-  fields[TypeFunc::Parms+2] = TypeLong::LONG;\n-  fields[TypeFunc::Parms+3] = Type::HALF;\n-  fields[TypeFunc::Parms+4] = TypeLong::LONG;\n-  fields[TypeFunc::Parms+5] = Type::HALF;\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+6, fields);\n-\n-  \/\/ create result type (range)\n-  fields = TypeTuple::fields(0);\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+0, fields);\n-  return TypeFunc::make(domain, range);\n- }\n-\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":0,"deletions":31,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -248,2 +248,0 @@\n-  static const TypeFunc* continuation_doYield_Type();\n-  static const TypeFunc* continuation_jump_Type();\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -633,32 +633,0 @@\n-#ifndef PRODUCT\n-\/\/----------------------------related------------------------------------------\n-\/\/ Related nodes of comparison nodes include all data inputs (until hitting a\n-\/\/ control boundary) as well as all outputs until and including control nodes\n-\/\/ as well as their projections. In compact mode, data inputs till depth 1 and\n-\/\/ all outputs till depth 1 are considered.\n-void CmpNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  if (compact) {\n-    this->collect_nodes(in_rel, 1, false, true);\n-    this->collect_nodes(out_rel, -1, false, false);\n-  } else {\n-    this->collect_nodes_in_all_data(in_rel, false);\n-    this->collect_nodes_out_all_ctrl_boundary(out_rel);\n-    \/\/ Now, find all control nodes in out_rel, and include their projections\n-    \/\/ and projection targets (if any) in the result.\n-    GrowableArray<Node*> proj(Compile::current()->unique());\n-    for (GrowableArrayIterator<Node*> it = out_rel->begin(); it != out_rel->end(); ++it) {\n-      Node* n = *it;\n-      if (n->is_CFG() && !n->is_Proj()) {\n-        \/\/ Assume projections and projection targets are found at levels 1 and 2.\n-        n->collect_nodes(&proj, -2, false, false);\n-        for (GrowableArrayIterator<Node*> p = proj.begin(); p != proj.end(); ++p) {\n-          out_rel->append_if_missing(*p);\n-        }\n-        proj.clear();\n-      }\n-    }\n-  }\n-}\n-\n-#endif\n-\n@@ -1723,0 +1691,1 @@\n+  \/\/ and    x u> 0 or u>= 1   to x != 0\n@@ -1725,1 +1694,1 @@\n-      ((_test._test == BoolTest::lt &&\n+      (((_test._test == BoolTest::lt || _test._test == BoolTest::ge) &&\n@@ -1727,1 +1696,1 @@\n-       (_test._test == BoolTest::le &&\n+       ((_test._test == BoolTest::le || _test._test == BoolTest::gt) &&\n@@ -1730,1 +1699,1 @@\n-    return new BoolNode(ncmp, BoolTest::eq);\n+    return new BoolNode(ncmp, _test.is_less() ? BoolTest::eq : BoolTest::ne);\n@@ -1897,14 +1866,0 @@\n-\n-\/\/-------------------------------related---------------------------------------\n-\/\/ A BoolNode's related nodes are all of its data inputs, and all of its\n-\/\/ outputs until control nodes are hit, which are included. In compact\n-\/\/ representation, inputs till level 3 and immediate outputs are included.\n-void BoolNode::related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const {\n-  if (compact) {\n-    this->collect_nodes(in_rel, 3, false, true);\n-    this->collect_nodes(out_rel, -1, false, false);\n-  } else {\n-    this->collect_nodes_in_all_data(in_rel, false);\n-    this->collect_nodes_out_all_ctrl_boundary(out_rel);\n-  }\n-}\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":4,"deletions":49,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -146,7 +146,0 @@\n-\n-#ifndef PRODUCT\n-  \/\/ CmpNode and subclasses include all data inputs (until hitting a control\n-  \/\/ boundary) in their related node set, as well as all outputs until and\n-  \/\/ including eventual control nodes and their projections.\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n-#endif\n@@ -386,1 +379,0 @@\n-  virtual void related(GrowableArray<Node*> *in_rel, GrowableArray<Node*> *out_rel, bool compact) const;\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1226,0 +1226,15 @@\n+\n+bool Type::has_category(Type::Category cat) const {\n+  if (category() == cat) {\n+    return true;\n+  }\n+  if (category() == Category::Mixed) {\n+    const TypeTuple* tuple = is_tuple();\n+    for (uint i = 0; i < tuple->cnt(); i++) {\n+      if (tuple->field_at(i)->has_category(cat)) {\n+        return true;\n+      }\n+    }\n+  }\n+  return false;\n+}\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -419,0 +419,2 @@\n+  \/\/ Check recursively in tuples.\n+  bool has_category(Category cat) const;\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3353,0 +3353,3 @@\n+  \/\/ PhantomReference has it's own implementation of refersTo().\n+  \/\/ See: JVM_PhantomReferenceRefersTo\n+  assert(!java_lang_ref_Reference::is_phantom(ref_oop), \"precondition\");\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1700,1 +1700,3 @@\n-  assert(env->is_enabled(JVMTI_EVENT_OBJECT_FREE), \"checking\");\n+  if (!env->is_enabled(JVMTI_EVENT_OBJECT_FREE)) {\n+    return; \/\/ the event type has been already disabled\n+  }\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"cds\/archiveHeapLoader.hpp\"\n@@ -30,1 +31,1 @@\n-#include \"cds\/heapShared.inline.hpp\"\n+#include \"cds\/heapShared.hpp\"\n@@ -2085,1 +2086,1 @@\n-  return HeapShared::closed_regions_mapped();\n+  return ArchiveHeapLoader::closed_regions_mapped();\n@@ -2110,1 +2111,1 @@\n-  return HeapShared::open_regions_mapped();\n+  return ArchiveHeapLoader::open_regions_mapped();\n@@ -2480,1 +2481,1 @@\n-  return (jint) SystemDictionary::pd_cache_table()->removed_entries_count();\n+  return (jint) ProtectionDomainCacheTable::removed_entries_count();\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/cds_globals.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1998,1 +1998,1 @@\n-  Events::log_deopt_message(current, \"Uncommon trap: trap_request=\" PTR32_FORMAT \" fr.pc=\" INTPTR_FORMAT \" relative=\" INTPTR_FORMAT,\n+  Events::log_deopt_message(current, \"Uncommon trap: trap_request=\" INT32_FORMAT_X_0 \" fr.pc=\" INTPTR_FORMAT \" relative=\" INTPTR_FORMAT,\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -161,0 +161,1 @@\n+    st->print(\" \");\n@@ -166,2 +167,1 @@\n-      as_int = (jint)obj->byte_field(offset());\n-      st->print(\" %d\", obj->byte_field(offset()));\n+      st->print(\"%d\", obj->byte_field(offset()));\n@@ -170,1 +170,0 @@\n-      as_int = (jint)obj->char_field(offset());\n@@ -173,2 +172,1 @@\n-        as_int = c;\n-        st->print(\" %c %d\", isprint(c) ? c : ' ', c);\n+        st->print(\"%c %d\", isprint(c) ? c : ' ', c);\n@@ -178,1 +176,1 @@\n-      st->print(\" %lf\", obj->double_field(offset()));\n+      st->print(\"%lf\", obj->double_field(offset()));\n@@ -181,2 +179,1 @@\n-      as_int = obj->int_field(offset());\n-      st->print(\" %f\", obj->float_field(offset()));\n+      st->print(\"%f\", obj->float_field(offset()));\n@@ -185,2 +182,1 @@\n-      as_int = obj->int_field(offset());\n-      st->print(\" %d\", obj->int_field(offset()));\n+      st->print(\"%d\", obj->int_field(offset()));\n@@ -189,1 +185,0 @@\n-      st->print(\" \");\n@@ -193,2 +188,1 @@\n-      as_int = obj->short_field(offset());\n-      st->print(\" %d\", obj->short_field(offset()));\n+      st->print(\"%d\", obj->short_field(offset()));\n@@ -197,2 +191,1 @@\n-      as_int = obj->bool_field(offset());\n-      st->print(\" %s\", obj->bool_field(offset()) ? \"true\" : \"false\");\n+      st->print(\"%s\", obj->bool_field(offset()) ? \"true\" : \"false\");\n@@ -214,2 +207,0 @@\n-      st->print(\" \");\n-      NOT_LP64(as_int = obj->int_field(offset()));\n@@ -226,2 +217,3 @@\n-  \/\/ Print a hint as to the underlying integer representation. This can be wrong for\n-  \/\/ pointers on an LP64 machine\n+\n+  \/\/ Print a hint as to the underlying integer representation.\n+  if (is_reference_type(ft)) {\n@@ -229,4 +221,7 @@\n-  if (is_reference_type(ft) && UseCompressedOops) {\n-    st->print(\" (%x)\", obj->int_field(offset()));\n-  }\n-  else \/\/ <- intended\n+    if (UseCompressedOops) {\n+      st->print(\" (\" INT32_FORMAT_X_0 \")\", obj->int_field(offset()));\n+    } else {\n+      st->print(\" (\" INT64_FORMAT_X_0 \")\", (int64_t)obj->long_field(offset()));\n+    }\n+#else\n+    st->print(\" (\" INT32_FORMAT_X_0 \")\", obj->int_field(offset()));\n@@ -234,4 +229,14 @@\n-  if (ft == T_LONG || ft == T_DOUBLE LP64_ONLY(|| !is_java_primitive(ft)) ) {\n-    st->print(\" (%x %x)\", obj->int_field(offset()), obj->int_field(offset()+sizeof(jint)));\n-  } else if (as_int < 0 || as_int > 9) {\n-    st->print(\" (%x)\", as_int);\n+  } else { \/\/ Primitives\n+    switch (ft) {\n+      case T_LONG:    st->print(\" (\" INT64_FORMAT_X_0 \")\", (int64_t)obj->long_field(offset())); break;\n+      case T_DOUBLE:  st->print(\" (\" INT64_FORMAT_X_0 \")\", (int64_t)obj->long_field(offset())); break;\n+      case T_BYTE:    st->print(\" (\" INT8_FORMAT_X_0  \")\", obj->byte_field(offset()));          break;\n+      case T_CHAR:    st->print(\" (\" INT16_FORMAT_X_0 \")\", obj->char_field(offset()));          break;\n+      case T_FLOAT:   st->print(\" (\" INT32_FORMAT_X_0 \")\", obj->int_field(offset()));           break;\n+      case T_INT:     st->print(\" (\" INT32_FORMAT_X_0 \")\", obj->int_field(offset()));           break;\n+      case T_SHORT:   st->print(\" (\" INT16_FORMAT_X_0 \")\", obj->short_field(offset()));         break;\n+      case T_BOOLEAN: st->print(\" (\" INT8_FORMAT_X_0  \")\", obj->bool_field(offset()));          break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+    }\n@@ -241,1 +246,0 @@\n-\n","filename":"src\/hotspot\/share\/runtime\/fieldDescriptor.cpp","additions":32,"deletions":28,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -1803,33 +1803,0 @@\n-  \/* Shared spaces *\/                                                       \\\n-                                                                            \\\n-  product(bool, VerifySharedSpaces, false,                                  \\\n-          \"Verify integrity of shared spaces\")                              \\\n-                                                                            \\\n-  product(bool, RecordDynamicDumpInfo, false,                               \\\n-          \"Record class info for jcmd VM.cds dynamic_dump\")                 \\\n-                                                                            \\\n-  product(bool, AutoCreateSharedArchive, false,                             \\\n-          \"Create shared archive at exit if cds mapping failed\")            \\\n-                                                                            \\\n-  product(bool, PrintSharedArchiveAndExit, false,                           \\\n-          \"Print shared archive file contents\")                             \\\n-                                                                            \\\n-  product(bool, PrintSharedDictionary, false,                               \\\n-          \"If PrintSharedArchiveAndExit is true, also print the shared \"    \\\n-          \"dictionary\")                                                     \\\n-                                                                            \\\n-  product(size_t, SharedBaseAddress, LP64_ONLY(32*G)                        \\\n-          NOT_LP64(LINUX_ONLY(2*G) NOT_LINUX(0)),                           \\\n-          \"Address to allocate shared memory region for class data\")        \\\n-          range(0, SIZE_MAX)                                                \\\n-                                                                            \\\n-  product(ccstr, SharedArchiveConfigFile, NULL,                             \\\n-          \"Data to add to the CDS archive file\")                            \\\n-                                                                            \\\n-  product(uint, SharedSymbolTableBucketSize, 4,                             \\\n-          \"Average number of symbols per bucket in shared table\")           \\\n-          range(2, 246)                                                     \\\n-                                                                            \\\n-  product(bool, AllowArchivingWithJavaAgent, false, DIAGNOSTIC,             \\\n-          \"Allow Java agent to be run with CDS dumping\")                    \\\n-                                                                            \\\n@@ -1939,24 +1906,0 @@\n-  product(ccstr, DumpLoadedClassList, NULL,                                 \\\n-          \"Dump the names all loaded classes, that could be stored into \"   \\\n-          \"the CDS archive, in the specified file\")                         \\\n-                                                                            \\\n-  product(ccstr, SharedClassListFile, NULL,                                 \\\n-          \"Override the default CDS class list\")                            \\\n-                                                                            \\\n-  product(ccstr, SharedArchiveFile, NULL,                                   \\\n-          \"Override the default location of the CDS archive file\")          \\\n-                                                                            \\\n-  product(ccstr, ArchiveClassesAtExit, NULL,                                \\\n-          \"The path and name of the dynamic archive file\")                  \\\n-                                                                            \\\n-  product(ccstr, ExtraSharedClassListFile, NULL,                            \\\n-          \"Extra classlist for building the CDS archive file\")              \\\n-                                                                            \\\n-  product(int, ArchiveRelocationMode, 0, DIAGNOSTIC,                        \\\n-           \"(0) first map at preferred address, and if \"                    \\\n-           \"unsuccessful, map at alternative address (default); \"           \\\n-           \"(1) always map at alternative address; \"                        \\\n-           \"(2) always map at preferred address, and if unsuccessful, \"     \\\n-           \"do not map the archive\")                                        \\\n-           range(0, 2)                                                      \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":0,"deletions":57,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -574,1 +574,1 @@\n-          \"class %s (in module %s) cannot access class %s (in unnamed module @\" SIZE_FORMAT_HEX \") because module %s does not read unnamed module @\" SIZE_FORMAT_HEX,\n+          \"class %s (in module %s) cannot access class %s (in unnamed module @\" SIZE_FORMAT_X \") because module %s does not read unnamed module @\" SIZE_FORMAT_X,\n@@ -601,1 +601,1 @@\n-          \"class %s (in unnamed module @\" SIZE_FORMAT_HEX \") cannot access class %s (in module %s) because module %s does not export %s to unnamed module @\" SIZE_FORMAT_HEX,\n+          \"class %s (in unnamed module @\" SIZE_FORMAT_X \") cannot access class %s (in module %s) because module %s does not export %s to unnamed module @\" SIZE_FORMAT_X,\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -104,0 +104,1 @@\n+nmethod*            SharedRuntime::_cont_doYield_stub;\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -77,0 +77,2 @@\n+  static nmethod*            _cont_doYield_stub;\n+\n@@ -253,0 +255,5 @@\n+  static nmethod* cont_doYield_stub() {\n+    assert(_cont_doYield_stub != nullptr, \"oops\");\n+    return _cont_doYield_stub;\n+  }\n+\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -179,2 +179,0 @@\n-RuntimeStub* StubRoutines::_cont_doYield_stub = NULL;\n-address StubRoutines::_cont_doYield       = NULL;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -254,2 +254,0 @@\n-  static RuntimeStub* _cont_doYield_stub;\n-  static address _cont_doYield;\n@@ -436,2 +434,0 @@\n-  static RuntimeStub* cont_doYield_stub() { return _cont_doYield_stub; }\n-  static address cont_doYield()        { return _cont_doYield; }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"cds\/cds_globals.hpp\"\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -93,1 +93,17 @@\n-#define BOOL_TO_STR(_b_) ((_b_) ? \"true\" : \"false\")\n+\/\/ Guide to the suffixes used in the format specifiers for integers:\n+\/\/        - print the decimal value:                   745565\n+\/\/  _X    - print as hexadecimal, without leading 0s: 0x12345\n+\/\/  _X_0  - print as hexadecimal, with leading 0s: 0x00012345\n+\/\/  _W(w) - prints w sized string with the given value right\n+\/\/          adjusted. Use -w to print left adjusted.\n+\/\/\n+\/\/ Note that the PTR format specifiers print using 0x with leading zeros,\n+\/\/ just like the _X_0 version for integers.\n+\n+\/\/ Format 8-bit quantities.\n+#define INT8_FORMAT_X_0          \"0x%02\"      PRIx8\n+#define UINT8_FORMAT_X_0         \"0x%02\"      PRIx8\n+\n+\/\/ Format 16-bit quantities.\n+#define INT16_FORMAT_X_0         \"0x%04\"      PRIx16\n+#define UINT16_FORMAT_X_0        \"0x%04\"      PRIx16\n@@ -96,7 +112,6 @@\n-#define INT32_FORMAT           \"%\" PRId32\n-#define UINT32_FORMAT          \"%\" PRIu32\n-#define INT32_FORMAT_W(width)  \"%\" #width PRId32\n-#define UINT32_FORMAT_W(width) \"%\" #width PRIu32\n-\n-#define PTR32_FORMAT           \"0x%08\" PRIx32\n-#define PTR32_FORMAT_W(width)  \"0x%\" #width PRIx32\n+#define INT32_FORMAT             \"%\"          PRId32\n+#define INT32_FORMAT_X_0         \"0x%08\"      PRIx32\n+#define INT32_FORMAT_W(width)    \"%\"   #width PRId32\n+#define UINT32_FORMAT            \"%\"          PRIu32\n+#define UINT32_FORMAT_X_0        \"0x%08\"      PRIx32\n+#define UINT32_FORMAT_W(width)   \"%\"   #width PRIu32\n@@ -105,6 +120,15 @@\n-#define INT64_FORMAT           \"%\" PRId64\n-#define UINT64_FORMAT          \"%\" PRIu64\n-#define UINT64_FORMAT_X        \"%\" PRIx64\n-#define INT64_FORMAT_W(width)  \"%\" #width PRId64\n-#define UINT64_FORMAT_W(width) \"%\" #width PRIu64\n-#define UINT64_FORMAT_X_W(width) \"%\" #width PRIx64\n+#define INT64_FORMAT             \"%\"          PRId64\n+#define INT64_FORMAT_X           \"0x%\"        PRIx64\n+#define INT64_FORMAT_X_0         \"0x%016\"     PRIx64\n+#define INT64_FORMAT_W(width)    \"%\"   #width PRId64\n+#define UINT64_FORMAT            \"%\"          PRIu64\n+#define UINT64_FORMAT_X          \"0x%\"        PRIx64\n+#define UINT64_FORMAT_X_0        \"0x%016\"     PRIx64\n+#define UINT64_FORMAT_W(width)   \"%\"   #width PRIu64\n+\n+\/\/ Format integers which change size between 32- and 64-bit.\n+#define SSIZE_FORMAT             \"%\"          PRIdPTR\n+#define SSIZE_FORMAT_W(width)    \"%\"   #width PRIdPTR\n+#define SIZE_FORMAT              \"%\"          PRIuPTR\n+#define SIZE_FORMAT_X            \"0x%\"        PRIxPTR\n+#define SIZE_FORMAT_W(width)     \"%\"   #width PRIuPTR\n@@ -112,1 +136,6 @@\n-#define PTR64_FORMAT           \"0x%016\" PRIx64\n+#define INTX_FORMAT              \"%\"          PRIdPTR\n+#define INTX_FORMAT_X            \"0x%\"        PRIxPTR\n+#define INTX_FORMAT_W(width)     \"%\"   #width PRIdPTR\n+#define UINTX_FORMAT             \"%\"          PRIuPTR\n+#define UINTX_FORMAT_X           \"0x%\"        PRIxPTR\n+#define UINTX_FORMAT_W(width)    \"%\"   #width PRIuPTR\n@@ -116,1 +145,1 @@\n-#define JLONG_FORMAT           INT64_FORMAT\n+#define JLONG_FORMAT             INT64_FORMAT\n@@ -119,1 +148,1 @@\n-#define JLONG_FORMAT_W(width)  INT64_FORMAT_W(width)\n+#define JLONG_FORMAT_W(width)    INT64_FORMAT_W(width)\n@@ -122,1 +151,1 @@\n-#define JULONG_FORMAT          UINT64_FORMAT\n+#define JULONG_FORMAT            UINT64_FORMAT\n@@ -125,1 +154,1 @@\n-#define JULONG_FORMAT_X        UINT64_FORMAT_X\n+#define JULONG_FORMAT_X          UINT64_FORMAT_X\n@@ -130,2 +159,2 @@\n-#define INTPTR_FORMAT \"0x%016\" PRIxPTR\n-#define PTR_FORMAT    \"0x%016\" PRIxPTR\n+#define INTPTR_FORMAT            \"0x%016\"     PRIxPTR\n+#define PTR_FORMAT               \"0x%016\"     PRIxPTR\n@@ -133,2 +162,2 @@\n-#define INTPTR_FORMAT \"0x%08\"  PRIxPTR\n-#define PTR_FORMAT    \"0x%08\"  PRIxPTR\n+#define INTPTR_FORMAT            \"0x%08\"      PRIxPTR\n+#define PTR_FORMAT               \"0x%08\"      PRIxPTR\n@@ -137,17 +166,0 @@\n-\/\/ Format pointers without leading zeros\n-#define INTPTRNZ_FORMAT \"0x%\"  PRIxPTR\n-\n-#define INTPTR_FORMAT_W(width)   \"%\" #width PRIxPTR\n-\n-#define SSIZE_FORMAT             \"%\"   PRIdPTR\n-#define SIZE_FORMAT              \"%\"   PRIuPTR\n-#define SIZE_FORMAT_HEX          \"0x%\" PRIxPTR\n-#define SSIZE_FORMAT_W(width)    \"%\"   #width PRIdPTR\n-#define SIZE_FORMAT_W(width)     \"%\"   #width PRIuPTR\n-#define SIZE_FORMAT_HEX_W(width) \"0x%\" #width PRIxPTR\n-\n-#define INTX_FORMAT           \"%\" PRIdPTR\n-#define UINTX_FORMAT          \"%\" PRIuPTR\n-#define INTX_FORMAT_W(width)  \"%\" #width PRIdPTR\n-#define UINTX_FORMAT_W(width) \"%\" #width PRIuPTR\n-\n@@ -159,0 +171,1 @@\n+#define BOOL_TO_STR(_b_) ((_b_) ? \"true\" : \"false\")\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":53,"deletions":40,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -166,1 +166,1 @@\n-    public static final Source MIN = Source.JDK7;\n+    public static final Source MIN = Source.JDK8;\n@@ -210,1 +210,1 @@\n-        DIAMOND(JDK7, Fragments.FeatureDiamond, DiagKind.NORMAL),\n+        DIAMOND(MIN, Fragments.FeatureDiamond, DiagKind.NORMAL), \/\/ Used in Analyzer\n@@ -216,12 +216,1 @@\n-        METHOD_REFERENCES(JDK8, Fragments.FeatureMethodReferences, DiagKind.PLURAL),\n-        STATIC_INTERFACE_METHODS(JDK8, Fragments.FeatureStaticIntfMethods, DiagKind.PLURAL),\n-        STATIC_INTERFACE_METHODS_INVOKE(JDK8, Fragments.FeatureStaticIntfMethodInvoke, DiagKind.PLURAL),\n-        EFFECTIVELY_FINAL_IN_INNER_CLASSES(JDK8),\n-        TYPE_ANNOTATIONS(JDK8, Fragments.FeatureTypeAnnotations, DiagKind.PLURAL),\n-        ANNOTATIONS_AFTER_TYPE_PARAMS(JDK8, Fragments.FeatureAnnotationsAfterTypeParams, DiagKind.PLURAL),\n-        REPEATED_ANNOTATIONS(JDK8, Fragments.FeatureRepeatableAnnotations, DiagKind.PLURAL),\n-        INTERSECTION_TYPES_IN_CAST(JDK8, Fragments.FeatureIntersectionTypesInCast, DiagKind.PLURAL),\n-        GRAPH_INFERENCE(JDK8),\n-        FUNCTIONAL_INTERFACE_MOST_SPECIFIC(JDK8),\n-        POST_APPLICABILITY_VARARGS_ACCESS_CHECK(JDK8),\n-        MAP_CAPTURES_TO_BOUNDS(MIN, JDK7),\n+        GRAPH_INFERENCE(JDK8), \/\/ Used in Analyzer\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Source.java","additions":3,"deletions":14,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -2419,15 +2419,0 @@\n-            \/\/ Per JDK-8075793: in pre-8 sources, follow legacy javac behavior\n-            \/\/ when capture variables are inferred as bounds: for lower bounds,\n-            \/\/ map to the capture variable's upper bound; for upper bounds,\n-            \/\/ if the capture variable has a lower bound, map to that type\n-            if (types.mapCapturesToBounds) {\n-                switch (ib) {\n-                    case LOWER:\n-                        bound = types.cvarUpperBound(bound);\n-                        break;\n-                    case UPPER:\n-                        Type altBound = types.cvarLowerBound(bound);\n-                        if (!altBound.hasTag(TypeTag.BOT)) bound = altBound;\n-                        break;\n-                }\n-            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Type.java","additions":1,"deletions":16,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -94,2 +94,0 @@\n-    final boolean allowDefaultMethods;\n-    final boolean mapCapturesToBounds;\n@@ -118,2 +116,0 @@\n-        allowDefaultMethods = Feature.DEFAULT_METHODS.allowedInSource(source);\n-        mapCapturesToBounds = Feature.MAP_CAPTURES_TO_BOUNDS.allowedInSource(source);\n@@ -3258,5 +3254,3 @@\n-                            if (allowDefaultMethods) {\n-                                MethodSymbol prov = interfaceCandidates(impl.type, absmeth).head;\n-                                if (prov != null && prov.overrides(absmeth, impl, this, true)) {\n-                                    implmeth = prov;\n-                                }\n+                            MethodSymbol prov = interfaceCandidates(impl.type, absmeth).head;\n+                            if (prov != null && prov.overrides(absmeth, impl, this, true)) {\n+                                implmeth = prov;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Types.java","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -171,3 +171,0 @@\n-        allowTypeAnnos = Feature.TYPE_ANNOTATIONS.allowedInSource(source);\n-        allowDefaultMethods = Feature.DEFAULT_METHODS.allowedInSource(source);\n-        allowStaticInterfaceMethods = Feature.STATIC_INTERFACE_METHODS.allowedInSource(source);\n@@ -198,4 +195,0 @@\n-    \/** Switch: support type annotations.\n-     *\/\n-    boolean allowTypeAnnos;\n-\n@@ -206,4 +199,0 @@\n-    \/** Switch: support default methods ?\n-     *\/\n-    boolean allowDefaultMethods;\n-\n@@ -214,4 +203,0 @@\n-    \/** Switch: static interface methods enabled?\n-     *\/\n-    boolean allowStaticInterfaceMethods;\n-\n@@ -1739,2 +1724,0 @@\n-                            } else if (wasUnconditionalPattern) {\n-                                log.error(label.pos(), Errors.PatternDominated);\n@@ -4515,4 +4498,0 @@\n-            if (!allowStaticInterfaceMethods && sitesym.isInterface() &&\n-                    sym.isStatic() && sym.kind == MTH) {\n-                log.error(DiagnosticFlag.SOURCE_LEVEL, tree.pos(), Feature.STATIC_INTERFACE_METHODS_INVOKE.error(sourceName));\n-            }\n@@ -5672,3 +5651,1 @@\n-            if (allowDefaultMethods) {\n-                chk.checkDefaultMethodClashes(tree.pos(), c.type);\n-            }\n+            chk.checkDefaultMethodClashes(tree.pos(), c.type);\n@@ -5730,3 +5707,2 @@\n-        if (allowTypeAnnos) {\n-            \/\/ Correctly organize the positions of the type annotations\n-            typeAnnotations.organizeTypeAnnotationsBodies(tree);\n+        \/\/ Correctly organize the positions of the type annotations\n+        typeAnnotations.organizeTypeAnnotationsBodies(tree);\n@@ -5734,3 +5710,2 @@\n-            \/\/ Check type annotations applicability rules\n-            validateTypeAnnotations(tree, false);\n-        }\n+        \/\/ Check type annotations applicability rules\n+        validateTypeAnnotations(tree, false);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":5,"deletions":30,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -3008,1 +3008,0 @@\n-                Feature.DEFAULT_METHODS.allowedInSource(source) &&\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -215,1 +215,0 @@\n-    private final boolean allowEffectivelyFinalInInnerClasses;\n@@ -340,1 +339,0 @@\n-        allowEffectivelyFinalInInnerClasses = Feature.EFFECTIVELY_FINAL_IN_INNER_CLASSES.allowedInSource(source);\n@@ -3143,6 +3141,0 @@\n-                        if (!allowEffectivelyFinalInInnerClasses) {\n-                            if ((sym.flags() & FINAL) == 0) {\n-                                reportInnerClsNeedsFinalError(pos, sym);\n-                            }\n-                            break;\n-                        }\n@@ -3169,4 +3161,0 @@\n-                            if (!allowEffectivelyFinalInInnerClasses) {\n-                                reportInnerClsNeedsFinalError(tree, sym);\n-                                break;\n-                            }\n@@ -3191,5 +3179,0 @@\n-        void reportInnerClsNeedsFinalError(DiagnosticPosition pos, Symbol sym) {\n-            log.error(pos,\n-                      Errors.LocalVarAccessedFromIclsNeedsFinal(sym));\n-        }\n-\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":0,"deletions":17,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -107,1 +107,0 @@\n-    public final boolean allowFunctionalInterfaceMostSpecific;\n@@ -110,1 +109,0 @@\n-    public final boolean checkVarargsAccessAfterResolution;\n@@ -146,1 +144,0 @@\n-        allowFunctionalInterfaceMostSpecific = Feature.FUNCTIONAL_INTERFACE_MOST_SPECIFIC.allowedInSource(source);\n@@ -149,2 +146,0 @@\n-        checkVarargsAccessAfterResolution =\n-                Feature.POST_APPLICABILITY_VARARGS_ACCESS_CHECK.allowedInSource(source);\n@@ -938,1 +933,1 @@\n-                if (deferredAttrContext.mode == AttrMode.CHECK || !checkVarargsAccessAfterResolution) {\n+                if (deferredAttrContext.mode == AttrMode.CHECK) {\n@@ -1193,3 +1188,2 @@\n-                if (allowFunctionalInterfaceMostSpecific &&\n-                        unrelatedFunctionalInterfaces(found, req) &&\n-                        (actual != null && actual.getTag() == DEFERRED)) {\n+                if (unrelatedFunctionalInterfaces(found, req) &&\n+                    (actual != null && actual.getTag() == DEFERRED)) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -82,3 +82,0 @@\n-    \/** Switch: are default methods supported? *\/\n-    private final boolean allowInterfaceBridges;\n-\n@@ -96,1 +93,0 @@\n-        allowInterfaceBridges = Feature.DEFAULT_METHODS.allowedInSource(source);\n@@ -981,3 +977,1 @@\n-                if (allowInterfaceBridges || (tree.sym.flags() & INTERFACE) == 0) {\n-                    addBridges(tree.pos(), c, bridges);\n-                }\n+                addBridges(tree.pos(), c, bridges);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TransTypes.java","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -147,1 +147,0 @@\n-        allowTypeAnnos = Feature.TYPE_ANNOTATIONS.allowedInSource(source);\n@@ -151,4 +150,0 @@\n-    \/** Switch: support type annotations.\n-     *\/\n-    boolean allowTypeAnnos;\n-\n@@ -1070,4 +1065,2 @@\n-            if (allowTypeAnnos) {\n-                typeAnnotations.organizeTypeAnnotationsSignatures(env, (JCClassDecl)env.tree);\n-                typeAnnotations.validateTypeAnnotationsSignatures(env, (JCClassDecl)env.tree);\n-            }\n+            typeAnnotations.organizeTypeAnnotationsSignatures(env, (JCClassDecl)env.tree);\n+            typeAnnotations.validateTypeAnnotationsSignatures(env, (JCClassDecl)env.tree);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TypeEnter.java","additions":2,"deletions":9,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -620,2 +620,0 @@\n-                \/\/ Make sure we're using a supported source version.\n-                checkSourceLevel(Feature.TYPE_ANNOTATIONS);\n@@ -1053,1 +1051,0 @@\n-                            checkSourceLevel(mods.annotations.head.pos, Feature.TYPE_ANNOTATIONS);\n@@ -1256,1 +1253,0 @@\n-                           checkSourceLevel(Feature.INTERSECTION_TYPES_IN_CAST);\n@@ -2369,1 +2365,0 @@\n-        checkSourceLevel(Feature.METHOD_REFERENCES);\n@@ -2393,3 +2388,0 @@\n-        if (!newAnnotations.isEmpty()) {\n-            checkSourceLevel(newAnnotations.head.pos, Feature.TYPE_ANNOTATIONS);\n-        }\n@@ -3454,3 +3446,0 @@\n-        if (kind == Tag.TYPE_ANNOTATION) {\n-            checkSourceLevel(Feature.TYPE_ANNOTATIONS);\n-        }\n@@ -4470,1 +4459,0 @@\n-                    checkSourceLevel(annosAfterParams.head.pos, Feature.ANNOTATIONS_AFTER_TYPE_PARAMS);\n@@ -4714,3 +4702,0 @@\n-            if ((mods.flags & Flags.STATIC) != 0) {\n-                checkSourceLevel(Feature.STATIC_INTERFACE_METHODS);\n-            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -772,4 +772,0 @@\n-# 0: symbol\n-compiler.err.local.var.accessed.from.icls.needs.final=\\\n-    local variable {0} is accessed from within inner class; needs to be declared final\n-\n@@ -2047,5 +2043,0 @@\n-\n-# 0: target, 1: target\n-compiler.warn.option.parameters.unsupported=\\\n-    -parameters is not supported for target value {0}. Use {1} or later.\n-\n@@ -3042,9 +3033,0 @@\n-compiler.misc.feature.type.annotations=\\\n-    type annotations\n-\n-compiler.misc.feature.annotations.after.type.params=\\\n-    annotations after method type parameters\n-\n-compiler.misc.feature.repeatable.annotations=\\\n-    repeated annotations\n-\n@@ -3057,3 +3039,0 @@\n-compiler.misc.feature.method.references=\\\n-    method references\n-\n@@ -3063,9 +3042,0 @@\n-compiler.misc.feature.intersection.types.in.cast=\\\n-    intersection types\n-\n-compiler.misc.feature.static.intf.methods=\\\n-    static interface methods\n-\n-compiler.misc.feature.static.intf.method.invoke=\\\n-    static interface method invocations\n-\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":0,"deletions":30,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+gc\/metaspace\/TestMetaspacePerfCounters.java#Epsilon-64 8293503 macosx-all,windows-x64\n@@ -183,1 +184,0 @@\n-vmTestbase\/gc\/lock\/jni\/jnilock001\/TestDescription.java 8292946 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,1 +43,8 @@\n-    static final int WEAK_ATTEMPTS = Integer.getInteger(\"weakAttempts\", 10);\n+\n+    \/\/ More resilience for Weak* tests. These operations may spuriously\n+    \/\/ fail, and so we do several attempts with delay on failure.\n+    \/\/ Be mindful of worst-case total time on test, which would be at\n+    \/\/ roughly (delay*attempts) milliseconds.\n+    \/\/\n+    static final int WEAK_ATTEMPTS = Integer.getInteger(\"weakAttempts\", 100);\n+    static final int WEAK_DELAY_MS = Math.max(1, Integer.getInteger(\"weakDelay\", 1));\n@@ -509,0 +516,10 @@\n+\n+    static void weakDelay() {\n+        try {\n+            if (WEAK_DELAY_MS > 0) {\n+                Thread.sleep(WEAK_DELAY_MS);\n+            }\n+        } catch (InterruptedException ie) {\n+            \/\/ Do nothing.\n+        }\n+    }\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleBaseTest.java","additions":19,"deletions":2,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -534,0 +534,1 @@\n+                if (!success) weakDelay();\n@@ -551,0 +552,1 @@\n+                if (!success) weakDelay();\n@@ -568,0 +570,1 @@\n+                if (!success) weakDelay();\n@@ -585,0 +588,1 @@\n+                if (!success) weakDelay();\n@@ -822,0 +826,1 @@\n+                if (!success) weakDelay();\n@@ -839,0 +844,1 @@\n+                if (!success) weakDelay();\n@@ -856,0 +862,1 @@\n+                if (!success) weakDelay();\n@@ -873,0 +880,1 @@\n+                if (!success) weakDelay();\n@@ -1113,0 +1121,1 @@\n+                    if (!success) weakDelay();\n@@ -1130,0 +1139,1 @@\n+                    if (!success) weakDelay();\n@@ -1147,0 +1157,1 @@\n+                    if (!success) weakDelay();\n@@ -1164,0 +1175,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessBoolean.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -512,0 +512,1 @@\n+                if (!success) weakDelay();\n@@ -529,0 +530,1 @@\n+                if (!success) weakDelay();\n@@ -546,0 +548,1 @@\n+                if (!success) weakDelay();\n@@ -563,0 +566,1 @@\n+                if (!success) weakDelay();\n@@ -816,0 +820,1 @@\n+                if (!success) weakDelay();\n@@ -833,0 +838,1 @@\n+                if (!success) weakDelay();\n@@ -850,0 +856,1 @@\n+                if (!success) weakDelay();\n@@ -867,0 +874,1 @@\n+                if (!success) weakDelay();\n@@ -1123,0 +1131,1 @@\n+                    if (!success) weakDelay();\n@@ -1140,0 +1149,1 @@\n+                    if (!success) weakDelay();\n@@ -1157,0 +1167,1 @@\n+                    if (!success) weakDelay();\n@@ -1174,0 +1185,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessByte.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -512,0 +512,1 @@\n+                if (!success) weakDelay();\n@@ -529,0 +530,1 @@\n+                if (!success) weakDelay();\n@@ -546,0 +548,1 @@\n+                if (!success) weakDelay();\n@@ -563,0 +566,1 @@\n+                if (!success) weakDelay();\n@@ -816,0 +820,1 @@\n+                if (!success) weakDelay();\n@@ -833,0 +838,1 @@\n+                if (!success) weakDelay();\n@@ -850,0 +856,1 @@\n+                if (!success) weakDelay();\n@@ -867,0 +874,1 @@\n+                if (!success) weakDelay();\n@@ -1123,0 +1131,1 @@\n+                    if (!success) weakDelay();\n@@ -1140,0 +1149,1 @@\n+                    if (!success) weakDelay();\n@@ -1157,0 +1167,1 @@\n+                    if (!success) weakDelay();\n@@ -1174,0 +1185,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessChar.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -582,0 +582,1 @@\n+                if (!success) weakDelay();\n@@ -599,0 +600,1 @@\n+                if (!success) weakDelay();\n@@ -616,0 +618,1 @@\n+                if (!success) weakDelay();\n@@ -633,0 +636,1 @@\n+                if (!success) weakDelay();\n@@ -838,0 +842,1 @@\n+                if (!success) weakDelay();\n@@ -855,0 +860,1 @@\n+                if (!success) weakDelay();\n@@ -872,0 +878,1 @@\n+                if (!success) weakDelay();\n@@ -889,0 +896,1 @@\n+                if (!success) weakDelay();\n@@ -1097,0 +1105,1 @@\n+                    if (!success) weakDelay();\n@@ -1114,0 +1123,1 @@\n+                    if (!success) weakDelay();\n@@ -1131,0 +1141,1 @@\n+                    if (!success) weakDelay();\n@@ -1148,0 +1159,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessDouble.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -582,0 +582,1 @@\n+                if (!success) weakDelay();\n@@ -599,0 +600,1 @@\n+                if (!success) weakDelay();\n@@ -616,0 +618,1 @@\n+                if (!success) weakDelay();\n@@ -633,0 +636,1 @@\n+                if (!success) weakDelay();\n@@ -838,0 +842,1 @@\n+                if (!success) weakDelay();\n@@ -855,0 +860,1 @@\n+                if (!success) weakDelay();\n@@ -872,0 +878,1 @@\n+                if (!success) weakDelay();\n@@ -889,0 +896,1 @@\n+                if (!success) weakDelay();\n@@ -1097,0 +1105,1 @@\n+                    if (!success) weakDelay();\n@@ -1114,0 +1123,1 @@\n+                    if (!success) weakDelay();\n@@ -1131,0 +1141,1 @@\n+                    if (!success) weakDelay();\n@@ -1148,0 +1159,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessFloat.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -512,0 +512,1 @@\n+                if (!success) weakDelay();\n@@ -529,0 +530,1 @@\n+                if (!success) weakDelay();\n@@ -546,0 +548,1 @@\n+                if (!success) weakDelay();\n@@ -563,0 +566,1 @@\n+                if (!success) weakDelay();\n@@ -816,0 +820,1 @@\n+                if (!success) weakDelay();\n@@ -833,0 +838,1 @@\n+                if (!success) weakDelay();\n@@ -850,0 +856,1 @@\n+                if (!success) weakDelay();\n@@ -867,0 +874,1 @@\n+                if (!success) weakDelay();\n@@ -1123,0 +1131,1 @@\n+                    if (!success) weakDelay();\n@@ -1140,0 +1149,1 @@\n+                    if (!success) weakDelay();\n@@ -1157,0 +1167,1 @@\n+                    if (!success) weakDelay();\n@@ -1174,0 +1185,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessInt.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -512,0 +512,1 @@\n+                if (!success) weakDelay();\n@@ -529,0 +530,1 @@\n+                if (!success) weakDelay();\n@@ -546,0 +548,1 @@\n+                if (!success) weakDelay();\n@@ -563,0 +566,1 @@\n+                if (!success) weakDelay();\n@@ -816,0 +820,1 @@\n+                if (!success) weakDelay();\n@@ -833,0 +838,1 @@\n+                if (!success) weakDelay();\n@@ -850,0 +856,1 @@\n+                if (!success) weakDelay();\n@@ -867,0 +874,1 @@\n+                if (!success) weakDelay();\n@@ -1123,0 +1131,1 @@\n+                    if (!success) weakDelay();\n@@ -1140,0 +1149,1 @@\n+                    if (!success) weakDelay();\n@@ -1157,0 +1167,1 @@\n+                    if (!success) weakDelay();\n@@ -1174,0 +1185,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessLong.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -512,0 +512,1 @@\n+                if (!success) weakDelay();\n@@ -529,0 +530,1 @@\n+                if (!success) weakDelay();\n@@ -546,0 +548,1 @@\n+                if (!success) weakDelay();\n@@ -563,0 +566,1 @@\n+                if (!success) weakDelay();\n@@ -816,0 +820,1 @@\n+                if (!success) weakDelay();\n@@ -833,0 +838,1 @@\n+                if (!success) weakDelay();\n@@ -850,0 +856,1 @@\n+                if (!success) weakDelay();\n@@ -867,0 +874,1 @@\n+                if (!success) weakDelay();\n@@ -1123,0 +1131,1 @@\n+                    if (!success) weakDelay();\n@@ -1140,0 +1149,1 @@\n+                    if (!success) weakDelay();\n@@ -1157,0 +1167,1 @@\n+                    if (!success) weakDelay();\n@@ -1174,0 +1185,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessShort.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -590,0 +590,1 @@\n+                if (!success) weakDelay();\n@@ -607,0 +608,1 @@\n+                if (!success) weakDelay();\n@@ -624,0 +626,1 @@\n+                if (!success) weakDelay();\n@@ -641,0 +644,1 @@\n+                if (!success) weakDelay();\n@@ -830,0 +834,1 @@\n+                if (!success) weakDelay();\n@@ -847,0 +852,1 @@\n+                if (!success) weakDelay();\n@@ -864,0 +870,1 @@\n+                if (!success) weakDelay();\n@@ -881,0 +888,1 @@\n+                if (!success) weakDelay();\n@@ -1073,0 +1081,1 @@\n+                    if (!success) weakDelay();\n@@ -1090,0 +1099,1 @@\n+                    if (!success) weakDelay();\n@@ -1107,0 +1117,1 @@\n+                    if (!success) weakDelay();\n@@ -1124,0 +1135,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestAccessString.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -225,0 +226,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -227,1 +229,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, true, false);\n+                success = (boolean) mh.invokeExact(recv, true, false);\n+                if (!success) weakDelay();\n@@ -242,0 +245,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -244,1 +248,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, false, true);\n+                success = (boolean) mh.invokeExact(recv, false, true);\n+                if (!success) weakDelay();\n@@ -259,0 +264,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -261,1 +267,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, true, false);\n+                success = (boolean) mh.invokeExact(recv, true, false);\n+                if (!success) weakDelay();\n@@ -277,0 +284,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -278,1 +286,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, false, true);\n+                success = (boolean) mh.invokeExact(recv, false, true);\n+                if (!success) weakDelay();\n@@ -510,0 +519,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -512,1 +522,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(true, false);\n+                success = (boolean) mh.invokeExact(true, false);\n+                if (!success) weakDelay();\n@@ -527,0 +538,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -529,1 +541,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(false, true);\n+                success = (boolean) mh.invokeExact(false, true);\n+                if (!success) weakDelay();\n@@ -537,1 +550,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(false, false);\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact(false, false);\n@@ -544,0 +558,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -546,1 +561,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(true, false);\n+                success = (boolean) mh.invokeExact(true, false);\n+                if (!success) weakDelay();\n@@ -561,0 +577,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -563,1 +580,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(false, true);\n+                success = (boolean) mh.invokeExact(false, true);\n+                if (!success) weakDelay();\n@@ -797,0 +815,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -799,1 +818,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, true, false);\n+                    success = (boolean) mh.invokeExact(array, i, true, false);\n+                    if (!success) weakDelay();\n@@ -814,0 +834,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -816,1 +837,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, false, true);\n+                    success = (boolean) mh.invokeExact(array, i, false, true);\n+                    if (!success) weakDelay();\n@@ -831,0 +853,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -833,1 +856,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, true, false);\n+                    success = (boolean) mh.invokeExact(array, i, true, false);\n+                    if (!success) weakDelay();\n@@ -848,0 +872,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -850,1 +875,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, false, true);\n+                    success = (boolean) mh.invokeExact(array, i, false, true);\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessBoolean.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -225,0 +226,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -227,1 +229,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, (byte)0x01, (byte)0x23);\n+                success = (boolean) mh.invokeExact(recv, (byte)0x01, (byte)0x23);\n+                if (!success) weakDelay();\n@@ -242,0 +245,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -244,1 +248,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, (byte)0x23, (byte)0x01);\n+                success = (boolean) mh.invokeExact(recv, (byte)0x23, (byte)0x01);\n+                if (!success) weakDelay();\n@@ -259,0 +264,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -261,1 +267,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, (byte)0x01, (byte)0x23);\n+                success = (boolean) mh.invokeExact(recv, (byte)0x01, (byte)0x23);\n+                if (!success) weakDelay();\n@@ -277,0 +284,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -278,1 +286,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, (byte)0x23, (byte)0x01);\n+                success = (boolean) mh.invokeExact(recv, (byte)0x23, (byte)0x01);\n+                if (!success) weakDelay();\n@@ -527,0 +536,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -529,1 +539,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact((byte)0x01, (byte)0x23);\n+                success = (boolean) mh.invokeExact((byte)0x01, (byte)0x23);\n+                if (!success) weakDelay();\n@@ -544,0 +555,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -546,1 +558,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact((byte)0x23, (byte)0x01);\n+                success = (boolean) mh.invokeExact((byte)0x23, (byte)0x01);\n+                if (!success) weakDelay();\n@@ -554,1 +567,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact((byte)0x23, (byte)0x45);\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact((byte)0x23, (byte)0x45);\n@@ -561,0 +575,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -563,1 +578,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact((byte)0x01, (byte)0x23);\n+                success = (boolean) mh.invokeExact((byte)0x01, (byte)0x23);\n+                if (!success) weakDelay();\n@@ -578,0 +594,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -580,1 +597,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact((byte)0x23, (byte)0x01);\n+                success = (boolean) mh.invokeExact((byte)0x23, (byte)0x01);\n+                if (!success) weakDelay();\n@@ -836,0 +854,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -838,1 +857,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, (byte)0x01, (byte)0x23);\n+                    success = (boolean) mh.invokeExact(array, i, (byte)0x01, (byte)0x23);\n+                    if (!success) weakDelay();\n@@ -853,0 +873,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -855,1 +876,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, (byte)0x23, (byte)0x01);\n+                    success = (boolean) mh.invokeExact(array, i, (byte)0x23, (byte)0x01);\n+                    if (!success) weakDelay();\n@@ -870,0 +892,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -872,1 +895,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, (byte)0x01, (byte)0x23);\n+                    success = (boolean) mh.invokeExact(array, i, (byte)0x01, (byte)0x23);\n+                    if (!success) weakDelay();\n@@ -887,0 +911,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -889,1 +914,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, (byte)0x23, (byte)0x01);\n+                    success = (boolean) mh.invokeExact(array, i, (byte)0x23, (byte)0x01);\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessByte.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -225,0 +226,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -227,1 +229,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, '\\u0123', '\\u4567');\n+                success = (boolean) mh.invokeExact(recv, '\\u0123', '\\u4567');\n+                if (!success) weakDelay();\n@@ -242,0 +245,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -244,1 +248,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, '\\u4567', '\\u0123');\n+                success = (boolean) mh.invokeExact(recv, '\\u4567', '\\u0123');\n+                if (!success) weakDelay();\n@@ -259,0 +264,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -261,1 +267,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, '\\u0123', '\\u4567');\n+                success = (boolean) mh.invokeExact(recv, '\\u0123', '\\u4567');\n+                if (!success) weakDelay();\n@@ -277,0 +284,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -278,1 +286,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, '\\u4567', '\\u0123');\n+                success = (boolean) mh.invokeExact(recv, '\\u4567', '\\u0123');\n+                if (!success) weakDelay();\n@@ -527,0 +536,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -529,1 +539,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact('\\u0123', '\\u4567');\n+                success = (boolean) mh.invokeExact('\\u0123', '\\u4567');\n+                if (!success) weakDelay();\n@@ -544,0 +555,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -546,1 +558,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact('\\u4567', '\\u0123');\n+                success = (boolean) mh.invokeExact('\\u4567', '\\u0123');\n+                if (!success) weakDelay();\n@@ -554,1 +567,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact('\\u4567', '\\u89AB');\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact('\\u4567', '\\u89AB');\n@@ -561,0 +575,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -563,1 +578,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact('\\u0123', '\\u4567');\n+                success = (boolean) mh.invokeExact('\\u0123', '\\u4567');\n+                if (!success) weakDelay();\n@@ -578,0 +594,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -580,1 +597,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact('\\u4567', '\\u0123');\n+                success = (boolean) mh.invokeExact('\\u4567', '\\u0123');\n+                if (!success) weakDelay();\n@@ -836,0 +854,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -838,1 +857,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, '\\u0123', '\\u4567');\n+                    success = (boolean) mh.invokeExact(array, i, '\\u0123', '\\u4567');\n+                    if (!success) weakDelay();\n@@ -853,0 +873,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -855,1 +876,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, '\\u4567', '\\u0123');\n+                    success = (boolean) mh.invokeExact(array, i, '\\u4567', '\\u0123');\n+                    if (!success) weakDelay();\n@@ -870,0 +892,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -872,1 +895,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, '\\u0123', '\\u4567');\n+                    success = (boolean) mh.invokeExact(array, i, '\\u0123', '\\u4567');\n+                    if (!success) weakDelay();\n@@ -887,0 +911,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -889,1 +914,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, '\\u4567', '\\u0123');\n+                    success = (boolean) mh.invokeExact(array, i, '\\u4567', '\\u0123');\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessChar.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -225,0 +226,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -227,1 +229,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, 1.0d, 2.0d);\n+                success = (boolean) mh.invokeExact(recv, 1.0d, 2.0d);\n+                if (!success) weakDelay();\n@@ -242,0 +245,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -244,1 +248,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, 2.0d, 1.0d);\n+                success = (boolean) mh.invokeExact(recv, 2.0d, 1.0d);\n+                if (!success) weakDelay();\n@@ -259,0 +264,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -261,1 +267,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, 1.0d, 2.0d);\n+                success = (boolean) mh.invokeExact(recv, 1.0d, 2.0d);\n+                if (!success) weakDelay();\n@@ -277,0 +284,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -278,1 +286,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, 2.0d, 1.0d);\n+                success = (boolean) mh.invokeExact(recv, 2.0d, 1.0d);\n+                if (!success) weakDelay();\n@@ -454,0 +463,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -456,1 +466,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(1.0d, 2.0d);\n+                success = (boolean) mh.invokeExact(1.0d, 2.0d);\n+                if (!success) weakDelay();\n@@ -471,0 +482,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -473,1 +485,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(2.0d, 1.0d);\n+                success = (boolean) mh.invokeExact(2.0d, 1.0d);\n+                if (!success) weakDelay();\n@@ -481,1 +494,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(2.0d, 3.0d);\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact(2.0d, 3.0d);\n@@ -488,0 +502,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -490,1 +505,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(1.0d, 2.0d);\n+                success = (boolean) mh.invokeExact(1.0d, 2.0d);\n+                if (!success) weakDelay();\n@@ -505,0 +521,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -507,1 +524,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(2.0d, 1.0d);\n+                success = (boolean) mh.invokeExact(2.0d, 1.0d);\n+                if (!success) weakDelay();\n@@ -685,0 +703,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -687,1 +706,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, 1.0d, 2.0d);\n+                    success = (boolean) mh.invokeExact(array, i, 1.0d, 2.0d);\n+                    if (!success) weakDelay();\n@@ -702,0 +722,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -704,1 +725,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, 2.0d, 1.0d);\n+                    success = (boolean) mh.invokeExact(array, i, 2.0d, 1.0d);\n+                    if (!success) weakDelay();\n@@ -719,0 +741,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -721,1 +744,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, 1.0d, 2.0d);\n+                    success = (boolean) mh.invokeExact(array, i, 1.0d, 2.0d);\n+                    if (!success) weakDelay();\n@@ -736,0 +760,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -738,1 +763,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, 2.0d, 1.0d);\n+                    success = (boolean) mh.invokeExact(array, i, 2.0d, 1.0d);\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessDouble.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -225,0 +226,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -227,1 +229,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, 1.0f, 2.0f);\n+                success = (boolean) mh.invokeExact(recv, 1.0f, 2.0f);\n+                if (!success) weakDelay();\n@@ -242,0 +245,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -244,1 +248,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, 2.0f, 1.0f);\n+                success = (boolean) mh.invokeExact(recv, 2.0f, 1.0f);\n+                if (!success) weakDelay();\n@@ -259,0 +264,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -261,1 +267,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, 1.0f, 2.0f);\n+                success = (boolean) mh.invokeExact(recv, 1.0f, 2.0f);\n+                if (!success) weakDelay();\n@@ -277,0 +284,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -278,1 +286,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, 2.0f, 1.0f);\n+                success = (boolean) mh.invokeExact(recv, 2.0f, 1.0f);\n+                if (!success) weakDelay();\n@@ -454,0 +463,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -456,1 +466,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(1.0f, 2.0f);\n+                success = (boolean) mh.invokeExact(1.0f, 2.0f);\n+                if (!success) weakDelay();\n@@ -471,0 +482,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -473,1 +485,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(2.0f, 1.0f);\n+                success = (boolean) mh.invokeExact(2.0f, 1.0f);\n+                if (!success) weakDelay();\n@@ -481,1 +494,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(2.0f, 3.0f);\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact(2.0f, 3.0f);\n@@ -488,0 +502,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -490,1 +505,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(1.0f, 2.0f);\n+                success = (boolean) mh.invokeExact(1.0f, 2.0f);\n+                if (!success) weakDelay();\n@@ -505,0 +521,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -507,1 +524,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(2.0f, 1.0f);\n+                success = (boolean) mh.invokeExact(2.0f, 1.0f);\n+                if (!success) weakDelay();\n@@ -685,0 +703,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -687,1 +706,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, 1.0f, 2.0f);\n+                    success = (boolean) mh.invokeExact(array, i, 1.0f, 2.0f);\n+                    if (!success) weakDelay();\n@@ -702,0 +722,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -704,1 +725,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, 2.0f, 1.0f);\n+                    success = (boolean) mh.invokeExact(array, i, 2.0f, 1.0f);\n+                    if (!success) weakDelay();\n@@ -719,0 +741,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -721,1 +744,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, 1.0f, 2.0f);\n+                    success = (boolean) mh.invokeExact(array, i, 1.0f, 2.0f);\n+                    if (!success) weakDelay();\n@@ -736,0 +760,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -738,1 +763,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, 2.0f, 1.0f);\n+                    success = (boolean) mh.invokeExact(array, i, 2.0f, 1.0f);\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessFloat.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -225,0 +226,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -227,1 +229,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, 0x01234567, 0x89ABCDEF);\n+                success = (boolean) mh.invokeExact(recv, 0x01234567, 0x89ABCDEF);\n+                if (!success) weakDelay();\n@@ -242,0 +245,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -244,1 +248,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, 0x89ABCDEF, 0x01234567);\n+                success = (boolean) mh.invokeExact(recv, 0x89ABCDEF, 0x01234567);\n+                if (!success) weakDelay();\n@@ -259,0 +264,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -261,1 +267,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, 0x01234567, 0x89ABCDEF);\n+                success = (boolean) mh.invokeExact(recv, 0x01234567, 0x89ABCDEF);\n+                if (!success) weakDelay();\n@@ -277,0 +284,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -278,1 +286,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, 0x89ABCDEF, 0x01234567);\n+                success = (boolean) mh.invokeExact(recv, 0x89ABCDEF, 0x01234567);\n+                if (!success) weakDelay();\n@@ -527,0 +536,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -529,1 +539,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(0x01234567, 0x89ABCDEF);\n+                success = (boolean) mh.invokeExact(0x01234567, 0x89ABCDEF);\n+                if (!success) weakDelay();\n@@ -544,0 +555,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -546,1 +558,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(0x89ABCDEF, 0x01234567);\n+                success = (boolean) mh.invokeExact(0x89ABCDEF, 0x01234567);\n+                if (!success) weakDelay();\n@@ -554,1 +567,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(0x89ABCDEF, 0xCAFEBABE);\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact(0x89ABCDEF, 0xCAFEBABE);\n@@ -561,0 +575,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -563,1 +578,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(0x01234567, 0x89ABCDEF);\n+                success = (boolean) mh.invokeExact(0x01234567, 0x89ABCDEF);\n+                if (!success) weakDelay();\n@@ -578,0 +594,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -580,1 +597,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(0x89ABCDEF, 0x01234567);\n+                success = (boolean) mh.invokeExact(0x89ABCDEF, 0x01234567);\n+                if (!success) weakDelay();\n@@ -836,0 +854,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -838,1 +857,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, 0x01234567, 0x89ABCDEF);\n+                    success = (boolean) mh.invokeExact(array, i, 0x01234567, 0x89ABCDEF);\n+                    if (!success) weakDelay();\n@@ -853,0 +873,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -855,1 +876,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, 0x89ABCDEF, 0x01234567);\n+                    success = (boolean) mh.invokeExact(array, i, 0x89ABCDEF, 0x01234567);\n+                    if (!success) weakDelay();\n@@ -870,0 +892,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -872,1 +895,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, 0x01234567, 0x89ABCDEF);\n+                    success = (boolean) mh.invokeExact(array, i, 0x01234567, 0x89ABCDEF);\n+                    if (!success) weakDelay();\n@@ -887,0 +911,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -889,1 +914,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, 0x89ABCDEF, 0x01234567);\n+                    success = (boolean) mh.invokeExact(array, i, 0x89ABCDEF, 0x01234567);\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessInt.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -225,0 +226,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -227,1 +229,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, 0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                success = (boolean) mh.invokeExact(recv, 0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                if (!success) weakDelay();\n@@ -242,0 +245,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -244,1 +248,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, 0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                success = (boolean) mh.invokeExact(recv, 0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                if (!success) weakDelay();\n@@ -259,0 +264,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -261,1 +267,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, 0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                success = (boolean) mh.invokeExact(recv, 0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                if (!success) weakDelay();\n@@ -277,0 +284,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -278,1 +286,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, 0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                success = (boolean) mh.invokeExact(recv, 0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                if (!success) weakDelay();\n@@ -527,0 +536,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -529,1 +539,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                success = (boolean) mh.invokeExact(0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                if (!success) weakDelay();\n@@ -544,0 +555,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -546,1 +558,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                success = (boolean) mh.invokeExact(0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                if (!success) weakDelay();\n@@ -554,1 +567,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(0xCAFEBABECAFEBABEL, 0xDEADBEEFDEADBEEFL);\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact(0xCAFEBABECAFEBABEL, 0xDEADBEEFDEADBEEFL);\n@@ -561,0 +575,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -563,1 +578,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                success = (boolean) mh.invokeExact(0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                if (!success) weakDelay();\n@@ -578,0 +594,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -580,1 +597,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                success = (boolean) mh.invokeExact(0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                if (!success) weakDelay();\n@@ -836,0 +854,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -838,1 +857,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, 0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                    success = (boolean) mh.invokeExact(array, i, 0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                    if (!success) weakDelay();\n@@ -853,0 +873,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -855,1 +876,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, 0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                    success = (boolean) mh.invokeExact(array, i, 0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                    if (!success) weakDelay();\n@@ -870,0 +892,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -872,1 +895,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, 0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                    success = (boolean) mh.invokeExact(array, i, 0x0123456789ABCDEFL, 0xCAFEBABECAFEBABEL);\n+                    if (!success) weakDelay();\n@@ -887,0 +911,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -889,1 +914,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, 0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                    success = (boolean) mh.invokeExact(array, i, 0xCAFEBABECAFEBABEL, 0x0123456789ABCDEFL);\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessLong.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -225,0 +226,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -227,1 +229,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, (short)0x0123, (short)0x4567);\n+                success = (boolean) mh.invokeExact(recv, (short)0x0123, (short)0x4567);\n+                if (!success) weakDelay();\n@@ -242,0 +245,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -244,1 +248,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, (short)0x4567, (short)0x0123);\n+                success = (boolean) mh.invokeExact(recv, (short)0x4567, (short)0x0123);\n+                if (!success) weakDelay();\n@@ -259,0 +264,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -261,1 +267,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, (short)0x0123, (short)0x4567);\n+                success = (boolean) mh.invokeExact(recv, (short)0x0123, (short)0x4567);\n+                if (!success) weakDelay();\n@@ -277,0 +284,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -278,1 +286,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, (short)0x4567, (short)0x0123);\n+                success = (boolean) mh.invokeExact(recv, (short)0x4567, (short)0x0123);\n+                if (!success) weakDelay();\n@@ -527,0 +536,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -529,1 +539,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact((short)0x0123, (short)0x4567);\n+                success = (boolean) mh.invokeExact((short)0x0123, (short)0x4567);\n+                if (!success) weakDelay();\n@@ -544,0 +555,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -546,1 +558,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact((short)0x4567, (short)0x0123);\n+                success = (boolean) mh.invokeExact((short)0x4567, (short)0x0123);\n+                if (!success) weakDelay();\n@@ -554,1 +567,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact((short)0x4567, (short)0x89AB);\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact((short)0x4567, (short)0x89AB);\n@@ -561,0 +575,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -563,1 +578,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact((short)0x0123, (short)0x4567);\n+                success = (boolean) mh.invokeExact((short)0x0123, (short)0x4567);\n+                if (!success) weakDelay();\n@@ -578,0 +594,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -580,1 +597,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact((short)0x4567, (short)0x0123);\n+                success = (boolean) mh.invokeExact((short)0x4567, (short)0x0123);\n+                if (!success) weakDelay();\n@@ -836,0 +854,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -838,1 +857,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, (short)0x0123, (short)0x4567);\n+                    success = (boolean) mh.invokeExact(array, i, (short)0x0123, (short)0x4567);\n+                    if (!success) weakDelay();\n@@ -853,0 +873,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -855,1 +876,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, (short)0x4567, (short)0x0123);\n+                    success = (boolean) mh.invokeExact(array, i, (short)0x4567, (short)0x0123);\n+                    if (!success) weakDelay();\n@@ -870,0 +892,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -872,1 +895,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, (short)0x0123, (short)0x4567);\n+                    success = (boolean) mh.invokeExact(array, i, (short)0x0123, (short)0x4567);\n+                    if (!success) weakDelay();\n@@ -887,0 +911,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -889,1 +914,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, (short)0x4567, (short)0x0123);\n+                    success = (boolean) mh.invokeExact(array, i, (short)0x4567, (short)0x0123);\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessShort.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -217,0 +218,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -219,1 +221,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, \"foo\", \"bar\");\n+                success = (boolean) mh.invokeExact(recv, \"foo\", \"bar\");\n+                if (!success) weakDelay();\n@@ -234,0 +237,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -236,1 +240,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, \"bar\", \"foo\");\n+                success = (boolean) mh.invokeExact(recv, \"bar\", \"foo\");\n+                if (!success) weakDelay();\n@@ -251,0 +256,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -253,1 +259,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, \"foo\", \"bar\");\n+                success = (boolean) mh.invokeExact(recv, \"foo\", \"bar\");\n+                if (!success) weakDelay();\n@@ -269,0 +276,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -270,1 +278,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, \"bar\", \"foo\");\n+                success = (boolean) mh.invokeExact(recv, \"bar\", \"foo\");\n+                if (!success) weakDelay();\n@@ -401,0 +410,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -403,1 +413,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(\"foo\", \"bar\");\n+                success = (boolean) mh.invokeExact(\"foo\", \"bar\");\n+                if (!success) weakDelay();\n@@ -418,0 +429,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -420,1 +432,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(\"bar\", \"foo\");\n+                success = (boolean) mh.invokeExact(\"bar\", \"foo\");\n+                if (!success) weakDelay();\n@@ -428,1 +441,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(\"bar\", \"baz\");\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact(\"bar\", \"baz\");\n@@ -435,0 +449,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -437,1 +452,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(\"foo\", \"bar\");\n+                success = (boolean) mh.invokeExact(\"foo\", \"bar\");\n+                if (!success) weakDelay();\n@@ -452,0 +468,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -454,1 +471,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(\"bar\", \"foo\");\n+                success = (boolean) mh.invokeExact(\"bar\", \"foo\");\n+                if (!success) weakDelay();\n@@ -610,0 +628,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -612,1 +631,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, \"foo\", \"bar\");\n+                    success = (boolean) mh.invokeExact(array, i, \"foo\", \"bar\");\n+                    if (!success) weakDelay();\n@@ -627,0 +647,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -629,1 +650,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, \"bar\", \"foo\");\n+                    success = (boolean) mh.invokeExact(array, i, \"bar\", \"foo\");\n+                    if (!success) weakDelay();\n@@ -644,0 +666,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -646,1 +669,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, \"foo\", \"bar\");\n+                    success = (boolean) mh.invokeExact(array, i, \"foo\", \"bar\");\n+                    if (!success) weakDelay();\n@@ -661,0 +685,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -663,1 +688,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, \"bar\", \"foo\");\n+                    success = (boolean) mh.invokeExact(array, i, \"bar\", \"foo\");\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/VarHandleTestMethodHandleAccessString.java","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -766,0 +766,1 @@\n+                if (!success) weakDelay();\n@@ -783,0 +784,1 @@\n+                if (!success) weakDelay();\n@@ -800,0 +802,1 @@\n+                if (!success) weakDelay();\n@@ -817,0 +820,1 @@\n+                if (!success) weakDelay();\n@@ -1171,0 +1175,1 @@\n+                if (!success) weakDelay();\n@@ -1188,0 +1193,1 @@\n+                if (!success) weakDelay();\n@@ -1205,0 +1211,1 @@\n+                if (!success) weakDelay();\n@@ -1222,0 +1229,1 @@\n+                if (!success) weakDelay();\n@@ -1579,0 +1587,1 @@\n+                    if (!success) weakDelay();\n@@ -1596,0 +1605,1 @@\n+                    if (!success) weakDelay();\n@@ -1613,0 +1623,1 @@\n+                    if (!success) weakDelay();\n@@ -1630,0 +1641,1 @@\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/X-VarHandleTestAccess.java.template","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+import java.lang.invoke.MethodHandle;\n@@ -236,0 +237,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -238,1 +240,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(recv, $value1$, $value2$);\n+                success = (boolean) mh.invokeExact(recv, $value1$, $value2$);\n+                if (!success) weakDelay();\n@@ -253,0 +256,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -255,1 +259,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(recv, $value2$, $value1$);\n+                success = (boolean) mh.invokeExact(recv, $value2$, $value1$);\n+                if (!success) weakDelay();\n@@ -270,0 +275,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -272,1 +278,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(recv, $value1$, $value2$);\n+                success = (boolean) mh.invokeExact(recv, $value1$, $value2$);\n+                if (!success) weakDelay();\n@@ -288,0 +295,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -289,1 +297,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(recv, $value2$, $value1$);\n+                success = (boolean) mh.invokeExact(recv, $value2$, $value1$);\n+                if (!success) weakDelay();\n@@ -612,0 +621,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -614,1 +624,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact($value1$, $value2$);\n+                success = (boolean) mh.invokeExact($value1$, $value2$);\n+                if (!success) weakDelay();\n@@ -629,0 +640,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -631,1 +643,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact($value2$, $value1$);\n+                success = (boolean) mh.invokeExact($value2$, $value1$);\n+                if (!success) weakDelay();\n@@ -639,1 +652,2 @@\n-            boolean success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact($value2$, $value3$);\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n+            boolean success = (boolean) mh.invokeExact($value2$, $value3$);\n@@ -646,0 +660,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -648,1 +663,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact($value1$, $value2$);\n+                success = (boolean) mh.invokeExact($value1$, $value2$);\n+                if (!success) weakDelay();\n@@ -663,0 +679,1 @@\n+            MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -665,1 +682,2 @@\n-                success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact($value2$, $value1$);\n+                success = (boolean) mh.invokeExact($value2$, $value1$);\n+                if (!success) weakDelay();\n@@ -960,0 +978,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN);\n@@ -962,1 +981,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_PLAIN).invokeExact(array, i, $value1$, $value2$);\n+                    success = (boolean) mh.invokeExact(array, i, $value1$, $value2$);\n+                    if (!success) weakDelay();\n@@ -977,0 +997,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE);\n@@ -979,1 +1000,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_ACQUIRE).invokeExact(array, i, $value2$, $value1$);\n+                    success = (boolean) mh.invokeExact(array, i, $value2$, $value1$);\n+                    if (!success) weakDelay();\n@@ -994,0 +1016,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE);\n@@ -996,1 +1019,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET_RELEASE).invokeExact(array, i, $value1$, $value2$);\n+                    success = (boolean) mh.invokeExact(array, i, $value1$, $value2$);\n+                    if (!success) weakDelay();\n@@ -1011,0 +1035,1 @@\n+                MethodHandle mh = hs.get(TestAccessMode.WEAK_COMPARE_AND_SET);\n@@ -1013,1 +1038,2 @@\n-                    success = (boolean) hs.get(TestAccessMode.WEAK_COMPARE_AND_SET).invokeExact(array, i, $value2$, $value1$);\n+                    success = (boolean) mh.invokeExact(array, i, $value2$, $value1$);\n+                    if (!success) weakDelay();\n","filename":"test\/jdk\/java\/lang\/invoke\/VarHandles\/X-VarHandleTestMethodHandleAccess.java.template","additions":39,"deletions":13,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -68,0 +68,1 @@\n+compiler.misc.feature.default.methods                   # just preserved for testing (for now)\n","filename":"test\/langtools\/tools\/javac\/diags\/examples.not-yet.txt","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}