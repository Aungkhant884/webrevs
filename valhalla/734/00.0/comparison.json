{"files":[{"patch":"@@ -4,1 +4,1 @@\n-version=19\n+version=20\n","filename":".jcheck\/conf","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -107,1 +107,1 @@\n-    -serialwarn -encoding ISO-8859-1 -docencoding UTF-8 -breakiterator \\\n+    -encoding ISO-8859-1 -docencoding UTF-8 -breakiterator \\\n@@ -115,1 +115,1 @@\n-    -serialwarn -encoding ISO-8859-1 -breakiterator -splitIndex --system none \\\n+    -encoding ISO-8859-1 -breakiterator -splitIndex --system none \\\n","filename":"make\/Docs.gmk","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -148,6 +148,0 @@\n-\n-  # --with-cpu-port is no longer supported\n-  UTIL_DEPRECATED_ARG_WITH(with-cpu-port)\n-\n-  # in jdk15 hotspot-gtest was replaced with --with-gtest\n-  UTIL_DEPRECATED_ARG_ENABLE(hotspot-gtest)\n","filename":"make\/autoconf\/hotspot.m4","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -247,1 +247,1 @@\n-    \/\/ These are the base setttings for all the main build profiles.\n+    \/\/ These are the base settings for all the main build profiles.\n@@ -396,2 +396,2 @@\n-    common.boot_jdk_version = \"17\";\n-    common.boot_jdk_build_number = \"35\";\n+    common.boot_jdk_version = \"18\";\n+    common.boot_jdk_build_number = \"36\";\n@@ -453,1 +453,1 @@\n-            configure_args: concat(common.configure_args_64bit, \"--with-zlib=system\",\n+            configure_args: concat(common.configure_args_64bit,\n@@ -623,1 +623,1 @@\n-    \/\/ verfication of this build configuration.\n+    \/\/ verification of this build configuration.\n@@ -840,1 +840,1 @@\n-            \/\/ Only compare the images target. This should pressumably be expanded\n+            \/\/ Only compare the images target. This should presumably be expanded\n@@ -1025,1 +1025,1 @@\n-    \/\/ test tasks. Care must however be taken not to polute that work dir by\n+    \/\/ test tasks. Care must however be taken not to pollute that work dir by\n@@ -1028,1 +1028,1 @@\n-    \/\/ Use the existance of the top level README.md as indication of if this is\n+    \/\/ Use the existence of the top level README.md as indication of if this is\n@@ -1057,1 +1057,1 @@\n-        windows_x64: \"VS2019-16.9.3+1.0\",\n+        windows_x64: \"VS2022-17.1.0+1.0\",\n","filename":"make\/conf\/jib-profiles.js","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,1 +29,1 @@\n-DEFAULT_VERSION_FEATURE=19\n+DEFAULT_VERSION_FEATURE=20\n@@ -36,2 +36,2 @@\n-DEFAULT_VERSION_DATE=2022-09-20\n-DEFAULT_VERSION_CLASSFILE_MAJOR=63  # \"`$EXPR $DEFAULT_VERSION_FEATURE + 44`\"\n+DEFAULT_VERSION_DATE=2023-03-21\n+DEFAULT_VERSION_CLASSFILE_MAJOR=64  # \"`$EXPR $DEFAULT_VERSION_FEATURE + 44`\"\n@@ -40,3 +40,3 @@\n-DEFAULT_ACCEPTABLE_BOOT_VERSIONS=\"17 18 19\"\n-DEFAULT_JDK_SOURCE_TARGET_VERSION=19\n-DEFAULT_PROMOTED_VERSION_PRE=lworld3ea\n+DEFAULT_ACCEPTABLE_BOOT_VERSIONS=\"18 19 20\"\n+DEFAULT_JDK_SOURCE_TARGET_VERSION=20\n+DEFAULT_PROMOTED_VERSION_PRE=lworld4ea\n","filename":"make\/conf\/version-numbers.conf","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -172,1 +172,1 @@\n- *  * enhance existing .sym.txt files with a a new set .sym.txt for the current platform\n+ *  * enhance existing .sym.txt files with a new set .sym.txt for the current platform\n","filename":"make\/langtools\/src\/classes\/build\/tools\/symbolgenerator\/CreateSymbols.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -173,1 +173,1 @@\n-# Setup a rule for generating a memory access var handle helper classes\n+# Setup a rule for generating a memory segment var handle view class\n@@ -176,1 +176,1 @@\n-define GenerateVarHandleMemoryAccess\n+define GenerateVarHandleMemorySegment\n@@ -180,1 +180,1 @@\n-  $1_FILENAME := $(VARHANDLES_GENSRC_DIR)\/MemoryAccessVarHandle$$($1_Type)Helper.java\n+  $1_FILENAME := $(VARHANDLES_GENSRC_DIR)\/VarHandleSegmentAs$$($1_Type)s.java\n@@ -261,1 +261,1 @@\n-  $$($1_FILENAME): $(VARHANDLES_SRC_DIR)\/X-VarHandleMemoryAccess.java.template $(BUILD_TOOLS_JDK)\n+  $$($1_FILENAME): $(VARHANDLES_SRC_DIR)\/X-VarHandleSegmentView.java.template $(BUILD_TOOLS_JDK)\n@@ -285,3 +285,3 @@\n-VARHANDLES_MEMORY_ADDRESS_TYPES := Byte Short Char Int Long Float Double\n-$(foreach t, $(VARHANDLES_MEMORY_ADDRESS_TYPES), \\\n-  $(eval $(call GenerateVarHandleMemoryAccess,VAR_HANDLE_MEMORY_ADDRESS_$t,$t)))\n+VARHANDLES_MEMORY_SEGMENT_TYPES := Byte Short Char Int Long Float Double\n+$(foreach t, $(VARHANDLES_MEMORY_SEGMENT_TYPES), \\\n+  $(eval $(call GenerateVarHandleMemorySegment,VAR_HANDLE_MEMORY_SEGMENT_$t,$t)))\n","filename":"make\/modules\/java.base\/gensrc\/GensrcVarHandles.gmk","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -97,2 +97,7 @@\n-    JAVAC_FLAGS := --add-exports java.base\/sun.security.util=ALL-UNNAMED -XDallowWithFieldOperator --enable-preview, \\\n-    JAVA_FLAGS := --add-modules jdk.unsupported --limit-modules java.management, \\\n+    JAVAC_FLAGS := --add-exports java.base\/sun.security.util=ALL-UNNAMED \\\n+        --add-exports java.base\/sun.invoke.util=ALL-UNNAMED \\\n+        --add-exports java.base\/jdk.internal.vm=ALL-UNNAMED \\\n+        --enable-preview, \\\n+    JAVA_FLAGS := --add-modules jdk.unsupported --limit-modules java.management \\\n+        --add-exports java.base\/jdk.internal.vm=ALL-UNNAMED \\\n+        --enable-preview, \\\n","filename":"make\/test\/BuildMicrobenchmark.gmk","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2015, 2020, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -151,2 +151,4 @@\n-BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS := -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS\n-BUILD_HOTSPOT_JTREG_EXECUTABLES_CFLAGS := -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS\n+JVMTI_COMMON_INCLUDES=-I$(TOPDIR)\/test\/lib\/jdk\/test\/lib\/jvmti\n+\n+BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS := -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS $(JVMTI_COMMON_INCLUDES)\n+BUILD_HOTSPOT_JTREG_EXECUTABLES_CFLAGS := -D__STDC_FORMAT_MACROS -D__STDC_LIMIT_MACROS -D__STDC_CONSTANT_MACROS $(JVMTI_COMMON_INCLUDES)\n@@ -349,0 +351,1 @@\n+BUILD_HOTSPOT_JTREG_LIBRARIES_CFLAGS_libsuspendvthr001 := $(NSK_JVMTI_AGENT_INCLUDES)\n@@ -875,1 +878,1 @@\n-    BUILD_HOTSPOT_JTREG_EXCLUDE += exesigtest.c libterminatedThread.c libTestJNI.c\n+    BUILD_HOTSPOT_JTREG_EXCLUDE += exesigtest.c libterminatedThread.c libTestJNI.c libCompleteExit.c libTestPsig.c\n@@ -1016,0 +1019,1 @@\n+    BUILD_HOTSPOT_JTREG_LIBRARIES_LIBS_libsuspendvthr001 += -lpthread\n@@ -1513,0 +1517,1 @@\n+    BUILD_HOTSPOT_JTREG_LIBRARIES_LIBS_libCompleteExit += -lpthread\n","filename":"make\/test\/JtregNativeHotspot.gmk","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-\/\/ archtecture.\n+\/\/ architecture.\n@@ -491,1 +491,1 @@\n-\/\/ the AArch64 CSPR status flag register is not directly acessible as\n+\/\/ the AArch64 CSPR status flag register is not directly accessible as\n@@ -1202,0 +1202,3 @@\n+reg_class p0_reg(P0);\n+reg_class p1_reg(P1);\n+\n@@ -1282,1 +1285,1 @@\n-    return MacroAssembler::far_branch_size();\n+    return MacroAssembler::far_codestub_branch_size();\n@@ -1287,1 +1290,1 @@\n-    return 4 * NativeInstruction::instruction_size;\n+    return NativeInstruction::instruction_size + MacroAssembler::far_codestub_branch_size();\n@@ -1511,1 +1514,1 @@\n-  \/\/ MemBarAcquire, possibly thorugh an optional DecodeN, is still\n+  \/\/ MemBarAcquire, possibly through an optional DecodeN, is still\n@@ -1787,10 +1790,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  \/\/ This is implemented using aarch64_enc_java_to_runtime as above.\n-  CodeBlob *cb = CodeCache::find_blob(_entry_point);\n-  if (cb) {\n-    return 1 * NativeInstruction::instruction_size;\n-  } else {\n-    return 6 * NativeInstruction::instruction_size;\n-  }\n-}\n-\n@@ -1927,1 +1920,18 @@\n-    bs->nmethod_entry_barrier(&_masm);\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+      \/\/ Dummy labels for just measuring the code size\n+      Label dummy_slow_path;\n+      Label dummy_continuation;\n+      Label dummy_guard;\n+      Label* slow_path = &dummy_slow_path;\n+      Label* continuation = &dummy_continuation;\n+      Label* guard = &dummy_guard;\n+      if (!Compile::current()->output()->in_scratch_emit_size()) {\n+        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+        C2EntryBarrierStub* stub = Compile::current()->output()->entry_barrier_table()->add_entry_barrier();\n+        slow_path = &stub->slow_path();\n+        continuation = &stub->continuation();\n+        guard = &stub->guard();\n+      }\n+      \/\/ In the C2 code, we move the non-hot part of nmethod entry barriers out-of-line to a stub.\n+      bs->nmethod_entry_barrier(&_masm, slow_path, continuation, guard);\n+    }\n@@ -2417,1 +2427,1 @@\n-  assert(__ offset() - offset <= (int) size_deopt_handler(), \"overflow\");\n+  assert(__ offset() - offset == (int) size_deopt_handler(), \"overflow\");\n@@ -2458,0 +2468,13 @@\n+const bool Matcher::match_rule_supported_superword(int opcode, int vlen, BasicType bt) {\n+  if (UseSVE == 0) {\n+    \/\/ ConvD2I and ConvL2F are not profitable to be vectorized on NEON, because no direct\n+    \/\/ NEON instructions support them. But the match rule support for them is profitable for\n+    \/\/ Vector API intrinsics.\n+    if ((opcode == Op_VectorCastD2X && bt == T_INT) ||\n+        (opcode == Op_VectorCastL2X && bt == T_FLOAT)) {\n+      return false;\n+    }\n+  }\n+  return match_rule_supported_vector(opcode, vlen, bt);\n+}\n+\n@@ -2484,0 +2507,1 @@\n+    case Op_PopulateIndex:\n@@ -2491,0 +2515,1 @@\n+    case Op_VectorMaskGen:\n@@ -2493,0 +2518,4 @@\n+    case Op_CompressV:\n+    case Op_CompressM:\n+    case Op_ExpandV:\n+    case Op_VectorLongToMask:\n@@ -2510,0 +2539,5 @@\n+const bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n+  \/\/ Only SVE has partial vector operations\n+  return (UseSVE > 0) && partial_op_sve_needed(node, vt);\n+}\n+\n@@ -2721,2 +2755,3 @@\n-bool can_combine_with_imm(Node* binary_node, Node* replicate_node) {\n-  if (UseSVE == 0 || !VectorNode::is_invariant_vector(replicate_node)){\n+\/\/ Binary src (Replicate con)\n+bool is_valid_sve_arith_imm_pattern(Node* n, Node* m) {\n+  if (n == NULL || m == NULL) {\n@@ -2725,1 +2760,6 @@\n-  Node* imm_node = replicate_node->in(1);\n+\n+  if (UseSVE == 0 || !VectorNode::is_invariant_vector(m)) {\n+    return false;\n+  }\n+\n+  Node* imm_node = m->in(1);\n@@ -2735,1 +2775,1 @@\n-  switch (binary_node->Opcode()) {\n+  switch (n->Opcode()) {\n@@ -2739,1 +2779,1 @@\n-    Assembler::SIMD_RegVariant T = Assembler::elemType_to_regVariant(Matcher::vector_element_basic_type(binary_node));\n+    Assembler::SIMD_RegVariant T = Assembler::elemType_to_regVariant(Matcher::vector_element_basic_type(n));\n@@ -2755,1 +2795,3 @@\n-bool is_vector_arith_imm_pattern(Node* n, Node* m) {\n+\/\/ (XorV src (Replicate m1))\n+\/\/ (XorVMask src (MaskAll m1))\n+bool is_vector_bitwise_not_pattern(Node* n, Node* m) {\n@@ -2757,1 +2799,2 @@\n-    return can_combine_with_imm(n, m);\n+    return (n->Opcode() == Op_XorV || n->Opcode() == Op_XorVMask) &&\n+           VectorNode::is_all_ones_vector(m);\n@@ -2764,3 +2807,3 @@\n-  \/\/ ShiftV src (ShiftCntV con)\n-  \/\/ Binary src (Replicate con)\n-  if (is_vshift_con_pattern(n, m) || is_vector_arith_imm_pattern(n, m)) {\n+  if (is_vshift_con_pattern(n, m) ||\n+      is_vector_bitwise_not_pattern(n, m) ||\n+      is_valid_sve_arith_imm_pattern(n, m)) {\n@@ -2770,1 +2813,0 @@\n-\n@@ -3835,5 +3877,11 @@\n-      \/\/ Emit stub for static call\n-      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n-      if (stub == NULL) {\n-        ciEnv::current()->record_failure(\"CodeCache is full\");\n-        return;\n+      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n+        \/\/ Calls of the same statically bound method can share\n+        \/\/ a stub to the interpreter.\n+        cbuf.shared_stub_to_interp_for(_method, cbuf.insts()->mark_off());\n+      } else {\n+        \/\/ Emit stub for static call\n+        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n+        if (stub == NULL) {\n+          ciEnv::current()->record_failure(\"CodeCache is full\");\n+          return;\n+        }\n@@ -3843,0 +3891,3 @@\n+    _masm.clear_inst_mark();\n+    __ post_call_nop();\n+\n@@ -3856,1 +3907,4 @@\n-    } else if (Compile::current()->max_vector_size() > 0) {\n+    }\n+    _masm.clear_inst_mark();\n+    __ post_call_nop();\n+    if (Compile::current()->max_vector_size() > 0) {\n@@ -3917,0 +3971,2 @@\n+      _masm.clear_inst_mark();\n+      __ post_call_nop();\n@@ -3925,0 +3981,1 @@\n+      __ post_call_nop();\n@@ -3971,1 +4028,1 @@\n-    Label cas_failed;\n+    Label no_count;\n@@ -4012,3 +4069,0 @@\n-      __ bind(cas_failed);\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n@@ -4059,0 +4113,5 @@\n+    __ br(Assembler::NE, no_count);\n+\n+    __ increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+    __ bind(no_count);\n@@ -4069,0 +4128,1 @@\n+    Label no_count;\n@@ -4127,0 +4187,5 @@\n+    __ br(Assembler::NE, no_count);\n+\n+    __ decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+    __ bind(no_count);\n@@ -4174,1 +4239,1 @@\n-\/\/         are owned by the CALLEE.  Holes should not be nessecary in the\n+\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n@@ -4177,1 +4242,1 @@\n-\/\/         avoid holes.  Holes in the outgoing arguments may be nessecary for\n+\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n@@ -4491,0 +4556,10 @@\n+operand immI_positive()\n+%{\n+  predicate(n->get_int() > 0);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -5777,0 +5852,18 @@\n+operand pRegGov_P0()\n+%{\n+  constraint(ALLOC_IN_RC(p0_reg));\n+  match(RegVectMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+operand pRegGov_P1()\n+%{\n+  constraint(ALLOC_IN_RC(p1_reg));\n+  match(RegVectMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -8694,1 +8787,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8716,1 +8808,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8739,1 +8830,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -8759,1 +8849,0 @@\n-  predicate(UsePopCountInstruction);\n@@ -11183,1 +11272,1 @@\n-  format %{ \"smulh   $dst, $src1, $src2, \\t# mulhi\" %}\n+  format %{ \"smulh   $dst, $src1, $src2\\t# mulhi\" %}\n@@ -11194,0 +11283,16 @@\n+instruct umulHiL_rReg(iRegLNoSp dst, iRegL src1, iRegL src2, rFlagsReg cr)\n+%{\n+  match(Set dst (UMulHiL src1 src2));\n+\n+  ins_cost(INSN_COST * 7);\n+  format %{ \"umulh   $dst, $src1, $src2\\t# umulhi\" %}\n+\n+  ins_encode %{\n+    __ umulh(as_Register($dst$$reg),\n+             as_Register($src1$$reg),\n+             as_Register($src2$$reg));\n+  %}\n+\n+  ins_pipe(lmul_reg_reg);\n+%}\n+\n@@ -11392,1 +11497,1 @@\n-            \"msubw($dst, rscratch1, $src2, $src1\" %}\n+            \"msubw  $dst, rscratch1, $src2, $src1\" %}\n@@ -11405,1 +11510,1 @@\n-            \"msub($dst, rscratch1, $src2, $src1\" %}\n+            \"msub   $dst, rscratch1, $src2, $src1\" %}\n@@ -11411,0 +11516,64 @@\n+\/\/ Unsigned Integer Divide\n+\n+instruct UdivI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (UDivI src1 src2));\n+\n+  ins_cost(INSN_COST * 19);\n+  format %{ \"udivw  $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ udivw($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+\n+  ins_pipe(idiv_reg_reg);\n+%}\n+\n+\/\/  Unsigned Long Divide\n+\n+instruct UdivL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (UDivL src1 src2));\n+\n+  ins_cost(INSN_COST * 35);\n+  format %{ \"udiv   $dst, $src1, $src2\" %}\n+\n+  ins_encode %{\n+    __ udiv($dst$$Register, $src1$$Register, $src2$$Register);\n+  %}\n+\n+  ins_pipe(ldiv_reg_reg);\n+%}\n+\n+\/\/ Unsigned Integer Remainder\n+\n+instruct UmodI_reg_reg(iRegINoSp dst, iRegIorL2I src1, iRegIorL2I src2) %{\n+  match(Set dst (UModI src1 src2));\n+\n+  ins_cost(INSN_COST * 22);\n+  format %{ \"udivw  rscratch1, $src1, $src2\\n\\t\"\n+            \"msubw  $dst, rscratch1, $src2, $src1\" %}\n+\n+  ins_encode %{\n+    __ udivw(rscratch1, $src1$$Register, $src2$$Register);\n+    __ msubw($dst$$Register, rscratch1, $src2$$Register, $src1$$Register);\n+  %}\n+\n+  ins_pipe(idiv_reg_reg);\n+%}\n+\n+\/\/ Unsigned Long Remainder\n+\n+instruct UModL_reg_reg(iRegLNoSp dst, iRegL src1, iRegL src2) %{\n+  match(Set dst (UModL src1 src2));\n+\n+  ins_cost(INSN_COST * 38);\n+  format %{ \"udiv   rscratch1, $src1, $src2\\n\"\n+            \"msub   $dst, rscratch1, $src2, $src1\" %}\n+\n+  ins_encode %{\n+    __ udiv(rscratch1, $src1$$Register, $src2$$Register);\n+    __ msub($dst$$Register, rscratch1, $src2$$Register, $src1$$Register);\n+  %}\n+\n+  ins_pipe(ldiv_reg_reg);\n+%}\n+\n@@ -11668,0 +11837,102 @@\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegI_reg_URShift_reg(iRegINoSp dst,\n+                              immI0 zero, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (SubI zero (URShiftI src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"negw  $dst, $src1, LSR $src2\" %}\n+\n+  ins_encode %{\n+    __ negw(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::LSR, $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegI_reg_RShift_reg(iRegINoSp dst,\n+                              immI0 zero, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (SubI zero (RShiftI src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"negw  $dst, $src1, ASR $src2\" %}\n+\n+  ins_encode %{\n+    __ negw(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::ASR, $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegI_reg_LShift_reg(iRegINoSp dst,\n+                              immI0 zero, iRegIorL2I src1, immI src2) %{\n+  match(Set dst (SubI zero (LShiftI src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"negw  $dst, $src1, LSL $src2\" %}\n+\n+  ins_encode %{\n+    __ negw(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::LSL, $src2$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegL_reg_URShift_reg(iRegLNoSp dst,\n+                              immL0 zero, iRegL src1, immI src2) %{\n+  match(Set dst (SubL zero (URShiftL src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"neg  $dst, $src1, LSR $src2\" %}\n+\n+  ins_encode %{\n+    __ neg(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::LSR, $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegL_reg_RShift_reg(iRegLNoSp dst,\n+                              immL0 zero, iRegL src1, immI src2) %{\n+  match(Set dst (SubL zero (RShiftL src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"neg  $dst, $src1, ASR $src2\" %}\n+\n+  ins_encode %{\n+    __ neg(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::ASR, $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct NegL_reg_LShift_reg(iRegLNoSp dst,\n+                              immL0 zero, iRegL src1, immI src2) %{\n+  match(Set dst (SubL zero (LShiftL src1 src2)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"neg  $dst, $src1, LSL $src2\" %}\n+\n+  ins_encode %{\n+    __ neg(as_Register($dst$$reg), as_Register($src1$$reg),\n+            Assembler::LSL, $src2$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_shift);\n+%}\n+\n@@ -15107,0 +15378,24 @@\n+instruct round_double_reg(iRegLNoSp dst, vRegD src, vRegD ftmp, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundD src));\n+  effect(TEMP_DEF dst, TEMP ftmp, KILL cr);\n+  format %{ \"java_round_double $dst,$src\"%}\n+  ins_encode %{\n+    __ java_round_double($dst$$Register, as_FloatRegister($src$$reg),\n+                         as_FloatRegister($ftmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_float_reg(iRegINoSp dst, vRegF src, vRegF ftmp, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundF src));\n+  effect(TEMP_DEF dst, TEMP ftmp, KILL cr);\n+  format %{ \"java_round_float $dst,$src\"%}\n+  ins_encode %{\n+    __ java_round_float($dst$$Register, as_FloatRegister($src$$reg),\n+                        as_FloatRegister($ftmp$$reg));\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -15375,1 +15670,5 @@\n-    __ zero_words($base$$Register, (uint64_t)$cnt$$constant);\n+    address tpc = __ zero_words($base$$Register, (uint64_t)$cnt$$constant);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -15427,1 +15726,1 @@\n-  format %{ \"cmn   $op1, $op2\\t# overflow check long\" %}\n+  format %{ \"adds  zr, $op1, $op2\\t# overflow check long\" %}\n@@ -15430,1 +15729,1 @@\n-    __ cmn($op1$$Register, $op2$$constant);\n+    __ adds(zr, $op1$$Register, $op2$$constant);\n@@ -16732,15 +17031,0 @@\n-instruct CallNativeDirect(method meth)\n-%{\n-  match(CallNative);\n-\n-  effect(USE meth);\n-\n-  ins_cost(CALL_COST);\n-\n-  format %{ \"CALL, native $meth\" %}\n-\n-  ins_encode( aarch64_enc_java_to_runtime(meth) );\n-\n-  ins_pipe(pipe_class_call);\n-%}\n-\n@@ -16879,1 +17163,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UU));\n@@ -16889,1 +17173,1 @@\n-                      fnoreg, fnoreg, fnoreg, StrIntrinsicNode::UU);\n+                      fnoreg, fnoreg, fnoreg, pnoreg, pnoreg, StrIntrinsicNode::UU);\n@@ -16897,1 +17181,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LL));\n@@ -16906,1 +17190,1 @@\n-                      fnoreg, fnoreg, fnoreg, StrIntrinsicNode::LL);\n+                      fnoreg, fnoreg, fnoreg, pnoreg, pnoreg, StrIntrinsicNode::LL);\n@@ -16915,1 +17199,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::UL));\n@@ -16926,1 +17210,1 @@\n-                      $vtmp3$$FloatRegister, StrIntrinsicNode::UL);\n+                      $vtmp3$$FloatRegister, pnoreg, pnoreg, StrIntrinsicNode::UL);\n@@ -16935,1 +17219,1 @@\n-  predicate(((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU);\n+  predicate((UseSVE == 0) && (((StrCompNode*)n)->encoding() == StrIntrinsicNode::LU));\n@@ -16946,1 +17230,1 @@\n-                      $vtmp3$$FloatRegister,StrIntrinsicNode::LU);\n+                      $vtmp3$$FloatRegister, pnoreg, pnoreg, StrIntrinsicNode::LU);\n@@ -17351,1 +17635,1 @@\n-\/\/   \/\/ increment preceeded by register-register move\n+\/\/   \/\/ increment preceded by register-register move\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":358,"deletions":74,"binary":false,"changes":432,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -90,1 +90,1 @@\n-  \/\/ on the transistion) Since the callee parameters already account\n+  \/\/ on the transition) Since the callee parameters already account\n@@ -94,1 +94,1 @@\n-         (callee_locals - callee_params) +\n+         (callee_locals - callee_params) * Interpreter::stackElementWords +\n@@ -127,3 +127,3 @@\n-  int max_locals = method->max_locals() * Interpreter::stackElementWords;\n-  int extra_locals = (method->max_locals() - method->size_of_parameters()) *\n-    Interpreter::stackElementWords;\n+  const int max_locals = method->max_locals() * Interpreter::stackElementWords;\n+  const int params = method->size_of_parameters() * Interpreter::stackElementWords;\n+  const int extra_locals = max_locals - params;\n@@ -139,1 +139,1 @@\n-  \/\/ sender_sp is fp+8\/16 (32bit\/64bit) XXX\n+  \/\/ sender_sp is fp+16\n@@ -148,6 +148,3 @@\n-  intptr_t* locals;\n-  if (caller->is_interpreted_frame()) {\n-    locals = caller->interpreter_frame_last_sp() + caller_actual_parameters - 1;\n-  } else {\n-    locals = interpreter_frame->sender_sp() + max_locals - 1;\n-  }\n+  intptr_t* const locals = caller->is_interpreted_frame()\n+    ? caller->interpreter_frame_last_sp() + caller_actual_parameters - 1\n+    : interpreter_frame->sender_sp() + max_locals - 1;\n@@ -172,0 +169,13 @@\n+  \/\/ We have to add extra reserved slots to max_stack. There are 3 users of the extra slots,\n+  \/\/ none of which are at the same time, so we just need to make sure there is enough room\n+  \/\/ for the biggest user:\n+  \/\/   -reserved slot for exception handler\n+  \/\/   -reserved slots for JSR292. Method::extra_stack_entries() is the size.\n+  \/\/   -reserved slots for TraceBytecodes\n+  int max_stack = method->constMethod()->max_stack() + MAX2(3, Method::extra_stack_entries());\n+  intptr_t* extended_sp = (intptr_t*) monbot  -\n+    (max_stack * Interpreter::stackElementWords) -\n+    popframe_extra_args;\n+  extended_sp = align_down(extended_sp, StackAlignmentInBytes);\n+  interpreter_frame->interpreter_frame_set_extended_sp(extended_sp);\n+\n@@ -175,5 +185,2 @@\n-  if (extra_locals != 0 &&\n-      interpreter_frame->sender_sp() ==\n-      interpreter_frame->interpreter_frame_sender_sp()) {\n-    interpreter_frame->set_interpreter_frame_sender_sp(caller->sp() +\n-                                                       extra_locals);\n+  if (extra_locals != 0 && interpreter_frame->sender_sp() == interpreter_frame->interpreter_frame_sender_sp()) {\n+    interpreter_frame->set_interpreter_frame_sender_sp(caller->sp() + extra_locals);\n@@ -181,4 +188,3 @@\n-  *interpreter_frame->interpreter_frame_cache_addr() =\n-    method->constants()->cache();\n-  *interpreter_frame->interpreter_frame_mirror_addr() =\n-    method->method_holder()->java_mirror();\n+\n+  *interpreter_frame->interpreter_frame_cache_addr()  = method->constants()->cache();\n+  *interpreter_frame->interpreter_frame_mirror_addr() = method->method_holder()->java_mirror();\n","filename":"src\/hotspot\/cpu\/aarch64\/abstractInterpreter_aarch64.cpp","additions":28,"deletions":22,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -127,1 +127,1 @@\n-  __ far_call(Address(Runtime1::entry_for(Runtime1::throw_div0_exception_id), relocInfo::runtime_call_type));\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::throw_div0_exception_id)));\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -287,1 +287,1 @@\n-      __ ldr(r19, Address(OSR_buf, slot_offset + 0));\n+      __ ldp(r19, r20, Address(OSR_buf, slot_offset));\n@@ -289,2 +289,1 @@\n-      __ ldr(r19, Address(OSR_buf, slot_offset + 1*BytesPerWord));\n-      __ str(r19, frame_map()->address_for_monitor_object(i));\n+      __ str(r20, frame_map()->address_for_monitor_object(i));\n@@ -383,7 +382,0 @@\n-  \/\/ if the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -408,1 +400,2 @@\n-  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));  __ should_not_reach_here();\n+  __ far_call(RuntimeAddress(Runtime1::entry_for(Runtime1::handle_exception_from_callee_id)));\n+  __ should_not_reach_here();\n@@ -438,1 +431,1 @@\n-  \/\/ Preform needed unlocking\n+  \/\/ Perform needed unlocking\n@@ -476,7 +469,0 @@\n-  \/\/ if the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -1005,1 +991,1 @@\n-         __ ldr(dest->as_register(), as_Address(from_addr));\n+        __ ldr(dest->as_register(), as_Address(from_addr));\n@@ -1753,1 +1739,3 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on aarch64\");\n@@ -2220,0 +2208,1 @@\n+  __ post_call_nop();\n@@ -2230,0 +2219,1 @@\n+  __ post_call_nop();\n@@ -2778,0 +2768,4 @@\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj, -1);\n+    }\n@@ -2907,1 +2901,1 @@\n-  if (offset) __ add(res, res, offset);\n+  __ add(res, res, offset);\n@@ -3141,0 +3135,1 @@\n+  __ post_call_nop();\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":18,"deletions":23,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -217,1 +217,1 @@\n-LIR_Opr LIRGenerator::load_immediate(int x, BasicType type) {\n+LIR_Opr LIRGenerator::load_immediate(jlong x, BasicType type) {\n@@ -227,1 +227,1 @@\n-    r = LIR_OprFact::intConst(x);\n+    r = LIR_OprFact::intConst(checked_cast<jint>(x));\n@@ -359,0 +359,11 @@\n+void LIRGenerator::do_continuation_doYield(Intrinsic* x) {\n+  BasicTypeList signature(0);\n+  CallingConvention* cc = frame_map()->java_calling_convention(&signature, true);\n+\n+  const LIR_Opr result_reg = result_register_for(x->type());\n+  address entry = StubRoutines::cont_doYield();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state());\n+  __ call_runtime(entry, LIR_OprFact::illegalOpr, result_reg, cc->args(), info);\n+  __ move(result_reg, result);\n+}\n@@ -927,1 +938,0 @@\n-  int flags = 0;\n@@ -952,1 +962,1 @@\n-      if(off.result()->is_constant()) {\n+      if (off.result()->is_constant()) {\n@@ -954,1 +964,1 @@\n-       offset += off.result()->as_jint();\n+        offset += off.result()->as_jint();\n@@ -1004,1 +1014,0 @@\n-  int flags = 0;\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRGenerator_aarch64.cpp","additions":16,"deletions":7,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -127,0 +127,1 @@\n+  increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -158,0 +159,1 @@\n+  decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -166,1 +168,1 @@\n-    eden_allocate(obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n+    b(slow_case);\n@@ -330,1 +332,1 @@\n-  bs->nmethod_entry_barrier(this);\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n@@ -380,1 +382,1 @@\n-  bs->nmethod_entry_barrier(this);\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n@@ -415,1 +417,1 @@\n-  \/\/ rbp, + 0: link\n+  \/\/ rfp, + 0: link\n@@ -428,1 +430,1 @@\n-  verify_oop_addr(Address(sp, stack_offset), \"oop\");\n+  verify_oop_addr(Address(sp, stack_offset));\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -662,50 +662,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) &&\n-            !UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Label slow_path;\n-          Register obj_size = r19;\n-          Register t1       = r10;\n-          Register t2       = r11;\n-          assert_different_registers(klass, obj, obj_size, t1, t2);\n-\n-          __ stp(r19, zr, Address(__ pre(sp, -2 * wordSize)));\n-\n-          if (id == fast_new_instance_init_check_id) {\n-            \/\/ make sure the klass is initialized\n-            __ ldrb(rscratch1, Address(klass, InstanceKlass::init_state_offset()));\n-            __ cmpw(rscratch1, InstanceKlass::fully_initialized);\n-            __ br(Assembler::NE, slow_path);\n-          }\n-\n-#ifdef ASSERT\n-          \/\/ assert object can be fast path allocated\n-          {\n-            Label ok, not_ok;\n-            __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));\n-            __ cmp(obj_size, (u1)0);\n-            __ br(Assembler::LE, not_ok);  \/\/ make sure it's an instance (LH > 0)\n-            __ tstw(obj_size, Klass::_lh_instance_slow_path_bit);\n-            __ br(Assembler::EQ, ok);\n-            __ bind(not_ok);\n-            __ stop(\"assert(can be fast path allocated)\");\n-            __ should_not_reach_here();\n-            __ bind(ok);\n-          }\n-#endif \/\/ ASSERT\n-\n-          \/\/ get the instance size (size is postive so movl is fine for 64bit)\n-          __ ldrw(obj_size, Address(klass, Klass::layout_helper_offset()));\n-\n-          __ eden_allocate(obj, obj_size, 0, t1, slow_path);\n-\n-          __ initialize_object(obj, klass, obj_size, 0, t1, t2, \/* is_tlab_allocated *\/ false);\n-          __ verify_oop(obj);\n-          __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));\n-          __ ret(lr);\n-\n-          __ bind(slow_path);\n-          __ ldp(r19, zr, Address(__ post(sp, 2 * wordSize)));\n-        }\n-\n@@ -801,45 +751,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Register arr_size = r5;\n-          Register t1       = r10;\n-          Register t2       = r11;\n-          Label slow_path;\n-          assert_different_registers(length, klass, obj, arr_size, t1, t2);\n-\n-          \/\/ check that array length is small enough for fast path.\n-          __ mov(rscratch1, C1_MacroAssembler::max_array_allocation_length);\n-          __ cmpw(length, rscratch1);\n-          __ br(Assembler::HI, slow_path);\n-\n-          \/\/ get the allocation size: round_up(hdr + length << (layout_helper & 0x1F))\n-          \/\/ since size is positive ldrw does right thing on 64bit\n-          __ ldrw(t1, Address(klass, Klass::layout_helper_offset()));\n-          \/\/ since size is positive movw does right thing on 64bit\n-          __ movw(arr_size, length);\n-          __ lslvw(arr_size, length, t1);\n-          __ ubfx(t1, t1, Klass::_lh_header_size_shift,\n-                  exact_log2(Klass::_lh_header_size_mask + 1));\n-          __ add(arr_size, arr_size, t1);\n-          __ add(arr_size, arr_size, MinObjAlignmentInBytesMask); \/\/ align up\n-          __ andr(arr_size, arr_size, ~MinObjAlignmentInBytesMask);\n-\n-          __ eden_allocate(obj, arr_size, 0, t1, slow_path);  \/\/ preserves arr_size\n-\n-          __ initialize_header(obj, klass, length, t1, t2);\n-          __ ldrb(t1, Address(klass, in_bytes(Klass::layout_helper_offset()) + (Klass::_lh_header_size_shift \/ BitsPerByte)));\n-          assert(Klass::_lh_header_size_shift % BitsPerByte == 0, \"bytewise\");\n-          assert(Klass::_lh_header_size_mask <= 0xFF, \"bytewise\");\n-          __ andr(t1, t1, Klass::_lh_header_size_mask);\n-          __ sub(arr_size, arr_size, t1);  \/\/ body length\n-          __ add(t1, t1, obj);       \/\/ body start\n-          __ initialize_body(t1, arr_size, 0, t1, t2);\n-          __ membar(Assembler::StoreStore);\n-          __ verify_oop(obj);\n-\n-          __ ret(lr);\n-\n-          __ bind(slow_path);\n-        }\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":0,"deletions":95,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -59,0 +59,3 @@\n+  if (is_heap_frame()) {\n+    return true;\n+  }\n@@ -118,0 +121,2 @@\n+    } else if (is_upcall_stub_frame()) {\n+      return fp_safe;\n@@ -161,0 +166,6 @@\n+    if (Continuation::is_return_barrier_entry(sender_pc)) {\n+      \/\/ If our sender_pc is the return barrier, then our \"real\" sender is the continuation entry\n+      frame s = Continuation::continuation_bottom_sender(thread, *this, sender_sp);\n+      sender_sp = s.sp();\n+      sender_pc = s.pc();\n+    }\n@@ -217,0 +228,2 @@\n+    } else if (sender_blob->is_upcall_stub()) {\n+      return false;\n@@ -277,0 +290,1 @@\n+\n@@ -286,0 +300,2 @@\n+  assert(!Continuation::is_return_barrier_entry(pc_old), \"return barrier\");\n+\n@@ -288,1 +304,2 @@\n-  assert(_pc == pc_old || pc == pc_old, \"must be\");\n+  assert(_pc == pc_old || pc == pc_old || pc_old == 0, \"\");\n+  DEBUG_ONLY(address old_pc = _pc;)\n@@ -290,0 +307,1 @@\n+  _pc = pc; \/\/ must be set before call to get_deopt_original_pc\n@@ -292,1 +310,1 @@\n-    assert(original_pc == _pc, \"expected original PC to be stored before patching\");\n+    assert(original_pc == old_pc, \"expected original PC to be stored before patching\");\n@@ -294,1 +312,1 @@\n-    \/\/ leave _pc as is\n+    _pc = original_pc;\n@@ -297,1 +315,0 @@\n-    _pc = pc;\n@@ -301,9 +318,0 @@\n-bool frame::is_interpreted_frame() const  {\n-  return Interpreter::contains(pc());\n-}\n-\n-int frame::frame_size(RegisterMap* map) const {\n-  frame sender = this->sender(map);\n-  return sender.sp() - sp();\n-}\n-\n@@ -336,1 +344,1 @@\n-  BasicObjectLock* result = (BasicObjectLock*) *addr_at(interpreter_frame_monitor_block_top_offset);\n+  BasicObjectLock* result = (BasicObjectLock*) at(interpreter_frame_monitor_block_top_offset);\n@@ -352,0 +360,5 @@\n+\/\/ Used by template based interpreter deoptimization\n+void frame::interpreter_frame_set_extended_sp(intptr_t* sp) {\n+  *((intptr_t**)addr_at(interpreter_frame_extended_sp_offset)) = sp;\n+}\n+\n@@ -361,4 +374,1 @@\n-  if (!jfa->walkable()) {\n-    \/\/ Capture _last_Java_pc (if needed) and mark anchor walkable.\n-    jfa->capture_last_Java_pc();\n-  }\n+  jfa->make_walkable();\n@@ -367,1 +377,0 @@\n-  vmassert(jfa->last_Java_pc() != NULL, \"not walkable\");\n@@ -374,3 +383,5 @@\n-OptimizedEntryBlob::FrameData* OptimizedEntryBlob::frame_data_for_frame(const frame& frame) const {\n-  ShouldNotCallThis();\n-  return nullptr;\n+UpcallStub::FrameData* UpcallStub::frame_data_for_frame(const frame& frame) const {\n+  assert(frame.is_upcall_stub_frame(), \"wrong frame\");\n+  \/\/ need unextended_sp here, since normal sp is wrong for interpreter callees\n+  return reinterpret_cast<UpcallStub::FrameData*>(\n+    reinterpret_cast<address>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n@@ -379,3 +390,5 @@\n-bool frame::optimized_entry_frame_is_first() const {\n-  ShouldNotCallThis();\n-  return false;\n+bool frame::upcall_stub_frame_is_first() const {\n+  assert(is_upcall_stub_frame(), \"must be optimzed entry frame\");\n+  UpcallStub* blob = _cb->as_upcall_stub();\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  return jfa->last_Java_sp() == NULL;\n@@ -384,3 +397,16 @@\n-frame frame::sender_for_optimized_entry_frame(RegisterMap* map) const {\n-  ShouldNotCallThis();\n-  return {};\n+frame frame::sender_for_upcall_stub_frame(RegisterMap* map) const {\n+  assert(map != NULL, \"map must be set\");\n+  UpcallStub* blob = _cb->as_upcall_stub();\n+  \/\/ Java frame called from C; skip all C frames and return top C\n+  \/\/ frame of that chunk as the sender\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  assert(!upcall_stub_frame_is_first(), \"must have a frame anchor to go back to\");\n+  assert(jfa->last_Java_sp() > sp(), \"must be above this frame on stack\");\n+  \/\/ Since we are walking the stack now this nested anchor is obviously walkable\n+  \/\/ even if it wasn't when it was stacked.\n+  jfa->make_walkable();\n+  map->clear();\n+  assert(map->include_argument_oops(), \"should be set by clear\");\n+  frame fr(jfa->last_Java_sp(), jfa->last_Java_fp(), jfa->last_Java_pc());\n+\n+  return fr;\n@@ -405,1 +431,1 @@\n-         \"original PC must be in the main code section of the the compiled method (or must be immediately following it)\");\n+         \"original PC must be in the main code section of the compiled method (or must be immediately following it)\");\n@@ -411,0 +437,1 @@\n+#ifdef ASSERT\n@@ -422,1 +449,1 @@\n-        DEBUG_ONLY(verify_deopt_original_pc(sender_cm, _unextended_sp));\n+        verify_deopt_original_pc(sender_cm, _unextended_sp);\n@@ -427,21 +454,1 @@\n-\n-\/\/------------------------------------------------------------------------------\n-\/\/ frame::update_map_with_saved_link\n-void frame::update_map_with_saved_link(RegisterMap* map, intptr_t** link_addr) {\n-  \/\/ The interpreter and compiler(s) always save fp in a known\n-  \/\/ location on entry. We must record where that location is\n-  \/\/ so that if fp was live on callout from c2 we can find\n-  \/\/ the saved copy no matter what it called.\n-\n-  \/\/ Since the interpreter always saves fp if we record where it is then\n-  \/\/ we don't have to always save fp on entry and exit to c2 compiled\n-  \/\/ code, on entry will be enough.\n-  map->set_location(rfp->as_VMReg(), (address) link_addr);\n-  \/\/ this is weird \"H\" ought to be at a higher address however the\n-  \/\/ oopMaps seems to have the \"H\" regs at the same address and the\n-  \/\/ vanilla register.\n-  \/\/ XXXX make this go away\n-  if (true) {\n-    map->set_location(rfp->as_VMReg()->next(), (address) link_addr);\n-  }\n-}\n+#endif\n@@ -459,0 +466,1 @@\n+  intptr_t* sender_fp = link();\n@@ -469,70 +477,5 @@\n-  return frame(sender_sp, unextended_sp, link(), sender_pc);\n-}\n-\n-\/\/------------------------------------------------------------------------------\n-\/\/ frame::sender_for_compiled_frame\n-frame frame::sender_for_compiled_frame(RegisterMap* map) const {\n-  \/\/ When the sp of a compiled frame is correct, we can get the correct sender sp\n-  \/\/ by unextended sp + frame size.\n-  \/\/ For the following two scenarios, the sp of a compiled frame is correct:\n-  \/\/  a) This compiled frame is built from the anchor.\n-  \/\/  b) This compiled frame is built from a callee frame, and the callee frame can\n-  \/\/    calculate its sp correctly.\n-  \/\/\n-  \/\/ For b), if the callee frame is a native code frame (such as leaf call), the sp of\n-  \/\/ the compiled frame cannot be calculated correctly. There is currently no suitable\n-  \/\/ solution to solve this problem perfectly. But when PreserveFramePointer is enabled,\n-  \/\/ we can get the correct sender sp by fp + 2 (that is sender_sp()).\n-\n-  assert(_cb->frame_size() >= 0, \"must have non-zero frame size\");\n-  intptr_t* l_sender_sp = (!PreserveFramePointer || _sp_is_trusted) ? unextended_sp() + _cb->frame_size()\n-                                                                    : sender_sp();\n-\n-#ifdef ASSERT\n-  address sender_pc_copy = pauth_strip_verifiable((address) *(l_sender_sp-1), (address) *(l_sender_sp-2));\n-#endif\n-\n-  intptr_t** saved_fp_addr = (intptr_t**) (l_sender_sp - frame::sender_sp_offset);\n-\n-  \/\/ assert (sender_sp() == l_sender_sp, \"should be\");\n-  \/\/ assert (*saved_fp_addr == link(), \"should be\");\n-\n-  \/\/ Repair the sender sp if the frame has been extended\n-  l_sender_sp = repair_sender_sp(l_sender_sp, saved_fp_addr);\n-\n-  \/\/ the return_address is always the word on the stack\n-\n-  \/\/ For ROP protection, C1\/C2 will have signed the sender_pc, but there is no requirement to authenticate it here.\n-  \/\/ The return address is always the first word on the stack\n-  address sender_pc = pauth_strip_verifiable((address) *(l_sender_sp-1), (address) *(l_sender_sp-2));\n-\n-#ifdef ASSERT\n-  if (sender_pc != sender_pc_copy) {\n-    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n-    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n-    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n-    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n-    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n-  }\n-#endif\n-\n-  if (map->update_map()) {\n-    \/\/ Tell GC to use argument oopmaps for some runtime stubs that need it.\n-    \/\/ For C1, the runtime stub might not have oop maps, so set this flag\n-    \/\/ outside of update_register_map.\n-    bool caller_args = _cb->caller_must_gc_arguments(map->thread());\n-#ifdef COMPILER1\n-    if (!caller_args) {\n-      nmethod* nm = _cb->as_nmethod_or_null();\n-      if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n-          pc() < nm->verified_inline_entry_point()) {\n-        \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n-        \/\/ before doing any argument shuffling, so we need to scan the oops\n-        \/\/ as the caller passes them.\n-        caller_args = true;\n-      }\n-    }\n-#endif\n-    map->set_include_argument_oops(caller_args);\n-    if (_cb->oop_maps() != NULL) {\n-      OopMapSet::update_register_map(this, map);\n+  if (Continuation::is_return_barrier_entry(sender_pc)) {\n+    if (map->walk_cont()) { \/\/ about to walk into an h-stack\n+      return Continuation::top_frame(*this, map);\n+    } else {\n+      return Continuation::continuation_bottom_sender(map->thread(), *this, sender_sp);\n@@ -540,28 +483,0 @@\n-\n-    \/\/ Since the prolog does the save and restore of FP there is no\n-    \/\/ oopmap for it so we must fill in its location as if there was\n-    \/\/ an oopmap entry since if our caller was compiled code there\n-    \/\/ could be live jvm state in it.\n-    update_map_with_saved_link(map, saved_fp_addr);\n-  }\n-\n-  return frame(l_sender_sp, l_sender_sp, *saved_fp_addr, sender_pc);\n-}\n-\n-\/\/------------------------------------------------------------------------------\n-\/\/ frame::sender_raw\n-frame frame::sender_raw(RegisterMap* map) const {\n-  \/\/ Default is we done have to follow them. The sender_for_xxx will\n-  \/\/ update it accordingly\n-   map->set_include_argument_oops(false);\n-\n-  if (is_entry_frame())\n-    return sender_for_entry_frame(map);\n-  if (is_interpreted_frame())\n-    return sender_for_interpreter_frame(map);\n-  assert(_cb == CodeCache::find_blob(pc()),\"Must be the same\");\n-\n-  \/\/ This test looks odd: why is it not is_compiled_frame() ?  That's\n-  \/\/ because stubs also have OOP maps.\n-  if (_cb != NULL) {\n-    return sender_for_compiled_frame(map);\n@@ -570,17 +485,1 @@\n-  \/\/ Must be native-compiled frame, i.e. the marshaling code for native\n-  \/\/ methods that exists in the core system.\n-\n-  \/\/ Native code may or may not have signed the return address, we have no way to be sure or what\n-  \/\/ signing methods they used. Instead, just ensure the stripped value is used.\n-\n-  return frame(sender_sp(), link(), sender_pc());\n-}\n-\n-frame frame::sender(RegisterMap* map) const {\n-  frame result = sender_raw(map);\n-\n-  if (map->process_frames()) {\n-    StackWatermarkSet::on_iteration(map->thread(), result);\n-  }\n-\n-  return result;\n+  return frame(sender_sp, unextended_sp, sender_fp, sender_pc);\n@@ -696,1 +595,0 @@\n-\n@@ -713,0 +611,1 @@\n+    DESCRIBE_FP_OFFSET(interpreter_frame_extended_sp);\n@@ -719,0 +618,16 @@\n+\n+  if (is_java_frame() || Continuation::is_continuation_enterSpecial(*this)) {\n+    intptr_t* ret_pc_loc;\n+    intptr_t* fp_loc;\n+    if (is_interpreted_frame()) {\n+      ret_pc_loc = fp() + return_addr_offset;\n+      fp_loc = fp();\n+    } else {\n+      ret_pc_loc = real_fp() - return_addr_offset;\n+      fp_loc = real_fp() - sender_sp_offset;\n+    }\n+    address ret_pc = *(address*)ret_pc_loc;\n+    values.describe(frame_no, ret_pc_loc,\n+      Continuation::is_return_barrier_entry(ret_pc) ? \"return address (return barrier)\" : \"return address\");\n+    values.describe(-1, fp_loc, \"saved fp\", 0); \/\/ \"unowned\" as value belongs to sender\n+  }\n@@ -727,13 +642,0 @@\n-intptr_t* frame::real_fp() const {\n-  if (_cb != NULL) {\n-    \/\/ use the frame size if valid\n-    int size = _cb->frame_size();\n-    if (size > 0) {\n-      return unextended_sp() + size;\n-    }\n-  }\n-  \/\/ else rely on fp()\n-  assert(! is_compiled_frame(), \"unknown compiled frame size\");\n-  return fp();\n-}\n-\n@@ -781,0 +683,2 @@\n+  DESCRIBE_FP_OFFSET(interpreter_frame_extended_sp);\n+  DESCRIBE_FP_OFFSET(interpreter_frame_mirror);\n@@ -842,1 +746,4 @@\n-    ::new (reg_map) RegisterMap((JavaThread*)thread, false);\n+    ::new (reg_map) RegisterMap((JavaThread*)thread,\n+                                RegisterMap::UpdateMap::skip,\n+                                RegisterMap::ProcessFrames::include,\n+                                RegisterMap::WalkContinuation::skip);\n@@ -844,1 +751,4 @@\n-    *reg_map = RegisterMap((JavaThread*)thread, false);\n+    *reg_map = RegisterMap((JavaThread*)thread,\n+                           RegisterMap::UpdateMap::skip,\n+                           RegisterMap::ProcessFrames::include,\n+                           RegisterMap::WalkContinuation::skip);\n@@ -889,1 +799,1 @@\n-void JavaFrameAnchor::make_walkable(JavaThread* thread) {\n+void JavaFrameAnchor::make_walkable() {\n@@ -894,1 +804,0 @@\n-  vmassert(Thread::current() == (Thread*)thread, \"not current thread\");\n@@ -897,7 +806,1 @@\n-  capture_last_Java_pc();\n-  vmassert(walkable(), \"something went wrong\");\n-}\n-\n-void JavaFrameAnchor::capture_last_Java_pc() {\n-  vmassert(_last_Java_sp != NULL, \"no last frame set\");\n-  vmassert(_last_Java_pc == NULL, \"already walkable\");\n+  vmassert(walkable(), \"something went wrong\");\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":94,"deletions":191,"binary":false,"changes":285,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-\/\/    [padding               ]\n+\/\/    [extended SP           ]                                          extended_sp offset\n@@ -56,1 +56,1 @@\n-\/\/    [old stack pointer     ]                     (sender_sp)          sender_sp_offset\n+\/\/    [sender's SP           ]                     (sender_sp)          sender_sp_offset\n@@ -85,2 +85,2 @@\n-    interpreter_frame_padding_offset                 = interpreter_frame_mdp_offset - 1,\n-    interpreter_frame_mirror_offset                  = interpreter_frame_padding_offset - 1,\n+    interpreter_frame_extended_sp_offset             = interpreter_frame_mdp_offset - 1,\n+    interpreter_frame_mirror_offset                  = interpreter_frame_extended_sp_offset - 1,\n@@ -102,2 +102,8 @@\n-    arg_reg_save_area_bytes                          =  0\n-\n+    arg_reg_save_area_bytes                          =  0,\n+\n+    \/\/ size, in words, of frame metadata (e.g. pc and link)\n+    metadata_words                                   = sender_sp_offset,\n+    \/\/ in bytes\n+    frame_alignment                                  = 16,\n+    \/\/ size, in words, of maximum shift in frame position due to alignment\n+    align_wiggle                                     =  1\n@@ -116,1 +122,4 @@\n-  intptr_t*   _fp; \/\/ frame pointer\n+  union {\n+    intptr_t*  _fp; \/\/ frame pointer\n+    int _offset_fp; \/\/ relative frame pointer for use in stack-chunk frames\n+  };\n@@ -124,2 +133,6 @@\n-  intptr_t*     _unextended_sp;\n-  void adjust_unextended_sp();\n+  union {\n+    intptr_t* _unextended_sp;\n+    int _offset_unextended_sp; \/\/ for use in stack-chunk frames\n+  };\n+\n+  void adjust_unextended_sp() NOT_DEBUG_RETURN;\n@@ -144,0 +157,2 @@\n+  const ImmutableOopMap* get_oop_map() const;\n+\n@@ -151,0 +166,4 @@\n+  frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc, CodeBlob* cb);\n+  \/\/ used for fast frame construction by continuations\n+  frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc, CodeBlob* cb, const ImmutableOopMap* oop_map, bool on_heap);\n+\n@@ -154,0 +173,1 @@\n+  void setup(address pc);\n@@ -157,1 +177,4 @@\n-  intptr_t*   fp() const { return _fp; }\n+  intptr_t*   fp() const        { assert_absolute(); return _fp; }\n+  void set_fp(intptr_t* newfp)  { _fp = newfp; }\n+  int offset_fp() const         { assert_offset();  return _offset_fp; }\n+  void set_offset_fp(int value) { assert_on_heap(); _offset_fp = value; }\n@@ -165,2 +188,4 @@\n-  \/\/ helper to update a map with callee-saved RBP\n-  static void update_map_with_saved_link(RegisterMap* map, intptr_t** link_addr);\n+  void interpreter_frame_set_extended_sp(intptr_t* sp);\n+\n+  template <typename RegisterMapT>\n+  static void update_map_with_saved_link(RegisterMapT* map, intptr_t** link_addr);\n@@ -174,1 +199,1 @@\n-  frame sender_raw(RegisterMap* map) const;\n+  inline frame sender_raw(RegisterMap* map) const;\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.hpp","additions":38,"deletions":13,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -29,1 +29,2 @@\n-#include \"code\/codeCache.hpp\"\n+#include \"code\/codeBlob.inline.hpp\"\n+#include \"code\/codeCache.inline.hpp\"\n@@ -31,0 +32,3 @@\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"interpreter\/oopMapCache.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -32,0 +36,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -45,0 +52,2 @@\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n@@ -57,0 +66,4 @@\n+  _oop_map = NULL;\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n+\n@@ -59,0 +72,4 @@\n+  setup(pc);\n+}\n+\n+inline void frame::setup(address pc) {\n@@ -65,0 +82,2 @@\n+    assert(_cb == NULL || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+           \"original PC must be in the main code section of the compiled method (or must be immediately following it)\");\n@@ -66,1 +85,5 @@\n-    _deopt_state = not_deoptimized;\n+    if (_cb == SharedRuntime::deopt_blob()) {\n+      _deopt_state = is_deoptimized;\n+    } else {\n+      _deopt_state = not_deoptimized;\n+    }\n@@ -75,1 +98,1 @@\n-inline frame::frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc) {\n+inline frame::frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc, CodeBlob* cb) {\n@@ -84,2 +107,5 @@\n-  _cb = CodeCache::find_blob(pc);\n-  adjust_unextended_sp();\n+  _cb = cb;\n+  _oop_map = NULL;\n+  assert(_cb != NULL, \"pc: \" INTPTR_FORMAT, p2i(pc));\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n@@ -87,9 +113,11 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n-  if (original_pc != NULL) {\n-    _pc = original_pc;\n-    assert(_cb->as_compiled_method()->insts_contains_inclusive(_pc),\n-           \"original PC must be in the main code section of the the compiled method (or must be immediately following it)\");\n-    _deopt_state = is_deoptimized;\n-  } else {\n-    _deopt_state = not_deoptimized;\n-  }\n+  setup(pc);\n+}\n+\n+inline frame::frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc, CodeBlob* cb, const ImmutableOopMap* oop_map, bool on_heap) {\n+  _sp = sp;\n+  _unextended_sp = unextended_sp;\n+  _fp = fp;\n+  _pc = pc;\n+  _cb = cb;\n+  _oop_map = oop_map;\n+  _deopt_state = not_deoptimized;\n@@ -97,0 +125,15 @@\n+  _on_heap = on_heap;\n+  DEBUG_ONLY(_frame_index = -1;)\n+\n+  \/\/ In thaw, non-heap frames use this constructor to pass oop_map.  I don't know why.\n+  assert(_on_heap || _cb != nullptr, \"these frames are always heap frames\");\n+  if (cb != NULL) {\n+    setup(pc);\n+  }\n+#ifdef ASSERT\n+  \/\/ The following assertion has been disabled because it would sometime trap for Continuation.run,\n+  \/\/ which is not *in* a continuation and therefore does not clear the _cont_fastpath flag, but this\n+  \/\/ is benign even in fast mode (see Freeze::setup_jump)\n+  \/\/ We might freeze deoptimized frame in slow mode\n+  \/\/ assert(_pc == pc && _deopt_state == not_deoptimized, \"\");\n+#endif\n@@ -99,0 +142,18 @@\n+inline frame::frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc) {\n+  intptr_t a = intptr_t(sp);\n+  intptr_t b = intptr_t(fp);\n+  _sp = sp;\n+  _unextended_sp = unextended_sp;\n+  _fp = fp;\n+  _pc = pc;\n+  _cb = CodeCache::find_blob_fast(pc);\n+  _oop_map = NULL;\n+  assert(_cb != NULL, \"pc: \" INTPTR_FORMAT \" sp: \" INTPTR_FORMAT \" unextended_sp: \" INTPTR_FORMAT \" fp: \" INTPTR_FORMAT, p2i(pc), p2i(sp), p2i(unextended_sp), p2i(fp));\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n+\n+  setup(pc);\n+}\n+\n+inline frame::frame(intptr_t* sp) : frame(sp, sp, *(intptr_t**)(sp - frame::sender_sp_offset), *(address*)(sp - 1)) {}\n+\n@@ -106,0 +167,2 @@\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n@@ -158,1 +221,35 @@\n-inline intptr_t* frame::unextended_sp() const     { return _unextended_sp; }\n+inline intptr_t* frame::unextended_sp() const          { assert_absolute(); return _unextended_sp; }\n+inline void frame::set_unextended_sp(intptr_t* value)  { _unextended_sp = value; }\n+inline int  frame::offset_unextended_sp() const        { assert_offset();   return _offset_unextended_sp; }\n+inline void frame::set_offset_unextended_sp(int value) { assert_on_heap();  _offset_unextended_sp = value; }\n+\n+inline intptr_t* frame::real_fp() const {\n+  if (_cb != NULL) {\n+    \/\/ use the frame size if valid\n+    int size = _cb->frame_size();\n+    if (size > 0) {\n+      return unextended_sp() + size;\n+    }\n+  }\n+  \/\/ else rely on fp()\n+  assert(! is_compiled_frame(), \"unknown compiled frame size\");\n+  return fp();\n+}\n+\n+inline int frame::frame_size() const {\n+  return is_interpreted_frame()\n+    ? sender_sp() - sp()\n+    : cb()->frame_size();\n+}\n+\n+inline int frame::compiled_frame_stack_argsize() const {\n+  assert(cb()->is_compiled(), \"\");\n+  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+}\n+\n+inline void frame::interpreted_frame_oop_map(InterpreterOopMap* mask) const {\n+  assert(mask != NULL, \"\");\n+  Method* m = interpreter_frame_method();\n+  int   bci = interpreter_frame_bci();\n+  m->mask_for(bci, mask); \/\/ OopMapCache::compute_one_oop_map(m, bci, mask);\n+}\n@@ -173,1 +270,1 @@\n-  return *(intptr_t**)addr_at(interpreter_frame_last_sp_offset);\n+  return (intptr_t*)at(interpreter_frame_last_sp_offset);\n@@ -247,1 +344,1 @@\n-  oop* result_adr = (oop *)map->location(r0->as_VMReg());\n+  oop* result_adr = (oop *)map->location(r0->as_VMReg(), sp());\n@@ -250,2 +347,1 @@\n-\n-  return (*result_adr);\n+  return *result_adr;\n@@ -257,1 +353,1 @@\n-  oop* result_adr = (oop *)map->location(r0->as_VMReg());\n+  oop* result_adr = (oop *)map->location(r0->as_VMReg(), sp());\n@@ -264,0 +360,153 @@\n+inline bool frame::is_interpreted_frame() const {\n+  return Interpreter::contains(pc());\n+}\n+\n+inline int frame::sender_sp_ret_address_offset() {\n+  return frame::sender_sp_offset - frame::return_addr_offset;\n+}\n+\n+inline const ImmutableOopMap* frame::get_oop_map() const {\n+  if (_cb == NULL) return NULL;\n+  if (_cb->oop_maps() != NULL) {\n+    NativePostCallNop* nop = nativePostCallNop_at(_pc);\n+    if (nop != NULL && nop->displacement() != 0) {\n+      int slot = ((nop->displacement() >> 24) & 0xff);\n+      return _cb->oop_map_for_slot(slot, _pc);\n+    }\n+    const ImmutableOopMap* oop_map = OopMapSet::find_map(this);\n+    return oop_map;\n+  }\n+  return NULL;\n+}\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::sender\n+inline frame frame::sender(RegisterMap* map) const {\n+  frame result = sender_raw(map);\n+\n+  if (map->process_frames() && !map->in_cont()) {\n+    StackWatermarkSet::on_iteration(map->thread(), result);\n+  }\n+\n+  return result;\n+}\n+\n+inline frame frame::sender_raw(RegisterMap* map) const {\n+  \/\/ Default is we done have to follow them. The sender_for_xxx will\n+  \/\/ update it accordingly\n+  map->set_include_argument_oops(false);\n+\n+  if (map->in_cont()) { \/\/ already in an h-stack\n+    return map->stack_chunk()->sender(*this, map);\n+  }\n+\n+  if (is_entry_frame())       return sender_for_entry_frame(map);\n+  if (is_upcall_stub_frame()) return sender_for_upcall_stub_frame(map);\n+  if (is_interpreted_frame()) return sender_for_interpreter_frame(map);\n+\n+  assert(_cb == CodeCache::find_blob(pc()), \"Must be the same\");\n+  if (_cb != NULL) return sender_for_compiled_frame(map);\n+\n+  \/\/ Must be native-compiled frame, i.e. the marshaling code for native\n+  \/\/ methods that exists in the core system.\n+\n+  \/\/ Native code may or may not have signed the return address, we have no way to be sure or what\n+  \/\/ signing methods they used. Instead, just ensure the stripped value is used.\n+\n+  return frame(sender_sp(), link(), sender_pc());\n+}\n+\n+inline frame frame::sender_for_compiled_frame(RegisterMap* map) const {\n+  \/\/ we cannot rely upon the last fp having been saved to the thread\n+  \/\/ in C2 code but it will have been pushed onto the stack. so we\n+  \/\/ have to find it relative to the unextended sp\n+\n+  assert(_cb->frame_size() >= 0, \"must have non-zero frame size\");\n+  intptr_t* l_sender_sp = (!PreserveFramePointer || _sp_is_trusted) ? unextended_sp() + _cb->frame_size()\n+                                                                    : sender_sp();\n+#ifdef ASSERT\n+   address sender_pc_copy = pauth_strip_verifiable((address) *(l_sender_sp-1), (address) *(l_sender_sp-2));\n+#endif\n+\n+  assert(!_sp_is_trusted || l_sender_sp == real_fp(), \"\");\n+\n+  intptr_t** saved_fp_addr = (intptr_t**) (l_sender_sp - frame::sender_sp_offset);\n+\n+  \/\/ Repair the sender sp if the frame has been extended\n+  l_sender_sp = repair_sender_sp(l_sender_sp, saved_fp_addr);\n+\n+  \/\/ the return_address is always the word on the stack\n+  \/\/ For ROP protection, C1\/C2 will have signed the sender_pc, but there is no requirement to authenticate it here.\n+  address sender_pc = pauth_strip_verifiable((address) *(l_sender_sp-1), (address) *(l_sender_sp-2));\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n+  if (map->update_map()) {\n+    \/\/ Tell GC to use argument oopmaps for some runtime stubs that need it.\n+    \/\/ For C1, the runtime stub might not have oop maps, so set this flag\n+    \/\/ outside of update_register_map.\n+    bool c1_buffering = false;\n+#ifdef COMPILER1\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+        pc() < nm->verified_inline_entry_point()) {\n+      \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+      \/\/ before doing any argument shuffling, so we need to scan the oops\n+      \/\/ as the caller passes them.\n+      c1_buffering = true;\n+    }\n+#endif\n+    if (!_cb->is_compiled() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+      bool caller_args = _cb->caller_must_gc_arguments(map->thread()) || c1_buffering;\n+      map->set_include_argument_oops(caller_args);\n+      if (oop_map() != NULL) {\n+        _oop_map->update_register_map(this, map);\n+      }\n+    } else {\n+      assert(!_cb->caller_must_gc_arguments(map->thread()), \"\");\n+      assert(!map->include_argument_oops(), \"\");\n+      assert(oop_map() == NULL || !oop_map()->has_any(OopMapValue::callee_saved_value), \"callee-saved value in compiled frame\");\n+    }\n+\n+    \/\/ Since the prolog does the save and restore of FP there is no oopmap\n+    \/\/ for it so we must fill in its location as if there was an oopmap entry\n+    \/\/ since if our caller was compiled code there could be live jvm state in it.\n+    update_map_with_saved_link(map, saved_fp_addr);\n+  }\n+\n+  if (Continuation::is_return_barrier_entry(sender_pc)) {\n+    if (map->walk_cont()) { \/\/ about to walk into an h-stack\n+      return Continuation::top_frame(*this, map);\n+    } else {\n+      return Continuation::continuation_bottom_sender(map->thread(), *this, l_sender_sp);\n+    }\n+  }\n+\n+  intptr_t* unextended_sp = l_sender_sp;\n+  return frame(l_sender_sp, unextended_sp, *saved_fp_addr, sender_pc);\n+}\n+\n+template <typename RegisterMapT>\n+void frame::update_map_with_saved_link(RegisterMapT* map, intptr_t** link_addr) {\n+  \/\/ The interpreter and compiler(s) always save FP in a known\n+  \/\/ location on entry. C2-compiled code uses FP as an allocatable\n+  \/\/ callee-saved register. We must record where that location is so\n+  \/\/ that if FP was live on callout from c2 we can find the saved copy.\n+\n+  map->set_location(rfp->as_VMReg(), (address) link_addr);\n+  \/\/ this is weird \"H\" ought to be at a higher address however the\n+  \/\/ oopMaps seems to have the \"H\" regs at the same address and the\n+  \/\/ vanilla register.\n+  \/\/ XXXX make this go away\n+  if (true) {\n+    map->set_location(rfp->as_VMReg()->next(), (address) link_addr);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.inline.hpp","additions":269,"deletions":20,"binary":false,"changes":289,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -35,1 +36,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -161,2 +161,0 @@\n-  if (tosca_live) saved += RegSet::of(r0);\n-  if (obj != noreg) saved += RegSet::of(obj);\n@@ -168,0 +166,2 @@\n+    if (tosca_live) saved += RegSet::of(r0);\n+    if (obj != noreg) saved += RegSet::of(obj);\n@@ -173,1 +173,0 @@\n-  }\n@@ -175,2 +174,5 @@\n-  __ push(saved, sp);\n-  __ push_fp(fsaved, sp);\n+    __ push(saved, sp);\n+    __ push_fp(fsaved, sp);\n+  } else {\n+    __ push_call_clobbered_registers();\n+  }\n@@ -183,1 +185,1 @@\n-  \/\/ intrinsified Reference.get() routine) then ebp might be pointing to\n+  \/\/ intrinsified Reference.get() routine) then rfp might be pointing to\n@@ -197,0 +199,1 @@\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n@@ -199,0 +202,3 @@\n+  } else {\n+    __ pop_call_clobbered_registers();\n+  }\n@@ -441,9 +447,0 @@\n-  \/\/ Is marking still active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ ldrw(tmp, in_progress);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ ldrb(tmp, in_progress);\n-  }\n-  __ cbzw(tmp, done);\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/g1\/g1BarrierSetAssembler_aarch64.cpp","additions":13,"deletions":16,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -37,1 +38,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -193,57 +193,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void BarrierSetAssembler::eden_allocate(MacroAssembler* masm, Register obj,\n-                                        Register var_size_in_bytes,\n-                                        int con_size_in_bytes,\n-                                        Register t1,\n-                                        Label& slow_case) {\n-  assert_different_registers(obj, var_size_in_bytes, t1);\n-  if (!Universe::heap()->supports_inline_contig_alloc()) {\n-    __ b(slow_case);\n-  } else {\n-    Register end = t1;\n-    Register heap_end = rscratch2;\n-    Label retry;\n-    __ bind(retry);\n-    {\n-      uint64_t offset;\n-      __ adrp(rscratch1, ExternalAddress((address) Universe::heap()->end_addr()), offset);\n-      __ ldr(heap_end, Address(rscratch1, offset));\n-    }\n-\n-    ExternalAddress heap_top((address) Universe::heap()->top_addr());\n-\n-    \/\/ Get the current top of the heap\n-    {\n-      uint64_t offset;\n-      __ adrp(rscratch1, heap_top, offset);\n-      \/\/ Use add() here after ARDP, rather than lea().\n-      \/\/ lea() does not generate anything if its offset is zero.\n-      \/\/ However, relocs expect to find either an ADD or a load\/store\n-      \/\/ insn after an ADRP.  add() always generates an ADD insn, even\n-      \/\/ for add(Rn, Rn, 0).\n-      __ add(rscratch1, rscratch1, offset);\n-      __ ldaxr(obj, rscratch1);\n-    }\n-\n-    \/\/ Adjust it my the size of our new object\n-    if (var_size_in_bytes == noreg) {\n-      __ lea(end, Address(obj, con_size_in_bytes));\n-    } else {\n-      __ lea(end, Address(obj, var_size_in_bytes));\n-    }\n-\n-    \/\/ if end < obj then we wrapped around high memory\n-    __ cmp(end, obj);\n-    __ br(Assembler::LO, slow_case);\n-\n-    __ cmp(end, heap_end);\n-    __ br(Assembler::HI, slow_case);\n-\n-    \/\/ If heap_top hasn't been changed by some other thread, update it.\n-    __ stlxr(rscratch2, end, rscratch1);\n-    __ cbnzw(rscratch2, retry);\n-\n-    incr_allocated_bytes(masm, var_size_in_bytes, con_size_in_bytes, t1);\n-  }\n-}\n-\n@@ -265,1 +208,15 @@\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+static volatile uint32_t _patching_epoch = 0;\n+\n+address BarrierSetAssembler::patching_epoch_addr() {\n+  return (address)&_patching_epoch;\n+}\n+\n+void BarrierSetAssembler::increment_patching_epoch() {\n+  Atomic::inc(&_patching_epoch);\n+}\n+\n+void BarrierSetAssembler::clear_patching_epoch() {\n+  _patching_epoch = 0;\n+}\n+\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation, Label* guard) {\n@@ -272,2 +229,3 @@\n-  Label skip, guard;\n-  Address thread_disarmed_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+  Label local_guard;\n+  Label skip_barrier;\n+  NMethodPatchingType patching_type = nmethod_patching_type();\n@@ -275,1 +233,3 @@\n-  __ ldrw(rscratch1, guard);\n+  if (slow_path == NULL) {\n+    guard = &local_guard;\n+  }\n@@ -277,6 +237,47 @@\n-  \/\/ Subsequent loads of oops must occur after load of guard value.\n-  \/\/ BarrierSetNMethod::disarm sets guard with release semantics.\n-  __ membar(__ LoadLoad);\n-  __ ldrw(rscratch2, thread_disarmed_addr);\n-  __ cmpw(rscratch1, rscratch2);\n-  __ br(Assembler::EQ, skip);\n+  \/\/ If the slow path is out of line in a stub, we flip the condition\n+  Assembler::Condition condition = slow_path == NULL ? Assembler::EQ : Assembler::NE;\n+  Label& barrier_target = slow_path == NULL ? skip_barrier : *slow_path;\n+\n+  __ ldrw(rscratch1, *guard);\n+\n+  if (patching_type == NMethodPatchingType::stw_instruction_and_data_patch) {\n+    \/\/ With STW patching, no data or instructions are updated concurrently,\n+    \/\/ which means there isn't really any need for any fencing for neither\n+    \/\/ data nor instruction modifications happening concurrently. The\n+    \/\/ instruction patching is handled with isb fences on the way back\n+    \/\/ from the safepoint to Java. So here we can do a plain conditional\n+    \/\/ branch with no fencing.\n+    Address thread_disarmed_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+    __ ldrw(rscratch2, thread_disarmed_addr);\n+    __ cmp(rscratch1, rscratch2);\n+  } else if (patching_type == NMethodPatchingType::conc_instruction_and_data_patch) {\n+    \/\/ If we patch code we need both a code patching and a loadload\n+    \/\/ fence. It's not super cheap, so we use a global epoch mechanism\n+    \/\/ to hide them in a slow path.\n+    \/\/ The high level idea of the global epoch mechanism is to detect\n+    \/\/ when any thread has performed the required fencing, after the\n+    \/\/ last nmethod was disarmed. This implies that the required\n+    \/\/ fencing has been performed for all preceding nmethod disarms\n+    \/\/ as well. Therefore, we do not need any further fencing.\n+    __ lea(rscratch2, ExternalAddress((address)&_patching_epoch));\n+    \/\/ Embed an artificial data dependency to order the guard load\n+    \/\/ before the epoch load.\n+    __ orr(rscratch2, rscratch2, rscratch1, Assembler::LSR, 32);\n+    \/\/ Read the global epoch value.\n+    __ ldrw(rscratch2, rscratch2);\n+    \/\/ Combine the guard value (low order) with the epoch value (high order).\n+    __ orr(rscratch1, rscratch1, rscratch2, Assembler::LSL, 32);\n+    \/\/ Compare the global values with the thread-local values.\n+    Address thread_disarmed_and_epoch_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+    __ ldr(rscratch2, thread_disarmed_and_epoch_addr);\n+    __ cmp(rscratch1, rscratch2);\n+  } else {\n+    assert(patching_type == NMethodPatchingType::conc_data_patch, \"must be\");\n+    \/\/ Subsequent loads of oops must occur after load of guard value.\n+    \/\/ BarrierSetNMethod::disarm sets guard with release semantics.\n+    __ membar(__ LoadLoad);\n+    Address thread_disarmed_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+    __ ldrw(rscratch2, thread_disarmed_addr);\n+    __ cmpw(rscratch1, rscratch2);\n+  }\n+  __ br(condition, barrier_target);\n@@ -284,3 +285,4 @@\n-  __ movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n-  __ blr(rscratch1);\n-  __ b(skip);\n+  if (slow_path == NULL) {\n+    __ movptr(rscratch1, (uintptr_t) StubRoutines::aarch64::method_entry_barrier());\n+    __ blr(rscratch1);\n+    __ b(skip_barrier);\n@@ -288,1 +290,1 @@\n-  __ bind(guard);\n+    __ bind(local_guard);\n@@ -290,1 +292,4 @@\n-  __ emit_int32(0);   \/\/ nmethod guard value. Skipped over in common case.\n+    __ emit_int32(0);   \/\/ nmethod guard value. Skipped over in common case.\n+  } else {\n+    __ bind(*continuation);\n+  }\n@@ -292,1 +297,1 @@\n-  __ bind(skip);\n+  __ bind(skip_barrier);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":80,"deletions":75,"binary":false,"changes":155,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,6 @@\n+enum class NMethodPatchingType {\n+  stw_instruction_and_data_patch,\n+  conc_instruction_and_data_patch,\n+  conc_data_patch\n+};\n+\n@@ -65,7 +71,0 @@\n-  void eden_allocate(MacroAssembler* masm,\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n@@ -74,1 +73,3 @@\n-  virtual void nmethod_entry_barrier(MacroAssembler* masm);\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::stw_instruction_and_data_patch; }\n+\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation, Label* guard);\n@@ -77,0 +78,3 @@\n+  static address patching_epoch_addr();\n+  static void clear_patching_epoch();\n+  static void increment_patching_epoch();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":13,"deletions":9,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -37,1 +38,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -162,1 +162,1 @@\n-  \/\/ intrinsified Reference.get() routine) then ebp might be pointing to\n+  \/\/ intrinsified Reference.get() routine) then rfp might be pointing to\n@@ -189,1 +189,1 @@\n-\/\/ IMPORTANT: This must preserve all registers, even rscratch1 and rscratch2, except those explicitely\n+\/\/ IMPORTANT: This must preserve all registers, even rscratch1 and rscratch2, except those explicitly\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -65,0 +65,2 @@\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -241,1 +241,1 @@\n-  \/\/ Set up SP to accomodate parameters and maybe r0..\n+  \/\/ Set up SP to accommodate parameters and maybe r0..\n@@ -256,1 +256,1 @@\n-  __ verify_oop(r0, \"Bad oop\");\n+  __ verify_oop(r0);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -80,0 +80,2 @@\n+  virtual NMethodPatchingType nmethod_patching_type() { return NMethodPatchingType::conc_data_patch; }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zBarrierSetAssembler_aarch64.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -56,0 +56,2 @@\n+define_pd_global(bool, VMContinuations, true);\n+\n@@ -97,1 +99,1 @@\n-          \"Use simpliest and shortest implementation for array equals\") \\\n+          \"Use simplest and shortest implementation for array equals\")  \\\n@@ -111,1 +113,1 @@\n-          range(1, max_jint)                                            \\\n+          range(wordSize, max_jint)                                     \\\n","filename":"src\/hotspot\/cpu\/aarch64\/globals_aarch64.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,0 +44,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -46,1 +47,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -121,1 +121,1 @@\n-               verify_oop(r0, state);               break;\n+               interp_verify_oop(r0, state);        break;\n@@ -174,1 +174,6 @@\n-  lea(rdispatch, Address(rdispatch, offset));\n+  \/\/ Use add() here after ARDP, rather than lea().\n+  \/\/ lea() does not generate anything if its offset is zero.\n+  \/\/ However, relocs expect to find either an ADD or a load\/store\n+  \/\/ insn after an ADRP.  add() always generates an ADD insn, even\n+  \/\/ for add(Rn, Rn, 0).\n+  add(rdispatch, rdispatch, offset);\n@@ -454,1 +459,1 @@\n-  verify_oop(r0, state);\n+  interp_verify_oop(r0, state);\n@@ -458,1 +463,1 @@\n-  verify_oop(r0, state);\n+  interp_verify_oop(r0, state);\n@@ -493,1 +498,1 @@\n-  mov(r13, sp);\n+  mov(r19_sender_sp, sp);\n@@ -536,1 +541,1 @@\n-    verify_oop(r0, state);\n+    interp_verify_oop(r0, state);\n@@ -595,1 +600,1 @@\n-\/\/ Unlock any Java monitors from syncronized blocks.\n+\/\/ Unlock any Java monitors from synchronized blocks.\n@@ -836,1 +841,1 @@\n-    Label done;\n+    Label count, done;\n@@ -874,1 +879,1 @@\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n@@ -911,1 +916,1 @@\n-    br(Assembler::EQ, done);\n+    br(Assembler::EQ, count);\n@@ -919,0 +924,4 @@\n+    b(done);\n+\n+    bind(count);\n+    increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -943,1 +952,1 @@\n-    Label done;\n+    Label count, done;\n@@ -966,1 +975,1 @@\n-    cbz(header_reg, done);\n+    cbz(header_reg, count);\n@@ -969,1 +978,1 @@\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, done, \/*fallthrough*\/NULL);\n+    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n@@ -974,0 +983,1 @@\n+    b(done);\n@@ -975,1 +985,2 @@\n-    bind(done);\n+    bind(count);\n+    decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -977,0 +988,1 @@\n+    bind(done);\n@@ -1658,1 +1670,1 @@\n-void InterpreterMacroAssembler::verify_oop(Register reg, TosState state) {\n+void InterpreterMacroAssembler::_interp_verify_oop(Register reg, TosState state, const char* file, int line) {\n@@ -1660,1 +1672,1 @@\n-    MacroAssembler::verify_oop(reg);\n+    MacroAssembler::_verify_oop_checked(reg, \"broken oop\", file, line);\n@@ -1937,1 +1949,1 @@\n-      \/\/ begining of the ProfileData we intend to update to check its\n+      \/\/ beginning of the ProfileData we intend to update to check its\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":31,"deletions":19,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-\/\/ This file specializes the assember with interpreter-specific macros\n+\/\/ This file specializes the assembler with interpreter-specific macros\n@@ -85,0 +85,25 @@\n+  void restore_sp_after_call() {\n+    Label L;\n+    ldr(rscratch1, Address(rfp, frame::interpreter_frame_extended_sp_offset * wordSize));\n+#ifdef ASSERT\n+    cbnz(rscratch1, L);\n+    stop(\"SP is null\");\n+#endif\n+    bind(L);\n+    mov(sp, rscratch1);\n+  }\n+\n+  void check_extended_sp(const char* msg = \"check extended SP\") {\n+#ifdef ASSERT\n+    Label L;\n+    ldr(rscratch1, Address(rfp, frame::interpreter_frame_extended_sp_offset * wordSize));\n+    cmp(sp, rscratch1);\n+    br(EQ, L);\n+    stop(msg);\n+    bind(L);\n+#endif\n+  }\n+\n+#define check_extended_sp()                                             \\\n+  check_extended_sp(\"SP does not match extended SP in frame at \" __FILE__ \":\" XSTR(__LINE__))\n+\n@@ -305,1 +330,2 @@\n-  void verify_oop(Register reg, TosState state = atos);\n+#define interp_verify_oop(reg, state) _interp_verify_oop(reg, state, __FILE__, __LINE__);\n+  void _interp_verify_oop(Register reg, TosState state, const char* file, int line);\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.hpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/aarch64\/jniFastGetField_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"compiler\/oopMap.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -52,0 +54,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -56,1 +59,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -77,65 +79,162 @@\n-\/\/ Patch any kind of instruction; there may be several instructions.\n-\/\/ Return the total length (in bytes) of the instructions.\n-int MacroAssembler::pd_patch_instruction_size(address branch, address target) {\n-  int instructions = 1;\n-  assert((uint64_t)target < (1ull << 48), \"48-bit overflow in address constant\");\n-  intptr_t offset = (target - branch) >> 2;\n-  unsigned insn = *(unsigned*)branch;\n-  if ((Instruction_aarch64::extract(insn, 29, 24) & 0b111011) == 0b011000) {\n-    \/\/ Load register (literal)\n-    Instruction_aarch64::spatch(branch, 23, 5, offset);\n-  } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {\n-    \/\/ Unconditional branch (immediate)\n-    Instruction_aarch64::spatch(branch, 25, 0, offset);\n-  } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {\n-    \/\/ Conditional branch (immediate)\n-    Instruction_aarch64::spatch(branch, 23, 5, offset);\n-  } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {\n-    \/\/ Compare & branch (immediate)\n-    Instruction_aarch64::spatch(branch, 23, 5, offset);\n-  } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {\n-    \/\/ Test & branch (immediate)\n-    Instruction_aarch64::spatch(branch, 18, 5, offset);\n-  } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {\n-    \/\/ PC-rel. addressing\n-    offset = target-branch;\n-    int shift = Instruction_aarch64::extract(insn, 31, 31);\n-    if (shift) {\n-      uint64_t dest = (uint64_t)target;\n-      uint64_t pc_page = (uint64_t)branch >> 12;\n-      uint64_t adr_page = (uint64_t)target >> 12;\n-      unsigned offset_lo = dest & 0xfff;\n-      offset = adr_page - pc_page;\n-\n-      \/\/ We handle 4 types of PC relative addressing\n-      \/\/   1 - adrp    Rx, target_page\n-      \/\/       ldr\/str Ry, [Rx, #offset_in_page]\n-      \/\/   2 - adrp    Rx, target_page\n-      \/\/       add     Ry, Rx, #offset_in_page\n-      \/\/   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)\n-      \/\/       movk    Rx, #imm16<<32\n-      \/\/   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)\n-      \/\/ In the first 3 cases we must check that Rx is the same in the adrp and the\n-      \/\/ subsequent ldr\/str, add or movk instruction. Otherwise we could accidentally end\n-      \/\/ up treating a type 4 relocation as a type 1, 2 or 3 just because it happened\n-      \/\/ to be followed by a random unrelated ldr\/str, add or movk instruction.\n-      \/\/\n-      unsigned insn2 = ((unsigned*)branch)[1];\n-      if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &&\n-                Instruction_aarch64::extract(insn, 4, 0) ==\n-                        Instruction_aarch64::extract(insn2, 9, 5)) {\n-        \/\/ Load\/store register (unsigned immediate)\n-        unsigned size = Instruction_aarch64::extract(insn2, 31, 30);\n-        Instruction_aarch64::patch(branch + sizeof (unsigned),\n-                                    21, 10, offset_lo >> size);\n-        guarantee(((dest >> size) << size) == dest, \"misaligned target\");\n-        instructions = 2;\n-      } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &&\n-                Instruction_aarch64::extract(insn, 4, 0) ==\n-                        Instruction_aarch64::extract(insn2, 4, 0)) {\n-        \/\/ add (immediate)\n-        Instruction_aarch64::patch(branch + sizeof (unsigned),\n-                                   21, 10, offset_lo);\n-        instructions = 2;\n-      } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &&\n-                   Instruction_aarch64::extract(insn, 4, 0) ==\n+#ifdef ASSERT\n+extern \"C\" void disnm(intptr_t p);\n+#endif\n+\/\/ Target-dependent relocation processing\n+\/\/\n+\/\/ Instruction sequences whose target may need to be retrieved or\n+\/\/ patched are distinguished by their leading instruction, sorting\n+\/\/ them into three main instruction groups and related subgroups.\n+\/\/\n+\/\/ 1) Branch, Exception and System (insn count = 1)\n+\/\/    1a) Unconditional branch (immediate):\n+\/\/      b\/bl imm19\n+\/\/    1b) Compare & branch (immediate):\n+\/\/      cbz\/cbnz Rt imm19\n+\/\/    1c) Test & branch (immediate):\n+\/\/      tbz\/tbnz Rt imm14\n+\/\/    1d) Conditional branch (immediate):\n+\/\/      b.cond imm19\n+\/\/\n+\/\/ 2) Loads and Stores (insn count = 1)\n+\/\/    2a) Load register literal:\n+\/\/      ldr Rt imm19\n+\/\/\n+\/\/ 3) Data Processing Immediate (insn count = 2 or 3)\n+\/\/    3a) PC-rel. addressing\n+\/\/      adr\/adrp Rx imm21; ldr\/str Ry Rx  #imm12\n+\/\/      adr\/adrp Rx imm21; add Ry Rx  #imm12\n+\/\/      adr\/adrp Rx imm21; movk Rx #imm16<<32; ldr\/str Ry, [Rx, #offset_in_page]\n+\/\/      adr\/adrp Rx imm21\n+\/\/      adr\/adrp Rx imm21; movk Rx #imm16<<32\n+\/\/      adr\/adrp Rx imm21; movk Rx #imm16<<32; add Ry, Rx, #offset_in_page\n+\/\/      The latter form can only happen when the target is an\n+\/\/      ExternalAddress, and (by definition) ExternalAddresses don't\n+\/\/      move. Because of that property, there is never any need to\n+\/\/      patch the last of the three instructions. However,\n+\/\/      MacroAssembler::target_addr_for_insn takes all three\n+\/\/      instructions into account and returns the correct address.\n+\/\/    3b) Move wide (immediate)\n+\/\/      movz Rx #imm16; movk Rx #imm16 << 16; movk Rx #imm16 << 32;\n+\/\/\n+\/\/ A switch on a subset of the instruction's bits provides an\n+\/\/ efficient dispatch to these subcases.\n+\/\/\n+\/\/ insn[28:26] -> main group ('x' == don't care)\n+\/\/   00x -> UNALLOCATED\n+\/\/   100 -> Data Processing Immediate\n+\/\/   101 -> Branch, Exception and System\n+\/\/   x1x -> Loads and Stores\n+\/\/\n+\/\/ insn[30:25] -> subgroup ('_' == group, 'x' == don't care).\n+\/\/ n.b. in some cases extra bits need to be checked to verify the\n+\/\/ instruction is as expected\n+\/\/\n+\/\/ 1) ... xx101x Branch, Exception and System\n+\/\/   1a)  00___x Unconditional branch (immediate)\n+\/\/   1b)  01___0 Compare & branch (immediate)\n+\/\/   1c)  01___1 Test & branch (immediate)\n+\/\/   1d)  10___0 Conditional branch (immediate)\n+\/\/        other  Should not happen\n+\/\/\n+\/\/ 2) ... xxx1x0 Loads and Stores\n+\/\/   2a)  xx1__00 Load\/Store register (insn[28] == 1 && insn[24] == 0)\n+\/\/   2aa) x01__00 Load register literal (i.e. requires insn[29] == 0)\n+\/\/                strictly should be 64 bit non-FP\/SIMD i.e.\n+\/\/       0101_000 (i.e. requires insn[31:24] == 01011000)\n+\/\/\n+\/\/ 3) ... xx100x Data Processing Immediate\n+\/\/   3a)  xx___00 PC-rel. addressing (n.b. requires insn[24] == 0)\n+\/\/   3b)  xx___101 Move wide (immediate) (n.b. requires insn[24:23] == 01)\n+\/\/                 strictly should be 64 bit movz #imm16<<0\n+\/\/       110___10100 (i.e. requires insn[31:21] == 11010010100)\n+\/\/\n+class RelocActions {\n+protected:\n+  typedef int (*reloc_insn)(address insn_addr, address &target);\n+\n+  virtual reloc_insn adrpMem() = 0;\n+  virtual reloc_insn adrpAdd() = 0;\n+  virtual reloc_insn adrpMovk() = 0;\n+\n+  const address _insn_addr;\n+  const uint32_t _insn;\n+\n+  static uint32_t insn_at(address insn_addr, int n) {\n+    return ((uint32_t*)insn_addr)[n];\n+  }\n+  uint32_t insn_at(int n) const {\n+    return insn_at(_insn_addr, n);\n+  }\n+\n+public:\n+\n+  RelocActions(address insn_addr) : _insn_addr(insn_addr), _insn(insn_at(insn_addr, 0)) {}\n+  RelocActions(address insn_addr, uint32_t insn)\n+    :  _insn_addr(insn_addr), _insn(insn) {}\n+\n+  virtual int unconditionalBranch(address insn_addr, address &target) = 0;\n+  virtual int conditionalBranch(address insn_addr, address &target) = 0;\n+  virtual int testAndBranch(address insn_addr, address &target) = 0;\n+  virtual int loadStore(address insn_addr, address &target) = 0;\n+  virtual int adr(address insn_addr, address &target) = 0;\n+  virtual int adrp(address insn_addr, address &target, reloc_insn inner) = 0;\n+  virtual int immediate(address insn_addr, address &target) = 0;\n+  virtual void verify(address insn_addr, address &target) = 0;\n+\n+  int ALWAYSINLINE run(address insn_addr, address &target) {\n+    int instructions = 1;\n+\n+    uint32_t dispatch = Instruction_aarch64::extract(_insn, 30, 25);\n+    switch(dispatch) {\n+      case 0b001010:\n+      case 0b001011: {\n+        instructions = unconditionalBranch(insn_addr, target);\n+        break;\n+      }\n+      case 0b101010:   \/\/ Conditional branch (immediate)\n+      case 0b011010: { \/\/ Compare & branch (immediate)\n+        instructions = conditionalBranch(insn_addr, target);\n+          break;\n+      }\n+      case 0b011011: {\n+        instructions = testAndBranch(insn_addr, target);\n+        break;\n+      }\n+      case 0b001100:\n+      case 0b001110:\n+      case 0b011100:\n+      case 0b011110:\n+      case 0b101100:\n+      case 0b101110:\n+      case 0b111100:\n+      case 0b111110: {\n+        \/\/ load\/store\n+        if ((Instruction_aarch64::extract(_insn, 29, 24) & 0b111011) == 0b011000) {\n+          \/\/ Load register (literal)\n+          instructions = loadStore(insn_addr, target);\n+          break;\n+        } else {\n+          \/\/ nothing to do\n+          assert(target == 0, \"did not expect to relocate target for polling page load\");\n+        }\n+        break;\n+      }\n+      case 0b001000:\n+      case 0b011000:\n+      case 0b101000:\n+      case 0b111000: {\n+        \/\/ adr\/adrp\n+        assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+        int shift = Instruction_aarch64::extract(_insn, 31, 31);\n+        if (shift) {\n+          uint32_t insn2 = insn_at(1);\n+          if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &&\n+              Instruction_aarch64::extract(_insn, 4, 0) ==\n+              Instruction_aarch64::extract(insn2, 9, 5)) {\n+            instructions = adrp(insn_addr, target, adrpMem());\n+          } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &&\n+                     Instruction_aarch64::extract(_insn, 4, 0) ==\n+                     Instruction_aarch64::extract(insn2, 4, 0)) {\n+            instructions = adrp(insn_addr, target, adrpAdd());\n+          } else if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110 &&\n+                     Instruction_aarch64::extract(_insn, 4, 0) ==\n@@ -143,7 +242,18 @@\n-        \/\/ movk #imm16<<32\n-        Instruction_aarch64::patch(branch + 4, 20, 5, (uint64_t)target >> 32);\n-        uintptr_t dest = ((uintptr_t)target & 0xffffffffULL) | ((uintptr_t)branch & 0xffff00000000ULL);\n-        uintptr_t pc_page = (uintptr_t)branch >> 12;\n-        uintptr_t adr_page = (uintptr_t)dest >> 12;\n-        offset = adr_page - pc_page;\n-        instructions = 2;\n+            instructions = adrp(insn_addr, target, adrpMovk());\n+          } else {\n+            ShouldNotReachHere();\n+          }\n+        } else {\n+          instructions = adr(insn_addr, target);\n+        }\n+        break;\n+      }\n+      case 0b001001:\n+      case 0b011001:\n+      case 0b101001:\n+      case 0b111001: {\n+        instructions = immediate(insn_addr, target);\n+        break;\n+      }\n+      default: {\n+        ShouldNotReachHere();\n@@ -152,0 +262,40 @@\n+\n+    verify(insn_addr, target);\n+    return instructions * NativeInstruction::instruction_size;\n+  }\n+};\n+\n+class Patcher : public RelocActions {\n+  virtual reloc_insn adrpMem() { return &Patcher::adrpMem_impl; }\n+  virtual reloc_insn adrpAdd() { return &Patcher::adrpAdd_impl; }\n+  virtual reloc_insn adrpMovk() { return &Patcher::adrpMovk_impl; }\n+\n+public:\n+  Patcher(address insn_addr) : RelocActions(insn_addr) {}\n+\n+  virtual int unconditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = (target - insn_addr) >> 2;\n+    Instruction_aarch64::spatch(insn_addr, 25, 0, offset);\n+    return 1;\n+  }\n+  virtual int conditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = (target - insn_addr) >> 2;\n+    Instruction_aarch64::spatch(insn_addr, 23, 5, offset);\n+    return 1;\n+  }\n+  virtual int testAndBranch(address insn_addr, address &target) {\n+    intptr_t offset = (target - insn_addr) >> 2;\n+    Instruction_aarch64::spatch(insn_addr, 18, 5, offset);\n+    return 1;\n+  }\n+  virtual int loadStore(address insn_addr, address &target) {\n+    intptr_t offset = (target - insn_addr) >> 2;\n+    Instruction_aarch64::spatch(insn_addr, 23, 5, offset);\n+    return 1;\n+  }\n+  virtual int adr(address insn_addr, address &target) {\n+#ifdef ASSERT\n+    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+#endif\n+    \/\/ PC-rel. addressing\n+    ptrdiff_t offset = target - insn_addr;\n@@ -154,3 +304,48 @@\n-    Instruction_aarch64::spatch(branch, 23, 5, offset);\n-    Instruction_aarch64::patch(branch, 30, 29, offset_lo);\n-  } else if (Instruction_aarch64::extract(insn, 31, 21) == 0b11010010100) {\n+    Instruction_aarch64::spatch(insn_addr, 23, 5, offset);\n+    Instruction_aarch64::patch(insn_addr, 30, 29, offset_lo);\n+    return 1;\n+  }\n+  virtual int adrp(address insn_addr, address &target, reloc_insn inner) {\n+    int instructions = 1;\n+#ifdef ASSERT\n+    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+#endif\n+    ptrdiff_t offset = target - insn_addr;\n+    instructions = 2;\n+    precond(inner != nullptr);\n+    \/\/ Give the inner reloc a chance to modify the target.\n+    address adjusted_target = target;\n+    instructions = (*inner)(insn_addr, adjusted_target);\n+    uintptr_t pc_page = (uintptr_t)insn_addr >> 12;\n+    uintptr_t adr_page = (uintptr_t)adjusted_target >> 12;\n+    offset = adr_page - pc_page;\n+    int offset_lo = offset & 3;\n+    offset >>= 2;\n+    Instruction_aarch64::spatch(insn_addr, 23, 5, offset);\n+    Instruction_aarch64::patch(insn_addr, 30, 29, offset_lo);\n+    return instructions;\n+  }\n+  static int adrpMem_impl(address insn_addr, address &target) {\n+    uintptr_t dest = (uintptr_t)target;\n+    int offset_lo = dest & 0xfff;\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n+    uint32_t size = Instruction_aarch64::extract(insn2, 31, 30);\n+    Instruction_aarch64::patch(insn_addr + sizeof (uint32_t), 21, 10, offset_lo >> size);\n+    guarantee(((dest >> size) << size) == dest, \"misaligned target\");\n+    return 2;\n+  }\n+  static int adrpAdd_impl(address insn_addr, address &target) {\n+    uintptr_t dest = (uintptr_t)target;\n+    int offset_lo = dest & 0xfff;\n+    Instruction_aarch64::patch(insn_addr + sizeof (uint32_t), 21, 10, offset_lo);\n+    return 2;\n+  }\n+  static int adrpMovk_impl(address insn_addr, address &target) {\n+    uintptr_t dest = uintptr_t(target);\n+    Instruction_aarch64::patch(insn_addr + sizeof (uint32_t), 20, 5, (uintptr_t)target >> 32);\n+    dest = (dest & 0xffffffffULL) | (uintptr_t(insn_addr) & 0xffff00000000ULL);\n+    target = address(dest);\n+    return 2;\n+  }\n+  virtual int immediate(address insn_addr, address &target) {\n+    assert(Instruction_aarch64::extract(_insn, 31, 21) == 0b11010010100, \"must be\");\n@@ -159,12 +354,6 @@\n-    assert(nativeInstruction_at(branch+4)->is_movk(), \"wrong insns in patch\");\n-    assert(nativeInstruction_at(branch+8)->is_movk(), \"wrong insns in patch\");\n-    Instruction_aarch64::patch(branch, 20, 5, dest & 0xffff);\n-    Instruction_aarch64::patch(branch+4, 20, 5, (dest >>= 16) & 0xffff);\n-    Instruction_aarch64::patch(branch+8, 20, 5, (dest >>= 16) & 0xffff);\n-    assert(target_addr_for_insn(branch) == target, \"should be\");\n-    instructions = 3;\n-  } else if (NativeInstruction::is_ldrw_to_zr(address(&insn))) {\n-    \/\/ nothing to do\n-    assert(target == 0, \"did not expect to relocate target for polling page load\");\n-  } else {\n-    ShouldNotReachHere();\n+    assert(nativeInstruction_at(insn_addr+4)->is_movk(), \"wrong insns in patch\");\n+    assert(nativeInstruction_at(insn_addr+8)->is_movk(), \"wrong insns in patch\");\n+    Instruction_aarch64::patch(insn_addr, 20, 5, dest & 0xffff);\n+    Instruction_aarch64::patch(insn_addr+4, 20, 5, (dest >>= 16) & 0xffff);\n+    Instruction_aarch64::patch(insn_addr+8, 20, 5, (dest >>= 16) & 0xffff);\n+    return 3;\n@@ -172,1 +361,147 @@\n-  return instructions * NativeInstruction::instruction_size;\n+  virtual void verify(address insn_addr, address &target) {\n+#ifdef ASSERT\n+    address address_is = MacroAssembler::target_addr_for_insn(insn_addr);\n+    if (!(address_is == target)) {\n+      tty->print_cr(\"%p at %p should be %p\", address_is, insn_addr, target);\n+      disnm((intptr_t)insn_addr);\n+      assert(address_is == target, \"should be\");\n+    }\n+#endif\n+  }\n+};\n+\n+\/\/ If insn1 and insn2 use the same register to form an address, either\n+\/\/ by an offsetted LDR or a simple ADD, return the offset. If the\n+\/\/ second instruction is an LDR, the offset may be scaled.\n+static bool offset_for(uint32_t insn1, uint32_t insn2, ptrdiff_t &byte_offset) {\n+  if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &&\n+      Instruction_aarch64::extract(insn1, 4, 0) ==\n+      Instruction_aarch64::extract(insn2, 9, 5)) {\n+    \/\/ Load\/store register (unsigned immediate)\n+    byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n+    uint32_t size = Instruction_aarch64::extract(insn2, 31, 30);\n+    byte_offset <<= size;\n+    return true;\n+  } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &&\n+             Instruction_aarch64::extract(insn1, 4, 0) ==\n+             Instruction_aarch64::extract(insn2, 4, 0)) {\n+    \/\/ add (immediate)\n+    byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n+    return true;\n+  }\n+  return false;\n+}\n+\n+class Decoder : public RelocActions {\n+  virtual reloc_insn adrpMem() { return &Decoder::adrpMem_impl; }\n+  virtual reloc_insn adrpAdd() { return &Decoder::adrpAdd_impl; }\n+  virtual reloc_insn adrpMovk() { return &Decoder::adrpMovk_impl; }\n+\n+public:\n+  Decoder(address insn_addr, uint32_t insn) : RelocActions(insn_addr, insn) {}\n+\n+  virtual int loadStore(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(_insn, 23, 5);\n+    target = insn_addr + (offset << 2);\n+    return 1;\n+  }\n+  virtual int unconditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(_insn, 25, 0);\n+    target = insn_addr + (offset << 2);\n+    return 1;\n+  }\n+  virtual int conditionalBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(_insn, 23, 5);\n+    target = address(((uint64_t)insn_addr + (offset << 2)));\n+    return 1;\n+  }\n+  virtual int testAndBranch(address insn_addr, address &target) {\n+    intptr_t offset = Instruction_aarch64::sextract(_insn, 18, 5);\n+    target = address(((uint64_t)insn_addr + (offset << 2)));\n+    return 1;\n+  }\n+  virtual int adr(address insn_addr, address &target) {\n+    \/\/ PC-rel. addressing\n+    intptr_t offset = Instruction_aarch64::extract(_insn, 30, 29);\n+    offset |= Instruction_aarch64::sextract(_insn, 23, 5) << 2;\n+    target = address((uint64_t)insn_addr + offset);\n+    return 1;\n+  }\n+  virtual int adrp(address insn_addr, address &target, reloc_insn inner) {\n+    assert(Instruction_aarch64::extract(_insn, 28, 24) == 0b10000, \"must be\");\n+    intptr_t offset = Instruction_aarch64::extract(_insn, 30, 29);\n+    offset |= Instruction_aarch64::sextract(_insn, 23, 5) << 2;\n+    int shift = 12;\n+    offset <<= shift;\n+    uint64_t target_page = ((uint64_t)insn_addr) + offset;\n+    target_page &= ((uint64_t)-1) << shift;\n+    uint32_t insn2 = insn_at(1);\n+    target = address(target_page);\n+    precond(inner != nullptr);\n+    (*inner)(insn_addr, target);\n+    return 2;\n+  }\n+  static int adrpMem_impl(address insn_addr, address &target) {\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n+    \/\/ Load\/store register (unsigned immediate)\n+    ptrdiff_t byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n+    uint32_t size = Instruction_aarch64::extract(insn2, 31, 30);\n+    byte_offset <<= size;\n+    target += byte_offset;\n+    return 2;\n+  }\n+  static int adrpAdd_impl(address insn_addr, address &target) {\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n+    \/\/ add (immediate)\n+    ptrdiff_t byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n+    target += byte_offset;\n+    return 2;\n+  }\n+  static int adrpMovk_impl(address insn_addr, address &target) {\n+    uint32_t insn2 = insn_at(insn_addr, 1);\n+    uint64_t dest = uint64_t(target);\n+    dest = (dest & 0xffff0000ffffffff) |\n+      ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) << 32);\n+    target = address(dest);\n+\n+    \/\/ We know the destination 4k page. Maybe we have a third\n+    \/\/ instruction.\n+    uint32_t insn = insn_at(insn_addr, 0);\n+    uint32_t insn3 = insn_at(insn_addr, 2);\n+    ptrdiff_t byte_offset;\n+    if (offset_for(insn, insn3, byte_offset)) {\n+      target += byte_offset;\n+      return 3;\n+    } else {\n+      return 2;\n+    }\n+  }\n+  virtual int immediate(address insn_addr, address &target) {\n+    uint32_t *insns = (uint32_t *)insn_addr;\n+    assert(Instruction_aarch64::extract(_insn, 31, 21) == 0b11010010100, \"must be\");\n+    \/\/ Move wide constant: movz, movk, movk.  See movptr().\n+    assert(nativeInstruction_at(insns+1)->is_movk(), \"wrong insns in patch\");\n+    assert(nativeInstruction_at(insns+2)->is_movk(), \"wrong insns in patch\");\n+    target = address(uint64_t(Instruction_aarch64::extract(_insn, 20, 5))\n+                 + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)\n+                 + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));\n+    assert(nativeInstruction_at(insn_addr+4)->is_movk(), \"wrong insns in patch\");\n+    assert(nativeInstruction_at(insn_addr+8)->is_movk(), \"wrong insns in patch\");\n+    return 3;\n+  }\n+  virtual void verify(address insn_addr, address &target) {\n+  }\n+};\n+\n+address MacroAssembler::target_addr_for_insn(address insn_addr, uint32_t insn) {\n+  Decoder decoder(insn_addr, insn);\n+  address target;\n+  decoder.run(insn_addr, target);\n+  return target;\n+}\n+\n+\/\/ Patch any kind of instruction; there may be several instructions.\n+\/\/ Return the total length (in bytes) of the instructions.\n+int MacroAssembler::pd_patch_instruction_size(address insn_addr, address target) {\n+  Patcher patcher(insn_addr);\n+  return patcher.run(insn_addr, target);\n@@ -202,1 +537,1 @@\n-  \/\/ Metatdata pointers are either narrow (32 bits) or wide (48 bits).\n+  \/\/ Metadata pointers are either narrow (32 bits) or wide (48 bits).\n@@ -214,81 +549,0 @@\n-address MacroAssembler::target_addr_for_insn(address insn_addr, unsigned insn) {\n-  intptr_t offset = 0;\n-  if ((Instruction_aarch64::extract(insn, 29, 24) & 0b011011) == 0b00011000) {\n-    \/\/ Load register (literal)\n-    offset = Instruction_aarch64::sextract(insn, 23, 5);\n-    return address(((uint64_t)insn_addr + (offset << 2)));\n-  } else if (Instruction_aarch64::extract(insn, 30, 26) == 0b00101) {\n-    \/\/ Unconditional branch (immediate)\n-    offset = Instruction_aarch64::sextract(insn, 25, 0);\n-  } else if (Instruction_aarch64::extract(insn, 31, 25) == 0b0101010) {\n-    \/\/ Conditional branch (immediate)\n-    offset = Instruction_aarch64::sextract(insn, 23, 5);\n-  } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011010) {\n-    \/\/ Compare & branch (immediate)\n-    offset = Instruction_aarch64::sextract(insn, 23, 5);\n-   } else if (Instruction_aarch64::extract(insn, 30, 25) == 0b011011) {\n-    \/\/ Test & branch (immediate)\n-    offset = Instruction_aarch64::sextract(insn, 18, 5);\n-  } else if (Instruction_aarch64::extract(insn, 28, 24) == 0b10000) {\n-    \/\/ PC-rel. addressing\n-    offset = Instruction_aarch64::extract(insn, 30, 29);\n-    offset |= Instruction_aarch64::sextract(insn, 23, 5) << 2;\n-    int shift = Instruction_aarch64::extract(insn, 31, 31) ? 12 : 0;\n-    if (shift) {\n-      offset <<= shift;\n-      uint64_t target_page = ((uint64_t)insn_addr) + offset;\n-      target_page &= ((uint64_t)-1) << shift;\n-      \/\/ Return the target address for the following sequences\n-      \/\/   1 - adrp    Rx, target_page\n-      \/\/       ldr\/str Ry, [Rx, #offset_in_page]\n-      \/\/   2 - adrp    Rx, target_page\n-      \/\/       add     Ry, Rx, #offset_in_page\n-      \/\/   3 - adrp    Rx, target_page (page aligned reloc, offset == 0)\n-      \/\/       movk    Rx, #imm12<<32\n-      \/\/   4 - adrp    Rx, target_page (page aligned reloc, offset == 0)\n-      \/\/\n-      \/\/ In the first two cases  we check that the register is the same and\n-      \/\/ return the target_page + the offset within the page.\n-      \/\/ Otherwise we assume it is a page aligned relocation and return\n-      \/\/ the target page only.\n-      \/\/\n-      unsigned insn2 = ((unsigned*)insn_addr)[1];\n-      if (Instruction_aarch64::extract(insn2, 29, 24) == 0b111001 &&\n-                Instruction_aarch64::extract(insn, 4, 0) ==\n-                        Instruction_aarch64::extract(insn2, 9, 5)) {\n-        \/\/ Load\/store register (unsigned immediate)\n-        unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n-        unsigned int size = Instruction_aarch64::extract(insn2, 31, 30);\n-        return address(target_page + (byte_offset << size));\n-      } else if (Instruction_aarch64::extract(insn2, 31, 22) == 0b1001000100 &&\n-                Instruction_aarch64::extract(insn, 4, 0) ==\n-                        Instruction_aarch64::extract(insn2, 4, 0)) {\n-        \/\/ add (immediate)\n-        unsigned int byte_offset = Instruction_aarch64::extract(insn2, 21, 10);\n-        return address(target_page + byte_offset);\n-      } else {\n-        if (Instruction_aarch64::extract(insn2, 31, 21) == 0b11110010110  &&\n-               Instruction_aarch64::extract(insn, 4, 0) ==\n-                 Instruction_aarch64::extract(insn2, 4, 0)) {\n-          target_page = (target_page & 0xffffffff) |\n-                         ((uint64_t)Instruction_aarch64::extract(insn2, 20, 5) << 32);\n-        }\n-        return (address)target_page;\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (Instruction_aarch64::extract(insn, 31, 23) == 0b110100101) {\n-    uint32_t *insns = (uint32_t *)insn_addr;\n-    \/\/ Move wide constant: movz, movk, movk.  See movptr().\n-    assert(nativeInstruction_at(insns+1)->is_movk(), \"wrong insns in patch\");\n-    assert(nativeInstruction_at(insns+2)->is_movk(), \"wrong insns in patch\");\n-    return address(uint64_t(Instruction_aarch64::extract(insns[0], 20, 5))\n-                   + (uint64_t(Instruction_aarch64::extract(insns[1], 20, 5)) << 16)\n-                   + (uint64_t(Instruction_aarch64::extract(insns[2], 20, 5)) << 32));\n-  } else {\n-    ShouldNotReachHere();\n-  }\n-  return address(((uint64_t)insn_addr + (offset << 2)));\n-}\n-\n@@ -297,1 +551,1 @@\n-    return 0;\n+    return nullptr;\n@@ -302,1 +556,1 @@\n-void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod) {\n+void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp) {\n@@ -304,2 +558,2 @@\n-    lea(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n-    ldar(rscratch1, rscratch1);\n+    lea(tmp, Address(rthread, JavaThread::polling_word_offset()));\n+    ldar(tmp, tmp);\n@@ -307,1 +561,1 @@\n-    ldr(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n+    ldr(tmp, Address(rthread, JavaThread::polling_word_offset()));\n@@ -312,1 +566,1 @@\n-    cmp(in_nmethod ? sp : rfp, rscratch1);\n+    cmp(in_nmethod ? sp : rfp, tmp);\n@@ -315,1 +569,1 @@\n-    tbnz(rscratch1, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+    tbnz(tmp, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n@@ -319,0 +573,31 @@\n+void MacroAssembler::rt_call(address dest, Register tmp) {\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb) {\n+    far_call(RuntimeAddress(dest));\n+  } else {\n+    lea(tmp, RuntimeAddress(dest));\n+    blr(tmp);\n+  }\n+}\n+\n+void MacroAssembler::push_cont_fastpath(Register java_thread) {\n+  if (!Continuations::enabled()) return;\n+  Label done;\n+  ldr(rscratch1, Address(java_thread, JavaThread::cont_fastpath_offset()));\n+  cmp(sp, rscratch1);\n+  br(Assembler::LS, done);\n+  mov(rscratch1, sp); \/\/ we can't use sp as the source in str\n+  str(rscratch1, Address(java_thread, JavaThread::cont_fastpath_offset()));\n+  bind(done);\n+}\n+\n+void MacroAssembler::pop_cont_fastpath(Register java_thread) {\n+  if (!Continuations::enabled()) return;\n+  Label done;\n+  ldr(rscratch1, Address(java_thread, JavaThread::cont_fastpath_offset()));\n+  cmp(sp, rscratch1);\n+  br(Assembler::LO, done);\n+  str(zr, Address(java_thread, JavaThread::cont_fastpath_offset()));\n+  bind(done);\n+}\n+\n@@ -392,0 +677,13 @@\n+static inline bool target_needs_far_branch(address addr) {\n+  \/\/ codecache size <= 128M\n+  if (!MacroAssembler::far_branches()) {\n+    return false;\n+  }\n+  \/\/ codecache size > 240M\n+  if (MacroAssembler::codestub_branch_needs_far_jump()) {\n+    return true;\n+  }\n+  \/\/ codecache size: 128M..240M\n+  return !CodeCache::is_non_nmethod(addr);\n+}\n+\n@@ -396,1 +694,4 @@\n-  if (far_branches()) {\n+  assert(entry.rspec().type() == relocInfo::external_word_type\n+         || entry.rspec().type() == relocInfo::runtime_call_type\n+         || entry.rspec().type() == relocInfo::none, \"wrong entry relocInfo type\");\n+  if (target_needs_far_branch(entry.target())) {\n@@ -399,1 +700,1 @@\n-    \/\/ the code cache cannot exceed 2Gb.\n+    \/\/ the code cache cannot exceed 2Gb (ADRP limit is 4GB).\n@@ -410,1 +711,1 @@\n-void MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {\n+int MacroAssembler::far_jump(Address entry, CodeBuffer *cbuf, Register tmp) {\n@@ -414,1 +715,5 @@\n-  if (far_branches()) {\n+  assert(entry.rspec().type() == relocInfo::external_word_type\n+         || entry.rspec().type() == relocInfo::runtime_call_type\n+         || entry.rspec().type() == relocInfo::none, \"wrong entry relocInfo type\");\n+  address start = pc();\n+  if (target_needs_far_branch(entry.target())) {\n@@ -417,1 +722,1 @@\n-    \/\/ the code cache cannot exceed 2Gb.\n+    \/\/ the code cache cannot exceed 2Gb (ADRP limit is 4GB).\n@@ -426,0 +731,1 @@\n+  return pc() - start;\n@@ -551,3 +857,1 @@\n-\n-address MacroAssembler::trampoline_call(Address entry, CodeBuffer* cbuf) {\n-  assert(JavaThread::current()->is_Compiler_thread(), \"just checking\");\n+address MacroAssembler::trampoline_call1(Address entry, CodeBuffer* cbuf, bool check_emit_size) {\n@@ -559,0 +863,15 @@\n+  bool need_trampoline = far_branches();\n+  if (!need_trampoline && entry.rspec().type() == relocInfo::runtime_call_type && !CodeCache::contains(entry.target())) {\n+    \/\/ If it is a runtime call of an address outside small CodeCache,\n+    \/\/ we need to check whether it is in range.\n+    address target = entry.target();\n+    assert(target < CodeCache::low_bound() || target >= CodeCache::high_bound(), \"target is inside CodeCache\");\n+    \/\/ Case 1: -------T-------L====CodeCache====H-------\n+    \/\/                ^-------longest branch---|\n+    \/\/ Case 2: -------L====CodeCache====H-------T-------\n+    \/\/                |-------longest branch ---^\n+    address longest_branch_start = (target < CodeCache::low_bound()) ? CodeCache::high_bound() - NativeInstruction::instruction_size\n+                                                                     : CodeCache::low_bound();\n+    need_trampoline = !reachable_from_branch_at(longest_branch_start, target);\n+  }\n+\n@@ -560,1 +879,1 @@\n-  if (far_branches()) {\n+  if (need_trampoline) {\n@@ -563,6 +882,8 @@\n-    \/\/ We don't want to emit a trampoline if C2 is generating dummy\n-    \/\/ code during its branch shortening phase.\n-    CompileTask* task = ciEnv::current()->task();\n-    in_scratch_emit_size =\n-      (task != NULL && is_c2_compile(task->comp_level()) &&\n-       Compile::current()->output()->in_scratch_emit_size());\n+    if (check_emit_size) {\n+      \/\/ We don't want to emit a trampoline if C2 is generating dummy\n+      \/\/ code during its branch shortening phase.\n+      CompileTask* task = ciEnv::current()->task();\n+      in_scratch_emit_size =\n+        (task != NULL && is_c2_compile(task->comp_level()) &&\n+         Compile::current()->output()->in_scratch_emit_size());\n+    }\n@@ -581,1 +902,1 @@\n-  if (!far_branches()) {\n+  if (!need_trampoline) {\n@@ -646,1 +967,1 @@\n-  \/\/ Jump to the entry point of the i2c stub.\n+  \/\/ Jump to the entry point of the c2i stub.\n@@ -763,1 +1084,1 @@\n-  verify_oop(oop_result, \"broken oop in call_VM_base\");\n+  verify_oop_msg(oop_result, \"broken oop in call_VM_base\");\n@@ -775,0 +1096,11 @@\n+void MacroAssembler::post_call_nop() {\n+  if (!Continuations::enabled()) {\n+    return;\n+  }\n+  InstructionMark im(this);\n+  relocate(post_call_nop_Relocation::spec());\n+  nop();\n+  movk(zr, 0);\n+  movk(zr, 0);\n+}\n+\n@@ -1019,1 +1351,1 @@\n-\/\/ scans count pointer sized words at [addr] for occurence of value,\n+\/\/ scans count pointer sized words at [addr] for occurrence of value,\n@@ -1034,1 +1366,1 @@\n-\/\/ scans count 4 byte words at [addr] for occurence of value,\n+\/\/ scans count 4 byte words at [addr] for occurrence of value,\n@@ -1087,1 +1419,1 @@\n-  if (super_klass != r0 || UseCompressedOops) {\n+  if (super_klass != r0) {\n@@ -1165,1 +1497,1 @@\n-void MacroAssembler::verify_oop(Register reg, const char* s) {\n+void MacroAssembler::_verify_oop(Register reg, const char* s, const char* file, int line) {\n@@ -1177,1 +1509,1 @@\n-    ss.print(\"verify_oop: %s: %s\", reg->name(), s);\n+    ss.print(\"verify_oop: %s: %s (%s:%d)\", reg->name(), s, file, line);\n@@ -1202,1 +1534,1 @@\n-void MacroAssembler::verify_oop_addr(Address addr, const char* s) {\n+void MacroAssembler::_verify_oop_addr(Address addr, const char* s, const char* file, int line) {\n@@ -1213,1 +1545,1 @@\n-    ss.print(\"verify_oop_addr: %s\", s);\n+    ss.print(\"verify_oop_addr: %s (%s:%d)\", s, file, line);\n@@ -1498,13 +1830,22 @@\n-\/\/  Vd will get the following values for different arrangements in T\n-\/\/   imm32 == hex 000000gh  T8B:  Vd = ghghghghghghghgh\n-\/\/   imm32 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh\n-\/\/   imm32 == hex 0000efgh  T4H:  Vd = efghefghefghefgh\n-\/\/   imm32 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh\n-\/\/   imm32 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh\n-\/\/   imm32 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh\n-\/\/   T1D\/T2D: invalid\n-void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32) {\n-  assert(T != T1D && T != T2D, \"invalid arrangement\");\n-  if (T == T8B || T == T16B) {\n-    assert((imm32 & ~0xff) == 0, \"extraneous bits in unsigned imm32 (T8B\/T16B)\");\n-    movi(Vd, T, imm32 & 0xff, 0);\n+\/\/ imm64: only the lower 8\/16\/32 bits are considered for B\/H\/S type. That is,\n+\/\/        the upper 56\/48\/32 bits must be zeros for B\/H\/S type.\n+\/\/ Vd will get the following values for different arrangements in T\n+\/\/   imm64 == hex 000000gh  T8B:  Vd = ghghghghghghghgh\n+\/\/   imm64 == hex 000000gh  T16B: Vd = ghghghghghghghghghghghghghghghgh\n+\/\/   imm64 == hex 0000efgh  T4H:  Vd = efghefghefghefgh\n+\/\/   imm64 == hex 0000efgh  T8H:  Vd = efghefghefghefghefghefghefghefgh\n+\/\/   imm64 == hex abcdefgh  T2S:  Vd = abcdefghabcdefgh\n+\/\/   imm64 == hex abcdefgh  T4S:  Vd = abcdefghabcdefghabcdefghabcdefgh\n+\/\/   imm64 == hex abcdefgh  T1D:  Vd = 00000000abcdefgh\n+\/\/   imm64 == hex abcdefgh  T2D:  Vd = 00000000abcdefgh00000000abcdefgh\n+\/\/ Clobbers rscratch1\n+void MacroAssembler::mov(FloatRegister Vd, SIMD_Arrangement T, uint64_t imm64) {\n+  assert(T != T1Q, \"unsupported\");\n+  if (T == T1D || T == T2D) {\n+    int imm = operand_valid_for_movi_immediate(imm64, T);\n+    if (-1 != imm) {\n+      movi(Vd, T, imm);\n+    } else {\n+      mov(rscratch1, imm64);\n+      dup(Vd, T, rscratch1);\n+    }\n@@ -1513,27 +1854,13 @@\n-  uint32_t nimm32 = ~imm32;\n-  if (T == T4H || T == T8H) {\n-    assert((imm32  & ~0xffff) == 0, \"extraneous bits in unsigned imm32 (T4H\/T8H)\");\n-    imm32 &= 0xffff;\n-    nimm32 &= 0xffff;\n-  }\n-  uint32_t x = imm32;\n-  int movi_cnt = 0;\n-  int movn_cnt = 0;\n-  while (x) { if (x & 0xff) movi_cnt++; x >>= 8; }\n-  x = nimm32;\n-  while (x) { if (x & 0xff) movn_cnt++; x >>= 8; }\n-  if (movn_cnt < movi_cnt) imm32 = nimm32;\n-  unsigned lsl = 0;\n-  while (imm32 && (imm32 & 0xff) == 0) { lsl += 8; imm32 >>= 8; }\n-  if (movn_cnt < movi_cnt)\n-    mvni(Vd, T, imm32 & 0xff, lsl);\n-  else\n-    movi(Vd, T, imm32 & 0xff, lsl);\n-  imm32 >>= 8; lsl += 8;\n-  while (imm32) {\n-    while ((imm32 & 0xff) == 0) { lsl += 8; imm32 >>= 8; }\n-    if (movn_cnt < movi_cnt)\n-      bici(Vd, T, imm32 & 0xff, lsl);\n-    else\n-      orri(Vd, T, imm32 & 0xff, lsl);\n-    lsl += 8; imm32 >>= 8;\n+\n+#ifdef ASSERT\n+  if (T == T8B || T == T16B) assert((imm64 & ~0xff) == 0, \"extraneous bits (T8B\/T16B)\");\n+  if (T == T4H || T == T8H) assert((imm64  & ~0xffff) == 0, \"extraneous bits (T4H\/T8H)\");\n+  if (T == T2S || T == T4S) assert((imm64  & ~0xffffffff) == 0, \"extraneous bits (T2S\/T4S)\");\n+#endif\n+  int shift = operand_valid_for_movi_immediate(imm64, T);\n+  uint32_t imm32 = imm64 & 0xffffffffULL;\n+  if (shift >= 0) {\n+    movi(Vd, T, (imm32 >> shift) & 0xff, shift);\n+  } else {\n+    movw(rscratch1, imm32);\n+    dup(Vd, T, rscratch1);\n@@ -1731,2 +2058,1 @@\n-  if ((VM_Version::features() & VM_Version::CPU_STXR_PREFETCH))\n-    prfm(Address(counter_addr), PSTL1STRM);\n+  prfm(Address(counter_addr), PSTL1STRM);\n@@ -1737,1 +2063,1 @@\n-  \/\/ if we store+flush with no intervening write tmp wil be zero\n+  \/\/ if we store+flush with no intervening write tmp will be zero\n@@ -2045,1 +2371,1 @@\n-  count &= ~1;  \/\/ Only push an even nuber of regs\n+  count &= ~1;  \/\/ Only push an even number of regs\n@@ -2314,1 +2640,1 @@\n-  tbz(r0, 0, not_weak);    \/\/ Test for jweak tag.\n+  tbz(value, 0, not_weak);    \/\/ Test for jweak tag.\n@@ -2346,0 +2672,9 @@\n+void MacroAssembler::_assert_asm(Assembler::Condition cc, const char* msg) {\n+#ifdef ASSERT\n+  Label OK;\n+  br(cc, OK);\n+  stop(msg);\n+  bind(OK);\n+#endif\n+}\n+\n@@ -2348,1 +2683,1 @@\n-void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,\n+void MacroAssembler::wrap_add_sub_imm_insn(Register Rd, Register Rn, uint64_t imm,\n@@ -2350,1 +2685,2 @@\n-                                           add_sub_reg_insn insn2) {\n+                                           add_sub_reg_insn insn2,\n+                                           bool is32) {\n@@ -2352,1 +2688,2 @@\n-  if (operand_valid_for_add_sub_immediate((int)imm)) {\n+  bool fits = operand_valid_for_add_sub_immediate(is32 ? (int32_t)imm : imm);\n+  if (fits) {\n@@ -2360,1 +2697,1 @@\n-       mov(Rd, (uint64_t)imm);\n+       mov(Rd, imm);\n@@ -2366,1 +2703,1 @@\n-\/\/ Seperate vsn which sets the flags. Optimisations are more restricted\n+\/\/ Separate vsn which sets the flags. Optimisations are more restricted\n@@ -2368,4 +2705,6 @@\n-void MacroAssembler::wrap_adds_subs_imm_insn(Register Rd, Register Rn, unsigned imm,\n-                                           add_sub_imm_insn insn1,\n-                                           add_sub_reg_insn insn2) {\n-  if (operand_valid_for_add_sub_immediate((int)imm)) {\n+void MacroAssembler::wrap_adds_subs_imm_insn(Register Rd, Register Rn, uint64_t imm,\n+                                             add_sub_imm_insn insn1,\n+                                             add_sub_reg_insn insn2,\n+                                             bool is32) {\n+  bool fits = operand_valid_for_add_sub_immediate(is32 ? (int32_t)imm : imm);\n+  if (fits) {\n@@ -2376,1 +2715,1 @@\n-    mov(Rd, (uint64_t)imm);\n+    mov(Rd, imm);\n@@ -2453,2 +2792,1 @@\n-    if ((VM_Version::features() & VM_Version::CPU_STXR_PREFETCH))\n-      prfm(Address(addr), PSTL1STRM);\n+    prfm(Address(addr), PSTL1STRM);\n@@ -2461,1 +2799,1 @@\n-    \/\/ if we store+flush with no intervening write tmp wil be zero\n+    \/\/ if we store+flush with no intervening write tmp will be zero\n@@ -2496,2 +2834,1 @@\n-    if ((VM_Version::features() & VM_Version::CPU_STXR_PREFETCH))\n-      prfm(Address(addr), PSTL1STRM);\n+    prfm(Address(addr), PSTL1STRM);\n@@ -2504,1 +2841,1 @@\n-    \/\/ if we store+flush with no intervening write tmp wil be zero\n+    \/\/ if we store+flush with no intervening write tmp will be zero\n@@ -2538,2 +2875,1 @@\n-    if ((VM_Version::features() & VM_Version::CPU_STXR_PREFETCH))\n-      prfm(Address(addr), PSTL1STRM);\n+    prfm(Address(addr), PSTL1STRM);\n@@ -2597,2 +2933,1 @@\n-  if ((VM_Version::features() & VM_Version::CPU_STXR_PREFETCH))         \\\n-    prfm(Address(addr), PSTL1STRM);                                     \\\n+  prfm(Address(addr), PSTL1STRM);                                       \\\n@@ -2628,2 +2963,1 @@\n-  if ((VM_Version::features() & VM_Version::CPU_STXR_PREFETCH))         \\\n-    prfm(Address(addr), PSTL1STRM);                                     \\\n+  prfm(Address(addr), PSTL1STRM);                                       \\\n@@ -3180,1 +3514,1 @@\n- * Code for BigInteger::multiplyToLen() instrinsic.\n+ * Code for BigInteger::multiplyToLen() intrinsic.\n@@ -3315,1 +3649,1 @@\n-\/\/ Code for BigInteger::mulAdd instrinsic\n+\/\/ Code for BigInteger::mulAdd intrinsic\n@@ -3434,1 +3768,1 @@\n-    cmn(len, 32);\n+    cmn(len, (u1)32);\n@@ -3497,1 +3831,1 @@\n-    cmn(len, 128);\n+    cmn(len, (u1)128);\n@@ -3514,1 +3848,0 @@\n-  uint64_t offset;\n@@ -3523,2 +3856,5 @@\n-    adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);\n-    if (offset) add(table0, table0, offset);\n+    {\n+      uint64_t offset;\n+      adrp(table0, ExternalAddress(StubRoutines::crc_table_addr()), offset);\n+      add(table0, table0, offset);\n+    }\n@@ -3731,1 +4067,1 @@\n-    cmn(len, 32);\n+    cmn(len, (u1)32);\n@@ -3794,1 +4130,1 @@\n-    cmn(len, 128);\n+    cmn(len, (u1)128);\n@@ -3961,1 +4297,1 @@\n-  verify_oop(s, \"broken oop in encode_heap_oop\");\n+  verify_oop_msg(s, \"broken oop in encode_heap_oop\");\n@@ -3994,1 +4330,1 @@\n-  verify_oop(r, \"broken oop in encode_heap_oop_not_null\");\n+  verify_oop_msg(r, \"broken oop in encode_heap_oop_not_null\");\n@@ -4014,1 +4350,1 @@\n-  verify_oop(src, \"broken oop in encode_heap_oop_not_null2\");\n+  verify_oop_msg(src, \"broken oop in encode_heap_oop_not_null2\");\n@@ -4046,1 +4382,1 @@\n-  verify_oop(d, \"broken oop in decode_heap_oop\");\n+  verify_oop_msg(d, \"broken oop in decode_heap_oop\");\n@@ -4352,1 +4688,2 @@\n-  if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL || !immediate) {\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  if ((bs->barrier_set_nmethod() != NULL && bs->barrier_set_assembler()->nmethod_patching_type() == NMethodPatchingType::conc_data_patch) || !immediate) {\n@@ -4391,1 +4728,1 @@\n-  assert(new_obj == r0, \"needs to be r0, according to barrier asm eden_allocate\");\n+  assert(new_obj == r0, \"needs to be r0\");\n@@ -4404,5 +4741,0 @@\n-  \/\/  Else If inline contiguous allocations are enabled:\n-  \/\/    Try to allocate in eden.\n-  \/\/    If fails due to heap end, go to slow path.\n-  \/\/\n-  \/\/  If TLAB is enabled OR inline contiguous is enabled:\n@@ -4413,4 +4745,0 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n-\n-  push(klass);\n@@ -4419,0 +4747,1 @@\n+    push(klass);\n@@ -4427,5 +4756,0 @@\n-  } else {\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    eden_allocate(new_obj, layout_size, 0, t2, slow_case);\n-  }\n@@ -4433,3 +4757,0 @@\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n@@ -4481,0 +4802,2 @@\n+    \/\/ TODO: Valhalla removed SharedRuntime::dtrace_object_alloc from here ?\n+\n@@ -4484,2 +4807,4 @@\n-  bind(slow_case);\n-  pop(klass);\n+  if (UseTLAB) {\n+    bind(slow_case);\n+    pop(klass);\n+  }\n@@ -4503,10 +4828,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void MacroAssembler::eden_allocate(Register obj,\n-                                   Register var_size_in_bytes,\n-                                   int con_size_in_bytes,\n-                                   Register t1,\n-                                   Label& slow_case) {\n-  BarrierSetAssembler *bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n-}\n-\n@@ -5138,1 +5453,0 @@\n-        assert(false, \"failed to allocate space for trampoline\");\n@@ -5172,1 +5486,1 @@\n-void MacroAssembler::zero_words(Register base, uint64_t cnt)\n+address MacroAssembler::zero_words(Register base, uint64_t cnt)\n@@ -5174,1 +5488,1 @@\n-  guarantee(zero_words_block_size < BlockZeroingLowLimit,\n+  assert(wordSize <= BlockZeroingLowLimit,\n@@ -5176,0 +5490,1 @@\n+  address result = nullptr;\n@@ -5209,0 +5524,1 @@\n+    result = pc();\n@@ -5211,1 +5527,1 @@\n-    zero_words(r10, r11);\n+    result = zero_words(r10, r11);\n@@ -5213,0 +5529,1 @@\n+  return result;\n@@ -5217,1 +5534,1 @@\n-\/\/ Aligns the base address first sufficently for DC ZVA, then uses\n+\/\/ Aligns the base address first sufficiently for DC ZVA, then uses\n@@ -5560,0 +5877,50 @@\n+\/\/ java.math.round(double a)\n+\/\/ Returns the closest long to the argument, with ties rounding to\n+\/\/ positive infinity.  This requires some fiddling for corner\n+\/\/ cases. We take care to avoid double rounding in e.g. (jlong)(a + 0.5).\n+void MacroAssembler::java_round_double(Register dst, FloatRegister src,\n+                                       FloatRegister ftmp) {\n+  Label DONE;\n+  BLOCK_COMMENT(\"java_round_double: { \");\n+  fmovd(rscratch1, src);\n+  \/\/ Use RoundToNearestTiesAway unless src small and -ve.\n+  fcvtasd(dst, src);\n+  \/\/ Test if src >= 0 || abs(src) >= 0x1.0p52\n+  eor(rscratch1, rscratch1, UCONST64(1) << 63); \/\/ flip sign bit\n+  mov(rscratch2, julong_cast(0x1.0p52));\n+  cmp(rscratch1, rscratch2);\n+  br(HS, DONE); {\n+    \/\/ src < 0 && abs(src) < 0x1.0p52\n+    \/\/ src may have a fractional part, so add 0.5\n+    fmovd(ftmp, 0.5);\n+    faddd(ftmp, src, ftmp);\n+    \/\/ Convert double to jlong, use RoundTowardsNegative\n+    fcvtmsd(dst, ftmp);\n+  }\n+  bind(DONE);\n+  BLOCK_COMMENT(\"} java_round_double\");\n+}\n+\n+void MacroAssembler::java_round_float(Register dst, FloatRegister src,\n+                                      FloatRegister ftmp) {\n+  Label DONE;\n+  BLOCK_COMMENT(\"java_round_float: { \");\n+  fmovs(rscratch1, src);\n+  \/\/ Use RoundToNearestTiesAway unless src small and -ve.\n+  fcvtassw(dst, src);\n+  \/\/ Test if src >= 0 || abs(src) >= 0x1.0p23\n+  eor(rscratch1, rscratch1, 0x80000000); \/\/ flip sign bit\n+  mov(rscratch2, jint_cast(0x1.0p23f));\n+  cmp(rscratch1, rscratch2);\n+  br(HS, DONE); {\n+    \/\/ src < 0 && |src| < 0x1.0p23\n+    \/\/ src may have a fractional part, so add 0.5\n+    fmovs(ftmp, 0.5f);\n+    fadds(ftmp, src, ftmp);\n+    \/\/ Convert float to jint, use RoundTowardsNegative\n+    fcvtmssw(dst, ftmp);\n+  }\n+  bind(DONE);\n+  BLOCK_COMMENT(\"} java_round_float\");\n+}\n+\n@@ -5643,1 +6010,1 @@\n-      eden_allocate(r0, noreg, obj_size, tmp1, slow_case);\n+      b(slow_case);\n@@ -5652,1 +6019,1 @@\n-      eden_allocate(r0, tmp2, 0, tmp1, slow_case);\n+      b(slow_case);\n@@ -5655,1 +6022,1 @@\n-  if (UseTLAB || Universe::heap()->supports_inline_contig_alloc()) {\n+  if (UseTLAB) {\n@@ -5679,1 +6046,1 @@\n-    \/\/ Must have already branched to slow_case in eden_allocate() above.\n+    \/\/ Must have already branched to slow_case above.\n@@ -6024,1 +6391,1 @@\n-  if (VM_Version::features() & VM_Version::CPU_DCPOP) {\n+  if (VM_Version::supports_dcpop()) {\n@@ -6040,1 +6407,1 @@\n-void MacroAssembler::verify_sve_vector_length() {\n+void MacroAssembler::verify_sve_vector_length(Register tmp) {\n@@ -6044,3 +6411,3 @@\n-  movw(rscratch1, zr);\n-  sve_inc(rscratch1, B);\n-  subsw(zr, rscratch1, VM_Version::get_initial_sve_vector_length());\n+  movw(tmp, zr);\n+  sve_inc(tmp, B);\n+  subsw(zr, tmp, VM_Version::get_initial_sve_vector_length());\n@@ -6146,1 +6513,1 @@\n-\/\/ Sign the return value in the given register. Use before updating the LR in the exisiting stack\n+\/\/ Sign the return value in the given register. Use before updating the LR in the existing stack\n@@ -6169,1 +6536,1 @@\n-\/\/ Authenticate the return value in the given register. Use before updating the LR in the exisiting\n+\/\/ Authenticate the return value in the given register. Use before updating the LR in the existing\n@@ -6206,0 +6573,174 @@\n+\n+\/\/ The java_calling_convention describes stack locations as ideal slots on\n+\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n+\/\/ (like the placement of the register window) the slots must be biased by\n+\/\/ the following value.\n+static int reg2offset_in(VMReg r) {\n+  \/\/ Account for saved rfp and lr\n+  \/\/ This should really be in_preserve_stack_slots\n+  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n+}\n+\n+static int reg2offset_out(VMReg r) {\n+  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n+}\n+\n+\/\/ On 64bit we will store integer like items to the stack as\n+\/\/ 64bits items (AArch64 ABI) even though java would only store\n+\/\/ 32bits for a parameter. On 32bit it will simply be 32bits\n+\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      sxtw(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ An oop arg. Must pass a handle not the oop itself\n+void MacroAssembler::object_move(\n+                        OopMap* map,\n+                        int oop_handle_offset,\n+                        int framesize_in_slots,\n+                        VMRegPair src,\n+                        VMRegPair dst,\n+                        bool is_receiver,\n+                        int* receiver_offset) {\n+\n+  \/\/ must pass a handle. First figure out the location we use as a handle\n+\n+  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n+\n+  \/\/ See if oop is NULL if it is we need no handle\n+\n+  if (src.first()->is_stack()) {\n+\n+    \/\/ Oop is already on the stack as an argument\n+    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n+    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n+    if (is_receiver) {\n+      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n+    }\n+\n+    ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+    lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n+    \/\/ conditionally move a NULL\n+    cmp(rscratch1, zr);\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  } else {\n+\n+    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+\n+    const Register rOop = src.first()->as_Register();\n+    int oop_slot;\n+    if (rOop == j_rarg0)\n+      oop_slot = 0;\n+    else if (rOop == j_rarg1)\n+      oop_slot = 1;\n+    else if (rOop == j_rarg2)\n+      oop_slot = 2;\n+    else if (rOop == j_rarg3)\n+      oop_slot = 3;\n+    else if (rOop == j_rarg4)\n+      oop_slot = 4;\n+    else if (rOop == j_rarg5)\n+      oop_slot = 5;\n+    else if (rOop == j_rarg6)\n+      oop_slot = 6;\n+    else {\n+      assert(rOop == j_rarg7, \"wrong register\");\n+      oop_slot = 7;\n+    }\n+\n+    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n+    int offset = oop_slot*VMRegImpl::stack_slot_size;\n+\n+    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n+    \/\/ Store oop in handle area, may be NULL\n+    str(rOop, Address(sp, offset));\n+    if (is_receiver) {\n+      *receiver_offset = offset;\n+    }\n+\n+    cmp(rOop, zr);\n+    lea(rHandle, Address(sp, offset));\n+    \/\/ conditionally move a NULL\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  }\n+\n+  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n+  if (dst.first()->is_stack()) {\n+    str(rHandle, Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A float arg may have to do float reg int reg conversion\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+ if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldrw(tmp, Address(rfp, reg2offset_in(src.first())));\n+      strw(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ldrs(dst.first()->as_FloatRegister(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      strs(src.first()->as_FloatRegister(), Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A long move\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      mov(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\n+\/\/ A double move\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+ if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ldrd(dst.first()->as_FloatRegister(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      strd(src.first()->as_FloatRegister(), Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":871,"deletions":330,"binary":false,"changes":1201,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -40,0 +41,2 @@\n+class OopMap;\n+\n@@ -112,1 +115,2 @@\n-  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod);\n+  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp = rscratch1);\n+  void rt_call(address dest, Register tmp = rscratch1);\n@@ -201,2 +205,5 @@\n-  inline void cmnw(Register Rd, unsigned imm) { addsw(zr, Rd, imm); }\n-  inline void cmn(Register Rd, unsigned imm) { adds(zr, Rd, imm); }\n+  template<class T>\n+  inline void cmnw(Register Rd, T imm) { addsw(zr, Rd, imm); }\n+\n+  inline void cmn(Register Rd, unsigned char imm8)  { adds(zr, Rd, imm8); }\n+  inline void cmn(Register Rd, unsigned imm) = delete;\n@@ -220,1 +227,1 @@\n-      addw(Rd, Rn, 0U);\n+      Assembler::addw(Rd, Rn, 0U);\n@@ -229,1 +236,1 @@\n-      add(Rd, Rn, 0U);\n+      Assembler::add(Rd, Rn, 0U);\n@@ -440,1 +447,1 @@\n-    if ((VM_Version::features() & VM_Version::CPU_A53MAC) && Ra != zr)        \\\n+    if (VM_Version::supports_a53mac() && Ra != zr)                            \\\n@@ -484,1 +491,1 @@\n-  \/\/ 64 bits of each vector register. Additonal registers can be excluded\n+  \/\/ 64 bits of each vector register. Additional registers can be excluded\n@@ -516,1 +523,1 @@\n-  void mov(FloatRegister Vd, SIMD_Arrangement T, uint32_t imm32);\n+  void mov(FloatRegister Vd, SIMD_Arrangement T, uint64_t imm64);\n@@ -696,1 +703,1 @@\n-  \/\/ increment\/decrement for 64 bit operatons and\n+  \/\/ increment\/decrement for 64 bit operations and\n@@ -726,0 +733,3 @@\n+  \/\/ nop\n+  void post_call_nop();\n+\n@@ -742,0 +752,14 @@\n+  \/\/ support for argument shuffling\n+  void move32_64(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void float_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void long_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void double_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void object_move(\n+                   OopMap* map,\n+                   int oop_handle_offset,\n+                   int framesize_in_slots,\n+                   VMRegPair src,\n+                   VMRegPair dst,\n+                   bool is_receiver,\n+                   int* receiver_offset);\n+\n@@ -926,0 +950,3 @@\n+  void push_cont_fastpath(Register java_thread);\n+  void pop_cont_fastpath(Register java_thread);\n+\n@@ -929,0 +956,4 @@\n+  \/\/ java.lang.Math::round intrinsics\n+  void java_round_double(Register dst, FloatRegister src, FloatRegister ftmp);\n+  void java_round_float(Register dst, FloatRegister src, FloatRegister ftmp);\n+\n@@ -939,7 +970,0 @@\n-  void eden_allocate(\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n@@ -1015,1 +1039,1 @@\n-  void verify_sve_vector_length();\n+  void verify_sve_vector_length(Register tmp = rscratch1);\n@@ -1026,2 +1050,13 @@\n-  void verify_oop(Register reg, const char* s = \"broken oop\");\n-  void verify_oop_addr(Address addr, const char * s = \"broken oop addr\");\n+  void _verify_oop(Register reg, const char* s, const char* file, int line);\n+  void _verify_oop_addr(Address addr, const char * s, const char* file, int line);\n+\n+  void _verify_oop_checked(Register reg, const char* s, const char* file, int line) {\n+    if (VerifyOops) {\n+      _verify_oop(reg, s, file, line);\n+    }\n+  }\n+  void _verify_oop_addr_checked(Address reg, const char* s, const char* file, int line) {\n+    if (VerifyOops) {\n+      _verify_oop_addr(reg, s, file, line);\n+    }\n+  }\n@@ -1033,0 +1068,3 @@\n+#define verify_oop(reg) _verify_oop_checked(reg, \"broken oop \" #reg, __FILE__, __LINE__)\n+#define verify_oop_msg(reg, msg) _verify_oop_checked(reg, \"broken oop \" #reg \", \" #msg, __FILE__, __LINE__)\n+#define verify_oop_addr(addr) _verify_oop_addr_checked(addr, \"broken oop addr \" #addr, __FILE__, __LINE__)\n@@ -1050,0 +1088,4 @@\n+  void _assert_asm(Condition cc, const char* msg);\n+#define assert_asm0(cc, msg) _assert_asm(cc, FILE_AND_LINE \": \" msg)\n+#define assert_asm(masm, command, cc, msg) DEBUG_ONLY(command; (masm)->_assert_asm(cc, FILE_AND_LINE \": \" #command \" \" #cc \": \" msg))\n+\n@@ -1075,1 +1117,1 @@\n-                          Label &suceed, Label *fail);\n+                          Label &succeed, Label *fail);\n@@ -1077,1 +1119,1 @@\n-                  Label &suceed, Label *fail);\n+                  Label &succeed, Label *fail);\n@@ -1080,1 +1122,1 @@\n-                  Label &suceed, Label *fail);\n+                  Label &succeed, Label *fail);\n@@ -1128,3 +1170,71 @@\n-  \/\/ Calls\n-\n-  address trampoline_call(Address entry, CodeBuffer* cbuf = NULL);\n+  \/\/ AArch64 OpenJDK uses four different types of calls:\n+  \/\/   - direct call: bl pc_relative_offset\n+  \/\/     This is the shortest and the fastest, but the offset has the range:\n+  \/\/     +\/-128MB for the release build, +\/-2MB for the debug build.\n+  \/\/\n+  \/\/   - far call: adrp reg, pc_relative_offset; add; bl reg\n+  \/\/     This is longer than a direct call. The offset has\n+  \/\/     the range +\/-4GB. As the code cache size is limited to 4GB,\n+  \/\/     far calls can reach anywhere in the code cache. If a jump is\n+  \/\/     needed rather than a call, a far jump 'b reg' can be used instead.\n+  \/\/     All instructions are embedded at a call site.\n+  \/\/\n+  \/\/   - trampoline call:\n+  \/\/     This is only available in C1\/C2-generated code (nmethod). It is a combination\n+  \/\/     of a direct call, which is used if the destination of a call is in range,\n+  \/\/     and a register-indirect call. It has the advantages of reaching anywhere in\n+  \/\/     the AArch64 address space and being patchable at runtime when the generated\n+  \/\/     code is being executed by other threads.\n+  \/\/\n+  \/\/     [Main code section]\n+  \/\/       bl trampoline\n+  \/\/     [Stub code section]\n+  \/\/     trampoline:\n+  \/\/       ldr reg, pc + 8\n+  \/\/       br reg\n+  \/\/       <64-bit destination address>\n+  \/\/\n+  \/\/     If the destination is in range when the generated code is moved to the code\n+  \/\/     cache, 'bl trampoline' is replaced with 'bl destination' and the trampoline\n+  \/\/     is not used.\n+  \/\/     The optimization does not remove the trampoline from the stub section.\n+  \/\/     This is necessary because the trampoline may well be redirected later when\n+  \/\/     code is patched, and the new destination may not be reachable by a simple BR\n+  \/\/     instruction.\n+  \/\/\n+  \/\/   - indirect call: move reg, address; blr reg\n+  \/\/     This too can reach anywhere in the address space, but it cannot be\n+  \/\/     patched while code is running, so it must only be modified at a safepoint.\n+  \/\/     This form of call is most suitable for targets at fixed addresses, which\n+  \/\/     will never be patched.\n+  \/\/\n+  \/\/ The patching we do conforms to the \"Concurrent modification and\n+  \/\/ execution of instructions\" section of the Arm Architectural\n+  \/\/ Reference Manual, which only allows B, BL, BRK, HVC, ISB, NOP, SMC,\n+  \/\/ or SVC instructions to be modified while another thread is\n+  \/\/ executing them.\n+  \/\/\n+  \/\/ To patch a trampoline call when the BL can't reach, we first modify\n+  \/\/ the 64-bit destination address in the trampoline, then modify the\n+  \/\/ BL to point to the trampoline, then flush the instruction cache to\n+  \/\/ broadcast the change to all executing threads. See\n+  \/\/ NativeCall::set_destination_mt_safe for the details.\n+  \/\/\n+  \/\/ There is a benign race in that the other thread might observe the\n+  \/\/ modified BL before it observes the modified 64-bit destination\n+  \/\/ address. That does not matter because the destination method has been\n+  \/\/ invalidated, so there will be a trap at its start.\n+  \/\/ For this to work, the destination address in the trampoline is\n+  \/\/ always updated, even if we're not using the trampoline.\n+\n+  \/\/ Emit a direct call if the entry address will always be in range,\n+  \/\/ otherwise a trampoline call.\n+  \/\/ Supported entry.rspec():\n+  \/\/ - relocInfo::runtime_call_type\n+  \/\/ - relocInfo::opt_virtual_call_type\n+  \/\/ - relocInfo::static_call_type\n+  \/\/ - relocInfo::virtual_call_type\n+  \/\/\n+  \/\/ Return: NULL if CodeCache is full.\n+  address trampoline_call(Address entry, CodeBuffer* cbuf = NULL) { return trampoline_call1(entry, cbuf, true); }\n+  address trampoline_call1(Address entry, CodeBuffer* cbuf, bool check_emit_size = true);\n@@ -1136,2 +1246,16 @@\n-  \/\/ Jumps that can reach anywhere in the code cache.\n-  \/\/ Trashes tmp.\n+  \/\/ Check if branches to the the non nmethod section require a far jump\n+  static bool codestub_branch_needs_far_jump() {\n+    return CodeCache::max_distance_to_non_nmethod() > branch_range;\n+  }\n+\n+  \/\/ Emit a direct call\/jump if the entry address will always be in range,\n+  \/\/ otherwise a far call\/jump.\n+  \/\/ The address must be inside the code cache.\n+  \/\/ Supported entry.rspec():\n+  \/\/ - relocInfo::external_word_type\n+  \/\/ - relocInfo::runtime_call_type\n+  \/\/ - relocInfo::none\n+  \/\/ In the case of a far call\/jump, the entry address is put in the tmp register.\n+  \/\/ The tmp register is invalidated.\n+  \/\/\n+  \/\/ Far_jump returns the amount of the emitted code.\n@@ -1139,1 +1263,1 @@\n-  void far_jump(Address entry, CodeBuffer *cbuf = NULL, Register tmp = rscratch1);\n+  int far_jump(Address entry, CodeBuffer *cbuf = NULL, Register tmp = rscratch1);\n@@ -1141,2 +1265,2 @@\n-  static int far_branch_size() {\n-    if (far_branches()) {\n+  static int far_codestub_branch_size() {\n+    if (codestub_branch_needs_far_jump()) {\n@@ -1162,1 +1286,1 @@\n-  \/\/ CRC32 code for java.util.zip.CRC32::updateBytes() instrinsic.\n+  \/\/ CRC32 code for java.util.zip.CRC32::updateBytes() intrinsic.\n@@ -1166,1 +1290,1 @@\n-  \/\/ CRC32 code for java.util.zip.CRC32C::updateBytes() instrinsic.\n+  \/\/ CRC32 code for java.util.zip.CRC32C::updateBytes() intrinsic.\n@@ -1185,1 +1309,1 @@\n-  void wrap_add_sub_imm_insn(Register Rd, Register Rn, unsigned imm,\n+  void wrap_add_sub_imm_insn(Register Rd, Register Rn, uint64_t imm,\n@@ -1187,9 +1311,9 @@\n-                             add_sub_reg_insn insn2);\n-  \/\/ Seperate vsn which sets the flags\n-  void wrap_adds_subs_imm_insn(Register Rd, Register Rn, unsigned imm,\n-                             add_sub_imm_insn insn1,\n-                             add_sub_reg_insn insn2);\n-\n-#define WRAP(INSN)                                                      \\\n-  void INSN(Register Rd, Register Rn, unsigned imm) {                   \\\n-    wrap_add_sub_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN); \\\n+                             add_sub_reg_insn insn2, bool is32);\n+  \/\/ Separate vsn which sets the flags\n+  void wrap_adds_subs_imm_insn(Register Rd, Register Rn, uint64_t imm,\n+                               add_sub_imm_insn insn1,\n+                               add_sub_reg_insn insn2, bool is32);\n+\n+#define WRAP(INSN, is32)                                                \\\n+  void INSN(Register Rd, Register Rn, uint64_t imm) {                   \\\n+    wrap_add_sub_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN, is32); \\\n@@ -1212,1 +1336,1 @@\n-  WRAP(add) WRAP(addw) WRAP(sub) WRAP(subw)\n+  WRAP(add, false) WRAP(addw, true) WRAP(sub, false) WRAP(subw, true)\n@@ -1215,3 +1339,3 @@\n-#define WRAP(INSN)                                                      \\\n-  void INSN(Register Rd, Register Rn, unsigned imm) {                   \\\n-    wrap_adds_subs_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN); \\\n+#define WRAP(INSN, is32)                                                \\\n+  void INSN(Register Rd, Register Rn, uint64_t imm) {                   \\\n+    wrap_adds_subs_imm_insn(Rd, Rn, imm, &Assembler::INSN, &Assembler::INSN, is32); \\\n@@ -1234,1 +1358,1 @@\n-  WRAP(adds) WRAP(addsw) WRAP(subs) WRAP(subsw)\n+  WRAP(adds, false) WRAP(addsw, true) WRAP(subs, false) WRAP(subsw, true)\n@@ -1310,1 +1434,1 @@\n-  \/\/ CRC32 code for java.util.zip.CRC32::updateBytes() instrinsic.\n+  \/\/ CRC32 code for java.util.zip.CRC32::updateBytes() intrinsic.\n@@ -1327,1 +1451,1 @@\n-  void zero_words(Register base, uint64_t cnt);\n+  address zero_words(Register base, uint64_t cnt);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":173,"deletions":49,"binary":false,"changes":222,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -192,1 +192,1 @@\n-  \/\/ r13: sender SP (must preserve; see prepare_to_jump_from_interpreted)\n+  \/\/ r19_sender_sp: sender SP (must preserve; see prepare_to_jump_from_interpreted)\n@@ -265,0 +265,15 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_foreign_abi_NativeEntryPoint::downcall_stub_address_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ br(temp_target);\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -275,1 +290,1 @@\n-  Register temp3 = r14;  \/\/ r13 is live by this point: it contains the sender SP\n+  Register temp3 = r14;\n@@ -277,1 +292,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n@@ -286,4 +301,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -293,0 +305,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n@@ -348,1 +363,1 @@\n-    \/\/  r13 - interpreter linkage (if interpreted)  ??? FIXME\n+    \/\/  r19 - interpreter linkage (if interpreted)\n@@ -435,1 +450,1 @@\n-    \/\/ live at this point:  rmethod, r13 (if interpreted)\n+    \/\/ live at this point:  rmethod, r19_sender_sp (if interpreted)\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":25,"deletions":10,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -45,0 +46,2 @@\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -82,2 +85,2 @@\n-    rbp_off = 0,\n-    rbp_off2,\n+    rfp_off = 0,\n+    rfp_off2,\n@@ -271,2 +274,2 @@\n-  __ leave();\n-\n+  __ ldp(rfp, lr, Address(__ post(sp, 2 * wordSize)));\n+  __ authenticate_return_address();\n@@ -283,14 +286,0 @@\n-\/\/ The java_calling_convention describes stack locations as ideal slots on\n-\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n-\/\/ (like the placement of the register window) the slots must be biased by\n-\/\/ the following value.\n-static int reg2offset_in(VMReg r) {\n-  \/\/ Account for saved rfp and lr\n-  \/\/ This should really be in_preserve_stack_slots\n-  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n-}\n-\n-static int reg2offset_out(VMReg r) {\n-  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n-}\n-\n@@ -705,1 +694,1 @@\n-  __ mov(r13, sp);\n+  __ mov(r19_sender_sp, sp);\n@@ -805,6 +794,4 @@\n-  \/\/ Note: r13 contains the senderSP on entry. We must preserve it since\n-  \/\/ we may do a i2c -> c2i transition if we lose a race where compiled\n-  \/\/ code goes non-entrant while we get args ready.\n-\n-  \/\/ In addition we use r13 to locate all the interpreter args because\n-  \/\/ we must align the stack to 16 bytes.\n+  \/\/ Note: r19_sender_sp contains the senderSP on entry. We must\n+  \/\/ preserve it since we may do a i2c -> c2i transition if we lose a\n+  \/\/ race where compiled code goes non-entrant while we get args\n+  \/\/ ready.\n@@ -966,0 +953,4 @@\n+  __ mov(rscratch2, rscratch1);\n+  __ push_cont_fastpath(rthread); \/\/ Set JavaThread::_cont_fastpath to the sp of the oldest interpreted frame we know about; kills rscratch1\n+  __ mov(rscratch1, rscratch2);\n+\n@@ -996,1 +987,1 @@\n-  \/\/ compiled code, which relys solely on SP and not FP, get sick).\n+  \/\/ compiled code, which relies solely on SP and not FP, get sick).\n@@ -1210,166 +1201,0 @@\n-\/\/ On 64 bit we will store integer like items to the stack as\n-\/\/ 64 bits items (Aarch64 abi) even though java would only store\n-\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n-\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n-static void move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ sxtw(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\/\/ An oop arg. Must pass a handle not the oop itself\n-static void object_move(MacroAssembler* masm,\n-                        OopMap* map,\n-                        int oop_handle_offset,\n-                        int framesize_in_slots,\n-                        VMRegPair src,\n-                        VMRegPair dst,\n-                        bool is_receiver,\n-                        int* receiver_offset) {\n-\n-  \/\/ must pass a handle. First figure out the location we use as a handle\n-\n-  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n-\n-  \/\/ See if oop is NULL if it is we need no handle\n-\n-  if (src.first()->is_stack()) {\n-\n-    \/\/ Oop is already on the stack as an argument\n-    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n-    if (is_receiver) {\n-      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n-    }\n-\n-    __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-    __ lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n-    \/\/ conditionally move a NULL\n-    __ cmp(rscratch1, zr);\n-    __ csel(rHandle, zr, rHandle, Assembler::EQ);\n-  } else {\n-\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n-\n-    const Register rOop = src.first()->as_Register();\n-    int oop_slot;\n-    if (rOop == j_rarg0)\n-      oop_slot = 0;\n-    else if (rOop == j_rarg1)\n-      oop_slot = 1;\n-    else if (rOop == j_rarg2)\n-      oop_slot = 2;\n-    else if (rOop == j_rarg3)\n-      oop_slot = 3;\n-    else if (rOop == j_rarg4)\n-      oop_slot = 4;\n-    else if (rOop == j_rarg5)\n-      oop_slot = 5;\n-    else if (rOop == j_rarg6)\n-      oop_slot = 6;\n-    else {\n-      assert(rOop == j_rarg7, \"wrong register\");\n-      oop_slot = 7;\n-    }\n-\n-    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n-    int offset = oop_slot*VMRegImpl::stack_slot_size;\n-\n-    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n-    \/\/ Store oop in handle area, may be NULL\n-    __ str(rOop, Address(sp, offset));\n-    if (is_receiver) {\n-      *receiver_offset = offset;\n-    }\n-\n-    __ cmp(rOop, zr);\n-    __ lea(rHandle, Address(sp, offset));\n-    \/\/ conditionally move a NULL\n-    __ csel(rHandle, zr, rHandle, Assembler::EQ);\n-  }\n-\n-  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n-  if (dst.first()->is_stack()) {\n-    __ str(rHandle, Address(sp, reg2offset_out(dst.first())));\n-  }\n-}\n-\n-\/\/ A float arg may have to do float reg int reg conversion\n-static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n-         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ ldrw(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ strw(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (src.first() != dst.first()) {\n-    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n-      __ fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n-    else\n-      ShouldNotReachHere();\n-  }\n-}\n-\n-\/\/ A long move\n-static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ mov(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\n-\/\/ A double move\n-static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n-         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (src.first() != dst.first()) {\n-    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n-      __ fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n-    else\n-      ShouldNotReachHere();\n-  }\n-}\n-\n@@ -1441,10 +1266,0 @@\n-static void rt_call(MacroAssembler* masm, address dest) {\n-  CodeBlob *cb = CodeCache::find_blob(dest);\n-  if (cb) {\n-    __ far_call(RuntimeAddress(dest));\n-  } else {\n-    __ lea(rscratch1, RuntimeAddress(dest));\n-    __ blr(rscratch1);\n-  }\n-}\n-\n@@ -1473,0 +1288,127 @@\n+\/\/ defined in stubGenerator_aarch64.cpp\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots);\n+void fill_continuation_entry(MacroAssembler* masm);\n+void continuation_enter_cleanup(MacroAssembler* masm);\n+\n+\/\/ enterSpecial(Continuation c, boolean isContinue, boolean isVirtualThread)\n+\/\/ On entry: c_rarg1 -- the continuation object\n+\/\/           c_rarg2 -- isContinue\n+\/\/           c_rarg3 -- isVirtualThread\n+static void gen_continuation_enter(MacroAssembler* masm,\n+                                 const methodHandle& method,\n+                                 const BasicType* sig_bt,\n+                                 const VMRegPair* regs,\n+                                 int& exception_offset,\n+                                 OopMapSet*oop_maps,\n+                                 int& frame_complete,\n+                                 int& stack_slots,\n+                                 int& interpreted_entry_offset,\n+                                 int& compiled_entry_offset) {\n+  \/\/verify_oop_args(masm, method, sig_bt, regs);\n+  Address resolve(SharedRuntime::get_resolve_static_call_stub(), relocInfo::static_call_type);\n+\n+  address start = __ pc();\n+\n+  Label call_thaw, exit;\n+\n+  \/\/ i2i entry used at interp_only_mode only\n+  interpreted_entry_offset = __ pc() - start;\n+  {\n+\n+#ifdef ASSERT\n+    Label is_interp_only;\n+    __ ldrw(rscratch1, Address(rthread, JavaThread::interp_only_mode_offset()));\n+    __ cbnzw(rscratch1, is_interp_only);\n+    __ stop(\"enterSpecial interpreter entry called when not in interp_only_mode\");\n+    __ bind(is_interp_only);\n+#endif\n+\n+    \/\/ Read interpreter arguments into registers (this is an ad-hoc i2c adapter)\n+    __ ldr(c_rarg1, Address(esp, Interpreter::stackElementSize*2));\n+    __ ldr(c_rarg2, Address(esp, Interpreter::stackElementSize*1));\n+    __ ldr(c_rarg3, Address(esp, Interpreter::stackElementSize*0));\n+    __ push_cont_fastpath(rthread);\n+\n+    __ enter();\n+    stack_slots = 2; \/\/ will be adjusted in setup\n+    OopMap* map = continuation_enter_setup(masm, stack_slots);\n+    \/\/ The frame is complete here, but we only record it for the compiled entry, so the frame would appear unsafe,\n+    \/\/ but that's okay because at the very worst we'll miss an async sample, but we're in interp_only_mode anyway.\n+\n+    fill_continuation_entry(masm);\n+\n+    __ cmp(c_rarg2, (u1)0);\n+    __ br(Assembler::NE, call_thaw);\n+\n+    address mark = __ pc();\n+    __ trampoline_call1(resolve, NULL, false);\n+\n+    oop_maps->add_gc_map(__ pc() - start, map);\n+    __ post_call_nop();\n+\n+    __ b(exit);\n+\n+    CodeBuffer* cbuf = masm->code_section()->outer();\n+    CompiledStaticCall::emit_to_interp_stub(*cbuf, mark);\n+  }\n+\n+  \/\/ compiled entry\n+  __ align(CodeEntryAlignment);\n+  compiled_entry_offset = __ pc() - start;\n+\n+  __ enter();\n+  stack_slots = 2; \/\/ will be adjusted in setup\n+  OopMap* map = continuation_enter_setup(masm, stack_slots);\n+  frame_complete = __ pc() - start;\n+\n+  fill_continuation_entry(masm);\n+\n+  __ cmp(c_rarg2, (u1)0);\n+  __ br(Assembler::NE, call_thaw);\n+\n+  address mark = __ pc();\n+  __ trampoline_call1(resolve, NULL, false);\n+\n+  oop_maps->add_gc_map(__ pc() - start, map);\n+  __ post_call_nop();\n+\n+  __ b(exit);\n+\n+  __ bind(call_thaw);\n+\n+  __ rt_call(CAST_FROM_FN_PTR(address, StubRoutines::cont_thaw()));\n+  oop_maps->add_gc_map(__ pc() - start, map->deep_copy());\n+  ContinuationEntry::_return_pc_offset = __ pc() - start;\n+  __ post_call_nop();\n+\n+  __ bind(exit);\n+  continuation_enter_cleanup(masm);\n+  __ leave();\n+  __ ret(lr);\n+\n+  \/\/\/ exception handling\n+\n+  exception_offset = __ pc() - start;\n+  {\n+      __ mov(r19, r0); \/\/ save return value contaning the exception oop in callee-saved R19\n+\n+      continuation_enter_cleanup(masm);\n+\n+      __ ldr(c_rarg1, Address(rfp, wordSize)); \/\/ return address\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), rthread, c_rarg1);\n+\n+      \/\/ see OptoRuntime::generate_exception_blob: r0 -- exception oop, r3 -- exception pc\n+\n+      __ mov(r1, r0); \/\/ the exception handler\n+      __ mov(r0, r19); \/\/ restore return value contaning the exception oop\n+      __ verify_oop(r0);\n+\n+      __ leave();\n+      __ mov(r3, lr);\n+      __ br(r1); \/\/ the exception handler\n+  }\n+\n+  CodeBuffer* cbuf = masm->code_section()->outer();\n+  CompiledStaticCall::emit_to_interp_stub(*cbuf, mark);\n+}\n+\n@@ -1490,1 +1432,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1492,0 +1434,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = r19;  \/\/ known to be free at this point\n@@ -1554,0 +1499,34 @@\n+  if (method->is_continuation_enter_intrinsic()) {\n+    vmIntrinsics::ID iid = method->intrinsic_id();\n+    intptr_t start = (intptr_t)__ pc();\n+    int vep_offset = 0;\n+    int exception_offset = 0;\n+    int frame_complete = 0;\n+    int stack_slots = 0;\n+    OopMapSet* oop_maps =  new OopMapSet();\n+    int interpreted_entry_offset = -1;\n+    gen_continuation_enter(masm,\n+                         method,\n+                         in_sig_bt,\n+                         in_regs,\n+                         exception_offset,\n+                         oop_maps,\n+                         frame_complete,\n+                         stack_slots,\n+                         interpreted_entry_offset,\n+                         vep_offset);\n+    __ flush();\n+    nmethod* nm = nmethod::new_native_nmethod(method,\n+                                              compile_id,\n+                                              masm->code(),\n+                                              vep_offset,\n+                                              frame_complete,\n+                                              stack_slots,\n+                                              in_ByteSize(-1),\n+                                              in_ByteSize(-1),\n+                                              oop_maps,\n+                                              exception_offset);\n+    ContinuationEntry::set_enter_code(nm, interpreted_entry_offset);\n+    return nm;\n+  }\n+\n@@ -1736,1 +1715,1 @@\n-  bs->nmethod_entry_barrier(masm);\n+  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n@@ -1825,3 +1804,3 @@\n-        object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n-                    ((i == 0) && (!is_static)),\n-                    &receiver_offset);\n+        __ object_move(map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n+                       ((i == 0) && (!is_static)),\n+                       &receiver_offset);\n@@ -1834,1 +1813,1 @@\n-        float_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ float_move(in_regs[i], out_regs[c_arg]);\n@@ -1842,1 +1821,1 @@\n-        double_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ double_move(in_regs[i], out_regs[c_arg]);\n@@ -1847,1 +1826,1 @@\n-        long_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ long_move(in_regs[i], out_regs[c_arg]);\n@@ -1854,1 +1833,1 @@\n-        move32_64(masm, in_regs[i], out_regs[c_arg]);\n+        __ move32_64(in_regs[i], out_regs[c_arg]);\n@@ -1922,1 +1901,1 @@\n-\n+    Label count;\n@@ -1948,3 +1927,1 @@\n-      { Label here;\n-        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, lock_done, \/*fallthrough*\/NULL);\n-      }\n+      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n@@ -1973,0 +1950,2 @@\n+    __ bind(count);\n+    __ increment(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -1989,1 +1968,1 @@\n-  rt_call(masm, native_func);\n+  __ rt_call(native_func);\n@@ -2075,1 +2054,1 @@\n-    Label done;\n+    Label done, not_recursive;\n@@ -2080,1 +2059,3 @@\n-      __ cbz(rscratch1, done);\n+      __ cbnz(rscratch1, not_recursive);\n+      __ decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n+      __ b(done);\n@@ -2083,0 +2064,2 @@\n+    __ bind(not_recursive);\n+\n@@ -2095,3 +2078,4 @@\n-      Label succeed;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, succeed, &slow_path_unlock);\n-      __ bind(succeed);\n+      Label count;\n+      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n+      __ bind(count);\n+      __ decrement(Address(rthread, JavaThread::held_monitor_count_offset()));\n@@ -2204,1 +2188,1 @@\n-    rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n+    __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n@@ -2231,1 +2215,1 @@\n-  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n@@ -2339,1 +2323,1 @@\n-  \/\/ address has been pushed on the the stack, and return values are in\n+  \/\/ address has been pushed on the stack, and return values are in\n@@ -2925,1 +2909,1 @@\n-  \/\/ When the signal occured, the LR was either signed and stored on the stack (in which\n+  \/\/ When the signal occurred, the LR was either signed and stored on the stack (in which\n@@ -2935,1 +2919,1 @@\n-  \/\/ work outselves.\n+  \/\/ work ourselves.\n@@ -3090,1 +3074,1 @@\n-  \/\/ We are back the the original state on entry and ready to go.\n+  \/\/ We are back to the original state on entry and ready to go.\n@@ -3117,1 +3101,1 @@\n-\/\/ This is here instead of runtime_x86_64.cpp because it uses SimpleRuntimeFrame\n+\/\/ This is here instead of runtime_aarch64_64.cpp because it uses SimpleRuntimeFrame\n@@ -3251,0 +3235,1 @@\n+\n@@ -3373,252 +3358,0 @@\n-\n-\/\/ ---------------------------------------------------------------\n-\n-class NativeInvokerGenerator : public StubCodeGenerator {\n-  address _call_target;\n-  int _shadow_space_bytes;\n-\n-  const GrowableArray<VMReg>& _input_registers;\n-  const GrowableArray<VMReg>& _output_registers;\n-\n-  int _frame_complete;\n-  int _framesize;\n-  OopMapSet* _oop_maps;\n-public:\n-  NativeInvokerGenerator(CodeBuffer* buffer,\n-                         address call_target,\n-                         int shadow_space_bytes,\n-                         const GrowableArray<VMReg>& input_registers,\n-                         const GrowableArray<VMReg>& output_registers)\n-   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n-     _call_target(call_target),\n-     _shadow_space_bytes(shadow_space_bytes),\n-     _input_registers(input_registers),\n-     _output_registers(output_registers),\n-     _frame_complete(0),\n-     _framesize(0),\n-     _oop_maps(NULL) {\n-    assert(_output_registers.length() <= 1\n-           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n-  }\n-\n-  void generate();\n-\n-  int spill_size_in_bytes() const {\n-    if (_output_registers.length() == 0) {\n-      return 0;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    if (reg->is_Register()) {\n-      return 8;\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        return Matcher::scalable_vector_reg_size(T_BYTE);\n-      }\n-      return 16;\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-\n-  void spill_output_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ spill(reg->as_Register(), true, 0);\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        __ spill_sve_vector(reg->as_FloatRegister(), 0, Matcher::scalable_vector_reg_size(T_BYTE));\n-      } else {\n-        __ spill(reg->as_FloatRegister(), __ Q, 0);\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  void fill_output_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ unspill(reg->as_Register(), true, 0);\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        __ unspill_sve_vector(reg->as_FloatRegister(), 0, Matcher::scalable_vector_reg_size(T_BYTE));\n-      } else {\n-        __ unspill(reg->as_FloatRegister(), __ Q, 0);\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  int frame_complete() const {\n-    return _frame_complete;\n-  }\n-\n-  int framesize() const {\n-    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n-  }\n-\n-  OopMapSet* oop_maps() const {\n-    return _oop_maps;\n-  }\n-\n-private:\n-#ifdef ASSERT\n-  bool target_uses_register(VMReg reg) {\n-    return _input_registers.contains(reg) || _output_registers.contains(reg);\n-  }\n-#endif\n-};\n-\n-static const int native_invoker_code_size = 1024;\n-\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  int locs_size  = 64;\n-  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n-  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n-  g.generate();\n-  code.log_section_sizes(\"nep_invoker_blob\");\n-\n-  RuntimeStub* stub =\n-    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n-                                  &code,\n-                                  g.frame_complete(),\n-                                  g.framesize(),\n-                                  g.oop_maps(), false);\n-  return stub;\n-}\n-\n-void NativeInvokerGenerator::generate() {\n-  assert(!(target_uses_register(rscratch1->as_VMReg())\n-           || target_uses_register(rscratch2->as_VMReg())\n-           || target_uses_register(rthread->as_VMReg())),\n-         \"Register conflict\");\n-\n-  enum layout {\n-    rbp_off,\n-    rbp_off2,\n-    return_off,\n-    return_off2,\n-    framesize \/\/ inclusive of return address\n-  };\n-\n-  assert(_shadow_space_bytes == 0, \"not expecting shadow space on AArch64\");\n-  _framesize = align_up(framesize + (spill_size_in_bytes() >> LogBytesPerInt), 4);\n-  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n-\n-  _oop_maps  = new OopMapSet();\n-  MacroAssembler* masm = _masm;\n-\n-  address start = __ pc();\n-\n-  __ enter();\n-\n-  \/\/ lr and fp are already in place\n-  __ sub(sp, rfp, ((unsigned)_framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-  _frame_complete = __ pc() - start;\n-\n-  address the_pc = __ pc();\n-  __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n-  OopMap* map = new OopMap(_framesize, 0);\n-  _oop_maps->add_gc_map(the_pc - start, map);\n-\n-  \/\/ State transition\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n-\n-  rt_call(masm, _call_target);\n-\n-  __ mov(rscratch1, _thread_in_native_trans);\n-  __ strw(rscratch1, Address(rthread, JavaThread::thread_state_offset()));\n-\n-  \/\/ Force this write out before the read below\n-  __ membar(Assembler::LoadLoad | Assembler::LoadStore |\n-            Assembler::StoreLoad | Assembler::StoreStore);\n-\n-  __ verify_sve_vector_length();\n-\n-  Label L_after_safepoint_poll;\n-  Label L_safepoint_poll_slow_path;\n-\n-  __ safepoint_poll(L_safepoint_poll_slow_path, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n-\n-  __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n-  __ cbnzw(rscratch1, L_safepoint_poll_slow_path);\n-\n-  __ bind(L_after_safepoint_poll);\n-\n-  \/\/ change thread state\n-  __ mov(rscratch1, _thread_in_Java);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n-\n-  __ block_comment(\"reguard stack check\");\n-  Label L_reguard;\n-  Label L_after_reguard;\n-  __ ldrb(rscratch1, Address(rthread, JavaThread::stack_guard_state_offset()));\n-  __ cmpw(rscratch1, StackOverflow::stack_guard_yellow_reserved_disabled);\n-  __ br(Assembler::EQ, L_reguard);\n-  __ bind(L_after_reguard);\n-\n-  __ reset_last_Java_frame(true);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(lr);\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n-  __ bind(L_safepoint_poll_slow_path);\n-\n-  \/\/ Need to save the native result registers around any runtime calls.\n-  spill_output_registers();\n-\n-  __ mov(c_rarg0, rthread);\n-  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n-  __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-  __ blr(rscratch1);\n-\n-  fill_output_registers();\n-\n-  __ b(L_after_safepoint_poll);\n-  __ block_comment(\"} L_safepoint_poll_slow_path\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_reguard\");\n-  __ bind(L_reguard);\n-\n-  spill_output_registers();\n-\n-  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n-\n-  fill_output_registers();\n-\n-  __ b(L_after_reguard);\n-\n-  __ block_comment(\"} L_reguard\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ flush();\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":213,"deletions":480,"binary":false,"changes":693,"status":"modified"},{"patch":"@@ -45,0 +45,2 @@\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -47,0 +49,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -50,1 +53,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -76,0 +79,4 @@\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots);\n+void fill_continuation_entry(MacroAssembler* masm);\n+void continuation_enter_cleanup(MacroAssembler* masm);\n+\n@@ -294,1 +301,1 @@\n-    \/\/      r13: sender sp\n+    \/\/      r19_sender_sp: sender sp\n@@ -296,1 +303,1 @@\n-    __ mov(r13, sp);\n+    __ mov(r19_sender_sp, sp);\n@@ -359,0 +366,2 @@\n+    __ pop_cont_fastpath(rthread);\n+\n@@ -474,1 +483,1 @@\n-  \/\/ so it just needs to be generated code wiht no x86 prolog\n+  \/\/ so it just needs to be generated code with no x86 prolog\n@@ -880,1 +889,1 @@\n-      \/\/ then 1 word at offsets {0, 1, 3, 5, 7}. Rather thna use a\n+      \/\/ then 1 word at offsets {0, 1, 3, 5, 7}. Rather than use a\n@@ -1006,1 +1015,1 @@\n-      \/\/ bits 2 and 1 in the count are the tell-tale for whetehr we\n+      \/\/ bits 2 and 1 in the count are the tell-tale for whether we\n@@ -1012,1 +1021,1 @@\n-       \/\/ with ony one intervening stp between the str instructions\n+       \/\/ with only one intervening stp between the str instructions\n@@ -4015,40 +4024,0 @@\n-  \/\/ Safefetch stubs.\n-  void generate_safefetch(const char* name, int size, address* entry,\n-                          address* fault_pc, address* continuation_pc) {\n-    \/\/ safefetch signatures:\n-    \/\/   int      SafeFetch32(int*      adr, int      errValue);\n-    \/\/   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);\n-    \/\/\n-    \/\/ arguments:\n-    \/\/   c_rarg0 = adr\n-    \/\/   c_rarg1 = errValue\n-    \/\/\n-    \/\/ result:\n-    \/\/   PPC_RET  = *adr or errValue\n-\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    \/\/ Entry point, pc or function descriptor.\n-    *entry = __ pc();\n-\n-    \/\/ Load *adr into c_rarg1, may fault.\n-    *fault_pc = __ pc();\n-    switch (size) {\n-      case 4:\n-        \/\/ int32_t\n-        __ ldrw(c_rarg1, Address(c_rarg0, 0));\n-        break;\n-      case 8:\n-        \/\/ int64_t\n-        __ ldr(c_rarg1, Address(c_rarg0, 0));\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-    }\n-\n-    \/\/ return errValue or *adr\n-    *continuation_pc = __ pc();\n-    __ mov(r0, c_rarg1);\n-    __ ret(lr);\n-  }\n-\n@@ -4063,1 +4032,1 @@\n-   * Ouput:\n+   * Output:\n@@ -4104,1 +4073,1 @@\n-   * Ouput:\n+   * Output:\n@@ -4365,1 +4334,1 @@\n-   *    c_rarg3   - y lenth\n+   *    c_rarg3   - y length\n@@ -5205,1 +5174,1 @@\n-    address generate_method_entry_barrier() {\n+  address generate_method_entry_barrier() {\n@@ -5213,0 +5182,14 @@\n+    BarrierSetAssembler* bs_asm = BarrierSet::barrier_set()->barrier_set_assembler();\n+\n+    if (bs_asm->nmethod_patching_type() == NMethodPatchingType::conc_instruction_and_data_patch) {\n+      BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+      \/\/ We can get here despite the nmethod being good, if we have not\n+      \/\/ yet applied our cross modification fence (or data fence).\n+      Address thread_epoch_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()) + 4);\n+      __ lea(rscratch2, ExternalAddress(bs_asm->patching_epoch_addr()));\n+      __ ldrw(rscratch2, rscratch2);\n+      __ strw(rscratch2, thread_epoch_addr);\n+      __ isb();\n+      __ membar(__ LoadLoad);\n+    }\n+\n@@ -5370,0 +5353,112 @@\n+  enum string_compare_mode {\n+    LL,\n+    LU,\n+    UL,\n+    UU,\n+  };\n+\n+  \/\/ The following registers are declared in aarch64.ad\n+  \/\/ r0  = result\n+  \/\/ r1  = str1\n+  \/\/ r2  = cnt1\n+  \/\/ r3  = str2\n+  \/\/ r4  = cnt2\n+  \/\/ r10 = tmp1\n+  \/\/ r11 = tmp2\n+  \/\/ z0  = ztmp1\n+  \/\/ z1  = ztmp2\n+  \/\/ p0  = pgtmp1\n+  \/\/ p1  = pgtmp2\n+  address generate_compare_long_string_sve(string_compare_mode mode) {\n+    __ align(CodeEntryAlignment);\n+    address entry = __ pc();\n+    Register result = r0, str1 = r1, cnt1 = r2, str2 = r3, cnt2 = r4,\n+             tmp1 = r10, tmp2 = r11;\n+\n+    Label LOOP, DONE, MISMATCH;\n+    Register vec_len = tmp1;\n+    Register idx = tmp2;\n+    \/\/ The minimum of the string lengths has been stored in cnt2.\n+    Register cnt = cnt2;\n+    FloatRegister ztmp1 = z0, ztmp2 = z1;\n+    PRegister pgtmp1 = p0, pgtmp2 = p1;\n+\n+#define LOAD_PAIR(ztmp1, ztmp2, pgtmp1, src1, src2, idx)                       \\\n+    switch (mode) {                                                            \\\n+      case LL:                                                                 \\\n+        __ sve_ld1b(ztmp1, __ B, pgtmp1, Address(str1, idx));                  \\\n+        __ sve_ld1b(ztmp2, __ B, pgtmp1, Address(str2, idx));                  \\\n+        break;                                                                 \\\n+      case LU:                                                                 \\\n+        __ sve_ld1b(ztmp1, __ H, pgtmp1, Address(str1, idx));                  \\\n+        __ sve_ld1h(ztmp2, __ H, pgtmp1, Address(str2, idx, Address::lsl(1))); \\\n+        break;                                                                 \\\n+      case UL:                                                                 \\\n+        __ sve_ld1h(ztmp1, __ H, pgtmp1, Address(str1, idx, Address::lsl(1))); \\\n+        __ sve_ld1b(ztmp2, __ H, pgtmp1, Address(str2, idx));                  \\\n+        break;                                                                 \\\n+      case UU:                                                                 \\\n+        __ sve_ld1h(ztmp1, __ H, pgtmp1, Address(str1, idx, Address::lsl(1))); \\\n+        __ sve_ld1h(ztmp2, __ H, pgtmp1, Address(str2, idx, Address::lsl(1))); \\\n+        break;                                                                 \\\n+      default:                                                                 \\\n+        ShouldNotReachHere();                                                  \\\n+    }\n+\n+    const char* stubname;\n+    switch (mode) {\n+      case LL: stubname = \"compare_long_string_same_encoding LL\";      break;\n+      case LU: stubname = \"compare_long_string_different_encoding LU\"; break;\n+      case UL: stubname = \"compare_long_string_different_encoding UL\"; break;\n+      case UU: stubname = \"compare_long_string_same_encoding UU\";      break;\n+      default: ShouldNotReachHere();\n+    }\n+\n+    StubCodeMark mark(this, \"StubRoutines\", stubname);\n+\n+    __ mov(idx, 0);\n+    __ sve_whilelt(pgtmp1, mode == LL ? __ B : __ H, idx, cnt);\n+\n+    if (mode == LL) {\n+      __ sve_cntb(vec_len);\n+    } else {\n+      __ sve_cnth(vec_len);\n+    }\n+\n+    __ sub(rscratch1, cnt, vec_len);\n+\n+    __ bind(LOOP);\n+\n+      \/\/ main loop\n+      LOAD_PAIR(ztmp1, ztmp2, pgtmp1, src1, src2, idx);\n+      __ add(idx, idx, vec_len);\n+      \/\/ Compare strings.\n+      __ sve_cmp(Assembler::NE, pgtmp2, mode == LL ? __ B : __ H, pgtmp1, ztmp1, ztmp2);\n+      __ br(__ NE, MISMATCH);\n+      __ cmp(idx, rscratch1);\n+      __ br(__ LT, LOOP);\n+\n+    \/\/ post loop, last iteration\n+    __ sve_whilelt(pgtmp1, mode == LL ? __ B : __ H, idx, cnt);\n+\n+    LOAD_PAIR(ztmp1, ztmp2, pgtmp1, src1, src2, idx);\n+    __ sve_cmp(Assembler::NE, pgtmp2, mode == LL ? __ B : __ H, pgtmp1, ztmp1, ztmp2);\n+    __ br(__ EQ, DONE);\n+\n+    __ bind(MISMATCH);\n+\n+    \/\/ Crop the vector to find its location.\n+    __ sve_brkb(pgtmp2, pgtmp1, pgtmp2, false \/* isMerge *\/);\n+    \/\/ Extract the first different characters of each string.\n+    __ sve_lasta(rscratch1, mode == LL ? __ B : __ H, pgtmp2, ztmp1);\n+    __ sve_lasta(rscratch2, mode == LL ? __ B : __ H, pgtmp2, ztmp2);\n+\n+    \/\/ Compute the difference of the first different characters.\n+    __ sub(result, rscratch1, rscratch2);\n+\n+    __ bind(DONE);\n+    __ ret(lr);\n+#undef LOAD_PAIR\n+    return entry;\n+  }\n+\n@@ -5371,0 +5466,1 @@\n+    if (UseSVE == 0) {\n@@ -5379,0 +5475,10 @@\n+    } else {\n+      StubRoutines::aarch64::_compare_long_string_LL\n+          = generate_compare_long_string_sve(LL);\n+      StubRoutines::aarch64::_compare_long_string_UU\n+          = generate_compare_long_string_sve(UU);\n+      StubRoutines::aarch64::_compare_long_string_LU\n+          = generate_compare_long_string_sve(LU);\n+      StubRoutines::aarch64::_compare_long_string_UL\n+          = generate_compare_long_string_sve(UL);\n+    }\n@@ -5764,1 +5870,1 @@\n-    \/\/ that) and keep the data in little-endian bit order throught the\n+    \/\/ that) and keep the data in little-endian bit order through the\n@@ -5952,1 +6058,1 @@\n-    Register isURL = c_rarg5;  \/\/ Base64 or URL chracter set\n+    Register isURL = c_rarg5;  \/\/ Base64 or URL character set\n@@ -6038,1 +6144,1 @@\n-    \/\/ we need unsigned saturating substract, to make sure all input values\n+    \/\/ we need unsigned saturating subtract, to make sure all input values\n@@ -6142,1 +6248,1 @@\n-    \/\/ except the trailing character '=' is also treated illegal value in this instrinsic. That\n+    \/\/ except the trailing character '=' is also treated illegal value in this intrinsic. That\n@@ -6337,1 +6443,1 @@\n-#ifdef LINUX\n+#if defined (LINUX) && !defined (__ARM_FEATURE_ATOMICS)\n@@ -6535,0 +6641,248 @@\n+  RuntimeStub* generate_cont_doYield() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    const char *name = \"cont_doYield\";\n+\n+    enum layout {\n+      rfp_off1,\n+      rfp_off2,\n+      lr_off,\n+      lr_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+    \/\/ assert(is_even(framesize\/2), \"sp not 16-byte aligned\");\n+\n+    int insts_size = 512;\n+    int locs_size  = 64;\n+    CodeBuffer code(name, insts_size, locs_size);\n+    OopMapSet* oop_maps  = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+    MacroAssembler* _masm = masm;\n+\n+    address start = __ pc();\n+\n+    __ enter();\n+\n+    __ mov(c_rarg1, sp);\n+\n+    int frame_complete = __ pc() - start;\n+    address the_pc = __ pc();\n+\n+    __ post_call_nop(); \/\/ this must be exactly after the pc value that is pushed into the frame info, we use this nop for fast CodeBlob lookup\n+\n+    __ mov(c_rarg0, rthread);\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n+    __ reset_last_Java_frame(true);\n+\n+    Label pinned;\n+\n+    __ cbnz(r0, pinned);\n+\n+    \/\/ We've succeeded, set sp to the ContinuationEntry\n+    __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+    __ mov(sp, rscratch1);\n+    continuation_enter_cleanup(masm);\n+\n+    __ bind(pinned); \/\/ pinned -- return to caller\n+\n+    __ leave();\n+    __ ret(lr);\n+\n+    OopMap* map = new OopMap(framesize, 1);\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+    RuntimeStub::new_runtime_stub(name,\n+                                  &code,\n+                                  frame_complete,\n+                                  (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                  oop_maps, false);\n+    return stub;\n+  }\n+\n+  address generate_cont_thaw(Continuation::thaw_kind kind) {\n+    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n+\n+    address start = __ pc();\n+\n+    if (return_barrier) {\n+      __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+      __ mov(sp, rscratch1);\n+    }\n+    assert_asm(_masm, (__ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset())), __ cmp(sp, rscratch1)), Assembler::EQ, \"incorrect sp\");\n+\n+    if (return_barrier) {\n+      \/\/ preserve possible return value from a method returning to the return barrier\n+      __ fmovd(rscratch1, v0);\n+      __ stp(rscratch1, r0, Address(__ pre(sp, -2 * wordSize)));\n+    }\n+\n+    __ movw(c_rarg1, (return_barrier ? 1 : 0));\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), rthread, c_rarg1);\n+    __ mov(rscratch2, r0); \/\/ r0 contains the size of the frames to thaw, 0 if overflow or no more frames\n+\n+    if (return_barrier) {\n+      \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n+      __ ldp(rscratch1, r0, Address(__ post(sp, 2 * wordSize)));\n+      __ fmovd(v0, rscratch1);\n+    }\n+    assert_asm(_masm, (__ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset())), __ cmp(sp, rscratch1)), Assembler::EQ, \"incorrect sp\");\n+\n+\n+    Label thaw_success;\n+    \/\/ rscratch2 contains the size of the frames to thaw, 0 if overflow or no more frames\n+    __ cbnz(rscratch2, thaw_success);\n+    __ lea(rscratch1, ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+    __ br(rscratch1);\n+    __ bind(thaw_success);\n+\n+    \/\/ make room for the thawed frames\n+    __ sub(rscratch1, sp, rscratch2);\n+    __ andr(rscratch1, rscratch1, -16); \/\/ align\n+    __ mov(sp, rscratch1);\n+\n+    if (return_barrier) {\n+      \/\/ save original return value -- again\n+      __ fmovd(rscratch1, v0);\n+      __ stp(rscratch1, r0, Address(__ pre(sp, -2 * wordSize)));\n+    }\n+\n+    \/\/ If we want, we can templatize thaw by kind, and have three different entries\n+    __ movw(c_rarg1, (uint32_t)kind);\n+\n+    __ call_VM_leaf(Continuation::thaw_entry(), rthread, c_rarg1);\n+    __ mov(rscratch2, r0); \/\/ r0 is the sp of the yielding frame\n+\n+    if (return_barrier) {\n+      \/\/ restore return value (no safepoint in the call to thaw, so even an oop return value should be OK)\n+      __ ldp(rscratch1, r0, Address(__ post(sp, 2 * wordSize)));\n+      __ fmovd(v0, rscratch1);\n+    } else {\n+      __ mov(r0, zr); \/\/ return 0 (success) from doYield\n+    }\n+\n+    \/\/ we're now on the yield frame (which is in an address above us b\/c rsp has been pushed down)\n+    __ sub(sp, rscratch2, 2*wordSize); \/\/ now pointing to rfp spill\n+    __ mov(rfp, sp);\n+\n+    if (return_barrier_exception) {\n+      __ ldr(c_rarg1, Address(rfp, wordSize)); \/\/ return address\n+      __ verify_oop(r0);\n+      __ mov(r19, r0); \/\/ save return value contaning the exception oop in callee-saved R19\n+\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), rthread, c_rarg1);\n+\n+      \/\/ Reinitialize the ptrue predicate register, in case the external runtime call clobbers ptrue reg, as we may return to SVE compiled code.\n+      \/\/ __ reinitialize_ptrue();\n+\n+      \/\/ see OptoRuntime::generate_exception_blob: r0 -- exception oop, r3 -- exception pc\n+\n+      __ mov(r1, r0); \/\/ the exception handler\n+      __ mov(r0, r19); \/\/ restore return value contaning the exception oop\n+      __ verify_oop(r0);\n+\n+      __ leave();\n+      __ mov(r3, lr);\n+      __ br(r1); \/\/ the exception handler\n+    } else {\n+      \/\/ We're \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n+      __ leave();\n+      __ ret(lr);\n+    }\n+\n+    return start;\n+  }\n+\n+  address generate_cont_thaw() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"Cont thaw\");\n+    address start = __ pc();\n+    generate_cont_thaw(Continuation::thaw_top);\n+    return start;\n+  }\n+\n+  address generate_cont_returnBarrier() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    \/\/ TODO: will probably need multiple return barriers depending on return type\n+    StubCodeMark mark(this, \"StubRoutines\", \"cont return barrier\");\n+    address start = __ pc();\n+\n+    generate_cont_thaw(Continuation::thaw_return_barrier);\n+\n+    return start;\n+  }\n+\n+  address generate_cont_returnBarrier_exception() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    StubCodeMark mark(this, \"StubRoutines\", \"cont return barrier exception handler\");\n+    address start = __ pc();\n+\n+    generate_cont_thaw(Continuation::thaw_return_barrier_exception);\n+\n+    return start;\n+  }\n+\n+#if INCLUDE_JFR\n+\n+  static void jfr_prologue(address the_pc, MacroAssembler* _masm, Register thread) {\n+    __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n+    __ mov(c_rarg0, thread);\n+  }\n+\n+  \/\/ The handle is dereferenced through a load barrier.\n+  static void jfr_epilogue(MacroAssembler* _masm, Register thread) {\n+    __ reset_last_Java_frame(true);\n+    Label null_jobject;\n+    __ cbz(r0, null_jobject);\n+    DecoratorSet decorators = ACCESS_READ | IN_NATIVE;\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->load_at(_masm, decorators, T_OBJECT, r0, Address(r0, 0), c_rarg0, thread);\n+    __ bind(null_jobject);\n+  }\n+\n+  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+  \/\/ It returns a jobject handle to the event writer.\n+  \/\/ The handle is dereferenced and the return value is the event writer oop.\n+  static RuntimeStub* generate_jfr_write_checkpoint() {\n+    enum layout {\n+      rbp_off,\n+      rbpH_off,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    int insts_size = 512;\n+    int locs_size = 64;\n+    CodeBuffer code(\"jfr_write_checkpoint\", insts_size, locs_size);\n+    OopMapSet* oop_maps = new OopMapSet();\n+    MacroAssembler* masm = new MacroAssembler(&code);\n+    MacroAssembler* _masm = masm;\n+\n+    address start = __ pc();\n+    __ enter();\n+    int frame_complete = __ pc() - start;\n+    address the_pc = __ pc();\n+    jfr_prologue(the_pc, _masm, rthread);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+    jfr_epilogue(_masm, rthread);\n+    __ leave();\n+    __ ret(lr);\n+\n+    OopMap* map = new OopMap(framesize, 1); \/\/ rfp\n+    oop_maps->add_gc_map(the_pc - start, map);\n+\n+    RuntimeStub* stub = \/\/ codeBlob framesize is in words (not VMRegImpl::slot_size)\n+      RuntimeStub::new_runtime_stub(\"jfr_write_checkpoint\", &code, frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps, false);\n+    return stub;\n+  }\n+\n+#endif \/\/ INCLUDE_JFR\n+\n@@ -6590,1 +6944,1 @@\n-    __ sub(sp, rfp, ((unsigned)framesize-4) << LogBytesPerInt); \/\/ prolog\n+    __ sub(sp, rfp, ((uint64_t)framesize-4) << LogBytesPerInt); \/\/ prolog\n@@ -6634,1 +6988,0 @@\n-\n@@ -7652,0 +8005,1 @@\n+  }\n@@ -7653,7 +8007,11 @@\n-    \/\/ Safefetch stubs.\n-    generate_safefetch(\"SafeFetch32\", sizeof(int),     &StubRoutines::_safefetch32_entry,\n-                                                       &StubRoutines::_safefetch32_fault_pc,\n-                                                       &StubRoutines::_safefetch32_continuation_pc);\n-    generate_safefetch(\"SafeFetchN\", sizeof(intptr_t), &StubRoutines::_safefetchN_entry,\n-                                                       &StubRoutines::_safefetchN_fault_pc,\n-                                                       &StubRoutines::_safefetchN_continuation_pc);\n+  void generate_phase1() {\n+    \/\/ Continuation stubs:\n+    StubRoutines::_cont_thaw          = generate_cont_thaw();\n+    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n+    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n+    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n+                                        : StubRoutines::_cont_doYield_stub->entry_point();\n+\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n@@ -7664,1 +8022,3 @@\n-    StubRoutines::_verify_oop_subroutine_entry     = generate_verify_oop();\n+    if (VerifyOops) {\n+      StubRoutines::_verify_oop_subroutine_entry   = generate_verify_oop();\n+    }\n@@ -7792,1 +8152,1 @@\n-#ifdef LINUX\n+#if defined (LINUX) && !defined (__ARM_FEATURE_ATOMICS)\n@@ -7802,4 +8162,2 @@\n-  StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {\n-    if (all) {\n-      generate_all();\n-    } else {\n+  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n+    if (phase == 0) {\n@@ -7807,0 +8165,4 @@\n+    } else if (phase == 1) {\n+      generate_phase1(); \/\/ stubs that must be available for the interpreter\n+    } else {\n+      generate_all();\n@@ -7812,1 +8174,1 @@\n-void StubGenerator_generate(CodeBuffer* code, bool all) {\n+void StubGenerator_generate(CodeBuffer* code, int phase) {\n@@ -7816,1 +8178,1 @@\n-  StubGenerator g(code, all);\n+  StubGenerator g(code, phase);\n@@ -7820,1 +8182,1 @@\n-#ifdef LINUX\n+#if defined (LINUX)\n@@ -7851,0 +8213,72 @@\n+\n+\n+#undef __\n+#define __ masm->\n+\n+\/\/ on exit, sp points to the ContinuationEntry\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots) {\n+  assert(ContinuationEntry::size() % VMRegImpl::stack_slot_size == 0, \"\");\n+  assert(in_bytes(ContinuationEntry::cont_offset())  % VMRegImpl::stack_slot_size == 0, \"\");\n+  assert(in_bytes(ContinuationEntry::chunk_offset()) % VMRegImpl::stack_slot_size == 0, \"\");\n+\n+  stack_slots += (int)ContinuationEntry::size()\/wordSize;\n+  __ sub(sp, sp, (int)ContinuationEntry::size()); \/\/ place Continuation metadata\n+\n+  OopMap* map = new OopMap(((int)ContinuationEntry::size() + wordSize)\/ VMRegImpl::stack_slot_size, 0 \/* arg_slots*\/);\n+  ContinuationEntry::setup_oopmap(map);\n+\n+  __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+  __ str(rscratch1, Address(sp, ContinuationEntry::parent_offset()));\n+  __ mov(rscratch1, sp); \/\/ we can't use sp as the source in str\n+  __ str(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+\n+  return map;\n+}\n+\n+\/\/ on entry c_rarg1 points to the continuation\n+\/\/          sp points to ContinuationEntry\n+\/\/          c_rarg3 -- isVirtualThread\n+void fill_continuation_entry(MacroAssembler* masm) {\n+#ifdef ASSERT\n+  __ movw(rscratch1, ContinuationEntry::cookie_value());\n+  __ strw(rscratch1, Address(sp, ContinuationEntry::cookie_offset()));\n+#endif\n+\n+  __ str (c_rarg1, Address(sp, ContinuationEntry::cont_offset()));\n+  __ strw(c_rarg3, Address(sp, ContinuationEntry::flags_offset()));\n+  __ str (zr,      Address(sp, ContinuationEntry::chunk_offset()));\n+  __ strw(zr,      Address(sp, ContinuationEntry::argsize_offset()));\n+  __ strw(zr,      Address(sp, ContinuationEntry::pin_count_offset()));\n+\n+  __ ldr(rscratch1, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  __ str(rscratch1, Address(sp, ContinuationEntry::parent_cont_fastpath_offset()));\n+  __ ldr(rscratch1, Address(rthread, JavaThread::held_monitor_count_offset()));\n+  __ str(rscratch1, Address(sp, ContinuationEntry::parent_held_monitor_count_offset()));\n+\n+  __ str(zr, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  __ str(zr, Address(rthread, JavaThread::held_monitor_count_offset()));\n+}\n+\n+\/\/ on entry, sp points to the ContinuationEntry\n+\/\/ on exit, rfp points to the spilled rfp in the entry frame\n+void continuation_enter_cleanup(MacroAssembler* masm) {\n+#ifndef PRODUCT\n+  Label OK;\n+  __ ldr(rscratch1, Address(rthread, JavaThread::cont_entry_offset()));\n+  __ cmp(sp, rscratch1);\n+  __ br(Assembler::EQ, OK);\n+  __ stop(\"incorrect sp1\");\n+  __ bind(OK);\n+#endif\n+\n+  __ ldr(rscratch1, Address(sp, ContinuationEntry::parent_cont_fastpath_offset()));\n+  __ str(rscratch1, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  __ ldr(rscratch1, Address(sp, ContinuationEntry::parent_held_monitor_count_offset()));\n+  __ str(rscratch1, Address(rthread, JavaThread::held_monitor_count_offset()));\n+\n+  __ ldr(rscratch2, Address(sp, ContinuationEntry::parent_offset()));\n+  __ str(rscratch2, Address(rthread, JavaThread::cont_entry_offset()));\n+  __ add(rfp, sp, (int)ContinuationEntry::size());\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":508,"deletions":74,"binary":false,"changes":582,"status":"modified"},{"patch":"@@ -150,1 +150,1 @@\n-  \/\/ r13: sender sp\n+  \/\/ r19_sender_sp: sender sp\n@@ -178,1 +178,1 @@\n-    __ mov(sp, r13); \/\/ Restore caller's SP\n+    __ mov(sp, r19_sender_sp); \/\/ Restore caller's SP\n@@ -184,1 +184,1 @@\n-    __ mov(sp, r13);\n+    __ mov(sp, r19_sender_sp);\n@@ -194,3 +194,3 @@\n-    __ mov(sp, r13);\n-    __ mov(r19, lr);\n-    continuation = r19;  \/\/ The first callee-saved register\n+    __ mov(sp, r19_sender_sp);\n+    __ mov(r23, lr);\n+    continuation = r23;  \/\/ The first free callee-saved register\n@@ -201,2 +201,2 @@\n-    __ mov(r19, lr);\n-    continuation = r19;\n+    __ mov(r23, lr);\n+    continuation = r23;\n@@ -205,1 +205,1 @@\n-    __ mov(sp, r13);\n+    __ mov(sp, r19_sender_sp);\n@@ -215,1 +215,1 @@\n-      __ mov(sp, r13); \/\/ Restore caller's SP\n+      __ mov(sp, r19_sender_sp); \/\/ Restore caller's SP\n@@ -225,1 +225,1 @@\n-      __ mov(sp, r13); \/\/ Restore caller's SP\n+      __ mov(sp, r19_sender_sp); \/\/ Restore caller's SP\n@@ -311,1 +311,1 @@\n-  \/\/ r13: sender SP\n+  \/\/ r19_sender_sp: sender SP\n@@ -467,7 +467,1 @@\n-  __ ldr(rscratch1, Address(rmethod, Method::const_offset()));\n-  __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));\n-  __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 2);\n-  __ ldr(rscratch2,\n-         Address(rfp, frame::interpreter_frame_initial_sp_offset * wordSize));\n-  __ sub(rscratch1, rscratch2, rscratch1, ext::uxtw, 3);\n-  __ andr(sp, rscratch1, -16);\n+  __ restore_sp_after_call();\n@@ -475,2 +469,2 @@\n- __ check_and_handle_popframe(rthread);\n- __ check_and_handle_earlyret(rthread);\n+  __ check_and_handle_popframe(rthread);\n+  __ check_and_handle_earlyret(rthread);\n@@ -494,8 +488,1 @@\n-  \/\/ Calculate stack limit\n-  __ ldr(rscratch1, Address(rmethod, Method::const_offset()));\n-  __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));\n-  __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 2);\n-  __ ldr(rscratch2,\n-         Address(rfp, frame::interpreter_frame_initial_sp_offset * wordSize));\n-  __ sub(rscratch1, rscratch2, rscratch1, ext::uxtx, 3);\n-  __ andr(sp, rscratch1, -16);\n+  __ restore_sp_after_call();  \/\/ Restore SP to extended SP\n@@ -583,0 +570,1 @@\n+  __ push_cont_fastpath(rthread);\n@@ -584,0 +572,1 @@\n+  __ pop_cont_fastpath(rthread);\n@@ -723,1 +712,1 @@\n-  \/\/ unnecessary because the sender SP in r13 is always aligned, but\n+  \/\/ unnecessary because the sender SP in r19 is always aligned, but\n@@ -725,1 +714,1 @@\n-  __ andr(sp, r13, -16);\n+  __ andr(sp, r19_sender_sp, -16);\n@@ -788,0 +777,1 @@\n+  __ check_extended_sp();\n@@ -790,2 +780,3 @@\n-  __ mov(rscratch1, esp);\n-  __ str(rscratch1, monitor_block_top);  \/\/ set new monitor block top\n+  __ mov(rscratch1, sp);\n+  __ str(rscratch1, Address(rfp, frame::interpreter_frame_extended_sp_offset * wordSize));\n+  __ str(esp, monitor_block_top);  \/\/ set new monitor block top\n@@ -817,1 +808,1 @@\n-    __ ldr(rscratch1, Address(rmethod, Method::const_offset()));      \/\/ get ConstMethod\n+    __ ldr(rscratch1, Address(rmethod, Method::const_offset()));    \/\/ get ConstMethod\n@@ -830,1 +821,1 @@\n-    __ stp(zr, rmethod, Address(sp, 6 * wordSize));        \/\/ save Method* (no mdp)\n+    __ stp(zr, rmethod, Address(sp, 6 * wordSize));         \/\/ save Method* (no mdp)\n@@ -833,4 +824,0 @@\n-  \/\/ Get mirror and store it in the frame as GC root for this Method*\n-  __ load_mirror(r10, rmethod);\n-  __ stp(r10, zr, Address(sp, 4 * wordSize));\n-\n@@ -848,1 +835,1 @@\n-  __ stp(zr, r13, Address(sp, 8 * wordSize));\n+  __ stp(zr, r19_sender_sp, Address(sp, 8 * wordSize));\n@@ -850,1 +837,2 @@\n-  \/\/ Move SP out of the way\n+  \/\/ Get mirror\n+  __ load_mirror(r10, rmethod);\n@@ -854,1 +842,1 @@\n-    __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 2);\n+    __ add(rscratch1, rscratch1, MAX2(3, Method::extra_stack_entries()));\n@@ -856,1 +844,8 @@\n-    __ andr(sp, rscratch1, -16);\n+    __ andr(rscratch1, rscratch1, -16);\n+    \/\/ Store extended SP and mirror\n+    __ stp(r10, rscratch1, Address(sp, 4 * wordSize));\n+    \/\/ Move SP out of the way\n+    __ mov(sp, rscratch1);\n+    } else {\n+    __ mov(rscratch1, sp);\n+    __ stp(zr, rscratch1, Address(sp, 4 * wordSize));\n@@ -862,0 +857,12 @@\n+address TemplateInterpreterGenerator::generate_Continuation_doYield_entry(void) {\n+  if (!Continuations::enabled()) return nullptr;\n+\n+  address entry = __ pc();\n+  assert(StubRoutines::cont_doYield() != NULL, \"stub not yet generated\");\n+\n+  __ push_cont_fastpath(rthread);\n+  __ far_jump(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::cont_doYield())));\n+\n+  return entry;\n+}\n+\n@@ -895,1 +902,1 @@\n-  \/\/ r13: senderSP must preserve for slow path, set SP to it on fast path\n+  \/\/ r19_sender_sp: senderSP must preserve for slow path, set SP to it on fast path\n@@ -910,2 +917,0 @@\n-  __ mov(r19, r13);   \/\/ Move senderSP to a callee-saved register\n-\n@@ -918,1 +923,1 @@\n-  __ andr(sp, r19, -16);  \/\/ done with stack\n+  __ andr(sp, r19_sender_sp, -16);  \/\/ done with stack\n@@ -937,1 +942,1 @@\n-    \/\/ r13: senderSP must preserved for slow path\n+    \/\/ r19_sender_sp: senderSP must preserved for slow path\n@@ -966,1 +971,1 @@\n-    __ andr(sp, r13, -16);\n+    __ andr(sp, r19_sender_sp, -16);\n@@ -987,1 +992,1 @@\n-    \/\/ r13: senderSP must preserved for slow path\n+    \/\/ r19_sender_sp: senderSP must preserved for slow path\n@@ -1019,1 +1024,1 @@\n-    __ andr(sp, r13, -16); \/\/ Restore the caller's SP\n+    __ andr(sp, r19_sender_sp, -16); \/\/ Restore the caller's SP\n@@ -1063,1 +1068,1 @@\n-    __ andr(sp, r13, -16); \/\/ Restore the caller's SP\n+    __ andr(sp, r19_sender_sp, -16); \/\/ Restore the caller's SP\n@@ -1074,5 +1079,3 @@\n-  \/\/ Bang each page in the shadow zone. We can't assume it's been done for\n-  \/\/ an interpreter frame with greater than a page of locals, so each page\n-  \/\/ needs to be checked.  Only true for non-native.\n-  const int n_shadow_pages = (int)(StackOverflow::stack_shadow_zone_size() \/ os::vm_page_size());\n-  const int start_page = native_call ? n_shadow_pages : 1;\n+  \/\/ See more discussion in stackOverflow.hpp.\n+\n+  const int shadow_zone_size = checked_cast<int>(StackOverflow::stack_shadow_zone_size());\n@@ -1080,2 +1083,24 @@\n-  for (int pages = start_page; pages <= n_shadow_pages ; pages++) {\n-    __ sub(rscratch2, sp, pages*page_size);\n+  const int n_shadow_pages = shadow_zone_size \/ page_size;\n+\n+#ifdef ASSERT\n+  Label L_good_limit;\n+  __ ldr(rscratch1, Address(rthread, JavaThread::shadow_zone_safe_limit()));\n+  __ cbnz(rscratch1, L_good_limit);\n+  __ stop(\"shadow zone safe limit is not initialized\");\n+  __ bind(L_good_limit);\n+\n+  Label L_good_watermark;\n+  __ ldr(rscratch1, Address(rthread, JavaThread::shadow_zone_growth_watermark()));\n+  __ cbnz(rscratch1, L_good_watermark);\n+  __ stop(\"shadow zone growth watermark is not initialized\");\n+  __ bind(L_good_watermark);\n+#endif\n+\n+  Label L_done;\n+\n+  __ ldr(rscratch1, Address(rthread, JavaThread::shadow_zone_growth_watermark()));\n+  __ cmp(sp, rscratch1);\n+  __ br(Assembler::HI, L_done);\n+\n+  for (int p = 1; p <= n_shadow_pages; p++) {\n+    __ sub(rscratch2, sp, p*page_size);\n@@ -1084,1 +1109,10 @@\n-}\n+  \/\/ Record the new watermark, but only if the update is above the safe limit.\n+  \/\/ Otherwise, the next time around the check above would pass the safe limit.\n+  __ ldr(rscratch1, Address(rthread, JavaThread::shadow_zone_safe_limit()));\n+  __ cmp(sp, rscratch1);\n+  __ br(Assembler::LS, L_done);\n+  __ mov(rscratch1, sp);\n+  __ str(rscratch1, Address(rthread, JavaThread::shadow_zone_growth_watermark()));\n+\n+  __ bind(L_done);\n+}\n@@ -1200,1 +1234,1 @@\n-    __ stop(\"broken stack frame setup in interpreter\");\n+    __ stop(\"broken stack frame setup in interpreter 1\");\n@@ -1497,1 +1531,1 @@\n-  \/\/ resture sender sp\n+  \/\/ restore sender sp\n@@ -1650,1 +1684,1 @@\n-    __ stop(\"broken stack frame setup in interpreter\");\n+    __ stop(\"broken stack frame setup in interpreter 2\");\n@@ -1708,8 +1742,2 @@\n-  \/\/ Calculate stack limit\n-  __ ldr(rscratch1, Address(rmethod, Method::const_offset()));\n-  __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));\n-  __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 4);\n-  __ ldr(rscratch2,\n-         Address(rfp, frame::interpreter_frame_initial_sp_offset * wordSize));\n-  __ sub(rscratch1, rscratch2, rscratch1, ext::uxtx, 3);\n-  __ andr(sp, rscratch1, -16);\n+  \/\/ Restore machine SP\n+  __ restore_sp_after_call();\n@@ -1844,7 +1872,1 @@\n-  __ ldr(rscratch1, Address(rmethod, Method::const_offset()));\n-  __ ldrh(rscratch1, Address(rscratch1, ConstMethod::max_stack_offset()));\n-  __ add(rscratch1, rscratch1, frame::interpreter_frame_monitor_size() + 4);\n-  __ ldr(rscratch2,\n-         Address(rfp, frame::interpreter_frame_initial_sp_offset * wordSize));\n-  __ sub(rscratch1, rscratch2, rscratch1, ext::uxtw, 3);\n-  __ andr(sp, rscratch1, -16);\n+  __ restore_sp_after_call();\n","filename":"src\/hotspot\/cpu\/aarch64\/templateInterpreterGenerator_aarch64.cpp","additions":100,"deletions":78,"binary":false,"changes":178,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -140,1 +140,1 @@\n-\/\/ Miscelaneous helper routines\n+\/\/ Miscellaneous helper routines\n@@ -2360,1 +2360,1 @@\n-\/\/     writes act as aquire & release, so:\n+\/\/     writes act as acquire & release, so:\n@@ -3765,1 +3765,0 @@\n-  Label initialize_object; \/\/ including clearing the fields\n@@ -3782,1 +3781,1 @@\n-  __ cmp(rscratch1, (u1)InstanceKlass::_kind_inline_type);\n+  __ cmp(rscratch1, (u1)InlineKlassKind);\n@@ -4046,1 +4045,1 @@\n-\/\/ [saved rbp    ] <--- rbp\n+\/\/ [saved rfp    ] <--- rfp\n@@ -4105,0 +4104,6 @@\n+\n+    __ check_extended_sp();\n+    __ sub(sp, sp, entry_size);           \/\/ make room for the monitor\n+    __ mov(rscratch1, sp);\n+    __ str(rscratch1, Address(rfp, frame::interpreter_frame_extended_sp_offset * wordSize));\n+\n@@ -4111,2 +4116,0 @@\n-    __ sub(sp, sp, entry_size);           \/\/ make room for the monitor\n-\n@@ -4132,1 +4135,1 @@\n-  \/\/ The object has already been poped from the stack, so the\n+  \/\/ The object has already been popped from the stack, so the\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":12,"deletions":9,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -202,3 +202,0 @@\n-  \/\/ TODO: ARM\n-  __ nop(); \/\/ See comments in other ports\n-\n@@ -245,1 +242,1 @@\n-  \/\/ Preform needed unlocking\n+  \/\/ Perform needed unlocking\n@@ -644,1 +641,1 @@\n-      case T_FLOAT:    \/\/ used in floatToRawIntBits intrinsic implemenation\n+      case T_FLOAT:    \/\/ used in floatToRawIntBits intrinsic implementation\n@@ -1415,1 +1412,4 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on arm\");\n+\n@@ -1481,0 +1481,3 @@\n+        case T_METADATA:\n+          __ mov_metadata(result->as_register(), c->as_metadata(), acond);\n+          break;\n@@ -2432,0 +2435,4 @@\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj);\n+    }\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":14,"deletions":7,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,0 +45,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -169,7 +170,0 @@\n-  \/\/ If the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri).\n-  __ nop();\n-\n@@ -249,7 +243,0 @@\n-  \/\/ If the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri).\n-  __ nop();\n-\n@@ -1550,0 +1537,3 @@\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on ppc\");\n@@ -1551,1 +1541,0 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n@@ -2711,0 +2700,4 @@\n+      if (op->info() != NULL) {\n+        add_debug_info_for_null_check_here(op->info());\n+        __ null_check(obj);\n+      }\n@@ -3097,1 +3090,1 @@\n-    __ stop(\"unexpect null obj\");\n+    __ stop(\"unexpected null obj\");\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":10,"deletions":17,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+define_pd_global(bool,  VMContinuations, false);\n+\n","filename":"src\/hotspot\/cpu\/ppc\/globals_ppc.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,0 +40,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -1840,1 +1841,1 @@\n-      \/\/ begining of the ProfileData we intend to update to check its\n+      \/\/ beginning of the ProfileData we intend to update to check its\n","filename":"src\/hotspot\/cpu\/ppc\/interp_masm_ppc_64.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,0 +43,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -167,7 +168,0 @@\n-  \/\/ If the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci. => Add a nop.\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -221,1 +215,1 @@\n-  \/\/ Preform needed unlocking.\n+  \/\/ Perform needed unlocking.\n@@ -265,7 +259,0 @@\n-  \/\/ If the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci. => Add a nop.\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -1443,1 +1430,4 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on s390\");\n+\n@@ -2734,0 +2724,4 @@\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj);\n+    }\n@@ -2991,1 +2985,1 @@\n-    __ asm_assert_ne(\"unexpect null obj\", __LINE__);\n+    __ asm_assert_ne(\"unexpected null obj\", __LINE__);\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":12,"deletions":18,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -44,1 +45,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n@@ -752,1 +753,1 @@\n-\/\/ Unlock any Java monitors from syncronized blocks.\n+\/\/ Unlock any Java monitors from synchronized blocks.\n@@ -912,1 +913,1 @@\n-\/\/ Unlock any Java monitors from syncronized blocks.\n+\/\/ Unlock any Java monitors from synchronized blocks.\n","filename":"src\/hotspot\/cpu\/s390\/interp_masm_s390.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,1 +47,1 @@\n-  \/\/ on the transistion) Since the callee parameters already account\n+  \/\/ on the transition) Since the callee parameters already account\n","filename":"src\/hotspot\/cpu\/x86\/abstractInterpreter_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -403,7 +403,0 @@\n-  \/\/ if the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -450,1 +443,1 @@\n-  NOT_LP64(__ get_thread(rsi));\n+  NOT_LP64(__ get_thread(thread));\n@@ -461,1 +454,1 @@\n-  \/\/ Preform needed unlocking\n+  \/\/ Perform needed unlocking\n@@ -504,7 +497,0 @@\n-  \/\/ if the last instruction is a call (typically to do a throw which\n-  \/\/ is coming at the end after block reordering) the return address\n-  \/\/ must still point into the code area in order to avoid assertion\n-  \/\/ failures when searching for the corresponding bci => add a nop\n-  \/\/ (was bug 5\/14\/1999 - gri)\n-  __ nop();\n-\n@@ -2181,1 +2167,4 @@\n-void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type) {\n+void LIR_Assembler::cmove(LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr result, BasicType type,\n+                          LIR_Opr cmp_opr1, LIR_Opr cmp_opr2) {\n+  assert(cmp_opr1 == LIR_OprFact::illegalOpr && cmp_opr2 == LIR_OprFact::illegalOpr, \"unnecessary cmp oprs on x86\");\n+\n@@ -2439,1 +2428,1 @@\n-        assert(const_addr != NULL, \"incorrect float\/double constant maintainance\");\n+        assert(const_addr != NULL, \"incorrect float\/double constant maintenance\");\n@@ -3046,0 +3035,1 @@\n+  __ post_call_nop();\n@@ -3054,0 +3044,1 @@\n+  __ post_call_nop();\n@@ -3714,0 +3705,4 @@\n+    if (op->info() != NULL) {\n+      add_debug_info_for_null_check_here(op->info());\n+      __ null_check(obj);\n+    }\n@@ -3862,1 +3857,1 @@\n-    __ stop(\"unexpect null obj\");\n+    __ stop(\"unexpected null obj\");\n@@ -4110,0 +4105,1 @@\n+  __ post_call_nop();\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":16,"deletions":20,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -139,1 +139,0 @@\n-    \/\/ there is no immediate move of word values in asembler_i486.?pp\n@@ -252,1 +251,1 @@\n-LIR_Opr LIRGenerator::load_immediate(int x, BasicType type) {\n+LIR_Opr LIRGenerator::load_immediate(jlong x, BasicType type) {\n@@ -257,1 +256,1 @@\n-    r = LIR_OprFact::intConst(x);\n+    r = LIR_OprFact::intConst(checked_cast<jint>(x));\n@@ -365,0 +364,11 @@\n+void LIRGenerator::do_continuation_doYield(Intrinsic* x) {\n+  BasicTypeList signature(0);\n+  CallingConvention* cc = frame_map()->java_calling_convention(&signature, true);\n+\n+  const LIR_Opr result_reg = result_register_for(x->type());\n+  address entry = StubRoutines::cont_doYield();\n+  LIR_Opr result = rlock_result(x);\n+  CodeEmitInfo* info = state_for(x, x->state());\n+  __ call_runtime(entry, LIR_OprFact::illegalOpr, result_reg, cc->args(), info);\n+  __ move(result_reg, result);\n+}\n@@ -391,1 +401,0 @@\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -103,0 +103,3 @@\n+\n+  inc_held_monitor_count();\n+\n@@ -106,1 +109,0 @@\n-\n@@ -134,0 +136,2 @@\n+\n+  dec_held_monitor_count();\n@@ -142,1 +146,1 @@\n-    eden_allocate(noreg, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n+    jmp(slow_case);\n@@ -353,1 +357,2 @@\n-  bs->nmethod_entry_barrier(this);\n+  \/\/ C1 code is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n@@ -410,1 +415,1 @@\n-  bs->nmethod_entry_barrier(this);\n+  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"c1\/c1_FrameMap.hpp\"\n@@ -372,6 +373,1 @@\n-  int xmm_bypass_limit = FrameMap::nof_xmm_regs;\n-#ifdef _LP64\n-  if (UseAVX < 3) {\n-    xmm_bypass_limit = xmm_bypass_limit \/ 2;\n-  }\n-#endif\n+  int xmm_bypass_limit = FrameMap::get_num_caller_save_xmms();\n@@ -490,1 +486,1 @@\n-      int xmm_bypass_limit = FrameMap::nof_xmm_regs;\n+      int xmm_bypass_limit = FrameMap::get_num_caller_save_xmms();\n@@ -492,5 +488,0 @@\n-#ifdef _LP64\n-      if (UseAVX < 3) {\n-        xmm_bypass_limit = xmm_bypass_limit \/ 2;\n-      }\n-#endif\n@@ -516,4 +507,1 @@\n-    int xmm_bypass_limit = FrameMap::nof_xmm_regs;\n-    if (UseAVX < 3) {\n-      xmm_bypass_limit = xmm_bypass_limit \/ 2;\n-    }\n+    int xmm_bypass_limit = FrameMap::get_num_caller_save_xmms();\n@@ -1048,55 +1036,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if ((id == fast_new_instance_id || id == fast_new_instance_init_check_id) && !UseTLAB\n-            && Universe::heap()->supports_inline_contig_alloc()) {\n-          Label slow_path;\n-          Register obj_size = rcx;\n-          Register t1       = rbx;\n-          Register t2       = rsi;\n-          assert_different_registers(klass, obj, obj_size, t1, t2);\n-\n-          __ push(rdi);\n-          __ push(rbx);\n-\n-          if (id == fast_new_instance_init_check_id) {\n-            \/\/ make sure the klass is initialized\n-            __ cmpb(Address(klass, InstanceKlass::init_state_offset()), InstanceKlass::fully_initialized);\n-            __ jcc(Assembler::notEqual, slow_path);\n-          }\n-\n-#ifdef ASSERT\n-          \/\/ assert object can be fast path allocated\n-          {\n-            Label ok, not_ok;\n-            __ movl(obj_size, Address(klass, Klass::layout_helper_offset()));\n-            __ cmpl(obj_size, 0);  \/\/ make sure it's an instance (LH > 0)\n-            __ jcc(Assembler::lessEqual, not_ok);\n-            __ testl(obj_size, Klass::_lh_instance_slow_path_bit);\n-            __ jcc(Assembler::zero, ok);\n-            __ bind(not_ok);\n-            __ stop(\"assert(can be fast path allocated)\");\n-            __ should_not_reach_here();\n-            __ bind(ok);\n-          }\n-#endif \/\/ ASSERT\n-\n-          const Register thread = NOT_LP64(rdi) LP64_ONLY(r15_thread);\n-          NOT_LP64(__ get_thread(thread));\n-\n-          \/\/ get the instance size (size is postive so movl is fine for 64bit)\n-          __ movl(obj_size, Address(klass, Klass::layout_helper_offset()));\n-\n-          __ eden_allocate(thread, obj, obj_size, 0, t1, slow_path);\n-\n-          __ initialize_object(obj, klass, obj_size, 0, t1, t2, \/* is_tlab_allocated *\/ false);\n-          __ verify_oop(obj);\n-          __ pop(rbx);\n-          __ pop(rdi);\n-          __ ret(0);\n-\n-          __ bind(slow_path);\n-          __ pop(rbx);\n-          __ pop(rdi);\n-        }\n-\n@@ -1192,41 +1125,0 @@\n-        \/\/ If TLAB is disabled, see if there is support for inlining contiguous\n-        \/\/ allocations.\n-        \/\/ Otherwise, just go to the slow path.\n-        if (!UseTLAB && Universe::heap()->supports_inline_contig_alloc()) {\n-          Register arr_size = rsi;\n-          Register t1       = rcx;  \/\/ must be rcx for use as shift count\n-          Register t2       = rdi;\n-          Label slow_path;\n-\n-          \/\/ get the allocation size: round_up(hdr + length << (layout_helper & 0x1F))\n-          \/\/ since size is positive movl does right thing on 64bit\n-          __ movl(t1, Address(klass, Klass::layout_helper_offset()));\n-          \/\/ since size is postive movl does right thing on 64bit\n-          __ movl(arr_size, length);\n-          assert(t1 == rcx, \"fixed register usage\");\n-          __ shlptr(arr_size \/* by t1=rcx, mod 32 *\/);\n-          __ shrptr(t1, Klass::_lh_header_size_shift);\n-          __ andptr(t1, Klass::_lh_header_size_mask);\n-          __ addptr(arr_size, t1);\n-          __ addptr(arr_size, MinObjAlignmentInBytesMask); \/\/ align up\n-          __ andptr(arr_size, ~MinObjAlignmentInBytesMask);\n-\n-          \/\/ Using t2 for non 64-bit.\n-          const Register thread = NOT_LP64(t2) LP64_ONLY(r15_thread);\n-          NOT_LP64(__ get_thread(thread));\n-          __ eden_allocate(thread, obj, arr_size, 0, t1, slow_path);  \/\/ preserves arr_size\n-\n-          __ initialize_header(obj, klass, length, t1, t2);\n-          __ movb(t1, Address(klass, in_bytes(Klass::layout_helper_offset()) + (Klass::_lh_header_size_shift \/ BitsPerByte)));\n-          assert(Klass::_lh_header_size_shift % BitsPerByte == 0, \"bytewise\");\n-          assert(Klass::_lh_header_size_mask <= 0xFF, \"bytewise\");\n-          __ andptr(t1, Klass::_lh_header_size_mask);\n-          __ subptr(arr_size, t1);  \/\/ body length\n-          __ addptr(t1, obj);       \/\/ body start\n-          __ initialize_body(t1, arr_size, 0, t2);\n-          __ verify_oop(obj);\n-          __ ret(0);\n-\n-          __ bind(slow_path);\n-        }\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_Runtime1_x86.cpp","additions":5,"deletions":113,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"opto\/output.hpp\"\n@@ -36,0 +39,112 @@\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#define STOP(error) stop(error)\n+#else\n+#define BLOCK_COMMENT(str) block_comment(str)\n+#define STOP(error) block_comment(error); stop(error)\n+#endif\n+\n+\/\/ C2 compiled method's prolog code.\n+void C2_MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n+  int framesize = C->output()->frame_size_in_bytes();\n+  int bangsize = C->output()->bang_size_in_bytes();\n+  bool fp_mode_24b = false;\n+  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n+\n+  \/\/ WARNING: Initial instruction MUST be 5 bytes or longer so that\n+  \/\/ NativeJump::patch_verified_entry will be able to patch out the entry\n+  \/\/ code safely. The push to verify stack depth is ok at 5 bytes,\n+  \/\/ the frame allocation can be either 3 or 6 bytes. So if we don't do\n+  \/\/ stack bang then we must use the 6 byte frame allocation even if\n+  \/\/ we have no frame. :-(\n+  assert(stack_bang_size >= framesize || stack_bang_size <= 0, \"stack bang size incorrect\");\n+\n+  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n+  \/\/ Remove word for return addr\n+  framesize -= wordSize;\n+  stack_bang_size -= wordSize;\n+\n+  \/\/ Calls to C2R adapters often do not accept exceptional returns.\n+  \/\/ We require that their callers must bang for them.  But be careful, because\n+  \/\/ some VM calls (such as call site linkage) can use several kilobytes of\n+  \/\/ stack.  But the stack safety zone should account for that.\n+  \/\/ See bugs 4446381, 4468289, 4497237.\n+  if (stack_bang_size > 0) {\n+    generate_stack_overflow_check(stack_bang_size);\n+\n+    \/\/ We always push rbp, so that on return to interpreter rbp, will be\n+    \/\/ restored correctly and we can correct the stack.\n+    push(rbp);\n+    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n+    if (PreserveFramePointer) {\n+      mov(rbp, rsp);\n+    }\n+    \/\/ Remove word for ebp\n+    framesize -= wordSize;\n+\n+    \/\/ Create frame\n+    if (framesize) {\n+      subptr(rsp, framesize);\n+    }\n+  } else {\n+    \/\/ Create frame (force generation of a 4 byte immediate value)\n+    subptr_imm32(rsp, framesize);\n+\n+    \/\/ Save RBP register now.\n+    framesize -= wordSize;\n+    movptr(Address(rsp, framesize), rbp);\n+    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n+    if (PreserveFramePointer) {\n+      movptr(rbp, rsp);\n+      if (framesize > 0) {\n+        addptr(rbp, framesize);\n+      }\n+    }\n+  }\n+\n+  if (C->needs_stack_repair()) {\n+    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n+    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n+    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n+  }\n+\n+  if (VerifyStackAtCalls) { \/\/ Majik cookie to verify stack depth\n+    framesize -= wordSize;\n+    movptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n+  }\n+\n+#ifndef _LP64\n+  \/\/ If method sets FPU control word do it now\n+  if (fp_mode_24b) {\n+    fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n+  }\n+  if (UseSSE >= 2 && VerifyFPU) {\n+    verify_FPU(0, \"FPU stack must be clean on entry\");\n+  }\n+#endif\n+\n+#ifdef ASSERT\n+  if (VerifyStackAtCalls) {\n+    Label L;\n+    push(rax);\n+    mov(rax, rsp);\n+    andptr(rax, StackAlignmentInBytes-1);\n+    cmpptr(rax, StackAlignmentInBytes-wordSize);\n+    pop(rax);\n+    jcc(Assembler::equal, L);\n+    STOP(\"Stack is not properly aligned!\");\n+    bind(L);\n+  }\n+#endif\n+}\n+\n+void C2_MacroAssembler::emit_entry_barrier_stub(C2EntryBarrierStub* stub) {\n+  bind(stub->slow_path());\n+  call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));\n+  jmp(stub->continuation(), false \/* maybe_short *\/);\n+}\n+\n+int C2_MacroAssembler::entry_barrier_stub_size() {\n+  return 10;\n+}\n+\n@@ -51,14 +166,0 @@\n-void C2_MacroAssembler::setvectmask(Register dst, Register src, KRegister mask) {\n-  guarantee(PostLoopMultiversioning, \"must be\");\n-  Assembler::movl(dst, 1);\n-  Assembler::shlxl(dst, dst, src);\n-  Assembler::decl(dst);\n-  Assembler::kmovdl(mask, dst);\n-  Assembler::movl(dst, src);\n-}\n-\n-void C2_MacroAssembler::restorevectmask(KRegister mask) {\n-  guarantee(PostLoopMultiversioning, \"must be\");\n-  Assembler::knotwl(mask, k0);\n-}\n-\n@@ -477,1 +578,1 @@\n-  Label IsInflated, DONE_LABEL;\n+  Label IsInflated, DONE_LABEL, NO_COUNT, COUNT;\n@@ -509,1 +610,1 @@\n-    jcc(Assembler::equal, DONE_LABEL);           \/\/ Success\n+    jcc(Assembler::equal, COUNT);           \/\/ Success\n@@ -565,1 +666,1 @@\n-  jccb  (Assembler::notZero, DONE_LABEL);\n+  jccb  (Assembler::notZero, NO_COUNT);\n@@ -588,1 +689,1 @@\n-  jcc(Assembler::equal, DONE_LABEL);           \/\/ CAS above succeeded; propagate ZF = 1 (success)\n+  jccb(Assembler::equal, COUNT);          \/\/ CAS above succeeded; propagate ZF = 1 (success)\n@@ -590,2 +691,2 @@\n-  cmpptr(r15_thread, rax);                     \/\/ Check if we are already the owner (recursive lock)\n-  jcc(Assembler::notEqual, DONE_LABEL);        \/\/ If not recursive, ZF = 0 at this point (fail)\n+  cmpptr(r15_thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n+  jccb(Assembler::notEqual, NO_COUNT);    \/\/ If not recursive, ZF = 0 at this point (fail)\n@@ -605,1 +706,18 @@\n-  \/\/ At DONE_LABEL the icc ZFlag is set as follows ...\n+  \/\/ ZFlag == 1 count in fast path\n+  \/\/ ZFlag == 0 count in slow path\n+  jccb(Assembler::notZero, NO_COUNT); \/\/ jump if ZFlag == 0\n+\n+  bind(COUNT);\n+  \/\/ Count monitors in fast path\n+#ifndef _LP64\n+  get_thread(tmpReg);\n+  incrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n+#else \/\/ _LP64\n+  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+\n+  xorl(tmpReg, tmpReg); \/\/ Set ZF == 1\n+\n+  bind(NO_COUNT);\n+\n+  \/\/ At NO_COUNT the icc ZFlag is set as follows ...\n@@ -647,1 +765,1 @@\n-  Label DONE_LABEL, Stacked, CheckSucc;\n+  Label DONE_LABEL, Stacked, CheckSucc, COUNT, NO_COUNT;\n@@ -665,1 +783,1 @@\n-    jcc   (Assembler::zero, DONE_LABEL);                              \/\/ 0 indicates recursive stack-lock\n+    jcc   (Assembler::zero, COUNT);                                   \/\/ 0 indicates recursive stack-lock\n@@ -667,1 +785,1 @@\n-  movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Examine the object's markword\n+  movptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes()));   \/\/ Examine the object's markword\n@@ -670,1 +788,1 @@\n-    jccb  (Assembler::zero, Stacked);\n+    jccb   (Assembler::zero, Stacked);\n@@ -792,1 +910,1 @@\n-  \/\/ coherence traffic on the lock *and* artifically extended the critical section\n+  \/\/ coherence traffic on the lock *and* artificially extended the critical section\n@@ -821,0 +939,17 @@\n+\n+  \/\/ ZFlag == 1 count in fast path\n+  \/\/ ZFlag == 0 count in slow path\n+  jccb(Assembler::notZero, NO_COUNT);\n+\n+  bind(COUNT);\n+  \/\/ Count monitors in fast path\n+#ifndef _LP64\n+  get_thread(tmpReg);\n+  decrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n+#else \/\/ _LP64\n+  decrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+\n+  xorl(tmpReg, tmpReg); \/\/ Set ZF == 1\n+\n+  bind(NO_COUNT);\n@@ -1522,1 +1657,1 @@\n-  if (vlen_in_bytes == 4) {\n+  if (vlen_in_bytes <= 4) {\n@@ -1954,1 +2089,0 @@\n-  assert(ArrayOperationPartialInlineSize > 0 && ArrayOperationPartialInlineSize <= 64, \"invalid\");\n@@ -2005,2 +2139,2 @@\n-void C2_MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len) {\n-  MacroAssembler::evmovdqu(type, kmask, dst, src, vector_len);\n+void C2_MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len) {\n+  MacroAssembler::evmovdqu(type, kmask, dst, src, merge, vector_len);\n@@ -2009,2 +2143,2 @@\n-void C2_MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len) {\n-  MacroAssembler::evmovdqu(type, kmask, dst, src, vector_len);\n+void C2_MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, bool merge, int vector_len) {\n+  MacroAssembler::evmovdqu(type, kmask, dst, src, merge, vector_len);\n@@ -2013,0 +2147,33 @@\n+void C2_MacroAssembler::vmovmask(BasicType elem_bt, XMMRegister dst, Address src, XMMRegister mask,\n+                                 int vec_enc) {\n+  switch(elem_bt) {\n+    case T_INT:\n+    case T_FLOAT:\n+      vmaskmovps(dst, src, mask, vec_enc);\n+      break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      vmaskmovpd(dst, src, mask, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(elem_bt));\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vmovmask(BasicType elem_bt, Address dst, XMMRegister src, XMMRegister mask,\n+                                 int vec_enc) {\n+  switch(elem_bt) {\n+    case T_INT:\n+    case T_FLOAT:\n+      vmaskmovps(dst, src, mask, vec_enc);\n+      break;\n+    case T_LONG:\n+    case T_DOUBLE:\n+      vmaskmovpd(dst, src, mask, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(elem_bt));\n+      break;\n+  }\n+}\n@@ -2296,0 +2463,80 @@\n+void C2_MacroAssembler::vpadd(BasicType elem_bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vlen_enc) {\n+  assert(UseAVX >= 2, \"required\");\n+#ifdef ASSERT\n+  bool is_bw = ((elem_bt == T_BYTE) || (elem_bt == T_SHORT));\n+  bool is_bw_supported = VM_Version::supports_avx512bw();\n+  if (is_bw && !is_bw_supported) {\n+    assert(vlen_enc != Assembler::AVX_512bit, \"required\");\n+    assert((dst->encoding() < 16) && (src1->encoding() < 16) && (src2->encoding() < 16),\n+           \"XMM register should be 0-15\");\n+  }\n+#endif \/\/ ASSERT\n+  switch (elem_bt) {\n+    case T_BYTE: vpaddb(dst, src1, src2, vlen_enc); return;\n+    case T_SHORT: vpaddw(dst, src1, src2, vlen_enc); return;\n+    case T_INT: vpaddd(dst, src1, src2, vlen_enc); return;\n+    case T_FLOAT: vaddps(dst, src1, src2, vlen_enc); return;\n+    case T_LONG: vpaddq(dst, src1, src2, vlen_enc); return;\n+    case T_DOUBLE: vaddpd(dst, src1, src2, vlen_enc); return;\n+    default: fatal(\"Unsupported type %s\", type2name(elem_bt)); return;\n+  }\n+}\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::vpbroadcast(BasicType elem_bt, XMMRegister dst, Register src, int vlen_enc) {\n+  assert(UseAVX >= 2, \"required\");\n+  bool is_bw = ((elem_bt == T_BYTE) || (elem_bt == T_SHORT));\n+  bool is_vl = vlen_enc != Assembler::AVX_512bit;\n+  if ((UseAVX > 2) &&\n+      (!is_bw || VM_Version::supports_avx512bw()) &&\n+      (!is_vl || VM_Version::supports_avx512vl())) {\n+    switch (elem_bt) {\n+      case T_BYTE: evpbroadcastb(dst, src, vlen_enc); return;\n+      case T_SHORT: evpbroadcastw(dst, src, vlen_enc); return;\n+      case T_FLOAT: case T_INT: evpbroadcastd(dst, src, vlen_enc); return;\n+      case T_DOUBLE: case T_LONG: evpbroadcastq(dst, src, vlen_enc); return;\n+      default: fatal(\"Unsupported type %s\", type2name(elem_bt)); return;\n+    }\n+  } else {\n+    assert(vlen_enc != Assembler::AVX_512bit, \"required\");\n+    assert((dst->encoding() < 16),\"XMM register should be 0-15\");\n+    switch (elem_bt) {\n+      case T_BYTE: movdl(dst, src); vpbroadcastb(dst, dst, vlen_enc); return;\n+      case T_SHORT: movdl(dst, src); vpbroadcastw(dst, dst, vlen_enc); return;\n+      case T_INT: movdl(dst, src); vpbroadcastd(dst, dst, vlen_enc); return;\n+      case T_FLOAT: movdl(dst, src); vbroadcastss(dst, dst, vlen_enc); return;\n+      case T_LONG: movdq(dst, src); vpbroadcastq(dst, dst, vlen_enc); return;\n+      case T_DOUBLE: movdq(dst, src); vbroadcastsd(dst, dst, vlen_enc); return;\n+      default: fatal(\"Unsupported type %s\", type2name(elem_bt)); return;\n+    }\n+  }\n+}\n+#endif\n+\n+void C2_MacroAssembler::vconvert_b2x(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, int vlen_enc) {\n+  switch (to_elem_bt) {\n+    case T_SHORT:\n+      vpmovsxbw(dst, src, vlen_enc);\n+      break;\n+    case T_INT:\n+      vpmovsxbd(dst, src, vlen_enc);\n+      break;\n+    case T_FLOAT:\n+      vpmovsxbd(dst, src, vlen_enc);\n+      vcvtdq2ps(dst, dst, vlen_enc);\n+      break;\n+    case T_LONG:\n+      vpmovsxbq(dst, src, vlen_enc);\n+      break;\n+    case T_DOUBLE: {\n+      int mid_vlen_enc = (vlen_enc == Assembler::AVX_512bit) ? Assembler::AVX_256bit : Assembler::AVX_128bit;\n+      vpmovsxbd(dst, src, mid_vlen_enc);\n+      vcvtdq2pd(dst, dst, vlen_enc);\n+      break;\n+    }\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(to_elem_bt));\n+      break;\n+  }\n+}\n+\n@@ -2934,1 +3181,1 @@\n-  jcc(Assembler::less, SCAN_TO_CHAR_INIT);\/\/less than 16 entires left\n+  jcc(Assembler::less, SCAN_TO_CHAR_INIT);\/\/less than 16 entries left\n@@ -4068,7 +4315,6 @@\n- * Algorithm for vector D2L and F2I conversions:-\n- * a) Perform vector D2L\/F2I cast.\n- * b) Choose fast path if none of the result vector lane contains 0x80000000 value.\n- *    It signifies that source value could be any of the special floating point\n- *    values(NaN,-Inf,Inf,Max,-Min).\n- * c) Set destination to zero if source is NaN value.\n- * d) Replace 0x80000000 with MaxInt if source lane contains a +ve value.\n+ * Following routine handles special floating point values(NaN\/Inf\/-Inf\/Max\/Min) for casting operation.\n+ * If src is NaN, the result is 0.\n+ * If the src is negative infinity or any value less than or equal to the value of Integer.MIN_VALUE,\n+ * the result is equal to the value of Integer.MIN_VALUE.\n+ * If the src is positive infinity or any value greater than or equal to the value of Integer.MAX_VALUE,\n+ * the result is equal to the value of Integer.MAX_VALUE.\n@@ -4076,4 +4322,4 @@\n-\n-void C2_MacroAssembler::vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n-                                            Register scratch, int vec_enc) {\n+void C2_MacroAssembler::vector_cast_float_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                            XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                                                            Register scratch, AddressLiteral float_sign_flip,\n+                                                            int vec_enc) {\n@@ -4081,22 +4327,0 @@\n-  evcvttpd2qq(dst, src, vec_enc);\n-  evmovdqul(xtmp1, k0, double_sign_flip, false, vec_enc, scratch);\n-  evpcmpeqq(ktmp1, xtmp1, dst, vec_enc);\n-  kortestwl(ktmp1, ktmp1);\n-  jccb(Assembler::equal, done);\n-\n-  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n-  evcmppd(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n-  evmovdquq(dst, ktmp2, xtmp2, true, vec_enc);\n-\n-  kxorwl(ktmp1, ktmp1, ktmp2);\n-  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n-  vpternlogq(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n-  evmovdquq(dst, ktmp1, xtmp2, true, vec_enc);\n-  bind(done);\n-}\n-\n-void C2_MacroAssembler::vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n-                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n-                                           AddressLiteral float_sign_flip, Register scratch, int vec_enc) {\n-  Label done;\n-  vcvttps2dq(dst, src, vec_enc);\n@@ -4127,3 +4351,4 @@\n-void C2_MacroAssembler::vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n-                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n-                                            Register scratch, int vec_enc) {\n+void C2_MacroAssembler::vector_cast_float_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                             XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                             Register scratch, AddressLiteral float_sign_flip,\n+                                                             int vec_enc) {\n@@ -4131,1 +4356,0 @@\n-  vcvttps2dq(dst, src, vec_enc);\n@@ -4148,0 +4372,160 @@\n+void C2_MacroAssembler::vector_cast_float_to_long_special_cases_evex(\n+                                                             XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                             XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                             Register scratch, AddressLiteral double_sign_flip,\n+                                                             int vec_enc) {\n+  Label done;\n+  evmovdquq(xtmp1, k0, double_sign_flip, false, vec_enc, scratch);\n+  Assembler::evpcmpeqq(ktmp1, k0, xtmp1, dst, vec_enc);\n+  kortestwl(ktmp1, ktmp1);\n+  jccb(Assembler::equal, done);\n+\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  evcmpps(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n+  evmovdquq(dst, ktmp2, xtmp2, true, vec_enc);\n+\n+  kxorwl(ktmp1, ktmp1, ktmp2);\n+  evcmpps(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n+  vpternlogq(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n+  evmovdquq(dst, ktmp1, xtmp2, true, vec_enc);\n+  bind(done);\n+}\n+\n+\/*\n+ * Following routine handles special floating point values(NaN\/Inf\/-Inf\/Max\/Min) for casting operation.\n+ * If src is NaN, the result is 0.\n+ * If the src is negative infinity or any value less than or equal to the value of Long.MIN_VALUE,\n+ * the result is equal to the value of Long.MIN_VALUE.\n+ * If the src is positive infinity or any value greater than or equal to the value of Long.MAX_VALUE,\n+ * the result is equal to the value of Long.MAX_VALUE.\n+ *\/\n+void C2_MacroAssembler::vector_cast_double_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                              XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                              Register scratch, AddressLiteral double_sign_flip,\n+                                                              int vec_enc) {\n+  Label done;\n+  evmovdqul(xtmp1, k0, double_sign_flip, false, vec_enc, scratch);\n+  evpcmpeqq(ktmp1, xtmp1, dst, vec_enc);\n+  kortestwl(ktmp1, ktmp1);\n+  jccb(Assembler::equal, done);\n+\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  evcmppd(ktmp2, k0, src, src, Assembler::UNORD_Q, vec_enc);\n+  evmovdquq(dst, ktmp2, xtmp2, true, vec_enc);\n+\n+  kxorwl(ktmp1, ktmp1, ktmp2);\n+  evcmppd(ktmp1, ktmp1, src, xtmp2, Assembler::NLT_UQ, vec_enc);\n+  vpternlogq(xtmp2, 0x11, xtmp1, xtmp1, vec_enc);\n+  evmovdquq(dst, ktmp1, xtmp2, true, vec_enc);\n+  bind(done);\n+}\n+\n+\/*\n+ * Algorithm for vector D2L and F2I conversions:-\n+ * a) Perform vector D2L\/F2I cast.\n+ * b) Choose fast path if none of the result vector lane contains 0x80000000 value.\n+ *    It signifies that source value could be any of the special floating point\n+ *    values(NaN,-Inf,Inf,Max,-Min).\n+ * c) Set destination to zero if source is NaN value.\n+ * d) Replace 0x80000000 with MaxInt if source lane contains a +ve value.\n+ *\/\n+\n+void C2_MacroAssembler::vector_castD2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                            Register scratch, int vec_enc) {\n+  evcvttpd2qq(dst, src, vec_enc);\n+  vector_cast_double_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, double_sign_flip, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_castF2I_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                                           AddressLiteral float_sign_flip, Register scratch, int vec_enc) {\n+  vcvttps2dq(dst, src, vec_enc);\n+  vector_cast_float_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, scratch, float_sign_flip, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_castF2I_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n+                                            Register scratch, int vec_enc) {\n+  vcvttps2dq(dst, src, vec_enc);\n+  vector_cast_float_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, float_sign_flip, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_castF2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                            Register scratch, int vec_enc) {\n+  evcvttps2qq(dst, src, vec_enc);\n+  vector_cast_float_to_long_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, double_sign_flip, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_castD2X_evex(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                            XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                            AddressLiteral double_sign_flip, Register scratch, int vec_enc) {\n+  vector_castD2L_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, double_sign_flip, scratch, vec_enc);\n+  if (to_elem_bt != T_LONG) {\n+    switch(to_elem_bt) {\n+      case T_INT:\n+        evpmovsqd(dst, dst, vec_enc);\n+        break;\n+      case T_SHORT:\n+        evpmovsqd(dst, dst, vec_enc);\n+        evpmovdw(dst, dst, vec_enc);\n+        break;\n+      case T_BYTE:\n+        evpmovsqd(dst, dst, vec_enc);\n+        evpmovdb(dst, dst, vec_enc);\n+        break;\n+      default: assert(false, \"%s\", type2name(to_elem_bt));\n+    }\n+  }\n+}\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::vector_round_double_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                 KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                                 AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+  \/\/ Perform floor(val+0.5) operation under the influence of MXCSR.RC mode roundTowards -inf.\n+  \/\/ and re-instantiate original MXCSR.RC mode after that.\n+  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+  ldmxcsr(new_mxcsr, scratch);\n+  mov64(scratch, julong_cast(0.5L));\n+  evpbroadcastq(xtmp1, scratch, vec_enc);\n+  vaddpd(xtmp1, src , xtmp1, vec_enc);\n+  evcvtpd2qq(dst, xtmp1, vec_enc);\n+  vector_cast_double_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, double_sign_flip, vec_enc);\n+  ldmxcsr(mxcsr_std, scratch);\n+}\n+\n+void C2_MacroAssembler::vector_round_float_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                                KRegister ktmp1, KRegister ktmp2, AddressLiteral float_sign_flip,\n+                                                AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+  \/\/ Perform floor(val+0.5) operation under the influence of MXCSR.RC mode roundTowards -inf.\n+  \/\/ and re-instantiate original MXCSR.RC mode after that.\n+  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+  ldmxcsr(new_mxcsr, scratch);\n+  movl(scratch, jint_cast(0.5));\n+  movq(xtmp1, scratch);\n+  vbroadcastss(xtmp1, xtmp1, vec_enc);\n+  vaddps(xtmp1, src , xtmp1, vec_enc);\n+  vcvtps2dq(dst, xtmp1, vec_enc);\n+  vector_cast_float_special_cases_evex(dst, src, xtmp1, xtmp2, ktmp1, ktmp2, scratch, float_sign_flip, vec_enc);\n+  ldmxcsr(mxcsr_std, scratch);\n+}\n+\n+void C2_MacroAssembler::vector_round_float_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                               XMMRegister xtmp3, XMMRegister xtmp4, AddressLiteral float_sign_flip,\n+                                               AddressLiteral new_mxcsr, Register scratch, int vec_enc) {\n+  \/\/ Perform floor(val+0.5) operation under the influence of MXCSR.RC mode roundTowards -inf.\n+  \/\/ and re-instantiate original MXCSR.RC mode after that.\n+  ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+  ldmxcsr(new_mxcsr, scratch);\n+  movl(scratch, jint_cast(0.5));\n+  movq(xtmp1, scratch);\n+  vbroadcastss(xtmp1, xtmp1, vec_enc);\n+  vaddps(xtmp1, src , xtmp1, vec_enc);\n+  vcvtps2dq(dst, xtmp1, vec_enc);\n+  vector_cast_float_special_cases_avx(dst, src, xtmp1, xtmp2, xtmp3, xtmp4, scratch, float_sign_flip, vec_enc);\n+  ldmxcsr(mxcsr_std, scratch);\n+}\n+#endif\n+\n@@ -4202,1 +4586,1 @@\n-  pdep(rtmp1, src, rtmp1);\n+  pdepq(rtmp1, src, rtmp1);\n@@ -4219,1 +4603,1 @@\n-    pdep(rtmp1, rtmp2, rtmp1);\n+    pdepq(rtmp1, rtmp2, rtmp1);\n@@ -4354,0 +4738,65 @@\n+\n+void C2_MacroAssembler::vector_mask_compress(KRegister dst, KRegister src, Register rtmp1,\n+                                             Register rtmp2, int mask_len) {\n+  kmov(rtmp1, src);\n+  andq(rtmp1, (0xFFFFFFFFFFFFFFFFUL >> (64 - mask_len)));\n+  mov64(rtmp2, -1L);\n+  pextq(rtmp2, rtmp2, rtmp1);\n+  kmov(dst, rtmp2);\n+}\n+\n+void C2_MacroAssembler::vector_compress_expand(int opcode, XMMRegister dst, XMMRegister src, KRegister mask,\n+                                               bool merge, BasicType bt, int vec_enc) {\n+  if (opcode == Op_CompressV) {\n+    switch(bt) {\n+    case T_BYTE:\n+      evpcompressb(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpcompressw(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      evpcompressd(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_FLOAT:\n+      evcompressps(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_LONG:\n+      evpcompressq(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_DOUBLE:\n+      evcompresspd(dst, mask, src, merge, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n+    }\n+  } else {\n+    assert(opcode == Op_ExpandV, \"\");\n+    switch(bt) {\n+    case T_BYTE:\n+      evpexpandb(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      evpexpandw(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      evpexpandd(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_FLOAT:\n+      evexpandps(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_LONG:\n+      evpexpandq(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_DOUBLE:\n+      evexpandpd(dst, mask, src, merge, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n+    }\n+  }\n+}\n@@ -4356,0 +4805,42 @@\n+void C2_MacroAssembler::vector_signum_evex(int opcode, XMMRegister dst, XMMRegister src, XMMRegister zero, XMMRegister one,\n+                                           KRegister ktmp1, int vec_enc) {\n+  if (opcode == Op_SignumVD) {\n+    vsubpd(dst, zero, one, vec_enc);\n+    \/\/ if src < 0 ? -1 : 1\n+    evcmppd(ktmp1, k0, src, zero, Assembler::LT_OQ, vec_enc);\n+    evblendmpd(dst, ktmp1, one, dst, true, vec_enc);\n+    \/\/ if src == NaN, -0.0 or 0.0 return src.\n+    evcmppd(ktmp1, k0, src, zero, Assembler::EQ_UQ, vec_enc);\n+    evblendmpd(dst, ktmp1, dst, src, true, vec_enc);\n+  } else {\n+    assert(opcode == Op_SignumVF, \"\");\n+    vsubps(dst, zero, one, vec_enc);\n+    \/\/ if src < 0 ? -1 : 1\n+    evcmpps(ktmp1, k0, src, zero, Assembler::LT_OQ, vec_enc);\n+    evblendmps(dst, ktmp1, one, dst, true, vec_enc);\n+    \/\/ if src == NaN, -0.0 or 0.0 return src.\n+    evcmpps(ktmp1, k0, src, zero, Assembler::EQ_UQ, vec_enc);\n+    evblendmps(dst, ktmp1, dst, src, true, vec_enc);\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_signum_avx(int opcode, XMMRegister dst, XMMRegister src, XMMRegister zero, XMMRegister one,\n+                                          XMMRegister xtmp1, int vec_enc) {\n+  if (opcode == Op_SignumVD) {\n+    vsubpd(dst, zero, one, vec_enc);\n+    \/\/ if src < 0 ? -1 : 1\n+    vblendvpd(dst, one, dst, src, vec_enc);\n+    \/\/ if src == NaN, -0.0 or 0.0 return src.\n+    vcmppd(xtmp1, src, zero, Assembler::EQ_UQ, vec_enc);\n+    vblendvpd(dst, dst, src, xtmp1, vec_enc);\n+  } else {\n+    assert(opcode == Op_SignumVF, \"\");\n+    vsubps(dst, zero, one, vec_enc);\n+    \/\/ if src < 0 ? -1 : 1\n+    vblendvps(dst, one, dst, src, vec_enc);\n+    \/\/ if src == NaN, -0.0 or 0.0 return src.\n+    vcmpps(xtmp1, src, zero, Assembler::EQ_UQ, vec_enc);\n+    vblendvps(dst, dst, src, xtmp1, vec_enc);\n+  }\n+}\n+\n@@ -4375,0 +4866,28 @@\n+void C2_MacroAssembler::vbroadcast(BasicType bt, XMMRegister dst, int imm32, Register rtmp, int vec_enc) {\n+  int lane_size = type2aelembytes(bt);\n+  bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if ((is_LP64 || lane_size < 8) &&\n+      ((is_non_subword_integral_type(bt) && VM_Version::supports_avx512vl()) ||\n+       (is_subword_type(bt) && VM_Version::supports_avx512vlbw()))) {\n+    movptr(rtmp, imm32);\n+    switch(lane_size) {\n+      case 1 : evpbroadcastb(dst, rtmp, vec_enc); break;\n+      case 2 : evpbroadcastw(dst, rtmp, vec_enc); break;\n+      case 4 : evpbroadcastd(dst, rtmp, vec_enc); break;\n+      case 8 : evpbroadcastq(dst, rtmp, vec_enc); break;\n+      fatal(\"Unsupported lane size %d\", lane_size);\n+      break;\n+    }\n+  } else {\n+    movptr(rtmp, imm32);\n+    LP64_ONLY(movq(dst, rtmp)) NOT_LP64(movdl(dst, rtmp));\n+    switch(lane_size) {\n+      case 1 : vpbroadcastb(dst, dst, vec_enc); break;\n+      case 2 : vpbroadcastw(dst, dst, vec_enc); break;\n+      case 4 : vpbroadcastd(dst, dst, vec_enc); break;\n+      case 8 : vpbroadcastq(dst, dst, vec_enc); break;\n+      fatal(\"Unsupported lane size %d\", lane_size);\n+      break;\n+    }\n+  }\n+}\n@@ -4405,0 +4924,14 @@\n+\n+void C2_MacroAssembler::vector_popcount_byte(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                             XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  assert((vec_enc == Assembler::AVX_512bit && VM_Version::supports_avx512bw()) || VM_Version::supports_avx2(), \"\");\n+  vbroadcast(T_INT, xtmp1, 0x0F0F0F0F, rtmp, vec_enc);\n+  vpsrlw(dst, src, 4, vec_enc);\n+  vpand(dst, dst, xtmp1, vec_enc);\n+  vpand(xtmp1, src, xtmp1, vec_enc);\n+  vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), rtmp, vec_enc);\n+  vpshufb(xtmp1, xtmp2, xtmp1, vec_enc);\n+  vpshufb(dst, xtmp2, dst, vec_enc);\n+  vpaddb(dst, dst, xtmp1, vec_enc);\n+}\n+\n@@ -4406,27 +4939,19 @@\n-                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                                            int vec_enc) {\n-  if (VM_Version::supports_avx512_vpopcntdq()) {\n-    vpopcntd(dst, src, vec_enc);\n-  } else {\n-    assert((vec_enc == Assembler::AVX_512bit && VM_Version::supports_avx512bw()) || VM_Version::supports_avx2(), \"\");\n-    movl(rtmp, 0x0F0F0F0F);\n-    movdl(xtmp1, rtmp);\n-    vpbroadcastd(xtmp1, xtmp1, vec_enc);\n-    if (Assembler::AVX_512bit == vec_enc) {\n-      evmovdqul(xtmp2, k0, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), false, vec_enc, rtmp);\n-    } else {\n-      vmovdqu(xtmp2, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), rtmp);\n-    }\n-    vpand(xtmp3, src, xtmp1, vec_enc);\n-    vpshufb(xtmp3, xtmp2, xtmp3, vec_enc);\n-    vpsrlw(dst, src, 4, vec_enc);\n-    vpand(dst, dst, xtmp1, vec_enc);\n-    vpshufb(dst, xtmp2, dst, vec_enc);\n-    vpaddb(xtmp3, dst, xtmp3, vec_enc);\n-    vpxor(xtmp1, xtmp1, xtmp1, vec_enc);\n-    vpunpckhdq(dst, xtmp3, xtmp1, vec_enc);\n-    vpsadbw(dst, dst, xtmp1, vec_enc);\n-    vpunpckldq(xtmp2, xtmp3, xtmp1, vec_enc);\n-    vpsadbw(xtmp2, xtmp2, xtmp1, vec_enc);\n-    vpackuswb(dst, xtmp2, dst, vec_enc);\n-  }\n+                                            XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  vector_popcount_byte(xtmp1, src, dst, xtmp2, rtmp, vec_enc);\n+  \/\/ Following code is as per steps e,f,g and h of above algorithm.\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  vpunpckhdq(dst, xtmp1, xtmp2, vec_enc);\n+  vpsadbw(dst, dst, xtmp2, vec_enc);\n+  vpunpckldq(xtmp1, xtmp1, xtmp2, vec_enc);\n+  vpsadbw(xtmp1, xtmp1, xtmp2, vec_enc);\n+  vpackuswb(dst, xtmp1, dst, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_popcount_short(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                              XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  vector_popcount_byte(xtmp1, src, dst, xtmp2, rtmp, vec_enc);\n+  \/\/ Add the popcount of upper and lower bytes of word.\n+  vbroadcast(T_INT, xtmp2, 0x00FF00FF, rtmp, vec_enc);\n+  vpsrlw(dst, xtmp1, 8, vec_enc);\n+  vpand(xtmp1, xtmp1, xtmp2, vec_enc);\n+  vpaddw(dst, dst, xtmp1, vec_enc);\n@@ -4436,23 +4961,54 @@\n-                                             XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                                             int vec_enc) {\n-  if (VM_Version::supports_avx512_vpopcntdq()) {\n-    vpopcntq(dst, src, vec_enc);\n-  } else if (vec_enc == Assembler::AVX_512bit) {\n-    assert(VM_Version::supports_avx512bw(), \"\");\n-    movl(rtmp, 0x0F0F0F0F);\n-    movdl(xtmp1, rtmp);\n-    vpbroadcastd(xtmp1, xtmp1, vec_enc);\n-    evmovdqul(xtmp2, k0, ExternalAddress(StubRoutines::x86::vector_popcount_lut()), true, vec_enc, rtmp);\n-    vpandq(xtmp3, src, xtmp1, vec_enc);\n-    vpshufb(xtmp3, xtmp2, xtmp3, vec_enc);\n-    vpsrlw(dst, src, 4, vec_enc);\n-    vpandq(dst, dst, xtmp1, vec_enc);\n-    vpshufb(dst, xtmp2, dst, vec_enc);\n-    vpaddb(xtmp3, dst, xtmp3, vec_enc);\n-    vpxorq(xtmp1, xtmp1, xtmp1, vec_enc);\n-    vpsadbw(dst, xtmp3, xtmp1, vec_enc);\n-  } else {\n-    \/\/ We do not see any performance benefit of running\n-    \/\/ above instruction sequence on 256 bit vector which\n-    \/\/ can operate over maximum 4 long elements.\n-    ShouldNotReachHere();\n+                                             XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  vector_popcount_byte(xtmp1, src, dst, xtmp2, rtmp, vec_enc);\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  vpsadbw(dst, xtmp1, xtmp2, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_popcount_integral(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                 XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  switch(bt) {\n+    case T_LONG:\n+      vector_popcount_long(dst, src, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_INT:\n+      vector_popcount_int(dst, src, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      vector_popcount_short(dst, src, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+      vector_popcount_byte(dst, src, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_popcount_integral_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                      KRegister mask, bool merge, int vec_enc) {\n+  assert(VM_Version::supports_avx512vl() || vec_enc == Assembler::AVX_512bit, \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      assert(VM_Version::supports_avx512_vpopcntdq(), \"\");\n+      evpopcntq(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      assert(VM_Version::supports_avx512_vpopcntdq(), \"\");\n+      evpopcntd(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      assert(VM_Version::supports_avx512_bitalg(), \"\");\n+      evpopcntw(dst, mask, src, merge, vec_enc);\n+      break;\n+    case T_BYTE:\n+    case T_BOOLEAN:\n+      assert(VM_Version::supports_avx512_bitalg(), \"\");\n+      evpopcntb(dst, mask, src, merge, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n@@ -4460,1 +5016,0 @@\n-  evpmovqd(dst, dst, vec_enc);\n@@ -4470,0 +5025,534 @@\n+\n+\/\/ Bit reversal algorithm first reverses the bits of each byte followed by\n+\/\/ a byte level reversal for multi-byte primitive types (short\/int\/long).\n+\/\/ Algorithm performs a lookup table access to get reverse bit sequence\n+\/\/ corresponding to a 4 bit value. Thus a reverse bit sequence for a byte\n+\/\/ is obtained by swapping the reverse bit sequences of upper and lower\n+\/\/ nibble of a byte.\n+void C2_MacroAssembler::vector_reverse_bit(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  if (VM_Version::supports_avx512vlbw()) {\n+\n+    \/\/ Get the reverse bit sequence of lower nibble of each byte.\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vbroadcast(T_INT, xtmp2, 0x0F0F0F0F, rtmp, vec_enc);\n+    vpandq(dst, xtmp2, src, vec_enc);\n+    vpshufb(dst, xtmp1, dst, vec_enc);\n+    vpsllq(dst, dst, 4, vec_enc);\n+\n+    \/\/ Get the reverse bit sequence of upper nibble of each byte.\n+    vpandn(xtmp2, xtmp2, src, vec_enc);\n+    vpsrlq(xtmp2, xtmp2, 4, vec_enc);\n+    vpshufb(xtmp2, xtmp1, xtmp2, vec_enc);\n+\n+    \/\/ Perform logical OR operation b\/w left shifted reverse bit sequence of lower nibble and\n+    \/\/ right shifted reverse bit sequence of upper nibble to obtain the reverse bit sequence of each byte.\n+    vporq(xtmp2, dst, xtmp2, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+\n+  } else if(vec_enc == Assembler::AVX_512bit) {\n+    \/\/ Shift based bit reversal.\n+    assert(bt == T_LONG || bt == T_INT, \"\");\n+\n+    \/\/ Swap lower and upper nibble of each byte.\n+    vector_swap_nbits(4, 0x0F0F0F0F, xtmp1, src, xtmp2, rtmp, vec_enc);\n+\n+    \/\/ Swap two least and most significant bits of each nibble.\n+    vector_swap_nbits(2, 0x33333333, dst, xtmp1, xtmp2, rtmp, vec_enc);\n+\n+    \/\/ Swap adjacent pair of bits.\n+    evmovdqul(xtmp1, k0, dst, true, vec_enc);\n+    vector_swap_nbits(1, 0x55555555, dst, xtmp1, xtmp2, rtmp, vec_enc);\n+\n+    evmovdqul(xtmp1, k0, dst, true, vec_enc);\n+    vector_reverse_byte64(bt, dst, xtmp1, xtmp1, xtmp2, rtmp, vec_enc);\n+  } else {\n+    vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_reverse_bit_lut()), rtmp, vec_enc);\n+    vbroadcast(T_INT, xtmp2, 0x0F0F0F0F, rtmp, vec_enc);\n+\n+    \/\/ Get the reverse bit sequence of lower nibble of each byte.\n+    vpand(dst, xtmp2, src, vec_enc);\n+    vpshufb(dst, xtmp1, dst, vec_enc);\n+    vpsllq(dst, dst, 4, vec_enc);\n+\n+    \/\/ Get the reverse bit sequence of upper nibble of each byte.\n+    vpandn(xtmp2, xtmp2, src, vec_enc);\n+    vpsrlq(xtmp2, xtmp2, 4, vec_enc);\n+    vpshufb(xtmp2, xtmp1, xtmp2, vec_enc);\n+\n+    \/\/ Perform logical OR operation b\/w left shifted reverse bit sequence of lower nibble and\n+    \/\/ right shifted reverse bit sequence of upper nibble to obtain the reverse bit sequence of each byte.\n+    vpor(xtmp2, dst, xtmp2, vec_enc);\n+    vector_reverse_byte(bt, dst, xtmp2, rtmp, vec_enc);\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_reverse_bit_gfni(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                XMMRegister xtmp, AddressLiteral mask, Register rtmp, int vec_enc) {\n+  \/\/ Galois field instruction based bit reversal based on following algorithm.\n+  \/\/ http:\/\/0x80.pl\/articles\/avx512-galois-field-for-bit-shuffling.html\n+  assert(VM_Version::supports_gfni(), \"\");\n+  vpbroadcastq(xtmp, mask, vec_enc, rtmp);\n+  vgf2p8affineqb(xtmp, src, xtmp, 0, vec_enc);\n+  vector_reverse_byte(bt, dst, xtmp, rtmp, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_swap_nbits(int nbits, int bitmask, XMMRegister dst, XMMRegister src,\n+                                          XMMRegister xtmp1, Register rtmp, int vec_enc) {\n+  vbroadcast(T_INT, xtmp1, bitmask, rtmp, vec_enc);\n+  vpandq(dst, xtmp1, src, vec_enc);\n+  vpsllq(dst, dst, nbits, vec_enc);\n+  vpandn(xtmp1, xtmp1, src, vec_enc);\n+  vpsrlq(xtmp1, xtmp1, nbits, vec_enc);\n+  vporq(dst, dst, xtmp1, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_reverse_byte64(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                              XMMRegister xtmp2, Register rtmp, int vec_enc) {\n+  \/\/ Shift based bit reversal.\n+  assert(VM_Version::supports_evex(), \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      \/\/ Swap upper and lower double word of each quad word.\n+      evprorq(xtmp1, k0, src, 32, true, vec_enc);\n+      evprord(xtmp1, k0, xtmp1, 16, true, vec_enc);\n+      vector_swap_nbits(8, 0x00FF00FF, dst, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_INT:\n+      \/\/ Swap upper and lower word of each double word.\n+      evprord(xtmp1, k0, src, 16, true, vec_enc);\n+      vector_swap_nbits(8, 0x00FF00FF, dst, xtmp1, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      \/\/ Swap upper and lower byte of each word.\n+      vector_swap_nbits(8, 0x00FF00FF, dst, src, xtmp2, rtmp, vec_enc);\n+      break;\n+    case T_BYTE:\n+      evmovdquq(dst, k0, src, true, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc) {\n+  if (bt == T_BYTE) {\n+    if (VM_Version::supports_avx512vl() || vec_enc == Assembler::AVX_512bit) {\n+      evmovdquq(dst, k0, src, true, vec_enc);\n+    } else {\n+      vmovdqu(dst, src);\n+    }\n+    return;\n+  }\n+  \/\/ Perform byte reversal by shuffling the bytes of a multi-byte primitive type using\n+  \/\/ pre-computed shuffle indices.\n+  switch(bt) {\n+    case T_LONG:\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_long()), rtmp, vec_enc);\n+      break;\n+    case T_INT:\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_int()), rtmp, vec_enc);\n+      break;\n+    case T_CHAR:\n+    case T_SHORT:\n+      vmovdqu(dst, ExternalAddress(StubRoutines::x86::vector_reverse_byte_perm_mask_short()), rtmp, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n+  }\n+  vpshufb(dst, src, dst, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                        XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                        KRegister ktmp, Register rtmp, bool merge, int vec_enc) {\n+  assert(is_integral_type(bt), \"\");\n+  assert(VM_Version::supports_avx512vl() || vec_enc == Assembler::AVX_512bit, \"\");\n+  assert(VM_Version::supports_avx512cd(), \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      evplzcntq(dst, ktmp, src, merge, vec_enc);\n+      break;\n+    case T_INT:\n+      evplzcntd(dst, ktmp, src, merge, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vpternlogd(xtmp1, 0xff, xtmp1, xtmp1, vec_enc);\n+      vpunpcklwd(xtmp2, xtmp1, src, vec_enc);\n+      evplzcntd(xtmp2, ktmp, xtmp2, merge, vec_enc);\n+      vpunpckhwd(dst, xtmp1, src, vec_enc);\n+      evplzcntd(dst, ktmp, dst, merge, vec_enc);\n+      vpackusdw(dst, xtmp2, dst, vec_enc);\n+      break;\n+    case T_BYTE:\n+      \/\/ T1 = Compute leading zero counts of 4 LSB bits of each byte by\n+      \/\/ accessing the lookup table.\n+      \/\/ T2 = Compute leading zero counts of 4 MSB bits of each byte by\n+      \/\/ accessing the lookup table.\n+      \/\/ Add T1 to T2 if 4 MSB bits of byte are all zeros.\n+      assert(VM_Version::supports_avx512bw(), \"\");\n+      evmovdquq(xtmp1, ExternalAddress(StubRoutines::x86::vector_count_leading_zeros_lut()), vec_enc, rtmp);\n+      vbroadcast(T_INT, dst, 0x0F0F0F0F, rtmp, vec_enc);\n+      vpand(xtmp2, dst, src, vec_enc);\n+      vpshufb(xtmp2, xtmp1, xtmp2, vec_enc);\n+      vpsrlw(xtmp3, src, 4, vec_enc);\n+      vpand(xtmp3, dst, xtmp3, vec_enc);\n+      vpshufb(dst, xtmp1, xtmp3, vec_enc);\n+      vpxor(xtmp1, xtmp1, xtmp1, vec_enc);\n+      evpcmpeqb(ktmp, xtmp1, xtmp3, vec_enc);\n+      evpaddb(dst, ktmp, dst, xtmp2, true, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_byte_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  vmovdqu(xtmp1, ExternalAddress(StubRoutines::x86::vector_count_leading_zeros_lut()), rtmp);\n+  vbroadcast(T_INT, xtmp2, 0x0F0F0F0F, rtmp, vec_enc);\n+  \/\/ T1 = Compute leading zero counts of 4 LSB bits of each byte by\n+  \/\/ accessing the lookup table.\n+  vpand(dst, xtmp2, src, vec_enc);\n+  vpshufb(dst, xtmp1, dst, vec_enc);\n+  \/\/ T2 = Compute leading zero counts of 4 MSB bits of each byte by\n+  \/\/ accessing the lookup table.\n+  vpsrlw(xtmp3, src, 4, vec_enc);\n+  vpand(xtmp3, xtmp2, xtmp3, vec_enc);\n+  vpshufb(xtmp2, xtmp1, xtmp3, vec_enc);\n+  \/\/ Add T1 to T2 if 4 MSB bits of byte are all zeros.\n+  vpxor(xtmp1, xtmp1, xtmp1, vec_enc);\n+  vpcmpeqb(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpaddb(dst, dst, xtmp2, vec_enc);\n+  vpblendvb(dst, xtmp2, dst, xtmp3, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_short_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                             XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  vector_count_leading_zeros_byte_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+  \/\/ Add zero counts of lower byte and upper byte of a word if\n+  \/\/ upper byte holds a zero value.\n+  vpsrlw(xtmp3, src, 8, vec_enc);\n+  \/\/ xtmp1 is set to all zeros by vector_count_leading_zeros_byte_avx.\n+  vpcmpeqw(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpsllw(xtmp2, dst, 8, vec_enc);\n+  vpaddw(xtmp2, xtmp2, dst, vec_enc);\n+  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+  vpsrlw(dst, dst, 8, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_int_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                           XMMRegister xtmp2, XMMRegister xtmp3, int vec_enc) {\n+  \/\/ Since IEEE 754 floating point format represents mantissa in 1.0 format\n+  \/\/ hence biased exponent can be used to compute leading zero count as per\n+  \/\/ following formula:-\n+  \/\/ LZCNT = 32 - (biased_exp - 127)\n+  \/\/ Special handling has been introduced for Zero, Max_Int and -ve source values.\n+\n+  \/\/ Broadcast 0xFF\n+  vpcmpeqd(xtmp1, xtmp1, xtmp1, vec_enc);\n+  vpsrld(xtmp1, xtmp1, 24, vec_enc);\n+\n+  \/\/ Extract biased exponent.\n+  vcvtdq2ps(dst, src, vec_enc);\n+  vpsrld(dst, dst, 23, vec_enc);\n+  vpand(dst, dst, xtmp1, vec_enc);\n+\n+  \/\/ Broadcast 127.\n+  vpsrld(xtmp1, xtmp1, 1, vec_enc);\n+  \/\/ Exponent = biased_exp - 127\n+  vpsubd(dst, dst, xtmp1, vec_enc);\n+\n+  \/\/ Exponent = Exponent  + 1\n+  vpsrld(xtmp3, xtmp1, 6, vec_enc);\n+  vpaddd(dst, dst, xtmp3, vec_enc);\n+\n+  \/\/ Replace -ve exponent with zero, exponent is -ve when src\n+  \/\/ lane contains a zero value.\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  vblendvps(dst, dst, xtmp2, dst, vec_enc);\n+\n+  \/\/ Rematerialize broadcast 32.\n+  vpslld(xtmp1, xtmp3, 5, vec_enc);\n+  \/\/ Exponent is 32 if corresponding source lane contains max_int value.\n+  vpcmpeqd(xtmp2, dst, xtmp1, vec_enc);\n+  \/\/ LZCNT = 32 - exponent\n+  vpsubd(dst, xtmp1, dst, vec_enc);\n+\n+  \/\/ Replace LZCNT with a value 1 if corresponding source lane\n+  \/\/ contains max_int value.\n+  vpblendvb(dst, dst, xtmp3, xtmp2, vec_enc);\n+\n+  \/\/ Replace biased_exp with 0 if source lane value is less than zero.\n+  vpxor(xtmp2, xtmp2, xtmp2, vec_enc);\n+  vblendvps(dst, dst, xtmp2, src, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_long_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  vector_count_leading_zeros_short_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+  \/\/ Add zero counts of lower word and upper word of a double word if\n+  \/\/ upper word holds a zero value.\n+  vpsrld(xtmp3, src, 16, vec_enc);\n+  \/\/ xtmp1 is set to all zeros by vector_count_leading_zeros_byte_avx.\n+  vpcmpeqd(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpslld(xtmp2, dst, 16, vec_enc);\n+  vpaddd(xtmp2, xtmp2, dst, vec_enc);\n+  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+  vpsrld(dst, dst, 16, vec_enc);\n+  \/\/ Add zero counts of lower doubleword and upper doubleword of a\n+  \/\/ quadword if upper doubleword holds a zero value.\n+  vpsrlq(xtmp3, src, 32, vec_enc);\n+  vpcmpeqq(xtmp3, xtmp1, xtmp3, vec_enc);\n+  vpsllq(xtmp2, dst, 32, vec_enc);\n+  vpaddq(xtmp2, xtmp2, dst, vec_enc);\n+  vpblendvb(dst, dst, xtmp2, xtmp3, vec_enc);\n+  vpsrlq(dst, dst, 32, vec_enc);\n+}\n+\n+void C2_MacroAssembler::vector_count_leading_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                       XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                       Register rtmp, int vec_enc) {\n+  assert(is_integral_type(bt), \"unexpected type\");\n+  assert(vec_enc < Assembler::AVX_512bit, \"\");\n+  switch(bt) {\n+    case T_LONG:\n+      vector_count_leading_zeros_long_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    case T_INT:\n+      vector_count_leading_zeros_int_avx(dst, src, xtmp1, xtmp2, xtmp3, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vector_count_leading_zeros_short_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    case T_BYTE:\n+      vector_count_leading_zeros_byte_avx(dst, src, xtmp1, xtmp2, xtmp3, rtmp, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n+  }\n+}\n+\n+void C2_MacroAssembler::vpsub(BasicType bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vec_enc) {\n+  switch(bt) {\n+    case T_BYTE:\n+      vpsubb(dst, src1, src2, vec_enc);\n+      break;\n+    case T_SHORT:\n+      vpsubw(dst, src1, src2, vec_enc);\n+      break;\n+    case T_INT:\n+      vpsubd(dst, src1, src2, vec_enc);\n+      break;\n+    case T_LONG:\n+      vpsubq(dst, src1, src2, vec_enc);\n+      break;\n+    default:\n+      fatal(\"Unsupported type %s\", type2name(bt));\n+      break;\n+  }\n+}\n+\n+\/\/ Trailing zero count computation is based on leading zero count operation as per\n+\/\/ following equation. All AVX3 targets support AVX512CD feature which offers\n+\/\/ direct vector instruction to compute leading zero count.\n+\/\/      CTZ = PRIM_TYPE_WIDHT - CLZ((x - 1) & ~x)\n+void C2_MacroAssembler::vector_count_trailing_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                                         XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                                         XMMRegister xtmp4, KRegister ktmp, Register rtmp, int vec_enc) {\n+  assert(is_integral_type(bt), \"\");\n+  \/\/ xtmp = -1\n+  vpternlogd(xtmp4, 0xff, xtmp4, xtmp4, vec_enc);\n+  \/\/ xtmp = xtmp + src\n+  vpadd(bt, xtmp4, xtmp4, src, vec_enc);\n+  \/\/ xtmp = xtmp & ~src\n+  vpternlogd(xtmp4, 0x40, xtmp4, src, vec_enc);\n+  vector_count_leading_zeros_evex(bt, dst, xtmp4, xtmp1, xtmp2, xtmp3, ktmp, rtmp, true, vec_enc);\n+  vbroadcast(bt, xtmp4, 8 * type2aelembytes(bt), rtmp, vec_enc);\n+  vpsub(bt, dst, xtmp4, dst, vec_enc);\n+}\n+\n+\/\/ Trailing zero count computation for AVX2 targets is based on popcount operation as per following equation\n+\/\/      CTZ = PRIM_TYPE_WIDHT - POPC(x | -x)\n+void C2_MacroAssembler::vector_count_trailing_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                        XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc) {\n+  assert(is_integral_type(bt), \"\");\n+  \/\/ xtmp = 0\n+  vpxor(xtmp3 , xtmp3, xtmp3, vec_enc);\n+  \/\/ xtmp = 0 - src\n+  vpsub(bt, xtmp3, xtmp3, src, vec_enc);\n+  \/\/ xtmp = xtmp | src\n+  vpor(xtmp3, xtmp3, src, vec_enc);\n+  vector_popcount_integral(bt, dst, xtmp3, xtmp1, xtmp2, rtmp, vec_enc);\n+  vbroadcast(bt, xtmp1, 8 * type2aelembytes(bt), rtmp, vec_enc);\n+  vpsub(bt, dst, xtmp1, dst, vec_enc);\n+}\n+\n+void C2_MacroAssembler::udivI(Register rax, Register divisor, Register rdx) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpl(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorl(rdx, rdx);\n+  divl(divisor);\n+  jmpb(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath for divisor < 0:\n+  \/\/ quotient = (dividend & ~(dividend - divisor)) >>> (Integer.SIZE - 1)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in java.lang.Long.divideUnsigned()\n+  movl(rdx, rax);\n+  subl(rdx, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnl(rax, rdx, rax);\n+  } else {\n+    notl(rdx);\n+    andl(rax, rdx);\n+  }\n+  shrl(rax, 31);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::umodI(Register rax, Register divisor, Register rdx) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpl(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorl(rdx, rdx);\n+  divl(divisor);\n+  jmpb(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath when divisor < 0:\n+  \/\/ remainder = dividend - (((dividend & ~(dividend - divisor)) >> (Integer.SIZE - 1)) & divisor)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in java.lang.Long.remainderUnsigned()\n+  movl(rdx, rax);\n+  subl(rax, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnl(rax, rax, rdx);\n+  } else {\n+    notl(rax);\n+    andl(rax, rdx);\n+  }\n+  sarl(rax, 31);\n+  andl(rax, divisor);\n+  subl(rdx, rax);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::udivmodI(Register rax, Register divisor, Register rdx, Register tmp) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+\n+  cmpl(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorl(rdx, rdx);\n+  divl(divisor);\n+  jmpb(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath for divisor < 0:\n+  \/\/ quotient = (dividend & ~(dividend - divisor)) >>> (Integer.SIZE - 1)\n+  \/\/ remainder = dividend - (((dividend & ~(dividend - divisor)) >> (Integer.SIZE - 1)) & divisor)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in\n+  \/\/ java.lang.Long.divideUnsigned() and java.lang.Long.remainderUnsigned()\n+  movl(rdx, rax);\n+  subl(rax, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnl(rax, rax, rdx);\n+  } else {\n+    notl(rax);\n+    andl(rax, rdx);\n+  }\n+  movl(tmp, rax);\n+  shrl(rax, 31); \/\/ quotient\n+  sarl(tmp, 31);\n+  andl(tmp, divisor);\n+  subl(rdx, tmp); \/\/ remainder\n+  bind(done);\n+}\n+\n+#ifdef _LP64\n+void C2_MacroAssembler::udivL(Register rax, Register divisor, Register rdx) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpq(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorl(rdx, rdx);\n+  divq(divisor);\n+  jmpb(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath for divisor < 0:\n+  \/\/ quotient = (dividend & ~(dividend - divisor)) >>> (Long.SIZE - 1)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in java.lang.Long.divideUnsigned()\n+  movq(rdx, rax);\n+  subq(rdx, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnq(rax, rdx, rax);\n+  } else {\n+    notq(rdx);\n+    andq(rax, rdx);\n+  }\n+  shrq(rax, 63);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::umodL(Register rax, Register divisor, Register rdx) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpq(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorq(rdx, rdx);\n+  divq(divisor);\n+  jmp(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath when divisor < 0:\n+  \/\/ remainder = dividend - (((dividend & ~(dividend - divisor)) >> (Long.SIZE - 1)) & divisor)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in java.lang.Long.remainderUnsigned()\n+  movq(rdx, rax);\n+  subq(rax, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnq(rax, rax, rdx);\n+  } else {\n+    notq(rax);\n+    andq(rax, rdx);\n+  }\n+  sarq(rax, 63);\n+  andq(rax, divisor);\n+  subq(rdx, rax);\n+  bind(done);\n+}\n+\n+void C2_MacroAssembler::udivmodL(Register rax, Register divisor, Register rdx, Register tmp) {\n+  Label done;\n+  Label neg_divisor_fastpath;\n+  cmpq(divisor, 0);\n+  jccb(Assembler::less, neg_divisor_fastpath);\n+  xorq(rdx, rdx);\n+  divq(divisor);\n+  jmp(done);\n+  bind(neg_divisor_fastpath);\n+  \/\/ Fastpath for divisor < 0:\n+  \/\/ quotient = (dividend & ~(dividend - divisor)) >>> (Long.SIZE - 1)\n+  \/\/ remainder = dividend - (((dividend & ~(dividend - divisor)) >> (Long.SIZE - 1)) & divisor)\n+  \/\/ See Hacker's Delight (2nd ed), section 9.3 which is implemented in\n+  \/\/ java.lang.Long.divideUnsigned() and java.lang.Long.remainderUnsigned()\n+  movq(rdx, rax);\n+  subq(rax, divisor);\n+  if (VM_Version::supports_bmi1()) {\n+    andnq(rax, rax, rdx);\n+  } else {\n+    notq(rax);\n+    andq(rax, rdx);\n+  }\n+  movq(tmp, rax);\n+  shrq(rax, 63); \/\/ quotient\n+  sarq(tmp, 63);\n+  andq(tmp, divisor);\n+  subq(rdx, tmp); \/\/ remainder\n+  bind(done);\n+}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":1212,"deletions":123,"binary":false,"changes":1335,"status":"modified"},{"patch":"@@ -31,1 +31,5 @@\n-  Assembler::AvxVectorLen vector_length_encoding(int vlen_in_bytes);\n+  \/\/ C2 compiled method's prolog code.\n+  void verified_entry(Compile* C, int sp_inc = 0);\n+\n+  void emit_entry_barrier_stub(C2EntryBarrierStub* stub);\n+  static int entry_barrier_stub_size();\n@@ -33,3 +37,1 @@\n-  \/\/ special instructions for EVEX\n-  void setvectmask(Register dst, Register src, KRegister mask);\n-  void restorevectmask(KRegister mask);\n+  Assembler::AvxVectorLen vector_length_encoding(int vlen_in_bytes);\n@@ -38,1 +40,1 @@\n-  \/\/ See full desription in macroAssembler_x86.cpp.\n+  \/\/ See full description in macroAssembler_x86.cpp.\n@@ -95,0 +97,5 @@\n+  void vector_compress_expand(int opcode, XMMRegister dst, XMMRegister src, KRegister mask,\n+                              bool merge, BasicType bt, int vec_enc);\n+\n+  void vector_mask_compress(KRegister dst, KRegister src, Register rtmp1, Register rtmp2, int mask_len);\n+\n@@ -126,2 +133,2 @@\n-  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len);\n-  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, bool merge, int vector_len);\n@@ -139,0 +146,6 @@\n+ \/\/ Covert B2X\n+ void vconvert_b2x(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, int vlen_enc);\n+#ifdef _LP64\n+ void vpbroadcast(BasicType elem_bt, XMMRegister dst, Register src, int vlen_enc);\n+#endif\n+\n@@ -306,0 +319,4 @@\n+  void vector_castF2L_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                           KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                           Register scratch, int vec_enc);\n+\n@@ -310,0 +327,4 @@\n+  void vector_castD2X_evex(BasicType to_elem_bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                           XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                           Register scratch, int vec_enc);\n+\n@@ -313,0 +334,42 @@\n+  void vector_cast_double_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                             KRegister ktmp1, KRegister ktmp2, Register scratch, AddressLiteral double_sign_flip,\n+                                             int vec_enc);\n+\n+  void vector_cast_float_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                            KRegister ktmp1, KRegister ktmp2, Register scratch, AddressLiteral float_sign_flip,\n+                                            int vec_enc);\n+\n+  void vector_cast_float_to_long_special_cases_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                                    XMMRegister xtmp2, KRegister ktmp1, KRegister ktmp2,\n+                                                    Register scratch, AddressLiteral double_sign_flip,\n+                                                    int vec_enc);\n+\n+  void vector_cast_float_special_cases_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4,\n+                                           Register scratch, AddressLiteral float_sign_flip,\n+                                           int vec_enc);\n+\n+#ifdef _LP64\n+  void vector_round_double_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                                KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                                AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n+\n+  void vector_round_float_evex(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                               KRegister ktmp1, KRegister ktmp2, AddressLiteral double_sign_flip,\n+                               AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n+\n+  void vector_round_float_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1, XMMRegister xtmp2,\n+                              XMMRegister xtmp3, XMMRegister xtmp4, AddressLiteral float_sign_flip,\n+                              AddressLiteral new_mxcsr, Register scratch, int vec_enc);\n+#endif\n+\n+  void udivI(Register rax, Register divisor, Register rdx);\n+  void umodI(Register rax, Register divisor, Register rdx);\n+  void udivmodI(Register rax, Register divisor, Register rdx, Register tmp);\n+\n+#ifdef _LP64\n+  void udivL(Register rax, Register divisor, Register rdx);\n+  void umodL(Register rax, Register divisor, Register rdx);\n+  void udivmodL(Register rax, Register divisor, Register rdx, Register tmp);\n+#endif\n+\n@@ -319,0 +382,8 @@\n+  void vector_reverse_bit(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                          XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_reverse_bit_gfni(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp,\n+                               AddressLiteral mask, Register rtmp, int vec_enc);\n+\n+  void vector_reverse_byte(BasicType bt, XMMRegister dst, XMMRegister src, Register rtmp, int vec_enc);\n+\n@@ -320,2 +391,1 @@\n-                           XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                           int vec_enc);\n+                           XMMRegister xtmp2, Register rtmp, int vec_enc);\n@@ -324,2 +394,61 @@\n-                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp,\n-                            int vec_enc);\n+                            XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_popcount_short(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                             XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_popcount_byte(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                            XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_popcount_integral(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_popcount_integral_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                     KRegister mask, bool merge, int vec_enc);\n+\n+  void vbroadcast(BasicType bt, XMMRegister dst, int imm32, Register rtmp, int vec_enc);\n+\n+  void vector_reverse_byte64(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                             XMMRegister xtmp2, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src,\n+                                       XMMRegister xtmp1, XMMRegister xtmp2, XMMRegister xtmp3,\n+                                       KRegister ktmp, Register rtmp, bool merge, int vec_enc);\n+\n+  void vector_count_leading_zeros_byte_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_short_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                            XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_int_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                          XMMRegister xtmp2, XMMRegister xtmp3, int vec_enc);\n+\n+  void vector_count_leading_zeros_long_avx(XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                           XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_count_leading_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                      XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vpadd(BasicType bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vec_enc);\n+\n+  void vpsub(BasicType bt, XMMRegister dst, XMMRegister src1, XMMRegister src2, int vec_enc);\n+\n+  void vector_count_trailing_zeros_evex(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                        XMMRegister xtmp2, XMMRegister xtmp3, XMMRegister xtmp4, KRegister ktmp,\n+                                        Register rtmp, int vec_enc);\n+\n+  void vector_swap_nbits(int nbits, int bitmask, XMMRegister dst, XMMRegister src,\n+                         XMMRegister xtmp1, Register rtmp, int vec_enc);\n+\n+  void vector_count_trailing_zeros_avx(BasicType bt, XMMRegister dst, XMMRegister src, XMMRegister xtmp1,\n+                                       XMMRegister xtmp2, XMMRegister xtmp3, Register rtmp, int vec_enc);\n+\n+  void vector_signum_avx(int opcode, XMMRegister dst, XMMRegister src, XMMRegister zero, XMMRegister one,\n+                         XMMRegister xtmp1, int vec_enc);\n+\n+  void vector_signum_evex(int opcode, XMMRegister dst, XMMRegister src, XMMRegister zero, XMMRegister one,\n+                          KRegister ktmp1, int vec_enc);\n+\n+  void vmovmask(BasicType elem_bt, XMMRegister dst, Address src, XMMRegister mask, int vec_enc);\n+\n+  void vmovmask(BasicType elem_bt, Address dst, XMMRegister src, XMMRegister mask, int vec_enc);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":140,"deletions":11,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,0 +34,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -57,0 +58,3 @@\n+  if (is_heap_frame()) {\n+    return true;\n+  }\n@@ -105,1 +109,1 @@\n-    } else if (is_optimized_entry_frame()) {\n+    } else if (is_upcall_stub_frame()) {\n@@ -152,0 +156,6 @@\n+    if (Continuation::is_return_barrier_entry(sender_pc)) {\n+      \/\/ If our sender_pc is the return barrier, then our \"real\" sender is the continuation entry\n+      frame s = Continuation::continuation_bottom_sender(thread, *this, sender_sp);\n+      sender_sp = s.sp();\n+      sender_pc = s.pc();\n+    }\n@@ -207,1 +217,1 @@\n-    } else if (sender_blob->is_optimized_entry_blob()) {\n+    } else if (sender_blob->is_upcall_stub()) {\n@@ -267,0 +277,1 @@\n+\n@@ -273,1 +284,5 @@\n-  assert(_pc == *pc_addr || pc == *pc_addr, \"must be\");\n+\n+  assert(!Continuation::is_return_barrier_entry(*pc_addr), \"return barrier\");\n+\n+  assert(_pc == *pc_addr || pc == *pc_addr || *pc_addr == 0, \"\");\n+  DEBUG_ONLY(address old_pc = _pc;)\n@@ -275,0 +290,1 @@\n+  _pc = pc; \/\/ must be set before call to get_deopt_original_pc\n@@ -277,1 +293,1 @@\n-    assert(original_pc == _pc, \"expected original PC to be stored before patching\");\n+    assert(original_pc == old_pc, \"expected original PC to be stored before patching\");\n@@ -279,1 +295,1 @@\n-    \/\/ leave _pc as is\n+    _pc = original_pc;\n@@ -282,2 +298,1 @@\n-    _pc = pc;\n-}\n+  assert(!is_compiled_frame() || !_cb->as_compiled_method()->is_deopt_entry(_pc), \"must be\");\n@@ -286,7 +301,9 @@\n-bool frame::is_interpreted_frame() const  {\n-  return Interpreter::contains(pc());\n-}\n-\n-int frame::frame_size(RegisterMap* map) const {\n-  frame sender = this->sender(map);\n-  return sender.sp() - sp();\n+#ifdef ASSERT\n+  {\n+    frame f(this->sp(), this->unextended_sp(), this->fp(), pc);\n+    assert(f.is_deoptimized_frame() == this->is_deoptimized_frame() && f.pc() == this->pc() && f.raw_pc() == this->raw_pc(),\n+      \"must be (f.is_deoptimized_frame(): %d this->is_deoptimized_frame(): %d \"\n+      \"f.pc(): \" INTPTR_FORMAT \" this->pc(): \" INTPTR_FORMAT \" f.raw_pc(): \" INTPTR_FORMAT \" this->raw_pc(): \" INTPTR_FORMAT \")\",\n+      f.is_deoptimized_frame(), this->is_deoptimized_frame(), p2i(f.pc()), p2i(this->pc()), p2i(f.raw_pc()), p2i(this->raw_pc()));\n+  }\n+#endif\n@@ -322,1 +339,1 @@\n-  BasicObjectLock* result = (BasicObjectLock*) *addr_at(interpreter_frame_monitor_block_top_offset);\n+  BasicObjectLock* result = (BasicObjectLock*) at(interpreter_frame_monitor_block_top_offset);\n@@ -325,1 +342,1 @@\n-  assert((intptr_t*) result < fp(),  \"monitor end should be strictly below the frame pointer\");\n+  assert((intptr_t*) result < fp(),  \"monitor end should be strictly below the frame pointer: result: \" INTPTR_FORMAT \" fp: \" INTPTR_FORMAT, p2i(result), p2i(fp()));\n@@ -347,4 +364,1 @@\n-  if (!jfa->walkable()) {\n-    \/\/ Capture _last_Java_pc (if needed) and mark anchor walkable.\n-    jfa->capture_last_Java_pc();\n-  }\n+  jfa->make_walkable();\n@@ -353,1 +367,0 @@\n-  vmassert(jfa->last_Java_pc() != NULL, \"not walkable\");\n@@ -359,2 +372,2 @@\n-OptimizedEntryBlob::FrameData* OptimizedEntryBlob::frame_data_for_frame(const frame& frame) const {\n-  assert(frame.is_optimized_entry_frame(), \"wrong frame\");\n+UpcallStub::FrameData* UpcallStub::frame_data_for_frame(const frame& frame) const {\n+  assert(frame.is_upcall_stub_frame(), \"wrong frame\");\n@@ -362,2 +375,2 @@\n-  return reinterpret_cast<OptimizedEntryBlob::FrameData*>(\n-    reinterpret_cast<char*>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n+  return reinterpret_cast<UpcallStub::FrameData*>(\n+    reinterpret_cast<address>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n@@ -366,3 +379,3 @@\n-bool frame::optimized_entry_frame_is_first() const {\n-  assert(is_optimized_entry_frame(), \"must be optimzed entry frame\");\n-  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+bool frame::upcall_stub_frame_is_first() const {\n+  assert(is_upcall_stub_frame(), \"must be optimzed entry frame\");\n+  UpcallStub* blob = _cb->as_upcall_stub();\n@@ -373,1 +386,1 @@\n-frame frame::sender_for_optimized_entry_frame(RegisterMap* map) const {\n+frame frame::sender_for_upcall_stub_frame(RegisterMap* map) const {\n@@ -375,1 +388,1 @@\n-  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+  UpcallStub* blob = _cb->as_upcall_stub();\n@@ -379,1 +392,1 @@\n-  assert(!optimized_entry_frame_is_first(), \"must have a frame anchor to go back to\");\n+  assert(!upcall_stub_frame_is_first(), \"must have a frame anchor to go back to\");\n@@ -383,4 +396,1 @@\n-  if (!jfa->walkable()) {\n-    \/\/ Capture _last_Java_pc (if needed) and mark anchor walkable.\n-    jfa->capture_last_Java_pc();\n-  }\n+  jfa->make_walkable();\n@@ -389,1 +399,0 @@\n-  vmassert(jfa->last_Java_pc() != NULL, \"not walkable\");\n@@ -411,1 +420,1 @@\n-         \"original PC must be in the main code section of the the compiled method (or must be immediately following it)\");\n+         \"original PC must be in the main code section of the compiled method (or must be immediately following it) original_pc: \" INTPTR_FORMAT \" unextended_sp: \" INTPTR_FORMAT \" name: %s\", p2i(original_pc), p2i(unextended_sp), nm->name());\n@@ -436,24 +445,0 @@\n-\/\/------------------------------------------------------------------------------\n-\/\/ frame::update_map_with_saved_link\n-void frame::update_map_with_saved_link(RegisterMap* map, intptr_t** link_addr) {\n-  \/\/ The interpreter and compiler(s) always save EBP\/RBP in a known\n-  \/\/ location on entry. We must record where that location is\n-  \/\/ so this if EBP\/RBP was live on callout from c2 we can find\n-  \/\/ the saved copy no matter what it called.\n-\n-  \/\/ Since the interpreter always saves EBP\/RBP if we record where it is then\n-  \/\/ we don't have to always save EBP\/RBP on entry and exit to c2 compiled\n-  \/\/ code, on entry will be enough.\n-  map->set_location(rbp->as_VMReg(), (address) link_addr);\n-#ifdef AMD64\n-  \/\/ this is weird \"H\" ought to be at a higher address however the\n-  \/\/ oopMaps seems to have the \"H\" regs at the same address and the\n-  \/\/ vanilla register.\n-  \/\/ XXXX make this go away\n-  if (true) {\n-    map->set_location(rbp->as_VMReg()->next(), (address) link_addr);\n-  }\n-#endif \/\/ AMD64\n-}\n-\n-\n@@ -469,0 +454,1 @@\n+  intptr_t* sender_fp = link();\n@@ -476,16 +462,1 @@\n-  return frame(sender_sp, unextended_sp, link(), sender_pc());\n-}\n-\n-\n-\/\/------------------------------------------------------------------------------\n-\/\/ frame::sender_for_compiled_frame\n-frame frame::sender_for_compiled_frame(RegisterMap* map) const {\n-  assert(map != NULL, \"map must be set\");\n-\n-  \/\/ frame owned by optimizing compiler\n-  assert(_cb->frame_size() >= 0, \"must have non-zero frame size\");\n-  intptr_t* sender_sp = unextended_sp() + _cb->frame_size();\n-\n-#ifdef ASSERT\n-  address sender_pc_copy = (address) *(sender_sp-1);\n-#endif\n+  address sender_pc = this->sender_pc();\n@@ -493,46 +464,5 @@\n-  \/\/ This is the saved value of EBP which may or may not really be an FP.\n-  \/\/ It is only an FP if the sender is an interpreter frame (or C1?).\n-  intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n-\n-  \/\/ Repair the sender sp if the frame has been extended\n-  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n-\n-  \/\/ On Intel the return_address is always the word on the stack\n-  address sender_pc = (address) *(sender_sp-1);\n-\n-#ifdef ASSERT\n-  if (sender_pc != sender_pc_copy) {\n-    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n-    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n-    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n-    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n-    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n-  }\n-#endif\n-\n-  if (map->update_map()) {\n-    \/\/ Tell GC to use argument oopmaps for some runtime stubs that need it.\n-    \/\/ For C1, the runtime stub might not have oop maps, so set this flag\n-    \/\/ outside of update_register_map.\n-    bool caller_args = _cb->caller_must_gc_arguments(map->thread());\n-#ifdef COMPILER1\n-    if (!caller_args) {\n-      nmethod* nm = _cb->as_nmethod_or_null();\n-      if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n-          pc() < nm->verified_inline_entry_point()) {\n-        \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n-        \/\/ before doing any argument shuffling, so we need to scan the oops\n-        \/\/ as the caller passes them.\n-        caller_args = true;\n-#ifdef ASSERT\n-        NativeCall* call = nativeCall_before(pc());\n-        address dest = call->destination();\n-        assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n-               dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n-#endif\n-      }\n-    }\n-#endif\n-    map->set_include_argument_oops(caller_args);\n-    if (_cb->oop_maps() != NULL) {\n-      OopMapSet::update_register_map(this, map);\n+  if (Continuation::is_return_barrier_entry(sender_pc)) {\n+    if (map->walk_cont()) { \/\/ about to walk into an h-stack\n+      return Continuation::top_frame(*this, map);\n+    } else {\n+      return Continuation::continuation_bottom_sender(map->thread(), *this, sender_sp);\n@@ -540,37 +470,0 @@\n-\n-    \/\/ Since the prolog does the save and restore of EBP there is no oopmap\n-    \/\/ for it so we must fill in its location as if there was an oopmap entry\n-    \/\/ since if our caller was compiled code there could be live jvm state in it.\n-    update_map_with_saved_link(map, saved_fp_addr);\n-  }\n-\n-  assert(sender_sp != sp(), \"must have changed\");\n-  return frame(sender_sp, sender_sp, *saved_fp_addr, sender_pc);\n-}\n-\n-\n-\/\/------------------------------------------------------------------------------\n-\/\/ frame::sender_raw\n-frame frame::sender_raw(RegisterMap* map) const {\n-  \/\/ Default is we don't have to follow them. The sender_for_xxx will\n-  \/\/ update it accordingly\n-  map->set_include_argument_oops(false);\n-\n-  if (is_entry_frame())        return sender_for_entry_frame(map);\n-  if (is_optimized_entry_frame()) return sender_for_optimized_entry_frame(map);\n-  if (is_interpreted_frame())  return sender_for_interpreter_frame(map);\n-  assert(_cb == CodeCache::find_blob(pc()),\"Must be the same\");\n-\n-  if (_cb != NULL) {\n-    return sender_for_compiled_frame(map);\n-  }\n-  \/\/ Must be native-compiled frame, i.e. the marshaling code for native\n-  \/\/ methods that exists in the core system.\n-  return frame(sender_sp(), link(), sender_pc());\n-}\n-\n-frame frame::sender(RegisterMap* map) const {\n-  frame result = sender_raw(map);\n-\n-  if (map->process_frames()) {\n-    StackWatermarkSet::on_iteration(map->thread(), result);\n@@ -579,1 +472,1 @@\n-  return result;\n+  return frame(sender_sp, unextended_sp, sender_fp, sender_pc);\n@@ -701,1 +594,0 @@\n-\n@@ -710,1 +602,1 @@\n-  values.describe(frame_no, fp() + frame::name##_offset, #name)\n+  values.describe(frame_no, fp() + frame::name##_offset, #name, 1)\n@@ -733,0 +625,16 @@\n+\n+  if (is_java_frame() || Continuation::is_continuation_enterSpecial(*this)) {\n+    intptr_t* ret_pc_loc;\n+    intptr_t* fp_loc;\n+    if (is_interpreted_frame()) {\n+      ret_pc_loc = fp() + return_addr_offset;\n+      fp_loc = fp();\n+    } else {\n+      ret_pc_loc = real_fp() - return_addr_offset;\n+      fp_loc = real_fp() - sender_sp_offset;\n+    }\n+    address ret_pc = *(address*)ret_pc_loc;\n+    values.describe(frame_no, ret_pc_loc,\n+      Continuation::is_return_barrier_entry(ret_pc) ? \"return address (return barrier)\" : \"return address\");\n+    values.describe(-1, fp_loc, \"saved fp\", 0); \/\/ \"unowned\" as value belongs to sender\n+  }\n@@ -734,0 +642,1 @@\n+\n@@ -741,13 +650,0 @@\n-intptr_t* frame::real_fp() const {\n-  if (_cb != NULL) {\n-    \/\/ use the frame size if valid\n-    int size = _cb->frame_size();\n-    if (size > 0) {\n-      return unextended_sp() + size;\n-    }\n-  }\n-  \/\/ else rely on fp()\n-  assert(! is_compiled_frame(), \"unknown compiled frame size\");\n-  return fp();\n-}\n-\n@@ -777,1 +673,1 @@\n-void JavaFrameAnchor::make_walkable(JavaThread* thread) {\n+void JavaFrameAnchor::make_walkable() {\n@@ -782,9 +678,1 @@\n-  vmassert(Thread::current() == (Thread*)thread, \"not current thread\");\n-  vmassert(last_Java_sp() != NULL, \"not called from Java code?\");\n-  capture_last_Java_pc();\n-  vmassert(walkable(), \"something went wrong\");\n-}\n-\n-void JavaFrameAnchor::capture_last_Java_pc() {\n-  vmassert(_last_Java_sp != NULL, \"no last frame set\");\n-  vmassert(_last_Java_pc == NULL, \"already walkable\");\n+  vmassert(walkable(), \"something went wrong\");\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.cpp","additions":75,"deletions":187,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -89,1 +89,1 @@\n-    arg_reg_save_area_bytes                          = 32 \/\/ Register argument save area\n+    arg_reg_save_area_bytes                          = 32, \/\/ Register argument save area\n@@ -94,1 +94,1 @@\n-    arg_reg_save_area_bytes                          =  0\n+    arg_reg_save_area_bytes                          =  0,\n@@ -97,1 +97,1 @@\n-    entry_frame_call_wrapper_offset                  =  2\n+    entry_frame_call_wrapper_offset                  =  2,\n@@ -99,0 +99,7 @@\n+\n+    \/\/ size, in words, of frame metadata (e.g. pc and link)\n+    metadata_words                                   = sender_sp_offset,\n+    \/\/ compiled frame alignment, in bytes\n+    frame_alignment                                  = 16,\n+    \/\/ size, in words, of maximum shift in frame position due to alignment\n+    align_wiggle                                     =  1\n@@ -111,1 +118,4 @@\n-  intptr_t*   _fp; \/\/ frame pointer\n+  union {\n+    intptr_t*  _fp; \/\/ frame pointer\n+    int _offset_fp; \/\/ relative frame pointer for use in stack-chunk frames\n+  };\n@@ -119,1 +129,5 @@\n-  intptr_t*     _unextended_sp;\n+  union {\n+    intptr_t* _unextended_sp;\n+    int _offset_unextended_sp; \/\/ for use in stack-chunk frames\n+  };\n+\n@@ -134,0 +148,2 @@\n+  const ImmutableOopMap* get_oop_map() const;\n+\n@@ -141,0 +157,4 @@\n+  frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc, CodeBlob* cb);\n+  \/\/ used for heap frame construction by continuations\n+  frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc, CodeBlob* cb, const ImmutableOopMap* oop_map, bool relative);\n+\n@@ -144,0 +164,1 @@\n+  void setup(address pc);\n@@ -147,1 +168,4 @@\n-  intptr_t*   fp() const { return _fp; }\n+  intptr_t* fp() const          { assert_absolute(); return _fp; }\n+  void set_fp(intptr_t* newfp)  { _fp = newfp; }\n+  int offset_fp() const         { assert_offset();  return _offset_fp; }\n+  void set_offset_fp(int value) { assert_on_heap(); _offset_fp = value; }\n@@ -154,2 +178,2 @@\n-  \/\/ helper to update a map with callee-saved RBP\n-  static void update_map_with_saved_link(RegisterMap* map, intptr_t** link_addr);\n+  template <typename RegisterMapT>\n+  static void update_map_with_saved_link(RegisterMapT* map, intptr_t** link_addr);\n@@ -163,1 +187,1 @@\n-  frame sender_raw(RegisterMap* map) const;\n+  inline frame sender_raw(RegisterMap* map) const;\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.hpp","additions":34,"deletions":10,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,2 @@\n-#include \"code\/codeCache.hpp\"\n+#include \"code\/codeBlob.inline.hpp\"\n+#include \"code\/codeCache.inline.hpp\"\n@@ -30,0 +31,4 @@\n+#include \"compiler\/oopMap.inline.hpp\"\n+#include \"interpreter\/interpreter.hpp\"\n+#include \"interpreter\/oopMapCache.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -31,0 +36,3 @@\n+#ifdef COMPILER1\n+#include \"c1\/c1_Runtime1.hpp\"\n+#endif\n@@ -43,0 +51,3 @@\n+  _oop_map = NULL;\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n@@ -50,0 +61,4 @@\n+  _oop_map = NULL;\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n+\n@@ -51,1 +66,5 @@\n-  _cb = CodeCache::find_blob(pc);\n+  _cb = CodeCache::find_blob(pc); \/\/ not fast because this constructor can be used on native frames\n+  setup(pc);\n+}\n+\n+inline void frame::setup(address pc) {\n@@ -58,0 +77,2 @@\n+    assert(_cb == NULL || _cb->as_compiled_method()->insts_contains_inclusive(_pc),\n+           \"original PC must be in the main code section of the compiled method (or must be immediately following it)\");\n@@ -59,1 +80,5 @@\n-    _deopt_state = not_deoptimized;\n+    if (_cb == SharedRuntime::deopt_blob()) {\n+      _deopt_state = is_deoptimized;\n+    } else {\n+      _deopt_state = not_deoptimized;\n+    }\n@@ -67,1 +92,1 @@\n-inline frame::frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc) {\n+inline frame::frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc, CodeBlob* cb) {\n@@ -73,2 +98,5 @@\n-  _cb = CodeCache::find_blob(pc);\n-  adjust_unextended_sp();\n+  _cb = cb;\n+  _oop_map = NULL;\n+  assert(_cb != NULL, \"pc: \" INTPTR_FORMAT, p2i(pc));\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n@@ -76,12 +104,19 @@\n-  address original_pc = CompiledMethod::get_deopt_original_pc(this);\n-  if (original_pc != NULL) {\n-    _pc = original_pc;\n-    assert(_cb->as_compiled_method()->insts_contains_inclusive(_pc),\n-           \"original PC must be in the main code section of the the compiled method (or must be immediately following it)\");\n-    _deopt_state = is_deoptimized;\n-  } else {\n-    if (_cb->is_deoptimization_stub()) {\n-      _deopt_state = is_deoptimized;\n-    } else {\n-      _deopt_state = not_deoptimized;\n-    }\n+  setup(pc);\n+}\n+\n+inline frame::frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc, CodeBlob* cb,\n+                    const ImmutableOopMap* oop_map, bool on_heap) {\n+  _sp = sp;\n+  _unextended_sp = unextended_sp;\n+  _fp = fp;\n+  _pc = pc;\n+  _cb = cb;\n+  _oop_map = oop_map;\n+  _deopt_state = not_deoptimized;\n+  _on_heap = on_heap;\n+  DEBUG_ONLY(_frame_index = -1;)\n+\n+  \/\/ In thaw, non-heap frames use this constructor to pass oop_map.  I don't know why.\n+  assert(_on_heap || _cb != nullptr, \"these frames are always heap frames\");\n+  if (cb != NULL) {\n+    setup(pc);\n@@ -89,0 +124,22 @@\n+#ifdef ASSERT\n+  \/\/ The following assertion has been disabled because it would sometime trap for Continuation.run,\n+  \/\/ which is not *in* a continuation and therefore does not clear the _cont_fastpath flag, but this\n+  \/\/ is benign even in fast mode (see Freeze::setup_jump)\n+  \/\/ We might freeze deoptimized frame in slow mode\n+  \/\/ assert(_pc == pc && _deopt_state == not_deoptimized, \"\");\n+#endif\n+}\n+\n+inline frame::frame(intptr_t* sp, intptr_t* unextended_sp, intptr_t* fp, address pc) {\n+  _sp = sp;\n+  _unextended_sp = unextended_sp;\n+  _fp = fp;\n+  _pc = pc;\n+  assert(pc != NULL, \"no pc?\");\n+  _cb = CodeCache::find_blob_fast(pc);\n+  _oop_map = NULL;\n+  assert(_cb != NULL, \"pc: \" INTPTR_FORMAT \" sp: \" INTPTR_FORMAT \" unextended_sp: \" INTPTR_FORMAT \" fp: \" INTPTR_FORMAT, p2i(pc), p2i(sp), p2i(unextended_sp), p2i(fp));\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n+\n+  setup(pc);\n@@ -91,0 +148,2 @@\n+inline frame::frame(intptr_t* sp) : frame(sp, sp, *(intptr_t**)(sp - frame::sender_sp_offset), *(address*)(sp - 1)) {}\n+\n@@ -96,0 +155,2 @@\n+  _on_heap = false;\n+  DEBUG_ONLY(_frame_index = -1;)\n@@ -119,0 +180,1 @@\n+  _oop_map = NULL;\n@@ -141,1 +203,1 @@\n-inline intptr_t* frame::link() const              { return (intptr_t*) *(intptr_t **)addr_at(link_offset); }\n+inline intptr_t* frame::link() const              { return *(intptr_t **)addr_at(link_offset); }\n@@ -148,1 +210,35 @@\n-inline intptr_t* frame::unextended_sp() const     { return _unextended_sp; }\n+inline intptr_t* frame::unextended_sp() const          { assert_absolute(); return _unextended_sp; }\n+inline void frame::set_unextended_sp(intptr_t* value)  { _unextended_sp = value; }\n+inline int  frame::offset_unextended_sp() const        { assert_offset();   return _offset_unextended_sp; }\n+inline void frame::set_offset_unextended_sp(int value) { assert_on_heap();  _offset_unextended_sp = value; }\n+\n+inline intptr_t* frame::real_fp() const {\n+  if (_cb != NULL) {\n+    \/\/ use the frame size if valid\n+    int size = _cb->frame_size();\n+    if (size > 0) {\n+      return unextended_sp() + size;\n+    }\n+  }\n+  \/\/ else rely on fp()\n+  assert(! is_compiled_frame(), \"unknown compiled frame size\");\n+  return fp();\n+}\n+\n+inline int frame::frame_size() const {\n+  return is_interpreted_frame()\n+    ? sender_sp() - sp()\n+    : cb()->frame_size();\n+}\n+\n+inline int frame::compiled_frame_stack_argsize() const {\n+  assert(cb()->is_compiled(), \"\");\n+  return (cb()->as_compiled_method()->method()->num_stack_arg_slots() * VMRegImpl::stack_slot_size) >> LogBytesPerWord;\n+}\n+\n+inline void frame::interpreted_frame_oop_map(InterpreterOopMap* mask) const {\n+  assert(mask != NULL, \"\");\n+  Method* m = interpreter_frame_method();\n+  int   bci = interpreter_frame_bci();\n+  m->mask_for(bci, mask); \/\/ OopMapCache::compute_one_oop_map(m, bci, mask);\n+}\n@@ -152,1 +248,1 @@\n-inline address* frame::sender_pc_addr()      const { return (address*) addr_at( return_addr_offset); }\n+inline address* frame::sender_pc_addr()      const { return (address*) addr_at(return_addr_offset); }\n@@ -155,1 +251,1 @@\n-inline intptr_t*    frame::sender_sp()        const { return            addr_at(   sender_sp_offset); }\n+inline intptr_t* frame::sender_sp()          const { return            addr_at(sender_sp_offset); }\n@@ -162,1 +258,1 @@\n-  return *(intptr_t**)addr_at(interpreter_frame_last_sp_offset);\n+  return (intptr_t*)at(interpreter_frame_last_sp_offset);\n@@ -169,1 +265,0 @@\n-\n@@ -238,1 +333,1 @@\n-  oop* result_adr = (oop *)map->location(rax->as_VMReg());\n+  oop* result_adr = (oop *)map->location(rax->as_VMReg(), sp());\n@@ -240,2 +335,1 @@\n-\n-  return (*result_adr);\n+  return *result_adr;\n@@ -245,1 +339,1 @@\n-  oop* result_adr = (oop *)map->location(rax->as_VMReg());\n+  oop* result_adr = (oop *)map->location(rax->as_VMReg(), sp());\n@@ -252,0 +346,163 @@\n+inline bool frame::is_interpreted_frame() const {\n+  return Interpreter::contains(pc());\n+}\n+\n+inline int frame::sender_sp_ret_address_offset() {\n+  return frame::sender_sp_offset - frame::return_addr_offset;\n+}\n+\n+inline const ImmutableOopMap* frame::get_oop_map() const {\n+  if (_cb == NULL) return NULL;\n+  if (_cb->oop_maps() != NULL) {\n+    NativePostCallNop* nop = nativePostCallNop_at(_pc);\n+    if (nop != NULL && nop->displacement() != 0) {\n+      int slot = ((nop->displacement() >> 24) & 0xff);\n+      return _cb->oop_map_for_slot(slot, _pc);\n+    }\n+    const ImmutableOopMap* oop_map = OopMapSet::find_map(this);\n+    return oop_map;\n+  }\n+  return NULL;\n+}\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::sender\n+\n+inline frame frame::sender(RegisterMap* map) const {\n+  frame result = sender_raw(map);\n+\n+  if (map->process_frames() && !map->in_cont()) {\n+    StackWatermarkSet::on_iteration(map->thread(), result);\n+  }\n+\n+  return result;\n+}\n+\n+inline frame frame::sender_raw(RegisterMap* map) const {\n+  \/\/ Default is we done have to follow them. The sender_for_xxx will\n+  \/\/ update it accordingly\n+  map->set_include_argument_oops(false);\n+\n+  if (map->in_cont()) { \/\/ already in an h-stack\n+    return map->stack_chunk()->sender(*this, map);\n+  }\n+\n+  if (is_entry_frame())       return sender_for_entry_frame(map);\n+  if (is_upcall_stub_frame()) return sender_for_upcall_stub_frame(map);\n+  if (is_interpreted_frame()) return sender_for_interpreter_frame(map);\n+\n+  assert(_cb == CodeCache::find_blob(pc()), \"Must be the same\");\n+  if (_cb != NULL) return sender_for_compiled_frame(map);\n+\n+  \/\/ Must be native-compiled frame, i.e. the marshaling code for native\n+  \/\/ methods that exists in the core system.\n+  return frame(sender_sp(), link(), sender_pc());\n+}\n+\n+inline frame frame::sender_for_compiled_frame(RegisterMap* map) const {\n+  assert(map != NULL, \"map must be set\");\n+\n+  \/\/ frame owned by optimizing compiler\n+  assert(_cb->frame_size() >= 0, \"must have non-zero frame size\");\n+  intptr_t* sender_sp = unextended_sp() + _cb->frame_size();\n+  assert(sender_sp == real_fp(), \"\");\n+\n+#ifdef ASSERT\n+  address sender_pc_copy = (address) *(sender_sp-1);\n+#endif\n+\n+  \/\/ This is the saved value of EBP which may or may not really be an FP.\n+  \/\/ It is only an FP if the sender is an interpreter frame (or C1?).\n+  \/\/ saved_fp_addr should be correct even for a bottom thawed frame (with a return barrier)\n+  intptr_t** saved_fp_addr = (intptr_t**) (sender_sp - frame::sender_sp_offset);\n+\n+  \/\/ Repair the sender sp if the frame has been extended\n+  sender_sp = repair_sender_sp(sender_sp, saved_fp_addr);\n+\n+  \/\/ On Intel the return_address is always the word on the stack\n+  address sender_pc = (address) *(sender_sp-1);\n+\n+#ifdef ASSERT\n+  if (sender_pc != sender_pc_copy) {\n+    \/\/ When extending the stack in the callee method entry to make room for unpacking of value\n+    \/\/ type args, we keep a copy of the sender pc at the expected location in the callee frame.\n+    \/\/ If the sender pc is patched due to deoptimization, the copy is not consistent anymore.\n+    nmethod* nm = CodeCache::find_blob(sender_pc)->as_nmethod();\n+    assert(sender_pc == nm->deopt_mh_handler_begin() || sender_pc == nm->deopt_handler_begin(), \"unexpected sender pc\");\n+  }\n+#endif\n+\n+  if (map->update_map()) {\n+    \/\/ Tell GC to use argument oopmaps for some runtime stubs that need it.\n+    \/\/ For C1, the runtime stub might not have oop maps, so set this flag\n+    \/\/ outside of update_register_map.\n+    bool c1_buffering = false;\n+#ifdef COMPILER1\n+    nmethod* nm = _cb->as_nmethod_or_null();\n+    if (nm != NULL && nm->is_compiled_by_c1() && nm->method()->has_scalarized_args() &&\n+        pc() < nm->verified_inline_entry_point()) {\n+      \/\/ The VEP and VIEP(RO) of C1-compiled methods call buffer_inline_args_xxx\n+      \/\/ before doing any argument shuffling, so we need to scan the oops\n+      \/\/ as the caller passes them.\n+      c1_buffering = true;\n+#ifdef ASSERT\n+      NativeCall* call = nativeCall_before(pc());\n+      address dest = call->destination();\n+      assert(dest == Runtime1::entry_for(Runtime1::buffer_inline_args_no_receiver_id) ||\n+             dest == Runtime1::entry_for(Runtime1::buffer_inline_args_id), \"unexpected safepoint in entry point\");\n+#endif\n+    }\n+#endif\n+    if (!_cb->is_compiled() || c1_buffering) { \/\/ compiled frames do not use callee-saved registers\n+      bool caller_args = _cb->caller_must_gc_arguments(map->thread()) || c1_buffering;\n+      map->set_include_argument_oops(caller_args);\n+      if (oop_map() != NULL) {\n+        _oop_map->update_register_map(this, map);\n+      }\n+    } else {\n+      assert(!_cb->caller_must_gc_arguments(map->thread()), \"\");\n+      assert(!map->include_argument_oops(), \"\");\n+      assert(oop_map() == NULL || !oop_map()->has_any(OopMapValue::callee_saved_value), \"callee-saved value in compiled frame\");\n+    }\n+\n+    \/\/ Since the prolog does the save and restore of EBP there is no oopmap\n+    \/\/ for it so we must fill in its location as if there was an oopmap entry\n+    \/\/ since if our caller was compiled code there could be live jvm state in it.\n+    update_map_with_saved_link(map, saved_fp_addr);\n+  }\n+\n+  assert(sender_sp != sp(), \"must have changed\");\n+\n+  if (Continuation::is_return_barrier_entry(sender_pc)) {\n+    if (map->walk_cont()) { \/\/ about to walk into an h-stack\n+      return Continuation::top_frame(*this, map);\n+    } else {\n+      return Continuation::continuation_bottom_sender(map->thread(), *this, sender_sp);\n+    }\n+  }\n+\n+  intptr_t* unextended_sp = sender_sp;\n+  return frame(sender_sp, unextended_sp, *saved_fp_addr, sender_pc);\n+}\n+\n+template <typename RegisterMapT>\n+void frame::update_map_with_saved_link(RegisterMapT* map, intptr_t** link_addr) {\n+  \/\/ The interpreter and compiler(s) always save EBP\/RBP in a known\n+  \/\/ location on entry. We must record where that location is\n+  \/\/ so this if EBP\/RBP was live on callout from c2 we can find\n+  \/\/ the saved copy no matter what it called.\n+\n+  \/\/ Since the interpreter always saves EBP\/RBP if we record where it is then\n+  \/\/ we don't have to always save EBP\/RBP on entry and exit to c2 compiled\n+  \/\/ code, on entry will be enough.\n+  map->set_location(rbp->as_VMReg(), (address) link_addr);\n+#ifdef AMD64\n+  \/\/ this is weird \"H\" ought to be at a higher address however the\n+  \/\/ oopMaps seems to have the \"H\" regs at the same address and the\n+  \/\/ vanilla register.\n+  \/\/ XXXX make this go away\n+  if (true) {\n+    map->set_location(rbp->as_VMReg()->next(), (address) link_addr);\n+  }\n+#endif \/\/ AMD64\n+}\n","filename":"src\/hotspot\/cpu\/x86\/frame_x86.inline.hpp","additions":286,"deletions":29,"binary":false,"changes":315,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -128,2 +128,18 @@\n-    const Register thread = NOT_LP64(tmp_thread) LP64_ONLY(r15_thread);\n-    NOT_LP64(__ get_thread(thread));\n+    Register thread = NOT_LP64(tmp_thread) LP64_ONLY(r15_thread);\n+\n+#ifndef _LP64\n+    \/\/ Work around the x86_32 bug that only manifests with Loom for some reason.\n+    \/\/ MacroAssembler::resolve_weak_handle calls this barrier with tmp_thread == noreg.\n+    if (thread == noreg) {\n+      if (dst != rcx && tmp1 != rcx) {\n+        thread = rcx;\n+      } else if (dst != rdx && tmp1 != rdx) {\n+        thread = rdx;\n+      } else if (dst != rdi && tmp1 != rdi) {\n+        thread = rdi;\n+      }\n+    }\n+    assert_different_registers(dst, tmp1, thread);\n+    __ push(thread);\n+    __ get_thread(thread);\n+#endif\n@@ -140,0 +156,4 @@\n+\n+#ifndef _LP64\n+    __ pop(thread);\n+#endif\n@@ -206,13 +226,19 @@\n-  \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n-  \/\/ types. Save all argument registers before calling into the runtime.\n-  \/\/ TODO: use push_set() (see JDK-8283327 push\/pop_call_clobbered_registers & aarch64 )\n-  __ pusha();\n-  __ subptr(rsp, 64);\n-  __ movdbl(Address(rsp, 0),  j_farg0);\n-  __ movdbl(Address(rsp, 8),  j_farg1);\n-  __ movdbl(Address(rsp, 16), j_farg2);\n-  __ movdbl(Address(rsp, 24), j_farg3);\n-  __ movdbl(Address(rsp, 32), j_farg4);\n-  __ movdbl(Address(rsp, 40), j_farg5);\n-  __ movdbl(Address(rsp, 48), j_farg6);\n-  __ movdbl(Address(rsp, 56), j_farg7);\n+\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Barriers might be emitted when converting between (scalarized) calling conventions for inline\n+    \/\/ types. Save all argument registers before calling into the runtime.\n+    \/\/ TODO: use push_set() (see JDK-8283327 push\/pop_call_clobbered_registers & aarch64 )\n+    __ pusha();\n+    __ subptr(rsp, 64);\n+    __ movdbl(Address(rsp, 0),  j_farg0);\n+    __ movdbl(Address(rsp, 8),  j_farg1);\n+    __ movdbl(Address(rsp, 16), j_farg2);\n+    __ movdbl(Address(rsp, 24), j_farg3);\n+    __ movdbl(Address(rsp, 32), j_farg4);\n+    __ movdbl(Address(rsp, 40), j_farg5);\n+    __ movdbl(Address(rsp, 48), j_farg6);\n+    __ movdbl(Address(rsp, 56), j_farg7);\n+  } else {\n+    \/\/ Determine and save the live input values\n+    __ push_call_clobbered_registers();\n+  }\n@@ -250,11 +276,15 @@\n-  \/\/ Restore registers\n-  __ movdbl(j_farg0, Address(rsp, 0));\n-  __ movdbl(j_farg1, Address(rsp, 8));\n-  __ movdbl(j_farg2, Address(rsp, 16));\n-  __ movdbl(j_farg3, Address(rsp, 24));\n-  __ movdbl(j_farg4, Address(rsp, 32));\n-  __ movdbl(j_farg5, Address(rsp, 40));\n-  __ movdbl(j_farg6, Address(rsp, 48));\n-  __ movdbl(j_farg7, Address(rsp, 56));\n-  __ addptr(rsp, 64);\n-  __ popa();\n+  if (EnableValhalla && InlineTypePassFieldsAsArgs) {\n+    \/\/ Restore registers\n+    __ movdbl(j_farg0, Address(rsp, 0));\n+    __ movdbl(j_farg1, Address(rsp, 8));\n+    __ movdbl(j_farg2, Address(rsp, 16));\n+    __ movdbl(j_farg3, Address(rsp, 24));\n+    __ movdbl(j_farg4, Address(rsp, 32));\n+    __ movdbl(j_farg5, Address(rsp, 40));\n+    __ movdbl(j_farg6, Address(rsp, 48));\n+    __ movdbl(j_farg7, Address(rsp, 56));\n+    __ addptr(rsp, 64);\n+    __ popa();\n+  } else {\n+    __ pop_call_clobbered_registers();\n+  }\n@@ -499,9 +529,0 @@\n-  \/\/ Is marking still active?\n-  if (in_bytes(SATBMarkQueue::byte_width_of_active()) == 4) {\n-    __ cmpl(queue_active, 0);\n-  } else {\n-    assert(in_bytes(SATBMarkQueue::byte_width_of_active()) == 1, \"Assumption\");\n-    __ cmpb(queue_active, 0);\n-  }\n-  __ jcc(Assembler::equal, done);\n-\n","filename":"src\/hotspot\/cpu\/x86\/gc\/g1\/g1BarrierSetAssembler_x86.cpp","additions":57,"deletions":36,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -38,1 +39,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -262,36 +262,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void BarrierSetAssembler::eden_allocate(MacroAssembler* masm,\n-                                        Register thread, Register obj,\n-                                        Register var_size_in_bytes,\n-                                        int con_size_in_bytes,\n-                                        Register t1,\n-                                        Label& slow_case) {\n-  assert(obj == rax, \"obj must be in rax, for cmpxchg\");\n-  assert_different_registers(obj, var_size_in_bytes, t1);\n-  if (!Universe::heap()->supports_inline_contig_alloc()) {\n-    __ jmp(slow_case);\n-  } else {\n-    Register end = t1;\n-    Label retry;\n-    __ bind(retry);\n-    ExternalAddress heap_top((address) Universe::heap()->top_addr());\n-    __ movptr(obj, heap_top);\n-    if (var_size_in_bytes == noreg) {\n-      __ lea(end, Address(obj, con_size_in_bytes));\n-    } else {\n-      __ lea(end, Address(obj, var_size_in_bytes, Address::times_1));\n-    }\n-    \/\/ if end < obj then we wrapped around => object too long => slow case\n-    __ cmpptr(end, obj);\n-    __ jcc(Assembler::below, slow_case);\n-    __ cmpptr(end, ExternalAddress((address) Universe::heap()->end_addr()));\n-    __ jcc(Assembler::above, slow_case);\n-    \/\/ Compare obj with the top addr, and if still equal, store the new top addr in\n-    \/\/ end at the address of the top addr pointer. Sets ZF if was equal, and clears\n-    \/\/ it otherwise. Use lock prefix for atomicity on MPs.\n-    __ locked_cmpxchgptr(end, heap_top);\n-    __ jcc(Assembler::notEqual, retry);\n-    incr_allocated_bytes(masm, thread, var_size_in_bytes, con_size_in_bytes, thread->is_valid() ? noreg : t1);\n-  }\n-}\n-\n@@ -329,1 +293,1 @@\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation) {\n@@ -334,1 +298,0 @@\n-  Label continuation;\n@@ -337,1 +300,5 @@\n-  __ align(8);\n+  \/\/ The immediate is the last 4 bytes, so if we align the start of the cmp\n+  \/\/ instruction to 4 bytes, we know that the second half of it is also 4\n+  \/\/ byte aligned, which means that the immediate will not cross a cache line\n+  __ align(4);\n+  uintptr_t before_cmp = (uintptr_t)__ pc();\n@@ -339,3 +306,12 @@\n-  __ jcc(Assembler::equal, continuation);\n-  __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));\n-  __ bind(continuation);\n+  uintptr_t after_cmp = (uintptr_t)__ pc();\n+  guarantee(after_cmp - before_cmp == 8, \"Wrong assumed instruction length\");\n+\n+  if (slow_path != NULL) {\n+    __ jcc(Assembler::notEqual, *slow_path);\n+    __ bind(*continuation);\n+  } else {\n+    Label done;\n+    __ jccb(Assembler::equal, done);\n+    __ call(RuntimeAddress(StubRoutines::x86::method_entry_barrier()));\n+    __ bind(done);\n+  }\n@@ -344,1 +320,1 @@\n-void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm, Label*, Label*) {\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.cpp","additions":20,"deletions":44,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -65,6 +65,0 @@\n-  virtual void eden_allocate(MacroAssembler* masm,\n-                             Register thread, Register obj,\n-                             Register var_size_in_bytes,\n-                             int con_size_in_bytes,\n-                             Register t1,\n-                             Label& slow_case);\n@@ -74,1 +68,1 @@\n-  virtual void nmethod_entry_barrier(MacroAssembler* masm);\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm, Label* slow_path, Label* continuation);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shared\/barrierSetAssembler_x86.hpp","additions":1,"deletions":7,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,1 +43,1 @@\n-\/\/ the the vep is aligned at CodeEntryAlignment whereas c2 only aligns\n+\/\/ the vep is aligned at CodeEntryAlignment whereas c2 only aligns\n@@ -66,1 +66,1 @@\n-#define DEFAULT_STACK_SHADOW_PAGES (NOT_WIN64(20) WIN64_ONLY(7) DEBUG_ONLY(+2))\n+#define DEFAULT_STACK_SHADOW_PAGES (NOT_WIN64(20) WIN64_ONLY(8) DEBUG_ONLY(+4))\n@@ -69,1 +69,1 @@\n-#define MIN_STACK_SHADOW_PAGES (NOT_WIN64(10) WIN64_ONLY(7) DEBUG_ONLY(+2))\n+#define MIN_STACK_SHADOW_PAGES (NOT_WIN64(10) WIN64_ONLY(8) DEBUG_ONLY(+4))\n@@ -80,0 +80,6 @@\n+#ifdef _LP64\n+define_pd_global(bool, VMContinuations, true);\n+#else\n+define_pd_global(bool, VMContinuations, false);\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/globals_x86.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,0 +40,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -42,1 +43,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -185,1 +185,1 @@\n-      \/\/ begining of the ProfileData we intend to update to check its\n+      \/\/ beginning of the ProfileData we intend to update to check its\n@@ -976,1 +976,1 @@\n-\/\/ Unlock any Java monitors from syncronized blocks.\n+\/\/ Unlock any Java monitors from synchronized blocks.\n@@ -1216,0 +1216,1 @@\n+  pop_cont_fastpath();\n@@ -1355,1 +1356,1 @@\n-    Label done;\n+    Label count_locking, done, slow_case;\n@@ -1367,2 +1368,0 @@\n-    Label slow_case;\n-\n@@ -1397,1 +1396,1 @@\n-    jcc(Assembler::zero, done);\n+    jcc(Assembler::zero, count_locking);\n@@ -1433,1 +1432,5 @@\n-    jcc(Assembler::zero, done);\n+    jcc(Assembler::notZero, slow_case);\n+\n+    bind(count_locking);\n+    inc_held_monitor_count();\n+    jmp(done);\n@@ -1466,1 +1469,1 @@\n-    Label done;\n+    Label count_locking, done, slow_case;\n@@ -1492,1 +1495,1 @@\n-    jcc(Assembler::zero, done);\n+    jcc(Assembler::zero, count_locking);\n@@ -1499,1 +1502,1 @@\n-    jcc(Assembler::zero, done);\n+    jcc(Assembler::notZero, slow_case);\n@@ -1501,0 +1504,3 @@\n+    bind(count_locking);\n+    dec_held_monitor_count();\n+    jmp(done);\n@@ -1502,0 +1508,1 @@\n+    bind(slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":19,"deletions":12,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-\/\/ This file specializes the assember with interpreter-specific macros\n+\/\/ This file specializes the assembler with interpreter-specific macros\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"c1\/c1_FrameMap.hpp\"\n@@ -45,0 +44,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -47,0 +47,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -55,1 +56,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -513,3 +513,1 @@\n-  {\n-    call(RuntimeAddress(entry_point));\n-  }\n+  call(RuntimeAddress(entry_point));\n@@ -520,3 +518,1 @@\n-  {\n-    call(RuntimeAddress(entry_point));\n-  }\n+  call(RuntimeAddress(entry_point));\n@@ -922,1 +918,1 @@\n-void MacroAssembler::long_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -933,2 +929,3 @@\n-      assert(dst.is_single_reg(), \"not a stack pair\");\n-      movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+      assert(dst.is_single_reg(), \"not a stack pair: (%s, %s), (%s, %s)\",\n+             src.first()->name(), src.second()->name(), dst.first()->name(), dst.second()->name());\n+      movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_Register());\n@@ -938,1 +935,1 @@\n-    movq(dst.first()->as_Register(), Address(rbp, reg2offset_out(src.first())));\n+    movq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -941,2 +938,2 @@\n-    movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    movq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -947,1 +944,1 @@\n-void MacroAssembler::double_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -960,1 +957,1 @@\n-      movdbl(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+      movdbl(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_XMMRegister());\n@@ -964,1 +961,1 @@\n-    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_out(src.first())));\n+    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -967,2 +964,2 @@\n-    movq(rax, Address(rbp, reg2offset_in(src.first())));\n-    movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+    movq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -974,1 +971,1 @@\n-void MacroAssembler::float_move(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -982,2 +979,2 @@\n-      movl(rax, Address(rbp, reg2offset_in(src.first())));\n-      movptr(Address(rsp, reg2offset_out(dst.first())), rax);\n+      movl(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+      movptr(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -987,1 +984,1 @@\n-      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first())));\n+      movflt(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -992,1 +989,1 @@\n-    movflt(Address(rsp, reg2offset_out(dst.first())), src.first()->as_XMMRegister());\n+    movflt(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_XMMRegister());\n@@ -1006,1 +1003,1 @@\n-void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst) {\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst, Register tmp, int in_stk_bias, int out_stk_bias) {\n@@ -1010,2 +1007,2 @@\n-      movslq(rax, Address(rbp, reg2offset_in(src.first())));\n-      movq(Address(rsp, reg2offset_out(dst.first())), rax);\n+      movslq(tmp, Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n+      movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), tmp);\n@@ -1014,1 +1011,1 @@\n-      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n+      movslq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first()) + in_stk_bias));\n@@ -1020,1 +1017,1 @@\n-    movq(Address(rsp, reg2offset_out(dst.first())), src.first()->as_Register());\n+    movq(Address(rsp, reg2offset_out(dst.first()) + out_stk_bias), src.first()->as_Register());\n@@ -1080,1 +1077,1 @@\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ Oop is in a register we must store it to the space we reserve\n@@ -1188,0 +1185,20 @@\n+void MacroAssembler::push_f(XMMRegister r) {\n+  subptr(rsp, wordSize);\n+  movflt(Address(rsp, 0), r);\n+}\n+\n+void MacroAssembler::pop_f(XMMRegister r) {\n+  movflt(r, Address(rsp, 0));\n+  addptr(rsp, wordSize);\n+}\n+\n+void MacroAssembler::push_d(XMMRegister r) {\n+  subptr(rsp, 2 * wordSize);\n+  movdbl(Address(rsp, 0), r);\n+}\n+\n+void MacroAssembler::pop_d(XMMRegister r) {\n+  movdbl(r, Address(rsp, 0));\n+  addptr(rsp, 2 * Interpreter::stackElementSize);\n+}\n+\n@@ -1320,0 +1337,7 @@\n+void MacroAssembler::emit_static_call_stub() {\n+  \/\/ Static stub relocation also tags the Method* in the code-stream.\n+  mov_metadata(rbx, (Metadata*) NULL);  \/\/ Method is zapped till fixup time.\n+  \/\/ This is recognized as unresolved by relocs\/nativeinst\/ic code.\n+  jump(RuntimeAddress(pc()));\n+}\n+\n@@ -1595,1 +1619,1 @@\n-  \/\/ stack pointer as the user finsihed with it. This allows\n+  \/\/ stack pointer as the user finished with it. This allows\n@@ -1645,0 +1669,14 @@\n+void MacroAssembler::call_VM_leaf(address entry_point, Register arg_0, Register arg_1, Register arg_2, Register arg_3) {\n+  LP64_ONLY(assert(arg_0 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_1 != c_rarg3, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_2 != c_rarg3, \"smashed arg\"));\n+  pass_arg3(this, arg_3);\n+  LP64_ONLY(assert(arg_0 != c_rarg2, \"smashed arg\"));\n+  LP64_ONLY(assert(arg_1 != c_rarg2, \"smashed arg\"));\n+  pass_arg2(this, arg_2);\n+  LP64_ONLY(assert(arg_0 != c_rarg1, \"smashed arg\"));\n+  pass_arg1(this, arg_1);\n+  pass_arg0(this, arg_0);\n+  call_VM_leaf(entry_point, 3);\n+}\n+\n@@ -1929,1 +1967,1 @@\n-  assert (shift_value > 0, \"illegal shift value\");\n+  assert(shift_value > 0, \"illegal shift value\");\n@@ -1968,0 +2006,13 @@\n+void MacroAssembler::post_call_nop() {\n+  if (!Continuations::enabled()) {\n+    return;\n+  }\n+  InstructionMark im(this);\n+  relocate(post_call_nop_Relocation::spec());\n+  emit_int8((int8_t)0x0f);\n+  emit_int8((int8_t)0x1f);\n+  emit_int8((int8_t)0x84);\n+  emit_int8((int8_t)0x00);\n+  emit_int32(0x00);\n+}\n+\n@@ -1973,5 +2024,5 @@\n-    emit_int8(0x26); \/\/ es:\n-    emit_int8(0x2e); \/\/ cs:\n-    emit_int8(0x64); \/\/ fs:\n-    emit_int8(0x65); \/\/ gs:\n-    emit_int8((unsigned char)0x90);\n+    emit_int8((int8_t)0x26); \/\/ es:\n+    emit_int8((int8_t)0x2e); \/\/ cs:\n+    emit_int8((int8_t)0x64); \/\/ fs:\n+    emit_int8((int8_t)0x65); \/\/ gs:\n+    emit_int8((int8_t)0x90);\n@@ -2266,1 +2317,1 @@\n-void MacroAssembler::ldmxcsr(AddressLiteral src) {\n+void MacroAssembler::ldmxcsr(AddressLiteral src, Register scratchReg) {\n@@ -2270,2 +2321,2 @@\n-    lea(rscratch1, src);\n-    Assembler::ldmxcsr(Address(rscratch1, 0));\n+    lea(scratchReg, src);\n+    Assembler::ldmxcsr(Address(scratchReg, 0));\n@@ -2546,2 +2597,3 @@\n-  assert(vector_len <= AVX_256bit, \"AVX2 vector length\");\n-  if (vector_len == AVX_256bit) {\n+  if (vector_len == AVX_512bit) {\n+    evmovdquq(dst, src, AVX_512bit, scratch_reg);\n+  } else if (vector_len == AVX_256bit) {\n@@ -2620,5 +2672,1 @@\n-    if (mask == k0) {\n-      Assembler::evmovdqub(dst, as_Address(src), merge, vector_len);\n-    } else {\n-      Assembler::evmovdqub(dst, mask, as_Address(src), merge, vector_len);\n-    }\n+    Assembler::evmovdqub(dst, mask, as_Address(src), merge, vector_len);\n@@ -2627,5 +2675,1 @@\n-    if (mask == k0) {\n-      Assembler::evmovdqub(dst, Address(scratch_reg, 0), merge, vector_len);\n-    } else {\n-      Assembler::evmovdqub(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n-    }\n+    Assembler::evmovdqub(dst, mask, Address(scratch_reg, 0), merge, vector_len);\n@@ -2949,0 +2993,103 @@\n+void MacroAssembler::push_cont_fastpath() {\n+  if (!Continuations::enabled()) return;\n+\n+#ifndef _LP64\n+  Register rthread = rax;\n+  Register rrealsp = rbx;\n+  push(rthread);\n+  push(rrealsp);\n+\n+  get_thread(rthread);\n+\n+  \/\/ The code below wants the original RSP.\n+  \/\/ Move it back after the pushes above.\n+  movptr(rrealsp, rsp);\n+  addptr(rrealsp, 2*wordSize);\n+#else\n+  Register rthread = r15_thread;\n+  Register rrealsp = rsp;\n+#endif\n+\n+  Label done;\n+  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::belowEqual, done);\n+  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), rrealsp);\n+  bind(done);\n+\n+#ifndef _LP64\n+  pop(rrealsp);\n+  pop(rthread);\n+#endif\n+}\n+\n+void MacroAssembler::pop_cont_fastpath() {\n+  if (!Continuations::enabled()) return;\n+\n+#ifndef _LP64\n+  Register rthread = rax;\n+  Register rrealsp = rbx;\n+  push(rthread);\n+  push(rrealsp);\n+\n+  get_thread(rthread);\n+\n+  \/\/ The code below wants the original RSP.\n+  \/\/ Move it back after the pushes above.\n+  movptr(rrealsp, rsp);\n+  addptr(rrealsp, 2*wordSize);\n+#else\n+  Register rthread = r15_thread;\n+  Register rrealsp = rsp;\n+#endif\n+\n+  Label done;\n+  cmpptr(rrealsp, Address(rthread, JavaThread::cont_fastpath_offset()));\n+  jccb(Assembler::below, done);\n+  movptr(Address(rthread, JavaThread::cont_fastpath_offset()), 0);\n+  bind(done);\n+\n+#ifndef _LP64\n+  pop(rrealsp);\n+  pop(rthread);\n+#endif\n+}\n+\n+void MacroAssembler::inc_held_monitor_count() {\n+#ifndef _LP64\n+  Register thread = rax;\n+  push(thread);\n+  get_thread(thread);\n+  incrementl(Address(thread, JavaThread::held_monitor_count_offset()));\n+  pop(thread);\n+#else \/\/ LP64\n+  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+}\n+\n+void MacroAssembler::dec_held_monitor_count() {\n+#ifndef _LP64\n+  Register thread = rax;\n+  push(thread);\n+  get_thread(thread);\n+  decrementl(Address(thread, JavaThread::held_monitor_count_offset()));\n+  pop(thread);\n+#else \/\/ LP64\n+  decrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+#endif\n+}\n+\n+#ifdef ASSERT\n+void MacroAssembler::stop_if_in_cont(Register cont, const char* name) {\n+#ifdef _LP64\n+  Label no_cont;\n+  movptr(cont, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  testl(cont, cont);\n+  jcc(Assembler::zero, no_cont);\n+  stop(name);\n+  bind(no_cont);\n+#else\n+  Unimplemented();\n+#endif\n+}\n+#endif\n+\n@@ -3299,0 +3446,9 @@\n+void MacroAssembler::vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch) {\n+  if (reachable(src)) {\n+    Assembler::vpbroadcastq(dst, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vpbroadcastq(dst, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n@@ -3730,1 +3886,1 @@\n-  assert(new_obj == rax, \"needs to be rax, according to barrier asm eden_allocate\");\n+  assert(new_obj == rax, \"needs to be rax\");\n@@ -3752,2 +3908,0 @@\n-  const bool allow_shared_alloc =\n-    Universe::heap()->supports_inline_contig_alloc();\n@@ -3758,1 +3912,1 @@\n-  if (UseTLAB || allow_shared_alloc) {\n+  if (UseTLAB) {\n@@ -3773,3 +3927,1 @@\n-    \/\/ Allocation in the shared Eden, if allowed.\n-    \/\/\n-    eden_allocate(thread, new_obj, layout_size, 0, t2, slow_case);\n+    jmp(slow_case);\n@@ -3778,3 +3930,3 @@\n-  \/\/ If UseTLAB or allow_shared_alloc are true, the object is created above and\n-  \/\/ there is an initialize need. Otherwise, skip and go to the slow path.\n-  if (UseTLAB || allow_shared_alloc) {\n+  \/\/ If UseTLAB is true, the object is created above and there is an initialize need.\n+  \/\/ Otherwise, skip and go to the slow path.\n+  if (UseTLAB) {\n@@ -3866,0 +4018,1 @@\n+  int num_xmm_registers = XMMRegisterImpl::available_xmm_registers();\n@@ -3868,2 +4021,2 @@\n-  if (FrameMap::get_num_caller_save_xmms() > 16) {\n-     result += XMMRegSet::range(xmm16, as_XMMRegister(FrameMap::get_num_caller_save_xmms() - 1));\n+  if (num_xmm_registers > 16) {\n+     result += XMMRegSet::range(xmm16, as_XMMRegister(num_xmm_registers - 1));\n@@ -3873,1 +4026,1 @@\n-  return XMMRegSet::range(xmm0, as_XMMRegister(FrameMap::get_num_caller_save_xmms() - 1));\n+  return XMMRegSet::range(xmm0, as_XMMRegister(num_xmm_registers - 1));\n@@ -4035,10 +4188,0 @@\n-\/\/ Defines obj, preserves var_size_in_bytes\n-void MacroAssembler::eden_allocate(Register thread, Register obj,\n-                                   Register var_size_in_bytes,\n-                                   int con_size_in_bytes,\n-                                   Register t1,\n-                                   Label& slow_case) {\n-  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-  bs->eden_allocate(this, thread, obj, var_size_in_bytes, con_size_in_bytes, t1, slow_case);\n-}\n-\n@@ -4391,1 +4534,1 @@\n-  if (super_klass != rax || UseCompressedOops) {\n+  if (super_klass != rax) {\n@@ -5001,10 +5144,0 @@\n-  \/\/ Reset k1 to 0xffff.\n-\n-#ifdef COMPILER2\n-  if (PostLoopMultiversioning && VM_Version::supports_evex()) {\n-    push(rcx);\n-    movl(rcx, 0xffff);\n-    kmovwl(k1, rcx);\n-    pop(rcx);\n-  }\n-#endif \/\/ COMPILER2\n@@ -5174,1 +5307,1 @@\n-\/\/ Doesn't do verfication, generates fixed size code\n+\/\/ Doesn't do verification, generates fixed size code\n@@ -5506,96 +5639,0 @@\n-#ifdef COMPILER2\n-\/\/ C2 compiled method's prolog code.\n-void MacroAssembler::verified_entry(Compile* C, int sp_inc) {\n-  int framesize = C->output()->frame_size_in_bytes();\n-  int bangsize = C->output()->bang_size_in_bytes();\n-  bool fp_mode_24b = false;\n-  int stack_bang_size = C->output()->need_stack_bang(bangsize) ? bangsize : 0;\n-\n-  \/\/ WARNING: Initial instruction MUST be 5 bytes or longer so that\n-  \/\/ NativeJump::patch_verified_entry will be able to patch out the entry\n-  \/\/ code safely. The push to verify stack depth is ok at 5 bytes,\n-  \/\/ the frame allocation can be either 3 or 6 bytes. So if we don't do\n-  \/\/ stack bang then we must use the 6 byte frame allocation even if\n-  \/\/ we have no frame. :-(\n-  assert(stack_bang_size >= framesize || stack_bang_size <= 0, \"stack bang size incorrect\");\n-\n-  assert((framesize & (StackAlignmentInBytes-1)) == 0, \"frame size not aligned\");\n-  \/\/ Remove word for return addr\n-  framesize -= wordSize;\n-  stack_bang_size -= wordSize;\n-\n-  \/\/ Calls to C2R adapters often do not accept exceptional returns.\n-  \/\/ We require that their callers must bang for them.  But be careful, because\n-  \/\/ some VM calls (such as call site linkage) can use several kilobytes of\n-  \/\/ stack.  But the stack safety zone should account for that.\n-  \/\/ See bugs 4446381, 4468289, 4497237.\n-  if (stack_bang_size > 0) {\n-    generate_stack_overflow_check(stack_bang_size);\n-\n-    \/\/ We always push rbp, so that on return to interpreter rbp, will be\n-    \/\/ restored correctly and we can correct the stack.\n-    push(rbp);\n-    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n-    if (PreserveFramePointer) {\n-      mov(rbp, rsp);\n-    }\n-    \/\/ Remove word for ebp\n-    framesize -= wordSize;\n-\n-    \/\/ Create frame\n-    if (framesize) {\n-      subptr(rsp, framesize);\n-    }\n-  } else {\n-    \/\/ Create frame (force generation of a 4 byte immediate value)\n-    subptr_imm32(rsp, framesize);\n-\n-    \/\/ Save RBP register now.\n-    framesize -= wordSize;\n-    movptr(Address(rsp, framesize), rbp);\n-    \/\/ Save caller's stack pointer into RBP if the frame pointer is preserved.\n-    if (PreserveFramePointer) {\n-      movptr(rbp, rsp);\n-      if (framesize > 0) {\n-        addptr(rbp, framesize);\n-      }\n-    }\n-  }\n-\n-  if (C->needs_stack_repair()) {\n-    \/\/ Save stack increment just below the saved rbp (also account for fixed framesize and rbp)\n-    assert((sp_inc & (StackAlignmentInBytes-1)) == 0, \"stack increment not aligned\");\n-    movptr(Address(rsp, framesize - wordSize), sp_inc + framesize + wordSize);\n-  }\n-\n-  if (VerifyStackAtCalls) { \/\/ Majik cookie to verify stack depth\n-    framesize -= wordSize;\n-    movptr(Address(rsp, framesize), (int32_t)0xbadb100d);\n-  }\n-\n-#ifndef _LP64\n-  \/\/ If method sets FPU control word do it now\n-  if (fp_mode_24b) {\n-    fldcw(ExternalAddress(StubRoutines::x86::addr_fpu_cntrl_wrd_24()));\n-  }\n-  if (UseSSE >= 2 && VerifyFPU) {\n-    verify_FPU(0, \"FPU stack must be clean on entry\");\n-  }\n-#endif\n-\n-#ifdef ASSERT\n-  if (VerifyStackAtCalls) {\n-    Label L;\n-    push(rax);\n-    mov(rax, rsp);\n-    andptr(rax, StackAlignmentInBytes-1);\n-    cmpptr(rax, StackAlignmentInBytes-wordSize);\n-    pop(rax);\n-    jcc(Assembler::equal, L);\n-    STOP(\"Stack is not properly aligned!\");\n-    bind(L);\n-  }\n-#endif\n-}\n-#endif \/\/ COMPILER2\n-\n@@ -5699,1 +5736,1 @@\n-      eden_allocate(r15_thread, rax, noreg, obj_size, r13, slow_case);\n+      jmp(slow_case);\n@@ -5709,1 +5746,1 @@\n-      eden_allocate(r15_thread, rax, r14, 0, r13, slow_case);\n+      jmp(slow_case);\n@@ -5712,1 +5749,1 @@\n-  if (UseTLAB || Universe::heap()->supports_inline_contig_alloc()) {\n+  if (UseTLAB) {\n@@ -6079,0 +6116,2 @@\n+  const int fill64_per_loop = 4;\n+  const int max_unrolled_fill64 = 8;\n@@ -6082,1 +6121,17 @@\n-  for (int i = 0; i < vector64_count; i++) {\n+  int start64 = 0;\n+  if (vector64_count > max_unrolled_fill64) {\n+    Label LOOP;\n+    Register index = rtmp;\n+\n+    start64 = vector64_count - (vector64_count % fill64_per_loop);\n+\n+    movl(index, 0);\n+    BIND(LOOP);\n+    for (int i = 0; i < fill64_per_loop; i++) {\n+      fill64(Address(base, index, Address::times_1, i * 64), xtmp, use64byteVector);\n+    }\n+    addl(index, fill64_per_loop * 64);\n+    cmpl(index, start64 * 64);\n+    jccb(Assembler::less, LOOP);\n+  }\n+  for (int i = start64; i < vector64_count; i++) {\n@@ -6094,1 +6149,1 @@\n-        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_128bit);\n+        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_128bit);\n@@ -6099,1 +6154,1 @@\n-        evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_256bit);\n@@ -6102,1 +6157,1 @@\n-        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+        evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -6108,1 +6163,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -6110,1 +6165,1 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -6118,1 +6173,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -6120,2 +6175,2 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n-          evmovdqu(T_LONG, k0, Address(base, disp + 32), xtmp, Assembler::AVX_128bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp + 32), xtmp, false, Assembler::AVX_128bit);\n@@ -6128,1 +6183,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, Assembler::AVX_512bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp), xtmp, true, Assembler::AVX_512bit);\n@@ -6130,1 +6185,1 @@\n-          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, k0, Address(base, disp), xtmp, false, Assembler::AVX_256bit);\n@@ -6133,1 +6188,1 @@\n-          evmovdqu(T_LONG, mask, Address(base, disp + 32), xtmp, Assembler::AVX_256bit);\n+          evmovdqu(T_LONG, mask, Address(base, disp + 32), xtmp, true, Assembler::AVX_256bit);\n@@ -6906,1 +6961,1 @@\n- * Code for BigInteger::multiplyToLen() instrinsic.\n+ * Code for BigInteger::multiplyToLen() intrinsic.\n@@ -7102,1 +7157,1 @@\n-    evmovdqub(rymm0, Address(obja, result), false, Assembler::AVX_512bit);\n+    evmovdqub(rymm0, Address(obja, result), Assembler::AVX_512bit);\n@@ -7115,1 +7170,1 @@\n-    \/\/ AVX512 code to compare upto 63 byte vectors.\n+    \/\/ AVX512 code to compare up to 63 byte vectors.\n@@ -7396,1 +7451,1 @@\n- * Add 64 bit long carry into z[] with carry propogation.\n+ * Add 64 bit long carry into z[] with carry propagation.\n@@ -7576,1 +7631,1 @@\n-  \/\/ Add 64 bit long carry into z with carry propogation.\n+  \/\/ Add 64 bit long carry into z with carry propagation.\n@@ -8486,1 +8541,1 @@\n-\/\/ Subtract from a lenght of a buffer\n+\/\/ Subtract from a length of a buffer\n@@ -8895,1 +8950,1 @@\n-    \/\/ First check whether a character is compressable ( <= 0xFF).\n+    \/\/ First check whether a character is compressible ( <= 0xFF).\n@@ -8943,1 +8998,1 @@\n-    evmovdquw(tmp1Reg, Address(src, len, Address::times_2), \/*merge*\/ false, Assembler::AVX_512bit);\n+    evmovdquw(tmp1Reg, Address(src, len, Address::times_2), Assembler::AVX_512bit);\n@@ -9113,1 +9168,1 @@\n-    evmovdquw(Address(dst, len, Address::times_2), tmp1, \/*merge*\/ false, Assembler::AVX_512bit);\n+    evmovdquw(Address(dst, len, Address::times_2), tmp1, Assembler::AVX_512bit);\n@@ -9215,1 +9270,1 @@\n-void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len) {\n+void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len) {\n@@ -9219,1 +9274,1 @@\n-      evmovdqub(dst, kmask, src, false, vector_len);\n+      evmovdqub(dst, kmask, src, merge, vector_len);\n@@ -9223,1 +9278,1 @@\n-      evmovdquw(dst, kmask, src, false, vector_len);\n+      evmovdquw(dst, kmask, src, merge, vector_len);\n@@ -9227,1 +9282,1 @@\n-      evmovdqul(dst, kmask, src, false, vector_len);\n+      evmovdqul(dst, kmask, src, merge, vector_len);\n@@ -9231,1 +9286,1 @@\n-      evmovdquq(dst, kmask, src, false, vector_len);\n+      evmovdquq(dst, kmask, src, merge, vector_len);\n@@ -9239,1 +9294,1 @@\n-void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len) {\n+void MacroAssembler::evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, bool merge, int vector_len) {\n@@ -9243,1 +9298,1 @@\n-      evmovdqub(dst, kmask, src, true, vector_len);\n+      evmovdqub(dst, kmask, src, merge, vector_len);\n@@ -9247,1 +9302,1 @@\n-      evmovdquw(dst, kmask, src, true, vector_len);\n+      evmovdquw(dst, kmask, src, merge, vector_len);\n@@ -9251,1 +9306,1 @@\n-      evmovdqul(dst, kmask, src, true, vector_len);\n+      evmovdqul(dst, kmask, src, merge, vector_len);\n@@ -9255,1 +9310,1 @@\n-      evmovdquq(dst, kmask, src, true, vector_len);\n+      evmovdquq(dst, kmask, src, merge, vector_len);\n@@ -9644,1 +9699,1 @@\n-  evmovdqu(bt, mask, dst, xmm, vec_enc);\n+  evmovdqu(bt, mask, dst, xmm, true, vec_enc);\n@@ -9673,1 +9728,1 @@\n-void MacroAssembler::fill32(Register dst, int disp, XMMRegister xmm) {\n+void MacroAssembler::fill32(Address dst, XMMRegister xmm) {\n@@ -9675,1 +9730,1 @@\n-  vmovdqu(Address(dst, disp), xmm);\n+  vmovdqu(dst, xmm);\n@@ -9678,1 +9733,5 @@\n-void MacroAssembler::fill64(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n+void MacroAssembler::fill32(Register dst, int disp, XMMRegister xmm) {\n+  fill32(Address(dst, disp), xmm);\n+}\n+\n+void MacroAssembler::fill64(Address dst, XMMRegister xmm, bool use64byteVector) {\n@@ -9680,3 +9739,2 @@\n-  BasicType type[] = {T_BYTE,  T_SHORT,  T_INT,   T_LONG};\n-    fill32(dst, disp, xmm);\n-    fill32(dst, disp + 32, xmm);\n+    fill32(dst, xmm);\n+    fill32(dst.plus_disp(32), xmm);\n@@ -9685,1 +9743,1 @@\n-    evmovdquq(Address(dst, disp), xmm, Assembler::AVX_512bit);\n+    evmovdquq(dst, xmm, Assembler::AVX_512bit);\n@@ -9689,0 +9747,4 @@\n+void MacroAssembler::fill64(Register dst, int disp, XMMRegister xmm, bool use64byteVector) {\n+  fill64(Address(dst, disp), xmm, use64byteVector);\n+}\n+\n@@ -9769,1 +9831,1 @@\n-      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, Assembler::AVX_256bit);\n+      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, true, Assembler::AVX_256bit);\n@@ -9839,1 +9901,1 @@\n-      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, Assembler::AVX_512bit);\n+      evmovdqu(T_BYTE, k2, Address(to, 0), xtmp, true, Assembler::AVX_512bit);\n@@ -9909,0 +9971,74 @@\n+void MacroAssembler::round_float(Register dst, XMMRegister src, Register rtmp, Register rcx) {\n+  \/\/ Following code is line by line assembly translation rounding algorithm.\n+  \/\/ Please refer to java.lang.Math.round(float) algorithm for details.\n+  const int32_t FloatConsts_EXP_BIT_MASK = 0x7F800000;\n+  const int32_t FloatConsts_SIGNIFICAND_WIDTH = 24;\n+  const int32_t FloatConsts_EXP_BIAS = 127;\n+  const int32_t FloatConsts_SIGNIF_BIT_MASK = 0x007FFFFF;\n+  const int32_t MINUS_32 = 0xFFFFFFE0;\n+  Label L_special_case, L_block1, L_exit;\n+  movl(rtmp, FloatConsts_EXP_BIT_MASK);\n+  movdl(dst, src);\n+  andl(dst, rtmp);\n+  sarl(dst, FloatConsts_SIGNIFICAND_WIDTH - 1);\n+  movl(rtmp, FloatConsts_SIGNIFICAND_WIDTH - 2 + FloatConsts_EXP_BIAS);\n+  subl(rtmp, dst);\n+  movl(rcx, rtmp);\n+  movl(dst, MINUS_32);\n+  testl(rtmp, dst);\n+  jccb(Assembler::notEqual, L_special_case);\n+  movdl(dst, src);\n+  andl(dst, FloatConsts_SIGNIF_BIT_MASK);\n+  orl(dst, FloatConsts_SIGNIF_BIT_MASK + 1);\n+  movdl(rtmp, src);\n+  testl(rtmp, rtmp);\n+  jccb(Assembler::greaterEqual, L_block1);\n+  negl(dst);\n+  bind(L_block1);\n+  sarl(dst);\n+  addl(dst, 0x1);\n+  sarl(dst, 0x1);\n+  jmp(L_exit);\n+  bind(L_special_case);\n+  convert_f2i(dst, src);\n+  bind(L_exit);\n+}\n+\n+void MacroAssembler::round_double(Register dst, XMMRegister src, Register rtmp, Register rcx) {\n+  \/\/ Following code is line by line assembly translation rounding algorithm.\n+  \/\/ Please refer to java.lang.Math.round(double) algorithm for details.\n+  const int64_t DoubleConsts_EXP_BIT_MASK = 0x7FF0000000000000L;\n+  const int64_t DoubleConsts_SIGNIFICAND_WIDTH = 53;\n+  const int64_t DoubleConsts_EXP_BIAS = 1023;\n+  const int64_t DoubleConsts_SIGNIF_BIT_MASK = 0x000FFFFFFFFFFFFFL;\n+  const int64_t MINUS_64 = 0xFFFFFFFFFFFFFFC0L;\n+  Label L_special_case, L_block1, L_exit;\n+  mov64(rtmp, DoubleConsts_EXP_BIT_MASK);\n+  movq(dst, src);\n+  andq(dst, rtmp);\n+  sarq(dst, DoubleConsts_SIGNIFICAND_WIDTH - 1);\n+  mov64(rtmp, DoubleConsts_SIGNIFICAND_WIDTH - 2 + DoubleConsts_EXP_BIAS);\n+  subq(rtmp, dst);\n+  movq(rcx, rtmp);\n+  mov64(dst, MINUS_64);\n+  testq(rtmp, dst);\n+  jccb(Assembler::notEqual, L_special_case);\n+  movq(dst, src);\n+  mov64(rtmp, DoubleConsts_SIGNIF_BIT_MASK);\n+  andq(dst, rtmp);\n+  mov64(rtmp, DoubleConsts_SIGNIF_BIT_MASK + 1);\n+  orq(dst, rtmp);\n+  movq(rtmp, src);\n+  testq(rtmp, rtmp);\n+  jccb(Assembler::greaterEqual, L_block1);\n+  negq(dst);\n+  bind(L_block1);\n+  sarq(dst);\n+  addq(dst, 0x1);\n+  sarq(dst, 0x1);\n+  jmp(L_exit);\n+  bind(L_special_case);\n+  convert_d2l(dst, src);\n+  bind(L_exit);\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":363,"deletions":227,"binary":false,"changes":590,"status":"modified"},{"patch":"@@ -237,0 +237,1 @@\n+  void post_call_nop();\n@@ -251,4 +252,5 @@\n-  void move32_64(VMRegPair src, VMRegPair dst);\n-  void long_move(VMRegPair src, VMRegPair dst);\n-  void float_move(VMRegPair src, VMRegPair dst);\n-  void double_move(VMRegPair src, VMRegPair dst);\n+  \/\/ bias in bytes\n+  void move32_64(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void long_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void float_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n+  void double_move(VMRegPair src, VMRegPair dst, Register tmp = rax, int in_stk_bias = 0, int out_stk_bias = 0);\n@@ -331,0 +333,3 @@\n+  void call_VM_leaf(address entry_point,\n+                    Register arg_1, Register arg_2, Register arg_3, Register arg_4);\n+\n@@ -569,0 +574,8 @@\n+  void push_cont_fastpath();\n+  void pop_cont_fastpath();\n+\n+  void inc_held_monitor_count();\n+  void dec_held_monitor_count();\n+\n+  DEBUG_ONLY(void stop_if_in_cont(Register cont_reg, const char* name);)\n+\n@@ -590,1 +603,1 @@\n-  \/\/ Additonal registers can be excluded in a passed RegSet.\n+  \/\/ Additional registers can be excluded in a passed RegSet.\n@@ -611,8 +624,0 @@\n-  void eden_allocate(\n-    Register thread,                   \/\/ Current thread\n-    Register obj,                      \/\/ result: pointer to object after successful allocation\n-    Register var_size_in_bytes,        \/\/ object size in bytes if unknown at compile time; invalid otherwise\n-    int      con_size_in_bytes,        \/\/ object size in bytes if   known at compile time\n-    Register t1,                       \/\/ temp register\n-    Label&   slow_case                 \/\/ continuation point if fast allocation fails\n-  );\n@@ -761,1 +766,1 @@\n-  Condition negate_condition(Condition cond);\n+  static Condition negate_condition(Condition cond);\n@@ -916,0 +921,2 @@\n+  void emit_static_call_stub();\n+\n@@ -918,1 +925,1 @@\n-  \/\/ NOTE: these jumps tranfer to the effective address of dst NOT\n+  \/\/ NOTE: these jumps transfer to the effective address of dst NOT\n@@ -925,1 +932,1 @@\n-  \/\/ to be installed in the Address class. This jump will tranfers to the address\n+  \/\/ to be installed in the Address class. This jump will transfer to the address\n@@ -931,0 +938,5 @@\n+  void push_f(XMMRegister r);\n+  void pop_f(XMMRegister r);\n+  void push_d(XMMRegister r);\n+  void pop_d(XMMRegister r);\n+\n@@ -969,1 +981,1 @@\n-  void ldmxcsr(AddressLiteral src);\n+  void ldmxcsr(AddressLiteral src, Register scratchReg = rscratch1);\n@@ -1210,6 +1222,10 @@\n-  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src, int vector_len);\n-  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, int vector_len);\n-\n-  void evmovdqub(Address dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, Address src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n-  void evmovdqub(XMMRegister dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqub(dst, src, merge, vector_len); }\n+  void evmovdqu(BasicType type, KRegister kmask, Address dst, XMMRegister src,  bool merge, int vector_len);\n+  void evmovdqu(BasicType type, KRegister kmask, XMMRegister dst, Address src, bool merge, int vector_len);\n+\n+  void evmovdqub(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+  void evmovdqub(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdqub(dst, src, vector_len); }\n+  void evmovdqub(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() != src->encoding() || mask != k0)  {\n+      Assembler::evmovdqub(dst, mask, src, merge, vector_len);\n+    }\n+  }\n@@ -1220,3 +1236,7 @@\n-  void evmovdquw(Address dst, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, src, merge, vector_len); }\n-  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n-  void evmovdquw(XMMRegister dst, Address src, bool merge, int vector_len) { Assembler::evmovdquw(dst, src, merge, vector_len); }\n+  void evmovdquw(XMMRegister dst, Address src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n+  void evmovdquw(Address dst, XMMRegister src, int vector_len) { Assembler::evmovdquw(dst, src, vector_len); }\n+  void evmovdquw(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() != src->encoding() || mask != k0) {\n+      Assembler::evmovdquw(dst, mask, src, merge, vector_len);\n+    }\n+  }\n@@ -1224,0 +1244,1 @@\n+  void evmovdquw(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquw(dst, mask, src, merge, vector_len); }\n@@ -1226,0 +1247,5 @@\n+  void evmovdqul(XMMRegister dst, XMMRegister src, int vector_len) {\n+     if (dst->encoding() != src->encoding()) {\n+       Assembler::evmovdqul(dst, src, vector_len);\n+     }\n+  }\n@@ -1228,3 +1254,5 @@\n-  void evmovdqul(XMMRegister dst, XMMRegister src, int vector_len) {\n-     if (dst->encoding() == src->encoding()) return;\n-     Assembler::evmovdqul(dst, src, vector_len);\n+\n+  void evmovdqul(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n+    if (dst->encoding() != src->encoding() || mask != k0)  {\n+      Assembler::evmovdqul(dst, mask, src, merge, vector_len);\n+    }\n@@ -1232,5 +1260,1 @@\n-  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n-  void evmovdqul(XMMRegister dst, KRegister mask, XMMRegister src, bool merge, int vector_len) {\n-    if (dst->encoding() == src->encoding() && mask == k0) return;\n-    Assembler::evmovdqul(dst, mask, src, merge, vector_len);\n-   }\n+  void evmovdqul(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdqul(dst, mask, src, merge, vector_len); }\n@@ -1240,0 +1264,5 @@\n+  void evmovdquq(XMMRegister dst, XMMRegister src, int vector_len) {\n+    if (dst->encoding() != src->encoding()) {\n+      Assembler::evmovdquq(dst, src, vector_len);\n+    }\n+  }\n@@ -1243,6 +1272,1 @@\n-  void evmovdquq(XMMRegister dst, XMMRegister src, int vector_len) {\n-    if (dst->encoding() == src->encoding()) return;\n-    Assembler::evmovdquq(dst, src, vector_len);\n-  }\n-  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n-  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+\n@@ -1250,2 +1274,3 @@\n-    if (dst->encoding() == src->encoding() && mask == k0) return;\n-    Assembler::evmovdquq(dst, mask, src, merge, vector_len);\n+    if (dst->encoding() != src->encoding() || mask != k0) {\n+      Assembler::evmovdquq(dst, mask, src, merge, vector_len);\n+    }\n@@ -1253,0 +1278,2 @@\n+  void evmovdquq(XMMRegister dst, KRegister mask, Address src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n+  void evmovdquq(Address dst, KRegister mask, XMMRegister src, bool merge, int vector_len) { Assembler::evmovdquq(dst, mask, src, merge, vector_len); }\n@@ -1378,0 +1405,5 @@\n+  void vpbroadcastq(XMMRegister dst, AddressLiteral src, int vector_len, Register rscratch = rscratch1);\n+  void vpbroadcastq(XMMRegister dst, XMMRegister src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n+  void vpbroadcastq(XMMRegister dst, Address src, int vector_len) { Assembler::vpbroadcastq(dst, src, vector_len); }\n+\n+\n@@ -1905,3 +1937,0 @@\n-  \/\/ C2 compiled method's prolog code.\n-  void verified_entry(Compile* C, int sp_inc = 0);\n-\n@@ -2063,0 +2092,2 @@\n+  void fill32(Address dst, XMMRegister xmm);\n+\n@@ -2065,0 +2096,2 @@\n+  void fill64(Address dst, XMMRegister xmm, bool use64byteVector = false);\n+\n@@ -2072,0 +2105,2 @@\n+  void round_double(Register dst, XMMRegister src, Register rtmp, Register rcx);\n+  void round_float(Register dst, XMMRegister src, Register rtmp, Register rcx);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":80,"deletions":45,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -210,0 +210,15 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_foreign_abi_NativeEntryPoint::downcall_stub_address_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ jmp(temp_target);\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -321,1 +336,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n@@ -331,1 +346,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : rcx), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : rcx), \"only valid assignment\");\n@@ -343,4 +358,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -349,1 +361,3 @@\n-\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n@@ -638,1 +652,1 @@\n-  __ mov(rbx, rsp); \/\/ for retreiving saved_regs\n+  __ mov(rbx, rsp); \/\/ for retrieving saved_regs\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":23,"deletions":9,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -980,1 +980,1 @@\n-  \/\/ compiled code, which relys solely on SP and not EBP, get sick).\n+  \/\/ compiled code, which relies solely on SP and not EBP, get sick).\n@@ -1128,1 +1128,1 @@\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ Oop is in a register we must store it to the space we reserve\n@@ -1538,1 +1538,1 @@\n-  bs->nmethod_entry_barrier(masm);\n+  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n@@ -1674,1 +1674,1 @@\n-  \/\/ be pushed on the stack when we do a a stack traversal). It is enough that the pc()\n+  \/\/ be pushed on the stack when we do a stack traversal). It is enough that the pc()\n@@ -1713,0 +1713,1 @@\n+    Label count_mon;\n@@ -1740,1 +1741,1 @@\n-      __ jcc(Assembler::equal, lock_done);\n+      __ jcc(Assembler::equal, count_mon);\n@@ -1760,0 +1761,2 @@\n+    __ bind(count_mon);\n+    __ inc_held_monitor_count();\n@@ -1874,1 +1877,1 @@\n-    Label done;\n+    Label fast_done;\n@@ -1880,0 +1883,1 @@\n+      Label not_recur;\n@@ -1881,2 +1885,4 @@\n-\n-      __ jcc(Assembler::equal, done);\n+      __ jcc(Assembler::notEqual, not_recur);\n+      __ dec_held_monitor_count();\n+      __ jmpb(fast_done);\n+      __ bind(not_recur);\n@@ -1904,0 +1910,1 @@\n+      __ dec_held_monitor_count();\n@@ -1914,2 +1921,1 @@\n-    __ bind(done);\n-\n+    __ bind(fast_done);\n@@ -2129,1 +2135,1 @@\n-  \/\/ address has been pushed on the the stack, and return values are in\n+  \/\/ address has been pushed on the stack, and return values are in\n@@ -2654,1 +2660,1 @@\n-  CodeBuffer   buffer(\"handler_blob\", 1024, 512);\n+  CodeBuffer   buffer(\"handler_blob\", 2048, 1024);\n@@ -2857,1 +2863,1 @@\n-  \/\/ We are back the the original state on entry and ready to go.\n+  \/\/ We are back to the original state on entry and ready to go.\n@@ -2887,10 +2893,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":20,"deletions":24,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"code\/compiledIC.hpp\"\n@@ -48,0 +49,2 @@\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -154,2 +157,2 @@\n-  static OopMap* save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words, bool save_vectors);\n-  static void restore_live_registers(MacroAssembler* masm, bool restore_vectors = false);\n+  static OopMap* save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words, bool save_wide_vectors);\n+  static void restore_live_registers(MacroAssembler* masm, bool restore_wide_vectors = false);\n@@ -176,1 +179,1 @@\n-OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words, bool save_vectors) {\n+OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words, bool save_wide_vectors) {\n@@ -178,4 +181,1 @@\n-  int num_xmm_regs = XMMRegisterImpl::number_of_registers;\n-  if (UseAVX < 3) {\n-    num_xmm_regs = num_xmm_regs\/2;\n-  }\n+  int num_xmm_regs = XMMRegisterImpl::available_xmm_registers();\n@@ -183,2 +183,2 @@\n-  if (save_vectors && UseAVX == 0) {\n-    save_vectors = false; \/\/ vectors larger than 16 byte long are supported only with AVX\n+  if (save_wide_vectors && UseAVX == 0) {\n+    save_wide_vectors = false; \/\/ vectors larger than 16 byte long are supported only with AVX\n@@ -186,1 +186,1 @@\n-  assert(!save_vectors || MaxVectorSize <= 64, \"Only up to 64 byte long vectors are supported\");\n+  assert(!save_wide_vectors || MaxVectorSize <= 64, \"Only up to 64 byte long vectors are supported\");\n@@ -188,1 +188,1 @@\n-  save_vectors = false; \/\/ vectors are generated only by C2 and JVMCI\n+  save_wide_vectors = false; \/\/ vectors are generated only by C2 and JVMCI\n@@ -209,1 +209,1 @@\n-  if (save_vectors) {\n+  if (save_wide_vectors) {\n@@ -238,1 +238,1 @@\n-      \/\/ Save upper bank of ZMM registers(16..31) for double\/float usage\n+      \/\/ Save upper bank of XMM registers(16..31) for scalar or 16-byte vector usage\n@@ -241,0 +241,1 @@\n+      int vector_len = VM_Version::supports_avx512vl() ?  Assembler::AVX_128bit : Assembler::AVX_512bit;\n@@ -242,1 +243,1 @@\n-        __ movsd(Address(rsp, base_addr+(off++*64)), as_XMMRegister(n));\n+        __ evmovdqul(Address(rsp, base_addr+(off++*64)), as_XMMRegister(n), vector_len);\n@@ -306,1 +307,1 @@\n-  if (save_vectors) {\n+  if (save_wide_vectors) {\n@@ -370,5 +371,2 @@\n-void RegisterSaver::restore_live_registers(MacroAssembler* masm, bool restore_vectors) {\n-  int num_xmm_regs = XMMRegisterImpl::number_of_registers;\n-  if (UseAVX < 3) {\n-    num_xmm_regs = num_xmm_regs\/2;\n-  }\n+void RegisterSaver::restore_live_registers(MacroAssembler* masm, bool restore_wide_vectors) {\n+  int num_xmm_regs = XMMRegisterImpl::available_xmm_registers();\n@@ -381,1 +379,1 @@\n-  if (restore_vectors) {\n+  if (restore_wide_vectors) {\n@@ -386,1 +384,1 @@\n-  assert(!restore_vectors, \"vectors are generated only by C2\");\n+  assert(!restore_wide_vectors, \"vectors are generated only by C2\");\n@@ -392,1 +390,1 @@\n-  if (restore_vectors) {\n+  if (restore_wide_vectors) {\n@@ -421,1 +419,1 @@\n-      \/\/ Restore upper bank of ZMM registers(16..31) for double\/float usage\n+      \/\/ Restore upper bank of XMM registers(16..31) for scalar or 16-byte vector usage\n@@ -424,0 +422,1 @@\n+      int vector_len = VM_Version::supports_avx512vl() ?  Assembler::AVX_128bit : Assembler::AVX_512bit;\n@@ -425,1 +424,1 @@\n-        __ movsd(as_XMMRegister(n), Address(rsp, base_addr+(off++*64)));\n+        __ evmovdqul(as_XMMRegister(n), Address(rsp, base_addr+(off++*64)), vector_len);\n@@ -1180,0 +1179,2 @@\n+  __ push_cont_fastpath(); \/\/ Set JavaThread::_cont_fastpath to the sp of the oldest interpreted frame we know about\n+\n@@ -1243,1 +1244,1 @@\n-  \/\/ compiled code, which relys solely on SP and not RBP, get sick).\n+  \/\/ compiled code, which relies solely on SP and not RBP, get sick).\n@@ -1514,174 +1515,0 @@\n-\/\/ Different signatures may require very different orders for the move\n-\/\/ to avoid clobbering other arguments.  There's no simple way to\n-\/\/ order them safely.  Compute a safe order for issuing stores and\n-\/\/ break any cycles in those stores.  This code is fairly general but\n-\/\/ it's not necessary on the other platforms so we keep it in the\n-\/\/ platform dependent code instead of moving it into a shared file.\n-\/\/ (See bugs 7013347 & 7145024.)\n-\/\/ Note that this code is specific to LP64.\n-class ComputeMoveOrder: public StackObj {\n-  class MoveOperation: public ResourceObj {\n-    friend class ComputeMoveOrder;\n-   private:\n-    VMRegPair        _src;\n-    VMRegPair        _dst;\n-    int              _src_index;\n-    int              _dst_index;\n-    bool             _processed;\n-    MoveOperation*  _next;\n-    MoveOperation*  _prev;\n-\n-    static int get_id(VMRegPair r) {\n-      return r.first()->value();\n-    }\n-\n-   public:\n-    MoveOperation(int src_index, VMRegPair src, int dst_index, VMRegPair dst):\n-      _src(src)\n-    , _dst(dst)\n-    , _src_index(src_index)\n-    , _dst_index(dst_index)\n-    , _processed(false)\n-    , _next(NULL)\n-    , _prev(NULL) {\n-    }\n-\n-    VMRegPair src() const              { return _src; }\n-    int src_id() const                 { return get_id(src()); }\n-    int src_index() const              { return _src_index; }\n-    VMRegPair dst() const              { return _dst; }\n-    void set_dst(int i, VMRegPair dst) { _dst_index = i, _dst = dst; }\n-    int dst_index() const              { return _dst_index; }\n-    int dst_id() const                 { return get_id(dst()); }\n-    MoveOperation* next() const       { return _next; }\n-    MoveOperation* prev() const       { return _prev; }\n-    void set_processed()               { _processed = true; }\n-    bool is_processed() const          { return _processed; }\n-\n-    \/\/ insert\n-    void break_cycle(VMRegPair temp_register) {\n-      \/\/ create a new store following the last store\n-      \/\/ to move from the temp_register to the original\n-      MoveOperation* new_store = new MoveOperation(-1, temp_register, dst_index(), dst());\n-\n-      \/\/ break the cycle of links and insert new_store at the end\n-      \/\/ break the reverse link.\n-      MoveOperation* p = prev();\n-      assert(p->next() == this, \"must be\");\n-      _prev = NULL;\n-      p->_next = new_store;\n-      new_store->_prev = p;\n-\n-      \/\/ change the original store to save it's value in the temp.\n-      set_dst(-1, temp_register);\n-    }\n-\n-    void link(GrowableArray<MoveOperation*>& killer) {\n-      \/\/ link this store in front the store that it depends on\n-      MoveOperation* n = killer.at_grow(src_id(), NULL);\n-      if (n != NULL) {\n-        assert(_next == NULL && n->_prev == NULL, \"shouldn't have been set yet\");\n-        _next = n;\n-        n->_prev = this;\n-      }\n-    }\n-  };\n-\n- private:\n-  GrowableArray<MoveOperation*> edges;\n-\n- public:\n-  ComputeMoveOrder(int total_in_args, const VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,\n-                  const BasicType* in_sig_bt, GrowableArray<int>& arg_order, VMRegPair tmp_vmreg) {\n-    \/\/ Move operations where the dest is the stack can all be\n-    \/\/ scheduled first since they can't interfere with the other moves.\n-    for (int i = total_in_args - 1, c_arg = total_c_args - 1; i >= 0; i--, c_arg--) {\n-      if (in_sig_bt[i] == T_ARRAY) {\n-        c_arg--;\n-        if (out_regs[c_arg].first()->is_stack() &&\n-            out_regs[c_arg + 1].first()->is_stack()) {\n-          arg_order.push(i);\n-          arg_order.push(c_arg);\n-        } else {\n-          if (out_regs[c_arg].first()->is_stack() ||\n-              in_regs[i].first() == out_regs[c_arg].first()) {\n-            add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg + 1]);\n-          } else {\n-            add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg]);\n-          }\n-        }\n-      } else if (in_sig_bt[i] == T_VOID) {\n-        arg_order.push(i);\n-        arg_order.push(c_arg);\n-      } else {\n-        if (out_regs[c_arg].first()->is_stack() ||\n-            in_regs[i].first() == out_regs[c_arg].first()) {\n-          arg_order.push(i);\n-          arg_order.push(c_arg);\n-        } else {\n-          add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg]);\n-        }\n-      }\n-    }\n-    \/\/ Break any cycles in the register moves and emit the in the\n-    \/\/ proper order.\n-    GrowableArray<MoveOperation*>* stores = get_store_order(tmp_vmreg);\n-    for (int i = 0; i < stores->length(); i++) {\n-      arg_order.push(stores->at(i)->src_index());\n-      arg_order.push(stores->at(i)->dst_index());\n-    }\n- }\n-\n-  \/\/ Collected all the move operations\n-  void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) {\n-    if (src.first() == dst.first()) return;\n-    edges.append(new MoveOperation(src_index, src, dst_index, dst));\n-  }\n-\n-  \/\/ Walk the edges breaking cycles between moves.  The result list\n-  \/\/ can be walked in order to produce the proper set of loads\n-  GrowableArray<MoveOperation*>* get_store_order(VMRegPair temp_register) {\n-    \/\/ Record which moves kill which values\n-    GrowableArray<MoveOperation*> killer;\n-    for (int i = 0; i < edges.length(); i++) {\n-      MoveOperation* s = edges.at(i);\n-      assert(killer.at_grow(s->dst_id(), NULL) == NULL, \"only one killer\");\n-      killer.at_put_grow(s->dst_id(), s, NULL);\n-    }\n-    assert(killer.at_grow(MoveOperation::get_id(temp_register), NULL) == NULL,\n-           \"make sure temp isn't in the registers that are killed\");\n-\n-    \/\/ create links between loads and stores\n-    for (int i = 0; i < edges.length(); i++) {\n-      edges.at(i)->link(killer);\n-    }\n-\n-    \/\/ at this point, all the move operations are chained together\n-    \/\/ in a doubly linked list.  Processing it backwards finds\n-    \/\/ the beginning of the chain, forwards finds the end.  If there's\n-    \/\/ a cycle it can be broken at any point,  so pick an edge and walk\n-    \/\/ backward until the list ends or we end where we started.\n-    GrowableArray<MoveOperation*>* stores = new GrowableArray<MoveOperation*>();\n-    for (int e = 0; e < edges.length(); e++) {\n-      MoveOperation* s = edges.at(e);\n-      if (!s->is_processed()) {\n-        MoveOperation* start = s;\n-        \/\/ search for the beginning of the chain or cycle\n-        while (start->prev() != NULL && start->prev() != s) {\n-          start = start->prev();\n-        }\n-        if (start->prev() == s) {\n-          start->break_cycle(temp_register);\n-        }\n-        \/\/ walk the chain forward inserting to store list\n-        while (start != NULL) {\n-          stores->append(start);\n-          start->set_processed();\n-          start = start->next();\n-        }\n-      }\n-    }\n-    return stores;\n-  }\n-};\n-\n@@ -1709,0 +1536,188 @@\n+\/\/ defined in stubGenerator_x86_64.cpp\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots);\n+void fill_continuation_entry(MacroAssembler* masm, Register reg_cont_obj, Register reg_flags);\n+void continuation_enter_cleanup(MacroAssembler* masm);\n+\n+static void check_continuation_enter_argument(VMReg actual_vmreg,\n+                                              Register expected_reg,\n+                                              const char* name) {\n+  assert(!actual_vmreg->is_stack(), \"%s cannot be on stack\", name);\n+  assert(actual_vmreg->as_Register() == expected_reg,\n+         \"%s is in unexpected register: %s instead of %s\",\n+         name, actual_vmreg->as_Register()->name(), expected_reg->name());\n+}\n+\n+static void gen_continuation_enter(MacroAssembler* masm,\n+                                 const VMRegPair* regs,\n+                                 int& exception_offset,\n+                                 OopMapSet* oop_maps,\n+                                 int& frame_complete,\n+                                 int& stack_slots,\n+                                 int& interpreted_entry_offset,\n+                                 int& compiled_entry_offset) {\n+\n+  \/\/ enterSpecial(Continuation c, boolean isContinue, boolean isVirtualThread)\n+  int pos_cont_obj   = 0;\n+  int pos_is_cont    = 1;\n+  int pos_is_virtual = 2;\n+\n+  \/\/ The platform-specific calling convention may present the arguments in various registers.\n+  \/\/ To simplify the rest of the code, we expect the arguments to reside at these known\n+  \/\/ registers, and we additionally check the placement here in case calling convention ever\n+  \/\/ changes.\n+  Register reg_cont_obj   = c_rarg1;\n+  Register reg_is_cont    = c_rarg2;\n+  Register reg_is_virtual = c_rarg3;\n+\n+  check_continuation_enter_argument(regs[pos_cont_obj].first(),   reg_cont_obj,   \"Continuation object\");\n+  check_continuation_enter_argument(regs[pos_is_cont].first(),    reg_is_cont,    \"isContinue\");\n+  check_continuation_enter_argument(regs[pos_is_virtual].first(), reg_is_virtual, \"isVirtualThread\");\n+\n+  \/\/ Utility methods kill rax, make sure there are no collisions\n+  assert_different_registers(rax, reg_cont_obj, reg_is_cont, reg_is_virtual);\n+\n+  AddressLiteral resolve(SharedRuntime::get_resolve_static_call_stub(),\n+                         relocInfo::static_call_type);\n+\n+  address start = __ pc();\n+\n+  Label L_thaw, L_exit;\n+\n+  \/\/ i2i entry used at interp_only_mode only\n+  interpreted_entry_offset = __ pc() - start;\n+  {\n+#ifdef ASSERT\n+    Label is_interp_only;\n+    __ cmpb(Address(r15_thread, JavaThread::interp_only_mode_offset()), 0);\n+    __ jcc(Assembler::notEqual, is_interp_only);\n+    __ stop(\"enterSpecial interpreter entry called when not in interp_only_mode\");\n+    __ bind(is_interp_only);\n+#endif\n+\n+    __ pop(rax); \/\/ return address\n+    \/\/ Read interpreter arguments into registers (this is an ad-hoc i2c adapter)\n+    __ movptr(c_rarg1, Address(rsp, Interpreter::stackElementSize*2));\n+    __ movl(c_rarg2,   Address(rsp, Interpreter::stackElementSize*1));\n+    __ movl(c_rarg3,   Address(rsp, Interpreter::stackElementSize*0));\n+    __ andptr(rsp, -16); \/\/ Ensure compiled code always sees stack at proper alignment\n+    __ push(rax); \/\/ return address\n+    __ push_cont_fastpath();\n+\n+    __ enter();\n+\n+    stack_slots = 2; \/\/ will be adjusted in setup\n+    OopMap* map = continuation_enter_setup(masm, stack_slots);\n+    \/\/ The frame is complete here, but we only record it for the compiled entry, so the frame would appear unsafe,\n+    \/\/ but that's okay because at the very worst we'll miss an async sample, but we're in interp_only_mode anyway.\n+\n+    __ verify_oop(reg_cont_obj);\n+\n+    fill_continuation_entry(masm, reg_cont_obj, reg_is_virtual);\n+\n+    \/\/ If continuation, call to thaw. Otherwise, resolve the call and exit.\n+    __ testptr(reg_is_cont, reg_is_cont);\n+    __ jcc(Assembler::notZero, L_thaw);\n+\n+    \/\/ --- Resolve path\n+\n+    \/\/ Make sure the call is patchable\n+    __ align(BytesPerWord, __ offset() + NativeCall::displacement_offset);\n+    \/\/ Emit stub for static call\n+    CodeBuffer* cbuf = masm->code_section()->outer();\n+    address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, __ pc());\n+    if (stub == nullptr) {\n+      fatal(\"CodeCache is full at gen_continuation_enter\");\n+    }\n+    __ call(resolve);\n+    oop_maps->add_gc_map(__ pc() - start, map);\n+    __ post_call_nop();\n+\n+    __ jmp(L_exit);\n+  }\n+\n+  \/\/ compiled entry\n+  __ align(CodeEntryAlignment);\n+  compiled_entry_offset = __ pc() - start;\n+  __ enter();\n+\n+  stack_slots = 2; \/\/ will be adjusted in setup\n+  OopMap* map = continuation_enter_setup(masm, stack_slots);\n+\n+  \/\/ Frame is now completed as far as size and linkage.\n+  frame_complete = __ pc() - start;\n+\n+  __ verify_oop(reg_cont_obj);\n+\n+  fill_continuation_entry(masm, reg_cont_obj, reg_is_virtual);\n+\n+  \/\/ If isContinue, call to thaw. Otherwise, call Continuation.enter(Continuation c, boolean isContinue)\n+  __ testptr(reg_is_cont, reg_is_cont);\n+  __ jccb(Assembler::notZero, L_thaw);\n+\n+  \/\/ --- call Continuation.enter(Continuation c, boolean isContinue)\n+\n+  \/\/ Make sure the call is patchable\n+  __ align(BytesPerWord, __ offset() + NativeCall::displacement_offset);\n+\n+  \/\/ Emit stub for static call\n+  CodeBuffer* cbuf = masm->code_section()->outer();\n+  address stub = CompiledStaticCall::emit_to_interp_stub(*cbuf, __ pc());\n+  if (stub == nullptr) {\n+    fatal(\"CodeCache is full at gen_continuation_enter\");\n+  }\n+\n+  \/\/ The call needs to be resolved. There's a special case for this in\n+  \/\/ SharedRuntime::find_callee_info_helper() which calls\n+  \/\/ LinkResolver::resolve_continuation_enter() which resolves the call to\n+  \/\/ Continuation.enter(Continuation c, boolean isContinue).\n+  __ call(resolve);\n+\n+  oop_maps->add_gc_map(__ pc() - start, map);\n+  __ post_call_nop();\n+\n+  __ jmpb(L_exit);\n+\n+  \/\/ --- Thawing path\n+\n+  __ bind(L_thaw);\n+\n+  __ call(RuntimeAddress(StubRoutines::cont_thaw()));\n+\n+  ContinuationEntry::_return_pc_offset = __ pc() - start;\n+  oop_maps->add_gc_map(__ pc() - start, map->deep_copy());\n+  __ post_call_nop();\n+\n+  \/\/ --- Normal exit (resolve\/thawing)\n+\n+  __ bind(L_exit);\n+\n+  continuation_enter_cleanup(masm);\n+  __ pop(rbp);\n+  __ ret(0);\n+\n+  \/\/ --- Exception handling path\n+\n+  exception_offset = __ pc() - start;\n+\n+  continuation_enter_cleanup(masm);\n+  __ pop(rbp);\n+\n+  __ movptr(c_rarg0, r15_thread);\n+  __ movptr(c_rarg1, Address(rsp, 0)); \/\/ return address\n+\n+  \/\/ rax still holds the original exception oop, save it before the call\n+  __ push(rax);\n+\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n+  __ movptr(rbx, rax);\n+\n+  \/\/ Continue at exception handler:\n+  \/\/   rax: exception oop\n+  \/\/   rbx: exception handler\n+  \/\/   rdx: exception pc\n+  __ pop(rax);\n+  __ verify_oop(rax);\n+  __ pop(rdx);\n+  __ jmp(rbx);\n+}\n+\n@@ -1726,1 +1741,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1728,0 +1743,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = rbx;  \/\/ known to be free at this point\n@@ -1790,0 +1808,32 @@\n+  if (method->is_continuation_enter_intrinsic()) {\n+    vmIntrinsics::ID iid = method->intrinsic_id();\n+    intptr_t start = (intptr_t)__ pc();\n+    int vep_offset = 0;\n+    int exception_offset = 0;\n+    int frame_complete = 0;\n+    int stack_slots = 0;\n+    OopMapSet* oop_maps =  new OopMapSet();\n+    int interpreted_entry_offset = -1;\n+    gen_continuation_enter(masm,\n+                         in_regs,\n+                         exception_offset,\n+                         oop_maps,\n+                         frame_complete,\n+                         stack_slots,\n+                         interpreted_entry_offset,\n+                         vep_offset);\n+    __ flush();\n+    nmethod* nm = nmethod::new_native_nmethod(method,\n+                                              compile_id,\n+                                              masm->code(),\n+                                              vep_offset,\n+                                              frame_complete,\n+                                              stack_slots,\n+                                              in_ByteSize(-1),\n+                                              in_ByteSize(-1),\n+                                              oop_maps,\n+                                              exception_offset);\n+    ContinuationEntry::set_enter_code(nm, interpreted_entry_offset);\n+    return nm;\n+  }\n+\n@@ -1973,1 +2023,2 @@\n-  bs->nmethod_entry_barrier(masm);\n+  \/\/ native wrapper is not hot enough to micro optimize the nmethod entry barrier with an out-of-line stub\n+  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n@@ -2137,1 +2188,1 @@\n-  \/\/ be pushed on the stack when we do a a stack traversal). It is enough that the pc()\n+  \/\/ be pushed on the stack when we do a stack traversal). It is enough that the pc()\n@@ -2185,0 +2236,1 @@\n+    Label count_mon;\n@@ -2199,0 +2251,1 @@\n+\n@@ -2215,1 +2268,1 @@\n-      __ jcc(Assembler::equal, lock_done);\n+      __ jcc(Assembler::equal, count_mon);\n@@ -2237,0 +2290,2 @@\n+    __ bind(count_mon);\n+    __ inc_held_monitor_count();\n@@ -2239,1 +2294,0 @@\n-\n@@ -2336,1 +2390,1 @@\n-  Label unlock_done;\n+  Label unlock_done;\n@@ -2340,0 +2394,2 @@\n+    Label fast_done;\n+\n@@ -2343,2 +2399,1 @@\n-    Label done;\n-\n+      Label not_recur;\n@@ -2348,1 +2403,4 @@\n-      __ jcc(Assembler::equal, done);\n+      __ jcc(Assembler::notEqual, not_recur);\n+      __ dec_held_monitor_count();\n+      __ jmpb(fast_done);\n+      __ bind(not_recur);\n@@ -2356,1 +2414,0 @@\n-\n@@ -2367,0 +2424,1 @@\n+      __ dec_held_monitor_count();\n@@ -2377,2 +2435,1 @@\n-    __ bind(done);\n-\n+    __ bind(fast_done);\n@@ -2582,1 +2639,1 @@\n-  \/\/ address has been pushed on the the stack, and return values are in\n+  \/\/ address has been pushed on the stack, and return values are in\n@@ -2616,1 +2673,1 @@\n-  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ true);\n+  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_wide_vectors*\/ true);\n@@ -2634,1 +2691,1 @@\n-  (void) RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ true);\n+  (void) RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_wide_vectors*\/ true);\n@@ -2653,1 +2710,1 @@\n-    RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ true);\n+    RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_wide_vectors*\/ true);\n@@ -2700,1 +2757,1 @@\n-  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ true);\n+  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_wide_vectors*\/ true);\n@@ -3131,1 +3188,1 @@\n-  bool save_vectors = (poll_type == POLL_AT_VECTOR_LOOP);\n+  bool save_wide_vectors = (poll_type == POLL_AT_VECTOR_LOOP);\n@@ -3146,1 +3203,1 @@\n-  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, save_vectors);\n+  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, save_wide_vectors);\n@@ -3150,1 +3207,1 @@\n-  \/\/ work outselves.\n+  \/\/ work ourselves.\n@@ -3152,1 +3209,1 @@\n-  __ set_last_Java_frame(noreg, noreg, NULL);\n+  __ set_last_Java_frame(noreg, noreg, NULL);  \/\/ JavaFrameAnchor::capture_last_Java_pc() will get the pc from the return address, which we store next:\n@@ -3185,1 +3242,1 @@\n-  RegisterSaver::restore_live_registers(masm, save_vectors);\n+  RegisterSaver::restore_live_registers(masm, save_wide_vectors);\n@@ -3258,1 +3315,1 @@\n-  RegisterSaver::restore_live_registers(masm, save_vectors);\n+  RegisterSaver::restore_live_registers(masm, save_wide_vectors);\n@@ -3298,1 +3355,1 @@\n-  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_vectors*\/ false);\n+  map = RegisterSaver::save_live_registers(masm, 0, &frame_size_in_words, \/*save_wide_vectors*\/ false);\n@@ -3332,1 +3389,1 @@\n-  \/\/ We are back the the original state on entry and ready to go.\n+  \/\/ We are back to the original state on entry and ready to go.\n@@ -3358,257 +3415,0 @@\n-#ifdef COMPILER2\n-static const int native_invoker_code_size = MethodHandles::adapter_code_size;\n-\n-class NativeInvokerGenerator : public StubCodeGenerator {\n-  address _call_target;\n-  int _shadow_space_bytes;\n-\n-  const GrowableArray<VMReg>& _input_registers;\n-  const GrowableArray<VMReg>& _output_registers;\n-\n-  int _frame_complete;\n-  int _framesize;\n-  OopMapSet* _oop_maps;\n-public:\n-  NativeInvokerGenerator(CodeBuffer* buffer,\n-                         address call_target,\n-                         int shadow_space_bytes,\n-                         const GrowableArray<VMReg>& input_registers,\n-                         const GrowableArray<VMReg>& output_registers)\n-   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n-     _call_target(call_target),\n-     _shadow_space_bytes(shadow_space_bytes),\n-     _input_registers(input_registers),\n-     _output_registers(output_registers),\n-     _frame_complete(0),\n-     _framesize(0),\n-     _oop_maps(NULL) {\n-    assert(_output_registers.length() <= 1\n-           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n-\n-  }\n-\n-  void generate();\n-\n-  int spill_size_in_bytes() const {\n-    if (_output_registers.length() == 0) {\n-      return 0;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    if (reg->is_Register()) {\n-      return 8;\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        return 64;\n-      } else if (UseAVX >= 1) {\n-        return 32;\n-      } else {\n-        return 16;\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-\n-  void spill_out_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ movptr(Address(rsp, 0), reg->as_Register());\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        __ evmovdqul(Address(rsp, 0), reg->as_XMMRegister(), Assembler::AVX_512bit);\n-      } else if (UseAVX >= 1) {\n-        __ vmovdqu(Address(rsp, 0), reg->as_XMMRegister());\n-      } else {\n-        __ movdqu(Address(rsp, 0), reg->as_XMMRegister());\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  void fill_out_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ movptr(reg->as_Register(), Address(rsp, 0));\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        __ evmovdqul(reg->as_XMMRegister(), Address(rsp, 0), Assembler::AVX_512bit);\n-      } else if (UseAVX >= 1) {\n-        __ vmovdqu(reg->as_XMMRegister(), Address(rsp, 0));\n-      } else {\n-        __ movdqu(reg->as_XMMRegister(), Address(rsp, 0));\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  int frame_complete() const {\n-    return _frame_complete;\n-  }\n-\n-  int framesize() const {\n-    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n-  }\n-\n-  OopMapSet* oop_maps() const {\n-    return _oop_maps;\n-  }\n-\n-private:\n-#ifdef ASSERT\n-bool target_uses_register(VMReg reg) {\n-  return _input_registers.contains(reg) || _output_registers.contains(reg);\n-}\n-#endif\n-};\n-\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  int locs_size  = 64;\n-  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n-  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n-  g.generate();\n-  code.log_section_sizes(\"nep_invoker_blob\");\n-\n-  RuntimeStub* stub =\n-    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n-                                  &code,\n-                                  g.frame_complete(),\n-                                  g.framesize(),\n-                                  g.oop_maps(), false);\n-  return stub;\n-}\n-\n-void NativeInvokerGenerator::generate() {\n-  assert(!(target_uses_register(r15_thread->as_VMReg()) || target_uses_register(rscratch1->as_VMReg())), \"Register conflict\");\n-\n-  enum layout {\n-    rbp_off,\n-    rbp_off2,\n-    return_off,\n-    return_off2,\n-    framesize \/\/ inclusive of return address\n-  };\n-\n-  _framesize = align_up(framesize + ((_shadow_space_bytes + spill_size_in_bytes()) >> LogBytesPerInt), 4);\n-  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n-\n-  _oop_maps  = new OopMapSet();\n-  MacroAssembler* masm = _masm;\n-\n-  address start = __ pc();\n-\n-  __ enter();\n-\n-  \/\/ return address and rbp are already in place\n-  __ subptr(rsp, (_framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-  _frame_complete = __ pc() - start;\n-\n-  address the_pc = __ pc();\n-\n-  __ set_last_Java_frame(rsp, rbp, (address)the_pc);\n-  OopMap* map = new OopMap(_framesize, 0);\n-  _oop_maps->add_gc_map(the_pc - start, map);\n-\n-  \/\/ State transition\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n-\n-  __ call(RuntimeAddress(_call_target));\n-\n-  __ restore_cpu_control_state_after_jni();\n-\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native_trans);\n-\n-  \/\/ Force this write out before the read below\n-  __ membar(Assembler::Membar_mask_bits(\n-          Assembler::LoadLoad | Assembler::LoadStore |\n-          Assembler::StoreLoad | Assembler::StoreStore));\n-\n-  Label L_after_safepoint_poll;\n-  Label L_safepoint_poll_slow_path;\n-\n-  __ safepoint_poll(L_safepoint_poll_slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n-  __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n-  __ jcc(Assembler::notEqual, L_safepoint_poll_slow_path);\n-\n-  __ bind(L_after_safepoint_poll);\n-\n-  \/\/ change thread state\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_Java);\n-\n-  __ block_comment(\"reguard stack check\");\n-  Label L_reguard;\n-  Label L_after_reguard;\n-  __ cmpl(Address(r15_thread, JavaThread::stack_guard_state_offset()), StackOverflow::stack_guard_yellow_reserved_disabled);\n-  __ jcc(Assembler::equal, L_reguard);\n-  __ bind(L_after_reguard);\n-\n-  __ reset_last_Java_frame(r15_thread, true);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n-  __ bind(L_safepoint_poll_slow_path);\n-  __ vzeroupper();\n-\n-  spill_out_registers();\n-\n-  __ mov(c_rarg0, r15_thread);\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  fill_out_registers();\n-\n-  __ jmp(L_after_safepoint_poll);\n-  __ block_comment(\"} L_safepoint_poll_slow_path\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_reguard\");\n-  __ bind(L_reguard);\n-  __ vzeroupper();\n-\n-  spill_out_registers();\n-\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  fill_out_registers();\n-\n-  __ jmp(L_after_reguard);\n-\n-  __ block_comment(\"} L_reguard\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ flush();\n-}\n-#endif \/\/ COMPILER2\n-\n@@ -4124,10 +3924,0 @@\n-\n-void SharedRuntime::compute_move_order(const BasicType* in_sig_bt,\n-                                       int total_in_args, const VMRegPair* in_regs,\n-                                       int total_out_args, VMRegPair* out_regs,\n-                                       GrowableArray<int>& arg_order,\n-                                       VMRegPair tmp_vmreg) {\n-  ComputeMoveOrder order(total_in_args, in_regs,\n-                         total_out_args, out_regs,\n-                         in_sig_bt, arg_order, tmp_vmreg);\n-}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":282,"deletions":492,"binary":false,"changes":774,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,1 @@\n+#include \"prims\/jvmtiExport.hpp\"\n@@ -45,0 +46,2 @@\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -47,0 +50,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -50,1 +54,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -62,0 +65,3 @@\n+#if INCLUDE_JFR\n+#include \"jfr\/support\/jfrIntrinsics.hpp\"\n+#endif\n@@ -81,0 +87,4 @@\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots);\n+void fill_continuation_entry(MacroAssembler* masm);\n+void continuation_enter_cleanup(MacroAssembler* masm);\n+\n@@ -393,0 +403,2 @@\n+    __ pop_cont_fastpath();\n+\n@@ -818,0 +830,15 @@\n+  address generate_count_leading_zeros_lut(const char *stub_name) {\n+    __ align64();\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    __ emit_data64(0x0101010102020304, relocInfo::none);\n+    __ emit_data64(0x0000000000000000, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -848,0 +875,60 @@\n+  address generate_vector_reverse_bit_lut(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    __ emit_data64(0x0E060A020C040800, relocInfo::none);\n+    __ emit_data64(0x0F070B030D050901, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_long(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    __ emit_data64(0x0001020304050607, relocInfo::none);\n+    __ emit_data64(0x08090A0B0C0D0E0F, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_int(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    __ emit_data64(0x0405060700010203, relocInfo::none);\n+    __ emit_data64(0x0C0D0E0F08090A0B, relocInfo::none);\n+    return start;\n+  }\n+\n+  address generate_vector_reverse_byte_perm_mask_short(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    __ emit_data64(0x0607040502030001, relocInfo::none);\n+    __ emit_data64(0x0E0F0C0D0A0B0809, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -2270,1 +2357,1 @@\n-  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/ cache line boundaries will still be loaded and stored atomically.\n@@ -2384,1 +2471,1 @@\n-  \/\/ cache line boundaries will still be loaded and stored atomicly.\n+  \/\/ cache line boundaries will still be loaded and stored atomically.\n@@ -3883,40 +3970,0 @@\n-  \/\/ Safefetch stubs.\n-  void generate_safefetch(const char* name, int size, address* entry,\n-                          address* fault_pc, address* continuation_pc) {\n-    \/\/ safefetch signatures:\n-    \/\/   int      SafeFetch32(int*      adr, int      errValue);\n-    \/\/   intptr_t SafeFetchN (intptr_t* adr, intptr_t errValue);\n-    \/\/\n-    \/\/ arguments:\n-    \/\/   c_rarg0 = adr\n-    \/\/   c_rarg1 = errValue\n-    \/\/\n-    \/\/ result:\n-    \/\/   PPC_RET  = *adr or errValue\n-\n-    StubCodeMark mark(this, \"StubRoutines\", name);\n-\n-    \/\/ Entry point, pc or function descriptor.\n-    *entry = __ pc();\n-\n-    \/\/ Load *adr into c_rarg1, may fault.\n-    *fault_pc = __ pc();\n-    switch (size) {\n-      case 4:\n-        \/\/ int32_t\n-        __ movl(c_rarg1, Address(c_rarg0, 0));\n-        break;\n-      case 8:\n-        \/\/ int64_t\n-        __ movq(c_rarg1, Address(c_rarg0, 0));\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-    }\n-\n-    \/\/ return errValue or *adr\n-    *continuation_pc = __ pc();\n-    __ movq(rax, c_rarg1);\n-    __ ret(0);\n-  }\n-\n@@ -4034,1 +4081,1 @@\n-        load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes upto 0xe0\n+        load_key(xmm15, key, 0xd0); \/\/ 0xd0; 256-bit key goes up to 0xe0\n@@ -4126,2 +4173,2 @@\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes upto 0xc0\n-        load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes upto 0xc0\n+        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 192-bit key goes up to 0xc0\n+        load_key(xmm_key12, key, 0xc0); \/\/ 0xc0; 192-bit key goes up to 0xc0\n@@ -4130,1 +4177,1 @@\n-        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes upto 0xe0\n+        load_key(xmm_key11, key, 0xb0); \/\/ 0xb0; 256-bit key goes up to 0xe0\n@@ -5692,1 +5739,1 @@\n-      \/\/ bits respecively.  This is done using vpmullw.  We end up\n+      \/\/ bits respectively.  This is done using vpmullw.  We end up\n@@ -6569,1 +6616,1 @@\n-   * Ouput:\n+   * Output:\n@@ -6625,1 +6672,1 @@\n-  * Ouput:\n+  * Output:\n@@ -6845,1 +6892,1 @@\n-  \/\/    c_rarg3   - z lenth\n+  \/\/    c_rarg3   - z length\n@@ -7501,0 +7548,254 @@\n+  RuntimeStub* generate_cont_doYield() {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    enum layout {\n+      rbp_off,\n+      rbpH_off,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(\"cont_doYield\", 512, 64);\n+    MacroAssembler* _masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+    __ enter();\n+    address the_pc = __ pc();\n+\n+    int frame_complete = the_pc - start;\n+\n+    \/\/ This nop must be exactly at the PC we push into the frame info.\n+    \/\/ We use this nop for fast CodeBlob lookup, associate the OopMap\n+    \/\/ with it right away.\n+    __ post_call_nop();\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(framesize, 1);\n+    oop_maps->add_gc_map(frame_complete, map);\n+\n+    __ set_last_Java_frame(rsp, rbp, the_pc);\n+    __ movptr(c_rarg0, r15_thread);\n+    __ movptr(c_rarg1, rsp);\n+    __ call_VM_leaf(Continuation::freeze_entry(), 2);\n+    __ reset_last_Java_frame(true);\n+\n+    Label L_pinned;\n+\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::notZero, L_pinned);\n+\n+    __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    continuation_enter_cleanup(_masm);\n+    __ pop(rbp);\n+    __ ret(0);\n+\n+    __ bind(L_pinned);\n+\n+    \/\/ Pinned, return to caller\n+    __ leave();\n+    __ ret(0);\n+\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(code.name(),\n+                                    &code,\n+                                    frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps,\n+                                    false);\n+    return stub;\n+  }\n+\n+  address generate_cont_thaw(const char* label, Continuation::thaw_kind kind) {\n+    if (!Continuations::enabled()) return nullptr;\n+\n+    bool return_barrier = Continuation::is_thaw_return_barrier(kind);\n+    bool return_barrier_exception = Continuation::is_thaw_return_barrier_exception(kind);\n+\n+    StubCodeMark mark(this, \"StubRoutines\", label);\n+    address start = __ pc();\n+\n+    \/\/ TODO: Handle Valhalla return types. May require generating different return barriers.\n+\n+    if (!return_barrier) {\n+      \/\/ Pop return address. If we don't do this, we get a drift,\n+      \/\/ where the bottom-most frozen frame continuously grows.\n+      __ pop(c_rarg3);\n+    } else {\n+      __ movptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+    }\n+\n+#ifdef ASSERT\n+    {\n+      Label L_good_sp;\n+      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+      __ jcc(Assembler::equal, L_good_sp);\n+      __ stop(\"Incorrect rsp at thaw entry\");\n+      __ BIND(L_good_sp);\n+    }\n+#endif\n+\n+    if (return_barrier) {\n+      \/\/ Preserve possible return value from a method returning to the return barrier.\n+      __ push(rax);\n+      __ push_d(xmm0);\n+    }\n+\n+    __ movptr(c_rarg0, r15_thread);\n+    __ movptr(c_rarg1, (return_barrier ? 1 : 0));\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, Continuation::prepare_thaw), 2);\n+    __ movptr(rbx, rax);\n+\n+    if (return_barrier) {\n+      \/\/ Restore return value from a method returning to the return barrier.\n+      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+      __ pop_d(xmm0);\n+      __ pop(rax);\n+    }\n+\n+#ifdef ASSERT\n+    {\n+      Label L_good_sp;\n+      __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+      __ jcc(Assembler::equal, L_good_sp);\n+      __ stop(\"Incorrect rsp after prepare thaw\");\n+      __ BIND(L_good_sp);\n+    }\n+#endif\n+\n+    \/\/ rbx contains the size of the frames to thaw, 0 if overflow or no more frames\n+    Label L_thaw_success;\n+    __ testptr(rbx, rbx);\n+    __ jccb(Assembler::notZero, L_thaw_success);\n+    __ jump(ExternalAddress(StubRoutines::throw_StackOverflowError_entry()));\n+    __ bind(L_thaw_success);\n+\n+    \/\/ Make room for the thawed frames and align the stack.\n+    __ subptr(rsp, rbx);\n+    __ andptr(rsp, -StackAlignmentInBytes);\n+\n+    if (return_barrier) {\n+      \/\/ Preserve possible return value from a method returning to the return barrier. (Again.)\n+      __ push(rax);\n+      __ push_d(xmm0);\n+    }\n+\n+    \/\/ If we want, we can templatize thaw by kind, and have three different entries.\n+    __ movptr(c_rarg0, r15_thread);\n+    __ movptr(c_rarg1, kind);\n+    __ call_VM_leaf(Continuation::thaw_entry(), 2);\n+    __ movptr(rbx, rax);\n+\n+    if (return_barrier) {\n+      \/\/ Restore return value from a method returning to the return barrier. (Again.)\n+      \/\/ No safepoint in the call to thaw, so even an oop return value should be OK.\n+      __ pop_d(xmm0);\n+      __ pop(rax);\n+    } else {\n+      \/\/ Return 0 (success) from doYield.\n+      __ xorptr(rax, rax);\n+    }\n+\n+    \/\/ After thawing, rbx is the SP of the yielding frame.\n+    \/\/ Move there, and then to saved RBP slot.\n+    __ movptr(rsp, rbx);\n+    __ subptr(rsp, 2*wordSize);\n+\n+    if (return_barrier_exception) {\n+      __ movptr(c_rarg0, r15_thread);\n+      __ movptr(c_rarg1, Address(rsp, wordSize)); \/\/ return address\n+\n+      \/\/ rax still holds the original exception oop, save it before the call\n+      __ push(rax);\n+\n+      __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::exception_handler_for_return_address), 2);\n+      __ movptr(rbx, rax);\n+\n+      \/\/ Continue at exception handler:\n+      \/\/   rax: exception oop\n+      \/\/   rbx: exception handler\n+      \/\/   rdx: exception pc\n+      __ pop(rax);\n+      __ verify_oop(rax);\n+      __ pop(rbp); \/\/ pop out RBP here too\n+      __ pop(rdx);\n+      __ jmp(rbx);\n+    } else {\n+      \/\/ We are \"returning\" into the topmost thawed frame; see Thaw::push_return_frame\n+      __ pop(rbp);\n+      __ ret(0);\n+    }\n+\n+    return start;\n+  }\n+\n+  address generate_cont_thaw() {\n+    return generate_cont_thaw(\"Cont thaw\", Continuation::thaw_top);\n+  }\n+\n+  \/\/ TODO: will probably need multiple return barriers depending on return type\n+\n+  address generate_cont_returnBarrier() {\n+    return generate_cont_thaw(\"Cont thaw return barrier\", Continuation::thaw_return_barrier);\n+  }\n+\n+  address generate_cont_returnBarrier_exception() {\n+    return generate_cont_thaw(\"Cont thaw return barrier exception\", Continuation::thaw_return_barrier_exception);\n+  }\n+\n+#if INCLUDE_JFR\n+\n+  \/\/ For c2: c_rarg0 is junk, call to runtime to write a checkpoint.\n+  \/\/ It returns a jobject handle to the event writer.\n+  \/\/ The handle is dereferenced and the return value is the event writer oop.\n+  RuntimeStub* generate_jfr_write_checkpoint() {\n+    enum layout {\n+      rbp_off,\n+      rbpH_off,\n+      return_off,\n+      return_off2,\n+      framesize \/\/ inclusive of return address\n+    };\n+\n+    CodeBuffer code(\"jfr_write_checkpoint\", 512, 64);\n+    MacroAssembler* _masm = new MacroAssembler(&code);\n+\n+    address start = __ pc();\n+    __ enter();\n+    address the_pc = __ pc();\n+\n+    int frame_complete = the_pc - start;\n+\n+    __ set_last_Java_frame(rsp, rbp, the_pc);\n+    __ movptr(c_rarg0, r15_thread);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::write_checkpoint), 1);\n+    __ reset_last_Java_frame(true);\n+\n+    \/\/ rax is jobject handle result, unpack and process it through a barrier.\n+    Label L_null_jobject;\n+    __ testptr(rax, rax);\n+    __ jcc(Assembler::zero, L_null_jobject);\n+\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->load_at(_masm, ACCESS_READ | IN_NATIVE, T_OBJECT, rax, Address(rax, 0), c_rarg0, r15_thread);\n+\n+    __ bind(L_null_jobject);\n+\n+    __ leave();\n+    __ ret(0);\n+\n+    OopMapSet* oop_maps = new OopMapSet();\n+    OopMap* map = new OopMap(framesize, 1);\n+    oop_maps->add_gc_map(frame_complete, map);\n+\n+    RuntimeStub* stub =\n+      RuntimeStub::new_runtime_stub(code.name(),\n+                                    &code,\n+                                    frame_complete,\n+                                    (framesize >> (LogBytesPerWord - LogBytesPerInt)),\n+                                    oop_maps,\n+                                    false);\n+    return stub;\n+  }\n+\n+#endif \/\/ INCLUDE_JFR\n+\n@@ -7869,0 +8170,1 @@\n+  }\n@@ -7870,7 +8172,11 @@\n-    \/\/ Safefetch stubs.\n-    generate_safefetch(\"SafeFetch32\", sizeof(int),     &StubRoutines::_safefetch32_entry,\n-                                                       &StubRoutines::_safefetch32_fault_pc,\n-                                                       &StubRoutines::_safefetch32_continuation_pc);\n-    generate_safefetch(\"SafeFetchN\", sizeof(intptr_t), &StubRoutines::_safefetchN_entry,\n-                                                       &StubRoutines::_safefetchN_fault_pc,\n-                                                       &StubRoutines::_safefetchN_continuation_pc);\n+  void generate_phase1() {\n+    \/\/ Continuation stubs:\n+    StubRoutines::_cont_thaw          = generate_cont_thaw();\n+    StubRoutines::_cont_returnBarrier = generate_cont_returnBarrier();\n+    StubRoutines::_cont_returnBarrierExc = generate_cont_returnBarrier_exception();\n+    StubRoutines::_cont_doYield_stub = generate_cont_doYield();\n+    StubRoutines::_cont_doYield      = StubRoutines::_cont_doYield_stub == nullptr ? nullptr\n+                                        : StubRoutines::_cont_doYield_stub->entry_point();\n+\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint_stub = generate_jfr_write_checkpoint();)\n+    JFR_ONLY(StubRoutines::_jfr_write_checkpoint = StubRoutines::_jfr_write_checkpoint_stub->entry_point();)\n@@ -7924,0 +8230,5 @@\n+    StubRoutines::x86::_vector_count_leading_zeros_lut = generate_count_leading_zeros_lut(\"count_leading_zeros_lut\");\n+    StubRoutines::x86::_vector_reverse_bit_lut = generate_vector_reverse_bit_lut(\"reverse_bit_lut\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_long = generate_vector_reverse_byte_perm_mask_long(\"perm_mask_long\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_int = generate_vector_reverse_byte_perm_mask_int(\"perm_mask_int\");\n+    StubRoutines::x86::_vector_reverse_byte_perm_mask_short = generate_vector_reverse_byte_perm_mask_short(\"perm_mask_short\");\n@@ -7925,1 +8236,1 @@\n-    if (UsePopCountInstruction && VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n+    if (VM_Version::supports_avx2() && !VM_Version::supports_avx512_vpopcntdq()) {\n@@ -8142,4 +8453,2 @@\n-  StubGenerator(CodeBuffer* code, bool all) : StubCodeGenerator(code) {\n-    if (all) {\n-      generate_all();\n-    } else {\n+  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n+    if (phase == 0) {\n@@ -8147,0 +8456,4 @@\n+    } else if (phase == 1) {\n+      generate_phase1(); \/\/ stubs that must be available for the interpreter\n+    } else {\n+      generate_all();\n@@ -8152,1 +8465,1 @@\n-void StubGenerator_generate(CodeBuffer* code, bool all) {\n+void StubGenerator_generate(CodeBuffer* code, int phase) {\n@@ -8156,1 +8469,1 @@\n-  StubGenerator g(code, all);\n+  StubGenerator g(code, phase);\n@@ -8158,0 +8471,98 @@\n+\n+#undef __\n+#define __ masm->\n+\n+\/\/---------------------------- continuation_enter_setup ---------------------------\n+\/\/\n+\/\/ Arguments:\n+\/\/   None.\n+\/\/\n+\/\/ Results:\n+\/\/   rsp: pointer to blank ContinuationEntry\n+\/\/\n+\/\/ Kills:\n+\/\/   rax\n+\/\/\n+OopMap* continuation_enter_setup(MacroAssembler* masm, int& stack_slots) {\n+  assert(ContinuationEntry::size() % VMRegImpl::stack_slot_size == 0, \"\");\n+  assert(in_bytes(ContinuationEntry::cont_offset())  % VMRegImpl::stack_slot_size == 0, \"\");\n+  assert(in_bytes(ContinuationEntry::chunk_offset()) % VMRegImpl::stack_slot_size == 0, \"\");\n+\n+  stack_slots += checked_cast<int>(ContinuationEntry::size()) \/ wordSize;\n+  __ subptr(rsp, checked_cast<int32_t>(ContinuationEntry::size()));\n+\n+  int frame_size = (checked_cast<int>(ContinuationEntry::size()) + wordSize) \/ VMRegImpl::stack_slot_size;\n+  OopMap* map = new OopMap(frame_size, 0);\n+  ContinuationEntry::setup_oopmap(map);\n+\n+  __ movptr(rax, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  __ movptr(Address(rsp, ContinuationEntry::parent_offset()), rax);\n+  __ movptr(Address(r15_thread, JavaThread::cont_entry_offset()), rsp);\n+\n+  return map;\n+}\n+\n+\/\/---------------------------- fill_continuation_entry ---------------------------\n+\/\/\n+\/\/ Arguments:\n+\/\/   rsp: pointer to blank Continuation entry\n+\/\/   reg_cont_obj: pointer to the continuation\n+\/\/   reg_flags: flags\n+\/\/\n+\/\/ Results:\n+\/\/   rsp: pointer to filled out ContinuationEntry\n+\/\/\n+\/\/ Kills:\n+\/\/   rax\n+\/\/\n+void fill_continuation_entry(MacroAssembler* masm, Register reg_cont_obj, Register reg_flags) {\n+  assert_different_registers(rax, reg_cont_obj, reg_flags);\n+\n+  DEBUG_ONLY(__ movl(Address(rsp, ContinuationEntry::cookie_offset()), ContinuationEntry::cookie_value());)\n+\n+  __ movptr(Address(rsp, ContinuationEntry::cont_offset()), reg_cont_obj);\n+  __ movl  (Address(rsp, ContinuationEntry::flags_offset()), reg_flags);\n+  __ movptr(Address(rsp, ContinuationEntry::chunk_offset()), 0);\n+  __ movl(Address(rsp, ContinuationEntry::argsize_offset()), 0);\n+  __ movl(Address(rsp, ContinuationEntry::pin_count_offset()), 0);\n+\n+  __ movptr(rax, Address(r15_thread, JavaThread::cont_fastpath_offset()));\n+  __ movptr(Address(rsp, ContinuationEntry::parent_cont_fastpath_offset()), rax);\n+  __ movq(rax, Address(r15_thread, JavaThread::held_monitor_count_offset()));\n+  __ movq(Address(rsp, ContinuationEntry::parent_held_monitor_count_offset()), rax);\n+\n+  __ movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), 0);\n+  __ movq(Address(r15_thread, JavaThread::held_monitor_count_offset()), 0);\n+}\n+\n+\/\/---------------------------- continuation_enter_cleanup ---------------------------\n+\/\/\n+\/\/ Arguments:\n+\/\/   rsp: pointer to the ContinuationEntry\n+\/\/\n+\/\/ Results:\n+\/\/   rsp: pointer to the spilled rbp in the entry frame\n+\/\/\n+\/\/ Kills:\n+\/\/   rbx\n+\/\/\n+void continuation_enter_cleanup(MacroAssembler* masm) {\n+#ifdef ASSERT\n+  Label L_good_sp;\n+  __ cmpptr(rsp, Address(r15_thread, JavaThread::cont_entry_offset()));\n+  __ jcc(Assembler::equal, L_good_sp);\n+  __ stop(\"Incorrect rsp at continuation_enter_cleanup\");\n+  __ bind(L_good_sp);\n+#endif\n+\n+  __ movptr(rbx, Address(rsp, ContinuationEntry::parent_cont_fastpath_offset()));\n+  __ movptr(Address(r15_thread, JavaThread::cont_fastpath_offset()), rbx);\n+  __ movq(rbx, Address(rsp, ContinuationEntry::parent_held_monitor_count_offset()));\n+  __ movq(Address(r15_thread, JavaThread::held_monitor_count_offset()), rbx);\n+\n+  __ movptr(rbx, Address(rsp, ContinuationEntry::parent_offset()));\n+  __ movptr(Address(r15_thread, JavaThread::cont_entry_offset()), rbx);\n+  __ addptr(rsp, (int32_t)ContinuationEntry::size());\n+}\n+\n+#undef __\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":477,"deletions":66,"binary":false,"changes":543,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -374,0 +375,1 @@\n+\n@@ -375,0 +377,1 @@\n+  __ push_cont_fastpath();\n@@ -376,0 +379,2 @@\n+  __ pop_cont_fastpath();\n+\n@@ -659,0 +664,14 @@\n+address TemplateInterpreterGenerator::generate_Continuation_doYield_entry(void) {\n+  if (!Continuations::enabled()) return nullptr;\n+\n+  address entry = __ pc();\n+  assert(StubRoutines::cont_doYield() != NULL, \"stub not yet generated\");\n+\n+  __ push_cont_fastpath();\n+\n+  __ jump(RuntimeAddress(CAST_FROM_FN_PTR(address, StubRoutines::cont_doYield())));\n+  \/\/ return value is in rax\n+\n+  return entry;\n+}\n+\n@@ -895,1 +914,1 @@\n-    __ stop(\"broken stack frame setup in interpreter\");\n+    __ stop(\"broken stack frame setup in interpreter 5\");\n@@ -1056,1 +1075,1 @@\n-  \/\/ NOTE: the order of theses push(es) is known to frame::interpreter_frame_result.\n+  \/\/ NOTE: the order of these push(es) is known to frame::interpreter_frame_result.\n@@ -1320,1 +1339,1 @@\n-  \/\/ rbcp: sender sp\n+  \/\/ rbcp: sender sp (set in InterpreterMacroAssembler::prepare_to_jump_from_interpreted \/ generate_call_stub)\n@@ -1448,1 +1467,1 @@\n-    __ stop(\"broken stack frame setup in interpreter\");\n+    __ stop(\"broken stack frame setup in interpreter 6\");\n","filename":"src\/hotspot\/cpu\/x86\/templateInterpreterGenerator_x86.cpp","additions":23,"deletions":4,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -146,1 +146,1 @@\n-\/\/ Miscelaneous helper routines\n+\/\/ Miscellaneous helper routines\n@@ -2717,0 +2717,1 @@\n+    __ push_cont_fastpath();\n@@ -2719,0 +2720,1 @@\n+    __ pop_cont_fastpath();\n@@ -2745,1 +2747,1 @@\n-\/\/     writes act as aquire & release, so:\n+\/\/     writes act as acquire & release, so:\n@@ -4300,1 +4302,1 @@\n-  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InlineKlassKind);\n@@ -4349,1 +4351,1 @@\n-  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InstanceKlass::_kind_inline_type);\n+  __ cmpb(Address(rcx, InstanceKlass::kind_offset()), InlineKlassKind);\n@@ -4691,1 +4693,1 @@\n-  \/\/ The object has already been poped from the stack, so the\n+  \/\/ The object has already been popped from the stack, so the\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -411,1 +411,1 @@\n-    \/\/ If UseAVX is unitialized or is set by the user to include EVEX\n+    \/\/ If UseAVX is uninitialized or is set by the user to include EVEX\n@@ -501,1 +501,1 @@\n-    \/\/ If UseAVX is unitialized or is set by the user to include EVEX\n+    \/\/ If UseAVX is uninitialized or is set by the user to include EVEX\n@@ -925,0 +925,1 @@\n+    _features &= ~CPU_AVX512_BITALG;\n@@ -954,0 +955,2 @@\n+      _features &= ~CPU_GFNI;\n+      _features &= ~CPU_AVX512_BITALG;\n@@ -1295,0 +1298,21 @@\n+#if defined(COMPILER2)\n+  if (FLAG_IS_DEFAULT(SuperWordMaxVectorSize)) {\n+    if (FLAG_IS_DEFAULT(UseAVX) && UseAVX > 2 &&\n+        is_intel_skylake() && _stepping >= 5) {\n+      \/\/ Limit auto vectorization to 256 bit (32 byte) by default on Cascade Lake\n+      FLAG_SET_DEFAULT(SuperWordMaxVectorSize, MIN2(MaxVectorSize, (intx)32));\n+    } else {\n+      FLAG_SET_DEFAULT(SuperWordMaxVectorSize, MaxVectorSize);\n+    }\n+  } else {\n+    if (SuperWordMaxVectorSize > MaxVectorSize) {\n+      warning(\"SuperWordMaxVectorSize cannot be greater than MaxVectorSize %i\", (int) MaxVectorSize);\n+      FLAG_SET_DEFAULT(SuperWordMaxVectorSize, MaxVectorSize);\n+    }\n+    if (!is_power_of_2(SuperWordMaxVectorSize)) {\n+      warning(\"SuperWordMaxVectorSize must be a power of 2, setting to MaxVectorSize: %i\", (int) MaxVectorSize);\n+      FLAG_SET_DEFAULT(SuperWordMaxVectorSize, MaxVectorSize);\n+    }\n+  }\n+#endif\n+\n@@ -2145,1 +2169,1 @@\n-  \"Pentium Pro\",   \/\/or Pentium-M\/Woodcrest depeding on model\n+  \"Pentium Pro\",   \/\/or Pentium-M\/Woodcrest depending on model\n@@ -2260,1 +2284,1 @@\n-\/* Brand ID is for back compability\n+\/* Brand ID is for back compatibility\n@@ -2796,1 +2820,1 @@\n-    \/\/ Compute freqency (in Hz) from brand string.\n+    \/\/ Compute frequency (in Hz) from brand string.\n@@ -2820,0 +2844,318 @@\n+uint64_t VM_Version::feature_flags() {\n+  uint64_t result = 0;\n+  if (_cpuid_info.std_cpuid1_edx.bits.cmpxchg8 != 0)\n+    result |= CPU_CX8;\n+  if (_cpuid_info.std_cpuid1_edx.bits.cmov != 0)\n+    result |= CPU_CMOV;\n+  if (_cpuid_info.std_cpuid1_edx.bits.clflush != 0)\n+    result |= CPU_FLUSH;\n+#ifdef _LP64\n+  \/\/ clflush should always be available on x86_64\n+  \/\/ if not we are in real trouble because we rely on it\n+  \/\/ to flush the code cache.\n+  assert ((result & CPU_FLUSH) != 0, \"clflush should be available\");\n+#endif\n+  if (_cpuid_info.std_cpuid1_edx.bits.fxsr != 0 || (is_amd_family() &&\n+      _cpuid_info.ext_cpuid1_edx.bits.fxsr != 0))\n+    result |= CPU_FXSR;\n+  \/\/ HT flag is set for multi-core processors also.\n+  if (threads_per_core() > 1)\n+    result |= CPU_HT;\n+  if (_cpuid_info.std_cpuid1_edx.bits.mmx != 0 || (is_amd_family() &&\n+      _cpuid_info.ext_cpuid1_edx.bits.mmx != 0))\n+    result |= CPU_MMX;\n+  if (_cpuid_info.std_cpuid1_edx.bits.sse != 0)\n+    result |= CPU_SSE;\n+  if (_cpuid_info.std_cpuid1_edx.bits.sse2 != 0)\n+    result |= CPU_SSE2;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse3 != 0)\n+    result |= CPU_SSE3;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.ssse3 != 0)\n+    result |= CPU_SSSE3;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse4_1 != 0)\n+    result |= CPU_SSE4_1;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.sse4_2 != 0)\n+    result |= CPU_SSE4_2;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.popcnt != 0)\n+    result |= CPU_POPCNT;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.avx != 0 &&\n+      _cpuid_info.std_cpuid1_ecx.bits.osxsave != 0 &&\n+      _cpuid_info.xem_xcr0_eax.bits.sse != 0 &&\n+      _cpuid_info.xem_xcr0_eax.bits.ymm != 0) {\n+    result |= CPU_AVX;\n+    result |= CPU_VZEROUPPER;\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.avx2 != 0)\n+      result |= CPU_AVX2;\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.avx512f != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.opmask != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.zmm512 != 0 &&\n+        _cpuid_info.xem_xcr0_eax.bits.zmm32 != 0) {\n+      result |= CPU_AVX512F;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512cd != 0)\n+        result |= CPU_AVX512CD;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512dq != 0)\n+        result |= CPU_AVX512DQ;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512pf != 0)\n+        result |= CPU_AVX512PF;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512er != 0)\n+        result |= CPU_AVX512ER;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512bw != 0)\n+        result |= CPU_AVX512BW;\n+      if (_cpuid_info.sef_cpuid7_ebx.bits.avx512vl != 0)\n+        result |= CPU_AVX512VL;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpopcntdq != 0)\n+        result |= CPU_AVX512_VPOPCNTDQ;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vpclmulqdq != 0)\n+        result |= CPU_AVX512_VPCLMULQDQ;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.vaes != 0)\n+        result |= CPU_AVX512_VAES;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.gfni != 0)\n+        result |= CPU_GFNI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vnni != 0)\n+        result |= CPU_AVX512_VNNI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_bitalg != 0)\n+        result |= CPU_AVX512_BITALG;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi != 0)\n+        result |= CPU_AVX512_VBMI;\n+      if (_cpuid_info.sef_cpuid7_ecx.bits.avx512_vbmi2 != 0)\n+        result |= CPU_AVX512_VBMI2;\n+    }\n+  }\n+  if (_cpuid_info.std_cpuid1_ecx.bits.hv != 0)\n+    result |= CPU_HV;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.bmi1 != 0)\n+    result |= CPU_BMI1;\n+  if (_cpuid_info.std_cpuid1_edx.bits.tsc != 0)\n+    result |= CPU_TSC;\n+  if (_cpuid_info.ext_cpuid7_edx.bits.tsc_invariance != 0)\n+    result |= CPU_TSCINV_BIT;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.aes != 0)\n+    result |= CPU_AES;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.erms != 0)\n+    result |= CPU_ERMS;\n+  if (_cpuid_info.sef_cpuid7_edx.bits.fast_short_rep_mov != 0)\n+    result |= CPU_FSRM;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.clmul != 0)\n+    result |= CPU_CLMUL;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.rtm != 0)\n+    result |= CPU_RTM;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.adx != 0)\n+     result |= CPU_ADX;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.bmi2 != 0)\n+    result |= CPU_BMI2;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.sha != 0)\n+    result |= CPU_SHA;\n+  if (_cpuid_info.std_cpuid1_ecx.bits.fma != 0)\n+    result |= CPU_FMA;\n+  if (_cpuid_info.sef_cpuid7_ebx.bits.clflushopt != 0)\n+    result |= CPU_FLUSHOPT;\n+  if (_cpuid_info.ext_cpuid1_edx.bits.rdtscp != 0)\n+    result |= CPU_RDTSCP;\n+  if (_cpuid_info.sef_cpuid7_ecx.bits.rdpid != 0)\n+    result |= CPU_RDPID;\n+\n+  \/\/ AMD|Hygon features.\n+  if (is_amd_family()) {\n+    if ((_cpuid_info.ext_cpuid1_edx.bits.tdnow != 0) ||\n+        (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0))\n+      result |= CPU_3DNOW_PREFETCH;\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0)\n+      result |= CPU_LZCNT;\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.sse4a != 0)\n+      result |= CPU_SSE4A;\n+  }\n+\n+  \/\/ Intel features.\n+  if (is_intel()) {\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n+      result |= CPU_LZCNT;\n+    }\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n+      result |= CPU_3DNOW_PREFETCH;\n+    }\n+    if (_cpuid_info.sef_cpuid7_ebx.bits.clwb != 0) {\n+      result |= CPU_CLWB;\n+    }\n+    if (_cpuid_info.sef_cpuid7_edx.bits.serialize != 0)\n+      result |= CPU_SERIALIZE;\n+  }\n+\n+  \/\/ ZX features.\n+  if (is_zx()) {\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.lzcnt != 0) {\n+      result |= CPU_LZCNT;\n+    }\n+    if (_cpuid_info.ext_cpuid1_ecx.bits.prefetchw != 0) {\n+      result |= CPU_3DNOW_PREFETCH;\n+    }\n+  }\n+\n+  \/\/ Composite features.\n+  if (supports_tscinv_bit() &&\n+      ((is_amd_family() && !is_amd_Barcelona()) ||\n+       is_intel_tsc_synched_at_init())) {\n+    result |= CPU_TSCINV;\n+  }\n+\n+  return result;\n+}\n+\n+bool VM_Version::os_supports_avx_vectors() {\n+  bool retVal = false;\n+  int nreg = 2 LP64_ONLY(+2);\n+  if (supports_evex()) {\n+    \/\/ Verify that OS save\/restore all bits of EVEX registers\n+    \/\/ during signal processing.\n+    retVal = true;\n+    for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n+      if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n+        retVal = false;\n+        break;\n+      }\n+    }\n+  } else if (supports_avx()) {\n+    \/\/ Verify that OS save\/restore all bits of AVX registers\n+    \/\/ during signal processing.\n+    retVal = true;\n+    for (int i = 0; i < 8 * nreg; i++) { \/\/ 32 bytes per ymm register\n+      if (_cpuid_info.ymm_save[i] != ymm_test_value()) {\n+        retVal = false;\n+        break;\n+      }\n+    }\n+    \/\/ zmm_save will be set on a EVEX enabled machine even if we choose AVX code gen\n+    if (retVal == false) {\n+      \/\/ Verify that OS save\/restore all bits of EVEX registers\n+      \/\/ during signal processing.\n+      retVal = true;\n+      for (int i = 0; i < 16 * nreg; i++) { \/\/ 64 bytes per zmm register\n+        if (_cpuid_info.zmm_save[i] != ymm_test_value()) {\n+          retVal = false;\n+          break;\n+        }\n+      }\n+    }\n+  }\n+  return retVal;\n+}\n+\n+uint VM_Version::cores_per_cpu() {\n+  uint result = 1;\n+  if (is_intel()) {\n+    bool supports_topology = supports_processor_topology();\n+    if (supports_topology) {\n+      result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n+               _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+    }\n+    if (!supports_topology || result == 0) {\n+      result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n+    }\n+  } else if (is_amd_family()) {\n+    result = (_cpuid_info.ext_cpuid8_ecx.bits.cores_per_cpu + 1);\n+  } else if (is_zx()) {\n+    bool supports_topology = supports_processor_topology();\n+    if (supports_topology) {\n+      result = _cpuid_info.tpl_cpuidB1_ebx.bits.logical_cpus \/\n+               _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+    }\n+    if (!supports_topology || result == 0) {\n+      result = (_cpuid_info.dcp_cpuid4_eax.bits.cores_per_cpu + 1);\n+    }\n+  }\n+  return result;\n+}\n+\n+uint VM_Version::threads_per_core() {\n+  uint result = 1;\n+  if (is_intel() && supports_processor_topology()) {\n+    result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+  } else if (is_zx() && supports_processor_topology()) {\n+    result = _cpuid_info.tpl_cpuidB0_ebx.bits.logical_cpus;\n+  } else if (_cpuid_info.std_cpuid1_edx.bits.ht != 0) {\n+    if (cpu_family() >= 0x17) {\n+      result = _cpuid_info.ext_cpuid1E_ebx.bits.threads_per_core + 1;\n+    } else {\n+      result = _cpuid_info.std_cpuid1_ebx.bits.threads_per_cpu \/\n+                 cores_per_cpu();\n+    }\n+  }\n+  return (result == 0 ? 1 : result);\n+}\n+\n+intx VM_Version::L1_line_size() {\n+  intx result = 0;\n+  if (is_intel()) {\n+    result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n+  } else if (is_amd_family()) {\n+    result = _cpuid_info.ext_cpuid5_ecx.bits.L1_line_size;\n+  } else if (is_zx()) {\n+    result = (_cpuid_info.dcp_cpuid4_ebx.bits.L1_line_size + 1);\n+  }\n+  if (result < 32) \/\/ not defined ?\n+    result = 32;   \/\/ 32 bytes by default on x86 and other x64\n+  return result;\n+}\n+\n+bool VM_Version::is_intel_tsc_synched_at_init() {\n+  if (is_intel_family_core()) {\n+    uint32_t ext_model = extended_cpu_model();\n+    if (ext_model == CPU_MODEL_NEHALEM_EP     ||\n+        ext_model == CPU_MODEL_WESTMERE_EP    ||\n+        ext_model == CPU_MODEL_SANDYBRIDGE_EP ||\n+        ext_model == CPU_MODEL_IVYBRIDGE_EP) {\n+      \/\/ <= 2-socket invariant tsc support. EX versions are usually used\n+      \/\/ in > 2-socket systems and likely don't synchronize tscs at\n+      \/\/ initialization.\n+      \/\/ Code that uses tsc values must be prepared for them to arbitrarily\n+      \/\/ jump forward or backward.\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+intx VM_Version::allocate_prefetch_distance(bool use_watermark_prefetch) {\n+  \/\/ Hardware prefetching (distance\/size in bytes):\n+  \/\/ Pentium 3 -  64 \/  32\n+  \/\/ Pentium 4 - 256 \/ 128\n+  \/\/ Athlon    -  64 \/  32 ????\n+  \/\/ Opteron   - 128 \/  64 only when 2 sequential cache lines accessed\n+  \/\/ Core      - 128 \/  64\n+  \/\/\n+  \/\/ Software prefetching (distance in bytes \/ instruction with best score):\n+  \/\/ Pentium 3 - 128 \/ prefetchnta\n+  \/\/ Pentium 4 - 512 \/ prefetchnta\n+  \/\/ Athlon    - 128 \/ prefetchnta\n+  \/\/ Opteron   - 256 \/ prefetchnta\n+  \/\/ Core      - 256 \/ prefetchnta\n+  \/\/ It will be used only when AllocatePrefetchStyle > 0\n+\n+  if (is_amd_family()) { \/\/ AMD | Hygon\n+    if (supports_sse2()) {\n+      return 256; \/\/ Opteron\n+    } else {\n+      return 128; \/\/ Athlon\n+    }\n+  } else { \/\/ Intel\n+    if (supports_sse3() && cpu_family() == 6) {\n+      if (supports_sse4_2() && supports_ht()) { \/\/ Nehalem based cpus\n+        return 192;\n+      } else if (use_watermark_prefetch) { \/\/ watermark prefetching on Core\n+#ifdef _LP64\n+        return 384;\n+#else\n+        return 320;\n+#endif\n+      }\n+    }\n+    if (supports_sse2()) {\n+      if (cpu_family() == 6) {\n+        return 256; \/\/ Pentium M, Core, Core2\n+      } else {\n+        return 512; \/\/ Pentium 4\n+      }\n+    } else {\n+      return 128; \/\/ Pentium 3 (and all other old CPUs)\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/vm_version_x86.cpp","additions":347,"deletions":5,"binary":false,"changes":352,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-\/\/ archtecture.\n+\/\/ architecture.\n@@ -1244,0 +1244,5 @@\n+static inline bool is_vector_popcount_predicate(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n@@ -1248,0 +1253,5 @@\n+static inline bool is_clz_non_subword_predicate_evex(BasicType bt, int vlen_bytes) {\n+  return is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd() &&\n+           (VM_Version::supports_avx512vl() || vlen_bytes == 64);\n+}\n+\n@@ -1408,1 +1418,1 @@\n-      if (!UsePopCountInstruction || (UseAVX < 2)) {\n+      if (UseAVX < 2) {\n@@ -1413,1 +1423,1 @@\n-      if (!UsePopCountInstruction || (UseAVX <= 2)) {\n+      if (UseAVX < 2) {\n@@ -1455,0 +1465,6 @@\n+    case Op_IsInfiniteF:\n+    case Op_IsInfiniteD:\n+      if (!VM_Version::supports_avx512dq()) {\n+        return false;\n+      }\n+      break;\n@@ -1471,0 +1487,15 @@\n+    case Op_PopulateIndex:\n+      if (!is_LP64 || (UseAVX < 2)) {\n+        return false;\n+      }\n+      break;\n+    case Op_RoundVF:\n+      if (UseAVX < 2) { \/\/ enabled for AVX2 only\n+        return false;\n+      }\n+      break;\n+    case Op_RoundVD:\n+      if (UseAVX < 3) {\n+        return false;  \/\/ enabled for AVX3 only\n+      }\n+      break;\n@@ -1561,2 +1592,0 @@\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n@@ -1575,0 +1604,6 @@\n+    case Op_RoundF:\n+    case Op_RoundD:\n+      if (!is_LP64) {\n+        return false;\n+      }\n+      break;\n@@ -1602,0 +1637,10 @@\n+    case Op_CompressBits:\n+      if (!VM_Version::supports_bmi2() || (!is_LP64 && UseSSE < 2)) {\n+        return false;\n+      }\n+      break;\n+    case Op_ExpandBits:\n+      if (!VM_Version::supports_bmi2() || (!is_LP64 && (UseSSE < 2 || !VM_Version::supports_bmi1()))) {\n+        return false;\n+      }\n+      break;\n@@ -1612,0 +1657,11 @@\n+    case Op_CompressM:\n+      if (!VM_Version::supports_avx512vl() || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+      if (!VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -1633,0 +1689,9 @@\n+static inline bool is_pop_count_instr_target(BasicType bt) {\n+  return (is_subword_type(bt) && VM_Version::supports_avx512_bitalg()) ||\n+         (is_non_subword_integral_type(bt) && VM_Version::supports_avx512_vpopcntdq());\n+}\n+\n+const bool Matcher::match_rule_supported_superword(int opcode, int vlen, BasicType bt) {\n+  return match_rule_supported_vector(opcode, vlen, bt);\n+}\n+\n@@ -1689,2 +1754,0 @@\n-    case Op_LoadVectorMasked:\n-    case Op_StoreVectorMasked:\n@@ -1698,0 +1761,6 @@\n+    case Op_LoadVectorMasked:\n+    case Op_StoreVectorMasked:\n+      if (!VM_Version::supports_avx512bw() && (is_subword_type(bt) || UseAVX < 1)) {\n+        return false;\n+      }\n+      break;\n@@ -1798,0 +1867,5 @@\n+    case Op_PopulateIndex:\n+      if (size_in_bits > 256 && !VM_Version::supports_avx512bw()) {\n+        return false;\n+      }\n+      break;\n@@ -1813,2 +1887,9 @@\n-      if (is_subword_type(bt) || bt == T_INT) {\n-        return false;\n+      \/\/ Conversion to integral type is only supported on AVX-512 platforms with avx512dq.\n+      \/\/ Need avx512vl for size_in_bits < 512\n+      if (is_integral_type(bt)) {\n+        if (!VM_Version::supports_avx512dq()) {\n+          return false;\n+        }\n+        if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+          return false;\n+        }\n@@ -1816,1 +1897,3 @@\n-      if (bt == T_LONG && !VM_Version::supports_avx512dq()) {\n+      break;\n+    case Op_RoundVD:\n+      if (!VM_Version::supports_avx512dq()) {\n@@ -1821,2 +1904,14 @@\n-      if (is_subword_type(bt) || bt == T_LONG) {\n-        return false;\n+      \/\/ F2I is supported on all AVX and above platforms\n+      \/\/ For conversion to other integral types need AVX512:\n+      \/\/     Conversion to long in addition needs avx512dq\n+      \/\/     Need avx512vl for size_in_bits < 512\n+      if (is_integral_type(bt) && (bt != T_INT)) {\n+        if (UseAVX <= 2) {\n+          return false;\n+        }\n+        if ((bt == T_LONG) && !VM_Version::supports_avx512dq()) {\n+          return false;\n+        }\n+        if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+          return false;\n+        }\n@@ -1833,1 +1928,1 @@\n-      if(is_subword_type(bt)) {\n+      if (is_subword_type(bt)) {\n@@ -1860,0 +1955,17 @@\n+    case Op_CompressM:\n+      if (UseAVX < 3 || !VM_Version::supports_bmi2()) {\n+        return false;\n+      }\n+      break;\n+    case Op_CompressV:\n+    case Op_ExpandV:\n+      if (is_subword_type(bt) && !VM_Version::supports_avx512_vbmi2()) {\n+        return false;\n+      }\n+      if (size_in_bits < 128 ) {\n+        return false;\n+      }\n+      if (size_in_bits < 512 && !VM_Version::supports_avx512vl()) {\n+        return false;\n+      }\n+      break;\n@@ -1868,0 +1980,6 @@\n+    case Op_SignumVD:\n+    case Op_SignumVF:\n+      if (UseAVX < 1) {\n+        return false;\n+      }\n+      break;\n@@ -1869,2 +1987,10 @@\n-      if (!VM_Version::supports_avx512_vpopcntdq() &&\n-          (vlen == 16) && !VM_Version::supports_avx512bw()) {\n+    case Op_PopCountVL: {\n+        if (!is_pop_count_instr_target(bt) &&\n+            (size_in_bits == 512) && !VM_Version::supports_avx512bw()) {\n+          return false;\n+        }\n+      }\n+      break;\n+    case Op_ReverseV:\n+    case Op_ReverseBytesV:\n+      if (UseAVX < 2) {\n@@ -1874,3 +2000,3 @@\n-    case Op_PopCountVL:\n-      if (!VM_Version::supports_avx512_vpopcntdq() &&\n-          ((vlen <= 4) || ((vlen == 8) && !VM_Version::supports_avx512bw()))) {\n+    case Op_CountTrailingZerosV:\n+    case Op_CountLeadingZerosV:\n+      if (UseAVX < 2) {\n@@ -2024,0 +2150,7 @@\n+    case Op_PopCountVI:\n+    case Op_PopCountVL:\n+      if (!is_pop_count_instr_target(bt)) {\n+        return false;\n+      }\n+      return true;\n+\n@@ -2027,0 +2160,4 @@\n+    case Op_CountLeadingZerosV:\n+      if (is_non_subword_integral_type(bt) && VM_Version::supports_avx512cd()) {\n+        return true;\n+      }\n@@ -2032,0 +2169,4 @@\n+const bool Matcher::vector_needs_partial_operations(Node* node, const TypeVect* vt) {\n+  return false;\n+}\n+\n@@ -2640,0 +2781,1 @@\n+    C2_MacroAssembler _masm(&cbuf);\n@@ -2643,1 +2785,0 @@\n-      C2_MacroAssembler _masm(&cbuf);\n@@ -2864,17 +3005,0 @@\n-\/\/ =================================EVEX special===============================\n-\/\/ Existing partial implementation for post-loop multi-versioning computes\n-\/\/ the mask corresponding to tail loop in K1 opmask register. This may then be\n-\/\/ used for predicating instructions in loop body during last post-loop iteration.\n-\/\/ TODO: Remove hard-coded K1 usage while fixing existing post-loop\n-\/\/ multiversioning support.\n-instruct setMask(rRegI dst, rRegI src, kReg_K1 mask) %{\n-  predicate(PostLoopMultiversioning && Matcher::has_predicated_vectors());\n-  match(Set dst (SetVectMaskI  src));\n-  effect(TEMP dst);\n-  format %{ \"setvectmask   $dst, $src\" %}\n-  ins_encode %{\n-    __ setvectmask($dst$$Register, $src$$Register, $mask$$KRegister);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -6124,0 +6248,30 @@\n+instruct signumV_reg_avx(vec dst, vec src, vec zero, vec one, vec xtmp1) %{\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n) <= 32);\n+  match(Set dst (SignumVF src (Binary zero one)));\n+  match(Set dst (SignumVD src (Binary zero one)));\n+  effect(TEMP dst, TEMP xtmp1);\n+  format %{ \"vector_signum_avx $dst, $src\\t! using $xtmp1 as TEMP\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    int vec_enc = vector_length_encoding(this);\n+    __ vector_signum_avx(opcode, $dst$$XMMRegister, $src$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister,\n+                         $xtmp1$$XMMRegister, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct signumV_reg_evex(vec dst, vec src, vec zero, vec one, kReg ktmp1) %{\n+  predicate(VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64);\n+  match(Set dst (SignumVF src (Binary zero one)));\n+  match(Set dst (SignumVD src (Binary zero one)));\n+  effect(TEMP dst, TEMP ktmp1);\n+  format %{ \"vector_signum_evex $dst, $src\\t! using $ktmp1 as TEMP\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    int vec_enc = vector_length_encoding(this);\n+    __ vector_signum_evex(opcode, $dst$$XMMRegister, $src$$XMMRegister, $zero$$XMMRegister, $one$$XMMRegister,\n+                          $ktmp1$$KRegister, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6169,0 +6323,1 @@\n+\n@@ -6171,0 +6326,42 @@\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsI_reg(rRegI dst, rRegI src, rRegI mask) %{\n+  predicate(n->bottom_type()->isa_int());\n+  match(Set dst (CompressBits src mask));\n+  format %{ \"pextl  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextl($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsI_reg(rRegI dst, rRegI src, rRegI mask) %{\n+  predicate(n->bottom_type()->isa_int());\n+  match(Set dst (ExpandBits src mask));\n+  format %{ \"pdepl  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepl($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct compressBitsI_mem(rRegI dst, rRegI src, memory mask) %{\n+  predicate(n->bottom_type()->isa_int());\n+  match(Set dst (CompressBits src (LoadI mask)));\n+  format %{ \"pextl  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextl($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsI_mem(rRegI dst, rRegI src, memory mask) %{\n+  predicate(n->bottom_type()->isa_int());\n+  match(Set dst (ExpandBits src (LoadI mask)));\n+  format %{ \"pdepl  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepl($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -6698,1 +6895,1 @@\n-    \/\/ Shift lower half, with result in vtmp2 usign vtmp1 as TEMP\n+    \/\/ Shift lower half, with result in vtmp2 using vtmp1 as TEMP\n@@ -6704,1 +6901,1 @@\n-    \/\/ Shift upper half, with result in dst usign vtmp1 as TEMP\n+    \/\/ Shift upper half, with result in dst using vtmp1 as TEMP\n@@ -6917,22 +7114,1 @@\n-    switch (to_elem_bt) {\n-      case T_SHORT:\n-        __ vpmovsxbw($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-        break;\n-      case T_INT:\n-        __ vpmovsxbd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-        break;\n-      case T_FLOAT:\n-        __ vpmovsxbd($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-        __ vcvtdq2ps($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-        break;\n-      case T_LONG:\n-        __ vpmovsxbq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n-        break;\n-      case T_DOUBLE: {\n-        int mid_vlen_enc = (vlen_enc == Assembler::AVX_512bit) ? Assembler::AVX_256bit : Assembler::AVX_128bit;\n-        __ vpmovsxbd($dst$$XMMRegister, $src$$XMMRegister, mid_vlen_enc);\n-        __ vcvtdq2pd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n-        break;\n-      }\n-      default: assert(false, \"%s\", type2name(to_elem_bt));\n-    }\n+    __ vconvert_b2x(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n@@ -7212,1 +7388,4 @@\n-instruct vcastFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+\n+instruct castFtoI_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+  \/\/ F2I conversion for < 64 byte vector using AVX instructions\n+  \/\/ AVX512 platforms that dont support avx512vl also use AVX instructions to support F2I\n@@ -7218,1 +7397,1 @@\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3 and $xtmp4 as TEMP\" %}\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $scratch as TEMP\" %}\n@@ -7228,1 +7407,1 @@\n-instruct vcastFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+instruct castFtoI_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n@@ -7234,1 +7413,1 @@\n-  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP\" %}\n+  format %{ \"vector_cast_f2i $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n@@ -7244,0 +7423,31 @@\n+instruct castFtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+  \/\/ F2X conversion for integral non T_INT target using AVX512 instructions\n+  \/\/ Platforms that dont support avx512vl can only support 64 byte vectors\n+  predicate(is_integral_type(Matcher::vector_element_basic_type(n)) &&\n+            Matcher::vector_element_basic_type(n) != T_INT);\n+  match(Set dst (VectorCastF2X src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n+  format %{ \"vector_cast_f2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  ins_encode %{\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    if (to_elem_bt == T_LONG) {\n+      int vlen_enc = vector_length_encoding(this);\n+      __ vector_castF2L_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                             $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                             ExternalAddress(vector_double_signflip()), $scratch$$Register, vlen_enc);\n+    } else {\n+      int vlen_enc = vector_length_encoding(this, $src);\n+      __ vector_castF2I_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                             $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                             ExternalAddress(vector_float_signflip()), $scratch$$Register, vlen_enc);\n+      if (to_elem_bt == T_SHORT) {\n+        __ evpmovdw($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      } else {\n+        assert(to_elem_bt == T_BYTE, \"required\");\n+        __ evpmovdb($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      }\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -7255,2 +7465,2 @@\n-instruct vcastDtoL_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n-  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+instruct castDtoX_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+  predicate(is_integral_type(Matcher::vector_element_basic_type(n)));\n@@ -7259,1 +7469,1 @@\n-  format %{ \"vector_cast_d2l $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1 and $ktmp2 as TEMP\" %}\n+  format %{ \"vector_cast_d2x $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n@@ -7261,2 +7471,3 @@\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vector_castD2L_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType to_elem_bt = Matcher::vector_element_basic_type(this);\n+    __ vector_castD2X_evex(to_elem_bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n@@ -7285,0 +7496,50 @@\n+#ifdef _LP64\n+instruct vround_float_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, rRegP scratch, rFlagsReg cr) %{\n+  predicate(!VM_Version::supports_avx512vl() &&\n+            Matcher::vector_length_in_bytes(n) < 64 &&\n+            Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP scratch, KILL cr);\n+  format %{ \"vector_round_float $dst,$src\\t! using $xtmp1, $xtmp2, $xtmp3, $xtmp4 and $scratch as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    __ vector_round_float_avx($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                              $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister,\n+                              ExternalAddress(vector_float_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vround_float_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+  predicate((VM_Version::supports_avx512vl() ||\n+             Matcher::vector_length_in_bytes(n) == 64) &&\n+             Matcher::vector_element_basic_type(n) == T_INT);\n+  match(Set dst (RoundVF src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n+  format %{ \"vector_round_float $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    __ vector_round_float_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                               $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                               ExternalAddress(vector_float_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vround_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, kReg ktmp1, kReg ktmp2, rRegP scratch, rFlagsReg cr) %{\n+  predicate(Matcher::vector_element_basic_type(n) == T_LONG);\n+  match(Set dst (RoundVD src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP ktmp1, TEMP ktmp2, TEMP scratch, KILL cr);\n+  format %{ \"vector_round_long $dst,$src\\t! using $xtmp1, $xtmp2, $ktmp1, $ktmp2 and $scratch as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this);\n+    InternalAddress new_mxcsr = $constantaddress((jint)0x3F80);\n+    __ vector_round_double_evex($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                $xtmp2$$XMMRegister, $ktmp1$$KRegister, $ktmp2$$KRegister,\n+                                ExternalAddress(vector_double_signflip()), new_mxcsr, $scratch$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n@@ -8220,0 +8481,39 @@\n+#ifdef _LP64\n+instruct VectorPopulateIndex(vec dst, rRegI src1, immI_1 src2, vec vtmp, rRegP scratch) %{\n+  match(Set dst (PopulateIndex src1 src2));\n+  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp and $scratch as TEMP\" %}\n+  ins_encode %{\n+     assert($src2$$constant == 1, \"required\");\n+     int vlen = Matcher::vector_length(this);\n+     int vlen_enc = vector_length_encoding(this);\n+     BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+     __ vpbroadcast(elem_bt, $vtmp$$XMMRegister, $src1$$Register, vlen_enc);\n+     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen);\n+     if (elem_bt != T_BYTE) {\n+       __ vconvert_b2x(elem_bt, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+     }\n+     __ vpadd(elem_bt, $dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct VectorPopulateLIndex(vec dst, rRegL src1, immI_1 src2, vec vtmp, rRegP scratch) %{\n+  match(Set dst (PopulateIndex src1 src2));\n+  effect(TEMP dst, TEMP vtmp, TEMP scratch);\n+  format %{ \"vector_populate_index $dst $src1 $src2\\t! using $vtmp and $scratch as TEMP\" %}\n+  ins_encode %{\n+     assert($src2$$constant == 1, \"required\");\n+     int vlen = Matcher::vector_length(this);\n+     int vlen_enc = vector_length_encoding(this);\n+     BasicType elem_bt = Matcher::vector_element_basic_type(this);\n+     __ vpbroadcast(elem_bt, $vtmp$$XMMRegister, $src1$$Register, vlen_enc);\n+     __ load_iota_indices($dst$$XMMRegister, $scratch$$Register, vlen);\n+     if (elem_bt != T_BYTE) {\n+       __ vconvert_b2x(elem_bt, $dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+     }\n+     __ vpadd(elem_bt, $dst$$XMMRegister, $dst$$XMMRegister, $vtmp$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+#endif\n@@ -8626,2 +8926,2 @@\n-instruct vpopcountI_popcntd(vec dst, vec src) %{\n-  predicate(VM_Version::supports_avx512_vpopcntdq());\n+instruct vpopcount_integral_reg_evex(vec dst, vec src) %{\n+  predicate(is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));\n@@ -8629,1 +8929,3 @@\n-  format %{ \"vector_popcount_int $dst, $src\\t! vector popcount packedI\" %}\n+  match(Set dst (PopCountVL src));\n+  ins_cost(400);\n+  format %{ \"vector_popcount_integral $dst, $src\" %}\n@@ -8631,3 +8933,10 @@\n-    assert(UsePopCountInstruction, \"not enabled\");\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vector_popcount_int($dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg, xnoreg, noreg, vlen_enc);\n+    int opcode = this->ideal_Opcode();\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_popcount_integral_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, k0, true, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (opcode == Op_PopCountVL && Matcher::vector_element_basic_type(this) == T_INT) {\n+      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -8638,5 +8947,5 @@\n-instruct vpopcountI(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp, rFlagsReg cc) %{\n-  predicate(!VM_Version::supports_avx512_vpopcntdq());\n-  match(Set dst (PopCountVI src));\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, KILL cc);\n-  format %{ \"vector_popcount_int  $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n+instruct vpopcount_integral_reg_evex_masked(vec dst, vec src, kReg mask) %{\n+  predicate(is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));\n+  match(Set dst (PopCountVI src mask));\n+  match(Set dst (PopCountVL src mask));\n+  format %{ \"vector_popcount_integral_masked $dst, $src, $mask\" %}\n@@ -8644,4 +8953,4 @@\n-    assert(UsePopCountInstruction, \"not enabled\");\n-    int vlen_enc = vector_length_encoding(this);\n-    __ vector_popcount_int($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister,\n-                           $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+    __ vector_popcount_integral_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $mask$$KRegister, true, vlen_enc);\n@@ -8652,2 +8961,3 @@\n-instruct vpopcountL_popcntd(vec dst, vec src) %{\n-  predicate(VM_Version::supports_avx512_vpopcntdq());\n+instruct vpopcount_avx_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegP rtmp) %{\n+  predicate(!is_vector_popcount_predicate(Matcher::vector_element_basic_type(n->in(1))));\n+  match(Set dst (PopCountVI src));\n@@ -8655,1 +8965,33 @@\n-  format %{ \"vector_popcount_long  $dst, $src\\t! vector popcount packedL\" %}\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);\n+  format %{ \"vector_popcount_integral $dst, $src\\t! using $xtmp1, $xtmp2, and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_popcount_integral(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                $xtmp2$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (opcode == Op_PopCountVL && Matcher::vector_element_basic_type(this) == T_INT) {\n+      if (VM_Version::supports_avx512vl()) {\n+        __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+      } else {\n+        assert(VM_Version::supports_avx2(), \"\");\n+        __ vpshufd($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n+        __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n+      }\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ --------------------------------- Vector Trailing Zeros Count --------------------------------------\n+\n+instruct vcount_trailing_zeros_reg_evex(vec dst, vec src, vec xtmp, rRegP rtmp) %{\n+  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),\n+                                              Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (CountTrailingZerosV src));\n+  effect(TEMP dst, TEMP xtmp, TEMP rtmp);\n+  ins_cost(400);\n+  format %{ \"vector_count_trailing_zeros $dst, $src!\\t using $xtmp and $rtmp as TEMP\" %}\n@@ -8657,2 +8999,10 @@\n-    assert(UsePopCountInstruction, \"not enabled\");\n-    __ vector_popcount_long($dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg, xnoreg, noreg, vlen_enc);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType rbt = Matcher::vector_element_basic_type(this);\n+    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg,\n+                                        xnoreg, xnoreg, $xtmp$$XMMRegister, k0, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountTrailingZerosV\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (bt == T_LONG && rbt == T_INT) {\n+      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n@@ -8664,5 +9014,38 @@\n-instruct vpopcountL(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp, rFlagsReg cc) %{\n-  predicate(!VM_Version::supports_avx512_vpopcntdq());\n-  match(Set dst (PopCountVL src));\n-  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp, KILL cc);\n-  format %{ \"vector_popcount_long  $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n+instruct vcount_trailing_zeros_short_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&\n+            VM_Version::supports_avx512cd() &&\n+            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64));\n+  match(Set dst (CountTrailingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);\n+  ins_cost(400);\n+  format %{ \"vector_count_trailing_zeros $dst, $src!\\t using $xtmp1, $xtmp2, $xtmp3 and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                        $xtmp2$$XMMRegister, xnoreg, $xtmp3$$XMMRegister, k0, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_trailing_zeros_byte_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, vec xtmp4, kReg ktmp, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_BYTE && VM_Version::supports_avx512vlbw());\n+  match(Set dst (CountTrailingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP xtmp4, TEMP ktmp, TEMP rtmp);\n+  ins_cost(400);\n+  format %{ \"vector_count_trailing_zeros $dst, $src!\\t using $xtmp1, $xtmp2, $xtmp3, $xtmp4, $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_trailing_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                        $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $xtmp4$$XMMRegister,\n+                                        $ktmp$$KRegister, $rtmp$$Register, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_trailing_zeros_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{\n+  predicate(!VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);\n+  match(Set dst (CountTrailingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);\n+  format %{ \"vector_count_trailing_zeros $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n@@ -8670,3 +9053,12 @@\n-    assert(UsePopCountInstruction, \"not enabled\");\n-    __ vector_popcount_long($dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister, $xtmp2$$XMMRegister,\n-                           $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType rbt = Matcher::vector_element_basic_type(this);\n+    __ vector_count_trailing_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, PopCountVL\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (bt == T_LONG && rbt == T_INT) {\n+      assert(VM_Version::supports_avx2(), \"\");\n+      __ vpshufd($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n+      __ vpermq($dst$$XMMRegister, $dst$$XMMRegister, 8, vlen_enc);\n+    }\n@@ -8678,0 +9070,1 @@\n+\n@@ -8730,1 +9123,37 @@\n-#ifdef _LP64\n+instruct vmasked_load_avx_non_subword(vec dst, memory mem, vec mask) %{\n+  predicate(!n->in(3)->bottom_type()->isa_vectmask());\n+  match(Set dst (LoadVectorMasked mem mask));\n+  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n+  ins_encode %{\n+    BasicType elmType = this->bottom_type()->is_vect()->element_basic_type();\n+    int vlen_enc = vector_length_encoding(this);\n+    __ vmovmask(elmType, $dst$$XMMRegister, $mem$$Address, $mask$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\n+instruct vmasked_load_evex(vec dst, memory mem, kReg mask) %{\n+  predicate(n->in(3)->bottom_type()->isa_vectmask());\n+  match(Set dst (LoadVectorMasked mem mask));\n+  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n+  ins_encode %{\n+    BasicType elmType =  this->bottom_type()->is_vect()->element_basic_type();\n+    int vector_len = vector_length_encoding(this);\n+    __ evmovdqu(elmType, $mask$$KRegister, $dst$$XMMRegister, $mem$$Address, false, vector_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vmasked_store_avx_non_subword(memory mem, vec src, vec mask) %{\n+  predicate(!n->in(3)->in(2)->bottom_type()->isa_vectmask());\n+  match(Set mem (StoreVectorMasked mem (Binary src mask)));\n+  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n+  ins_encode %{\n+    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n+    int vlen_enc = vector_length_encoding(src_node);\n+    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n+    __ vmovmask(elmType, $mem$$Address, $src$$XMMRegister, $mask$$XMMRegister, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -8733,0 +9162,14 @@\n+instruct vmasked_store_evex(memory mem, vec src, kReg mask) %{\n+  predicate(n->in(3)->in(2)->bottom_type()->isa_vectmask());\n+  match(Set mem (StoreVectorMasked mem (Binary src mask)));\n+  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n+  ins_encode %{\n+    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n+    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n+    int vlen_enc = vector_length_encoding(src_node);\n+    __ evmovdqu(elmType, $mask$$KRegister, $mem$$Address, $src$$XMMRegister, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+#ifdef _LP64\n@@ -8759,11 +9202,0 @@\n-instruct vmasked_load64(vec dst, memory mem, kReg mask) %{\n-  match(Set dst (LoadVectorMasked mem mask));\n-  format %{ \"vector_masked_load $dst, $mem, $mask \\t! vector masked copy\" %}\n-  ins_encode %{\n-    BasicType elmType =  this->bottom_type()->is_vect()->element_basic_type();\n-    int vector_len = vector_length_encoding(this);\n-    __ evmovdqu(elmType, $mask$$KRegister, $dst$$XMMRegister, $mem$$Address, vector_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -8791,12 +9223,0 @@\n-instruct vmasked_store64(memory mem, vec src, kReg mask) %{\n-  match(Set mem (StoreVectorMasked mem (Binary src mask)));\n-  format %{ \"vector_masked_store $mem, $src, $mask \\t! vector masked store\" %}\n-  ins_encode %{\n-    const MachNode* src_node = static_cast<const MachNode*>(this->in(this->operand_index($src)));\n-    BasicType elmType =  src_node->bottom_type()->is_vect()->element_basic_type();\n-    int vector_len = vector_length_encoding(src_node);\n-    __ evmovdqu(elmType, $mask$$KRegister, $mem$$Address, $src$$XMMRegister, vector_len);\n-  %}\n-  ins_pipe( pipe_slow );\n-%}\n-\n@@ -8952,0 +9372,28 @@\n+\n+\/\/ --------------------------------- Compress\/Expand Operations ---------------------------\n+\n+instruct vcompress_expand_reg_evex(vec dst, vec src, kReg mask) %{\n+  match(Set dst (CompressV src mask));\n+  match(Set dst (ExpandV src mask));\n+  format %{ \"vector_compress_expand $dst, $src, $mask\" %}\n+  ins_encode %{\n+    int opcode = this->ideal_Opcode();\n+    int vector_len = vector_length_encoding(this);\n+    BasicType bt  = Matcher::vector_element_basic_type(this);\n+    __ vector_compress_expand(opcode, $dst$$XMMRegister, $src$$XMMRegister, $mask$$KRegister, false, bt, vector_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcompress_mask_reg_evex(kReg dst, kReg mask, rRegL rtmp1, rRegL rtmp2, rFlagsReg cr) %{\n+  match(Set dst (CompressM mask));\n+  effect(TEMP rtmp1, TEMP rtmp2, KILL cr);\n+  format %{ \"mask_compress_evex $dst, $mask\\t! using $rtmp1 and $rtmp2 as TEMP\" %}\n+  ins_encode %{\n+    assert(this->in(1)->bottom_type()->isa_vectmask(), \"\");\n+    int mask_len = Matcher::vector_length(this);\n+    __ vector_mask_compress($dst$$KRegister, $mask$$KRegister, $rtmp1$$Register, $rtmp2$$Register, mask_len);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -8954,0 +9402,164 @@\n+\/\/ -------------------------------- Bit and Byte Reversal Vector Operations ------------------------\n+\n+instruct vreverse_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegI rtmp) %{\n+  predicate(!VM_Version::supports_gfni());\n+  match(Set dst (ReverseV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);\n+  format %{ \"vector_reverse_bit_evex $dst, $src!\\t using $xtmp1, $xtmp2 and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vector_reverse_bit(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                          $xtmp2$$XMMRegister, $rtmp$$Register, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vreverse_reg_gfni(vec dst, vec src, vec xtmp, rRegI rtmp) %{\n+  predicate(VM_Version::supports_gfni());\n+  match(Set dst (ReverseV src));\n+  effect(TEMP dst, TEMP xtmp, TEMP rtmp);\n+  format %{ \"vector_reverse_bit_gfni $dst, $src!\\t using $rtmp and $xtmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_enc = vector_length_encoding(this);\n+    BasicType bt  = Matcher::vector_element_basic_type(this);\n+    InternalAddress addr = $constantaddress(T_LONG, vreplicate_imm(T_LONG, 0x8040201008040201L, 1));\n+    __ vector_reverse_bit_gfni(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp$$XMMRegister,\n+                               addr, $rtmp$$Register, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vreverse_byte_reg(vec dst, vec src, rRegI rtmp) %{\n+  predicate(VM_Version::supports_avx512bw() || Matcher::vector_length_in_bytes(n) < 64);\n+  match(Set dst (ReverseBytesV src));\n+  effect(TEMP dst, TEMP rtmp);\n+  format %{ \"vector_reverse_byte $dst, $src!\\t using $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vector_reverse_byte(bt, $dst$$XMMRegister, $src$$XMMRegister, $rtmp$$Register, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vreverse_byte64_reg(vec dst, vec src, vec xtmp1, vec xtmp2, rRegI rtmp) %{\n+  predicate(!VM_Version::supports_avx512bw() && Matcher::vector_length_in_bytes(n) == 64);\n+  match(Set dst (ReverseBytesV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP rtmp);\n+  format %{ \"vector_reverse_byte $dst, $src!\\t using $xtmp1, $xtmp2 and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vec_enc = vector_length_encoding(this);\n+    BasicType bt = Matcher::vector_element_basic_type(this);\n+    __ vector_reverse_byte64(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                             $xtmp2$$XMMRegister, $rtmp$$Register, vec_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+\/\/ ---------------------------------- Vector Count Leading Zeros -----------------------------------\n+\n+instruct vcount_leading_zeros_IL_reg_evex(vec dst, vec src) %{\n+  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),\n+                                              Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (CountLeadingZerosV src));\n+  format %{ \"vector_count_leading_zeros $dst, $src\" %}\n+  ins_encode %{\n+     int vlen_enc = vector_length_encoding(this, $src);\n+     BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+     BasicType rbt = Matcher::vector_element_basic_type(this);\n+     __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg,\n+                                        xnoreg, xnoreg, k0, noreg, true, vlen_enc);\n+     \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountLeadingZerosV\n+     \/\/ should be succeeded by its corresponding vector IR and following\n+     \/\/ special handling should be removed.\n+     if (rbt == T_INT && bt == T_LONG) {\n+       __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+     }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_IL_reg_evex_masked(vec dst, vec src, kReg mask) %{\n+  predicate(is_clz_non_subword_predicate_evex(Matcher::vector_element_basic_type(n->in(1)),\n+                                              Matcher::vector_length_in_bytes(n->in(1))));\n+  match(Set dst (CountLeadingZerosV src mask));\n+  format %{ \"vector_count_leading_zeros $dst, $src, $mask\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ evmovdquq($dst$$XMMRegister, $src$$XMMRegister, vlen_enc);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, xnoreg, xnoreg,\n+                                       xnoreg, $mask$$KRegister, noreg, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_short_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_SHORT &&\n+            VM_Version::supports_avx512cd() &&\n+            (VM_Version::supports_avx512vl() || Matcher::vector_length_in_bytes(n) == 64));\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2);\n+  format %{ \"vector_count_leading_zeros $dst, $src!\\t using $xtmp1 and $xtmp2 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                       $xtmp2$$XMMRegister, xnoreg, k0, noreg, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_byte_reg_evex(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, kReg ktmp, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_BYTE && VM_Version::supports_avx512vlbw());\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP ktmp, TEMP rtmp);\n+  format %{ \"vector_count_leading_zeros $dst, $src!\\t using $xtmp1, $xtmp2, $xtmp3, $ktmp and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_evex(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                       $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $ktmp$$KRegister,\n+                                       $rtmp$$Register, true, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_int_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) == T_INT &&\n+            !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3);\n+  format %{ \"vector_count_leading_zeros $dst, $src\\t! using $xtmp1, $xtmp2 and $xtmp3 as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    __ vector_count_leading_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                      $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, noreg, vlen_enc);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct vcount_leading_zeros_reg_avx(vec dst, vec src, vec xtmp1, vec xtmp2, vec xtmp3, rRegP rtmp) %{\n+  predicate(Matcher::vector_element_basic_type(n->in(1)) != T_INT &&\n+            !VM_Version::supports_avx512vl() && Matcher::vector_length_in_bytes(n->in(1)) < 64);\n+  match(Set dst (CountLeadingZerosV src));\n+  effect(TEMP dst, TEMP xtmp1, TEMP xtmp2, TEMP xtmp3, TEMP rtmp);\n+  format %{ \"vector_count_leading_zeros $dst, $src\\t! using $xtmp1, $xtmp2, $xtmp3, and $rtmp as TEMP\" %}\n+  ins_encode %{\n+    int vlen_enc = vector_length_encoding(this, $src);\n+    BasicType bt = Matcher::vector_element_basic_type(this, $src);\n+    BasicType rbt = Matcher::vector_element_basic_type(this);\n+    __ vector_count_leading_zeros_avx(bt, $dst$$XMMRegister, $src$$XMMRegister, $xtmp1$$XMMRegister,\n+                                      $xtmp2$$XMMRegister, $xtmp3$$XMMRegister, $rtmp$$Register, vlen_enc);\n+    \/\/ TODO: Once auto-vectorizer supports ConvL2I operation, CountLeadingZerosV\n+    \/\/ should be succeeded by its corresponding vector IR and following\n+    \/\/ special handling should be removed.\n+    if (rbt == T_INT && bt == T_LONG) {\n+      __ evpmovqd($dst$$XMMRegister, $dst$$XMMRegister, vlen_enc);\n+    }\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -9710,0 +10322,26 @@\n+\n+instruct FloatClassCheck_reg_reg_vfpclass(rRegI dst, regF src, kReg ktmp, rFlagsReg cr)\n+%{\n+  match(Set dst (IsInfiniteF src));\n+  effect(TEMP ktmp, KILL cr);\n+  format %{ \"float_class_check $dst, $src\" %}\n+  ins_encode %{\n+    __ vfpclassss($ktmp$$KRegister, $src$$XMMRegister, 0x18);\n+    __ kmovbl($dst$$Register, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct DoubleClassCheck_reg_reg_vfpclass(rRegI dst, regD src, kReg ktmp, rFlagsReg cr)\n+%{\n+  match(Set dst (IsInfiniteD src));\n+  effect(TEMP ktmp, KILL cr);\n+  format %{ \"double_class_check $dst, $src\" %}\n+  ins_encode %{\n+    __ vfpclasssd($ktmp$$KRegister, $src$$XMMRegister, 0x18);\n+    __ kmovbl($dst$$Register, $ktmp$$KRegister);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n","filename":"src\/hotspot\/cpu\/x86\/x86.ad","additions":759,"deletions":121,"binary":false,"changes":880,"status":"modified"},{"patch":"@@ -216,0 +216,1 @@\n+reg_class ebpd_reg( EBP,EDI );\n@@ -263,11 +264,1 @@\n-void reg_mask_init() {\n-  if (Matcher::has_predicated_vectors()) {\n-    \/\/ Post-loop multi-versioning expects mask to be present in K1 register, till the time\n-    \/\/ its fixed, RA should not be allocting K1 register, this shall prevent any accidental\n-    \/\/ curruption of value held in K1 register.\n-    if (PostLoopMultiversioning) {\n-      const_cast<RegMask*>(&_VECTMASK_REG_mask)->Remove(OptoReg::as_OptoReg(k1->as_VMReg()));\n-      const_cast<RegMask*>(&_VECTMASK_REG_mask)->Remove(OptoReg::as_OptoReg(k1->as_VMReg()->next()));\n-    }\n-  }\n-}\n+void reg_mask_init() {}\n@@ -326,5 +317,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  ShouldNotCallThis();\n-  return -1;\n-}\n-\n@@ -625,1 +611,1 @@\n-  MacroAssembler _masm(&cbuf);\n+  C2_MacroAssembler _masm(&cbuf);\n@@ -1754,0 +1740,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -1759,0 +1746,1 @@\n+    __ post_call_nop();\n@@ -1810,0 +1798,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -1817,0 +1806,1 @@\n+      __ post_call_nop();\n@@ -1823,5 +1813,13 @@\n-      \/\/ Emit stubs for static call.\n-      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf);\n-      if (stub == NULL) {\n-        ciEnv::current()->record_failure(\"CodeCache is full\");\n-        return;\n+      __ post_call_nop();\n+      address mark = cbuf.insts_mark();\n+      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n+        \/\/ Calls of the same statically bound method can share\n+        \/\/ a stub to the interpreter.\n+        cbuf.shared_stub_to_interp_for(_method, cbuf.insts()->mark_off());\n+      } else {\n+        \/\/ Emit stubs for static call.\n+        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n+        if (stub == NULL) {\n+          ciEnv::current()->record_failure(\"CodeCache is full\");\n+          return;\n+        }\n@@ -1835,0 +1833,1 @@\n+    __ post_call_nop();\n@@ -1842,0 +1841,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -1846,1 +1846,1 @@\n-\n+    __ post_call_nop();\n@@ -2788,0 +2788,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -2791,0 +2792,1 @@\n+    __ post_call_nop();\n@@ -2807,0 +2809,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -2810,0 +2813,1 @@\n+    __ post_call_nop();\n@@ -2877,0 +2881,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -2881,0 +2886,1 @@\n+    __ post_call_nop();\n@@ -2926,0 +2932,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -2929,0 +2936,1 @@\n+    __ post_call_nop();\n@@ -2968,0 +2976,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -2971,0 +2980,1 @@\n+    __ post_call_nop();\n@@ -3104,1 +3114,1 @@\n-\/\/         are owned by the CALLEE.  Holes should not be nessecary in the\n+\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n@@ -3107,1 +3117,1 @@\n-\/\/         avoid holes.  Holes in the outgoing arguments may be nessecary for\n+\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n@@ -3926,0 +3936,7 @@\n+operand eBDPRegL( eRegL reg ) %{\n+  constraint(ALLOC_IN_RC(ebpd_reg));\n+  match(reg);\n+\n+  format %{ \"EBP:EDI\" %}\n+  interface(REG_INTER);\n+%}\n@@ -4383,1 +4400,1 @@\n-\/\/ Comparision Code\n+\/\/ Comparison Code\n@@ -10827,0 +10844,1 @@\n+    __ post_call_nop();\n@@ -10902,0 +10920,1 @@\n+    __ post_call_nop();\n@@ -10957,0 +10976,1 @@\n+    __ post_call_nop();\n@@ -11033,0 +11053,1 @@\n+    __ post_call_nop();\n@@ -11501,0 +11522,89 @@\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsL_reg(eADXRegL dst, eBCXRegL src, eBDPRegL mask, eSIRegI rtmp, regF xtmp, eFlagsReg cr) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src mask));\n+  effect(TEMP rtmp, TEMP xtmp, KILL cr);\n+  format %{ \"compress_bits $dst, $src, $mask\\t! using $rtmp and $xtmp as TEMP\" %}\n+  ins_encode %{\n+    Label exit, partail_result;\n+    \/\/ Parallely extract both upper and lower 32 bits of source into destination register pair.\n+    \/\/ Merge the results of upper and lower destination registers such that upper destination\n+    \/\/ results are contiguously laid out after the lower destination result.\n+    __ pextl($dst$$Register, $src$$Register, $mask$$Register);\n+    __ pextl(HIGH_FROM_LOW($dst$$Register), HIGH_FROM_LOW($src$$Register), HIGH_FROM_LOW($mask$$Register));\n+    __ popcntl($rtmp$$Register, $mask$$Register);\n+    \/\/ Skip merging if bit count of lower mask register is equal to 32 (register size).\n+    __ cmpl($rtmp$$Register, 32);\n+    __ jccb(Assembler::equal, exit);\n+    \/\/ Due to constraint on number of GPRs on 32 bit target, using XMM register as potential spill slot.\n+    __ movdl($xtmp$$XMMRegister, $rtmp$$Register);\n+    \/\/ Shift left the contents of upper destination register by true bit count of lower mask register\n+    \/\/ and merge with lower destination register.\n+    __ shlxl($rtmp$$Register, HIGH_FROM_LOW($dst$$Register), $rtmp$$Register);\n+    __ orl($dst$$Register, $rtmp$$Register);\n+    __ movdl($rtmp$$Register, $xtmp$$XMMRegister);\n+    \/\/ Zero out upper destination register if true bit count of lower 32 bit mask is zero\n+    \/\/ since contents of upper destination have already been copied to lower destination\n+    \/\/ register.\n+    __ cmpl($rtmp$$Register, 0);\n+    __ jccb(Assembler::greater, partail_result);\n+    __ movl(HIGH_FROM_LOW($dst$$Register), 0);\n+    __ jmp(exit);\n+    __ bind(partail_result);\n+    \/\/ Perform right shift over upper destination register to move out bits already copied\n+    \/\/ to lower destination register.\n+    __ subl($rtmp$$Register, 32);\n+    __ negl($rtmp$$Register);\n+    __ shrxl(HIGH_FROM_LOW($dst$$Register), HIGH_FROM_LOW($dst$$Register), $rtmp$$Register);\n+    __ bind(exit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_reg(eADXRegL dst, eBCXRegL src, eBDPRegL mask, eSIRegI rtmp, regF xtmp, eFlagsReg cr) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src mask));\n+  effect(TEMP rtmp, TEMP xtmp, KILL cr);\n+  format %{ \"expand_bits $dst, $src, $mask\\t! using $rtmp and $xtmp as TEMP\" %}\n+  ins_encode %{\n+    \/\/ Extraction operation sequentially reads the bits from source register starting from LSB\n+    \/\/ and lays them out into destination register at bit locations corresponding to true bits\n+    \/\/ in mask register. Thus number of source bits read are equal to combined true bit count\n+    \/\/ of mask register pair.\n+    Label exit, mask_clipping;\n+    __ pdepl($dst$$Register, $src$$Register, $mask$$Register);\n+    __ pdepl(HIGH_FROM_LOW($dst$$Register), HIGH_FROM_LOW($src$$Register), HIGH_FROM_LOW($mask$$Register));\n+    __ popcntl($rtmp$$Register, $mask$$Register);\n+    \/\/ If true bit count of lower mask register is 32 then none of bit of lower source register\n+    \/\/ will feed to upper destination register.\n+    __ cmpl($rtmp$$Register, 32);\n+    __ jccb(Assembler::equal, exit);\n+    \/\/ Due to constraint on number of GPRs on 32 bit target, using XMM register as potential spill slot.\n+    __ movdl($xtmp$$XMMRegister, $rtmp$$Register);\n+    \/\/ Shift right the contents of lower source register to remove already consumed bits.\n+    __ shrxl($rtmp$$Register, $src$$Register, $rtmp$$Register);\n+    \/\/ Extract the bits from lower source register starting from LSB under the influence\n+    \/\/ of upper mask register.\n+    __ pdepl(HIGH_FROM_LOW($dst$$Register), $rtmp$$Register, HIGH_FROM_LOW($mask$$Register));\n+    __ movdl($rtmp$$Register, $xtmp$$XMMRegister);\n+    __ subl($rtmp$$Register, 32);\n+    __ negl($rtmp$$Register);\n+    __ movdl($xtmp$$XMMRegister, $mask$$Register);\n+    __ movl($mask$$Register, HIGH_FROM_LOW($mask$$Register));\n+    \/\/ Clear the set bits in upper mask register which have been used to extract the contents\n+    \/\/ from lower source register.\n+    __ bind(mask_clipping);\n+    __ blsrl($mask$$Register, $mask$$Register);\n+    __ decrementl($rtmp$$Register, 1);\n+    __ jccb(Assembler::greater, mask_clipping);\n+    \/\/ Starting from LSB extract the bits from upper source register under the influence of\n+    \/\/ remaining set bits in upper mask register.\n+    __ pdepl($rtmp$$Register, HIGH_FROM_LOW($src$$Register), $mask$$Register);\n+    \/\/ Merge the partial results extracted from lower and upper source register bits.\n+    __ orl(HIGH_FROM_LOW($dst$$Register), $rtmp$$Register);\n+    __ movdl($mask$$Register, $xtmp$$XMMRegister);\n+    __ bind(exit);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n@@ -12549,1 +12659,1 @@\n-    \/\/ signed devision: (EAX:EDX) \/ pos_stride\n+    \/\/ signed division: (EAX:EDX) \/ pos_stride\n@@ -12610,1 +12720,0 @@\n-  predicate(!n->has_vector_mask_set());\n@@ -12626,1 +12735,0 @@\n-  predicate(!n->has_vector_mask_set());\n@@ -12641,1 +12749,0 @@\n-  predicate(!n->has_vector_mask_set());\n@@ -12655,60 +12762,0 @@\n-\/\/ mask version\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-\/\/ Bounded mask operand used in following patten is needed for\n-\/\/ post-loop multiversioning.\n-instruct jmpLoopEnd_and_restoreMask(cmpOp cop, kReg_K1 ktmp, eFlagsReg cr, label labl) %{\n-  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n-  match(CountedLoopEnd cop cr);\n-  effect(USE labl, TEMP ktmp);\n-\n-  ins_cost(400);\n-  format %{ \"J$cop    $labl\\t# Loop end\\n\\t\"\n-            \"restorevectmask \\t# vector mask restore for loops\" %}\n-  size(10);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-    __ restorevectmask($ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_jcc );\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-\/\/ Bounded mask operand used in following patten is needed for\n-\/\/ post-loop multiversioning.\n-instruct jmpLoopEndU_and_restoreMask(cmpOpU cop, kReg_K1 ktmp, eFlagsRegU cmp, label labl) %{\n-  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl, TEMP ktmp);\n-\n-  ins_cost(400);\n-  format %{ \"J$cop,u  $labl\\t# Loop end\\n\\t\"\n-            \"restorevectmask \\t# vector mask restore for loops\" %}\n-  size(10);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-    __ restorevectmask($ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_jcc );\n-%}\n-\n-\/\/ Bounded mask operand used in following patten is needed for\n-\/\/ post-loop multiversioning.\n-instruct jmpLoopEndUCF_and_restoreMask(cmpOpUCF cop, kReg_K1 ktmp, eFlagsRegUCF cmp, label labl) %{\n-  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl, TEMP ktmp);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,u  $labl\\t# Loop end\\n\\t\"\n-            \"restorevectmask \\t# vector mask restore for loops\" %}\n-  size(10);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-    __ restorevectmask($ktmp$$KRegister);\n-  %}\n-  ins_pipe( pipe_jcc );\n-%}\n-\n@@ -13966,1 +14013,1 @@\n-\/\/   \/\/ increment preceeded by register-register move\n+\/\/   \/\/ increment preceded by register-register move\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":138,"deletions":91,"binary":false,"changes":229,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-\/\/ archtecture.\n+\/\/ architecture.\n@@ -457,10 +457,0 @@\n-\n-  if (Matcher::has_predicated_vectors()) {\n-    \/\/ Post-loop multi-versioning expects mask to be present in K1 register, till the time\n-    \/\/ its fixed, RA should not be allocting K1 register, this shall prevent any accidental\n-    \/\/ curruption of value held in K1 register.\n-    if (PostLoopMultiversioning) {\n-      const_cast<RegMask*>(&_VECTMASK_REG_mask)->Remove(OptoReg::as_OptoReg(k1->as_VMReg()));\n-      const_cast<RegMask*>(&_VECTMASK_REG_mask)->Remove(OptoReg::as_OptoReg(k1->as_VMReg()->next()));\n-    }\n-  }\n@@ -506,6 +496,0 @@\n-int MachCallNativeNode::ret_addr_offset() {\n-  int offset = 13; \/\/ movq r10,#addr; callq (r10)\n-  offset += clear_avx_size();\n-  return offset;\n-}\n-\n@@ -918,1 +902,1 @@\n-  MacroAssembler _masm(&cbuf);\n+  C2_MacroAssembler _masm(&cbuf);\n@@ -940,1 +924,19 @@\n-    bs->nmethod_entry_barrier(&_masm);\n+ #ifdef _LP64\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+      \/\/ We put the non-hot code of the nmethod entry barrier out-of-line in a stub.\n+      Label dummy_slow_path;\n+      Label dummy_continuation;\n+      Label* slow_path = &dummy_slow_path;\n+      Label* continuation = &dummy_continuation;\n+      if (!Compile::current()->output()->in_scratch_emit_size()) {\n+        \/\/ Use real labels from actual stub when not emitting code for the purpose of measuring its size\n+        C2EntryBarrierStub* stub = Compile::current()->output()->entry_barrier_table()->add_entry_barrier();\n+        slow_path = &stub->slow_path();\n+        continuation = &stub->continuation();\n+      }\n+      bs->nmethod_entry_barrier(&_masm, slow_path, continuation);\n+    }\n+#else\n+    \/\/ Don't bother with out-of-line nmethod entry barrier stub for x86_32.\n+    bs->nmethod_entry_barrier(&_masm, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n+#endif\n@@ -1662,1 +1664,1 @@\n-  MacroAssembler _masm(&cbuf);\n+  C2_MacroAssembler _masm(&cbuf);\n@@ -2164,13 +2166,1 @@\n-  %}\n-\n-  enc_class Java_To_Interpreter(method meth)\n-  %{\n-    \/\/ CALL Java_To_Interpreter\n-    \/\/ This is the instruction starting address for relocation info.\n-    cbuf.set_insts_mark();\n-    $$$emit8$primary;\n-    \/\/ CALL directly to the runtime\n-    emit_d32_reloc(cbuf,\n-                   (int) ($meth$$method - ((intptr_t) cbuf.insts_end()) - 4),\n-                   runtime_call_Relocation::spec(),\n-                   RELOC_DISP32);\n+    __ post_call_nop();\n@@ -2184,0 +2174,1 @@\n+    MacroAssembler _masm(&cbuf);\n@@ -2197,5 +2188,11 @@\n-      \/\/ Emit stubs for static call.\n-      address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n-      if (stub == NULL) {\n-        ciEnv::current()->record_failure(\"CodeCache is full\");\n-        return;\n+      if (CodeBuffer::supports_shared_stubs() && _method->can_be_statically_bound()) {\n+        \/\/ Calls of the same statically bound method can share\n+        \/\/ a stub to the interpreter.\n+        cbuf.shared_stub_to_interp_for(_method, cbuf.insts()->mark_off());\n+      } else {\n+        \/\/ Emit stubs for static call.\n+        address stub = CompiledStaticCall::emit_to_interp_stub(cbuf, mark);\n+        if (stub == NULL) {\n+          ciEnv::current()->record_failure(\"CodeCache is full\");\n+          return;\n+        }\n@@ -2205,0 +2202,2 @@\n+    _masm.clear_inst_mark();\n+    __ post_call_nop();\n@@ -2210,20 +2209,1 @@\n-  %}\n-\n-  enc_class Java_Compiled_Call(method meth)\n-  %{\n-    \/\/ JAVA COMPILED CALL\n-    int disp = in_bytes(Method:: from_compiled_offset());\n-\n-    \/\/ XXX XXX offset is 128 is 1.5 NON-PRODUCT !!!\n-    \/\/ assert(-0x80 <= disp && disp < 0x80, \"compiled_code_offset isn't small\");\n-\n-    \/\/ callq *disp(%rax)\n-    cbuf.set_insts_mark();\n-    $$$emit8$primary;\n-    if (disp < 0x80) {\n-      emit_rm(cbuf, 0x01, $secondary, RAX_enc); \/\/ R\/M byte\n-      emit_d8(cbuf, disp); \/\/ Displacement\n-    } else {\n-      emit_rm(cbuf, 0x02, $secondary, RAX_enc); \/\/ R\/M byte\n-      emit_d32(cbuf, disp); \/\/ Displacement\n-    }\n+    __ post_call_nop();\n@@ -2802,1 +2782,1 @@\n-\/\/         are owned by the CALLEE.  Holes should not be nessecary in the\n+\/\/         are owned by the CALLEE.  Holes should not be necessary in the\n@@ -2805,1 +2785,1 @@\n-\/\/         avoid holes.  Holes in the outgoing arguments may be nessecary for\n+\/\/         avoid holes.  Holes in the outgoing arguments may be necessary for\n@@ -3255,1 +3235,1 @@\n-\/\/ Int Immediate: 2^n-1, postive\n+\/\/ Int Immediate: 2^n-1, positive\n@@ -4300,1 +4280,1 @@\n-\/\/ Comparision Code\n+\/\/ Comparison Code\n@@ -4330,1 +4310,1 @@\n-    greater_equal(0x3, \"nb\");\n+    greater_equal(0x3, \"ae\");\n@@ -4332,1 +4312,1 @@\n-    greater(0x7, \"nbe\");\n+    greater(0x7, \"a\");\n@@ -4339,1 +4319,3 @@\n-\/\/ Floating comparisons that don't require any fixup for the unordered case\n+\/\/ Floating comparisons that don't require any fixup for the unordered case,\n+\/\/ If both inputs of the comparison are the same, ZF is always set so we\n+\/\/ don't need to use cmpOpUCF2 for eq\/ne\n@@ -4345,1 +4327,2 @@\n-            n->as_Bool()->_test._test == BoolTest::gt);\n+            n->as_Bool()->_test._test == BoolTest::gt ||\n+            n->in(1)->in(1) == n->in(1)->in(2));\n@@ -4348,2 +4331,2 @@\n-    equal(0x4, \"e\");\n-    not_equal(0x5, \"ne\");\n+    equal(0xb, \"np\");\n+    not_equal(0xa, \"p\");\n@@ -4351,1 +4334,1 @@\n-    greater_equal(0x3, \"nb\");\n+    greater_equal(0x3, \"ae\");\n@@ -4353,1 +4336,1 @@\n-    greater(0x7, \"nbe\");\n+    greater(0x7, \"a\");\n@@ -4363,2 +4346,3 @@\n-  predicate(n->as_Bool()->_test._test == BoolTest::ne ||\n-            n->as_Bool()->_test._test == BoolTest::eq);\n+  predicate((n->as_Bool()->_test._test == BoolTest::ne ||\n+             n->as_Bool()->_test._test == BoolTest::eq) &&\n+            n->in(1)->in(1) != n->in(1)->in(2));\n@@ -4370,1 +4354,1 @@\n-    greater_equal(0x3, \"nb\");\n+    greater_equal(0x3, \"ae\");\n@@ -4372,1 +4356,1 @@\n-    greater(0x7, \"nbe\");\n+    greater(0x7, \"a\");\n@@ -6599,0 +6583,12 @@\n+instruct countLeadingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntl  $dst, $src\\t# count leading zeros (int)\" %}\n+  ins_encode %{\n+    __ lzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -6636,0 +6632,12 @@\n+instruct countLeadingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountLeadingZerosInstruction);\n+  match(Set dst (CountLeadingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"lzcntq  $dst, $src\\t# count leading zeros (long)\" %}\n+  ins_encode %{\n+    __ lzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -6673,0 +6681,12 @@\n+instruct countTrailingZerosI_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosI (LoadI src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntl    $dst, $src\\t# count trailing zeros (int)\" %}\n+  ins_encode %{\n+    __ tzcntl($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -6705,0 +6725,12 @@\n+instruct countTrailingZerosL_mem(rRegI dst, memory src, rFlagsReg cr) %{\n+  predicate(UseCountTrailingZerosInstruction);\n+  match(Set dst (CountTrailingZerosL (LoadL src)));\n+  effect(KILL cr);\n+  ins_cost(175);\n+  format %{ \"tzcntq    $dst, $src\\t# count trailing zeros (long)\" %}\n+  ins_encode %{\n+    __ tzcntq($dst$$Register, $src$$Address);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -7081,0 +7113,14 @@\n+instruct cmovI_imm_01(rRegI dst, immI_1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7093,0 +7139,14 @@\n+instruct cmovI_imm_01U(rRegI dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7104,0 +7164,14 @@\n+instruct cmovI_imm_01UCF(rRegI dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_int() == 0);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, int\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7112,0 +7186,30 @@\n+instruct cmovI_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovI_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegI dst, rRegI src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveI (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n@@ -7179,0 +7283,30 @@\n+instruct cmovN_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovN_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegN dst, rRegN src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveN (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpl  $dst, $src\\n\\t\"\n+            \"cmovnel $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovl(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n@@ -7213,0 +7347,30 @@\n+instruct cmovP_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovP_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegP dst, rRegP src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveP (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n@@ -7240,0 +7404,14 @@\n+instruct cmovL_imm_01(rRegL dst, immI_1 src, rFlagsReg cr, cmpOp cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# signed, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7264,0 +7442,14 @@\n+instruct cmovL_imm_01U(rRegL dst, immI_1 src, rFlagsRegU cr, cmpOpU cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7276,0 +7468,14 @@\n+instruct cmovL_imm_01UCF(rRegL dst, immI_1 src, rFlagsRegUCF cr, cmpOpUCF cop)\n+%{\n+  predicate(n->in(2)->in(2)->is_Con() && n->in(2)->in(2)->get_long() == 0);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(100); \/\/ XXX\n+  format %{ \"setbn$cop $dst\\t# unsigned, long\" %}\n+  ins_encode %{\n+    Assembler::Condition cond = (Assembler::Condition)($cop$$cmpcode);\n+    __ setb(MacroAssembler::negate_condition(cond), $dst$$Register);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n@@ -7284,0 +7490,30 @@\n+instruct cmovL_regUCF2_ne(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::ne);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary dst src)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n+\/\/ Since (x == y) == !(x != y), we can flip the sense of the test by flipping the\n+\/\/ inputs of the CMove\n+instruct cmovL_regUCF2_eq(cmpOpUCF2 cop, rFlagsRegUCF cr, rRegL dst, rRegL src) %{\n+  predicate(n->in(1)->in(1)->as_Bool()->_test._test == BoolTest::eq);\n+  match(Set dst (CMoveL (Binary cop cr) (Binary src dst)));\n+\n+  ins_cost(200); \/\/ XXX\n+  format %{ \"cmovpq  $dst, $src\\n\\t\"\n+            \"cmovneq $dst, $src\" %}\n+  ins_encode %{\n+    __ cmovq(Assembler::parity, $dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::notEqual, $dst$$Register, $src$$Register);\n+  %}\n+  ins_pipe(pipe_cmov_reg);\n+%}\n+\n@@ -8729,0 +8965,26 @@\n+instruct udivI_rReg(rax_RegI rax, rdx_RegI rdx, no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(Set rax (UDivI rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivl $rax,$rax,$div\\t# UDivI\\n\" %}\n+  ins_encode %{\n+    __ udivI($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct udivL_rReg(rax_RegL rax, rdx_RegL rdx, no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(Set rax (UDivL rax div));\n+  effect(KILL rdx, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivq $rax,$rax,$div\\t# UDivL\\n\" %}\n+  ins_encode %{\n+     __ udivL($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n@@ -8770,0 +9032,35 @@\n+\/\/ Unsigned integer DIVMOD with Register, both quotient and mod results\n+instruct udivModI_rReg_divmod(rax_RegI rax, no_rax_rdx_RegI tmp, rdx_RegI rdx,\n+                              no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(UDivModI rax div);\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivl $rax,$rax,$div\\t# begin UDivModI\\n\\t\"\n+            \"umodl $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModI\\n\"\n+          %}\n+  ins_encode %{\n+    __ udivmodI($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\/\/ Unsigned long DIVMOD with Register, both quotient and mod results\n+instruct udivModL_rReg_divmod(rax_RegL rax, no_rax_rdx_RegL tmp, rdx_RegL rdx,\n+                              no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(UDivModL rax div);\n+  effect(TEMP tmp, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"udivq $rax,$rax,$div\\t# begin UDivModL\\n\\t\"\n+            \"umodq $rdx,$rax,$div\\t! using $tmp as TEMP # end UDivModL\\n\"\n+          %}\n+  ins_encode %{\n+    __ udivmodL($rax$$Register, $div$$Register, $rdx$$Register, $tmp$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+\n@@ -8873,0 +9170,26 @@\n+instruct umodI_rReg(rdx_RegI rdx, rax_RegI rax, no_rax_rdx_RegI div, rFlagsReg cr)\n+%{\n+  match(Set rdx (UModI rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"umodl $rdx,$rax,$div\\t# UModI\\n\" %}\n+  ins_encode %{\n+    __ umodI($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n+instruct umodL_rReg(rdx_RegL rdx, rax_RegL rax, no_rax_rdx_RegL div, rFlagsReg cr)\n+%{\n+  match(Set rdx (UModL rax div));\n+  effect(KILL rax, KILL cr);\n+\n+  ins_cost(300);\n+  format %{ \"umodq $rdx,$rax,$div\\t# UModL\\n\" %}\n+  ins_encode %{\n+    __ umodL($rax$$Register, $div$$Register, $rdx$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg_alu0);\n+%}\n+\n@@ -8929,0 +9252,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -8942,0 +9266,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -8952,0 +9277,24 @@\n+instruct salI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI src shift));\n+\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9007,0 +9356,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9019,0 +9369,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9029,2 +9380,1 @@\n-\/\/ Logical shift right by one\n-instruct shrI_rReg_1(rRegI dst, immI_1 shift, rFlagsReg cr)\n+instruct sarI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n@@ -9032,2 +9382,2 @@\n-  match(Set dst (URShiftI dst shift));\n-  effect(KILL cr);\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI src shift));\n@@ -9035,1 +9385,1 @@\n-  format %{ \"shrl    $dst, $shift\" %}\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n@@ -9037,1 +9387,1 @@\n-    __ shrl($dst$$Register, $shift$$constant);\n+    __ sarxl($dst$$Register, $src$$Register, $shift$$Register);\n@@ -9039,1 +9389,1 @@\n-  ins_pipe(ialu_reg);\n+  ins_pipe(ialu_reg_reg);\n@@ -9042,2 +9392,1 @@\n-\/\/ Logical shift right by one\n-instruct shrI_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n+instruct sarI_mem_rReg(rRegI dst, memory src, rRegI shift)\n@@ -9045,3 +9394,29 @@\n-  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n-  effect(KILL cr);\n-\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n+\/\/ Logical shift right by one\n+instruct shrI_rReg_1(rRegI dst, immI_1 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (URShiftI dst shift));\n+  effect(KILL cr);\n+\n+  format %{ \"shrl    $dst, $shift\" %}\n+  ins_encode %{\n+    __ shrl($dst$$Register, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg);\n+%}\n+\n+\/\/ Logical shift right by one\n+instruct shrI_mem_1(memory dst, immI_1 shift, rFlagsReg cr)\n+%{\n+  match(Set dst (StoreI dst (URShiftI (LoadI dst) shift)));\n+  effect(KILL cr);\n+\n@@ -9084,0 +9459,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9097,0 +9473,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9107,0 +9484,24 @@\n+instruct shrI_rReg_rReg(rRegI dst, rRegI src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI src shift));\n+\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrI_mem_rReg(rRegI dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftI (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxl($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9163,0 +9564,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9176,0 +9578,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9186,0 +9589,24 @@\n+instruct salL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL src shift));\n+\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct salL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (LShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shlxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shlxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9213,1 +9640,1 @@\n-instruct sarL_rReg_imm(rRegL dst, immI8 shift, rFlagsReg cr)\n+instruct sarL_rReg_imm(rRegL dst, immI shift, rFlagsReg cr)\n@@ -9226,1 +9653,1 @@\n-instruct sarL_mem_imm(memory dst, immI8 shift, rFlagsReg cr)\n+instruct sarL_mem_imm(memory dst, immI shift, rFlagsReg cr)\n@@ -9241,0 +9668,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9254,0 +9682,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9264,0 +9693,24 @@\n+instruct sarL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL src shift));\n+\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct sarL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (RShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"sarxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ sarxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9319,0 +9772,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9332,0 +9786,1 @@\n+  predicate(!VM_Version::supports_bmi2());\n@@ -9342,0 +9797,24 @@\n+instruct shrL_rReg_rReg(rRegL dst, rRegL src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL src shift));\n+\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Register, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct shrL_mem_rReg(rRegL dst, memory src, rRegI shift)\n+%{\n+  predicate(VM_Version::supports_bmi2());\n+  match(Set dst (URShiftL (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"shrxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ shrxq($dst$$Register, $src$$Address, $shift$$Register);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9371,1 +9850,1 @@\n-instruct rolI_imm(rRegI dst, immI8 shift, rFlagsReg cr)\n+instruct rolI_immI8_legacy(rRegI dst, immI8 shift, rFlagsReg cr)\n@@ -9373,1 +9852,1 @@\n-  predicate(n->bottom_type()->basic_type() == T_INT);\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n@@ -9383,0 +9862,25 @@\n+instruct rolI_immI8(rRegI dst, rRegI src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateLeft (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 32 - ($shift$$constant & 31);\n+    __ rorxl($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9410,1 +9914,1 @@\n-instruct rorI_immI8(rRegI dst, immI8 shift)\n+instruct rorI_immI8(rRegI dst, rRegI src, immI8 shift)\n@@ -9413,2 +9917,2 @@\n-  match(Set dst (RotateRight dst shift));\n-  format %{ \"rorxd     $dst, $shift\" %}\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n@@ -9416,1 +9920,1 @@\n-    __ rorxd($dst$$Register, $dst$$Register, $shift$$constant);\n+    __ rorxl($dst$$Register, $src$$Register, $shift$$constant);\n@@ -9421,0 +9925,12 @@\n+instruct rorI_mem_immI8(rRegI dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_INT);\n+  match(Set dst (RotateRight (LoadI src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxl   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxl($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9434,2 +9950,1 @@\n-\n-instruct rolL_immI8(rRegL dst, immI8 shift, rFlagsReg cr)\n+instruct rolL_immI8_legacy(rRegL dst, immI8 shift, rFlagsReg cr)\n@@ -9438,1 +9953,1 @@\n-  predicate(n->bottom_type()->basic_type() == T_LONG);\n+  predicate(!VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n@@ -9448,0 +9963,25 @@\n+instruct rolL_immI8(rRegL dst, rRegL src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft src shift));\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Register, shift);\n+  %}\n+  ins_pipe(ialu_reg_reg);\n+%}\n+\n+instruct rolL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateLeft (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rolxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    int shift = 64 - ($shift$$constant & 63);\n+    __ rorxq($dst$$Register, $src$$Address, shift);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9461,1 +10001,0 @@\n-\n@@ -9475,2 +10014,1 @@\n-\n-instruct rorL_immI8(rRegL dst, immI8 shift)\n+instruct rorL_immI8(rRegL dst, rRegL src, immI8 shift)\n@@ -9480,2 +10018,2 @@\n-  match(Set dst (RotateRight dst shift));\n-  format %{ \"rorxq    $dst, $shift\" %}\n+  match(Set dst (RotateRight src shift));\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n@@ -9483,1 +10021,1 @@\n-    __ rorxq($dst$$Register, $dst$$Register, $shift$$constant);\n+    __ rorxq($dst$$Register, $src$$Register, $shift$$constant);\n@@ -9488,0 +10026,12 @@\n+instruct rorL_mem_immI8(rRegL dst, memory src, immI8 shift)\n+%{\n+  predicate(VM_Version::supports_bmi2() && n->bottom_type()->basic_type() == T_LONG);\n+  match(Set dst (RotateRight (LoadL src) shift));\n+  ins_cost(175);\n+  format %{ \"rorxq   $dst, $src, $shift\" %}\n+  ins_encode %{\n+    __ rorxq($dst$$Register, $src$$Address, $shift$$constant);\n+  %}\n+  ins_pipe(ialu_reg_mem);\n+%}\n+\n@@ -9501,0 +10051,42 @@\n+\/\/----------------------------- CompressBits\/ExpandBits ------------------------\n+\n+instruct compressBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src mask));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_reg(rRegL dst, rRegL src, rRegL mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src mask));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Register);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct compressBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (CompressBits src (LoadL mask)));\n+  format %{ \"pextq  $dst, $src, $mask\\t! parallel bit extract\" %}\n+  ins_encode %{\n+    __ pextq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n+instruct expandBitsL_mem(rRegL dst, rRegL src, memory mask) %{\n+  predicate(n->bottom_type()->isa_long());\n+  match(Set dst (ExpandBits src (LoadL mask)));\n+  format %{ \"pdepq  $dst, $src, $mask\\t! parallel bit deposit\" %}\n+  ins_encode %{\n+    __ pdepq($dst$$Register, $src$$Register, $mask$$Address);\n+  %}\n+  ins_pipe( pipe_slow );\n+%}\n+\n@@ -10467,0 +11059,1 @@\n+\/\/ Really expensive, avoid\n@@ -10471,1 +11064,1 @@\n-  ins_cost(145);\n+  ins_cost(500);\n@@ -10496,18 +11089,0 @@\n-instruct cmpF_cc_mem(rFlagsRegU cr, regF src1, memory src2)\n-%{\n-  match(Set cr (CmpF src1 (LoadF src2)));\n-\n-  ins_cost(145);\n-  format %{ \"ucomiss $src1, $src2\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomiss($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp_fixup(_masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -10525,17 +11100,0 @@\n-instruct cmpF_cc_imm(rFlagsRegU cr, regF src, immF con) %{\n-  match(Set cr (CmpF src con));\n-\n-  ins_cost(145);\n-  format %{ \"ucomiss $src, [$constantaddress]\\t# load from constant table: float=$con\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomiss($src$$XMMRegister, $constantaddress($con));\n-    emit_cmpfp_fixup(_masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -10552,0 +11110,1 @@\n+\/\/ Really expensive, avoid\n@@ -10556,1 +11115,1 @@\n-  ins_cost(145);\n+  ins_cost(500);\n@@ -10581,18 +11140,0 @@\n-instruct cmpD_cc_mem(rFlagsRegU cr, regD src1, memory src2)\n-%{\n-  match(Set cr (CmpD src1 (LoadD src2)));\n-\n-  ins_cost(145);\n-  format %{ \"ucomisd $src1, $src2\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomisd($src1$$XMMRegister, $src2$$Address);\n-    emit_cmpfp_fixup(_masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -10610,17 +11151,0 @@\n-instruct cmpD_cc_imm(rFlagsRegU cr, regD src, immD con) %{\n-  match(Set cr (CmpD src con));\n-\n-  ins_cost(145);\n-  format %{ \"ucomisd $src, [$constantaddress]\\t# load from constant table: double=$con\\n\\t\"\n-            \"jnp,s   exit\\n\\t\"\n-            \"pushfq\\t# saw NaN, set CF\\n\\t\"\n-            \"andq    [rsp], #0xffffff2b\\n\\t\"\n-            \"popfq\\n\"\n-    \"exit:\" %}\n-  ins_encode %{\n-    __ ucomisd($src$$XMMRegister, $constantaddress($con));\n-    emit_cmpfp_fixup(_masm);\n-  %}\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -10852,0 +11376,22 @@\n+instruct round_double_reg(rRegL dst, regD src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundD src));\n+  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n+  format %{ \"round_double $dst,$src \\t! using $rtmp and $rcx as TEMP\"%}\n+  ins_encode %{\n+    __ round_double($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n+instruct round_float_reg(rRegI dst, regF src, rRegL rtmp, rcx_RegL rcx, rFlagsReg cr)\n+%{\n+  match(Set dst (RoundF src));\n+  effect(TEMP dst, TEMP rtmp, TEMP rcx, KILL cr);\n+  format %{ \"round_float $dst,$src\" %}\n+  ins_encode %{\n+    __ round_float($dst$$Register, $src$$XMMRegister, $rtmp$$Register, $rcx$$Register);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12743,0 +13289,26 @@\n+\/\/ Manifest a CmpU result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpU3_reg_reg(rRegI dst, rRegI src1, rRegI src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpU3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpl    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\\t\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpl($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setne($dst$$Register);\n+    __ movzbl($dst$$Register, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12769,0 +13341,26 @@\n+\/\/ Manifest a CmpUL result in an integer register.  Very painful.\n+\/\/ This is the test to avoid.\n+instruct cmpUL3_reg_reg(rRegI dst, rRegL src1, rRegL src2, rFlagsReg flags)\n+%{\n+  match(Set dst (CmpUL3 src1 src2));\n+  effect(KILL flags);\n+\n+  ins_cost(275); \/\/ XXX\n+  format %{ \"cmpq    $src1, $src2\\t# CmpL3\\n\\t\"\n+            \"movl    $dst, -1\\n\\t\"\n+            \"jb,u    done\\n\\t\"\n+            \"setne   $dst\\n\\t\"\n+            \"movzbl  $dst, $dst\\n\\t\"\n+    \"done:\" %}\n+  ins_encode %{\n+    Label done;\n+    __ cmpq($src1$$Register, $src2$$Register);\n+    __ movl($dst$$Register, -1);\n+    __ jccb(Assembler::below, done);\n+    __ setne($dst$$Register);\n+    __ movzbl($dst$$Register, $dst$$Register);\n+    __ bind(done);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n@@ -12934,1 +13532,0 @@\n-  predicate(!n->has_vector_mask_set());\n@@ -12950,1 +13547,0 @@\n-  predicate(!n->has_vector_mask_set());\n@@ -12965,1 +13561,0 @@\n-  predicate(!n->has_vector_mask_set());\n@@ -12979,61 +13574,0 @@\n-\/\/ mask version\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-\/\/ Bounded mask operand used in following patten is needed for\n-\/\/ post-loop multiversioning.\n-instruct jmpLoopEnd_and_restoreMask(cmpOp cop, kReg_K1 ktmp, rFlagsReg cr, label labl)\n-%{\n-  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n-  match(CountedLoopEnd cop cr);\n-  effect(USE labl, TEMP ktmp);\n-\n-  ins_cost(400);\n-  format %{ \"j$cop     $labl\\t# loop end\\n\\t\"\n-            \"restorevectmask \\t# vector mask restore for loops\" %}\n-  size(10);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-    __ restorevectmask($ktmp$$KRegister);\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-\/\/ Bounded mask operand used in following patten is needed for\n-\/\/ post-loop multiversioning.\n-instruct jmpLoopEndU_and_restoreMask(cmpOpU cop, kReg_K1 ktmp, rFlagsRegU cmp, label labl) %{\n-  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl, TEMP ktmp);\n-\n-  ins_cost(400);\n-  format %{ \"j$cop,u   $labl\\t# loop end\\n\\t\"\n-            \"restorevectmask \\t# vector mask restore for loops\" %}\n-  size(10);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-    __ restorevectmask($ktmp$$KRegister);\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-\/\/ Bounded mask operand used in following patten is needed for\n-\/\/ post-loop multiversioning.\n-instruct jmpLoopEndUCF_and_restoreMask(cmpOpUCF cop, kReg_K1 ktmp, rFlagsRegUCF cmp, label labl) %{\n-  predicate(PostLoopMultiversioning && n->has_vector_mask_set());\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl, TEMP ktmp);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,u   $labl\\t# loop end\\n\\t\"\n-            \"restorevectmask \\t# vector mask restore for loops\" %}\n-  size(10);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-    __ restorevectmask($ktmp$$KRegister);\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n@@ -13464,12 +13998,0 @@\n-\/\/\n-instruct CallNativeDirect(method meth)\n-%{\n-  match(CallNative);\n-  effect(USE meth);\n-\n-  ins_cost(300);\n-  format %{ \"call_native \" %}\n-  ins_encode(clear_avx, Java_To_Runtime(meth));\n-  ins_pipe(pipe_slow);\n-%}\n-\n@@ -13640,1 +14162,1 @@\n-\/\/   \/\/ increment preceeded by register-register move\n+\/\/   \/\/ increment preceded by register-register move\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":774,"deletions":252,"binary":false,"changes":1026,"status":"modified"},{"patch":"@@ -65,0 +65,2 @@\n+define_pd_global(bool,  VMContinuations, false);\n+\n","filename":"src\/hotspot\/cpu\/zero\/globals_zero.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -424,2 +424,0 @@\n-  if(_matrule->find_type(\"CallNative\",idx))       return Form::JAVA_NATIVE;\n-  idx = 0;\n@@ -1147,3 +1145,0 @@\n-  else if( is_ideal_call() == Form::JAVA_NATIVE ) {\n-    return \"MachCallNativeNode\";\n-  }\n@@ -1358,1 +1353,1 @@\n-\/\/ Seach through operands to determine parameters unique positions.\n+\/\/ Search through operands to determine parameters unique positions.\n@@ -4107,0 +4102,1 @@\n+        strcmp(opType,\"PopulateIndex\")==0 ||\n@@ -4216,1 +4212,1 @@\n-    \"NegVF\",\"NegVD\",\"NegVI\",\n+    \"NegVF\",\"NegVD\",\"NegVI\",\"NegVL\",\n@@ -4220,0 +4216,1 @@\n+    \"CompressV\", \"ExpandV\", \"CompressM\",\n@@ -4231,1 +4228,1 @@\n-    \"ReplicateB\",\"ReplicateS\",\"ReplicateI\",\"ReplicateL\",\"ReplicateF\",\"ReplicateD\",\n+    \"ReplicateB\",\"ReplicateS\",\"ReplicateI\",\"ReplicateL\",\"ReplicateF\",\"ReplicateD\",\"ReverseV\",\"ReverseBytesV\",\n@@ -4240,1 +4237,2 @@\n-    \"FmaVD\",\"FmaVF\",\"PopCountVI\", \"PopCountVL\", \"VectorLongToMask\",\n+    \"FmaVD\",\"FmaVF\",\"PopCountVI\",\"PopCountVL\",\"PopulateIndex\",\"VectorLongToMask\",\n+    \"CountLeadingZerosV\", \"CountTrailingZerosV\", \"SignumVF\", \"SignumVD\",\n@@ -4243,0 +4241,1 @@\n+    \"RoundVF\", \"RoundVD\",\n","filename":"src\/hotspot\/share\/adlc\/formssel.cpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -351,1 +351,1 @@\n-  printf(\" q  quiet mode, supresses all non-essential messages\\n\");\n+  printf(\" q  quiet mode, suppresses all non-essential messages\\n\");\n@@ -492,2 +492,1 @@\n-\/\/ VS2005 has its own definition, identical to this one.\n-#if !defined(_WIN32) || defined(_WIN64) || _MSC_VER < 1400\n+#if !defined(_WIN32) || defined(_WIN64)\n","filename":"src\/hotspot\/share\/adlc\/main.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"compiler\/compiler_globals.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -39,0 +41,2 @@\n+class ciMethod;\n+class SharedStubToInterpRequest;\n@@ -261,5 +265,2 @@\n-  \/\/ alignment requirement for starting offset\n-  \/\/ Requirements are that the instruction area and the\n-  \/\/ stubs area must start on CodeEntryAlignment, and\n-  \/\/ the ctable on sizeof(jdouble)\n-  int alignment() const             { return MAX2((int)sizeof(jdouble), (int)CodeEntryAlignment); }\n+  static int alignment(int section);\n+  int alignment() { return alignment(_index); }\n@@ -270,1 +271,7 @@\n-  csize_t align_at_start(csize_t off) const { return (csize_t) align_up(off, alignment()); }\n+  csize_t align_at_start(csize_t off, int section) const {\n+    return (csize_t) align_up(off, alignment(section));\n+  }\n+\n+  csize_t align_at_start(csize_t off) const {\n+    return (csize_t) align_up(off, alignment(_index));\n+  }\n@@ -352,0 +359,2 @@\n+typedef GrowableArray<SharedStubToInterpRequest> SharedStubToInterpRequests;\n+\n@@ -424,0 +433,3 @@\n+  SharedStubToInterpRequests* _shared_stub_to_interp_requests; \/\/ used to collect requests for shared iterpreter stubs\n+  bool         _finalize_stubs; \/\/ Indicate if we need to finalize stubs to make CodeBuffer final.\n+\n@@ -441,0 +453,2 @@\n+    _finalize_stubs  = false;\n+    _shared_stub_to_interp_requests = NULL;\n@@ -692,0 +706,6 @@\n+  \/\/ Make a set of stubs final. It can create\/optimize stubs.\n+  void finalize_stubs();\n+\n+  \/\/ Request for a shared stub to the interpreter\n+  void shared_stub_to_interp_for(ciMethod* callee, csize_t call_offset);\n+\n@@ -707,0 +727,17 @@\n+\/\/ A Java method can have calls of Java methods which can be statically bound.\n+\/\/ Calls of Java methods need stubs to the interpreter. Calls sharing the same Java method\n+\/\/ can share a stub to the interpreter.\n+\/\/ A SharedStubToInterpRequest is a request for a shared stub to the interpreter.\n+class SharedStubToInterpRequest : public ResourceObj {\n+ private:\n+  ciMethod* _shared_method;\n+  CodeBuffer::csize_t _call_offset; \/\/ The offset of the call in CodeBuffer\n+\n+ public:\n+  SharedStubToInterpRequest(ciMethod* method = NULL, CodeBuffer::csize_t call_offset = -1) : _shared_method(method),\n+      _call_offset(call_offset) {}\n+\n+  ciMethod* shared_method() const { return _shared_method; }\n+  CodeBuffer::csize_t call_offset() const { return _call_offset; }\n+};\n+\n@@ -712,0 +749,15 @@\n+inline int CodeSection::alignment(int section) {\n+  if (section == CodeBuffer::SECT_CONSTS) {\n+    return (int) sizeof(jdouble);\n+  }\n+  if (section == CodeBuffer::SECT_INSTS) {\n+    return (int) CodeEntryAlignment;\n+  }\n+  if (CodeBuffer::SECT_STUBS) {\n+    \/\/ CodeBuffer installer expects sections to be HeapWordSize aligned\n+    return HeapWordSize;\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n","filename":"src\/hotspot\/share\/asm\/codeBuffer.hpp","additions":59,"deletions":7,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -60,0 +60,1 @@\n+  virtual int nr_immediate_oops_patched() const  { return 0; }\n@@ -497,0 +498,7 @@\n+  virtual int nr_immediate_oops_patched() const  {\n+    if (_id == load_mirror_id || _id == load_appendix_id) {\n+      return 1;\n+    }\n+    return 0;\n+  }\n+\n@@ -598,3 +606,0 @@\n- private:\n-  CodeEmitInfo* _info;\n-\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -365,0 +365,1 @@\n+  _immediate_oops_patched = lir_asm.nr_immediate_oops_patched();\n@@ -384,0 +385,4 @@\n+  if (method()->is_synchronized()) {\n+    set_has_monitors(true);\n+  }\n+\n@@ -428,1 +433,3 @@\n-    SharedRuntime::is_wide_vector(max_vector_size())\n+    SharedRuntime::is_wide_vector(max_vector_size()),\n+    has_monitors(),\n+    _immediate_oops_patched\n@@ -566,0 +573,1 @@\n+, _has_monitors(false)\n@@ -574,0 +582,1 @@\n+, _immediate_oops_patched(0)\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -85,0 +85,1 @@\n+  bool               _has_monitors; \/\/ Fastpath monitors detection for Continuations\n@@ -96,0 +97,1 @@\n+  int                _immediate_oops_patched;\n@@ -141,0 +143,1 @@\n+  bool has_monitors() const                      { return _has_monitors; }\n@@ -172,0 +175,1 @@\n+  void set_has_monitors(bool f)                  { _has_monitors = f; }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,1 +61,1 @@\n-\/\/  x =  ABI area (SPARC) or  return adress and link (i486)\n+\/\/  x =  ABI area (SPARC) or  return address and link (i486)\n","filename":"src\/hotspot\/share\/c1\/c1_FrameMap.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,0 +51,4 @@\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JFR\n+#include \"jfr\/jfr.hpp\"\n+#endif\n@@ -67,0 +71,1 @@\n+  int            _block_id_start;\n@@ -68,0 +73,1 @@\n+  int           bit_number(int block_id) const   { return block_id - _block_id_start; }\n@@ -123,0 +129,1 @@\n+ , _block_id_start(0)\n@@ -229,2 +236,4 @@\n-  \/\/ start a new block after jsr-bytecode and link this block into cfg\n-  make_block_at(next_bci, current);\n+  if (next_bci < method()->code_size()) {\n+    \/\/ start a new block after jsr-bytecode and link this block into cfg\n+    make_block_at(next_bci, current);\n+  }\n@@ -250,0 +259,2 @@\n+  int end_bci = method()->code_size();\n+\n@@ -320,1 +331,3 @@\n-        make_block_at(s.next_bci(), current);\n+        if (s.next_bci() < end_bci) {\n+          make_block_at(s.next_bci(), current);\n+        }\n@@ -379,5 +392,6 @@\n-  _active.initialize(BlockBegin::number_of_blocks());\n-  _visited.initialize(BlockBegin::number_of_blocks());\n-  _loop_map = GrowableArray<ResourceBitMap>(BlockBegin::number_of_blocks(), BlockBegin::number_of_blocks(), ResourceBitMap());\n-  for (int i = 0; i < BlockBegin::number_of_blocks(); i++) {\n-    _loop_map.at(i).initialize(BlockBegin::number_of_blocks());\n+  const int number_of_blocks = _blocks.length();\n+  _active.initialize(number_of_blocks);\n+  _visited.initialize(number_of_blocks);\n+  _loop_map = GrowableArray<ResourceBitMap>(number_of_blocks, number_of_blocks, ResourceBitMap());\n+  for (int i = 0; i < number_of_blocks; i++) {\n+    _loop_map.at(i).initialize(number_of_blocks);\n@@ -396,1 +410,1 @@\n-  \/\/ -  Now, the tricky part here is how we detect irriducible loops. In the algorithm above the loop state bits\n+  \/\/ -  Now, the tricky part here is how we detect irreducible loops. In the algorithm above the loop state bits\n@@ -401,1 +415,3 @@\n-  BitMap& loop_state = mark_loops(_bci2block->at(0), false);\n+  BlockBegin* start = _bci2block->at(0);\n+  _block_id_start = start->block_id();\n+  BitMap& loop_state = mark_loops(start, false);\n@@ -414,0 +430,2 @@\n+  int block_id = block->block_id();\n+  int block_bit = bit_number(block_id);\n@@ -422,3 +440,3 @@\n-    assert(_loop_map.at(block->block_id()).is_empty(), \"must not be set yet\");\n-    assert(0 <= _next_loop_index && _next_loop_index < BlockBegin::number_of_blocks(), \"_next_loop_index is too large\");\n-    _loop_map.at(block->block_id()).set_bit(_next_loop_index++);\n+    assert(_loop_map.at(block_bit).is_empty(), \"must not be set yet\");\n+    assert(0 <= _next_loop_index && _next_loop_index < _loop_map.length(), \"_next_loop_index is too large\");\n+    _loop_map.at(block_bit).set_bit(_next_loop_index++);\n@@ -427,1 +445,1 @@\n-    assert(_loop_map.at(block->block_id()).count_one_bits() == 1, \"exactly one bit must be set\");\n+    assert(_loop_map.at(block_bit).count_one_bits() == 1, \"exactly one bit must be set\");\n@@ -433,2 +451,3 @@\n-  if (_visited.at(block_id)) {\n-    if (_active.at(block_id)) {\n+  int block_bit = bit_number(block_id);\n+  if (_visited.at(block_bit)) {\n+    if (_active.at(block_bit)) {\n@@ -439,1 +458,1 @@\n-    return _loop_map.at(block_id);\n+    return _loop_map.at(block_bit);\n@@ -447,2 +466,2 @@\n-  _visited.set_bit(block_id);\n-  _active.set_bit(block_id);\n+  _visited.set_bit(block_bit);\n+  _active.set_bit(block_bit);\n@@ -451,1 +470,1 @@\n-  ResourceBitMap loop_state(BlockBegin::number_of_blocks());\n+  ResourceBitMap loop_state(_loop_map.length());\n@@ -453,0 +472,1 @@\n+    BlockBegin* sux = successor_at(block, i);\n@@ -454,1 +474,1 @@\n-    loop_state.set_union(mark_loops(successor_at(block, i), in_subroutine));\n+    loop_state.set_union(mark_loops(sux, in_subroutine));\n@@ -458,1 +478,1 @@\n-  _active.clear_bit(block_id);\n+  _active.clear_bit(block_bit);\n@@ -471,1 +491,1 @@\n-    BitMap& header_loop_state = _loop_map.at(block_id);\n+    BitMap& header_loop_state = _loop_map.at(block_bit);\n@@ -478,2 +498,2 @@\n-  _loop_map.at(block_id).set_from(loop_state);\n-  return _loop_map.at(block_id);\n+  _loop_map.at(block_bit).set_from(loop_state);\n+  return _loop_map.at(block_bit);\n@@ -962,1 +982,1 @@\n-        if (!obj->is_loaded() || (PatchALot && (obj->is_null_object() || obj->klass() != ciEnv::current()->String_klass()))) {\n+        if (!obj->is_loaded() || (PatchALot && !stream()->is_string_constant())) {\n@@ -983,1 +1003,3 @@\n-      bool kills_memory = stream()->is_dynamic_constant(); \/\/ arbitrary memory effects from running BSM during linkage\n+      \/\/ Arbitrary memory effects from running BSM or class loading (using custom loader) during linkage.\n+      bool kills_memory = stream()->is_dynamic_constant() ||\n+                          (!stream()->is_string_constant() && !method()->holder()->has_trusted_loader());\n@@ -2260,1 +2282,0 @@\n-\n@@ -2269,0 +2290,1 @@\n+  JFR_ONLY(Jfr::on_resolution(this, holder, target); CHECK_BAILOUT();)\n@@ -2686,0 +2708,1 @@\n+  compilation()->set_has_monitors(true);\n@@ -3044,1 +3067,1 @@\n-    \/\/ sucessfully simplified phi function\n+    \/\/ successfully simplified phi function\n@@ -3521,1 +3544,1 @@\n-  \/\/ can increment the the counters.\n+  \/\/ can increment the counters.\n@@ -3711,1 +3734,1 @@\n-      \/\/ Intrinsic node should be emitted.  If this isn't done the the\n+      \/\/ Intrinsic node should be emitted.  If this isn't done the\n@@ -4229,1 +4252,1 @@\n-  \/\/ perform the throw as if at the the call site\n+  \/\/ perform the throw as if at the call site\n@@ -4530,8 +4553,14 @@\n-        ciMethod* target = type->as_ObjectType()->constant_value()->as_method_handle()->get_vmtarget();\n-        \/\/ We don't do CHA here so only inline static and statically bindable methods.\n-        if (target->is_static() || target->can_be_statically_bound()) {\n-          if (ciMethod::is_consistent_info(callee, target)) {\n-            Bytecodes::Code bc = target->is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokevirtual;\n-            ignore_return = ignore_return || (callee->return_type()->is_void() && !target->return_type()->is_void());\n-            if (try_inline(target, \/*holder_known*\/ !callee->is_static(), ignore_return, bc)) {\n-              return true;\n+        ciObject* mh = type->as_ObjectType()->constant_value();\n+        if (mh->is_method_handle()) {\n+          ciMethod* target = mh->as_method_handle()->get_vmtarget();\n+\n+          \/\/ We don't do CHA here so only inline static and statically bindable methods.\n+          if (target->is_static() || target->can_be_statically_bound()) {\n+            if (ciMethod::is_consistent_info(callee, target)) {\n+              Bytecodes::Code bc = target->is_static() ? Bytecodes::_invokestatic : Bytecodes::_invokevirtual;\n+              ignore_return = ignore_return || (callee->return_type()->is_void() && !target->return_type()->is_void());\n+              if (try_inline(target, \/*holder_known*\/ !callee->is_static(), ignore_return, bc)) {\n+                return true;\n+              }\n+            } else {\n+              print_inlining(target, \"signatures mismatch\", \/*success*\/ false);\n@@ -4540,1 +4569,2 @@\n-            print_inlining(target, \"signatures mismatch\", \/*success*\/ false);\n+            assert(false, \"no inlining through MH::invokeBasic\"); \/\/ missing optimization opportunity due to suboptimal LF shape\n+            print_inlining(target, \"not static or statically bindable\", \/*success*\/ false);\n@@ -4543,1 +4573,2 @@\n-          print_inlining(target, \"not static or statically bindable\", \/*success*\/ false);\n+          assert(mh->is_null_object(), \"not a null\");\n+          print_inlining(callee, \"receiver is always null\", \/*success*\/ false);\n@@ -4616,1 +4647,2 @@\n-    break; \/\/ TODO: NYI\n+    print_inlining(callee, \"native call\", \/*success*\/ false);\n+    break;\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":76,"deletions":44,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -54,0 +54,1 @@\n+  friend class JfrResolution;\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -469,1 +469,1 @@\n-  ResourceBitMap _dominator_blocks; \/\/ temproary BitMap used for computation of dominator\n+  ResourceBitMap _dominator_blocks; \/\/ temporary BitMap used for computation of dominator\n@@ -694,1 +694,1 @@\n-\/\/ all other loop blocks = loops with mulitple entries).\n+\/\/ all other loop blocks = loops with multiple entries).\n@@ -827,1 +827,1 @@\n-  \/\/ this is necessery for the (very rare) case that two successing blocks have\n+  \/\/ this is necessary for the (very rare) case that two successive blocks have\n@@ -836,1 +836,1 @@\n-  \/\/ critical edge split blocks are prefered because than they have a bigger\n+  \/\/ critical edge split blocks are preferred because than they have a bigger\n@@ -933,1 +933,1 @@\n-    assert(osr_entry->sux_at(0)->number_of_preds() >= 2, \"sucessor of osr entry must have two predecessors (otherwise it is not present in normal control flow\");\n+    assert(osr_entry->sux_at(0)->number_of_preds() >= 2, \"successor of osr entry must have two predecessors (otherwise it is not present in normal control flow\");\n@@ -1141,1 +1141,1 @@\n-        assert(cur->loop_index() == sux->loop_index() || sux->is_set(BlockBegin::linear_scan_loop_header_flag), \"successing blocks with same loop depth must have same loop index\");\n+        assert(cur->loop_index() == sux->loop_index() || sux->is_set(BlockBegin::linear_scan_loop_header_flag), \"successive blocks with same loop depth must have same loop index\");\n@@ -1153,1 +1153,1 @@\n-        assert(cur->loop_index() == pred->loop_index() || cur->is_set(BlockBegin::linear_scan_loop_header_flag), \"successing blocks with same loop depth must have same loop index\");\n+        assert(cur->loop_index() == pred->loop_index() || cur->is_set(BlockBegin::linear_scan_loop_header_flag), \"successive blocks with same loop depth must have same loop index\");\n","filename":"src\/hotspot\/share\/c1\/c1_IR.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -254,1 +254,0 @@\n-    bool is_opt_native = false;\n@@ -258,1 +257,1 @@\n-                             reexecute, rethrow_exception, is_method_handle_invoke, is_opt_native, return_oop, return_scalarized,\n+                             reexecute, rethrow_exception, is_method_handle_invoke, return_oop, return_scalarized,\n","filename":"src\/hotspot\/share\/c1\/c1_IR.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -501,1 +501,1 @@\n-\/\/ Implementation of Contant\n+\/\/ Implementation of Constant\n@@ -909,1 +909,1 @@\n-    TRACE_PHI(tty->print_cr(\"exisiting state found\"));\n+    TRACE_PHI(tty->print_cr(\"existing state found\"));\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -304,1 +304,1 @@\n-  int          _use_count;                       \/\/ the number of instructions refering to this value (w\/o prev\/next); only roots can have use count = 0 or > 1\n+  int          _use_count;                       \/\/ the number of instructions referring to this value (w\/o prev\/next); only roots can have use count = 0 or > 1\n","filename":"src\/hotspot\/share\/c1\/c1_Instruction.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -192,1 +192,0 @@\n-    case lir_cmove:\n@@ -243,2 +242,1 @@\n-  : LIR_Op(lir_branch, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n-  , _cond(cond)\n+  : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n@@ -252,2 +250,1 @@\n-  LIR_Op(lir_branch, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n-  , _cond(cond)\n+  LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n@@ -261,2 +258,1 @@\n-  : LIR_Op(lir_cond_float_branch, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n-  , _cond(cond)\n+  : LIR_Op2(lir_cond_float_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n@@ -284,7 +280,7 @@\n-  switch (_cond) {\n-    case lir_cond_equal:        _cond = lir_cond_notEqual;     break;\n-    case lir_cond_notEqual:     _cond = lir_cond_equal;        break;\n-    case lir_cond_less:         _cond = lir_cond_greaterEqual; break;\n-    case lir_cond_lessEqual:    _cond = lir_cond_greater;      break;\n-    case lir_cond_greaterEqual: _cond = lir_cond_less;         break;\n-    case lir_cond_greater:      _cond = lir_cond_lessEqual;    break;\n+  switch (cond()) {\n+    case lir_cond_equal:        set_cond(lir_cond_notEqual);     break;\n+    case lir_cond_notEqual:     set_cond(lir_cond_equal);        break;\n+    case lir_cond_less:         set_cond(lir_cond_greaterEqual); break;\n+    case lir_cond_lessEqual:    set_cond(lir_cond_greater);      break;\n+    case lir_cond_greaterEqual: set_cond(lir_cond_less);         break;\n+    case lir_cond_greater:      set_cond(lir_cond_lessEqual);    break;\n@@ -547,0 +543,7 @@\n+      assert(opBranch->_tmp1->is_illegal() && opBranch->_tmp2->is_illegal() &&\n+             opBranch->_tmp3->is_illegal() && opBranch->_tmp4->is_illegal() &&\n+             opBranch->_tmp5->is_illegal(), \"not used\");\n+\n+      if (opBranch->_opr1->is_valid()) do_input(opBranch->_opr1);\n+      if (opBranch->_opr2->is_valid()) do_input(opBranch->_opr2);\n+\n@@ -635,2 +638,2 @@\n-      assert(op->as_Op2() != NULL, \"must be\");\n-      LIR_Op2* op2 = (LIR_Op2*)op;\n+      assert(op->as_Op4() != NULL, \"must be\");\n+      LIR_Op4* op4 = (LIR_Op4*)op;\n@@ -638,3 +641,3 @@\n-      assert(op2->_info == NULL && op2->_tmp1->is_illegal() && op2->_tmp2->is_illegal() &&\n-             op2->_tmp3->is_illegal() && op2->_tmp4->is_illegal() && op2->_tmp5->is_illegal(), \"not used\");\n-      assert(op2->_opr1->is_valid() && op2->_opr2->is_valid() && op2->_result->is_valid(), \"used\");\n+      assert(op4->_info == NULL && op4->_tmp1->is_illegal() && op4->_tmp2->is_illegal() &&\n+             op4->_tmp3->is_illegal() && op4->_tmp4->is_illegal() && op4->_tmp5->is_illegal(), \"not used\");\n+      assert(op4->_opr1->is_valid() && op4->_opr2->is_valid() && op4->_result->is_valid(), \"used\");\n@@ -642,4 +645,6 @@\n-      do_input(op2->_opr1);\n-      do_input(op2->_opr2);\n-      do_temp(op2->_opr2);\n-      do_output(op2->_result);\n+      do_input(op4->_opr1);\n+      do_input(op4->_opr2);\n+      if (op4->_opr3->is_valid()) do_input(op4->_opr3);\n+      if (op4->_opr4->is_valid()) do_input(op4->_opr4);\n+      do_temp(op4->_opr2);\n+      do_output(op4->_result);\n@@ -1192,0 +1197,4 @@\n+void LIR_Op4::emit_code(LIR_Assembler* masm) {\n+  masm->emit_op4(this);\n+}\n+\n@@ -1239,0 +1248,4 @@\n+#ifdef RISCV\n+  , _cmp_opr1(LIR_OprFact::illegalOpr)\n+  , _cmp_opr2(LIR_OprFact::illegalOpr)\n+#endif\n@@ -1256,0 +1269,32 @@\n+#ifdef RISCV\n+void LIR_List::set_cmp_oprs(LIR_Op* op) {\n+  switch (op->code()) {\n+    case lir_cmp:\n+      _cmp_opr1 = op->as_Op2()->in_opr1();\n+      _cmp_opr2 = op->as_Op2()->in_opr2();\n+      break;\n+    case lir_branch: \/\/ fall through\n+    case lir_cond_float_branch:\n+      assert(op->as_OpBranch()->cond() == lir_cond_always ||\n+            (_cmp_opr1 != LIR_OprFact::illegalOpr && _cmp_opr2 != LIR_OprFact::illegalOpr),\n+            \"conditional branches must have legal operands\");\n+      if (op->as_OpBranch()->cond() != lir_cond_always) {\n+        op->as_Op2()->set_in_opr1(_cmp_opr1);\n+        op->as_Op2()->set_in_opr2(_cmp_opr2);\n+      }\n+      break;\n+    case lir_cmove:\n+      op->as_Op4()->set_in_opr3(_cmp_opr1);\n+      op->as_Op4()->set_in_opr4(_cmp_opr2);\n+      break;\n+#if INCLUDE_ZGC\n+    case lir_zloadbarrier_test:\n+      _cmp_opr1 = FrameMap::as_opr(t1);\n+      _cmp_opr2 = LIR_OprFact::intConst(0);\n+      break;\n+#endif\n+    default:\n+      break;\n+  }\n+}\n+#endif\n@@ -1855,1 +1900,0 @@\n-     case lir_cmove:                 s = \"cmove\";         break;\n@@ -1878,0 +1922,2 @@\n+     \/\/ LIR_Op4\n+     case lir_cmove:                 s = \"cmove\";         break;\n@@ -2019,0 +2065,2 @@\n+  in_opr1()->print(out); out->print(\" \");\n+  in_opr2()->print(out); out->print(\" \");\n@@ -2099,1 +2147,1 @@\n-  if (code() == lir_cmove || code() == lir_cmp) {\n+  if (code() == lir_cmp || code() == lir_branch || code() == lir_cond_float_branch) {\n@@ -2188,0 +2236,9 @@\n+\/\/ LIR_Op4\n+void LIR_Op4::print_instr(outputStream* out) const {\n+  print_condition(out, condition()); out->print(\" \");\n+  in_opr1()->print(out);             out->print(\" \");\n+  in_opr2()->print(out);             out->print(\" \");\n+  in_opr3()->print(out);             out->print(\" \");\n+  in_opr4()->print(out);             out->print(\" \");\n+  result_opr()->print(out);\n+}\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":83,"deletions":26,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -201,5 +201,5 @@\n-  \/\/     data       opr-type opr-kind\n-  \/\/ +--------------+-------+-------+\n-  \/\/ [max...........|7 6 5 4|3 2 1 0]\n-  \/\/                               ^\n-  \/\/                         is_pointer bit\n+  \/\/          data        other-non-data opr-type opr-kind\n+  \/\/ +-------------------+--------------+-------+-----+\n+  \/\/ [max...............................|6 5 4 3|2 1 0]\n+  \/\/                                                 ^\n+  \/\/                                           is_pointer bit\n@@ -208,1 +208,1 @@\n-  \/\/ we need  4 bits to represent types\n+  \/\/ we need 4 bits to represent types\n@@ -239,1 +239,1 @@\n-    , non_data_bits  = pointer_bits + kind_bits + type_bits + size_bits + destroys_bits + virtual_bits\n+    , non_data_bits  = kind_bits + type_bits + size_bits + destroys_bits + virtual_bits\n@@ -245,1 +245,1 @@\n-  enum OprShift {\n+  enum OprShift : uintptr_t {\n@@ -278,1 +278,1 @@\n-  uintptr_t data() const                         { return value() >> data_shift; }\n+  uint32_t data() const                          { return (uint32_t)value() >> data_shift; }\n@@ -302,1 +302,3 @@\n-    vreg_max = (1 << data_bits) - 1\n+    data_max = (1 << data_bits) - 1,      \/\/ max unsigned value for data bit field\n+    vreg_limit =  10000,                  \/\/ choose a reasonable limit,\n+    vreg_max = MIN2(vreg_limit, data_max) \/\/ and make sure if fits in the bit field\n@@ -635,1 +637,1 @@\n-  \/\/ Platform dependant.\n+  \/\/ Platform dependent.\n@@ -761,1 +763,0 @@\n-    assert(index <= (max_jint >> LIR_Opr::data_shift), \"index is too big\");\n@@ -782,1 +783,1 @@\n-  \/\/ the index is platform independent; a double stack useing indeces 2 and 3 has always\n+  \/\/ the index is platform independent; a double stack using indices 2 and 3 has always\n@@ -841,1 +842,1 @@\n-    assert(index <= (max_jint >> LIR_Opr::data_shift), \"index is too big\");\n+    assert(index == (int)res->data(), \"conversion check\");\n@@ -894,0 +895,1 @@\n+class    LIR_Op4;\n@@ -943,2 +945,0 @@\n-      , lir_branch\n-      , lir_cond_float_branch\n@@ -955,0 +955,2 @@\n+      , lir_branch\n+      , lir_cond_float_branch\n@@ -959,1 +961,0 @@\n-      , lir_cmove\n@@ -987,0 +988,3 @@\n+  , begin_op4\n+      , lir_cmove\n+  , end_op4\n@@ -1033,0 +1037,5 @@\n+#ifdef INCLUDE_ZGC\n+  , begin_opZLoadBarrierTest\n+    , lir_zloadbarrier_test\n+  , end_opZLoadBarrierTest\n+#endif\n@@ -1168,0 +1177,1 @@\n+  virtual LIR_Op4* as_Op4() { return NULL; }\n@@ -1450,39 +1460,0 @@\n-class LIR_OpBranch: public LIR_Op {\n- friend class LIR_OpVisitState;\n-\n- private:\n-  LIR_Condition _cond;\n-  Label*        _label;\n-  BlockBegin*   _block;  \/\/ if this is a branch to a block, this is the block\n-  BlockBegin*   _ublock; \/\/ if this is a float-branch, this is the unorderd block\n-  CodeStub*     _stub;   \/\/ if this is a branch to a stub, this is the stub\n-\n- public:\n-  LIR_OpBranch(LIR_Condition cond, Label* lbl)\n-    : LIR_Op(lir_branch, LIR_OprFact::illegalOpr, (CodeEmitInfo*) NULL)\n-    , _cond(cond)\n-    , _label(lbl)\n-    , _block(NULL)\n-    , _ublock(NULL)\n-    , _stub(NULL) { }\n-\n-  LIR_OpBranch(LIR_Condition cond, BlockBegin* block);\n-  LIR_OpBranch(LIR_Condition cond, CodeStub* stub);\n-\n-  \/\/ for unordered comparisons\n-  LIR_OpBranch(LIR_Condition cond, BlockBegin* block, BlockBegin* ublock);\n-\n-  LIR_Condition cond()        const              { return _cond;        }\n-  Label*        label()       const              { return _label;       }\n-  BlockBegin*   block()       const              { return _block;       }\n-  BlockBegin*   ublock()      const              { return _ublock;      }\n-  CodeStub*     stub()        const              { return _stub;       }\n-\n-  void          change_block(BlockBegin* b);\n-  void          change_ublock(BlockBegin* b);\n-  void          negate_cond();\n-\n-  virtual void emit_code(LIR_Assembler* masm);\n-  virtual LIR_OpBranch* as_OpBranch() { return this; }\n-  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n-};\n@@ -1739,1 +1710,1 @@\n-  LIR_Op2(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, CodeEmitInfo* info = NULL)\n+  LIR_Op2(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, CodeEmitInfo* info = NULL, BasicType type = T_ILLEGAL)\n@@ -1744,1 +1715,1 @@\n-    , _type(T_ILLEGAL)\n+    , _type(type)\n@@ -1751,1 +1722,1 @@\n-    assert(code == lir_cmp || code == lir_assert, \"code check\");\n+    assert(code == lir_cmp || code == lir_branch || code == lir_cond_float_branch || code == lir_assert, \"code check\");\n@@ -1783,1 +1754,1 @@\n-    assert(code != lir_cmp && is_in_range(code, begin_op2, end_op2), \"code check\");\n+    assert(code != lir_cmp && code != lir_branch && code != lir_cond_float_branch && is_in_range(code, begin_op2, end_op2), \"code check\");\n@@ -1799,1 +1770,1 @@\n-    assert(code != lir_cmp && is_in_range(code, begin_op2, end_op2), \"code check\");\n+    assert(code != lir_cmp && code != lir_branch && code != lir_cond_float_branch && is_in_range(code, begin_op2, end_op2), \"code check\");\n@@ -1811,1 +1782,1 @@\n-    assert(code() == lir_cmp || code() == lir_cmove || code() == lir_assert, \"only valid for cmp and cmove and assert\"); return _condition;\n+    assert(code() == lir_cmp || code() == lir_branch || code() == lir_cond_float_branch || code() == lir_assert, \"only valid for branch and assert\"); return _condition;\n@@ -1814,1 +1785,1 @@\n-    assert(code() == lir_cmp || code() == lir_cmove, \"only valid for cmp and cmove\");  _condition = condition;\n+    assert(code() == lir_cmp || code() == lir_branch || code() == lir_cond_float_branch, \"only valid for branch\"); _condition = condition;\n@@ -1828,0 +1799,45 @@\n+class LIR_OpBranch: public LIR_Op2 {\n+ friend class LIR_OpVisitState;\n+\n+ private:\n+  Label*        _label;\n+  BlockBegin*   _block;  \/\/ if this is a branch to a block, this is the block\n+  BlockBegin*   _ublock; \/\/ if this is a float-branch, this is the unordered block\n+  CodeStub*     _stub;   \/\/ if this is a branch to a stub, this is the stub\n+\n+ public:\n+  LIR_OpBranch(LIR_Condition cond, Label* lbl)\n+    : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*) NULL)\n+    , _label(lbl)\n+    , _block(NULL)\n+    , _ublock(NULL)\n+    , _stub(NULL) { }\n+\n+  LIR_OpBranch(LIR_Condition cond, BlockBegin* block);\n+  LIR_OpBranch(LIR_Condition cond, CodeStub* stub);\n+\n+  \/\/ for unordered comparisons\n+  LIR_OpBranch(LIR_Condition cond, BlockBegin* block, BlockBegin* ublock);\n+\n+  LIR_Condition cond() const {\n+    return condition();\n+  }\n+\n+  void set_cond(LIR_Condition cond) {\n+    set_condition(cond);\n+  }\n+\n+  Label*        label()       const              { return _label;       }\n+  BlockBegin*   block()       const              { return _block;       }\n+  BlockBegin*   ublock()      const              { return _ublock;      }\n+  CodeStub*     stub()        const              { return _stub;        }\n+\n+  void          change_block(BlockBegin* b);\n+  void          change_ublock(BlockBegin* b);\n+  void          negate_cond();\n+\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_OpBranch* as_OpBranch() { return this; }\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n+\n@@ -1891,0 +1907,57 @@\n+class LIR_Op4: public LIR_Op {\n+  friend class LIR_OpVisitState;\n+ protected:\n+  LIR_Opr   _opr1;\n+  LIR_Opr   _opr2;\n+  LIR_Opr   _opr3;\n+  LIR_Opr   _opr4;\n+  BasicType _type;\n+  LIR_Opr   _tmp1;\n+  LIR_Opr   _tmp2;\n+  LIR_Opr   _tmp3;\n+  LIR_Opr   _tmp4;\n+  LIR_Opr   _tmp5;\n+  LIR_Condition _condition;\n+\n+ public:\n+  LIR_Op4(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr opr3, LIR_Opr opr4,\n+          LIR_Opr result, BasicType type)\n+    : LIR_Op(code, result, NULL)\n+    , _opr1(opr1)\n+    , _opr2(opr2)\n+    , _opr3(opr3)\n+    , _opr4(opr4)\n+    , _type(type)\n+    , _tmp1(LIR_OprFact::illegalOpr)\n+    , _tmp2(LIR_OprFact::illegalOpr)\n+    , _tmp3(LIR_OprFact::illegalOpr)\n+    , _tmp4(LIR_OprFact::illegalOpr)\n+    , _tmp5(LIR_OprFact::illegalOpr)\n+    , _condition(condition) {\n+    assert(code == lir_cmove, \"code check\");\n+    assert(type != T_ILLEGAL, \"cmove should have type\");\n+  }\n+\n+  LIR_Opr in_opr1() const                        { return _opr1; }\n+  LIR_Opr in_opr2() const                        { return _opr2; }\n+  LIR_Opr in_opr3() const                        { return _opr3; }\n+  LIR_Opr in_opr4() const                        { return _opr4; }\n+  BasicType type()  const                        { return _type; }\n+  LIR_Opr tmp1_opr() const                       { return _tmp1; }\n+  LIR_Opr tmp2_opr() const                       { return _tmp2; }\n+  LIR_Opr tmp3_opr() const                       { return _tmp3; }\n+  LIR_Opr tmp4_opr() const                       { return _tmp4; }\n+  LIR_Opr tmp5_opr() const                       { return _tmp5; }\n+\n+  LIR_Condition condition() const                { return _condition; }\n+  void set_condition(LIR_Condition condition)    { _condition = condition; }\n+\n+  void set_in_opr1(LIR_Opr opr)                  { _opr1 = opr; }\n+  void set_in_opr2(LIR_Opr opr)                  { _opr2 = opr; }\n+  void set_in_opr3(LIR_Opr opr)                  { _opr3 = opr; }\n+  void set_in_opr4(LIR_Opr opr)                  { _opr4 = opr; }\n+  virtual void emit_code(LIR_Assembler* masm);\n+  virtual LIR_Op4* as_Op4() { return this; }\n+\n+  virtual void print_instr(outputStream* out) const PRODUCT_RETURN;\n+};\n@@ -2165,0 +2238,4 @@\n+#ifdef RISCV\n+  LIR_Opr       _cmp_opr1;\n+  LIR_Opr       _cmp_opr2;\n+#endif\n@@ -2177,0 +2254,6 @@\n+#ifdef RISCV\n+    set_cmp_oprs(op);\n+    \/\/ lir_cmp set cmp oprs only on riscv\n+    if (op->code() == lir_cmp) return;\n+#endif\n+\n@@ -2193,0 +2276,4 @@\n+#ifdef RISCV\n+  void set_cmp_oprs(LIR_Op* op);\n+#endif\n+\n@@ -2309,2 +2396,3 @@\n-  void cmove(LIR_Condition condition, LIR_Opr src1, LIR_Opr src2, LIR_Opr dst, BasicType type) {\n-    append(new LIR_Op2(lir_cmove, condition, src1, src2, dst, type));\n+  void cmove(LIR_Condition condition, LIR_Opr src1, LIR_Opr src2, LIR_Opr dst, BasicType type,\n+             LIR_Opr cmp_opr1 = LIR_OprFact::illegalOpr, LIR_Opr cmp_opr2 = LIR_OprFact::illegalOpr) {\n+    append(new LIR_Op4(lir_cmove, condition, src1, src2, cmp_opr1, cmp_opr2, dst, type));\n@@ -2503,1 +2591,1 @@\n-    maxNumberOfOperands = 20,\n+    maxNumberOfOperands = 21,\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":155,"deletions":67,"binary":false,"changes":222,"status":"modified"},{"patch":"@@ -115,0 +115,1 @@\n+ , _immediate_oops_patched(0)\n@@ -137,0 +138,1 @@\n+  _immediate_oops_patched += stub->nr_immediate_oops_patched();\n@@ -453,1 +455,0 @@\n-\n@@ -460,2 +461,8 @@\n-  \/\/ emit the static call stub stuff out of line\n-  emit_static_call_stub();\n+  if (CodeBuffer::supports_shared_stubs() && op->method()->can_be_statically_bound()) {\n+    \/\/ Calls of the same statically bound method can share\n+    \/\/ a stub to the interpreter.\n+    CodeBuffer::csize_t call_offset = pc() - _masm->code()->insts_begin();\n+    _masm->code()->shared_stub_to_interp_for(op->method(), call_offset);\n+  } else {\n+    emit_static_call_stub();\n+  }\n@@ -828,4 +835,0 @@\n-    case lir_cmove:\n-      cmove(op->condition(), op->in_opr1(), op->in_opr2(), op->result_opr(), op->type());\n-      break;\n-\n@@ -893,0 +896,11 @@\n+void LIR_Assembler::emit_op4(LIR_Op4* op) {\n+  switch(op->code()) {\n+    case lir_cmove:\n+      cmove(op->condition(), op->in_opr1(), op->in_opr2(), op->result_opr(), op->type(), op->in_opr3(), op->in_opr4());\n+      break;\n+\n+    default:\n+      Unimplemented();\n+      break;\n+  }\n+}\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+  int                _immediate_oops_patched;\n@@ -193,0 +194,1 @@\n+  void emit_op4(LIR_Op4* op);\n@@ -233,2 +235,2 @@\n-  void cmove(LIR_Condition code, LIR_Opr left, LIR_Opr right, LIR_Opr result, BasicType type);\n-\n+  void cmove(LIR_Condition code, LIR_Opr left, LIR_Opr right, LIR_Opr result, BasicType type,\n+             LIR_Opr cmp_opr1 = LIR_OprFact::illegalOpr, LIR_Opr cmp_opr2 = LIR_OprFact::illegalOpr);\n@@ -274,0 +276,1 @@\n+  int nr_immediate_oops_patched() const { return _immediate_oops_patched; }\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -441,1 +441,1 @@\n-        assert(value->subst() == value, \"missed substition\");\n+        assert(value->subst() == value, \"missed substitution\");\n@@ -1210,1 +1210,1 @@\n-\/\/ Examble: ref.get()\n+\/\/ Example: ref.get()\n@@ -1221,1 +1221,1 @@\n-  \/\/ need to perform the null check on the reference objecy\n+  \/\/ need to perform the null check on the reference object\n@@ -1321,2 +1321,13 @@\n-  LabelObj* L_not_prim = new LabelObj();\n-  LabelObj* L_done = new LabelObj();\n+  \/\/ While reading off the universal constant mirror is less efficient than doing\n+  \/\/ another branch and returning the constant answer, this branchless code runs into\n+  \/\/ much less risk of confusion for C1 register allocator. The choice of the universe\n+  \/\/ object here is correct as long as it returns the same modifiers we would expect\n+  \/\/ from the primitive class itself. See spec for Class.getModifiers that provides\n+  \/\/ the typed array klasses with similar modifiers as their component types.\n+\n+  Klass* univ_klass_obj = Universe::byteArrayKlassObj();\n+  assert(univ_klass_obj->modifier_flags() == (JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC), \"Sanity\");\n+  LIR_Opr prim_klass = LIR_OprFact::metadataConst(univ_klass_obj);\n+\n+  LIR_Opr recv_klass = new_register(T_METADATA);\n+  __ move(new LIR_Address(receiver.result(), java_lang_Class::klass_offset(), T_ADDRESS), recv_klass, info);\n@@ -1324,0 +1335,1 @@\n+  \/\/ Check if this is a Java mirror of primitive type, and select the appropriate klass.\n@@ -1325,6 +1337,2 @@\n-  \/\/ Checking if it's a java mirror of primitive type\n-  __ move(new LIR_Address(receiver.result(), java_lang_Class::klass_offset(), T_ADDRESS), klass, info);\n-  __ cmp(lir_cond_notEqual, klass, LIR_OprFact::metadataConst(0));\n-  __ branch(lir_cond_notEqual, L_not_prim->label());\n-  __ move(LIR_OprFact::intConst(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC), result);\n-  __ branch(lir_cond_always, L_done->label());\n+  __ cmp(lir_cond_equal, recv_klass, LIR_OprFact::metadataConst(0));\n+  __ cmove(lir_cond_equal, prim_klass, recv_klass, klass, T_ADDRESS);\n@@ -1332,1 +1340,1 @@\n-  __ branch_destination(L_not_prim->label());\n+  \/\/ Get the answer.\n@@ -1334,12 +1342,0 @@\n-  __ branch_destination(L_done->label());\n-}\n-\n-\/\/ Example: Thread.currentThread()\n-void LIRGenerator::do_currentThread(Intrinsic* x) {\n-  assert(x->number_of_arguments() == 0, \"wrong type\");\n-  LIR_Opr temp = new_register(T_ADDRESS);\n-  LIR_Opr reg = rlock_result(x);\n-  __ move(new LIR_Address(getThreadPointer(), in_bytes(JavaThread::threadObj_offset()), T_ADDRESS), temp);\n-  \/\/ threadObj = ((OopHandle)_threadObj)->resolve();\n-  access_load(IN_NATIVE, T_OBJECT,\n-              LIR_OprFact::address(new LIR_Address(temp, T_OBJECT)), reg);\n@@ -1368,1 +1364,0 @@\n-  __ convert(Bytecodes::_i2l, layout, result_reg);\n@@ -1371,2 +1366,4 @@\n-  jlong mask = ~(jlong) right_n_bits(LogBytesPerLong);\n-  __ logical_and(result_reg, LIR_OprFact::longConst(mask), result_reg);\n+\n+  LIR_Opr mask = load_immediate(~(jint) right_n_bits(LogBytesPerLong), T_INT);\n+  __ logical_and(layout, mask, layout);\n+  __ convert(Bytecodes::_i2l, layout, result_reg);\n@@ -1385,2 +1382,2 @@\n-  LIR_Opr hss = LIR_OprFact::intConst(Klass::_lh_header_size_shift);\n-  LIR_Opr hsm = LIR_OprFact::intConst(Klass::_lh_header_size_mask);\n+  LIR_Opr hss = load_immediate(Klass::_lh_header_size_shift, T_INT);\n+  LIR_Opr hsm = load_immediate(Klass::_lh_header_size_mask, T_INT);\n@@ -1397,1 +1394,1 @@\n-  LIR_Opr l2esm = LIR_OprFact::intConst(Klass::_lh_log2_element_size_mask);\n+  LIR_Opr l2esm = load_immediate(Klass::_lh_log2_element_size_mask, T_INT);\n@@ -1437,1 +1434,2 @@\n-    __ logical_and(length, LIR_OprFact::longConst(~round_mask), length);\n+    LIR_Opr round_mask_opr = load_immediate(~(jlong)round_mask, T_LONG);\n+    __ logical_and(length, round_mask_opr, length);\n@@ -1443,1 +1441,2 @@\n-    __ logical_and(length_int, LIR_OprFact::intConst(~round_mask), length_int);\n+    LIR_Opr round_mask_opr = load_immediate(~round_mask, T_INT);\n+    __ logical_and(length_int, round_mask_opr, length_int);\n@@ -1451,0 +1450,22 @@\n+void LIRGenerator::do_extentLocalCache(Intrinsic* x) {\n+  do_JavaThreadField(x, JavaThread::extentLocalCache_offset());\n+}\n+\n+\/\/ Example: Thread.currentCarrierThread()\n+void LIRGenerator::do_currentCarrierThread(Intrinsic* x) {\n+  do_JavaThreadField(x, JavaThread::threadObj_offset());\n+}\n+\n+void LIRGenerator::do_vthread(Intrinsic* x) {\n+  do_JavaThreadField(x, JavaThread::vthread_offset());\n+}\n+\n+void LIRGenerator::do_JavaThreadField(Intrinsic* x, ByteSize offset) {\n+  assert(x->number_of_arguments() == 0, \"wrong type\");\n+  LIR_Opr temp = new_register(T_ADDRESS);\n+  LIR_Opr reg = rlock_result(x);\n+  __ move(new LIR_Address(getThreadPointer(), in_bytes(offset), T_ADDRESS), temp);\n+  access_load(IN_NATIVE, T_OBJECT,\n+              LIR_OprFact::address(new LIR_Address(temp, T_OBJECT)), reg);\n+}\n+\n@@ -1599,1 +1620,1 @@\n-\/\/ ALSO reads & writes act as aquire & release, so:\n+\/\/ ALSO reads & writes act as acquire & release, so:\n@@ -3399,25 +3420,0 @@\n-#ifdef JFR_HAVE_INTRINSICS\n-\n-void LIRGenerator::do_getEventWriter(Intrinsic* x) {\n-  LabelObj* L_end = new LabelObj();\n-\n-  \/\/ FIXME T_ADDRESS should actually be T_METADATA but it can't because the\n-  \/\/ meaning of these two is mixed up (see JDK-8026837).\n-  LIR_Address* jobj_addr = new LIR_Address(getThreadPointer(),\n-                                           in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR),\n-                                           T_ADDRESS);\n-  LIR_Opr result = rlock_result(x);\n-  __ move(LIR_OprFact::oopConst(NULL), result);\n-  LIR_Opr jobj = new_register(T_METADATA);\n-  __ move_wide(jobj_addr, jobj);\n-  __ cmp(lir_cond_equal, jobj, LIR_OprFact::metadataConst(0));\n-  __ branch(lir_cond_equal, L_end->label());\n-\n-  access_load(IN_NATIVE, T_OBJECT, LIR_OprFact::address(new LIR_Address(jobj, T_OBJECT)), result);\n-\n-  __ branch_destination(L_end->label());\n-}\n-\n-#endif\n-\n-\n@@ -3449,4 +3445,1 @@\n-  case vmIntrinsics::_getEventWriter:\n-    do_getEventWriter(x);\n-    break;\n-    do_RuntimeCall(CAST_FROM_FN_PTR(address, JFR_TIME_FUNCTION), x);\n+    do_RuntimeCall(CAST_FROM_FN_PTR(address, JfrTime::time_function()), x);\n@@ -3470,1 +3463,3 @@\n-  case vmIntrinsics::_currentThread:  do_currentThread(x); break;\n+  case vmIntrinsics::_currentCarrierThread: do_currentCarrierThread(x); break;\n+  case vmIntrinsics::_currentThread:  do_vthread(x);       break;\n+  case vmIntrinsics::_extentLocalCache: do_extentLocalCache(x); break;\n@@ -3538,0 +3533,4 @@\n+\n+  case vmIntrinsics::_Continuation_doYield:\n+    do_continuation_doYield(x);\n+    break;\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":63,"deletions":64,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -221,1 +221,1 @@\n-  LIR_Opr load_immediate(int x, BasicType type);\n+  LIR_Opr load_immediate(jlong x, BasicType type);\n@@ -261,1 +261,4 @@\n-  void do_currentThread(Intrinsic* x);\n+  void do_currentCarrierThread(Intrinsic* x);\n+  void do_extentLocalCache(Intrinsic* x);\n+  void do_vthread(Intrinsic* x);\n+  void do_JavaThreadField(Intrinsic* x, ByteSize offset);\n@@ -275,0 +278,1 @@\n+  void do_continuation_doYield(Intrinsic* x);\n@@ -664,1 +668,1 @@\n-           \"shouldn't use set_destroys_register with physical regsiters\");\n+           \"shouldn't use set_destroys_register with physical registers\");\n@@ -674,1 +678,0 @@\n-    return _result;\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.hpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -271,1 +271,1 @@\n-  assert(_intervals.at(reg_num) == NULL, \"overwriting exisiting interval\");\n+  assert(_intervals.at(reg_num) == NULL, \"overwriting existing interval\");\n@@ -387,1 +387,1 @@\n-\/\/ called once before asignment of register numbers\n+\/\/ called once before assignment of register numbers\n@@ -543,2 +543,2 @@\n-  assert(con == NULL || opr->is_virtual() || opr->is_constant() || opr->is_illegal(), \"asumption: Constant instructions have only constant operands\");\n-  assert(con != NULL || opr->is_virtual(), \"asumption: non-Constant instructions have only virtual operands\");\n+  assert(con == NULL || opr->is_virtual() || opr->is_constant() || opr->is_illegal(), \"assumption: Constant instructions have only constant operands\");\n+  assert(con != NULL || opr->is_virtual(), \"assumption: non-Constant instructions have only virtual operands\");\n@@ -1243,2 +1243,2 @@\n-      assert(op->as_Op2() != NULL, \"lir_cmove must be LIR_Op2\");\n-      LIR_Op2* cmove = (LIR_Op2*)op;\n+      assert(op->as_Op4() != NULL, \"lir_cmove must be LIR_Op4\");\n+      LIR_Op4* cmove = (LIR_Op4*)op;\n@@ -1247,1 +1247,1 @@\n-      LIR_Opr move_to = cmove->result_opr();\n+      LIR_Opr move_to   = cmove->result_opr();\n@@ -1598,1 +1598,1 @@\n-        \/\/ the asumption that the intervals are already sorted failed,\n+        \/\/ the assumption that the intervals are already sorted failed,\n@@ -1746,1 +1746,1 @@\n-    assert(r < num_virtual_regs(), \"live information set for not exisiting interval\");\n+    assert(r < num_virtual_regs(), \"live information set for not existing interval\");\n@@ -2832,2 +2832,2 @@\n-    assert(con == NULL || opr->is_virtual() || opr->is_constant() || opr->is_illegal(), \"asumption: Constant instructions have only constant operands (or illegal if constant is optimized away)\");\n-    assert(con != NULL || opr->is_virtual(), \"asumption: non-Constant instructions have only virtual operands\");\n+    assert(con == NULL || opr->is_virtual() || opr->is_constant() || opr->is_illegal(), \"assumption: Constant instructions have only constant operands (or illegal if constant is optimized away)\");\n+    assert(con != NULL || opr->is_virtual(), \"assumption: non-Constant instructions have only virtual operands\");\n@@ -3134,0 +3134,3 @@\n+#ifndef RISCV\n+  \/\/ Disable these optimizations on riscv temporarily, because it does not\n+  \/\/ work when the comparison operands are bound to branches or cmoves.\n@@ -3141,0 +3144,1 @@\n+#endif\n@@ -3476,1 +3480,1 @@\n-      \/\/ TKR assert(value->as_Constant() == NULL || value->is_pinned(), \"only pinned constants can be alive accross block boundaries\");\n+      \/\/ TKR assert(value->as_Constant() == NULL || value->is_pinned(), \"only pinned constants can be alive across block boundaries\");\n@@ -4001,1 +4005,1 @@\n-        \/\/ this inverval can be processed because target is free\n+        \/\/ this interval can be processed because target is free\n@@ -4251,1 +4255,1 @@\n-    assert(_register_hint->is_split_parent(), \"ony split parents are valid hint registers\");\n+    assert(_register_hint->is_split_parent(), \"only split parents are valid hint registers\");\n@@ -5164,1 +5168,1 @@\n-      \/\/ seach optimal block boundary between min_split_pos and max_split_pos\n+      \/\/ search optimal block boundary between min_split_pos and max_split_pos\n@@ -5734,1 +5738,1 @@\n-    \/\/ perform splitting and spilling for all affected intervalls\n+    \/\/ perform splitting and spilling for all affected intervals\n@@ -6246,1 +6250,1 @@\n-  assert(instructions->last()->as_OpBranch() != NULL, \"last instrcution must always be a branch\");\n+  assert(instructions->last()->as_OpBranch() != NULL, \"last instruction must always be a branch\");\n@@ -6364,1 +6368,1 @@\n-              LIR_Op2* prev_cmove = NULL;\n+              LIR_Op4* prev_cmove = NULL;\n@@ -6370,2 +6374,2 @@\n-                  assert(prev_op->as_Op2() != NULL, \"cmove must be of type LIR_Op2\");\n-                  prev_cmove = (LIR_Op2*)prev_op;\n+                  assert(prev_op->as_Op4() != NULL, \"cmove must be of type LIR_Op4\");\n+                  prev_cmove = (LIR_Op4*)prev_op;\n@@ -6427,1 +6431,1 @@\n-      \/\/       -> this may lead to unnecesary return instructions in the final code\n+      \/\/       -> this may lead to unnecessary return instructions in the final code\n@@ -6856,1 +6860,1 @@\n-    \/\/ is necesary to start and stop itself\n+    \/\/ is necessary to start and stop itself\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":27,"deletions":23,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -169,1 +169,1 @@\n-  \/\/ this can happen when t_block or f_block contained additonal stores to local variables\n+  \/\/ this can happen when t_block or f_block contained additional stores to local variables\n@@ -340,1 +340,1 @@\n-\/\/ This removes others' relation to block, but doesnt empty block's lists\n+\/\/ This removes others' relation to block, but doesn't empty block's lists\n","filename":"src\/hotspot\/share\/c1\/c1_Optimizer.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -178,1 +178,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -188,1 +191,4 @@\n-    RegisterMap reg_map(current, false);\n+    RegisterMap reg_map(current,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -335,1 +341,1 @@\n-  FUNCTION_CASE(entry, JFR_TIME_FUNCTION);\n+  FUNCTION_CASE(entry, JfrTime::time_function());\n@@ -347,0 +353,1 @@\n+  FUNCTION_CASE(entry, StubRoutines::cont_doYield());\n@@ -565,1 +572,4 @@\n-  RegisterMap map(current, false);\n+  RegisterMap map(current,\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::include,\n+                  RegisterMap::WalkContinuation::skip);\n@@ -604,1 +614,4 @@\n-      RegisterMap map(current, false);\n+      RegisterMap map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -650,1 +663,4 @@\n-    RegisterMap map(current, false);\n+    RegisterMap map(current,\n+                    RegisterMap::UpdateMap::skip,\n+                    RegisterMap::ProcessFrames::include,\n+                    RegisterMap::WalkContinuation::skip);\n@@ -675,1 +691,1 @@\n-  \/\/ Check the stack guard pages and reenable them if necessary and there is\n+  \/\/ Check the stack guard pages and re-enable them if necessary and there is\n@@ -689,1 +705,4 @@\n-    RegisterMap reg_map(current);\n+    RegisterMap reg_map(current,\n+                        RegisterMap::UpdateMap::include,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -891,1 +910,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -996,1 +1018,1 @@\n-\/\/ thread against the intializing thread so other threads will enter\n+\/\/ thread against the initializing thread so other threads will enter\n@@ -1040,1 +1062,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1415,1 +1440,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1542,1 +1570,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1556,1 +1587,1 @@\n-    Method::build_interpreter_method_data(m, THREAD);\n+    Method::build_profiling_method_data(m, THREAD);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":46,"deletions":15,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -135,1 +135,1 @@\n-          \"Eliminate unneccessary basic blocks\")                            \\\n+          \"Eliminate unnecessary basic blocks\")                            \\\n@@ -141,1 +141,1 @@\n-          \"Eliminate unneccessary null checks\")                             \\\n+          \"Eliminate unnecessary null checks\")                             \\\n@@ -317,1 +317,1 @@\n-          \"Update MethodData*s in Tier1-generated code\")                    \\\n+          \"Update MethodData*s in Tier 3 C1 generated code\")                \\\n","filename":"src\/hotspot\/share\/c1\/c1_globals.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"cds\/heapShared.hpp\"\n@@ -43,0 +44,1 @@\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -46,0 +48,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -47,1 +50,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -330,1 +332,1 @@\n-    vm_direct_exit(0);\n+    os::_exit(0);\n@@ -380,1 +382,1 @@\n-    vm_direct_exit(0);\n+    os::_exit(0);\n@@ -739,0 +741,1 @@\n+    const char* generated = \"\";\n@@ -783,0 +786,3 @@\n+      if (ik->is_generated_shared_class()) {\n+        generated = \" ** generated\";\n+      }\n@@ -789,1 +795,3 @@\n-      log_debug(cds, class)(\"klasses[%5d] = \" PTR_FORMAT \" %-5s %s%s%s\", i, p2i(to_requested(k)), type, k->external_name(), hidden, unlinked);\n+      log_debug(cds, class)(\"klasses[%5d] = \" PTR_FORMAT \" %-5s %s%s%s%s\", i,\n+                            p2i(to_requested(k)), type, k->external_name(),\n+                            hidden, unlinked, generated);\n@@ -943,1 +951,2 @@\n-  static void write_dump_region(const char* name, DumpRegion* region) {\n+  static void log_metaspace_region(const char* name, DumpRegion* region,\n+                                   const ArchiveBuilder::SourceObjList* src_objs) {\n@@ -946,1 +955,2 @@\n-    write_region(name, region_base, region_top, region_base + buffer_to_runtime_delta());\n+    log_region(name, region_base, region_top, region_base + buffer_to_runtime_delta());\n+    log_metaspace_objects(region, src_objs);\n@@ -951,1 +961,1 @@\n-  static void write_klass(Klass* k, address runtime_dest, const char* type_name, int bytes, Thread* current) {\n+  static void log_klass(Klass* k, address runtime_dest, const char* type_name, int bytes, Thread* current) {\n@@ -956,1 +966,1 @@\n-  static void write_method(Method* m, address runtime_dest, const char* type_name, int bytes, Thread* current) {\n+  static void log_method(Method* m, address runtime_dest, const char* type_name, int bytes, Thread* current) {\n@@ -963,1 +973,1 @@\n-  static void write_objects(DumpRegion* region, const ArchiveBuilder::SourceObjList* src_objs) {\n+  static void log_metaspace_objects(DumpRegion* region, const ArchiveBuilder::SourceObjList* src_objs) {\n@@ -972,1 +982,1 @@\n-      write_data(last_obj_base, dest, last_obj_base + buffer_to_runtime_delta());\n+      log_data(last_obj_base, dest, last_obj_base + buffer_to_runtime_delta());\n@@ -981,1 +991,1 @@\n-        write_klass((Klass*)src, runtime_dest, type_name, bytes, current);\n+        log_klass((Klass*)src, runtime_dest, type_name, bytes, current);\n@@ -984,1 +994,1 @@\n-        write_klass(((ConstantPool*)src)->pool_holder(),\n+        log_klass(((ConstantPool*)src)->pool_holder(),\n@@ -988,1 +998,1 @@\n-        write_klass(((ConstantPoolCache*)src)->constant_pool()->pool_holder(),\n+        log_klass(((ConstantPoolCache*)src)->constant_pool()->pool_holder(),\n@@ -992,1 +1002,1 @@\n-        write_method((Method*)src, runtime_dest, type_name, bytes, current);\n+        log_method((Method*)src, runtime_dest, type_name, bytes, current);\n@@ -995,1 +1005,1 @@\n-        write_method(((ConstMethod*)src)->method(), runtime_dest, type_name, bytes, current);\n+        log_method(((ConstMethod*)src)->method(), runtime_dest, type_name, bytes, current);\n@@ -1014,1 +1024,1 @@\n-    write_data(last_obj_base, last_obj_end, last_obj_base + buffer_to_runtime_delta());\n+    log_data(last_obj_base, last_obj_end, last_obj_base + buffer_to_runtime_delta());\n@@ -1019,1 +1029,1 @@\n-      write_data(last_obj_end, region_end, last_obj_end + buffer_to_runtime_delta());\n+      log_data(last_obj_end, region_end, last_obj_end + buffer_to_runtime_delta());\n@@ -1025,2 +1035,2 @@\n-  \/\/ Write information about a region, whose address at dump time is [base .. top). At\n-  \/\/ runtime, this region will be mapped to runtime_base.  runtime_base is 0 if this\n+  \/\/ Log information about a region, whose address at dump time is [base .. top). At\n+  \/\/ runtime, this region will be mapped to requested_base. requested_base is 0 if this\n@@ -1029,1 +1039,4 @@\n-  static void write_region(const char* name, address base, address top, address runtime_base) {\n+  \/\/\n+  \/\/ Note: across -Xshare:dump runs, base may be different, but requested_base should\n+  \/\/ be the same as the archive contents should be deterministic.\n+  static void log_region(const char* name, address base, address top, address requested_base) {\n@@ -1031,2 +1044,2 @@\n-    base = runtime_base;\n-    top = runtime_base + size;\n+    base = requested_base;\n+    top = requested_base + size;\n@@ -1037,0 +1050,1 @@\n+#if INCLUDE_CDS_JAVA_HEAP\n@@ -1038,1 +1052,1 @@\n-  static void write_heap_region(const char* which, GrowableArray<MemRegion> *regions) {\n+  static void log_heap_regions(const char* which, GrowableArray<MemRegion> *regions) {\n@@ -1042,2 +1056,30 @@\n-      write_region(which, start, end, start);\n-      write_data(start, end, start);\n+      log_region(which, start, end, to_requested(start));\n+\n+      while (start < end) {\n+        size_t byte_size;\n+        oop archived_oop = cast_to_oop(start);\n+        oop original_oop = HeapShared::get_original_object(archived_oop);\n+        if (original_oop != NULL) {\n+          ResourceMark rm;\n+          log_info(cds, map)(PTR_FORMAT \": @@ Object %s\",\n+                             p2i(to_requested(start)), original_oop->klass()->external_name());\n+          byte_size = original_oop->size() * BytesPerWord;\n+        } else if (archived_oop == HeapShared::roots()) {\n+          \/\/ HeapShared::roots() is copied specially so it doesn't exist in\n+          \/\/ HeapShared::OriginalObjectTable. See HeapShared::copy_roots().\n+          log_info(cds, map)(PTR_FORMAT \": @@ Object HeapShared::roots (ObjArray)\",\n+                             p2i(to_requested(start)));\n+          byte_size = objArrayOopDesc::object_size(HeapShared::roots()->length()) * BytesPerWord;\n+        } else {\n+          \/\/ We have reached the end of the region\n+          break;\n+        }\n+        address oop_end = start + byte_size;\n+        log_data(start, oop_end, to_requested(start), \/*is_heap=*\/true);\n+        start = oop_end;\n+      }\n+      if (start < end) {\n+        log_info(cds, map)(PTR_FORMAT \": @@ Unused heap space \" SIZE_FORMAT \" bytes\",\n+                           p2i(to_requested(start)), size_t(end - start));\n+        log_data(start, end, to_requested(start), \/*is_heap=*\/true);\n+      }\n@@ -1046,0 +1088,4 @@\n+  static address to_requested(address p) {\n+    return HeapShared::to_requested_address(p);\n+  }\n+#endif\n@@ -1047,3 +1093,3 @@\n-  \/\/ Dump all the data [base...top). Pretend that the base address\n-  \/\/ will be mapped to runtime_base at run-time.\n-  static void write_data(address base, address top, address runtime_base) {\n+  \/\/ Log all the data [base...top). Pretend that the base address\n+  \/\/ will be mapped to requested_base at run-time.\n+  static void log_data(address base, address top, address requested_base, bool is_heap = false) {\n@@ -1054,1 +1100,7 @@\n-      os::print_hex_dump(&lsh, base, top, sizeof(address), 32, runtime_base);\n+      int unitsize = sizeof(address);\n+      if (is_heap && UseCompressedOops) {\n+        \/\/ This makes the compressed oop pointers easier to read, but\n+        \/\/ longs and doubles will be split into two words.\n+        unitsize = sizeof(narrowOop);\n+      }\n+      os::print_hex_dump(&lsh, base, top, unitsize, 32, requested_base);\n@@ -1058,1 +1110,1 @@\n-  static void write_header(FileMapInfo* mapinfo) {\n+  static void log_header(FileMapInfo* mapinfo) {\n@@ -1066,4 +1118,4 @@\n-  static void write(ArchiveBuilder* builder, FileMapInfo* mapinfo,\n-             GrowableArray<MemRegion> *closed_heap_regions,\n-             GrowableArray<MemRegion> *open_heap_regions,\n-             char* bitmap, size_t bitmap_size_in_bytes) {\n+  static void log(ArchiveBuilder* builder, FileMapInfo* mapinfo,\n+                  GrowableArray<MemRegion> *closed_heap_regions,\n+                  GrowableArray<MemRegion> *open_heap_regions,\n+                  char* bitmap, size_t bitmap_size_in_bytes) {\n@@ -1074,3 +1126,3 @@\n-    write_region(\"header\", header, header_end, 0);\n-    write_header(mapinfo);\n-    write_data(header, header_end, 0);\n+    log_region(\"header\", header, header_end, 0);\n+    log_header(mapinfo);\n+    log_data(header, header_end, 0);\n@@ -1081,5 +1133,2 @@\n-    write_dump_region(\"rw region\", rw_region);\n-    write_objects(rw_region, &builder->_rw_src_objs);\n-\n-    write_dump_region(\"ro region\", ro_region);\n-    write_objects(ro_region, &builder->_ro_src_objs);\n+    log_metaspace_region(\"rw region\", rw_region, &builder->_rw_src_objs);\n+    log_metaspace_region(\"ro region\", ro_region, &builder->_ro_src_objs);\n@@ -1088,2 +1137,2 @@\n-    write_region(\"bitmap\", address(bitmap), bitmap_end, 0);\n-    write_data(header, header_end, 0);\n+    log_region(\"bitmap\", address(bitmap), bitmap_end, 0);\n+    log_data((address)bitmap, bitmap_end, 0);\n@@ -1091,0 +1140,1 @@\n+#if INCLUDE_CDS_JAVA_HEAP\n@@ -1092,1 +1142,1 @@\n-      write_heap_region(\"closed heap region\", closed_heap_regions);\n+      log_heap_regions(\"closed heap region\", closed_heap_regions);\n@@ -1095,1 +1145,1 @@\n-      write_heap_region(\"open heap region\", open_heap_regions);\n+      log_heap_regions(\"open heap region\", open_heap_regions);\n@@ -1097,0 +1147,1 @@\n+#endif\n@@ -1100,1 +1151,1 @@\n-};\n+}; \/\/ end ArchiveBuilder::CDSMapLogger\n@@ -1154,2 +1205,2 @@\n-    CDSMapLogger::write(this, mapinfo, closed_heap_regions, open_heap_regions,\n-                        bitmap, bitmap_size_in_bytes);\n+    CDSMapLogger::log(this, mapinfo, closed_heap_regions, open_heap_regions,\n+                      bitmap, bitmap_size_in_bytes);\n@@ -1157,0 +1208,1 @@\n+  CDS_JAVA_HEAP_ONLY(HeapShared::destroy_archived_object_cache());\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":101,"deletions":49,"binary":false,"changes":150,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,1 @@\n+#include \"oops\/instanceStackChunkKlass.hpp\"\n@@ -63,0 +64,1 @@\n+  f(InstanceStackChunkKlass) \\\n","filename":"src\/hotspot\/share\/cds\/cppVtables.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -234,4 +234,3 @@\n-FileMapInfo::FileMapInfo(const char* full_path, bool is_static) {\n-  memset((void*)this, 0, sizeof(FileMapInfo));\n-  _full_path = full_path;\n-  _is_static = is_static;\n+FileMapInfo::FileMapInfo(const char* full_path, bool is_static) :\n+  _is_static(is_static), _file_open(false), _is_mapped(false), _fd(-1), _file_offset(0),\n+  _full_path(full_path), _base_archive_name(nullptr), _header(nullptr) {\n@@ -245,2 +244,0 @@\n-  _file_offset = 0;\n-  _file_open = false;\n@@ -257,0 +254,5 @@\n+\n+  if (_header != nullptr) {\n+    os::free(_header);\n+  }\n+\n@@ -275,2 +277,5 @@\n-    if (!FLAG_IS_DEFAULT(SharedArchiveFile)) {\n-      base_archive_name_size = strlen(Arguments::GetSharedArchivePath()) + 1;\n+\n+    const char* default_base_archive_name = Arguments::get_default_shared_archive_path();\n+    const char* current_base_archive_name = Arguments::GetSharedArchivePath();\n+    if (!os::same_files(current_base_archive_name, default_base_archive_name)) {\n+      base_archive_name_size = strlen(current_base_archive_name) + 1;\n@@ -280,0 +285,1 @@\n+    FREE_C_HEAP_ARRAY(const char, default_base_archive_name);\n@@ -317,2 +323,4 @@\n-      _heap_begin = (address)G1CollectedHeap::heap()->reserved().start();\n-      _heap_end = (address)G1CollectedHeap::heap()->reserved().end();\n+      address start = (address)G1CollectedHeap::heap()->reserved().start();\n+      address end = (address)G1CollectedHeap::heap()->reserved().end();\n+      _heap_begin = HeapShared::to_requested_address(start);\n+      _heap_end = HeapShared::to_requested_address(end);\n@@ -366,1 +374,1 @@\n-  st->print_cr(\"- version:                        %d\", version());\n+  st->print_cr(\"- version:                        0x%x\", version());\n@@ -928,1 +936,1 @@\n-  char* runtime_boot_path = Arguments::get_sysclasspath();\n+  char* runtime_boot_path = Arguments::get_boot_class_path();\n@@ -1217,1 +1225,1 @@\n-      FileMapInfo::fail_continue(\"Cannot handle shared archive file version %d. Must be at least %d\",\n+      FileMapInfo::fail_continue(\"Cannot handle shared archive file version 0x%x. Must be at least 0x%x.\",\n@@ -1223,2 +1231,2 @@\n-      FileMapInfo::fail_continue(\"The shared archive file version %d does not match the required version %d\",\n-                                    gen_header._version, CURRENT_CDS_ARCHIVE_VERSION);\n+      FileMapInfo::fail_continue(\"The shared archive file version 0x%x does not match the required version 0x%x.\",\n+                                 gen_header._version, CURRENT_CDS_ARCHIVE_VERSION);\n@@ -1398,2 +1406,2 @@\n-    log_info(cds)(\"_version expected: %d\", CURRENT_CDS_ARCHIVE_VERSION);\n-    log_info(cds)(\"           actual: %d\", header()->version());\n+    log_info(cds)(\"_version expected: 0x%x\", CURRENT_CDS_ARCHIVE_VERSION);\n+    log_info(cds)(\"           actual: 0x%x\", header()->version());\n@@ -1436,6 +1444,4 @@\n-  if (is_static()) {\n-    \/\/ just checking the last region is sufficient since the archive is written\n-    \/\/ in sequential order\n-    size_t len = os::lseek(fd, 0, SEEK_END);\n-    FileMapRegion* si = space_at(MetaspaceShared::last_valid_region);\n-    \/\/ The last space might be empty\n+  size_t len = os::lseek(fd, 0, SEEK_END);\n+\n+  for (int i = 0; i <= MetaspaceShared::last_valid_region; i++) {\n+    FileMapRegion* si = space_at(i);\n@@ -1831,1 +1837,1 @@\n-        \/\/ regions, or else it would mess up the simple comparision in MetaspaceObj::is_shared().\n+        \/\/ regions, or else it would mess up the simple comparison in MetaspaceObj::is_shared().\n@@ -1976,1 +1982,1 @@\n-    \/\/ Patch all pointers in the the mapped region that are marked by ptrmap.\n+    \/\/ Patch all pointers in the mapped region that are marked by ptrmap.\n@@ -1981,1 +1987,1 @@\n-    \/\/ range (i.e., must be between the requesed base address, and the of the current archive).\n+    \/\/ range (i.e., must be between the requested base address and the address of the current archive).\n@@ -2404,0 +2410,5 @@\n+    \/\/ G1 marking uses the BOT for object chunking during marking in\n+    \/\/ G1CMObjArrayProcessor::process_slice(); for this reason we need to\n+    \/\/ initialize the BOT for closed archive regions too.\n+    G1CollectedHeap::heap()->populate_archive_regions_bot_part(closed_heap_regions,\n+                                                               num_closed_heap_regions);\n@@ -2416,5 +2427,0 @@\n-    \/\/\n-    \/\/ This is only needed for open archive regions but not the closed archive\n-    \/\/ regions, because objects in closed archive regions never reference objects\n-    \/\/ outside the closed archive regions and they are immutable. So we never\n-    \/\/ need their BOT during garbage collection.\n@@ -2587,1 +2593,1 @@\n-                  \" does not equal the current ObjectAlignmentInBytes of \" INTX_FORMAT \".\",\n+                  \" does not equal the current ObjectAlignmentInBytes of %d.\",\n","filename":"src\/hotspot\/share\/cds\/filemap.cpp","additions":38,"deletions":32,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-#include \"oops\/objArrayOop.hpp\"\n+#include \"oops\/objArrayOop.inline.hpp\"\n@@ -56,0 +56,1 @@\n+#include \"oops\/typeArrayOop.inline.hpp\"\n@@ -93,1 +94,1 @@\n-\/\/ Suport for mapped heap (!UseCompressedOops only)\n+\/\/ Support for mapped heap (!UseCompressedOops only)\n@@ -181,5 +182,3 @@\n-  \/\/ We are at a safepoint, so the object won't move. It's OK to use its\n-  \/\/ address as the hashcode.\n-  \/\/ We can't use p->identity_hash() as it's not available for primitive oops.\n-  assert_at_safepoint();\n-  return (unsigned)(p2i(p) >> LogBytesPerWord);\n+  \/\/ Do not call p->identity_hash() as that will update the\n+  \/\/ object header.\n+  return primitive_hash(cast_from_oop<intptr_t>(p));\n@@ -216,0 +215,17 @@\n+\n+  \/\/ Clean up jdk.internal.loader.ClassLoaders::bootLoader(), which is not\n+  \/\/ directly used for class loading, but rather is used by the core library\n+  \/\/ to keep track of resources, etc, loaded by the null class loader.\n+  \/\/\n+  \/\/ Note, this object is non-null, and is not the same as\n+  \/\/ ClassLoaderData::the_null_class_loader_data()->class_loader(),\n+  \/\/ which is null.\n+  log_debug(cds)(\"Resetting boot loader\");\n+  JavaValue result(T_OBJECT);\n+  JavaCalls::call_static(&result,\n+                         vmClasses::jdk_internal_loader_ClassLoaders_klass(),\n+                         vmSymbols::bootLoader_name(),\n+                         vmSymbols::void_BuiltinClassLoader_signature(),\n+                         CHECK);\n+  Handle boot_loader(THREAD, result.get_oop());\n+  reset_states(boot_loader(), CHECK);\n@@ -219,0 +235,1 @@\n+HeapShared::OriginalObjectTable* HeapShared::_original_object_table = NULL;\n@@ -291,0 +308,2 @@\n+  assert(!obj->is_stackChunk(), \"do not archive stack chunks\");\n+\n@@ -325,0 +344,3 @@\n+    if (_original_object_table != NULL) {\n+      _original_object_table->put(archived_oop, obj);\n+    }\n@@ -334,3 +356,3 @@\n-    vm_direct_exit(-1,\n-      err_msg(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n-              SIZE_FORMAT \"M\", MaxHeapSize\/M));\n+    log_error(cds)(\"Out of memory. Please run with a larger Java heap, current MaxHeapSize = \"\n+        SIZE_FORMAT \"M\", MaxHeapSize\/M);\n+    os::_exit(-1);\n@@ -371,1 +393,1 @@\n-\/\/ no way of programatically handling this inside the Java code (as you would handle\n+\/\/ no way of programmatically handling this inside the Java code (as you would handle\n@@ -474,1 +496,1 @@\n-    create_archived_object_cache();\n+    create_archived_object_cache(log_is_enabled(Info, cds, map));\n@@ -488,1 +510,0 @@\n-    destroy_archived_object_cache();\n@@ -540,0 +561,6 @@\n+  \/\/ HeapShared::roots() points into an ObjArray in the open archive region. A portion of the\n+  \/\/ objects in this array are discovered during HeapShared::archive_objects(). For example,\n+  \/\/ in HeapShared::archive_reachable_objects_from() ->  HeapShared::check_enum_obj().\n+  \/\/ However, HeapShared::archive_objects() happens inside a safepoint, so we can't\n+  \/\/ allocate a \"regular\" ObjArray and pass the result to HeapShared::archive_object().\n+  \/\/ Instead, we have to roll our own alloc\/copy routine here.\n@@ -1143,1 +1170,1 @@\n-    vm_direct_exit(1);\n+    os::_exit(1);\n@@ -1153,1 +1180,1 @@\n-    vm_direct_exit(1);\n+    os::_exit(1);\n@@ -1189,1 +1216,1 @@\n-        vm_direct_exit(1);\n+        os::_exit(1);\n@@ -1237,1 +1264,1 @@\n-\/\/ refererence field and points to a non-null java object, proceed to\n+\/\/ reference field and points to a non-null java object, proceed to\n@@ -1252,1 +1279,1 @@\n-\/\/ for loading and initialzing before any object in the archived graph can\n+\/\/ for loading and initializing before any object in the archived graph can\n@@ -1558,0 +1585,1 @@\n+    assert(UseCompressedOops, \"sanity\");\n@@ -1561,0 +1589,2 @@\n+      \/\/ Note: HeapShared::to_requested_address() is not necessary because\n+      \/\/ the heap always starts at a deterministic address with UseCompressedOops==true.\n@@ -1568,0 +1598,1 @@\n+    assert(!UseCompressedOops, \"sanity\");\n@@ -1572,0 +1603,4 @@\n+      if (DumpSharedSpaces) {\n+        \/\/ Make heap content deterministic.\n+        *p = HeapShared::to_requested_address(*p);\n+      }\n@@ -1580,0 +1615,28 @@\n+\n+address HeapShared::to_requested_address(address dumptime_addr) {\n+  assert(DumpSharedSpaces, \"static dump time only\");\n+  if (dumptime_addr == NULL || UseCompressedOops) {\n+    return dumptime_addr;\n+  }\n+\n+  \/\/ With UseCompressedOops==false, actual_base is selected by the OS so\n+  \/\/ it's different across -Xshare:dump runs.\n+  address actual_base = (address)G1CollectedHeap::heap()->reserved().start();\n+  address actual_end  = (address)G1CollectedHeap::heap()->reserved().end();\n+  assert(actual_base <= dumptime_addr && dumptime_addr <= actual_end, \"must be an address in the heap\");\n+\n+  \/\/ We always write the objects as if the heap started at this address. This\n+  \/\/ makes the heap content deterministic.\n+  \/\/\n+  \/\/ Note that at runtime, the heap address is also selected by the OS, so\n+  \/\/ the archive heap will not be mapped at 0x10000000. Instead, we will call\n+  \/\/ HeapShared::patch_embedded_pointers() to relocate the heap contents\n+  \/\/ accordingly.\n+  const address REQUESTED_BASE = (address)0x10000000;\n+  intx delta = REQUESTED_BASE - actual_base;\n+\n+  address requested_addr = dumptime_addr + delta;\n+  assert(REQUESTED_BASE != 0 && requested_addr != NULL, \"sanity\");\n+  return requested_addr;\n+}\n+\n@@ -1600,1 +1663,1 @@\n-  log_info(cds, heap)(\"calculate_oopmap: objects = %6d, embedded oops = %7d, nulls = %7d\",\n+  log_info(cds, heap)(\"calculate_oopmap: objects = %6d, oop fields = %7d (nulls = %7d)\",\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":82,"deletions":19,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -169,1 +169,1 @@\n-    if (_dumped_classes.maybe_grow(MAX_TABLE_SIZE)) {\n+    if (_dumped_classes.maybe_grow()) {\n@@ -185,1 +185,1 @@\n-  : CLDClosure(), _dumped_classes(INITIAL_TABLE_SIZE) {\n+  : CLDClosure(), _dumped_classes(INITIAL_TABLE_SIZE, MAX_TABLE_SIZE) {\n@@ -587,1 +587,1 @@\n-  vm_direct_exit(0);\n+  os::_exit(0);\n@@ -1060,1 +1060,1 @@\n-    \/\/ archives, or else it would mess up the simple comparision in MetaspaceObj::is_shared().\n+    \/\/ archives, or else it would mess up the simple comparison in MetaspaceObj::is_shared().\n@@ -1585,1 +1585,1 @@\n-    st->print(\"SharedBaseAddress: \" PTR_FORMAT \", ArchiveRelocationMode: %d.\", SharedBaseAddress, (int)ArchiveRelocationMode);\n+    st->print(\"SharedBaseAddress: \" PTR_FORMAT \", ArchiveRelocationMode: %d.\", SharedBaseAddress, ArchiveRelocationMode);\n","filename":"src\/hotspot\/share\/cds\/metaspaceShared.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,1 +51,0 @@\n-class     ciNativeEntryPoint;\n@@ -104,1 +103,0 @@\n-friend class ciNativeEntryPoint;       \\\n","filename":"src\/hotspot\/share\/ci\/ciClassList.hpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-#include \"runtime\/reflection.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -75,0 +75,1 @@\n+#include \"runtime\/reflection.hpp\"\n@@ -77,1 +78,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -313,1 +313,1 @@\n-  \/\/ Get Jvmti capabilities under lock to get consistant values.\n+  \/\/ Get Jvmti capabilities under lock to get consistent values.\n@@ -761,1 +761,0 @@\n-  bool ignore_will_link;\n@@ -791,3 +790,5 @@\n-    ciKlass* klass = get_klass_by_index_impl(cpool, index, ignore_will_link, accessor);\n-    if (!klass->is_loaded()) {\n-      return ciConstant(T_OBJECT, get_unloaded_klass_mirror(klass));\n+    bool will_link;\n+    ciKlass* klass = get_klass_by_index_impl(cpool, index, will_link, accessor);\n+    ciInstance* mirror = (will_link ? klass->java_mirror() : get_unloaded_klass_mirror(klass));\n+    if (klass->is_loaded() && tag.is_Qdescriptor_klass()) {\n+      return ciConstant(T_OBJECT, klass->as_inline_klass()->val_mirror());\n@@ -795,5 +796,1 @@\n-      if (tag.is_Qdescriptor_klass()) {\n-        return ciConstant(T_OBJECT, klass->as_inline_klass()->val_mirror());\n-      } else {\n-        return ciConstant(T_OBJECT, klass->java_mirror());\n-      }\n+      return ciConstant(T_OBJECT, mirror);\n@@ -810,0 +807,1 @@\n+    bool ignore_will_link;\n@@ -1089,2 +1087,3 @@\n-                            RTMState  rtm_state,\n-                            const GrowableArrayView<RuntimeStub*>& native_invokers) {\n+                            bool has_monitors,\n+                            int immediate_oops_patched,\n+                            RTMState  rtm_state) {\n@@ -1154,0 +1153,4 @@\n+    if (!failing()) {\n+      code_buffer->finalize_stubs();\n+    }\n+\n@@ -1179,2 +1182,1 @@\n-                               compiler, task()->comp_level(),\n-                               native_invokers);\n+                               compiler, task()->comp_level());\n@@ -1188,0 +1190,2 @@\n+      nm->set_has_monitors(has_monitors);\n+      assert(!method->is_synchronized() || nm->has_monitors(), \"\");\n@@ -1249,0 +1253,7 @@\n+\/\/ ------------------------------------------------------------------\n+\/\/ ciEnv::find_system_klass\n+ciKlass* ciEnv::find_system_klass(ciSymbol* klass_name) {\n+  VM_ENTRY_MARK;\n+  return get_klass_by_name_impl(NULL, constantPoolHandle(), klass_name, false);\n+}\n+\n@@ -1652,1 +1663,1 @@\n-      \/\/ Look for MethodHandle contant pool entries\n+      \/\/ Look for MethodHandle constant pool entries\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":28,"deletions":17,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,1 +37,1 @@\n-#include \"runtime\/thread.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -343,1 +343,1 @@\n-  \/\/ Return state of appropriate compilability\n+  \/\/ Return state of appropriate compatibility\n@@ -401,2 +401,3 @@\n-                       RTMState                  rtm_state = NoRTM,\n-                       const GrowableArrayView<RuntimeStub*>& native_invokers = GrowableArrayView<RuntimeStub*>::EMPTY);\n+                       bool                      has_monitors,\n+                       int                       immediate_oops_patched,\n+                       RTMState                  rtm_state = NoRTM);\n@@ -443,0 +444,2 @@\n+  ciKlass*  find_system_klass(ciSymbol* klass_name);\n+\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":8,"deletions":5,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -256,1 +256,1 @@\n-      holder->is_in_package(\"jdk\/internal\/foreign\") || holder->is_in_package(\"jdk\/incubator\/foreign\") ||\n+      holder->is_in_package(\"jdk\/internal\/foreign\") || holder->is_in_package(\"java\/lang\/foreign\") ||\n","filename":"src\/hotspot\/share\/ci\/ciField.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -106,0 +106,2 @@\n+  _has_trusted_loader = compute_has_trusted_loader();\n+\n@@ -139,0 +141,1 @@\n+  _has_trusted_loader = compute_has_trusted_loader();\n@@ -287,25 +290,0 @@\n-static bool is_klass_initialized(Symbol* klass_name) {\n-  VM_ENTRY_MARK;\n-  InstanceKlass* ik = SystemDictionary::find_instance_klass(klass_name, Handle(), Handle());\n-  return ik != nullptr && ik->is_initialized();\n-}\n-\n-bool ciInstanceKlass::is_box_cache_valid() const {\n-  BasicType box_type = box_klass_type();\n-\n-  if (box_type != T_OBJECT) {\n-    switch(box_type) {\n-      case T_INT:     return is_klass_initialized(java_lang_Integer_IntegerCache::symbol());\n-      case T_CHAR:    return is_klass_initialized(java_lang_Character_CharacterCache::symbol());\n-      case T_SHORT:   return is_klass_initialized(java_lang_Short_ShortCache::symbol());\n-      case T_BYTE:    return is_klass_initialized(java_lang_Byte_ByteCache::symbol());\n-      case T_LONG:    return is_klass_initialized(java_lang_Long_LongCache::symbol());\n-      case T_BOOLEAN:\n-      case T_FLOAT:\n-      case T_DOUBLE:  return true;\n-      default:;\n-    }\n-  }\n-  return false;\n-}\n-\n@@ -640,0 +618,9 @@\n+bool ciInstanceKlass::compute_has_trusted_loader() {\n+  ASSERT_IN_VM;\n+  oop loader_oop = loader();\n+  if (loader_oop == NULL) {\n+    return true; \/\/ bootstrap class loader\n+  }\n+  return java_lang_ClassLoader::is_trusted_loader(loader_oop);\n+}\n+\n@@ -679,2 +666,4 @@\n-    \/\/ Go into the VM to fetch the implementor.\n-    {\n+    if (is_shared()) {\n+      impl = this; \/\/ assume a well-known interface never has a unique implementor\n+    } else {\n+      \/\/ Go into the VM to fetch the implementor.\n@@ -694,3 +683,1 @@\n-    if (!is_shared()) {\n-      _implementor = impl;\n-    }\n+    _implementor = impl;\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.cpp","additions":18,"deletions":31,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -62,0 +62,1 @@\n+  bool                   _has_trusted_loader;\n@@ -111,0 +112,1 @@\n+  bool compute_has_trusted_loader();\n@@ -273,1 +275,0 @@\n-  bool is_box_cache_valid() const;\n@@ -296,0 +297,4 @@\n+  bool has_trusted_loader() const {\n+    return _has_trusted_loader;\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciInstanceKlass.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -86,1 +86,0 @@\n-  _intrinsic_id       = h_m->intrinsic_id();\n@@ -106,0 +105,4 @@\n+  \/\/ Check for blackhole intrinsic and then populate the intrinsic ID.\n+  CompilerOracle::tag_blackhole_if_possible(h_m);\n+  _intrinsic_id       = h_m->intrinsic_id();\n+\n@@ -161,2 +164,0 @@\n-\n-  CompilerOracle::tag_blackhole_if_possible(h_m);\n@@ -1034,1 +1035,1 @@\n-    Method::build_interpreter_method_data(h_m, THREAD);\n+    Method::build_profiling_method_data(h_m, THREAD);\n","filename":"src\/hotspot\/share\/ci\/ciMethod.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -203,5 +203,6 @@\n-  bool caller_sensitive()      const { return get_Method()->caller_sensitive();      }\n-  bool force_inline()          const { return get_Method()->force_inline();          }\n-  bool dont_inline()           const { return get_Method()->dont_inline();           }\n-  bool intrinsic_candidate()   const { return get_Method()->intrinsic_candidate();   }\n-  bool is_class_initializer()  const { return get_Method()->is_class_initializer(); }\n+  bool caller_sensitive()       const { return get_Method()->caller_sensitive();       }\n+  bool force_inline()           const { return get_Method()->force_inline();           }\n+  bool dont_inline()            const { return get_Method()->dont_inline();            }\n+  bool intrinsic_candidate()    const { return get_Method()->intrinsic_candidate();    }\n+  bool is_class_initializer()   const { return get_Method()->is_class_initializer();   }\n+  bool changes_current_thread() const { return get_Method()->changes_current_thread(); }\n","filename":"src\/hotspot\/share\/ci\/ciMethod.hpp","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -251,1 +251,7 @@\n-  _state = mdo->is_mature()? mature_state: immature_state;\n+  if (_invocation_counter == 0 && mdo->backedge_count() > 0) {\n+    \/\/ Avoid skewing counter data during OSR compilation.\n+    \/\/ Sometimes, MDO is allocated during the very first invocation and OSR compilation is triggered\n+    \/\/ solely by backedge counter while invocation counter stays zero. In such case, it's important\n+    \/\/ to observe non-zero invocation count to properly scale profile counts (see ciMethod::scale_count()).\n+    _invocation_counter = 1;\n+  }\n@@ -253,0 +259,1 @@\n+  _state = mdo->is_mature() ? mature_state : immature_state;\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -525,1 +525,1 @@\n-  \/\/ Also set the numer of loops and blocks in the method.\n+  \/\/ Also set the number of loops and blocks in the method.\n","filename":"src\/hotspot\/share\/ci\/ciMethodData.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -172,0 +172,8 @@\n+ciObjArrayKlass* ciObjArrayKlass::make(ciKlass* element_klass, int dims) {\n+  ciKlass* klass = element_klass;\n+  for (int i = 0; i < dims; i++) {\n+    klass = ciObjArrayKlass::make(klass);\n+  }\n+  return klass->as_obj_array_klass();\n+}\n+\n","filename":"src\/hotspot\/share\/ci\/ciObjArrayKlass.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+  static ciObjArrayKlass* make(ciKlass* element_klass, int dims);\n","filename":"src\/hotspot\/share\/ci\/ciObjArrayKlass.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,0 @@\n-#include \"ci\/ciNativeEntryPoint.hpp\"\n@@ -350,2 +349,0 @@\n-    else if (jdk_internal_invoke_NativeEntryPoint::is_instance(o))\n-      return new (arena()) ciNativeEntryPoint(h_i);\n","filename":"src\/hotspot\/share\/ci\/ciObjectFactory.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -402,1 +403,6 @@\n-      Bytecode_invoke bytecode(caller, bci);\n+      Bytecode_invoke bytecode = Bytecode_invoke_check(caller, bci);\n+      if (!Bytecodes::is_defined(bytecode.code()) || !bytecode.is_valid()) {\n+        report_error(\"no invoke found at bci\");\n+        return NULL;\n+      }\n+      bytecode.verify();\n@@ -832,1 +838,1 @@\n-    \/\/ Method::build_interpreter_method_data(method, CHECK);\n+    \/\/ Method::build_profiling_method_data(method, CHECK);\n@@ -1495,1 +1501,1 @@\n-    \/\/ don't allow any intializers to be run.\n+    \/\/ don't allow any initializers to be run.\n","filename":"src\/hotspot\/share\/ci\/ciReplay.cpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -273,1 +273,1 @@\n-constantTag ciBytecodeStream::get_raw_pool_tag(int index) const {\n+constantTag ciBytecodeStream::get_raw_pool_tag_at(int index) const {\n","filename":"src\/hotspot\/share\/ci\/ciStreams.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -234,1 +234,1 @@\n-  constantTag get_raw_pool_tag(int index) const;\n+  constantTag get_raw_pool_tag_at(int index) const;\n@@ -236,1 +236,6 @@\n-  \/\/ True if the klass-using bytecode points to an unresolved klass\n+  constantTag get_raw_pool_tag() const {\n+    int index = get_constant_pool_index();\n+    return get_raw_pool_tag_at(index);\n+  }\n+\n+    \/\/ True if the klass-using bytecode points to an unresolved klass\n@@ -247,2 +252,1 @@\n-    int index = get_constant_pool_index();\n-    constantTag tag = get_raw_pool_tag(index);\n+    constantTag tag = get_raw_pool_tag();\n@@ -253,0 +257,9 @@\n+  bool is_string_constant() const {\n+    assert(cur_bc() == Bytecodes::_ldc    ||\n+           cur_bc() == Bytecodes::_ldc_w  ||\n+           cur_bc() == Bytecodes::_ldc2_w, \"not supported: %s\", Bytecodes::name(cur_bc()));\n+\n+    constantTag tag = get_raw_pool_tag();\n+    return tag.is_string();\n+  }\n+\n","filename":"src\/hotspot\/share\/ci\/ciStreams.hpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -57,2 +57,0 @@\n-  Symbol* get_symbol() const { return _symbol; }\n-\n@@ -118,0 +116,3 @@\n+\n+  Symbol* get_symbol() const { return _symbol; }\n+\n","filename":"src\/hotspot\/share\/ci\/ciSymbol.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2403,24 +2403,6 @@\n-  \/\/ Insert clone after (orig) tail in reverse post order\n-  clone->set_rpo_next(tail->rpo_next());\n-  tail->set_rpo_next(clone);\n-\n-  \/\/ tail->head becomes tail->clone\n-  for (SuccIter iter(tail); !iter.done(); iter.next()) {\n-    if (iter.succ() == head) {\n-      iter.set_succ(clone);\n-      \/\/ Update predecessor information\n-      head->predecessors()->remove(tail);\n-      clone->predecessors()->append(tail);\n-    }\n-  }\n-  flow_block(tail, temp_vector, temp_set);\n-  if (head == tail) {\n-    \/\/ For self-loops, clone->head becomes clone->clone\n-    flow_block(clone, temp_vector, temp_set);\n-    for (SuccIter iter(clone); !iter.done(); iter.next()) {\n-      if (iter.succ() == head) {\n-        iter.set_succ(clone);\n-        \/\/ Update predecessor information\n-        head->predecessors()->remove(clone);\n-        clone->predecessors()->append(clone);\n-        break;\n+  \/\/ Accumulate profiled count for all backedges that share this loop's head\n+  int total_count = lp->profiled_count();\n+  for (Loop* lp1 = lp->parent(); lp1 != NULL; lp1 = lp1->parent()) {\n+    for (Loop* lp2 = lp1; lp2 != NULL; lp2 = lp2->sibling()) {\n+      if (lp2->head() == head && !lp2->tail()->is_backedge_copy()) {\n+        total_count += lp2->profiled_count();\n@@ -2430,0 +2412,44 @@\n+  \/\/ Have the most frequent ones branch to the clone instead\n+  int count = 0;\n+  int loops_with_shared_head = 0;\n+  Block* latest_tail = tail;\n+  bool done = false;\n+  for (Loop* lp1 = lp; lp1 != NULL && !done; lp1 = lp1->parent()) {\n+    for (Loop* lp2 = lp1; lp2 != NULL && !done; lp2 = lp2->sibling()) {\n+      if (lp2->head() == head && !lp2->tail()->is_backedge_copy()) {\n+        count += lp2->profiled_count();\n+        if (lp2->tail()->post_order() < latest_tail->post_order()) {\n+          latest_tail = lp2->tail();\n+        }\n+        loops_with_shared_head++;\n+        for (SuccIter iter(lp2->tail()); !iter.done(); iter.next()) {\n+          if (iter.succ() == head) {\n+            iter.set_succ(clone);\n+            \/\/ Update predecessor information\n+            head->predecessors()->remove(lp2->tail());\n+            clone->predecessors()->append(lp2->tail());\n+          }\n+        }\n+        flow_block(lp2->tail(), temp_vector, temp_set);\n+        if (lp2->head() == lp2->tail()) {\n+          \/\/ For self-loops, clone->head becomes clone->clone\n+          flow_block(clone, temp_vector, temp_set);\n+          for (SuccIter iter(clone); !iter.done(); iter.next()) {\n+            if (iter.succ() == lp2->head()) {\n+              iter.set_succ(clone);\n+              \/\/ Update predecessor information\n+              lp2->head()->predecessors()->remove(clone);\n+              clone->predecessors()->append(clone);\n+              break;\n+            }\n+          }\n+        }\n+        if (total_count == 0 || count > (total_count * .9)) {\n+          done = true;\n+        }\n+      }\n+    }\n+  }\n+  assert(loops_with_shared_head >= 1, \"at least one new\");\n+  clone->set_rpo_next(latest_tail->rpo_next());\n+  latest_tail->set_rpo_next(clone);\n@@ -2569,2 +2595,5 @@\n-int ciTypeFlow::profiled_count(ciTypeFlow::Loop* loop) {\n-  ciMethodData* methodData = method()->method_data();\n+int ciTypeFlow::Loop::profiled_count() {\n+  if (_profiled_count >= 0) {\n+    return _profiled_count;\n+  }\n+  ciMethodData* methodData = outer()->method()->method_data();\n@@ -2572,0 +2601,1 @@\n+    _profiled_count = 0;\n@@ -2574,2 +2604,3 @@\n-  ciTypeFlow::Block* tail = loop->tail();\n-  if (tail->control() == -1) {\n+  ciTypeFlow::Block* tail = this->tail();\n+  if (tail->control() == -1 || tail->has_trap()) {\n+    _profiled_count = 0;\n@@ -2582,0 +2613,1 @@\n+    _profiled_count = 0;\n@@ -2585,1 +2617,1 @@\n-  ciBytecodeStream iter(method());\n+  ciBytecodeStream iter(outer()->method());\n@@ -2624,3 +2656,4 @@\n-    assert(((wide ? iter.get_far_dest() : iter.get_dest()) == loop->head()->start()) == (succs->at(ciTypeFlow::GOTO_TARGET) == loop->head()), \"branch should lead to loop head\");\n-    if (succs->at(ciTypeFlow::GOTO_TARGET) == loop->head()) {\n-      return method()->scale_count(data->as_JumpData()->taken());\n+    assert(((wide ? iter.get_far_dest() : iter.get_dest()) == head()->start()) == (succs->at(ciTypeFlow::GOTO_TARGET) == head()), \"branch should lead to loop head\");\n+    if (succs->at(ciTypeFlow::GOTO_TARGET) == head()) {\n+      _profiled_count = outer()->method()->scale_count(data->as_JumpData()->taken());\n+      return _profiled_count;\n@@ -2629,6 +2662,8 @@\n-    assert((iter.get_dest() == loop->head()->start()) == (succs->at(ciTypeFlow::IF_TAKEN) == loop->head()), \"bytecode and CFG not consistent\");\n-    assert((tail->limit() == loop->head()->start()) == (succs->at(ciTypeFlow::IF_NOT_TAKEN) == loop->head()), \"bytecode and CFG not consistent\");\n-    if (succs->at(ciTypeFlow::IF_TAKEN) == loop->head()) {\n-      return method()->scale_count(data->as_JumpData()->taken());\n-    } else if (succs->at(ciTypeFlow::IF_NOT_TAKEN) == loop->head()) {\n-      return method()->scale_count(data->as_BranchData()->not_taken());\n+    assert((iter.get_dest() == head()->start()) == (succs->at(ciTypeFlow::IF_TAKEN) == head()), \"bytecode and CFG not consistent\");\n+    assert((tail->limit() == head()->start()) == (succs->at(ciTypeFlow::IF_NOT_TAKEN) == head()), \"bytecode and CFG not consistent\");\n+    if (succs->at(ciTypeFlow::IF_TAKEN) == head()) {\n+      _profiled_count = outer()->method()->scale_count(data->as_JumpData()->taken());\n+      return _profiled_count;\n+    } else if (succs->at(ciTypeFlow::IF_NOT_TAKEN) == head()) {\n+      _profiled_count = outer()->method()->scale_count(data->as_BranchData()->not_taken());\n+      return _profiled_count;\n@@ -2638,1 +2673,2 @@\n-  return 0;\n+  _profiled_count = 0;\n+  return _profiled_count;\n@@ -2650,2 +2686,2 @@\n-    int lp_count = outer()->profiled_count(lp);\n-    int current_count = outer()->profiled_count(current);\n+    int lp_count = lp->profiled_count();\n+    int current_count = current->profiled_count();\n","filename":"src\/hotspot\/share\/ci\/ciTypeFlow.cpp","additions":77,"deletions":41,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -374,1 +374,1 @@\n-      \/\/ a double or long value since it's seconf half is being overwritten.\n+      \/\/ a double or long value since its second half is being overwritten.\n@@ -722,0 +722,1 @@\n+    int _profiled_count;\n@@ -730,1 +731,1 @@\n-      _irreducible(false), _def_locals() {}\n+      _irreducible(false), _def_locals(), _profiled_count(-1) {}\n@@ -766,0 +767,2 @@\n+    int profiled_count();\n+\n@@ -924,2 +927,0 @@\n-  int profiled_count(ciTypeFlow::Loop* loop);\n-\n","filename":"src\/hotspot\/share\/ci\/ciTypeFlow.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -147,1 +147,3 @@\n-#define CONSTANT_CLASS_DESCRIPTORS        63\n+#define JAVA_20_VERSION                   64\n+\n+#define CONSTANT_CLASS_DESCRIPTORS        64\n@@ -1000,0 +1002,2 @@\n+    _method_ChangesCurrentThread,\n+    _method_JvmtiMountTransition,\n@@ -1721,1 +1725,1 @@\n-  \/\/ fields array is trimed. Also unused slots that were reserved\n+  \/\/ fields array is trimmed. Also unused slots that were reserved\n@@ -2067,0 +2071,10 @@\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_ChangesCurrentThread_signature): {\n+      if (_location != _in_method)  break;  \/\/ only allow for methods\n+      if (!privileged)              break;  \/\/ only allow in privileged code\n+      return _method_ChangesCurrentThread;\n+    }\n+    case VM_SYMBOL_ENUM_NAME(jdk_internal_vm_annotation_JvmtiMountTransition_signature): {\n+      if (_location != _in_method)  break;  \/\/ only allow for methods\n+      if (!privileged)              break;  \/\/ only allow in privileged code\n+      return _method_JvmtiMountTransition;\n+    }\n@@ -2113,1 +2127,1 @@\n-      if (!privileged)              break;  \/\/ only allow in priviledged code\n+      if (!privileged)              break;  \/\/ only allow in privileged code\n@@ -2143,0 +2157,4 @@\n+  if (has_annotation(_method_ChangesCurrentThread))\n+    m->set_changes_current_thread(true);\n+  if (has_annotation(_method_JvmtiMountTransition))\n+    m->set_jvmti_mount_transition(true);\n@@ -4151,1 +4169,1 @@\n-    \/\/ The annotations arrays below has been transfered the\n+    \/\/ The annotations arrays below has been transferred the\n@@ -4304,1 +4322,1 @@\n-   * Since field layout sneeks in oops before values, we will be able to condense\n+   * Since field layout sneaks in oops before values, we will be able to condense\n@@ -4461,2 +4479,2 @@\n-  return _major_version > JAVA_17_VERSION ||\n-         (_major_version == JAVA_17_VERSION && _minor_version == JAVA_PREVIEW_MINOR_VERSION);\n+  return _major_version > JAVA_20_VERSION ||\n+         (_major_version == JAVA_20_VERSION \/*&& _minor_version == JAVA_PREVIEW_MINOR_VERSION*\/); \/\/ JAVA_PREVIEW_MINOR_VERSION not yet implemented by javac, check JVMS draft\n@@ -5935,1 +5953,0 @@\n-  _rt(REF_NONE),\n@@ -6237,2 +6254,2 @@\n-                           class_name_in_cp->as_C_string(),\n-                           _class_name->as_C_string()\n+                           _class_name->as_C_string(),\n+                           class_name_in_cp->as_C_string()\n@@ -6556,3 +6573,0 @@\n-\n-  \/\/ Compute reference type\n-  _rt = (NULL ==_super_klass) ? REF_NONE : _super_klass->reference_type();\n@@ -6591,0 +6605,26 @@\n+ReferenceType ClassFileParser::super_reference_type() const {\n+  return _super_klass == NULL ? REF_NONE : _super_klass->reference_type();\n+}\n+\n+bool ClassFileParser::is_instance_ref_klass() const {\n+  \/\/ Only the subclasses of j.l.r.Reference are InstanceRefKlass.\n+  \/\/ j.l.r.Reference itself is InstanceKlass because InstanceRefKlass denotes a\n+  \/\/ klass requiring special treatment in ref-processing. The abstract\n+  \/\/ j.l.r.Reference cannot be instantiated so doesn't partake in\n+  \/\/ ref-processing.\n+  return is_java_lang_ref_Reference_subclass();\n+}\n+\n+bool ClassFileParser::is_java_lang_ref_Reference_subclass() const {\n+  if (_super_klass == NULL) {\n+    return false;\n+  }\n+\n+  if (_super_klass->name() == vmSymbols::java_lang_ref_Reference()) {\n+    \/\/ Direct subclass of j.l.r.Reference: Soft|Weak|Final|Phantom\n+    return true;\n+  }\n+\n+  return _super_klass->reference_type() != REF_NONE;\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.cpp","additions":53,"deletions":13,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -166,1 +166,0 @@\n-  ReferenceType _rt;\n@@ -612,1 +611,4 @@\n-  ReferenceType reference_type() const { return _rt; }\n+  ReferenceType super_reference_type() const;\n+  bool is_instance_ref_klass() const;\n+  bool is_java_lang_ref_Reference_subclass() const;\n+\n","filename":"src\/hotspot\/share\/classfile\/classFileParser.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -510,2 +510,2 @@\n-  const char* sys_class_path = Arguments::get_sysclasspath();\n-  assert(sys_class_path != NULL, \"System boot class path must not be NULL\");\n+  const char* bootcp = Arguments::get_boot_class_path();\n+  assert(bootcp != NULL, \"Boot class path must not be NULL\");\n@@ -513,2 +513,2 @@\n-    \/\/ Don't print sys_class_path - this is the bootcp of this current VM process, not necessarily\n-    \/\/ the same as the bootcp of the shared archive.\n+    \/\/ Don't print bootcp - this is the bootcp of this current VM process, not necessarily\n+    \/\/ the same as the boot classpath of the shared archive.\n@@ -516,1 +516,1 @@\n-    trace_class_path(\"bootstrap loader class path=\", sys_class_path);\n+    trace_class_path(\"bootstrap loader class path=\", bootcp);\n@@ -518,1 +518,1 @@\n-  setup_bootstrap_search_path_impl(current, sys_class_path);\n+  setup_bootstrap_search_path_impl(current, bootcp);\n@@ -676,1 +676,1 @@\n-      \/\/ Every entry on the system boot class path after the initial base piece,\n+      \/\/ Every entry on the boot class path after the initial base piece,\n@@ -1180,1 +1180,1 @@\n-    \/\/ environemnt, including the runtime --patch-module setting.\n+    \/\/ environment, including the runtime --patch-module setting.\n@@ -1565,1 +1565,1 @@\n-  \/\/ class loader prior to loading j.l.Ojbect.\n+  \/\/ class loader prior to loading j.l.Object.\n@@ -1588,1 +1588,1 @@\n-\/\/ they could get inlined by agressive compiler, an unknown trick, see bug 6966589.\n+\/\/ they could get inlined by aggressive compiler, an unknown trick, see bug 6966589.\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -1012,3 +1012,7 @@\n-  out->print   (\" - klasses             {\");\n-  PrintKlassClosure closure(out);\n-  ((ClassLoaderData*)this)->classes_do(&closure);\n+  out->print   (\" - klasses             { \");\n+  if (Verbose) {\n+    PrintKlassClosure closure(out);\n+    ((ClassLoaderData*)this)->classes_do(&closure);\n+  } else {\n+     out->print(\"...\");\n+  }\n@@ -1019,1 +1023,6 @@\n-  out->print_cr(\" - dictionary          \" INTPTR_FORMAT, p2i(_dictionary));\n+  if (_dictionary != nullptr) {\n+    out->print   (\" - dictionary          \" INTPTR_FORMAT \" \", p2i(_dictionary));\n+    _dictionary->print_size(out);\n+  } else {\n+    out->print_cr(\" - dictionary          \" INTPTR_FORMAT, p2i(_dictionary));\n+  }\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.cpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -101,1 +101,2 @@\n-  friend class ClassLoaderDataGraphIterator;\n+  template <bool keep_alive>\n+  friend class ClassLoaderDataGraphIteratorBase;\n@@ -122,1 +123,1 @@\n-                                 \/\/ to these class loader datas.\n+                                 \/\/ to these class loader data.\n@@ -257,0 +258,1 @@\n+  inline oop class_loader_no_keepalive() const;\n@@ -315,1 +317,1 @@\n-  \/\/ Returns the class loader's explict name as specified during\n+  \/\/ Returns the class loader's explicit name as specified during\n","filename":"src\/hotspot\/share\/classfile\/classLoaderData.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"classfile\/javaClassesImpl.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"oops\/oopCast.inline.hpp\"\n@@ -67,0 +69,2 @@\n+#include \"runtime\/continuationEntry.inline.hpp\"\n+#include \"runtime\/continuationJavaClasses.inline.hpp\"\n@@ -70,0 +74,1 @@\n+#include \"runtime\/handshake.hpp\"\n@@ -74,0 +79,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -78,1 +84,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threadSMR.hpp\"\n@@ -82,0 +88,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -88,8 +95,0 @@\n-#define INJECTED_FIELD_COMPUTE_OFFSET(klass, name, signature, may_be_java)    \\\n-  klass::_##name##_offset = JavaClasses::compute_injected_offset(JavaClasses::klass##_##name##_enum);\n-\n-#if INCLUDE_CDS\n-#define INJECTED_FIELD_SERIALIZE_OFFSET(klass, name, signature, may_be_java) \\\n-  f->do_u4((u4*)&_##name##_offset);\n-#endif\n-\n@@ -119,1 +118,1 @@\n-  return _injected_fields[id].compute_offset();\n+  return _injected_fields[(int)id].compute_offset();\n@@ -137,1 +136,3 @@\n-    if (start == -1) start = klass##_##name##_enum;                \\\n+    if (start == -1) {                                             \\\n+      start = (int)InjectedFieldID::klass##_##name##_enum;         \\\n+    }                                                              \\\n@@ -153,3 +154,3 @@\n-static void compute_offset(int &dest_offset,\n-                           InstanceKlass* ik, Symbol* name_symbol, Symbol* signature_symbol,\n-                           bool is_static = false) {\n+void JavaClasses::compute_offset(int &dest_offset,\n+                                 InstanceKlass* ik, Symbol* name_symbol, Symbol* signature_symbol,\n+                                 bool is_static) {\n@@ -179,3 +180,3 @@\n-static void compute_offset(int& dest_offset, InstanceKlass* ik,\n-                           const char* name_string, Symbol* signature_symbol,\n-                           bool is_static = false) {\n+void JavaClasses::compute_offset(int& dest_offset, InstanceKlass* ik,\n+                                 const char* name_string, Symbol* signature_symbol,\n+                                 bool is_static) {\n@@ -191,10 +192,0 @@\n-\n-#if INCLUDE_CDS\n-#define FIELD_SERIALIZE_OFFSET(offset, klass, name, signature, is_static) \\\n-  f->do_u4((u4*)&offset)\n-#endif\n-\n-#define FIELD_COMPUTE_OFFSET(offset, klass, name, signature, is_static) \\\n-  compute_offset(offset, klass, name, vmSymbols::signature(), is_static)\n-\n-\n@@ -459,0 +450,1 @@\n+  jstring js;\n@@ -460,2 +452,1 @@\n-    jstring js = (jstring) JNIHandles::make_local(thread, java_string());\n-    bool is_copy;\n+    js = (jstring) JNIHandles::make_local(thread, java_string());\n@@ -465,0 +456,1 @@\n+    bool is_copy;\n@@ -467,1 +459,4 @@\n-    JNIHandles::destroy_local(js);\n+\n+  \/\/ Uses a store barrier and therefore needs to be in vm state\n+  JNIHandles::destroy_local(js);\n+\n@@ -790,1 +785,0 @@\n-int java_lang_Class::_init_lock_offset;\n@@ -924,6 +918,0 @@\n-  \/\/ Allocate a simple java object for a lock.\n-  \/\/ This needs to be a java object because during class initialization\n-  \/\/ it can be held across a java call.\n-  typeArrayOop r = oopFactory::new_typeArray(T_INT, 0, CHECK);\n-  set_init_lock(mirror(), r);\n-\n@@ -1098,2 +1086,0 @@\n-  \/\/ ## do we need to set init lock?\n-  java_lang_Class::set_init_lock(secondary_mirror(), init_lock(mirror()));\n@@ -1312,2 +1298,0 @@\n-    set_init_lock(archived_mirror, NULL);\n-\n@@ -1395,4 +1379,0 @@\n-    \/\/ create the init_lock\n-    typeArrayOop r = oopFactory::new_typeArray(T_INT, 0, CHECK_(false));\n-    set_init_lock(mirror(), r);\n-\n@@ -1483,9 +1463,0 @@\n-oop java_lang_Class::init_lock(oop java_class) {\n-  assert(_init_lock_offset != 0, \"must be set\");\n-  return java_class->obj_field(_init_lock_offset);\n-}\n-void java_lang_Class::set_init_lock(oop java_class, oop init_lock) {\n-  assert(_init_lock_offset != 0, \"must be set\");\n-  java_class->obj_field_put(_init_lock_offset, init_lock);\n-}\n-\n@@ -1714,5 +1685,0 @@\n-  \/\/ Init lock is a C union with component_mirror.  Only instanceKlass mirrors have\n-  \/\/ init_lock and only ArrayKlass mirrors have component_mirror.  Since both are oops\n-  \/\/ GC treats them the same.\n-  _init_lock_offset = _component_mirror_offset;\n-\n@@ -1725,1 +1691,0 @@\n-  f->do_u4((u4*)&_init_lock_offset);\n@@ -1752,0 +1717,102 @@\n+int java_lang_Thread_FieldHolder::_group_offset;\n+int java_lang_Thread_FieldHolder::_priority_offset;\n+int java_lang_Thread_FieldHolder::_stackSize_offset;\n+int java_lang_Thread_FieldHolder::_stillborn_offset;\n+int java_lang_Thread_FieldHolder::_daemon_offset;\n+int java_lang_Thread_FieldHolder::_thread_status_offset;\n+\n+#define THREAD_FIELD_HOLDER_FIELDS_DO(macro) \\\n+  macro(_group_offset,         k, vmSymbols::group_name(),    threadgroup_signature, false); \\\n+  macro(_priority_offset,      k, vmSymbols::priority_name(), int_signature,         false); \\\n+  macro(_stackSize_offset,     k, \"stackSize\",                long_signature,        false); \\\n+  macro(_stillborn_offset,     k, \"stillborn\",                bool_signature,        false); \\\n+  macro(_daemon_offset,        k, vmSymbols::daemon_name(),   bool_signature,        false); \\\n+  macro(_thread_status_offset, k, \"threadStatus\",             int_signature,         false)\n+\n+void java_lang_Thread_FieldHolder::compute_offsets() {\n+  assert(_group_offset == 0, \"offsets should be initialized only once\");\n+\n+  InstanceKlass* k = vmClasses::Thread_FieldHolder_klass();\n+  THREAD_FIELD_HOLDER_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void java_lang_Thread_FieldHolder::serialize_offsets(SerializeClosure* f) {\n+  THREAD_FIELD_HOLDER_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+oop java_lang_Thread_FieldHolder::threadGroup(oop holder) {\n+  return holder->obj_field(_group_offset);\n+}\n+\n+ThreadPriority java_lang_Thread_FieldHolder::priority(oop holder) {\n+  return (ThreadPriority)holder->int_field(_priority_offset);\n+}\n+\n+void java_lang_Thread_FieldHolder::set_priority(oop holder, ThreadPriority priority) {\n+  holder->int_field_put(_priority_offset, priority);\n+}\n+\n+jlong java_lang_Thread_FieldHolder::stackSize(oop holder) {\n+  return holder->long_field(_stackSize_offset);\n+}\n+\n+bool java_lang_Thread_FieldHolder::is_stillborn(oop holder) {\n+  return holder->bool_field(_stillborn_offset) != 0;\n+}\n+\n+void java_lang_Thread_FieldHolder::set_stillborn(oop holder) {\n+  holder->bool_field_put(_stillborn_offset, true);\n+}\n+\n+bool java_lang_Thread_FieldHolder::is_daemon(oop holder) {\n+  return holder->bool_field(_daemon_offset) != 0;\n+}\n+\n+void java_lang_Thread_FieldHolder::set_daemon(oop holder) {\n+  holder->bool_field_put(_daemon_offset, true);\n+}\n+\n+void java_lang_Thread_FieldHolder::set_thread_status(oop holder, JavaThreadStatus status) {\n+  holder->int_field_put(_thread_status_offset, static_cast<int>(status));\n+}\n+\n+JavaThreadStatus java_lang_Thread_FieldHolder::get_thread_status(oop holder) {\n+  return static_cast<JavaThreadStatus>(holder->int_field(_thread_status_offset));\n+}\n+\n+\n+int java_lang_Thread_Constants::_static_VTHREAD_GROUP_offset = 0;\n+int java_lang_Thread_Constants::_static_NOT_SUPPORTED_CLASSLOADER_offset = 0;\n+\n+#define THREAD_CONSTANTS_STATIC_FIELDS_DO(macro) \\\n+  macro(_static_VTHREAD_GROUP_offset,             k, \"VTHREAD_GROUP\",             threadgroup_signature, true); \\\n+  macro(_static_NOT_SUPPORTED_CLASSLOADER_offset, k, \"NOT_SUPPORTED_CLASSLOADER\", classloader_signature, true);\n+\n+void java_lang_Thread_Constants::compute_offsets() {\n+  assert(_static_VTHREAD_GROUP_offset == 0, \"offsets should be initialized only once\");\n+\n+  InstanceKlass* k = vmClasses::Thread_Constants_klass();\n+  THREAD_CONSTANTS_STATIC_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void java_lang_Thread_Constants::serialize_offsets(SerializeClosure* f) {\n+  THREAD_CONSTANTS_STATIC_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+oop java_lang_Thread_Constants::get_VTHREAD_GROUP() {\n+  InstanceKlass* k = vmClasses::Thread_Constants_klass();\n+  oop base = k->static_field_base_raw();\n+  return base->obj_field(_static_VTHREAD_GROUP_offset);\n+}\n+\n+oop java_lang_Thread_Constants::get_NOT_SUPPORTED_CLASSLOADER() {\n+  InstanceKlass* k = vmClasses::Thread_Constants_klass();\n+  oop base = k->static_field_base_raw();\n+  return base->obj_field(_static_NOT_SUPPORTED_CLASSLOADER_offset);\n+}\n+\n+int java_lang_Thread::_holder_offset;\n@@ -1753,1 +1820,0 @@\n-int java_lang_Thread::_group_offset;\n@@ -1756,1 +1822,1 @@\n-int java_lang_Thread::_priority_offset;\n+int java_lang_Thread::_jvmti_thread_state_offset;\n@@ -1759,4 +1825,1 @@\n-int java_lang_Thread::_daemon_offset;\n-int java_lang_Thread::_stillborn_offset;\n-int java_lang_Thread::_stackSize_offset;\n-int java_lang_Thread::_thread_status_offset;\n+int java_lang_Thread::_continuation_offset;\n@@ -1765,0 +1828,2 @@\n+int java_lang_Thread::_extentLocalBindings_offset;\n+JFR_ONLY(int java_lang_Thread::_jfr_epoch_offset;)\n@@ -1767,0 +1832,1 @@\n+  macro(_holder_offset,        k, \"holder\", thread_fieldholder_signature, false); \\\n@@ -1768,1 +1834,0 @@\n-  macro(_group_offset,         k, vmSymbols::group_name(), threadgroup_signature, false); \\\n@@ -1771,2 +1836,0 @@\n-  macro(_priority_offset,      k, vmSymbols::priority_name(), int_signature, false); \\\n-  macro(_daemon_offset,        k, vmSymbols::daemon_name(), bool_signature, false); \\\n@@ -1775,4 +1838,3 @@\n-  macro(_stillborn_offset,     k, \"stillborn\", bool_signature, false); \\\n-  macro(_stackSize_offset,     k, \"stackSize\", long_signature, false); \\\n-  macro(_thread_status_offset, k, \"threadStatus\", int_signature, false); \\\n-  macro(_park_blocker_offset,  k, \"parkBlocker\", object_signature, false)\n+  macro(_park_blocker_offset,  k, \"parkBlocker\", object_signature, false); \\\n+  macro(_continuation_offset,  k, \"cont\", continuation_signature, false); \\\n+  macro(_extentLocalBindings_offset, k, \"extentLocalBindings\", object_signature, false);\n@@ -1782,1 +1844,1 @@\n-  assert(_group_offset == 0, \"offsets should be initialized only once\");\n+  assert(_holder_offset == 0, \"offsets should be initialized only once\");\n@@ -1786,0 +1848,1 @@\n+  THREAD_INJECTED_FIELDS(INJECTED_FIELD_COMPUTE_OFFSET);\n@@ -1791,0 +1854,1 @@\n+  THREAD_INJECTED_FIELDS(INJECTED_FIELD_SERIALIZE_OFFSET);\n@@ -1798,1 +1862,0 @@\n-\n@@ -1803,0 +1866,16 @@\n+JvmtiThreadState* java_lang_Thread::jvmti_thread_state(oop java_thread) {\n+  return (JvmtiThreadState*)java_thread->address_field(_jvmti_thread_state_offset);\n+}\n+\n+void java_lang_Thread::set_jvmti_thread_state(oop java_thread, JvmtiThreadState* state) {\n+  java_thread->address_field_put(_jvmti_thread_state_offset, (address)state);\n+}\n+\n+void java_lang_Thread::clear_extentLocalBindings(oop java_thread) {\n+  java_thread->obj_field_put(_extentLocalBindings_offset, NULL);\n+}\n+\n+oop java_lang_Thread::holder(oop java_thread) {\n+    return java_thread->obj_field(_holder_offset);\n+}\n+\n@@ -1833,1 +1912,3 @@\n-  return (ThreadPriority)java_thread->int_field(_priority_offset);\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  return java_lang_Thread_FieldHolder::priority(holder);\n@@ -1838,1 +1919,3 @@\n-  java_thread->int_field_put(_priority_offset, priority);\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  java_lang_Thread_FieldHolder::set_priority(holder, priority);\n@@ -1843,1 +1926,3 @@\n-  return java_thread->obj_field(_group_offset);\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  return java_lang_Thread_FieldHolder::threadGroup(holder);\n@@ -1848,1 +1933,3 @@\n-  return java_thread->bool_field(_stillborn_offset) != 0;\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  return java_lang_Thread_FieldHolder::is_stillborn(holder);\n@@ -1854,1 +1941,3 @@\n-  java_thread->bool_field_put(_stillborn_offset, true);\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  java_lang_Thread_FieldHolder::set_stillborn(holder);\n@@ -1865,1 +1954,3 @@\n-  return java_thread->bool_field(_daemon_offset) != 0;\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  return java_lang_Thread_FieldHolder::is_daemon(holder);\n@@ -1870,1 +1961,3 @@\n-  java_thread->bool_field_put(_daemon_offset, true);\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  java_lang_Thread_FieldHolder::set_daemon(holder);\n@@ -1883,1 +1976,3 @@\n-  return java_thread->long_field(_stackSize_offset);\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  return java_lang_Thread_FieldHolder::stackSize(holder);\n@@ -1887,3 +1982,4 @@\n-void java_lang_Thread::set_thread_status(oop java_thread,\n-                                         JavaThreadStatus status) {\n-  java_thread->int_field_put(_thread_status_offset, static_cast<int>(status));\n+void java_lang_Thread::set_thread_status(oop java_thread, JavaThreadStatus status) {\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  java_lang_Thread_FieldHolder::set_thread_status(holder, status);\n@@ -1899,1 +1995,6 @@\n-  return static_cast<JavaThreadStatus>(java_thread->int_field(_thread_status_offset));\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  if (holder == NULL) {\n+    return JavaThreadStatus::NEW;  \/\/ Java Thread not initialized\n+  } else {\n+    return java_lang_Thread_FieldHolder::get_thread_status(holder);\n+  }\n@@ -1902,3 +2003,2 @@\n-\n-jlong java_lang_Thread::thread_id(oop java_thread) {\n-  return java_thread->long_field(_tid_offset);\n+ByteSize java_lang_Thread::thread_id_offset() {\n+  return in_ByteSize(_tid_offset);\n@@ -1911,0 +2011,118 @@\n+oop java_lang_Thread::async_get_stack_trace(oop java_thread, TRAPS) {\n+  ThreadsListHandle tlh(JavaThread::current());\n+  JavaThread* thread;\n+  bool is_virtual = java_lang_VirtualThread::is_instance(java_thread);\n+  if (is_virtual) {\n+    oop carrier_thread = java_lang_VirtualThread::carrier_thread(java_thread);\n+    if (carrier_thread == NULL) {\n+      return NULL;\n+    }\n+    thread = java_lang_Thread::thread(carrier_thread);\n+  } else {\n+    thread = java_lang_Thread::thread(java_thread);\n+  }\n+  if (thread == NULL) {\n+    return NULL;\n+  }\n+\n+  class GetStackTraceClosure : public HandshakeClosure {\n+  public:\n+    const Handle _java_thread;\n+    int _depth;\n+    bool _retry_handshake;\n+    GrowableArray<Method*>* _methods;\n+    GrowableArray<int>*     _bcis;\n+\n+    GetStackTraceClosure(Handle java_thread) :\n+        HandshakeClosure(\"GetStackTraceClosure\"), _java_thread(java_thread), _depth(0), _retry_handshake(false) {\n+      \/\/ Pick some initial length\n+      int init_length = MaxJavaStackTraceDepth \/ 2;\n+      _methods = new GrowableArray<Method*>(init_length);\n+      _bcis = new GrowableArray<int>(init_length);\n+    }\n+\n+    bool read_reset_retry() {\n+      bool ret = _retry_handshake;\n+      \/\/ If we re-execute the handshake this method need to return false\n+      \/\/ when the handshake cannot be performed. (E.g. thread terminating)\n+      _retry_handshake = false;\n+      return ret;\n+    }\n+\n+    void do_thread(Thread* th) {\n+      if (!Thread::current()->is_Java_thread()) {\n+        _retry_handshake = true;\n+        return;\n+      }\n+\n+      JavaThread* thread = JavaThread::cast(th);\n+\n+      if (!thread->has_last_Java_frame()) {\n+        return;\n+      }\n+\n+      bool carrier = false;\n+      if (java_lang_VirtualThread::is_instance(_java_thread())) {\n+        \/\/ if (thread->vthread() != _java_thread()) \/\/ We might be inside a System.executeOnCarrierThread\n+        const ContinuationEntry* ce = thread->vthread_continuation();\n+        if (ce == nullptr || ce->cont_oop() != java_lang_VirtualThread::continuation(_java_thread())) {\n+          return; \/\/ not mounted\n+        }\n+      } else {\n+        carrier = (thread->vthread_continuation() != NULL);\n+      }\n+\n+      const int max_depth = MaxJavaStackTraceDepth;\n+      const bool skip_hidden = !ShowHiddenFrames;\n+\n+      int total_count = 0;\n+      for (vframeStream vfst(thread, false, false, carrier); \/\/ we don't process frames as we don't care about oops\n+           !vfst.at_end() && (max_depth == 0 || max_depth != total_count);\n+           vfst.next()) {\n+\n+        if (skip_hidden && (vfst.method()->is_hidden() ||\n+                            vfst.method()->is_continuation_enter_intrinsic())) {\n+          continue;\n+        }\n+\n+        _methods->push(vfst.method());\n+        _bcis->push(vfst.bci());\n+        total_count++;\n+      }\n+\n+      _depth = total_count;\n+    }\n+  };\n+\n+  \/\/ Handshake with target\n+  ResourceMark rm(THREAD);\n+  HandleMark   hm(THREAD);\n+  GetStackTraceClosure gstc(Handle(THREAD, java_thread));\n+  do {\n+   Handshake::execute(&gstc, &tlh, thread);\n+  } while (gstc.read_reset_retry());\n+\n+  \/\/ Stop if no stack trace is found.\n+  if (gstc._depth == 0) {\n+    return NULL;\n+  }\n+\n+  \/\/ Convert to StackTraceElement array\n+  InstanceKlass* k = vmClasses::StackTraceElement_klass();\n+  assert(k != NULL, \"must be loaded in 1.4+\");\n+  if (k->should_be_initialized()) {\n+    k->initialize(CHECK_NULL);\n+  }\n+  objArrayHandle trace = oopFactory::new_objArray_handle(k, gstc._depth, CHECK_NULL);\n+\n+  for (int i = 0; i < gstc._depth; i++) {\n+    methodHandle method(THREAD, gstc._methods->at(i));\n+    oop element = java_lang_StackTraceElement::create(method,\n+                                                      gstc._bcis->at(i),\n+                                                      CHECK_NULL);\n+    trace->obj_at_put(i, element);\n+  }\n+\n+  return trace();\n+}\n+\n@@ -1912,1 +2130,3 @@\n-  JavaThreadStatus status = static_cast<JavaThreadStatus>(java_thread->int_field(_thread_status_offset));\n+  oop holder = java_lang_Thread::holder(java_thread);\n+  assert(holder != NULL, \"Java Thread not initialized\");\n+  JavaThreadStatus status = java_lang_Thread_FieldHolder::get_thread_status(holder);\n@@ -1928,4 +2148,3 @@\n-int java_lang_ThreadGroup::_threads_offset;\n-int java_lang_ThreadGroup::_groups_offset;\n-int java_lang_ThreadGroup::_destroyed_offset;\n-int java_lang_ThreadGroup::_nthreads_offset;\n+int java_lang_ThreadGroup::_groups_offset;\n+int java_lang_ThreadGroup::_nweaks_offset;\n+int java_lang_ThreadGroup::_weaks_offset;\n@@ -1952,1 +2171,1 @@\n-int java_lang_ThreadGroup::nthreads(oop java_thread_group) {\n+ThreadPriority java_lang_ThreadGroup::maxPriority(oop java_thread_group) {\n@@ -1954,1 +2173,1 @@\n-  return java_thread_group->int_field(_nthreads_offset);\n+  return (ThreadPriority) java_thread_group->int_field(_maxPriority_offset);\n@@ -1957,5 +2176,3 @@\n-objArrayOop java_lang_ThreadGroup::threads(oop java_thread_group) {\n-  oop threads = java_thread_group->obj_field(_threads_offset);\n-  assert(threads != NULL, \"threadgroups should have threads\");\n-  assert(threads->is_objArray(), \"just checking\"); \/\/ Todo: Add better type checking code\n-  return objArrayOop(threads);\n+bool java_lang_ThreadGroup::is_daemon(oop java_thread_group) {\n+  assert(oopDesc::is_oop(java_thread_group), \"thread group must be oop\");\n+  return java_thread_group->bool_field(_daemon_offset) != 0;\n@@ -1975,6 +2192,1 @@\n-ThreadPriority java_lang_ThreadGroup::maxPriority(oop java_thread_group) {\n-  assert(oopDesc::is_oop(java_thread_group), \"thread group must be oop\");\n-  return (ThreadPriority) java_thread_group->int_field(_maxPriority_offset);\n-}\n-\n-bool java_lang_ThreadGroup::is_destroyed(oop java_thread_group) {\n+int java_lang_ThreadGroup::nweaks(oop java_thread_group) {\n@@ -1982,1 +2194,1 @@\n-  return java_thread_group->bool_field(_destroyed_offset) != 0;\n+  return java_thread_group->int_field(_nweaks_offset);\n@@ -1985,3 +2197,4 @@\n-bool java_lang_ThreadGroup::is_daemon(oop java_thread_group) {\n-  assert(oopDesc::is_oop(java_thread_group), \"thread group must be oop\");\n-  return java_thread_group->bool_field(_daemon_offset) != 0;\n+objArrayOop java_lang_ThreadGroup::weaks(oop java_thread_group) {\n+  oop weaks = java_thread_group->obj_field(_weaks_offset);\n+  assert(weaks == NULL || weaks->is_objArray(), \"just checking\");\n+  return objArrayOop(weaks);\n@@ -1991,9 +2204,8 @@\n-  macro(_parent_offset,      k, vmSymbols::parent_name(),      threadgroup_signature,       false); \\\n-  macro(_name_offset,        k, vmSymbols::name_name(),        string_signature,            false); \\\n-  macro(_threads_offset,     k, vmSymbols::threads_name(),     thread_array_signature,      false); \\\n-  macro(_groups_offset,      k, vmSymbols::groups_name(),      threadgroup_array_signature, false); \\\n-  macro(_maxPriority_offset, k, vmSymbols::maxPriority_name(), int_signature,               false); \\\n-  macro(_destroyed_offset,   k, vmSymbols::destroyed_name(),   bool_signature,              false); \\\n-  macro(_daemon_offset,      k, vmSymbols::daemon_name(),      bool_signature,              false); \\\n-  macro(_nthreads_offset,    k, vmSymbols::nthreads_name(),    int_signature,               false); \\\n-  macro(_ngroups_offset,     k, vmSymbols::ngroups_name(),     int_signature,               false)\n+  macro(_parent_offset,      k, vmSymbols::parent_name(),      threadgroup_signature,         false); \\\n+  macro(_name_offset,        k, vmSymbols::name_name(),        string_signature,              false); \\\n+  macro(_maxPriority_offset, k, vmSymbols::maxPriority_name(), int_signature,                 false); \\\n+  macro(_daemon_offset,      k, vmSymbols::daemon_name(),      bool_signature,                false); \\\n+  macro(_ngroups_offset,     k, vmSymbols::ngroups_name(),     int_signature,                 false); \\\n+  macro(_groups_offset,      k, vmSymbols::groups_name(),      threadgroup_array_signature,   false); \\\n+  macro(_nweaks_offset,      k, vmSymbols::nweaks_name(),      int_signature,                 false); \\\n+  macro(_weaks_offset,       k, vmSymbols::weaks_name(),       weakreference_array_signature, false);\n@@ -2014,0 +2226,92 @@\n+\n+\/\/ java_lang_VirtualThread\n+\n+int java_lang_VirtualThread::static_notify_jvmti_events_offset;\n+int java_lang_VirtualThread::static_vthread_scope_offset;\n+int java_lang_VirtualThread::_carrierThread_offset;\n+int java_lang_VirtualThread::_continuation_offset;\n+int java_lang_VirtualThread::_state_offset;\n+\n+#define VTHREAD_FIELDS_DO(macro) \\\n+  macro(static_notify_jvmti_events_offset, k, \"notifyJvmtiEvents\",  bool_signature,              true);  \\\n+  macro(static_vthread_scope_offset,       k, \"VTHREAD_SCOPE\",      continuationscope_signature, true);  \\\n+  macro(_carrierThread_offset,             k, \"carrierThread\",      thread_signature,            false); \\\n+  macro(_continuation_offset,              k, \"cont\",               continuation_signature,      false); \\\n+  macro(_state_offset,                     k, \"state\",              int_signature,               false)\n+\n+static bool vthread_notify_jvmti_events = JNI_FALSE;\n+\n+void java_lang_VirtualThread::compute_offsets() {\n+  InstanceKlass* k = vmClasses::VirtualThread_klass();\n+  VTHREAD_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+void java_lang_VirtualThread::init_static_notify_jvmti_events() {\n+  if (vthread_notify_jvmti_events) {\n+    InstanceKlass* ik = vmClasses::VirtualThread_klass();\n+    oop base = ik->static_field_base_raw();\n+    base->release_bool_field_put(static_notify_jvmti_events_offset, vthread_notify_jvmti_events);\n+  }\n+}\n+\n+bool java_lang_VirtualThread::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n+}\n+\n+oop java_lang_VirtualThread::carrier_thread(oop vthread) {\n+  oop thread = vthread->obj_field(_carrierThread_offset);\n+  return thread;\n+}\n+\n+oop java_lang_VirtualThread::continuation(oop vthread) {\n+  oop cont = vthread->obj_field(_continuation_offset);\n+  return cont;\n+}\n+\n+int java_lang_VirtualThread::state(oop vthread) {\n+  return vthread->int_field_acquire(_state_offset);\n+}\n+\n+JavaThreadStatus java_lang_VirtualThread::map_state_to_thread_status(int state) {\n+  JavaThreadStatus status = JavaThreadStatus::NEW;\n+  switch (state) {\n+    case NEW :\n+      status = JavaThreadStatus::NEW;\n+      break;\n+    case STARTED :\n+    case RUNNABLE :\n+    case RUNNABLE_SUSPENDED :\n+    case RUNNING :\n+    case PARKING :\n+    case YIELDING :\n+      status = JavaThreadStatus::RUNNABLE;\n+      break;\n+    case PARKED :\n+    case PARKED_SUSPENDED :\n+    case PINNED :\n+      status = JavaThreadStatus::PARKED;\n+      break;\n+    case TERMINATED :\n+      status = JavaThreadStatus::TERMINATED;\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+  return status;\n+}\n+\n+#if INCLUDE_CDS\n+void java_lang_VirtualThread::serialize_offsets(SerializeClosure* f) {\n+   VTHREAD_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+bool java_lang_VirtualThread::notify_jvmti_events() {\n+  return vthread_notify_jvmti_events == JNI_TRUE;\n+}\n+\n+void java_lang_VirtualThread::set_notify_jvmti_events(bool enable) {\n+  vthread_notify_jvmti_events = enable;\n+}\n+\n+\n@@ -2140,0 +2444,1 @@\n+    trace_conts_offset   = java_lang_Throwable::trace_conts_offset,\n@@ -2262,0 +2567,1 @@\n+\n@@ -2520,1 +2826,4 @@\n-  RegisterMap map(thread, false \/* update *\/, false \/* process_frames *\/);\n+  RegisterMap map(thread,\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::skip,\n+                  RegisterMap::WalkContinuation::include);\n@@ -2526,1 +2835,2 @@\n-\n+  bool show_carrier = ShowCarrierFrames;\n+  ContinuationEntry* cont_entry = thread->last_continuation();\n@@ -2539,0 +2849,9 @@\n+\n+      if (Continuation::is_continuation_enterSpecial(fr)) {\n+        assert(cont_entry == Continuation::get_continuation_entry_for_entry_frame(thread, fr), \"\");\n+        if (!show_carrier && cont_entry->is_virtual_thread()) {\n+          break;\n+        }\n+        cont_entry = cont_entry->parent();\n+      }\n+\n@@ -2541,2 +2860,8 @@\n-        address bcp = fr.interpreter_frame_bcp();\n-        method = fr.interpreter_frame_method();\n+        address bcp;\n+        if (!map.in_cont()) {\n+          bcp = fr.interpreter_frame_bcp();\n+          method = fr.interpreter_frame_method();\n+        } else {\n+          bcp = map.stack_chunk()->interpreter_frame_bcp(fr);\n+          method = map.stack_chunk()->interpreter_frame_method(fr);\n+        }\n@@ -2554,0 +2879,1 @@\n+        assert(nm->method() != NULL, \"must be\");\n@@ -2567,3 +2893,4 @@\n-    assert(st.method() == method && st.bci() == bci,\n-           \"Wrong stack trace\");\n-    st.next();\n+    if (!st.at_end()) { \/\/ TODO LOOM remove once we show only vthread trace\n+      assert(st.method() == method && st.bci() == bci, \"Wrong stack trace\");\n+      st.next();\n+    }\n@@ -2590,1 +2917,1 @@\n-      \/\/ This is similar to classic VM (before HotSpot).\n+      \/\/ This is similar to classic VM.\n@@ -2599,1 +2926,1 @@\n-    if (method->is_hidden()) {\n+    if (method->is_hidden() || method->is_continuation_enter_intrinsic()) {\n@@ -2608,0 +2935,1 @@\n+\n@@ -2687,1 +3015,1 @@\n-void java_lang_Throwable::get_stack_trace_elements(Handle throwable,\n+void java_lang_Throwable::get_stack_trace_elements(int depth, Handle backtrace,\n@@ -2690,1 +3018,1 @@\n-  if (throwable.is_null() || stack_trace_array_h.is_null()) {\n+  if (backtrace.is_null() || stack_trace_array_h.is_null()) {\n@@ -2696,1 +3024,1 @@\n-  if (stack_trace_array_h->length() != depth(throwable())) {\n+  if (stack_trace_array_h->length() != depth) {\n@@ -2700,1 +3028,1 @@\n-  objArrayHandle result(THREAD, objArrayOop(backtrace(throwable())));\n+  objArrayHandle result(THREAD, objArrayOop(backtrace()));\n@@ -2720,1 +3048,2 @@\n-                                         bte._name, CHECK);\n+                                         bte._name,\n+                                         CHECK);\n@@ -2919,0 +3248,1 @@\n+int java_lang_StackFrameInfo::_contScope_offset;\n@@ -2921,2 +3251,3 @@\n-  macro(_memberName_offset,     k, \"memberName\",  object_signature, false); \\\n-  macro(_bci_offset,            k, \"bci\",         int_signature,    false)\n+  macro(_memberName_offset, k, \"memberName\", object_signature,            false); \\\n+  macro(_bci_offset,        k, \"bci\",        int_signature,               false); \\\n+  macro(_contScope_offset,  k, \"contScope\",  continuationscope_signature, false)\n@@ -2946,1 +3277,1 @@\n-void java_lang_StackFrameInfo::set_method_and_bci(Handle stackFrame, const methodHandle& method, int bci, TRAPS) {\n+void java_lang_StackFrameInfo::set_method_and_bci(Handle stackFrame, const methodHandle& method, int bci, oop cont, TRAPS) {\n@@ -2950,0 +3281,1 @@\n+  Handle cont_h (THREAD, cont);\n@@ -2959,0 +3291,3 @@\n+\n+  oop contScope = cont_h() != NULL ? jdk_internal_vm_Continuation::scope(cont_h()) : (oop)NULL;\n+  java_lang_StackFrameInfo::set_contScope(stackFrame(), contScope);\n@@ -2972,2 +3307,1 @@\n-  java_lang_StackTraceElement::fill_in(stack_trace_element, holder, methodHandle(THREAD, method),\n-                                       version, bci, name, CHECK);\n+  java_lang_StackTraceElement::fill_in(stack_trace_element, holder, methodHandle(THREAD, method), version, bci, name, CHECK);\n@@ -2985,0 +3319,4 @@\n+void java_lang_StackFrameInfo::set_contScope(oop element, oop value) {\n+  element->obj_field_put(_contScope_offset, value);\n+}\n+\n@@ -3091,1 +3429,1 @@\n-  \/\/ This class is eagerly initialized during VM initialization, since we keep a refence\n+  \/\/ This class is eagerly initialized during VM initialization, since we keep a reference\n@@ -3940,6 +4278,2 @@\n-int jdk_internal_invoke_NativeEntryPoint::_shadow_space_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_argMoves_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_returnMoves_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_need_transition_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_method_type_offset;\n-int jdk_internal_invoke_NativeEntryPoint::_name_offset;\n+int jdk_internal_foreign_abi_NativeEntryPoint::_method_type_offset;\n+int jdk_internal_foreign_abi_NativeEntryPoint::_downcall_stub_address_offset;\n@@ -3948,8 +4282,4 @@\n-  macro(_shadow_space_offset,    k, \"shadowSpace\",    int_signature, false); \\\n-  macro(_argMoves_offset,        k, \"argMoves\",       long_array_signature, false); \\\n-  macro(_returnMoves_offset,     k, \"returnMoves\",    long_array_signature, false); \\\n-  macro(_need_transition_offset, k, \"needTransition\", bool_signature, false); \\\n-  macro(_method_type_offset,     k, \"methodType\",     java_lang_invoke_MethodType_signature, false); \\\n-  macro(_name_offset,            k, \"name\",           string_signature, false);\n-\n-bool jdk_internal_invoke_NativeEntryPoint::is_instance(oop obj) {\n+  macro(_method_type_offset,           k, \"methodType\",          java_lang_invoke_MethodType_signature, false); \\\n+  macro(_downcall_stub_address_offset, k, \"downcallStubAddress\", long_signature, false);\n+\n+bool jdk_internal_foreign_abi_NativeEntryPoint::is_instance(oop obj) {\n@@ -3959,1 +4289,1 @@\n-void jdk_internal_invoke_NativeEntryPoint::compute_offsets() {\n+void jdk_internal_foreign_abi_NativeEntryPoint::compute_offsets() {\n@@ -3965,1 +4295,1 @@\n-void jdk_internal_invoke_NativeEntryPoint::serialize_offsets(SerializeClosure* f) {\n+void jdk_internal_foreign_abi_NativeEntryPoint::serialize_offsets(SerializeClosure* f) {\n@@ -3970,2 +4300,2 @@\n-jint jdk_internal_invoke_NativeEntryPoint::shadow_space(oop entry) {\n-  return entry->int_field(_shadow_space_offset);\n+oop jdk_internal_foreign_abi_NativeEntryPoint::method_type(oop entry) {\n+  return entry->obj_field(_method_type_offset);\n@@ -3974,2 +4304,2 @@\n-oop jdk_internal_invoke_NativeEntryPoint::argMoves(oop entry) {\n-  return entry->obj_field(_argMoves_offset);\n+jlong jdk_internal_foreign_abi_NativeEntryPoint::downcall_stub_address(oop entry) {\n+  return entry->long_field(_downcall_stub_address_offset);\n@@ -3978,2 +4308,19 @@\n-oop jdk_internal_invoke_NativeEntryPoint::returnMoves(oop entry) {\n-  return entry->obj_field(_returnMoves_offset);\n+int jdk_internal_foreign_abi_ABIDescriptor::_inputStorage_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_outputStorage_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_volatileStorage_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_stackAlignment_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_shadowSpace_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_targetAddrStorage_offset;\n+int jdk_internal_foreign_abi_ABIDescriptor::_retBufAddrStorage_offset;\n+\n+#define ABIDescriptor_FIELDS_DO(macro) \\\n+  macro(_inputStorage_offset,      k, \"inputStorage\",      jdk_internal_foreign_abi_VMStorage_array_array_signature, false); \\\n+  macro(_outputStorage_offset,     k, \"outputStorage\",     jdk_internal_foreign_abi_VMStorage_array_array_signature, false); \\\n+  macro(_volatileStorage_offset,   k, \"volatileStorage\",   jdk_internal_foreign_abi_VMStorage_array_array_signature, false); \\\n+  macro(_stackAlignment_offset,    k, \"stackAlignment\",    int_signature, false); \\\n+  macro(_shadowSpace_offset,       k, \"shadowSpace\",       int_signature, false); \\\n+  macro(_targetAddrStorage_offset, k, \"targetAddrStorage\", jdk_internal_foreign_abi_VMStorage_signature, false); \\\n+  macro(_retBufAddrStorage_offset, k, \"retBufAddrStorage\", jdk_internal_foreign_abi_VMStorage_signature, false);\n+\n+bool jdk_internal_foreign_abi_ABIDescriptor::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n@@ -3982,2 +4329,3 @@\n-jboolean jdk_internal_invoke_NativeEntryPoint::need_transition(oop entry) {\n-  return entry->bool_field(_need_transition_offset);\n+void jdk_internal_foreign_abi_ABIDescriptor::compute_offsets() {\n+  InstanceKlass* k = vmClasses::ABIDescriptor_klass();\n+  ABIDescriptor_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n@@ -3986,2 +4334,79 @@\n-oop jdk_internal_invoke_NativeEntryPoint::method_type(oop entry) {\n-  return entry->obj_field(_method_type_offset);\n+#if INCLUDE_CDS\n+void jdk_internal_foreign_abi_ABIDescriptor::serialize_offsets(SerializeClosure* f) {\n+  ABIDescriptor_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+objArrayOop jdk_internal_foreign_abi_ABIDescriptor::inputStorage(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_inputStorage_offset));\n+}\n+\n+objArrayOop jdk_internal_foreign_abi_ABIDescriptor::outputStorage(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_outputStorage_offset));\n+}\n+\n+objArrayOop jdk_internal_foreign_abi_ABIDescriptor::volatileStorage(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_volatileStorage_offset));\n+}\n+\n+jint jdk_internal_foreign_abi_ABIDescriptor::stackAlignment(oop entry) {\n+  return entry->int_field(_stackAlignment_offset);\n+}\n+\n+jint jdk_internal_foreign_abi_ABIDescriptor::shadowSpace(oop entry) {\n+  return entry->int_field(_shadowSpace_offset);\n+}\n+\n+oop jdk_internal_foreign_abi_ABIDescriptor::targetAddrStorage(oop entry) {\n+  return entry->obj_field(_targetAddrStorage_offset);\n+}\n+\n+oop jdk_internal_foreign_abi_ABIDescriptor::retBufAddrStorage(oop entry) {\n+  return entry->obj_field(_retBufAddrStorage_offset);\n+}\n+\n+int jdk_internal_foreign_abi_VMStorage::_type_offset;\n+int jdk_internal_foreign_abi_VMStorage::_index_offset;\n+int jdk_internal_foreign_abi_VMStorage::_debugName_offset;\n+\n+#define VMStorage_FIELDS_DO(macro) \\\n+  macro(_type_offset,      k, \"type\",      int_signature, false); \\\n+  macro(_index_offset,     k, \"index\",     int_signature, false); \\\n+  macro(_debugName_offset, k, \"debugName\", string_signature, false); \\\n+\n+bool jdk_internal_foreign_abi_VMStorage::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n+}\n+\n+void jdk_internal_foreign_abi_VMStorage::compute_offsets() {\n+  InstanceKlass* k = vmClasses::VMStorage_klass();\n+  VMStorage_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void jdk_internal_foreign_abi_VMStorage::serialize_offsets(SerializeClosure* f) {\n+  VMStorage_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+jint jdk_internal_foreign_abi_VMStorage::type(oop entry) {\n+  return entry->int_field(_type_offset);\n+}\n+\n+jint jdk_internal_foreign_abi_VMStorage::index(oop entry) {\n+  return entry->int_field(_index_offset);\n+}\n+\n+oop jdk_internal_foreign_abi_VMStorage::debugName(oop entry) {\n+  return entry->obj_field(_debugName_offset);\n+}\n+\n+int jdk_internal_foreign_abi_CallConv::_argRegs_offset;\n+int jdk_internal_foreign_abi_CallConv::_retRegs_offset;\n+\n+#define CallConv_FIELDS_DO(macro) \\\n+  macro(_argRegs_offset, k, \"argRegs\", jdk_internal_foreign_abi_VMStorage_array_signature, false); \\\n+  macro(_retRegs_offset, k, \"retRegs\", jdk_internal_foreign_abi_VMStorage_array_signature, false); \\\n+\n+bool jdk_internal_foreign_abi_CallConv::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n@@ -3990,2 +4415,17 @@\n-oop jdk_internal_invoke_NativeEntryPoint::name(oop entry) {\n-  return entry->obj_field(_name_offset);\n+void jdk_internal_foreign_abi_CallConv::compute_offsets() {\n+  InstanceKlass* k = vmClasses::CallConv_klass();\n+  CallConv_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void jdk_internal_foreign_abi_CallConv::serialize_offsets(SerializeClosure* f) {\n+  CallConv_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+objArrayOop jdk_internal_foreign_abi_CallConv::argRegs(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_argRegs_offset));\n+}\n+\n+objArrayOop jdk_internal_foreign_abi_CallConv::retRegs(oop entry) {\n+  return oop_cast<objArrayOop>(entry->obj_field(_retRegs_offset));\n@@ -5028,0 +5468,57 @@\n+#define BASIC_JAVA_CLASSES_DO_PART1(f) \\\n+  f(java_lang_Class) \\\n+  f(java_lang_String) \\\n+  f(java_lang_ref_Reference) \\\n+  \/\/end\n+\n+#define BASIC_JAVA_CLASSES_DO_PART2(f) \\\n+  f(java_lang_System) \\\n+  f(java_lang_ClassLoader) \\\n+  f(java_lang_Throwable) \\\n+  f(java_lang_Thread) \\\n+  f(java_lang_Thread_FieldHolder) \\\n+  f(java_lang_Thread_Constants) \\\n+  f(java_lang_ThreadGroup) \\\n+  f(java_lang_VirtualThread) \\\n+  f(java_lang_InternalError) \\\n+  f(java_lang_AssertionStatusDirectives) \\\n+  f(java_lang_ref_SoftReference) \\\n+  f(java_lang_invoke_MethodHandle) \\\n+  f(java_lang_invoke_DirectMethodHandle) \\\n+  f(java_lang_invoke_MemberName) \\\n+  f(java_lang_invoke_ResolvedMethodName) \\\n+  f(java_lang_invoke_LambdaForm) \\\n+  f(java_lang_invoke_MethodType) \\\n+  f(java_lang_invoke_CallSite) \\\n+  f(java_lang_invoke_ConstantCallSite) \\\n+  f(java_lang_invoke_MethodHandleNatives_CallSiteContext) \\\n+  f(java_security_AccessControlContext) \\\n+  f(java_lang_reflect_AccessibleObject) \\\n+  f(java_lang_reflect_Method) \\\n+  f(java_lang_reflect_Constructor) \\\n+  f(java_lang_reflect_Field) \\\n+  f(java_lang_reflect_RecordComponent) \\\n+  f(reflect_ConstantPool) \\\n+  f(reflect_UnsafeStaticFieldAccessorImpl) \\\n+  f(java_lang_reflect_Parameter) \\\n+  f(java_lang_Module) \\\n+  f(java_lang_StackTraceElement) \\\n+  f(java_lang_StackFrameInfo) \\\n+  f(java_lang_LiveStackFrameInfo) \\\n+  f(jdk_internal_vm_ContinuationScope) \\\n+  f(jdk_internal_vm_Continuation) \\\n+  f(jdk_internal_vm_StackChunk) \\\n+  f(java_util_concurrent_locks_AbstractOwnableSynchronizer) \\\n+  f(jdk_internal_foreign_abi_NativeEntryPoint) \\\n+  f(jdk_internal_foreign_abi_ABIDescriptor) \\\n+  f(jdk_internal_foreign_abi_VMStorage) \\\n+  f(jdk_internal_foreign_abi_CallConv) \\\n+  f(jdk_internal_misc_UnsafeConstants) \\\n+  f(java_lang_boxing_object) \\\n+  f(vector_VectorPayload) \\\n+  \/\/end\n+\n+#define BASIC_JAVA_CLASSES_DO(f) \\\n+        BASIC_JAVA_CLASSES_DO_PART1(f) \\\n+        BASIC_JAVA_CLASSES_DO_PART2(f)\n+\n@@ -5071,1 +5568,4 @@\n-      klass == vmClasses::Context_klass()) {\n+      klass == vmClasses::Context_klass() ||\n+      \/\/ It's problematic to archive Reference objects. One of the reasons is that\n+      \/\/ Reference::discovered may pull in unwanted objects (see JDK-8284336)\n+      klass->is_subclass_of(vmClasses::Reference_klass())) {\n@@ -5161,0 +5661,1 @@\n+  java_lang_VirtualThread::init_static_notify_jvmti_events();\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":671,"deletions":170,"binary":false,"changes":841,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,3 +29,3 @@\n-#include \"oops\/oop.hpp\"\n-#include \"oops\/symbol.hpp\"\n-#include \"runtime\/os.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/handles.hpp\"\n+#include \"utilities\/macros.hpp\"\n@@ -35,0 +35,1 @@\n+class JvmtiThreadState;\n@@ -37,50 +38,0 @@\n-\/\/ Interface for manipulating the basic Java classes.\n-\n-#define BASIC_JAVA_CLASSES_DO_PART1(f) \\\n-  f(java_lang_Class) \\\n-  f(java_lang_String) \\\n-  f(java_lang_ref_Reference) \\\n-  \/\/end\n-\n-#define BASIC_JAVA_CLASSES_DO_PART2(f) \\\n-  f(java_lang_System) \\\n-  f(java_lang_ClassLoader) \\\n-  f(java_lang_Throwable) \\\n-  f(java_lang_Thread) \\\n-  f(java_lang_ThreadGroup) \\\n-  f(java_lang_InternalError) \\\n-  f(java_lang_AssertionStatusDirectives) \\\n-  f(java_lang_ref_SoftReference) \\\n-  f(java_lang_invoke_MethodHandle) \\\n-  f(java_lang_invoke_DirectMethodHandle) \\\n-  f(java_lang_invoke_MemberName) \\\n-  f(java_lang_invoke_ResolvedMethodName) \\\n-  f(java_lang_invoke_LambdaForm) \\\n-  f(java_lang_invoke_MethodType) \\\n-  f(java_lang_invoke_CallSite) \\\n-  f(java_lang_invoke_ConstantCallSite) \\\n-  f(java_lang_invoke_MethodHandleNatives_CallSiteContext) \\\n-  f(java_security_AccessControlContext) \\\n-  f(java_lang_reflect_AccessibleObject) \\\n-  f(java_lang_reflect_Method) \\\n-  f(java_lang_reflect_Constructor) \\\n-  f(java_lang_reflect_Field) \\\n-  f(java_lang_reflect_RecordComponent) \\\n-  f(reflect_ConstantPool) \\\n-  f(reflect_UnsafeStaticFieldAccessorImpl) \\\n-  f(java_lang_reflect_Parameter) \\\n-  f(java_lang_Module) \\\n-  f(java_lang_StackTraceElement) \\\n-  f(java_lang_StackFrameInfo) \\\n-  f(java_lang_LiveStackFrameInfo) \\\n-  f(java_util_concurrent_locks_AbstractOwnableSynchronizer) \\\n-  f(jdk_internal_invoke_NativeEntryPoint) \\\n-  f(jdk_internal_misc_UnsafeConstants) \\\n-  f(java_lang_boxing_object) \\\n-  f(vector_VectorPayload) \\\n-  \/\/end\n-\n-#define BASIC_JAVA_CLASSES_DO(f) \\\n-        BASIC_JAVA_CLASSES_DO_PART1(f) \\\n-        BASIC_JAVA_CLASSES_DO_PART2(f)\n-\n@@ -276,1 +227,0 @@\n-  static int _init_lock_offset;\n@@ -294,1 +244,0 @@\n-  static void set_init_lock(oop java_class, oop init_lock);\n@@ -357,4 +306,0 @@\n-  static oop  init_lock(oop java_class);\n-  static void clear_init_lock(oop java_class) {\n-    set_init_lock(java_class, NULL);\n-  }\n@@ -406,0 +351,4 @@\n+#define THREAD_INJECTED_FIELDS(macro)                                  \\\n+  macro(java_lang_Thread, jvmti_thread_state, intptr_signature, false) \\\n+  JFR_ONLY(macro(java_lang_Thread, jfr_epoch, short_signature, false))\n+\n@@ -407,0 +356,1 @@\n+  friend class java_lang_VirtualThread;\n@@ -410,0 +360,1 @@\n+  static int _holder_offset;\n@@ -411,1 +362,0 @@\n-  static int _group_offset;\n@@ -414,1 +364,1 @@\n-  static int _priority_offset;\n+  static int _jvmti_thread_state_offset;\n@@ -417,4 +367,1 @@\n-  static int _daemon_offset;\n-  static int _stillborn_offset;\n-  static int _stackSize_offset;\n-  static int _thread_status_offset;\n+  static int _continuation_offset;\n@@ -423,0 +370,2 @@\n+  static int _extentLocalBindings_offset;\n+  JFR_ONLY(static int _jfr_epoch_offset;)\n@@ -433,0 +382,2 @@\n+  \/\/ FieldHolder\n+  static oop holder(oop java_thread);\n@@ -460,1 +411,10 @@\n-  static jlong thread_id(oop java_thread);\n+  static int64_t thread_id(oop java_thread);\n+  static ByteSize thread_id_offset();\n+  \/\/ Continuation\n+  static inline oop continuation(oop java_thread);\n+\n+  static JvmtiThreadState* jvmti_thread_state(oop java_thread);\n+  static void set_jvmti_thread_state(oop java_thread, JvmtiThreadState* state);\n+\n+  \/\/ Clear all extent local bindings on error\n+  static void clear_extentLocalBindings(oop java_thread);\n@@ -472,0 +432,7 @@\n+  \/\/ Fill in current stack trace, can cause GC\n+  static oop async_get_stack_trace(oop java_thread, TRAPS);\n+\n+  JFR_ONLY(static u2 jfr_epoch(oop java_thread);)\n+  JFR_ONLY(static void set_jfr_epoch(oop java_thread, u2 epoch);)\n+  JFR_ONLY(static int jfr_epoch_offset() { CHECK_INIT(_jfr_epoch_offset); })\n+\n@@ -476,0 +443,52 @@\n+\/\/ Interface to java.lang.Thread$FieldHolder objects\n+\n+class java_lang_Thread_FieldHolder : AllStatic {\n+ private:\n+  static int _group_offset;\n+  static int _priority_offset;\n+  static int _stackSize_offset;\n+  static int _stillborn_offset;\n+  static int _daemon_offset;\n+  static int _thread_status_offset;\n+\n+  static void compute_offsets();\n+\n+ public:\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  static oop threadGroup(oop holder);\n+\n+  static ThreadPriority priority(oop holder);\n+  static void set_priority(oop holder, ThreadPriority priority);\n+\n+  static jlong stackSize(oop holder);\n+\n+  static bool is_stillborn(oop holder);\n+  static void set_stillborn(oop holder);\n+\n+  static bool is_daemon(oop holder);\n+  static void set_daemon(oop holder);\n+\n+  static void set_thread_status(oop holder, JavaThreadStatus);\n+  static JavaThreadStatus get_thread_status(oop holder);\n+\n+  friend class JavaClasses;\n+};\n+\n+\/\/ Interface to java.lang.Thread$Constants objects\n+\n+class java_lang_Thread_Constants : AllStatic {\n+ private:\n+  static int _static_VTHREAD_GROUP_offset;\n+  static int _static_NOT_SUPPORTED_CLASSLOADER_offset;\n+\n+  static void compute_offsets();\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+ public:\n+  static oop get_VTHREAD_GROUP();\n+  static oop get_NOT_SUPPORTED_CLASSLOADER();\n+\n+  friend class JavaClasses;\n+};\n+\n@@ -482,4 +501,1 @@\n-  static int _threads_offset;\n-  static int _groups_offset;\n-  static int _destroyed_offset;\n-  static int _nthreads_offset;\n+\n@@ -489,0 +505,3 @@\n+  static int _groups_offset;\n+  static int _nweaks_offset;\n+  static int _weaks_offset;\n@@ -496,1 +515,1 @@\n-  static oop  parent(oop java_thread_group);\n+  static oop parent(oop java_thread_group);\n@@ -499,9 +518,0 @@\n-  \/\/ (\"name as oop\" accessor is not necessary)\n-  \/\/ Number of threads in group\n-  static int nthreads(oop java_thread_group);\n-  \/\/ threads\n-  static objArrayOop threads(oop java_thread_group);\n-  \/\/ Number of threads in group\n-  static int ngroups(oop java_thread_group);\n-  \/\/ groups\n-  static objArrayOop groups(oop java_thread_group);\n@@ -510,2 +520,0 @@\n-  \/\/ Destroyed\n-  static bool is_destroyed(oop java_thread_group);\n@@ -514,0 +522,10 @@\n+\n+  \/\/ Number of strongly reachable thread groups\n+  static int ngroups(oop java_thread_group);\n+  \/\/ Strongly reachable thread groups\n+  static objArrayOop groups(oop java_thread_group);\n+  \/\/ Number of weakly reachable thread groups\n+  static int nweaks(oop java_thread_group);\n+  \/\/ Weakly reachable thread groups\n+  static objArrayOop weaks(oop java_thread_group);\n+\n@@ -519,0 +537,47 @@\n+\/\/ Interface to java.lang.VirtualThread objects\n+\n+class java_lang_VirtualThread : AllStatic {\n+ private:\n+  static int static_notify_jvmti_events_offset;\n+  static int static_vthread_scope_offset;\n+  static int _carrierThread_offset;\n+  static int _continuation_offset;\n+  static int _state_offset;\n+  JFR_ONLY(static int _jfr_epoch_offset;)\n+ public:\n+  enum {\n+    NEW          = 0,\n+    STARTED      = 1,\n+    RUNNABLE     = 2,\n+    RUNNING      = 3,\n+    PARKING      = 4,\n+    PARKED       = 5,\n+    PINNED       = 6,\n+    YIELDING     = 7,\n+    TERMINATED   = 99,\n+\n+    \/\/ can be suspended from scheduling when unmounted\n+    SUSPENDED    = 1 << 8,\n+    RUNNABLE_SUSPENDED = (RUNNABLE | SUSPENDED),\n+    PARKED_SUSPENDED   = (PARKED | SUSPENDED)\n+  };\n+\n+  static void compute_offsets();\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Testers\n+  static bool is_subclass(Klass* klass) {\n+    return klass->is_subclass_of(vmClasses::VirtualThread_klass());\n+  }\n+  static bool is_instance(oop obj);\n+\n+  static oop vthread_scope();\n+  static oop carrier_thread(oop vthread);\n+  static oop continuation(oop vthread);\n+  static int state(oop vthread);\n+  static JavaThreadStatus map_state_to_thread_status(int state);\n+  static bool notify_jvmti_events();\n+  static void set_notify_jvmti_events(bool enable);\n+  static void init_static_notify_jvmti_events();\n+};\n+\n@@ -533,3 +598,4 @@\n-    trace_next_offset    = 4,\n-    trace_hidden_offset  = 5,\n-    trace_size           = 6,\n+    trace_conts_offset   = 4,\n+    trace_next_offset    = 5,\n+    trace_hidden_offset  = 6,\n+    trace_size           = 7,\n@@ -576,0 +642,1 @@\n+\n@@ -577,1 +644,1 @@\n-  static void get_stack_trace_elements(Handle throwable, objArrayHandle stack_trace, TRAPS);\n+  static void get_stack_trace_elements(int depth, Handle backtrace, objArrayHandle stack_trace, TRAPS);\n@@ -921,0 +988,1 @@\n+  static inline oop weak_referent(oop ref);\n@@ -1054,1 +1122,1 @@\n-class jdk_internal_invoke_NativeEntryPoint: AllStatic {\n+class jdk_internal_foreign_abi_NativeEntryPoint: AllStatic {\n@@ -1058,5 +1126,1 @@\n-  static int _shadow_space_offset;\n-  static int _argMoves_offset;\n-  static int _returnMoves_offset;\n-  static int _need_transition_offset;\n-  static int _name_offset;\n+  static int _downcall_stub_address_offset;\n@@ -1071,5 +1135,1 @@\n-  static jint       shadow_space(oop entry);\n-  static oop        argMoves(oop entry);\n-  static oop        returnMoves(oop entry);\n-  static jboolean   need_transition(oop entry);\n-  static oop        name(oop entry);\n+  static jlong      downcall_stub_address(oop entry);\n@@ -1086,6 +1146,86 @@\n-  static int shadow_space_offset_in_bytes()    { return _shadow_space_offset;    }\n-  static int argMoves_offset_in_bytes()        { return _argMoves_offset;        }\n-  static int returnMoves_offset_in_bytes()     { return _returnMoves_offset;     }\n-  static int need_transition_offset_in_bytes() { return _need_transition_offset; }\n-  static int method_type_offset_in_bytes()     { return _method_type_offset;     }\n-  static int name_offset_in_bytes()            { return _name_offset;            }\n+  static int method_type_offset_in_bytes()           { return _method_type_offset; }\n+  static int downcall_stub_address_offset_in_bytes() { return _downcall_stub_address_offset; }\n+};\n+\n+class jdk_internal_foreign_abi_ABIDescriptor: AllStatic {\n+  friend class JavaClasses;\n+\n+ private:\n+  static int _inputStorage_offset;\n+  static int _outputStorage_offset;\n+  static int _volatileStorage_offset;\n+  static int _stackAlignment_offset;\n+  static int _shadowSpace_offset;\n+  static int _targetAddrStorage_offset;\n+  static int _retBufAddrStorage_offset;\n+\n+  static void compute_offsets();\n+\n+ public:\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Accessors\n+  static objArrayOop inputStorage(oop entry);\n+  static objArrayOop outputStorage(oop entry);\n+  static objArrayOop volatileStorage(oop entry);\n+  static jint        stackAlignment(oop entry);\n+  static jint        shadowSpace(oop entry);\n+  static oop         targetAddrStorage(oop entry);\n+  static oop         retBufAddrStorage(oop entry);\n+\n+  \/\/ Testers\n+  static bool is_subclass(Klass* klass) {\n+    return vmClasses::ABIDescriptor_klass() != NULL &&\n+      klass->is_subclass_of(vmClasses::ABIDescriptor_klass());\n+  }\n+  static bool is_instance(oop obj);\n+};\n+\n+class jdk_internal_foreign_abi_VMStorage: AllStatic {\n+  friend class JavaClasses;\n+\n+ private:\n+  static int _type_offset;\n+  static int _index_offset;\n+  static int _debugName_offset;\n+\n+  static void compute_offsets();\n+\n+ public:\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Accessors\n+  static jint        type(oop entry);\n+  static jint        index(oop entry);\n+  static oop         debugName(oop entry);\n+\n+  \/\/ Testers\n+  static bool is_subclass(Klass* klass) {\n+    return vmClasses::VMStorage_klass() != NULL &&\n+      klass->is_subclass_of(vmClasses::VMStorage_klass());\n+  }\n+  static bool is_instance(oop obj);\n+};\n+\n+class jdk_internal_foreign_abi_CallConv: AllStatic {\n+  friend class JavaClasses;\n+\n+ private:\n+  static int _argRegs_offset;\n+  static int _retRegs_offset;\n+\n+  static void compute_offsets();\n+\n+ public:\n+  static void serialize_offsets(SerializeClosure* f) NOT_CDS_RETURN;\n+\n+  \/\/ Accessors\n+  static objArrayOop argRegs(oop entry);\n+  static objArrayOop retRegs(oop entry);\n+\n+  \/\/ Testers\n+  static bool is_subclass(Klass* klass) {\n+    return vmClasses::CallConv_klass() != NULL &&\n+      klass->is_subclass_of(vmClasses::CallConv_klass());\n+  }\n+  static bool is_instance(oop obj);\n@@ -1502,0 +1642,1 @@\n+  static int _contScope_offset;\n@@ -1507,1 +1648,1 @@\n-  static void set_method_and_bci(Handle stackFrame, const methodHandle& method, int bci, TRAPS);\n+  static void set_method_and_bci(Handle stackFrame, const methodHandle& method, int bci, oop cont, TRAPS);\n@@ -1511,0 +1652,1 @@\n+  static void set_contScope(oop info, oop value);\n@@ -1768,14 +1910,0 @@\n-#define DECLARE_INJECTED_FIELD_ENUM(klass, name, signature, may_be_java) \\\n-  klass##_##name##_enum,\n-\n-#define ALL_INJECTED_FIELDS(macro)          \\\n-  STRING_INJECTED_FIELDS(macro)             \\\n-  CLASS_INJECTED_FIELDS(macro)              \\\n-  CLASSLOADER_INJECTED_FIELDS(macro)        \\\n-  RESOLVEDMETHOD_INJECTED_FIELDS(macro)     \\\n-  MEMBERNAME_INJECTED_FIELDS(macro)         \\\n-  CALLSITECONTEXT_INJECTED_FIELDS(macro)    \\\n-  STACKFRAMEINFO_INJECTED_FIELDS(macro)     \\\n-  MODULE_INJECTED_FIELDS(macro)             \\\n-  INTERNALERROR_INJECTED_FIELDS(macro)\n-\n@@ -1785,0 +1913,2 @@\n+enum class InjectedFieldID : int;\n+\n@@ -1792,4 +1922,0 @@\n-  enum InjectedFieldID {\n-    ALL_INJECTED_FIELDS(DECLARE_INJECTED_FIELD_ENUM)\n-    MAX_enum\n-  };\n@@ -1804,2 +1930,7 @@\n-};\n-#undef DECLARE_INJECTED_FIELD_ENUM\n+  static void compute_offset(int &dest_offset,\n+                             InstanceKlass* ik, Symbol* name_symbol, Symbol* signature_symbol,\n+                             bool is_static = false);\n+  static void compute_offset(int& dest_offset, InstanceKlass* ik,\n+                             const char* name_string, Symbol* signature_symbol,\n+                             bool is_static = false);\n+};\n@@ -1809,0 +1940,1 @@\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":257,"deletions":125,"binary":false,"changes":382,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"oops\/instanceKlass.inline.hpp\"\n@@ -34,0 +35,1 @@\n+#include \"oops\/typeArrayOop.inline.hpp\"\n@@ -136,0 +138,4 @@\n+oop java_lang_ref_Reference::weak_referent(oop ref) {\n+  return ref->obj_field_access<ON_WEAK_OOP_REF>(_referent_offset);\n+}\n+\n@@ -192,0 +198,24 @@\n+inline oop java_lang_Thread::continuation(oop java_thread) {\n+  return java_thread->obj_field(_continuation_offset);\n+}\n+\n+inline int64_t java_lang_Thread::thread_id(oop java_thread) {\n+  return java_thread->long_field(_tid_offset);\n+}\n+\n+inline oop java_lang_VirtualThread::vthread_scope() {\n+  oop base = vmClasses::VirtualThread_klass()->static_field_base_raw();\n+  return base->obj_field(static_vthread_scope_offset);\n+}\n+\n+#if INCLUDE_JFR\n+inline u2 java_lang_Thread::jfr_epoch(oop ref) {\n+  return ref->short_field(_jfr_epoch_offset);\n+}\n+\n+inline void java_lang_Thread::set_jfr_epoch(oop ref, u2 epoch) {\n+  ref->short_field_put(_jfr_epoch_offset, epoch);\n+}\n+#endif \/\/ INCLUDE_JFR\n+\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.inline.hpp","additions":31,"deletions":1,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -33,1 +34,0 @@\n-#include \"runtime\/thread.hpp\"\n","filename":"src\/hotspot\/share\/classfile\/placeholders.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -176,0 +176,2 @@\n+#define CHECK_NT CHECK_(VerificationType::bogus_type())\n+\n@@ -177,1 +179,1 @@\n-  u1 tag = _stream->get_u1(THREAD);\n+  u1 tag = _stream->get_u1(CHECK_NT);\n@@ -182,1 +184,1 @@\n-    u2 class_index = _stream->get_u2(THREAD);\n+    u2 class_index = _stream->get_u2(CHECK_NT);\n@@ -209,1 +211,1 @@\n-    u2 offset = _stream->get_u2(THREAD);\n+    u2 offset = _stream->get_u2(CHECK_NT);\n@@ -227,1 +229,1 @@\n-  u1 frame_type = _stream->get_u1(THREAD);\n+  u1 frame_type = _stream->get_u1(CHECK_NULL);\n@@ -281,1 +283,1 @@\n-  u2 offset_delta = _stream->get_u2(THREAD);\n+  u2 offset_delta = _stream->get_u2(CHECK_NULL);\n@@ -373,1 +375,1 @@\n-      locals[real_length] = parse_verification_type(&flags, THREAD);\n+      locals[real_length] = parse_verification_type(&flags, CHECK_NULL);\n@@ -395,1 +397,1 @@\n-    u2 locals_size = _stream->get_u2(THREAD);\n+    u2 locals_size = _stream->get_u2(CHECK_NULL);\n@@ -403,1 +405,1 @@\n-      locals[real_locals_size] = parse_verification_type(&flags, THREAD);\n+      locals[real_locals_size] = parse_verification_type(&flags, CHECK_NULL);\n@@ -413,1 +415,1 @@\n-    u2 stack_size = _stream->get_u2(THREAD);\n+    u2 stack_size = _stream->get_u2(CHECK_NULL);\n@@ -421,1 +423,1 @@\n-      stack[real_stack_size] = parse_verification_type(NULL, THREAD);\n+      stack[real_stack_size] = parse_verification_type(NULL, CHECK_NULL);\n","filename":"src\/hotspot\/share\/classfile\/stackMapTable.cpp","additions":13,"deletions":11,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -93,1 +93,0 @@\n-ResolutionErrorTable*  SystemDictionary::_resolution_errors   = NULL;\n@@ -280,18 +279,10 @@\n-    return resolve_instance_class_or_null_helper(class_name, class_loader, protection_domain, THREAD);\n-  }\n-}\n-\n-\/\/ name may be in the form of \"java\/lang\/Object\" or \"Ljava\/lang\/Object;\"\n-InstanceKlass* SystemDictionary::resolve_instance_class_or_null_helper(Symbol* class_name,\n-                                                                       Handle class_loader,\n-                                                                       Handle protection_domain,\n-                                                                       TRAPS) {\n-  assert(class_name != NULL && !Signature::is_array(class_name), \"must be\");\n-  if (Signature::has_envelope(class_name)) {\n-    ResourceMark rm(THREAD);\n-    \/\/ Ignore wrapping L and ;. (and Q and ; for value types);\n-    TempNewSymbol name = SymbolTable::new_symbol(class_name->as_C_string() + 1,\n-                                                 class_name->utf8_length() - 2);\n-    return resolve_instance_class_or_null(name, class_loader, protection_domain, THREAD);\n-  } else {\n-    return resolve_instance_class_or_null(class_name, class_loader, protection_domain, THREAD);\n+    assert(class_name != NULL && !Signature::is_array(class_name), \"must be\");\n+    if (Signature::has_envelope(class_name)) {\n+      ResourceMark rm(THREAD);\n+      \/\/ Ignore wrapping L and ; (and Q and ; for value types).\n+      TempNewSymbol name = SymbolTable::new_symbol(class_name->as_C_string() + 1,\n+                                                   class_name->utf8_length() - 2);\n+      return resolve_instance_class_or_null(name, class_loader, protection_domain, THREAD);\n+    } else {\n+      return resolve_instance_class_or_null(class_name, class_loader, protection_domain, THREAD);\n+    }\n@@ -437,4 +428,4 @@\n-    SystemDictionary::resolve_instance_class_or_null_helper(super_name,\n-                                                            class_loader,\n-                                                            protection_domain,\n-                                                            THREAD);\n+    SystemDictionary::resolve_instance_class_or_null(super_name,\n+                                                     class_loader,\n+                                                     protection_domain,\n+                                                     THREAD);\n@@ -1074,0 +1065,9 @@\n+  if (pkg_entry == NULL) {\n+    \/\/ We might have looked up pkg_entry before the module system was initialized.\n+    \/\/ Need to reload it now.\n+    TempNewSymbol pkg_name = ClassLoader::package_from_class_name(class_name);\n+    if (pkg_name != NULL) {\n+      pkg_entry = ClassLoaderData::class_loader_data(class_loader())->packages()->lookup_only(pkg_name);\n+    }\n+  }\n+\n@@ -1524,1 +1524,0 @@\n-  k->eager_initialize(THREAD);\n@@ -1685,1 +1684,1 @@\n-      resolution_errors()->purge_resolution_errors();\n+      ResolutionErrorTable::purge_resolution_errors();\n@@ -1727,1 +1726,0 @@\n-  _resolution_errors   = new ResolutionErrorTable(_resolution_error_size);\n@@ -1742,1 +1740,1 @@\n-\/\/ that the dictionary needs to maintain a set of contraints that\n+\/\/ that the dictionary needs to maintain a set of constraints that\n@@ -1932,2 +1930,0 @@\n-  unsigned int hash = resolution_errors()->compute_hash(pool, which);\n-  int index = resolution_errors()->hash_to_index(hash);\n@@ -1936,1 +1932,1 @@\n-    ResolutionErrorEntry* entry = resolution_errors()->find_entry(index, hash, pool, which);\n+    ResolutionErrorEntry* entry = ResolutionErrorTable::find_entry(pool, which);\n@@ -1938,1 +1934,1 @@\n-      resolution_errors()->add_entry(index, hash, pool, which, error, message, cause, cause_msg);\n+      ResolutionErrorTable::add_entry(pool, which, error, message, cause, cause_msg);\n@@ -1945,1 +1941,1 @@\n-  resolution_errors()->delete_entry(pool);\n+  ResolutionErrorTable::delete_entry(pool);\n@@ -1951,2 +1947,1 @@\n-  unsigned int hash = resolution_errors()->compute_hash(pool, which);\n-  int index = resolution_errors()->hash_to_index(hash);\n+\n@@ -1955,1 +1950,1 @@\n-    ResolutionErrorEntry* entry = resolution_errors()->find_entry(index, hash, pool, which);\n+    ResolutionErrorEntry* entry = ResolutionErrorTable::find_entry(pool, which);\n@@ -1971,0 +1966,1 @@\n+\n@@ -1974,2 +1970,0 @@\n-  unsigned int hash = resolution_errors()->compute_hash(pool, which);\n-  int index = resolution_errors()->hash_to_index(hash);\n@@ -1978,1 +1972,1 @@\n-    ResolutionErrorEntry* entry = resolution_errors()->find_entry(index, hash, pool, which);\n+    ResolutionErrorEntry* entry = ResolutionErrorTable::find_entry(pool, which);\n@@ -1986,1 +1980,1 @@\n-      resolution_errors()->add_entry(index, hash, pool, which, message);\n+      ResolutionErrorTable::add_entry(pool, which, message);\n@@ -1993,2 +1987,0 @@\n-  unsigned int hash = resolution_errors()->compute_hash(pool, which);\n-  int index = resolution_errors()->hash_to_index(hash);\n@@ -1997,1 +1989,1 @@\n-    ResolutionErrorEntry* entry = resolution_errors()->find_entry(index, hash, pool, which);\n+    ResolutionErrorEntry* entry = ResolutionErrorTable::find_entry(pool, which);\n@@ -2006,1 +1998,0 @@\n-\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":34,"deletions":43,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -75,1 +75,0 @@\n-class ResolutionErrorTable;\n@@ -303,3 +302,0 @@\n-  \/\/ Resolution errors\n-  static ResolutionErrorTable*   _resolution_errors;\n-\n@@ -322,1 +318,0 @@\n-  static ResolutionErrorTable* resolution_errors() { return _resolution_errors; }\n@@ -327,4 +322,0 @@\n-  static InstanceKlass* resolve_instance_class_or_null_helper(Symbol* name,\n-                                                              Handle class_loader,\n-                                                              Handle protection_domain,\n-                                                              TRAPS);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.hpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-  \/\/ As specifed in the JVM spec\n+  \/\/ As specified in the JVM spec\n@@ -213,1 +213,1 @@\n-    \/\/ Since noone should call this on a query type anyway, this is ok.\n+    \/\/ Since no one should call this on a query type anyway, this is ok.\n","filename":"src\/hotspot\/share\/classfile\/verificationType.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,0 +55,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -58,1 +59,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -613,1 +613,1 @@\n-      _message(NULL), _method_signatures_table(NULL), _klass(klass) {\n+      _message(NULL), _klass(klass) {\n@@ -644,4 +644,0 @@\n-  \/\/ Create hash table containing method signatures.\n-  method_signatures_table_type method_signatures_table;\n-  set_method_signatures_table(&method_signatures_table);\n-\n@@ -653,1 +649,1 @@\n-    if (was_recursively_verified())  return;\n+    if (was_recursively_verified()) return;\n@@ -1238,1 +1234,1 @@\n-          \/\/ 4938384: relaxed constraint in JVMS 3nd edition.\n+          \/\/ 4938384: relaxed constraint in JVMS 3rd edition.\n@@ -3012,9 +3008,9 @@\n-              bool is_assignable = current_type().is_assignable_from(\n-                stack_object_type, this, true, CHECK_VERIFY(this));\n-              if (!is_assignable) {\n-                if (ref_class_type.name() == vmSymbols::java_lang_Object()\n-                    && stack_object_type.is_array()\n-                    && method_name == vmSymbols::clone_name()) {\n-                  \/\/ Special case: arrays pretend to implement public Object\n-                  \/\/ clone().\n-                } else {\n+              if (ref_class_type.name() == vmSymbols::java_lang_Object()\n+                  && stack_object_type.is_array()\n+                  && method_name == vmSymbols::clone_name()) {\n+                \/\/ Special case: arrays pretend to implement public Object\n+                \/\/ clone().\n+              } else {\n+                bool is_assignable = current_type().is_assignable_from(\n+                  stack_object_type, this, true, CHECK_VERIFY(this));\n+                if (!is_assignable) {\n","filename":"src\/hotspot\/share\/classfile\/verifier.cpp","additions":14,"deletions":18,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -292,1 +292,1 @@\n-  method_signatures_table_type* _method_signatures_table;\n+  method_signatures_table_type _method_signatures_table;\n@@ -444,6 +444,2 @@\n-  method_signatures_table_type* method_signatures_table() const {\n-    return _method_signatures_table;\n-  }\n-\n-  void set_method_signatures_table(method_signatures_table_type* method_signatures_table) {\n-    _method_signatures_table = method_signatures_table;\n+  method_signatures_table_type* method_signatures_table() {\n+    return &_method_signatures_table;\n","filename":"src\/hotspot\/share\/classfile\/verifier.hpp","additions":3,"deletions":7,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+  do_klass(Thread_FieldHolder_klass,                    java_lang_Thread_FieldHolder                          ) \\\n+  do_klass(Thread_Constants_klass,                      java_lang_Thread_Constants                            ) \\\n@@ -91,0 +93,2 @@\n+  do_klass(BasicVirtualThread_klass,                    java_lang_BaseVirtualThread                           ) \\\n+  do_klass(VirtualThread_klass,                         java_lang_VirtualThread                               ) \\\n@@ -99,2 +103,5 @@\n-  \/* NOTE: needed too early in bootstrapping process to have checks based on JDK version *\/                     \\\n-  \/* It's okay if this turns out to be NULL in non-1.4 JDKs. *\/                                                 \\\n+  do_klass(Runnable_klass,                              java_lang_Runnable                                    ) \\\n+  do_klass(ContinuationScope_klass,                     jdk_internal_vm_ContinuationScope                     ) \\\n+  do_klass(Continuation_klass,                          jdk_internal_vm_Continuation                          ) \\\n+  do_klass(StackChunk_klass,                            jdk_internal_vm_StackChunk                            ) \\\n+                                                                                                                \\\n@@ -110,1 +117,1 @@\n-  \/* support for dynamic typing; it's OK if these are NULL in earlier JDKs *\/                                   \\\n+  \/* support for dynamic typing *\/                                                                              \\\n@@ -121,1 +128,4 @@\n-  do_klass(NativeEntryPoint_klass,                      jdk_internal_invoke_NativeEntryPoint                  ) \\\n+  do_klass(NativeEntryPoint_klass,                      jdk_internal_foreign_abi_NativeEntryPoint             ) \\\n+  do_klass(ABIDescriptor_klass,                         jdk_internal_foreign_abi_ABIDescriptor                ) \\\n+  do_klass(VMStorage_klass,                             jdk_internal_foreign_abi_VMStorage                    ) \\\n+  do_klass(CallConv_klass,                              jdk_internal_foreign_abi_CallConv                     ) \\\n@@ -127,1 +137,0 @@\n-  \/* Note: MethodHandle must be first, and VolatileCallSite last in group *\/                                    \\\n@@ -152,1 +161,0 @@\n-  \/* It's okay if this turns out to be NULL in non-1.4 JDKs. *\/                                                 \\\n@@ -187,0 +195,3 @@\n+  \/* GC support *\/                                                                                              \\\n+  do_klass(FillerObject_klass,                          jdk_internal_vm_FillerObject                          ) \\\n+                                                                                                                \\\n","filename":"src\/hotspot\/share\/classfile\/vmClassMacros.hpp","additions":17,"deletions":6,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,0 +78,1 @@\n+  case vmIntrinsics::_currentCarrierThread:\n@@ -79,0 +80,1 @@\n+  case vmIntrinsics::_extentLocalCache:\n@@ -95,0 +97,1 @@\n+  case vmIntrinsics::_Continuation_doYield:\n@@ -117,1 +120,0 @@\n-  case vmIntrinsics::_getClassId:\n@@ -125,0 +127,1 @@\n+  case vmIntrinsics::_currentCarrierThread:\n@@ -126,0 +129,3 @@\n+  case vmIntrinsics::_setCurrentThread:\n+  case vmIntrinsics::_extentLocalCache:\n+  case vmIntrinsics::_setExtentLocalCache:\n@@ -234,0 +240,1 @@\n+    case vmIntrinsics::_Continuation_doYield:\n@@ -257,0 +264,1 @@\n+  case vmIntrinsics::_currentCarrierThread:\n@@ -260,0 +268,3 @@\n+  case vmIntrinsics::_setCurrentThread:\n+  case vmIntrinsics::_extentLocalCache:\n+  case vmIntrinsics::_setExtentLocalCache:\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.cpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -107,1 +107,1 @@\n-\/\/ add the declaration of the intrinsic to the approriate section of the list.\n+\/\/ add the declaration of the intrinsic to the appropriate section of the list.\n@@ -129,0 +129,1 @@\n+  do_signature(long2_int_signature,       \"(JJ)I\")                                                                      \\\n@@ -137,0 +138,1 @@\n+  do_name(round_name, \"round\")                                                                                          \\\n@@ -149,0 +151,1 @@\n+  do_name(expand_name,\"expand\")                                                                                         \\\n@@ -151,3 +154,3 @@\n-  do_intrinsic(_fabs,                     java_lang_Math,         abs_name,   float_float_signature,           F_S)   \\\n-  do_intrinsic(_iabs,                     java_lang_Math,         abs_name,   int_int_signature,           F_S)   \\\n-  do_intrinsic(_labs,                     java_lang_Math,         abs_name,   long_long_signature,           F_S)   \\\n+  do_intrinsic(_fabs,                     java_lang_Math,         abs_name,   float_float_signature,             F_S)   \\\n+  do_intrinsic(_iabs,                     java_lang_Math,         abs_name,   int_int_signature,                 F_S)   \\\n+  do_intrinsic(_labs,                     java_lang_Math,         abs_name,   long_long_signature,               F_S)   \\\n@@ -188,0 +191,2 @@\n+  do_intrinsic(_roundD,                   java_lang_Math,         round_name,         double_long_signature,     F_S)   \\\n+  do_intrinsic(_roundF,                   java_lang_Math,         round_name,         float_int_signature,       F_S)   \\\n@@ -203,0 +208,4 @@\n+  do_intrinsic(_floatIsInfinite,          java_lang_Float,        isInfinite_name,    float_bool_signature,      F_S)   \\\n+   do_name(     isInfinite_name,                                 \"isInfinite\")                                          \\\n+  do_intrinsic(_doubleIsInfinite,         java_lang_Double,       isInfinite_name,    double_bool_signature,     F_S)   \\\n+                                                                                                                        \\\n@@ -216,0 +225,11 @@\n+  do_intrinsic(_compareUnsigned_i,        java_lang_Integer,      compareUnsigned_name,     int2_int_signature,  F_S)   \\\n+  do_intrinsic(_compareUnsigned_l,        java_lang_Long,         compareUnsigned_name,     long2_int_signature, F_S)   \\\n+   do_name(     compareUnsigned_name,                            \"compareUnsigned\")                                     \\\n+                                                                                                                        \\\n+  do_intrinsic(_divideUnsigned_i,         java_lang_Integer,      divideUnsigned_name,      int2_int_signature,  F_S)   \\\n+  do_intrinsic(_remainderUnsigned_i,      java_lang_Integer,      remainderUnsigned_name,   int2_int_signature,  F_S)   \\\n+   do_name(     divideUnsigned_name,                             \"divideUnsigned\")                                      \\\n+  do_intrinsic(_divideUnsigned_l,         java_lang_Long,         divideUnsigned_name,      long2_long_signature, F_S)  \\\n+  do_intrinsic(_remainderUnsigned_l,      java_lang_Long,         remainderUnsigned_name,   long2_long_signature, F_S)  \\\n+   do_name(     remainderUnsigned_name,                          \"remainderUnsigned\")                                   \\\n+                                                                                                                        \\\n@@ -224,0 +244,4 @@\n+  do_intrinsic(_compress_i,               java_lang_Integer,      compress_name,            int2_int_signature,   F_S)  \\\n+  do_intrinsic(_compress_l,               java_lang_Long,         compress_name,            long2_long_signature, F_S)  \\\n+  do_intrinsic(_expand_i,                 java_lang_Integer,      expand_name,              int2_int_signature,   F_S)  \\\n+  do_intrinsic(_expand_l,                 java_lang_Long,         expand_name,              long2_long_signature, F_S)  \\\n@@ -237,1 +261,1 @@\n-                                                                                                                        \\\n+                                                                                                                       \\\n@@ -247,0 +271,3 @@\n+                                                                                                                        \\\n+  do_intrinsic(_currentCarrierThread,     java_lang_Thread,       currentCarrierThread_name, currentThread_signature, F_SN) \\\n+   do_name(     currentCarrierThread_name,                       \"currentCarrierThread\")                                \\\n@@ -250,0 +277,8 @@\n+  do_intrinsic(_extentLocalCache,          java_lang_Thread,       extentLocalCache_name, extentLocalCache_signature, F_SN) \\\n+   do_name(     extentLocalCache_name,                            \"extentLocalCache\")                                     \\\n+   do_signature(extentLocalCache_signature,                       \"()[Ljava\/lang\/Object;\")                               \\\n+  do_intrinsic(_setExtentLocalCache,       java_lang_Thread,       setExtentLocalCache_name, setExtentLocalCache_signature, F_SN) \\\n+   do_name(     setExtentLocalCache_name,                         \"setExtentLocalCache\")                                  \\\n+   do_signature(setExtentLocalCache_signature,                    \"([Ljava\/lang\/Object;)V\")                              \\\n+  do_intrinsic(_setCurrentThread,         java_lang_Thread,       setCurrentThread_name, thread_void_signature,   F_RN) \\\n+   do_name(     setCurrentThread_name,                           \"setCurrentThread\")                                    \\\n@@ -511,0 +546,11 @@\n+  \/* jdk\/internal\/vm\/Continuation *\/                                                                                    \\\n+  do_class(jdk_internal_vm_Continuation, \"jdk\/internal\/vm\/Continuation\")                                                \\\n+  do_intrinsic(_Continuation_enter,        jdk_internal_vm_Continuation, enter_name,        continuationEnter_signature, F_S) \\\n+   do_signature(continuationEnter_signature,                      \"(Ljdk\/internal\/vm\/Continuation;Z)V\")                 \\\n+  do_intrinsic(_Continuation_enterSpecial, jdk_internal_vm_Continuation, enterSpecial_name, continuationEnterSpecial_signature, F_SN) \\\n+   do_signature(continuationEnterSpecial_signature,               \"(Ljdk\/internal\/vm\/Continuation;ZZ)V\")                \\\n+  do_signature(continuationGetStacks_signature,                   \"(III)V\")                                             \\\n+  do_alias(continuationOnPinned_signature,      int_void_signature)                                                     \\\n+  do_intrinsic(_Continuation_doYield,      jdk_internal_vm_Continuation, doYield_name,      continuationDoYield_signature, F_S) \\\n+   do_alias(    continuationDoYield_signature,     void_int_signature)                                                  \\\n+                                                                                                                        \\\n@@ -921,1 +967,1 @@\n-                                     \"I\"                                                                                                       \\\n+                                     \"J\"                                                                                                       \\\n@@ -935,1 +981,2 @@\n-                                            \"Ljava\/lang\/Object;\"                                                                               \\\n+                                            \"Ljava\/lang\/Object;\"                                                                               \\\n+                                            \"J\"                                                                                                \\\n@@ -948,2 +995,4 @@\n-                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n-                                      \"Ljava\/lang\/Object;ILjdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)\"                        \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\"                                                   \\\n+                                      \"Ljava\/lang\/Object;\"                                                                                     \\\n+                                      \"J\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$StoreVectorOperation;)\"                                           \\\n@@ -963,1 +1012,1 @@\n-                                             \"I\"                                                                                               \\\n+                                             \"J\"                                                                                               \\\n@@ -1123,0 +1172,11 @@\n+  do_intrinsic(_VectorCompressExpand, jdk_internal_vm_vector_VectorSupport, vector_compress_expand_op_name, vector_compress_expand_op_sig, F_S)\\\n+   do_signature(vector_compress_expand_op_sig, \"(I\"                                                                                            \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"Ljava\/lang\/Class;\"                                                                                      \\\n+                                      \"I\"                                                                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$Vector;\"                                                          \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorMask;\"                                                      \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$CompressExpandOperation;)\"                                        \\\n+                                      \"Ljdk\/internal\/vm\/vector\/VectorSupport$VectorPayload;\")                                                  \\\n+   do_name(vector_compress_expand_op_name,     \"compressExpandOp\")                                                                             \\\n@@ -1231,1 +1291,1 @@\n-  LAST_COMPILER_INLINE = _VectorMaskOp,\n+  LAST_COMPILER_INLINE = _VectorCompressExpand,\n","filename":"src\/hotspot\/share\/classfile\/vmIntrinsics.hpp","additions":72,"deletions":12,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -65,0 +65,2 @@\n+  template(java_lang_Thread_FieldHolder,              \"java\/lang\/Thread$FieldHolder\")             \\\n+  template(java_lang_Thread_Constants,                \"java\/lang\/Thread$Constants\")               \\\n@@ -66,0 +68,2 @@\n+  template(java_lang_BaseVirtualThread,               \"java\/lang\/BaseVirtualThread\")              \\\n+  template(java_lang_VirtualThread,                   \"java\/lang\/VirtualThread\")                  \\\n@@ -71,0 +75,3 @@\n+  template(java_lang_Runnable,                        \"java\/lang\/Runnable\")                       \\\n+  template(jdk_internal_vm_ContinuationScope,         \"jdk\/internal\/vm\/ContinuationScope\")        \\\n+  template(jdk_internal_vm_StackChunk,                \"jdk\/internal\/vm\/StackChunk\")               \\\n@@ -95,0 +102,2 @@\n+  template(jdk_internal_vm_FillerObject,              \"jdk\/internal\/vm\/FillerObject\")             \\\n+                                                                                                  \\\n@@ -313,0 +322,4 @@\n+                                                                                                  \\\n+  template(jdk_internal_vm_annotation_ChangesCurrentThread_signature,  \"Ljdk\/internal\/vm\/annotation\/ChangesCurrentThread;\")  \\\n+  template(jdk_internal_vm_annotation_JvmtiMountTransition_signature,  \"Ljdk\/internal\/vm\/annotation\/JvmtiMountTransition;\")  \\\n+                                                                                                  \\\n@@ -341,0 +354,2 @@\n+  template(asFixedArity_name,                         \"asFixedArity\")                             \\\n+  template(asFixedArity_signature,                    \"()Ljava\/lang\/invoke\/MethodHandle;\")        \\\n@@ -354,3 +369,8 @@\n-  \/* Foreign API Support *\/                                                                                          \\\n-  template(jdk_internal_invoke_NativeEntryPoint,                 \"jdk\/internal\/invoke\/NativeEntryPoint\")           \\\n-  template(jdk_internal_invoke_NativeEntryPoint_signature,       \"Ljdk\/internal\/invoke\/NativeEntryPoint;\")         \\\n+  \/* Foreign API Support *\/                                                                       \\\n+  template(jdk_internal_foreign_abi_NativeEntryPoint,                \"jdk\/internal\/foreign\/abi\/NativeEntryPoint\") \\\n+  template(jdk_internal_foreign_abi_ABIDescriptor,                   \"jdk\/internal\/foreign\/abi\/ABIDescriptor\") \\\n+  template(jdk_internal_foreign_abi_VMStorage,                       \"jdk\/internal\/foreign\/abi\/VMStorage\") \\\n+  template(jdk_internal_foreign_abi_VMStorage_signature,             \"Ljdk\/internal\/foreign\/abi\/VMStorage;\") \\\n+  template(jdk_internal_foreign_abi_VMStorage_array_signature,       \"[Ljdk\/internal\/foreign\/abi\/VMStorage;\") \\\n+  template(jdk_internal_foreign_abi_VMStorage_array_array_signature, \"[[Ljdk\/internal\/foreign\/abi\/VMStorage;\") \\\n+  template(jdk_internal_foreign_abi_CallConv,                        \"jdk\/internal\/foreign\/abi\/UpcallLinker$CallRegs\") \\\n@@ -383,0 +403,1 @@\n+  template(interrupt_method_name,                     \"interrupt\")                                \\\n@@ -384,1 +405,0 @@\n-  template(add_method_name,                           \"add\")                                      \\\n@@ -387,1 +407,1 @@\n-  template(threads_name,                              \"threads\")                                  \\\n+  template(ngroups_name,                              \"ngroups\")                                  \\\n@@ -389,0 +409,2 @@\n+  template(nweaks_name,                               \"nweaks\")                                   \\\n+  template(weaks_name,                                \"weaks\")                                    \\\n@@ -390,3 +412,0 @@\n-  template(destroyed_name,                            \"destroyed\")                                \\\n-  template(nthreads_name,                             \"nthreads\")                                 \\\n-  template(ngroups_name,                              \"ngroups\")                                  \\\n@@ -400,0 +419,27 @@\n+  template(doYield_name,                              \"doYield\")                                  \\\n+  template(enter_name,                                \"enter\")                                    \\\n+  template(enterSpecial_name,                         \"enterSpecial\")                             \\\n+  template(onContinue_name,                           \"onContinue0\")                              \\\n+  template(getStacks_name,                            \"getStacks\")                                \\\n+  template(onPinned_name,                             \"onPinned0\")                                \\\n+  template(scope_name,                                \"scope\")                                    \\\n+  template(yieldInfo_name,                            \"yieldInfo\")                                \\\n+  template(tail_name,                                 \"tail\")                                     \\\n+  template(size_name,                                 \"size\")                                     \\\n+  template(argsize_name,                              \"argsize\")                                  \\\n+  template(mode_name,                                 \"mode\")                                     \\\n+  template(numFrames_name,                            \"numFrames\")                                \\\n+  template(numOops_name,                              \"numOops\")                                  \\\n+  template(stack_name,                                \"stack\")                                    \\\n+  template(maxSize_name,                              \"maxSize\")                                  \\\n+  template(reset_name,                                \"reset\")                                    \\\n+  template(done_name,                                 \"done\")                                     \\\n+  template(mounted_name,                              \"mounted\")                                  \\\n+  template(numInterpretedFrames_name,                 \"numInterpretedFrames\")                     \\\n+  template(jfrTraceId_name,                           \"jfrTraceId\")                               \\\n+  template(fp_name,                                   \"fp\")                                       \\\n+  template(sp_name,                                   \"sp\")                                       \\\n+  template(pc_name,                                   \"pc\")                                       \\\n+  template(cs_name,                                   \"cs\")                                       \\\n+  template(refStack_name,                             \"refStack\")                                 \\\n+  template(refSP_name,                                \"refSP\")                                    \\\n@@ -408,0 +454,1 @@\n+  template(bootLoader_name,                           \"bootLoader\")                               \\\n@@ -418,1 +465,1 @@\n-  template(wait_name,                                 \"wait\")                                     \\\n+  template(wait_name,                                 \"wait0\")                                    \\\n@@ -460,0 +507,1 @@\n+  template(cont_name,                                 \"cont\")                                     \\\n@@ -470,2 +518,0 @@\n-  template(fileToEncodedURL_name,                     \"fileToEncodedURL\")                         \\\n-  template(fileToEncodedURL_signature,                \"(Ljava\/io\/File;)Ljava\/net\/URL;\")           \\\n@@ -481,0 +527,1 @@\n+  template(jvmti_thread_state_name,                   \"jvmti_thread_state\")                       \\\n@@ -493,0 +540,2 @@\n+  template(jfr_epoch_name,                            \"jfr_epoch\")                                \\\n+  template(maxThawingSize_name,                       \"maxThawingSize\")                           \\\n@@ -513,0 +562,2 @@\n+  template(float_bool_signature,                      \"(F)Z\")                                     \\\n+  template(double_bool_signature,                     \"(D)Z\")                                     \\\n@@ -536,0 +587,5 @@\n+  template(runnable_signature,                        \"Ljava\/lang\/Runnable;\")                     \\\n+  template(continuation_signature,                    \"Ljdk\/internal\/vm\/Continuation;\")           \\\n+  template(continuationscope_signature,               \"Ljdk\/internal\/vm\/ContinuationScope;\")      \\\n+  template(stackchunk_signature,                      \"Ljdk\/internal\/vm\/StackChunk;\")             \\\n+  template(vthread_signature,                         \"Ljava\/lang\/VirtualThread;\")                \\\n@@ -543,0 +599,2 @@\n+  template(string_byte_array_signature,               \"(Ljava\/lang\/String;)[B\")                   \\\n+  template(string_bool_byte_array_signature,          \"(Ljava\/lang\/String;Z)[B\")                  \\\n@@ -569,0 +627,1 @@\n+  template(void_byte_array_signature,                 \"()[B\")                                                     \\\n@@ -570,0 +629,1 @@\n+  template(void_BuiltinClassLoader_signature,         \"()Ljdk\/internal\/loader\/BuiltinClassLoader;\")               \\\n@@ -582,0 +642,1 @@\n+  template(thread_fieldholder_signature,              \"Ljava\/lang\/Thread$FieldHolder;\")                           \\\n@@ -594,0 +655,1 @@\n+  template(weakreference_array_signature,             \"[Ljava\/lang\/ref\/WeakReference;\")                           \\\n@@ -691,1 +753,0 @@\n-  template(serializePropertiesToByteArray_signature,   \"()[B\")                                                    \\\n@@ -728,0 +789,5 @@\n+  \/* Thread.dump_to_file jcmd *\/                                                                                  \\\n+  template(jdk_internal_vm_ThreadDumper,           \"jdk\/internal\/vm\/ThreadDumper\")                                \\\n+  template(dumpThreads_name,                       \"dumpThreads\")                                                 \\\n+  template(dumpThreadsToJson_name,                 \"dumpThreadsToJson\")                                           \\\n+\n","filename":"src\/hotspot\/share\/classfile\/vmSymbols.hpp","additions":78,"deletions":12,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -81,1 +81,1 @@\n-CodeBlob::CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+CodeBlob::CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled) :\n@@ -96,0 +96,1 @@\n+  _is_compiled(compiled),\n@@ -109,1 +110,1 @@\n-CodeBlob::CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, CodeBuffer* cb \/*UNUSED*\/, int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments) :\n+CodeBlob::CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, CodeBuffer* cb \/*UNUSED*\/, int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled) :\n@@ -123,0 +124,1 @@\n+  _is_compiled(compiled),\n@@ -162,0 +164,12 @@\n+void RuntimeBlob::free(RuntimeBlob* blob) {\n+  assert(blob != NULL, \"caller must check for NULL\");\n+  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n+  blob->flush();\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+    CodeCache::free(blob);\n+  }\n+  \/\/ Track memory usage statistic after releasing CodeCache_lock\n+  MemoryService::track_code_cache_memory_usage();\n+}\n+\n@@ -184,1 +198,3 @@\n-  if (stub != NULL) {\n+  if (stub != NULL && (PrintStubCode ||\n+                       Forte::is_enabled() ||\n+                       JvmtiExport::should_post_dynamic_code_generated())) {\n@@ -201,1 +217,3 @@\n-    Forte::register_stub(stub_id, stub->code_begin(), stub->code_end());\n+    if (Forte::is_enabled()) {\n+      Forte::register_stub(stub_id, stub->code_begin(), stub->code_end());\n+    }\n@@ -214,1 +232,1 @@\n-const ImmutableOopMap* CodeBlob::oop_map_for_return_address(address return_address) {\n+const ImmutableOopMap* CodeBlob::oop_map_for_return_address(address return_address) const {\n@@ -277,9 +295,1 @@\n-  assert(blob != NULL, \"caller must check for NULL\");\n-  ThreadInVMfromUnknown __tiv;  \/\/ get to VM state in case we block on CodeCache_lock\n-  blob->flush();\n-  {\n-    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-    CodeCache::free((RuntimeBlob*)blob);\n-  }\n-  \/\/ Track memory usage statistic after releasing CodeCache_lock\n-  MemoryService::track_code_cache_memory_usage();\n+  RuntimeBlob::free(blob);\n@@ -435,0 +445,1 @@\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(RuntimeStub));\n@@ -438,1 +449,0 @@\n-    unsigned int size = CodeBlob::allocation_size(cb, sizeof(RuntimeStub));\n@@ -494,0 +504,1 @@\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(DeoptimizationBlob));\n@@ -497,1 +508,0 @@\n-    unsigned int size = CodeBlob::allocation_size(cb, sizeof(DeoptimizationBlob));\n@@ -533,0 +543,1 @@\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(UncommonTrapBlob));\n@@ -536,1 +547,0 @@\n-    unsigned int size = CodeBlob::allocation_size(cb, sizeof(UncommonTrapBlob));\n@@ -569,0 +579,1 @@\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(ExceptionBlob));\n@@ -572,1 +583,0 @@\n-    unsigned int size = CodeBlob::allocation_size(cb, sizeof(ExceptionBlob));\n@@ -604,0 +614,1 @@\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(SafepointBlob));\n@@ -607,1 +618,0 @@\n-    unsigned int size = CodeBlob::allocation_size(cb, sizeof(SafepointBlob));\n@@ -749,1 +759,1 @@\n-\/\/ Implementation of OptimizedEntryBlob\n+\/\/ Implementation of UpcallStub\n@@ -751,3 +761,5 @@\n-OptimizedEntryBlob::OptimizedEntryBlob(const char* name, int size, CodeBuffer* cb, intptr_t exception_handler_offset,\n-                                       jobject receiver, ByteSize frame_data_offset) :\n-  BufferBlob(name, sizeof(OptimizedEntryBlob), size, cb),\n+UpcallStub::UpcallStub(const char* name, CodeBuffer* cb, int size,\n+                       intptr_t exception_handler_offset,\n+                       jobject receiver, ByteSize frame_data_offset) :\n+  RuntimeBlob(name, cb, sizeof(UpcallStub), size, CodeOffsets::frame_never_safe, 0 \/* no frame size *\/,\n+              \/* oop maps = *\/ nullptr, \/* caller must gc arguments = *\/ false),\n@@ -760,2 +772,7 @@\n-OptimizedEntryBlob* OptimizedEntryBlob::create(const char* name, CodeBuffer* cb, intptr_t exception_handler_offset,\n-                                               jobject receiver, ByteSize frame_data_offset) {\n+void* UpcallStub::operator new(size_t s, unsigned size) throw() {\n+  return CodeCache::allocate(size, CodeBlobType::NonNMethod);\n+}\n+\n+UpcallStub* UpcallStub::create(const char* name, CodeBuffer* cb,\n+                               intptr_t exception_handler_offset,\n+                               jobject receiver, ByteSize frame_data_offset) {\n@@ -764,2 +781,2 @@\n-  OptimizedEntryBlob* blob = nullptr;\n-  unsigned int size = CodeBlob::allocation_size(cb, sizeof(OptimizedEntryBlob));\n+  UpcallStub* blob = nullptr;\n+  unsigned int size = CodeBlob::allocation_size(cb, sizeof(UpcallStub));\n@@ -768,1 +785,2 @@\n-    blob = new (size) OptimizedEntryBlob(name, size, cb, exception_handler_offset, receiver, frame_data_offset);\n+    blob = new (size) UpcallStub(name, cb, size,\n+                                         exception_handler_offset, receiver, frame_data_offset);\n@@ -773,0 +791,2 @@\n+  trace_new_stub(blob, \"UpcallStub\");\n+\n@@ -776,1 +796,1 @@\n-void OptimizedEntryBlob::oops_do(OopClosure* f, const frame& frame) {\n+void UpcallStub::oops_do(OopClosure* f, const frame& frame) {\n@@ -780,1 +800,1 @@\n-JavaFrameAnchor* OptimizedEntryBlob::jfa_for_frame(const frame& frame) const {\n+JavaFrameAnchor* UpcallStub::jfa_for_frame(const frame& frame) const {\n@@ -783,0 +803,24 @@\n+\n+void UpcallStub::free(UpcallStub* blob) {\n+  assert(blob != nullptr, \"caller must check for NULL\");\n+  JNIHandles::destroy_global(blob->receiver());\n+  RuntimeBlob::free(blob);\n+}\n+\n+void UpcallStub::preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) {\n+  ShouldNotReachHere(); \/\/ caller should never have to gc arguments\n+}\n+\n+\/\/ Misc.\n+void UpcallStub::verify() {\n+  \/\/ unimplemented\n+}\n+\n+void UpcallStub::print_on(outputStream* st) const {\n+  RuntimeBlob::print_on(st);\n+  print_value_on(st);\n+}\n+\n+void UpcallStub::print_value_on(outputStream* st) const {\n+  st->print_cr(\"UpcallStub (\" INTPTR_FORMAT  \") used for %s\", p2i(this), name());\n+}\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":75,"deletions":31,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,1 @@\n+#include \"compiler\/oopMap.hpp\"\n@@ -63,1 +64,0 @@\n-\/\/    OptimizedEntryBlob : Used for upcalls from native code\n@@ -70,0 +70,1 @@\n+\/\/   UpcallStub  : Used for upcalls from native code\n@@ -81,2 +82,3 @@\n-class OptimizedEntryBlob; \/\/ for as_optimized_entry_blob()\n-class JavaFrameAnchor; \/\/ for OptimizedEntryBlob::jfa_for_frame\n+class UpcallStub; \/\/ for as_upcall_stub()\n+class RuntimeStub; \/\/ for as_runtime_stub()\n+class JavaFrameAnchor; \/\/ for UpcallStub::jfa_for_frame\n@@ -112,0 +114,2 @@\n+  bool                _is_compiled;\n+\n@@ -125,3 +129,6 @@\n-  CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments);\n-  CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, CodeBuffer* cb, int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments);\n-\n+  CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset,\n+           int frame_size, ImmutableOopMapSet* oop_maps,\n+           bool caller_must_gc_arguments, bool compiled = false);\n+  CodeBlob(const char* name, CompilerType type, const CodeBlobLayout& layout, CodeBuffer* cb, int frame_complete_offset,\n+           int frame_size, OopMapSet* oop_maps,\n+           bool caller_must_gc_arguments, bool compiled = false);\n@@ -150,1 +157,3 @@\n-  virtual bool is_compiled() const                    { return false; }\n+  virtual bool is_upcall_stub() const                 { return false; }\n+  bool is_compiled() const                            { return _is_compiled; }\n+  const bool* is_compiled_addr() const                { return &_is_compiled; }\n@@ -152,1 +161,0 @@\n-  virtual bool is_optimized_entry_blob() const                  { return false; }\n@@ -166,1 +174,2 @@\n-  OptimizedEntryBlob* as_optimized_entry_blob() const             { assert(is_optimized_entry_blob(), \"must be entry blob\"); return (OptimizedEntryBlob*) this; }\n+  UpcallStub* as_upcall_stub() const           { assert(is_upcall_stub(), \"must be upcall stub\"); return (UpcallStub*) this; }\n+  RuntimeStub* as_runtime_stub() const         { assert(is_runtime_stub(), \"must be runtime blob\"); return (RuntimeStub*) this; }\n@@ -221,1 +230,3 @@\n-  const ImmutableOopMap* oop_map_for_return_address(address return_address);\n+\n+  const ImmutableOopMap* oop_map_for_slot(int slot, address return_address) const;\n+  const ImmutableOopMap* oop_map_for_return_address(address return_address) const;\n@@ -375,0 +386,2 @@\n+  static void free(RuntimeBlob* blob);\n+\n@@ -401,1 +414,1 @@\n-  friend class OptimizedEntryBlob;\n+  friend class UpcallStub;\n@@ -477,1 +490,1 @@\n-  MethodHandlesAdapterBlob(int size)                 : BufferBlob(\"MethodHandles adapters\", size) {}\n+  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", size) {}\n@@ -544,0 +557,2 @@\n+  static void free(RuntimeStub* stub) { RuntimeBlob::free(stub); }\n+\n@@ -772,1 +787,1 @@\n-class ProgrammableUpcallHandler;\n+class UpcallLinker;\n@@ -774,2 +789,3 @@\n-class OptimizedEntryBlob: public BufferBlob {\n-  friend class ProgrammableUpcallHandler;\n+\/\/ A (Panama) upcall stub. Not used by JNI.\n+class UpcallStub: public RuntimeBlob {\n+  friend class UpcallLinker;\n@@ -781,1 +797,2 @@\n-  OptimizedEntryBlob(const char* name, int size, CodeBuffer* cb, intptr_t exception_handler_offset,\n+  UpcallStub(const char* name, CodeBuffer* cb, int size,\n+                     intptr_t exception_handler_offset,\n@@ -784,0 +801,6 @@\n+  \/\/ This ordinary operator delete is needed even though not used, so the\n+  \/\/ below two-argument operator delete will be treated as a placement\n+  \/\/ delete rather than an ordinary sized delete; see C++14 3.7.4.2\/p2.\n+  void operator delete(void* p);\n+  void* operator new(size_t s, unsigned size) throw();\n+\n@@ -789,1 +812,0 @@\n-    bool should_detach;\n@@ -796,3 +818,5 @@\n-  static OptimizedEntryBlob* create(const char* name, CodeBuffer* cb,\n-                                    intptr_t exception_handler_offset, jobject receiver,\n-                                    ByteSize frame_data_offset);\n+  static UpcallStub* create(const char* name, CodeBuffer* cb,\n+                            intptr_t exception_handler_offset,\n+                            jobject receiver, ByteSize frame_data_offset);\n+\n+  static void free(UpcallStub* blob);\n@@ -805,0 +829,4 @@\n+  \/\/ Typing\n+  virtual bool is_upcall_stub() const override { return true; }\n+\n+  \/\/ GC\/Verification support\n@@ -806,0 +834,3 @@\n+  virtual void preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) override;\n+  virtual bool is_alive() const override { return true; }\n+  virtual void verify() override;\n@@ -807,2 +838,3 @@\n-  \/\/ Typing\n-  virtual bool is_optimized_entry_blob() const override { return true; }\n+  \/\/ Misc.\n+  virtual void print_on(outputStream* st) const override;\n+  virtual void print_value_on(outputStream* st) const override;\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":55,"deletions":23,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,0 +42,1 @@\n+#include \"runtime\/continuationEntry.hpp\"\n@@ -424,1 +425,1 @@\n-  \/\/ In both of these cases the only thing being modifed is the jump\/call target and these\n+  \/\/ In both of these cases the only thing being modified is the jump\/call target and these\n@@ -671,0 +672,7 @@\n+void CompiledStaticCall::compute_entry_for_continuation_entry(const methodHandle& m, StaticCallInfo& info) {\n+  if (ContinuationEntry::is_interpreted_call(instruction_address())) {\n+    info._to_interpreter = true;\n+    info._entry = m()->get_c2i_entry();\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/code\/compiledIC.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -248,1 +248,1 @@\n-  address end_of_call() { return  _call->return_address(); }\n+  address end_of_call() const { return  _call->return_address(); }\n@@ -348,0 +348,1 @@\n+  void compute_entry_for_continuation_entry(const methodHandle& m, StaticCallInfo& info);\n@@ -367,0 +368,1 @@\n+  virtual address end_of_call() const = 0;\n@@ -412,0 +414,1 @@\n+  address end_of_call() const { return _call->return_address(); }\n","filename":"src\/hotspot\/share\/code\/compiledIC.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,0 +43,1 @@\n+#include \"oops\/weakHandle.inline.hpp\"\n@@ -46,0 +47,2 @@\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n@@ -52,2 +55,2 @@\n-                               bool caller_must_gc_arguments)\n-  : CodeBlob(name, type, layout, frame_complete_offset, frame_size, oop_maps, caller_must_gc_arguments),\n+                               bool caller_must_gc_arguments, bool compiled)\n+  : CodeBlob(name, type, layout, frame_complete_offset, frame_size, oop_maps, caller_must_gc_arguments, compiled),\n@@ -63,1 +66,1 @@\n-                               OopMapSet* oop_maps, bool caller_must_gc_arguments)\n+                               OopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled)\n@@ -65,1 +68,1 @@\n-             frame_complete_offset, frame_size, oop_maps, caller_must_gc_arguments),\n+             frame_complete_offset, frame_size, oop_maps, caller_must_gc_arguments, compiled),\n@@ -83,0 +86,1 @@\n+  _has_monitors               = 0;\n@@ -117,0 +121,1 @@\n+  \/\/ assert(can_be_deoptimized(), \"\"); \/\/ in some places we check before marking, in others not.\n@@ -119,1 +124,3 @@\n-  _mark_for_deoptimization_status = (inc_recompile_counts ? deoptimize : deoptimize_noupdate);\n+  if (_mark_for_deoptimization_status != deoptimize_done) { \/\/ can't go backwards\n+     _mark_for_deoptimization_status = (inc_recompile_counts ? deoptimize : deoptimize_noupdate);\n+  }\n@@ -169,1 +176,1 @@\n-  \/\/ Also note that concurent readers will walk through Klass* pointers that are not\n+  \/\/ Also note that concurrent readers will walk through Klass* pointers that are not\n@@ -322,1 +329,1 @@\n-  assert(BarrierSet::barrier_set()->barrier_set_nmethod() == NULL, \"Not safe oop scan\");\n+  \/\/ assert(BarrierSet::barrier_set()->barrier_set_nmethod() == NULL, \"Not safe oop scan\");\n@@ -360,1 +367,11 @@\n-  if (method() != NULL && !method()->is_native()) {\n+  if (method() == NULL) {\n+    return;\n+  }\n+\n+  \/\/ handle the case of an anchor explicitly set in continuation code that doesn't have a callee\n+  JavaThread* thread = reg_map->thread();\n+  if (thread->has_last_Java_frame() && fr.sp() == thread->last_Java_sp()) {\n+    return;\n+  }\n+\n+  if (!method()->is_native()) {\n@@ -362,0 +379,3 @@\n+    bool has_receiver, has_appendix;\n+    Symbol* signature;\n+\n@@ -365,3 +385,0 @@\n-    bool has_receiver = false;\n-    bool has_appendix = false;\n-    Symbol* signature = NULL;\n@@ -375,1 +392,1 @@\n-      if (this->is_compiled_by_c2() && callee->has_scalarized_args()) {\n+      if (is_compiled_by_c2() && callee->has_scalarized_args()) {\n@@ -385,1 +402,1 @@\n-      if (ssd.is_optimized_linkToNative()) return; \/\/ call was replaced\n+\n@@ -389,1 +406,1 @@\n-      signature = call.signature();\n+      signature    = call.signature();\n@@ -393,0 +410,4 @@\n+  } else if (method()->is_continuation_enter_intrinsic()) {\n+    \/\/ This method only calls Continuation.enter()\n+    Symbol* signature = vmSymbols::continuationEnter_signature();\n+    fr.oops_compiled_arguments_do(signature, false, false, reg_map, f);\n@@ -472,1 +493,1 @@\n-    \/\/ The only exception is compiledICHolder metdata which may\n+    \/\/ The only exception is compiledICHolder metadata which may\n@@ -604,1 +625,1 @@\n-    if (nm != NULL) {\n+    if (nm != NULL && bs_nm->is_armed(nm)) {\n@@ -621,0 +642,4 @@\n+    if (!clean_all) {\n+      MutexLocker ml(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+      CodeCache::Sweep::end();\n+    }\n@@ -622,0 +647,4 @@\n+    if (!clean_all) {\n+      MutexLocker ml(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n+      CodeCache::Sweep::begin();\n+    }\n@@ -625,0 +654,4 @@\n+address* CompiledMethod::orig_pc_addr(const frame* fr) {\n+  return (address*) ((address)fr->unextended_sp() + orig_pc_offset());\n+}\n+\n","filename":"src\/hotspot\/share\/code\/compiledMethod.cpp","additions":50,"deletions":17,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -147,1 +147,1 @@\n-  enum MarkForDeoptimizationStatus {\n+  enum MarkForDeoptimizationStatus : u1 {\n@@ -150,1 +150,2 @@\n-    deoptimize_noupdate\n+    deoptimize_noupdate,\n+    deoptimize_done\n@@ -159,0 +160,1 @@\n+  unsigned int _has_monitors:1;              \/\/ Fastpath monitor detection for continuations\n@@ -175,0 +177,1 @@\n+\n@@ -176,2 +179,2 @@\n-  CompiledMethod(Method* method, const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments);\n-  CompiledMethod(Method* method, const char* name, CompilerType type, int size, int header_size, CodeBuffer* cb, int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments);\n+  CompiledMethod(Method* method, const char* name, CompilerType type, const CodeBlobLayout& layout, int frame_complete_offset, int frame_size, ImmutableOopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled);\n+  CompiledMethod(Method* method, const char* name, CompilerType type, int size, int header_size, CodeBuffer* cb, int frame_complete_offset, int frame_size, OopMapSet* oop_maps, bool caller_must_gc_arguments, bool compiled);\n@@ -183,2 +186,0 @@\n-  virtual bool is_compiled() const                { return true; }\n-\n@@ -193,0 +194,3 @@\n+  bool  has_monitors() const                      { return _has_monitors; }\n+  void  set_has_monitors(bool z)                  { _has_monitors = z; }\n+\n@@ -257,0 +261,5 @@\n+  bool  has_been_deoptimized() const { return _mark_for_deoptimization_status == deoptimize_done; }\n+  void  mark_deoptimized() { _mark_for_deoptimization_status = deoptimize_done; }\n+\n+  virtual void  make_deoptimized() { assert(false, \"not supported\"); };\n+\n@@ -261,1 +270,2 @@\n-    return _mark_for_deoptimization_status != deoptimize_noupdate;\n+    return _mark_for_deoptimization_status != deoptimize_noupdate &&\n+           _mark_for_deoptimization_status != deoptimize_done;\n@@ -312,1 +322,0 @@\n-  virtual void    set_original_pc(const frame* fr, address pc) = 0;\n@@ -334,1 +343,1 @@\n-  virtual address get_original_pc(const frame* fr) = 0;\n+  address* deopt_handler_begin_addr() { return &_deopt_handler_begin; }\n@@ -341,0 +350,10 @@\n+  \/\/ Accessor\/mutator for the original pc of a frame before a frame was deopted.\n+  address get_original_pc(const frame* fr) { return *orig_pc_addr(fr); }\n+  void    set_original_pc(const frame* fr, address pc) { *orig_pc_addr(fr) = pc; }\n+\n+  virtual int orig_pc_offset() = 0;\n+\n+private:\n+  address* orig_pc_addr(const frame* fr);\n+\n+public:\n","filename":"src\/hotspot\/share\/code\/compiledMethod.hpp","additions":29,"deletions":10,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -35,1 +36,0 @@\n-#include \"runtime\/thread.hpp\"\n","filename":"src\/hotspot\/share\/code\/debugInfo.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,2 +32,1 @@\n-#include \"runtime\/stackValue.hpp\"\n-#include \"runtime\/thread.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -45,0 +44,1 @@\n+class ConstantOopWriteValue;\n@@ -66,0 +66,5 @@\n+  ConstantOopWriteValue* as_ConstantOopWriteValue() {\n+    assert(is_constant_oop(), \"must be\");\n+    return (ConstantOopWriteValue*) this;\n+  }\n+\n","filename":"src\/hotspot\/share\/code\/debugInfo.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -291,1 +291,0 @@\n-                                              bool        is_optimized_linkToNative,\n@@ -311,1 +310,0 @@\n-  last_pd->set_is_optimized_linkToNative(is_optimized_linkToNative);\n@@ -317,1 +315,1 @@\n-  \/\/ serialize sender stream offest\n+  \/\/ serialize sender stream offset\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -109,1 +109,0 @@\n-                      bool        is_optimized_linkToNative = false,\n@@ -126,1 +125,1 @@\n-  \/\/ helper fuctions for describe_scope to enable sharing\n+  \/\/ helper functions for describe_scope to enable sharing\n","filename":"src\/hotspot\/share\/code\/debugInfoRec.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,3 @@\n-#include \"compiler\/oopMap.hpp\"\n+#include \"compiler\/oopMap.inline.hpp\"\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -54,0 +56,1 @@\n+#include \"oops\/weakHandle.inline.hpp\"\n@@ -57,0 +60,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -425,4 +429,0 @@\n-address* nmethod::orig_pc_addr(const frame* fr) {\n-  return (address*) ((address)fr->unextended_sp() + _orig_pc_offset);\n-}\n-\n@@ -431,1 +431,6 @@\n-  if (method() != NULL && is_native_method())  return \"c2n\";\n+  if (method() != NULL && is_native_method()) {\n+    if (method()->is_continuation_enter_intrinsic()) {\n+      return \"cnt\";\n+    }\n+    return \"c2n\";\n+  }\n@@ -463,1 +468,2 @@\n-  OopMapSet* oop_maps) {\n+  OopMapSet* oop_maps,\n+  int exception_handler) {\n@@ -467,0 +473,1 @@\n+  int native_nmethod_size = CodeBlob::allocation_size(code_buffer, sizeof(nmethod));\n@@ -469,1 +476,0 @@\n-    int native_nmethod_size = CodeBlob::allocation_size(code_buffer, sizeof(nmethod));\n@@ -474,0 +480,3 @@\n+    if (exception_handler != -1) {\n+      offsets.set_value(CodeOffsets::Exceptions, exception_handler);\n+    }\n@@ -505,2 +514,1 @@\n-  int comp_level,\n-  const GrowableArrayView<RuntimeStub*>& native_invokers\n+  int comp_level\n@@ -520,2 +528,1 @@\n-  { MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-    int jvmci_data_size = !compiler->is_jvmci() ? 0 : JVMCINMethodData::compute_size(nmethod_mirror_name);\n+  int jvmci_data_size = !compiler->is_jvmci() ? 0 : JVMCINMethodData::compute_size(nmethod_mirror_name);\n@@ -524,7 +531,6 @@\n-    int nmethod_size =\n-      CodeBlob::allocation_size(code_buffer, sizeof(nmethod))\n-      + adjust_pcs_size(debug_info->pcs_size())\n-      + align_up((int)dependencies->size_in_bytes(), oopSize)\n-      + align_up(checked_cast<int>(native_invokers.data_size_in_bytes()), oopSize)\n-      + align_up(handler_table->size_in_bytes()    , oopSize)\n-      + align_up(nul_chk_table->size_in_bytes()    , oopSize)\n+  int nmethod_size =\n+    CodeBlob::allocation_size(code_buffer, sizeof(nmethod))\n+    + adjust_pcs_size(debug_info->pcs_size())\n+    + align_up((int)dependencies->size_in_bytes(), oopSize)\n+    + align_up(handler_table->size_in_bytes()    , oopSize)\n+    + align_up(nul_chk_table->size_in_bytes()    , oopSize)\n@@ -532,2 +538,2 @@\n-      + align_up(speculations_len                  , oopSize)\n-      + align_up(jvmci_data_size                   , oopSize)\n+    + align_up(speculations_len                  , oopSize)\n+    + align_up(jvmci_data_size                   , oopSize)\n@@ -535,1 +541,3 @@\n-      + align_up(debug_info->data_size()           , oopSize);\n+    + align_up(debug_info->data_size()           , oopSize);\n+  {\n+    MutexLocker mu(CodeCache_lock, Mutex::_no_safepoint_check_flag);\n@@ -544,2 +552,1 @@\n-            comp_level,\n-            native_invokers\n+            comp_level\n@@ -606,1 +613,1 @@\n-  : CompiledMethod(method, \"native nmethod\", type, nmethod_size, sizeof(nmethod), code_buffer, offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false),\n+  : CompiledMethod(method, \"native nmethod\", type, nmethod_size, sizeof(nmethod), code_buffer, offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false, true),\n@@ -625,0 +632,1 @@\n+    _gc_epoch                = Continuations::gc_epoch();\n@@ -627,1 +635,1 @@\n-    _stub_offset             = data_offset();\n+    _stub_offset             = content_offset()      + code_buffer->total_offset_of(code_buffer->stubs());\n@@ -629,2 +637,2 @@\n-    _metadata_offset         = _oops_offset         + align_up(code_buffer->total_oop_size(), oopSize);\n-    scopes_data_offset       = _metadata_offset     + align_up(code_buffer->total_metadata_size(), wordSize);\n+    _metadata_offset         = _oops_offset          + align_up(code_buffer->total_oop_size(), oopSize);\n+    scopes_data_offset       = _metadata_offset      + align_up(code_buffer->total_metadata_size(), wordSize);\n@@ -633,2 +641,1 @@\n-    _native_invokers_offset     = _dependencies_offset;\n-    _handler_table_offset    = _native_invokers_offset;\n+    _handler_table_offset    = _dependencies_offset;\n@@ -658,0 +665,2 @@\n+    _exception_offset        = code_offset()         + offsets->value(CodeOffsets::Exceptions);\n+\n@@ -671,0 +680,2 @@\n+\n+    finalize_relocations();\n@@ -736,2 +747,1 @@\n-  int comp_level,\n-  const GrowableArrayView<RuntimeStub*>& native_invokers\n+  int comp_level\n@@ -744,1 +754,1 @@\n-  : CompiledMethod(method, \"nmethod\", type, nmethod_size, sizeof(nmethod), code_buffer, offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false),\n+  : CompiledMethod(method, \"nmethod\", type, nmethod_size, sizeof(nmethod), code_buffer, offsets->value(CodeOffsets::Frame_Complete), frame_size, oop_maps, false, true),\n@@ -763,0 +773,1 @@\n+    _gc_epoch                = Continuations::gc_epoch();\n@@ -814,2 +825,1 @@\n-    _native_invokers_offset  = _dependencies_offset  + align_up((int)dependencies->size_in_bytes(), oopSize);\n-    _handler_table_offset    = _native_invokers_offset + align_up(checked_cast<int>(native_invokers.data_size_in_bytes()), oopSize);\n+    _handler_table_offset    = _dependencies_offset  + align_up((int)dependencies->size_in_bytes(), oopSize);\n@@ -840,4 +850,0 @@\n-    if (native_invokers.is_nonempty()) { \/\/ can not get address of zero-length array\n-      \/\/ Copy native stubs\n-      memcpy(native_invokers_begin(), native_invokers.adr_at(0), native_invokers.data_size_in_bytes());\n-    }\n@@ -851,0 +857,2 @@\n+    finalize_relocations();\n+\n@@ -1008,4 +1016,0 @@\n-    if (printmethod && native_invokers_begin() < native_invokers_end()) {\n-      print_native_invokers();\n-      tty->print_cr(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \");\n-    }\n@@ -1072,6 +1076,0 @@\n-void nmethod::free_native_invokers() {\n-  for (RuntimeStub** it = native_invokers_begin(); it < native_invokers_end(); it++) {\n-    CodeCache::free(*it);\n-  }\n-}\n-\n@@ -1097,0 +1095,78 @@\n+static void install_post_call_nop_displacement(nmethod* nm, address pc) {\n+  NativePostCallNop* nop = nativePostCallNop_at((address) pc);\n+  intptr_t cbaddr = (intptr_t) nm;\n+  intptr_t offset = ((intptr_t) pc) - cbaddr;\n+\n+  int oopmap_slot = nm->oop_maps()->find_slot_for_offset((intptr_t) pc - (intptr_t) nm->code_begin());\n+  if (oopmap_slot < 0) { \/\/ this can happen at asynchronous (non-safepoint) stackwalks\n+    log_debug(codecache)(\"failed to find oopmap for cb: \" INTPTR_FORMAT \" offset: %d\", cbaddr, (int) offset);\n+  } else if (((oopmap_slot & 0xff) == oopmap_slot) && ((offset & 0xffffff) == offset)) {\n+    jint value = (oopmap_slot << 24) | (jint) offset;\n+    nop->patch(value);\n+  } else {\n+    log_debug(codecache)(\"failed to encode %d %d\", oopmap_slot, (int) offset);\n+  }\n+}\n+\n+void nmethod::finalize_relocations() {\n+  NoSafepointVerifier nsv;\n+\n+  \/\/ Make sure that post call nops fill in nmethod offsets eagerly so\n+  \/\/ we don't have to race with deoptimization\n+  RelocIterator iter(this);\n+  while (iter.next()) {\n+    if (iter.type() == relocInfo::post_call_nop_type) {\n+      post_call_nop_Relocation* const reloc = iter.post_call_nop_reloc();\n+      address pc = reloc->addr();\n+      install_post_call_nop_displacement(this, pc);\n+    }\n+  }\n+}\n+\n+void nmethod::make_deoptimized() {\n+  if (!Continuations::enabled()) {\n+    return;\n+  }\n+\n+  assert(method() == NULL || can_be_deoptimized(), \"\");\n+  assert(!is_zombie(), \"\");\n+\n+  CompiledICLocker ml(this);\n+  assert(CompiledICLocker::is_safe(this), \"mt unsafe call\");\n+  ResourceMark rm;\n+  RelocIterator iter(this, oops_reloc_begin());\n+\n+  while (iter.next()) {\n+\n+    switch (iter.type()) {\n+      case relocInfo::virtual_call_type:\n+      case relocInfo::opt_virtual_call_type: {\n+        CompiledIC *ic = CompiledIC_at(&iter);\n+        address pc = ic->end_of_call();\n+        NativePostCallNop* nop = nativePostCallNop_at(pc);\n+        if (nop != NULL) {\n+          nop->make_deopt();\n+        }\n+        assert(NativeDeoptInstruction::is_deopt_at(pc), \"check\");\n+        break;\n+      }\n+      case relocInfo::static_call_type: {\n+        CompiledStaticCall *csc = compiledStaticCall_at(iter.reloc());\n+        address pc = csc->end_of_call();\n+        NativePostCallNop* nop = nativePostCallNop_at(pc);\n+        \/\/tty->print_cr(\" - static pc %p\", pc);\n+        if (nop != NULL) {\n+          nop->make_deopt();\n+        }\n+        \/\/ We can't assert here, there are some calls to stubs \/ runtime\n+        \/\/ that have reloc data and doesn't have a post call NOP.\n+        \/\/assert(NativeDeoptInstruction::is_deopt_at(pc), \"check\");\n+        break;\n+      }\n+      default:\n+        break;\n+    }\n+  }\n+  \/\/ Don't deopt this again.\n+  mark_deoptimized();\n+}\n@@ -1147,0 +1223,15 @@\n+void nmethod::mark_as_maybe_on_continuation() {\n+  assert(is_alive(), \"Must be an alive method\");\n+  _gc_epoch = Continuations::gc_epoch();\n+}\n+\n+bool nmethod::is_maybe_on_continuation_stack() {\n+  if (!Continuations::enabled()) {\n+    return false;\n+  }\n+\n+  \/\/ If the condition below is true, it means that the nmethod was found to\n+  \/\/ be alive the previous completed marking cycle.\n+  return _gc_epoch >= Continuations::previous_completed_gc_marking_cycle();\n+}\n+\n@@ -1165,1 +1256,1 @@\n-  return stack_traversal_mark() + 1 < NMethodSweeper::traversal_count() &&\n+  return stack_traversal_mark()+1 < NMethodSweeper::traversal_count() && !is_maybe_on_continuation_stack() &&\n@@ -1858,0 +1949,4 @@\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != NULL) {\n+      bs_nm->disarm(this);\n+    }\n@@ -1863,1 +1958,1 @@\n-  assert(allow_dead || is_alive(), \"should not call follow on dead nmethod\");\n+  assert(allow_dead || is_alive(), \"should not call follow on dead nmethod: %d\", _state);\n@@ -1892,0 +1987,14 @@\n+void nmethod::follow_nmethod(OopIterateClosure* cl) {\n+  \/\/ Process oops in the nmethod\n+  oops_do(cl);\n+\n+  \/\/ CodeCache sweeper support\n+  mark_as_maybe_on_continuation();\n+\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  bs_nm->disarm(this);\n+\n+  \/\/ There's an assumption made that this function is not used by GCs that\n+  \/\/ relocate objects, and therefore we don't call fix_oop_relocations.\n+}\n+\n@@ -2727,8 +2836,0 @@\n-void nmethod::print_native_invokers() {\n-  ResourceMark m;       \/\/ in case methods get printed via debugger\n-  tty->print_cr(\"Native invokers:\");\n-  for (RuntimeStub** itt = native_invokers_begin(); itt < native_invokers_end(); itt++) {\n-    (*itt)->print_on(tty);\n-  }\n-}\n-\n@@ -2915,1 +3016,1 @@\n-  \/\/ The following stati are defined\/supported:\n+  \/\/ The following status values are defined\/supported:\n@@ -3142,1 +3243,1 @@\n-\/\/ Return a the last scope in (begin..end]\n+\/\/ Return the last scope in (begin..end]\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":163,"deletions":62,"binary":false,"changes":225,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+class OopIterateClosure;\n@@ -77,0 +78,2 @@\n+  uint64_t  _gc_epoch;\n+\n@@ -133,1 +136,1 @@\n-  \/\/ \"|\" describes the concatentation of bits in _oops_do_mark_link.\n+  \/\/ \"|\" describes the concatenation of bits in _oops_do_mark_link.\n@@ -214,1 +217,0 @@\n-  int _native_invokers_offset;\n@@ -316,2 +318,1 @@\n-          int comp_level,\n-          const GrowableArrayView<RuntimeStub*>& native_invokers\n+          int comp_level\n@@ -341,1 +342,1 @@\n-  \/\/ Initailize fields to their default values\n+  \/\/ Initialize fields to their default values\n@@ -365,2 +366,1 @@\n-                              int comp_level,\n-                              const GrowableArrayView<RuntimeStub*>& native_invokers = GrowableArrayView<RuntimeStub*>::EMPTY\n+                              int comp_level\n@@ -392,1 +392,2 @@\n-                                     OopMapSet* oop_maps);\n+                                     OopMapSet* oop_maps,\n+                                     int exception_handler = -1);\n@@ -415,3 +416,1 @@\n-  address dependencies_end      () const          { return           header_begin() + _native_invokers_offset ; }\n-  RuntimeStub** native_invokers_begin() const     { return (RuntimeStub**)(header_begin() + _native_invokers_offset) ; }\n-  RuntimeStub** native_invokers_end  () const     { return (RuntimeStub**)(header_begin() + _handler_table_offset); }\n+  address dependencies_end      () const          { return           header_begin() + _handler_table_offset ; }\n@@ -536,2 +535,0 @@\n-  void free_native_invokers();\n-\n@@ -576,0 +573,2 @@\n+  void mark_as_maybe_on_continuation();\n+  bool is_maybe_on_continuation_stack();\n@@ -605,0 +604,3 @@\n+  \/\/ Loom support for following nmethods on the stack\n+  void follow_nmethod(OopIterateClosure* cl);\n+\n@@ -642,3 +644,1 @@\n-  \/\/ Accessor\/mutator for the original pc of a frame before a frame was deopted.\n-  address get_original_pc(const frame* fr) { return *orig_pc_addr(fr); }\n-  void    set_original_pc(const frame* fr, address pc) { *orig_pc_addr(fr) = pc; }\n+  int orig_pc_offset() { return _orig_pc_offset; }\n@@ -674,1 +674,0 @@\n-  void print_native_invokers();\n@@ -764,0 +763,3 @@\n+\n+  virtual void  make_deoptimized();\n+  void finalize_relocations();\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":20,"deletions":18,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,2 +48,1 @@\n-    PCDESC_is_optimized_linkToNative = 1 << 6,\n-    PCDESC_return_scalarized         = 1 << 7\n+    PCDESC_return_scalarized         = 1 << 6\n@@ -93,3 +92,0 @@\n-  bool     is_optimized_linkToNative()     const { return (_flags & PCDESC_is_optimized_linkToNative) != 0;     }\n-  void set_is_optimized_linkToNative(bool z)     { set_flag(PCDESC_is_optimized_linkToNative, z); }\n-\n","filename":"src\/hotspot\/share\/code\/pcDesc.hpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -42,1 +42,0 @@\n-  bool _is_optimized_linkToNative;\n@@ -49,1 +48,0 @@\n-    _is_optimized_linkToNative = pc_desc->is_optimized_linkToNative();\n@@ -58,1 +56,0 @@\n-  bool is_optimized_linkToNative() { return _is_optimized_linkToNative; }\n","filename":"src\/hotspot\/share\/code\/scopeDesc.hpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n@@ -67,0 +67,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -425,1 +426,1 @@\n-CompileTask* CompileQueue::get() {\n+CompileTask* CompileQueue::get(CompilerThread* thread) {\n@@ -443,0 +444,9 @@\n+    AbstractCompiler* compiler = thread->compiler();\n+    guarantee(compiler != nullptr, \"Compiler object must exist\");\n+    compiler->on_empty_queue(this, thread);\n+    if (_first != nullptr) {\n+      \/\/ The call to on_empty_queue may have temporarily unlocked the MCQ lock\n+      \/\/ so check again whether any tasks were added to the queue.\n+      break;\n+    }\n+\n@@ -1342,1 +1352,1 @@\n-  \/\/ Do nothing if compilebroker is not initalized or compiles are submitted on level none\n+  \/\/ Do nothing if compilebroker is not initialized or compiles are submitted on level none\n@@ -1351,1 +1361,1 @@\n-  \/\/ CompileBroker::compile_method can trap and can have pending aysnc exception.\n+  \/\/ CompileBroker::compile_method can trap and can have pending async exception.\n@@ -1935,1 +1945,1 @@\n-    CompileTask* task = queue->get();\n+    CompileTask* task = queue->get(thread);\n@@ -1945,0 +1955,4 @@\n+\n+          \/\/ Notify compiler that the compiler thread is about to stop\n+          thread->compiler()->stopping_compiler_thread(thread);\n+\n@@ -2253,0 +2267,3 @@\n+    if (runtime != nullptr) {\n+      runtime->post_compile(thread);\n+    }\n@@ -2299,0 +2316,1 @@\n+        ResourceMark rm(thread);\n@@ -2300,1 +2318,1 @@\n-        comp->compile_method(&ci_env, target, osr_bci, false , directive);\n+        comp->compile_method(&ci_env, target, osr_bci, false, directive);\n@@ -2720,2 +2738,5 @@\n-    tty->cr();\n-    JVMCICompiler::print_hosted_timers();\n+    JVMCICompiler *jvmci_comp = JVMCICompiler::instance(false, JavaThread::current_or_null());\n+    if (jvmci_comp != nullptr && jvmci_comp != comp) {\n+      tty->cr();\n+      jvmci_comp->print_timers();\n+    }\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":29,"deletions":8,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -165,4 +165,5 @@\n-        dest = _block_map->at(bytes.next_bci());\n-        assert(dest != NULL, \"must be a block immediately following this one.\");\n-        dest->add_normal_predecessor(current_block);\n-\n+        if (bytes.next_bci() < method_len) {\n+          dest = _block_map->at(bytes.next_bci());\n+          assert(dest != NULL, \"must be a block immediately following this one.\");\n+          dest->add_normal_predecessor(current_block);\n+        }\n@@ -170,1 +171,1 @@\n-        assert(dest != NULL, \"branch desination must start a block.\");\n+        assert(dest != NULL, \"branch destination must start a block.\");\n@@ -175,1 +176,1 @@\n-        assert(dest != NULL, \"branch desination must start a block.\");\n+        assert(dest != NULL, \"branch destination must start a block.\");\n@@ -180,1 +181,1 @@\n-        assert(dest != NULL, \"branch desination must start a block.\");\n+        assert(dest != NULL, \"branch destination must start a block.\");\n@@ -190,1 +191,1 @@\n-          assert(dest != NULL, \"branch desination must start a block.\");\n+          assert(dest != NULL, \"branch destination must start a block.\");\n@@ -194,1 +195,1 @@\n-            assert(dest != NULL, \"branch desination must start a block.\");\n+            assert(dest != NULL, \"branch destination must start a block.\");\n@@ -207,1 +208,1 @@\n-          assert(dest != NULL, \"branch desination must start a block.\");\n+          assert(dest != NULL, \"branch destination must start a block.\");\n@@ -212,1 +213,1 @@\n-            assert(dest != NULL, \"branch desination must start a block.\");\n+            assert(dest != NULL, \"branch destination must start a block.\");\n@@ -222,1 +223,1 @@\n-          assert(dest != NULL, \"branch desination must start a block.\");\n+          assert(dest != NULL, \"branch destination must start a block.\");\n@@ -232,1 +233,1 @@\n-          assert(dest != NULL, \"branch desination must start a block.\");\n+          assert(dest != NULL, \"branch destination must start a block.\");\n","filename":"src\/hotspot\/share\/compiler\/methodLiveness.cpp","additions":15,"deletions":14,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"compiler\/oopMap.hpp\"\n+#include \"compiler\/oopMap.inline.hpp\"\n@@ -32,0 +32,2 @@\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -35,1 +37,0 @@\n-#include \"memory\/universe.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -74,2 +76,2 @@\n-OopMapStream::OopMapStream(OopMap* oop_map) {\n-  _stream = new CompressedReadStream(oop_map->write_stream()->buffer());\n+OopMapStream::OopMapStream(const OopMap* oop_map)\n+  : _stream(oop_map->write_stream()->buffer()) {\n@@ -81,2 +83,2 @@\n-OopMapStream::OopMapStream(const ImmutableOopMap* oop_map) {\n-  _stream = new CompressedReadStream(oop_map->data_addr());\n+OopMapStream::OopMapStream(const ImmutableOopMap* oop_map)\n+  : _stream(oop_map->data_addr()) {\n@@ -90,1 +92,1 @@\n-    _omv.read_from(_stream);\n+    _omv.read_from(&_stream);\n@@ -106,0 +108,3 @@\n+  _num_oops = 0;\n+  _has_derived_oops = false;\n+  _index = -1;\n@@ -121,0 +126,3 @@\n+  _num_oops = source->num_oops();\n+  _has_derived_oops = source->has_derived_oops();\n+  _index = -1;\n@@ -145,0 +153,160 @@\n+class OopMapSort {\n+private:\n+  const OopMap* _map;\n+  OopMapValue* _values;\n+  int _count;\n+\n+public:\n+  OopMapSort(const OopMap* map) : _map(map), _count(0) {\n+    _values = NEW_RESOURCE_ARRAY(OopMapValue, _map->omv_count());\n+  }\n+\n+  void sort();\n+\n+  void print();\n+\n+  void write(CompressedWriteStream* stream) {\n+    for (int i = 0; i < _count; ++i) {\n+      _values[i].write_on(stream);\n+    }\n+  }\n+\n+private:\n+  int find_derived_position(OopMapValue omv, int start) {\n+    assert(omv.type() == OopMapValue::derived_oop_value, \"\");\n+\n+    VMReg base = omv.content_reg();\n+    int i = start;\n+\n+    for (; i < _count; ++i) {\n+      if (base == _values[i].reg()) {\n+\n+        for (int n = i + 1; n < _count; ++n) {\n+          if (_values[i].type() != OopMapValue::derived_oop_value || _values[i].content_reg() != base) {\n+            return n;\n+          }\n+\n+          if (derived_cost(_values[i]) > derived_cost(omv)) {\n+            return n;\n+          }\n+        }\n+        return _count;\n+      }\n+    }\n+\n+    assert(false, \"failed to find base\");\n+    return -1;\n+  }\n+\n+  int find_position(OopMapValue omv, int start) {\n+    assert(omv.type() != OopMapValue::derived_oop_value, \"\");\n+\n+    int i = start;\n+    for (; i < _count; ++i) {\n+      if (omv_cost(_values[i]) > omv_cost(omv)) {\n+        return i;\n+      }\n+    }\n+    assert(i < _map->omv_count(), \"bounds check\");\n+    return i;\n+  }\n+\n+  void insert(OopMapValue value, int pos) {\n+    assert(pos >= 0 && pos < _map->omv_count(), \"bounds check\");\n+    assert(pos <= _count, \"sanity\");\n+\n+    if (pos < _count) {\n+      OopMapValue prev = _values[pos];\n+\n+      for (int i = pos; i < _count; ++i) {\n+        OopMapValue tmp = _values[i+1];\n+        _values[i+1] = prev;\n+        prev = tmp;\n+      }\n+    }\n+    _values[pos] = value;\n+\n+    ++_count;\n+  }\n+\n+  int omv_cost(OopMapValue omv) {\n+    assert(omv.type() == OopMapValue::oop_value || omv.type() == OopMapValue::narrowoop_value, \"\");\n+    return reg_cost(omv.reg());\n+  }\n+\n+  int reg_cost(VMReg reg) {\n+    if (reg->is_reg()) {\n+      return 0;\n+    }\n+    return reg->reg2stack() * VMRegImpl::stack_slot_size;\n+  }\n+\n+  int derived_cost(OopMapValue omv) {\n+    return reg_cost(omv.reg());\n+  }\n+};\n+\n+void OopMapSort::sort() {\n+  for (OopMapStream oms(_map); !oms.is_done(); oms.next()) {\n+    OopMapValue omv = oms.current();\n+    assert(omv.type() == OopMapValue::oop_value || omv.type() == OopMapValue::narrowoop_value || omv.type() == OopMapValue::derived_oop_value || omv.type() == OopMapValue::callee_saved_value, \"\");\n+  }\n+\n+  for (OopMapStream oms(_map); !oms.is_done(); oms.next()) {\n+    if (oms.current().type() == OopMapValue::callee_saved_value) {\n+      insert(oms.current(), _count);\n+    }\n+  }\n+\n+  int start = _count;\n+  for (OopMapStream oms(_map); !oms.is_done(); oms.next()) {\n+    OopMapValue omv = oms.current();\n+    if (omv.type() == OopMapValue::oop_value || omv.type() == OopMapValue::narrowoop_value) {\n+      int pos = find_position(omv, start);\n+      insert(omv, pos);\n+    }\n+  }\n+\n+  for (OopMapStream oms(_map); !oms.is_done(); oms.next()) {\n+    OopMapValue omv = oms.current();\n+    if (omv.type() == OopMapValue::derived_oop_value) {\n+      int pos = find_derived_position(omv, start);\n+      assert(pos > 0, \"\");\n+      insert(omv, pos);\n+    }\n+  }\n+}\n+\n+void OopMapSort::print() {\n+  for (int i = 0; i < _count; ++i) {\n+    OopMapValue omv = _values[i];\n+    if (omv.type() == OopMapValue::oop_value || omv.type() == OopMapValue::narrowoop_value) {\n+      if (omv.reg()->is_reg()) {\n+        tty->print_cr(\"[%c][%d] -> reg (\" INTPTR_FORMAT \")\", omv.type() == OopMapValue::narrowoop_value ? 'n' : 'o', i, omv.reg()->value());\n+      } else {\n+        tty->print_cr(\"[%c][%d] -> stack (\"  INTPTR_FORMAT \")\", omv.type() == OopMapValue::narrowoop_value ? 'n' : 'o', i, omv.reg()->reg2stack() * VMRegImpl::stack_slot_size);\n+      }\n+    } else {\n+      if (omv.content_reg()->is_reg()) {\n+        tty->print_cr(\"[d][%d] -> reg (\" INTPTR_FORMAT \") stack (\" INTPTR_FORMAT \")\", i, omv.content_reg()->value(), omv.reg()->reg2stack() * VMRegImpl::stack_slot_size);\n+      } else if (omv.reg()->is_reg()) {\n+        tty->print_cr(\"[d][%d] -> stack (\" INTPTR_FORMAT \") reg (\" INTPTR_FORMAT \")\", i, omv.content_reg()->reg2stack() * VMRegImpl::stack_slot_size, omv.reg()->value());\n+      } else {\n+        int derived_offset = omv.reg()->reg2stack() * VMRegImpl::stack_slot_size;\n+        int base_offset = omv.content_reg()->reg2stack() * VMRegImpl::stack_slot_size;\n+        tty->print_cr(\"[d][%d] -> stack (%x) stack (%x)\", i, base_offset, derived_offset);\n+      }\n+    }\n+  }\n+}\n+\n+void OopMap::copy_and_sort_data_to(address addr) const {\n+  OopMapSort sort(this);\n+  sort.sort();\n+  CompressedWriteStream* stream = new CompressedWriteStream(_write_stream->position());\n+  sort.write(stream);\n+\n+  assert(stream->position() == write_stream()->position(), \"\");\n+  memcpy(addr, stream->buffer(), stream->position());\n+}\n+\n@@ -165,0 +333,5 @@\n+  if (x == OopMapValue::oop_value || x == OopMapValue::narrowoop_value) {\n+    increment_num_oops();\n+  } else if (x == OopMapValue::derived_oop_value) {\n+    set_has_derived_oops(true);\n+  }\n@@ -196,1 +369,1 @@\n-void OopMapSet::add_gc_map(int pc_offset, OopMap *map ) {\n+int OopMapSet::add_gc_map(int pc_offset, OopMap *map ) {\n@@ -212,1 +385,3 @@\n-  add(map);\n+  int index = add(map);\n+  map->_index = index;\n+  return index;\n@@ -215,1 +390,7 @@\n-static void add_derived_oop(oop* base, derived_pointer* derived, OopClosure* oop_fn) {\n+class AddDerivedOop : public DerivedOopClosure {\n+ public:\n+  enum {\n+    SkipNull = true, NeedsLock = true\n+  };\n+\n+  virtual void do_derived_oop(oop* base, derived_pointer* derived) {\n@@ -217,1 +398,1 @@\n-  DerivedPointerTable::add(derived, base);\n+    DerivedPointerTable::add(derived, base);\n@@ -219,1 +400,2 @@\n-}\n+  }\n+};\n@@ -221,2 +403,2 @@\n-static void ignore_derived_oop(oop* base, derived_pointer* derived, OopClosure* oop_fn) {\n-}\n+class ProcessDerivedOop : public DerivedOopClosure {\n+  OopClosure* _oop_cl;\n@@ -224,10 +406,19 @@\n-static void process_derived_oop(oop* base, derived_pointer* derived, OopClosure* oop_fn) {\n-  \/\/ All derived pointers must be processed before the base pointer of any derived pointer is processed.\n-  \/\/ Otherwise, if two derived pointers use the same base, the second derived pointer will get an obscured\n-  \/\/ offset, if the base pointer is processed in the first derived pointer.\n-  derived_pointer derived_base = to_derived_pointer(*base);\n-  intptr_t offset = *derived - derived_base;\n-  *derived = derived_base;\n-  oop_fn->do_oop((oop*)derived);\n-  *derived = *derived + offset;\n-}\n+public:\n+  ProcessDerivedOop(OopClosure* oop_cl) :\n+      _oop_cl(oop_cl) {}\n+\n+  enum {\n+    SkipNull = true, NeedsLock = true\n+  };\n+\n+  virtual void do_derived_oop(oop* base, derived_pointer* derived) {\n+    \/\/ All derived pointers must be processed before the base pointer of any derived pointer is processed.\n+    \/\/ Otherwise, if two derived pointers use the same base, the second derived pointer will get an obscured\n+    \/\/ offset, if the base pointer is processed in the first derived pointer.\n+    derived_pointer derived_base = to_derived_pointer(*base);\n+    intptr_t offset = *derived - derived_base;\n+    *derived = derived_base;\n+    _oop_cl->do_oop((oop*)derived);\n+    *derived = *derived + offset;\n+  }\n+};\n@@ -235,0 +426,2 @@\n+class IgnoreDerivedOop : public DerivedOopClosure {\n+  OopClosure* _oop_cl;\n@@ -236,24 +429,7 @@\n-#ifndef PRODUCT\n-static void trace_codeblob_maps(const frame *fr, const RegisterMap *reg_map) {\n-  \/\/ Print oopmap and regmap\n-  tty->print_cr(\"------ \");\n-  CodeBlob* cb = fr->cb();\n-  const ImmutableOopMapSet* maps = cb->oop_maps();\n-  const ImmutableOopMap* map = cb->oop_map_for_return_address(fr->pc());\n-  map->print();\n-  if( cb->is_nmethod() ) {\n-    nmethod* nm = (nmethod*)cb;\n-    \/\/ native wrappers have no scope data, it is implied\n-    if (nm->is_native_method()) {\n-      tty->print(\"bci: 0 (native)\");\n-    } else {\n-      ScopeDesc* scope  = nm->scope_desc_at(fr->pc());\n-      tty->print(\"bci: %d \",scope->bci());\n-    }\n-  }\n-  tty->cr();\n-  fr->print_on(tty);\n-  tty->print(\"     \");\n-  cb->print_value_on(tty);  tty->cr();\n-  reg_map->print();\n-  tty->print_cr(\"------ \");\n+public:\n+  enum {\n+    SkipNull = true, NeedsLock = true\n+  };\n+\n+  virtual void do_derived_oop(oop* base, derived_pointer* derived) {}\n+};\n@@ -261,0 +437,6 @@\n+void OopMapSet::oops_do(const frame* fr, const RegisterMap* reg_map, OopClosure* f, DerivedPointerIterationMode mode) {\n+  find_map(fr)->oops_do(fr, reg_map, f, mode);\n+}\n+\n+void OopMapSet::oops_do(const frame *fr, const RegisterMap* reg_map, OopClosure* f, DerivedOopClosure* df) {\n+  find_map(fr)->oops_do(fr, reg_map, f, df);\n@@ -262,3 +444,14 @@\n-#endif \/\/ PRODUCT\n-void OopMapSet::oops_do(const frame *fr, const RegisterMap* reg_map, OopClosure* f, DerivedPointerIterationMode mode) {\n-  switch (mode) {\n+void ImmutableOopMap::oops_do(const frame *fr, const RegisterMap *reg_map,\n+                              OopClosure* oop_fn, DerivedOopClosure* derived_oop_fn) const {\n+  assert(derived_oop_fn != NULL, \"sanity\");\n+  OopMapDo<OopClosure, DerivedOopClosure, SkipNullValue> visitor(oop_fn, derived_oop_fn);\n+  visitor.oops_do(fr, reg_map, this);\n+}\n+\n+void ImmutableOopMap::oops_do(const frame *fr, const RegisterMap *reg_map,\n+                              OopClosure* oop_fn, DerivedPointerIterationMode derived_mode) const {\n+  ProcessDerivedOop process_cl(oop_fn);\n+  AddDerivedOop add_cl;\n+  IgnoreDerivedOop ignore_cl;\n+  DerivedOopClosure* derived_cl;\n+  switch (derived_mode) {\n@@ -267,1 +460,1 @@\n-    all_do(fr, reg_map, f, process_derived_oop);\n+    derived_cl = &process_cl;\n@@ -270,1 +463,1 @@\n-    all_do(fr, reg_map, f, add_derived_oop);\n+    derived_cl = &add_cl;\n@@ -273,1 +466,1 @@\n-    all_do(fr, reg_map, f, ignore_derived_oop);\n+    derived_cl = &ignore_cl;\n@@ -275,0 +468,2 @@\n+  default:\n+    guarantee (false, \"unreachable\");\n@@ -276,0 +471,2 @@\n+  OopMapDo<OopClosure, DerivedOopClosure, SkipNullValue> visitor(oop_fn, derived_cl);\n+  visitor.oops_do(fr, reg_map, this);\n@@ -278,39 +475,6 @@\n-\n-void OopMapSet::all_do(const frame *fr, const RegisterMap *reg_map,\n-                       OopClosure* oop_fn, void derived_oop_fn(oop*, derived_pointer*, OopClosure*)) {\n-  CodeBlob* cb = fr->cb();\n-  assert(cb != NULL, \"no codeblob\");\n-\n-  NOT_PRODUCT(if (TraceCodeBlobStacks) trace_codeblob_maps(fr, reg_map);)\n-\n-  const ImmutableOopMap* map = cb->oop_map_for_return_address(fr->pc());\n-  assert(map != NULL, \"no ptr map found\");\n-\n-  \/\/ handle derived pointers first (otherwise base pointer may be\n-  \/\/ changed before derived pointer offset has been collected)\n-  {\n-    for (OopMapStream oms(map); !oms.is_done(); oms.next()) {\n-      OopMapValue omv = oms.current();\n-      if (omv.type() != OopMapValue::derived_oop_value) {\n-        continue;\n-      }\n-\n-#ifndef COMPILER2\n-      COMPILER1_PRESENT(ShouldNotReachHere();)\n-#if INCLUDE_JVMCI\n-      if (UseJVMCICompiler) {\n-        ShouldNotReachHere();\n-      }\n-#endif\n-#endif \/\/ !COMPILER2\n-      derived_pointer* derived_loc = (derived_pointer*)fr->oopmapreg_to_location(omv.reg(),reg_map);\n-      guarantee(derived_loc != NULL, \"missing saved register\");\n-      oop* base_loc = fr->oopmapreg_to_oop_location(omv.content_reg(), reg_map);\n-      \/\/ Ignore NULL oops and decoded NULL narrow oops which\n-      \/\/ equal to CompressedOops::base() when a narrow oop\n-      \/\/ implicit null check is used in compiled code.\n-      \/\/ The narrow_oop_base could be NULL or be the address\n-      \/\/ of the page below heap depending on compressed oops mode.\n-      if (base_loc != NULL && *base_loc != NULL && !CompressedOops::is_base(*base_loc)) {\n-        derived_oop_fn(base_loc, derived_loc, oop_fn);\n-      }\n+void ImmutableOopMap::all_type_do(const frame *fr, OopMapClosure* fn) const {\n+  OopMapValue omv;\n+  for (OopMapStream oms(this); !oms.is_done(); oms.next()) {\n+    omv = oms.current();\n+    if (fn->handle_type(omv.type())) {\n+      fn->do_value(omv.reg(), omv.type());\n@@ -319,0 +483,1 @@\n+}\n@@ -320,34 +485,6 @@\n-  {\n-    \/\/ We want coop and oop oop_types\n-    for (OopMapStream oms(map); !oms.is_done(); oms.next()) {\n-      OopMapValue omv = oms.current();\n-      oop* loc = fr->oopmapreg_to_oop_location(omv.reg(),reg_map);\n-      \/\/ It should be an error if no location can be found for a\n-      \/\/ register mentioned as contained an oop of some kind.  Maybe\n-      \/\/ this was allowed previously because value_value items might\n-      \/\/ be missing?\n-      guarantee(loc != NULL, \"missing saved register\");\n-      if ( omv.type() == OopMapValue::oop_value ) {\n-        oop val = *loc;\n-        if (val == NULL || CompressedOops::is_base(val)) {\n-          \/\/ Ignore NULL oops and decoded NULL narrow oops which\n-          \/\/ equal to CompressedOops::base() when a narrow oop\n-          \/\/ implicit null check is used in compiled code.\n-          \/\/ The narrow_oop_base could be NULL or be the address\n-          \/\/ of the page below heap depending on compressed oops mode.\n-          continue;\n-        }\n-        oop_fn->do_oop(loc);\n-      } else if ( omv.type() == OopMapValue::narrowoop_value ) {\n-        narrowOop *nl = (narrowOop*)loc;\n-#ifndef VM_LITTLE_ENDIAN\n-        VMReg vmReg = omv.reg();\n-        if (!vmReg->is_stack()) {\n-          \/\/ compressed oops in registers only take up 4 bytes of an\n-          \/\/ 8 byte register but they are in the wrong part of the\n-          \/\/ word so adjust loc to point at the right place.\n-          nl = (narrowOop*)((address)nl + 4);\n-        }\n-#endif\n-        oop_fn->do_oop(nl);\n-      }\n+void ImmutableOopMap::all_type_do(const frame *fr, OopMapValue::oop_types type, OopMapClosure* fn) const {\n+  OopMapValue omv;\n+  for (OopMapStream oms(this); !oms.is_done(); oms.next()) {\n+    omv = oms.current();\n+    if (omv.type() == type) {\n+      fn->do_value(omv.reg(), omv.type());\n@@ -358,0 +495,11 @@\n+static void update_register_map1(const ImmutableOopMap* oopmap, const frame* fr, RegisterMap* reg_map) {\n+  for (OopMapStream oms(oopmap); !oms.is_done(); oms.next()) {\n+    OopMapValue omv = oms.current();\n+    if (omv.type() == OopMapValue::callee_saved_value) {\n+      VMReg reg = omv.content_reg();\n+      address loc = fr->oopmapreg_to_location(omv.reg(), reg_map);\n+      reg_map->set_location(reg, loc);\n+      \/\/DEBUG_ONLY(nof_callee++;)\n+    }\n+  }\n+}\n@@ -360,2 +508,1 @@\n-void OopMapSet::update_register_map(const frame *fr, RegisterMap *reg_map) {\n-  ResourceMark rm;\n+void ImmutableOopMap::update_register_map(const frame *fr, RegisterMap *reg_map) const {\n@@ -364,1 +511,0 @@\n-\n@@ -378,13 +524,1 @@\n-  address pc = fr->pc();\n-  const ImmutableOopMap* map  = cb->oop_map_for_return_address(pc);\n-  assert(map != NULL, \"no ptr map found\");\n-\n-  for (OopMapStream oms(map); !oms.is_done(); oms.next()) {\n-    OopMapValue omv = oms.current();\n-    if (omv.type() == OopMapValue::callee_saved_value) {\n-      VMReg reg = omv.content_reg();\n-      oop* loc = fr->oopmapreg_to_oop_location(omv.reg(), reg_map);\n-      reg_map->set_location(reg, (address) loc);\n-      DEBUG_ONLY(nof_callee++;)\n-    }\n-  }\n+  update_register_map1(this, fr, reg_map);\n@@ -395,1 +529,1 @@\n-  assert(cb->is_compiled_by_c1() || cb->is_compiled_by_jvmci() || !cb->is_runtime_stub() ||\n+  assert(cb == NULL || cb->is_compiled_by_c1() || cb->is_compiled_by_jvmci() || !cb->is_runtime_stub() ||\n@@ -401,0 +535,49 @@\n+const ImmutableOopMap* OopMapSet::find_map(const frame *fr) {\n+  return find_map(fr->cb(), fr->pc());\n+}\n+\n+const ImmutableOopMap* OopMapSet::find_map(const CodeBlob* cb, address pc) {\n+  assert(cb != NULL, \"no codeblob\");\n+  const ImmutableOopMap* map = cb->oop_map_for_return_address(pc);\n+  assert(map != NULL, \"no ptr map found\");\n+  return map;\n+}\n+\n+\/\/ Update callee-saved register info for the following frame\n+void OopMapSet::update_register_map(const frame *fr, RegisterMap *reg_map) {\n+  find_map(fr)->update_register_map(fr, reg_map);\n+}\n+\n+\/\/=============================================================================\n+\/\/ Non-Product code\n+\n+#ifndef PRODUCT\n+void OopMapSet::trace_codeblob_maps(const frame *fr, const RegisterMap *reg_map) {\n+  \/\/ Print oopmap and regmap\n+  tty->print_cr(\"------ \");\n+  CodeBlob* cb = fr->cb();\n+  const ImmutableOopMapSet* maps = cb->oop_maps();\n+  const ImmutableOopMap* map = cb->oop_map_for_return_address(fr->pc());\n+  map->print();\n+  if( cb->is_nmethod() ) {\n+    nmethod* nm = (nmethod*)cb;\n+    \/\/ native wrappers have no scope data, it is implied\n+    if (nm->is_native_method()) {\n+      tty->print(\"bci: 0 (native)\");\n+    } else {\n+      ScopeDesc* scope  = nm->scope_desc_at(fr->pc());\n+      tty->print(\"bci: %d \",scope->bci());\n+    }\n+  }\n+  tty->cr();\n+  fr->print_on(tty);\n+  tty->print(\"     \");\n+  cb->print_value_on(tty);  tty->cr();\n+  if (reg_map != NULL) {\n+    reg_map->print();\n+  }\n+  tty->print_cr(\"------ \");\n+\n+}\n+#endif \/\/ PRODUCT\n+\n@@ -511,0 +694,12 @@\n+int ImmutableOopMapSet::find_slot_for_offset(int pc_offset) const {\n+  \/\/ we might not have an oopmap at asynchronous (non-safepoint) stackwalks\n+  ImmutableOopMapPair* pairs = get_pairs();\n+  for (int i = 0; i < _count; ++i) {\n+    if (pairs[i].pc_offset() >= pc_offset) {\n+      ImmutableOopMapPair* last = &pairs[i];\n+      return last->pc_offset() == pc_offset ? i : -1;\n+    }\n+  }\n+  return -1;\n+}\n+\n@@ -528,2 +723,6 @@\n-const ImmutableOopMap* ImmutableOopMapPair::get_from(const ImmutableOopMapSet* set) const {\n-  return set->oopmap_at_offset(_oopmap_offset);\n+ImmutableOopMap::ImmutableOopMap(const OopMap* oopmap)\n+  : _count(oopmap->count()), _num_oops(oopmap->num_oops()) {\n+  _num_oops = oopmap->num_oops();\n+  _has_derived_oops = oopmap->has_derived_oops();\n+  address addr = data_addr();\n+  oopmap->copy_and_sort_data_to(addr);\n@@ -532,3 +731,7 @@\n-ImmutableOopMap::ImmutableOopMap(const OopMap* oopmap) : _count(oopmap->count()) {\n-  address addr = data_addr();\n-  oopmap->copy_data_to(addr);\n+bool ImmutableOopMap::has_any(OopMapValue::oop_types type) const {\n+  for (OopMapStream oms(this); !oms.is_done(); oms.next()) {\n+    if (oms.current().type() == type) {\n+      return true;\n+    }\n+  }\n+  return false;\n@@ -626,2 +829,2 @@\n-    const ImmutableOopMap* nv = set->find_map_at_offset(map->offset());\n-    assert(memcmp(map->data(), nv->data_addr(), map->data_size()) == 0, \"check identity\");\n+    \/\/const ImmutableOopMap* nv = set->find_map_at_offset(map->offset());\n+    \/\/assert(memcmp(map->data(), nv->data_addr(), map->data_size()) == 0, \"check identity\");\n@@ -759,0 +962,2 @@\n+    \/\/ assert(offset >= 0 && offset <= (intptr_t)(base->size() << LogHeapWordSize), \"offset: %ld base->size: %zu relative: %d\", offset, base->size() << LogHeapWordSize, *(intptr_t*)derived_loc <= 0);\n+\n","filename":"src\/hotspot\/share\/compiler\/oopMap.cpp","additions":357,"deletions":152,"binary":false,"changes":509,"status":"modified"},{"patch":"@@ -531,2 +531,2 @@\n-      ciKlass* klass = itype->klass();\n-      if ( klass->is_loaded() &&\n+      ciKlass* klass = itype->instance_klass();\n+      if (klass->is_loaded() &&\n","filename":"src\/hotspot\/share\/gc\/g1\/c2\/g1BarrierSetC2.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,1 @@\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -266,1 +267,1 @@\n-  \/\/ intitial chunk to allow other workers to steal while we're processing.\n+  \/\/ initial chunk to allow other workers to steal while we're processing.\n@@ -530,0 +531,2 @@\n+    ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -625,3 +628,3 @@\n-    \/\/ are relabeled as such. We mark the failing objects in the prev bitmap and\n-    \/\/ later use it to handle all failed objects.\n-    _g1h->mark_evac_failure_object(old, _worker_id);\n+    \/\/ are relabeled as such. We mark the failing objects in the marking bitmap\n+    \/\/ and later use it to handle all failed objects.\n+    _g1h->mark_evac_failure_object(old);\n@@ -634,0 +637,3 @@\n+\n+    ContinuationGCSupport::transform_stack_chunk(old);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -79,0 +79,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -82,0 +83,1 @@\n+#include \"runtime\/threads.hpp\"\n@@ -964,0 +966,3 @@\n+  Continuations::on_gc_marking_cycle_start();\n+  Continuations::arm_all_nmethods();\n+\n@@ -994,0 +999,3 @@\n+  Continuations::on_gc_marking_cycle_finish();\n+  Continuations::arm_all_nmethods();\n+\n@@ -1918,1 +1926,1 @@\n-    MarkingCodeBlobClosure mark_and_push_in_blobs(&mark_and_push_closure, !CodeBlobToOopClosure::FixRelocations);\n+    MarkingCodeBlobClosure mark_and_push_in_blobs(&mark_and_push_closure, !CodeBlobToOopClosure::FixRelocations, true \/* keepalive nmethods *\/);\n","filename":"src\/hotspot\/share\/gc\/parallel\/psParallelCompact.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/continuationGCSupport.inline.hpp\"\n@@ -340,0 +341,2 @@\n+    ContinuationGCSupport::transform_stack_chunk(obj);\n+\n","filename":"src\/hotspot\/share\/gc\/parallel\/psPromotionManager.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -31,1 +32,2 @@\n-#include \"runtime\/thread.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -82,0 +84,32 @@\n+static BarrierSetNMethod* select_barrier_set_nmethod(BarrierSetNMethod* barrier_set_nmethod) {\n+  if (barrier_set_nmethod != NULL) {\n+    \/\/ The GC needs nmethod entry barriers to do concurrent GC\n+    return barrier_set_nmethod;\n+  } else if (Continuations::enabled()) {\n+    \/\/ The GC needs nmethod entry barriers to deal with continuations\n+    return new BarrierSetNMethod();\n+  } else {\n+    \/\/ The GC does not need nmethod entry barriers\n+    return NULL;\n+  }\n+}\n+\n+BarrierSet::BarrierSet(BarrierSetAssembler* barrier_set_assembler,\n+                       BarrierSetC1* barrier_set_c1,\n+                       BarrierSetC2* barrier_set_c2,\n+                       BarrierSetNMethod* barrier_set_nmethod,\n+                       const FakeRtti& fake_rtti) :\n+    _fake_rtti(fake_rtti),\n+    _barrier_set_assembler(barrier_set_assembler),\n+    _barrier_set_c1(barrier_set_c1),\n+    _barrier_set_c2(barrier_set_c2),\n+    _barrier_set_nmethod(select_barrier_set_nmethod(barrier_set_nmethod)) {\n+}\n+\n+void BarrierSet::on_thread_attach(Thread* thread) {\n+  if (Continuations::enabled()) {\n+    BarrierSetNMethod* bs_nm = barrier_set_nmethod();\n+    thread->set_nmethod_disarm_value(bs_nm->disarmed_value());\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.cpp","additions":36,"deletions":2,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -102,6 +102,1 @@\n-             const FakeRtti& fake_rtti) :\n-    _fake_rtti(fake_rtti),\n-    _barrier_set_assembler(barrier_set_assembler),\n-    _barrier_set_c1(barrier_set_c1),\n-    _barrier_set_c2(barrier_set_c2),\n-    _barrier_set_nmethod(barrier_set_nmethod) {}\n+             const FakeRtti& fake_rtti);\n@@ -146,1 +141,1 @@\n-  virtual void on_thread_attach(Thread* thread) {}\n+  virtual void on_thread_attach(Thread* thread);\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.hpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"runtime\/javaThread.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSet.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  assert(md->is_inline_type_klass(), \"invariant\");\n+  assert(md->is_inline_klass(), \"invariant\");\n@@ -37,1 +37,1 @@\n-  assert(md->is_inline_type_klass(), \"invariant\");\n+  assert(md->is_inline_klass(), \"invariant\");\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetRuntime.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  BasicType bt = access.type();\n@@ -102,1 +103,1 @@\n-    if (access.type() == T_DOUBLE) {\n+    if (bt == T_DOUBLE) {\n@@ -107,2 +108,2 @@\n-    store = kit->store_to_memory(kit->control(), access.addr().node(), val.node(), access.type(),\n-                                     access.addr().type(), mo, requires_atomic_access, unaligned, mismatched, unsafe);\n+    store = kit->store_to_memory(kit->control(), access.addr().node(), val.node(), bt,\n+                                 access.addr().type(), mo, requires_atomic_access, unaligned, mismatched, unsafe);\n@@ -110,1 +111,0 @@\n-    assert(!requires_atomic_access, \"not yet supported\");\n@@ -120,1 +120,1 @@\n-    StoreNode* st = StoreNode::make(gvn, ctl, mem, access.addr().node(), adr_type, val.node(), access.type(), mo);\n+    StoreNode* st = StoreNode::make(gvn, ctl, mem, access.addr().node(), adr_type, val.node(), bt, mo, requires_atomic_access);\n@@ -163,1 +163,0 @@\n-      assert(!requires_atomic_access, \"can't ensure atomicity\");\n@@ -167,2 +166,2 @@\n-                            adr_type, val_type, access.type(), mo, dep, unaligned,\n-                            mismatched, unsafe, access.barrier_data());\n+                            adr_type, val_type, access.type(), mo, dep, requires_atomic_access,\n+                            unaligned, mismatched, unsafe, access.barrier_data());\n@@ -176,1 +175,0 @@\n-    assert(!requires_atomic_access, \"not yet supported\");\n@@ -183,2 +181,2 @@\n-    load = LoadNode::make(gvn, control, mem, adr, adr_type, val_type, access.type(), mo,\n-                          dep, unaligned, mismatched, unsafe, access.barrier_data());\n+    load = LoadNode::make(gvn, control, mem, adr, adr_type, val_type, access.type(), mo, dep,\n+                          requires_atomic_access, unaligned, mismatched, unsafe, access.barrier_data());\n@@ -381,1 +379,1 @@\n-        int s = Klass::layout_helper_size_in_bytes(adr_type->isa_instptr()->klass()->layout_helper());\n+        int s = Klass::layout_helper_size_in_bytes(adr_type->isa_instptr()->instance_klass()->layout_helper());\n@@ -395,3 +393,0 @@\n-  if (!access.needs_pinning()) {\n-    return;\n-  }\n@@ -711,0 +706,1 @@\n+  assert(UseTLAB, \"Only for TLAB enabled allocations\");\n@@ -712,4 +708,3 @@\n-  Node* eden_top_adr;\n-  Node* eden_end_adr;\n-\n-  macro->set_eden_pointers(eden_top_adr, eden_end_adr);\n+  Node* thread = macro->transform_later(new ThreadLocalNode());\n+  Node* tlab_top_adr = macro->basic_plus_adr(macro->top()\/*not oop*\/, thread, in_bytes(JavaThread::tlab_top_offset()));\n+  Node* tlab_end_adr = macro->basic_plus_adr(macro->top()\/*not oop*\/, thread, in_bytes(JavaThread::tlab_end_offset()));\n@@ -717,1 +712,1 @@\n-  \/\/ Load Eden::end.  Loop invariant and hoisted.\n+  \/\/ Load TLAB end.\n@@ -719,2 +714,2 @@\n-  \/\/ Note: We set the control input on \"eden_end\" and \"old_eden_top\" when using\n-  \/\/       a TLAB to work around a bug where these values were being moved across\n+  \/\/ Note: We set the control input on \"tlab_end\" and \"old_tlab_top\" to work around\n+  \/\/       a bug where these values were being moved across\n@@ -727,19 +722,13 @@\n-  Node *eden_end = macro->make_load(toobig_false, mem, eden_end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n-\n-  \/\/ We need a Region for the loop-back contended case.\n-  enum { fall_in_path = 1, contended_loopback_path = 2 };\n-  Node *contended_region;\n-  Node *contended_phi_rawmem;\n-  if (UseTLAB) {\n-    contended_region = toobig_false;\n-    contended_phi_rawmem = mem;\n-  } else {\n-    contended_region = new RegionNode(3);\n-    contended_phi_rawmem = new PhiNode(contended_region, Type::MEMORY, TypeRawPtr::BOTTOM);\n-    \/\/ Now handle the passing-too-big test.  We fall into the contended\n-    \/\/ loop-back merge point.\n-    contended_region    ->init_req(fall_in_path, toobig_false);\n-    contended_phi_rawmem->init_req(fall_in_path, mem);\n-    macro->transform_later(contended_region);\n-    macro->transform_later(contended_phi_rawmem);\n-  }\n+  Node* tlab_end = macro->make_load(toobig_false, mem, tlab_end_adr, 0, TypeRawPtr::BOTTOM, T_ADDRESS);\n+\n+  \/\/ Load the TLAB top.\n+  Node* old_tlab_top = new LoadPNode(toobig_false, mem, tlab_top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered);\n+  macro->transform_later(old_tlab_top);\n+\n+  \/\/ Add to heap top to get a new TLAB top\n+  Node* new_tlab_top = new AddPNode(macro->top(), old_tlab_top, size_in_bytes);\n+  macro->transform_later(new_tlab_top);\n+\n+  \/\/ Check against TLAB end\n+  Node* tlab_full = new CmpPNode(new_tlab_top, tlab_end);\n+  macro->transform_later(tlab_full);\n@@ -747,14 +736,1 @@\n-  \/\/ Load(-locked) the heap top.\n-  \/\/ See note above concerning the control input when using a TLAB\n-  Node *old_eden_top = UseTLAB\n-    ? new LoadPNode      (toobig_false, contended_phi_rawmem, eden_top_adr, TypeRawPtr::BOTTOM, TypeRawPtr::BOTTOM, MemNode::unordered)\n-    : new LoadPLockedNode(contended_region, contended_phi_rawmem, eden_top_adr, MemNode::acquire);\n-\n-  macro->transform_later(old_eden_top);\n-  \/\/ Add to heap top to get a new heap top\n-  Node *new_eden_top = new AddPNode(macro->top(), old_eden_top, size_in_bytes);\n-  macro->transform_later(new_eden_top);\n-  \/\/ Check for needing a GC; compare against heap end\n-  Node *needgc_cmp = new CmpPNode(new_eden_top, eden_end);\n-  macro->transform_later(needgc_cmp);\n-  Node *needgc_bol = new BoolNode(needgc_cmp, BoolTest::ge);\n+  Node* needgc_bol = new BoolNode(tlab_full, BoolTest::ge);\n@@ -762,1 +738,1 @@\n-  IfNode *needgc_iff = new IfNode(contended_region, needgc_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);\n+  IfNode* needgc_iff = new IfNode(toobig_false, needgc_bol, PROB_UNLIKELY_MAG(4), COUNT_UNKNOWN);\n@@ -766,1 +742,1 @@\n-  Node *needgc_true = new IfTrueNode(needgc_iff);\n+  Node* needgc_true = new IfTrueNode(needgc_iff);\n@@ -770,2 +746,2 @@\n-  \/\/ No need for a GC.  Setup for the Store-Conditional\n-  Node *needgc_false = new IfFalseNode(needgc_iff);\n+  \/\/ No need for a GC.\n+  Node* needgc_false = new IfFalseNode(needgc_iff);\n@@ -774,58 +750,12 @@\n-  i_o = macro->prefetch_allocation(i_o, needgc_false, contended_phi_rawmem,\n-                                   old_eden_top, new_eden_top, prefetch_lines);\n-\n-  Node* fast_oop = old_eden_top;\n-\n-  \/\/ Store (-conditional) the modified eden top back down.\n-  \/\/ StorePConditional produces flags for a test PLUS a modified raw\n-  \/\/ memory state.\n-  if (UseTLAB) {\n-    Node* store_eden_top =\n-      new StorePNode(needgc_false, contended_phi_rawmem, eden_top_adr,\n-                     TypeRawPtr::BOTTOM, new_eden_top, MemNode::unordered);\n-    macro->transform_later(store_eden_top);\n-    fast_oop_ctrl = needgc_false; \/\/ No contention, so this is the fast path\n-    fast_oop_rawmem = store_eden_top;\n-  } else {\n-    Node* store_eden_top =\n-      new StorePConditionalNode(needgc_false, contended_phi_rawmem, eden_top_adr,\n-                                new_eden_top, fast_oop\/*old_eden_top*\/);\n-    macro->transform_later(store_eden_top);\n-    Node *contention_check = new BoolNode(store_eden_top, BoolTest::ne);\n-    macro->transform_later(contention_check);\n-    store_eden_top = new SCMemProjNode(store_eden_top);\n-    macro->transform_later(store_eden_top);\n-\n-    \/\/ If not using TLABs, check to see if there was contention.\n-    IfNode *contention_iff = new IfNode (needgc_false, contention_check, PROB_MIN, COUNT_UNKNOWN);\n-    macro->transform_later(contention_iff);\n-    Node *contention_true = new IfTrueNode(contention_iff);\n-    macro->transform_later(contention_true);\n-    \/\/ If contention, loopback and try again.\n-    contended_region->init_req(contended_loopback_path, contention_true);\n-    contended_phi_rawmem->init_req(contended_loopback_path, store_eden_top);\n-\n-    \/\/ Fast-path succeeded with no contention!\n-    Node *contention_false = new IfFalseNode(contention_iff);\n-    macro->transform_later(contention_false);\n-    fast_oop_ctrl = contention_false;\n-\n-    \/\/ Bump total allocated bytes for this thread\n-    Node* thread = new ThreadLocalNode();\n-    macro->transform_later(thread);\n-    Node* alloc_bytes_adr = macro->basic_plus_adr(macro->top()\/*not oop*\/, thread,\n-                                                  in_bytes(JavaThread::allocated_bytes_offset()));\n-    Node* alloc_bytes = macro->make_load(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,\n-                                         0, TypeLong::LONG, T_LONG);\n-#ifdef _LP64\n-    Node* alloc_size = size_in_bytes;\n-#else\n-    Node* alloc_size = new ConvI2LNode(size_in_bytes);\n-    macro->transform_later(alloc_size);\n-#endif\n-    Node* new_alloc_bytes = new AddLNode(alloc_bytes, alloc_size);\n-    macro->transform_later(new_alloc_bytes);\n-    fast_oop_rawmem = macro->make_store(fast_oop_ctrl, store_eden_top, alloc_bytes_adr,\n-                                        0, new_alloc_bytes, T_LONG);\n-  }\n-  return fast_oop;\n+  \/\/ Fast path:\n+  i_o = macro->prefetch_allocation(i_o, needgc_false, mem,\n+                                   old_tlab_top, new_tlab_top, prefetch_lines);\n+\n+  \/\/ Store the modified TLAB top back down.\n+  Node* store_tlab_top = new StorePNode(needgc_false, mem, tlab_top_adr,\n+                   TypeRawPtr::BOTTOM, new_tlab_top, MemNode::unordered);\n+  macro->transform_later(store_tlab_top);\n+\n+  fast_oop_ctrl = needgc_false;\n+  fast_oop_rawmem = store_tlab_top;\n+  return old_tlab_top;\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.cpp","additions":47,"deletions":117,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -99,1 +99,1 @@\n-\/\/ This class wraps a bunch of context parameters thare are passed around in the\n+\/\/ This class wraps a bunch of context parameters that are passed around in the\n@@ -170,1 +170,1 @@\n-\/\/ This class wraps a bunch of context parameters thare are passed around in the\n+\/\/ This class wraps a bunch of context parameters that are passed around in the\n@@ -175,1 +175,0 @@\n-  bool  _needs_pinning;\n@@ -182,2 +181,1 @@\n-    _alias_idx(alias_idx),\n-    _needs_pinning(true) {}\n+    _alias_idx(alias_idx) {}\n@@ -190,1 +188,0 @@\n-  bool needs_pinning() const { return _needs_pinning; }\n","filename":"src\/hotspot\/share\/gc\/shared\/c2\/barrierSetC2.hpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"oops\/stackChunkOop.hpp\"\n@@ -107,0 +108,4 @@\n+  \/\/ First, set it to java_lang_Object.\n+  \/\/ Then, set it to FillerObject after the FillerObject_klass loading is complete.\n+  static Klass* _filler_object_klass;\n+\n@@ -113,0 +118,2 @@\n+  \/\/ (Minimum) Alignment reserve for TLABs and PLABs.\n+  static size_t _lab_alignment_reserve;\n@@ -116,0 +123,2 @@\n+  static size_t _stack_chunk_max_size; \/\/ 0 for no limit\n+\n@@ -207,0 +216,12 @@\n+  static inline size_t stack_chunk_max_size() {\n+    return _stack_chunk_max_size;\n+  }\n+\n+  static inline Klass* filler_object_klass() {\n+    return _filler_object_klass;\n+  }\n+\n+  static inline void set_filler_object_klass(Klass* k) {\n+    _filler_object_klass = k;\n+  }\n+\n@@ -298,21 +319,3 @@\n-  size_t tlab_alloc_reserve() const;\n-\n-  \/\/ Some heaps may offer a contiguous region for shared non-blocking\n-  \/\/ allocation, via inlined code (by exporting the address of the top and\n-  \/\/ end fields defining the extent of the contiguous allocation region.)\n-\n-  \/\/ This function returns \"true\" iff the heap supports this kind of\n-  \/\/ allocation.  (Default is \"no\".)\n-  virtual bool supports_inline_contig_alloc() const {\n-    return false;\n-  }\n-  \/\/ These functions return the addresses of the fields that define the\n-  \/\/ boundaries of the contiguous allocation area.  (These fields should be\n-  \/\/ physically near to one another.)\n-  virtual HeapWord* volatile* top_addr() const {\n-    guarantee(false, \"inline contiguous allocation not supported\");\n-    return NULL;\n-  }\n-  virtual HeapWord** end_addr() const {\n-    guarantee(false, \"inline contiguous allocation not supported\");\n-    return NULL;\n+  static size_t lab_alignment_reserve() {\n+    assert(_lab_alignment_reserve != ~(size_t)0, \"uninitialized\");\n+    return _lab_alignment_reserve;\n@@ -377,0 +380,9 @@\n+  \/\/ Return true, if accesses to the object would require barriers.\n+  \/\/ This is used by continuations to copy chunks of a thread stack into StackChunk object or out of a StackChunk\n+  \/\/ object back into the thread stack. These chunks may contain references to objects. It is crucial that\n+  \/\/ the GC does not attempt to traverse the object while we modify it, because its structure (oopmap) is changed\n+  \/\/ when stack chunks are stored into it.\n+  \/\/ StackChunk objects may be reused, the GC must not assume that a StackChunk object is always a freshly\n+  \/\/ allocated object.\n+  virtual bool requires_barriers(stackChunkOop obj) const = 0;\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":33,"deletions":21,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"classfile\/vmClasses.hpp\"\n@@ -36,1 +37,1 @@\n-#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/continuationJavaClasses.inline.hpp\"\n@@ -38,1 +39,2 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -272,1 +274,1 @@\n-  HeapWord* mem = _thread->tlab().allocate(_word_size);\n+  HeapWord* mem = allocate_inside_tlab_fast();\n@@ -281,0 +283,4 @@\n+HeapWord* MemAllocator::allocate_inside_tlab_fast() const {\n+  return _thread->tlab().allocate(_word_size);\n+}\n+\n@@ -375,0 +381,15 @@\n+oop MemAllocator::try_allocate_in_existing_tlab() {\n+  oop obj = NULL;\n+  {\n+    HeapWord* mem = allocate_inside_tlab_fast();\n+    if (mem != NULL) {\n+      obj = initialize(mem);\n+    } else {\n+      \/\/ The unhandled oop detector will poison local variable obj,\n+      \/\/ so reset it to NULL if mem is NULL.\n+      obj = NULL;\n+    }\n+  }\n+  return obj;\n+}\n+\n@@ -434,0 +455,17 @@\n+\n+\/\/ Does the minimal amount of initialization needed for a TLAB allocation.\n+\/\/ We don't need to do a full initialization, as such an allocation need not be immediately walkable.\n+oop StackChunkAllocator::initialize(HeapWord* mem) const {\n+  assert(_stack_size > 0, \"\");\n+  assert(_stack_size <= max_jint, \"\");\n+  assert(_word_size > _stack_size, \"\");\n+\n+  \/\/ zero out fields (but not the stack)\n+  const size_t hs = oopDesc::header_size();\n+  Copy::fill_to_aligned_words(mem + hs, vmClasses::StackChunk_klass()->size_helper() - hs);\n+\n+  jdk_internal_vm_StackChunk::set_size(mem, (int)_stack_size);\n+  jdk_internal_vm_StackChunk::set_sp(mem, (int)_stack_size);\n+\n+  return finish(mem);\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":42,"deletions":4,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/thread.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -48,0 +48,1 @@\n+  HeapWord* allocate_inside_tlab_fast() const;\n@@ -75,0 +76,1 @@\n+  oop try_allocate_in_existing_tlab();\n@@ -115,0 +117,10 @@\n+class StackChunkAllocator : public MemAllocator {\n+  const size_t _stack_size;\n+\n+public:\n+  StackChunkAllocator(Klass* klass, size_t word_size, size_t stack_size, Thread* thread = Thread::current())\n+    : MemAllocator(klass, word_size, thread),\n+      _stack_size(stack_size) {}\n+  virtual oop initialize(HeapWord* mem) const;\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.hpp","additions":14,"deletions":2,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -392,2 +392,2 @@\n-      ciKlass* klass = itype->klass();\n-      if ( klass->is_loaded() &&\n+      ciKlass* klass = itype->instance_klass();\n+      if (klass->is_loaded() &&\n@@ -786,2 +786,2 @@\n-    ciInstanceKlass* ik = src_type->klass()->as_instance_klass();\n-    if ((src_type->klass_is_exact() || (!ik->is_interface() && !ik->has_subklass())) && !ik->has_injected_fields()) {\n+    ciInstanceKlass* ik = src_type->is_instptr()->instance_klass();\n+    if ((src_type->klass_is_exact() || !ik->has_subklass()) && !ik->has_injected_fields()) {\n@@ -799,2 +799,2 @@\n-    BasicType src_elem  = src_type->klass()->as_array_klass()->element_type()->basic_type();\n-    if (is_reference_type(src_elem)) {\n+    BasicType src_elem = src_type->isa_aryptr()->elem()->array_element_basic_type();\n+    if (is_reference_type(src_elem, true)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -301,1 +301,1 @@\n-                   adr_type->is_instptr()->klass()->is_subtype_of(Compile::current()->env()->Reference_klass()) &&\n+                   adr_type->is_instptr()->instance_klass()->is_subtype_of(Compile::current()->env()->Reference_klass()) &&\n@@ -1335,0 +1335,1 @@\n+    Node* raw_mem_for_ctrl = fixer.find_mem(ctrl, NULL);\n@@ -1361,1 +1362,1 @@\n-    \/\/ even for non-cset objects to prevent ressurrection of such objects.\n+    \/\/ even for non-cset objects to prevent resurrection of such objects.\n@@ -1436,0 +1437,1 @@\n+    fixer.record_new_ctrl(ctrl, region, raw_mem, raw_mem_for_ctrl);\n@@ -2670,0 +2672,7 @@\n+void MemoryGraphFixer::record_new_ctrl(Node* ctrl, Node* new_ctrl, Node* mem, Node* mem_for_ctrl) {\n+  if (mem_for_ctrl != mem && new_ctrl != ctrl) {\n+    _memory_nodes.map(ctrl->_idx, mem);\n+    _memory_nodes.map(new_ctrl->_idx, mem_for_ctrl);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2015, 2022, Red Hat, Inc. All rights reserved.\n@@ -87,6 +87,1 @@\n-    \/\/ TODO: It should not be necessary to check evac-in-progress here.\n-    \/\/ We do it for mark-compact, which may have forwarded objects,\n-    \/\/ and objects in cset and gets here via runtime barriers.\n-    \/\/ We can probably fix this as soon as mark-compact has its own\n-    \/\/ marking phase.\n-       Thread* t = Thread::current();\n+      Thread* t = Thread::current();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -146,1 +146,1 @@\n-  \/\/ will eventually be bound. Any lable will do, as it will only act as\n+  \/\/ will eventually be bound. Any label will do, as it will only act as\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n-#include \"runtime\/thread.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -84,0 +84,1 @@\n+    java_lang_continuation_doYield,                             \/\/ implementation of jdk.internal.vm.Continuation.doYield()\n@@ -160,0 +161,1 @@\n+      case vmIntrinsics::_Continuation_doYield : \/\/ fall thru\n@@ -161,0 +163,1 @@\n+\n","filename":"src\/hotspot\/share\/interpreter\/abstractInterpreter.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -173,0 +173,4 @@\n+  if (_closure == NULL) {\n+    return;\n+  }\n+\n@@ -192,0 +196,4 @@\n+  if (_closure == NULL) {\n+    return;\n+  }\n+\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeTracer.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -105,1 +105,1 @@\n-  \/\/ more, but 'local<i>'. Similary for 'this'.\n+  \/\/ more, but 'local<i>'. Similarly for 'this'.\n@@ -155,1 +155,1 @@\n-\/\/ exception occured.\n+\/\/ exception occurred.\n@@ -186,1 +186,1 @@\n-  \/\/ Merges the stack the the given bci with the given stack. If there\n+  \/\/ Merges the stack at the given bci with the given stack. If there\n@@ -1155,1 +1155,1 @@\n-        \/\/ Assume the the call of a constructor can never cause a NullPointerException\n+        \/\/ Assume the call of a constructor can never cause a NullPointerException\n","filename":"src\/hotspot\/share\/interpreter\/bytecodeUtils.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,10 +32,0 @@\n-\n-#if defined(WIN32) && (defined(_MSC_VER) && (_MSC_VER < 1600))\n-\/\/ Windows AMD64 Compiler Hangs compiling this file\n-\/\/ unless optimization is off\n-#ifdef _M_AMD64\n-#pragma optimize (\"\", off)\n-#endif\n-#endif\n-\n-\n","filename":"src\/hotspot\/share\/interpreter\/bytecodes.cpp","additions":1,"deletions":11,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -62,0 +62,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -400,1 +401,1 @@\n-  if (klass->is_being_initialized() && klass->is_reentrant_initialization(THREAD)) {\n+  if (klass->is_being_initialized() && klass->is_init_thread(THREAD)) {\n@@ -569,1 +570,1 @@\n-      Method::build_interpreter_method_data(trap_method, THREAD);\n+      Method::build_profiling_method_data(trap_method, THREAD);\n@@ -1378,1 +1379,1 @@\n-  \/\/ We used to need an explict preserve_arguments here for invoke bytecodes. However,\n+  \/\/ We used to need an explicit preserve_arguments here for invoke bytecodes. However,\n@@ -1385,0 +1386,4 @@\n+  if (java_lang_VirtualThread::notify_jvmti_events()) {\n+    JvmtiExport::check_vthread_and_suspend_at_safepoint(current);\n+  }\n+\n@@ -1514,1 +1519,1 @@\n-  return (Interpreter::contains(pc) ? 1 : 0);\n+  return (Interpreter::contains(Continuation::get_top_return_pc_post_barrier(JavaThread::current(), pc)) ? 1 : 0);\n@@ -1523,1 +1528,1 @@\n-\/\/ dependant code)\n+\/\/ dependent code)\n@@ -1584,1 +1589,1 @@\n-      \/\/ allow CPU dependant code to optimize the fingerprints for the fast handler\n+      \/\/ allow CPU dependent code to optimize the fingerprints for the fast handler\n@@ -1599,1 +1604,1 @@\n-          \/\/ debugging suppport\n+          \/\/ debugging support\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -33,1 +34,0 @@\n-#include \"runtime\/thread.hpp\"\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,0 @@\n-#include \"classfile\/resolutionErrors.hpp\"\n@@ -49,1 +48,1 @@\n-#include \"oops\/method.hpp\"\n+#include \"oops\/method.inline.hpp\"\n@@ -57,0 +56,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -59,0 +59,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -60,1 +61,4 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JFR\n+#include \"jfr\/jfr.hpp\"\n+#endif\n@@ -595,0 +599,10 @@\n+void LinkResolver::resolve_continuation_enter(CallInfo& callinfo, TRAPS) {\n+  Klass* resolved_klass = vmClasses::Continuation_klass();\n+  Symbol* method_name = vmSymbols::enter_name();\n+  Symbol* method_signature = vmSymbols::continuationEnter_signature();\n+  Klass*  current_klass = resolved_klass;\n+  LinkInfo link_info(resolved_klass, method_name, method_signature, current_klass);\n+  Method* resolved_method = resolve_method(link_info, Bytecodes::_invokestatic, CHECK);\n+  callinfo.set_static(resolved_klass, methodHandle(THREAD, resolved_method), CHECK);\n+}\n+\n@@ -816,1 +830,1 @@\n-\/\/ Do linktime resolution of a method in the interface within the context of the specied bytecode.\n+\/\/ Do linktime resolution of a method in the interface within the context of the specified bytecode.\n@@ -1101,0 +1115,7 @@\n+  if (resolved_method->is_continuation_enter_intrinsic()\n+      && resolved_method->from_interpreted_entry() == NULL) { \/\/ does a load_acquire\n+    methodHandle mh(THREAD, resolved_method);\n+    \/\/ Generate a compiled form of the enterSpecial intrinsic.\n+    AdapterHandlerLibrary::create_native_wrapper(mh);\n+  }\n+\n@@ -1103,0 +1124,1 @@\n+  JFR_ONLY(Jfr::on_resolution(result, CHECK);)\n@@ -1310,0 +1332,1 @@\n+  JFR_ONLY(Jfr::on_resolution(result, CHECK);)\n@@ -1430,0 +1453,1 @@\n+  JFR_ONLY(Jfr::on_resolution(result, CHECK);)\n@@ -1541,0 +1565,1 @@\n+  JFR_ONLY(Jfr::on_resolution(result, CHECK);)\n@@ -1713,0 +1738,1 @@\n+    JFR_ONLY(Jfr::on_resolution(result, CHECK_false);)\n@@ -1742,2 +1768,26 @@\n-  Method* resolved_method = lookup_polymorphic_method(link_info, &resolved_appendix, CHECK);\n-  result.set_handle(resolved_klass, methodHandle(THREAD, resolved_method), resolved_appendix, CHECK);\n+  Method* m = lookup_polymorphic_method(link_info, &resolved_appendix, CHECK);\n+  methodHandle resolved_method(THREAD, m);\n+\n+  if (link_info.check_access()) {\n+    Symbol* name = link_info.name();\n+    vmIntrinsics::ID iid = MethodHandles::signature_polymorphic_name_id(name);\n+    if (MethodHandles::is_signature_polymorphic_intrinsic(iid)) {\n+      \/\/ Check if method can be accessed by the referring class.\n+      \/\/ MH.linkTo* invocations are not rewritten to invokehandle.\n+      assert(iid == vmIntrinsicID::_invokeBasic, \"%s\", vmIntrinsics::name_at(iid));\n+\n+      Klass* current_klass = link_info.current_klass();\n+      assert(current_klass != NULL , \"current_klass should not be null\");\n+      check_method_accessability(current_klass,\n+                                 resolved_klass,\n+                                 resolved_method->method_holder(),\n+                                 resolved_method,\n+                                 CHECK);\n+    } else {\n+      \/\/ Java code is free to arbitrarily link signature-polymorphic invokers.\n+      assert(iid == vmIntrinsics::_invokeGeneric, \"not an invoker: %s\", vmIntrinsics::name_at(iid));\n+      assert(MethodHandles::is_signature_polymorphic_public_name(resolved_klass, name), \"not public\");\n+    }\n+  }\n+  result.set_handle(resolved_klass, resolved_method, resolved_appendix, CHECK);\n+  JFR_ONLY(Jfr::on_resolution(result, CHECK);)\n@@ -1779,1 +1829,1 @@\n-  \/\/ an ObjectLocker to do the final serialization of updates\n+  \/\/ a lock to do the final serialization of updates\n@@ -1825,0 +1875,1 @@\n+  JFR_ONLY(Jfr::on_resolution(result, CHECK);)\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.cpp","additions":59,"deletions":8,"binary":false,"changes":67,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -286,0 +286,2 @@\n+\n+  static void resolve_continuation_enter(CallInfo& callinfo, TRAPS);\n","filename":"src\/hotspot\/share\/interpreter\/linkResolver.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -181,1 +181,1 @@\n-  \/\/ The expection is that the bit mask was allocated\n+  \/\/ The expectation is that the bit mask was allocated\n@@ -205,0 +205,1 @@\n+  _num_oops  = 0;\n@@ -361,0 +362,1 @@\n+  _num_oops = 0;\n@@ -378,0 +380,1 @@\n+      _num_oops++;\n@@ -410,0 +413,1 @@\n+  _num_oops = from->num_oops();\n","filename":"src\/hotspot\/share\/interpreter\/oopMapCache.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -50,2 +50,2 @@\n-  \/\/ 270+ interpreter codelets are generated and each of them is required to be aligned to\n-  \/\/ CodeEntryAlignment twice. So we need additional size due to alignment.\n+  \/\/ 270+ interpreter codelets are generated and each of them is aligned to HeapWordSize,\n+  \/\/ plus their code section is aligned to CodeEntryAlignement. So we need additional size due to alignment.\n@@ -53,1 +53,1 @@\n-  int max_aligned_bytes = max_aligned_codelets * CodeEntryAlignment * 2;\n+  int max_aligned_bytes = max_aligned_codelets * (HeapWordSize + CodeEntryAlignment);\n@@ -290,1 +290,1 @@\n-\/\/ Suport for invokes\n+\/\/ Support for invokes\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreter.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -181,2 +181,2 @@\n-#define method_entry(kind)                                              \\\n-  { CodeletMark cm(_masm, \"method entry point (kind = \" #kind \")\"); \\\n+#define method_entry(kind)                                                                   \\\n+  { CodeletMark cm(_masm, \"method entry point (kind = \" #kind \")\");                          \\\n@@ -227,0 +227,2 @@\n+  method_entry(java_lang_continuation_doYield)\n+\n@@ -427,0 +429,2 @@\n+  case Interpreter::java_lang_continuation_doYield\n+                                           : entry_point = generate_Continuation_doYield_entry(); break;\n","filename":"src\/hotspot\/share\/interpreter\/templateInterpreterGenerator.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,0 +40,1 @@\n+#include \"runtime\/os.hpp\" \/\/ malloc\n@@ -55,3 +56,10 @@\n-VMReg CodeInstaller::getVMRegFromLocation(JVMCIObject location, int total_frame_size, JVMCI_TRAPS) {\n-  if (location.is_null()) {\n-    JVMCI_THROW_NULL(NullPointerException);\n+static bool is_set(u1 flags, u1 bit) {\n+  return flags & bit;\n+}\n+\n+oop HotSpotCompiledCodeStream::get_oop(int id, JVMCI_TRAPS) const {\n+  if (_object_pool.is_null()) {\n+    JVMCI_ERROR_NULL(\"object pool is null%s\", context());\n+  }\n+  if (!_object_pool.is_null() && 0 <= id && id < _object_pool->length()) {\n+    return _object_pool->obj_at(id);\n@@ -59,0 +67,2 @@\n+  JVMCI_ERROR_NULL(\"unknown direct object id %d%s\", id, context());\n+}\n@@ -60,2 +70,12 @@\n-  JVMCIObject reg = jvmci_env()->get_code_Location_reg(location);\n-  jint offset = jvmci_env()->get_code_Location_offset(location);\n+u4 HotSpotCompiledCodeStream::offset() const {\n+  u4 res = 0;\n+  for (Chunk* c = _head; c != nullptr; c = c->next()) {\n+    if (c == _chunk) {\n+      res += _pos - c->data();\n+      break;\n+    } else {\n+      res += c->size();\n+    }\n+  }\n+  return res;\n+}\n@@ -63,4 +83,142 @@\n-  if (reg.is_non_null()) {\n-    \/\/ register\n-    jint number = jvmci_env()->get_code_Register_number(reg);\n-    VMReg vmReg = CodeInstaller::get_hotspot_reg(number, JVMCI_CHECK_NULL);\n+bool HotSpotCompiledCodeStream::available() const {\n+  u4 rem = _chunk->data_end() - _pos;\n+  for (Chunk* c = _chunk->next(); c != nullptr; c = c->next()) {\n+    rem += c->size();\n+  }\n+  return rem;\n+}\n+\n+void HotSpotCompiledCodeStream::dump_buffer(outputStream* st) const {\n+  st->print_cr(\"HotSpotCompiledCode stream for %s:\", code_desc());\n+  int chunk_index = 0;\n+  for (Chunk* c = _head; c != nullptr; c = c->next()) {\n+    const u1* data     = c->data();\n+    const u1* data_end = c->data_end();\n+\n+    int to_dump = data_end - data;\n+    st->print_cr(\"# chunk %d, %d bytes\", chunk_index, to_dump);\n+    st->print_data((void*) data, to_dump, true, false);\n+    chunk_index++;\n+  }\n+}\n+\n+void HotSpotCompiledCodeStream::dump_buffer_tail(int len, outputStream* st) const {\n+  const u1* start;\n+  int avail = _pos - _chunk->data();\n+  if (len >= avail) {\n+    len = avail;\n+    start = _chunk->data();\n+  } else {\n+    start = _pos - len;\n+\n+    \/\/ Ensure start is 16-byte aligned wrt chunk start\n+    int start_offset = start - _chunk->data();\n+    start -= (start_offset % 16);\n+    len = _pos - start;\n+  }\n+\n+  st->print_cr(\"Last %d bytes up to current read position \" INTPTR_FORMAT \" in HotSpotCompiledCode stream for %s:\", len, p2i(_pos), code_desc());\n+  st->print_data((void*) start, len, true, false);\n+}\n+\n+const char* HotSpotCompiledCodeStream::context() const {\n+  stringStream st;\n+  st.cr();\n+  st.print_cr(\"at \" INTPTR_FORMAT \" in HotSpotCompiledCode stream\", p2i(_pos));\n+  dump_buffer_tail(100, &st);\n+  return st.as_string();\n+}\n+\n+void HotSpotCompiledCodeStream::before_read(u1 size) {\n+  if (_pos + size > _chunk->data_end()) {\n+    Chunk* next = _chunk->next();\n+    if (next == nullptr || size > next->size()) {\n+      dump_buffer();\n+      fatal(\"%s: reading %d bytes overflows buffer at \" INTPTR_FORMAT, code_desc(), size, p2i(_pos));\n+    }\n+    _chunk = next;\n+    _pos = _chunk->data();\n+  }\n+}\n+\n+\/\/ Reads a size followed by an ascii string from the stream and\n+\/\/ checks that they match `expect_size` and `expect_name` respectively. This\n+\/\/ implements a rudimentary type checking of the stream between the stream producer\n+\/\/ (Java) and the consumer (C++).\n+void HotSpotCompiledCodeStream::check_data(u2 expect_size, const char* expect_name) {\n+  u2 actual_size = get_u1();\n+  u2 ascii_len = get_u1();\n+  const char* actual_name = (const char*) _pos;\n+  char* end = (char*) _pos + ascii_len;\n+  _pos = (const u1*) end;\n+  if (strlen(expect_name) != ascii_len || strncmp(expect_name, actual_name, ascii_len) != 0) {\n+    dump_buffer();\n+    fatal(\"%s: expected \\\"%s\\\" at \" INTPTR_FORMAT \", got \\\"%.*s\\\" (len: %d)\",\n+        code_desc(), expect_name, p2i(actual_name), ascii_len, actual_name, ascii_len);\n+  }\n+  if (actual_size != expect_size) {\n+    dump_buffer();\n+    fatal(\"%s: expected \\\"%s\\\" at \" INTPTR_FORMAT \" to have size %u, got %u\",\n+        code_desc(), expect_name, p2i(actual_name), expect_size, actual_size);\n+  }\n+}\n+\n+const char* HotSpotCompiledCodeStream::read_utf8(const char* name, JVMCI_TRAPS) {\n+  jint utf_len = read_s4(name);\n+  if (utf_len == -1) {\n+    return nullptr;\n+  }\n+  guarantee(utf_len >= 0, \"bad utf_len: %d\", utf_len);\n+\n+  const char* utf = (const char*) _pos;\n+  char* end = (char*) _pos + utf_len;\n+  _pos = (const u1*) (end + 1);\n+  if (*end != '\\0') {\n+    JVMCI_ERROR_NULL(\"UTF8 string at \" INTPTR_FORMAT \" of length %d missing 0 terminator: \\\"%.*s\\\"%s\",\n+        p2i(utf), utf_len, utf_len, utf, context());\n+  }\n+  return utf;\n+}\n+\n+Method* HotSpotCompiledCodeStream::read_method(const char* name) {\n+  return (Method*) read_u8(name);\n+}\n+\n+Klass* HotSpotCompiledCodeStream::read_klass(const char* name) {\n+  return (Klass*) read_u8(name);\n+}\n+\n+ScopeValue* HotSpotCompiledCodeStream::virtual_object_at(int id, JVMCI_TRAPS) const {\n+  if (_virtual_objects == nullptr) {\n+    JVMCI_ERROR_NULL(\"virtual object id %d read outside scope of decoding DebugInfo%s\", id, context());\n+  }\n+  if (id < 0 || id >= _virtual_objects->length()) {\n+    JVMCI_ERROR_NULL(\"invalid virtual object id %d%s\", id, context());\n+  }\n+  return _virtual_objects->at(id);\n+}\n+\n+#ifndef PRODUCT\n+void CodeInstaller::verify_bci_constants(JVMCIEnv* env) {\n+#define CHECK_IN_SYNC(name) do { \\\n+  int expect = env->get_BytecodeFrame_ ## name ##_BCI(); \\\n+  int actual = name##_BCI; \\\n+  if (expect != actual) fatal(\"CodeInstaller::\" #name \"_BCI(%d) != BytecodeFrame.\" #name \"_BCI(%d)\", expect, actual); \\\n+} while(0)\n+\n+  CHECK_IN_SYNC(UNWIND);\n+  CHECK_IN_SYNC(BEFORE);\n+  CHECK_IN_SYNC(AFTER);\n+  CHECK_IN_SYNC(AFTER_EXCEPTION);\n+  CHECK_IN_SYNC(UNKNOWN);\n+  CHECK_IN_SYNC(INVALID_FRAMESTATE);\n+#undef CHECK_IN_SYNC\n+}\n+#endif\n+\n+VMReg CodeInstaller::getVMRegFromLocation(HotSpotCompiledCodeStream* stream, int total_frame_size, JVMCI_TRAPS) {\n+  u2 reg = stream->read_u2(\"register\");\n+  u2 offset = stream->read_u2(\"offset\");\n+\n+  if (reg != NO_REGISTER) {\n+    VMReg vmReg = CodeInstaller::get_hotspot_reg(reg, JVMCI_CHECK_NULL);\n@@ -70,1 +228,1 @@\n-      JVMCI_ERROR_NULL(\"unaligned subregister offset %d in oop map\", offset);\n+      JVMCI_ERROR_NULL(\"unaligned subregister offset %d in oop map%s\", offset, stream->context());\n@@ -73,1 +231,0 @@\n-    \/\/ stack slot\n@@ -83,2 +240,2 @@\n-        JVMCI_ERROR_NULL(\"stack offset %d is too large to be encoded in OopMap (max %d)\",\n-                         offset, CompilerToVM::Data::max_oop_map_stack_offset());\n+        JVMCI_ERROR_NULL(\"stack offset %d is too large to be encoded in OopMap (max %d)%s\",\n+                         offset, CompilerToVM::Data::max_oop_map_stack_offset(), stream->context());\n@@ -89,1 +246,1 @@\n-      JVMCI_ERROR_NULL(\"unaligned stack offset %d in oop map\", offset);\n+      JVMCI_ERROR_NULL(\"unaligned stack offset %d in oop map%s\", offset, stream->context());\n@@ -94,10 +251,4 @@\n-\/\/ creates a HotSpot oop map out of the byte arrays provided by DebugInfo\n-OopMap* CodeInstaller::create_oop_map(JVMCIObject debug_info, JVMCI_TRAPS) {\n-  JVMCIObject reference_map = jvmci_env()->get_DebugInfo_referenceMap(debug_info);\n-  if (reference_map.is_null()) {\n-    JVMCI_THROW_NULL(NullPointerException);\n-  }\n-  if (!jvmci_env()->isa_HotSpotReferenceMap(reference_map)) {\n-    JVMCI_ERROR_NULL(\"unknown reference map: %s\", jvmci_env()->klass_name(reference_map));\n-  }\n-  if (!_has_wide_vector && SharedRuntime::is_wide_vector(jvmci_env()->get_HotSpotReferenceMap_maxRegisterSize(reference_map))) {\n+OopMap* CodeInstaller::create_oop_map(HotSpotCompiledCodeStream* stream, u1 debug_info_flags, JVMCI_TRAPS) {\n+  assert(is_set(debug_info_flags, DI_HAS_REFERENCE_MAP), \"must be\");\n+  u2 max_register_size = stream->read_u2(\"maxRegisterSize\");\n+  if (!_has_wide_vector && SharedRuntime::is_wide_vector(max_register_size)) {\n@@ -105,1 +256,1 @@\n-      JVMCI_ERROR_NULL(\"JVMCI is producing code using vectors larger than the runtime supports\");\n+      JVMCI_ERROR_NULL(\"JVMCI is producing code using vectors larger than the runtime supports%s\", stream->context());\n@@ -109,14 +260,1 @@\n-  OopMap* map = new OopMap(_total_frame_size, _parameter_count);\n-  JVMCIObjectArray objects = jvmci_env()->get_HotSpotReferenceMap_objects(reference_map);\n-  JVMCIObjectArray derivedBase = jvmci_env()->get_HotSpotReferenceMap_derivedBase(reference_map);\n-  JVMCIPrimitiveArray sizeInBytes = jvmci_env()->get_HotSpotReferenceMap_sizeInBytes(reference_map);\n-  if (objects.is_null() || derivedBase.is_null() || sizeInBytes.is_null()) {\n-    JVMCI_THROW_NULL(NullPointerException);\n-  }\n-  if (JVMCIENV->get_length(objects) != JVMCIENV->get_length(derivedBase) || JVMCIENV->get_length(objects) != JVMCIENV->get_length(sizeInBytes)) {\n-    JVMCI_ERROR_NULL(\"arrays in reference map have different sizes: %d %d %d\", JVMCIENV->get_length(objects), JVMCIENV->get_length(derivedBase), JVMCIENV->get_length(sizeInBytes));\n-  }\n-  for (int i = 0; i < JVMCIENV->get_length(objects); i++) {\n-    JVMCIObject location = JVMCIENV->get_object_at(objects, i);\n-    JVMCIObject baseLocation = JVMCIENV->get_object_at(derivedBase, i);\n-    jint bytes = JVMCIENV->get_int_at(sizeInBytes, i);\n+  u2 length = stream->read_u2(\"referenceMap:length\");\n@@ -124,2 +262,6 @@\n-    VMReg vmReg = getVMRegFromLocation(location, _total_frame_size, JVMCI_CHECK_NULL);\n-    if (baseLocation.is_non_null()) {\n+  OopMap* map = new OopMap(_total_frame_size, _parameter_count);\n+  for (int i = 0; i < length; i++) {\n+    bool has_derived = stream->read_bool(\"hasDerived\");\n+    u2 bytes = stream->read_u2(\"sizeInBytes\");\n+    VMReg vmReg = getVMRegFromLocation(stream, _total_frame_size, JVMCI_CHECK_NULL);\n+    if (has_derived) {\n@@ -127,6 +269,2 @@\n-#ifdef _LP64\n-      if (bytes == 8) {\n-#else\n-      if (bytes == 4) {\n-#endif\n-        VMReg baseReg = getVMRegFromLocation(baseLocation, _total_frame_size, JVMCI_CHECK_NULL);\n+      if (bytes == LP64_ONLY(8) NOT_LP64(4)) {\n+        VMReg baseReg = getVMRegFromLocation(stream, _total_frame_size, JVMCI_CHECK_NULL);\n@@ -135,1 +273,1 @@\n-        JVMCI_ERROR_NULL(\"invalid derived oop size in ReferenceMap: %d\", bytes);\n+        JVMCI_ERROR_NULL(\"invalid derived oop size in ReferenceMap: %d%s\", bytes, stream->context());\n@@ -149,1 +287,1 @@\n-      JVMCI_ERROR_NULL(\"invalid oop size in ReferenceMap: %d\", bytes);\n+      JVMCI_ERROR_NULL(\"invalid oop size in ReferenceMap: %d%s\", bytes, stream->context());\n@@ -153,7 +291,4 @@\n-  JVMCIObject callee_save_info = jvmci_env()->get_DebugInfo_calleeSaveInfo(debug_info);\n-  if (callee_save_info.is_non_null()) {\n-    JVMCIObjectArray registers = jvmci_env()->get_RegisterSaveLayout_registers(callee_save_info);\n-    JVMCIPrimitiveArray slots = jvmci_env()->get_RegisterSaveLayout_slots(callee_save_info);\n-    for (jint i = 0; i < JVMCIENV->get_length(slots); i++) {\n-      JVMCIObject jvmci_reg = JVMCIENV->get_object_at(registers, i);\n-      jint jvmci_reg_number = jvmci_env()->get_code_Register_number(jvmci_reg);\n+  if (is_set(debug_info_flags, DI_HAS_CALLEE_SAVE_INFO)) {\n+    length = stream->read_u2(\"calleeSaveInfo:length\");\n+    for (jint i = 0; i < length; i++) {\n+      u2 jvmci_reg_number = stream->read_u2(\"register\");\n@@ -162,1 +297,1 @@\n-      jint jvmci_slot = JVMCIENV->get_int_at(slots, i);\n+      u2 jvmci_slot = stream->read_u2(\"slot\");\n@@ -176,1 +311,1 @@\n-void* CodeInstaller::record_metadata_reference(CodeSection* section, address dest, JVMCIObject constant, JVMCI_TRAPS) {\n+void* CodeInstaller::record_metadata_reference(CodeSection* section, address dest, HotSpotCompiledCodeStream* stream, u1 tag, JVMCI_TRAPS) {\n@@ -183,4 +318,2 @@\n-  JVMCIObject obj = jvmci_env()->get_HotSpotMetaspaceConstantImpl_metaspaceObject(constant);\n-  if (jvmci_env()->isa_HotSpotResolvedObjectTypeImpl(obj)) {\n-    Klass* klass = JVMCIENV->asKlass(obj);\n-    assert(!jvmci_env()->get_HotSpotMetaspaceConstantImpl_compressed(constant), \"unexpected compressed klass pointer %s @ \" INTPTR_FORMAT, klass->name()->as_C_string(), p2i(klass));\n+  if (tag == PATCH_KLASS) {\n+    Klass* klass = stream->read_klass(\"patch:klass\");\n@@ -191,3 +324,2 @@\n-  } else if (jvmci_env()->isa_HotSpotResolvedJavaMethodImpl(obj)) {\n-    Method* method = jvmci_env()->asMethod(obj);\n-    assert(!jvmci_env()->get_HotSpotMetaspaceConstantImpl_compressed(constant), \"unexpected compressed method pointer %s @ \" INTPTR_FORMAT, method->name()->as_C_string(), p2i(method));\n+  } else if (tag == PATCH_METHOD) {\n+    Method* method = stream->read_method(\"patch:method\");\n@@ -199,1 +331,1 @@\n-    JVMCI_ERROR_NULL(\"unexpected metadata reference for constant of type %s\", jvmci_env()->klass_name(obj));\n+    JVMCI_ERROR_NULL(\"unexpected metadata reference tag: %d%s\", tag, stream->context());\n@@ -204,6 +336,3 @@\n-narrowKlass CodeInstaller::record_narrow_metadata_reference(CodeSection* section, address dest, JVMCIObject constant, JVMCI_TRAPS) {\n-  JVMCIObject obj = jvmci_env()->get_HotSpotMetaspaceConstantImpl_metaspaceObject(constant);\n-  assert(jvmci_env()->get_HotSpotMetaspaceConstantImpl_compressed(constant), \"unexpected uncompressed pointer\");\n-\n-  if (!jvmci_env()->isa_HotSpotResolvedObjectTypeImpl(obj)) {\n-    JVMCI_ERROR_0(\"unexpected compressed pointer of type %s\", jvmci_env()->klass_name(obj));\n+narrowKlass CodeInstaller::record_narrow_metadata_reference(CodeSection* section, address dest, HotSpotCompiledCodeStream* stream, u1 tag, JVMCI_TRAPS) {\n+  if (tag != PATCH_NARROW_KLASS) {\n+    JVMCI_ERROR_0(\"unexpected compressed pointer tag %d%s\", tag, stream->context());\n@@ -211,2 +340,1 @@\n-\n-  Klass* klass = JVMCIENV->asKlass(obj);\n+  Klass* klass = stream->read_klass(\"patch:klass\");\n@@ -220,3 +348,18 @@\n-Location::Type CodeInstaller::get_oop_type(JVMCIObject value) {\n-  JVMCIObject valueKind = jvmci_env()->get_Value_valueKind(value);\n-  JVMCIObject platformKind = jvmci_env()->get_ValueKind_platformKind(valueKind);\n+ScopeValue* CodeInstaller::to_primitive_value(HotSpotCompiledCodeStream* stream, jlong raw, BasicType type, ScopeValue* &second, JVMCI_TRAPS) {\n+  if (type == T_INT || type == T_FLOAT) {\n+    jint prim = (jint) raw;\n+    switch (prim) {\n+      case -1: return _int_m1_scope_value;\n+      case  0: return _int_0_scope_value;\n+      case  1: return _int_1_scope_value;\n+      case  2: return _int_2_scope_value;\n+      default: return new ConstantIntValue(prim);\n+    }\n+  } else if (type == T_LONG || type == T_DOUBLE) {\n+    jlong prim = raw;\n+    second = _int_1_scope_value;\n+    return new ConstantLongValue(prim);\n+  } else {\n+    JVMCI_ERROR_NULL(\"unexpected primitive constant type %s%s\", basictype_to_str(type), stream->context());\n+  }\n+}\n@@ -224,2 +367,14 @@\n-  if (jvmci_env()->equals(platformKind, word_kind())) {\n-    return Location::oop;\n+Handle CodeInstaller::read_oop(HotSpotCompiledCodeStream* stream, u1 tag, JVMCI_TRAPS) {\n+  oop obj;\n+  if (tag == OBJECT_ID) {\n+    obj = stream->get_oop(stream->read_u1(\"id\"), JVMCI_CHECK_(Handle()));\n+  } else if (tag == OBJECT_ID2) {\n+    obj = stream->get_oop(stream->read_u2(\"id:2\"), JVMCI_CHECK_(Handle()));\n+  } else if (tag == JOBJECT) {\n+    jlong object_handle = stream->read_u8(\"jobject\");\n+    obj = jvmci_env()->resolve_oop_handle(object_handle);\n+  } else {\n+    JVMCI_ERROR_(Handle(), \"unexpected oop tag: %d\", tag)\n+  }\n+  if (obj == nullptr) {\n+    JVMCI_THROW_MSG_(InternalError, \"Constant was unexpectedly NULL\", Handle());\n@@ -227,1 +382,1 @@\n-    return Location::narrowoop;\n+    oopDesc::verify(obj);\n@@ -229,0 +384,1 @@\n+  return Handle(stream->thread(), obj);\n@@ -231,1 +387,1 @@\n-ScopeValue* CodeInstaller::get_scope_value(JVMCIObject value, BasicType type, GrowableArray<ScopeValue*>* objects, ScopeValue* &second, JVMCI_TRAPS) {\n+ScopeValue* CodeInstaller::get_scope_value(HotSpotCompiledCodeStream* stream, u1 tag, BasicType type, ScopeValue* &second, JVMCI_TRAPS) {\n@@ -233,5 +389,6 @@\n-  if (value.is_null()) {\n-    JVMCI_THROW_NULL(NullPointerException);\n-  } else if (JVMCIENV->equals(value, jvmci_env()->get_Value_ILLEGAL())) {\n-    if (type != T_ILLEGAL) {\n-      JVMCI_ERROR_NULL(\"unexpected illegal value, expected %s\", basictype_to_str(type));\n+  switch (tag) {\n+    case ILLEGAL: {\n+      if (type != T_ILLEGAL) {\n+        JVMCI_ERROR_NULL(\"unexpected illegal value, expected %s%s\", basictype_to_str(type), stream->context());\n+      }\n+      return _illegal_value;\n@@ -239,13 +396,21 @@\n-    return _illegal_value;\n-  } else if (jvmci_env()->isa_RegisterValue(value)) {\n-    JVMCIObject reg = jvmci_env()->get_RegisterValue_reg(value);\n-    jint number = jvmci_env()->get_code_Register_number(reg);\n-    VMReg hotspotRegister = get_hotspot_reg(number, JVMCI_CHECK_NULL);\n-    if (is_general_purpose_reg(hotspotRegister)) {\n-      Location::Type locationType;\n-      if (type == T_OBJECT) {\n-        locationType = get_oop_type(value);\n-      } else if (type == T_LONG) {\n-        locationType = Location::lng;\n-      } else if (type == T_INT || type == T_FLOAT || type == T_SHORT || type == T_CHAR || type == T_BYTE || type == T_BOOLEAN) {\n-        locationType = Location::int_in_long;\n+    case REGISTER_PRIMITIVE:\n+    case REGISTER_NARROW_OOP:\n+    case REGISTER_OOP: {\n+      u2 number = stream->read_u2(\"register\");\n+      VMReg hotspotRegister = get_hotspot_reg(number, JVMCI_CHECK_NULL);\n+      if (is_general_purpose_reg(hotspotRegister)) {\n+        Location::Type locationType;\n+        if (type == T_OBJECT) {\n+          locationType = tag == REGISTER_NARROW_OOP ? Location::narrowoop : Location::oop;\n+        } else if (type == T_LONG) {\n+          locationType = Location::lng;\n+        } else if (type == T_INT || type == T_FLOAT || type == T_SHORT || type == T_CHAR || type == T_BYTE || type == T_BOOLEAN) {\n+          locationType = Location::int_in_long;\n+        } else {\n+          JVMCI_ERROR_NULL(\"unexpected type %s in CPU register%s\", basictype_to_str(type), stream->context());\n+        }\n+        ScopeValue* value = new LocationValue(Location::new_reg_loc(locationType, hotspotRegister));\n+        if (type == T_LONG) {\n+          second = value;\n+        }\n+        return value;\n@@ -253,1 +418,14 @@\n-        JVMCI_ERROR_NULL(\"unexpected type %s in cpu register\", basictype_to_str(type));\n+        Location::Type locationType;\n+        if (type == T_FLOAT) {\n+          \/\/ this seems weird, but the same value is used in c1_LinearScan\n+          locationType = Location::normal;\n+        } else if (type == T_DOUBLE) {\n+          locationType = Location::dbl;\n+        } else {\n+          JVMCI_ERROR_NULL(\"unexpected type %s in floating point register%s\", basictype_to_str(type), stream->context());\n+        }\n+        ScopeValue* value = new LocationValue(Location::new_reg_loc(locationType, hotspotRegister));\n+        if (type == T_DOUBLE) {\n+          second = value;\n+        }\n+        return value;\n@@ -255,3 +433,7 @@\n-      ScopeValue* value = new LocationValue(Location::new_reg_loc(locationType, hotspotRegister));\n-      if (type == T_LONG) {\n-        second = value;\n+    }\n+    case STACK_SLOT_PRIMITIVE:\n+    case STACK_SLOT_NARROW_OOP:\n+    case STACK_SLOT_OOP: {\n+      jint offset = (jshort) stream->read_s2(\"offset\");\n+      if (stream->read_bool(\"addRawFrameSize\")) {\n+        offset += _total_frame_size;\n@@ -259,5 +441,4 @@\n-      return value;\n-    } else {\n-      if (type == T_FLOAT) {\n-        \/\/ this seems weird, but the same value is used in c1_LinearScan\n-        locationType = Location::normal;\n+      if (type == T_OBJECT) {\n+        locationType = tag == STACK_SLOT_NARROW_OOP ? Location::narrowoop : Location::oop;\n+      } else if (type == T_LONG) {\n+        locationType = Location::lng;\n@@ -267,0 +448,2 @@\n+      } else if (type == T_INT || type == T_FLOAT || type == T_SHORT || type == T_CHAR || type == T_BYTE || type == T_BOOLEAN) {\n+        locationType = Location::normal;\n@@ -268,1 +451,1 @@\n-        JVMCI_ERROR_NULL(\"unexpected type %s in floating point register\", basictype_to_str(type));\n+        JVMCI_ERROR_NULL(\"unexpected type %s in stack slot%s\", basictype_to_str(type), stream->context());\n@@ -270,2 +453,2 @@\n-      ScopeValue* value = new LocationValue(Location::new_reg_loc(locationType, hotspotRegister));\n-      if (type == T_DOUBLE) {\n+      ScopeValue* value = new LocationValue(Location::new_stk_loc(locationType, offset));\n+      if (type == T_DOUBLE || type == T_LONG) {\n@@ -276,66 +459,13 @@\n-  } else if (jvmci_env()->isa_StackSlot(value)) {\n-    jint offset = jvmci_env()->get_StackSlot_offset(value);\n-    if (jvmci_env()->get_StackSlot_addFrameSize(value)) {\n-      offset += _total_frame_size;\n-    }\n-\n-    Location::Type locationType;\n-    if (type == T_OBJECT) {\n-      locationType = get_oop_type(value);\n-    } else if (type == T_LONG) {\n-      locationType = Location::lng;\n-    } else if (type == T_DOUBLE) {\n-      locationType = Location::dbl;\n-    } else if (type == T_INT || type == T_FLOAT || type == T_SHORT || type == T_CHAR || type == T_BYTE || type == T_BOOLEAN) {\n-      locationType = Location::normal;\n-    } else {\n-      JVMCI_ERROR_NULL(\"unexpected type %s in stack slot\", basictype_to_str(type));\n-    }\n-    ScopeValue* value = new LocationValue(Location::new_stk_loc(locationType, offset));\n-    if (type == T_DOUBLE || type == T_LONG) {\n-      second = value;\n-    }\n-    return value;\n-  } else if (jvmci_env()->isa_JavaConstant(value)) {\n-    if (jvmci_env()->isa_PrimitiveConstant(value)) {\n-      if (jvmci_env()->isa_RawConstant(value)) {\n-        jlong prim = jvmci_env()->get_PrimitiveConstant_primitive(value);\n-        return new ConstantLongValue(prim);\n-      } else {\n-        BasicType constantType = jvmci_env()->kindToBasicType(jvmci_env()->get_PrimitiveConstant_kind(value), JVMCI_CHECK_NULL);\n-        if (type != constantType) {\n-          JVMCI_ERROR_NULL(\"primitive constant type doesn't match, expected %s but got %s\", basictype_to_str(type), basictype_to_str(constantType));\n-        }\n-        if (type == T_INT || type == T_FLOAT) {\n-          jint prim = (jint)jvmci_env()->get_PrimitiveConstant_primitive(value);\n-          switch (prim) {\n-            case -1: return _int_m1_scope_value;\n-            case  0: return _int_0_scope_value;\n-            case  1: return _int_1_scope_value;\n-            case  2: return _int_2_scope_value;\n-            default: return new ConstantIntValue(prim);\n-          }\n-        } else if (type == T_LONG || type == T_DOUBLE) {\n-          jlong prim = jvmci_env()->get_PrimitiveConstant_primitive(value);\n-          second = _int_1_scope_value;\n-          return new ConstantLongValue(prim);\n-        } else {\n-          JVMCI_ERROR_NULL(\"unexpected primitive constant type %s\", basictype_to_str(type));\n-        }\n-      }\n-    } else if (jvmci_env()->isa_NullConstant(value) || jvmci_env()->isa_HotSpotCompressedNullConstant(value)) {\n-      if (type == T_OBJECT) {\n-        return _oop_null_scope_value;\n-      } else {\n-        JVMCI_ERROR_NULL(\"unexpected null constant, expected %s\", basictype_to_str(type));\n-      }\n-    } else if (jvmci_env()->isa_HotSpotObjectConstantImpl(value)) {\n-      if (type == T_OBJECT) {\n-        Handle obj = jvmci_env()->asConstant(value, JVMCI_CHECK_NULL);\n-        if (obj == NULL) {\n-          JVMCI_ERROR_NULL(\"null value must be in NullConstant\");\n-        }\n-        return new ConstantOopWriteValue(JNIHandles::make_local(obj()));\n-      } else {\n-        JVMCI_ERROR_NULL(\"unexpected object constant, expected %s\", basictype_to_str(type));\n-      }\n+    case NULL_CONSTANT:      { return _oop_null_scope_value; }\n+    case RAW_CONSTANT:       { return new ConstantLongValue(stream->read_u8(\"primitive\")); }\n+    case PRIMITIVE_0:        { ScopeValue* v = to_primitive_value(stream, 0, type, second, JVMCI_CHECK_NULL); return v; }\n+    case PRIMITIVE4:         { ScopeValue* v = to_primitive_value(stream, stream->read_s4(\"primitive4\"), type, second, JVMCI_CHECK_NULL); return v; }\n+    case PRIMITIVE8:         { ScopeValue* v = to_primitive_value(stream, stream->read_s8(\"primitive8\"), type, second, JVMCI_CHECK_NULL); return v; }\n+    case VIRTUAL_OBJECT_ID:  { ScopeValue* v = stream->virtual_object_at(stream->read_u1(\"id\"),   JVMCI_CHECK_NULL); return v; }\n+    case VIRTUAL_OBJECT_ID2: { ScopeValue* v = stream->virtual_object_at(stream->read_u2(\"id:2\"), JVMCI_CHECK_NULL); return v; }\n+\n+    case OBJECT_ID:\n+    case OBJECT_ID2:\n+    case JOBJECT: {\n+      Handle obj = read_oop(stream, tag, JVMCI_CHECK_NULL);\n+      return new ConstantOopWriteValue(JNIHandles::make_local(obj()));\n@@ -343,12 +473,2 @@\n-  } else if (jvmci_env()->isa_VirtualObject(value)) {\n-    if (type == T_OBJECT) {\n-      int id = jvmci_env()->get_VirtualObject_id(value);\n-      if (0 <= id && id < objects->length()) {\n-        ScopeValue* object = objects->at(id);\n-        if (object != NULL) {\n-          return object;\n-        }\n-      }\n-      JVMCI_ERROR_NULL(\"unknown virtual object id %d\", id);\n-    } else {\n-      JVMCI_ERROR_NULL(\"unexpected virtual object, expected %s\", basictype_to_str(type));\n+    default: {\n+      JVMCI_ERROR_NULL(\"unexpected tag in scope: %d%s\", tag, stream->context())\n@@ -357,2 +477,0 @@\n-\n-  JVMCI_ERROR_NULL(\"unexpected value in scope: %s\", jvmci_env()->klass_name(value))\n@@ -361,4 +479,3 @@\n-void CodeInstaller::record_object_value(ObjectValue* sv, JVMCIObject value, GrowableArray<ScopeValue*>* objects, JVMCI_TRAPS) {\n-  JVMCIObject type = jvmci_env()->get_VirtualObject_type(value);\n-  int id = jvmci_env()->get_VirtualObject_id(value);\n-  Klass* klass = JVMCIENV->asKlass(type);\n+void CodeInstaller::record_object_value(ObjectValue* sv, HotSpotCompiledCodeStream* stream, JVMCI_TRAPS) {\n+  oop javaMirror = JNIHandles::resolve(sv->klass()->as_ConstantOopWriteValue()->value());\n+  Klass* klass = java_lang_Class::as_Klass(javaMirror);\n@@ -368,3 +485,2 @@\n-  JVMCIObjectArray values = jvmci_env()->get_VirtualObject_values(value);\n-  JVMCIObjectArray slotKinds = jvmci_env()->get_VirtualObject_slotKinds(value);\n-  for (jint i = 0; i < JVMCIENV->get_length(values); i++) {\n+  u2 length = stream->read_u2(\"values:length\");\n+  for (jint i = 0; i < length; i++) {\n@@ -372,2 +488,1 @@\n-    JVMCIObject object = JVMCIENV->get_object_at(values, i);\n-    BasicType type = jvmci_env()->kindToBasicType(JVMCIENV->get_object_at(slotKinds, i), JVMCI_CHECK);\n+    BasicType type = (BasicType) stream->read_u1(\"basicType\");\n@@ -375,1 +490,2 @@\n-    if (JVMCIENV->equals(object, jvmci_env()->get_Value_ILLEGAL())) {\n+    u1 tag = stream->read_u1(\"tag\");\n+    if (tag == ILLEGAL) {\n@@ -390,1 +506,1 @@\n-      value = get_scope_value(object, type, objects, cur_second, JVMCI_CHECK);\n+      value = get_scope_value(stream, tag, type, cur_second, JVMCI_CHECK);\n@@ -413,3 +529,12 @@\n-MonitorValue* CodeInstaller::get_monitor_value(JVMCIObject value, GrowableArray<ScopeValue*>* objects, JVMCI_TRAPS) {\n-  if (value.is_null()) {\n-    JVMCI_THROW_NULL(NullPointerException);\n+GrowableArray<ScopeValue*>* CodeInstaller::read_local_or_stack_values(HotSpotCompiledCodeStream* stream, u1 frame_flags, bool is_locals, JVMCI_TRAPS) {\n+  u2 length;\n+  if (is_locals) {\n+    if (!is_set(frame_flags, DIF_HAS_LOCALS)) {\n+      return nullptr;\n+    }\n+    length = stream->read_u2(\"numLocals\");\n+  } else {\n+    if (!is_set(frame_flags, DIF_HAS_STACK)) {\n+      return nullptr;\n+    }\n+    length = stream->read_u2(\"numStack\");\n@@ -417,2 +542,19 @@\n-  if (!jvmci_env()->isa_StackLockValue(value)) {\n-    JVMCI_ERROR_NULL(\"Monitors must be of type StackLockValue, got %s\", jvmci_env()->klass_name(value));\n+  GrowableArray<ScopeValue*>* values = new GrowableArray<ScopeValue*> (length);\n+  for (int i = 0; i < length; i++) {\n+    ScopeValue* second = nullptr;\n+    BasicType type = (BasicType) stream->read_u1(\"basicType\");\n+    u1 tag = stream->read_u1(\"tag\");\n+    ScopeValue* first = get_scope_value(stream, tag, type, second, JVMCI_CHECK_NULL);\n+    if (second != nullptr) {\n+      if (i == length) {\n+        JVMCI_ERROR_NULL(\"double-slot value not followed by Value.ILLEGAL%s\", stream->context());\n+      }\n+      i++;\n+      stream->read_u1(\"basicType\");\n+      tag = stream->read_u1(\"tag\");\n+      if (tag != ILLEGAL) {\n+        JVMCI_ERROR_NULL(\"double-slot value not followed by Value.ILLEGAL%s\", stream->context());\n+      }\n+      values->append(second);\n+    }\n+    values->append(first);\n@@ -420,0 +562,2 @@\n+  return values;\n+}\n@@ -421,12 +565,3 @@\n-  ScopeValue* second = NULL;\n-  ScopeValue* owner_value = get_scope_value(jvmci_env()->get_StackLockValue_owner(value), T_OBJECT, objects, second, JVMCI_CHECK_NULL);\n-  assert(second == NULL, \"monitor cannot occupy two stack slots\");\n-\n-  ScopeValue* lock_data_value = get_scope_value(jvmci_env()->get_StackLockValue_slot(value), T_LONG, objects, second, JVMCI_CHECK_NULL);\n-  assert(second == lock_data_value, \"monitor is LONG value that occupies two stack slots\");\n-  assert(lock_data_value->is_location(), \"invalid monitor location\");\n-  Location lock_data_loc = ((LocationValue*)lock_data_value)->location();\n-\n-  bool eliminated = false;\n-  if (jvmci_env()->get_StackLockValue_eliminated(value)) {\n-    eliminated = true;\n+GrowableArray<MonitorValue*>* CodeInstaller::read_monitor_values(HotSpotCompiledCodeStream* stream, u1 frame_flags, JVMCI_TRAPS) {\n+  if (!is_set(frame_flags, DIF_HAS_LOCKS)) {\n+    return nullptr;\n@@ -434,2 +569,19 @@\n-\n-  return new MonitorValue(owner_value, lock_data_loc, eliminated);\n+  if (!_has_monitors) {\n+    _has_monitors = true;\n+  }\n+  u2 length = stream->read_u2(\"numLocks\");\n+  GrowableArray<MonitorValue*>* monitors = new GrowableArray<MonitorValue*>(length);\n+  for (int i = 0; i < length; i++) {\n+    bool eliminated = stream->read_bool(\"isEliminated\");\n+    ScopeValue* second = NULL;\n+    ScopeValue* owner_value = get_scope_value(stream, stream->read_u1(\"tag\"), T_OBJECT, second, JVMCI_CHECK_NULL);\n+    assert(second == NULL, \"monitor cannot occupy two stack slots\");\n+\n+    ScopeValue* lock_data_value = get_scope_value(stream, stream->read_u1(\"tag\"), T_LONG, second, JVMCI_CHECK_NULL);\n+    assert(second == lock_data_value, \"monitor is LONG value that occupies two stack slots\");\n+    assert(lock_data_value->is_location(), \"invalid monitor location\");\n+    Location lock_data_loc = ((LocationValue*) lock_data_value)->location();\n+\n+    monitors->append(new MonitorValue(owner_value, lock_data_loc, eliminated));\n+  }\n+  return monitors;\n@@ -438,2 +590,2 @@\n-void CodeInstaller::initialize_dependencies(JVMCIObject compiled_code, OopRecorder* oop_recorder, JVMCI_TRAPS) {\n-  JavaThread* thread = JavaThread::current();\n+void CodeInstaller::initialize_dependencies(HotSpotCompiledCodeStream* stream, u1 code_flags, OopRecorder* oop_recorder, JVMCI_TRAPS) {\n+  JavaThread* thread = stream->thread();\n@@ -443,3 +595,2 @@\n-  JVMCIObjectArray assumptions = jvmci_env()->get_HotSpotCompiledCode_assumptions(compiled_code);\n-  if (assumptions.is_non_null()) {\n-    int length = JVMCIENV->get_length(assumptions);\n+  if (is_set(code_flags, HCC_HAS_ASSUMPTIONS)) {\n+    u2 length = stream->read_u2(\"assumptions:length\");\n@@ -447,14 +598,35 @@\n-      JVMCIObject assumption = JVMCIENV->get_object_at(assumptions, i);\n-      if (assumption.is_non_null()) {\n-        if (jvmci_env()->isa_Assumptions_NoFinalizableSubclass(assumption)) {\n-          assumption_NoFinalizableSubclass(assumption);\n-        } else if (jvmci_env()->isa_Assumptions_ConcreteSubtype(assumption)) {\n-          assumption_ConcreteSubtype(assumption);\n-        } else if (jvmci_env()->isa_Assumptions_LeafType(assumption)) {\n-          assumption_LeafType(assumption);\n-        } else if (jvmci_env()->isa_Assumptions_ConcreteMethod(assumption)) {\n-          assumption_ConcreteMethod(assumption);\n-        } else if (jvmci_env()->isa_Assumptions_CallSiteTargetValue(assumption)) {\n-          assumption_CallSiteTargetValue(assumption, JVMCI_CHECK);\n-        } else {\n-          JVMCI_ERROR(\"unexpected Assumption subclass %s\", jvmci_env()->klass_name(assumption));\n+      u1 tag = stream->read_u1(\"tag\");\n+      switch (tag) {\n+        case NO_FINALIZABLE_SUBCLASS: {\n+          Klass* receiver_type = stream->read_klass(\"receiverType\");\n+          _dependencies->assert_has_no_finalizable_subclasses(receiver_type);\n+          break;\n+        }\n+        case CONCRETE_SUBTYPE: {\n+          Klass* context = stream->read_klass(\"context\");\n+          Klass* subtype = stream->read_klass(\"subtype\");\n+          assert(context->is_abstract(), \"must be\");\n+          _dependencies->assert_abstract_with_unique_concrete_subtype(context, subtype);\n+          break;\n+        }\n+        case LEAF_TYPE: {\n+          Klass* context = stream->read_klass(\"context\");\n+          _dependencies->assert_leaf_type(context);\n+          break;\n+        }\n+        case CONCRETE_METHOD: {\n+          Klass* context = stream->read_klass(\"context\");\n+          Method* impl = stream->read_method(\"impl\");\n+          _dependencies->assert_unique_concrete_method(context, impl);\n+          break;\n+        }\n+        case CALLSITE_TARGET_VALUE: {\n+          u1 obj_tag = stream->read_u1(\"tag\");\n+          Handle callSite = read_oop(stream, obj_tag, JVMCI_CHECK);\n+          obj_tag = stream->read_u1(\"tag\");\n+          Handle methodHandle = read_oop(stream, obj_tag, JVMCI_CHECK);\n+          _dependencies->assert_call_site_target_value(callSite(), methodHandle());\n+          break;\n+        }\n+        default: {\n+          JVMCI_ERROR(\"unexpected assumption tag %d%s\", tag, stream->context());\n@@ -465,7 +637,5 @@\n-  if (JvmtiExport::can_hotswap_or_post_breakpoint()) {\n-    JVMCIObjectArray methods = jvmci_env()->get_HotSpotCompiledCode_methods(compiled_code);\n-    if (methods.is_non_null()) {\n-      int length = JVMCIENV->get_length(methods);\n-      for (int i = 0; i < length; ++i) {\n-        JVMCIObject method_handle = JVMCIENV->get_object_at(methods, i);\n-        Method* method = jvmci_env()->asMethod(method_handle);\n+  if (is_set(code_flags, HCC_HAS_METHODS)) {\n+    u2 length = stream->read_u2(\"methods:length\");\n+    for (int i = 0; i < length; ++i) {\n+      Method* method = stream->read_method(\"method\");\n+      if (JvmtiExport::can_hotswap_or_post_breakpoint()) {\n@@ -478,2 +648,2 @@\n-\/\/ constructor used to create a method\n-    JVMCIObject target,\n+    jlong compiled_code_buffer,\n+    bool with_type_info,\n@@ -482,0 +652,1 @@\n+    objArrayHandle object_pool,\n@@ -490,0 +661,22 @@\n+  JavaThread* thread = JavaThread::current();\n+  HotSpotCompiledCodeStream* stream = new HotSpotCompiledCodeStream(thread, (const u1*) compiled_code_buffer, with_type_info, object_pool);\n+\n+  u1 code_flags = stream->read_u1(\"code:flags\");\n+  bool is_nmethod = is_set(code_flags, HCC_IS_NMETHOD);\n+  const char* name = stream->read_utf8(\"name\", JVMCI_CHECK_OK);\n+\n+  methodHandle method;\n+  jint entry_bci = -1;\n+  JVMCICompileState* compile_state = nullptr;\n+  bool has_unsafe_access = false;\n+  jint id = -1;\n+\n+  if (is_nmethod) {\n+    method = methodHandle(thread, stream->read_method(\"method\"));\n+    entry_bci = is_nmethod ? stream->read_s4(\"entryBCI\") : -1;\n+    compile_state = (JVMCICompileState*) stream->read_u8(\"compileState\");\n+    has_unsafe_access = stream->read_bool(\"hasUnsafeAccess\");\n+    id = stream->read_s4(\"id\");\n+  }\n+  stream->set_code_desc(name, method);\n+\n@@ -492,1 +685,1 @@\n-  initialize_dependencies(compiled_code, recorder, JVMCI_CHECK_OK);\n+  initialize_dependencies(stream, code_flags, recorder, JVMCI_CHECK_OK);\n@@ -498,2 +691,8 @@\n-  initialize_fields(target, compiled_code, JVMCI_CHECK_OK);\n-  JVMCI::CodeInstallResult result = initialize_buffer(buffer, true, JVMCI_CHECK_OK);\n+  initialize_fields(stream, code_flags, method, JVMCI_CHECK_OK);\n+  JVMCI::CodeInstallResult result = initialize_buffer(compiled_code, buffer, stream, code_flags, JVMCI_CHECK_OK);\n+\n+  u4 available = stream->available();\n+  if (result == JVMCI::ok && available != 0) {\n+    JVMCI_ERROR_OK(\"%d bytes remaining in stream%s\", available, stream->context());\n+  }\n+\n@@ -506,3 +705,2 @@\n-  if (!jvmci_env()->isa_HotSpotCompiledNmethod(compiled_code)) {\n-    JVMCIObject stubName = jvmci_env()->get_HotSpotCompiledCode_name(compiled_code);\n-    if (stubName.is_null()) {\n+  if (!is_nmethod) {\n+    if (name == nullptr) {\n@@ -511,1 +709,1 @@\n-    char* name = strdup(jvmci_env()->as_utf8_string(stubName));\n+    name = os::strdup(name); \/\/ Note: this leaks. See JDK-8289632\n@@ -520,1 +718,0 @@\n-    JVMCICompileState* compile_state = (JVMCICompileState*) (address) jvmci_env()->get_HotSpotCompiledNmethod_compileState(compiled_code);\n@@ -525,6 +722,0 @@\n-    Thread* thread = Thread::current();\n-\n-    methodHandle method(thread, jvmci_env()->asMethod(jvmci_env()->get_HotSpotCompiledNmethod_method(compiled_code)));\n-    jint entry_bci = jvmci_env()->get_HotSpotCompiledNmethod_entryBCI(compiled_code);\n-    bool has_unsafe_access = jvmci_env()->get_HotSpotCompiledNmethod_hasUnsafeAccess(compiled_code) == JNI_TRUE;\n-    jint id = jvmci_env()->get_HotSpotCompiledNmethod_id(compiled_code);\n@@ -541,5 +732,23 @@\n-    result = runtime()->register_method(jvmci_env(), method, nmethod_handle, entry_bci, &_offsets, _orig_pc_offset, &buffer,\n-                                        stack_slots, _debug_recorder->_oopmaps, &_exception_handler_table, &_implicit_exception_table,\n-                                        compiler, _debug_recorder, _dependencies, id,\n-                                        has_unsafe_access, _has_wide_vector, compiled_code, mirror,\n-                                        failed_speculations, speculations, speculations_len);\n+    result = runtime()->register_method(jvmci_env(),\n+                                        method,\n+                                        nmethod_handle,\n+                                        entry_bci,\n+                                        &_offsets,\n+                                        _orig_pc_offset,\n+                                        &buffer,\n+                                        stack_slots,\n+                                        _debug_recorder->_oopmaps,\n+                                        &_exception_handler_table,\n+                                        &_implicit_exception_table,\n+                                        compiler,\n+                                        _debug_recorder,\n+                                        _dependencies,\n+                                        id,\n+                                        _has_monitors,\n+                                        has_unsafe_access,\n+                                        _has_wide_vector,\n+                                        compiled_code,\n+                                        mirror,\n+                                        failed_speculations,\n+                                        speculations,\n+                                        speculations_len);\n@@ -565,5 +774,2 @@\n-void CodeInstaller::initialize_fields(JVMCIObject target, JVMCIObject compiled_code, JVMCI_TRAPS) {\n-  if (jvmci_env()->isa_HotSpotCompiledNmethod(compiled_code)) {\n-    JVMCIObject hotspotJavaMethod = jvmci_env()->get_HotSpotCompiledNmethod_method(compiled_code);\n-    Thread* thread = Thread::current();\n-    methodHandle method(thread, jvmci_env()->asMethod(hotspotJavaMethod));\n+void CodeInstaller::initialize_fields(HotSpotCompiledCodeStream* stream, u1 code_flags, methodHandle& method, JVMCI_TRAPS) {\n+  if (!method.is_null()) {\n@@ -573,1 +779,1 @@\n-    \/\/ Must be a HotSpotCompiledRuntimeStub.\n+    \/\/ Must be a HotSpotCompiledCode for a stub.\n@@ -577,8 +783,4 @@\n-  _sites_handle = jvmci_env()->get_HotSpotCompiledCode_sites(compiled_code);\n-\n-  _code_handle = jvmci_env()->get_HotSpotCompiledCode_targetCode(compiled_code);\n-  _code_size = jvmci_env()->get_HotSpotCompiledCode_targetCodeSize(compiled_code);\n-  _total_frame_size = jvmci_env()->get_HotSpotCompiledCode_totalFrameSize(compiled_code);\n-\n-  JVMCIObject deoptRescueSlot = jvmci_env()->get_HotSpotCompiledCode_deoptRescueSlot(compiled_code);\n-  if (deoptRescueSlot.is_null()) {\n+  _sites_count = stream->read_s4(\"sites:length\");\n+  _code_size = stream->read_s4(\"targetCodeSize\");\n+  _total_frame_size = stream->read_s4(\"totalFrameSize\");\n+  if (!is_set(code_flags, HCC_HAS_DEOPT_RESCUE_SLOT)) {\n@@ -587,2 +789,2 @@\n-    _orig_pc_offset = jvmci_env()->get_StackSlot_offset(deoptRescueSlot);\n-    if (jvmci_env()->get_StackSlot_addFrameSize(deoptRescueSlot)) {\n+    _orig_pc_offset = stream->read_s2(\"offset\");\n+    if (stream->read_bool(\"addRawFrameSize\")) {\n@@ -592,1 +794,1 @@\n-      JVMCI_ERROR(\"invalid deopt rescue slot: %d\", _orig_pc_offset);\n+      JVMCI_ERROR(\"invalid deopt rescue slot: %d%s\", _orig_pc_offset, stream->context());\n@@ -597,3 +799,5 @@\n-  _data_section_handle = jvmci_env()->get_HotSpotCompiledCode_dataSection(compiled_code);\n-  if ((_constants->alignment() % jvmci_env()->get_HotSpotCompiledCode_dataSectionAlignment(compiled_code)) != 0) {\n-    JVMCI_ERROR(\"invalid data section alignment: %d\", jvmci_env()->get_HotSpotCompiledCode_dataSectionAlignment(compiled_code));\n+  u4 data_section_size = stream->read_u4(\"dataSectionSize\");\n+  u1 data_section_alignment = stream->read_u1(\"dataSectionAlignment\");\n+  if ((_constants->alignment() % data_section_alignment) != 0) {\n+    JVMCI_ERROR(\"invalid data section alignment: %d [constants alignment: %d]%s\",\n+        data_section_alignment, _constants->alignment(), stream->context());\n@@ -601,8 +805,1 @@\n-  _constants_size = JVMCIENV->get_length(data_section());\n-\n-  _data_section_patches_handle = jvmci_env()->get_HotSpotCompiledCode_dataSectionPatches(compiled_code);\n-\n-#ifndef PRODUCT\n-  _comments_handle = jvmci_env()->get_HotSpotCompiledCode_comments(compiled_code);\n-#endif\n-\n+  _constants_size = data_section_size;\n@@ -610,4 +807,0 @@\n-\n-\n-  JVMCIObject arch = jvmci_env()->get_TargetDescription_arch(target);\n-  _word_kind_handle = jvmci_env()->get_Architecture_wordKind(arch);\n@@ -617,30 +810,16 @@\n-int CodeInstaller::estimate_stubs_size(JVMCI_TRAPS) {\n-  \/\/ Estimate the number of static call stubs that might be emitted.\n-  int static_call_stubs = 0;\n-  int trampoline_stubs = 0;\n-  JVMCIObjectArray sites = this->sites();\n-  for (int i = 0; i < JVMCIENV->get_length(sites); i++) {\n-    JVMCIObject site = JVMCIENV->get_object_at(sites, i);\n-    if (!site.is_null()) {\n-      if (jvmci_env()->isa_site_Mark(site)) {\n-        JVMCIObject id_obj = jvmci_env()->get_site_Mark_id(site);\n-        if (id_obj.is_non_null()) {\n-          if (!jvmci_env()->is_boxing_object(T_INT, id_obj)) {\n-            JVMCI_ERROR_0(\"expected Integer id, got %s\", jvmci_env()->klass_name(id_obj));\n-          }\n-          jint id = jvmci_env()->get_boxed_value(T_INT, id_obj).i;\n-          switch (id) {\n-            case INVOKEINTERFACE:\n-            case INVOKEVIRTUAL:\n-              trampoline_stubs++;\n-              break;\n-            case INVOKESTATIC:\n-            case INVOKESPECIAL:\n-              static_call_stubs++;\n-              trampoline_stubs++;\n-              break;\n-            default:\n-              break;\n-          }\n-        }\n-      }\n+u1 CodeInstaller::as_read_oop_tag(HotSpotCompiledCodeStream* stream, u1 patch_object_tag, JVMCI_TRAPS) {\n+  switch (patch_object_tag) {\n+    case PATCH_OBJECT_ID:\n+    case PATCH_NARROW_OBJECT_ID: {\n+      return OBJECT_ID;\n+    }\n+    case PATCH_OBJECT_ID2:\n+    case PATCH_NARROW_OBJECT_ID2: {\n+      return OBJECT_ID2;\n+    }\n+    case PATCH_NARROW_JOBJECT:\n+    case PATCH_JOBJECT: {\n+      return JOBJECT;\n+    }\n+    default: {\n+      JVMCI_ERROR_0(\"unknown object patch tag: %d%s\", patch_object_tag, stream->context());\n@@ -649,0 +828,6 @@\n+}\n+\n+int CodeInstaller::estimate_stubs_size(HotSpotCompiledCodeStream* stream, JVMCI_TRAPS) {\n+  \/\/ Estimate the number of static call stubs that might be emitted.\n+  u2 static_call_stubs = stream->read_u2(\"numStaticCallStubs\");\n+  u2 trampoline_stubs = stream->read_u2(\"numTrampolineStubs\");\n@@ -655,4 +840,5 @@\n-JVMCI::CodeInstallResult CodeInstaller::initialize_buffer(CodeBuffer& buffer, bool check_size, JVMCI_TRAPS) {\n-  HandleMark hm(Thread::current());\n-  JVMCIObjectArray sites = this->sites();\n-  int locs_buffer_size = JVMCIENV->get_length(sites) * (relocInfo::length_limit + sizeof(relocInfo));\n+JVMCI::CodeInstallResult CodeInstaller::initialize_buffer(JVMCIObject compiled_code, CodeBuffer& buffer, HotSpotCompiledCodeStream* stream, u1 code_flags, JVMCI_TRAPS) {\n+  JavaThread* thread = stream->thread();\n+  HandleMark hm(thread);\n+  int locs_buffer_size = _sites_count * (relocInfo::length_limit + sizeof(relocInfo));\n+\n@@ -664,2 +850,1 @@\n-  int stubs_size = estimate_stubs_size(JVMCI_CHECK_OK);\n-  int total_size = align_up(_code_size, buffer.insts()->alignment()) + align_up(_constants_size, buffer.consts()->alignment()) + align_up(stubs_size, buffer.stubs()->alignment());\n+  int stubs_size = estimate_stubs_size(stream, JVMCI_CHECK_OK);\n@@ -667,1 +852,8 @@\n-  if (check_size && total_size > JVMCINMethodSizeLimit) {\n+  assert((CodeBuffer::SECT_INSTS == CodeBuffer::SECT_STUBS - 1) &&\n+         (CodeBuffer::SECT_CONSTS == CodeBuffer::SECT_INSTS - 1), \"sections order: consts, insts, stubs\");\n+  \/\/ buffer content: [constants + code_align] + [code + stubs_align] + [stubs]\n+  int total_size = align_up(_constants_size, CodeSection::alignment(CodeBuffer::SECT_INSTS)) +\n+                   align_up(_code_size, CodeSection::alignment(CodeBuffer::SECT_STUBS)) +\n+                   stubs_size;\n+\n+  if (total_size > JVMCINMethodSizeLimit) {\n@@ -685,1 +877,2 @@\n-  JVMCIENV->copy_bytes_to(data_section(), (jbyte*) _constants->start(), 0, _constants_size);\n+  JVMCIObject data_section = jvmci_env()->get_HotSpotCompiledCode_dataSection(compiled_code);\n+  JVMCIENV->copy_bytes_to(data_section, (jbyte*) _constants->start(), 0, _constants_size);\n@@ -691,1 +884,3 @@\n-  JVMCIENV->copy_bytes_to(code(), (jbyte*) _instructions->start(), 0, _code_size);\n+\n+  JVMCIPrimitiveArray code = jvmci_env()->get_HotSpotCompiledCode_targetCode(compiled_code);\n+  JVMCIENV->copy_bytes_to(code, (jbyte*) _instructions->start(), 0, _code_size);\n@@ -694,20 +889,13 @@\n-  for (int i = 0; i < JVMCIENV->get_length(data_section_patches()); i++) {\n-    \/\/ HandleMark hm(THREAD);\n-    JVMCIObject patch = JVMCIENV->get_object_at(data_section_patches(), i);\n-    if (patch.is_null()) {\n-      JVMCI_THROW_(NullPointerException, JVMCI::ok);\n-    }\n-    JVMCIObject reference = jvmci_env()->get_site_DataPatch_reference(patch);\n-    if (reference.is_null()) {\n-      JVMCI_THROW_(NullPointerException, JVMCI::ok);\n-    }\n-    if (!jvmci_env()->isa_site_ConstantReference(reference)) {\n-      JVMCI_ERROR_OK(\"invalid patch in data section: %s\", jvmci_env()->klass_name(reference));\n-    }\n-    JVMCIObject constant = jvmci_env()->get_site_ConstantReference_constant(reference);\n-    if (constant.is_null()) {\n-      JVMCI_THROW_(NullPointerException, JVMCI::ok);\n-    }\n-    address dest = _constants->start() + jvmci_env()->get_site_Site_pcOffset(patch);\n-    if (jvmci_env()->isa_HotSpotMetaspaceConstantImpl(constant)) {\n-      if (jvmci_env()->get_HotSpotMetaspaceConstantImpl_compressed(constant)) {\n+\n+  u2 length = stream->read_u2(\"dataSectionPatches:length\");\n+  for (int i = 0; i < length; i++) {\n+    address dest = _constants->start() + stream->read_u4(\"patch:pcOffset\");\n+    u1 tag = stream->read_u1(\"tag\");\n+\n+    switch (tag) {\n+      case PATCH_METHOD:\n+      case PATCH_KLASS: {\n+        *((void**) dest) = record_metadata_reference(_constants, dest, stream, tag, JVMCI_CHECK_OK);\n+        break;\n+      }\n+      case PATCH_NARROW_KLASS: {\n@@ -715,1 +903,1 @@\n-        *((narrowKlass*) dest) = record_narrow_metadata_reference(_constants, dest, constant, JVMCI_CHECK_OK);\n+        *((narrowKlass*) dest) = record_narrow_metadata_reference(_constants, dest, stream, tag, JVMCI_CHECK_OK);\n@@ -719,2 +907,1 @@\n-      } else {\n-        *((void**) dest) = record_metadata_reference(_constants, dest, constant, JVMCI_CHECK_OK);\n+        break;\n@@ -722,13 +909,14 @@\n-    } else if (jvmci_env()->isa_HotSpotObjectConstantImpl(constant)) {\n-      Handle obj = jvmci_env()->asConstant(constant, JVMCI_CHECK_OK);\n-      jobject value = JNIHandles::make_local(obj());\n-      int oop_index = _oop_recorder->find_index(value);\n-\n-      if (jvmci_env()->get_HotSpotObjectConstantImpl_compressed(constant)) {\n-#ifdef _LP64\n-        _constants->relocate(dest, oop_Relocation::spec(oop_index), relocInfo::narrow_oop_in_const);\n-#else\n-        JVMCI_ERROR_OK(\"unexpected compressed oop in 32-bit mode\");\n-#endif\n-      } else {\n-        _constants->relocate(dest, oop_Relocation::spec(oop_index));\n+      case PATCH_OBJECT_ID:\n+      case PATCH_OBJECT_ID2:\n+      case PATCH_NARROW_OBJECT_ID:\n+      case PATCH_NARROW_OBJECT_ID2:\n+      case PATCH_JOBJECT:\n+      case PATCH_NARROW_JOBJECT: {\n+        bool narrow = tag == PATCH_NARROW_OBJECT_ID || tag == PATCH_NARROW_OBJECT_ID2  || tag == PATCH_NARROW_JOBJECT;\n+        u1 read_tag = as_read_oop_tag(stream, tag, JVMCI_CHECK_OK);\n+        record_oop_patch(stream, dest, read_tag, narrow, JVMCI_CHECK_OK);\n+        break;\n+      }\n+      default: {\n+        JVMCI_ERROR_OK(\"invalid constant tag: %d%s\", tag, stream->context());\n+        break;\n@@ -736,2 +924,0 @@\n-    } else {\n-      JVMCI_ERROR_OK(\"invalid constant in data section: %s\", jvmci_env()->klass_name(constant));\n@@ -740,36 +926,35 @@\n-  jint last_pc_offset = -1;\n-  for (int i = 0; i < JVMCIENV->get_length(sites); i++) {\n-    \/\/ HandleMark hm(THREAD);\n-    JVMCIObject site = JVMCIENV->get_object_at(sites, i);\n-    if (site.is_null()) {\n-      JVMCI_THROW_(NullPointerException, JVMCI::ok);\n-    }\n-    jint pc_offset = jvmci_env()->get_site_Site_pcOffset(site);\n-\n-    if (jvmci_env()->isa_site_Call(site)) {\n-      JVMCI_event_4(\"call at %i\", pc_offset);\n-      site_Call(buffer, pc_offset, site, JVMCI_CHECK_OK);\n-    } else if (jvmci_env()->isa_site_Infopoint(site)) {\n-      \/\/ three reasons for infopoints denote actual safepoints\n-      JVMCIObject reason = jvmci_env()->get_site_Infopoint_reason(site);\n-      if (JVMCIENV->equals(reason, jvmci_env()->get_site_InfopointReason_SAFEPOINT()) ||\n-          JVMCIENV->equals(reason, jvmci_env()->get_site_InfopointReason_CALL()) ||\n-          JVMCIENV->equals(reason, jvmci_env()->get_site_InfopointReason_IMPLICIT_EXCEPTION())) {\n-        JVMCI_event_4(\"safepoint at %i\", pc_offset);\n-        site_Safepoint(buffer, pc_offset, site, JVMCI_CHECK_OK);\n-        if (_orig_pc_offset < 0) {\n-          JVMCI_ERROR_OK(\"method contains safepoint, but has no deopt rescue slot\");\n-        }\n-        if (JVMCIENV->equals(reason, jvmci_env()->get_site_InfopointReason_IMPLICIT_EXCEPTION())) {\n-          if (jvmci_env()->isa_site_ImplicitExceptionDispatch(site)) {\n-            jint dispatch_offset = jvmci_env()->get_site_ImplicitExceptionDispatch_dispatchOffset(site);\n-            JVMCI_event_4(\"implicit exception at %i, dispatch to %i\", pc_offset, dispatch_offset);\n-            _implicit_exception_table.append(pc_offset, dispatch_offset);\n-          } else {\n-            JVMCI_event_4(\"implicit exception at %i\", pc_offset);\n-            _implicit_exception_table.add_deoptimize(pc_offset);\n-          }\n-        }\n-      } else {\n-        JVMCI_event_4(\"infopoint at %i\", pc_offset);\n-        site_Infopoint(buffer, pc_offset, site, JVMCI_CHECK_OK);\n+  jint last_pc_offset = -1;\n+  for (int i = 0; i < _sites_count; i++) {\n+    u4 pc_offset = stream->read_s4(\"site:pcOffset\");\n+    u1 tag = stream->read_u1(\"tag\");\n+    switch (tag) {\n+      case SITE_FOREIGN_CALL:\n+      case SITE_FOREIGN_CALL_NO_DEBUG_INFO:\n+      case SITE_CALL: {\n+        site_Call(buffer, tag, pc_offset, stream, JVMCI_CHECK_OK);\n+        break;\n+      }\n+      case SITE_SAFEPOINT:\n+      case SITE_IMPLICIT_EXCEPTION:\n+      case SITE_IMPLICIT_EXCEPTION_DISPATCH: {\n+        site_Safepoint(buffer, pc_offset, stream, tag, JVMCI_CHECK_OK);\n+        break;\n+      }\n+      case SITE_INFOPOINT: {\n+        site_Infopoint(buffer, pc_offset, stream, JVMCI_CHECK_OK);\n+        break;\n+      }\n+      case SITE_MARK: {\n+        site_Mark(buffer, pc_offset, stream, JVMCI_CHECK_OK);\n+        break;\n+      }\n+      case SITE_DATA_PATCH: {\n+        site_DataPatch(buffer, pc_offset, stream, JVMCI_CHECK_OK);\n+        break;\n+      }\n+      case SITE_EXCEPTION_HANDLER: {\n+        site_ExceptionHandler(pc_offset, stream);\n+        break;\n+      }\n+      default: {\n+        JVMCI_ERROR_OK(\"unexpected site tag at \" INTPTR_FORMAT \": %d\", p2i(stream->pos() - 1), tag);\n@@ -778,11 +963,1 @@\n-    } else if (jvmci_env()->isa_site_DataPatch(site)) {\n-      JVMCI_event_4(\"datapatch at %i\", pc_offset);\n-      site_DataPatch(buffer, pc_offset, site, JVMCI_CHECK_OK);\n-    } else if (jvmci_env()->isa_site_Mark(site)) {\n-      JVMCI_event_4(\"mark at %i\", pc_offset);\n-      site_Mark(buffer, pc_offset, site, JVMCI_CHECK_OK);\n-    } else if (jvmci_env()->isa_site_ExceptionHandler(site)) {\n-      JVMCI_event_4(\"exceptionhandler at %i\", pc_offset);\n-      site_ExceptionHandler(pc_offset, site);\n-    } else {\n-      JVMCI_ERROR_OK(\"unexpected site subclass: %s\", jvmci_env()->klass_name(site));\n+\n@@ -792,3 +967,2 @@\n-    JavaThread* thread = JavaThread::current();\n-    if (SafepointMechanism::should_process(thread)) {\n-      \/\/ this is a hacky way to force a safepoint check but nothing else was jumping out at me.\n+    if ((i % 32 == 0) && SafepointMechanism::should_process(thread)) {\n+      \/\/ Force a safepoint to mitigate pause time installing large code\n@@ -799,0 +973,5 @@\n+  if (is_set(code_flags, HCC_HAS_COMMENTS)) {\n+    u2 length = stream->read_u2(\"comments:length\");\n+    for (int i = 0; i < length; i++) {\n+      u4 pc_offset = stream->read_u4(\"comment:pcOffset\");\n+      const char* text = stream->read_utf8(\"comment:text\", JVMCI_CHECK_OK);\n@@ -800,7 +979,2 @@\n-  if (comments().is_non_null()) {\n-    for (int i = 0; i < JVMCIENV->get_length(comments()); i++) {\n-      JVMCIObject comment = JVMCIENV->get_object_at(comments(), i);\n-      assert(jvmci_env()->isa_HotSpotCompiledCode_Comment(comment), \"cce\");\n-      jint offset = jvmci_env()->get_HotSpotCompiledCode_Comment_pcOffset(comment);\n-      const char* text = jvmci_env()->as_utf8_string(jvmci_env()->get_HotSpotCompiledCode_Comment_text(comment));\n-      buffer.block_comment(offset, text);\n+      buffer.block_comment(pc_offset, text);\n+#endif\n@@ -809,2 +983,1 @@\n-#endif\n-    JavaThread* THREAD = JavaThread::current(); \/\/ For exception macros.\n+    JavaThread* THREAD = thread; \/\/ For exception macros.\n@@ -817,39 +990,13 @@\n-void CodeInstaller::assumption_NoFinalizableSubclass(JVMCIObject assumption) {\n-  JVMCIObject receiverType_handle = jvmci_env()->get_Assumptions_NoFinalizableSubclass_receiverType(assumption);\n-  Klass* receiverType = jvmci_env()->asKlass(receiverType_handle);\n-  _dependencies->assert_has_no_finalizable_subclasses(receiverType);\n-}\n-\n-void CodeInstaller::assumption_ConcreteSubtype(JVMCIObject assumption) {\n-  JVMCIObject context_handle = jvmci_env()->get_Assumptions_ConcreteSubtype_context(assumption);\n-  JVMCIObject subtype_handle = jvmci_env()->get_Assumptions_ConcreteSubtype_subtype(assumption);\n-  Klass* context = jvmci_env()->asKlass(context_handle);\n-  Klass* subtype = jvmci_env()->asKlass(subtype_handle);\n-\n-  assert(context->is_abstract(), \"\");\n-  _dependencies->assert_abstract_with_unique_concrete_subtype(context, subtype);\n-}\n-\n-void CodeInstaller::assumption_LeafType(JVMCIObject assumption) {\n-  JVMCIObject context_handle = jvmci_env()->get_Assumptions_LeafType_context(assumption);\n-  Klass* context = jvmci_env()->asKlass(context_handle);\n-\n-  _dependencies->assert_leaf_type(context);\n-}\n-\n-void CodeInstaller::assumption_ConcreteMethod(JVMCIObject assumption) {\n-  JVMCIObject impl_handle = jvmci_env()->get_Assumptions_ConcreteMethod_impl(assumption);\n-  JVMCIObject context_handle = jvmci_env()->get_Assumptions_ConcreteMethod_context(assumption);\n-\n-  Method* impl = jvmci_env()->asMethod(impl_handle);\n-  Klass* context = jvmci_env()->asKlass(context_handle);\n-\n-  _dependencies->assert_unique_concrete_method(context, impl);\n-}\n-\n-void CodeInstaller::assumption_CallSiteTargetValue(JVMCIObject assumption, JVMCI_TRAPS) {\n-  JVMCIObject callSiteConstant = jvmci_env()->get_Assumptions_CallSiteTargetValue_callSite(assumption);\n-  Handle callSite = jvmci_env()->asConstant(callSiteConstant, JVMCI_CHECK);\n-  JVMCIObject methodConstant = jvmci_env()->get_Assumptions_CallSiteTargetValue_methodHandle(assumption);\n-  Handle methodHandle = jvmci_env()->asConstant(methodConstant, JVMCI_CHECK);\n-  _dependencies->assert_call_site_target_value(callSite(), methodHandle());\n+void CodeInstaller::record_oop_patch(HotSpotCompiledCodeStream* stream, address dest, u1 read_tag, bool narrow, JVMCI_TRAPS) {\n+  Handle obj = read_oop(stream, read_tag, JVMCI_CHECK);\n+  jobject value = JNIHandles::make_local(obj());\n+  int oop_index = _oop_recorder->find_index(value);\n+  if (narrow) {\n+#ifdef _LP64\n+    _constants->relocate(dest, oop_Relocation::spec(oop_index), relocInfo::narrow_oop_in_const);\n+#else\n+    JVMCI_ERROR(\"unexpected compressed oop in 32-bit mode\");\n+#endif\n+  } else {\n+    _constants->relocate(dest, oop_Relocation::spec(oop_index));\n+  }\n@@ -858,2 +1005,2 @@\n-void CodeInstaller::site_ExceptionHandler(jint pc_offset, JVMCIObject exc) {\n-  jint handler_offset = jvmci_env()->get_site_ExceptionHandler_handlerPos(exc);\n+void CodeInstaller::site_ExceptionHandler(jint pc_offset, HotSpotCompiledCodeStream* stream) {\n+  u4 handler_offset = stream->read_u4(\"site:handlerPos\");\n@@ -868,20 +1015,4 @@\n-\/\/ If deoptimization happens, the interpreter should reexecute these bytecodes.\n-\/\/ This function mainly helps the compilers to set up the reexecute bit.\n-static bool bytecode_should_reexecute(Bytecodes::Code code) {\n-  switch (code) {\n-    case Bytecodes::_invokedynamic:\n-    case Bytecodes::_invokevirtual:\n-    case Bytecodes::_invokeinterface:\n-    case Bytecodes::_invokespecial:\n-    case Bytecodes::_invokestatic:\n-      return false;\n-    default:\n-      return true;\n-    }\n-  return true;\n-}\n-\n-GrowableArray<ScopeValue*>* CodeInstaller::record_virtual_objects(JVMCIObject debug_info, JVMCI_TRAPS) {\n-  JVMCIObjectArray virtualObjects = jvmci_env()->get_DebugInfo_virtualObjectMapping(debug_info);\n-  if (virtualObjects.is_null()) {\n-    return NULL;\n+void CodeInstaller::read_virtual_objects(HotSpotCompiledCodeStream* stream, JVMCI_TRAPS) {\n+  u2 length = stream->read_u2(\"virtualObjects:length\");\n+  if (length == 0) {\n+    return;\n@@ -889,1 +1020,2 @@\n-  GrowableArray<ScopeValue*>* objects = new GrowableArray<ScopeValue*>(JVMCIENV->get_length(virtualObjects), JVMCIENV->get_length(virtualObjects), NULL);\n+  GrowableArray<ScopeValue*> *objects = new GrowableArray<ScopeValue*>(length, length, NULL);\n+  stream->set_virtual_objects(objects);\n@@ -891,6 +1023,4 @@\n-  for (int i = 0; i < JVMCIENV->get_length(virtualObjects); i++) {\n-    \/\/ HandleMark hm(THREAD);\n-    JVMCIObject value = JVMCIENV->get_object_at(virtualObjects, i);\n-    int id = jvmci_env()->get_VirtualObject_id(value);\n-    JVMCIObject type = jvmci_env()->get_VirtualObject_type(value);\n-    bool is_auto_box = jvmci_env()->get_VirtualObject_isAutoBox(value);\n+  JavaThread* thread = stream->thread();\n+  for (int id = 0; id < length; id++) {\n+    Klass* klass = stream->read_klass(\"type\");\n+    bool is_auto_box = stream->read_bool(\"isAutoBox\");\n@@ -900,1 +1030,0 @@\n-    Klass* klass = jvmci_env()->asKlass(type);\n@@ -904,6 +1033,0 @@\n-    if (id < 0 || id >= objects->length()) {\n-      JVMCI_ERROR_NULL(\"virtual object id %d out of bounds\", id);\n-    }\n-    if (objects->at(id) != NULL) {\n-      JVMCI_ERROR_NULL(\"duplicate virtual object id %d\", id);\n-    }\n@@ -914,5 +1037,2 @@\n-  for (int i = 0; i < JVMCIENV->get_length(virtualObjects); i++) {\n-    \/\/ HandleMark hm(THREAD);\n-    JVMCIObject value = JVMCIENV->get_object_at(virtualObjects, i);\n-    int id = jvmci_env()->get_VirtualObject_id(value);\n-    record_object_value(objects->at(id)->as_ObjectValue(), value, objects, JVMCI_CHECK_NULL);\n+  for (int id = 0; id < length; id++) {\n+    record_object_value(objects->at(id)->as_ObjectValue(), stream, JVMCI_CHECK);\n@@ -922,17 +1042,1 @@\n-  return objects;\n-}\n-\n-void CodeInstaller::record_scope(jint pc_offset, JVMCIObject debug_info, ScopeMode scope_mode, bool is_mh_invoke, bool return_oop, JVMCI_TRAPS) {\n-  JVMCIObject position = jvmci_env()->get_DebugInfo_bytecodePosition(debug_info);\n-  if (position.is_null()) {\n-    \/\/ Stubs do not record scope info, just oop maps\n-    return;\n-  }\n-\n-  GrowableArray<ScopeValue*>* objectMapping;\n-  if (scope_mode == CodeInstaller::FullFrame) {\n-    objectMapping = record_virtual_objects(debug_info, JVMCI_CHECK);\n-  } else {\n-    objectMapping = NULL;\n-  }\n-  record_scope(pc_offset, position, scope_mode, objectMapping, is_mh_invoke, return_oop, JVMCI_CHECK);\n+  stream->set_virtual_objects(objects);\n@@ -943,12 +1047,7 @@\n-    if (bci == jvmci_env()->get_BytecodeFrame_BEFORE_BCI()) {\n-      return BeforeBci;\n-    } else if (bci == jvmci_env()->get_BytecodeFrame_AFTER_BCI()) {\n-      return AfterBci;\n-    } else if (bci == jvmci_env()->get_BytecodeFrame_UNWIND_BCI()) {\n-      return UnwindBci;\n-    } else if (bci == jvmci_env()->get_BytecodeFrame_AFTER_EXCEPTION_BCI()) {\n-      return AfterExceptionBci;\n-    } else if (bci == jvmci_env()->get_BytecodeFrame_UNKNOWN_BCI()) {\n-      return UnknownBci;\n-    } else if (bci == jvmci_env()->get_BytecodeFrame_INVALID_FRAMESTATE_BCI()) {\n-      return InvalidFrameStateBci;\n+    switch (bci) {\n+      case BEFORE_BCI: return BeforeBci;\n+      case AFTER_BCI: return AfterBci;\n+      case UNWIND_BCI: return UnwindBci;\n+      case AFTER_EXCEPTION_BCI: return AfterExceptionBci;\n+      case UNKNOWN_BCI: return UnknownBci;\n+      case INVALID_FRAMESTATE_BCI: return InvalidFrameStateBci;\n@@ -961,11 +1060,3 @@\n-void CodeInstaller::record_scope(jint pc_offset, JVMCIObject position, ScopeMode scope_mode, GrowableArray<ScopeValue*>* objects, bool is_mh_invoke, bool return_oop, JVMCI_TRAPS) {\n-  JVMCIObject frame;\n-  if (scope_mode == CodeInstaller::FullFrame) {\n-    if (!jvmci_env()->isa_BytecodeFrame(position)) {\n-      JVMCI_ERROR(\"Full frame expected for debug info at %i\", pc_offset);\n-    }\n-    frame = position;\n-  }\n-  JVMCIObject caller_frame = jvmci_env()->get_BytecodePosition_caller(position);\n-  if (caller_frame.is_non_null()) {\n-    record_scope(pc_offset, caller_frame, scope_mode, objects, is_mh_invoke, return_oop, JVMCI_CHECK);\n+void CodeInstaller::record_scope(jint pc_offset, HotSpotCompiledCodeStream* stream, u1 debug_info_flags, bool full_info, bool is_mh_invoke, bool return_oop, JVMCI_TRAPS) {\n+  if (full_info) {\n+    read_virtual_objects(stream, JVMCI_CHECK);\n@@ -973,0 +1064,9 @@\n+  if (is_set(debug_info_flags, DI_HAS_FRAMES)) {\n+    u2 depth = stream->read_u2(\"depth\");\n+    for (int i = 0; i < depth; i++) {\n+      Thread* thread = Thread::current();\n+      methodHandle method(thread, stream->read_method(\"method\"));\n+      jint bci = map_jvmci_bci(stream->read_s4(\"bci\"));\n+      if (bci == BEFORE_BCI) {\n+        bci = SynchronizationEntryBCI;\n+      }\n@@ -974,7 +1074,1 @@\n-  JVMCIObject hotspot_method = jvmci_env()->get_BytecodePosition_method(position);\n-  Thread* thread = Thread::current();\n-  methodHandle method(thread, jvmci_env()->asMethod(hotspot_method));\n-  jint bci = map_jvmci_bci(jvmci_env()->get_BytecodePosition_bci(position));\n-  if (bci == jvmci_env()->get_BytecodeFrame_BEFORE_BCI()) {\n-    bci = SynchronizationEntryBCI;\n-  }\n+      JVMCI_event_2(\"Recording scope pc_offset=%d bci=%d method=%s\", pc_offset, bci, method->name_and_sig_as_C_string());\n@@ -982,1 +1076,2 @@\n-  JVMCI_event_2(\"Recording scope pc_offset=%d bci=%d method=%s\", pc_offset, bci, method->name_and_sig_as_C_string());\n+      bool reexecute = false;\n+      bool rethrow_exception = false;\n@@ -984,12 +1079,3 @@\n-  bool reexecute = false;\n-  if (frame.is_non_null()) {\n-    if (bci < 0){\n-       reexecute = false;\n-    } else {\n-      Bytecodes::Code code = Bytecodes::java_code_at(method(), method->bcp_from(bci));\n-      reexecute = bytecode_should_reexecute(code);\n-      if (frame.is_non_null()) {\n-        reexecute = (jvmci_env()->get_BytecodeFrame_duringCall(frame) == JNI_FALSE);\n-      }\n-    }\n-  }\n+      DebugToken* locals_token = NULL;\n+      DebugToken* stack_token = NULL;\n+      DebugToken* monitors_token = NULL;\n@@ -997,4 +1083,3 @@\n-  DebugToken* locals_token = NULL;\n-  DebugToken* expressions_token = NULL;\n-  DebugToken* monitors_token = NULL;\n-  bool throw_exception = false;\n+      if (full_info) {\n+        u1 frame_flags = stream->read_u1(\"flags\");\n+        rethrow_exception = is_set(frame_flags, DIF_RETHROW_EXCEPTION);\n@@ -1002,6 +1087,3 @@\n-  if (frame.is_non_null()) {\n-    jint local_count = jvmci_env()->get_BytecodeFrame_numLocals(frame);\n-    jint expression_count = jvmci_env()->get_BytecodeFrame_numStack(frame);\n-    jint monitor_count = jvmci_env()->get_BytecodeFrame_numLocks(frame);\n-    JVMCIObjectArray values = jvmci_env()->get_BytecodeFrame_values(frame);\n-    JVMCIObjectArray slotKinds = jvmci_env()->get_BytecodeFrame_slotKinds(frame);\n+        if (bci >= 0) {\n+          reexecute = !is_set(frame_flags, DIF_DURING_CALL);\n+        }\n@@ -1009,9 +1091,3 @@\n-    if (values.is_null() || slotKinds.is_null()) {\n-      JVMCI_THROW(NullPointerException);\n-    }\n-    if (local_count + expression_count + monitor_count != JVMCIENV->get_length(values)) {\n-      JVMCI_ERROR(\"unexpected values length %d in scope (%d locals, %d expressions, %d monitors)\", JVMCIENV->get_length(values), local_count, expression_count, monitor_count);\n-    }\n-    if (local_count + expression_count != JVMCIENV->get_length(slotKinds)) {\n-      JVMCI_ERROR(\"unexpected slotKinds length %d in scope (%d locals, %d expressions)\", JVMCIENV->get_length(slotKinds), local_count, expression_count);\n-    }\n+        GrowableArray<ScopeValue*>* locals = read_local_or_stack_values(stream, frame_flags, true, JVMCI_CHECK);\n+        GrowableArray<ScopeValue*>* stack = read_local_or_stack_values(stream, frame_flags, false, JVMCI_CHECK);\n+        GrowableArray<MonitorValue*>* monitors = read_monitor_values(stream, frame_flags, JVMCI_CHECK);\n@@ -1019,34 +1095,3 @@\n-    GrowableArray<ScopeValue*>* locals = local_count > 0 ? new GrowableArray<ScopeValue*> (local_count) : NULL;\n-    GrowableArray<ScopeValue*>* expressions = expression_count > 0 ? new GrowableArray<ScopeValue*> (expression_count) : NULL;\n-    GrowableArray<MonitorValue*>* monitors = monitor_count > 0 ? new GrowableArray<MonitorValue*> (monitor_count) : NULL;\n-\n-    JVMCI_event_2(\"Scope at bci %d with %d values\", bci, JVMCIENV->get_length(values));\n-    JVMCI_event_2(\"%d locals %d expressions, %d monitors\", local_count, expression_count, monitor_count);\n-\n-    for (jint i = 0; i < JVMCIENV->get_length(values); i++) {\n-      \/\/ HandleMark hm(THREAD);\n-      ScopeValue* second = NULL;\n-      JVMCIObject value = JVMCIENV->get_object_at(values, i);\n-      if (i < local_count) {\n-        BasicType type = jvmci_env()->kindToBasicType(JVMCIENV->get_object_at(slotKinds, i), JVMCI_CHECK);\n-        ScopeValue* first = get_scope_value(value, type, objects, second, JVMCI_CHECK);\n-        if (second != NULL) {\n-          locals->append(second);\n-        }\n-        locals->append(first);\n-      } else if (i < local_count + expression_count) {\n-        BasicType type = jvmci_env()->kindToBasicType(JVMCIENV->get_object_at(slotKinds, i), JVMCI_CHECK);\n-        ScopeValue* first = get_scope_value(value, type, objects, second, JVMCI_CHECK);\n-        if (second != NULL) {\n-          expressions->append(second);\n-        }\n-        expressions->append(first);\n-      } else {\n-        MonitorValue *monitor = get_monitor_value(value, objects, JVMCI_CHECK);\n-        monitors->append(monitor);\n-      }\n-      if (second != NULL) {\n-        i++;\n-        if (i >= JVMCIENV->get_length(values) || !JVMCIENV->equals(JVMCIENV->get_object_at(values, i), jvmci_env()->get_Value_ILLEGAL())) {\n-          JVMCI_ERROR(\"double-slot value not followed by Value.ILLEGAL\");\n-        }\n+        locals_token = _debug_recorder->create_scope_values(locals);\n+        stack_token = _debug_recorder->create_scope_values(stack);\n+        monitors_token = _debug_recorder->create_monitor_values(monitors);\n@@ -1054,6 +1099,8 @@\n-    }\n-\n-    locals_token = _debug_recorder->create_scope_values(locals);\n-    expressions_token = _debug_recorder->create_scope_values(expressions);\n-    monitors_token = _debug_recorder->create_monitor_values(monitors);\n-    throw_exception = jvmci_env()->get_BytecodeFrame_rethrowException(frame) == JNI_TRUE;\n+      \/\/ has_ea_local_in_scope and arg_escape should be added to JVMCI\n+      const bool return_scalarized     = false;\n+      const bool has_ea_local_in_scope = false;\n+      const bool arg_escape            = false;\n+      _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, rethrow_exception, is_mh_invoke, return_oop,\n+                                      return_scalarized, has_ea_local_in_scope, arg_escape,\n+                                      locals_token, stack_token, monitors_token);\n+    }\n@@ -1062,14 +1109,3 @@\n-\n-  \/\/ has_ea_local_in_scope and arg_escape should be added to JVMCI\n-  const bool is_opt_native         = false;\n-  const bool has_ea_local_in_scope = false;\n-  const bool arg_escape            = false;\n-  _debug_recorder->describe_scope(pc_offset, method, NULL, bci, reexecute, throw_exception, is_mh_invoke, is_opt_native, return_oop, false,\n-                                  has_ea_local_in_scope, arg_escape,\n-                                  locals_token, expressions_token, monitors_token);\n-}\n-\n-void CodeInstaller::site_Safepoint(CodeBuffer& buffer, jint pc_offset, JVMCIObject site, JVMCI_TRAPS) {\n-  JVMCIObject debug_info = jvmci_env()->get_site_Infopoint_debugInfo(site);\n-  if (debug_info.is_null()) {\n-    JVMCI_ERROR(\"debug info expected at safepoint at %i\", pc_offset);\n+  if (full_info) {\n+    \/\/ Clear the virtual objects as they are specific to one DebugInfo\n+    stream->set_virtual_objects(nullptr);\n@@ -1077,0 +1113,1 @@\n+}\n@@ -1078,3 +1115,3 @@\n-  \/\/ address instruction = _instructions->start() + pc_offset;\n-  \/\/ jint next_pc_offset = Assembler::locate_next_instruction(instruction) - _instructions->start();\n-  OopMap *map = create_oop_map(debug_info, JVMCI_CHECK);\n+void CodeInstaller::site_Safepoint(CodeBuffer& buffer, jint pc_offset, HotSpotCompiledCodeStream* stream, u1 tag, JVMCI_TRAPS) {\n+  u1 flags = stream->read_u1(\"debugInfo:flags\");\n+  OopMap *map = create_oop_map(stream, flags, JVMCI_CHECK);\n@@ -1082,1 +1119,1 @@\n-  record_scope(pc_offset, debug_info, CodeInstaller::FullFrame, JVMCI_CHECK);\n+  record_scope(pc_offset, stream, flags, true, JVMCI_CHECK);\n@@ -1084,6 +1121,2 @@\n-}\n-\n-void CodeInstaller::site_Infopoint(CodeBuffer& buffer, jint pc_offset, JVMCIObject site, JVMCI_TRAPS) {\n-  JVMCIObject debug_info = jvmci_env()->get_site_Infopoint_debugInfo(site);\n-  if (debug_info.is_null()) {\n-    JVMCI_ERROR(\"debug info expected at infopoint at %i\", pc_offset);\n+  if (_orig_pc_offset < 0) {\n+    JVMCI_ERROR(\"method contains safepoint, but has no deopt rescue slot\");\n@@ -1091,0 +1124,7 @@\n+  if (tag == SITE_IMPLICIT_EXCEPTION_DISPATCH) {\n+    jint dispatch_offset = stream->read_s4(\"dispatchOffset\");\n+    _implicit_exception_table.append(pc_offset, dispatch_offset);\n+  } else if (tag == SITE_IMPLICIT_EXCEPTION) {\n+    _implicit_exception_table.add_deoptimize(pc_offset);\n+  }\n+}\n@@ -1092,4 +1132,2 @@\n-  \/\/ We'd like to check that pc_offset is greater than the\n-  \/\/ last pc recorded with _debug_recorder (raising an exception if not)\n-  \/\/ but DebugInformationRecorder doesn't have sufficient public API.\n-\n+void CodeInstaller::site_Infopoint(CodeBuffer& buffer, jint pc_offset, HotSpotCompiledCodeStream* stream, JVMCI_TRAPS) {\n+  u1 flags = stream->read_u1(\"debugInfo:flags\");\n@@ -1097,1 +1135,1 @@\n-  record_scope(pc_offset, debug_info, CodeInstaller::BytecodePosition, JVMCI_CHECK);\n+  record_scope(pc_offset, stream, flags, false, JVMCI_CHECK);\n@@ -1101,9 +1139,12 @@\n-void CodeInstaller::site_Call(CodeBuffer& buffer, jint pc_offset, JVMCIObject site, JVMCI_TRAPS) {\n-  JVMCIObject target = jvmci_env()->get_site_Call_target(site);\n-  JVMCIObject hotspot_method; \/\/ JavaMethod\n-  JVMCIObject foreign_call;\n-\n-  if (jvmci_env()->isa_HotSpotForeignCallTarget(target)) {\n-    foreign_call = target;\n-  } else {\n-    hotspot_method = target;\n+void CodeInstaller::site_Call(CodeBuffer& buffer, u1 tag, jint pc_offset, HotSpotCompiledCodeStream* stream, JVMCI_TRAPS) {\n+  JavaThread* thread = stream->thread();\n+  jlong target = stream->read_u8(\"target\");\n+  methodHandle method;\n+  bool direct_call = false;\n+  if (tag == SITE_CALL) {\n+    method = methodHandle(thread, (Method*) target);\n+    assert(Method::is_valid_method(method()), \"invalid method\");\n+    direct_call = stream->read_bool(\"direct\");\n+    if (method.is_null()) {\n+      JVMCI_THROW(NullPointerException);\n+    }\n@@ -1112,5 +1153,1 @@\n-  JVMCIObject debug_info = jvmci_env()->get_site_Infopoint_debugInfo(site);\n-\n-  assert(hotspot_method.is_non_null() ^ foreign_call.is_non_null(), \"Call site needs exactly one type\");\n-\n-  jint next_pc_offset = CodeInstaller::pd_next_offset(inst, pc_offset, hotspot_method, JVMCI_CHECK);\n+  jint next_pc_offset = CodeInstaller::pd_next_offset(inst, pc_offset, JVMCI_CHECK);\n@@ -1119,2 +1156,3 @@\n-  if (debug_info.is_non_null()) {\n-    OopMap *map = create_oop_map(debug_info, JVMCI_CHECK);\n+  if (tag != SITE_FOREIGN_CALL_NO_DEBUG_INFO) {\n+    u1 flags = stream->read_u1(\"debugInfo:flags\");\n+    OopMap *map = create_oop_map(stream, flags, JVMCI_CHECK);\n@@ -1123,2 +1161,1 @@\n-    if (hotspot_method.is_non_null()) {\n-      Method *method = jvmci_env()->asMethod(hotspot_method);\n+    if (!method.is_null()) {\n@@ -1127,1 +1164,1 @@\n-      if (jvmci_env()->get_site_Call_direct(site)) {\n+      if (direct_call) {\n@@ -1132,1 +1169,1 @@\n-      record_scope(next_pc_offset, debug_info, CodeInstaller::FullFrame, is_mh_invoke, return_oop, JVMCI_CHECK);\n+      record_scope(next_pc_offset, stream, flags, true, is_mh_invoke, return_oop, JVMCI_CHECK);\n@@ -1134,1 +1171,1 @@\n-      record_scope(next_pc_offset, debug_info, CodeInstaller::FullFrame, JVMCI_CHECK);\n+      record_scope(next_pc_offset, stream, flags, true, JVMCI_CHECK);\n@@ -1138,2 +1175,2 @@\n-  if (foreign_call.is_non_null()) {\n-    jlong foreign_call_destination = jvmci_env()->get_HotSpotForeignCallTarget_address(foreign_call);\n+  if (tag != SITE_CALL) {\n+    jlong foreign_call_destination = target;\n@@ -1141,7 +1178,2 @@\n-  } else { \/\/ method != NULL\n-    if (debug_info.is_null()) {\n-      JVMCI_ERROR(\"debug info expected at call at %i\", pc_offset);\n-    }\n-\n-    JVMCI_event_3(\"method call\");\n-    CodeInstaller::pd_relocate_JavaMethod(buffer, hotspot_method, pc_offset, JVMCI_CHECK);\n+  } else {\n+    CodeInstaller::pd_relocate_JavaMethod(buffer, method, pc_offset, JVMCI_CHECK);\n@@ -1156,1 +1188,1 @@\n-  if (debug_info.is_non_null()) {\n+  if (tag != SITE_FOREIGN_CALL_NO_DEBUG_INFO) {\n@@ -1161,13 +1193,27 @@\n-void CodeInstaller::site_DataPatch(CodeBuffer& buffer, jint pc_offset, JVMCIObject site, JVMCI_TRAPS) {\n-  JVMCIObject reference = jvmci_env()->get_site_DataPatch_reference(site);\n-  if (reference.is_null()) {\n-    JVMCI_THROW(NullPointerException);\n-  } else if (jvmci_env()->isa_site_ConstantReference(reference)) {\n-    JVMCIObject constant = jvmci_env()->get_site_ConstantReference_constant(reference);\n-    if (constant.is_null()) {\n-      JVMCI_THROW(NullPointerException);\n-    } else if (jvmci_env()->isa_DirectHotSpotObjectConstantImpl(constant)) {\n-      if (!JVMCIENV->is_hotspot()) {\n-        JVMCIObject string = JVMCIENV->call_HotSpotJVMCIRuntime_callToString(constant, JVMCI_CHECK);\n-        const char* to_string = JVMCIENV->as_utf8_string(string);\n-        JVMCI_THROW_MSG(IllegalArgumentException, err_msg(\"Direct object constant reached the backend: %s\", to_string));\n+void CodeInstaller::site_DataPatch(CodeBuffer& buffer, jint pc_offset, HotSpotCompiledCodeStream* stream, JVMCI_TRAPS) {\n+  u1 tag = stream->read_u1(\"tag\");\n+  switch (tag) {\n+    case PATCH_OBJECT_ID:\n+    case PATCH_OBJECT_ID2:\n+    case PATCH_NARROW_OBJECT_ID:\n+    case PATCH_NARROW_OBJECT_ID2:\n+    case PATCH_JOBJECT:\n+    case PATCH_NARROW_JOBJECT: {\n+      bool narrow = tag == PATCH_NARROW_OBJECT_ID || tag == PATCH_NARROW_OBJECT_ID2  || tag == PATCH_NARROW_JOBJECT;\n+      u1 read_tag = as_read_oop_tag(stream, tag, JVMCI_CHECK);\n+      Handle obj = read_oop(stream, read_tag, JVMCI_CHECK);\n+      pd_patch_OopConstant(pc_offset, obj, narrow, JVMCI_CHECK);\n+      break;\n+    }\n+    case PATCH_METHOD:\n+    case PATCH_KLASS:\n+    case PATCH_NARROW_KLASS: {\n+      pd_patch_MetaspaceConstant(pc_offset, stream, tag, JVMCI_CHECK);\n+      break;\n+    }\n+    case PATCH_DATA_SECTION_REFERENCE: {\n+      int data_offset = stream->read_u4(\"data:offset\");\n+      if (0 <= data_offset && data_offset < _constants_size) {\n+        pd_patch_DataSectionReference(pc_offset, data_offset, JVMCI_CHECK);\n+      } else {\n+        JVMCI_ERROR(\"data offset 0x%x points outside data section (size 0x%x)%s\", data_offset, _constants_size, stream->context());\n@@ -1175,7 +1221,1 @@\n-      pd_patch_OopConstant(pc_offset, constant, JVMCI_CHECK);\n-    } else if (jvmci_env()->isa_IndirectHotSpotObjectConstantImpl(constant)) {\n-      pd_patch_OopConstant(pc_offset, constant, JVMCI_CHECK);\n-    } else if (jvmci_env()->isa_HotSpotMetaspaceConstantImpl(constant)) {\n-      pd_patch_MetaspaceConstant(pc_offset, constant, JVMCI_CHECK);\n-    } else {\n-      JVMCI_ERROR(\"unknown constant type in data patch: %s\", jvmci_env()->klass_name(constant));\n+      break;\n@@ -1183,6 +1223,2 @@\n-  } else if (jvmci_env()->isa_site_DataSectionReference(reference)) {\n-    int data_offset = jvmci_env()->get_site_DataSectionReference_offset(reference);\n-    if (0 <= data_offset && data_offset < _constants_size) {\n-      pd_patch_DataSectionReference(pc_offset, data_offset, JVMCI_CHECK);\n-    } else {\n-      JVMCI_ERROR(\"data offset 0x%X points outside data section (size 0x%X)\", data_offset, _constants_size);\n+    default: {\n+      JVMCI_ERROR(\"unknown data patch tag: %d%s\", tag, stream->context());\n@@ -1190,2 +1226,0 @@\n-  } else {\n-    JVMCI_ERROR(\"unknown data patch type: %s\", jvmci_env()->klass_name(reference));\n@@ -1195,67 +1229,59 @@\n-void CodeInstaller::site_Mark(CodeBuffer& buffer, jint pc_offset, JVMCIObject site, JVMCI_TRAPS) {\n-  JVMCIObject id_obj = jvmci_env()->get_site_Mark_id(site);\n-\n-  if (id_obj.is_non_null()) {\n-    if (!jvmci_env()->is_boxing_object(T_INT, id_obj)) {\n-      JVMCI_ERROR(\"expected Integer id, got %s\", jvmci_env()->klass_name(id_obj));\n-    }\n-    jint id = jvmci_env()->get_boxed_value(T_INT, id_obj).i;\n-\n-    address pc = _instructions->start() + pc_offset;\n-\n-    switch (id) {\n-      case UNVERIFIED_ENTRY:\n-        _offsets.set_value(CodeOffsets::Entry, pc_offset);\n-        break;\n-      case VERIFIED_ENTRY:\n-        _offsets.set_value(CodeOffsets::Verified_Entry, pc_offset);\n-        _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n-        _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n-        break;\n-      case OSR_ENTRY:\n-        _offsets.set_value(CodeOffsets::OSR_Entry, pc_offset);\n-        break;\n-      case EXCEPTION_HANDLER_ENTRY:\n-        _offsets.set_value(CodeOffsets::Exceptions, pc_offset);\n-        break;\n-      case DEOPT_HANDLER_ENTRY:\n-        _offsets.set_value(CodeOffsets::Deopt, pc_offset);\n-        break;\n-      case DEOPT_MH_HANDLER_ENTRY:\n-        _offsets.set_value(CodeOffsets::DeoptMH, pc_offset);\n-        break;\n-      case FRAME_COMPLETE:\n-        _offsets.set_value(CodeOffsets::Frame_Complete, pc_offset);\n-        break;\n-      case INVOKEVIRTUAL:\n-      case INVOKEINTERFACE:\n-      case INLINE_INVOKE:\n-      case INVOKESTATIC:\n-      case INVOKESPECIAL:\n-        _next_call_type = (MarkId) id;\n-        _invoke_mark_pc = pc;\n-        break;\n-      case POLL_NEAR:\n-      case POLL_FAR:\n-      case POLL_RETURN_NEAR:\n-      case POLL_RETURN_FAR:\n-        pd_relocate_poll(pc, id, JVMCI_CHECK);\n-        break;\n-      case CARD_TABLE_SHIFT:\n-      case CARD_TABLE_ADDRESS:\n-      case HEAP_TOP_ADDRESS:\n-      case HEAP_END_ADDRESS:\n-      case NARROW_KLASS_BASE_ADDRESS:\n-      case NARROW_OOP_BASE_ADDRESS:\n-      case CRC_TABLE_ADDRESS:\n-      case LOG_OF_HEAP_REGION_GRAIN_BYTES:\n-      case INLINE_CONTIGUOUS_ALLOCATION_SUPPORTED:\n-      case VERIFY_OOPS:\n-      case VERIFY_OOP_BITS:\n-      case VERIFY_OOP_MASK:\n-      case VERIFY_OOP_COUNT_ADDRESS:\n-        break;\n-      default:\n-        JVMCI_ERROR(\"invalid mark id: %d\", id);\n-        break;\n-    }\n+void CodeInstaller::site_Mark(CodeBuffer& buffer, jint pc_offset, HotSpotCompiledCodeStream* stream, JVMCI_TRAPS) {\n+  u1 id = stream->read_u1(\"mark:id\");\n+  address pc = _instructions->start() + pc_offset;\n+\n+  switch (id) {\n+    case UNVERIFIED_ENTRY:\n+      _offsets.set_value(CodeOffsets::Entry, pc_offset);\n+      break;\n+    case VERIFIED_ENTRY:\n+      _offsets.set_value(CodeOffsets::Verified_Entry, pc_offset);\n+      _offsets.set_value(CodeOffsets::Verified_Inline_Entry, pc_offset);\n+      _offsets.set_value(CodeOffsets::Verified_Inline_Entry_RO, pc_offset);\n+      break;\n+    case OSR_ENTRY:\n+      _offsets.set_value(CodeOffsets::OSR_Entry, pc_offset);\n+      break;\n+    case EXCEPTION_HANDLER_ENTRY:\n+      _offsets.set_value(CodeOffsets::Exceptions, pc_offset);\n+      break;\n+    case DEOPT_HANDLER_ENTRY:\n+      _offsets.set_value(CodeOffsets::Deopt, pc_offset);\n+      break;\n+    case DEOPT_MH_HANDLER_ENTRY:\n+      _offsets.set_value(CodeOffsets::DeoptMH, pc_offset);\n+      break;\n+    case FRAME_COMPLETE:\n+      _offsets.set_value(CodeOffsets::Frame_Complete, pc_offset);\n+      break;\n+    case INVOKEVIRTUAL:\n+    case INVOKEINTERFACE:\n+    case INLINE_INVOKE:\n+    case INVOKESTATIC:\n+    case INVOKESPECIAL:\n+      _next_call_type = (MarkId) id;\n+      _invoke_mark_pc = pc;\n+      break;\n+    case POLL_NEAR:\n+    case POLL_FAR:\n+    case POLL_RETURN_NEAR:\n+    case POLL_RETURN_FAR:\n+      pd_relocate_poll(pc, id, JVMCI_CHECK);\n+      break;\n+    case CARD_TABLE_SHIFT:\n+    case CARD_TABLE_ADDRESS:\n+    case HEAP_TOP_ADDRESS:\n+    case HEAP_END_ADDRESS:\n+    case NARROW_KLASS_BASE_ADDRESS:\n+    case NARROW_OOP_BASE_ADDRESS:\n+    case CRC_TABLE_ADDRESS:\n+    case LOG_OF_HEAP_REGION_GRAIN_BYTES:\n+    case INLINE_CONTIGUOUS_ALLOCATION_SUPPORTED:\n+    case VERIFY_OOPS:\n+    case VERIFY_OOP_BITS:\n+    case VERIFY_OOP_MASK:\n+    case VERIFY_OOP_COUNT_ADDRESS:\n+      break;\n+    default:\n+      JVMCI_ERROR(\"invalid mark id: %d%s\", id, stream->context());\n+      break;\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":817,"deletions":791,"binary":false,"changes":1608,"status":"modified"},{"patch":"@@ -36,2 +36,0 @@\n-elapsedTimer JVMCICompiler::_codeInstallTimer;\n-elapsedTimer JVMCICompiler::_hostedCodeInstallTimer;\n@@ -159,1 +157,31 @@\n-\/\/ Print CompileBroker compilation timers\n+void JVMCICompiler::stopping_compiler_thread(CompilerThread* current) {\n+  if (UseJVMCINativeLibrary) {\n+    JVMCIRuntime* runtime = JVMCI::compiler_runtime(current, false);\n+    if (runtime != nullptr) {\n+      MutexUnlocker unlock(CompileThread_lock);\n+      runtime->detach_thread(current, \"stopping idle compiler thread\");\n+    }\n+  }\n+}\n+\n+void JVMCICompiler::on_empty_queue(CompileQueue* queue, CompilerThread* thread) {\n+  if (UseJVMCINativeLibrary) {\n+    int delay = JVMCICompilerIdleDelay;\n+    JVMCIRuntime* runtime = JVMCI::compiler_runtime(thread, false);\n+    \/\/ Don't detach JVMCI compiler threads from their JVMCI\n+    \/\/ runtime during the VM startup grace period\n+    if (runtime != nullptr && delay > 0 && tty->time_stamp().milliseconds() > DEFAULT_COMPILER_IDLE_DELAY) {\n+      bool timeout = MethodCompileQueue_lock->wait(delay);\n+      \/\/ Unlock as detaching or repacking can result in a JNI call to shutdown a JavaVM\n+      \/\/ and locks cannot be held when making a VM to native transition.\n+      MutexUnlocker unlock(MethodCompileQueue_lock);\n+      if (timeout) {\n+        runtime->detach_thread(thread, \"releasing idle compiler thread\");\n+      } else {\n+        runtime->repack(thread);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Print compilation timers\n@@ -161,1 +189,0 @@\n-  double code_install_time = _codeInstallTimer.seconds();\n@@ -164,1 +191,4 @@\n-  tty->print_cr(\"       Install Code:   %7.3f s\", code_install_time);\n+  _jit_code_installs.print_on(tty, \"       Install Code:   \");\n+  tty->cr();\n+  tty->print_cr(\"    JVMCI Hosted Time:\");\n+  _hosted_code_installs.print_on(tty, \"       Install Code:   \");\n@@ -167,5 +197,10 @@\n-\/\/ Print non-CompileBroker compilation timers\n-void JVMCICompiler::print_hosted_timers() {\n-  double code_install_time = _hostedCodeInstallTimer.seconds();\n-  tty->print_cr(\"    JVMCI Hosted Time:\");\n-  tty->print_cr(\"       Install Code:   %7.3f s\", code_install_time);\n+void JVMCICompiler::CodeInstallStats::print_on(outputStream* st, const char* prefix) const {\n+  double time = _timer.seconds();\n+  st->print_cr(\"%s%7.3f s (installs: %d, CodeBlob total size: %d, CodeBlob code size: %d)\",\n+      prefix, time, _count, _codeBlobs_size, _codeBlobs_code_size);\n+}\n+\n+void JVMCICompiler::CodeInstallStats::on_install(CodeBlob* cb) {\n+  Atomic::inc(&_count);\n+  Atomic::add(&_codeBlobs_size, cb->size());\n+  Atomic::add(&_codeBlobs_code_size, cb->code_size());\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompiler.cpp","additions":45,"deletions":10,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -231,2 +231,6 @@\n-C2V_VMENTRY_NULL(jbyteArray, getBytecode, (JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+\/\/ Macros for argument pairs representing a wrapper object and its wrapped VM pointer\n+#define ARGUMENT_PAIR(name) jobject name ## _obj, jlong name ## _pointer\n+#define UNPACK_PAIR(type, name) ((type*) name ## _pointer)\n+\n+C2V_VMENTRY_NULL(jbyteArray, getBytecode, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -309,2 +313,2 @@\n-C2V_VMENTRY_0(jint, getExceptionTableLength, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jint, getExceptionTableLength, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -314,2 +318,2 @@\n-C2V_VMENTRY_0(jlong, getExceptionTableStart, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jlong, getExceptionTableStart, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -347,1 +351,1 @@\n-  } else if (JVMCIENV->isa_HotSpotObjectConstantImpl(base_object)) {\n+  } else {\n@@ -354,2 +358,0 @@\n-  } else if (JVMCIENV->isa_HotSpotResolvedJavaMethodImpl(base_object)) {\n-    method = JVMCIENV->asMethod(base_object);\n@@ -365,1 +367,1 @@\n-C2V_VMENTRY_NULL(jobject, getConstantPool, (JNIEnv* env, jobject, jobject object_handle))\n+C2V_VMENTRY_NULL(jobject, getConstantPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass_or_method), jboolean is_klass))\n@@ -367,2 +369,1 @@\n-  JVMCIObject object = JVMCIENV->wrap(object_handle);\n-  if (object.is_null()) {\n+  if (UNPACK_PAIR(address, klass_or_method) == 0) {\n@@ -371,4 +372,2 @@\n-  if (JVMCIENV->isa_HotSpotResolvedJavaMethodImpl(object)) {\n-    cp = JVMCIENV->asMethod(object)->constMethod()->constants();\n-  } else if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(object)) {\n-    cp = InstanceKlass::cast(JVMCIENV->asKlass(object))->constants();\n+  if (!is_klass) {\n+    cp = (UNPACK_PAIR(Method, klass_or_method))->constMethod()->constants();\n@@ -376,2 +375,1 @@\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                err_msg(\"Unexpected type: %s\", JVMCIENV->klass_name(object)));\n+    cp = InstanceKlass::cast(UNPACK_PAIR(Klass, klass_or_method))->constants();\n@@ -379,1 +377,0 @@\n-  assert(cp != NULL, \"npe\");\n@@ -390,1 +387,0 @@\n-    \/\/ klass = JVMCIENV->unhandle(base_object)->klass();\n@@ -428,3 +424,3 @@\n-C2V_VMENTRY_NULL(jobject, findUniqueConcreteMethod, (JNIEnv* env, jobject, jobject jvmci_type, jobject jvmci_method))\n-  methodHandle method (THREAD, JVMCIENV->asMethod(jvmci_method));\n-  InstanceKlass* holder = InstanceKlass::cast(JVMCIENV->asKlass(jvmci_type));\n+C2V_VMENTRY_NULL(jobject, findUniqueConcreteMethod, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), ARGUMENT_PAIR(method)))\n+  methodHandle method (THREAD, UNPACK_PAIR(Method, method));\n+  InstanceKlass* holder = InstanceKlass::cast(UNPACK_PAIR(Klass, klass));\n@@ -447,2 +443,2 @@\n-C2V_VMENTRY_NULL(jobject, getImplementor, (JNIEnv* env, jobject, jobject jvmci_type))\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n+C2V_VMENTRY_NULL(jobject, getImplementor, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n@@ -464,2 +460,2 @@\n-C2V_VMENTRY_0(jboolean, methodIsIgnoredBySecurityStackWalk,(JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jboolean, methodIsIgnoredBySecurityStackWalk,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -469,2 +465,2 @@\n-C2V_VMENTRY_0(jboolean, isCompilable,(JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jboolean, isCompilable,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -478,2 +474,2 @@\n-C2V_VMENTRY_0(jboolean, hasNeverInlineDirective,(JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method (THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_0(jboolean, hasNeverInlineDirective,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method (THREAD, UNPACK_PAIR(Method, method));\n@@ -483,2 +479,2 @@\n-C2V_VMENTRY_0(jboolean, shouldInlineMethod,(JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method (THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_0(jboolean, shouldInlineMethod,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method (THREAD, UNPACK_PAIR(Method, method));\n@@ -488,1 +484,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupType, (JNIEnv* env, jobject, jstring jname, jclass accessing_class, jboolean resolve))\n+C2V_VMENTRY_NULL(jobject, lookupType, (JNIEnv* env, jobject, jstring jname, ARGUMENT_PAIR(accessing_klass), jboolean resolve))\n@@ -498,1 +494,1 @@\n-  Klass* accessing_klass = NULL;\n+  Klass* accessing_klass = UNPACK_PAIR(Klass, accessing_klass);\n@@ -501,2 +497,1 @@\n-  if (accessing_class != NULL) {\n-    accessing_klass = JVMCIENV->asKlass(accessing_class);\n+  if (accessing_klass != nullptr) {\n@@ -513,1 +508,1 @@\n-    if (resolved_klass == NULL) {\n+    if (resolved_klass == nullptr) {\n@@ -548,6 +543,1 @@\n-C2V_VMENTRY_NULL(jobject, getArrayType, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-\n-  JVMCIObject jvmci_type_object = JVMCIENV->wrap(jvmci_type);\n+C2V_VMENTRY_NULL(jobject, getArrayType, (JNIEnv* env, jobject, jchar type_char, ARGUMENT_PAIR(klass)))\n@@ -555,2 +545,3 @@\n-  if (JVMCIENV->isa_HotSpotResolvedPrimitiveType(jvmci_type_object)) {\n-    BasicType type = JVMCIENV->kindToBasicType(JVMCIENV->get_HotSpotResolvedPrimitiveType_kind(jvmci_type_object), JVMCI_CHECK_0);\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n+    BasicType type = JVMCIENV->typeCharToBasicType(type_char, JVMCI_CHECK_0);\n@@ -558,1 +549,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -561,1 +552,1 @@\n-    if (array_klass == NULL) {\n+    if (array_klass == nullptr) {\n@@ -565,4 +556,0 @@\n-    Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-    if (klass == NULL) {\n-      JVMCI_THROW_0(NullPointerException);\n-    }\n@@ -577,2 +564,2 @@\n-  if (mirror == NULL) {\n-    return NULL;\n+  if (mirror == nullptr) {\n+    return nullptr;\n@@ -582,1 +569,1 @@\n-  if (klass == NULL) {\n+  if (klass == nullptr) {\n@@ -587,1 +574,1 @@\n-}\n+C2V_END\n@@ -589,2 +576,2 @@\n-C2V_VMENTRY_NULL(jobject, resolvePossiblyCachedConstantInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, resolvePossiblyCachedConstantInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -606,1 +593,1 @@\n-      JVMCIObject kind;\n+      jchar type_char;\n@@ -610,8 +597,8 @@\n-        case T_LONG:    kind = JVMCIENV->get_JavaKind_Long();    raw_value = value.j; break;\n-        case T_DOUBLE:  kind = JVMCIENV->get_JavaKind_Double();  raw_value = value.j; break;\n-        case T_FLOAT:   kind = JVMCIENV->get_JavaKind_Float();   raw_value = value.i; break;\n-        case T_INT:     kind = JVMCIENV->get_JavaKind_Int();     raw_value = value.i; break;\n-        case T_SHORT:   kind = JVMCIENV->get_JavaKind_Short();   raw_value = value.s; break;\n-        case T_BYTE:    kind = JVMCIENV->get_JavaKind_Byte();    raw_value = value.b; break;\n-        case T_CHAR:    kind = JVMCIENV->get_JavaKind_Char();    raw_value = value.c; break;\n-        case T_BOOLEAN: kind = JVMCIENV->get_JavaKind_Boolean(); raw_value = value.z; break;\n+        case T_LONG:    type_char = 'J'; raw_value = value.j; break;\n+        case T_DOUBLE:  type_char = 'D'; raw_value = value.j; break;\n+        case T_FLOAT:   type_char = 'F'; raw_value = value.i; break;\n+        case T_INT:     type_char = 'I'; raw_value = value.i; break;\n+        case T_SHORT:   type_char = 'S'; raw_value = value.s; break;\n+        case T_BYTE:    type_char = 'B'; raw_value = value.b; break;\n+        case T_CHAR:    type_char = 'C'; raw_value = value.c; break;\n+        case T_BOOLEAN: type_char = 'Z'; raw_value = value.z; break;\n@@ -621,1 +608,1 @@\n-      JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(kind, raw_value, JVMCI_CHECK_NULL);\n+      JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(type_char, raw_value, JVMCI_CHECK_NULL);\n@@ -628,2 +615,73 @@\n-C2V_VMENTRY_0(jint, lookupNameAndTypeRefIndexInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobjectArray, resolveBootstrapMethod, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n+  constantTag tag = cp->tag_at(index);\n+  bool is_indy = tag.is_invoke_dynamic();\n+  bool is_condy = tag.is_dynamic_constant();\n+  if (!(is_condy || is_indy)) {\n+    JVMCI_THROW_MSG_0(IllegalArgumentException, err_msg(\"Unexpected constant pool tag at index %d: %d\", index, tag.value()));\n+  }\n+  \/\/ Resolve the bootstrap specifier, its name, type, and static arguments\n+  BootstrapInfo bootstrap_specifier(cp, index);\n+  Handle bsm = bootstrap_specifier.resolve_bsm(CHECK_NULL);\n+\n+  \/\/ call java.lang.invoke.MethodHandle::asFixedArity() -> MethodHandle\n+  \/\/ to get a DirectMethodHandle from which we can then extract a Method*\n+  JavaValue result(T_OBJECT);\n+  JavaCalls::call_virtual(&result,\n+                         bsm,\n+                         vmClasses::MethodHandle_klass(),\n+                         vmSymbols::asFixedArity_name(),\n+                         vmSymbols::asFixedArity_signature(),\n+                         CHECK_NULL);\n+  bsm = Handle(THREAD, result.get_oop());\n+\n+  \/\/ Check assumption about getting a DirectMethodHandle\n+  if (!java_lang_invoke_DirectMethodHandle::is_instance(bsm())) {\n+    JVMCI_THROW_MSG_NULL(InternalError, err_msg(\"Unexpected MethodHandle subclass: %s\", bsm->klass()->external_name()));\n+  }\n+  \/\/ Create return array describing the bootstrap method invocation (BSMI)\n+  JVMCIObjectArray bsmi = JVMCIENV->new_Object_array(4, JVMCI_CHECK_NULL);\n+\n+  \/\/ Extract Method* and wrap it in a ResolvedJavaMethod\n+  Handle member = Handle(THREAD, java_lang_invoke_DirectMethodHandle::member(bsm()));\n+  JVMCIObject bsmi_method = JVMCIENV->get_jvmci_method(methodHandle(THREAD, java_lang_invoke_MemberName::vmtarget(member())), JVMCI_CHECK_NULL);\n+  JVMCIENV->put_object_at(bsmi, 0, bsmi_method);\n+\n+  JVMCIObject bsmi_name = JVMCIENV->create_string(bootstrap_specifier.name(), JVMCI_CHECK_NULL);\n+  JVMCIENV->put_object_at(bsmi, 1, bsmi_name);\n+\n+  Handle type_arg = bootstrap_specifier.type_arg();\n+  JVMCIObject bsmi_type = JVMCIENV->get_object_constant(type_arg());\n+  JVMCIENV->put_object_at(bsmi, 2, bsmi_type);\n+\n+  Handle arg_values = bootstrap_specifier.arg_values();\n+  if (arg_values.not_null()) {\n+    if (!arg_values->is_array()) {\n+      JVMCIENV->put_object_at(bsmi, 3, JVMCIENV->get_object_constant(arg_values()));\n+    } else if (arg_values->is_objArray()) {\n+      objArrayHandle args_array = objArrayHandle(THREAD, (objArrayOop) arg_values());\n+      int len = args_array->length();\n+      JVMCIObjectArray arguments = JVMCIENV->new_JavaConstant_array(len, JVMCI_CHECK_NULL);\n+      JVMCIENV->put_object_at(bsmi, 3, arguments);\n+      for (int i = 0; i < len; i++) {\n+        oop x = args_array->obj_at(i);\n+        if (x != nullptr) {\n+          JVMCIENV->put_object_at(arguments, i, JVMCIENV->get_object_constant(x));\n+        } else {\n+          JVMCIENV->put_object_at(arguments, i, JVMCIENV->get_JavaConstant_NULL_POINTER());\n+        }\n+      }\n+    } else if (arg_values->is_typeArray()) {\n+      typeArrayHandle bsci = typeArrayHandle(THREAD, (typeArrayOop) arg_values());\n+      JVMCIPrimitiveArray arguments = JVMCIENV->new_intArray(bsci->length(), JVMCI_CHECK_NULL);\n+      JVMCIENV->put_object_at(bsmi, 3, arguments);\n+      for (int i = 0; i < bsci->length(); i++) {\n+        JVMCIENV->put_int_at(arguments, i, bsci->int_at(i));\n+      }\n+    }\n+  }\n+  return JVMCIENV->get_jobjectArray(bsmi);\n+C2V_END\n+\n+C2V_VMENTRY_0(jint, lookupNameAndTypeRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -633,2 +691,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupNameInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint which))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupNameInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -639,2 +697,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupSignatureInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint which))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupSignatureInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -645,2 +703,2 @@\n-C2V_VMENTRY_0(jint, lookupKlassRefIndexInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_0(jint, lookupKlassRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -650,2 +708,2 @@\n-C2V_VMENTRY_NULL(jobject, resolveTypeInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, resolveTypeInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -665,2 +723,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupKlassInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index, jbyte opcode))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupKlassInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -670,1 +728,1 @@\n-  Symbol* symbol = NULL;\n+  Symbol* symbol = nullptr;\n@@ -693,2 +751,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupAppendixInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupAppendixInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -699,2 +757,2 @@\n-C2V_VMENTRY_NULL(jobject, lookupMethodInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index, jbyte opcode))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, lookupMethodInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jbyte opcode))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -708,2 +766,2 @@\n-C2V_VMENTRY_0(jint, constantPoolRemapInstructionOperandFromCache, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_0(jint, constantPoolRemapInstructionOperandFromCache, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -713,2 +771,2 @@\n-C2V_VMENTRY_NULL(jobject, resolveFieldInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index, jobject jvmci_method, jbyte opcode, jintArray info_handle))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_NULL(jobject, resolveFieldInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, ARGUMENT_PAIR(method), jbyte opcode, jintArray info_handle))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -717,1 +775,1 @@\n-  methodHandle mh(THREAD, (jvmci_method != NULL) ? JVMCIENV->asMethod(jvmci_method) : NULL);\n+  methodHandle mh(THREAD, UNPACK_PAIR(Method, method));\n@@ -732,3 +790,3 @@\n-C2V_VMENTRY_0(jint, getVtableIndexForInterfaceMethod, (JNIEnv* env, jobject, jobject jvmci_type, jobject jvmci_method))\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_0(jint, getVtableIndexForInterfaceMethod, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), ARGUMENT_PAIR(method)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -754,4 +812,4 @@\n-C2V_VMENTRY_NULL(jobject, resolveMethod, (JNIEnv* env, jobject, jobject receiver_jvmci_type, jobject jvmci_method, jobject caller_jvmci_type))\n-  Klass* recv_klass = JVMCIENV->asKlass(receiver_jvmci_type);\n-  Klass* caller_klass = JVMCIENV->asKlass(caller_jvmci_type);\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_NULL(jobject, resolveMethod, (JNIEnv* env, jobject, ARGUMENT_PAIR(receiver), ARGUMENT_PAIR(method), ARGUMENT_PAIR(caller)))\n+  Klass* recv_klass = UNPACK_PAIR(Klass, receiver);\n+  Klass* caller_klass = UNPACK_PAIR(Klass, caller);\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -765,1 +823,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -780,1 +838,1 @@\n-  Method* m = NULL;\n+  Method* m = nullptr;\n@@ -792,1 +850,1 @@\n-  if (m == NULL) {\n+  if (m == nullptr) {\n@@ -794,1 +852,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -801,3 +859,3 @@\n-C2V_VMENTRY_0(jboolean, hasFinalizableSubclass,(JNIEnv* env, jobject, jobject jvmci_type))\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  assert(klass != NULL, \"method must not be called for primitive types\");\n+C2V_VMENTRY_0(jboolean, hasFinalizableSubclass,(JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  assert(klass != nullptr, \"method must not be called for primitive types\");\n@@ -808,1 +866,1 @@\n-  return Dependencies::find_finalizable_subclass(iklass) != NULL;\n+  return Dependencies::find_finalizable_subclass(iklass) != nullptr;\n@@ -811,2 +869,2 @@\n-C2V_VMENTRY_NULL(jobject, getClassInitializer, (JNIEnv* env, jobject, jobject jvmci_type))\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n+C2V_VMENTRY_NULL(jobject, getClassInitializer, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n@@ -814,1 +872,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -832,2 +890,2 @@\n-C2V_VMENTRY(void, setNotInlinableOrCompilable,(JNIEnv* env, jobject,  jobject jvmci_method))\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY(void, setNotInlinableOrCompilable,(JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -839,2 +897,22 @@\n-C2V_VMENTRY_0(jint, installCode, (JNIEnv *env, jobject, jobject target, jobject compiled_code,\n-            jobject installed_code, jlong failed_speculations_address, jbyteArray speculations_obj))\n+C2V_VMENTRY_0(jint, getInstallCodeFlags, (JNIEnv *env, jobject))\n+  int flags = 0;\n+#ifndef PRODUCT\n+  flags |= 0x0001; \/\/ VM will install block comments\n+  flags |= 0x0004; \/\/ Enable HotSpotJVMCIRuntime.Option.CodeSerializationTypeInfo if not explicitly set\n+#endif\n+  if (JvmtiExport::can_hotswap_or_post_breakpoint()) {\n+    \/\/ VM needs to track method dependencies\n+    flags |= 0x0002;\n+  }\n+  return flags;\n+C2V_END\n+\n+C2V_VMENTRY_0(jint, installCode0, (JNIEnv *env, jobject,\n+    jlong compiled_code_buffer,\n+    jlong serialization_ns,\n+    bool with_type_info,\n+    jobject compiled_code,\n+    jobjectArray object_pool,\n+    jobject installed_code,\n+    jlong failed_speculations_address,\n+    jbyteArray speculations_obj))\n@@ -844,1 +922,2 @@\n-  JVMCIObject target_handle = JVMCIENV->wrap(target);\n+  objArrayHandle object_pool_handle(thread, JVMCIENV->is_hotspot() ? (objArrayOop) JNIHandles::resolve(object_pool) : nullptr);\n+\n@@ -855,2 +934,4 @@\n-\n-  TraceTime install_time(\"installCode\", JVMCICompiler::codeInstallTimer(!thread->is_Compiler_thread()));\n+  JVMCICompiler::CodeInstallStats* stats = compiler->code_install_stats(!thread->is_Compiler_thread());\n+  elapsedTimer *timer = stats->timer();\n+  timer->add_nanoseconds(serialization_ns);\n+  TraceTime install_time(\"installCode\", timer);\n@@ -861,1 +942,2 @@\n-      target_handle,\n+      compiled_code_buffer,\n+      with_type_info,\n@@ -863,0 +945,1 @@\n+      object_pool_handle,\n@@ -885,0 +968,1 @@\n+    stats->on_install(cb);\n@@ -901,4 +985,0 @@\n-C2V_VMENTRY_0(jint, getMetadata, (JNIEnv *env, jobject, jobject target, jobject compiled_code, jobject metadata))\n-  JVMCI_THROW_MSG_0(InternalError, \"unimplemented\");\n-C2V_END\n-\n@@ -950,1 +1030,1 @@\n-C2V_VMENTRY_NULL(jobject, getStackTraceElement, (JNIEnv* env, jobject, jobject jvmci_method, int bci))\n+C2V_VMENTRY_NULL(jobject, getStackTraceElement, (JNIEnv* env, jobject, ARGUMENT_PAIR(method), int bci))\n@@ -953,1 +1033,1 @@\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -1008,2 +1088,2 @@\n-C2V_VMENTRY_NULL(jlongArray, getLineNumberTable, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_NULL(jlongArray, getLineNumberTable, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -1025,1 +1105,0 @@\n-    \/\/ FIXME: Why was this long before?\n@@ -1036,2 +1115,2 @@\n-C2V_VMENTRY_0(jlong, getLocalVariableTableStart, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jlong, getLocalVariableTableStart, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -1044,2 +1123,2 @@\n-C2V_VMENTRY_0(jint, getLocalVariableTableLength, (JNIEnv* env, jobject, jobject jvmci_method))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jint, getLocalVariableTableLength, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -1049,2 +1128,2 @@\n-C2V_VMENTRY(void, reprofile, (JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY(void, reprofile, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -1097,1 +1176,1 @@\n-C2V_VMENTRY_0(jint, allocateCompileId, (JNIEnv* env, jobject, jobject jvmci_method, int entry_bci))\n+C2V_VMENTRY_0(jint, allocateCompileId, (JNIEnv* env, jobject, ARGUMENT_PAIR(method), int entry_bci))\n@@ -1099,1 +1178,2 @@\n-  if (jvmci_method == NULL) {\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n+  if (method.is_null()) {\n@@ -1102,1 +1182,0 @@\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n@@ -1110,2 +1189,2 @@\n-C2V_VMENTRY_0(jboolean, isMature, (JNIEnv* env, jobject, jlong metaspace_method_data))\n-  MethodData* mdo = JVMCIENV->asMethodData(metaspace_method_data);\n+C2V_VMENTRY_0(jboolean, isMature, (JNIEnv* env, jobject, jlong method_data_pointer))\n+  MethodData* mdo = (MethodData*) method_data_pointer;\n@@ -1115,2 +1194,2 @@\n-C2V_VMENTRY_0(jboolean, hasCompiledCodeForOSR, (JNIEnv* env, jobject, jobject jvmci_method, int entry_bci, int comp_level))\n-  Method* method = JVMCIENV->asMethod(jvmci_method);\n+C2V_VMENTRY_0(jboolean, hasCompiledCodeForOSR, (JNIEnv* env, jobject, ARGUMENT_PAIR(method), int entry_bci, int comp_level))\n+  Method* method = UNPACK_PAIR(Method, method);\n@@ -1125,0 +1204,6 @@\n+C2V_VMENTRY_NULL(jobject, getSignatureName, (JNIEnv* env, jobject, jlong klass_pointer))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  JVMCIObject signature = JVMCIENV->create_string(klass->signature_name(), JVMCI_CHECK_NULL);\n+  return JVMCIENV->get_jobject(signature);\n+C2V_END\n+\n@@ -1370,2 +1455,2 @@\n-C2V_VMENTRY(void, resolveInvokeDynamicInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY(void, resolveInvokeDynamicInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -1378,2 +1463,2 @@\n-C2V_VMENTRY(void, resolveInvokeHandleInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY(void, resolveInvokeHandleInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -1390,2 +1475,2 @@\n-C2V_VMENTRY_0(jint, isResolvedInvokeHandleInPool, (JNIEnv* env, jobject, jobject jvmci_constant_pool, jint index))\n-  constantPoolHandle cp(THREAD, JVMCIENV->asConstantPool(jvmci_constant_pool));\n+C2V_VMENTRY_0(jint, isResolvedInvokeHandleInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  constantPoolHandle cp(THREAD, UNPACK_PAIR(ConstantPool, cp));\n@@ -1521,1 +1606,1 @@\n-    GrowableArray<ScopeValue*>* scopeLocals = cvf->scope()->locals();\n+    GrowableArray<ScopeValue*>* extentLocals = cvf->scope()->locals();\n@@ -1526,1 +1611,1 @@\n-        if (var->type() == T_OBJECT && scopeLocals->at(i2)->is_object()) {\n+        if (var->type() == T_OBJECT && extentLocals->at(i2)->is_object()) {\n@@ -1589,2 +1674,2 @@\n-C2V_VMENTRY_0(jint, methodDataProfileDataSize, (JNIEnv* env, jobject, jlong metaspace_method_data, jint position))\n-  MethodData* mdo = JVMCIENV->asMethodData(metaspace_method_data);\n+C2V_VMENTRY_0(jint, methodDataProfileDataSize, (JNIEnv* env, jobject, jlong method_data_pointer, jint position))\n+  MethodData* mdo = (MethodData*) method_data_pointer;\n@@ -1607,6 +1692,3 @@\n-C2V_VMENTRY_0(jlong, getFingerprint, (JNIEnv* env, jobject, jlong metaspace_klass))\n-  JVMCI_THROW_MSG_0(InternalError, \"unimplemented\");\n-C2V_END\n-\n-C2V_VMENTRY_NULL(jobject, getInterfaces, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n+C2V_VMENTRY_NULL(jobject, getInterfaces, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1616,4 +1698,0 @@\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  if (klass == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n@@ -1638,2 +1716,3 @@\n-C2V_VMENTRY_NULL(jobject, getComponentType, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n+C2V_VMENTRY_NULL(jobject, getComponentType, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1643,5 +1722,2 @@\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  oop mirror = klass->java_mirror();\n-  if (java_lang_Class::is_primitive(mirror) ||\n-      !java_lang_Class::as_Klass(mirror)->is_array_klass()) {\n-    return NULL;\n+  if (!klass->is_array_klass()) {\n+    return nullptr;\n@@ -1649,1 +1725,1 @@\n-\n+  oop mirror = klass->java_mirror();\n@@ -1651,2 +1727,3 @@\n-  if (component_mirror == NULL) {\n-    return NULL;\n+  if (component_mirror == nullptr) {\n+    JVMCI_THROW_MSG_0(NullPointerException,\n+                    err_msg(\"Component mirror for array class %s is null\", klass->external_name()))\n@@ -1654,0 +1731,1 @@\n+\n@@ -1656,2 +1734,1 @@\n-    JVMCIKlassHandle klass_handle(THREAD);\n-    klass_handle = component_klass;\n+    JVMCIKlassHandle klass_handle(THREAD, component_klass);\n@@ -1666,2 +1743,3 @@\n-C2V_VMENTRY(void, ensureInitialized, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n+C2V_VMENTRY(void, ensureInitialized, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1670,3 +1748,1 @@\n-\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  if (klass != NULL && klass->should_be_initialized()) {\n+  if (klass->should_be_initialized()) {\n@@ -1678,2 +1754,3 @@\n-C2V_VMENTRY(void, ensureLinked, (JNIEnv* env, jobject, jobject jvmci_type))\n-  if (jvmci_type == NULL) {\n+C2V_VMENTRY(void, ensureLinked, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1682,3 +1759,1 @@\n-\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n-  if (klass != NULL && klass->is_instance_klass()) {\n+  if (klass->is_instance_klass()) {\n@@ -1812,2 +1887,3 @@\n-C2V_VMENTRY_NULL(jobjectArray, getDeclaredConstructors, (JNIEnv* env, jobject, jobject holder))\n-  if (holder == NULL) {\n+C2V_VMENTRY_NULL(jobjectArray, getDeclaredConstructors, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1816,1 +1892,0 @@\n-  Klass* klass = JVMCIENV->asKlass(holder);\n@@ -1842,2 +1917,3 @@\n-C2V_VMENTRY_NULL(jobjectArray, getDeclaredMethods, (JNIEnv* env, jobject, jobject holder))\n-  if (holder == NULL) {\n+C2V_VMENTRY_NULL(jobjectArray, getDeclaredMethods, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -1846,1 +1922,0 @@\n-  Klass* klass = JVMCIENV->asKlass(holder);\n@@ -1872,39 +1947,1 @@\n-C2V_VMENTRY_NULL(jobject, readFieldValue, (JNIEnv* env, jobject, jobject object, jobject expected_type, jlong displacement, jobject kind_object))\n-  if (object == NULL || kind_object == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-\n-  JVMCIObject kind = JVMCIENV->wrap(kind_object);\n-  BasicType basic_type = JVMCIENV->kindToBasicType(kind, JVMCI_CHECK_NULL);\n-\n-  InstanceKlass* holder = NULL;\n-  if (expected_type != NULL) {\n-    holder = InstanceKlass::cast(JVMCIENV->asKlass(JVMCIENV->wrap(expected_type)));\n-  }\n-\n-  bool is_static = false;\n-  Handle obj;\n-  JVMCIObject base = JVMCIENV->wrap(object);\n-  if (JVMCIENV->isa_HotSpotObjectConstantImpl(base)) {\n-    obj = JVMCIENV->asConstant(base, JVMCI_CHECK_NULL);\n-    \/\/ asConstant will throw an NPE if a constant contains NULL\n-\n-    if (holder != NULL && !obj->is_a(holder)) {\n-      \/\/ Not a subtype of field holder\n-      return NULL;\n-    }\n-    is_static = false;\n-    if (holder == NULL && java_lang_Class::is_instance(obj()) && displacement >= InstanceMirrorKlass::offset_of_static_fields()) {\n-      is_static = true;\n-    }\n-  } else if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(base)) {\n-    is_static = true;\n-    Klass* klass = JVMCIENV->asKlass(base);\n-    if (holder != NULL && holder != klass) {\n-      return NULL;\n-    }\n-    obj = Handle(THREAD, klass->java_mirror());\n-  } else {\n-    \/\/ The Java code is expected to guard against this path\n-    ShouldNotReachHere();\n-  }\n+static jobject read_field_value(Handle obj, long displacement, jchar type_char, bool is_static, Thread* THREAD, JVMCIEnv* JVMCIENV) {\n@@ -1912,0 +1949,1 @@\n+  BasicType basic_type = JVMCIENV->typeCharToBasicType(type_char, JVMCI_CHECK_NULL);\n@@ -1978,1 +2016,1 @@\n-          (java_lang_Class::as_Klass(obj()) == NULL || !java_lang_Class::as_Klass(obj())->is_array_klass())) {\n+          (java_lang_Class::as_Klass(obj()) == nullptr || !java_lang_Class::as_Klass(obj())->is_array_klass())) {\n@@ -1986,1 +2024,1 @@\n-      if (value == NULL) {\n+      if (value == nullptr) {\n@@ -1989,1 +2027,1 @@\n-        if (value != NULL && !oopDesc::is_oop(value)) {\n+        if (value != nullptr && !oopDesc::is_oop(value)) {\n@@ -2006,1 +2044,1 @@\n-  JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(kind, value, JVMCI_CHECK_NULL);\n+  JVMCIObject result = JVMCIENV->call_JavaConstant_forPrimitive(type_char, value, JVMCI_CHECK_NULL);\n@@ -2008,0 +2046,26 @@\n+}\n+\n+C2V_VMENTRY_NULL(jobject, readStaticFieldValue, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), long displacement, jchar type_char))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  Handle obj(THREAD, klass->java_mirror());\n+  return read_field_value(obj, displacement, type_char, true, THREAD, JVMCIENV);\n+C2V_END\n+\n+C2V_VMENTRY_NULL(jobject, readFieldValue, (JNIEnv* env, jobject, jobject object, ARGUMENT_PAIR(expected_type), long displacement, jchar type_char))\n+  if (object == nullptr) {\n+    JVMCI_THROW_0(NullPointerException);\n+  }\n+\n+  \/\/ asConstant will throw an NPE if a constant contains NULL\n+  Handle obj = JVMCIENV->asConstant(JVMCIENV->wrap(object), JVMCI_CHECK_NULL);\n+\n+  Klass* expected_klass = UNPACK_PAIR(Klass, expected_type);\n+  if (expected_klass != nullptr) {\n+    InstanceKlass* expected_iklass = InstanceKlass::cast(expected_klass);\n+    if (!obj->is_a(expected_iklass)) {\n+      \/\/ Not of the expected type\n+      return nullptr;\n+    }\n+  }\n+  bool is_static = expected_klass == nullptr && java_lang_Class::is_instance(obj()) && displacement >= InstanceMirrorKlass::offset_of_static_fields();\n+  return read_field_value(obj, displacement, type_char, is_static, THREAD, JVMCIENV);\n@@ -2010,2 +2074,3 @@\n-C2V_VMENTRY_0(jboolean, isInstance, (JNIEnv* env, jobject, jobject holder, jobject object))\n-  if (object == NULL || holder == NULL) {\n+C2V_VMENTRY_0(jboolean, isInstance, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), jobject object))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (object == NULL || klass == nullptr) {\n@@ -2015,1 +2080,0 @@\n-  Klass* klass = JVMCIENV->asKlass(JVMCIENV->wrap(holder));\n@@ -2019,2 +2083,4 @@\n-C2V_VMENTRY_0(jboolean, isAssignableFrom, (JNIEnv* env, jobject, jobject holder, jobject otherHolder))\n-  if (holder == NULL || otherHolder == NULL) {\n+C2V_VMENTRY_0(jboolean, isAssignableFrom, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), ARGUMENT_PAIR(subklass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  Klass* subklass = UNPACK_PAIR(Klass, subklass);\n+  if (klass == nullptr || subklass == nullptr) {\n@@ -2023,3 +2089,1 @@\n-  Klass* klass = JVMCIENV->asKlass(JVMCIENV->wrap(holder));\n-  Klass* otherKlass = JVMCIENV->asKlass(JVMCIENV->wrap(otherHolder));\n-  return otherKlass->is_subtype_of(klass);\n+  return subklass->is_subtype_of(klass);\n@@ -2028,2 +2092,3 @@\n-C2V_VMENTRY_0(jboolean, isTrustedForIntrinsics, (JNIEnv* env, jobject, jobject holder))\n-  if (holder == NULL) {\n+C2V_VMENTRY_0(jboolean, isTrustedForIntrinsics, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -2032,1 +2097,1 @@\n-  InstanceKlass* ik = InstanceKlass::cast(JVMCIENV->asKlass(JVMCIENV->wrap(holder)));\n+  InstanceKlass* ik = InstanceKlass::cast(klass);\n@@ -2074,1 +2139,1 @@\n-  return JVMCIENV->resolve_handle(xHandle) == JVMCIENV->resolve_handle(yHandle);\n+  return JVMCIENV->resolve_oop_handle(xHandle) == JVMCIENV->resolve_oop_handle(yHandle);\n@@ -2077,2 +2142,3 @@\n-C2V_VMENTRY_NULL(jobject, getJavaMirror, (JNIEnv* env, jobject, jobject object))\n-  if (object == NULL) {\n+C2V_VMENTRY_NULL(jobject, getJavaMirror, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n@@ -2081,10 +2147,1 @@\n-  JVMCIObject base_object = JVMCIENV->wrap(object);\n-  Handle mirror;\n-  if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(base_object)) {\n-    mirror = Handle(THREAD, JVMCIENV->asKlass(base_object)->java_mirror());\n-  } else if (JVMCIENV->isa_HotSpotResolvedPrimitiveType(base_object)) {\n-    mirror = JVMCIENV->asConstant(JVMCIENV->get_HotSpotResolvedPrimitiveType_mirror(base_object), JVMCI_CHECK_NULL);\n-  } else {\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                         err_msg(\"Unexpected type: %s\", JVMCIENV->klass_name(base_object)));\n- }\n+  Handle mirror(THREAD, klass->java_mirror());\n@@ -2148,5 +2205,2 @@\n-C2V_VMENTRY_0(jint, arrayBaseOffset, (JNIEnv* env, jobject, jobject kind))\n-  if (kind == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  BasicType type = JVMCIENV->kindToBasicType(JVMCIENV->wrap(kind), JVMCI_CHECK_0);\n+C2V_VMENTRY_0(jint, arrayBaseOffset, (JNIEnv* env, jobject, jchar type_char))\n+  BasicType type = JVMCIENV->typeCharToBasicType(type_char, JVMCI_CHECK_0);\n@@ -2156,5 +2210,2 @@\n-C2V_VMENTRY_0(jint, arrayIndexScale, (JNIEnv* env, jobject, jobject kind))\n-  if (kind == NULL) {\n-    JVMCI_THROW_0(NullPointerException);\n-  }\n-  BasicType type = JVMCIENV->kindToBasicType(JVMCIENV->wrap(kind), JVMCI_CHECK_0);\n+C2V_VMENTRY_0(jint, arrayIndexScale, (JNIEnv* env, jobject, jchar type_char))\n+  BasicType type = JVMCIENV->typeCharToBasicType(type_char, JVMCI_CHECK_0);\n@@ -2164,4 +2215,3 @@\n-C2V_VMENTRY(void, deleteGlobalHandle, (JNIEnv* env, jobject, jlong h))\n-  jobject handle = (jobject)(address)h;\n-  if (handle != NULL) {\n-    JVMCIENV->runtime()->destroy_global(handle);\n+C2V_VMENTRY(void, deleteGlobalHandle, (JNIEnv* env, jobject, jlong handle))\n+  if (handle != 0) {\n+    JVMCIENV->runtime()->destroy_oop_handle(handle);\n@@ -2182,1 +2232,1 @@\n-  JVMCIRuntime* runtime = JVMCI::compiler_runtime();\n+  JVMCIRuntime* runtime;\n@@ -2188,1 +2238,1 @@\n-    JVMCIObject receiver = runtime->get_HotSpotJVMCIRuntime(peerEnv);\n+    runtime = JVMCI::compiler_runtime(thread);\n@@ -2263,1 +2313,1 @@\n-  if (thread == NULL) {\n+  if (thread == nullptr || thread->libjvmci_runtime() == nullptr) {\n@@ -2271,2 +2321,2 @@\n-    JVMCIRuntime* runtime = JVMCI::compiler_runtime();\n-    if (runtime == NULL || !runtime->has_shared_library_javavm()) {\n+    JVMCIRuntime* runtime = thread->libjvmci_runtime();\n+    if (runtime == nullptr || !runtime->has_shared_library_javavm()) {\n@@ -2290,7 +2340,36 @@\n-C2V_VMENTRY_PREFIX(jboolean, attachCurrentThread, (JNIEnv* env, jobject c2vm, jbyteArray name, jboolean as_daemon))\n-  if (thread == NULL) {\n-    \/\/ Called from unattached JVMCI shared library thread\n-    guarantee(name != NULL, \"libjvmci caller must pass non-null name\");\n-\n-    extern struct JavaVM_ main_vm;\n-    JNIEnv* hotspotEnv;\n+\/\/ Attaches a thread started in a JVMCI shared library to a JavaThread and JVMCI runtime.\n+static void attachSharedLibraryThread(JNIEnv* env, jbyteArray name, jboolean as_daemon) {\n+  JavaVM* javaVM = nullptr;\n+  jint res = env->GetJavaVM(&javaVM);\n+  if (res != JNI_OK) {\n+    JNI_THROW(\"attachSharedLibraryThread\", InternalError, err_msg(\"Error getting shared library JavaVM from shared library JNIEnv: %d\", res));\n+  }\n+  extern struct JavaVM_ main_vm;\n+  JNIEnv* hotspotEnv;\n+\n+  int name_len = env->GetArrayLength(name);\n+  char name_buf[64]; \/\/ Cannot use Resource heap as it requires a current thread\n+  int to_copy = MIN2(name_len, (int) sizeof(name_buf) - 1);\n+  env->GetByteArrayRegion(name, 0, to_copy, (jbyte*) name_buf);\n+  name_buf[to_copy] = '\\0';\n+  JavaVMAttachArgs attach_args;\n+  attach_args.version = JNI_VERSION_1_2;\n+  attach_args.name = name_buf;\n+  attach_args.group = nullptr;\n+  res = as_daemon ? main_vm.AttachCurrentThreadAsDaemon((void**)&hotspotEnv, &attach_args) :\n+                    main_vm.AttachCurrentThread((void**)&hotspotEnv, &attach_args);\n+  if (res != JNI_OK) {\n+    JNI_THROW(\"attachSharedLibraryThread\", InternalError, err_msg(\"Trying to attach thread returned %d\", res));\n+  }\n+  JavaThread* thread = get_current_thread(false);\n+  const char* attach_error;\n+  {\n+    \/\/ Transition to VM\n+    JVMCI_VM_ENTRY_MARK\n+    attach_error = JVMCIRuntime::attach_shared_library_thread(thread, javaVM);\n+    \/\/ Transition back to Native\n+  }\n+  if (attach_error != nullptr) {\n+    JNI_THROW(\"attachCurrentThread\", InternalError, attach_error);\n+  }\n+}\n@@ -2298,14 +2377,3 @@\n-    int name_len = env->GetArrayLength(name);\n-    char name_buf[64]; \/\/ Cannot use Resource heap as it requires a current thread\n-    int to_copy = MIN2(name_len, (int) sizeof(name_buf) - 1);\n-    env->GetByteArrayRegion(name, 0, to_copy, (jbyte*) name_buf);\n-    name_buf[to_copy] = '\\0';\n-    JavaVMAttachArgs attach_args;\n-    attach_args.version = JNI_VERSION_1_2;\n-    attach_args.name = name_buf;\n-    attach_args.group = NULL;\n-    jint res = as_daemon ? main_vm.AttachCurrentThreadAsDaemon((void**) &hotspotEnv, &attach_args) :\n-                           main_vm.AttachCurrentThread((void**) &hotspotEnv, &attach_args);\n-    if (res != JNI_OK) {\n-      JNI_THROW_(\"attachCurrentThread\", InternalError, err_msg(\"Trying to attach thread returned %d\", res), false);\n-    }\n+C2V_VMENTRY_PREFIX(jboolean, attachCurrentThread, (JNIEnv* env, jobject c2vm, jbyteArray name, jboolean as_daemon, jlongArray javaVM_info))\n+  if (thread == nullptr) {\n+    attachSharedLibraryThread(env, name, as_daemon);\n@@ -2319,3 +2387,24 @@\n-    JVMCIRuntime* runtime = JVMCI::compiler_runtime();\n-    if (runtime == NULL || !runtime->has_shared_library_javavm()) {\n-        JVMCI_THROW_MSG_0(IllegalStateException, \"Require JVMCI shared library JavaVM to be initialized in attachCurrentThread\");\n+\n+    JVMCIRuntime* runtime = JVMCI::compiler_runtime(thread);\n+    JNIEnv* peerJNIEnv;\n+    if (runtime->has_shared_library_javavm()) {\n+      if (runtime->GetEnv(thread, (void**)&peerJNIEnv, JNI_VERSION_1_2) == JNI_OK) {\n+        \/\/ Already attached\n+        runtime->init_JavaVM_info(javaVM_info, JVMCI_CHECK_0);\n+        return false;\n+      }\n+    }\n+\n+    {\n+      \/\/ Ensure the JVMCI shared library runtime is initialized.\n+      JVMCIEnv __peer_jvmci_env__(thread, false, __FILE__, __LINE__);\n+      JVMCIEnv* peerJVMCIEnv = &__peer_jvmci_env__;\n+      HandleMark hm(thread);\n+      JVMCIObject receiver = runtime->get_HotSpotJVMCIRuntime(peerJVMCIEnv);\n+      if (peerJVMCIEnv->has_pending_exception()) {\n+        peerJVMCIEnv->describe_pending_exception(true);\n+      }\n+      char* sl_path;\n+      if (JVMCI::get_shared_library(sl_path, false) == nullptr) {\n+        JVMCI_THROW_MSG_0(InternalError, \"Error initializing JVMCI runtime\");\n+      }\n@@ -2327,2 +2416,1 @@\n-    attach_args.group = NULL;\n-    JNIEnv* peerJNIEnv;\n+    attach_args.group = nullptr;\n@@ -2336,2 +2424,3 @@\n-      guarantee(peerJNIEnv != NULL, \"must be\");\n-      JVMCI_event_1(\"attached to JavaVM for JVMCI runtime %d\", runtime->id());\n+      guarantee(peerJNIEnv != nullptr, \"must be\");\n+      runtime->init_JavaVM_info(javaVM_info, JVMCI_CHECK_0);\n+      JVMCI_event_1(\"attached to JavaVM[%d] for JVMCI runtime %d\", runtime->get_shared_library_javavm_id(), runtime->id());\n@@ -2346,2 +2435,2 @@\n-C2V_VMENTRY_PREFIX(void, detachCurrentThread, (JNIEnv* env, jobject c2vm))\n-  if (thread == NULL) {\n+C2V_VMENTRY_PREFIX(jboolean, detachCurrentThread, (JNIEnv* env, jobject c2vm, jboolean release))\n+  if (thread == nullptr) {\n@@ -2349,1 +2438,1 @@\n-    JNI_THROW(\"detachCurrentThread\", IllegalStateException, \"Cannot detach non-attached thread\");\n+    JNI_THROW_(\"detachCurrentThread\", IllegalStateException, \"Cannot detach non-attached thread\", false);\n@@ -2355,5 +2444,5 @@\n-    requireJVMCINativeLibrary(JVMCI_CHECK);\n-    requireInHotSpot(\"detachCurrentThread\", JVMCI_CHECK);\n-    JVMCIRuntime* runtime = JVMCI::compiler_runtime();\n-    if (runtime == NULL || !runtime->has_shared_library_javavm()) {\n-      JVMCI_THROW_MSG(IllegalStateException, \"Require JVMCI shared library JavaVM to be initialized in detachCurrentThread\");\n+    requireJVMCINativeLibrary(JVMCI_CHECK_0);\n+    requireInHotSpot(\"detachCurrentThread\", JVMCI_CHECK_0);\n+    JVMCIRuntime* runtime = thread->libjvmci_runtime();\n+    if (runtime == nullptr || !runtime->has_shared_library_javavm()) {\n+      JVMCI_THROW_MSG_0(IllegalStateException, \"Require JVMCI shared library JavaVM to be initialized in detachCurrentThread\");\n@@ -2361,3 +2450,4 @@\n-    JNIEnv* peerJNIEnv;\n-    if (runtime->GetEnv(thread, (void**) &peerJNIEnv, JNI_VERSION_1_2) != JNI_OK) {\n-      JVMCI_THROW_MSG(IllegalStateException, err_msg(\"Cannot detach non-attached thread: %s\", thread->name()));\n+    JNIEnv* peerEnv;\n+\n+    if (runtime->GetEnv(thread, (void**) &peerEnv, JNI_VERSION_1_2) != JNI_OK) {\n+      JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"Cannot detach non-attached thread: %s\", thread->name()));\n@@ -2367,1 +2457,6 @@\n-      JVMCI_THROW_MSG(InternalError, err_msg(\"Error %d while attaching %s\", res, thread->name()));\n+      JVMCI_THROW_MSG_0(InternalError, err_msg(\"Error %d while attaching %s\", res, thread->name()));\n+    }\n+    JVMCI_event_1(\"detached from JavaVM[%d] for JVMCI runtime %d\",\n+        runtime->get_shared_library_javavm_id(), runtime->id());\n+    if (release) {\n+      return runtime->detach_thread(thread, \"user thread detach\");\n@@ -2371,0 +2466,15 @@\n+    if (release) {\n+      JNI_THROW_(\"detachCurrentThread\", InternalError, \"JVMCI shared library thread cannot release JVMCI shared library JavaVM\", false);\n+    }\n+    JVMCIRuntime* runtime = thread->libjvmci_runtime();\n+    if (runtime == nullptr) {\n+      JNI_THROW_(\"detachCurrentThread\", InternalError, \"JVMCI shared library thread should have a JVMCI runtime\", false);\n+    }\n+    {\n+      \/\/ Transition to VM\n+      C2V_BLOCK(jboolean, detachCurrentThread, (JNIEnv* env, jobject))\n+      \/\/ Cannot destroy shared library JavaVM as we're about to return to it.\n+      runtime->detach_thread(thread, \"shared library thread detach\", false);\n+      JVMCI_event_1(\"detaching JVMCI shared library thread from HotSpot JavaVM\");\n+      \/\/ Transition back to Native\n+    }\n@@ -2374,1 +2484,1 @@\n-      JNI_THROW(\"detachCurrentThread\", InternalError, \"Cannot detach non-attached thread\");\n+      JNI_THROW_(\"detachCurrentThread\", InternalError, \"Cannot detach non-attached thread\", false);\n@@ -2377,0 +2487,1 @@\n+  return false;\n@@ -2471,1 +2582,1 @@\n-  JVMCIObject global_handle_obj = JVMCIENV->wrap((jobject) obj_handle);\n+  JVMCIObject global_handle_obj = JVMCIENV->wrap(global_handle);\n@@ -2498,1 +2609,1 @@\n-C2V_VMENTRY_NULL(jobject, asReflectionExecutable, (JNIEnv* env, jobject, jobject jvmci_method))\n+C2V_VMENTRY_NULL(jobject, asReflectionExecutable, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n@@ -2500,1 +2611,1 @@\n-  methodHandle m(THREAD, JVMCIENV->asMethod(jvmci_method));\n+  methodHandle m(THREAD, UNPACK_PAIR(Method, method));\n@@ -2514,1 +2625,1 @@\n-C2V_VMENTRY_NULL(jobject, asReflectionField, (JNIEnv* env, jobject, jobject jvmci_type, jint index))\n+C2V_VMENTRY_NULL(jobject, asReflectionField, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), jint index))\n@@ -2516,1 +2627,1 @@\n-  Klass* klass = JVMCIENV->asKlass(jvmci_type);\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n@@ -2564,2 +2675,2 @@\n-C2V_VMENTRY_0(jlong, getFailedSpeculationsAddress, (JNIEnv* env, jobject, jobject jvmci_method))\n-  methodHandle method(THREAD, JVMCIENV->asMethod(jvmci_method));\n+C2V_VMENTRY_0(jlong, getFailedSpeculationsAddress, (JNIEnv* env, jobject, ARGUMENT_PAIR(method)))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n@@ -2620,1 +2731,1 @@\n-C2V_VMENTRY(void, notifyCompilerInliningEvent, (JNIEnv* env, jobject, jint compileId, jobject caller, jobject callee, jboolean succeeded, jstring jmessage, jint bci))\n+C2V_VMENTRY(void, notifyCompilerInliningEvent, (JNIEnv* env, jobject, jint compileId, ARGUMENT_PAIR(caller), ARGUMENT_PAIR(callee), jboolean succeeded, jstring jmessage, jint bci))\n@@ -2623,2 +2734,2 @@\n-    Method* caller_method = JVMCIENV->asMethod(caller);\n-    Method* callee_method = JVMCIENV->asMethod(callee);\n+    Method* caller = UNPACK_PAIR(Method, caller);\n+    Method* callee = UNPACK_PAIR(Method, callee);\n@@ -2626,1 +2737,1 @@\n-    CompilerEvent::InlineEvent::post(event, compileId, caller_method, callee_method, succeeded, JVMCIENV->as_utf8_string(message), bci);\n+    CompilerEvent::InlineEvent::post(event, compileId, caller, callee, succeeded, JVMCIENV->as_utf8_string(message), bci);\n@@ -2680,1 +2791,0 @@\n-#define HANDLECONSTANT          \"Ljdk\/vm\/ci\/hotspot\/IndirectHotSpotObjectConstantImpl;\"\n@@ -2684,1 +2794,0 @@\n-#define TARGET_DESCRIPTION      \"Ljdk\/vm\/ci\/code\/TargetDescription;\"\n@@ -2689,3 +2798,0 @@\n-#define HS_RESOLVED_METHOD      \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedJavaMethodImpl;\"\n-#define HS_RESOLVED_KLASS       \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl;\"\n-#define HS_RESOLVED_FIELD       \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedJavaField;\"\n@@ -2695,1 +2801,0 @@\n-#define HS_CONSTANT_POOL        \"Ljdk\/vm\/ci\/hotspot\/HotSpotConstantPool;\"\n@@ -2698,1 +2803,0 @@\n-#define HS_METADATA             \"Ljdk\/vm\/ci\/hotspot\/HotSpotMetaData;\"\n@@ -2701,1 +2805,0 @@\n-#define METASPACE_OBJECT        \"Ljdk\/vm\/ci\/hotspot\/MetaspaceObject;\"\n@@ -2704,1 +2807,8 @@\n-#define METASPACE_METHOD_DATA   \"J\"\n+\n+\/\/ Types wrapping VM pointers. The ...2 macro is for a pair: (wrapper, pointer)\n+#define HS_METHOD               \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedJavaMethodImpl;\"\n+#define HS_METHOD2              \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedJavaMethodImpl;J\"\n+#define HS_KLASS                \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl;\"\n+#define HS_KLASS2               \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl;J\"\n+#define HS_CONSTANT_POOL        \"Ljdk\/vm\/ci\/hotspot\/HotSpotConstantPool;\"\n+#define HS_CONSTANT_POOL2       \"Ljdk\/vm\/ci\/hotspot\/HotSpotConstantPool;J\"\n@@ -2707,13 +2817,13 @@\n-  {CC \"getBytecode\",                                  CC \"(\" HS_RESOLVED_METHOD \")[B\",                                                      FN_PTR(getBytecode)},\n-  {CC \"getExceptionTableStart\",                       CC \"(\" HS_RESOLVED_METHOD \")J\",                                                       FN_PTR(getExceptionTableStart)},\n-  {CC \"getExceptionTableLength\",                      CC \"(\" HS_RESOLVED_METHOD \")I\",                                                       FN_PTR(getExceptionTableLength)},\n-  {CC \"findUniqueConcreteMethod\",                     CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_METHOD \")\" HS_RESOLVED_METHOD,                   FN_PTR(findUniqueConcreteMethod)},\n-  {CC \"getImplementor\",                               CC \"(\" HS_RESOLVED_KLASS \")\" HS_RESOLVED_KLASS,                                       FN_PTR(getImplementor)},\n-  {CC \"getStackTraceElement\",                         CC \"(\" HS_RESOLVED_METHOD \"I)\" STACK_TRACE_ELEMENT,                                   FN_PTR(getStackTraceElement)},\n-  {CC \"methodIsIgnoredBySecurityStackWalk\",           CC \"(\" HS_RESOLVED_METHOD \")Z\",                                                       FN_PTR(methodIsIgnoredBySecurityStackWalk)},\n-  {CC \"setNotInlinableOrCompilable\",                  CC \"(\" HS_RESOLVED_METHOD \")V\",                                                       FN_PTR(setNotInlinableOrCompilable)},\n-  {CC \"isCompilable\",                                 CC \"(\" HS_RESOLVED_METHOD \")Z\",                                                       FN_PTR(isCompilable)},\n-  {CC \"hasNeverInlineDirective\",                      CC \"(\" HS_RESOLVED_METHOD \")Z\",                                                       FN_PTR(hasNeverInlineDirective)},\n-  {CC \"shouldInlineMethod\",                           CC \"(\" HS_RESOLVED_METHOD \")Z\",                                                       FN_PTR(shouldInlineMethod)},\n-  {CC \"lookupType\",                                   CC \"(\" STRING HS_RESOLVED_KLASS \"Z)\" HS_RESOLVED_TYPE,                                FN_PTR(lookupType)},\n-  {CC \"getArrayType\",                                 CC \"(\" HS_RESOLVED_TYPE \")\" HS_RESOLVED_KLASS,                                        FN_PTR(getArrayType)},\n+  {CC \"getBytecode\",                                  CC \"(\" HS_METHOD2 \")[B\",                                                              FN_PTR(getBytecode)},\n+  {CC \"getExceptionTableStart\",                       CC \"(\" HS_METHOD2 \")J\",                                                               FN_PTR(getExceptionTableStart)},\n+  {CC \"getExceptionTableLength\",                      CC \"(\" HS_METHOD2 \")I\",                                                               FN_PTR(getExceptionTableLength)},\n+  {CC \"findUniqueConcreteMethod\",                     CC \"(\" HS_KLASS2 HS_METHOD2 \")\" HS_METHOD,                                            FN_PTR(findUniqueConcreteMethod)},\n+  {CC \"getImplementor\",                               CC \"(\" HS_KLASS2 \")\" HS_KLASS,                                                        FN_PTR(getImplementor)},\n+  {CC \"getStackTraceElement\",                         CC \"(\" HS_METHOD2 \"I)\" STACK_TRACE_ELEMENT,                                           FN_PTR(getStackTraceElement)},\n+  {CC \"methodIsIgnoredBySecurityStackWalk\",           CC \"(\" HS_METHOD2 \")Z\",                                                               FN_PTR(methodIsIgnoredBySecurityStackWalk)},\n+  {CC \"setNotInlinableOrCompilable\",                  CC \"(\" HS_METHOD2 \")V\",                                                               FN_PTR(setNotInlinableOrCompilable)},\n+  {CC \"isCompilable\",                                 CC \"(\" HS_METHOD2 \")Z\",                                                               FN_PTR(isCompilable)},\n+  {CC \"hasNeverInlineDirective\",                      CC \"(\" HS_METHOD2 \")Z\",                                                               FN_PTR(hasNeverInlineDirective)},\n+  {CC \"shouldInlineMethod\",                           CC \"(\" HS_METHOD2 \")Z\",                                                               FN_PTR(shouldInlineMethod)},\n+  {CC \"lookupType\",                                   CC \"(\" STRING HS_KLASS2 \"Z)\" HS_RESOLVED_TYPE,                                        FN_PTR(lookupType)},\n+  {CC \"getArrayType\",                                 CC \"(C\" HS_KLASS2 \")\" HS_KLASS,                                                       FN_PTR(getArrayType)},\n@@ -2721,15 +2831,16 @@\n-  {CC \"lookupNameInPool\",                             CC \"(\" HS_CONSTANT_POOL \"I)\" STRING,                                                  FN_PTR(lookupNameInPool)},\n-  {CC \"lookupNameAndTypeRefIndexInPool\",              CC \"(\" HS_CONSTANT_POOL \"I)I\",                                                        FN_PTR(lookupNameAndTypeRefIndexInPool)},\n-  {CC \"lookupSignatureInPool\",                        CC \"(\" HS_CONSTANT_POOL \"I)\" STRING,                                                  FN_PTR(lookupSignatureInPool)},\n-  {CC \"lookupKlassRefIndexInPool\",                    CC \"(\" HS_CONSTANT_POOL \"I)I\",                                                        FN_PTR(lookupKlassRefIndexInPool)},\n-  {CC \"lookupKlassInPool\",                            CC \"(\" HS_CONSTANT_POOL \"I)Ljava\/lang\/Object;\",                                       FN_PTR(lookupKlassInPool)},\n-  {CC \"lookupAppendixInPool\",                         CC \"(\" HS_CONSTANT_POOL \"I)\" OBJECTCONSTANT,                                          FN_PTR(lookupAppendixInPool)},\n-  {CC \"lookupMethodInPool\",                           CC \"(\" HS_CONSTANT_POOL \"IB)\" HS_RESOLVED_METHOD,                                     FN_PTR(lookupMethodInPool)},\n-  {CC \"constantPoolRemapInstructionOperandFromCache\", CC \"(\" HS_CONSTANT_POOL \"I)I\",                                                        FN_PTR(constantPoolRemapInstructionOperandFromCache)},\n-  {CC \"resolvePossiblyCachedConstantInPool\",          CC \"(\" HS_CONSTANT_POOL \"I)\" JAVACONSTANT,                                            FN_PTR(resolvePossiblyCachedConstantInPool)},\n-  {CC \"resolveTypeInPool\",                            CC \"(\" HS_CONSTANT_POOL \"I)\" HS_RESOLVED_KLASS,                                       FN_PTR(resolveTypeInPool)},\n-  {CC \"resolveFieldInPool\",                           CC \"(\" HS_CONSTANT_POOL \"I\" HS_RESOLVED_METHOD \"B[I)\" HS_RESOLVED_KLASS,              FN_PTR(resolveFieldInPool)},\n-  {CC \"resolveInvokeDynamicInPool\",                   CC \"(\" HS_CONSTANT_POOL \"I)V\",                                                        FN_PTR(resolveInvokeDynamicInPool)},\n-  {CC \"resolveInvokeHandleInPool\",                    CC \"(\" HS_CONSTANT_POOL \"I)V\",                                                        FN_PTR(resolveInvokeHandleInPool)},\n-  {CC \"isResolvedInvokeHandleInPool\",                 CC \"(\" HS_CONSTANT_POOL \"I)I\",                                                        FN_PTR(isResolvedInvokeHandleInPool)},\n-  {CC \"resolveMethod\",                                CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_METHOD HS_RESOLVED_KLASS \")\" HS_RESOLVED_METHOD, FN_PTR(resolveMethod)},\n+  {CC \"lookupNameInPool\",                             CC \"(\" HS_CONSTANT_POOL2 \"I)\" STRING,                                                 FN_PTR(lookupNameInPool)},\n+  {CC \"lookupNameAndTypeRefIndexInPool\",              CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(lookupNameAndTypeRefIndexInPool)},\n+  {CC \"lookupSignatureInPool\",                        CC \"(\" HS_CONSTANT_POOL2 \"I)\" STRING,                                                 FN_PTR(lookupSignatureInPool)},\n+  {CC \"lookupKlassRefIndexInPool\",                    CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(lookupKlassRefIndexInPool)},\n+  {CC \"lookupKlassInPool\",                            CC \"(\" HS_CONSTANT_POOL2 \"I)Ljava\/lang\/Object;\",                                      FN_PTR(lookupKlassInPool)},\n+  {CC \"lookupAppendixInPool\",                         CC \"(\" HS_CONSTANT_POOL2 \"I)\" OBJECTCONSTANT,                                         FN_PTR(lookupAppendixInPool)},\n+  {CC \"lookupMethodInPool\",                           CC \"(\" HS_CONSTANT_POOL2 \"IB)\" HS_METHOD,                                             FN_PTR(lookupMethodInPool)},\n+  {CC \"constantPoolRemapInstructionOperandFromCache\", CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(constantPoolRemapInstructionOperandFromCache)},\n+  {CC \"resolveBootstrapMethod\",                       CC \"(\" HS_CONSTANT_POOL2 \"I)[\" OBJECT,                                                FN_PTR(resolveBootstrapMethod)},\n+  {CC \"resolvePossiblyCachedConstantInPool\",          CC \"(\" HS_CONSTANT_POOL2 \"I)\" JAVACONSTANT,                                           FN_PTR(resolvePossiblyCachedConstantInPool)},\n+  {CC \"resolveTypeInPool\",                            CC \"(\" HS_CONSTANT_POOL2 \"I)\" HS_KLASS,                                               FN_PTR(resolveTypeInPool)},\n+  {CC \"resolveFieldInPool\",                           CC \"(\" HS_CONSTANT_POOL2 \"I\" HS_METHOD2 \"B[I)\" HS_KLASS,                              FN_PTR(resolveFieldInPool)},\n+  {CC \"resolveInvokeDynamicInPool\",                   CC \"(\" HS_CONSTANT_POOL2 \"I)V\",                                                       FN_PTR(resolveInvokeDynamicInPool)},\n+  {CC \"resolveInvokeHandleInPool\",                    CC \"(\" HS_CONSTANT_POOL2 \"I)V\",                                                       FN_PTR(resolveInvokeHandleInPool)},\n+  {CC \"isResolvedInvokeHandleInPool\",                 CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(isResolvedInvokeHandleInPool)},\n+  {CC \"resolveMethod\",                                CC \"(\" HS_KLASS2 HS_METHOD2 HS_KLASS2 \")\" HS_METHOD,                                  FN_PTR(resolveMethod)},\n@@ -2737,3 +2848,3 @@\n-  {CC \"getVtableIndexForInterfaceMethod\",             CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_METHOD \")I\",                                     FN_PTR(getVtableIndexForInterfaceMethod)},\n-  {CC \"getClassInitializer\",                          CC \"(\" HS_RESOLVED_KLASS \")\" HS_RESOLVED_METHOD,                                      FN_PTR(getClassInitializer)},\n-  {CC \"hasFinalizableSubclass\",                       CC \"(\" HS_RESOLVED_KLASS \")Z\",                                                        FN_PTR(hasFinalizableSubclass)},\n+  {CC \"getVtableIndexForInterfaceMethod\",             CC \"(\" HS_KLASS2 HS_METHOD2 \")I\",                                                     FN_PTR(getVtableIndexForInterfaceMethod)},\n+  {CC \"getClassInitializer\",                          CC \"(\" HS_KLASS2 \")\" HS_METHOD,                                                       FN_PTR(getClassInitializer)},\n+  {CC \"hasFinalizableSubclass\",                       CC \"(\" HS_KLASS2 \")Z\",                                                                FN_PTR(hasFinalizableSubclass)},\n@@ -2741,4 +2852,4 @@\n-  {CC \"asResolvedJavaMethod\",                         CC \"(\" EXECUTABLE \")\" HS_RESOLVED_METHOD,                                             FN_PTR(asResolvedJavaMethod)},\n-  {CC \"getResolvedJavaMethod\",                        CC \"(\" OBJECTCONSTANT \"J)\" HS_RESOLVED_METHOD,                                        FN_PTR(getResolvedJavaMethod)},\n-  {CC \"getConstantPool\",                              CC \"(\" METASPACE_OBJECT \")\" HS_CONSTANT_POOL,                                         FN_PTR(getConstantPool)},\n-  {CC \"getResolvedJavaType0\",                         CC \"(Ljava\/lang\/Object;JZ)\" HS_RESOLVED_KLASS,                                        FN_PTR(getResolvedJavaType0)},\n+  {CC \"asResolvedJavaMethod\",                         CC \"(\" EXECUTABLE \")\" HS_METHOD,                                                      FN_PTR(asResolvedJavaMethod)},\n+  {CC \"getResolvedJavaMethod\",                        CC \"(\" OBJECTCONSTANT \"J)\" HS_METHOD,                                                 FN_PTR(getResolvedJavaMethod)},\n+  {CC \"getConstantPool\",                              CC \"(\" OBJECT \"JZ)\" HS_CONSTANT_POOL,                                                 FN_PTR(getConstantPool)},\n+  {CC \"getResolvedJavaType0\",                         CC \"(Ljava\/lang\/Object;JZ)\" HS_KLASS,                                                 FN_PTR(getResolvedJavaType0)},\n@@ -2746,2 +2857,2 @@\n-  {CC \"installCode\",                                  CC \"(\" TARGET_DESCRIPTION HS_COMPILED_CODE INSTALLED_CODE \"J[B)I\",                    FN_PTR(installCode)},\n-  {CC \"getMetadata\",                                  CC \"(\" TARGET_DESCRIPTION HS_COMPILED_CODE HS_METADATA \")I\",                          FN_PTR(getMetadata)},\n+  {CC \"installCode0\",                                 CC \"(JJZ\" HS_COMPILED_CODE \"[\" OBJECT INSTALLED_CODE \"J[B)I\",                         FN_PTR(installCode0)},\n+  {CC \"getInstallCodeFlags\",                          CC \"()I\",                                                                             FN_PTR(getInstallCodeFlags)},\n@@ -2751,4 +2862,4 @@\n-  {CC \"getLineNumberTable\",                           CC \"(\" HS_RESOLVED_METHOD \")[J\",                                                      FN_PTR(getLineNumberTable)},\n-  {CC \"getLocalVariableTableStart\",                   CC \"(\" HS_RESOLVED_METHOD \")J\",                                                       FN_PTR(getLocalVariableTableStart)},\n-  {CC \"getLocalVariableTableLength\",                  CC \"(\" HS_RESOLVED_METHOD \")I\",                                                       FN_PTR(getLocalVariableTableLength)},\n-  {CC \"reprofile\",                                    CC \"(\" HS_RESOLVED_METHOD \")V\",                                                       FN_PTR(reprofile)},\n+  {CC \"getLineNumberTable\",                           CC \"(\" HS_METHOD2 \")[J\",                                                              FN_PTR(getLineNumberTable)},\n+  {CC \"getLocalVariableTableStart\",                   CC \"(\" HS_METHOD2 \")J\",                                                               FN_PTR(getLocalVariableTableStart)},\n+  {CC \"getLocalVariableTableLength\",                  CC \"(\" HS_METHOD2 \")I\",                                                               FN_PTR(getLocalVariableTableLength)},\n+  {CC \"reprofile\",                                    CC \"(\" HS_METHOD2 \")V\",                                                               FN_PTR(reprofile)},\n@@ -2759,3 +2870,3 @@\n-  {CC \"allocateCompileId\",                            CC \"(\" HS_RESOLVED_METHOD \"I)I\",                                                      FN_PTR(allocateCompileId)},\n-  {CC \"isMature\",                                     CC \"(\" METASPACE_METHOD_DATA \")Z\",                                                    FN_PTR(isMature)},\n-  {CC \"hasCompiledCodeForOSR\",                        CC \"(\" HS_RESOLVED_METHOD \"II)Z\",                                                     FN_PTR(hasCompiledCodeForOSR)},\n+  {CC \"allocateCompileId\",                            CC \"(\" HS_METHOD2 \"I)I\",                                                              FN_PTR(allocateCompileId)},\n+  {CC \"isMature\",                                     CC \"(J)Z\",                                                                            FN_PTR(isMature)},\n+  {CC \"hasCompiledCodeForOSR\",                        CC \"(\" HS_METHOD2 \"II)Z\",                                                             FN_PTR(hasCompiledCodeForOSR)},\n@@ -2763,0 +2874,1 @@\n+  {CC \"getSignatureName\",                             CC \"(J)\" STRING,                                                                      FN_PTR(getSignatureName)},\n@@ -2769,1 +2881,0 @@\n-  {CC \"getFingerprint\",                               CC \"(J)J\",                                                                            FN_PTR(getFingerprint)},\n@@ -2773,4 +2884,4 @@\n-  {CC \"getInterfaces\",                                CC \"(\" HS_RESOLVED_KLASS \")[\" HS_RESOLVED_KLASS,                                      FN_PTR(getInterfaces)},\n-  {CC \"getComponentType\",                             CC \"(\" HS_RESOLVED_KLASS \")\" HS_RESOLVED_TYPE,                                        FN_PTR(getComponentType)},\n-  {CC \"ensureInitialized\",                            CC \"(\" HS_RESOLVED_KLASS \")V\",                                                        FN_PTR(ensureInitialized)},\n-  {CC \"ensureLinked\",                                 CC \"(\" HS_RESOLVED_KLASS \")V\",                                                        FN_PTR(ensureLinked)},\n+  {CC \"getInterfaces\",                                CC \"(\" HS_KLASS2 \")[\" HS_KLASS,                                                       FN_PTR(getInterfaces)},\n+  {CC \"getComponentType\",                             CC \"(\" HS_KLASS2 \")\" HS_RESOLVED_TYPE,                                                FN_PTR(getComponentType)},\n+  {CC \"ensureInitialized\",                            CC \"(\" HS_KLASS2 \")V\",                                                                FN_PTR(ensureInitialized)},\n+  {CC \"ensureLinked\",                                 CC \"(\" HS_KLASS2 \")V\",                                                                FN_PTR(ensureLinked)},\n@@ -2781,7 +2892,7 @@\n-  {CC \"getDeclaredConstructors\",                      CC \"(\" HS_RESOLVED_KLASS \")[\" RESOLVED_METHOD,                                        FN_PTR(getDeclaredConstructors)},\n-  {CC \"getDeclaredMethods\",                           CC \"(\" HS_RESOLVED_KLASS \")[\" RESOLVED_METHOD,                                        FN_PTR(getDeclaredMethods)},\n-  {CC \"readFieldValue\",                               CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_KLASS \"JLjdk\/vm\/ci\/meta\/JavaKind;)\" JAVACONSTANT, FN_PTR(readFieldValue)},\n-  {CC \"readFieldValue\",                               CC \"(\" OBJECTCONSTANT HS_RESOLVED_KLASS \"JLjdk\/vm\/ci\/meta\/JavaKind;)\" JAVACONSTANT,   FN_PTR(readFieldValue)},\n-  {CC \"isInstance\",                                   CC \"(\" HS_RESOLVED_KLASS OBJECTCONSTANT \")Z\",                                         FN_PTR(isInstance)},\n-  {CC \"isAssignableFrom\",                             CC \"(\" HS_RESOLVED_KLASS HS_RESOLVED_KLASS \")Z\",                                      FN_PTR(isAssignableFrom)},\n-  {CC \"isTrustedForIntrinsics\",                       CC \"(\" HS_RESOLVED_KLASS \")Z\",                                                        FN_PTR(isTrustedForIntrinsics)},\n+  {CC \"getDeclaredConstructors\",                      CC \"(\" HS_KLASS2 \")[\" RESOLVED_METHOD,                                                FN_PTR(getDeclaredConstructors)},\n+  {CC \"getDeclaredMethods\",                           CC \"(\" HS_KLASS2 \")[\" RESOLVED_METHOD,                                                FN_PTR(getDeclaredMethods)},\n+  {CC \"readStaticFieldValue\",                         CC \"(\" HS_KLASS2 \"JC)\" JAVACONSTANT,                                                  FN_PTR(readStaticFieldValue)},\n+  {CC \"readFieldValue\",                               CC \"(\" OBJECTCONSTANT HS_KLASS2 \"JC)\" JAVACONSTANT,                                   FN_PTR(readFieldValue)},\n+  {CC \"isInstance\",                                   CC \"(\" HS_KLASS2 OBJECTCONSTANT \")Z\",                                                 FN_PTR(isInstance)},\n+  {CC \"isAssignableFrom\",                             CC \"(\" HS_KLASS2 HS_KLASS2 \")Z\",                                                      FN_PTR(isAssignableFrom)},\n+  {CC \"isTrustedForIntrinsics\",                       CC \"(\" HS_KLASS2 \")Z\",                                                                FN_PTR(isTrustedForIntrinsics)},\n@@ -2791,1 +2902,1 @@\n-  {CC \"getJavaMirror\",                                CC \"(\" HS_RESOLVED_TYPE \")\" OBJECTCONSTANT,                                           FN_PTR(getJavaMirror)},\n+  {CC \"getJavaMirror\",                                CC \"(\" HS_KLASS2 \")\" OBJECTCONSTANT,                                                  FN_PTR(getJavaMirror)},\n@@ -2794,2 +2905,2 @@\n-  {CC \"arrayBaseOffset\",                              CC \"(Ljdk\/vm\/ci\/meta\/JavaKind;)I\",                                                    FN_PTR(arrayBaseOffset)},\n-  {CC \"arrayIndexScale\",                              CC \"(Ljdk\/vm\/ci\/meta\/JavaKind;)I\",                                                    FN_PTR(arrayIndexScale)},\n+  {CC \"arrayBaseOffset\",                              CC \"(C)I\",                                                                            FN_PTR(arrayBaseOffset)},\n+  {CC \"arrayIndexScale\",                              CC \"(C)I\",                                                                            FN_PTR(arrayIndexScale)},\n@@ -2800,2 +2911,2 @@\n-  {CC \"attachCurrentThread\",                          CC \"([BZ)Z\",                                                                          FN_PTR(attachCurrentThread)},\n-  {CC \"detachCurrentThread\",                          CC \"()V\",                                                                             FN_PTR(detachCurrentThread)},\n+  {CC \"attachCurrentThread\",                          CC \"([BZ[J)Z\",                                                                        FN_PTR(attachCurrentThread)},\n+  {CC \"detachCurrentThread\",                          CC \"(Z)Z\",                                                                            FN_PTR(detachCurrentThread)},\n@@ -2806,2 +2917,2 @@\n-  {CC \"asReflectionExecutable\",                       CC \"(\" HS_RESOLVED_METHOD \")\" REFLECTION_EXECUTABLE,                                  FN_PTR(asReflectionExecutable)},\n-  {CC \"asReflectionField\",                            CC \"(\" HS_RESOLVED_KLASS \"I)\" REFLECTION_FIELD,                                       FN_PTR(asReflectionField)},\n+  {CC \"asReflectionExecutable\",                       CC \"(\" HS_METHOD2 \")\" REFLECTION_EXECUTABLE,                                          FN_PTR(asReflectionExecutable)},\n+  {CC \"asReflectionField\",                            CC \"(\" HS_KLASS2 \"I)\" REFLECTION_FIELD,                                               FN_PTR(asReflectionField)},\n@@ -2809,1 +2920,1 @@\n-  {CC \"getFailedSpeculationsAddress\",                 CC \"(\" HS_RESOLVED_METHOD \")J\",                                                       FN_PTR(getFailedSpeculationsAddress)},\n+  {CC \"getFailedSpeculationsAddress\",                 CC \"(\" HS_METHOD2 \")J\",                                                               FN_PTR(getFailedSpeculationsAddress)},\n@@ -2820,1 +2931,1 @@\n-  {CC \"notifyCompilerInliningEvent\",                  CC \"(I\" HS_RESOLVED_METHOD HS_RESOLVED_METHOD \"ZLjava\/lang\/String;I)V\",               FN_PTR(notifyCompilerInliningEvent)},\n+  {CC \"notifyCompilerInliningEvent\",                  CC \"(I\" HS_METHOD2 HS_METHOD2 \"ZLjava\/lang\/String;I)V\",                               FN_PTR(notifyCompilerInliningEvent)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":509,"deletions":398,"binary":false,"changes":907,"status":"modified"},{"patch":"@@ -159,1 +159,1 @@\n-  nonstatic_field(InstanceKlass,               _init_state,                                   u1)                                    \\\n+  nonstatic_field(InstanceKlass,               _init_state,                                   InstanceKlass::ClassState)             \\\n@@ -174,0 +174,1 @@\n+  nonstatic_field(JavaThread,                  _vthread,                                      OopHandle)                             \\\n@@ -193,0 +194,1 @@\n+  nonstatic_field(JavaThread,                  _held_monitor_count,                           int64_t)                               \\\n@@ -338,0 +340,2 @@\n+  static_field(StubRoutines,                _cont_doYield,                                    address)                               \\\n+  static_field(StubRoutines,                _cont_thaw,                                       address)                               \\\n@@ -412,0 +416,1 @@\n+  declare_constant(JVM_ACC_IS_VALUE_BASED_CLASS)                          \\\n@@ -491,0 +496,64 @@\n+  declare_constant(CodeInstaller::ILLEGAL)                                \\\n+  declare_constant(CodeInstaller::REGISTER_PRIMITIVE)                     \\\n+  declare_constant(CodeInstaller::REGISTER_OOP)                           \\\n+  declare_constant(CodeInstaller::REGISTER_NARROW_OOP)                    \\\n+  declare_constant(CodeInstaller::STACK_SLOT_PRIMITIVE)                   \\\n+  declare_constant(CodeInstaller::STACK_SLOT_OOP)                         \\\n+  declare_constant(CodeInstaller::STACK_SLOT_NARROW_OOP)                  \\\n+  declare_constant(CodeInstaller::VIRTUAL_OBJECT_ID)                      \\\n+  declare_constant(CodeInstaller::VIRTUAL_OBJECT_ID2)                     \\\n+  declare_constant(CodeInstaller::NULL_CONSTANT)                          \\\n+  declare_constant(CodeInstaller::RAW_CONSTANT)                           \\\n+  declare_constant(CodeInstaller::PRIMITIVE_0)                            \\\n+  declare_constant(CodeInstaller::PRIMITIVE4)                             \\\n+  declare_constant(CodeInstaller::PRIMITIVE8)                             \\\n+  declare_constant(CodeInstaller::JOBJECT)                                \\\n+  declare_constant(CodeInstaller::OBJECT_ID)                              \\\n+  declare_constant(CodeInstaller::OBJECT_ID2)                             \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::NO_FINALIZABLE_SUBCLASS)                \\\n+  declare_constant(CodeInstaller::CONCRETE_SUBTYPE)                       \\\n+  declare_constant(CodeInstaller::LEAF_TYPE)                              \\\n+  declare_constant(CodeInstaller::CONCRETE_METHOD)                        \\\n+  declare_constant(CodeInstaller::CALLSITE_TARGET_VALUE)                  \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::PATCH_OBJECT_ID)                        \\\n+  declare_constant(CodeInstaller::PATCH_OBJECT_ID2)                       \\\n+  declare_constant(CodeInstaller::PATCH_NARROW_OBJECT_ID)                 \\\n+  declare_constant(CodeInstaller::PATCH_NARROW_OBJECT_ID2)                \\\n+  declare_constant(CodeInstaller::PATCH_JOBJECT)                          \\\n+  declare_constant(CodeInstaller::PATCH_NARROW_JOBJECT)                   \\\n+  declare_constant(CodeInstaller::PATCH_KLASS)                            \\\n+  declare_constant(CodeInstaller::PATCH_NARROW_KLASS)                     \\\n+  declare_constant(CodeInstaller::PATCH_METHOD)                           \\\n+  declare_constant(CodeInstaller::PATCH_DATA_SECTION_REFERENCE)           \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::SITE_CALL)                              \\\n+  declare_constant(CodeInstaller::SITE_FOREIGN_CALL)                      \\\n+  declare_constant(CodeInstaller::SITE_FOREIGN_CALL_NO_DEBUG_INFO)        \\\n+  declare_constant(CodeInstaller::SITE_SAFEPOINT)                         \\\n+  declare_constant(CodeInstaller::SITE_INFOPOINT)                         \\\n+  declare_constant(CodeInstaller::SITE_IMPLICIT_EXCEPTION)                \\\n+  declare_constant(CodeInstaller::SITE_IMPLICIT_EXCEPTION_DISPATCH)       \\\n+  declare_constant(CodeInstaller::SITE_MARK)                              \\\n+  declare_constant(CodeInstaller::SITE_DATA_PATCH)                        \\\n+  declare_constant(CodeInstaller::SITE_EXCEPTION_HANDLER)                 \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::DI_HAS_REFERENCE_MAP)                   \\\n+  declare_constant(CodeInstaller::DI_HAS_CALLEE_SAVE_INFO)                \\\n+  declare_constant(CodeInstaller::DI_HAS_FRAMES)                          \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::DIF_HAS_LOCALS)                         \\\n+  declare_constant(CodeInstaller::DIF_HAS_STACK)                          \\\n+  declare_constant(CodeInstaller::DIF_HAS_LOCKS)                          \\\n+  declare_constant(CodeInstaller::DIF_DURING_CALL)                        \\\n+  declare_constant(CodeInstaller::DIF_RETHROW_EXCEPTION)                  \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::HCC_IS_NMETHOD)                         \\\n+  declare_constant(CodeInstaller::HCC_HAS_ASSUMPTIONS)                    \\\n+  declare_constant(CodeInstaller::HCC_HAS_METHODS)                        \\\n+  declare_constant(CodeInstaller::HCC_HAS_DEOPT_RESCUE_SLOT)              \\\n+  declare_constant(CodeInstaller::HCC_HAS_COMMENTS)                       \\\n+                                                                          \\\n+  declare_constant(CodeInstaller::NO_REGISTER)                            \\\n+                                                                          \\\n@@ -623,0 +692,1 @@\n+  declare_constant(Method::_changes_current_thread)                       \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":71,"deletions":1,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+  LOG_TAG(continuations) \\\n@@ -69,0 +70,1 @@\n+  LOG_TAG(deoptimization) \\\n@@ -70,0 +72,1 @@\n+  NOT_PRODUCT(LOG_TAG(downcall)) \\\n@@ -78,1 +81,1 @@\n-  DEBUG_ONLY(LOG_TAG(foreign)) \\\n+  NOT_PRODUCT(LOG_TAG(foreign)) \\\n@@ -145,0 +148,1 @@\n+  LOG_TAG(preempt) \\\n@@ -194,0 +198,1 @@\n+  NOT_PRODUCT(LOG_TAG(upcall)) \\\n","filename":"src\/hotspot\/share\/logging\/logTag.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -80,1 +80,1 @@\n-\/\/ WARNING: The array variant must only be used for a homogenous array\n+\/\/ WARNING: The array variant must only be used for a homogeneous array\n@@ -553,1 +553,1 @@\n-\/\/ Uses mmaped memory for all allocations. All allocations are initially\n+\/\/ Uses mmapped memory for all allocations. All allocations are initially\n","filename":"src\/hotspot\/share\/memory\/allocation.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -442,1 +442,1 @@\n-\/\/ Print the class name and its unique ClassLoader identifer.\n+\/\/ Print the class name and its unique ClassLoader identifier.\n@@ -474,1 +474,1 @@\n-  \/\/ Print the class name, its unique ClassLoader identifer, and if it is an interface.\n+  \/\/ Print the class name, its unique ClassLoader identifier, and if it is an interface.\n","filename":"src\/hotspot\/share\/memory\/heapInspection.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -103,0 +103,2 @@\n+  \/\/\n+  \/\/ Used to determine metadata liveness for class unloading GCs.\n@@ -107,0 +109,5 @@\n+\n+  \/\/ Class redefinition needs to get notified about methods from stackChunkOops\n+  virtual void do_method(Method* m) = 0;\n+  \/\/ The code cache sweeper needs to get notified about methods from stackChunkOops\n+  virtual void do_nmethod(nmethod* nm) = 0;\n@@ -117,0 +124,9 @@\n+  virtual void do_method(Method* m) { ShouldNotReachHere(); }\n+  virtual void do_nmethod(nmethod* nm) { ShouldNotReachHere(); }\n+};\n+\n+enum class derived_pointer : intptr_t;\n+class DerivedOopClosure : public Closure {\n+ public:\n+  enum { SkipNull = true };\n+  virtual void do_derived_oop(oop* base, derived_pointer* derived) = 0;\n@@ -171,0 +187,2 @@\n+  virtual void do_method(Method* m);\n+  virtual void do_nmethod(nmethod* nm);\n@@ -244,0 +262,1 @@\n+ protected:\n@@ -246,1 +265,0 @@\n- protected:\n@@ -259,0 +277,2 @@\n+  bool _keepalive_nmethods;\n+\n@@ -260,2 +280,3 @@\n-  MarkingCodeBlobClosure(OopClosure* cl, bool fix_relocations) : CodeBlobToOopClosure(cl, fix_relocations) {}\n-  \/\/ Called for each code blob, but at most once per unique blob.\n+  MarkingCodeBlobClosure(OopClosure* cl, bool fix_relocations, bool keepalive_nmethods) :\n+      CodeBlobToOopClosure(cl, fix_relocations),\n+      _keepalive_nmethods(keepalive_nmethods) {}\n@@ -263,0 +284,1 @@\n+  \/\/ Called for each code blob, but at most once per unique blob.\n@@ -299,1 +321,1 @@\n-\/\/ of an interruptable task so as to allow other\n+\/\/ of an interruptible task so as to allow other\n@@ -357,10 +379,0 @@\n-\/\/ Dispatches to the non-virtual functions if OopClosureType has\n-\/\/ a concrete implementation, otherwise a virtual call is taken.\n-class Devirtualizer {\n- public:\n-  template <typename OopClosureType, typename T> static void do_oop(OopClosureType* closure, T* p);\n-  template <typename OopClosureType>             static void do_klass(OopClosureType* closure, Klass* k);\n-  template <typename OopClosureType>             static void do_cld(OopClosureType* closure, ClassLoaderData* cld);\n-  template <typename OopClosureType>             static bool do_metadata(OopClosureType* closure);\n-};\n-\n","filename":"src\/hotspot\/share\/memory\/iterator.hpp","additions":27,"deletions":15,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -31,0 +31,1 @@\n+#include \"code\/nmethod.hpp\"\n@@ -39,0 +40,1 @@\n+#include \"oops\/instanceStackChunkKlass.inline.hpp\"\n@@ -56,72 +58,2 @@\n-\/\/ Implementation of the non-virtual do_oop dispatch.\n-\/\/\n-\/\/ The same implementation is used for do_metadata, do_klass, and do_cld.\n-\/\/\n-\/\/ Preconditions:\n-\/\/  - Base has a pure virtual do_oop\n-\/\/  - Only one of the classes in the inheritance chain from OopClosureType to\n-\/\/    Base implements do_oop.\n-\/\/\n-\/\/ Given the preconditions:\n-\/\/  - If &OopClosureType::do_oop is resolved to &Base::do_oop, then there is no\n-\/\/    implementation of do_oop between Base and OopClosureType. However, there\n-\/\/    must be one implementation in one of the subclasses of OopClosureType.\n-\/\/    In this case we take the virtual call.\n-\/\/\n-\/\/  - Conversely, if &OopClosureType::do_oop is not resolved to &Base::do_oop,\n-\/\/    then we've found the one and only concrete implementation. In this case we\n-\/\/    take a non-virtual call.\n-\/\/\n-\/\/ Because of this it's clear when we should call the virtual call and\n-\/\/   when the non-virtual call should be made.\n-\/\/\n-\/\/ The way we find if &OopClosureType::do_oop is resolved to &Base::do_oop is to\n-\/\/   check if the resulting type of the class of a member-function pointer to\n-\/\/   &OopClosureType::do_oop is equal to the type of the class of a\n-\/\/   &Base::do_oop member-function pointer. Template parameter deduction is used\n-\/\/   to find these types, and then the IsSame trait is used to check if they are\n-\/\/   equal. Finally, SFINAE is used to select the appropriate implementation.\n-\/\/\n-\/\/ Template parameters:\n-\/\/   T              - narrowOop or oop\n-\/\/   Receiver       - the resolved type of the class of the\n-\/\/                    &OopClosureType::do_oop member-function pointer. That is,\n-\/\/                    the klass with the do_oop member function.\n-\/\/   Base           - klass with the pure virtual do_oop member function.\n-\/\/   OopClosureType - The dynamic closure type\n-\/\/\n-\/\/ Parameters:\n-\/\/   closure - The closure to call\n-\/\/   p       - The oop (or narrowOop) field to pass to the closure\n-\n-template <typename T, typename Receiver, typename Base, typename OopClosureType>\n-static typename EnableIf<IsSame<Receiver, Base>::value, void>::type\n-call_do_oop(void (Receiver::*)(T*), void (Base::*)(T*), OopClosureType* closure, T* p) {\n-  closure->do_oop(p);\n-}\n-\n-template <typename T, typename Receiver, typename Base, typename OopClosureType>\n-static typename EnableIf<!IsSame<Receiver, Base>::value, void>::type\n-call_do_oop(void (Receiver::*)(T*), void (Base::*)(T*), OopClosureType* closure, T* p) {\n-  \/\/ Sanity check\n-  STATIC_ASSERT((!IsSame<OopClosureType, OopIterateClosure>::value));\n-  closure->OopClosureType::do_oop(p);\n-}\n-\n-template <typename OopClosureType, typename T>\n-inline void Devirtualizer::do_oop(OopClosureType* closure, T* p) {\n-  call_do_oop<T>(&OopClosureType::do_oop, &OopClosure::do_oop, closure, p);\n-}\n-\n-\/\/ Implementation of the non-virtual do_metadata dispatch.\n-\n-template <typename Receiver, typename Base, typename OopClosureType>\n-static typename EnableIf<IsSame<Receiver, Base>::value, bool>::type\n-call_do_metadata(bool (Receiver::*)(), bool (Base::*)(), OopClosureType* closure) {\n-  return closure->do_metadata();\n-}\n-\n-template <typename Receiver, typename Base, typename OopClosureType>\n-static typename EnableIf<!IsSame<Receiver, Base>::value, bool>::type\n-call_do_metadata(bool (Receiver::*)(), bool (Base::*)(), OopClosureType* closure) {\n-  return closure->OopClosureType::do_metadata();\n+inline void ClaimMetadataVisitingOopIterateClosure::do_nmethod(nmethod* nm) {\n+  nm->follow_nmethod(this);\n@@ -130,41 +62,3 @@\n-template <typename OopClosureType>\n-inline bool Devirtualizer::do_metadata(OopClosureType* closure) {\n-  return call_do_metadata(&OopClosureType::do_metadata, &OopIterateClosure::do_metadata, closure);\n-}\n-\n-\/\/ Implementation of the non-virtual do_klass dispatch.\n-\n-template <typename Receiver, typename Base, typename OopClosureType>\n-static typename EnableIf<IsSame<Receiver, Base>::value, void>::type\n-call_do_klass(void (Receiver::*)(Klass*), void (Base::*)(Klass*), OopClosureType* closure, Klass* k) {\n-  closure->do_klass(k);\n-}\n-\n-template <typename Receiver, typename Base, typename OopClosureType>\n-static typename EnableIf<!IsSame<Receiver, Base>::value, void>::type\n-call_do_klass(void (Receiver::*)(Klass*), void (Base::*)(Klass*), OopClosureType* closure, Klass* k) {\n-  closure->OopClosureType::do_klass(k);\n-}\n-\n-template <typename OopClosureType>\n-inline void Devirtualizer::do_klass(OopClosureType* closure, Klass* k) {\n-  call_do_klass(&OopClosureType::do_klass, &OopIterateClosure::do_klass, closure, k);\n-}\n-\n-\/\/ Implementation of the non-virtual do_cld dispatch.\n-\n-template <typename Receiver, typename Base, typename OopClosureType>\n-static typename EnableIf<IsSame<Receiver, Base>::value, void>::type\n-call_do_cld(void (Receiver::*)(ClassLoaderData*), void (Base::*)(ClassLoaderData*), OopClosureType* closure, ClassLoaderData* cld) {\n-  closure->do_cld(cld);\n-}\n-\n-template <typename Receiver, typename Base, typename OopClosureType>\n-static typename EnableIf<!IsSame<Receiver, Base>::value, void>::type\n-call_do_cld(void (Receiver::*)(ClassLoaderData*), void (Base::*)(ClassLoaderData*), OopClosureType* closure, ClassLoaderData* cld) {\n-  closure->OopClosureType::do_cld(cld);\n-}\n-\n-template <typename OopClosureType>\n-void Devirtualizer::do_cld(OopClosureType* closure, ClassLoaderData* cld) {\n-  call_do_cld(&OopClosureType::do_cld, &OopIterateClosure::do_cld, closure, cld);\n+inline void ClaimMetadataVisitingOopIterateClosure::do_method(Method* m) {\n+  \/\/ Mark interpreted frames for class redefinition\n+  m->record_gc_epoch();\n@@ -178,1 +72,1 @@\n-\/\/   - Klass*            : dynamic to static type through Klass::id() -> table index\n+\/\/   - Klass*            : dynamic to static type through Klass::kind() -> table index\n@@ -194,1 +88,1 @@\n-\/\/   A table mapping from *Klass::ID to function is setup. This happens once\n+\/\/   A table mapping from *Klass::Kind to function is setup. This happens once\n@@ -227,1 +121,1 @@\n-      _function[KlassType::ID] = &init<KlassType>;\n+      _function[KlassType::Kind] = &init<KlassType>;\n@@ -236,1 +130,1 @@\n-        _function[KlassType::ID] = &oop_oop_iterate<KlassType, narrowOop>;\n+        _function[KlassType::Kind] = &oop_oop_iterate<KlassType, narrowOop>;\n@@ -238,1 +132,1 @@\n-        _function[KlassType::ID] = &oop_oop_iterate<KlassType, oop>;\n+        _function[KlassType::Kind] = &oop_oop_iterate<KlassType, oop>;\n@@ -245,1 +139,1 @@\n-      _function[KlassType::ID](cl, obj, k);\n+      _function[KlassType::Kind](cl, obj, k);\n@@ -249,1 +143,1 @@\n-    FunctionType _function[KLASS_ID_COUNT];\n+    FunctionType _function[KLASS_KIND_COUNT];\n@@ -253,0 +147,1 @@\n+      set_init_function<InlineKlass>();\n@@ -256,0 +151,1 @@\n+      set_init_function<InstanceStackChunkKlass>();\n@@ -266,1 +162,1 @@\n-    return _table._function[klass->id()];\n+    return _table._function[klass->kind()];\n@@ -293,1 +189,1 @@\n-      _function[KlassType::ID] = &init<KlassType>;\n+      _function[KlassType::Kind] = &init<KlassType>;\n@@ -299,1 +195,1 @@\n-        _function[KlassType::ID] = &oop_oop_iterate_bounded<KlassType, narrowOop>;\n+        _function[KlassType::Kind] = &oop_oop_iterate_bounded<KlassType, narrowOop>;\n@@ -301,1 +197,1 @@\n-        _function[KlassType::ID] = &oop_oop_iterate_bounded<KlassType, oop>;\n+        _function[KlassType::Kind] = &oop_oop_iterate_bounded<KlassType, oop>;\n@@ -308,1 +204,1 @@\n-      _function[KlassType::ID](cl, obj, k, mr);\n+      _function[KlassType::Kind](cl, obj, k, mr);\n@@ -312,1 +208,1 @@\n-    FunctionType _function[KLASS_ID_COUNT];\n+    FunctionType _function[KLASS_KIND_COUNT];\n@@ -316,0 +212,1 @@\n+      set_init_function<InlineKlass>();\n@@ -319,0 +216,1 @@\n+      set_init_function<InstanceStackChunkKlass>();\n@@ -329,1 +227,1 @@\n-    return _table._function[klass->id()];\n+    return _table._function[klass->kind()];\n@@ -356,1 +254,1 @@\n-      _function[KlassType::ID] = &init<KlassType>;\n+      _function[KlassType::Kind] = &init<KlassType>;\n@@ -362,1 +260,1 @@\n-        _function[KlassType::ID] = &oop_oop_iterate_backwards<KlassType, narrowOop>;\n+        _function[KlassType::Kind] = &oop_oop_iterate_backwards<KlassType, narrowOop>;\n@@ -364,1 +262,1 @@\n-        _function[KlassType::ID] = &oop_oop_iterate_backwards<KlassType, oop>;\n+        _function[KlassType::Kind] = &oop_oop_iterate_backwards<KlassType, oop>;\n@@ -371,1 +269,1 @@\n-      _function[KlassType::ID](cl, obj, k);\n+      _function[KlassType::Kind](cl, obj, k);\n@@ -375,1 +273,1 @@\n-    FunctionType _function[KLASS_ID_COUNT];\n+    FunctionType _function[KLASS_KIND_COUNT];\n@@ -379,0 +277,1 @@\n+      set_init_function<InlineKlass>();\n@@ -382,0 +281,1 @@\n+      set_init_function<InstanceStackChunkKlass>();\n@@ -392,1 +292,1 @@\n-    return _table._function[klass->id()];\n+    return _table._function[klass->kind()];\n","filename":"src\/hotspot\/share\/memory\/iterator.inline.hpp","additions":34,"deletions":134,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"gc\/shared\/plab.hpp\"\n@@ -72,0 +73,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -73,1 +75,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -87,0 +89,1 @@\n+Klass* Universe::_fillerArrayKlassObj                 = NULL;\n@@ -205,0 +208,5 @@\n+  \/\/ We don't do the following because it will confuse JVMTI.\n+  \/\/ _fillerArrayKlassObj is used only by GC, which doesn't need to see\n+  \/\/ this klass from basic_type_classes_do().\n+  \/\/\n+  \/\/ closure->do_klass(_fillerArrayKlassObj);\n@@ -212,0 +220,1 @@\n+  it->push(&_fillerArrayKlassObj);\n@@ -262,0 +271,1 @@\n+  f->do_ptr((void**)&_fillerArrayKlassObj);\n@@ -323,0 +333,4 @@\n+        \/\/ Initialization of the fillerArrayKlass must come before regular\n+        \/\/ int-TypeArrayKlass so that the int-Array mirror points to the\n+        \/\/ int-TypeArrayKlass.\n+        _fillerArrayKlassObj = TypeArrayKlass::create_klass(T_INT, \"Ljava\/internal\/vm\/FillerArray;\", CHECK);\n@@ -365,0 +379,2 @@\n+    initialize_basic_type_klass(_fillerArrayKlassObj, CHECK);\n+\n@@ -373,0 +389,3 @@\n+\n+    assert(_fillerArrayKlassObj != intArrayKlassObj(),\n+           \"Internal filler array klass should be different to int array Klass\");\n@@ -833,0 +852,1 @@\n+  PLAB::startup_initialization();\n@@ -1243,1 +1263,1 @@\n-    \/\/ sharing initilization should have already set up _klass\n+    \/\/ sharing initialization should have already set up _klass\n","filename":"src\/hotspot\/share\/memory\/universe.cpp","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -98,0 +98,3 @@\n+  \/\/ Special int-Array that represents filler objects that are used by GC to overwrite\n+  \/\/ dead objects. References to them are generally an error.\n+  static Klass* _fillerArrayKlassObj;\n@@ -214,0 +217,2 @@\n+  static Klass* fillerArrayKlassObj()               { return _fillerArrayKlassObj; }\n+\n@@ -363,1 +368,1 @@\n-    verify(VerifyOption_Default, prefix);\n+    verify(VerifyOption::Default, prefix);\n","filename":"src\/hotspot\/share\/memory\/universe.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -406,0 +406,5 @@\n+namespace AccessInternal {\n+  DEBUG_ONLY(void check_access_thread_state());\n+#define assert_access_thread_state() DEBUG_ONLY(check_access_thread_state())\n+}\n+\n@@ -467,0 +472,1 @@\n+      assert_access_thread_state();\n@@ -479,0 +485,1 @@\n+      assert_access_thread_state();\n@@ -491,0 +498,1 @@\n+      assert_access_thread_state();\n@@ -503,0 +511,1 @@\n+      assert_access_thread_state();\n@@ -515,0 +524,1 @@\n+      assert_access_thread_state();\n@@ -527,0 +537,1 @@\n+      assert_access_thread_state();\n@@ -539,0 +550,1 @@\n+      assert_access_thread_state();\n@@ -551,0 +563,1 @@\n+      assert_access_thread_state();\n@@ -567,0 +580,1 @@\n+      assert_access_thread_state();\n@@ -581,0 +595,1 @@\n+      assert_access_thread_state();\n","filename":"src\/hotspot\/share\/oops\/accessBackend.hpp","additions":16,"deletions":1,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -134,1 +134,1 @@\n-\/\/   in the pipeline and hardwire to raw accesses without going trough the GC access barriers.\n+\/\/   in the pipeline and hardwire to raw accesses without going through the GC access barriers.\n","filename":"src\/hotspot\/share\/oops\/accessDecorators.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -88,2 +88,2 @@\n-ArrayKlass::ArrayKlass(Symbol* name, KlassID id) :\n-  Klass(id),\n+ArrayKlass::ArrayKlass(Symbol* name, KlassKind kind) :\n+  Klass(kind),\n@@ -127,1 +127,1 @@\n-\/\/ Initialization of vtables and mirror object is done separatly from base_create_array_klass,\n+\/\/ Initialization of vtables and mirror object is done separately from base_create_array_klass,\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -55,1 +55,1 @@\n-  ArrayKlass(Symbol* name, KlassID id);\n+  ArrayKlass(Symbol* name, KlassKind kind);\n","filename":"src\/hotspot\/share\/oops\/arrayKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+#include \"code\/codeCache.hpp\"\n@@ -60,0 +61,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -61,1 +63,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -484,1 +485,1 @@\n-  \/\/ the entry and tag is not updated atomicly.\n+  \/\/ the entry and tag is not updated atomically.\n@@ -2261,1 +2262,1 @@\n-  \/\/ Keep temorarily for debugging until it's stable.\n+  \/\/ Keep temporarily for debugging until it's stable.\n@@ -2268,0 +2269,25 @@\n+bool ConstantPool::is_maybe_on_continuation_stack() const {\n+  \/\/ This method uses the similar logic as nmethod::is_maybe_on_continuation_stack()\n+  if (!Continuations::enabled()) {\n+    return false;\n+  }\n+\n+  \/\/ If the condition below is true, it means that the nmethod was found to\n+  \/\/ be alive the previous completed marking cycle.\n+  return cache()->gc_epoch() >= Continuations::previous_completed_gc_marking_cycle();\n+}\n+\n+\/\/ For redefinition, if any methods found in loom stack chunks, the gc_epoch is\n+\/\/ recorded in their constant pool cache. The on_stack-ness of the constant pool controls whether\n+\/\/ memory for the method is reclaimed.\n+bool ConstantPool::on_stack() const {\n+  if ((_flags &_on_stack) != 0) {\n+    return true;\n+  }\n+\n+  if (_cache == nullptr) {\n+    return false;\n+  }\n+\n+  return is_maybe_on_continuation_stack();\n+}\n","filename":"src\/hotspot\/share\/oops\/constantPool.cpp","additions":30,"deletions":4,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,1 +36,1 @@\n-#include \"runtime\/thread.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -204,1 +204,1 @@\n-  \/\/ Redefine classes support.  If a method refering to this constant pool\n+  \/\/ Redefine classes support.  If a method referring to this constant pool\n@@ -208,1 +208,2 @@\n-  bool on_stack() const                      { return (_flags &_on_stack) != 0; }\n+  bool on_stack() const;\n+  bool is_maybe_on_continuation_stack() const;\n@@ -897,1 +898,1 @@\n-  \/\/ JVMTI accesss - GetConstantPool, RetransformClasses, ...\n+  \/\/ JVMTI access - GetConstantPool, RetransformClasses, ...\n","filename":"src\/hotspot\/share\/oops\/constantPool.hpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -49,0 +49,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -50,0 +51,1 @@\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -273,1 +275,1 @@\n-             method->method_holder()->is_reentrant_initialization(Thread::current()),\n+             method->method_holder()->is_init_thread(Thread::current()),\n@@ -381,0 +383,2 @@\n+  \/\/ Lock fields to write\n+  MutexLocker ml(cpool->pool_holder()->init_monitor());\n@@ -382,8 +386,0 @@\n-  JavaThread* current = JavaThread::current();\n-  objArrayHandle resolved_references(current, cpool->resolved_references());\n-  \/\/ Use the resolved_references() lock for this cpCache entry.\n-  \/\/ resolved_references are created for all classes with Invokedynamic, MethodHandle\n-  \/\/ or MethodType constant pool cache entries.\n-  assert(resolved_references() != NULL,\n-         \"a resolved_references array should have been created for this class\");\n-  ObjectLocker ol(resolved_references, current);\n@@ -461,0 +457,1 @@\n+    objArrayOop resolved_references = cpool->resolved_references();\n@@ -488,8 +485,1 @@\n-  \/\/ Use the resolved_references() lock for this cpCache entry.\n-  \/\/ resolved_references are created for all classes with Invokedynamic, MethodHandle\n-  \/\/ or MethodType constant pool cache entries.\n-  JavaThread* current = THREAD;\n-  objArrayHandle resolved_references(current, cpool->resolved_references());\n-  assert(resolved_references() != NULL,\n-         \"a resolved_references array should have been created for this class\");\n-  ObjectLocker ol(resolved_references, current);\n+  MutexLocker ml(THREAD, cpool->pool_holder()->init_monitor());\n@@ -714,0 +704,5 @@\n+\/\/ Record the GC marking cycle when redefined vs. when found in the loom stack chunks.\n+void ConstantPoolCache::record_gc_epoch() {\n+  _gc_epoch = Continuations::gc_epoch();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/cpCache.cpp","additions":13,"deletions":18,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -425,0 +425,4 @@\n+\n+  \/\/ RedefineClasses support\n+  uint64_t             _gc_epoch;\n+\n@@ -518,0 +522,2 @@\n+  void record_gc_epoch();\n+  uint64_t gc_epoch() { return _gc_epoch; }\n","filename":"src\/hotspot\/share\/oops\/cpCache.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -168,1 +168,1 @@\n-    assert((_shorts[low_packed_offset] & FIELDINFO_TAG_CONTENDED) == 0, \"Overwritting contended group\");\n+    assert((_shorts[low_packed_offset] & FIELDINFO_TAG_CONTENDED) == 0, \"Overwriting contended group\");\n","filename":"src\/hotspot\/share\/oops\/fieldInfo.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-FlatArrayKlass::FlatArrayKlass(Klass* element_klass, Symbol* name) : ArrayKlass(name, ID) {\n+FlatArrayKlass::FlatArrayKlass(Klass* element_klass, Symbol* name) : ArrayKlass(name, Kind) {\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  static const KlassID ID = FlatArrayKlassID;\n+  static const KlassKind Kind = FlatArrayKlassKind;\n","filename":"src\/hotspot\/share\/oops\/flatArrayKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -976,1 +976,1 @@\n-    \/\/ Remember prevous bci.\n+    \/\/ Remember previous bci.\n@@ -1160,1 +1160,1 @@\n-      \/\/ Automatically handles 'wide' ret indicies\n+      \/\/ Automatically handles 'wide' ret indices\n@@ -1762,1 +1762,1 @@\n-\/\/ Replace all occurences of the state 'match' with the state 'replace'\n+\/\/ Replace all occurrences of the state 'match' with the state 'replace'\n@@ -2383,1 +2383,1 @@\n-        tty->print_cr(\"Supress rewriting of astore at bci: %d\", bci);\n+        tty->print_cr(\"Suppress rewriting of astore at bci: %d\", bci);\n@@ -2414,1 +2414,1 @@\n-  \/\/ This is neccesary, since relocating the instruction at a certain bci, might\n+  \/\/ This is necessary, since relocating the instruction at a certain bci, might\n@@ -2466,1 +2466,1 @@\n-\/\/ Returns true if expanding was succesful. Otherwise, reports an error and\n+\/\/ Returns true if expanding was successful. Otherwise, reports an error and\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,1 @@\n-\/\/ These two should be removed. But requires som code to be cleaned up\n+\/\/ These two should be removed. But requires some code to be cleaned up\n@@ -50,1 +50,1 @@\n-\/\/ Contains maping between jsr targets and there return addresses. One-to-many mapping\n+\/\/ Contains mapping between jsr targets and there return addresses. One-to-many mapping\n@@ -312,1 +312,1 @@\n-  bool         _did_relocation;             \/\/ was relocation neccessary\n+  bool         _did_relocation;             \/\/ was relocation necessary\n@@ -423,1 +423,1 @@\n-  bool  _report_result_for_send;            \/\/ Unfortunatly, stackmaps for sends are special, so we need some extra\n+  bool  _report_result_for_send;            \/\/ Unfortunately, stackmaps for sends are special, so we need some extra\n@@ -459,1 +459,1 @@\n-  \/\/ Helper method. Can be used in subclasses to fx. calculate gc_points. If the current instuction\n+  \/\/ Helper method. Can be used in subclasses to fx. calculate gc_points. If the current instruction\n@@ -494,1 +494,1 @@\n-  \/\/   stackmaps for deadcode, fewer gc_points might have been encounted than assumed during the epilog. It is the\n+  \/\/   stackmaps for deadcode, fewer gc_points might have been encountered than assumed during the epilog. It is the\n@@ -552,1 +552,1 @@\n-\/\/ Subclass used by the compiler to generate pairing infomation\n+\/\/ Subclass used by the compiler to generate pairing information\n","filename":"src\/hotspot\/share\/oops\/generateOopMap.hpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-    : InstanceKlass(parser, InstanceKlass::_kind_inline_type, InstanceKlass::ID) {\n+    : InstanceKlass(parser, InlineKlass::Kind) {\n@@ -65,1 +65,1 @@\n-  assert(is_inline_type_klass(), \"sanity\");\n+  assert(is_inline_klass(), \"sanity\");\n@@ -353,1 +353,1 @@\n-      address loc = reg_map.location(pair.first());\n+      address loc = reg_map.location(pair.first(), nullptr);\n@@ -384,1 +384,1 @@\n-      address loc = reg_map.location(pair.first());\n+      address loc = reg_map.location(pair.first(), nullptr);\n@@ -424,1 +424,1 @@\n-    address loc = reg_map.location(pair.first());\n+    address loc = reg_map.location(pair.first(), nullptr);\n@@ -486,1 +486,1 @@\n-  address loc = map.location(pair.first());\n+  address loc = map.location(pair.first(), nullptr);\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -42,0 +42,2 @@\n+  static const KlassKind Kind = InlineKlassKind;\n+\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"utilities\/devirtualizer.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/inlineKlass.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -37,1 +37,0 @@\n-#include \"classfile\/resolutionErrors.hpp\"\n@@ -68,0 +67,1 @@\n+#include \"oops\/instanceStackChunkKlass.hpp\"\n@@ -83,0 +83,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -86,1 +87,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -169,0 +170,6 @@\n+static inline bool is_stack_chunk_class(const Symbol* class_name,\n+                                        const ClassLoaderData* loader_data) {\n+  return (class_name == vmSymbols::jdk_internal_vm_StackChunk() &&\n+          loader_data->is_the_null_class_loader_data());\n+}\n+\n@@ -446,16 +453,2 @@\n-  if (REF_NONE == parser.reference_type()) {\n-    if (class_name == vmSymbols::java_lang_Class()) {\n-      \/\/ mirror\n-      ik = new (loader_data, size, THREAD) InstanceMirrorKlass(parser);\n-    } else if (is_class_loader(class_name, parser)) {\n-      \/\/ class loader\n-      ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(parser);\n-    } else if (parser.is_inline_type()) {\n-      \/\/ inline type\n-      ik = new (loader_data, size, THREAD) InlineKlass(parser);\n-    } else {\n-      \/\/ normal\n-      ik = new (loader_data, size, THREAD) InstanceKlass(parser, InstanceKlass::_kind_other);\n-    }\n-  } else {\n-    \/\/ reference\n+  if (parser.is_instance_ref_klass()) {\n+    \/\/ java.lang.ref.Reference\n@@ -463,0 +456,15 @@\n+  } else if (class_name == vmSymbols::java_lang_Class()) {\n+    \/\/ mirror - java.lang.Class\n+    ik = new (loader_data, size, THREAD) InstanceMirrorKlass(parser);\n+  } else if (is_stack_chunk_class(class_name, loader_data)) {\n+    \/\/ stack chunk\n+    ik = new (loader_data, size, THREAD) InstanceStackChunkKlass(parser);\n+  } else if (is_class_loader(class_name, parser)) {\n+    \/\/ class loader - java.lang.ClassLoader\n+    ik = new (loader_data, size, THREAD) InstanceClassLoaderKlass(parser);\n+  } else if (parser.is_inline_type()) {\n+    \/\/ inline type\n+    ik = new (loader_data, size, THREAD) InlineKlass(parser);\n+  } else {\n+    \/\/ normal\n+    ik = new (loader_data, size, THREAD) InstanceKlass(parser);\n@@ -526,2 +534,6 @@\n-InstanceKlass::InstanceKlass(const ClassFileParser& parser, unsigned kind, KlassID id) :\n-  Klass(id),\n+static Monitor* create_init_monitor(const char* name) {\n+  return new Monitor(Mutex::safepoint, name);\n+}\n+\n+InstanceKlass::InstanceKlass(const ClassFileParser& parser, KlassKind kind, ReferenceType reference_type) :\n+  Klass(kind),\n@@ -537,1 +549,2 @@\n-  _reference_type(parser.reference_type()),\n+  _reference_type(reference_type),\n+  _init_monitor(create_init_monitor(\"InstanceKlassInitMonitor_lock\")),\n@@ -544,1 +557,0 @@\n-  set_kind(kind);\n@@ -760,19 +772,0 @@\n-void InstanceKlass::eager_initialize(Thread *thread) {\n-  if (!EagerInitialization) return;\n-\n-  if (this->is_not_initialized()) {\n-    \/\/ abort if the the class has a class initializer\n-    if (this->class_initializer() != NULL) return;\n-\n-    \/\/ abort if it is java.lang.Object (initialization is handled in genesis)\n-    Klass* super_klass = super();\n-    if (super_klass == NULL) return;\n-\n-    \/\/ abort if the super class should be initialized\n-    if (!InstanceKlass::cast(super_klass)->is_initialized()) return;\n-\n-    \/\/ call body to expose the this pointer\n-    eager_initialize_impl();\n-  }\n-}\n-\n@@ -792,54 +785,0 @@\n-oop InstanceKlass::init_lock() const {\n-  \/\/ return the init lock from the mirror\n-  oop lock = java_lang_Class::init_lock(java_mirror());\n-  \/\/ Prevent reordering with any access of initialization state\n-  OrderAccess::loadload();\n-  assert(lock != NULL || !is_not_initialized(), \/\/ initialized or in_error state\n-         \"only fully initialized state can have a null lock\");\n-  return lock;\n-}\n-\n-\/\/ Set the initialization lock to null so the object can be GC'ed.  Any racing\n-\/\/ threads to get this lock will see a null lock and will not lock.\n-\/\/ That's okay because they all check for initialized state after getting\n-\/\/ the lock and return.\n-void InstanceKlass::fence_and_clear_init_lock() {\n-  \/\/ make sure previous stores are all done, notably the init_state.\n-  OrderAccess::storestore();\n-  java_lang_Class::clear_init_lock(java_mirror());\n-  assert(!is_not_initialized(), \"class must be initialized now\");\n-}\n-\n-void InstanceKlass::eager_initialize_impl() {\n-  EXCEPTION_MARK;\n-  HandleMark hm(THREAD);\n-  Handle h_init_lock(THREAD, init_lock());\n-  ObjectLocker ol(h_init_lock, THREAD);\n-\n-  \/\/ abort if someone beat us to the initialization\n-  if (!is_not_initialized()) return;  \/\/ note: not equivalent to is_initialized()\n-\n-  ClassState old_state = init_state();\n-  link_class_impl(THREAD);\n-  if (HAS_PENDING_EXCEPTION) {\n-    CLEAR_PENDING_EXCEPTION;\n-    \/\/ Abort if linking the class throws an exception.\n-\n-    \/\/ Use a test to avoid redundantly resetting the state if there's\n-    \/\/ no change.  Set_init_state() asserts that state changes make\n-    \/\/ progress, whereas here we might just be spinning in place.\n-    if (old_state != _init_state)\n-      set_init_state(old_state);\n-  } else {\n-    \/\/ linking successfull, mark class as initialized\n-    set_init_state(fully_initialized);\n-    fence_and_clear_init_lock();\n-    \/\/ trace\n-    if (log_is_enabled(Info, class, init)) {\n-      ResourceMark rm(THREAD);\n-      log_info(class, init)(\"[Initialized %s without side effects]\", external_name());\n-    }\n-  }\n-}\n-\n-\n@@ -873,0 +812,20 @@\n+void InstanceKlass::check_link_state_and_wait(JavaThread* current) {\n+  MonitorLocker ml(current, _init_monitor);\n+\n+  \/\/ Another thread is linking this class, wait.\n+  while (is_being_linked() && !is_init_thread(current)) {\n+    ml.wait();\n+  }\n+\n+  \/\/ This thread is recursively linking this class, continue\n+  if (is_being_linked() && is_init_thread(current)) {\n+    return;\n+  }\n+\n+  \/\/ If this class wasn't linked already, set state to being_linked\n+  if (!is_linked()) {\n+    set_init_state(being_linked);\n+    set_init_thread(current);\n+  }\n+}\n+\n@@ -1027,3 +986,2 @@\n-    HandleMark hm(THREAD);\n-    Handle h_init_lock(THREAD, init_lock());\n-    ObjectLocker ol(h_init_lock, jt);\n+    LockLinkState init_lock(this, jt);\n+\n@@ -1087,11 +1045,1 @@\n-      if (UseVtableBasedCHA) {\n-        MutexLocker ml(THREAD, Compile_lock);\n-        set_init_state(linked);\n-\n-        \/\/ Now flush all code that assume the class is not linked.\n-        if (Universe::is_fully_initialized()) {\n-          CodeCache::flush_dependents_on(this);\n-        }\n-      } else {\n-        set_init_state(linked);\n-      }\n+      set_initialization_state_and_notify(linked, THREAD);\n@@ -1210,0 +1158,1 @@\n+  bool throw_error = false;\n@@ -1216,2 +1165,1 @@\n-    Handle h_init_lock(THREAD, init_lock());\n-    ObjectLocker ol(h_init_lock, jt);\n+    MonitorLocker ml(THREAD, _init_monitor);\n@@ -1220,4 +1168,1 @@\n-    \/\/ If we were to use wait() instead of waitInterruptibly() then\n-    \/\/ we might end up throwing IE from link\/symbol resolution sites\n-    \/\/ that aren't expected to throw.  This would wreak havoc.  See 6320309.\n-    while (is_being_initialized() && !is_reentrant_initialization(jt)) {\n+    while (is_being_initialized() && !is_init_thread(jt)) {\n@@ -1226,1 +1171,1 @@\n-      ol.wait_uninterruptibly(jt);\n+      ml.wait();\n@@ -1231,1 +1176,1 @@\n-    if (is_being_initialized() && is_reentrant_initialization(jt)) {\n+    if (is_being_initialized() && is_init_thread(jt)) {\n@@ -1244,3 +1189,2 @@\n-      DTRACE_CLASSINIT_PROBE_WAIT(erroneous, -1, wait);\n-      ResourceMark rm(THREAD);\n-      Handle cause(THREAD, get_initialization_error(THREAD));\n+      throw_error = true;\n+    } else {\n@@ -1248,8 +1192,3 @@\n-      stringStream ss;\n-      ss.print(\"Could not initialize class %s\", external_name());\n-      if (cause.is_null()) {\n-        THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), ss.as_string());\n-      } else {\n-        THROW_MSG_CAUSE(vmSymbols::java_lang_NoClassDefFoundError(),\n-                        ss.as_string(), cause);\n-      }\n+      \/\/ Step 6\n+      set_init_state(being_initialized);\n+      set_init_thread(jt);\n@@ -1257,0 +1196,7 @@\n+  }\n+\n+  \/\/ Throw error outside lock\n+  if (throw_error) {\n+    DTRACE_CLASSINIT_PROBE_WAIT(erroneous, -1, wait);\n+    ResourceMark rm(THREAD);\n+    Handle cause(THREAD, get_initialization_error(THREAD));\n@@ -1258,3 +1204,8 @@\n-    \/\/ Step 6\n-    set_init_state(being_initialized);\n-    set_init_thread(jt);\n+    stringStream ss;\n+    ss.print(\"Could not initialize class %s\", external_name());\n+    if (cause.is_null()) {\n+      THROW_MSG(vmSymbols::java_lang_NoClassDefFoundError(), ss.as_string());\n+    } else {\n+      THROW_MSG_CAUSE(vmSymbols::java_lang_NoClassDefFoundError(),\n+                      ss.as_string(), cause);\n+    }\n@@ -1379,1 +1330,1 @@\n-    set_initialization_state_and_notify(fully_initialized, CHECK);\n+    set_initialization_state_and_notify(fully_initialized, THREAD);\n@@ -1412,4 +1363,8 @@\n-void InstanceKlass::set_initialization_state_and_notify(ClassState state, TRAPS) {\n-  Handle h_init_lock(THREAD, init_lock());\n-  if (h_init_lock() != NULL) {\n-    ObjectLocker ol(h_init_lock, THREAD);\n+void InstanceKlass::set_initialization_state_and_notify(ClassState state, JavaThread* current) {\n+  MonitorLocker ml(current, _init_monitor);\n+\n+  \/\/ Now flush all code that assume the class is not linked.\n+  \/\/ Set state under the Compile_lock also.\n+  if (state == linked && UseVtableBasedCHA && Universe::is_fully_initialized()) {\n+    MutexLocker ml(current, Compile_lock);\n+\n@@ -1418,2 +1373,2 @@\n-    fence_and_clear_init_lock();\n-    ol.notify_all(CHECK);\n+\n+    CodeCache::flush_dependents_on(this);\n@@ -1421,1 +1376,0 @@\n-    assert(h_init_lock() != NULL, \"The initialization state should never be set twice\");\n@@ -1425,0 +1379,1 @@\n+  ml.notify_all();\n@@ -2776,0 +2731,1 @@\n+  _init_monitor = NULL;\n@@ -2859,0 +2815,3 @@\n+\n+  \/\/ restore the monitor\n+  _init_monitor = create_init_monitor(\"InstanceKlassInitMonitorRestored_lock\");\n@@ -2966,0 +2925,3 @@\n+  \/\/ Destroy the init_monitor\n+  delete _init_monitor;\n+\n@@ -3642,1 +3604,1 @@\n-  \"allocated\", \"loaded\", \"linked\", \"being_initialized\", \"fully_initialized\", \"initialization_error\"\n+  \"allocated\", \"loaded\", \"being_linked\", \"linked\", \"being_initialized\", \"fully_initialized\", \"initialization_error\"\n@@ -4221,0 +4183,3 @@\n+  if (state > loaded) {\n+    assert_lock_strong(_init_monitor);\n+  }\n@@ -4224,1 +4189,2 @@\n-  assert(good_state || state == allocated, \"illegal state transition\");\n+  bool link_failed = _init_state == being_linked && state == loaded;\n+  assert(good_state || state == allocated || link_failed, \"illegal state transition\");\n@@ -4227,1 +4193,1 @@\n-  _init_state = (u1)state;\n+  _init_state = state;\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.cpp","additions":108,"deletions":142,"binary":false,"changes":250,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -158,1 +158,1 @@\n-  static const KlassID ID = InstanceKlassID;\n+  static const KlassKind Kind = InstanceKlassKind;\n@@ -161,1 +161,1 @@\n-  InstanceKlass(const ClassFileParser& parser, unsigned kind, KlassID id = ID);\n+  InstanceKlass(const ClassFileParser& parser, KlassKind kind = Kind, ReferenceType reference_type = REF_NONE);\n@@ -168,1 +168,1 @@\n-  enum ClassState {\n+  enum ClassState : u1 {\n@@ -171,0 +171,1 @@\n+    being_linked,                       \/\/ currently running verifier and rewriter\n@@ -249,4 +250,1 @@\n-  \/\/ Class states are defined as ClassState (see above).\n-  \/\/ Place the _init_state here to utilize the unused 2-byte after\n-  \/\/ _idnum_allocated_count.\n-  u1              _init_state;              \/\/ state of class\n+  ClassState      _init_state;              \/\/ state of class\n@@ -254,10 +252,1 @@\n-  \/\/ This can be used to quickly discriminate among the five kinds of\n-  \/\/ InstanceKlass. This should be an enum (?)\n-  static const unsigned _kind_other        = 0; \/\/ concrete InstanceKlass\n-  static const unsigned _kind_reference    = 1; \/\/ InstanceRefKlass\n-  static const unsigned _kind_class_loader = 2; \/\/ InstanceClassLoaderKlass\n-  static const unsigned _kind_mirror       = 3; \/\/ InstanceMirrorKlass\n-  static const unsigned _kind_inline_type  = 4; \/\/ InlineKlass\n-\n-  u1              _reference_type;                \/\/ reference type\n-  u1              _kind;                          \/\/ kind of InstanceKlass\n+  u1              _reference_type;          \/\/ reference type\n@@ -300,0 +289,1 @@\n+  Monitor*        _init_monitor;         \/\/ mutual exclusion to _init_state and _init_thread.\n@@ -301,0 +291,1 @@\n+\n@@ -588,0 +579,5 @@\n+  \/\/ Call this only if you know that the nest host has been initialized.\n+  InstanceKlass* nest_host_not_null() {\n+    assert(_nest_host != NULL, \"must be\");\n+    return _nest_host;\n+  }\n@@ -645,8 +641,9 @@\n-  bool is_loaded() const                   { return _init_state >= loaded; }\n-  bool is_linked() const                   { return _init_state >= linked; }\n-  bool is_initialized() const              { return _init_state == fully_initialized; }\n-  bool is_not_initialized() const          { return _init_state <  being_initialized; }\n-  bool is_being_initialized() const        { return _init_state == being_initialized; }\n-  bool is_in_error_state() const           { return _init_state == initialization_error; }\n-  bool is_reentrant_initialization(Thread *thread)  { return thread == _init_thread; }\n-  ClassState  init_state()                 { return (ClassState)_init_state; }\n+  bool is_loaded() const                   { return init_state() >= loaded; }\n+  bool is_linked() const                   { return init_state() >= linked; }\n+  bool is_being_linked() const             { return init_state() == being_linked; }\n+  bool is_initialized() const              { return init_state() == fully_initialized; }\n+  bool is_not_initialized() const          { return init_state() <  being_initialized; }\n+  bool is_being_initialized() const        { return init_state() == being_initialized; }\n+  bool is_in_error_state() const           { return init_state() == initialization_error; }\n+  bool is_init_thread(Thread *thread)      { return thread == _init_thread; }\n+  ClassState  init_state() const           { return Atomic::load(&_init_state); }\n@@ -656,0 +653,15 @@\n+  class LockLinkState : public StackObj {\n+    InstanceKlass* _ik;\n+    JavaThread*    _current;\n+   public:\n+    LockLinkState(InstanceKlass* ik, JavaThread* current) : _ik(ik), _current(current) {\n+      ik->check_link_state_and_wait(current);\n+    }\n+    ~LockLinkState() {\n+      if (!_ik->is_linked()) {\n+        \/\/ Reset to loaded if linking failed.\n+        _ik->set_initialization_state_and_notify(loaded, _current);\n+      }\n+    }\n+  };\n+\n@@ -687,3 +699,0 @@\n-  \/\/ set the class to initialized if no static initializer is present\n-  void eager_initialize(Thread *thread);\n-\n@@ -692,4 +701,0 @@\n-  void set_reference_type(ReferenceType t) {\n-    assert(t == (u1)t, \"overflow\");\n-    _reference_type = (u1)t;\n-  }\n@@ -897,9 +902,0 @@\n-private:\n-\n-  void set_kind(unsigned kind) {\n-    _kind = (u1)kind;\n-  }\n-\n-  bool is_kind(unsigned desired) const {\n-    return _kind == (u1)desired;\n-  }\n@@ -908,8 +904,0 @@\n-\n-  \/\/ Other is anything that is not one of the more specialized kinds of InstanceKlass.\n-  bool is_other_instance_klass() const        { return is_kind(_kind_other); }\n-  bool is_reference_instance_klass() const    { return is_kind(_kind_reference); }\n-  bool is_mirror_instance_klass() const       { return is_kind(_kind_mirror); }\n-  bool is_class_loader_instance_klass() const { return is_kind(_kind_class_loader); }\n-  bool is_inline_type_klass()           const { return is_kind(_kind_inline_type); }\n-\n@@ -1044,1 +1032,1 @@\n-  void set_initialization_state_and_notify(ClassState state, TRAPS);\n+  void set_initialization_state_and_notify(ClassState state, JavaThread* current);\n@@ -1287,1 +1275,1 @@\n-private:\n+ private:\n@@ -1291,1 +1279,4 @@\n-  void set_init_thread(Thread *thread)  { _init_thread = thread; }\n+  void set_init_thread(Thread *thread)  {\n+    assert(thread == nullptr || _init_thread == nullptr, \"Only one thread is allowed to own initialization\");\n+    _init_thread = thread;\n+  }\n@@ -1303,6 +1294,0 @@\n-  \/\/ Lock for (1) initialization; (2) access to the ConstantPool of this class.\n-  \/\/ Must be one per class and it has to be a VM internal object so java code\n-  \/\/ cannot lock it (like the mirror).\n-  \/\/ It has to be an object not a Mutex because it's held through java calls.\n-  oop init_lock() const;\n-\n@@ -1318,2 +1303,3 @@\n-private:\n-  void fence_and_clear_init_lock();\n+  Monitor* init_monitor() const { return _init_monitor; }\n+private:\n+  void check_link_state_and_wait(JavaThread* current);\n@@ -1325,1 +1311,0 @@\n-  void eager_initialize_impl                     ();\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.hpp","additions":47,"deletions":62,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,0 @@\n-#include \"memory\/iterator.hpp\"\n@@ -38,0 +37,1 @@\n+#include \"utilities\/devirtualizer.inline.hpp\"\n","filename":"src\/hotspot\/share\/oops\/instanceKlass.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -200,1 +200,1 @@\n-\/\/ \"Normal\" instantiation is preceeded by a MetaspaceObj allocation\n+\/\/ \"Normal\" instantiation is preceded by a MetaspaceObj allocation\n@@ -204,3 +204,3 @@\n-Klass::Klass(KlassID id) : _id(id),\n-                            _prototype_header(markWord::prototype()),\n-                           _shared_class_path_index(-1) {\n+Klass::Klass(KlassKind kind) : _kind(kind),\n+                               _prototype_header(markWord::prototype()),\n+                               _shared_class_path_index(-1) {\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -40,9 +40,11 @@\n-\/\/ Klass IDs for all subclasses of Klass\n-enum KlassID {\n-  InstanceKlassID,\n-  InstanceRefKlassID,\n-  InstanceMirrorKlassID,\n-  InstanceClassLoaderKlassID,\n-  TypeArrayKlassID,\n-  FlatArrayKlassID,\n-  ObjArrayKlassID\n+\/\/ Klass Kinds for all subclasses of Klass\n+enum KlassKind {\n+  InstanceKlassKind,\n+  InlineKlassKind,\n+  InstanceRefKlassKind,\n+  InstanceMirrorKlassKind,\n+  InstanceClassLoaderKlassKind,\n+  InstanceStackChunkKlassKind,\n+  TypeArrayKlassKind,\n+  FlatArrayKlassKind,\n+  ObjArrayKlassKind\n@@ -51,1 +53,1 @@\n-const uint KLASS_ID_COUNT = 7;\n+const uint KLASS_KIND_COUNT = ObjArrayKlassKind + 1;\n@@ -118,2 +120,4 @@\n-  \/\/ Klass identifier used to implement devirtualized oop closure dispatching.\n-  const KlassID _id;\n+  \/\/ Klass kind used to resolve the runtime type of the instance.\n+  \/\/  - Used to implement devirtualized oop closure dispatching.\n+  \/\/  - Various type checking in the JVM\n+  const KlassKind _kind;\n@@ -175,1 +179,1 @@\n-  \/\/ Flags of the current shared class.\n+  \/\/ Various attributes for shared classes. Should be zero for a non-shared class.\n@@ -177,1 +181,1 @@\n-  enum {\n+  enum CDSSharedClassFlags {\n@@ -182,1 +186,3 @@\n-    _regenerated                           = 1 << 5\n+    \/\/ This class was not loaded from a classfile in the module image\n+    \/\/ or classpath.\n+    _is_generated_shared_class             = 1 << 5\n@@ -191,2 +197,2 @@\n-  Klass(KlassID id);\n-  Klass() : _id(KlassID(-1)) { assert(DumpSharedSpaces || UseSharedSpaces, \"only for cds\"); }\n+  Klass(KlassKind kind);\n+  Klass() : _kind(KlassKind(-1)) { assert(DumpSharedSpaces || UseSharedSpaces, \"only for cds\"); }\n@@ -197,1 +203,1 @@\n-  int id() { return _id; }\n+  int kind() { return _kind; }\n@@ -354,2 +360,2 @@\n-  void set_regenerated() {\n-    CDS_ONLY(_shared_class_flags |= _regenerated;)\n+  void set_is_generated_shared_class() {\n+    CDS_ONLY(_shared_class_flags |= _is_generated_shared_class;)\n@@ -357,2 +363,2 @@\n-  bool is_regenerated() const {\n-    CDS_ONLY(return (_shared_class_flags & _regenerated) != 0;)\n+  bool is_generated_shared_class() const {\n+    CDS_ONLY(return (_shared_class_flags & _is_generated_shared_class) != 0;)\n@@ -641,16 +647,12 @@\n-  inline  bool is_instance_klass()            const { return assert_same_query(\n-                                                      layout_helper_is_instance(layout_helper()),\n-                                                      is_instance_klass_slow()); }\n-  inline  bool is_array_klass()               const { return assert_same_query(\n-                                                    layout_helper_is_array(layout_helper()),\n-                                                    is_array_klass_slow()); }\n-  inline  bool is_objArray_klass()            const { return assert_same_query(\n-                                                    layout_helper_is_objArray(layout_helper()),\n-                                                    is_objArray_klass_slow()); }\n-  inline  bool is_typeArray_klass()           const { return assert_same_query(\n-                                                    layout_helper_is_typeArray(layout_helper()),\n-                                                    is_typeArray_klass_slow()); }\n-  inline  bool is_inline_klass()              const { return is_inline_klass_slow(); } \/\/temporary hack\n-  inline  bool is_flatArray_klass()           const { return assert_same_query(\n-                                                    layout_helper_is_flatArray(layout_helper()),\n-                                                    is_flatArray_klass_slow()); }\n+  bool is_instance_klass()              const { return assert_same_query(_kind <= InstanceStackChunkKlassKind, is_instance_klass_slow()); }\n+  bool is_inline_klass()                const { return is_inline_klass_slow(); } \/\/temporary hack\n+  \/\/ Other is anything that is not one of the more specialized kinds of InstanceKlass.\n+  bool is_other_instance_klass()        const { return _kind <= InlineKlassKind; }\n+  bool is_reference_instance_klass()    const { return _kind == InstanceRefKlassKind; }\n+  bool is_mirror_instance_klass()       const { return _kind == InstanceMirrorKlassKind; }\n+  bool is_class_loader_instance_klass() const { return _kind == InstanceClassLoaderKlassKind; }\n+  bool is_array_klass()                 const { return assert_same_query( _kind >= TypeArrayKlassKind, is_array_klass_slow()); }\n+  bool is_stack_chunk_instance_klass()  const { return _kind == InstanceStackChunkKlassKind; }\n+  bool is_flatArray_klass()             const { return assert_same_query( _kind == FlatArrayKlassKind, is_flatArray_klass_slow()); }\n+  bool is_objArray_klass()              const { return assert_same_query( _kind == ObjArrayKlassKind,  is_objArray_klass_slow()); }\n+  bool is_typeArray_klass()             const { return assert_same_query( _kind == TypeArrayKlassKind, is_typeArray_klass_slow()); }\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":40,"deletions":38,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,0 +47,4 @@\n+\/\/ This returns false if the Klass is unloaded, or about to be unloaded because the holder of\n+\/\/ the CLD is no longer strongly reachable.\n+\/\/ The return value of this function may change from true to false after a safepoint. So the caller\n+\/\/ of this function must ensure that a safepoint doesn't happen while interpreting the return value.\n","filename":"src\/hotspot\/share\/oops\/klass.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1150,1 +1150,1 @@\n-  \/\/ There's alway an extra itable entry so we can null-terminate it.\n+  \/\/ There's always an extra itable entry so we can null-terminate it.\n","filename":"src\/hotspot\/share\/oops\/klassVtable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -154,1 +154,1 @@\n-  \/\/ When loading a class from CDS archive at run time, and no class redefintion\n+  \/\/ When loading a class from CDS archive at run time, and no class redefinition\n","filename":"src\/hotspot\/share\/oops\/klassVtable.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -287,1 +287,1 @@\n-    return ((value() & monitor_value) != 0);\n+    return ((value() & lock_mask_in_place) == monitor_value);\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+#include \"runtime\/continuationEntry.hpp\"\n@@ -108,0 +109,1 @@\n+  set_changes_current_thread(false);\n@@ -529,1 +531,1 @@\n-  \/\/ be excuted more than n times.\n+  \/\/ be executed more than n times.\n@@ -585,3 +587,3 @@\n-\/\/ Build a MethodData* object to hold information about this method\n-\/\/ collected in the interpreter.\n-void Method::build_interpreter_method_data(const methodHandle& method, TRAPS) {\n+\/\/ Build a MethodData* object to hold profiling information collected on this\n+\/\/ method when requested.\n+void Method::build_profiling_method_data(const methodHandle& method, TRAPS) {\n@@ -610,1 +612,1 @@\n-      tty->print(\"build_interpreter_method_data for \");\n+      tty->print(\"build_profiling_method_data for \");\n@@ -675,0 +677,1 @@\n+  set_num_stack_arg_slots(fp.num_stack_arg_slots());\n@@ -829,1 +832,0 @@\n-  ResourceMark rm;\n@@ -832,0 +834,1 @@\n+      ResourceMark rm;\n@@ -1029,1 +1032,1 @@\n-  assert(!is_method_handle_intrinsic() || function == SharedRuntime::native_method_throw_unsatisfied_link_error_entry(), \"\");\n+  assert(!is_special_native_intrinsic() || function == SharedRuntime::native_method_throw_unsatisfied_link_error_entry(), \"\");\n@@ -1042,1 +1045,1 @@\n-      \"post_event_flag mis-match\");\n+      \"post_event_flag mismatch\");\n@@ -1059,1 +1062,1 @@\n-  if (is_method_handle_intrinsic())\n+  if (is_special_native_intrinsic())\n@@ -1116,1 +1119,1 @@\n-  if (is_method_handle_intrinsic() && is_synthetic()) {\n+  if (is_special_native_intrinsic() && is_synthetic()) {\n@@ -1285,0 +1288,6 @@\n+\n+  if (h_method->is_continuation_enter_intrinsic()) {\n+    \/\/ the entry points to this method will be set in set_code, called when first resolving this method\n+    _from_interpreted_entry = NULL;\n+    _from_compiled_entry = NULL;\n+  }\n@@ -1392,2 +1401,10 @@\n-  \/\/ Instantly compiled code can execute.\n-  if (!mh->is_method_handle_intrinsic())\n+\n+  if (mh->is_continuation_enter_intrinsic()) {\n+    assert(mh->_from_interpreted_entry == NULL, \"initialized incorrectly\"); \/\/ see link_method\n+\n+    \/\/ This is the entry used when we're in interpreter-only mode; see InterpreterMacroAssembler::jump_from_interpreted\n+    mh->_i2i_entry = ContinuationEntry::interpreted_entry();\n+    \/\/ This must come last, as it is what's tested in LinkResolver::resolve_static_call\n+    Atomic::release_store(&mh->_from_interpreted_entry , mh->get_i2c_entry());\n+  } else if (!mh->is_method_handle_intrinsic()) {\n+    \/\/ Instantly compiled code can execute.\n@@ -1395,0 +1412,1 @@\n+  }\n@@ -1449,1 +1467,1 @@\n-    \/\/ This is an auxilary frame -- ignore it\n+    \/\/ This is an auxiliary frame -- ignore it\n@@ -1554,2 +1572,2 @@\n-  if (log_is_enabled(Info, methodhandles) && (Verbose || WizardMode)) {\n-    LogTarget(Info, methodhandles) lt;\n+  if (log_is_enabled(Debug, methodhandles)) {\n+    LogTarget(Debug, methodhandles) lt;\n@@ -1802,1 +1820,1 @@\n-void Method::print_short_name(outputStream* st) {\n+void Method::print_short_name(outputStream* st) const {\n@@ -1870,1 +1888,1 @@\n-void Method::print_name(outputStream* st) {\n+void Method::print_name(outputStream* st) const {\n@@ -2355,0 +2373,7 @@\n+void Method::record_gc_epoch() {\n+  \/\/ If any method is on the stack in continuations, none of them can be reclaimed,\n+  \/\/ so save the marking cycle to check for the whole class in the cpCache.\n+  \/\/ The cpCache is writeable.\n+  constants()->cache()->record_gc_epoch();\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/method.cpp","additions":42,"deletions":17,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"classfile\/vmSymbols.hpp\"\n@@ -87,12 +88,14 @@\n-    _caller_sensitive      = 1 << 0,\n-    _force_inline          = 1 << 1,\n-    _dont_inline           = 1 << 2,\n-    _hidden                = 1 << 3,\n-    _has_injected_profile  = 1 << 4,\n-    _intrinsic_candidate   = 1 << 5,\n-    _reserved_stack_access = 1 << 6,\n-    _scalarized_args       = 1 << 7,\n-    _scalarized_return     = 1 << 8,\n-    _c1_needs_stack_repair = 1 << 9,\n-    _c2_needs_stack_repair = 1 << 10,\n-    _scoped                = 1 << 11\n+    _caller_sensitive       = 1 << 0,\n+    _force_inline           = 1 << 1,\n+    _dont_inline            = 1 << 2,\n+    _hidden                 = 1 << 3,\n+    _has_injected_profile   = 1 << 4,\n+    _intrinsic_candidate    = 1 << 5,\n+    _reserved_stack_access  = 1 << 6,\n+    _scalarized_args        = 1 << 7,\n+    _scalarized_return      = 1 << 8,\n+    _c1_needs_stack_repair  = 1 << 9,\n+    _c2_needs_stack_repair  = 1 << 10,\n+    _scoped                 = 1 << 11,\n+    _changes_current_thread = 1 << 12,\n+    _jvmti_mount_transition = 1 << 13,\n@@ -427,1 +430,1 @@\n-  static void build_interpreter_method_data(const methodHandle& method, TRAPS);\n+  static void build_profiling_method_data(const methodHandle& method, TRAPS);\n@@ -490,0 +493,3 @@\n+  \/\/ the number of argument reg slots that the compiled method uses on the stack.\n+  int num_stack_arg_slots() const { return constMethod()->num_stack_arg_slots(); }\n+\n@@ -752,0 +758,7 @@\n+\n+\n+  \/\/ Continuation\n+  bool is_continuation_enter_intrinsic() const { return intrinsic_id() == vmIntrinsics::_Continuation_enterSpecial; }\n+\n+  bool is_special_native_intrinsic() const { return is_method_handle_intrinsic() || is_continuation_enter_intrinsic(); }\n+\n@@ -776,0 +789,2 @@\n+  void record_gc_epoch();\n+\n@@ -860,0 +875,14 @@\n+  bool changes_current_thread() {\n+    return (_flags & _changes_current_thread) != 0;\n+  }\n+  void set_changes_current_thread(bool x) {\n+    _flags = x ? (_flags | _changes_current_thread) : (_flags & ~_changes_current_thread);\n+  }\n+\n+  bool jvmti_mount_transition() {\n+    return (_flags & _jvmti_mount_transition) != 0;\n+  }\n+  void set_jvmti_mount_transition(bool x) {\n+    _flags = x ? (_flags | _jvmti_mount_transition) : (_flags & ~_jvmti_mount_transition);\n+  }\n+\n@@ -1008,1 +1037,1 @@\n-  void print_short_name(outputStream* st = tty); \/\/ prints as klassname::methodname; Exposed so field engineers can debug VM\n+  void print_short_name(outputStream* st = tty) const; \/\/ prints as klassname::methodname; Exposed so field engineers can debug VM\n@@ -1010,1 +1039,1 @@\n-  void print_name(outputStream* st = tty); \/\/ prints as \"virtual void foo(int)\"; exposed for -Xlog:redefine+class\n+  void print_name(outputStream* st = tty) const; \/\/ prints as \"virtual void foo(int)\"; exposed for -Xlog:redefine+class\n@@ -1012,1 +1041,1 @@\n-  void print_name(outputStream* st = tty)        PRODUCT_RETURN; \/\/ prints as \"virtual void foo(int)\"\n+  void print_name(outputStream* st = tty) const  PRODUCT_RETURN; \/\/ prints as \"virtual void foo(int)\"\n@@ -1056,0 +1085,2 @@\n+\n+  void set_num_stack_arg_slots(int n) { constMethod()->set_num_stack_arg_slots(n); }\n","filename":"src\/hotspot\/share\/oops\/method.hpp","additions":47,"deletions":16,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -544,1 +544,1 @@\n-\/\/ case was taken and specify the data displacment for each branch target.\n+\/\/ case was taken and specify the data displacement for each branch target.\n@@ -1292,1 +1292,3 @@\n-  Copy::zero_to_bytes(((address)_data) + data_size, extra_size);\n+  if (extra_size > 0) {\n+    Copy::zero_to_bytes(((address)_data) + data_size, extra_size);\n+  }\n","filename":"src\/hotspot\/share\/oops\/methodData.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,3 @@\n-\/\/ during zeroth-tier (interpretive) and first-tier execution.\n+\/\/ during zeroth-tier (interpreter) and third-tier (C1 with full profiling)\n+\/\/ execution.\n+\/\/\n@@ -99,1 +101,1 @@\n-  \/\/ to accomodate a pointer or an integer.\n+  \/\/ to accommodate a pointer or an integer.\n@@ -1622,1 +1624,1 @@\n-\/\/ case was taken and specify the data displacment for each branch target.\n+\/\/ case was taken and specify the data displacement for each branch target.\n@@ -2042,1 +2044,1 @@\n-\/\/ intepretation, when a bytecode is encountered that has profile data\n+\/\/ interpretation, when a bytecode is encountered that has profile data\n","filename":"src\/hotspot\/share\/oops\/methodData.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -143,1 +143,1 @@\n-ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name, bool null_free) : ArrayKlass(name, ID) {\n+ObjArrayKlass::ObjArrayKlass(int n, Klass* element_klass, Symbol* name, bool null_free) : ArrayKlass(name, Kind) {\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-  static const KlassID ID = ObjArrayKlassID;\n+  static const KlassKind Kind = ObjArrayKlassKind;\n","filename":"src\/hotspot\/share\/oops\/objArrayKlass.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,2 +37,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n-#include \"utilities\/copy.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -42,1 +41,7 @@\n-  klass()->oop_print_on(const_cast<oopDesc*>(this), st);\n+  if (*((juint*)this) == badHeapWordVal) {\n+    st->print(\"BAD WORD\");\n+  } else if (*((juint*)this) == badMetaWordVal) {\n+    st->print(\"BAD META WORD\");\n+  } else {\n+    klass()->oop_print_on(cast_to_oop(this), st);\n+  }\n@@ -137,6 +142,8 @@\n-bool oopDesc::is_instance_noinline()          const { return is_instance();            }\n-bool oopDesc::is_array_noinline()             const { return is_array();               }\n-bool oopDesc::is_objArray_noinline()          const { return is_objArray();            }\n-bool oopDesc::is_typeArray_noinline()         const { return is_typeArray();           }\n-bool oopDesc::is_flatArray_noinline()         const { return is_flatArray();           }\n-bool oopDesc::is_null_free_array_noinline()   const { return is_null_free_array();     }\n+bool oopDesc::is_instance_noinline()        const { return is_instance();         }\n+bool oopDesc::is_instanceRef_noinline()     const { return is_instanceRef();      }\n+bool oopDesc::is_stackChunk_noinline()      const { return is_stackChunk();       }\n+bool oopDesc::is_array_noinline()           const { return is_array();            }\n+bool oopDesc::is_objArray_noinline()        const { return is_objArray();         }\n+bool oopDesc::is_typeArray_noinline()       const { return is_typeArray();        }\n+bool oopDesc::is_flatArray_noinline()       const { return is_flatArray();        }\n+bool oopDesc::is_null_free_array_noinline() const { return is_null_free_array();  }\n@@ -228,2 +235,8 @@\n-bool oopDesc::get_UseParallelGC() { return UseParallelGC; }\n-bool oopDesc::get_UseG1GC()       { return UseG1GC;       }\n+bool oopDesc::size_might_change() {\n+  \/\/ UseParallelGC and UseG1GC can change the length field\n+  \/\/ of an \"old copy\" of an object array in the young gen so it indicates\n+  \/\/ the grey portion of an already copied array. This will cause the first\n+  \/\/ disjunct below to fail if the two comparands are computed across such\n+  \/\/ a concurrent change.\n+  return Universe::heap()->is_gc_active() && is_objArray() && is_forwarded() && (UseParallelGC || UseG1GC);\n+}\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":25,"deletions":12,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -112,1 +112,1 @@\n-  \/\/ Returns the actual oop size of the object\n+  \/\/ Returns the actual oop size of the object in machine words\n@@ -120,7 +120,9 @@\n-  inline bool is_instance()            const;\n-  inline bool is_array()               const;\n-  inline bool is_objArray()            const;\n-  inline bool is_typeArray()           const;\n-  inline bool is_inline_type()         const;\n-  inline bool is_flatArray()           const;\n-  inline bool is_null_free_array()     const;\n+  inline bool is_instance()         const;\n+  inline bool is_inline_type()      const;\n+  inline bool is_instanceRef()      const;\n+  inline bool is_stackChunk()       const;\n+  inline bool is_array()            const;\n+  inline bool is_objArray()         const;\n+  inline bool is_typeArray()        const;\n+  inline bool is_flatArray()        const;\n+  inline bool is_null_free_array()  const;\n@@ -129,6 +131,8 @@\n-  bool is_instance_noinline()          const;\n-  bool is_array_noinline()             const;\n-  bool is_objArray_noinline()          const;\n-  bool is_typeArray_noinline()         const;\n-  bool is_flatArray_noinline()         const;\n-  bool is_null_free_array_noinline()   const;\n+  bool is_instance_noinline()         const;\n+  bool is_instanceRef_noinline()      const;\n+  bool is_stackChunk_noinline()       const;\n+  bool is_array_noinline()            const;\n+  bool is_objArray_noinline()         const;\n+  bool is_typeArray_noinline()        const;\n+  bool is_flatArray_noinline()        const;\n+  bool is_null_free_array_noinline()  const;\n@@ -328,3 +332,1 @@\n-  \/\/ Avoid include gc_globals.hpp in oop.inline.hpp\n-  DEBUG_ONLY(bool get_UseParallelGC();)\n-  DEBUG_ONLY(bool get_UseG1GC();)\n+  DEBUG_ONLY(bool size_might_change();)\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":20,"deletions":18,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,0 +35,1 @@\n+#include \"oops\/instanceKlass.hpp\"\n@@ -179,8 +180,1 @@\n-      \/\/ UseParallelGC and UseG1GC can change the length field\n-      \/\/ of an \"old copy\" of an object array in the young gen so it indicates\n-      \/\/ the grey portion of an already copied array. This will cause the first\n-      \/\/ disjunct below to fail if the two comparands are computed across such\n-      \/\/ a concurrent change.\n-      assert((s == klass->oop_size(this)) ||\n-             (Universe::is_gc_active() && is_objArray() && is_forwarded() && (get_UseParallelGC() || get_UseG1GC())),\n-             \"wrong array object size\");\n+      assert(s == klass->oop_size(this) || size_might_change(), \"wrong array object size\");\n@@ -198,4 +192,6 @@\n-bool oopDesc::is_instance()  const { return klass()->is_instance_klass();  }\n-bool oopDesc::is_array()     const { return klass()->is_array_klass();     }\n-bool oopDesc::is_objArray()  const { return klass()->is_objArray_klass();  }\n-bool oopDesc::is_typeArray() const { return klass()->is_typeArray_klass(); }\n+bool oopDesc::is_instance()    const { return klass()->is_instance_klass();             }\n+bool oopDesc::is_instanceRef() const { return klass()->is_reference_instance_klass();   }\n+bool oopDesc::is_stackChunk()  const { return klass()->is_stack_chunk_instance_klass(); }\n+bool oopDesc::is_array()       const { return klass()->is_array_klass();                }\n+bool oopDesc::is_objArray()    const { return klass()->is_objArray_klass();             }\n+bool oopDesc::is_typeArray()   const { return klass()->is_typeArray_klass();            }\n@@ -279,1 +275,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n+  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n@@ -286,1 +282,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversable\");\n+  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":11,"deletions":15,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,0 +48,1 @@\n+typedef class     stackChunkOopDesc*          stackChunkOop;\n@@ -147,0 +148,1 @@\n+DEF_OOP(stackChunk);\n@@ -179,0 +181,1 @@\n+class     InlineKlass;\n@@ -182,1 +185,1 @@\n-class     InlineKlass;\n+class     InstanceStackChunkKlass;\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,1 +61,1 @@\n-\/\/              ^ increment on assignmnet\n+\/\/              ^ increment on assignment\n@@ -276,1 +276,1 @@\n-  \/\/ seperated by ', ' to the outputStream.  Prints external names as\n+  \/\/ separated by ', ' to the outputStream.  Prints external names as\n","filename":"src\/hotspot\/share\/oops\/symbol.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -78,1 +78,1 @@\n-TypeArrayKlass::TypeArrayKlass(BasicType type, Symbol* name) : ArrayKlass(name, ID) {\n+TypeArrayKlass::TypeArrayKlass(BasicType type, Symbol* name) : ArrayKlass(name, Kind) {\n","filename":"src\/hotspot\/share\/oops\/typeArrayKlass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -307,3 +307,4 @@\n-  \/\/ Convert \"x+(0-y)\" into \"(x-y)\"\n-  if (op2 == Op_Sub(bt) && phase->type(in2->in(1)) == TypeInteger::zero(bt)) {\n-    return SubNode::make(in1, in2->in(2), bt);\n+  \/\/ Convert (con - y) + x into \"(x - y) + con\"\n+  if (op1 == Op_Sub(bt) && in1->in(1)->Opcode() == Op_ConIL(bt)\n+      && in1 != in1->in(2) && !(in1->in(2)->is_Phi() && in1->in(2)->as_Phi()->is_tripcount(bt))) {\n+    return AddNode::make(phase->transform(SubNode::make(in2, in1->in(2), bt)), in1->in(1), bt);\n@@ -312,3 +313,4 @@\n-  \/\/ Convert \"(0-y)+x\" into \"(x-y)\"\n-  if (op1 == Op_Sub(bt) && phase->type(in1->in(1)) == TypeInteger::zero(bt)) {\n-    return SubNode::make(in2, in1->in(2), bt);\n+  \/\/ Convert x + (con - y) into \"(x - y) + con\"\n+  if (op2 == Op_Sub(bt) && in2->in(1)->Opcode() == Op_ConIL(bt)\n+      && in2 != in2->in(2) && !(in2->in(2)->is_Phi() && in2->in(2)->as_Phi()->is_tripcount(bt))) {\n+    return AddNode::make(phase->transform(SubNode::make(in1, in2->in(2), bt)), in2->in(1), bt);\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":8,"deletions":6,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -129,1 +129,1 @@\n-      ciInstanceKlass* ik = inst_src->klass()->as_instance_klass();\n+      ciInstanceKlass* ik = inst_src->instance_klass();\n@@ -204,0 +204,3 @@\n+\n+  ciInstanceKlass* ik = inst_src->instance_klass();\n+\n@@ -205,1 +208,0 @@\n-    ciInstanceKlass* ik = inst_src->klass()->as_instance_klass();\n@@ -216,1 +218,0 @@\n-  ciInstanceKlass* ik = inst_src->klass()->as_instance_klass();\n@@ -274,3 +275,2 @@\n-\n-    if (ary_src  == NULL || ary_src->klass()  == NULL ||\n-        ary_dest == NULL || ary_dest->klass() == NULL) {\n+    if (ary_src  == NULL || ary_src->elem()  == Type::BOTTOM ||\n+        ary_dest == NULL || ary_dest->elem() == Type::BOTTOM) {\n@@ -281,3 +281,3 @@\n-    BasicType src_elem  = ary_src->klass()->as_array_klass()->element_type()->basic_type();\n-    BasicType dest_elem = ary_dest->klass()->as_array_klass()->element_type()->basic_type();\n-    if (src_elem == T_ARRAY || (src_elem == T_PRIMITIVE_OBJECT && ary_src->klass()->is_obj_array_klass())) {\n+    BasicType src_elem = ary_src->elem()->array_element_basic_type();\n+    BasicType dest_elem = ary_dest->elem()->array_element_basic_type();\n+    if (src_elem == T_ARRAY || src_elem == T_NARROWOOP || (src_elem == T_PRIMITIVE_OBJECT && !ary_src->is_flat())) {\n@@ -286,1 +286,1 @@\n-    if (dest_elem == T_ARRAY || (dest_elem == T_PRIMITIVE_OBJECT && ary_dest->klass()->is_obj_array_klass())) {\n+    if (dest_elem == T_ARRAY || dest_elem == T_NARROWOOP || (dest_elem == T_PRIMITIVE_OBJECT && !ary_dest->is_flat())) {\n@@ -348,2 +348,2 @@\n-    BasicType elem = ary_src->klass()->as_array_klass()->element_type()->basic_type();\n-    if (elem == T_ARRAY || (elem == T_PRIMITIVE_OBJECT && ary_src->klass()->is_obj_array_klass())) {\n+    BasicType elem = ary_src->isa_aryptr()->elem()->array_element_basic_type();\n+    if (elem == T_ARRAY || elem == T_NARROWOOP || (elem == T_PRIMITIVE_OBJECT && !ary_src->is_flat())) {\n@@ -364,1 +364,1 @@\n-    \/\/ The address is offseted to an aligned address where a raw copy would start.\n+    \/\/ The address is offsetted to an aligned address where a raw copy would start.\n@@ -555,1 +555,1 @@\n-      BasicType elem = ary_src != NULL ? ary_src->klass()->as_array_klass()->element_type()->basic_type() : T_CONFLICT;\n+      BasicType elem = ary_src != NULL ? ary_src->elem()->array_element_basic_type() : T_CONFLICT;\n@@ -558,1 +558,1 @@\n-             (ary_src != NULL && elem == T_PRIMITIVE_OBJECT && ary_src->klass()->is_obj_array_klass()), \"added control for clone?\");\n+             (ary_src != NULL && elem == T_PRIMITIVE_OBJECT && ary_src->is_not_flat()), \"added control for clone?\");\n@@ -826,2 +826,3 @@\n-  ciArrayKlass* klass = ary_t->klass()->as_array_klass();\n-  BasicType ary_elem = klass->element_type()->basic_type();\n+  BasicType ary_elem = ary_t->isa_aryptr()->elem()->array_element_basic_type();\n+  if (is_reference_type(ary_elem, true)) ary_elem = T_OBJECT;\n+\n@@ -830,2 +831,2 @@\n-  if (klass->is_flat_array_klass()) {\n-    elemsize = klass->as_flat_array_klass()->element_byte_size();\n+  if (ary_t->klass()->is_flat_array_klass()) {\n+    elemsize = ary_t->klass()->as_flat_array_klass()->element_byte_size();\n","filename":"src\/hotspot\/share\/opto\/arraycopynode.cpp","additions":21,"deletions":20,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -370,0 +370,1 @@\n+  bool has_derived_oops = false;\n@@ -375,0 +376,1 @@\n+    has_derived_oops = true;\n@@ -386,0 +388,1 @@\n+    assert(has_derived_oops == omap->has_derived_oops(), \"\");\n@@ -388,0 +391,9 @@\n+\n+  int num_oops = 0;\n+  for (OopMapStream oms2(omap); !oms2.is_done(); oms2.next()) {\n+    OopMapValue omv = oms2.current();\n+    if (omv.type() == OopMapValue::oop_value || omv.type() == OopMapValue::narrowoop_value) {\n+      num_oops++;\n+    }\n+  }\n+  assert(num_oops == omap->num_oops(), \"num_oops: %d omap->num_oops(): %d\", num_oops, omap->num_oops());\n","filename":"src\/hotspot\/share\/opto\/buildOopMap.cpp","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -218,0 +218,7 @@\n+  \/\/ Don't inline a method that changes Thread.currentThread() except\n+  \/\/ into another method that is annotated @ChangesCurrentThread.\n+  if (callee_method->changes_current_thread()\n+      && ! C->method()->changes_current_thread()) {\n+    fail_msg = \"method changes current thread\";\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/bytecodeInfo.cpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -85,0 +85,5 @@\n+  product(intx, SuperWordMaxVectorSize, 64, DIAGNOSTIC,                     \\\n+          \"Vector size limit in bytes for superword, \"                      \\\n+          \"superword vector size limit in bytes\")                           \\\n+          range(0, max_jint)                                                \\\n+                                                                            \\\n@@ -168,1 +173,1 @@\n-          \"Print CFG block freqencies\")                                     \\\n+          \"Print CFG block frequencies\")                                    \\\n@@ -337,1 +342,1 @@\n-          \"Globally supress vectorization set in VectorizeMethod\")          \\\n+          \"Globally suppress vectorization set in VectorizeMethod\")         \\\n@@ -414,0 +419,3 @@\n+  product(bool, OptimizeUnstableIf, true, DIAGNOSTIC,                       \\\n+          \"Optimize UnstableIf traps\")                                      \\\n+                                                                            \\\n@@ -532,1 +540,1 @@\n-          \"Miniumum %% of a successor (predecessor) for which block \"       \\\n+          \"Minimum %% of a successor (predecessor) for which block \"        \\\n@@ -633,1 +641,2 @@\n-          \"0 for no aliasing, 1 for oop\/field\/static\/array split, \"         \\\n+          \"(Deprecated) 0 for no aliasing, \"                                \\\n+          \"1 for oop\/field\/static\/array split, \"                            \\\n@@ -774,0 +783,8 @@\n+  product(bool, DuplicateBackedge, true, DIAGNOSTIC,                        \\\n+          \"Transform loop with a merge point into 2 loops if inner loop is\" \\\n+          \"expected to optimize better\")                                    \\\n+                                                                            \\\n+  develop(bool, StressDuplicateBackedge, false,                             \\\n+          \"Run DuplicateBackedge whenever possible ignoring benefit\"        \\\n+          \"analysis\")                                                       \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/opto\/c2_globals.hpp","additions":22,"deletions":5,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -246,0 +246,8 @@\n+  case vmIntrinsics::_compress_i:\n+  case vmIntrinsics::_compress_l:\n+    if (!Matcher::match_rule_supported(Op_CompressBits)) return false;\n+    break;\n+  case vmIntrinsics::_expand_i:\n+  case vmIntrinsics::_expand_l:\n+    if (!Matcher::match_rule_supported(Op_ExpandBits)) return false;\n+    break;\n@@ -270,0 +278,18 @@\n+  case vmIntrinsics::_compareUnsigned_i:\n+    if (!Matcher::match_rule_supported(Op_CmpU3)) return false;\n+    break;\n+  case vmIntrinsics::_compareUnsigned_l:\n+    if (!Matcher::match_rule_supported(Op_CmpUL3)) return false;\n+    break;\n+  case vmIntrinsics::_divideUnsigned_i:\n+    if (!Matcher::match_rule_supported(Op_UDivI)) return false;\n+    break;\n+  case vmIntrinsics::_remainderUnsigned_i:\n+    if (!Matcher::match_rule_supported(Op_UModI)) return false;\n+    break;\n+  case vmIntrinsics::_divideUnsigned_l:\n+    if (!Matcher::match_rule_supported(Op_UDivL)) return false;\n+    break;\n+  case vmIntrinsics::_remainderUnsigned_l:\n+    if (!Matcher::match_rule_supported(Op_UModL)) return false;\n+    break;\n@@ -509,0 +535,6 @@\n+  case vmIntrinsics::_floatIsInfinite:\n+    if (!Matcher::match_rule_supported(Op_IsInfiniteF)) return false;\n+    break;\n+  case vmIntrinsics::_doubleIsInfinite:\n+    if (!Matcher::match_rule_supported(Op_IsInfiniteD)) return false;\n+    break;\n@@ -526,0 +558,2 @@\n+  case vmIntrinsics::_roundD:\n+  case vmIntrinsics::_roundF:\n@@ -631,0 +665,1 @@\n+  case vmIntrinsics::_currentCarrierThread:\n@@ -632,0 +667,3 @@\n+  case vmIntrinsics::_setCurrentThread:\n+  case vmIntrinsics::_extentLocalCache:\n+  case vmIntrinsics::_setExtentLocalCache:\n@@ -634,1 +672,0 @@\n-  case vmIntrinsics::_getClassId:\n@@ -704,0 +741,1 @@\n+  case vmIntrinsics::_Continuation_doYield:\n@@ -706,0 +744,1 @@\n+  case vmIntrinsics::_VectorCompressExpand:\n","filename":"src\/hotspot\/share\/opto\/c2compiler.cpp","additions":41,"deletions":2,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,1 +44,0 @@\n-#include \"ci\/ciNativeEntryPoint.hpp\"\n@@ -440,1 +439,1 @@\n-  \/\/ expression stacks which causes late inlining to break. The MH invoker is not expected to be called from a method wih\n+  \/\/ expression stacks which causes late inlining to break. The MH invoker is not expected to be called from a method with\n@@ -613,56 +612,0 @@\n-static bool has_non_debug_usages(Node* n) {\n-  for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n-    Node* m = n->fast_out(i);\n-    if (!m->is_SafePoint()\n-        || (m->is_Call() && m->as_Call()->has_non_debug_use(n))) {\n-      return true;\n-    }\n-  }\n-  return false;\n-}\n-\n-static bool is_box_cache_valid(CallNode* call) {\n-  ciInstanceKlass* klass = call->as_CallStaticJava()->method()->holder();\n-  return klass->is_box_cache_valid();\n-}\n-\n-\/\/ delay box in runtime, treat box as a scalarized object\n-static void scalarize_debug_usages(CallNode* call, Node* resproj) {\n-  GraphKit kit(call->jvms());\n-  PhaseGVN& gvn = kit.gvn();\n-\n-  ProjNode* res = resproj->as_Proj();\n-  ciInstanceKlass* klass = call->as_CallStaticJava()->method()->holder();\n-  int n_fields = klass->nof_nonstatic_fields();\n-  assert(n_fields == 1, \"the klass must be an auto-boxing klass\");\n-\n-  for (DUIterator_Last imin, i = res->last_outs(imin); i >= imin;) {\n-    SafePointNode* sfpt = res->last_out(i)->as_SafePoint();\n-    uint first_ind = sfpt->req() - sfpt->jvms()->scloff();\n-    Node* sobj = new SafePointScalarObjectNode(gvn.type(res)->isa_oopptr(),\n-#ifdef ASSERT\n-                                                call,\n-#endif \/\/ ASSERT\n-                                                first_ind, n_fields, true);\n-    sobj->init_req(0, kit.root());\n-    sfpt->add_req(call->in(TypeFunc::Parms));\n-    sobj = gvn.transform(sobj);\n-    JVMState* jvms = sfpt->jvms();\n-    jvms->set_endoff(sfpt->req());\n-    int start = jvms->debug_start();\n-    int end   = jvms->debug_end();\n-    int num_edges = sfpt->replace_edges_in_range(res, sobj, start, end, &gvn);\n-    i -= num_edges;\n-  }\n-\n-  assert(res->outcnt() == 0, \"the box must have no use after replace\");\n-\n-#ifndef PRODUCT\n-  if (PrintEliminateAllocations) {\n-    tty->print(\"++++ Eliminated: %d \", call->_idx);\n-    call->as_CallStaticJava()->method()->print_short_name(tty);\n-    tty->cr();\n-  }\n-#endif\n-}\n-\n@@ -716,1 +659,0 @@\n-  bool result_not_used = false;\n@@ -718,21 +660,8 @@\n-  if (is_pure_call()) {\n-    \/\/ Disabled due to JDK-8276112\n-    if (false && is_boxing_late_inline() && callprojs->resproj[0] != nullptr) {\n-        \/\/ replace box node to scalar node only in case it is directly referenced by debug info\n-        assert(call->as_CallStaticJava()->is_boxing_method(), \"sanity\");\n-        if (!has_non_debug_usages(callprojs->resproj[0]) && is_box_cache_valid(call)) {\n-          scalarize_debug_usages(call, callprojs->resproj[0]);\n-        }\n-    }\n-\n-    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n-    \/\/ It's safe to remove the call.\n-    result_not_used = true;\n-    for (uint i = 0; i < callprojs->nb_resproj; i++) {\n-      if (callprojs->resproj[i] != NULL) {\n-        if (callprojs->resproj[i]->outcnt() != 0) {\n-          result_not_used = false;\n-        }\n-        if (call->find_edge(callprojs->resproj[i]) != -1) {\n-          return;\n-        }\n+  bool result_not_used = true;\n+  for (uint i = 0; i < callprojs->nb_resproj; i++) {\n+    if (callprojs->resproj[i] != NULL) {\n+      if (callprojs->resproj[i]->outcnt() != 0) {\n+        result_not_used = false;\n+      }\n+      if (call->find_edge(callprojs->resproj[i]) != -1) {\n+        return;\n@@ -743,1 +672,3 @@\n-  if (result_not_used) {\n+  if (is_pure_call() && result_not_used) {\n+    \/\/ The call is marked as pure (no important side effects), but result isn't used.\n+    \/\/ It's safe to remove the call.\n@@ -962,2 +893,0 @@\n-  virtual bool is_boxing_late_inline() const { return true; }\n-\n@@ -1243,25 +1172,0 @@\n-class NativeCallGenerator : public CallGenerator {\n-private:\n-  address _call_addr;\n-  ciNativeEntryPoint* _nep;\n-public:\n-  NativeCallGenerator(ciMethod* m, address call_addr, ciNativeEntryPoint* nep)\n-   : CallGenerator(m), _call_addr(call_addr), _nep(nep) {}\n-\n-  virtual JVMState* generate(JVMState* jvms);\n-};\n-\n-JVMState* NativeCallGenerator::generate(JVMState* jvms) {\n-  GraphKit kit(jvms);\n-\n-  Node* call = kit.make_native_call(_call_addr, tf(), method()->arg_size(), _nep); \/\/ -fallback, - nep\n-  if (call == NULL) return NULL;\n-\n-  kit.C->print_inlining_update(this);\n-  if (kit.C->log() != NULL) {\n-    kit.C->log()->elem(\"l2n_intrinsification_success bci='%d' entry_point='\" INTPTR_FORMAT \"'\", jvms->bci(), p2i(_call_addr));\n-  }\n-\n-  return kit.transfer_exceptions_into_jvms();\n-}\n-\n@@ -1284,3 +1188,10 @@\n-        const TypeOopPtr* oop_ptr = receiver->bottom_type()->is_oopptr();\n-        ciMethod* target = oop_ptr->const_oop()->as_method_handle()->get_vmtarget();\n-        const int vtable_index = Method::invalid_vtable_index;\n+        const TypeOopPtr* recv_toop = receiver->bottom_type()->isa_oopptr();\n+        if (recv_toop != NULL) {\n+          ciMethod* target = recv_toop->const_oop()->as_method_handle()->get_vmtarget();\n+          const int vtable_index = Method::invalid_vtable_index;\n+\n+          if (!ciMethod::is_consistent_info(callee, target)) {\n+            print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n+                                   \"signatures mismatch\");\n+            return NULL;\n+          }\n@@ -1288,1 +1199,9 @@\n-        if (!ciMethod::is_consistent_info(callee, target)) {\n+          CallGenerator *cg = C->call_generator(target, vtable_index,\n+                                                false \/* call_does_dispatch *\/,\n+                                                jvms,\n+                                                allow_inline,\n+                                                PROB_ALWAYS);\n+          return cg;\n+        } else {\n+          assert(receiver->bottom_type() == TypePtr::NULL_PTR, \"not a null: %s\",\n+                 Type::str(receiver->bottom_type()));\n@@ -1290,2 +1209,1 @@\n-                                 \"signatures mismatch\");\n-          return NULL;\n+                                 \"receiver is always null\");\n@@ -1293,9 +1211,0 @@\n-\n-        CallGenerator* cg = C->call_generator(target, vtable_index,\n-                                              false \/* call_does_dispatch *\/,\n-                                              jvms,\n-                                              allow_inline,\n-                                              PROB_ALWAYS,\n-                                              NULL,\n-                                              true);\n-        return cg;\n@@ -1385,16 +1294,2 @@\n-    {\n-      Node* addr_n = kit.argument(1); \/\/ target address\n-      Node* nep_n = kit.argument(callee->arg_size() - 1); \/\/ NativeEntryPoint\n-      \/\/ This check needs to be kept in sync with the one in CallStaticJavaNode::Ideal\n-      if (addr_n->Opcode() == Op_ConL && nep_n->Opcode() == Op_ConP) {\n-        input_not_const = false;\n-        const TypeLong* addr_t = addr_n->bottom_type()->is_long();\n-        const TypeOopPtr* nep_t = nep_n->bottom_type()->is_oopptr();\n-        address addr = (address) addr_t->get_con();\n-        ciNativeEntryPoint* nep = nep_t->const_oop()->as_native_entry_point();\n-        return new NativeCallGenerator(callee, addr, nep);\n-      } else {\n-        print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n-                               \"NativeEntryPoint not constant\");\n-      }\n-    }\n+    print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n+                           \"native call\");\n@@ -1410,1 +1305,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":35,"deletions":141,"binary":false,"changes":176,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -76,1 +76,0 @@\n-  virtual bool      is_boxing_late_inline() const  { return false; }\n","filename":"src\/hotspot\/share\/opto\/callGenerator.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -187,2 +187,2 @@\n-void ReturnNode::dump_req(outputStream *st) const {\n-  \/\/ Dump the required inputs, enclosed in '(' and ')'\n+void ReturnNode::dump_req(outputStream *st, DumpConfig* dc) const {\n+  \/\/ Dump the required inputs, after printing \"returns\"\n@@ -191,3 +191,8 @@\n-    if (i == TypeFunc::Parms) st->print(\"returns\");\n-    if (in(i)) st->print(\"%c%d \", Compile::current()->node_arena()->contains(in(i)) ? ' ' : 'o', in(i)->_idx);\n-    else st->print(\"_ \");\n+    if (i == TypeFunc::Parms) st->print(\"returns \");\n+    Node* p = in(i);\n+    if (p != nullptr) {\n+      p->dump_idx(false, st, dc);\n+      st->print(\" \");\n+    } else {\n+      st->print(\"_ \");\n+    }\n@@ -230,2 +235,2 @@\n-void RethrowNode::dump_req(outputStream *st) const {\n-  \/\/ Dump the required inputs, enclosed in '(' and ')'\n+void RethrowNode::dump_req(outputStream *st, DumpConfig* dc) const {\n+  \/\/ Dump the required inputs, after printing \"exception\"\n@@ -234,3 +239,8 @@\n-    if (i == TypeFunc::Parms) st->print(\"exception\");\n-    if (in(i)) st->print(\"%c%d \", Compile::current()->node_arena()->contains(in(i)) ? ' ' : 'o', in(i)->_idx);\n-    else st->print(\"_ \");\n+    if (i == TypeFunc::Parms) st->print(\"exception \");\n+    Node* p = in(i);\n+    if (p != nullptr) {\n+      p->dump_idx(false, st, dc);\n+      st->print(\" \");\n+    } else {\n+      st->print(\"_ \");\n+    }\n@@ -375,1 +385,1 @@\n-      st->print(\" %s%d]=#Ptr\" INTPTR_FORMAT,msg,i,p2i(t->make_ptr()->isa_klassptr()->klass()));\n+      st->print(\" %s%d]=#Ptr\" INTPTR_FORMAT,msg,i,p2i(t->make_ptr()->isa_klassptr()->exact_klass()));\n@@ -466,1 +476,1 @@\n-      ciKlass* cik = spobj->bottom_type()->is_oopptr()->klass();\n+      ciKlass* cik = spobj->bottom_type()->is_oopptr()->exact_klass();\n@@ -699,1 +709,1 @@\n-void CallNode::dump_req(outputStream *st) const {\n+void CallNode::dump_req(outputStream *st, DumpConfig* dc) const {\n@@ -704,2 +714,7 @@\n-    if (in(i)) st->print(\"%c%d \", Compile::current()->node_arena()->contains(in(i)) ? ' ' : 'o', in(i)->_idx);\n-    else st->print(\"_ \");\n+    Node* p = in(i);\n+    if (p != nullptr) {\n+      p->dump_idx(false, st, dc);\n+      st->print(\" \");\n+    } else {\n+      st->print(\"_ \");\n+    }\n@@ -836,1 +851,1 @@\n-    ciKlass* boxing_klass = t_oop->klass();\n+    ciKlass* boxing_klass = t_oop->is_instptr()->instance_klass();\n@@ -840,1 +855,1 @@\n-      if ((proj == NULL) || (phase->type(proj)->is_instptr()->klass() != boxing_klass)) {\n+      if ((proj == NULL) || (phase->type(proj)->is_instptr()->instance_klass() != boxing_klass)) {\n@@ -855,1 +870,1 @@\n-                                 (inst_t->klass() == boxing_klass))) {\n+                                 (inst_t->instance_klass() == boxing_klass))) {\n@@ -863,1 +878,1 @@\n-                                 (inst_t->klass() == boxing_klass))) {\n+                                 (inst_t->instance_klass() == boxing_klass))) {\n@@ -1144,5 +1159,1 @@\n-      if (in(TypeFunc::Parms + callee->arg_size() - 1)->Opcode() == Op_ConP \/* NEP *\/\n-          && in(TypeFunc::Parms + 1)->Opcode() == Op_ConL \/* address *\/) {\n-        phase->C->prepend_late_inline(cg);\n-        set_generator(NULL);\n-      }\n+      \/\/ never retry\n@@ -1408,65 +1419,0 @@\n-\/\/=============================================================================\n-uint CallNativeNode::size_of() const { return sizeof(*this); }\n-bool CallNativeNode::cmp( const Node &n ) const {\n-  CallNativeNode &call = (CallNativeNode&)n;\n-  return CallNode::cmp(call) && !strcmp(_name,call._name)\n-    && _arg_regs == call._arg_regs && _ret_regs == call._ret_regs;\n-}\n-Node* CallNativeNode::match(const ProjNode *proj, const Matcher *matcher, const RegMask* mask) {\n-  switch (proj->_con) {\n-    case TypeFunc::Control:\n-    case TypeFunc::I_O:\n-    case TypeFunc::Memory:\n-      return new MachProjNode(this,proj->_con,RegMask::Empty,MachProjNode::unmatched_proj);\n-    case TypeFunc::ReturnAdr:\n-    case TypeFunc::FramePtr:\n-      ShouldNotReachHere();\n-    case TypeFunc::Parms: {\n-      const Type* field_at_con = tf()->range_sig()->field_at(proj->_con);\n-      const BasicType bt = field_at_con->basic_type();\n-      OptoReg::Name optoreg = OptoReg::as_OptoReg(_ret_regs.at(proj->_con - TypeFunc::Parms));\n-      OptoRegPair regs;\n-      if (bt == T_DOUBLE || bt == T_LONG) {\n-        regs.set2(optoreg);\n-      } else {\n-        regs.set1(optoreg);\n-      }\n-      RegMask rm = RegMask(regs.first());\n-      if(OptoReg::is_valid(regs.second()))\n-        rm.Insert(regs.second());\n-      return new MachProjNode(this, proj->_con, rm, field_at_con->ideal_reg());\n-    }\n-    case TypeFunc::Parms + 1: {\n-      assert(tf()->range_sig()->field_at(proj->_con) == Type::HALF, \"Expected HALF\");\n-      assert(_ret_regs.at(proj->_con - TypeFunc::Parms) == VMRegImpl::Bad(), \"Unexpected register for Type::HALF\");\n-      \/\/ 2nd half of doubles and longs\n-      return new MachProjNode(this, proj->_con, RegMask::Empty, (uint) OptoReg::Bad);\n-    }\n-    default:\n-      ShouldNotReachHere();\n-  }\n-  return NULL;\n-}\n-#ifndef PRODUCT\n-void CallNativeNode::print_regs(const GrowableArray<VMReg>& regs, outputStream* st) {\n-  st->print(\"{ \");\n-  for (int i = 0; i < regs.length(); i++) {\n-    regs.at(i)->print_on(st);\n-    if (i < regs.length() - 1) {\n-      st->print(\", \");\n-    }\n-  }\n-  st->print(\" } \");\n-}\n-\n-void CallNativeNode::dump_spec(outputStream *st) const {\n-  st->print(\"# \");\n-  st->print(\"%s \", _name);\n-  st->print(\"_arg_regs: \");\n-  print_regs(_arg_regs, st);\n-  st->print(\"_ret_regs: \");\n-  print_regs(_ret_regs, st);\n-  CallNode::dump_spec(st);\n-}\n-#endif\n-\n@@ -1500,34 +1446,0 @@\n-void CallNativeNode::calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const {\n-  assert((tf()->domain_sig()->cnt() - TypeFunc::Parms) == argcnt, \"arg counts must match!\");\n-#ifdef ASSERT\n-  for (uint i = 0; i < argcnt; i++) {\n-    assert(tf()->domain_sig()->field_at(TypeFunc::Parms + i)->basic_type() == sig_bt[i], \"types must match!\");\n-  }\n-#endif\n-  for (uint i = 0; i < argcnt; i++) {\n-    switch (sig_bt[i]) {\n-      case T_BOOLEAN:\n-      case T_CHAR:\n-      case T_BYTE:\n-      case T_SHORT:\n-      case T_INT:\n-      case T_FLOAT:\n-        parm_regs[i].set1(_arg_regs.at(i));\n-        break;\n-      case T_LONG:\n-      case T_DOUBLE:\n-        assert((i + 1) < argcnt && sig_bt[i + 1] == T_VOID, \"expecting half\");\n-        parm_regs[i].set2(_arg_regs.at(i));\n-        break;\n-      case T_VOID: \/\/ Halves of longs and doubles\n-        assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n-        assert(_arg_regs.at(i) == VMRegImpl::Bad(), \"expecting bad reg\");\n-        parm_regs[i].set_bad();\n-        break;\n-      default:\n-        ShouldNotReachHere();\n-        break;\n-    }\n-  }\n-}\n-\n@@ -1561,1 +1473,1 @@\n-    \/\/ represent the 2nd half ofthe long\/double.\n+    \/\/ represent the 2nd half of the long\/double.\n@@ -1758,0 +1670,6 @@\n+Node* SafePointNode::peek_operand(uint off) const {\n+  assert(jvms()->sp() > 0, \"must have an operand\");\n+  assert(off < jvms()->sp(), \"off is out-of-range\");\n+  return stack(jvms(), jvms()->sp() - off - 1);\n+}\n+\n@@ -1778,2 +1696,1 @@\n-                                                     uint n_fields,\n-                                                     bool is_auto_box) :\n+                                                     uint n_fields) :\n@@ -1782,2 +1699,1 @@\n-  _n_fields(n_fields),\n-  _is_auto_box(is_auto_box)\n+  _n_fields(n_fields)\n@@ -1790,2 +1706,1 @@\n-      && !(alloc->Opcode() == Op_VectorBox)\n-      && (!alloc->is_CallStaticJava() || !alloc->as_CallStaticJava()->is_boxing_method())) {\n+      && !(alloc->Opcode() == Op_VectorBox)) {\n@@ -1931,1 +1846,3 @@\n-      if (!allow_new_nodes) return NULL;\n+      if (!allow_new_nodes) {\n+        return NULL;\n+      }\n@@ -1935,3 +1852,4 @@\n-      assert(init != NULL, \"initialization not found\");\n-      length = new CastIINode(length, narrow_length_type);\n-      length->set_req(TypeFunc::Control, init->proj_out_or_null(TypeFunc::Control));\n+      if (init != NULL) {\n+        length = new CastIINode(length, narrow_length_type);\n+        length->set_req(TypeFunc::Control, init->proj_out_or_null(TypeFunc::Control));\n+      }\n@@ -1979,1 +1897,1 @@\n-\/\/ Addtionally we can eliminate versions without the else case:\n+\/\/ Additionally we can eliminate versions without the else case:\n@@ -2540,1 +2458,1 @@\n-  if (dest_t->isa_instptr() && !dest_t->klass()->equals(phase->C->env()->Object_klass())) {\n+  if (dest_t->isa_instptr() && !dest_t->is_instptr()->instance_klass()->equals(phase->C->env()->Object_klass())) {\n@@ -2548,1 +2466,1 @@\n-    if (dest_t->klass()->is_subtype_of(t_oop->klass()) || t_oop->klass()->is_subtype_of(dest_t->klass())) {\n+    if (dest_t->maybe_java_subtype_of(t_oop) || t_oop->maybe_java_subtype_of(dest_t)) {\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":55,"deletions":137,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,1 +52,0 @@\n-class     CallNativeNode;\n@@ -130,1 +129,1 @@\n-  virtual void dump_req(outputStream *st = tty) const;\n+  virtual void dump_req(outputStream *st = tty, DumpConfig* dc = nullptr) const;\n@@ -151,1 +150,1 @@\n-  virtual void dump_req(outputStream *st = tty) const;\n+  virtual void dump_req(outputStream *st = tty, DumpConfig* dc = nullptr) const;\n@@ -207,1 +206,1 @@\n-  uint              _sp;        \/\/ Jave Expression Stack Pointer for this state\n+  uint              _sp;        \/\/ Java Expression Stack Pointer for this state\n@@ -419,0 +418,2 @@\n+  \/\/ Peek Operand Stacks, JVMS 2.6.2\n+  Node* peek_operand(uint off = 0) const;\n@@ -514,1 +515,0 @@\n-  bool _is_auto_box; \/\/ True if the scalarized object is an auto box.\n@@ -527,1 +527,1 @@\n-                            uint first_index, uint n_fields, bool is_auto_box = false);\n+                            uint first_index, uint n_fields);\n@@ -540,1 +540,0 @@\n-  bool is_auto_box() const { return _is_auto_box; }\n@@ -680,1 +679,1 @@\n-  virtual void        dump_req(outputStream* st = tty) const;\n+  virtual void        dump_req(outputStream* st = tty, DumpConfig* dc = nullptr) const;\n@@ -860,36 +859,0 @@\n-\/\/------------------------------CallNativeNode-----------------------------------\n-\/\/ Make a direct call into a foreign function with an arbitrary ABI\n-\/\/ safepoints\n-class CallNativeNode : public CallNode {\n-  friend class MachCallNativeNode;\n-  virtual bool cmp( const Node &n ) const;\n-  virtual uint size_of() const;\n-  static void print_regs(const GrowableArray<VMReg>& regs, outputStream* st);\n-public:\n-  GrowableArray<VMReg> _arg_regs;\n-  GrowableArray<VMReg> _ret_regs;\n-  const int _shadow_space_bytes;\n-  const bool _need_transition;\n-\n-  CallNativeNode(const TypeFunc* tf, address addr, const char* name,\n-                 const TypePtr* adr_type,\n-                 const GrowableArray<VMReg>& arg_regs,\n-                 const GrowableArray<VMReg>& ret_regs,\n-                 int shadow_space_bytes,\n-                 bool need_transition)\n-    : CallNode(tf, addr, adr_type), _arg_regs(arg_regs),\n-      _ret_regs(ret_regs), _shadow_space_bytes(shadow_space_bytes),\n-      _need_transition(need_transition)\n-  {\n-    init_class_id(Class_CallNative);\n-    _name = name;\n-  }\n-  virtual int   Opcode() const;\n-  virtual bool  guaranteed_safepoint()  { return _need_transition; }\n-  virtual Node* match(const ProjNode *proj, const Matcher *m, const RegMask* mask);\n-  virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;\n-#ifndef PRODUCT\n-  virtual void  dump_spec(outputStream *st) const;\n-#endif\n-};\n-\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":8,"deletions":45,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"castnode.hpp\"\n@@ -148,1 +149,1 @@\n-Node* ConstraintCastNode::make(Node* c, Node *n, const Type *t, BasicType bt) {\n+Node* ConstraintCastNode::make(Node* c, Node *n, const Type *t, DependencyType dependency, BasicType bt) {\n@@ -151,1 +152,1 @@\n-    return make_cast(Op_CastII, c, n, t, RegularDependency);\n+    return make_cast(Op_CastII, c, n, t, dependency);\n@@ -154,1 +155,1 @@\n-    return make_cast(Op_CastLL, c, n, t, RegularDependency);\n+    return make_cast(Op_CastLL, c, n, t, dependency);\n@@ -225,22 +226,2 @@\n-  if (!_range_check_dependency && phase->C->post_loop_opts_phase()) {\n-    const TypeInt* this_type = res->is_int();\n-    const TypeInt* in_type = phase->type(in(1))->isa_int();\n-    if (in_type != NULL &&\n-        (in_type->_lo != this_type->_lo ||\n-         in_type->_hi != this_type->_hi)) {\n-      jint lo1 = this_type->_lo;\n-      jint hi1 = this_type->_hi;\n-      int w1 = this_type->_widen;\n-      if (lo1 >= 0) {\n-        \/\/ Keep a range assertion of >=0.\n-        lo1 = 0;        hi1 = max_jint;\n-      } else if (hi1 < 0) {\n-        \/\/ Keep a range assertion of <0.\n-        lo1 = min_jint; hi1 = -1;\n-      } else {\n-        lo1 = min_jint; hi1 = max_jint;\n-      }\n-      res = TypeInt::make(MAX2(in_type->_lo, lo1),\n-                          MIN2(in_type->_hi, hi1),\n-                          MAX2((int)in_type->_widen, w1));\n-    }\n+  if (!_range_check_dependency) {\n+    res = widen_type(phase, res, T_INT);\n@@ -311,3 +292,2 @@\n-static Node* find_or_make_CastII(PhaseIterGVN* igvn, Node* parent, Node* control, const TypeInt* type, ConstraintCastNode::DependencyType dependency) {\n-  Node* n = new CastIINode(parent, type, dependency);\n-  n->set_req(0, control);\n+static Node* find_or_make_integer_cast(PhaseIterGVN* igvn, Node* parent, Node* control, const TypeInteger* type, ConstraintCastNode::DependencyType dependency, BasicType bt) {\n+  Node* n = ConstraintCastNode::make(control, parent, type, dependency, bt);\n@@ -331,24 +311,2 @@\n-  PhaseIterGVN* igvn = phase->is_IterGVN();\n-  const TypeInt* this_type = this->type()->is_int();\n-  Node* z = in(1);\n-  const TypeInteger* rx = NULL;\n-  const TypeInteger* ry = NULL;\n-  \/\/ Similar to ConvI2LNode::Ideal() for the same reasons\n-  if (!_range_check_dependency && Compile::push_thru_add(phase, z, this_type, rx, ry, T_INT)) {\n-    if (igvn == NULL) {\n-      \/\/ Postpone this optimization to iterative GVN, where we can handle deep\n-      \/\/ AddI chains without an exponential number of recursive Ideal() calls.\n-      phase->record_for_igvn(this);\n-      return NULL;\n-    }\n-    int op = z->Opcode();\n-    Node* x = z->in(1);\n-    Node* y = z->in(2);\n-\n-    Node* cx = find_or_make_CastII(igvn, x, in(0), rx->is_int(), _dependency);\n-    Node* cy = find_or_make_CastII(igvn, y, in(0), ry->is_int(), _dependency);\n-    switch (op) {\n-      case Op_AddI:  return new AddINode(cx, cy);\n-      case Op_SubI:  return new SubINode(cx, cy);\n-      default:       ShouldNotReachHere();\n-    }\n+  if (!_range_check_dependency) {\n+    return optimize_integer_cast(phase, T_INT);\n@@ -391,0 +349,42 @@\n+const Type* CastLLNode::Value(PhaseGVN* phase) const {\n+  const Type* res = ConstraintCastNode::Value(phase);\n+  if (res == Type::TOP) {\n+    return Type::TOP;\n+  }\n+  assert(res->isa_long(), \"res must be long\");\n+\n+  return widen_type(phase, res, T_LONG);\n+}\n+\n+Node* CastLLNode::Ideal(PhaseGVN* phase, bool can_reshape) {\n+  Node* progress = ConstraintCastNode::Ideal(phase, can_reshape);\n+  if (progress != NULL) {\n+    return progress;\n+  }\n+  if (can_reshape && !phase->C->post_loop_opts_phase()) {\n+    \/\/ makes sure we run ::Value to potentially remove type assertion after loop opts\n+    phase->C->record_for_post_loop_opts_igvn(this);\n+  }\n+  \/\/ transform (CastLL (ConvI2L ..)) into (ConvI2L (CastII ..)) if the type of the CastLL is narrower than the type of\n+  \/\/ the ConvI2L.\n+  Node* in1 = in(1);\n+  if (in1 != NULL && in1->Opcode() == Op_ConvI2L) {\n+    const Type* t = Value(phase);\n+    const Type* t_in = phase->type(in1);\n+    if (t != Type::TOP && t_in != Type::TOP) {\n+      const TypeLong* tl = t->is_long();\n+      const TypeLong* t_in_l = t_in->is_long();\n+      assert(tl->_lo >= t_in_l->_lo && tl->_hi <= t_in_l->_hi, \"CastLL type should be narrower than or equal to the type of its input\");\n+      assert((tl != t_in_l) == (tl->_lo > t_in_l->_lo || tl->_hi < t_in_l->_hi), \"if type differs then this nodes's type must be narrower\");\n+      if (tl != t_in_l) {\n+        const TypeInt* ti = TypeInt::make(checked_cast<jint>(tl->_lo), checked_cast<jint>(tl->_hi), tl->_widen);\n+        Node* castii = phase->transform(new CastIINode(in(0), in1->in(1), ti));\n+        Node* convi2l = in1->clone();\n+        convi2l->set_req(1, castii);\n+        return convi2l;\n+      }\n+    }\n+  }\n+  return optimize_integer_cast(phase, T_LONG);\n+}\n+\n@@ -395,1 +395,1 @@\n-  if (in(1)->is_InlineTypeBase() && _type->isa_oopptr() && phase->type(in(1))->inline_klass()->is_subtype_of(_type->is_oopptr()->klass())) {\n+  if (in(1)->is_InlineTypeBase() && _type->isa_instptr() && phase->type(in(1))->inline_klass()->is_subtype_of(_type->is_instptr()->instance_klass())) {\n@@ -640,0 +640,59 @@\n+\n+Node* ConstraintCastNode::optimize_integer_cast(PhaseGVN* phase, BasicType bt) {\n+  PhaseIterGVN *igvn = phase->is_IterGVN();\n+  const TypeInteger* this_type = this->type()->is_integer(bt);\n+  Node* z = in(1);\n+  const TypeInteger* rx = NULL;\n+  const TypeInteger* ry = NULL;\n+  \/\/ Similar to ConvI2LNode::Ideal() for the same reasons\n+  if (Compile::push_thru_add(phase, z, this_type, rx, ry, bt, bt)) {\n+    if (igvn == NULL) {\n+      \/\/ Postpone this optimization to iterative GVN, where we can handle deep\n+      \/\/ AddI chains without an exponential number of recursive Ideal() calls.\n+      phase->record_for_igvn(this);\n+      return NULL;\n+    }\n+    int op = z->Opcode();\n+    Node* x = z->in(1);\n+    Node* y = z->in(2);\n+\n+    Node* cx = find_or_make_integer_cast(igvn, x, in(0), rx, _dependency, bt);\n+    Node* cy = find_or_make_integer_cast(igvn, y, in(0), ry, _dependency, bt);\n+    if (op == Op_Add(bt)) {\n+      return AddNode::make(cx, cy, bt);\n+    } else {\n+      assert(op == Op_Sub(bt), \"\");\n+      return SubNode::make(cx, cy, bt);\n+    }\n+    return NULL;\n+  }\n+  return NULL;\n+}\n+\n+const Type* ConstraintCastNode::widen_type(const PhaseGVN* phase, const Type* res, BasicType bt) const {\n+  if (!phase->C->post_loop_opts_phase()) {\n+    return res;\n+  }\n+  const TypeInteger* this_type = res->is_integer(bt);\n+  const TypeInteger* in_type = phase->type(in(1))->isa_integer(bt);\n+  if (in_type != NULL &&\n+      (in_type->lo_as_long() != this_type->lo_as_long() ||\n+       in_type->hi_as_long() != this_type->hi_as_long())) {\n+    jlong lo1 = this_type->lo_as_long();\n+    jlong hi1 = this_type->hi_as_long();\n+    int w1 = this_type->_widen;\n+    if (lo1 >= 0) {\n+      \/\/ Keep a range assertion of >=0.\n+      lo1 = 0;        hi1 = max_signed_integer(bt);\n+    } else if (hi1 < 0) {\n+      \/\/ Keep a range assertion of <0.\n+      lo1 = min_signed_integer(bt); hi1 = -1;\n+    } else {\n+      lo1 = min_signed_integer(bt); hi1 = max_signed_integer(bt);\n+    }\n+    return TypeInteger::make(MAX2(in_type->lo_as_long(), lo1),\n+                             MIN2(in_type->hi_as_long(), hi1),\n+                             MAX2((int)in_type->_widen, w1), bt);\n+  }\n+  return res;\n+}\n","filename":"src\/hotspot\/share\/opto\/castnode.cpp","additions":112,"deletions":53,"binary":false,"changes":165,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+  const Type* widen_type(const PhaseGVN* phase, const Type* res, BasicType bt) const;\n@@ -62,1 +63,1 @@\n-  static Node* make(Node* c, Node *n, const Type *t, BasicType bt);\n+  static Node* make(Node* c, Node *n, const Type *t, DependencyType dependency, BasicType bt);\n@@ -69,0 +70,2 @@\n+\n+  Node* optimize_integer_cast(PhaseGVN* phase, BasicType bt);\n@@ -121,0 +124,2 @@\n+  virtual const Type* Value(PhaseGVN* phase) const;\n+  virtual Node* Ideal(PhaseGVN* phase, bool can_reshape);\n","filename":"src\/hotspot\/share\/opto\/castnode.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -318,1 +318,1 @@\n-\/\/ Find if the Region node is reachable from the root.\n+\/\/ Check if the RegionNode is part of an unsafe loop and unreachable from root.\n@@ -377,1 +377,1 @@\n-  Node *n = (Node*)phase->C->root();\n+  Node* n = (Node*)phase->C->root();\n@@ -485,1 +485,1 @@\n-  bool add_to_worklist = false;\n+  bool add_to_worklist = true;\n@@ -511,1 +511,1 @@\n-        set_req(i, NULL);       \/\/ Ignore TOP inputs\n+        set_req_X(i, NULL, phase); \/\/ Ignore TOP inputs\n@@ -542,1 +542,2 @@\n-      add_to_worklist = true;\n+      add_to_worklist = false;\n+      phase->is_IterGVN()->add_users_to_worklist(this);\n@@ -557,29 +558,37 @@\n-      \/\/ Yes,  the region will be removed during the next step below.\n-      \/\/ Cut the backedge input and remove phis since no data paths left.\n-      \/\/ We don't cut outputs to other nodes here since we need to put them\n-      \/\/ on the worklist.\n-      PhaseIterGVN *igvn = phase->is_IterGVN();\n-      if (in(1)->outcnt() == 1) {\n-        igvn->_worklist.push(in(1));\n-      }\n-      del_req(1);\n-      cnt = 0;\n-      assert( req() == 1, \"no more inputs expected\" );\n-      uint max = outcnt();\n-      bool progress = true;\n-      Node *top = phase->C->top();\n-      DUIterator j;\n-      while(progress) {\n-        progress = false;\n-        for (j = outs(); has_out(j); j++) {\n-          Node *n = out(j);\n-          if( n->is_Phi() ) {\n-            assert(n->in(0) == this, \"\");\n-            assert( n->req() == 2 &&  n->in(1) != NULL, \"Only one data input expected\" );\n-            \/\/ Break dead loop data path.\n-            \/\/ Eagerly replace phis with top to avoid regionless phis.\n-            igvn->replace_node(n, top);\n-            if( max != outcnt() ) {\n-              progress = true;\n-              j = refresh_out_pos(j);\n-              max = outcnt();\n+      \/\/ This region and therefore all nodes on the input control path(s) are unreachable\n+      \/\/ from root. To avoid incomplete removal of unreachable subgraphs, walk up the CFG\n+      \/\/ and aggressively replace all nodes by top.\n+      PhaseIterGVN* igvn = phase->is_IterGVN();\n+      Node* top = phase->C->top();\n+      ResourceMark rm;\n+      Node_List nstack;\n+      VectorSet visited;\n+      nstack.push(this);\n+      visited.set(_idx);\n+      while (nstack.size() != 0) {\n+        Node* n = nstack.pop();\n+        for (uint i = 0; i < n->req(); ++i) {\n+          Node* m = n->in(i);\n+          assert(m != (Node*)phase->C->root(), \"Should be unreachable from root\");\n+          if (m != NULL && m->is_CFG() && !visited.test_set(m->_idx)) {\n+            nstack.push(m);\n+          }\n+        }\n+        if (n->is_Region()) {\n+          \/\/ Eagerly replace phis with top to avoid regionless phis.\n+          n->set_req(0, NULL);\n+          bool progress = true;\n+          uint max = n->outcnt();\n+          DUIterator j;\n+          while (progress) {\n+            progress = false;\n+            for (j = n->outs(); n->has_out(j); j++) {\n+              Node* u = n->out(j);\n+              if (u->is_Phi()) {\n+                igvn->replace_node(u, top);\n+                if (max != n->outcnt()) {\n+                  progress = true;\n+                  j = n->refresh_out_pos(j);\n+                  max = n->outcnt();\n+                }\n+              }\n@@ -589,0 +598,1 @@\n+        igvn->replace_node(n, top);\n@@ -590,1 +600,1 @@\n-      add_to_worklist = true;\n+      return NULL;\n@@ -593,3 +603,0 @@\n-  if (add_to_worklist) {\n-    phase->is_IterGVN()->add_users_to_worklist(this); \/\/ Revisit collapsed Phis\n-  }\n@@ -639,1 +646,1 @@\n-      if (!add_to_worklist)\n+      if (add_to_worklist) {\n@@ -641,0 +648,1 @@\n+      }\n@@ -1127,1 +1135,1 @@\n-          assert(stride_t->hi_as_long() >= stride_t->lo_as_long(), \"bad stride type\");\n+          assert(stride_t->is_con(), \"bad stride type\");\n@@ -1134,1 +1142,2 @@\n-            if (stride_t->hi_as_long() < 0) {          \/\/ Down-counter loop\n+            jlong stride_con = stride_t->get_con_as_long(l->bt());\n+            if (stride_con < 0) {          \/\/ Down-counter loop\n@@ -1136,3 +1145,39 @@\n-              return TypeInteger::make(MIN2(lo->lo_as_long(), hi->lo_as_long()), hi->hi_as_long(), 3, l->bt())->filter_speculative(_type);\n-            } else if (stride_t->lo_as_long() >= 0) {\n-              return TypeInteger::make(lo->lo_as_long(), MAX2(lo->hi_as_long(), hi->hi_as_long()), 3, l->bt())->filter_speculative(_type);\n+              jlong iv_range_lower_limit = lo->lo_as_long();\n+              \/\/ Prevent overflow when adding one below\n+              if (iv_range_lower_limit < max_signed_integer(l->bt())) {\n+                \/\/ The loop exit condition is: iv + stride > limit (iv is this Phi). So the loop iterates until\n+                \/\/ iv + stride <= limit\n+                \/\/ We know that: limit >= lo->lo_as_long() and stride <= -1\n+                \/\/ So when the loop exits, iv has to be at most lo->lo_as_long() + 1\n+                iv_range_lower_limit += 1; \/\/ lo is after decrement\n+                \/\/ Exact bounds for the phi can be computed when ABS(stride) greater than 1 if bounds are constant.\n+                if (lo->is_con() && hi->is_con() && hi->lo_as_long() > lo->hi_as_long() && stride_con != -1) {\n+                  julong uhi = static_cast<julong>(hi->lo_as_long());\n+                  julong ulo = static_cast<julong>(lo->hi_as_long());\n+                  julong diff = ((uhi - ulo - 1) \/ (-stride_con)) * (-stride_con);\n+                  julong ufirst = hi->lo_as_long() - diff;\n+                  iv_range_lower_limit = reinterpret_cast<jlong &>(ufirst);\n+                  assert(iv_range_lower_limit >= lo->lo_as_long() + 1, \"should end up with narrower range\");\n+                }\n+              }\n+              return TypeInteger::make(MIN2(iv_range_lower_limit, hi->lo_as_long()), hi->hi_as_long(), 3, l->bt())->filter_speculative(_type);\n+            } else if (stride_con >= 0) {\n+              jlong iv_range_upper_limit = hi->hi_as_long();\n+              \/\/ Prevent overflow when subtracting one below\n+              if (iv_range_upper_limit > min_signed_integer(l->bt())) {\n+                \/\/ The loop exit condition is: iv + stride < limit (iv is this Phi). So the loop iterates until\n+                \/\/ iv + stride >= limit\n+                \/\/ We know that: limit <= hi->hi_as_long() and stride >= 1\n+                \/\/ So when the loop exits, iv has to be at most hi->hi_as_long() - 1\n+                iv_range_upper_limit -= 1;\n+                \/\/ Exact bounds for the phi can be computed when ABS(stride) greater than 1 if bounds are constant.\n+                if (lo->is_con() && hi->is_con() && hi->lo_as_long() > lo->hi_as_long() && stride_con != 1) {\n+                  julong uhi = static_cast<julong>(hi->lo_as_long());\n+                  julong ulo = static_cast<julong>(lo->hi_as_long());\n+                  julong diff = ((uhi - ulo - 1) \/ stride_con) * stride_con;\n+                  julong ulast = lo->hi_as_long() + diff;\n+                  iv_range_upper_limit = reinterpret_cast<jlong &>(ulast);\n+                  assert(iv_range_upper_limit <= hi->hi_as_long() - 1, \"should end up with narrower range\");\n+                }\n+              }\n+              return TypeInteger::make(lo->lo_as_long(), MAX2(lo->hi_as_long(), iv_range_upper_limit), 3, l->bt())->filter_speculative(_type);\n@@ -1159,1 +1204,1 @@\n-  const TypeKlassPtr* ttkp = (ttp != NULL) ? ttp->isa_instklassptr() : NULL;\n+  const TypeInstKlassPtr* ttkp = (ttp != NULL) ? ttp->isa_instklassptr() : NULL;\n@@ -1161,4 +1206,7 @@\n-  if (ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n-    is_intf = true;\n-  } else if (ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n-    is_intf = true;\n+  if (ttip != NULL) {\n+    if (ttip->is_interface())\n+      is_intf = true;\n+  }\n+  if (ttkp != NULL) {\n+    if (ttkp->is_interface())\n+      is_intf = true;\n@@ -1182,2 +1230,1 @@\n-        ciKlass* k = tiip->klass();\n-        if (k->is_loaded() && k->is_interface())\n+        if (tiip->is_interface())\n@@ -1221,1 +1268,1 @@\n-    if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n+    if (!t->empty() && ttip && ttip->is_interface()) {\n@@ -1223,1 +1270,1 @@\n-    } else if (!t->empty() && ttkp != NULL && ttkp->is_loaded() && ttkp->klass()->is_interface()) {\n+    } else if (!t->empty() && ttkp && ttkp->is_interface()) {\n@@ -1228,1 +1275,1 @@\n-      if (!t->empty() && ttip != NULL && ttip->is_loaded() && ttip->klass()->is_interface()) {\n+      if (!t->empty() && ttip != NULL && ttip->is_interface()) {\n@@ -1247,4 +1294,4 @@\n-    const TypeKlassPtr *jtkp = (jtp != NULL) ? jtp->isa_instklassptr() : NULL;\n-    if( jtip && ttip ) {\n-      if( jtip->is_loaded() &&  jtip->klass()->is_interface() &&\n-          ttip->is_loaded() && !ttip->klass()->is_interface() ) {\n+    const TypeInstKlassPtr *jtkp = (jtp != NULL) ? jtp->isa_instklassptr() : NULL;\n+    if (jtip && ttip) {\n+      if (jtip->is_interface() &&\n+          !ttip->is_interface()) {\n@@ -1256,2 +1303,2 @@\n-    if( jtkp && ttkp ) {\n-      if( jtkp->is_loaded() &&  jtkp->klass()->is_interface() &&\n+    if (jtkp && ttkp) {\n+      if (jtkp->is_interface() &&\n@@ -1259,1 +1306,1 @@\n-          ttkp->is_loaded() && !ttkp->klass()->is_interface() ) {\n+          ttkp->is_loaded() && !ttkp->is_interface()) {\n@@ -1805,1 +1852,1 @@\n-\/\/  as dead loop when the phi references itselfs through an other phi.\n+\/\/  as dead loop when the phi references itself through an other phi.\n@@ -2201,1 +2248,1 @@\n-        if (in(i)->in(AddPNode::Offset) != base) {\n+        if (in(i)->in(AddPNode::Base) != base) {\n","filename":"src\/hotspot\/share\/opto\/cfgnode.cpp","additions":110,"deletions":63,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -556,1 +556,1 @@\n-\/\/ CatchProjNode controls which exception handler is targetted after a call.\n+\/\/ CatchProjNode controls which exception handler is targeted after a call.\n","filename":"src\/hotspot\/share\/opto\/cfgnode.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -376,1 +376,1 @@\n-  \/\/ Veify the graph before RA.\n+  \/\/ Verify the graph before RA.\n@@ -596,1 +596,1 @@\n-  \/\/ Veify the graph after RA.\n+  \/\/ Verify the graph after RA.\n@@ -1730,0 +1730,13 @@\n+          } else {\n+            \/\/ There is no space reserved for a memory edge before the inputs for\n+            \/\/ instructions which have \"stackSlotX\" parameter instead of \"memory\".\n+            \/\/ For example, \"MoveF2I_stack_reg\". We always need a memory edge from\n+            \/\/ src to cisc, else we might schedule cisc before src, loading from a\n+            \/\/ spill location before storing the spill. On some platforms, we land\n+            \/\/ in this else case because mach->oper_input_base() > 1, i.e. we have\n+            \/\/ multiple inputs. In some rare cases there are even multiple memory\n+            \/\/ operands, before and after spilling.\n+            \/\/ (e.g. spilling \"addFPR24_reg_mem\" to \"addFPR24_mem_cisc\")\n+            \/\/ In either case, there is no space in the inputs for the memory edge\n+            \/\/ so we add an additional precedence \/ memory edge.\n+            cisc->add_prec(src);\n","filename":"src\/hotspot\/share\/opto\/chaitin.cpp","additions":16,"deletions":3,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -54,0 +54,1 @@\n+macro(ReverseBytesV)\n@@ -64,1 +65,0 @@\n-macro(CallNative)\n@@ -78,0 +78,2 @@\n+macro(CompressBits)\n+macro(ExpandBits)\n@@ -98,0 +100,1 @@\n+macro(CmpU3)\n@@ -99,0 +102,1 @@\n+macro(CmpUL3)\n@@ -156,0 +160,1 @@\n+macro(CountLeadingZerosV)\n@@ -158,0 +163,1 @@\n+macro(CountTrailingZerosV)\n@@ -165,0 +171,2 @@\n+macro(UDivI)\n+macro(UDivL)\n@@ -168,0 +176,2 @@\n+macro(UDivModI)\n+macro(UDivModL)\n@@ -218,1 +228,0 @@\n-macro(SetVectMaskI)\n@@ -236,0 +245,2 @@\n+macro(UModI)\n+macro(UModL)\n@@ -240,0 +251,2 @@\n+macro(IsInfiniteF)\n+macro(IsInfiniteD)\n@@ -277,0 +290,1 @@\n+macro(PopulateIndex)\n@@ -284,0 +298,3 @@\n+macro(ReverseI)\n+macro(ReverseL)\n+macro(ReverseV)\n@@ -313,0 +330,2 @@\n+macro(SignumVF)\n+macro(SignumVD)\n@@ -315,0 +334,2 @@\n+macro(RoundF)\n+macro(RoundD)\n@@ -392,0 +413,1 @@\n+macro(NegVL)\n@@ -420,0 +442,3 @@\n+macro(CompressV)\n+macro(CompressM)\n+macro(ExpandV)\n@@ -451,0 +476,2 @@\n+macro(RoundVF)\n+macro(RoundVD)\n","filename":"src\/hotspot\/share\/opto\/classes.hpp","additions":29,"deletions":2,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -24,1 +24,0 @@\n-\n@@ -253,0 +252,1 @@\n+    PhaseStringOpts::print_statistics();\n@@ -258,0 +258,2 @@\n+    ConnectionGraph::print_statistics();\n+    PhaseMacroExpand::print_statistics();\n@@ -401,0 +403,4 @@\n+\n+    if (dead->is_CallStaticJava()) {\n+      remove_unstable_if_trap(dead->as_CallStaticJava(), false);\n+    }\n@@ -448,0 +454,1 @@\n+  remove_useless_unstable_if_traps(useful);          \/\/ remove useless unstable_if traps\n@@ -564,1 +571,7 @@\n-  root()->dump(9999);\n+  if (_output == nullptr) {\n+    root()->dump(9999);\n+  } else {\n+    \/\/ Dump the node blockwise if we have a scheduling\n+    _output->print_scheduling();\n+  }\n+\n@@ -616,0 +629,1 @@\n+                  _unstable_if_traps (comp_arena(), 8, 0, NULL),\n@@ -633,1 +647,0 @@\n-                  _native_invokers(comp_arena(), 1, 0, NULL),\n@@ -641,1 +654,2 @@\n-                  _interpreter_frame_size(0)\n+                  _interpreter_frame_size(0),\n+                  _output(NULL)\n@@ -914,1 +928,0 @@\n-    _native_invokers(),\n@@ -923,0 +936,1 @@\n+    _output(NULL),\n@@ -1018,0 +1032,1 @@\n+  set_has_monitors(false);\n@@ -1315,2 +1330,2 @@\n-    bool default_value_load = EnableValhalla && tj->is_instptr()->klass() == ciEnv::current()->Class_klass();\n-    assert(InlineUnsafeOps || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n+    bool default_value_load = EnableValhalla && tj->is_instptr()->instance_klass() == ciEnv::current()->Class_klass();\n+    assert(InlineUnsafeOps || StressReflectiveCode || default_value_load, \"indeterminate pointers come only from unsafe ops\");\n@@ -1324,1 +1339,1 @@\n-  const TypeAryPtr *ta = tj->isa_aryptr();\n+  const TypeAryPtr* ta = tj->isa_aryptr();\n@@ -1342,1 +1357,4 @@\n-      tj = ta = TypeAryPtr::make(ptr, ta->ary(), ta->klass(), true, Type::Offset(offset), ta->field_offset(), ta->instance_id());\n+      tj = ta = ta->\n+              remove_speculative()->\n+              cast_to_ptr_type(ptr)->\n+              with_offset(offset);\n@@ -1353,1 +1371,5 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n+        tj = ta = ta->\n+                remove_speculative()->\n+                cast_to_ptr_type(ptr)->\n+                cast_to_exactness(false)->\n+                with_offset(offset);\n@@ -1367,1 +1389,5 @@\n-        tj = ta = TypeAryPtr::make(ptr,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n+        tj = ta = ta->\n+                remove_speculative()->\n+                cast_to_ptr_type(ptr)->\n+                cast_to_exactness(false)->\n+                with_offset(offset);\n@@ -1373,1 +1399,5 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,ta->klass(),false,Type::Offset(offset), ta->field_offset());\n+      tj = ta = ta->\n+              remove_speculative()->\n+              cast_to_ptr_type(ptr)->\n+              with_ary(tary)->\n+              cast_to_exactness(false);\n@@ -1400,1 +1430,5 @@\n-      tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta->ary(),ta->klass(),false,Type::Offset(offset), ta->field_offset());\n+      tj = ta = ta->\n+              remove_speculative()->\n+              cast_to_ptr_type(TypePtr::BotPTR)->\n+              cast_to_exactness(false)->\n+              with_offset(offset);\n@@ -1407,1 +1441,1 @@\n-    ciInstanceKlass *k = to->klass()->as_instance_klass();\n+    ciInstanceKlass* ik = to->instance_klass();\n@@ -1409,2 +1443,2 @@\n-      if (to->klass() != ciEnv::current()->Class_klass() ||\n-          offset < k->layout_helper_size_in_bytes()) {\n+      if (ik != ciEnv::current()->Class_klass() ||\n+          offset < ik->layout_helper_size_in_bytes()) {\n@@ -1414,1 +1448,5 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n+        tj = to = to->\n+                cast_to_instance_id(TypeOopPtr::InstanceBot)->\n+                remove_speculative()->\n+                cast_to_ptr_type(TypePtr::BotPTR)->\n+                cast_to_exactness(false);\n@@ -1422,1 +1460,5 @@\n-      tj = to = TypeInstPtr::make(TypePtr::BotPTR,to->klass(),false,0,Type::Offset(offset));\n+      tj = to = to->\n+              remove_speculative()->\n+              cast_to_instance_id(TypeOopPtr::InstanceBot)->\n+              cast_to_ptr_type(TypePtr::BotPTR)->\n+              cast_to_exactness(false);\n@@ -1425,1 +1467,1 @@\n-      tj = to = TypeInstPtr::make(to->ptr(),to->klass(),to->klass_is_exact(),to->const_oop(),Type::Offset(to->offset()), to->klass()->flatten_array(), to->instance_id());\n+      tj = to = to->remove_speculative();\n@@ -1434,1 +1476,1 @@\n-    } else if (offset < 0 || offset >= k->layout_helper_size_in_bytes()) {\n+    } else if (offset < 0 || offset >= ik->layout_helper_size_in_bytes()) {\n@@ -1437,1 +1479,1 @@\n-      if (to->klass() != ciEnv::current()->Class_klass()) {\n+      if (ik != ciEnv::current()->Class_klass()) {\n@@ -1443,1 +1485,1 @@\n-      ciInstanceKlass *canonical_holder = k->get_canonical_holder(offset);\n+      ciInstanceKlass *canonical_holder = ik->get_canonical_holder(offset);\n@@ -1445,1 +1487,1 @@\n-      if (!k->equals(canonical_holder) || tj->offset() != offset) {\n+      if (!ik->equals(canonical_holder) || tj->offset() != offset) {\n@@ -1463,4 +1505,3 @@\n-\n-      tj = tk = TypeKlassPtr::make(TypePtr::NotNull,\n-                                   TypeInstKlassPtr::OBJECT->klass(),\n-                                   Type::Offset(offset));\n+      tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull,\n+                                       env()->Object_klass(),\n+                                       Type::Offset(offset));\n@@ -1469,6 +1510,7 @@\n-    ciKlass* klass = tk->klass();\n-    if (klass != NULL && klass->is_obj_array_klass()) {\n-      ciKlass* k = TypeAryPtr::OOPS->klass();\n-      if( !k || !k->is_loaded() )                  \/\/ Only fails for some -Xcomp runs\n-        k = TypeInstPtr::BOTTOM->klass();\n-      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, k, Type::Offset(offset));\n+    if (tk->isa_aryklassptr() && tk->is_aryklassptr()->elem()->isa_klassptr()) {\n+      ciKlass* k = ciObjArrayKlass::make(env()->Object_klass());\n+      if (!k || !k->is_loaded()) {                  \/\/ Only fails for some -Xcomp runs\n+        tj = tk = TypeInstKlassPtr::make(TypePtr::NotNull, env()->Object_klass(), Type::Offset(offset));\n+      } else {\n+        tj = tk = TypeAryKlassPtr::make(TypePtr::NotNull, tk->is_aryklassptr()->elem(), k, Type::Offset(offset), tk->is_not_flat(), tk->is_not_null_free(), tk->is_null_free());\n+      }\n@@ -1490,1 +1532,1 @@\n-      tj = tk = TypeKlassPtr::make(TypePtr::NotNull, tk->klass(), Type::Offset(offset));\n+      tj = tk = tk->with_offset(offset);\n@@ -1589,1 +1631,1 @@\n-    if (tjp->klass()  != field()->holder() ||\n+    if (tjp->is_instptr()->instance_klass()  != field()->holder() ||\n@@ -1687,1 +1729,1 @@\n-          && flat->is_instptr()->klass() == env()->Class_klass())\n+          && flat->is_instptr()->instance_klass() == env()->Class_klass())\n@@ -1730,2 +1772,2 @@\n-          tinst->klass() == ciEnv::current()->Class_klass() &&\n-          tinst->offset() >= (tinst->klass()->as_instance_klass()->layout_helper_size_in_bytes())) {\n+          tinst->instance_klass() == ciEnv::current()->Class_klass() &&\n+          tinst->offset() >= (tinst->instance_klass()->layout_helper_size_in_bytes())) {\n@@ -1736,1 +1778,1 @@\n-      } else if (tinst->klass()->is_inlinetype()) {\n+      } else if (tinst->is_inlinetypeptr()) {\n@@ -1741,1 +1783,1 @@\n-        ciInstanceKlass* k = tinst->klass()->as_instance_klass();\n+        ciInstanceKlass *k = tinst->instance_klass();\n@@ -2295,0 +2337,99 @@\n+void Compile::record_unstable_if_trap(UnstableIfTrap* trap) {\n+  if (OptimizeUnstableIf) {\n+    _unstable_if_traps.append(trap);\n+  }\n+}\n+\n+void Compile::remove_useless_unstable_if_traps(Unique_Node_List& useful) {\n+  for (int i = _unstable_if_traps.length() - 1; i >= 0; i--) {\n+    UnstableIfTrap* trap = _unstable_if_traps.at(i);\n+    Node* n = trap->uncommon_trap();\n+    if (!useful.member(n)) {\n+      _unstable_if_traps.delete_at(i); \/\/ replaces i-th with last element which is known to be useful (already processed)\n+    }\n+  }\n+}\n+\n+\/\/ Remove the unstable if trap associated with 'unc' from candidates. It is either dead\n+\/\/ or fold-compares case. Return true if succeed or not found.\n+\/\/\n+\/\/ In rare cases, the found trap has been processed. It is too late to delete it. Return\n+\/\/ false and ask fold-compares to yield.\n+\/\/\n+\/\/ 'fold-compares' may use the uncommon_trap of the dominating IfNode to cover the fused\n+\/\/ IfNode. This breaks the unstable_if trap invariant: control takes the unstable path\n+\/\/ when deoptimization does happen.\n+bool Compile::remove_unstable_if_trap(CallStaticJavaNode* unc, bool yield) {\n+  for (int i = 0; i < _unstable_if_traps.length(); ++i) {\n+    UnstableIfTrap* trap = _unstable_if_traps.at(i);\n+    if (trap->uncommon_trap() == unc) {\n+      if (yield && trap->modified()) {\n+        return false;\n+      }\n+      _unstable_if_traps.delete_at(i);\n+      break;\n+    }\n+  }\n+  return true;\n+}\n+\n+\/\/ Re-calculate unstable_if traps with the liveness of next_bci, which points to the unlikely path.\n+\/\/ It needs to be done after igvn because fold-compares may fuse uncommon_traps and before renumbering.\n+void Compile::process_for_unstable_if_traps(PhaseIterGVN& igvn) {\n+  for (int i = _unstable_if_traps.length() - 1; i >= 0; --i) {\n+    UnstableIfTrap* trap = _unstable_if_traps.at(i);\n+    CallStaticJavaNode* unc = trap->uncommon_trap();\n+    int next_bci = trap->next_bci();\n+    bool modified = trap->modified();\n+\n+    if (next_bci != -1 && !modified) {\n+      assert(!_dead_node_list.test(unc->_idx), \"changing a dead node!\");\n+      JVMState* jvms = unc->jvms();\n+      ciMethod* method = jvms->method();\n+      ciBytecodeStream iter(method);\n+\n+      iter.force_bci(jvms->bci());\n+      assert(next_bci == iter.next_bci() || next_bci == iter.get_dest(), \"wrong next_bci at unstable_if\");\n+      Bytecodes::Code c = iter.cur_bc();\n+      Node* lhs = nullptr;\n+      Node* rhs = nullptr;\n+      if (c == Bytecodes::_if_acmpeq || c == Bytecodes::_if_acmpne) {\n+        lhs = unc->peek_operand(0);\n+        rhs = unc->peek_operand(1);\n+      } else if (c == Bytecodes::_ifnull || c == Bytecodes::_ifnonnull) {\n+        lhs = unc->peek_operand(0);\n+      }\n+\n+      ResourceMark rm;\n+      const MethodLivenessResult& live_locals = method->liveness_at_bci(next_bci);\n+      assert(live_locals.is_valid(), \"broken liveness info\");\n+      int len = (int)live_locals.size();\n+\n+      for (int i = 0; i < len; i++) {\n+        Node* local = unc->local(jvms, i);\n+        \/\/ kill local using the liveness of next_bci.\n+        \/\/ give up when the local looks like an operand to secure reexecution.\n+        if (!live_locals.at(i) && !local->is_top() && local != lhs && local!= rhs) {\n+          uint idx = jvms->locoff() + i;\n+#ifdef ASSERT\n+          if (Verbose) {\n+            tty->print(\"[unstable_if] kill local#%d: \", idx);\n+            local->dump();\n+            tty->cr();\n+          }\n+#endif\n+          igvn.replace_input_of(unc, idx, top());\n+          modified = true;\n+        }\n+      }\n+    }\n+\n+    \/\/ keep the mondified trap for late query\n+    if (modified) {\n+      trap->set_modified();\n+    } else {\n+      _unstable_if_traps.delete_at(i);\n+    }\n+  }\n+  igvn.optimize();\n+}\n@@ -2307,1 +2448,1 @@\n-    PhaseStringOpts pso(initial_gvn(), for_igvn());\n+    PhaseStringOpts pso(initial_gvn());\n@@ -2507,1 +2648,0 @@\n-    debug_only( int cnt = 0; );\n@@ -2510,1 +2650,0 @@\n-      assert( cnt++ < 40, \"infinite cycle in loop optimization\" );\n@@ -2586,0 +2725,2 @@\n+  process_for_unstable_if_traps(igvn);\n+\n@@ -3547,1 +3688,0 @@\n-  case Op_CallNative:\n@@ -4000,0 +4140,40 @@\n+  case Op_UModI:\n+    if (UseDivMod) {\n+      \/\/ Check if a%b and a\/b both exist\n+      Node* d = n->find_similar(Op_UDivI);\n+      if (d) {\n+        \/\/ Replace them with a fused unsigned divmod if supported\n+        if (Matcher::has_match_rule(Op_UDivModI)) {\n+          UDivModINode* divmod = UDivModINode::make(n);\n+          d->subsume_by(divmod->div_proj(), this);\n+          n->subsume_by(divmod->mod_proj(), this);\n+        } else {\n+          \/\/ replace a%b with a-((a\/b)*b)\n+          Node* mult = new MulINode(d, d->in(2));\n+          Node* sub  = new SubINode(d->in(1), mult);\n+          n->subsume_by(sub, this);\n+        }\n+      }\n+    }\n+    break;\n+\n+  case Op_UModL:\n+    if (UseDivMod) {\n+      \/\/ Check if a%b and a\/b both exist\n+      Node* d = n->find_similar(Op_UDivL);\n+      if (d) {\n+        \/\/ Replace them with a fused unsigned divmod if supported\n+        if (Matcher::has_match_rule(Op_UDivModL)) {\n+          UDivModLNode* divmod = UDivModLNode::make(n);\n+          d->subsume_by(divmod->div_proj(), this);\n+          n->subsume_by(divmod->mod_proj(), this);\n+        } else {\n+          \/\/ replace a%b with a-((a\/b)*b)\n+          Node* mult = new MulLNode(d, d->in(2));\n+          Node* sub  = new SubLNode(d->in(1), mult);\n+          n->subsume_by(sub, this);\n+        }\n+      }\n+    }\n+    break;\n+\n@@ -4099,1 +4279,1 @@\n-        if (m->outcnt() == 0) {\n+        if (m->outcnt() == 0 && m != top()) {\n@@ -4392,1 +4572,1 @@\n-  \/\/ check if the optimizer has made it homogenous, item (3).\n+  \/\/ check if the optimizer has made it homogeneous, item (3).\n@@ -4665,2 +4845,2 @@\n-int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {\n-  if (StressReflectiveCode || superk == NULL || subk == NULL) {\n+Compile::SubTypeCheckResult Compile::static_subtype_check(const TypeKlassPtr* superk, const TypeKlassPtr* subk) {\n+  if (StressReflectiveCode) {\n@@ -4670,2 +4850,2 @@\n-  if (superk == env()->Object_klass()) {\n-    return SSC_always_true;     \/\/ (0) this test cannot fail\n+  if (subk->is_java_subtype_of(superk)) {\n+    return SSC_always_true; \/\/ (0) and (1)  this test cannot fail\n@@ -4674,18 +4854,2 @@\n-  ciType* superelem = superk;\n-  ciType* subelem = subk;\n-  if (superelem->is_array_klass()) {\n-    superelem = superelem->as_array_klass()->base_element_type();\n-  }\n-  if (subelem->is_array_klass()) {\n-    subelem = subelem->as_array_klass()->base_element_type();\n-  }\n-\n-  if (!subk->is_interface()) {  \/\/ cannot trust static interface types yet\n-    if (subk->is_subtype_of(superk)) {\n-      return SSC_always_true;   \/\/ (1) false path dead; no dynamic test needed\n-    }\n-    if (!(superelem->is_klass() && superelem->as_klass()->is_interface()) &&\n-        !(subelem->is_klass() && subelem->as_klass()->is_interface()) &&\n-        !superk->is_subtype_of(subk)) {\n-      return SSC_always_false;  \/\/ (2) true path dead; no dynamic test needed\n-    }\n+  if (!subk->maybe_java_subtype_of(superk)) {\n+    return SSC_always_false; \/\/ (2) true path dead; no dynamic test needed\n@@ -4697,2 +4861,2 @@\n-  if (superk->is_obj_array_klass() && !superk->as_array_klass()->is_elem_null_free() &&\n-      superk->as_array_klass()->element_klass()->is_inlinetype()) {\n+  if (superk->isa_aryklassptr() && !superk->is_aryklassptr()->is_null_free() &&\n+      superk->is_aryklassptr()->elem()->isa_klassptr() && superk->is_aryklassptr()->elem()->is_klassptr()->klass()->is_inlinetype()) {\n@@ -4701,7 +4865,10 @@\n-  \/\/ If casting to an instance klass, it must have no subtypes\n-  if (superk->is_interface()) {\n-    \/\/ Cannot trust interfaces yet.\n-    \/\/ %%% S.B. superk->nof_implementors() == 1\n-  } else if (superelem->is_instance_klass()) {\n-    ciInstanceKlass* ik = superelem->as_instance_klass();\n-    if (!ik->has_subklass() && !ik->is_interface()) {\n+\n+  const Type* superelem = superk;\n+  if (superk->isa_aryklassptr()) {\n+    int ignored;\n+    superelem = superk->is_aryklassptr()->base_element_type(ignored);\n+  }\n+\n+  if (superelem->isa_instklassptr()) {\n+    ciInstanceKlass* ik = superelem->is_instklassptr()->instance_klass();\n+    if (!ik->has_subklass()) {\n@@ -4712,0 +4879,3 @@\n+      if (!superk->maybe_java_subtype_of(subk)) {\n+        return SSC_always_false;\n+      }\n@@ -4766,1 +4936,1 @@\n-\/\/ _print_inlining_stream and transfered into the _print_inlining_list\n+\/\/ _print_inlining_stream and transferred into the _print_inlining_list\n@@ -4857,1 +5027,1 @@\n-  assert(!_print_inlining || _print_inlining_stream->size() == 0, \"loosing data\");\n+  assert(!_print_inlining || _print_inlining_stream->size() == 0, \"losing data\");\n@@ -5517,4 +5687,0 @@\n-void Compile::add_native_invoker(RuntimeStub* stub) {\n-  _native_invokers.append(stub);\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":249,"deletions":83,"binary":false,"changes":332,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+class CallStaticJavaNode;\n@@ -88,0 +89,1 @@\n+class TypeKlassPtr;\n@@ -93,0 +95,1 @@\n+class UnstableIfTrap;\n@@ -108,0 +111,4 @@\n+\/\/ The type of all node counts and indexes.\n+\/\/ It must hold at least 16 bits, but must also be fast to load and store.\n+\/\/ This type, if less than 32 bits, could limit the number of possible nodes.\n+\/\/ (To make this type platform-specific, move to globalDefinitions_xxx.hpp.)\n@@ -109,0 +116,1 @@\n+\n@@ -337,0 +345,1 @@\n+  bool                  _has_monitors;          \/\/ Metadata transfered to nmethod to enable Continuations lock-detection fastpath\n@@ -359,0 +368,1 @@\n+  GrowableArray<UnstableIfTrap*> _unstable_if_traps;        \/\/ List of ifnodes after IGVN\n@@ -429,2 +439,0 @@\n-  GrowableArray<RuntimeStub*>   _native_invokers;\n-\n@@ -645,0 +653,3 @@\n+  bool              has_monitors() const         { return _has_monitors; }\n+  void          set_has_monitors(bool v)         { _has_monitors = v; }\n+\n@@ -750,0 +761,5 @@\n+  void record_unstable_if_trap(UnstableIfTrap* trap);\n+  bool remove_unstable_if_trap(CallStaticJavaNode* unc, bool yield);\n+  void remove_useless_unstable_if_traps(Unique_Node_List &useful);\n+  void process_for_unstable_if_traps(PhaseIterGVN& igvn);\n+\n@@ -989,4 +1005,0 @@\n-  void add_native_invoker(RuntimeStub* stub);\n-\n-  const GrowableArray<RuntimeStub*> native_invokers() const { return _native_invokers; }\n-\n@@ -1189,2 +1201,2 @@\n-  enum { SSC_always_false, SSC_always_true, SSC_easy_test, SSC_full_test };\n-  int static_subtype_check(ciKlass* superk, ciKlass* subk);\n+  enum SubTypeCheckResult { SSC_always_false, SSC_always_true, SSC_easy_test, SSC_full_test };\n+  SubTypeCheckResult static_subtype_check(const TypeKlassPtr* superk, const TypeKlassPtr* subk);\n@@ -1235,1 +1247,1 @@\n-                            BasicType bt);\n+                            BasicType out_bt, BasicType in_bt);\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":21,"deletions":9,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -309,0 +309,1 @@\n+#ifdef ASSERT\n@@ -314,0 +315,233 @@\n+#endif\n+\n+template<class T> static bool subtract_overflows(T x, T y) {\n+  T s = java_subtract(x, y);\n+  return (x >= 0) && (y < 0) && (s < 0);\n+}\n+\n+template<class T> static bool subtract_underflows(T x, T y) {\n+  T s = java_subtract(x, y);\n+  return (x < 0) && (y > 0) && (s > 0);\n+}\n+\n+template<class T> static bool add_overflows(T x, T y) {\n+  T s = java_add(x, y);\n+  return (x > 0) && (y > 0) && (s < 0);\n+}\n+\n+template<class T> static bool add_underflows(T x, T y) {\n+  T s = java_add(x, y);\n+  return (x < 0) && (y < 0) && (s >= 0);\n+}\n+\n+template<class T> static bool ranges_overlap(T xlo, T ylo, T xhi, T yhi, T zlo, T zhi,\n+                                             const Node* n, bool pos) {\n+  assert(xlo <= xhi && ylo <= yhi && zlo <= zhi, \"should not be empty types\");\n+  T x_y_lo;\n+  T x_y_hi;\n+  bool x_y_lo_overflow;\n+  bool x_y_hi_overflow;\n+\n+  if (n->is_Sub()) {\n+    x_y_lo = java_subtract(xlo, yhi);\n+    x_y_hi = java_subtract(xhi, ylo);\n+    x_y_lo_overflow = pos ? subtract_overflows(xlo, yhi) : subtract_underflows(xlo, yhi);\n+    x_y_hi_overflow = pos ? subtract_overflows(xhi, ylo) : subtract_underflows(xhi, ylo);\n+  } else {\n+    assert(n->is_Add(), \"Add or Sub only\");\n+    x_y_lo = java_add(xlo, ylo);\n+    x_y_hi = java_add(xhi, yhi);\n+    x_y_lo_overflow = pos ? add_overflows(xlo, ylo) : add_underflows(xlo, ylo);\n+    x_y_hi_overflow = pos ? add_overflows(xhi, yhi) : add_underflows(xhi, yhi);\n+  }\n+  assert(!pos || !x_y_lo_overflow || x_y_hi_overflow, \"x_y_lo_overflow => x_y_hi_overflow\");\n+  assert(pos || !x_y_hi_overflow || x_y_lo_overflow, \"x_y_hi_overflow => x_y_lo_overflow\");\n+\n+  \/\/ Two ranges overlap iff one range's low point falls in the other range.\n+  \/\/ nbits = 32 or 64\n+  if (pos) {\n+    \/\/ (zlo + 2**nbits  <= x_y_lo && x_y_lo <= zhi ** nbits)\n+    if (x_y_lo_overflow) {\n+      if (zlo <= x_y_lo && x_y_lo <= zhi) {\n+        return true;\n+      }\n+    }\n+\n+    \/\/ (x_y_lo <= zlo + 2**nbits && zlo + 2**nbits <= x_y_hi)\n+    if (x_y_hi_overflow) {\n+      if ((!x_y_lo_overflow || x_y_lo <= zlo) && zlo <= x_y_hi) {\n+        return true;\n+      }\n+    }\n+  } else {\n+    \/\/ (zlo - 2**nbits <= x_y_hi && x_y_hi <= zhi - 2**nbits)\n+    if (x_y_hi_overflow) {\n+      if (zlo <= x_y_hi && x_y_hi <= zhi) {\n+        return true;\n+      }\n+    }\n+\n+    \/\/ (x_y_lo <= zhi - 2**nbits && zhi - 2**nbits <= x_y_hi)\n+    if (x_y_lo_overflow) {\n+      if (x_y_lo <= zhi && (!x_y_hi_overflow || zhi <= x_y_hi)) {\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+static bool ranges_overlap(const TypeInteger* tx, const TypeInteger* ty, const TypeInteger* tz,\n+                           const Node* n, bool pos, BasicType bt) {\n+  jlong xlo = tx->lo_as_long();\n+  jlong xhi = tx->hi_as_long();\n+  jlong ylo = ty->lo_as_long();\n+  jlong yhi = ty->hi_as_long();\n+  jlong zlo = tz->lo_as_long();\n+  jlong zhi = tz->hi_as_long();\n+\n+  if (bt == T_INT) {\n+    \/\/ See if x+y can cause positive overflow into z+2**32\n+    \/\/ See if x+y can cause negative overflow into z-2**32\n+    bool res =  ranges_overlap(checked_cast<jint>(xlo), checked_cast<jint>(ylo),\n+                               checked_cast<jint>(xhi), checked_cast<jint>(yhi),\n+                               checked_cast<jint>(zlo), checked_cast<jint>(zhi), n, pos);\n+#ifdef ASSERT\n+    jlong vbit = CONST64(1) << BitsPerInt;\n+    if (n->Opcode() == Op_SubI) {\n+      jlong ylo0 = ylo;\n+      ylo = -yhi;\n+      yhi = -ylo0;\n+    }\n+    assert(res == long_ranges_overlap(xlo+ylo, xhi+yhi, pos ? zlo+vbit : zlo-vbit, pos ? zhi+vbit : zhi-vbit), \"inconsistent result\");\n+#endif\n+    return res;\n+  }\n+  assert(bt == T_LONG, \"only int or long\");\n+  \/\/ See if x+y can cause positive overflow into z+2**64\n+  \/\/ See if x+y can cause negative overflow into z-2**64\n+  return ranges_overlap(xlo, ylo, xhi, yhi, zlo, zhi, n, pos);\n+}\n+\n+#ifdef ASSERT\n+static bool compute_updates_ranges_verif(const TypeInteger* tx, const TypeInteger* ty, const TypeInteger* tz,\n+                                         jlong& rxlo, jlong& rxhi, jlong& rylo, jlong& ryhi,\n+                                         const Node* n) {\n+  jlong xlo = tx->lo_as_long();\n+  jlong xhi = tx->hi_as_long();\n+  jlong ylo = ty->lo_as_long();\n+  jlong yhi = ty->hi_as_long();\n+  jlong zlo = tz->lo_as_long();\n+  jlong zhi = tz->hi_as_long();\n+  if (n->is_Sub()) {\n+    swap(ylo, yhi);\n+    ylo = -ylo;\n+    yhi = -yhi;\n+  }\n+\n+  rxlo = MAX2(xlo, zlo - yhi);\n+  rxhi = MIN2(xhi, zhi - ylo);\n+  rylo = MAX2(ylo, zlo - xhi);\n+  ryhi = MIN2(yhi, zhi - xlo);\n+  if (rxlo > rxhi || rylo > ryhi) {\n+    return false;\n+  }\n+  if (n->is_Sub()) {\n+    swap(rylo, ryhi);\n+    rylo = -rylo;\n+    ryhi = -ryhi;\n+  }\n+  assert(rxlo == (int) rxlo && rxhi == (int) rxhi, \"x should not overflow\");\n+  assert(rylo == (int) rylo && ryhi == (int) ryhi, \"y should not overflow\");\n+  return true;\n+}\n+#endif\n+\n+template<class T> static bool compute_updates_ranges(T xlo, T ylo, T xhi, T yhi, T zlo, T zhi,\n+                                                     jlong& rxlo, jlong& rxhi, jlong& rylo, jlong& ryhi,\n+                                                     const Node* n) {\n+  assert(xlo <= xhi && ylo <= yhi && zlo <= zhi, \"should not be empty types\");\n+\n+  \/\/ Now it's always safe to assume x+y does not overflow.\n+  \/\/ This is true even if some pairs x,y might cause overflow, as long\n+  \/\/ as that overflow value cannot fall into [zlo,zhi].\n+\n+  \/\/ Confident that the arithmetic is \"as if infinite precision\",\n+  \/\/ we can now use n's range to put constraints on those of x and y.\n+  \/\/ The \"natural\" range of x [xlo,xhi] can perhaps be narrowed to a\n+  \/\/ more \"restricted\" range by intersecting [xlo,xhi] with the\n+  \/\/ range obtained by subtracting y's range from the asserted range\n+  \/\/ of the I2L conversion.  Here's the interval arithmetic algebra:\n+  \/\/    x == n-y == [zlo,zhi]-[ylo,yhi] == [zlo,zhi]+[-yhi,-ylo]\n+  \/\/    => x in [zlo-yhi, zhi-ylo]\n+  \/\/    => x in [zlo-yhi, zhi-ylo] INTERSECT [xlo,xhi]\n+  \/\/    => x in [xlo MAX zlo-yhi, xhi MIN zhi-ylo]\n+  \/\/ And similarly, x changing place with y.\n+  if (n->is_Sub()) {\n+    if (add_overflows(zlo, ylo) || add_underflows(zhi, yhi) || subtract_underflows(xhi, zlo) ||\n+        subtract_overflows(xlo, zhi)) {\n+      return false;\n+    }\n+    rxlo = add_underflows(zlo, ylo) ? xlo : MAX2(xlo, java_add(zlo, ylo));\n+    rxhi = add_overflows(zhi, yhi) ? xhi : MIN2(xhi, java_add(zhi, yhi));\n+    ryhi = subtract_overflows(xhi, zlo) ? yhi : MIN2(yhi, java_subtract(xhi, zlo));\n+    rylo = subtract_underflows(xlo, zhi) ? ylo : MAX2(ylo, java_subtract(xlo, zhi));\n+  } else {\n+    assert(n->is_Add(), \"Add or Sub only\");\n+    if (subtract_overflows(zlo, yhi) || subtract_underflows(zhi, ylo) ||\n+        subtract_overflows(zlo, xhi) || subtract_underflows(zhi, xlo)) {\n+      return false;\n+    }\n+    rxlo = subtract_underflows(zlo, yhi) ? xlo : MAX2(xlo, java_subtract(zlo, yhi));\n+    rxhi = subtract_overflows(zhi, ylo) ? xhi : MIN2(xhi, java_subtract(zhi, ylo));\n+    rylo = subtract_underflows(zlo, xhi) ? ylo : MAX2(ylo, java_subtract(zlo, xhi));\n+    ryhi = subtract_overflows(zhi, xlo) ? yhi : MIN2(yhi, java_subtract(zhi, xlo));\n+  }\n+\n+  if (rxlo > rxhi || rylo > ryhi) {\n+    return false; \/\/ x or y is dying; don't mess w\/ it\n+  }\n+\n+  return true;\n+}\n+\n+static bool compute_updates_ranges(const TypeInteger* tx, const TypeInteger* ty, const TypeInteger* tz,\n+                                   const TypeInteger*& rx, const TypeInteger*& ry,\n+                                   const Node* n, const BasicType in_bt, BasicType out_bt) {\n+\n+  jlong xlo = tx->lo_as_long();\n+  jlong xhi = tx->hi_as_long();\n+  jlong ylo = ty->lo_as_long();\n+  jlong yhi = ty->hi_as_long();\n+  jlong zlo = tz->lo_as_long();\n+  jlong zhi = tz->hi_as_long();\n+  jlong rxlo, rxhi, rylo, ryhi;\n+\n+  if (in_bt == T_INT) {\n+#ifdef ASSERT\n+    jlong expected_rxlo, expected_rxhi, expected_rylo, expected_ryhi;\n+    bool expected = compute_updates_ranges_verif(tx, ty, tz,\n+                                                 expected_rxlo, expected_rxhi,\n+                                                 expected_rylo, expected_ryhi, n);\n+#endif\n+    if (!compute_updates_ranges(checked_cast<jint>(xlo), checked_cast<jint>(ylo),\n+                                checked_cast<jint>(xhi), checked_cast<jint>(yhi),\n+                                checked_cast<jint>(zlo), checked_cast<jint>(zhi),\n+                                rxlo, rxhi, rylo, ryhi, n)) {\n+      assert(!expected, \"inconsistent\");\n+      return false;\n+    }\n+    assert(expected && rxlo == expected_rxlo && rxhi == expected_rxhi && rylo == expected_rylo && ryhi == expected_ryhi, \"inconsistent\");\n+  } else {\n+    if (!compute_updates_ranges(xlo, ylo, xhi, yhi, zlo, zhi,\n+                            rxlo, rxhi, rylo, ryhi, n)) {\n+      return false;\n+    }\n+  }\n+\n+  int widen =  MAX2(tx->widen_limit(), ty->widen_limit());\n+  rx = TypeInteger::make(rxlo, rxhi, widen, out_bt);\n+  ry = TypeInteger::make(rylo, ryhi, widen, out_bt);\n+  return true;\n+}\n@@ -333,1 +567,1 @@\n-                            BasicType bt) {\n+                            BasicType in_bt, BasicType out_bt) {\n@@ -335,1 +569,1 @@\n-  if (op == Op_AddI || op == Op_SubI) {\n+  if (op == Op_Add(in_bt) || op == Op_Sub(in_bt)) {\n@@ -345,22 +579,5 @@\n-    const TypeInt*  tx = phase->type(x)->is_int();\n-    const TypeInt*  ty = phase->type(y)->is_int();\n-\n-    jlong xlo = tx->is_int()->_lo;\n-    jlong xhi = tx->is_int()->_hi;\n-    jlong ylo = ty->is_int()->_lo;\n-    jlong yhi = ty->is_int()->_hi;\n-    jlong zlo = tz->lo_as_long();\n-    jlong zhi = tz->hi_as_long();\n-    jlong vbit = CONST64(1) << BitsPerInt;\n-    int widen =  MAX2(tx->_widen, ty->_widen);\n-    if (op == Op_SubI) {\n-      jlong ylo0 = ylo;\n-      ylo = -yhi;\n-      yhi = -ylo0;\n-    }\n-    \/\/ See if x+y can cause positive overflow into z+2**32\n-    if (long_ranges_overlap(xlo+ylo, xhi+yhi, zlo+vbit, zhi+vbit)) {\n-      return false;\n-    }\n-    \/\/ See if x+y can cause negative overflow into z-2**32\n-    if (long_ranges_overlap(xlo+ylo, xhi+yhi, zlo-vbit, zhi-vbit)) {\n+    const TypeInteger* tx = phase->type(x)->is_integer(in_bt);\n+    const TypeInteger* ty = phase->type(y)->is_integer(in_bt);\n+\n+    if (ranges_overlap(tx, ty, tz, z, true, in_bt) ||\n+        ranges_overlap(tx, ty, tz, z, false, in_bt)) {\n@@ -369,32 +586,1 @@\n-    \/\/ Now it's always safe to assume x+y does not overflow.\n-    \/\/ This is true even if some pairs x,y might cause overflow, as long\n-    \/\/ as that overflow value cannot fall into [zlo,zhi].\n-\n-    \/\/ Confident that the arithmetic is \"as if infinite precision\",\n-    \/\/ we can now use z's range to put constraints on those of x and y.\n-    \/\/ The \"natural\" range of x [xlo,xhi] can perhaps be narrowed to a\n-    \/\/ more \"restricted\" range by intersecting [xlo,xhi] with the\n-    \/\/ range obtained by subtracting y's range from the asserted range\n-    \/\/ of the I2L conversion.  Here's the interval arithmetic algebra:\n-    \/\/    x == z-y == [zlo,zhi]-[ylo,yhi] == [zlo,zhi]+[-yhi,-ylo]\n-    \/\/    => x in [zlo-yhi, zhi-ylo]\n-    \/\/    => x in [zlo-yhi, zhi-ylo] INTERSECT [xlo,xhi]\n-    \/\/    => x in [xlo MAX zlo-yhi, xhi MIN zhi-ylo]\n-    jlong rxlo = MAX2(xlo, zlo - yhi);\n-    jlong rxhi = MIN2(xhi, zhi - ylo);\n-    \/\/ And similarly, x changing place with y:\n-    jlong rylo = MAX2(ylo, zlo - xhi);\n-    jlong ryhi = MIN2(yhi, zhi - xlo);\n-    if (rxlo > rxhi || rylo > ryhi) {\n-      return false;  \/\/ x or y is dying; don't mess w\/ it\n-    }\n-    if (op == Op_SubI) {\n-      jlong rylo0 = rylo;\n-      rylo = -ryhi;\n-      ryhi = -rylo0;\n-    }\n-    assert(rxlo == (int)rxlo && rxhi == (int)rxhi, \"x should not overflow\");\n-    assert(rylo == (int)rylo && ryhi == (int)ryhi, \"y should not overflow\");\n-    rx = TypeInteger::make(rxlo, rxhi, widen, bt);\n-    ry = TypeInteger::make(rylo, ryhi, widen, bt);\n-    return true;\n+    return compute_updates_ranges(tx, ty, tz, rx, ry, z, in_bt, out_bt);\n@@ -439,1 +625,1 @@\n-  if (Compile::push_thru_add(phase, z, this_type, rx, ry, T_LONG)) {\n+  if (Compile::push_thru_add(phase, z, this_type, rx, ry, T_INT, T_LONG)) {\n","filename":"src\/hotspot\/share\/opto\/convertnode.cpp","additions":243,"deletions":57,"binary":false,"changes":300,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -85,0 +85,8 @@\n+class RoundDNode : public Node {\n+  public:\n+  RoundDNode( Node *dbl ) : Node(0,dbl) {}\n+  virtual int Opcode() const;\n+  virtual const Type *bottom_type() const { return TypeLong::LONG; }\n+  virtual uint ideal_reg() const { return Op_RegL; }\n+};\n+\n@@ -109,0 +117,1 @@\n+\n@@ -145,0 +154,8 @@\n+class RoundFNode : public Node {\n+  public:\n+  RoundFNode( Node *in1 ) : Node(0,in1) {}\n+  virtual int Opcode() const;\n+  virtual const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual uint  ideal_reg() const { return Op_RegI; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/convertnode.hpp","additions":18,"deletions":1,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -844,0 +844,78 @@\n+\/\/=============================================================================\n+\/\/------------------------------Identity---------------------------------------\n+\/\/ If the divisor is 1, we are an identity on the dividend.\n+Node* UDivINode::Identity(PhaseGVN* phase) {\n+  return (phase->type( in(2) )->higher_equal(TypeInt::ONE)) ? in(1) : this;\n+}\n+\/\/------------------------------Value------------------------------------------\n+\/\/ A UDivINode divides its inputs.  The third input is a Control input, used to\n+\/\/ prevent hoisting the divide above an unsafe test.\n+const Type* UDivINode::Value(PhaseGVN* phase) const {\n+  \/\/ Either input is TOP ==> the result is TOP\n+  const Type *t1 = phase->type( in(1) );\n+  const Type *t2 = phase->type( in(2) );\n+  if( t1 == Type::TOP ) return Type::TOP;\n+  if( t2 == Type::TOP ) return Type::TOP;\n+\n+  \/\/ x\/x == 1 since we always generate the dynamic divisor check for 0.\n+  if (in(1) == in(2)) {\n+    return TypeInt::ONE;\n+  }\n+\n+  \/\/ Either input is BOTTOM ==> the result is the local BOTTOM\n+  const Type *bot = bottom_type();\n+  if( (t1 == bot) || (t2 == bot) ||\n+      (t1 == Type::BOTTOM) || (t2 == Type::BOTTOM) )\n+    return bot;\n+\n+  \/\/ Otherwise we give up all hope\n+  return TypeInt::INT;\n+}\n+\n+\/\/------------------------------Idealize---------------------------------------\n+Node *UDivINode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ Check for dead control input\n+  if (in(0) && remove_dead_region(phase, can_reshape))  return this;\n+  return NULL;\n+}\n+\n+\n+\/\/=============================================================================\n+\/\/------------------------------Identity---------------------------------------\n+\/\/ If the divisor is 1, we are an identity on the dividend.\n+Node* UDivLNode::Identity(PhaseGVN* phase) {\n+  return (phase->type( in(2) )->higher_equal(TypeLong::ONE)) ? in(1) : this;\n+}\n+\/\/------------------------------Value------------------------------------------\n+\/\/ A UDivLNode divides its inputs.  The third input is a Control input, used to\n+\/\/ prevent hoisting the divide above an unsafe test.\n+const Type* UDivLNode::Value(PhaseGVN* phase) const {\n+  \/\/ Either input is TOP ==> the result is TOP\n+  const Type *t1 = phase->type( in(1) );\n+  const Type *t2 = phase->type( in(2) );\n+  if( t1 == Type::TOP ) return Type::TOP;\n+  if( t2 == Type::TOP ) return Type::TOP;\n+\n+  \/\/ x\/x == 1 since we always generate the dynamic divisor check for 0.\n+  if (in(1) == in(2)) {\n+    return TypeLong::ONE;\n+  }\n+\n+  \/\/ Either input is BOTTOM ==> the result is the local BOTTOM\n+  const Type *bot = bottom_type();\n+  if( (t1 == bot) || (t2 == bot) ||\n+      (t1 == Type::BOTTOM) || (t2 == Type::BOTTOM) )\n+    return bot;\n+\n+  \/\/ Otherwise we give up all hope\n+  return TypeLong::LONG;\n+}\n+\n+\/\/------------------------------Idealize---------------------------------------\n+Node *UDivLNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ Check for dead control input\n+  if (in(0) && remove_dead_region(phase, can_reshape))  return this;\n+  return NULL;\n+}\n+\n+\n@@ -1008,0 +1086,7 @@\n+\/\/=============================================================================\n+\/\/------------------------------Idealize---------------------------------------\n+Node *UModINode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ Check for dead control input\n+  if( in(0) && remove_dead_region(phase, can_reshape) )  return this;\n+  return NULL;\n+}\n@@ -1219,0 +1304,8 @@\n+\/\/=============================================================================\n+\/\/------------------------------Idealize---------------------------------------\n+Node *UModLNode::Ideal(PhaseGVN *phase, bool can_reshape) {\n+  \/\/ Check for dead control input\n+  if( in(0) && remove_dead_region(phase, can_reshape) )  return this;\n+  return NULL;\n+}\n+\n@@ -1323,0 +1416,53 @@\n+\n+\/\/------------------------------make------------------------------------------\n+UDivModINode* UDivModINode::make(Node* div_or_mod) {\n+  Node* n = div_or_mod;\n+  assert(n->Opcode() == Op_UDivI || n->Opcode() == Op_UModI,\n+         \"only div or mod input pattern accepted\");\n+\n+  UDivModINode* divmod = new UDivModINode(n->in(0), n->in(1), n->in(2));\n+  Node*        dproj  = new ProjNode(divmod, DivModNode::div_proj_num);\n+  Node*        mproj  = new ProjNode(divmod, DivModNode::mod_proj_num);\n+  return divmod;\n+}\n+\n+\/\/------------------------------make------------------------------------------\n+UDivModLNode* UDivModLNode::make(Node* div_or_mod) {\n+  Node* n = div_or_mod;\n+  assert(n->Opcode() == Op_UDivL || n->Opcode() == Op_UModL,\n+         \"only div or mod input pattern accepted\");\n+\n+  UDivModLNode* divmod = new UDivModLNode(n->in(0), n->in(1), n->in(2));\n+  Node*        dproj  = new ProjNode(divmod, DivModNode::div_proj_num);\n+  Node*        mproj  = new ProjNode(divmod, DivModNode::mod_proj_num);\n+  return divmod;\n+}\n+\n+\/\/------------------------------match------------------------------------------\n+\/\/ return result(s) along with their RegMask info\n+Node* UDivModINode::match(const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n+  uint ideal_reg = proj->ideal_reg();\n+  RegMask rm;\n+  if (proj->_con == div_proj_num) {\n+    rm = match->divI_proj_mask();\n+  } else {\n+    assert(proj->_con == mod_proj_num, \"must be div or mod projection\");\n+    rm = match->modI_proj_mask();\n+  }\n+  return new MachProjNode(this, proj->_con, rm, ideal_reg);\n+}\n+\n+\n+\/\/------------------------------match------------------------------------------\n+\/\/ return result(s) along with their RegMask info\n+Node* UDivModLNode::match( const ProjNode* proj, const Matcher* match, const RegMask* mask) {\n+  uint ideal_reg = proj->ideal_reg();\n+  RegMask rm;\n+  if (proj->_con == div_proj_num) {\n+    rm = match->divL_proj_mask();\n+  } else {\n+    assert(proj->_con == mod_proj_num, \"must be div or mod projection\");\n+    rm = match->modL_proj_mask();\n+  }\n+  return new MachProjNode(this, proj->_con, rm, ideal_reg);\n+}\n","filename":"src\/hotspot\/share\/opto\/divnode.cpp","additions":146,"deletions":0,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -93,0 +93,26 @@\n+\/\/------------------------------UDivINode---------------------------------------\n+\/\/ Unsigned integer division\n+class UDivINode : public Node {\n+public:\n+  UDivINode( Node *c, Node *dividend, Node *divisor ) : Node(c, dividend, divisor ) {}\n+  virtual int Opcode() const;\n+  virtual Node* Identity(PhaseGVN* phase);\n+  virtual const Type* Value(PhaseGVN* phase) const;\n+  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual uint ideal_reg() const { return Op_RegI; }\n+};\n+\n+\/\/------------------------------UDivLNode---------------------------------------\n+\/\/ Unsigned long division\n+class UDivLNode : public Node {\n+public:\n+  UDivLNode( Node *c, Node *dividend, Node *divisor ) : Node(c, dividend, divisor ) {}\n+  virtual int Opcode() const;\n+  virtual Node* Identity(PhaseGVN* phase);\n+  virtual const Type* Value(PhaseGVN* phase) const;\n+  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual const Type *bottom_type() const { return TypeLong::LONG; }\n+  virtual uint ideal_reg() const { return Op_RegL; }\n+};\n+\n@@ -139,0 +165,22 @@\n+\/\/------------------------------UModINode---------------------------------------\n+\/\/ Unsigned integer modulus\n+class UModINode : public Node {\n+public:\n+  UModINode( Node *c, Node *in1, Node *in2 ) : Node(c,in1, in2) {}\n+  virtual int Opcode() const;\n+  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual uint ideal_reg() const { return Op_RegI; }\n+};\n+\n+\/\/------------------------------UModLNode---------------------------------------\n+\/\/ Unsigned long modulus\n+class UModLNode : public Node {\n+public:\n+  UModLNode( Node *c, Node *in1, Node *in2 ) : Node(c,in1, in2) {}\n+  virtual int Opcode() const;\n+  virtual Node *Ideal(PhaseGVN *phase, bool can_reshape);\n+  virtual const Type *bottom_type() const { return TypeLong::LONG; }\n+  virtual uint ideal_reg() const { return Op_RegL; }\n+};\n+\n@@ -187,0 +235,27 @@\n+\n+\/\/------------------------------UDivModINode---------------------------------------\n+\/\/ Unsigend integer division with remainder result.\n+class UDivModINode : public DivModNode {\n+public:\n+  UDivModINode( Node *c, Node *dividend, Node *divisor ) : DivModNode(c, dividend, divisor) {}\n+  virtual int Opcode() const;\n+  virtual const Type *bottom_type() const { return TypeTuple::INT_PAIR; }\n+  virtual Node* match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n+\n+  \/\/ Make a divmod and associated projections from a div or mod.\n+  static UDivModINode* make(Node* div_or_mod);\n+};\n+\n+\/\/------------------------------UDivModLNode---------------------------------------\n+\/\/ Unsigned long division with remainder result.\n+class UDivModLNode : public DivModNode {\n+public:\n+  UDivModLNode( Node *c, Node *dividend, Node *divisor ) : DivModNode(c, dividend, divisor) {}\n+  virtual int Opcode() const;\n+  virtual const Type *bottom_type() const { return TypeTuple::LONG_PAIR; }\n+  virtual Node* match(const ProjNode* proj, const Matcher* m, const RegMask* mask);\n+\n+  \/\/ Make a divmod and associated projections from a div or mod.\n+  static UDivModLNode* make(Node* div_or_mod);\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/divnode.hpp","additions":76,"deletions":1,"binary":false,"changes":77,"status":"modified"},{"patch":"@@ -45,0 +45,4 @@\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JFR\n+#include \"jfr\/jfr.hpp\"\n+#endif\n@@ -515,0 +519,1 @@\n+  JFR_ONLY(Jfr::on_resolution(this, holder, orig_callee);)\n@@ -871,1 +876,1 @@\n-        extype->klass()->print_name();\n+        extype->instance_klass()->print_name();\n@@ -881,1 +886,1 @@\n-                    extype->klass(), \"!loaded exception\");\n+                    extype->instance_klass(), \"!loaded exception\");\n@@ -928,1 +933,1 @@\n-                                    ex_type->klass()->as_instance_klass(),\n+                                    ex_type->instance_klass(),\n@@ -1150,1 +1155,1 @@\n-  ciInstanceKlass* receiver_klass = receiver_type->klass()->as_instance_klass();\n+  ciInstanceKlass* receiver_klass = receiver_type->is_instptr()->instance_klass();\n","filename":"src\/hotspot\/share\/opto\/doCall.cpp","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -183,1 +183,1 @@\n-    \/\/ Collect some interesting nodes for futher use.\n+    \/\/ Collect some interesting nodes for further use.\n@@ -247,0 +247,1 @@\n+    NOT_PRODUCT(escape_state_statistics(java_objects_worklist);)\n@@ -276,1 +277,4 @@\n-  if (C->failing())  return false;\n+  if (C->failing()) {\n+    NOT_PRODUCT(escape_state_statistics(java_objects_worklist);)\n+    return false;\n+  }\n@@ -284,0 +288,1 @@\n+    NOT_PRODUCT(escape_state_statistics(java_objects_worklist);)\n@@ -353,1 +358,4 @@\n-    if (C->failing())  return false;\n+    if (C->failing()) {\n+      NOT_PRODUCT(escape_state_statistics(java_objects_worklist);)\n+      return false;\n+    }\n@@ -389,0 +397,1 @@\n+  NOT_PRODUCT(escape_state_statistics(java_objects_worklist);)\n@@ -679,0 +688,18 @@\n+    case Op_Blackhole: {\n+      \/\/ All blackhole pointer arguments are globally escaping.\n+      \/\/ Only do this if there is at least one pointer argument.\n+      \/\/ Do not add edges during first iteration because some could be\n+      \/\/ not defined yet, defer to final step.\n+      for (uint i = 0; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in != nullptr) {\n+          const Type* at = _igvn->type(in);\n+          if (!at->isa_ptr()) continue;\n+\n+          add_local_var(n, PointsToNode::GlobalEscape);\n+          delayed_worklist->push(n);\n+          break;\n+        }\n+      }\n+      break;\n+    }\n@@ -829,0 +856,20 @@\n+    case Op_Blackhole: {\n+      \/\/ All blackhole pointer arguments are globally escaping.\n+      for (uint i = 0; i < n->req(); i++) {\n+        Node* in = n->in(i);\n+        if (in != nullptr) {\n+          const Type* at = _igvn->type(in);\n+          if (!at->isa_ptr()) continue;\n+\n+          if (in->is_AddP()) {\n+            in = get_addp_base(in);\n+          }\n+\n+          PointsToNode* ptn = ptnode_adr(in->_idx);\n+          assert(ptn != nullptr, \"should be defined already\");\n+          set_escape_state(ptn, PointsToNode::GlobalEscape NOT_PRODUCT(COMMA \"blackhole\"));\n+          add_edge(n_ptn, ptn);\n+        }\n+      }\n+      break;\n+    }\n@@ -932,1 +979,0 @@\n-    ciKlass* cik = kt->klass();\n@@ -937,1 +983,1 @@\n-      if (!cik->is_array_klass()) { \/\/ StressReflectiveCode\n+      if (!kt->isa_aryklassptr()) { \/\/ StressReflectiveCode\n@@ -952,5 +998,1 @@\n-      if (cik->is_subclass_of(_compile->env()->Thread_klass()) ||\n-          cik->is_subclass_of(_compile->env()->Reference_klass()) ||\n-         !cik->is_instance_klass() || \/\/ StressReflectiveCode\n-         !cik->as_instance_klass()->can_be_instantiated() ||\n-          cik->as_instance_klass()->has_finalizer()) {\n+      if (!kt->isa_instklassptr()) { \/\/ StressReflectiveCode\n@@ -959,5 +1001,14 @@\n-        int nfields = cik->as_instance_klass()->nof_nonstatic_fields();\n-        if (nfields > EliminateAllocationFieldsLimit) {\n-          \/\/ Not scalar replaceable if there are too many fields.\n-          scalar_replaceable = false;\n-          NOT_PRODUCT(nsr_reason = \"has too many fields\");\n+        const TypeInstKlassPtr* ikt = kt->is_instklassptr();\n+        ciInstanceKlass* ik = ikt->klass_is_exact() ? ikt->exact_klass()->as_instance_klass() : ikt->instance_klass();\n+        if (ik->is_subclass_of(_compile->env()->Thread_klass()) ||\n+            ik->is_subclass_of(_compile->env()->Reference_klass()) ||\n+            !ik->can_be_instantiated() ||\n+            ik->has_finalizer()) {\n+          es = PointsToNode::GlobalEscape;\n+        } else {\n+          int nfields = ik->as_instance_klass()->nof_nonstatic_fields();\n+          if (nfields > EliminateAllocationFieldsLimit) {\n+            \/\/ Not scalar replaceable if there are too many fields.\n+            scalar_replaceable = false;\n+            NOT_PRODUCT(nsr_reason = \"has too many fields\");\n+          }\n@@ -1105,2 +1156,2 @@\n-                              (aat->isa_oopptr()->klass() == NULL || aat->isa_instptr() ||\n-                               (aat->isa_aryptr() && aat->isa_aryptr()->klass()->is_obj_array_klass()) ||\n+                              (aat->isa_instptr() ||\n+                               (aat->isa_aryptr() && (aat->isa_aryptr()->elem() == Type::BOTTOM || aat->isa_aryptr()->elem()->make_oopptr() != NULL)) ||\n@@ -1108,2 +1159,2 @@\n-                                aat->isa_aryptr()->is_flat() &&\n-                                aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n+                                                               aat->isa_aryptr()->is_flat() &&\n+                                                               aat->isa_aryptr()->elem()->inline_klass()->contains_oops()));\n@@ -1590,1 +1641,1 @@\n-    \/\/ accessable by uses (loads) of fields of destination objects.\n+    \/\/ accessible by uses (loads) of fields of destination objects.\n@@ -2252,2 +2303,2 @@\n-        (adr_type->isa_aryptr()->klass() == NULL) ||\n-         adr_type->isa_aryptr()->klass()->is_obj_array_klass()) {\n+        adr_type->isa_aryptr()->elem() == Type::BOTTOM ||\n+        adr_type->isa_aryptr()->elem()->make_oopptr() != NULL) {\n@@ -2655,1 +2706,1 @@\n-      !base_t->klass()->is_subtype_of(t->klass())) {\n+      !base_t->maybe_java_subtype_of(t)) {\n@@ -2832,2 +2883,2 @@\n-      !(toop->klass() != NULL &&\n-        toop->klass()->is_java_lang_Object() &&\n+      !(toop->isa_instptr() &&\n+        toop->is_instptr()->instance_klass()->is_java_lang_Object() &&\n@@ -3350,1 +3401,1 @@\n-        if (tn_t != NULL && tinst->klass()->is_subtype_of(tn_t->klass())) {\n+        if (tn_t != NULL && tinst->maybe_java_subtype_of(tn_t)) {\n@@ -3370,1 +3421,1 @@\n-                 tn_t != NULL && !tinst->klass()->is_subtype_of(tn_t->klass()),\n+                 tn_t != NULL && !tinst->maybe_java_subtype_of(tn_t),\n@@ -3724,0 +3775,4 @@\n+int ConnectionGraph::_no_escape_counter = 0;\n+int ConnectionGraph::_arg_escape_counter = 0;\n+int ConnectionGraph::_global_escape_counter = 0;\n+\n@@ -3832,0 +3887,24 @@\n+void ConnectionGraph::print_statistics() {\n+  tty->print_cr(\"No escape = %d, Arg escape = %d, Global escape = %d\", Atomic::load(&_no_escape_counter), Atomic::load(&_arg_escape_counter), Atomic::load(&_global_escape_counter));\n+}\n+\n+void ConnectionGraph::escape_state_statistics(GrowableArray<JavaObjectNode*>& java_objects_worklist) {\n+  if (!PrintOptoStatistics || (_invocation > 0)) { \/\/ Collect data only for the first invocation\n+    return;\n+  }\n+  for (int next = 0; next < java_objects_worklist.length(); ++next) {\n+    JavaObjectNode* ptn = java_objects_worklist.at(next);\n+    if (ptn->ideal_node()->is_Allocate()) {\n+      if (ptn->escape_state() == PointsToNode::NoEscape) {\n+        Atomic::inc(&ConnectionGraph::_no_escape_counter);\n+      } else if (ptn->escape_state() == PointsToNode::ArgEscape) {\n+        Atomic::inc(&ConnectionGraph::_arg_escape_counter);\n+      } else if (ptn->escape_state() == PointsToNode::GlobalEscape) {\n+        Atomic::inc(&ConnectionGraph::_global_escape_counter);\n+      } else {\n+        assert(false, \"Unexpected Escape State\");\n+      }\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/escape.cpp","additions":107,"deletions":28,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -514,1 +514,1 @@\n-  \/\/ In the comparision below, add one to account for the control input,\n+  \/\/ In the comparison below, add one to account for the control input,\n","filename":"src\/hotspot\/share\/opto\/gcm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"ci\/ciNativeEntryPoint.hpp\"\n@@ -541,1 +540,1 @@\n-void GraphKit::builtin_throw(Deoptimization::DeoptReason reason, Node* arg) {\n+void GraphKit::builtin_throw(Deoptimization::DeoptReason reason) {\n@@ -1225,1 +1224,1 @@\n-      \/\/ do not transfrom ccast here, it might convert to top node for\n+      \/\/ do not transform ccast here, it might convert to top node for\n@@ -1305,1 +1304,1 @@\n-      if (tp != NULL && tp->klass() != NULL && !tp->klass()->is_loaded()\n+      if (tp != NULL && !tp->is_loaded()\n@@ -1319,0 +1318,1 @@\n+        ciKlass* klass = tp->unloaded_klass();\n@@ -1320,1 +1320,1 @@\n-        if (WizardMode) { tty->print(\"Null check of unloaded \"); tp->klass()->print(); tty->cr(); }\n+        if (WizardMode) { tty->print(\"Null check of unloaded \"); klass->print(); tty->cr(); }\n@@ -1324,1 +1324,1 @@\n-                      tp->klass(), \"!loaded\");\n+                      klass, \"!loaded\");\n@@ -1622,8 +1622,1 @@\n-  Node* ld;\n-  if (require_atomic_access && bt == T_LONG) {\n-    ld = LoadLNode::make_atomic(ctl, mem, adr, adr_type, t, mo, control_dependency, unaligned, mismatched, unsafe, barrier_data);\n-  } else if (require_atomic_access && bt == T_DOUBLE) {\n-    ld = LoadDNode::make_atomic(ctl, mem, adr, adr_type, t, mo, control_dependency, unaligned, mismatched, unsafe, barrier_data);\n-  } else {\n-    ld = LoadNode::make(_gvn, ctl, mem, adr, adr_type, t, bt, mo, control_dependency, unaligned, mismatched, unsafe, barrier_data);\n-  }\n+  Node* ld = LoadNode::make(_gvn, ctl, mem, adr, adr_type, t, bt, mo, control_dependency, require_atomic_access, unaligned, mismatched, unsafe, barrier_data);\n@@ -1650,8 +1643,1 @@\n-  Node* st;\n-  if (require_atomic_access && bt == T_LONG) {\n-    st = StoreLNode::make_atomic(ctl, mem, adr, adr_type, val, mo);\n-  } else if (require_atomic_access && bt == T_DOUBLE) {\n-    st = StoreDNode::make_atomic(ctl, mem, adr, adr_type, val, mo);\n-  } else {\n-    st = StoreNode::make(_gvn, ctl, mem, adr, adr_type, val, bt, mo);\n-  }\n+  Node* st = StoreNode::make(_gvn, ctl, mem, adr, adr_type, val, bt, mo, require_atomic_access);\n@@ -1716,1 +1702,1 @@\n-                               Node* adr,   \/\/ actual adress to store val at\n+                               Node* adr,   \/\/ actual address to store val at\n@@ -1735,1 +1721,1 @@\n-Node* GraphKit::access_load(Node* adr,   \/\/ actual adress to load val at\n+Node* GraphKit::access_load(Node* adr,   \/\/ actual address to load val at\n@@ -2181,1 +2167,1 @@\n-void GraphKit::uncommon_trap(int trap_request,\n+Node* GraphKit::uncommon_trap(int trap_request,\n@@ -2186,1 +2172,1 @@\n-  if (stopped())  return; \/\/ trap reachable?\n+  if (stopped())  return NULL; \/\/ trap reachable?\n@@ -2302,0 +2288,1 @@\n+  return call;\n@@ -2736,113 +2723,0 @@\n-\/\/-----------------------------make_native_call-------------------------------\n-Node* GraphKit::make_native_call(address call_addr, const TypeFunc* call_type, uint nargs, ciNativeEntryPoint* nep) {\n-  \/\/ Select just the actual call args to pass on\n-  \/\/ [MethodHandle fallback, long addr, HALF addr, ... args , NativeEntryPoint nep]\n-  \/\/                                             |          |\n-  \/\/                                             V          V\n-  \/\/                                             [ ... args ]\n-  uint n_filtered_args = nargs - 4; \/\/ -fallback, -addr (2), -nep;\n-  ResourceMark rm;\n-  Node** argument_nodes = NEW_RESOURCE_ARRAY(Node*, n_filtered_args);\n-  const Type** arg_types = TypeTuple::fields(n_filtered_args);\n-  GrowableArray<VMReg> arg_regs(C->comp_arena(), n_filtered_args, n_filtered_args, VMRegImpl::Bad());\n-\n-  VMReg* argRegs = nep->argMoves();\n-  {\n-    for (uint vm_arg_pos = 0, java_arg_read_pos = 0;\n-        vm_arg_pos < n_filtered_args; vm_arg_pos++) {\n-      uint vm_unfiltered_arg_pos = vm_arg_pos + 3; \/\/ +3 to skip fallback handle argument and addr (2 since long)\n-      Node* node = argument(vm_unfiltered_arg_pos);\n-      const Type* type = call_type->domain_sig()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n-      VMReg reg = type == Type::HALF\n-        ? VMRegImpl::Bad()\n-        : argRegs[java_arg_read_pos++];\n-\n-      argument_nodes[vm_arg_pos] = node;\n-      arg_types[TypeFunc::Parms + vm_arg_pos] = type;\n-      arg_regs.at_put(vm_arg_pos, reg);\n-    }\n-  }\n-\n-  uint n_returns = call_type->range_sig()->cnt() - TypeFunc::Parms;\n-  GrowableArray<VMReg> ret_regs(C->comp_arena(), n_returns, n_returns, VMRegImpl::Bad());\n-  const Type** ret_types = TypeTuple::fields(n_returns);\n-\n-  VMReg* retRegs = nep->returnMoves();\n-  {\n-    for (uint vm_ret_pos = 0, java_ret_read_pos = 0;\n-        vm_ret_pos < n_returns; vm_ret_pos++) { \/\/ 0 or 1\n-      const Type* type = call_type->range_sig()->field_at(TypeFunc::Parms + vm_ret_pos);\n-      VMReg reg = type == Type::HALF\n-        ? VMRegImpl::Bad()\n-        : retRegs[java_ret_read_pos++];\n-\n-      ret_regs.at_put(vm_ret_pos, reg);\n-      ret_types[TypeFunc::Parms + vm_ret_pos] = type;\n-    }\n-  }\n-\n-  const TypeFunc* new_call_type = TypeFunc::make(\n-    TypeTuple::make(TypeFunc::Parms + n_filtered_args, arg_types),\n-    TypeTuple::make(TypeFunc::Parms + n_returns, ret_types)\n-  );\n-\n-  if (nep->need_transition()) {\n-    RuntimeStub* invoker = SharedRuntime::make_native_invoker(call_addr,\n-                                                              nep->shadow_space(),\n-                                                              arg_regs, ret_regs);\n-    if (invoker == NULL) {\n-      C->record_failure(\"native invoker not implemented on this platform\");\n-      return NULL;\n-    }\n-    C->add_native_invoker(invoker);\n-    call_addr = invoker->code_begin();\n-  }\n-  assert(call_addr != NULL, \"sanity\");\n-\n-  CallNativeNode* call = new CallNativeNode(new_call_type, call_addr, nep->name(), TypePtr::BOTTOM,\n-                                            arg_regs,\n-                                            ret_regs,\n-                                            nep->shadow_space(),\n-                                            nep->need_transition());\n-\n-  if (call->_need_transition) {\n-    add_safepoint_edges(call);\n-  }\n-\n-  set_predefined_input_for_runtime_call(call);\n-\n-  for (uint i = 0; i < n_filtered_args; i++) {\n-    call->init_req(i + TypeFunc::Parms, argument_nodes[i]);\n-  }\n-\n-  Node* c = gvn().transform(call);\n-  assert(c == call, \"cannot disappear\");\n-\n-  set_predefined_output_for_runtime_call(call);\n-\n-  Node* ret;\n-  if (method() == NULL || method()->return_type()->basic_type() == T_VOID) {\n-    ret = top();\n-  } else {\n-    ret =  gvn().transform(new ProjNode(call, TypeFunc::Parms));\n-    \/\/ Unpack native results if needed\n-    \/\/ Need this method type since it's unerased\n-    switch (nep->method_type()->rtype()->basic_type()) {\n-      case T_CHAR:\n-        ret = _gvn.transform(new AndINode(ret, _gvn.intcon(0xFFFF)));\n-        break;\n-      case T_BYTE:\n-        ret = sign_extend_byte(ret);\n-        break;\n-      case T_SHORT:\n-        ret = sign_extend_short(ret);\n-        break;\n-      default: \/\/ do nothing\n-        break;\n-    }\n-  }\n-\n-  push_node(method()->return_type()->basic_type(), ret);\n-\n-  return call;\n-}\n@@ -2952,2 +2826,2 @@\n-    ciKlass* superk = gvn.type(superklass)->is_klassptr()->klass();\n-    ciKlass* subk   = gvn.type(subklass)->is_klassptr()->klass();\n+    const TypeKlassPtr* superk = gvn.type(superklass)->is_klassptr();\n+    const TypeKlassPtr* subk   = gvn.type(subklass)->is_klassptr();\n@@ -3292,1 +3166,1 @@\n-                                             ciKlass* require_klass,\n+                                             const TypeKlassPtr* require_klass,\n@@ -3323,1 +3197,1 @@\n-        C->static_subtype_check(require_klass, exact_kls) == Compile::SSC_always_true) {\n+        C->static_subtype_check(require_klass, TypeKlassPtr::make(exact_kls)) == Compile::SSC_always_true) {\n@@ -3455,2 +3329,2 @@\n-      ciKlass* superk = _gvn.type(superklass)->is_klassptr()->klass();\n-      ciKlass* subk = _gvn.type(obj)->is_oopptr()->klass();\n+      const TypeKlassPtr* superk = _gvn.type(superklass)->is_klassptr();\n+      const TypeKlassPtr* subk = _gvn.type(obj)->is_oopptr()->as_klass_type();\n@@ -3518,1 +3392,1 @@\n-  const TypeOopPtr* toop = TypeOopPtr::make_from_klass(tk->klass());\n+  const TypeOopPtr* toop = tk->cast_to_exactness(false)->as_instance_type();\n@@ -3530,8 +3404,7 @@\n-    ciKlass* klass = NULL;\n-    if (obj->is_InlineTypeBase()) {\n-      klass = _gvn.type(obj)->inline_klass();\n-    } else {\n-      const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-      if (objtp != NULL) {\n-        klass = objtp->klass();\n-      }\n+    const TypeKlassPtr* kptr = NULL;\n+    const Type* t = _gvn.type(obj);\n+    if (t->isa_oop_ptr()) {\n+      kptr = t->is_oopptr()->as_klass_type();\n+    } else if (obj->is_InlineTypeBase()) {\n+      ciInlineKlass* vk = t->inline_klass();\n+      kptr = TypeInstKlassPtr::make(TypePtr::NotNull, vk, Type::Offset(0), vk->flatten_array());\n@@ -3539,2 +3412,2 @@\n-    if (klass != NULL) {\n-      switch (C->static_subtype_check(tk->klass(), klass)) {\n+    if (kptr != NULL) {\n+      switch (C->static_subtype_check(tk, kptr)) {\n@@ -3560,2 +3433,1 @@\n-        const TypeOopPtr* objtp = _gvn.type(obj)->isa_oopptr();\n-        if (objtp != NULL && !objtp->maybe_null()) {\n+        if (t->isa_oopptr() != NULL && !t->is_oopptr()->maybe_null()) {\n@@ -3565,1 +3437,1 @@\n-          builtin_throw(reason, makecon(TypeKlassPtr::make(klass)));\n+          builtin_throw(reason);\n@@ -3571,0 +3443,2 @@\n+      default:\n+        break;\n@@ -3641,1 +3515,1 @@\n-      cast_obj = maybe_cast_profiled_receiver(not_null_obj, tk->klass(), spec_obj_type, safe_for_replace);\n+      cast_obj = maybe_cast_profiled_receiver(not_null_obj, tk, spec_obj_type, safe_for_replace);\n@@ -3671,1 +3545,1 @@\n-        builtin_throw(reason, obj_klass);\n+        builtin_throw(reason);\n@@ -3999,2 +3873,0 @@\n-    ciKlass* klass = inst_klass->klass();\n-    assert(klass != NULL, \"klass should not be NULL\");\n@@ -4003,2 +3875,3 @@\n-    if (UseFlatArray && klass->is_obj_array_klass() && !klass->as_obj_array_klass()->is_elem_null_free()) {\n-      \/\/ The runtime type of [LMyValue might be [QMyValue due to [QMyValue <: [LMyValue.\n+    ciKlass* klass = inst_klass->klass();\n+    if (UseFlatArray && !xklass && klass->is_obj_array_klass() && !klass->as_obj_array_klass()->is_elem_null_free()) {\n+      \/\/ The runtime type of [LMyValue might be [QMyValue due to [QMyValue <: [LMyValue. Don't constant fold.\n@@ -4009,1 +3882,12 @@\n-      jint lhelper = klass->layout_helper();\n+      jint lhelper;\n+      if (klass->is_flat_array_klass()) {\n+        lhelper = klass->layout_helper();\n+      } else if (inst_klass->isa_aryklassptr()) {\n+        BasicType elem = inst_klass->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+        if (is_reference_type(elem, true)) {\n+          elem = T_OBJECT;\n+        }\n+        lhelper = Klass::array_layout_helper(elem);\n+      } else {\n+        lhelper = inst_klass->is_instklassptr()->exact_klass()->layout_helper();\n+      }\n@@ -4111,1 +3995,1 @@\n-      ciInstanceKlass* ik = oop_type->klass()->as_instance_klass();\n+      ciInstanceKlass* ik = oop_type->is_instptr()->instance_klass();\n@@ -4156,1 +4040,1 @@\n-\/\/  - If 'return_size_val', report the the total object size to the caller.\n+\/\/  - If 'return_size_val', report the total object size to the caller.\n@@ -4390,2 +4274,2 @@\n-  if (ary_type->klass()->is_array_klass()) {\n-    BasicType bt = ary_type->klass()->as_array_klass()->element_type()->basic_type();\n+  if (ary_type->isa_aryptr()) {\n+    BasicType bt = ary_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -4434,1 +4318,1 @@\n-    Node* val = access_load_at(elem_mirror, default_value_addr, _gvn.type(default_value_addr)->is_ptr(), TypeInstPtr::BOTTOM, T_OBJECT, IN_HEAP);\n+    Node* val = access_load_at(elem_mirror, default_value_addr, TypeInstPtr::MIRROR, TypeInstPtr::NOTNULL, T_OBJECT, IN_HEAP);\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":55,"deletions":171,"binary":false,"changes":226,"status":"modified"},{"patch":"@@ -278,6 +278,2 @@\n-  \/\/ Range checks take the offending index.\n-  \/\/ Cast and array store checks take the offending class.\n-  \/\/ Others do not take the optional argument.\n-  \/\/ The JVMS must allow the bytecode to be re-executed\n-  \/\/ via an uncommon trap.\n-  void builtin_throw(Deoptimization::DeoptReason reason, Node* arg = NULL);\n+  \/\/ The JVMS must allow the bytecode to be re-executed via an uncommon trap.\n+  void builtin_throw(Deoptimization::DeoptReason reason);\n@@ -436,1 +432,1 @@\n-                                     ciKlass* require_klass,\n+                                     const TypeKlassPtr* require_klass,\n@@ -605,1 +601,1 @@\n-                        Node* adr,   \/\/ actual adress to store val at\n+                        Node* adr,   \/\/ actual address to store val at\n@@ -614,1 +610,1 @@\n-                       Node* adr,   \/\/ actual adress to load val at\n+                       Node* adr,   \/\/ actual address to load val at\n@@ -621,1 +617,1 @@\n-  Node* access_load(Node* adr,   \/\/ actual adress to load val at\n+  Node* access_load(Node* adr,   \/\/ actual address to load val at\n@@ -762,1 +758,1 @@\n-  void uncommon_trap(int trap_request,\n+  Node* uncommon_trap(int trap_request,\n@@ -767,1 +763,1 @@\n-  void uncommon_trap(Deoptimization::DeoptReason reason,\n+  Node* uncommon_trap(Deoptimization::DeoptReason reason,\n@@ -771,1 +767,1 @@\n-    uncommon_trap(Deoptimization::make_trap_request(reason, action),\n+    return uncommon_trap(Deoptimization::make_trap_request(reason, action),\n@@ -776,1 +772,1 @@\n-  void uncommon_trap_exact(Deoptimization::DeoptReason reason,\n+  Node* uncommon_trap_exact(Deoptimization::DeoptReason reason,\n@@ -780,1 +776,1 @@\n-    uncommon_trap(Deoptimization::make_trap_request(reason, action),\n+    return uncommon_trap(Deoptimization::make_trap_request(reason, action),\n@@ -834,2 +830,0 @@\n-  Node* make_native_call(address call_addr, const TypeFunc* call_type, uint nargs, ciNativeEntryPoint* nep);\n-\n","filename":"src\/hotspot\/share\/opto\/graphKit.hpp","additions":11,"deletions":17,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -89,1 +89,1 @@\n-  \/\/ Delay gvn.tranform on if-nodes until construction is finished\n+  \/\/ Delay gvn.transform on if-nodes until construction is finished\n@@ -363,6 +363,1 @@\n-  Node* ld;\n-  if (require_atomic_access && bt == T_LONG) {\n-    ld = LoadLNode::make_atomic(ctl, mem, adr, adr_type, t, mo);\n-  } else {\n-    ld = LoadNode::make(_gvn, ctl, mem, adr, adr_type, t, bt, mo);\n-  }\n+  Node* ld = LoadNode::make(_gvn, ctl, mem, adr, adr_type, t, bt, mo, LoadNode::DependsOnlyOnTest, require_atomic_access);\n@@ -380,6 +375,1 @@\n-  Node* st;\n-  if (require_atomic_access && bt == T_LONG) {\n-    st = StoreLNode::make_atomic(ctl, mem, adr, adr_type, val, mo);\n-  } else {\n-    st = StoreNode::make(_gvn, ctl, mem, adr, adr_type, val, bt, mo);\n-  }\n+  Node* st = StoreNode::make(_gvn, ctl, mem, adr, adr_type, val, bt, mo, require_atomic_access);\n","filename":"src\/hotspot\/share\/opto\/idealKit.cpp","additions":4,"deletions":14,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -189,1 +189,1 @@\n-          tty->print_cr(\"Phi has use I cant be bothered with\");\n+          tty->print_cr(\"Phi has use I can't be bothered with\");\n@@ -841,1 +841,3 @@\n-          !igvn->C->too_many_traps(dom_method, dom_bci, Deoptimization::Reason_range_check)) {\n+          !igvn->C->too_many_traps(dom_method, dom_bci, Deoptimization::Reason_range_check) &&\n+          \/\/ Return true if c2 manages to reconcile with UnstableIf optimization. See the comments for it.\n+          igvn->C->remove_unstable_if_trap(dom_unc, true\/*yield*\/)) {\n@@ -1806,1 +1808,1 @@\n-  \/\/ CountedLoopEnds want the back-control test to be TRUE, irregardless of\n+  \/\/ CountedLoopEnds want the back-control test to be TRUE, regardless of\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -895,1 +895,1 @@\n-        if (vtptr == NULL || !vtptr->klass()->equals(vk)) {\n+        if (vtptr == NULL || !vtptr->instance_klass()->equals(vk)) {\n","filename":"src\/hotspot\/share\/opto\/inlinetypenode.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,1 +55,1 @@\n-      \/\/ We should not reach here exept for PPC\/AIX, as os::zero_page_read_protected()\n+      \/\/ We should not reach here except for PPC\/AIX, as os::zero_page_read_protected()\n@@ -279,1 +279,1 @@\n-        offset += tptr->offset(); \/\/ correct if base is offseted\n+        offset += tptr->offset(); \/\/ correct if base is offsetted\n@@ -333,19 +333,21 @@\n-    if( was_store ) {\n-      Block *b = mb;            \/\/ Start searching here for a local load\n-      \/\/ mach use (faulting) trying to hoist\n-      \/\/ n might be blocker to hoisting\n-      while( b != block ) {\n-        uint k;\n-        for( k = 1; k < b->number_of_nodes(); k++ ) {\n-          Node *n = b->get_node(k);\n-          if( n->needs_anti_dependence_check() &&\n-              n->in(LoadNode::Memory) == mach->in(StoreNode::Memory) )\n-            break;              \/\/ Found anti-dependent load\n-        }\n-        if( k < b->number_of_nodes() )\n-          break;                \/\/ Found anti-dependent load\n-        \/\/ Make sure control does not do a merge (would have to check allpaths)\n-        if( b->num_preds() != 2 ) break;\n-        b = get_block_for_node(b->pred(1)); \/\/ Move up to predecessor block\n-      }\n-      if( b != block ) continue;\n+    if (was_store) {\n+       \/\/ Make sure control does not do a merge (would have to check allpaths)\n+       if (mb->num_preds() != 2) {\n+         continue;\n+       }\n+       \/\/ mach is a store, hence block is the immediate dominator of mb.\n+       \/\/ Due to the null-check shape of block (where its successors cannot re-join),\n+       \/\/ block must be the direct predecessor of mb.\n+       assert(get_block_for_node(mb->pred(1)) == block, \"Unexpected predecessor block\");\n+       uint k;\n+       uint num_nodes = mb->number_of_nodes();\n+       for (k = 1; k < num_nodes; k++) {\n+         Node *n = mb->get_node(k);\n+         if (n->needs_anti_dependence_check() &&\n+             n->in(LoadNode::Memory) == mach->in(StoreNode::Memory)) {\n+           break;              \/\/ Found anti-dependent load\n+         }\n+       }\n+       if (k < num_nodes) {\n+         continue;             \/\/ Found anti-dependent load\n+       }\n@@ -516,3 +518,6 @@\n-\/\/ Select a nice fellow from the worklist to schedule next. If there is only\n-\/\/ one choice, then use it. Projections take top priority for correctness\n-\/\/ reasons - if I see a projection, then it is next.  There are a number of\n+\/\/ Select a nice fellow from the worklist to schedule next. If there is only one\n+\/\/ choice, then use it. CreateEx nodes that are initially ready must start their\n+\/\/ blocks and are given the highest priority, by being placed at the beginning\n+\/\/ of the worklist. Next after initially-ready CreateEx nodes are projections,\n+\/\/ which must follow their parents, and CreateEx nodes with local input\n+\/\/ dependencies. Next are constants and CheckCastPP nodes. There are a number of\n@@ -556,5 +561,6 @@\n-    if( n->is_Proj() ||         \/\/ Projections always win\n-        n->Opcode()== Op_Con || \/\/ So does constant 'Top'\n-        iop == Op_CreateEx ||   \/\/ Create-exception must start block\n-        iop == Op_CheckCastPP\n-        ) {\n+    if (iop == Op_CreateEx || n->is_Proj()) {\n+      \/\/ CreateEx nodes that are initially ready must start the block (after Phi\n+      \/\/ and Parm nodes which are pre-scheduled) and get top priority. This is\n+      \/\/ currently enforced by placing them at the beginning of the initial\n+      \/\/ worklist and selecting them eagerly here. After these, projections and\n+      \/\/ other CreateEx nodes are selected with equal priority.\n@@ -565,0 +571,12 @@\n+    if (n->Opcode() == Op_Con || iop == Op_CheckCastPP) {\n+      \/\/ Constants and CheckCastPP nodes have higher priority than the rest of\n+      \/\/ the nodes tested below. Record as current winner, but keep looking for\n+      \/\/ higher-priority nodes in the worklist.\n+      choice  = 4;\n+      \/\/ Latency and score are only used to break ties among low-priority nodes.\n+      latency = 0;\n+      score   = 0;\n+      idx     = i;\n+      continue;\n+    }\n+\n@@ -583,1 +601,1 @@\n-    uint n_choice  = 2;\n+    uint n_choice = 2;\n@@ -638,1 +656,1 @@\n-        \/\/ now caculate its effect upon the graph if we did\n+        \/\/ now calculate its effect upon the graph if we did\n@@ -909,6 +927,0 @@\n-    case Op_CallNative:\n-      \/\/ We use the c reg save policy here since Foreign Linker\n-      \/\/ only supports the C ABI currently.\n-      \/\/ TODO compute actual save policy based on nep->abi\n-      save_policy = _matcher._c_reg_save_policy;\n-      break;\n@@ -928,8 +940,1 @@\n-  \/\/\n-  \/\/ Also, native callees can not save oops, so we kill the SOE registers\n-  \/\/ here in case a native call has a safepoint. This doesn't work for\n-  \/\/ RBP though, which seems to be special-cased elsewhere to always be\n-  \/\/ treated as alive, so we instead manually save the location of RBP\n-  \/\/ before doing the native call (see NativeInvokerGenerator::generate).\n-  bool exclude_soe = op == Op_CallRuntime\n-    || (op == Op_CallNative && mcall->guaranteed_safepoint());\n+  bool exclude_soe = op == Op_CallRuntime;\n@@ -980,1 +985,1 @@\n-  \/\/ we know when a given instruction is avialable to be scheduled.\n+  \/\/ we know when a given instruction is available to be scheduled.\n@@ -1089,2 +1094,2 @@\n-        \/\/ Force the CreateEx to the top of the list so it's processed\n-        \/\/ first and ends up at the start of the block.\n+        \/\/ Place CreateEx nodes that are initially ready at the beginning of the\n+        \/\/ worklist so they are selected first and scheduled at the block start.\n","filename":"src\/hotspot\/share\/opto\/lcm.cpp","additions":54,"deletions":49,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,4 +61,0 @@\n-#if INCLUDE_JFR\n-#include \"jfr\/jfr.hpp\"\n-#endif\n-\n@@ -273,0 +269,2 @@\n+  case vmIntrinsics::_roundF:\n+  case vmIntrinsics::_roundD:\n@@ -478,0 +476,1 @@\n+  case vmIntrinsics::_currentCarrierThread:     return inline_native_currentCarrierThread();\n@@ -479,0 +478,4 @@\n+  case vmIntrinsics::_setCurrentThread:         return inline_native_setCurrentThread();\n+\n+  case vmIntrinsics::_extentLocalCache:          return inline_native_extentLocalCache();\n+  case vmIntrinsics::_setExtentLocalCache:       return inline_native_setExtentLocalCache();\n@@ -481,2 +484,1 @@\n-  case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, JFR_TIME_FUNCTION), \"counterTime\");\n-  case vmIntrinsics::_getClassId:               return inline_native_classID();\n+  case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, JfrTime::time_function()), \"counterTime\");\n@@ -525,0 +527,3 @@\n+  case vmIntrinsics::_floatIsInfinite:\n+  case vmIntrinsics::_doubleIsInfinite:         return inline_fp_range_check(intrinsic_id());\n+\n@@ -536,0 +541,13 @@\n+  case vmIntrinsics::_compress_i:\n+  case vmIntrinsics::_compress_l:\n+  case vmIntrinsics::_expand_i:\n+  case vmIntrinsics::_expand_l:                 return inline_bitshuffle_methods(intrinsic_id());\n+\n+  case vmIntrinsics::_compareUnsigned_i:\n+  case vmIntrinsics::_compareUnsigned_l:        return inline_compare_unsigned(intrinsic_id());\n+\n+  case vmIntrinsics::_divideUnsigned_i:\n+  case vmIntrinsics::_divideUnsigned_l:\n+  case vmIntrinsics::_remainderUnsigned_i:\n+  case vmIntrinsics::_remainderUnsigned_l:      return inline_divmod_methods(intrinsic_id());\n+\n@@ -635,0 +653,3 @@\n+  case vmIntrinsics::_Continuation_doYield:\n+    return inline_continuation_do_yield();\n+\n@@ -701,0 +722,2 @@\n+  case vmIntrinsics::_VectorCompressExpand:\n+    return inline_vector_compress_expand();\n@@ -756,1 +779,1 @@\n-    set_control(top()); \/\/ No fast path instrinsic\n+    set_control(top()); \/\/ No fast path intrinsic\n@@ -893,4 +916,6 @@\n-\/\/--------------------------generate_current_thread--------------------\n-Node* LibraryCallKit::generate_current_thread(Node* &tls_output) {\n-  ciKlass*    thread_klass = env()->Thread_klass();\n-  const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)->cast_to_ptr_type(TypePtr::NotNull);\n+Node* LibraryCallKit::current_thread_helper(Node*& tls_output, ByteSize handle_offset,\n+                                            bool is_immutable) {\n+  ciKlass* thread_klass = env()->Thread_klass();\n+  const Type* thread_type\n+    = TypeOopPtr::make_from_klass(thread_klass)->cast_to_ptr_type(TypePtr::NotNull);\n+\n@@ -898,1 +923,1 @@\n-  Node* p = basic_plus_adr(top()\/*!oop*\/, thread, in_bytes(JavaThread::threadObj_offset()));\n+  Node* p = basic_plus_adr(top()\/*!oop*\/, thread, in_bytes(handle_offset));\n@@ -900,1 +925,6 @@\n-  Node* thread_obj_handle = LoadNode::make(_gvn, NULL, immutable_memory(), p, p->bottom_type()->is_ptr(), TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);\n+\n+  Node* thread_obj_handle\n+    = (is_immutable\n+      ? LoadNode::make(_gvn, NULL, immutable_memory(), p, p->bottom_type()->is_ptr(),\n+        TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered)\n+      : make_load(NULL, p, p->bottom_type()->is_ptr(), T_ADDRESS, MemNode::unordered));\n@@ -902,1 +932,12 @@\n-  return access_load(thread_obj_handle, thread_type, T_OBJECT, IN_NATIVE | C2_IMMUTABLE_MEMORY);\n+\n+  DecoratorSet decorators = IN_NATIVE;\n+  if (is_immutable) {\n+    decorators |= C2_IMMUTABLE_MEMORY;\n+  }\n+  return access_load(thread_obj_handle, thread_type, T_OBJECT, decorators);\n+}\n+\n+\/\/--------------------------generate_current_thread--------------------\n+Node* LibraryCallKit::generate_current_thread(Node* &tls_output) {\n+  return current_thread_helper(tls_output, JavaThread::threadObj_offset(),\n+                               \/*is_immutable*\/false);\n@@ -905,0 +946,5 @@\n+\/\/--------------------------generate_virtual_thread--------------------\n+Node* LibraryCallKit::generate_virtual_thread(Node* tls_output) {\n+  return current_thread_helper(tls_output, JavaThread::vthread_offset(),\n+                               !C->method()->changes_current_thread());\n+}\n@@ -1069,1 +1115,1 @@\n-  \/\/ length is now known postive, add a cast node to make this explicit\n+  \/\/ length is now known positive, add a cast node to make this explicit\n@@ -1071,1 +1117,1 @@\n-  Node* casted_length = ConstraintCastNode::make(control(), length, TypeInteger::make(0, upper_bound, Type::WidenMax, bt), bt);\n+  Node* casted_length = ConstraintCastNode::make(control(), length, TypeInteger::make(0, upper_bound, Type::WidenMax, bt), ConstraintCastNode::RegularDependency, bt);\n@@ -1099,1 +1145,1 @@\n-  Node* result = ConstraintCastNode::make(control(), index, TypeInteger::make(0, upper_bound, Type::WidenMax, bt), bt);\n+  Node* result = ConstraintCastNode::make(control(), index, TypeInteger::make(0, upper_bound, Type::WidenMax, bt), ConstraintCastNode::RegularDependency, bt);\n@@ -1319,2 +1365,2 @@\n-  BasicType src_elem = src_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType dst_elem = dst_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType src_elem = src_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType dst_elem = dst_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -1590,1 +1636,1 @@\n-    ch = access_load_at(value, adr, TypeAryPtr::BYTES, TypeInt::CHAR, T_CHAR, IN_HEAP | MO_UNORDERED | C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD);\n+    ch = access_load_at(value, adr, TypeAryPtr::BYTES, TypeInt::CHAR, T_CHAR, IN_HEAP | MO_UNORDERED | C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD | C2_UNKNOWN_CONTROL_LOAD);\n@@ -1616,0 +1662,1 @@\n+\/\/ public static double Math.round(double)\n@@ -1627,0 +1674,1 @@\n+  case vmIntrinsics::_roundD: n = new RoundDNode(arg); break;\n@@ -1648,0 +1696,1 @@\n+  case vmIntrinsics::_roundF: n = new RoundFNode(arg); break;\n@@ -1763,0 +1812,1 @@\n+  case vmIntrinsics::_roundD: return Matcher::match_rule_supported(Op_RoundD) ? inline_double_math(id) : false;\n@@ -1766,0 +1816,1 @@\n+\n@@ -1785,0 +1836,1 @@\n+  case vmIntrinsics::_roundF: return Matcher::match_rule_supported(Op_RoundF) ? inline_math(id) : false;\n@@ -1796,8 +1848,0 @@\n-static bool is_simple_name(Node* n) {\n-  return (n->req() == 1         \/\/ constant\n-          || (n->is_Type() && n->as_Type()->type()->singleton())\n-          || n->is_Proj()       \/\/ parameter or return value\n-          || n->is_Phi()        \/\/ local of some sort\n-          );\n-}\n-\n@@ -1902,164 +1946,1 @@\n-  \/\/ These are the candidate return value:\n-  Node* xvalue = x0;\n-  Node* yvalue = y0;\n-\n-  if (xvalue == yvalue) {\n-    return xvalue;\n-  }\n-\n-  bool want_max = (id == vmIntrinsics::_max || id == vmIntrinsics::_max_strict);\n-\n-  const TypeInt* txvalue = _gvn.type(xvalue)->isa_int();\n-  const TypeInt* tyvalue = _gvn.type(yvalue)->isa_int();\n-  if (txvalue == NULL || tyvalue == NULL)  return top();\n-  \/\/ This is not really necessary, but it is consistent with a\n-  \/\/ hypothetical MaxINode::Value method:\n-  int widen = MAX2(txvalue->_widen, tyvalue->_widen);\n-\n-  \/\/ %%% This folding logic should (ideally) be in a different place.\n-  \/\/ Some should be inside IfNode, and there to be a more reliable\n-  \/\/ transformation of ?: style patterns into cmoves.  We also want\n-  \/\/ more powerful optimizations around cmove and min\/max.\n-\n-  \/\/ Try to find a dominating comparison of these guys.\n-  \/\/ It can simplify the index computation for Arrays.copyOf\n-  \/\/ and similar uses of System.arraycopy.\n-  \/\/ First, compute the normalized version of CmpI(x, y).\n-  int   cmp_op = Op_CmpI;\n-  Node* xkey = xvalue;\n-  Node* ykey = yvalue;\n-  Node* ideal_cmpxy = _gvn.transform(new CmpINode(xkey, ykey));\n-  if (ideal_cmpxy->is_Cmp()) {\n-    \/\/ E.g., if we have CmpI(length - offset, count),\n-    \/\/ it might idealize to CmpI(length, count + offset)\n-    cmp_op = ideal_cmpxy->Opcode();\n-    xkey = ideal_cmpxy->in(1);\n-    ykey = ideal_cmpxy->in(2);\n-  }\n-\n-  \/\/ Start by locating any relevant comparisons.\n-  Node* start_from = (xkey->outcnt() < ykey->outcnt()) ? xkey : ykey;\n-  Node* cmpxy = NULL;\n-  Node* cmpyx = NULL;\n-  for (DUIterator_Fast kmax, k = start_from->fast_outs(kmax); k < kmax; k++) {\n-    Node* cmp = start_from->fast_out(k);\n-    if (cmp->outcnt() > 0 &&            \/\/ must have prior uses\n-        cmp->in(0) == NULL &&           \/\/ must be context-independent\n-        cmp->Opcode() == cmp_op) {      \/\/ right kind of compare\n-      if (cmp->in(1) == xkey && cmp->in(2) == ykey)  cmpxy = cmp;\n-      if (cmp->in(1) == ykey && cmp->in(2) == xkey)  cmpyx = cmp;\n-    }\n-  }\n-\n-  const int NCMPS = 2;\n-  Node* cmps[NCMPS] = { cmpxy, cmpyx };\n-  int cmpn;\n-  for (cmpn = 0; cmpn < NCMPS; cmpn++) {\n-    if (cmps[cmpn] != NULL)  break;     \/\/ find a result\n-  }\n-  if (cmpn < NCMPS) {\n-    \/\/ Look for a dominating test that tells us the min and max.\n-    int depth = 0;                \/\/ Limit search depth for speed\n-    Node* dom = control();\n-    for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {\n-      if (++depth >= 100)  break;\n-      Node* ifproj = dom;\n-      if (!ifproj->is_Proj())  continue;\n-      Node* iff = ifproj->in(0);\n-      if (!iff->is_If())  continue;\n-      Node* bol = iff->in(1);\n-      if (!bol->is_Bool())  continue;\n-      Node* cmp = bol->in(1);\n-      if (cmp == NULL)  continue;\n-      for (cmpn = 0; cmpn < NCMPS; cmpn++)\n-        if (cmps[cmpn] == cmp)  break;\n-      if (cmpn == NCMPS)  continue;\n-      BoolTest::mask btest = bol->as_Bool()->_test._test;\n-      if (ifproj->is_IfFalse())  btest = BoolTest(btest).negate();\n-      if (cmp->in(1) == ykey)    btest = BoolTest(btest).commute();\n-      \/\/ At this point, we know that 'x btest y' is true.\n-      switch (btest) {\n-      case BoolTest::eq:\n-        \/\/ They are proven equal, so we can collapse the min\/max.\n-        \/\/ Either value is the answer.  Choose the simpler.\n-        if (is_simple_name(yvalue) && !is_simple_name(xvalue))\n-          return yvalue;\n-        return xvalue;\n-      case BoolTest::lt:          \/\/ x < y\n-      case BoolTest::le:          \/\/ x <= y\n-        return (want_max ? yvalue : xvalue);\n-      case BoolTest::gt:          \/\/ x > y\n-      case BoolTest::ge:          \/\/ x >= y\n-        return (want_max ? xvalue : yvalue);\n-      default:\n-        break;\n-      }\n-    }\n-  }\n-\n-  \/\/ We failed to find a dominating test.\n-  \/\/ Let's pick a test that might GVN with prior tests.\n-  Node*          best_bol   = NULL;\n-  BoolTest::mask best_btest = BoolTest::illegal;\n-  for (cmpn = 0; cmpn < NCMPS; cmpn++) {\n-    Node* cmp = cmps[cmpn];\n-    if (cmp == NULL)  continue;\n-    for (DUIterator_Fast jmax, j = cmp->fast_outs(jmax); j < jmax; j++) {\n-      Node* bol = cmp->fast_out(j);\n-      if (!bol->is_Bool())  continue;\n-      BoolTest::mask btest = bol->as_Bool()->_test._test;\n-      if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;\n-      if (cmp->in(1) == ykey)   btest = BoolTest(btest).commute();\n-      if (bol->outcnt() > (best_bol == NULL ? 0 : best_bol->outcnt())) {\n-        best_bol   = bol->as_Bool();\n-        best_btest = btest;\n-      }\n-    }\n-  }\n-\n-  Node* answer_if_true  = NULL;\n-  Node* answer_if_false = NULL;\n-  switch (best_btest) {\n-  default:\n-    if (cmpxy == NULL)\n-      cmpxy = ideal_cmpxy;\n-    best_bol = _gvn.transform(new BoolNode(cmpxy, BoolTest::lt));\n-    \/\/ and fall through:\n-  case BoolTest::lt:          \/\/ x < y\n-  case BoolTest::le:          \/\/ x <= y\n-    answer_if_true  = (want_max ? yvalue : xvalue);\n-    answer_if_false = (want_max ? xvalue : yvalue);\n-    break;\n-  case BoolTest::gt:          \/\/ x > y\n-  case BoolTest::ge:          \/\/ x >= y\n-    answer_if_true  = (want_max ? xvalue : yvalue);\n-    answer_if_false = (want_max ? yvalue : xvalue);\n-    break;\n-  }\n-\n-  jint hi, lo;\n-  if (want_max) {\n-    \/\/ We can sharpen the minimum.\n-    hi = MAX2(txvalue->_hi, tyvalue->_hi);\n-    lo = MAX2(txvalue->_lo, tyvalue->_lo);\n-  } else {\n-    \/\/ We can sharpen the maximum.\n-    hi = MIN2(txvalue->_hi, tyvalue->_hi);\n-    lo = MIN2(txvalue->_lo, tyvalue->_lo);\n-  }\n-\n-  \/\/ Use a flow-free graph structure, to avoid creating excess control edges\n-  \/\/ which could hinder other optimizations.\n-  \/\/ Since Math.min\/max is often used with arraycopy, we want\n-  \/\/ tightly_coupled_allocation to be able to see beyond min\/max expressions.\n-  Node* cmov = CMoveNode::make(NULL, best_bol,\n-                               answer_if_false, answer_if_true,\n-                               TypeInt::make(lo, hi, widen));\n-\n-  return _gvn.transform(cmov);\n-\n-  \/*\n-  \/\/ This is not as desirable as it may seem, since Min and Max\n-  \/\/ nodes do not have a full set of optimizations.\n-  \/\/ And they would interfere, anyway, with 'if' optimizations\n-  \/\/ and with CMoveI canonical forms.\n+  Node* result_val = NULL;\n@@ -2068,1 +1949,3 @@\n-    result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;\n+  case vmIntrinsics::_min_strict:\n+    result_val = _gvn.transform(new MinINode(x0, y0));\n+    break;\n@@ -2070,1 +1953,3 @@\n-    result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;\n+  case vmIntrinsics::_max_strict:\n+    result_val = _gvn.transform(new MaxINode(x0, y0));\n+    break;\n@@ -2072,1 +1957,2 @@\n-    ShouldNotReachHere();\n+    fatal_unexpected_iid(id);\n+    break;\n@@ -2074,1 +1960,1 @@\n-  *\/\n+  return result_val;\n@@ -2191,0 +2077,84 @@\n+\/\/--------------------------inline_bitshuffle_methods-----------------------------\n+\/\/ inline int Integer.compress(int, int)\n+\/\/ inline int Integer.expand(int, int)\n+\/\/ inline long Long.compress(long, long)\n+\/\/ inline long Long.expand(long, long)\n+bool LibraryCallKit::inline_bitshuffle_methods(vmIntrinsics::ID id) {\n+  Node* n = NULL;\n+  switch (id) {\n+    case vmIntrinsics::_compress_i:  n = new CompressBitsNode(argument(0), argument(1), TypeInt::INT); break;\n+    case vmIntrinsics::_expand_i:    n = new ExpandBitsNode(argument(0),  argument(1), TypeInt::INT); break;\n+    case vmIntrinsics::_compress_l:  n = new CompressBitsNode(argument(0), argument(2), TypeLong::LONG); break;\n+    case vmIntrinsics::_expand_l:    n = new ExpandBitsNode(argument(0), argument(2), TypeLong::LONG); break;\n+    default:  fatal_unexpected_iid(id);  break;\n+  }\n+  set_result(_gvn.transform(n));\n+  return true;\n+}\n+\n+\/\/--------------------------inline_number_methods-----------------------------\n+\/\/ inline int Integer.compareUnsigned(int, int)\n+\/\/ inline int    Long.compareUnsigned(long, long)\n+bool LibraryCallKit::inline_compare_unsigned(vmIntrinsics::ID id) {\n+  Node* arg1 = argument(0);\n+  Node* arg2 = (id == vmIntrinsics::_compareUnsigned_l) ? argument(2) : argument(1);\n+  Node* n = NULL;\n+  switch (id) {\n+    case vmIntrinsics::_compareUnsigned_i:   n = new CmpU3Node(arg1, arg2);  break;\n+    case vmIntrinsics::_compareUnsigned_l:   n = new CmpUL3Node(arg1, arg2); break;\n+    default:  fatal_unexpected_iid(id);  break;\n+  }\n+  set_result(_gvn.transform(n));\n+  return true;\n+}\n+\n+\/\/--------------------------inline_unsigned_divmod_methods-----------------------------\n+\/\/ inline int Integer.divideUnsigned(int, int)\n+\/\/ inline int Integer.remainderUnsigned(int, int)\n+\/\/ inline long Long.divideUnsigned(long, long)\n+\/\/ inline long Long.remainderUnsigned(long, long)\n+bool LibraryCallKit::inline_divmod_methods(vmIntrinsics::ID id) {\n+  Node* n = NULL;\n+  switch (id) {\n+    case vmIntrinsics::_divideUnsigned_i: {\n+      zero_check_int(argument(1));\n+      \/\/ Compile-time detect of null-exception\n+      if (stopped()) {\n+        return true; \/\/ keep the graph constructed so far\n+      }\n+      n = new UDivINode(control(), argument(0), argument(1));\n+      break;\n+    }\n+    case vmIntrinsics::_divideUnsigned_l: {\n+      zero_check_long(argument(2));\n+      \/\/ Compile-time detect of null-exception\n+      if (stopped()) {\n+        return true; \/\/ keep the graph constructed so far\n+      }\n+      n = new UDivLNode(control(), argument(0), argument(2));\n+      break;\n+    }\n+    case vmIntrinsics::_remainderUnsigned_i: {\n+      zero_check_int(argument(1));\n+      \/\/ Compile-time detect of null-exception\n+      if (stopped()) {\n+        return true; \/\/ keep the graph constructed so far\n+      }\n+      n = new UModINode(control(), argument(0), argument(1));\n+      break;\n+    }\n+    case vmIntrinsics::_remainderUnsigned_l: {\n+      zero_check_long(argument(2));\n+      \/\/ Compile-time detect of null-exception\n+      if (stopped()) {\n+        return true; \/\/ keep the graph constructed so far\n+      }\n+      n = new UModLNode(control(), argument(0), argument(2));\n+      break;\n+    }\n+    default:  fatal_unexpected_iid(id);  break;\n+  }\n+  set_result(_gvn.transform(n));\n+  return true;\n+}\n+\n@@ -2206,0 +2176,1 @@\n+  const TypeOopPtr* result = NULL;\n@@ -2211,2 +2182,3 @@\n-      if (elem_type != NULL) {\n-        sharpened_klass = elem_type->klass();\n+      if (elem_type != NULL && elem_type->is_loaded()) {\n+        \/\/ Sharpen the value type.\n+        result = elem_type;\n@@ -2219,2 +2191,3 @@\n-  if (sharpened_klass != NULL && sharpened_klass->is_loaded()) {\n-    const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);\n+  if (result == NULL && sharpened_klass != NULL && sharpened_klass->is_loaded()) {\n+    \/\/ Sharpen the value type.\n+    result = TypeOopPtr::make_from_klass(sharpened_klass);\n@@ -2222,1 +2195,1 @@\n-      tjp = tjp->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n+      result = result->join_speculative(TypePtr::NOTNULL)->is_oopptr();\n@@ -2224,1 +2197,2 @@\n-\n+  }\n+  if (result != NULL) {\n@@ -2228,1 +2202,1 @@\n-      tty->print(\"  sharpened value: \");  tjp->dump();      tty->cr();\n+      tty->print(\"  sharpened value: \");  result->dump();    tty->cr();\n@@ -2231,3 +2205,1 @@\n-    \/\/ Sharpen the value type.\n-    return tjp;\n-  return NULL;\n+  return result;\n@@ -2416,1 +2388,1 @@\n-    ciInstanceKlass* k = instptr->klass()->as_instance_klass();\n+    ciInstanceKlass* k = instptr->instance_klass();\n@@ -2419,2 +2391,2 @@\n-        instptr->klass() == ciEnv::current()->Class_klass() &&\n-        instptr->offset() >= (instptr->klass()->as_instance_klass()->size_helper() * wordSize)) {\n+        k == ciEnv::current()->Class_klass() &&\n+        instptr->offset() >= (k->size_helper() * wordSize)) {\n@@ -2444,1 +2416,1 @@\n-    if (bt == T_ARRAY || bt == T_NARROWOOP) {\n+    if (bt != T_PRIMITIVE_OBJECT && is_reference_type(bt, true)) {\n@@ -2529,1 +2501,1 @@\n-          ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();\n+          ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n@@ -2582,1 +2554,1 @@\n-        ciInstanceKlass* holder = adr_type->is_instptr()->klass()->as_instance_klass();\n+        ciInstanceKlass* holder = adr_type->is_instptr()->instance_klass();\n@@ -2933,1 +2905,1 @@\n-  const TypeKlassPtr* klsptr = kls->bottom_type()->isa_klassptr();\n+  const TypeInstKlassPtr* klsptr = kls->bottom_type()->isa_instklassptr();\n@@ -2937,1 +2909,1 @@\n-  ciInstanceKlass* ik = klsptr->klass()->as_instance_klass();\n+  ciInstanceKlass* ik = klsptr->instance_klass();\n@@ -3078,1 +3050,1 @@\n-    Node* epoch_address = makecon(TypeRawPtr::make(Jfr::epoch_address()));\n+    Node* epoch_address = makecon(TypeRawPtr::make(JfrIntrinsicSupport::epoch_address()));\n@@ -3089,3 +3061,3 @@\n-                        OptoRuntime::get_class_id_intrinsic_Type(),\n-                        CAST_FROM_FN_PTR(address, Jfr::get_class_id_intrinsic),\n-                        \"get_class_id_intrinsic\",\n+                        OptoRuntime::class_id_load_barrier_Type(),\n+                        CAST_FROM_FN_PTR(address, JfrIntrinsicSupport::load_barrier),\n+                        \"class id load barrier\",\n@@ -3112,1 +3084,1 @@\n-    Node* signaled_flag_address = makecon(TypeRawPtr::make(Jfr::signal_address()));\n+    Node* signaled_flag_address = makecon(TypeRawPtr::make(JfrIntrinsicSupport::signal_address()));\n@@ -3125,0 +3097,36 @@\n+\/*\n+ * The intrinsic is a model of this pseudo-code:\n+ *\n+ * JfrThreadLocal* const tl = Thread::jfr_thread_local()\n+ * jobject h_event_writer = tl->java_event_writer();\n+ * if (h_event_writer == NULL) {\n+ *   return NULL;\n+ * }\n+ * oop threadObj = Thread::threadObj();\n+ * oop vthread = java_lang_Thread::vthread(threadObj);\n+ * traceid tid;\n+ * bool excluded;\n+ * if (vthread != threadObj) {  \/\/ i.e. current thread is virtual\n+ *   tid = java_lang_Thread::tid(vthread);\n+ *   u2 vthread_epoch_raw = java_lang_Thread::jfr_epoch(vthread);\n+ *   excluded = vthread_epoch_raw & excluded_mask;\n+ *   if (!excluded) {\n+ *     traceid current_epoch = JfrTraceIdEpoch::current_generation();\n+ *     u2 vthread_epoch = vthread_epoch_raw & epoch_mask;\n+ *     if (vthread_epoch != current_epoch) {\n+ *       write_checkpoint();\n+ *     }\n+ *   }\n+ * } else {\n+ *   tid = java_lang_Thread::tid(threadObj);\n+ *   u2 thread_epoch_raw = java_lang_Thread::jfr_epoch(threadObj);\n+ *   excluded = thread_epoch_raw & excluded_mask;\n+ * }\n+ * oop event_writer = JNIHandles::resolve_non_null(h_event_writer);\n+ * traceid tid_in_event_writer = getField(event_writer, \"threadID\");\n+ * if (tid_in_event_writer != tid) {\n+ *   setField(event_writer, \"threadID\", tid);\n+ *   setField(event_writer, \"excluded\", excluded);\n+ * }\n+ * return event_writer\n+ *\/\n@@ -3126,0 +3134,11 @@\n+  enum { _true_path = 1, _false_path = 2, PATH_LIMIT };\n+\n+  \/\/ Save input memory and i_o state.\n+  Node* input_memory_state = reset_memory();\n+  set_all_memory(input_memory_state);\n+  Node* input_io_state = i_o();\n+\n+  Node* excluded_mask = _gvn.intcon(32768);\n+  Node* epoch_mask = _gvn.intcon(32767);\n+\n+  \/\/ TLS\n@@ -3128,2 +3147,2 @@\n-  Node* jobj_ptr = basic_plus_adr(top(), tls_ptr,\n-                                  in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR));\n+  \/\/ Load the address of java event writer jobject handle from the jfr_thread_local structure.\n+  Node* jobj_ptr = basic_plus_adr(top(), tls_ptr, in_bytes(THREAD_LOCAL_WRITER_OFFSET_JFR));\n@@ -3131,0 +3150,1 @@\n+  \/\/ Load the eventwriter jobject handle.\n@@ -3133,2 +3153,7 @@\n-  Node* jobj_cmp_null = _gvn.transform( new CmpPNode(jobj, null()) );\n-  Node* test_jobj_eq_null  = _gvn.transform( new BoolNode(jobj_cmp_null, BoolTest::eq) );\n+  \/\/ Null check the jobject handle.\n+  Node* jobj_cmp_null = _gvn.transform(new CmpPNode(jobj, null()));\n+  Node* test_jobj_not_equal_null = _gvn.transform(new BoolNode(jobj_cmp_null, BoolTest::ne));\n+  IfNode* iff_jobj_not_equal_null = create_and_map_if(control(), test_jobj_not_equal_null, PROB_MAX, COUNT_UNKNOWN);\n+\n+  \/\/ False path, jobj is null.\n+  Node* jobj_is_null = _gvn.transform(new IfFalseNode(iff_jobj_not_equal_null));\n@@ -3136,2 +3161,2 @@\n-  IfNode* iff_jobj_null =\n-    create_and_map_if(control(), test_jobj_eq_null, PROB_MIN, COUNT_UNKNOWN);\n+  \/\/ True path, jobj is not null.\n+  Node* jobj_is_not_null = _gvn.transform(new IfTrueNode(iff_jobj_not_equal_null));\n@@ -3139,3 +3164,1 @@\n-  enum { _normal_path = 1,\n-         _null_path = 2,\n-         PATH_LIMIT };\n+  set_control(jobj_is_not_null);\n@@ -3143,0 +3166,193 @@\n+  \/\/ Load the threadObj for the CarrierThread.\n+  Node* threadObj = generate_current_thread(tls_ptr);\n+\n+  \/\/ Load the vthread.\n+  Node* vthread = generate_virtual_thread(tls_ptr);\n+\n+  \/\/ If vthread != threadObj, this is a virtual thread.\n+  Node* vthread_cmp_threadObj = _gvn.transform(new CmpPNode(vthread, threadObj));\n+  Node* test_vthread_not_equal_threadObj = _gvn.transform(new BoolNode(vthread_cmp_threadObj, BoolTest::ne));\n+  IfNode* iff_vthread_not_equal_threadObj =\n+    create_and_map_if(jobj_is_not_null, test_vthread_not_equal_threadObj, PROB_FAIR, COUNT_UNKNOWN);\n+\n+  \/\/ False branch, fallback to threadObj.\n+  Node* vthread_equal_threadObj = _gvn.transform(new IfFalseNode(iff_vthread_not_equal_threadObj));\n+  set_control(vthread_equal_threadObj);\n+\n+  \/\/ Load the tid field from the vthread object.\n+  Node* thread_obj_tid = load_field_from_object(threadObj, \"tid\", \"J\");\n+\n+  \/\/ Load the raw epoch value from the threadObj.\n+  Node* threadObj_epoch_offset = basic_plus_adr(threadObj, java_lang_Thread::jfr_epoch_offset());\n+  Node* threadObj_epoch_raw = access_load_at(threadObj, threadObj_epoch_offset, TypeRawPtr::BOTTOM, TypeInt::CHAR, T_CHAR,\n+                                             IN_HEAP | MO_UNORDERED | C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD);\n+\n+  \/\/ Mask off the excluded information from the epoch.\n+  Node * threadObj_is_excluded = _gvn.transform(new AndINode(threadObj_epoch_raw, excluded_mask));\n+\n+  \/\/ True branch, this is a virtual thread.\n+  Node* vthread_not_equal_threadObj = _gvn.transform(new IfTrueNode(iff_vthread_not_equal_threadObj));\n+  set_control(vthread_not_equal_threadObj);\n+\n+  \/\/ Load the tid field from the vthread object.\n+  Node* vthread_tid = load_field_from_object(vthread, \"tid\", \"J\");\n+\n+  \/\/ Load the raw epoch value from the vthread.\n+  Node* vthread_epoch_offset = basic_plus_adr(vthread, java_lang_Thread::jfr_epoch_offset());\n+  Node* vthread_epoch_raw = access_load_at(vthread, vthread_epoch_offset, TypeRawPtr::BOTTOM, TypeInt::CHAR, T_CHAR,\n+                                           IN_HEAP | MO_UNORDERED | C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD);\n+\n+  \/\/ Mask off the excluded information from the epoch.\n+  Node * vthread_is_excluded = _gvn.transform(new AndINode(vthread_epoch_raw, _gvn.transform(excluded_mask)));\n+\n+  \/\/ Branch on excluded to conditionalize updating the epoch for the virtual thread.\n+  Node* is_excluded_cmp = _gvn.transform(new CmpINode(vthread_is_excluded, _gvn.transform(excluded_mask)));\n+  Node* test_not_excluded = _gvn.transform(new BoolNode(is_excluded_cmp, BoolTest::ne));\n+  IfNode* iff_not_excluded = create_and_map_if(control(), test_not_excluded, PROB_MAX, COUNT_UNKNOWN);\n+\n+  \/\/ False branch, vthread is excluded, no need to write epoch info.\n+  Node* excluded = _gvn.transform(new IfFalseNode(iff_not_excluded));\n+\n+  \/\/ True branch, vthread is included, update epoch info.\n+  Node* included = _gvn.transform(new IfTrueNode(iff_not_excluded));\n+  set_control(included);\n+\n+  \/\/ Get epoch value.\n+  Node* epoch = _gvn.transform(new AndINode(vthread_epoch_raw, _gvn.transform(epoch_mask)));\n+\n+  \/\/ Load the current epoch generation. The value is unsigned 16-bit, so we type it as T_CHAR.\n+  Node* epoch_generation_address = makecon(TypeRawPtr::make(JfrIntrinsicSupport::epoch_generation_address()));\n+  Node* current_epoch_generation = make_load(control(), epoch_generation_address, TypeInt::CHAR, T_CHAR, MemNode::unordered);\n+\n+  \/\/ Compare the epoch in the vthread to the current epoch generation.\n+  Node* const epoch_cmp = _gvn.transform(new CmpUNode(current_epoch_generation, epoch));\n+  Node* test_epoch_not_equal = _gvn.transform(new BoolNode(epoch_cmp, BoolTest::ne));\n+  IfNode* iff_epoch_not_equal = create_and_map_if(control(), test_epoch_not_equal, PROB_FAIR, COUNT_UNKNOWN);\n+\n+  \/\/ False path, epoch is equal, checkpoint information is valid.\n+  Node* epoch_is_equal = _gvn.transform(new IfFalseNode(iff_epoch_not_equal));\n+\n+  \/\/ True path, epoch is not equal, write a checkpoint for the vthread.\n+  Node* epoch_is_not_equal = _gvn.transform(new IfTrueNode(iff_epoch_not_equal));\n+\n+  set_control(epoch_is_not_equal);\n+\n+  \/\/ Make a runtime call, which can safepoint, to write a checkpoint for the vthread for this epoch.\n+  \/\/ The call also updates the native thread local thread id and the vthread with the current epoch.\n+  Node* call_write_checkpoint = make_runtime_call(RC_NO_LEAF,\n+                                                  OptoRuntime::jfr_write_checkpoint_Type(),\n+                                                  StubRoutines::jfr_write_checkpoint(),\n+                                                  \"write_checkpoint\", TypePtr::BOTTOM);\n+  Node* call_write_checkpoint_control = _gvn.transform(new ProjNode(call_write_checkpoint, TypeFunc::Control));\n+\n+  \/\/ vthread epoch != current epoch\n+  RegionNode* epoch_compare_rgn = new RegionNode(PATH_LIMIT);\n+  record_for_igvn(epoch_compare_rgn);\n+  PhiNode* epoch_compare_mem = new PhiNode(epoch_compare_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  record_for_igvn(epoch_compare_mem);\n+  PhiNode* epoch_compare_io = new PhiNode(epoch_compare_rgn, Type::ABIO);\n+  record_for_igvn(epoch_compare_io);\n+\n+  \/\/ Update control and phi nodes.\n+  epoch_compare_rgn->init_req(_true_path, call_write_checkpoint_control);\n+  epoch_compare_rgn->init_req(_false_path, epoch_is_equal);\n+  epoch_compare_mem->init_req(_true_path, _gvn.transform(reset_memory()));\n+  epoch_compare_mem->init_req(_false_path, input_memory_state);\n+  epoch_compare_io->init_req(_true_path, i_o());\n+  epoch_compare_io->init_req(_false_path, input_io_state);\n+\n+  \/\/ excluded != true\n+  RegionNode* exclude_compare_rgn = new RegionNode(PATH_LIMIT);\n+  record_for_igvn(exclude_compare_rgn);\n+  PhiNode* exclude_compare_mem = new PhiNode(exclude_compare_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  record_for_igvn(exclude_compare_mem);\n+  PhiNode* exclude_compare_io = new PhiNode(exclude_compare_rgn, Type::ABIO);\n+  record_for_igvn(exclude_compare_io);\n+\n+  \/\/ Update control and phi nodes.\n+  exclude_compare_rgn->init_req(_true_path, _gvn.transform(epoch_compare_rgn));\n+  exclude_compare_rgn->init_req(_false_path, excluded);\n+  exclude_compare_mem->init_req(_true_path, _gvn.transform(epoch_compare_mem));\n+  exclude_compare_mem->init_req(_false_path, input_memory_state);\n+  exclude_compare_io->init_req(_true_path, _gvn.transform(epoch_compare_io));\n+  exclude_compare_io->init_req(_false_path, input_io_state);\n+\n+  \/\/ vthread != threadObj\n+  RegionNode* vthread_compare_rgn = new RegionNode(PATH_LIMIT);\n+  record_for_igvn(vthread_compare_rgn);\n+  PhiNode* vthread_compare_mem = new PhiNode(vthread_compare_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  PhiNode* vthread_compare_io = new PhiNode(vthread_compare_rgn, Type::ABIO);\n+  record_for_igvn(vthread_compare_io);\n+  PhiNode* tid = new PhiNode(vthread_compare_rgn, TypeLong::LONG);\n+  record_for_igvn(tid);\n+  PhiNode* exclusion = new PhiNode(vthread_compare_rgn, TypeInt::BOOL);\n+  record_for_igvn(exclusion);\n+\n+  \/\/ Update control and phi nodes.\n+  vthread_compare_rgn->init_req(_true_path, _gvn.transform(exclude_compare_rgn));\n+  vthread_compare_rgn->init_req(_false_path, vthread_equal_threadObj);\n+  vthread_compare_mem->init_req(_true_path, _gvn.transform(exclude_compare_mem));\n+  vthread_compare_mem->init_req(_false_path, input_memory_state);\n+  vthread_compare_io->init_req(_true_path, _gvn.transform(exclude_compare_io));\n+  vthread_compare_io->init_req(_false_path, input_io_state);\n+  tid->init_req(_true_path, _gvn.transform(vthread_tid));\n+  tid->init_req(_false_path, _gvn.transform(thread_obj_tid));\n+  exclusion->init_req(_true_path, _gvn.transform(vthread_is_excluded));\n+  exclusion->init_req(_false_path, _gvn.transform(threadObj_is_excluded));\n+\n+  \/\/ Update branch state.\n+  set_control(_gvn.transform(vthread_compare_rgn));\n+  set_all_memory(_gvn.transform(vthread_compare_mem));\n+  set_i_o(_gvn.transform(vthread_compare_io));\n+\n+  \/\/ Load the event writer oop by dereferencing the jobject handle.\n+  ciKlass* klass_EventWriter = env()->find_system_klass(ciSymbol::make(\"jdk\/jfr\/internal\/event\/EventWriter\"));\n+  assert(klass_EventWriter->is_loaded(), \"invariant\");\n+  ciInstanceKlass* const instklass_EventWriter = klass_EventWriter->as_instance_klass();\n+  const TypeKlassPtr* const aklass = TypeKlassPtr::make(instklass_EventWriter);\n+  const TypeOopPtr* const xtype = aklass->as_instance_type();\n+  Node* event_writer = access_load(jobj, xtype, T_OBJECT, IN_NATIVE | C2_CONTROL_DEPENDENT_LOAD);\n+\n+  \/\/ Load the current thread id from the event writer object.\n+  Node* const event_writer_tid = load_field_from_object(event_writer, \"threadID\", \"J\");\n+  \/\/ Get the field offset to, conditionally, store an updated tid value later.\n+  Node* const event_writer_tid_field = field_address_from_object(event_writer, \"threadID\", \"J\", false);\n+  const TypePtr* event_writer_tid_field_type = _gvn.type(event_writer_tid_field)->isa_ptr();\n+  \/\/ Get the field offset to, conditionally, store an updated exclusion value later.\n+  Node* const event_writer_excluded_field = field_address_from_object(event_writer, \"excluded\", \"Z\", false);\n+  const TypePtr* event_writer_excluded_field_type = _gvn.type(event_writer_excluded_field)->isa_ptr();\n+\n+  RegionNode* event_writer_tid_compare_rgn = new RegionNode(PATH_LIMIT);\n+  record_for_igvn(event_writer_tid_compare_rgn);\n+  PhiNode* event_writer_tid_compare_mem = new PhiNode(event_writer_tid_compare_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  record_for_igvn(event_writer_tid_compare_mem);\n+  PhiNode* event_writer_tid_compare_io = new PhiNode(event_writer_tid_compare_rgn, Type::ABIO);\n+  record_for_igvn(event_writer_tid_compare_io);\n+\n+  \/\/ Compare the current tid from the thread object to what is currently stored in the event writer object.\n+  Node* const tid_cmp = _gvn.transform(new CmpLNode(event_writer_tid, _gvn.transform(tid)));\n+  Node* test_tid_not_equal = _gvn.transform(new BoolNode(tid_cmp, BoolTest::ne));\n+  IfNode* iff_tid_not_equal = create_and_map_if(_gvn.transform(vthread_compare_rgn), test_tid_not_equal, PROB_FAIR, COUNT_UNKNOWN);\n+\n+  \/\/ False path, tids are the same.\n+  Node* tid_is_equal = _gvn.transform(new IfFalseNode(iff_tid_not_equal));\n+\n+  \/\/ True path, tid is not equal, need to update the tid in the event writer.\n+  Node* tid_is_not_equal = _gvn.transform(new IfTrueNode(iff_tid_not_equal));\n+  record_for_igvn(tid_is_not_equal);\n+\n+  \/\/ Store the exclusion state to the event writer.\n+  store_to_memory(tid_is_not_equal, event_writer_excluded_field, _gvn.transform(exclusion), T_BOOLEAN, event_writer_excluded_field_type, MemNode::unordered);\n+\n+  \/\/ Store the tid to the event writer.\n+  store_to_memory(tid_is_not_equal, event_writer_tid_field, tid, T_LONG, event_writer_tid_field_type, MemNode::unordered);\n+\n+  \/\/ Update control and phi nodes.\n+  event_writer_tid_compare_rgn->init_req(_true_path, tid_is_not_equal);\n+  event_writer_tid_compare_rgn->init_req(_false_path, tid_is_equal);\n+  event_writer_tid_compare_mem->init_req(_true_path, _gvn.transform(reset_memory()));\n+  event_writer_tid_compare_mem->init_req(_false_path, _gvn.transform(vthread_compare_mem));\n+  event_writer_tid_compare_io->init_req(_true_path, _gvn.transform(i_o()));\n+  event_writer_tid_compare_io->init_req(_false_path, _gvn.transform(vthread_compare_io));\n+\n+  \/\/ Result of top level CFG, Memory, IO and Value.\n@@ -3144,1 +3360,3 @@\n-  PhiNode*    result_val = new PhiNode(result_rgn, TypeInstPtr::BOTTOM);\n+  PhiNode* result_mem = new PhiNode(result_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  PhiNode* result_io = new PhiNode(result_rgn, Type::ABIO);\n+  PhiNode* result_value = new PhiNode(result_rgn, TypeInstPtr::BOTTOM);\n@@ -3146,3 +3364,3 @@\n-  Node* jobj_is_null = _gvn.transform(new IfTrueNode(iff_jobj_null));\n-  result_rgn->init_req(_null_path, jobj_is_null);\n-  result_val->init_req(_null_path, null());\n+  \/\/ Result control.\n+  result_rgn->init_req(_true_path, _gvn.transform(event_writer_tid_compare_rgn));\n+  result_rgn->init_req(_false_path, jobj_is_null);\n@@ -3150,6 +3368,7 @@\n-  Node* jobj_is_not_null = _gvn.transform(new IfFalseNode(iff_jobj_null));\n-  set_control(jobj_is_not_null);\n-  Node* res = access_load(jobj, TypeInstPtr::NOTNULL, T_OBJECT,\n-                          IN_NATIVE | C2_CONTROL_DEPENDENT_LOAD);\n-  result_rgn->init_req(_normal_path, control());\n-  result_val->init_req(_normal_path, res);\n+  \/\/ Result memory.\n+  result_mem->init_req(_true_path, _gvn.transform(event_writer_tid_compare_mem));\n+  result_mem->init_req(_false_path, _gvn.transform(input_memory_state));\n+\n+  \/\/ Result IO.\n+  result_io->init_req(_true_path, _gvn.transform(event_writer_tid_compare_io));\n+  result_io->init_req(_false_path, _gvn.transform(input_io_state));\n@@ -3157,1 +3376,3 @@\n-  set_result(result_rgn, result_val);\n+  \/\/ Result value.\n+  result_value->init_req(_true_path, _gvn.transform(event_writer)); \/\/ return event writer oop\n+  result_value->init_req(_false_path, null()); \/\/ return NULL\n@@ -3159,0 +3380,5 @@\n+  \/\/ Set output state.\n+  set_control(_gvn.transform(result_rgn));\n+  set_all_memory(_gvn.transform(result_mem));\n+  set_i_o(_gvn.transform(result_io));\n+  set_result(result_rgn, result_value);\n@@ -3162,0 +3388,128 @@\n+\/*\n+ * The intrinsic is a model of this pseudo-code:\n+ *\n+ * JfrThreadLocal* const tl = thread->jfr_thread_local();\n+ * if (carrierThread != thread) { \/\/ is virtual thread\n+ *   const u2 vthread_epoch_raw = java_lang_Thread::jfr_epoch(thread);\n+ *   bool excluded = vthread_epoch_raw & excluded_mask;\n+ *   Atomic::store(&tl->_contextual_tid, java_lang_Thread::tid(thread));\n+ *   Atomic::store(&tl->_contextual_thread_excluded, is_excluded);\n+ *   if (!excluded) {\n+ *     const u2 vthread_epoch = vthread_epoch_raw & epoch_mask;\n+ *     Atomic::store(&tl->_vthread_epoch, vthread_epoch);\n+ *   }\n+ *   Atomic::release_store(&tl->_vthread, true);\n+ *   return;\n+ * }\n+ * Atomic::release_store(&tl->_vthread, false);\n+ *\/\n+void LibraryCallKit::extend_setCurrentThread(Node* jt, Node* thread) {\n+  enum { _true_path = 1, _false_path = 2, PATH_LIMIT };\n+\n+  Node* input_memory_state = reset_memory();\n+  set_all_memory(input_memory_state);\n+\n+  Node* excluded_mask = _gvn.intcon(32768);\n+  Node* epoch_mask = _gvn.intcon(32767);\n+\n+  Node* const carrierThread = generate_current_thread(jt);\n+  \/\/ If thread != carrierThread, this is a virtual thread.\n+  Node* thread_cmp_carrierThread = _gvn.transform(new CmpPNode(thread, carrierThread));\n+  Node* test_thread_not_equal_carrierThread = _gvn.transform(new BoolNode(thread_cmp_carrierThread, BoolTest::ne));\n+  IfNode* iff_thread_not_equal_carrierThread =\n+    create_and_map_if(control(), test_thread_not_equal_carrierThread, PROB_FAIR, COUNT_UNKNOWN);\n+\n+  Node* vthread_offset = basic_plus_adr(jt, in_bytes(THREAD_LOCAL_OFFSET_JFR + VTHREAD_OFFSET_JFR));\n+\n+  \/\/ False branch, is carrierThread.\n+  Node* thread_equal_carrierThread = _gvn.transform(new IfFalseNode(iff_thread_not_equal_carrierThread));\n+  \/\/ Store release\n+  Node* vthread_false_memory = store_to_memory(thread_equal_carrierThread, vthread_offset, _gvn.intcon(0), T_BOOLEAN, Compile::AliasIdxRaw, MemNode::release, true);\n+\n+  set_all_memory(input_memory_state);\n+\n+  \/\/ True branch, is virtual thread.\n+  Node* thread_not_equal_carrierThread = _gvn.transform(new IfTrueNode(iff_thread_not_equal_carrierThread));\n+  set_control(thread_not_equal_carrierThread);\n+\n+  \/\/ Load the raw epoch value from the vthread.\n+  Node* epoch_offset = basic_plus_adr(thread, java_lang_Thread::jfr_epoch_offset());\n+  Node* epoch_raw = access_load_at(thread, epoch_offset, TypeRawPtr::BOTTOM, TypeInt::CHAR, T_CHAR,\n+                                   IN_HEAP | MO_UNORDERED | C2_MISMATCHED | C2_CONTROL_DEPENDENT_LOAD);\n+\n+  \/\/ Mask off the excluded information from the epoch.\n+  Node * const is_excluded = _gvn.transform(new AndINode(epoch_raw, _gvn.transform(excluded_mask)));\n+\n+  \/\/ Load the tid field from the thread.\n+  Node* tid = load_field_from_object(thread, \"tid\", \"J\");\n+\n+  \/\/ Store the vthread tid to the jfr thread local.\n+  Node* thread_id_offset = basic_plus_adr(jt, in_bytes(THREAD_LOCAL_OFFSET_JFR + VTHREAD_ID_OFFSET_JFR));\n+  Node* tid_memory = store_to_memory(control(), thread_id_offset, tid, T_LONG, Compile::AliasIdxRaw, MemNode::unordered, true);\n+\n+  \/\/ Branch is_excluded to conditionalize updating the epoch .\n+  Node* excluded_cmp = _gvn.transform(new CmpINode(is_excluded, _gvn.transform(excluded_mask)));\n+  Node* test_excluded = _gvn.transform(new BoolNode(excluded_cmp, BoolTest::eq));\n+  IfNode* iff_excluded = create_and_map_if(control(), test_excluded, PROB_MIN, COUNT_UNKNOWN);\n+\n+  \/\/ True branch, vthread is excluded, no need to write epoch info.\n+  Node* excluded = _gvn.transform(new IfTrueNode(iff_excluded));\n+  set_control(excluded);\n+  Node* vthread_is_excluded = _gvn.intcon(1);\n+\n+  \/\/ False branch, vthread is included, update epoch info.\n+  Node* included = _gvn.transform(new IfFalseNode(iff_excluded));\n+  set_control(included);\n+  Node* vthread_is_included = _gvn.intcon(0);\n+\n+  \/\/ Get epoch value.\n+  Node* epoch = _gvn.transform(new AndINode(epoch_raw, _gvn.transform(epoch_mask)));\n+\n+  \/\/ Store the vthread epoch to the jfr thread local.\n+  Node* vthread_epoch_offset = basic_plus_adr(jt, in_bytes(THREAD_LOCAL_OFFSET_JFR + VTHREAD_EPOCH_OFFSET_JFR));\n+  Node* included_memory = store_to_memory(control(), vthread_epoch_offset, epoch, T_CHAR, Compile::AliasIdxRaw, MemNode::unordered, true);\n+\n+  RegionNode* excluded_rgn = new RegionNode(PATH_LIMIT);\n+  record_for_igvn(excluded_rgn);\n+  PhiNode* excluded_mem = new PhiNode(excluded_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  record_for_igvn(excluded_mem);\n+  PhiNode* exclusion = new PhiNode(excluded_rgn, TypeInt::BOOL);\n+  record_for_igvn(exclusion);\n+\n+  \/\/ Merge the excluded control and memory.\n+  excluded_rgn->init_req(_true_path, excluded);\n+  excluded_rgn->init_req(_false_path, included);\n+  excluded_mem->init_req(_true_path, tid_memory);\n+  excluded_mem->init_req(_false_path, included_memory);\n+  exclusion->init_req(_true_path, _gvn.transform(vthread_is_excluded));\n+  exclusion->init_req(_false_path, _gvn.transform(vthread_is_included));\n+\n+  \/\/ Set intermediate state.\n+  set_control(_gvn.transform(excluded_rgn));\n+  set_all_memory(excluded_mem);\n+\n+  \/\/ Store the vthread exclusion state to the jfr thread local.\n+  Node* thread_local_excluded_offset = basic_plus_adr(jt, in_bytes(THREAD_LOCAL_OFFSET_JFR + VTHREAD_EXCLUDED_OFFSET_JFR));\n+  store_to_memory(control(), thread_local_excluded_offset, _gvn.transform(exclusion), T_BOOLEAN, Compile::AliasIdxRaw, MemNode::unordered, true);\n+\n+  \/\/ Store release\n+  Node * vthread_true_memory = store_to_memory(control(), vthread_offset, _gvn.intcon(1), T_BOOLEAN, Compile::AliasIdxRaw, MemNode::release, true);\n+\n+  RegionNode* thread_compare_rgn = new RegionNode(PATH_LIMIT);\n+  record_for_igvn(thread_compare_rgn);\n+  PhiNode* thread_compare_mem = new PhiNode(thread_compare_rgn, Type::MEMORY, TypePtr::BOTTOM);\n+  record_for_igvn(thread_compare_mem);\n+  PhiNode* vthread = new PhiNode(thread_compare_rgn, TypeInt::BOOL);\n+  record_for_igvn(vthread);\n+\n+  \/\/ Merge the thread_compare control and memory.\n+  thread_compare_rgn->init_req(_true_path, control());\n+  thread_compare_rgn->init_req(_false_path, thread_equal_carrierThread);\n+  thread_compare_mem->init_req(_true_path, vthread_true_memory);\n+  thread_compare_mem->init_req(_false_path, vthread_false_memory);\n+\n+  \/\/ Set output state.\n+  set_control(_gvn.transform(thread_compare_rgn));\n+  set_all_memory(_gvn.transform(thread_compare_mem));\n+}\n+\n@@ -3164,0 +3518,7 @@\n+\/\/------------------------inline_native_currentCarrierThread------------------\n+bool LibraryCallKit::inline_native_currentCarrierThread() {\n+  Node* junk = NULL;\n+  set_result(generate_current_thread(junk));\n+  return true;\n+}\n+\n@@ -3167,1 +3528,59 @@\n-  set_result(generate_current_thread(junk));\n+  set_result(generate_virtual_thread(junk));\n+  return true;\n+}\n+\n+\/\/------------------------inline_native_setVthread------------------\n+bool LibraryCallKit::inline_native_setCurrentThread() {\n+  assert(C->method()->changes_current_thread(),\n+         \"method changes current Thread but is not annotated ChangesCurrentThread\");\n+  Node* arr = argument(1);\n+  Node* thread = _gvn.transform(new ThreadLocalNode());\n+  Node* p = basic_plus_adr(top()\/*!oop*\/, thread, in_bytes(JavaThread::vthread_offset()));\n+  Node* thread_obj_handle\n+    = make_load(NULL, p, p->bottom_type()->is_ptr(), T_OBJECT, MemNode::unordered);\n+  thread_obj_handle = _gvn.transform(thread_obj_handle);\n+  const TypePtr *adr_type = _gvn.type(thread_obj_handle)->isa_ptr();\n+  \/\/ Stores of oops to native memory not supported yet by BarrierSetC2::store_at_resolved\n+  \/\/ access_store_at(NULL, thread_obj_handle, adr_type, arr, _gvn.type(arr), T_OBJECT, IN_NATIVE | MO_UNORDERED);\n+  store_to_memory(control(), thread_obj_handle, arr, T_OBJECT, adr_type, MemNode::unordered);\n+  JFR_ONLY(extend_setCurrentThread(thread, arr);)\n+  return true;\n+}\n+\n+Node* LibraryCallKit::extentLocalCache_helper() {\n+  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n+  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+\n+  bool xk = etype->klass_is_exact();\n+\n+  Node* thread = _gvn.transform(new ThreadLocalNode());\n+  Node* p = basic_plus_adr(top()\/*!oop*\/, thread, in_bytes(JavaThread::extentLocalCache_offset()));\n+  return _gvn.transform(LoadNode::make(_gvn, NULL, immutable_memory(), p, p->bottom_type()->is_ptr(),\n+        TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered));\n+}\n+\n+\/\/------------------------inline_native_extentLocalCache------------------\n+bool LibraryCallKit::inline_native_extentLocalCache() {\n+  ciKlass *objects_klass = ciObjArrayKlass::make(env()->Object_klass());\n+  const TypeOopPtr *etype = TypeOopPtr::make_from_klass(env()->Object_klass());\n+  const TypeAry* arr0 = TypeAry::make(etype, TypeInt::POS);\n+\n+  \/\/ Because we create the extentLocal cache lazily we have to make the\n+  \/\/ type of the result BotPTR.\n+  bool xk = etype->klass_is_exact();\n+  const Type* objects_type = TypeAryPtr::make(TypePtr::BotPTR, arr0, objects_klass, xk, TypeAryPtr::Offset(0));\n+  Node* cache_obj_handle = extentLocalCache_helper();\n+  set_result(access_load(cache_obj_handle, objects_type, T_OBJECT, IN_NATIVE));\n+\n+  return true;\n+}\n+\n+\/\/------------------------inline_native_setExtentLocalCache------------------\n+bool LibraryCallKit::inline_native_setExtentLocalCache() {\n+  Node* arr = argument(0);\n+  Node* cache_obj_handle = extentLocalCache_helper();\n+\n+  const TypePtr *adr_type = _gvn.type(cache_obj_handle)->isa_ptr();\n+  store_to_memory(control(), cache_obj_handle, arr, T_OBJECT, adr_type,\n+                  MemNode::unordered);\n+\n@@ -3447,7 +3866,1 @@\n-  ciKlass* obj_klass = NULL;\n-  const Type* obj_t = _gvn.type(obj);\n-  if (obj->is_InlineType()) {\n-    obj_klass = obj_t->inline_klass();\n-  } else if (obj_t->isa_oopptr()) {\n-    obj_klass = obj_t->is_oopptr()->klass();\n-  }\n+  const TypeOopPtr* tp = _gvn.type(obj)->isa_oopptr();\n@@ -3459,3 +3872,3 @@\n-  \/\/ Check for null if casting to QMyValue\n-  if (tm != NULL && tm->is_klass() && obj_klass != NULL) {\n-    if (!obj_klass->is_loaded()) {\n+  if (tm != NULL && tm->is_klass() &&\n+      tp != NULL) {\n+    if (!tp->is_loaded()) {\n@@ -3465,1 +3878,1 @@\n-      int static_res = C->static_subtype_check(tm->as_klass(), obj_klass);\n+      int static_res = C->static_subtype_check(TypeKlassPtr::make(tm->as_klass()), tp->as_klass_type());\n@@ -3967,2 +4380,2 @@\n-        ciKlass* subk   = _gvn.type(load_object_klass(original))->is_klassptr()->klass();\n-        ciKlass* superk = _gvn.type(klass_node)->is_klassptr()->klass();\n+        const TypeKlassPtr* subk = _gvn.type(load_object_klass(original))->is_klassptr();\n+        const TypeKlassPtr* superk = _gvn.type(klass_node)->is_klassptr();\n@@ -4443,0 +4856,19 @@\n+bool LibraryCallKit::inline_fp_range_check(vmIntrinsics::ID id) {\n+  Node* arg = argument(0);\n+  Node* result = NULL;\n+\n+  switch (id) {\n+  case vmIntrinsics::_floatIsInfinite:\n+    result = new IsInfiniteFNode(arg);\n+    break;\n+  case vmIntrinsics::_doubleIsInfinite:\n+    result = new IsInfiniteDNode(arg);\n+    break;\n+  default:\n+    fatal_unexpected_iid(id);\n+    break;\n+  }\n+  set_result(_gvn.transform(result));\n+  return true;\n+}\n+\n@@ -4609,4 +5041,2 @@\n-        ciKlass* k = obj_type->klass();\n-        if (!k->is_instance_klass() ||\n-            k->as_instance_klass()->is_interface() ||\n-            k->as_instance_klass()->has_subklass()) {\n+        if (!obj_type->isa_instptr() ||\n+            obj_type->is_instptr()->instance_klass()->has_subklass()) {\n@@ -4773,1 +5203,1 @@\n-\/\/ unitialized array will escape the compiled method. To prevent that\n+\/\/ uninitialized array will escape the compiled method. To prevent that\n@@ -5000,1 +5430,1 @@\n-  bool has_src = (top_src != NULL && top_src->klass() != NULL);\n+  bool has_src = (top_src != NULL && top_src->elem() != Type::BOTTOM);\n@@ -5002,1 +5432,1 @@\n-  bool has_dest = (top_dest != NULL && top_dest->klass() != NULL);\n+  bool has_dest = (top_dest != NULL && top_dest->elem() != Type::BOTTOM);\n@@ -5040,1 +5470,1 @@\n-        has_src = (top_src != NULL && top_src->klass() != NULL);\n+        has_src = (top_src != NULL && top_src->elem() != Type::BOTTOM);\n@@ -5047,1 +5477,1 @@\n-        has_dest = (top_dest != NULL && top_dest->klass() != NULL);\n+        has_dest = (top_dest != NULL && top_dest->elem() != Type::BOTTOM);\n@@ -5054,4 +5484,4 @@\n-    BasicType src_elem  = top_src->klass()->as_array_klass()->element_type()->basic_type();\n-    BasicType dest_elem = top_dest->klass()->as_array_klass()->element_type()->basic_type();\n-    if (is_reference_type(src_elem))   src_elem  = T_OBJECT;\n-    if (is_reference_type(dest_elem))  dest_elem = T_OBJECT;\n+    BasicType src_elem = top_src->isa_aryptr()->elem()->array_element_basic_type();\n+    BasicType dest_elem = top_dest->isa_aryptr()->elem()->array_element_basic_type();\n+    if (is_reference_type(src_elem, true)) src_elem = T_OBJECT;\n+    if (is_reference_type(dest_elem, true)) dest_elem = T_OBJECT;\n@@ -5068,2 +5498,2 @@\n-      ciKlass* src_k = top_src->klass();\n-      ciKlass* dest_k = top_dest->klass();\n+      ciKlass* src_k = NULL;\n+      ciKlass* dest_k = NULL;\n@@ -5155,1 +5585,4 @@\n-    const Type* toop = TypeOopPtr::make_from_klass(dest_klass_t->klass());\n+    const Type* toop = dest_klass_t->cast_to_exactness(false)->as_instance_type();\n+    if (toop->isa_aryptr() != NULL) {\n+      toop = toop->is_aryptr()->cast_to_not_flat(false)->cast_to_not_null_free(false);\n+    }\n@@ -5310,2 +5743,2 @@\n-  if (top_src  == NULL || top_src->klass()  == NULL ||\n-      top_dest == NULL || top_dest->klass() == NULL) {\n+  if (top_src  == NULL || top_src->elem()  == Type::BOTTOM ||\n+      top_dest == NULL || top_dest->elem() == Type::BOTTOM) {\n@@ -5317,2 +5750,2 @@\n-  BasicType src_elem = src_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType dst_elem = dst_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType src_elem = src_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType dst_elem = dst_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -5365,2 +5798,2 @@\n-  if (top_x  == NULL || top_x->klass()  == NULL ||\n-      top_y == NULL || top_y->klass() == NULL) {\n+  if (top_x  == NULL || top_x->elem()  == Type::BOTTOM ||\n+      top_y == NULL || top_y->elem() == Type::BOTTOM) {\n@@ -5371,2 +5804,2 @@\n-  BasicType x_elem = x_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType y_elem = y_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType x_elem = x_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType y_elem = y_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -5473,2 +5906,2 @@\n-  if (top_x  == NULL || top_x->klass()  == NULL ||\n-      top_z  == NULL || top_z->klass()  == NULL) {\n+  if (top_x  == NULL || top_x->elem()  == Type::BOTTOM ||\n+      top_z  == NULL || top_z->elem()  == Type::BOTTOM) {\n@@ -5479,2 +5912,2 @@\n-  BasicType x_elem = x_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType z_elem = z_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType x_elem = x_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType z_elem = z_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -5522,2 +5955,2 @@\n-  if (top_out  == NULL || top_out->klass()  == NULL ||\n-      top_in == NULL || top_in->klass() == NULL) {\n+  if (top_out  == NULL || top_out->elem()  == Type::BOTTOM ||\n+      top_in == NULL || top_in->elem() == Type::BOTTOM) {\n@@ -5528,2 +5961,2 @@\n-  BasicType out_elem = out_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType in_elem = in_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType out_elem = out_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType in_elem = in_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -5575,4 +6008,4 @@\n-  if (top_a  == NULL || top_a->klass()  == NULL ||\n-      top_b == NULL || top_b->klass()  == NULL ||\n-      top_n == NULL || top_n->klass()  == NULL ||\n-      top_m == NULL || top_m->klass()  == NULL) {\n+  if (top_a  == NULL || top_a->elem()  == Type::BOTTOM ||\n+      top_b == NULL || top_b->elem()  == Type::BOTTOM ||\n+      top_n == NULL || top_n->elem()  == Type::BOTTOM ||\n+      top_m == NULL || top_m->elem()  == Type::BOTTOM) {\n@@ -5583,4 +6016,4 @@\n-  BasicType a_elem = a_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType b_elem = b_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType n_elem = n_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType m_elem = m_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType a_elem = a_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType b_elem = b_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType n_elem = n_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType m_elem = m_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -5632,3 +6065,3 @@\n-  if (top_a  == NULL || top_a->klass()  == NULL ||\n-      top_n == NULL || top_n->klass()  == NULL ||\n-      top_m == NULL || top_m->klass()  == NULL) {\n+  if (top_a  == NULL || top_a->elem()  == Type::BOTTOM ||\n+      top_n == NULL || top_n->elem()  == Type::BOTTOM ||\n+      top_m == NULL || top_m->elem()  == Type::BOTTOM) {\n@@ -5639,3 +6072,3 @@\n-  BasicType a_elem = a_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType n_elem = n_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType m_elem = m_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType a_elem = a_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType n_elem = n_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType m_elem = m_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -5686,2 +6119,2 @@\n-  if (top_newArr == NULL || top_newArr->klass() == NULL || top_oldArr == NULL\n-      || top_oldArr->klass() == NULL) {\n+  if (top_newArr == NULL || top_newArr->elem() == Type::BOTTOM || top_oldArr == NULL\n+      || top_oldArr->elem() == Type::BOTTOM) {\n@@ -5691,2 +6124,2 @@\n-  BasicType newArr_elem = newArr_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n-  BasicType oldArr_elem = oldArr_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType newArr_elem = newArr_type->isa_aryptr()->elem()->array_element_basic_type();\n+  BasicType oldArr_elem = oldArr_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -5731,2 +6164,2 @@\n-  if (obja_t == NULL || obja_t->klass() == NULL ||\n-      objb_t == NULL || objb_t->klass() == NULL ||\n+  if (obja_t == NULL || obja_t->elem() == Type::BOTTOM ||\n+      objb_t == NULL || objb_t->elem() == Type::BOTTOM ||\n@@ -5899,1 +6332,1 @@\n-  if (top_src  == NULL || top_src->klass()  == NULL) {\n+  if (top_src  == NULL || top_src->elem()  == Type::BOTTOM) {\n@@ -5905,1 +6338,1 @@\n-  BasicType src_elem = src_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType src_elem = src_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -5988,1 +6421,1 @@\n-  if (top_src  == NULL || top_src->klass()  == NULL) {\n+  if (top_src  == NULL || top_src->elem()  == Type::BOTTOM) {\n@@ -5994,1 +6427,1 @@\n-  BasicType src_elem = src_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType src_elem = src_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -6070,1 +6503,1 @@\n-  assert(UseAdler32Intrinsics, \"Adler32 Instrinsic support need\"); \/\/ check if we actually need to check this flag or check a different one\n+  assert(UseAdler32Intrinsics, \"Adler32 Intrinsic support need\"); \/\/ check if we actually need to check this flag or check a different one\n@@ -6081,1 +6514,1 @@\n-  if (top_src  == NULL || top_src->klass()  == NULL) {\n+  if (top_src  == NULL || top_src->elem()  == Type::BOTTOM) {\n@@ -6087,1 +6520,1 @@\n-  BasicType src_elem = src_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType src_elem = src_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -6116,1 +6549,1 @@\n-  assert(UseAdler32Intrinsics, \"Adler32 Instrinsic support need\"); \/\/ check if we actually need to check this flag or check a different one\n+  assert(UseAdler32Intrinsics, \"Adler32 Intrinsic support need\"); \/\/ check if we actually need to check this flag or check a different one\n@@ -6209,2 +6642,2 @@\n-                                             DecoratorSet decorators = IN_HEAP, bool is_static = false,\n-                                             ciInstanceKlass* fromKls = NULL) {\n+                                             DecoratorSet decorators, bool is_static,\n+                                             ciInstanceKlass* fromKls) {\n@@ -6214,2 +6647,2 @@\n-    assert(tinst->klass()->is_loaded(), \"obj is not loaded\");\n-    fromKls = tinst->klass()->as_instance_klass();\n+    assert(tinst->is_loaded(), \"obj is not loaded\");\n+    fromKls = tinst->instance_klass();\n@@ -6223,1 +6656,1 @@\n-  assert (field != NULL, \"undefined field\");\n+  assert(field != NULL, \"undefined field %s %s %s\", fieldTypeString, fromKls->name()->as_utf8(), fieldName);\n@@ -6258,2 +6691,2 @@\n-                                                 bool is_exact = true, bool is_static = false,\n-                                                 ciInstanceKlass * fromKls = NULL) {\n+                                                 bool is_exact \/* true *\/, bool is_static \/* false *\/,\n+                                                 ciInstanceKlass * fromKls \/* NULL *\/) {\n@@ -6263,1 +6696,1 @@\n-    assert(tinst->klass()->is_loaded(), \"obj is not loaded\");\n+    assert(tinst->is_loaded(), \"obj is not loaded\");\n@@ -6265,1 +6698,1 @@\n-    fromKls = tinst->klass()->as_instance_klass();\n+    fromKls = tinst->instance_klass();\n@@ -6325,1 +6758,1 @@\n-  assert (top_src  != NULL && top_src->klass()  != NULL &&  top_dest != NULL && top_dest->klass() != NULL, \"args are strange\");\n+  assert (top_src  != NULL && top_src->elem()  != Type::BOTTOM &&  top_dest != NULL && top_dest->elem() != Type::BOTTOM, \"args are strange\");\n@@ -6386,2 +6819,2 @@\n-  assert (top_src  != NULL && top_src->klass()  != NULL\n-          &&  top_dest != NULL && top_dest->klass() != NULL, \"args are strange\");\n+  assert (top_src  != NULL && top_src->elem()  != Type::BOTTOM\n+          &&  top_dest != NULL && top_dest->elem() != Type::BOTTOM, \"args are strange\");\n@@ -6409,2 +6842,2 @@\n-  assert(tinst->klass()->is_loaded(), \"CBC obj is not loaded\");\n-  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  assert(tinst->is_loaded(), \"CBC obj is not loaded\");\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n@@ -6474,2 +6907,2 @@\n-  assert(top_src != NULL && top_src->klass() != NULL\n-         &&  top_dest != NULL && top_dest->klass() != NULL, \"args are strange\");\n+  assert(top_src != NULL && top_src->elem() != Type::BOTTOM\n+         &&  top_dest != NULL && top_dest->elem() != Type::BOTTOM, \"args are strange\");\n@@ -6497,2 +6930,2 @@\n-  assert(tinst->klass()->is_loaded(), \"ECB obj is not loaded\");\n-  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  assert(tinst->is_loaded(), \"ECB obj is not loaded\");\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n@@ -6548,2 +6981,2 @@\n-  assert(top_src != NULL && top_src->klass() != NULL &&\n-         top_dest != NULL && top_dest->klass() != NULL, \"args are strange\");\n+  assert(top_src != NULL && top_src->elem() != Type::BOTTOM &&\n+         top_dest != NULL && top_dest->elem() != Type::BOTTOM, \"args are strange\");\n@@ -6569,2 +7002,2 @@\n-  assert(tinst->klass()->is_loaded(), \"CTR obj is not loaded\");\n-  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  assert(tinst->is_loaded(), \"CTR obj is not loaded\");\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n@@ -6606,1 +7039,1 @@\n-  \/\/ Intel's extention is based on this optimization and AESCrypt generates round keys by preprocessing MixColumns.\n+  \/\/ Intel's extension is based on this optimization and AESCrypt generates round keys by preprocessing MixColumns.\n@@ -6650,1 +7083,1 @@\n-  assert(tinst->klass()->is_loaded(), \"CBCobj is not loaded\");\n+  assert(tinst->is_loaded(), \"CBCobj is not loaded\");\n@@ -6653,1 +7086,1 @@\n-  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n@@ -6713,1 +7146,1 @@\n-  assert(tinst->klass()->is_loaded(), \"ECBobj is not loaded\");\n+  assert(tinst->is_loaded(), \"ECBobj is not loaded\");\n@@ -6716,1 +7149,1 @@\n-  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n@@ -6773,1 +7206,1 @@\n-  assert(tinst->klass()->is_loaded(), \"CTRobj is not loaded\");\n+  assert(tinst->is_loaded(), \"CTRobj is not loaded\");\n@@ -6776,1 +7209,1 @@\n-  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n@@ -6919,1 +7352,1 @@\n-  if (top_src  == NULL || top_src->klass()  == NULL) {\n+  if (top_src  == NULL || top_src->elem()  == Type::BOTTOM) {\n@@ -6924,1 +7357,1 @@\n-  BasicType src_elem = src_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType src_elem = src_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -6939,1 +7372,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6945,1 +7378,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6951,1 +7384,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[I\");\n+    state = get_state_from_digest_object(digestBase_obj, T_INT);\n@@ -6957,1 +7390,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[J\");\n+    state = get_state_from_digest_object(digestBase_obj, T_LONG);\n@@ -6963,1 +7396,1 @@\n-    state = get_state_from_digest_object(digestBase_obj, \"[B\");\n+    state = get_state_from_digest_object(digestBase_obj, T_BYTE);\n@@ -7011,1 +7444,1 @@\n-  if (top_src  == NULL || top_src->klass()  == NULL) {\n+  if (top_src  == NULL || top_src->elem()  == Type::BOTTOM) {\n@@ -7016,1 +7449,1 @@\n-  BasicType src_elem = src_type->isa_aryptr()->klass()->as_array_klass()->element_type()->basic_type();\n+  BasicType src_elem = src_type->isa_aryptr()->elem()->array_element_basic_type();\n@@ -7027,1 +7460,1 @@\n-  const char* state_type = \"[I\";\n+  BasicType elem_type = T_INT;\n@@ -7056,1 +7489,1 @@\n-      state_type = \"[J\";\n+      elem_type = T_LONG;\n@@ -7064,1 +7497,1 @@\n-      state_type = \"[B\";\n+      elem_type = T_BYTE;\n@@ -7077,1 +7510,1 @@\n-    assert(tinst->klass()->is_loaded(), \"DigestBase is not loaded\");\n+    assert(tinst->is_loaded(), \"DigestBase is not loaded\");\n@@ -7079,1 +7512,1 @@\n-    ciKlass* klass_digestBase = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(klass_digestBase_name));\n+    ciKlass* klass_digestBase = tinst->instance_klass()->find_klass(ciSymbol::make(klass_digestBase_name));\n@@ -7082,1 +7515,1 @@\n-    return inline_digestBase_implCompressMB(digestBase_obj, instklass_digestBase, state_type, stub_addr, stub_name, src_start, ofs, limit);\n+    return inline_digestBase_implCompressMB(digestBase_obj, instklass_digestBase, elem_type, stub_addr, stub_name, src_start, ofs, limit);\n@@ -7089,1 +7522,1 @@\n-                                                      const char* state_type, address stubAddr, const char *stubName,\n+                                                      BasicType elem_type, address stubAddr, const char *stubName,\n@@ -7096,1 +7529,1 @@\n-  Node* state = get_state_from_digest_object(digest_obj, state_type);\n+  Node* state = get_state_from_digest_object(digest_obj, elem_type);\n@@ -7153,3 +7586,3 @@\n-  assert(top_in != NULL && top_in->klass() != NULL &&\n-         top_ct != NULL && top_ct->klass() != NULL &&\n-         top_out != NULL && top_out->klass() != NULL, \"args are strange\");\n+  assert(top_in != NULL && top_in->elem() != Type::BOTTOM &&\n+         top_ct != NULL && top_ct->elem() != Type::BOTTOM &&\n+         top_out != NULL && top_out->elem() != Type::BOTTOM, \"args are strange\");\n@@ -7183,2 +7616,2 @@\n-  assert(tinst->klass()->is_loaded(), \"GCTR obj is not loaded\");\n-  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  assert(tinst->is_loaded(), \"GCTR obj is not loaded\");\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n@@ -7235,1 +7668,1 @@\n-  assert(tinst->klass()->is_loaded(), \"GCTR obj is not loaded\");\n+  assert(tinst->is_loaded(), \"GCTR obj is not loaded\");\n@@ -7238,1 +7671,1 @@\n-  ciKlass* klass_AESCrypt = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n+  ciKlass* klass_AESCrypt = tinst->instance_klass()->find_klass(ciSymbol::make(\"com\/sun\/crypto\/provider\/AESCrypt\"));\n@@ -7256,1 +7689,8 @@\n-Node * LibraryCallKit::get_state_from_digest_object(Node *digest_object, const char *state_type) {\n+Node * LibraryCallKit::get_state_from_digest_object(Node *digest_object, BasicType elem_type) {\n+  const char* state_type;\n+  switch (elem_type) {\n+    case T_BYTE: state_type = \"[B\"; break;\n+    case T_INT:  state_type = \"[I\"; break;\n+    case T_LONG: state_type = \"[J\"; break;\n+    default: ShouldNotReachHere();\n+  }\n@@ -7262,1 +7702,1 @@\n-  Node* state = array_element_address(digest_state, intcon(0), T_INT);\n+  Node* state = array_element_address(digest_state, intcon(0), elem_type);\n@@ -7289,1 +7729,1 @@\n-  assert(tinst->klass()->is_loaded(), \"DigestBase is not loaded\");\n+  assert(tinst->is_loaded(), \"DigestBase is not loaded\");\n@@ -7329,1 +7769,1 @@\n-    klass = tinst->klass()->as_instance_klass()->find_klass(ciSymbol::make(klass_name));\n+    klass = tinst->instance_klass()->find_klass(ciSymbol::make(klass_name));\n@@ -7347,0 +7787,9 @@\n+bool LibraryCallKit::inline_continuation_do_yield() {\n+  address call_addr = StubRoutines::cont_doYield();\n+  const TypeFunc* tf = OptoRuntime::continuation_doYield_Type();\n+  Node* call = make_runtime_call(RC_NO_LEAF, tf, call_addr, \"doYield\", TypeRawPtr::BOTTOM);\n+  Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));\n+  set_result(result);\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":820,"deletions":371,"binary":false,"changes":1191,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -146,0 +146,2 @@\n+  Node* current_thread_helper(Node* &tls_output, ByteSize handle_offset,\n+                              bool is_immutable);\n@@ -147,0 +149,1 @@\n+  Node* generate_virtual_thread(Node* threadObj);\n@@ -204,2 +207,2 @@\n-  Node* load_field_from_object(Node* fromObj, const char* fieldName, const char* fieldTypeString, DecoratorSet decorators, bool is_static, ciInstanceKlass* fromKls);\n-  Node* field_address_from_object(Node* fromObj, const char* fieldName, const char* fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass* fromKls);\n+  Node* load_field_from_object(Node* fromObj, const char* fieldName, const char* fieldTypeString, DecoratorSet decorators = IN_HEAP, bool is_static = false, ciInstanceKlass* fromKls = NULL);\n+  Node* field_address_from_object(Node* fromObj, const char* fieldName, const char* fieldTypeString, bool is_exact = true, bool is_static = false, ciInstanceKlass* fromKls = NULL);\n@@ -256,0 +259,2 @@\n+\n+  bool inline_native_currentCarrierThread();\n@@ -257,0 +262,5 @@\n+  bool inline_native_setCurrentThread();\n+\n+  bool inline_native_extentLocalCache();\n+  Node* extentLocalCache_helper();\n+  bool inline_native_setExtentLocalCache();\n@@ -262,0 +272,1 @@\n+  void extend_setCurrentThread(Node* jt, Node* thread);\n@@ -289,0 +300,1 @@\n+  bool inline_fp_range_check(vmIntrinsics::ID id);\n@@ -290,0 +302,3 @@\n+  bool inline_bitshuffle_methods(vmIntrinsics::ID id);\n+  bool inline_compare_unsigned(vmIntrinsics::ID id);\n+  bool inline_divmod_methods(vmIntrinsics::ID id);\n@@ -307,1 +322,1 @@\n-                                        const char* state_type, address stubAddr, const char *stubName,\n+                                        BasicType elem_type, address stubAddr, const char *stubName,\n@@ -309,1 +324,1 @@\n-  Node* get_state_from_digest_object(Node *digestBase_object, const char* state_type);\n+  Node* get_state_from_digest_object(Node *digestBase_object, BasicType elem_type);\n@@ -338,0 +353,2 @@\n+  bool inline_continuation_do_yield();\n+\n@@ -356,0 +373,2 @@\n+  bool inline_vector_compress_expand();\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.hpp","additions":24,"deletions":5,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -198,0 +198,2 @@\n+  C->set_has_monitors(true);\n+\n@@ -215,0 +217,4 @@\n+  \/\/ need to set it for monitor exit as well.\n+  \/\/ OSR compiled methods can start with lock taken\n+  C->set_has_monitors(true);\n+\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -90,1 +90,1 @@\n-  \/\/ FastLock and FastUnlockNode do not hash, we need one for each correspoding\n+  \/\/ FastLock and FastUnlockNode do not hash, we need one for each corresponding\n@@ -116,1 +116,1 @@\n-  \/\/ FastLock and FastUnlockNode do not hash, we need one for each correspoding\n+  \/\/ FastLock and FastUnlockNode do not hash, we need one for each corresponding\n","filename":"src\/hotspot\/share\/opto\/locknode.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -253,0 +253,3 @@\n+  \/\/ The eventual count of vectorizable packs in slp\n+  int _slp_vector_pack_count;\n+\n@@ -257,1 +260,1 @@\n-      _slp_maximum_unroll_factor(0) {\n+      _slp_maximum_unroll_factor(0), _slp_vector_pack_count(0) {\n@@ -334,0 +337,2 @@\n+  void set_slp_pack_count(int pack_count)    { _slp_vector_pack_count = pack_count; }\n+  int  slp_pack_count() const                { return _slp_vector_pack_count; }\n@@ -780,0 +785,5 @@\n+#ifdef ASSERT\n+  \/\/ Tell whether the body contains nodes marked as reductions.\n+  bool has_reduction_nodes() const;\n+#endif \/\/ ASSERT\n+\n@@ -924,2 +934,2 @@\n-  Node* clone_skeleton_predicate_for_main_or_post_loop(Node* iff, Node* new_init, Node* new_stride, Node* predicate, Node* uncommon_proj, Node* control,\n-                                                       IdealLoopTree* outer_loop, Node* input_proj);\n+  Node* clone_skeleton_predicate_and_initialize(Node* iff, Node* new_init, Node* new_stride, Node* predicate, Node* uncommon_proj, Node* control,\n+                                                IdealLoopTree* outer_loop, Node* input_proj);\n@@ -931,0 +941,3 @@\n+  void initialize_skeleton_predicates_for_peeled_loop(ProjNode* predicate, LoopNode* outer_loop_head, int dd_outer_loop_head,\n+                                                      Node* init, Node* stride, IdealLoopTree* outer_loop,\n+                                                      const uint idx_before_clone, const Node_List& old_new);\n@@ -1325,0 +1338,1 @@\n+  static ProjNode* next_predicate(ProjNode* predicate);\n@@ -1328,0 +1342,33 @@\n+\n+  class Predicates {\n+  public:\n+    \/\/ given loop entry, find all predicates above loop\n+    Predicates(Node* entry);\n+\n+    \/\/ Proj of empty loop limit check predicate\n+    ProjNode* loop_limit_check() {\n+      return _loop_limit_check;\n+    }\n+\n+    \/\/ Proj of empty profile predicate\n+    ProjNode* profile_predicate() {\n+      return _profile_predicate;\n+    }\n+\n+    \/\/ Proj of empty predicate\n+    ProjNode* predicate() {\n+      return _predicate;\n+    }\n+\n+    \/\/ First control node above all predicates\n+    Node* skip_all() {\n+      return _entry_to_all_predicates;\n+    }\n+\n+  private:\n+    ProjNode*_loop_limit_check = nullptr;\n+    ProjNode* _profile_predicate = nullptr;\n+    ProjNode* _predicate = nullptr;\n+    Node* _entry_to_all_predicates = nullptr;\n+  };\n+\n@@ -1422,0 +1469,1 @@\n+  bool duplicate_loop_backedge(IdealLoopTree *loop, Node_List &old_new);\n@@ -1468,2 +1516,2 @@\n-  Node *clone_iff( PhiNode *phi, IdealLoopTree *loop );\n-  CmpNode *clone_bool( PhiNode *phi, IdealLoopTree *loop );\n+  Node* clone_iff(PhiNode* phi);\n+  CmpNode* clone_bool(PhiNode* phi);\n@@ -1684,0 +1732,14 @@\n+\n+  void clone_loop_body(const Node_List& body, Node_List &old_new, CloneMap* cm);\n+\n+  void fix_body_edges(const Node_List &body, IdealLoopTree* loop, const Node_List &old_new, int dd,\n+                      IdealLoopTree* parent, bool partial);\n+\n+  void fix_ctrl_uses(const Node_List& body, const IdealLoopTree* loop, Node_List &old_new, CloneLoopMode mode,\n+                Node* side_by_side_idom, CloneMap* cm, Node_List &worklist);\n+\n+  void fix_data_uses(Node_List& body, IdealLoopTree* loop, CloneLoopMode mode, IdealLoopTree* outer_loop,\n+                     uint new_counter, Node_List& old_new, Node_List& worklist, Node_List*& split_if_set,\n+                     Node_List*& split_bool_set, Node_List*& split_cex_set);\n+\n+  void finish_clone_loop(Node_List* split_if_set, Node_List* split_bool_set, Node_List* split_cex_set);\n@@ -1744,1 +1806,1 @@\n-\/\/ The original loop is dominated by the the same node chain but IfTrue projection:\n+\/\/ The original loop is dominated by the same node chain but IfTrue projection:\n@@ -1833,0 +1895,29 @@\n+\/\/ Compute probability of reaching some CFG node from a fixed\n+\/\/ dominating CFG node\n+class PathFrequency {\n+private:\n+  Node* _dom; \/\/ frequencies are computed relative to this node\n+  Node_Stack _stack;\n+  GrowableArray<float> _freqs_stack; \/\/ keep track of intermediate result at regions\n+  GrowableArray<float> _freqs; \/\/ cache frequencies\n+  PhaseIdealLoop* _phase;\n+\n+  float check_and_truncate_frequency(float f) {\n+    assert(f >= 0, \"Incorrect frequency\");\n+    \/\/ We do not perform an exact (f <= 1) check\n+    \/\/ this would be error prone with rounding of floats.\n+    \/\/ Performing a check like (f <= 1+eps) would be of benefit,\n+    \/\/ however, it is not evident how to determine such an eps,\n+    \/\/ given that an arbitrary number of add\/mul operations\n+    \/\/ are performed on these frequencies.\n+    return (f > 1) ? 1 : f;\n+  }\n+\n+public:\n+  PathFrequency(Node* dom, PhaseIdealLoop* phase)\n+    : _dom(dom), _stack(0), _phase(phase) {\n+  }\n+\n+  float to(Node* n);\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/loopnode.hpp","additions":97,"deletions":6,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -343,3 +343,5 @@\n-          get_ctrl(m->in(2)) != n_ctrl &&\n-          get_ctrl(m->in(3)) != n_ctrl) {\n-        \/\/ Move the AddP up to dominating point\n+          get_ctrl(m->in(AddPNode::Base)) != n_ctrl &&\n+          get_ctrl(m->in(AddPNode::Address)) != n_ctrl &&\n+          get_ctrl(m->in(AddPNode::Offset)) != n_ctrl) {\n+        \/\/ Move the AddP up to the dominating point. That's fine because control of m's inputs\n+        \/\/ must dominate get_ctrl(m) == n_ctrl and we just checked that the input controls are != n_ctrl.\n@@ -1140,0 +1142,5 @@\n+  \/\/ Pushing a shift through the iv Phi can get in the way of addressing optimizations or range check elimination\n+  if (n_blk->is_BaseCountedLoop() && n->Opcode() == Op_LShift(n_blk->as_BaseCountedLoop()->bt()) &&\n+      n->in(1) == n_blk->as_BaseCountedLoop()->phi()) {\n+    return n;\n+  }\n@@ -1599,1 +1606,1 @@\n-\/\/ Tranform:\n+\/\/ Transform:\n@@ -1873,0 +1880,2 @@\n+\/\/ Compute the early control of a node by following its inputs until we reach\n+\/\/ nodes that are pinned. Then compute the LCA of the control of all pinned nodes.\n@@ -1888,2 +1897,2 @@\n-        if (in == NULL) {\n-          continue;\n+        if (in != NULL) {\n+          wq.push(in);\n@@ -1891,1 +1900,0 @@\n-        wq.push(in);\n@@ -1895,4 +1903,2 @@\n-      assert(is_dominator(c, n_ctrl), \"\");\n-      if (early_ctrl == NULL) {\n-        early_ctrl = c;\n-      } else if (is_dominator(early_ctrl, c)) {\n+      assert(is_dominator(c, n_ctrl), \"control input must dominate current control\");\n+      if (early_ctrl == NULL || is_dominator(early_ctrl, c)) {\n@@ -2006,1 +2012,1 @@\n-Node* PhaseIdealLoop::clone_iff(PhiNode *phi, IdealLoopTree *loop) {\n+Node* PhaseIdealLoop::clone_iff(PhiNode* phi) {\n@@ -2013,1 +2019,1 @@\n-      _igvn.replace_input_of(phi, i, clone_iff(b->as_Phi(), loop));\n+      _igvn.replace_input_of(phi, i, clone_iff(b->as_Phi()));\n@@ -2101,1 +2107,1 @@\n-CmpNode *PhaseIdealLoop::clone_bool( PhiNode *phi, IdealLoopTree *loop ) {\n+CmpNode*PhaseIdealLoop::clone_bool(PhiNode* phi) {\n@@ -2107,1 +2113,1 @@\n-      _igvn.replace_input_of(phi, i, clone_bool( b->as_Phi(), loop ));\n+      _igvn.replace_input_of(phi, i, clone_bool(b->as_Phi()));\n@@ -2446,1 +2452,1 @@\n-    \/\/ Some other transformation may have pessimistically assign some\n+    \/\/ Some other transformation may have pessimistically assigned some\n@@ -2518,1 +2524,0 @@\n-  Dict* dict = cm.dict();\n@@ -2530,14 +2535,1 @@\n-  uint i;\n-  for (i = 0; i < loop->_body.size(); i++) {\n-    Node* old = loop->_body.at(i);\n-    Node* nnn = old->clone();\n-    old_new.map(old->_idx, nnn);\n-    if (old->is_reduction()) {\n-      \/\/ Reduction flag is not copied by default. Copy it here when cloning the entire loop body.\n-      nnn->add_flag(Node::Flag_is_reduction);\n-    }\n-    if (C->do_vector_loop()) {\n-      cm.verify_insert_and_clone(old, nnn, cm.clone_idx());\n-    }\n-    _igvn.register_new_node_with_optimizer(nnn);\n-  }\n+  clone_loop_body(loop->_body, old_new, &cm);\n@@ -2550,23 +2542,1 @@\n-  for( i = 0; i < loop->_body.size(); i++ ) {\n-    Node *old = loop->_body.at(i);\n-    Node *nnn = old_new[old->_idx];\n-    \/\/ Fix CFG\/Loop controlling the new node\n-    if (has_ctrl(old)) {\n-      set_ctrl(nnn, old_new[get_ctrl(old)->_idx]);\n-    } else {\n-      set_loop(nnn, outer_loop->_parent);\n-      if (old->outcnt() > 0) {\n-        set_idom( nnn, old_new[idom(old)->_idx], dd );\n-      }\n-    }\n-    \/\/ Correct edges to the new node\n-    for( uint j = 0; j < nnn->req(); j++ ) {\n-        Node *n = nnn->in(j);\n-        if( n ) {\n-          IdealLoopTree *old_in_loop = get_loop( has_ctrl(n) ? get_ctrl(n) : n );\n-          if( loop->is_member( old_in_loop ) )\n-            nnn->set_req(j, old_new[n->_idx]);\n-        }\n-    }\n-    _igvn.hash_find_insert(nnn);\n-  }\n+  fix_body_edges(loop->_body, loop, old_new, dd, outer_loop->_parent, false);\n@@ -2584,2 +2554,72 @@\n-  for( i = 0; i < loop->_body.size(); i++ ) {\n-    Node* old = loop->_body.at(i);\n+  fix_ctrl_uses(loop->_body, loop, old_new, mode, side_by_side_idom, &cm, worklist);\n+\n+  \/\/ Step 4: If loop-invariant use is not control, it must be dominated by a\n+  \/\/ loop exit IfFalse\/IfTrue.  Find \"proper\" loop exit.  Make a Region\n+  \/\/ there if needed.  Make a Phi there merging old and new used values.\n+  Node_List *split_if_set = NULL;\n+  Node_List *split_bool_set = NULL;\n+  Node_List *split_cex_set = NULL;\n+  fix_data_uses(loop->_body, loop, mode, outer_loop, new_counter, old_new, worklist, split_if_set, split_bool_set, split_cex_set);\n+\n+  for (uint i = 0; i < extra_data_nodes.size(); i++) {\n+    Node* old = extra_data_nodes.at(i);\n+    clone_loop_handle_data_uses(old, old_new, loop, outer_loop, split_if_set,\n+                                split_bool_set, split_cex_set, worklist, new_counter,\n+                                mode);\n+  }\n+\n+  \/\/ Check for IFs that need splitting\/cloning.  Happens if an IF outside of\n+  \/\/ the loop uses a condition set in the loop.  The original IF probably\n+  \/\/ takes control from one or more OLD Regions (which in turn get from NEW\n+  \/\/ Regions).  In any case, there will be a set of Phis for each merge point\n+  \/\/ from the IF up to where the original BOOL def exists the loop.\n+  finish_clone_loop(split_if_set, split_bool_set, split_cex_set);\n+\n+}\n+\n+void PhaseIdealLoop::finish_clone_loop(Node_List* split_if_set, Node_List* split_bool_set, Node_List* split_cex_set) {\n+  if (split_if_set) {\n+    while (split_if_set->size()) {\n+      Node *iff = split_if_set->pop();\n+      if (iff->in(1)->is_Phi()) {\n+        Node *b = clone_iff(iff->in(1)->as_Phi());\n+        _igvn.replace_input_of(iff, 1, b);\n+      }\n+    }\n+  }\n+  if (split_bool_set) {\n+    while (split_bool_set->size()) {\n+      Node *b = split_bool_set->pop();\n+      Node *phi = b->in(1);\n+      assert(phi->is_Phi(), \"\");\n+      CmpNode *cmp = clone_bool((PhiNode*) phi);\n+      _igvn.replace_input_of(b, 1, cmp);\n+    }\n+  }\n+  if (split_cex_set) {\n+    while (split_cex_set->size()) {\n+      Node *b = split_cex_set->pop();\n+      assert(b->in(0)->is_Region(), \"\");\n+      assert(b->in(1)->is_Phi(), \"\");\n+      assert(b->in(0)->in(0) == b->in(1)->in(0), \"\");\n+      split_up(b, b->in(0), NULL);\n+    }\n+  }\n+}\n+\n+void PhaseIdealLoop::fix_data_uses(Node_List& body, IdealLoopTree* loop, CloneLoopMode mode, IdealLoopTree* outer_loop,\n+                                   uint new_counter, Node_List &old_new, Node_List &worklist, Node_List*& split_if_set,\n+                                   Node_List*& split_bool_set, Node_List*& split_cex_set) {\n+  for(uint i = 0; i < body.size(); i++ ) {\n+    Node* old = body.at(i);\n+    clone_loop_handle_data_uses(old, old_new, loop, outer_loop, split_if_set,\n+                                split_bool_set, split_cex_set, worklist, new_counter,\n+                                mode);\n+  }\n+}\n+\n+void PhaseIdealLoop::fix_ctrl_uses(const Node_List& body, const IdealLoopTree* loop, Node_List &old_new, CloneLoopMode mode,\n+                                   Node* side_by_side_idom, CloneMap* cm, Node_List &worklist) {\n+  LoopNode* head = loop->_head->as_Loop();\n+  for(uint i = 0; i < body.size(); i++ ) {\n+    Node* old = body.at(i);\n@@ -2590,1 +2630,1 @@\n-    for (DUIterator_Fast jmax, j = old->fast_outs(jmax); j < jmax; j++)\n+    for (DUIterator_Fast jmax, j = old->fast_outs(jmax); j < jmax; j++) {\n@@ -2592,0 +2632,1 @@\n+    }\n@@ -2593,1 +2634,1 @@\n-    while( worklist.size() ) {  \/\/ Visit all uses\n+    while (worklist.size()) {  \/\/ Visit all uses\n@@ -2596,2 +2637,2 @@\n-      IdealLoopTree *use_loop = get_loop( has_ctrl(use) ? get_ctrl(use) : use );\n-      if( !loop->is_member( use_loop ) && use->is_CFG() ) {\n+      IdealLoopTree *use_loop = get_loop(has_ctrl(use) ? get_ctrl(use) : use );\n+      if (!loop->is_member(use_loop) && use->is_CFG()) {\n@@ -2599,1 +2640,1 @@\n-        assert( use->is_Proj(), \"\" );\n+        assert(use->is_Proj(), \"\" );\n@@ -2623,2 +2664,2 @@\n-        if (C->do_vector_loop()) {\n-          cm.verify_insert_and_clone(use, newuse, cm.clone_idx());\n+        if (C->do_vector_loop() && cm != NULL) {\n+          cm->verify_insert_and_clone(use, newuse, cm->clone_idx());\n@@ -2636,2 +2677,2 @@\n-        uint dd_r = MIN2(dom_depth(newuse),dom_depth(use));\n-        assert( dd_r >= dom_depth(dom_lca(newuse,use)), \"\" );\n+        uint dd_r = MIN2(dom_depth(newuse), dom_depth(use));\n+        assert(dd_r >= dom_depth(dom_lca(newuse, use)), \"\" );\n@@ -2644,1 +2685,1 @@\n-          if( useuse->in(0) == use ) {\n+          if (useuse->in(0) == use) {\n@@ -2647,1 +2688,1 @@\n-            if( useuse->is_CFG() ) {\n+            if (useuse->is_CFG()) {\n@@ -2655,1 +2696,1 @@\n-          for( uint k = 1; k < useuse->req(); k++ ) {\n+          for (uint k = 1; k < useuse->req(); k++) {\n@@ -2672,2 +2713,2 @@\n-        r->set_req( 1, newuse );\n-        r->set_req( 2,    use );\n+        r->set_req(1, newuse);\n+        r->set_req(2,    use);\n@@ -2676,1 +2717,1 @@\n-        set_idom(r, !side_by_side_idom ? newuse->in(0) : side_by_side_idom, dd_r);\n+        set_idom(r, (side_by_side_idom == NULL) ? newuse->in(0) : side_by_side_idom, dd_r);\n@@ -2680,0 +2721,1 @@\n+}\n@@ -2681,31 +2723,16 @@\n-  \/\/ Step 4: If loop-invariant use is not control, it must be dominated by a\n-  \/\/ loop exit IfFalse\/IfTrue.  Find \"proper\" loop exit.  Make a Region\n-  \/\/ there if needed.  Make a Phi there merging old and new used values.\n-  Node_List *split_if_set = NULL;\n-  Node_List *split_bool_set = NULL;\n-  Node_List *split_cex_set = NULL;\n-  for( i = 0; i < loop->_body.size(); i++ ) {\n-    Node* old = loop->_body.at(i);\n-    clone_loop_handle_data_uses(old, old_new, loop, outer_loop, split_if_set,\n-                                split_bool_set, split_cex_set, worklist, new_counter,\n-                                mode);\n-  }\n-\n-  for (i = 0; i < extra_data_nodes.size(); i++) {\n-    Node* old = extra_data_nodes.at(i);\n-    clone_loop_handle_data_uses(old, old_new, loop, outer_loop, split_if_set,\n-                                split_bool_set, split_cex_set, worklist, new_counter,\n-                                mode);\n-  }\n-\n-  \/\/ Check for IFs that need splitting\/cloning.  Happens if an IF outside of\n-  \/\/ the loop uses a condition set in the loop.  The original IF probably\n-  \/\/ takes control from one or more OLD Regions (which in turn get from NEW\n-  \/\/ Regions).  In any case, there will be a set of Phis for each merge point\n-  \/\/ from the IF up to where the original BOOL def exists the loop.\n-  if (split_if_set) {\n-    while (split_if_set->size()) {\n-      Node *iff = split_if_set->pop();\n-      if (iff->in(1)->is_Phi()) {\n-        Node *b = clone_iff(iff->in(1)->as_Phi(), loop);\n-        _igvn.replace_input_of(iff, 1, b);\n+void PhaseIdealLoop::fix_body_edges(const Node_List &body, IdealLoopTree* loop, const Node_List &old_new, int dd,\n+                                    IdealLoopTree* parent, bool partial) {\n+  for(uint i = 0; i < body.size(); i++ ) {\n+    Node *old = body.at(i);\n+    Node *nnn = old_new[old->_idx];\n+    \/\/ Fix CFG\/Loop controlling the new node\n+    if (has_ctrl(old)) {\n+      set_ctrl(nnn, old_new[get_ctrl(old)->_idx]);\n+    } else {\n+      set_loop(nnn, parent);\n+      if (old->outcnt() > 0) {\n+        Node* dom = idom(old);\n+        if (old_new[dom->_idx] != NULL) {\n+          dom = old_new[dom->_idx];\n+          set_idom(nnn, dom, dd );\n+        }\n@@ -2714,8 +2741,14 @@\n-  }\n-  if (split_bool_set) {\n-    while (split_bool_set->size()) {\n-      Node *b = split_bool_set->pop();\n-      Node *phi = b->in(1);\n-      assert(phi->is_Phi(), \"\");\n-      CmpNode *cmp = clone_bool((PhiNode*)phi, loop);\n-      _igvn.replace_input_of(b, 1, cmp);\n+    \/\/ Correct edges to the new node\n+    for (uint j = 0; j < nnn->req(); j++) {\n+        Node *n = nnn->in(j);\n+        if (n != NULL) {\n+          IdealLoopTree *old_in_loop = get_loop(has_ctrl(n) ? get_ctrl(n) : n);\n+          if (loop->is_member(old_in_loop)) {\n+            if (old_new[n->_idx] != NULL) {\n+              nnn->set_req(j, old_new[n->_idx]);\n+            } else {\n+              assert(!body.contains(n), \"\");\n+              assert(partial, \"node not cloned\");\n+            }\n+          }\n+        }\n@@ -2723,0 +2756,1 @@\n+    _igvn.hash_find_insert(nnn);\n@@ -2724,7 +2758,10 @@\n-  if (split_cex_set) {\n-    while (split_cex_set->size()) {\n-      Node *b = split_cex_set->pop();\n-      assert(b->in(0)->is_Region(), \"\");\n-      assert(b->in(1)->is_Phi(), \"\");\n-      assert(b->in(0)->in(0) == b->in(1)->in(0), \"\");\n-      split_up(b, b->in(0), NULL);\n+}\n+\n+void PhaseIdealLoop::clone_loop_body(const Node_List& body, Node_List &old_new, CloneMap* cm) {\n+  for (uint i = 0; i < body.size(); i++) {\n+    Node* old = body.at(i);\n+    Node* nnn = old->clone();\n+    old_new.map(old->_idx, nnn);\n+    if (old->is_reduction()) {\n+      \/\/ Reduction flag is not copied by default. Copy it here when cloning the entire loop body.\n+      nnn->add_flag(Node::Flag_is_reduction);\n@@ -2732,0 +2769,4 @@\n+    if (C->do_vector_loop() && cm != NULL) {\n+      cm->verify_insert_and_clone(old, nnn, cm->clone_idx());\n+    }\n+    _igvn.register_new_node_with_optimizer(nnn);\n@@ -2733,1 +2774,0 @@\n-\n@@ -3948,0 +3988,279 @@\n+\/\/ Transform:\n+\/\/\n+\/\/ loop<-----------------+\n+\/\/  |                    |\n+\/\/ stmt1 stmt2 .. stmtn  |\n+\/\/  |     |        |     |\n+\/\/  \\     |       \/      |\n+\/\/    v   v     v        |\n+\/\/       region          |\n+\/\/         |             |\n+\/\/     shared_stmt       |\n+\/\/         |             |\n+\/\/         v             |\n+\/\/         if            |\n+\/\/         \/ \\           |\n+\/\/        |   -----------+\n+\/\/        v\n+\/\/\n+\/\/ into:\n+\/\/\n+\/\/    loop<-------------------+\n+\/\/     |                      |\n+\/\/     v                      |\n+\/\/ +->loop                    |\n+\/\/ |   |                      |\n+\/\/ |  stmt1 stmt2 .. stmtn    |\n+\/\/ |   |     |        |       |\n+\/\/ |   |      \\       \/       |\n+\/\/ |   |       v     v        |\n+\/\/ |   |        region1       |\n+\/\/ |   |           |          |\n+\/\/ |  shared_stmt shared_stmt |\n+\/\/ |   |           |          |\n+\/\/ |   v           v          |\n+\/\/ |   if          if         |\n+\/\/ |   \/\\          \/ \\        |\n+\/\/ +--   |         |   -------+\n+\/\/       \\         \/\n+\/\/        v       v\n+\/\/         region2\n+\/\/\n+\/\/ (region2 is shown to merge mirrored projections of the loop exit\n+\/\/ ifs to make the diagram clearer but they really merge the same\n+\/\/ projection)\n+\/\/\n+\/\/ Conditions for this transformation to trigger:\n+\/\/ - the path through stmt1 is frequent enough\n+\/\/ - the inner loop will be turned into a counted loop after transformation\n+bool PhaseIdealLoop::duplicate_loop_backedge(IdealLoopTree *loop, Node_List &old_new) {\n+  if (!DuplicateBackedge) {\n+    return false;\n+  }\n+  assert(!loop->_head->is_CountedLoop() || StressDuplicateBackedge, \"Non-counted loop only\");\n+  if (!loop->_head->is_Loop()) {\n+    return false;\n+  }\n+\n+  uint estimate = loop->est_loop_clone_sz(1);\n+  if (exceeding_node_budget(estimate)) {\n+    return false;\n+  }\n+\n+  LoopNode *head = loop->_head->as_Loop();\n+\n+  Node* region = NULL;\n+  IfNode* exit_test = NULL;\n+  uint inner;\n+  float f;\n+  if (StressDuplicateBackedge) {\n+    if (head->is_strip_mined()) {\n+      return false;\n+    }\n+    Node* c = head->in(LoopNode::LoopBackControl);\n+\n+    while (c != head) {\n+      if (c->is_Region()) {\n+        region = c;\n+      }\n+      c = idom(c);\n+    }\n+\n+    if (region == NULL) {\n+      return false;\n+    }\n+\n+    inner = 1;\n+  } else {\n+    \/\/ Is the shape of the loop that of a counted loop...\n+    Node* back_control = loop_exit_control(head, loop);\n+    if (back_control == NULL) {\n+      return false;\n+    }\n+\n+    BoolTest::mask bt = BoolTest::illegal;\n+    float cl_prob = 0;\n+    Node* incr = NULL;\n+    Node* limit = NULL;\n+    Node* cmp = loop_exit_test(back_control, loop, incr, limit, bt, cl_prob);\n+    if (cmp == NULL || cmp->Opcode() != Op_CmpI) {\n+      return false;\n+    }\n+\n+    \/\/ With an extra phi for the candidate iv?\n+    if (!incr->is_Phi()) {\n+      return false;\n+    }\n+\n+    PathFrequency pf(head, this);\n+    region = incr->in(0);\n+\n+    \/\/ Go over all paths for the extra phi's region and see if that\n+    \/\/ path is frequent enough and would match the expected iv shape\n+    \/\/ if the extra phi is removed\n+    inner = 0;\n+    for (uint i = 1; i < incr->req(); ++i) {\n+      Node* in = incr->in(i);\n+      Node* trunc1 = NULL;\n+      Node* trunc2 = NULL;\n+      const TypeInteger* iv_trunc_t = NULL;\n+      Node* orig_in = in;\n+      if (!(in = CountedLoopNode::match_incr_with_optional_truncation(in, &trunc1, &trunc2, &iv_trunc_t, T_INT))) {\n+        continue;\n+      }\n+      assert(in->Opcode() == Op_AddI, \"wrong increment code\");\n+      Node* xphi = NULL;\n+      Node* stride = loop_iv_stride(in, loop, xphi);\n+\n+      if (stride == NULL) {\n+        continue;\n+      }\n+\n+      PhiNode* phi = loop_iv_phi(xphi, NULL, head, loop);\n+      if (phi == NULL ||\n+          (trunc1 == NULL && phi->in(LoopNode::LoopBackControl) != incr) ||\n+          (trunc1 != NULL && phi->in(LoopNode::LoopBackControl) != trunc1)) {\n+        return false;\n+      }\n+\n+      f = pf.to(region->in(i));\n+      if (f > 0.5) {\n+        inner = i;\n+        break;\n+      }\n+    }\n+\n+    if (inner == 0) {\n+      return false;\n+    }\n+\n+    exit_test = back_control->in(0)->as_If();\n+  }\n+\n+  if (idom(region)->is_Catch()) {\n+    return false;\n+  }\n+\n+  \/\/ Collect all control nodes that need to be cloned (shared_stmt in the diagram)\n+  Unique_Node_List wq;\n+  wq.push(head->in(LoopNode::LoopBackControl));\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* c = wq.at(i);\n+    assert(get_loop(c) == loop, \"not in the right loop?\");\n+    if (c->is_Region()) {\n+      if (c != region) {\n+        for (uint j = 1; j < c->req(); ++j) {\n+          wq.push(c->in(j));\n+        }\n+      }\n+    } else {\n+      wq.push(c->in(0));\n+    }\n+    assert(!is_dominator(c, region) || c == region, \"shouldn't go above region\");\n+  }\n+\n+  Node* region_dom = idom(region);\n+\n+  \/\/ Can't do the transformation if this would cause a membar pair to\n+  \/\/ be split\n+  for (uint i = 0; i < wq.size(); i++) {\n+    Node* c = wq.at(i);\n+    if (c->is_MemBar() && (c->as_MemBar()->trailing_store() || c->as_MemBar()->trailing_load_store())) {\n+      assert(c->as_MemBar()->leading_membar()->trailing_membar() == c, \"bad membar pair\");\n+      if (!wq.member(c->as_MemBar()->leading_membar())) {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  \/\/ Collect data nodes that need to be clones as well\n+  int dd = dom_depth(head);\n+\n+  for (uint i = 0; i < loop->_body.size(); ++i) {\n+    Node* n = loop->_body.at(i);\n+    if (has_ctrl(n)) {\n+      Node* c = get_ctrl(n);\n+      if (wq.member(c)) {\n+        wq.push(n);\n+      }\n+    } else {\n+      set_idom(n, idom(n), dd);\n+    }\n+  }\n+\n+  \/\/ clone shared_stmt\n+  clone_loop_body(wq, old_new, NULL);\n+\n+  Node* region_clone = old_new[region->_idx];\n+  region_clone->set_req(inner, C->top());\n+  set_idom(region, region->in(inner), dd);\n+\n+  \/\/ Prepare the outer loop\n+  Node* outer_head = new LoopNode(head->in(LoopNode::EntryControl), old_new[head->in(LoopNode::LoopBackControl)->_idx]);\n+  register_control(outer_head, loop->_parent, outer_head->in(LoopNode::EntryControl));\n+  _igvn.replace_input_of(head, LoopNode::EntryControl, outer_head);\n+  set_idom(head, outer_head, dd);\n+\n+  fix_body_edges(wq, loop, old_new, dd, loop->_parent, true);\n+\n+  \/\/ Make one of the shared_stmt copies only reachable from stmt1, the\n+  \/\/ other only from stmt2..stmtn.\n+  Node* dom = NULL;\n+  for (uint i = 1; i < region->req(); ++i) {\n+    if (i != inner) {\n+      _igvn.replace_input_of(region, i, C->top());\n+    }\n+    Node* in = region_clone->in(i);\n+    if (in->is_top()) {\n+      continue;\n+    }\n+    if (dom == NULL) {\n+      dom = in;\n+    } else {\n+      dom = dom_lca(dom, in);\n+    }\n+  }\n+\n+  set_idom(region_clone, dom, dd);\n+\n+  \/\/ Set up the outer loop\n+  for (uint i = 0; i < head->outcnt(); i++) {\n+    Node* u = head->raw_out(i);\n+    if (u->is_Phi()) {\n+      Node* outer_phi = u->clone();\n+      outer_phi->set_req(0, outer_head);\n+      Node* backedge = old_new[u->in(LoopNode::LoopBackControl)->_idx];\n+      if (backedge == NULL) {\n+        backedge = u->in(LoopNode::LoopBackControl);\n+      }\n+      outer_phi->set_req(LoopNode::LoopBackControl, backedge);\n+      register_new_node(outer_phi, outer_head);\n+      _igvn.replace_input_of(u, LoopNode::EntryControl, outer_phi);\n+    }\n+  }\n+\n+  \/\/ create control and data nodes for out of loop uses (including region2)\n+  Node_List worklist;\n+  uint new_counter = C->unique();\n+  fix_ctrl_uses(wq, loop, old_new, ControlAroundStripMined, outer_head, NULL, worklist);\n+\n+  Node_List *split_if_set = NULL;\n+  Node_List *split_bool_set = NULL;\n+  Node_List *split_cex_set = NULL;\n+  fix_data_uses(wq, loop, ControlAroundStripMined, head->is_strip_mined() ? loop->_parent : loop, new_counter, old_new, worklist, split_if_set, split_bool_set, split_cex_set);\n+\n+  finish_clone_loop(split_if_set, split_bool_set, split_cex_set);\n+\n+  if (exit_test != NULL) {\n+    float cnt = exit_test->_fcnt;\n+    if (cnt != COUNT_UNKNOWN) {\n+      exit_test->_fcnt = cnt * f;\n+      old_new[exit_test->_idx]->as_If()->_fcnt = cnt * (1 - f);\n+    }\n+  }\n+\n+  C->set_major_progress();\n+\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/loopopts.cpp","additions":438,"deletions":119,"binary":false,"changes":557,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -846,17 +846,0 @@\n-uint MachCallNativeNode::size_of() const { return sizeof(*this); }\n-bool MachCallNativeNode::cmp( const Node &n ) const {\n-  MachCallNativeNode &call = (MachCallNativeNode&)n;\n-  return MachCallNode::cmp(call) && !strcmp(_name,call._name)\n-    && _arg_regs == call._arg_regs && _ret_regs == call._ret_regs;\n-}\n-#ifndef PRODUCT\n-void MachCallNativeNode::dump_spec(outputStream *st) const {\n-  st->print(\"%s \",_name);\n-  st->print(\"_arg_regs: \");\n-  CallNativeNode::print_regs(_arg_regs, st);\n-  st->print(\"_ret_regs: \");\n-  CallNativeNode::print_regs(_ret_regs, st);\n-  MachCallNode::dump_spec(st);\n-}\n-#endif\n-\/\/=============================================================================\n","filename":"src\/hotspot\/share\/opto\/machnode.cpp","additions":1,"deletions":18,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-class MachCallNativeNode;\n@@ -264,1 +263,1 @@\n-  \/\/ index of the input which must match the output.  Not nessecary\n+  \/\/ index of the input which must match the output.  Not necessary\n@@ -266,2 +265,2 @@\n-  \/\/ same singleton regiser (e.g., Intel IDIV which binds AX to be\n-  \/\/ both an input and an output).  It is nessecary when the input and\n+  \/\/ same singleton register (e.g., Intel IDIV which binds AX to be\n+  \/\/ both an input and an output).  It is necessary when the input and\n@@ -1056,19 +1055,0 @@\n-class MachCallNativeNode: public MachCallNode {\n-  virtual bool cmp( const Node &n ) const;\n-  virtual uint size_of() const;\n-  void print_regs(const GrowableArray<VMReg>& regs, outputStream* st) const;\n-public:\n-  const char *_name;\n-  GrowableArray<VMReg> _arg_regs;\n-  GrowableArray<VMReg> _ret_regs;\n-\n-  MachCallNativeNode() : MachCallNode() {\n-    init_class_id(Class_MachCallNative);\n-  }\n-\n-  virtual int ret_addr_offset();\n-#ifndef PRODUCT\n-  virtual void dump_spec(outputStream *st) const;\n-#endif\n-};\n-\n","filename":"src\/hotspot\/share\/opto\/machnode.hpp","additions":3,"deletions":23,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -56,0 +56,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -153,0 +154,5 @@\n+#ifndef PRODUCT\n+  if (PrintOptoStatistics) {\n+    Atomic::inc(&PhaseMacroExpand::_GC_barriers_removed_counter);\n+  }\n+#endif\n@@ -752,1 +758,0 @@\n-  ciKlass* klass = NULL;\n@@ -758,1 +763,1 @@\n-  ciType* elem_type = NULL;\n+  const Type* field_type = NULL;\n@@ -768,1 +773,0 @@\n-    klass = res_type->klass();\n@@ -771,2 +775,1 @@\n-      assert(klass->is_instance_klass(), \"must be an instance klass.\");\n-      iklass = klass->as_instance_klass();\n+      iklass = res_type->is_instptr()->instance_klass();\n@@ -777,5 +780,3 @@\n-      assert(klass->is_array_klass() && nfields >= 0, \"must be an array klass.\");\n-      elem_type = klass->as_array_klass()->element_type();\n-      basic_elem_type = elem_type->basic_type();\n-      if (elem_type->is_inlinetype() && !klass->is_flat_array_klass()) {\n-        assert(basic_elem_type == T_PRIMITIVE_OBJECT, \"unexpected element basic type\");\n+      assert(nfields >= 0, \"must be an array klass.\");\n+      basic_elem_type = res_type->is_aryptr()->elem()->array_element_basic_type();\n+      if (basic_elem_type == T_PRIMITIVE_OBJECT && !res_type->is_aryptr()->is_flat()) {\n@@ -786,1 +787,2 @@\n-      if (klass->is_flat_array_klass()) {\n+      field_type = res_type->is_aryptr()->elem();\n+      if (res_type->is_aryptr()->is_flat()) {\n@@ -788,1 +790,1 @@\n-        element_size = klass->as_flat_array_klass()->element_byte_size();\n+        element_size = res_type->is_aryptr()->klass()->as_flat_array_klass()->element_byte_size();\n@@ -821,1 +823,1 @@\n-        elem_type = field->type();\n+        ciType* elem_type = field->type();\n@@ -824,15 +826,17 @@\n-      } else {\n-        offset = array_base + j * (intptr_t)element_size;\n-      }\n-      const Type *field_type;\n-      \/\/ The next code is taken from Parse::do_get_xxx().\n-      if (is_reference_type(basic_elem_type)) {\n-        if (!elem_type->is_loaded()) {\n-          field_type = TypeInstPtr::BOTTOM;\n-        } else if (field != NULL && field->is_static_constant()) {\n-          \/\/ This can happen if the constant oop is non-perm.\n-          ciObject* con = field->constant_value().as_object();\n-          \/\/ Do not \"join\" in the previous type; it doesn't add value,\n-          \/\/ and may yield a vacuous result if the field is of interface type.\n-          field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n-          assert(field_type != NULL, \"field singleton type must be consistent\");\n+        \/\/ The next code is taken from Parse::do_get_xxx().\n+        if (is_reference_type(basic_elem_type)) {\n+          if (!elem_type->is_loaded()) {\n+            field_type = TypeInstPtr::BOTTOM;\n+          } else if (field != NULL && field->is_static_constant()) {\n+            ciObject* con = field->constant_value().as_object();\n+            \/\/ Do not \"join\" in the previous type; it doesn't add value,\n+            \/\/ and may yield a vacuous result if the field is of interface type.\n+            field_type = TypeOopPtr::make_from_constant(con)->isa_oopptr();\n+            assert(field_type != NULL, \"field singleton type must be consistent\");\n+          } else {\n+            field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n+          }\n+          if (UseCompressedOops) {\n+            field_type = field_type->make_narrowoop();\n+            basic_elem_type = T_NARROWOOP;\n+          }\n@@ -841,5 +845,1 @@\n-          field_type = TypeOopPtr::make_from_klass(elem_type->as_klass());\n-        }\n-        if (UseCompressedOops) {\n-          field_type = field_type->make_narrowoop();\n-          basic_elem_type = T_NARROWOOP;\n+          field_type = Type::get_const_basic_type(basic_elem_type);\n@@ -848,1 +848,1 @@\n-        field_type = Type::get_const_basic_type(basic_elem_type);\n+        offset = array_base + j * (intptr_t)element_size;\n@@ -853,2 +853,2 @@\n-      if (klass->is_flat_array_klass()) {\n-        ciInlineKlass* vk = elem_type->as_inline_klass();\n+      if (res_type->isa_aryptr() && res_type->is_aryptr()->is_flat()) {\n+        ciInlineKlass* vk = res_type->is_aryptr()->elem()->inline_klass();\n@@ -945,1 +945,1 @@\n-  bool allow_oop = (klass == NULL) || !klass->is_flat_array_klass();\n+  bool allow_oop = res_type != NULL && (!res_type->isa_aryptr() || !res_type->is_aryptr()->is_flat());\n@@ -1149,1 +1149,1 @@\n-  \/\/ regardless of scalar replaceable status.\n+  \/\/ regardless scalar replaceable status.\n@@ -1152,3 +1152,3 @@\n-                      tklass->klass()->is_instance_klass() &&\n-                      tklass->klass()->as_instance_klass()->is_box_klass();\n-  if (!alloc->_is_scalar_replaceable && !boxing_alloc && !inline_alloc) {\n+                      tklass->isa_instklassptr() &&\n+                      tklass->is_instklassptr()->instance_klass()->is_box_klass();\n+  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != NULL))) {\n@@ -1183,1 +1183,1 @@\n-              log->identify(tklass->klass()));\n+              log->identify(tklass->exact_klass()));\n@@ -1224,1 +1224,1 @@\n-              log->identify(t->klass()));\n+              log->identify(t->instance_klass()));\n@@ -1246,17 +1246,0 @@\n-\/\/---------------------------set_eden_pointers-------------------------\n-void PhaseMacroExpand::set_eden_pointers(Node* &eden_top_adr, Node* &eden_end_adr) {\n-  if (UseTLAB) {                \/\/ Private allocation: load from TLS\n-    Node* thread = transform_later(new ThreadLocalNode());\n-    int tlab_top_offset = in_bytes(JavaThread::tlab_top_offset());\n-    int tlab_end_offset = in_bytes(JavaThread::tlab_end_offset());\n-    eden_top_adr = basic_plus_adr(top()\/*not oop*\/, thread, tlab_top_offset);\n-    eden_end_adr = basic_plus_adr(top()\/*not oop*\/, thread, tlab_end_offset);\n-  } else {                      \/\/ Shared allocation: load from globals\n-    CollectedHeap* ch = Universe::heap();\n-    address top_adr = (address)ch->top_addr();\n-    address end_adr = (address)ch->end_addr();\n-    eden_top_adr = makecon(TypeRawPtr::make(top_adr));\n-    eden_end_adr = basic_plus_adr(eden_top_adr, end_adr - top_adr);\n-  }\n-}\n-\n@@ -1373,1 +1356,1 @@\n-  if (!UseTLAB && !Universe::heap()->supports_inline_contig_alloc()) {\n+  if (!UseTLAB) {\n@@ -1821,3 +1804,7 @@\n-    ciKlass* k = _igvn.type(klass_node)->is_klassptr()->klass();\n-    if (k->is_array_klass())    \/\/ we know the exact header size in most cases:\n-      header_size = Klass::layout_helper_header_size(k->layout_helper());\n+    if (_igvn.type(klass_node)->isa_aryklassptr()) {   \/\/ we know the exact header size in most cases:\n+      BasicType elem = _igvn.type(klass_node)->is_klassptr()->as_instance_type()->isa_aryptr()->elem()->array_element_basic_type();\n+      if (is_reference_type(elem, true)) {\n+        elem = T_OBJECT;\n+      }\n+      header_size = Klass::layout_helper_header_size(Klass::array_layout_helper(elem));\n+    }\n@@ -2025,1 +2012,1 @@\n-  ciKlass* k = _igvn.type(klass_node)->is_klassptr()->klass();\n+  const TypeAryKlassPtr* ary_klass_t = _igvn.type(klass_node)->isa_aryklassptr();\n@@ -2028,1 +2015,1 @@\n-      k->is_type_array_klass()) {\n+      ary_klass_t && ary_klass_t->elem()->isa_klassptr() == NULL) {\n@@ -2302,1 +2289,1 @@\n-    \/\/ Seach for MemBarAcquireLock node and delete it also.\n+    \/\/ Search for MemBarAcquireLock node and delete it also.\n@@ -2319,1 +2306,1 @@\n-  \/\/ Seach for MemBarReleaseLock node and delete it also.\n+  \/\/ Search for MemBarReleaseLock node and delete it also.\n@@ -2392,1 +2379,3 @@\n-  mem_phi->init_req(1, memproj );\n+\n+  mem_phi->init_req(1, memproj);\n+\n@@ -2394,0 +2383,1 @@\n+\n@@ -2448,0 +2438,1 @@\n+\n@@ -2500,1 +2491,1 @@\n-  const bool alloc_in_place = (UseTLAB || Universe::heap()->supports_inline_contig_alloc());\n+  const bool alloc_in_place = UseTLAB;\n@@ -2794,0 +2785,1 @@\n+  NOT_PRODUCT(int membar_before = count_MemBar(C);)\n@@ -2819,0 +2811,5 @@\n+#ifndef PRODUCT\n+        if (success && PrintOptoStatistics) {\n+          Atomic::inc(&PhaseMacroExpand::_monitor_objects_removed_counter);\n+        }\n+#endif\n@@ -2837,0 +2834,5 @@\n+#ifndef PRODUCT\n+        if (success && PrintOptoStatistics) {\n+          Atomic::inc(&PhaseMacroExpand::_objs_scalar_replaced_counter);\n+        }\n+#endif\n@@ -2872,0 +2874,6 @@\n+#ifndef PRODUCT\n+  if (PrintOptoStatistics) {\n+    int membar_after = count_MemBar(C);\n+    Atomic::add(&PhaseMacroExpand::_memory_barriers_removed_counter, membar_before - membar_after);\n+  }\n+#endif\n@@ -3070,0 +3078,35 @@\n+\n+#ifndef PRODUCT\n+int PhaseMacroExpand::_objs_scalar_replaced_counter = 0;\n+int PhaseMacroExpand::_monitor_objects_removed_counter = 0;\n+int PhaseMacroExpand::_GC_barriers_removed_counter = 0;\n+int PhaseMacroExpand::_memory_barriers_removed_counter = 0;\n+\n+void PhaseMacroExpand::print_statistics() {\n+  tty->print(\"Objects scalar replaced = %d, \", Atomic::load(&_objs_scalar_replaced_counter));\n+  tty->print(\"Monitor objects removed = %d, \", Atomic::load(&_monitor_objects_removed_counter));\n+  tty->print(\"GC barriers removed = %d, \", Atomic::load(&_GC_barriers_removed_counter));\n+  tty->print_cr(\"Memory barriers removed = %d\", Atomic::load(&_memory_barriers_removed_counter));\n+}\n+\n+int PhaseMacroExpand::count_MemBar(Compile *C) {\n+  if (!PrintOptoStatistics) {\n+    return 0;\n+  }\n+  Unique_Node_List ideal_nodes;\n+  int total = 0;\n+  ideal_nodes.map(C->live_nodes(), NULL);\n+  ideal_nodes.push(C->root());\n+  for (uint next = 0; next < ideal_nodes.size(); ++next) {\n+    Node* n = ideal_nodes.at(next);\n+    if (n->is_MemBar()) {\n+      total++;\n+    }\n+    for (DUIterator_Fast imax, i = n->fast_outs(imax); i < imax; i++) {\n+      Node* m = n->fast_out(i);\n+      ideal_nodes.push(m);\n+    }\n+  }\n+  return total;\n+}\n+#endif\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":114,"deletions":71,"binary":false,"changes":185,"status":"modified"},{"patch":"@@ -61,1 +61,0 @@\n-  void set_eden_pointers(Node* &eden_top_adr, Node* &eden_end_adr);\n@@ -228,0 +227,9 @@\n+#ifndef PRODUCT\n+    static int _objs_scalar_replaced_counter;\n+    static int _monitor_objects_removed_counter;\n+    static int _GC_barriers_removed_counter;\n+    static int _memory_barriers_removed_counter;\n+    static void print_statistics();\n+    static int count_MemBar(Compile *C);\n+#endif\n+\n","filename":"src\/hotspot\/share\/opto\/macro.hpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1086,1 +1086,1 @@\n-                         false \/*unaligned*\/, is_mismatched));\n+                         false \/*require_atomic_access*\/, false \/*unaligned*\/, is_mismatched));\n@@ -1428,2 +1428,2 @@\n-  if (top_dest != NULL && top_dest->klass() != NULL) {\n-    dest_elem = top_dest->klass()->as_array_klass()->element_type()->basic_type();\n+  if (top_src != NULL && top_src->elem() != Type::BOTTOM) {\n+    src_elem = top_src->elem()->array_element_basic_type();\n@@ -1431,2 +1431,2 @@\n-  if (top_src != NULL && top_src->klass() != NULL) {\n-    src_elem = top_src->klass()->as_array_klass()->element_type()->basic_type();\n+  if (top_dest != NULL && top_dest->elem() != Type::BOTTOM) {\n+    dest_elem = top_dest->elem()->array_element_basic_type();\n@@ -1434,1 +1434,1 @@\n-  if (src_elem == T_ARRAY || (src_elem == T_PRIMITIVE_OBJECT && top_src->klass()->is_obj_array_klass())) {\n+  if (src_elem == T_ARRAY || src_elem == T_NARROWOOP || (src_elem == T_PRIMITIVE_OBJECT && top_src->klass()->is_obj_array_klass())) {\n@@ -1437,1 +1437,1 @@\n-  if (dest_elem == T_ARRAY || (dest_elem == T_PRIMITIVE_OBJECT && top_dest->klass()->is_obj_array_klass())) {\n+  if (dest_elem == T_ARRAY || dest_elem == T_NARROWOOP || (dest_elem == T_PRIMITIVE_OBJECT && top_dest->klass()->is_obj_array_klass())) {\n","filename":"src\/hotspot\/share\/opto\/macroArrayCopy.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1345,7 +1345,0 @@\n-    else if( mcall->is_MachCallNative() ) {\n-      MachCallNativeNode* mach_call_native = mcall->as_MachCallNative();\n-      CallNativeNode* call_native = call->as_CallNative();\n-      mach_call_native->_name = call_native->_name;\n-      mach_call_native->_arg_regs = call_native->_arg_regs;\n-      mach_call_native->_ret_regs = call_native->_ret_regs;\n-    }\n@@ -1386,2 +1379,0 @@\n-  if( call != NULL && call->is_CallNative() )\n-    out_arg_limit_per_call = OptoReg::add(out_arg_limit_per_call, call->as_CallNative()->_shadow_space_bytes);\n@@ -2274,1 +2265,1 @@\n-    case Op_BoxLock:         \/\/ Cant match until we get stack-regs in ADLC\n+    case Op_BoxLock:         \/\/ Can't match until we get stack-regs in ADLC\n@@ -2305,1 +2296,3 @@\n-    case Op_LoadVectorMasked:\n+    case Op_CompressV:\n+    case Op_CompressM:\n+    case Op_ExpandV:\n@@ -2507,0 +2500,2 @@\n+    case Op_SignumVF:\n+    case Op_SignumVD:\n","filename":"src\/hotspot\/share\/opto\/matcher.cpp","additions":6,"deletions":11,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -330,0 +330,4 @@\n+  \/\/ Identify extra cases that we might want to vectorize automatically\n+  \/\/ And exclude cases which are not profitable to auto-vectorize.\n+  static const bool match_rule_supported_superword(int opcode, int vlen, BasicType bt);\n+\n@@ -336,0 +340,2 @@\n+  static const bool vector_needs_partial_operations(Node* node, const TypeVect* vt);\n+\n","filename":"src\/hotspot\/share\/opto\/matcher.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -183,1 +183,1 @@\n-          if (tklass->klass_is_exact() && !tklass->klass()->equals(t_oop->klass())) {\n+          if (tklass->klass_is_exact() && !tklass->exact_klass()->equals(t_oop->is_instptr()->exact_klass())) {\n@@ -271,1 +271,1 @@\n-  if( tp->base() != Type::AnyPtr &&\n+  if (tp->base() != Type::AnyPtr &&\n@@ -273,3 +273,3 @@\n-        toop->klass() != NULL &&\n-        toop->klass()->is_java_lang_Object() &&\n-        toop->offset() == Type::OffsetBot) ) {\n+        toop->isa_instptr() &&\n+        toop->is_instptr()->instance_klass()->is_java_lang_Object() &&\n+        toop->offset() == Type::OffsetBot)) {\n@@ -385,1 +385,1 @@\n-  \/\/ initializations was removed. After macro-expansion all stores catched\n+  \/\/ initializations was removed. After macro-expansion all stores caught\n@@ -556,1 +556,1 @@\n-      BasicType ary_elem = ary_t->klass()->as_array_klass()->element_type()->basic_type();\n+      BasicType ary_elem = ary_t->elem()->array_element_basic_type();\n@@ -856,6 +856,19 @@\n-  return (adr->is_AddP() && adr->in(AddPNode::Base)->is_top() &&\n-          adr->in(AddPNode::Address)->Opcode() == Op_ThreadLocal &&\n-          (adr->in(AddPNode::Offset)->find_intptr_t_con(-1) ==\n-           in_bytes(JavaThread::osthread_offset()) ||\n-           adr->in(AddPNode::Offset)->find_intptr_t_con(-1) ==\n-           in_bytes(JavaThread::threadObj_offset())));\n+  if (adr->is_AddP() && adr->in(AddPNode::Base)->is_top() &&\n+      adr->in(AddPNode::Address)->Opcode() == Op_ThreadLocal) {\n+\n+    jlong offset = adr->in(AddPNode::Offset)->find_intptr_t_con(-1);\n+    int offsets[] = {\n+      in_bytes(JavaThread::osthread_offset()),\n+      in_bytes(JavaThread::threadObj_offset()),\n+      in_bytes(JavaThread::vthread_offset()),\n+      in_bytes(JavaThread::extentLocalCache_offset()),\n+    };\n+\n+    for (size_t i = 0; i < sizeof offsets \/ sizeof offsets[0]; i++) {\n+      if (offset == offsets[i]) {\n+        return true;\n+      }\n+    }\n+  }\n+\n+  return false;\n@@ -867,2 +880,2 @@\n-Node *LoadNode::make(PhaseGVN& gvn, Node *ctl, Node *mem, Node *adr, const TypePtr* adr_type, const Type *rt, BasicType bt, MemOrd mo,\n-                     ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {\n+Node* LoadNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, BasicType bt, MemOrd mo,\n+                     ControlDependency control_dependency, bool require_atomic_access, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {\n@@ -890,1 +903,1 @@\n-  case T_LONG:    load = new LoadLNode (ctl, mem, adr, adr_type, rt->is_long(), mo, control_dependency); break;\n+  case T_LONG:    load = new LoadLNode (ctl, mem, adr, adr_type, rt->is_long(), mo, control_dependency, require_atomic_access); break;\n@@ -892,1 +905,1 @@\n-  case T_DOUBLE:  load = new LoadDNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency); break;\n+  case T_DOUBLE:  load = new LoadDNode (ctl, mem, adr, adr_type, rt,            mo, control_dependency, require_atomic_access); break;\n@@ -929,36 +942,0 @@\n-LoadLNode* LoadLNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo,\n-                                  ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {\n-  bool require_atomic = true;\n-  LoadLNode* load = new LoadLNode(ctl, mem, adr, adr_type, rt->is_long(), mo, control_dependency, require_atomic);\n-  if (unaligned) {\n-    load->set_unaligned_access();\n-  }\n-  if (mismatched) {\n-    load->set_mismatched_access();\n-  }\n-  if (unsafe) {\n-    load->set_unsafe_access();\n-  }\n-  load->set_barrier_data(barrier_data);\n-  return load;\n-}\n-\n-LoadDNode* LoadDNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, const Type* rt, MemOrd mo,\n-                                  ControlDependency control_dependency, bool unaligned, bool mismatched, bool unsafe, uint8_t barrier_data) {\n-  bool require_atomic = true;\n-  LoadDNode* load = new LoadDNode(ctl, mem, adr, adr_type, rt, mo, control_dependency, require_atomic);\n-  if (unaligned) {\n-    load->set_unaligned_access();\n-  }\n-  if (mismatched) {\n-    load->set_mismatched_access();\n-  }\n-  if (unsafe) {\n-    load->set_unsafe_access();\n-  }\n-  load->set_barrier_data(barrier_data);\n-  return load;\n-}\n-\n-\n-\n@@ -1022,1 +999,3 @@\n-      BasicType ary_elem = ary_t->klass()->as_array_klass()->element_type()->basic_type();\n+      BasicType ary_elem = ary_t->isa_aryptr()->elem()->array_element_basic_type();\n+      if (is_reference_type(ary_elem, true)) ary_elem = T_OBJECT;\n+\n@@ -1333,1 +1312,1 @@\n-                        is_unaligned_access(), is_mismatched_access());\n+                        false \/*require_atomic_access*\/, is_unaligned_access(), is_mismatched_access());\n@@ -1353,1 +1332,1 @@\n-                        is_unaligned_access(), is_mismatched_access());\n+                        false \/*require_atomic_access*\/, is_unaligned_access(), is_mismatched_access());\n@@ -1376,0 +1355,3 @@\n+  const int op = Opcode();\n+  bool require_atomic_access = (op == Op_LoadL && ((LoadLNode*)this)->require_atomic_access()) ||\n+                               (op == Op_LoadD && ((LoadDNode*)this)->require_atomic_access());\n@@ -1378,1 +1360,1 @@\n-                        is_unaligned_access(), is_mismatched);\n+                        require_atomic_access, is_unaligned_access(), is_mismatched);\n@@ -1396,1 +1378,5 @@\n-  StoreNode* st = StoreNode::make(gvn, in(MemNode::Control), in(MemNode::Memory), in(MemNode::Address), raw_adr_type(), val, bt, _mo);\n+  const int op = Opcode();\n+  bool require_atomic_access = (op == Op_StoreL && ((StoreLNode*)this)->require_atomic_access()) ||\n+                               (op == Op_StoreD && ((StoreDNode*)this)->require_atomic_access());\n+  StoreNode* st = StoreNode::make(gvn, in(MemNode::Control), in(MemNode::Memory), in(MemNode::Address),\n+                                  raw_adr_type(), val, bt, _mo, require_atomic_access);\n@@ -1553,1 +1539,1 @@\n-          t_oop->is_ptr_to_boxed_value()), \"invalide conditions\");\n+          t_oop->is_ptr_to_boxed_value()), \"invalid conditions\");\n@@ -2003,2 +1989,4 @@\n-            tp->is_oopptr()->klass()->is_java_lang_Object() ||\n-            tp->is_oopptr()->klass() == ciEnv::current()->Class_klass() ||\n+            !tp->isa_instptr() ||\n+            tp->is_instptr()->instance_klass()->is_java_lang_Object() ||\n+            \/\/ Default value load\n+            tp->is_instptr()->instance_klass() == ciEnv::current()->Class_klass() ||\n@@ -2040,8 +2028,8 @@\n-    assert( off != Type::OffsetBot ||\n-            \/\/ arrays can be cast to Objects\n-            tp->is_klassptr()->klass() == NULL ||\n-            tp->is_klassptr()->klass()->is_java_lang_Object() ||\n-            \/\/ also allow array-loading from the primary supertype\n-            \/\/ array during subtype checks\n-            Opcode() == Op_LoadKlass,\n-            \"Field accesses must be precise\" );\n+    assert(off != Type::OffsetBot ||\n+            !tp->isa_instklassptr() ||\n+           \/\/ arrays can be cast to Objects\n+           tp->isa_instklassptr()->instance_klass()->is_java_lang_Object() ||\n+           \/\/ also allow array-loading from the primary supertype\n+           \/\/ array during subtype checks\n+           Opcode() == Op_LoadKlass,\n+           \"Field accesses must be precise\");\n@@ -2060,2 +2048,2 @@\n-        ciKlass* klass = tkls->klass();\n-        if (klass != NULL && klass->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n+        if (tkls->is_loaded() && tkls->klass_is_exact() && tkls->offset() == in_bytes(Klass::java_mirror_offset())) {\n+          ciKlass* klass = tkls->exact_klass();\n@@ -2086,1 +2074,1 @@\n-    ciKlass* klass = tkls->klass();\n+      ciKlass* klass = tkls->exact_klass();\n@@ -2115,5 +2103,12 @@\n-      ciType *inner = klass;\n-      while( inner->is_obj_array_klass() )\n-        inner = inner->as_obj_array_klass()->base_element_type();\n-      if( inner->is_instance_klass() &&\n-          !inner->as_instance_klass()->flags().is_interface() ) {\n+      ciKlass* klass = NULL;\n+      if (tkls->isa_instklassptr()) {\n+        klass = tkls->is_instklassptr()->instance_klass();\n+      } else {\n+        int dims;\n+        const Type* inner = tkls->is_aryklassptr()->base_element_type(dims);\n+        if (inner->isa_instklassptr()) {\n+          klass = inner->is_instklassptr()->instance_klass();\n+          klass = ciObjArrayKlass::make(klass, dims);\n+        }\n+      }\n+      if (klass != NULL) {\n@@ -2123,2 +2118,2 @@\n-        if( depth < ciKlass::primary_super_limit() &&\n-            depth <= klass->super_depth() ) { \/\/ allow self-depth checks to handle self-check case\n+        if (depth < ciKlass::primary_super_limit() &&\n+            depth <= klass->super_depth()) { \/\/ allow self-depth checks to handle self-check case\n@@ -2137,4 +2132,3 @@\n-    if (tkls->offset() == in_bytes(Klass::layout_helper_offset())\n-        && !klass->is_array_klass() \/\/ not directly typed as an array\n-        && !klass->is_interface()  \/\/ specifically not Serializable & Cloneable\n-        && !klass->is_java_lang_Object()   \/\/ not the supertype of all T[]\n+    if (tkls->offset() == in_bytes(Klass::layout_helper_offset()) &&\n+        tkls->isa_instklassptr() && \/\/ not directly typed as an array\n+        !tkls->is_instklassptr()->instance_klass()->is_java_lang_Object() \/\/ not the supertype of all T[] and specifically not Serializable & Cloneable\n@@ -2383,1 +2377,1 @@\n-    ciInstanceKlass* ik = tinst->klass()->as_instance_klass();\n+    ciInstanceKlass* ik = tinst->instance_klass();\n@@ -2412,1 +2406,1 @@\n-    if( !ik->is_loaded() )\n+    if (!tinst->is_loaded())\n@@ -2415,17 +2409,1 @@\n-      if (tinst->klass_is_exact()) {\n-        return TypeKlassPtr::make(ik);\n-      }\n-      \/\/ See if we can become precise: no subklasses and no interface\n-      \/\/ (Note:  We need to support verified interfaces.)\n-      if (!ik->is_interface() && !ik->has_subklass()) {\n-        \/\/ Add a dependence; if any subclass added we need to recompile\n-        if (!ik->is_final()) {\n-          \/\/ %%% should use stronger assert_unique_concrete_subtype instead\n-          phase->C->dependencies()->assert_leaf_type(ik);\n-        }\n-        \/\/ Return precise klass\n-        return TypeKlassPtr::make(ik);\n-      }\n-\n-      \/\/ Return root of possible klass\n-      return TypeInstKlassPtr::make(TypePtr::NotNull, ik, Type::Offset(0), tinst->flatten_array());\n+      return tinst->as_klass_type(true);\n@@ -2436,33 +2414,4 @@\n-  const TypeAryPtr *tary = tp->isa_aryptr();\n-  if (tary != NULL) {\n-    ciKlass *tary_klass = tary->klass();\n-    if (tary_klass != NULL   \/\/ can be NULL when at BOTTOM or TOP\n-        && tary->offset() == oopDesc::klass_offset_in_bytes()) {\n-      if (tary->klass_is_exact()) {\n-        return TypeKlassPtr::make(tary_klass);\n-      }\n-      ciArrayKlass* ak = tary_klass->as_array_klass();\n-      \/\/ If the klass is an object array, we defer the question to the\n-      \/\/ array component klass.\n-      if (ak->is_obj_array_klass()) {\n-        assert(ak->is_loaded(), \"\");\n-        ciKlass *base_k = ak->as_obj_array_klass()->base_element_klass();\n-        if (base_k->is_loaded() && base_k->is_instance_klass()) {\n-          ciInstanceKlass *ik = base_k->as_instance_klass();\n-          \/\/ See if we can become precise: no subklasses and no interface\n-          \/\/ Do not fold klass loads from [LMyValue. The runtime type might be [QMyValue due to [QMyValue <: [LMyValue\n-          \/\/ and the klass for [QMyValue is not equal to the klass for [LMyValue.\n-          if (!ik->is_interface() && !ik->has_subklass() && (!ik->is_inlinetype() || ak->is_elem_null_free())) {\n-            \/\/ Add a dependence; if any subclass added we need to recompile\n-            if (!ik->is_final()) {\n-              phase->C->dependencies()->assert_leaf_type(ik);\n-            }\n-            \/\/ Return precise array klass\n-            return TypeKlassPtr::make(ak);\n-          }\n-        }\n-        return TypeAryKlassPtr::make(TypePtr::NotNull, ak, Type::Offset(0), tary->is_not_flat(), tary->is_not_null_free(), tary->is_null_free());\n-      } else if (ak->is_type_array_klass()) {\n-        return TypeKlassPtr::make(ak); \/\/ These are always precise\n-      }\n-    }\n+  const TypeAryPtr* tary = tp->isa_aryptr();\n+  if (tary != NULL && tary->elem() != Type::BOTTOM &&\n+      tary->offset() == oopDesc::klass_offset_in_bytes()) {\n+    return tary->as_klass_type(true);\n@@ -2474,5 +2423,3 @@\n-    if (!tkls->is_loaded()) {\n-      return _type;             \/\/ Bail out if not loaded\n-    }\n-    ciKlass* klass = tkls->klass();\n-    if( klass->is_obj_array_klass() &&\n+    if (!tkls->is_loaded())\n+     return _type;             \/\/ Bail out if not loaded\n+    if (tkls->isa_aryklassptr() && tkls->is_aryklassptr()->elem()->isa_klassptr() &&\n@@ -2480,1 +2427,0 @@\n-      ciKlass* elem = klass->as_obj_array_klass()->element_klass();\n@@ -2487,5 +2433,1 @@\n-      return TypeKlassPtr::make(tkls->ptr(), elem, Type::Offset(0));\n-    } else if (klass->is_flat_array_klass() &&\n-               tkls->offset() == in_bytes(ObjArrayKlass::element_klass_offset())) {\n-      ciKlass* elem = klass->as_flat_array_klass()->element_klass();\n-      return TypeInstKlassPtr::make(tkls->ptr(), elem, Type::Offset(0), \/* flatten_array= *\/ true);\n+      return tkls->is_aryklassptr()->elem();\n@@ -2493,1 +2435,1 @@\n-    if( klass->is_instance_klass() && tkls->klass_is_exact() &&\n+    if (tkls->isa_instklassptr() != NULL && tkls->klass_is_exact() &&\n@@ -2495,1 +2437,1 @@\n-      ciKlass* sup = klass->as_instance_klass()->super();\n+      ciKlass* sup = tkls->is_instklassptr()->instance_klass()->super();\n@@ -2550,1 +2492,1 @@\n-  if (toop->isa_instptr() && toop->klass() == phase->C->env()->Class_klass()\n+  if (toop->isa_instptr() && toop->is_instptr()->instance_klass() == phase->C->env()->Class_klass()\n@@ -2558,2 +2500,1 @@\n-            && (tkls->klass()->is_instance_klass() ||\n-              tkls->klass()->is_array_klass())\n+            && (tkls->isa_instklassptr() || tkls->isa_aryklassptr())\n@@ -2682,1 +2623,1 @@\n-StoreNode* StoreNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo) {\n+StoreNode* StoreNode::make(PhaseGVN& gvn, Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, BasicType bt, MemOrd mo, bool require_atomic_access) {\n@@ -2694,1 +2635,1 @@\n-  case T_LONG:    return new StoreLNode(ctl, mem, adr, adr_type, val, mo);\n+  case T_LONG:    return new StoreLNode(ctl, mem, adr, adr_type, val, mo, require_atomic_access);\n@@ -2696,1 +2637,1 @@\n-  case T_DOUBLE:  return new StoreDNode(ctl, mem, adr, adr_type, val, mo);\n+  case T_DOUBLE:  return new StoreDNode(ctl, mem, adr, adr_type, val, mo, require_atomic_access);\n@@ -2721,11 +2662,0 @@\n-StoreLNode* StoreLNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo) {\n-  bool require_atomic = true;\n-  return new StoreLNode(ctl, mem, adr, adr_type, val, mo, require_atomic);\n-}\n-\n-StoreDNode* StoreDNode::make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo) {\n-  bool require_atomic = true;\n-  return new StoreDNode(ctl, mem, adr, adr_type, val, mo, require_atomic);\n-}\n-\n-\n@@ -3219,1 +3149,1 @@\n-  if (!IdealizeClearArrayNode || _is_large) return NULL;\n+  if (_is_large) return NULL;\n@@ -3240,0 +3170,1 @@\n+  if (!IdealizeClearArrayNode) return NULL;\n@@ -4660,1 +4591,1 @@\n-        ciKlass* k = phase->type(klass_node)->is_klassptr()->klass();\n+        ciKlass* k = phase->type(klass_node)->is_instklassptr()->instance_klass();\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":99,"deletions":168,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -58,1 +58,1 @@\n-         OopStore               \/\/ Preceeding oop store, only in StoreCM\n+         OopStore               \/\/ Preceding oop store, only in StoreCM\n@@ -233,2 +233,2 @@\n-  static Node* make(PhaseGVN& gvn, Node *c, Node *mem, Node *adr,\n-                    const TypePtr* at, const Type *rt, BasicType bt,\n+  static Node* make(PhaseGVN& gvn, Node* c, Node* mem, Node* adr,\n+                    const TypePtr* at, const Type* rt, BasicType bt,\n@@ -236,1 +236,1 @@\n-                    bool unaligned = false, bool mismatched = false, bool unsafe = false,\n+                    bool require_atomic_access = false, bool unaligned = false, bool mismatched = false, bool unsafe = false,\n@@ -292,0 +292,2 @@\n+  ControlDependency control_dependency() {return _control_dependency; }\n+\n@@ -421,3 +423,1 @@\n-  static LoadLNode* make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type,\n-                                const Type* rt, MemOrd mo, ControlDependency control_dependency = DependsOnlyOnTest,\n-                                bool unaligned = false, bool mismatched = false, bool unsafe = false, uint8_t barrier_data = 0);\n+\n@@ -473,3 +473,1 @@\n-  static LoadDNode* make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type,\n-                                const Type* rt, MemOrd mo, ControlDependency control_dependency = DependsOnlyOnTest,\n-                                bool unaligned = false, bool mismatched = false, bool unsafe = false, uint8_t barrier_data = 0);\n+\n@@ -615,2 +613,3 @@\n-  static StoreNode* make(PhaseGVN& gvn, Node *c, Node *mem, Node *adr,\n-                         const TypePtr* at, Node *val, BasicType bt, MemOrd mo);\n+  static StoreNode* make(PhaseGVN& gvn, Node* c, Node* mem, Node* adr,\n+                         const TypePtr* at, Node* val, BasicType bt,\n+                         MemOrd mo, bool require_atomic_access = false);\n@@ -697,1 +696,1 @@\n-  static StoreLNode* make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo);\n+\n@@ -733,1 +732,1 @@\n-  static StoreDNode* make_atomic(Node* ctl, Node* mem, Node* adr, const TypePtr* adr_type, Node* val, MemOrd mo);\n+\n@@ -776,1 +775,1 @@\n-\/\/ Preceeding equivalent StoreCMs may be eliminated.\n+\/\/ Preceding equivalent StoreCMs may be eliminated.\n@@ -1183,1 +1182,1 @@\n-\/\/ Model.  Monitor-enter and volatile-load act as Aquires: no following ref\n+\/\/ Model.  Monitor-enter and volatile-load act as Acquires: no following ref\n@@ -1264,1 +1263,1 @@\n-\/\/ visibility.  Inserted independ of any load, as required\n+\/\/ visibility.  Inserted independent of any load, as required\n","filename":"src\/hotspot\/share\/opto\/memnode.hpp","additions":17,"deletions":18,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -277,1 +277,1 @@\n-  unsigned int bit1 = abs_con & (0-abs_con);       \/\/ Extract low bit\n+  unsigned int bit1 = submultiple_power_of_2(abs_con);\n@@ -285,0 +285,5 @@\n+      if (!phase->C->post_loop_opts_phase()) {\n+        \/\/ Defer this because it breaks loop range check hoisting\n+        phase->C->record_for_post_loop_opts_igvn(this);\n+        return MulNode::Ideal(phase, can_reshape);\n+      }\n@@ -289,1 +294,5 @@\n-      \/\/ Sleezy: power-of-2 - 1.  Next time be generic.\n+      if (!phase->C->post_loop_opts_phase()) {\n+        \/\/ Defer this because it breaks loop range check hoisting\n+        phase->C->record_for_post_loop_opts_igvn(this);\n+        return MulNode::Ideal(phase, can_reshape);\n+      }\n@@ -371,1 +380,1 @@\n-  julong bit1 = abs_con & (0-abs_con);      \/\/ Extract low bit\n+  julong bit1 = submultiple_power_of_2(abs_con);\n@@ -375,1 +384,0 @@\n-\n@@ -380,0 +388,5 @@\n+      if (!phase->C->post_loop_opts_phase()) {\n+        \/\/ Defer this because it breaks loop range check hoisting\n+        phase->C->record_for_post_loop_opts_igvn(this);\n+        return MulNode::Ideal(phase, can_reshape);\n+      }\n@@ -383,2 +396,5 @@\n-\n-      \/\/ Sleezy: power-of-2 -1.  Next time be generic.\n+      if (!phase->C->post_loop_opts_phase()) {\n+        \/\/ Defer this because it breaks loop range check hoisting\n+        phase->C->record_for_post_loop_opts_igvn(this);\n+        return MulNode::Ideal(phase, can_reshape);\n+      }\n@@ -836,0 +852,2 @@\n+        \/\/ In general, this optimization cannot be applied for c0 == 31 since\n+        \/\/ 2x << 31 != x << 32 = x << 0 = x (e.g. x = 1: 2 << 31 = 0 != 1)\n@@ -953,1 +971,1 @@\n-    if (add1->in(1) == add1->in(2)) {\n+    if (con != (BitsPerJavaLong - 1) && add1->in(1) == add1->in(2)) {\n@@ -955,0 +973,5 @@\n+      \/\/ Can only be applied if c0 != 63 because:\n+      \/\/ (x + x) << 63 = 2x << 63, while\n+      \/\/ (x + x) << 63 --transform--> x << 64 = x << 0 = x (!= 2x << 63, for example for x = 1)\n+      \/\/ According to the Java spec, chapter 15.19, we only consider the six lowest-order bits of the right-hand operand\n+      \/\/ (i.e. \"right-hand operand\" & 0b111111). Therefore, x << 64 is the same as x << 0 (64 = 0b10000000 & 0b0111111 = 0).\n@@ -1782,0 +1805,4 @@\n+  shift = shift->uncast();\n+  if (shift == NULL) {\n+    return false;\n+  }\n@@ -1794,0 +1821,4 @@\n+    val = val->uncast();\n+    if (val == NULL) {\n+      return false;\n+    }\n","filename":"src\/hotspot\/share\/opto\/mulnode.cpp","additions":38,"deletions":7,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -47,0 +47,1 @@\n+#include \"utilities\/stringUtils.hpp\"\n@@ -673,0 +674,4 @@\n+\n+    if (is_CallStaticJava()) {\n+      compile->remove_unstable_if_trap(as_CallStaticJava(), false);\n+    }\n@@ -1225,0 +1230,3 @@\n+  } else if ((is_IfFalse() || is_IfTrue()) && n->is_If()) {\n+    \/\/ See IfNode::fold_compares\n+    return true;\n@@ -1597,0 +1605,127 @@\n+Node* old_root() {\n+  Matcher* matcher = Compile::current()->matcher();\n+  if (matcher != nullptr) {\n+    Node* new_root = Compile::current()->root();\n+    Node* old_root = matcher->find_old_node(new_root);\n+    if (old_root != nullptr) {\n+      return old_root;\n+    }\n+  }\n+  tty->print(\"old_root: not found.\\n\");\n+  return nullptr;\n+}\n+\n+\/\/ BFS traverse all reachable nodes from start, call callback on them\n+template <typename Callback>\n+void visit_nodes(Node* start, Callback callback, bool traverse_output, bool only_ctrl) {\n+  Unique_Mixed_Node_List worklist;\n+  worklist.add(start);\n+  for (uint i = 0; i < worklist.size(); i++) {\n+    Node* n = worklist[i];\n+    callback(n);\n+    for (uint i = 0; i < n->len(); i++) {\n+      if (!only_ctrl || n->is_Region() || (n->Opcode() == Op_Root) || (i == TypeFunc::Control)) {\n+        \/\/ If only_ctrl is set: Add regions, the root node, or control inputs only\n+        worklist.add(n->in(i));\n+      }\n+    }\n+    if (traverse_output && !only_ctrl) {\n+      for (uint i = 0; i < n->outcnt(); i++) {\n+        worklist.add(n->raw_out(i));\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ BFS traverse from start, return node with idx\n+Node* find_node_by_idx(Node* start, uint idx, bool traverse_output, bool only_ctrl) {\n+  ResourceMark rm;\n+  Node* result = nullptr;\n+  auto callback = [&] (Node* n) {\n+    if (n->_idx == idx) {\n+      if (result != nullptr) {\n+        tty->print(\"find_node_by_idx: \" INTPTR_FORMAT \" and \" INTPTR_FORMAT \" both have idx==%d\\n\",\n+          (uintptr_t)result, (uintptr_t)n, idx);\n+      }\n+      result = n;\n+    }\n+  };\n+  visit_nodes(start, callback, traverse_output, only_ctrl);\n+  return result;\n+}\n+\n+int node_idx_cmp(Node** n1, Node** n2) {\n+  return (*n1)->_idx - (*n2)->_idx;\n+}\n+\n+Node* find_node_by_name(Node* start, const char* name) {\n+  ResourceMark rm;\n+  Node* result = nullptr;\n+  GrowableArray<Node*> ns;\n+  auto callback = [&] (Node* n) {\n+    if (StringUtils::is_star_match(name, n->Name())) {\n+      ns.push(n);\n+      result = n;\n+    }\n+  };\n+  visit_nodes(start, callback, true, false);\n+  ns.sort(node_idx_cmp);\n+  for (int i = 0; i < ns.length(); i++) {\n+    ns.at(i)->dump();\n+  }\n+  return result;\n+}\n+\n+Node* find_node_by_dump(Node* start, const char* pattern) {\n+  ResourceMark rm;\n+  Node* result = nullptr;\n+  GrowableArray<Node*> ns;\n+  auto callback = [&] (Node* n) {\n+    stringStream stream;\n+    n->dump(\"\", false, &stream);\n+    if (StringUtils::is_star_match(pattern, stream.base())) {\n+      ns.push(n);\n+      result = n;\n+    }\n+  };\n+  visit_nodes(start, callback, true, false);\n+  ns.sort(node_idx_cmp);\n+  for (int i = 0; i < ns.length(); i++) {\n+    ns.at(i)->dump();\n+  }\n+  return result;\n+}\n+\n+\/\/ call from debugger: find node with name pattern in new\/current graph\n+\/\/ name can contain \"*\" in match pattern to match any characters\n+\/\/ the matching is case insensitive\n+Node* find_node_by_name(const char* name) {\n+  Node* root = Compile::current()->root();\n+  return find_node_by_name(root, name);\n+}\n+\n+\/\/ call from debugger: find node with name pattern in old graph\n+\/\/ name can contain \"*\" in match pattern to match any characters\n+\/\/ the matching is case insensitive\n+Node* find_old_node_by_name(const char* name) {\n+  Node* root = old_root();\n+  return find_node_by_name(root, name);\n+}\n+\n+\/\/ call from debugger: find node with dump pattern in new\/current graph\n+\/\/ can contain \"*\" in match pattern to match any characters\n+\/\/ the matching is case insensitive\n+Node* find_node_by_dump(const char* pattern) {\n+  Node* root = Compile::current()->root();\n+  return find_node_by_dump(root, pattern);\n+}\n+\n+\/\/ call from debugger: find node with name pattern in old graph\n+\/\/ can contain \"*\" in match pattern to match any characters\n+\/\/ the matching is case insensitive\n+Node* find_old_node_by_dump(const char* pattern) {\n+  Node* root = old_root();\n+  return find_node_by_dump(root, pattern);\n+}\n+\n+\/\/ Call this from debugger, search in same graph as n:\n@@ -1601,1 +1736,1 @@\n-\/\/ Call this from debugger with root node as default:\n+\/\/ Call this from debugger, search in new nodes:\n@@ -1606,1 +1741,7 @@\n-\/\/ Call this from debugger:\n+\/\/ Call this from debugger, search in old nodes:\n+Node* find_old_node(const int idx) {\n+  Node* root = old_root();\n+  return (root == nullptr) ? nullptr : root->find(idx);\n+}\n+\n+\/\/ Call this from debugger, search in same graph as n:\n@@ -1611,1 +1752,1 @@\n-\/\/ Call this from debugger with root node as default:\n+\/\/ Call this from debugger, search in new nodes:\n@@ -1616,0 +1757,6 @@\n+\/\/ Call this from debugger, search in old nodes:\n+Node* find_old_ctrl(const int idx) {\n+  Node* root = old_root();\n+  return (root == nullptr) ? nullptr : root->find_ctrl(idx);\n+}\n+\n@@ -1629,15 +1776,167 @@\n-  VectorSet old_space;\n-  VectorSet new_space;\n-  Node_List worklist;\n-  Arena* old_arena = Compile::current()->old_arena();\n-  add_to_worklist(this, &worklist, old_arena, &old_space, &new_space);\n-  Node* result = NULL;\n-  int node_idx = (idx >= 0) ? idx : -idx;\n-\n-  for (uint list_index = 0; list_index < worklist.size(); list_index++) {\n-    Node* n = worklist[list_index];\n-\n-    if ((int)n->_idx == node_idx debug_only(|| n->debug_idx() == node_idx)) {\n-      if (result != NULL) {\n-        tty->print(\"find: \" INTPTR_FORMAT \" and \" INTPTR_FORMAT \" both have idx==%d\\n\",\n-                  (uintptr_t)result, (uintptr_t)n, node_idx);\n+  return find_node_by_idx(this, abs(idx), (idx < 0), only_ctrl);\n+}\n+\n+class PrintBFS {\n+public:\n+  PrintBFS(Node* start, const int max_distance, Node* target, const char* options)\n+  : _start(start), _max_distance(max_distance), _target(target), _options(options),\n+    _dcc(this), _info_uid(cmpkey, hashkey) {}\n+\n+  void run();\n+private:\n+  \/\/ pipeline steps\n+  bool configure();\n+  void collect();\n+  void select();\n+  void select_all();\n+  void select_all_paths();\n+  void select_shortest_path();\n+  void sort();\n+  void print();\n+\n+  \/\/ inputs\n+  Node* _start;\n+  const int _max_distance;\n+  Node* _target;\n+  const char* _options;\n+\n+  \/\/ options\n+  bool _traverse_inputs = false;\n+  bool _traverse_outputs = false;\n+  struct Filter {\n+    bool _control = false;\n+    bool _memory = false;\n+    bool _data = false;\n+    bool _mixed = false;\n+    bool _other = false;\n+    bool is_empty() const {\n+      return !(_control || _memory || _data || _mixed || _other);\n+    }\n+    void set_all() {\n+      _control = true;\n+      _memory = true;\n+      _data = true;\n+      _mixed = true;\n+      _other = true;\n+    }\n+  };\n+  Filter _filter_visit;\n+  Filter _filter_boundary;\n+  bool _sort_idx = false;\n+  bool _all_paths = false;\n+  bool _use_color = false;\n+  bool _print_blocks = false;\n+  bool _print_old = false;\n+  static void print_options_help(bool print_examples);\n+  bool parse_options();\n+\n+  \/\/ node category (filter \/ color)\n+  static bool filter_category(Node* n, Filter& filter); \/\/ filter node category against options\n+public:\n+  class DumpConfigColored : public Node::DumpConfig {\n+  public:\n+    DumpConfigColored(PrintBFS* bfs) : _bfs(bfs) {};\n+    virtual void pre_dump(outputStream* st, const Node* n);\n+    virtual void post_dump(outputStream* st);\n+  private:\n+    PrintBFS* _bfs;\n+  };\n+private:\n+  DumpConfigColored _dcc;\n+\n+  \/\/ node info\n+  static Node* old_node(Node* n); \/\/ mach node -> prior IR node\n+  static void print_node_idx(Node* n); \/\/ to tty\n+  static void print_block_id(Block* b); \/\/ to tty\n+  static void print_node_block(Node* n); \/\/ to tty: _pre_order, head idx, _idom, _dom_depth\n+\n+  \/\/ traversal data structures\n+  Node_List _worklist; \/\/ BFS queue\n+  void maybe_traverse(Node* src, Node* dst);\n+\n+  \/\/ node info annotation\n+  class Info {\n+  public:\n+    Info() : Info(nullptr, 0) {};\n+    Info(Node* node, int distance)\n+      : _node(node), _distance_from_start(distance) {};\n+    Node* node() { return _node; };\n+    int distance() const { return _distance_from_start; };\n+    int distance_from_target() const { return _distance_from_target; }\n+    void set_distance_from_target(int d) { _distance_from_target = d; }\n+    Node_List edge_bwd; \/\/ pointing toward _start\n+    bool is_marked() const { return _mark; } \/\/ marked to keep during select\n+    void set_mark() { _mark = true; }\n+  private:\n+    Node* _node;\n+    int _distance_from_start; \/\/ distance from _start\n+    int _distance_from_target = 0; \/\/ distance from _target if _all_paths\n+    bool _mark = false;\n+  };\n+  Dict _info_uid;            \/\/ Node -> uid\n+  GrowableArray<Info> _info; \/\/ uid  -> info\n+\n+  Info* find_info(const Node* n) {\n+    size_t uid = (size_t)_info_uid[n];\n+    if (uid == 0) {\n+      return nullptr;\n+    }\n+    return &_info.at((int)uid);\n+  }\n+\n+  void make_info(Node* node, const int distance) {\n+    assert(find_info(node) == nullptr, \"node does not yet have info\");\n+    size_t uid = _info.length() + 1;\n+    _info_uid.Insert(node, (void*)uid);\n+    _info.at_put_grow((int)uid, Info(node, distance));\n+    assert(find_info(node)->node() == node, \"stored correct node\");\n+  };\n+\n+  \/\/ filled by sort, printed by print\n+  GrowableArray<Node*> _print_list;\n+\n+  \/\/ print header + node table\n+  void print_header() const;\n+  void print_node(Node* n);\n+};\n+\n+void PrintBFS::run() {\n+  if (!configure()) {\n+    return;\n+  }\n+  collect();\n+  select();\n+  sort();\n+  print();\n+}\n+\n+\/\/ set up configuration for BFS and print\n+bool PrintBFS::configure() {\n+  if (_max_distance < 0) {\n+    tty->print(\"dump_bfs: max_distance must be non-negative!\\n\");\n+    return false;\n+  }\n+  return parse_options();\n+}\n+\n+\/\/ BFS traverse according to configuration, fill worklist and info\n+void PrintBFS::collect() {\n+  maybe_traverse(_start, _start);\n+  uint pos = 0;\n+  while (pos < _worklist.size()) {\n+    Node* n = _worklist.at(pos++); \/\/ next node to traverse\n+    Info* info = find_info(n);\n+    if (!filter_category(n, _filter_visit) && n != _start) {\n+      continue; \/\/ we hit boundary, do not traverse further\n+    }\n+    if (n != _start && n->is_Root()) {\n+      continue; \/\/ traversing through root node would lead to unrelated nodes\n+    }\n+    if (_traverse_inputs && _max_distance > info->distance()) {\n+      for (uint i = 0; i < n->req(); i++) {\n+        maybe_traverse(n, n->in(i));\n+      }\n+    }\n+    if (_traverse_outputs && _max_distance > info->distance()) {\n+      for (uint i = 0; i < n->outcnt(); i++) {\n+        maybe_traverse(n, n->raw_out(i));\n@@ -1645,1 +1944,2 @@\n-      result = n;\n+  }\n+}\n@@ -1648,4 +1948,49 @@\n-    for (uint i = 0; i < n->len(); i++) {\n-      if (!only_ctrl || n->is_Region() || (n->Opcode() == Op_Root) || (i == TypeFunc::Control)) {\n-        \/\/ If only_ctrl is set: Add regions, the root node, or control inputs only\n-        add_to_worklist(n->in(i), &worklist, old_arena, &old_space, &new_space);\n+\/\/ go through work list, mark those that we want to print\n+void PrintBFS::select() {\n+  if (_target == nullptr ) {\n+    select_all();\n+  } else {\n+    if (find_info(_target) == nullptr) {\n+      tty->print(\"Could not find target in BFS.\\n\");\n+      return;\n+    }\n+    if (_all_paths) {\n+      select_all_paths();\n+    } else {\n+      select_shortest_path();\n+    }\n+  }\n+}\n+\n+\/\/ take all nodes from BFS\n+void PrintBFS::select_all() {\n+  for (uint i = 0; i < _worklist.size(); i++) {\n+    Node* n = _worklist.at(i);\n+    Info* info = find_info(n);\n+    info->set_mark();\n+  }\n+}\n+\n+\/\/ traverse backward from target, along edges found in BFS\n+void PrintBFS::select_all_paths() {\n+  uint pos = 0;\n+  Node_List backtrace;\n+  \/\/ start from target\n+  backtrace.push(_target);\n+  find_info(_target)->set_mark();\n+  \/\/ traverse backward\n+  while (pos < backtrace.size()) {\n+    Node* n = backtrace.at(pos++);\n+    Info* info = find_info(n);\n+    for (uint i = 0; i < info->edge_bwd.size(); i++) {\n+      \/\/ all backward edges\n+      Node* back = info->edge_bwd.at(i);\n+      Info* back_info = find_info(back);\n+      if (!back_info->is_marked()) {\n+        \/\/ not yet found this on way back.\n+        back_info->set_distance_from_target(info->distance_from_target() + 1);\n+        if (back_info->distance_from_target() + back_info->distance() <= _max_distance) {\n+          \/\/ total distance is small enough\n+          back_info->set_mark();\n+          backtrace.push(back);\n+        }\n@@ -1654,0 +1999,2 @@\n+  }\n+}\n@@ -1655,4 +2002,22 @@\n-    \/\/ Also search along forward edges if idx is negative and the search is not done on control nodes only\n-    if (idx < 0 && !only_ctrl) {\n-      for (uint i = 0; i < n->outcnt(); i++) {\n-        add_to_worklist(n->raw_out(i), &worklist, old_arena, &old_space, &new_space);\n+void PrintBFS::select_shortest_path() {\n+  Node* current = _target;\n+  while (true) {\n+    Info* info = find_info(current);\n+    info->set_mark();\n+    if (current == _start) {\n+      break;\n+    }\n+    \/\/ first edge -> leads us one step closer to _start\n+    current = info->edge_bwd.at(0);\n+  }\n+}\n+\n+\/\/ go through worklist in desired order, put the marked ones in print list\n+void PrintBFS::sort() {\n+  if (_traverse_inputs && !_traverse_outputs) {\n+    \/\/ reverse order\n+    for (int i = _worklist.size() - 1; i >= 0; i--) {\n+      Node* n = _worklist.at(i);\n+      Info* info = find_info(n);\n+      if (info->is_marked()) {\n+        _print_list.push(n);\n@@ -1661,5 +2026,8 @@\n-#ifdef ASSERT\n-    \/\/ Search along debug_orig edges last\n-    Node* orig = n->debug_orig();\n-    while (orig != NULL && add_to_worklist(orig, &worklist, old_arena, &old_space, &new_space)) {\n-      orig = orig->debug_orig();\n+  } else {\n+    \/\/ same order as worklist\n+    for (uint i = 0; i < _worklist.size(); i++) {\n+      Node* n = _worklist.at(i);\n+      Info* info = find_info(n);\n+      if (info->is_marked()) {\n+        _print_list.push(n);\n+      }\n@@ -1667,2 +2035,3 @@\n-#endif \/\/ ASSERT\n-  return result;\n+  if (_sort_idx) {\n+    _print_list.sort(node_idx_cmp);\n+  }\n@@ -1672,3 +2041,167 @@\n-bool Node::add_to_worklist(Node* n, Node_List* worklist, Arena* old_arena, VectorSet* old_space, VectorSet* new_space) {\n-  if (not_a_node(n)) {\n-    return false; \/\/ Gracefully handle NULL, -1, 0xabababab, etc.\n+\/\/ go through printlist and print\n+void PrintBFS::print() {\n+  if (_print_list.length() > 0 ) {\n+    print_header();\n+    for (int i = 0; i < _print_list.length(); i++) {\n+      Node* n = _print_list.at(i);\n+      print_node(n);\n+    }\n+  } else {\n+    tty->print(\"No nodes to print.\\n\");\n+  }\n+}\n+\n+void PrintBFS::print_options_help(bool print_examples) {\n+  tty->print(\"Usage: node->dump_bfs(int max_distance, Node* target, char* options)\\n\");\n+  tty->print(\"\\n\");\n+  tty->print(\"Use cases:\\n\");\n+  tty->print(\"  BFS traversal: no target required\\n\");\n+  tty->print(\"  shortest path: set target\\n\");\n+  tty->print(\"  all paths: set target and put 'A' in options\\n\");\n+  tty->print(\"  detect loop: subcase of all paths, have start==target\\n\");\n+  tty->print(\"\\n\");\n+  tty->print(\"Arguments:\\n\");\n+  tty->print(\"  this\/start: staring point of BFS\\n\");\n+  tty->print(\"  target:\\n\");\n+  tty->print(\"    if nullptr: simple BFS\\n\");\n+  tty->print(\"    else: shortest path or all paths between this\/start and target\\n\");\n+  tty->print(\"  options:\\n\");\n+  tty->print(\"    if nullptr: same as \\\"cdmxo@B\\\"\\n\");\n+  tty->print(\"    else: use combination of following characters\\n\");\n+  tty->print(\"      h: display this help info\\n\");\n+  tty->print(\"      H: display this help info, with examples\\n\");\n+  tty->print(\"      +: traverse in-edges (on if neither + nor -)\\n\");\n+  tty->print(\"      -: traverse out-edges\\n\");\n+  tty->print(\"      c: visit control nodes\\n\");\n+  tty->print(\"      m: visit memory nodes\\n\");\n+  tty->print(\"      d: visit data nodes\\n\");\n+  tty->print(\"      x: visit mixed nodes\\n\");\n+  tty->print(\"      o: visit other nodes\\n\");\n+  tty->print(\"      C: boundary control nodes\\n\");\n+  tty->print(\"      M: boundary memory nodes\\n\");\n+  tty->print(\"      D: boundary data nodes\\n\");\n+  tty->print(\"      X: boundary mixed nodes\\n\");\n+  tty->print(\"      O: boundary other nodes\\n\");\n+  tty->print(\"      S: sort displayed nodes by node idx\\n\");\n+  tty->print(\"      A: all paths (not just shortest path to target)\\n\");\n+  tty->print(\"      #: display node category in color (not supported in all terminals)\\n\");\n+  tty->print(\"      @: print old nodes - before matching (if available)\\n\");\n+  tty->print(\"      B: print scheduling blocks (if available)\\n\");\n+  tty->print(\"\\n\");\n+  tty->print(\"recursively follow edges to nodes with permitted visit types,\\n\");\n+  tty->print(\"on the boundary additionally display nodes allowed in boundary types\\n\");\n+  tty->print(\"\\n\");\n+  tty->print(\"output columns:\\n\");\n+  tty->print(\"  dist:  BFS distance to this\/start\\n\");\n+  tty->print(\"  apd:   all paths distance (d_start + d_target)\\n\");\n+  tty->print(\"  block: block identifier, based on _pre_order\\n\");\n+  tty->print(\"  head:  first node in block\\n\");\n+  tty->print(\"  idom:  head node of idom block\\n\");\n+  tty->print(\"  depth: depth of block (_dom_depth)\\n\");\n+  tty->print(\"  old:   old IR node - before matching\\n\");\n+  tty->print(\"  dump:  node->dump()\\n\");\n+  tty->print(\"\\n\");\n+  tty->print(\"Note: if none of the \\\"cmdxo\\\" characters are in the options string\\n\");\n+  tty->print(\"      then we set all of them.\\n\");\n+  tty->print(\"      This allows for short strings like \\\"#\\\" for colored input traversal\\n\");\n+  tty->print(\"      or \\\"-#\\\" for colored output traversal.\\n\");\n+  if (print_examples) {\n+    tty->print(\"\\n\");\n+    tty->print(\"Examples:\\n\");\n+    tty->print(\"  if->dump_bfs(10, 0, \\\"+cxo\\\")\\n\");\n+    tty->print(\"    starting at some if node, traverse inputs recursively\\n\");\n+    tty->print(\"    only along control (mixed and other can also be control)\\n\");\n+    tty->print(\"  phi->dump_bfs(5, 0, \\\"-dxo\\\")\\n\");\n+    tty->print(\"    starting at phi node, traverse outputs recursively\\n\");\n+    tty->print(\"    only along data (mixed and other can also have data flow)\\n\");\n+    tty->print(\"  find_node(385)->dump_bfs(3, 0, \\\"cdmox+#@B\\\")\\n\");\n+    tty->print(\"    find inputs of node 385, up to 3 nodes up (+)\\n\");\n+    tty->print(\"    traverse all nodes (cdmox), use colors (#)\\n\");\n+    tty->print(\"    display old nodes and blocks, if they exist\\n\");\n+    tty->print(\"    useful call to start with\\n\");\n+    tty->print(\"  find_node(102)->dump_bfs(10, 0, \\\"dCDMOX-\\\")\\n\");\n+    tty->print(\"    find non-data dependencies of a data node\\n\");\n+    tty->print(\"    follow data node outputs until we find another category\\n\");\n+    tty->print(\"    node as the boundary\\n\");\n+    tty->print(\"  x->dump_bfs(10, y, 0)\\n\");\n+    tty->print(\"    find shortest path from x to y, along any edge or node\\n\");\n+    tty->print(\"    will not find a path if it is longer than 10\\n\");\n+    tty->print(\"    useful to find how x and y are related\\n\");\n+    tty->print(\"  find_node(741)->dump_bfs(20, find_node(746), \\\"c+\\\")\\n\");\n+    tty->print(\"    find shortest control path between two nodes\\n\");\n+    tty->print(\"  find_node(741)->dump_bfs(8, find_node(746), \\\"cdmxo+A\\\")\\n\");\n+    tty->print(\"    find all paths (A) between two nodes of length at most 8\\n\");\n+    tty->print(\"  find_node(741)->dump_bfs(7, find_node(741), \\\"c+A\\\")\\n\");\n+    tty->print(\"    find all control loops for this node\\n\");\n+  }\n+}\n+\n+bool PrintBFS::parse_options() {\n+  if (_options == nullptr) {\n+    _options = \"cmdxo@B\"; \/\/ default options\n+  }\n+  size_t len = strlen(_options);\n+  for (size_t i = 0; i < len; i++) {\n+    switch (_options[i]) {\n+      case '+':\n+        _traverse_inputs = true;\n+        break;\n+      case '-':\n+        _traverse_outputs = true;\n+        break;\n+      case 'c':\n+        _filter_visit._control = true;\n+        break;\n+      case 'm':\n+        _filter_visit._memory = true;\n+        break;\n+      case 'd':\n+        _filter_visit._data = true;\n+        break;\n+      case 'x':\n+        _filter_visit._mixed = true;\n+        break;\n+      case 'o':\n+        _filter_visit._other = true;\n+        break;\n+      case 'C':\n+        _filter_boundary._control = true;\n+        break;\n+      case 'M':\n+        _filter_boundary._memory = true;\n+        break;\n+      case 'D':\n+        _filter_boundary._data = true;\n+        break;\n+      case 'X':\n+        _filter_boundary._mixed = true;\n+        break;\n+      case 'O':\n+        _filter_boundary._other = true;\n+        break;\n+      case 'S':\n+        _sort_idx = true;\n+        break;\n+      case 'A':\n+        _all_paths = true;\n+        break;\n+      case '#':\n+        _use_color = true;\n+        break;\n+      case 'B':\n+        _print_blocks = true;\n+        break;\n+      case '@':\n+        _print_old = true;\n+        break;\n+      case 'h':\n+        print_options_help(false);\n+        return false;\n+       case 'H':\n+        print_options_help(true);\n+        return false;\n+      default:\n+        tty->print_cr(\"dump_bfs: Unrecognized option \\'%c\\'\", _options[i]);\n+        tty->print_cr(\"for help, run: find_node(0)->dump_bfs(0,0,\\\"H\\\")\");\n+        return false;\n+    }\n@@ -1676,0 +2209,11 @@\n+  if (!_traverse_inputs && !_traverse_outputs) {\n+    _traverse_inputs = true;\n+  }\n+  if (_filter_visit.is_empty()) {\n+    _filter_visit.set_all();\n+  }\n+  Compile* C = Compile::current();\n+  _print_old &= (C->matcher() != nullptr); \/\/ only show old if there are new\n+  _print_blocks &= (C->cfg() != nullptr); \/\/ only show blocks if available\n+  return true;\n+}\n@@ -1677,5 +2221,19 @@\n-  \/\/ Contained in new_space or old_space? Check old_arena first since it's mostly empty.\n-  VectorSet* v = old_arena->contains(n) ? old_space : new_space;\n-  if (!v->test_set(n->_idx)) {\n-    worklist->push(n);\n-    return true;\n+bool PrintBFS::filter_category(Node* n, Filter& filter) {\n+  const Type* t = n->bottom_type();\n+  switch (t->category()) {\n+    case Type::Category::Data:\n+      return filter._data;\n+    case Type::Category::Memory:\n+      return filter._memory;\n+    case Type::Category::Mixed:\n+      return filter._mixed;\n+    case Type::Category::Control:\n+      return filter._control;\n+    case Type::Category::Other:\n+      return filter._other;\n+    case Type::Category::Undef:\n+      n->dump();\n+      assert(false, \"category undef ??\");\n+    default:\n+      n->dump();\n+      assert(false, \"not covered\");\n@@ -1686,0 +2244,213 @@\n+void PrintBFS::DumpConfigColored::pre_dump(outputStream* st, const Node* n) {\n+  if (!_bfs->_use_color) {\n+    return;\n+  }\n+  Info* info = _bfs->find_info(n);\n+  if (info == nullptr || !info->is_marked()) {\n+    return;\n+  }\n+\n+  const Type* t = n->bottom_type();\n+  switch (t->category()) {\n+    case Type::Category::Data:\n+      st->print(\"\\u001b[34m\");\n+      break;\n+    case Type::Category::Memory:\n+      st->print(\"\\u001b[32m\");\n+      break;\n+    case Type::Category::Mixed:\n+      st->print(\"\\u001b[35m\");\n+      break;\n+    case Type::Category::Control:\n+      st->print(\"\\u001b[31m\");\n+      break;\n+    case Type::Category::Other:\n+      st->print(\"\\u001b[33m\");\n+      break;\n+    case Type::Category::Undef:\n+      n->dump();\n+      assert(false, \"category undef ??\");\n+      break;\n+    default:\n+      n->dump();\n+      assert(false, \"not covered\");\n+      break;\n+  }\n+}\n+\n+void PrintBFS::DumpConfigColored::post_dump(outputStream* st) {\n+  if (!_bfs->_use_color) {\n+    return;\n+  }\n+  st->print(\"\\u001b[0m\"); \/\/ white\n+}\n+\n+Node* PrintBFS::old_node(Node* n) {\n+  Compile* C = Compile::current();\n+  if (C->matcher() == nullptr || !C->node_arena()->contains(n)) {\n+    return (Node*)nullptr;\n+  } else {\n+    return C->matcher()->find_old_node(n);\n+  }\n+}\n+\n+void PrintBFS::print_node_idx(Node* n) {\n+  Compile* C = Compile::current();\n+  char buf[30];\n+  if (n == nullptr) {\n+    sprintf(buf,\"_\");           \/\/ null\n+  } else if (C->node_arena()->contains(n)) {\n+    sprintf(buf, \"%d\", n->_idx);  \/\/ new node\n+  } else {\n+    sprintf(buf, \"o%d\", n->_idx); \/\/ old node\n+  }\n+  tty->print(\"%6s\", buf);\n+}\n+\n+void PrintBFS::print_block_id(Block* b) {\n+  Compile* C = Compile::current();\n+  char buf[30];\n+  sprintf(buf, \"B%d\", b->_pre_order);\n+  tty->print(\"%7s\", buf);\n+}\n+\n+void PrintBFS::print_node_block(Node* n) {\n+  Compile* C = Compile::current();\n+  Block* b = C->node_arena()->contains(n)\n+             ? C->cfg()->get_block_for_node(n)\n+             : nullptr; \/\/ guard against old nodes\n+  if (b == nullptr) {\n+    tty->print(\"      _\"); \/\/ Block\n+    tty->print(\"     _\");  \/\/ head\n+    tty->print(\"     _\");  \/\/ idom\n+    tty->print(\"      _\"); \/\/ depth\n+  } else {\n+    print_block_id(b);\n+    print_node_idx(b->head());\n+    if (b->_idom) {\n+      print_node_idx(b->_idom->head());\n+    } else {\n+      tty->print(\"     _\"); \/\/ idom\n+    }\n+    tty->print(\"%6d \", b->_dom_depth);\n+  }\n+}\n+\n+\/\/ filter, and add to worklist, add info, note traversal edges\n+void PrintBFS::maybe_traverse(Node* src, Node* dst) {\n+  if (dst != nullptr &&\n+     (filter_category(dst, _filter_visit) ||\n+      filter_category(dst, _filter_boundary) ||\n+      dst == _start)) { \/\/ correct category or start?\n+    if (find_info(dst) == nullptr) {\n+      \/\/ never visited - set up info\n+      _worklist.push(dst);\n+      int d = 0;\n+      if (dst != _start) {\n+        d = find_info(src)->distance() + 1;\n+      }\n+      make_info(dst, d);\n+    }\n+    if (src != dst) {\n+      \/\/ traversal edges useful during select\n+      find_info(dst)->edge_bwd.push(src);\n+    }\n+  }\n+}\n+\n+void PrintBFS::print_header() const {\n+  tty->print(\"dist\");                         \/\/ distance\n+  if (_all_paths) {\n+    tty->print(\" apd\");                       \/\/ all paths distance\n+  }\n+  if (_print_blocks) {\n+    tty->print(\" [block  head  idom depth]\"); \/\/ block\n+  }\n+  if (_print_old) {\n+    tty->print(\"   old\");                     \/\/ old node\n+  }\n+  tty->print(\" dump\\n\");                      \/\/ node dump\n+  tty->print(\"---------------------------------------------\\n\");\n+}\n+\n+void PrintBFS::print_node(Node* n) {\n+  tty->print(\"%4d\", find_info(n)->distance());\/\/ distance\n+  if (_all_paths) {\n+    Info* info = find_info(n);\n+    int apd = info->distance() + info->distance_from_target();\n+    tty->print(\"%4d\", apd);                   \/\/ all paths distance\n+  }\n+  if (_print_blocks) {\n+    print_node_block(n);                      \/\/ block\n+  }\n+  if (_print_old) {\n+    print_node_idx(old_node(n));              \/\/ old node\n+  }\n+  tty->print(\" \");\n+  n->dump(\"\\n\", false, tty, &_dcc);           \/\/ node dump\n+}\n+\n+\/\/------------------------------dump_bfs--------------------------------------\n+\/\/ Call this from debugger\n+\/\/ Useful for BFS traversal, shortest path, all path, loop detection, etc\n+\/\/ Designed to be more readable, and provide additional info\n+\/\/ To find all options, run:\n+\/\/   find_node(0)->dump_bfs(0,0,\"H\")\n+void Node::dump_bfs(const int max_distance, Node* target, const char* options) {\n+  PrintBFS bfs(this, max_distance, target, options);\n+  bfs.run();\n+}\n+\n+\/\/ Call this from debugger, with default arguments\n+void Node::dump_bfs(const int max_distance) {\n+  dump_bfs(max_distance, nullptr, nullptr);\n+}\n+\n+\/\/ log10 rounded down\n+uint log10(const uint i) {\n+  uint v = 10;\n+  uint e = 0;\n+  while(v <= i) {\n+    v *= 10;\n+    e++;\n+  }\n+  return e;\n+}\n+\n+\/\/ -----------------------------dump_idx---------------------------------------\n+void Node::dump_idx(bool align, outputStream* st, DumpConfig* dc) const {\n+  if (dc != nullptr) {\n+    dc->pre_dump(st, this);\n+  }\n+  Compile* C = Compile::current();\n+  bool is_new = C->node_arena()->contains(this);\n+  if (align) { \/\/ print prefix empty spaces$\n+    \/\/ +1 for leading digit, +1 for \"o\"\n+    uint max_width = log10(C->unique()) + 2;\n+    \/\/ +1 for leading digit, maybe +1 for \"o\"\n+    uint width = log10(_idx) + 1 + (is_new ? 0 : 1);\n+    while (max_width > width) {\n+      st->print(\" \");\n+      width++;\n+    }\n+  }\n+  if (!is_new) {\n+    st->print(\"o\");\n+  }\n+  st->print(\"%d\", _idx);\n+  if (dc != nullptr) {\n+    dc->post_dump(st);\n+  }\n+}\n+\n+\/\/ -----------------------------dump_name--------------------------------------\n+void Node::dump_name(outputStream* st, DumpConfig* dc) const {\n+  if (dc != nullptr) {\n+    dc->pre_dump(st, this);\n+  }\n+  st->print(\"%s\", Name());\n+  if (dc != nullptr) {\n+    dc->post_dump(st);\n+  }\n+}\n+\n@@ -1756,1 +2527,1 @@\n-void Node::dump(const char* suffix, bool mark, outputStream *st) const {\n+void Node::dump(const char* suffix, bool mark, outputStream* st, DumpConfig* dc) const {\n@@ -1765,1 +2536,5 @@\n-  st->print(\"%c%d%s%s  === \", is_new ? ' ' : 'o', _idx, mark ? \" >\" : \"  \", Name());\n+  \/\/ idx mark name ===\n+  dump_idx(true, st, dc);\n+  st->print(mark ? \" >\" : \"  \");\n+  dump_name(st, dc);\n+  st->print(\"  === \");\n@@ -1768,2 +2543,2 @@\n-  dump_req(st);\n-  dump_prec(st);\n+  dump_req(st, dc);\n+  dump_prec(st, dc);\n@@ -1771,1 +2546,1 @@\n-  dump_out(st);\n+  dump_out(st, dc);\n@@ -1797,1 +2572,1 @@\n-  if (t != NULL && (t->isa_instptr() || t->isa_klassptr())) {\n+  if (t != NULL && (t->isa_instptr() || t->isa_instklassptr())) {\n@@ -1799,3 +2574,3 @@\n-    const TypeKlassPtr *tkls = t->isa_klassptr();\n-    ciKlass*           klass = toop ? toop->klass() : (tkls ? tkls->klass() : NULL );\n-    if (klass && klass->is_loaded() && klass->is_interface()) {\n+    const TypeInstKlassPtr *tkls = t->isa_instklassptr();\n+    ciKlass*           klass = toop ? toop->instance_klass() : (tkls ? tkls->instance_klass() : NULL );\n+    if (klass && klass->is_loaded() && ((toop && toop->is_interface()) || (tkls && tkls->is_interface()))) {\n@@ -1838,1 +2613,1 @@\n-void Node::dump_req(outputStream *st) const {\n+void Node::dump_req(outputStream* st, DumpConfig* dc) const {\n@@ -1847,1 +2622,2 @@\n-      st->print(\"%c%d \", Compile::current()->node_arena()->contains(d) ? ' ' : 'o', d->_idx);\n+      d->dump_idx(false, st, dc);\n+      st->print(\" \");\n@@ -1854,1 +2630,1 @@\n-void Node::dump_prec(outputStream *st) const {\n+void Node::dump_prec(outputStream* st, DumpConfig* dc) const {\n@@ -1862,1 +2638,2 @@\n-      st->print(\"%c%d \", Compile::current()->node_arena()->contains(in(i)) ? ' ' : 'o', in(i)->_idx);\n+      p->dump_idx(false, st, dc);\n+      st->print(\" \");\n@@ -1868,1 +2645,1 @@\n-void Node::dump_out(outputStream *st) const {\n+void Node::dump_out(outputStream* st, DumpConfig* dc) const {\n@@ -1870,1 +2647,1 @@\n-  st->print(\" [[\");\n+  st->print(\" [[ \");\n@@ -1879,1 +2656,2 @@\n-      st->print(\"%c%d \", Compile::current()->node_arena()->contains(u) ? ' ' : 'o', u->_idx);\n+      u->dump_idx(false, st, dc);\n+      st->print(\" \");\n","filename":"src\/hotspot\/share\/opto\/node.cpp","additions":837,"deletions":59,"binary":false,"changes":896,"status":"modified"},{"patch":"@@ -58,1 +58,0 @@\n-class CallNativeNode;\n@@ -108,1 +107,0 @@\n-class MachCallNativeNode;\n@@ -155,0 +153,1 @@\n+class PopulateIndexNode;\n@@ -185,6 +184,3 @@\n-\n-\/\/ The type of all node counts and indexes.\n-\/\/ It must hold at least 16 bits, but must also be fast to load and store.\n-\/\/ This type, if less than 32 bits, could limit the number of possible nodes.\n-\/\/ (To make this type platform-specific, move to globalDefinitions_xxx.hpp.)\n-typedef unsigned int node_idx_t;\n+class ExpandVNode;\n+class CompressVNode;\n+class CompressMNode;\n@@ -661,1 +657,0 @@\n-          DEFINE_CLASS_ID(CallNative,       Call, 5)\n@@ -688,1 +683,0 @@\n-            DEFINE_CLASS_ID(MachCallNative,       MachCall, 2)\n@@ -725,0 +719,3 @@\n+        DEFINE_CLASS_ID(CompressV, Vector, 4)\n+        DEFINE_CLASS_ID(ExpandV, Vector, 5)\n+        DEFINE_CLASS_ID(CompressM, Vector, 6)\n@@ -728,0 +725,1 @@\n+\n@@ -741,1 +739,2 @@\n-          DEFINE_CLASS_ID(LoadVectorMasked, LoadVector, 1)\n+          DEFINE_CLASS_ID(LoadVectorGatherMasked, LoadVector, 1)\n+          DEFINE_CLASS_ID(LoadVectorMasked, LoadVector, 2)\n@@ -745,1 +744,2 @@\n-          DEFINE_CLASS_ID(StoreVectorMasked, StoreVector, 1)\n+          DEFINE_CLASS_ID(StoreVectorScatterMasked, StoreVector, 1)\n+          DEFINE_CLASS_ID(StoreVectorMasked, StoreVector, 2)\n@@ -797,6 +797,6 @@\n-    Flag_has_vector_mask_set         = 1 << 13,\n-    Flag_is_expensive                = 1 << 14,\n-    Flag_is_predicated_vector        = 1 << 15,\n-    Flag_for_post_loop_opts_igvn     = 1 << 16,\n-    Flag_is_removed_by_peephole      = 1 << 17,\n-    _last_flag                       = Flag_is_removed_by_peephole\n+    Flag_is_expensive                = 1 << 13,\n+    Flag_is_predicated_vector        = 1 << 14,\n+    Flag_for_post_loop_opts_igvn     = 1 << 15,\n+    Flag_is_removed_by_peephole      = 1 << 16,\n+    Flag_is_predicated_using_blend   = 1 << 17,\n+    _last_flag                       = Flag_is_predicated_using_blend\n@@ -867,1 +867,0 @@\n-  DEFINE_CLASS_QUERY(CallNative)\n@@ -914,1 +913,0 @@\n-  DEFINE_CLASS_QUERY(MachCallNative)\n@@ -965,1 +963,4 @@\n-  DEFINE_CLASS_QUERY(VectorReinterpret);\n+  DEFINE_CLASS_QUERY(VectorReinterpret)\n+  DEFINE_CLASS_QUERY(CompressV)\n+  DEFINE_CLASS_QUERY(ExpandV)\n+  DEFINE_CLASS_QUERY(CompressM)\n@@ -1023,2 +1024,1 @@\n-  \/\/ The node is a CountedLoopEnd with a mask annotation so as to emit a restore context\n-  bool has_vector_mask_set() const { return (_flags & Flag_has_vector_mask_set) != 0; }\n+  bool is_predicated_using_blend() const { return (_flags & Flag_is_predicated_using_blend) != 0; }\n@@ -1057,1 +1057,1 @@\n-  \/\/ treatise in node.cpp above the default implemention AND TEST WITH\n+  \/\/ treatise in node.cpp above the default implementation AND TEST WITH\n@@ -1221,0 +1221,10 @@\n+  void dump_bfs(const int max_distance, Node* target, const char* options); \/\/ Print BFS traversal\n+  void dump_bfs(const int max_distance); \/\/ dump_bfs(max_distance, nullptr, nullptr)\n+  class DumpConfig {\n+  public:\n+    \/\/ overridden to implement coloring of node idx\n+    virtual void pre_dump(outputStream *st, const Node* n) = 0;\n+    virtual void post_dump(outputStream *st) = 0;\n+  };\n+  void dump_idx(bool align = false, outputStream* st = tty, DumpConfig* dc = nullptr) const;\n+  void dump_name(outputStream* st = tty, DumpConfig* dc = nullptr) const;\n@@ -1222,1 +1232,1 @@\n-  void dump(const char* suffix, bool mark = false, outputStream *st = tty) const; \/\/ Print this node.\n+  void dump(const char* suffix, bool mark = false, outputStream* st = tty, DumpConfig* dc = nullptr) const; \/\/ Print this node.\n@@ -1228,3 +1238,3 @@\n-  virtual void dump_req(outputStream *st = tty) const;    \/\/ Print required-edge info\n-  virtual void dump_prec(outputStream *st = tty) const;   \/\/ Print precedence-edge info\n-  virtual void dump_out(outputStream *st = tty) const;    \/\/ Print the output edge info\n+  virtual void dump_req(outputStream* st = tty, DumpConfig* dc = nullptr) const;    \/\/ Print required-edge info\n+  virtual void dump_prec(outputStream* st = tty, DumpConfig* dc = nullptr) const;   \/\/ Print precedence-edge info\n+  virtual void dump_out(outputStream* st = tty, DumpConfig* dc = nullptr) const;    \/\/ Print the output edge info\n@@ -1678,0 +1688,30 @@\n+\/\/ Unique_Mixed_Node_List\n+\/\/ unique: nodes are added only once\n+\/\/ mixed: allow new and old nodes\n+class Unique_Mixed_Node_List : public ResourceObj {\n+public:\n+  Unique_Mixed_Node_List() : _visited_set(cmpkey, hashkey) {}\n+\n+  void add(Node* node) {\n+    if (not_a_node(node)) {\n+      return; \/\/ Gracefully handle NULL, -1, 0xabababab, etc.\n+    }\n+    if (_visited_set[node] == nullptr) {\n+      _visited_set.Insert(node, node);\n+      _worklist.push(node);\n+    }\n+  }\n+\n+  Node* operator[] (uint i) const {\n+    return _worklist[i];\n+  }\n+\n+  size_t size() {\n+    return _worklist.size();\n+  }\n+\n+private:\n+  Dict _visited_set;\n+  Node_List _worklist;\n+};\n+\n@@ -1879,0 +1919,8 @@\n+inline int Op_ConIL(BasicType bt) {\n+  assert(bt == T_INT || bt == T_LONG, \"only for int or longs\");\n+  if (bt == T_INT) {\n+    return Op_ConI;\n+  }\n+  return Op_ConL;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/node.hpp","additions":76,"deletions":28,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"opto\/c2_MacroAssembler.hpp\"\n@@ -288,0 +289,37 @@\n+\/\/ Nmethod entry barrier stubs\n+C2EntryBarrierStub* C2EntryBarrierStubTable::add_entry_barrier() {\n+  assert(_stub == NULL, \"There can only be one entry barrier stub\");\n+  _stub = new (Compile::current()->comp_arena()) C2EntryBarrierStub();\n+  return _stub;\n+}\n+\n+void C2EntryBarrierStubTable::emit(CodeBuffer& cb) {\n+  if (_stub == NULL) {\n+    \/\/ No stub - nothing to do\n+    return;\n+  }\n+\n+  C2_MacroAssembler masm(&cb);\n+  \/\/ Make sure there is enough space in the code buffer\n+  if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n+    ciEnv::current()->record_failure(\"CodeCache is full\");\n+    return;\n+  }\n+\n+  intptr_t before = masm.offset();\n+  masm.emit_entry_barrier_stub(_stub);\n+  intptr_t after = masm.offset();\n+  int actual_size = (int)(after - before);\n+  int expected_size = masm.entry_barrier_stub_size();\n+  assert(actual_size == expected_size, \"Estimated size is wrong, expected %d, was %d\", expected_size, actual_size);\n+}\n+\n+int C2EntryBarrierStubTable::estimate_stub_size() const {\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() == NULL) {\n+    \/\/ No nmethod entry barrier?\n+    return 0;\n+  }\n+\n+  return C2_MacroAssembler::entry_barrier_stub_size();\n+}\n+\n@@ -294,0 +332,2 @@\n+    _safepoint_poll_table(),\n+    _entry_barrier_table(),\n@@ -336,0 +376,2 @@\n+\n+  C->print_method(CompilerPhaseType::PHASE_MACHANALYSIS, 4);\n@@ -863,1 +905,1 @@\n-      ciKlass* cik = t->is_oopptr()->klass();\n+      ciKlass* cik = t->is_oopptr()->exact_klass();\n@@ -883,3 +925,2 @@\n-      ScopeValue* klass_sv = new ConstantOopWriteValue(cik->java_mirror()->constant_encoding());\n-      sv = spobj->is_auto_box() ? new AutoBoxObjectValue(spobj->_idx, klass_sv)\n-                                    : new ObjectValue(spobj->_idx, klass_sv, is_init);\n+      sv = new ObjectValue(spobj->_idx,\n+                           new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()), is_init);\n@@ -924,1 +965,1 @@\n-      \/\/ jsr\/ret return address which must be restored into a the full\n+      \/\/ jsr\/ret return address which must be restored into the full\n@@ -1060,1 +1101,0 @@\n-  bool is_opt_native = false;\n@@ -1080,2 +1120,0 @@\n-    } else if (mcall->is_MachCallNative()) {\n-      is_opt_native = true;\n@@ -1157,1 +1195,1 @@\n-          ciKlass* cik = t->is_oopptr()->klass();\n+          ciKlass* cik = t->is_oopptr()->exact_klass();\n@@ -1160,3 +1198,2 @@\n-          ScopeValue* klass_sv = new ConstantOopWriteValue(cik->java_mirror()->constant_encoding());\n-          ObjectValue* sv = spobj->is_auto_box() ? new AutoBoxObjectValue(spobj->_idx, klass_sv)\n-                                        : new ObjectValue(spobj->_idx, klass_sv);\n+          ObjectValue* sv = new ObjectValue(spobj->_idx,\n+                                            new ConstantOopWriteValue(cik->java_mirror()->constant_encoding()));\n@@ -1214,1 +1251,0 @@\n-      is_opt_native,\n@@ -1376,0 +1412,1 @@\n+  stub_req += entry_barrier_table()->estimate_stub_size();\n@@ -1887,0 +1924,4 @@\n+  \/\/ Fill in stubs for calling the runtime from nmethod entries.\n+  entry_barrier_table()->emit(*cb);\n+  if (C->failing())  return;\n+\n@@ -1933,0 +1974,13 @@\n+      \/\/ print_metadata and dump_asm may safepoint which makes us loose the ttylock.\n+      \/\/ We call them first and write to a stringStream, then we retake the lock to\n+      \/\/ make sure the end tag is coherent, and that xmlStream->pop_tag is done thread safe.\n+      ResourceMark rm;\n+      stringStream method_metadata_str;\n+      if (C->method() != NULL) {\n+        C->method()->print_metadata(&method_metadata_str);\n+      }\n+      stringStream dump_asm_str;\n+      dump_asm_on(&dump_asm_str, node_offsets, node_offset_limit);\n+\n+      NoSafepointVerifier nsv;\n+      ttyLocker ttyl2;\n@@ -1942,1 +1996,1 @@\n-        C->method()->print_metadata();\n+        tty->print_raw(method_metadata_str.as_string());\n@@ -1948,1 +2002,1 @@\n-      dump_asm(node_offsets, node_offset_limit);\n+      tty->print_raw(dump_asm_str.as_string());\n@@ -1951,4 +2005,0 @@\n-        \/\/ print_metadata and dump_asm above may safepoint which makes us loose the ttylock.\n-        \/\/ Retake lock too make sure the end tag is coherent, and that xmlStream->pop_tag is done\n-        \/\/ thread safe\n-        ttyLocker ttyl2;\n@@ -2188,10 +2238,1 @@\n-    for (uint i = 0; i < C->cfg()->number_of_blocks(); i++) {\n-      tty->print(\"\\nBB#%03d:\\n\", i);\n-      Block* block = C->cfg()->get_block(i);\n-      for (uint j = 0; j < block->number_of_nodes(); j++) {\n-        Node* n = block->get_node(j);\n-        OptoReg::Name reg = C->regalloc()->get_reg_first(n);\n-        tty->print(\" %-6s \", reg >= 0 && reg < REG_COUNT ? Matcher::regName[reg] : \"\");\n-        n->dump();\n-      }\n-    }\n+    print_scheduling();\n@@ -2202,0 +2243,16 @@\n+#ifndef PRODUCT\n+\/\/ Separated out so that it can be called directly from debugger\n+void PhaseOutput::print_scheduling() {\n+  for (uint i = 0; i < C->cfg()->number_of_blocks(); i++) {\n+    tty->print(\"\\nBB#%03d:\\n\", i);\n+    Block* block = C->cfg()->get_block(i);\n+    for (uint j = 0; j < block->number_of_nodes(); j++) {\n+      Node* n = block->get_node(j);\n+      OptoReg::Name reg = C->regalloc()->get_reg_first(n);\n+      tty->print(\" %-6s \", reg >= 0 && reg < REG_COUNT ? Matcher::regName[reg] : \"\");\n+      n->dump();\n+    }\n+  }\n+}\n+#endif\n+\n@@ -3466,1 +3523,1 @@\n-                              &_inc_table,\n+                              inc_table(),\n@@ -3470,2 +3527,3 @@\n-                              C->rtm_state(),\n-                              C->native_invokers());\n+                              C->has_monitors(),\n+                              0,\n+                              C->rtm_state());\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":90,"deletions":32,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -613,0 +613,37 @@\n+\/\/ Specialized uncommon_trap of unstable_if. C2 uses next_bci of path to update the live locals of it.\n+class UnstableIfTrap {\n+  CallStaticJavaNode* const _unc;\n+  bool _modified;            \/\/ modified locals based on next_bci()\n+  int _next_bci;\n+\n+public:\n+  UnstableIfTrap(CallStaticJavaNode* call, Parse::Block* path): _unc(call), _modified(false) {\n+    assert(_unc != NULL && Deoptimization::trap_request_reason(_unc->uncommon_trap_request()) == Deoptimization::Reason_unstable_if,\n+          \"invalid uncommon_trap call!\");\n+    _next_bci = path != nullptr ? path->start() : -1;\n+  }\n+\n+  \/\/ The starting point of the pruned block, where control goes when\n+  \/\/ deoptimization does happen.\n+  int next_bci() const {\n+    return _next_bci;\n+  }\n+\n+  bool modified() const {\n+    return _modified;\n+  }\n+\n+  void set_modified() {\n+    _modified = true;\n+  }\n+\n+  CallStaticJavaNode* uncommon_trap() const {\n+    return _unc;\n+  }\n+\n+  inline void* operator new(size_t x) throw() {\n+    Compile* C = Compile::current();\n+    return C->comp_arena()->AmallocWords(x);\n+  }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/parse.hpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -168,1 +168,1 @@\n-      (tp != NULL && !tp->klass()->is_loaded())) {\n+      (tp != NULL && !tp->is_loaded())) {\n@@ -184,1 +184,1 @@\n-  if (tp != NULL && tp->klass() != C->env()->Object_klass()) {\n+  if (tp != NULL && !tp->is_same_java_type_as(TypeInstPtr::BOTTOM)) {\n@@ -193,1 +193,1 @@\n-    l = gen_checkcast(l, makecon(TypeKlassPtr::make(tp->klass())), &bad_type_ctrl);\n+    l = gen_checkcast(l, makecon(tp->as_klass_type()->cast_to_exactness(true)), &bad_type_ctrl);\n@@ -361,1 +361,1 @@\n-      \/\/ isn't precise enought to figure out that they are dead in all\n+      \/\/ isn't precise enough to figure out that they are dead in all\n@@ -433,0 +433,4 @@\n+  if (parse_method->is_synchronized()) {\n+    C->set_has_monitors(true);\n+  }\n+\n@@ -824,1 +828,1 @@\n-    if (ret_oop_type && !ret_oop_type->klass()->is_loaded()) {\n+    if (ret_oop_type && !ret_oop_type->is_loaded()) {\n@@ -1528,1 +1532,1 @@\n-  tty->print_cr(\"transforms: Average amount of tranform progress per bytecode compiled\");\n+  tty->print_cr(\"transforms: Average amount of transform progress per bytecode compiled\");\n@@ -2234,1 +2238,1 @@\n-  if (tinst != NULL && tinst->klass()->is_loaded() && !tinst->klass_is_exact()) {\n+  if (tinst != NULL && tinst->is_loaded() && !tinst->klass_is_exact()) {\n@@ -2236,1 +2240,1 @@\n-    ciInstanceKlass* ik = tinst->klass()->as_instance_klass();\n+    ciInstanceKlass* ik = tinst->instance_klass();\n@@ -2391,1 +2395,1 @@\n-    const TypeOopPtr* tr = return_type->isa_oopptr();\n+    const TypeInstPtr* tr = return_type->isa_instptr();\n@@ -2414,1 +2418,1 @@\n-    } else if (tr && tr->isa_instptr() && tr->klass()->is_loaded() && tr->klass()->is_interface()) {\n+    } else if (tr && tr->isa_instptr() && tr->is_loaded() && tr->is_interface()) {\n@@ -2418,1 +2422,1 @@\n-      if (tp && tp->klass()->is_loaded() && !tp->klass()->is_interface()) {\n+      if (tp && tp->is_loaded() && !tp->is_interface()) {\n@@ -2431,2 +2435,2 @@\n-      if (phi_tip != NULL && phi_tip->is_loaded() && phi_tip->klass()->is_interface() &&\n-          val_tip != NULL && val_tip->is_loaded() && !val_tip->klass()->is_interface()) {\n+      if (phi_tip != NULL && phi_tip->is_loaded() && phi_tip->is_interface() &&\n+          val_tip != NULL && val_tip->is_loaded() && !val_tip->is_interface()) {\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":18,"deletions":14,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -355,1 +355,1 @@\n-      if (toop->klass()->as_instance_klass()->unique_concrete_subklass()) {\n+      if (toop->instance_klass()->unique_concrete_subklass()) {\n@@ -357,1 +357,1 @@\n-        const Type* subklass = Type::get_const_type(toop->klass());\n+        const Type* subklass = Type::get_const_type(toop->instance_klass());\n@@ -374,2 +374,1 @@\n-  ciKlass * arytype_klass = arytype->klass();\n-  if ((arytype_klass != NULL) && (!arytype_klass->is_loaded())) {\n+  if (!arytype->is_loaded()) {\n@@ -378,0 +377,2 @@\n+    ciKlass* klass = arytype->unloaded_klass();\n+\n@@ -380,1 +381,1 @@\n-                  arytype->klass(), \"!loaded array\");\n+                  klass, \"!loaded array\");\n@@ -425,1 +426,1 @@\n-        builtin_throw(Deoptimization::Reason_range_check, idx);\n+        builtin_throw(Deoptimization::Reason_range_check);\n@@ -1679,1 +1680,1 @@\n-\/\/ classifed as untaken (by seems_never_taken), so really,\n+\/\/ classified as untaken (by seems_never_taken), so really,\n@@ -2332,1 +2333,1 @@\n-    uncommon_trap(Deoptimization::Reason_unstable_if,\n+    Node* call = uncommon_trap(Deoptimization::Reason_unstable_if,\n@@ -2336,0 +2337,4 @@\n+\n+    if (call != nullptr) {\n+      C->record_unstable_if_trap(new UnstableIfTrap(call->as_CallStaticJava(), path));\n+    }\n","filename":"src\/hotspot\/share\/opto\/parse2.cpp","additions":13,"deletions":8,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -79,1 +79,1 @@\n-  if (!will_link || (tp && tp->klass() && !tp->klass()->is_loaded())) {\n+  if (!will_link || (tp && !tp->is_loaded())) {\n@@ -86,1 +86,1 @@\n-      if (tp && tp->klass() && !tp->klass()->is_loaded()) {\n+      if (tp && !tp->is_loaded()) {\n@@ -88,0 +88,1 @@\n+        ciKlass* klass = tp->unloaded_klass();\n@@ -89,1 +90,1 @@\n-                       C->log()->identify(tp->klass()));\n+                       C->log()->identify(klass));\n@@ -216,1 +217,1 @@\n-      extak = tak->cast_to_exactness(true)->is_klassptr();\n+      extak = tak->cast_to_exactness(true);\n@@ -219,1 +220,1 @@\n-    if (extak != NULL) {\n+    if (extak != NULL && extak->exact_klass(true) != NULL) {\n@@ -229,1 +230,1 @@\n-                        tak->klass());\n+                        extak->exact_klass());\n@@ -246,1 +247,1 @@\n-                    log->identify(tak->klass()));\n+                    log->identify(extak->exact_klass()));\n@@ -403,1 +404,0 @@\n-\n","filename":"src\/hotspot\/share\/opto\/parseHelper.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -1523,1 +1523,1 @@\n-\/\/ compares the the induction variable with n\n+\/\/ compares the induction variable with n\n@@ -1918,0 +1918,18 @@\n+        push_and(worklist, n, m);\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ AndI\/L::Value() optimizes patterns similar to (v << 2) & 3 to zero if they are bitwise disjoint.\n+\/\/ Add the AndI\/L nodes back to the worklist to re-apply Value() in case the shift value changed.\n+void PhaseCCP::push_and(Unique_Node_List& worklist, const Node* parent, const Node* use) const {\n+  uint use_op = use->Opcode();\n+  if ((use_op == Op_LShiftI || use_op == Op_LShiftL)\n+      && use->in(2) == parent) { \/\/ is shift value (right-hand side of LShift)\n+    for (DUIterator_Fast imax, i = use->fast_outs(imax); i < imax; i++) {\n+      Node* and_node = use->fast_out(i);\n+      uint and_node_op = and_node->Opcode();\n+      if ((and_node_op == Op_AndI || and_node_op == Op_AndL)\n+          && and_node->bottom_type() != type(and_node)) {\n+        worklist.push(and_node);\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -576,0 +576,2 @@\n+  void push_and(Unique_Node_List& worklist, const Node* parent, const Node* use) const;\n+\n","filename":"src\/hotspot\/share\/opto\/phaseX.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+  flags(MACHANALYSIS,                 \"After mach analysis\") \\\n@@ -101,2 +102,2 @@\n-  static int to_bitmask(CompilerPhaseType cpt) {\n-    return (1 << cpt);\n+  static uint64_t to_bitmask(CompilerPhaseType cpt) {\n+    return (UINT64_C(1) << cpt);\n","filename":"src\/hotspot\/share\/opto\/phasetype.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -94,1 +94,1 @@\n-\/\/ Perfom node replacement (used when returning to caller)\n+\/\/ Perform node replacement (used when returning to caller)\n","filename":"src\/hotspot\/share\/opto\/replacednodes.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -122,1 +122,4 @@\n-  RegisterMap map(thread, false);\n+  RegisterMap map(thread,\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::include,\n+                  RegisterMap::WalkContinuation::skip);\n@@ -725,0 +728,54 @@\n+const TypeFunc* OptoRuntime::void_void_Type() {\n+   \/\/ create input type (domain)\n+   const Type **fields = TypeTuple::fields(0);\n+   const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+0, fields);\n+\n+   \/\/ create result type (range)\n+   fields = TypeTuple::fields(0);\n+   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+0, fields);\n+   return TypeFunc::make(domain, range);\n+ }\n+\n+ const TypeFunc* OptoRuntime::continuation_doYield_Type() {\n+   \/\/ create input type (domain)\n+   const Type **fields = TypeTuple::fields(0);\n+   const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+0, fields);\n+\n+   \/\/ create result type (range)\n+   fields = TypeTuple::fields(1);\n+   fields[TypeFunc::Parms+0] = TypeInt::INT;\n+   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+   return TypeFunc::make(domain, range);\n+ }\n+\n+ const TypeFunc* OptoRuntime::continuation_jump_Type() {\n+  \/\/ create input type (domain)\n+  const Type **fields = TypeTuple::fields(6);\n+  fields[TypeFunc::Parms+0] = TypeLong::LONG;\n+  fields[TypeFunc::Parms+1] = Type::HALF;\n+  fields[TypeFunc::Parms+2] = TypeLong::LONG;\n+  fields[TypeFunc::Parms+3] = Type::HALF;\n+  fields[TypeFunc::Parms+4] = TypeLong::LONG;\n+  fields[TypeFunc::Parms+5] = Type::HALF;\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+6, fields);\n+\n+  \/\/ create result type (range)\n+  fields = TypeTuple::fields(0);\n+  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+0, fields);\n+  return TypeFunc::make(domain, range);\n+ }\n+\n+\n+ const TypeFunc* OptoRuntime::jfr_write_checkpoint_Type() {\n+   \/\/ create input type (domain)\n+   const Type **fields = TypeTuple::fields(0);\n+   const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+   \/\/ create result type (range)\n+   fields = TypeTuple::fields(0);\n+   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms, fields);\n+   return TypeFunc::make(domain, range);\n+ }\n+\n+\n@@ -1355,1 +1412,4 @@\n-      RegisterMap map(current, false);\n+      RegisterMap map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1436,1 +1496,4 @@\n-    RegisterMap map(current, false \/* update_map *\/, false \/* process_frames *\/);\n+    RegisterMap map(current,\n+                    RegisterMap::UpdateMap::skip,\n+                    RegisterMap::ProcessFrames::skip,\n+                    RegisterMap::WalkContinuation::skip);\n@@ -1523,1 +1586,4 @@\n-  RegisterMap reg_map(thread);\n+  RegisterMap reg_map(thread,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1535,1 +1601,4 @@\n-  RegisterMap reg_map(thread);\n+  RegisterMap reg_map(thread,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1560,1 +1629,1 @@\n-const TypeFunc *OptoRuntime::get_class_id_intrinsic_Type() {\n+const TypeFunc *OptoRuntime::class_id_load_barrier_Type() {\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":75,"deletions":6,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -247,0 +247,5 @@\n+  static const TypeFunc* void_void_Type();\n+  static const TypeFunc* continuation_doYield_Type();\n+  static const TypeFunc* continuation_jump_Type();\n+\n+  static const TypeFunc* jfr_write_checkpoint_Type();\n@@ -293,1 +298,1 @@\n-  JFR_ONLY(static const TypeFunc* get_class_id_intrinsic_Type();)\n+  JFR_ONLY(static const TypeFunc* class_id_load_barrier_Type();)\n","filename":"src\/hotspot\/share\/opto\/runtime.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2009, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2009, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,3 +29,0 @@\n-#include \"opto\/addnode.hpp\"\n-#include \"opto\/callGenerator.hpp\"\n-#include \"opto\/divnode.hpp\"\n@@ -38,2 +35,1 @@\n-#include \"opto\/subnode.hpp\"\n-#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/atomic.hpp\"\n@@ -47,1 +43,0 @@\n-  Node*               _string_alloc;\n@@ -74,1 +69,0 @@\n-    _string_alloc(NULL),\n@@ -85,23 +79,0 @@\n-  void merge_add() {\n-#if 0\n-    \/\/ XXX This is place holder code for reusing an existing String\n-    \/\/ allocation but the logic for checking the state safety is\n-    \/\/ probably inadequate at the moment.\n-    CallProjections endprojs;\n-    sc->end()->extract_projections(&endprojs, false);\n-    if (endprojs.resproj != NULL) {\n-      for (SimpleDUIterator i(endprojs.resproj); i.has_next(); i.next()) {\n-        CallStaticJavaNode *use = i.get()->isa_CallStaticJava();\n-        if (use != NULL && use->method() != NULL &&\n-            use->method()->intrinsic_id() == vmIntrinsics::_String_String &&\n-            use->in(TypeFunc::Parms + 1) == endprojs.resproj) {\n-          \/\/ Found useless new String(sb.toString()) so reuse the newly allocated String\n-          \/\/ when creating the result instead of allocating a new one.\n-          sc->set_string_alloc(use->in(TypeFunc::Parms));\n-          sc->set_end(use);\n-        }\n-      }\n-    }\n-#endif\n-  }\n-\n@@ -212,1 +183,0 @@\n-  Node* string_alloc() { return _string_alloc; }\n@@ -221,4 +191,1 @@\n-      log->head(\"replace_string_concat arguments='%d' string_alloc='%d' multiple='%d'\",\n-                num_arguments(),\n-                _string_alloc != NULL,\n-                _multiple);\n+      log->head(\"replace_string_concat arguments='%d' multiple='%d'\", num_arguments(), _multiple);\n@@ -417,0 +384,1 @@\n+  uint encountered = 0;\n@@ -422,0 +390,1 @@\n+      encountered++;\n@@ -434,0 +403,3 @@\n+#ifndef PRODUCT\n+  Atomic::add(&_stropts_total, encountered);\n+#endif\n@@ -437,1 +409,22 @@\n-\n+\/\/ Recognize a fluent-chain of StringBuilder\/Buffer. They are either explicit usages\n+\/\/ of them or the legacy bytecodes of string concatenation prior to JEP-280. eg.\n+\/\/\n+\/\/ String result = new StringBuilder()\n+\/\/   .append(\"foo\")\n+\/\/   .append(\"bar\")\n+\/\/   .append(123)\n+\/\/   .toString(); \/\/ \"foobar123\"\n+\/\/\n+\/\/ PS: Only a certain subset of constructor and append methods are acceptable.\n+\/\/ The criterion is that the length of argument is easy to work out in this phrase.\n+\/\/ It will drop complex cases such as Object.\n+\/\/\n+\/\/ Since it walks along the receivers of fluent-chain, it will give up if the codeshape is\n+\/\/ not \"fluent\" enough. eg.\n+\/\/   StringBuilder sb = new StringBuilder();\n+\/\/   sb.append(\"foo\");\n+\/\/   sb.toString();\n+\/\/\n+\/\/ The receiver of toString method is the result of Allocation Node(CheckCastPP).\n+\/\/ The append method is overlooked. It will fail at validate_control_flow() test.\n+\/\/\n@@ -462,2 +455,0 @@\n-\n-  InitializeNode* init = NULL;\n@@ -641,1 +632,1 @@\n-PhaseStringOpts::PhaseStringOpts(PhaseGVN* gvn, Unique_Node_List*):\n+PhaseStringOpts::PhaseStringOpts(PhaseGVN* gvn):\n@@ -694,0 +685,1 @@\n+              Atomic::inc(&_stropts_merged);\n@@ -759,1 +751,1 @@\n-        \/\/ Recurisvely clean up references to CreateEx so EA doesn't\n+        \/\/ Recursively clean up references to CreateEx so EA doesn't\n@@ -1033,0 +1025,15 @@\n+      \/\/ Check for side effect between Initialize and the constructor\n+      for (SimpleDUIterator iter(ptr); iter.has_next(); iter.next()) {\n+        Node* use = iter.get();\n+        if (!use->is_CFG() && !use->is_CheckCastPP() && !use->is_Load()) {\n+#ifndef PRODUCT\n+          if (PrintOptimizeStringConcat) {\n+            tty->print_cr(\"unexpected control use of Initialize\");\n+            ptr->in(0)->dump(); \/\/ Initialize node\n+            use->dump(1);\n+          }\n+#endif\n+          fail = true;\n+          break;\n+        }\n+      }\n@@ -1765,1 +1772,1 @@\n-  \/\/ new starting state.  This allows any preceeding tests to feed\n+  \/\/ new starting state.  This allows any preceding tests to feed\n@@ -2034,3 +2041,1 @@\n-    \/\/ If we're not reusing an existing String allocation then allocate one here.\n-    result = sc->string_alloc();\n-    if (result == NULL) {\n+    {\n@@ -2063,0 +2068,12 @@\n+#ifndef PRODUCT\n+  Atomic::inc(&_stropts_replaced);\n+#endif\n+}\n+\n+#ifndef PRODUCT\n+uint PhaseStringOpts::_stropts_replaced = 0;\n+uint PhaseStringOpts::_stropts_merged = 0;\n+uint PhaseStringOpts::_stropts_total = 0;\n+\n+void PhaseStringOpts::print_statistics() {\n+  tty->print_cr(\"StringConcat: %4d\/%4d\/%4d(replaced\/merged\/total)\", _stropts_replaced, _stropts_merged, _stropts_total);\n@@ -2064,0 +2081,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/opto\/stringopts.cpp","additions":63,"deletions":45,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -831,5 +831,4 @@\n-        const Type* cmp1 = sub(tr1, t2);\n-        const Type* cmp2 = sub(tr2, t2);\n-        if (cmp1 == cmp2) {\n-          return cmp1; \/\/ Hit!\n-        }\n+        const TypeInt* cmp1 = sub(tr1, t2)->is_int();\n+        const TypeInt* cmp2 = sub(tr2, t2)->is_int();\n+        \/\/ compute union, so that cmp handles all possible results from the two cases\n+        return cmp1->meet(cmp2);\n@@ -853,0 +852,2 @@\n+    case Op_CmpU3:              \/\/ Collapse a CmpU3\/CmpI into a CmpU\n+      return new CmpUNode(in(1)->in(1),in(1)->in(2));\n@@ -855,0 +856,2 @@\n+    case Op_CmpUL3:             \/\/ Collapse a CmpUL3\/CmpI into a CmpUL\n+      return new CmpULNode(in(1)->in(1),in(1)->in(2));\n@@ -1006,11 +1009,13 @@\n-  const TypeOopPtr* oop_p0 = r0->isa_oopptr();\n-  const TypeOopPtr* oop_p1 = r1->isa_oopptr();\n-  bool both_oop_ptr = oop_p0 && oop_p1;\n-\n-  if (both_oop_ptr) {\n-    Node* in1 = in(1)->uncast();\n-    Node* in2 = in(2)->uncast();\n-    AllocateNode* alloc1 = AllocateNode::Ideal_allocation(in1, NULL);\n-    AllocateNode* alloc2 = AllocateNode::Ideal_allocation(in2, NULL);\n-    if (MemNode::detect_ptr_independence(in1, alloc1, in2, alloc2, NULL)) {\n-      return TypeInt::CC_GT;  \/\/ different pointers\n+  const TypeOopPtr* p0 = r0->isa_oopptr();\n+  const TypeOopPtr* p1 = r1->isa_oopptr();\n+  const TypeKlassPtr* k0 = r0->isa_klassptr();\n+  const TypeKlassPtr* k1 = r1->isa_klassptr();\n+  if ((p0 && p1) || (k0 && k1)) {\n+    if (p0 && p1) {\n+      Node* in1 = in(1)->uncast();\n+      Node* in2 = in(2)->uncast();\n+      AllocateNode* alloc1 = AllocateNode::Ideal_allocation(in1, NULL);\n+      AllocateNode* alloc2 = AllocateNode::Ideal_allocation(in2, NULL);\n+      if (MemNode::detect_ptr_independence(in1, alloc1, in2, alloc2, NULL)) {\n+        return TypeInt::CC_GT;  \/\/ different pointers\n+      }\n@@ -1018,18 +1023,15 @@\n-  }\n-\n-  const TypeKlassPtr* klass_p0 = r0->isa_klassptr();\n-  const TypeKlassPtr* klass_p1 = r1->isa_klassptr();\n-\n-  if (both_oop_ptr || (klass_p0 && klass_p1)) { \/\/ both or neither are klass pointers\n-    ciKlass* klass0 = NULL;\n-    bool    xklass0 = false;\n-    ciKlass* klass1 = NULL;\n-    bool    xklass1 = false;\n-\n-    if (oop_p0) {\n-      klass0 = oop_p0->klass();\n-      xklass0 = oop_p0->klass_is_exact();\n-    } else {\n-      assert(klass_p0, \"must be non-null if oop_p0 is null\");\n-      klass0 = klass_p0->klass();\n-      xklass0 = klass_p0->klass_is_exact();\n+    bool    xklass0 = p0 ? p0->klass_is_exact() : k0->klass_is_exact();\n+    bool    xklass1 = p1 ? p1->klass_is_exact() : k1->klass_is_exact();\n+    bool unrelated_classes = false;\n+\n+    if ((p0 && p0->is_same_java_type_as(p1)) ||\n+        (k0 && k0->is_same_java_type_as(k1))) {\n+    } else if ((p0 && !p1->maybe_java_subtype_of(p0) && !p0->maybe_java_subtype_of(p1)) ||\n+               (k0 && !k1->maybe_java_subtype_of(k0) && !k0->maybe_java_subtype_of(k1))) {\n+      unrelated_classes = true;\n+    } else if ((p0 && !p1->maybe_java_subtype_of(p0)) ||\n+               (k0 && !k1->maybe_java_subtype_of(k0))) {\n+      unrelated_classes = xklass1;\n+    } else if ((p0 && !p0->maybe_java_subtype_of(p1)) ||\n+               (k0 && !k0->maybe_java_subtype_of(k1))) {\n+      unrelated_classes = xklass0;\n@@ -1037,30 +1039,13 @@\n-\n-    if (oop_p1) {\n-      klass1 = oop_p1->klass();\n-      xklass1 = oop_p1->klass_is_exact();\n-    } else {\n-      assert(klass_p1, \"must be non-null if oop_p1 is null\");\n-      klass1 = klass_p1->klass();\n-      xklass1 = klass_p1->klass_is_exact();\n-    }\n-\n-    if (klass0 && klass1 &&\n-        klass0->is_loaded() && !klass0->is_interface() && \/\/ do not trust interfaces\n-        klass1->is_loaded() && !klass1->is_interface() &&\n-        (!klass0->is_obj_array_klass() ||\n-         !klass0->as_obj_array_klass()->base_element_klass()->is_interface()) &&\n-        (!klass1->is_obj_array_klass() ||\n-         !klass1->as_obj_array_klass()->base_element_klass()->is_interface())) {\n-      bool unrelated_classes = false;\n-      \/\/ See if neither subclasses the other, or if the class on top\n-      \/\/ is precise.  In either of these cases, the compare is known\n-      \/\/ to fail if at least one of the pointers is provably not null.\n-      if (klass0->equals(klass1)) {  \/\/ if types are unequal but klasses are equal\n-        \/\/ Do nothing; we know nothing for imprecise types\n-      } else if (klass0->is_subtype_of(klass1)) {\n-        \/\/ If klass1's type is PRECISE, then classes are unrelated.\n-        unrelated_classes = xklass1;\n-      } else if (klass1->is_subtype_of(klass0)) {\n-        \/\/ If klass0's type is PRECISE, then classes are unrelated.\n-        unrelated_classes = xklass0;\n-      } else {                  \/\/ Neither subtypes the other\n+    if (!unrelated_classes) {\n+      \/\/ Handle inline type arrays\n+      if ((r0->flatten_array() && r1->not_flatten_array()) ||\n+          (r1->flatten_array() && r0->not_flatten_array())) {\n+        \/\/ One type is flattened in arrays but the other type is not. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_flat() && r1->is_flat()) ||\n+                 (r1->is_not_flat() && r0->is_flat())) {\n+        \/\/ One type is a non-flattened array and the other type is a flattened array. Must be unrelated.\n+        unrelated_classes = true;\n+      } else if ((r0->is_not_null_free() && r1->is_null_free()) ||\n+                 (r1->is_not_null_free() && r0->is_null_free())) {\n+        \/\/ One type is a nullable array and the other type is a null-free array. Must be unrelated.\n@@ -1069,24 +1054,8 @@\n-      if (!unrelated_classes) {\n-        \/\/ Handle inline type arrays\n-        if ((r0->flatten_array() && (!r1->can_be_inline_type() || (klass1->is_inlinetype() && !klass1->flatten_array()))) ||\n-            (r1->flatten_array() && (!r0->can_be_inline_type() || (klass0->is_inlinetype() && !klass0->flatten_array())))) {\n-          \/\/ One type is flattened in arrays but the other type is not. Must be unrelated.\n-          unrelated_classes = true;\n-        } else if ((r0->is_not_flat() && klass1->is_flat_array_klass()) ||\n-                   (r1->is_not_flat() && klass0->is_flat_array_klass())) {\n-          \/\/ One type is a non-flattened array and the other type is a flattened array. Must be unrelated.\n-          unrelated_classes = true;\n-        } else if ((r0->is_not_null_free() && klass1->is_array_klass() && klass1->as_array_klass()->is_elem_null_free()) ||\n-                   (r1->is_not_null_free() && klass0->is_array_klass() && klass0->as_array_klass()->is_elem_null_free())) {\n-          \/\/ One type is a non-null-free array and the other type is a null-free array. Must be unrelated.\n-          unrelated_classes = true;\n-        }\n-      }\n-      if (unrelated_classes) {\n-        \/\/ The oops classes are known to be unrelated. If the joined PTRs of\n-        \/\/ two oops is not Null and not Bottom, then we are sure that one\n-        \/\/ of the two oops is non-null, and the comparison will always fail.\n-        TypePtr::PTR jp = r0->join_ptr(r1->_ptr);\n-        if (jp != TypePtr::Null && jp != TypePtr::BotPTR) {\n-          return TypeInt::CC_GT;\n-        }\n+    }\n+    if (unrelated_classes) {\n+      \/\/ The oops classes are known to be unrelated. If the joined PTRs of\n+      \/\/ two oops is not Null and not Bottom, then we are sure that one\n+      \/\/ of the two oops is non-null, and the comparison will always fail.\n+      TypePtr::PTR jp = r0->join_ptr(r1->_ptr);\n+      if (jp != TypePtr::Null && jp != TypePtr::BotPTR) {\n+        return TypeInt::CC_GT;\n@@ -1122,1 +1091,1 @@\n-  if (!tp || tp->klass() != phase->C->env()->Class_klass()) return NULL;\n+  if (!tp || tp->instance_klass() != phase->C->env()->Class_klass()) return NULL;\n@@ -1209,1 +1178,1 @@\n-  ciKlass* superklass = t2->klass();\n+  ciKlass* superklass = t2->exact_klass();\n","filename":"src\/hotspot\/share\/opto\/subnode.cpp","additions":59,"deletions":90,"binary":false,"changes":149,"status":"modified"},{"patch":"@@ -176,0 +176,12 @@\n+\/\/------------------------------CmpU3Node--------------------------------------\n+\/\/ Compare 2 unsigned values, returning integer value (-1, 0 or 1).\n+class CmpU3Node : public CmpUNode {\n+public:\n+  CmpU3Node( Node *in1, Node *in2 ) : CmpUNode(in1,in2) {\n+    \/\/ Since it is not consumed by Bools, it is not really a Cmp.\n+    init_class_id(Class_Sub);\n+  }\n+  virtual int Opcode() const;\n+  virtual uint ideal_reg() const { return Op_RegI; }\n+};\n+\n@@ -225,1 +237,13 @@\n-  virtual int    Opcode() const;\n+  virtual int Opcode() const;\n+  virtual uint ideal_reg() const { return Op_RegI; }\n+};\n+\n+\/\/------------------------------CmpUL3Node-------------------------------------\n+\/\/ Compare 2 unsigned long values, returning integer value (-1, 0 or 1).\n+class CmpUL3Node : public CmpULNode {\n+public:\n+  CmpUL3Node( Node *in1, Node *in2 ) : CmpULNode(in1,in2) {\n+    \/\/ Since it is not consumed by Bools, it is not really a Cmp.\n+    init_class_id(Class_Sub);\n+  }\n+  virtual int Opcode() const;\n@@ -401,1 +425,1 @@\n-\/\/ implemention on most chips.  Since a naive graph involves control flow, we\n+\/\/ implementation on most chips.  Since a naive graph involves control flow, we\n@@ -413,1 +437,1 @@\n-\/\/ implemention on most chips.  Since a naive graph involves control flow, we\n+\/\/ implementation on most chips.  Since a naive graph involves control flow, we\n@@ -573,0 +597,20 @@\n+\/\/-------------------------------ReverseINode--------------------------------\n+\/\/ reverse bits of an int\n+class ReverseINode : public Node {\n+public:\n+  ReverseINode(Node *c, Node *in1) : Node(c, in1) {}\n+  virtual int Opcode() const;\n+  const Type *bottom_type() const { return TypeInt::INT; }\n+  virtual uint ideal_reg() const { return Op_RegI; }\n+};\n+\n+\/\/-------------------------------ReverseLNode--------------------------------\n+\/\/ reverse bits of a long\n+class ReverseLNode : public Node {\n+public:\n+  ReverseLNode(Node *c, Node *in1) : Node(c, in1) {}\n+  virtual int Opcode() const;\n+  const Type *bottom_type() const { return TypeLong::LONG; }\n+  virtual uint ideal_reg() const { return Op_RegL; }\n+};\n+\n","filename":"src\/hotspot\/share\/opto\/subnode.hpp","additions":47,"deletions":3,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -36,5 +36,2 @@\n-  ciKlass* superk = super_t->is_klassptr()->klass();\n-  ciKlass* subk   = sub_t->isa_klassptr() ? sub_t->is_klassptr()->klass() : sub_t->is_oopptr()->klass();\n-\n-  bool xsubk = sub_t->isa_klassptr() ? sub_t->is_klassptr()->klass_is_exact() : sub_t->is_oopptr()->klass_is_exact();\n-\n+  const TypeKlassPtr* superk = super_t->isa_klassptr();\n+  const TypeKlassPtr* subk = sub_t->isa_klassptr() ? sub_t->is_klassptr() : sub_t->is_oopptr()->as_klass_type();\n@@ -43,5 +40,7 @@\n-  if (sub_t->isa_oopptr() && superk->is_instance_klass() &&\n-      !superk->is_interface() && superk->is_abstract() &&\n-      !superk->as_instance_klass()->has_subklass()) {\n-    Compile::current()->dependencies()->assert_leaf_type(superk);\n-    return TypeInt::CC_GT;\n+  if (sub_t->isa_oopptr() && superk->isa_instklassptr() && superk->klass_is_exact()) {\n+    ciKlass* superklass = superk->exact_klass();\n+    if (!superklass->is_interface() && superklass->is_abstract() &&\n+        !superklass->as_instance_klass()->has_subklass()) {\n+      Compile::current()->dependencies()->assert_leaf_type(superklass);\n+      return TypeInt::CC_GT;\n+    }\n@@ -51,47 +50,16 @@\n-\n-  \/\/ Interfaces can't be trusted unless the subclass is an exact\n-  \/\/ interface (it can then only be a constant) or the subclass is an\n-  \/\/ exact array of interfaces (a newly allocated array of interfaces\n-  \/\/ for instance)\n-  if (superk && subk &&\n-      superk->is_loaded() && !superk->is_interface() &&\n-      subk->is_loaded() && (!subk->is_interface() || xsubk) &&\n-      (!superk->is_obj_array_klass() ||\n-       !superk->as_obj_array_klass()->base_element_klass()->is_interface()) &&\n-      (!subk->is_obj_array_klass() ||\n-       !subk->as_obj_array_klass()->base_element_klass()->is_interface() ||\n-       xsubk)) {\n-    bool unrelated_classes = false;\n-    if (superk->equals(subk)) {\n-      \/\/ skip\n-    } else if (superk->is_subtype_of(subk)) {\n-      \/\/ If the subclass is exact then the superclass is a subtype of\n-      \/\/ the subclass. Given they're no equals, that subtype check can\n-      \/\/ only fail.\n-      unrelated_classes = xsubk;\n-    } else if (subk->is_subtype_of(superk)) {\n-      \/\/ skip\n-    } else {\n-      \/\/ Neither class subtypes the other: they are unrelated and this\n-      \/\/ type check is known to fail.\n-      unrelated_classes = true;\n-    }\n-    if (!unrelated_classes) {\n-      \/\/ Handle inline type arrays\n-      if (sub_t->isa_aryptr() && sub_t->is_aryptr()->is_not_flat() && superk->is_flat_array_klass()) {\n-        \/\/ Subtype is not a flat array but supertype is. Must be unrelated.\n-        unrelated_classes = true;\n-      } else if (sub_t->isa_aryptr() && sub_t->is_aryptr()->is_not_null_free() &&\n-                 superk->is_array_klass() && superk->as_array_klass()->is_elem_null_free()) {\n-        \/\/ Subtype is not a null-free array but supertype is. Must be unrelated.\n-        unrelated_classes = true;\n-      } else if (sub_t->is_ptr()->flatten_array() && (!superk->can_be_inline_klass() || (superk->is_inlinetype() && !superk->flatten_array()))) {\n-        \/\/ Subtype is flattened in arrays but supertype is not. Must be unrelated.\n-        unrelated_classes = true;\n-      }\n-    }\n-    if (unrelated_classes) {\n-      TypePtr::PTR jp = sub_t->is_ptr()->join_ptr(super_t->is_ptr()->_ptr);\n-      if (jp != TypePtr::Null && jp != TypePtr::BotPTR) {\n-        return TypeInt::CC_GT;\n-      }\n+  bool unrelated_classes = false;\n+  \/\/ Handle inline type arrays\n+  if (subk->flatten_array() && superk->not_flatten_array()) {\n+    \/\/ The subtype is flattened in arrays and the supertype is not flattened in arrays. Must be unrelated.\n+    unrelated_classes = true;\n+  } else if (subk->is_not_flat() && superk->is_flat()) {\n+    \/\/ The subtype is a non-flattened array and the supertype is a flattened array. Must be unrelated.\n+    unrelated_classes = true;\n+  } else if (subk->is_not_null_free() && superk->is_null_free()) {\n+    \/\/ The subtype is a nullable array and the supertype is null-free array. Must be unrelated.\n+    unrelated_classes = true;\n+  }\n+  if (unrelated_classes) {\n+    TypePtr::PTR jp = sub_t->is_ptr()->join_ptr(super_t->is_ptr()->_ptr);\n+    if (jp != TypePtr::Null && jp != TypePtr::BotPTR) {\n+      return TypeInt::CC_GT;\n@@ -101,3 +69,2 @@\n-  if (super_t->singleton()) {\n-    if (subk != NULL) {\n-      switch (Compile::current()->static_subtype_check(superk, subk)) {\n+  if (subk != NULL) {\n+    switch (Compile::current()->static_subtype_check(superk, subk)) {\n@@ -113,1 +80,0 @@\n-      }\n@@ -220,2 +186,2 @@\n-  ciKlass* subk = sub_t->isa_klassptr() ? sub_t->is_klassptr()->klass() : sub_t->is_oopptr()->klass(); \/\/ can be NULL for bottom[]\n-  ciKlass* superk = super_t->is_klassptr()->klass();\n+  const TypeKlassPtr* superk = super_t->isa_klassptr();\n+  const TypeKlassPtr* subk = sub_t->isa_klassptr() ? sub_t->is_klassptr() : sub_t->is_oopptr()->as_klass_type();\n","filename":"src\/hotspot\/share\/opto\/subtypenode.cpp","additions":29,"deletions":63,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -863,2 +863,2 @@\n-    bool this_interface = this_inst->klass()->is_interface();\n-    bool    t_interface =    t_inst->klass()->is_interface();\n+    bool this_interface = this_inst->is_interface();\n+    bool    t_interface =    t_inst->is_interface();\n@@ -1514,1 +1514,1 @@\n-\/\/ Convience common pre-built types.\n+\/\/ Convenience common pre-built types.\n@@ -1538,1 +1538,1 @@\n-TypeInt::TypeInt( jint lo, jint hi, int w ) : TypeInteger(Int), _lo(lo), _hi(hi), _widen(w) {\n+TypeInt::TypeInt( jint lo, jint hi, int w ) : TypeInteger(Int, w), _lo(lo), _hi(hi) {\n@@ -1686,1 +1686,1 @@\n-    return this;                \/\/ doesn't narrow; pretty wierd\n+    return this;                \/\/ doesn't narrow; pretty weird\n@@ -1801,1 +1801,1 @@\n-TypeLong::TypeLong(jlong lo, jlong hi, int w) : TypeInteger(Long), _lo(lo), _hi(hi), _widen(w) {\n+TypeLong::TypeLong(jlong lo, jlong hi, int w) : TypeInteger(Long, w), _lo(lo), _hi(hi) {\n@@ -1952,1 +1952,1 @@\n-    return this;                \/\/ doesn't narrow; pretty wierd\n+    return this;                \/\/ doesn't narrow; pretty weird\n@@ -2414,1 +2414,1 @@\n-const Type* TypeAry::remove_speculative() const {\n+const TypeAry* TypeAry::remove_speculative() const {\n@@ -2838,1 +2838,1 @@\n-const Type *TypePtr::cast_to_ptr_type(PTR ptr) const {\n+const TypePtr* TypePtr::cast_to_ptr_type(PTR ptr) const {\n@@ -2947,0 +2947,4 @@\n+const TypePtr *TypePtr::with_offset(intptr_t offset) const {\n+  return make(AnyPtr, _ptr, Offset(offset), _speculative, _inline_depth);\n+}\n+\n@@ -2964,1 +2968,1 @@\n-const Type* TypePtr::remove_speculative() const {\n+const TypePtr* TypePtr::remove_speculative() const {\n@@ -3099,0 +3103,7 @@\n+const TypePtr* TypePtr::with_offset_speculative(intptr_t offset) const {\n+  if (_speculative == NULL) {\n+    return NULL;\n+  }\n+  return _speculative->with_offset(offset)->is_ptr();\n+}\n+\n@@ -3352,1 +3363,1 @@\n-const TypePtr *TypeRawPtr::add_offset( intptr_t offset ) const {\n+const TypePtr* TypeRawPtr::add_offset(intptr_t offset) const {\n@@ -3516,1 +3527,1 @@\n-const Type *TypeOopPtr::cast_to_exactness(bool klass_is_exact) const {\n+const TypeOopPtr* TypeOopPtr::cast_to_exactness(bool klass_is_exact) const {\n@@ -3760,1 +3771,1 @@\n-    \/\/ but this assertion here is to help prevent its occurence.\n+    \/\/ but this assertion here is to help prevent its occurrence.\n@@ -3871,0 +3882,4 @@\n+const TypeOopPtr* TypeOopPtr::with_offset(intptr_t offset) const {\n+  return make(_ptr, Offset(offset), _instance_id, with_offset_speculative(offset), _inline_depth);\n+}\n+\n@@ -3874,1 +3889,1 @@\n-const Type* TypeOopPtr::remove_speculative() const {\n+const TypeOopPtr* TypeOopPtr::remove_speculative() const {\n@@ -3954,0 +3969,4 @@\n+ciKlass* TypeInstPtr::exact_klass_helper() const {\n+  return _klass;\n+}\n+\n@@ -3956,2 +3975,1 @@\n-                         bool flatten_array, int instance_id, const TypePtr* speculative,\n-                         int inline_depth)\n+                         bool flatten_array, int instance_id, const TypePtr* speculative, int inline_depth)\n@@ -3959,6 +3977,6 @@\n-    _name(k->name()), _flatten_array(flatten_array) {\n-  assert(k != NULL &&\n-         (k->is_loaded() || o == NULL),\n-         \"cannot have constants with non-loaded klass\");\n-  assert(!klass()->flatten_array() || flatten_array, \"Should be flat in array\");\n-  assert(!flatten_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n+    _flatten_array(flatten_array) {\n+   assert(k != NULL &&\n+          (k->is_loaded() || o == NULL),\n+          \"cannot have constants with non-loaded klass\");\n+   assert(!klass()->flatten_array() || flatten_array, \"Should be flat in array\");\n+   assert(!flatten_array || can_be_inline_type(), \"Only inline types can be flat in array\");\n@@ -4028,1 +4046,1 @@\n-const TypeInstPtr *TypeInstPtr::cast_to_ptr_type(PTR ptr) const {\n+const TypeInstPtr* TypeInstPtr::cast_to_ptr_type(PTR ptr) const {\n@@ -4032,1 +4050,1 @@\n-  return make(ptr, klass(), klass_is_exact(), const_oop(), _offset, _flatten_array, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), klass_is_exact(), ptr == Constant ? const_oop() : NULL, _offset, _flatten_array, _instance_id, _speculative, _inline_depth);\n@@ -4037,1 +4055,1 @@\n-const Type *TypeInstPtr::cast_to_exactness(bool klass_is_exact) const {\n+const TypeInstPtr* TypeInstPtr::cast_to_exactness(bool klass_is_exact) const {\n@@ -4047,1 +4065,1 @@\n-const TypeOopPtr *TypeInstPtr::cast_to_instance_id(int instance_id) const {\n+const TypeInstPtr* TypeInstPtr::cast_to_instance_id(int instance_id) const {\n@@ -4490,0 +4508,64 @@\n+bool TypeInstPtr::is_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return false;\n+  }\n+  if (!other->isa_instptr()) {\n+    return false;\n+  }\n+\n+  if (!other_exact) {\n+    return false;\n+  }\n+\n+  if (other->klass()->equals(ciEnv::current()->Object_klass())) {\n+    return true;\n+  }\n+\n+  if (!this_exact && klass()->is_interface()) {\n+    return false;\n+  }\n+\n+  return _klass->is_subtype_of(other->klass());\n+}\n+\n+bool TypeInstPtr::is_same_java_type_as(const TypeOopPtr* other) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return false;\n+  }\n+  if (!other->isa_instptr()) {\n+    return false;\n+  }\n+  return _klass->equals(other->_klass);\n+}\n+\n+bool TypeInstPtr::maybe_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return true;\n+  }\n+\n+  if (other->isa_aryptr()) {\n+    return !this_exact && (_klass->equals(ciEnv::current()->Object_klass()) || _klass->is_interface());\n+  }\n+\n+  if ((_klass->is_interface() && !this_exact) || (other->klass()->is_interface() \/*&& !other_exact*\/)) {\n+    return true;\n+  }\n+\n+  assert(other->isa_instptr(), \"unsupported\");\n+\n+  if (this_exact && other_exact) {\n+    return is_java_subtype_of(other);\n+  }\n+\n+  if (!_klass->is_subtype_of(other->_klass) && !other->_klass->is_subtype_of(_klass)) {\n+    return false;\n+  }\n+\n+  if (this_exact) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+\n+  return true;\n+}\n+\n+\n@@ -4545,1 +4627,1 @@\n-const TypePtr *TypeInstPtr::add_offset(intptr_t offset) const {\n+const TypePtr* TypeInstPtr::add_offset(intptr_t offset) const {\n@@ -4550,1 +4632,6 @@\n-const Type *TypeInstPtr::remove_speculative() const {\n+const TypeInstPtr* TypeInstPtr::with_offset(intptr_t offset) const {\n+  return make(_ptr, klass(), klass_is_exact(), const_oop(), Offset(offset), flatten_array(),\n+              _instance_id, with_offset_speculative(offset), _inline_depth);\n+}\n+\n+const TypeInstPtr* TypeInstPtr::remove_speculative() const {\n@@ -4626,1 +4713,1 @@\n-  return make(ptr, const_oop(), _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+  return make(ptr, ptr == Constant ? const_oop() : NULL, _ary, klass(), klass_is_exact(), _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n@@ -4631,1 +4718,1 @@\n-const Type *TypeAryPtr::cast_to_exactness(bool klass_is_exact) const {\n+const TypeAryPtr* TypeAryPtr::cast_to_exactness(bool klass_is_exact) const {\n@@ -4644,1 +4731,1 @@\n-const TypeOopPtr *TypeAryPtr::cast_to_instance_id(int instance_id) const {\n+const TypeAryPtr* TypeAryPtr::cast_to_instance_id(int instance_id) const {\n@@ -4799,0 +4886,67 @@\n+bool TypeAryPtr::is_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other_exact) {\n+    return true;\n+  }\n+\n+  if (!is_loaded() || !other->is_loaded() || other->klass() == NULL || klass() == NULL) {\n+    return false;\n+  }\n+  if (other->isa_instptr()) {\n+    return _klass->is_subtype_of(other->_klass) && other_exact;\n+  }\n+  if (klass() == NULL) {\n+    return false;\n+  }\n+  assert(other->isa_aryptr(), \"\");\n+  const TypeAryPtr* other_ary = other->isa_aryptr();\n+  if (other_ary->elem()->make_oopptr() && elem()->make_oopptr()) {\n+    return elem()->make_oopptr()->is_java_subtype_of_helper(other_ary->elem()->make_oopptr(), this_exact, other_exact);\n+  }\n+  if (!other_ary->elem()->make_oopptr() && !elem()->make_oopptr()) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+  return false;\n+}\n+\n+bool TypeAryPtr::is_same_java_type_as(const TypeOopPtr* other) const {\n+  if (!other->isa_aryptr() ||\n+      !is_loaded() || !other->is_loaded() || klass() == NULL || other->klass() == NULL) {\n+    return false;\n+  }\n+  const TypeAryPtr* other_ary = other->isa_aryptr();\n+  if (other_ary->elem()->make_oopptr() && elem()->make_oopptr()) {\n+    return elem()->make_oopptr()->is_same_java_type_as(other_ary->elem()->make_oopptr());\n+  }\n+  if (!other_ary->elem()->make_oopptr() && !elem()->make_oopptr()) {\n+    return _klass->equals(other->_klass);\n+  }\n+  return false;\n+}\n+\n+bool TypeAryPtr::maybe_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const {\n+  if (other->klass() == ciEnv::current()->Object_klass()) {\n+    return true;\n+  }\n+\n+  if (!is_loaded() || !other->is_loaded() || klass() == NULL || other->klass() == NULL) {\n+    return true;\n+  }\n+  if (other->isa_instptr()) {\n+    return (!other_exact && other->_klass->is_interface()) || _klass->is_subtype_of(other->_klass);\n+  }\n+  assert(other->isa_aryptr(), \"\");\n+\n+  if (this_exact && other_exact) {\n+    return is_java_subtype_of(other);\n+  }\n+\n+  const TypeAryPtr* other_ary = other->isa_aryptr();\n+  if (other_ary->elem()->make_oopptr() && elem()->make_oopptr()) {\n+    return elem()->make_oopptr()->maybe_java_subtype_of_helper(other_ary->elem()->make_oopptr(), this_exact,\n+                                                               other_exact);\n+  }\n+  if (!other_ary->elem()->make_oopptr() && !elem()->make_oopptr()) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+  return false;\n+}\n@@ -5157,3 +5311,7 @@\n-      int array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n-      int elem_size = type2aelembytes(basic_elem_type);\n-      st->print(\"[%d]\", (offset() - array_base)\/elem_size);\n+      if (basic_elem_type == T_ILLEGAL) {\n+        st->print(\"+any\");\n+      } else {\n+        int array_base = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n+        int elem_size = type2aelembytes(basic_elem_type);\n+        st->print(\"[%d]\", (offset() - array_base)\/elem_size);\n+      }\n@@ -5183,1 +5341,9 @@\n-const Type *TypeAryPtr::remove_speculative() const {\n+const TypeAryPtr* TypeAryPtr::with_offset(intptr_t offset) const {\n+  return make(_ptr, _const_oop, _ary, _klass, _klass_is_exact, Offset(offset), _field_offset, _instance_id, with_offset_speculative(offset), _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::with_ary(const TypeAry* ary) const {\n+  return make(_ptr, _const_oop, ary, _klass, _klass_is_exact, _offset, _field_offset, _instance_id, _speculative, _inline_depth, _is_autobox_cache);\n+}\n+\n+const TypeAryPtr* TypeAryPtr::remove_speculative() const {\n@@ -5386,1 +5552,1 @@\n-const Type* TypeNarrowOop::remove_speculative() const {\n+const TypeNarrowOop* TypeNarrowOop::remove_speculative() const {\n@@ -5471,1 +5637,1 @@\n-    \/\/ but this assertion here is to help prevent its occurence.\n+    \/\/ but this assertion here is to help prevent its occurrence.\n@@ -5614,1 +5780,3 @@\n-    if (elem->is_klassptr()->klass_is_exact()) {\n+    if (elem->is_klassptr()->klass_is_exact() &&\n+        \/\/ Even if MyValue is exact, [LMyValue is not exact due to [QMyValue <: [LMyValue.\n+        (is_null_free() || !_ary->_elem->make_oopptr()->is_inlinetypeptr())) {\n@@ -5617,0 +5785,2 @@\n+  } else if (elem->isa_inlinetype() != NULL) {\n+    elem = TypeKlassPtr::make(elem->inline_klass());\n@@ -5621,1 +5791,1 @@\n-const TypeKlassPtr* TypeKlassPtr::make(ciKlass *klass) {\n+const TypeKlassPtr* TypeKlassPtr::make(ciKlass* klass) {\n@@ -5632,1 +5802,1 @@\n-  return TypeAryKlassPtr::make(klass, ptr, offset);\n+  return TypeAryKlassPtr::make(ptr, klass, offset);\n@@ -5640,0 +5810,4 @@\n+ciKlass* TypeKlassPtr::exact_klass_helper() const {\n+  return _klass;\n+}\n+\n@@ -5702,1 +5876,1 @@\n-    \/\/ but this assertion here is to help prevent its occurence.\n+    \/\/ but this assertion here is to help prevent its occurrence.\n@@ -5707,1 +5881,3 @@\n-  return (intptr_t)klass()->constant_encoding();\n+  ciKlass* k = exact_klass();\n+\n+  return (intptr_t)k->constant_encoding();\n@@ -5778,1 +5954,1 @@\n-const TypeKlassPtr *TypeInstKlassPtr::with_offset(intptr_t offset) const {\n+const TypeInstKlassPtr* TypeInstKlassPtr::with_offset(intptr_t offset) const {\n@@ -5783,1 +5959,1 @@\n-const TypePtr* TypeInstKlassPtr::cast_to_ptr_type(PTR ptr) const {\n+const TypeInstKlassPtr* TypeInstKlassPtr::cast_to_ptr_type(PTR ptr) const {\n@@ -5809,1 +5985,1 @@\n-const TypeOopPtr* TypeInstKlassPtr::as_instance_type() const {\n+const TypeOopPtr* TypeInstKlassPtr::as_instance_type(bool klass_change) const {\n@@ -5811,1 +5987,20 @@\n-  bool    xk = klass_is_exact();\n+  bool xk = klass_is_exact();\n+  Compile* C = Compile::current();\n+  Dependencies* deps = C->dependencies();\n+  assert((deps != NULL) == (C->method() != NULL && C->method()->code_size() > 0), \"sanity\");\n+  \/\/ Element is an instance\n+  bool klass_is_exact = false;\n+  if (k->is_loaded()) {\n+    \/\/ Try to set klass_is_exact.\n+    ciInstanceKlass* ik = k->as_instance_klass();\n+    klass_is_exact = ik->is_final();\n+    if (!klass_is_exact && klass_change\n+        && deps != NULL && UseUniqueSubclasses) {\n+      ciInstanceKlass* sub = ik->unique_concrete_subklass();\n+      if (sub != NULL) {\n+        deps->assert_abstract_with_unique_concrete_subtype(ik, sub);\n+        k = ik = sub;\n+        xk = sub->is_final();\n+      }\n+    }\n+  }\n@@ -5987,1 +6182,64 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free) {\n+bool TypeInstKlassPtr::is_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return false;\n+  }\n+  if (!other->isa_instklassptr()) {\n+    return false;\n+  }\n+\n+  if (!other_exact) {\n+    return false;\n+  }\n+\n+  if (other->_klass->equals(ciEnv::current()->Object_klass())) {\n+    return true;\n+  }\n+\n+  if (!this_exact && klass()->is_interface()) {\n+    return false;\n+  }\n+\n+  return _klass->is_subtype_of(other->_klass);\n+}\n+\n+bool TypeInstKlassPtr::is_same_java_type_as(const TypeKlassPtr* other) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return false;\n+  }\n+  if (!other->isa_instklassptr()) {\n+    return false;\n+  }\n+  return _klass->equals(other->_klass);\n+}\n+\n+bool TypeInstKlassPtr::maybe_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const {\n+  if (!is_loaded() || !other->is_loaded()) {\n+    return true;\n+  }\n+\n+  if (other->isa_aryklassptr()) {\n+    return !this_exact && (_klass->equals(ciEnv::current()->Object_klass()) || _klass->is_interface());\n+  }\n+\n+  if ((_klass->is_interface() && !this_exact) || (other->klass()->is_interface() \/*&& !other_exact*\/)) {\n+    return true;\n+  }\n+\n+  assert(other->isa_instklassptr(), \"unsupported\");\n+\n+  if (this_exact && other_exact) {\n+    return is_java_subtype_of(other);\n+  }\n+\n+  if (!_klass->is_subtype_of(other->_klass) && !other->_klass->is_subtype_of(_klass)) {\n+    return false;\n+  }\n+\n+  if (this_exact) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+\n+  return true;\n+}\n+\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free) {\n@@ -5991,1 +6249,1 @@\n-const TypeAryKlassPtr *TypeAryKlassPtr::make(PTR ptr, ciKlass* klass, Offset offset, bool not_flat, bool not_null_free, bool null_free) {\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* klass, Offset offset, bool not_flat, bool not_null_free, bool null_free) {\n@@ -5995,1 +6253,1 @@\n-    const TypeKlassPtr *etype = TypeKlassPtr::make(eklass)->cast_to_exactness(false);\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass)->cast_to_exactness(false);\n@@ -6010,2 +6268,3 @@\n-    ciInlineKlass* vk = klass->as_array_klass()->element_klass()->as_inline_klass();\n-    return TypeAryKlassPtr::make(ptr, TypeInlineType::make(vk), klass, offset, not_flat, not_null_free, null_free);\n+    ciKlass* eklass = klass->as_flat_array_klass()->element_klass();\n+    const TypeKlassPtr* etype = TypeKlassPtr::make(eklass);\n+    return TypeAryKlassPtr::make(ptr, etype, klass, offset, not_flat, not_null_free, null_free);\n@@ -6018,1 +6277,1 @@\n-const TypeAryKlassPtr* TypeAryKlassPtr::make(ciKlass* k, PTR ptr, Offset offset) {\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(PTR ptr, ciKlass* k, Offset offset) {\n@@ -6020,1 +6279,1 @@\n-  bool not_null_free = ptr == Constant ? !null_free : !k->is_flat_array_klass() && (k->is_type_array_klass() || !k->as_array_klass()->element_klass()->can_be_inline_klass(false));\n+  bool not_null_free = (ptr == Constant) ? !null_free : !k->is_flat_array_klass() && (k->is_type_array_klass() || !k->as_array_klass()->element_klass()->can_be_inline_klass(false));\n@@ -6029,0 +6288,4 @@\n+const TypeAryKlassPtr* TypeAryKlassPtr::make(ciKlass* klass) {\n+  return TypeAryKlassPtr::make(Constant, klass, Offset(0));\n+}\n+\n@@ -6132,1 +6395,2 @@\n-        offset() != 0 && offset() != arrayOopDesc::length_offset_in_bytes()) {\n+        offset() != 0 && offset() != arrayOopDesc::length_offset_in_bytes() &&\n+        offset() != arrayOopDesc::klass_offset_in_bytes()) {\n@@ -6139,0 +6403,22 @@\n+ciKlass* TypeAryPtr::exact_klass_helper() const {\n+  if (_ary->_elem->make_ptr() && _ary->_elem->make_ptr()->isa_oopptr()) {\n+    ciKlass* k = _ary->_elem->make_ptr()->is_oopptr()->exact_klass_helper();\n+    if (k == NULL) {\n+      return NULL;\n+    }\n+    k = ciArrayKlass::make(k, is_null_free());\n+    return k;\n+  }\n+\n+  return klass();\n+}\n+\n+const Type* TypeAryPtr::base_element_type(int& dims) const {\n+  const Type* elem = this->elem();\n+  dims = 1;\n+  while (elem->make_ptr() && elem->make_ptr()->isa_aryptr()) {\n+    elem = elem->make_ptr()->is_aryptr()->elem();\n+    dims++;\n+  }\n+  return elem;\n+}\n@@ -6146,1 +6432,1 @@\n-const TypeKlassPtr *TypeAryKlassPtr::with_offset(intptr_t offset) const {\n+const TypeAryKlassPtr* TypeAryKlassPtr::with_offset(intptr_t offset) const {\n@@ -6151,1 +6437,1 @@\n-const TypePtr* TypeAryKlassPtr::cast_to_ptr_type(PTR ptr) const {\n+const TypeAryKlassPtr* TypeAryKlassPtr::cast_to_ptr_type(PTR ptr) const {\n@@ -6176,4 +6462,12 @@\n-  if (klass() != NULL && klass()->is_obj_array_klass() && klass_is_exact) {\n-    \/\/ An object array can't be flat or null-free if the klass is exact\n-    not_flat = true;\n-    not_null_free = true;\n+  if (k != NULL && k->is_obj_array_klass()) {\n+    if (klass_is_exact) {\n+      \/\/ An object array can't be flat or null-free if the klass is exact\n+      not_flat = true;\n+      not_null_free = true;\n+    } else {\n+      \/\/ Klass is not exact (anymore), re-compute null-free\/flat properties\n+      not_null_free = !k->as_array_klass()->element_klass()->can_be_inline_klass(false);\n+      not_flat = !UseFlatArray || not_null_free || (k->as_array_klass()->element_klass() != NULL &&\n+                                                    k->as_array_klass()->element_klass()->is_inlinetype() &&\n+                                                    !k->as_array_klass()->element_klass()->flatten_array());\n+    }\n@@ -6187,2 +6481,2 @@\n-\/\/ It will be exact if and only if the klass type is exact.\n-const TypeOopPtr* TypeAryKlassPtr::as_instance_type() const {\n+\/\/ It will be NotNull, and exact if and only if the klass type is exact.\n+const TypeOopPtr* TypeAryKlassPtr::as_instance_type(bool klass_change) const {\n@@ -6192,1 +6486,7 @@\n-  const Type* el = elem()->isa_klassptr() ? elem()->is_klassptr()->as_instance_type()->is_oopptr()->cast_to_exactness(false) : elem();\n+  const Type* el = NULL;\n+  if (elem()->isa_klassptr()) {\n+    el = elem()->is_klassptr()->as_instance_type(false)->cast_to_exactness(false);\n+    k = NULL;\n+  } else {\n+    el = elem();\n+  }\n@@ -6199,0 +6499,3 @@\n+  if (is_flat()) {\n+    el = TypeInlineType::make(el->inline_klass());\n+  }\n@@ -6357,0 +6660,69 @@\n+bool TypeAryKlassPtr::is_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const {\n+  if (other->klass() == ciEnv::current()->Object_klass() && other_exact) {\n+    return true;\n+  }\n+\n+  if (!is_loaded() || !other->is_loaded() || other->klass() == NULL || klass() == NULL) {\n+    return false;\n+  }\n+  if (other->isa_instklassptr()) {\n+    return _klass->is_subtype_of(other->_klass) && other_exact;\n+  }\n+  if (klass() == NULL) {\n+    return false;\n+  }\n+  assert(other->isa_aryklassptr(), \"\");\n+  const TypeAryKlassPtr* other_ary = other->isa_aryklassptr();\n+  if (other_ary->_elem->isa_klassptr() && _elem->isa_klassptr()) {\n+    if (other->is_null_free() && !is_null_free()) {\n+      return false; \/\/ [LMyValue is not a subtype of [QMyValue\n+    }\n+    return _elem->is_klassptr()->is_java_subtype_of_helper(other_ary->_elem->is_klassptr(), this_exact, other_exact);\n+  }\n+  if (!other_ary->_elem->isa_klassptr() && !_elem->isa_klassptr()) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+  return false;\n+}\n+\n+bool TypeAryKlassPtr::is_same_java_type_as(const TypeKlassPtr* other) const {\n+  if (!other->isa_aryklassptr() ||\n+      !is_loaded() || !other->is_loaded() || klass() == NULL || other->klass() == NULL) {\n+    return false;\n+  }\n+  const TypeAryKlassPtr* other_ary = other->isa_aryklassptr();\n+  if (other_ary->_elem->isa_klassptr() && _elem->isa_klassptr()) {\n+    return _elem->is_klassptr()->is_same_java_type_as(other_ary->_elem->is_klassptr());\n+  }\n+  if (!other_ary->_elem->isa_klassptr() && !_elem->isa_klassptr()) {\n+    return _klass->equals(other->_klass);\n+  }\n+  return false;\n+}\n+\n+bool TypeAryKlassPtr::maybe_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const {\n+  if (other->klass() == ciEnv::current()->Object_klass()) {\n+    return true;\n+  }\n+  if (!is_loaded() || !other->is_loaded() || klass() == NULL || other->klass() == NULL) {\n+    return true;\n+  }\n+  if (other->isa_instklassptr()) {\n+    return (!other_exact && other->_klass->is_interface()) || _klass->is_subtype_of(other->_klass);\n+  }\n+  assert(other->isa_aryklassptr(), \"\");\n+\n+  if (this_exact && other_exact) {\n+    return is_java_subtype_of(other);\n+  }\n+\n+  const TypeAryKlassPtr* other_ary = other->isa_aryklassptr();\n+  if (other_ary->_elem->isa_klassptr() && _elem->isa_klassptr()) {\n+    return _elem->is_klassptr()->maybe_java_subtype_of_helper(other_ary->_elem->is_klassptr(), this_exact, other_exact);\n+  }\n+  if (!other_ary->_elem->isa_klassptr() && !_elem->isa_klassptr()) {\n+    return _klass->is_subtype_of(other->_klass);\n+  }\n+  return false;\n+}\n+\n@@ -6364,0 +6736,13 @@\n+ciKlass* TypeAryKlassPtr::exact_klass_helper() const {\n+  if (elem()->isa_klassptr()) {\n+    ciKlass* k = elem()->is_klassptr()->exact_klass_helper();\n+    if (k == NULL) {\n+      return NULL;\n+    }\n+    k = ciArrayKlass::make(k, _null_free);\n+    return k;\n+  }\n+\n+  return klass();\n+}\n+\n@@ -6365,1 +6750,1 @@\n-    if (_klass != NULL) {\n+  if (_klass != NULL) {\n@@ -6425,0 +6810,1 @@\n+    if (is_flat()) st->print(\":flat\");\n@@ -6427,1 +6813,1 @@\n-    if (_null_free != 0) st->print(\":null free(%d)\", _null_free);\n+    if (_null_free) st->print(\":null free\");\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":452,"deletions":66,"binary":false,"changes":518,"status":"modified"},{"patch":"@@ -585,1 +585,1 @@\n-  TypeInteger(TYPES t) : Type(t) {}\n+  TypeInteger(TYPES t, int w) : Type(t), _widen(w) {}\n@@ -588,0 +588,2 @@\n+  const short _widen;           \/\/ Limit on times we widen this sucker\n+\n@@ -592,0 +594,1 @@\n+  virtual short widen_limit() const { return _widen; }\n@@ -618,1 +621,0 @@\n-  const short _widen;           \/\/ Limit on times we widen this sucker\n@@ -686,1 +688,0 @@\n-  const short _widen;           \/\/ Limit on times we widen this sucker\n@@ -813,1 +814,1 @@\n-  virtual const Type* remove_speculative() const;\n+  virtual const TypeAry* remove_speculative() const;\n@@ -998,0 +999,1 @@\n+  const TypePtr* with_offset_speculative(intptr_t offset) const;\n@@ -1043,1 +1045,1 @@\n-  virtual const Type *cast_to_ptr_type(PTR ptr) const;\n+  virtual const TypePtr* cast_to_ptr_type(PTR ptr) const;\n@@ -1047,2 +1049,3 @@\n-  Offset xadd_offset(intptr_t offset) const;\n-  virtual const TypePtr *add_offset( intptr_t offset ) const;\n+  Type::Offset xadd_offset(intptr_t offset) const;\n+  virtual const TypePtr* add_offset(intptr_t offset) const;\n+  virtual const TypePtr* with_offset(intptr_t offset) const;\n@@ -1050,1 +1053,0 @@\n-\n@@ -1079,1 +1081,1 @@\n-  virtual const Type* remove_speculative() const;\n+  virtual const TypePtr* remove_speculative() const;\n@@ -1089,0 +1091,1 @@\n+  virtual bool not_flatten_array()  const { return false; }\n@@ -1126,1 +1129,2 @@\n-  virtual const TypePtr *add_offset( intptr_t offset ) const;\n+  virtual const TypePtr* add_offset(intptr_t offset) const;\n+  virtual const TypeRawPtr* with_offset(intptr_t offset) const { ShouldNotReachHere(); return NULL;}\n@@ -1141,0 +1145,4 @@\n+  friend class TypeAry;\n+  friend class TypePtr;\n+  friend class TypeInstPtr;\n+  friend class TypeAryPtr;\n@@ -1177,0 +1185,2 @@\n+  virtual ciKlass* exact_klass_helper() const { return NULL; }\n+\n@@ -1178,0 +1188,14 @@\n+  \/\/ TODO Tobias\n+  virtual ciKlass* klass() const { return _klass; }\n+\n+  bool is_java_subtype_of(const TypeOopPtr* other) const {\n+    return is_java_subtype_of_helper(other, klass_is_exact(), other->klass_is_exact());\n+  }\n+  virtual bool is_same_java_type_as(const TypeOopPtr* other) const { ShouldNotReachHere(); return false; }\n+  bool maybe_java_subtype_of(const TypeOopPtr* other) const {\n+    return maybe_java_subtype_of_helper(other, klass_is_exact(), other->klass_is_exact());\n+  }\n+  virtual bool is_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const { ShouldNotReachHere(); return false; }\n+  virtual bool maybe_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const { ShouldNotReachHere(); return false; }\n+\n+\n@@ -1207,1 +1231,5 @@\n-  virtual ciKlass* klass() const { return _klass;     }\n+  \/\/ Exact klass, possibly an interface or an array of interface\n+  ciKlass* exact_klass(bool maybe_null = false) const { assert(klass_is_exact(), \"\"); ciKlass* k = exact_klass_helper(); assert(k != NULL || maybe_null, \"\"); return k;  }\n+  ciKlass* unloaded_klass() const { assert(!is_loaded(), \"only for unloaded types\"); return klass(); }\n+\n+  virtual bool  is_loaded() const { return klass()->is_loaded(); }\n@@ -1219,1 +1247,1 @@\n-  virtual bool can_be_inline_type() const { return EnableValhalla && (_klass == NULL || _klass->can_be_inline_klass(_klass_is_exact)); }\n+  virtual bool can_be_inline_type() const { return (_klass == NULL || _klass->can_be_inline_klass(_klass_is_exact)); }\n@@ -1225,1 +1253,1 @@\n-  virtual const Type *cast_to_exactness(bool klass_is_exact) const;\n+  virtual const TypeOopPtr* cast_to_exactness(bool klass_is_exact) const;\n@@ -1232,1 +1260,2 @@\n-  virtual const TypePtr *add_offset( intptr_t offset ) const;\n+  virtual const TypeOopPtr* with_offset(intptr_t offset) const;\n+  virtual const TypePtr* add_offset(intptr_t offset) const;\n@@ -1235,1 +1264,1 @@\n-  virtual const Type* remove_speculative() const;\n+  virtual const TypeOopPtr* remove_speculative() const;\n@@ -1263,1 +1292,0 @@\n-  ciSymbol*  _name;        \/\/ class name\n@@ -1265,0 +1293,1 @@\n+  ciKlass* exact_klass_helper() const;\n@@ -1266,2 +1295,9 @@\n- public:\n-  ciSymbol* name()         const { return _name; }\n+public:\n+\n+  \/\/ Instance klass, ignoring any interface\n+  ciInstanceKlass* instance_klass() const {\n+    if (klass()->is_loaded() && klass()->is_interface()) {\n+      return Compile::current()->env()->Object_klass();\n+    }\n+    return klass()->as_instance_klass();\n+  }\n@@ -1269,1 +1305,3 @@\n-  bool  is_loaded() const { return _klass->is_loaded(); }\n+  bool is_same_java_type_as(const TypeOopPtr* other) const;\n+  bool is_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const;\n+  bool maybe_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const;\n@@ -1312,1 +1350,1 @@\n-  virtual const Type *cast_to_exactness(bool klass_is_exact) const;\n+  virtual const TypeInstPtr* cast_to_exactness(bool klass_is_exact) const;\n@@ -1314,1 +1352,1 @@\n-  virtual const TypeOopPtr *cast_to_instance_id(int instance_id) const;\n+  virtual const TypeInstPtr* cast_to_instance_id(int instance_id) const;\n@@ -1316,1 +1354,2 @@\n-  virtual const TypePtr *add_offset( intptr_t offset ) const;\n+  virtual const TypePtr* add_offset(intptr_t offset) const;\n+  virtual const TypeInstPtr* with_offset(intptr_t offset) const;\n@@ -1319,1 +1358,1 @@\n-  virtual const Type* remove_speculative() const;\n+  virtual const TypeInstPtr* remove_speculative() const;\n@@ -1325,0 +1364,1 @@\n+  virtual bool not_flatten_array() const { return !can_be_inline_type() || (_klass->is_inlinetype() && !flatten_array()); }\n@@ -1333,0 +1373,2 @@\n+  bool is_interface() const { return is_loaded() && klass()->is_interface(); }\n+\n@@ -1385,0 +1427,1 @@\n+  ciKlass* exact_klass_helper() const;\n@@ -1386,1 +1429,1 @@\n-  \/\/ Accessors\n+  \/\/ TODO Tobias\n@@ -1388,0 +1431,12 @@\n+\n+\n+  bool is_same_java_type_as(const TypeOopPtr* other) const;\n+  bool is_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const;\n+  bool maybe_java_subtype_of_helper(const TypeOopPtr* other, bool this_exact, bool other_exact) const;\n+\n+  \/\/ returns base element type, an instance klass (and not interface) for object arrays\n+  const Type* base_element_type(int& dims) const;\n+\n+  \/\/ Accessors\n+  bool  is_loaded() const { return (_ary->_elem->make_oopptr() ? _ary->_elem->make_oopptr()->is_loaded() : true); }\n+\n@@ -1417,1 +1472,1 @@\n-  virtual const Type *cast_to_exactness(bool klass_is_exact) const;\n+  virtual const TypeAryPtr* cast_to_exactness(bool klass_is_exact) const;\n@@ -1419,1 +1474,1 @@\n-  virtual const TypeOopPtr *cast_to_instance_id(int instance_id) const;\n+  virtual const TypeAryPtr* cast_to_instance_id(int instance_id) const;\n@@ -1426,0 +1481,2 @@\n+  virtual const TypeAryPtr *with_offset( intptr_t offset ) const;\n+  const TypeAryPtr* with_ary(const TypeAry* ary) const;\n@@ -1428,1 +1485,1 @@\n-  virtual const Type* remove_speculative() const;\n+  virtual const TypeAryPtr* remove_speculative() const;\n@@ -1528,0 +1585,2 @@\n+  friend class TypeInstKlassPtr;\n+  friend class TypeAryKlassPtr;\n@@ -1537,1 +1596,0 @@\n-  virtual bool must_be_exact() const { ShouldNotReachHere(); return false; }\n@@ -1543,0 +1601,2 @@\n+  virtual bool must_be_exact() const { ShouldNotReachHere(); return false; }\n+  virtual ciKlass* exact_klass_helper() const;\n@@ -1544,1 +1604,1 @@\n-\n+  \/\/ TODO Tobias\n@@ -1546,0 +1606,15 @@\n+\n+\n+  bool is_java_subtype_of(const TypeKlassPtr* other) const {\n+    return is_java_subtype_of_helper(other, klass_is_exact(), other->klass_is_exact());\n+  }\n+  bool maybe_java_subtype_of(const TypeKlassPtr* other) const {\n+    return maybe_java_subtype_of_helper(other, klass_is_exact(), other->klass_is_exact());\n+  }\n+  virtual bool is_same_java_type_as(const TypeKlassPtr* other) const { ShouldNotReachHere(); return false; }\n+  virtual bool is_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const { ShouldNotReachHere(); return false; }\n+  virtual bool maybe_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const { ShouldNotReachHere(); return false; }\n+\n+  \/\/ Exact klass, possibly an interface or an array of interface\n+  ciKlass* exact_klass(bool maybe_null = false) const { assert(klass_is_exact(), \"\"); ciKlass* k = exact_klass_helper(); assert(k != NULL || maybe_null, \"\"); return k;  }\n+\n@@ -1547,1 +1622,0 @@\n-  bool  is_loaded() const { return klass()->is_loaded(); }\n@@ -1550,1 +1624,1 @@\n-  static const TypeKlassPtr *make(PTR ptr, ciKlass* klass, Offset offset);\n+  static const TypeKlassPtr* make(PTR ptr, ciKlass* klass, Offset offset);\n@@ -1552,0 +1626,1 @@\n+  virtual bool  is_loaded() const { return _klass->is_loaded(); }\n@@ -1553,1 +1628,1 @@\n-  virtual const TypePtr* cast_to_ptr_type(PTR ptr) const { ShouldNotReachHere(); return NULL; }\n+  virtual const TypeKlassPtr* cast_to_ptr_type(PTR ptr) const { ShouldNotReachHere(); return NULL; }\n@@ -1558,1 +1633,1 @@\n-  virtual const TypeOopPtr* as_instance_type() const { ShouldNotReachHere(); return NULL; }\n+  virtual const TypeOopPtr* as_instance_type(bool klass_change = true) const { ShouldNotReachHere(); return NULL; }\n@@ -1582,1 +1657,10 @@\n-  ciInstanceKlass* instance_klass() const { return klass()->as_instance_klass();     }\n+  ciInstanceKlass* instance_klass() const {\n+    if (klass()->is_interface()) {\n+      return Compile::current()->env()->Object_klass();\n+    }\n+    return klass()->as_instance_klass();\n+  }\n+\n+  bool is_same_java_type_as(const TypeKlassPtr* other) const;\n+  bool is_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const;\n+  bool maybe_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const;\n@@ -1584,1 +1668,1 @@\n-  virtual bool can_be_inline_type() const { return EnableValhalla && (_klass == NULL || _klass->can_be_inline_klass(klass_is_exact())); }\n+  virtual bool can_be_inline_type() const { return (_klass == NULL || _klass->can_be_inline_klass(klass_is_exact())); }\n@@ -1591,1 +1675,1 @@\n-  virtual const TypePtr* cast_to_ptr_type(PTR ptr) const;\n+  virtual const TypeInstKlassPtr* cast_to_ptr_type(PTR ptr) const;\n@@ -1596,1 +1680,1 @@\n-  virtual const TypeOopPtr* as_instance_type() const;\n+  virtual const TypeOopPtr* as_instance_type(bool klass_change = true) const;\n@@ -1603,1 +1687,3 @@\n-  virtual const TypeKlassPtr* with_offset(intptr_t offset) const;\n+  virtual const TypeInstKlassPtr* with_offset(intptr_t offset) const;\n+\n+  bool is_interface() const { return klass()->is_interface(); }\n@@ -1606,0 +1692,1 @@\n+  virtual bool not_flatten_array() const { return !_klass->can_be_inline_klass() || (_klass->is_inlinetype() && !flatten_array()); }\n@@ -1618,0 +1705,1 @@\n+  friend class TypeInstKlassPtr;\n@@ -1627,0 +1715,3 @@\n+  virtual ciKlass* exact_klass_helper() const;\n+  virtual ciKlass* klass() const;\n+\n@@ -1638,1 +1729,0 @@\n-  virtual ciKlass* klass() const;\n@@ -1643,3 +1733,11 @@\n-  static const TypeAryKlassPtr *make(PTR ptr, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free);\n-  static const TypeAryKlassPtr *make(PTR ptr, const Type *elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free);\n-  static const TypeAryKlassPtr* make(ciKlass* klass, PTR ptr = Constant, Offset offset= Offset(0));\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free);\n+\n+  bool is_same_java_type_as(const TypeKlassPtr* other) const;\n+  bool is_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const;\n+  bool maybe_java_subtype_of_helper(const TypeKlassPtr* other, bool this_exact, bool other_exact) const;\n+\n+  bool  is_loaded() const { return (_elem->isa_klassptr() ? _elem->is_klassptr()->is_loaded() : true); }\n+\n+  static const TypeAryKlassPtr* make(PTR ptr, const Type* elem, ciKlass* k, Offset offset, bool not_flat, bool not_null_free, bool null_free);\n+  static const TypeAryKlassPtr* make(PTR ptr, ciKlass* k, Offset offset);\n+  static const TypeAryKlassPtr* make(ciKlass* klass);\n@@ -1652,1 +1750,1 @@\n-  virtual const TypePtr* cast_to_ptr_type(PTR ptr) const;\n+  virtual const TypeAryKlassPtr* cast_to_ptr_type(PTR ptr) const;\n@@ -1657,1 +1755,1 @@\n-  virtual const TypeOopPtr* as_instance_type() const;\n+  virtual const TypeOopPtr* as_instance_type(bool klass_change = true) const;\n@@ -1663,1 +1761,1 @@\n-  virtual const TypeKlassPtr* with_offset(intptr_t offset) const;\n+  virtual const TypeAryKlassPtr* with_offset(intptr_t offset) const;\n@@ -1669,1 +1767,1 @@\n-  bool is_flat()          const { return klass()->is_flat_array_klass(); }\n+  bool is_flat()          const { return klass() != NULL && klass()->is_flat_array_klass(); }\n@@ -1760,1 +1858,1 @@\n-  virtual const Type* remove_speculative() const;\n+  virtual const TypeNarrowOop* remove_speculative() const;\n@@ -2122,1 +2220,1 @@\n-  return isa_instptr() != NULL && is_instptr()->klass()->is_inlinetype();\n+  return isa_instptr() != NULL && is_instptr()->instance_klass()->is_inlinetype();\n@@ -2125,1 +2223,0 @@\n-\n@@ -2128,1 +2225,1 @@\n-  return is_instptr()->klass()->as_inline_klass();\n+  return is_instptr()->instance_klass()->as_inline_klass();\n","filename":"src\/hotspot\/share\/opto\/type.hpp","additions":147,"deletions":50,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -250,1 +250,1 @@\n-  ciInstanceKlass* iklass = vec_box->box_type()->klass()->as_instance_klass();\n+  ciInstanceKlass* iklass = vec_box->box_type()->instance_klass();\n@@ -351,1 +351,1 @@\n-  ciInstanceKlass* box_klass = box_type->klass()->as_instance_klass();\n+  ciInstanceKlass* box_klass = box_type->instance_klass();\n@@ -423,1 +423,1 @@\n-    ciInstanceKlass* from_kls = tinst->klass()->as_instance_klass();\n+    ciInstanceKlass* from_kls = tinst->instance_klass();\n","filename":"src\/hotspot\/share\/opto\/vector.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -58,1 +59,0 @@\n-#include \"runtime\/thread.hpp\"\n","filename":"src\/hotspot\/share\/precompiled\/precompiled.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -79,0 +79,1 @@\n+#include \"runtime\/arguments.hpp\"\n@@ -85,0 +86,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -104,1 +106,1 @@\n-static jint CurrentVersion = JNI_VERSION_10;\n+static jint CurrentVersion = JNI_VERSION_19;\n@@ -114,1 +116,1 @@\n-\/\/ since those macros can cause an immedate uninstrumented return.\n+\/\/ since those macros can cause an immediate uninstrumented return.\n@@ -117,1 +119,1 @@\n-\/\/ the return value must be passed to the contructor of the object, and\n+\/\/ the return value must be passed to the constructor of the object, and\n@@ -565,1 +567,3 @@\n-  thread->check_and_handle_async_exceptions();\n+  if (thread->has_async_exception_condition()) {\n+    SafepointMechanism::process_if_requested_with_exit_check(thread, true \/* check asyncs *\/);\n+  }\n@@ -2316,1 +2320,1 @@\n-    result = AllocateHeap(length + 1, mtInternal, 0, AllocFailStrategy::RETURN_NULL);\n+    result = AllocateHeap(length + 1, mtInternal, AllocFailStrategy::RETURN_NULL);\n@@ -2838,2 +2842,1 @@\n-  ret = JNI_OK;\n-  return ret;\n+  return JNI_OK;\n@@ -2857,3 +2860,1 @@\n-\n-  ret = JNI_OK;\n-  return ret;\n+  return JNI_OK;\n@@ -3256,0 +3257,10 @@\n+JNI_ENTRY(jboolean, jni_IsVirtualThread(JNIEnv* env, jobject obj))\n+  oop thread_obj = JNIHandles::resolve_external_guard(obj);\n+  if (thread_obj != NULL && thread_obj->is_a(vmClasses::BasicVirtualThread_klass())) {\n+    return JNI_TRUE;\n+  } else {\n+    return JNI_FALSE;\n+  }\n+JNI_END\n+\n+\n@@ -3541,0 +3552,3 @@\n+    \/\/ Virtual threads\n+\n+    jni_IsVirtualThread\n@@ -3615,1 +3629,1 @@\n-    event.set_thread(JFR_THREAD_ID(jt));\n+    event.set_thread(JFR_JVM_THREAD_ID(jt));\n@@ -4093,0 +4107,7 @@\n+  \/\/ No JVM TI with --enable-preview and no continuations support.\n+  if (!VMContinuations && Arguments::enable_preview() && JvmtiExport::is_jvmti_version(version)) {\n+    *penv = NULL;\n+    ret = JNI_EVERSION;\n+    return ret;\n+  }\n+\n","filename":"src\/hotspot\/share\/prims\/jni.cpp","additions":33,"deletions":12,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,0 +43,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -45,1 +46,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -62,1 +62,1 @@\n-\/\/ the code, and transtition back.  The ThreadInVMfromNative constructor\n+\/\/ the code, and transition back.  The ThreadInVMfromNative constructor\n@@ -2019,1 +2019,10 @@\n-    jobject result = UNCHECKED()->GetModule(env,clazz);\n+    jobject result = UNCHECKED()->GetModule(env, clazz);\n+    functionExit(thr);\n+    return result;\n+JNI_END\n+\n+JNI_ENTRY_CHECKED(jboolean,\n+  checked_jni_IsVirtualThread(JNIEnv *env,\n+                              jobject obj))\n+    functionEnter(thr);\n+    jboolean result = UNCHECKED()->IsVirtualThread(env, obj);\n@@ -2311,0 +2320,3 @@\n+    \/\/ Virtual threads\n+\n+    checked_jni_IsVirtualThread\n","filename":"src\/hotspot\/share\/prims\/jniCheck.cpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-#include \"prims\/jvmtiThreadState.hpp\"\n+#include \"prims\/jvmtiThreadState.inline.hpp\"\n@@ -73,0 +73,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -81,0 +82,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -88,1 +90,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threadIdentifier.hpp\"\n@@ -443,1 +445,1 @@\n-  before_exit(thread);\n+  before_exit(thread, true);\n@@ -536,2 +538,2 @@\n-JVM_ENTRY(void, JVM_InitStackTraceElementArray(JNIEnv *env, jobjectArray elements, jobject throwable))\n-  Handle exception(THREAD, JNIHandles::resolve(throwable));\n+JVM_ENTRY(void, JVM_InitStackTraceElementArray(JNIEnv *env, jobjectArray elements, jobject backtrace, jint depth))\n+  Handle backtraceh(THREAD, JNIHandles::resolve(backtrace));\n@@ -541,1 +543,1 @@\n-  java_lang_Throwable::get_stack_trace_elements(exception, stack_trace, CHECK);\n+  java_lang_Throwable::get_stack_trace_elements(depth, backtraceh, stack_trace, CHECK);\n@@ -556,2 +558,2 @@\n-                                     jint skip_frames, jint frame_count, jint start_index,\n-                                     jobjectArray frames))\n+                                     jint skip_frames, jobject contScope, jobject cont,\n+                                     jint frame_count, jint start_index, jobjectArray frames))\n@@ -563,1 +565,2 @@\n-\n+  Handle contScope_h(THREAD, JNIHandles::resolve(contScope));\n+  Handle cont_h(THREAD, JNIHandles::resolve(cont));\n@@ -575,2 +578,2 @@\n-  oop result = StackWalk::walk(stackStream_h, mode, skip_frames, frame_count,\n-                               start_index, frames_array_h, CHECK_NULL);\n+  oop result = StackWalk::walk(stackStream_h, mode, skip_frames, contScope_h, cont_h,\n+                               frame_count, start_index, frames_array_h, CHECK_NULL);\n@@ -597,1 +600,10 @@\n-                                   start_index, frames_array_h, THREAD);\n+                                  start_index, frames_array_h, THREAD);\n+JVM_END\n+\n+JVM_ENTRY(void, JVM_SetStackWalkContinuation(JNIEnv *env, jobject stackStream, jlong anchor, jobjectArray frames, jobject cont))\n+  objArrayOop fa = objArrayOop(JNIHandles::resolve_non_null(frames));\n+  objArrayHandle frames_array_h(THREAD, fa);\n+  Handle stackStream_h(THREAD, JNIHandles::resolve_non_null(stackStream));\n+  Handle cont_h(THREAD, JNIHandles::resolve_non_null(cont));\n+\n+  StackWalk::setContinuation(stackStream_h, anchor, frames_array_h, cont_h, THREAD);\n@@ -720,0 +732,6 @@\n+\/\/ jdk.internal.vm.Continuation \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+JVM_ENTRY(void, JVM_RegisterContinuationMethods(JNIEnv *env, jclass cls))\n+  CONT_RegisterNativeMethods(env, cls);\n+JVM_END\n+\n@@ -1518,1 +1536,1 @@\n-  \/\/ Return null for arrays and primatives\n+  \/\/ Return null for arrays and primitives\n@@ -3027,8 +3045,1 @@\n-#if INCLUDE_JFR\n-  if (Jfr::is_recording() && EventThreadStart::is_enabled() &&\n-      EventThreadStart::is_stacktrace_enabled()) {\n-    JfrThreadLocal* tl = native_thread->jfr_thread_local();\n-    \/\/ skip Thread.start() and Thread.start0()\n-    tl->set_cached_stack_trace_id(JfrStackTraceRepository::record(thread, 2));\n-  }\n-#endif\n+  JFR_ONLY(Jfr::on_java_thread_start(thread, native_thread);)\n@@ -3132,7 +3143,0 @@\n-static void post_thread_sleep_event(EventThreadSleep* event, jlong millis) {\n-  assert(event != NULL, \"invariant\");\n-  assert(event->should_commit(), \"invariant\");\n-  event->set_time(millis);\n-  event->commit();\n-}\n-\n@@ -3153,1 +3157,0 @@\n-  EventThreadSleep event;\n@@ -3164,3 +3167,0 @@\n-        if (event.should_commit()) {\n-          post_thread_sleep_event(&event, millis);\n-        }\n@@ -3176,3 +3176,0 @@\n-  if (event.should_commit()) {\n-    post_thread_sleep_event(&event, millis);\n-  }\n@@ -3182,1 +3179,1 @@\n-JVM_ENTRY(jobject, JVM_CurrentThread(JNIEnv* env, jclass threadClass))\n+JVM_ENTRY(jobject, JVM_CurrentCarrierThread(JNIEnv* env, jclass threadClass))\n@@ -3184,1 +3181,1 @@\n-  assert(jthread != NULL, \"no current thread!\");\n+  assert(jthread != NULL, \"no current carrier thread!\");\n@@ -3188,0 +3185,17 @@\n+JVM_ENTRY(jobject, JVM_CurrentThread(JNIEnv* env, jclass threadClass))\n+  oop theThread = thread->vthread();\n+  assert(theThread != (oop)NULL, \"no current thread!\");\n+  return JNIHandles::make_local(THREAD, theThread);\n+JVM_END\n+\n+JVM_ENTRY(void, JVM_SetCurrentThread(JNIEnv* env, jobject thisThread,\n+                                     jobject theThread))\n+  oop threadObj = JNIHandles::resolve(theThread);\n+  thread->set_vthread(threadObj);\n+  JFR_ONLY(Jfr::on_set_current_thread(thread, threadObj);)\n+JVM_END\n+\n+JVM_ENTRY(jlong, JVM_GetNextThreadIdOffset(JNIEnv* env, jclass threadClass))\n+  return ThreadIdentifier::unsafe_offset();\n+JVM_END\n+\n@@ -3198,1 +3212,0 @@\n-\n@@ -3209,0 +3222,4 @@\n+JVM_ENTRY(jobject, JVM_GetStackTrace(JNIEnv *env, jobject jthread))\n+  oop trace = java_lang_Thread::async_get_stack_trace(JNIHandles::resolve(jthread), THREAD);\n+  return JNIHandles::make_local(THREAD, trace);\n+JVM_END\n@@ -3233,0 +3250,18 @@\n+JVM_ENTRY(jobject, JVM_ExtentLocalCache(JNIEnv* env, jclass threadClass))\n+  oop theCache = thread->extentLocalCache();\n+  if (theCache) {\n+    arrayOop objs = arrayOop(theCache);\n+    assert(objs->length() == ExtentLocalCacheSize * 2, \"wrong length\");\n+  }\n+  return JNIHandles::make_local(THREAD, theCache);\n+JVM_END\n+\n+JVM_ENTRY(void, JVM_SetExtentLocalCache(JNIEnv* env, jclass threadClass,\n+                                       jobject theCache))\n+  arrayOop objs = arrayOop(JNIHandles::resolve(theCache));\n+  if (objs != NULL) {\n+    assert(objs->length() == ExtentLocalCacheSize * 2, \"wrong length\");\n+  }\n+  thread->set_extentLocalCache(objs);\n+JVM_END\n+\n@@ -3523,0 +3558,8 @@\n+JVM_LEAF(jboolean, JVM_IsPreviewEnabled(void))\n+  return Arguments::enable_preview() ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n+JVM_LEAF(jboolean, JVM_IsContinuationsSupported(void))\n+  return VMContinuations ? JNI_TRUE : JNI_FALSE;\n+JVM_END\n+\n@@ -3545,1 +3588,1 @@\n-  return new os::PlatformMutex();\n+  return new PlatformMutex();\n@@ -3551,1 +3594,1 @@\n-  delete ((os::PlatformMutex*) mon);\n+  delete ((PlatformMutex*) mon);\n@@ -3557,1 +3600,1 @@\n-  ((os::PlatformMutex*) mon)->lock();\n+  ((PlatformMutex*) mon)->lock();\n@@ -3564,1 +3607,1 @@\n-  ((os::PlatformMutex*) mon)->unlock();\n+  ((PlatformMutex*) mon)->unlock();\n@@ -3966,0 +4009,106 @@\n+JVM_ENTRY(void, JVM_VirtualThreadMountBegin(JNIEnv* env, jobject vthread, jboolean first_mount))\n+#if INCLUDE_JVMTI\n+  if (!DoJVMTIVirtualThreadTransitions) {\n+    assert(!JvmtiExport::can_support_virtual_threads(), \"sanity check\");\n+    return;\n+  }\n+  JvmtiVTMSTransitionDisabler::start_VTMS_transition(vthread, \/* is_mount *\/ true);\n+#else\n+  fatal(\"Should only be called with JVMTI enabled\");\n+#endif\n+JVM_END\n+\n+JVM_ENTRY(void, JVM_VirtualThreadMountEnd(JNIEnv* env, jobject vthread, jboolean first_mount))\n+#if INCLUDE_JVMTI\n+  if (!DoJVMTIVirtualThreadTransitions) {\n+    assert(!JvmtiExport::can_support_virtual_threads(), \"sanity check\");\n+    return;\n+  }\n+  oop vt = JNIHandles::resolve(vthread);\n+\n+  thread->rebind_to_jvmti_thread_state_of(vt);\n+\n+  {\n+    MutexLocker mu(JvmtiThreadState_lock);\n+    JvmtiThreadState* state = thread->jvmti_thread_state();\n+    if (state != NULL && state->is_pending_interp_only_mode()) {\n+      JvmtiEventController::enter_interp_only_mode();\n+    }\n+  }\n+  assert(thread->is_in_VTMS_transition(), \"sanity check\");\n+  JvmtiVTMSTransitionDisabler::finish_VTMS_transition(vthread, \/* is_mount *\/ true);\n+  if (first_mount) {\n+    \/\/ thread start\n+    if (JvmtiExport::can_support_virtual_threads()) {\n+      JvmtiEventController::thread_started(thread);\n+      if (JvmtiExport::should_post_vthread_start()) {\n+        JvmtiExport::post_vthread_start(vthread);\n+      }\n+    } else { \/\/ compatibility for vthread unaware agents: legacy thread_start\n+      if (PostVirtualThreadCompatibleLifecycleEvents &&\n+          JvmtiExport::should_post_thread_life()) {\n+        \/\/ JvmtiEventController::thread_started is called here\n+        JvmtiExport::post_thread_start(thread);\n+      }\n+    }\n+  }\n+  if (JvmtiExport::should_post_vthread_mount()) {\n+    JvmtiExport::post_vthread_mount(vthread);\n+  }\n+#else\n+  fatal(\"Should only be called with JVMTI enabled\");\n+#endif\n+JVM_END\n+\n+JVM_ENTRY(void, JVM_VirtualThreadUnmountBegin(JNIEnv* env, jobject vthread, jboolean last_unmount))\n+#if INCLUDE_JVMTI\n+  if (!DoJVMTIVirtualThreadTransitions) {\n+    assert(!JvmtiExport::can_support_virtual_threads(), \"sanity check\");\n+    return;\n+  }\n+  HandleMark hm(thread);\n+  Handle ct(thread, thread->threadObj());\n+\n+  if (JvmtiExport::should_post_vthread_unmount()) {\n+    JvmtiExport::post_vthread_unmount(vthread);\n+  }\n+  if (last_unmount) {\n+    if (JvmtiExport::can_support_virtual_threads()) {\n+      if (JvmtiExport::should_post_vthread_end()) {\n+        JvmtiExport::post_vthread_end(vthread);\n+      }\n+    } else { \/\/ compatibility for vthread unaware agents: legacy thread_end\n+      if (PostVirtualThreadCompatibleLifecycleEvents &&\n+          JvmtiExport::should_post_thread_life()) {\n+        JvmtiExport::post_thread_end(thread);\n+      }\n+    }\n+  }\n+\n+  assert(!thread->is_in_VTMS_transition(), \"sanity check\");\n+  JvmtiVTMSTransitionDisabler::start_VTMS_transition(vthread, \/* is_mount *\/ false);\n+\n+  if (last_unmount && thread->jvmti_thread_state() != NULL) {\n+    JvmtiExport::cleanup_thread(thread);\n+    thread->set_jvmti_thread_state(NULL);\n+    oop vt = JNIHandles::resolve(vthread);\n+    java_lang_Thread::set_jvmti_thread_state(vt, NULL);\n+  }\n+  thread->rebind_to_jvmti_thread_state_of(ct());\n+#else\n+  fatal(\"Should only be called with JVMTI enabled\");\n+#endif\n+JVM_END\n+\n+JVM_ENTRY(void, JVM_VirtualThreadUnmountEnd(JNIEnv* env, jobject vthread, jboolean last_unmount))\n+#if INCLUDE_JVMTI\n+  if (!DoJVMTIVirtualThreadTransitions) {\n+    assert(!JvmtiExport::can_support_virtual_threads(), \"sanity check\");\n+    return;\n+  }\n+  assert(thread->is_in_VTMS_transition(), \"sanity check\");\n+  JvmtiVTMSTransitionDisabler::finish_VTMS_transition(vthread, \/* is_mount *\/ false);\n+#else\n+  fatal(\"Should only be called with JVMTI enabled\");\n+#endif\n+JVM_END\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":191,"deletions":42,"binary":false,"changes":233,"status":"modified"},{"patch":"@@ -367,1 +367,1 @@\n-\/\/ to do is add the attrubute name and fill in the length.\n+\/\/ to do is add the attribute name and fill in the length.\n","filename":"src\/hotspot\/share\/prims\/jvmtiClassFileReconstituter.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,0 +40,1 @@\n+#include \"memory\/allocation.hpp\"\n@@ -66,0 +67,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -69,0 +71,1 @@\n+#include \"runtime\/os.hpp\"\n@@ -72,1 +75,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -143,1 +146,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -146,2 +149,18 @@\n-JvmtiEnv::SetThreadLocalStorage(JavaThread* java_thread, const void* data) {\n-  JvmtiThreadState* state = java_thread->jvmti_thread_state();\n+JvmtiEnv::SetThreadLocalStorage(jthread thread, const void* data) {\n+  JavaThread* current = JavaThread::current();\n+  JvmtiThreadState* state = NULL;\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh(current);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_obj = NULL;\n+  if (thread == NULL) {\n+    java_thread = current;\n+    state = java_thread->jvmti_thread_state();\n+  } else {\n+    jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    state = java_lang_Thread::jvmti_thread_state(thread_obj);\n+  }\n@@ -154,1 +173,3 @@\n-    state = JvmtiThreadState::state_for(java_thread);\n+    HandleMark hm(current);\n+    Handle thread_handle(current, thread_obj);\n+    state = JvmtiThreadState::state_for(java_thread, thread_handle);\n@@ -184,1 +205,1 @@\n-    JavaThread* java_thread = NULL;\n+    JvmtiVTMSTransitionDisabler disabler;\n@@ -186,1 +207,4 @@\n-    jvmtiError err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), thread, &java_thread, NULL);\n+\n+    JavaThread* java_thread = NULL;\n+    oop thread_obj = NULL;\n+    jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n@@ -191,1 +215,3 @@\n-    JvmtiThreadState* state = java_thread->jvmti_thread_state();\n+    HandleMark hm(current_thread);\n+    Handle thread_handle(current_thread, thread_obj);\n+    JvmtiThreadState* state = JvmtiThreadState::state_for(java_thread, thread_handle);\n@@ -219,0 +245,1 @@\n+\n@@ -527,0 +554,1 @@\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -535,0 +563,17 @@\n+  bool enabled = (mode == JVMTI_ENABLE);\n+\n+  \/\/ event_type must be valid\n+  if (!JvmtiEventController::is_valid_event_type(event_type)) {\n+    return JVMTI_ERROR_INVALID_EVENT_TYPE;\n+  }\n+\n+  \/\/ assure that needed capabilities are present\n+  if (enabled && !JvmtiUtil::has_event_capability(event_type, get_capabilities())) {\n+    return JVMTI_ERROR_MUST_POSSESS_CAPABILITY;\n+  }\n+\n+  if (event_type == JVMTI_EVENT_CLASS_FILE_LOAD_HOOK && enabled) {\n+    record_class_file_load_hook_enabled();\n+  }\n+  JvmtiVTMSTransitionDisabler disabler;\n+\n@@ -541,17 +586,1 @@\n-    \/\/ event_type must be valid\n-    if (!JvmtiEventController::is_valid_event_type(event_type)) {\n-      return JVMTI_ERROR_INVALID_EVENT_TYPE;\n-    }\n-\n-    bool enabled = (mode == JVMTI_ENABLE);\n-\n-    \/\/ assure that needed capabilities are present\n-    if (enabled && !JvmtiUtil::has_event_capability(event_type, get_capabilities())) {\n-      return JVMTI_ERROR_MUST_POSSESS_CAPABILITY;\n-    }\n-\n-    if (event_type == JVMTI_EVENT_CLASS_FILE_LOAD_HOOK && enabled) {\n-      record_class_file_load_hook_enabled();\n-    }\n-\n-    JvmtiEventController::set_user_enabled(this, (JavaThread*) NULL, event_type, enabled);\n+    JvmtiEventController::set_user_enabled(this, (JavaThread*) NULL, (oop) NULL, event_type, enabled);\n@@ -560,2 +589,4 @@\n-    JavaThread* java_thread = NULL;\n-    jvmtiError err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), event_thread, &java_thread, NULL);\n+\n+    JavaThread* java_thread = NULL;\n+    oop thread_obj = NULL;\n+    jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), event_thread, &java_thread, &thread_obj);\n@@ -567,5 +598,0 @@\n-    \/\/ event_type must be valid\n-    if (!JvmtiEventController::is_valid_event_type(event_type)) {\n-      return JVMTI_ERROR_INVALID_EVENT_TYPE;\n-    }\n-\n@@ -577,11 +603,1 @@\n-    bool enabled = (mode == JVMTI_ENABLE);\n-\n-    \/\/ assure that needed capabilities are present\n-    if (enabled && !JvmtiUtil::has_event_capability(event_type, get_capabilities())) {\n-      return JVMTI_ERROR_MUST_POSSESS_CAPABILITY;\n-    }\n-\n-    if (event_type == JVMTI_EVENT_CLASS_FILE_LOAD_HOOK && enabled) {\n-      record_class_file_load_hook_enabled();\n-    }\n-    JvmtiEventController::set_user_enabled(this, java_thread, event_type, enabled);\n+    JvmtiEventController::set_user_enabled(this, java_thread, thread_obj, event_type, enabled);\n@@ -845,2 +861,1 @@\n-  JavaThread* java_thread = NULL;\n-  oop thread_oop = NULL;\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -849,18 +864,8 @@\n-  if (thread == NULL) {\n-    java_thread = current_thread;\n-    thread_oop = java_thread->threadObj();\n-\n-    if (thread_oop == NULL || !thread_oop->is_a(vmClasses::Thread_klass())) {\n-      return JVMTI_ERROR_INVALID_THREAD;\n-    }\n-  } else {\n-    jvmtiError err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n-    if (err != JVMTI_ERROR_NONE) {\n-      \/\/ We got an error code so we don't have a JavaThread *, but\n-      \/\/ only return an error from here if we didn't get a valid\n-      \/\/ thread_oop.\n-      if (thread_oop == NULL) {\n-        return err;\n-      }\n-      \/\/ We have a valid thread_oop so we can return some thread state.\n-    }\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+  if (err != JVMTI_ERROR_NONE && err != JVMTI_ERROR_THREAD_NOT_ALIVE) {\n+    \/\/ We got an error code so we don't have a JavaThread*, but only\n+    \/\/ return an error from here if the error is not because the thread\n+    \/\/ is a virtual thread.\n+    return err;\n@@ -869,16 +874,4 @@\n-  \/\/ get most state bits\n-  jint state = (jint)java_lang_Thread::get_thread_status(thread_oop);\n-\n-  if (java_thread != NULL) {\n-    \/\/ We have a JavaThread* so add more state bits.\n-    JavaThreadState jts = java_thread->thread_state();\n-\n-    if (java_thread->is_suspended()) {\n-      state |= JVMTI_THREAD_STATE_SUSPENDED;\n-    }\n-    if (jts == _thread_in_native) {\n-      state |= JVMTI_THREAD_STATE_IN_NATIVE;\n-    }\n-    if (java_thread->is_interrupted(false)) {\n-      state |= JVMTI_THREAD_STATE_INTERRUPTED;\n-    }\n+  if (java_lang_VirtualThread::is_instance(thread_oop)) {\n+    *thread_state_ptr = JvmtiEnvBase::get_vthread_state(thread_oop, java_thread);\n+  } else {\n+    *thread_state_ptr = JvmtiEnvBase::get_thread_state(thread_oop, java_thread);\n@@ -886,2 +879,0 @@\n-\n-  *thread_state_ptr = state;\n@@ -895,2 +886,4 @@\n-  JavaThread* current_thread  = JavaThread::current();\n-  *thread_ptr = (jthread)JNIHandles::make_local(current_thread, current_thread->threadObj());\n+  JavaThread* cur_thread = JavaThread::current();\n+  oop thread_oop = get_vthread_or_thread_oop(cur_thread);\n+\n+  *thread_ptr = (jthread)JNIHandles::make_local(cur_thread, thread_oop);\n@@ -936,1 +929,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -938,13 +931,19 @@\n-JvmtiEnv::SuspendThread(JavaThread* java_thread) {\n-  \/\/ don't allow hidden thread suspend request.\n-  if (java_thread->is_hidden_from_external_view()) {\n-    return JVMTI_ERROR_NONE;\n-  }\n-  if (java_thread->is_suspended()) {\n-    return JVMTI_ERROR_THREAD_SUSPENDED;\n-  }\n-  if (!JvmtiSuspendControl::suspend(java_thread)) {\n-    \/\/ Either the thread is already suspended or\n-    \/\/ it was in the process of exiting.\n-    if (java_thread->is_exiting()) {\n-      return JVMTI_ERROR_THREAD_NOT_ALIVE;\n+JvmtiEnv::SuspendThread(jthread thread) {\n+  JavaThread* current = JavaThread::current();\n+\n+  jvmtiError err;\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+  {\n+    JvmtiVTMSTransitionDisabler disabler(true);\n+    ThreadsListHandle tlh(current);\n+\n+    err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+\n+    \/\/ Do not use JvmtiVTMSTransitionDisabler in context of self suspend to avoid deadlocks.\n+    if (java_thread != current) {\n+      err = suspend_thread(thread_oop, java_thread, \/* single_suspend *\/ true, NULL);\n+      return err;\n@@ -952,2 +951,3 @@\n-    return JVMTI_ERROR_THREAD_SUSPENDED;\n-  return JVMTI_ERROR_NONE;\n+  \/\/ Do self suspend for current JavaThread.\n+  err = suspend_thread(thread_oop, current, \/* single_suspend *\/ true, NULL);\n+  return err;\n@@ -963,29 +963,18 @@\n-  int self_index = -1;\n-  int needSafepoint = 0;  \/\/ > 0 if we need a safepoint\n-  ThreadsListHandle tlh(current);\n-  for (int i = 0; i < request_count; i++) {\n-    JavaThread *java_thread = NULL;\n-    jvmtiError err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), request_list[i], &java_thread, NULL);\n-    if (err != JVMTI_ERROR_NONE) {\n-      results[i] = err;\n-      continue;\n-    }\n-    \/\/ don't allow hidden thread suspend request.\n-    if (java_thread->is_hidden_from_external_view()) {\n-      results[i] = JVMTI_ERROR_NONE;  \/\/ indicate successful suspend\n-      continue;\n-    }\n-    if (java_thread->is_suspended()) {\n-      results[i] = JVMTI_ERROR_THREAD_SUSPENDED;\n-      continue;\n-    }\n-    if (java_thread == current) {\n-      self_index = i;\n-      continue;\n-    }\n-    if (!JvmtiSuspendControl::suspend(java_thread)) {\n-      \/\/ Either the thread is already suspended or\n-      \/\/ it was in the process of exiting.\n-      if (java_thread->is_exiting()) {\n-        results[i] = JVMTI_ERROR_THREAD_NOT_ALIVE;\n-        continue;\n+  HandleMark hm(current);\n+  Handle self_tobj = Handle(current, NULL);\n+  int self_idx = -1;\n+\n+  {\n+    JvmtiVTMSTransitionDisabler disabler(true);\n+    ThreadsListHandle tlh(current);\n+\n+    for (int i = 0; i < request_count; i++) {\n+      JavaThread *java_thread = NULL;\n+      oop thread_oop = NULL;\n+      jthread thread = request_list[i];\n+      jvmtiError err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+\n+      if (thread_oop != NULL &&\n+          java_lang_VirtualThread::is_instance(thread_oop) &&\n+          !JvmtiEnvBase::is_vthread_alive(thread_oop)) {\n+        err = JVMTI_ERROR_THREAD_NOT_ALIVE;\n@@ -994,13 +983,5 @@\n-      results[i] = JVMTI_ERROR_THREAD_SUSPENDED;\n-      continue;\n-    }\n-    results[i] = JVMTI_ERROR_NONE;  \/\/ indicate successful suspend\n-  }\n-  if (self_index >= 0) {\n-    if (!JvmtiSuspendControl::suspend(current)) {\n-      \/\/ Either the thread is already suspended or\n-      \/\/ it was in the process of exiting.\n-      if (current->is_exiting()) {\n-        results[self_index] = JVMTI_ERROR_THREAD_NOT_ALIVE;\n-      } else {\n-        results[self_index] = JVMTI_ERROR_THREAD_SUSPENDED;\n+      if (err != JVMTI_ERROR_NONE) {\n+        if (thread_oop == NULL || err != JVMTI_ERROR_INVALID_THREAD) {\n+          results[i] = err;\n+          continue;\n+        }\n@@ -1008,2 +989,6 @@\n-    } else {\n-      results[self_index] = JVMTI_ERROR_NONE;  \/\/ indicate successful suspend\n+      if (java_thread == current) {\n+        self_idx = i;\n+        self_tobj = Handle(current, thread_oop);\n+        continue; \/\/ self suspend after all other suspends\n+      }\n+      results[i] = suspend_thread(thread_oop, java_thread, \/* single_suspend *\/ true, NULL);\n@@ -1012,0 +997,6 @@\n+  \/\/ Self suspend after all other suspends if necessary.\n+  \/\/ Do not use JvmtiVTMSTransitionDisabler in context of self suspend to avoid deadlocks.\n+  if (self_tobj() != NULL) {\n+    \/\/ there should not be any error for current java_thread\n+    results[self_idx] = suspend_thread(self_tobj(), current, \/* single_suspend *\/ true, NULL);\n+  }\n@@ -1017,5 +1008,3 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n-JvmtiEnv::ResumeThread(JavaThread* java_thread) {\n-  \/\/ don't allow hidden thread resume request.\n-  if (java_thread->is_hidden_from_external_view()) {\n-    return JVMTI_ERROR_NONE;\n+JvmtiEnv::SuspendAllVirtualThreads(jint except_count, const jthread* except_list) {\n+  if (!JvmtiExport::can_support_virtual_threads()) {\n+    return JVMTI_ERROR_MUST_POSSESS_CAPABILITY;\n@@ -1024,2 +1013,2 @@\n-  if (!java_thread->is_suspended()) {\n-    return JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n+  if (!Continuations::enabled()) {\n+    return JVMTI_ERROR_NONE; \/\/ Nothing to do when there are no virtual threads;\n@@ -1027,2 +1016,58 @@\n-  if (!JvmtiSuspendControl::resume(java_thread)) {\n-    return JVMTI_ERROR_INTERNAL;\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle self_tobj = Handle(current, NULL);\n+\n+  {\n+    ResourceMark rm(current);\n+    JvmtiVTMSTransitionDisabler disabler(true);\n+    ThreadsListHandle tlh(current);\n+    GrowableArray<jthread>* elist = new GrowableArray<jthread>(except_count);\n+\n+    jvmtiError err = JvmtiEnvBase::check_thread_list(except_count, except_list);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+\n+    \/\/ Collect threads from except_list for which resumed status must be restored.\n+    for (int idx = 0; idx < except_count; idx++) {\n+      jthread thread = except_list[idx];\n+      oop thread_oop = JNIHandles::resolve_external_guard(thread);\n+      if (!JvmtiVTSuspender::is_vthread_suspended(thread_oop)) {\n+          \/\/ is not suspended, so its resumed status must be restored\n+          elist->append(except_list[idx]);\n+      }\n+    }\n+\n+    for (JavaThreadIteratorWithHandle jtiwh; JavaThread *java_thread = jtiwh.next(); ) {\n+      oop vt_oop = java_thread->jvmti_vthread();\n+      if (!java_thread->is_exiting() &&\n+          !java_thread->is_jvmti_agent_thread() &&\n+          !java_thread->is_hidden_from_external_view() &&\n+          vt_oop != NULL &&\n+          java_lang_VirtualThread::is_instance(vt_oop) &&\n+          JvmtiEnvBase::is_vthread_alive(vt_oop) &&\n+          !JvmtiVTSuspender::is_vthread_suspended(vt_oop) &&\n+          !is_in_thread_list(except_count, except_list, vt_oop)\n+         ) {\n+        if (java_thread == current) {\n+          self_tobj = Handle(current, vt_oop);\n+          continue; \/\/ self suspend after all other suspends\n+        }\n+        suspend_thread(vt_oop, java_thread, \/* single_suspend *\/ false, NULL);\n+      }\n+    }\n+    JvmtiVTSuspender::register_all_vthreads_suspend();\n+\n+    \/\/ Restore resumed state for threads from except list that were not suspended before.\n+    for (int idx = 0; idx < elist->length(); idx++) {\n+      jthread thread = elist->at(idx);\n+      oop thread_oop = JNIHandles::resolve_external_guard(thread);\n+      if (JvmtiVTSuspender::is_vthread_suspended(thread_oop)) {\n+        JvmtiVTSuspender::register_vthread_resume(thread_oop);\n+      }\n+    }\n+  }\n+  \/\/ Self suspend after all other suspends if necessary.\n+  \/\/ Do not use JvmtiVTMSTransitionDisabler in context of self suspend to avoid deadlocks.\n+  if (self_tobj() != NULL) {\n+    suspend_thread(self_tobj(), current, \/* single_suspend *\/ false, NULL);\n@@ -1031,0 +1076,17 @@\n+} \/* end SuspendAllVirtualThreads *\/\n+\n+\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n+jvmtiError\n+JvmtiEnv::ResumeThread(jthread thread) {\n+  JvmtiVTMSTransitionDisabler disabler(true);\n+  ThreadsListHandle tlh;\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+  err = resume_thread(thread_oop, java_thread, \/* single_resume *\/ true);\n+  return err;\n@@ -1039,0 +1101,3 @@\n+  oop thread_oop = NULL;\n+  JavaThread* java_thread = NULL;\n+  JvmtiVTMSTransitionDisabler disabler(true);\n@@ -1040,0 +1105,1 @@\n+\n@@ -1041,5 +1107,7 @@\n-    JavaThread* java_thread = NULL;\n-    jvmtiError err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), request_list[i], &java_thread, NULL);\n-    if (err != JVMTI_ERROR_NONE) {\n-      results[i] = err;\n-      continue;\n+    jthread thread = request_list[i];\n+    jvmtiError err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+\n+    if (thread_oop != NULL &&\n+        java_lang_VirtualThread::is_instance(thread_oop) &&\n+        !JvmtiEnvBase::is_vthread_alive(thread_oop)) {\n+      err = JVMTI_ERROR_THREAD_NOT_ALIVE;\n@@ -1047,4 +1115,5 @@\n-    \/\/ don't allow hidden thread resume request.\n-    if (java_thread->is_hidden_from_external_view()) {\n-      results[i] = JVMTI_ERROR_NONE;  \/\/ indicate successful resume\n-      continue;\n+    if (err != JVMTI_ERROR_NONE) {\n+      if (thread_oop == NULL || err != JVMTI_ERROR_INVALID_THREAD) {\n+        results[i] = err;\n+        continue;\n+      }\n@@ -1052,3 +1121,30 @@\n-    if (!java_thread->is_suspended()) {\n-      results[i] = JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n-      continue;\n+    results[i] = resume_thread(thread_oop, java_thread, \/* single_resume *\/ true);\n+  }\n+  \/\/ per-thread resume results returned via results parameter\n+  return JVMTI_ERROR_NONE;\n+} \/* end ResumeThreadList *\/\n+\n+\n+jvmtiError\n+JvmtiEnv::ResumeAllVirtualThreads(jint except_count, const jthread* except_list) {\n+  if (!JvmtiExport::can_support_virtual_threads()) {\n+    return JVMTI_ERROR_MUST_POSSESS_CAPABILITY;\n+  }\n+  if (!Continuations::enabled()) {\n+    return JVMTI_ERROR_NONE; \/\/ Nothing to do when there are no virtual threads;\n+  }\n+  jvmtiError err = JvmtiEnvBase::check_thread_list(except_count, except_list);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+  ResourceMark rm;\n+  JvmtiVTMSTransitionDisabler disabler(true);\n+  GrowableArray<jthread>* elist = new GrowableArray<jthread>(except_count);\n+\n+  \/\/ Collect threads from except_list for which suspended status must be restored.\n+  for (int idx = 0; idx < except_count; idx++) {\n+    jthread thread = except_list[idx];\n+    oop thread_oop = JNIHandles::resolve_external_guard(thread);\n+    if (JvmtiVTSuspender::is_vthread_suspended(thread_oop)) {\n+      \/\/ is suspended, so its suspended status must be restored\n+      elist->append(except_list[idx]);\n@@ -1056,0 +1152,1 @@\n+  }\n@@ -1057,3 +1154,12 @@\n-    if (!JvmtiSuspendControl::resume(java_thread)) {\n-      results[i] = JVMTI_ERROR_INTERNAL;\n-      continue;\n+  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *java_thread = jtiwh.next(); ) {\n+    oop vt_oop = java_thread->jvmti_vthread();\n+    if (!java_thread->is_exiting() &&\n+        !java_thread->is_jvmti_agent_thread() &&\n+        !java_thread->is_hidden_from_external_view() &&\n+        vt_oop != NULL &&\n+        java_lang_VirtualThread::is_instance(vt_oop) &&\n+        JvmtiEnvBase::is_vthread_alive(vt_oop) &&\n+        JvmtiVTSuspender::is_vthread_suspended(vt_oop) &&\n+        !is_in_thread_list(except_count, except_list, vt_oop)\n+    ) {\n+      resume_thread(vt_oop, java_thread, \/* single_resume *\/ false);\n@@ -1061,0 +1167,2 @@\n+  }\n+  JvmtiVTSuspender::register_all_vthreads_resume();\n@@ -1062,1 +1170,7 @@\n-    results[i] = JVMTI_ERROR_NONE;  \/\/ indicate successful resume\n+  \/\/ Restore suspended state for threads from except list that were suspended before.\n+  for (int idx = 0; idx < elist->length(); idx++) {\n+    jthread thread = elist->at(idx);\n+    oop thread_oop = JNIHandles::resolve_external_guard(thread);\n+    if (!JvmtiVTSuspender::is_vthread_suspended(thread_oop)) {\n+      JvmtiVTSuspender::register_vthread_suspend(thread_oop);\n+    }\n@@ -1064,2 +1178,1 @@\n-  \/\/ per-thread resume results returned via results parameter\n-} \/* end ResumeThreadList *\/\n+} \/* end ResumeAllVirtualThreads *\/\n@@ -1069,2 +1182,17 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n-JvmtiEnv::StopThread(JavaThread* java_thread, jobject exception) {\n+JvmtiEnv::StopThread(jthread thread, jobject exception) {\n+  JavaThread* current_thread = JavaThread::current();\n+  ThreadsListHandle tlh(current_thread);\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+\n+  NULL_CHECK(thread, JVMTI_ERROR_INVALID_THREAD);\n+\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+\n+  if (thread_oop != NULL && java_lang_VirtualThread::is_instance(thread_oop)) {\n+    \/\/ No support for virtual threads (yet).\n+    return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n+  }\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n@@ -1086,1 +1214,3 @@\n-  JavaThread* java_thread = NULL;\n+  HandleMark hm(current_thread);\n+\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -1088,1 +1218,4 @@\n-  jvmtiError err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), thread, &java_thread, NULL);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_obj = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n@@ -1092,0 +1225,15 @@\n+\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    \/\/ For virtual threads we have to call into Java to interrupt:\n+    Handle obj(current_thread, thread_obj);\n+    JavaValue result(T_VOID);\n+    JavaCalls::call_virtual(&result,\n+                            obj,\n+                            vmClasses::Thread_klass(),\n+                            vmSymbols::interrupt_method_name(),\n+                            vmSymbols::void_method_signature(),\n+                            current_thread);\n+\n+    return JVMTI_ERROR_NONE;\n+  }\n+\n@@ -1096,1 +1244,1 @@\n-  java_lang_Thread::set_interrupted(JNIHandles::resolve(thread), true);\n+  java_lang_Thread::set_interrupted(thread_obj, true);\n@@ -1110,0 +1258,2 @@\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n@@ -1111,0 +1261,1 @@\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -1114,2 +1265,2 @@\n-  oop thread_oop = NULL;\n-    thread_oop = current_thread->threadObj();\n+    java_thread = JavaThread::current();\n+    thread_oop = get_vthread_or_thread_oop(java_thread);\n@@ -1121,1 +1272,0 @@\n-    JavaThread* java_thread = NULL;\n@@ -1127,0 +1277,2 @@\n+      \/\/ In the virtual thread case the cv_external_thread_to_JavaThread is expected to correctly set\n+      \/\/ the thread_oop and return JVMTI_ERROR_INVALID_THREAD which we ignore here.\n@@ -1130,1 +1282,0 @@\n-      \/\/ We have a valid thread_oop so we can return some thread info.\n@@ -1133,0 +1284,1 @@\n+  \/\/ We have a valid thread_oop so we can return some thread info.\n@@ -1142,3 +1294,18 @@\n-  priority = java_lang_Thread::priority(thread_obj());\n-  thread_group = Handle(current_thread, java_lang_Thread::threadGroup(thread_obj()));\n-  is_daemon = java_lang_Thread::is_daemon(thread_obj());\n+\n+  if (java_lang_VirtualThread::is_instance(thread_obj())) {\n+    priority = (ThreadPriority)JVMTI_THREAD_NORM_PRIORITY;\n+    is_daemon = true;\n+    if (java_lang_VirtualThread::state(thread_obj()) == java_lang_VirtualThread::TERMINATED) {\n+      thread_group = Handle(current_thread, NULL);\n+    } else {\n+      thread_group = Handle(current_thread, java_lang_Thread_Constants::get_VTHREAD_GROUP());\n+    }\n+  } else {\n+    priority = java_lang_Thread::priority(thread_obj());\n+    is_daemon = java_lang_Thread::is_daemon(thread_obj());\n+    if (java_lang_Thread::get_thread_status(thread_obj()) == JavaThreadStatus::TERMINATED) {\n+      thread_group = Handle(current_thread, NULL);\n+    } else {\n+      thread_group = Handle(current_thread, java_lang_Thread::threadGroup(thread_obj()));\n+    }\n+  }\n@@ -1147,0 +1314,7 @@\n+  if (loader != NULL) {\n+    \/\/ Do the same as Thread.getContextClassLoader and set context_class_loader to be\n+    \/\/ the system class loader when the field value is the \"not supported\" placeholder.\n+    if (loader == java_lang_Thread_Constants::get_NOT_SUPPORTED_CLASSLOADER()) {\n+      loader = SystemDictionary::java_system_loader();\n+    }\n+  }\n@@ -1168,1 +1342,1 @@\n-                                     jni_reference(context_class_loader);\n+                                    jni_reference(context_class_loader);\n@@ -1175,1 +1349,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1179,2 +1353,1 @@\n-JvmtiEnv::GetOwnedMonitorInfo(JavaThread* java_thread, jint* owned_monitor_count_ptr, jobject** owned_monitors_ptr) {\n-  jvmtiError err = JVMTI_ERROR_NONE;\n+JvmtiEnv::GetOwnedMonitorInfo(jthread thread, jint* owned_monitor_count_ptr, jobject** owned_monitors_ptr) {\n@@ -1182,5 +1355,1 @@\n-\n-  EscapeBarrier eb(true, calling_thread, java_thread);\n-  if (!eb.deoptimize_objects(MaxJavaStackTraceDepth)) {\n-    return JVMTI_ERROR_OUT_OF_MEMORY;\n-  }\n+  HandleMark hm(calling_thread);\n@@ -1192,4 +1361,20 @@\n-  \/\/ It is only safe to perform the direct operation on the current\n-  \/\/ thread. All other usage needs to use a direct handshake for safety.\n-  if (java_thread == calling_thread) {\n-    err = get_owned_monitors(calling_thread, java_thread, owned_monitors_list);\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh(calling_thread);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+  if (err != JVMTI_ERROR_NONE) {\n+    delete owned_monitors_list;\n+    return err;\n+  }\n+\n+  if (java_lang_VirtualThread::is_instance(thread_oop)) {\n+    \/\/ There is no monitor info to collect if target virtual thread is unmounted.\n+    if (java_thread != NULL) {\n+      VirtualThreadGetOwnedMonitorInfoClosure op(this,\n+                                                 Handle(calling_thread, thread_oop),\n+                                                 owned_monitors_list);\n+      Handshake::execute(&op, java_thread);\n+      err = op.result();\n+    }\n@@ -1197,4 +1382,16 @@\n-    \/\/ get owned monitors info with handshake\n-    GetOwnedMonitorInfoClosure op(calling_thread, this, owned_monitors_list);\n-    Handshake::execute(&op, java_thread);\n-    err = op.result();\n+    EscapeBarrier eb(true, calling_thread, java_thread);\n+    if (!eb.deoptimize_objects(MaxJavaStackTraceDepth)) {\n+      delete owned_monitors_list;\n+      return JVMTI_ERROR_OUT_OF_MEMORY;\n+    }\n+\n+    if (java_thread == calling_thread) {\n+      \/\/ It is only safe to make a direct call on the current thread.\n+      \/\/ All other usage needs to use a direct handshake for safety.\n+      err = get_owned_monitors(calling_thread, java_thread, owned_monitors_list);\n+    } else {\n+      \/\/ get owned monitors info with handshake\n+      GetOwnedMonitorInfoClosure op(calling_thread, this, owned_monitors_list);\n+      Handshake::execute(&op, java_thread);\n+      err = op.result();\n+    }\n@@ -1202,0 +1399,1 @@\n+\n@@ -1224,1 +1422,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1228,2 +1426,1 @@\n-JvmtiEnv::GetOwnedMonitorStackDepthInfo(JavaThread* java_thread, jint* monitor_info_count_ptr, jvmtiMonitorStackDepthInfo** monitor_info_ptr) {\n-  jvmtiError err = JVMTI_ERROR_NONE;\n+JvmtiEnv::GetOwnedMonitorStackDepthInfo(jthread thread, jint* monitor_info_count_ptr, jvmtiMonitorStackDepthInfo** monitor_info_ptr) {\n@@ -1231,5 +1428,1 @@\n-\n-  EscapeBarrier eb(true, calling_thread, java_thread);\n-  if (!eb.deoptimize_objects(MaxJavaStackTraceDepth)) {\n-    return JVMTI_ERROR_OUT_OF_MEMORY;\n-  }\n+  HandleMark hm(calling_thread);\n@@ -1241,9 +1434,9 @@\n-  \/\/ It is only safe to perform the direct operation on the current\n-  \/\/ thread. All other usage needs to use a direct handshake for safety.\n-  if (java_thread == calling_thread) {\n-    err = get_owned_monitors(calling_thread, java_thread, owned_monitors_list);\n-  } else {\n-    \/\/ get owned monitors info with handshake\n-    GetOwnedMonitorInfoClosure op(calling_thread, this, owned_monitors_list);\n-    Handshake::execute(&op, java_thread);\n-    err = op.result();\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh(calling_thread);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+  if (err != JVMTI_ERROR_NONE) {\n+    delete owned_monitors_list;\n+    return err;\n@@ -1252,0 +1445,27 @@\n+  if (java_lang_VirtualThread::is_instance(thread_oop)) {\n+    \/\/ There is no monitor info to collect if target virtual thread is unmounted.\n+    if (java_thread != NULL) {\n+      VirtualThreadGetOwnedMonitorInfoClosure op(this,\n+                                                 Handle(calling_thread, thread_oop),\n+                                                 owned_monitors_list);\n+      Handshake::execute(&op, java_thread);\n+      err = op.result();\n+    }\n+  } else {\n+    EscapeBarrier eb(true, calling_thread, java_thread);\n+    if (!eb.deoptimize_objects(MaxJavaStackTraceDepth)) {\n+      delete owned_monitors_list;\n+      return JVMTI_ERROR_OUT_OF_MEMORY;\n+    }\n+\n+    if (java_thread == calling_thread) {\n+      \/\/ It is only safe to make a direct call on the current thread.\n+      \/\/ All other usage needs to use a direct handshake for safety.\n+      err = get_owned_monitors(calling_thread, java_thread, owned_monitors_list);\n+    } else {\n+      \/\/ get owned monitors info with handshake\n+      GetOwnedMonitorInfoClosure op(calling_thread, this, owned_monitors_list);\n+      Handshake::execute(&op, java_thread);\n+      err = op.result();\n+    }\n+  }\n@@ -1255,1 +1475,1 @@\n-                      (unsigned char**)monitor_info_ptr)) == JVMTI_ERROR_NONE) {\n+                        (unsigned char**)monitor_info_ptr)) == JVMTI_ERROR_NONE) {\n@@ -1277,1 +1497,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1280,2 +1500,1 @@\n-JvmtiEnv::GetCurrentContendedMonitor(JavaThread* java_thread, jobject* monitor_ptr) {\n-  jvmtiError err = JVMTI_ERROR_NONE;\n+JvmtiEnv::GetCurrentContendedMonitor(jthread thread, jobject* monitor_ptr) {\n@@ -1283,0 +1502,1 @@\n+  HandleMark hm(calling_thread);\n@@ -1284,2 +1504,24 @@\n-  \/\/ It is only safe to perform the direct operation on the current\n-  \/\/ thread. All other usage needs to use a direct handshake for safety.\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh(calling_thread);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+\n+  if (java_lang_VirtualThread::is_instance(thread_oop)) {\n+    \/\/ There is no monitor info to collect if target virtual thread is unmounted.\n+    if (java_thread != NULL) {\n+      GetCurrentContendedMonitorClosure op(calling_thread, this, monitor_ptr, \/* is_virtual *\/ true);\n+      Handshake::execute(&op, java_thread);\n+      err = op.result();\n+    } else {\n+      *monitor_ptr = NULL;\n+      if (!JvmtiEnvBase::is_vthread_alive(thread_oop)) {\n+        err = JVMTI_ERROR_THREAD_NOT_ALIVE;\n+      }\n+    }\n+    return err;\n+  }\n@@ -1287,1 +1529,3 @@\n-    err = get_current_contended_monitor(calling_thread, java_thread, monitor_ptr);\n+    \/\/ It is only safe to make a direct call on the current thread.\n+    \/\/ All other usage needs to use a direct handshake for safety.\n+    err = get_current_contended_monitor(calling_thread, java_thread, monitor_ptr, \/* is_virtual *\/ false);\n@@ -1290,1 +1534,1 @@\n-    GetCurrentContendedMonitorClosure op(calling_thread, this, monitor_ptr);\n+    GetCurrentContendedMonitorClosure op(calling_thread, this, monitor_ptr, \/* is_virtual *\/ false);\n@@ -1323,0 +1567,4 @@\n+  if (java_lang_VirtualThread::is_instance(thread_oop)) {\n+    \/\/ No support for virtual threads.\n+    return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n+  }\n@@ -1411,1 +1659,0 @@\n-\n@@ -1418,0 +1665,1 @@\n+  jvmtiError err;\n@@ -1424,2 +1672,2 @@\n-  int nthreads = 0;\n-  int ngroups = 0;\n+  jint nthreads = 0;\n+  jint ngroups = 0;\n@@ -1433,50 +1681,8 @@\n-  { \/\/ Cannot allow thread or group counts to change.\n-    ObjectLocker ol(group_hdl, current_thread);\n-\n-    nthreads = java_lang_ThreadGroup::nthreads(group_hdl());\n-    ngroups  = java_lang_ThreadGroup::ngroups(group_hdl());\n-\n-    if (nthreads > 0) {\n-      ThreadsListHandle tlh(current_thread);\n-      objArrayOop threads = java_lang_ThreadGroup::threads(group_hdl());\n-      assert(nthreads <= threads->length(), \"too many threads\");\n-      thread_objs = NEW_RESOURCE_ARRAY(Handle,nthreads);\n-      for (int i = 0, j = 0; i < nthreads; i++) {\n-        oop thread_obj = threads->obj_at(i);\n-        assert(thread_obj != NULL, \"thread_obj is NULL\");\n-        JavaThread *java_thread = NULL;\n-        jvmtiError err = JvmtiExport::cv_oop_to_JavaThread(tlh.list(), thread_obj, &java_thread);\n-        if (err == JVMTI_ERROR_NONE) {\n-          \/\/ Have a valid JavaThread*.\n-          if (java_thread->is_hidden_from_external_view()) {\n-            \/\/ Filter out hidden java threads.\n-            hidden_threads++;\n-            continue;\n-          }\n-        } else {\n-          \/\/ We couldn't convert thread_obj into a JavaThread*.\n-          if (err == JVMTI_ERROR_INVALID_THREAD) {\n-            \/\/ The thread_obj does not refer to a java.lang.Thread object\n-            \/\/ so skip it.\n-            hidden_threads++;\n-            continue;\n-          }\n-          \/\/ We have a valid thread_obj, but no JavaThread*; the caller\n-          \/\/ can still have limited use for the thread_obj.\n-        }\n-        thread_objs[j++] = Handle(current_thread, thread_obj);\n-      }\n-      nthreads -= hidden_threads;\n-    } \/\/ ThreadsListHandle is destroyed here.\n-\n-    if (ngroups > 0) {\n-      objArrayOop groups = java_lang_ThreadGroup::groups(group_hdl());\n-      assert(ngroups <= groups->length(), \"too many groups\");\n-      group_objs = NEW_RESOURCE_ARRAY(Handle,ngroups);\n-      for (int i = 0; i < ngroups; i++) {\n-        oop group_obj = groups->obj_at(i);\n-        assert(group_obj != NULL, \"group_obj != NULL\");\n-        group_objs[i] = Handle(current_thread, group_obj);\n-      }\n-    }\n-  } \/\/ ThreadGroup unlocked here\n+  err = get_live_threads(current_thread, group_hdl, &nthreads, &thread_objs);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+  err = get_subgroups(current_thread, group_hdl, &ngroups, &group_objs);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n@@ -1488,1 +1694,1 @@\n-  if ((nthreads > 0) && (*threads_ptr == NULL)) {\n+  if (nthreads > 0 && *threads_ptr == NULL) {\n@@ -1491,1 +1697,1 @@\n-  if ((ngroups > 0) && (*groups_ptr == NULL)) {\n+  if (ngroups > 0 && *groups_ptr == NULL) {\n@@ -1503,1 +1709,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1508,2 +1714,29 @@\n-JvmtiEnv::GetStackTrace(JavaThread* java_thread, jint start_depth, jint max_frame_count, jvmtiFrameInfo* frame_buffer, jint* count_ptr) {\n-  jvmtiError err = JVMTI_ERROR_NONE;\n+JvmtiEnv::GetStackTrace(jthread thread, jint start_depth, jint max_frame_count, jvmtiFrameInfo* frame_buffer, jint* count_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n+  HandleMark hm(current_thread);\n+\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh(current_thread);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_obj = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    if (java_thread == NULL) {  \/\/ Target virtual thread is unmounted.\n+      ResourceMark rm(current_thread);\n+\n+      VM_VirtualThreadGetStackTrace op(this, Handle(current_thread, thread_obj),\n+                                       start_depth, max_frame_count,\n+                                       frame_buffer, count_ptr);\n+      VMThread::execute(&op);\n+      return op.result();\n+    }\n+    VirtualThreadGetStackTraceClosure op(this, Handle(current_thread, thread_obj),\n+                                         start_depth, max_frame_count, frame_buffer, count_ptr);\n+    Handshake::execute(&op, java_thread);\n+    return op.result();\n+  }\n@@ -1553,0 +1786,2 @@\n+    JvmtiVTMSTransitionDisabler disabler;\n+\n@@ -1556,0 +1791,2 @@\n+\n+    jthread thread = thread_list[0];\n@@ -1557,1 +1794,2 @@\n-    err = JvmtiExport::cv_external_thread_to_JavaThread(tlh.list(), *thread_list, &java_thread, NULL);\n+    oop thread_obj = NULL;\n+    err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n@@ -1562,1 +1800,11 @@\n-    GetSingleStackTraceClosure op(this, current_thread, *thread_list, max_frame_count);\n+    if (java_lang_VirtualThread::is_instance(thread_obj) && java_thread == NULL) {\n+      \/\/ Target virtual thread is unmounted.\n+      ResourceMark rm(current_thread);\n+      MultipleStackTracesCollector collector(this, max_frame_count);\n+      collector.fill_frames(thread, java_thread, thread_obj);\n+      collector.allocate_and_fill_stacks(\/* thread_count *\/ 1);\n+      *stack_info_ptr = collector.stack_info();\n+      return collector.result();\n+    }\n+\n+    GetSingleStackTraceClosure op(this, current_thread, thread, max_frame_count);\n@@ -1581,1 +1829,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1584,2 +1832,3 @@\n-JvmtiEnv::GetFrameCount(JavaThread* java_thread, jint* count_ptr) {\n-  jvmtiError err = JVMTI_ERROR_NONE;\n+JvmtiEnv::GetFrameCount(jthread thread, jint* count_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n+  HandleMark hm(current_thread);\n@@ -1587,4 +1836,19 @@\n-  \/\/ retrieve or create JvmtiThreadState.\n-  JvmtiThreadState* state = JvmtiThreadState::state_for(java_thread);\n-  if (state == NULL) {\n-    return JVMTI_ERROR_THREAD_NOT_ALIVE;\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh(current_thread);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_obj = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    if (java_thread == NULL) {  \/\/ Target virtual thread is unmounted.\n+      VM_VirtualThreadGetFrameCount op(this, Handle(current_thread, thread_obj),  count_ptr);\n+      VMThread::execute(&op);\n+      return op.result();\n+    }\n+    VirtualThreadGetFrameCountClosure op(this, Handle(current_thread, thread_obj), count_ptr);\n+    Handshake::execute(&op, java_thread);\n+    return op.result();\n@@ -1596,1 +1860,1 @@\n-    err = get_frame_count(state, count_ptr);\n+    err = get_frame_count(java_thread, count_ptr);\n@@ -1599,1 +1863,1 @@\n-    GetFrameCountClosure op(this, state, count_ptr);\n+    GetFrameCountClosure op(this, count_ptr);\n@@ -1607,1 +1871,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1609,1 +1873,22 @@\n-JvmtiEnv::PopFrame(JavaThread* java_thread) {\n+JvmtiEnv::PopFrame(jthread thread) {\n+  JavaThread* current_thread = JavaThread::current();\n+  HandleMark hm(current_thread);\n+\n+  if (thread == NULL) {\n+    return JVMTI_ERROR_INVALID_THREAD;\n+  }\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh(current_thread);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_obj = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n+\n+  if (thread_obj != NULL && java_lang_VirtualThread::is_instance(thread_obj)) {\n+    \/\/ No support for virtual threads (yet).\n+    return JVMTI_ERROR_OPAQUE_FRAME;\n+  }\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+\n@@ -1617,1 +1902,0 @@\n-  JavaThread* current_thread = JavaThread::current();\n@@ -1635,1 +1919,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1640,2 +1924,24 @@\n-JvmtiEnv::GetFrameLocation(JavaThread* java_thread, jint depth, jmethodID* method_ptr, jlocation* location_ptr) {\n-  jvmtiError err = JVMTI_ERROR_NONE;\n+JvmtiEnv::GetFrameLocation(jthread thread, jint depth, jmethodID* method_ptr, jlocation* location_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n+  HandleMark hm(current_thread);\n+\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh(current_thread);\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_obj = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    if (java_thread == NULL) {  \/\/ Target virtual thread is unmounted.\n+      err = get_frame_location(thread_obj, depth, method_ptr, location_ptr);\n+      return err;\n+    }\n+    VirtualThreadGetFrameLocationClosure op(this, Handle(current_thread, thread_obj),\n+                                            depth, method_ptr, location_ptr);\n+    Handshake::execute(&op, java_thread);\n+    return op.result();\n+  }\n@@ -1657,1 +1963,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ Threads_lock NOT held, java_thread not protected by lock\n@@ -1660,2 +1966,16 @@\n-JvmtiEnv::NotifyFramePop(JavaThread* java_thread, jint depth) {\n-  JvmtiThreadState *state = JvmtiThreadState::state_for(java_thread);\n+JvmtiEnv::NotifyFramePop(jthread thread, jint depth) {\n+  ResourceMark rm;\n+  JvmtiVTMSTransitionDisabler disabler;\n+  ThreadsListHandle tlh;\n+\n+  JavaThread* java_thread = NULL;\n+  oop thread_obj = NULL;\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_obj);\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+\n+  JavaThread* current = JavaThread::current();\n+  HandleMark hm(current);\n+  Handle thread_handle(current, thread_obj);\n+  JvmtiThreadState *state = JvmtiThreadState::state_for(java_thread, thread_handle);\n@@ -1666,0 +1986,12 @@\n+  if (java_lang_VirtualThread::is_instance(thread_handle())) {\n+    VirtualThreadSetFramePopClosure op(this, thread_handle, state, depth);\n+    MutexLocker mu(current, JvmtiThreadState_lock);\n+    if (java_thread == NULL || java_thread == current) {\n+      \/\/ Target virtual thread is unmounted or current.\n+      op.doit(java_thread, true \/* self *\/);\n+    } else {\n+      Handshake::execute(&op, java_thread);\n+    }\n+    return op.result();\n+  }\n+\n@@ -1667,2 +1999,2 @@\n-  MutexLocker mu(JvmtiThreadState_lock);\n-  if (java_thread == JavaThread::current()) {\n+  MutexLocker mu(current, JvmtiThreadState_lock);\n+  if (java_thread == current) {\n@@ -1681,1 +2013,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1683,1 +2015,1 @@\n-JvmtiEnv::ForceEarlyReturnObject(JavaThread* java_thread, jobject value) {\n+JvmtiEnv::ForceEarlyReturnObject(jthread thread, jobject value) {\n@@ -1686,1 +2018,1 @@\n-  return force_early_return(java_thread, val, atos);\n+  return force_early_return(thread, val, atos);\n@@ -1690,1 +2022,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1692,1 +2024,1 @@\n-JvmtiEnv::ForceEarlyReturnInt(JavaThread* java_thread, jint value) {\n+JvmtiEnv::ForceEarlyReturnInt(jthread thread, jint value) {\n@@ -1695,1 +2027,1 @@\n-  return force_early_return(java_thread, val, itos);\n+  return force_early_return(thread, val, itos);\n@@ -1699,1 +2031,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1701,1 +2033,1 @@\n-JvmtiEnv::ForceEarlyReturnLong(JavaThread* java_thread, jlong value) {\n+JvmtiEnv::ForceEarlyReturnLong(jthread thread, jlong value) {\n@@ -1704,1 +2036,1 @@\n-  return force_early_return(java_thread, val, ltos);\n+  return force_early_return(thread, val, ltos);\n@@ -1708,1 +2040,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1710,1 +2042,1 @@\n-JvmtiEnv::ForceEarlyReturnFloat(JavaThread* java_thread, jfloat value) {\n+JvmtiEnv::ForceEarlyReturnFloat(jthread thread, jfloat value) {\n@@ -1713,1 +2045,1 @@\n-  return force_early_return(java_thread, val, ftos);\n+  return force_early_return(thread, val, ftos);\n@@ -1717,1 +2049,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1719,1 +2051,1 @@\n-JvmtiEnv::ForceEarlyReturnDouble(JavaThread* java_thread, jdouble value) {\n+JvmtiEnv::ForceEarlyReturnDouble(jthread thread, jdouble value) {\n@@ -1722,1 +2054,1 @@\n-  return force_early_return(java_thread, val, dtos);\n+  return force_early_return(thread, val, dtos);\n@@ -1726,1 +2058,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1728,1 +2060,1 @@\n-JvmtiEnv::ForceEarlyReturnVoid(JavaThread* java_thread) {\n+JvmtiEnv::ForceEarlyReturnVoid(jthread thread) {\n@@ -1731,1 +2063,1 @@\n-  return force_early_return(java_thread, val, vtos);\n+  return force_early_return(thread, val, vtos);\n@@ -1904,1 +2236,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1908,1 +2240,1 @@\n-JvmtiEnv::GetLocalObject(JavaThread* java_thread, jint depth, jint slot, jobject* value_ptr) {\n+JvmtiEnv::GetLocalObject(jthread thread, jint depth, jint slot, jobject* value_ptr) {\n@@ -1913,0 +2245,2 @@\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -1914,5 +2248,10 @@\n-  VM_GetOrSetLocal op(java_thread, current_thread, depth, slot);\n-  VMThread::execute(&op);\n-  jvmtiError err = op.result();\n-  if (err != JVMTI_ERROR_NONE) {\n-    return err;\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     current_thread, depth, slot);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().l;\n+    }\n@@ -1920,2 +2259,13 @@\n-    *value_ptr = op.value().l;\n-    return JVMTI_ERROR_NONE;\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, current_thread, depth, slot);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().l;\n+    }\n@@ -1923,0 +2273,1 @@\n+  return err;\n@@ -1925,1 +2276,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1929,1 +2280,1 @@\n-JvmtiEnv::GetLocalInstance(JavaThread* java_thread, jint depth, jobject* value_ptr){\n+JvmtiEnv::GetLocalInstance(jthread thread, jint depth, jobject* value_ptr){\n@@ -1934,0 +2285,2 @@\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -1935,5 +2288,10 @@\n-  VM_GetReceiver op(java_thread, current_thread, depth);\n-  VMThread::execute(&op);\n-  jvmtiError err = op.result();\n-  if (err != JVMTI_ERROR_NONE) {\n-    return err;\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetReceiver op(this, Handle(current_thread, thread_obj),\n+                                   current_thread, depth);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().l;\n+    }\n@@ -1941,2 +2299,13 @@\n-    *value_ptr = op.value().l;\n-    return JVMTI_ERROR_NONE;\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetReceiver op(java_thread, current_thread, depth);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().l;\n+    }\n@@ -1944,0 +2313,1 @@\n+  return err;\n@@ -1947,1 +2317,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1951,1 +2321,2 @@\n-JvmtiEnv::GetLocalInt(JavaThread* java_thread, jint depth, jint slot, jint* value_ptr) {\n+JvmtiEnv::GetLocalInt(jthread thread, jint depth, jint slot, jint* value_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -1954,1 +2325,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -1956,4 +2329,26 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_INT);\n-  VMThread::execute(&op);\n-  *value_ptr = op.value().i;\n-  return op.result();\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_INT);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().i;\n+    }\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_INT);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().i;\n+    }\n+  }\n+  return err;\n@@ -1963,1 +2358,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1967,1 +2362,2 @@\n-JvmtiEnv::GetLocalLong(JavaThread* java_thread, jint depth, jint slot, jlong* value_ptr) {\n+JvmtiEnv::GetLocalLong(jthread thread, jint depth, jint slot, jlong* value_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -1970,1 +2366,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -1972,4 +2370,26 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_LONG);\n-  VMThread::execute(&op);\n-  *value_ptr = op.value().j;\n-  return op.result();\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_LONG);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().j;\n+    }\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_LONG);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().j;\n+    }\n+  }\n+  return err;\n@@ -1979,1 +2399,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1983,1 +2403,2 @@\n-JvmtiEnv::GetLocalFloat(JavaThread* java_thread, jint depth, jint slot, jfloat* value_ptr) {\n+JvmtiEnv::GetLocalFloat(jthread thread, jint depth, jint slot, jfloat* value_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -1986,1 +2407,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -1988,4 +2411,26 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_FLOAT);\n-  VMThread::execute(&op);\n-  *value_ptr = op.value().f;\n-  return op.result();\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_FLOAT);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().f;\n+    }\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_FLOAT);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().f;\n+    }\n+  }\n+  return err;\n@@ -1995,1 +2440,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -1999,1 +2444,2 @@\n-JvmtiEnv::GetLocalDouble(JavaThread* java_thread, jint depth, jint slot, jdouble* value_ptr) {\n+JvmtiEnv::GetLocalDouble(jthread thread, jint depth, jint slot, jdouble* value_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -2002,1 +2448,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -2004,4 +2452,26 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_DOUBLE);\n-  VMThread::execute(&op);\n-  *value_ptr = op.value().d;\n-  return op.result();\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_DOUBLE);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().d;\n+    }\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_DOUBLE);\n+    VMThread::execute(&op);\n+    err = op.result();\n+    if (err == JVMTI_ERROR_NONE) {\n+      *value_ptr = op.value().d;\n+    }\n+  }\n+  return err;\n@@ -2011,1 +2481,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -2014,1 +2484,2 @@\n-JvmtiEnv::SetLocalObject(JavaThread* java_thread, jint depth, jint slot, jobject value) {\n+JvmtiEnv::SetLocalObject(jthread thread, jint depth, jint slot, jobject value) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -2017,1 +2488,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -2020,3 +2493,21 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_OBJECT, val);\n-  VMThread::execute(&op);\n-  return op.result();\n+\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_OBJECT, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_OBJECT, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  }\n+  return err;\n@@ -2026,1 +2517,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -2029,1 +2520,2 @@\n-JvmtiEnv::SetLocalInt(JavaThread* java_thread, jint depth, jint slot, jint value) {\n+JvmtiEnv::SetLocalInt(jthread thread, jint depth, jint slot, jint value) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -2032,1 +2524,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -2035,3 +2529,21 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_INT, val);\n-  VMThread::execute(&op);\n-  return op.result();\n+\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_INT, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_INT, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  }\n+  return err;\n@@ -2041,1 +2553,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -2044,1 +2556,2 @@\n-JvmtiEnv::SetLocalLong(JavaThread* java_thread, jint depth, jint slot, jlong value) {\n+JvmtiEnv::SetLocalLong(jthread thread, jint depth, jint slot, jlong value) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -2047,1 +2560,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -2050,3 +2565,21 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_LONG, val);\n-  VMThread::execute(&op);\n-  return op.result();\n+\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_LONG, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_LONG, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  }\n+  return err;\n@@ -2056,1 +2589,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -2059,1 +2592,2 @@\n-JvmtiEnv::SetLocalFloat(JavaThread* java_thread, jint depth, jint slot, jfloat value) {\n+JvmtiEnv::SetLocalFloat(jthread thread, jint depth, jint slot, jfloat value) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -2062,1 +2596,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -2065,3 +2601,21 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_FLOAT, val);\n-  VMThread::execute(&op);\n-  return op.result();\n+\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_FLOAT, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_FLOAT, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  }\n+  return err;\n@@ -2071,1 +2625,1 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n+\/\/ thread - NOT protected by ThreadsListHandle and NOT pre-checked\n@@ -2074,1 +2628,2 @@\n-JvmtiEnv::SetLocalDouble(JavaThread* java_thread, jint depth, jint slot, jdouble value) {\n+JvmtiEnv::SetLocalDouble(jthread thread, jint depth, jint slot, jdouble value) {\n+  JavaThread* current_thread = JavaThread::current();\n@@ -2077,1 +2632,3 @@\n-  ResourceMark rm;\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  JvmtiVTMSTransitionDisabler disabler;\n@@ -2080,3 +2637,21 @@\n-  VM_GetOrSetLocal op(java_thread, depth, slot, T_DOUBLE, val);\n-  VMThread::execute(&op);\n-  return op.result();\n+\n+  jvmtiError err = JVMTI_ERROR_NONE;\n+  oop thread_obj = JNIHandles::resolve_external_guard(thread);\n+  if (java_lang_VirtualThread::is_instance(thread_obj)) {\n+    VM_VirtualThreadGetOrSetLocal op(this, Handle(current_thread, thread_obj),\n+                                     depth, slot, T_DOUBLE, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  } else {\n+    \/\/ Support for ordinary threads\n+    ThreadsListHandle tlh(current_thread);\n+    JavaThread* java_thread = NULL;\n+    err = get_JavaThread(tlh.list(), thread, &java_thread);\n+    if (err != JVMTI_ERROR_NONE) {\n+      return err;\n+    }\n+    VM_GetOrSetLocal op(java_thread, depth, slot, T_DOUBLE, val);\n+    VMThread::execute(&op);\n+    err = op.result();\n+  }\n+  return err;\n@@ -3310,0 +3885,9 @@\n+  Thread* thread = Thread::current();\n+\n+  \/\/ Surprisingly the GetCurrentThreadCpuTime is used by non-JavaThread's.\n+  if (thread->is_Java_thread()) {\n+    if (JavaThread::cast(thread)->is_vthread_mounted()) {\n+      \/\/ No support for virtual threads (yet).\n+      return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n+    }\n+  }\n@@ -3323,1 +3907,0 @@\n-\/\/ java_thread - protected by ThreadsListHandle and pre-checked\n@@ -3326,1 +3909,17 @@\n-JvmtiEnv::GetThreadCpuTime(JavaThread* java_thread, jlong* nanos_ptr) {\n+JvmtiEnv::GetThreadCpuTime(jthread thread, jlong* nanos_ptr) {\n+  JavaThread* current_thread = JavaThread::current();\n+  ThreadsListHandle tlh(current_thread);\n+  JavaThread* java_thread = NULL;\n+  oop thread_oop = NULL;\n+\n+  jvmtiError err = get_threadOop_and_JavaThread(tlh.list(), thread, &java_thread, &thread_oop);\n+\n+  if (thread_oop != NULL && java_lang_VirtualThread::is_instance(thread_oop)) {\n+    \/\/ No support for virtual threads (yet).\n+    return JVMTI_ERROR_UNSUPPORTED_OPERATION;\n+  }\n+  if (err != JVMTI_ERROR_NONE) {\n+    return err;\n+  }\n+  NULL_CHECK(nanos_ptr, JVMTI_ERROR_NULL_POINTER);\n+\n@@ -3385,1 +3984,1 @@\n-    if (p->is_readable()) {\n+    if (p->readable()) {\n@@ -3432,2 +4031,0 @@\n-  jvmtiError err =JVMTI_ERROR_NOT_AVAILABLE;\n-\n@@ -3436,2 +4033,9 @@\n-      if (p->set_writeable_value(value_ptr)) {\n-        err =  JVMTI_ERROR_NONE;\n+      if (p->writeable()) {\n+        if (p->set_value(value_ptr, AllocFailStrategy::RETURN_NULL)) {\n+          return JVMTI_ERROR_NONE;\n+        } else {\n+          return JVMTI_ERROR_OUT_OF_MEMORY;\n+        }\n+      } else {\n+        \/\/ We found a property, but it's not writeable\n+        return JVMTI_ERROR_NOT_AVAILABLE;\n@@ -3441,1 +4045,3 @@\n-  return err;\n+\n+  \/\/ We cannot find a property of the given name\n+  return JVMTI_ERROR_NOT_AVAILABLE;\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":1013,"deletions":407,"binary":false,"changes":1420,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -61,0 +61,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -68,1 +69,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -158,1 +159,1 @@\n-    thread->frame_anchor()->make_walkable(thread);\n+    thread->frame_anchor()->make_walkable();\n@@ -183,1 +184,1 @@\n-  jthread _jt;\n+  jobject _jthread;\n@@ -188,1 +189,1 @@\n-    _jt = (jthread)(to_jobject(thread->threadObj()));\n+    _jthread = to_jobject(thread->threadObj());\n@@ -190,1 +191,1 @@\n- jthread jni_thread() { return _jt; }\n+ jthread jni_thread() { return (jthread)_jthread; }\n@@ -193,1 +194,18 @@\n-class JvmtiClassEventMark : public JvmtiThreadEventMark {\n+class JvmtiVirtualThreadEventMark : public JvmtiEventMark {\n+private:\n+  jobject _jthread;\n+\n+public:\n+  JvmtiVirtualThreadEventMark(JavaThread *thread) :\n+    JvmtiEventMark(thread) {\n+    JvmtiThreadState* state = thread->jvmti_thread_state();\n+    if (state != NULL && state->is_virtual()) {\n+      _jthread = to_jobject(thread->vthread());\n+    } else {\n+      _jthread = to_jobject(thread->threadObj());\n+    }\n+  };\n+  jthread jni_thread() { return (jthread)_jthread; }\n+};\n+\n+class JvmtiClassEventMark : public JvmtiVirtualThreadEventMark {\n@@ -199,1 +217,1 @@\n-    JvmtiThreadEventMark(thread) {\n+    JvmtiVirtualThreadEventMark(thread) {\n@@ -205,1 +223,1 @@\n-class JvmtiMethodEventMark : public JvmtiThreadEventMark {\n+class JvmtiMethodEventMark : public JvmtiVirtualThreadEventMark {\n@@ -211,1 +229,1 @@\n-    JvmtiThreadEventMark(thread),\n+    JvmtiVirtualThreadEventMark(thread),\n@@ -365,0 +383,4 @@\n+  if (Continuations::enabled()) {\n+    \/\/ Virtual threads support. There is a performance impact when VTMS transitions are enabled.\n+    java_lang_VirtualThread::set_notify_jvmti_events(true);\n+  }\n@@ -749,0 +771,4 @@\n+  if (thread_oop_p != NULL) {\n+    *thread_oop_p = NULL;\n+  }\n+\n@@ -770,0 +796,3 @@\n+    if (java_lang_VirtualThread::is_instance(thread_oop)) {\n+      return JVMTI_ERROR_INVALID_THREAD;\n+    }\n@@ -867,0 +896,1 @@\n+    assert(!_thread->is_in_VTMS_transition(), \"CFLH events are not allowed in VTMS transition\");\n@@ -1034,1 +1064,1 @@\n-  \/\/ tries to access nonexistant services.\n+  \/\/ tries to access nonexistent services.\n@@ -1095,1 +1125,1 @@\n-class JvmtiMonitorEventMark : public JvmtiThreadEventMark {\n+class JvmtiMonitorEventMark : public JvmtiVirtualThreadEventMark {\n@@ -1100,1 +1130,1 @@\n-          : JvmtiThreadEventMark(thread){\n+          : JvmtiVirtualThreadEventMark(thread){\n@@ -1157,0 +1187,4 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -1197,0 +1231,1 @@\n+bool              JvmtiExport::_can_post_frame_pop                        = false;\n@@ -1199,0 +1234,1 @@\n+bool              JvmtiExport::_can_support_virtual_threads               = false;\n@@ -1227,0 +1263,4 @@\n+bool              JvmtiExport::_should_post_vthread_start                 = false;\n+bool              JvmtiExport::_should_post_vthread_end                   = false;\n+bool              JvmtiExport::_should_post_vthread_mount                 = false;\n+bool              JvmtiExport::_should_post_vthread_unmount               = false;\n@@ -1231,0 +1271,16 @@\n+void JvmtiExport::check_vthread_and_suspend_at_safepoint(JavaThread *thread) {\n+  oop vt = thread->jvmti_vthread();\n+\n+  if (vt != NULL && java_lang_VirtualThread::is_instance(vt)) {\n+    int64_t id = java_lang_Thread::thread_id(vt);\n+\n+    ThreadBlockInVM tbivm(thread);\n+    MonitorLocker ml(JvmtiVTMSTransition_lock, Mutex::_no_safepoint_check_flag);\n+\n+    \/\/ block while vthread is externally suspended\n+    while (JvmtiVTSuspender::is_vthread_suspended(id)) {\n+      ml.wait();\n+    }\n+  }\n+}\n+\n@@ -1283,2 +1339,0 @@\n-  EVT_TRIG_TRACE(JVMTI_EVENT_CLASS_LOAD, (\"[%s] Trg Class Load triggered\",\n-                      JvmtiTrace::safe_get_thread_name(thread)));\n@@ -1289,0 +1343,4 @@\n+  assert(!thread->is_in_VTMS_transition(), \"class load events are not allowed in VTMS transition\");\n+\n+  EVT_TRIG_TRACE(JVMTI_EVENT_CLASS_LOAD, (\"[%s] Trg Class Load triggered\",\n+                      JvmtiTrace::safe_get_thread_name(thread)));\n@@ -1316,2 +1374,0 @@\n-  EVT_TRIG_TRACE(JVMTI_EVENT_CLASS_PREPARE, (\"[%s] Trg Class Prepare triggered\",\n-                      JvmtiTrace::safe_get_thread_name(thread)));\n@@ -1322,0 +1378,4 @@\n+  assert(!thread->is_in_VTMS_transition(), \"class prepare events are not allowed in VTMS transition\");\n+\n+  EVT_TRIG_TRACE(JVMTI_EVENT_CLASS_PREPARE, (\"[%s] Trg Class Prepare triggered\",\n+                      JvmtiTrace::safe_get_thread_name(thread)));\n@@ -1411,1 +1471,1 @@\n-        JvmtiThreadEventMark jem(thread);\n+        JvmtiVirtualThreadEventMark jem(thread);\n@@ -1449,1 +1509,1 @@\n-        JvmtiThreadEventMark jem(thread);\n+        JvmtiVirtualThreadEventMark jem(thread);\n@@ -1460,1 +1520,180 @@\n-void JvmtiExport::post_object_free(JvmtiEnv* env, jlong tag) {\n+\n+void JvmtiExport::post_vthread_start(jobject vthread) {\n+  if (JvmtiEnv::get_phase() < JVMTI_PHASE_PRIMORDIAL) {\n+    return;\n+  }\n+  EVT_TRIG_TRACE(JVMTI_EVENT_VIRTUAL_THREAD_START, (\"[%p] Trg Virtual Thread Start event triggered\", vthread));\n+\n+  JavaThread *cur_thread = JavaThread::current();\n+  JvmtiThreadState *state = cur_thread->jvmti_thread_state();\n+  if (state == NULL) {\n+    return;\n+  }\n+\n+  if (state->is_enabled(JVMTI_EVENT_VIRTUAL_THREAD_START)) {\n+    JvmtiEnvThreadStateIterator it(state);\n+\n+    for (JvmtiEnvThreadState* ets = it.first(); ets != NULL; ets = it.next(ets)) {\n+      JvmtiEnv *env = ets->get_env();\n+      if (env->phase() == JVMTI_PHASE_PRIMORDIAL) {\n+        continue;\n+      }\n+      if (ets->is_enabled(JVMTI_EVENT_VIRTUAL_THREAD_START)) {\n+        EVT_TRACE(JVMTI_EVENT_VIRTUAL_THREAD_START, (\"[%p] Evt Virtual Thread Start event sent\", vthread));\n+\n+        JvmtiVirtualThreadEventMark jem(cur_thread);\n+        JvmtiJavaThreadEventTransition jet(cur_thread);\n+        jvmtiEventVirtualThreadStart callback = env->callbacks()->VirtualThreadStart;\n+        if (callback != NULL) {\n+          (*callback)(env->jvmti_external(), jem.jni_env(), jem.jni_thread());\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void JvmtiExport::post_vthread_end(jobject vthread) {\n+  if (JvmtiEnv::get_phase() < JVMTI_PHASE_PRIMORDIAL) {\n+    return;\n+  }\n+  EVT_TRIG_TRACE(JVMTI_EVENT_VIRTUAL_THREAD_END, (\"[%p] Trg Virtual Thread End event triggered\", vthread));\n+\n+  JavaThread *cur_thread = JavaThread::current();\n+  JvmtiThreadState *state = cur_thread->jvmti_thread_state();\n+  if (state == NULL) {\n+    return;\n+  }\n+\n+  if (state->is_enabled(JVMTI_EVENT_VIRTUAL_THREAD_END)) {\n+    JvmtiEnvThreadStateIterator it(state);\n+\n+    for (JvmtiEnvThreadState* ets = it.first(); ets != NULL; ets = it.next(ets)) {\n+      JvmtiEnv *env = ets->get_env();\n+      if (env->phase() == JVMTI_PHASE_PRIMORDIAL) {\n+        continue;\n+      }\n+      if (ets->is_enabled(JVMTI_EVENT_VIRTUAL_THREAD_END)) {\n+        EVT_TRACE(JVMTI_EVENT_VIRTUAL_THREAD_END, (\"[%p] Evt Virtual Thread End event sent\", vthread));\n+\n+        JvmtiVirtualThreadEventMark jem(cur_thread);\n+        JvmtiJavaThreadEventTransition jet(cur_thread);\n+        jvmtiEventVirtualThreadEnd callback = env->callbacks()->VirtualThreadEnd;\n+        if (callback != NULL) {\n+          (*callback)(env->jvmti_external(), jem.jni_env(), vthread);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void JvmtiExport::post_vthread_mount(jobject vthread) {\n+  if (JvmtiEnv::get_phase() < JVMTI_PHASE_PRIMORDIAL) {\n+    return;\n+  }\n+  JavaThread *thread = JavaThread::current();\n+  HandleMark hm(thread);\n+  EVT_TRIG_TRACE(EXT_EVENT_VIRTUAL_THREAD_MOUNT, (\"[%p] Trg Virtual Thread Mount event triggered\", vthread));\n+\n+  JvmtiThreadState *state = thread->jvmti_thread_state();\n+  if (state == NULL) {\n+    return;\n+  }\n+\n+  if (state->is_enabled((jvmtiEvent)EXT_EVENT_VIRTUAL_THREAD_MOUNT)) {\n+    JvmtiEnvThreadStateIterator it(state);\n+\n+    for (JvmtiEnvThreadState* ets = it.first(); ets != NULL; ets = it.next(ets)) {\n+      JvmtiEnv *env = ets->get_env();\n+      if (env->phase() == JVMTI_PHASE_PRIMORDIAL) {\n+        continue;\n+      }\n+      if (ets->is_enabled((jvmtiEvent)EXT_EVENT_VIRTUAL_THREAD_MOUNT)) {\n+        EVT_TRACE(EXT_EVENT_VIRTUAL_THREAD_MOUNT, (\"[%p] Evt Virtual Thread Mount event sent\", vthread));\n+\n+        JvmtiVirtualThreadEventMark jem(thread);\n+        JvmtiJavaThreadEventTransition jet(thread);\n+        jvmtiExtensionEvent callback = env->ext_callbacks()->VirtualThreadMount;\n+        if (callback != NULL) {\n+          (*callback)(env->jvmti_external(), jem.jni_env(), jem.jni_thread());\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void JvmtiExport::post_vthread_unmount(jobject vthread) {\n+  if (JvmtiEnv::get_phase() < JVMTI_PHASE_PRIMORDIAL) {\n+    return;\n+  }\n+  JavaThread *thread = JavaThread::current();\n+  HandleMark hm(thread);\n+  EVT_TRIG_TRACE(EXT_EVENT_VIRTUAL_THREAD_UNMOUNT, (\"[%p] Trg Virtual Thread Unmount event triggered\", vthread));\n+\n+  JvmtiThreadState *state = thread->jvmti_thread_state();\n+  if (state == NULL) {\n+    return;\n+  }\n+\n+  if (state->is_enabled((jvmtiEvent)EXT_EVENT_VIRTUAL_THREAD_UNMOUNT)) {\n+    JvmtiEnvThreadStateIterator it(state);\n+\n+    for (JvmtiEnvThreadState* ets = it.first(); ets != NULL; ets = it.next(ets)) {\n+      JvmtiEnv *env = ets->get_env();\n+      if (env->phase() == JVMTI_PHASE_PRIMORDIAL) {\n+        continue;\n+      }\n+      if (ets->is_enabled((jvmtiEvent)EXT_EVENT_VIRTUAL_THREAD_UNMOUNT)) {\n+        EVT_TRACE(EXT_EVENT_VIRTUAL_THREAD_UNMOUNT, (\"[%p] Evt Virtual Thread Unmount event sent\", vthread));\n+\n+        JvmtiVirtualThreadEventMark jem(thread);\n+        JvmtiJavaThreadEventTransition jet(thread);\n+        jvmtiExtensionEvent callback = env->ext_callbacks()->VirtualThreadUnmount;\n+        if (callback != NULL) {\n+          (*callback)(env->jvmti_external(), jem.jni_env(), jem.jni_thread());\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void JvmtiExport::continuation_yield_cleanup(JavaThread* thread, jint continuation_frame_count) {\n+  if (JvmtiEnv::get_phase() < JVMTI_PHASE_PRIMORDIAL) {\n+    return;\n+  }\n+\n+  assert(thread == JavaThread::current(), \"must be\");\n+  JvmtiThreadState *state = thread->jvmti_thread_state();\n+  if (state == NULL) {\n+    return;\n+  }\n+  state->invalidate_cur_stack_depth();\n+\n+  \/\/ Clear frame_pop requests in frames popped by yield\n+  if (can_post_frame_pop()) {\n+    JvmtiEnvThreadStateIterator it(state);\n+    int top_frame_num = state->cur_stack_depth() + continuation_frame_count;\n+\n+    for (JvmtiEnvThreadState* ets = it.first(); ets != NULL; ets = it.next(ets)) {\n+      if (!ets->has_frame_pops()) {\n+        continue;\n+      }\n+      for (int frame_idx = 0; frame_idx < continuation_frame_count; frame_idx++) {\n+        int frame_num = top_frame_num - frame_idx;\n+\n+        if (!state->is_virtual() && ets->is_frame_pop(frame_num)) {\n+          \/\/ remove the frame's entry\n+          MutexLocker mu(JvmtiThreadState_lock);\n+          ets->clear_frame_pop(frame_num);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void JvmtiExport::post_object_free(JvmtiEnv* env, GrowableArray<jlong>* objects) {\n+  assert(objects != NULL, \"Nothing to post\");\n+\n+  JavaThread *javaThread = JavaThread::current();\n+  if (javaThread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -1466,0 +1705,2 @@\n+  JvmtiThreadEventMark jem(javaThread);\n+  JvmtiJavaThreadEventTransition jet(javaThread);\n@@ -1468,1 +1709,3 @@\n-    (*callback)(env->jvmti_external(), tag);\n+    for (int index = 0; index < objects->length(); index++) {\n+      (*callback)(env->jvmti_external(), objects->at(index));\n+    }\n@@ -1476,0 +1719,4 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -1510,5 +1757,0 @@\n-  EVT_TRIG_TRACE(JVMTI_EVENT_METHOD_ENTRY, (\"[%s] Trg Method Entry triggered %s.%s\",\n-                     JvmtiTrace::safe_get_thread_name(thread),\n-                     (mh() == NULL) ? \"NULL\" : mh()->klass_name()->as_C_string(),\n-                     (mh() == NULL) ? \"NULL\" : mh()->name()->as_C_string() ));\n-\n@@ -1520,0 +1762,7 @@\n+  if (mh->jvmti_mount_transition() || thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+  EVT_TRIG_TRACE(JVMTI_EVENT_METHOD_ENTRY, (\"[%s] Trg Method Entry triggered %s.%s\",\n+                     JvmtiTrace::safe_get_thread_name(thread),\n+                     (mh() == NULL) ? \"NULL\" : mh()->klass_name()->as_C_string(),\n+                     (mh() == NULL) ? \"NULL\" : mh()->name()->as_C_string() ));\n@@ -1597,0 +1846,4 @@\n+  if (mh->jvmti_mount_transition() || thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -1669,0 +1922,4 @@\n+  if (mh->jvmti_mount_transition() || thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -1702,0 +1959,3 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -1821,0 +2081,3 @@\n+      if (mh->jvmti_mount_transition() || thread->is_in_VTMS_transition()) {\n+        return; \/\/ no events should be posted if thread is in a VTMS transition\n+      }\n@@ -1864,0 +2127,4 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -1896,0 +2163,4 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -1942,0 +2213,4 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -1969,0 +2244,4 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -2040,0 +2319,4 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n+\n@@ -2043,1 +2326,0 @@\n-\n@@ -2076,0 +2358,3 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -2147,0 +2432,2 @@\n+  assert(!thread->is_in_VTMS_transition(), \"compiled method load events are not allowed in VTMS transition\");\n+\n@@ -2168,0 +2455,2 @@\n+  assert(!thread->is_in_VTMS_transition(), \"compiled method load events are not allowed in VTMS transition\");\n+\n@@ -2191,0 +2480,3 @@\n+\n+  assert(!thread->is_in_VTMS_transition(), \"dynamic code generated events are not allowed in VTMS transition\");\n+\n@@ -2236,0 +2528,3 @@\n+\n+  assert(!thread->is_in_VTMS_transition(), \"dynamic code generated events are not allowed in VTMS transition\");\n+\n@@ -2257,0 +2552,3 @@\n+  JavaThread* thread = JavaThread::current();\n+  assert(!thread->is_in_VTMS_transition(), \"dynamic code generated events are not allowed in VTMS transition\");\n+\n@@ -2262,1 +2560,1 @@\n-  JvmtiThreadState* state = JavaThread::current()->jvmti_thread_state();\n+  JvmtiThreadState* state = thread->jvmti_thread_state();\n@@ -2385,0 +2683,3 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -2392,1 +2693,0 @@\n-\n@@ -2416,0 +2716,3 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -2447,0 +2750,3 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -2454,1 +2760,0 @@\n-\n@@ -2479,0 +2784,3 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -2486,1 +2794,0 @@\n-\n@@ -2506,2 +2813,0 @@\n-  EVT_TRIG_TRACE(JVMTI_EVENT_VM_OBJECT_ALLOC, (\"[%s] Trg vm object alloc triggered\",\n-                      JvmtiTrace::safe_get_thread_name(thread)));\n@@ -2511,0 +2816,3 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -2513,0 +2821,3 @@\n+\n+  EVT_TRIG_TRACE(JVMTI_EVENT_VM_OBJECT_ALLOC, (\"[%s] Trg vm object alloc triggered\",\n+                      JvmtiTrace::safe_get_thread_name(thread)));\n@@ -2536,4 +2847,0 @@\n-\n-  EVT_TRIG_TRACE(JVMTI_EVENT_SAMPLED_OBJECT_ALLOC,\n-                 (\"[%s] Trg sampled object alloc triggered\",\n-                  JvmtiTrace::safe_get_thread_name(thread)));\n@@ -2543,0 +2850,3 @@\n+  if (thread->is_in_VTMS_transition()) {\n+    return; \/\/ no events should be posted if thread is in a VTMS transition\n+  }\n@@ -2546,0 +2856,3 @@\n+  EVT_TRIG_TRACE(JVMTI_EVENT_SAMPLED_OBJECT_ALLOC,\n+                 (\"[%s] Trg sampled object alloc triggered\",\n+                  JvmtiTrace::safe_get_thread_name(thread)));\n@@ -2612,1 +2925,1 @@\n-  \/\/ The abs paramter should be \"true\" or \"false\"\n+  \/\/ The abs parameter should be \"true\" or \"false\"\n@@ -2667,0 +2980,5 @@\n+\n+        \/\/ Agent_OnAttach may have used JNI\n+        if (THREAD->is_pending_jni_exception_check()) {\n+          THREAD->clear_pending_jni_exception_check();\n+        }\n@@ -2890,1 +3208,1 @@\n-JvmtiSampledObjectAllocEventCollector::JvmtiSampledObjectAllocEventCollector() {\n+void JvmtiSampledObjectAllocEventCollector::start() {\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":357,"deletions":39,"binary":false,"changes":396,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -36,1 +37,0 @@\n-#include \"runtime\/thread.hpp\"\n","filename":"src\/hotspot\/share\/prims\/jvmtiGetLoadedClasses.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,0 +44,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -49,0 +50,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -53,2 +55,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n-#include \"runtime\/vframe.hpp\"\n+#include \"runtime\/vframe.inline.hpp\"\n@@ -427,1 +428,1 @@\n-\/\/ class VM_GetOrSetLocal\n+\/\/ class VM_BaseGetOrSetLocal\n@@ -430,0 +431,1 @@\n+const jvalue VM_BaseGetOrSetLocal::_DEFAULT_VALUE = {0L};\n@@ -431,16 +433,3 @@\n-VM_GetOrSetLocal::VM_GetOrSetLocal(JavaThread* thread, jint depth, jint index, BasicType type)\n-  : _thread(thread)\n-  , _calling_thread(NULL)\n-  , _depth(depth)\n-  , _index(index)\n-  , _type(type)\n-  , _jvf(NULL)\n-  , _set(false)\n-  , _eb(false, NULL, NULL)\n-  , _result(JVMTI_ERROR_NONE)\n-{\n-}\n-\/\/ Constructor for object or non-object setter\n-VM_GetOrSetLocal::VM_GetOrSetLocal(JavaThread* thread, jint depth, jint index, BasicType type, jvalue value)\n-  : _thread(thread)\n-  , _calling_thread(NULL)\n+VM_BaseGetOrSetLocal::VM_BaseGetOrSetLocal(JavaThread* calling_thread, jint depth,\n+                                           jint index, BasicType type, jvalue value, bool set)\n+  : _calling_thread(calling_thread)\n@@ -453,16 +442,1 @@\n-  , _set(true)\n-  , _eb(type == T_OBJECT, JavaThread::current(), thread)\n-  , _result(JVMTI_ERROR_NONE)\n-{\n-}\n-\n-\/\/ Constructor for object getter\n-VM_GetOrSetLocal::VM_GetOrSetLocal(JavaThread* thread, JavaThread* calling_thread, jint depth, int index)\n-  : _thread(thread)\n-  , _calling_thread(calling_thread)\n-  , _depth(depth)\n-  , _index(index)\n-  , _type(T_OBJECT)\n-  , _jvf(NULL)\n-  , _set(false)\n-  , _eb(true, calling_thread, thread)\n+  , _set(set)\n@@ -473,29 +447,0 @@\n-vframe *VM_GetOrSetLocal::get_vframe() {\n-  if (!_thread->has_last_Java_frame()) {\n-    return NULL;\n-  }\n-  RegisterMap reg_map(_thread);\n-  vframe *vf = _thread->last_java_vframe(&reg_map);\n-  int d = 0;\n-  while ((vf != NULL) && (d < _depth)) {\n-    vf = vf->java_sender();\n-    d++;\n-  }\n-  return vf;\n-}\n-\n-javaVFrame *VM_GetOrSetLocal::get_java_vframe() {\n-  vframe* vf = get_vframe();\n-  if (vf == NULL) {\n-    _result = JVMTI_ERROR_NO_MORE_FRAMES;\n-    return NULL;\n-  }\n-  javaVFrame *jvf = (javaVFrame*)vf;\n-\n-  if (!vf->is_java_frame()) {\n-    _result = JVMTI_ERROR_OPAQUE_FRAME;\n-    return NULL;\n-  }\n-  return jvf;\n-}\n-\n@@ -507,1 +452,1 @@\n-bool VM_GetOrSetLocal::is_assignable(const char* ty_sign, Klass* klass, Thread* thread) {\n+bool VM_BaseGetOrSetLocal::is_assignable(const char* ty_sign, Klass* klass, Thread* thread) {\n@@ -546,1 +491,1 @@\n-bool VM_GetOrSetLocal::check_slot_type_lvt(javaVFrame* jvf) {\n+bool VM_BaseGetOrSetLocal::check_slot_type_lvt(javaVFrame* jvf) {\n@@ -548,0 +493,10 @@\n+  if (!method->has_localvariable_table()) {\n+    \/\/ Just to check index boundaries.\n+    jint extra_slot = (_type == T_LONG || _type == T_DOUBLE) ? 1 : 0;\n+    if (_index < 0 || _index + extra_slot >= method->max_locals()) {\n+      _result = JVMTI_ERROR_INVALID_SLOT;\n+      return false;\n+    }\n+    return true;\n+  }\n+\n@@ -610,1 +565,1 @@\n-bool VM_GetOrSetLocal::check_slot_type_no_lvt(javaVFrame* jvf) {\n+bool VM_BaseGetOrSetLocal::check_slot_type_no_lvt(javaVFrame* jvf) {\n@@ -653,2 +608,2 @@\n-void VM_GetOrSetLocal::doit() {\n-  _jvf = _jvf == NULL ? get_java_vframe() : _jvf;\n+void VM_BaseGetOrSetLocal::doit() {\n+  _jvf = get_java_vframe();\n@@ -659,0 +614,6 @@\n+  frame fr = _jvf->fr();\n+  if (_set && _depth != 0 && Continuation::is_frame_in_continuation(_jvf->thread(), fr)) {\n+    _result = JVMTI_ERROR_OPAQUE_FRAME; \/\/ deferred locals are not fully supported in continuations\n+    return;\n+  }\n+\n@@ -688,0 +649,11 @@\n+    if (fr.is_heap_frame()) { \/\/ we want this check after the check for JVMTI_ERROR_INVALID_SLOT\n+      assert(Continuation::is_frame_in_continuation(_jvf->thread(), fr), \"sanity check\");\n+      \/\/ If the topmost frame is a heap frame, then it hasn't been thawed. This can happen\n+      \/\/ if we are executing at a return barrier safepoint. The callee frame has been popped,\n+      \/\/ but the caller frame has not been thawed. We can't support a JVMTI SetLocal in the callee\n+      \/\/ frame at this point, because we aren't truly in the callee yet.\n+      \/\/ fr.is_heap_frame() is impossible if a continuation is at a single step or breakpoint.\n+      _result = JVMTI_ERROR_OPAQUE_FRAME; \/\/ deferred locals are not fully supported in continuations\n+      return;\n+    }\n+\n@@ -692,0 +664,5 @@\n+      \/\/ Continuation can't be unmounted at this point (it was checked\/reported in get_java_vframe).\n+      if (Continuation::is_frame_in_continuation(_jvf->thread(), fr)) {\n+        _result = JVMTI_ERROR_OPAQUE_FRAME; \/\/ can't deoptimize for top continuation frame\n+        return;\n+      }\n@@ -762,2 +739,1 @@\n-\n-bool VM_GetOrSetLocal::allow_nested_vm_operations() const {\n+bool VM_BaseGetOrSetLocal::allow_nested_vm_operations() const {\n@@ -768,0 +744,61 @@\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ class VM_GetOrSetLocal\n+\/\/\n+\n+\/\/ Constructor for non-object getter\n+VM_GetOrSetLocal::VM_GetOrSetLocal(JavaThread* thread, jint depth, jint index, BasicType type)\n+  : VM_BaseGetOrSetLocal((JavaThread*)NULL, depth, index, type, _DEFAULT_VALUE, false),\n+    _thread(thread),\n+    _eb(false, NULL, NULL)\n+{\n+}\n+\n+\/\/ Constructor for object or non-object setter\n+VM_GetOrSetLocal::VM_GetOrSetLocal(JavaThread* thread, jint depth, jint index, BasicType type, jvalue value)\n+  : VM_BaseGetOrSetLocal((JavaThread*)NULL, depth, index, type, value, true),\n+    _thread(thread),\n+    _eb(type == T_OBJECT, JavaThread::current(), thread)\n+{\n+}\n+\n+\/\/ Constructor for object getter\n+VM_GetOrSetLocal::VM_GetOrSetLocal(JavaThread* thread, JavaThread* calling_thread, jint depth, int index)\n+  : VM_BaseGetOrSetLocal(calling_thread, depth, index, T_OBJECT, _DEFAULT_VALUE, false),\n+    _thread(thread),\n+    _eb(true, calling_thread, thread)\n+{\n+}\n+\n+vframe *VM_GetOrSetLocal::get_vframe() {\n+  if (!_thread->has_last_Java_frame()) {\n+    return NULL;\n+  }\n+  RegisterMap reg_map(_thread,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::include);\n+  vframe *vf = JvmtiEnvBase::get_cthread_last_java_vframe(_thread, &reg_map);\n+  int d = 0;\n+  while ((vf != NULL) && (d < _depth)) {\n+    vf = vf->java_sender();\n+    d++;\n+  }\n+  return vf;\n+}\n+\n+javaVFrame *VM_GetOrSetLocal::get_java_vframe() {\n+  vframe* vf = get_vframe();\n+  if (vf == NULL) {\n+    _result = JVMTI_ERROR_NO_MORE_FRAMES;\n+    return NULL;\n+  }\n+  javaVFrame *jvf = (javaVFrame*)vf;\n+\n+  if (!vf->is_java_frame()) {\n+    _result = JVMTI_ERROR_OPAQUE_FRAME;\n+    return NULL;\n+  }\n+  return jvf;\n+}\n+\n@@ -772,1 +809,80 @@\n-\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/\n+\/\/ class VM_VirtualThreadGetOrSetLocal\n+\/\/\n+\n+\/\/ Constructor for non-object getter\n+VM_VirtualThreadGetOrSetLocal::VM_VirtualThreadGetOrSetLocal(JvmtiEnv* env, Handle vthread_h, jint depth,\n+                                                             jint index, BasicType type)\n+  : VM_BaseGetOrSetLocal((JavaThread*)NULL, depth, index, type, _DEFAULT_VALUE, false)\n+{\n+  _env = env;\n+  _vthread_h = vthread_h;\n+}\n+\n+\/\/ Constructor for object or non-object setter\n+VM_VirtualThreadGetOrSetLocal::VM_VirtualThreadGetOrSetLocal(JvmtiEnv* env, Handle vthread_h, jint depth,\n+                                                             jint index, BasicType type, jvalue value)\n+  : VM_BaseGetOrSetLocal((JavaThread*)NULL, depth, index, type, value, true)\n+{\n+  _env = env;\n+  _vthread_h = vthread_h;\n+}\n+\n+\/\/ Constructor for object getter\n+VM_VirtualThreadGetOrSetLocal::VM_VirtualThreadGetOrSetLocal(JvmtiEnv* env, Handle vthread_h, JavaThread* calling_thread,\n+                                                             jint depth, int index)\n+  : VM_BaseGetOrSetLocal(calling_thread, depth, index, T_OBJECT, _DEFAULT_VALUE, false)\n+{\n+  _env = env;\n+  _vthread_h = vthread_h;\n+}\n+\n+javaVFrame *VM_VirtualThreadGetOrSetLocal::get_java_vframe() {\n+  Thread* cur_thread = Thread::current();\n+  oop cont = java_lang_VirtualThread::continuation(_vthread_h());\n+  assert(cont != NULL, \"vthread contintuation must not be NULL\");\n+\n+  javaVFrame* jvf = NULL;\n+  JavaThread* java_thread = JvmtiEnvBase::get_JavaThread_or_null(_vthread_h());\n+  bool is_cont_mounted = (java_thread != NULL);\n+\n+  if (is_cont_mounted) {\n+    vframeStream vfs(java_thread);\n+\n+    if (!vfs.at_end()) {\n+      jvf = vfs.asJavaVFrame();\n+      jvf = JvmtiEnvBase::check_and_skip_hidden_frames(java_thread, jvf);\n+    }\n+  } else {\n+    vframeStream vfs(cont);\n+\n+    if (!vfs.at_end()) {\n+      jvf = vfs.asJavaVFrame();\n+      jvf = JvmtiEnvBase::check_and_skip_hidden_frames(_vthread_h(), jvf);\n+    }\n+  }\n+  int d = 0;\n+  while ((jvf != NULL) && (d < _depth)) {\n+    jvf = jvf->java_sender();\n+    d++;\n+  }\n+\n+  if (d < _depth || jvf == NULL) {\n+    _result = JVMTI_ERROR_NO_MORE_FRAMES;\n+    return NULL;\n+  }\n+\n+  if ((_set && !is_cont_mounted) || !jvf->is_java_frame()) {\n+    _result = JVMTI_ERROR_OPAQUE_FRAME;\n+    return NULL;\n+  }\n+  return jvf;\n+}\n+\n+VM_VirtualThreadGetReceiver::VM_VirtualThreadGetReceiver(\n+    JvmtiEnv* env, Handle vthread_h, JavaThread* caller_thread, jint depth)\n+    : VM_VirtualThreadGetOrSetLocal(env, vthread_h, caller_thread, depth, 0) {}\n+\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":189,"deletions":73,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -240,0 +240,4 @@\n+  if (log_is_enabled(Info, redefine, class, timer)) {\n+    _timer_vm_op_doit.start();\n+  }\n+\n@@ -249,0 +253,1 @@\n+      _timer_vm_op_doit.stop();\n@@ -298,0 +303,2 @@\n+\n+  _timer_vm_op_doit.stop();\n@@ -312,2 +319,1 @@\n-    julong doit_time = _timer_rsc_phase1.milliseconds() +\n-                       _timer_rsc_phase2.milliseconds();\n+    julong doit_time = _timer_vm_op_doit.milliseconds();\n@@ -1242,1 +1248,1 @@\n-\/\/ by seaching the index map. Returns zero (0) if there is no mapped\n+\/\/ by searching the index map. Returns zero (0) if there is no mapped\n@@ -1268,1 +1274,1 @@\n-\/\/ value by seaching the index map. Returns unused index (-1) if there is\n+\/\/ value by searching the index map. Returns unused index (-1) if there is\n@@ -1876,2 +1882,0 @@\n-  \/\/ Copy attributes from scratch_cp to merge_cp\n-  merge_cp->copy_fields(scratch_cp());\n@@ -1884,0 +1888,3 @@\n+    \/\/ Copy attributes from scratch_cp to merge_cp\n+    merge_cp->copy_fields(scratch_cp());\n+\n@@ -1937,0 +1944,3 @@\n+    \/\/ Copy attributes from scratch_cp to merge_cp (should be done after rewrite_cp_refs())\n+    merge_cp->copy_fields(scratch_cp());\n+\n@@ -3533,4 +3543,3 @@\n-\/\/ Change the constant pool associated with klass scratch_class to\n-\/\/ scratch_cp. If shrink is true, then scratch_cp_length elements\n-\/\/ are copied from scratch_cp to a smaller constant pool and the\n-\/\/ smaller constant pool is associated with scratch_class.\n+\/\/ Change the constant pool associated with klass scratch_class to scratch_cp.\n+\/\/ scratch_cp_length elements are copied from scratch_cp to a smaller constant pool\n+\/\/ and the smaller constant pool is associated with scratch_class.\n@@ -4398,4 +4407,0 @@\n-  \/\/ Copy the \"source file name\" attribute from new class version\n-  the_class->set_source_file_name_index(\n-    scratch_class->source_file_name_index());\n-\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.cpp","additions":19,"deletions":14,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -386,0 +386,1 @@\n+  elapsedTimer  _timer_vm_op_doit;\n","filename":"src\/hotspot\/share\/prims\/jvmtiRedefineClasses.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -55,0 +55,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -60,1 +61,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -665,1 +665,1 @@\n-  \/\/ offset from the base of a a klass metaobject.  Thus, the full dynamic\n+  \/\/ offset from the base of a klass metaobject.  Thus, the full dynamic\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -89,1 +90,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -658,1 +658,1 @@\n-\/\/ Alloc memory with pseudo call stack. The test can create psudo malloc\n+\/\/ Alloc memory with pseudo call stack. The test can create pseudo malloc\n@@ -773,1 +773,4 @@\n-    RegisterMap reg_map(thread);\n+    RegisterMap reg_map(thread,\n+                        RegisterMap::UpdateMap::include,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -1176,1 +1179,1 @@\n-    Method::build_interpreter_method_data(mh, CHECK_AND_CLEAR);\n+    Method::build_profiling_method_data(mh, CHECK_AND_CLEAR);\n@@ -2196,1 +2199,4 @@\n-      RegisterMap rmap(jt);\n+      RegisterMap rmap(jt,\n+                       RegisterMap::UpdateMap::include,\n+                       RegisterMap::ProcessFrames::include,\n+                       RegisterMap::WalkContinuation::skip);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -120,1 +120,1 @@\n-PathString *Arguments::_system_boot_class_path = NULL;\n+PathString *Arguments::_boot_class_path = NULL;\n@@ -128,3 +128,5 @@\n-bool PathString::set_value(const char *value) {\n-  if (_value != NULL) {\n-    FreeHeap(_value);\n+bool PathString::set_value(const char *value, AllocFailType alloc_failmode) {\n+  char* new_value = AllocateHeap(strlen(value)+1, mtArguments, alloc_failmode);\n+  if (new_value == NULL) {\n+    assert(alloc_failmode == AllocFailStrategy::RETURN_NULL, \"must be\");\n+    return false;\n@@ -132,6 +134,1 @@\n-  _value = AllocateHeap(strlen(value)+1, mtArguments);\n-  assert(_value != NULL, \"Unable to allocate space for new path value\");\n-    strcpy(_value, value);\n-  } else {\n-    \/\/ not able to allocate\n-    return false;\n+    FreeHeap(_value);\n@@ -140,0 +137,2 @@\n+  _value = new_value;\n+  strcpy(_value, value);\n@@ -393,1 +392,1 @@\n-  \/\/ Set up _system_boot_class_path which is not a property but\n+  \/\/ Set up _boot_class_path which is not a property but\n@@ -395,2 +394,2 @@\n-  \/\/ property. It is used to store the underlying system boot class path.\n-  _system_boot_class_path = new PathString(NULL);\n+  \/\/ property. It is used to store the underlying boot class path.\n+  _boot_class_path = new PathString(NULL);\n@@ -540,6 +539,0 @@\n-#ifdef PRODUCT\n-  { \"UseHeavyMonitors\",             JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n-#endif\n-  { \"ExtendedDTraceProbes\",         JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"UseContainerCpuShares\",        JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n-  { \"PreferContainerQuotaForCPUCount\", JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n@@ -554,3 +547,5 @@\n-  { \"FilterSpuriousWakeups\",        JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n-  { \"MinInliningThreshold\",         JDK_Version::jdk(18), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n-  { \"PrefetchFieldsAhead\",          JDK_Version::undefined(), JDK_Version::jdk(19), JDK_Version::jdk(20) },\n+  { \"ExtendedDTraceProbes\",         JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n+  { \"UseContainerCpuShares\",        JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n+  { \"PreferContainerQuotaForCPUCount\", JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n+  { \"AliasLevel\",                   JDK_Version::jdk(19), JDK_Version::jdk(20), JDK_Version::jdk(21) },\n+\n@@ -895,1 +890,5 @@\n-static bool set_fp_numeric_flag(JVMFlag* flag, char* value, JVMFlagOrigin origin) {\n+static bool set_fp_numeric_flag(JVMFlag* flag, const char* value, JVMFlagOrigin origin) {\n+  \/\/ strtod allows leading whitespace, but our flag format does not.\n+  if (*value == '\\0' || isspace(*value)) {\n+    return false;\n+  }\n@@ -902,0 +901,4 @@\n+  if (g_isnan(v) || !g_isfinite(v)) {\n+    \/\/ Currently we cannot handle these special values.\n+    return false;\n+  }\n@@ -909,4 +912,2 @@\n-static JVMFlag::Error set_numeric_flag(JVMFlag* flag, char* value, JVMFlagOrigin origin) {\n-  if (flag == NULL) {\n-    return JVMFlag::INVALID_FLAG;\n-  }\n+static bool set_numeric_flag(JVMFlag* flag, const char* value, JVMFlagOrigin origin) {\n+  JVMFlag::Error result = JVMFlag::WRONG_FORMAT;\n@@ -917,1 +918,1 @@\n-      return JVMFlagAccess::set_int(flag, &v, origin);\n+      result = JVMFlagAccess::set_int(flag, &v, origin);\n@@ -922,1 +923,1 @@\n-      return JVMFlagAccess::set_uint(flag, &v, origin);\n+      result = JVMFlagAccess::set_uint(flag, &v, origin);\n@@ -927,1 +928,1 @@\n-      return JVMFlagAccess::set_intx(flag, &v, origin);\n+      result = JVMFlagAccess::set_intx(flag, &v, origin);\n@@ -932,1 +933,1 @@\n-      return JVMFlagAccess::set_uintx(flag, &v, origin);\n+      result = JVMFlagAccess::set_uintx(flag, &v, origin);\n@@ -937,1 +938,1 @@\n-      return JVMFlagAccess::set_uint64_t(flag, &v, origin);\n+      result = JVMFlagAccess::set_uint64_t(flag, &v, origin);\n@@ -942,14 +943,1 @@\n-      return JVMFlagAccess::set_size_t(flag, &v, origin);\n-    }\n-  } else if (flag->is_double()) {\n-    \/\/ This function parses only input strings without a decimal\n-    \/\/ point character (.)\n-    \/\/ If a string looks like a FP number, it would be parsed by\n-    \/\/ set_fp_numeric_flag(). See Arguments::parse_argument().\n-    jlong v;\n-    if (parse_integer(value, &v)) {\n-      double double_v = (double) v;\n-      if (value[0] == '-' && v == 0) { \/\/ special case: 0.0 is different than -0.0.\n-        double_v = -0.0;\n-      }\n-      return JVMFlagAccess::set_double(flag, &double_v, origin);\n+      result = JVMFlagAccess::set_size_t(flag, &v, origin);\n@@ -959,1 +947,1 @@\n-  return JVMFlag::WRONG_FORMAT;\n+  return result == JVMFlag::SUCCESS;\n@@ -963,0 +951,3 @@\n+  if (value[0] == '\\0') {\n+    value = NULL;\n+  }\n@@ -996,1 +987,1 @@\n-const char* Arguments::handle_aliases_and_deprecation(const char* arg, bool warn) {\n+const char* Arguments::handle_aliases_and_deprecation(const char* arg) {\n@@ -1014,10 +1005,8 @@\n-      if (warn) {\n-        char version[256];\n-        since.to_string(version, sizeof(version));\n-        if (real_name != arg) {\n-          warning(\"Option %s was deprecated in version %s and will likely be removed in a future release. Use option %s instead.\",\n-                  arg, version, real_name);\n-        } else {\n-          warning(\"Option %s was deprecated in version %s and will likely be removed in a future release.\",\n-                  arg, version);\n-        }\n+      char version[256];\n+      since.to_string(version, sizeof(version));\n+      if (real_name != arg) {\n+        warning(\"Option %s was deprecated in version %s and will likely be removed in a future release. Use option %s instead.\",\n+                arg, version, real_name);\n+      } else {\n+        warning(\"Option %s was deprecated in version %s and will likely be removed in a future release.\",\n+                arg, version);\n@@ -1032,12 +1021,9 @@\n-bool Arguments::parse_argument(const char* arg, JVMFlagOrigin origin) {\n-\n-  \/\/ range of acceptable characters spelled out for portability reasons\n-#define NAME_RANGE  \"[abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_]\"\n-  char name[BUFLEN+1];\n-  char dummy;\n-  const char* real_name;\n-  bool warn_if_deprecated = true;\n-  if (sscanf(arg, \"-%\" XSTR(BUFLEN) NAME_RANGE \"%c\", name, &dummy) == 1) {\n-    real_name = handle_aliases_and_deprecation(name, warn_if_deprecated);\n-    if (real_name == NULL) {\n-      return false;\n+JVMFlag* Arguments::find_jvm_flag(const char* name, size_t name_length) {\n+  char name_copied[BUFLEN+1];\n+  if (name[name_length] != 0) {\n+    if (name_length > BUFLEN) {\n+      return NULL;\n+    } else {\n+      strncpy(name_copied, name, name_length);\n+      name_copied[name_length] = '\\0';\n+      name = name_copied;\n@@ -1047,6 +1033,26 @@\n-    JVMFlag* flag = JVMFlag::find_flag(real_name);\n-    return set_bool_flag(flag, false, origin);\n-  if (sscanf(arg, \"+%\" XSTR(BUFLEN) NAME_RANGE \"%c\", name, &dummy) == 1) {\n-    real_name = handle_aliases_and_deprecation(name, warn_if_deprecated);\n-    if (real_name == NULL) {\n-      return false;\n+\n+  const char* real_name = Arguments::handle_aliases_and_deprecation(name);\n+  if (real_name == NULL) {\n+    return NULL;\n+  }\n+  JVMFlag* flag = JVMFlag::find_flag(real_name);\n+  return flag;\n+}\n+\n+bool Arguments::parse_argument(const char* arg, JVMFlagOrigin origin) {\n+  bool is_bool = false;\n+  bool bool_val = false;\n+  char c = *arg;\n+  if (c == '+' || c == '-') {\n+    is_bool = true;\n+    bool_val = (c == '+');\n+    arg++;\n+  }\n+\n+  const char* name = arg;\n+  while (true) {\n+    c = *arg;\n+    if (isalnum(c) || (c == '_')) {\n+      ++arg;\n+    } else {\n+      break;\n@@ -1055,2 +1061,0 @@\n-    JVMFlag* flag = JVMFlag::find_flag(real_name);\n-    return set_bool_flag(flag, true, origin);\n@@ -1059,3 +1063,9 @@\n-  char punct;\n-  if (sscanf(arg, \"%\" XSTR(BUFLEN) NAME_RANGE \"%c\", name, &punct) == 2 && punct == '=') {\n-    const char* value = strchr(arg, '=') + 1;\n+  size_t name_len = size_t(arg - name);\n+  if (name_len == 0) {\n+    return false;\n+  }\n+\n+  JVMFlag* flag = find_jvm_flag(name, name_len);\n+  if (flag == NULL) {\n+    return false;\n+  }\n@@ -1063,3 +1073,3 @@\n-    \/\/ this scanf pattern matches both strings (handled here) and numbers (handled later))\n-    real_name = handle_aliases_and_deprecation(name, warn_if_deprecated);\n-    if (real_name == NULL) {\n+  if (is_bool) {\n+    if (*arg != 0) {\n+      \/\/ Error -- extra characters such as -XX:+BoolFlag=123\n@@ -1068,2 +1078,6 @@\n-    JVMFlag* flag = JVMFlag::find_flag(real_name);\n-    if (flag != NULL && flag->is_ccstr()) {\n+    return set_bool_flag(flag, bool_val, origin);\n+  }\n+\n+  if (arg[0] == '=') {\n+    const char* value = arg + 1;\n+    if (flag->is_ccstr()) {\n@@ -1073,3 +1087,0 @@\n-        if (value[0] == '\\0') {\n-          value = NULL;\n-        }\n@@ -1078,0 +1089,2 @@\n+    } else if (flag->is_double()) {\n+      return set_fp_numeric_flag(flag, value, origin);\n@@ -1079,1 +1092,1 @@\n-      warn_if_deprecated = false; \/\/ if arg is deprecated, we've already done warning...\n+      return set_numeric_flag(flag, value, origin);\n@@ -1083,2 +1096,1 @@\n-  if (sscanf(arg, \"%\" XSTR(BUFLEN) NAME_RANGE \":%c\", name, &punct) == 2 && punct == '=') {\n-    const char* value = strchr(arg, '=') + 1;\n+  if (arg[0] == ':' && arg[1] == '=') {\n@@ -1086,8 +1098,1 @@\n-    if (value[0] == '\\0') {\n-      value = NULL;\n-    }\n-    real_name = handle_aliases_and_deprecation(name, warn_if_deprecated);\n-    if (real_name == NULL) {\n-      return false;\n-    }\n-    JVMFlag* flag = JVMFlag::find_flag(real_name);\n+    const char* value = arg + 2;\n@@ -1097,27 +1102,0 @@\n-#define SIGNED_FP_NUMBER_RANGE \"[-0123456789.eE+]\"\n-#define SIGNED_NUMBER_RANGE    \"[-0123456789]\"\n-#define        NUMBER_RANGE    \"[0123456789eE+-]\"\n-  char value[BUFLEN + 1];\n-  char value2[BUFLEN + 1];\n-  if (sscanf(arg, \"%\" XSTR(BUFLEN) NAME_RANGE \"=\" \"%\" XSTR(BUFLEN) SIGNED_NUMBER_RANGE \".\" \"%\" XSTR(BUFLEN) NUMBER_RANGE \"%c\", name, value, value2, &dummy) == 3) {\n-    \/\/ Looks like a floating-point number -- try again with more lenient format string\n-    if (sscanf(arg, \"%\" XSTR(BUFLEN) NAME_RANGE \"=\" \"%\" XSTR(BUFLEN) SIGNED_FP_NUMBER_RANGE \"%c\", name, value, &dummy) == 2) {\n-      real_name = handle_aliases_and_deprecation(name, warn_if_deprecated);\n-      if (real_name == NULL) {\n-        return false;\n-      }\n-      JVMFlag* flag = JVMFlag::find_flag(real_name);\n-      return set_fp_numeric_flag(flag, value, origin);\n-    }\n-  }\n-\n-#define VALUE_RANGE \"[-kmgtxKMGTX0123456789abcdefABCDEF]\"\n-  if (sscanf(arg, \"%\" XSTR(BUFLEN) NAME_RANGE \"=\" \"%\" XSTR(BUFLEN) VALUE_RANGE \"%c\", name, value, &dummy) == 2) {\n-    real_name = handle_aliases_and_deprecation(name, warn_if_deprecated);\n-    if (real_name == NULL) {\n-      return false;\n-    }\n-    JVMFlag* flag = JVMFlag::find_flag(real_name);\n-    return set_numeric_flag(flag, value, origin) == JVMFlag::SUCCESS;\n-  }\n-\n@@ -1781,0 +1759,12 @@\n+    reasonable_max = limit_heap_by_allocatable_memory(reasonable_max);\n+\n+    if (!FLAG_IS_DEFAULT(InitialHeapSize)) {\n+      \/\/ An initial heap size was specified on the command line,\n+      \/\/ so be sure that the maximum size is consistent.  Done\n+      \/\/ after call to limit_heap_by_allocatable_memory because that\n+      \/\/ method might reduce the allocation size.\n+      reasonable_max = MAX2(reasonable_max, (julong)InitialHeapSize);\n+    } else if (!FLAG_IS_DEFAULT(MinHeapSize)) {\n+      reasonable_max = MAX2(reasonable_max, (julong)MinHeapSize);\n+    }\n+\n@@ -1827,12 +1817,0 @@\n-    reasonable_max = limit_heap_by_allocatable_memory(reasonable_max);\n-\n-    if (!FLAG_IS_DEFAULT(InitialHeapSize)) {\n-      \/\/ An initial heap size was specified on the command line,\n-      \/\/ so be sure that the maximum size is consistent.  Done\n-      \/\/ after call to limit_heap_by_allocatable_memory because that\n-      \/\/ method might reduce the allocation size.\n-      reasonable_max = MAX2(reasonable_max, (julong)InitialHeapSize);\n-    } else if (!FLAG_IS_DEFAULT(MinHeapSize)) {\n-      reasonable_max = MAX2(reasonable_max, (julong)MinHeapSize);\n-    }\n-\n@@ -2098,1 +2076,1 @@\n-#if !defined(X86) && !defined(AARCH64) && !defined(PPC64)\n+#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64)\n@@ -4292,1 +4270,1 @@\n-    if (pl->is_readable()) {\n+    if (pl->readable()) {\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":119,"deletions":141,"binary":false,"changes":260,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,0 +41,4 @@\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logLevel.hpp\"\n+#include \"logging\/logMessage.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -48,0 +52,1 @@\n+#include \"oops\/fieldStreams.inline.hpp\"\n@@ -52,1 +57,0 @@\n-#include \"oops\/fieldStreams.inline.hpp\"\n@@ -59,1 +63,1 @@\n-#include \"prims\/vectorSupport.hpp\"\n+#include \"prims\/vectorSupport.hpp\"\n@@ -62,0 +66,2 @@\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -69,0 +75,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -77,0 +84,1 @@\n+#include \"runtime\/stackValue.hpp\"\n@@ -79,1 +87,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -87,0 +94,1 @@\n+#include \"utilities\/growableArray.hpp\"\n@@ -139,1 +147,1 @@\n-  \/\/ Acount first for the adjustment of the initial frame\n+  \/\/ Account first for the adjustment of the initial frame\n@@ -400,2 +408,8 @@\n-  RegisterMap map(current, true);\n-  RegisterMap dummy_map(current, false);\n+  RegisterMap map(current,\n+                  RegisterMap::UpdateMap::include,\n+                  RegisterMap::ProcessFrames::include,\n+                  RegisterMap::WalkContinuation::skip);\n+  RegisterMap dummy_map(current,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -473,0 +487,5 @@\n+    \/\/ FIXME: This very crudely destroys all ExtentLocal bindings. This\n+    \/\/ is better than a bound value escaping, but far from ideal.\n+    oop java_thread = current->threadObj();\n+    current->set_extentLocalCache(NULL);\n+    java_lang_Thread::clear_extentLocalBindings(java_thread);\n@@ -616,1 +635,1 @@\n-  if (deopt_sender.is_compiled_caller() || caller_was_method_handle) {\n+  if (!deopt_sender.is_interpreted_frame() || caller_was_method_handle) {\n@@ -629,1 +648,4 @@\n-  frame_pcs[0] = deopt_sender.raw_pc();\n+  frame_pcs[0] = Continuation::is_cont_barrier_frame(deoptee) ? StubRoutines::cont_returnBarrier() : deopt_sender.raw_pc();\n+  if (Continuation::is_continuation_enterSpecial(deopt_sender)) {\n+    ContinuationEntry::from_frame(deopt_sender)->set_argsize(0);\n+  }\n@@ -770,0 +792,2 @@\n+  Continuation::notify_deopt(thread, stub_frame.sp());\n+\n@@ -808,1 +832,4 @@\n-    RegisterMap rm(thread, false);\n+    RegisterMap rm(thread,\n+                   RegisterMap::UpdateMap::skip,\n+                   RegisterMap::ProcessFrames::include,\n+                   RegisterMap::WalkContinuation::skip);\n@@ -944,0 +971,1 @@\n+    CodeCache::make_nmethod_deoptimized(nmethod_only);\n@@ -945,2 +973,1 @@\n-    MutexLocker mu(SafepointSynchronize::is_at_safepoint() ? NULL : CodeCache_lock, Mutex::_no_safepoint_check_flag);\n-    CodeCache::make_marked_nmethods_not_entrant();\n+    CodeCache::make_marked_nmethods_deoptimized();\n@@ -960,1 +987,1 @@\n-#if COMPILER2_OR_JVMCI\n+#if INCLUDE_JVMCI\n@@ -1088,0 +1115,1 @@\n+#endif \/\/ INCLUDE_JVMCI\n@@ -1089,0 +1117,1 @@\n+#if COMPILER2_OR_JVMCI\n@@ -1115,1 +1144,3 @@\n-      if (sv->is_auto_box()) {\n+#if INCLUDE_JVMCI\n+      CompiledMethod* cm = fr->cb()->as_compiled_method_or_null();\n+      if (cm->is_compiled_by_jvmci() && sv->is_auto_box()) {\n@@ -1123,0 +1154,1 @@\n+#endif \/\/ INCLUDE_JVMCI\n@@ -1522,0 +1554,1 @@\n+#if INCLUDE_JVMCI\n@@ -1526,0 +1559,1 @@\n+#endif \/\/ INCLUDE_JVMCI\n@@ -1704,0 +1738,2 @@\n+  Continuation::notify_deopt(thread, fr.sp());\n+\n@@ -1731,1 +1767,4 @@\n-  RegisterMap reg_map(thread, false);\n+  RegisterMap reg_map(thread,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1764,1 +1803,4 @@\n-  RegisterMap reg_map(thread, false);\n+  RegisterMap reg_map(thread,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1802,1 +1844,1 @@\n-    Method::build_interpreter_method_data(m, THREAD);\n+    Method::build_profiling_method_data(m, THREAD);\n@@ -1909,0 +1951,20 @@\n+static void log_deopt(CompiledMethod* nm, Method* tm, intptr_t pc, frame& fr, int trap_bci,\n+                              const char* reason_name, const char* reason_action) {\n+  LogTarget(Debug, deoptimization) lt;\n+  if (lt.is_enabled()) {\n+    LogStream ls(lt);\n+    bool is_osr = nm->is_osr_method();\n+    ls.print(\"cid=%4d %s level=%d\",\n+             nm->compile_id(), (is_osr ? \"osr\" : \"   \"), nm->comp_level());\n+    ls.print(\" %s\", tm->name_and_sig_as_C_string());\n+    ls.print(\" trap_bci=%d \", trap_bci);\n+    if (is_osr) {\n+      ls.print(\"osr_bci=%d \", nm->osr_entry_bci());\n+    }\n+    ls.print(\"%s \", reason_name);\n+    ls.print(\"%s \", reason_action);\n+    ls.print_cr(\"pc=\" INTPTR_FORMAT \" relative_pc=\" INTPTR_FORMAT,\n+             pc, fr.pc() - nm->code_begin());\n+  }\n+}\n+\n@@ -1921,1 +1983,4 @@\n-  RegisterMap reg_map(current, true);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1923,1 +1988,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -2002,6 +2070,12 @@\n-    JFR_ONLY(post_deoptimization_event(nm, trap_method(), trap_bci, trap_bc, reason, action);)\n-\n-    \/\/ Log a message\n-    Events::log_deopt_message(current, \"Uncommon trap: reason=%s action=%s pc=\" INTPTR_FORMAT \" method=%s @ %d %s\",\n-                              trap_reason_name(reason), trap_action_name(action), p2i(fr.pc()),\n-                              trap_method->name_and_sig_as_C_string(), trap_bci, nm->compiler_name());\n+    { \/\/ Log Deoptimization event for JFR, UL and event system\n+      Method* tm = trap_method();\n+      const char* reason_name = trap_reason_name(reason);\n+      const char* reason_action = trap_action_name(action);\n+      intptr_t pc = p2i(fr.pc());\n+\n+      JFR_ONLY(post_deoptimization_event(nm, tm, trap_bci, trap_bc, reason, action);)\n+      log_deopt(nm, tm, pc, fr, trap_bci, reason_name, reason_action);\n+      Events::log_deopt_message(current, \"Uncommon trap: reason=%s action=%s pc=\" INTPTR_FORMAT \" method=%s @ %d %s\",\n+                                reason_name, reason_action, pc,\n+                                tm->name_and_sig_as_C_string(), trap_bci, nm->compiler_name());\n+    }\n@@ -2802,1 +2876,1 @@\n-  \/\/ no udpate\n+  \/\/ no update\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":100,"deletions":26,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -163,2 +163,1 @@\n-#endif\n-\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"code\/scopeDesc.hpp\"\n@@ -35,0 +36,1 @@\n+#include \"logging\/log.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"oops\/stackChunkOop.inline.hpp\"\n@@ -44,0 +47,2 @@\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n@@ -47,0 +52,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -51,0 +57,1 @@\n+#include \"runtime\/stackValue.hpp\"\n@@ -53,1 +60,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -61,1 +67,1 @@\n-RegisterMap::RegisterMap(JavaThread *thread, bool update_map, bool process_frames) {\n+RegisterMap::RegisterMap(JavaThread *thread, UpdateMap update_map, ProcessFrames process_frames, WalkContinuation walk_cont) {\n@@ -63,2 +69,3 @@\n-  _update_map     = update_map;\n-  _process_frames = process_frames;\n+  _update_map     = update_map == UpdateMap::include;\n+  _process_frames = process_frames == ProcessFrames::include;\n+  _walk_cont      = walk_cont == WalkContinuation::include;\n@@ -66,1 +73,27 @@\n-  debug_only(_update_for_id = NULL;)\n+  DEBUG_ONLY (_update_for_id = NULL;)\n+  NOT_PRODUCT(_skip_missing = false;)\n+  NOT_PRODUCT(_async = false;)\n+\n+  if (walk_cont == WalkContinuation::include && thread != NULL && thread->last_continuation() != NULL) {\n+    _chunk = stackChunkHandle(Thread::current()->handle_area()->allocate_null_handle(), true \/* dummy *\/);\n+  }\n+  _chunk_index = -1;\n+\n+#ifndef PRODUCT\n+  for (int i = 0; i < reg_count ; i++ ) _location[i] = NULL;\n+#endif \/* PRODUCT *\/\n+}\n+\n+RegisterMap::RegisterMap(oop continuation, UpdateMap update_map) {\n+  _thread         = NULL;\n+  _update_map     = update_map == UpdateMap::include;\n+  _process_frames = false;\n+  _walk_cont      = true;\n+  clear();\n+  DEBUG_ONLY (_update_for_id = NULL;)\n+  NOT_PRODUCT(_skip_missing = false;)\n+  NOT_PRODUCT(_async = false;)\n+\n+  _chunk = stackChunkHandle(Thread::current()->handle_area()->allocate_null_handle(), true \/* dummy *\/);\n+  _chunk_index = -1;\n+\n@@ -78,0 +111,1 @@\n+  _walk_cont             = map->_walk_cont;\n@@ -79,1 +113,8 @@\n-  debug_only(_update_for_id = map->_update_for_id;)\n+  DEBUG_ONLY (_update_for_id = map->_update_for_id;)\n+  NOT_PRODUCT(_skip_missing = map->_skip_missing;)\n+  NOT_PRODUCT(_async = map->_async;)\n+\n+  \/\/ only the original RegisterMap's handle lives long enough for StackWalker; this is bound to cause trouble with nested continuations.\n+  _chunk = map->_chunk;\n+  _chunk_index = map->_chunk_index;\n+\n@@ -83,1 +124,1 @@\n-      LocationValidType bits = !update_map() ? 0 : map->_location_valid[i];\n+      LocationValidType bits = map->_location_valid[i];\n@@ -99,0 +140,17 @@\n+oop RegisterMap::cont() const {\n+  return _chunk() != NULL ? _chunk()->cont() : (oop)NULL;\n+}\n+\n+void RegisterMap::set_stack_chunk(stackChunkOop chunk) {\n+  assert(chunk == NULL || _walk_cont, \"\");\n+  assert(chunk == NULL || _chunk.not_null(), \"\");\n+  if (_chunk.is_null()) return;\n+  log_trace(continuations)(\"set_stack_chunk: \" INTPTR_FORMAT \" this: \" INTPTR_FORMAT, p2i((oopDesc*)chunk), p2i(this));\n+  _chunk.replace(chunk); \/\/ reuse handle. see comment above in the constructor\n+  if (chunk == NULL) {\n+    _chunk_index = -1;\n+  } else {\n+    _chunk_index++;\n+  }\n+}\n+\n@@ -101,1 +159,1 @@\n-  if (_update_map) {\n+  if (update_map()) {\n@@ -113,0 +171,8 @@\n+VMReg RegisterMap::find_register_spilled_here(void* p, intptr_t* sp) {\n+  for(int i = 0; i < RegisterMap::reg_count; i++) {\n+    VMReg r = VMRegImpl::as_VMReg(i);\n+    if (p == location(r, sp)) return r;\n+  }\n+  return NULL;\n+}\n+\n@@ -118,1 +184,1 @@\n-    intptr_t* src = (intptr_t*) location(r);\n+    intptr_t* src = (intptr_t*) location(r, nullptr);\n@@ -172,0 +238,15 @@\n+void frame::set_pc_preserve_deopt(address newpc) {\n+  set_pc_preserve_deopt(newpc, CodeCache::find_blob_unsafe(newpc));\n+}\n+\n+void frame::set_pc_preserve_deopt(address newpc, CodeBlob* cb) {\n+#ifdef ASSERT\n+  if (_cb != NULL && _cb->is_nmethod()) {\n+    assert(!((nmethod*)_cb)->is_deopt_pc(_pc), \"invariant violation\");\n+  }\n+#endif \/\/ ASSERT\n+\n+  _pc = newpc;\n+  _cb = cb;\n+}\n+\n@@ -176,4 +257,0 @@\n-bool frame::is_deoptimized_frame() const {\n-  assert(_deopt_state != unknown, \"not answerable\");\n-  return _deopt_state == is_deoptimized;\n-}\n@@ -193,11 +270,0 @@\n-\n-bool frame::is_compiled_frame() const {\n-  if (_cb != NULL &&\n-      _cb->is_compiled() &&\n-      ((CompiledMethod*)_cb)->is_java_method()) {\n-    return true;\n-  }\n-  return false;\n-}\n-\n-\n@@ -215,1 +281,4 @@\n-  RegisterMap map(JavaThread::current(), false); \/\/ No update\n+  RegisterMap map(JavaThread::current(),\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::include,\n+                  RegisterMap::WalkContinuation::skip); \/\/ No update\n@@ -221,0 +290,4 @@\n+bool frame::is_first_vthread_frame(JavaThread* thread) const {\n+  return Continuation::is_continuation_enterSpecial(*this)\n+    && Continuation::get_continuation_entry_for_entry_frame(thread, *this)->is_virtual_thread();\n+}\n@@ -273,1 +346,1 @@\n-  if( !nm->can_be_deoptimized() )\n+  if(!nm->can_be_deoptimized())\n@@ -280,2 +353,3 @@\n-  assert(thread->frame_anchor()->has_last_Java_frame() &&\n-         thread->frame_anchor()->walkable(), \"must be\");\n+  assert(thread == NULL\n+         || (thread->frame_anchor()->has_last_Java_frame() &&\n+             thread->frame_anchor()->walkable()), \"must be\");\n@@ -285,2 +359,1 @@\n-  \/\/ If the call site is a MethodHandle call site use the MH deopt\n-  \/\/ handler.\n+  \/\/ If the call site is a MethodHandle call site use the MH deopt handler.\n@@ -292,0 +365,2 @@\n+  NativePostCallNop* inst = nativePostCallNop_at(pc());\n+\n@@ -314,0 +389,1 @@\n+  assert(is_deoptimized_frame(), \"must be\");\n@@ -316,2 +392,1 @@\n-  {\n-    RegisterMap map(thread, false);\n+  if (thread != NULL) {\n@@ -319,2 +394,9 @@\n-    while (id() != check.id()) {\n-      check = check.sender(&map);\n+    if (is_older(check.id())) {\n+      RegisterMap map(thread,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+      while (id() != check.id()) {\n+        check = check.sender(&map);\n+      }\n+      assert(check.is_deoptimized_frame(), \"missed deopt\");\n@@ -322,1 +404,0 @@\n-    assert(check.is_deoptimized_frame(), \"missed deopt\");\n@@ -328,1 +409,4 @@\n-  RegisterMap map(JavaThread::current(), false);\n+  RegisterMap map(JavaThread::current(),\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::include,\n+                  RegisterMap::WalkContinuation::skip);\n@@ -421,1 +505,3 @@\n-  return &((*interpreter_frame_locals_addr())[n]);\n+  intptr_t* first = _on_heap ? fp() + (intptr_t)*interpreter_frame_locals_addr()\n+                             : *interpreter_frame_locals_addr();\n+  return &(first[n]);\n@@ -442,2 +528,2 @@\n-  assert( stack_size <= (size_t)max_jint, \"stack size too big\");\n-  return ((jint)stack_size);\n+  assert(stack_size <= (size_t)max_jint, \"stack size too big\");\n+  return (jint)stack_size;\n@@ -498,1 +584,0 @@\n-\n@@ -506,1 +591,0 @@\n-\n@@ -620,1 +704,1 @@\n-        st->print(\"v  ~StubRoutines::%s\", desc->name());\n+        st->print(\"v  ~StubRoutines::%s \" PTR_FORMAT, desc->name(), p2i(pc()));\n@@ -625,1 +709,1 @@\n-      st->print(\"v  ~BufferBlob::%s\", ((BufferBlob *)_cb)->name());\n+      st->print(\"v  ~BufferBlob::%s \" PTR_FORMAT, ((BufferBlob *)_cb)->name(), p2i(pc()));\n@@ -661,1 +745,1 @@\n-      st->print(\"v  ~RuntimeStub::%s\", ((RuntimeStub *)_cb)->name());\n+      st->print(\"v  ~RuntimeStub::%s \" PTR_FORMAT, ((RuntimeStub *)_cb)->name(), p2i(pc()));\n@@ -663,1 +747,1 @@\n-      st->print(\"v  ~DeoptimizationBlob\");\n+      st->print(\"v  ~DeoptimizationBlob \" PTR_FORMAT, p2i(pc()));\n@@ -665,1 +749,1 @@\n-      st->print(\"v  ~ExceptionBlob\");\n+      st->print(\"v  ~ExceptionBlob \" PTR_FORMAT, p2i(pc()));\n@@ -667,1 +751,1 @@\n-      st->print(\"v  ~SafepointBlob\");\n+      st->print(\"v  ~SafepointBlob \" PTR_FORMAT, p2i(pc()));\n@@ -669,1 +753,1 @@\n-      st->print(\"v  ~AdapterBlob\");\n+      st->print(\"v  ~AdapterBlob \" PTR_FORMAT, p2i(pc()));\n@@ -671,1 +755,1 @@\n-      st->print(\"v  ~VtableBlob\");\n+      st->print(\"v  ~VtableBlob \" PTR_FORMAT, p2i(pc()));\n@@ -673,1 +757,1 @@\n-      st->print(\"v  ~MethodHandlesAdapterBlob\");\n+      st->print(\"v  ~MethodHandlesAdapterBlob \" PTR_FORMAT, p2i(pc()));\n@@ -675,1 +759,1 @@\n-      st->print(\"v  ~UncommonTrapBlob\");\n+      st->print(\"v  ~UncommonTrapBlob \" PTR_FORMAT, p2i(pc()));\n@@ -836,0 +920,3 @@\n+oop frame::interpreter_callee_receiver(Symbol* signature) {\n+  return *interpreter_callee_receiver_addr(signature);\n+}\n@@ -839,1 +926,0 @@\n-  assert(map != NULL, \"map must be set\");\n@@ -884,1 +970,1 @@\n-    if (call.is_valid()) {\n+    if (map != nullptr && call.is_valid()) {\n@@ -940,2 +1026,1 @@\n-void frame::oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* reg_map,\n-                              DerivedPointerIterationMode derived_mode) const {\n+void frame::oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf, DerivedOopClosure* df, DerivedPointerIterationMode derived_mode, const RegisterMap* reg_map) const {\n@@ -943,2 +1028,7 @@\n-  if (_cb->oop_maps() != NULL) {\n-    OopMapSet::oops_do(this, reg_map, f, derived_mode);\n+  assert((oop_map() == NULL) == (_cb->oop_maps() == NULL), \"frame and _cb must agree that oopmap is set or not\");\n+  if (oop_map() != NULL) {\n+    if (df != NULL) {\n+      _oop_map->oops_do(this, reg_map, f, df);\n+    } else {\n+      _oop_map->oops_do(this, reg_map, f, derived_mode);\n+    }\n@@ -984,1 +1074,10 @@\n-    assert(loc != NULL, \"missing register map entry\");\n+  #ifdef ASSERT\n+    if (loc == NULL) {\n+      if (_reg_map->should_skip_missing()) {\n+        return;\n+      }\n+      tty->print_cr(\"Error walking frame oops:\");\n+      _fr.print_on(tty);\n+      assert(loc != NULL, \"missing register map entry reg: \" INTPTR_FORMAT \" %s loc: \" INTPTR_FORMAT, reg->value(), reg->name(), p2i(loc));\n+    }\n+  #endif\n@@ -1017,1 +1116,1 @@\n-  ResourceMark rm;\n+  \/\/ ResourceMark rm;\n@@ -1022,1 +1121,0 @@\n-\n@@ -1078,4 +1176,5 @@\n-void frame::oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n-                    DerivedPointerIterationMode derived_mode) const {\n-  oops_do_internal(f, cf, map, true, derived_mode);\n-}\n+bool frame::is_deoptimized_frame() const {\n+  assert(_deopt_state != unknown, \"not answerable\");\n+  if (_deopt_state == is_deoptimized) {\n+    return true;\n+  }\n@@ -1083,8 +1182,8 @@\n-void frame::oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map) const {\n-#if COMPILER2_OR_JVMCI\n-  oops_do_internal(f, cf, map, true, DerivedPointerTable::is_active() ?\n-                                     DerivedPointerIterationMode::_with_table :\n-                                     DerivedPointerIterationMode::_ignore);\n-#else\n-  oops_do_internal(f, cf, map, true, DerivedPointerIterationMode::_ignore);\n-#endif\n+  \/* This method only checks if the frame is deoptimized\n+   * as in return address being patched.\n+   * It doesn't care if the OP that we return to is a\n+   * deopt instruction *\/\n+  \/*if (_cb != NULL && _cb->is_nmethod()) {\n+    return NativeDeoptInstruction::is_deopt_at(_pc);\n+  }*\/\n+  return false;\n@@ -1093,2 +1192,3 @@\n-void frame::oops_do_internal(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n-                             bool use_interpreter_oop_map_cache, DerivedPointerIterationMode derived_mode) const {\n+void frame::oops_do_internal(OopClosure* f, CodeBlobClosure* cf,\n+                             DerivedOopClosure* df, DerivedPointerIterationMode derived_mode,\n+                             const RegisterMap* map, bool use_interpreter_oop_map_cache) const {\n@@ -1106,2 +1206,2 @@\n-  } else if (is_optimized_entry_frame()) {\n-    _cb->as_optimized_entry_blob()->oops_do(f, *this);\n+  } else if (is_upcall_stub_frame()) {\n+    _cb->as_upcall_stub()->oops_do(f, *this);\n@@ -1109,1 +1209,1 @@\n-    oops_code_blob_do(f, cf, map, derived_mode);\n+    oops_code_blob_do(f, cf, df, derived_mode, map);\n@@ -1133,0 +1233,7 @@\n+#ifndef PRODUCT\n+  if (TraceCodeBlobStacks) {\n+    tty->print_cr(\"*** verify\");\n+    print_on(tty);\n+  }\n+#endif\n+\n@@ -1146,0 +1253,1 @@\n+\n@@ -1147,1 +1255,1 @@\n-    oops_do_internal(&VerifyOopClosure::verify_oop, NULL, map, false, DerivedPointerIterationMode::_ignore);\n+    oops_do_internal(&VerifyOopClosure::verify_oop, NULL, NULL, DerivedPointerIterationMode::_ignore, map, false);\n@@ -1190,0 +1298,85 @@\n+\n+\/\/ Returns true iff the address p is readable and *(intptr_t*)p != errvalue\n+extern \"C\" bool dbg_is_safe(const void* p, intptr_t errvalue);\n+\n+class FrameValuesOopClosure: public OopClosure, public DerivedOopClosure {\n+private:\n+  GrowableArray<oop*>* _oops;\n+  GrowableArray<narrowOop*>* _narrow_oops;\n+  GrowableArray<oop*>* _base;\n+  GrowableArray<derived_pointer*>* _derived;\n+  NoSafepointVerifier nsv;\n+\n+public:\n+  FrameValuesOopClosure() {\n+    _oops = new (ResourceObj::C_HEAP, mtThread) GrowableArray<oop*>(100, mtThread);\n+    _narrow_oops = new (ResourceObj::C_HEAP, mtThread) GrowableArray<narrowOop*>(100, mtThread);\n+    _base = new (ResourceObj::C_HEAP, mtThread) GrowableArray<oop*>(100, mtThread);\n+    _derived = new (ResourceObj::C_HEAP, mtThread) GrowableArray<derived_pointer*>(100, mtThread);\n+  }\n+  ~FrameValuesOopClosure() {\n+    delete _oops;\n+    delete _narrow_oops;\n+    delete _base;\n+    delete _derived;\n+  }\n+\n+  virtual void do_oop(oop* p) override { _oops->push(p); }\n+  virtual void do_oop(narrowOop* p) override { _narrow_oops->push(p); }\n+  virtual void do_derived_oop(oop* base_loc, derived_pointer* derived_loc) override {\n+    _base->push(base_loc);\n+    _derived->push(derived_loc);\n+  }\n+\n+  bool is_good(oop* p) {\n+    return *p == nullptr || (dbg_is_safe(*p, -1) && dbg_is_safe((*p)->klass(), -1) && oopDesc::is_oop_or_null(*p));\n+  }\n+  void describe(FrameValues& values, int frame_no) {\n+    for (int i = 0; i < _oops->length(); i++) {\n+      oop* p = _oops->at(i);\n+      values.describe(frame_no, (intptr_t*)p, err_msg(\"oop%s for #%d\", is_good(p) ? \"\" : \" (BAD)\", frame_no));\n+    }\n+    for (int i = 0; i < _narrow_oops->length(); i++) {\n+      narrowOop* p = _narrow_oops->at(i);\n+      \/\/ we can't check for bad compressed oops, as decoding them might crash\n+      values.describe(frame_no, (intptr_t*)p, err_msg(\"narrow oop for #%d\", frame_no));\n+    }\n+    assert(_base->length() == _derived->length(), \"should be the same\");\n+    for (int i = 0; i < _base->length(); i++) {\n+      oop* base = _base->at(i);\n+      derived_pointer* derived = _derived->at(i);\n+      values.describe(frame_no, (intptr_t*)derived, err_msg(\"derived pointer (base: \" INTPTR_FORMAT \") for #%d\", p2i(base), frame_no));\n+    }\n+  }\n+};\n+\n+class FrameValuesOopMapClosure: public OopMapClosure {\n+private:\n+  const frame* _fr;\n+  const RegisterMap* _reg_map;\n+  FrameValues& _values;\n+  int _frame_no;\n+\n+public:\n+  FrameValuesOopMapClosure(const frame* fr, const RegisterMap* reg_map, FrameValues& values, int frame_no)\n+   : _fr(fr), _reg_map(reg_map), _values(values), _frame_no(frame_no) {}\n+\n+  virtual void do_value(VMReg reg, OopMapValue::oop_types type) override {\n+    intptr_t* p = (intptr_t*)_fr->oopmapreg_to_location(reg, _reg_map);\n+    if (p != NULL && (((intptr_t)p & WordAlignmentMask) == 0)) {\n+      const char* type_name = NULL;\n+      switch(type) {\n+        case OopMapValue::oop_value:          type_name = \"oop\";          break;\n+        case OopMapValue::narrowoop_value:    type_name = \"narrow oop\";   break;\n+        case OopMapValue::callee_saved_value: type_name = \"callee-saved\"; break;\n+        case OopMapValue::derived_oop_value:  type_name = \"derived\";      break;\n+        \/\/ case OopMapValue::live_value:         type_name = \"live\";         break;\n+        default: break;\n+      }\n+      if (type_name != NULL) {\n+        _values.describe(_frame_no, p, err_msg(\"%s for #%d\", type_name, _frame_no));\n+      }\n+    }\n+  }\n+};\n+\n@@ -1192,1 +1385,1 @@\n-void frame::describe(FrameValues& values, int frame_no) {\n+void frame::describe(FrameValues& values, int frame_no, const RegisterMap* reg_map) {\n@@ -1194,1 +1387,1 @@\n-  values.describe(-1, sp(), err_msg(\"sp for #%d\", frame_no), 1);\n+  values.describe(-1, sp(), err_msg(\"sp for #%d\", frame_no), 0);\n@@ -1207,1 +1400,1 @@\n-    values.describe(-1, unextended_sp(), err_msg(\"unextended_sp for #%d\", frame_no));\n+    values.describe(-1, unextended_sp(), err_msg(\"unextended_sp for #%d\", frame_no), 0);\n@@ -1213,0 +1406,1 @@\n+    InterpreterCodelet* desc = Interpreter::codelet_containing(pc());\n@@ -1216,1 +1410,6 @@\n-                    FormatBuffer<1024>(\"#%d method %s @ %d\", frame_no, m->name_and_sig_as_C_string(), bci), 2);\n+                    FormatBuffer<1024>(\"#%d method %s @ %d\", frame_no, m->name_and_sig_as_C_string(), bci), 3);\n+    if (desc != NULL) {\n+      values.describe(-1, info_address, err_msg(\"- %s codelet: %s\",\n+        desc->bytecode()    >= 0    ? Bytecodes::name(desc->bytecode()) : \"\",\n+        desc->description() != NULL ? desc->description()               : \"?\"), 2);\n+    }\n@@ -1218,1 +1417,4 @@\n-                    err_msg(\"- %d locals %d max stack\", m->max_locals(), m->max_stack()), 1);\n+                    err_msg(\"- %d locals %d max stack\", m->max_locals(), m->max_stack()), 2);\n+    \/\/ return address will be emitted by caller in describe_pd\n+    \/\/ values.describe(frame_no, (intptr_t*)sender_pc_addr(), Continuation::is_return_barrier_entry(*sender_pc_addr()) ? \"return address (return barrier)\" : \"return address\");\n+\n@@ -1222,1 +1424,1 @@\n-      values.describe(-1, MAX2(l0, ln), err_msg(\"locals for #%d\", frame_no), 1);\n+      values.describe(-1, MAX2(l0, ln), err_msg(\"locals for #%d\", frame_no), 2);\n@@ -1226,1 +1428,1 @@\n-        values.describe(frame_no, l0, err_msg(\"local %d\", l));\n+        values.describe(frame_no, l0, err_msg(\"local %d\", l), 1);\n@@ -1230,0 +1432,5 @@\n+    if (interpreter_frame_monitor_begin() != interpreter_frame_monitor_end()) {\n+      values.describe(frame_no, (intptr_t*)interpreter_frame_monitor_begin(), \"monitors begin\");\n+      values.describe(frame_no, (intptr_t*)interpreter_frame_monitor_end(), \"monitors end\");\n+    }\n+\n@@ -1238,1 +1445,1 @@\n-                      err_msg(\"stack %d\", e));\n+                      err_msg(\"stack %d\", e), 1);\n@@ -1241,1 +1448,1 @@\n-      values.describe(-1, tos, err_msg(\"expression stack for #%d\", frame_no), 1);\n+      values.describe(-1, tos, err_msg(\"expression stack for #%d\", frame_no), 2);\n@@ -1243,3 +1450,5 @@\n-    if (interpreter_frame_monitor_begin() != interpreter_frame_monitor_end()) {\n-      values.describe(frame_no, (intptr_t*)interpreter_frame_monitor_begin(), \"monitors begin\");\n-      values.describe(frame_no, (intptr_t*)interpreter_frame_monitor_end(), \"monitors end\");\n+\n+    if (reg_map != NULL) {\n+      FrameValuesOopClosure oopsFn;\n+      oops_do(&oopsFn, NULL, &oopsFn, reg_map);\n+      oopsFn.describe(values, frame_no);\n@@ -1250,1 +1459,1 @@\n-  } else if (is_compiled_frame()) {\n+  } else if (cb()->is_compiled()) {\n@@ -1252,1 +1461,1 @@\n-    CompiledMethod* cm = (CompiledMethod*)cb();\n+    CompiledMethod* cm = cb()->as_compiled_method();\n@@ -1260,1 +1469,98 @@\n-                    2);\n+                    3);\n+\n+    { \/\/ mark arguments (see nmethod::print_nmethod_labels)\n+      Method* m = cm->method();\n+\n+      int stack_slot_offset = cm->frame_size() * wordSize; \/\/ offset, in bytes, to caller sp\n+      int sizeargs = m->size_of_parameters();\n+\n+      BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n+      VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n+      {\n+        int sig_index = 0;\n+        if (!m->is_static()) {\n+          sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n+        }\n+        for (SignatureStream ss(m->signature()); !ss.at_return_type(); ss.next()) {\n+          BasicType t = ss.type();\n+          assert(type2size[t] == 1 || type2size[t] == 2, \"size is 1 or 2\");\n+          sig_bt[sig_index++] = t;\n+          if (type2size[t] == 2) {\n+            sig_bt[sig_index++] = T_VOID;\n+          }\n+        }\n+        assert(sig_index == sizeargs, \"\");\n+      }\n+      int stack_arg_slots = SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n+      assert(stack_arg_slots ==  m->num_stack_arg_slots(), \"\");\n+      int out_preserve = SharedRuntime::out_preserve_stack_slots();\n+      int sig_index = 0;\n+      int arg_index = (m->is_static() ? 0 : -1);\n+      for (SignatureStream ss(m->signature()); !ss.at_return_type(); ) {\n+        bool at_this = (arg_index == -1);\n+        bool at_old_sp = false;\n+        BasicType t = (at_this ? T_OBJECT : ss.type());\n+        assert(t == sig_bt[sig_index], \"sigs in sync\");\n+        VMReg fst = regs[sig_index].first();\n+        if (fst->is_stack()) {\n+          assert(((int)fst->reg2stack()) >= 0, \"reg2stack: \" INTPTR_FORMAT, fst->reg2stack());\n+          int offset = (fst->reg2stack() + out_preserve) * VMRegImpl::stack_slot_size + stack_slot_offset;\n+          intptr_t* stack_address = (intptr_t*)((address)unextended_sp() + offset);\n+          if (at_this) {\n+            values.describe(frame_no, stack_address, err_msg(\"this for #%d\", frame_no), 1);\n+          } else {\n+            values.describe(frame_no, stack_address, err_msg(\"param %d %s for #%d\", arg_index, type2name(t), frame_no), 1);\n+          }\n+        }\n+        sig_index += type2size[t];\n+        arg_index += 1;\n+        if (!at_this) {\n+          ss.next();\n+        }\n+      }\n+    }\n+\n+    if (reg_map != NULL && is_java_frame()) {\n+      int scope_no = 0;\n+      for (ScopeDesc* scope = cm->scope_desc_at(pc()); scope != NULL; scope = scope->sender(), scope_no++) {\n+        Method* m = scope->method();\n+        int  bci = scope->bci();\n+        values.describe(-1, info_address, err_msg(\"- #%d scope %s @ %d\", scope_no, m->name_and_sig_as_C_string(), bci), 2);\n+\n+        { \/\/ mark locals\n+          GrowableArray<ScopeValue*>* scvs = scope->locals();\n+          int scvs_length = scvs != NULL ? scvs->length() : 0;\n+          for (int i = 0; i < scvs_length; i++) {\n+            intptr_t* stack_address = (intptr_t*)StackValue::stack_value_address(this, reg_map, scvs->at(i));\n+            if (stack_address != NULL) {\n+              values.describe(frame_no, stack_address, err_msg(\"local %d for #%d (scope %d)\", i, frame_no, scope_no), 1);\n+            }\n+          }\n+        }\n+        { \/\/ mark expression stack\n+          GrowableArray<ScopeValue*>* scvs = scope->expressions();\n+          int scvs_length = scvs != NULL ? scvs->length() : 0;\n+          for (int i = 0; i < scvs_length; i++) {\n+            intptr_t* stack_address = (intptr_t*)StackValue::stack_value_address(this, reg_map, scvs->at(i));\n+            if (stack_address != NULL) {\n+              values.describe(frame_no, stack_address, err_msg(\"stack %d for #%d (scope %d)\", i, frame_no, scope_no), 1);\n+            }\n+          }\n+        }\n+      }\n+\n+      FrameValuesOopClosure oopsFn;\n+      oops_do(&oopsFn, NULL, &oopsFn, reg_map);\n+      oopsFn.describe(values, frame_no);\n+\n+      if (oop_map() != NULL) {\n+        FrameValuesOopMapClosure valuesFn(this, reg_map, values, frame_no);\n+        \/\/ also OopMapValue::live_value ??\n+        oop_map()->all_type_do(this, OopMapValue::callee_saved_value, &valuesFn);\n+      }\n+    }\n+\n+    if (cm->method()->is_continuation_enter_intrinsic()) {\n+      ContinuationEntry* ce = Continuation::get_continuation_entry_for_entry_frame(reg_map->thread(), *this); \/\/ (ContinuationEntry*)unextended_sp();\n+      ce->describe(values, frame_no);\n+    }\n@@ -1320,0 +1626,1 @@\n+  \/\/ if (error) { tty->cr(); print_on((JavaThread*)nullptr, tty); }\n@@ -1335,13 +1642,7 @@\n-  if (thread == Thread::current()) {\n-    while (!thread->is_in_live_stack((address)v0)) {\n-      v0 = _values.at(++min_index).location;\n-    }\n-    while (!thread->is_in_live_stack((address)v1)) {\n-      v1 = _values.at(--max_index).location;\n-    }\n-  } else {\n-    while (!thread->is_in_full_stack((address)v0)) {\n-      v0 = _values.at(++min_index).location;\n-    }\n-    while (!thread->is_in_full_stack((address)v1)) {\n-      v1 = _values.at(--max_index).location;\n+  if (thread != NULL) {\n+    if (thread == Thread::current()) {\n+      while (!thread->is_in_live_stack((address)v0)) v0 = _values.at(++min_index).location;\n+      while (!thread->is_in_live_stack((address)v1)) v1 = _values.at(--max_index).location;\n+    } else {\n+      while (!thread->is_in_full_stack((address)v0)) v0 = _values.at(++min_index).location;\n+      while (!thread->is_in_full_stack((address)v1)) v1 = _values.at(--max_index).location;\n@@ -1350,0 +1651,21 @@\n+\n+  print_on(st, min_index, max_index, v0, v1);\n+}\n+\n+void FrameValues::print_on(stackChunkOop chunk, outputStream* st) {\n+  _values.sort(compare);\n+\n+  intptr_t* start = chunk->start_address();\n+  intptr_t* end = chunk->end_address() + 1;\n+\n+  int min_index = 0;\n+  int max_index = _values.length() - 1;\n+  intptr_t* v0 = _values.at(min_index).location;\n+  intptr_t* v1 = _values.at(max_index).location;\n+  while (!(start <= v0 && v0 <= end)) v0 = _values.at(++min_index).location;\n+  while (!(start <= v1 && v1 <= end)) v1 = _values.at(--max_index).location;\n+\n+  print_on(st, min_index, max_index, v0, v1, true \/* on_heap *\/);\n+}\n+\n+void FrameValues::print_on(outputStream* st, int min_index, int max_index, intptr_t* v0, intptr_t* v1, bool on_heap) {\n@@ -1364,1 +1686,7 @@\n-      st->print_cr(\" \" INTPTR_FORMAT \": \" INTPTR_FORMAT \" %s\", p2i(fv.location), *fv.location, fv.description);\n+      if (on_heap\n+          && *fv.location != 0 && *fv.location > -100 && *fv.location < 100\n+          && (strncmp(fv.description, \"interpreter_frame_\", 18) == 0 || strstr(fv.description, \" method \"))) {\n+        st->print_cr(\" \" INTPTR_FORMAT \": %18d %s\", p2i(fv.location), (int)*fv.location, fv.description);\n+      } else {\n+        st->print_cr(\" \" INTPTR_FORMAT \": \" INTPTR_FORMAT \" %s\", p2i(fv.location), *fv.location, fv.description);\n+      }\n","filename":"src\/hotspot\/share\/runtime\/frame.cpp","additions":435,"deletions":107,"binary":false,"changes":542,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,2 @@\n+#include \"compiler\/oopMap.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n@@ -42,1 +44,1 @@\n-class vframeArray;\n+class InterpreterOopMap;\n@@ -47,0 +49,1 @@\n+class vframeArray;\n@@ -63,1 +66,4 @@\n-  intptr_t* _sp; \/\/ stack pointer (from Thread::last_Java_sp)\n+  union {\n+    intptr_t* _sp; \/\/ stack pointer (from Thread::last_Java_sp)\n+    int _offset_sp; \/\/ used by frames in stack chunks\n+  };\n@@ -65,2 +71,2 @@\n-\n-  CodeBlob* _cb; \/\/ CodeBlob that \"owns\" pc\n+  mutable CodeBlob* _cb; \/\/ CodeBlob that \"owns\" pc\n+  mutable const ImmutableOopMap* _oop_map; \/\/ oop map, for compiled\/stubs frames only\n@@ -75,0 +81,11 @@\n+  \/\/ Do internal pointers in interpreter frames use absolute adddresses or relative (to fp)?\n+  \/\/ Frames in stack chunks are on the Java heap and use relative addressing; on the stack\n+  \/\/ they use absolute addressing\n+  bool        _on_heap;  \/\/ This frame represents a frame on the heap.\n+  DEBUG_ONLY(int _frame_index;) \/\/ the frame index in a stack chunk; -1 when on a thread stack\n+\n+  \/\/ We use different assertions to allow for intermediate states (e.g. during thawing or relativizing the frame)\n+  void assert_on_heap() const  { assert(is_heap_frame(), \"Using offset with a non-chunk frame\"); }\n+  void assert_offset() const   { assert(_frame_index >= 0,  \"Using offset with a non-chunk frame\"); assert_on_heap(); }\n+  void assert_absolute() const { assert(_frame_index == -1, \"Using absolute addresses with a chunk frame\"); }\n+\n@@ -79,0 +96,4 @@\n+  explicit frame(bool dummy) {} \/\/ no initialization\n+\n+  explicit frame(intptr_t* sp);\n+\n@@ -101,1 +122,3 @@\n-  void set_pc( address   newpc );\n+  void set_pc(address newpc);\n+  void set_pc_preserve_deopt(address newpc);\n+  void set_pc_preserve_deopt(address newpc, CodeBlob* cb);\n@@ -103,1 +126,1 @@\n-  intptr_t* sp() const           { return _sp; }\n+  intptr_t* sp() const           { assert_absolute(); return _sp; }\n@@ -106,0 +129,17 @@\n+  int offset_sp() const           { assert_offset();  return _offset_sp; }\n+  void set_offset_sp( int newsp ) { assert_on_heap(); _offset_sp = newsp; }\n+\n+  int frame_index() const {\n+  #ifdef ASSERT\n+    return _frame_index;\n+  #else\n+    return -1;\n+  #endif\n+  }\n+  void set_frame_index( int index ) {\n+    #ifdef ASSERT\n+      _frame_index = index;\n+    #endif\n+  }\n+\n+  static int sender_sp_ret_address_offset();\n@@ -108,0 +148,9 @@\n+  inline CodeBlob* get_cb() const;\n+  \/\/ inline void set_cb(CodeBlob* cb);\n+\n+  const ImmutableOopMap* oop_map() const {\n+    if (_oop_map == NULL) {\n+      _oop_map = get_oop_map();\n+    }\n+    return _oop_map;\n+  }\n@@ -131,0 +180,1 @@\n+  bool is_empty()                const { return _pc == NULL; }\n@@ -141,1 +191,2 @@\n-  bool is_optimized_entry_frame()         const;\n+  bool is_upcall_stub_frame()    const;\n+  bool is_heap_frame()             const { return _on_heap; }\n@@ -146,0 +197,1 @@\n+  bool is_first_vthread_frame(JavaThread* thread) const;\n@@ -151,1 +203,1 @@\n-    return is_compiled_frame() || is_optimized_entry_frame();\n+    return is_compiled_frame() || is_upcall_stub_frame();\n@@ -160,2 +212,10 @@\n-  \/\/ returns the frame size in stack slots\n-  int frame_size(RegisterMap* map) const;\n+  \/\/ the frame size in machine words\n+  inline int frame_size() const;\n+\n+  \/\/ the number of oops in the frame for non-interpreted frames\n+  inline int num_oops() const;\n+\n+  \/\/ the size, in words, of stack-passed arguments\n+  inline int compiled_frame_stack_argsize() const;\n+\n+  inline void interpreted_frame_oop_map(InterpreterOopMap* mask) const;\n@@ -164,1 +224,1 @@\n-  frame sender(RegisterMap* map) const;\n+  inline frame sender(RegisterMap* map) const;\n@@ -171,1 +231,1 @@\n-  \/\/ returns the the sending Java frame, skipping any intermediate C frames\n+  \/\/ returns the sending Java frame, skipping any intermediate C frames\n@@ -177,1 +237,1 @@\n-  frame sender_for_compiled_frame(RegisterMap* map) const;\n+  inline frame sender_for_compiled_frame(RegisterMap* map) const;\n@@ -181,1 +241,1 @@\n-  frame sender_for_optimized_entry_frame(RegisterMap* map) const;\n+  frame sender_for_upcall_stub_frame(RegisterMap* map) const;\n@@ -192,1 +252,7 @@\n-  intptr_t  at(int index) const                  { return *addr_at(index); }\n+  intptr_t  at_absolute(int index) const         { return *addr_at(index); }\n+  \/\/ Interpreter frames in continuation stacks are on the heap, and internal addresses are relative to fp.\n+  intptr_t  at_relative(int index) const         { return (intptr_t)(fp() + fp()[index]); }\n+\n+  intptr_t at(int index) const                   {\n+    return _on_heap ? at_relative(index) : at_absolute(index);\n+  }\n@@ -225,0 +291,4 @@\n+  void set_unextended_sp(intptr_t* value);\n+\n+  int offset_unextended_sp() const;\n+  void set_offset_unextended_sp(int value);\n@@ -280,1 +350,1 @@\n-  oop interpreter_callee_receiver(Symbol* signature)     { return *interpreter_callee_receiver_addr(signature); }\n+  oop interpreter_callee_receiver(Symbol* signature);\n@@ -353,1 +423,1 @@\n-  bool optimized_entry_frame_is_first() const;\n+  bool upcall_stub_frame_is_first() const;\n@@ -376,1 +446,1 @@\n-  void describe(FrameValues& values, int frame_no);\n+  void describe(FrameValues& values, int frame_no, const RegisterMap* reg_map=NULL);\n@@ -379,2 +449,4 @@\n-  address oopmapreg_to_location(VMReg reg, const RegisterMap* reg_map) const;\n-  oop* oopmapreg_to_oop_location(VMReg reg, const RegisterMap* reg_map) const;\n+  template <typename RegisterMapT>\n+  address oopmapreg_to_location(VMReg reg, const RegisterMapT* reg_map) const;\n+  template <typename RegisterMapT>\n+  oop* oopmapreg_to_oop_location(VMReg reg, const RegisterMapT* reg_map) const;\n@@ -391,2 +463,4 @@\n-  void oops_do_internal(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n-                        bool use_interpreter_oop_map_cache, DerivedPointerIterationMode derived_mode) const;\n+  void oops_do_internal(OopClosure* f, CodeBlobClosure* cf,\n+                        DerivedOopClosure* df, DerivedPointerIterationMode derived_mode,\n+                        const RegisterMap* map, bool use_interpreter_oop_map_cache) const;\n+\n@@ -394,2 +468,3 @@\n-  void oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map,\n-                         DerivedPointerIterationMode derived_mode) const;\n+  void oops_code_blob_do(OopClosure* f, CodeBlobClosure* cf,\n+                         DerivedOopClosure* df, DerivedPointerIterationMode derived_mode,\n+                         const RegisterMap* map) const;\n@@ -399,0 +474,15 @@\n+  void oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map) {\n+#if COMPILER2_OR_JVMCI\n+    DerivedPointerIterationMode dpim = DerivedPointerTable::is_active() ?\n+                                       DerivedPointerIterationMode::_with_table :\n+                                       DerivedPointerIterationMode::_ignore;\n+#else\n+    DerivedPointerIterationMode dpim = DerivedPointerIterationMode::_ignore;;\n+#endif\n+    oops_do_internal(f, cf, NULL, dpim, map, true);\n+  }\n+\n+  void oops_do(OopClosure* f, CodeBlobClosure* cf, DerivedOopClosure* df, const RegisterMap* map) {\n+    oops_do_internal(f, cf, df, DerivedPointerIterationMode::_ignore, map, true);\n+  }\n+\n@@ -400,2 +490,4 @@\n-               DerivedPointerIterationMode derived_mode) const;\n-  void oops_do(OopClosure* f, CodeBlobClosure* cf, const RegisterMap* map) const;\n+               DerivedPointerIterationMode derived_mode) const {\n+    oops_do_internal(f, cf, NULL, derived_mode, map, true);\n+  }\n+\n@@ -432,1 +524,0 @@\n-\n@@ -451,0 +542,3 @@\n+  void print_on(outputStream* out, int min_index, int max_index, intptr_t* v0, intptr_t* v1,\n+                bool on_heap = false);\n+\n@@ -460,0 +554,2 @@\n+  void print(stackChunkOop chunk) { print_on(chunk, tty); }\n+  void print_on(stackChunkOop chunk, outputStream* out);\n","filename":"src\/hotspot\/share\/runtime\/frame.hpp","additions":124,"deletions":28,"binary":false,"changes":152,"status":"modified"},{"patch":"@@ -132,1 +132,1 @@\n-  product(intx, ObjectAlignmentInBytes, 8,                                  \\\n+  product(int, ObjectAlignmentInBytes, 8,                                   \\\n@@ -149,1 +149,1 @@\n-const intx ObjectAlignmentInBytes = 8;\n+const int ObjectAlignmentInBytes = 8;\n@@ -477,1 +477,1 @@\n-  develop(uintx, ErrorHandlerTest, 0,                                       \\\n+  develop(uint, ErrorHandlerTest, 0,                                        \\\n@@ -483,1 +483,1 @@\n-  develop(uintx, TestCrashInErrorHandler, 0,                                \\\n+  develop(uint, TestCrashInErrorHandler, 0,                                 \\\n@@ -689,0 +689,8 @@\n+  product(bool, PostVirtualThreadCompatibleLifecycleEvents, true, EXPERIMENTAL, \\\n+               \"Post virtual thread ThreadStart and ThreadEnd events for \"  \\\n+               \"virtual thread unaware agents\")                             \\\n+                                                                            \\\n+  product(bool, DoJVMTIVirtualThreadTransitions, true, EXPERIMENTAL,        \\\n+               \"Do JVMTI virtual thread mount\/unmount transitions \"         \\\n+               \"(disabling this flag implies no JVMTI events are posted)\")  \\\n+                                                                            \\\n@@ -809,1 +817,1 @@\n-  product(intx, DiagnoseSyncOnValueBasedClasses, 0, DIAGNOSTIC,             \\\n+  product(int, DiagnoseSyncOnValueBasedClasses, 0, DIAGNOSTIC,              \\\n@@ -986,3 +994,0 @@\n-  develop(bool, EagerInitialization, false,                                 \\\n-          \"Eagerly initialize classes if possible\")                         \\\n-                                                                            \\\n@@ -1082,1 +1087,1 @@\n-  product(bool, UseHeavyMonitors, false,                                    \\\n+  develop(bool, UseHeavyMonitors, false,                                    \\\n@@ -1307,4 +1312,5 @@\n-  product(intx, SelfDestructTimer, 0,                                       \\\n-          \"Will cause VM to terminate after a given time (in minutes) \"     \\\n-          \"(0 means off)\")                                                  \\\n-          range(0, max_intx)                                                \\\n+  product(double, SelfDestructTimer, 0.0,                                   \\\n+          \"Will cause VM to terminate after a given time \"                  \\\n+          \"(in fractional minutes) \"                                        \\\n+          \"(0.0 means off)\")                                                \\\n+          range(0.0, (double)max_intx)                                      \\\n@@ -1624,1 +1630,1 @@\n-  product(intx, ThreadPriorityPolicy, 0,                                    \\\n+  product(int, ThreadPriorityPolicy, 0,                                     \\\n@@ -1649,1 +1655,1 @@\n-  product(intx, CompilerThreadPriority, -1,                                 \\\n+  product(int, CompilerThreadPriority, -1,                                  \\\n@@ -1654,1 +1660,1 @@\n-  product(intx, VMThreadPriority, -1,                                       \\\n+  product(int, VMThreadPriority, -1,                                        \\\n@@ -1659,1 +1665,1 @@\n-  product(intx, JavaPriority1_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority1_To_OSPriority, -1,                             \\\n@@ -1663,1 +1669,1 @@\n-  product(intx, JavaPriority2_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority2_To_OSPriority, -1,                             \\\n@@ -1667,1 +1673,1 @@\n-  product(intx, JavaPriority3_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority3_To_OSPriority, -1,                             \\\n@@ -1671,1 +1677,1 @@\n-  product(intx, JavaPriority4_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority4_To_OSPriority, -1,                             \\\n@@ -1675,1 +1681,1 @@\n-  product(intx, JavaPriority5_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority5_To_OSPriority, -1,                             \\\n@@ -1679,1 +1685,1 @@\n-  product(intx, JavaPriority6_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority6_To_OSPriority, -1,                             \\\n@@ -1683,1 +1689,1 @@\n-  product(intx, JavaPriority7_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority7_To_OSPriority, -1,                             \\\n@@ -1687,1 +1693,1 @@\n-  product(intx, JavaPriority8_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority8_To_OSPriority, -1,                             \\\n@@ -1691,1 +1697,1 @@\n-  product(intx, JavaPriority9_To_OSPriority, -1,                            \\\n+  product(int, JavaPriority9_To_OSPriority, -1,                             \\\n@@ -1695,1 +1701,1 @@\n-  product(intx, JavaPriority10_To_OSPriority,-1,                            \\\n+  product(int, JavaPriority10_To_OSPriority,-1,                             \\\n@@ -1724,1 +1730,2 @@\n-          \"Maximum total size of NIO direct-buffer allocations\")            \\\n+          \"Maximum total size of NIO direct-buffer allocations. \"           \\\n+          \"Ignored if not explicitly set.\")                                 \\\n@@ -1774,1 +1781,1 @@\n-  product(intx, PerfDataMemorySize, 32*K,                                   \\\n+  product(int, PerfDataMemorySize, 32*K,                                    \\\n@@ -1779,1 +1786,1 @@\n-  product(intx, PerfMaxStringConstLength, 1024,                             \\\n+  product(int, PerfMaxStringConstLength, 1024,                              \\\n@@ -1789,1 +1796,1 @@\n-  product(intx, UnguardOnExecutionViolation, 0,                             \\\n+  product(int, UnguardOnExecutionViolation, 0,                              \\\n@@ -1837,1 +1844,1 @@\n-  product(uintx, SharedSymbolTableBucketSize, 4,                            \\\n+  product(uint, SharedSymbolTableBucketSize, 4,                             \\\n@@ -1853,0 +1860,3 @@\n+  product(bool, ShowCarrierFrames, false, DIAGNOSTIC,                       \\\n+          \"show virtual threads' carrier frames in exceptions\")             \\\n+                                                                            \\\n@@ -1960,1 +1970,1 @@\n-  product(intx, ArchiveRelocationMode, 0, DIAGNOSTIC,                       \\\n+  product(int, ArchiveRelocationMode, 0, DIAGNOSTIC,                        \\\n@@ -2011,0 +2021,20 @@\n+  product_pd(bool, VMContinuations, EXPERIMENTAL,                           \\\n+          \"Enable VM continuations support\")                                \\\n+                                                                            \\\n+  develop(bool, LoomDeoptAfterThaw, false,                                  \\\n+          \"Deopt stack after thaw\")                                         \\\n+                                                                            \\\n+  develop(bool, LoomVerifyAfterThaw, false,                                 \\\n+          \"Verify stack after thaw\")                                        \\\n+                                                                            \\\n+  develop(bool, VerifyContinuations, false,                                 \\\n+          \"Verify continuation consistency\")                                \\\n+                                                                            \\\n+  develop(bool, UseContinuationFastPath, true,                              \\\n+          \"Use fast-path frame walking in continuations\")                   \\\n+                                                                            \\\n+  product(intx, ExtentLocalCacheSize, 16,                                   \\\n+          \"Size of the cache for scoped values\")                            \\\n+           range(0, max_intx)                                               \\\n+           constraint(ExtentLocalCacheSizeConstraintFunc, AtParse)          \\\n+                                                                            \\\n@@ -2067,2 +2097,2 @@\n-  develop(bool, TraceOptimizedUpcallStubs, false,                              \\\n-                \"Trace optimized upcall stub generation\")                      \\\n+  develop(bool, TraceOptimizedUpcallStubs, false,                           \\\n+                \"Trace optimized upcall stub generation\")                   \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":64,"deletions":34,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -36,0 +36,5 @@\n+#define assert_handle_mark_nesting()                                                     \\\n+  assert(_handle_mark_nesting > 1, \"memory leak: allocating handle outside HandleMark\"); \\\n+  assert(_no_handle_mark_nesting == 0, \"allocating handle inside NoHandleMark\");         \\\n+\n+\n@@ -37,2 +42,1 @@\n-  assert(_handle_mark_nesting > 1, \"memory leak: allocating handle outside HandleMark\");\n-  assert(_no_handle_mark_nesting == 0, \"allocating handle inside NoHandleMark\");\n+  assert_handle_mark_nesting();\n@@ -42,0 +46,5 @@\n+\n+oop* HandleArea::allocate_null_handle() {\n+  assert_handle_mark_nesting();\n+  return real_allocate_handle(NULL);\n+}\n","filename":"src\/hotspot\/share\/runtime\/handles.cpp","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -104,0 +104,2 @@\n+\n+  inline void replace(oop obj);\n@@ -115,1 +117,1 @@\n-    type##Handle ()                              : Handle()                 {} \\\n+    type##Handle ()                              : Handle() {} \\\n@@ -117,1 +119,2 @@\n-    \\\n+    type##Handle (oop *handle, bool dummy)       : Handle(handle, dummy) {} \\\n+                                                 \\\n@@ -125,0 +128,1 @@\n+DEF_HANDLE(stackChunk       , is_stackChunk_noinline       )\n@@ -205,0 +209,1 @@\n+  oop* allocate_null_handle();\n@@ -207,0 +212,1 @@\n+  oop* allocate_null_handle()   { return allocate_handle(nullptr); }\n","filename":"src\/hotspot\/share\/runtime\/handles.hpp","additions":9,"deletions":3,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,1 +30,1 @@\n-#include \"runtime\/thread.hpp\"\n+#include \"runtime\/javaThread.hpp\"\n@@ -46,0 +46,8 @@\n+inline void Handle::replace(oop obj) {\n+  \/\/ Unlike in OopHandle::replace, we shouldn't use a barrier here.\n+  \/\/ OopHandle has its storage in OopStorage, which is walked concurrently and uses barriers.\n+  \/\/ Handle is thread private, and iterated by Thread::oops_do, which is why it shouldn't have any barriers at all.\n+  assert(_handle != NULL, \"should not use replace\");\n+  *_handle = obj;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/handles.inline.hpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -41,1 +41,1 @@\n-#include \"prims\/universalNativeInvoker.hpp\"\n+#include \"prims\/downcallLinker.hpp\"\n@@ -44,0 +44,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -70,0 +71,1 @@\n+void stubRoutines_initContinuationStubs();\n@@ -94,0 +96,2 @@\n+void continuations_init(); \/\/ depends on flags (UseCompressedOops) and barrier sets\n+\n@@ -128,0 +132,2 @@\n+  continuations_init(); \/\/ must precede continuation stub generation\n+  stubRoutines_initContinuationStubs(); \/\/ depends on continuations_init\n","filename":"src\/hotspot\/share\/runtime\/init.cpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -123,1 +124,1 @@\n-  \/\/ on sparc\/ia64 which will catch violations of the reseting of last_Java_frame\n+  \/\/ on sparc\/ia64 which will catch violations of the resetting of last_Java_frame\n@@ -393,1 +394,1 @@\n-  \/\/ When we reenter Java, we need to reenable the reserved\/yellow zone which\n+  \/\/ When we reenter Java, we need to re-enable the reserved\/yellow zone which\n","filename":"src\/hotspot\/share\/runtime\/javaCalls.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,2091 @@\n+\/*\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, Azul Systems, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"jvm.h\"\n+#include \"cds\/dynamicArchive.hpp\"\n+#include \"ci\/ciEnv.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n+#include \"classfile\/javaThreadStatus.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmClasses.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/scopeDesc.hpp\"\n+#include \"compiler\/compileTask.hpp\"\n+#include \"compiler\/compilerThread.hpp\"\n+#include \"gc\/shared\/oopStorage.hpp\"\n+#include \"gc\/shared\/oopStorageSet.hpp\"\n+#include \"gc\/shared\/tlab_globals.hpp\"\n+#include \"jfr\/jfrEvents.hpp\"\n+#include \"jvmtifiles\/jvmtiEnv.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logAsyncWriter.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/allocation.inline.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"memory\/universe.hpp\"\n+#include \"oops\/access.inline.hpp\"\n+#include \"oops\/inlineKlass.hpp\"\n+#include \"oops\/instanceKlass.hpp\"\n+#include \"oops\/klass.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"oops\/oopHandle.inline.hpp\"\n+#include \"oops\/verifyOopClosure.hpp\"\n+#include \"prims\/jvm_misc.hpp\"\n+#include \"prims\/jvmtiDeferredUpdates.hpp\"\n+#include \"prims\/jvmtiExport.hpp\"\n+#include \"prims\/jvmtiThreadState.inline.hpp\"\n+#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/continuation.hpp\"\n+#include \"runtime\/continuationEntry.inline.hpp\"\n+#include \"runtime\/continuationHelper.inline.hpp\"\n+#include \"runtime\/deoptimization.hpp\"\n+#include \"runtime\/frame.inline.hpp\"\n+#include \"runtime\/handles.inline.hpp\"\n+#include \"runtime\/handshake.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"runtime\/java.hpp\"\n+#include \"runtime\/javaCalls.hpp\"\n+#include \"runtime\/javaThread.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n+#include \"runtime\/orderAccess.hpp\"\n+#include \"runtime\/osThread.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/safepointMechanism.inline.hpp\"\n+#include \"runtime\/safepointVerifiers.hpp\"\n+#include \"runtime\/serviceThread.hpp\"\n+#include \"runtime\/stackFrameStream.inline.hpp\"\n+#include \"runtime\/stackWatermarkSet.hpp\"\n+#include \"runtime\/threadCritical.hpp\"\n+#include \"runtime\/threadSMR.inline.hpp\"\n+#include \"runtime\/threadStatisticalInfo.hpp\"\n+#include \"runtime\/threadWXSetters.inline.hpp\"\n+#include \"runtime\/timer.hpp\"\n+#include \"runtime\/timerTrace.hpp\"\n+#include \"runtime\/vframe.inline.hpp\"\n+#include \"runtime\/vframeArray.hpp\"\n+#include \"runtime\/vframe_hp.hpp\"\n+#include \"runtime\/vmThread.hpp\"\n+#include \"runtime\/vmOperations.hpp\"\n+#include \"services\/threadService.hpp\"\n+#include \"utilities\/copy.hpp\"\n+#include \"utilities\/defaultStream.hpp\"\n+#include \"utilities\/dtrace.hpp\"\n+#include \"utilities\/events.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#include \"utilities\/preserveException.hpp\"\n+#include \"utilities\/spinYield.hpp\"\n+#if INCLUDE_JVMCI\n+#include \"jvmci\/jvmci.hpp\"\n+#include \"jvmci\/jvmciEnv.hpp\"\n+#endif\n+#if INCLUDE_JFR\n+#include \"jfr\/jfr.hpp\"\n+#endif\n+\n+\/\/ Set by os layer.\n+size_t      JavaThread::_stack_size_at_create = 0;\n+\n+#ifdef DTRACE_ENABLED\n+\n+\/\/ Only bother with this argument setup if dtrace is available\n+\n+  #define HOTSPOT_THREAD_PROBE_start HOTSPOT_THREAD_START\n+  #define HOTSPOT_THREAD_PROBE_stop HOTSPOT_THREAD_STOP\n+\n+  #define DTRACE_THREAD_PROBE(probe, javathread)                           \\\n+    {                                                                      \\\n+      ResourceMark rm(this);                                               \\\n+      int len = 0;                                                         \\\n+      const char* name = (javathread)->name();                             \\\n+      len = strlen(name);                                                  \\\n+      HOTSPOT_THREAD_PROBE_##probe(\/* probe = start, stop *\/               \\\n+        (char *) name, len,                                                \\\n+        java_lang_Thread::thread_id((javathread)->threadObj()),            \\\n+        (uintptr_t) (javathread)->osthread()->thread_id(),                 \\\n+        java_lang_Thread::is_daemon((javathread)->threadObj()));           \\\n+    }\n+\n+#else \/\/  ndef DTRACE_ENABLED\n+\n+  #define DTRACE_THREAD_PROBE(probe, javathread)\n+\n+#endif \/\/ ndef DTRACE_ENABLED\n+\n+void JavaThread::smr_delete() {\n+  if (_on_thread_list) {\n+    ThreadsSMRSupport::smr_delete(this);\n+  } else {\n+    delete this;\n+  }\n+}\n+\n+\/\/ Initialized by VMThread at vm_global_init\n+OopStorage* JavaThread::_thread_oop_storage = NULL;\n+\n+OopStorage* JavaThread::thread_oop_storage() {\n+  assert(_thread_oop_storage != NULL, \"not yet initialized\");\n+  return _thread_oop_storage;\n+}\n+\n+void JavaThread::set_threadOopHandles(oop p) {\n+  assert(_thread_oop_storage != NULL, \"not yet initialized\");\n+  _threadObj   = OopHandle(_thread_oop_storage, p);\n+  _vthread     = OopHandle(_thread_oop_storage, p);\n+  _jvmti_vthread = OopHandle(_thread_oop_storage, NULL);\n+  _extentLocalCache = OopHandle(_thread_oop_storage, NULL);\n+}\n+\n+oop JavaThread::threadObj() const {\n+  Thread* current = Thread::current_or_null_safe();\n+  assert(current != nullptr, \"cannot be called by a detached thread\");\n+  guarantee(current != this || JavaThread::cast(current)->is_oop_safe(),\n+            \"current cannot touch oops after its GC barrier is detached.\");\n+  return _threadObj.resolve();\n+}\n+\n+oop JavaThread::vthread() const {\n+  return _vthread.resolve();\n+}\n+\n+void JavaThread::set_vthread(oop p) {\n+  assert(_thread_oop_storage != NULL, \"not yet initialized\");\n+  _vthread.replace(p);\n+}\n+\n+oop JavaThread::jvmti_vthread() const {\n+  return _jvmti_vthread.resolve();\n+}\n+\n+void JavaThread::set_jvmti_vthread(oop p) {\n+  assert(_thread_oop_storage != NULL, \"not yet initialized\");\n+  _jvmti_vthread.replace(p);\n+}\n+\n+oop JavaThread::extentLocalCache() const {\n+  return _extentLocalCache.resolve();\n+}\n+\n+void JavaThread::set_extentLocalCache(oop p) {\n+  assert(_thread_oop_storage != NULL, \"not yet initialized\");\n+  _extentLocalCache.replace(p);\n+}\n+\n+void JavaThread::allocate_threadObj(Handle thread_group, const char* thread_name,\n+                                    bool daemon, TRAPS) {\n+  assert(thread_group.not_null(), \"thread group should be specified\");\n+  assert(threadObj() == NULL, \"should only create Java thread object once\");\n+\n+  InstanceKlass* ik = vmClasses::Thread_klass();\n+  assert(ik->is_initialized(), \"must be\");\n+  instanceHandle thread_oop = ik->allocate_instance_handle(CHECK);\n+\n+  \/\/ We are called from jni_AttachCurrentThread\/jni_AttachCurrentThreadAsDaemon.\n+  \/\/ We cannot use JavaCalls::construct_new_instance because the java.lang.Thread\n+  \/\/ constructor calls Thread.current(), which must be set here.\n+  java_lang_Thread::set_thread(thread_oop(), this);\n+  set_threadOopHandles(thread_oop());\n+\n+  JavaValue result(T_VOID);\n+  if (thread_name != NULL) {\n+    Handle name = java_lang_String::create_from_str(thread_name, CHECK);\n+    \/\/ Thread gets assigned specified name and null target\n+    JavaCalls::call_special(&result,\n+                            thread_oop,\n+                            ik,\n+                            vmSymbols::object_initializer_name(),\n+                            vmSymbols::threadgroup_string_void_signature(),\n+                            thread_group,\n+                            name,\n+                            THREAD);\n+  } else {\n+    \/\/ Thread gets assigned name \"Thread-nnn\" and null target\n+    \/\/ (java.lang.Thread doesn't have a constructor taking only a ThreadGroup argument)\n+    JavaCalls::call_special(&result,\n+                            thread_oop,\n+                            ik,\n+                            vmSymbols::object_initializer_name(),\n+                            vmSymbols::threadgroup_runnable_void_signature(),\n+                            thread_group,\n+                            Handle(),\n+                            THREAD);\n+  }\n+  os::set_priority(this, NormPriority);\n+\n+  if (daemon) {\n+    java_lang_Thread::set_daemon(thread_oop());\n+  }\n+}\n+\n+\/\/ ======= JavaThread ========\n+\n+#if INCLUDE_JVMCI\n+\n+jlong* JavaThread::_jvmci_old_thread_counters;\n+\n+bool jvmci_counters_include(JavaThread* thread) {\n+  return !JVMCICountersExcludeCompiler || !thread->is_Compiler_thread();\n+}\n+\n+void JavaThread::collect_counters(jlong* array, int length) {\n+  assert(length == JVMCICounterSize, \"wrong value\");\n+  for (int i = 0; i < length; i++) {\n+    array[i] = _jvmci_old_thread_counters[i];\n+  }\n+  for (JavaThread* tp : ThreadsListHandle()) {\n+    if (jvmci_counters_include(tp)) {\n+      for (int i = 0; i < length; i++) {\n+        array[i] += tp->_jvmci_counters[i];\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Attempt to enlarge the array for per thread counters.\n+jlong* resize_counters_array(jlong* old_counters, int current_size, int new_size) {\n+  jlong* new_counters = NEW_C_HEAP_ARRAY_RETURN_NULL(jlong, new_size, mtJVMCI);\n+  if (new_counters == NULL) {\n+    return NULL;\n+  }\n+  if (old_counters == NULL) {\n+    old_counters = new_counters;\n+    memset(old_counters, 0, sizeof(jlong) * new_size);\n+  } else {\n+    for (int i = 0; i < MIN2((int) current_size, new_size); i++) {\n+      new_counters[i] = old_counters[i];\n+    }\n+    if (new_size > current_size) {\n+      memset(new_counters + current_size, 0, sizeof(jlong) * (new_size - current_size));\n+    }\n+    FREE_C_HEAP_ARRAY(jlong, old_counters);\n+  }\n+  return new_counters;\n+}\n+\n+\/\/ Attempt to enlarge the array for per thread counters.\n+bool JavaThread::resize_counters(int current_size, int new_size) {\n+  jlong* new_counters = resize_counters_array(_jvmci_counters, current_size, new_size);\n+  if (new_counters == NULL) {\n+    return false;\n+  } else {\n+    _jvmci_counters = new_counters;\n+    return true;\n+  }\n+}\n+\n+class VM_JVMCIResizeCounters : public VM_Operation {\n+ private:\n+  int _new_size;\n+  bool _failed;\n+\n+ public:\n+  VM_JVMCIResizeCounters(int new_size) : _new_size(new_size), _failed(false) { }\n+  VMOp_Type type()                  const        { return VMOp_JVMCIResizeCounters; }\n+  bool allow_nested_vm_operations() const        { return true; }\n+  void doit() {\n+    \/\/ Resize the old thread counters array\n+    jlong* new_counters = resize_counters_array(JavaThread::_jvmci_old_thread_counters, JVMCICounterSize, _new_size);\n+    if (new_counters == NULL) {\n+      _failed = true;\n+      return;\n+    } else {\n+      JavaThread::_jvmci_old_thread_counters = new_counters;\n+    }\n+\n+    \/\/ Now resize each threads array\n+    for (JavaThread* tp : ThreadsListHandle()) {\n+      if (!tp->resize_counters(JVMCICounterSize, _new_size)) {\n+        _failed = true;\n+        break;\n+      }\n+    }\n+    if (!_failed) {\n+      JVMCICounterSize = _new_size;\n+    }\n+  }\n+\n+  bool failed() { return _failed; }\n+};\n+\n+bool JavaThread::resize_all_jvmci_counters(int new_size) {\n+  VM_JVMCIResizeCounters op(new_size);\n+  VMThread::execute(&op);\n+  return !op.failed();\n+}\n+\n+#endif \/\/ INCLUDE_JVMCI\n+\n+#ifdef ASSERT\n+\/\/ Checks safepoint allowed and clears unhandled oops at potential safepoints.\n+void JavaThread::check_possible_safepoint() {\n+  if (_no_safepoint_count > 0) {\n+    print_owned_locks();\n+    assert(false, \"Possible safepoint reached by thread that does not allow it\");\n+  }\n+#ifdef CHECK_UNHANDLED_OOPS\n+  \/\/ Clear unhandled oops in JavaThreads so we get a crash right away.\n+  clear_unhandled_oops();\n+#endif \/\/ CHECK_UNHANDLED_OOPS\n+\n+  \/\/ Macos\/aarch64 should be in the right state for safepoint (e.g.\n+  \/\/ deoptimization needs WXWrite).  Crashes caused by the wrong state rarely\n+  \/\/ happens in practice, making such issues hard to find and reproduce.\n+#if defined(__APPLE__) && defined(AARCH64)\n+  if (AssertWXAtThreadSync) {\n+    assert_wx_state(WXWrite);\n+  }\n+#endif\n+}\n+\n+void JavaThread::check_for_valid_safepoint_state() {\n+  \/\/ Check NoSafepointVerifier, which is implied by locks taken that can be\n+  \/\/ shared with the VM thread.  This makes sure that no locks with allow_vm_block\n+  \/\/ are held.\n+  check_possible_safepoint();\n+\n+  if (thread_state() != _thread_in_vm) {\n+    fatal(\"LEAF method calling lock?\");\n+  }\n+\n+  if (GCALotAtAllSafepoints) {\n+    \/\/ We could enter a safepoint here and thus have a gc\n+    InterfaceSupport::check_gc_alot();\n+  }\n+}\n+#endif \/\/ ASSERT\n+\n+\/\/ A JavaThread is a normal Java thread\n+\n+JavaThread::JavaThread() :\n+  \/\/ Initialize fields\n+\n+  _in_asgct(false),\n+  _on_thread_list(false),\n+  DEBUG_ONLY(_java_call_counter(0) COMMA)\n+  _entry_point(nullptr),\n+  _deopt_mark(nullptr),\n+  _deopt_nmethod(nullptr),\n+  _vframe_array_head(nullptr),\n+  _vframe_array_last(nullptr),\n+  _jvmti_deferred_updates(nullptr),\n+  _callee_target(nullptr),\n+  _vm_result(nullptr),\n+  _vm_result_2(nullptr),\n+\n+  _current_pending_monitor(NULL),\n+  _current_pending_monitor_is_from_java(true),\n+  _current_waiting_monitor(NULL),\n+  _active_handles(NULL),\n+  _free_handle_block(NULL),\n+  _Stalled(0),\n+\n+  _monitor_chunks(nullptr),\n+\n+  _suspend_flags(0),\n+\n+  _thread_state(_thread_new),\n+  _saved_exception_pc(nullptr),\n+#ifdef ASSERT\n+  _no_safepoint_count(0),\n+  _visited_for_critical_count(false),\n+#endif\n+\n+  _terminated(_not_terminated),\n+  _in_deopt_handler(0),\n+  _doing_unsafe_access(false),\n+  _do_not_unlock_if_synchronized(false),\n+#if INCLUDE_JVMTI\n+  _carrier_thread_suspended(false),\n+  _is_in_VTMS_transition(false),\n+#ifdef ASSERT\n+  _is_VTMS_transition_disabler(false),\n+#endif\n+#endif\n+  _jni_attach_state(_not_attaching_via_jni),\n+#if INCLUDE_JVMCI\n+  _pending_deoptimization(-1),\n+  _pending_monitorenter(false),\n+  _pending_transfer_to_interpreter(false),\n+  _in_retryable_allocation(false),\n+  _pending_failed_speculation(0),\n+  _jvmci{nullptr},\n+  _libjvmci_runtime(nullptr),\n+  _jvmci_counters(nullptr),\n+  _jvmci_reserved0(0),\n+  _jvmci_reserved1(0),\n+  _jvmci_reserved_oop0(nullptr),\n+#endif \/\/ INCLUDE_JVMCI\n+\n+  _exception_oop(oop()),\n+  _exception_pc(0),\n+  _exception_handler_pc(0),\n+  _is_method_handle_return(0),\n+\n+  _jni_active_critical(0),\n+  _pending_jni_exception_check_fn(nullptr),\n+  _depth_first_number(0),\n+\n+  \/\/ JVMTI PopFrame support\n+  _popframe_condition(popframe_inactive),\n+  _frames_to_pop_failed_realloc(0),\n+\n+  _cont_entry(nullptr),\n+  _cont_fastpath(0),\n+  _cont_fastpath_thread_state(1),\n+  _held_monitor_count(0),\n+  _jni_monitor_count(0),\n+\n+  _handshake(this),\n+\n+  _popframe_preserved_args(nullptr),\n+  _popframe_preserved_args_size(0),\n+\n+  _jvmti_thread_state(nullptr),\n+  _interp_only_mode(0),\n+  _should_post_on_exceptions_flag(JNI_FALSE),\n+  _thread_stat(new ThreadStatistics()),\n+\n+  _parker(),\n+\n+  _class_to_be_initialized(nullptr),\n+\n+  _SleepEvent(ParkEvent::Allocate(this))\n+{\n+  set_jni_functions(jni_functions());\n+\n+#if INCLUDE_JVMCI\n+  assert(_jvmci._implicit_exception_pc == nullptr, \"must be\");\n+  if (JVMCICounterSize > 0) {\n+    resize_counters(0, (int) JVMCICounterSize);\n+  }\n+#endif \/\/ INCLUDE_JVMCI\n+\n+  \/\/ Setup safepoint state info for this thread\n+  ThreadSafepointState::create(this);\n+\n+  SafepointMechanism::initialize_header(this);\n+\n+  set_requires_cross_modify_fence(false);\n+\n+  pd_initialize();\n+  assert(deferred_card_mark().is_empty(), \"Default MemRegion ctor\");\n+}\n+\n+JavaThread::JavaThread(bool is_attaching_via_jni) : JavaThread() {\n+  if (is_attaching_via_jni) {\n+    _jni_attach_state = _attaching_via_jni;\n+  }\n+}\n+\n+\n+\/\/ interrupt support\n+\n+void JavaThread::interrupt() {\n+  \/\/ All callers should have 'this' thread protected by a\n+  \/\/ ThreadsListHandle so that it cannot terminate and deallocate\n+  \/\/ itself.\n+  debug_only(check_for_dangling_thread_pointer(this);)\n+\n+  \/\/ For Windows _interrupt_event\n+  WINDOWS_ONLY(osthread()->set_interrupted(true);)\n+\n+  \/\/ For Thread.sleep\n+  _SleepEvent->unpark();\n+\n+  \/\/ For JSR166 LockSupport.park\n+  parker()->unpark();\n+\n+  \/\/ For ObjectMonitor and JvmtiRawMonitor\n+  _ParkEvent->unpark();\n+}\n+\n+\n+bool JavaThread::is_interrupted(bool clear_interrupted) {\n+  debug_only(check_for_dangling_thread_pointer(this);)\n+\n+  if (_threadObj.peek() == NULL) {\n+    \/\/ If there is no j.l.Thread then it is impossible to have\n+    \/\/ been interrupted. We can find NULL during VM initialization\n+    \/\/ or when a JNI thread is still in the process of attaching.\n+    \/\/ In such cases this must be the current thread.\n+    assert(this == Thread::current(), \"invariant\");\n+    return false;\n+  }\n+\n+  bool interrupted = java_lang_Thread::interrupted(threadObj());\n+\n+  \/\/ NOTE that since there is no \"lock\" around the interrupt and\n+  \/\/ is_interrupted operations, there is the possibility that the\n+  \/\/ interrupted flag will be \"false\" but that the\n+  \/\/ low-level events will be in the signaled state. This is\n+  \/\/ intentional. The effect of this is that Object.wait() and\n+  \/\/ LockSupport.park() will appear to have a spurious wakeup, which\n+  \/\/ is allowed and not harmful, and the possibility is so rare that\n+  \/\/ it is not worth the added complexity to add yet another lock.\n+  \/\/ For the sleep event an explicit reset is performed on entry\n+  \/\/ to JavaThread::sleep, so there is no early return. It has also been\n+  \/\/ recommended not to put the interrupted flag into the \"event\"\n+  \/\/ structure because it hides the issue.\n+  \/\/ Also, because there is no lock, we must only clear the interrupt\n+  \/\/ state if we are going to report that we were interrupted; otherwise\n+  \/\/ an interrupt that happens just after we read the field would be lost.\n+  if (interrupted && clear_interrupted) {\n+    assert(this == Thread::current(), \"only the current thread can clear\");\n+    java_lang_Thread::set_interrupted(threadObj(), false);\n+    WINDOWS_ONLY(osthread()->set_interrupted(false);)\n+  }\n+\n+  return interrupted;\n+}\n+\n+void JavaThread::block_if_vm_exited() {\n+  if (_terminated == _vm_exited) {\n+    \/\/ _vm_exited is set at safepoint, and Threads_lock is never released\n+    \/\/ so we will block here forever.\n+    \/\/ Here we can be doing a jump from a safe state to an unsafe state without\n+    \/\/ proper transition, but it happens after the final safepoint has begun so\n+    \/\/ this jump won't cause any safepoint problems.\n+    set_thread_state(_thread_in_vm);\n+    Threads_lock->lock();\n+    ShouldNotReachHere();\n+  }\n+}\n+\n+JavaThread::JavaThread(ThreadFunction entry_point, size_t stack_sz) : JavaThread() {\n+  _jni_attach_state = _not_attaching_via_jni;\n+  set_entry_point(entry_point);\n+  \/\/ Create the native thread itself.\n+  \/\/ %note runtime_23\n+  os::ThreadType thr_type = os::java_thread;\n+  thr_type = entry_point == &CompilerThread::thread_entry ? os::compiler_thread :\n+                                                            os::java_thread;\n+  os::create_thread(this, thr_type, stack_sz);\n+  \/\/ The _osthread may be NULL here because we ran out of memory (too many threads active).\n+  \/\/ We need to throw and OutOfMemoryError - however we cannot do this here because the caller\n+  \/\/ may hold a lock and all locks must be unlocked before throwing the exception (throwing\n+  \/\/ the exception consists of creating the exception object & initializing it, initialization\n+  \/\/ will leave the VM via a JavaCall and then all locks must be unlocked).\n+  \/\/\n+  \/\/ The thread is still suspended when we reach here. Thread must be explicit started\n+  \/\/ by creator! Furthermore, the thread must also explicitly be added to the Threads list\n+  \/\/ by calling Threads:add. The reason why this is not done here, is because the thread\n+  \/\/ object must be fully initialized (take a look at JVM_Start)\n+}\n+\n+JavaThread::~JavaThread() {\n+\n+  \/\/ Ask ServiceThread to release the threadObj OopHandle\n+  ServiceThread::add_oop_handle_release(_threadObj);\n+  ServiceThread::add_oop_handle_release(_vthread);\n+  ServiceThread::add_oop_handle_release(_jvmti_vthread);\n+\n+  \/\/ Return the sleep event to the free list\n+  ParkEvent::Release(_SleepEvent);\n+  _SleepEvent = NULL;\n+\n+  \/\/ Free any remaining  previous UnrollBlock\n+  vframeArray* old_array = vframe_array_last();\n+\n+  if (old_array != NULL) {\n+    Deoptimization::UnrollBlock* old_info = old_array->unroll_block();\n+    old_array->set_unroll_block(NULL);\n+    delete old_info;\n+    delete old_array;\n+  }\n+\n+  JvmtiDeferredUpdates* updates = deferred_updates();\n+  if (updates != NULL) {\n+    \/\/ This can only happen if thread is destroyed before deoptimization occurs.\n+    assert(updates->count() > 0, \"Updates holder not deleted\");\n+    \/\/ free deferred updates.\n+    delete updates;\n+    set_deferred_updates(NULL);\n+  }\n+\n+  \/\/ All Java related clean up happens in exit\n+  ThreadSafepointState::destroy(this);\n+  if (_thread_stat != NULL) delete _thread_stat;\n+\n+#if INCLUDE_JVMCI\n+  if (JVMCICounterSize > 0) {\n+    FREE_C_HEAP_ARRAY(jlong, _jvmci_counters);\n+  }\n+#endif \/\/ INCLUDE_JVMCI\n+}\n+\n+\n+\/\/ First JavaThread specific code executed by a new Java thread.\n+void JavaThread::pre_run() {\n+  \/\/ empty - see comments in run()\n+}\n+\n+\/\/ The main routine called by a new Java thread. This isn't overridden\n+\/\/ by subclasses, instead different subclasses define a different \"entry_point\"\n+\/\/ which defines the actual logic for that kind of thread.\n+void JavaThread::run() {\n+  \/\/ initialize thread-local alloc buffer related fields\n+  initialize_tlab();\n+\n+  _stack_overflow_state.create_stack_guard_pages();\n+\n+  cache_global_variables();\n+\n+  \/\/ Thread is now sufficiently initialized to be handled by the safepoint code as being\n+  \/\/ in the VM. Change thread state from _thread_new to _thread_in_vm\n+  assert(this->thread_state() == _thread_new, \"wrong thread state\");\n+  set_thread_state(_thread_in_vm);\n+\n+  \/\/ Before a thread is on the threads list it is always safe, so after leaving the\n+  \/\/ _thread_new we should emit a instruction barrier. The distance to modified code\n+  \/\/ from here is probably far enough, but this is consistent and safe.\n+  OrderAccess::cross_modify_fence();\n+\n+  assert(JavaThread::current() == this, \"sanity check\");\n+  assert(!Thread::current()->owns_locks(), \"sanity check\");\n+\n+  DTRACE_THREAD_PROBE(start, this);\n+\n+  \/\/ This operation might block. We call that after all safepoint checks for a new thread has\n+  \/\/ been completed.\n+  set_active_handles(JNIHandleBlock::allocate_block());\n+\n+  if (JvmtiExport::should_post_thread_life()) {\n+    JvmtiExport::post_thread_start(this);\n+\n+  }\n+\n+  \/\/ We call another function to do the rest so we are sure that the stack addresses used\n+  \/\/ from there will be lower than the stack base just computed.\n+  thread_main_inner();\n+}\n+\n+void JavaThread::thread_main_inner() {\n+  assert(JavaThread::current() == this, \"sanity check\");\n+  assert(_threadObj.peek() != NULL, \"just checking\");\n+\n+  \/\/ Execute thread entry point unless this thread has a pending exception\n+  \/\/ or has been stopped before starting.\n+  \/\/ Note: Due to JVM_StopThread we can have pending exceptions already!\n+  if (!this->has_pending_exception() &&\n+      !java_lang_Thread::is_stillborn(this->threadObj())) {\n+    {\n+      ResourceMark rm(this);\n+      this->set_native_thread_name(this->name());\n+    }\n+    HandleMark hm(this);\n+    this->entry_point()(this, this);\n+  }\n+\n+  DTRACE_THREAD_PROBE(stop, this);\n+\n+  \/\/ Cleanup is handled in post_run()\n+}\n+\n+\/\/ Shared teardown for all JavaThreads\n+void JavaThread::post_run() {\n+  this->exit(false);\n+  this->unregister_thread_stack_with_NMT();\n+  \/\/ Defer deletion to here to ensure 'this' is still referenceable in call_run\n+  \/\/ for any shared tear-down.\n+  this->smr_delete();\n+}\n+\n+static void ensure_join(JavaThread* thread) {\n+  \/\/ We do not need to grab the Threads_lock, since we are operating on ourself.\n+  Handle threadObj(thread, thread->threadObj());\n+  assert(threadObj.not_null(), \"java thread object must exist\");\n+  ObjectLocker lock(threadObj, thread);\n+  \/\/ Ignore pending exception (ThreadDeath), since we are exiting anyway\n+  thread->clear_pending_exception();\n+  \/\/ Thread is exiting. So set thread_status field in  java.lang.Thread class to TERMINATED.\n+  java_lang_Thread::set_thread_status(threadObj(), JavaThreadStatus::TERMINATED);\n+  \/\/ Clear the native thread instance - this makes isAlive return false and allows the join()\n+  \/\/ to complete once we've done the notify_all below\n+  java_lang_Thread::set_thread(threadObj(), NULL);\n+  lock.notify_all(thread);\n+  \/\/ Ignore pending exception (ThreadDeath), since we are exiting anyway\n+  thread->clear_pending_exception();\n+}\n+\n+static bool is_daemon(oop threadObj) {\n+  return (threadObj != NULL && java_lang_Thread::is_daemon(threadObj));\n+}\n+\n+\/\/ For any new cleanup additions, please check to see if they need to be applied to\n+\/\/ cleanup_failed_attach_current_thread as well.\n+void JavaThread::exit(bool destroy_vm, ExitType exit_type) {\n+  assert(this == JavaThread::current(), \"thread consistency check\");\n+  assert(!is_exiting(), \"should not be exiting or terminated already\");\n+\n+  elapsedTimer _timer_exit_phase1;\n+  elapsedTimer _timer_exit_phase2;\n+  elapsedTimer _timer_exit_phase3;\n+  elapsedTimer _timer_exit_phase4;\n+\n+  if (log_is_enabled(Debug, os, thread, timer)) {\n+    _timer_exit_phase1.start();\n+  }\n+\n+  HandleMark hm(this);\n+  Handle uncaught_exception(this, this->pending_exception());\n+  this->clear_pending_exception();\n+  Handle threadObj(this, this->threadObj());\n+  assert(threadObj.not_null(), \"Java thread object should be created\");\n+\n+  if (!destroy_vm) {\n+    if (uncaught_exception.not_null()) {\n+      EXCEPTION_MARK;\n+      \/\/ Call method Thread.dispatchUncaughtException().\n+      Klass* thread_klass = vmClasses::Thread_klass();\n+      JavaValue result(T_VOID);\n+      JavaCalls::call_virtual(&result,\n+                              threadObj, thread_klass,\n+                              vmSymbols::dispatchUncaughtException_name(),\n+                              vmSymbols::throwable_void_signature(),\n+                              uncaught_exception,\n+                              THREAD);\n+      if (HAS_PENDING_EXCEPTION) {\n+        ResourceMark rm(this);\n+        jio_fprintf(defaultStream::error_stream(),\n+                    \"\\nException: %s thrown from the UncaughtExceptionHandler\"\n+                    \" in thread \\\"%s\\\"\\n\",\n+                    pending_exception()->klass()->external_name(),\n+                    name());\n+        CLEAR_PENDING_EXCEPTION;\n+      }\n+    }\n+\n+    if (!is_Compiler_thread()) {\n+      \/\/ We have finished executing user-defined Java code and now have to do the\n+      \/\/ implementation specific clean-up by calling Thread.exit(). We prevent any\n+      \/\/ asynchronous exceptions from being delivered while in Thread.exit()\n+      \/\/ to ensure the clean-up is not corrupted.\n+      NoAsyncExceptionDeliveryMark _no_async(this);\n+\n+      EXCEPTION_MARK;\n+      JavaValue result(T_VOID);\n+      Klass* thread_klass = vmClasses::Thread_klass();\n+      JavaCalls::call_virtual(&result,\n+                              threadObj, thread_klass,\n+                              vmSymbols::exit_method_name(),\n+                              vmSymbols::void_method_signature(),\n+                              THREAD);\n+      CLEAR_PENDING_EXCEPTION;\n+    }\n+\n+    \/\/ notify JVMTI\n+    if (JvmtiExport::should_post_thread_life()) {\n+      JvmtiExport::post_thread_end(this);\n+    }\n+  } else {\n+    \/\/ before_exit() has already posted JVMTI THREAD_END events\n+  }\n+\n+  \/\/ Cleanup any pending async exception now since we cannot access oops after\n+  \/\/ BarrierSet::barrier_set()->on_thread_detach() has been executed.\n+  if (has_async_exception_condition()) {\n+    handshake_state()->clean_async_exception_operation();\n+  }\n+\n+  \/\/ The careful dance between thread suspension and exit is handled here.\n+  \/\/ Since we are in thread_in_vm state and suspension is done with handshakes,\n+  \/\/ we can just put in the exiting state and it will be correctly handled.\n+  \/\/ Also, no more async exceptions will be added to the queue after this point.\n+  set_terminated(_thread_exiting);\n+  ThreadService::current_thread_exiting(this, is_daemon(threadObj()));\n+\n+  if (log_is_enabled(Debug, os, thread, timer)) {\n+    _timer_exit_phase1.stop();\n+    _timer_exit_phase2.start();\n+  }\n+\n+  \/\/ Capture daemon status before the thread is marked as terminated.\n+  bool daemon = is_daemon(threadObj());\n+\n+  \/\/ Notify waiters on thread object. This has to be done after exit() is called\n+  \/\/ on the thread (if the thread is the last thread in a daemon ThreadGroup the\n+  \/\/ group should have the destroyed bit set before waiters are notified).\n+  ensure_join(this);\n+  assert(!this->has_pending_exception(), \"ensure_join should have cleared\");\n+\n+  if (log_is_enabled(Debug, os, thread, timer)) {\n+    _timer_exit_phase2.stop();\n+    _timer_exit_phase3.start();\n+  }\n+  \/\/ 6282335 JNI DetachCurrentThread spec states that all Java monitors\n+  \/\/ held by this thread must be released. The spec does not distinguish\n+  \/\/ between JNI-acquired and regular Java monitors. We can only see\n+  \/\/ regular Java monitors here if monitor enter-exit matching is broken.\n+  \/\/\n+  \/\/ ensure_join() ignores IllegalThreadStateExceptions, and so does\n+  \/\/ ObjectSynchronizer::release_monitors_owned_by_thread().\n+  if (exit_type == jni_detach) {\n+    \/\/ Sanity check even though JNI DetachCurrentThread() would have\n+    \/\/ returned JNI_ERR if there was a Java frame. JavaThread exit\n+    \/\/ should be done executing Java code by the time we get here.\n+    assert(!this->has_last_Java_frame(),\n+           \"should not have a Java frame when detaching or exiting\");\n+    ObjectSynchronizer::release_monitors_owned_by_thread(this);\n+    assert(!this->has_pending_exception(), \"release_monitors should have cleared\");\n+  }\n+\n+  \/\/ Since above code may not release JNI monitors and if someone forgot to do an\n+  \/\/ JNI monitorexit, held count should be equal jni count.\n+  \/\/ Consider scan all object monitor for this owner if JNI count > 0 (at least on detach).\n+  assert(this->held_monitor_count() == this->jni_monitor_count(),\n+         \"held monitor count should be equal to jni: \" INT64_FORMAT \" != \" INT64_FORMAT,\n+         (int64_t)this->held_monitor_count(), (int64_t)this->jni_monitor_count());\n+  if (CheckJNICalls && this->jni_monitor_count() > 0) {\n+    \/\/ We would like a fatal here, but due to we never checked this before there\n+    \/\/ is a lot of tests which breaks, even with an error log.\n+    log_debug(jni)(\"JavaThread %s (tid: \" UINTX_FORMAT \") with Objects still locked by JNI MonitorEnter.\",\n+      exit_type == JavaThread::normal_exit ? \"exiting\" : \"detaching\", os::current_thread_id());\n+  }\n+\n+  \/\/ These things needs to be done while we are still a Java Thread. Make sure that thread\n+  \/\/ is in a consistent state, in case GC happens\n+  JFR_ONLY(Jfr::on_thread_exit(this);)\n+\n+  if (active_handles() != NULL) {\n+    JNIHandleBlock* block = active_handles();\n+    set_active_handles(NULL);\n+    JNIHandleBlock::release_block(block);\n+  }\n+\n+  if (free_handle_block() != NULL) {\n+    JNIHandleBlock* block = free_handle_block();\n+    set_free_handle_block(NULL);\n+    JNIHandleBlock::release_block(block);\n+  }\n+\n+  \/\/ These have to be removed while this is still a valid thread.\n+  _stack_overflow_state.remove_stack_guard_pages();\n+\n+  if (UseTLAB) {\n+    tlab().retire();\n+  }\n+\n+  if (JvmtiEnv::environments_might_exist()) {\n+    JvmtiExport::cleanup_thread(this);\n+  }\n+\n+  \/\/ We need to cache the thread name for logging purposes below as once\n+  \/\/ we have called on_thread_detach this thread must not access any oops.\n+  char* thread_name = NULL;\n+  if (log_is_enabled(Debug, os, thread, timer)) {\n+    ResourceMark rm(this);\n+    thread_name = os::strdup(name());\n+  }\n+\n+  log_info(os, thread)(\"JavaThread %s (tid: \" UINTX_FORMAT \").\",\n+    exit_type == JavaThread::normal_exit ? \"exiting\" : \"detaching\",\n+    os::current_thread_id());\n+\n+  if (log_is_enabled(Debug, os, thread, timer)) {\n+    _timer_exit_phase3.stop();\n+    _timer_exit_phase4.start();\n+  }\n+\n+#if INCLUDE_JVMCI\n+  if (JVMCICounterSize > 0) {\n+    if (jvmci_counters_include(this)) {\n+      for (int i = 0; i < JVMCICounterSize; i++) {\n+        _jvmci_old_thread_counters[i] += _jvmci_counters[i];\n+      }\n+    }\n+  }\n+#endif \/\/ INCLUDE_JVMCI\n+\n+  \/\/ Remove from list of active threads list, and notify VM thread if we are the last non-daemon thread.\n+  \/\/ We call BarrierSet::barrier_set()->on_thread_detach() here so no touching of oops after this point.\n+  Threads::remove(this, daemon);\n+\n+  if (log_is_enabled(Debug, os, thread, timer)) {\n+    _timer_exit_phase4.stop();\n+    log_debug(os, thread, timer)(\"name='%s'\"\n+                                 \", exit-phase1=\" JLONG_FORMAT\n+                                 \", exit-phase2=\" JLONG_FORMAT\n+                                 \", exit-phase3=\" JLONG_FORMAT\n+                                 \", exit-phase4=\" JLONG_FORMAT,\n+                                 thread_name,\n+                                 _timer_exit_phase1.milliseconds(),\n+                                 _timer_exit_phase2.milliseconds(),\n+                                 _timer_exit_phase3.milliseconds(),\n+                                 _timer_exit_phase4.milliseconds());\n+    os::free(thread_name);\n+  }\n+}\n+\n+void JavaThread::cleanup_failed_attach_current_thread(bool is_daemon) {\n+  if (active_handles() != NULL) {\n+    JNIHandleBlock* block = active_handles();\n+    set_active_handles(NULL);\n+    JNIHandleBlock::release_block(block);\n+  }\n+\n+  if (free_handle_block() != NULL) {\n+    JNIHandleBlock* block = free_handle_block();\n+    set_free_handle_block(NULL);\n+    JNIHandleBlock::release_block(block);\n+  }\n+\n+  \/\/ These have to be removed while this is still a valid thread.\n+  _stack_overflow_state.remove_stack_guard_pages();\n+\n+  if (UseTLAB) {\n+    tlab().retire();\n+  }\n+\n+  Threads::remove(this, is_daemon);\n+  this->smr_delete();\n+}\n+\n+JavaThread* JavaThread::active() {\n+  Thread* thread = Thread::current();\n+  if (thread->is_Java_thread()) {\n+    return JavaThread::cast(thread);\n+  } else {\n+    assert(thread->is_VM_thread(), \"this must be a vm thread\");\n+    VM_Operation* op = ((VMThread*) thread)->vm_operation();\n+    JavaThread *ret = op == NULL ? NULL : JavaThread::cast(op->calling_thread());\n+    return ret;\n+  }\n+}\n+\n+bool JavaThread::is_lock_owned(address adr) const {\n+  if (Thread::is_lock_owned(adr)) return true;\n+\n+  for (MonitorChunk* chunk = monitor_chunks(); chunk != NULL; chunk = chunk->next()) {\n+    if (chunk->contains(adr)) return true;\n+  }\n+\n+  return false;\n+}\n+\n+bool JavaThread::is_lock_owned_current(address adr) const {\n+  address stack_end = _stack_base - _stack_size;\n+  const ContinuationEntry* ce = vthread_continuation();\n+  address stack_base = ce != nullptr ? (address)ce->entry_sp() : _stack_base;\n+  if (stack_base > adr && adr >= stack_end) {\n+    return true;\n+  }\n+\n+  for (MonitorChunk* chunk = monitor_chunks(); chunk != NULL; chunk = chunk->next()) {\n+    if (chunk->contains(adr)) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n+}\n+\n+bool JavaThread::is_lock_owned_carrier(address adr) const {\n+  assert(is_vthread_mounted(), \"\");\n+  address stack_end = _stack_base - _stack_size;\n+  address stack_base = (address)vthread_continuation()->entry_sp();\n+  return stack_base > adr && adr >= stack_end;\n+}\n+\n+oop JavaThread::exception_oop() const {\n+  return Atomic::load(&_exception_oop);\n+}\n+\n+void JavaThread::set_exception_oop(oop o) {\n+  Atomic::store(&_exception_oop, o);\n+}\n+\n+void JavaThread::add_monitor_chunk(MonitorChunk* chunk) {\n+  chunk->set_next(monitor_chunks());\n+  set_monitor_chunks(chunk);\n+}\n+\n+void JavaThread::remove_monitor_chunk(MonitorChunk* chunk) {\n+  guarantee(monitor_chunks() != NULL, \"must be non empty\");\n+  if (monitor_chunks() == chunk) {\n+    set_monitor_chunks(chunk->next());\n+  } else {\n+    MonitorChunk* prev = monitor_chunks();\n+    while (prev->next() != chunk) prev = prev->next();\n+    prev->set_next(chunk->next());\n+  }\n+}\n+\n+void JavaThread::handle_special_runtime_exit_condition() {\n+  if (is_obj_deopt_suspend()) {\n+    frame_anchor()->make_walkable();\n+    wait_for_object_deoptimization();\n+  }\n+  JFR_ONLY(SUSPEND_THREAD_CONDITIONAL(this);)\n+}\n+\n+\n+\/\/ Asynchronous exceptions support\n+\/\/\n+void JavaThread::handle_async_exception(oop java_throwable) {\n+  assert(java_throwable != NULL, \"should have an _async_exception to throw\");\n+  assert(!is_at_poll_safepoint(), \"should have never called this method\");\n+\n+  if (has_last_Java_frame()) {\n+    frame f = last_frame();\n+    if (f.is_runtime_frame()) {\n+      \/\/ If the topmost frame is a runtime stub, then we are calling into\n+      \/\/ OptoRuntime from compiled code. Some runtime stubs (new, monitor_exit..)\n+      \/\/ must deoptimize the caller before continuing, as the compiled exception\n+      \/\/ handler table may not be valid.\n+      RegisterMap reg_map(this,\n+                          RegisterMap::UpdateMap::skip,\n+                          RegisterMap::ProcessFrames::include,\n+                          RegisterMap::WalkContinuation::skip);\n+      frame compiled_frame = f.sender(&reg_map);\n+      if (!StressCompiledExceptionHandlers && compiled_frame.can_be_deoptimized()) {\n+        Deoptimization::deoptimize(this, compiled_frame);\n+      }\n+    }\n+  }\n+\n+  \/\/ Only overwrite an already pending exception if it is not a ThreadDeath.\n+  if (!has_pending_exception() || !pending_exception()->is_a(vmClasses::ThreadDeath_klass())) {\n+\n+    \/\/ We cannot call Exceptions::_throw(...) here because we cannot block\n+    set_pending_exception(java_throwable, __FILE__, __LINE__);\n+\n+    \/\/ Clear any extent-local bindings on ThreadDeath\n+    set_extentLocalCache(NULL);\n+    oop threadOop = threadObj();\n+    assert(threadOop != NULL, \"must be\");\n+    java_lang_Thread::clear_extentLocalBindings(threadOop);\n+\n+    LogTarget(Info, exceptions) lt;\n+    if (lt.is_enabled()) {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      ls.print(\"Async. exception installed at runtime exit (\" INTPTR_FORMAT \")\", p2i(this));\n+      if (has_last_Java_frame()) {\n+        frame f = last_frame();\n+        ls.print(\" (pc: \" INTPTR_FORMAT \" sp: \" INTPTR_FORMAT \" )\", p2i(f.pc()), p2i(f.sp()));\n+      }\n+      ls.print_cr(\" of type: %s\", java_throwable->klass()->external_name());\n+    }\n+  }\n+}\n+\n+void JavaThread::install_async_exception(AsyncExceptionHandshake* aeh) {\n+  \/\/ Do not throw asynchronous exceptions against the compiler thread\n+  \/\/ or if the thread is already exiting.\n+  if (!can_call_java() || is_exiting()) {\n+    delete aeh;\n+    return;\n+  }\n+\n+  \/\/ Don't install a new pending async exception if there is already\n+  \/\/ a pending ThreadDeath one. Just interrupt thread from potential\n+  \/\/ wait()\/sleep()\/park() and return.\n+  if (has_async_exception_condition(true \/* ThreadDeath_only *\/)) {\n+    java_lang_Thread::set_interrupted(threadObj(), true);\n+    this->interrupt();\n+    delete aeh;\n+    return;\n+  }\n+\n+  oop exception = aeh->exception();\n+  Handshake::execute(aeh, this);  \/\/ Install asynchronous handshake\n+\n+  ResourceMark rm;\n+  if (log_is_enabled(Info, exceptions)) {\n+    log_info(exceptions)(\"Pending Async. exception installed of type: %s\",\n+                         InstanceKlass::cast(exception->klass())->external_name());\n+  }\n+  \/\/ for AbortVMOnException flag\n+  Exceptions::debug_check_abort(exception->klass()->external_name());\n+\n+  \/\/ Interrupt thread so it will wake up from a potential wait()\/sleep()\/park()\n+  java_lang_Thread::set_interrupted(threadObj(), true);\n+  this->interrupt();\n+}\n+\n+class InstallAsyncExceptionHandshake : public HandshakeClosure {\n+  AsyncExceptionHandshake* _aeh;\n+public:\n+  InstallAsyncExceptionHandshake(AsyncExceptionHandshake* aeh) :\n+    HandshakeClosure(\"InstallAsyncException\"), _aeh(aeh) {}\n+  ~InstallAsyncExceptionHandshake() {\n+    \/\/ If InstallAsyncExceptionHandshake was never executed we need to clean up _aeh.\n+    delete _aeh;\n+  }\n+  void do_thread(Thread* thr) {\n+    JavaThread* target = JavaThread::cast(thr);\n+    target->install_async_exception(_aeh);\n+    _aeh = nullptr;\n+  }\n+};\n+\n+void JavaThread::send_async_exception(JavaThread* target, oop java_throwable) {\n+  OopHandle e(Universe::vm_global(), java_throwable);\n+  InstallAsyncExceptionHandshake iaeh(new AsyncExceptionHandshake(e));\n+  Handshake::execute(&iaeh, target);\n+}\n+\n+#if INCLUDE_JVMTI\n+void JavaThread::set_is_in_VTMS_transition(bool val) {\n+  _is_in_VTMS_transition = val;\n+}\n+\n+#ifdef ASSERT\n+void JavaThread::set_is_VTMS_transition_disabler(bool val) {\n+  _is_VTMS_transition_disabler = val;\n+}\n+#endif\n+#endif\n+\n+\/\/ External suspension mechanism.\n+\/\/\n+\/\/ Guarantees on return (for a valid target thread):\n+\/\/   - Target thread will not execute any new bytecode.\n+\/\/   - Target thread will not enter any new monitors.\n+\/\/\n+bool JavaThread::java_suspend() {\n+#if INCLUDE_JVMTI\n+  \/\/ Suspending a JavaThread in VTMS transition or disabling VTMS transitions can cause deadlocks.\n+  assert(!is_in_VTMS_transition(), \"no suspend allowed in VTMS transition\");\n+  assert(!is_VTMS_transition_disabler(), \"no suspend allowed for VTMS transition disablers\");\n+#endif\n+\n+  guarantee(Thread::is_JavaThread_protected(\/* target *\/ this),\n+            \"target JavaThread is not protected in calling context.\");\n+  return this->handshake_state()->suspend();\n+}\n+\n+bool JavaThread::java_resume() {\n+  guarantee(Thread::is_JavaThread_protected_by_TLH(\/* target *\/ this),\n+            \"missing ThreadsListHandle in calling context.\");\n+  return this->handshake_state()->resume();\n+}\n+\n+\/\/ Wait for another thread to perform object reallocation and relocking on behalf of\n+\/\/ this thread. The current thread is required to change to _thread_blocked in order\n+\/\/ to be seen to be safepoint\/handshake safe whilst suspended and only after becoming\n+\/\/ handshake safe, the other thread can complete the handshake used to synchronize\n+\/\/ with this thread and then perform the reallocation and relocking.\n+\/\/ See EscapeBarrier::sync_and_suspend_*()\n+\n+void JavaThread::wait_for_object_deoptimization() {\n+  assert(!has_last_Java_frame() || frame_anchor()->walkable(), \"should have walkable stack\");\n+  assert(this == Thread::current(), \"invariant\");\n+\n+  bool spin_wait = os::is_MP();\n+  do {\n+    ThreadBlockInVM tbivm(this, true \/* allow_suspend *\/);\n+    \/\/ Wait for object deoptimization if requested.\n+    if (spin_wait) {\n+      \/\/ A single deoptimization is typically very short. Microbenchmarks\n+      \/\/ showed 5% better performance when spinning.\n+      const uint spin_limit = 10 * SpinYield::default_spin_limit;\n+      SpinYield spin(spin_limit);\n+      for (uint i = 0; is_obj_deopt_suspend() && i < spin_limit; i++) {\n+        spin.wait();\n+      }\n+      \/\/ Spin just once\n+      spin_wait = false;\n+    } else {\n+      MonitorLocker ml(this, EscapeBarrier_lock, Monitor::_no_safepoint_check_flag);\n+      if (is_obj_deopt_suspend()) {\n+        ml.wait();\n+      }\n+    }\n+    \/\/ A handshake for obj. deoptimization suspend could have been processed so\n+    \/\/ we must check after processing.\n+  } while (is_obj_deopt_suspend());\n+}\n+\n+#ifdef ASSERT\n+\/\/ Verify the JavaThread has not yet been published in the Threads::list, and\n+\/\/ hence doesn't need protection from concurrent access at this stage.\n+void JavaThread::verify_not_published() {\n+  \/\/ Cannot create a ThreadsListHandle here and check !tlh.includes(this)\n+  \/\/ since an unpublished JavaThread doesn't participate in the\n+  \/\/ Thread-SMR protocol for keeping a ThreadsList alive.\n+  assert(!on_thread_list(), \"JavaThread shouldn't have been published yet!\");\n+}\n+#endif\n+\n+\/\/ Slow path when the native==>Java barriers detect a safepoint\/handshake is\n+\/\/ pending, when _suspend_flags is non-zero or when we need to process a stack\n+\/\/ watermark. Also check for pending async exceptions (except unsafe access error).\n+\/\/ Note only the native==>Java barriers can call this function when thread state\n+\/\/ is _thread_in_native_trans.\n+void JavaThread::check_special_condition_for_native_trans(JavaThread *thread) {\n+  assert(thread->thread_state() == _thread_in_native_trans, \"wrong state\");\n+  assert(!thread->has_last_Java_frame() || thread->frame_anchor()->walkable(), \"Unwalkable stack in native->Java transition\");\n+\n+  thread->set_thread_state(_thread_in_vm);\n+\n+  \/\/ Enable WXWrite: called directly from interpreter native wrapper.\n+  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, thread));\n+\n+  SafepointMechanism::process_if_requested_with_exit_check(thread, true \/* check asyncs *\/);\n+\n+  \/\/ After returning from native, it could be that the stack frames are not\n+  \/\/ yet safe to use. We catch such situations in the subsequent stack watermark\n+  \/\/ barrier, which will trap unsafe stack frames.\n+  StackWatermarkSet::before_unwind(thread);\n+}\n+\n+#ifndef PRODUCT\n+\/\/ Deoptimization\n+\/\/ Function for testing deoptimization\n+void JavaThread::deoptimize() {\n+  StackFrameStream fst(this, false \/* update *\/, true \/* process_frames *\/);\n+  bool deopt = false;           \/\/ Dump stack only if a deopt actually happens.\n+  bool only_at = strlen(DeoptimizeOnlyAt) > 0;\n+  \/\/ Iterate over all frames in the thread and deoptimize\n+  for (; !fst.is_done(); fst.next()) {\n+    if (fst.current()->can_be_deoptimized()) {\n+\n+      if (only_at) {\n+        \/\/ Deoptimize only at particular bcis.  DeoptimizeOnlyAt\n+        \/\/ consists of comma or carriage return separated numbers so\n+        \/\/ search for the current bci in that string.\n+        address pc = fst.current()->pc();\n+        nmethod* nm =  (nmethod*) fst.current()->cb();\n+        ScopeDesc* sd = nm->scope_desc_at(pc);\n+        char buffer[8];\n+        jio_snprintf(buffer, sizeof(buffer), \"%d\", sd->bci());\n+        size_t len = strlen(buffer);\n+        const char * found = strstr(DeoptimizeOnlyAt, buffer);\n+        while (found != NULL) {\n+          if ((found[len] == ',' || found[len] == '\\n' || found[len] == '\\0') &&\n+              (found == DeoptimizeOnlyAt || found[-1] == ',' || found[-1] == '\\n')) {\n+            \/\/ Check that the bci found is bracketed by terminators.\n+            break;\n+          }\n+          found = strstr(found + 1, buffer);\n+        }\n+        if (!found) {\n+          continue;\n+        }\n+      }\n+\n+      if (DebugDeoptimization && !deopt) {\n+        deopt = true; \/\/ One-time only print before deopt\n+        tty->print_cr(\"[BEFORE Deoptimization]\");\n+        trace_frames();\n+        trace_stack();\n+      }\n+      Deoptimization::deoptimize(this, *fst.current());\n+    }\n+  }\n+\n+  if (DebugDeoptimization && deopt) {\n+    tty->print_cr(\"[AFTER Deoptimization]\");\n+    trace_frames();\n+  }\n+}\n+\n+\n+\/\/ Make zombies\n+void JavaThread::make_zombies() {\n+  for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n+    if (fst.current()->can_be_deoptimized()) {\n+      \/\/ it is a Java nmethod\n+      nmethod* nm = CodeCache::find_nmethod(fst.current()->pc());\n+      nm->make_not_entrant();\n+    }\n+  }\n+}\n+#endif \/\/ PRODUCT\n+\n+\n+void JavaThread::deoptimize_marked_methods() {\n+  if (!has_last_Java_frame()) return;\n+  StackFrameStream fst(this, false \/* update *\/, true \/* process_frames *\/);\n+  for (; !fst.is_done(); fst.next()) {\n+    if (fst.current()->should_be_deoptimized()) {\n+      Deoptimization::deoptimize(this, *fst.current());\n+    }\n+  }\n+}\n+\n+#ifdef ASSERT\n+void JavaThread::verify_frame_info() {\n+  assert((!has_last_Java_frame() && java_call_counter() == 0) ||\n+         (has_last_Java_frame() && java_call_counter() > 0),\n+         \"unexpected frame info: has_last_frame=%s, java_call_counter=%d\",\n+         has_last_Java_frame() ? \"true\" : \"false\", java_call_counter());\n+}\n+#endif\n+\n+\/\/ Push on a new block of JNI handles.\n+void JavaThread::push_jni_handle_block() {\n+  \/\/ Allocate a new block for JNI handles.\n+  \/\/ Inlined code from jni_PushLocalFrame()\n+  JNIHandleBlock* old_handles = active_handles();\n+  JNIHandleBlock* new_handles = JNIHandleBlock::allocate_block(this);\n+  assert(old_handles != NULL && new_handles != NULL, \"should not be NULL\");\n+  new_handles->set_pop_frame_link(old_handles);  \/\/ make sure java handles get gc'd.\n+  set_active_handles(new_handles);\n+}\n+\n+\/\/ Pop off the current block of JNI handles.\n+void JavaThread::pop_jni_handle_block() {\n+  \/\/ Release our JNI handle block\n+  JNIHandleBlock* old_handles = active_handles();\n+  JNIHandleBlock* new_handles = old_handles->pop_frame_link();\n+  assert(new_handles != nullptr, \"should never set active handles to null\");\n+  set_active_handles(new_handles);\n+  old_handles->set_pop_frame_link(NULL);\n+  JNIHandleBlock::release_block(old_handles, this);\n+}\n+\n+void JavaThread::oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf) {\n+  \/\/ Verify that the deferred card marks have been flushed.\n+  assert(deferred_card_mark().is_empty(), \"Should be empty during GC\");\n+\n+  \/\/ Traverse the GCHandles\n+  Thread::oops_do_no_frames(f, cf);\n+\n+  if (active_handles() != NULL) {\n+    active_handles()->oops_do(f);\n+  }\n+\n+  DEBUG_ONLY(verify_frame_info();)\n+\n+  if (has_last_Java_frame()) {\n+    \/\/ Traverse the monitor chunks\n+    for (MonitorChunk* chunk = monitor_chunks(); chunk != NULL; chunk = chunk->next()) {\n+      chunk->oops_do(f);\n+    }\n+  }\n+\n+  assert(vframe_array_head() == NULL, \"deopt in progress at a safepoint!\");\n+  \/\/ If we have deferred set_locals there might be oops waiting to be\n+  \/\/ written\n+  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(this);\n+  if (list != NULL) {\n+    for (int i = 0; i < list->length(); i++) {\n+      list->at(i)->oops_do(f);\n+    }\n+  }\n+\n+  \/\/ Traverse instance variables at the end since the GC may be moving things\n+  \/\/ around using this function\n+  f->do_oop((oop*) &_vm_result);\n+  f->do_oop((oop*) &_exception_oop);\n+#if INCLUDE_JVMCI\n+  f->do_oop((oop*) &_jvmci_reserved_oop0);\n+#endif\n+\n+  if (jvmti_thread_state() != NULL) {\n+    jvmti_thread_state()->oops_do(f, cf);\n+  }\n+}\n+\n+void JavaThread::oops_do_frames(OopClosure* f, CodeBlobClosure* cf) {\n+  if (!has_last_Java_frame()) {\n+    return;\n+  }\n+  \/\/ Finish any pending lazy GC activity for the frames\n+  StackWatermarkSet::finish_processing(this, NULL \/* context *\/, StackWatermarkKind::gc);\n+  \/\/ Traverse the execution stack\n+  for (StackFrameStream fst(this, true \/* update *\/, false \/* process_frames *\/); !fst.is_done(); fst.next()) {\n+    fst.current()->oops_do(f, cf, fst.register_map());\n+  }\n+}\n+\n+#ifdef ASSERT\n+void JavaThread::verify_states_for_handshake() {\n+  \/\/ This checks that the thread has a correct frame state during a handshake.\n+  verify_frame_info();\n+}\n+#endif\n+\n+void JavaThread::nmethods_do(CodeBlobClosure* cf) {\n+  DEBUG_ONLY(verify_frame_info();)\n+  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, Thread::current());)\n+\n+  if (has_last_Java_frame()) {\n+    \/\/ Traverse the execution stack\n+    for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n+      fst.current()->nmethods_do(cf);\n+    }\n+  }\n+\n+  if (jvmti_thread_state() != NULL) {\n+    jvmti_thread_state()->nmethods_do(cf);\n+  }\n+}\n+\n+void JavaThread::metadata_do(MetadataClosure* f) {\n+  if (has_last_Java_frame()) {\n+    \/\/ Traverse the execution stack to call f() on the methods in the stack\n+    for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n+      fst.current()->metadata_do(f);\n+    }\n+  } else if (is_Compiler_thread()) {\n+    \/\/ need to walk ciMetadata in current compile tasks to keep alive.\n+    CompilerThread* ct = (CompilerThread*)this;\n+    if (ct->env() != NULL) {\n+      ct->env()->metadata_do(f);\n+    }\n+    CompileTask* task = ct->task();\n+    if (task != NULL) {\n+      task->metadata_do(f);\n+    }\n+  }\n+}\n+\n+\/\/ Printing\n+const char* _get_thread_state_name(JavaThreadState _thread_state) {\n+  switch (_thread_state) {\n+  case _thread_uninitialized:     return \"_thread_uninitialized\";\n+  case _thread_new:               return \"_thread_new\";\n+  case _thread_new_trans:         return \"_thread_new_trans\";\n+  case _thread_in_native:         return \"_thread_in_native\";\n+  case _thread_in_native_trans:   return \"_thread_in_native_trans\";\n+  case _thread_in_vm:             return \"_thread_in_vm\";\n+  case _thread_in_vm_trans:       return \"_thread_in_vm_trans\";\n+  case _thread_in_Java:           return \"_thread_in_Java\";\n+  case _thread_in_Java_trans:     return \"_thread_in_Java_trans\";\n+  case _thread_blocked:           return \"_thread_blocked\";\n+  case _thread_blocked_trans:     return \"_thread_blocked_trans\";\n+  default:                        return \"unknown thread state\";\n+  }\n+}\n+\n+void JavaThread::print_thread_state_on(outputStream *st) const {\n+  st->print_cr(\"   JavaThread state: %s\", _get_thread_state_name(_thread_state));\n+}\n+\n+const char* JavaThread::thread_state_name() const {\n+  return _get_thread_state_name(_thread_state);\n+}\n+\n+\/\/ Called by Threads::print() for VM_PrintThreads operation\n+void JavaThread::print_on(outputStream *st, bool print_extended_info) const {\n+  st->print_raw(\"\\\"\");\n+  st->print_raw(name());\n+  st->print_raw(\"\\\" \");\n+  oop thread_oop = threadObj();\n+  if (thread_oop != NULL) {\n+    st->print(\"#\" INT64_FORMAT \" [%ld] \", (int64_t)java_lang_Thread::thread_id(thread_oop), (long) osthread()->thread_id());\n+    if (java_lang_Thread::is_daemon(thread_oop))  st->print(\"daemon \");\n+    st->print(\"prio=%d \", java_lang_Thread::priority(thread_oop));\n+  }\n+  Thread::print_on(st, print_extended_info);\n+  \/\/ print guess for valid stack memory region (assume 4K pages); helps lock debugging\n+  st->print_cr(\"[\" INTPTR_FORMAT \"]\", (intptr_t)last_Java_sp() & ~right_n_bits(12));\n+  if (thread_oop != NULL) {\n+    if (is_vthread_mounted()) {\n+      oop vt = vthread();\n+      assert(vt != NULL, \"\");\n+      st->print_cr(\"   Carrying virtual thread #\" INT64_FORMAT, (int64_t)java_lang_Thread::thread_id(vt));\n+    } else {\n+      st->print_cr(\"   java.lang.Thread.State: %s\", java_lang_Thread::thread_status_name(thread_oop));\n+    }\n+  }\n+#ifndef PRODUCT\n+  _safepoint_state->print_on(st);\n+#endif \/\/ PRODUCT\n+  if (is_Compiler_thread()) {\n+    CompileTask *task = ((CompilerThread*)this)->task();\n+    if (task != NULL) {\n+      st->print(\"   Compiling: \");\n+      task->print(st, NULL, true, false);\n+    } else {\n+      st->print(\"   No compile task\");\n+    }\n+    st->cr();\n+  }\n+}\n+\n+void JavaThread::print() const { print_on(tty); }\n+\n+void JavaThread::print_name_on_error(outputStream* st, char *buf, int buflen) const {\n+  st->print(\"%s\", get_thread_name_string(buf, buflen));\n+}\n+\n+\/\/ Called by fatal error handler. The difference between this and\n+\/\/ JavaThread::print() is that we can't grab lock or allocate memory.\n+void JavaThread::print_on_error(outputStream* st, char *buf, int buflen) const {\n+  st->print(\"%s \\\"%s\\\"\", type_name(), get_thread_name_string(buf, buflen));\n+  Thread* current = Thread::current_or_null_safe();\n+  assert(current != nullptr, \"cannot be called by a detached thread\");\n+  if (!current->is_Java_thread() || JavaThread::cast(current)->is_oop_safe()) {\n+    \/\/ Only access threadObj() if current thread is not a JavaThread\n+    \/\/ or if it is a JavaThread that can safely access oops.\n+    oop thread_obj = threadObj();\n+    if (thread_obj != nullptr) {\n+      if (java_lang_Thread::is_daemon(thread_obj)) st->print(\" daemon\");\n+    }\n+  }\n+  st->print(\" [\");\n+  st->print(\"%s\", _get_thread_state_name(_thread_state));\n+  if (osthread()) {\n+    st->print(\", id=%d\", osthread()->thread_id());\n+  }\n+  st->print(\", stack(\" PTR_FORMAT \",\" PTR_FORMAT \")\",\n+            p2i(stack_end()), p2i(stack_base()));\n+  st->print(\"]\");\n+\n+  ThreadsSMRSupport::print_info_on(this, st);\n+  return;\n+}\n+\n+\n+\/\/ Verification\n+\n+void JavaThread::frames_do(void f(frame*, const RegisterMap* map)) {\n+  \/\/ ignore if there is no stack\n+  if (!has_last_Java_frame()) return;\n+  \/\/ traverse the stack frames. Starts from top frame.\n+  for (StackFrameStream fst(this, true \/* update_map *\/, true \/* process_frames *\/, false \/* walk_cont *\/); !fst.is_done(); fst.next()) {\n+    frame* fr = fst.current();\n+    f(fr, fst.register_map());\n+  }\n+}\n+\n+static void frame_verify(frame* f, const RegisterMap *map) { f->verify(map); }\n+\n+void JavaThread::verify() {\n+  \/\/ Verify oops in the thread.\n+  oops_do(&VerifyOopClosure::verify_oop, NULL);\n+\n+  \/\/ Verify the stack frames.\n+  frames_do(frame_verify);\n+}\n+\n+\/\/ CR 6300358 (sub-CR 2137150)\n+\/\/ Most callers of this method assume that it can't return NULL but a\n+\/\/ thread may not have a name whilst it is in the process of attaching to\n+\/\/ the VM - see CR 6412693, and there are places where a JavaThread can be\n+\/\/ seen prior to having its threadObj set (e.g., JNI attaching threads and\n+\/\/ if vm exit occurs during initialization). These cases can all be accounted\n+\/\/ for such that this method never returns NULL.\n+const char* JavaThread::name() const  {\n+  if (Thread::is_JavaThread_protected(\/* target *\/ this)) {\n+    \/\/ The target JavaThread is protected so get_thread_name_string() is safe:\n+    return get_thread_name_string();\n+  }\n+\n+  \/\/ The target JavaThread is not protected so we return the default:\n+  return Thread::name();\n+}\n+\n+\/\/ Returns a non-NULL representation of this thread's name, or a suitable\n+\/\/ descriptive string if there is no set name.\n+const char* JavaThread::get_thread_name_string(char* buf, int buflen) const {\n+  const char* name_str;\n+#ifdef ASSERT\n+  Thread* current = Thread::current_or_null_safe();\n+  assert(current != nullptr, \"cannot be called by a detached thread\");\n+  if (!current->is_Java_thread() || JavaThread::cast(current)->is_oop_safe()) {\n+    \/\/ Only access threadObj() if current thread is not a JavaThread\n+    \/\/ or if it is a JavaThread that can safely access oops.\n+#endif\n+    oop thread_obj = threadObj();\n+    if (thread_obj != NULL) {\n+      oop name = java_lang_Thread::name(thread_obj);\n+      if (name != NULL) {\n+        if (buf == NULL) {\n+          name_str = java_lang_String::as_utf8_string(name);\n+        } else {\n+          name_str = java_lang_String::as_utf8_string(name, buf, buflen);\n+        }\n+      } else if (is_attaching_via_jni()) { \/\/ workaround for 6412693 - see 6404306\n+        name_str = \"<no-name - thread is attaching>\";\n+      } else {\n+        name_str = \"<un-named>\";\n+      }\n+    } else {\n+      name_str = Thread::name();\n+    }\n+#ifdef ASSERT\n+  } else {\n+    \/\/ Current JavaThread has exited...\n+    if (current == this) {\n+      \/\/ ... and is asking about itself:\n+      name_str = \"<no-name - current JavaThread has exited>\";\n+    } else {\n+      \/\/ ... and it can't safely determine this JavaThread's name so\n+      \/\/ use the default thread name.\n+      name_str = Thread::name();\n+    }\n+  }\n+#endif\n+  assert(name_str != NULL, \"unexpected NULL thread name\");\n+  return name_str;\n+}\n+\n+\/\/ Helper to extract the name from the thread oop for logging.\n+const char* JavaThread::name_for(oop thread_obj) {\n+  assert(thread_obj != NULL, \"precondition\");\n+  oop name = java_lang_Thread::name(thread_obj);\n+  const char* name_str;\n+  if (name != NULL) {\n+    name_str = java_lang_String::as_utf8_string(name);\n+  } else {\n+    name_str = \"<un-named>\";\n+  }\n+  return name_str;\n+}\n+\n+void JavaThread::prepare(jobject jni_thread, ThreadPriority prio) {\n+\n+  assert(Threads_lock->owner() == Thread::current(), \"must have threads lock\");\n+  assert(NoPriority <= prio && prio <= MaxPriority, \"sanity check\");\n+  \/\/ Link Java Thread object <-> C++ Thread\n+\n+  \/\/ Get the C++ thread object (an oop) from the JNI handle (a jthread)\n+  \/\/ and put it into a new Handle.  The Handle \"thread_oop\" can then\n+  \/\/ be used to pass the C++ thread object to other methods.\n+\n+  \/\/ Set the Java level thread object (jthread) field of the\n+  \/\/ new thread (a JavaThread *) to C++ thread object using the\n+  \/\/ \"thread_oop\" handle.\n+\n+  \/\/ Set the thread field (a JavaThread *) of the\n+  \/\/ oop representing the java_lang_Thread to the new thread (a JavaThread *).\n+\n+  Handle thread_oop(Thread::current(),\n+                    JNIHandles::resolve_non_null(jni_thread));\n+  assert(InstanceKlass::cast(thread_oop->klass())->is_linked(),\n+         \"must be initialized\");\n+  set_threadOopHandles(thread_oop());\n+  java_lang_Thread::set_thread(thread_oop(), this);\n+\n+  if (prio == NoPriority) {\n+    prio = java_lang_Thread::priority(thread_oop());\n+    assert(prio != NoPriority, \"A valid priority should be present\");\n+  }\n+\n+  \/\/ Push the Java priority down to the native thread; needs Threads_lock\n+  Thread::set_priority(this, prio);\n+\n+  \/\/ Add the new thread to the Threads list and set it in motion.\n+  \/\/ We must have threads lock in order to call Threads::add.\n+  \/\/ It is crucial that we do not block before the thread is\n+  \/\/ added to the Threads list for if a GC happens, then the java_thread oop\n+  \/\/ will not be visited by GC.\n+  Threads::add(this);\n+}\n+\n+oop JavaThread::current_park_blocker() {\n+  \/\/ Support for JSR-166 locks\n+  oop thread_oop = threadObj();\n+  if (thread_oop != NULL) {\n+    return java_lang_Thread::park_blocker(thread_oop);\n+  }\n+  return NULL;\n+}\n+\n+\n+void JavaThread::print_stack_on(outputStream* st) {\n+  if (!has_last_Java_frame()) return;\n+\n+  Thread* current_thread = Thread::current();\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+\n+  RegisterMap reg_map(this,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  vframe* start_vf = platform_thread_last_java_vframe(&reg_map);\n+  int count = 0;\n+  for (vframe* f = start_vf; f != NULL; f = f->sender()) {\n+    if (f->is_java_frame()) {\n+      javaVFrame* jvf = javaVFrame::cast(f);\n+      java_lang_Throwable::print_stack_element(st, jvf->method(), jvf->bci());\n+\n+      \/\/ Print out lock information\n+      if (JavaMonitorsInStackTrace) {\n+        jvf->print_lock_info_on(st, count);\n+      }\n+    } else {\n+      \/\/ Ignore non-Java frames\n+    }\n+\n+    \/\/ Bail-out case for too deep stacks if MaxJavaStackTraceDepth > 0\n+    count++;\n+    if (MaxJavaStackTraceDepth > 0 && MaxJavaStackTraceDepth == count) return;\n+  }\n+}\n+\n+#if INCLUDE_JVMTI\n+\/\/ Rebind JVMTI thread state from carrier to virtual or from virtual to carrier.\n+JvmtiThreadState* JavaThread::rebind_to_jvmti_thread_state_of(oop thread_oop) {\n+  set_jvmti_vthread(thread_oop);\n+\n+  \/\/ unbind current JvmtiThreadState from JavaThread\n+  JvmtiThreadState::unbind_from(jvmti_thread_state(), this);\n+\n+  \/\/ bind new JvmtiThreadState to JavaThread\n+  JvmtiThreadState::bind_to(java_lang_Thread::jvmti_thread_state(thread_oop), this);\n+\n+  return jvmti_thread_state();\n+}\n+#endif\n+\n+\/\/ JVMTI PopFrame support\n+void JavaThread::popframe_preserve_args(ByteSize size_in_bytes, void* start) {\n+  assert(_popframe_preserved_args == NULL, \"should not wipe out old PopFrame preserved arguments\");\n+  if (in_bytes(size_in_bytes) != 0) {\n+    _popframe_preserved_args = NEW_C_HEAP_ARRAY(char, in_bytes(size_in_bytes), mtThread);\n+    _popframe_preserved_args_size = in_bytes(size_in_bytes);\n+    Copy::conjoint_jbytes(start, _popframe_preserved_args, _popframe_preserved_args_size);\n+  }\n+}\n+\n+void* JavaThread::popframe_preserved_args() {\n+  return _popframe_preserved_args;\n+}\n+\n+ByteSize JavaThread::popframe_preserved_args_size() {\n+  return in_ByteSize(_popframe_preserved_args_size);\n+}\n+\n+WordSize JavaThread::popframe_preserved_args_size_in_words() {\n+  int sz = in_bytes(popframe_preserved_args_size());\n+  assert(sz % wordSize == 0, \"argument size must be multiple of wordSize\");\n+  return in_WordSize(sz \/ wordSize);\n+}\n+\n+void JavaThread::popframe_free_preserved_args() {\n+  assert(_popframe_preserved_args != NULL, \"should not free PopFrame preserved arguments twice\");\n+  FREE_C_HEAP_ARRAY(char, (char*)_popframe_preserved_args);\n+  _popframe_preserved_args = NULL;\n+  _popframe_preserved_args_size = 0;\n+}\n+\n+#ifndef PRODUCT\n+\n+void JavaThread::trace_frames() {\n+  tty->print_cr(\"[Describe stack]\");\n+  int frame_no = 1;\n+  for (StackFrameStream fst(this, true \/* update *\/, true \/* process_frames *\/); !fst.is_done(); fst.next()) {\n+    tty->print(\"  %d. \", frame_no++);\n+    fst.current()->print_value_on(tty, this);\n+    tty->cr();\n+  }\n+}\n+\n+class PrintAndVerifyOopClosure: public OopClosure {\n+ protected:\n+  template <class T> inline void do_oop_work(T* p) {\n+    oop obj = RawAccess<>::oop_load(p);\n+    if (obj == NULL) return;\n+    tty->print(INTPTR_FORMAT \": \", p2i(p));\n+    if (oopDesc::is_oop_or_null(obj)) {\n+      if (obj->is_objArray()) {\n+        tty->print_cr(\"valid objArray: \" INTPTR_FORMAT, p2i(obj));\n+      } else {\n+        obj->print();\n+      }\n+    } else {\n+      tty->print_cr(\"invalid oop: \" INTPTR_FORMAT, p2i(obj));\n+    }\n+    tty->cr();\n+  }\n+ public:\n+  virtual void do_oop(oop* p) { do_oop_work(p); }\n+  virtual void do_oop(narrowOop* p)  { do_oop_work(p); }\n+};\n+\n+#ifdef ASSERT\n+\/\/ Print or validate the layout of stack frames\n+void JavaThread::print_frame_layout(int depth, bool validate_only) {\n+  ResourceMark rm;\n+  PreserveExceptionMark pm(this);\n+  FrameValues values;\n+  int frame_no = 0;\n+  for (StackFrameStream fst(this, true, true, true); !fst.is_done(); fst.next()) {\n+    fst.current()->describe(values, ++frame_no, fst.register_map());\n+    if (depth == frame_no) break;\n+  }\n+  Continuation::describe(values);\n+  if (validate_only) {\n+    values.validate();\n+  } else {\n+    tty->print_cr(\"[Describe stack layout]\");\n+    values.print(this);\n+  }\n+}\n+#endif\n+\n+void JavaThread::trace_stack_from(vframe* start_vf) {\n+  ResourceMark rm;\n+  int vframe_no = 1;\n+  for (vframe* f = start_vf; f; f = f->sender()) {\n+    if (f->is_java_frame()) {\n+      javaVFrame::cast(f)->print_activation(vframe_no++);\n+    } else {\n+      f->print();\n+    }\n+    if (vframe_no > StackPrintLimit) {\n+      tty->print_cr(\"...<more frames>...\");\n+      return;\n+    }\n+  }\n+}\n+\n+\n+void JavaThread::trace_stack() {\n+  if (!has_last_Java_frame()) return;\n+  Thread* current_thread = Thread::current();\n+  ResourceMark rm(current_thread);\n+  HandleMark hm(current_thread);\n+  RegisterMap reg_map(this,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n+  trace_stack_from(last_java_vframe(&reg_map));\n+}\n+\n+\n+#endif \/\/ PRODUCT\n+\n+void JavaThread::inc_held_monitor_count(int i, bool jni) {\n+#ifdef SUPPORT_MONITOR_COUNT\n+  assert(_held_monitor_count >= 0, \"Must always be greater than 0: \" INT64_FORMAT, (int64_t)_held_monitor_count);\n+  _held_monitor_count += i;\n+  if (jni) {\n+    assert(_jni_monitor_count >= 0, \"Must always be greater than 0: \" INT64_FORMAT, (int64_t)_jni_monitor_count);\n+    _jni_monitor_count += i;\n+  }\n+#endif\n+}\n+\n+void JavaThread::dec_held_monitor_count(int i, bool jni) {\n+#ifdef SUPPORT_MONITOR_COUNT\n+  _held_monitor_count -= i;\n+  assert(_held_monitor_count >= 0, \"Must always be greater than 0: \" INT64_FORMAT, (int64_t)_held_monitor_count);\n+  if (jni) {\n+    _jni_monitor_count -= i;\n+    assert(_jni_monitor_count >= 0, \"Must always be greater than 0: \" INT64_FORMAT, (int64_t)_jni_monitor_count);\n+  }\n+#endif\n+}\n+\n+frame JavaThread::vthread_last_frame() {\n+  assert (is_vthread_mounted(), \"Virtual thread not mounted\");\n+  return last_frame();\n+}\n+\n+frame JavaThread::carrier_last_frame(RegisterMap* reg_map) {\n+  const ContinuationEntry* entry = vthread_continuation();\n+  guarantee (entry != NULL, \"Not a carrier thread\");\n+  frame f = entry->to_frame();\n+  if (reg_map->process_frames()) {\n+    entry->flush_stack_processing(this);\n+  }\n+  entry->update_register_map(reg_map);\n+  return f.sender(reg_map);\n+}\n+\n+frame JavaThread::platform_thread_last_frame(RegisterMap* reg_map) {\n+  return is_vthread_mounted() ? carrier_last_frame(reg_map) : last_frame();\n+}\n+\n+javaVFrame* JavaThread::last_java_vframe(const frame f, RegisterMap *reg_map) {\n+  assert(reg_map != NULL, \"a map must be given\");\n+  for (vframe* vf = vframe::new_vframe(&f, reg_map, this); vf; vf = vf->sender()) {\n+    if (vf->is_java_frame()) return javaVFrame::cast(vf);\n+  }\n+  return NULL;\n+}\n+\n+oop JavaThread::get_continuation() const {\n+  assert(threadObj() != nullptr, \"must be set\");\n+  return java_lang_Thread::continuation(threadObj());\n+}\n+\n+Klass* JavaThread::security_get_caller_class(int depth) {\n+  ResetNoHandleMark rnhm;\n+  HandleMark hm(Thread::current());\n+\n+  vframeStream vfst(this);\n+  vfst.security_get_caller_frame(depth);\n+  if (!vfst.at_end()) {\n+    return vfst.method()->method_holder();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ java.lang.Thread.sleep support\n+\/\/ Returns true if sleep time elapsed as expected, and false\n+\/\/ if the thread was interrupted.\n+bool JavaThread::sleep(jlong millis) {\n+  assert(this == Thread::current(),  \"thread consistency check\");\n+\n+  ParkEvent * const slp = this->_SleepEvent;\n+  \/\/ Because there can be races with thread interruption sending an unpark()\n+  \/\/ to the event, we explicitly reset it here to avoid an immediate return.\n+  \/\/ The actual interrupt state will be checked before we park().\n+  slp->reset();\n+  \/\/ Thread interruption establishes a happens-before ordering in the\n+  \/\/ Java Memory Model, so we need to ensure we synchronize with the\n+  \/\/ interrupt state.\n+  OrderAccess::fence();\n+\n+  jlong prevtime = os::javaTimeNanos();\n+\n+  for (;;) {\n+    \/\/ interruption has precedence over timing out\n+    if (this->is_interrupted(true)) {\n+      return false;\n+    }\n+\n+    if (millis <= 0) {\n+      return true;\n+    }\n+\n+    {\n+      ThreadBlockInVM tbivm(this);\n+      OSThreadWaitState osts(this->osthread(), false \/* not Object.wait() *\/);\n+      slp->park(millis);\n+    }\n+\n+    \/\/ Update elapsed time tracking\n+    jlong newtime = os::javaTimeNanos();\n+    if (newtime - prevtime < 0) {\n+      \/\/ time moving backwards, should only happen if no monotonic clock\n+      \/\/ not a guarantee() because JVM should not abort on kernel\/glibc bugs\n+      assert(false,\n+             \"unexpected time moving backwards detected in JavaThread::sleep()\");\n+    } else {\n+      millis -= (newtime - prevtime) \/ NANOSECS_PER_MILLISEC;\n+    }\n+    prevtime = newtime;\n+  }\n+}\n+\n+\/\/ Last thread running calls java.lang.Shutdown.shutdown()\n+void JavaThread::invoke_shutdown_hooks() {\n+  HandleMark hm(this);\n+\n+  \/\/ We could get here with a pending exception, if so clear it now or\n+  \/\/ it will cause MetaspaceShared::link_shared_classes to\n+  \/\/ fail for dynamic dump.\n+  if (this->has_pending_exception()) {\n+    this->clear_pending_exception();\n+  }\n+\n+#if INCLUDE_CDS\n+  \/\/ Link all classes for dynamic CDS dumping before vm exit.\n+  \/\/ Same operation is being done in JVM_BeforeHalt for handling the\n+  \/\/ case where the application calls System.exit().\n+  if (DynamicArchive::should_dump_at_vm_exit()) {\n+    DynamicArchive::prepare_for_dump_at_exit();\n+  }\n+#endif\n+\n+  EXCEPTION_MARK;\n+  Klass* shutdown_klass =\n+    SystemDictionary::resolve_or_null(vmSymbols::java_lang_Shutdown(),\n+                                      THREAD);\n+  if (shutdown_klass != NULL) {\n+    \/\/ SystemDictionary::resolve_or_null will return null if there was\n+    \/\/ an exception.  If we cannot load the Shutdown class, just don't\n+    \/\/ call Shutdown.shutdown() at all.  This will mean the shutdown hooks\n+    \/\/ won't be run.  Note that if a shutdown hook was registered,\n+    \/\/ the Shutdown class would have already been loaded\n+    \/\/ (Runtime.addShutdownHook will load it).\n+    JavaValue result(T_VOID);\n+    JavaCalls::call_static(&result,\n+                           shutdown_klass,\n+                           vmSymbols::shutdown_name(),\n+                           vmSymbols::void_method_signature(),\n+                           THREAD);\n+  }\n+  CLEAR_PENDING_EXCEPTION;\n+}\n+\n+#ifndef PRODUCT\n+void JavaThread::verify_cross_modify_fence_failure(JavaThread *thread) {\n+   report_vm_error(__FILE__, __LINE__, \"Cross modify fence failure\", \"%p\", thread);\n+}\n+#endif\n+\n+\/\/ Helper function to create the java.lang.Thread object for a\n+\/\/ VM-internal thread. The thread will have the given name, and be\n+\/\/ a member of the \"system\" ThreadGroup.\n+Handle JavaThread::create_system_thread_object(const char* name,\n+                                               bool is_visible, TRAPS) {\n+  Handle string = java_lang_String::create_from_str(name, CHECK_NH);\n+\n+  \/\/ Initialize thread_oop to put it into the system threadGroup.\n+  \/\/ This is done by calling the Thread(ThreadGroup group, String name) constructor.\n+  Handle thread_group(THREAD, Universe::system_thread_group());\n+  Handle thread_oop =\n+    JavaCalls::construct_new_instance(vmClasses::Thread_klass(),\n+                                      vmSymbols::threadgroup_string_void_signature(),\n+                                      thread_group,\n+                                      string,\n+                                      CHECK_NH);\n+\n+  return thread_oop;\n+}\n+\n+\/\/ Starts the target JavaThread as a daemon of the given priority, and\n+\/\/ bound to the given java.lang.Thread instance.\n+\/\/ The Threads_lock is held for the duration.\n+void JavaThread::start_internal_daemon(JavaThread* current, JavaThread* target,\n+                                       Handle thread_oop, ThreadPriority prio) {\n+\n+  assert(target->osthread() != NULL, \"target thread is not properly initialized\");\n+\n+  MutexLocker mu(current, Threads_lock);\n+\n+  \/\/ Initialize the fields of the thread_oop first.\n+\n+  java_lang_Thread::set_thread(thread_oop(), target); \/\/ isAlive == true now\n+\n+  if (prio != NoPriority) {\n+    java_lang_Thread::set_priority(thread_oop(), prio);\n+    \/\/ Note: we don't call os::set_priority here. Possibly we should,\n+    \/\/ else all threads should call it themselves when they first run.\n+  }\n+\n+  java_lang_Thread::set_daemon(thread_oop());\n+\n+  \/\/ Now bind the thread_oop to the target JavaThread.\n+  target->set_threadOopHandles(thread_oop());\n+\n+  Threads::add(target); \/\/ target is now visible for safepoint\/handshake\n+  Thread::start(target);\n+}\n+\n+void JavaThread::vm_exit_on_osthread_failure(JavaThread* thread) {\n+  \/\/ At this point it may be possible that no osthread was created for the\n+  \/\/ JavaThread due to lack of resources. However, since this must work\n+  \/\/ for critical system threads just check and abort if this fails.\n+  if (thread->osthread() == nullptr) {\n+    \/\/ This isn't really an OOM condition, but historically this is what\n+    \/\/ we report.\n+    vm_exit_during_initialization(\"java.lang.OutOfMemoryError\",\n+                                  os::native_thread_creation_failed_msg());\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":2091,"deletions":0,"binary":false,"changes":2091,"status":"added"},{"patch":"@@ -0,0 +1,1201 @@\n+\/*\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, Azul Systems, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_JAVATHREAD_HPP\n+#define SHARE_RUNTIME_JAVATHREAD_HPP\n+\n+#include \"jni.h\"\n+#include \"memory\/allocation.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"oops\/oopHandle.hpp\"\n+#include \"runtime\/frame.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/handshake.hpp\"\n+#include \"runtime\/javaFrameAnchor.hpp\"\n+#include \"runtime\/park.hpp\"\n+#include \"runtime\/safepointMechanism.hpp\"\n+#include \"runtime\/stackWatermarkSet.hpp\"\n+#include \"runtime\/stackOverflow.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"runtime\/threadHeapSampler.hpp\"\n+#include \"runtime\/threadStatisticalInfo.hpp\"\n+#include \"utilities\/exceptions.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"utilities\/macros.hpp\"\n+#if INCLUDE_JFR\n+#include \"jfr\/support\/jfrThreadExtension.hpp\"\n+#endif\n+\n+class AsyncExceptionHandshake;\n+class ContinuationEntry;\n+class DeoptResourceMark;\n+class JNIHandleBlock;\n+class JVMCIRuntime;\n+\n+class JvmtiDeferredUpdates;\n+class JvmtiSampledObjectAllocEventCollector;\n+class JvmtiThreadState;\n+\n+class Metadata;\n+class OopStorage;\n+class OSThread;\n+\n+class ThreadsList;\n+class ThreadSafepointState;\n+class ThreadStatistics;\n+\n+class vframeArray;\n+class vframe;\n+class javaVFrame;\n+\n+class JavaThread;\n+typedef void (*ThreadFunction)(JavaThread*, TRAPS);\n+\n+class JavaThread: public Thread {\n+  friend class VMStructs;\n+  friend class JVMCIVMStructs;\n+  friend class WhiteBox;\n+  friend class ThreadsSMRSupport; \/\/ to access _threadObj for exiting_threads_oops_do\n+  friend class HandshakeState;\n+  friend class Continuation;\n+  friend class Threads;\n+ private:\n+  bool           _in_asgct;                      \/\/ Is set when this JavaThread is handling ASGCT call\n+  bool           _on_thread_list;                \/\/ Is set when this JavaThread is added to the Threads list\n+  OopHandle      _threadObj;                     \/\/ The Java level thread object\n+  OopHandle      _vthread; \/\/ the value returned by Thread.currentThread(): the virtual thread, if mounted, otherwise _threadObj\n+  OopHandle      _jvmti_vthread;\n+  OopHandle      _extentLocalCache;\n+\n+  static OopStorage* _thread_oop_storage;\n+\n+#ifdef ASSERT\n+ private:\n+  int _java_call_counter;\n+\n+ public:\n+  int  java_call_counter()                       { return _java_call_counter; }\n+  void inc_java_call_counter()                   { _java_call_counter++; }\n+  void dec_java_call_counter() {\n+    assert(_java_call_counter > 0, \"Invalid nesting of JavaCallWrapper\");\n+    _java_call_counter--;\n+  }\n+ private:  \/\/ restore original namespace restriction\n+#endif  \/\/ ifdef ASSERT\n+\n+  JavaFrameAnchor _anchor;                       \/\/ Encapsulation of current java frame and it state\n+\n+  ThreadFunction _entry_point;\n+\n+  JNIEnv        _jni_environment;\n+\n+  \/\/ Deopt support\n+  DeoptResourceMark*  _deopt_mark;               \/\/ Holds special ResourceMark for deoptimization\n+\n+  CompiledMethod*       _deopt_nmethod;         \/\/ CompiledMethod that is currently being deoptimized\n+  vframeArray*  _vframe_array_head;              \/\/ Holds the heap of the active vframeArrays\n+  vframeArray*  _vframe_array_last;              \/\/ Holds last vFrameArray we popped\n+  \/\/ Holds updates by JVMTI agents for compiled frames that cannot be performed immediately. They\n+  \/\/ will be carried out as soon as possible which, in most cases, is just before deoptimization of\n+  \/\/ the frame, when control returns to it.\n+  JvmtiDeferredUpdates* _jvmti_deferred_updates;\n+\n+  \/\/ Handshake value for fixing 6243940. We need a place for the i2c\n+  \/\/ adapter to store the callee Method*. This value is NEVER live\n+  \/\/ across a gc point so it does NOT have to be gc'd\n+  \/\/ The handshake is open ended since we can't be certain that it will\n+  \/\/ be NULLed. This is because we rarely ever see the race and end up\n+  \/\/ in handle_wrong_method which is the backend of the handshake. See\n+  \/\/ code in i2c adapters and handle_wrong_method.\n+\n+  Method*       _callee_target;\n+\n+  \/\/ Used to pass back results to the interpreter or generated code running Java code.\n+  oop           _vm_result;    \/\/ oop result is GC-preserved\n+  Metadata*     _vm_result_2;  \/\/ non-oop result\n+  oop           _return_buffered_value; \/\/ buffered value being returned\n+\n+  \/\/ See ReduceInitialCardMarks: this holds the precise space interval of\n+  \/\/ the most recent slow path allocation for which compiled code has\n+  \/\/ elided card-marks for performance along the fast-path.\n+  MemRegion     _deferred_card_mark;\n+\n+  ObjectMonitor* volatile _current_pending_monitor;     \/\/ ObjectMonitor this thread is waiting to lock\n+  bool           _current_pending_monitor_is_from_java; \/\/ locking is from Java code\n+  ObjectMonitor* volatile _current_waiting_monitor;     \/\/ ObjectMonitor on which this thread called Object.wait()\n+\n+  \/\/ Active_handles points to a block of handles\n+  JNIHandleBlock* _active_handles;\n+\n+  \/\/ One-element thread local free list\n+  JNIHandleBlock* _free_handle_block;\n+\n+ public:\n+  volatile intptr_t _Stalled;\n+\n+  \/\/ For tracking the heavyweight monitor the thread is pending on.\n+  ObjectMonitor* current_pending_monitor() {\n+    \/\/ Use Atomic::load() to prevent data race between concurrent modification and\n+    \/\/ concurrent readers, e.g. ThreadService::get_current_contended_monitor().\n+    \/\/ Especially, reloading pointer from thread after NULL check must be prevented.\n+    return Atomic::load(&_current_pending_monitor);\n+  }\n+  void set_current_pending_monitor(ObjectMonitor* monitor) {\n+    Atomic::store(&_current_pending_monitor, monitor);\n+  }\n+  void set_current_pending_monitor_is_from_java(bool from_java) {\n+    _current_pending_monitor_is_from_java = from_java;\n+  }\n+  bool current_pending_monitor_is_from_java() {\n+    return _current_pending_monitor_is_from_java;\n+  }\n+  ObjectMonitor* current_waiting_monitor() {\n+    \/\/ See the comment in current_pending_monitor() above.\n+    return Atomic::load(&_current_waiting_monitor);\n+  }\n+  void set_current_waiting_monitor(ObjectMonitor* monitor) {\n+    Atomic::store(&_current_waiting_monitor, monitor);\n+  }\n+\n+  \/\/ JNI handle support\n+  JNIHandleBlock* active_handles() const         { return _active_handles; }\n+  void set_active_handles(JNIHandleBlock* block) { _active_handles = block; }\n+  JNIHandleBlock* free_handle_block() const      { return _free_handle_block; }\n+  void set_free_handle_block(JNIHandleBlock* block) { _free_handle_block = block; }\n+\n+  void push_jni_handle_block();\n+  void pop_jni_handle_block();\n+\n+ private:\n+  MonitorChunk* _monitor_chunks;              \/\/ Contains the off stack monitors\n+                                              \/\/ allocated during deoptimization\n+                                              \/\/ and by JNI_MonitorEnter\/Exit\n+\n+  enum SuspendFlags {\n+    \/\/ NOTE: avoid using the sign-bit as cc generates different test code\n+    \/\/       when the sign-bit is used, and sometimes incorrectly - see CR 6398077\n+    _trace_flag             = 0x00000004U, \/\/ call tracing backend\n+    _obj_deopt              = 0x00000008U  \/\/ suspend for object reallocation and relocking for JVMTI agent\n+  };\n+\n+  \/\/ various suspension related flags - atomically updated\n+  volatile uint32_t _suspend_flags;\n+\n+  inline void set_suspend_flag(SuspendFlags f);\n+  inline void clear_suspend_flag(SuspendFlags f);\n+\n+ public:\n+  inline void set_trace_flag();\n+  inline void clear_trace_flag();\n+  inline void set_obj_deopt_flag();\n+  inline void clear_obj_deopt_flag();\n+  bool is_trace_suspend()      { return (_suspend_flags & _trace_flag) != 0; }\n+  bool is_obj_deopt_suspend()  { return (_suspend_flags & _obj_deopt) != 0; }\n+\n+  \/\/ Asynchronous exception support\n+ private:\n+  friend class InstallAsyncExceptionHandshake;\n+  friend class AsyncExceptionHandshake;\n+  friend class HandshakeState;\n+\n+  void install_async_exception(AsyncExceptionHandshake* aec = NULL);\n+  void handle_async_exception(oop java_throwable);\n+ public:\n+  bool has_async_exception_condition(bool ThreadDeath_only = false);\n+  inline void set_pending_unsafe_access_error();\n+  static void send_async_exception(JavaThread* jt, oop java_throwable);\n+\n+  class NoAsyncExceptionDeliveryMark : public StackObj {\n+    friend JavaThread;\n+    JavaThread *_target;\n+    inline NoAsyncExceptionDeliveryMark(JavaThread *t);\n+    inline ~NoAsyncExceptionDeliveryMark();\n+  };\n+\n+  \/\/ Safepoint support\n+ public:                                                        \/\/ Expose _thread_state for SafeFetchInt()\n+  volatile JavaThreadState _thread_state;\n+ private:\n+  SafepointMechanism::ThreadData _poll_data;\n+  ThreadSafepointState*          _safepoint_state;              \/\/ Holds information about a thread during a safepoint\n+  address                        _saved_exception_pc;           \/\/ Saved pc of instruction where last implicit exception happened\n+  NOT_PRODUCT(bool               _requires_cross_modify_fence;) \/\/ State used by VerifyCrossModifyFence\n+#ifdef ASSERT\n+  \/\/ Debug support for checking if code allows safepoints or not.\n+  \/\/ Safepoints in the VM can happen because of allocation, invoking a VM operation, or blocking on\n+  \/\/ mutex, or blocking on an object synchronizer (Java locking).\n+  \/\/ If _no_safepoint_count is non-zero, then an assertion failure will happen in any of\n+  \/\/ the above cases. The class NoSafepointVerifier is used to set this counter.\n+  int _no_safepoint_count;                             \/\/ If 0, thread allow a safepoint to happen\n+\n+ public:\n+  void inc_no_safepoint_count() { _no_safepoint_count++; }\n+  void dec_no_safepoint_count() { _no_safepoint_count--; }\n+#endif \/\/ ASSERT\n+ public:\n+  \/\/ These functions check conditions before possibly going to a safepoint.\n+  \/\/ including NoSafepointVerifier.\n+  void check_for_valid_safepoint_state() NOT_DEBUG_RETURN;\n+  void check_possible_safepoint()        NOT_DEBUG_RETURN;\n+\n+#ifdef ASSERT\n+ private:\n+  volatile uint64_t _visited_for_critical_count;\n+\n+ public:\n+  void set_visited_for_critical_count(uint64_t safepoint_id) {\n+    assert(_visited_for_critical_count == 0, \"Must be reset before set\");\n+    assert((safepoint_id & 0x1) == 1, \"Must be odd\");\n+    _visited_for_critical_count = safepoint_id;\n+  }\n+  void reset_visited_for_critical_count(uint64_t safepoint_id) {\n+    assert(_visited_for_critical_count == safepoint_id, \"Was not visited\");\n+    _visited_for_critical_count = 0;\n+  }\n+  bool was_visited_for_critical_count(uint64_t safepoint_id) const {\n+    return _visited_for_critical_count == safepoint_id;\n+  }\n+#endif \/\/ ASSERT\n+\n+  \/\/ JavaThread termination support\n+ public:\n+  enum TerminatedTypes {\n+    _not_terminated = 0xDEAD - 3,\n+    _thread_exiting,                             \/\/ JavaThread::exit() has been called for this thread\n+    _thread_gc_barrier_detached,                 \/\/ thread's GC barrier has been detached\n+    _thread_terminated,                          \/\/ JavaThread is removed from thread list\n+    _vm_exited                                   \/\/ JavaThread is still executing native code, but VM is terminated\n+                                                 \/\/ only VM_Exit can set _vm_exited\n+  };\n+\n+ private:\n+  \/\/ In general a JavaThread's _terminated field transitions as follows:\n+  \/\/\n+  \/\/   _not_terminated => _thread_exiting => _thread_gc_barrier_detached => _thread_terminated\n+  \/\/\n+  \/\/ _vm_exited is a special value to cover the case of a JavaThread\n+  \/\/ executing native code after the VM itself is terminated.\n+  \/\/\n+  \/\/ A JavaThread that fails to JNI attach has these _terminated field transitions:\n+  \/\/   _not_terminated => _thread_terminated\n+  \/\/\n+  volatile TerminatedTypes _terminated;\n+\n+  jint                  _in_deopt_handler;       \/\/ count of deoptimization\n+                                                 \/\/ handlers thread is in\n+  volatile bool         _doing_unsafe_access;    \/\/ Thread may fault due to unsafe access\n+  bool                  _do_not_unlock_if_synchronized;  \/\/ Do not unlock the receiver of a synchronized method (since it was\n+                                                         \/\/ never locked) when throwing an exception. Used by interpreter only.\n+#if INCLUDE_JVMTI\n+  volatile bool         _carrier_thread_suspended;       \/\/ Carrier thread is externally suspended\n+  bool                  _is_in_VTMS_transition;          \/\/ thread is in virtual thread mount state transition\n+#ifdef ASSERT\n+  bool                  _is_VTMS_transition_disabler;    \/\/ thread currently disabled VTMS transitions\n+#endif\n+#endif\n+\n+  \/\/ JNI attach states:\n+  enum JNIAttachStates {\n+    _not_attaching_via_jni = 1,  \/\/ thread is not attaching via JNI\n+    _attaching_via_jni,          \/\/ thread is attaching via JNI\n+    _attached_via_jni            \/\/ thread has attached via JNI\n+  };\n+\n+  \/\/ A regular JavaThread's _jni_attach_state is _not_attaching_via_jni.\n+  \/\/ A native thread that is attaching via JNI starts with a value\n+  \/\/ of _attaching_via_jni and transitions to _attached_via_jni.\n+  volatile JNIAttachStates _jni_attach_state;\n+\n+\n+#if INCLUDE_JVMCI\n+  \/\/ The _pending_* fields below are used to communicate extra information\n+  \/\/ from an uncommon trap in JVMCI compiled code to the uncommon trap handler.\n+\n+  \/\/ Communicates the DeoptReason and DeoptAction of the uncommon trap\n+  int       _pending_deoptimization;\n+\n+  \/\/ Specifies whether the uncommon trap is to bci 0 of a synchronized method\n+  \/\/ before the monitor has been acquired.\n+  bool      _pending_monitorenter;\n+\n+  \/\/ Specifies if the DeoptReason for the last uncommon trap was Reason_transfer_to_interpreter\n+  bool      _pending_transfer_to_interpreter;\n+\n+  \/\/ True if in a runtime call from compiled code that will deoptimize\n+  \/\/ and re-execute a failed heap allocation in the interpreter.\n+  bool      _in_retryable_allocation;\n+\n+  \/\/ An id of a speculation that JVMCI compiled code can use to further describe and\n+  \/\/ uniquely identify the speculative optimization guarded by an uncommon trap.\n+  \/\/ See JVMCINMethodData::SPECULATION_LENGTH_BITS for further details.\n+  jlong     _pending_failed_speculation;\n+\n+  \/\/ These fields are mutually exclusive in terms of live ranges.\n+  union {\n+    \/\/ Communicates the pc at which the most recent implicit exception occurred\n+    \/\/ from the signal handler to a deoptimization stub.\n+    address   _implicit_exception_pc;\n+\n+    \/\/ Communicates an alternative call target to an i2c stub from a JavaCall .\n+    address   _alternate_call_target;\n+  } _jvmci;\n+\n+  \/\/ The JVMCIRuntime in a JVMCI shared library\n+  JVMCIRuntime* _libjvmci_runtime;\n+\n+  \/\/ Support for high precision, thread sensitive counters in JVMCI compiled code.\n+  jlong*    _jvmci_counters;\n+\n+  \/\/ Fast thread locals for use by JVMCI\n+  jlong      _jvmci_reserved0;\n+  jlong      _jvmci_reserved1;\n+  oop        _jvmci_reserved_oop0;\n+\n+ public:\n+  static jlong* _jvmci_old_thread_counters;\n+  static void collect_counters(jlong* array, int length);\n+\n+  bool resize_counters(int current_size, int new_size);\n+\n+  static bool resize_all_jvmci_counters(int new_size);\n+\n+  void set_jvmci_reserved_oop0(oop value) {\n+    _jvmci_reserved_oop0 = value;\n+  }\n+\n+  oop get_jvmci_reserved_oop0() {\n+    return _jvmci_reserved_oop0;\n+  }\n+\n+  void set_jvmci_reserved0(jlong value) {\n+    _jvmci_reserved0 = value;\n+  }\n+\n+  jlong get_jvmci_reserved0() {\n+    return _jvmci_reserved0;\n+  }\n+\n+  void set_jvmci_reserved1(jlong value) {\n+    _jvmci_reserved1 = value;\n+  }\n+\n+  jlong get_jvmci_reserved1() {\n+    return _jvmci_reserved1;\n+  }\n+\n+ private:\n+#endif \/\/ INCLUDE_JVMCI\n+\n+  StackOverflow    _stack_overflow_state;\n+\n+  \/\/ Compiler exception handling (NOTE: The _exception_oop is *NOT* the same as _pending_exception. It is\n+  \/\/ used to temp. parsing values into and out of the runtime system during exception handling for compiled\n+  \/\/ code)\n+  volatile oop     _exception_oop;               \/\/ Exception thrown in compiled code\n+  volatile address _exception_pc;                \/\/ PC where exception happened\n+  volatile address _exception_handler_pc;        \/\/ PC for handler of exception\n+  volatile int     _is_method_handle_return;     \/\/ true (== 1) if the current exception PC is a MethodHandle call site.\n+\n+ private:\n+  \/\/ support for JNI critical regions\n+  jint    _jni_active_critical;                  \/\/ count of entries into JNI critical region\n+\n+  \/\/ Checked JNI: function name requires exception check\n+  char* _pending_jni_exception_check_fn;\n+\n+  \/\/ For deadlock detection.\n+  int _depth_first_number;\n+\n+  \/\/ JVMTI PopFrame support\n+  \/\/ This is set to popframe_pending to signal that top Java frame should be popped immediately\n+  int _popframe_condition;\n+\n+  \/\/ If reallocation of scalar replaced objects fails, we throw OOM\n+  \/\/ and during exception propagation, pop the top\n+  \/\/ _frames_to_pop_failed_realloc frames, the ones that reference\n+  \/\/ failed reallocations.\n+  int _frames_to_pop_failed_realloc;\n+\n+  ContinuationEntry* _cont_entry;\n+  intptr_t* _cont_fastpath; \/\/ the sp of the oldest known interpreted\/call_stub frame inside the\n+                            \/\/ continuation that we know about\n+  int _cont_fastpath_thread_state; \/\/ whether global thread state allows continuation fastpath (JVMTI)\n+  \/\/ It's signed for error detection.\n+#ifdef _LP64\n+  int64_t _held_monitor_count;  \/\/ used by continuations for fast lock detection\n+  int64_t _jni_monitor_count;\n+#else\n+  int32_t _held_monitor_count;  \/\/ used by continuations for fast lock detection\n+  int32_t _jni_monitor_count;\n+#endif\n+\n+private:\n+\n+  friend class VMThread;\n+  friend class ThreadWaitTransition;\n+  friend class VM_Exit;\n+\n+  \/\/ Stack watermark barriers.\n+  StackWatermarks _stack_watermarks;\n+\n+ public:\n+  inline StackWatermarks* stack_watermarks() { return &_stack_watermarks; }\n+\n+ public:\n+  jlong _extentLocal_hash_table_shift;\n+\n+  void allocate_extentLocal_hash_table(int count);\n+\n+ public:\n+  \/\/ Constructor\n+  JavaThread();                            \/\/ delegating constructor\n+  JavaThread(bool is_attaching_via_jni);   \/\/ for main thread and JNI attached threads\n+  JavaThread(ThreadFunction entry_point, size_t stack_size = 0);\n+  ~JavaThread();\n+\n+#ifdef ASSERT\n+  \/\/ verify this JavaThread hasn't be published in the Threads::list yet\n+  void verify_not_published();\n+#endif \/\/ ASSERT\n+\n+  StackOverflow* stack_overflow_state() { return &_stack_overflow_state; }\n+\n+  \/\/JNI functiontable getter\/setter for JVMTI jni function table interception API.\n+  void set_jni_functions(struct JNINativeInterface_* functionTable) {\n+    _jni_environment.functions = functionTable;\n+  }\n+  struct JNINativeInterface_* get_jni_functions() {\n+    return (struct JNINativeInterface_ *)_jni_environment.functions;\n+  }\n+\n+  \/\/ This function is called at thread creation to allow\n+  \/\/ platform specific thread variables to be initialized.\n+  void cache_global_variables();\n+\n+  \/\/ Executes Shutdown.shutdown()\n+  void invoke_shutdown_hooks();\n+\n+  \/\/ Cleanup on thread exit\n+  enum ExitType {\n+    normal_exit,\n+    jni_detach\n+  };\n+  void exit(bool destroy_vm, ExitType exit_type = normal_exit);\n+\n+  void cleanup_failed_attach_current_thread(bool is_daemon);\n+\n+  \/\/ Testers\n+  virtual bool is_Java_thread() const            { return true;  }\n+  virtual bool can_call_java() const             { return true; }\n+\n+  virtual bool is_active_Java_thread() const {\n+    return on_thread_list() && !is_terminated();\n+  }\n+\n+  \/\/ Thread oop. threadObj() can be NULL for initial JavaThread\n+  \/\/ (or for threads attached via JNI)\n+  oop threadObj() const;\n+  void set_threadOopHandles(oop p);\n+  oop vthread() const;\n+  void set_vthread(oop p);\n+  oop extentLocalCache() const;\n+  void set_extentLocalCache(oop p);\n+  oop jvmti_vthread() const;\n+  void set_jvmti_vthread(oop p);\n+\n+  \/\/ Prepare thread and add to priority queue.  If a priority is\n+  \/\/ not specified, use the priority of the thread object. Threads_lock\n+  \/\/ must be held while this function is called.\n+  void prepare(jobject jni_thread, ThreadPriority prio=NoPriority);\n+\n+  void set_saved_exception_pc(address pc)        { _saved_exception_pc = pc; }\n+  address saved_exception_pc()                   { return _saved_exception_pc; }\n+\n+  ThreadFunction entry_point() const             { return _entry_point; }\n+\n+  \/\/ Allocates a new Java level thread object for this thread. thread_name may be NULL.\n+  void allocate_threadObj(Handle thread_group, const char* thread_name, bool daemon, TRAPS);\n+\n+  \/\/ Last frame anchor routines\n+\n+  JavaFrameAnchor* frame_anchor(void)            { return &_anchor; }\n+\n+  \/\/ last_Java_sp\n+  bool has_last_Java_frame() const               { return _anchor.has_last_Java_frame(); }\n+  intptr_t* last_Java_sp() const                 { return _anchor.last_Java_sp(); }\n+\n+  \/\/ last_Java_pc\n+\n+  address last_Java_pc(void)                     { return _anchor.last_Java_pc(); }\n+\n+  \/\/ Safepoint support\n+  inline JavaThreadState thread_state() const;\n+  inline void set_thread_state(JavaThreadState s);\n+  inline void set_thread_state_fence(JavaThreadState s);  \/\/ fence after setting thread state\n+  inline ThreadSafepointState* safepoint_state() const;\n+  inline void set_safepoint_state(ThreadSafepointState* state);\n+  inline bool is_at_poll_safepoint();\n+\n+  \/\/ JavaThread termination and lifecycle support:\n+  void smr_delete();\n+  bool on_thread_list() const { return _on_thread_list; }\n+  void set_on_thread_list() { _on_thread_list = true; }\n+\n+  \/\/ thread has called JavaThread::exit(), thread's GC barrier is detached\n+  \/\/ or thread is terminated\n+  bool is_exiting() const;\n+  \/\/ thread's GC barrier is NOT detached and thread is NOT terminated\n+  bool is_oop_safe() const;\n+  \/\/ thread is terminated (no longer on the threads list); the thread must\n+  \/\/ be protected by a ThreadsListHandle to avoid potential crashes.\n+  bool check_is_terminated(TerminatedTypes l_terminated) const {\n+    return l_terminated == _thread_terminated || l_terminated == _vm_exited;\n+  }\n+  bool is_terminated() const;\n+  void set_terminated(TerminatedTypes t);\n+\n+  void block_if_vm_exited();\n+\n+  bool doing_unsafe_access()                     { return _doing_unsafe_access; }\n+  void set_doing_unsafe_access(bool val)         { _doing_unsafe_access = val; }\n+\n+  bool do_not_unlock_if_synchronized()             { return _do_not_unlock_if_synchronized; }\n+  void set_do_not_unlock_if_synchronized(bool val) { _do_not_unlock_if_synchronized = val; }\n+\n+  SafepointMechanism::ThreadData* poll_data() { return &_poll_data; }\n+\n+  void set_requires_cross_modify_fence(bool val) PRODUCT_RETURN NOT_PRODUCT({ _requires_cross_modify_fence = val; })\n+\n+  \/\/ Continuation support\n+  oop get_continuation() const;\n+  ContinuationEntry* last_continuation() const { return _cont_entry; }\n+  void set_cont_fastpath(intptr_t* x)          { _cont_fastpath = x; }\n+  void push_cont_fastpath(intptr_t* sp)        { if (sp > _cont_fastpath) _cont_fastpath = sp; }\n+  void set_cont_fastpath_thread_state(bool x)  { _cont_fastpath_thread_state = (int)x; }\n+  intptr_t* raw_cont_fastpath() const          { return _cont_fastpath; }\n+  bool cont_fastpath() const                   { return _cont_fastpath == NULL && _cont_fastpath_thread_state != 0; }\n+  bool cont_fastpath_thread_state() const      { return _cont_fastpath_thread_state != 0; }\n+\n+  void inc_held_monitor_count(int i = 1, bool jni = false);\n+  void dec_held_monitor_count(int i = 1, bool jni = false);\n+\n+  int64_t held_monitor_count() { return (int64_t)_held_monitor_count; }\n+  int64_t jni_monitor_count()  { return (int64_t)_jni_monitor_count;  }\n+  void clear_jni_monitor_count() { _jni_monitor_count = 0;   }\n+\n+  inline bool is_vthread_mounted() const;\n+  inline const ContinuationEntry* vthread_continuation() const;\n+\n+ private:\n+  DEBUG_ONLY(void verify_frame_info();)\n+\n+  \/\/ Support for thread handshake operations\n+  HandshakeState _handshake;\n+ public:\n+  HandshakeState* handshake_state() { return &_handshake; }\n+\n+  \/\/ A JavaThread can always safely operate on it self and other threads\n+  \/\/ can do it safely if they are the active handshaker.\n+  bool is_handshake_safe_for(Thread* th) const {\n+    return _handshake.active_handshaker() == th || this == th;\n+  }\n+\n+  \/\/ Suspend\/resume support for JavaThread\n+  \/\/ higher-level suspension\/resume logic called by the public APIs\n+  bool java_suspend();\n+  bool java_resume();\n+  bool is_suspended()     { return _handshake.is_suspended(); }\n+\n+  \/\/ Check for async exception in addition to safepoint.\n+  static void check_special_condition_for_native_trans(JavaThread *thread);\n+\n+  \/\/ Synchronize with another thread that is deoptimizing objects of the\n+  \/\/ current thread, i.e. reverts optimizations based on escape analysis.\n+  void wait_for_object_deoptimization();\n+\n+#if INCLUDE_JVMTI\n+  inline void set_carrier_thread_suspended();\n+  inline void clear_carrier_thread_suspended();\n+\n+  bool is_carrier_thread_suspended() const {\n+    return _carrier_thread_suspended;\n+  }\n+\n+  bool is_in_VTMS_transition() const             { return _is_in_VTMS_transition; }\n+  void set_is_in_VTMS_transition(bool val);\n+#ifdef ASSERT\n+  bool is_VTMS_transition_disabler() const       { return _is_VTMS_transition_disabler; }\n+  void set_is_VTMS_transition_disabler(bool val);\n+#endif\n+#endif\n+\n+  \/\/ Support for object deoptimization and JFR suspension\n+  void handle_special_runtime_exit_condition();\n+  bool has_special_runtime_exit_condition() {\n+    return (_suspend_flags & (_obj_deopt JFR_ONLY(| _trace_flag))) != 0;\n+  }\n+\n+  \/\/ Fast-locking support\n+  bool is_lock_owned(address adr) const;\n+  bool is_lock_owned_current(address adr) const; \/\/ virtual if mounted, otherwise whole thread\n+  bool is_lock_owned_carrier(address adr) const;\n+\n+  \/\/ Accessors for vframe array top\n+  \/\/ The linked list of vframe arrays are sorted on sp. This means when we\n+  \/\/ unpack the head must contain the vframe array to unpack.\n+  void set_vframe_array_head(vframeArray* value) { _vframe_array_head = value; }\n+  vframeArray* vframe_array_head() const         { return _vframe_array_head;  }\n+\n+  \/\/ Side structure for deferring update of java frame locals until deopt occurs\n+  JvmtiDeferredUpdates* deferred_updates() const      { return _jvmti_deferred_updates; }\n+  void set_deferred_updates(JvmtiDeferredUpdates* du) { _jvmti_deferred_updates = du; }\n+\n+  \/\/ These only really exist to make debugging deopt problems simpler\n+\n+  void set_vframe_array_last(vframeArray* value) { _vframe_array_last = value; }\n+  vframeArray* vframe_array_last() const         { return _vframe_array_last;  }\n+\n+  \/\/ The special resourceMark used during deoptimization\n+\n+  void set_deopt_mark(DeoptResourceMark* value)  { _deopt_mark = value; }\n+  DeoptResourceMark* deopt_mark(void)            { return _deopt_mark; }\n+\n+  void set_deopt_compiled_method(CompiledMethod* nm)  { _deopt_nmethod = nm; }\n+  CompiledMethod* deopt_compiled_method()        { return _deopt_nmethod; }\n+\n+  Method*    callee_target() const               { return _callee_target; }\n+  void set_callee_target  (Method* x)          { _callee_target   = x; }\n+\n+  \/\/ Oop results of vm runtime calls\n+  oop  vm_result() const                         { return _vm_result; }\n+  void set_vm_result  (oop x)                    { _vm_result   = x; }\n+\n+  Metadata*    vm_result_2() const               { return _vm_result_2; }\n+  void set_vm_result_2  (Metadata* x)          { _vm_result_2   = x; }\n+\n+  oop return_buffered_value() const              { return _return_buffered_value; }\n+  void set_return_buffered_value(oop val)        { _return_buffered_value = val; }\n+\n+  MemRegion deferred_card_mark() const           { return _deferred_card_mark; }\n+  void set_deferred_card_mark(MemRegion mr)      { _deferred_card_mark = mr;   }\n+\n+#if INCLUDE_JVMCI\n+  int  pending_deoptimization() const             { return _pending_deoptimization; }\n+  jlong pending_failed_speculation() const        { return _pending_failed_speculation; }\n+  bool has_pending_monitorenter() const           { return _pending_monitorenter; }\n+  void set_pending_monitorenter(bool b)           { _pending_monitorenter = b; }\n+  void set_pending_deoptimization(int reason)     { _pending_deoptimization = reason; }\n+  void set_pending_failed_speculation(jlong failed_speculation) { _pending_failed_speculation = failed_speculation; }\n+  void set_pending_transfer_to_interpreter(bool b) { _pending_transfer_to_interpreter = b; }\n+  void set_jvmci_alternate_call_target(address a) { assert(_jvmci._alternate_call_target == NULL, \"must be\"); _jvmci._alternate_call_target = a; }\n+  void set_jvmci_implicit_exception_pc(address a) { assert(_jvmci._implicit_exception_pc == NULL, \"must be\"); _jvmci._implicit_exception_pc = a; }\n+\n+  virtual bool in_retryable_allocation() const    { return _in_retryable_allocation; }\n+  void set_in_retryable_allocation(bool b)        { _in_retryable_allocation = b; }\n+\n+  JVMCIRuntime* libjvmci_runtime() const          { return _libjvmci_runtime; }\n+  void set_libjvmci_runtime(JVMCIRuntime* rt) {\n+    assert((_libjvmci_runtime == nullptr && rt != nullptr) || (_libjvmci_runtime != nullptr && rt == nullptr), \"must be\");\n+    _libjvmci_runtime = rt;\n+  }\n+#endif \/\/ INCLUDE_JVMCI\n+\n+  \/\/ Exception handling for compiled methods\n+  oop      exception_oop() const;\n+  address  exception_pc() const                  { return _exception_pc; }\n+  address  exception_handler_pc() const          { return _exception_handler_pc; }\n+  bool     is_method_handle_return() const       { return _is_method_handle_return == 1; }\n+\n+  void set_exception_oop(oop o);\n+  void set_exception_pc(address a)               { _exception_pc = a; }\n+  void set_exception_handler_pc(address a)       { _exception_handler_pc = a; }\n+  void set_is_method_handle_return(bool value)   { _is_method_handle_return = value ? 1 : 0; }\n+\n+  void clear_exception_oop_and_pc() {\n+    set_exception_oop(NULL);\n+    set_exception_pc(NULL);\n+  }\n+\n+  \/\/ Check if address is in the usable part of the stack (excludes protected\n+  \/\/ guard pages). Can be applied to any thread and is an approximation for\n+  \/\/ using is_in_live_stack when the query has to happen from another thread.\n+  bool is_in_usable_stack(address adr) const {\n+    return is_in_stack_range_incl(adr, _stack_overflow_state.stack_reserved_zone_base());\n+  }\n+\n+  \/\/ Misc. accessors\/mutators\n+  void set_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = true; }\n+  void clr_do_not_unlock(void)                   { _do_not_unlock_if_synchronized = false; }\n+  bool do_not_unlock(void)                       { return _do_not_unlock_if_synchronized; }\n+\n+  static ByteSize extentLocalCache_offset()       { return byte_offset_of(JavaThread, _extentLocalCache); }\n+\n+  \/\/ For assembly stub generation\n+  static ByteSize threadObj_offset()             { return byte_offset_of(JavaThread, _threadObj); }\n+  static ByteSize vthread_offset()               { return byte_offset_of(JavaThread, _vthread); }\n+  static ByteSize jni_environment_offset()       { return byte_offset_of(JavaThread, _jni_environment); }\n+  static ByteSize pending_jni_exception_check_fn_offset() {\n+    return byte_offset_of(JavaThread, _pending_jni_exception_check_fn);\n+  }\n+  static ByteSize last_Java_sp_offset() {\n+    return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_sp_offset();\n+  }\n+  static ByteSize last_Java_pc_offset() {\n+    return byte_offset_of(JavaThread, _anchor) + JavaFrameAnchor::last_Java_pc_offset();\n+  }\n+  static ByteSize frame_anchor_offset() {\n+    return byte_offset_of(JavaThread, _anchor);\n+  }\n+  static ByteSize callee_target_offset()         { return byte_offset_of(JavaThread, _callee_target); }\n+  static ByteSize vm_result_offset()             { return byte_offset_of(JavaThread, _vm_result); }\n+  static ByteSize vm_result_2_offset()           { return byte_offset_of(JavaThread, _vm_result_2); }\n+  static ByteSize return_buffered_value_offset() { return byte_offset_of(JavaThread, _return_buffered_value); }\n+  static ByteSize thread_state_offset()          { return byte_offset_of(JavaThread, _thread_state); }\n+  static ByteSize polling_word_offset()          { return byte_offset_of(JavaThread, _poll_data) + byte_offset_of(SafepointMechanism::ThreadData, _polling_word);}\n+  static ByteSize polling_page_offset()          { return byte_offset_of(JavaThread, _poll_data) + byte_offset_of(SafepointMechanism::ThreadData, _polling_page);}\n+  static ByteSize saved_exception_pc_offset()    { return byte_offset_of(JavaThread, _saved_exception_pc); }\n+  static ByteSize osthread_offset()              { return byte_offset_of(JavaThread, _osthread); }\n+#if INCLUDE_JVMCI\n+  static ByteSize pending_deoptimization_offset() { return byte_offset_of(JavaThread, _pending_deoptimization); }\n+  static ByteSize pending_monitorenter_offset()  { return byte_offset_of(JavaThread, _pending_monitorenter); }\n+  static ByteSize pending_failed_speculation_offset() { return byte_offset_of(JavaThread, _pending_failed_speculation); }\n+  static ByteSize jvmci_alternate_call_target_offset() { return byte_offset_of(JavaThread, _jvmci._alternate_call_target); }\n+  static ByteSize jvmci_implicit_exception_pc_offset() { return byte_offset_of(JavaThread, _jvmci._implicit_exception_pc); }\n+  static ByteSize jvmci_counters_offset()        { return byte_offset_of(JavaThread, _jvmci_counters); }\n+#endif \/\/ INCLUDE_JVMCI\n+  static ByteSize exception_oop_offset()         { return byte_offset_of(JavaThread, _exception_oop); }\n+  static ByteSize exception_pc_offset()          { return byte_offset_of(JavaThread, _exception_pc); }\n+  static ByteSize exception_handler_pc_offset()  { return byte_offset_of(JavaThread, _exception_handler_pc); }\n+  static ByteSize is_method_handle_return_offset() { return byte_offset_of(JavaThread, _is_method_handle_return); }\n+\n+  static ByteSize active_handles_offset()        { return byte_offset_of(JavaThread, _active_handles); }\n+\n+  \/\/ StackOverflow offsets\n+  static ByteSize stack_overflow_limit_offset()  {\n+    return byte_offset_of(JavaThread, _stack_overflow_state._stack_overflow_limit);\n+  }\n+  static ByteSize stack_guard_state_offset()     {\n+    return byte_offset_of(JavaThread, _stack_overflow_state._stack_guard_state);\n+  }\n+  static ByteSize reserved_stack_activation_offset() {\n+    return byte_offset_of(JavaThread, _stack_overflow_state._reserved_stack_activation);\n+  }\n+  static ByteSize shadow_zone_safe_limit()  {\n+    return byte_offset_of(JavaThread, _stack_overflow_state._shadow_zone_safe_limit);\n+  }\n+  static ByteSize shadow_zone_growth_watermark()  {\n+    return byte_offset_of(JavaThread, _stack_overflow_state._shadow_zone_growth_watermark);\n+  }\n+\n+  static ByteSize suspend_flags_offset()         { return byte_offset_of(JavaThread, _suspend_flags); }\n+\n+  static ByteSize do_not_unlock_if_synchronized_offset() { return byte_offset_of(JavaThread, _do_not_unlock_if_synchronized); }\n+  static ByteSize should_post_on_exceptions_flag_offset() {\n+    return byte_offset_of(JavaThread, _should_post_on_exceptions_flag);\n+  }\n+  static ByteSize doing_unsafe_access_offset() { return byte_offset_of(JavaThread, _doing_unsafe_access); }\n+  NOT_PRODUCT(static ByteSize requires_cross_modify_fence_offset()  { return byte_offset_of(JavaThread, _requires_cross_modify_fence); })\n+\n+  static ByteSize cont_entry_offset()         { return byte_offset_of(JavaThread, _cont_entry); }\n+  static ByteSize cont_fastpath_offset()      { return byte_offset_of(JavaThread, _cont_fastpath); }\n+  static ByteSize held_monitor_count_offset() { return byte_offset_of(JavaThread, _held_monitor_count); }\n+\n+  \/\/ Returns the jni environment for this thread\n+  JNIEnv* jni_environment()                      { return &_jni_environment; }\n+\n+  \/\/ Returns the current thread as indicated by the given JNIEnv.\n+  \/\/ We don't assert it is Thread::current here as that is done at the\n+  \/\/ external JNI entry points where the JNIEnv is passed into the VM.\n+  static JavaThread* thread_from_jni_environment(JNIEnv* env) {\n+    JavaThread* current = (JavaThread*)((intptr_t)env - in_bytes(jni_environment_offset()));\n+    \/\/ We can't normally get here in a thread that has completed its\n+    \/\/ execution and so \"is_terminated\", except when the call is from\n+    \/\/ AsyncGetCallTrace, which can be triggered by a signal at any point in\n+    \/\/ a thread's lifecycle. A thread is also considered terminated if the VM\n+    \/\/ has exited, so we have to check this and block in case this is a daemon\n+    \/\/ thread returning to the VM (the JNI DirectBuffer entry points rely on\n+    \/\/ this).\n+    if (current->is_terminated()) {\n+      current->block_if_vm_exited();\n+    }\n+    return current;\n+  }\n+\n+  \/\/ JNI critical regions. These can nest.\n+  bool in_critical()    { return _jni_active_critical > 0; }\n+  bool in_last_critical()  { return _jni_active_critical == 1; }\n+  inline void enter_critical();\n+  void exit_critical() {\n+    assert(Thread::current() == this, \"this must be current thread\");\n+    _jni_active_critical--;\n+    assert(_jni_active_critical >= 0, \"JNI critical nesting problem?\");\n+  }\n+\n+  \/\/ Checked JNI: is the programmer required to check for exceptions, if so specify\n+  \/\/ which function name. Returning to a Java frame should implicitly clear the\n+  \/\/ pending check, this is done for Native->Java transitions (i.e. user JNI code).\n+  \/\/ VM->Java transitions are not cleared, it is expected that JNI code enclosed\n+  \/\/ within ThreadToNativeFromVM makes proper exception checks (i.e. VM internal).\n+  bool is_pending_jni_exception_check() const { return _pending_jni_exception_check_fn != NULL; }\n+  void clear_pending_jni_exception_check() { _pending_jni_exception_check_fn = NULL; }\n+  const char* get_pending_jni_exception_check() const { return _pending_jni_exception_check_fn; }\n+  void set_pending_jni_exception_check(const char* fn_name) { _pending_jni_exception_check_fn = (char*) fn_name; }\n+\n+  \/\/ For deadlock detection\n+  int depth_first_number() { return _depth_first_number; }\n+  void set_depth_first_number(int dfn) { _depth_first_number = dfn; }\n+\n+ private:\n+  void set_monitor_chunks(MonitorChunk* monitor_chunks) { _monitor_chunks = monitor_chunks; }\n+\n+ public:\n+  MonitorChunk* monitor_chunks() const           { return _monitor_chunks; }\n+  void add_monitor_chunk(MonitorChunk* chunk);\n+  void remove_monitor_chunk(MonitorChunk* chunk);\n+  bool in_deopt_handler() const                  { return _in_deopt_handler > 0; }\n+  void inc_in_deopt_handler()                    { _in_deopt_handler++; }\n+  void dec_in_deopt_handler() {\n+    assert(_in_deopt_handler > 0, \"mismatched deopt nesting\");\n+    if (_in_deopt_handler > 0) { \/\/ robustness\n+      _in_deopt_handler--;\n+    }\n+  }\n+\n+ private:\n+  void set_entry_point(ThreadFunction entry_point) { _entry_point = entry_point; }\n+\n+  \/\/ factor out low-level mechanics for use in both normal and error cases\n+  const char* get_thread_name_string(char* buf = NULL, int buflen = 0) const;\n+\n+ public:\n+\n+  \/\/ Frame iteration; calls the function f for all frames on the stack\n+  void frames_do(void f(frame*, const RegisterMap*));\n+\n+  \/\/ Memory operations\n+  void oops_do_frames(OopClosure* f, CodeBlobClosure* cf);\n+  void oops_do_no_frames(OopClosure* f, CodeBlobClosure* cf);\n+\n+  \/\/ Sweeper operations\n+  virtual void nmethods_do(CodeBlobClosure* cf);\n+\n+  \/\/ RedefineClasses Support\n+  void metadata_do(MetadataClosure* f);\n+\n+  \/\/ Debug method asserting thread states are correct during a handshake operation.\n+  DEBUG_ONLY(void verify_states_for_handshake();)\n+\n+  \/\/ Misc. operations\n+  const char* name() const;\n+  const char* type_name() const { return \"JavaThread\"; }\n+  static const char* name_for(oop thread_obj);\n+\n+  void print_on(outputStream* st, bool print_extended_info) const;\n+  void print_on(outputStream* st) const { print_on(st, false); }\n+  void print() const;\n+  void print_thread_state_on(outputStream*) const;\n+  const char* thread_state_name() const;\n+  void print_on_error(outputStream* st, char* buf, int buflen) const;\n+  void print_name_on_error(outputStream* st, char* buf, int buflen) const;\n+  void verify();\n+\n+  \/\/ Accessing frames\n+  frame last_frame() {\n+    _anchor.make_walkable();\n+    return pd_last_frame();\n+  }\n+  javaVFrame* last_java_vframe(RegisterMap* reg_map) { return last_java_vframe(last_frame(), reg_map); }\n+\n+  frame carrier_last_frame(RegisterMap* reg_map);\n+  javaVFrame* carrier_last_java_vframe(RegisterMap* reg_map) { return last_java_vframe(carrier_last_frame(reg_map), reg_map); }\n+\n+  frame vthread_last_frame();\n+  javaVFrame* vthread_last_java_vframe(RegisterMap* reg_map) { return last_java_vframe(vthread_last_frame(), reg_map); }\n+\n+  frame platform_thread_last_frame(RegisterMap* reg_map);\n+  javaVFrame*  platform_thread_last_java_vframe(RegisterMap* reg_map) {\n+    return last_java_vframe(platform_thread_last_frame(reg_map), reg_map);\n+  }\n+\n+  javaVFrame* last_java_vframe(const frame f, RegisterMap* reg_map);\n+\n+  \/\/ Returns method at 'depth' java or native frames down the stack\n+  \/\/ Used for security checks\n+  Klass* security_get_caller_class(int depth);\n+\n+  \/\/ Print stack trace in external format\n+  void print_stack_on(outputStream* st);\n+  void print_stack() { print_stack_on(tty); }\n+\n+  \/\/ Print stack traces in various internal formats\n+  void trace_stack()                             PRODUCT_RETURN;\n+  void trace_stack_from(vframe* start_vf)        PRODUCT_RETURN;\n+  void trace_frames()                            PRODUCT_RETURN;\n+\n+  \/\/ Print an annotated view of the stack frames\n+  void print_frame_layout(int depth = 0, bool validate_only = false) NOT_DEBUG_RETURN;\n+  void validate_frame_layout() {\n+    print_frame_layout(0, true);\n+  }\n+\n+  \/\/ Function for testing deoptimization\n+  void deoptimize();\n+  void make_zombies();\n+\n+  void deoptimize_marked_methods();\n+\n+ public:\n+  \/\/ Returns the running thread as a JavaThread\n+  static JavaThread* current() {\n+    return JavaThread::cast(Thread::current());\n+  }\n+\n+  \/\/ Returns the current thread as a JavaThread, or NULL if not attached\n+  static inline JavaThread* current_or_null();\n+\n+  \/\/ Casts\n+  static JavaThread* cast(Thread* t) {\n+    assert(t->is_Java_thread(), \"incorrect cast to JavaThread\");\n+    return static_cast<JavaThread*>(t);\n+  }\n+\n+  static const JavaThread* cast(const Thread* t) {\n+    assert(t->is_Java_thread(), \"incorrect cast to const JavaThread\");\n+    return static_cast<const JavaThread*>(t);\n+  }\n+\n+  \/\/ Returns the active Java thread.  Do not use this if you know you are calling\n+  \/\/ from a JavaThread, as it's slower than JavaThread::current.  If called from\n+  \/\/ the VMThread, it also returns the JavaThread that instigated the VMThread's\n+  \/\/ operation.  You may not want that either.\n+  static JavaThread* active();\n+\n+ protected:\n+  virtual void pre_run();\n+  virtual void run();\n+  void thread_main_inner();\n+  virtual void post_run();\n+\n+ public:\n+  \/\/ Thread local information maintained by JVMTI.\n+  void set_jvmti_thread_state(JvmtiThreadState *value)                           { _jvmti_thread_state = value; }\n+  \/\/ A JvmtiThreadState is lazily allocated. This jvmti_thread_state()\n+  \/\/ getter is used to get this JavaThread's JvmtiThreadState if it has\n+  \/\/ one which means NULL can be returned. JvmtiThreadState::state_for()\n+  \/\/ is used to get the specified JavaThread's JvmtiThreadState if it has\n+  \/\/ one or it allocates a new JvmtiThreadState for the JavaThread and\n+  \/\/ returns it. JvmtiThreadState::state_for() will return NULL only if\n+  \/\/ the specified JavaThread is exiting.\n+  JvmtiThreadState *jvmti_thread_state() const                                   { return _jvmti_thread_state; }\n+  static ByteSize jvmti_thread_state_offset()                                    { return byte_offset_of(JavaThread, _jvmti_thread_state); }\n+\n+#if INCLUDE_JVMTI\n+  \/\/ Rebind JVMTI thread state from carrier to virtual or from virtual to carrier.\n+  JvmtiThreadState *rebind_to_jvmti_thread_state_of(oop thread_oop);\n+#endif\n+\n+  \/\/ JVMTI PopFrame support\n+  \/\/ Setting and clearing popframe_condition\n+  \/\/ All of these enumerated values are bits. popframe_pending\n+  \/\/ indicates that a PopFrame() has been requested and not yet been\n+  \/\/ completed. popframe_processing indicates that that PopFrame() is in\n+  \/\/ the process of being completed. popframe_force_deopt_reexecution_bit\n+  \/\/ indicates that special handling is required when returning to a\n+  \/\/ deoptimized caller.\n+  enum PopCondition {\n+    popframe_inactive                      = 0x00,\n+    popframe_pending_bit                   = 0x01,\n+    popframe_processing_bit                = 0x02,\n+    popframe_force_deopt_reexecution_bit   = 0x04\n+  };\n+  PopCondition popframe_condition()                   { return (PopCondition) _popframe_condition; }\n+  void set_popframe_condition(PopCondition c)         { _popframe_condition = c; }\n+  void set_popframe_condition_bit(PopCondition c)     { _popframe_condition |= c; }\n+  void clear_popframe_condition()                     { _popframe_condition = popframe_inactive; }\n+  static ByteSize popframe_condition_offset()         { return byte_offset_of(JavaThread, _popframe_condition); }\n+  bool has_pending_popframe()                         { return (popframe_condition() & popframe_pending_bit) != 0; }\n+  bool popframe_forcing_deopt_reexecution()           { return (popframe_condition() & popframe_force_deopt_reexecution_bit) != 0; }\n+  void clear_popframe_forcing_deopt_reexecution()     { _popframe_condition &= ~popframe_force_deopt_reexecution_bit; }\n+\n+  bool pop_frame_in_process(void)                     { return ((_popframe_condition & popframe_processing_bit) != 0); }\n+  void set_pop_frame_in_process(void)                 { _popframe_condition |= popframe_processing_bit; }\n+  void clr_pop_frame_in_process(void)                 { _popframe_condition &= ~popframe_processing_bit; }\n+\n+  int frames_to_pop_failed_realloc() const            { return _frames_to_pop_failed_realloc; }\n+  void set_frames_to_pop_failed_realloc(int nb)       { _frames_to_pop_failed_realloc = nb; }\n+  void dec_frames_to_pop_failed_realloc()             { _frames_to_pop_failed_realloc--; }\n+\n+ private:\n+  \/\/ Saved incoming arguments to popped frame.\n+  \/\/ Used only when popped interpreted frame returns to deoptimized frame.\n+  void*    _popframe_preserved_args;\n+  int      _popframe_preserved_args_size;\n+\n+ public:\n+  void  popframe_preserve_args(ByteSize size_in_bytes, void* start);\n+  void* popframe_preserved_args();\n+  ByteSize popframe_preserved_args_size();\n+  WordSize popframe_preserved_args_size_in_words();\n+  void  popframe_free_preserved_args();\n+\n+\n+ private:\n+  JvmtiThreadState *_jvmti_thread_state;\n+\n+  \/\/ Used by the interpreter in fullspeed mode for frame pop, method\n+  \/\/ entry, method exit and single stepping support. This field is\n+  \/\/ only set to non-zero at a safepoint or using a direct handshake\n+  \/\/ (see EnterInterpOnlyModeClosure).\n+  \/\/ It can be set to zero asynchronously to this threads execution (i.e., without\n+  \/\/ safepoint\/handshake or a lock) so we have to be very careful.\n+  \/\/ Accesses by other threads are synchronized using JvmtiThreadState_lock though.\n+  int               _interp_only_mode;\n+\n+ public:\n+  \/\/ used by the interpreter for fullspeed debugging support (see above)\n+  static ByteSize interp_only_mode_offset() { return byte_offset_of(JavaThread, _interp_only_mode); }\n+  bool is_interp_only_mode()                { return (_interp_only_mode != 0); }\n+  int get_interp_only_mode()                { return _interp_only_mode; }\n+  int set_interp_only_mode(int val)         { return _interp_only_mode = val; }\n+  void increment_interp_only_mode()         { ++_interp_only_mode; }\n+  void decrement_interp_only_mode()         { --_interp_only_mode; }\n+\n+  \/\/ support for cached flag that indicates whether exceptions need to be posted for this thread\n+  \/\/ if this is false, we can avoid deoptimizing when events are thrown\n+  \/\/ this gets set to reflect whether jvmtiExport::post_exception_throw would actually do anything\n+ private:\n+  int    _should_post_on_exceptions_flag;\n+\n+ public:\n+  int   should_post_on_exceptions_flag()  { return _should_post_on_exceptions_flag; }\n+  void  set_should_post_on_exceptions_flag(int val)  { _should_post_on_exceptions_flag = val; }\n+\n+ private:\n+  ThreadStatistics *_thread_stat;\n+\n+ public:\n+  ThreadStatistics* get_thread_stat() const    { return _thread_stat; }\n+\n+  \/\/ Return a blocker object for which this thread is blocked parking.\n+  oop current_park_blocker();\n+\n+ private:\n+  static size_t _stack_size_at_create;\n+\n+ public:\n+  static inline size_t stack_size_at_create(void) {\n+    return _stack_size_at_create;\n+  }\n+  static inline void set_stack_size_at_create(size_t value) {\n+    _stack_size_at_create = value;\n+  }\n+\n+  \/\/ Machine dependent stuff\n+#include OS_CPU_HEADER(javaThread)\n+\n+  \/\/ JSR166 per-thread parker\n+ private:\n+  Parker _parker;\n+ public:\n+  Parker* parker() { return &_parker; }\n+\n+ public:\n+  \/\/ clearing\/querying jni attach status\n+  bool is_attaching_via_jni() const { return _jni_attach_state == _attaching_via_jni; }\n+  bool has_attached_via_jni() const { return is_attaching_via_jni() || _jni_attach_state == _attached_via_jni; }\n+  inline void set_done_attaching_via_jni();\n+\n+  \/\/ Stack dump assistance:\n+  \/\/ Track the class we want to initialize but for which we have to wait\n+  \/\/ on its init_lock() because it is already being initialized.\n+  void set_class_to_be_initialized(InstanceKlass* k);\n+  InstanceKlass* class_to_be_initialized() const;\n+\n+private:\n+  InstanceKlass* _class_to_be_initialized;\n+\n+  \/\/ java.lang.Thread.sleep support\n+  ParkEvent * _SleepEvent;\n+public:\n+  bool sleep(jlong millis);\n+\n+  \/\/ java.lang.Thread interruption support\n+  void interrupt();\n+  bool is_interrupted(bool clear_interrupted);\n+\n+  static OopStorage* thread_oop_storage();\n+\n+  static void verify_cross_modify_fence_failure(JavaThread *thread) PRODUCT_RETURN;\n+\n+  \/\/ Helper function to create the java.lang.Thread object for a\n+  \/\/ VM-internal thread. The thread will have the given name, be\n+  \/\/ part of the System ThreadGroup and if is_visible is true will be\n+  \/\/ discoverable via the system ThreadGroup.\n+  static Handle create_system_thread_object(const char* name, bool is_visible, TRAPS);\n+\n+  \/\/ Helper function to start a VM-internal daemon thread.\n+  \/\/ E.g. ServiceThread, NotificationThread, CompilerThread etc.\n+  static void start_internal_daemon(JavaThread* current, JavaThread* target,\n+                                    Handle thread_oop, ThreadPriority prio);\n+\n+  \/\/ Helper function to do vm_exit_on_initialization for osthread\n+  \/\/ resource allocation failure.\n+  static void vm_exit_on_osthread_failure(JavaThread* thread);\n+\n+  \/\/ AsyncGetCallTrace support\n+  inline bool in_asgct(void) {return _in_asgct;}\n+  inline void set_in_asgct(bool value) {_in_asgct = value;}\n+};\n+\n+inline JavaThread* JavaThread::current_or_null() {\n+  Thread* current = Thread::current_or_null();\n+  return current != nullptr ? JavaThread::cast(current) : nullptr;\n+}\n+\n+class UnlockFlagSaver {\n+  private:\n+    JavaThread* _thread;\n+    bool _do_not_unlock;\n+  public:\n+    UnlockFlagSaver(JavaThread* t) {\n+      _thread = t;\n+      _do_not_unlock = t->do_not_unlock_if_synchronized();\n+      t->set_do_not_unlock_if_synchronized(false);\n+    }\n+    ~UnlockFlagSaver() {\n+      _thread->set_do_not_unlock_if_synchronized(_do_not_unlock);\n+    }\n+};\n+\n+class JNIHandleMark : public StackObj {\n+  JavaThread* _thread;\n+ public:\n+  JNIHandleMark(JavaThread* thread) : _thread(thread) {\n+    thread->push_jni_handle_block();\n+  }\n+  ~JNIHandleMark() { _thread->pop_jni_handle_block(); }\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_JAVATHREAD_HPP\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":1201,"deletions":0,"binary":false,"changes":1201,"status":"added"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -39,1 +40,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -150,1 +150,1 @@\n-    assert(!is_jweak(handle), \"wrong method for detroying jweak\");\n+    assert(!is_jweak(handle), \"wrong method for destroying jweak\");\n","filename":"src\/hotspot\/share\/runtime\/jniHandles.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -131,5 +131,0 @@\n-    enum PerfMemoryMode {\n-      PERF_MODE_RO = 0,\n-      PERF_MODE_RW = 1\n-    };\n-\n@@ -151,2 +146,1 @@\n-    static void attach(const char* user, int vmid, PerfMemoryMode mode,\n-                       char** addrp, size_t* size, TRAPS);\n+    static void attach(int vmid, char** addrp, size_t* size, TRAPS);\n","filename":"src\/hotspot\/share\/runtime\/perfMemory.hpp","additions":2,"deletions":8,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,0 +50,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -53,1 +54,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -741,2 +741,9 @@\n-  \/\/ Allocate array holding parameter types (java.lang.Class instances)\n-  objArrayOop m = oopFactory::new_objArray(vmClasses::Class_klass(), parameter_count, CHECK_(objArrayHandle()));\n+  objArrayOop m;\n+  if (parameter_count == 0) {\n+    \/\/ Avoid allocating an array for the empty case\n+    \/\/ Still need to parse the signature for the return type below\n+    m = Universe::the_empty_class_array();\n+  } else {\n+    \/\/ Allocate array holding parameter types (java.lang.Class instances)\n+    m = oopFactory::new_objArray(vmClasses::Class_klass(), parameter_count, CHECK_(objArrayHandle()));\n+  }\n","filename":"src\/hotspot\/share\/runtime\/reflection.cpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -67,1 +68,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -519,10 +520,1 @@\n-class ParallelSPCleanupThreadClosure : public ThreadClosure {\n-public:\n-  void do_thread(Thread* thread) {\n-    if (thread->is_Java_thread()) {\n-      StackWatermarkSet::start_processing(JavaThread::cast(thread), StackWatermarkKind::gc);\n-    }\n-  }\n-};\n-\n-class ParallelSPCleanupTask : public WorkerTask {\n+class ParallelCleanupTask : public WorkerTask {\n@@ -531,1 +523,0 @@\n-  uint _num_workers;\n@@ -551,1 +542,1 @@\n-  ParallelSPCleanupTask(uint num_workers) :\n+  ParallelCleanupTask() :\n@@ -554,1 +545,0 @@\n-    _num_workers(num_workers),\n@@ -559,18 +549,1 @@\n-    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_LAZY_ROOT_PROCESSING)) {\n-      if (_do_lazy_roots) {\n-        Tracer t(\"lazy partial thread root processing\");\n-        ParallelSPCleanupThreadClosure cl;\n-        Threads::threads_do(&cl);\n-      }\n-    }\n-\n-    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_UPDATE_INLINE_CACHES)) {\n-      Tracer t(\"updating inline caches\");\n-      InlineCacheBuffer::update_inline_caches();\n-    }\n-\n-    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_COMPILATION_POLICY)) {\n-      Tracer t(\"compilation policy safepoint handler\");\n-      CompilationPolicy::do_safepoint_work();\n-    }\n-\n+    \/\/ These tasks are ordered by relative length of time to execute so that potentially longer tasks start first.\n@@ -598,0 +571,19 @@\n+    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_LAZY_ROOT_PROCESSING)) {\n+      if (_do_lazy_roots) {\n+        Tracer t(\"lazy partial thread root processing\");\n+        class LazyRootClosure : public ThreadClosure {\n+        public:\n+          void do_thread(Thread* thread) {\n+            StackWatermarkSet::start_processing(JavaThread::cast(thread), StackWatermarkKind::gc);\n+          }\n+        };\n+        LazyRootClosure cl;\n+        Threads::java_threads_do(&cl);\n+      }\n+    }\n+\n+    if (_subtasks.try_claim_task(SafepointSynchronize::SAFEPOINT_CLEANUP_UPDATE_INLINE_CACHES)) {\n+      Tracer t(\"updating inline caches\");\n+      InlineCacheBuffer::update_inline_caches();\n+    }\n+\n@@ -615,0 +607,1 @@\n+  ParallelCleanupTask cleanup;\n@@ -618,2 +611,0 @@\n-    uint num_cleanup_workers = cleanup_workers->active_workers();\n-    ParallelSPCleanupTask cleanup(num_cleanup_workers);\n@@ -623,1 +614,0 @@\n-    ParallelSPCleanupTask cleanup(1);\n@@ -718,1 +708,1 @@\n-  thread->frame_anchor()->make_walkable(thread);\n+  thread->frame_anchor()->make_walkable();\n@@ -918,1 +908,4 @@\n-  RegisterMap map(self, true, false);\n+  RegisterMap map(self,\n+                  RegisterMap::UpdateMap::include,\n+                  RegisterMap::ProcessFrames::skip,\n+                  RegisterMap::WalkContinuation::skip);\n@@ -988,4 +981,6 @@\n-    \/\/ compiler may not have an exception handler for it. The polling\n-    \/\/ code will notice the pending async exception, deoptimize and\n-    \/\/ the exception will be delivered. (Polling at a return point\n-    \/\/ is ok though). Sure is a lot of bother for a deprecated feature...\n+    \/\/ compiler may not have an exception handler for it (polling at\n+    \/\/ a return point is ok though). We will check for a pending async\n+    \/\/ exception below and deoptimize if needed. We also cannot deoptimize\n+    \/\/ and still install the exception here because live registers needed\n+    \/\/ during deoptimization are clobbered by the exception path. The\n+    \/\/ exception will just be delivered once we get into the interpreter.\n@@ -995,2 +990,0 @@\n-    \/\/ If we have a pending async exception deoptimize the frame\n-    \/\/ as otherwise we may never deliver it.\n@@ -999,0 +992,1 @@\n+      log_info(exceptions)(\"deferred async exception at compiled safepoint\");\n@@ -1001,3 +995,1 @@\n-    \/\/ If an exception has been installed we must check for a pending deoptimization\n-    \/\/ Deoptimize frame if exception has been thrown.\n-\n+    \/\/ If an exception has been installed we must verify that the top frame wasn't deoptimized.\n@@ -1005,1 +997,4 @@\n-      RegisterMap map(self, true, false);\n+      RegisterMap map(self,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::skip,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1008,7 +1003,9 @@\n-        \/\/ The exception patch will destroy registers that are still\n-        \/\/ live and will be needed during deoptimization. Defer the\n-        \/\/ Async exception should have deferred the exception until the\n-        \/\/ next safepoint which will be detected when we get into\n-        \/\/ the interpreter so if we have an exception now things\n-        \/\/ are messed up.\n-\n+        \/\/ The exception path will destroy registers that are still\n+        \/\/ live and will be needed during deoptimization, so if we\n+        \/\/ have an exception now things are messed up. We only check\n+        \/\/ at this scope because for a poll return it is ok to deoptimize\n+        \/\/ while having a pending exception since the call we are returning\n+        \/\/ from already collides with exception handling registers and\n+        \/\/ so there is no issue (the exception handling path kills call\n+        \/\/ result registers but this is ok since the exception kills\n+        \/\/ the result anyway).\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":51,"deletions":54,"binary":false,"changes":105,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n@@ -478,0 +478,4 @@\n+  if (Continuation::is_return_barrier_entry(return_address)) {\n+    return StubRoutines::cont_returnBarrierExc();\n+  }\n+\n@@ -485,1 +489,1 @@\n-    assert(!nm->is_native_method(), \"no exception handler\");\n+    assert(!nm->is_native_method() || nm->method()->is_continuation_enter_intrinsic(), \"no exception handler\");\n@@ -515,2 +519,2 @@\n-  if (blob != NULL && blob->is_optimized_entry_blob()) {\n-    return ((OptimizedEntryBlob*)blob)->exception_handler();\n+  if (blob != NULL && blob->is_upcall_stub()) {\n+    return ((UpcallStub*)blob)->exception_handler();\n@@ -531,0 +535,1 @@\n+    os::print_location(tty, (intptr_t)return_address);\n@@ -556,1 +561,1 @@\n-    \"safepoint polling: type must be poll\");\n+      \"safepoint polling: type must be poll at pc \" INTPTR_FORMAT, p2i(pc));\n@@ -745,1 +750,1 @@\n-    tty->print_cr(\"MISSING EXCEPTION HANDLER for pc \" INTPTR_FORMAT \" and handler bci %d\", p2i(ret_pc), handler_bci);\n+    tty->print_cr(\"MISSING EXCEPTION HANDLER for pc \" INTPTR_FORMAT \" and handler bci %d, catch_pco: %d\", p2i(ret_pc), handler_bci, catch_pco);\n@@ -751,0 +756,1 @@\n+    nm->print();\n@@ -805,0 +811,3 @@\n+  \/\/ Remove the ExtentLocal cache in case we got a StackOverflowError\n+  \/\/ while we were trying to remove ExtentLocal bindings.\n+  current->set_extentLocalCache(NULL);\n@@ -994,5 +1003,3 @@\n-  if (thread != NULL) {\n-    if (thread->is_Java_thread()) {\n-      oop obj = JavaThread::cast(thread)->threadObj();\n-      return (obj == NULL) ? 0 : java_lang_Thread::thread_id(obj);\n-    }\n+  if (thread != NULL && thread->is_Java_thread()) {\n+    oop obj = JavaThread::cast(thread)->threadObj();\n+    return (obj == NULL) ? 0 : java_lang_Thread::thread_id(obj);\n@@ -1097,0 +1104,6 @@\n+  if (caller->is_continuation_enter_intrinsic()) {\n+    bc = Bytecodes::_invokestatic;\n+    LinkResolver::resolve_continuation_enter(callinfo, CHECK_NH);\n+    return receiver;\n+  }\n+\n@@ -1167,1 +1180,4 @@\n-    RegisterMap reg_map2(current);\n+    RegisterMap reg_map2(current,\n+                         RegisterMap::UpdateMap::include,\n+                         RegisterMap::ProcessFrames::include,\n+                         RegisterMap::WalkContinuation::skip);\n@@ -1195,0 +1211,1 @@\n+      assert(oopDesc::is_oop_or_null(receiver()), \"\");\n@@ -1253,1 +1270,4 @@\n-    RegisterMap reg_map(current, false);\n+    RegisterMap reg_map(current,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -1379,0 +1399,3 @@\n+        if (is_nmethod && caller_nm->method()->is_continuation_enter_intrinsic()) {\n+          ssc->compute_entry_for_continuation_entry(callee_method, static_call_info);\n+        }\n@@ -1391,1 +1414,4 @@\n-  RegisterMap cbl_map(current, false);\n+  RegisterMap cbl_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1440,1 +1466,1 @@\n-           callee_method->method_holder()->is_reentrant_initialization(current),\n+           callee_method->method_holder()->is_init_thread(current),\n@@ -1490,1 +1516,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1494,1 +1523,1 @@\n-  assert(!caller_frame.is_interpreted_frame() && !caller_frame.is_entry_frame()  && !caller_frame.is_optimized_entry_frame(), \"unexpected frame\");\n+  assert(!caller_frame.is_interpreted_frame() && !caller_frame.is_entry_frame() && !caller_frame.is_upcall_stub_frame(), \"unexpected frame\");\n@@ -1521,1 +1550,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1528,1 +1560,1 @@\n-      caller_frame.is_optimized_entry_frame()) {\n+      caller_frame.is_upcall_stub_frame()) {\n@@ -1573,1 +1605,4 @@\n-  RegisterMap reg_map(current);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1598,0 +1633,1 @@\n+  bool enter_special = false;\n@@ -1601,0 +1637,12 @@\n+\n+    if (current->is_interp_only_mode()) {\n+      RegisterMap reg_map(current,\n+                          RegisterMap::UpdateMap::skip,\n+                          RegisterMap::ProcessFrames::include,\n+                          RegisterMap::WalkContinuation::skip);\n+      frame stub_frame = current->last_frame();\n+      assert(stub_frame.is_runtime_frame(), \"must be a runtimeStub\");\n+      frame caller = stub_frame.sender(&reg_map);\n+      enter_special = caller.cb() != NULL && caller.cb()->is_compiled()\n+        && caller.cb()->as_compiled_method()->method()->is_continuation_enter_intrinsic();\n+    }\n@@ -1602,0 +1650,12 @@\n+\n+  if (current->is_interp_only_mode() && enter_special) {\n+    \/\/ enterSpecial is compiled and calls this method to resolve the call to Continuation::enter\n+    \/\/ but in interp_only_mode we need to go to the interpreted entry\n+    \/\/ The c2i won't patch in this mode -- see fixup_callers_callsite\n+    \/\/\n+    \/\/ This should probably be done in all cases, not just enterSpecial (see JDK-8218403),\n+    \/\/ but that's part of a larger fix, and the situation is worse for enterSpecial, as it has no\n+    \/\/ interpreted version.\n+    return callee_method->get_c2i_entry();\n+  }\n+\n@@ -1748,1 +1808,4 @@\n-      RegisterMap reg_map(current, false);\n+      RegisterMap reg_map(current,\n+                          RegisterMap::UpdateMap::skip,\n+                          RegisterMap::ProcessFrames::include,\n+                          RegisterMap::WalkContinuation::skip);\n@@ -1774,1 +1837,4 @@\n-    RegisterMap reg_map(current, false);\n+    RegisterMap reg_map(current,\n+                        RegisterMap::UpdateMap::skip,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -1792,1 +1858,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1837,1 +1906,4 @@\n-  RegisterMap reg_map(current, false);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::skip,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -1885,0 +1957,2 @@\n+    \/\/ Check relocations for the matching call to 1) avoid false positives,\n+    \/\/ and 2) determine the type.\n@@ -1886,0 +1960,2 @@\n+      \/\/ On x86 the logic for finding a call instruction is blindly checking for a call opcode 5\n+      \/\/ bytes back in the instruction stream so we must also check for reloc info.\n@@ -1887,1 +1963,1 @@\n-      int ret = iter.next(); \/\/ Get item\n+      bool ret = iter.next(); \/\/ Get item\n@@ -1889,26 +1965,27 @@\n-        assert(iter.addr() == call_addr, \"must find call\");\n-        if (iter.type() == relocInfo::static_call_type) {\n-          is_static_call = true;\n-        } else {\n-          assert(iter.type() == relocInfo::virtual_call_type ||\n-                 iter.type() == relocInfo::opt_virtual_call_type\n-                , \"unexpected relocInfo. type\");\n-          is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n-        }\n-      } else {\n-        assert(!UseInlineCaches, \"relocation info. must exist for this address\");\n-      }\n-\n-      \/\/ Cleaning the inline cache will force a new resolve. This is more robust\n-      \/\/ than directly setting it to the new destination, since resolving of calls\n-      \/\/ is always done through the same code path. (experience shows that it\n-      \/\/ leads to very hard to track down bugs, if an inline cache gets updated\n-      \/\/ to a wrong method). It should not be performance critical, since the\n-      \/\/ resolve is only done once.\n-\n-      for (;;) {\n-        ICRefillVerifier ic_refill_verifier;\n-        if (!clear_ic_at_addr(caller_nm, call_addr, is_static_call)) {\n-          InlineCacheBuffer::refill_ic_stubs();\n-        } else {\n-          break;\n+        is_static_call = false;\n+        is_optimized = false;\n+        switch (iter.type()) {\n+          case relocInfo::static_call_type:\n+            is_static_call = true;\n+\n+          case relocInfo::virtual_call_type:\n+          case relocInfo::opt_virtual_call_type:\n+            is_optimized = (iter.type() == relocInfo::opt_virtual_call_type);\n+            \/\/ Cleaning the inline cache will force a new resolve. This is more robust\n+            \/\/ than directly setting it to the new destination, since resolving of calls\n+            \/\/ is always done through the same code path. (experience shows that it\n+            \/\/ leads to very hard to track down bugs, if an inline cache gets updated\n+            \/\/ to a wrong method). It should not be performance critical, since the\n+            \/\/ resolve is only done once.\n+            guarantee(iter.addr() == call_addr, \"must find call\");\n+            for (;;) {\n+              ICRefillVerifier ic_refill_verifier;\n+              if (!clear_ic_at_addr(caller_nm, call_addr, is_static_call)) {\n+                InlineCacheBuffer::refill_ic_stubs();\n+              } else {\n+                break;\n+              }\n+            }\n+            break;\n+          default:\n+            break;\n@@ -2043,0 +2120,3 @@\n+  assert(!JavaThread::current()->is_interp_only_mode() || !nm->method()->is_continuation_enter_intrinsic()\n+    || ContinuationEntry::is_interpreted_call(return_pc), \"interp_only_mode but not in enterSpecial interpreted entry\");\n+\n@@ -2079,0 +2159,7 @@\n+      if (nm->method()->is_continuation_enter_intrinsic()) {\n+        assert(ContinuationEntry::is_interpreted_call(call->instruction_address()) == JavaThread::current()->is_interp_only_mode(),\n+          \"mode: %d\", JavaThread::current()->is_interp_only_mode());\n+        if (ContinuationEntry::is_interpreted_call(call->instruction_address())) {\n+          return;\n+        }\n+      }\n@@ -2155,1 +2242,1 @@\n-  \/\/ add 3 for parenthesis and preceeding space\n+  \/\/ add 3 for parenthesis and preceding space\n@@ -2184,1 +2271,3 @@\n-    if (ObjectSynchronizer::quick_enter(obj, current, lock)) return;\n+    if (ObjectSynchronizer::quick_enter(obj, current, lock)) {\n+      return;\n+    }\n@@ -2754,7 +2843,10 @@\n-  char blob_id[256];\n-  jio_snprintf(blob_id,\n-                sizeof(blob_id),\n-                \"%s(%s)\",\n-                new_adapter->name(),\n-                entry->fingerprint()->as_string());\n-  Forte::register_stub(blob_id, new_adapter->content_begin(), new_adapter->content_end());\n+  if (Forte::is_enabled() || JvmtiExport::should_post_dynamic_code_generated()) {\n+    char blob_id[256];\n+    jio_snprintf(blob_id,\n+                 sizeof(blob_id),\n+                 \"%s(%s)\",\n+                 new_adapter->name(),\n+                 entry->fingerprint()->as_string());\n+    if (Forte::is_enabled()) {\n+      Forte::register_stub(blob_id, new_adapter->content_begin(), new_adapter->content_end());\n+    }\n@@ -2762,2 +2854,3 @@\n-  if (JvmtiExport::should_post_dynamic_code_generated()) {\n-    JvmtiExport::post_dynamic_code_generated(blob_id, new_adapter->content_begin(), new_adapter->content_end());\n+    if (JvmtiExport::should_post_dynamic_code_generated()) {\n+      JvmtiExport::post_dynamic_code_generated(blob_id, new_adapter->content_begin(), new_adapter->content_end());\n+    }\n@@ -3166,1 +3259,1 @@\n-  \/\/ debugging suppport\n+  \/\/ debugging support\n@@ -3173,1 +3266,1 @@\n-    tty->print_cr(\"c2i argument handler starts at %p\", entry->get_c2i_entry());\n+    tty->print_cr(\"c2i argument handler starts at \" INTPTR_FORMAT, p2i(entry->get_c2i_entry()));\n@@ -3272,1 +3365,1 @@\n-  assert(method->is_method_handle_intrinsic() ||\n+  assert(method->is_special_native_intrinsic() ||\n@@ -3291,0 +3384,5 @@\n+\n+      if (method->is_continuation_enter_intrinsic()) {\n+        buffer.initialize_stubs_size(128);\n+      }\n+\n@@ -3292,0 +3390,1 @@\n+      struct { double data[20]; } stubs_locs_buf;\n@@ -3299,0 +3398,1 @@\n+      buffer.stubs()->initialize_shared_locs((relocInfo*)&stubs_locs_buf, sizeof(stubs_locs_buf) \/ sizeof(relocInfo));\n@@ -3505,0 +3605,9 @@\n+  RegisterMap map(current,\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::include,\n+                  RegisterMap::WalkContinuation::skip);\n+  frame sender = fr.sender(&map);\n+  if (sender.is_interpreted_frame()) {\n+    current->push_cont_fastpath(sender.sp());\n+  }\n+\n@@ -3582,1 +3691,9 @@\n-  while (true) {\n+  RegisterMap map(JavaThread::current(),\n+                  RegisterMap::UpdateMap::skip,\n+                  RegisterMap::ProcessFrames::skip,\n+                  RegisterMap::WalkContinuation::skip); \/\/ don't walk continuations\n+  for (; !fr.is_first_frame(); fr = fr.sender(&map)) {\n+    if (!fr.is_java_frame()) {\n+      continue;\n+    }\n+\n@@ -3616,5 +3733,0 @@\n-    if (fr.is_first_java_frame()) {\n-      break;\n-    } else {\n-      fr = fr.java_sender();\n-    }\n@@ -3699,1 +3811,4 @@\n-  RegisterMap reg_map(current);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n@@ -3730,1 +3845,1 @@\n-    address loc = reg_map.location(pair.first());\n+    address loc = reg_map.location(pair.first(), nullptr);\n@@ -3775,1 +3890,1 @@\n-  address loc = reg_map.location(pair.first());\n+  address loc = reg_map.location(pair.first(), nullptr);\n@@ -3789,1 +3904,4 @@\n-  RegisterMap reg_map(current);\n+  RegisterMap reg_map(current,\n+                      RegisterMap::UpdateMap::include,\n+                      RegisterMap::ProcessFrames::include,\n+                      RegisterMap::WalkContinuation::skip);\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":193,"deletions":75,"binary":false,"changes":268,"status":"modified"},{"patch":"@@ -286,1 +286,1 @@\n-  \/\/ used by native wrappers to reenable yellow if overflow happened in native code\n+  \/\/ used by native wrappers to re-enable yellow if overflow happened in native code\n@@ -542,13 +542,0 @@\n-#ifdef COMPILER2\n-  static RuntimeStub* make_native_invoker(address call_target,\n-                                          int shadow_space_bytes,\n-                                          const GrowableArray<VMReg>& input_registers,\n-                                          const GrowableArray<VMReg>& output_registers);\n-#endif\n-\n-  static void compute_move_order(const BasicType* in_sig_bt,\n-                                 int total_in_args, const VMRegPair* in_regs,\n-                                 int total_out_args, VMRegPair* out_regs,\n-                                 GrowableArray<int>& arg_order,\n-                                 VMRegPair tmp_vmreg);\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":1,"deletions":14,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"asm\/assembler.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -41,0 +43,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -42,0 +45,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -117,0 +121,24 @@\n+#if !defined(_LP64) || defined(ZERO) || defined(ASSERT)\n+static int compute_num_stack_arg_slots(Symbol* signature, int sizeargs, bool is_static) {\n+  ResourceMark rm;\n+  BasicType* sig_bt = NEW_RESOURCE_ARRAY(BasicType, sizeargs);\n+  VMRegPair* regs   = NEW_RESOURCE_ARRAY(VMRegPair, sizeargs);\n+\n+  int sig_index = 0;\n+  if (!is_static) {\n+    sig_bt[sig_index++] = T_OBJECT; \/\/ 'this'\n+  }\n+  for (SignatureStream ss(signature); !ss.at_return_type(); ss.next()) {\n+    BasicType t = ss.type();\n+    assert(type2size[t] == 1 || type2size[t] == 2, \"size is 1 or 2\");\n+    sig_bt[sig_index++] = t;\n+    if (type2size[t] == 2) {\n+      sig_bt[sig_index++] = T_VOID;\n+    }\n+  }\n+  assert(sig_index == sizeargs, \"sig_index: %d sizeargs: %d\", sig_index, sizeargs);\n+\n+  return SharedRuntime::java_calling_convention(sig_bt, regs, sizeargs);\n+}\n+#endif\n+\n@@ -142,0 +170,1 @@\n+  initialize_calling_convention(static_flag);\n@@ -153,0 +182,11 @@\n+#if defined(_LP64) && !defined(ZERO)\n+  _stack_arg_slots = align_up(_stack_arg_slots, 2);\n+#ifdef ASSERT\n+  int dbg_stack_arg_slots = compute_num_stack_arg_slots(_signature, _param_size, static_flag);\n+  assert(_stack_arg_slots == dbg_stack_arg_slots, \"fingerprinter: %d full: %d\", _stack_arg_slots, dbg_stack_arg_slots);\n+#endif\n+#else\n+  \/\/ Fallback: computed _stack_arg_slots is unreliable, compute directly.\n+  _stack_arg_slots = compute_num_stack_arg_slots(_signature, _param_size, static_flag);\n+#endif\n+\n@@ -176,0 +216,70 @@\n+void Fingerprinter::initialize_calling_convention(bool static_flag) {\n+  _int_args = 0;\n+  _fp_args = 0;\n+\n+  if (!static_flag) { \/\/ `this` takes up an int register\n+    _int_args++;\n+  }\n+}\n+\n+void Fingerprinter::do_type_calling_convention(BasicType type) {\n+  \/\/ We compute the number of slots for stack-passed arguments in compiled calls.\n+  \/\/ TODO: SharedRuntime::java_calling_convention is the shared code that knows all details\n+  \/\/ about the platform-specific calling conventions. This method tries to compute the stack\n+  \/\/ args number... poorly, at least for 32-bit ports and for zero. Current code has the fallback\n+  \/\/ that recomputes the stack args number from SharedRuntime::java_calling_convention.\n+#if defined(_LP64) && !defined(ZERO)\n+  switch (type) {\n+  case T_VOID:\n+    break;\n+  case T_BOOLEAN:\n+  case T_CHAR:\n+  case T_BYTE:\n+  case T_SHORT:\n+  case T_INT:\n+#if defined(PPC64) || defined(S390)\n+    if (_int_args < Argument::n_int_register_parameters_j) {\n+      _int_args++;\n+    } else {\n+      _stack_arg_slots += 1;\n+    }\n+    break;\n+#endif \/\/ defined(PPC64) || defined(S390)\n+  case T_LONG:\n+  case T_OBJECT:\n+  case T_ARRAY:\n+  case T_ADDRESS:\n+  case T_PRIMITIVE_OBJECT:\n+    if (_int_args < Argument::n_int_register_parameters_j) {\n+      _int_args++;\n+    } else {\n+      PPC64_ONLY(_stack_arg_slots = align_up(_stack_arg_slots, 2));\n+      S390_ONLY(_stack_arg_slots = align_up(_stack_arg_slots, 2));\n+      _stack_arg_slots += 2;\n+    }\n+    break;\n+  case T_FLOAT:\n+#if defined(PPC64) || defined(S390)\n+    if (_fp_args < Argument::n_float_register_parameters_j) {\n+      _fp_args++;\n+    } else {\n+      _stack_arg_slots += 1;\n+    }\n+    break;\n+#endif \/\/ defined(PPC64) || defined(S390)\n+  case T_DOUBLE:\n+    if (_fp_args < Argument::n_float_register_parameters_j) {\n+      _fp_args++;\n+    } else {\n+      PPC64_ONLY(_stack_arg_slots = align_up(_stack_arg_slots, 2));\n+      S390_ONLY(_stack_arg_slots = align_up(_stack_arg_slots, 2));\n+      _stack_arg_slots += 2;\n+    }\n+    break;\n+  default:\n+    ShouldNotReachHere();\n+    break;\n+  }\n+#endif\n+}\n+\n@@ -392,0 +502,1 @@\n+  ThreadInVMfromUnknown __tiv;\n","filename":"src\/hotspot\/share\/runtime\/signature.cpp","additions":112,"deletions":1,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -334,0 +334,1 @@\n+  int _stack_arg_slots;\n@@ -337,0 +338,3 @@\n+  uint _int_args;\n+  uint _fp_args;\n+\n@@ -341,0 +345,1 @@\n+    _stack_arg_slots = 0;\n@@ -346,0 +351,3 @@\n+  void initialize_calling_convention(bool static_flag);\n+  void do_type_calling_convention(BasicType type);\n+\n@@ -352,0 +360,1 @@\n+    do_type_calling_convention(type);\n@@ -356,0 +365,2 @@\n+  int num_stack_arg_slots() const { return _stack_arg_slots; }\n+\n","filename":"src\/hotspot\/share\/runtime\/signature.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"oops\/access.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -39,1 +41,21 @@\n-StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv) {\n+class RegisterMap;\n+class SmallRegisterMap;\n+\n+\n+template <typename OopT>\n+static oop read_oop_local(OopT* p) {\n+  \/\/ We can't do a native access directly from p because load barriers\n+  \/\/ may self-heal. If that happens on a base pointer for compressed oops,\n+  \/\/ then there will be a crash later on. Only the stack watermark API is\n+  \/\/ allowed to heal oops, because it heals derived pointers before their\n+  \/\/ corresponding base pointers.\n+  oop obj = RawAccess<>::oop_load(p);\n+  return NativeAccess<>::oop_load(&obj);\n+}\n+\n+template StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n+template StackValue* StackValue::create_stack_value(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n+\n+template<typename RegisterMapT>\n+StackValue* StackValue::create_stack_value(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n+  address value_addr = stack_value_address(fr, reg_map, sv);\n@@ -44,9 +66,0 @@\n-    \/\/ First find address of value\n-\n-    address value_addr = loc.is_register()\n-      \/\/ Value was in a callee-save register\n-      ? reg_map->location(VMRegImpl::as_VMReg(loc.register_number()))\n-      \/\/ Else value was directly saved on the stack. The frame's original stack pointer,\n-      \/\/ before any extension by its callee (due to Compiler1 linkage on SPARC), must be used.\n-      : ((address)fr->unextended_sp()) + loc.stack_offset();\n-\n@@ -95,0 +108,1 @@\n+      assert(UseCompressedOops, \"\");\n@@ -111,7 +125,1 @@\n-      oop val = CompressedOops::decode(value.noop);\n-      \/\/ Deoptimization must make sure all oops have passed load barriers\n-#if INCLUDE_SHENANDOAHGC\n-      if (UseShenandoahGC) {\n-        val = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(val);\n-      }\n-#endif\n+      oop val = read_oop_local(&value.noop);\n@@ -123,1 +131,6 @@\n-      oop val = *(oop *)value_addr;\n+      oop val;\n+      if (reg_map->in_cont() && reg_map->stack_chunk()->has_bitmap() && UseCompressedOops) {\n+        val = CompressedOops::decode(*(narrowOop*)value_addr);\n+      } else {\n+        val = *(oop *)value_addr;\n+      }\n@@ -133,7 +146,3 @@\n-      \/\/ Deoptimization must make sure all oops have passed load barriers\n-#if INCLUDE_SHENANDOAHGC\n-      if (UseShenandoahGC) {\n-        val = ShenandoahBarrierSet::barrier_set()->load_reference_barrier(val);\n-      }\n-#endif\n-      assert(oopDesc::is_oop_or_null(val, false), \"bad oop found\");\n+      val = read_oop_local(&val);\n+      assert(oopDesc::is_oop_or_null(val), \"bad oop found at \" INTPTR_FORMAT \" in_cont: %d compressed: %d\",\n+        p2i(value_addr), reg_map->in_cont(), reg_map->in_cont() && reg_map->stack_chunk()->has_bitmap() && UseCompressedOops);\n@@ -208,0 +217,32 @@\n+template address StackValue::stack_value_address(const frame* fr, const RegisterMap* reg_map, ScopeValue* sv);\n+template address StackValue::stack_value_address(const frame* fr, const SmallRegisterMap* reg_map, ScopeValue* sv);\n+\n+template<typename RegisterMapT>\n+address StackValue::stack_value_address(const frame* fr, const RegisterMapT* reg_map, ScopeValue* sv) {\n+  if (!sv->is_location()) {\n+    return NULL;\n+  }\n+  Location loc = ((LocationValue *)sv)->location();\n+  if (loc.type() == Location::invalid) {\n+    return NULL;\n+  }\n+\n+  if (!reg_map->in_cont()) {\n+    address value_addr = loc.is_register()\n+      \/\/ Value was in a callee-save register\n+      ? reg_map->location(VMRegImpl::as_VMReg(loc.register_number()), fr->sp())\n+      \/\/ Else value was directly saved on the stack. The frame's original stack pointer,\n+      \/\/ before any extension by its callee (due to Compiler1 linkage on SPARC), must be used.\n+      : ((address)fr->unextended_sp()) + loc.stack_offset();\n+\n+    assert(value_addr == NULL || reg_map->thread() == NULL || reg_map->thread()->is_in_usable_stack(value_addr), INTPTR_FORMAT, p2i(value_addr));\n+    return value_addr;\n+  }\n+\n+  address value_addr = loc.is_register()\n+    ? reg_map->as_RegisterMap()->stack_chunk()->reg_to_location(*fr, reg_map->as_RegisterMap(), VMRegImpl::as_VMReg(loc.register_number()))\n+    : reg_map->as_RegisterMap()->stack_chunk()->usp_offset_to_location(*fr, loc.stack_offset());\n+\n+  assert(value_addr == NULL || Continuation::is_in_usable_stack(value_addr, reg_map->as_RegisterMap()) || (reg_map->thread() != NULL && reg_map->thread()->is_in_usable_stack(value_addr)), INTPTR_FORMAT, p2i(value_addr));\n+  return value_addr;\n+}\n","filename":"src\/hotspot\/share\/runtime\/stackValue.cpp","additions":67,"deletions":26,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -34,1 +35,1 @@\n-#include \"runtime\/safefetch.inline.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n@@ -54,0 +55,1 @@\n+BufferBlob* StubRoutines::_code3                                = NULL;\n@@ -171,7 +173,0 @@\n-address StubRoutines::_safefetch32_entry                 = NULL;\n-address StubRoutines::_safefetch32_fault_pc              = NULL;\n-address StubRoutines::_safefetch32_continuation_pc       = NULL;\n-address StubRoutines::_safefetchN_entry                  = NULL;\n-address StubRoutines::_safefetchN_fault_pc               = NULL;\n-address StubRoutines::_safefetchN_continuation_pc        = NULL;\n-\n@@ -184,0 +179,9 @@\n+RuntimeStub* StubRoutines::_cont_doYield_stub = NULL;\n+address StubRoutines::_cont_doYield       = NULL;\n+address StubRoutines::_cont_thaw          = NULL;\n+address StubRoutines::_cont_returnBarrier = NULL;\n+address StubRoutines::_cont_returnBarrierExc = NULL;\n+\n+JFR_ONLY(RuntimeStub* StubRoutines::_jfr_write_checkpoint_stub = NULL;)\n+JFR_ONLY(address StubRoutines::_jfr_write_checkpoint = NULL;)\n+\n@@ -190,1 +194,1 @@\n-extern void StubGenerator_generate(CodeBuffer* code, bool all); \/\/ only interface to generators\n+extern void StubGenerator_generate(CodeBuffer* code, int phase); \/\/ only interface to generators\n@@ -229,1 +233,1 @@\n-    StubGenerator_generate(&buffer, false);\n+    StubGenerator_generate(&buffer, 0);\n@@ -236,1 +240,0 @@\n-\n@@ -274,0 +277,16 @@\n+void StubRoutines::initializeContinuationStubs() {\n+  if (_code3 == NULL) {\n+    ResourceMark rm;\n+    TraceTime timer(\"StubRoutines generation 3\", TRACETIME_LOG(Info, startuptime));\n+    _code3 = BufferBlob::create(\"StubRoutines (3)\", code_size2);\n+    if (_code3 == NULL) {\n+      vm_exit_out_of_memory(code_size2, OOM_MALLOC_ERROR, \"CodeCache: no room for StubRoutines (3)\");\n+    }\n+    CodeBuffer buffer(_code3);\n+    StubGenerator_generate(&buffer, 1);\n+    \/\/ When new stubs added we need to make sure there is some space left\n+    \/\/ to catch situation when we should increase size again.\n+    assert(code_size2 == 0 || buffer.insts_remaining() > 200, \"increase code_size3\");\n+  }\n+}\n+\n@@ -286,1 +305,1 @@\n-    StubGenerator_generate(&buffer, true);\n+    StubGenerator_generate(&buffer, 2);\n@@ -377,0 +396,1 @@\n+void stubRoutines_initContinuationStubs() { StubRoutines::initializeContinuationStubs(); }\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":32,"deletions":12,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -156,1 +156,2 @@\n-  static BufferBlob* _code2;                               \/\/ code buffer for all other routines\n+  static BufferBlob* _code2;\n+  static BufferBlob* _code3;                               \/\/ code buffer for all other routines\n@@ -253,7 +254,8 @@\n-  \/\/ Safefetch stubs.\n-  static address _safefetch32_entry;\n-  static address _safefetch32_fault_pc;\n-  static address _safefetch32_continuation_pc;\n-  static address _safefetchN_entry;\n-  static address _safefetchN_fault_pc;\n-  static address _safefetchN_continuation_pc;\n+  static RuntimeStub* _cont_doYield_stub;\n+  static address _cont_doYield;\n+  static address _cont_thaw;\n+  static address _cont_returnBarrier;\n+  static address _cont_returnBarrierExc;\n+\n+  JFR_ONLY(static RuntimeStub* _jfr_write_checkpoint_stub;)\n+  JFR_ONLY(static address _jfr_write_checkpoint;)\n@@ -272,0 +274,1 @@\n+  static void    initializeContinuationStubs();            \/\/ must happen after  universe::genesis\n@@ -283,0 +286,1 @@\n+  static RuntimeBlob* code3() { return _code3; }\n@@ -432,8 +436,5 @@\n-  static address select_fill_function(BasicType t, bool aligned, const char* &name);\n-\n-  \/\/\n-  \/\/ Safefetch stub support\n-  \/\/\n-\n-  typedef int      (*SafeFetch32Stub)(int*      adr, int      errValue);\n-  typedef intptr_t (*SafeFetchNStub) (intptr_t* adr, intptr_t errValue);\n+  static RuntimeStub* cont_doYield_stub() { return _cont_doYield_stub; }\n+  static address cont_doYield()        { return _cont_doYield; }\n+  static address cont_thaw()           { return _cont_thaw; }\n+  static address cont_returnBarrier()  { return _cont_returnBarrier; }\n+  static address cont_returnBarrierExc(){return _cont_returnBarrierExc; }\n@@ -441,2 +442,1 @@\n-  static SafeFetch32Stub SafeFetch32_stub() { return CAST_TO_FN_PTR(SafeFetch32Stub, _safefetch32_entry); }\n-  static SafeFetchNStub  SafeFetchN_stub()  { return CAST_TO_FN_PTR(SafeFetchNStub,  _safefetchN_entry); }\n+  JFR_ONLY(static address jfr_write_checkpoint() { return _jfr_write_checkpoint; })\n@@ -444,17 +444,1 @@\n-  static bool is_safefetch_fault(address pc) {\n-    return pc != NULL &&\n-          (pc == _safefetch32_fault_pc ||\n-           pc == _safefetchN_fault_pc);\n-  }\n-\n-  static address continuation_for_safefetch_fault(address pc) {\n-    assert(_safefetch32_continuation_pc != NULL &&\n-           _safefetchN_continuation_pc  != NULL,\n-           \"not initialized\");\n-\n-    if (pc == _safefetch32_fault_pc) return _safefetch32_continuation_pc;\n-    if (pc == _safefetchN_fault_pc)  return _safefetchN_continuation_pc;\n-\n-    ShouldNotReachHere();\n-    return NULL;\n-  }\n+  static address select_fill_function(BasicType t, bool aligned, const char* &name);\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":20,"deletions":36,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -37,0 +37,1 @@\n+#include \"runtime\/frame.inline.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -51,1 +53,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -241,1 +243,1 @@\n-static os::PlatformMutex* gInflationLocks[NINFLATIONLOCKS];\n+static PlatformMutex* gInflationLocks[NINFLATIONLOCKS];\n@@ -245,1 +247,1 @@\n-    gInflationLocks[i] = new os::PlatformMutex();\n+    gInflationLocks[i] = new PlatformMutex();\n@@ -392,0 +394,1 @@\n+      current->inc_held_monitor_count();\n@@ -408,0 +411,1 @@\n+      current->inc_held_monitor_count();\n@@ -471,1 +475,1 @@\n-#if defined(X86) || defined(AARCH64) || defined(PPC64)\n+#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64)\n@@ -490,0 +494,2 @@\n+  current->inc_held_monitor_count();\n+\n@@ -529,0 +535,2 @@\n+  current->dec_held_monitor_count();\n+\n@@ -603,2 +611,3 @@\n-  intptr_t ret_code = monitor->complete_exit(current);\n-  return ret_code;\n+  intx recur_count = monitor->complete_exit(current);\n+  current->dec_held_monitor_count(recur_count + 1);\n+  return recur_count;\n@@ -618,0 +627,1 @@\n+      current->inc_held_monitor_count(recursions + 1);\n@@ -640,0 +650,1 @@\n+      current->inc_held_monitor_count(1, true);\n@@ -659,0 +670,1 @@\n+    current->dec_held_monitor_count(1, true);\n@@ -1614,1 +1626,2 @@\n-    (void)mid->complete_exit(_thread);\n+    intx rec = mid->complete_exit(_thread);\n+    _thread->dec_held_monitor_count(rec + 1);\n@@ -1640,0 +1653,3 @@\n+  assert(current->held_monitor_count() == 0, \"Should not be possible\");\n+  \/\/ All monitors (including entered via JNI) have been unlocked above, so we need to clear jni count.\n+  current->clear_jni_monitor_count();\n@@ -1694,1 +1710,1 @@\n-    \/\/ level at a safepoint in ObjectSynchronizer::do_safepoint_work().\n+    \/\/ level at a safepoint in SafepointSynchronize::do_cleanup_tasks.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":25,"deletions":9,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,0 +38,1 @@\n+#include \"runtime\/continuation.hpp\"\n@@ -41,0 +42,1 @@\n+#include \"runtime\/registerMap.hpp\"\n@@ -42,0 +44,1 @@\n+#include \"runtime\/stackValue.hpp\"\n@@ -68,7 +71,9 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(thread());\n-  if (list != NULL ) {\n-    \/\/ In real life this never happens or is typically a single element search\n-    for (int i = 0; i < list->length(); i++) {\n-      if (list->at(i)->matches(this)) {\n-        list->at(i)->update_locals(result);\n-        break;\n+  if (!register_map()->in_cont()) { \/\/ LOOM TODO\n+    GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(thread());\n+    if (list != NULL ) {\n+      \/\/ In real life this never happens or is typically a single element search\n+      for (int i = 0; i < list->length(); i++) {\n+        if (list->at(i)->matches(this)) {\n+          list->at(i)->update_locals(result);\n+          break;\n+        }\n@@ -107,0 +112,1 @@\n+  assert(!Continuation::is_frame_in_continuation(thread(), fr()), \"No support for deferred values in continuations\");\n@@ -142,1 +148,1 @@\n-  GrowableArray<ScopeValue*>* scopeLocals = scope()->locals();\n+  GrowableArray<ScopeValue*>* extentLocals = scope()->locals();\n@@ -147,1 +153,1 @@\n-      if (var->type() == T_OBJECT && scopeLocals->at(i2)->is_object()) {\n+      if (var->type() == T_OBJECT && extentLocals->at(i2)->is_object()) {\n@@ -196,9 +202,11 @@\n-  \/\/ Replace the original values with any stores that have been\n-  \/\/ performed through compiledVFrame::update_stack.\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(thread());\n-  if (list != NULL ) {\n-    \/\/ In real life this never happens or is typically a single element search\n-    for (int i = 0; i < list->length(); i++) {\n-      if (list->at(i)->matches(this)) {\n-        list->at(i)->update_stack(result);\n-        break;\n+  if (!register_map()->in_cont()) { \/\/ LOOM TODO\n+    \/\/ Replace the original values with any stores that have been\n+    \/\/ performed through compiledVFrame::update_stack.\n+    GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(thread());\n+    if (list != NULL ) {\n+      \/\/ In real life this never happens or is typically a single element search\n+      for (int i = 0; i < list->length(); i++) {\n+        if (list->at(i)->matches(this)) {\n+          list->at(i)->update_stack(result);\n+          break;\n+        }\n@@ -218,1 +226,9 @@\n-  return StackValue::create_stack_value(&_fr, register_map(), sv);\n+  stackChunkOop c = _reg_map.stack_chunk()();\n+  int index = _reg_map.stack_chunk_index();\n+  const_cast<RegisterMap*>(&_reg_map)->set_stack_chunk(_chunk());\n+\n+  StackValue* res = StackValue::create_stack_value(&_fr, register_map(), sv);\n+\n+  const_cast<RegisterMap*>(&_reg_map)->set_stack_chunk(c);\n+  const_cast<RegisterMap*>(&_reg_map)->set_stack_chunk_index(index);\n+  return res;\n@@ -399,1 +415,1 @@\n-  \/\/ Alway will need at least one, must be on C heap\n+  \/\/ Always will need at least one, must be on C heap\n","filename":"src\/hotspot\/share\/runtime\/vframe_hp.cpp","additions":37,"deletions":21,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,0 +52,1 @@\n+  template(CollectForCodeCacheAllocation)         \\\n@@ -73,0 +74,2 @@\n+  template(VirtualThreadGetStackTrace)            \\\n+  template(VirtualThreadGetFrameCount)            \\\n@@ -75,0 +78,2 @@\n+  template(VirtualThreadGetOrSetLocal)            \\\n+  template(VirtualThreadGetCurrentLocation)       \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -44,0 +44,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -47,1 +48,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -351,1 +352,1 @@\n-  snapshot->dump_stack_at_safepoint(_max_depth, _with_locked_monitors, table);\n+  snapshot->dump_stack_at_safepoint(_max_depth, _with_locked_monitors, table, false);\n@@ -369,1 +370,1 @@\n-    if (thr!=thr_cur && thr->thread_state() == _thread_in_native) {\n+    if (thr != thr_cur && thr->thread_state() == _thread_in_native) {\n@@ -497,1 +498,1 @@\n-    \/\/ we will block here until the process dies\n+    \/\/ so we will block here until the process dies.\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"runtime\/thread.hpp\"\n@@ -39,1 +39,1 @@\n-    \/\/ Neither the doit function nor the the safepoint\n+    \/\/ Neither the doit function nor the safepoint\n@@ -118,2 +118,0 @@\n- private:\n-  Klass* _dependee;\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+#include \"oops\/instanceStackChunkKlass.hpp\"\n@@ -95,0 +96,1 @@\n+#include \"runtime\/javaThread.hpp\"\n@@ -105,1 +107,0 @@\n-#include \"runtime\/thread.inline.hpp\"\n@@ -242,1 +243,1 @@\n-  nonstatic_field(InstanceKlass,               _init_state,                                   u1)                                    \\\n+  nonstatic_field(InstanceKlass,               _init_state,                                   InstanceKlass::ClassState)             \\\n@@ -322,0 +323,1 @@\n+  nonstatic_field(ConstMethod,                 _num_stack_arg_slots,                          u2)                                    \\\n@@ -460,0 +462,1 @@\n+     static_field(vmClasses,                   VM_CLASS_AT(Thread_FieldHolder_klass),            InstanceKlass*)                     \\\n@@ -714,0 +717,3 @@\n+  nonstatic_field(JavaThread,                  _vthread,                                      OopHandle)                             \\\n+  nonstatic_field(JavaThread,                  _jvmti_vthread,                                OopHandle)                             \\\n+  nonstatic_field(JavaThread,                  _extentLocalCache,                              OopHandle)                             \\\n@@ -721,1 +727,0 @@\n-  nonstatic_field(JavaThread,                  _pending_async_exception,                      oop)                                   \\\n@@ -1237,0 +1242,1 @@\n+        declare_type(InstanceStackChunkKlass, InstanceKlass)              \\\n@@ -1501,1 +1507,0 @@\n-  declare_c2_type(CallNativeNode, CallNode)                               \\\n@@ -1565,1 +1570,0 @@\n-  declare_c2_type(SetVectMaskINode, Node)                                 \\\n@@ -1588,0 +1592,2 @@\n+  declare_c2_type(UDivINode, Node)                                        \\\n+  declare_c2_type(UDivLNode, Node)                                        \\\n@@ -1592,0 +1598,2 @@\n+  declare_c2_type(UModINode, Node)                                        \\\n+  declare_c2_type(UModLNode, Node)                                        \\\n@@ -1595,0 +1603,2 @@\n+  declare_c2_type(UDivModINode, DivModNode)                               \\\n+  declare_c2_type(UDivModLNode, DivModNode)                               \\\n@@ -1621,1 +1631,0 @@\n-  declare_c2_type(MachCallNativeNode, MachCallNode)                       \\\n@@ -1707,0 +1716,1 @@\n+  declare_c2_type(CmpU3Node, CmpUNode)                                    \\\n@@ -1712,0 +1722,1 @@\n+  declare_c2_type(CmpUL3Node, CmpULNode)                                  \\\n@@ -1765,3 +1776,5 @@\n-  declare_c2_type(NegVINode, VectorNode)                                  \\\n-  declare_c2_type(NegVFNode, VectorNode)                                  \\\n-  declare_c2_type(NegVDNode, VectorNode)                                  \\\n+  declare_c2_type(NegVNode, VectorNode)                                   \\\n+  declare_c2_type(NegVINode, NegVNode)                                    \\\n+  declare_c2_type(NegVLNode, NegVNode)                                    \\\n+  declare_c2_type(NegVFNode, NegVNode)                                    \\\n+  declare_c2_type(NegVDNode, NegVNode)                                    \\\n@@ -1772,0 +1785,3 @@\n+  declare_c2_type(CompressVNode, VectorNode)                              \\\n+  declare_c2_type(CompressMNode, VectorNode)                              \\\n+  declare_c2_type(ExpandVNode, VectorNode)                                \\\n@@ -1807,0 +1823,1 @@\n+  declare_c2_type(PopulateIndexNode, VectorNode)                          \\\n@@ -1840,0 +1857,2 @@\n+  declare_c2_type(IsInfiniteFNode, Node)                                  \\\n+  declare_c2_type(IsInfiniteDNode, Node)                                  \\\n@@ -1862,0 +1881,4 @@\n+  declare_c2_type(CountLeadingZerosVNode, VectorNode)                     \\\n+  declare_c2_type(CountTrailingZerosVNode, VectorNode)                    \\\n+  declare_c2_type(ReverseBytesVNode, VectorNode)                          \\\n+  declare_c2_type(ReverseVNode, VectorNode)                               \\\n@@ -2001,1 +2024,1 @@\n-  declare_type(OopMapValue, StackObj)                                     \\\n+  declare_toplevel_type(OopMapValue)                                      \\\n@@ -2127,6 +2150,0 @@\n-  \/*****************************\/                                         \\\n-  \/* Thread::SuspendFlags enum *\/                                         \\\n-  \/*****************************\/                                         \\\n-                                                                          \\\n-  declare_constant(JavaThread::_has_async_exception)                      \\\n-                                                                          \\\n@@ -2190,0 +2207,1 @@\n+  declare_constant(Method::_changes_current_thread)                       \\\n@@ -2272,0 +2290,1 @@\n+  declare_constant(InstanceKlass::being_linked)                           \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":35,"deletions":16,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -120,0 +120,1 @@\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<ThreadDumpToFileDCmd>(full_export, true, false));\n@@ -361,1 +362,1 @@\n-  Symbol* signature = vmSymbols::serializePropertiesToByteArray_signature();\n+  Symbol* signature = vmSymbols::void_byte_array_signature();\n@@ -698,1 +699,1 @@\n-    \/\/ agruments explicitly set by user. All arguments passed\n+    \/\/ arguments explicitly set by user. All arguments passed\n@@ -1113,0 +1114,74 @@\n+\n+ThreadDumpToFileDCmd::ThreadDumpToFileDCmd(outputStream* output, bool heap) :\n+                                           DCmdWithParser(output, heap),\n+  _overwrite(\"-overwrite\", \"May overwrite existing file\", \"BOOLEAN\", false, \"false\"),\n+  _format(\"-format\", \"Output format (\\\"plain\\\" or \\\"json\\\")\", \"STRING\", false, \"plain\"),\n+  _filepath(\"filepath\", \"The file path to the output file\", \"STRING\", true) {\n+  _dcmdparser.add_dcmd_option(&_overwrite);\n+  _dcmdparser.add_dcmd_option(&_format);\n+  _dcmdparser.add_dcmd_argument(&_filepath);\n+}\n+\n+int ThreadDumpToFileDCmd::num_arguments() {\n+  ResourceMark rm;\n+  ThreadDumpToFileDCmd* dcmd = new ThreadDumpToFileDCmd(NULL, false);\n+  if (dcmd != NULL) {\n+    DCmdMark mark(dcmd);\n+    return dcmd->_dcmdparser.num_arguments();\n+  } else {\n+    return 0;\n+  }\n+}\n+\n+void ThreadDumpToFileDCmd::execute(DCmdSource source, TRAPS) {\n+  bool json = (_format.value() != NULL) && (strcmp(_format.value(), \"json\") == 0);\n+  char* path = _filepath.value();\n+  bool overwrite = _overwrite.value();\n+  Symbol* name = (json) ? vmSymbols::dumpThreadsToJson_name() : vmSymbols::dumpThreads_name();\n+  dumpToFile(name, vmSymbols::string_bool_byte_array_signature(), path, overwrite, CHECK);\n+}\n+\n+void ThreadDumpToFileDCmd::dumpToFile(Symbol* name, Symbol* signature, const char* path, bool overwrite, TRAPS) {\n+  ResourceMark rm(THREAD);\n+  HandleMark hm(THREAD);\n+\n+  Handle h_path = java_lang_String::create_from_str(path, CHECK);\n+\n+  Symbol* sym = vmSymbols::jdk_internal_vm_ThreadDumper();\n+  Klass* k = SystemDictionary::resolve_or_fail(sym, true, CHECK);\n+  InstanceKlass* ik = InstanceKlass::cast(k);\n+  if (HAS_PENDING_EXCEPTION) {\n+    java_lang_Throwable::print(PENDING_EXCEPTION, output());\n+    output()->cr();\n+    CLEAR_PENDING_EXCEPTION;\n+    return;\n+  }\n+\n+  \/\/ invoke the ThreadDump method to dump to file\n+  JavaValue result(T_OBJECT);\n+  JavaCallArguments args;\n+  args.push_oop(h_path);\n+  args.push_int(overwrite ? JNI_TRUE : JNI_FALSE);\n+  JavaCalls::call_static(&result,\n+                         k,\n+                         name,\n+                         signature,\n+                         &args,\n+                         THREAD);\n+  if (HAS_PENDING_EXCEPTION) {\n+    java_lang_Throwable::print(PENDING_EXCEPTION, output());\n+    output()->cr();\n+    CLEAR_PENDING_EXCEPTION;\n+    return;\n+  }\n+\n+  \/\/ check that result is byte array\n+  oop res = cast_to_oop(result.get_jobject());\n+  assert(res->is_typeArray(), \"just checking\");\n+  assert(TypeArrayKlass::cast(res->klass())->element_type() == T_BYTE, \"just checking\");\n+\n+  \/\/ copy the bytes to the output stream\n+  typeArrayOop ba = typeArrayOop(res);\n+  jbyte* addr = typeArrayOop(res)->byte_at_addr(0);\n+  output()->print_raw((const char*)addr, ba->length());\n+}\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":78,"deletions":3,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -952,0 +952,26 @@\n+class ThreadDumpToFileDCmd : public DCmdWithParser {\n+private:\n+  void dumpToFile(Symbol* name, Symbol* signature, const char* path, bool overwrite, TRAPS);\n+protected:\n+  DCmdArgument<bool> _overwrite;\n+  DCmdArgument<char*> _format;\n+  DCmdArgument<char*> _filepath;\n+public:\n+  ThreadDumpToFileDCmd(outputStream *output, bool heap);\n+  static const char *name() {\n+    return \"Thread.dump_to_file\";\n+  }\n+  static const char *description() {\n+    return \"Dump threads, with stack traces, to a file in plain text or JSON format.\";\n+  }\n+  static const char* impact() {\n+    return \"Medium: Depends on the number of threads.\";\n+  }\n+  static const JavaPermission permission() {\n+    JavaPermission p = {\"java.lang.management.ManagementPermission\", \"monitor\", NULL};\n+    return p;\n+  }\n+  static int num_arguments();\n+  virtual void execute(DCmdSource source, TRAPS);\n+};\n+\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.hpp","additions":27,"deletions":1,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"runtime\/javaThread.inline.hpp\"\n@@ -54,1 +55,1 @@\n-#include \"runtime\/thread.inline.hpp\"\n+#include \"runtime\/threads.hpp\"\n@@ -676,1 +677,1 @@\n-    assert(Bytes::get_Java_u4((address)(buffer() + 5)) == len, \"Inconsitent size!\");\n+    assert(Bytes::get_Java_u4((address)(buffer() + 5)) == len, \"Inconsistent size!\");\n@@ -681,1 +682,1 @@\n-    \/\/ Finish the current segement and try again.\n+    \/\/ Finish the current segment and try again.\n@@ -764,1 +765,1 @@\n-    assert(_tail->_next == NULL, \"Bufer queue is polluted\");\n+    assert(_tail->_next == NULL, \"Buffer queue is polluted\");\n@@ -1256,8 +1257,0 @@\n-  \/\/ Also provide a pointer to the init_lock if present, so there aren't unreferenced int[0]\n-  \/\/ arrays.\n-  oop init_lock = ik->init_lock();\n-  if (init_lock != NULL) {\n-    field_count++;\n-    size += sizeof(address);\n-  }\n-\n@@ -1303,8 +1296,0 @@\n-\n-  \/\/ Add init lock to the end if the class is not yet initialized\n-  oop init_lock = ik->init_lock();\n-  if (init_lock != NULL) {\n-    writer->write_symbolID(vmSymbols::init_lock_name());         \/\/ name\n-    writer->write_u1(sig2tag(vmSymbols::int_array_signature())); \/\/ type\n-    writer->write_objectID(init_lock);\n-  }\n@@ -2543,1 +2528,4 @@\n-    RegisterMap reg_map(java_thread);\n+    RegisterMap reg_map(java_thread,\n+                        RegisterMap::UpdateMap::include,\n+                        RegisterMap::ProcessFrames::include,\n+                        RegisterMap::WalkContinuation::skip);\n@@ -2849,1 +2837,1 @@\n-      stack_trace->dump_stack_at_safepoint(-1, \/* ObjectMonitorsHashtable is not needed here *\/ nullptr);\n+      stack_trace->dump_stack_at_safepoint(-1, \/* ObjectMonitorsHashtable is not needed here *\/ nullptr, true);\n","filename":"src\/hotspot\/share\/services\/heapDumper.cpp","additions":10,"deletions":22,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -176,2 +176,0 @@\n-  static void throw_unsafe_access_internal_error(JavaThread* thread, const char* file, int line, const char* message);\n-\n","filename":"src\/hotspot\/share\/utilities\/exceptions.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -332,1 +332,1 @@\n-  assert(allow_address || t != T_ADDRESS, \" \");\n+  assert((allow_address || t != T_ADDRESS) && t <= T_CONFLICT, \"unexpected basic type\");\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -159,0 +159,29 @@\n+\n+\/\/----------------------------------------------------------------------------------------------------\n+\/\/ Forbid the use of various C library functions.\n+\/\/ Some of these have os:: replacements that should normally be used instead.\n+\/\/ Others are considered security concerns, with preferred alternatives.\n+\n+FORBID_C_FUNCTION(void exit(int), \"use os::exit\");\n+FORBID_C_FUNCTION(void _exit(int), \"use os::exit\");\n+FORBID_C_FUNCTION(char* strerror(int), \"use os::strerror\");\n+FORBID_C_FUNCTION(char* strtok(char*, const char*), \"use strtok_r\");\n+FORBID_C_FUNCTION(int vsprintf(char*, const char*, va_list), \"use os::vsnprintf\");\n+FORBID_C_FUNCTION(int vsnprintf(char*, size_t, const char*, va_list), \"use os::vsnprintf\");\n+\n+\/\/ All of the following functions return raw C-heap pointers (sometimes as an option, e.g. realpath or getwd)\n+\/\/ or, in case of free(), take raw C-heap pointers. Don't use them unless you are really sure you must.\n+FORBID_C_FUNCTION(void* malloc(size_t size), \"use os::malloc\");\n+FORBID_C_FUNCTION(void* calloc(size_t nmemb, size_t size), \"use os::malloc and zero out manually\");\n+FORBID_C_FUNCTION(void free(void *ptr), \"use os::free\");\n+FORBID_C_FUNCTION(void* realloc(void *ptr, size_t size), \"use os::realloc\");\n+FORBID_C_FUNCTION(char* strdup(const char *s), \"use os::strdup\");\n+FORBID_C_FUNCTION(char* strndup(const char *s, size_t n), \"don't use\");\n+FORBID_C_FUNCTION(int posix_memalign(void **memptr, size_t alignment, size_t size), \"don't use\");\n+FORBID_C_FUNCTION(void* aligned_alloc(size_t alignment, size_t size), \"don't use\");\n+FORBID_C_FUNCTION(char* realpath(const char* path, char* resolved_path), \"use os::Posix::realpath\");\n+FORBID_C_FUNCTION(char* get_current_dir_name(void), \"use os::get_current_directory()\");\n+FORBID_C_FUNCTION(char* getwd(char *buf), \"use os::get_current_directory()\");\n+FORBID_C_FUNCTION(wchar_t* wcsdup(const wchar_t *s), \"don't use\");\n+FORBID_C_FUNCTION(void* reallocf(void *ptr, size_t size), \"don't use\");\n+\n@@ -281,1 +310,1 @@\n-\/\/ The caller is responsible for considering overlow.\n+\/\/ The caller is responsible for considering overflow.\n@@ -475,1 +504,1 @@\n-\/\/ Unsigned one, two, four and eigth byte quantities used for describing\n+\/\/ Unsigned one, two, four and eight byte quantities used for describing\n@@ -723,0 +752,4 @@\n+inline bool is_unsigned_subword_type(BasicType t) {\n+  return (t == T_BOOLEAN || t == T_CHAR);\n+}\n+\n@@ -727,2 +760,2 @@\n-inline bool is_reference_type(BasicType t) {\n-  return (t == T_OBJECT || t == T_ARRAY || t == T_PRIMITIVE_OBJECT);\n+inline bool is_reference_type(BasicType t, bool include_narrow_oop = false) {\n+  return (t == T_OBJECT || t == T_ARRAY || t == T_PRIMITIVE_OBJECT || (include_narrow_oop && t == T_NARROWOOP));\n@@ -735,0 +768,4 @@\n+inline bool is_non_subword_integral_type(BasicType t) {\n+  return t == T_INT || t == T_LONG;\n+}\n+\n@@ -742,2 +779,1 @@\n-extern const char* type2name_tab[T_CONFLICT+1];     \/\/ Map a BasicType to a jchar\n-inline const char* type2name(BasicType t) { return (uint)t < T_CONFLICT+1 ? type2name_tab[t] : NULL; }\n+extern const char* type2name_tab[T_CONFLICT+1];     \/\/ Map a BasicType to a char*\n@@ -746,0 +782,5 @@\n+inline const char* type2name(BasicType t) {\n+  assert((uint)t < T_CONFLICT + 1, \"invalid type\");\n+  return type2name_tab[t];\n+}\n+\n@@ -830,0 +871,3 @@\n+inline bool same_type_or_subword_size(BasicType t1, BasicType t2) {\n+  return (t1 == t2) || (is_subword_type(t1) && type2aelembytes(t1) == type2aelembytes(t2));\n+}\n@@ -971,1 +1015,1 @@\n-  _thread_new_trans         =  3, \/\/ corresponding transition state (not used, included for completness)\n+  _thread_new_trans         =  3, \/\/ corresponding transition state (not used, included for completeness)\n@@ -977,1 +1021,1 @@\n-  _thread_in_Java_trans     =  9, \/\/ corresponding transition state (not used, included for completness)\n+  _thread_in_Java_trans     =  9, \/\/ corresponding transition state (not used, included for completeness)\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":53,"deletions":9,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"jvm_io.h\"\n@@ -30,0 +31,1 @@\n+#include <ctype.h>\n@@ -286,0 +288,53 @@\n+\n+const char* StringUtils::strstr_nocase(const char* haystack, const char* needle) {\n+  if (needle[0] == '\\0') {\n+    return haystack; \/\/ empty needle matches with anything\n+  }\n+  for (size_t i = 0; haystack[i] != '\\0'; i++) {\n+    bool matches = true;\n+    for (size_t j = 0; needle[j] != '\\0'; j++) {\n+      if (haystack[i + j] == '\\0') {\n+        return nullptr; \/\/ hit end of haystack, abort\n+      }\n+      if (tolower(haystack[i + j]) != tolower(needle[j])) {\n+        matches = false;\n+        break; \/\/ abort, try next i\n+      }\n+    }\n+    if (matches) {\n+      return &haystack[i]; \/\/ all j were ok for this i\n+    }\n+  }\n+  return nullptr; \/\/ no i was a match\n+}\n+\n+bool StringUtils::is_star_match(const char* star_pattern, const char* str) {\n+  const int N = 1000;\n+  char pattern[N]; \/\/ copy pattern into this to ensure null termination\n+  jio_snprintf(pattern, N, \"%s\", star_pattern);\/\/ ensures null termination\n+  char buf[N]; \/\/ copy parts of pattern into this\n+  const char* str_idx = str;\n+  const char* pattern_idx = pattern;\n+  while (strlen(pattern_idx) > 0) {\n+    \/\/ find next section in pattern\n+    const char* pattern_part_end = strstr(pattern_idx, \"*\");\n+    const char* pattern_part = pattern_idx;\n+    if (pattern_part_end != nullptr) { \/\/ copy part into buffer\n+      size_t pattern_part_len = pattern_part_end-pattern_part;\n+      strncpy(buf, pattern_part, pattern_part_len);\n+      buf[pattern_part_len] = '\\0'; \/\/ end of string\n+      pattern_part = buf;\n+    }\n+    \/\/ find this section in s, case insensitive\n+    const char* str_match = strstr_nocase(str_idx, pattern_part);\n+    if (str_match == nullptr) {\n+      return false; \/\/ r_part did not match - abort\n+    }\n+    size_t match_len = strlen(pattern_part);\n+    \/\/ advance to match position plus part length\n+    str_idx = str_match + match_len;\n+    \/\/ advance by part length and \"*\"\n+    pattern_idx += match_len + (pattern_part_end == nullptr ? 0 : 1);\n+  }\n+  return true; \/\/ all parts of pattern matched\n+}\n","filename":"src\/hotspot\/share\/utilities\/stringUtils.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -47,0 +47,9 @@\n+\n+  \/\/ Find needle in haystack, case insensitive.\n+  \/\/ Custom implementation of strcasestr, as it is not available on windows.\n+  static const char* strstr_nocase(const char* haystack, const char* needle);\n+\n+  \/\/ Check if str matches the star_pattern.\n+  \/\/ eg. str \"_abc____def__\" would match pattern \"abc*def\".\n+  \/\/ The matching is case insensitive.\n+  static bool is_star_match(const char* star_pattern, const char* str);\n","filename":"src\/hotspot\/share\/utilities\/stringUtils.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -692,0 +692,1 @@\n+    @Override\n@@ -703,0 +704,1 @@\n+    @Override\n@@ -713,1 +715,1 @@\n-     * @throws  IOException If an I\/O error has occurred.\n+     * @throws  IOException {@inheritDoc}\n@@ -715,0 +717,1 @@\n+    @Override\n@@ -727,1 +730,1 @@\n-     * @throws  IOException If an I\/O error has occurred.\n+     * @throws  IOException {@inheritDoc}\n@@ -729,0 +732,1 @@\n+    @Override\n@@ -750,0 +754,1 @@\n+    @Override\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectOutputStream.java","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -84,1 +84,1 @@\n-public class ObjectStreamClass implements Serializable {\n+public final class ObjectStreamClass implements Serializable {\n@@ -1654,1 +1654,1 @@\n-        Set<String> fieldNames = new HashSet<>(serialPersistentFields.length);\n+        Set<String> fieldNames = HashSet.newHashSet(serialPersistentFields.length);\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectStreamClass.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -382,0 +382,5 @@\n+     * <p>\n+     * In cases where this method is called from a context where there is no\n+     * caller frame on the stack (e.g. when called directly from a JNI\n+     * attached thread), the system class loader is used.\n+     *\n@@ -405,1 +410,3 @@\n-        return forName0(className, true, ClassLoader.getClassLoader(caller), caller);\n+        ClassLoader loader = (caller == null) ? ClassLoader.getSystemClassLoader()\n+                                              : ClassLoader.getClassLoader(caller);\n+        return forName0(className, true, loader, caller);\n@@ -1474,16 +1481,1 @@\n-    \/**\n-     * Gets the signers of this class.\n-     *\n-     * @return  the signers of this class, or null if there are no signers.  In\n-     *          particular, this method returns null if this {@code Class} object represents\n-     *          a primitive type or void.\n-     * @since   1.1\n-     *\/\n-    public native Object[] getSigners();\n-\n-    \/**\n-     * Set the signers of this class.\n-     *\/\n-    native void setSigners(Object[] signers);\n-\n-    \/**\n+   \/**\n@@ -1492,0 +1484,13 @@\n+     *\n+     * <p> If the underlying class is an array class, then its\n+     * {@code PUBLIC}, {@code PRIVATE} and {@code PROTECTED}\n+     * access flags are the same as those of its component type.  If this\n+     * {@code Class} object represents a primitive type or void, the\n+     * {@code PUBLIC} access flag is present, and the\n+     * {@code PROTECTED} and {@code PRIVATE} access flags are always\n+     * absent. If this {@code Class} object represents an array class, a\n+     * primitive type or void, then the {@code FINAL} access flag is always\n+     * present and the interface access flag is always\n+     * absent. The values of its other access flags are not determined\n+     * by this specification.\n+     *\n@@ -1504,1 +1509,2 @@\n-                                            (isMemberClass() || isLocalClass() || isAnonymousClass()) ?\n+                                            (isMemberClass() || isLocalClass() ||\n+                                             isAnonymousClass() || isArray()) ?\n@@ -1509,0 +1515,15 @@\n+   \/**\n+     * Gets the signers of this class.\n+     *\n+     * @return  the signers of this class, or null if there are no signers.  In\n+     *          particular, this method returns null if this {@code Class} object represents\n+     *          a primitive type or void.\n+     * @since   1.1\n+     *\/\n+    public native Object[] getSigners();\n+\n+    \/**\n+     * Set the signers of this class.\n+     *\/\n+    native void setSigners(Object[] signers);\n+\n@@ -3183,3 +3204,3 @@\n-                if (callerModule == null && !thisModule.isOpen(pn)) {\n-                    \/\/ no caller, package not open\n-                    return false;\n+                if (callerModule == null) {\n+                    \/\/ no caller, return true if the package is open to all modules\n+                    return thisModule.isOpen(pn);\n@@ -3306,1 +3327,1 @@\n-            ReflectUtil.checkProxyPackageAccess(ccl, this.getInterfaces());\n+            ReflectUtil.checkProxyPackageAccess(ccl, this.getInterfaces(\/* cloneArray *\/ false));\n@@ -3554,1 +3575,1 @@\n-        for (Class<?> si : getInterfaces()) {\n+        for (Class<?> si : getInterfaces(\/* cloneArray *\/ false)) {\n@@ -4091,1 +4112,1 @@\n-            directory = new HashMap<>((int)(universe.length \/ 0.75f) + 1);\n+            directory = HashMap.newHashMap(universe.length);\n@@ -4311,1 +4332,1 @@\n-                        annotations = new LinkedHashMap<>((Math.max(\n+                        annotations = LinkedHashMap.newLinkedHashMap(Math.max(\n@@ -4314,1 +4335,1 @@\n-                            ) * 4 + 2) \/ 3\n+                            )\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Class.java","additions":47,"deletions":26,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.misc.Blocker;\n@@ -85,1 +86,1 @@\n-     *     equals(Object) equals} method, then calling the {@code\n+     *     #equals(Object) equals} method, then calling the {@code\n@@ -89,1 +90,1 @@\n-     *     according to the {@link equals(Object) equals} method, then\n+     *     according to the {@link #equals(Object) equals} method, then\n@@ -154,1 +155,1 @@\n-     * It is generally necessary to override the {@link hashCode hashCode}\n+     * It is generally necessary to override the {@link #hashCode() hashCode}\n@@ -372,1 +373,16 @@\n-    public final native void wait(long timeoutMillis) throws InterruptedException;\n+    public final void wait(long timeoutMillis) throws InterruptedException {\n+        long comp = Blocker.begin();\n+        try {\n+            wait0(timeoutMillis);\n+        } catch (InterruptedException e) {\n+            Thread thread = Thread.currentThread();\n+            if (thread.isVirtual())\n+                thread.getAndClearInterrupt();\n+            throw e;\n+        } finally {\n+            Blocker.end(comp);\n+        }\n+    }\n+\n+    \/\/ final modifier so method not in vtable\n+    private final native void wait0(long timeoutMillis) throws InterruptedException;\n@@ -426,4 +442,4 @@\n-     * \"Condition Queues,\" in Brian Goetz and others' <em>Java Concurrency\n-     * in Practice<\/em> (Addison-Wesley, 2006) or Item 69 in Joshua\n-     * Bloch's <em>Effective Java, Second Edition<\/em> (Addison-Wesley,\n-     * 2008).\n+     * \"Condition Queues,\" in Brian Goetz and others' <cite>Java Concurrency\n+     * in Practice<\/cite> (Addison-Wesley, 2006) or Item 81 in Joshua\n+     * Bloch's <cite>Effective Java, Third Edition<\/cite> (Addison-Wesley,\n+     * 2018).\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Object.java","additions":24,"deletions":8,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2017, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -50,1 +50,1 @@\n-abstract class BoundMethodHandle extends MethodHandle {\n+abstract non-sealed class BoundMethodHandle extends MethodHandle {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/BoundMethodHandle.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,1 +33,0 @@\n-import sun.invoke.util.VerifyType;\n@@ -52,1 +51,1 @@\n-class DirectMethodHandle extends MethodHandle {\n+sealed class DirectMethodHandle extends MethodHandle {\n@@ -400,1 +399,1 @@\n-    static class Special extends DirectMethodHandle {\n+    static final class Special extends DirectMethodHandle {\n@@ -421,3 +420,8 @@\n-                String msg = String.format(\"Receiver class %s is not a subclass of caller class %s\",\n-                                           recv.getClass().getName(), caller.getName());\n-                throw new IncompatibleClassChangeError(msg);\n+                if (recv != null) {\n+                    String msg = String.format(\"Receiver class %s is not a subclass of caller class %s\",\n+                                               recv.getClass().getName(), caller.getName());\n+                    throw new IncompatibleClassChangeError(msg);\n+                } else {\n+                    String msg = String.format(\"Cannot invoke %s with null receiver\", member);\n+                    throw new NullPointerException(msg);\n+                }\n@@ -430,1 +434,1 @@\n-    static class Interface extends DirectMethodHandle {\n+    static final class Interface extends DirectMethodHandle {\n@@ -449,3 +453,8 @@\n-                String msg = String.format(\"Receiver class %s does not implement the requested interface %s\",\n-                                           recv.getClass().getName(), refc.getName());\n-                throw new IncompatibleClassChangeError(msg);\n+                if (recv != null) {\n+                    String msg = String.format(\"Receiver class %s does not implement the requested interface %s\",\n+                                               recv.getClass().getName(), refc.getName());\n+                    throw new IncompatibleClassChangeError(msg);\n+                } else {\n+                    String msg = String.format(\"Cannot invoke %s with null receiver\", member);\n+                    throw new NullPointerException(msg);\n+                }\n@@ -463,1 +472,1 @@\n-    static class Constructor extends DirectMethodHandle {\n+    static final class Constructor extends DirectMethodHandle {\n@@ -498,1 +507,1 @@\n-    static class Accessor extends DirectMethodHandle {\n+    static final class Accessor extends DirectMethodHandle {\n@@ -543,1 +552,2 @@\n-    static class StaticAccessor extends DirectMethodHandle {\n+    \/** This subclass handles static field references. *\/\n+    static final class StaticAccessor extends DirectMethodHandle {\n@@ -635,1 +645,1 @@\n-        if (ftype.isPrimitive())\n+        if (ftype.isPrimitive()) {\n@@ -637,1 +647,2 @@\n-        else if (VerifyType.isNullReferenceConversion(Object.class, ftype)) {\n+        } else if (ftype.isInterface() || ftype.isAssignableFrom(Object.class)) {\n+            \/\/ retyping can be done without a cast\n@@ -639,1 +650,1 @@\n-        } else\n+        } else {\n@@ -642,0 +653,1 @@\n+        }\n@@ -923,1 +935,1 @@\n-                    MemberName member = new MemberName(MethodHandleStatics.class, \"UNSAFE\", Unsafe.class, REF_getField);\n+                    MemberName member = new MemberName(MethodHandleStatics.class, \"UNSAFE\", Unsafe.class, REF_getStatic);\n@@ -925,1 +937,1 @@\n-                            MemberName.getFactory().resolveOrFail(REF_getField, member,\n+                            MemberName.getFactory().resolveOrFail(REF_getStatic, member,\n@@ -927,1 +939,1 @@\n-                                                                  NoSuchMethodException.class));\n+                                                                  NoSuchFieldException.class));\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/DirectMethodHandle.java","additions":32,"deletions":20,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -264,2 +264,2 @@\n-        \/\/ CDS does not handle disableEagerInitialization.\n-        if (!disableEagerInitialization) {\n+        \/\/ CDS does not handle disableEagerInitialization or useImplMethodHandle\n+        if (!disableEagerInitialization && !useImplMethodHandle) {\n@@ -314,1 +314,1 @@\n-            Set<String> itfs = new LinkedHashSet<>(altInterfaces.length + 1);\n+            Set<String> itfs = LinkedHashSet.newLinkedHashSet(altInterfaces.length + 1);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/InnerClassLambdaMetafactory.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2011, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2011, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -202,6 +202,1 @@\n-        static BasicType basicType(Wrapper type) {\n-            char c = type.basicTypeChar();\n-            return basicType(c);\n-        }\n-            if (!type.isPrimitive())  return L_TYPE;\n-            return basicType(Wrapper.forPrimitiveType(type));\n+            return basicType(Wrapper.basicTypeChar(type));\n@@ -225,2 +220,2 @@\n-        static byte[] basicTypesOrd(Class<?>[] types) {\n-            byte[] ords = new byte[types.length];\n+        static int[] basicTypesOrd(Class<?>[] types) {\n+            int[] ords = new int[types.length];\n@@ -228,1 +223,1 @@\n-                ords[i] = (byte)basicType(types[i]).ordinal();\n+                ords[i] = basicType(types[i]).ordinal();\n@@ -629,1 +624,1 @@\n-        return MethodType.makeImpl(returnType().btClass, ptypes, true);\n+        return MethodType.methodType(returnType().btClass, ptypes, true);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/LambdaForm.java","additions":6,"deletions":11,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -75,12 +75,9 @@\n-            FILTER_RETURN_TO_ZERO = 7,\n-            COLLECT_ARGS = 8,\n-            COLLECT_ARGS_TO_VOID = 9,\n-            COLLECT_ARGS_TO_ARRAY = 10,\n-            FOLD_ARGS = 11,\n-            FOLD_ARGS_TO_VOID = 12,\n-            PERMUTE_ARGS = 13,\n-            LOCAL_TYPES = 14,\n-            FOLD_SELECT_ARGS = 15,\n-            FOLD_SELECT_ARGS_TO_VOID = 16,\n-            FILTER_SELECT_ARGS = 17,\n-            REPEAT_FILTER_ARGS = 18;\n+            COLLECT_ARGS = 7,\n+            COLLECT_ARGS_TO_VOID = 8,\n+            REPEAT_FILTER_ARGS = 9,\n+            FOLD_ARGS = 10,\n+            FOLD_ARGS_TO_VOID = 11,\n+            PERMUTE_ARGS = 12,\n+            LOCAL_TYPES = 13,\n+            FILTER_SELECT_ARGS = 14,\n+            FOLD_SELECT_ARGS = 15;\n@@ -158,0 +155,1 @@\n+            assert(packedBytes(fullBytes) == 0);\n@@ -163,0 +161,1 @@\n+            assert(fullBytes == null || packedBytes == 0);\n@@ -171,0 +170,6 @@\n+\n+        private static int ival(int b) {\n+            assert((b & 0xFF) == b);  \/\/ incoming value must fit in *unsigned* byte\n+            return b;\n+        }\n+\n@@ -190,2 +195,10 @@\n-        private static final byte[] NO_BYTES = {};\n-            return ofBothArrays(kind, b123, NO_BYTES);\n+            long packedBytes = packedBytes(kind, b123);\n+            if (packedBytes != 0) {\n+                return new TransformKey(packedBytes);\n+            }\n+            byte[] fullBytes = new byte[b123.length + 1];\n+            fullBytes[0] = kind;\n+            for (int i = 0; i < b123.length; i++) {\n+                fullBytes[i + 1] = TransformKey.bval(b123[i]);\n+            }\n+            return new TransformKey(fullBytes);\n@@ -194,3 +207,6 @@\n-\n-        static TransformKey of(byte kind, int b1, int[] b23456) {\n-            byte[] fullBytes = new byte[b23456.length + 2];\n+        static TransformKey of(byte kind, int b1, int... b234) {\n+            long packedBytes = packedBytes(kind, b1, b234);\n+            if (packedBytes != 0) {\n+                return new TransformKey(packedBytes);\n+            }\n+            byte[] fullBytes = new byte[b234.length + 2];\n@@ -199,2 +215,2 @@\n-            for (int i = 0; i < b23456.length; i++) {\n-                fullBytes[i + 2] = TransformKey.bval(b23456[i]);\n+            for (int i = 0; i < b234.length; i++) {\n+                fullBytes[i + 2] = TransformKey.bval(b234[i]);\n@@ -202,9 +218,1 @@\n-            long packedBytes = packedBytes(fullBytes);\n-            if (packedBytes != 0)\n-                return new TransformKey(packedBytes);\n-            else\n-                return new TransformKey(fullBytes);\n-        }\n-\n-        static TransformKey of(byte kind, int b1, int b2, byte[] b345) {\n-            return ofBothArrays(kind, new int[]{ b1, b2 }, b345);\n+            return new TransformKey(fullBytes);\n@@ -212,6 +220,4 @@\n-        private static TransformKey ofBothArrays(byte kind, int[] b123, byte[] b456) {\n-            byte[] fullBytes = new byte[1 + b123.length + b456.length];\n-            int i = 0;\n-            fullBytes[i++] = bval(kind);\n-            for (int bv : b123) {\n-                fullBytes[i++] = bval(bv);\n+        static TransformKey of(byte kind, int b1, int b2, int... b345) {\n+            long packedBytes = packedBytes(kind, b1, b2, b345);\n+            if (packedBytes != 0) {\n+                return new TransformKey(packedBytes);\n@@ -219,2 +225,6 @@\n-            for (byte bv : b456) {\n-                fullBytes[i++] = bv;\n+            byte[] fullBytes = new byte[b345.length + 3];\n+            fullBytes[0] = kind;\n+            fullBytes[1] = bval(b1);\n+            fullBytes[2] = bval(b2);\n+            for (int i = 0; i < b345.length; i++) {\n+                fullBytes[i + 3] = TransformKey.bval(b345[i]);\n@@ -222,5 +232,1 @@\n-            long packedBytes = packedBytes(fullBytes);\n-            if (packedBytes != 0)\n-                return new TransformKey(packedBytes);\n-            else\n-                return new TransformKey(fullBytes);\n+            return new TransformKey(fullBytes);\n@@ -235,0 +241,46 @@\n+        private static long packedBytes(byte b0, int b1, int b2, int[] b345) {\n+            if (b345.length + 3 > PACKED_BYTE_MAX_LENGTH)\n+                return 0;\n+            long pb = 0;\n+            int bitset = b0 | b1 | b2;\n+            for (int i = 0; i < b345.length; i++) {\n+                int b = ival(b345[i]);\n+                bitset |= b;\n+                pb |= (long)b << ((i + 3) * PACKED_BYTE_SIZE);\n+            }\n+            if (!inRange(bitset))\n+                return 0;\n+            pb = pb | packedBytes(b0, b1, b2);\n+            return pb;\n+        }\n+        private static long packedBytes(byte b0, int b1, int[] b234) {\n+            if (b234.length + 2 > PACKED_BYTE_MAX_LENGTH)\n+                return 0;\n+            long pb = 0;\n+            int bitset = b0 | b1;\n+            for (int i = 0; i < b234.length; i++) {\n+                int b = ival(b234[i]);\n+                bitset |= b;\n+                pb |= (long)b << ((i + 2) * PACKED_BYTE_SIZE);\n+            }\n+            if (!inRange(bitset))\n+                return 0;\n+            pb = pb | packedBytes(b0, b1);\n+            return pb;\n+        }\n+        private static long packedBytes(byte b0, int[] b123) {\n+            if (b123.length + 1 > PACKED_BYTE_MAX_LENGTH)\n+                return 0;\n+            long pb = 0;\n+            int bitset = b0;\n+            for (int i = 0; i < b123.length; i++) {\n+                int b = ival(b123[i]);\n+                bitset |= b;\n+                pb |= (long)b << ((i + 1) * PACKED_BYTE_SIZE);\n+            }\n+            if (!inRange(bitset))\n+                return 0;\n+            pb = pb | b0;\n+            return pb;\n+        }\n+\n@@ -251,1 +303,1 @@\n-            return (  (b0 << 0*PACKED_BYTE_SIZE)\n+            return (  (b0)\n@@ -256,1 +308,1 @@\n-            return (  (b0 << 0*PACKED_BYTE_SIZE)\n+            return (  (b0)\n@@ -262,1 +314,1 @@\n-            return (  (b0 << 0*PACKED_BYTE_SIZE)\n+            return (  (b0)\n@@ -324,1 +376,0 @@\n-                assert(fullBytes == null);\n@@ -640,1 +691,1 @@\n-        byte[] newTypes = BasicType.basicTypesOrd(collectorType.ptypes());\n+        int[] newTypes = BasicType.basicTypesOrd(collectorType.ptypes());\n@@ -891,2 +942,1 @@\n-        byte kind = (constantZero ? FILTER_RETURN_TO_ZERO : FILTER_RETURN);\n-        TransformKey key = TransformKey.of(kind, newType.ordinal());\n+        TransformKey key = TransformKey.of(FILTER_RETURN, constantZero ? (byte) 1 : (byte)0, newType.ordinal());\n@@ -1006,2 +1056,1 @@\n-        byte kind = (dropResult ? FOLD_SELECT_ARGS_TO_VOID : FOLD_SELECT_ARGS);\n-        TransformKey key = TransformKey.of(kind, foldPos, argPositions);\n+        TransformKey key = TransformKey.of(FOLD_SELECT_ARGS, foldPos, dropResult ? 1 : 0, argPositions);\n@@ -1010,1 +1059,1 @@\n-            assert(form.arity == lambdaForm.arity - (kind == FOLD_SELECT_ARGS ? 1 : 0));\n+            assert(form.arity == lambdaForm.arity - (dropResult ? 0 : 1));\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/LambdaFormEditor.java","additions":100,"deletions":51,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -156,1 +156,1 @@\n-                MethodType res = MethodType.makeImpl(rtype, ptypes, true);\n+                MethodType res = MethodType.methodType(rtype, ptypes, true);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MemberName.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -242,1 +242,1 @@\n-                \/\/ ignore exotic ops the JVM cares about; we just wont issue them\n+                \/\/ ignore exotic ops the JVM cares about; we just won't issue them\n@@ -397,1 +397,1 @@\n-        return MethodType.makeImpl(rtype, ptypes, true);\n+        return MethodType.methodType(rtype, ptypes, true);\n@@ -565,1 +565,1 @@\n-            MethodType guardType = MethodType.makeImpl(guardReturnType, guardParams, true);\n+            MethodType guardType = MethodType.methodType(guardReturnType, guardParams, true);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleNatives.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+import jdk.internal.foreign.Utils;\n+import jdk.internal.javac.PreviewFeature;\n@@ -45,0 +47,4 @@\n+import java.lang.foreign.GroupLayout;\n+import java.lang.foreign.MemoryAddress;\n+import java.lang.foreign.MemoryLayout;\n+import java.lang.foreign.ValueLayout;\n@@ -803,2 +809,1 @@\n-     * <blockquote><pre>\n-     * {@code\n+     * {@snippet lang=\"java\" :\n@@ -808,1 +813,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -2571,1 +2576,1 @@\n-         * <blockquote><pre>{@code\n+         * {@snippet lang=\"java\" :\n@@ -2578,1 +2583,1 @@\n-         * }<\/pre><\/blockquote>\n+         * }\n@@ -2642,1 +2647,1 @@\n-         * <blockquote><pre>{@code\n+         * {@snippet lang=\"java\" :\n@@ -2667,1 +2672,1 @@\n-         * }<\/pre><\/blockquote>\n+         * }\n@@ -2728,1 +2733,1 @@\n-         * <blockquote><pre>{@code\n+         * {@snippet lang=\"java\" :\n@@ -2744,1 +2749,1 @@\n-         * }<\/pre><\/blockquote>\n+         * }\n@@ -2971,1 +2976,1 @@\n-         * <blockquote><pre>{@code\n+         * {@snippet lang=\"java\" :\n@@ -3002,1 +3007,1 @@\n-         * }<\/pre><\/blockquote>\n+         * }\n@@ -3291,1 +3296,1 @@\n-         * <blockquote><pre>{@code\n+         * {@snippet lang=\"java\" :\n@@ -3299,1 +3304,1 @@\n-         * }<\/pre><\/blockquote>\n+         * }\n@@ -4615,1 +4620,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -4620,1 +4625,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -4859,1 +4864,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -4875,1 +4880,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5145,1 +5150,1 @@\n-        return dropArguments(zero(type.returnType()), 0, type.parameterList());\n+        return dropArgumentsTrusted(zero(type.returnType()), 0, type.ptypes());\n@@ -5275,1 +5280,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5286,1 +5291,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5304,1 +5309,1 @@\n-        return dropArguments0(target, pos, copyTypes(valueTypes.toArray()));\n+        return dropArgumentsTrusted(target, pos, valueTypes.toArray(new Class<?>[0]).clone());\n@@ -5307,5 +5312,1 @@\n-    private static List<Class<?>> copyTypes(Object[] array) {\n-        return Arrays.asList(Arrays.copyOf(array, array.length, Class[].class));\n-    }\n-\n-    private static MethodHandle dropArguments0(MethodHandle target, int pos, List<Class<?>> valueTypes) {\n+    static MethodHandle dropArgumentsTrusted(MethodHandle target, int pos, Class<?>[] valueTypes) {\n@@ -5326,2 +5327,2 @@\n-    private static int dropArgumentChecks(MethodType oldType, int pos, List<Class<?>> valueTypes) {\n-        int dropped = valueTypes.size();\n+    private static int dropArgumentChecks(MethodType oldType, int pos, Class<?>[] valueTypes) {\n+        int dropped = valueTypes.length;\n@@ -5351,1 +5352,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5366,1 +5367,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5385,1 +5386,9 @@\n-        return dropArguments0(target, pos, copyTypes(valueTypes));\n+        return dropArgumentsTrusted(target, pos, valueTypes.clone());\n+    }\n+\n+    \/* Convenience overloads for trusting internal low-arity call-sites *\/\n+    static MethodHandle dropArguments(MethodHandle target, int pos, Class<?> valueType1) {\n+        return dropArgumentsTrusted(target, pos, new Class<?>[] { valueType1 });\n+    }\n+    static MethodHandle dropArguments(MethodHandle target, int pos, Class<?> valueType1, Class<?> valueType2) {\n+        return dropArgumentsTrusted(target, pos, new Class<?>[] { valueType1, valueType2 });\n@@ -5389,1 +5398,1 @@\n-    private static MethodHandle dropArgumentsToMatch(MethodHandle target, int skip, List<Class<?>> newTypes, int pos,\n+    private static MethodHandle dropArgumentsToMatch(MethodHandle target, int skip, Class<?>[] newTypes, int pos,\n@@ -5391,3 +5400,2 @@\n-        newTypes = copyTypes(newTypes.toArray());\n-        List<Class<?>> oldTypes = target.type().parameterList();\n-        int match = oldTypes.size();\n+        Class<?>[] oldTypes = target.type().ptypes();\n+        int match = oldTypes.length;\n@@ -5398,1 +5406,1 @@\n-            oldTypes = oldTypes.subList(skip, match);\n+            oldTypes = Arrays.copyOfRange(oldTypes, skip, match);\n@@ -5401,2 +5409,2 @@\n-        List<Class<?>> addTypes = newTypes;\n-        int add = addTypes.size();\n+        Class<?>[] addTypes = newTypes;\n+        int add = addTypes.length;\n@@ -5405,1 +5413,1 @@\n-                throw newIllegalArgumentException(\"illegal pos\", pos, newTypes);\n+                throw newIllegalArgumentException(\"illegal pos\", pos, Arrays.toString(newTypes));\n@@ -5407,1 +5415,1 @@\n-            addTypes = addTypes.subList(pos, add);\n+            addTypes = Arrays.copyOfRange(addTypes, pos, add);\n@@ -5409,1 +5417,1 @@\n-            assert(addTypes.size() == add);\n+            assert(addTypes.length == add);\n@@ -5412,1 +5420,1 @@\n-        if (match > add || !oldTypes.equals(addTypes.subList(0, match))) {\n+        if (match > add || !Arrays.equals(oldTypes, 0, oldTypes.length, addTypes, 0, match)) {\n@@ -5416,1 +5424,2 @@\n-            throw newIllegalArgumentException(\"argument lists do not match\", oldTypes, newTypes);\n+            throw newIllegalArgumentException(\"argument lists do not match\",\n+                Arrays.toString(oldTypes), Arrays.toString(newTypes));\n@@ -5418,1 +5427,1 @@\n-        addTypes = addTypes.subList(match, add);\n+        addTypes = Arrays.copyOfRange(addTypes, match, add);\n@@ -5420,1 +5429,1 @@\n-        assert(addTypes.size() == add);\n+        assert(addTypes.length == add);\n@@ -5425,1 +5434,1 @@\n-            adapter = dropArguments0(adapter, skip+ match, addTypes);\n+            adapter = dropArgumentsTrusted(adapter, skip+ match, addTypes);\n@@ -5429,1 +5438,1 @@\n-            adapter = dropArguments0(adapter, skip, newTypes.subList(0, pos));\n+            adapter = dropArgumentsTrusted(adapter, skip, Arrays.copyOfRange(newTypes, 0, pos));\n@@ -5461,1 +5470,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5476,1 +5485,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5493,1 +5502,1 @@\n-        return dropArgumentsToMatch(target, skip, newTypes, pos, false);\n+        return dropArgumentsToMatch(target, skip, newTypes.toArray(new Class<?>[0]).clone(), pos, false);\n@@ -5547,1 +5556,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5562,1 +5571,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5572,1 +5581,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5578,1 +5587,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5647,1 +5656,1 @@\n-        MethodType newType = MethodType.makeImpl(targetType.rtype(), ptypes, true);\n+        MethodType newType = MethodType.methodType(targetType.rtype(), ptypes, true);\n@@ -5707,1 +5716,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5732,1 +5741,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5744,1 +5753,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5765,1 +5774,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5770,1 +5779,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5773,1 +5782,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5812,1 +5821,1 @@\n-        List<Class<?>> filterArgs = filterType.parameterList();\n+        Class<?>[] filterArgs = filterType.ptypes();\n@@ -5823,1 +5832,1 @@\n-        return targetType.dropParameterTypes(pos, pos+1).insertParameterTypes(pos, filterArgs);\n+        return targetType.dropParameterTypes(pos, pos + 1).insertParameterTypes(pos, filterArgs);\n@@ -5841,1 +5850,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5852,1 +5861,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5858,1 +5867,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5879,1 +5888,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -5916,1 +5925,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5922,1 +5931,2 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n+     * }\n@@ -5979,1 +5989,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -5992,1 +6002,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6002,1 +6012,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6017,1 +6027,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6047,1 +6057,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6060,1 +6070,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6071,1 +6081,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6086,1 +6096,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6247,1 +6257,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6257,1 +6267,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6280,2 +6290,2 @@\n-        List<Class<?>> targs = ttype.parameterList();\n-        test = dropArgumentsToMatch(test, 0, targs, 0, true);\n+\n+        test = dropArgumentsToMatch(test, 0, ttype.ptypes(), 0, true);\n@@ -6310,1 +6320,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6320,1 +6330,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6354,1 +6364,1 @@\n-        handler = dropArgumentsToMatch(handler, 1, ttype.parameterList(), 0, true);\n+        handler = dropArgumentsToMatch(handler, 1, ttype.ptypes(), 0, true);\n@@ -6592,1 +6602,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6608,1 +6618,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6614,1 +6624,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6627,1 +6637,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6629,1 +6639,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6643,1 +6653,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6645,1 +6655,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6664,1 +6674,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6729,1 +6739,0 @@\n-\n@@ -6740,1 +6749,1 @@\n-                pred.set(i, dropArguments0(constant(boolean.class, true), 0, commonParameterSequence));\n+                pred.set(i, dropArguments(constant(boolean.class, true), 0, commonParameterSequence));\n@@ -6800,1 +6809,1 @@\n-        return longestParameterList(Arrays.asList(longest1, longest2));\n+        return longestParameterList(List.of(longest1, longest2));\n@@ -6839,1 +6848,1 @@\n-            return pc < tpsize ? dropArguments0(h, pc, targetParams.subList(pc, tpsize)) : h;\n+            return pc < tpsize ? dropArguments(h, pc, targetParams.subList(pc, tpsize)) : h;\n@@ -6894,1 +6903,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6905,1 +6914,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6908,1 +6917,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6923,1 +6932,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -6927,1 +6936,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -6936,1 +6945,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7007,1 +7016,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7018,1 +7027,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7021,1 +7030,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7029,1 +7038,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7033,1 +7042,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7040,1 +7049,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7150,1 +7159,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7162,1 +7171,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7165,1 +7174,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7174,1 +7183,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7178,1 +7187,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7187,1 +7196,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7191,1 +7200,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7200,1 +7209,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7204,1 +7213,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7215,1 +7224,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7218,1 +7227,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7222,1 +7231,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7305,1 +7314,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7319,1 +7328,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7322,1 +7331,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7344,1 +7353,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7507,1 +7516,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7520,1 +7529,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7523,1 +7532,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7535,1 +7544,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7538,1 +7547,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7557,1 +7566,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7657,1 +7666,1 @@\n-                externalParamList = Arrays.asList(Iterable.class);\n+                externalParamList = List.of(Iterable.class);\n@@ -7723,1 +7732,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7739,1 +7748,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7783,1 +7792,1 @@\n-        List<Class<?>> targetParamTypes = target.type().parameterList();\n+        Class<?>[] targetParamTypes = target.type().ptypes();\n@@ -7791,1 +7800,1 @@\n-        cleanup = dropArgumentsToMatch(cleanup, (rtype == void.class ? 1 : 2), targetParamTypes, 0);\n+        cleanup = dropArgumentsToMatch(cleanup, (rtype == void.class ? 1 : 2), targetParamTypes, 0, false);\n@@ -7845,1 +7854,1 @@\n-     * <blockquote><pre>{@code\n+     * {@snippet lang=\"java\" :\n@@ -7865,1 +7874,1 @@\n-     * }<\/pre><\/blockquote>\n+     * }\n@@ -7908,0 +7917,295 @@\n+    \/**\n+     * Creates a var handle object, which can be used to dereference a {@linkplain java.lang.foreign.MemorySegment memory segment}\n+     * by viewing its contents as a sequence of the provided value layout.\n+     *\n+     * <p>The provided layout specifies the {@linkplain ValueLayout#carrier() carrier type},\n+     * the {@linkplain ValueLayout#byteSize() byte size},\n+     * the {@linkplain ValueLayout#byteAlignment() byte alignment} and the {@linkplain ValueLayout#order() byte order}\n+     * associated with the returned var handle.\n+     *\n+     * <p>The returned var handle's type is {@code carrier} and the list of coordinate types is\n+     * {@code (MemorySegment, long)}, where the {@code long} coordinate type corresponds to byte offset into\n+     * a given memory segment. The returned var handle accesses bytes at an offset in a given\n+     * memory segment, composing bytes to or from a value of the type {@code carrier} according to the given endianness;\n+     * the alignment constraint (in bytes) for the resulting var handle is given by {@code alignmentBytes}.\n+     *\n+     * <p>As an example, consider the memory layout expressed by a {@link GroupLayout} instance constructed as follows:\n+     * {@snippet lang=\"java\" :\n+     *     GroupLayout seq = java.lang.foreign.MemoryLayout.structLayout(\n+     *             MemoryLayout.paddingLayout(32),\n+     *             ValueLayout.JAVA_INT.withOrder(ByteOrder.BIG_ENDIAN).withName(\"value\")\n+     *     );\n+     * }\n+     * To access the member layout named {@code value}, we can construct a memory segment view var handle as follows:\n+     * {@snippet lang=\"java\" :\n+     *     VarHandle handle = MethodHandles.memorySegmentViewVarHandle(ValueLayout.JAVA_INT.withOrder(ByteOrder.BIG_ENDIAN)); \/\/(MemorySegment, long) -> int\n+     *     handle = MethodHandles.insertCoordinates(handle, 1, 4); \/\/(MemorySegment) -> int\n+     * }\n+     *\n+     * @apiNote The resulting var handle features certain <i>access mode restrictions<\/i>,\n+     * which are common to all memory segment view var handles. A memory segment view var handle is associated\n+     * with an access size {@code S} and an alignment constraint {@code B}\n+     * (both expressed in bytes). We say that a memory access operation is <em>fully aligned<\/em> if it occurs\n+     * at a memory address {@code A} which is compatible with both alignment constraints {@code S} and {@code B}.\n+     * If access is fully aligned then following access modes are supported and are\n+     * guaranteed to support atomic access:\n+     * <ul>\n+     * <li>read write access modes for all {@code T}, with the exception of\n+     *     access modes {@code get} and {@code set} for {@code long} and\n+     *     {@code double} on 32-bit platforms.\n+     * <li>atomic update access modes for {@code int}, {@code long},\n+     *     {@code float}, {@code double} or {@link MemoryAddress}.\n+     *     (Future major platform releases of the JDK may support additional\n+     *     types for certain currently unsupported access modes.)\n+     * <li>numeric atomic update access modes for {@code int}, {@code long} and {@link MemoryAddress}.\n+     *     (Future major platform releases of the JDK may support additional\n+     *     numeric types for certain currently unsupported access modes.)\n+     * <li>bitwise atomic update access modes for {@code int}, {@code long} and {@link MemoryAddress}.\n+     *     (Future major platform releases of the JDK may support additional\n+     *     numeric types for certain currently unsupported access modes.)\n+     * <\/ul>\n+     *\n+     * If {@code T} is {@code float}, {@code double} or {@link MemoryAddress} then atomic\n+     * update access modes compare values using their bitwise representation\n+     * (see {@link Float#floatToRawIntBits},\n+     * {@link Double#doubleToRawLongBits} and {@link MemoryAddress#toRawLongValue()}, respectively).\n+     * <p>\n+     * Alternatively, a memory access operation is <em>partially aligned<\/em> if it occurs at a memory address {@code A}\n+     * which is only compatible with the alignment constraint {@code B}; in such cases, access for anything other than the\n+     * {@code get} and {@code set} access modes will result in an {@code IllegalStateException}. If access is partially aligned,\n+     * atomic access is only guaranteed with respect to the largest power of two that divides the GCD of {@code A} and {@code S}.\n+     * <p>\n+     * Finally, in all other cases, we say that a memory access operation is <em>misaligned<\/em>; in such cases an\n+     * {@code IllegalStateException} is thrown, irrespective of the access mode being used.\n+     *\n+     * @param layout the value layout for which a memory access handle is to be obtained.\n+     * @return the new memory segment view var handle.\n+     * @throws IllegalArgumentException if an illegal carrier type is used, or if {@code alignmentBytes} is not a power of two.\n+     * @throws NullPointerException if {@code layout} is {@code null}.\n+     * @see MemoryLayout#varHandle(MemoryLayout.PathElement...)\n+     * @since 19\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.FOREIGN)\n+    public static VarHandle memorySegmentViewVarHandle(ValueLayout layout) {\n+        Objects.requireNonNull(layout);\n+        return Utils.makeSegmentViewVarHandle(layout);\n+    }\n+\n+    \/**\n+     * Adapts a target var handle by pre-processing incoming and outgoing values using a pair of filter functions.\n+     * <p>\n+     * When calling e.g. {@link VarHandle#set(Object...)} on the resulting var handle, the incoming value (of type {@code T}, where\n+     * {@code T} is the <em>last<\/em> parameter type of the first filter function) is processed using the first filter and then passed\n+     * to the target var handle.\n+     * Conversely, when calling e.g. {@link VarHandle#get(Object...)} on the resulting var handle, the return value obtained from\n+     * the target var handle (of type {@code T}, where {@code T} is the <em>last<\/em> parameter type of the second filter function)\n+     * is processed using the second filter and returned to the caller. More advanced access mode types, such as\n+     * {@link VarHandle.AccessMode#COMPARE_AND_EXCHANGE} might apply both filters at the same time.\n+     * <p>\n+     * For the boxing and unboxing filters to be well-formed, their types must be of the form {@code (A... , S) -> T} and\n+     * {@code (A... , T) -> S}, respectively, where {@code T} is the type of the target var handle. If this is the case,\n+     * the resulting var handle will have type {@code S} and will feature the additional coordinates {@code A...} (which\n+     * will be appended to the coordinates of the target var handle).\n+     * <p>\n+     * If the boxing and unboxing filters throw any checked exceptions when invoked, the resulting var handle will\n+     * throw an {@link IllegalStateException}.\n+     * <p>\n+     * The resulting var handle will feature the same access modes (see {@link VarHandle.AccessMode}) and\n+     * atomic access guarantees as those featured by the target var handle.\n+     *\n+     * @param target the target var handle\n+     * @param filterToTarget a filter to convert some type {@code S} into the type of {@code target}\n+     * @param filterFromTarget a filter to convert the type of {@code target} to some type {@code S}\n+     * @return an adapter var handle which accepts a new type, performing the provided boxing\/unboxing conversions.\n+     * @throws IllegalArgumentException if {@code filterFromTarget} and {@code filterToTarget} are not well-formed, that is, they have types\n+     * other than {@code (A... , S) -> T} and {@code (A... , T) -> S}, respectively, where {@code T} is the type of the target var handle,\n+     * or if it's determined that either {@code filterFromTarget} or {@code filterToTarget} throws any checked exceptions.\n+     * @throws NullPointerException if any of the arguments is {@code null}.\n+     * @since 19\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.FOREIGN)\n+    public static VarHandle filterValue(VarHandle target, MethodHandle filterToTarget, MethodHandle filterFromTarget) {\n+        return VarHandles.filterValue(target, filterToTarget, filterFromTarget);\n+    }\n+\n+    \/**\n+     * Adapts a target var handle by pre-processing incoming coordinate values using unary filter functions.\n+     * <p>\n+     * When calling e.g. {@link VarHandle#get(Object...)} on the resulting var handle, the incoming coordinate values\n+     * starting at position {@code pos} (of type {@code C1, C2 ... Cn}, where {@code C1, C2 ... Cn} are the return types\n+     * of the unary filter functions) are transformed into new values (of type {@code S1, S2 ... Sn}, where {@code S1, S2 ... Sn} are the\n+     * parameter types of the unary filter functions), and then passed (along with any coordinate that was left unaltered\n+     * by the adaptation) to the target var handle.\n+     * <p>\n+     * For the coordinate filters to be well-formed, their types must be of the form {@code S1 -> T1, S2 -> T1 ... Sn -> Tn},\n+     * where {@code T1, T2 ... Tn} are the coordinate types starting at position {@code pos} of the target var handle.\n+     * <p>\n+     * If any of the filters throws a checked exception when invoked, the resulting var handle will\n+     * throw an {@link IllegalStateException}.\n+     * <p>\n+     * The resulting var handle will feature the same access modes (see {@link VarHandle.AccessMode}) and\n+     * atomic access guarantees as those featured by the target var handle.\n+     *\n+     * @param target the target var handle\n+     * @param pos the position of the first coordinate to be transformed\n+     * @param filters the unary functions which are used to transform coordinates starting at position {@code pos}\n+     * @return an adapter var handle which accepts new coordinate types, applying the provided transformation\n+     * to the new coordinate values.\n+     * @throws IllegalArgumentException if the handles in {@code filters} are not well-formed, that is, they have types\n+     * other than {@code S1 -> T1, S2 -> T2, ... Sn -> Tn} where {@code T1, T2 ... Tn} are the coordinate types starting\n+     * at position {@code pos} of the target var handle, if {@code pos} is not between 0 and the target var handle coordinate arity, inclusive,\n+     * or if more filters are provided than the actual number of coordinate types available starting at {@code pos},\n+     * or if it's determined that any of the filters throws any checked exceptions.\n+     * @throws NullPointerException if any of the arguments is {@code null} or {@code filters} contains {@code null}.\n+     * @since 19\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.FOREIGN)\n+    public static VarHandle filterCoordinates(VarHandle target, int pos, MethodHandle... filters) {\n+        return VarHandles.filterCoordinates(target, pos, filters);\n+    }\n+\n+    \/**\n+     * Provides a target var handle with one or more <em>bound coordinates<\/em>\n+     * in advance of the var handle's invocation. As a consequence, the resulting var handle will feature less\n+     * coordinate types than the target var handle.\n+     * <p>\n+     * When calling e.g. {@link VarHandle#get(Object...)} on the resulting var handle, incoming coordinate values\n+     * are joined with bound coordinate values, and then passed to the target var handle.\n+     * <p>\n+     * For the bound coordinates to be well-formed, their types must be {@code T1, T2 ... Tn },\n+     * where {@code T1, T2 ... Tn} are the coordinate types starting at position {@code pos} of the target var handle.\n+     * <p>\n+     * The resulting var handle will feature the same access modes (see {@link VarHandle.AccessMode}) and\n+     * atomic access guarantees as those featured by the target var handle.\n+     *\n+     * @param target the var handle to invoke after the bound coordinates are inserted\n+     * @param pos the position of the first coordinate to be inserted\n+     * @param values the series of bound coordinates to insert\n+     * @return an adapter var handle which inserts additional coordinates,\n+     *         before calling the target var handle\n+     * @throws IllegalArgumentException if {@code pos} is not between 0 and the target var handle coordinate arity, inclusive,\n+     * or if more values are provided than the actual number of coordinate types available starting at {@code pos}.\n+     * @throws ClassCastException if the bound coordinates in {@code values} are not well-formed, that is, they have types\n+     * other than {@code T1, T2 ... Tn }, where {@code T1, T2 ... Tn} are the coordinate types starting at position {@code pos}\n+     * of the target var handle.\n+     * @throws NullPointerException if any of the arguments is {@code null} or {@code values} contains {@code null}.\n+     * @since 19\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.FOREIGN)\n+    public static VarHandle insertCoordinates(VarHandle target, int pos, Object... values) {\n+        return VarHandles.insertCoordinates(target, pos, values);\n+    }\n+\n+    \/**\n+     * Provides a var handle which adapts the coordinate values of the target var handle, by re-arranging them\n+     * so that the new coordinates match the provided ones.\n+     * <p>\n+     * The given array controls the reordering.\n+     * Call {@code #I} the number of incoming coordinates (the value\n+     * {@code newCoordinates.size()}), and call {@code #O} the number\n+     * of outgoing coordinates (the number of coordinates associated with the target var handle).\n+     * Then the length of the reordering array must be {@code #O},\n+     * and each element must be a non-negative number less than {@code #I}.\n+     * For every {@code N} less than {@code #O}, the {@code N}-th\n+     * outgoing coordinate will be taken from the {@code I}-th incoming\n+     * coordinate, where {@code I} is {@code reorder[N]}.\n+     * <p>\n+     * No coordinate value conversions are applied.\n+     * The type of each incoming coordinate, as determined by {@code newCoordinates},\n+     * must be identical to the type of the corresponding outgoing coordinate\n+     * in the target var handle.\n+     * <p>\n+     * The reordering array need not specify an actual permutation.\n+     * An incoming coordinate will be duplicated if its index appears\n+     * more than once in the array, and an incoming coordinate will be dropped\n+     * if its index does not appear in the array.\n+     * <p>\n+     * The resulting var handle will feature the same access modes (see {@link VarHandle.AccessMode}) and\n+     * atomic access guarantees as those featured by the target var handle.\n+     * @param target the var handle to invoke after the coordinates have been reordered\n+     * @param newCoordinates the new coordinate types\n+     * @param reorder an index array which controls the reordering\n+     * @return an adapter var handle which re-arranges the incoming coordinate values,\n+     * before calling the target var handle\n+     * @throws IllegalArgumentException if the index array length is not equal to\n+     * the number of coordinates of the target var handle, or if any index array element is not a valid index for\n+     * a coordinate of {@code newCoordinates}, or if two corresponding coordinate types in\n+     * the target var handle and in {@code newCoordinates} are not identical.\n+     * @throws NullPointerException if any of the arguments is {@code null} or {@code newCoordinates} contains {@code null}.\n+     * @since 19\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.FOREIGN)\n+    public static VarHandle permuteCoordinates(VarHandle target, List<Class<?>> newCoordinates, int... reorder) {\n+        return VarHandles.permuteCoordinates(target, newCoordinates, reorder);\n+    }\n+\n+    \/**\n+     * Adapts a target var handle by pre-processing\n+     * a sub-sequence of its coordinate values with a filter (a method handle).\n+     * The pre-processed coordinates are replaced by the result (if any) of the\n+     * filter function and the target var handle is then called on the modified (usually shortened)\n+     * coordinate list.\n+     * <p>\n+     * If {@code R} is the return type of the filter (which cannot be void), the target var handle must accept a value of\n+     * type {@code R} as its coordinate in position {@code pos}, preceded and\/or followed by\n+     * any coordinate not passed to the filter.\n+     * No coordinates are reordered, and the result returned from the filter\n+     * replaces (in order) the whole subsequence of coordinates originally\n+     * passed to the adapter.\n+     * <p>\n+     * The argument types (if any) of the filter\n+     * replace zero or one coordinate types of the target var handle, at position {@code pos},\n+     * in the resulting adapted var handle.\n+     * The return type of the filter must be identical to the\n+     * coordinate type of the target var handle at position {@code pos}, and that target var handle\n+     * coordinate is supplied by the return value of the filter.\n+     * <p>\n+     * If any of the filters throws a checked exception when invoked, the resulting var handle will\n+     * throw an {@link IllegalStateException}.\n+     * <p>\n+     * The resulting var handle will feature the same access modes (see {@link VarHandle.AccessMode}) and\n+     * atomic access guarantees as those featured by the target var handle.\n+     *\n+     * @param target the var handle to invoke after the coordinates have been filtered\n+     * @param pos the position of the coordinate to be filtered\n+     * @param filter the filter method handle\n+     * @return an adapter var handle which filters the incoming coordinate values,\n+     * before calling the target var handle\n+     * @throws IllegalArgumentException if the return type of {@code filter}\n+     * is void, or it is not the same as the {@code pos} coordinate of the target var handle,\n+     * if {@code pos} is not between 0 and the target var handle coordinate arity, inclusive,\n+     * if the resulting var handle's type would have <a href=\"MethodHandle.html#maxarity\">too many coordinates<\/a>,\n+     * or if it's determined that {@code filter} throws any checked exceptions.\n+     * @throws NullPointerException if any of the arguments is {@code null}.\n+     * @since 19\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.FOREIGN)\n+    public static VarHandle collectCoordinates(VarHandle target, int pos, MethodHandle filter) {\n+        return VarHandles.collectCoordinates(target, pos, filter);\n+    }\n+\n+    \/**\n+     * Returns a var handle which will discard some dummy coordinates before delegating to the\n+     * target var handle. As a consequence, the resulting var handle will feature more\n+     * coordinate types than the target var handle.\n+     * <p>\n+     * The {@code pos} argument may range between zero and <i>N<\/i>, where <i>N<\/i> is the arity of the\n+     * target var handle's coordinate types. If {@code pos} is zero, the dummy coordinates will precede\n+     * the target's real arguments; if {@code pos} is <i>N<\/i> they will come after.\n+     * <p>\n+     * The resulting var handle will feature the same access modes (see {@link VarHandle.AccessMode}) and\n+     * atomic access guarantees as those featured by the target var handle.\n+     *\n+     * @param target the var handle to invoke after the dummy coordinates are dropped\n+     * @param pos position of the first coordinate to drop (zero for the leftmost)\n+     * @param valueTypes the type(s) of the coordinate(s) to drop\n+     * @return an adapter var handle which drops some dummy coordinates,\n+     *         before calling the target var handle\n+     * @throws IllegalArgumentException if {@code pos} is not between 0 and the target var handle coordinate arity, inclusive.\n+     * @throws NullPointerException if any of the arguments is {@code null} or {@code valueTypes} contains {@code null}.\n+     * @since 19\n+     *\/\n+    @PreviewFeature(feature=PreviewFeature.Feature.FOREIGN)\n+    public static VarHandle dropCoordinates(VarHandle target, int pos, Class<?>... valueTypes) {\n+        return VarHandles.dropCoordinates(target, pos, valueTypes);\n+    }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandles.java","additions":441,"deletions":137,"binary":false,"changes":578,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -46,0 +46,1 @@\n+import jdk.internal.access.SharedSecrets;\n@@ -244,1 +245,1 @@\n-        return makeImpl(rtype, ptypes, false);\n+        return methodType(rtype, ptypes, false);\n@@ -258,1 +259,1 @@\n-        return makeImpl(rtype, listToArray(ptypes), notrust);\n+        return methodType(rtype, listToArray(ptypes), notrust);\n@@ -279,1 +280,15 @@\n-        Class<?>[] ptypes1 = new Class<?>[1+ptypes.length];\n+        int len = ptypes.length;\n+        if (rtype == Object.class && ptype0 == Object.class) {\n+            if (len == 0) {\n+                return genericMethodType(1, false);\n+            }\n+            if (isAllObject(ptypes, len - 1)) {\n+                Class<?> lastParam = ptypes[len - 1];\n+                if (lastParam == Object.class) {\n+                    return genericMethodType(len + 1, false);\n+                } else if (lastParam == Object[].class) {\n+                    return genericMethodType(len, true);\n+                }\n+            }\n+        }\n+        Class<?>[] ptypes1 = new Class<?>[1 + len];\n@@ -281,1 +296,1 @@\n-        System.arraycopy(ptypes, 0, ptypes1, 1, ptypes.length);\n+        System.arraycopy(ptypes, 0, ptypes1, 1, len);\n@@ -294,0 +309,3 @@\n+        if (rtype == Object.class) {\n+            return genericMethodType(0, false);\n+        }\n@@ -308,0 +326,7 @@\n+        if (rtype == Object.class) {\n+            if (ptype0 == Object.class) {\n+                return genericMethodType(1, false);\n+            } else if (ptype0 == Object[].class) {\n+                return genericMethodType(0, true);\n+            }\n+        }\n@@ -322,1 +347,29 @@\n-        return makeImpl(rtype, ptypes.ptypes, true);\n+        return methodType(rtype, ptypes.ptypes, true);\n+    }\n+\n+    private static boolean isAllObject(Class<?>[] ptypes, int to) {\n+        for (int i = 0; i < to; i++) {\n+            if (ptypes[i] != Object.class) {\n+                return false;\n+            }\n+        }\n+        return true;\n+    }\n+\n+    \/*trusted*\/\n+    static MethodType methodType(Class<?> rtype, Class<?>[] ptypes, boolean trusted) {\n+        if (rtype == Object.class) {\n+            int last = ptypes.length - 1;\n+            if (last < 0) {\n+                return genericMethodType(0, false);\n+            }\n+            if (isAllObject(ptypes, last)) {\n+                Class<?> lastParam = ptypes[last];\n+                if (lastParam == Object.class) {\n+                    return genericMethodType(last + 1, false);\n+                } else if (lastParam == Object[].class) {\n+                    return genericMethodType(last, true);\n+                }\n+            }\n+        }\n+        return makeImpl(rtype, ptypes, trusted);\n@@ -336,2 +389,1 @@\n-    \/*trusted*\/\n-    static MethodType makeImpl(Class<?> rtype, Class<?>[] ptypes, boolean trusted) {\n+    private static MethodType makeImpl(Class<?> rtype, Class<?>[] ptypes, boolean trusted) {\n@@ -633,1 +685,1 @@\n-        return makeImpl(rtype, nptypes, true);\n+        return methodType(rtype, nptypes, true);\n@@ -645,1 +697,1 @@\n-        return makeImpl(nrtype, ptypes, true);\n+        return methodType(nrtype, ptypes, true);\n@@ -1175,1 +1227,1 @@\n-        return makeImpl(rtype, ptypes, true);\n+        return methodType(rtype, ptypes, true);\n@@ -1362,1 +1414,1 @@\n-            this.stale = new ReferenceQueue<>();\n+            this.stale = SharedSecrets.getJavaLangRefAccess().newNativeReferenceQueue();\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodType.java","additions":64,"deletions":12,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,2 +34,0 @@\n-import java.util.HashMap;\n-import java.util.Map;\n@@ -39,3 +37,0 @@\n-import java.util.function.BiFunction;\n-import java.util.function.Function;\n-import jdk.internal.util.Preconditions;\n@@ -479,1 +474,38 @@\n-public abstract class VarHandle implements Constable {\n+public abstract sealed class VarHandle implements Constable\n+     permits IndirectVarHandle, VarHandleSegmentViewBase,\n+             VarHandleByteArrayAsChars.ByteArrayViewVarHandle,\n+             VarHandleByteArrayAsDoubles.ByteArrayViewVarHandle,\n+             VarHandleByteArrayAsFloats.ByteArrayViewVarHandle,\n+             VarHandleByteArrayAsInts.ByteArrayViewVarHandle,\n+             VarHandleByteArrayAsLongs.ByteArrayViewVarHandle,\n+             VarHandleByteArrayAsShorts.ByteArrayViewVarHandle,\n+             VarHandleBooleans.Array,\n+             VarHandleBooleans.FieldInstanceReadOnly,\n+             VarHandleBooleans.FieldStaticReadOnly,\n+             VarHandleBytes.Array,\n+             VarHandleBytes.FieldInstanceReadOnly,\n+             VarHandleBytes.FieldStaticReadOnly,\n+             VarHandleChars.Array,\n+             VarHandleChars.FieldInstanceReadOnly,\n+             VarHandleChars.FieldStaticReadOnly,\n+             VarHandleDoubles.Array,\n+             VarHandleDoubles.FieldInstanceReadOnly,\n+             VarHandleDoubles.FieldStaticReadOnly,\n+             VarHandleFloats.Array,\n+             VarHandleFloats.FieldInstanceReadOnly,\n+             VarHandleFloats.FieldStaticReadOnly,\n+             VarHandleInts.Array,\n+             VarHandleInts.FieldInstanceReadOnly,\n+             VarHandleInts.FieldStaticReadOnly,\n+             VarHandleLongs.Array,\n+             VarHandleLongs.FieldInstanceReadOnly,\n+             VarHandleLongs.FieldStaticReadOnly,\n+             VarHandleReferences.Array,\n+             VarHandleReferences.FieldInstanceReadOnly,\n+             VarHandleReferences.FieldStaticReadOnly,\n+             VarHandleShorts.Array,\n+             VarHandleShorts.FieldInstanceReadOnly,\n+             VarHandleShorts.FieldStaticReadOnly,\n+             VarHandleValues.Array,\n+             VarHandleValues.FieldInstanceReadOnly,\n+             VarHandleValues.FieldStaticReadOnly {\n@@ -496,4 +528,0 @@\n-    boolean isDirect() {\n-        return true;\n-    }\n-\n@@ -2046,0 +2074,10 @@\n+    \/**\n+     * Validates that the given access descriptors method type matches up with\n+     * the access mode of this VarHandle, then returns if this is a direct\n+     * method handle. These operations were grouped together to slightly\n+     * improve efficiency during startup\/warmup.\n+     *\n+     * @return true if this is a direct VarHandle, false if it's an indirect\n+     *         VarHandle.\n+     * @throws WrongMethodTypeException if there's an access type mismatch\n+     *\/\n@@ -2047,1 +2085,1 @@\n-    final void checkExactAccessMode(VarHandle.AccessDescriptor ad) {\n+    boolean checkAccessModeThenIsDirect(VarHandle.AccessDescriptor ad) {\n@@ -2051,0 +2089,2 @@\n+        \/\/ return true unless overridden in an IndirectVarHandle\n+        return true;\n@@ -2061,2 +2101,5 @@\n-        TypesAndInvokers tis = getTypesAndInvokers();\n-        MethodType mt = tis.methodType_table[accessTypeOrdinal];\n+        MethodType[] mtTable = methodTypeTable;\n+        if (mtTable == null) {\n+            mtTable = methodTypeTable = new MethodType[VarHandle.AccessType.COUNT];\n+        }\n+        MethodType mt = mtTable[accessTypeOrdinal];\n@@ -2064,1 +2107,1 @@\n-            mt = tis.methodType_table[accessTypeOrdinal] =\n+            mt = mtTable[accessTypeOrdinal] =\n@@ -2139,5 +2182,1 @@\n-    TypesAndInvokers typesAndInvokers;\n-\n-    static class TypesAndInvokers {\n-        final @Stable\n-        MethodType[] methodType_table = new MethodType[VarHandle.AccessType.COUNT];\n+    MethodType[] methodTypeTable;\n@@ -2145,12 +2184,2 @@\n-        final @Stable\n-        MethodHandle[] methodHandle_table = new MethodHandle[AccessMode.COUNT];\n-    }\n-\n-    @ForceInline\n-    private final TypesAndInvokers getTypesAndInvokers() {\n-        TypesAndInvokers tis = typesAndInvokers;\n-        if (tis == null) {\n-            tis = typesAndInvokers = new TypesAndInvokers();\n-        }\n-        return tis;\n-    }\n+    @Stable\n+    MethodHandle[] methodHandleTable;\n@@ -2160,2 +2189,5 @@\n-        TypesAndInvokers tis = getTypesAndInvokers();\n-        MethodHandle mh = tis.methodHandle_table[mode];\n+        MethodHandle[] mhTable = methodHandleTable;\n+        if (mhTable == null) {\n+            mhTable = methodHandleTable = new MethodHandle[AccessMode.COUNT];\n+        }\n+        MethodHandle mh = mhTable[mode];\n@@ -2163,1 +2195,1 @@\n-            mh = tis.methodHandle_table[mode] = getMethodHandleUncached(mode);\n+            mh = mhTable[mode] = getMethodHandleUncached(mode);\n@@ -2167,0 +2199,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandle.java","additions":68,"deletions":35,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -320,1 +320,1 @@\n-     * Creates a memory access VarHandle.\n+     * Creates a memory segment view var handle.\n@@ -322,6 +322,2 @@\n-     * Resulting VarHandle will take a memory address as first argument,\n-     * and a certain number of coordinate {@code long} parameters, depending on the length\n-     * of the {@code strides} argument array.\n-     *\n-     * Coordinates are multiplied with corresponding scale factors ({@code strides}) and added\n-     * to a single fixed offset to compute an effective offset from the given MemoryAddress for the access.\n+     * The resulting var handle will take a memory segment as first argument (the segment to be dereferenced),\n+     * and a {@code long} as second argument (the offset into the segment).\n@@ -330,1 +326,0 @@\n-     * @param skipAlignmentMaskCheck if true, only the base part of the address will be checked for alignment.\n@@ -335,2 +330,2 @@\n-    static VarHandle makeMemoryAddressViewHandle(Class<?> carrier, boolean skipAlignmentMaskCheck, long alignmentMask,\n-                                                 ByteOrder byteOrder) {\n+    static VarHandle memorySegmentViewHandle(Class<?> carrier, long alignmentMask,\n+                                             ByteOrder byteOrder) {\n@@ -345,1 +340,1 @@\n-            return maybeAdapt(new MemoryAccessVarHandleByteHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+            return maybeAdapt(new VarHandleSegmentAsBytes(be, size, alignmentMask, exact));\n@@ -347,1 +342,1 @@\n-            return maybeAdapt(new MemoryAccessVarHandleCharHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+            return maybeAdapt(new VarHandleSegmentAsChars(be, size, alignmentMask, exact));\n@@ -349,1 +344,1 @@\n-            return maybeAdapt(new MemoryAccessVarHandleShortHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+            return maybeAdapt(new VarHandleSegmentAsShorts(be, size, alignmentMask, exact));\n@@ -351,1 +346,1 @@\n-            return maybeAdapt(new MemoryAccessVarHandleIntHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+            return maybeAdapt(new VarHandleSegmentAsInts(be, size, alignmentMask, exact));\n@@ -353,1 +348,1 @@\n-            return maybeAdapt(new MemoryAccessVarHandleFloatHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+            return maybeAdapt(new VarHandleSegmentAsFloats(be, size, alignmentMask, exact));\n@@ -355,1 +350,1 @@\n-            return maybeAdapt(new MemoryAccessVarHandleLongHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+            return maybeAdapt(new VarHandleSegmentAsLongs(be, size, alignmentMask, exact));\n@@ -357,1 +352,1 @@\n-            return maybeAdapt(new MemoryAccessVarHandleDoubleHelper(skipAlignmentMaskCheck, be, size, alignmentMask, exact));\n+            return maybeAdapt(new VarHandleSegmentAsDoubles(be, size, alignmentMask, exact));\n@@ -715,2 +710,2 @@\n-\/\/                    handle.checkExactAccessMode(ad);\n-\/\/                    if (handle.isDirect() && handle.vform.methodType_table[ad.type] == ad.symbolicMethodTypeErased) {\n+\/\/                    boolean direct = handle.checkAccessModeThenIsDirect(ad);\n+\/\/                    if (direct && handle.vform.methodType_table[ad.type] == ad.symbolicMethodTypeErased) {\n@@ -730,2 +725,2 @@\n-\/\/                    handle.checkExactAccessMode(ad);\n-\/\/                    if (handle.isDirect() && handle.vform.methodType_table[ad.type] == ad.symbolicMethodTypeErased) {\n+\/\/                    boolean direct = handle.checkAccessModeThenIsDirect(ad);\n+\/\/                    if (direct && handle.vform.methodType_table[ad.type] == ad.symbolicMethodTypeErased) {\n@@ -733,1 +728,1 @@\n-\/\/                    } else if (handle.isDirect() && handle.vform.getMethodType_V(ad.type) == ad.symbolicMethodTypeErased) {\n+\/\/                    } else if (direct && handle.vform.getMethodType_V(ad.type) == ad.symbolicMethodTypeErased) {\n@@ -879,3 +874,0 @@\n-\/\/            List<String> LINK_TO_STATIC_ARGS_V = params.keySet().stream().\n-\/\/                    collect(toList());\n-\/\/            LINK_TO_STATIC_ARGS_V.add(\"handle.vform.getMemberName_V(ad.mode)\");\n@@ -914,2 +906,0 @@\n-\/\/                    replaceAll(\"<LINK_TO_STATIC_ARGS_V>\", LINK_TO_STATIC_ARGS_V.stream().\n-\/\/                            collect(joining(\", \"))).\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/VarHandles.java","additions":17,"deletions":27,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -40,1 +40,1 @@\n-    static class FieldInstanceReadOnly extends VarHandle {\n+    static sealed class FieldInstanceReadOnly extends VarHandle {\n@@ -392,1 +392,1 @@\n-    static class FieldStaticReadOnly extends VarHandle {\n+    static sealed class FieldStaticReadOnly extends VarHandle {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/X-VarHandle.java.template","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+ * @param <T> the type of the referent\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/PhantomReference.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+import jdk.internal.misc.Unsafe;\n@@ -45,0 +46,1 @@\n+ * @param <T> the type of the referent\n@@ -200,7 +202,2 @@\n-\n-        private static void ensureClassInitialized(Class<?> clazz) {\n-            try {\n-                Class.forName(clazz.getName(), true, clazz.getClassLoader());\n-            } catch (ClassNotFoundException e) {\n-                throw (Error) new NoClassDefFoundError(e.getMessage()).initCause(e);\n-            }\n+        ReferenceHandler(ThreadGroup g, String name) {\n+            super(g, null, name, 0, false);\n@@ -209,1 +206,1 @@\n-        static {\n+        public void run() {\n@@ -213,2 +210,1 @@\n-            ensureClassInitialized(Cleaner.class);\n-        }\n+            Unsafe.getUnsafe().ensureClassInitialized(Cleaner.class);\n@@ -216,5 +212,0 @@\n-        ReferenceHandler(ThreadGroup g, String name) {\n-            super(g, null, name, 0, false);\n-        }\n-\n-        public void run() {\n@@ -310,5 +301,4 @@\n-    static {\n-        ThreadGroup tg = Thread.currentThread().getThreadGroup();\n-        for (ThreadGroup tgn = tg;\n-             tgn != null;\n-             tg = tgn, tgn = tg.getParent());\n+    \/**\n+     * Start the Reference Handler thread as a daemon thread.\n+     *\/\n+    static void startReferenceHandlerThread(ThreadGroup tg) {\n@@ -322,0 +312,1 @@\n+    }\n@@ -323,0 +314,1 @@\n+    static {\n@@ -325,0 +317,10 @@\n+            @Override\n+            public void startThreads() {\n+                ThreadGroup tg = Thread.currentThread().getThreadGroup();\n+                for (ThreadGroup tgn = tg;\n+                     tgn != null;\n+                     tg = tgn, tgn = tg.getParent());\n+                Reference.startReferenceHandlerThread(tg);\n+                Finalizer.startFinalizerThread(tg);\n+            }\n+\n@@ -336,0 +338,5 @@\n+\n+            @Override\n+            public <T> ReferenceQueue<T> newNativeReferenceQueue() {\n+                return new NativeReferenceQueue<T>();\n+            }\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/Reference.java","additions":27,"deletions":20,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -63,0 +63,1 @@\n+ * @param <T> the type of the referent\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/SoftReference.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+ * @param <T> the type of the referent\n","filename":"src\/java.base\/share\/classes\/java\/lang\/ref\/WeakReference.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -64,1 +64,2 @@\n- * <p>The access flag constants are ordered by non-decreasing mask\n+ * @implSpec\n+ * The access flag constants are ordered by non-decreasing mask\n@@ -71,0 +72,12 @@\n+ * @apiNote\n+ * The JVM class file format has a new version defined for each new\n+ * {@linkplain Runtime.Version#feature() feature release}. A new class\n+ * file version may define new access flags or retire old ones. {@code\n+ * AccessFlag} is intended to model the set of access flags across\n+ * class file format versions. The range of versions an access flag is\n+ * recognized is not explicitly indicated in this API. See the current\n+ * <cite>The Java Virtual Machine Specification<\/cite> for\n+ * details. Unless otherwise indicated, access flags can be assumed to\n+ * be recognized in the {@linkplain Runtime#version() current\n+ * version}.\n+ *\n@@ -84,1 +97,1 @@\n-     * {@code 0x0001}.\n+     * <code>{@value \"0x%04x\" Modifier#PUBLIC}<\/code>.\n@@ -93,1 +106,1 @@\n-     * value of {@code 0x0002}.\n+     * value of <code>{@value \"0x%04x\" Modifier#PRIVATE}<\/code>.\n@@ -95,2 +108,1 @@\n-    PRIVATE(Modifier.PRIVATE, true,\n-            Set.of(Location.FIELD, Location.METHOD, Location.INNER_CLASS)),\n+    PRIVATE(Modifier.PRIVATE, true, Location.SET_FIELD_METHOD_INNER_CLASS),\n@@ -101,1 +113,1 @@\n-     * value of {@code 0x0004}.\n+     * value of <code>{@value \"0x%04x\" Modifier#PROTECTED}<\/code>.\n@@ -103,2 +115,1 @@\n-    PROTECTED(Modifier.PROTECTED, true,\n-              Set.of(Location.FIELD, Location.METHOD, Location.INNER_CLASS)),\n+    PROTECTED(Modifier.PROTECTED, true, Location.SET_FIELD_METHOD_INNER_CLASS),\n@@ -109,1 +120,1 @@\n-     * {@code 0x0008}.\n+     * <code>{@value \"0x%04x\" Modifier#STATIC}<\/code>.\n@@ -111,2 +122,1 @@\n-    STATIC(Modifier.STATIC, true,\n-           Set.of(Location.FIELD, Location.METHOD, Location.INNER_CLASS)),\n+    STATIC(Modifier.STATIC, true, Location.SET_FIELD_METHOD_INNER_CLASS),\n@@ -117,1 +127,1 @@\n-     * value of {@code 0x0010}.\n+     * value of <code>{@value \"0x%04x\" Modifier#FINAL}<\/code>.\n@@ -126,0 +136,4 @@\n+     *\n+     * @apiNote\n+     * In Java SE 8 and above, the JVM treats the {@code ACC_SUPER}\n+     * flag as set in every class file (JVMS {@jvms 4.1}).\n@@ -127,1 +141,1 @@\n-    SUPER(0x0000_0020, false, Set.of(Location.CLASS)),\n+    SUPER(0x0000_0020, false, Location.SET_CLASS),\n@@ -133,1 +147,1 @@\n-\/\/    IDENTITY(0x0000_0020, false, Set.of(Location.CLASS)),\n+\/\/    IDENTITY(0x0000_0020, false, Location.SET_CLASS),\n@@ -147,1 +161,1 @@\n-    TRANSITIVE(0x0000_0020, false, Set.of(Location.MODULE_REQUIRES)),\n+    TRANSITIVE(0x0000_0020, false, Location.SET_MODULE_REQUIRES),\n@@ -152,1 +166,1 @@\n-     * a mask value of {@code 0x0020}.\n+     * a mask value of <code>{@value \"0x%04x\" Modifier#SYNCHRONIZED}<\/code>.\n@@ -154,1 +168,1 @@\n-    SYNCHRONIZED(Modifier.SYNCHRONIZED, true, Set.of(Location.METHOD)),\n+    SYNCHRONIZED(Modifier.SYNCHRONIZED, true, Location.SET_METHOD),\n@@ -161,1 +175,1 @@\n-    STATIC_PHASE(0x0000_0040, false, Set.of(Location.MODULE_REQUIRES)),\n+    STATIC_PHASE(0x0000_0040, false, Location.SET_MODULE_REQUIRES),\n@@ -166,1 +180,1 @@\n-      * value of {@code 0x0040}.\n+      * value of <code>{@value \"0x%04x\" Modifier#VOLATILE}<\/code>.\n@@ -168,1 +182,1 @@\n-    VOLATILE(Modifier.VOLATILE, true, Set.of(Location.FIELD)),\n+    VOLATILE(Modifier.VOLATILE, true, Location.SET_FIELD),\n@@ -171,2 +185,2 @@\n-     * The access flag {@code ACC_BRIDGE} with a mask value of {@code\n-     * 0x0040}.\n+     * The access flag {@code ACC_BRIDGE} with a mask value of\n+     * <code>{@value \"0x%04x\" Modifier#BRIDGE}<\/code>\n@@ -175,1 +189,1 @@\n-    BRIDGE(0x0000_0040, false, Set.of(Location.METHOD)),\n+    BRIDGE(Modifier.BRIDGE, false, Location.SET_METHOD),\n@@ -180,1 +194,1 @@\n-     * mask value of {@code 0x0080}.\n+     * mask value of <code>{@value \"0x%04x\" Modifier#TRANSIENT}<\/code>.\n@@ -182,1 +196,1 @@\n-    TRANSIENT(Modifier.TRANSIENT, true, Set.of(Location.FIELD)),\n+    TRANSIENT(Modifier.TRANSIENT, true, Location.SET_FIELD),\n@@ -185,2 +199,2 @@\n-     * The access flag {@code ACC_VARARGS} with a mask value of {@code\n-     * 0x0080}.\n+     * The access flag {@code ACC_VARARGS} with a mask value of\n+     <code>{@value \"0x%04x\" Modifier#VARARGS}<\/code>.\n@@ -189,1 +203,1 @@\n-    VARARGS(0x0000_0080, false, Set.of(Location.METHOD)),\n+    VARARGS(Modifier.VARARGS, false, Location.SET_METHOD),\n@@ -193,2 +207,2 @@\n-     * modifier {@link Modifier#NATIVE native} with a mask value of {@code\n-     * 0x0100}.\n+     * modifier {@link Modifier#NATIVE native} with a mask value of\n+     * <code>{@value \"0x%04x\" Modifier#NATIVE}<\/code>.\n@@ -196,1 +210,1 @@\n-    NATIVE(Modifier.NATIVE, true, Set.of(Location.METHOD)),\n+    NATIVE(Modifier.NATIVE, true, Location.SET_METHOD),\n@@ -203,2 +217,1 @@\n-    INTERFACE(Modifier.INTERFACE, false,\n-              Set.of(Location.CLASS, Location.INNER_CLASS)),\n+    INTERFACE(Modifier.INTERFACE, false, Location.SET_CLASS_INNER_CLASS),\n@@ -209,1 +222,1 @@\n-     * value of {@code 0x0400}.\n+     * value of <code>{@value \"0x%04x\" Modifier#ABSTRACT}<\/code>.\n@@ -217,1 +230,6 @@\n-     * {@code 0x0800}.\n+     * <code>{@value \"0x%04x\" Modifier#STRICT}<\/code>.\n+     *\n+     * @apiNote\n+     * The {@code ACC_STRICT} access flag is defined for class file\n+     * major versions 46 through 60, inclusive (JVMS {@jvms 4.6}),\n+     * corresponding to Java SE 1.2 through 16.\n@@ -219,1 +237,1 @@\n-    STRICT(Modifier.STRICT, true, Set.of(Location.METHOD)),\n+    STRICT(Modifier.STRICT, true, Location.SET_METHOD),\n@@ -223,1 +241,1 @@\n-     * {@code 0x1000}.\n+     * <code>{@value \"0x%04x\" Modifier#SYNTHETIC}<\/code>.\n@@ -228,1 +246,1 @@\n-    SYNTHETIC(0x0000_1000, false,\n+    SYNTHETIC(Modifier.SYNTHETIC, false,\n@@ -236,1 +254,1 @@\n-     * {@code 0x2000}.\n+     * <code>{@value \"0x%04x\" Modifier#ANNOTATION}<\/code>.\n@@ -239,2 +257,1 @@\n-    ANNOTATION(0x0000_2000, false,\n-               Set.of(Location.CLASS, Location.INNER_CLASS)),\n+    ANNOTATION(Modifier.ANNOTATION, false, Location.SET_CLASS_INNER_CLASS),\n@@ -243,2 +260,2 @@\n-     * The access flag {@code ACC_ENUM} with a mask value of {@code\n-     * 0x4000}.\n+     * The access flag {@code ACC_ENUM} with a mask value of\n+     * <code>{@value \"0x%04x\" Modifier#ENUM}<\/code>.\n@@ -247,1 +264,1 @@\n-    ENUM(0x0000_4000, false,\n+    ENUM(Modifier.ENUM, false,\n@@ -252,1 +269,1 @@\n-     * {@code 0x8000}.\n+     * <code>{@value \"0x%04x\" Modifier#MANDATED}<\/code>.\n@@ -254,1 +271,1 @@\n-    MANDATED(0x0000_8000, false,\n+    MANDATED(Modifier.MANDATED, false,\n@@ -263,1 +280,1 @@\n-    MODULE(0x0000_8000, false, Set.of(Location.CLASS))\n+    MODULE(0x0000_8000, false, Location.SET_CLASS)\n@@ -269,2 +286,2 @@\n-    private int mask;\n-    private boolean sourceModifier;\n+    private final int mask;\n+    private final boolean sourceModifier;\n@@ -274,1 +291,1 @@\n-    private Set<Location> locations;\n+    private final Set<Location> locations;\n@@ -305,1 +322,1 @@\n-     * {@return a set of access flags for the given mask value\n+     * {@return an unmodifiable set of access flags for the given mask value\n@@ -393,0 +410,10 @@\n+        \/\/ Repeated sets of locations used by AccessFlag constants\n+        private static final Set<Location> SET_FIELD_METHOD_INNER_CLASS =\n+            Set.of(FIELD, METHOD, INNER_CLASS);\n+        private static final Set<Location> SET_METHOD = Set.of(METHOD);\n+        private static final Set<Location> SET_FIELD = Set.of(FIELD);\n+        private static final Set<Location> SET_CLASS = Set.of(CLASS);\n+        private static final Set<Location> SET_CLASS_INNER_CLASS =\n+            Set.of(CLASS, INNER_CLASS);\n+        private static final Set<Location> SET_MODULE_REQUIRES =\n+            Set.of(MODULE_REQUIRES);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/AccessFlag.java","additions":78,"deletions":51,"binary":false,"changes":129,"status":"modified"},{"patch":"@@ -485,1 +485,1 @@\n-            if (!declaringClass.isAssignableFrom(obj.getClass())) {\n+            if (!declaringClass.isInstance(obj)) {\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/AccessibleObject.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,0 +38,1 @@\n+import sun.reflect.generics.repository.GenericDeclRepository;\n@@ -248,1 +249,1 @@\n-          return (TypeVariable<Constructor<T>>[])new TypeVariable[0];\n+          return (TypeVariable<Constructor<T>>[])GenericDeclRepository.EMPTY_TYPE_VARS;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Constructor.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -217,5 +217,0 @@\n-     * @implSpec\n-     * Map this executable's {@linkplain #getModifiers() modifiers} to\n-     * access flags using {@link AccessFlag#maskToAccessFlags} for a\n-     * {@linkplain AccessFlag.Location#METHOD method location}\n-     *\n@@ -228,1 +223,2 @@\n-        return AccessFlag.maskToAccessFlags(getModifiers(), AccessFlag.Location.METHOD);\n+        return AccessFlag.maskToAccessFlags(getModifiers(),\n+                                            AccessFlag.Location.METHOD);\n@@ -277,3 +273,1 @@\n-    public int getParameterCount() {\n-        throw new AbstractMethodError();\n-    }\n+    public abstract int getParameterCount();\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Executable.java","additions":4,"deletions":10,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,0 +39,1 @@\n+import sun.reflect.generics.repository.GenericDeclRepository;\n@@ -258,1 +259,1 @@\n-            return (TypeVariable<Method>[])new TypeVariable[0];\n+            return (TypeVariable<Method>[])GenericDeclRepository.EMPTY_TYPE_VARS;\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Method.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -352,0 +352,1 @@\n+     * @see AccessFlag#STRICT\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Modifier.java","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -498,4 +498,9 @@\n-        private static Class<?> defineProxyClass(Module m, List<Class<?>> interfaces) {\n-            String proxyPkg = null;     \/\/ package to define proxy class in\n-            int accessFlags = Modifier.PUBLIC | Modifier.FINAL | Modifier.IDENTITY;\n-            boolean nonExported = false;\n+        private record ProxyClassContext(Module module, String packageName, int accessFlags) {\n+            private ProxyClassContext {\n+                if (module.isNamed()) {\n+                    if (packageName.isEmpty()) {\n+                        \/\/ Per JLS 7.4.2, unnamed package can only exist in unnamed modules.\n+                        \/\/ This means a package-private superinterface exist in the unnamed\n+                        \/\/ package of a named module.\n+                        throw new InternalError(\"Unnamed package cannot be added to \" + module);\n+                    }\n@@ -503,15 +508,7 @@\n-            \/*\n-             * Record the package of a non-public proxy interface so that the\n-             * proxy class will be defined in the same package.  Verify that\n-             * all non-public proxy interfaces are in the same package.\n-             *\/\n-            for (Class<?> intf : interfaces) {\n-                int flags = intf.getModifiers();\n-                if (!Modifier.isPublic(flags)) {\n-                    accessFlags = Modifier.FINAL | Modifier.IDENTITY;  \/\/ non-public, final\n-                    String pkg = intf.getPackageName();\n-                    if (proxyPkg == null) {\n-                        proxyPkg = pkg;\n-                    } else if (!pkg.equals(proxyPkg)) {\n-                        throw new IllegalArgumentException(\n-                                \"non-public interfaces from different packages\");\n+                    if (!module.getDescriptor().packages().contains(packageName)) {\n+                        throw new InternalError(packageName + \" not exist in \" + module.getName());\n+                    }\n+\n+                    if (!module.isOpen(packageName, Proxy.class.getModule())) {\n+                        \/\/ Required for default method invocation\n+                        throw new InternalError(packageName + \" not open to \" + Proxy.class.getModule());\n@@ -520,3 +517,3 @@\n-                    if (!intf.getModule().isExported(intf.getPackageName())) {\n-                        \/\/ module-private types\n-                        nonExported = true;\n+                    if (Modifier.isPublic(accessFlags)) {\n+                        \/\/ All proxy superinterfaces are public, must be in named dynamic module\n+                        throw new InternalError(\"public proxy in unnamed module: \" + module);\n@@ -525,15 +522,2 @@\n-            }\n-            if (proxyPkg == null) {\n-                \/\/ all proxy interfaces are public and exported\n-                if (!m.isNamed())\n-                    throw new InternalError(\"unnamed module: \" + m);\n-                proxyPkg = nonExported ? PROXY_PACKAGE_PREFIX + \".\" + m.getName()\n-                                       : m.getName();\n-            } else if (proxyPkg.isEmpty() && m.isNamed()) {\n-                throw new IllegalArgumentException(\n-                        \"Unnamed package cannot be added to \" + m);\n-            }\n-\n-            if (m.isNamed()) {\n-                if (!m.getDescriptor().packages().contains(proxyPkg)) {\n-                    throw new InternalError(proxyPkg + \" not exist in \" + m.getName());\n+                if ((accessFlags & ~(Modifier.PUBLIC | Modifier.IDENTITY)) != 0) {\n+                    throw new InternalError(\"proxy access flags must be Modifier.PUBLIC or 0\");\n@@ -543,0 +527,1 @@\n+        }\n@@ -544,0 +529,1 @@\n+        private static Class<?> defineProxyClass(ProxyClassContext context, List<Class<?>> interfaces) {\n@@ -548,1 +534,1 @@\n-            String proxyName = proxyPkg.isEmpty()\n+            String proxyName = context.packageName().isEmpty()\n@@ -550,1 +536,1 @@\n-                                    : proxyPkg + \".\" + proxyClassNamePrefix + num;\n+                                    : context.packageName() + \".\" + proxyClassNamePrefix + num;\n@@ -552,2 +538,2 @@\n-            ClassLoader loader = getLoader(m);\n-            trace(proxyName, m, loader, interfaces);\n+            ClassLoader loader = getLoader(context.module());\n+            trace(proxyName, context.module(), loader, interfaces);\n@@ -558,1 +544,2 @@\n-            byte[] proxyClassFile = ProxyGenerator.generateProxyClass(loader, proxyName, interfaces, accessFlags);\n+            byte[] proxyClassFile = ProxyGenerator.generateProxyClass(loader, proxyName, interfaces,\n+                                                                      context.accessFlags() | Modifier.FINAL | Modifier.IDENTITY);\n@@ -578,1 +565,1 @@\n-         * {@link #defineProxyClass(Module, List)}\n+         * {@link #defineProxyClass(ProxyClassContext, List)}\n@@ -634,1 +621,1 @@\n-        private final Module module;\n+        private final ProxyClassContext context;\n@@ -651,2 +638,2 @@\n-            this.module = mapToModule(loader, interfaces, refTypes);\n-            assert getLoader(module) == loader;\n+            this.context = proxyClassContext(loader, interfaces, refTypes);\n+            assert getLoader(context.module()) == loader;\n@@ -670,2 +657,1 @@\n-            Class<?> proxyClass = defineProxyClass(module, interfaces);\n-            assert !module.isNamed() || module.isOpen(proxyClass.getPackageName(), Proxy.class.getModule());\n+            Class<?> proxyClass = defineProxyClass(context, interfaces);\n@@ -771,1 +757,2 @@\n-         * Returns the module that the generated proxy class belongs to.\n+         * Returns the context for the generated proxy class, including the\n+         * module and the package it belongs to and whether it is package-private.\n@@ -774,1 +761,1 @@\n-         * is in the same module of the package-private interface.\n+         * is in the same package and module as the package-private interface.\n@@ -788,3 +775,3 @@\n-        private static Module mapToModule(ClassLoader loader,\n-                                          List<Class<?>> interfaces,\n-                                          Set<Class<?>> refTypes) {\n+        private static ProxyClassContext proxyClassContext(ClassLoader loader,\n+                                                           List<Class<?>> interfaces,\n+                                                           Set<Class<?>> refTypes) {\n@@ -792,0 +779,2 @@\n+            boolean nonExported = false;\n+\n@@ -796,0 +785,5 @@\n+                } else {\n+                    if (!intf.getModule().isExported(intf.getPackageName())) {\n+                        \/\/ module-private types\n+                        nonExported = true;\n+                    }\n@@ -841,1 +835,1 @@\n-                return targetModule;\n+                return new ProxyClassContext(targetModule, targetPackageName, 0);\n@@ -855,1 +849,4 @@\n-            return targetModule;\n+\n+            var pkgName = nonExported ? PROXY_PACKAGE_PREFIX + '.' + targetModule.getName()\n+                                      : targetModule.getName();\n+            return new ProxyClassContext(targetModule, pkgName, Modifier.PUBLIC);\n@@ -901,1 +898,1 @@\n-         * Define a dynamic module with a packge named $MODULE which\n+         * Define a dynamic module with a package named $MODULE which\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/Proxy.java","additions":54,"deletions":57,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,1 @@\n+import sun.invoke.util.Wrapper;\n@@ -44,1 +45,0 @@\n-import java.util.HashMap;\n@@ -80,0 +80,1 @@\n+    private static final String LJL_CLASSLOADER = \"Ljava\/lang\/ClassLoader;\";\n@@ -609,0 +610,7 @@\n+        \/\/ Put ClassLoader at local variable index 0, used by\n+        \/\/ Class.forName(String, boolean, ClassLoader) calls\n+        mv.visitLdcInsn(Type.getObjectType(dotToSlash(className)));\n+        mv.visitMethodInsn(INVOKEVIRTUAL, JL_CLASS,\n+                \"getClassLoader\", \"()\" + LJL_CLASSLOADER, false);\n+        mv.visitVarInsn(ASTORE, 0);\n+\n@@ -864,15 +872,1 @@\n-                if (type == int.class ||\n-                        type == boolean.class ||\n-                        type == byte.class ||\n-                        type == char.class ||\n-                        type == short.class) {\n-                    mv.visitVarInsn(ILOAD, slot);\n-                } else if (type == long.class) {\n-                    mv.visitVarInsn(LLOAD, slot);\n-                } else if (type == float.class) {\n-                    mv.visitVarInsn(FLOAD, slot);\n-                } else if (type == double.class) {\n-                    mv.visitVarInsn(DLOAD, slot);\n-                } else {\n-                    throw new AssertionError();\n-                }\n+                mv.visitVarInsn(prim.loadOpcode, slot);\n@@ -900,15 +894,1 @@\n-                if (type == int.class ||\n-                        type == boolean.class ||\n-                        type == byte.class ||\n-                        type == char.class ||\n-                        type == short.class) {\n-                    mv.visitInsn(IRETURN);\n-                } else if (type == long.class) {\n-                    mv.visitInsn(LRETURN);\n-                } else if (type == float.class) {\n-                    mv.visitInsn(FRETURN);\n-                } else if (type == double.class) {\n-                    mv.visitInsn(DRETURN);\n-                } else {\n-                    throw new AssertionError();\n-                }\n+                mv.visitInsn(prim.returnOpcode);\n@@ -927,1 +907,2 @@\n-         * the Method object for this proxy method.\n+         * the Method object for this proxy method. A class loader is\n+         * anticipated at local variable index 0.\n@@ -976,0 +957,1 @@\n+         * A class loader is anticipated at local variable index 0.\n@@ -979,0 +961,2 @@\n+            mv.visitInsn(ICONST_0); \/\/ false\n+            mv.visitVarInsn(ALOAD, 0); \/\/ classLoader\n@@ -981,1 +965,3 @@\n-                    \"forName\", \"(Ljava\/lang\/String;)Ljava\/lang\/Class;\", false);\n+                    \"forName\",\n+                    \"(Ljava\/lang\/String;Z\" + LJL_CLASSLOADER + \")Ljava\/lang\/Class;\",\n+                    false);\n@@ -987,0 +973,1 @@\n+\n@@ -1014,2 +1001,2 @@\n-     * A PrimitiveTypeInfo object contains assorted information about\n-     * a primitive type in its public fields.  The struct for a particular\n+     * A PrimitiveTypeInfo object contains bytecode-related information about\n+     * a primitive type in its instance fields. The struct for a particular\n@@ -1018,14 +1005,9 @@\n-    private static class PrimitiveTypeInfo {\n-\n-        private static Map<Class<?>, PrimitiveTypeInfo> table = new HashMap<>();\n-\n-        static {\n-            add(byte.class, Byte.class);\n-            add(char.class, Character.class);\n-            add(double.class, Double.class);\n-            add(float.class, Float.class);\n-            add(int.class, Integer.class);\n-            add(long.class, Long.class);\n-            add(short.class, Short.class);\n-            add(boolean.class, Boolean.class);\n-        }\n+    private enum PrimitiveTypeInfo {\n+        BYTE(byte.class, ILOAD, IRETURN),\n+        CHAR(char.class, ILOAD, IRETURN),\n+        DOUBLE(double.class, DLOAD, DRETURN),\n+        FLOAT(float.class, FLOAD, FRETURN),\n+        INT(int.class, ILOAD, IRETURN),\n+        LONG(long.class, LLOAD, LRETURN),\n+        SHORT(short.class, ILOAD, IRETURN),\n+        BOOLEAN(boolean.class, ILOAD, IRETURN);\n@@ -1034,1 +1016,1 @@\n-         * name of corresponding wrapper class\n+         * internal name of corresponding wrapper class\n@@ -1036,1 +1018,1 @@\n-        private String wrapperClassName;\n+        private final String wrapperClassName;\n@@ -1040,1 +1022,1 @@\n-        private String wrapperValueOfDesc;\n+        private final String wrapperValueOfDesc;\n@@ -1044,1 +1026,1 @@\n-        private String unwrapMethodName;\n+        private final String unwrapMethodName;\n@@ -1048,1 +1030,9 @@\n-        private String unwrapMethodDesc;\n+        private final String unwrapMethodDesc;\n+        \/**\n+         * Load opcode used by this primitive\n+         *\/\n+        private final int loadOpcode;\n+        \/**\n+         * Return opcode used by this primitive\n+         *\/\n+        private final int returnOpcode;\n@@ -1050,1 +1040,1 @@\n-        private PrimitiveTypeInfo(Class<?> primitiveClass, Class<?> wrapperClass) {\n+        PrimitiveTypeInfo(Class<?> primitiveClass, int loadOpcode, int returnOpcode) {\n@@ -1052,0 +1042,1 @@\n+            assert returnOpcode - IRETURN == loadOpcode - ILOAD;\n@@ -1053,7 +1044,5 @@\n-            \/**\n-             * \"base type\" used in various descriptors (see JVMS section 4.3.2)\n-             *\/\n-            String baseTypeString =\n-                    Array.newInstance(primitiveClass, 0)\n-                            .getClass().getName().substring(1);\n-            wrapperClassName = dotToSlash(wrapperClass.getName());\n+            Wrapper wrapper = Wrapper.forPrimitiveType(primitiveClass);\n+            \/\/ single-char BaseType descriptor (see JVMS section 4.3.2)\n+            String baseTypeString = wrapper.basicTypeString();\n+            var wrapperType = wrapper.wrapperType();\n+            wrapperClassName = dotToSlash(wrapperType.getName());\n@@ -1061,1 +1050,1 @@\n-                    \"(\" + baseTypeString + \")L\" + wrapperClassName + \";\";\n+                    \"(\" + baseTypeString + \")\" + wrapperType.descriptorString();\n@@ -1064,5 +1053,2 @@\n-        }\n-\n-        private static void add(Class<?> primitiveClass, Class<?> wrapperClass) {\n-            table.put(primitiveClass,\n-                    new PrimitiveTypeInfo(primitiveClass, wrapperClass));\n+            this.loadOpcode = loadOpcode;\n+            this.returnOpcode = returnOpcode;\n@@ -1072,1 +1058,10 @@\n-            return table.get(cl);\n+            \/\/ Uses if chain for speed: 8284880\n+            if (cl == int.class)     return INT;\n+            if (cl == long.class)    return LONG;\n+            if (cl == boolean.class) return BOOLEAN;\n+            if (cl == short.class)   return SHORT;\n+            if (cl == byte.class)    return BYTE;\n+            if (cl == char.class)    return CHAR;\n+            if (cl == float.class)   return FLOAT;\n+            if (cl == double.class)  return DOUBLE;\n+            throw new AssertionError(cl);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/reflect\/ProxyGenerator.java","additions":65,"deletions":70,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -338,1 +338,1 @@\n-     * @return chunks that wont surpass the maximum number of slots StringConcatFactory::makeConcatWithConstants can chew\n+     * @return chunks that won't surpass the maximum number of slots StringConcatFactory::makeConcatWithConstants can chew\n","filename":"src\/java.base\/share\/classes\/java\/lang\/runtime\/ObjectMethods.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -122,0 +123,3 @@\n+    \/** The maximum size of array to allocate. *\/\n+    private static final int MAX_BUFFER_SIZE = 1024 * 1024;\n+\n@@ -135,0 +139,3 @@\n+    \/** The offset in bytes of the ClassFile's access_flags field. *\/\n+    public final int header;\n+\n@@ -181,3 +188,0 @@\n-    \/** The offset in bytes of the ClassFile's access_flags field. *\/\n-    public final int header;\n-\n@@ -225,1 +229,1 @@\n-        if (checkClassVersion && readShort(classFileOffset + 6) > Opcodes.V19) {\n+        if (checkClassVersion && readShort(classFileOffset + 6) > Opcodes.V20) {\n@@ -339,0 +343,1 @@\n+    @SuppressWarnings(\"PMD.UseTryWithResources\")\n@@ -344,0 +349,1 @@\n+        int bufferSize = computeBufferSize(inputStream);\n@@ -345,1 +351,1 @@\n-            byte[] data = new byte[INPUT_STREAM_DATA_CHUNK_SIZE];\n+            byte[] data = new byte[bufferSize];\n@@ -347,1 +353,2 @@\n-            while ((bytesRead = inputStream.read(data, 0, data.length)) != -1) {\n+            int readCount = 0;\n+            while ((bytesRead = inputStream.read(data, 0, bufferSize)) != -1) {\n@@ -349,0 +356,1 @@\n+                readCount++;\n@@ -351,0 +359,3 @@\n+            if (readCount == 1) {\n+                return data;\n+            }\n@@ -359,0 +370,13 @@\n+    private static int computeBufferSize(final InputStream inputStream) throws IOException {\n+        int expectedLength = inputStream.available();\n+        \/*\n+          * Some implementations can return 0 while holding available data (e.g. new\n+          * FileInputStream(\"\/proc\/a_file\")). Also in some pathological cases a very small number might\n+          * be returned, and in this case we use a default size.\n+          *\/\n+        if (expectedLength < 256) {\n+            return INPUT_STREAM_DATA_CHUNK_SIZE;\n+        }\n+        return Math.min(expectedLength, MAX_BUFFER_SIZE);\n+    }\n+\n@@ -449,1 +473,0 @@\n-    @SuppressWarnings(\"deprecation\")\n@@ -713,1 +736,1 @@\n-            int currentPermittedSubclassOffset = permittedSubclassesOffset + 2;\n+            int currentPermittedSubclassesOffset = permittedSubclassesOffset + 2;\n@@ -715,3 +738,3 @@\n-                classVisitor.visitPermittedSubclassExperimental(\n-                        readClass(currentPermittedSubclassOffset, charBuffer));\n-                currentPermittedSubclassOffset += 2;\n+                classVisitor.visitPermittedSubclass(\n+                        readClass(currentPermittedSubclassesOffset, charBuffer));\n+                currentPermittedSubclassesOffset += 2;\n@@ -871,1 +894,1 @@\n-        \/\/ Read the  'provides_count' and 'provides' fields.\n+        \/\/ Read the 'provides_count' and 'provides' fields.\n@@ -3012,1 +3035,1 @@\n-                        readElementValue(annotationVisitor, currentOffset, \/* named = *\/ null, charBuffer);\n+                        readElementValue(annotationVisitor, currentOffset, \/* elementName= *\/ null, charBuffer);\n@@ -3491,1 +3514,0 @@\n-        int[] currentBootstrapMethodOffsets = null;\n@@ -3499,1 +3521,1 @@\n-                currentBootstrapMethodOffsets = new int[readUnsignedShort(currentAttributeOffset)];\n+                int[] result = new int[readUnsignedShort(currentAttributeOffset)];\n@@ -3502,2 +3524,2 @@\n-                for (int j = 0; j < currentBootstrapMethodOffsets.length; ++j) {\n-                    currentBootstrapMethodOffsets[j] = currentBootstrapMethodOffset;\n+                for (int j = 0; j < result.length; ++j) {\n+                    result[j] = currentBootstrapMethodOffset;\n@@ -3509,1 +3531,1 @@\n-                return currentBootstrapMethodOffsets;\n+                return result;\n@@ -3868,0 +3890,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/ClassReader.java","additions":41,"deletions":18,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -256,0 +257,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/Constants.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -615,1 +616,1 @@\n-            outputStackTop -= elements;\n+            outputStackTop -= (short) elements;\n@@ -619,1 +620,1 @@\n-            outputStackStart -= elements - outputStackTop;\n+            outputStackStart -= (short) (elements - outputStackTop);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/Frame.java","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -503,1 +504,2 @@\n-      * #COMPUTE_INSERTED_FRAMES}, {@link #COMPUTE_MAX_STACK_AND_LOCAL} or {@link #COMPUTE_NOTHING}.\n+      * #COMPUTE_INSERTED_FRAMES}, {@link COMPUTE_MAX_STACK_AND_LOCAL_FROM_FRAMES}, {@link\n+      * #COMPUTE_MAX_STACK_AND_LOCAL} or {@link #COMPUTE_NOTHING}.\n@@ -629,1 +631,1 @@\n-        super(\/* latest api = *\/ Opcodes.ASM8);\n+        super(\/* latest api = *\/ Opcodes.ASM9);\n@@ -941,1 +943,1 @@\n-    public void visitVarInsn(final int opcode, final int var) {\n+    public void visitVarInsn(final int opcode, final int varIndex) {\n@@ -944,1 +946,1 @@\n-        if (var < 4 && opcode != Opcodes.RET) {\n+        if (varIndex < 4 && opcode != Opcodes.RET) {\n@@ -947,1 +949,1 @@\n-                optimizedOpcode = Constants.ILOAD_0 + ((opcode - Opcodes.ILOAD) << 2) + var;\n+                optimizedOpcode = Constants.ILOAD_0 + ((opcode - Opcodes.ILOAD) << 2) + varIndex;\n@@ -949,1 +951,1 @@\n-                optimizedOpcode = Constants.ISTORE_0 + ((opcode - Opcodes.ISTORE) << 2) + var;\n+                optimizedOpcode = Constants.ISTORE_0 + ((opcode - Opcodes.ISTORE) << 2) + varIndex;\n@@ -952,2 +954,2 @@\n-        } else if (var >= 256) {\n-            code.putByte(Constants.WIDE).put12(opcode, var);\n+        } else if (varIndex >= 256) {\n+            code.putByte(Constants.WIDE).put12(opcode, varIndex);\n@@ -955,1 +957,1 @@\n-            code.put11(opcode, var);\n+            code.put11(opcode, varIndex);\n@@ -960,1 +962,1 @@\n-                currentBasicBlock.frame.execute(opcode, var, null, null);\n+                currentBasicBlock.frame.execute(opcode, varIndex, null, null);\n@@ -982,1 +984,1 @@\n-                currentMaxLocals = var + 2;\n+                currentMaxLocals = varIndex + 2;\n@@ -984,1 +986,1 @@\n-                currentMaxLocals = var + 1;\n+                currentMaxLocals = varIndex + 1;\n@@ -1251,1 +1253,1 @@\n-                    currentBasicBlock.flags |= (label.flags & Label.FLAG_JUMP_TARGET);\n+                    currentBasicBlock.flags |= (short) (label.flags & Label.FLAG_JUMP_TARGET);\n@@ -1267,1 +1269,1 @@\n-                    lastBasicBlock.flags |= (label.flags & Label.FLAG_JUMP_TARGET);\n+                    lastBasicBlock.flags |= (short) (label.flags & Label.FLAG_JUMP_TARGET);\n@@ -1347,1 +1349,1 @@\n-    public void visitIincInsn(final int var, final int increment) {\n+    public void visitIincInsn(final int varIndex, final int increment) {\n@@ -1350,2 +1352,2 @@\n-        if ((var > 255) || (increment > 127) || (increment < -128)) {\n-            code.putByte(Constants.WIDE).put12(Opcodes.IINC, var).putShort(increment);\n+        if ((varIndex > 255) || (increment > 127) || (increment < -128)) {\n+            code.putByte(Constants.WIDE).put12(Opcodes.IINC, varIndex).putShort(increment);\n@@ -1353,1 +1355,1 @@\n-            code.putByte(Opcodes.IINC).put11(var, increment);\n+            code.putByte(Opcodes.IINC).put11(varIndex, increment);\n@@ -1358,1 +1360,1 @@\n-            currentBasicBlock.frame.execute(Opcodes.IINC, var, null, null);\n+            currentBasicBlock.frame.execute(Opcodes.IINC, varIndex, null, null);\n@@ -1361,1 +1363,1 @@\n-            int currentMaxLocals = var + 1;\n+            int currentMaxLocals = varIndex + 1;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/MethodWriter.java","additions":22,"deletions":20,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -82,8 +83,1 @@\n-\n-    \/**\n-      * <i>Experimental, use at your own risk. This field will be renamed when it becomes stable, this\n-      * will break existing code using it. Only code compiled with --enable-preview can use this.<\/i>\n-      *\n-      * @deprecated This API is experimental.\n-      *\/\n-    @Deprecated int ASM9_EXPERIMENTAL = 1 << 24 | 9 << 16 | 0 << 8;\n+    int ASM9 = 9 << 16 | 0 << 8;\n@@ -166,1 +160,1 @@\n-      *     visitNewStuf(arg | SOURCE_DEPRECATED, ...);\n+      *     visitNewStuff(arg | SOURCE_DEPRECATED, ...);\n@@ -188,1 +182,1 @@\n-      *   <li>call visitOldSuff: in the call to super.visitOldStuff, the source is set to\n+      *   <li>call visitOldStuff: in the call to super.visitOldStuff, the source is set to\n@@ -318,0 +312,1 @@\n+    int V20 = 0 << 16 | 64;\n@@ -599,0 +594,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/Opcodes.java","additions":6,"deletions":10,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -479,1 +480,1 @@\n-                throw new IllegalArgumentException();\n+                throw new IllegalArgumentException(\"Invalid descriptor: \" + descriptorBuffer);\n@@ -938,0 +939,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/Type.java","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -150,1 +151,1 @@\n-        this(\/* latest api = *\/ Opcodes.ASM8, owner, access, name, descriptor, methodVisitor);\n+        this(\/* latest api = *\/ Opcodes.ASM9, owner, access, name, descriptor, methodVisitor);\n@@ -159,3 +160,2 @@\n-      * @param api the ASM API version implemented by this visitor. Must be one of {@link\n-      *     Opcodes#ASM4}, {@link Opcodes#ASM5}, {@link Opcodes#ASM6}, {@link Opcodes#ASM7} or {@link\n-      *     Opcodes#ASM8}.\n+      * @param api the ASM API version implemented by this visitor. Must be one of the {@code\n+      *     ASM}<i>x<\/i> values in {@link Opcodes}.\n@@ -277,2 +277,2 @@\n-    public void visitVarInsn(final int opcode, final int var) {\n-        super.visitVarInsn(opcode, var);\n+    public void visitVarInsn(final int opcode, final int varIndex) {\n+        super.visitVarInsn(opcode, varIndex);\n@@ -284,2 +284,2 @@\n-        maxLocals = Math.max(maxLocals, var + (isLongOrDouble ? 2 : 1));\n-        execute(opcode, var, null);\n+        maxLocals = Math.max(maxLocals, varIndex + (isLongOrDouble ? 2 : 1));\n+        execute(opcode, varIndex, null);\n@@ -433,4 +433,4 @@\n-    public void visitIincInsn(final int var, final int increment) {\n-        super.visitIincInsn(var, increment);\n-        maxLocals = Math.max(maxLocals, var + 1);\n-        execute(Opcodes.IINC, var, null);\n+    public void visitIincInsn(final int varIndex, final int increment) {\n+        super.visitIincInsn(varIndex, increment);\n+        maxLocals = Math.max(maxLocals, varIndex + 1);\n+        execute(Opcodes.IINC, varIndex, null);\n@@ -942,0 +942,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/commons\/AnalyzerAdapter.java","additions":13,"deletions":12,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -295,0 +296,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/commons\/Method.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -194,0 +195,2 @@\n+            boolean isFieldHandle = handle.getTag() <= Opcodes.H_PUTSTATIC;\n+\n@@ -197,4 +200,4 @@\n-                    mapMethodName(handle.getOwner(), handle.getName(), handle.getDesc()),\n-                    handle.getTag() <= Opcodes.H_PUTSTATIC\n-                            ? mapDesc(handle.getDesc())\n-                            : mapMethodDesc(handle.getDesc()),\n+                    isFieldHandle\n+                            ? mapFieldName(handle.getOwner(), handle.getName(), handle.getDesc())\n+                            : mapMethodName(handle.getOwner(), handle.getName(), handle.getDesc()),\n+                    isFieldHandle ? mapDesc(handle.getDesc()) : mapMethodDesc(handle.getDesc()),\n@@ -270,0 +273,12 @@\n+    \/**\n+      * Maps an annotation attribute name. The default implementation of this method returns the given\n+      * name, unchanged. Subclasses can override.\n+      *\n+      * @param descriptor the descriptor of the annotation class.\n+      * @param name the name of the annotation attribute.\n+      * @return the new name of the annotation attribute.\n+      *\/\n+    public String mapAnnotationAttributeName(final String descriptor, final String name) {\n+        return name;\n+    }\n+\n@@ -380,0 +395,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/commons\/Remapper.java","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -284,0 +285,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/signature\/SignatureReader.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -73,0 +74,1 @@\n+import jdk.internal.org.objectweb.asm.ClassWriter;\n@@ -195,1 +197,1 @@\n-        this(classVisitor, true);\n+        this(classVisitor, \/* checkDataFlow = *\/ true);\n@@ -203,2 +205,1 @@\n-      * @param checkDataFlow whether to perform basic data flow checks. This option requires valid\n-      *     maxLocals and maxStack values.\n+      * @param checkDataFlow whether to perform basic data flow checks.\n@@ -208,1 +209,1 @@\n-        this(\/* latest api = *\/ Opcodes.ASM8, classVisitor, checkDataFlow);\n+        this(\/* latest api = *\/ Opcodes.ASM9, classVisitor, checkDataFlow);\n@@ -217,3 +218,2 @@\n-      * @param api the ASM API version implemented by this visitor. Must be one of {@link\n-      *     Opcodes#ASM4}, {@link Opcodes#ASM5}, {@link Opcodes#ASM6}, {@link Opcodes#ASM7} or {@link\n-      *     Opcodes#ASM8}.\n+      * @param api the ASM API version implemented by this visitor. Must be one of the {@code\n+      *     ASM}<i>x<\/i> values in {@link Opcodes}.\n@@ -222,2 +222,1 @@\n-      *     not perform any data flow check (see {@link CheckMethodAdapter}). This option requires\n-      *     valid maxLocals and maxStack values.\n+      *     not perform any data flow check (see {@link CheckMethodAdapter}).\n@@ -356,8 +355,1 @@\n-    \/**\n-      * <b>Experimental, use at your own risk.<\/b>.\n-      *\n-      * @param permittedSubclass the internal name of a permitted subclass.\n-      * @deprecated this API is experimental.\n-      *\/\n-    @Deprecated\n-    public void visitPermittedSubclassExperimental(final String permittedSubclass) {\n+    public void visitPermittedSubclass(final String permittedSubclass) {\n@@ -367,1 +359,1 @@\n-        super.visitPermittedSubclassExperimental(permittedSubclass);\n+        super.visitPermittedSubclass(permittedSubclass);\n@@ -471,1 +463,2 @@\n-        checkAccess(\n+        checkMethodAccess(\n+                version,\n@@ -501,0 +494,2 @@\n+        MethodVisitor methodVisitor =\n+                super.visitMethod(access, name, descriptor, signature, exceptions);\n@@ -502,0 +497,4 @@\n+            if (cv instanceof ClassWriter) {\n+                methodVisitor =\n+                        new CheckMethodAdapter.MethodWriterWrapper(api, (ClassWriter) cv, methodVisitor);\n+            }\n@@ -503,7 +502,1 @@\n-                    new CheckMethodAdapter(\n-                            api,\n-                            access,\n-                            name,\n-                            descriptor,\n-                            super.visitMethod(access, name, descriptor, signature, exceptions),\n-                            labelInsnIndices);\n+                    new CheckMethodAdapter(api, access, name, descriptor, methodVisitor, labelInsnIndices);\n@@ -511,5 +504,1 @@\n-            checkMethodAdapter =\n-                    new CheckMethodAdapter(\n-                            api,\n-                            super.visitMethod(access, name, descriptor, signature, exceptions),\n-                            labelInsnIndices);\n+            checkMethodAdapter = new CheckMethodAdapter(api, methodVisitor, labelInsnIndices);\n@@ -596,0 +585,17 @@\n+    \/**\n+      * Checks that the given access flags do not contain invalid flags for a method. This method also\n+      * checks that mutually incompatible flags are not set simultaneously.\n+      *\n+      * @param version the class version.\n+      * @param access the method access flags to be checked.\n+      * @param possibleAccess the valid access flags.\n+      *\/\n+    private static void checkMethodAccess(\n+            final int version, final int access, final int possibleAccess) {\n+        checkAccess(access, possibleAccess);\n+        if ((version & 0xFFFF) < Opcodes.V17\n+                && Integer.bitCount(access & (Opcodes.ACC_STRICT | Opcodes.ACC_ABSTRACT)) > 1) {\n+            throw new IllegalArgumentException(\"strictfp and abstract are mutually exclusive: \" + access);\n+        }\n+    }\n+\n@@ -989,1 +995,1 @@\n-                throw new AssertionError();\n+                break;\n@@ -991,1 +997,1 @@\n-        if ((typeRef & ~mask) != 0) {\n+        if (mask == 0 || (typeRef & ~mask) != 0) {\n@@ -1042,3 +1048,4 @@\n-            InputStream inputStream =\n-                    new FileInputStream(args[0]); \/\/ NOPMD(AvoidFileStream): can't fix for 1.5 compatibility\n-            classReader = new ClassReader(inputStream);\n+            \/\/ Can't fix PMD warning for 1.5 compatibility.\n+            try (InputStream inputStream = new FileInputStream(args[0])) { \/\/ NOPMD(AvoidFileStream)\n+                classReader = new ClassReader(inputStream);\n+            }\n@@ -1073,1 +1080,0 @@\n-    @SuppressWarnings(\"deprecation\")\n@@ -1081,1 +1087,1 @@\n-                new CheckClassAdapter(Opcodes.ASM9_EXPERIMENTAL, classNode, false) {},\n+                new CheckClassAdapter(\/*latest*\/ Opcodes.ASM9, classNode, false) {},\n@@ -1160,1 +1166,5 @@\n-            return name.substring(lastSlashIndex + 1, endIndex);\n+            int lastBracketIndex = name.lastIndexOf('[');\n+            if (lastBracketIndex == -1) {\n+                return name.substring(lastSlashIndex + 1, endIndex);\n+            }\n+            return name.substring(0, lastBracketIndex + 1) + name.substring(lastSlashIndex + 1, endIndex);\n@@ -1164,0 +1174,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/util\/CheckClassAdapter.java","additions":50,"deletions":39,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+\n@@ -72,0 +73,1 @@\n+import jdk.internal.org.objectweb.asm.ClassWriter;\n@@ -385,1 +387,1 @@\n-        this(methodvisitor, new HashMap<Label, Integer>());\n+        this(methodvisitor, new HashMap<>());\n@@ -401,1 +403,1 @@\n-        this(\/* latest api = *\/ Opcodes.ASM8, methodVisitor, labelInsnIndices);\n+        this(\/* latest api = *\/ Opcodes.ASM9, methodVisitor, labelInsnIndices);\n@@ -411,3 +413,2 @@\n-      * @param api the ASM API version implemented by this CheckMethodAdapter. Must be one of {@link\n-      *     Opcodes#ASM4}, {@link Opcodes#ASM5}, {@link Opcodes#ASM6}, {@link Opcodes#ASM7} or {@link\n-      *     Opcodes#ASM8}.\n+      * @param api the ASM API version implemented by this CheckMethodAdapter. Must be one of the\n+      *     {@code ASM}<i>x<\/i> values in {@link Opcodes}.\n@@ -449,1 +450,1 @@\n-                \/* latest api = *\/ Opcodes.ASM8, access, name, descriptor, methodVisitor, labelInsnIndices);\n+                \/* latest api = *\/ Opcodes.ASM9, access, name, descriptor, methodVisitor, labelInsnIndices);\n@@ -460,3 +461,2 @@\n-      * @param api the ASM API version implemented by this CheckMethodAdapter. Must be one of {@link\n-      *     Opcodes#ASM4}, {@link Opcodes#ASM5}, {@link Opcodes#ASM6}, {@link Opcodes#ASM7} or {@link\n-      *     Opcodes#ASM8}.\n+      * @param api the ASM API version implemented by this CheckMethodAdapter. Must be one of the\n+      *     {@code ASM}<i>x<\/i> values in {@link Opcodes}.\n@@ -484,6 +484,12 @@\n-                            analyzer.analyze(\"dummy\", this);\n-                        } catch (IndexOutOfBoundsException e) {\n-                            if (maxLocals == 0 && maxStack == 0) {\n-                                throw new IllegalArgumentException(\n-                                        \"Data flow checking option requires valid, non zero maxLocals and maxStack.\",\n-                                        e);\n+                            \/\/ If 'methodVisitor' is a MethodWriter of a ClassWriter with no flags to compute the\n+                            \/\/ max stack and locals nor the stack map frames, we know that valid max stack and\n+                            \/\/ locals must be provided. Otherwise we assume they are not needed at this stage.\n+                            \/\/ TODO(ebruneton): similarly, check that valid stack map frames are provided if the\n+                            \/\/ class writer has no flags to compute them, and the class version is V1_7 or more.\n+                            boolean checkMaxStackAndLocals =\n+                                    (methodVisitor instanceof MethodWriterWrapper)\n+                                            && !((MethodWriterWrapper) methodVisitor).computesMaxs();\n+                            if (checkMaxStackAndLocals) {\n+                                analyzer.analyze(\"dummy\", this);\n+                            } else {\n+                                analyzer.analyzeAndComputeMaxs(\"dummy\", this);\n@@ -491,2 +497,1 @@\n-                            throwError(analyzer, e);\n-                        } catch (AnalyzerException e) {\n+                        } catch (IndexOutOfBoundsException | AnalyzerException e) {\n@@ -518,1 +523,1 @@\n-                access, Opcodes.ACC_FINAL + Opcodes.ACC_MANDATED + Opcodes.ACC_SYNTHETIC);\n+                access, Opcodes.ACC_FINAL | Opcodes.ACC_MANDATED | Opcodes.ACC_SYNTHETIC);\n@@ -709,1 +714,1 @@\n-    public void visitVarInsn(final int opcode, final int var) {\n+    public void visitVarInsn(final int opcode, final int varIndex) {\n@@ -713,2 +718,2 @@\n-        checkUnsignedShort(var, INVALID_LOCAL_VARIABLE_INDEX);\n-        super.visitVarInsn(opcode, var);\n+        checkUnsignedShort(varIndex, INVALID_LOCAL_VARIABLE_INDEX);\n+        super.visitVarInsn(opcode, varIndex);\n@@ -818,1 +823,1 @@\n-            throw new IllegalArgumentException(\"Already visited label\");\n+            throw new IllegalStateException(\"Already visited label\");\n@@ -834,1 +839,1 @@\n-    public void visitIincInsn(final int var, final int increment) {\n+    public void visitIincInsn(final int varIndex, final int increment) {\n@@ -837,1 +842,1 @@\n-        checkUnsignedShort(var, INVALID_LOCAL_VARIABLE_INDEX);\n+        checkUnsignedShort(varIndex, INVALID_LOCAL_VARIABLE_INDEX);\n@@ -839,1 +844,1 @@\n-        super.visitIincInsn(var, increment);\n+        super.visitIincInsn(varIndex, increment);\n@@ -1376,1 +1381,1 @@\n-      * Checks that a the given substring is a valid type descriptor.\n+      * Checks that the given substring is a valid type descriptor.\n@@ -1476,0 +1481,14 @@\n+\n+    static class MethodWriterWrapper extends MethodVisitor {\n+\n+        private final ClassWriter owner;\n+\n+        MethodWriterWrapper(final int api, final ClassWriter owner, final MethodVisitor methodWriter) {\n+            super(api, methodWriter);\n+            this.owner = owner;\n+        }\n+\n+        boolean computesMaxs() {\n+            return owner.hasFlags(ClassWriter.COMPUTE_MAXS) || owner.hasFlags(ClassWriter.COMPUTE_FRAMES);\n+        }\n+    }\n@@ -1477,0 +1496,1 @@\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/org\/objectweb\/asm\/util\/CheckMethodAdapter.java","additions":46,"deletions":26,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-            if (!field.getType().isAssignableFrom(type)) {\n+            if (!field.getType().isInstance(value)) {\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/reflect\/UnsafeFieldAccessorImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2005, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/reflect\/UnsafeObjectFieldAccessorImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2005, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2022, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/reflect\/UnsafeQualifiedObjectFieldAccessorImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2004, 2005, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2004, 2022, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/reflect\/UnsafeQualifiedStaticObjectFieldAccessorImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2005, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/reflect\/UnsafeStaticObjectFieldAccessorImpl.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -83,0 +83,1 @@\n+    exports java.lang.foreign;\n@@ -139,2 +140,1 @@\n-        jdk.compiler,\n-        jdk.incubator.foreign;\n+        jdk.compiler;\n@@ -143,0 +143,5 @@\n+    \/\/ Note: all modules in the exported list participate in preview  features\n+    \/\/ and therefore if they use preview features they do not need to be\n+    \/\/ compiled with \"--enable-preview\".\n+    \/\/ It is recommended for any modules that do participate that their\n+    \/\/ module declaration be annotated with jdk.internal.javac.ParticipatesInPreview\n@@ -145,0 +150,1 @@\n+        java.management, \/\/ participates in preview features\n@@ -146,1 +152,6 @@\n-        jdk.jshell;\n+        jdk.incubator.concurrent, \/\/ participates in preview features\n+        jdk.incubator.vector, \/\/ participates in preview features\n+        jdk.jdi,\n+        jdk.jfr,\n+        jdk.jshell,\n+        jdk.management;\n@@ -156,4 +167,3 @@\n-        jdk.net,\n-        jdk.incubator.foreign;\n-    exports jdk.internal.access.foreign to\n-        jdk.incubator.foreign;\n+        jdk.net;\n+    exports jdk.internal.foreign to\n+        jdk.incubator.vector;\n@@ -169,2 +179,1 @@\n-        java.naming,\n-        jdk.incubator.foreign;\n+        java.naming;\n@@ -203,0 +212,1 @@\n+        jdk.incubator.concurrent,\n@@ -208,2 +218,1 @@\n-        jdk.internal.vm.ci,\n-        jdk.incubator.foreign;\n+        jdk.internal.vm.ci;\n@@ -216,2 +225,1 @@\n-        jdk.jpackage,\n-        jdk.incubator.foreign;\n+        jdk.jpackage;\n@@ -227,1 +235,1 @@\n-        jdk.incubator.foreign;\n+        java.net.http;\n@@ -234,2 +242,1 @@\n-        jdk.unsupported,\n-        jdk.incubator.foreign;\n+        jdk.unsupported;\n@@ -237,0 +244,1 @@\n+        java.management,\n@@ -238,0 +246,1 @@\n+        jdk.management,\n@@ -243,1 +252,0 @@\n-        jdk.incubator.foreign,\n@@ -248,2 +256,0 @@\n-    exports jdk.internal.util to\n-            jdk.incubator.foreign;\n@@ -280,2 +286,1 @@\n-        jdk.sctp,\n-        jdk.incubator.foreign;\n+        jdk.sctp;\n@@ -300,2 +305,1 @@\n-        jdk.crypto.ec,\n-        jdk.incubator.foreign;\n+        jdk.crypto.ec;\n@@ -364,2 +368,0 @@\n-    exports jdk.internal.invoke to\n-        jdk.incubator.foreign;\n","filename":"src\/java.base\/share\/classes\/module-info.java","additions":28,"deletions":26,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -232,1 +232,1 @@\n-         * Used for instances of {@link GuardedPatternTree}.\n+         * Used for instances of {@link ParenthesizedPatternTree}.\n@@ -237,1 +237,1 @@\n-        GUARDED_PATTERN(GuardedPatternTree.class),\n+        PARENTHESIZED_PATTERN(ParenthesizedPatternTree.class),\n@@ -240,1 +240,1 @@\n-         * Used for instances of {@link ParenthesizedPatternTree}.\n+         * Used for instances of {@link DefaultCaseLabelTree}.\n@@ -245,1 +245,1 @@\n-        PARENTHESIZED_PATTERN(ParenthesizedPatternTree.class),\n+        DEFAULT_CASE_LABEL(DefaultCaseLabelTree.class),\n@@ -248,1 +248,1 @@\n-         * Used for instances of {@link DefaultCaseLabelTree}.\n+         * Used for instances of {@link ConstantCaseLabelTree}.\n@@ -250,1 +250,1 @@\n-         * @since 17\n+         * @since 19\n@@ -253,1 +253,17 @@\n-        DEFAULT_CASE_LABEL(DefaultCaseLabelTree.class),\n+        CONSTANT_CASE_LABEL(ConstantCaseLabelTree.class),\n+\n+        \/**\n+         * Used for instances of {@link PatternCaseLabelTree}.\n+         *\n+         * @since 19\n+         *\/\n+        @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n+        PATTERN_CASE_LABEL(PatternCaseLabelTree.class),\n+\n+        \/**\n+         * Used for instances of {@link DeconstructionPatternTree}.\n+         *\n+         * @since 19\n+         *\/\n+        @PreviewFeature(feature=PreviewFeature.Feature.RECORD_PATTERNS, reflective=true)\n+        DECONSTRUCTION_PATTERN(DeconstructionPatternTree.class),\n@@ -628,1 +644,1 @@\n-         * an extends bounded wildcard type argument.\n+         * an upper-bounded wildcard type argument.\n@@ -634,1 +650,1 @@\n-         * a super bounded wildcard type argument.\n+         * a lower-bounded wildcard type argument.\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/Tree.java","additions":26,"deletions":10,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -290,1 +290,1 @@\n-     * Visits a {@code MethodTree} node.\n+     * Visits a {@code ConstantCaseLabelTree} node.\n@@ -294,0 +294,1 @@\n+     * @since 19\n@@ -295,1 +296,2 @@\n-    R visitMethod(MethodTree node, P p);\n+    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n+    R visitConstantCaseLabel(ConstantCaseLabelTree node, P p);\n@@ -298,1 +300,1 @@\n-     * Visits a {@code ModifiersTree} node.\n+     * Visits a {@code PatternCaseLabelTree} node.\n@@ -302,0 +304,1 @@\n+     * @since 19\n@@ -303,1 +306,2 @@\n-    R visitModifiers(ModifiersTree node, P p);\n+    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n+    R visitPatternCaseLabel(PatternCaseLabelTree node, P p);\n@@ -306,1 +310,1 @@\n-     * Visits a {@code NewArrayTree} node.\n+     * Visits a {@code DeconstructionPatternTree} node.\n@@ -310,0 +314,1 @@\n+     * @since 19\n@@ -311,1 +316,2 @@\n-    R visitNewArray(NewArrayTree node, P p);\n+    @PreviewFeature(feature=PreviewFeature.Feature.RECORD_PATTERNS, reflective=true)\n+    R visitDeconstructionPattern(DeconstructionPatternTree node, P p);\n@@ -314,1 +320,1 @@\n-     * Visits a {@code GuardPatternTree} node.\n+     * Visits a {@code MethodTree} node.\n@@ -318,3 +324,17 @@\n-     * @since 17\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-    R visitGuardedPattern(GuardedPatternTree node, P p);\n+    R visitMethod(MethodTree node, P p);\n+\n+    \/**\n+     * Visits a {@code ModifiersTree} node.\n+     * @param node the node being visited\n+     * @param p a parameter value\n+     * @return a result value\n+     *\/\n+    R visitModifiers(ModifiersTree node, P p);\n+\n+    \/**\n+     * Visits a {@code NewArrayTree} node.\n+     * @param node the node being visited\n+     * @param p a parameter value\n+     * @return a result value\n+     *\/\n+    R visitNewArray(NewArrayTree node, P p);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/tree\/TreeVisitor.java","additions":31,"deletions":11,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -694,0 +694,1 @@\n+     * @since 19\n@@ -696,1 +697,2 @@\n-    public R visitArrayAccess(ArrayAccessTree node, P p) {\n+    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n+    public R visitConstantCaseLabel(ConstantCaseLabelTree node, P p) {\n@@ -708,0 +710,1 @@\n+     * @since 19\n@@ -710,1 +713,2 @@\n-    public R visitMemberSelect(MemberSelectTree node, P p) {\n+    @PreviewFeature(feature=PreviewFeature.Feature.RECORD_PATTERNS, reflective=true)\n+    public R visitDeconstructionPattern(DeconstructionPatternTree node, P p) {\n@@ -722,1 +726,1 @@\n-     * @since 17\n+     * @since 19\n@@ -726,1 +730,29 @@\n-    public R visitParenthesizedPattern(ParenthesizedPatternTree node, P p) {\n+    public R visitPatternCaseLabel(PatternCaseLabelTree node, P p) {\n+        return defaultAction(node, p);\n+    }\n+\n+    \/**\n+     * {@inheritDoc}\n+     *\n+     * @implSpec This implementation calls {@code defaultAction}.\n+     *\n+     * @param node {@inheritDoc}\n+     * @param p {@inheritDoc}\n+     * @return  the result of {@code defaultAction}\n+     *\/\n+    @Override\n+    public R visitArrayAccess(ArrayAccessTree node, P p) {\n+        return defaultAction(node, p);\n+    }\n+\n+    \/**\n+     * {@inheritDoc}\n+     *\n+     * @implSpec This implementation calls {@code defaultAction}.\n+     *\n+     * @param node {@inheritDoc}\n+     * @param p {@inheritDoc}\n+     * @return  the result of {@code defaultAction}\n+     *\/\n+    @Override\n+    public R visitMemberSelect(MemberSelectTree node, P p) {\n@@ -742,1 +774,1 @@\n-    public R visitGuardedPattern(GuardedPatternTree node, P p) {\n+    public R visitParenthesizedPattern(ParenthesizedPatternTree node, P p) {\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/util\/SimpleTreeVisitor.java","additions":38,"deletions":6,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -413,1 +413,1 @@\n-        R r = scan(node.getExpressions(), p);\n+        R r = scan(node.getLabels(), p);\n@@ -822,1 +822,1 @@\n-     * @implSpec This implementation scans the children in left to right order.\n+     * @implSpec This implementation returns {@code null}.\n@@ -827,0 +827,1 @@\n+     * @since 19\n@@ -829,3 +830,20 @@\n-    public R visitArrayAccess(ArrayAccessTree node, P p) {\n-        R r = scan(node.getExpression(), p);\n-        r = scanAndReduce(node.getIndex(), p, r);\n+    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n+    public R visitConstantCaseLabel(ConstantCaseLabelTree node, P p) {\n+        return scan(node.getConstantExpression(), p);\n+    }\n+\n+    \/**\n+     * {@inheritDoc}\n+     *\n+     * @implSpec This implementation returns {@code null}.\n+     *\n+     * @param node  {@inheritDoc}\n+     * @param p  {@inheritDoc}\n+     * @return the result of scanning\n+     * @since 19\n+     *\/\n+    @Override\n+    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n+    public R visitPatternCaseLabel(PatternCaseLabelTree node, P p) {\n+        R r = scan(node.getPattern(), p);\n+        r = scanAndReduce(node.getGuard(), p, r);\n@@ -843,0 +861,1 @@\n+     * @since 19\n@@ -845,2 +864,20 @@\n-    public R visitMemberSelect(MemberSelectTree node, P p) {\n-        return scan(node.getExpression(), p);\n+    @PreviewFeature(feature=PreviewFeature.Feature.RECORD_PATTERNS, reflective=true)\n+    public R visitDeconstructionPattern(DeconstructionPatternTree node, P p) {\n+        R r = scan(node.getDeconstructor(), p);\n+        r = scanAndReduce(node.getNestedPatterns(), p, r);\n+        r = scanAndReduce(node.getVariable(), p, r);\n+        return r;\n+    }\n+\n+    \/**\n+     * {@inheritDoc} This implementation scans the children in left to right order.\n+     *\n+     * @param node  {@inheritDoc}\n+     * @param p  {@inheritDoc}\n+     * @return the result of scanning\n+     *\/\n+    @Override\n+    public R visitArrayAccess(ArrayAccessTree node, P p) {\n+        R r = scan(node.getExpression(), p);\n+        r = scanAndReduce(node.getIndex(), p, r);\n+        return r;\n@@ -857,1 +894,0 @@\n-     * @since 17\n@@ -860,3 +896,2 @@\n-    @PreviewFeature(feature=PreviewFeature.Feature.SWITCH_PATTERN_MATCHING, reflective=true)\n-    public R visitParenthesizedPattern(ParenthesizedPatternTree node, P p) {\n-        return scan(node.getPattern(), p);\n+    public R visitMemberSelect(MemberSelectTree node, P p) {\n+        return scan(node.getExpression(), p);\n@@ -877,3 +912,2 @@\n-    public R visitGuardedPattern(GuardedPatternTree node, P p) {\n-        R r = scan(node.getPattern(), p);\n-        return scanAndReduce(node.getExpression(), p, r);\n+    public R visitParenthesizedPattern(ParenthesizedPatternTree node, P p) {\n+        return scan(node.getPattern(), p);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/source\/util\/TreeScanner.java","additions":49,"deletions":15,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -211,1 +211,1 @@\n-     * Flag to indicate the super classes of this ClassSymbol has been attributed.\n+     * Flag to indicate the superclasses of this ClassSymbol has been attributed.\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Flags.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2002, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2002, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -125,1 +125,6 @@\n-    JDK19(\"19\");\n+    JDK19(\"19\"),\n+\n+    \/**\n+      * 20, tbd\n+      *\/\n+    JDK20(\"20\");\n@@ -177,0 +182,1 @@\n+        case JDK20  -> Target.JDK1_20;\n@@ -242,2 +248,4 @@\n-        PRIMITIVE_CLASSES(JDK18, Fragments.FeaturePrimitiveClasses, DiagKind.PLURAL),\n-        VALUE_CLASSES(JDK18, Fragments.FeatureValueClasses, DiagKind.PLURAL),\n+        UNCONDITIONAL_PATTERN_IN_INSTANCEOF(JDK19, Fragments.FeatureUnconditionalPatternsInInstanceof, DiagKind.PLURAL),\n+        RECORD_PATTERNS(JDK19, Fragments.FeatureDeconstructionPatterns, DiagKind.PLURAL),\n+        PRIMITIVE_CLASSES(JDK19, Fragments.FeaturePrimitiveClasses, DiagKind.PLURAL),\n+        VALUE_CLASSES(JDK19, Fragments.FeatureValueClasses, DiagKind.PLURAL),\n@@ -325,0 +333,1 @@\n+        case JDK20  -> RELEASE_20;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Source.java","additions":13,"deletions":4,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -521,1 +521,1 @@\n-    \/** An inner class has an outer instance if it is not an interface\n+    \/** An inner class has an outer instance if it is not an interface, enum or record,\n@@ -531,1 +531,1 @@\n-            type.getEnclosingType().hasTag(CLASS) && (flags() & (INTERFACE | NOOUTERTHIS)) == 0;\n+            type.getEnclosingType().hasTag(CLASS) && (flags() & (INTERFACE | ENUM | RECORD | NOOUTERTHIS)) == 0;\n@@ -1542,1 +1542,4 @@\n-        public RecordComponent getRecordComponent(JCVariableDecl var, boolean addIfMissing, List<JCAnnotation> annotations) {\n+        \/* creates a record component if non is related to the given variable and recreates a brand new one\n+         * in other case\n+         *\/\n+        public RecordComponent createRecordComponent(JCVariableDecl var, List<JCAnnotation> annotations) {\n@@ -1549,9 +1552,1 @@\n-                    if (rc.type.hasTag(TypeTag.ERROR) && !var.sym.type.hasTag(TypeTag.ERROR)) {\n-                        \/\/ Found a record component with an erroneous type: save it so that it can be removed later.\n-                        \/\/ If the class type of the record component is generated by annotation processor, it should\n-                        \/\/ use the new actual class type and symbol instead of the old dummy ErrorType.\n-                        toRemove = rc;\n-                    } else {\n-                        \/\/ Found a good record component: just return.\n-                        return rc;\n-                    }\n+                    toRemove = rc;\n@@ -1564,2 +1559,2 @@\n-                recordComponents = recordComponents.append(rc = new RecordComponent(var.sym, annotations));\n-            } else if (addIfMissing) {\n+                recordComponents = recordComponents.append(rc = new RecordComponent(var.sym, toRemove.originalAnnos, toRemove.isVarargs));\n+            } else {\n@@ -1849,0 +1844,4 @@\n+            this(field, annotations, field.type.hasTag(TypeTag.ARRAY) && ((ArrayType)field.type).isVarargs());\n+        }\n+\n+        public RecordComponent(VarSymbol field, List<JCAnnotation> annotations, boolean isVarargs) {\n@@ -1857,1 +1856,1 @@\n-            this.isVarargs = type.hasTag(TypeTag.ARRAY) && ((ArrayType)type).isVarargs();\n+            this.isVarargs = isVarargs;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symbol.java","additions":14,"deletions":15,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -194,0 +194,1 @@\n+    public final Type matchExceptionType;\n@@ -567,0 +568,1 @@\n+        matchExceptionType = enterClass(\"java.lang.MatchException\");\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Symtab.java","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1242,0 +1242,8 @@\n+                    ListBuffer<Name> names = new ListBuffer<>();\n+                    for (Symbol sym = tsym.owner; sym != null && sym.kind == TYP; sym = sym.owner) {\n+                        names.prepend(sym.name);\n+                    }\n+                    for (Name name : names) {\n+                        buf.append(name);\n+                        buf.append(\".\");\n+                    }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Type.java","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2009, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2009, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -1365,1 +1365,1 @@\n-            } \/\/ else super type will already have been scanned in the context of the anonymous class.\n+            } \/\/ else supertype will already have been scanned in the context of the anonymous class.\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/TypeAnnotations.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -668,0 +668,6 @@\n+\n+        @Override\n+        public Throwable fillInStackTrace() {\n+            \/\/ This is an internal exception; the stack trace is irrelevant.\n+            return this;\n+        }\n@@ -2650,1 +2656,1 @@\n-                \/\/ determine a supertype for a super bounded wildcard.\n+                \/\/ determine a supertype for a lower-bounded wildcard.\n@@ -2912,1 +2918,1 @@\n-    \/\/ <editor-fold defaultstate=\"collapsed\" desc=\"sub signature \/ override equivalence\">\n+    \/\/ <editor-fold defaultstate=\"collapsed\" desc=\"subsignature \/ override equivalence\">\n@@ -2914,2 +2920,2 @@\n-     * Returns true iff the first signature is a <em>sub\n-     * signature<\/em> of the other.  This is <b>not<\/b> an equivalence\n+     * Returns true iff the first signature is a <em>subsignature<\/em>\n+     * of the other.  This is <b>not<\/b> an equivalence\n@@ -2922,1 +2928,1 @@\n-     * @return true if t is a sub signature of s.\n+     * @return true if t is a subsignature of s.\n@@ -2943,1 +2949,1 @@\n-     * @return true if either argument is a sub signature of the other.\n+     * @return true if either argument is a subsignature of the other.\n@@ -5196,0 +5202,6 @@\n+\n+            @Override\n+            public Throwable fillInStackTrace() {\n+                \/\/ This is an internal exception; the stack trace is irrelevant.\n+                return this;\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/code\/Types.java","additions":19,"deletions":7,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import java.util.stream.Stream;\n@@ -179,0 +180,2 @@\n+        allowUnconditionalPatternsInstanceOf = (preview.isEnabled() || !preview.isPreview(Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF)) &&\n+                                     Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF.allowedInSource(source);\n@@ -227,0 +230,4 @@\n+    \/** Are unconditional patterns in instanceof allowed\n+     *\/\n+    private final boolean allowUnconditionalPatternsInstanceOf;\n+\n@@ -903,1 +910,6 @@\n-        return checkBase(t, tree, env, classExpected, interfaceExpected, checkExtensible);\n+        try {\n+            return checkBase(t, tree, env, classExpected, interfaceExpected, checkExtensible);\n+        } catch (CompletionFailure ex) {\n+            chk.completionError(tree.pos(), ex);\n+            return t;\n+        }\n@@ -1696,1 +1708,1 @@\n-                                     .anyMatch(l -> l.isPattern());\n+                                     .anyMatch(l -> l.hasTag(PATTERNCASELABEL));\n@@ -1701,3 +1713,1 @@\n-            Set<Object> labels = new HashSet<>(); \/\/ The set of case labels.\n-            List<Type> coveredTypesForPatterns = List.nil();\n-            List<Type> coveredTypesForConstants = List.nil();\n+            Set<Object> constants = new HashSet<>(); \/\/ The set of case constants.\n@@ -1705,1 +1715,2 @@\n-            boolean hasTotalPattern = false;      \/\/ Is there a total pattern?\n+            boolean hasUnconditionalPattern = false; \/\/ Is there a unconditional pattern?\n+            boolean lastPatternErroneous = false; \/\/ Has the last pattern erroneous type?\n@@ -1720,4 +1731,4 @@\n-                boolean wasTotalPattern = hasTotalPattern;\n-                for (JCCaseLabel pat : c.labels) {\n-                    if (pat.isExpression()) {\n-                        JCExpression expr = (JCExpression) pat;\n+                boolean wasUnconditionalPattern = hasUnconditionalPattern;\n+                for (JCCaseLabel label : c.labels) {\n+                    if (label instanceof JCConstantCaseLabel constLabel) {\n+                        JCExpression expr = constLabel.expr;\n@@ -1727,3 +1738,3 @@\n-                                log.error(pat.pos(), Errors.DuplicateCaseLabel);\n-                            } else if (wasTotalPattern) {\n-                                log.error(pat.pos(), Errors.PatternDominated);\n+                                log.error(label.pos(), Errors.DuplicateCaseLabel);\n+                            } else if (wasUnconditionalPattern) {\n+                                log.error(label.pos(), Errors.PatternDominated);\n@@ -1738,4 +1749,2 @@\n-                            } else if (!labels.add(sym)) {\n-                                log.error(pat.pos(), Errors.DuplicateCaseLabel);\n-                            } else {\n-                                checkCaseLabelDominated(pat.pos(), coveredTypesForConstants, sym.type);\n+                            } else if (!constants.add(sym)) {\n+                                log.error(label.pos(), Errors.DuplicateCaseLabel);\n@@ -1750,1 +1759,1 @@\n-                                attribExpr(pat, switchEnv, seltype);\n+                                attribExpr(expr, switchEnv, seltype);\n@@ -1770,2 +1779,2 @@\n-                                    log.error(pat.pos(), Errors.ConstantLabelNotCompatible(pattype, seltype));\n-                                } else if (!labels.add(pattype.constValue())) {\n+                                    log.error(label.pos(), Errors.ConstantLabelNotCompatible(pattype, seltype));\n+                                } else if (!constants.add(pattype.constValue())) {\n@@ -1773,2 +1782,0 @@\n-                                } else {\n-                                    checkCaseLabelDominated(pat.pos(), coveredTypesForConstants, types.boxedTypeOrType(pattype));\n@@ -1778,1 +1785,1 @@\n-                    } else if (pat.hasTag(DEFAULTCASELABEL)) {\n+                    } else if (label instanceof JCDefaultCaseLabel def) {\n@@ -1780,3 +1787,3 @@\n-                            log.error(pat.pos(), Errors.DuplicateDefaultLabel);\n-                        } else if (hasTotalPattern) {\n-                            log.error(pat.pos(), Errors.TotalPatternAndDefault);\n+                            log.error(label.pos(), Errors.DuplicateDefaultLabel);\n+                        } else if (hasUnconditionalPattern) {\n+                            log.error(label.pos(), Errors.UnconditionalPatternAndDefault);\n@@ -1786,2 +1793,3 @@\n-                    } else {\n-                        \/\/binding pattern\n+                    } else if (label instanceof JCPatternCaseLabel patternlabel) {\n+                        \/\/pattern\n+                        JCPattern pat = patternlabel.pat;\n@@ -1789,2 +1797,1 @@\n-                        var primary = TreeInfo.primaryPatternType((JCPattern) pat);\n-                        Type primaryType = primary.type();\n+                        Type primaryType = TreeInfo.primaryPatternType(pat);\n@@ -1796,8 +1803,13 @@\n-                        boolean isTotal = primary.unconditional() &&\n-                                          !patternType.isErroneous() &&\n-                                          types.isSubtype(types.erasure(seltype), patternType);\n-                        if (isTotal) {\n-                            if (hasTotalPattern) {\n-                                log.error(pat.pos(), Errors.DuplicateTotalPattern);\n-                            } else if (hasDefault) {\n-                                log.error(pat.pos(), Errors.TotalPatternAndDefault);\n+                        JCExpression guard = patternlabel.guard;\n+                        if (guard != null) {\n+                            MatchBindings afterPattern = matchBindings;\n+                            Env<AttrContext> bodyEnv = bindingEnv(env, matchBindings.bindingsWhenTrue);\n+                            try {\n+                                attribExpr(guard, bodyEnv, syms.booleanType);\n+                            } finally {\n+                                bodyEnv.info.scope.leave();\n+                            }\n+                            matchBindings = matchBindingsComputer.caseGuard(c, afterPattern, matchBindings);\n+\n+                            if (TreeInfo.isBooleanWithValue(guard, 0)) {\n+                                log.error(guard.pos(), Errors.GuardHasConstantExpressionFalse);\n@@ -1805,6 +1817,11 @@\n-                            hasTotalPattern = true;\n-                        checkCaseLabelDominated(pat.pos(), coveredTypesForPatterns, patternType);\n-                        if (!patternType.isErroneous()) {\n-                            coveredTypesForConstants = coveredTypesForConstants.prepend(patternType);\n-                            if (primary.unconditional()) {\n-                                coveredTypesForPatterns = coveredTypesForPatterns.prepend(patternType);\n+                        boolean unguarded = TreeInfo.unguardedCaseLabel(label) && !pat.hasTag(RECORDPATTERN);\n+                        boolean unconditional =\n+                                unguarded &&\n+                                !patternType.isErroneous() &&\n+                                types.isSubtype(types.boxedTypeOrType(types.erasure(seltype)),\n+                                                patternType);\n+                        if (unconditional) {\n+                            if (hasUnconditionalPattern) {\n+                                log.error(pat.pos(), Errors.DuplicateUnconditionalPattern);\n+                            } else if (hasDefault) {\n+                                log.error(pat.pos(), Errors.UnconditionalPatternAndDefault);\n@@ -1813,0 +1830,1 @@\n+                            hasUnconditionalPattern = true;\n@@ -1814,0 +1832,3 @@\n+                        lastPatternErroneous = patternType.isErroneous();\n+                    } else {\n+                        Assert.error();\n@@ -1815,1 +1836,1 @@\n-                    currentBindings = matchBindingsComputer.switchCase(pat, currentBindings, matchBindings);\n+                    currentBindings = matchBindingsComputer.switchCase(label, currentBindings, matchBindings);\n@@ -1817,0 +1838,1 @@\n+\n@@ -1834,0 +1856,1 @@\n+                chk.checkSwitchCaseLabelDominated(cases);\n@@ -1836,1 +1859,2 @@\n-                ((JCSwitch) switchTree).hasTotalPattern = hasDefault || hasTotalPattern;\n+                ((JCSwitch) switchTree).hasUnconditionalPattern =\n+                        hasDefault || hasUnconditionalPattern || lastPatternErroneous;\n@@ -1839,1 +1863,2 @@\n-                ((JCSwitchExpression) switchTree).hasTotalPattern = hasDefault || hasTotalPattern;\n+                ((JCSwitchExpression) switchTree).hasUnconditionalPattern =\n+                        hasDefault || hasUnconditionalPattern || lastPatternErroneous;\n@@ -1857,8 +1882,0 @@\n-        private void checkCaseLabelDominated(DiagnosticPosition pos,\n-                                             List<Type> coveredTypes, Type patternType) {\n-            for (Type existing : coveredTypes) {\n-                if (types.isSubtype(patternType, existing)) {\n-                    log.error(pos, Errors.PatternDominated);\n-                }\n-            }\n-        }\n@@ -4125,1 +4142,2 @@\n-           tree.pattern.getTag() == PARENTHESIZEDPATTERN) {\n+            tree.pattern.getTag() == PARENTHESIZEDPATTERN ||\n+            tree.pattern.getTag() == RECORDPATTERN) {\n@@ -4129,2 +4147,7 @@\n-                !exprtype.isErroneous() && !clazztype.isErroneous()) {\n-                log.error(tree.pos(), Errors.InstanceofPatternNoSubtype(exprtype, clazztype));\n+                !exprtype.isErroneous() && !clazztype.isErroneous() &&\n+                tree.pattern.getTag() != RECORDPATTERN) {\n+                if (!allowUnconditionalPatternsInstanceOf) {\n+                    log.error(tree.pos(), Errors.InstanceofPatternNoSubtype(exprtype, clazztype));\n+                } else if (preview.isPreview(Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF)) {\n+                    preview.warnPreview(tree.pattern.pos(), Feature.UNCONDITIONAL_PATTERN_IN_INSTANCEOF);\n+                }\n@@ -4132,1 +4155,1 @@\n-            typeTree = TreeInfo.primaryPatternTree((JCPattern) tree.pattern).var.vartype;\n+            typeTree = TreeInfo.primaryPatternTypeTree((JCPattern) tree.pattern);\n@@ -4166,0 +4189,7 @@\n+        } else if ((exprType.isPrimitive() || pattType.isPrimitive()) &&\n+                   (!exprType.isPrimitive() ||\n+                    !pattType.isPrimitive() ||\n+                    !types.isSameType(exprType, pattType))) {\n+            chk.basicHandler.report(pos,\n+                    diags.fragment(Fragments.NotApplicableTypes(exprType, pattType)));\n+            return false;\n@@ -4176,3 +4206,9 @@\n-        ResultInfo varInfo = new ResultInfo(KindSelector.TYP, resultInfo.pt, resultInfo.checkContext);\n-        tree.type = tree.var.type = attribTree(tree.var.vartype, env, varInfo);\n-        BindingSymbol v = new BindingSymbol(tree.var.mods.flags, tree.var.name, tree.var.vartype.type, env.info.scope.owner);\n+        Type type;\n+        if (tree.var.vartype != null) {\n+            ResultInfo varInfo = new ResultInfo(KindSelector.TYP, resultInfo.pt, resultInfo.checkContext);\n+            type = attribTree(tree.var.vartype, env, varInfo);\n+        } else {\n+            type = resultInfo.pt;\n+        }\n+        tree.type = tree.var.type = type;\n+        BindingSymbol v = new BindingSymbol(tree.var.mods.flags, tree.var.name, type, env.info.scope.owner);\n@@ -4184,3 +4220,5 @@\n-        annotate.annotateLater(tree.var.mods.annotations, env, v, tree.pos());\n-        annotate.queueScanTreeAndTypeAnnotate(tree.var.vartype, env, v, tree.var.pos());\n-        annotate.flush();\n+        if (tree.var.vartype != null) {\n+            annotate.annotateLater(tree.var.mods.annotations, env, v, tree.pos());\n+            annotate.queueScanTreeAndTypeAnnotate(tree.var.vartype, env, v, tree.var.pos());\n+            annotate.flush();\n+        }\n@@ -4193,10 +4231,23 @@\n-    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n-        attribExpr(tree.pattern, env);\n-        result = tree.type = tree.pattern.type;\n-    }\n-\n-    @Override\n-    public void visitGuardPattern(JCGuardPattern tree) {\n-        attribExpr(tree.patt, env);\n-        MatchBindings afterPattern = matchBindings;\n-        Env<AttrContext> bodyEnv = bindingEnv(env, matchBindings.bindingsWhenTrue);\n+    public void visitRecordPattern(JCRecordPattern tree) {\n+        tree.type = attribType(tree.deconstructor, env);\n+        Type site = types.removeWildcards(tree.type);\n+        List<Type> expectedRecordTypes;\n+        if (site.tsym.kind == Kind.TYP && ((ClassSymbol) site.tsym).isRecord()) {\n+            ClassSymbol record = (ClassSymbol) site.tsym;\n+            if (record.type.getTypeArguments().nonEmpty() && tree.type.isRaw()) {\n+                log.error(tree.pos(),Errors.RawDeconstructionPattern);\n+            }\n+            expectedRecordTypes = record.getRecordComponents()\n+                                        .stream()\n+                                        .map(rc -> types.memberType(site, rc)).collect(List.collector());\n+            tree.record = record;\n+        } else {\n+            log.error(tree.pos(), Errors.DeconstructionPatternOnlyRecords(site.tsym));\n+            expectedRecordTypes = Stream.generate(() -> Type.noType)\n+                                .limit(tree.nested.size())\n+                                .collect(List.collector());\n+        }\n+        ListBuffer<BindingSymbol> outBindings = new ListBuffer<>();\n+        List<Type> recordTypes = expectedRecordTypes;\n+        List<JCPattern> nestedPatterns = tree.nested;\n+        Env<AttrContext> localEnv = env.dup(tree, env.info.dup(env.info.scope.dup()));\n@@ -4204,1 +4255,37 @@\n-            attribExpr(tree.expr, bodyEnv, syms.booleanType);\n+            while (recordTypes.nonEmpty() && nestedPatterns.nonEmpty()) {\n+                boolean nestedIsVarPattern = false;\n+                nestedIsVarPattern |= nestedPatterns.head.hasTag(BINDINGPATTERN) &&\n+                                      ((JCBindingPattern) nestedPatterns.head).var.vartype == null;\n+                attribExpr(nestedPatterns.head, localEnv, nestedIsVarPattern ? recordTypes.head : Type.noType);\n+                checkCastablePattern(nestedPatterns.head.pos(), recordTypes.head, nestedPatterns.head.type);\n+                outBindings.addAll(matchBindings.bindingsWhenTrue);\n+                matchBindings.bindingsWhenTrue.forEach(localEnv.info.scope::enter);\n+                nestedPatterns = nestedPatterns.tail;\n+                recordTypes = recordTypes.tail;\n+            }\n+            if (recordTypes.nonEmpty() || nestedPatterns.nonEmpty()) {\n+                while (nestedPatterns.nonEmpty()) {\n+                    attribExpr(nestedPatterns.head, localEnv, Type.noType);\n+                    nestedPatterns = nestedPatterns.tail;\n+                }\n+                List<Type> nestedTypes =\n+                        tree.nested.stream().map(p -> p.type).collect(List.collector());\n+                log.error(tree.pos(),\n+                          Errors.IncorrectNumberOfNestedPatterns(expectedRecordTypes,\n+                                                                 nestedTypes));\n+            }\n+            if (tree.var != null) {\n+                BindingSymbol v = new BindingSymbol(tree.var.mods.flags, tree.var.name, tree.type,\n+                                                    localEnv.info.scope.owner);\n+                v.pos = tree.pos;\n+                tree.var.sym = v;\n+                if (chk.checkUnique(tree.var.pos(), v, localEnv.info.scope)) {\n+                    chk.checkTransparentVar(tree.var.pos(), v, localEnv.info.scope);\n+                }\n+                if (tree.var.vartype != null) {\n+                    annotate.annotateLater(tree.var.mods.annotations, localEnv, v, tree.pos());\n+                    annotate.queueScanTreeAndTypeAnnotate(tree.var.vartype, localEnv, v, tree.var.pos());\n+                    annotate.flush();\n+                }\n+                outBindings.add(v);\n+            }\n@@ -4206,1 +4293,1 @@\n-            bodyEnv.info.scope.leave();\n+            localEnv.info.scope.leave();\n@@ -4208,2 +4295,8 @@\n-        result = tree.type = tree.patt.type;\n-        matchBindings = matchBindingsComputer.guardedPattern(tree, afterPattern, matchBindings);\n+        chk.validate(tree.deconstructor, env, true);\n+        result = tree.type;\n+        matchBindings = new MatchBindings(outBindings.toList(), List.nil());\n+    }\n+\n+    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n+        attribExpr(tree.pattern, env);\n+        result = tree.type = tree.pattern.type;\n@@ -4498,1 +4591,1 @@\n-                \/\/ when determining the super type which *must* be\n+                \/\/ when determining the supertype which *must* be\n@@ -5244,3 +5337,0 @@\n-            case TOPLEVEL:\n-                attribTopLevel(env);\n-                break;\n@@ -5255,13 +5345,0 @@\n-    \/**\n-     * Attribute a top level tree. These trees are encountered when the\n-     * package declaration has annotations.\n-     *\/\n-    public void attribTopLevel(Env<AttrContext> env) {\n-        JCCompilationUnit toplevel = env.toplevel;\n-        try {\n-            annotate.flush();\n-        } catch (CompletionFailure ex) {\n-            chk.completionError(toplevel.pos(), ex);\n-        }\n-    }\n-\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Attr.java","additions":173,"deletions":96,"binary":false,"changes":269,"status":"modified"},{"patch":"@@ -2120,0 +2120,17 @@\n+        private boolean shouldCheckPreview(MethodSymbol m, MethodSymbol other, ClassSymbol origin) {\n+            if (m.owner != origin ||\n+                \/\/performance - only do the expensive checks when the overridden method is a Preview API:\n+                (other.flags() & PREVIEW_API) == 0) {\n+                return false;\n+            }\n+\n+            for (Symbol s : types.membersClosure(origin.type, false).getSymbolsByName(m.name)) {\n+                if (m != s && m.overrides(s, origin, types, false)) {\n+                    \/\/only produce preview warnings or errors if \"m\" immediatelly overrides \"other\"\n+                    \/\/without intermediate overriding methods:\n+                    return s == other;\n+                }\n+            }\n+\n+            return false;\n+        }\n@@ -3217,0 +3234,3 @@\n+        \/** NOTE: if annotation processors are present, annotation processing rounds can happen after this method,\n+         *  this can impact in particular records for which annotations are forcibly propagated.\n+         *\/\n@@ -3615,1 +3635,1 @@\n-        \/* the optional could be emtpy if the annotation is unknown in that case\n+        \/* the optional could be empty if the annotation is unknown in that case\n@@ -3857,1 +3877,1 @@\n-        if ((s.flags() & PREVIEW_API) != 0 && s.packge().modle != other.packge().modle) {\n+        if ((s.flags() & PREVIEW_API) != 0 && !preview.participatesInPreview(syms, other, s)) {\n@@ -4584,3 +4604,3 @@\n-            for (JCCaseLabel pat : c.labels) {\n-                if (pat.isExpression()) {\n-                    JCExpression expr = (JCExpression) pat;\n+            for (JCCaseLabel label : c.labels) {\n+                if (label.hasTag(CONSTANTCASELABEL)) {\n+                    JCExpression expr = ((JCConstantCaseLabel) label).expr;\n@@ -4589,1 +4609,1 @@\n-                            log.error(pat.pos(), Errors.FlowsThroughFromPattern);\n+                            log.error(label.pos(), Errors.FlowsThroughFromPattern);\n@@ -4594,1 +4614,1 @@\n-                            log.error(pat.pos(), Errors.FlowsThroughFromPattern);\n+                            log.error(label.pos(), Errors.FlowsThroughFromPattern);\n@@ -4598,1 +4618,1 @@\n-                } else if (pat.hasTag(DEFAULTCASELABEL)) {\n+                } else if (label.hasTag(DEFAULTCASELABEL)) {\n@@ -4600,1 +4620,1 @@\n-                        log.error(pat.pos(), Errors.FlowsThroughFromPattern);\n+                        log.error(label.pos(), Errors.FlowsThroughFromPattern);\n@@ -4604,0 +4624,4 @@\n+                    JCPattern pat = ((JCPatternCaseLabel) label).pat;\n+                    while (pat instanceof JCParenthesizedPattern parenthesized) {\n+                        pat = parenthesized.pattern;\n+                    }\n@@ -4607,1 +4631,1 @@\n-                        log.error(pat.pos(), Errors.FlowsThroughToPattern);\n+                        log.error(label.pos(), Errors.FlowsThroughToPattern);\n@@ -4629,0 +4653,87 @@\n+    void checkSwitchCaseLabelDominated(List<JCCase> cases) {\n+        List<JCCaseLabel> caseLabels = List.nil();\n+        for (List<JCCase> l = cases; l.nonEmpty(); l = l.tail) {\n+            JCCase c = l.head;\n+            for (JCCaseLabel label : c.labels) {\n+                if (label.hasTag(DEFAULTCASELABEL) || TreeInfo.isNullCaseLabel(label)) {\n+                    continue;\n+                }\n+                Type currentType = labelType(label);\n+                for (JCCaseLabel testCaseLabel : caseLabels) {\n+                    Type testType = labelType(testCaseLabel);\n+                    if (types.isSubtype(currentType, testType) &&\n+                        !currentType.hasTag(ERROR) && !testType.hasTag(ERROR)) {\n+                        \/\/the current label is potentially dominated by the existing (test) label, check:\n+                        boolean dominated = false;\n+                        if (label instanceof JCConstantCaseLabel) {\n+                            dominated |= !(testCaseLabel instanceof JCConstantCaseLabel);\n+                        } else if (label instanceof JCPatternCaseLabel patternCL &&\n+                                   testCaseLabel instanceof JCPatternCaseLabel testPatternCaseLabel &&\n+                                   TreeInfo.unguardedCaseLabel(testCaseLabel)) {\n+                            dominated = patternDominated(testPatternCaseLabel.pat,\n+                                                         patternCL.pat);\n+                        }\n+                        if (dominated) {\n+                            log.error(label.pos(), Errors.PatternDominated);\n+                        }\n+                    }\n+                }\n+                caseLabels = caseLabels.prepend(label);\n+            }\n+        }\n+    }\n+        \/\/where:\n+        private Type labelType(JCCaseLabel label) {\n+            return types.erasure(switch (label.getTag()) {\n+                case PATTERNCASELABEL -> ((JCPatternCaseLabel) label).pat.type;\n+                case CONSTANTCASELABEL -> types.boxedTypeOrType(((JCConstantCaseLabel) label).expr.type);\n+                default -> throw Assert.error(\"Unexpected tree kind: \" + label.getTag());\n+            });\n+        }\n+        private boolean patternDominated(JCPattern existingPattern, JCPattern currentPattern) {\n+            Type existingPatternType = types.erasure(existingPattern.type);\n+            Type currentPatternType = types.erasure(currentPattern.type);\n+            if (existingPatternType.isPrimitive() ^ currentPatternType.isPrimitive()) {\n+                return false;\n+            }\n+            if (existingPatternType.isPrimitive()) {\n+                return types.isSameType(existingPatternType, currentPatternType);\n+            } else {\n+                if (!types.isSubtype(currentPatternType, existingPatternType)) {\n+                    return false;\n+                }\n+            }\n+            while (existingPattern instanceof JCParenthesizedPattern parenthesized) {\n+                existingPattern = parenthesized.pattern;\n+            }\n+            while (currentPattern instanceof JCParenthesizedPattern parenthesized) {\n+                currentPattern = parenthesized.pattern;\n+            }\n+            if (currentPattern instanceof JCBindingPattern) {\n+                return existingPattern instanceof JCBindingPattern;\n+            } else if (currentPattern instanceof JCRecordPattern currentRecordPattern) {\n+                if (existingPattern instanceof JCBindingPattern) {\n+                    return true;\n+                } else if (existingPattern instanceof JCRecordPattern existingRecordPattern) {\n+                    List<JCPattern> existingNested = existingRecordPattern.nested;\n+                    List<JCPattern> currentNested = currentRecordPattern.nested;\n+                    if (existingNested.size() != currentNested.size()) {\n+                        return false;\n+                    }\n+                    while (existingNested.nonEmpty()) {\n+                        if (!patternDominated(existingNested.head, currentNested.head)) {\n+                            return false;\n+                        }\n+                        existingNested = existingNested.tail;\n+                        currentNested = currentNested.tail;\n+                    }\n+                    return true;\n+                } else {\n+                    Assert.error(\"Unknown pattern: \" + existingPattern.getTag());\n+                }\n+            } else {\n+                Assert.error(\"Unknown pattern: \" + currentPattern.getTag());\n+            }\n+            return false;\n+        }\n+\n@@ -4863,1 +4974,1 @@\n-                \/\/ Non-Serializable super class\n+                \/\/ Non-Serializable superclass\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Check.java","additions":122,"deletions":11,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -30,0 +30,2 @@\n+import java.util.Map;\n+import java.util.Map.Entry;\n@@ -33,0 +35,1 @@\n+import java.util.stream.Collectors;\n@@ -42,1 +45,0 @@\n-import com.sun.tools.javac.tree.TreeInfo.PatternPrimaryType;\n@@ -56,0 +58,1 @@\n+import static com.sun.tools.javac.code.TypeTag.NONE;\n@@ -670,1 +673,0 @@\n-            Set<Symbol> constants = exhaustiveSwitch ? new HashSet<>() : null;\n@@ -676,1 +678,0 @@\n-                    handleConstantCaseLabel(constants, pat);\n@@ -691,1 +692,1 @@\n-            tree.isExhaustive = tree.hasTotalPattern ||\n+            tree.isExhaustive = tree.hasUnconditionalPattern ||\n@@ -694,1 +695,2 @@\n-                tree.isExhaustive |= isExhaustive(tree.selector.pos(), tree.selector.type, constants);\n+                Set<Symbol> coveredSymbols = coveredSymbolsForCases(tree.pos(), tree.selector, tree.cases);\n+                tree.isExhaustive |= isExhaustive(tree.selector.pos(), tree.selector.type, coveredSymbols);\n@@ -699,1 +701,1 @@\n-            if (!tree.hasTotalPattern) {\n+            if (!tree.hasUnconditionalPattern) {\n@@ -710,1 +712,0 @@\n-            Set<Symbol> constants = new HashSet<>();\n@@ -717,1 +718,0 @@\n-                    handleConstantCaseLabel(constants, pat);\n@@ -730,1 +730,2 @@\n-            tree.isExhaustive = tree.hasTotalPattern ||\n+            Set<Symbol> coveredSymbols = coveredSymbolsForCases(tree.pos(), tree.selector, tree.cases);\n+            tree.isExhaustive = tree.hasUnconditionalPattern ||\n@@ -732,1 +733,1 @@\n-                                isExhaustive(tree.selector.pos(), tree.selector.type, constants);\n+                                isExhaustive(tree.selector.pos(), tree.selector.type, coveredSymbols);\n@@ -740,8 +741,73 @@\n-        private void handleConstantCaseLabel(Set<Symbol> constants, JCCaseLabel pat) {\n-            if (constants != null) {\n-                if (pat.isExpression()) {\n-                    JCExpression expr = (JCExpression) pat;\n-                    if (expr.hasTag(IDENT) && ((JCIdent) expr).sym.isEnum())\n-                        constants.add(((JCIdent) expr).sym);\n-                } else if (pat.isPattern()) {\n-                    PatternPrimaryType patternType = TreeInfo.primaryPatternType((JCPattern) pat);\n+        private Set<Symbol> coveredSymbolsForCases(DiagnosticPosition pos,\n+                                                   JCExpression selector, List<JCCase> cases) {\n+            HashSet<JCTree> labelValues = cases.stream()\n+                                               .flatMap(c -> c.labels.stream())\n+                                               .filter(TreeInfo::unguardedCaseLabel)\n+                                               .filter(l -> !l.hasTag(DEFAULTCASELABEL))\n+                                               .map(l -> l.hasTag(CONSTANTCASELABEL) ? ((JCConstantCaseLabel) l).expr\n+                                                                                     : ((JCPatternCaseLabel) l).pat)\n+                                               .collect(Collectors.toCollection(HashSet::new));\n+            return coveredSymbols(pos, selector.type, labelValues);\n+        }\n+\n+        private Set<Symbol> coveredSymbols(DiagnosticPosition pos, Type targetType,\n+                                           Iterable<? extends JCTree> labels) {\n+            Set<Symbol> coveredSymbols = new HashSet<>();\n+            Map<Symbol, List<JCRecordPattern>> deconstructionPatternsBySymbol = new HashMap<>();\n+\n+            for (JCTree labelValue : labels) {\n+                switch (labelValue.getTag()) {\n+                    case BINDINGPATTERN, PARENTHESIZEDPATTERN -> {\n+                        Type primaryPatternType = TreeInfo.primaryPatternType((JCPattern) labelValue);\n+                        if (!primaryPatternType.hasTag(NONE)) {\n+                            coveredSymbols.add(primaryPatternType.tsym);\n+                        }\n+                    }\n+                    case RECORDPATTERN -> {\n+                        JCRecordPattern dpat = (JCRecordPattern) labelValue;\n+                        Symbol type = dpat.record;\n+                        List<JCRecordPattern> augmentedPatterns =\n+                                deconstructionPatternsBySymbol.getOrDefault(type, List.nil())\n+                                                                 .prepend(dpat);\n+\n+                        deconstructionPatternsBySymbol.put(type, augmentedPatterns);\n+                    }\n+\n+                    default -> {\n+                        Assert.check(labelValue instanceof JCExpression, labelValue.getTag().name());\n+                        JCExpression expr = (JCExpression) labelValue;\n+                        if (expr.hasTag(IDENT) && ((JCIdent) expr).sym.isEnum())\n+                            coveredSymbols.add(((JCIdent) expr).sym);\n+                    }\n+                }\n+            }\n+            for (Entry<Symbol, List<JCRecordPattern>> e : deconstructionPatternsBySymbol.entrySet()) {\n+                if (coversDeconstructionFromComponent(pos, targetType, e.getValue(), 0)) {\n+                    coveredSymbols.add(e.getKey());\n+                }\n+            }\n+            return coveredSymbols;\n+        }\n+\n+        private boolean coversDeconstructionFromComponent(DiagnosticPosition pos,\n+                                                          Type targetType,\n+                                                          List<JCRecordPattern> deconstructionPatterns,\n+                                                          int component) {\n+            \/\/Given a set of record patterns for the same record, and a starting component,\n+            \/\/this method checks, whether the nested patterns for the components are exhaustive,\n+            \/\/i.e. represent all possible combinations.\n+            \/\/This is done by categorizing the patterns based on the type covered by the given\n+            \/\/starting component.\n+            \/\/For each such category, it is then checked if the nested patterns starting at the next\n+            \/\/component are exhaustive, by recursivelly invoking this method. If these nested patterns\n+            \/\/are exhaustive, the given covered type is accepted.\n+            \/\/All such covered types are then checked whether they cover the declared type of\n+            \/\/the starting component's declaration. If yes, the given set of patterns starting at\n+            \/\/the given component cover the given record exhaustivelly, and true is returned.\n+            List<? extends RecordComponent> components =\n+                    deconstructionPatterns.head.record.getRecordComponents();\n+\n+            if (components.size() == component) {\n+                \/\/no components remain to be checked:\n+                return true;\n+            }\n@@ -749,2 +815,18 @@\n-                    if (patternType.unconditional()) {\n-                        constants.add(patternType.type().tsym);\n+            \/\/for the first tested component, gather symbols covered by the nested patterns:\n+            Type instantiatedComponentType = types.memberType(targetType, components.get(component));\n+            List<JCPattern> nestedComponentPatterns = deconstructionPatterns.map(d -> d.nested.get(component));\n+            Set<Symbol> coveredSymbolsForComponent = coveredSymbols(pos, instantiatedComponentType,\n+                                                                    nestedComponentPatterns);\n+\n+            \/\/for each of the symbols covered by the starting component, find all deconstruction patterns\n+            \/\/that have the given type, or its supertype, as a type of the starting nested pattern:\n+            Map<Symbol, List<JCRecordPattern>> coveredSymbol2Patterns = new HashMap<>();\n+\n+            for (JCRecordPattern deconstructionPattern : deconstructionPatterns) {\n+                JCPattern nestedPattern = deconstructionPattern.nested.get(component);\n+                Symbol componentPatternType;\n+                switch (nestedPattern.getTag()) {\n+                    case BINDINGPATTERN, PARENTHESIZEDPATTERN -> {\n+                        Type primaryPatternType =\n+                                TreeInfo.primaryPatternType(nestedPattern);\n+                        componentPatternType = primaryPatternType.tsym;\n@@ -752,0 +834,26 @@\n+                    case RECORDPATTERN -> {\n+                        componentPatternType = ((JCRecordPattern) nestedPattern).record;\n+                    }\n+                    default -> {\n+                        throw Assert.error(\"Unexpected tree kind: \" + nestedPattern.getTag());\n+                    }\n+                }\n+                for (Symbol currentType : coveredSymbolsForComponent) {\n+                    if (types.isSubtype(types.erasure(currentType.type),\n+                                        types.erasure(componentPatternType.type))) {\n+                        coveredSymbol2Patterns.put(currentType,\n+                                                   coveredSymbol2Patterns.getOrDefault(currentType,\n+                                                                                       List.nil())\n+                                              .prepend(deconstructionPattern));\n+                    }\n+                }\n+            }\n+\n+            \/\/Check the components following the starting component, for each of the covered symbol,\n+            \/\/if they are exhaustive. If yes, the given covered symbol should be part of the following\n+            \/\/exhaustiveness check:\n+            Set<Symbol> covered = new HashSet<>();\n+\n+            for (Entry<Symbol, List<JCRecordPattern>> e : coveredSymbol2Patterns.entrySet()) {\n+                if (coversDeconstructionFromComponent(pos, targetType, e.getValue(), component + 1)) {\n+                    covered.add(e.getKey());\n@@ -754,0 +862,3 @@\n+\n+            \/\/verify whether the filtered symbols cover the given record's declared type:\n+            return isExhaustive(pos, instantiatedComponentType, covered);\n@@ -821,1 +932,4 @@\n-                    yield covered.contains(seltype.tsym);\n+                    yield covered.stream()\n+                                 .filter(coveredSym -> coveredSym.kind == TYP)\n+                                 .anyMatch(coveredSym -> types.isSubtype(types.erasure(seltype),\n+                                                                         types.erasure(coveredSym.type)));\n@@ -824,1 +938,3 @@\n-                default -> false;\n+                default -> {\n+                    yield covered.contains(seltype.tsym);\n+                }\n@@ -2052,0 +2168,8 @@\n+        void scanPattern(JCTree tree) {\n+            scan(tree);\n+            if (inits.isReset()) {\n+                inits.assign(initsWhenTrue);\n+                uninits.assign(uninitsWhenTrue);\n+            }\n+        }\n+\n@@ -2500,5 +2624,1 @@\n-                    scan(pat);\n-                    if (inits.isReset()) {\n-                        inits.assign(initsWhenTrue);\n-                        uninits.assign(uninitsWhenTrue);\n-                    }\n+                    scanPattern(pat);\n@@ -2509,2 +2629,1 @@\n-                    l.tail.head.labels.head.isExpression() &&\n-                    TreeInfo.isNull(l.tail.head.labels.head)) {\n+                    TreeInfo.isNullCaseLabel(l.tail.head.labels.head)) {\n@@ -2915,0 +3034,6 @@\n+        @Override\n+        public void visitTypeTest(JCInstanceOf tree) {\n+            scanExpr(tree.expr);\n+            scan(tree.pattern);\n+        }\n+\n@@ -2917,1 +3042,1 @@\n-            super.visitBindingPattern(tree);\n+            scan(tree.var);\n@@ -2921,0 +3046,14 @@\n+        @Override\n+        public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n+            scan(tree.pat);\n+            scan(tree.guard);\n+        }\n+\n+        @Override\n+        public void visitRecordPattern(JCRecordPattern tree) {\n+            super.visitRecordPattern(tree);\n+            if (tree.var != null) {\n+                initParam(tree.var);\n+            }\n+        }\n+\n@@ -3010,1 +3149,1 @@\n-                    case GUARDPATTERN:\n+                    case PATTERNCASELABEL:\n@@ -3034,1 +3173,1 @@\n-                        case GUARDPATTERN:\n+                        case CASE:\n@@ -3045,1 +3184,1 @@\n-                case GUARDPATTERN -> Fragments.Guard;\n+                case PATTERNCASELABEL -> Fragments.Guard;\n@@ -3085,2 +3224,12 @@\n-        public void visitGuardPattern(JCGuardPattern tree) {\n-            scan(tree.patt);\n+        public void visitBindingPattern(JCBindingPattern tree) {\n+            scan(tree.var);\n+        }\n+\n+        @Override\n+        public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n+            scan(tree.pattern);\n+        }\n+\n+        @Override\n+        public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n+             scan(tree.pat);\n@@ -3090,1 +3239,1 @@\n-                scan(tree.expr);\n+                scan(tree.guard);\n@@ -3096,0 +3245,7 @@\n+        @Override\n+        public void visitRecordPattern(JCRecordPattern tree) {\n+            scan(tree.deconstructor);\n+            scan(tree.nested);\n+            scan(tree.var);\n+        }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Flow.java","additions":193,"deletions":37,"binary":false,"changes":230,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2010, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2010, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -706,1 +706,1 @@\n-            cases.add(make.Case(JCCase.STATEMENT, List.of(make.Literal(entry.getKey())), stmts, null));\n+            cases.add(make.Case(JCCase.STATEMENT, List.of(make.ConstantCaseLabel(make.Literal(entry.getKey()))), stmts, null));\n@@ -758,0 +758,8 @@\n+        Symbol baseMethod = refSym.baseSymbol();\n+        Symbol origMethod = baseMethod.baseSymbol();\n+        if (baseMethod != origMethod && origMethod.owner == syms.objectType.tsym) {\n+            \/\/the implementation method is a java.lang.Object method transferred to an\n+            \/\/interface that does not declare it. Runtime will refer to this method as to\n+            \/\/a java.lang.Object method, so do the same:\n+            refSym = ((MethodSymbol) origMethod).asHandle();\n+        }\n@@ -2057,1 +2065,1 @@\n-                        Name name = names.fromString(new String(sym.flatName().toString().replace('.', '$') + names.dollarThis));\n+                        Name name = names.fromString(sym.flatName().toString().replace('.', '$') + names.dollarThis);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/LambdaToMethod.java","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -399,1 +399,1 @@\n-        if (c.owner.kind.matches(KindSelector.VAL_MTH)) {\n+        if (c.owner.kind.matches(KindSelector.VAL_MTH) && !c.isStatic()) {\n@@ -502,44 +502,50 @@\n-            make.at(pos.getStartPosition());\n-            JCClassDecl owner = classDef((ClassSymbol)mapVar.owner);\n-\n-            \/\/ synthetic static final int[] $SwitchMap$Color = new int[Color.values().length];\n-            MethodSymbol valuesMethod = lookupMethod(pos,\n-                                                     names.values,\n-                                                     forEnum.type,\n-                                                     List.nil());\n-            JCExpression size = make \/\/ Color.values().length\n-                .Select(make.App(make.QualIdent(valuesMethod)),\n-                        syms.lengthVar);\n-            JCExpression mapVarInit = make\n-                .NewArray(make.Type(syms.intType), List.of(size), null)\n-                .setType(new ArrayType(syms.intType, syms.arrayClass));\n-\n-            \/\/ try { $SwitchMap$Color[red.ordinal()] = 1; } catch (java.lang.NoSuchFieldError ex) {}\n-            ListBuffer<JCStatement> stmts = new ListBuffer<>();\n-            Symbol ordinalMethod = lookupMethod(pos,\n-                                                names.ordinal,\n-                                                forEnum.type,\n-                                                List.nil());\n-            List<JCCatch> catcher = List.<JCCatch>nil()\n-                .prepend(make.Catch(make.VarDef(new VarSymbol(PARAMETER, names.ex,\n-                                                              syms.noSuchFieldErrorType,\n-                                                              syms.noSymbol),\n-                                                null),\n-                                    make.Block(0, List.nil())));\n-            for (Map.Entry<VarSymbol,Integer> e : values.entrySet()) {\n-                VarSymbol enumerator = e.getKey();\n-                Integer mappedValue = e.getValue();\n-                JCExpression assign = make\n-                    .Assign(make.Indexed(mapVar,\n-                                         make.App(make.Select(make.QualIdent(enumerator),\n-                                                              ordinalMethod))),\n-                            make.Literal(mappedValue))\n-                    .setType(syms.intType);\n-                JCStatement exec = make.Exec(assign);\n-                JCStatement _try = make.Try(make.Block(0, List.of(exec)), catcher, null);\n-                stmts.append(_try);\n-            }\n-\n-            owner.defs = owner.defs\n-                .prepend(make.Block(STATIC, stmts.toList()))\n-                .prepend(make.VarDef(mapVar, mapVarInit));\n+            boolean prevAllowProtectedAccess = attrEnv.info.allowProtectedAccess;\n+            try {\n+                make.at(pos.getStartPosition());\n+                attrEnv.info.allowProtectedAccess = true;\n+                JCClassDecl owner = classDef((ClassSymbol)mapVar.owner);\n+\n+                \/\/ synthetic static final int[] $SwitchMap$Color = new int[Color.values().length];\n+                MethodSymbol valuesMethod = lookupMethod(pos,\n+                                                         names.values,\n+                                                         forEnum.type,\n+                                                         List.nil());\n+                JCExpression size = make \/\/ Color.values().length\n+                    .Select(make.App(make.QualIdent(valuesMethod)),\n+                            syms.lengthVar);\n+                JCExpression mapVarInit = make\n+                    .NewArray(make.Type(syms.intType), List.of(size), null)\n+                    .setType(new ArrayType(syms.intType, syms.arrayClass));\n+\n+                \/\/ try { $SwitchMap$Color[red.ordinal()] = 1; } catch (java.lang.NoSuchFieldError ex) {}\n+                ListBuffer<JCStatement> stmts = new ListBuffer<>();\n+                Symbol ordinalMethod = lookupMethod(pos,\n+                                                    names.ordinal,\n+                                                    forEnum.type,\n+                                                    List.nil());\n+                List<JCCatch> catcher = List.<JCCatch>nil()\n+                    .prepend(make.Catch(make.VarDef(new VarSymbol(PARAMETER, names.ex,\n+                                                                  syms.noSuchFieldErrorType,\n+                                                                  syms.noSymbol),\n+                                                    null),\n+                                        make.Block(0, List.nil())));\n+                for (Map.Entry<VarSymbol,Integer> e : values.entrySet()) {\n+                    VarSymbol enumerator = e.getKey();\n+                    Integer mappedValue = e.getValue();\n+                    JCExpression assign = make\n+                        .Assign(make.Indexed(mapVar,\n+                                             make.App(make.Select(make.QualIdent(enumerator),\n+                                                                  ordinalMethod))),\n+                                make.Literal(mappedValue))\n+                        .setType(syms.intType);\n+                    JCStatement exec = make.Exec(assign);\n+                    JCStatement _try = make.Try(make.Block(0, List.of(exec)), catcher, null);\n+                    stmts.append(_try);\n+                }\n+\n+                owner.defs = owner.defs\n+                    .prepend(make.Block(STATIC, stmts.toList()))\n+                    .prepend(make.VarDef(mapVar, mapVarInit));\n+            } finally {\n+                attrEnv.info.allowProtectedAccess = prevAllowProtectedAccess;\n+            }\n@@ -1602,1 +1608,1 @@\n-     * @return A a desugared try-with-resources tree, or the original\n+     * @return a desugared try-with-resources tree, or the original\n@@ -2698,0 +2704,1 @@\n+            !currentClass.isStatic() &&\n@@ -2833,1 +2840,1 @@\n-        if (c.isDirectlyOrIndirectlyLocal()) {\n+        if (c.isDirectlyOrIndirectlyLocal() && !c.isStatic()) {\n@@ -3035,1 +3042,1 @@\n-            if (c.isDirectlyOrIndirectlyLocal()) {\n+            if (c.isDirectlyOrIndirectlyLocal() && !c.isStatic()) {\n@@ -3662,1 +3669,3 @@\n-        List<JCCase> cases = tree.patternSwitch ? addDefaultIfNeeded(tree.cases) : tree.cases;\n+        boolean matchException = tree.patternSwitch && !tree.wasEnumSelector;\n+        List<JCCase> cases = tree.patternSwitch ? addDefaultIfNeeded(matchException, tree.cases)\n+                                                : tree.cases;\n@@ -3668,1 +3677,2 @@\n-        List<JCCase> cases = addDefaultIfNeeded(tree.cases);\n+        boolean matchException = tree.patternSwitch && !tree.wasEnumSelector;\n+        List<JCCase> cases = addDefaultIfNeeded(matchException, tree.cases);\n@@ -3672,1 +3682,1 @@\n-    private List<JCCase> addDefaultIfNeeded(List<JCCase> cases) {\n+    private List<JCCase> addDefaultIfNeeded(boolean matchException, List<JCCase> cases) {\n@@ -3674,2 +3684,5 @@\n-            JCThrow thr = make.Throw(makeNewClass(syms.incompatibleClassChangeErrorType,\n-                                                  List.nil()));\n+            Type exception = matchException ? syms.matchExceptionType\n+                                            : syms.incompatibleClassChangeErrorType;\n+            List<JCExpression> params = matchException ? List.of(makeNull(), makeNull())\n+                                                       : List.nil();\n+            JCThrow thr = make.Throw(makeNewClass(exception, params));\n@@ -3763,1 +3776,1 @@\n-        if (cases.stream().anyMatch(c -> TreeInfo.isNull(c.labels.head))) {\n+        if (cases.stream().anyMatch(c -> TreeInfo.isNullCaseLabel(c.labels.head))) {\n@@ -3789,1 +3802,1 @@\n-            if (c.labels.head.isExpression()) {\n+            if (c.labels.head.hasTag(CONSTANTCASELABEL)) {\n@@ -3791,1 +3804,1 @@\n-                if (TreeInfo.isNull(c.labels.head)) {\n+                if (TreeInfo.isNullCaseLabel(c.labels.head)) {\n@@ -3794,1 +3807,1 @@\n-                    VarSymbol label = (VarSymbol)TreeInfo.symbol((JCExpression) c.labels.head);\n+                    VarSymbol label = (VarSymbol)TreeInfo.symbol(((JCConstantCaseLabel) c.labels.head).expr);\n@@ -3797,1 +3810,1 @@\n-                newCases.append(make.Case(JCCase.STATEMENT, List.of(pat), c.stats, null));\n+                newCases.append(make.Case(JCCase.STATEMENT, List.of(make.ConstantCaseLabel(pat)), c.stats, null));\n@@ -3877,2 +3890,2 @@\n-                if (oneCase.labels.head.isExpression()) {\n-                    if (TreeInfo.isNull(oneCase.labels.head)) {\n+                if (oneCase.labels.head.hasTag(CONSTANTCASELABEL)) {\n+                    if (TreeInfo.isNullCaseLabel(oneCase.labels.head)) {\n@@ -3882,1 +3895,1 @@\n-                        JCExpression expression = (JCExpression) oneCase.labels.head;\n+                        JCExpression expression = ((JCConstantCaseLabel) oneCase.labels.head).expr;\n@@ -3967,1 +3980,4 @@\n-                caseBuffer.append(make.Case(JCCase.STATEMENT, List.of(make.Literal(hashCode)), lb.toList(), null));\n+                caseBuffer.append(make.Case(JCCase.STATEMENT,\n+                                            List.of(make.ConstantCaseLabel(make.Literal(hashCode))),\n+                                            lb.toList(),\n+                                            null));\n@@ -3986,2 +4002,2 @@\n-                boolean isDefault = !oneCase.labels.head.isExpression();\n-                JCCaseLabel caseExpr;\n+                boolean isDefault = !oneCase.labels.head.hasTag(CONSTANTCASELABEL);\n+                JCExpression caseExpr;\n@@ -3993,2 +4009,4 @@\n-                    caseExpr = make.Literal(caseLabelToPosition.get((String)TreeInfo.skipParens((JCExpression) oneCase.labels.head).\n-                                                                    type.constValue()));\n+                    JCExpression expression = ((JCConstantCaseLabel) oneCase.labels.head).expr;\n+                    String name = (String) TreeInfo.skipParens(expression)\n+                                                   .type.constValue();\n+                    caseExpr = make.Literal(caseLabelToPosition.get(name));\n@@ -3997,1 +4015,2 @@\n-                lb.append(make.Case(JCCase.STATEMENT, caseExpr == null ? List.of(make.DefaultCaseLabel()) : List.of(caseExpr),\n+                lb.append(make.Case(JCCase.STATEMENT, caseExpr == null ? List.of(make.DefaultCaseLabel())\n+                                                                       : List.of(make.ConstantCaseLabel(caseExpr)),\n@@ -4034,1 +4053,1 @@\n-        if (cases.stream().anyMatch(c -> TreeInfo.isNull(c.labels.head))) {\n+        if (cases.stream().anyMatch(c -> TreeInfo.isNullCaseLabel(c.labels.head))) {\n@@ -4044,1 +4063,1 @@\n-                if (TreeInfo.isNull(c.labels.head)) {\n+                if (TreeInfo.isNullCaseLabel(c.labels.head)) {\n@@ -4047,1 +4066,1 @@\n-                    constants.add((int) c.labels.head.type.constValue());\n+                    constants.add((int) ((JCConstantCaseLabel) c.labels.head).expr.type.constValue());\n@@ -4058,1 +4077,1 @@\n-            nullCase.labels.head = makeLit(syms.intType, nullValue);\n+            nullCase.labels.head = make.ConstantCaseLabel(makeLit(syms.intType, nullValue));\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Lower.java","additions":91,"deletions":72,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,0 +69,1 @@\n+import java.util.function.UnaryOperator;\n@@ -360,1 +361,3 @@\n-                    isInnerSubClass(env.enclClass.sym, c.owner);\n+                    isInnerSubClass(env.enclClass.sym, c.owner)\n+                    ||\n+                    env.info.allowProtectedAccess;\n@@ -1430,0 +1433,6 @@\n+\n+        @Override\n+        public Throwable fillInStackTrace() {\n+            \/\/ This is an internal exception; the stack trace is irrelevant.\n+            return this;\n+        }\n@@ -2460,1 +2469,1 @@\n-        return checkRestrictedType(pos, findIdentInternal(env, name, kind), name);\n+        return checkNonExistentType(checkRestrictedType(pos, findIdentInternal(env, name, kind), name));\n@@ -2496,1 +2505,1 @@\n-        return checkRestrictedType(pos, findIdentInPackageInternal(env, pck, name, kind), name);\n+        return checkNonExistentType(checkRestrictedType(pos, findIdentInPackageInternal(env, pck, name, kind), name));\n@@ -2533,1 +2542,11 @@\n-        return checkRestrictedType(pos, findIdentInTypeInternal(env, site, name, kind), name);\n+        return checkNonExistentType(checkRestrictedType(pos, findIdentInTypeInternal(env, site, name, kind), name));\n+    }\n+\n+    private Symbol checkNonExistentType(Symbol symbol) {\n+        \/*  Guard against returning a type is not on the class path of the current compilation,\n+         *  but *was* on the class path of a separate compilation that produced a class file\n+         *  that is on the class path of the current compilation. Such a type will fail completion\n+         *  but the completion failure may have been silently swallowed (e.g. missing annotation types)\n+         *  with an error stub symbol lingering in the symbol tables.\n+         *\/\n+        return symbol instanceof ClassSymbol c && c.type.isErroneous() && c.classfile == null ? typeNotFound : symbol;\n@@ -3807,1 +3826,1 @@\n-            \/\/find a direct super type that is a subtype of 'c'\n+            \/\/find a direct supertype that is a subtype of 'c'\n@@ -4203,7 +4222,0 @@\n-            if (compactMethodDiags) {\n-                JCDiagnostic simpleDiag =\n-                    MethodResolutionDiagHelper.rewrite(diags, pos, log.currentSource(), dkind, c.snd);\n-                if (simpleDiag != null) {\n-                    return simpleDiag;\n-                }\n-            }\n@@ -4213,0 +4225,2 @@\n+                      compactMethodDiags ?\n+                              d -> MethodResolutionDiagHelper.rewrite(diags, pos, log.currentSource(), dkind, c.snd) : null,\n@@ -4273,2 +4287,2 @@\n-                            EnumSet.of(DiagnosticFlag.COMPRESSED) :\n-                            EnumSet.noneOf(DiagnosticFlag.class),\n+                                EnumSet.of(DiagnosticFlag.COMPRESSED) :\n+                                EnumSet.noneOf(DiagnosticFlag.class),\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/Resolve.java","additions":29,"deletions":15,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -578,0 +578,13 @@\n+    @Override\n+    public void visitConstantCaseLabel(JCConstantCaseLabel tree) {\n+        tree.expr = translate(tree.expr, null);\n+        result = tree;\n+    }\n+\n+    @Override\n+    public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n+        tree.pat = translate(tree.pat, null);\n+        tree.guard = translate(tree.guard, syms.booleanType);\n+        result = tree;\n+    }\n+\n@@ -595,4 +608,5 @@\n-    @Override\n-    public void visitGuardPattern(JCGuardPattern tree) {\n-        tree.patt = translate(tree.patt, null);\n-        tree.expr = translate(tree.expr, syms.booleanType);\n+    public void visitRecordPattern(JCRecordPattern tree) {\n+        tree.fullComponentTypes = tree.record.getRecordComponents()\n+                                             .map(rc -> types.memberType(tree.type, rc));\n+        tree.deconstructor = translate(tree.deconstructor, null);\n+        tree.nested = translate(tree.nested, null);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TransTypes.java","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -987,1 +987,1 @@\n-                    sym.getRecordComponent(field, true,\n+                    sym.createRecordComponent(field,\n@@ -1288,1 +1288,18 @@\n-            \/\/ fields can't be varargs, lets remove the flag\n+            \/** Some notes regarding the code below. Annotations applied to elements of a record header are propagated\n+             *  to other elements which, when applicable, not explicitly declared by the user: the canonical constructor,\n+             *  accessors, fields and record components. Of all these the only ones that can't be explicitly declared are\n+             *  the fields and the record components.\n+             *\n+             *  Now given that annotations are propagated to all possible targets  regardless of applicability,\n+             *  annotations not applicable to a given element should be removed. See Check::validateAnnotation. Once\n+             *  annotations are removed we could lose the whole picture, that's why original annotations are stored in\n+             *  the record component, see RecordComponent::originalAnnos, but there is no real AST representing a record\n+             *  component so if there is an annotation processing round it could be that we need to reenter a record for\n+             *  which we need to re-attribute its annotations. This is why one of the things the code below is doing is\n+             *  copying the original annotations from the record component to the corresponding field, again this applies\n+             *  only if APs are present.\n+             *\n+             *  We need to copy the annotations to the field so that annotations applicable only to the record component\n+             *  can be attributed as if declared in the field and then stored in the metadata associated to the record\n+             *  component.\n+             *\/\n@@ -1291,0 +1308,4 @@\n+                RecordComponent rec = tree.sym.getRecordComponent(field.sym);\n+                TreeCopier<JCTree> tc = new TreeCopier<>(make.at(field.pos));\n+                List<JCAnnotation> originalAnnos = tc.copy(rec.getOriginalAnnos());\n+\n@@ -1292,0 +1313,6 @@\n+                if (originalAnnos.length() != field.mods.annotations.length()) {\n+                    field.mods.annotations = originalAnnos;\n+                    annotate.annotateLater(originalAnnos, env, field.sym, field.pos());\n+                }\n+\n+                \/\/ also here\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/comp\/TypeEnter.java","additions":29,"deletions":2,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -341,0 +341,15 @@\n+        @Override\n+        public void visitConstantCaseLabel(JCConstantCaseLabel tree) {\n+            SourceRange sr = new SourceRange(startPos(tree), endPos(tree));\n+            sr.mergeWith(csp(tree.expr));\n+            result = sr;\n+        }\n+\n+        @Override\n+        public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n+            SourceRange sr = new SourceRange(startPos(tree), endPos(tree));\n+            sr.mergeWith(csp(tree.pat));\n+            sr.mergeWith(csp(tree.guard));\n+            result = sr;\n+        }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/CRTable.java","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -123,1 +123,2 @@\n-        V63(63, 0);   \/\/ JDK 19\n+        V63(63, 0),   \/\/ JDK 19\n+        V64(64, 0);   \/\/ JDK 20\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassFile.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -831,1 +831,1 @@\n-            char flags = (char) adjustFlags(inner.flags_field & ~STRICTFP); \/\/ inner classes should not have the strictfp flag set.\n+            int flags = adjustFlags(inner.flags_field);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/ClassWriter.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -623,1 +623,1 @@\n-            state.pop(1);\n+            state.pop(state.stacksize);\n@@ -2025,1 +2025,1 @@\n-                        range.length += delta;\n+                        range.length += (char)delta;\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Code.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -880,0 +880,4 @@\n+        if (!code.isAlive()) {\n+            return items.makeStackItem(pt);\n+        }\n+\n@@ -1357,1 +1361,1 @@\n-                if (l.head.labels.head.isExpression()) {\n+                if (l.head.labels.head instanceof JCConstantCaseLabel constLabel) {\n@@ -1359,1 +1363,1 @@\n-                    int val = ((Number)((JCExpression) l.head.labels.head).type.constValue()).intValue();\n+                    int val = ((Number) constLabel.expr.type.constValue()).intValue();\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/jvm\/Gen.java","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -793,1 +793,0 @@\n-\n@@ -797,1 +796,2 @@\n-    public JCPattern parsePattern(int pos, JCModifiers mods, JCExpression parsedType, boolean inInstanceOf) {\n+    public JCPattern parsePattern(int pos, JCModifiers mods, JCExpression parsedType,\n+                                  boolean allowVar, boolean checkGuard) {\n@@ -800,0 +800,1 @@\n+            \/\/parenthesized pattern:\n@@ -802,1 +803,1 @@\n-            JCPattern p = parsePattern(token.pos, null, null, false);\n+            JCPattern p = parsePattern(token.pos, null, null, true, false);\n@@ -807,9 +808,41 @@\n-            JCExpression e = parsedType == null ? term(TYPE | NOLAMBDA) : parsedType;\n-            JCVariableDecl var = toP(F.at(token.pos).VarDef(mods, ident(), e, null));\n-            pattern = toP(F.at(pos).BindingPattern(var));\n-        }\n-        if (!inInstanceOf && token.kind == AMPAMP) {\n-            checkSourceLevel(Feature.PATTERN_SWITCH);\n-            nextToken();\n-            JCExpression guard = term(EXPR | NOLAMBDA);\n-            pattern = F.at(pos).GuardPattern(pattern, guard);\n+            JCExpression e;\n+            if (parsedType == null) {\n+                boolean var = token.kind == IDENTIFIER && token.name() == names.var;\n+                e = unannotatedType(allowVar, TYPE | NOLAMBDA);\n+                if (var) {\n+                    e = null;\n+                }\n+            } else {\n+                e = parsedType;\n+            }\n+            if (token.kind == LPAREN) {\n+                \/\/deconstruction pattern:\n+                checkSourceLevel(Feature.RECORD_PATTERNS);\n+                ListBuffer<JCPattern> nested = new ListBuffer<>();\n+                if (!peekToken(RPAREN)) {\n+                    do {\n+                        nextToken();\n+                        JCPattern nestedPattern = parsePattern(token.pos, null, null, true, false);\n+                        nested.append(nestedPattern);\n+                    } while (token.kind == COMMA);\n+                } else {\n+                    nextToken();\n+                }\n+                accept(RPAREN);\n+                JCVariableDecl var;\n+                if (token.kind == IDENTIFIER) {\n+                    if (!checkGuard || token.name() != names.when) {\n+                        var = to(F.at(token.pos).VarDef(F.Modifiers(0), token.name(), e, null));\n+                        nextToken();\n+                    } else {\n+                        var = null;\n+                    }\n+                } else {\n+                    var = null;\n+                }\n+                pattern = toP(F.at(pos).RecordPattern(e, nested.toList(), var));\n+            } else {\n+                \/\/type test pattern:\n+                JCVariableDecl var = toP(F.at(token.pos).VarDef(mods, ident(), e, null));\n+                pattern = toP(F.at(pos).BindingPattern(var));\n+            }\n@@ -820,0 +853,1 @@\n+\n@@ -852,1 +886,5 @@\n-        JCExpression result = term(TYPE);\n+        return unannotatedType(allowVar, TYPE);\n+    }\n+\n+    public JCExpression unannotatedType(boolean allowVar, int newmode) {\n+        JCExpression result = term(newmode);\n@@ -1001,1 +1039,1 @@\n-                    pattern = parsePattern(token.pos, null, null, true);\n+                    pattern = parsePattern(token.pos, null, null, false, false);\n@@ -1009,1 +1047,3 @@\n-                        pattern = parsePattern(patternPos, mods, type, true);\n+                        pattern = parsePattern(patternPos, mods, type, false, false);\n+                    } else if (token.kind == LPAREN) {\n+                        pattern = parsePattern(patternPos, mods, type, false, false);\n@@ -3138,4 +3178,0 @@\n-            int lookahead = 0;\n-            while (S.token(lookahead).kind == LPAREN) {\n-                lookahead++;\n-            }\n@@ -3144,1 +3180,1 @@\n-                              analyzePattern(lookahead) == PatternResult.PATTERN;\n+                              analyzePattern(0) == PatternResult.PATTERN;\n@@ -3147,1 +3183,7 @@\n-                return parsePattern(patternPos, mods, null, false);\n+                JCPattern p = parsePattern(patternPos, mods, null, false, true);\n+                JCExpression guard = null;\n+                if (token.kind == IDENTIFIER && token.name() == names.when) {\n+                    nextToken();\n+                    guard = term(EXPR | NOLAMBDA);\n+                }\n+                return toP(F.at(patternPos).PatternCaseLabel(p, guard));\n@@ -3149,1 +3191,2 @@\n-                return term(EXPR | NOLAMBDA);\n+                JCExpression expr = term(EXPR | NOLAMBDA);\n+                return toP(F.at(patternPos).ConstantCaseLabel(expr));\n@@ -3158,1 +3201,3 @@\n-        int depth = 0;\n+        int typeDepth = 0;\n+        int parenDepth = 0;\n+        PatternResult pendingResult = PatternResult.EXPRESSION;\n@@ -3165,1 +3210,7 @@\n-                    if (depth == 0 && peekToken(lookahead, LAX_IDENTIFIER)) return PatternResult.PATTERN;\n+                    if (typeDepth == 0 && peekToken(lookahead, LAX_IDENTIFIER)) {\n+                        if (parenDepth == 0) {\n+                            return PatternResult.PATTERN;\n+                        } else {\n+                            pendingResult = PatternResult.PATTERN;\n+                        }\n+                    }\n@@ -3168,3 +3219,3 @@\n-                case LT: depth++; break;\n-                case GTGTGT: depth--;\n-                case GTGT: depth--;\n+                case LT: typeDepth++; break;\n+                case GTGTGT: typeDepth--;\n+                case GTGT: typeDepth--;\n@@ -3172,5 +3223,6 @@\n-                    depth--;\n-                    if (depth == 0) {\n-                         return peekToken(lookahead, LAX_IDENTIFIER) ? PatternResult.PATTERN\n-                                                          : PatternResult.EXPRESSION;\n-                    } else if (depth < 0) return PatternResult.EXPRESSION;\n+                    typeDepth--;\n+                    if (typeDepth == 0) {\n+                         return peekToken(lookahead, LAX_IDENTIFIER) ||\n+                                peekToken(lookahead, tk -> tk == LPAREN) ? PatternResult.PATTERN\n+                                                                         : PatternResult.EXPRESSION;\n+                    } else if (typeDepth < 0) return PatternResult.EXPRESSION;\n@@ -3188,1 +3240,2 @@\n-                        return PatternResult.EXPRESSION;\n+                        \/\/ This is a potential guard, if we are already in a pattern\n+                        return pendingResult;\n@@ -3190,1 +3243,11 @@\n-                default: return PatternResult.EXPRESSION;\n+                case LPAREN:\n+                    if (S.token(lookahead + 1).kind == RPAREN) {\n+                        return parenDepth != 0 && S.token(lookahead + 2).kind == ARROW\n+                                ? PatternResult.EXPRESSION\n+                                : PatternResult.PATTERN;\n+                    }\n+                    parenDepth++; break;\n+                case RPAREN: parenDepth--; break;\n+                case ARROW: return parenDepth > 0 ? PatternResult.EXPRESSION\n+                                                   : pendingResult;\n+                default: return pendingResult;\n@@ -4147,0 +4210,9 @@\n+        int typeNamePos = token.pos;\n+        List<JCTypeParameter> typarams = typeParametersOpt(true);\n+        if (typarams == null || !typarams.isEmpty()) {\n+            int errorPosition = typarams == null\n+                    ? typeNamePos\n+                    : typarams.head.pos;\n+            log.error(DiagnosticFlag.SYNTAX, errorPosition, Errors.EnumCantBeGeneric);\n+        }\n+\n@@ -4730,0 +4802,6 @@\n+        return typeParametersOpt(false);\n+    }\n+    \/** Parses a potentially empty type parameter list if needed with `allowEmpty`.\n+     *  The caller is free to choose the desirable error message in this (erroneous) case.\n+     *\/\n+    protected List<JCTypeParameter> typeParametersOpt(boolean parseEmpty) {\n@@ -4733,0 +4811,6 @@\n+\n+            if (parseEmpty && token.kind == GT) {\n+                accept(GT);\n+                return null;\n+            }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/parser\/JavacParser.java","additions":119,"deletions":35,"binary":false,"changes":154,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -515,2 +515,2 @@\n-compiler.err.duplicate.total.pattern=\\\n-    duplicate total pattern\n+compiler.err.duplicate.unconditional.pattern=\\\n+    duplicate unconditional pattern\n@@ -518,2 +518,5 @@\n-compiler.err.total.pattern.and.default=\\\n-    switch has both a total pattern and a default label\n+compiler.err.unconditional.pattern.and.default=\\\n+    switch has both an unconditional pattern and a default label\n+\n+compiler.err.guard.has.constant.expression.false=\\\n+    this case label has a guard that is a constant expression with value ''false''\n@@ -559,0 +562,3 @@\n+compiler.err.enum.cant.be.generic=\\\n+    enums cannot be generic\n+\n@@ -1574,3 +1580,0 @@\n-compiler.misc.base.membership=\\\n-    all your base class are belong to us\n-\n@@ -2530,0 +2533,4 @@\n+# 0: type, 1: type\n+compiler.misc.not.applicable.types=\\\n+    pattern of type {1} is not applicable at {0}\n+\n@@ -2623,1 +2630,1 @@\n-        lower bounds: {0}\n+        upper bounds: {0}\n@@ -3085,0 +3092,3 @@\n+compiler.misc.feature.deconstruction.patterns=\\\n+    deconstruction patterns\n+\n@@ -3097,0 +3107,3 @@\n+compiler.misc.feature.unconditional.patterns.in.instanceof=\\\n+    unconditional patterns in instanceof\n+\n@@ -3338,0 +3351,3 @@\n+compiler.err.dc.invalid.html=\\\n+    invalid HTML\n+\n@@ -4014,0 +4030,13 @@\n+# 0: symbol\n+compiler.err.deconstruction.pattern.only.records=\\\n+    deconstruction patterns can only be applied to records, {0} is not a record\n+\n+# 0: list of type, 1: list of type\n+compiler.err.incorrect.number.of.nested.patterns=\\\n+    incorrect number of nested patterns\\n\\\n+    required: {0}\\n\\\n+    found: {1}\n+\n+compiler.err.raw.deconstruction.pattern=\\\n+    raw deconstruction patterns are not allowed\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/compiler.properties","additions":38,"deletions":9,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,4 @@\n+## tool\n+\n+javac.description=read Java class and interface definitions and compile them into bytecode and class files\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/resources\/javac.properties","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -247,2 +247,7 @@\n-        DEFAULTCASELABEL,\n-        GUARDPATTERN,\n+        RECORDPATTERN,\n+\n+        \/* Case labels.\n+         *\/\n+        DEFAULTCASELABEL,\n+        CONSTANTCASELABEL,\n+        PATTERNCASELABEL,\n@@ -714,5 +719,0 @@\n-        public abstract boolean isExpression();\n-        public boolean isNullPattern() {\n-            return isExpression() && TreeInfo.isNull((JCExpression) this);\n-        }\n-        public abstract boolean isPattern();\n@@ -721,1 +721,1 @@\n-    public abstract static class JCExpression extends JCCaseLabel implements ExpressionTree {\n+    public abstract static class JCExpression extends JCTree implements ExpressionTree {\n@@ -735,9 +735,0 @@\n-        @Override\n-        public boolean isExpression() {\n-            return true;\n-        }\n-\n-        @Override\n-        public boolean isPattern() {\n-            return false;\n-        }\n@@ -1327,1 +1318,1 @@\n-        public boolean hasTotalPattern;\n+        public boolean hasUnconditionalPattern;\n@@ -1330,0 +1321,1 @@\n+        public boolean wasEnumSelector;\n@@ -1382,0 +1374,1 @@\n+\n@@ -1383,1 +1376,7 @@\n-        public List<JCExpression> getExpressions() { return labels.stream().filter(p -> p instanceof JCExpression).map(p -> (JCExpression) p).collect(List.collector()); }\n+        public List<JCExpression> getExpressions() {\n+            return labels.stream()\n+                         .filter(p -> p.hasTag(CONSTANTCASELABEL))\n+                         .map(p -> ((JCConstantCaseLabel) p).expr)\n+                         .collect(List.collector());\n+        }\n+\n@@ -1440,1 +1439,1 @@\n-        public boolean hasTotalPattern;\n+        public boolean hasUnconditionalPattern;\n@@ -1443,0 +1442,1 @@\n+        public boolean wasEnumSelector;\n@@ -2309,1 +2309,1 @@\n-    public abstract static class JCPattern extends JCCaseLabel\n+    public abstract static class JCPattern extends JCTree\n@@ -2311,10 +2311,0 @@\n-\n-        @Override\n-        public boolean isExpression() {\n-            return false;\n-        }\n-\n-        @Override\n-        public boolean isPattern() {\n-            return true;\n-        }\n@@ -2385,0 +2375,16 @@\n+    }\n+\n+    public static class JCConstantCaseLabel extends JCCaseLabel\n+            implements ConstantCaseLabelTree {\n+\n+        public JCExpression expr;\n+\n+        protected JCConstantCaseLabel(JCExpression expr) {\n+            this.expr = expr;\n+        }\n+\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public JCExpression getConstantExpression() {\n+            return expr;\n+        }\n+\n@@ -2386,2 +2392,7 @@\n-        public boolean isExpression() {\n-            return false;\n+        public void accept(Visitor v) {\n+            v.visitConstantCaseLabel(this);\n+        }\n+\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public Kind getKind() {\n+            return Kind.CONSTANT_CASE_LABEL;\n@@ -2391,2 +2402,3 @@\n-        public boolean isPattern() {\n-            return false;\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public <R, D> R accept(TreeVisitor<R, D> v, D d) {\n+            return v.visitConstantCaseLabel(this, d);\n@@ -2394,0 +2406,50 @@\n+\n+        @Override\n+        public Tag getTag() {\n+            return CONSTANTCASELABEL;\n+        }\n+\n+    }\n+\n+    public static class JCPatternCaseLabel extends JCCaseLabel\n+            implements PatternCaseLabelTree {\n+\n+        public JCPattern pat;\n+        public JCExpression guard;\n+\n+        protected JCPatternCaseLabel(JCPattern pat, JCExpression guard) {\n+            this.pat = pat;\n+            this.guard = guard;\n+        }\n+\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public JCPattern getPattern() {\n+            return pat;\n+        }\n+\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public JCExpression getGuard() {\n+            return guard;\n+        }\n+\n+        @Override\n+        public void accept(Visitor v) {\n+            v.visitPatternCaseLabel(this);\n+        }\n+\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public Kind getKind() {\n+            return Kind.PATTERN_CASE_LABEL;\n+        }\n+\n+        @Override\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public <R, D> R accept(TreeVisitor<R, D> v, D d) {\n+            return v.visitPatternCaseLabel(this, d);\n+        }\n+\n+        @Override\n+        public Tag getTag() {\n+            return PATTERNCASELABEL;\n+        }\n+\n@@ -2431,4 +2493,7 @@\n-    public static class JCGuardPattern extends JCPattern\n-            implements GuardedPatternTree {\n-        public JCPattern patt;\n-        public JCExpression expr;\n+    public static class JCRecordPattern extends JCPattern\n+            implements DeconstructionPatternTree {\n+        public JCExpression deconstructor;\n+        public List<JCPattern> nested;\n+        public JCVariableDecl var;\n+        public ClassSymbol record;\n+        public List<Type> fullComponentTypes;\n@@ -2436,3 +2501,10 @@\n-        public JCGuardPattern(JCPattern patt, JCExpression expr) {\n-            this.patt = patt;\n-            this.expr = expr;\n+        protected JCRecordPattern(JCExpression deconstructor, List<JCPattern> nested,\n+                                  JCVariableDecl var) {\n+            this.deconstructor = deconstructor;\n+            this.nested = nested;\n+            this.var = var;\n+        }\n+\n+        @DefinedBy(Api.COMPILER_TREE)\n+        public Name getBinding() {\n+            return null;\n@@ -2442,2 +2514,2 @@\n-        public PatternTree getPattern() {\n-            return patt;\n+        public ExpressionTree getDeconstructor() {\n+            return deconstructor;\n@@ -2447,2 +2519,2 @@\n-        public ExpressionTree getExpression() {\n-            return expr;\n+        public List<? extends JCPattern> getNestedPatterns() {\n+            return nested;\n@@ -2453,1 +2525,1 @@\n-            v.visitGuardPattern(this);\n+            v.visitRecordPattern(this);\n@@ -2458,1 +2530,1 @@\n-            return Kind.GUARDED_PATTERN;\n+            return Kind.DECONSTRUCTION_PATTERN;\n@@ -2464,1 +2536,1 @@\n-            return v.visitGuardedPattern(this, d);\n+            return v.visitDeconstructionPattern(this, d);\n@@ -2469,1 +2541,1 @@\n-            return Tag.GUARDPATTERN;\n+            return RECORDPATTERN;\n@@ -2471,0 +2543,6 @@\n+\n+        @Override @DefinedBy(Api.COMPILER_TREE)\n+        public VariableTree getVariable() {\n+            return var;\n+        }\n+\n@@ -3517,0 +3595,2 @@\n+        public void visitConstantCaseLabel(JCConstantCaseLabel that) { visitTree(that); }\n+        public void visitPatternCaseLabel(JCPatternCaseLabel that) { visitTree(that); }\n@@ -3518,1 +3598,1 @@\n-        public void visitGuardPattern(JCGuardPattern that) { visitTree(that); }\n+        public void visitRecordPattern(JCRecordPattern that) { visitTree(that); }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/JCTree.java","additions":132,"deletions":52,"binary":false,"changes":184,"status":"modified"},{"patch":"@@ -173,8 +173,0 @@\n-    \/** Exception to propagate IOException through visitXYZ methods *\/\n-    private static class UncheckedIOException extends Error {\n-        static final long serialVersionUID = -4032692679158424751L;\n-        UncheckedIOException(IOException e) {\n-            super(e.getMessage(), e);\n-        }\n-    }\n-\n@@ -197,1 +189,1 @@\n-            throw new IOException(ex.getMessage(), ex);\n+            throw ex.getCause();\n@@ -909,0 +901,22 @@\n+    @Override\n+    public void visitConstantCaseLabel(JCConstantCaseLabel tree) {\n+        try {\n+            print(tree.expr);\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n+    @Override\n+    public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n+        try {\n+            print(tree.pat);\n+            if (tree.guard != null) {\n+                print(\" when \");\n+                print(tree.guard);\n+            }\n+        } catch (IOException e) {\n+            throw new UncheckedIOException(e);\n+        }\n+    }\n+\n@@ -949,1 +963,1 @@\n-    public void visitGuardPattern(JCGuardPattern patt) {\n+    public void visitRecordPattern(JCRecordPattern tree) {\n@@ -951,3 +965,8 @@\n-            printExpr(patt.patt);\n-            print(\" && \");\n-            printExpr(patt.expr);\n+            printExpr(tree.deconstructor);\n+            print(\"(\");\n+            printExprs(tree.nested);\n+            print(\")\");\n+            if (tree.var != null) {\n+                print(\" \");\n+                print(tree.var.name);\n+            }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/Pretty.java","additions":32,"deletions":13,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2006, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2006, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -68,2 +68,2 @@\n-        if (trees == null)\n-            return null;\n+        if (trees == null || trees.isEmpty())\n+            return trees;\n@@ -509,8 +509,0 @@\n-    @DefinedBy(Api.COMPILER_TREE)\n-    public JCTree visitGuardedPattern(GuardedPatternTree node, P p) {\n-        JCGuardPattern t = (JCGuardPattern) node;\n-        JCPattern patt = copy(t.patt, p);\n-        JCExpression expr = copy(t.expr, p);\n-        return M.at(t.pos).GuardPattern(patt, expr);\n-    }\n-\n@@ -530,0 +522,24 @@\n+    @Override @DefinedBy(Api.COMPILER_TREE)\n+    public JCTree visitConstantCaseLabel(ConstantCaseLabelTree node, P p) {\n+        JCConstantCaseLabel t = (JCConstantCaseLabel) node;\n+        JCExpression expr = copy(t.expr, p);\n+        return M.at(t.pos).ConstantCaseLabel(expr);\n+    }\n+\n+    @Override\n+    public JCTree visitPatternCaseLabel(PatternCaseLabelTree node, P p) {\n+        JCPatternCaseLabel t = (JCPatternCaseLabel) node;\n+        JCPattern pat = copy(t.pat, p);\n+        JCExpression guard = copy(t.guard, p);\n+        return M.at(t.pos).PatternCaseLabel(pat, guard);\n+    }\n+\n+    @DefinedBy(Api.COMPILER_TREE)\n+    public JCTree visitDeconstructionPattern(DeconstructionPatternTree node, P p) {\n+        JCRecordPattern t = (JCRecordPattern) node;\n+        JCExpression deconstructor = copy(t.deconstructor, p);\n+        List<JCPattern> nested = copy(t.nested, p);\n+        JCVariableDecl var = copy(t.var, p);\n+        return M.at(t.pos).RecordPattern(deconstructor, nested, var);\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeCopier.java","additions":27,"deletions":11,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -550,4 +550,0 @@\n-            case GUARDPATTERN: {\n-                JCGuardPattern node = (JCGuardPattern) tree;\n-                return getStartPos(node.patt);\n-            }\n@@ -654,4 +650,0 @@\n-            case GUARDPATTERN: {\n-                JCGuardPattern node = (JCGuardPattern) tree;\n-                return getEndPos(node.expr, endPosTable);\n-            }\n@@ -803,38 +795,0 @@\n-    public static Env<AttrContext> scopeFor(JCTree node, JCCompilationUnit unit) {\n-        return scopeFor(pathFor(node, unit));\n-    }\n-\n-    public static Env<AttrContext> scopeFor(List<JCTree> path) {\n-        \/\/ TODO: not implemented yet\n-        throw new UnsupportedOperationException(\"not implemented yet\");\n-    }\n-\n-    public static List<JCTree> pathFor(final JCTree node, final JCCompilationUnit unit) {\n-        class Result extends Error {\n-            static final long serialVersionUID = -5942088234594905625L;\n-            @SuppressWarnings(\"serial\") \/\/ List not statically Serilizable\n-            List<JCTree> path;\n-            Result(List<JCTree> path) {\n-                this.path = path;\n-            }\n-        }\n-        class PathFinder extends TreeScanner {\n-            List<JCTree> path = List.nil();\n-            public void scan(JCTree tree) {\n-                if (tree != null) {\n-                    path = path.prepend(tree);\n-                    if (tree == node)\n-                        throw new Result(path);\n-                    super.scan(tree);\n-                    path = path.tail;\n-                }\n-            }\n-        }\n-        try {\n-            new PathFinder().scan(unit);\n-        } catch (Result result) {\n-            return result.path;\n-        }\n-        return List.nil();\n-    }\n-\n@@ -1359,0 +1313,2 @@\n+                             .filter(l -> l.hasTag(CONSTANTCASELABEL))\n+                             .map(l -> ((JCConstantCaseLabel) l).expr)\n@@ -1362,1 +1318,1 @@\n-    public static PatternPrimaryType primaryPatternType(JCPattern pat) {\n+    public static Type primaryPatternType(JCTree pat) {\n@@ -1364,14 +1320,1 @@\n-            case BINDINGPATTERN -> new PatternPrimaryType(((JCBindingPattern) pat).type, true);\n-            case GUARDPATTERN -> {\n-                JCGuardPattern guarded = (JCGuardPattern) pat;\n-                PatternPrimaryType nested = primaryPatternType(guarded.patt);\n-                boolean unconditional = nested.unconditional();\n-                if (guarded.expr.type.hasTag(BOOLEAN) && unconditional) {\n-                    unconditional = false;\n-                    var constValue = guarded.expr.type.constValue();\n-                    if (constValue != null && ((int) constValue) == 1) {\n-                        unconditional = true;\n-                    }\n-                }\n-                yield new PatternPrimaryType(nested.type(), unconditional);\n-            }\n+            case BINDINGPATTERN -> pat.type;\n@@ -1379,0 +1322,1 @@\n+            case RECORDPATTERN -> ((JCRecordPattern) pat).type;\n@@ -1383,1 +1327,1 @@\n-    public static JCBindingPattern primaryPatternTree(JCPattern pat) {\n+    public static JCTree primaryPatternTypeTree(JCTree pat) {\n@@ -1385,3 +1329,3 @@\n-            case BINDINGPATTERN -> (JCBindingPattern) pat;\n-            case GUARDPATTERN -> primaryPatternTree(((JCGuardPattern) pat).patt);\n-            case PARENTHESIZEDPATTERN -> primaryPatternTree(((JCParenthesizedPattern) pat).pattern);\n+            case BINDINGPATTERN -> ((JCBindingPattern) pat).var.vartype;\n+            case PARENTHESIZEDPATTERN -> primaryPatternTypeTree(((JCParenthesizedPattern) pat).pattern);\n+            case RECORDPATTERN -> ((JCRecordPattern) pat).deconstructor;\n@@ -1392,2 +1336,0 @@\n-    public record PatternPrimaryType(Type type, boolean unconditional) {}\n-\n@@ -1398,1 +1340,24 @@\n-                         .anyMatch(l -> TreeInfo.isNull(l));\n+                         .anyMatch(l -> TreeInfo.isNullCaseLabel(l));\n+    }\n+\n+    public static boolean unguardedCaseLabel(JCCaseLabel cse) {\n+        if (!cse.hasTag(PATTERNCASELABEL)) {\n+            return true;\n+        }\n+        JCExpression guard = ((JCPatternCaseLabel) cse).guard;\n+        if (guard == null) {\n+            return true;\n+        }\n+        return isBooleanWithValue(guard, 1);\n+    }\n+\n+    public static boolean isBooleanWithValue(JCExpression guard, int value) {\n+        var constValue = guard.type.constValue();\n+        return constValue != null &&\n+                guard.type.hasTag(BOOLEAN) &&\n+                ((int) constValue) == value;\n+    }\n+\n+    public static boolean isNullCaseLabel(JCCaseLabel label) {\n+        return label.hasTag(CONSTANTCASELABEL) &&\n+               TreeInfo.isNull(((JCConstantCaseLabel) label).expr);\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeInfo.java","additions":34,"deletions":69,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -509,0 +509,12 @@\n+    public JCConstantCaseLabel ConstantCaseLabel(JCExpression expr) {\n+        JCConstantCaseLabel tree = new JCConstantCaseLabel(expr);\n+        tree.pos = pos;\n+        return tree;\n+    }\n+\n+    public JCPatternCaseLabel PatternCaseLabel(JCPattern pat, JCExpression guard) {\n+        JCPatternCaseLabel tree = new JCPatternCaseLabel(pat, guard);\n+        tree.pos = pos;\n+        return tree;\n+    }\n+\n@@ -515,2 +527,3 @@\n-    public JCGuardPattern GuardPattern(JCPattern guardedPattern, JCExpression expr) {\n-        JCGuardPattern tree = new JCGuardPattern(guardedPattern, expr);\n+    public JCRecordPattern RecordPattern(JCExpression deconstructor, List<JCPattern> nested,\n+                                         JCVariableDecl var) {\n+        JCRecordPattern tree = new JCRecordPattern(deconstructor, nested, var);\n@@ -1082,2 +1095,2 @@\n-    \/** Create a a list of value parameter trees x0, ..., xn from a list of\n-     *  their types and an their owner.\n+    \/** Create a list of value parameter trees x0, ..., xn from a list of\n+     *  their types and their owner.\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeMaker.java","additions":18,"deletions":5,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -324,2 +324,13 @@\n-    public void visitParenthesizedPattern(JCParenthesizedPattern that) {\n-        scan(that.pattern);\n+    public void visitConstantCaseLabel(JCConstantCaseLabel tree) {\n+        scan(tree.expr);\n+    }\n+\n+    @Override\n+    public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n+        scan(tree.pat);\n+        scan(tree.guard);\n+    }\n+\n+    @Override\n+    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n+        scan(tree.pattern);\n@@ -329,3 +340,6 @@\n-    public void visitGuardPattern(JCGuardPattern that) {\n-        scan(that.patt);\n-        scan(that.expr);\n+    public void visitRecordPattern(JCRecordPattern that) {\n+        scan(that.deconstructor);\n+        scan(that.nested);\n+        if (that.var != null) {\n+            scan(that.var);\n+        }\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeScanner.java","additions":20,"deletions":6,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -378,2 +378,2 @@\n-    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n-        tree.pattern = translate(tree.pattern);\n+    public void visitConstantCaseLabel(JCConstantCaseLabel tree) {\n+        tree.expr = translate(tree.expr);\n@@ -384,3 +384,9 @@\n-    public void visitGuardPattern(JCGuardPattern tree) {\n-        tree.patt = translate(tree.patt);\n-        tree.expr = translate(tree.expr);\n+    public void visitPatternCaseLabel(JCPatternCaseLabel tree) {\n+        tree.pat = translate(tree.pat);\n+        tree.guard = translate(tree.guard);\n+        result = tree;\n+    }\n+\n+    @Override\n+    public void visitParenthesizedPattern(JCParenthesizedPattern tree) {\n+        tree.pattern = translate(tree.pattern);\n@@ -489,0 +495,7 @@\n+    @Override\n+    public void visitRecordPattern(JCRecordPattern tree) {\n+        tree.deconstructor = translate(tree.deconstructor);\n+        tree.nested = translate(tree.nested);\n+        result = tree;\n+    }\n+\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/tree\/TreeTranslator.java","additions":19,"deletions":6,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -73,0 +73,1 @@\n+    public final Name when;\n@@ -131,0 +132,1 @@\n+    public final Name jdk_internal_javac;\n@@ -268,0 +270,1 @@\n+        when = fromString(\"when\");\n@@ -327,0 +330,1 @@\n+        jdk_internal_javac = fromString(\"jdk.internal.javac\");\n","filename":"src\/jdk.compiler\/share\/classes\/com\/sun\/tools\/javac\/util\/Names.java","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -116,1 +116,1 @@\n-    metadataTypeArray = new Type[10];\n+    metadataTypeArray = new Type[11];\n@@ -123,5 +123,6 @@\n-    metadataTypeArray[5] = db.lookupType(\"Method\");\n-    metadataTypeArray[6] = db.lookupType(\"ObjArrayKlass\");\n-    metadataTypeArray[7] = db.lookupType(\"TypeArrayKlass\");\n-    metadataTypeArray[8] = db.lookupType(\"FlatArrayKlass\");\n-    metadataTypeArray[9] = db.lookupType(\"InlineKlass\");\n+    metadataTypeArray[5] = db.lookupType(\"InstanceStackChunkKlass\");\n+    metadataTypeArray[6] = db.lookupType(\"Method\");\n+    metadataTypeArray[7] = db.lookupType(\"ObjArrayKlass\");\n+    metadataTypeArray[8] = db.lookupType(\"TypeArrayKlass\");\n+    metadataTypeArray[9] = db.lookupType(\"FlatArrayKlass\");\n+    metadataTypeArray[10] = db.lookupType(\"InlineKlass\");\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/memory\/FileMapInfo.java","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -66,0 +66,1 @@\n+    metadataConstructor.addMapping(\"InstanceStackChunkKlass\", InstanceStackChunkKlass.class);\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Metadata.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -167,1 +167,1 @@\n-     * Reference to the C++ MethodData object.\n+     * A {@code MethodData*} value.\n@@ -169,1 +169,1 @@\n-    final long metaspaceMethodData;\n+    final long methodDataPointer;\n@@ -173,2 +173,2 @@\n-    HotSpotMethodData(long metaspaceMethodData, HotSpotResolvedJavaMethodImpl method) {\n-        this.metaspaceMethodData = metaspaceMethodData;\n+    HotSpotMethodData(long methodDataPointer, HotSpotResolvedJavaMethodImpl method) {\n+        this.methodDataPointer = methodDataPointer;\n@@ -183,1 +183,1 @@\n-        return UNSAFE.getInt(metaspaceMethodData + state.config.methodDataDataSize);\n+        return UNSAFE.getInt(methodDataPointer + state.config.methodDataDataSize);\n@@ -194,1 +194,1 @@\n-        final int extraDataLimit = UNSAFE.getInt(metaspaceMethodData + state.config.methodDataSize);\n+        final int extraDataLimit = UNSAFE.getInt(methodDataPointer + state.config.methodDataSize);\n@@ -217,1 +217,1 @@\n-        return UNSAFE.getByte(metaspaceMethodData + state.config.methodDataOopTrapHistoryOffset + reasonIndex) & 0xFF;\n+        return UNSAFE.getByte(methodDataPointer + state.config.methodDataOopTrapHistoryOffset + reasonIndex) & 0xFF;\n@@ -223,1 +223,1 @@\n-        return UNSAFE.getByte(metaspaceMethodData + state.config.methodDataOopTrapHistoryOffset + state.config.deoptReasonOSROffset + reasonIndex) & 0xFF;\n+        return UNSAFE.getByte(methodDataPointer + state.config.methodDataOopTrapHistoryOffset + state.config.deoptReasonOSROffset + reasonIndex) & 0xFF;\n@@ -227,1 +227,1 @@\n-        return UNSAFE.getInt(metaspaceMethodData + state.config.methodDataDecompiles);\n+        return UNSAFE.getInt(methodDataPointer + state.config.methodDataDecompiles);\n@@ -231,1 +231,1 @@\n-        return UNSAFE.getInt(metaspaceMethodData + state.config.methodDataOverflowRecompiles);\n+        return UNSAFE.getInt(methodDataPointer + state.config.methodDataOverflowRecompiles);\n@@ -235,1 +235,1 @@\n-        return UNSAFE.getInt(metaspaceMethodData + state.config.methodDataOverflowTraps);\n+        return UNSAFE.getInt(methodDataPointer + state.config.methodDataOverflowTraps);\n@@ -275,1 +275,1 @@\n-        return UNSAFE.getByte(metaspaceMethodData + fullOffsetInBytes) & 0xFF;\n+        return UNSAFE.getByte(methodDataPointer + fullOffsetInBytes) & 0xFF;\n@@ -280,1 +280,1 @@\n-        return UNSAFE.getShort(metaspaceMethodData + fullOffsetInBytes) & 0xFFFF;\n+        return UNSAFE.getShort(methodDataPointer + fullOffsetInBytes) & 0xFFFF;\n@@ -289,1 +289,1 @@\n-        return UNSAFE.getAddress(metaspaceMethodData + fullOffsetInBytes) & 0xFFFFFFFFL;\n+        return UNSAFE.getAddress(methodDataPointer + fullOffsetInBytes) & 0xFFFFFFFFL;\n@@ -303,1 +303,1 @@\n-        return (int) UNSAFE.getAddress(metaspaceMethodData + fullOffsetInBytes);\n+        return (int) UNSAFE.getAddress(methodDataPointer + fullOffsetInBytes);\n@@ -308,1 +308,1 @@\n-        return compilerToVM().getResolvedJavaMethod(null, metaspaceMethodData + fullOffsetInBytes);\n+        return compilerToVM().getResolvedJavaMethod(null, methodDataPointer + fullOffsetInBytes);\n@@ -313,1 +313,1 @@\n-        return compilerToVM().getResolvedJavaType(metaspaceMethodData + fullOffsetInBytes, false);\n+        return compilerToVM().getResolvedJavaType(methodDataPointer + fullOffsetInBytes);\n@@ -321,1 +321,1 @@\n-        return runtime().getCompilerToVM().isMature(metaspaceMethodData);\n+        return runtime().getCompilerToVM().isMature(methodDataPointer);\n@@ -718,1 +718,1 @@\n-            return HotSpotJVMCIRuntime.runtime().compilerToVm.methodDataProfileDataSize(data.metaspaceMethodData, position);\n+            return HotSpotJVMCIRuntime.runtime().compilerToVm.methodDataProfileDataSize(data.methodDataPointer, position);\n@@ -875,1 +875,1 @@\n-            return HotSpotJVMCIRuntime.runtime().compilerToVm.methodDataProfileDataSize(data.metaspaceMethodData, position);\n+            return HotSpotJVMCIRuntime.runtime().compilerToVm.methodDataProfileDataSize(data.methodDataPointer, position);\n@@ -886,1 +886,1 @@\n-        UNSAFE.putInt(metaspaceMethodData + state.config.methodDataIRSizeOffset, size);\n+        UNSAFE.putInt(methodDataPointer + state.config.methodDataIRSizeOffset, size);\n@@ -890,1 +890,1 @@\n-        return UNSAFE.getInt(metaspaceMethodData + state.config.methodDataIRSizeOffset);\n+        return UNSAFE.getInt(methodDataPointer + state.config.methodDataIRSizeOffset);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotMethodData.java","additions":22,"deletions":22,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -81,1 +81,1 @@\n-        int vmSize = HotSpotJVMCIRuntime.runtime().compilerToVm.methodDataProfileDataSize(data.metaspaceMethodData, position);\n+        int vmSize = HotSpotJVMCIRuntime.runtime().compilerToVm.methodDataProfileDataSize(data.methodDataPointer, position);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotMethodDataAccessor.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-    final int instanceKlassInitStateOffset = getFieldOffset(\"InstanceKlass::_init_state\", Integer.class, \"u1\");\n+    final int instanceKlassInitStateOffset = getFieldOffset(\"InstanceKlass::_init_state\", Integer.class, \"InstanceKlass::ClassState\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -420,1 +420,1 @@\n-         * There is an implict VM-wide suspend at the conclusion\n+         * There is an implicit VM-wide suspend at the conclusion\n","filename":"src\/jdk.jdi\/share\/classes\/com\/sun\/tools\/jdi\/ObjectReferenceImpl.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -1042,1 +1042,1 @@\n-     * b- gotten a suprious wakeup\n+     * b- gotten a spurious wakeup\n@@ -1550,0 +1550,7 @@\n+jboolean\n+isVThread(jobject object)\n+{\n+    JNIEnv *env = getEnv();\n+    return JNI_FUNC_PTR(env,IsVirtualThread)(env, object);\n+}\n+\n@@ -1943,0 +1950,2 @@\n+    index2jvmti[EI_VIRTUAL_THREAD_START -EI_min] = JVMTI_EVENT_VIRTUAL_THREAD_START;\n+    index2jvmti[EI_VIRTUAL_THREAD_END   -EI_min] = JVMTI_EVENT_VIRTUAL_THREAD_END;\n@@ -1964,0 +1973,3 @@\n+    \/* Just map VIRTUAL_THREAD_START\/END to THREAD_START\/END. *\/\n+    index2jdwp[EI_VIRTUAL_THREAD_START -EI_min] = JDWP_EVENT(THREAD_START);\n+    index2jdwp[EI_VIRTUAL_THREAD_END   -EI_min] = JDWP_EVENT(THREAD_END);\n@@ -2030,0 +2042,4 @@\n+        case EI_VIRTUAL_THREAD_START:\n+            return \"EI_VIRTUAL_THREAD_START\";\n+        case EI_VIRTUAL_THREAD_END:\n+            return \"EI_VIRTUAL_THREAD_END\";\n@@ -2143,0 +2159,6 @@\n+        \/* vthread events *\/\n+        case JVMTI_EVENT_VIRTUAL_THREAD_START:\n+            return EI_VIRTUAL_THREAD_START;\n+        case JVMTI_EVENT_VIRTUAL_THREAD_END:\n+            return EI_VIRTUAL_THREAD_END;\n+\n@@ -2256,0 +2278,2 @@\n+        case JVMTI_ERROR_UNSUPPORTED_OPERATION:\n+            return JDWP_ERROR(NOT_IMPLEMENTED);\n@@ -2411,1 +2435,1 @@\n-        LOG_LOC((\"%s: debugee: thread=%p(%s:0x%x),method=%p(%s@%d;%s)\",\n+        LOG_LOC((\"%s: debuggee: thread=%p(%s:0x%x),method=%p(%s@%d;%s)\",\n","filename":"src\/jdk.jdwp.agent\/share\/native\/libjdwp\/util.c","additions":27,"deletions":3,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -70,5 +70,0 @@\n-compiler\/whitebox\/ClearMethodStateTest.java 8265360 macosx-all\n-compiler\/whitebox\/EnqueueMethodForCompilationTest.java 8265360 macosx-all\n-compiler\/whitebox\/MakeMethodNotCompilableTest.java 8265360 macosx-all\n-\n-compiler\/codecache\/TestStressCodeBuffers.java 8272094 generic-aarch64\n@@ -90,0 +85,1 @@\n+gc\/stress\/TestStressG1Humongous.java 8286554 windows-x64\n@@ -107,1 +103,1 @@\n-containers\/docker\/TestJcmd.java 8278102 linux-aarch64\n+containers\/docker\/TestJcmd.java 8278102 linux-all\n@@ -121,0 +117,1 @@\n+serviceability\/jvmti\/vthread\/GetSetLocalTest\/GetSetLocalTest.java 8286836 generic-all\n@@ -171,8 +168,0 @@\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isCollectionUsageThresholdExceeded\/isexceeded003\/TestDescription.java 8153598 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded001\/TestDescription.java 8198668 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded002\/TestDescription.java 8153598 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded003\/TestDescription.java 8198668 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded004\/TestDescription.java 8153598 generic-all\n-vmTestbase\/nsk\/monitoring\/MemoryPoolMBean\/isUsageThresholdExceeded\/isexceeded005\/TestDescription.java 8153598 generic-all\n-\n-vmTestbase\/nsk\/jdi\/HiddenClass\/events\/events001.java                 8257705 generic-all\n@@ -191,0 +180,1 @@\n+vmTestbase\/nsk\/jvmti\/scenarios\/capability\/CM03\/cm03t001\/TestDescription.java 8073470 linux-all\n@@ -204,2 +194,0 @@\n-\n-#############################################################################\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":4,"deletions":16,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n-# Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n+# Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -95,0 +95,1 @@\n+  compiler\/c2\/irTests \\\n@@ -119,0 +120,16 @@\n+tier1_loom = \\\n+  :tier1_loom_runtime \\\n+  :tier1_loom_serviceability\n+\n+tier1_loom_runtime = \\\n+  runtime\/vthread \\\n+  runtime\/jni\/IsVirtualThread\n+\n+tier1_loom_serviceability = \\\n+  serviceability\/jvmti\/vthread \\\n+  serviceability\/jvmti\/events \\\n+  serviceability\/dcmd\/thread\n+\n+hotspot_loom = \\\n+  :tier1_loom\n+\n@@ -141,0 +158,1 @@\n+  resourcehogs\/compiler \\\n@@ -441,0 +459,1 @@\n+ -runtime\/cds\/appcds\/LambdaWithUseImplMethodHandle.java \\\n@@ -693,3 +712,0 @@\n-  vmTestbase\/gc\/gctests\/mallocWithGC1\/mallocWithGC1.java \\\n-  vmTestbase\/gc\/gctests\/mallocWithGC2\/mallocWithGC2.java \\\n-  vmTestbase\/gc\/gctests\/mallocWithGC3\/mallocWithGC3.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -0,0 +1,118 @@\n+\/*\n+ * Copyright (c) 2022, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\/*\n+ * @test\n+ * @library \/test\/lib\n+ * @run main\/othervm -Xint\n+ *                   -XX:CompileCommand=dontinline,*TestGetModifiers.test\n+ *                   compiler.intrinsics.klass.TestGetModifiers\n+ *\/\n+\n+\/*\n+ * @test\n+ * @requires vm.compiler1.enabled\n+ * @library \/test\/lib\n+ * @run main\/othervm -XX:TieredStopAtLevel=1 -XX:+TieredCompilation\n+ *                   -XX:CompileCommand=dontinline,*TestGetModifiers.test\n+ *                   compiler.intrinsics.klass.TestGetModifiers\n+ *\/\n+\n+\/*\n+ * @test\n+ * @requires vm.compiler2.enabled\n+ * @library \/test\/lib\n+ * @run main\/othervm -XX:-TieredCompilation\n+ *                   -XX:CompileCommand=dontinline,*TestGetModifiers.test\n+ *                   compiler.intrinsics.klass.TestGetModifiers\n+ *\/\n+\n+package compiler.intrinsics.klass;\n+\n+import java.lang.reflect.Modifier;\n+import static java.lang.reflect.Modifier.*;\n+\n+import jdk.test.lib.Asserts;\n+\n+public class TestGetModifiers {\n+    public static class T1 {\n+    }\n+\n+    public static final class T2 {\n+    }\n+\n+    private static class T3 {\n+    }\n+\n+    protected static class T4 {\n+    }\n+\n+    class T5 {\n+    }\n+\n+    interface T6 {\n+    }\n+\n+    static void test(Class cl, int expectedMods) {\n+        for (int i = 0; i < 100_000; i++) {\n+            int actualMods = cl.getModifiers();\n+            if (actualMods != expectedMods) {\n+                throw new IllegalStateException(\"Error with: \" + cl);\n+            }\n+        }\n+    }\n+\n+    public static void main(String... args) {\n+        test(T1.class,                                      PUBLIC | STATIC | IDENTITY);\n+        test(T2.class,                                      PUBLIC | FINAL | STATIC | IDENTITY);\n+        test(T3.class,                                      PRIVATE | STATIC | IDENTITY);\n+        test(T4.class,                                      PROTECTED | STATIC | IDENTITY);\n+        test(new TestGetModifiers().new T5().getClass(),    IDENTITY);\n+        test(T6.class,                                      ABSTRACT | STATIC | INTERFACE);\n+\n+        test(int.class,                                     PUBLIC | ABSTRACT | FINAL);\n+        test(long.class,                                    PUBLIC | ABSTRACT | FINAL);\n+        test(double.class,                                  PUBLIC | ABSTRACT | FINAL);\n+        test(float.class,                                   PUBLIC | ABSTRACT | FINAL);\n+        test(char.class,                                    PUBLIC | ABSTRACT | FINAL);\n+        test(byte.class,                                    PUBLIC | ABSTRACT | FINAL);\n+        test(short.class,                                   PUBLIC | ABSTRACT | FINAL);\n+        test(void.class,                                    PUBLIC | ABSTRACT | FINAL);\n+        test(int[].class,                                   PUBLIC | ABSTRACT | FINAL);\n+        test(long[].class,                                  PUBLIC | ABSTRACT | FINAL);\n+        test(double[].class,                                PUBLIC | ABSTRACT | FINAL);\n+        test(float[].class,                                 PUBLIC | ABSTRACT | FINAL);\n+        test(char[].class,                                  PUBLIC | ABSTRACT | FINAL);\n+        test(byte[].class,                                  PUBLIC | ABSTRACT | FINAL);\n+        test(short[].class,                                 PUBLIC | ABSTRACT | FINAL);\n+        test(Object[].class,                                PUBLIC | ABSTRACT | FINAL);\n+        test(TestGetModifiers[].class,                      PUBLIC | ABSTRACT | FINAL);\n+\n+        test(new TestGetModifiers().getClass(),             PUBLIC | IDENTITY);\n+        test(new T1().getClass(),                           PUBLIC | STATIC | IDENTITY);\n+        test(new T2().getClass(),                           PUBLIC | FINAL | STATIC | IDENTITY);\n+        test(new T3().getClass(),                           PRIVATE | STATIC | IDENTITY);\n+        test(new T4().getClass(),                           PROTECTED | STATIC | IDENTITY);\n+    }\n+}\n","filename":"test\/hotspot\/jtreg\/compiler\/intrinsics\/klass\/TestGetModifiers.java","additions":118,"deletions":0,"binary":false,"changes":118,"status":"added"},{"patch":"@@ -94,0 +94,11 @@\n+    @Test\n+    public void equalsTest() {\n+        for (ResolvedJavaType t : javaTypes) {\n+            for (ResolvedJavaType that : javaTypes) {\n+                boolean expect = t == that;\n+                boolean actual = t.equals(that);\n+                assertEquals(expect, actual);\n+            }\n+        }\n+    }\n+\n@@ -179,1 +190,2 @@\n-    @Test\n+    \/\/ TODO Re-enable once JDK-8291719 is fixed.\n+    \/\/ @Test\n","filename":"test\/hotspot\/jtreg\/compiler\/jvmci\/jdk.vm.ci.runtime.test\/src\/jdk\/vm\/ci\/runtime\/test\/TestResolvedJavaType.java","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -26,1 +26,3 @@\n-import compiler.lib.ir_framework.driver.*;\n+import compiler.lib.ir_framework.driver.FlagVMProcess;\n+import compiler.lib.ir_framework.driver.TestVMException;\n+import compiler.lib.ir_framework.driver.TestVMProcess;\n@@ -30,1 +32,1 @@\n-import compiler.lib.ir_framework.test.*;\n+import compiler.lib.ir_framework.test.TestVM;\n@@ -34,1 +36,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -39,0 +41,4 @@\n+import java.net.MalformedURLException;\n+import java.net.URL;\n+import java.net.URLClassLoader;\n+import java.nio.file.Path;\n@@ -130,0 +136,3 @@\n+                    \"UseAVX\",\n+                    \"UseSSE\",\n+                    \"UseSVE\",\n@@ -140,1 +149,0 @@\n-    private static final boolean SKIP_WHITEBOX_INSTALL = Boolean.getBoolean(\"SkipWhiteBoxInstall\");\n@@ -317,1 +325,1 @@\n-        if (!SKIP_WHITEBOX_INSTALL) {\n+        if (shouldInstallWhiteBox()) {\n@@ -339,0 +347,22 @@\n+    \/**\n+     * Try to load the Whitebox class from the user directory with a custom class loader. If the user has already built the\n+     * Whitebox, we can load it. Otherwise, the framework needs to install it.\n+     *\n+     * @return true if the framework needs to install the Whitebox\n+     *\/\n+    private boolean shouldInstallWhiteBox() {\n+        try {\n+            URL url = Path.of(System.getProperty(\"user.dir\")).toUri().toURL();\n+            URLClassLoader userDirClassLoader =\n+                    URLClassLoader.newInstance(new URL[] {url}, TestFramework.class.getClassLoader().getParent());\n+            Class.forName(WhiteBox.class.getName(), false, userDirClassLoader);\n+        } catch (MalformedURLException e) {\n+            throw new TestFrameworkException(\"corrupted user.dir property\", e);\n+        } catch (ClassNotFoundException e) {\n+            \/\/ We need to manually install the WhiteBox if we cannot load the WhiteBox class from the user directory.\n+            \/\/ This happens when the user test does not explicitly install the WhiteBox as part of the test.\n+            return true;\n+        }\n+        return false;\n+    }\n+\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/TestFramework.java","additions":35,"deletions":5,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/compiler\/lib\/ir_framework\/test\/TestVM.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -34,1 +34,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n","filename":"test\/hotspot\/jtreg\/compiler\/tiered\/ConstantGettersTransitionsTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -34,2 +34,2 @@\n- * @build sun.hotspot.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -88,1 +88,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -220,1 +220,1 @@\n-     * This will epectedly fail with an \"IncompatibleClassChangeError\" because the objects returned\n+     * This will expectedly fail with an \"IncompatibleClassChangeError\" because the objects returned\n","filename":"test\/hotspot\/jtreg\/compiler\/types\/TestMeetIncompatibleInterfaceArrays.java","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -164,1 +164,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -324,1 +324,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -475,1 +475,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -631,1 +631,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/GetfieldChains.jcod","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/MyValue5.jcod","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -62,1 +62,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestC2CCalls.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestCallingConventionC1.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -38,1 +38,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -40,1 +40,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestDeoptimizationWhenBuffering.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-    version 63:0\n+    version 64:0\n@@ -32,1 +32,1 @@\n-    \n+\n@@ -38,1 +38,1 @@\n-        \n+\n@@ -40,1 +40,1 @@\n-        \n+\n@@ -43,1 +43,1 @@\n-        \n+\n@@ -46,1 +46,1 @@\n-        \n+\n@@ -49,1 +49,1 @@\n-        \n+\n@@ -56,1 +56,1 @@\n-        \n+\n@@ -61,1 +61,1 @@\n-        \n+\n@@ -66,1 +66,1 @@\n-        \n+\n@@ -80,1 +80,1 @@\n-    version 63:0\n+    version 64:0\n@@ -84,1 +84,1 @@\n-    \n+\n@@ -90,1 +90,1 @@\n-        \n+\n@@ -92,1 +92,1 @@\n-        \n+\n@@ -95,1 +95,1 @@\n-        \n+\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestDeoptimizationWhenBufferingClasses.jasm","additions":15,"deletions":15,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -2465,0 +2465,1 @@\n+        Asserts.assertFalse(test91(new MyValue2.ref[1]));\n@@ -3141,2 +3142,4 @@\n-    @IR(failOn = {ALLOC_G, MEMBAR},\n-        counts = {PREDICATE_TRAP, \"= 1\"})\n+\/\/ TODO Tobias\n+\/\/    @IR(failOn = {ALLOC_G, MEMBAR},\n+\/\/        counts = {PREDICATE_TRAP, \"= 1\"})\n+    @IR(failOn = {ALLOC_G, MEMBAR})\n@@ -3178,2 +3181,4 @@\n-    @IR(failOn = {ALLOC_G, MEMBAR},\n-        counts = {PREDICATE_TRAP, \"= 1\"})\n+\/\/ TODO Tobias\n+\/\/    @IR(failOn = {ALLOC_G, MEMBAR},\n+\/\/        counts = {PREDICATE_TRAP, \"= 1\"})\n+    @IR(failOn = {ALLOC_G, MEMBAR})\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestLWorld.java","additions":9,"deletions":4,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestLWorldProfiling.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestMethodHandles.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -50,1 +50,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestNewAcmp.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -48,1 +48,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestTrivialMethods.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,2 +28,2 @@\n- * @build sun.hotspot.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -37,1 +37,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestUnloadedReturnTypes.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-    version 63:0\n+    version 64:0\n@@ -39,1 +39,1 @@\n-        \n+\n@@ -41,1 +41,1 @@\n-        \n+\n@@ -44,1 +44,1 @@\n-        \n+\n@@ -54,1 +54,1 @@\n-        \n+\n@@ -56,1 +56,1 @@\n-        \n+\n@@ -59,1 +59,1 @@\n-        \n+\n@@ -69,1 +69,1 @@\n-        \n+\n@@ -71,1 +71,1 @@\n-        \n+\n@@ -74,1 +74,1 @@\n-        \n+\n@@ -83,1 +83,1 @@\n-        \n+\n@@ -87,1 +87,1 @@\n-            \n+\n@@ -90,1 +90,1 @@\n-            \n+\n@@ -92,1 +92,1 @@\n-            \n+\n@@ -94,1 +94,1 @@\n-        \n+\n@@ -105,1 +105,1 @@\n-        \n+\n@@ -108,1 +108,1 @@\n-        \n+\n@@ -110,1 +110,1 @@\n-        \n+\n@@ -113,1 +113,1 @@\n-        \n+\n@@ -116,1 +116,1 @@\n-    \n+\n@@ -123,1 +123,1 @@\n-        \n+\n@@ -126,1 +126,1 @@\n-        \n+\n@@ -129,1 +129,1 @@\n-        \n+\n@@ -139,1 +139,1 @@\n-        \n+\n@@ -142,1 +142,1 @@\n-        \n+\n@@ -145,1 +145,1 @@\n-        \n+\n@@ -155,1 +155,1 @@\n-        \n+\n@@ -157,1 +157,1 @@\n-        \n+\n@@ -160,1 +160,1 @@\n-        \n+\n@@ -163,1 +163,1 @@\n-    \n+\n@@ -170,1 +170,1 @@\n-    \n+\n@@ -177,1 +177,1 @@\n-        \n+\n@@ -179,1 +179,1 @@\n-        \n+\n@@ -182,1 +182,1 @@\n-        \n+\n@@ -185,1 +185,1 @@\n-        \n+\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/TestWithfieldC1Classes.jasm","additions":35,"deletions":35,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -36,2 +36,2 @@\n- * @build sun.hotspot.WhiteBox TestBootClassloader InstallBootstrapClasses\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox TestBootClassloader InstallBootstrapClasses\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n","filename":"test\/hotspot\/jtreg\/compiler\/valhalla\/inlinetypes\/bootstrap\/TestBootClassloader.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -51,0 +51,1 @@\n+import jdk.test.lib.Platform;\n@@ -58,1 +59,0 @@\n-\n@@ -74,1 +74,2 @@\n-                crashInNative1(10);\n+                \/\/ Low address reads are allowed on PPC\n+                crashInNative1(Platform.isPPC() ? -1 : 10);\n@@ -130,3 +131,2 @@\n-            throw new RuntimeException(\"Did not find hs_err_pid file in output.\\n\" +\n-                                       \"stderr:\\n\" + output.getStderr() + \"\\n\" +\n-                                       \"stdout:\\n\" + output.getStdout());\n+            output.reportDiagnosticSummary();\n+            throw new RuntimeException(\"Did not find hs_err_pid file in output\");\n@@ -157,0 +157,11 @@\n+        String preCodeBlobSectionHeader = \"Stack slot to memory mapping:\";\n+        if (!hsErr.contains(preCodeBlobSectionHeader) &&\n+            System.getProperty(\"os.arch\").equals(\"aarch64\") &&\n+            System.getProperty(\"os.name\").toLowerCase().startsWith(\"mac\")) {\n+            \/\/ JDK-8282607: hs_err can be truncated. If the section preceding\n+            \/\/ code blob dumping is missing, exit successfully.\n+            System.out.println(\"Could not find \\\"\" + preCodeBlobSectionHeader + \"\\\" in \" + hsErrPath);\n+            System.out.println(\"Looks like hs-err is truncated - exiting with success\");\n+            return;\n+        }\n+\n@@ -167,1 +178,1 @@\n-     * \"Native frames:\" or \"Java frames:\" up to the next blank line\n+     * \"Native frame\" or \"Java frame\" up to the next blank line\n@@ -171,1 +182,2 @@\n-        String marker = (nativeStack ? \"Native\" : \"Java\") + \" frames: \";\n+        String marker = (nativeStack ? \"Native\" : \"Java\") + \" frame\";\n+\n@@ -183,1 +195,2 @@\n-        throw new RuntimeException(\"\\\"\" + marker + \"\\\" line missing in hs_err_pid file:\\n\" + hsErr);\n+        System.err.println(hsErr);\n+        throw new RuntimeException(\"\\\"\" + marker + \"\\\" line missing in hs_err_pid file\");\n","filename":"test\/hotspot\/jtreg\/runtime\/ErrorHandling\/MachCodeFramesInErrorFile.java","additions":22,"deletions":9,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -32,2 +32,2 @@\n- * @build sun.hotspot.WhiteBox\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n@@ -42,1 +42,1 @@\n-    String wbJar = JarBuilder.build(true, \"WhiteBox\", \"sun\/hotspot\/WhiteBox\");\n+    String wbJar = JarBuilder.build(true, \"WhiteBox\", \"jdk\/test\/whitebox\/WhiteBox\");\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/RewriteBytecodesInlineTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -34,1 +34,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -36,1 +36,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/DynamicArchiveRelocationTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n- * @build sun.hotspot.WhiteBox\n+ * @build jdk.test.whitebox.WhiteBox\n@@ -33,1 +33,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox sun.hotspot.WhiteBox$WhiteBoxPermission\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox jdk.test.whitebox.WhiteBox$WhiteBoxPermission\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/dynamicArchive\/HelloDynamicInlineClass.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,1 +27,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\/RewriteBytecodesInline.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/HiddenPoint.jcod","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,1 +32,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -42,2 +42,2 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n- *                   sun.hotspot.WhiteBox$WhiteBoxPermission\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ *                   jdk.test.whitebox.WhiteBox$WhiteBoxPermission\n@@ -56,2 +56,2 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n- *                   sun.hotspot.WhiteBox$WhiteBoxPermission\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ *                   jdk.test.whitebox.WhiteBox$WhiteBoxPermission\n@@ -70,2 +70,2 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n- *                   sun.hotspot.WhiteBox$WhiteBoxPermission\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ *                   jdk.test.whitebox.WhiteBox$WhiteBoxPermission\n@@ -84,2 +84,2 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n- *                   sun.hotspot.WhiteBox$WhiteBoxPermission\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n+ *                   jdk.test.whitebox.WhiteBox$WhiteBoxPermission\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/InlineOops.java","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,1 +27,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -35,1 +35,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/InlineTypeDensity.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/MultiANewArrayTest\/MultiANewArrayTypeCheck.jcod","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-    version 63:0\n+    version 64:0\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/TestFieldNullabilityClasses.jasm","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-super public class runtime\/valhalla\/inlinetypes\/TestFieldTypeMismatchClass version 63:0 {\n+super public class runtime\/valhalla\/inlinetypes\/TestFieldTypeMismatchClass version 64:0 {\n@@ -45,1 +45,1 @@\n-super public final class runtime\/valhalla\/inlinetypes\/MyValue version 63:0 {\n+super public final class runtime\/valhalla\/inlinetypes\/MyValue version 64:0 {\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/TestFieldTypeMismatchClasses.jasm","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -39,1 +39,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/ValuePreloadClient1.jcod","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -35,1 +35,1 @@\n-import sun.hotspot.WhiteBox;\n+import jdk.test.whitebox.WhiteBox;\n@@ -44,1 +44,1 @@\n- * @run driver jdk.test.lib.helpers.ClassFileInstaller sun.hotspot.WhiteBox\n+ * @run driver jdk.test.lib.helpers.ClassFileInstaller jdk.test.whitebox.WhiteBox\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/ValueTearing.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -25,1 +25,1 @@\n-    version 63:0\n+    version 64:0\n@@ -109,1 +109,1 @@\n-    version 63:0\n+    version 64:0\n@@ -152,1 +152,1 @@\n-    version 63:0\n+    version 64:0\n@@ -194,1 +194,1 @@\n-    version 63:0\n+    version 64:0\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/WithFieldAccessorTestClasses.jasm","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-    version 63:0\n+    version 64:0\n@@ -107,1 +107,1 @@\n-    version 63:0\n+    version 64:0\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/WithFieldTestClasses.jasm","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-  62; \/\/ version\n+  64; \/\/ version\n@@ -184,1 +184,1 @@\n-  62; \/\/ version\n+  64; \/\/ version\n@@ -326,1 +326,1 @@\n-  62; \/\/ version\n+  64; \/\/ version\n@@ -468,1 +468,1 @@\n-  62; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/classfileparser\/ACCCFETests.jcod","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-  62; \/\/ version\n+  64; \/\/ version\n@@ -194,1 +194,1 @@\n-  62; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/classfileparser\/ACCICCETests.jcod","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -207,1 +207,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -363,1 +363,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -519,1 +519,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -626,1 +626,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -881,1 +881,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1160,1 +1160,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1474,1 +1474,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1752,1 +1752,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -2039,1 +2039,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -2500,1 +2500,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/classfileparser\/cfpTests.jcod","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -89,1 +89,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -150,1 +150,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -211,1 +211,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -275,1 +275,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -338,1 +338,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -404,1 +404,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -476,1 +476,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -551,1 +551,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -623,1 +623,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -711,1 +711,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -774,1 +774,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/testClassModifiers\/ClassesWithInvalidModifiers.jcod","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -42,1 +42,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -302,1 +302,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -562,1 +562,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -821,1 +821,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1082,1 +1082,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1342,1 +1342,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1602,1 +1602,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1862,1 +1862,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -2121,1 +2121,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -2382,1 +2382,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/testSupers\/InlineClassWithBadSupers.jcod","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -265,1 +265,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -366,1 +366,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -457,1 +457,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -547,1 +547,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -739,1 +739,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -830,1 +830,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1054,1 +1054,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1236,1 +1236,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/verifier\/verifierTests.jcod","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -187,1 +187,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -344,1 +344,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -485,1 +485,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -613,1 +613,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -718,1 +718,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -807,1 +807,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -896,1 +896,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n@@ -1022,1 +1022,1 @@\n-  63; \/\/ version\n+  64; \/\/ version\n","filename":"test\/hotspot\/jtreg\/runtime\/valhalla\/inlinetypes\/withfieldTests\/withfieldTests.jcod","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -43,1 +43,1 @@\n-    private final String OBJECT_WAIT = \"java.lang.Object.wait\";\n+    private final String OBJECT_WAIT = \"java.lang.Object.wait0\";\n","filename":"test\/hotspot\/jtreg\/serviceability\/tmtools\/jstack\/WaitNotifyThreadTest.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1,1 +1,1 @@\n-#  Copyright (c) 2013, 2021, Oracle and\/or its affiliates. All rights reserved.\n+#  Copyright (c) 2013, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -113,0 +113,3 @@\n+    jdk\/internal\/vm \\\n+    jdk\/modules \\\n+    jni\/nullCaller \\\n@@ -275,0 +278,13 @@\n+jdk_loom = \\\n+    com\/sun\/management\/HotSpotDiagnosticMXBean\/ \\\n+    java\/lang\/Thread \\\n+    java\/lang\/ThreadGroup \\\n+    java\/lang\/management\/ThreadMXBean \\\n+    java\/util\/concurrent \\\n+    java\/net\/vthread \\\n+    java\/nio\/channels\/vthread \\\n+    jdk\/incubator\/concurrent \\\n+    jdk\/internal\/misc\/ThreadFlock \\\n+    jdk\/internal\/vm\/Continuation \\\n+    jdk\/jfr\/threading\n+\n@@ -310,0 +326,1 @@\n+    jdk\/incubator\/concurrent \\\n@@ -489,0 +506,1 @@\n+    com\/sun\/jdi\/SuspendAfterDeath.java \\\n@@ -578,1 +596,3 @@\n-    jdk\/nio\/zipfs\/TestLocOffsetFromZip64EF.java\n+    jdk\/nio\/zipfs\/TestLocOffsetFromZip64EF.java \\\n+    java\/util\/ArrayList\/Bug8146568.java \\\n+    java\/util\/Vector\/Bug8148174.java\n","filename":"test\/jdk\/TEST.groups","additions":22,"deletions":2,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -220,1 +220,1 @@\n-                    .withMajorVersion(63)\n+                    .withMajorVersion(64)\n","filename":"test\/jdk\/java\/lang\/invoke\/common\/test\/java\/lang\/invoke\/lib\/InstructionHelper.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -115,0 +115,1 @@\n+        map.put(\"vm.continuations\", this::vmContinuations);\n@@ -425,0 +426,11 @@\n+    \/**\n+     * @return \"true\" if this VM supports continuations.\n+     *\/\n+    protected String vmContinuations() {\n+        if (WB.getBooleanVMFlag(\"VMContinuations\")) {\n+            return \"true\";\n+        } else {\n+            return \"false\";\n+        }\n+    }\n+\n","filename":"test\/jtreg-ext\/requires\/VMProps.java","additions":13,"deletions":1,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -60,1 +60,0 @@\n-compiler.misc.base.membership                           # UNUSED\n","filename":"test\/langtools\/tools\/javac\/diags\/examples.not-yet.txt","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,1 +28,1 @@\n- * @bug 8250629 8252307 8247352 8241151 8246774 8259025\n+ * @bug 8250629 8252307 8247352 8241151 8246774 8259025 8288130 8282714\n@@ -180,0 +180,17 @@\n+        assertOK(\n+                \"\"\"\n+                record R<T>(T x) {\n+                    public T x() {\n+                        return this.x;\n+                    }\n+                }\n+                \"\"\");\n+        assertOK(\n+                \"\"\"\n+                import java.util.List;\n+                record R<T>(List<T> x) {\n+                    public List<T> x() {\n+                        return this.x;\n+                    }\n+                }\n+                \"\"\");\n@@ -1245,12 +1262,41 @@\n-        int numberOfFieldRefs = 0;\n-        File dir = assertOK(true, \"record R(int recordComponent) {}\");\n-        for (final File fileEntry : dir.listFiles()) {\n-            if (fileEntry.getName().equals(\"R.class\")) {\n-                ClassFile classFile = ClassFile.read(fileEntry);\n-                for (CPInfo cpInfo : classFile.constant_pool.entries()) {\n-                    if (cpInfo instanceof ConstantPool.CONSTANT_Fieldref_info) {\n-                        numberOfFieldRefs++;\n-                        ConstantPool.CONSTANT_NameAndType_info nameAndType =\n-                                (ConstantPool.CONSTANT_NameAndType_info)classFile.constant_pool\n-                                        .get(((ConstantPool.CONSTANT_Fieldref_info)cpInfo).name_and_type_index);\n-                        Assert.check(nameAndType.getName().equals(\"recordComponent\"));\n+        for (String source : List.of(\n+                \"record R(int recordComponent) {}\",\n+                \"\"\"\n+                class Test {\n+                    class Inner {\n+                        Inner() {\n+                            record R(int recordComponent) {}\n+                        }\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                class Test {\n+                    class Inner {\n+                        void m() {\n+                            record R(int recordComponent) {}\n+                        }\n+                    }\n+                }\n+                \"\"\",\n+                \"\"\"\n+                class Test {\n+                    void m() {\n+                        record R(int recordComponent) {}\n+                    }\n+                }\n+                \"\"\"\n+        )) {\n+            File dir = assertOK(true, source);\n+            int numberOfFieldRefs = 0;\n+            for (final File fileEntry : dir.listFiles()) {\n+                if (fileEntry.getName().endsWith(\"R.class\")) {\n+                    ClassFile classFile = ClassFile.read(fileEntry);\n+                    for (CPInfo cpInfo : classFile.constant_pool.entries()) {\n+                        if (cpInfo instanceof ConstantPool.CONSTANT_Fieldref_info) {\n+                            numberOfFieldRefs++;\n+                            ConstantPool.CONSTANT_NameAndType_info nameAndType =\n+                                    (ConstantPool.CONSTANT_NameAndType_info)classFile.constant_pool\n+                                            .get(((ConstantPool.CONSTANT_Fieldref_info)cpInfo).name_and_type_index);\n+                            Assert.check(nameAndType.getName().equals(\"recordComponent\"));\n+                        }\n@@ -1258,0 +1304,1 @@\n+                    Assert.check(numberOfFieldRefs == 1);\n@@ -1261,1 +1308,0 @@\n-        Assert.check(numberOfFieldRefs == 1);\n@@ -1264,3 +1310,2 @@\n-    \/*  check that fields are initialized in a canonical constructor in the same declaration order as the corresponding\n-     *  record component\n-     *\/\n+    \/\/  check that fields are initialized in a canonical constructor in the same declaration order as the corresponding\n+    \/\/  record component\n@@ -1307,10 +1352,13 @@\n-        String[] testOptions = {\/* no options *\/};\n-        setCompileOptions(testOptions);\n-        assertFail(\"compiler.err.illegal.start.of.type\",\n-                \"class R {\\n\" +\n-                \"    record RR(int i) {\\n\" +\n-                \"        return null;\\n\" +\n-                \"    }\\n\" +\n-                \"    class record {}\\n\" +\n-                \"}\");\n-        setCompileOptions(previousOptions);\n+        try {\n+            String[] testOptions = {};\n+            setCompileOptions(testOptions);\n+            assertFail(\"compiler.err.illegal.start.of.type\",\n+                    \"class R {\\n\" +\n+                            \"    record RR(int i) {\\n\" +\n+                            \"        return null;\\n\" +\n+                            \"    }\\n\" +\n+                            \"    class record {}\\n\" +\n+                            \"}\");\n+        } finally {\n+            setCompileOptions(previousOptions);\n+        }\n@@ -1321,2 +1369,3 @@\n-        String srcTemplate =\n-                \"\"\"\n+        try {\n+            String srcTemplate =\n+                    \"\"\"\n@@ -1327,73 +1376,30 @@\n-\n-                \"\"\";\n-\n-        \/\/ testing several combinations, adding even more combinations won't add too much value\n-        List<String> annoApplicableTargets = List.of(\n-                \"ElementType.FIELD\",\n-                \"ElementType.METHOD\",\n-                \"ElementType.PARAMETER\",\n-                \"ElementType.RECORD_COMPONENT\",\n-                \"ElementType.TYPE_USE\",\n-                \"ElementType.TYPE_USE,ElementType.FIELD\",\n-                \"ElementType.TYPE_USE,ElementType.METHOD\",\n-                \"ElementType.TYPE_USE,ElementType.PARAMETER\",\n-                \"ElementType.TYPE_USE,ElementType.RECORD_COMPONENT\",\n-                \"ElementType.TYPE_USE,ElementType.FIELD,ElementType.METHOD\",\n-                \"ElementType.TYPE_USE,ElementType.FIELD,ElementType.PARAMETER\",\n-                \"ElementType.TYPE_USE,ElementType.FIELD,ElementType.RECORD_COMPONENT\",\n-                \"ElementType.FIELD,ElementType.TYPE_USE\",\n-                \"ElementType.FIELD,ElementType.CONSTRUCTOR\",\n-                \"ElementType.FIELD,ElementType.LOCAL_VARIABLE\",\n-                \"ElementType.FIELD,ElementType.ANNOTATION_TYPE\",\n-                \"ElementType.FIELD,ElementType.PACKAGE\",\n-                \"ElementType.FIELD,ElementType.TYPE_PARAMETER\",\n-                \"ElementType.FIELD,ElementType.MODULE\",\n-                \"ElementType.METHOD,ElementType.TYPE_USE\",\n-                \"ElementType.PARAMETER,ElementType.TYPE_USE\",\n-                \"ElementType.RECORD_COMPONENT,ElementType.TYPE_USE\",\n-                \"ElementType.FIELD,ElementType.METHOD,ElementType.TYPE_USE\",\n-                \"ElementType.FIELD,ElementType.PARAMETER,ElementType.TYPE_USE\",\n-                \"ElementType.FIELD,ElementType.RECORD_COMPONENT,ElementType.TYPE_USE\"\n-        );\n-\n-        String[] generalOptions = {\n-                \"-processor\", Processor.class.getName(),\n-                \"-Atargets=\"\n-        };\n-\n-        for (String target : annoApplicableTargets) {\n-            String code = srcTemplate.replaceFirst(\"#TARGET\", target);\n-            String[] testOptions = generalOptions.clone();\n-            testOptions[testOptions.length - 1] = testOptions[testOptions.length - 1] + target;\n-            setCompileOptions(testOptions);\n-\n-            File dir = assertOK(true, code);\n-\n-            ClassFile classFile = ClassFile.read(findClassFileOrFail(dir, \"R.class\"));\n-\n-            \/\/ field first\n-            Assert.check(classFile.fields.length == 1);\n-            Field field = classFile.fields[0];\n-            \/* if FIELD is one of the targets then there must be a declaration annotation applied to the field, apart from\n-             * the type annotation\n-             *\/\n-            if (target.contains(\"ElementType.FIELD\")) {\n-                checkAnno(classFile,\n-                        (RuntimeAnnotations_attribute)findAttributeOrFail(\n-                                field.attributes,\n-                                RuntimeVisibleAnnotations_attribute.class),\n-                        \"Anno\");\n-            } else {\n-                assertAttributeNotPresent(field.attributes, RuntimeVisibleAnnotations_attribute.class);\n-            }\n-\n-            \/\/ lets check now for the type annotation\n-            if (target.contains(\"ElementType.TYPE_USE\")) {\n-                checkTypeAnno(\n-                        classFile,\n-                        (RuntimeVisibleTypeAnnotations_attribute)findAttributeOrFail(field.attributes, RuntimeVisibleTypeAnnotations_attribute.class),\n-                        \"FIELD\",\n-                        \"Anno\");\n-            } else {\n-                assertAttributeNotPresent(field.attributes, RuntimeVisibleTypeAnnotations_attribute.class);\n-            }\n+                    \"\"\";\n+\n+            \/\/ testing several combinations, adding even more combinations won't add too much value\n+            List<String> annoApplicableTargets = List.of(\n+                    \"ElementType.FIELD\",\n+                    \"ElementType.METHOD\",\n+                    \"ElementType.PARAMETER\",\n+                    \"ElementType.RECORD_COMPONENT\",\n+                    \"ElementType.TYPE_USE\",\n+                    \"ElementType.TYPE_USE,ElementType.FIELD\",\n+                    \"ElementType.TYPE_USE,ElementType.METHOD\",\n+                    \"ElementType.TYPE_USE,ElementType.PARAMETER\",\n+                    \"ElementType.TYPE_USE,ElementType.RECORD_COMPONENT\",\n+                    \"ElementType.TYPE_USE,ElementType.FIELD,ElementType.METHOD\",\n+                    \"ElementType.TYPE_USE,ElementType.FIELD,ElementType.PARAMETER\",\n+                    \"ElementType.TYPE_USE,ElementType.FIELD,ElementType.RECORD_COMPONENT\",\n+                    \"ElementType.FIELD,ElementType.TYPE_USE\",\n+                    \"ElementType.FIELD,ElementType.CONSTRUCTOR\",\n+                    \"ElementType.FIELD,ElementType.LOCAL_VARIABLE\",\n+                    \"ElementType.FIELD,ElementType.ANNOTATION_TYPE\",\n+                    \"ElementType.FIELD,ElementType.PACKAGE\",\n+                    \"ElementType.FIELD,ElementType.TYPE_PARAMETER\",\n+                    \"ElementType.FIELD,ElementType.MODULE\",\n+                    \"ElementType.METHOD,ElementType.TYPE_USE\",\n+                    \"ElementType.PARAMETER,ElementType.TYPE_USE\",\n+                    \"ElementType.RECORD_COMPONENT,ElementType.TYPE_USE\",\n+                    \"ElementType.FIELD,ElementType.METHOD,ElementType.TYPE_USE\",\n+                    \"ElementType.FIELD,ElementType.PARAMETER,ElementType.TYPE_USE\",\n+                    \"ElementType.FIELD,ElementType.RECORD_COMPONENT,ElementType.TYPE_USE\"\n+            );\n@@ -1402,23 +1408,29 @@\n-            \/\/ checking for the annotation on the corresponding parameter of the canonical constructor\n-            Method init = findMethodOrFail(classFile, \"<init>\");\n-            \/* if PARAMETER is one of the targets then there must be a declaration annotation applied to the parameter, apart from\n-             * the type annotation\n-             *\/\n-            if (target.contains(\"ElementType.PARAMETER\")) {\n-                checkParameterAnno(classFile,\n-                        (RuntimeVisibleParameterAnnotations_attribute)findAttributeOrFail(\n-                                init.attributes,\n-                                RuntimeVisibleParameterAnnotations_attribute.class),\n-                        \"Anno\");\n-            } else {\n-                assertAttributeNotPresent(init.attributes, RuntimeVisibleAnnotations_attribute.class);\n-            }\n-            \/\/ let's check now for the type annotation\n-            if (target.contains(\"ElementType.TYPE_USE\")) {\n-                checkTypeAnno(\n-                        classFile,\n-                        (RuntimeVisibleTypeAnnotations_attribute) findAttributeOrFail(init.attributes, RuntimeVisibleTypeAnnotations_attribute.class),\n-                        \"METHOD_FORMAL_PARAMETER\", \"Anno\");\n-            } else {\n-                assertAttributeNotPresent(init.attributes, RuntimeVisibleTypeAnnotations_attribute.class);\n-            }\n+            String[] generalOptions = {\n+                    \"-processor\", Processor.class.getName(),\n+                    \"-Atargets=\"\n+            };\n+\n+            for (String target : annoApplicableTargets) {\n+                String code = srcTemplate.replaceFirst(\"#TARGET\", target);\n+                String[] testOptions = generalOptions.clone();\n+                testOptions[testOptions.length - 1] = testOptions[testOptions.length - 1] + target;\n+                setCompileOptions(testOptions);\n+\n+                File dir = assertOK(true, code);\n+\n+                ClassFile classFile = ClassFile.read(findClassFileOrFail(dir, \"R.class\"));\n+\n+                \/\/ field first\n+                Assert.check(classFile.fields.length == 1);\n+                Field field = classFile.fields[0];\n+                \/\/ if FIELD is one of the targets then there must be a declaration annotation applied to the field, apart from\n+                \/\/ the type annotation\n+                if (target.contains(\"ElementType.FIELD\")) {\n+                    checkAnno(classFile,\n+                            (RuntimeAnnotations_attribute) findAttributeOrFail(\n+                                    field.attributes,\n+                                    RuntimeVisibleAnnotations_attribute.class),\n+                            \"Anno\");\n+                } else {\n+                    assertAttributeNotPresent(field.attributes, RuntimeVisibleAnnotations_attribute.class);\n+                }\n@@ -1426,22 +1438,82 @@\n-            \/\/ checking for the annotation in the accessor\n-            Method accessor = findMethodOrFail(classFile, \"s\");\n-            \/* if METHOD is one of the targets then there must be a declaration annotation applied to the accessor, apart from\n-             * the type annotation\n-             *\/\n-            if (target.contains(\"ElementType.METHOD\")) {\n-                checkAnno(classFile,\n-                        (RuntimeAnnotations_attribute)findAttributeOrFail(\n-                                accessor.attributes,\n-                                RuntimeVisibleAnnotations_attribute.class),\n-                        \"Anno\");\n-            } else {\n-                assertAttributeNotPresent(accessor.attributes, RuntimeVisibleAnnotations_attribute.class);\n-            }\n-            \/\/ let's check now for the type annotation\n-            if (target.contains(\"ElementType.TYPE_USE\")) {\n-                checkTypeAnno(\n-                        classFile,\n-                        (RuntimeVisibleTypeAnnotations_attribute)findAttributeOrFail(accessor.attributes, RuntimeVisibleTypeAnnotations_attribute.class),\n-                        \"METHOD_RETURN\", \"Anno\");\n-            } else {\n-                assertAttributeNotPresent(accessor.attributes, RuntimeVisibleTypeAnnotations_attribute.class);\n+                \/\/ lets check now for the type annotation\n+                if (target.contains(\"ElementType.TYPE_USE\")) {\n+                    checkTypeAnno(\n+                            classFile,\n+                            (RuntimeVisibleTypeAnnotations_attribute) findAttributeOrFail(field.attributes, RuntimeVisibleTypeAnnotations_attribute.class),\n+                            \"FIELD\",\n+                            \"Anno\");\n+                } else {\n+                    assertAttributeNotPresent(field.attributes, RuntimeVisibleTypeAnnotations_attribute.class);\n+                }\n+\n+                \/\/ checking for the annotation on the corresponding parameter of the canonical constructor\n+                Method init = findMethodOrFail(classFile, \"<init>\");\n+                \/\/ if PARAMETER is one of the targets then there must be a declaration annotation applied to the parameter, apart from\n+                \/\/ the type annotation\n+                if (target.contains(\"ElementType.PARAMETER\")) {\n+                    checkParameterAnno(classFile,\n+                            (RuntimeVisibleParameterAnnotations_attribute) findAttributeOrFail(\n+                                    init.attributes,\n+                                    RuntimeVisibleParameterAnnotations_attribute.class),\n+                            \"Anno\");\n+                } else {\n+                    assertAttributeNotPresent(init.attributes, RuntimeVisibleAnnotations_attribute.class);\n+                }\n+                \/\/ let's check now for the type annotation\n+                if (target.contains(\"ElementType.TYPE_USE\")) {\n+                    checkTypeAnno(\n+                            classFile,\n+                            (RuntimeVisibleTypeAnnotations_attribute) findAttributeOrFail(init.attributes, RuntimeVisibleTypeAnnotations_attribute.class),\n+                            \"METHOD_FORMAL_PARAMETER\", \"Anno\");\n+                } else {\n+                    assertAttributeNotPresent(init.attributes, RuntimeVisibleTypeAnnotations_attribute.class);\n+                }\n+\n+                \/\/ checking for the annotation in the accessor\n+                Method accessor = findMethodOrFail(classFile, \"s\");\n+                \/\/ if METHOD is one of the targets then there must be a declaration annotation applied to the accessor, apart from\n+                \/\/ the type annotation\n+                if (target.contains(\"ElementType.METHOD\")) {\n+                    checkAnno(classFile,\n+                            (RuntimeAnnotations_attribute) findAttributeOrFail(\n+                                    accessor.attributes,\n+                                    RuntimeVisibleAnnotations_attribute.class),\n+                            \"Anno\");\n+                } else {\n+                    assertAttributeNotPresent(accessor.attributes, RuntimeVisibleAnnotations_attribute.class);\n+                }\n+                \/\/ let's check now for the type annotation\n+                if (target.contains(\"ElementType.TYPE_USE\")) {\n+                    checkTypeAnno(\n+                            classFile,\n+                            (RuntimeVisibleTypeAnnotations_attribute) findAttributeOrFail(accessor.attributes, RuntimeVisibleTypeAnnotations_attribute.class),\n+                            \"METHOD_RETURN\", \"Anno\");\n+                } else {\n+                    assertAttributeNotPresent(accessor.attributes, RuntimeVisibleTypeAnnotations_attribute.class);\n+                }\n+\n+                \/\/ checking for the annotation in the Record attribute\n+                Record_attribute record = (Record_attribute) findAttributeOrFail(classFile.attributes, Record_attribute.class);\n+                Assert.check(record.component_count == 1);\n+                \/\/ if RECORD_COMPONENT is one of the targets then there must be a declaration annotation applied to the\n+                \/\/ field, apart from the type annotation\n+                if (target.contains(\"ElementType.RECORD_COMPONENT\")) {\n+                    checkAnno(classFile,\n+                            (RuntimeAnnotations_attribute) findAttributeOrFail(\n+                                    record.component_info_arr[0].attributes,\n+                                    RuntimeVisibleAnnotations_attribute.class),\n+                            \"Anno\");\n+                } else {\n+                    assertAttributeNotPresent(record.component_info_arr[0].attributes, RuntimeVisibleAnnotations_attribute.class);\n+                }\n+                \/\/ lets check now for the type annotation\n+                if (target.contains(\"ElementType.TYPE_USE\")) {\n+                    checkTypeAnno(\n+                            classFile,\n+                            (RuntimeVisibleTypeAnnotations_attribute) findAttributeOrFail(\n+                                    record.component_info_arr[0].attributes,\n+                                    RuntimeVisibleTypeAnnotations_attribute.class),\n+                            \"FIELD\", \"Anno\");\n+                } else {\n+                    assertAttributeNotPresent(record.component_info_arr[0].attributes, RuntimeVisibleTypeAnnotations_attribute.class);\n+                }\n@@ -1450,26 +1522,3 @@\n-            \/\/ checking for the annotation in the Record attribute\n-            Record_attribute record = (Record_attribute)findAttributeOrFail(classFile.attributes, Record_attribute.class);\n-            Assert.check(record.component_count == 1);\n-            \/* if RECORD_COMPONENT is one of the targets then there must be a declaration annotation applied to the\n-             * field, apart from the type annotation\n-             *\/\n-            if (target.contains(\"ElementType.RECORD_COMPONENT\")) {\n-                checkAnno(classFile,\n-                        (RuntimeAnnotations_attribute)findAttributeOrFail(\n-                                record.component_info_arr[0].attributes,\n-                                RuntimeVisibleAnnotations_attribute.class),\n-                        \"Anno\");\n-            } else {\n-                assertAttributeNotPresent(record.component_info_arr[0].attributes, RuntimeVisibleAnnotations_attribute.class);\n-            }\n-            \/\/ lets check now for the type annotation\n-            if (target.contains(\"ElementType.TYPE_USE\")) {\n-                checkTypeAnno(\n-                        classFile,\n-                        (RuntimeVisibleTypeAnnotations_attribute)findAttributeOrFail(\n-                                record.component_info_arr[0].attributes,\n-                                RuntimeVisibleTypeAnnotations_attribute.class),\n-                        \"FIELD\", \"Anno\");\n-            } else {\n-                assertAttributeNotPresent(record.component_info_arr[0].attributes, RuntimeVisibleTypeAnnotations_attribute.class);\n-            }\n+            \/\/ let's reset the default compiler options for other tests\n+        } finally {\n+            setCompileOptions(previousOptions);\n@@ -1477,3 +1526,0 @@\n-\n-        \/\/ let's reset the default compiler options for other tests\n-        setCompileOptions(previousOptions);\n@@ -1658,1 +1704,1 @@\n-                        default -> { \/* do nothing *\/ }\n+                        default -> {}\n@@ -1966,3 +2012,2 @@\n-            \/* dont execute this test when the default annotation processor is on as it will fail due to\n-             * spurious warnings\n-             *\/\n+            \/\/ dont execute this test when the default annotation processor is on as it will fail due to\n+            \/\/ spurious warnings\n","filename":"test\/langtools\/tools\/javac\/records\/RecordCompilationTests.java","additions":225,"deletions":180,"binary":false,"changes":405,"status":"modified"},{"patch":"@@ -1,1 +1,1 @@\n-CheckFeatureGate1.java:10:12: compiler.err.feature.not.supported.in.source.plural: (compiler.misc.feature.primitive.classes), 13, 18\n+CheckFeatureGate1.java:10:12: compiler.err.feature.not.supported.in.source.plural: (compiler.misc.feature.primitive.classes), 13, 19\n","filename":"test\/langtools\/tools\/javac\/valhalla\/lworld-values\/CheckFeatureGate1.out","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1,1 +1,1 @@\n-CheckFeatureGate2.java:11:17: compiler.err.feature.not.supported.in.source.plural: (compiler.misc.feature.primitive.classes), 13, 18\n+CheckFeatureGate2.java:11:17: compiler.err.feature.not.supported.in.source.plural: (compiler.misc.feature.primitive.classes), 13, 19\n","filename":"test\/langtools\/tools\/javac\/valhalla\/lworld-values\/CheckFeatureGate2.out","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -5,1 +5,1 @@\n-PrimitiveAsTypeName.java:12:24: compiler.err.feature.not.supported.in.source.plural: (compiler.misc.feature.primitive.classes), 16, 18\n+PrimitiveAsTypeName.java:12:24: compiler.err.feature.not.supported.in.source.plural: (compiler.misc.feature.primitive.classes), 16, 19\n","filename":"test\/langtools\/tools\/javac\/valhalla\/lworld-values\/PrimitiveAsTypeName.out","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2012, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2022, Oracle and\/or its affiliates. All rights reserved.\n@@ -152,0 +152,14 @@\n+  private native Object[] getObjectsViaKlassOopMaps0(Object thing);\n+  public Object[] getObjectsViaKlassOopMaps(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaKlassOopMaps0(thing);\n+  }\n+\n+  private native Object[] getObjectsViaOopIterator0(Object thing);\n+  public Object[] getObjectsViaOopIterator(Object thing) {\n+    Objects.requireNonNull(thing);\n+    return getObjectsViaOopIterator0(thing);\n+  }\n+\n+  public native Object[] getObjectsViaFrameOopIterator(int depth);\n+\n@@ -490,0 +504,6 @@\n+  \/\/ G1 specific GC breakpoints.\n+  public final String G1_AFTER_REBUILD_STARTED = \"AFTER REBUILD STARTED\";\n+  public final String G1_BEFORE_REBUILD_COMPLETED = \"BEFORE REBUILD COMPLETED\";\n+  public final String G1_AFTER_CLEANUP_STARTED = \"AFTER CLEANUP STARTED\";\n+  public final String G1_BEFORE_CLEANUP_COMPLETED = \"BEFORE CLEANUP COMPLETED\";\n+\n","filename":"test\/lib\/jdk\/test\/whitebox\/WhiteBox.java","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"}]}