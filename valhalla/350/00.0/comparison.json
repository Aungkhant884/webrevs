{"files":[{"patch":"@@ -480,1 +480,0 @@\n-\t\t\"--disable-jvm-feature-jvmci\",\n","filename":"make\/conf\/jib-profiles.js","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -11337,2 +11337,1 @@\n-                         iRegIorL2I src1, iRegIorL2I src2, immI_M1 m1,\n-                         rFlagsReg cr) %{\n+                         iRegIorL2I src1, iRegIorL2I src2, immI_M1 m1) %{\n@@ -11356,2 +11355,1 @@\n-                         iRegL src1, iRegL src2, immL_M1 m1,\n-                         rFlagsReg cr) %{\n+                         iRegL src1, iRegL src2, immL_M1 m1) %{\n@@ -11375,2 +11373,1 @@\n-                         iRegIorL2I src1, iRegIorL2I src2, immI_M1 m1,\n-                         rFlagsReg cr) %{\n+                         iRegIorL2I src1, iRegIorL2I src2, immI_M1 m1) %{\n@@ -11394,2 +11391,1 @@\n-                         iRegL src1, iRegL src2, immL_M1 m1,\n-                         rFlagsReg cr) %{\n+                         iRegL src1, iRegL src2, immL_M1 m1) %{\n@@ -11413,2 +11409,1 @@\n-                         iRegIorL2I src1, iRegIorL2I src2, immI_M1 m1,\n-                         rFlagsReg cr) %{\n+                         iRegIorL2I src1, iRegIorL2I src2, immI_M1 m1) %{\n@@ -11432,2 +11427,1 @@\n-                         iRegL src1, iRegL src2, immL_M1 m1,\n-                         rFlagsReg cr) %{\n+                         iRegL src1, iRegL src2, immL_M1 m1) %{\n@@ -11450,0 +11444,1 @@\n+\/\/ val & (-1 ^ (val >>> shift)) ==> bicw\n@@ -11452,1 +11447,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11470,0 +11465,1 @@\n+\/\/ val & (-1 ^ (val >>> shift)) ==> bic\n@@ -11472,1 +11468,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11490,0 +11486,1 @@\n+\/\/ val & (-1 ^ (val >> shift)) ==> bicw\n@@ -11492,1 +11489,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11510,0 +11507,1 @@\n+\/\/ val & (-1 ^ (val >> shift)) ==> bic\n@@ -11512,1 +11510,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11530,0 +11528,43 @@\n+\/\/ val & (-1 ^ (val ror shift)) ==> bicw\n+instruct AndI_reg_RotateRight_not_reg(iRegINoSp dst,\n+                         iRegIorL2I src1, iRegIorL2I src2,\n+                         immI src3, immI_M1 src4) %{\n+  match(Set dst (AndI src1 (XorI(RotateRight src2 src3) src4)));\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"bicw  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ bicw(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+\/\/ val & (-1 ^ (val ror shift)) ==> bic\n+instruct AndL_reg_RotateRight_not_reg(iRegLNoSp dst,\n+                         iRegL src1, iRegL src2,\n+                         immI src3, immL_M1 src4) %{\n+  match(Set dst (AndL src1 (XorL(RotateRight src2 src3) src4)));\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"bic  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ bic(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+\/\/ val & (-1 ^ (val << shift)) ==> bicw\n@@ -11532,1 +11573,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11550,0 +11591,1 @@\n+\/\/ val & (-1 ^ (val << shift)) ==> bic\n@@ -11552,1 +11594,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11570,0 +11612,1 @@\n+\/\/ val ^ (-1 ^ (val >>> shift)) ==> eonw\n@@ -11572,1 +11615,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11590,0 +11633,1 @@\n+\/\/ val ^ (-1 ^ (val >>> shift)) ==> eon\n@@ -11592,1 +11636,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11610,0 +11654,1 @@\n+\/\/ val ^ (-1 ^ (val >> shift)) ==> eonw\n@@ -11612,1 +11657,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11630,0 +11675,1 @@\n+\/\/ val ^ (-1 ^ (val >> shift)) ==> eon\n@@ -11632,1 +11678,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11650,0 +11696,43 @@\n+\/\/ val ^ (-1 ^ (val ror shift)) ==> eonw\n+instruct XorI_reg_RotateRight_not_reg(iRegINoSp dst,\n+                         iRegIorL2I src1, iRegIorL2I src2,\n+                         immI src3, immI_M1 src4) %{\n+  match(Set dst (XorI src4 (XorI(RotateRight src2 src3) src1)));\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"eonw  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ eonw(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+\/\/ val ^ (-1 ^ (val ror shift)) ==> eon\n+instruct XorL_reg_RotateRight_not_reg(iRegLNoSp dst,\n+                         iRegL src1, iRegL src2,\n+                         immI src3, immL_M1 src4) %{\n+  match(Set dst (XorL src4 (XorL(RotateRight src2 src3) src1)));\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"eon  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ eon(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+\/\/ val ^ (-1 ^ (val << shift)) ==> eonw\n@@ -11652,1 +11741,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11670,0 +11759,1 @@\n+\/\/ val ^ (-1 ^ (val << shift)) ==> eon\n@@ -11672,1 +11762,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11690,0 +11780,1 @@\n+\/\/ val | (-1 ^ (val >>> shift)) ==> ornw\n@@ -11692,1 +11783,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11710,0 +11801,1 @@\n+\/\/ val | (-1 ^ (val >>> shift)) ==> orn\n@@ -11712,1 +11804,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11730,0 +11822,1 @@\n+\/\/ val | (-1 ^ (val >> shift)) ==> ornw\n@@ -11732,1 +11825,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11750,0 +11843,1 @@\n+\/\/ val | (-1 ^ (val >> shift)) ==> orn\n@@ -11752,1 +11846,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11770,0 +11864,43 @@\n+\/\/ val | (-1 ^ (val ror shift)) ==> ornw\n+instruct OrI_reg_RotateRight_not_reg(iRegINoSp dst,\n+                         iRegIorL2I src1, iRegIorL2I src2,\n+                         immI src3, immI_M1 src4) %{\n+  match(Set dst (OrI src1 (XorI(RotateRight src2 src3) src4)));\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"ornw  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ ornw(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+\/\/ val | (-1 ^ (val ror shift)) ==> orn\n+instruct OrL_reg_RotateRight_not_reg(iRegLNoSp dst,\n+                         iRegL src1, iRegL src2,\n+                         immI src3, immL_M1 src4) %{\n+  match(Set dst (OrL src1 (XorL(RotateRight src2 src3) src4)));\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"orn  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ orn(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+\/\/ val | (-1 ^ (val << shift)) ==> ornw\n@@ -11772,1 +11909,1 @@\n-                         immI src3, immI_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immI_M1 src4) %{\n@@ -11790,0 +11927,1 @@\n+\/\/ val | (-1 ^ (val << shift)) ==> orn\n@@ -11792,1 +11930,1 @@\n-                         immI src3, immL_M1 src4, rFlagsReg cr) %{\n+                         immI src3, immL_M1 src4) %{\n@@ -11812,1 +11950,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -11833,1 +11971,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -11854,1 +11992,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -11875,1 +12013,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -11896,1 +12034,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -11917,1 +12055,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -11934,0 +12072,42 @@\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct AndI_reg_RotateRight_reg(iRegINoSp dst,\n+                         iRegIorL2I src1, iRegIorL2I src2,\n+                         immI src3) %{\n+  match(Set dst (AndI src1 (RotateRight src2 src3)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"andw  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ andw(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct AndL_reg_RotateRight_reg(iRegLNoSp dst,\n+                         iRegL src1, iRegL src2,\n+                         immI src3) %{\n+  match(Set dst (AndL src1 (RotateRight src2 src3)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"andr  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ andr(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n@@ -11938,1 +12118,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -11959,1 +12139,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -11980,1 +12160,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12001,1 +12181,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12022,1 +12202,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12043,1 +12223,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12060,0 +12240,42 @@\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct XorI_reg_RotateRight_reg(iRegINoSp dst,\n+                         iRegIorL2I src1, iRegIorL2I src2,\n+                         immI src3) %{\n+  match(Set dst (XorI src1 (RotateRight src2 src3)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"eorw  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ eorw(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct XorL_reg_RotateRight_reg(iRegLNoSp dst,\n+                         iRegL src1, iRegL src2,\n+                         immI src3) %{\n+  match(Set dst (XorL src1 (RotateRight src2 src3)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"eor  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ eor(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n@@ -12064,1 +12286,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12085,1 +12307,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12106,1 +12328,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12127,1 +12349,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12148,1 +12370,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12169,1 +12391,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12186,0 +12408,42 @@\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct OrI_reg_RotateRight_reg(iRegINoSp dst,\n+                         iRegIorL2I src1, iRegIorL2I src2,\n+                         immI src3) %{\n+  match(Set dst (OrI src1 (RotateRight src2 src3)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"orrw  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ orrw(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x1f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n+\/\/ This pattern is automatically generated from aarch64_ad.m4\n+\/\/ DO NOT EDIT ANYTHING IN THIS SECTION OF THE FILE\n+instruct OrL_reg_RotateRight_reg(iRegLNoSp dst,\n+                         iRegL src1, iRegL src2,\n+                         immI src3) %{\n+  match(Set dst (OrL src1 (RotateRight src2 src3)));\n+\n+  ins_cost(1.9 * INSN_COST);\n+  format %{ \"orr  $dst, $src1, $src2, ROR $src3\" %}\n+\n+  ins_encode %{\n+    __ orr(as_Register($dst$$reg),\n+              as_Register($src1$$reg),\n+              as_Register($src2$$reg),\n+              Assembler::ROR,\n+              $src3$$constant & 0x3f);\n+  %}\n+\n+  ins_pipe(ialu_reg_reg_shift);\n+%}\n+\n@@ -12190,1 +12454,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12211,1 +12475,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12232,1 +12496,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12253,1 +12517,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12274,1 +12538,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12295,1 +12559,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12316,1 +12580,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12337,1 +12601,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12358,1 +12622,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12379,1 +12643,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12400,1 +12664,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12421,1 +12685,1 @@\n-                         immI src3, rFlagsReg cr) %{\n+                         immI src3) %{\n@@ -12438,1 +12702,0 @@\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":324,"deletions":61,"binary":false,"changes":385,"status":"modified"},{"patch":"@@ -49,1 +49,0 @@\n-    __ membar(Assembler::StoreLoad);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/cardTableBarrierSetAssembler_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -5592,0 +5592,1 @@\n+\n@@ -5596,3 +5597,15 @@\n-  void generate_atomic_entry_points() {\n-    if (! UseLSE) {\n-      return;\n+  \/\/ class AtomicStubMark records the entry point of a stub and the\n+  \/\/ stub pointer which will point to it. The stub pointer is set to\n+  \/\/ the entry point when ~AtomicStubMark() is called, which must be\n+  \/\/ after ICache::invalidate_range. This ensures safe publication of\n+  \/\/ the generated code.\n+  class AtomicStubMark {\n+    address _entry_point;\n+    aarch64_atomic_stub_t *_stub;\n+    MacroAssembler *_masm;\n+  public:\n+    AtomicStubMark(MacroAssembler *masm, aarch64_atomic_stub_t *stub) {\n+      _masm = masm;\n+      __ align(32);\n+      _entry_point = __ pc();\n+      _stub = stub;\n@@ -5601,0 +5614,4 @@\n+    ~AtomicStubMark() {\n+      *_stub = (aarch64_atomic_stub_t)_entry_point;\n+    }\n+  };\n@@ -5602,10 +5619,56 @@\n-    __ align(CodeEntryAlignment);\n-    StubCodeMark mark(this, \"StubRoutines\", \"atomic entry points\");\n-\n-    __ align(32);\n-    aarch64_atomic_fetch_add_8_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n-      __ atomic_addal(prev, incr, addr);\n-      __ mov(r0, prev);\n-      __ ret(lr);\n+  \/\/ NB: For memory_order_conservative we need a trailing membar after\n+  \/\/ LSE atomic operations but not a leading membar.\n+  \/\/\n+  \/\/ We don't need a leading membar because a clause in the Arm ARM\n+  \/\/ says:\n+  \/\/\n+  \/\/   Barrier-ordered-before\n+  \/\/\n+  \/\/   Barrier instructions order prior Memory effects before subsequent\n+  \/\/   Memory effects generated by the same Observer. A read or a write\n+  \/\/   RW1 is Barrier-ordered-before a read or a write RW 2 from the same\n+  \/\/   Observer if and only if RW1 appears in program order before RW 2\n+  \/\/   and [ ... ] at least one of RW 1 and RW 2 is generated by an atomic\n+  \/\/   instruction with both Acquire and Release semantics.\n+  \/\/\n+  \/\/ All the atomic instructions {ldaddal, swapal, casal} have Acquire\n+  \/\/ and Release semantics, therefore we don't need a leading\n+  \/\/ barrier. However, there is no corresponding Barrier-ordered-after\n+  \/\/ relationship, therefore we need a trailing membar to prevent a\n+  \/\/ later store or load from being reordered with the store in an\n+  \/\/ atomic instruction.\n+  \/\/\n+  \/\/ This was checked by using the herd7 consistency model simulator\n+  \/\/ (http:\/\/diy.inria.fr\/) with this test case:\n+  \/\/\n+  \/\/ AArch64 LseCas\n+  \/\/ { 0:X1=x; 0:X2=y; 1:X1=x; 1:X2=y; }\n+  \/\/ P0 | P1;\n+  \/\/ LDR W4, [X2] | MOV W3, #0;\n+  \/\/ DMB LD       | MOV W4, #1;\n+  \/\/ LDR W3, [X1] | CASAL W3, W4, [X1];\n+  \/\/              | DMB ISH;\n+  \/\/              | STR W4, [X2];\n+  \/\/ exists\n+  \/\/ (0:X3=0 \/\\ 0:X4=1)\n+  \/\/\n+  \/\/ If X3 == 0 && X4 == 1, the store to y in P1 has been reordered\n+  \/\/ with the store to x in P1. Without the DMB in P1 this may happen.\n+  \/\/\n+  \/\/ At the time of writing we don't know of any AArch64 hardware that\n+  \/\/ reorders stores in this way, but the Reference Manual permits it.\n+\n+  void gen_cas_entry(Assembler::operand_size size,\n+                     atomic_memory_order order) {\n+    Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n+      exchange_val = c_rarg2;\n+    bool acquire, release;\n+    switch (order) {\n+      case memory_order_relaxed:\n+        acquire = false;\n+        release = false;\n+        break;\n+      default:\n+        acquire = true;\n+        release = true;\n+        break;\n@@ -5613,7 +5676,4 @@\n-    __ align(32);\n-    aarch64_atomic_fetch_add_4_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n-      __ atomic_addalw(prev, incr, addr);\n-      __ movw(r0, prev);\n-      __ ret(lr);\n+    __ mov(prev, compare_val);\n+    __ lse_cas(prev, exchange_val, ptr, size, acquire, release, \/*not_pair*\/true);\n+    if (order == memory_order_conservative) {\n+      __ membar(Assembler::StoreStore|Assembler::StoreLoad);\n@@ -5621,5 +5681,3 @@\n-    __ align(32);\n-    aarch64_atomic_xchg_4_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r2, addr = c_rarg0, newv = c_rarg1;\n-      __ atomic_xchglw(prev, newv, addr);\n+    if (size == Assembler::xword) {\n+      __ mov(r0, prev);\n+    } else {\n@@ -5627,6 +5685,8 @@\n-      __ ret(lr);\n-    __ align(32);\n-    aarch64_atomic_xchg_8_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r2, addr = c_rarg0, newv = c_rarg1;\n-      __ atomic_xchgl(prev, newv, addr);\n+    __ ret(lr);\n+  }\n+\n+  void gen_ldaddal_entry(Assembler::operand_size size) {\n+    Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n+    __ ldaddal(size, incr, prev, addr);\n+    __ membar(Assembler::StoreStore|Assembler::StoreLoad);\n+    if (size == Assembler::xword) {\n@@ -5635,11 +5695,1 @@\n-      __ ret(lr);\n-    }\n-    __ align(32);\n-    aarch64_atomic_cmpxchg_1_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n-        exchange_val = c_rarg2;\n-      __ cmpxchg(ptr, compare_val, exchange_val,\n-                 MacroAssembler::byte,\n-                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n-                 prev);\n+    } else {\n@@ -5647,10 +5697,10 @@\n-      __ ret(lr);\n-    __ align(32);\n-    aarch64_atomic_cmpxchg_4_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n-        exchange_val = c_rarg2;\n-      __ cmpxchg(ptr, compare_val, exchange_val,\n-                 MacroAssembler::word,\n-                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n-                 prev);\n+    __ ret(lr);\n+  }\n+\n+  void gen_swpal_entry(Assembler::operand_size size) {\n+    Register prev = r2, addr = c_rarg0, incr = c_rarg1;\n+    __ swpal(size, incr, prev, addr);\n+    __ membar(Assembler::StoreStore|Assembler::StoreLoad);\n+    if (size == Assembler::xword) {\n+      __ mov(r0, prev);\n+    } else {\n@@ -5659,12 +5709,6 @@\n-      __ ret(lr);\n-    __ align(32);\n-    aarch64_atomic_cmpxchg_8_impl = (aarch64_atomic_stub_t)__ pc();\n-    {\n-      Register prev = r3, ptr = c_rarg0, compare_val = c_rarg1,\n-        exchange_val = c_rarg2;\n-      __ cmpxchg(ptr, compare_val, exchange_val,\n-                 MacroAssembler::xword,\n-                 \/*acquire*\/false, \/*release*\/false, \/*weak*\/false,\n-                 prev);\n-      __ mov(r0, prev);\n-      __ ret(lr);\n+    __ ret(lr);\n+  }\n+\n+  void generate_atomic_entry_points() {\n+    if (! UseLSE) {\n+      return;\n@@ -5673,0 +5717,36 @@\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"atomic entry points\");\n+    address first_entry = __ pc();\n+\n+    \/\/ All memory_order_conservative\n+    AtomicStubMark mark_fetch_add_4(_masm, &aarch64_atomic_fetch_add_4_impl);\n+    gen_ldaddal_entry(Assembler::word);\n+    AtomicStubMark mark_fetch_add_8(_masm, &aarch64_atomic_fetch_add_8_impl);\n+    gen_ldaddal_entry(Assembler::xword);\n+\n+    AtomicStubMark mark_xchg_4(_masm, &aarch64_atomic_xchg_4_impl);\n+    gen_swpal_entry(Assembler::word);\n+    AtomicStubMark mark_xchg_8_impl(_masm, &aarch64_atomic_xchg_8_impl);\n+    gen_swpal_entry(Assembler::xword);\n+\n+    \/\/ CAS, memory_order_conservative\n+    AtomicStubMark mark_cmpxchg_1(_masm, &aarch64_atomic_cmpxchg_1_impl);\n+    gen_cas_entry(MacroAssembler::byte, memory_order_conservative);\n+    AtomicStubMark mark_cmpxchg_4(_masm, &aarch64_atomic_cmpxchg_4_impl);\n+    gen_cas_entry(MacroAssembler::word, memory_order_conservative);\n+    AtomicStubMark mark_cmpxchg_8(_masm, &aarch64_atomic_cmpxchg_8_impl);\n+    gen_cas_entry(MacroAssembler::xword, memory_order_conservative);\n+\n+    \/\/ CAS, memory_order_relaxed\n+    AtomicStubMark mark_cmpxchg_1_relaxed\n+      (_masm, &aarch64_atomic_cmpxchg_1_relaxed_impl);\n+    gen_cas_entry(MacroAssembler::byte, memory_order_relaxed);\n+    AtomicStubMark mark_cmpxchg_4_relaxed\n+      (_masm, &aarch64_atomic_cmpxchg_4_relaxed_impl);\n+    gen_cas_entry(MacroAssembler::word, memory_order_relaxed);\n+    AtomicStubMark mark_cmpxchg_8_relaxed\n+      (_masm, &aarch64_atomic_cmpxchg_8_relaxed_impl);\n+    gen_cas_entry(MacroAssembler::xword, memory_order_relaxed);\n+\n+    ICache::invalidate_range(first_entry, __ pc() - first_entry);\n@@ -6973,2 +7053,0 @@\n-#if 0  \/\/ JDK-8261660: disabled for now.\n-#endif\n@@ -7006,2 +7084,2 @@\n-#define DEFAULT_ATOMIC_OP(OPNAME, SIZE)                                 \\\n-  extern \"C\" uint64_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _default_impl \\\n+#define DEFAULT_ATOMIC_OP(OPNAME, SIZE, RELAXED)                                \\\n+  extern \"C\" uint64_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## RELAXED ## _default_impl \\\n@@ -7009,10 +7087,13 @@\n-  aarch64_atomic_stub_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _impl \\\n-    = aarch64_atomic_ ## OPNAME ## _ ## SIZE ## _default_impl;\n-\n-DEFAULT_ATOMIC_OP(fetch_add, 4)\n-DEFAULT_ATOMIC_OP(fetch_add, 8)\n-DEFAULT_ATOMIC_OP(xchg, 4)\n-DEFAULT_ATOMIC_OP(xchg, 8)\n-DEFAULT_ATOMIC_OP(cmpxchg, 1)\n-DEFAULT_ATOMIC_OP(cmpxchg, 4)\n-DEFAULT_ATOMIC_OP(cmpxchg, 8)\n+  aarch64_atomic_stub_t aarch64_atomic_ ## OPNAME ## _ ## SIZE ## RELAXED ## _impl \\\n+    = aarch64_atomic_ ## OPNAME ## _ ## SIZE ## RELAXED ## _default_impl;\n+\n+DEFAULT_ATOMIC_OP(fetch_add, 4, )\n+DEFAULT_ATOMIC_OP(fetch_add, 8, )\n+DEFAULT_ATOMIC_OP(xchg, 4, )\n+DEFAULT_ATOMIC_OP(xchg, 8, )\n+DEFAULT_ATOMIC_OP(cmpxchg, 1, )\n+DEFAULT_ATOMIC_OP(cmpxchg, 4, )\n+DEFAULT_ATOMIC_OP(cmpxchg, 8, )\n+DEFAULT_ATOMIC_OP(cmpxchg, 1, _relaxed)\n+DEFAULT_ATOMIC_OP(cmpxchg, 4, _relaxed)\n+DEFAULT_ATOMIC_OP(cmpxchg, 8, _relaxed)\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":159,"deletions":78,"binary":false,"changes":237,"status":"modified"},{"patch":"@@ -1902,11 +1902,3 @@\n-  if (ArrayCopyPartialInlineSize <= 32) {\n-    mov64(dst, 1);\n-    shlxq(dst, dst, len);\n-    decq(dst);\n-  } else {\n-    mov64(dst, -1);\n-    movq(temp, len);\n-    negptr(temp);\n-    addptr(temp, 64);\n-    shrxq(dst, dst, temp);\n-  }\n+  assert(ArrayCopyPartialInlineSize <= 64,\"\");\n+  mov64(dst, -1L);\n+  bzhiq(dst, dst, len);\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -3158,0 +3158,10 @@\n+void MacroAssembler::vpaddb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch) {\n+  assert(UseAVX > 0, \"requires some form of AVX\");\n+  if (reachable(src)) {\n+    Assembler::vpaddb(dst, nds, as_Address(src), vector_len);\n+  } else {\n+    lea(rscratch, src);\n+    Assembler::vpaddb(dst, nds, Address(rscratch, 0), vector_len);\n+  }\n+}\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -1306,0 +1306,1 @@\n+  void vpaddb(XMMRegister dst, XMMRegister nds, AddressLiteral src, int vector_len, Register rscratch);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -826,0 +826,11 @@\n+  address generate_vector_byte_shuffle_mask(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x7070707070707070, relocInfo::none);\n+    __ emit_data64(0x7070707070707070, relocInfo::none);\n+    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+    __ emit_data64(0xF0F0F0F0F0F0F0F0, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -1489,0 +1500,1 @@\n+        __ align(32);\n@@ -1555,0 +1567,1 @@\n+        __ align(32);\n@@ -1694,0 +1707,1 @@\n+        __ align(32);\n@@ -1726,0 +1740,1 @@\n+        __ align(32);\n@@ -1788,1 +1803,1 @@\n-    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n@@ -1904,1 +1919,1 @@\n-    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n@@ -2015,1 +2030,1 @@\n-    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n@@ -2146,1 +2161,1 @@\n-    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n@@ -2250,1 +2265,1 @@\n-    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n@@ -2361,1 +2376,1 @@\n-    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n@@ -2474,1 +2489,1 @@\n-    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n@@ -2584,1 +2599,1 @@\n-    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32) {\n+    if (VM_Version::supports_avx512vlbw() && VM_Version::supports_bmi2() && MaxVectorSize  >= 32) {\n@@ -7013,0 +7028,1 @@\n+    StubRoutines::x86::_vector_byte_shuffle_mask = generate_vector_byte_shuffle_mask(\"vector_byte_shuffle_mask\");\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":24,"deletions":8,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -234,2 +234,2 @@\n-    , non_data_bits  = kind_bits + type_bits + size_bits + destroys_bits + last_use_bits +\n-                       is_fpu_stack_offset_bits + virtual_bits + is_xmm_bits\n+    , non_data_bits  = pointer_bits + kind_bits + type_bits + size_bits + destroys_bits + virtual_bits\n+                       + is_xmm_bits + last_use_bits + is_fpu_stack_offset_bits\n@@ -654,0 +654,5 @@\n+    if (index > LIR_OprDesc::vreg_max) {\n+      \/\/ Running out of virtual registers. Caller should bailout.\n+      return illegalOpr;\n+    }\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1072,9 +1072,7 @@\n-  int vreg = _virtual_register_number;\n-  \/\/ add a little fudge factor for the bailout, since the bailout is\n-  \/\/ only checked periodically.  This gives a few extra registers to\n-  \/\/ hand out before we really run out, which helps us keep from\n-  \/\/ tripping over assertions.\n-  if (vreg + 20 >= LIR_OprDesc::vreg_max) {\n-    bailout(\"out of virtual registers\");\n-    if (vreg + 2 >= LIR_OprDesc::vreg_max) {\n-      \/\/ wrap it around\n+  int vreg_num = _virtual_register_number;\n+  \/\/ Add a little fudge factor for the bailout since the bailout is only checked periodically. This allows us to hand out\n+  \/\/ a few extra registers before we really run out which helps to avoid to trip over assertions.\n+  if (vreg_num + 20 >= LIR_OprDesc::vreg_max) {\n+    bailout(\"out of virtual registers in LIR generator\");\n+    if (vreg_num + 2 >= LIR_OprDesc::vreg_max) {\n+      \/\/ Wrap it around and continue until bailout really happens to avoid hitting assertions.\n@@ -1082,0 +1080,1 @@\n+      vreg_num = LIR_OprDesc::vreg_base;\n@@ -1085,1 +1084,3 @@\n-  return LIR_OprFact::virtual_register(vreg, type);\n+  LIR_Opr vreg = LIR_OprFact::virtual_register(vreg_num, type);\n+  assert(vreg != LIR_OprFact::illegal(), \"ran out of virtual registers\");\n+  return vreg;\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":11,"deletions":10,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -3931,2 +3931,2 @@\n-  LIR_Opr from_opr = LIR_OprFact::virtual_register(from_interval->reg_num(), from_interval->type());\n-  LIR_Opr to_opr = LIR_OprFact::virtual_register(to_interval->reg_num(), to_interval->type());\n+  LIR_Opr from_opr = get_virtual_register(from_interval);\n+  LIR_Opr to_opr = get_virtual_register(to_interval);\n@@ -3950,1 +3950,1 @@\n-  LIR_Opr to_opr = LIR_OprFact::virtual_register(to_interval->reg_num(), to_interval->type());\n+  LIR_Opr to_opr = get_virtual_register(to_interval);\n@@ -3956,0 +3956,15 @@\n+LIR_Opr MoveResolver::get_virtual_register(Interval* interval) {\n+  \/\/ Add a little fudge factor for the bailout since the bailout is only checked periodically. This allows us to hand out\n+  \/\/ a few extra registers before we really run out which helps to avoid to trip over assertions.\n+  int reg_num = interval->reg_num();\n+  if (reg_num + 20 >= LIR_OprDesc::vreg_max) {\n+    _allocator->bailout(\"out of virtual registers in linear scan\");\n+    if (reg_num + 2 >= LIR_OprDesc::vreg_max) {\n+      \/\/ Wrap it around and continue until bailout really happens to avoid hitting assertions.\n+      reg_num = LIR_OprDesc::vreg_base;\n+    }\n+  }\n+  LIR_Opr vreg = LIR_OprFact::virtual_register(reg_num, interval->type());\n+  assert(vreg != LIR_OprFact::illegal(), \"ran out of virtual registers\");\n+  return vreg;\n+}\n","filename":"src\/hotspot\/share\/c1\/c1_LinearScan.cpp","additions":18,"deletions":3,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -53,0 +54,1 @@\n+volatile Thread* ClassListParser::_parsing_thread = NULL;\n@@ -56,2 +58,0 @@\n-  assert(_instance == NULL, \"must be singleton\");\n-  _instance = this;\n@@ -76,0 +76,9 @@\n+\n+  \/\/ _instance should only be accessed by the thread that created _instance.\n+  assert(_instance == NULL, \"must be singleton\");\n+  _instance = this;\n+  Atomic::store(&_parsing_thread, Thread::current());\n+}\n+\n+bool ClassListParser::is_parsing_thread() {\n+  return Atomic::load(&_parsing_thread) == Thread::current();\n@@ -82,0 +91,1 @@\n+  Atomic::store(&_parsing_thread, (Thread*)NULL);\n","filename":"src\/hotspot\/share\/classfile\/classListParser.cpp","additions":12,"deletions":2,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -715,0 +715,8 @@\n+jzfile* ClassLoader::open_zip_file(const char* canonical_path, char** error_msg, JavaThread* thread) {\n+  \/\/ enable call to C land\n+  ThreadToNativeFromVM ttn(thread);\n+  HandleMark hm(thread);\n+  load_zip_library_if_needed();\n+  return (*ZipOpen)(canonical_path, error_msg);\n+}\n+\n@@ -726,2 +734,2 @@\n-    char* canonical_path = NEW_RESOURCE_ARRAY_IN_THREAD(thread, char, JVM_MAXPATHLEN);\n-    if (!get_canonical_path(path, canonical_path, JVM_MAXPATHLEN)) {\n+    const char* canonical_path = get_canonical_path(path, thread);\n+    if (canonical_path == NULL) {\n@@ -741,8 +749,1 @@\n-      jzfile* zip;\n-      {\n-        \/\/ enable call to C land\n-        ThreadToNativeFromVM ttn(thread);\n-        HandleMark hm(thread);\n-        load_zip_library_if_needed();\n-        zip = (*ZipOpen)(canonical_path, &error_msg);\n-      }\n+      jzfile* zip = open_zip_file(canonical_path, &error_msg, thread);\n@@ -787,2 +788,4 @@\n-      char canonical_path[JVM_MAXPATHLEN];\n-      if (get_canonical_path(path, canonical_path, JVM_MAXPATHLEN)) {\n+      JavaThread* thread = JavaThread::current();\n+      ResourceMark rm(thread);\n+      const char* canonical_path = get_canonical_path(path, thread);\n+      if (canonical_path != NULL) {\n@@ -790,9 +793,1 @@\n-        jzfile* zip;\n-        {\n-          \/\/ enable call to C land\n-          JavaThread* thread = JavaThread::current();\n-          ThreadToNativeFromVM ttn(thread);\n-          HandleMark hm(thread);\n-          load_zip_library_if_needed();\n-          zip = (*ZipOpen)(canonical_path, &error_msg);\n-        }\n+        jzfile* zip = open_zip_file(canonical_path, &error_msg, thread);\n@@ -1337,4 +1332,3 @@\n-    char* canonical_path_table_entry = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, char, JVM_MAXPATHLEN);\n-\n-    \/\/ save the path from the file: protocol or the module name from the jrt: protocol\n-    \/\/ if no protocol prefix is found, path is the same as stream->source()\n+    \/\/ Save the path from the file: protocol or the module name from the jrt: protocol\n+    \/\/ if no protocol prefix is found, path is the same as stream->source(). This path\n+    \/\/ must be valid since the class has been successfully parsed.\n@@ -1342,5 +1336,1 @@\n-    char* canonical_class_src_path = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, char, JVM_MAXPATHLEN);\n-    bool success = get_canonical_path(path, canonical_class_src_path, JVM_MAXPATHLEN);\n-    \/\/ The path is from the ClassFileStream. Since a ClassFileStream has been created successfully in functions\n-    \/\/ such as ClassLoader::load_class(), its source path must be valid.\n-    assert(success, \"must be valid path\");\n+    assert(path != NULL, \"sanity\");\n@@ -1349,1 +1339,0 @@\n-      success = get_canonical_path(ent->name(), canonical_path_table_entry, JVM_MAXPATHLEN);\n@@ -1352,1 +1341,1 @@\n-      assert(success, \"must be valid path\");\n+      assert(ent->name() != NULL, \"sanity\");\n@@ -1355,1 +1344,4 @@\n-      if (strcmp(canonical_path_table_entry, canonical_class_src_path) == 0) {\n+      \/\/ src may come from the App\/Platform class loaders, which would canonicalize\n+      \/\/ the file name. We cannot use strcmp to check for equality against ent->name().\n+      \/\/ We must use os::same_files (which is faster than canonicalizing ent->name()).\n+      if (os::same_files(ent->name(), path)) {\n@@ -1599,5 +1591,5 @@\n-bool ClassLoader::get_canonical_path(const char* orig, char* out, int len) {\n-  assert(orig != NULL && out != NULL && len > 0, \"bad arguments\");\n-  JavaThread* THREAD = JavaThread::current();\n-  ResourceMark rm(THREAD);\n-\n+char* ClassLoader::get_canonical_path(const char* orig, Thread* thread) {\n+  assert(orig != NULL, \"bad arguments\");\n+  \/\/ caller needs to allocate ResourceMark for the following output buffer\n+  char* canonical_path = NEW_RESOURCE_ARRAY_IN_THREAD(thread, char, JVM_MAXPATHLEN);\n+  ResourceMark rm(thread);\n@@ -1605,1 +1597,1 @@\n-  char* orig_copy = NEW_RESOURCE_ARRAY_IN_THREAD(THREAD, char, strlen(orig)+1);\n+  char* orig_copy = NEW_RESOURCE_ARRAY_IN_THREAD(thread, char, strlen(orig)+1);\n@@ -1607,2 +1599,2 @@\n-  if ((CanonicalizeEntry)(os::native_path(orig_copy), out, len) < 0) {\n-    return false;\n+  if ((CanonicalizeEntry)(os::native_path(orig_copy), canonical_path, JVM_MAXPATHLEN) < 0) {\n+    return NULL;\n@@ -1610,1 +1602,1 @@\n-  return true;\n+  return canonical_path;\n","filename":"src\/hotspot\/share\/classfile\/classLoader.cpp","additions":34,"deletions":42,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -378,2 +378,2 @@\n-    InstanceKlass* k = SystemDictionaryShared::dump_time_resolve_super_or_fail(class_name,\n-        super_name, class_loader, protection_domain, is_superclass, CHECK_NULL);\n+    InstanceKlass* k = SystemDictionaryShared::lookup_super_for_unregistered_class(class_name,\n+                           super_name, is_superclass);\n","filename":"src\/hotspot\/share\/classfile\/systemDictionary.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -67,0 +67,1 @@\n+#include \"runtime\/threadSMR.hpp\"\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -470,2 +470,0 @@\n-  int localNum;\n-\n@@ -698,1 +696,1 @@\n-      store_two(localNum = instruction->get_index());\n+      store_two(instruction->get_index());\n","filename":"src\/hotspot\/share\/compiler\/methodLiveness.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1233,1 +1233,1 @@\n-  vframe* vf = vframe::new_vframe(fst.current(), fst.register_map(), thread);\n+  vframe* vf = vframe::new_vframe(fst, thread);\n@@ -1343,1 +1343,1 @@\n-          vf = vframe::new_vframe(fst.current(), fst.register_map(), thread);\n+          vf = vframe::new_vframe(fst, thread);\n@@ -1370,1 +1370,1 @@\n-    vf = vframe::new_vframe(fst.current(), fst.register_map(), thread);\n+    vf = vframe::new_vframe(fst, thread);\n@@ -1578,77 +1578,7 @@\n-\/\/ Creates a scope where the current thread is attached and detached\n-\/\/ from HotSpot if it wasn't already attached when entering the scope.\n-extern \"C\" int jio_printf(const char *fmt, ...);\n-class AttachDetach : public StackObj {\n- public:\n-  bool _attached;\n-  AttachDetach(JNIEnv* env, JavaThread* current_thread) {\n-    if (current_thread == NULL) {\n-      extern struct JavaVM_ main_vm;\n-      JNIEnv* hotspotEnv;\n-      jint res = main_vm.AttachCurrentThread((void**)&hotspotEnv, NULL);\n-      _attached = res == JNI_OK;\n-      static volatile int report_attach_error = 0;\n-      if (res != JNI_OK && report_attach_error == 0 && Atomic::cmpxchg(&report_attach_error, 0, 1) == 0) {\n-        \/\/ Only report an attach error once\n-        jio_printf(\"Warning: attaching current thread to VM failed with %d (future attach errors are suppressed)\\n\", res);\n-      }\n-    } else {\n-      _attached = false;\n-    }\n-  }\n-  ~AttachDetach() {\n-    if (_attached && get_current_thread() != NULL) {\n-      extern struct JavaVM_ main_vm;\n-      jint res = main_vm.DetachCurrentThread();\n-      static volatile int report_detach_error = 0;\n-      if (res != JNI_OK && report_detach_error == 0 && Atomic::cmpxchg(&report_detach_error, 0, 1) == 0) {\n-        \/\/ Only report an attach error once\n-        jio_printf(\"Warning: detaching current thread from VM failed with %d (future attach errors are suppressed)\\n\", res);\n-      }\n-    }\n-  }\n-};\n-\n-C2V_VMENTRY_PREFIX(jint, writeDebugOutput, (JNIEnv* env, jobject, jbyteArray bytes, jint offset, jint length, bool flush, bool can_throw))\n-  AttachDetach ad(env, thread);\n-  bool use_tty = true;\n-  if (thread == NULL) {\n-    if (!ad._attached) {\n-      \/\/ Can only use tty if the current thread is attached\n-      JVMCI_event_1(\"Cannot write to tty on unattached thread\");\n-      return 0;\n-    }\n-    thread = get_current_thread();\n-  }\n-  JVMCITraceMark jtm(\"writeDebugOutput\");\n-  C2V_BLOCK(void, writeDebugOutput, (JNIEnv* env, jobject, jbyteArray bytes, jint offset, jint length))\n-  if (bytes == NULL) {\n-    if (can_throw) {\n-      JVMCI_THROW_0(NullPointerException);\n-    }\n-    return -1;\n-  }\n-  JVMCIPrimitiveArray array = JVMCIENV->wrap(bytes);\n-\n-  \/\/ Check if offset and length are non negative.\n-  if (offset < 0 || length < 0) {\n-    if (can_throw) {\n-      JVMCI_THROW_0(ArrayIndexOutOfBoundsException);\n-    }\n-    return -2;\n-  }\n-  \/\/ Check if the range is valid.\n-  int array_length = JVMCIENV->get_length(array);\n-  if ((((unsigned int) length + (unsigned int) offset) > (unsigned int) array_length)) {\n-    if (can_throw) {\n-      JVMCI_THROW_0(ArrayIndexOutOfBoundsException);\n-    }\n-    return -2;\n-  }\n-  jbyte buffer[O_BUFLEN];\n-  while (length > 0) {\n-    int copy_len = MIN2(length, (jint)O_BUFLEN);\n-    JVMCIENV->copy_bytes_to(array, buffer, offset, copy_len);\n-    tty->write((char*) buffer, copy_len);\n-    length -= O_BUFLEN;\n-    offset += O_BUFLEN;\n+\/\/ Use of tty does not require the current thread to be attached to the VM\n+\/\/ so no need for a full C2V_VMENTRY transition.\n+C2V_VMENTRY_PREFIX(void, writeDebugOutput, (JNIEnv* env, jobject, jlong buffer, jint length, bool flush))\n+  if (length <= 8) {\n+    tty->write((char*) &buffer, length);\n+  } else {\n+    tty->write((char*) buffer, length);\n@@ -1659,1 +1589,0 @@\n-  return 0;\n@@ -1662,1 +1591,3 @@\n-C2V_VMENTRY(void, flushDebugOutput, (JNIEnv* env, jobject))\n+\/\/ Use of tty does not require the current thread to be attached to the VM\n+\/\/ so no need for a full C2V_VMENTRY transition.\n+C2V_VMENTRY_PREFIX(void, flushDebugOutput, (JNIEnv* env, jobject))\n@@ -2796,1 +2727,1 @@\n-  {CC \"writeDebugOutput\",                             CC \"([BIIZZ)I\",                                                                       FN_PTR(writeDebugOutput)},\n+  {CC \"writeDebugOutput\",                             CC \"(JIZ)V\",                                                                          FN_PTR(writeDebugOutput)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":14,"deletions":83,"binary":false,"changes":97,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2012, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -335,0 +335,17 @@\n+  \/\/ Convert (x >>> rshift) + (x << lshift) into RotateRight(x, rshift)\n+  if (Matcher::match_rule_supported(Op_RotateRight) &&\n+      ((op1 == Op_URShiftI && op2 == Op_LShiftI) || (op1 == Op_LShiftI && op2 == Op_URShiftI)) &&\n+      in1->in(1) != NULL && in1->in(1) == in2->in(1)) {\n+    Node* rshift = op1 == Op_URShiftI ? in1->in(2) : in2->in(2);\n+    Node* lshift = op1 == Op_URShiftI ? in2->in(2) : in1->in(2);\n+    if (rshift != NULL && lshift != NULL) {\n+      const TypeInt* rshift_t = phase->type(rshift)->isa_int();\n+      const TypeInt* lshift_t = phase->type(lshift)->isa_int();\n+      if (lshift_t != NULL && lshift_t->is_con() &&\n+          rshift_t != NULL && rshift_t->is_con() &&\n+          ((lshift_t->get_con() & 0x1F) == (32 - (rshift_t->get_con() & 0x1F)))) {\n+        return new RotateRightNode(in1->in(1), phase->intcon(rshift_t->get_con() & 0x1F), TypeInt::INT);\n+      }\n+    }\n+  }\n+\n@@ -451,0 +468,18 @@\n+  \/\/ Convert (x >>> rshift) + (x << lshift) into RotateRight(x, rshift)\n+  if (Matcher::match_rule_supported(Op_RotateRight) &&\n+      ((op1 == Op_URShiftL && op2 == Op_LShiftL) || (op1 == Op_LShiftL && op2 == Op_URShiftL)) &&\n+      in1->in(1) != NULL && in1->in(1) == in2->in(1)) {\n+    Node* rshift = op1 == Op_URShiftL ? in1->in(2) : in2->in(2);\n+    Node* lshift = op1 == Op_URShiftL ? in2->in(2) : in1->in(2);\n+    if (rshift != NULL && lshift != NULL) {\n+      const TypeInt* rshift_t = phase->type(rshift)->isa_int();\n+      const TypeInt* lshift_t = phase->type(lshift)->isa_int();\n+      if (lshift_t != NULL && lshift_t->is_con() &&\n+          rshift_t != NULL && rshift_t->is_con() &&\n+          ((lshift_t->get_con() & 0x3F) == (64 - (rshift_t->get_con() & 0x3F)))) {\n+        return new RotateRightNode(in1->in(1), phase->intcon(rshift_t->get_con() & 0x3F), TypeLong::LONG);\n+      }\n+    }\n+  }\n+\n+\n","filename":"src\/hotspot\/share\/opto\/addnode.cpp","additions":36,"deletions":1,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -911,2 +911,1 @@\n-    } else {\n-      assert(hi_test == BoolTest::le, \"bad test\");\n+    } else if (hi_test == BoolTest::le) {\n@@ -917,2 +916,1 @@\n-      } else {\n-        assert(lo_test == BoolTest::gt || lo_test == BoolTest::le, \"bad test\");\n+      } else if (lo_test == BoolTest::gt || lo_test == BoolTest::le) {\n@@ -922,0 +920,3 @@\n+      } else {\n+        assert(false, \"unhandled lo_test: %d\", lo_test);\n+        return false;\n@@ -923,0 +924,3 @@\n+    } else {\n+      assert(false, \"unhandled hi_test: %d\", hi_test);\n+      return false;\n@@ -958,2 +962,1 @@\n-      } else {\n-        assert(hi_test == BoolTest::le || hi_test == BoolTest::gt, \"bad test\");\n+      } else if (hi_test == BoolTest::le || hi_test == BoolTest::gt) {\n@@ -963,0 +966,3 @@\n+      } else {\n+        assert(false, \"unhandled hi_test: %d\", hi_test);\n+        return false;\n@@ -968,2 +974,1 @@\n-      } else {\n-        assert(hi_test == BoolTest::le || hi_test == BoolTest::gt, \"bad test\");\n+      } else if (hi_test == BoolTest::le || hi_test == BoolTest::gt) {\n@@ -973,0 +978,3 @@\n+      } else {\n+        assert(false, \"unhandled hi_test: %d\", hi_test);\n+        return false;\n@@ -974,0 +982,3 @@\n+    } else {\n+      assert(false, \"unhandled lo_test: %d\", lo_test);\n+      return false;\n","filename":"src\/hotspot\/share\/opto\/ifnode.cpp","additions":19,"deletions":8,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -333,0 +333,2 @@\n+  Unique_Node_List live_nodes;\n+  Compile::current()->identify_useful_nodes(live_nodes);\n@@ -336,1 +338,4 @@\n-    if(n != NULL && n != sentinel_node && n->is_Type() && n->outcnt() > 0) {\n+    if (n != NULL &&\n+        n != sentinel_node &&\n+        n->is_Type() &&\n+        live_nodes.member(n)) {\n","filename":"src\/hotspot\/share\/opto\/phaseX.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"runtime\/vmOperations.hpp\"\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+#include \"runtime\/vmOperations.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -33,0 +33,1 @@\n+#include \"runtime\/safefetch.hpp\"\n@@ -34,1 +35,0 @@\n-#include \"runtime\/stubRoutines.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -497,20 +497,0 @@\n-\/\/ Safefetch allows to load a value from a location that's not known\n-\/\/ to be valid. If the load causes a fault, the error value is returned.\n-inline int SafeFetch32(int* adr, int errValue) {\n-  assert(StubRoutines::SafeFetch32_stub(), \"stub not yet generated\");\n-  return StubRoutines::SafeFetch32_stub()(adr, errValue);\n-}\n-inline intptr_t SafeFetchN(intptr_t* adr, intptr_t errValue) {\n-  assert(StubRoutines::SafeFetchN_stub(), \"stub not yet generated\");\n-  return StubRoutines::SafeFetchN_stub()(adr, errValue);\n-}\n-\n-\n-\/\/ returns true if SafeFetch32 and SafeFetchN can be used safely (stubroutines are already generated)\n-inline bool CanUseSafeFetch32() {\n-  return StubRoutines::SafeFetch32_stub() ? true : false;\n-}\n-\n-inline bool CanUseSafeFetchN() {\n-  return StubRoutines::SafeFetchN_stub() ? true : false;\n-}\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":1,"deletions":21,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -0,0 +1,175 @@\n+\/*\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_RUNTIME_VMOPERATION_HPP\n+#define SHARE_RUNTIME_VMOPERATION_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+\n+\/\/ The following classes are used for operations\n+\/\/ initiated by a Java thread but that must\n+\/\/ take place in the VMThread.\n+\n+#define VM_OP_ENUM(type)   VMOp_##type,\n+\n+\/\/ Note: When new VM_XXX comes up, add 'XXX' to the template table.\n+#define VM_OPS_DO(template)                       \\\n+  template(None)                                  \\\n+  template(Cleanup)                               \\\n+  template(ThreadDump)                            \\\n+  template(PrintThreads)                          \\\n+  template(FindDeadlocks)                         \\\n+  template(ClearICs)                              \\\n+  template(ForceSafepoint)                        \\\n+  template(ForceAsyncSafepoint)                   \\\n+  template(DeoptimizeFrame)                       \\\n+  template(DeoptimizeAll)                         \\\n+  template(ZombieAll)                             \\\n+  template(Verify)                                \\\n+  template(PrintJNI)                              \\\n+  template(HeapDumper)                            \\\n+  template(DeoptimizeTheWorld)                    \\\n+  template(CollectForMetadataAllocation)          \\\n+  template(GC_HeapInspection)                     \\\n+  template(GenCollectFull)                        \\\n+  template(GenCollectFullConcurrent)              \\\n+  template(GenCollectForAllocation)               \\\n+  template(ParallelGCFailedAllocation)            \\\n+  template(ParallelGCSystemGC)                    \\\n+  template(G1CollectForAllocation)                \\\n+  template(G1CollectFull)                         \\\n+  template(G1Concurrent)                          \\\n+  template(G1TryInitiateConcMark)                 \\\n+  template(ZMarkStart)                            \\\n+  template(ZMarkEnd)                              \\\n+  template(ZRelocateStart)                        \\\n+  template(ZVerify)                               \\\n+  template(HandshakeOneThread)                    \\\n+  template(HandshakeAllThreads)                   \\\n+  template(HandshakeFallback)                     \\\n+  template(EnableBiasedLocking)                   \\\n+  template(BulkRevokeBias)                        \\\n+  template(PopulateDumpSharedSpace)               \\\n+  template(JNIFunctionTableCopier)                \\\n+  template(RedefineClasses)                       \\\n+  template(GetObjectMonitorUsage)                 \\\n+  template(GetAllStackTraces)                     \\\n+  template(GetThreadListStackTraces)              \\\n+  template(ChangeBreakpoints)                     \\\n+  template(GetOrSetLocal)                         \\\n+  template(ChangeSingleStep)                      \\\n+  template(HeapWalkOperation)                     \\\n+  template(HeapIterateOperation)                  \\\n+  template(ReportJavaOutOfMemory)                 \\\n+  template(JFRCheckpoint)                         \\\n+  template(ShenandoahFullGC)                      \\\n+  template(ShenandoahInitMark)                    \\\n+  template(ShenandoahFinalMarkStartEvac)          \\\n+  template(ShenandoahInitUpdateRefs)              \\\n+  template(ShenandoahFinalUpdateRefs)             \\\n+  template(ShenandoahDegeneratedGC)               \\\n+  template(Exit)                                  \\\n+  template(LinuxDllLoad)                          \\\n+  template(RotateGCLog)                           \\\n+  template(WhiteBoxOperation)                     \\\n+  template(JVMCIResizeCounters)                   \\\n+  template(ClassLoaderStatsOperation)             \\\n+  template(ClassLoaderHierarchyOperation)         \\\n+  template(DumpHashtable)                         \\\n+  template(DumpTouchedMethods)                    \\\n+  template(CleanClassLoaderDataMetaspaces)        \\\n+  template(PrintCompileQueue)                     \\\n+  template(PrintClassHierarchy)                   \\\n+  template(ThreadSuspend)                         \\\n+  template(ThreadsSuspendJVMTI)                   \\\n+  template(ICBufferFull)                          \\\n+  template(ScavengeMonitors)                      \\\n+  template(PrintMetadata)                         \\\n+  template(GTestExecuteAtSafepoint)               \\\n+  template(JFROldObject)                          \\\n+  template(ClassPrintLayout)                      \\\n+  template(JvmtiPostObjectFree)\n+\n+class Thread;\n+class outputStream;\n+\n+class VM_Operation : public StackObj {\n+ public:\n+  enum VMOp_Type {\n+    VM_OPS_DO(VM_OP_ENUM)\n+    VMOp_Terminating\n+  };\n+\n+ private:\n+  Thread*         _calling_thread;\n+\n+  \/\/ The VM operation name array\n+  static const char* _names[];\n+\n+ public:\n+  VM_Operation() : _calling_thread(NULL) {}\n+\n+  \/\/ VM operation support (used by VM thread)\n+  Thread* calling_thread() const                 { return _calling_thread; }\n+  void set_calling_thread(Thread* thread);\n+\n+  \/\/ Called by VM thread - does in turn invoke doit(). Do not override this\n+  void evaluate();\n+\n+  \/\/ evaluate() is called by the VMThread and in turn calls doit().\n+  \/\/ If the thread invoking VMThread::execute((VM_Operation*) is a JavaThread,\n+  \/\/ doit_prologue() is called in that thread before transferring control to\n+  \/\/ the VMThread.\n+  \/\/ If doit_prologue() returns true the VM operation will proceed, and\n+  \/\/ doit_epilogue() will be called by the JavaThread once the VM operation\n+  \/\/ completes. If doit_prologue() returns false the VM operation is cancelled.\n+  virtual void doit()                            = 0;\n+  virtual bool doit_prologue()                   { return true; };\n+  virtual void doit_epilogue()                   {};\n+\n+  \/\/ Configuration. Override these appropriately in subclasses.\n+  virtual VMOp_Type type() const = 0;\n+  virtual bool allow_nested_vm_operations() const { return false; }\n+\n+  \/\/ You may override skip_thread_oop_barriers to return true if the operation\n+  \/\/ does not access thread-private oops (including frames).\n+  virtual bool skip_thread_oop_barriers() const { return false; }\n+\n+  \/\/ An operation can either be done inside a safepoint\n+  \/\/ or concurrently with Java threads running.\n+  virtual bool evaluate_at_safepoint() const { return true; }\n+\n+  \/\/ Debugging\n+  virtual void print_on_error(outputStream* st) const;\n+  virtual const char* name() const  { return _names[type()]; }\n+  static const char* name(int type) {\n+    assert(type >= 0 && type < VMOp_Terminating, \"invalid VM operation type\");\n+    return _names[type];\n+  }\n+#ifndef PRODUCT\n+  void print_on(outputStream* st) const { print_on_error(st); }\n+#endif\n+};\n+\n+#endif \/\/ SHARE_RUNTIME_VMOPERATION_HPP\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":175,"deletions":0,"binary":false,"changes":175,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -28,1 +28,1 @@\n-#include \"memory\/allocation.hpp\"\n+#include \"runtime\/vmOperation.hpp\"\n@@ -33,141 +33,1 @@\n-\/\/ The following classes are used for operations\n-\/\/ initiated by a Java thread but that must\n-\/\/ take place in the VMThread.\n-\n-#define VM_OP_ENUM(type)   VMOp_##type,\n-\n-\/\/ Note: When new VM_XXX comes up, add 'XXX' to the template table.\n-#define VM_OPS_DO(template)                       \\\n-  template(None)                                  \\\n-  template(Cleanup)                               \\\n-  template(ThreadDump)                            \\\n-  template(PrintThreads)                          \\\n-  template(FindDeadlocks)                         \\\n-  template(ClearICs)                              \\\n-  template(ForceSafepoint)                        \\\n-  template(ForceAsyncSafepoint)                   \\\n-  template(DeoptimizeFrame)                       \\\n-  template(DeoptimizeAll)                         \\\n-  template(ZombieAll)                             \\\n-  template(Verify)                                \\\n-  template(PrintJNI)                              \\\n-  template(HeapDumper)                            \\\n-  template(DeoptimizeTheWorld)                    \\\n-  template(CollectForMetadataAllocation)          \\\n-  template(GC_HeapInspection)                     \\\n-  template(GenCollectFull)                        \\\n-  template(GenCollectFullConcurrent)              \\\n-  template(GenCollectForAllocation)               \\\n-  template(ParallelGCFailedAllocation)            \\\n-  template(ParallelGCSystemGC)                    \\\n-  template(G1CollectForAllocation)                \\\n-  template(G1CollectFull)                         \\\n-  template(G1Concurrent)                          \\\n-  template(G1TryInitiateConcMark)                 \\\n-  template(ZMarkStart)                            \\\n-  template(ZMarkEnd)                              \\\n-  template(ZRelocateStart)                        \\\n-  template(ZVerify)                               \\\n-  template(HandshakeOneThread)                    \\\n-  template(HandshakeAllThreads)                   \\\n-  template(HandshakeFallback)                     \\\n-  template(EnableBiasedLocking)                   \\\n-  template(BulkRevokeBias)                        \\\n-  template(PopulateDumpSharedSpace)               \\\n-  template(JNIFunctionTableCopier)                \\\n-  template(RedefineClasses)                       \\\n-  template(GetObjectMonitorUsage)                 \\\n-  template(GetAllStackTraces)                     \\\n-  template(GetThreadListStackTraces)              \\\n-  template(ChangeBreakpoints)                     \\\n-  template(GetOrSetLocal)                         \\\n-  template(ChangeSingleStep)                      \\\n-  template(HeapWalkOperation)                     \\\n-  template(HeapIterateOperation)                  \\\n-  template(ReportJavaOutOfMemory)                 \\\n-  template(JFRCheckpoint)                         \\\n-  template(ShenandoahFullGC)                      \\\n-  template(ShenandoahInitMark)                    \\\n-  template(ShenandoahFinalMarkStartEvac)          \\\n-  template(ShenandoahInitUpdateRefs)              \\\n-  template(ShenandoahFinalUpdateRefs)             \\\n-  template(ShenandoahDegeneratedGC)               \\\n-  template(Exit)                                  \\\n-  template(LinuxDllLoad)                          \\\n-  template(RotateGCLog)                           \\\n-  template(WhiteBoxOperation)                     \\\n-  template(JVMCIResizeCounters)                   \\\n-  template(ClassLoaderStatsOperation)             \\\n-  template(ClassLoaderHierarchyOperation)         \\\n-  template(DumpHashtable)                         \\\n-  template(DumpTouchedMethods)                    \\\n-  template(CleanClassLoaderDataMetaspaces)        \\\n-  template(PrintCompileQueue)                     \\\n-  template(PrintClassHierarchy)                   \\\n-  template(ThreadSuspend)                         \\\n-  template(ThreadsSuspendJVMTI)                   \\\n-  template(ICBufferFull)                          \\\n-  template(ScavengeMonitors)                      \\\n-  template(PrintMetadata)                         \\\n-  template(GTestExecuteAtSafepoint)               \\\n-  template(JFROldObject)                          \\\n-  template(ClassPrintLayout)                      \\\n-  template(JvmtiPostObjectFree)\n-\n-class VM_Operation : public StackObj {\n- public:\n-  enum VMOp_Type {\n-    VM_OPS_DO(VM_OP_ENUM)\n-    VMOp_Terminating\n-  };\n-\n- private:\n-  Thread*         _calling_thread;\n-\n-  \/\/ The VM operation name array\n-  static const char* _names[];\n-\n- public:\n-  VM_Operation() : _calling_thread(NULL) {}\n-\n-  \/\/ VM operation support (used by VM thread)\n-  Thread* calling_thread() const                 { return _calling_thread; }\n-  void set_calling_thread(Thread* thread);\n-\n-  \/\/ Called by VM thread - does in turn invoke doit(). Do not override this\n-  void evaluate();\n-\n-  \/\/ evaluate() is called by the VMThread and in turn calls doit().\n-  \/\/ If the thread invoking VMThread::execute((VM_Operation*) is a JavaThread,\n-  \/\/ doit_prologue() is called in that thread before transferring control to\n-  \/\/ the VMThread.\n-  \/\/ If doit_prologue() returns true the VM operation will proceed, and\n-  \/\/ doit_epilogue() will be called by the JavaThread once the VM operation\n-  \/\/ completes. If doit_prologue() returns false the VM operation is cancelled.\n-  virtual void doit()                            = 0;\n-  virtual bool doit_prologue()                   { return true; };\n-  virtual void doit_epilogue()                   {};\n-\n-  \/\/ Configuration. Override these appropriately in subclasses.\n-  virtual VMOp_Type type() const = 0;\n-  virtual bool allow_nested_vm_operations() const { return false; }\n-\n-  \/\/ You may override skip_thread_oop_barriers to return true if the operation\n-  \/\/ does not access thread-private oops (including frames).\n-  virtual bool skip_thread_oop_barriers() const { return false; }\n-\n-  \/\/ An operation can either be done inside a safepoint\n-  \/\/ or concurrently with Java threads running.\n-  virtual bool evaluate_at_safepoint() const { return true; }\n-\n-  \/\/ Debugging\n-  virtual void print_on_error(outputStream* st) const;\n-  virtual const char* name() const  { return _names[type()]; }\n-  static const char* name(int type) {\n-    assert(type >= 0 && type < VMOp_Terminating, \"invalid VM operation type\");\n-    return _names[type];\n-  }\n-#ifndef PRODUCT\n-  void print_on(outputStream* st) const { print_on_error(st); }\n-#endif\n-};\n+\/\/ A hodge podge of commonly used VM Operations\n","filename":"src\/hotspot\/share\/runtime\/vmOperations.hpp","additions":3,"deletions":143,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"runtime\/vmOperations.hpp\"\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1996, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1996, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -515,1 +515,1 @@\n-                        suid = Long.valueOf(0);\n+                        suid = 0L;\n@@ -560,1 +560,1 @@\n-            suid = Long.valueOf(0);\n+            suid = 0L;\n@@ -680,1 +680,1 @@\n-        suid = Long.valueOf(0);\n+        suid = 0L;\n@@ -705,1 +705,1 @@\n-        long suid = Long.valueOf(model.getSerialVersionUID());\n+        long suid = model.getSerialVersionUID();\n@@ -803,1 +803,1 @@\n-        suid = Long.valueOf(in.readLong());\n+        suid = in.readLong();\n@@ -1853,1 +1853,1 @@\n-                return Long.valueOf(f.getLong(null));\n+                return f.getLong(null);\n","filename":"src\/java.base\/share\/classes\/java\/io\/ObjectStreamClass.java","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1994, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1994, 2021, Oracle and\/or its affiliates. All rights reserved.\n@@ -87,3 +87,4 @@\n-     * <li>If two objects are equal according to the {@code equals(Object)}\n-     *     method, then calling the {@code hashCode} method on each of\n-     *     the two objects must produce the same integer result.\n+     * <li>If two objects are equal according to the {@link\n+     *     equals(Object) equals} method, then calling the {@code\n+     *     hashCode} method on each of the two objects must produce the\n+     *     same integer result.\n@@ -91,5 +92,5 @@\n-     *     according to the {@link java.lang.Object#equals(java.lang.Object)}\n-     *     method, then calling the {@code hashCode} method on each of the\n-     *     two objects must produce distinct integer results.  However, the\n-     *     programmer should be aware that producing distinct integer results\n-     *     for unequal objects may improve the performance of hash tables.\n+     *     according to the {@link equals(Object) equals} method, then\n+     *     calling the {@code hashCode} method on each of the two objects\n+     *     must produce distinct integer results.  However, the programmer\n+     *     should be aware that producing distinct integer results for\n+     *     unequal objects may improve the performance of hash tables.\n@@ -136,0 +137,1 @@\n+     *\n@@ -137,0 +139,7 @@\n+     * An equivalence relation partitions the elements it operates on\n+     * into <i>equivalence classes<\/i>; all the members of an\n+     * equivalence class are equal to each other. Members of an\n+     * equivalence class are substitutable for each other, at least\n+     * for some purposes.\n+     *\n+     * @implSpec\n@@ -143,2 +152,6 @@\n-     * <p>\n-     * Note that it is generally necessary to override the {@code hashCode}\n+     *\n+     * In other words, under the reference equality equivalence\n+     * relation, each equivalence class only has a single element.\n+     *\n+     * @apiNote\n+     * It is generally necessary to override the {@link hashCode hashCode}\n@@ -192,1 +205,2 @@\n-     * <p>\n+     *\n+     * @implSpec\n@@ -223,1 +237,3 @@\n-     * Returns a string representation of the object. In general, the\n+     * Returns a string representation of the object.\n+     * @apiNote\n+     * In general, the\n@@ -229,1 +245,3 @@\n-     * <p>\n+     * The string output is not necessarily stable over time or across\n+     * JVM invocations.\n+     * @implSpec\n@@ -231,4 +249,5 @@\n-     * the {@code toString} method returns a string consisting of the name\n-     * of the class of which the object is an instance, the at-sign character\n-     * `{@code @}', and the unsigned hexadecimal representation of the hash code\n-     * of the object. In other words, this method returns a string equal to the\n+     * the {@code toString} method for class {@code Object}\n+     * returns a string consisting of the name of the class of which the\n+     * object is an instance, the at-sign character `{@code @}', and\n+     * the unsigned hexadecimal representation of the hash code of the\n+     * object. In other words, this method returns a string equal to the\n","filename":"src\/java.base\/share\/classes\/java\/lang\/Object.java","additions":37,"deletions":18,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -38,0 +38,2 @@\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n@@ -48,0 +50,1 @@\n+import jdk.internal.misc.Unsafe;\n@@ -580,1 +583,1 @@\n-            configStore.printConfig();\n+            configStore.printConfig(this);\n@@ -895,1 +898,40 @@\n-        return compilerToVm.writeDebugOutput(bytes, offset, length, flush, canThrow);\n+        return writeDebugOutput0(compilerToVm, bytes, offset, length, flush, canThrow);\n+    }\n+\n+    \/**\n+     * @see #writeDebugOutput\n+     *\/\n+    static int writeDebugOutput0(CompilerToVM vm, byte[] bytes, int offset, int length, boolean flush, boolean canThrow) {\n+        if (bytes == null) {\n+            if (!canThrow) {\n+                return -1;\n+            }\n+            throw new NullPointerException();\n+        }\n+        if (offset < 0 || length < 0 || offset + length > bytes.length) {\n+            if (!canThrow) {\n+                return -2;\n+            }\n+            throw new ArrayIndexOutOfBoundsException();\n+        }\n+        if (length <= 8) {\n+            ByteBuffer buffer = ByteBuffer.wrap(bytes, offset, length);\n+            if (length != 8) {\n+                ByteBuffer buffer8 = ByteBuffer.allocate(8);\n+                buffer8.put(buffer);\n+                buffer8.position(8);\n+                buffer = buffer8;\n+            }\n+            buffer.order(ByteOrder.nativeOrder());\n+            vm.writeDebugOutput(buffer.getLong(0), length, flush);\n+        } else {\n+            Unsafe unsafe = UnsafeAccess.UNSAFE;\n+            long buffer = unsafe.allocateMemory(length);\n+            try {\n+                unsafe.copyMemory(bytes, vm.ARRAY_BYTE_BASE_OFFSET, null, buffer, length);\n+                vm.writeDebugOutput(buffer, length, flush);\n+            } finally {\n+                unsafe.freeMemory(buffer);\n+            }\n+        }\n+        return 0;\n@@ -913,1 +955,1 @@\n-                compilerToVm.writeDebugOutput(b, off, len, false, true);\n+                writeDebugOutput(b, off, len, false, true);\n@@ -1205,1 +1247,1 @@\n-        compilerToVm.writeDebugOutput(messageBytes, 0, messageBytes.length, true, true);\n+        writeDebugOutput(messageBytes, 0, messageBytes.length, true, true);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk.vm.ci.hotspot\/src\/jdk\/vm\/ci\/hotspot\/HotSpotJVMCIRuntime.java","additions":46,"deletions":4,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -1117,14 +1117,0 @@\n-    \/**\n-     * Given a string, replace all occurrences of 'newStr' with 'oldStr'.\n-     * @param originalStr the string to modify.\n-     * @param oldStr the string to replace.\n-     * @param newStr the string to insert in place of the old string.\n-     *\/\n-    public String replaceText(String originalStr, String oldStr,\n-            String newStr) {\n-        if (oldStr == null || newStr == null || oldStr.equals(newStr)) {\n-            return originalStr;\n-        }\n-        return originalStr.replace(oldStr, newStr);\n-    }\n-\n@@ -2436,0 +2422,2 @@\n+     * Use {@link jdk.javadoc.internal.doclets.formats.html.HtmlDocletWriter#getLocalizedPackageName(PackageElement)}\n+     * to get a localized string for the unnamed package instead.\n@@ -2442,1 +2430,1 @@\n-            return DocletConstants.DEFAULT_PACKAGE_NAME;\n+            return DocletConstants.DEFAULT_ELEMENT_NAME;\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclets\/toolkit\/util\/Utils.java","additions":3,"deletions":15,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -147,0 +147,3 @@\n+    private boolean inIndex;\n+    private boolean inLink;\n+    private boolean inSummary;\n@@ -786,0 +789,3 @@\n+        if (inIndex) {\n+            env.messages.warning(HTML, tree, \"dc.tag.nested.tag\", \"@\" + tree.getTagName());\n+        }\n@@ -793,1 +799,7 @@\n-        return super.visitIndex(tree, ignore);\n+        boolean prevInIndex = inIndex;\n+        try {\n+            inIndex = true;\n+            return super.visitIndex(tree, ignore);\n+        } finally {\n+            inIndex = prevInIndex;\n+        }\n@@ -807,0 +819,4 @@\n+        if (inLink) {\n+            env.messages.warning(HTML, tree, \"dc.tag.nested.tag\", \"@\" + tree.getTagName());\n+        }\n+        boolean prevInLink = inLink;\n@@ -812,0 +828,1 @@\n+            inLink = true;\n@@ -815,0 +832,1 @@\n+            inLink = prevInLink;\n@@ -944,1 +962,1 @@\n-    public Void visitSummary(SummaryTree node, Void aVoid) {\n+    public Void visitSummary(SummaryTree tree, Void aVoid) {\n@@ -946,1 +964,4 @@\n-        int idx = env.currDocComment.getFullBody().indexOf(node);\n+        if (inSummary) {\n+            env.messages.warning(HTML, tree, \"dc.tag.nested.tag\", \"@\" + tree.getTagName());\n+        }\n+        int idx = env.currDocComment.getFullBody().indexOf(tree);\n@@ -950,1 +971,8 @@\n-            env.messages.warning(SYNTAX, node, \"dc.invalid.summary\", node.getTagName());\n+            env.messages.warning(SYNTAX, tree, \"dc.invalid.summary\", tree.getTagName());\n+        }\n+        boolean prevInSummary = inSummary;\n+        try {\n+            inSummary = true;\n+            return super.visitSummary(tree, aVoid);\n+        } finally {\n+            inSummary = prevInSummary;\n@@ -952,1 +980,0 @@\n-        return super.visitSummary(node, aVoid);\n","filename":"src\/jdk.javadoc\/share\/classes\/jdk\/javadoc\/internal\/doclint\/Checker.java","additions":32,"deletions":5,"binary":false,"changes":37,"status":"modified"}]}