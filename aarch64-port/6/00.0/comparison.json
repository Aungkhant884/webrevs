{"files":[{"patch":"@@ -234,1 +234,7 @@\n-      ENABLE_FORTIFY_CFLAGS=\"-D_FORTIFY_SOURCE=2\"\n+      # ASan doesn't work well with _FORTIFY_SOURCE\n+      # See https:\/\/github.com\/google\/sanitizers\/wiki\/AddressSanitizer#faq\n+      if test \"x$ASAN_ENABLED\" = xyes; then\n+        ENABLE_FORTIFY_CFLAGS=\"${DISABLE_FORTIFY_CFLAGS}\"\n+      else\n+        ENABLE_FORTIFY_CFLAGS=\"-D_FORTIFY_SOURCE=2\"\n+      fi\n@@ -561,0 +567,5 @@\n+  OS_CFLAGS=\"$OS_CFLAGS -DLIBC=$OPENJDK_TARGET_LIBC\"\n+  if test \"x$OPENJDK_TARGET_LIBC\" = xmusl; then\n+    OS_CFLAGS=\"$OS_CFLAGS -DMUSL_LIBC\"\n+  fi\n+\n@@ -655,10 +666,4 @@\n-  if test \"x$FLAGS_CPU_BITS\" = x64; then\n-    # -D_LP64=1 is only set on linux and mac. Setting on windows causes diff in\n-    # unpack200.exe.\n-    if test \"x$FLAGS_OS\" = xlinux || test \"x$FLAGS_OS\" = xmacosx; then\n-      $1_DEFINES_CPU_JDK=\"${$1_DEFINES_CPU_JDK} -D_LP64=1\"\n-    fi\n-    if test \"x$FLAGS_OS\" != xaix; then\n-      # xlc on AIX defines _LP64=1 by default and issues a warning if we redefine it.\n-      $1_DEFINES_CPU_JVM=\"${$1_DEFINES_CPU_JVM} -D_LP64=1\"\n-    fi\n+  if test \"x$FLAGS_CPU_BITS\" = x64 && test \"x$FLAGS_OS\" != xaix; then\n+    # xlc on AIX defines _LP64=1 by default and issues a warning if we redefine it.\n+    $1_DEFINES_CPU_JDK=\"${$1_DEFINES_CPU_JDK} -D_LP64=1\"\n+    $1_DEFINES_CPU_JVM=\"${$1_DEFINES_CPU_JVM} -D_LP64=1\"\n","filename":"make\/autoconf\/flags-cflags.m4","additions":16,"deletions":11,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -144,0 +144,1 @@\n+        $d\/cpu\/$(HOTSPOT_TARGET_CPU_ARCH)\/$(HOTSPOT_TARGET_CPU_ARCH)_neon.ad \\\n","filename":"make\/hotspot\/gensrc\/GensrcAdlc.gmk","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -438,1 +438,0 @@\n-  HARFBUZZ_CFLAGS := -DHAVE_OT -DHAVE_FALLBACK -DHAVE_UCDN -DHAVE_ROUND\n@@ -496,1 +495,1 @@\n-        undef missing-field-initializers, \\\n+        undef missing-field-initializers range-loop-analysis, \\\n","filename":"make\/modules\/java.desktop\/lib\/Awt2dLibraries.gmk","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -617,3 +617,1 @@\n-\/\/ 2) reg_class compiler_method_reg        ( \/* as def'd in frame section *\/ )\n-\/\/ 2) reg_class interpreter_method_reg     ( \/* as def'd in frame section *\/ )\n-\/\/ 3) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n+\/\/ 2) reg_class stack_slots( \/* one chunk of stack-based \"registers\" *\/ )\n@@ -2413,0 +2411,6 @@\n+    case Op_VectorLoadShuffle:\n+    case Op_VectorRearrange:\n+      if (vlen < 4) {\n+        return false;\n+      }\n+      break;\n@@ -2424,0 +2428,4 @@\n+bool Matcher::supports_vector_variable_shifts(void) {\n+  return true;\n+}\n+\n@@ -2469,2 +2477,2 @@\n-  } else {\n-    \/\/  For the moment limit the vector size to 8 bytes with NEON.\n+  } else { \/\/ NEON\n+    \/\/ Limit the vector size to 8 bytes\n@@ -2472,0 +2480,7 @@\n+    if (bt == T_BYTE) {\n+      \/\/ To support vector api shuffle\/rearrange.\n+      size = 4;\n+    } else if (bt == T_BOOLEAN) {\n+      \/\/ To support vector api load\/store mask.\n+      size = 2;\n+    }\n@@ -2473,1 +2488,1 @@\n-    return size;\n+    return MIN2(size,max_size);\n@@ -2492,0 +2507,3 @@\n+    \/\/ For 16-bit\/32-bit mask vector, reuse VecD.\n+    case  2:\n+    case  4:\n@@ -2591,5 +2609,0 @@\n-\/\/ No-op on amd64\n-void Matcher::pd_implicit_null_fixup(MachNode *node, uint idx) {\n-  Unimplemented();\n-}\n-\n@@ -3134,0 +3147,6 @@\n+  enc_class aarch64_enc_ldrvH(vecD dst, memory mem) %{\n+    FloatRegister dst_reg = as_FloatRegister($dst$$reg);\n+    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::ldr, dst_reg, MacroAssembler::H,\n+       $mem->opcode(), as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+\n@@ -3152,0 +3171,6 @@\n+  enc_class aarch64_enc_strvH(vecD src, memory mem) %{\n+    FloatRegister src_reg = as_FloatRegister($src$$reg);\n+    loadStore(C2_MacroAssembler(&cbuf), &MacroAssembler::str, src_reg, MacroAssembler::H,\n+       $mem->opcode(), as_Register($mem$$base), $mem$$index, $mem$$scale, $mem$$disp);\n+  %}\n+\n@@ -3743,0 +3768,4 @@\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n@@ -3748,1 +3777,4 @@\n-\n+      if (call == NULL) {\n+        ciEnv::current()->record_failure(\"CodeCache is full\");\n+        return;\n+      }\n@@ -3756,4 +3788,2 @@\n-    if (call == NULL) {\n-      ciEnv::current()->record_failure(\"CodeCache is full\");\n-      return;\n-    } else if (UseSVE > 0 && Compile::current()->max_vector_size() >= 16) {\n+\n+    if (UseSVE > 0 && Compile::current()->max_vector_size() >= 16) {\n@@ -4061,3 +4091,0 @@\n-  \/\/ Method Register when calling interpreter.\n-  interpreter_method_reg(R12);\n-\n@@ -4255,0 +4282,20 @@\n+operand immI_2()\n+%{\n+  predicate(n->get_int() == 2);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+operand immI_4()\n+%{\n+  predicate(n->get_int() == 4);\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -5631,10 +5678,0 @@\n-operand interpreter_method_RegP(iRegP reg)\n-%{\n-  constraint(ALLOC_IN_RC(method_reg)); \/\/ interpreter_method_reg\n-  match(reg);\n-  match(iRegPNoSp);\n-  op_cost(0);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n@@ -11225,0 +11262,1 @@\n+\/\/ This section is generated from aarch64_ad.m4\n@@ -14695,1 +14733,5 @@\n-    __ zero_words($base$$Register, $cnt$$Register);\n+    address tpc = __ zero_words($base$$Register, $cnt$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -15973,2 +16015,2 @@\n-  ins_encode( aarch64_enc_java_static_call(meth),\n-              aarch64_enc_call_epilog );\n+  ins_encode(aarch64_enc_java_static_call(meth),\n+             aarch64_enc_call_epilog);\n@@ -15992,2 +16034,2 @@\n-  ins_encode( aarch64_enc_java_dynamic_call(meth),\n-               aarch64_enc_call_epilog );\n+  ins_encode(aarch64_enc_java_dynamic_call(meth),\n+             aarch64_enc_call_epilog);\n@@ -16379,1 +16421,1 @@\n-instruct string_indexofU_char(iRegP_R1 str1, iRegI_R2 cnt1, iRegI_R3 ch,\n+instruct string_indexof_char(iRegP_R1 str1, iRegI_R2 cnt1, iRegI_R3 ch,\n@@ -16384,0 +16426,1 @@\n+  predicate(((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::U);\n@@ -16387,1 +16430,1 @@\n-  format %{ \"String IndexOf char[] $str1,$cnt1,$ch -> $result\" %}\n+  format %{ \"StringUTF16 IndexOf char[] $str1,$cnt1,$ch -> $result\" %}\n@@ -16397,0 +16440,19 @@\n+instruct stringL_indexof_char(iRegP_R1 str1, iRegI_R2 cnt1, iRegI_R3 ch,\n+                              iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n+                              iRegINoSp tmp3, rFlagsReg cr)\n+%{\n+  match(Set result (StrIndexOfChar (Binary str1 cnt1) ch));\n+  predicate(((StrIndexOfCharNode*)n)->encoding() == StrIntrinsicNode::L);\n+  effect(USE_KILL str1, USE_KILL cnt1, USE_KILL ch,\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+\n+  format %{ \"StringLatin1 IndexOf char[] $str1,$cnt1,$ch -> $result\" %}\n+\n+  ins_encode %{\n+    __ stringL_indexof_char($str1$$Register, $cnt1$$Register, $ch$$Register,\n+                           $result$$Register, $tmp1$$Register, $tmp2$$Register,\n+                           $tmp3$$Register);\n+  %}\n+  ins_pipe(pipe_class_memory);\n+%}\n+\n@@ -16439,4 +16501,8 @@\n-    __ arrays_equals($ary1$$Register, $ary2$$Register,\n-                     $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                     $result$$Register, $tmp$$Register, 1);\n-    %}\n+    address tpc = __ arrays_equals($ary1$$Register, $ary2$$Register,\n+                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                                   $result$$Register, $tmp$$Register, 1);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+  %}\n@@ -16456,3 +16522,7 @@\n-    __ arrays_equals($ary1$$Register, $ary2$$Register,\n-                     $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n-                     $result$$Register, $tmp$$Register, 2);\n+    address tpc = __ arrays_equals($ary1$$Register, $ary2$$Register,\n+                                   $tmp1$$Register, $tmp2$$Register, $tmp3$$Register,\n+                                   $result$$Register, $tmp$$Register, 2);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16469,1 +16539,5 @@\n-    __ has_negatives($ary1$$Register, $len$$Register, $result$$Register);\n+    address tpc = __ has_negatives($ary1$$Register, $len$$Register, $result$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16502,2 +16576,7 @@\n-    __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n-                          $tmp1$$FloatRegister, $tmp2$$FloatRegister, $tmp3$$FloatRegister, $tmp4$$Register);\n+    address tpc = __ byte_array_inflate($src$$Register, $dst$$Register, $len$$Register,\n+                                        $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n+                                        $tmp3$$FloatRegister, $tmp4$$Register);\n+    if (tpc == NULL) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n@@ -16831,0 +16910,1 @@\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -16850,0 +16930,1 @@\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -16868,0 +16949,1 @@\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -16887,0 +16969,1 @@\n+  predicate(n->in(2)->bottom_type()->is_vect()->element_basic_type() == T_INT);\n@@ -17968,2 +18051,1 @@\n-    __ fabs(as_FloatRegister($dst$$reg), __ T2S,\n-            as_FloatRegister($src$$reg));\n+    __ fabs(as_FloatRegister($dst$$reg), __ T2S, as_FloatRegister($src$$reg));\n@@ -17981,2 +18063,1 @@\n-    __ fabs(as_FloatRegister($dst$$reg), __ T4S,\n-            as_FloatRegister($src$$reg));\n+    __ fabs(as_FloatRegister($dst$$reg), __ T4S, as_FloatRegister($src$$reg));\n@@ -17994,2 +18075,1 @@\n-    __ fabs(as_FloatRegister($dst$$reg), __ T2D,\n-            as_FloatRegister($src$$reg));\n+    __ fabs(as_FloatRegister($dst$$reg), __ T2D, as_FloatRegister($src$$reg));\n@@ -18136,1 +18216,2 @@\n-  predicate(n->as_Vector()->length_in_bytes() == 8);\n+  predicate(n->as_Vector()->length_in_bytes() == 4 ||\n+            n->as_Vector()->length_in_bytes() == 8);\n@@ -18844,0 +18925,210 @@\n+instruct vsraa8B_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (AddVB dst (RShiftVB src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (8B)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 8) sh = 7;\n+    __ ssra(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg), sh);\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsraa16B_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 16);\n+  match(Set dst (AddVB dst (RShiftVB src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (16B)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 8) sh = 7;\n+    __ ssra(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg), sh);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsraa4S_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (AddVS dst (RShiftVS src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (4H)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 16) sh = 15;\n+    __ ssra(as_FloatRegister($dst$$reg), __ T4H,\n+           as_FloatRegister($src$$reg), sh);\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsraa8S_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (AddVS dst (RShiftVS src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (8H)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 16) sh = 15;\n+    __ ssra(as_FloatRegister($dst$$reg), __ T8H,\n+           as_FloatRegister($src$$reg), sh);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsraa2I_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (AddVI dst (RShiftVI src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (2S)\" %}\n+  ins_encode %{\n+    __ ssra(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsraa4I_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (AddVI dst (RShiftVI src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (4S)\" %}\n+  ins_encode %{\n+    __ ssra(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsraa2L_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (AddVL dst (RShiftVL src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"ssra    $dst, $src, $shift\\t# vector (2D)\" %}\n+  ins_encode %{\n+    __ ssra(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsrla8B_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (AddVB dst (URShiftVB src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (8B)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 8) {\n+      __ eor(as_FloatRegister($src$$reg), __ T8B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    } else {\n+      __ usra(as_FloatRegister($dst$$reg), __ T8B,\n+             as_FloatRegister($src$$reg), sh);\n+    }\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsrla16B_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 16);\n+  match(Set dst (AddVB dst (URShiftVB src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (16B)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 8) {\n+      __ eor(as_FloatRegister($src$$reg), __ T16B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    } else {\n+      __ usra(as_FloatRegister($dst$$reg), __ T16B,\n+             as_FloatRegister($src$$reg), sh);\n+    }\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsrla4S_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (AddVS dst (URShiftVS src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (4H)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 16) {\n+      __ eor(as_FloatRegister($src$$reg), __ T8B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    } else {\n+      __ ushr(as_FloatRegister($dst$$reg), __ T4H,\n+             as_FloatRegister($src$$reg), sh);\n+    }\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsrla8S_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 8);\n+  match(Set dst (AddVS dst (URShiftVS src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (8H)\" %}\n+  ins_encode %{\n+    int sh = (int)$shift$$constant;\n+    if (sh >= 16) {\n+      __ eor(as_FloatRegister($src$$reg), __ T16B,\n+             as_FloatRegister($src$$reg),\n+             as_FloatRegister($src$$reg));\n+    } else {\n+      __ usra(as_FloatRegister($dst$$reg), __ T8H,\n+             as_FloatRegister($src$$reg), sh);\n+    }\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsrla2I_imm(vecD dst, vecD src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (AddVI dst (URShiftVI src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (2S)\" %}\n+  ins_encode %{\n+    __ usra(as_FloatRegister($dst$$reg), __ T2S,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift64_imm);\n+%}\n+\n+instruct vsrla4I_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 4);\n+  match(Set dst (AddVI dst (URShiftVI src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (4S)\" %}\n+  ins_encode %{\n+    __ usra(as_FloatRegister($dst$$reg), __ T4S,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n+instruct vsrla2L_imm(vecX dst, vecX src, immI shift) %{\n+  predicate(n->as_Vector()->length() == 2);\n+  match(Set dst (AddVL dst (URShiftVL src (RShiftCntV shift))));\n+  ins_cost(INSN_COST);\n+  format %{ \"usra    $dst, $src, $shift\\t# vector (2D)\" %}\n+  ins_encode %{\n+    __ usra(as_FloatRegister($dst$$reg), __ T2D,\n+            as_FloatRegister($src$$reg),\n+            (int)$shift$$constant);\n+  %}\n+  ins_pipe(vshift128_imm);\n+%}\n+\n@@ -18960,6 +19251,6 @@\n-     __ cnt(as_FloatRegister($dst$$reg), __ T16B,\n-            as_FloatRegister($src$$reg));\n-     __ uaddlp(as_FloatRegister($dst$$reg), __ T16B,\n-               as_FloatRegister($dst$$reg));\n-     __ uaddlp(as_FloatRegister($dst$$reg), __ T8H,\n-               as_FloatRegister($dst$$reg));\n+    __ cnt(as_FloatRegister($dst$$reg), __ T16B,\n+           as_FloatRegister($src$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T16B,\n+              as_FloatRegister($dst$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T8H,\n+              as_FloatRegister($dst$$reg));\n@@ -18979,6 +19270,6 @@\n-     __ cnt(as_FloatRegister($dst$$reg), __ T8B,\n-            as_FloatRegister($src$$reg));\n-     __ uaddlp(as_FloatRegister($dst$$reg), __ T8B,\n-               as_FloatRegister($dst$$reg));\n-     __ uaddlp(as_FloatRegister($dst$$reg), __ T4H,\n-               as_FloatRegister($dst$$reg));\n+    __ cnt(as_FloatRegister($dst$$reg), __ T8B,\n+           as_FloatRegister($src$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T8B,\n+              as_FloatRegister($dst$$reg));\n+    __ uaddlp(as_FloatRegister($dst$$reg), __ T4H,\n+              as_FloatRegister($dst$$reg));\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":356,"deletions":65,"binary":false,"changes":421,"status":"modified"},{"patch":"@@ -708,1 +708,1 @@\n-address MacroAssembler::trampoline_call(Address entry, CodeBuffer *cbuf) {\n+address MacroAssembler::trampoline_call(Address entry, CodeBuffer* cbuf) {\n@@ -729,0 +729,1 @@\n+        postcond(pc() == badAddress);\n@@ -742,0 +743,1 @@\n+  postcond(pc() != badAddress);\n@@ -935,17 +937,0 @@\n-\n-RegisterOrConstant MacroAssembler::delayed_value_impl(intptr_t* delayed_value_addr,\n-                                                      Register tmp,\n-                                                      int offset) {\n-  intptr_t value = *delayed_value_addr;\n-  if (value != 0)\n-    return RegisterOrConstant(value + offset);\n-\n-  \/\/ load indirectly to solve generation ordering problem\n-  ldr(tmp, ExternalAddress((address) delayed_value_addr));\n-\n-  if (offset != 0)\n-    add(tmp, tmp, offset);\n-\n-  return RegisterOrConstant(tmp);\n-}\n-\n@@ -1831,1 +1816,1 @@\n-    const unsigned mask = size_in_bytes - 1;\n+    const uint64_t mask = size_in_bytes - 1;\n@@ -2895,1 +2880,1 @@\n-  const int sz = prev_ldst->size_in_bytes();\n+  const size_t sz = prev_ldst->size_in_bytes();\n@@ -4493,1 +4478,1 @@\n-void MacroAssembler::has_negatives(Register ary1, Register len, Register result) {\n+address MacroAssembler::has_negatives(Register ary1, Register len, Register result) {\n@@ -4530,1 +4515,1 @@\n-    RuntimeAddress has_neg =  RuntimeAddress(StubRoutines::aarch64::has_negatives());\n+    RuntimeAddress has_neg = RuntimeAddress(StubRoutines::aarch64::has_negatives());\n@@ -4532,1 +4517,6 @@\n-    trampoline_call(has_neg);\n+    address tpc1 = trampoline_call(has_neg);\n+    if (tpc1 == NULL) {\n+      DEBUG_ONLY(reset_labels(STUB_LONG, SET_RESULT, DONE));\n+      postcond(pc() == badAddress);\n+      return NULL;\n+    }\n@@ -4536,2 +4526,1 @@\n-    RuntimeAddress has_neg_long =  RuntimeAddress(\n-            StubRoutines::aarch64::has_negatives_long());\n+    RuntimeAddress has_neg_long = RuntimeAddress(StubRoutines::aarch64::has_negatives_long());\n@@ -4539,1 +4528,6 @@\n-    trampoline_call(has_neg_long);\n+    address tpc2 = trampoline_call(has_neg_long);\n+    if (tpc2 == NULL) {\n+      DEBUG_ONLY(reset_labels(SET_RESULT, DONE));\n+      postcond(pc() == badAddress);\n+      return NULL;\n+    }\n@@ -4546,0 +4540,2 @@\n+  postcond(pc() != badAddress);\n+  return pc();\n@@ -4548,3 +4544,3 @@\n-void MacroAssembler::arrays_equals(Register a1, Register a2, Register tmp3,\n-                                   Register tmp4, Register tmp5, Register result,\n-                                   Register cnt1, int elem_size) {\n+address MacroAssembler::arrays_equals(Register a1, Register a2, Register tmp3,\n+                                      Register tmp4, Register tmp5, Register result,\n+                                      Register cnt1, int elem_size) {\n@@ -4654,1 +4650,1 @@\n-    Label NEXT_DWORD, SHORT, TAIL, TAIL2, STUB, EARLY_OUT,\n+    Label NEXT_DWORD, SHORT, TAIL, TAIL2, STUB,\n@@ -4713,1 +4709,6 @@\n-    trampoline_call(stub);\n+    address tpc = trampoline_call(stub);\n+    if (tpc == NULL) {\n+      DEBUG_ONLY(reset_labels(SHORT, LAST_CHECK, CSET_EQ, SAME, DONE));\n+      postcond(pc() == badAddress);\n+      return NULL;\n+    }\n@@ -4716,1 +4717,0 @@\n-    bind(EARLY_OUT);\n@@ -4743,0 +4743,2 @@\n+  postcond(pc() != badAddress);\n+  return pc();\n@@ -4850,1 +4852,1 @@\n-void MacroAssembler::zero_words(Register ptr, Register cnt)\n+address MacroAssembler::zero_words(Register ptr, Register cnt)\n@@ -4860,1 +4862,1 @@\n-    RuntimeAddress zero_blocks =  RuntimeAddress(StubRoutines::aarch64::zero_blocks());\n+    RuntimeAddress zero_blocks = RuntimeAddress(StubRoutines::aarch64::zero_blocks());\n@@ -4863,1 +4865,6 @@\n-      trampoline_call(zero_blocks);\n+      address tpc = trampoline_call(zero_blocks);\n+      if (tpc == NULL) {\n+        DEBUG_ONLY(reset_labels(around));\n+        postcond(pc() == badAddress);\n+        return NULL;\n+      }\n@@ -4884,0 +4891,2 @@\n+  postcond(pc() != badAddress);\n+  return pc();\n@@ -4896,1 +4905,1 @@\n-    for (; i < (int)cnt; i += 2)\n+    for (; i < (int)cnt; i += 2) {\n@@ -4898,0 +4907,1 @@\n+    }\n@@ -4901,1 +4911,1 @@\n-    for (; i < remainder; i += 2)\n+    for (; i < remainder; i += 2) {\n@@ -4903,1 +4913,1 @@\n-\n+    }\n@@ -4913,1 +4923,1 @@\n-    for (i = 1; i < unroll; i++)\n+    for (i = 1; i < unroll; i++) {\n@@ -4915,0 +4925,1 @@\n+    }\n@@ -5130,3 +5141,3 @@\n-void MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,\n-                                        FloatRegister vtmp1, FloatRegister vtmp2, FloatRegister vtmp3,\n-                                        Register tmp4) {\n+address MacroAssembler::byte_array_inflate(Register src, Register dst, Register len,\n+                                           FloatRegister vtmp1, FloatRegister vtmp2,\n+                                           FloatRegister vtmp3, Register tmp4) {\n@@ -5169,1 +5180,1 @@\n-      RuntimeAddress stub =  RuntimeAddress(StubRoutines::aarch64::large_byte_array_inflate());\n+      RuntimeAddress stub = RuntimeAddress(StubRoutines::aarch64::large_byte_array_inflate());\n@@ -5171,1 +5182,6 @@\n-      trampoline_call(stub);\n+      address tpc = trampoline_call(stub);\n+      if (tpc == NULL) {\n+        DEBUG_ONLY(reset_labels(big, done));\n+        postcond(pc() == badAddress);\n+        return NULL;\n+      }\n@@ -5225,0 +5241,2 @@\n+  postcond(pc() != badAddress);\n+  return pc();\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":62,"deletions":44,"binary":false,"changes":106,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"prims\/methodHandles.hpp\"\n@@ -1093,14 +1094,0 @@\n-\n-\/\/ Check GCLocker::needs_gc and enter the runtime if it's true.  This\n-\/\/ keeps a new JNI critical region from starting until a GC has been\n-\/\/ forced.  Save down any oops in registers and describe them in an\n-\/\/ OopMap.\n-static void check_needs_gc_for_critical_native(MacroAssembler* masm,\n-                                               int stack_slots,\n-                                               int total_c_args,\n-                                               int total_in_args,\n-                                               int arg_save_area,\n-                                               OopMapSet* oop_maps,\n-                                               VMRegPair* in_regs,\n-                                               BasicType* in_sig_bt) { Unimplemented(); }\n-\n@@ -1272,4 +1259,3 @@\n-\/\/ passing them to the callee and perform checks before and after the\n-\/\/ native call to ensure that they GCLocker\n-\/\/ lock_critical\/unlock_critical semantics are followed.  Some other\n-\/\/ parts of JNI setup are skipped like the tear down of the JNI handle\n+\/\/ passing them to the callee. Critical native functions leave the state _in_Java,\n+\/\/ since they block out GC.\n+\/\/ Some other parts of JNI setup are skipped like the tear down of the JNI handle\n@@ -1279,12 +1265,0 @@\n-\/\/ They are roughly structured like this:\n-\/\/    if (GCLocker::needs_gc())\n-\/\/      SharedRuntime::block_for_jni_critical();\n-\/\/    tranistion to thread_in_native\n-\/\/    unpack arrray arguments and call native entry point\n-\/\/    check for safepoint in progress\n-\/\/    check if any thread suspend flags are set\n-\/\/      call into JVM and possible unlock the JNI critical\n-\/\/      if a GC was suppressed while in the critical native.\n-\/\/    transition back to thread_in_Java\n-\/\/    return to caller\n-\/\/\n@@ -1541,1 +1515,1 @@\n-    __ bang_stack_with_offset(StackOverflow::stack_shadow_zone_size());\n+    __ bang_stack_with_offset(checked_cast<int>(StackOverflow::stack_shadow_zone_size()));\n@@ -1562,5 +1536,0 @@\n-  if (is_critical_native) {\n-    check_needs_gc_for_critical_native(masm, stack_slots, total_c_args, total_in_args,\n-                                       oop_handle_offset, oop_maps, in_regs, in_sig_bt);\n-  }\n-\n@@ -1839,5 +1808,5 @@\n-  }\n-  \/\/ Now set thread in native\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n+    \/\/ Now set thread in native\n+    __ mov(rscratch1, _thread_in_native);\n+    __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n+    __ stlrw(rscratch1, rscratch2);\n+  }\n@@ -1872,0 +1841,15 @@\n+  Label safepoint_in_progress, safepoint_in_progress_done;\n+  Label after_transition;\n+\n+  \/\/ If this is a critical native, check for a safepoint or suspend request after the call.\n+  \/\/ If a safepoint is needed, transition to native, then to native_trans to handle\n+  \/\/ safepoints like the native methods that are not critical natives.\n+  if (is_critical_native) {\n+    Label needs_safepoint;\n+    __ safepoint_poll(needs_safepoint, false \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n+    __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n+    __ cbnzw(rscratch1, needs_safepoint);\n+    __ b(after_transition);\n+    __ bind(needs_safepoint);\n+  }\n+\n@@ -1892,1 +1876,0 @@\n-  Label safepoint_in_progress, safepoint_in_progress_done;\n@@ -1910,1 +1893,0 @@\n-  Label after_transition;\n@@ -2115,5 +2097,1 @@\n-    if (!is_critical_native) {\n-      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-    } else {\n-      __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)));\n-    }\n+    __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n@@ -2125,6 +2103,0 @@\n-    if (is_critical_native) {\n-      \/\/ The call above performed the transition to thread_in_Java so\n-      \/\/ skip the transition logic above.\n-      __ b(after_transition);\n-    }\n-\n@@ -2179,5 +2151,0 @@\n-  if (is_critical_native) {\n-    nm->set_lazy_critical_native(true);\n-  }\n-\n-\n@@ -2495,1 +2462,1 @@\n-  __ mov(rscratch1, (address)0xDEADDEAD);        \/\/ Make a recognizable pattern\n+  __ mov(rscratch1, (uint64_t)0xDEADDEAD);        \/\/ Make a recognizable pattern\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":27,"deletions":60,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -614,0 +614,10 @@\n+  \/\/ Generate indices for iota vector.\n+  address generate_iota_indices(const char *stub_name) {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", stub_name);\n+    address start = __ pc();\n+    __ emit_data64(0x0706050403020100, relocInfo::none);\n+    __ emit_data64(0x0F0E0D0C0B0A0908, relocInfo::none);\n+    return start;\n+  }\n+\n@@ -1298,1 +1308,1 @@\n-  void verify_oop_array (size_t size, Register a, Register count, Register temp) {\n+  void verify_oop_array (int size, Register a, Register count, Register temp) {\n@@ -1305,1 +1315,1 @@\n-    if (size == (size_t)wordSize) {\n+    if (size == wordSize) {\n@@ -1336,1 +1346,1 @@\n-  address generate_disjoint_copy(size_t size, bool aligned, bool is_oop, address *entry,\n+  address generate_disjoint_copy(int size, bool aligned, bool is_oop, address *entry,\n@@ -1402,1 +1412,1 @@\n-  address generate_conjoint_copy(size_t size, bool aligned, bool is_oop, address nooverlap_target,\n+  address generate_conjoint_copy(int size, bool aligned, bool is_oop, address nooverlap_target,\n@@ -1653,1 +1663,1 @@\n-    const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);\n+    const int size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);\n@@ -1671,1 +1681,1 @@\n-    const size_t size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);\n+    const int size = UseCompressedOops ? sizeof (jint) : sizeof (jlong);\n@@ -3302,0 +3312,219 @@\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Inputs:\n+  \/\/   c_rarg0   - byte[]  source+offset\n+  \/\/   c_rarg1   - byte[]   SHA.state\n+  \/\/   c_rarg2   - int     digest_length\n+  \/\/   c_rarg3   - int     offset\n+  \/\/   c_rarg4   - int     limit\n+  \/\/\n+  address generate_sha3_implCompress(bool multi_block, const char *name) {\n+    static const uint64_t round_consts[24] = {\n+      0x0000000000000001L, 0x0000000000008082L, 0x800000000000808AL,\n+      0x8000000080008000L, 0x000000000000808BL, 0x0000000080000001L,\n+      0x8000000080008081L, 0x8000000000008009L, 0x000000000000008AL,\n+      0x0000000000000088L, 0x0000000080008009L, 0x000000008000000AL,\n+      0x000000008000808BL, 0x800000000000008BL, 0x8000000000008089L,\n+      0x8000000000008003L, 0x8000000000008002L, 0x8000000000000080L,\n+      0x000000000000800AL, 0x800000008000000AL, 0x8000000080008081L,\n+      0x8000000000008080L, 0x0000000080000001L, 0x8000000080008008L\n+    };\n+\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", name);\n+    address start = __ pc();\n+\n+    Register buf           = c_rarg0;\n+    Register state         = c_rarg1;\n+    Register digest_length = c_rarg2;\n+    Register ofs           = c_rarg3;\n+    Register limit         = c_rarg4;\n+\n+    Label sha3_loop, rounds24_loop;\n+    Label sha3_512, sha3_384_or_224, sha3_256;\n+\n+    __ stpd(v8, v9, __ pre(sp, -64));\n+    __ stpd(v10, v11, Address(sp, 16));\n+    __ stpd(v12, v13, Address(sp, 32));\n+    __ stpd(v14, v15, Address(sp, 48));\n+\n+    \/\/ load state\n+    __ add(rscratch1, state, 32);\n+    __ ld1(v0, v1, v2,  v3,  __ T1D, state);\n+    __ ld1(v4, v5, v6,  v7,  __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v8, v9, v10, v11, __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v12, v13, v14, v15, __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v16, v17, v18, v19, __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v20, v21, v22, v23, __ T1D, __ post(rscratch1, 32));\n+    __ ld1(v24, __ T1D, rscratch1);\n+\n+    __ BIND(sha3_loop);\n+\n+    \/\/ 24 keccak rounds\n+    __ movw(rscratch2, 24);\n+\n+    \/\/ load round_constants base\n+    __ lea(rscratch1, ExternalAddress((address) round_consts));\n+\n+    \/\/ load input\n+    __ ld1(v25, v26, v27, v28, __ T8B, __ post(buf, 32));\n+    __ ld1(v29, v30, v31, __ T8B, __ post(buf, 24));\n+    __ eor(v0, __ T8B, v0, v25);\n+    __ eor(v1, __ T8B, v1, v26);\n+    __ eor(v2, __ T8B, v2, v27);\n+    __ eor(v3, __ T8B, v3, v28);\n+    __ eor(v4, __ T8B, v4, v29);\n+    __ eor(v5, __ T8B, v5, v30);\n+    __ eor(v6, __ T8B, v6, v31);\n+\n+    \/\/ digest_length == 64, SHA3-512\n+    __ tbnz(digest_length, 6, sha3_512);\n+\n+    __ ld1(v25, v26, v27, v28, __ T8B, __ post(buf, 32));\n+    __ ld1(v29, v30, __ T8B, __ post(buf, 16));\n+    __ eor(v7, __ T8B, v7, v25);\n+    __ eor(v8, __ T8B, v8, v26);\n+    __ eor(v9, __ T8B, v9, v27);\n+    __ eor(v10, __ T8B, v10, v28);\n+    __ eor(v11, __ T8B, v11, v29);\n+    __ eor(v12, __ T8B, v12, v30);\n+\n+    \/\/ digest_length == 28, SHA3-224;  digest_length == 48, SHA3-384\n+    __ tbnz(digest_length, 4, sha3_384_or_224);\n+\n+    \/\/ SHA3-256\n+    __ ld1(v25, v26, v27, v28, __ T8B, __ post(buf, 32));\n+    __ eor(v13, __ T8B, v13, v25);\n+    __ eor(v14, __ T8B, v14, v26);\n+    __ eor(v15, __ T8B, v15, v27);\n+    __ eor(v16, __ T8B, v16, v28);\n+    __ b(rounds24_loop);\n+\n+    __ BIND(sha3_384_or_224);\n+    __ tbz(digest_length, 2, rounds24_loop); \/\/ bit 2 cleared? SHA-384\n+\n+    \/\/ SHA3-224\n+    __ ld1(v25, v26, v27, v28, __ T8B, __ post(buf, 32));\n+    __ ld1(v29, __ T8B, __ post(buf, 8));\n+    __ eor(v13, __ T8B, v13, v25);\n+    __ eor(v14, __ T8B, v14, v26);\n+    __ eor(v15, __ T8B, v15, v27);\n+    __ eor(v16, __ T8B, v16, v28);\n+    __ eor(v17, __ T8B, v17, v29);\n+    __ b(rounds24_loop);\n+\n+    __ BIND(sha3_512);\n+    __ ld1(v25, v26, __ T8B, __ post(buf, 16));\n+    __ eor(v7, __ T8B, v7, v25);\n+    __ eor(v8, __ T8B, v8, v26);\n+\n+    __ BIND(rounds24_loop);\n+    __ subw(rscratch2, rscratch2, 1);\n+\n+    __ eor3(v29, __ T16B, v4, v9, v14);\n+    __ eor3(v26, __ T16B, v1, v6, v11);\n+    __ eor3(v28, __ T16B, v3, v8, v13);\n+    __ eor3(v25, __ T16B, v0, v5, v10);\n+    __ eor3(v27, __ T16B, v2, v7, v12);\n+    __ eor3(v29, __ T16B, v29, v19, v24);\n+    __ eor3(v26, __ T16B, v26, v16, v21);\n+    __ eor3(v28, __ T16B, v28, v18, v23);\n+    __ eor3(v25, __ T16B, v25, v15, v20);\n+    __ eor3(v27, __ T16B, v27, v17, v22);\n+\n+    __ rax1(v30, __ T2D, v29, v26);\n+    __ rax1(v26, __ T2D, v26, v28);\n+    __ rax1(v28, __ T2D, v28, v25);\n+    __ rax1(v25, __ T2D, v25, v27);\n+    __ rax1(v27, __ T2D, v27, v29);\n+\n+    __ eor(v0, __ T16B, v0, v30);\n+    __ xar(v29, __ T2D, v1,  v25, (64 - 1));\n+    __ xar(v1,  __ T2D, v6,  v25, (64 - 44));\n+    __ xar(v6,  __ T2D, v9,  v28, (64 - 20));\n+    __ xar(v9,  __ T2D, v22, v26, (64 - 61));\n+    __ xar(v22, __ T2D, v14, v28, (64 - 39));\n+    __ xar(v14, __ T2D, v20, v30, (64 - 18));\n+    __ xar(v31, __ T2D, v2,  v26, (64 - 62));\n+    __ xar(v2,  __ T2D, v12, v26, (64 - 43));\n+    __ xar(v12, __ T2D, v13, v27, (64 - 25));\n+    __ xar(v13, __ T2D, v19, v28, (64 - 8));\n+    __ xar(v19, __ T2D, v23, v27, (64 - 56));\n+    __ xar(v23, __ T2D, v15, v30, (64 - 41));\n+    __ xar(v15, __ T2D, v4,  v28, (64 - 27));\n+    __ xar(v28, __ T2D, v24, v28, (64 - 14));\n+    __ xar(v24, __ T2D, v21, v25, (64 - 2));\n+    __ xar(v8,  __ T2D, v8,  v27, (64 - 55));\n+    __ xar(v4,  __ T2D, v16, v25, (64 - 45));\n+    __ xar(v16, __ T2D, v5,  v30, (64 - 36));\n+    __ xar(v5,  __ T2D, v3,  v27, (64 - 28));\n+    __ xar(v27, __ T2D, v18, v27, (64 - 21));\n+    __ xar(v3,  __ T2D, v17, v26, (64 - 15));\n+    __ xar(v25, __ T2D, v11, v25, (64 - 10));\n+    __ xar(v26, __ T2D, v7,  v26, (64 - 6));\n+    __ xar(v30, __ T2D, v10, v30, (64 - 3));\n+\n+    __ bcax(v20, __ T16B, v31, v22, v8);\n+    __ bcax(v21, __ T16B, v8,  v23, v22);\n+    __ bcax(v22, __ T16B, v22, v24, v23);\n+    __ bcax(v23, __ T16B, v23, v31, v24);\n+    __ bcax(v24, __ T16B, v24, v8,  v31);\n+\n+    __ ld1r(v31, __ T2D, __ post(rscratch1, 8));\n+\n+    __ bcax(v17, __ T16B, v25, v19, v3);\n+    __ bcax(v18, __ T16B, v3,  v15, v19);\n+    __ bcax(v19, __ T16B, v19, v16, v15);\n+    __ bcax(v15, __ T16B, v15, v25, v16);\n+    __ bcax(v16, __ T16B, v16, v3,  v25);\n+\n+    __ bcax(v10, __ T16B, v29, v12, v26);\n+    __ bcax(v11, __ T16B, v26, v13, v12);\n+    __ bcax(v12, __ T16B, v12, v14, v13);\n+    __ bcax(v13, __ T16B, v13, v29, v14);\n+    __ bcax(v14, __ T16B, v14, v26, v29);\n+\n+    __ bcax(v7, __ T16B, v30, v9,  v4);\n+    __ bcax(v8, __ T16B, v4,  v5,  v9);\n+    __ bcax(v9, __ T16B, v9,  v6,  v5);\n+    __ bcax(v5, __ T16B, v5,  v30, v6);\n+    __ bcax(v6, __ T16B, v6,  v4,  v30);\n+\n+    __ bcax(v3, __ T16B, v27, v0,  v28);\n+    __ bcax(v4, __ T16B, v28, v1,  v0);\n+    __ bcax(v0, __ T16B, v0,  v2,  v1);\n+    __ bcax(v1, __ T16B, v1,  v27, v2);\n+    __ bcax(v2, __ T16B, v2,  v28, v27);\n+\n+    __ eor(v0, __ T16B, v0, v31);\n+\n+    __ cbnzw(rscratch2, rounds24_loop);\n+\n+    if (multi_block) {\n+      \/\/ block_size =  200 - 2 * digest_length, ofs += block_size\n+      __ add(ofs, ofs, 200);\n+      __ sub(ofs, ofs, digest_length, Assembler::LSL, 1);\n+\n+      __ cmp(ofs, limit);\n+      __ br(Assembler::LE, sha3_loop);\n+      __ mov(c_rarg0, ofs); \/\/ return ofs\n+    }\n+\n+    __ st1(v0, v1, v2,  v3,  __ T1D, __ post(state, 32));\n+    __ st1(v4, v5, v6,  v7,  __ T1D, __ post(state, 32));\n+    __ st1(v8, v9, v10, v11, __ T1D, __ post(state, 32));\n+    __ st1(v12, v13, v14, v15, __ T1D, __ post(state, 32));\n+    __ st1(v16, v17, v18, v19, __ T1D, __ post(state, 32));\n+    __ st1(v20, v21, v22, v23, __ T1D, __ post(state, 32));\n+    __ st1(v24, __ T1D, state);\n+\n+    __ ldpd(v14, v15, Address(sp, 48));\n+    __ ldpd(v12, v13, Address(sp, 32));\n+    __ ldpd(v10, v11, Address(sp, 16));\n+    __ ldpd(v8, v9, __ post(sp, 64));\n+\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -3742,0 +3971,232 @@\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Input:\n+  \/\/   c_rarg0   - newArr address\n+  \/\/   c_rarg1   - oldArr address\n+  \/\/   c_rarg2   - newIdx\n+  \/\/   c_rarg3   - shiftCount\n+  \/\/   c_rarg4   - numIter\n+  \/\/\n+  address generate_bigIntegerRightShift() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerRightShiftWorker\");\n+    address start = __ pc();\n+\n+    Label ShiftSIMDLoop, ShiftTwoLoop, ShiftThree, ShiftTwo, ShiftOne, Exit;\n+\n+    Register newArr        = c_rarg0;\n+    Register oldArr        = c_rarg1;\n+    Register newIdx        = c_rarg2;\n+    Register shiftCount    = c_rarg3;\n+    Register numIter       = c_rarg4;\n+    Register idx           = numIter;\n+\n+    Register newArrCur     = rscratch1;\n+    Register shiftRevCount = rscratch2;\n+    Register oldArrCur     = r13;\n+    Register oldArrNext    = r14;\n+\n+    FloatRegister oldElem0        = v0;\n+    FloatRegister oldElem1        = v1;\n+    FloatRegister newElem         = v2;\n+    FloatRegister shiftVCount     = v3;\n+    FloatRegister shiftVRevCount  = v4;\n+\n+    __ cbz(idx, Exit);\n+\n+    __ add(newArr, newArr, newIdx, Assembler::LSL, 2);\n+\n+    \/\/ left shift count\n+    __ movw(shiftRevCount, 32);\n+    __ subw(shiftRevCount, shiftRevCount, shiftCount);\n+\n+    \/\/ numIter too small to allow a 4-words SIMD loop, rolling back\n+    __ cmp(numIter, (u1)4);\n+    __ br(Assembler::LT, ShiftThree);\n+\n+    __ dup(shiftVCount,    __ T4S, shiftCount);\n+    __ dup(shiftVRevCount, __ T4S, shiftRevCount);\n+    __ negr(shiftVCount,   __ T4S, shiftVCount);\n+\n+    __ BIND(ShiftSIMDLoop);\n+\n+    \/\/ Calculate the load addresses\n+    __ sub(idx, idx, 4);\n+    __ add(oldArrNext, oldArr, idx, Assembler::LSL, 2);\n+    __ add(newArrCur,  newArr, idx, Assembler::LSL, 2);\n+    __ add(oldArrCur,  oldArrNext, 4);\n+\n+    \/\/ Load 4 words and process\n+    __ ld1(oldElem0,  __ T4S,  Address(oldArrCur));\n+    __ ld1(oldElem1,  __ T4S,  Address(oldArrNext));\n+    __ ushl(oldElem0, __ T4S,  oldElem0, shiftVCount);\n+    __ ushl(oldElem1, __ T4S,  oldElem1, shiftVRevCount);\n+    __ orr(newElem,   __ T16B, oldElem0, oldElem1);\n+    __ st1(newElem,   __ T4S,  Address(newArrCur));\n+\n+    __ cmp(idx, (u1)4);\n+    __ br(Assembler::LT, ShiftTwoLoop);\n+    __ b(ShiftSIMDLoop);\n+\n+    __ BIND(ShiftTwoLoop);\n+    __ cbz(idx, Exit);\n+    __ cmp(idx, (u1)1);\n+    __ br(Assembler::EQ, ShiftOne);\n+\n+    \/\/ Calculate the load addresses\n+    __ sub(idx, idx, 2);\n+    __ add(oldArrNext, oldArr, idx, Assembler::LSL, 2);\n+    __ add(newArrCur,  newArr, idx, Assembler::LSL, 2);\n+    __ add(oldArrCur,  oldArrNext, 4);\n+\n+    \/\/ Load 2 words and process\n+    __ ld1(oldElem0,  __ T2S, Address(oldArrCur));\n+    __ ld1(oldElem1,  __ T2S, Address(oldArrNext));\n+    __ ushl(oldElem0, __ T2S, oldElem0, shiftVCount);\n+    __ ushl(oldElem1, __ T2S, oldElem1, shiftVRevCount);\n+    __ orr(newElem,   __ T8B, oldElem0, oldElem1);\n+    __ st1(newElem,   __ T2S, Address(newArrCur));\n+    __ b(ShiftTwoLoop);\n+\n+    __ BIND(ShiftThree);\n+    __ tbz(idx, 1, ShiftOne);\n+    __ tbz(idx, 0, ShiftTwo);\n+    __ ldrw(r10,  Address(oldArr, 12));\n+    __ ldrw(r11,  Address(oldArr, 8));\n+    __ lsrvw(r10, r10, shiftCount);\n+    __ lslvw(r11, r11, shiftRevCount);\n+    __ orrw(r12,  r10, r11);\n+    __ strw(r12,  Address(newArr, 8));\n+\n+    __ BIND(ShiftTwo);\n+    __ ldrw(r10,  Address(oldArr, 8));\n+    __ ldrw(r11,  Address(oldArr, 4));\n+    __ lsrvw(r10, r10, shiftCount);\n+    __ lslvw(r11, r11, shiftRevCount);\n+    __ orrw(r12,  r10, r11);\n+    __ strw(r12,  Address(newArr, 4));\n+\n+    __ BIND(ShiftOne);\n+    __ ldrw(r10,  Address(oldArr, 4));\n+    __ ldrw(r11,  Address(oldArr));\n+    __ lsrvw(r10, r10, shiftCount);\n+    __ lslvw(r11, r11, shiftRevCount);\n+    __ orrw(r12,  r10, r11);\n+    __ strw(r12,  Address(newArr));\n+\n+    __ BIND(Exit);\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n+  \/\/ Arguments:\n+  \/\/\n+  \/\/ Input:\n+  \/\/   c_rarg0   - newArr address\n+  \/\/   c_rarg1   - oldArr address\n+  \/\/   c_rarg2   - newIdx\n+  \/\/   c_rarg3   - shiftCount\n+  \/\/   c_rarg4   - numIter\n+  \/\/\n+  address generate_bigIntegerLeftShift() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this,  \"StubRoutines\", \"bigIntegerLeftShiftWorker\");\n+    address start = __ pc();\n+\n+    Label ShiftSIMDLoop, ShiftTwoLoop, ShiftThree, ShiftTwo, ShiftOne, Exit;\n+\n+    Register newArr        = c_rarg0;\n+    Register oldArr        = c_rarg1;\n+    Register newIdx        = c_rarg2;\n+    Register shiftCount    = c_rarg3;\n+    Register numIter       = c_rarg4;\n+\n+    Register shiftRevCount = rscratch1;\n+    Register oldArrNext    = rscratch2;\n+\n+    FloatRegister oldElem0        = v0;\n+    FloatRegister oldElem1        = v1;\n+    FloatRegister newElem         = v2;\n+    FloatRegister shiftVCount     = v3;\n+    FloatRegister shiftVRevCount  = v4;\n+\n+    __ cbz(numIter, Exit);\n+\n+    __ add(oldArrNext, oldArr, 4);\n+    __ add(newArr, newArr, newIdx, Assembler::LSL, 2);\n+\n+    \/\/ right shift count\n+    __ movw(shiftRevCount, 32);\n+    __ subw(shiftRevCount, shiftRevCount, shiftCount);\n+\n+    \/\/ numIter too small to allow a 4-words SIMD loop, rolling back\n+    __ cmp(numIter, (u1)4);\n+    __ br(Assembler::LT, ShiftThree);\n+\n+    __ dup(shiftVCount,     __ T4S, shiftCount);\n+    __ dup(shiftVRevCount,  __ T4S, shiftRevCount);\n+    __ negr(shiftVRevCount, __ T4S, shiftVRevCount);\n+\n+    __ BIND(ShiftSIMDLoop);\n+\n+    \/\/ load 4 words and process\n+    __ ld1(oldElem0,  __ T4S,  __ post(oldArr, 16));\n+    __ ld1(oldElem1,  __ T4S,  __ post(oldArrNext, 16));\n+    __ ushl(oldElem0, __ T4S,  oldElem0, shiftVCount);\n+    __ ushl(oldElem1, __ T4S,  oldElem1, shiftVRevCount);\n+    __ orr(newElem,   __ T16B, oldElem0, oldElem1);\n+    __ st1(newElem,   __ T4S,  __ post(newArr, 16));\n+    __ sub(numIter,   numIter, 4);\n+\n+    __ cmp(numIter, (u1)4);\n+    __ br(Assembler::LT, ShiftTwoLoop);\n+    __ b(ShiftSIMDLoop);\n+\n+    __ BIND(ShiftTwoLoop);\n+    __ cbz(numIter, Exit);\n+    __ cmp(numIter, (u1)1);\n+    __ br(Assembler::EQ, ShiftOne);\n+\n+    \/\/ load 2 words and process\n+    __ ld1(oldElem0,  __ T2S,  __ post(oldArr, 8));\n+    __ ld1(oldElem1,  __ T2S,  __ post(oldArrNext, 8));\n+    __ ushl(oldElem0, __ T2S,  oldElem0, shiftVCount);\n+    __ ushl(oldElem1, __ T2S,  oldElem1, shiftVRevCount);\n+    __ orr(newElem,   __ T8B,  oldElem0, oldElem1);\n+    __ st1(newElem,   __ T2S,  __ post(newArr, 8));\n+    __ sub(numIter,   numIter, 2);\n+    __ b(ShiftTwoLoop);\n+\n+    __ BIND(ShiftThree);\n+    __ ldrw(r10,  __ post(oldArr, 4));\n+    __ ldrw(r11,  __ post(oldArrNext, 4));\n+    __ lslvw(r10, r10, shiftCount);\n+    __ lsrvw(r11, r11, shiftRevCount);\n+    __ orrw(r12,  r10, r11);\n+    __ strw(r12,  __ post(newArr, 4));\n+    __ tbz(numIter, 1, Exit);\n+    __ tbz(numIter, 0, ShiftOne);\n+\n+    __ BIND(ShiftTwo);\n+    __ ldrw(r10,  __ post(oldArr, 4));\n+    __ ldrw(r11,  __ post(oldArrNext, 4));\n+    __ lslvw(r10, r10, shiftCount);\n+    __ lsrvw(r11, r11, shiftRevCount);\n+    __ orrw(r12,  r10, r11);\n+    __ strw(r12,  __ post(newArr, 4));\n+\n+    __ BIND(ShiftOne);\n+    __ ldrw(r10,  Address(oldArr));\n+    __ ldrw(r11,  Address(oldArrNext));\n+    __ lslvw(r10, r10, shiftCount);\n+    __ lsrvw(r11, r11, shiftRevCount);\n+    __ orrw(r12,  r10, r11);\n+    __ strw(r12,  Address(newArr));\n+\n+    __ BIND(Exit);\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -5961,0 +6422,2 @@\n+    StubRoutines::aarch64::_vector_iota_indices    = generate_iota_indices(\"iota_indices\");\n+\n@@ -5996,0 +6459,5 @@\n+    if (UseSIMDForBigIntegerShiftIntrinsics) {\n+      StubRoutines::_bigIntegerRightShiftWorker = generate_bigIntegerRightShift();\n+      StubRoutines::_bigIntegerLeftShiftWorker  = generate_bigIntegerLeftShift();\n+    }\n+\n@@ -6039,0 +6507,4 @@\n+    if (UseSHA3Intrinsics) {\n+      StubRoutines::_sha3_implCompress     = generate_sha3_implCompress(false,   \"sha3_implCompress\");\n+      StubRoutines::_sha3_implCompressMB   = generate_sha3_implCompress(true,    \"sha3_implCompressMB\");\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":478,"deletions":6,"binary":false,"changes":484,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n-  static int set_and_get_current_sve_vector_lenght(int len);\n+  static int set_and_get_current_sve_vector_length(int len);\n@@ -107,0 +107,1 @@\n+    CPU_SHA3         = (1<<17),\n","filename":"src\/hotspot\/cpu\/aarch64\/vm_version_aarch64.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"runtime\/stubRoutines.inline.hpp\"\n@@ -2239,1 +2240,1 @@\n-char* os::pd_attempt_reserve_memory_at(char* requested_addr, size_t bytes, int file_desc) {\n+char* os::pd_attempt_map_memory_to_file_at(char* requested_addr, size_t bytes, int file_desc) {\n","filename":"src\/hotspot\/os\/aix\/os_aix.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1826,3 +1826,2 @@\n-  int flags;\n-\n-  flags = MAP_PRIVATE | MAP_NORESERVE | MAP_ANONYMOUS;\n+  \/\/ MAP_FIXED is intentionally left out, to leave existing mappings intact.\n+  int flags = MAP_PRIVATE | MAP_NORESERVE | MAP_ANONYMOUS;\n@@ -1931,1 +1930,1 @@\n-char* os::pd_attempt_reserve_memory_at(char* requested_addr, size_t bytes, int file_desc) {\n+char* os::pd_attempt_map_memory_to_file_at(char* requested_addr, size_t bytes, int file_desc) {\n","filename":"src\/hotspot\/os\/bsd\/os_bsd.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"runtime\/stubRoutines.inline.hpp\"\n","filename":"src\/hotspot\/os\/linux\/gc\/z\/zPhysicalMemoryBacking_linux.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -107,1 +107,0 @@\n-# include <gnu\/libc-version.h>\n@@ -140,0 +139,11 @@\n+#ifdef MUSL_LIBC\n+\/\/ dlvsym is not a part of POSIX\n+\/\/ and musl libc doesn't implement it.\n+static void *dlvsym(void *handle,\n+                    const char *symbol,\n+                    const char *version) {\n+   \/\/ load the latest version of symbol\n+   return dlsym(handle, symbol);\n+}\n+#endif\n+\n@@ -159,1 +169,1 @@\n-const char * os::Linux::_glibc_version = NULL;\n+const char * os::Linux::_libc_version = NULL;\n@@ -513,0 +523,6 @@\n+#ifdef MUSL_LIBC\n+  \/\/ confstr() from musl libc returns EINVAL for\n+  \/\/ _CS_GNU_LIBC_VERSION and _CS_GNU_LIBPTHREAD_VERSION\n+  os::Linux::set_libc_version(\"musl - unknown\");\n+  os::Linux::set_libpthread_version(\"musl - unknown\");\n+#else\n@@ -517,1 +533,1 @@\n-  os::Linux::set_glibc_version(str);\n+  os::Linux::set_libc_version(str);\n@@ -524,0 +540,1 @@\n+#endif\n@@ -2214,1 +2231,1 @@\n-  st->print(\"%s \", os::Linux::glibc_version());\n+  st->print(\"%s \", os::Linux::libc_version());\n@@ -3073,0 +3090,2 @@\n+      set_numa_node_to_cpus_v2(CAST_TO_FN_PTR(numa_node_to_cpus_v2_func_t,\n+                                              libnuma_v2_dlsym(handle, \"numa_node_to_cpus\")));\n@@ -3201,1 +3220,11 @@\n-              cpu_to_node()->at_put(j * BitsPerCLong + k, closest_node);\n+              int cpu_index = j * BitsPerCLong + k;\n+\n+#ifndef PRODUCT\n+              if (UseDebuggerErgo1 && cpu_index >= (int)cpu_num) {\n+                \/\/ Some debuggers limit the processor count without\n+                \/\/ intercepting the NUMA APIs. Just fake the values.\n+                cpu_index = 0;\n+              }\n+#endif\n+\n+              cpu_to_node()->at_put(cpu_index, closest_node);\n@@ -3211,0 +3240,20 @@\n+int os::Linux::numa_node_to_cpus(int node, unsigned long *buffer, int bufferlen) {\n+  \/\/ use the latest version of numa_node_to_cpus if available\n+  if (_numa_node_to_cpus_v2 != NULL) {\n+\n+    \/\/ libnuma bitmask struct\n+    struct bitmask {\n+      unsigned long size; \/* number of bits in the map *\/\n+      unsigned long *maskp;\n+    };\n+\n+    struct bitmask mask;\n+    mask.maskp = (unsigned long *)buffer;\n+    mask.size = bufferlen * 8;\n+    return _numa_node_to_cpus_v2(node, &mask);\n+  } else if (_numa_node_to_cpus != NULL) {\n+    return _numa_node_to_cpus(node, buffer, bufferlen);\n+  }\n+  return -1;\n+}\n+\n@@ -3222,0 +3271,1 @@\n+os::Linux::numa_node_to_cpus_v2_func_t os::Linux::_numa_node_to_cpus_v2;\n@@ -4167,1 +4217,1 @@\n-char* os::pd_attempt_reserve_memory_at(char* requested_addr, size_t bytes, int file_desc) {\n+char* os::pd_attempt_map_memory_to_file_at(char* requested_addr, size_t bytes, int file_desc) {\n@@ -4324,0 +4374,34 @@\n+\/\/ Some linux distributions (notably: Alpine Linux) include the\n+\/\/ grsecurity in the kernel. Of particular interest from a JVM perspective\n+\/\/ is PaX (https:\/\/pax.grsecurity.net\/), which adds some security features\n+\/\/ related to page attributes. Specifically, the MPROTECT PaX functionality\n+\/\/ (https:\/\/pax.grsecurity.net\/docs\/mprotect.txt) prevents dynamic\n+\/\/ code generation by disallowing a (previously) writable page to be\n+\/\/ marked as executable. This is, of course, exactly what HotSpot does\n+\/\/ for both JIT compiled method, as well as for stubs, adapters, etc.\n+\/\/\n+\/\/ Instead of crashing \"lazily\" when trying to make a page executable,\n+\/\/ this code probes for the presence of PaX and reports the failure\n+\/\/ eagerly.\n+static void check_pax(void) {\n+  \/\/ Zero doesn't generate code dynamically, so no need to perform the PaX check\n+#ifndef ZERO\n+  size_t size = os::Linux::page_size();\n+\n+  void* p = ::mmap(NULL, size, PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n+  if (p == MAP_FAILED) {\n+    log_debug(os)(\"os_linux.cpp: check_pax: mmap failed (%s)\" , os::strerror(errno));\n+    vm_exit_out_of_memory(size, OOM_MMAP_ERROR, \"failed to allocate memory for PaX check.\");\n+  }\n+\n+  int res = ::mprotect(p, size, PROT_WRITE|PROT_EXEC);\n+  if (res == -1) {\n+    log_debug(os)(\"os_linux.cpp: check_pax: mprotect failed (%s)\" , os::strerror(errno));\n+    vm_exit_during_initialization(\n+      \"Failed to mark memory page as executable - check if grsecurity\/PaX is enabled\");\n+  }\n+\n+  ::munmap(p, size);\n+#endif\n+}\n+\n@@ -4357,0 +4441,2 @@\n+  check_pax();\n+\n@@ -4496,1 +4582,1 @@\n-               Linux::glibc_version(), Linux::libpthread_version());\n+               Linux::libc_version(), Linux::libpthread_version());\n@@ -4552,0 +4638,6 @@\n+  if (DumpPerfMapAtExit && FLAG_IS_DEFAULT(UseCodeCacheFlushing)) {\n+    \/\/ Disable code cache flushing to ensure the map file written at\n+    \/\/ exit contains all nmethods generated during execution.\n+    FLAG_SET_DEFAULT(UseCodeCacheFlushing, false);\n+  }\n+\n@@ -4689,1 +4781,10 @@\n-  assert(id >= 0 && id < _processor_count, \"Invalid processor id\");\n+\n+#ifndef PRODUCT\n+  if (UseDebuggerErgo1 && id >= _processor_count) {\n+    \/\/ Some debuggers limit the processor count without limiting\n+    \/\/ the returned processor ids. Fake the processor id.\n+    return 0;\n+  }\n+#endif\n+\n+  assert(id >= 0 && id < _processor_count, \"Invalid processor id [%d]\", id);\n","filename":"src\/hotspot\/os\/linux\/os_linux.cpp","additions":109,"deletions":8,"binary":false,"changes":117,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"runtime\/atomic.hpp\"\n@@ -74,4 +75,0 @@\n-\/\/ This boolean allows users to forward their own non-matching signals\n-\/\/ to JVM_handle_bsd_signal\/JVM_handle_linux_signal, harmlessly.\n-static bool signal_handlers_are_installed = false;\n-\n@@ -264,0 +261,2 @@\n+static const char* get_signal_name(int sig, char* out, size_t outlen);\n+\n@@ -316,0 +315,2 @@\n+  ShouldNotReachHere();\n+  return 0; \/\/ Satisfy compiler\n@@ -411,0 +412,57 @@\n+\/\/\/\/\/ Synchronous (non-deferrable) error signals (ILL, SEGV, FPE, BUS, TRAP):\n+\n+\/\/ These signals are special because they cannot be deferred and, if they\n+\/\/ happen while delivery is blocked for the receiving thread, will cause UB\n+\/\/ (in practice typically resulting in sudden process deaths or hangs, see\n+\/\/ JDK-8252533). So we must take care never to block them when we cannot be\n+\/\/ absolutely sure they won't happen. In practice, this is always.\n+\/\/\n+\/\/ Relevant Posix quote:\n+\/\/ \"The behavior of a process is undefined after it ignores a SIGFPE, SIGILL,\n+\/\/  SIGSEGV, or SIGBUS signal that was not generated by kill(), sigqueue(), or\n+\/\/  raise().\"\n+\/\/\n+\/\/ We also include SIGTRAP in that list of never-to-block-signals. While not\n+\/\/ mentioned by the Posix documentation, in our (SAPs) experience blocking it\n+\/\/ causes similar problems. Beside, during normal operation - outside of error\n+\/\/ handling - SIGTRAP may be used for implicit NULL checking, so it makes sense\n+\/\/ to never block it.\n+\/\/\n+\/\/ We deal with those signals in two ways:\n+\/\/ - we just never explicitly block them, which includes not accidentally blocking\n+\/\/   them via sa_mask when establishing signal handlers.\n+\/\/ - as an additional safety measure, at the entrance of a signal handler, we\n+\/\/   unblock them explicitly.\n+\n+static void add_error_signals_to_set(sigset_t* set) {\n+  sigaddset(set, SIGILL);\n+  sigaddset(set, SIGBUS);\n+  sigaddset(set, SIGFPE);\n+  sigaddset(set, SIGSEGV);\n+  sigaddset(set, SIGTRAP);\n+}\n+\n+static void remove_error_signals_from_set(sigset_t* set) {\n+  sigdelset(set, SIGILL);\n+  sigdelset(set, SIGBUS);\n+  sigdelset(set, SIGFPE);\n+  sigdelset(set, SIGSEGV);\n+  sigdelset(set, SIGTRAP);\n+}\n+\n+\/\/ Unblock all signals whose delivery cannot be deferred and which, if they happen\n+\/\/  while delivery is blocked, would cause crashes or hangs (JDK-8252533).\n+void PosixSignals::unblock_error_signals() {\n+  sigset_t set;\n+  sigemptyset(&set);\n+  add_error_signals_to_set(&set);\n+  ::pthread_sigmask(SIG_UNBLOCK, &set, NULL);\n+}\n+\n+class ErrnoPreserver: public StackObj {\n+  const int _saved;\n+public:\n+  ErrnoPreserver() : _saved(errno) {}\n+  ~ErrnoPreserver() { errno = _saved; }\n+};\n+\n@@ -412,1 +470,1 @@\n-\/\/ signal handling (except suspend\/resume)\n+\/\/ JVM_handle_(linux|aix|bsd)_signal()\n@@ -414,1 +472,5 @@\n-\/\/ This routine may be used by user applications as a \"hook\" to catch signals.\n+\/\/ This routine is the shared part of the central hotspot signal handler. It can\n+\/\/ also be called by a user application, if a user application prefers to do\n+\/\/ signal handling itself - in that case it needs to pass signals the VM\n+\/\/ internally uses on to the VM first.\n+\/\/\n@@ -418,2 +480,2 @@\n-\/\/ routine will never retun false (zero), but instead will execute a VM panic\n-\/\/ routine kill the process.\n+\/\/ routine will never return false (zero), but instead will execute a VM panic\n+\/\/ routine to kill the process.\n@@ -441,3 +503,1 @@\n-extern \"C\" JNIEXPORT int JVM_handle_bsd_signal(int signo, siginfo_t* siginfo,\n-                                               void* ucontext,\n-                                               int abort_if_unrecognized);\n+#define JVM_HANDLE_XXX_SIGNAL JVM_handle_bsd_signal\n@@ -445,3 +505,3 @@\n-extern \"C\" JNIEXPORT int JVM_handle_aix_signal(int signo, siginfo_t* siginfo,\n-                                               void* ucontext,\n-                                               int abort_if_unrecognized);\n+#define JVM_HANDLE_XXX_SIGNAL JVM_handle_aix_signal\n+#elif defined(LINUX)\n+#define JVM_HANDLE_XXX_SIGNAL JVM_handle_linux_signal\n@@ -449,3 +509,1 @@\n-extern \"C\" JNIEXPORT int JVM_handle_linux_signal(int signo, siginfo_t* siginfo,\n-                                               void* ucontext,\n-                                               int abort_if_unrecognized);\n+#error who are you?\n@@ -454,1 +512,5 @@\n-#if defined(AIX)\n+extern \"C\" JNIEXPORT\n+int JVM_HANDLE_XXX_SIGNAL(int sig, siginfo_t* info,\n+                          void* ucVoid, int abort_if_unrecognized)\n+{\n+  assert(info != NULL && ucVoid != NULL, \"sanity\");\n@@ -456,11 +518,6 @@\n-\/\/ Set thread signal mask (for some reason on AIX sigthreadmask() seems\n-\/\/ to be the thing to call; documentation is not terribly clear about whether\n-\/\/ pthread_sigmask also works, and if it does, whether it does the same.\n-bool set_thread_signal_mask(int how, const sigset_t* set, sigset_t* oset) {\n-  const int rc = ::pthread_sigmask(how, set, oset);\n-  \/\/ return value semantics differ slightly for error case:\n-  \/\/ pthread_sigmask returns error number, sigthreadmask -1 and sets global errno\n-  \/\/ (so, pthread_sigmask is more theadsafe for error handling)\n-  \/\/ But success is always 0.\n-  return rc == 0 ? true : false;\n-}\n+  \/\/ Note: it's not uncommon that JNI code uses signal\/sigset to install,\n+  \/\/ then restore certain signal handler (e.g. to temporarily block SIGPIPE,\n+  \/\/ or have a SIGILL handler when detecting CPU type). When that happens,\n+  \/\/ this handler might be invoked with junk info\/ucVoid. To avoid unnecessary\n+  \/\/ crash when libjsig is not preloaded, try handle signals that do not require\n+  \/\/ siginfo\/ucontext first.\n@@ -468,12 +525,3 @@\n-\/\/ Function to unblock all signals which are, according\n-\/\/ to POSIX, typical program error signals. If they happen while being blocked,\n-\/\/ they typically will bring down the process immediately.\n-bool unblock_program_error_signals() {\n-  sigset_t set;\n-  sigemptyset(&set);\n-  sigaddset(&set, SIGILL);\n-  sigaddset(&set, SIGBUS);\n-  sigaddset(&set, SIGFPE);\n-  sigaddset(&set, SIGSEGV);\n-  return set_thread_signal_mask(SIG_UNBLOCK, &set, NULL);\n-}\n+  \/\/ Preserve errno value over signal handler.\n+  \/\/  (note: RAII ok here, even with JFR thread crash protection, see below).\n+  ErrnoPreserver ep;\n@@ -481,1 +529,2 @@\n-#endif\n+  \/\/ Unblock all synchronous error signals (see JDK-8252533)\n+  PosixSignals::unblock_error_signals();\n@@ -483,3 +532,2 @@\n-\/\/ Renamed from 'signalHandler' to avoid collision with other shared libs.\n-static void javaSignalHandler(int sig, siginfo_t* info, void* uc) {\n-  assert(info != NULL && uc != NULL, \"it must be old kernel\");\n+  ucontext_t* const uc = (ucontext_t*) ucVoid;\n+  Thread* const t = Thread::current_or_null_safe();\n@@ -487,6 +535,12 @@\n-\/\/ TODO: reconcile the differences between Linux\/BSD vs AIX here!\n-#if defined(AIX)\n-  \/\/ Never leave program error signals blocked;\n-  \/\/ on all our platforms they would bring down the process immediately when\n-  \/\/ getting raised while being blocked.\n-  unblock_program_error_signals();\n+  \/\/ Handle JFR thread crash protection.\n+  \/\/  Note: this may cause us to longjmp away. Do not use any code before this\n+  \/\/  point which really needs any form of epilogue code running, eg RAII objects.\n+  os::ThreadCrashProtection::check_crash_protection(sig, t);\n+\n+  bool signal_was_handled = false;\n+\n+  \/\/ Handle assertion poison page accesses.\n+#ifdef CAN_SHOW_REGISTERS_ON_ASSERT\n+  if ((sig == SIGSEGV || sig == SIGBUS) && info != NULL && info->si_addr == g_assert_poison) {\n+    signal_was_handled = handle_assert_poison_fault(ucVoid, info->si_addr);\n+  }\n@@ -495,5 +549,44 @@\n-  int orig_errno = errno;  \/\/ Preserve errno value over signal handler.\n-#if defined(BSD)\n-  JVM_handle_bsd_signal(sig, info, uc, true);\n-#elif defined(AIX)\n-  JVM_handle_aix_signal(sig, info, uc, true);\n+  \/\/ Ignore SIGPIPE and SIGXFSZ (4229104, 6499219).\n+  if (sig == SIGPIPE || sig == SIGXFSZ) {\n+    PosixSignals::chained_handler(sig, info, ucVoid);\n+    signal_was_handled = true; \/\/ unconditionally.\n+  }\n+\n+  \/\/ Call platform dependent signal handler.\n+  if (!signal_was_handled) {\n+    JavaThread* const jt = (t != NULL && t->is_Java_thread()) ? (JavaThread*) t : NULL;\n+    signal_was_handled = PosixSignals::pd_hotspot_signal_handler(sig, info, uc, jt);\n+  }\n+\n+  \/\/ From here on, if the signal had not been handled, it is a fatal error.\n+\n+  \/\/ Give the chained signal handler - should it exist - a shot.\n+  if (!signal_was_handled) {\n+    signal_was_handled = PosixSignals::chained_handler(sig, info, ucVoid);\n+  }\n+\n+  \/\/ Invoke fatal error handling.\n+  if (!signal_was_handled && abort_if_unrecognized) {\n+    \/\/ Extract pc from context for the error handler to display.\n+    address pc = NULL;\n+    if (uc != NULL) {\n+      \/\/ prepare fault pc address for error reporting.\n+      if (S390_ONLY(sig == SIGILL || sig == SIGFPE) NOT_S390(false)) {\n+        pc = (address)info->si_addr;\n+      } else {\n+        pc = PosixSignals::ucontext_get_pc(uc);\n+      }\n+    }\n+#if defined(ZERO) && !defined(PRODUCT)\n+    char buf[64];\n+    VMError::report_and_die(t, sig, pc, info, ucVoid,\n+          \"\\n#\"\n+          \"\\n#    \/--------------------\\\\\"\n+          \"\\n#    |      %-7s       |\"\n+          \"\\n#    \\\\---\\\\ \/--------------\/\"\n+          \"\\n#        \/\"\n+          \"\\n#    [-]        |\\\\_\/|    \"\n+          \"\\n#    (+)=C      |o o|__  \"\n+          \"\\n#    | |        =-*-=__\\\\ \"\n+          \"\\n#    OOO        c_c_(___)\",\n+          get_signal_name(sig, buf, sizeof(buf)));\n@@ -501,1 +594,1 @@\n-  JVM_handle_linux_signal(sig, info, uc, true);\n+    VMError::report_and_die(t, sig, pc, info, ucVoid);\n@@ -503,1 +596,11 @@\n-  errno = orig_errno;\n+    \/\/ VMError should not return.\n+    ShouldNotReachHere();\n+  }\n+  return signal_was_handled;\n+}\n+\n+\/\/ Entry point for the hotspot signal handler.\n+static void javaSignalHandler(int sig, siginfo_t* info, void* ucVoid) {\n+  \/\/ Do not add any code here!\n+  \/\/ Only add code to either JVM_HANDLE_XXX_SIGNAL or PosixSignals::pd_hotspot_signal_handler.\n+  (void)JVM_HANDLE_XXX_SIGNAL(sig, info, ucVoid, true);\n@@ -507,0 +610,3 @@\n+\n+  PosixSignals::unblock_error_signals();\n+\n@@ -705,17 +811,1 @@\n-\n-#if defined(AIX)\n-  \/\/ Do not block out synchronous signals in the signal handler.\n-  \/\/ Blocking synchronous signals only makes sense if you can really\n-  \/\/ be sure that those signals won't happen during signal handling,\n-  \/\/ when the blocking applies. Normal signal handlers are lean and\n-  \/\/ do not cause signals. But our signal handlers tend to be \"risky\"\n-  \/\/ - secondary SIGSEGV, SIGILL, SIGBUS' may and do happen.\n-  \/\/ On AIX, PASE there was a case where a SIGSEGV happened, followed\n-  \/\/ by a SIGILL, which was blocked due to the signal mask. The process\n-  \/\/ just hung forever. Better to crash from a secondary signal than to hang.\n-  sigdelset(&(sigAct.sa_mask), SIGSEGV);\n-  sigdelset(&(sigAct.sa_mask), SIGBUS);\n-  sigdelset(&(sigAct.sa_mask), SIGILL);\n-  sigdelset(&(sigAct.sa_mask), SIGFPE);\n-  sigdelset(&(sigAct.sa_mask), SIGTRAP);\n-#endif\n+  remove_error_signals_from_set(&(sigAct.sa_mask));\n@@ -766,3 +856,1 @@\n-#if defined(PPC64)\n-  do_signal_check(SIGTRAP);\n-#endif\n+  PPC64_ONLY(do_signal_check(SIGTRAP);)\n@@ -935,1 +1023,0 @@\n-\/\/ Returned string is a constant. For unknown signals \"UNKNOWN\" is returned.\n@@ -1075,1 +1162,1 @@\n-void set_signal_handler(int sig, bool set_installed) {\n+void set_signal_handler(int sig) {\n@@ -1086,1 +1173,1 @@\n-    if (AllowUserSignalHandlers || !set_installed) {\n+    if (AllowUserSignalHandlers) {\n@@ -1102,7 +1189,3 @@\n-  sigAct.sa_handler = SIG_DFL;\n-  if (!set_installed) {\n-    sigAct.sa_flags = SA_SIGINFO|SA_RESTART;\n-  } else {\n-    sigAct.sa_sigaction = javaSignalHandler;\n-    sigAct.sa_flags = SA_SIGINFO|SA_RESTART;\n-  }\n+  remove_error_signals_from_set(&(sigAct.sa_mask));\n+  sigAct.sa_sigaction = javaSignalHandler;\n+  sigAct.sa_flags = SA_SIGINFO|SA_RESTART;\n@@ -1135,7 +1218,0 @@\n-\/\/ install signal handlers for signals that HotSpot needs to\n-\/\/ handle in order to support Java-level exception handling.\n-\n-bool PosixSignals::are_signal_handlers_installed() {\n-  return signal_handlers_are_installed;\n-}\n-\n@@ -1145,30 +1221,26 @@\n-  if (!signal_handlers_are_installed) {\n-    signal_handlers_are_installed = true;\n-\n-    \/\/ signal-chaining\n-    typedef void (*signal_setting_t)();\n-    signal_setting_t begin_signal_setting = NULL;\n-    signal_setting_t end_signal_setting = NULL;\n-    begin_signal_setting = CAST_TO_FN_PTR(signal_setting_t,\n-                                          dlsym(RTLD_DEFAULT, \"JVM_begin_signal_setting\"));\n-    if (begin_signal_setting != NULL) {\n-      end_signal_setting = CAST_TO_FN_PTR(signal_setting_t,\n-                                          dlsym(RTLD_DEFAULT, \"JVM_end_signal_setting\"));\n-      get_signal_action = CAST_TO_FN_PTR(get_signal_t,\n-                                         dlsym(RTLD_DEFAULT, \"JVM_get_signal_action\"));\n-      libjsig_is_loaded = true;\n-      assert(UseSignalChaining, \"should enable signal-chaining\");\n-    }\n-    if (libjsig_is_loaded) {\n-      \/\/ Tell libjsig jvm is setting signal handlers\n-      (*begin_signal_setting)();\n-    }\n-    set_signal_handler(SIGSEGV, true);\n-    set_signal_handler(SIGPIPE, true);\n-    set_signal_handler(SIGBUS, true);\n-    set_signal_handler(SIGILL, true);\n-    set_signal_handler(SIGFPE, true);\n-#if defined(PPC64) || defined(AIX)\n-    set_signal_handler(SIGTRAP, true);\n-#endif\n-    set_signal_handler(SIGXFSZ, true);\n+  \/\/ signal-chaining\n+  typedef void (*signal_setting_t)();\n+  signal_setting_t begin_signal_setting = NULL;\n+  signal_setting_t end_signal_setting = NULL;\n+  begin_signal_setting = CAST_TO_FN_PTR(signal_setting_t,\n+                                        dlsym(RTLD_DEFAULT, \"JVM_begin_signal_setting\"));\n+  if (begin_signal_setting != NULL) {\n+    end_signal_setting = CAST_TO_FN_PTR(signal_setting_t,\n+                                        dlsym(RTLD_DEFAULT, \"JVM_end_signal_setting\"));\n+    get_signal_action = CAST_TO_FN_PTR(get_signal_t,\n+                                       dlsym(RTLD_DEFAULT, \"JVM_get_signal_action\"));\n+    libjsig_is_loaded = true;\n+    assert(UseSignalChaining, \"should enable signal-chaining\");\n+  }\n+  if (libjsig_is_loaded) {\n+    \/\/ Tell libjsig jvm is setting signal handlers\n+    (*begin_signal_setting)();\n+  }\n+\n+  set_signal_handler(SIGSEGV);\n+  set_signal_handler(SIGPIPE);\n+  set_signal_handler(SIGBUS);\n+  set_signal_handler(SIGILL);\n+  set_signal_handler(SIGFPE);\n+  PPC64_ONLY(set_signal_handler(SIGTRAP);)\n+  set_signal_handler(SIGXFSZ);\n@@ -1178,19 +1250,19 @@\n-    \/\/ In Mac OS X 10.4, CrashReporter will write a crash log for all 'fatal' signals, including\n-    \/\/ signals caught and handled by the JVM. To work around this, we reset the mach task\n-    \/\/ signal handler that's placed on our process by CrashReporter. This disables\n-    \/\/ CrashReporter-based reporting.\n-    \/\/\n-    \/\/ This work-around is not necessary for 10.5+, as CrashReporter no longer intercedes\n-    \/\/ on caught fatal signals.\n-    \/\/\n-    \/\/ Additionally, gdb installs both standard BSD signal handlers, and mach exception\n-    \/\/ handlers. By replacing the existing task exception handler, we disable gdb's mach\n-    \/\/ exception handling, while leaving the standard BSD signal handlers functional.\n-    kern_return_t kr;\n-    kr = task_set_exception_ports(mach_task_self(),\n-                                  EXC_MASK_BAD_ACCESS | EXC_MASK_BAD_INSTRUCTION | EXC_MASK_ARITHMETIC,\n-                                  MACH_PORT_NULL,\n-                                  EXCEPTION_STATE_IDENTITY,\n-                                  MACHINE_THREAD_STATE);\n-\n-    assert(kr == KERN_SUCCESS, \"could not set mach task signal handler\");\n+  \/\/ In Mac OS X 10.4, CrashReporter will write a crash log for all 'fatal' signals, including\n+  \/\/ signals caught and handled by the JVM. To work around this, we reset the mach task\n+  \/\/ signal handler that's placed on our process by CrashReporter. This disables\n+  \/\/ CrashReporter-based reporting.\n+  \/\/\n+  \/\/ This work-around is not necessary for 10.5+, as CrashReporter no longer intercedes\n+  \/\/ on caught fatal signals.\n+  \/\/\n+  \/\/ Additionally, gdb installs both standard BSD signal handlers, and mach exception\n+  \/\/ handlers. By replacing the existing task exception handler, we disable gdb's mach\n+  \/\/ exception handling, while leaving the standard BSD signal handlers functional.\n+  kern_return_t kr;\n+  kr = task_set_exception_ports(mach_task_self(),\n+                                EXC_MASK_BAD_ACCESS | EXC_MASK_BAD_INSTRUCTION | EXC_MASK_ARITHMETIC,\n+                                MACH_PORT_NULL,\n+                                EXCEPTION_STATE_IDENTITY,\n+                                MACHINE_THREAD_STATE);\n+\n+  assert(kr == KERN_SUCCESS, \"could not set mach task signal handler\");\n@@ -1199,0 +1271,9 @@\n+  if (libjsig_is_loaded) {\n+    \/\/ Tell libjsig jvm finishes setting signal handlers\n+    (*end_signal_setting)();\n+  }\n+\n+  \/\/ We don't activate signal checker if libjsig is in place, we trust ourselves\n+  \/\/ and if UserSignalHandler is installed all bets are off.\n+  \/\/ Log that signal checking is off only if -verbose:jni is specified.\n+  if (CheckJNICalls) {\n@@ -1200,2 +1281,2 @@\n-      \/\/ Tell libjsig jvm finishes setting signal handlers\n-      (*end_signal_setting)();\n+      log_debug(jni, resolve)(\"Info: libjsig is activated, all active signal checking is disabled\");\n+      check_signals = false;\n@@ -1203,13 +1284,3 @@\n-\n-    \/\/ We don't activate signal checker if libjsig is in place, we trust ourselves\n-    \/\/ and if UserSignalHandler is installed all bets are off.\n-    \/\/ Log that signal checking is off only if -verbose:jni is specified.\n-    if (CheckJNICalls) {\n-      if (libjsig_is_loaded) {\n-        log_debug(jni, resolve)(\"Info: libjsig is activated, all active signal checking is disabled\");\n-        check_signals = false;\n-      }\n-      if (AllowUserSignalHandlers) {\n-        log_debug(jni, resolve)(\"Info: AllowUserSignalHandlers is activated, all active signal checking is disabled\");\n-        check_signals = false;\n-      }\n+    if (AllowUserSignalHandlers) {\n+      log_debug(jni, resolve)(\"Info: AllowUserSignalHandlers is activated, all active signal checking is disabled\");\n+      check_signals = false;\n@@ -1306,4 +1377,0 @@\n-int PosixSignals::unblock_thread_signal_mask(const sigset_t *set) {\n-  return pthread_sigmask(SIG_UNBLOCK, set, NULL);\n-}\n-\n@@ -1357,3 +1424,1 @@\n-  #if defined(PPC64) || defined(AIX)\n-    sigaddset(&unblocked_sigs, SIGTRAP);\n-  #endif\n+  PPC64_ONLY(sigaddset(&unblocked_sigs, SIGTRAP);)\n@@ -1473,0 +1538,1 @@\n+\n@@ -1477,0 +1543,2 @@\n+  PosixSignals::unblock_error_signals();\n+\n@@ -1570,0 +1638,1 @@\n+  remove_error_signals_from_set(&(act.sa_mask));\n","filename":"src\/hotspot\/os\/posix\/signals_posix.cpp","additions":235,"deletions":166,"binary":false,"changes":401,"status":"modified"},{"patch":"@@ -2147,0 +2147,2 @@\n+  ShouldNotReachHere();\n+  return 0; \/\/ Satisfy compiler\n@@ -2357,1 +2359,1 @@\n-  \/\/ If UseOsErrorReporting, this will return here and save the error file\n+  \/\/ If UseOSErrorReporting, this will return here and save the error file\n@@ -3140,1 +3142,1 @@\n-char* os::reserve_memory_aligned(size_t size, size_t alignment, int file_desc) {\n+static char* map_or_reserve_memory_aligned(size_t size, size_t alignment, int file_desc) {\n@@ -3151,1 +3153,3 @@\n-    char* extra_base = os::reserve_memory_with_fd(extra_size, file_desc);\n+    char* extra_base = file_desc != -1 ?\n+      os::map_memory_to_file(extra_size, file_desc) :\n+      os::reserve_memory(extra_size);\n@@ -3164,1 +3168,3 @@\n-    aligned_base = os::attempt_reserve_memory_at(aligned_base, size, file_desc);\n+    aligned_base = file_desc != -1 ?\n+      os::attempt_map_memory_to_file_at(aligned_base, size, file_desc) :\n+      os::attempt_reserve_memory_at(aligned_base, size);\n@@ -3171,0 +3177,8 @@\n+char* os::reserve_memory_aligned(size_t size, size_t alignment) {\n+  return map_or_reserve_memory_aligned(size, alignment, -1 \/* file_desc *\/);\n+}\n+\n+char* os::map_memory_to_file_aligned(size_t size, size_t alignment, int fd) {\n+  return map_or_reserve_memory_aligned(size, alignment, fd);\n+}\n+\n@@ -3208,1 +3222,1 @@\n-char* os::pd_attempt_reserve_memory_at(char* requested_addr, size_t bytes, int file_desc) {\n+char* os::pd_attempt_map_memory_to_file_at(char* requested_addr, size_t bytes, int file_desc) {\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -179,27 +179,8 @@\n-bool os::Bsd::get_frame_at_stack_banging_point(JavaThread* thread, ucontext_t* uc, frame* fr) {\n-  address pc = (address) os::Bsd::ucontext_get_pc(uc);\n-  if (Interpreter::contains(pc)) {\n-    \/\/ interpreter performs stack banging after the fixed frame header has\n-    \/\/ been generated while the compilers perform it before. To maintain\n-    \/\/ semantic consistency between interpreted and compiled frames, the\n-    \/\/ method returns the Java sender of the current frame.\n-    *fr = os::fetch_frame_from_context(uc);\n-    if (!fr->is_first_java_frame()) {\n-      assert(fr->safe_for_sender(thread), \"Safety check\");\n-      *fr = fr->java_sender();\n-    }\n-  } else {\n-    \/\/ more complex code with compiled code\n-    assert(!Interpreter::contains(pc), \"Interpreted methods should have been handled above\");\n-    CodeBlob* cb = CodeCache::find_blob(pc);\n-    if (cb == NULL || !cb->is_nmethod() || cb->is_frame_complete_at(pc)) {\n-      \/\/ Not sure where the pc points to, fallback to default\n-      \/\/ stack overflow handling\n-      return false;\n-    } else {\n-      \/\/ In compiled code, the stack banging is performed before LR\n-      \/\/ has been saved in the frame.  LR is live, and SP and FP\n-      \/\/ belong to the caller.\n-      intptr_t* fp = os::Bsd::ucontext_get_fp(uc);\n-      intptr_t* sp = os::Bsd::ucontext_get_sp(uc);\n-      address pc = (address)(uc->context_lr\n+frame os::fetch_compiled_frame_from_context(const void* ucVoid) {\n+  const ucontext_t* uc = (const ucontext_t*)ucVoid;\n+  \/\/ In compiled code, the stack banging is performed before LR\n+  \/\/ has been saved in the frame.  LR is live, and SP and FP\n+  \/\/ belong to the caller.\n+  intptr_t* fp = os::Bsd::ucontext_get_fp(uc);\n+  intptr_t* sp = os::Bsd::ucontext_get_sp(uc);\n+  address pc = (address)(uc->context_lr\n@@ -207,10 +188,1 @@\n-      *fr = frame(sp, fp, pc);\n-      if (!fr->is_java_frame()) {\n-        assert(fr->safe_for_sender(thread), \"Safety check\");\n-        assert(!fr->is_first_frame(), \"Safety check\");\n-        *fr = fr->java_sender();\n-      }\n-    }\n-  }\n-  assert(fr->is_java_frame(), \"Safety check\");\n-  return true;\n+  return frame(sp, fp, pc);\n@@ -238,39 +210,2 @@\n-extern \"C\" JNIEXPORT int\n-JVM_handle_bsd_signal(int sig,\n-                        siginfo_t* info,\n-                        void* ucVoid,\n-                        int abort_if_unrecognized) {\n-  ucontext_t* uc = (ucontext_t*) ucVoid;\n-\n-  Thread* t = Thread::current_or_null_safe();\n-\n-  \/\/ Must do this before SignalHandlerMark, if crash protection installed we will longjmp away\n-  \/\/ (no destructors can be run)\n-  os::ThreadCrashProtection::check_crash_protection(sig, t);\n-\n-  SignalHandlerMark shm(t);\n-\n-  \/\/ Note: it's not uncommon that JNI code uses signal\/sigset to install\n-  \/\/ then restore certain signal handler (e.g. to temporarily block SIGPIPE,\n-  \/\/ or have a SIGILL handler when detecting CPU type). When that happens,\n-  \/\/ JVM_handle_bsd_signal() might be invoked with junk info\/ucVoid. To\n-  \/\/ avoid unnecessary crash when libjsig is not preloaded, try handle signals\n-  \/\/ that do not require siginfo\/ucontext first.\n-\n-  if (sig == SIGPIPE || sig == SIGXFSZ) {\n-    \/\/ allow chained handler to go first\n-    if (PosixSignals::chained_handler(sig, info, ucVoid)) {\n-      return true;\n-    } else {\n-      \/\/ Ignoring SIGPIPE\/SIGXFSZ - see bugs 4229104 or 6499219\n-      return true;\n-    }\n-  }\n-\n-#ifdef CAN_SHOW_REGISTERS_ON_ASSERT\n-  if ((sig == SIGSEGV || sig == SIGBUS) && info != NULL && info->si_addr == g_assert_poison) {\n-    if (handle_assert_poison_fault(ucVoid, info->si_addr)) {\n-      return 1;\n-    }\n-  }\n-#endif\n+bool PosixSignals::pd_hotspot_signal_handler(int sig, siginfo_t* info,\n+                                             ucontext_t* uc, JavaThread* thread) {\n@@ -278,12 +213,0 @@\n-  JavaThread* thread = NULL;\n-  VMThread* vmthread = NULL;\n-  if (PosixSignals::are_signal_handlers_installed()) {\n-    if (t != NULL ){\n-      if(t->is_Java_thread()) {\n-        thread = (JavaThread*)t;\n-      }\n-      else if(t->is_VM_thread()){\n-        vmthread = (VMThread *)t;\n-      }\n-    }\n-  }\n@@ -310,1 +233,1 @@\n-      return 1;\n+      return true;\n@@ -326,35 +249,2 @@\n-        StackOverflow* overflow_state = thread->stack_overflow_state();\n-        if (overflow_state->in_stack_yellow_reserved_zone(addr)) {\n-          if (thread->thread_state() == _thread_in_Java) {\n-            if (overflow_state->in_stack_reserved_zone(addr)) {\n-              frame fr;\n-              if (os::Bsd::get_frame_at_stack_banging_point(thread, uc, &fr)) {\n-                assert(fr.is_java_frame(), \"Must be a Java frame\");\n-                frame activation =\n-                  SharedRuntime::look_for_reserved_stack_annotated_method(thread, fr);\n-                if (activation.sp() != NULL) {\n-                  overflow_state->disable_stack_reserved_zone();\n-                  if (activation.is_interpreted_frame()) {\n-                    overflow_state->set_reserved_stack_activation((address)(\n-                      activation.fp() + frame::interpreter_frame_initial_sp_offset));\n-                  } else {\n-                    overflow_state->set_reserved_stack_activation((address)activation.unextended_sp());\n-                  }\n-                  return 1;\n-                }\n-              }\n-            }\n-            \/\/ Throw a stack overflow exception.  Guard pages will be reenabled\n-            \/\/ while unwinding the stack.\n-            overflow_state->disable_stack_yellow_reserved_zone();\n-            stub = SharedRuntime::continuation_for_implicit_exception(thread, pc, SharedRuntime::STACK_OVERFLOW);\n-          } else {\n-            \/\/ Thread was in the vm or native code.  Return and try to finish.\n-            overflow_state->disable_stack_yellow_reserved_zone();\n-            return 1;\n-          }\n-        } else if (overflow_state->in_stack_red_zone(addr)) {\n-          \/\/ Fatal red zone violation.  Disable the guard pages and fall through\n-          \/\/ to handle_unexpected_exception way down below.\n-          overflow_state->disable_stack_red_zone();\n-          tty->print_raw_cr(\"An irrecoverable stack overflow has occurred.\");\n+        if (os::Posix::handle_stack_overflow(thread, addr, pc, uc, &stub)) {\n+          return true; \/\/ continue\n@@ -417,8 +307,1 @@\n-PRAGMA_DIAG_PUSH\n-PRAGMA_DISABLE_GCC_WARNING(\"-Wformat-nonliteral\")\n-PRAGMA_DISABLE_GCC_WARNING(\"-Wuninitialized\")\n-        va_list detail_args;\n-        VMError::report_and_die(INTERNAL_ERROR, msg, detail_msg, detail_args, thread,\n-                                pc, info, ucVoid, NULL, 0, 0);\n-        va_end(detail_args);\n-PRAGMA_DIAG_POP\n+        return false;\n@@ -519,24 +402,1 @@\n-  \/\/ signal-chaining\n-  if (PosixSignals::chained_handler(sig, info, ucVoid)) {\n-     return true;\n-  }\n-\n-  if (!abort_if_unrecognized) {\n-    \/\/ caller wants another chance, so give it to him\n-    return false;\n-  }\n-\n-  if (pc == NULL && uc != NULL) {\n-    pc = os::Bsd::ucontext_get_pc(uc);\n-  }\n-\n-  \/\/ unmask current signal\n-  sigset_t newset;\n-  sigemptyset(&newset);\n-  sigaddset(&newset, sig);\n-  sigprocmask(SIG_UNBLOCK, &newset, NULL);\n-\n-  VMError::report_and_die(t, sig, pc, info, ucVoid);\n-\n-  ShouldNotReachHere();\n-  return true; \/\/ Mute compiler\n+  return false; \/\/ Mute compiler\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/os_bsd_aarch64.cpp","additions":16,"deletions":156,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-int VM_Version::set_and_get_current_sve_vector_lenght(int length) {\n+int VM_Version::set_and_get_current_sve_vector_length(int length) {\n","filename":"src\/hotspot\/os_cpu\/bsd_aarch64\/vm_version_bsd_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -56,0 +56,1 @@\n+#include \"prims\/methodHandles.hpp\"\n@@ -87,1 +88,1 @@\n-  { SystemDictionary::WK_KLASS_ENUM_NAME(klass), vmSymbols::VM_SYMBOL_ENUM_NAME(name##_name), vmSymbols::VM_SYMBOL_ENUM_NAME(signature), may_be_java },\n+  { SystemDictionary::WK_KLASS_ENUM_NAME(klass), VM_SYMBOL_ENUM_NAME(name##_name), VM_SYMBOL_ENUM_NAME(signature), may_be_java },\n@@ -115,2 +116,2 @@\n-  vmSymbols::SID sid = vmSymbols::find_sid(class_name);\n-  if (sid == vmSymbols::NO_SID) {\n+  vmSymbolID sid = vmSymbols::find_sid(class_name);\n+  if (sid == vmSymbolID::NO_SID) {\n@@ -125,1 +126,1 @@\n-  if (sid == vmSymbols::VM_SYMBOL_ENUM_NAME(klass)) {              \\\n+  if (sid == VM_SYMBOL_ENUM_NAME(klass)) {                         \\\n@@ -1249,0 +1250,2 @@\n+    set_signers(archived_mirror, NULL);\n+    set_source_file(archived_mirror, NULL);\n@@ -4541,0 +4544,24 @@\n+int vector_VectorPayload::_payload_offset;\n+\n+#define VECTORPAYLOAD_FIELDS_DO(macro) \\\n+  macro(_payload_offset, k, \"payload\", object_signature, false)\n+\n+void vector_VectorPayload::compute_offsets() {\n+  InstanceKlass* k = SystemDictionary::vector_VectorPayload_klass();\n+  VECTORPAYLOAD_FIELDS_DO(FIELD_COMPUTE_OFFSET);\n+}\n+\n+#if INCLUDE_CDS\n+void vector_VectorPayload::serialize_offsets(SerializeClosure* f) {\n+  VECTORPAYLOAD_FIELDS_DO(FIELD_SERIALIZE_OFFSET);\n+}\n+#endif\n+\n+void vector_VectorPayload::set_payload(oop o, oop val) {\n+  o->obj_field_put(_payload_offset, val);\n+}\n+\n+bool vector_VectorPayload::is_instance(oop obj) {\n+  return obj != NULL && is_subclass(obj->klass());\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":31,"deletions":4,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/escapeBarrier.hpp\"\n@@ -801,0 +802,4 @@\n+#if defined(ASSERT) && COMPILER2_OR_JVMCI\n+\/\/ Stress testing. Dedicated threads revert optimizations based on escape analysis concurrently to\n+\/\/ the running java application.  Configured with vm options DeoptimizeObjectsALot*.\n+class DeoptimizeObjectsALotThread : public JavaThread {\n@@ -802,1 +807,64 @@\n-JavaThread* CompileBroker::make_thread(jobject thread_handle, CompileQueue* queue, AbstractCompiler* comp, Thread* THREAD) {\n+  static void deopt_objs_alot_thread_entry(JavaThread* thread, TRAPS);\n+  void deoptimize_objects_alot_loop_single();\n+  void deoptimize_objects_alot_loop_all();\n+\n+public:\n+  DeoptimizeObjectsALotThread() : JavaThread(&deopt_objs_alot_thread_entry) { }\n+\n+  bool is_hidden_from_external_view() const      { return true; }\n+};\n+\n+\/\/ Entry for DeoptimizeObjectsALotThread. The threads are started in\n+\/\/ CompileBroker::init_compiler_sweeper_threads() iff DeoptimizeObjectsALot is enabled\n+void DeoptimizeObjectsALotThread::deopt_objs_alot_thread_entry(JavaThread* thread, TRAPS) {\n+    DeoptimizeObjectsALotThread* dt = ((DeoptimizeObjectsALotThread*) thread);\n+    bool enter_single_loop;\n+    {\n+      MonitorLocker ml(dt, EscapeBarrier_lock, Mutex::_no_safepoint_check_flag);\n+      static int single_thread_count = 0;\n+      enter_single_loop = single_thread_count++ < DeoptimizeObjectsALotThreadCountSingle;\n+    }\n+    if (enter_single_loop) {\n+      dt->deoptimize_objects_alot_loop_single();\n+    } else {\n+      dt->deoptimize_objects_alot_loop_all();\n+    }\n+  }\n+\n+\/\/ Execute EscapeBarriers in an endless loop to revert optimizations based on escape analysis. Each\n+\/\/ barrier targets a single thread which is selected round robin.\n+void DeoptimizeObjectsALotThread::deoptimize_objects_alot_loop_single() {\n+  HandleMark hm(this);\n+  while (true) {\n+    for (JavaThreadIteratorWithHandle jtiwh; JavaThread *deoptee_thread = jtiwh.next(); ) {\n+      { \/\/ Begin new scope for escape barrier\n+        HandleMarkCleaner hmc(this);\n+        ResourceMark rm(this);\n+        EscapeBarrier eb(true, this, deoptee_thread);\n+        eb.deoptimize_objects(100);\n+      }\n+      \/\/ Now sleep after the escape barriers destructor resumed deoptee_thread.\n+      sleep(DeoptimizeObjectsALotInterval);\n+    }\n+  }\n+}\n+\n+\/\/ Execute EscapeBarriers in an endless loop to revert optimizations based on escape analysis. Each\n+\/\/ barrier targets all java threads in the vm at once.\n+void DeoptimizeObjectsALotThread::deoptimize_objects_alot_loop_all() {\n+  HandleMark hm(this);\n+  while (true) {\n+    { \/\/ Begin new scope for escape barrier\n+      HandleMarkCleaner hmc(this);\n+      ResourceMark rm(this);\n+      EscapeBarrier eb(true, this);\n+      eb.deoptimize_objects_all_threads();\n+    }\n+    \/\/ Now sleep after the escape barriers destructor resumed the java threads.\n+    sleep(DeoptimizeObjectsALotInterval);\n+  }\n+}\n+#endif \/\/ defined(ASSERT) && COMPILER2_OR_JVMCI\n+\n+\n+JavaThread* CompileBroker::make_thread(ThreadType type, jobject thread_handle, CompileQueue* queue, AbstractCompiler* comp, Thread* THREAD) {\n@@ -806,7 +874,18 @@\n-    if (comp != NULL) {\n-      if (!InjectCompilerCreationFailure || comp->num_compiler_threads() == 0) {\n-        CompilerCounters* counters = new CompilerCounters();\n-        new_thread = new CompilerThread(queue, counters);\n-      }\n-    } else {\n-      new_thread = new CodeCacheSweeperThread();\n+    switch (type) {\n+      case compiler_t:\n+        assert(comp != NULL, \"Compiler instance missing.\");\n+        if (!InjectCompilerCreationFailure || comp->num_compiler_threads() == 0) {\n+          CompilerCounters* counters = new CompilerCounters();\n+          new_thread = new CompilerThread(queue, counters);\n+        }\n+        break;\n+      case sweeper_t:\n+        new_thread = new CodeCacheSweeperThread();\n+        break;\n+#if defined(ASSERT) && COMPILER2_OR_JVMCI\n+      case deoptimizer_t:\n+        new_thread = new DeoptimizeObjectsALotThread();\n+        break;\n+#endif \/\/ ASSERT\n+      default:\n+        ShouldNotReachHere();\n@@ -814,0 +893,1 @@\n+\n@@ -855,1 +935,1 @@\n-      if (comp != NULL) {\n+      if (type == compiler_t) {\n@@ -865,1 +945,1 @@\n-    if (UseDynamicNumberOfCompilerThreads && comp != NULL && comp->num_compiler_threads() > 0) {\n+    if (UseDynamicNumberOfCompilerThreads && type == compiler_t && comp->num_compiler_threads() > 0) {\n@@ -920,1 +1000,1 @@\n-      JavaThread *ct = make_thread(thread_handle, _c2_compile_queue, _compilers[1], THREAD);\n+      JavaThread *ct = make_thread(compiler_t, thread_handle, _c2_compile_queue, _compilers[1], THREAD);\n@@ -940,1 +1020,1 @@\n-      JavaThread *ct = make_thread(thread_handle, _c1_compile_queue, _compilers[0], THREAD);\n+      JavaThread *ct = make_thread(compiler_t, thread_handle, _c1_compile_queue, _compilers[0], THREAD);\n@@ -959,1 +1039,12 @@\n-    make_thread(thread_handle, NULL, NULL, THREAD);\n+    make_thread(sweeper_t, thread_handle, NULL, NULL, THREAD);\n+  }\n+\n+#if defined(ASSERT) && COMPILER2_OR_JVMCI\n+  if (DeoptimizeObjectsALot) {\n+    \/\/ Initialize and start the object deoptimizer threads\n+    const int total_count = DeoptimizeObjectsALotThreadCountSingle + DeoptimizeObjectsALotThreadCountAll;\n+    for (int count = 0; count < total_count; count++) {\n+      Handle thread_oop = create_thread_oop(\"Deoptimize objects a lot single mode\", CHECK);\n+      jobject thread_handle = JNIHandles::make_local(THREAD, thread_oop());\n+      make_thread(deoptimizer_t, thread_handle, NULL, NULL, THREAD);\n+    }\n@@ -961,0 +1052,1 @@\n+#endif \/\/ defined(ASSERT) && COMPILER2_OR_JVMCI\n@@ -1014,1 +1106,1 @@\n-      JavaThread *ct = make_thread(compiler2_object(i), _c2_compile_queue, _compilers[1], THREAD);\n+      JavaThread *ct = make_thread(compiler_t, compiler2_object(i), _c2_compile_queue, _compilers[1], THREAD);\n@@ -1034,1 +1126,1 @@\n-      JavaThread *ct = make_thread(compiler1_object(i), _c1_compile_queue, _compilers[0], THREAD);\n+      JavaThread *ct = make_thread(compiler_t, compiler1_object(i), _c1_compile_queue, _compilers[0], THREAD);\n","filename":"src\/hotspot\/share\/compiler\/compileBroker.cpp","additions":107,"deletions":15,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/pretouchTask.hpp\"\n@@ -237,48 +238,2 @@\n-class G1PretouchTask : public AbstractGangTask {\n-private:\n-  char* volatile _cur_addr;\n-  char* const _start_addr;\n-  char* const _end_addr;\n-  size_t _page_size;\n-public:\n-  G1PretouchTask(char* start_address, char* end_address, size_t page_size) :\n-    AbstractGangTask(\"G1 PreTouch\"),\n-    _cur_addr(start_address),\n-    _start_addr(start_address),\n-    _end_addr(end_address),\n-    _page_size(0) {\n-#ifdef LINUX\n-    _page_size = UseTransparentHugePages ? (size_t)os::vm_page_size(): page_size;\n-#else\n-    _page_size = page_size;\n-#endif\n-  }\n-\n-  virtual void work(uint worker_id) {\n-    size_t const actual_chunk_size = MAX2(chunk_size(), _page_size);\n-    while (true) {\n-      char* touch_addr = Atomic::fetch_and_add(&_cur_addr, actual_chunk_size);\n-      if (touch_addr < _start_addr || touch_addr >= _end_addr) {\n-        break;\n-      }\n-      char* end_addr = touch_addr + MIN2(actual_chunk_size, pointer_delta(_end_addr, touch_addr, sizeof(char)));\n-      os::pretouch_memory(touch_addr, end_addr, _page_size);\n-    }\n-  }\n-\n-  static size_t chunk_size() { return PreTouchParallelChunkSize; }\n-};\n-\n-  G1PretouchTask cl(page_start(start_page), bounded_end_addr(start_page + size_in_pages), _page_size);\n-\n-  if (pretouch_gang != NULL) {\n-    size_t num_chunks = MAX2((size_t)1, size_in_pages * _page_size \/ MAX2(G1PretouchTask::chunk_size(), _page_size));\n-    uint num_workers = MIN2((uint)num_chunks, pretouch_gang->total_workers());\n-    log_debug(gc, heap)(\"Running %s with %u workers for \" SIZE_FORMAT \" work units pre-touching \" SIZE_FORMAT \"B.\",\n-                        cl.name(), num_workers, num_chunks, size_in_pages * _page_size);\n-    pretouch_gang->run_task(&cl, num_workers);\n-  } else {\n-    log_debug(gc, heap)(\"Running %s pre-touching \" SIZE_FORMAT \"B.\",\n-                        cl.name(), size_in_pages * _page_size);\n-    cl.work(0);\n-  }\n+  PretouchTask::pretouch(\"G1 PreTouch\", page_start(start_page), bounded_end_addr(start_page + size_in_pages),\n+                         _page_size, pretouch_gang);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1PageBasedVirtualSpace.cpp","additions":4,"deletions":49,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n@@ -74,1 +75,1 @@\n-#include \"memory\/metaspace.hpp\"\n+#include \"memory\/classLoaderMetaspace.hpp\"\n@@ -187,2 +188,2 @@\n-#if SHENANDOAH_OPTIMIZED_OBJTASK\n-  \/\/ The optimized ObjArrayChunkedTask takes some bits away from the full object bits.\n+#if SHENANDOAH_OPTIMIZED_MARKTASK\n+  \/\/ The optimized ShenandoahMarkTask takes some bits away from the full object bits.\n@@ -190,1 +191,1 @@\n-  if ((uintptr_t)heap_rs.end() >= ObjArrayChunkedTask::max_addressable()) {\n+  if ((uintptr_t)heap_rs.end() >= ShenandoahMarkTask::max_addressable()) {\n@@ -194,1 +195,1 @@\n-                p2i(heap_rs.base()), p2i(heap_rs.end()), ObjArrayChunkedTask::max_addressable());\n+                p2i(heap_rs.base()), p2i(heap_rs.end()), ShenandoahMarkTask::max_addressable());\n@@ -209,1 +210,1 @@\n-  _bitmap_size = MarkBitMap::compute_size(heap_rs.size());\n+  _bitmap_size = ShenandoahMarkBitMap::compute_size(heap_rs.size());\n@@ -212,1 +213,1 @@\n-  size_t bitmap_bytes_per_region = reg_size_bytes \/ MarkBitMap::heap_map_factor();\n+  size_t bitmap_bytes_per_region = reg_size_bytes \/ ShenandoahMarkBitMap::heap_map_factor();\n@@ -396,3 +397,0 @@\n-  _ref_proc_mt_processing = ParallelRefProcEnabled && (ParallelGCThreads > 1);\n-  _ref_proc_mt_discovery = _max_workers > 1;\n-\n@@ -478,1 +476,1 @@\n-  _ref_processor(NULL),\n+  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -618,2 +616,0 @@\n-  ref_processing_init();\n-\n@@ -1794,7 +1790,3 @@\n-    if (process_references()) {\n-      \/\/ Abandon reference processing right away: pre-cleaning must have failed.\n-      ReferenceProcessor *rp = ref_processor();\n-      rp->disable_discovery();\n-      rp->abandon_partial_discovery();\n-      rp->verify_no_references_recorded();\n-    }\n+    \/\/ Abandon reference processing right away: pre-cleaning must have failed.\n+    ShenandoahReferenceProcessor* rp = ref_processor();\n+    rp->abandon_partial_discovery();\n@@ -1944,1 +1936,1 @@\n-  ShenandoahClassLoaderDataRoots<true \/* concurrent *\/, false \/* single thread*\/>\n+  ShenandoahClassLoaderDataRoots<true \/* concurrent *\/, true \/* single thread*\/>\n@@ -2007,0 +1999,9 @@\n+void ShenandoahHeap::op_weak_refs() {\n+  \/\/ Concurrent weak refs processing\n+  {\n+    ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_refs_work);\n+    ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_refs_work);\n+    ref_processor()->process_references(workers(), true \/* concurrent *\/);\n+  }\n+}\n+\n@@ -2080,7 +2081,0 @@\n-void ShenandoahHeap::op_preclean() {\n-  if (ShenandoahPacing) {\n-    pacer()->setup_for_preclean();\n-  }\n-  concurrent_mark()->preclean_weak_refs();\n-}\n-\n@@ -2128,1 +2122,0 @@\n-      set_process_references(heuristics()->can_process_references());\n@@ -2153,0 +2146,6 @@\n+      {\n+        ShenandoahTimingsTracker t(ShenandoahPhaseTimings::conc_weak_refs_work);\n+        ShenandoahGCWorkerPhase worker_phase(ShenandoahPhaseTimings::conc_weak_refs_work);\n+        ref_processor()->process_references(workers(), false \/* concurrent *\/);\n+      }\n+\n@@ -2313,16 +2312,0 @@\n-void ShenandoahHeap::ref_processing_init() {\n-  assert(_max_workers > 0, \"Sanity\");\n-\n-  _ref_processor =\n-    new ReferenceProcessor(&_subject_to_discovery,  \/\/ is_subject_to_discovery\n-                           _ref_proc_mt_processing, \/\/ MT processing\n-                           _max_workers,            \/\/ Degree of MT processing\n-                           _ref_proc_mt_discovery,  \/\/ MT discovery\n-                           _max_workers,            \/\/ Degree of MT discovery\n-                           false,                   \/\/ Reference discovery is not atomic\n-                           NULL,                    \/\/ No closure, should be installed before use\n-                           true);                   \/\/ Scale worker threads\n-\n-  shenandoah_assert_rp_isalive_not_installed();\n-}\n-\n@@ -2413,1 +2396,1 @@\n-  MetaspaceUtils::verify_metrics();\n+  DEBUG_ONLY(MetaspaceUtils::verify();)\n@@ -2436,1 +2419,1 @@\n-      cleaning_task(timing_phase, &is_alive, &keep_alive, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());\n+      cleaning_task(timing_phase, &is_alive, &keep_alive, num_workers, is_stw_gc_in_progress());\n@@ -2443,1 +2426,1 @@\n-      cleaning_task(timing_phase, &is_alive, &verify_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());\n+      cleaning_task(timing_phase, &is_alive, &verify_cl, num_workers, is_stw_gc_in_progress());\n@@ -2446,1 +2429,1 @@\n-      cleaning_task(timing_phase, &is_alive, &do_nothing_cl, num_workers, !ShenandoahConcurrentRoots::should_do_concurrent_class_unloading());\n+      cleaning_task(timing_phase, &is_alive, &do_nothing_cl, num_workers, is_stw_gc_in_progress());\n@@ -2464,4 +2447,0 @@\n-void ShenandoahHeap::set_process_references(bool pr) {\n-  _process_references.set_cond(pr);\n-}\n-\n@@ -2472,4 +2451,0 @@\n-bool ShenandoahHeap::process_references() const {\n-  return _process_references.is_set();\n-}\n-\n@@ -3070,0 +3045,13 @@\n+void ShenandoahHeap::entry_weak_refs() {\n+  static const char* msg = \"Concurrent weak references\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_weak_refs);\n+  EventMark em(\"%s\", msg);\n+\n+  ShenandoahWorkerScope scope(workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_refs_processing(),\n+                              \"concurrent weak references\");\n+\n+  try_inject_alloc_failure();\n+  op_weak_refs();\n+}\n+\n@@ -3156,16 +3144,0 @@\n-void ShenandoahHeap::entry_preclean() {\n-  if (ShenandoahPreclean && process_references()) {\n-    static const char* msg = \"Concurrent precleaning\";\n-    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_preclean);\n-    EventMark em(\"%s\", msg);\n-\n-    ShenandoahWorkerScope scope(workers(),\n-                                ShenandoahWorkerPolicy::calc_workers_for_conc_preclean(),\n-                                \"concurrent preclean\",\n-                                \/* check_workers = *\/ false);\n-\n-    try_inject_alloc_failure();\n-    op_preclean();\n-  }\n-}\n-\n@@ -3248,1 +3220,0 @@\n-  bool proc_refs = process_references();\n@@ -3251,5 +3222,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Pause Init Mark (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Pause Init Mark (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n@@ -3265,1 +3232,0 @@\n-  bool proc_refs = process_references();\n@@ -3268,5 +3234,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Pause Final Mark (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Pause Final Mark (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n@@ -3282,1 +3244,0 @@\n-  bool proc_refs = process_references();\n@@ -3285,5 +3246,1 @@\n-  if (proc_refs && unload_cls) {\n-    return \"Concurrent marking (process weakrefs) (unload classes)\";\n-  } else if (proc_refs) {\n-    return \"Concurrent marking (process weakrefs)\";\n-  } else if (unload_cls) {\n+  if (unload_cls) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":48,"deletions":91,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,1 +45,1 @@\n-  const uintptr_t addr = (uintptr_t)os::reserve_memory(size, mtGC);\n+  const uintptr_t addr = (uintptr_t)os::reserve_memory(size, !ExecMem, mtGC);\n@@ -131,3 +131,0 @@\n-  guarantee(sizeof(ZMarkStack) == ZMarkStackSize, \"Size mismatch\");\n-  guarantee(sizeof(ZMarkStackMagazine) <= ZMarkStackSize, \"Size mismatch\");\n-\n","filename":"src\/hotspot\/share\/gc\/z\/zMarkStackAllocator.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"prims\/methodHandles.hpp\"\n@@ -1191,0 +1192,3 @@\n+  \/\/ has_ea_local_in_scope and arg_escape should be added to JVMCI\n+  const bool has_ea_local_in_scope = false;\n+  const bool arg_escape            = false;\n@@ -1192,0 +1196,1 @@\n+                                  has_ea_local_in_scope, arg_escape,\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCodeInstaller.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"prims\/methodHandles.hpp\"\n@@ -1998,0 +1999,8 @@\n+\n+  if (displacement == java_lang_Class::component_mirror_offset() && java_lang_Class::is_instance(obj()) &&\n+      !java_lang_Class::as_Klass(obj())->is_array_klass()) {\n+    \/\/ Class.componentType for non-array classes can transiently contain an int[] that's\n+    \/\/ used for locking so always return null to mimic Class.getComponentType()\n+    return JVMCIENV->get_jobject(JVMCIENV->get_JavaConstant_NULL_POINTER());\n+  }\n+\n@@ -2223,0 +2232,7 @@\n+  if (displacement == java_lang_Class::component_mirror_offset() && java_lang_Class::is_instance(xobj()) &&\n+      !java_lang_Class::as_Klass(xobj())->is_array_klass()) {\n+    \/\/ Class.componentType for non-array classes can transiently contain an int[] that's\n+    \/\/ used for locking so always return null to mimic Class.getComponentType()\n+    return JVMCIENV->get_jobject(JVMCIENV->get_JavaConstant_NULL_POINTER());\n+  }\n+\n@@ -2283,0 +2299,1 @@\n+      guarantee(pure_name != NULL, \"Illegal native method name encountered\");\n@@ -2293,0 +2310,1 @@\n+        guarantee(long_name != NULL, \"Illegal native method name encountered\");\n@@ -2352,1 +2370,1 @@\n-C2V_VMENTRY_PREFIX(jboolean, attachCurrentThread, (JNIEnv* env, jobject c2vm, jboolean as_daemon))\n+C2V_VMENTRY_PREFIX(jboolean, attachCurrentThread, (JNIEnv* env, jobject c2vm, jbyteArray name, jboolean as_daemon))\n@@ -2355,0 +2373,2 @@\n+    guarantee(name != NULL, \"libjvmci caller must pass non-null name\");\n+\n@@ -2357,2 +2377,12 @@\n-    jint res = as_daemon ? main_vm.AttachCurrentThreadAsDaemon((void**) &hotspotEnv, NULL) :\n-                           main_vm.AttachCurrentThread((void**) &hotspotEnv, NULL);\n+\n+    int name_len = env->GetArrayLength(name);\n+    char name_buf[64]; \/\/ Cannot use Resource heap as it requires a current thread\n+    int to_copy = MIN2(name_len, (int) sizeof(name_buf) - 1);\n+    env->GetByteArrayRegion(name, 0, to_copy, (jbyte*) name_buf);\n+    name_buf[to_copy] = '\\0';\n+    JavaVMAttachArgs attach_args;\n+    attach_args.version = JNI_VERSION_1_2;\n+    attach_args.name = name_buf;\n+    attach_args.group = NULL;\n+    jint res = as_daemon ? main_vm.AttachCurrentThreadAsDaemon((void**) &hotspotEnv, &attach_args) :\n+                           main_vm.AttachCurrentThread((void**) &hotspotEnv, &attach_args);\n@@ -2806,1 +2836,1 @@\n-  {CC \"attachCurrentThread\",                          CC \"(Z)Z\",                                                                            FN_PTR(attachCurrentThread)},\n+  {CC \"attachCurrentThread\",                          CC \"([BZ)Z\",                                                                          FN_PTR(attachCurrentThread)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":34,"deletions":4,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"prims\/methodHandles.hpp\"\n@@ -647,2 +648,3 @@\n-  uint index = (speculation >> 32) & 0xFFFFFFFF;\n-  int length = (int) speculation;\n+  jlong index = speculation >> JVMCINMethodData::SPECULATION_LENGTH_BITS;\n+  guarantee(index >= 0 && index <= max_jint, \"Encoded JVMCI speculation index is not a positive Java int: \" INTPTR_FORMAT, index);\n+  int length = speculation & JVMCINMethodData::SPECULATION_LENGTH_MASK;\n@@ -650,1 +652,1 @@\n-    fatal(INTPTR_FORMAT \"[index: %d, length: %d] out of bounds wrt encoded speculations of length %u\", speculation, index, length, nm->speculations_size());\n+    fatal(INTPTR_FORMAT \"[index: \" JLONG_FORMAT \", length: %d out of bounds wrt encoded speculations of length %u\", speculation, index, length, nm->speculations_size());\n@@ -1331,2 +1333,4 @@\n-  Method* dest_method;\n-  LinkInfo link_info(holder, name, sig, accessor, LinkInfo::AccessCheck::required, LinkInfo::LoaderConstraintCheck::required, tag);\n+  LinkInfo link_info(holder, name, sig, accessor,\n+                     LinkInfo::AccessCheck::required,\n+                     LinkInfo::LoaderConstraintCheck::required,\n+                     tag);\n@@ -1334,20 +1338,12 @@\n-  case Bytecodes::_invokestatic:\n-    dest_method =\n-      LinkResolver::resolve_static_call_or_null(link_info);\n-    break;\n-  case Bytecodes::_invokespecial:\n-    dest_method =\n-      LinkResolver::resolve_special_call_or_null(link_info);\n-    break;\n-  case Bytecodes::_invokeinterface:\n-    dest_method =\n-      LinkResolver::linktime_resolve_interface_method_or_null(link_info);\n-    break;\n-  case Bytecodes::_invokevirtual:\n-    dest_method =\n-      LinkResolver::linktime_resolve_virtual_method_or_null(link_info);\n-    break;\n-  default: ShouldNotReachHere();\n-  }\n-\n-  return dest_method;\n+    case Bytecodes::_invokestatic:\n+      return LinkResolver::resolve_static_call_or_null(link_info);\n+    case Bytecodes::_invokespecial:\n+      return LinkResolver::resolve_special_call_or_null(link_info);\n+    case Bytecodes::_invokeinterface:\n+      return LinkResolver::linktime_resolve_interface_method_or_null(link_info);\n+    case Bytecodes::_invokevirtual:\n+      return LinkResolver::linktime_resolve_virtual_method_or_null(link_info);\n+    default:\n+      fatal(\"Unhandled bytecode: %s\", Bytecodes::name(bc));\n+      return NULL; \/\/ silence compiler warnings\n+  }\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":21,"deletions":25,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -59,1 +59,1 @@\n-  char* addr = os::reserve_memory(size, flags);\n+  char* addr = os::reserve_memory(size, !ExecMem, flags);\n@@ -76,1 +76,1 @@\n-  char* addr = os::reserve_memory(size, flags);\n+  char* addr = os::reserve_memory(size, !ExecMem, flags);\n","filename":"src\/hotspot\/share\/memory\/allocation.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,2 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020 SAP SE. All rights reserved.\n@@ -26,5 +27,7 @@\n-\n-#include \"logging\/logStream.hpp\"\n-#include \"memory\/metaspace\/metachunk.hpp\"\n-#include \"memory\/metaspace\/chunkManager.hpp\"\n-#include \"memory\/metaspace\/metaDebug.hpp\"\n+#include \"memory\/metaspace\/chunkHeaderPool.hpp\"\n+#include \"memory\/metaspace\/chunklevel.hpp\"\n+#include \"memory\/metaspace\/commitLimiter.hpp\"\n+#include \"memory\/metaspace\/counters.hpp\"\n+#include \"memory\/metaspace\/freeChunkList.hpp\"\n+#include \"memory\/metaspace\/internalStats.hpp\"\n+#include \"memory\/metaspace\/metachunk.hpp\"\n@@ -34,1 +37,3 @@\n-#include \"memory\/metaspace\/occupancyMap.hpp\"\n+#include \"memory\/metaspace\/metaspaceSettings.hpp\"\n+#include \"memory\/metaspace\/rootChunkArea.hpp\"\n+#include \"memory\/metaspace\/runningCounters.hpp\"\n@@ -36,2 +41,2 @@\n-#include \"memory\/virtualspace.hpp\"\n-#include \"runtime\/atomic.hpp\"\n+#include \"runtime\/globals.hpp\"\n+#include \"runtime\/mutexLocker.hpp\"\n@@ -39,2 +44,1 @@\n-#include \"services\/memTracker.hpp\"\n-#include \"utilities\/copy.hpp\"\n+#include \"utilities\/align.hpp\"\n@@ -43,0 +47,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -46,10 +51,2 @@\n-\/\/ Decide if large pages should be committed when the memory is reserved.\n-static bool should_commit_large_pages_when_reserving(size_t bytes) {\n-  if (UseLargePages && UseLargePagesInMetaspace && !os::can_commit_large_page_memory()) {\n-    size_t words = bytes \/ BytesPerWord;\n-    bool is_class = false; \/\/ We never reserve large pages for the class space.\n-    if (MetaspaceGC::can_expand(words, is_class) &&\n-        MetaspaceGC::allowed_expansion() >= words) {\n-      return true;\n-    }\n-  }\n+#define LOGFMT         \"VsListNode @\" PTR_FORMAT \" base \" PTR_FORMAT \" \"\n+#define LOGFMT_ARGS    p2i(this), p2i(_base)\n@@ -57,1 +54,5 @@\n-  return false;\n+#ifdef ASSERT\n+void check_pointer_is_aligned_to_commit_granule(const MetaWord* p) {\n+  assert(is_aligned(p, Settings::commit_granule_bytes()),\n+         \"Pointer not aligned to commit granule size: \" PTR_FORMAT \".\",\n+         p2i(p));\n@@ -59,0 +60,27 @@\n+void check_word_size_is_aligned_to_commit_granule(size_t word_size) {\n+  assert(is_aligned(word_size, Settings::commit_granule_words()),\n+         \"Not aligned to commit granule size: \" SIZE_FORMAT \".\", word_size);\n+}\n+#endif\n+\n+\/\/ Given an address range, ensure it is committed.\n+\/\/\n+\/\/ The range has to be aligned to granule size.\n+\/\/\n+\/\/ Function will:\n+\/\/ - check how many granules in that region are uncommitted; If all are committed, it\n+\/\/    returns true immediately.\n+\/\/ - check if committing those uncommitted granules would bring us over the commit limit\n+\/\/    (GC threshold, MaxMetaspaceSize). If true, it returns false.\n+\/\/ - commit the memory.\n+\/\/ - mark the range as committed in the commit mask\n+\/\/\n+\/\/ Returns true if success, false if it did hit a commit limit.\n+bool VirtualSpaceNode::commit_range(MetaWord* p, size_t word_size) {\n+  DEBUG_ONLY(check_pointer_is_aligned_to_commit_granule(p);)\n+  DEBUG_ONLY(check_word_size_is_aligned_to_commit_granule(word_size);)\n+  assert_lock_strong(MetaspaceExpand_lock);\n+\n+  \/\/ First calculate how large the committed regions in this range are\n+  const size_t committed_words_in_range = _commit_mask.get_committed_size_in_range(p, word_size);\n+  DEBUG_ONLY(check_word_size_is_aligned_to_commit_granule(committed_words_in_range);)\n@@ -60,6 +88,3 @@\n-\/\/ byte_size is the size of the associated virtualspace.\n-VirtualSpaceNode::VirtualSpaceNode(bool is_class, size_t bytes) :\n-    _next(NULL), _is_class(is_class), _rs(), _top(NULL), _container_count(0), _occupancy_map(NULL) {\n-  assert_is_aligned(bytes, Metaspace::reserve_alignment());\n-  bool large_pages = should_commit_large_pages_when_reserving(bytes);\n-  _rs = ReservedSpace(bytes, Metaspace::reserve_alignment(), large_pages);\n+  \/\/ By how much words we would increase commit charge\n+  \/\/  were we to commit the given address range completely.\n+  const size_t commit_increase_words = word_size - committed_words_in_range;\n@@ -67,5 +92,2 @@\n-  if (_rs.is_reserved()) {\n-    assert(_rs.base() != NULL, \"Catch if we get a NULL address\");\n-    assert(_rs.size() != 0, \"Catch if we get a 0 size\");\n-    assert_is_aligned(_rs.base(), Metaspace::reserve_alignment());\n-    assert_is_aligned(_rs.size(), Metaspace::reserve_alignment());\n+  UL2(debug, \"committing range \" PTR_FORMAT \"..\" PTR_FORMAT \"(\" SIZE_FORMAT \" words)\",\n+      p2i(p), p2i(p + word_size), word_size);\n@@ -73,1 +95,3 @@\n-    MemTracker::record_virtual_memory_type((address)_rs.base(), mtClass);\n+  if (commit_increase_words == 0) {\n+    UL(debug, \"... already fully committed.\");\n+    return true; \/\/ Already fully committed, nothing to do.\n@@ -75,15 +99,4 @@\n-}\n-void VirtualSpaceNode::purge(ChunkManager* chunk_manager) {\n-  \/\/ When a node is purged, lets give it a thorough examination.\n-  DEBUG_ONLY(verify(true);)\n-  Metachunk* chunk = first_chunk();\n-  Metachunk* invalid_chunk = (Metachunk*) top();\n-  while (chunk < invalid_chunk ) {\n-    assert(chunk->is_tagged_free(), \"Should be tagged free\");\n-    MetaWord* next = ((MetaWord*)chunk) + chunk->word_size();\n-    chunk_manager->remove_chunk(chunk);\n-    chunk->remove_sentinel();\n-    assert(chunk->next() == NULL &&\n-        chunk->prev() == NULL,\n-        \"Was not removed from its list\");\n-    chunk = (Metachunk*) next;\n+  \/\/ Before committing any more memory, check limits.\n+  if (_commit_limiter->possible_expansion_words() < commit_increase_words) {\n+    UL(debug, \"... cannot commit (limit).\");\n+    return false;\n@@ -92,2 +105,4 @@\n-}\n-void VirtualSpaceNode::print_map(outputStream* st, bool is_class) const {\n+  \/\/ Commit...\n+  if (os::commit_memory((char*)p, word_size * BytesPerWord, false) == false) {\n+    vm_exit_out_of_memory(word_size * BytesPerWord, OOM_MMAP_ERROR, \"Failed to commit metaspace.\");\n+  }\n@@ -96,2 +111,2 @@\n-  if (bottom() == top()) {\n-    return;\n+  if (AlwaysPreTouch) {\n+    os::pretouch_memory(p, p + word_size);\n@@ -100,3 +115,1 @@\n-  const size_t spec_chunk_size = is_class ? ClassSpecializedChunk : SpecializedChunk;\n-  const size_t small_chunk_size = is_class ? ClassSmallChunk : SmallChunk;\n-  const size_t med_chunk_size = is_class ? ClassMediumChunk : MediumChunk;\n+  UL2(debug, \"... committed \" SIZE_FORMAT \" additional words.\", commit_increase_words);\n@@ -104,3 +117,2 @@\n-  int line_len = 100;\n-  const size_t section_len = align_up(spec_chunk_size * line_len, med_chunk_size);\n-  line_len = (int)(section_len \/ spec_chunk_size);\n+  \/\/ ... tell commit limiter...\n+  _commit_limiter->increase_committed(commit_increase_words);\n@@ -108,1 +120,2 @@\n-  static const int NUM_LINES = 4;\n+  \/\/ ... update counters in containing vslist ...\n+  _total_committed_words_counter->increment_by(commit_increase_words);\n@@ -110,41 +123,2 @@\n-  char* lines[NUM_LINES];\n-  for (int i = 0; i < NUM_LINES; i ++) {\n-    lines[i] = (char*)os::malloc(line_len, mtInternal);\n-  }\n-  int pos = 0;\n-  const MetaWord* p = bottom();\n-  const Metachunk* chunk = (const Metachunk*)p;\n-  const MetaWord* chunk_end = p + chunk->word_size();\n-  while (p < top()) {\n-    if (pos == line_len) {\n-      pos = 0;\n-      for (int i = 0; i < NUM_LINES; i ++) {\n-        st->fill_to(22);\n-        st->print_raw(lines[i], line_len);\n-        st->cr();\n-      }\n-    }\n-    if (pos == 0) {\n-      st->print(PTR_FORMAT \":\", p2i(p));\n-    }\n-    if (p == chunk_end) {\n-      chunk = (Metachunk*)p;\n-      chunk_end = p + chunk->word_size();\n-    }\n-    \/\/ line 1: chunk starting points (a dot if that area is a chunk start).\n-    lines[0][pos] = p == (const MetaWord*)chunk ? '.' : ' ';\n-\n-    \/\/ Line 2: chunk type (x=spec, s=small, m=medium, h=humongous), uppercase if\n-    \/\/ chunk is in use.\n-    const bool chunk_is_free = ((Metachunk*)chunk)->is_tagged_free();\n-    if (chunk->word_size() == spec_chunk_size) {\n-      lines[1][pos] = chunk_is_free ? 'x' : 'X';\n-    } else if (chunk->word_size() == small_chunk_size) {\n-      lines[1][pos] = chunk_is_free ? 's' : 'S';\n-    } else if (chunk->word_size() == med_chunk_size) {\n-      lines[1][pos] = chunk_is_free ? 'm' : 'M';\n-    } else if (chunk->word_size() > med_chunk_size) {\n-      lines[1][pos] = chunk_is_free ? 'h' : 'H';\n-    } else {\n-      ShouldNotReachHere();\n-    }\n+  \/\/ ... and update the commit mask.\n+  _commit_mask.mark_range_as_committed(p, word_size);\n@@ -152,3 +126,7 @@\n-    \/\/ Line 3: chunk origin\n-    const ChunkOrigin origin = chunk->get_origin();\n-    lines[2][pos] = origin == origin_normal ? ' ' : '0' + (int) origin;\n+#ifdef ASSERT\n+  \/\/ The commit boundary maintained in the CommitLimiter should be equal the sum of committed words\n+  \/\/ in both class and non-class vslist (outside gtests).\n+  if (_commit_limiter == CommitLimiter::globalLimiter()) {\n+    assert(_commit_limiter->committed_words() == RunningCounters::committed_words(), \"counter mismatch\");\n+  }\n+#endif\n@@ -156,3 +134,3 @@\n-    \/\/ Line 4: Virgin chunk? Virgin chunks are chunks created as a byproduct of padding or splitting,\n-    \/\/         but were never used.\n-    lines[3][pos] = chunk->get_use_count() > 0 ? ' ' : 'v';\n+  InternalStats::inc_num_space_committed();\n+  return true;\n+}\n@@ -160,13 +138,25 @@\n-    p += spec_chunk_size;\n-    pos ++;\n-  }\n-  if (pos > 0) {\n-    for (int i = 0; i < NUM_LINES; i ++) {\n-      st->fill_to(22);\n-      st->print_raw(lines[i], line_len);\n-      st->cr();\n-    }\n-  }\n-  for (int i = 0; i < NUM_LINES; i ++) {\n-    os::free(lines[i]);\n-  }\n+\/\/ Given an address range, ensure it is committed.\n+\/\/\n+\/\/ The range does not have to be aligned to granule size. However, the function will always commit\n+\/\/ whole granules.\n+\/\/\n+\/\/ Function will:\n+\/\/ - check how many granules in that region are uncommitted; If all are committed, it\n+\/\/    returns true immediately.\n+\/\/ - check if committing those uncommitted granules would bring us over the commit limit\n+\/\/    (GC threshold, MaxMetaspaceSize). If true, it returns false.\n+\/\/ - commit the memory.\n+\/\/ - mark the range as committed in the commit mask\n+\/\/\n+\/\/ !! Careful:\n+\/\/    calling ensure_range_is_committed on a range which contains both committed and uncommitted\n+\/\/    areas will commit the whole area, thus erase the content in the existing committed parts.\n+\/\/    Make sure you never call this on an address range containing live data. !!\n+\/\/\n+\/\/ Returns true if success, false if it did hit a commit limit.\n+bool VirtualSpaceNode::ensure_range_is_committed(MetaWord* p, size_t word_size) {\n+  assert_lock_strong(MetaspaceExpand_lock);\n+  assert(p != NULL && word_size > 0, \"Sanity\");\n+  MetaWord* p_start = align_down(p, Settings::commit_granule_bytes());\n+  MetaWord* p_end = align_up(p + word_size, Settings::commit_granule_bytes());\n+  return commit_range(p_start, p_end - p_start);\n@@ -175,0 +165,7 @@\n+\/\/ Given an address range (which has to be aligned to commit granule size):\n+\/\/  - uncommit it\n+\/\/  - mark it as uncommitted in the commit mask\n+void VirtualSpaceNode::uncommit_range(MetaWord* p, size_t word_size) {\n+  DEBUG_ONLY(check_pointer_is_aligned_to_commit_granule(p);)\n+  DEBUG_ONLY(check_word_size_is_aligned_to_commit_granule(word_size);)\n+  assert_lock_strong(MetaspaceExpand_lock);\n@@ -176,1 +173,3 @@\n-#ifdef ASSERT\n+  \/\/ First calculate how large the committed regions in this range are\n+  const size_t committed_words_in_range = _commit_mask.get_committed_size_in_range(p, word_size);\n+  DEBUG_ONLY(check_word_size_is_aligned_to_commit_granule(committed_words_in_range);)\n@@ -178,29 +177,6 @@\n-\/\/ Verify counters, all chunks in this list node and the occupancy map.\n-void VirtualSpaceNode::verify(bool slow) {\n-  log_trace(gc, metaspace, freelist)(\"verifying %s virtual space node (%s).\",\n-    (is_class() ? \"class space\" : \"metaspace\"), (slow ? \"slow\" : \"quick\"));\n-  \/\/ Fast mode: just verify chunk counters and basic geometry\n-  \/\/ Slow mode: verify chunks and occupancy map\n-  uintx num_in_use_chunks = 0;\n-  Metachunk* chunk = first_chunk();\n-  Metachunk* invalid_chunk = (Metachunk*) top();\n-\n-  \/\/ Iterate the chunks in this node and verify each chunk.\n-  while (chunk < invalid_chunk ) {\n-    if (slow) {\n-      do_verify_chunk(chunk);\n-    }\n-    if (!chunk->is_tagged_free()) {\n-      num_in_use_chunks ++;\n-    }\n-    const size_t s = chunk->word_size();\n-    \/\/ Prevent endless loop on invalid chunk size.\n-    assert(is_valid_chunksize(is_class(), s), \"Invalid chunk size: \" SIZE_FORMAT \".\", s);\n-    MetaWord* next = ((MetaWord*)chunk) + s;\n-    chunk = (Metachunk*) next;\n-  }\n-  assert(_container_count == num_in_use_chunks, \"Container count mismatch (real: \" UINTX_FORMAT\n-      \", counter: \" UINTX_FORMAT \".\", num_in_use_chunks, _container_count);\n-  \/\/ Also verify the occupancy map.\n-  if (slow) {\n-    occupancy_map()->verify(bottom(), top());\n+  UL2(debug, \"uncommitting range \" PTR_FORMAT \"..\" PTR_FORMAT \"(\" SIZE_FORMAT \" words)\",\n+      p2i(p), p2i(p + word_size), word_size);\n+\n+  if (committed_words_in_range == 0) {\n+    UL(debug, \"... already fully uncommitted.\");\n+    return; \/\/ Already fully uncommitted, nothing to do.\n@@ -208,49 +184,5 @@\n-}\n-\/\/ Verify that all free chunks in this node are ideally merged\n-\/\/ (there not should be multiple small chunks where a large chunk could exist.)\n-void VirtualSpaceNode::verify_free_chunks_are_ideally_merged() {\n-  Metachunk* chunk = first_chunk();\n-  Metachunk* invalid_chunk = (Metachunk*) top();\n-  \/\/ Shorthands.\n-  const size_t size_med = (is_class() ? ClassMediumChunk : MediumChunk) * BytesPerWord;\n-  const size_t size_small = (is_class() ? ClassSmallChunk : SmallChunk) * BytesPerWord;\n-  int num_free_chunks_since_last_med_boundary = -1;\n-  int num_free_chunks_since_last_small_boundary = -1;\n-  bool error = false;\n-  char err[256];\n-  while (!error && chunk < invalid_chunk ) {\n-    \/\/ Test for missed chunk merge opportunities: count number of free chunks since last chunk boundary.\n-    \/\/ Reset the counter when encountering a non-free chunk.\n-    if (chunk->get_chunk_type() != HumongousIndex) {\n-      if (chunk->is_tagged_free()) {\n-        \/\/ Count successive free, non-humongous chunks.\n-        if (is_aligned(chunk, size_small)) {\n-          if (num_free_chunks_since_last_small_boundary > 0) {\n-            error = true;\n-            jio_snprintf(err, sizeof(err), \"Missed chunk merge opportunity to merge a small chunk preceding \" PTR_FORMAT \".\", p2i(chunk));\n-          } else {\n-            num_free_chunks_since_last_small_boundary = 0;\n-          }\n-        } else if (num_free_chunks_since_last_small_boundary != -1) {\n-          num_free_chunks_since_last_small_boundary ++;\n-        }\n-        if (is_aligned(chunk, size_med)) {\n-          if (num_free_chunks_since_last_med_boundary > 0) {\n-            error = true;\n-            jio_snprintf(err, sizeof(err), \"Missed chunk merge opportunity to merge a medium chunk preceding \" PTR_FORMAT \".\", p2i(chunk));\n-          } else {\n-            num_free_chunks_since_last_med_boundary = 0;\n-          }\n-        } else if (num_free_chunks_since_last_med_boundary != -1) {\n-          num_free_chunks_since_last_med_boundary ++;\n-        }\n-      } else {\n-        \/\/ Encountering a non-free chunk, reset counters.\n-        num_free_chunks_since_last_med_boundary = -1;\n-        num_free_chunks_since_last_small_boundary = -1;\n-      }\n-    } else {\n-      \/\/ One cannot merge areas with a humongous chunk in the middle. Reset counters.\n-      num_free_chunks_since_last_med_boundary = -1;\n-      num_free_chunks_since_last_small_boundary = -1;\n-    }\n+  \/\/ Uncommit...\n+  if (os::uncommit_memory((char*)p, word_size * BytesPerWord, !ExecMem) == false) {\n+    \/\/ Note: this can actually happen, since uncommit may increase the number of mappings.\n+    fatal(\"Failed to uncommit metaspace.\");\n+  }\n@@ -259,4 +191,1 @@\n-    if (error) {\n-      print_map(tty, is_class());\n-      fatal(\"%s\", err);\n-    }\n+  UL2(debug, \"... uncommitted \" SIZE_FORMAT \" words.\", committed_words_in_range);\n@@ -264,5 +193,2 @@\n-    MetaWord* next = ((MetaWord*)chunk) + chunk->word_size();\n-    chunk = (Metachunk*) next;\n-  }\n-}\n-#endif \/\/ ASSERT\n+  \/\/ ... tell commit limiter...\n+  _commit_limiter->decrease_committed(committed_words_in_range);\n@@ -270,4 +196,2 @@\n-void VirtualSpaceNode::inc_container_count() {\n-  assert_lock_strong(MetaspaceExpand_lock);\n-  _container_count++;\n-}\n+  \/\/ ... and global counters...\n+  _total_committed_words_counter->decrement_by(committed_words_in_range);\n@@ -275,4 +199,2 @@\n-void VirtualSpaceNode::dec_container_count() {\n-  assert_lock_strong(MetaspaceExpand_lock);\n-  _container_count--;\n-}\n+   \/\/ ... and update the commit mask.\n+  _commit_mask.mark_range_as_uncommitted(p, word_size);\n@@ -280,7 +202,5 @@\n-VirtualSpaceNode::~VirtualSpaceNode() {\n-  _rs.release();\n-  if (_occupancy_map != NULL) {\n-    delete _occupancy_map;\n-  }\n-  size_t word_size = sizeof(*this) \/ BytesPerWord;\n-  Copy::fill_to_words((HeapWord*) this, word_size, 0xf1f1f1f1);\n+  \/\/ The commit boundary maintained in the CommitLimiter should be equal the sum of committed words\n+  \/\/ in both class and non-class vslist (outside gtests).\n+  if (_commit_limiter == CommitLimiter::globalLimiter()) { \/\/ We are outside a test scenario\n+    assert(_commit_limiter->committed_words() == RunningCounters::committed_words(), \"counter mismatch\");\n+  }\n@@ -289,0 +209,1 @@\n+  InternalStats::inc_num_space_uncommitted();\n@@ -291,2 +212,23 @@\n-size_t VirtualSpaceNode::used_words_in_vs() const {\n-  return pointer_delta(top(), bottom(), sizeof(MetaWord));\n+\/\/\/\/ creation, destruction \/\/\/\/\n+\n+VirtualSpaceNode::VirtualSpaceNode(ReservedSpace rs, bool owns_rs, CommitLimiter* limiter,\n+                                   SizeCounter* reserve_counter, SizeCounter* commit_counter) :\n+  _next(NULL),\n+  _rs(rs),\n+  _owns_rs(owns_rs),\n+  _base((MetaWord*)rs.base()),\n+  _word_size(rs.size() \/ BytesPerWord),\n+  _used_words(0),\n+  _commit_mask((MetaWord*)rs.base(), rs.size() \/ BytesPerWord),\n+  _root_chunk_area_lut((MetaWord*)rs.base(), rs.size() \/ BytesPerWord),\n+  _commit_limiter(limiter),\n+  _total_reserved_words_counter(reserve_counter),\n+  _total_committed_words_counter(commit_counter)\n+{\n+  UL2(debug, \"born (word_size \" SIZE_FORMAT \").\", _word_size);\n+\n+  \/\/ Update reserved counter in vslist\n+  _total_reserved_words_counter->increment_by(_word_size);\n+\n+  assert_is_aligned(_base, chunklevel::MAX_CHUNK_BYTE_SIZE);\n+  assert_is_aligned(_word_size, chunklevel::MAX_CHUNK_WORD_SIZE);\n@@ -295,3 +237,16 @@\n-\/\/ Space committed in the VirtualSpace\n-size_t VirtualSpaceNode::capacity_words_in_vs() const {\n-  return pointer_delta(end(), bottom(), sizeof(MetaWord));\n+\/\/ Create a node of a given size (it will create its own space).\n+VirtualSpaceNode* VirtualSpaceNode::create_node(size_t word_size,\n+                                                CommitLimiter* limiter, SizeCounter* reserve_words_counter,\n+                                                SizeCounter* commit_words_counter)\n+{\n+  DEBUG_ONLY(assert_is_aligned(word_size, chunklevel::MAX_CHUNK_WORD_SIZE);)\n+  ReservedSpace rs(word_size * BytesPerWord,\n+                   Settings::virtual_space_node_reserve_alignment_words() * BytesPerWord,\n+                   false \/\/ large\n+                   );\n+  if (!rs.is_reserved()) {\n+    vm_exit_out_of_memory(word_size * BytesPerWord, OOM_MMAP_ERROR, \"Failed to reserve memory for metaspace\");\n+  }\n+  assert_is_aligned(rs.base(), chunklevel::MAX_CHUNK_BYTE_SIZE);\n+  InternalStats::inc_num_vsnodes_births();\n+  return new VirtualSpaceNode(rs, true, limiter, reserve_words_counter, commit_words_counter);\n@@ -300,2 +255,6 @@\n-size_t VirtualSpaceNode::free_words_in_vs() const {\n-  return pointer_delta(end(), top(), sizeof(MetaWord));\n+\/\/ Create a node over an existing space\n+VirtualSpaceNode* VirtualSpaceNode::create_node(ReservedSpace rs, CommitLimiter* limiter,\n+                                                SizeCounter* reserve_words_counter, SizeCounter* commit_words_counter)\n+{\n+  InternalStats::inc_num_vsnodes_births();\n+  return new VirtualSpaceNode(rs, false, limiter, reserve_words_counter, commit_words_counter);\n@@ -304,14 +263,2 @@\n-\/\/ Given an address larger than top(), allocate padding chunks until top is at the given address.\n-void VirtualSpaceNode::allocate_padding_chunks_until_top_is_at(MetaWord* target_top) {\n-\n-  assert(target_top > top(), \"Sanity\");\n-\n-  \/\/ Padding chunks are added to the freelist.\n-  ChunkManager* const chunk_manager = Metaspace::get_chunk_manager(is_class());\n-\n-  \/\/ shorthands\n-  const size_t spec_word_size = chunk_manager->specialized_chunk_word_size();\n-  const size_t small_word_size = chunk_manager->small_chunk_word_size();\n-  const size_t med_word_size = chunk_manager->medium_chunk_word_size();\n-\n-  while (top() < target_top) {\n+VirtualSpaceNode::~VirtualSpaceNode() {\n+  DEBUG_ONLY(verify_locked();)\n@@ -319,39 +266,3 @@\n-    \/\/ We could make this coding more generic, but right now we only deal with two possible chunk sizes\n-    \/\/ for padding chunks, so it is not worth it.\n-    size_t padding_chunk_word_size = small_word_size;\n-    if (is_aligned(top(), small_word_size * sizeof(MetaWord)) == false) {\n-      assert_is_aligned(top(), spec_word_size * sizeof(MetaWord)); \/\/ Should always hold true.\n-      padding_chunk_word_size = spec_word_size;\n-    }\n-    MetaWord* here = top();\n-    assert_is_aligned(here, padding_chunk_word_size * sizeof(MetaWord));\n-    inc_top(padding_chunk_word_size);\n-\n-    \/\/ Create new padding chunk.\n-    ChunkIndex padding_chunk_type = get_chunk_type_by_size(padding_chunk_word_size, is_class());\n-    assert(padding_chunk_type == SpecializedIndex || padding_chunk_type == SmallIndex, \"sanity\");\n-\n-    Metachunk* const padding_chunk =\n-        ::new (here) Metachunk(padding_chunk_type, is_class(), padding_chunk_word_size, this);\n-    assert(padding_chunk == (Metachunk*)here, \"Sanity\");\n-    DEBUG_ONLY(padding_chunk->set_origin(origin_pad);)\n-    log_trace(gc, metaspace, freelist)(\"Created padding chunk in %s at \"\n-        PTR_FORMAT \", size \" SIZE_FORMAT_HEX \".\",\n-        (is_class() ? \"class space \" : \"metaspace\"),\n-        p2i(padding_chunk), padding_chunk->word_size() * sizeof(MetaWord));\n-\n-    \/\/ Mark chunk start in occupancy map.\n-    occupancy_map()->set_chunk_starts_at_address((MetaWord*)padding_chunk, true);\n-\n-    \/\/ Chunks are born as in-use (see MetaChunk ctor). So, before returning\n-    \/\/ the padding chunk to its chunk manager, mark it as in use (ChunkManager\n-    \/\/ will assert that).\n-    do_update_in_use_info_for_chunk(padding_chunk, true);\n-\n-    \/\/ Return Chunk to freelist.\n-    inc_container_count();\n-    chunk_manager->return_single_chunk(padding_chunk);\n-    \/\/ Please note: at this point, ChunkManager::return_single_chunk()\n-    \/\/ may already have merged the padding chunk with neighboring chunks, so\n-    \/\/ it may have vanished at this point. Do not reference the padding\n-    \/\/ chunk beyond this point.\n+  UL(debug, \": dies.\");\n+  if (_owns_rs) {\n+    _rs.release();\n@@ -360,41 +271,4 @@\n-  assert(top() == target_top, \"Sanity\");\n-\n-} \/\/ allocate_padding_chunks_until_top_is_at()\n-\n-\/\/ Allocates the chunk from the virtual space only.\n-\/\/ This interface is also used internally for debugging.  Not all\n-\/\/ chunks removed here are necessarily used for allocation.\n-Metachunk* VirtualSpaceNode::take_from_committed(size_t chunk_word_size) {\n-  \/\/ Non-humongous chunks are to be allocated aligned to their chunk\n-  \/\/ size. So, start addresses of medium chunks are aligned to medium\n-  \/\/ chunk size, those of small chunks to small chunk size and so\n-  \/\/ forth. This facilitates merging of free chunks and reduces\n-  \/\/ fragmentation. Chunk sizes are spec < small < medium, with each\n-  \/\/ larger chunk size being a multiple of the next smaller chunk\n-  \/\/ size.\n-  \/\/ Because of this alignment, me may need to create a number of padding\n-  \/\/ chunks. These chunks are created and added to the freelist.\n-\n-  \/\/ The chunk manager to which we will give our padding chunks.\n-  ChunkManager* const chunk_manager = Metaspace::get_chunk_manager(is_class());\n-\n-  \/\/ shorthands\n-  const size_t spec_word_size = chunk_manager->specialized_chunk_word_size();\n-  const size_t small_word_size = chunk_manager->small_chunk_word_size();\n-  const size_t med_word_size = chunk_manager->medium_chunk_word_size();\n-\n-  assert(chunk_word_size == spec_word_size || chunk_word_size == small_word_size ||\n-      chunk_word_size >= med_word_size, \"Invalid chunk size requested.\");\n-\n-  \/\/ Chunk alignment (in bytes) == chunk size unless humongous.\n-  \/\/ Humongous chunks are aligned to the smallest chunk size (spec).\n-  const size_t required_chunk_alignment = (chunk_word_size > med_word_size ?\n-      spec_word_size : chunk_word_size) * sizeof(MetaWord);\n-\n-  \/\/ Do we have enough space to create the requested chunk plus\n-  \/\/ any padding chunks needed?\n-  MetaWord* const next_aligned =\n-      static_cast<MetaWord*>(align_up(top(), required_chunk_alignment));\n-  if (!is_available((next_aligned - top()) + chunk_word_size)) {\n-    return NULL;\n-  }\n+  \/\/ Update counters in vslist\n+  size_t committed = committed_words();\n+  _total_committed_words_counter->decrement_by(committed);\n+  _total_reserved_words_counter->decrement_by(_word_size);\n@@ -402,12 +276,2 @@\n-  \/\/ Before allocating the requested chunk, allocate padding chunks if necessary.\n-  \/\/ We only need to do this for small or medium chunks: specialized chunks are the\n-  \/\/ smallest size, hence always aligned. Homungous chunks are allocated unaligned\n-  \/\/ (implicitly, also aligned to smallest chunk size).\n-  if ((chunk_word_size == med_word_size || chunk_word_size == small_word_size) && next_aligned > top())  {\n-    log_trace(gc, metaspace, freelist)(\"Creating padding chunks in %s between %p and %p...\",\n-        (is_class() ? \"class space \" : \"metaspace\"),\n-        top(), next_aligned);\n-    allocate_padding_chunks_until_top_is_at(next_aligned);\n-    \/\/ Now, top should be aligned correctly.\n-    assert_is_aligned(top(), required_chunk_alignment);\n-  }\n+  \/\/ ... and tell commit limiter\n+  _commit_limiter->decrease_committed(committed);\n@@ -415,23 +279,2 @@\n-  \/\/ Now, top should be aligned correctly.\n-  assert_is_aligned(top(), required_chunk_alignment);\n-\n-  \/\/ Bottom of the new chunk\n-  MetaWord* chunk_limit = top();\n-  assert(chunk_limit != NULL, \"Not safe to call this method\");\n-\n-  \/\/ The virtual spaces are always expanded by the\n-  \/\/ commit granularity to enforce the following condition.\n-  \/\/ Without this the is_available check will not work correctly.\n-  assert(_virtual_space.committed_size() == _virtual_space.actual_committed_size(),\n-      \"The committed memory doesn't match the expanded memory.\");\n-\n-  if (!is_available(chunk_word_size)) {\n-    LogTarget(Trace, gc, metaspace, freelist) lt;\n-    if (lt.is_enabled()) {\n-      LogStream ls(lt);\n-      ls.print(\"VirtualSpaceNode::take_from_committed() not available \" SIZE_FORMAT \" words \", chunk_word_size);\n-      \/\/ Dump some information about the virtual space that is nearly full\n-      print_on(&ls);\n-    }\n-    return NULL;\n-  }\n+  InternalStats::inc_num_vsnodes_deaths();\n+}\n@@ -439,2 +282,1 @@\n-  \/\/ Take the space  (bump top on the current virtual space).\n-  inc_top(chunk_word_size);\n+\/\/\/\/ Chunk allocation, splitting, merging \/\/\/\/\/\n@@ -442,6 +284,9 @@\n-  \/\/ Initialize the chunk\n-  ChunkIndex chunk_type = get_chunk_type_by_size(chunk_word_size, is_class());\n-  Metachunk* result = ::new (chunk_limit) Metachunk(chunk_type, is_class(), chunk_word_size, this);\n-  assert(result == (Metachunk*)chunk_limit, \"Sanity\");\n-  occupancy_map()->set_chunk_starts_at_address((MetaWord*)result, true);\n-  do_update_in_use_info_for_chunk(result, true);\n+\/\/ Allocate a root chunk from this node. Will fail and return NULL if the node is full\n+\/\/  - if we used up the whole address space of this node's memory region.\n+\/\/    (in case this node backs compressed class space, this is how we hit\n+\/\/     CompressedClassSpaceSize).\n+\/\/ Note that this just returns reserved memory; caller must take care of committing this\n+\/\/  chunk before using it.\n+Metachunk* VirtualSpaceNode::allocate_root_chunk() {\n+  assert_lock_strong(MetaspaceExpand_lock);\n+  assert_is_aligned(free_words(), chunklevel::MAX_CHUNK_WORD_SIZE);\n@@ -449,1 +294,1 @@\n-  inc_container_count();\n+  if (free_words() >= chunklevel::MAX_CHUNK_WORD_SIZE) {\n@@ -451,7 +296,2 @@\n-#ifdef ASSERT\n-  EVERY_NTH(VerifyMetaspaceInterval)\n-    chunk_manager->locked_verify(true);\n-    verify(true);\n-  END_EVERY_NTH\n-  do_verify_chunk(result);\n-#endif\n+    MetaWord* loc = _base + _used_words;\n+    _used_words += chunklevel::MAX_CHUNK_WORD_SIZE;\n@@ -459,1 +299,1 @@\n-  result->inc_use_count();\n+    RootChunkArea* rca = _root_chunk_area_lut.get_area_by_address(loc);\n@@ -461,1 +301,10 @@\n-  return result;\n+    \/\/ Create a root chunk header and initialize it;\n+    Metachunk* c = rca->alloc_root_chunk_header(this);\n+    assert(c->base() == loc && c->vsnode() == this &&\n+           c->is_free(), \"Sanity\");\n+    DEBUG_ONLY(c->verify();)\n+\n+    UL2(debug, \"new root chunk \" METACHUNK_FORMAT \".\", METACHUNK_FORMAT_ARGS(c));\n+    return c;\n+  }\n+  return NULL; \/\/ Node is full.\n@@ -464,0 +313,11 @@\n+\/\/ Given a chunk c, split it recursively until you get a chunk of the given target_level.\n+\/\/\n+\/\/ The resulting target chunk resides at the same address as the original chunk.\n+\/\/ The resulting splinters are added to freelists.\n+void VirtualSpaceNode::split(chunklevel_t target_level, Metachunk* c, FreeChunkListVector* freelists) {\n+  assert_lock_strong(MetaspaceExpand_lock);\n+  \/\/ Get the area associated with this chunk and let it handle the splitting\n+  RootChunkArea* rca = _root_chunk_area_lut.get_area_by_address(c->base());\n+  DEBUG_ONLY(rca->verify_area_is_ideally_merged();)\n+  rca->split(target_level, c, freelists);\n+}\n@@ -465,4 +325,12 @@\n-\/\/ Expand the virtual space (commit more of the reserved space)\n-bool VirtualSpaceNode::expand_by(size_t min_words, size_t preferred_words) {\n-  size_t min_bytes = min_words * BytesPerWord;\n-  size_t preferred_bytes = preferred_words * BytesPerWord;\n+\/\/ Given a chunk, attempt to merge it recursively with its neighboring chunks.\n+\/\/\n+\/\/ If successful (merged at least once), returns address of\n+\/\/ the merged chunk; NULL otherwise.\n+\/\/\n+\/\/ The merged chunks are removed from the freelists.\n+\/\/\n+\/\/ !!! Please note that if this method returns a non-NULL value, the\n+\/\/ original chunk will be invalid and should not be accessed anymore! !!!\n+Metachunk* VirtualSpaceNode::merge(Metachunk* c, FreeChunkListVector* freelists) {\n+  assert(c != NULL && c->is_free(), \"Sanity\");\n+  assert_lock_strong(MetaspaceExpand_lock);\n@@ -470,1 +338,6 @@\n-  size_t uncommitted = virtual_space()->reserved_size() - virtual_space()->actual_committed_size();\n+  \/\/ Get the rca associated with this chunk and let it handle the merging\n+  RootChunkArea* rca = _root_chunk_area_lut.get_area_by_address(c->base());\n+  Metachunk* c2 = rca->merge(c, freelists);\n+  DEBUG_ONLY(rca->verify_area_is_ideally_merged();)\n+  return c2;\n+}\n@@ -472,3 +345,12 @@\n-  if (uncommitted < min_bytes) {\n-    return false;\n-  }\n+\/\/ Given a chunk c, which must be \"in use\" and must not be a root chunk, attempt to\n+\/\/ enlarge it in place by claiming its trailing buddy.\n+\/\/\n+\/\/ This will only work if c is the leader of the buddy pair and the trailing buddy is free.\n+\/\/\n+\/\/ If successful, the follower chunk will be removed from the freelists, the leader chunk c will\n+\/\/ double in size (level decreased by one).\n+\/\/\n+\/\/ On success, true is returned, false otherwise.\n+bool VirtualSpaceNode::attempt_enlarge_chunk(Metachunk* c, FreeChunkListVector* freelists) {\n+  assert(c != NULL && c->is_in_use() && !c->is_root_chunk(), \"Sanity\");\n+  assert_lock_strong(MetaspaceExpand_lock);\n@@ -476,2 +358,2 @@\n-  size_t commit = MIN2(preferred_bytes, uncommitted);\n-  bool result = virtual_space()->expand_by(commit, false);\n+  \/\/ Get the rca associated with this chunk and let it handle the merging\n+  RootChunkArea* rca = _root_chunk_area_lut.get_area_by_address(c->base());\n@@ -479,7 +361,4 @@\n-  if (result) {\n-    log_trace(gc, metaspace, freelist)(\"Expanded %s virtual space list node by \" SIZE_FORMAT \" words.\",\n-        (is_class() ? \"class\" : \"non-class\"), commit);\n-    DEBUG_ONLY(Atomic::inc(&g_internal_statistics.num_committed_space_expanded));\n-  } else {\n-    log_trace(gc, metaspace, freelist)(\"Failed to expand %s virtual space list node by \" SIZE_FORMAT \" words.\",\n-        (is_class() ? \"class\" : \"non-class\"), commit);\n+  bool rc = rca->attempt_enlarge_chunk(c, freelists);\n+  DEBUG_ONLY(rca->verify_area_is_ideally_merged();)\n+  if (rc) {\n+    InternalStats::inc_num_chunks_enlarged();\n@@ -488,3 +367,1 @@\n-  assert(result, \"Failed to commit memory\");\n-\n-  return result;\n+  return rc;\n@@ -493,1 +370,8 @@\n-Metachunk* VirtualSpaceNode::get_chunk_vs(size_t chunk_word_size) {\n+\/\/ Attempts to purge the node:\n+\/\/\n+\/\/ If all chunks living in this node are free, they will all be removed from\n+\/\/  the freelist they currently reside in. Then, the node will be deleted.\n+\/\/\n+\/\/ Returns true if the node has been deleted, false if not.\n+\/\/ !! If this returns true, do not access the node from this point on. !!\n+bool VirtualSpaceNode::attempt_purge(FreeChunkListVector* freelists) {\n@@ -495,6 +379,3 @@\n-  Metachunk* result = take_from_committed(chunk_word_size);\n-  return result;\n-}\n-bool VirtualSpaceNode::initialize() {\n-\n-  if (!_rs.is_reserved()) {\n+  if (!_owns_rs) {\n+    \/\/ We do not allow purging of nodes if we do not own the\n+    \/\/ underlying ReservedSpace (CompressClassSpace case).\n@@ -505,16 +386,5 @@\n-  \/\/ These are necessary restriction to make sure that the virtual space always\n-  \/\/ grows in steps of Metaspace::commit_alignment(). If both base and size are\n-  \/\/ aligned only the middle alignment of the VirtualSpace is used.\n-  assert_is_aligned(_rs.base(), Metaspace::commit_alignment());\n-  assert_is_aligned(_rs.size(), Metaspace::commit_alignment());\n-\n-  \/\/ ReservedSpaces marked as special will have the entire memory\n-  \/\/ pre-committed. Setting a committed size will make sure that\n-  \/\/ committed_size and actual_committed_size agrees.\n-  size_t pre_committed_size = _rs.special() ? _rs.size() : 0;\n-\n-  bool result = virtual_space()->initialize_with_granularity(_rs, pre_committed_size,\n-      Metaspace::commit_alignment());\n-  if (result) {\n-    assert(virtual_space()->committed_size() == virtual_space()->actual_committed_size(),\n-        \"Checking that the pre-committed memory was registered by the VirtualSpace\");\n+  \/\/ First find out if all areas are empty. Since empty chunks collapse to root chunk\n+  \/\/ size, if all chunks in this node are free root chunks we are good to go.\n+  if (!_root_chunk_area_lut.is_free()) {\n+    return false;\n+  }\n@@ -522,1 +392,12 @@\n-    set_top((MetaWord*)virtual_space()->low());\n+  UL(debug, \": purging.\");\n+\n+  \/\/ Okay, we can purge. Before we can do this, we need to remove all chunks from the freelist.\n+  for (int narea = 0; narea < _root_chunk_area_lut.number_of_areas(); narea++) {\n+    RootChunkArea* ra = _root_chunk_area_lut.get_area_by_index(narea);\n+    Metachunk* c = ra->first_chunk();\n+    if (c != NULL) {\n+      UL2(trace, \"removing chunk from to-be-purged node: \"\n+          METACHUNK_FULL_FORMAT \".\", METACHUNK_FULL_FORMAT_ARGS(c));\n+      assert(c->is_free() && c->is_root_chunk(), \"Sanity\");\n+      freelists->remove(c);\n+    }\n@@ -525,3 +406,2 @@\n-  \/\/ Initialize Occupancy Map.\n-  const size_t smallest_chunk_size = is_class() ? ClassSpecializedChunk : SpecializedChunk;\n-  _occupancy_map = new OccupancyMap(bottom(), reserved_words(), smallest_chunk_size);\n+  \/\/ Now, delete the node, then right away return since this object is invalid.\n+  delete this;\n@@ -529,1 +409,1 @@\n-  return result;\n+  return true;\n@@ -532,5 +412,2 @@\n-void VirtualSpaceNode::print_on(outputStream* st, size_t scale) const {\n-  size_t used_words = used_words_in_vs();\n-  size_t commit_words = committed_words();\n-  size_t res_words = reserved_words();\n-  VirtualSpace* vs = virtual_space();\n+void VirtualSpaceNode::print_on(outputStream* st) const {\n+  size_t scale = K;\n@@ -538,1 +415,1 @@\n-  st->print(\"node @\" PTR_FORMAT \": \", p2i(this));\n+  st->print(\"base \" PTR_FORMAT \": \", p2i(base()));\n@@ -540,1 +417,1 @@\n-  print_scaled_words(st, res_words, scale);\n+  print_scaled_words(st, word_size(), scale);\n@@ -542,1 +419,1 @@\n-  print_scaled_words_and_percentage(st, commit_words, res_words, scale);\n+  print_scaled_words_and_percentage(st, committed_words(), word_size(), scale);\n@@ -544,1 +421,2 @@\n-  print_scaled_words_and_percentage(st, used_words, res_words, scale);\n+  print_scaled_words_and_percentage(st, used_words(), word_size(), scale);\n+\n@@ -546,4 +424,2 @@\n-  st->print(\"   [\" PTR_FORMAT \", \" PTR_FORMAT \", \"\n-      PTR_FORMAT \", \" PTR_FORMAT \")\",\n-      p2i(bottom()), p2i(top()), p2i(end()),\n-      p2i(vs->high_boundary()));\n+  _root_chunk_area_lut.print_on(st);\n+  _commit_mask.print_on(st);\n@@ -552,4 +428,4 @@\n-#ifdef ASSERT\n-void VirtualSpaceNode::mangle() {\n-  size_t word_size = capacity_words_in_vs();\n-  Copy::fill_to_words((HeapWord*) low(), word_size, 0xf1f1f1f1);\n+\/\/ Returns size, in words, of committed space in this node alone.\n+\/\/ Note: iterates over commit mask and hence may be a tad expensive on large nodes.\n+size_t VirtualSpaceNode::committed_words() const {\n+  return _commit_mask.get_committed_size();\n@@ -557,22 +433,27 @@\n-#endif \/\/ ASSERT\n-void VirtualSpaceNode::retire(ChunkManager* chunk_manager) {\n-  assert(is_class() == chunk_manager->is_class(), \"Wrong ChunkManager?\");\n-  verify(false);\n-  EVERY_NTH(VerifyMetaspaceInterval)\n-    verify(true);\n-  END_EVERY_NTH\n-#endif\n-  for (int i = (int)MediumIndex; i >= (int)ZeroIndex; --i) {\n-    ChunkIndex index = (ChunkIndex)i;\n-    size_t chunk_size = chunk_manager->size_by_index(index);\n-\n-    while (free_words_in_vs() >= chunk_size) {\n-      Metachunk* chunk = get_chunk_vs(chunk_size);\n-      \/\/ Chunk will be allocated aligned, so allocation may require\n-      \/\/ additional padding chunks. That may cause above allocation to\n-      \/\/ fail. Just ignore the failed allocation and continue with the\n-      \/\/ next smaller chunk size. As the VirtualSpaceNode comitted\n-      \/\/ size should be a multiple of the smallest chunk size, we\n-      \/\/ should always be able to fill the VirtualSpace completely.\n-      if (chunk == NULL) {\n-        break;\n+void VirtualSpaceNode::verify() const {\n+  MutexLocker fcl(MetaspaceExpand_lock, Mutex::_no_safepoint_check_flag);\n+  verify_locked();\n+}\n+\n+volatile int test_access = 0;\n+\n+\/\/ Verify counters and basic structure. Slow mode: verify all chunks in depth\n+void VirtualSpaceNode::verify_locked() const {\n+  assert_lock_strong(MetaspaceExpand_lock);\n+  assert(base() != NULL, \"Invalid base\");\n+  assert(base() == (MetaWord*)_rs.base() &&\n+         word_size() == _rs.size() \/ BytesPerWord,\n+         \"Sanity\");\n+  assert_is_aligned(base(), chunklevel::MAX_CHUNK_BYTE_SIZE);\n+  assert(used_words() <= word_size(), \"Sanity\");\n+  \/\/ Since we only ever hand out root chunks from a vsnode, top should always be aligned\n+  \/\/ to root chunk size.\n+  assert_is_aligned(used_words(), chunklevel::MAX_CHUNK_WORD_SIZE);\n+\n+  _commit_mask.verify();\n+\n+  \/\/ Verify memory against commit mask.\n+  SOMETIMES(\n+    for (MetaWord* p = base(); p < base() + used_words(); p += os::vm_page_size()) {\n+      if (_commit_mask.is_committed_address(p)) {\n+        test_access += *(int*)p;\n@@ -582,3 +463,5 @@\n-      chunk_manager->return_single_chunk(chunk);\n-  }\n-  assert(free_words_in_vs() == 0, \"should be empty now\");\n+  )\n+\n+  assert(committed_words() <= word_size(), \"Sanity\");\n+  assert_is_aligned(committed_words(), Settings::commit_granule_words());\n+  _root_chunk_area_lut.verify();\n@@ -588,0 +471,2 @@\n+#endif\n+\n","filename":"src\/hotspot\/share\/memory\/metaspace\/virtualSpaceNode.cpp","additions":352,"deletions":467,"binary":false,"changes":819,"status":"modified"},{"patch":"@@ -83,0 +83,24 @@\n+\/\/ Helper method\n+static char* attempt_map_or_reserve_memory_at(char* base, size_t size, int fd) {\n+  if (fd != -1) {\n+    return os::attempt_map_memory_to_file_at(base, size, fd);\n+  }\n+  return os::attempt_reserve_memory_at(base, size);\n+}\n+\n+\/\/ Helper method\n+static char* map_or_reserve_memory(size_t size, int fd) {\n+  if (fd != -1) {\n+    return os::map_memory_to_file(size, fd);\n+  }\n+  return os::reserve_memory(size);\n+}\n+\n+\/\/ Helper method\n+static char* map_or_reserve_memory_aligned(size_t size, size_t alignment, int fd) {\n+  if (fd != -1) {\n+    return os::map_memory_to_file_aligned(size, alignment, fd);\n+  }\n+  return os::reserve_memory_aligned(size, alignment);\n+}\n+\n@@ -191,1 +215,1 @@\n-      base = os::attempt_reserve_memory_at(requested_address, size, _fd_for_heap);\n+      base = attempt_map_or_reserve_memory_at(requested_address, size, _fd_for_heap);\n@@ -197,1 +221,5 @@\n-      base = os::reserve_memory_with_fd(size, _fd_for_heap, _executable);\n+      if (_executable) {\n+        base = os::reserve_memory(size, ExecMem);\n+      } else {\n+        base = map_or_reserve_memory(size, _fd_for_heap);\n+      }\n@@ -209,1 +237,1 @@\n-      base = os::reserve_memory_aligned(size, alignment, _fd_for_heap);\n+      base = map_or_reserve_memory_aligned(size, alignment, _fd_for_heap);\n@@ -375,1 +403,1 @@\n-      base = os::attempt_reserve_memory_at(requested_address, size, _fd_for_heap);\n+      base = attempt_map_or_reserve_memory_at(requested_address, size, _fd_for_heap);\n@@ -381,1 +409,1 @@\n-      base = os::reserve_memory_with_fd(size, _fd_for_heap);\n+      base = map_or_reserve_memory(size, _fd_for_heap);\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":33,"deletions":5,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -964,1 +964,1 @@\n-const TypeFunc* OptoRuntime::digestBase_implCompress_Type() {\n+const TypeFunc* OptoRuntime::digestBase_implCompress_Type(bool is_sha3) {\n@@ -966,1 +966,1 @@\n-  int num_args = 2;\n+  int num_args = is_sha3 ? 3 : 2;\n@@ -972,0 +972,1 @@\n+  if (is_sha3) fields[argp++] = TypeInt::INT; \/\/ digest_length\n@@ -985,1 +986,1 @@\n-const TypeFunc* OptoRuntime::digestBase_implCompressMB_Type() {\n+const TypeFunc* OptoRuntime::digestBase_implCompressMB_Type(bool is_sha3) {\n@@ -987,1 +988,1 @@\n-  int num_args = 4;\n+  int num_args = is_sha3 ? 5 : 4;\n@@ -993,0 +994,1 @@\n+  if (is_sha3) fields[argp++] = TypeInt::INT; \/\/ digest_length\n@@ -1211,54 +1213,0 @@\n-\/\/-------------- methodData update helpers\n-\n-const TypeFunc* OptoRuntime::profile_receiver_type_Type() {\n-  \/\/ create input type (domain)\n-  const Type **fields = TypeTuple::fields(2);\n-  fields[TypeFunc::Parms+0] = TypeAryPtr::NOTNULL;    \/\/ methodData pointer\n-  fields[TypeFunc::Parms+1] = TypeInstPtr::BOTTOM;    \/\/ receiver oop\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2, fields);\n-\n-  \/\/ create result type\n-  fields = TypeTuple::fields(1);\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n-  const TypeTuple *range = TypeTuple::make(TypeFunc::Parms, fields);\n-  return TypeFunc::make(domain,range);\n-}\n-\n-JRT_LEAF(void, OptoRuntime::profile_receiver_type_C(DataLayout* data, oopDesc* receiver))\n-  if (receiver == NULL) return;\n-  Klass* receiver_klass = receiver->klass();\n-\n-  intptr_t* mdp = ((intptr_t*)(data)) + DataLayout::header_size_in_cells();\n-  int empty_row = -1;           \/\/ free row, if any is encountered\n-\n-  \/\/ ReceiverTypeData* vc = new ReceiverTypeData(mdp);\n-  for (uint row = 0; row < ReceiverTypeData::row_limit(); row++) {\n-    \/\/ if (vc->receiver(row) == receiver_klass)\n-    int receiver_off = ReceiverTypeData::receiver_cell_index(row);\n-    intptr_t row_recv = *(mdp + receiver_off);\n-    if (row_recv == (intptr_t) receiver_klass) {\n-      \/\/ vc->set_receiver_count(row, vc->receiver_count(row) + DataLayout::counter_increment);\n-      int count_off = ReceiverTypeData::receiver_count_cell_index(row);\n-      *(mdp + count_off) += DataLayout::counter_increment;\n-      return;\n-    } else if (row_recv == 0) {\n-      \/\/ else if (vc->receiver(row) == NULL)\n-      empty_row = (int) row;\n-    }\n-  }\n-\n-  if (empty_row != -1) {\n-    int receiver_off = ReceiverTypeData::receiver_cell_index(empty_row);\n-    \/\/ vc->set_receiver(empty_row, receiver_klass);\n-    *(mdp + receiver_off) = (intptr_t) receiver_klass;\n-    \/\/ vc->set_receiver_count(empty_row, DataLayout::counter_increment);\n-    int count_off = ReceiverTypeData::receiver_count_cell_index(empty_row);\n-    *(mdp + count_off) = DataLayout::counter_increment;\n-  } else {\n-    \/\/ Receiver did not match any saved receiver and there is no empty row for it.\n-    \/\/ Increment total counter to indicate polymorphic case.\n-    intptr_t* count_p = (intptr_t*)(((uint8_t*)(data)) + in_bytes(CounterData::count_offset()));\n-    *count_p += DataLayout::counter_increment;\n-  }\n-JRT_END\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":6,"deletions":58,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -28,0 +28,2 @@\n+#include \"classfile\/classListParser.hpp\"\n+#include \"classfile\/classListWriter.hpp\"\n@@ -3440,0 +3442,18 @@\n+JVM_ENTRY(jboolean, JVM_ReferenceRefersTo(JNIEnv* env, jobject ref, jobject o))\n+  JVMWrapper(\"JVM_ReferenceRefersTo\");\n+  oop ref_oop = JNIHandles::resolve_non_null(ref);\n+  oop referent = java_lang_ref_Reference::weak_referent_no_keepalive(ref_oop);\n+  return referent == JNIHandles::resolve(o);\n+JVM_END\n+\n+\n+\/\/ java.lang.ref.PhantomReference \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+\n+JVM_ENTRY(jboolean, JVM_PhantomReferenceRefersTo(JNIEnv* env, jobject ref, jobject o))\n+  JVMWrapper(\"JVM_PhantomReferenceRefersTo\");\n+  oop ref_oop = JNIHandles::resolve_non_null(ref);\n+  oop referent = java_lang_ref_Reference::phantom_referent_no_keepalive(ref_oop);\n+  return referent == JNIHandles::resolve(o);\n+JVM_END\n+\n@@ -3745,1 +3765,1 @@\n-  if (!DynamicDumpSharedSpaces) {\n+  if (!Arguments::is_dumping_archive()) {\n@@ -3780,1 +3800,1 @@\n-                                                 method_type, m, instantiated_method_type);\n+                                                 method_type, m, instantiated_method_type, THREAD);\n@@ -3794,3 +3814,0 @@\n-  if (!DynamicArchive::is_mapped()) {\n-    return NULL;\n-  }\n@@ -3837,3 +3854,3 @@\n-JVM_ENTRY(jboolean, JVM_IsDynamicDumpingEnabled(JNIEnv* env))\n-    JVMWrapper(\"JVM_IsDynamicDumpingEnable\");\n-    return DynamicDumpSharedSpaces;\n+JVM_ENTRY(jboolean, JVM_IsCDSDumpingEnabled(JNIEnv* env))\n+    JVMWrapper(\"JVM_IsCDSDumpingEnabled\");\n+    return Arguments::is_dumping_archive();\n@@ -3870,0 +3887,23 @@\n+JVM_ENTRY(jboolean, JVM_IsDumpingClassList(JNIEnv *env))\n+  JVMWrapper(\"JVM_IsDumpingClassList\");\n+#if INCLUDE_CDS\n+  return ClassListWriter::is_enabled();\n+#else\n+  return false;\n+#endif \/\/ INCLUDE_CDS\n+JVM_END\n+\n+JVM_ENTRY(void, JVM_LogLambdaFormInvoker(JNIEnv *env, jstring line))\n+  JVMWrapper(\"JVM_LogLambdaFormInvoker\");\n+#if INCLUDE_CDS\n+  assert(ClassListWriter::is_enabled(), \"Should be set and open\");\n+  if (line != NULL) {\n+    ResourceMark rm(THREAD);\n+    Handle h_line (THREAD, JNIHandles::resolve_non_null(line));\n+    char* c_line = java_lang_String::as_utf8_string(h_line());\n+    ClassListWriter w;\n+    w.stream()->print_cr(\"%s %s\", LAMBDA_FORM_TAG, c_line);\n+  }\n+#endif \/\/ INCLUDE_CDS\n+JVM_END\n+\n","filename":"src\/hotspot\/share\/prims\/jvm.cpp","additions":48,"deletions":8,"binary":false,"changes":56,"status":"modified"},{"patch":"@@ -1210,0 +1210,5 @@\n+  EscapeBarrier eb(true, calling_thread, java_thread);\n+  if (!eb.deoptimize_objects(MaxJavaStackTraceDepth)) {\n+    return JVMTI_ERROR_OUT_OF_MEMORY;\n+  }\n+\n@@ -1255,0 +1260,5 @@\n+  EscapeBarrier eb(true, calling_thread, java_thread);\n+  if (!eb.deoptimize_objects(MaxJavaStackTraceDepth)) {\n+    return JVMTI_ERROR_OUT_OF_MEMORY;\n+  }\n+\n@@ -1638,4 +1648,0 @@\n-  JavaThread* current_thread  = JavaThread::current();\n-  HandleMark hm(current_thread);\n-  uint32_t debug_bits = 0;\n-\n@@ -1648,20 +1654,6 @@\n-  \/\/ Check if java_thread is fully suspended\n-  if (!java_thread->is_thread_fully_suspended(true \/* wait for suspend completion *\/, &debug_bits)) {\n-    return JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n-  }\n-  \/\/ Check to see if a PopFrame was already in progress\n-  if (java_thread->popframe_condition() != JavaThread::popframe_inactive) {\n-    \/\/ Probably possible for JVMTI clients to trigger this, but the\n-    \/\/ JPDA backend shouldn't allow this to happen\n-    return JVMTI_ERROR_INTERNAL;\n-  }\n-\n-  {\n-    \/\/ Was workaround bug\n-    \/\/    4812902: popFrame hangs if the method is waiting at a synchronize\n-    \/\/ Catch this condition and return an error to avoid hanging.\n-    \/\/ Now JVMTI spec allows an implementation to bail out with an opaque frame error.\n-    OSThread* osThread = java_thread->osthread();\n-    if (osThread->get_state() == MONITOR_WAIT) {\n-      return JVMTI_ERROR_OPAQUE_FRAME;\n-    }\n+  \/\/ Eagerly reallocate scalar replaced objects.\n+  JavaThread* current_thread = JavaThread::current();\n+  EscapeBarrier eb(true, current_thread, java_thread);\n+  if (!eb.deoptimize_objects(1)) {\n+    \/\/ Reallocation of scalar replaced objects failed -> return with error\n+    return JVMTI_ERROR_OUT_OF_MEMORY;\n@@ -1670,66 +1662,6 @@\n-  {\n-    ResourceMark rm(current_thread);\n-    \/\/ Check if there are more than one Java frame in this thread, that the top two frames\n-    \/\/ are Java (not native) frames, and that there is no intervening VM frame\n-    int frame_count = 0;\n-    bool is_interpreted[2];\n-    intptr_t *frame_sp[2];\n-    \/\/ The 2-nd arg of constructor is needed to stop iterating at java entry frame.\n-    for (vframeStream vfs(java_thread, true); !vfs.at_end(); vfs.next()) {\n-      methodHandle mh(current_thread, vfs.method());\n-      if (mh->is_native()) return(JVMTI_ERROR_OPAQUE_FRAME);\n-      is_interpreted[frame_count] = vfs.is_interpreted_frame();\n-      frame_sp[frame_count] = vfs.frame_id();\n-      if (++frame_count > 1) break;\n-    }\n-    if (frame_count < 2)  {\n-      \/\/ We haven't found two adjacent non-native Java frames on the top.\n-      \/\/ There can be two situations here:\n-      \/\/  1. There are no more java frames\n-      \/\/  2. Two top java frames are separated by non-java native frames\n-      if(vframeFor(java_thread, 1) == NULL) {\n-        return JVMTI_ERROR_NO_MORE_FRAMES;\n-      } else {\n-        \/\/ Intervening non-java native or VM frames separate java frames.\n-        \/\/ Current implementation does not support this. See bug #5031735.\n-        \/\/ In theory it is possible to pop frames in such cases.\n-        return JVMTI_ERROR_OPAQUE_FRAME;\n-      }\n-    }\n-\n-    \/\/ If any of the top 2 frames is a compiled one, need to deoptimize it\n-    for (int i = 0; i < 2; i++) {\n-      if (!is_interpreted[i]) {\n-        Deoptimization::deoptimize_frame(java_thread, frame_sp[i]);\n-      }\n-    }\n-\n-    \/\/ Update the thread state to reflect that the top frame is popped\n-    \/\/ so that cur_stack_depth is maintained properly and all frameIDs\n-    \/\/ are invalidated.\n-    \/\/ The current frame will be popped later when the suspended thread\n-    \/\/ is resumed and right before returning from VM to Java.\n-    \/\/ (see call_VM_base() in assembler_<cpu>.cpp).\n-\n-    \/\/ It's fine to update the thread state here because no JVMTI events\n-    \/\/ shall be posted for this PopFrame.\n-\n-    \/\/ It is only safe to perform the direct operation on the current\n-    \/\/ thread. All other usage needs to use a handshake for safety.\n-    {\n-      MutexLocker mu(JvmtiThreadState_lock);\n-      if (java_thread == JavaThread::current()) {\n-        state->update_for_pop_top_frame();\n-      } else {\n-        UpdateForPopTopFrameClosure op(state);\n-        Handshake::execute(&op, java_thread);\n-        if (op.result() != JVMTI_ERROR_NONE) {\n-          return op.result();\n-        }\n-      }\n-    }\n-\n-    java_thread->set_popframe_condition(JavaThread::popframe_pending_bit);\n-    \/\/ Set pending step flag for this popframe and it is cleared when next\n-    \/\/ step event is posted.\n-    state->set_pending_step_for_popframe();\n+  MutexLocker mu(JvmtiThreadState_lock);\n+  UpdateForPopTopFrameClosure op(state);\n+  if (java_thread == current_thread) {\n+    op.doit(java_thread, true \/* self *\/);\n+  } else {\n+    Handshake::execute(&op, java_thread);\n@@ -1737,2 +1669,1 @@\n-\n-  return JVMTI_ERROR_NONE;\n+  return op.result();\n@@ -1772,4 +1703,0 @@\n-  jvmtiError err = JVMTI_ERROR_NONE;\n-  ResourceMark rm;\n-  uint32_t debug_bits = 0;\n-\n@@ -1781,21 +1708,1 @@\n-  if (!java_thread->is_thread_fully_suspended(true, &debug_bits)) {\n-    return JVMTI_ERROR_THREAD_NOT_SUSPENDED;\n-  }\n-\n-  if (TraceJVMTICalls) {\n-    JvmtiSuspendControl::print();\n-  }\n-\n-  vframe *vf = vframeFor(java_thread, depth);\n-  if (vf == NULL) {\n-    return JVMTI_ERROR_NO_MORE_FRAMES;\n-  }\n-\n-  if (!vf->is_java_frame() || ((javaVFrame*) vf)->method()->is_native()) {\n-    return JVMTI_ERROR_OPAQUE_FRAME;\n-  }\n-\n-  assert(vf->frame_pointer() != NULL, \"frame pointer mustn't be NULL\");\n-\n-  \/\/ It is only safe to perform the direct operation on the current\n-  \/\/ thread. All other usage needs to use a vm-safepoint-op for safety.\n+  SetFramePopClosure op(this, state, depth);\n@@ -1804,2 +1711,1 @@\n-    int frame_number = state->count_frames() - depth;\n-    state->env_thread_state(this)->set_frame_pop(frame_number);\n+    op.doit(java_thread, true \/* self *\/);\n@@ -1807,3 +1713,1 @@\n-    SetFramePopClosure op(this, state, depth);\n-    err = op.result();\n-  return err;\n+  return op.result();\n","filename":"src\/hotspot\/share\/prims\/jvmtiEnv.cpp","additions":26,"deletions":122,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -1567,1 +1567,1 @@\n-void JvmtiExport::post_method_exit(JavaThread *thread, Method* method, frame current_frame) {\n+void JvmtiExport::post_method_exit(JavaThread* thread, Method* method, frame current_frame) {\n@@ -1571,5 +1571,1 @@\n-  EVT_TRIG_TRACE(JVMTI_EVENT_METHOD_EXIT, (\"[%s] Trg Method Exit triggered %s.%s\",\n-                     JvmtiTrace::safe_get_thread_name(thread),\n-                     (mh() == NULL) ? \"NULL\" : mh()->klass_name()->as_C_string(),\n-                     (mh() == NULL) ? \"NULL\" : mh()->name()->as_C_string() ));\n-\n+\n@@ -1585,1 +1581,3 @@\n-\n+  Handle result;\n+  jvalue value;\n+  value.j = 0L;\n@@ -1588,4 +1586,0 @@\n-    Handle result;\n-    jvalue value;\n-    value.j = 0L;\n-\n@@ -1601,0 +1595,1 @@\n+        value.l = JNIHandles::make_local(thread, result());\n@@ -1603,0 +1598,1 @@\n+  }\n@@ -1604,0 +1600,25 @@\n+  \/\/ Deferred transition to VM, so we can stash away the return oop before GC\n+  \/\/ Note that this transition is not needed when throwing an exception, because\n+  \/\/ there is no oop to retain.\n+  JRT_BLOCK\n+    post_method_exit_inner(thread, mh, state, exception_exit, current_frame, value);\n+  JRT_BLOCK_END\n+\n+  if (result.not_null() && !mh->is_native()) {\n+    \/\/ We have to restore the oop on the stack for interpreter frames\n+    *(oop*)current_frame.interpreter_frame_tos_address() = result();\n+  }\n+}\n+\n+void JvmtiExport::post_method_exit_inner(JavaThread* thread,\n+                                         methodHandle& mh,\n+                                         JvmtiThreadState *state,\n+                                         bool exception_exit,\n+                                         frame current_frame,\n+                                         jvalue& value) {\n+  EVT_TRIG_TRACE(JVMTI_EVENT_METHOD_EXIT, (\"[%s] Trg Method Exit triggered %s.%s\",\n+                                           JvmtiTrace::safe_get_thread_name(thread),\n+                                           (mh() == NULL) ? \"NULL\" : mh()->klass_name()->as_C_string(),\n+                                           (mh() == NULL) ? \"NULL\" : mh()->name()->as_C_string() ));\n+\n+  if (state->is_enabled(JVMTI_EVENT_METHOD_EXIT)) {\n@@ -1614,3 +1635,0 @@\n-        if (result.not_null()) {\n-          value.l = JNIHandles::make_local(thread, result());\n-        }\n@@ -1808,1 +1826,3 @@\n-        JvmtiExport::post_method_exit(thread, method, thread->last_frame());\n+        jvalue no_value;\n+        no_value.j = 0L;\n+        JvmtiExport::post_method_exit_inner(thread, mh, state, true, thread->last_frame(), no_value);\n","filename":"src\/hotspot\/share\/prims\/jvmtiExport.cpp","additions":35,"deletions":15,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -430,0 +430,1 @@\n+  , _eb(false, NULL, NULL)\n@@ -444,0 +445,1 @@\n+  , _eb(type == T_OBJECT, JavaThread::current(), thread)\n@@ -457,0 +459,1 @@\n+  , _eb(true, calling_thread, thread)\n@@ -629,0 +632,10 @@\n+bool VM_GetOrSetLocal::doit_prologue() {\n+  if (!_eb.deoptimize_objects(_depth, _depth)) {\n+    \/\/ The target frame is affected by a reallocation failure.\n+    _result = JVMTI_ERROR_OUT_OF_MEMORY;\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n@@ -630,1 +643,1 @@\n-  _jvf = get_java_vframe();\n+  _jvf = _jvf == NULL ? get_java_vframe() : _jvf;\n","filename":"src\/hotspot\/share\/prims\/jvmtiImpl.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -462,1 +462,1 @@\n-  vmSymbols::SID name_id = vmSymbols::find_sid(name);\n+  vmSymbolID name_id = vmSymbols::find_sid(name);\n@@ -465,1 +465,1 @@\n-  case vmSymbols::VM_SYMBOL_ENUM_NAME(invoke_name):           return vmIntrinsics::_invokeGeneric;\n+  case VM_SYMBOL_ENUM_NAME(invoke_name):           return vmIntrinsics::_invokeGeneric;\n@@ -467,1 +467,1 @@\n-  case vmSymbols::VM_SYMBOL_ENUM_NAME(invokeBasic_name):      return vmIntrinsics::_invokeBasic;\n+  case VM_SYMBOL_ENUM_NAME(invokeBasic_name):      return vmIntrinsics::_invokeBasic;\n@@ -470,4 +470,4 @@\n-  case vmSymbols::VM_SYMBOL_ENUM_NAME(linkToVirtual_name):    return vmIntrinsics::_linkToVirtual;\n-  case vmSymbols::VM_SYMBOL_ENUM_NAME(linkToStatic_name):     return vmIntrinsics::_linkToStatic;\n-  case vmSymbols::VM_SYMBOL_ENUM_NAME(linkToSpecial_name):    return vmIntrinsics::_linkToSpecial;\n-  case vmSymbols::VM_SYMBOL_ENUM_NAME(linkToInterface_name):  return vmIntrinsics::_linkToInterface;\n+  case VM_SYMBOL_ENUM_NAME(linkToVirtual_name):    return vmIntrinsics::_linkToVirtual;\n+  case VM_SYMBOL_ENUM_NAME(linkToStatic_name):     return vmIntrinsics::_linkToStatic;\n+  case VM_SYMBOL_ENUM_NAME(linkToSpecial_name):    return vmIntrinsics::_linkToSpecial;\n+  case VM_SYMBOL_ENUM_NAME(linkToInterface_name):  return vmIntrinsics::_linkToInterface;\n","filename":"src\/hotspot\/share\/prims\/methodHandles.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -54,1 +54,69 @@\n-static void mangle_name_on(outputStream* st, Symbol* name, int begin, int end) {\n+\/*\n+\n+The JNI specification defines the mapping from a Java native method name to\n+a C native library implementation function name as follows:\n+\n+  The mapping produces a native method name by concatenating the following components\n+  derived from a `native` method declaration:\n+\n+  1. the prefix Java_\n+  2. given the binary name, in internal form, of the class which declares the native method:\n+     the result of escaping the name.\n+  3. an underscore (\"_\")\n+  4. the escaped method name\n+  5. if the native method declaration is overloaded: two underscores (\"__\") followed by the\n+   escaped parameter descriptor (JVMS 4.3.3) of the method declaration.\n+\n+  Escaping leaves every alphanumeric ASCII character (A-Za-z0-9) unchanged, and replaces each\n+  UTF-16 code unit n the table below with the corresponding escape sequence. If the name to be\n+  escaped contains a surrogate pair, then the high-surrogate code unit and the low-surrogate code\n+  unit are escaped separately. The result of escaping is a string consisting only of the ASCII\n+  characters A-Za-z0-9 and underscore.\n+\n+  ------------------------------                  ------------------------------------\n+  UTF-16 code unit                                Escape sequence\n+  ------------------------------                  ------------------------------------\n+  Forward slash (\/, U+002F)                       _\n+  Underscore (_, U+005F)                          _1\n+  Semicolon (;, U+003B)                           _2\n+  Left square bracket ([, U+005B)                 _3\n+  Any UTF-16 code unit \\u_WXYZ_ that does not     _0wxyz where w, x, y, and z are the lower-case\n+  represent alphanumeric ASCII (A-Za-z0-9),       forms of the hexadecimal digits W, X, Y, and Z.\n+  forward slash, underscore, semicolon,           (For example, U+ABCD becomes _0abcd.)\n+  or left square bracket\n+  ------------------------------                  ------------------------------------\n+\n+  Note that escape sequences can safely begin _0, _1, etc, because class and method\n+  names in Java source code never begin with a number. However, that is not the case in\n+  class files that were not generated from Java source code.\n+\n+  To preserve the 1:1 mapping to a native method name, the VM checks the resulting name as\n+  follows. If the process of escaping any precursor string from the native  method declaration\n+  (class or method name, or argument type) causes a \"0\", \"1\", \"2\", or \"3\" character\n+  from the precursor string to appear unchanged in the result *either* immediately after an\n+  underscore *or* at the beginning of the escaped string (where it will follow an underscore\n+  in the fully assembled name), then the escaping process is said to have \"failed\".\n+  In such cases, no native library search is performed, and the attempt to link the native\n+  method invocation will throw UnsatisfiedLinkError.\n+\n+\n+For example:\n+\n+  package\/my_class\/method\n+\n+and\n+\n+  package\/my\/1class\/method\n+\n+both map to\n+\n+  Java_package_my_1class_method\n+\n+To address this potential conflict we need only check if the character after\n+\/ is a digit 0..3, or if the first character after an injected '_' seperator\n+is a digit 0..3. If we encounter an invalid identifier we reset the\n+stringStream and return false. Otherwise the stringStream contains the mapped\n+name and we return true.\n+\n+*\/\n+static bool map_escaped_name_on(stringStream* st, Symbol* name, int begin, int end) {\n@@ -57,0 +125,1 @@\n+  bool check_escape_char = true; \/\/ initially true as first character here follows '_'\n@@ -61,0 +130,11 @@\n+      if (check_escape_char && (c >= '0' && c <= '3')) {\n+        \/\/ This is a non-Java identifier and we won't escape it to\n+        \/\/ ensure no name collisions with a Java identifier.\n+        if (log_is_enabled(Debug, jni, resolve)) {\n+          ResourceMark rm;\n+          log_debug(jni, resolve)(\"[Lookup of native method with non-Java identifier rejected: %s]\",\n+                                  name->as_C_string());\n+        }\n+        st->reset();  \/\/ restore to \"\" on error\n+        return false;\n+      }\n@@ -62,0 +142,1 @@\n+      check_escape_char = false;\n@@ -63,2 +144,7 @@\n-           if (c == '_') st->print(\"_1\");\n-      else if (c == '\/') st->print(\"_\");\n+      check_escape_char = false;\n+      if (c == '_') st->print(\"_1\");\n+      else if (c == '\/') {\n+        st->print(\"_\");\n+        \/\/ Following a \/ we must have non-escape character\n+        check_escape_char = true;\n+      }\n@@ -70,0 +156,1 @@\n+  return true;\n@@ -73,2 +160,2 @@\n-static void mangle_name_on(outputStream* st, Symbol* name) {\n-  mangle_name_on(st, name, 0, name->utf8_length());\n+static bool map_escaped_name_on(stringStream* st, Symbol* name) {\n+  return map_escaped_name_on(st, name, 0, name->utf8_length());\n@@ -83,1 +170,3 @@\n-  mangle_name_on(&st, method->klass_name());\n+  if (!map_escaped_name_on(&st, method->klass_name())) {\n+    return NULL;\n+  }\n@@ -86,1 +175,3 @@\n-  mangle_name_on(&st, method->name());\n+  if (!map_escaped_name_on(&st, method->name())) {\n+    return NULL;\n+  }\n@@ -96,1 +187,3 @@\n-  mangle_name_on(&st, method->klass_name());\n+  if (!map_escaped_name_on(&st, method->klass_name())) {\n+    return NULL;\n+  }\n@@ -99,1 +192,3 @@\n-  mangle_name_on(&st, method->name());\n+  if (!map_escaped_name_on(&st, method->name())) {\n+    return NULL;\n+  }\n@@ -105,1 +200,1 @@\n-  \/\/ Signature ignore the wrapping parenteses and the trailing return type\n+  \/\/ Signatures ignore the wrapping parentheses and the trailing return type\n@@ -113,1 +208,4 @@\n-  mangle_name_on(&st, signature, 1, end);\n+  if (!map_escaped_name_on(&st, signature, 1, end)) {\n+    return NULL;\n+  }\n+\n@@ -121,0 +219,1 @@\n+  void JNICALL JVM_RegisterVectorSupportMethods(JNIEnv *env, jclass vsclass);\n@@ -135,0 +234,1 @@\n+  { CC\"Java_jdk_internal_vm_vector_VectorSupport_registerNatives\", NULL, FN_PTR(JVM_RegisterVectorSupportMethods)},\n@@ -234,0 +334,5 @@\n+  if (pure_name == NULL) {\n+    \/\/ JNI name mapping rejected this method so return\n+    \/\/ NULL to indicate UnsatisfiedLinkError should be thrown.\n+    return NULL;\n+  }\n@@ -246,0 +351,5 @@\n+  if (long_name == NULL) {\n+    \/\/ JNI name mapping rejected this method so return\n+    \/\/ NULL to indicate UnsatisfiedLinkError should be thrown.\n+    return NULL;\n+  }\n@@ -328,0 +438,5 @@\n+  if (critical_name == NULL) {\n+    \/\/ JNI name mapping rejected this method so return\n+    \/\/ NULL to indicate UnsatisfiedLinkError should be thrown.\n+    return NULL;\n+  }\n@@ -336,0 +451,5 @@\n+  if (long_name == NULL) {\n+    \/\/ JNI name mapping rejected this method so return\n+    \/\/ NULL to indicate UnsatisfiedLinkError should be thrown.\n+    return NULL;\n+  }\n","filename":"src\/hotspot\/share\/prims\/nativeLookup.cpp","additions":131,"deletions":11,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -49,0 +49,1 @@\n+#include \"memory\/metaspace\/testHelpers.hpp\"\n@@ -81,0 +82,1 @@\n+#include \"runtime\/vframe.hpp\"\n@@ -88,0 +90,1 @@\n+#include \"utilities\/ostream.hpp\"\n@@ -384,0 +387,5 @@\n+#endif\n+#if INCLUDE_SHENANDOAHGC\n+  if (UseShenandoahGC) {\n+    return Universe::heap()->is_in(p);\n+  }\n@@ -884,0 +892,13 @@\n+WB_ENTRY(jboolean, WB_IsFrameDeoptimized(JNIEnv* env, jobject o, jint depth))\n+  bool result = false;\n+  if (thread->has_last_Java_frame()) {\n+    RegisterMap reg_map(thread);\n+    javaVFrame *jvf = thread->last_java_vframe(&reg_map);\n+    for (jint d = 0; d < depth && jvf != NULL; d++) {\n+      jvf = jvf->java_sender();\n+    }\n+    result = jvf != NULL && jvf->fr().is_deoptimized_frame();\n+  }\n+  return result;\n+WB_END\n+\n@@ -1235,1 +1256,1 @@\n-  JVMFlag::Error result = JVMFlagAccess::set<T, type_enum>(flag, value, JVMFlag::INTERNAL);\n+  JVMFlag::Error result = JVMFlagAccess::set<T, type_enum>(flag, value, JVMFlagOrigin::INTERNAL);\n@@ -1716,0 +1737,59 @@\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\/\/ MetaspaceTestContext and MetaspaceTestArena\n+WB_ENTRY(jlong, WB_CreateMetaspaceTestContext(JNIEnv* env, jobject wb, jlong commit_limit, jlong reserve_limit))\n+  metaspace::MetaspaceTestContext* context =\n+      new metaspace::MetaspaceTestContext(\"whitebox-metaspace-context\", (size_t) commit_limit, (size_t) reserve_limit);\n+  return (jlong)p2i(context);\n+WB_END\n+\n+WB_ENTRY(void, WB_DestroyMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  delete (metaspace::MetaspaceTestContext*) context;\n+WB_END\n+\n+WB_ENTRY(void, WB_PurgeMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  context0->purge_area();\n+WB_END\n+\n+WB_ENTRY(void, WB_PrintMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  context0->print_on(tty);\n+WB_END\n+\n+WB_ENTRY(jlong, WB_GetTotalCommittedWordsInMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  return context0->committed_words();\n+WB_END\n+\n+WB_ENTRY(jlong, WB_GetTotalUsedWordsInMetaspaceTestContext(JNIEnv* env, jobject wb, jlong context))\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  return context0->used_words();\n+WB_END\n+\n+WB_ENTRY(jlong, WB_CreateArenaInTestContext(JNIEnv* env, jobject wb, jlong context, jboolean is_micro))\n+  const Metaspace::MetaspaceType type = is_micro ? Metaspace::ReflectionMetaspaceType : Metaspace::StandardMetaspaceType;\n+  metaspace::MetaspaceTestContext* context0 = (metaspace::MetaspaceTestContext*) context;\n+  return (jlong)p2i(context0->create_arena(type));\n+WB_END\n+\n+WB_ENTRY(void, WB_DestroyMetaspaceTestArena(JNIEnv* env, jobject wb, jlong arena))\n+  delete (metaspace::MetaspaceTestArena*) arena;\n+WB_END\n+\n+WB_ENTRY(jlong, WB_AllocateFromMetaspaceTestArena(JNIEnv* env, jobject wb, jlong arena, jlong word_size))\n+  metaspace::MetaspaceTestArena* arena0 = (metaspace::MetaspaceTestArena*) arena;\n+  MetaWord* p = arena0->allocate((size_t) word_size);\n+  return (jlong)p2i(p);\n+WB_END\n+\n+WB_ENTRY(void, WB_DeallocateToMetaspaceTestArena(JNIEnv* env, jobject wb, jlong arena, jlong p, jlong word_size))\n+  metaspace::MetaspaceTestArena* arena0 = (metaspace::MetaspaceTestArena*) arena;\n+  arena0->deallocate((MetaWord*)p, (size_t) word_size);\n+WB_END\n+\n+WB_ENTRY(jlong, WB_GetMaxMetaspaceAllocationSize(JNIEnv* env, jobject wb))\n+  return (jlong) Metaspace::max_allocation_word_size() * BytesPerWord;\n+WB_END\n+\n+\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n@@ -1732,9 +1812,0 @@\n-WB_ENTRY(void, WB_FreeMetaspace(JNIEnv* env, jobject wb, jobject class_loader, jlong addr, jlong size))\n-  oop class_loader_oop = JNIHandles::resolve(class_loader);\n-  ClassLoaderData* cld = class_loader_oop != NULL\n-      ? java_lang_ClassLoader::loader_data_acquire(class_loader_oop)\n-      : ClassLoaderData::the_null_class_loader_data();\n-\n-  MetadataFactory::free_array(cld, (Array<u1>*)(uintptr_t)addr);\n-WB_END\n-\n@@ -2328,0 +2399,7 @@\n+WB_ENTRY(jstring, WB_GetLibcName(JNIEnv* env, jobject o))\n+  ThreadToNativeFromVM ttn(thread);\n+  jstring info_string = env->NewStringUTF(XSTR(LIBC));\n+  CHECK_JNI_EXCEPTION_(env, NULL);\n+  return info_string;\n+WB_END\n+\n@@ -2401,0 +2479,1 @@\n+  {CC\"isFrameDeoptimized\", CC\"(I)Z\",                  (void*)&WB_IsFrameDeoptimized},\n@@ -2481,2 +2560,0 @@\n-  {CC\"freeMetaspace\",\n-     CC\"(Ljava\/lang\/ClassLoader;JJ)V\",                (void*)&WB_FreeMetaspace },\n@@ -2578,0 +2655,13 @@\n+\n+  {CC\"createMetaspaceTestContext\", CC\"(JJ)J\",         (void*)&WB_CreateMetaspaceTestContext},\n+  {CC\"destroyMetaspaceTestContext\", CC\"(J)V\",         (void*)&WB_DestroyMetaspaceTestContext},\n+  {CC\"purgeMetaspaceTestContext\", CC\"(J)V\",           (void*)&WB_PurgeMetaspaceTestContext},\n+  {CC\"printMetaspaceTestContext\", CC\"(J)V\",           (void*)&WB_PrintMetaspaceTestContext},\n+  {CC\"getTotalCommittedWordsInMetaspaceTestContext\", CC\"(J)J\",(void*)&WB_GetTotalCommittedWordsInMetaspaceTestContext},\n+  {CC\"getTotalUsedWordsInMetaspaceTestContext\", CC\"(J)J\", (void*)&WB_GetTotalUsedWordsInMetaspaceTestContext},\n+  {CC\"createArenaInTestContext\", CC\"(JZ)J\",           (void*)&WB_CreateArenaInTestContext},\n+  {CC\"destroyMetaspaceTestArena\", CC\"(J)V\",           (void*)&WB_DestroyMetaspaceTestArena},\n+  {CC\"allocateFromMetaspaceTestArena\", CC\"(JJ)J\",     (void*)&WB_AllocateFromMetaspaceTestArena},\n+  {CC\"deallocateToMetaspaceTestArena\", CC\"(JJJ)V\",    (void*)&WB_DeallocateToMetaspaceTestArena},\n+  {CC\"maxMetaspaceAllocationSize\", CC\"()J\",           (void*)&WB_GetMaxMetaspaceAllocationSize},\n+\n@@ -2580,0 +2670,1 @@\n+  {CC\"getLibcName\",     CC\"()Ljava\/lang\/String;\",     (void*)&WB_GetLibcName},\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":103,"deletions":12,"binary":false,"changes":115,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"prims\/jvmtiDeferredUpdates.hpp\"\n@@ -52,0 +53,2 @@\n+#include \"prims\/vectorSupport.hpp\"\n+#include \"prims\/methodHandles.hpp\"\n@@ -55,0 +58,1 @@\n+#include \"runtime\/escapeBarrier.hpp\"\n@@ -61,0 +65,1 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n@@ -176,1 +181,2 @@\n-                                  frame& deoptee, RegisterMap& map, GrowableArray<compiledVFrame*>* chunk) {\n+                                  frame& deoptee, RegisterMap& map, GrowableArray<compiledVFrame*>* chunk,\n+                                  bool& deoptimized_objects) {\n@@ -180,0 +186,4 @@\n+  JavaThread* deoptee_thread = chunk->at(0)->thread();\n+  assert(exec_mode == Deoptimization::Unpack_none || (deoptee_thread == thread),\n+         \"a frame can only be deoptimized by the owner thread\");\n+\n@@ -206,1 +216,8 @@\n-    JRT_BLOCK\n+    if (exec_mode == Deoptimization::Unpack_none) {\n+      assert(thread->thread_state() == _thread_in_vm, \"assumption\");\n+      Thread* THREAD = thread;\n+      \/\/ Clear pending OOM if reallocation fails and return true indicating allocation failure\n+      realloc_failures = Deoptimization::realloc_objects(thread, &deoptee, &map, objects, CHECK_AND_CLEAR_(true));\n+      deoptimized_objects = true;\n+    } else {\n+      JRT_BLOCK\n@@ -208,1 +225,2 @@\n-    JRT_END\n+      JRT_END\n+    }\n@@ -214,1 +232,1 @@\n-      tty->print_cr(\"REALLOC OBJECTS in thread \" INTPTR_FORMAT, p2i(thread));\n+      tty->print_cr(\"REALLOC OBJECTS in thread \" INTPTR_FORMAT, p2i(deoptee_thread));\n@@ -226,1 +244,4 @@\n-static void eliminate_locks(JavaThread* thread, GrowableArray<compiledVFrame*>* chunk, bool realloc_failures) {\n+static void eliminate_locks(JavaThread* thread, GrowableArray<compiledVFrame*>* chunk, bool realloc_failures,\n+                            frame& deoptee, int exec_mode, bool& deoptimized_objects) {\n+  JavaThread* deoptee_thread = chunk->at(0)->thread();\n+  assert(!EscapeBarrier::objs_are_deoptimized(deoptee_thread, deoptee.id()), \"must relock just once\");\n@@ -237,1 +258,3 @@\n-      Deoptimization::relock_objects(monitors, thread, realloc_failures);\n+      bool relocked = Deoptimization::relock_objects(thread, monitors, deoptee_thread, deoptee,\n+                                                     exec_mode, realloc_failures);\n+      deoptimized_objects = deoptimized_objects || relocked;\n@@ -248,0 +271,7 @@\n+            if (exec_mode == Deoptimization::Unpack_none) {\n+              ObjectMonitor* monitor = deoptee_thread->current_waiting_monitor();\n+              if (monitor != NULL && (oop)monitor->object() == mi->owner()) {\n+                tty->print_cr(\"     object <\" INTPTR_FORMAT \"> DEFERRED relocking after wait\", p2i(mi->owner()));\n+                continue;\n+              }\n+            }\n@@ -261,0 +291,30 @@\n+\n+\/\/ Deoptimize objects, that is reallocate and relock them, just before they escape through JVMTI.\n+\/\/ The given vframes cover one physical frame.\n+bool Deoptimization::deoptimize_objects_internal(JavaThread* thread, GrowableArray<compiledVFrame*>* chunk,\n+                                                 bool& realloc_failures) {\n+  frame deoptee = chunk->at(0)->fr();\n+  JavaThread* deoptee_thread = chunk->at(0)->thread();\n+  CompiledMethod* cm = deoptee.cb()->as_compiled_method_or_null();\n+  RegisterMap map(chunk->at(0)->register_map());\n+  bool deoptimized_objects = false;\n+\n+  bool const jvmci_enabled = JVMCI_ONLY(UseJVMCICompiler) NOT_JVMCI(false);\n+\n+  \/\/ Reallocate the non-escaping objects and restore their fields.\n+  if (jvmci_enabled COMPILER2_PRESENT(|| (DoEscapeAnalysis && EliminateAllocations))) {\n+    realloc_failures = eliminate_allocations(thread, Unpack_none, cm, deoptee, map, chunk, deoptimized_objects);\n+  }\n+\n+  \/\/ Revoke biases of objects with eliminated locks in the given frame.\n+  Deoptimization::revoke_for_object_deoptimization(deoptee_thread, deoptee, &map, thread);\n+\n+  \/\/ MonitorInfo structures used in eliminate_locks are not GC safe.\n+  NoSafepointVerifier no_safepoint;\n+\n+  \/\/ Now relock objects if synchronization on them was eliminated.\n+  if (jvmci_enabled COMPILER2_PRESENT(|| ((DoEscapeAnalysis || EliminateNestedLocks) && EliminateLocks))) {\n+    eliminate_locks(thread, chunk, realloc_failures, deoptee, Unpack_none, deoptimized_objects);\n+  }\n+  return deoptimized_objects;\n+}\n@@ -319,1 +379,2 @@\n-    realloc_failures = eliminate_allocations(thread, exec_mode, cm, deoptee, map, chunk);\n+    bool unused;\n+    realloc_failures = eliminate_allocations(thread, exec_mode, cm, deoptee, map, chunk, unused);\n@@ -334,2 +395,4 @@\n-  if (jvmci_enabled COMPILER2_PRESENT( || ((DoEscapeAnalysis || EliminateNestedLocks) && EliminateLocks) )) {\n-    eliminate_locks(thread, chunk, realloc_failures);\n+  if ((jvmci_enabled COMPILER2_PRESENT( || ((DoEscapeAnalysis || EliminateNestedLocks) && EliminateLocks) ))\n+      && !EscapeBarrier::objs_are_deoptimized(thread, deoptee.id())) {\n+    bool unused;\n+    eliminate_locks(thread, chunk, realloc_failures, deoptee, exec_mode, unused);\n@@ -366,22 +429,1 @@\n-  if (thread->deferred_locals() != NULL) {\n-    GrowableArray<jvmtiDeferredLocalVariableSet*>* list = thread->deferred_locals();\n-    int i = 0;\n-    do {\n-      \/\/ Because of inlining we could have multiple vframes for a single frame\n-      \/\/ and several of the vframes could have deferred writes. Find them all.\n-      if (list->at(i)->id() == array->original().id()) {\n-        jvmtiDeferredLocalVariableSet* dlv = list->at(i);\n-        list->remove_at(i);\n-        \/\/ individual jvmtiDeferredLocalVariableSet are CHeapObj's\n-        delete dlv;\n-      } else {\n-        i++;\n-      }\n-    } while ( i < list->length() );\n-    if (list->length() == 0) {\n-      thread->set_deferred_locals(NULL);\n-      \/\/ free the list and elements back to C heap.\n-      delete list;\n-    }\n-\n-  }\n+  JvmtiDeferredUpdates::delete_updates_for_frame(thread, array->original().id());\n@@ -1017,0 +1059,7 @@\n+#ifdef COMPILER2\n+        if (EnableVectorSupport && VectorSupport::is_vector(ik)) {\n+          obj = VectorSupport::allocate_vector(ik, fr, reg_map, sv, THREAD);\n+        } else {\n+          obj = ik->allocate_instance(THREAD);\n+        }\n+#else\n@@ -1018,0 +1067,1 @@\n+#endif \/\/ COMPILER2\n@@ -1354,0 +1404,5 @@\n+#ifdef COMPILER2\n+    if (EnableVectorSupport && VectorSupport::is_vector(k)) {\n+      continue; \/\/ skip field reassignment for vectors\n+    }\n+#endif\n@@ -1368,1 +1423,3 @@\n-void Deoptimization::relock_objects(GrowableArray<MonitorInfo*>* monitors, JavaThread* thread, bool realloc_failures) {\n+bool Deoptimization::relock_objects(JavaThread* thread, GrowableArray<MonitorInfo*>* monitors,\n+                                    JavaThread* deoptee_thread, frame& fr, int exec_mode, bool realloc_failures) {\n+  bool relocked_objects = false;\n@@ -1373,0 +1430,1 @@\n+      relocked_objects = true;\n@@ -1381,1 +1439,1 @@\n-                 mark.biased_locker() == thread, \"should be locked to current thread\");\n+                 mark.biased_locker() == deoptee_thread, \"should be locked to current thread\");\n@@ -1385,0 +1443,19 @@\n+        } else if (exec_mode == Unpack_none) {\n+          if (mark.has_locker() && fr.sp() > (intptr_t*)mark.locker()) {\n+            \/\/ With exec_mode == Unpack_none obj may be thread local and locked in\n+            \/\/ a callee frame. In this case the bias was revoked before in revoke_for_object_deoptimization().\n+            \/\/ Make the lock in the callee a recursive lock and restore the displaced header.\n+            markWord dmw = mark.displaced_mark_helper();\n+            mark.locker()->set_displaced_header(markWord::encode((BasicLock*) NULL));\n+            obj->set_mark(dmw);\n+          }\n+          if (mark.has_monitor()) {\n+            \/\/ defer relocking if the deoptee thread is currently waiting for obj\n+            ObjectMonitor* waiting_monitor = deoptee_thread->current_waiting_monitor();\n+            if (waiting_monitor != NULL && (oop)waiting_monitor->object() == obj()) {\n+              assert(fr.is_deoptimized_frame(), \"frame must be scheduled for deoptimization\");\n+              mon_info->lock()->set_displaced_header(markWord::unused_mark());\n+              JvmtiDeferredUpdates::inc_relock_count_after_wait(deoptee_thread);\n+              continue;\n+            }\n+          }\n@@ -1387,1 +1464,1 @@\n-        ObjectSynchronizer::enter(obj, lock, thread);\n+        ObjectSynchronizer::enter(obj, lock, deoptee_thread);\n@@ -1392,0 +1469,1 @@\n+  return relocked_objects;\n@@ -1509,1 +1587,2 @@\n-static void collect_monitors(compiledVFrame* cvf, GrowableArray<Handle>* objects_to_revoke) {\n+static void collect_monitors(compiledVFrame* cvf, GrowableArray<Handle>* objects_to_revoke,\n+                             bool only_eliminated) {\n@@ -1514,1 +1593,3 @@\n-    if (!mon_info->eliminated() && mon_info->owner() != NULL) {\n+    if (mon_info->eliminated() == only_eliminated &&\n+        !mon_info->owner_is_scalar_replaced() &&\n+        mon_info->owner() != NULL) {\n@@ -1520,1 +1601,2 @@\n-static void get_monitors_from_stack(GrowableArray<Handle>* objects_to_revoke, JavaThread* thread, frame fr, RegisterMap* map) {\n+static void get_monitors_from_stack(GrowableArray<Handle>* objects_to_revoke, JavaThread* thread,\n+                                    frame fr, RegisterMap* map, bool only_eliminated) {\n@@ -1540,1 +1622,1 @@\n-    collect_monitors(cvf, objects_to_revoke);\n+    collect_monitors(cvf, objects_to_revoke, only_eliminated);\n@@ -1543,1 +1625,1 @@\n-  collect_monitors(cvf, objects_to_revoke);\n+  collect_monitors(cvf, objects_to_revoke, only_eliminated);\n@@ -1554,1 +1636,1 @@\n-  get_monitors_from_stack(objects_to_revoke, thread, fr, map);\n+  get_monitors_from_stack(objects_to_revoke, thread, fr, map, false);\n@@ -1564,0 +1646,35 @@\n+\/\/ Revoke the bias of objects with eliminated locking to prepare subsequent relocking.\n+void Deoptimization::revoke_for_object_deoptimization(JavaThread* deoptee_thread, frame fr,\n+                                                      RegisterMap* map, JavaThread* thread) {\n+  if (!UseBiasedLocking) {\n+    return;\n+  }\n+  GrowableArray<Handle>* objects_to_revoke = new GrowableArray<Handle>();\n+  if (deoptee_thread != thread) {\n+    \/\/ Process stack of deoptee thread as we will access oops during object deoptimization.\n+    StackWatermarkSet::start_processing(deoptee_thread, StackWatermarkKind::gc);\n+  }\n+  \/\/ Collect monitors but only those with eliminated locking.\n+  get_monitors_from_stack(objects_to_revoke, deoptee_thread, fr, map, true);\n+\n+  int len = objects_to_revoke->length();\n+  for (int i = 0; i < len; i++) {\n+    oop obj = (objects_to_revoke->at(i))();\n+    markWord mark = obj->mark();\n+    if (!mark.has_bias_pattern() ||\n+        mark.is_biased_anonymously() || \/\/ eliminated locking does not bias an object if it wasn't before\n+        !obj->klass()->prototype_header().has_bias_pattern() || \/\/ bulk revoke ignores eliminated monitors\n+        (obj->klass()->prototype_header().bias_epoch() != mark.bias_epoch())) { \/\/ bulk rebias ignores eliminated monitors\n+      \/\/ We reach here regularly if there's just eliminated locking on obj.\n+      \/\/ We must not call BiasedLocking::revoke_own_lock() in this case, as we\n+      \/\/ would hit assertions because it is a prerequisite that there has to be\n+      \/\/ non-eliminated locking on obj by deoptee_thread.\n+      \/\/ Luckily we don't have to revoke here because obj has to be a\n+      \/\/ non-escaping obj and can be relocked without revoking the bias. See\n+      \/\/ Deoptimization::relock_objects().\n+      continue;\n+    }\n+    BiasedLocking::revoke(objects_to_revoke->at(i), thread);\n+    assert(!objects_to_revoke->at(i)->mark().has_bias_pattern(), \"biases should be revoked by now\");\n+  }\n+}\n@@ -1630,2 +1747,4 @@\n-  assert(thread == Thread::current() || SafepointSynchronize::is_at_safepoint(),\n-         \"can only deoptimize other thread at a safepoint\");\n+  assert(thread == Thread::current() ||\n+         thread->is_handshake_safe_for(Thread::current()) ||\n+         SafepointSynchronize::is_at_safepoint(),\n+         \"can only deoptimize other thread at a safepoint\/handshake\");\n@@ -1643,1 +1762,2 @@\n-  if (thread == Thread::current()) {\n+  Thread* current = Thread::current();\n+  if (thread == current || thread->is_handshake_safe_for(current)) {\n@@ -2611,0 +2731,1 @@\n+\n","filename":"src\/hotspot\/share\/runtime\/deoptimization.cpp","additions":164,"deletions":43,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -289,4 +289,0 @@\n-  notproduct(bool, StressDerivedPointers, false,                            \\\n-          \"Force scavenge when a derived pointer is detected on stack \"     \\\n-          \"after rtm call\")                                                 \\\n-                                                                            \\\n@@ -328,5 +324,2 @@\n-  product(bool, CriticalJNINatives, true,                                   \\\n-          \"Check for critical JNI entry points\")                            \\\n-                                                                            \\\n-  notproduct(bool, StressCriticalJNINatives, false,                         \\\n-          \"Exercise register saving code in critical natives\")              \\\n+  product(bool, CriticalJNINatives, false,                                  \\\n+          \"(Deprecated) Check for critical JNI entry points\")               \\\n@@ -355,0 +348,4 @@\n+  product(bool, UseSHA3Intrinsics, false, DIAGNOSTIC,                       \\\n+          \"Use intrinsics for SHA3 crypto hash function. \"                  \\\n+          \"Requires that UseSHA is enabled.\")                               \\\n+                                                                            \\\n@@ -398,0 +395,23 @@\n+  develop(bool, DeoptimizeObjectsALot, false,                               \\\n+          \"For testing purposes concurrent threads revert optimizations \"   \\\n+          \"based on escape analysis at intervals given with \"               \\\n+          \"DeoptimizeObjectsALotInterval=n. The thread count is given \"     \\\n+          \"with DeoptimizeObjectsALotThreadCountSingle and \"                \\\n+          \"DeoptimizeObjectsALotThreadCountAll.\")                           \\\n+                                                                            \\\n+  develop(uint64_t, DeoptimizeObjectsALotInterval, 5,                       \\\n+          \"Interval for DeoptimizeObjectsALot.\")                            \\\n+          range(0, max_jlong)                                               \\\n+                                                                            \\\n+  develop(int, DeoptimizeObjectsALotThreadCountSingle, 1,                   \\\n+          \"The number of threads that revert optimizations based on \"       \\\n+          \"escape analysis for a single thread if DeoptimizeObjectsALot \"   \\\n+          \"is enabled. The target thread is selected round robin.\" )        \\\n+          range(0, max_jint)                                                \\\n+                                                                            \\\n+  develop(int, DeoptimizeObjectsALotThreadCountAll, 1,                      \\\n+          \"The number of threads that revert optimizations based on \"       \\\n+          \"escape analysis for all threads if DeoptimizeObjectsALot \"       \\\n+          \"is enabled.\" )                                                   \\\n+          range(0, max_jint)                                                \\\n+                                                                            \\\n@@ -480,3 +500,0 @@\n-  develop(bool, PrintVMMessages, true,                                      \\\n-          \"Print VM messages on console\")                                   \\\n-                                                                            \\\n@@ -518,3 +535,0 @@\n-  product_pd(bool, UseOSErrorReporting,                                     \\\n-          \"Let VM fatal error propagate to the OS (ie. WER on Windows)\")    \\\n-                                                                            \\\n@@ -744,1 +758,1 @@\n-          \"Do not complain if the application installs signal handlers \"    \\\n+          \"Application will install primary signal handlers for the JVM \"   \\\n@@ -929,1 +943,0 @@\n-          constraint(InitialBootClassLoaderMetaspaceSizeConstraintFunc, AfterErgo)\\\n@@ -1331,5 +1344,0 @@\n-  develop(bool, TraceProfileInterpreter, false,                             \\\n-          \"Trace profiling at the bytecode level during interpretation. \"   \\\n-          \"This outputs the profiling information collected to improve \"    \\\n-          \"jit compilation.\")                                               \\\n-                                                                            \\\n@@ -1350,4 +1358,0 @@\n-  develop(bool, VerifyCompiledCode, false,                                  \\\n-          \"Include miscellaneous runtime verifications in nmethod code; \"   \\\n-          \"default off because it disturbs nmethod size heuristics\")        \\\n-                                                                            \\\n@@ -1561,4 +1565,0 @@\n-  develop(intx, ProfilerNodeSize,  1024,                                    \\\n-          \"Size in K to allocate for the Profile Nodes of each thread\")     \\\n-          range(0, 1024)                                                    \\\n-                                                                            \\\n@@ -1579,0 +1579,9 @@\n+  product(ccstr, MetaspaceReclaimPolicy, \"balanced\",                        \\\n+          \"options: balanced, aggressive, none\")                            \\\n+                                                                            \\\n+  product(bool, MetaspaceGuardAllocations, false, DIAGNOSTIC,               \\\n+          \"Metapace allocations are guarded.\")                              \\\n+                                                                            \\\n+  product(bool, MetaspaceHandleDeallocations, true, DIAGNOSTIC,             \\\n+          \"Switch off Metapace deallocation handling.\")                     \\\n+                                                                            \\\n@@ -1806,1 +1815,1 @@\n-   product(ccstr, InlineDataFile, NULL,                                     \\\n+  product(ccstr, InlineDataFile, NULL,                                      \\\n@@ -2198,0 +2207,11 @@\n+  notproduct(bool, UseDebuggerErgo, false,                                  \\\n+          \"Debugging Only: Adjust the VM to be more debugger-friendly. \"    \\\n+          \"Turns on the other UseDebuggerErgo* flags\")                      \\\n+                                                                            \\\n+  notproduct(bool, UseDebuggerErgo1, false,                                 \\\n+          \"Debugging Only: Enable workarounds for debugger induced \"        \\\n+          \"os::processor_id() >= os::processor_count() problems\")           \\\n+                                                                            \\\n+  notproduct(bool, UseDebuggerErgo2, false,                                 \\\n+          \"Debugging Only: Limit the number of spawned JVM threads\")        \\\n+                                                                            \\\n@@ -2373,5 +2393,0 @@\n-  product(intx, SurvivorAlignmentInBytes, 0, EXPERIMENTAL,                  \\\n-           \"Default survivor space alignment in bytes\")                     \\\n-           range(8, 256)                                                    \\\n-           constraint(SurvivorAlignmentInBytesConstraintFunc,AfterErgo)     \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":51,"deletions":36,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -131,15 +131,0 @@\n-\n-  void transition_back() {\n-    \/\/ This can be invoked from transition states and must return to the original state properly\n-    assert(_thread->thread_state() == _thread_in_vm, \"should only call when leaving VM after handshake\");\n-\n-    _thread->set_thread_state(_original_state);\n-\n-    if (_original_state != _thread_blocked_trans &&  _original_state != _thread_in_vm_trans &&\n-        _thread->has_special_runtime_exit_condition()) {\n-      _thread->handle_special_runtime_exit_condition(\n-          !_thread->is_at_poll_safepoint() && (_original_state != _thread_in_native_trans));\n-    }\n-  }\n-\n-\n@@ -161,1 +146,2 @@\n-    transition_back();\n+    assert(_thread->thread_state() == _thread_in_vm, \"should only call when leaving VM after handshake\");\n+    _thread->set_thread_state(_original_state);\n@@ -248,1 +234,0 @@\n-    OrderAccess::cross_modify_fence();\n@@ -298,1 +283,0 @@\n-    OrderAccess::cross_modify_fence();\n","filename":"src\/hotspot\/share\/runtime\/interfaceSupport.inline.hpp","additions":2,"deletions":18,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"prims\/jvmtiDeferredUpdates.hpp\"\n@@ -1563,1 +1564,2 @@\n-  _recursions = save;     \/\/ restore the old recursion count\n+  _recursions = save      \/\/ restore the old recursion count\n+                + JvmtiDeferredUpdates::get_and_reset_relock_count_after_wait(jt); \/\/  increased by the deferred relock count\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1005,2 +1005,8 @@\n-    ::strftime(buf, buflen, \"%Z\", &tz);\n-    st->print(\"Time: %s %s\", timestring, buf);\n+    wchar_t w_buf[80];\n+    size_t n = ::wcsftime(w_buf, 80, L\"%Z\", &tz);\n+    if (n > 0) {\n+      ::wcstombs(buf, w_buf, buflen);\n+      st->print(\"Time: %s %s\", timestring, buf);\n+    } else {\n+      st->print(\"Time: %s\", timestring);\n+    }\n@@ -1649,2 +1655,2 @@\n-char* os::reserve_memory(size_t bytes, MEMFLAGS flags) {\n-  char* result = pd_reserve_memory(bytes);\n+char* os::reserve_memory(size_t bytes, bool executable, MEMFLAGS flags) {\n+  char* result = pd_reserve_memory(bytes, executable);\n@@ -1661,32 +1667,4 @@\n-char* os::reserve_memory_with_fd(size_t bytes, int file_desc, bool executable) {\n-  char* result;\n-\n-  if (file_desc != -1) {\n-    \/\/ Could have called pd_reserve_memory() followed by replace_existing_mapping_with_file_mapping(),\n-    \/\/ but AIX may use SHM in which case its more trouble to detach the segment and remap memory to the file.\n-    result = os::map_memory_to_file(NULL \/* addr *\/, bytes, file_desc);\n-    if (result != NULL) {\n-      MemTracker::record_virtual_memory_reserve_and_commit(result, bytes, CALLER_PC);\n-    }\n-  } else {\n-    result = pd_reserve_memory(bytes, executable);\n-    if (result != NULL) {\n-      MemTracker::record_virtual_memory_reserve(result, bytes, CALLER_PC);\n-    }\n-  }\n-\n-  return result;\n-}\n-\n-char* os::attempt_reserve_memory_at(char* addr, size_t bytes, int file_desc) {\n-  char* result = NULL;\n-  if (file_desc != -1) {\n-    result = pd_attempt_reserve_memory_at(addr, bytes, file_desc);\n-    if (result != NULL) {\n-      MemTracker::record_virtual_memory_reserve_and_commit((address)result, bytes, CALLER_PC);\n-    }\n-  } else {\n-    result = pd_attempt_reserve_memory_at(addr, bytes);\n-    if (result != NULL) {\n-      MemTracker::record_virtual_memory_reserve((address)result, bytes, CALLER_PC);\n-    }\n+char* os::attempt_reserve_memory_at(char* addr, size_t bytes) {\n+  char* result = pd_attempt_reserve_memory_at(addr, bytes);\n+  if (result != NULL) {\n+    MemTracker::record_virtual_memory_reserve((address)result, bytes, CALLER_PC);\n@@ -1761,0 +1739,19 @@\n+char* os::map_memory_to_file(size_t bytes, int file_desc) {\n+  \/\/ Could have called pd_reserve_memory() followed by replace_existing_mapping_with_file_mapping(),\n+  \/\/ but AIX may use SHM in which case its more trouble to detach the segment and remap memory to the file.\n+  \/\/ On all current implementations NULL is interpreted as any available address.\n+  char* result = os::map_memory_to_file(NULL \/* addr *\/, bytes, file_desc);\n+  if (result != NULL) {\n+    MemTracker::record_virtual_memory_reserve_and_commit(result, bytes, CALLER_PC);\n+  }\n+  return result;\n+}\n+\n+char* os::attempt_map_memory_to_file_at(char* addr, size_t bytes, int file_desc) {\n+  char* result = pd_attempt_map_memory_to_file_at(addr, bytes, file_desc);\n+  if (result != NULL) {\n+    MemTracker::record_virtual_memory_reserve_and_commit((address)result, bytes, CALLER_PC);\n+  }\n+  return result;\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":33,"deletions":36,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -121,1 +121,1 @@\n-  static char*  pd_reserve_memory(size_t bytes, bool executable = false);\n+  static char*  pd_reserve_memory(size_t bytes, bool executable);\n@@ -124,1 +124,0 @@\n-  static char*  pd_attempt_reserve_memory_at(char* addr, size_t bytes, int file_desc);\n@@ -139,0 +138,2 @@\n+  static char*  pd_attempt_map_memory_to_file_at(char* addr, size_t bytes, int file_desc);\n+\n@@ -321,6 +322,1 @@\n-  \/\/ alignment_hint - currently only used by AIX\n-  static char*  reserve_memory(size_t bytes, MEMFLAGS flags = mtOther);\n-\n-  \/\/ Reserves virtual memory.\n-  \/\/ if file_desc != -1, also attaches the memory to the file.\n-  static char*  reserve_memory_with_fd(size_t bytes, int file_desc, bool executable = false);\n+  static char*  reserve_memory(size_t bytes, bool executable = false, MEMFLAGS flags = mtOther);\n@@ -329,1 +325,1 @@\n-  static char*  reserve_memory_aligned(size_t size, size_t alignment, int file_desc = -1);\n+  static char*  reserve_memory_aligned(size_t size, size_t alignment);\n@@ -333,1 +329,1 @@\n-  static char*  attempt_reserve_memory_at(char* addr, size_t bytes, int file_desc = -1);\n+  static char*  attempt_reserve_memory_at(char* addr, size_t bytes);\n@@ -378,0 +374,2 @@\n+  static char* map_memory_to_file(size_t size, int fd);\n+  static char* map_memory_to_file_aligned(size_t size, size_t alignment, int fd);\n@@ -379,0 +377,1 @@\n+  static char* attempt_map_memory_to_file_at(char* base, size_t size, int fd);\n@@ -490,0 +489,1 @@\n+  static frame      fetch_compiled_frame_from_context(const void* ucVoid);\n","filename":"src\/hotspot\/share\/runtime\/os.hpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -676,33 +676,0 @@\n-\/\/ See if the thread is running inside a lazy critical native and\n-\/\/ update the thread critical count if so.  Also set a suspend flag to\n-\/\/ cause the native wrapper to return into the JVM to do the unlock\n-\/\/ once the native finishes.\n-static void check_for_lazy_critical_native(JavaThread *thread, JavaThreadState state) {\n-  if (state == _thread_in_native &&\n-      thread->has_last_Java_frame() &&\n-      thread->frame_anchor()->walkable()) {\n-    \/\/ This thread might be in a critical native nmethod so look at\n-    \/\/ the top of the stack and increment the critical count if it\n-    \/\/ is.\n-    frame wrapper_frame = thread->last_frame();\n-    CodeBlob* stub_cb = wrapper_frame.cb();\n-    if (stub_cb != NULL &&\n-        stub_cb->is_nmethod() &&\n-        stub_cb->as_nmethod_or_null()->is_lazy_critical_native()) {\n-      \/\/ A thread could potentially be in a critical native across\n-      \/\/ more than one safepoint, so only update the critical state on\n-      \/\/ the first one.  When it returns it will perform the unlock.\n-      if (!thread->do_critical_native_unlock()) {\n-#ifdef ASSERT\n-        if (!thread->in_critical()) {\n-          GCLocker::increment_debug_jni_lock_count();\n-        }\n-#endif\n-        thread->enter_critical();\n-        \/\/ Make sure the native wrapper calls back on return to\n-        \/\/ perform the needed critical unlock.\n-        thread->set_critical_native_unlock();\n-      }\n-    }\n-  }\n-}\n@@ -904,1 +871,0 @@\n-    check_for_lazy_critical_native(_thread, stable_state);\n@@ -948,0 +914,2 @@\n+  JavaThread* self = thread();\n+  assert(self == Thread::current()->as_Java_thread(), \"must be self\");\n@@ -950,1 +918,1 @@\n-  address real_return_addr = thread()->saved_exception_pc();\n+  address real_return_addr = self->saved_exception_pc();\n@@ -957,1 +925,1 @@\n-  frame stub_fr = thread()->last_frame();\n+  frame stub_fr = self->last_frame();\n@@ -960,1 +928,1 @@\n-  RegisterMap map(thread(), true, false);\n+  RegisterMap map(self, true, false);\n@@ -981,1 +949,1 @@\n-      return_value = Handle(thread(), result);\n+      return_value = Handle(self, result);\n@@ -988,1 +956,1 @@\n-    StackWatermarkSet::after_unwind(thread());\n+    StackWatermarkSet::after_unwind(self);\n@@ -991,1 +959,6 @@\n-    SafepointMechanism::process_if_requested(thread());\n+    SafepointMechanism::process_if_requested(self);\n+    \/\/ We have to wait if we are here because of a handshake for object deoptimization.\n+    if (self->is_obj_deopt_suspend()) {\n+      self->wait_for_object_deoptimization();\n+    }\n+    self->check_and_handle_async_exceptions();\n@@ -1007,1 +980,5 @@\n-    SafepointMechanism::process_if_requested(thread());\n+    SafepointMechanism::process_if_requested(self);\n+    \/\/ We have to wait if we are here because of a handshake for object deoptimization.\n+    if (self->is_obj_deopt_suspend()) {\n+      self->wait_for_object_deoptimization();\n+    }\n@@ -1012,3 +989,3 @@\n-    if (thread()->has_async_condition()) {\n-      ThreadInVMfromJavaNoAsyncException __tiv(thread());\n-      Deoptimization::deoptimize_frame(thread(), caller_fr.id());\n+    if (self->has_async_condition()) {\n+      ThreadInVMfromJavaNoAsyncException __tiv(self);\n+      Deoptimization::deoptimize_frame(self, caller_fr.id());\n@@ -1020,2 +997,2 @@\n-    if (thread()->has_pending_exception() ) {\n-      RegisterMap map(thread(), true, false);\n+    if (self->has_pending_exception() ) {\n+      RegisterMap map(self, true, false);\n","filename":"src\/hotspot\/share\/runtime\/safepoint.cpp","additions":23,"deletions":46,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -2460,1 +2460,1 @@\n-    : BasicHashtable<mtCode>(293, (DumpSharedSpaces ? sizeof(CDSAdapterHandlerEntry) : sizeof(AdapterHandlerEntry))) { }\n+    : BasicHashtable<mtCode>(293, (sizeof(AdapterHandlerEntry))) { }\n@@ -2466,3 +2466,0 @@\n-    if (DumpSharedSpaces) {\n-      ((CDSAdapterHandlerEntry*)entry)->init();\n-    }\n@@ -2944,30 +2941,0 @@\n-JRT_ENTRY_NO_ASYNC(void, SharedRuntime::block_for_jni_critical(JavaThread* thread))\n-  assert(thread == JavaThread::current(), \"must be\");\n-  \/\/ The code is about to enter a JNI lazy critical native method and\n-  \/\/ _needs_gc is true, so if this thread is already in a critical\n-  \/\/ section then just return, otherwise this thread should block\n-  \/\/ until needs_gc has been cleared.\n-  if (thread->in_critical()) {\n-    return;\n-  }\n-  \/\/ Lock and unlock a critical section to give the system a chance to block\n-  GCLocker::lock_critical(thread);\n-  GCLocker::unlock_critical(thread);\n-JRT_END\n-\n-JRT_LEAF(oopDesc*, SharedRuntime::pin_object(JavaThread* thread, oopDesc* obj))\n-  assert(Universe::heap()->supports_object_pinning(), \"Why we are here?\");\n-  assert(obj != NULL, \"Should not be null\");\n-  oop o(obj);\n-  o = Universe::heap()->pin_object(thread, o);\n-  assert(o != NULL, \"Should not be null\");\n-  return o;\n-JRT_END\n-\n-JRT_LEAF(void, SharedRuntime::unpin_object(JavaThread* thread, oopDesc* obj))\n-  assert(Universe::heap()->supports_object_pinning(), \"Why we are here?\");\n-  assert(obj != NULL, \"Should not be null\");\n-  oop o(obj);\n-  Universe::heap()->unpin_object(thread, o);\n-JRT_END\n-\n@@ -3166,11 +3133,0 @@\n-#if INCLUDE_CDS\n-\n-void CDSAdapterHandlerEntry::init() {\n-  assert(DumpSharedSpaces, \"used during dump time only\");\n-  _c2i_entry_trampoline = (address)MetaspaceShared::misc_code_space_alloc(SharedRuntime::trampoline_size());\n-  _adapter_trampoline = (AdapterHandlerEntry**)MetaspaceShared::misc_code_space_alloc(sizeof(AdapterHandlerEntry*));\n-};\n-\n-#endif \/\/ INCLUDE_CDS\n-\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":1,"deletions":45,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -114,2 +114,0 @@\n-address StubRoutines::_zero_aligned_words = CAST_FROM_FN_PTR(address, Copy::zero_to_words);\n-\n@@ -149,0 +147,2 @@\n+address StubRoutines::_sha3_implCompress     = NULL;\n+address StubRoutines::_sha3_implCompressMB   = NULL;\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -210,3 +210,0 @@\n-  \/\/ zero heap space aligned to jlong (8 bytes)\n-  static address _zero_aligned_words;\n-\n@@ -231,0 +228,2 @@\n+  static address _sha3_implCompress;\n+  static address _sha3_implCompressMB;\n@@ -410,0 +409,2 @@\n+  static address sha3_implCompress()     { return _sha3_implCompress; }\n+  static address sha3_implCompressMB()   { return _sha3_implCompressMB; }\n@@ -441,2 +442,0 @@\n-  static address zero_aligned_words()  { return _zero_aligned_words; }\n-\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":4,"deletions":5,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -65,0 +65,1 @@\n+#include \"prims\/jvmtiDeferredUpdates.hpp\"\n@@ -122,0 +123,1 @@\n+#include \"utilities\/spinYield.hpp\"\n@@ -263,1 +265,0 @@\n-  _num_nested_signal = 0;\n@@ -540,82 +541,1 @@\n-\/\/\n-\/\/ The bits parameter returns information about the code path through\n-\/\/ the routine. Useful for debugging:\n-\/\/\n-\/\/ set in is_ext_suspend_completed():\n-\/\/ 0x00000001 - routine was entered\n-\/\/ 0x00000010 - routine return false at end\n-\/\/ 0x00000100 - thread exited (return false)\n-\/\/ 0x00000200 - suspend request cancelled (return false)\n-\/\/ 0x00000400 - thread suspended (return true)\n-\/\/ 0x00001000 - thread is in a suspend equivalent state (return true)\n-\/\/ 0x00002000 - thread is native and walkable (return true)\n-\/\/ 0x00004000 - thread is native_trans and walkable (needed retry)\n-\/\/\n-\/\/ set in wait_for_ext_suspend_completion():\n-\/\/ 0x00010000 - routine was entered\n-\/\/ 0x00020000 - suspend request cancelled before loop (return false)\n-\/\/ 0x00040000 - thread suspended before loop (return true)\n-\/\/ 0x00080000 - suspend request cancelled in loop (return false)\n-\/\/ 0x00100000 - thread suspended in loop (return true)\n-\/\/ 0x00200000 - suspend not completed during retry loop (return false)\n-\n-\/\/ Helper class for tracing suspend wait debug bits.\n-\/\/\n-\/\/ 0x00000100 indicates that the target thread exited before it could\n-\/\/ self-suspend which is not a wait failure. 0x00000200, 0x00020000 and\n-\/\/ 0x00080000 each indicate a cancelled suspend request so they don't\n-\/\/ count as wait failures either.\n-#define DEBUG_FALSE_BITS (0x00000010 | 0x00200000)\n-\n-class TraceSuspendDebugBits : public StackObj {\n- private:\n-  JavaThread * jt;\n-  bool         is_wait;\n-  bool         called_by_wait;  \/\/ meaningful when !is_wait\n-  uint32_t *   bits;\n-\n- public:\n-  TraceSuspendDebugBits(JavaThread *_jt, bool _is_wait, bool _called_by_wait,\n-                        uint32_t *_bits) {\n-    jt             = _jt;\n-    is_wait        = _is_wait;\n-    called_by_wait = _called_by_wait;\n-    bits           = _bits;\n-  }\n-\n-  ~TraceSuspendDebugBits() {\n-    if (!is_wait) {\n-#if 1\n-      \/\/ By default, don't trace bits for is_ext_suspend_completed() calls.\n-      \/\/ That trace is very chatty.\n-      return;\n-#else\n-      if (!called_by_wait) {\n-        \/\/ If tracing for is_ext_suspend_completed() is enabled, then only\n-        \/\/ trace calls to it from wait_for_ext_suspend_completion()\n-        return;\n-      }\n-#endif\n-    }\n-\n-    if (AssertOnSuspendWaitFailure || TraceSuspendWaitFailures) {\n-      if (bits != NULL && (*bits & DEBUG_FALSE_BITS) != 0) {\n-        MutexLocker ml(Threads_lock);  \/\/ needed for get_thread_name()\n-        ResourceMark rm;\n-\n-        tty->print_cr(\n-                      \"Failed wait_for_ext_suspend_completion(thread=%s, debug_bits=%x)\",\n-                      jt->get_thread_name(), *bits);\n-\n-        guarantee(!AssertOnSuspendWaitFailure, \"external suspend wait failed\");\n-      }\n-    }\n-  }\n-};\n-#undef DEBUG_FALSE_BITS\n-\n-\n-bool JavaThread::is_ext_suspend_completed(bool called_by_wait, int delay,\n-                                          uint32_t *bits) {\n-  TraceSuspendDebugBits tsdb(this, false \/* !is_wait *\/, called_by_wait, bits);\n-\n+bool JavaThread::is_ext_suspend_completed() {\n@@ -625,2 +545,0 @@\n-  *bits |= 0x00000001;\n-\n@@ -633,1 +551,0 @@\n-      *bits |= 0x00000100;\n@@ -641,1 +558,0 @@\n-      *bits |= 0x00000200;\n@@ -647,1 +563,0 @@\n-      *bits |= 0x00000400;\n@@ -675,1 +590,0 @@\n-      *bits |= 0x00001000;\n@@ -682,2 +596,1 @@\n-      *bits |= 0x00002000;\n-    } else if (!called_by_wait && !did_trans_retry &&\n+    } else if (!did_trans_retry &&\n@@ -691,3 +604,1 @@\n-      \/\/ code check above and the self-suspend. Lucky us. If we were\n-      \/\/ called by wait_for_ext_suspend_completion(), then it\n-      \/\/ will be doing the retries so we don't have to.\n+      \/\/ code check above and the self-suspend.\n@@ -701,2 +612,0 @@\n-      *bits |= 0x00004000;\n-\n@@ -722,1 +631,1 @@\n-          SR_lock()->wait(i * delay);\n+          SR_lock()->wait(i * SuspendRetryDelay);\n@@ -724,1 +633,1 @@\n-          SR_lock()->wait_without_safepoint_check(i * delay);\n+          SR_lock()->wait_without_safepoint_check(i * SuspendRetryDelay);\n@@ -735,2 +644,0 @@\n-\n-\n@@ -740,89 +647,0 @@\n-  *bits |= 0x00000010;\n-  return false;\n-}\n-\n-\/\/ Wait for an external suspend request to complete (or be cancelled).\n-\/\/ Returns true if the thread is externally suspended and false otherwise.\n-\/\/\n-bool JavaThread::wait_for_ext_suspend_completion(int retries, int delay,\n-                                                 uint32_t *bits) {\n-  TraceSuspendDebugBits tsdb(this, true \/* is_wait *\/,\n-                             false \/* !called_by_wait *\/, bits);\n-\n-  \/\/ local flag copies to minimize SR_lock hold time\n-  bool is_suspended;\n-  bool pending;\n-  uint32_t reset_bits;\n-\n-  \/\/ set a marker so is_ext_suspend_completed() knows we are the caller\n-  *bits |= 0x00010000;\n-\n-  \/\/ We use reset_bits to reinitialize the bits value at the top of\n-  \/\/ each retry loop. This allows the caller to make use of any\n-  \/\/ unused bits for their own marking purposes.\n-  reset_bits = *bits;\n-\n-  {\n-    MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-    is_suspended = is_ext_suspend_completed(true \/* called_by_wait *\/,\n-                                            delay, bits);\n-    pending = is_external_suspend();\n-  }\n-  \/\/ must release SR_lock to allow suspension to complete\n-\n-  if (!pending) {\n-    \/\/ A cancelled suspend request is the only false return from\n-    \/\/ is_ext_suspend_completed() that keeps us from entering the\n-    \/\/ retry loop.\n-    *bits |= 0x00020000;\n-    return false;\n-  }\n-\n-  if (is_suspended) {\n-    *bits |= 0x00040000;\n-    return true;\n-  }\n-\n-  for (int i = 1; i <= retries; i++) {\n-    *bits = reset_bits;  \/\/ reinit to only track last retry\n-\n-    \/\/ We used to do an \"os::yield_all(i)\" call here with the intention\n-    \/\/ that yielding would increase on each retry. However, the parameter\n-    \/\/ is ignored on Linux which means the yield didn't scale up. Waiting\n-    \/\/ on the SR_lock below provides a much more predictable scale up for\n-    \/\/ the delay. It also provides a simple\/direct point to check for any\n-    \/\/ safepoint requests from the VMThread\n-\n-    {\n-      Thread* t = Thread::current();\n-      MonitorLocker ml(SR_lock(),\n-                       t->is_Java_thread() ? Mutex::_safepoint_check_flag : Mutex::_no_safepoint_check_flag);\n-      \/\/ wait with safepoint check (if we're a JavaThread - the WatcherThread\n-      \/\/ can also call this)  and increase delay with each retry\n-      ml.wait(i * delay);\n-\n-      is_suspended = is_ext_suspend_completed(true \/* called_by_wait *\/,\n-                                              delay, bits);\n-\n-      \/\/ It is possible for the external suspend request to be cancelled\n-      \/\/ (by a resume) before the actual suspend operation is completed.\n-      \/\/ Refresh our local copy to see if we still need to wait.\n-      pending = is_external_suspend();\n-    }\n-\n-    if (!pending) {\n-      \/\/ A cancelled suspend request is the only false return from\n-      \/\/ is_ext_suspend_completed() that keeps us from staying in the\n-      \/\/ retry loop.\n-      *bits |= 0x00080000;\n-      return false;\n-    }\n-\n-    if (is_suspended) {\n-      *bits |= 0x00100000;\n-      return true;\n-    }\n-  } \/\/ end retry loop\n-\n-  \/\/ thread did not suspend after all our retries\n-  *bits |= 0x00200000;\n@@ -832,31 +650,0 @@\n-\/\/ Called from API entry points which perform stack walking. If the\n-\/\/ associated JavaThread is the current thread, then wait_for_suspend\n-\/\/ is not used. Otherwise, it determines if we should wait for the\n-\/\/ \"other\" thread to complete external suspension. (NOTE: in future\n-\/\/ releases the suspension mechanism should be reimplemented so this\n-\/\/ is not necessary.)\n-\/\/\n-bool\n-JavaThread::is_thread_fully_suspended(bool wait_for_suspend, uint32_t *bits) {\n-  if (this != Thread::current()) {\n-    \/\/ \"other\" threads require special handling.\n-    if (wait_for_suspend) {\n-      \/\/ We are allowed to wait for the external suspend to complete\n-      \/\/ so give the other thread a chance to get suspended.\n-      if (!wait_for_ext_suspend_completion(SuspendRetryCount,\n-                                           SuspendRetryDelay, bits)) {\n-        \/\/ Didn't make it so let the caller know.\n-        return false;\n-      }\n-    }\n-    \/\/ We aren't allowed to wait for the external suspend to complete\n-    \/\/ so if the other thread isn't externally suspended we need to\n-    \/\/ let the caller know.\n-    else if (!is_ext_suspend_completed_with_lock(bits)) {\n-      return false;\n-    }\n-  }\n-\n-  return true;\n-}\n-\n@@ -1114,0 +901,1 @@\n+char java_version[64] = \"\";\n@@ -1119,0 +907,22 @@\n+\/\/ extract the JRE version string from java.lang.VersionProps.java_version\n+static const char* get_java_version(TRAPS) {\n+  Klass* k = SystemDictionary::find(vmSymbols::java_lang_VersionProps(),\n+                                    Handle(), Handle(), CHECK_AND_CLEAR_NULL);\n+  fieldDescriptor fd;\n+  bool found = k != NULL &&\n+               InstanceKlass::cast(k)->find_local_field(vmSymbols::java_version_name(),\n+                                                        vmSymbols::string_signature(), &fd);\n+  if (found) {\n+    oop name_oop = k->java_mirror()->obj_field(fd.offset());\n+    if (name_oop == NULL) {\n+      return NULL;\n+    }\n+    const char* name = java_lang_String::as_utf8_string(name_oop,\n+                                                        java_version,\n+                                                        sizeof(java_version));\n+    return name;\n+  } else {\n+    return NULL;\n+  }\n+}\n+\n@@ -1710,1 +1520,1 @@\n-  _deferred_locals_updates(nullptr),\n+  _jvmti_deferred_updates(nullptr),\n@@ -1911,2 +1721,2 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* deferred = deferred_locals();\n-  if (deferred != NULL) {\n+  JvmtiDeferredUpdates* updates = deferred_updates();\n+  if (updates != NULL) {\n@@ -1914,8 +1724,4 @@\n-    assert(deferred->length() != 0, \"empty array!\");\n-    do {\n-      jvmtiDeferredLocalVariableSet* dlv = deferred->at(0);\n-      deferred->remove_at(0);\n-      \/\/ individual jvmtiDeferredLocalVariableSet are CHeapObj's\n-      delete dlv;\n-    } while (deferred->length() != 0);\n-    delete deferred;\n+    assert(updates->count() > 0, \"Updates holder not deleted\");\n+    \/\/ free deferred updates.\n+    delete updates;\n+    set_deferred_updates(NULL);\n@@ -2413,0 +2219,5 @@\n+  if (is_obj_deopt_suspend()) {\n+    frame_anchor()->make_walkable(this);\n+    wait_for_object_deoptimization();\n+  }\n+\n@@ -2494,1 +2305,1 @@\n-    uint32_t debug_bits = 0;\n+\n@@ -2498,2 +2309,1 @@\n-    if (is_ext_suspend_completed(false \/* !called_by_wait *\/,\n-                                 SuspendRetryDelay, &debug_bits)) {\n+    if (is_ext_suspend_completed()) {\n@@ -2553,3 +2363,1 @@\n-    \/\/ flag is not cleared until we set the ext_suspended flag so\n-    \/\/ that wait_for_ext_suspend_completion() returns consistent\n-    \/\/ results.\n+    \/\/ flag is not cleared until we set the ext_suspended flag.\n@@ -2613,3 +2421,0 @@\n-  \/\/ Since we are not using a regular thread-state transition helper here,\n-  \/\/ we must manually emit the instruction barrier after leaving a safe state.\n-  OrderAccess::cross_modify_fence();\n@@ -2621,0 +2426,57 @@\n+\/\/ Wait for another thread to perform object reallocation and relocking on behalf of\n+\/\/ this thread.\n+\/\/ This method is very similar to JavaThread::java_suspend_self_with_safepoint_check()\n+\/\/ and has the same callers. It also performs a raw thread state transition to\n+\/\/ _thread_blocked and back again to the original state before returning. The current\n+\/\/ thread is required to change to _thread_blocked in order to be seen to be\n+\/\/ safepoint\/handshake safe whilst suspended and only after becoming handshake safe,\n+\/\/ the other thread can complete the handshake used to synchronize with this thread\n+\/\/ and then perform the reallocation and relocking. We cannot use the thread state\n+\/\/ transition helpers because we arrive here in various states and also because the\n+\/\/ helpers indirectly call this method.  After leaving _thread_blocked we have to\n+\/\/ check for safepoint\/handshake, except if _thread_in_native. The thread is safe\n+\/\/ without blocking then. Allowed states are enumerated in\n+\/\/ SafepointSynchronize::block(). See also EscapeBarrier::sync_and_suspend_*()\n+\n+void JavaThread::wait_for_object_deoptimization() {\n+  assert(!has_last_Java_frame() || frame_anchor()->walkable(), \"should have walkable stack\");\n+  assert(this == Thread::current(), \"invariant\");\n+  JavaThreadState state = thread_state();\n+\n+  bool spin_wait = os::is_MP();\n+  do {\n+    set_thread_state(_thread_blocked);\n+    \/\/ Check if _external_suspend was set in the previous loop iteration.\n+    if (is_external_suspend()) {\n+      java_suspend_self();\n+    }\n+    \/\/ Wait for object deoptimization if requested.\n+    if (spin_wait) {\n+      \/\/ A single deoptimization is typically very short. Microbenchmarks\n+      \/\/ showed 5% better performance when spinning.\n+      const uint spin_limit = 10 * SpinYield::default_spin_limit;\n+      SpinYield spin(spin_limit);\n+      for (uint i = 0; is_obj_deopt_suspend() && i < spin_limit; i++) {\n+        spin.wait();\n+      }\n+      \/\/ Spin just once\n+      spin_wait = false;\n+    } else {\n+      MonitorLocker ml(this, EscapeBarrier_lock, Monitor::_no_safepoint_check_flag);\n+      if (is_obj_deopt_suspend()) {\n+        ml.wait();\n+      }\n+    }\n+    \/\/ The current thread could have been suspended again. We have to check for\n+    \/\/ suspend after restoring the saved state. Without this the current thread\n+    \/\/ might return to _thread_in_Java and execute bytecode.\n+    set_thread_state_fence(state);\n+\n+    if (state != _thread_in_native) {\n+      SafepointMechanism::process_if_requested(this);\n+    }\n+    \/\/ A handshake for obj. deoptimization suspend could have been processed so\n+    \/\/ we must check after processing.\n+  } while (is_obj_deopt_suspend() || is_external_suspend());\n+}\n+\n@@ -2650,0 +2512,4 @@\n+  if (thread->is_obj_deopt_suspend()) {\n+    thread->wait_for_object_deoptimization();\n+  }\n+\n@@ -2677,22 +2543,0 @@\n-\/\/ This is a variant of the normal\n-\/\/ check_special_condition_for_native_trans with slightly different\n-\/\/ semantics for use by critical native wrappers.  It does all the\n-\/\/ normal checks but also performs the transition back into\n-\/\/ thread_in_Java state.  This is required so that critical natives\n-\/\/ can potentially block and perform a GC if they are the last thread\n-\/\/ exiting the GCLocker.\n-void JavaThread::check_special_condition_for_native_trans_and_transition(JavaThread *thread) {\n-  check_special_condition_for_native_trans(thread);\n-\n-  Thread::WXWriteFromExecSetter wx_write;\n-\n-  \/\/ Finish the transition\n-  thread->set_thread_state(_thread_in_Java);\n-\n-  if (thread->do_critical_native_unlock()) {\n-    ThreadInVMfromJavaNoAsyncException tiv(thread);\n-    GCLocker::unlock_critical(thread);\n-    thread->clear_critical_native_unlock();\n-  }\n-}\n-\n@@ -2817,1 +2661,1 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = deferred_locals();\n+  GrowableArray<jvmtiDeferredLocalVariableSet*>* list = JvmtiDeferredUpdates::deferred_locals(this);\n@@ -3571,0 +3415,1 @@\n+  JDK_Version::set_java_version(get_java_version(THREAD));\n@@ -4404,0 +4249,3 @@\n+\n+  \/\/ Make new thread known to active EscapeBarrier\n+  EscapeBarrier::thread_added(p);\n@@ -4444,0 +4292,3 @@\n+\n+    \/\/ Notify threads waiting in EscapeBarriers\n+    EscapeBarrier::thread_removed(p);\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":114,"deletions":263,"binary":false,"changes":377,"status":"modified"},{"patch":"@@ -86,1 +86,1 @@\n-class jvmtiDeferredLocalVariableSet;\n+class JvmtiDeferredUpdates;\n@@ -304,2 +304,2 @@\n-    _critical_native_unlock = 0x00000002U, \/\/ Must call back to unlock JNI critical lock\n-    _trace_flag             = 0x00000004U  \/\/ call tracing backend\n+    _trace_flag             = 0x00000004U, \/\/ call tracing backend\n+    _obj_deopt              = 0x00000008U  \/\/ suspend for object reallocation and relocking for JVMTI agent\n@@ -314,2 +314,0 @@\n-  int _num_nested_signal;\n-\n@@ -319,4 +317,0 @@\n-  void enter_signal_handler() { _num_nested_signal++; }\n-  void leave_signal_handler() { _num_nested_signal--; }\n-  bool is_inside_signal_handler() const { return _num_nested_signal > 0; }\n-\n@@ -551,5 +545,0 @@\n-  bool do_critical_native_unlock() const { return (_suspend_flags & _critical_native_unlock) != 0; }\n-\n-  inline void set_critical_native_unlock();\n-  inline void clear_critical_native_unlock();\n-\n@@ -559,0 +548,3 @@\n+  inline void set_obj_deopt_flag();\n+  inline void clear_obj_deopt_flag();\n+\n@@ -633,0 +625,2 @@\n+  bool is_obj_deopt_suspend()           { return (_suspend_flags & _obj_deopt) != 0; }\n+\n@@ -1143,5 +1137,4 @@\n-  \/\/ Because deoptimization is lazy we must save jvmti requests to set locals\n-  \/\/ in compiled frames until we deoptimize and we have an interpreter frame.\n-  \/\/ This holds the pointer to array (yeah like there might be more than one) of\n-  \/\/ description of compiled vframes that have locals that need to be updated.\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* _deferred_locals_updates;\n+  \/\/ Holds updates by JVMTI agents for compiled frames that cannot be performed immediately. They\n+  \/\/ will be carried out as soon as possible which, in most cases, is just before deoptimization of\n+  \/\/ the frame, when control returns to it.\n+  JvmtiDeferredUpdates* _jvmti_deferred_updates;\n@@ -1244,1 +1237,2 @@\n-  \/\/ uniquely identify the  speculative optimization guarded by the uncommon trap\n+  \/\/ uniquely identify the speculative optimization guarded by an uncommon trap.\n+  \/\/ See JVMCINMethodData::SPECULATION_LENGTH_BITS for further details.\n@@ -1439,0 +1433,4 @@\n+  \/\/ Synchronize with another thread that is deoptimizing objects of the\n+  \/\/ current thread, i.e. reverts optimizations based on escape analysis.\n+  void wait_for_object_deoptimization();\n+\n@@ -1463,25 +1461,1 @@\n-  \/\/ Same as check_special_condition_for_native_trans but finishes the\n-  \/\/ transition into thread_in_Java mode so that it can potentially\n-  \/\/ block.\n-  static void check_special_condition_for_native_trans_and_transition(JavaThread *thread);\n-\n-  bool is_ext_suspend_completed(bool called_by_wait, int delay, uint32_t *bits);\n-  bool is_ext_suspend_completed_with_lock(uint32_t *bits) {\n-    MutexLocker ml(SR_lock(), Mutex::_no_safepoint_check_flag);\n-    \/\/ Warning: is_ext_suspend_completed() may temporarily drop the\n-    \/\/ SR_lock to allow the thread to reach a stable thread state if\n-    \/\/ it is currently in a transient thread state.\n-    return is_ext_suspend_completed(false \/* !called_by_wait *\/,\n-                                    SuspendRetryDelay, bits);\n-  }\n-\n-  \/\/ We cannot allow wait_for_ext_suspend_completion() to run forever or\n-  \/\/ we could hang. SuspendRetryCount and SuspendRetryDelay are normally\n-  \/\/ passed as the count and delay parameters. Experiments with specific\n-  \/\/ calls to wait_for_ext_suspend_completion() can be done by passing\n-  \/\/ other values in the code. Experiments with all calls can be done\n-  \/\/ via the appropriate -XX options.\n-  bool wait_for_ext_suspend_completion(int count, int delay, uint32_t *bits);\n-\n-  \/\/ test for suspend - most (all?) of these should go away\n-  bool is_thread_fully_suspended(bool wait_for_suspend, uint32_t *bits);\n+  bool is_ext_suspend_completed();\n@@ -1498,1 +1472,1 @@\n-    return (_suspend_flags & (_external_suspend JFR_ONLY(| _trace_flag))) != 0;\n+    return (_suspend_flags & (_external_suspend | _obj_deopt JFR_ONLY(| _trace_flag))) != 0;\n@@ -1571,1 +1545,1 @@\n-            is_external_suspend() || is_trace_suspend();\n+            is_external_suspend() || is_trace_suspend() || is_obj_deopt_suspend();\n@@ -1588,2 +1562,2 @@\n-  GrowableArray<jvmtiDeferredLocalVariableSet*>* deferred_locals() const { return _deferred_locals_updates; }\n-  void set_deferred_locals(GrowableArray<jvmtiDeferredLocalVariableSet *>* vf) { _deferred_locals_updates = vf; }\n+  JvmtiDeferredUpdates* deferred_updates() const      { return _jvmti_deferred_updates; }\n+  void set_deferred_updates(JvmtiDeferredUpdates* du) { _jvmti_deferred_updates = du; }\n@@ -2224,14 +2198,0 @@\n-class SignalHandlerMark: public StackObj {\n- private:\n-  Thread* _thread;\n- public:\n-  SignalHandlerMark(Thread* t) {\n-    _thread = t;\n-    if (_thread) _thread->enter_signal_handler();\n-  }\n-  ~SignalHandlerMark() {\n-    if (_thread) _thread->leave_signal_handler();\n-    _thread = NULL;\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":23,"deletions":63,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -113,0 +113,3 @@\n+#ifdef LINUX\n+  DCmdFactory::register_DCmdFactory(new DCmdFactoryImpl<PerfMapDCmd>(full_export, true, false));\n+#endif \/\/ LINUX\n@@ -279,1 +282,1 @@\n-  int ret = WriteableFlags::set_flag(_flag.value(), val, JVMFlag::MANAGEMENT, err_msg);\n+  int ret = WriteableFlags::set_flag(_flag.value(), val, JVMFlagOrigin::MANAGEMENT, err_msg);\n@@ -304,0 +307,1 @@\n+#if INCLUDE_JVMTI\n@@ -363,0 +367,1 @@\n+#endif \/\/ INCLUDE_JVMTI\n@@ -896,0 +901,6 @@\n+#ifdef LINUX\n+void PerfMapDCmd::execute(DCmdSource source, TRAPS) {\n+  CodeCache::write_perf_map();\n+}\n+#endif \/\/ LINUX\n+\n","filename":"src\/hotspot\/share\/services\/diagnosticCommand.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -1325,13 +1325,0 @@\n-void VMError::report_and_die(const char* message, const char* detail_fmt, ...)\n-{\n-  va_list detail_args;\n-  va_start(detail_args, detail_fmt);\n-  report_and_die(INTERNAL_ERROR, message, detail_fmt, detail_args, NULL, NULL, NULL, NULL, NULL, 0, 0);\n-  va_end(detail_args);\n-}\n-\n-void VMError::report_and_die(const char* message)\n-{\n-  report_and_die(message, \"%s\", \"\");\n-}\n-\n@@ -1437,1 +1424,2 @@\n-    \/\/ If UseOsErrorReporting we call this for each level of the call stack\n+#if defined(_WINDOWS)\n+    \/\/ If UseOSErrorReporting we call this for each level of the call stack\n@@ -1441,0 +1429,1 @@\n+#endif\n@@ -1630,1 +1619,1 @@\n-  if (!UseOSErrorReporting) {\n+  if (WINDOWS_ONLY(!UseOSErrorReporting) NOT_WINDOWS(true)) {\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":4,"deletions":15,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -66,0 +66,2 @@\n+#elif defined (__KERNEL__)\n+#  include <linux\/types.h>\n@@ -111,1 +113,1 @@\n-#define HB_UNTAG(tag)   (uint8_t)(((tag)>>24)&0xFF), (uint8_t)(((tag)>>16)&0xFF), (uint8_t)(((tag)>>8)&0xFF), (uint8_t)((tag)&0xFF)\n+#define HB_UNTAG(tag)   (char)(((tag)>>24)&0xFF), (char)(((tag)>>16)&0xFF), (char)(((tag)>>8)&0xFF), (char)((tag)&0xFF)\n@@ -360,0 +362,16 @@\n+  \/*\n+   * Since 2.4.0\n+   *\/\n+  \/*12.0*\/HB_SCRIPT_ELYMAIC                     = HB_TAG ('E','l','y','m'),\n+  \/*12.0*\/HB_SCRIPT_NANDINAGARI                 = HB_TAG ('N','a','n','d'),\n+  \/*12.0*\/HB_SCRIPT_NYIAKENG_PUACHUE_HMONG      = HB_TAG ('H','m','n','p'),\n+  \/*12.0*\/HB_SCRIPT_WANCHO                      = HB_TAG ('W','c','h','o'),\n+\n+  \/*\n+   * Since 2.6.7\n+   *\/\n+  \/*13.0*\/HB_SCRIPT_CHORASMIAN                  = HB_TAG ('C','h','r','s'),\n+  \/*13.0*\/HB_SCRIPT_DIVES_AKURU                 = HB_TAG ('D','i','a','k'),\n+  \/*13.0*\/HB_SCRIPT_KHITAN_SMALL_SCRIPT         = HB_TAG ('K','i','t','s'),\n+  \/*13.0*\/HB_SCRIPT_YEZIDI                      = HB_TAG ('Y','e','z','i'),\n+\n@@ -418,0 +436,15 @@\n+\/**\n+ * hb_feature_t:\n+ * @tag: a feature tag\n+ * @value: 0 disables the feature, non-zero (usually 1) enables the feature.\n+ * For features implemented as lookup type 3 (like 'salt') the @value is a one\n+ * based index into the alternates.\n+ * @start: the cluster to start applying this feature setting (inclusive).\n+ * @end: the cluster to end applying this feature setting (exclusive).\n+ *\n+ * The #hb_feature_t is the structure that holds information about requested\n+ * feature application. The feature will be applied with the given value to all\n+ * glyphs which are in clusters between @start (inclusive) and @end (exclusive).\n+ * Setting start to @HB_FEATURE_GLOBAL_START and end to @HB_FEATURE_GLOBAL_END\n+ * specifies that the feature always applies to the entire buffer.\n+ *\/\n@@ -462,7 +495,2 @@\n-\/**\n- * hb_color_get_alpha:\n- *\n- *\n- *\n- * Since: 2.1.0\n- *\/\n+HB_EXTERN uint8_t\n+hb_color_get_alpha (hb_color_t color);\n@@ -470,7 +498,3 @@\n-\/**\n- * hb_color_get_red:\n- *\n- *\n- *\n- * Since: 2.1.0\n- *\/\n+\n+HB_EXTERN uint8_t\n+hb_color_get_red (hb_color_t color);\n@@ -478,7 +502,3 @@\n-\/**\n- * hb_color_get_green:\n- *\n- *\n- *\n- * Since: 2.1.0\n- *\/\n+\n+HB_EXTERN uint8_t\n+hb_color_get_green (hb_color_t color);\n@@ -486,8 +506,3 @@\n-\/**\n- * hb_color_get_blue:\n- *\n- *\n- *\n- * Since: 2.1.0\n- *\/\n-#define hb_color_get_blue(color)        (((color) >> 24) & 0xFF)\n+HB_EXTERN uint8_t\n+hb_color_get_blue (hb_color_t color);\n+#define hb_color_get_blue(color)        (((color) >> 24) & 0xFF)\n","filename":"src\/java.desktop\/share\/native\/libharfbuzz\/hb-common.h","additions":45,"deletions":30,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -30,0 +30,3 @@\n+\n+#ifdef HAVE_CORETEXT\n+\n@@ -49,18 +52,0 @@\n-static CGFloat\n-coretext_font_size_from_ptem (float ptem)\n-{\n-  \/* CoreText points are CSS pixels (96 per inch),\n-   * NOT typographic points (72 per inch).\n-   *\n-   * https:\/\/developer.apple.com\/library\/content\/documentation\/GraphicsAnimation\/Conceptual\/HighResolutionOSX\/Explained\/Explained.html\n-   *\/\n-  ptem *= 96.f \/ 72.f;\n-  return ptem <= 0.f ? HB_CORETEXT_DEFAULT_FONT_SIZE : ptem;\n-}\n-static float\n-coretext_font_size_to_ptem (CGFloat size)\n-{\n-  size *= 72.f \/ 96.f;\n-  return size <= 0.f ? 0 : size;\n-}\n-\n@@ -75,1 +60,1 @@\n-reference_table  (hb_face_t *face HB_UNUSED, hb_tag_t tag, void *user_data)\n+_hb_cg_reference_table (hb_face_t *face HB_UNUSED, hb_tag_t tag, void *user_data)\n@@ -174,1 +159,1 @@\n-#if MAC_OS_X_VERSION_MIN_REQUIRED < 1080\n+#if !(defined(TARGET_OS_IPHONE) && TARGET_OS_IPHONE) && MAC_OS_X_VERSION_MIN_REQUIRED < 1080\n@@ -219,1 +204,1 @@\n-#if TARGET_OS_OSX && MAC_OS_X_VERSION_MIN_REQUIRED < 1060\n+#if !(defined(TARGET_OS_IPHONE) && TARGET_OS_IPHONE) && MAC_OS_X_VERSION_MIN_REQUIRED < 1060\n@@ -249,1 +234,1 @@\n-#if TARGET_OS_OSX && MAC_OS_X_VERSION_MIN_REQUIRED < 1060\n+#if !(defined(TARGET_OS_IPHONE) && TARGET_OS_IPHONE) && MAC_OS_X_VERSION_MIN_REQUIRED < 1060\n@@ -298,0 +283,11 @@\n+\/**\n+ * hb_coretext_face_create:\n+ * @cg_font: The CGFontRef to work upon\n+ *\n+ * Creates an #hb_face_t face object from the specified\n+ * CGFontRef.\n+ *\n+ * Return value: the new #hb_face_t face object\n+ *\n+ * Since: 0.9.10\n+ *\/\n@@ -301,1 +297,1 @@\n-  return hb_face_create_for_tables (reference_table, CGFontRetain (cg_font), _hb_cg_font_release);\n+  return hb_face_create_for_tables (_hb_cg_reference_table, CGFontRetain (cg_font), _hb_cg_font_release);\n@@ -304,1 +300,9 @@\n-\/*\n+\/**\n+ * hb_coretext_face_get_cg_font:\n+ * @face: The #hb_face_t to work upon\n+ *\n+ * Fetches the CGFontRef associated with an #hb_face_t\n+ * face object\n+ *\n+ * Return value: the CGFontRef found\n+ *\n@@ -322,1 +326,2 @@\n-  CTFontRef ct_font = create_ct_font (cg_font, coretext_font_size_from_ptem (font->ptem));\n+  CGFloat font_size = (CGFloat) (font->ptem <= 0.f ? HB_CORETEXT_DEFAULT_FONT_SIZE : font->ptem);\n+  CTFontRef ct_font = create_ct_font (cg_font, font_size);\n@@ -346,1 +351,1 @@\n-  if (fabs (CTFontGetSize((CTFontRef) data) - coretext_font_size_from_ptem (font->ptem)) > .5)\n+  if (fabs (CTFontGetSize ((CTFontRef) data) - (CGFloat) font->ptem) > .5)\n@@ -370,2 +375,9 @@\n-\n-\/*\n+\/**\n+ * hb_coretext_font_create:\n+ * @ct_font: The CTFontRef to work upon\n+ *\n+ * Creates an #hb_font_t font object from the specified\n+ * CTFontRef.\n+ *\n+ * Return value: the new #hb_font_t font object\n+ *\n@@ -373,1 +385,1 @@\n- *\/\n+ **\/\n@@ -386,1 +398,1 @@\n-  hb_font_set_ptem (font, coretext_font_size_to_ptem (CTFontGetSize(ct_font)));\n+  hb_font_set_ptem (font, CTFontGetSize (ct_font));\n@@ -394,0 +406,11 @@\n+\/**\n+ * hb_coretext_face_get_ct_font:\n+ * @font: #hb_font_t to work upon\n+ *\n+ * Fetches the CTFontRef associated with the specified\n+ * #hb_font_t font object.\n+ *\n+ * Return value: the CTFontRef found\n+ *\n+ * Since: 0.9.10\n+ *\/\n@@ -415,1 +438,1 @@\n-  static int cmp (const void *pa, const void *pb) {\n+  HB_INTERNAL static int cmp (const void *pa, const void *pb) {\n@@ -433,1 +456,1 @@\n-  static int cmp (const void *pa, const void *pb) {\n+  HB_INTERNAL static int cmp (const void *pa, const void *pb) {\n@@ -494,0 +517,3 @@\n+      active_feature_t feature;\n+\n+#if MAC_OS_X_VERSION_MIN_REQUIRED < 101000\n@@ -498,1 +524,0 @@\n-      active_feature_t feature;\n@@ -501,0 +526,4 @@\n+#else\n+      feature.rec.feature = features[i].tag;\n+      feature.rec.setting = features[i].value;\n+#endif\n@@ -549,0 +578,1 @@\n+#if MAC_OS_X_VERSION_MIN_REQUIRED < 101000\n@@ -557,0 +587,11 @@\n+#else\n+            char tag[5] = {HB_UNTAG (active_features[j].rec.feature)};\n+            CFTypeRef keys[] = {\n+              kCTFontOpenTypeFeatureTag,\n+              kCTFontOpenTypeFeatureValue\n+            };\n+            CFTypeRef values[] = {\n+              CFStringCreateWithCString (kCFAllocatorDefault, tag, kCFStringEncodingASCII),\n+              CFNumberCreate (kCFAllocatorDefault, kCFNumberIntType, &active_features[j].rec.setting)\n+            };\n+#endif\n@@ -603,1 +644,1 @@\n-          active_features.remove (feature - active_features.arrayZ ());\n+          active_features.remove (feature - active_features.arrayZ);\n@@ -613,1 +654,1 @@\n-  { \\\n+  do { \\\n@@ -622,1 +663,1 @@\n-  }\n+  } while (0)\n@@ -624,1 +665,1 @@\n-  ALLOCATE_ARRAY (UniChar, pchars, buffer->len * 2, \/*nothing*\/);\n+  ALLOCATE_ARRAY (UniChar, pchars, buffer->len * 2, ((void)nullptr) \/*nothing*\/);\n@@ -638,1 +679,1 @@\n-  ALLOCATE_ARRAY (unsigned int, log_clusters, chars_len, \/*nothing*\/);\n+  ALLOCATE_ARRAY (unsigned int, log_clusters, chars_len, ((void)nullptr) \/*nothing*\/);\n@@ -654,1 +695,1 @@\n-  } HB_STMT_END;\n+  } HB_STMT_END\n@@ -716,1 +757,1 @@\n-#if MAC_OS_X_VERSION_MIN_REQUIRED < 1090\n+#if !(defined(TARGET_OS_IPHONE) && TARGET_OS_IPHONE) && MAC_OS_X_VERSION_MIN_REQUIRED < 1090\n@@ -776,1 +817,1 @@\n-                                                 MIN (feature.end, chars_len) - feature.start);\n+                                                 hb_min (feature.end, chars_len) - feature.start);\n@@ -788,1 +829,1 @@\n-#if MAC_OS_X_VERSION_MIN_REQUIRED < 1060\n+#if !(defined(TARGET_OS_IPHONE) && TARGET_OS_IPHONE) && MAC_OS_X_VERSION_MIN_REQUIRED < 1060\n@@ -982,1 +1023,1 @@\n-  scratch = scratch_saved;\n+  scratch = scratch_saved\n@@ -1074,1 +1115,1 @@\n-      bool backward = HB_DIRECTION_IS_BACKWARD (buffer->props.direction);\n+      HB_UNUSED bool backward = HB_DIRECTION_IS_BACKWARD (buffer->props.direction);\n@@ -1121,1 +1162,1 @@\n-          cluster = MIN (cluster, info[i - 1].cluster);\n+          cluster = hb_min (cluster, info[i - 1].cluster);\n@@ -1130,1 +1171,1 @@\n-          cluster = MIN (cluster, info[i].cluster);\n+          cluster = hb_min (cluster, info[i].cluster);\n@@ -1155,54 +1196,1 @@\n-\/*\n- * AAT shaper\n- *\/\n-\n-\/*\n- * shaper face data\n- *\/\n-\n-struct hb_coretext_aat_face_data_t {};\n-\n-hb_coretext_aat_face_data_t *\n-_hb_coretext_aat_shaper_face_data_create (hb_face_t *face)\n-{\n-  return hb_aat_layout_has_substitution (face) || hb_aat_layout_has_positioning (face) ?\n-         (hb_coretext_aat_face_data_t *) HB_SHAPER_DATA_SUCCEEDED : nullptr;\n-}\n-\n-void\n-_hb_coretext_aat_shaper_face_data_destroy (hb_coretext_aat_face_data_t *data HB_UNUSED)\n-{\n-}\n-\n-\n-\/*\n- * shaper font data\n- *\/\n-\n-struct hb_coretext_aat_font_data_t {};\n-\n-hb_coretext_aat_font_data_t *\n-_hb_coretext_aat_shaper_font_data_create (hb_font_t *font)\n-{\n-  return font->data.coretext ? (hb_coretext_aat_font_data_t *) HB_SHAPER_DATA_SUCCEEDED : nullptr;\n-}\n-\n-void\n-_hb_coretext_aat_shaper_font_data_destroy (hb_coretext_aat_font_data_t *data HB_UNUSED)\n-{\n-}\n-\n-\n-\/*\n- * shaper\n- *\/\n-\n-hb_bool_t\n-_hb_coretext_aat_shape (hb_shape_plan_t    *shape_plan,\n-                        hb_font_t          *font,\n-                        hb_buffer_t        *buffer,\n-                        const hb_feature_t *features,\n-                        unsigned int        num_features)\n-{\n-  return _hb_coretext_shape (shape_plan, font, buffer, features, num_features);\n-}\n+#endif\n","filename":"src\/java.desktop\/share\/native\/libharfbuzz\/hb-coretext.cc","additions":88,"deletions":100,"binary":false,"changes":188,"status":"modified"},{"patch":"@@ -104,1 +104,1 @@\n-    char* base = os::reserve_memory(size, mtThreadStack);\n+    char* base = os::reserve_memory(size, false, mtThreadStack);\n@@ -172,1 +172,1 @@\n-    char* base = os::reserve_memory(size, mtTest);\n+    char* base = os::reserve_memory(size, false, mtTest);\n","filename":"test\/hotspot\/gtest\/runtime\/test_committed_virtualmemory.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}