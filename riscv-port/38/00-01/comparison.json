{"files":[{"patch":"@@ -3249,1 +3249,1 @@\n- * Multiply 128 bit by 128. Unrolled inner loop.\n+ * Multiply 128 bit by 128 bit. Unrolled inner loop.\n@@ -3381,1 +3381,1 @@\n- * Code for BigInteger::multiplyToLen() instrinsic.\n+ * Code for BigInteger::multiplyToLen() intrinsic.\n@@ -3417,1 +3417,1 @@\n-  Label L_multiply_64_or_128, L_done;\n+  Label L_multiply_64_x_64_loop, L_done;\n@@ -3424,1 +3424,1 @@\n-  \/\/ if x and y are both 8 bytes aligend.\n+  \/\/ Check if x and y are both 8-byte aligned.\n@@ -3427,1 +3427,1 @@\n-  beqz(t0, L_multiply_64_or_128);\n+  beqz(t0, L_multiply_64_x_64_loop);\n@@ -3434,2 +3434,2 @@\n-  Label L_second_loop_1;\n-  bind(L_second_loop_1);\n+  Label L_second_loop_unaliged;\n+  bind(L_second_loop_unaliged);\n@@ -3475,1 +3475,1 @@\n-  j(L_second_loop_1);\n+  j(L_second_loop_unaliged);\n@@ -3477,1 +3477,1 @@\n-  bind(L_multiply_64_or_128);\n+  bind(L_multiply_64_x_64_loop);\n@@ -3480,2 +3480,2 @@\n-  Label L_second_loop_2;\n-  beqz(kdx, L_second_loop_2);\n+  Label L_second_loop_aligned;\n+  beqz(kdx, L_second_loop_aligned);\n@@ -3513,1 +3513,1 @@\n-  bind(L_second_loop_2);\n+  bind(L_second_loop_aligned);\n@@ -3561,1 +3561,1 @@\n-  j(L_second_loop_2);\n+  j(L_second_loop_aligned);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":13,"deletions":13,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -2840,3 +2840,0 @@\n-    \/\/ squareToLen algorithm for sizes 1..127 described in java code works\n-    \/\/ faster than multiply_to_len on some CPUs and slower on others, but\n-    \/\/ multiply_to_len shows a bit better overall results\n","filename":"src\/hotspot\/cpu\/riscv\/stubGenerator_riscv.cpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"}]}