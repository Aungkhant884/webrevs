{"files":[{"patch":"@@ -37,0 +37,1 @@\n+#include \"nativeInst_riscv.hpp\"\n@@ -83,0 +84,5 @@\n+  if (UseRVC && is_imm_in_range(imm, 6, 0) && Rd != x0) {\n+    li_c(Rd, imm);\n+    return;\n+  }\n+\n@@ -127,2 +133,2 @@\n-   lui(Rd, up);\n-   addi(Rd, Rd, lo);\n+   lui_nc(Rd, up);\n+   addi_nc(Rd, Rd, lo);\n@@ -132,1 +138,1 @@\n-   addi(Rd, Rd, (int32_t)lower >> 20);\n+   addi_nc(Rd, Rd, (int32_t)lower >> 20);\n@@ -135,1 +141,1 @@\n-   addi(Rd, Rd, lower);\n+   addi_nc(Rd, Rd, lower);\n@@ -138,1 +144,1 @@\n-   addi(Rd, Rd, lower);\n+   addi_nc(Rd, Rd, lower);\n@@ -148,1 +154,1 @@\n-  lui(Rd, upper);\n+  lui_nc(Rd, upper);\n@@ -150,1 +156,1 @@\n-  addiw(Rd, Rd, lower);\n+  addiw_nc(Rd, Rd, lower);\n@@ -153,1 +159,1 @@\n-#define INSN(NAME, REGISTER)                                       \\\n+#define INSN(NAME, REGISTER, C)                                    \\\n@@ -158,1 +164,1 @@\n-      jal(REGISTER, distance);                                     \\\n+      EMIT_MAY_COMPRESS_NAME(C, jal, (REGISTER, distance));        \\\n@@ -162,2 +168,2 @@\n-      movptr_with_offset(temp, dest, offset);                      \\\n-      jalr(REGISTER, temp, offset);                                \\\n+      movptr_with_offset(temp, dest, offset, C);                   \\\n+      EMIT_MAY_COMPRESS_NAME(C, jalr, (REGISTER, temp, offset));   \\\n@@ -167,1 +173,1 @@\n-    jal(REGISTER, l, temp);                                        \\\n+    EMIT_MAY_COMPRESS_NAME(C, jal, (REGISTER, l, temp));           \\\n@@ -170,2 +176,6 @@\n-  INSN(j,   x0);\n-  INSN(jal, x1);\n+  INSN(j,      x0, COMPRESSIBLE);\n+  INSN(jal,    x1, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(j_nc,   x0, NOT_COMPRESSIBLE);\n+  INSN(jal_nc, x1, NOT_COMPRESSIBLE);\n@@ -175,1 +185,1 @@\n-#define INSN(NAME, REGISTER)                                       \\\n+#define INSN(NAME, REGISTER, C)                                    \\\n@@ -177,1 +187,1 @@\n-    jalr(REGISTER, Rs, 0);                                         \\\n+    EMIT_MAY_COMPRESS_NAME(C, jalr, (REGISTER, Rs, 0));            \\\n@@ -180,2 +190,6 @@\n-  INSN(jr,   x0);\n-  INSN(jalr, x1);\n+  INSN(jr,      x0, COMPRESSIBLE);\n+  INSN(jalr,    x1, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(jr_nc,   x0, NOT_COMPRESSIBLE);\n+  INSN(jalr_nc, x1, NOT_COMPRESSIBLE);\n@@ -189,13 +203,13 @@\n-#define INSN(NAME, REGISTER)                                      \\\n-  void Assembler::NAME(const address &dest, Register temp) {      \\\n-    assert_cond(dest != NULL);                                    \\\n-    assert(temp != noreg, \"temp must not be empty register!\");    \\\n-    int64_t distance = dest - pc();                               \\\n-    if (is_offset_in_range(distance, 32)) {                       \\\n-      auipc(temp, distance + 0x800);                              \\\n-      jalr(REGISTER, temp, ((int32_t)distance << 20) >> 20);      \\\n-    } else {                                                      \\\n-      int32_t offset = 0;                                         \\\n-      movptr_with_offset(temp, dest, offset);                     \\\n-      jalr(REGISTER, temp, offset);                               \\\n-    }                                                             \\\n+#define INSN(NAME, REGISTER, C)                                                             \\\n+  void Assembler::NAME(const address &dest, Register temp) {                                \\\n+    assert_cond(dest != NULL);                                                              \\\n+    assert(temp != noreg, \"temp must not be empty register!\");                              \\\n+    int64_t distance = dest - pc();                                                         \\\n+    if (is_offset_in_range(distance, 32)) {                                                 \\\n+      auipc(temp, distance + 0x800);                                                        \\\n+      EMIT_MAY_COMPRESS_NAME(C, jalr, (REGISTER, temp, ((int32_t)distance << 20) >> 20));   \\\n+    } else {                                                                                \\\n+      int32_t offset = 0;                                                                   \\\n+      movptr_with_offset(temp, dest, offset, C);                                            \\\n+      EMIT_MAY_COMPRESS_NAME(C, jalr, (REGISTER, temp, offset));                            \\\n+    }                                                                                       \\\n@@ -204,2 +218,6 @@\n-  INSN(call, x1);\n-  INSN(tail, x0);\n+  INSN(call,    x1, COMPRESSIBLE);\n+  INSN(tail,    x0, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(call_nc, x1, NOT_COMPRESSIBLE);\n+  INSN(tail_nc, x0, NOT_COMPRESSIBLE);\n@@ -209,1 +227,1 @@\n-#define INSN(NAME, REGISTER)                                   \\\n+#define INSN(NAME, REGISTER, NAME_NC)                          \\\n@@ -214,1 +232,1 @@\n-        NAME(adr.target(), temp);                              \\\n+        NAME_NC(adr.target(), temp);                           \\\n@@ -228,4 +246,8 @@\n-  INSN(j,    x0);\n-  INSN(jal,  x1);\n-  INSN(call, x1);\n-  INSN(tail, x0);\n+  INSN(j,      x0, j_nc);\n+  INSN(jal,    x1, jal_nc);\n+  INSN(call,   x1, call_nc);\n+  INSN(tail,   x0, tail_nc);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(j_nc,   x0, j_nc);\n+  INSN(jal_nc, x1, jal_nc);\n@@ -240,1 +262,1 @@\n-    j(L);\n+    j_nc(L);\n@@ -270,1 +292,19 @@\n-void Assembler::movptr_with_offset(Register Rd, address addr, int32_t &offset) {\n+void Assembler::wrap_label(Label &L, j_c_insn insn) {\n+  if (L.is_bound()) {\n+    (this->*insn)(target(L));\n+  } else {\n+    L.add_patch_at(code(), locator());\n+    (this->*insn)(pc());\n+  }\n+}\n+\n+void Assembler::wrap_label(Label &L, Register r, compare_and_branch_c_insn insn) {\n+  if (L.is_bound()) {\n+    (this->*insn)(r, target(L));\n+  } else {\n+    L.add_patch_at(code(), locator());\n+    (this->*insn)(r, pc());\n+  }\n+}\n+\n+void Assembler::movptr_with_offset(Register Rd, address addr, int32_t &offset, bool compressible) {\n@@ -286,2 +326,2 @@\n-  lui(Rd, upper);\n-  addi(Rd, Rd, lower);\n+  EMIT_MAY_COMPRESS_INST(compressible, lui, (Rd, upper));\n+  EMIT_MAY_COMPRESS_INST(compressible, addi, (Rd, Rd, lower));\n@@ -291,1 +331,1 @@\n-  addi(Rd, Rd, (imm64 >> 5) & 0x7ff);\n+  EMIT_MAY_COMPRESS_INST(compressible, addi, (Rd, Rd, (imm64 >> 5) & 0x7ff));\n@@ -298,2 +338,2 @@\n-void Assembler::movptr(Register Rd, uintptr_t imm64) {\n-  movptr(Rd, (address)imm64);\n+void Assembler::movptr(Register Rd, uintptr_t imm64, bool compressible) {\n+  movptr(Rd, (address)imm64, compressible);\n@@ -302,1 +342,1 @@\n-void Assembler::movptr(Register Rd, address addr) {\n+void Assembler::movptr(Register Rd, address addr, bool compressible) {\n@@ -304,2 +344,2 @@\n-  movptr_with_offset(Rd, addr, offset);\n-  addi(Rd, Rd, offset);\n+  movptr_with_offset(Rd, addr, offset, compressible);\n+  EMIT_MAY_COMPRESS_INST(compressible, addi, (Rd, Rd, offset));\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.cpp","additions":89,"deletions":49,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -258,0 +258,1 @@\n+#include \"assembler_riscv_cext.hpp\"\n@@ -307,3 +308,3 @@\n-  void movptr(Register Rd, address addr);\n-  void movptr_with_offset(Register Rd, address addr, int32_t &offset);\n-  void movptr(Register Rd, uintptr_t imm64);\n+  void movptr(Register Rd, address addr, bool COMPRESSIBLE = true);\n+  void movptr_with_offset(Register Rd, address addr, int32_t &offset, bool COMPRESSIBLE = true);\n+  void movptr(Register Rd, uintptr_t imm64, bool COMPRESSIBLE = true);\n@@ -382,1 +383,5 @@\n-    emit_int32(0);\n+    if (UseRVC) {\n+      emit_int16(0);\n+    } else {\n+      emit_int32(0);\n+    }\n@@ -385,2 +390,6 @@\n-\/\/ Rigster Instruction\n-#define INSN(NAME, op, funct3, funct7)                          \\\n+\/\/ two C-Ext macros\n+#define COMPRESSIBLE      true\n+#define NOT_COMPRESSIBLE  false\n+\n+\/\/ Register Instruction\n+#define INSN(NAME, op, funct3, funct7, C)                       \\\n@@ -388,0 +397,1 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rd, Rs1, Rs2)                    \\\n@@ -398,28 +408,28 @@\n-  INSN(add,   0b0110011, 0b000, 0b0000000);\n-  INSN(sub,   0b0110011, 0b000, 0b0100000);\n-  INSN(andr,  0b0110011, 0b111, 0b0000000);\n-  INSN(orr,   0b0110011, 0b110, 0b0000000);\n-  INSN(xorr,  0b0110011, 0b100, 0b0000000);\n-  INSN(sll,   0b0110011, 0b001, 0b0000000);\n-  INSN(sra,   0b0110011, 0b101, 0b0100000);\n-  INSN(srl,   0b0110011, 0b101, 0b0000000);\n-  INSN(slt,   0b0110011, 0b010, 0b0000000);\n-  INSN(sltu,  0b0110011, 0b011, 0b0000000);\n-  INSN(addw,  0b0111011, 0b000, 0b0000000);\n-  INSN(subw,  0b0111011, 0b000, 0b0100000);\n-  INSN(sllw,  0b0111011, 0b001, 0b0000000);\n-  INSN(sraw,  0b0111011, 0b101, 0b0100000);\n-  INSN(srlw,  0b0111011, 0b101, 0b0000000);\n-  INSN(mul,   0b0110011, 0b000, 0b0000001);\n-  INSN(mulh,  0b0110011, 0b001, 0b0000001);\n-  INSN(mulhsu,0b0110011, 0b010, 0b0000001);\n-  INSN(mulhu, 0b0110011, 0b011, 0b0000001);\n-  INSN(mulw,  0b0111011, 0b000, 0b0000001);\n-  INSN(div,   0b0110011, 0b100, 0b0000001);\n-  INSN(divu,  0b0110011, 0b101, 0b0000001);\n-  INSN(divw,  0b0111011, 0b100, 0b0000001);\n-  INSN(divuw, 0b0111011, 0b101, 0b0000001);\n-  INSN(rem,   0b0110011, 0b110, 0b0000001);\n-  INSN(remu,  0b0110011, 0b111, 0b0000001);\n-  INSN(remw,  0b0111011, 0b110, 0b0000001);\n-  INSN(remuw, 0b0111011, 0b111, 0b0000001);\n+  INSN(add,   0b0110011, 0b000, 0b0000000, COMPRESSIBLE);\n+  INSN(sub,   0b0110011, 0b000, 0b0100000, COMPRESSIBLE);\n+  INSN(andr,  0b0110011, 0b111, 0b0000000, COMPRESSIBLE);\n+  INSN(orr,   0b0110011, 0b110, 0b0000000, COMPRESSIBLE);\n+  INSN(xorr,  0b0110011, 0b100, 0b0000000, COMPRESSIBLE);\n+  INSN(sll,   0b0110011, 0b001, 0b0000000, NOT_COMPRESSIBLE);\n+  INSN(sra,   0b0110011, 0b101, 0b0100000, NOT_COMPRESSIBLE);\n+  INSN(srl,   0b0110011, 0b101, 0b0000000, NOT_COMPRESSIBLE);\n+  INSN(slt,   0b0110011, 0b010, 0b0000000, NOT_COMPRESSIBLE);\n+  INSN(sltu,  0b0110011, 0b011, 0b0000000, NOT_COMPRESSIBLE);\n+  INSN(addw,  0b0111011, 0b000, 0b0000000, COMPRESSIBLE);\n+  INSN(subw,  0b0111011, 0b000, 0b0100000, COMPRESSIBLE);\n+  INSN(sllw,  0b0111011, 0b001, 0b0000000, NOT_COMPRESSIBLE);\n+  INSN(sraw,  0b0111011, 0b101, 0b0100000, NOT_COMPRESSIBLE);\n+  INSN(srlw,  0b0111011, 0b101, 0b0000000, NOT_COMPRESSIBLE);\n+  INSN(mul,   0b0110011, 0b000, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(mulh,  0b0110011, 0b001, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(mulhsu,0b0110011, 0b010, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(mulhu, 0b0110011, 0b011, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(mulw,  0b0111011, 0b000, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(div,   0b0110011, 0b100, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(divu,  0b0110011, 0b101, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(divw,  0b0111011, 0b100, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(divuw, 0b0111011, 0b101, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(rem,   0b0110011, 0b110, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(remu,  0b0110011, 0b111, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(remw,  0b0111011, 0b110, 0b0000001, NOT_COMPRESSIBLE);\n+  INSN(remuw, 0b0111011, 0b111, 0b0000001, NOT_COMPRESSIBLE);\n@@ -428,1 +438,1 @@\n-  INSN(vsetvl, 0b1010111, 0b111, 0b1000000);\n+  INSN(vsetvl, 0b1010111, 0b111, 0b1000000, NOT_COMPRESSIBLE);\n@@ -440,1 +450,1 @@\n-#define INSN(NAME, op, funct3)                                                                     \\\n+#define INSN(NAME, op, funct3, NAME_NC, C)                                                         \\\n@@ -442,1 +452,0 @@\n-    unsigned insn = 0;                                                                             \\\n@@ -444,0 +453,2 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rd, Rs, offset)                                                     \\\n+    unsigned insn = 0;                                                                             \\\n@@ -460,1 +471,1 @@\n-      movptr_with_offset(Rd, dest, offset);                                                        \\\n+      movptr_with_offset(Rd, dest, offset, C);                                                     \\\n@@ -465,1 +476,1 @@\n-    NAME(Rd, dest);                                                                                \\\n+    NAME_NC(Rd, dest);                                                                             \\\n@@ -471,1 +482,1 @@\n-        NAME(Rd, adr.target());                                                                    \\\n+        NAME_NC(Rd, adr.target());                                                                 \\\n@@ -497,7 +508,7 @@\n-  INSN(lb,  0b0000011, 0b000);\n-  INSN(lbu, 0b0000011, 0b100);\n-  INSN(ld,  0b0000011, 0b011);\n-  INSN(lh,  0b0000011, 0b001);\n-  INSN(lhu, 0b0000011, 0b101);\n-  INSN(lw,  0b0000011, 0b010);\n-  INSN(lwu, 0b0000011, 0b110);\n+  INSN(lb,     0b0000011, 0b000, lb,    NOT_COMPRESSIBLE);\n+  INSN(lbu,    0b0000011, 0b100, lbu,   NOT_COMPRESSIBLE);\n+  INSN(lh,     0b0000011, 0b001, lh,    NOT_COMPRESSIBLE);\n+  INSN(lhu,    0b0000011, 0b101, lhu,   NOT_COMPRESSIBLE);\n+  INSN(lw,     0b0000011, 0b010, lw_nc, COMPRESSIBLE);\n+  INSN(lwu,    0b0000011, 0b110, lwu,   NOT_COMPRESSIBLE);\n+  INSN(ld,     0b0000011, 0b011, ld_nc, COMPRESSIBLE);\n@@ -505,0 +516,3 @@\n+  \/\/ C-Ext: incompressible version\n+  INSN(lw_nc,  0b0000011, 0b010, lw_nc, NOT_COMPRESSIBLE);\n+  INSN(ld_nc,  0b0000011, 0b011, ld_nc, NOT_COMPRESSIBLE);\n@@ -507,1 +521,1 @@\n-#define INSN(NAME, op, funct3)                                                                     \\\n+#define INSN(NAME, op, funct3, NAME_NC, C)                                                         \\\n@@ -509,1 +523,0 @@\n-    unsigned insn = 0;                                                                             \\\n@@ -511,0 +524,2 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rd, Rs, offset)                                                     \\\n+    unsigned insn = 0;                                                                             \\\n@@ -527,1 +542,1 @@\n-      movptr_with_offset(temp, dest, offset);                                                      \\\n+      movptr_with_offset(temp, dest, offset, C);                                                   \\\n@@ -532,1 +547,1 @@\n-    NAME(Rd, dest, temp);                                                                          \\\n+    NAME_NC(Rd, dest, temp);                                                                       \\\n@@ -538,1 +553,1 @@\n-        NAME(Rd, adr.target(), temp);                                                              \\\n+        NAME_NC(Rd, adr.target(), temp);                                                           \\\n@@ -556,2 +571,5 @@\n-  INSN(flw, 0b0000111, 0b010);\n-  INSN(fld, 0b0000111, 0b011);\n+  INSN(flw,    0b0000111, 0b010, flw,    NOT_COMPRESSIBLE);\n+  INSN(fld,    0b0000111, 0b011, fld_nc, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(fld_nc, 0b0000111, 0b011, fld_nc, NOT_COMPRESSIBLE);\n@@ -560,1 +578,1 @@\n-#define INSN(NAME, op, funct3)                                                                           \\\n+#define INSN(NAME, op, funct3, NAME_NC, C)                                                               \\\n@@ -562,1 +580,0 @@\n-    unsigned insn = 0;                                                                                   \\\n@@ -564,0 +581,2 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rs1, Rs2, offset)                                                         \\\n+    unsigned insn = 0;                                                                                   \\\n@@ -586,1 +605,1 @@\n-    NAME(Rs1, Rs2, dest);                                                                                \\\n+    NAME_NC(Rs1, Rs2, dest);                                                                             \\\n@@ -589,6 +608,10 @@\n-  INSN(beq,  0b1100011, 0b000);\n-  INSN(bge,  0b1100011, 0b101);\n-  INSN(bgeu, 0b1100011, 0b111);\n-  INSN(blt,  0b1100011, 0b100);\n-  INSN(bltu, 0b1100011, 0b110);\n-  INSN(bne,  0b1100011, 0b001);\n+  INSN(beq,     0b1100011, 0b000, beq_nc, COMPRESSIBLE);\n+  INSN(bne,     0b1100011, 0b001, bne_nc, COMPRESSIBLE);\n+  INSN(bge,     0b1100011, 0b101, bge,    NOT_COMPRESSIBLE);\n+  INSN(bgeu,    0b1100011, 0b111, bgeu,   NOT_COMPRESSIBLE);\n+  INSN(blt,     0b1100011, 0b100, blt,    NOT_COMPRESSIBLE);\n+  INSN(bltu,    0b1100011, 0b110, bltu,   NOT_COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(beq_nc,  0b1100011, 0b000, beq_nc, NOT_COMPRESSIBLE);\n+  INSN(bne_nc,  0b1100011, 0b001, bne_nc, NOT_COMPRESSIBLE);\n@@ -610,0 +633,4 @@\n+  \/\/ C-Ext: incompressible version\n+  INSN(beq_nc,  bne_nc);\n+  INSN(bne_nc,  beq_nc);\n+\n@@ -612,1 +639,1 @@\n-#define INSN(NAME, REGISTER, op, funct3)                                                                    \\\n+#define INSN(NAME, REGISTER, op, funct3, NAME_NC, C)                                                        \\\n@@ -614,1 +641,0 @@\n-    unsigned insn = 0;                                                                                      \\\n@@ -616,0 +642,2 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rs1, Rs2, offset)                                                            \\\n+    unsigned insn = 0;                                                                                      \\\n@@ -628,1 +656,1 @@\n-    NAME(Rs, dest, temp);                                                                                   \\\n+    NAME_NC(Rs, dest, temp);                                                                                \\\n@@ -631,6 +659,11 @@\n-  INSN(sb,  Register,      0b0100011, 0b000);\n-  INSN(sh,  Register,      0b0100011, 0b001);\n-  INSN(sw,  Register,      0b0100011, 0b010);\n-  INSN(sd,  Register,      0b0100011, 0b011);\n-  INSN(fsw, FloatRegister, 0b0100111, 0b010);\n-  INSN(fsd, FloatRegister, 0b0100111, 0b011);\n+  INSN(sb,     Register,      0b0100011, 0b000, sb,     NOT_COMPRESSIBLE);\n+  INSN(sh,     Register,      0b0100011, 0b001, sh,     NOT_COMPRESSIBLE);\n+  INSN(sw,     Register,      0b0100011, 0b010, sw_nc,  COMPRESSIBLE);\n+  INSN(sd,     Register,      0b0100011, 0b011, sd_nc,  COMPRESSIBLE);\n+  INSN(fsw,    FloatRegister, 0b0100111, 0b010, fsw,    NOT_COMPRESSIBLE);\n+  INSN(fsd,    FloatRegister, 0b0100111, 0b011, fsd_nc, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(sw_nc,  Register,      0b0100011, 0b010, sw_nc,  NOT_COMPRESSIBLE);\n+  INSN(sd_nc,  Register,      0b0100011, 0b011, sd_nc,  NOT_COMPRESSIBLE);\n+  INSN(fsd_nc, FloatRegister, 0b0100111, 0b011, fsd_nc, NOT_COMPRESSIBLE);\n@@ -640,1 +673,1 @@\n-#define INSN(NAME)                                                                                 \\\n+#define INSN(NAME, NAME_NC, C)                                                                     \\\n@@ -650,1 +683,1 @@\n-      movptr_with_offset(temp, dest, offset);                                                      \\\n+      movptr_with_offset(temp, dest, offset, C);                                                   \\\n@@ -659,1 +692,1 @@\n-        NAME(Rs, adr.target(), temp);                                                              \\\n+        NAME_NC(Rs, adr.target(), temp);                                                           \\\n@@ -678,4 +711,8 @@\n-  INSN(sb);\n-  INSN(sh);\n-  INSN(sw);\n-  INSN(sd);\n+  INSN(sb,    sb,    NOT_COMPRESSIBLE);\n+  INSN(sh,    sh,    NOT_COMPRESSIBLE);\n+  INSN(sw,    sw_nc, COMPRESSIBLE);\n+  INSN(sd,    sd_nc, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(sw_nc, sw_nc, NOT_COMPRESSIBLE);\n+  INSN(sd_nc, sd_nc, NOT_COMPRESSIBLE);\n@@ -685,1 +722,1 @@\n-#define INSN(NAME)                                                                                 \\\n+#define INSN(NAME, NAME_NC, C)                                                                     \\\n@@ -694,1 +731,1 @@\n-      movptr_with_offset(temp, dest, offset);                                                      \\\n+      movptr_with_offset(temp, dest, offset, C);                                                   \\\n@@ -702,1 +739,1 @@\n-        NAME(Rs, adr.target(), temp);                                                              \\\n+        NAME_NC(Rs, adr.target(), temp);                                                           \\\n@@ -720,2 +757,5 @@\n-  INSN(fsw);\n-  INSN(fsd);\n+  INSN(fsw,    fsw,    NOT_COMPRESSIBLE);\n+  INSN(fsd,    fsd_nc, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(fsd_nc, fsd_nc, NOT_COMPRESSIBLE);\n@@ -763,1 +803,1 @@\n-#define INSN(NAME, op)                                                                        \\\n+#define INSN(NAME, op, C)                                                                     \\\n@@ -765,1 +805,0 @@\n-    unsigned insn = 0;                                                                        \\\n@@ -767,0 +806,2 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rd, offset)                                                    \\\n+    unsigned insn = 0;                                                                        \\\n@@ -783,2 +824,2 @@\n-      movptr_with_offset(temp, dest, off);                                                    \\\n-      jalr(Rd, temp, off);                                                                    \\\n+      movptr_with_offset(temp, dest, off, C);                                                 \\\n+      EMIT_MAY_COMPRESS_NAME(C, jalr, (Rd, temp, off));                                       \\\n@@ -792,1 +833,4 @@\n-  INSN(jal, 0b1101111);\n+  INSN(jal,    0b1101111, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(jal_nc, 0b1101111, NOT_COMPRESSIBLE);\n@@ -798,1 +842,1 @@\n-#define INSN(NAME, op, funct)                                                              \\\n+#define INSN(NAME, op, funct, C)                                                           \\\n@@ -800,1 +844,0 @@\n-    unsigned insn = 0;                                                                     \\\n@@ -802,0 +845,2 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rd, Rs, offset)                                             \\\n+    unsigned insn = 0;                                                                     \\\n@@ -811,1 +856,4 @@\n-  INSN(jalr, 0b1100111, 0b000);\n+  INSN(jalr,    0b1100111, 0b000, COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(jalr_nc, 0b1100111, 0b000, NOT_COMPRESSIBLE);\n@@ -834,1 +882,1 @@\n-#define INSN(NAME, op, funct3, funct7)                      \\\n+#define INSN(NAME, op, funct3, funct7, C)                   \\\n@@ -836,0 +884,1 @@\n+    EMIT_MAY_COMPRESS(C, NAME)                              \\\n@@ -845,3 +894,3 @@\n-  INSN(fence_i, 0b0001111, 0b001, 0b000000000000);\n-  INSN(ecall,   0b1110011, 0b000, 0b000000000000);\n-  INSN(ebreak,  0b1110011, 0b000, 0b000000000001);\n+  INSN(fence_i, 0b0001111, 0b001, 0b000000000000, NOT_COMPRESSIBLE);\n+  INSN(ecall,   0b1110011, 0b000, 0b000000000000, NOT_COMPRESSIBLE);\n+  INSN(ebreak,  0b1110011, 0b000, 0b000000000001, COMPRESSIBLE);\n@@ -943,1 +992,1 @@\n-#define INSN(NAME, op, funct3)                                                              \\\n+#define INSN(NAME, op, funct3, C)                                                           \\\n@@ -946,0 +995,1 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rd, Rs1, imm)                                                \\\n@@ -955,6 +1005,10 @@\n-  INSN(addi,  0b0010011, 0b000);\n-  INSN(slti,  0b0010011, 0b010);\n-  INSN(addiw, 0b0011011, 0b000);\n-  INSN(and_imm12,  0b0010011, 0b111);\n-  INSN(ori,   0b0010011, 0b110);\n-  INSN(xori,  0b0010011, 0b100);\n+  INSN(addi,      0b0010011, 0b000, COMPRESSIBLE);\n+  INSN(slti,      0b0010011, 0b010, NOT_COMPRESSIBLE);\n+  INSN(addiw,     0b0011011, 0b000, COMPRESSIBLE);\n+  INSN(and_imm12, 0b0010011, 0b111, COMPRESSIBLE);\n+  INSN(ori,       0b0010011, 0b110, NOT_COMPRESSIBLE);\n+  INSN(xori,      0b0010011, 0b100, NOT_COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(addi_nc,   0b0010011, 0b000, NOT_COMPRESSIBLE);\n+  INSN(addiw_nc,  0b0011011, 0b000, NOT_COMPRESSIBLE);\n@@ -981,1 +1035,1 @@\n-#define INSN(NAME, op, funct3, funct6)                                   \\\n+#define INSN(NAME, op, funct3, funct6, C)                                \\\n@@ -984,0 +1038,1 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rd, Rs1, shamt)                           \\\n@@ -994,3 +1049,3 @@\n-  INSN(slli,  0b0010011, 0b001, 0b000000);\n-  INSN(srai,  0b0010011, 0b101, 0b010000);\n-  INSN(srli,  0b0010011, 0b101, 0b000000);\n+  INSN(slli,  0b0010011, 0b001, 0b000000, COMPRESSIBLE);\n+  INSN(srai,  0b0010011, 0b101, 0b010000, COMPRESSIBLE);\n+  INSN(srli,  0b0010011, 0b101, 0b000000, COMPRESSIBLE);\n@@ -1021,1 +1076,1 @@\n-#define INSN(NAME, op)                                                  \\\n+#define INSN(NAME, op, C)                                               \\\n@@ -1023,0 +1078,1 @@\n+    EMIT_MAY_COMPRESS(C, NAME, Rd, imm)                                 \\\n@@ -1032,2 +1088,5 @@\n-  INSN(lui,   0b0110111);\n-  INSN(auipc, 0b0010111);\n+  INSN(lui,    0b0110111, COMPRESSIBLE);\n+  INSN(auipc,  0b0010111, NOT_COMPRESSIBLE);\n+\n+  \/\/ C-Ext: incompressible version\n+  INSN(lui_nc, 0b0110111, NOT_COMPRESSIBLE);\n@@ -1916,2 +1975,0 @@\n-class BiasedLockingCounters;\n-\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":169,"deletions":112,"binary":false,"changes":281,"status":"modified"},{"patch":"@@ -0,0 +1,865 @@\n+\/*\n+ * Copyright (c) 1997, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2021, 2021, Alibaba Group Holding Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef CPU_RISCV_ASSEMBLER_RISCV_CEXT_HPP\n+#define CPU_RISCV_ASSEMBLER_RISCV_CEXT_HPP\n+\n+  \/\/ C-Ext: If an instruction is compressible, then\n+  \/\/   we will implicitly emit a 16-bit compressed instruction instead of the 32-bit\n+  \/\/   instruction in Assembler. All below logic follows Chapter -\n+  \/\/   \"C\" Standard Extension for Compressed Instructions, Version 2.0.\n+  \/\/   We can get code size reduction and performance improvement with this extension,\n+  \/\/   considering the reduction of instruction size and the code density increment.\n+\n+  \/\/ Note:\n+  \/\/   1. When UseRVC is enabled, some of normal instructions will be implicitly\n+  \/\/      changed to its 16-bit version.\n+  \/\/   2. C-Ext's instructions in Assembler always end with '_c' suffix, as 'li_c',\n+  \/\/      but most of time we have no need to explicitly use these instructions.\n+  \/\/      (Although spec says 'c.li', we use 'li_c' to unify related names - see below.\n+  \/\/   3. In some cases, we need to force using one instruction's uncompressed version,\n+  \/\/      for instance code being patched should remain its general and longest version\n+  \/\/      to cover all possible cases, or code requiring a fixed length.\n+  \/\/      So we introduce '_nc' suffix (short for: not compressible) to force an instruction\n+  \/\/      to remain its normal 4-byte version.\n+  \/\/     An example:\n+  \/\/      j() (32-bit) could become j_c() (16-bit) with -XX:+UseRVC if compressible. We could\n+  \/\/      use j_nc() to force it to remain its normal 4-byte version.\n+  \/\/   4. Using -XX:PrintAssemblyOptions=no-aliases could print C-Ext instructions instead of\n+  \/\/      normal ones.\n+  \/\/\n+\n+  \/\/ C-Ext: incompressible version\n+  void j_nc(const address &dest, Register temp = t0);\n+  void j_nc(const Address &adr, Register temp = t0) ;\n+  void j_nc(Label &l, Register temp = t0);\n+  void jal_nc(Label &l, Register temp = t0);\n+  void jal_nc(const address &dest, Register temp = t0);\n+  void jal_nc(const Address &adr, Register temp = t0);\n+  void jr_nc(Register Rs);\n+  void jalr_nc(Register Rs);\n+  void call_nc(const address &dest, Register temp = t0);\n+  void tail_nc(const address &dest, Register temp = t0);\n+\n+  \/\/ C-Ext: extract a 16-bit instruction.\n+  static inline uint16_t extract_c(uint16_t val, unsigned msb, unsigned lsb) {\n+    assert_cond(msb >= lsb && msb <= 15);\n+    unsigned nbits = msb - lsb + 1;\n+    uint16_t mask = (1U << nbits) - 1;\n+    uint16_t result = val >> lsb;\n+    result &= mask;\n+    return result;\n+  }\n+\n+  static inline int16_t sextract_c(uint16_t val, unsigned msb, unsigned lsb) {\n+    assert_cond(msb >= lsb && msb <= 15);\n+    int16_t result = val << (15 - msb);\n+    result >>= (15 - msb + lsb);\n+    return result;\n+  }\n+\n+  \/\/ C-Ext: patch a 16-bit instruction.\n+  static void patch_c(address a, unsigned msb, unsigned lsb, uint16_t val) {\n+    assert_cond(a != NULL);\n+    assert_cond(msb >= lsb && msb <= 15);\n+    unsigned nbits = msb - lsb + 1;\n+    guarantee(val < (1U << nbits), \"Field too big for insn\");\n+    uint16_t mask = (1U << nbits) - 1;\n+    val <<= lsb;\n+    mask <<= lsb;\n+    uint16_t target = *(uint16_t *)a;\n+    target &= ~mask;\n+    target |= val;\n+    *(uint16_t *)a = target;\n+  }\n+\n+  static void patch_c(address a, unsigned bit, uint16_t val) {\n+    patch_c(a, bit, bit, val);\n+  }\n+\n+  \/\/ C-Ext: patch a 16-bit instruction with a general purpose register ranging [0, 31] (5 bits)\n+  static void patch_reg_c(address a, unsigned lsb, Register reg) {\n+    patch_c(a, lsb + 4, lsb, reg->encoding_nocheck());\n+  }\n+\n+  \/\/ C-Ext: patch a 16-bit instruction with a general purpose register ranging [8, 15] (3 bits)\n+  static void patch_compressed_reg_c(address a, unsigned lsb, Register reg) {\n+    patch_c(a, lsb + 2, lsb, reg->compressed_encoding_nocheck());\n+  }\n+\n+  \/\/ C-Ext: patch a 16-bit instruction with a float register ranging [0, 31] (5 bits)\n+  static void patch_reg_c(address a, unsigned lsb, FloatRegister reg) {\n+    patch_c(a, lsb + 4, lsb, reg->encoding_nocheck());\n+  }\n+\n+  \/\/ C-Ext: patch a 16-bit instruction with a float register ranging [8, 15] (3 bits)\n+  static void patch_compressed_reg_c(address a, unsigned lsb, FloatRegister reg) {\n+    patch_c(a, lsb + 2, lsb, reg->compressed_encoding_nocheck());\n+  }\n+\n+public:\n+\n+\/\/ C-Ext: Compressed Instructions\n+\n+\/\/ --------------  C-Ext Instruction Definitions  --------------\n+\n+  void nop_c() {\n+    addi_c(x0, 0);\n+  }\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd_Rs1, int32_t imm) {                                                  \\\n+    assert_cond(is_imm_in_range(imm, 6, 0));                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 6, 2, (imm & right_n_bits(5)));                                  \\\n+    patch_reg_c((address)&insn, 7, Rd_Rs1);                                                  \\\n+    patch_c((address)&insn, 12, 12, (imm & nth_bit(5)) >> 5);                                \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(addi_c,   0b000, 0b01);\n+  INSN(addiw_c,  0b001, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(int32_t imm) {                                                                   \\\n+    assert_cond(is_imm_in_range(imm, 10, 0));                                                \\\n+    assert_cond((imm & 0b1111) == 0);                                                        \\\n+    assert_cond(imm != 0);                                                                   \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 2, 2, (imm & nth_bit(5)) >> 5);                                  \\\n+    patch_c((address)&insn, 4, 3, (imm & right_n_bits(9)) >> 7);                             \\\n+    patch_c((address)&insn, 5, 5, (imm & nth_bit(6)) >> 6);                                  \\\n+    patch_c((address)&insn, 6, 6, (imm & nth_bit(4)) >> 4);                                  \\\n+    patch_reg_c((address)&insn, 7, sp);                                                      \\\n+    patch_c((address)&insn, 12, 12, (imm & nth_bit(9)) >> 9);                                \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(addi16sp_c, 0b011, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, uint32_t uimm) {                                                    \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 10, 0));                                      \\\n+    assert_cond((uimm & 0b11) == 0);                                                         \\\n+    assert_cond(uimm != 0);                                                                  \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_compressed_reg_c((address)&insn, 2, Rd);                                           \\\n+    patch_c((address)&insn, 5, 5, (uimm & nth_bit(3)) >> 3);                                 \\\n+    patch_c((address)&insn, 6, 6, (uimm & nth_bit(2)) >> 2);                                 \\\n+    patch_c((address)&insn, 10, 7, (uimm & right_n_bits(10)) >> 6);                          \\\n+    patch_c((address)&insn, 12, 11, (uimm & right_n_bits(6)) >> 4);                          \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(addi4spn_c, 0b000, 0b00);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd_Rs1, uint32_t shamt) {                                               \\\n+    assert_cond(is_unsigned_imm_in_range(shamt, 6, 0));                                      \\\n+    assert_cond(shamt != 0);                                                                 \\\n+    assert_cond(Rd_Rs1 != x0);                                                               \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 6, 2, (shamt & right_n_bits(5)));                                \\\n+    patch_reg_c((address)&insn, 7, Rd_Rs1);                                                  \\\n+    patch_c((address)&insn, 12, 12, (shamt & nth_bit(5)) >> 5);                              \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(slli_c, 0b000, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, funct2, op)                                                       \\\n+  void NAME(Register Rd_Rs1, uint32_t shamt) {                                               \\\n+    assert_cond(is_unsigned_imm_in_range(shamt, 6, 0));                                      \\\n+    assert_cond(shamt != 0);                                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 6, 2, (shamt & right_n_bits(5)));                                \\\n+    patch_compressed_reg_c((address)&insn, 7, Rd_Rs1);                                       \\\n+    patch_c((address)&insn, 11, 10, funct2);                                                 \\\n+    patch_c((address)&insn, 12, 12, (shamt & nth_bit(5)) >> 5);                              \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(srli_c, 0b100, 0b00, 0b01);\n+  INSN(srai_c, 0b100, 0b01, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, funct2, op)                                                       \\\n+  void NAME(Register Rd_Rs1, int32_t imm) {                                                  \\\n+    assert_cond(is_imm_in_range(imm, 6, 0));                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 6, 2, (imm & right_n_bits(5)));                                  \\\n+    patch_compressed_reg_c((address)&insn, 7, Rd_Rs1);                                       \\\n+    patch_c((address)&insn, 11, 10, funct2);                                                 \\\n+    patch_c((address)&insn, 12, 12, (imm & nth_bit(5)) >> 5);                                \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(andi_c, 0b100, 0b10, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct6, funct2, op)                                                       \\\n+  void NAME(Register Rd_Rs1, Register Rs2) {                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_compressed_reg_c((address)&insn, 2, Rs2);                                          \\\n+    patch_c((address)&insn, 6, 5, funct2);                                                   \\\n+    patch_compressed_reg_c((address)&insn, 7, Rd_Rs1);                                       \\\n+    patch_c((address)&insn, 15, 10, funct6);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(sub_c,  0b100011, 0b00, 0b01);\n+  INSN(xor_c,  0b100011, 0b01, 0b01);\n+  INSN(or_c,   0b100011, 0b10, 0b01);\n+  INSN(and_c,  0b100011, 0b11, 0b01);\n+  INSN(subw_c, 0b100111, 0b00, 0b01);\n+  INSN(addw_c, 0b100111, 0b01, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct4, op)                                                               \\\n+  void NAME(Register Rd_Rs1, Register Rs2) {                                                 \\\n+    assert_cond(Rd_Rs1 != x0);                                                               \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_reg_c((address)&insn, 2, Rs2);                                                     \\\n+    patch_reg_c((address)&insn, 7, Rd_Rs1);                                                  \\\n+    patch_c((address)&insn, 15, 12, funct4);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(mv_c,  0b1000, 0b10);\n+  INSN(add_c, 0b1001, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct4, op)                                                               \\\n+  void NAME(Register Rs1) {                                                                  \\\n+    assert_cond(Rs1 != x0);                                                                  \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_reg_c((address)&insn, 2, x0);                                                      \\\n+    patch_reg_c((address)&insn, 7, Rs1);                                                     \\\n+    patch_c((address)&insn, 15, 12, funct4);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(jr_c,   0b1000, 0b10);\n+  INSN(jalr_c, 0b1001, 0b10);\n+\n+#undef INSN\n+\n+  typedef void (Assembler::* j_c_insn)(address dest);\n+  typedef void (Assembler::* compare_and_branch_c_insn)(Register Rs1, address dest);\n+\n+  void wrap_label(Label &L, j_c_insn insn);\n+  void wrap_label(Label &L, Register r, compare_and_branch_c_insn insn);\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(int32_t offset) {                                                                \\\n+    assert_cond(is_imm_in_range(offset, 11, 1));                                             \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 2, 2, (offset & nth_bit(5)) >> 5);                               \\\n+    patch_c((address)&insn, 5, 3, (offset & right_n_bits(4)) >> 1);                          \\\n+    patch_c((address)&insn, 6, 6, (offset & nth_bit(7)) >> 7);                               \\\n+    patch_c((address)&insn, 7, 7, (offset & nth_bit(6)) >> 6);                               \\\n+    patch_c((address)&insn, 8, 8, (offset & nth_bit(10)) >> 10);                             \\\n+    patch_c((address)&insn, 10, 9, (offset & right_n_bits(10)) >> 8);                        \\\n+    patch_c((address)&insn, 11, 11, (offset & nth_bit(4)) >> 4);                             \\\n+    patch_c((address)&insn, 12, 12, (offset & nth_bit(11)) >> 11);                           \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }                                                                                          \\\n+  void NAME(address dest) {                                                                  \\\n+    assert_cond(dest != NULL);                                                               \\\n+    int64_t distance = dest - pc();                                                          \\\n+    assert_cond(is_imm_in_range(distance, 11, 1));                                           \\\n+    j_c(distance);                                                                           \\\n+  }                                                                                          \\\n+  void NAME(Label &L) {                                                                      \\\n+    wrap_label(L, &Assembler::NAME);                                                         \\\n+  }\n+\n+  INSN(j_c, 0b101, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rs1, int32_t imm) {                                                     \\\n+    assert_cond(is_imm_in_range(imm, 8, 1));                                                 \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 2, 2, (imm & nth_bit(5)) >> 5);                                  \\\n+    patch_c((address)&insn, 4, 3, (imm & right_n_bits(3)) >> 1);                             \\\n+    patch_c((address)&insn, 6, 5, (imm & right_n_bits(8)) >> 6);                             \\\n+    patch_compressed_reg_c((address)&insn, 7, Rs1);                                          \\\n+    patch_c((address)&insn, 11, 10, (imm & right_n_bits(5)) >> 3);                           \\\n+    patch_c((address)&insn, 12, 12, (imm & nth_bit(8)) >> 8);                                \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }                                                                                          \\\n+  void NAME(Register Rs1, address dest) {                                                    \\\n+    assert_cond(dest != NULL);                                                               \\\n+    int64_t distance = dest - pc();                                                          \\\n+    assert_cond(is_imm_in_range(distance, 8, 1));                                            \\\n+    NAME(Rs1, distance);                                                                     \\\n+  }                                                                                          \\\n+  void NAME(Register Rs1, Label &L) {                                                        \\\n+    wrap_label(L, Rs1, &Assembler::NAME);                                                    \\\n+  }\n+\n+  INSN(beqz_c, 0b110, 0b01);\n+  INSN(bnez_c, 0b111, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, int32_t imm) {                                                      \\\n+    assert_cond(is_imm_in_range(imm, 18, 0));                                                \\\n+    assert_cond((imm & 0xfff) == 0);                                                         \\\n+    assert_cond(imm != 0);                                                                   \\\n+    assert_cond(Rd != x0 && Rd != x2);                                                       \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 6, 2, (imm & right_n_bits(17)) >> 12);                           \\\n+    patch_reg_c((address)&insn, 7, Rd);                                                      \\\n+    patch_c((address)&insn, 12, 12, (imm & nth_bit(17)) >> 17);                              \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(lui_c, 0b011, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, int32_t imm) {                                                      \\\n+    assert_cond(is_imm_in_range(imm, 6, 0));                                                 \\\n+    assert_cond(Rd != x0);                                                                   \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 6, 2, (imm & right_n_bits(5)));                                  \\\n+    patch_reg_c((address)&insn, 7, Rd);                                                      \\\n+    patch_c((address)&insn, 12, 12, (imm & right_n_bits(6)) >> 5);                           \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(li_c, 0b010, 0b01);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op, REGISTER_TYPE, CHECK)                                         \\\n+  void NAME(REGISTER_TYPE Rd, uint32_t uimm) {                                               \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 9, 0));                                       \\\n+    assert_cond((uimm & 0b111) == 0);                                                        \\\n+    IF(CHECK, assert_cond(Rd != x0);)                                                        \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 4, 2, (uimm & right_n_bits(9)) >> 6);                            \\\n+    patch_c((address)&insn, 6, 5, (uimm & right_n_bits(5)) >> 3);                            \\\n+    patch_reg_c((address)&insn, 7, Rd);                                                      \\\n+    patch_c((address)&insn, 12, 12, (uimm & nth_bit(5)) >> 5);                               \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+#define IF(BOOL, ...)       IF_##BOOL(__VA_ARGS__)\n+#define IF_true(code)       code\n+#define IF_false(code)\n+\n+  INSN(ldsp_c,  0b011, 0b10, Register,      true);\n+  INSN(fldsp_c, 0b001, 0b10, FloatRegister, false);\n+\n+#undef IF_false\n+#undef IF_true\n+#undef IF\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op, REGISTER_TYPE)                                                \\\n+  void NAME(REGISTER_TYPE Rd_Rs2, Register Rs1, uint32_t uimm) {                             \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 8, 0));                                       \\\n+    assert_cond((uimm & 0b111) == 0);                                                        \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_compressed_reg_c((address)&insn, 2, Rd_Rs2);                                       \\\n+    patch_c((address)&insn, 6, 5, (uimm & right_n_bits(8)) >> 6);                            \\\n+    patch_compressed_reg_c((address)&insn, 7, Rs1);                                          \\\n+    patch_c((address)&insn, 12, 10, (uimm & right_n_bits(6)) >> 3);                          \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(ld_c,  0b011, 0b00, Register);\n+  INSN(sd_c,  0b111, 0b00, Register);\n+  INSN(fld_c, 0b001, 0b00, FloatRegister);\n+  INSN(fsd_c, 0b101, 0b00, FloatRegister);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op, REGISTER_TYPE)                                                \\\n+  void NAME(REGISTER_TYPE Rs2, uint32_t uimm) {                                              \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 9, 0));                                       \\\n+    assert_cond((uimm & 0b111) == 0);                                                        \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_reg_c((address)&insn, 2, Rs2);                                                     \\\n+    patch_c((address)&insn, 9, 7, (uimm & right_n_bits(9)) >> 6);                            \\\n+    patch_c((address)&insn, 12, 10, (uimm & right_n_bits(6)) >> 3);                          \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(sdsp_c,  0b111, 0b10, Register);\n+  INSN(fsdsp_c, 0b101, 0b10, FloatRegister);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rs2, uint32_t uimm) {                                                   \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 8, 0));                                       \\\n+    assert_cond((uimm & 0b11) == 0);                                                         \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_reg_c((address)&insn, 2, Rs2);                                                     \\\n+    patch_c((address)&insn, 8, 7, (uimm & right_n_bits(8)) >> 6);                            \\\n+    patch_c((address)&insn, 12, 9, (uimm & right_n_bits(6)) >> 2);                           \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(swsp_c, 0b110, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd, uint32_t uimm) {                                                    \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 8, 0));                                       \\\n+    assert_cond((uimm & 0b11) == 0);                                                         \\\n+    assert_cond(Rd != x0);                                                                   \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 3, 2, (uimm & right_n_bits(8)) >> 6);                            \\\n+    patch_c((address)&insn, 6, 4, (uimm & right_n_bits(5)) >> 2);                            \\\n+    patch_reg_c((address)&insn, 7, Rd);                                                      \\\n+    patch_c((address)&insn, 12, 12, (uimm & nth_bit(5)) >> 5);                               \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(lwsp_c, 0b010, 0b10);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME(Register Rd_Rs2, Register Rs1, uint32_t uimm) {                                  \\\n+    assert_cond(is_unsigned_imm_in_range(uimm, 7, 0));                                       \\\n+    assert_cond((uimm & 0b11) == 0);                                                         \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_compressed_reg_c((address)&insn, 2, Rd_Rs2);                                       \\\n+    patch_c((address)&insn, 5, 5, (uimm & nth_bit(6)) >> 6);                                 \\\n+    patch_c((address)&insn, 6, 6, (uimm & nth_bit(2)) >> 2);                                 \\\n+    patch_compressed_reg_c((address)&insn, 7, Rs1);                                          \\\n+    patch_c((address)&insn, 12, 10, (uimm & right_n_bits(6)) >> 3);                          \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(lw_c, 0b010, 0b00);\n+  INSN(sw_c, 0b110, 0b00);\n+\n+#undef INSN\n+\n+#define INSN(NAME, funct3, op)                                                               \\\n+  void NAME() {                                                                              \\\n+    uint16_t insn = 0;                                                                       \\\n+    patch_c((address)&insn, 1, 0, op);                                                       \\\n+    patch_c((address)&insn, 11, 2, 0x0);                                                     \\\n+    patch_c((address)&insn, 12, 12, 0b1);                                                    \\\n+    patch_c((address)&insn, 15, 13, funct3);                                                 \\\n+    emit_int16(insn);                                                                        \\\n+  }\n+\n+  INSN(ebreak_c, 0b100, 0b10);\n+\n+#undef INSN\n+\n+\/\/ --------------  C-Ext Transformation Macros  --------------\n+\n+\/\/ a pivotal dispatcher for C-Ext\n+#define EMIT_MAY_COMPRESS(COMPRESSIBLE, NAME, ...)    EMIT_MAY_COMPRESS_##COMPRESSIBLE(NAME, __VA_ARGS__)\n+#define EMIT_MAY_COMPRESS_true(NAME, ...)             EMIT_MAY_COMPRESS_##NAME(__VA_ARGS__)\n+#define EMIT_MAY_COMPRESS_false(NAME, ...)\n+\n+#define IS_COMPRESSIBLE(...)                          if (__VA_ARGS__)\n+#define CHECK_CEXT_AND_COMPRESSIBLE(...)              IS_COMPRESSIBLE(UseRVC && __VA_ARGS__)\n+#define CHECK_CEXT()                                  if (UseRVC)\n+\n+\/\/ C-Ext transformation macros\n+#define EMIT_RVC_cond(PREFIX, COND, EMIT) {                                            \\\n+    PREFIX                                                                             \\\n+    CHECK_CEXT_AND_COMPRESSIBLE(COND) {                                                \\\n+      EMIT;                                                                            \\\n+      return;                                                                          \\\n+    }                                                                                  \\\n+  }\n+\n+#define EMIT_RVC_cond2(PREFIX, COND1, EMIT1, COND2, EMIT2) {                           \\\n+    PREFIX                                                                             \\\n+    CHECK_CEXT() {                                                                     \\\n+      IS_COMPRESSIBLE(COND1) {                                                         \\\n+        EMIT1;                                                                         \\\n+        return;                                                                        \\\n+      } else IS_COMPRESSIBLE(COND2) {                                                  \\\n+        EMIT2;                                                                         \\\n+        return;                                                                        \\\n+      }                                                                                \\\n+    }                                                                                  \\\n+  }\n+\n+#define EMIT_RVC_cond4(PREFIX, COND1, EMIT1, COND2, EMIT2, COND3, EMIT3, COND4, EMIT4) {  \\\n+    PREFIX                                                                             \\\n+    CHECK_CEXT() {                                                                     \\\n+      IS_COMPRESSIBLE(COND1) {                                                         \\\n+        EMIT1;                                                                         \\\n+        return;                                                                        \\\n+      } else IS_COMPRESSIBLE(COND2) {                                                  \\\n+        EMIT2;                                                                         \\\n+        return;                                                                        \\\n+      } else IS_COMPRESSIBLE(COND3) {                                                  \\\n+        EMIT3;                                                                         \\\n+        return;                                                                        \\\n+      } else IS_COMPRESSIBLE(COND4) {                                                  \\\n+        EMIT4;                                                                         \\\n+        return;                                                                        \\\n+      }                                                                                \\\n+    }                                                                                  \\\n+  }\n+\n+\/\/ --------------------------\n+\/\/ Register instructions\n+\/\/ --------------------------\n+\/\/ add -> c.add\n+#define EMIT_MAY_COMPRESS_add(Rd, Rs1, Rs2)                                            \\\n+  EMIT_RVC_cond(                                                                       \\\n+    Register src = noreg;,                                                             \\\n+    Rs1 != x0 && Rs2 != x0 && ((src = Rs1, Rs2 == Rd) || (src = Rs2, Rs1 == Rd)),      \\\n+    add_c(Rd, src)                                                                     \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ sub\/subw -> c.sub\/c.subw\n+#define EMIT_MAY_COMPRESS_sub_helper(NAME_C, Rd, Rs1, Rs2)                             \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    Rs1 == Rd && Rd->is_compressed_valid() && Rs2->is_compressed_valid(),              \\\n+    NAME_C(Rd, Rs2)                                                                    \\\n+  )\n+\n+#define EMIT_MAY_COMPRESS_sub(Rd, Rs1, Rs2)                                            \\\n+  EMIT_MAY_COMPRESS_sub_helper(sub_c, Rd, Rs1, Rs2)\n+\n+#define EMIT_MAY_COMPRESS_subw(Rd, Rs1, Rs2)                                           \\\n+  EMIT_MAY_COMPRESS_sub_helper(subw_c, Rd, Rs1, Rs2)\n+\n+\/\/ --------------------------\n+\/\/ xor\/or\/and\/addw -> c.xor\/c.or\/c.and\/c.addw\n+#define EMIT_MAY_COMPRESS_xorr_orr_andr_addw_helper(NAME_C, Rd, Rs1, Rs2)              \\\n+  EMIT_RVC_cond(                                                                       \\\n+    Register src = noreg;,                                                             \\\n+    Rs1->is_compressed_valid() && Rs2->is_compressed_valid() &&                        \\\n+      ((src = Rs1, Rs2 == Rd) || (src = Rs2, Rs1 == Rd)),                              \\\n+    NAME_C(Rd, src)                                                                    \\\n+  )\n+\n+#define EMIT_MAY_COMPRESS_xorr(Rd, Rs1, Rs2)                                           \\\n+  EMIT_MAY_COMPRESS_xorr_orr_andr_addw_helper(xor_c, Rd, Rs1, Rs2)\n+\n+#define EMIT_MAY_COMPRESS_orr(Rd, Rs1, Rs2)                                            \\\n+  EMIT_MAY_COMPRESS_xorr_orr_andr_addw_helper(or_c, Rd, Rs1, Rs2)\n+\n+#define EMIT_MAY_COMPRESS_andr(Rd, Rs1, Rs2)                                           \\\n+  EMIT_MAY_COMPRESS_xorr_orr_andr_addw_helper(and_c, Rd, Rs1, Rs2)\n+\n+#define EMIT_MAY_COMPRESS_addw(Rd, Rs1, Rs2)                                           \\\n+  EMIT_MAY_COMPRESS_xorr_orr_andr_addw_helper(addw_c, Rd, Rs1, Rs2)\n+\n+\/\/ --------------------------\n+\/\/ Load\/store register (all modes)\n+\/\/ --------------------------\n+private:\n+\n+#define FUNC(NAME, funct3, bits)                                                       \\\n+  bool NAME(Register rs1, Register rd_rs2, int32_t imm12, bool ld) {                   \\\n+    return rs1 == sp &&                                                                \\\n+      is_unsigned_imm_in_range(imm12, bits, 0) &&                                      \\\n+      (intx(imm12) & funct3) == 0x0 &&                                                 \\\n+      (!ld || rd_rs2 != x0);                                                           \\\n+  }                                                                                    \\\n+\n+  FUNC(is_ldsdsp_c,  0b111, 9);\n+  FUNC(is_lwswsp_c,  0b011, 8);\n+#undef FUNC\n+\n+#define FUNC(NAME, funct3, bits)                                                       \\\n+  bool NAME(Register rs1, int32_t imm12) {                                             \\\n+    return rs1 == sp &&                                                                \\\n+      is_unsigned_imm_in_range(imm12, bits, 0) &&                                      \\\n+      (intx(imm12) & funct3) == 0x0;                                                   \\\n+  }                                                                                    \\\n+\n+  FUNC(is_fldsdsp_c, 0b111, 9);\n+#undef FUNC\n+\n+#define FUNC(NAME, REG_TYPE, funct3, bits)                                             \\\n+  bool NAME(Register rs1, REG_TYPE rd_rs2, int32_t imm12) {                            \\\n+    return rs1->is_compressed_valid() &&                                               \\\n+      rd_rs2->is_compressed_valid() &&                                                 \\\n+      is_unsigned_imm_in_range(imm12, bits, 0) &&                                      \\\n+      (intx(imm12) & funct3) == 0x0;                                                   \\\n+  }                                                                                    \\\n+\n+  FUNC(is_ldsd_c,  Register,      0b111, 8);\n+  FUNC(is_lwsw_c,  Register,      0b011, 7);\n+  FUNC(is_fldsd_c, FloatRegister, 0b111, 8);\n+#undef FUNC\n+\n+public:\n+\/\/ --------------------------\n+\/\/ ld -> c.ldsp\/c.ld\n+#define EMIT_MAY_COMPRESS_ld(Rd, Rs, offset)                                           \\\n+  EMIT_RVC_cond2(,                                                                     \\\n+     is_ldsdsp_c(Rs, Rd, offset, true),                                                \\\n+     ldsp_c(Rd, offset),                                                               \\\n+     is_ldsd_c(Rs, Rd, offset),                                                        \\\n+     ld_c(Rd, Rs, offset)                                                              \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ sd -> c.sdsp\/c.sd\n+#define EMIT_MAY_COMPRESS_sd(Rd, Rs, offset)                                           \\\n+  EMIT_RVC_cond2(,                                                                     \\\n+     is_ldsdsp_c(Rs, Rd, offset, false),                                               \\\n+     sdsp_c(Rd, offset),                                                               \\\n+     is_ldsd_c(Rs, Rd, offset),                                                        \\\n+     sd_c(Rd, Rs, offset)                                                              \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ lw -> c.lwsp\/c.lw\n+#define EMIT_MAY_COMPRESS_lw(Rd, Rs, offset)                                           \\\n+  EMIT_RVC_cond2(,                                                                     \\\n+     is_lwswsp_c(Rs, Rd, offset, true),                                                \\\n+     lwsp_c(Rd, offset),                                                               \\\n+     is_lwsw_c(Rs, Rd, offset),                                                        \\\n+     lw_c(Rd, Rs, offset)                                                              \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ sw -> c.swsp\/c.sw\n+#define EMIT_MAY_COMPRESS_sw(Rd, Rs, offset)                                           \\\n+  EMIT_RVC_cond2(,                                                                     \\\n+     is_lwswsp_c(Rs, Rd, offset, false),                                               \\\n+     swsp_c(Rd, offset),                                                               \\\n+     is_lwsw_c(Rs, Rd, offset),                                                        \\\n+     sw_c(Rd, Rs, offset)                                                              \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ fld -> c.fldsp\/c.fld\n+#define EMIT_MAY_COMPRESS_fld(Rd, Rs, offset)                                          \\\n+  EMIT_RVC_cond2(,                                                                     \\\n+     is_fldsdsp_c(Rs, offset),                                                         \\\n+     fldsp_c(Rd, offset),                                                              \\\n+     is_fldsd_c(Rs, Rd, offset),                                                       \\\n+     fld_c(Rd, Rs, offset)                                                             \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ fsd -> c.fsdsp\/c.fsd\n+#define EMIT_MAY_COMPRESS_fsd(Rd, Rs, offset)                                          \\\n+  EMIT_RVC_cond2(,                                                                     \\\n+     is_fldsdsp_c(Rs, offset),                                                         \\\n+     fsdsp_c(Rd, offset),                                                              \\\n+     is_fldsd_c(Rs, Rd, offset),                                                       \\\n+     fsd_c(Rd, Rs, offset)                                                             \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ Conditional branch instructions\n+\/\/ --------------------------\n+\/\/ beq\/bne -> c.beqz\/c.bnez\n+\n+\/\/ TODO: Removing the below 'offset != 0' check needs us to fix lots of '__ beqz() \/ __ benz()'\n+\/\/   to '__ beqz_nc() \/ __ bnez_nc()' everywhere.\n+#define EMIT_MAY_COMPRESS_beqz_bnez_helper(NAME_C, Rs1, Rs2, offset)                   \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    offset != 0 && Rs2 == x0 && Rs1->is_compressed_valid() &&                          \\\n+      is_imm_in_range(offset, 8, 1),                                                   \\\n+    NAME_C(Rs1, offset)                                                                \\\n+  )\n+\n+#define EMIT_MAY_COMPRESS_beq(Rs1, Rs2, offset)                                        \\\n+  EMIT_MAY_COMPRESS_beqz_bnez_helper(beqz_c, Rs1, Rs2, offset)\n+\n+#define EMIT_MAY_COMPRESS_bne(Rs1, Rs2, offset)                                        \\\n+  EMIT_MAY_COMPRESS_beqz_bnez_helper(bnez_c, Rs1, Rs2, offset)\n+\n+\/\/ --------------------------\n+\/\/ Unconditional branch instructions\n+\/\/ --------------------------\n+\/\/ jalr\/jal -> c.jr\/c.jalr\/c.j\n+\n+#define EMIT_MAY_COMPRESS_jalr(Rd, Rs, offset)                                         \\\n+  EMIT_RVC_cond2(,                                                                     \\\n+    offset == 0 && Rd == x1 && Rs != x0,                                               \\\n+    jalr_c(Rs),                                                                        \\\n+    offset == 0 && Rd == x0 && Rs != x0,                                               \\\n+    jr_c(Rs)                                                                           \\\n+  )\n+\n+\/\/ TODO: Removing the 'offset != 0' check needs us to fix lots of '__ j()'\n+\/\/   to '__ j_nc()' manually everywhere.\n+#define EMIT_MAY_COMPRESS_jal(Rd, offset)                                              \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    offset != 0 && Rd == x0 && is_imm_in_range(offset, 11, 1),                         \\\n+    j_c(offset)                                                                        \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ Upper Immediate Instruction\n+\/\/ --------------------------\n+\/\/ lui -> c.lui\n+#define EMIT_MAY_COMPRESS_lui(Rd, imm)                                                 \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    Rd != x0 && Rd != x2 && imm != 0 && is_imm_in_range(imm, 18, 0),                   \\\n+    lui_c(Rd, imm)                                                                     \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ Miscellaneous Instructions\n+\/\/ --------------------------\n+\/\/ ebreak -> c.ebreak\n+#define EMIT_MAY_COMPRESS_ebreak()                                                     \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    true,                                                                              \\\n+    ebreak_c()                                                                         \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ Immediate Instructions\n+\/\/ --------------------------\n+\/\/ addi -> c.addi16sp\/c.addi4spn\/c.mv\/c.addi\/. An addi instruction able to transform to c.nop will be ignored.\n+#define EMIT_MAY_COMPRESS_addi(Rd, Rs1, imm)                                                          \\\n+  EMIT_RVC_cond4(,                                                                                    \\\n+    Rs1 == sp && Rd == Rs1 && imm != 0 && (imm & 0b1111) == 0x0 && is_imm_in_range(imm, 10, 0),       \\\n+    addi16sp_c(imm),                                                                                  \\\n+    Rs1 == sp && Rd->is_compressed_valid() && imm != 0 && (imm & 0b11) == 0x0 && is_unsigned_imm_in_range(imm, 10, 0),  \\\n+    addi4spn_c(Rd, imm),                                                                              \\\n+    Rd == Rs1 && is_imm_in_range(imm, 6, 0),                                                          \\\n+    if (imm != 0) { addi_c(Rd, imm); },                                                               \\\n+    imm == 0 && Rd != x0 && Rs1 != x0,                                                                \\\n+    mv_c(Rd, Rs1)                                                                                     \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ addiw -> c.addiw\n+#define EMIT_MAY_COMPRESS_addiw(Rd, Rs1, imm)                                          \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    Rd == Rs1 && Rd != x0 && is_imm_in_range(imm, 6, 0),                               \\\n+    addiw_c(Rd, imm)                                                                   \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ and_imm12 -> c.andi\n+#define EMIT_MAY_COMPRESS_and_imm12(Rd, Rs1, imm)                                      \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    Rd == Rs1 && Rd->is_compressed_valid() && is_imm_in_range(imm, 6, 0),              \\\n+    andi_c(Rd, imm)                                                                    \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ Shift Immediate Instructions\n+\/\/ --------------------------\n+\/\/ slli -> c.slli\n+#define EMIT_MAY_COMPRESS_slli(Rd, Rs1, shamt)                                         \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    Rd == Rs1 && Rd != x0 && shamt != 0,                                               \\\n+    slli_c(Rd, shamt)                                                                  \\\n+  )\n+\n+\/\/ --------------------------\n+\/\/ srai\/srli -> c.srai\/c.srli\n+#define EMIT_MAY_COMPRESS_srai_srli_helper(NAME_C, Rd, Rs1, shamt)                     \\\n+  EMIT_RVC_cond(,                                                                      \\\n+    Rd == Rs1 && Rd->is_compressed_valid() && shamt != 0,                              \\\n+    NAME_C(Rd, shamt)                                                                  \\\n+  )\n+\n+#define EMIT_MAY_COMPRESS_srai(Rd, Rs1, shamt)                                         \\\n+  EMIT_MAY_COMPRESS_srai_srli_helper(srai_c, Rd, Rs1, shamt)\n+\n+#define EMIT_MAY_COMPRESS_srli(Rd, Rs1, shamt)                                         \\\n+  EMIT_MAY_COMPRESS_srai_srli_helper(srli_c, Rd, Rs1, shamt)\n+\n+\/\/ --------------------------\n+\n+\/\/ a compile time dispatcher\n+#define EMIT_MAY_COMPRESS_NAME_true(NAME, ARGS)            NAME ARGS\n+#define EMIT_MAY_COMPRESS_NAME_false(NAME, ARGS)           NAME##_nc ARGS\n+#define EMIT_MAY_COMPRESS_NAME(COMPRESSIBLE, NAME, ARGS)   EMIT_MAY_COMPRESS_NAME_##COMPRESSIBLE(NAME, ARGS)\n+\n+\/\/ a runtime dispatcher (if clause is needed)\n+#define EMIT_MAY_COMPRESS_INST(COMPRESSIBLE, NAME, ARGS) \\\n+  if (COMPRESSIBLE) {                                    \\\n+    EMIT_MAY_COMPRESS_NAME_true(NAME, ARGS);             \\\n+  } else {                                               \\\n+    EMIT_MAY_COMPRESS_NAME_false(NAME, ARGS);            \\\n+  }\n+\n+#endif \/\/ CPU_RISCV_ASSEMBLER_RISCV_CEXT_HPP\n\\ No newline at end of file\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv_cext.hpp","additions":865,"deletions":0,"binary":false,"changes":865,"status":"added"},{"patch":"@@ -47,1 +47,1 @@\n-  __ la(t0, safepoint_pc.target());\n+  __ la(t0, safepoint_pc.target(), NOT_COMPRESSIBLE);\n@@ -109,3 +109,1 @@\n-  int32_t off = 0;\n-  __ la_patchable(ra, RuntimeAddress(Runtime1::entry_for(stub_id)), off);\n-  __ jalr(ra, ra, off);\n+  __ jalr_patchable(ra, RuntimeAddress(Runtime1::entry_for(stub_id)), ra);\n@@ -260,1 +258,1 @@\n-int PatchingStub::_patch_info_offset = -NativeGeneralJump::instruction_size;\n+int PatchingStub::_patch_info_offset = -NativeGeneralJump::get_instruction_size();\n","filename":"src\/hotspot\/cpu\/riscv\/c1_CodeStubs_riscv.cpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1321,1 +1321,6 @@\n-void LIR_Assembler::align_call(LIR_Code code) {  }\n+void LIR_Assembler::align_call(LIR_Code code) {\n+  \/\/ C-Ext: With C-Ext a call may get 2-byte aligned.\n+  \/\/   the address of jal itself (which will be patched later) should not span the cache line.\n+  \/\/   See CallDynamicJavaDirectNode::compute_padding() for more info.\n+  __ align(4);\n+}\n@@ -1378,3 +1383,1 @@\n-  int32_t off = 0;\n-  __ la_patchable(exceptionPC->as_register(), pc_for_athrow, off);\n-  __ addi(exceptionPC->as_register(), exceptionPC->as_register(), off);\n+  __ addi_patchable(exceptionPC->as_register(), pc_for_athrow, exceptionPC->as_register());\n@@ -1804,3 +1807,1 @@\n-    int32_t offset = 0;\n-    __ la_patchable(t0, RuntimeAddress(dest), offset);\n-    __ jalr(x1, t0, offset);\n+    __ jalr_patchable(x1, RuntimeAddress(dest), t0);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -328,1 +328,1 @@\n-  nop();\n+  nop_nc();\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -70,3 +70,1 @@\n-  int32_t off = 0;\n-  la_patchable(t0, RuntimeAddress(entry), off);\n-  jalr(x1, t0, off);\n+  jalr_patchable(x1, RuntimeAddress(entry), t0);\n@@ -572,3 +570,1 @@\n-  int32_t off = 0;\n-  __ la_patchable(t0, RuntimeAddress(target), off);\n-  __ jalr(x1, t0, off);\n+  __ jalr_patchable(x1, RuntimeAddress(target), t0);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_Runtime1_riscv.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1199,1 +1199,1 @@\n-  (conditional_branch_insn)&Assembler::beq,\n+  (conditional_branch_insn)&Assembler::beq_nc,\n@@ -1203,1 +1203,1 @@\n-  (conditional_branch_insn)&Assembler::bne,\n+  (conditional_branch_insn)&Assembler::bne_nc,\n@@ -1209,1 +1209,1 @@\n-  (conditional_branch_insn)&Assembler::beq,\n+  (conditional_branch_insn)&Assembler::beq_nc,\n@@ -1213,1 +1213,1 @@\n-  (conditional_branch_insn)&Assembler::bne,\n+  (conditional_branch_insn)&Assembler::bne_nc,\n@@ -1262,1 +1262,1 @@\n-      beqz(op1, L, is_far);\n+      beqz_nc(op1, L, is_far);\n@@ -1266,1 +1266,1 @@\n-      bnez(op1, L, is_far);\n+      bnez_nc(op1, L, is_far);\n@@ -1276,1 +1276,1 @@\n-      beqz(op1, L, is_far);\n+      beqz_nc(op1, L, is_far);\n@@ -1279,1 +1279,1 @@\n-      bnez(op1, L, is_far);\n+      bnez_nc(op1, L, is_far);\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -85,0 +85,10 @@\n+  \/\/ C-Ext: these cmp functions remain uncompressed in C2 MachNodes' emission -\n+  \/\/   as the reason described in MachEpilogNode::emit() in PhaseOutput::scratch_emit_size()\n+  \/\/   it simulates a node's size, but for MachBranchNodes it emits a fake Label just\n+  \/\/   near the node itself - the offset is so small that in scratch emission phase it always\n+  \/\/   get compressed in our implicit compression phase - but in real world the Label may be\n+  \/\/   anywhere so it may not be compressed, so here is the mismatch: it runs shorten_branches();\n+  \/\/   but with C-Ext we may need a further, say, shorten_compressed_branches() or something.\n+  \/\/   After researching we find performance will not have much enhancement even if compressing\n+  \/\/   them and the cost is a bit big to support MachBranchNodes' compression.\n+  \/\/   So as a solution, we can simply disable the compression of MachBranchNodes.\n","filename":"src\/hotspot\/cpu\/riscv\/c2_MacroAssembler_riscv.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -49,1 +49,1 @@\n-define_pd_global(intx, InteriorEntryAlignment,       16);\n+define_pd_global(intx, InteriorEntryAlignment,       4);\n","filename":"src\/hotspot\/cpu\/riscv\/c2_globals_riscv.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-  __ la(t0, safepoint_pc.target());\n+  __ la(t0, safepoint_pc.target(), NOT_COMPRESSIBLE);\n","filename":"src\/hotspot\/cpu\/riscv\/c2_safepointPollStubTable_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,2 +72,6 @@\n-  \/\/ fence_i + fence* + (lui, addi, slli, addi, slli, addi) + (lui, addi, slli, addi, slli) + jalr\n-  return NativeFenceI::instruction_size() + 12 * NativeInstruction::instruction_size;\n+  \/\/ fence_i + fence* + (lui, addi, slli(C), addi, slli(C), addi) + (lui, addi, slli(C), addi, slli(C)) + jalr\n+  return NativeFenceI::instruction_size() +\n+         (!UseRVC ?\n+           12 * NativeInstruction::instruction_size :\n+           8 * NativeInstruction::instruction_size + 4 * NativeInstruction::compressed_instruction_size\n+         );\n","filename":"src\/hotspot\/cpu\/riscv\/compiledIC_riscv.cpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -183,5 +183,1 @@\n-    {\n-      int32_t offset;\n-      __ la_patchable(t1, address_end, offset);\n-      __ ld(t1, Address(t1, offset));\n-    }\n+    __ ld_patchable(t1, address_end, t1);\n@@ -191,6 +187,2 @@\n-    {\n-      int32_t offset;\n-      __ la_patchable(t0, address_top, offset);\n-      __ addi(t0, t0, offset);\n-      __ lr_d(obj, t0, Assembler::aqrl);\n-    }\n+    __ addi_patchable(t0, address_top, t0);\n+    __ lr_d(obj, t0, Assembler::aqrl);\n@@ -234,0 +226,2 @@\n+extern int nmethod_barrier_guard_offset();\n+\n@@ -241,0 +235,6 @@\n+  \/\/ C-Ext: With C-Ext we may come here with a 2-byte alignment, hence an alignment is needed.\n+  \/\/ See below comments about amo, also native_nmethod_barrier() to find the entry's calculation strategy.\n+  while ((__ offset() + nmethod_barrier_guard_offset()) % 4 != 0) { __ nop(); }\n+\n+  int start = __ offset();\n+\n@@ -253,2 +253,2 @@\n-  __ movptr_with_offset(t0, StubRoutines::riscv64::method_entry_barrier(), offset);\n-  __ jalr(ra, t0, offset);\n+  __ movptr_with_offset(t0, StubRoutines::riscv64::method_entry_barrier(), offset, NOT_COMPRESSIBLE);\n+  __ jalr_nc(ra, t0, offset);\n@@ -257,0 +257,4 @@\n+  \/\/ RISCV's amoswap instructions need an alignment for the memory address it swaps\n+  \/\/ C-Ext: So with C-Ext we need to manually align it to 4-byte\n+  assert(__ offset() - start == nmethod_barrier_guard_offset() && __ offset() % 4 == 0, \"offsets equality and alignment\");\n+\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetAssembler_riscv.cpp","additions":17,"deletions":13,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -39,0 +39,10 @@\n+public:\n+  enum {\n+    total_normal_guard_offset     = 12 * instruction_size,\n+    total_compressed_guard_offset = 10 * instruction_size + 2 * compressed_instruction_size,\n+\n+    total_normal_size             = total_normal_guard_offset + 4,\n+    total_compressed_size         = total_compressed_guard_offset + 4,\n+  };\n+\n+private:\n@@ -42,2 +52,2 @@\n-    \/* auipc + lwu + fence + lwu + beq + lui + addi + slli + addi + slli + jalr + j *\/\n-    return reinterpret_cast<int*>(instruction_address() + 12 * 4);\n+    \/* auipc + lwu + fence + lwu + beq + lui + addi + (C)slli + addi + (C)slli + jalr + j *\/\n+    return reinterpret_cast<int*>(instruction_address() + guard_offset());\n@@ -56,0 +66,4 @@\n+\n+  static int guard_offset() {\n+    return UseRVC ? total_compressed_guard_offset : total_normal_guard_offset;\n+  }\n@@ -58,0 +72,4 @@\n+int nmethod_barrier_guard_offset() {\n+  return NativeNMethodBarrier::guard_offset();\n+}\n+\n@@ -63,0 +81,1 @@\n+  int instruction_size;\n@@ -66,12 +85,30 @@\n-  { 0x00000fff, 0x00000297, \"auipc  t0, 0           \"},\n-  { 0x000fffff, 0x0002e283, \"lwu    t0, 48(t0)      \"},\n-  { 0xffffffff, 0x0aa0000f, \"fence  ir, ir          \"},\n-  { 0x000fffff, 0x000be303, \"lwu    t1, 112(xthread)\"},\n-  { 0x01fff07f, 0x00628063, \"beq    t0, t1, skip    \"},\n-  { 0x00000fff, 0x000002b7, \"lui    t0, imm0        \"},\n-  { 0x000fffff, 0x00028293, \"addi   t0, t0, imm1    \"},\n-  { 0xffffffff, 0x00b29293, \"slli   t0, t0, 11      \"},\n-  { 0x000fffff, 0x00028293, \"addi   t0, t0, imm2    \"},\n-  { 0xffffffff, 0x00529293, \"slli   t0, t0, 5       \"},\n-  { 0x000fffff, 0x000280e7, \"jalr   ra, imm3(t0)    \"},\n-  { 0x00000fff, 0x0000006f, \"j      skip            \"}\n+  { 0x00000fff, 0x00000297, \"auipc  t0, 0           \", NativeInstruction::instruction_size},\n+  { 0x000fffff, 0x0002e283, \"lwu    t0, 48(t0)      \", NativeInstruction::instruction_size},\n+  { 0xffffffff, 0x0aa0000f, \"fence  ir, ir          \", NativeInstruction::instruction_size},\n+  { 0x000fffff, 0x000be303, \"lwu    t1, 36(xthread) \", NativeInstruction::instruction_size},\n+  { 0x01fff07f, 0x00628063, \"beq    t0, t1, skip    \", NativeInstruction::instruction_size},\n+  { 0x00000fff, 0x000002b7, \"lui    t0, imm0        \", NativeInstruction::instruction_size},\n+  { 0x000fffff, 0x00028293, \"addi   t0, t0, imm1    \", NativeInstruction::instruction_size},\n+  { 0xffffffff, 0x00b29293, \"slli   t0, t0, 11      \", NativeInstruction::instruction_size},\n+  { 0x000fffff, 0x00028293, \"addi   t0, t0, imm2    \", NativeInstruction::instruction_size},\n+  { 0xffffffff, 0x00529293, \"slli   t0, t0, 5       \", NativeInstruction::instruction_size},\n+  { 0x000fffff, 0x000280e7, \"jalr   ra, imm3(t0)    \", NativeInstruction::instruction_size},\n+  { 0x00000fff, 0x0000006f, \"j      skip            \", NativeInstruction::instruction_size}\n+  \/* guard: *\/\n+  \/* 32bit nmethod guard value *\/\n+  \/* skip: *\/\n+};\n+\n+static const struct CheckInsn barrierCInsn[] = {\n+  { 0x00000fff, 0x00000297, \"auipc  t0, 0           \", NativeInstruction::instruction_size},\n+  { 0x000fffff, 0x0002e283, \"lwu    t0, 44(t0)      \", NativeInstruction::instruction_size},\n+  { 0xffffffff, 0x0aa0000f, \"fence  ir, ir          \", NativeInstruction::instruction_size},\n+  { 0x000fffff, 0x000be303, \"lwu    t1, 36(xthread) \", NativeInstruction::instruction_size},\n+  { 0x01fff07f, 0x00628063, \"beq    t0, t1, skip    \", NativeInstruction::instruction_size},\n+  { 0x00000fff, 0x000002b7, \"lui    t0, imm0        \", NativeInstruction::instruction_size},\n+  { 0x000fffff, 0x00028293, \"addi   t0, t0, imm1    \", NativeInstruction::instruction_size},\n+  { 0x00000fff, 0x02ae,     \"c.slli t0, t0, 11      \", NativeInstruction::compressed_instruction_size},\n+  { 0x000fffff, 0x00028293, \"addi   t0, t0, imm2    \", NativeInstruction::instruction_size},\n+  { 0x0000ffff, 0x0296,     \"c.slli t0, t0, 5       \", NativeInstruction::compressed_instruction_size},\n+  { 0x000fffff, 0x000280e7, \"jalr   ra, imm3(t0)    \", NativeInstruction::instruction_size},\n+  { 0x00000fff, 0x0000006f, \"j      skip            \", NativeInstruction::instruction_size}\n@@ -88,3 +125,12 @@\n-  for(unsigned int i = 0; i < sizeof(barrierInsn)\/sizeof(struct CheckInsn); i++ ) {\n-    uint32_t inst = *((uint32_t*) addr);\n-    if ((inst & barrierInsn[i].mask) != barrierInsn[i].bits) {\n+  const struct CheckInsn *insns;\n+  size_t size;\n+  if (!UseRVC) {\n+    insns = barrierInsn;\n+    size = sizeof(barrierInsn) \/ sizeof(struct CheckInsn);\n+  } else {\n+    insns = barrierCInsn;\n+    size = sizeof(barrierCInsn) \/ sizeof(struct CheckInsn);\n+  }\n+  for(unsigned int i = 0; i < size; i++ ) {\n+    uint32_t inst = insns[i].instruction_size == NativeInstruction::compressed_instruction_size ? *((uint16_t*) addr) : *((uint32_t*) addr);\n+    if ((inst & insns[i].mask) != insns[i].bits) {\n@@ -92,1 +138,1 @@\n-      fatal(\"not an %s instruction.\", barrierInsn[i].name);\n+      fatal(\"not an %s instruction.\", insns[i].name);\n@@ -94,1 +140,1 @@\n-    addr += 4;\n+    addr += insns[i].instruction_size;\n@@ -144,1 +190,6 @@\n-static const int entry_barrier_offset = -4 * 13;\n+static const int entry_barrier_normal_offset = -NativeNMethodBarrier::total_normal_size;\n+static const int entry_barrier_compressed_offset = -NativeNMethodBarrier::total_compressed_size;\n+\n+static const int entry_barrier_offset() {\n+  return !UseRVC ? entry_barrier_normal_offset : entry_barrier_compressed_offset;\n+}\n@@ -147,1 +198,1 @@\n-  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset;\n+  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset();\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/shared\/barrierSetNMethod_riscv.cpp","additions":72,"deletions":21,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -339,3 +339,1 @@\n-    int32_t offset = 0;\n-    __ la_patchable(t0, stub->slow_path(), offset);\n-    __ jalr(x1, t0, offset);\n+    __ jalr_patchable(x1, stub->slow_path(), t0);\n","filename":"src\/hotspot\/cpu\/riscv\/gc\/z\/zBarrierSetAssembler_riscv.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-define_pd_global(intx, CodeEntryAlignment,       64);\n+define_pd_global(intx, CodeEntryAlignment,       16);\n@@ -93,1 +93,2 @@\n-  product(bool, UseRVV, false, EXPERIMENTAL, \"Use RVV instructions\")\n+  product(bool, UseRVV, false, EXPERIMENTAL, \"Use RVV instructions\")             \\\n+  product(bool, UseRVC, true, EXPERIMENTAL, \"Use RVC instructions\")              \\\n","filename":"src\/hotspot\/cpu\/riscv\/globals_riscv.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -185,3 +185,1 @@\n-  int32_t offset = 0;\n-  la_patchable(xdispatch, ExternalAddress((address)Interpreter::dispatch_table()), offset);\n-  addi(xdispatch, xdispatch, offset);\n+  addi_patchable(xdispatch, ExternalAddress((address)Interpreter::dispatch_table()), xdispatch);\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -77,3 +77,1 @@\n-  int32_t offset = 0;\n-  __ la_patchable(rcounter_addr, SafepointSynchronize::safepoint_counter_addr(), offset);\n-  __ addi(rcounter_addr, rcounter_addr, offset);\n+  __ addi_patchable(rcounter_addr, SafepointSynchronize::safepoint_counter_addr(), rcounter_addr);\n@@ -172,3 +170,1 @@\n-    int32_t tmp_offset = 0;\n-    __ la_patchable(t0, ExternalAddress(slow_case_addr), tmp_offset);\n-    __ jalr(x1, t0, tmp_offset);\n+    __ jalr_patchable(x1, ExternalAddress(slow_case_addr), t0);\n","filename":"src\/hotspot\/cpu\/riscv\/jniFastGetField_riscv.cpp","additions":2,"deletions":6,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -228,1 +228,2 @@\n-                                         Register temp) {\n+                                         Register temp,\n+                                         bool compressible) {\n@@ -231,1 +232,1 @@\n-  la(temp, last_java_pc);\n+  la(temp, last_java_pc, compressible);\n@@ -246,1 +247,1 @@\n-    set_last_Java_frame(last_java_sp, last_java_fp, pc() \/* Patched later *\/, temp);\n+    set_last_Java_frame(last_java_sp, last_java_fp, pc() \/* Patched later *\/, temp, NOT_COMPRESSIBLE);\n@@ -311,3 +312,1 @@\n-    int32_t offset = 0;\n-    la_patchable(t0, RuntimeAddress(StubRoutines::forward_exception_entry()), offset);\n-    jalr(x0, t0, offset);\n+    jalr_patchable(x0, RuntimeAddress(StubRoutines::forward_exception_entry()), t0);\n@@ -387,3 +386,1 @@\n-  int32_t offset = 0;\n-  la_patchable(t1, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()), offset);\n-  ld(t1, Address(t1, offset));\n+  ld_patchable(t1, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()), t1);\n@@ -426,3 +423,1 @@\n-  int32_t offset = 0;\n-  la_patchable(t1, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()), offset);\n-  ld(t1, Address(t1, offset));\n+  ld_patchable(t1, ExternalAddress(StubRoutines::verify_oop_subroutine_entry_address()), t1);\n@@ -538,1 +533,6 @@\n-void MacroAssembler::stop(const char* msg) {\n+\/\/ C-Ext: we may need to disable the compression for some instructions\n+\/\/   in some Nodes during C2 code emission, to emit the same constant\n+\/\/   instruction size both in PhaseOutput::scratch_emit_size()\n+\/\/   and the final real code emission.\n+\/\/   See: MachEpilogNode::emit() for more details.\n+void MacroAssembler::stop(const char* msg, bool compressible) {\n@@ -543,1 +543,2 @@\n-    li(c_rarg1, (uintptr_t)(address)ip);\n+    \/\/ C-Ext: use a fixed-length movptr\n+    movptr(c_rarg1, (address)ip, compressible);\n@@ -574,2 +575,2 @@\n-  movptr_with_offset(t0, 0, offset);\n-  jalr(x0, t0, offset);\n+  movptr_with_offset(t0, 0, offset, NOT_COMPRESSIBLE);\n+  jalr_nc(x0, t0, offset);\n@@ -661,1 +662,9 @@\n-  addi(x0, x0, 0);\n+  if (UseRVC) {\n+    nop_c();\n+  } else {\n+    addi(x0, x0, 0);\n+  }\n+}\n+\n+void MacroAssembler::nop_nc() {\n+  addi_nc(x0, x0, 0);\n@@ -742,1 +751,1 @@\n-void MacroAssembler::la(Register Rd, const address &dest) {\n+void MacroAssembler::la(Register Rd, const address &dest, bool compressible) {\n@@ -746,1 +755,1 @@\n-    addi(Rd, Rd, ((int64_t)offset << 52) >> 52);\n+    EMIT_MAY_COMPRESS_INST(compressible, addi, (Rd, Rd, ((int64_t)offset << 52) >> 52));\n@@ -748,1 +757,1 @@\n-    movptr(Rd, dest);\n+    movptr(Rd, dest, compressible);\n@@ -762,1 +771,1 @@\n-        movptr(Rd, adr.target());\n+        movptr(Rd, adr.target(), NOT_COMPRESSIBLE);\n@@ -778,1 +787,1 @@\n-  la(Rd, target(label));\n+  la(Rd, target(label), NOT_COMPRESSIBLE);\n@@ -798,0 +807,13 @@\n+#define INSN(NAME)                                                                \\\n+  void MacroAssembler::NAME##z_nc(Register Rs, const address &dest) {             \\\n+    NAME##_nc(Rs, zr, dest);                                                      \\\n+  }                                                                               \\\n+  void MacroAssembler::NAME##z_nc(Register Rs, Label &l, bool is_far) {           \\\n+    NAME##_nc(Rs, zr, l, is_far);                                                 \\\n+  }                                                                               \\\n+\n+  INSN(beq);\n+  INSN(bne);\n+\n+#undef INSN\n+\n@@ -810,2 +832,3 @@\n-  INSN(beq, feq, bnez);\n-  INSN(bne, feq, beqz);\n+  INSN(beq, feq, bnez_nc);\n+  INSN(bne, feq, beqz_nc);\n+\n@@ -821,1 +844,1 @@\n-      beqz(t0, l, is_far);                                                            \\\n+      beqz_nc(t0, l, is_far);                                                         \\\n@@ -825,1 +848,1 @@\n-      bnez(t0, l, is_far);                                                            \\\n+      bnez_nc(t0, l, is_far);                                                         \\\n@@ -833,1 +856,1 @@\n-      beqz(t0, l, is_far);                                                            \\\n+      beqz_nc(t0, l, is_far);                                                         \\\n@@ -837,1 +860,1 @@\n-      bnez(t0, l, is_far);                                                            \\\n+      bnez_nc(t0, l, is_far);                                                         \\\n@@ -1189,6 +1212,19 @@\n-  assert(is_imm_in_range(offset, 20, 1), \"offset is too large to be patched in one jal insrusction!\\n\");\n-  Assembler::patch(branch, 31, 31, (offset >> 20) & 0x1);                       \/\/ offset[20]    ==> branch[31]\n-  Assembler::patch(branch, 30, 21, (offset >> 1)  & 0x3ff);                     \/\/ offset[10:1]  ==> branch[30:21]\n-  Assembler::patch(branch, 20, 20, (offset >> 11) & 0x1);                       \/\/ offset[11]    ==> branch[20]\n-  Assembler::patch(branch, 19, 12, (offset >> 12) & 0xff);                      \/\/ offset[19:12] ==> branch[19:12]\n-  return NativeInstruction::instruction_size;                                   \/\/ only one instruction\n+  if (!NativeInstruction::is_compressed_instr(branch)) {\n+    assert(is_imm_in_range(offset, 20, 1), \"offset is too large to be patched in one jal instruction!\\n\");\n+    Assembler::patch(branch, 31, 31, (offset >> 20) & 0x1);                       \/\/ offset[20]    ==> branch[31]\n+    Assembler::patch(branch, 30, 21, (offset >> 1)  & 0x3ff);                     \/\/ offset[10:1]  ==> branch[30:21]\n+    Assembler::patch(branch, 20, 20, (offset >> 11) & 0x1);                       \/\/ offset[11]    ==> branch[20]\n+    Assembler::patch(branch, 19, 12, (offset >> 12) & 0xff);                      \/\/ offset[19:12] ==> branch[19:12]\n+    return NativeInstruction::instruction_size;                                   \/\/ only one instruction\n+  } else {  \/\/ we must patch it, so I don't check if current instruction is a compressed instruction because it must be.\n+    assert(is_imm_in_range(offset, 11, 1), \"offset is too large to be patched in one c.j instruction: use j_nc() instead of your j().\\n\");\n+    Assembler::patch_c(branch, 2, 2, (offset & nth_bit(5)) >> 5);              \/\/ offset[5]     ==> branch[2]\n+    Assembler::patch_c(branch, 5, 3, (offset & right_n_bits(4)) >> 1);         \/\/ offset[3:1]   ==> branch[5:3]\n+    Assembler::patch_c(branch, 6, 6, (offset & nth_bit(7)) >> 7);              \/\/ offset[7]     ==> branch[6]\n+    Assembler::patch_c(branch, 7, 7, (offset & nth_bit(6)) >> 6);              \/\/ offset[6]     ==> branch[7]\n+    Assembler::patch_c(branch, 8, 8, (offset & nth_bit(10)) >> 10);            \/\/ offset[10]    ==> branch[8]\n+    Assembler::patch_c(branch, 10, 9, (offset & right_n_bits(10)) >> 8);       \/\/ offset[9:8]   ==> branch[10:9]\n+    Assembler::patch_c(branch, 11, 11, (offset & nth_bit(4)) >> 4);            \/\/ offset[4]     ==> branch[11]\n+    Assembler::patch_c(branch, 12, 12, (offset & nth_bit(11)) >> 11);          \/\/ offset[11]    ==> branch[12]\n+    return NativeInstruction::compressed_instruction_size;                     \/\/ only one instruction\n+  }\n@@ -1198,6 +1234,18 @@\n-  assert(is_imm_in_range(offset, 12, 1), \"offset is too large to be patched in one beq\/bge\/bgeu\/blt\/bltu\/bne insrusction!\\n\");\n-  Assembler::patch(branch, 31, 31, (offset >> 12) & 0x1);                       \/\/ offset[12]    ==> branch[31]\n-  Assembler::patch(branch, 30, 25, (offset >> 5)  & 0x3f);                      \/\/ offset[10:5]  ==> branch[30:25]\n-  Assembler::patch(branch, 7,  7,  (offset >> 11) & 0x1);                       \/\/ offset[11]    ==> branch[7]\n-  Assembler::patch(branch, 11, 8,  (offset >> 1)  & 0xf);                       \/\/ offset[4:1]   ==> branch[11:8]\n-  return NativeInstruction::instruction_size;                                   \/\/ only one instruction\n+  if (!NativeInstruction::is_compressed_instr(branch)) {\n+    assert(is_imm_in_range(offset, 12, 1),\n+           \"offset is too large to be patched in one beq\/bge\/bgeu\/blt\/bltu\/bne insrusction!\\n\");\n+    Assembler::patch(branch, 31, 31, (offset >> 12) & 0x1);                       \/\/ offset[12]    ==> branch[31]\n+    Assembler::patch(branch, 30, 25, (offset >> 5) & 0x3f);                       \/\/ offset[10:5]  ==> branch[30:25]\n+    Assembler::patch(branch, 7, 7, (offset >> 11) & 0x1);                         \/\/ offset[11]    ==> branch[7]\n+    Assembler::patch(branch, 11, 8, (offset >> 1) & 0xf);                         \/\/ offset[4:1]   ==> branch[11:8]\n+    return NativeInstruction::instruction_size;                                   \/\/ only one instruction\n+  } else {\n+    assert(is_imm_in_range(offset, 8, 1),\n+            \"offset is too large to be patched in one c.beqz\/c.bnez instruction: use beqz_nc()\/bnez.nc() instead.\\n\");\n+    Assembler::patch_c(branch, 2, 2, (offset & nth_bit(5)) >> 5);\n+    Assembler::patch_c(branch, 4, 3, (offset & right_n_bits(3)) >> 1);\n+    Assembler::patch_c(branch, 6, 5, (offset & right_n_bits(8)) >> 6);\n+    Assembler::patch_c(branch, 11, 10, (offset & right_n_bits(5)) >> 3);\n+    Assembler::patch_c(branch, 12, 12, (offset & nth_bit(8)) >> 8);\n+    return NativeInstruction::compressed_instruction_size;                     \/\/ only one instruction\n+  }\n@@ -1214,1 +1262,4 @@\n-  const int MOVPTR_INSTRUCTIONS_NUM = 6;                                        \/\/ lui + addi + slli + addi + slli + addi\/jalr\/load\n+  \/\/ lui + addi + slli(C) + addi + slli(C) + addi\/jalr\/load\n+  const int size = !UseRVC ?\n+          6 * NativeInstruction::instruction_size :\n+          4 * NativeInstruction::instruction_size + 2 * NativeInstruction::compressed_instruction_size;\n@@ -1219,3 +1270,3 @@\n-  Assembler::patch(branch + 12, 31, 20, (lower >> 5) & 0x7ff);                  \/\/ Addi.            target[15: 5] ==> branch[31:20]\n-  Assembler::patch(branch + 20, 31, 20, lower & 0x1f);                          \/\/ Addi\/Jalr\/Load.  target[ 4: 0] ==> branch[31:20]\n-  return MOVPTR_INSTRUCTIONS_NUM * NativeInstruction::instruction_size;\n+  Assembler::patch(branch + (!UseRVC ? 12 : 10), 31, 20, (lower >> 5) & 0x7ff);  \/\/ Addi.    target[15: 5] ==> branch[31:20]\n+  Assembler::patch(branch + (!UseRVC ? 20 : 16), 31, 20, lower & 0x1f);  \/\/ Addi\/Jalr\/Load.  target[ 4: 0] ==> branch[31:20]\n+  return size;\n@@ -1225,1 +1276,4 @@\n-  const int LI64_INSTRUCTIONS_NUM = 8;                                          \/\/ lui + addi + slli + addi + slli + addi + slli + addi\n+  \/\/ lui + addi + slli(C) + addi + slli(C) + addi + slli(C) + addi\n+  const int size = !UseRVC ?\n+          8 * NativeInstruction::instruction_size :\n+          5 * NativeInstruction::instruction_size + 3 * NativeInstruction::compressed_instruction_size;\n@@ -1239,4 +1293,4 @@\n-  Assembler::patch(branch + 12, 31, 20, ((int32_t)lower >> 20) & 0xfff);            \/\/ Addi.\n-  Assembler::patch(branch + 20, 31, 20, (((intptr_t)target << 44) >> 52) & 0xfff);  \/\/ Addi.\n-  Assembler::patch(branch + 28, 31, 20, (intptr_t)target & 0xff);                   \/\/ Addi.\n-  return LI64_INSTRUCTIONS_NUM * NativeInstruction::instruction_size;\n+  Assembler::patch(branch + (!UseRVC ? 12 : 10), 31, 20, ((int32_t)lower >> 20) & 0xfff);            \/\/ Addi.\n+  Assembler::patch(branch + (!UseRVC ? 20 : 16), 31, 20, (((intptr_t)target << 44) >> 52) & 0xfff);  \/\/ Addi.\n+  Assembler::patch(branch + (!UseRVC ? 28 : 22), 31, 20, (intptr_t)target & 0xff);                   \/\/ Addi.\n+  return size;\n@@ -1256,2 +1310,1 @@\n-static long get_offset_of_jal(address insn_addr) {\n-  assert_cond(insn_addr != NULL);\n+static long get_offset_of_jal(unsigned insn) {\n@@ -1259,1 +1312,0 @@\n-  unsigned insn = *(unsigned*)insn_addr;\n@@ -1269,1 +1321,1 @@\n-static long get_offset_of_conditional_branch(address insn_addr) {\n+static long get_offset_of_conditional_branch(unsigned insn) {\n@@ -1271,2 +1323,0 @@\n-  assert_cond(insn_addr != NULL);\n-  unsigned insn = *(unsigned*)insn_addr;\n@@ -1274,3 +1324,3 @@\n-  offset = (offset << 12) | (((long)(Assembler::sextract(insn, 7, 7) & 0x1)) << 11);\n-  offset = offset | (((long)(Assembler::sextract(insn, 30, 25) & 0x3f)) << 5);\n-  offset = offset | (((long)(Assembler::sextract(insn, 11, 8) & 0xf)) << 1);\n+  offset = (offset << 12) | ((Assembler::sextract(insn, 7, 7) & 0x1) << 11);\n+  offset = offset | ((Assembler::sextract(insn, 30, 25) & 0x3f) << 5);\n+  offset = offset | ((Assembler::sextract(insn, 11, 8) & 0xf) << 1);\n@@ -1293,3 +1343,3 @@\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20)) << 16;                        \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[3], 31, 20)) << 5;                         \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[5], 31, 20));                              \/\/ Addi\/Jalr\/Load.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20)) << 16;                                     \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)(insn_addr - (!UseRVC ? 0 : 2)))[3], 31, 20)) << 5;         \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)(insn_addr - (!UseRVC ? 0 : 4)))[5], 31, 20));              \/\/ Addi\/Jalr\/Load.\n@@ -1303,3 +1353,3 @@\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[3], 31, 20)) << 20;                        \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[5], 31, 20)) << 8;                         \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[7], 31, 20));                              \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)(insn_addr - (!UseRVC ? 0 : 2)))[3], 31, 20)) << 20;        \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)(insn_addr - (!UseRVC ? 0 : 4)))[5], 31, 20)) << 8;         \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(((unsigned*)(insn_addr - (!UseRVC ? 0 : 6)))[7], 31, 20));              \/\/ Addi.\n@@ -1345,1 +1395,1 @@\n-    offset = get_offset_of_jal(insn_addr);\n+    offset = get_offset_of_jal(*(unsigned*)insn_addr);\n@@ -1347,1 +1397,1 @@\n-    offset = get_offset_of_conditional_branch(insn_addr);\n+    offset = get_offset_of_conditional_branch(*(unsigned*)insn_addr);\n@@ -1384,2 +1434,1 @@\n-      la_patchable(xheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()), offset);\n-      ld(xheapbase, Address(xheapbase, offset));\n+      ld_patchable(xheapbase, ExternalAddress((address)CompressedOops::ptrs_base_addr()), xheapbase);\n@@ -1405,1 +1454,1 @@\n-  movptr(Rd, dest.target());\n+  movptr(Rd, dest.target(), NOT_COMPRESSIBLE);\n@@ -2486,1 +2535,1 @@\n-    jalr(x0, tmp, offset);\n+    jalr_nc(x0, tmp, offset);\n@@ -2489,1 +2538,1 @@\n-    j(entry);\n+    j_nc(entry);\n@@ -2503,1 +2552,1 @@\n-    jalr(x1, tmp, offset); \/\/ link\n+    jalr_nc(x1, tmp, offset); \/\/ link\n@@ -2506,1 +2555,1 @@\n-    jal(entry); \/\/ link\n+    jal_nc(entry); \/\/ link\n@@ -2765,1 +2814,1 @@\n-    movptr_with_offset(reg1, dest.target(), offset);\n+    movptr_with_offset(reg1, dest.target(), offset, NOT_COMPRESSIBLE);\n@@ -2769,0 +2818,21 @@\n+void MacroAssembler::ld_patchable(Register Rd, const Address &dest, Register tmp) {\n+  int offset = 0;\n+  la_patchable(tmp, dest, offset);\n+  \/\/ C-Ext: use uncompressed instructions to match pd_patch_instruction_size()\n+  ld_nc(Rd, tmp, offset);\n+}\n+\n+void MacroAssembler::addi_patchable(Register Rd, const Address &dest, Register tmp) {\n+  int offset = 0;\n+  la_patchable(tmp, dest, offset);\n+  \/\/ C-Ext: use uncompressed instructions to match pd_patch_instruction_size()\n+  addi_nc(Rd, tmp, offset);\n+}\n+\n+void MacroAssembler::jalr_patchable(Register Rd, const Address &dest, Register tmp) {\n+  int offset = 0;\n+  la_patchable(tmp, dest, offset);\n+  \/\/ C-Ext: use uncompressed instructions to match pd_patch_instruction_size()\n+  jalr_nc(x1, tmp, offset);\n+}\n+\n@@ -2787,1 +2857,1 @@\n-void MacroAssembler::reserved_stack_check() {\n+void MacroAssembler::reserved_stack_check(bool compressible) {\n@@ -2796,3 +2866,1 @@\n-    int32_t offset = 0;\n-    la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone)), offset);\n-    jalr(x1, t0, offset);\n+    jalr_patchable(x1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::enable_stack_reserved_zone)), t0);\n@@ -2804,4 +2872,2 @@\n-    offset = 0;\n-    la_patchable(t0, RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()), offset);\n-    jalr(x0, t0, offset);\n-    should_not_reach_here();\n+    jalr_patchable(x0, RuntimeAddress(StubRoutines::throw_delayed_StackOverflowError_entry()), t0);\n+    should_not_reach_here(compressible);\n@@ -2895,1 +2961,1 @@\n-    jal(entry.target());\n+    jal_nc(entry.target());\n@@ -2897,1 +2963,1 @@\n-    jal(pc());\n+    jal_nc(pc());\n@@ -2906,1 +2972,1 @@\n-  movptr(t1, (address)Universe::non_oop_word());\n+  movptr(t1, (address)Universe::non_oop_word(), NOT_COMPRESSIBLE);\n@@ -2936,1 +3002,3 @@\n-  while (offset() % wordSize == 0) { nop(); }\n+  \/\/ C-Ext: when we reach here we may get a 2-byte alignment and\n+  \/\/   nop() will be 2 bytes in length.\n+  while (offset() % wordSize != 4) { nop(); }\n@@ -2946,2 +3014,2 @@\n-  ld(t0, target);  \/\/ auipc + ld\n-  jr(t0);          \/\/ jalr\n+  ld_nc(t0, target);  \/\/ auipc + ld\n+  jr_nc(t0);          \/\/ jalr\n@@ -2951,0 +3019,1 @@\n+  assert(offset() % wordSize == 0, \"address loaded by ld must be 8-byte aligned under riscv64\");\n@@ -2992,2 +3061,1 @@\n-  la_patchable(t0, src2, offset);\n-  ld(t0, Address(t0, offset));\n+  ld_patchable(t0, src2, t0);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":158,"deletions":90,"binary":false,"changes":248,"status":"modified"},{"patch":"@@ -139,1 +139,1 @@\n-  void set_last_Java_frame(Register last_java_sp, Register last_java_fp, address last_java_pc, Register temp);\n+  void set_last_Java_frame(Register last_java_sp, Register last_java_fp, address last_java_pc, Register temp, bool compressible = true);\n@@ -360,1 +360,1 @@\n-  void stop(const char* msg);\n+  void stop(const char* msg, bool compressible = true);\n@@ -366,1 +366,1 @@\n-  void should_not_reach_here() { stop(\"should not reach here\"); }\n+  void should_not_reach_here(bool compressible = true) { stop(\"should not reach here\", compressible); }\n@@ -401,0 +401,1 @@\n+  void nop_nc();\n@@ -447,0 +448,1 @@\n+  void bnez(Register Rs, const address &dest);\n@@ -451,1 +453,0 @@\n-  void bnez(Register Rs, const address &dest);\n@@ -453,1 +454,1 @@\n-  void la(Register Rd, const address &dest);\n+  void la(Register Rd, const address &dest, bool compressible = true);\n@@ -475,0 +476,6 @@\n+  \/\/ C-Ext: incompressible version\n+  void beqz_nc(Register Rs, const address &dest);\n+  void bnez_nc(Register Rs, const address &dest);\n+  void beqz_nc(Register Rs, Label &l, bool is_far = false);\n+  void bnez_nc(Register Rs, Label &l, bool is_far = false);\n+\n@@ -615,0 +622,6 @@\n+  \/\/ Note: programmers should use these functions\n+  \/\/       if wanting to write the same logic - to prevent from misuse.\n+  void ld_patchable(Register Rd, const Address &dest, Register tmp);\n+  void addi_patchable(Register Rd, const Address &dest, Register tmp);\n+  void jalr_patchable(Register Rd, const Address &dest, Register tmp);\n+\n@@ -636,1 +649,1 @@\n-  void reserved_stack_check();\n+  void reserved_stack_check(bool compressible = true);\n@@ -791,1 +804,1 @@\n-      ld(dest, const_addr);\n+      ld_nc(dest, const_addr);\n@@ -793,3 +806,1 @@\n-      int32_t offset = 0;\n-      la_patchable(dest, InternalAddress(const_addr.target()), offset);\n-      ld(dest, Address(dest, offset));\n+      ld_patchable(dest, InternalAddress(const_addr.target()), dest);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":21,"deletions":10,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -42,0 +42,88 @@\n+uint32_t NativeInstruction::extract_rs1(address instr, int &size) {\n+  assert_cond(instr != NULL);\n+  if (is_compressed_instr(instr)) {\n+    size = compressed_instruction_size;\n+    uint16_t op = Assembler::extract_c(((uint16_t*)instr)[0], 1, 0);\n+    switch (op) {\n+      case 0b00: {\n+        return Assembler::extract_c(((uint16_t*)instr)[0], 9, 7);\n+      }\n+      case 0b01: {\n+        if (!is_set_nth_bit(((uint16_t*)instr)[0], 15)) {\n+          return Assembler::extract_c(((uint16_t*)instr)[0], 11, 7);\n+        } else {\n+          return Assembler::extract_c(((uint16_t*)instr)[0], 9, 7);\n+        }\n+      }\n+      case 0b10: {\n+        return Assembler::extract_c(((uint16_t*)instr)[0], 11, 7);\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    return 0;\n+  } else {\n+    size = instruction_size;\n+    return Assembler::extract(((unsigned*)instr)[0], 19, 15);\n+  }\n+}\n+\n+uint32_t NativeInstruction::extract_rs2(address instr, int &size) {\n+  assert_cond(instr != NULL);\n+  if (is_compressed_instr(instr)) {\n+    size = compressed_instruction_size;\n+    uint16_t op = Assembler::extract_c(((uint16_t*)instr)[0], 1, 0);\n+    switch (op) {\n+      case 0b00: {\n+        return Assembler::extract_c(((uint16_t*)instr)[0], 4, 2);\n+      }\n+      case 0b01: {\n+        if (!is_set_nth_bit(((uint16_t*)instr)[0], 15)) {\n+          ShouldNotReachHere();\n+          return 0;\n+        } else {\n+          return Assembler::extract_c(((uint16_t*)instr)[0], 4, 2);\n+        }\n+      }\n+      case 0b10: {\n+        return Assembler::extract_c(((uint16_t*)instr)[0], 6, 2);\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    return 0;\n+  } else {\n+    size = instruction_size;\n+    return Assembler::extract(((unsigned*)instr)[0], 24, 20);\n+  }\n+}\n+\n+uint32_t NativeInstruction::extract_rd(address instr, int &size) {\n+  assert_cond(instr != NULL);\n+  if (is_compressed_instr(instr)) {\n+    size = compressed_instruction_size;\n+    uint16_t op = Assembler::extract_c(((uint16_t*)instr)[0], 1, 0);\n+    switch (op) {\n+      case 0b00: {\n+        return Assembler::extract_c(((uint16_t*)instr)[0], 4, 2);\n+      }\n+      case 0b01: {\n+        if (!is_set_nth_bit(((uint16_t*)instr)[0], 15)) {\n+          return Assembler::extract_c(((uint16_t*)instr)[0], 11, 7);\n+        } else {\n+          return Assembler::extract_c(((uint16_t*)instr)[0], 9, 7);\n+        }\n+      }\n+      case 0b10: {\n+        return Assembler::extract_c(((uint16_t*)instr)[0], 11, 7);\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n+    return 0;\n+  } else {\n+    size = instruction_size;\n+    return Assembler::extract(((unsigned*)instr)[0], 11, 7);\n+  }\n+}\n+\n@@ -47,6 +135,6 @@\n-  if ((is_auipc_at(instr)) &&\n-      (is_addi_at(instr + 4) || is_jalr_at(instr + 4) || is_load_at(instr + 4) || is_float_load_at(instr + 4)) &&\n-      check_pc_relative_data_dependency(instr)) {\n-    return true;\n-  }\n-  return false;\n+  return (is_auipc_at(instr)) &&\n+         (is_addi_at(instr + instruction_size) ||\n+          is_jalr_at(instr + instruction_size) ||\n+          is_load_at(instr + instruction_size) ||\n+          is_float_load_at(instr + instruction_size)) &&\n+         check_pc_relative_data_dependency(instr);\n@@ -57,6 +145,3 @@\n-  if (is_auipc_at(instr) && \/\/ auipc\n-      is_ld_at(instr + 4) && \/\/ ld\n-      check_load_pc_relative_data_dependency(instr)) {\n-      return true;\n-  }\n-  return false;\n+  return is_auipc_at(instr) && \/\/ auipc\n+         is_ld_at(instr + instruction_size) && \/\/ ld\n+         check_load_pc_relative_data_dependency(instr);\n@@ -66,10 +151,9 @@\n-  if (is_lui_at(instr) && \/\/ Lui\n-      is_addi_at(instr + 4) && \/\/ Addi\n-      is_slli_shift_at(instr + 8, 11) && \/\/ Slli Rd, Rs, 11\n-      is_addi_at(instr + 12) && \/\/ Addi\n-      is_slli_shift_at(instr + 16, 5) && \/\/ Slli Rd, Rs, 5\n-      (is_addi_at(instr + 20) || is_jalr_at(instr + 20) || is_load_at(instr + 20)) && \/\/ Addi\/Jalr\/Load\n-      check_movptr_data_dependency(instr)) {\n-    return true;\n-  }\n-  return false;\n+  address pos = instr;\n+  int size = 0;\n+  return is_lui_at(pos) && \/\/ Lui\n+         is_addi_at(pos += instruction_size) && \/\/ Addi\n+         is_slli_shift_at(pos += instruction_size, 11, size) && \/\/ Slli Rd, Rs, 11\n+         is_addi_at(pos += size) && \/\/ Addi\n+         is_slli_shift_at(pos += instruction_size, 5, size) && \/\/ Slli Rd, Rs, 5\n+         (is_addi_at(pos += size) || is_jalr_at(pos) || is_load_at(pos)) && \/\/ Addi\/Jalr\/Load\n+         check_movptr_data_dependency(instr);\n@@ -79,6 +163,4 @@\n-  if (is_lui_at(instr) && \/\/ lui\n-      is_addiw_at(instr + 4) && \/\/ addiw\n-      check_li32_data_dependency(instr)) {\n-    return true;\n-  }\n-  return false;\n+  address pos = instr;\n+  return is_lui_at(pos) && \/\/ lui\n+         is_addiw_at(pos += instruction_size) && \/\/ addiw\n+         check_li32_data_dependency(instr);\n@@ -88,12 +170,11 @@\n-  if (is_lui_at(instr) && \/\/ lui\n-      is_addi_at(instr + 4) && \/\/ addi\n-      is_slli_shift_at(instr + 8, 12)&&  \/\/ Slli Rd, Rs, 12\n-      is_addi_at(instr + 12) && \/\/ addi\n-      is_slli_shift_at(instr + 16, 12) && \/\/ Slli Rd, Rs, 12\n-      is_addi_at(instr + 20) && \/\/ addi\n-      is_slli_shift_at(instr + 24, 8) && \/\/ Slli Rd, Rs, 8\n-      is_addi_at(instr + 28) && \/\/ addi\n-      check_li64_data_dependency(instr)) {\n-    return true;\n-  }\n-  return false;\n+  address pos = instr;\n+  int size = 0;\n+  return is_lui_at(pos) && \/\/ lui\n+         is_addi_at(pos += instruction_size) && \/\/ addi\n+         is_slli_shift_at(pos += instruction_size, 12, size) &&  \/\/ Slli Rd, Rs, 12\n+         is_addi_at(pos += size) && \/\/ addi\n+         is_slli_shift_at(pos += instruction_size, 12, size) &&  \/\/ Slli Rd, Rs, 12\n+         is_addi_at(pos += size) && \/\/ addi\n+         is_slli_shift_at(pos += instruction_size, 8, size) &&   \/\/ Slli Rd, Rs, 8\n+         is_addi_at(pos += size) && \/\/ addi\n+         check_li64_data_dependency(instr);\n@@ -206,1 +287,1 @@\n-    ICache::invalidate_range(instruction_address(), movptr_instruction_size);\n+    ICache::invalidate_range(instruction_address(), get_movptr_instruction_size());\n@@ -344,1 +425,1 @@\n-  assert(nativeInstruction_at(verified_entry)->is_jump_or_nop() ||\n+  assert(nativeInstruction_at(verified_entry)->is_jump_or_nop_nc() ||\n@@ -374,1 +455,1 @@\n-  CodeBuffer cb(code_pos, instruction_size);\n+  CodeBuffer cb(code_pos, get_instruction_size());\n@@ -378,2 +459,2 @@\n-  a.movptr_with_offset(t0, entry, offset); \/\/ lui, addi, slli, addi, slli\n-  a.jalr(x0, t0, offset); \/\/ jalr\n+  a.movptr_with_offset(t0, entry, offset, NOT_COMPRESSIBLE); \/\/ lui, addi, slli, addi, slli\n+  a.jalr_nc(x0, t0, offset); \/\/ jalr\n@@ -381,1 +462,1 @@\n-  ICache::invalidate_range(code_pos, instruction_size);\n+  ICache::invalidate_range(code_pos, get_instruction_size());\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.cpp","additions":127,"deletions":46,"binary":false,"changes":173,"status":"modified"},{"patch":"@@ -56,1 +56,2 @@\n-    instruction_size = 4\n+    instruction_size = 4,\n+    compressed_instruction_size = 2,\n@@ -68,0 +69,10 @@\n+  static bool is_compressed_instr(address instr) {\n+    if ((((unsigned*)instr)[0] & 0b11) == 0b11) {\n+      return false;\n+    }\n+    assert((((uint16_t *)instr)[0] & 0b11) != 0b11, \"seems instr is not an illegal instruction beginning: 0x%x\", ((unsigned*)instr)[0]);\n+    return true;\n+  }\n+  static int instr_size(address instr) {\n+    return is_compressed_instr(instr) ? compressed_instruction_size : instruction_size;\n+  }\n@@ -70,1 +81,1 @@\n-                                                Assembler::extract(((unsigned*)instr)[0], 14, 12) == 0b000); }\n+                                                                                    Assembler::extract(((unsigned*)instr)[0], 14, 12) == 0b000); }\n@@ -73,1 +84,1 @@\n-                                                Assembler::extract(((unsigned*)instr)[0], 14, 12) == 0b011); }\n+                                                                                    Assembler::extract(((unsigned*)instr)[0], 14, 12) == 0b011); }\n@@ -79,1 +90,1 @@\n-                                                Assembler::extract(((unsigned*)instr)[0], 14, 12) == 0b000); }\n+                                                                                    Assembler::extract(((unsigned*)instr)[0], 14, 12) == 0b000); }\n@@ -81,1 +92,1 @@\n-                                                Assembler::extract(((unsigned*)instr)[0], 14, 12) == 0b000); }\n+                                                                                    Assembler::extract(((unsigned*)instr)[0], 14, 12) == 0b000); }\n@@ -83,1 +94,7 @@\n-  static bool is_slli_shift_at(address instr, uint32_t shift) {\n+  static bool is_slli_shift_at(address instr, uint32_t shift) { int size = 0; return is_slli_shift_at(instr, shift, size); }\n+  static uint16_t extract_slli_c(address instr) {\n+    uint16_t low5 = Assembler::extract_c(((uint16_t*)instr)[0], 6, 2);\n+    uint16_t high1 = Assembler::extract_c(((uint16_t*)instr)[0], 12, 12);\n+    return (high1 << 5 | low5);\n+  }\n+  static bool is_slli_shift_at(address instr, uint32_t shift, int &size) {\n@@ -85,0 +102,6 @@\n+    if (is_compressed_instr(instr)) {\n+      return Assembler::extract_c(((uint16_t*)instr)[0], 15, 13) == 0b000 &&\n+             Assembler::extract_c(((uint16_t*)instr)[0], 1, 0) == 0b10 &&\n+             extract_slli_c(instr) == shift &&\n+             (size = compressed_instruction_size);\n+    }\n@@ -87,1 +110,2 @@\n-            Assembler::extract(((unsigned*)instr)[0], 25, 20) == shift);    \/\/ shamt field\n+            Assembler::extract(((unsigned*)instr)[0], 25, 20) == shift) &&  \/\/ shamt field\n+            (size = instruction_size);\n@@ -96,0 +120,7 @@\n+  static uint32_t extract_rs1(address instr) { int size = 0; return extract_rs1(instr, size); }\n+  static uint32_t extract_rs2(address instr) { int size = 0; return extract_rs2(instr, size); }\n+  static uint32_t extract_rd(address instr) { int size = 0; return extract_rd(instr, size); }\n+  static uint32_t extract_rs1(address instr, int &size);\n+  static uint32_t extract_rs2(address instr, int &size);\n+  static uint32_t extract_rd(address instr, int &size);\n+\n@@ -104,9 +135,15 @@\n-    return compare_instr_field(instr + 4, 19, 15, instr, 11, 7)       &&     \/\/ check the rs1 field of addi and the rd field of lui\n-           compare_instr_field(instr + 4, 19, 15, instr + 4, 11, 7)   &&     \/\/ check the rs1 field and the rd field of addi\n-           compare_instr_field(instr + 8, 19, 15, instr + 4, 11, 7)   &&     \/\/ check the rs1 field of slli and the rd field of addi\n-           compare_instr_field(instr + 8, 19, 15, instr + 8, 11, 7)   &&     \/\/ check the rs1 field and the rd field of slli\n-           compare_instr_field(instr + 12, 19, 15, instr + 8, 11, 7)  &&     \/\/ check the rs1 field of addi and the rd field of slli\n-           compare_instr_field(instr + 12, 19, 15, instr + 12, 11, 7) &&     \/\/ check the rs1 field and the rd field of addi\n-           compare_instr_field(instr + 16, 19, 15, instr + 12, 11, 7) &&     \/\/ check the rs1 field of slli and the rd field of addi\n-           compare_instr_field(instr + 16, 19, 15, instr + 16, 11, 7) &&     \/\/ check the rs1 field and the rd field of slli\n-           compare_instr_field(instr + 20, 19, 15, instr + 16, 11, 7);       \/\/ check the rs1 field of addi\/jalr\/load and the rd field of slli\n+    address lui = instr;\n+    address addi1 = lui + instruction_size;\n+    address slli1 = addi1 + instruction_size;\n+    address addi2 = slli1 + instr_size(slli1);\n+    address slli2 = addi2 + instruction_size;\n+    address final = slli2 + instr_size(slli2);\n+    return extract_rs1(addi1) == extract_rd(lui) &&\n+           extract_rs1(addi1) == extract_rd(addi1) &&\n+           extract_rs1(slli1) == extract_rd(addi1) &&\n+           extract_rs1(slli1) == extract_rd(slli1) &&\n+           extract_rs1(addi2) == extract_rd(slli1) &&\n+           extract_rs1(addi2) == extract_rd(addi2) &&\n+           extract_rs1(slli2) == extract_rd(addi2) &&\n+           extract_rs1(slli2) == extract_rd(slli2) &&\n+           extract_rs1(final) == extract_rd(slli2);\n@@ -124,15 +161,23 @@\n-  static bool check_li64_data_dependency(address instr) {\n-    return compare_instr_field(instr + 4, 19, 15, instr, 11, 7)       &&  \/\/ check the rs1 field of addi and the rd field of lui\n-           compare_instr_field(instr + 4, 19, 15, instr + 4, 11, 7)   &&  \/\/ check the rs1 field and the rd field of addi\n-           compare_instr_field(instr + 8, 19, 15, instr + 4, 11, 7)   &&  \/\/ check the rs1 field of slli and the rd field of addi\n-           compare_instr_field(instr + 8, 19, 15, instr + 8, 11, 7)   &&  \/\/ check the rs1 field and the rd field of slli\n-           compare_instr_field(instr + 12, 19, 15, instr + 8, 11, 7)  &&  \/\/ check the rs1 field of addi and the rd field of slli\n-           compare_instr_field(instr + 12, 19, 15, instr + 12, 11, 7) &&  \/\/ check the rs1 field and the rd field of addi\n-           compare_instr_field(instr + 16, 19, 15, instr + 12, 11, 7) &&  \/\/ check the rs1 field of slli and the rd field of addi\n-           compare_instr_field(instr + 16, 19, 15, instr + 16, 11, 7) &&  \/\/ check the rs1 field and the rd field fof slli\n-           compare_instr_field(instr + 20, 19, 15, instr + 16, 11, 7) &&  \/\/ check the rs1 field of addi and the rd field of slli\n-           compare_instr_field(instr + 20, 19, 15, instr + 20, 11, 7) &&  \/\/ check the rs1 field and the rd field of addi\n-           compare_instr_field(instr + 24, 19, 15, instr + 20, 11, 7) &&  \/\/ check the rs1 field of slli and the rd field of addi\n-           compare_instr_field(instr + 24, 19, 15, instr + 24, 11, 7) &&  \/\/ check the rs1 field and the rd field of slli\n-           compare_instr_field(instr + 28, 19, 15, instr + 24, 11, 7) &&  \/\/ check the rs1 field of addi and the rd field of slli\n-           compare_instr_field(instr + 28, 19, 15, instr + 28, 11, 7);    \/\/ check the rs1 field and the rd field of addi\n+  static bool check_li64_data_dependency(address instr) {  \/\/ FIXME: maybe retrive back origin code because we can only optimize 'slli' here.\n+    address lui = instr;\n+    address addi1 = lui + instruction_size;\n+    address slli1 = addi1 + instruction_size;\n+    address addi2 = slli1 + instr_size(slli1);\n+    address slli2 = addi2 + instruction_size;\n+    address addi3 = slli2 + instr_size(slli2);\n+    address slli3 = addi3 + instruction_size;\n+    address addi4 = slli3 + instr_size(slli3);\n+    return extract_rs1(addi1) == extract_rd(lui) &&\n+           extract_rs1(addi1) == extract_rd(addi1) &&\n+           extract_rs1(slli1) == extract_rd(addi1) &&\n+           extract_rs1(slli1) == extract_rd(slli1) &&\n+           extract_rs1(addi2) == extract_rd(slli1) &&\n+           extract_rs1(addi2) == extract_rd(addi2) &&\n+           extract_rs1(slli2) == extract_rd(addi2) &&\n+           extract_rs1(slli2) == extract_rd(slli2) &&\n+           extract_rs1(addi3) == extract_rd(slli2) &&\n+           extract_rs1(addi3) == extract_rd(addi3) &&\n+           extract_rs1(slli3) == extract_rd(addi3) &&\n+           extract_rs1(slli3) == extract_rd(slli3) &&\n+           extract_rs1(addi4) == extract_rd(slli3) &&\n+           extract_rs1(addi4) == extract_rd(addi4);\n@@ -145,2 +190,5 @@\n-    return compare_instr_field(instr + 4, 19, 15, instr, 11, 7) &&     \/\/ check the rs1 field of addiw and the rd field of lui\n-           compare_instr_field(instr + 4, 19, 15, instr + 4, 11, 7);   \/\/ check the rs1 field and the rd field of addiw\n+    address lui = instr;\n+    address addiw = lui + instruction_size;\n+\n+    return extract_rs1(addiw) == extract_rd(lui) &&\n+           extract_rs1(addiw) == extract_rd(addiw);\n@@ -153,1 +201,4 @@\n-    return compare_instr_field(instr, 11, 7, instr + 4, 19, 15);          \/\/ check the rd field of auipc and the rs1 field of jalr\/addi\/load\/float_load\n+    address auipc = instr;\n+    address final = auipc + instruction_size;\n+\n+    return extract_rs1(final) == extract_rd(auipc);\n@@ -160,2 +211,5 @@\n-    return compare_instr_field(instr, 11, 7, instr + 4, 11, 7) &&      \/\/ check the rd field of auipc and the rd field of load\n-           compare_instr_field(instr + 4, 19, 15, instr + 4, 11, 7);   \/\/ check the rs1 field of load and the rd field of load\n+    address auipc = instr;\n+    address load = auipc + instruction_size;\n+\n+    return extract_rd(load) == extract_rd(auipc) &&\n+           extract_rs1(load) == extract_rd(load);\n@@ -171,0 +225,1 @@\n+    assert(!is_compressed_instr(instr), \"we need to reserve the 4-byte instruction to handle all cases\");\n@@ -179,0 +234,2 @@\n+  inline bool is_compressed_nop();\n+  inline bool is_uncompressed_nop();\n@@ -181,1 +238,1 @@\n-  inline bool is_jump_or_nop();\n+  inline bool is_jump_or_nop_nc();\n@@ -192,0 +249,1 @@\n+  jushort uint16_at(int offset) const { return *(jushort *) addr_at(offset); }\n@@ -321,1 +379,2 @@\n-    movptr_instruction_size             =    6 * NativeInstruction::instruction_size, \/\/ lui, addi, slli, addi, slli, addi.  See movptr().\n+    movptr_instruction_size      =    6 * NativeInstruction::instruction_size, \/\/ lui, addi, slli, addi, slli, addi.  See movptr().\n+    compressed_movptr_instruction_size  =    4 * NativeInstruction::instruction_size + 2 * NativeInstruction::compressed_instruction_size, \/\/ lui, addi, slli(C), addi, slli(C), addi.  See movptr().\n@@ -323,0 +382,1 @@\n+    compressed_movptr_with_offset_instruction_size = 3 * NativeInstruction::instruction_size + 2 * NativeInstruction::compressed_instruction_size, \/\/ lui, addi, slli(C), addi, slli(C). See movptr_with_offset().\n@@ -328,0 +388,8 @@\n+  static const int get_movptr_with_offset_instruction_size() {\n+    return !UseRVC ? movptr_with_offset_instruction_size : compressed_movptr_with_offset_instruction_size;\n+  }\n+\n+  static const int get_movptr_instruction_size() {\n+    return !UseRVC ? movptr_instruction_size : compressed_movptr_instruction_size;\n+  }\n+\n@@ -336,1 +404,1 @@\n-      if (is_addi_at(addr_at(movptr_with_offset_instruction_size))) {\n+      if (is_addi_at(addr_at(get_movptr_with_offset_instruction_size()))) {\n@@ -338,1 +406,1 @@\n-        return addr_at(movptr_instruction_size);\n+        return addr_at(get_movptr_instruction_size());\n@@ -341,1 +409,1 @@\n-        return addr_at(movptr_with_offset_instruction_size);\n+        return addr_at(get_movptr_with_offset_instruction_size());\n@@ -356,1 +424,1 @@\n-      ICache::invalidate_range(instruction_address(), movptr_instruction_size);\n+      ICache::invalidate_range(instruction_address(), get_movptr_instruction_size());\n@@ -425,1 +493,1 @@\n-    instruction_size            =    4,\n+    instruction_size            =    NativeInstruction::instruction_size,\n@@ -428,1 +496,1 @@\n-    next_instruction_offset     =    4\n+    next_instruction_offset     =    NativeInstruction::instruction_size\n@@ -459,1 +527,2 @@\n-    instruction_size            =    6 * NativeInstruction::instruction_size, \/\/ lui, addi, slli, addi, slli, jalr\n+    instruction_size     =    6 * NativeInstruction::instruction_size, \/\/ lui, addi, slli, addi, slli, jalr\n+    compressed_instruction_size =    4 * NativeInstruction::instruction_size + 2 * NativeInstruction::compressed_instruction_size, \/\/ lui, addi, slli(C), addi, slli(C), jalr\n@@ -462,1 +531,2 @@\n-    next_instruction_offset     =    6 * NativeInstruction::instruction_size  \/\/ lui, addi, slli, addi, slli, jalr\n+    normal_next_instruction_offset     =    6 * NativeInstruction::instruction_size,  \/\/ lui, addi, slli, addi, slli, jalr\n+    compressed_next_instruction_offset =    4 * NativeInstruction::instruction_size + 2 * NativeInstruction::compressed_instruction_size  \/\/ lui, addi, slli(C), addi, slli(C), jalr\n@@ -465,0 +535,4 @@\n+  static const int get_instruction_size() {\n+    return !UseRVC ? instruction_size : compressed_instruction_size;\n+  }\n+\n@@ -484,2 +558,16 @@\n-inline bool NativeInstruction::is_nop()         {\n-  uint32_t insn = *(uint32_t*)addr_at(0);\n+inline bool NativeInstruction::is_nop() {\n+  return is_compressed_nop() || is_uncompressed_nop();\n+}\n+\n+inline bool NativeInstruction::is_compressed_nop() {\n+  address instr_addr = addr_at(0);\n+  if (is_compressed_instr(instr_addr)) {\n+    uint16_t insn = *(uint16_t*)instr_addr;\n+    return insn == 0x1;\n+  }\n+  return false;\n+}\n+\n+inline bool NativeInstruction::is_uncompressed_nop() {\n+  address instr_addr = addr_at(0);\n+  uint32_t insn = *(uint32_t*)instr_addr;\n@@ -489,2 +577,2 @@\n-inline bool NativeInstruction::is_jump_or_nop() {\n-  return is_nop() || is_jump();\n+inline bool NativeInstruction::is_jump_or_nop_nc() {\n+  return is_uncompressed_nop() || is_jump();\n","filename":"src\/hotspot\/cpu\/riscv\/nativeInst_riscv.hpp","additions":138,"deletions":50,"binary":false,"changes":188,"status":"modified"},{"patch":"@@ -61,1 +61,5 @@\n-    max_slots_per_register   = 2\n+    max_slots_per_register   = 2,\n+\n+    \/\/ C-Ext: integer registers in the range of [x8~x15] are correspond for RVC. Please see Table 16.2 in spec.\n+    compressed_register_base = 8,\n+    compressed_register_top  = 15,\n@@ -74,0 +78,1 @@\n+  int   compressed_encoding() const              { assert(is_compressed_valid(), \"invalid compressed register\"); return ((intptr_t)this - compressed_register_base); }\n@@ -75,0 +80,1 @@\n+  bool  is_compressed_valid() const              { return compressed_register_base <= (intptr_t)this && (intptr_t)this <= compressed_register_top; }\n@@ -78,0 +84,1 @@\n+  int   compressed_encoding_nocheck() const      { return ((intptr_t)this - compressed_register_base); }\n@@ -134,1 +141,5 @@\n-    max_slots_per_register  = 2\n+    max_slots_per_register  = 2,\n+\n+    \/\/ C-Ext: float registers in the range of [f8~f15] are correspond for RVC. Please see Table 16.2 in spec.\n+    compressed_register_base = 8,\n+    compressed_register_top  = 15,\n@@ -147,0 +158,1 @@\n+  int   compressed_encoding() const               { assert(is_compressed_valid(), \"invalid compressed register\"); return ((intptr_t)this - compressed_register_base); }\n@@ -148,0 +160,1 @@\n+  int   compressed_encoding_nocheck() const       { return ((intptr_t)this - compressed_register_base); }\n@@ -149,0 +162,1 @@\n+  bool  is_compressed_valid() const               { return compressed_register_base <= (intptr_t)this && (intptr_t)this <= compressed_register_top; }\n","filename":"src\/hotspot\/cpu\/riscv\/register_riscv.hpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -1159,3 +1159,2 @@\n-  \/\/ call should be a simple jal\n-  int off = 4;\n-  return off;\n+  \/\/ jal\n+  return 1 * NativeInstruction::instruction_size;\n@@ -1166,1 +1165,3 @@\n-  return 28; \/\/ movptr, jal\n+  return 4 * NativeInstruction::instruction_size +\n+         2 * (!UseRVC ? NativeInstruction::instruction_size : NativeInstruction::compressed_instruction_size) +\n+         1 * NativeInstruction::instruction_size; \/\/ movptr, jal\n@@ -1174,1 +1175,1 @@\n-  \/\/ for real runtime callouts it will be six instructions\n+  \/\/ for real runtime callouts it will be 12 instructions\n@@ -1176,6 +1177,6 @@\n-  \/\/   la(t1, retaddr)\n-  \/\/   la(t0, RuntimeAddress(addr))\n-  \/\/   addi(sp, sp, -2 * wordSize)\n-  \/\/   sd(zr, Address(sp))\n-  \/\/   sd(t1, Address(sp, wordSize))\n-  \/\/   jalr(t0)\n+  \/\/   la(t1, retaddr)                ->  auipc + addi\n+  \/\/   la(t0, RuntimeAddress(addr))   ->  lui + addi + slli(C) + addi + slli(C) + addi\n+  \/\/   addi(sp, sp, -2 * wordSize)    ->  addi(C)\n+  \/\/   sd(zr, Address(sp))            ->  sd(C)\n+  \/\/   sd(t1, Address(sp, wordSize))  ->  sd(C)\n+  \/\/   jalr(t0)                       ->  jalr(C)\n@@ -1186,1 +1187,8 @@\n-    return 12 * NativeInstruction::instruction_size;\n+    const int instruction_size = NativeInstruction::instruction_size;\n+    const int compressed_instruction_size = (!UseRVC ? instruction_size : NativeInstruction::compressed_instruction_size);\n+    return 2 * instruction_size +\n+           4 * instruction_size + 2 * compressed_instruction_size +\n+           1 * compressed_instruction_size +\n+           1 * compressed_instruction_size +\n+           1 * compressed_instruction_size +\n+           1 * compressed_instruction_size;\n@@ -1195,0 +1203,35 @@\n+\/\/ C-Ext: With C-Ext a call may get 2-byte aligned.\n+\/\/   The offset encoding in jal ranges bits [12, 31], which could span the cache line.\n+\/\/   Patching this unaligned address will make the write operation not atomic.\n+\/\/   Other threads may be running the same piece of code at full speed, causing concurrency issues.\n+\/\/   So we must ensure that it does not span a cache line so that it can be patched.\n+int CallStaticJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  \/\/ to make sure the address of jal 4-byte aligned.\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n+\n+\/\/ C-Ext: With C-Ext a call may get 2-byte aligned.\n+\/\/   The offset encoding in jal ranges bits [12, 31], which could span the cache line.\n+\/\/   Patching this unaligned address will make the write operation not atomic.\n+\/\/   Other threads may be running the same piece of code at full speed, causing concurrency issues.\n+\/\/   So we must ensure that it does not span a cache line so that it can be patched.\n+int CallDynamicJavaDirectNode::compute_padding(int current_offset) const\n+{\n+  \/\/ skip the movptr in MacroAssembler::ic_call():\n+  \/\/ lui + addi + slli(C) + addi + slli(C) + addi\n+  \/\/ Though movptr() has already 4-byte aligned with or without C-Ext,\n+  \/\/ We need to prevent from further changes by explicitly calculating the size.\n+  const int instruction_size = NativeInstruction::instruction_size;\n+  const int compressed_instruction_size = (!UseRVC ? instruction_size : NativeInstruction::compressed_instruction_size);\n+  const int movptr_size =\n+         2 * instruction_size +\n+         1 * compressed_instruction_size +\n+         1 * instruction_size +\n+         1 * compressed_instruction_size +\n+         1 * instruction_size;\n+  current_offset += movptr_size;\n+  \/\/ to make sure the address of jal 4-byte aligned.\n+  return align_up(current_offset, alignment_required()) - current_offset;\n+}\n+\n@@ -1229,1 +1272,1 @@\n-    return _count * NativeInstruction::instruction_size;\n+    return _count * (!UseRVC ? NativeInstruction::instruction_size : NativeInstruction::compressed_instruction_size);\n@@ -1298,1 +1341,1 @@\n-  __ nop();\n+  __ nop_nc();  \/\/ 4 bytes\n@@ -1390,1 +1433,8 @@\n-    __ reserved_stack_check();\n+    \/\/ C-Ext: we need to emit instructions of the same constant size here.\n+    \/\/   This Node will emit should_not_reach_here(), further emitting a movptr of pc() address.\n+    \/\/   However, C2 will do PhaseOutput::scratch_emit_size() to simulate the size of Node -\n+    \/\/   this time, the pc() is a different value from the final emission and it may get compressed.\n+    \/\/   We may get a case that Node size is different between scratch_emit and real emission phase,\n+    \/\/   which are not allowed. So we need to emit the same constant size by disabling compression\n+    \/\/   of the movptr of pc() to align with the logic.\n+    __ reserved_stack_check(NOT_COMPRESSIBLE);\n@@ -1647,1 +1697,2 @@\n-    __ addi(as_Register(reg), sp, offset);\n+    \/\/ C-Ext: See BoxLockNode::size(). We need to manually calculate this node's size.\n+    __ addi_nc(as_Register(reg), sp, offset);\n@@ -9795,0 +9846,1 @@\n+  ins_alignment(4);\n@@ -9814,0 +9866,1 @@\n+  ins_alignment(4);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":69,"deletions":16,"binary":false,"changes":85,"status":"modified"},{"patch":"@@ -351,3 +351,1 @@\n-  int32_t offset = 0;\n-  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite)), offset);\n-  __ jalr(x1, t0, offset);\n+  __ jalr_patchable(x1, RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite)), t0);\n@@ -1023,3 +1021,1 @@\n-    int32_t offset = 0;\n-    __ la_patchable(t0, RuntimeAddress(dest), offset);\n-    __ jalr(x1, t0, offset);\n+    __ jalr_patchable(x1, RuntimeAddress(dest), t0);\n@@ -1150,1 +1146,1 @@\n-    __ nop();\n+    __ nop_nc();\n@@ -1301,1 +1297,1 @@\n-  __ nop();\n+  __ nop_nc();\n@@ -1802,3 +1798,1 @@\n-    int32_t offset = 0;\n-    __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)), offset);\n-    __ jalr(x1, t0, offset);\n+    __ jalr_patchable(x1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)), t0);\n@@ -2021,3 +2015,1 @@\n-  int32_t offset = 0;\n-  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info)), offset);\n-  __ jalr(x1, t0, offset);\n+  __ jalr_patchable(x1, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info)), t0);\n@@ -2159,3 +2151,1 @@\n-  offset = 0;\n-  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)), offset);\n-  __ jalr(x1, t0, offset);\n+  __ jalr_patchable(x1, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)), t0);\n@@ -2245,2 +2235,1 @@\n-  int32_t offset = 0;\n-  __ la_patchable(t0,\n+  __ jalr_patchable(x1,\n@@ -2248,2 +2237,1 @@\n-                                        Deoptimization::uncommon_trap)), offset);\n-  __ jalr(x1, t0, offset);\n+                                        Deoptimization::uncommon_trap)), t0);\n@@ -2371,3 +2359,1 @@\n-  offset = 0;\n-  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)), offset);\n-  __ jalr(x1, t0, offset);\n+  __ jalr_patchable(x1, RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)), t0);\n@@ -2442,3 +2428,1 @@\n-  int32_t offset = 0;\n-  __ la_patchable(t0, RuntimeAddress(call_ptr), offset);\n-  __ jalr(x1, t0, offset);\n+  __ jalr_patchable(x1, RuntimeAddress(call_ptr), t0);\n@@ -2552,3 +2536,1 @@\n-    int32_t offset = 0;\n-    __ la_patchable(t0, RuntimeAddress(destination), offset);\n-    __ jalr(x1, t0, offset);\n+    __ jalr_patchable(x1, RuntimeAddress(destination), t0);\n@@ -2691,3 +2673,1 @@\n-  int32_t offset = 0;\n-  __ la_patchable(t0, RuntimeAddress(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C)), offset);\n-  __ jalr(x1, t0, offset);\n+  __ jalr_patchable(x1, RuntimeAddress(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C)), t0);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":13,"deletions":33,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -106,0 +106,6 @@\n+  \/\/ compressed instruction extension\n+  if (UseRVC && !(_features & CPU_C)) {\n+    warning(\"RVC is not supported on this CPU\");\n+    FLAG_SET_DEFAULT(UseRVC, false);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/vm_version_riscv.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -434,1 +434,1 @@\n-      _bytes_to_copy = MAX2(n_move->num_bytes_to_end_of_patch(), (int)NativeGeneralJump::instruction_size);\n+      _bytes_to_copy = MAX2(n_move->num_bytes_to_end_of_patch(), NOT_RISCV((int)NativeGeneralJump::instruction_size) RISCV_ONLY(NativeGeneralJump::get_instruction_size()));\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-  while ((intx) _masm->pc() - (intx) patch->pc_start() < NativeGeneralJump::instruction_size) {\n+  while ((intx) _masm->pc() - (intx) patch->pc_start() < NOT_RISCV(NativeGeneralJump::instruction_size) RISCV_ONLY(NativeGeneralJump::get_instruction_size()) ) {\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -332,1 +332,1 @@\n-#if defined(X86) && !defined(AMD64)\n+#if (defined(X86) && !defined(AMD64)) || defined(RISCV)\n","filename":"src\/hotspot\/share\/runtime\/flags\/jvmFlagConstraintsCompiler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}