{"files":[{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -82,0 +82,4 @@\n+void Assembler::zext_w(Register Rd, Register Rs) {\n+  add_uw(Rd, Rs, zr);\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -1214,0 +1214,746 @@\n+#undef INSN\n+\n+\/\/ ==========================\n+\/\/ RISC-V Vector Extension\n+\/\/ ==========================\n+enum SEW {\n+  e8,\n+  e16,\n+  e32,\n+  e64,\n+  RESERVED,\n+};\n+\n+enum LMUL {\n+  mf8 = 0b101,\n+  mf4 = 0b110,\n+  mf2 = 0b111,\n+  m1  = 0b000,\n+  m2  = 0b001,\n+  m4  = 0b010,\n+  m8  = 0b011,\n+};\n+\n+enum VMA {\n+  mu, \/\/ undisturbed\n+  ma, \/\/ agnostic\n+};\n+\n+enum VTA {\n+  tu, \/\/ undisturbed\n+  ta, \/\/ agnostic\n+};\n+\n+static Assembler::SEW elembytes_to_sew(int ebytes) {\n+  assert(ebytes > 0 && ebytes <= 8, \"unsupported element size\");\n+  return (Assembler::SEW) exact_log2(ebytes);\n+}\n+\n+static Assembler::SEW elemtype_to_sew(BasicType etype) {\n+  return Assembler::elembytes_to_sew(type2aelembytes(etype));\n+}\n+\n+#define patch_vtype(hsb, lsb, vlmul, vsew, vta, vma, vill)   \\\n+    if (vill == 1) {                                         \\\n+      guarantee((vlmul | vsew | vta | vma == 0),             \\\n+                \"the other bits in vtype shall be zero\");    \\\n+    }                                                        \\\n+    patch((address)&insn, lsb + 2, lsb, vlmul);              \\\n+    patch((address)&insn, lsb + 5, lsb + 3, vsew);           \\\n+    patch((address)&insn, lsb + 6, vta);                     \\\n+    patch((address)&insn, lsb + 7, vma);                     \\\n+    patch((address)&insn, hsb - 1, lsb + 8, 0);              \\\n+    patch((address)&insn, hsb, vill)\n+\n+#define INSN(NAME, op, funct3)                                            \\\n+  void NAME(Register Rd, Register Rs1, SEW sew, LMUL lmul = m1,           \\\n+            VMA vma = mu, VTA vta = tu, bool vill = false) {              \\\n+    unsigned insn = 0;                                                    \\\n+    patch((address)&insn, 6, 0, op);                                      \\\n+    patch((address)&insn, 14, 12, funct3);                                \\\n+    patch_vtype(30, 20, lmul, sew, vta, vma, vill);                       \\\n+    patch((address)&insn, 31, 0);                                         \\\n+    patch_reg((address)&insn, 7, Rd);                                     \\\n+    patch_reg((address)&insn, 15, Rs1);                                   \\\n+    emit(insn);                                                           \\\n+  }\n+\n+  INSN(vsetvli, 0b1010111, 0b111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3)                                            \\\n+  void NAME(Register Rd, uint32_t imm, SEW sew, LMUL lmul = m1,           \\\n+            VMA vma = mu, VTA vta = tu, bool vill = false) {              \\\n+    unsigned insn = 0;                                                    \\\n+    guarantee(is_unsigned_imm_in_range(imm, 5, 0), \"imm is invalid\");     \\\n+    patch((address)&insn, 6, 0, op);                                      \\\n+    patch((address)&insn, 14, 12, funct3);                                \\\n+    patch((address)&insn, 19, 15, imm);                                   \\\n+    patch_vtype(29, 20, lmul, sew, vta, vma, vill);                       \\\n+    patch((address)&insn, 31, 30, 0b11);                                  \\\n+    patch_reg((address)&insn, 7, Rd);                                     \\\n+    emit(insn);                                                           \\\n+  }\n+\n+  INSN(vsetivli, 0b1010111, 0b111);\n+\n+#undef INSN\n+\n+#undef patch_vtype\n+\n+#define INSN(NAME, op, funct3, funct7)                          \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2) {          \\\n+    unsigned insn = 0;                                          \\\n+    patch((address)&insn, 6,  0, op);                           \\\n+    patch((address)&insn, 14, 12, funct3);                      \\\n+    patch((address)&insn, 31, 25, funct7);                      \\\n+    patch_reg((address)&insn, 7, Rd);                           \\\n+    patch_reg((address)&insn, 15, Rs1);                         \\\n+    patch_reg((address)&insn, 20, Rs2);                         \\\n+    emit(insn);                                                 \\\n+  }\n+\n+  \/\/ Vector Configuration Instruction\n+  INSN(vsetvl, 0b1010111, 0b111, 0b1000000);\n+\n+#undef INSN\n+\n+enum VectorMask {\n+  v0_t = 0b0,\n+  unmasked = 0b1\n+};\n+\n+#define patch_VArith(op, Reg, funct3, Reg_or_Imm5, Vs2, vm, funct6)            \\\n+    unsigned insn = 0;                                                         \\\n+    patch((address)&insn, 6, 0, op);                                           \\\n+    patch((address)&insn, 14, 12, funct3);                                     \\\n+    patch((address)&insn, 19, 15, Reg_or_Imm5);                                \\\n+    patch((address)&insn, 25, vm);                                             \\\n+    patch((address)&insn, 31, 26, funct6);                                     \\\n+    patch_reg((address)&insn, 7, Reg);                                         \\\n+    patch_reg((address)&insn, 20, Vs2);                                        \\\n+    emit(insn)\n+\n+\/\/ r2_vm\n+#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n+  void NAME(Register Rd, VectorRegister Vs2, VectorMask vm = unmasked) {       \\\n+    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Mask\n+  INSN(vpopc_m,  0b1010111, 0b010, 0b10000, 0b010000);\n+  INSN(vfirst_m, 0b1010111, 0b010, 0b10001, 0b010000);\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1, Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Integer Extension\n+  INSN(vzext_vf2, 0b1010111, 0b010, 0b00110, 0b010010);\n+  INSN(vzext_vf4, 0b1010111, 0b010, 0b00100, 0b010010);\n+  INSN(vzext_vf8, 0b1010111, 0b010, 0b00010, 0b010010);\n+  INSN(vsext_vf2, 0b1010111, 0b010, 0b00111, 0b010010);\n+  INSN(vsext_vf4, 0b1010111, 0b010, 0b00101, 0b010010);\n+  INSN(vsext_vf8, 0b1010111, 0b010, 0b00011, 0b010010);\n+\n+  \/\/ Vector Mask\n+  INSN(vmsbf_m,   0b1010111, 0b010, 0b00001, 0b010100);\n+  INSN(vmsif_m,   0b1010111, 0b010, 0b00011, 0b010100);\n+  INSN(vmsof_m,   0b1010111, 0b010, 0b00010, 0b010100);\n+  INSN(viota_m,   0b1010111, 0b010, 0b10000, 0b010100);\n+\n+  \/\/ Vector Single-Width Floating-Point\/Integer Type-Convert Instructions\n+  INSN(vfcvt_xu_f_v, 0b1010111, 0b001, 0b00000, 0b010010);\n+  INSN(vfcvt_x_f_v,  0b1010111, 0b001, 0b00001, 0b010010);\n+  INSN(vfcvt_f_xu_v, 0b1010111, 0b001, 0b00010, 0b010010);\n+  INSN(vfcvt_f_x_v,  0b1010111, 0b001, 0b00011, 0b010010);\n+  INSN(vfcvt_rtz_xu_f_v, 0b1010111, 0b001, 0b00110, 0b010010);\n+  INSN(vfcvt_rtz_x_f_v,  0b1010111, 0b001, 0b00111, 0b010010);\n+\n+  \/\/ Vector Floating-Point Instruction\n+  INSN(vfsqrt_v,  0b1010111, 0b001, 0b00000, 0b010011);\n+  INSN(vfclass_v, 0b1010111, 0b001, 0b10000, 0b010011);\n+\n+#undef INSN\n+\n+\/\/ r2rd\n+#define INSN(NAME, op, funct3, simm5, vm, funct6)         \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2) {      \\\n+    patch_VArith(op, Vd, funct3, simm5, Vs2, vm, funct6); \\\n+  }\n+\n+  \/\/ Vector Whole Vector Register Move\n+  INSN(vmv1r_v, 0b1010111, 0b011, 0b00000, 0b1, 0b100111);\n+  INSN(vmv2r_v, 0b1010111, 0b011, 0b00001, 0b1, 0b100111);\n+  INSN(vmv4r_v, 0b1010111, 0b011, 0b00011, 0b1, 0b100111);\n+  INSN(vmv8r_v, 0b1010111, 0b011, 0b00111, 0b1, 0b100111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs1, vm, funct6)           \\\n+  void NAME(FloatRegister Rd, VectorRegister Vs2) {       \\\n+    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);   \\\n+  }\n+\n+  \/\/ Vector Floating-Point Move Instruction\n+  INSN(vfmv_f_s, 0b1010111, 0b001, 0b00000, 0b1, 0b010000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs1, vm, funct6)          \\\n+  void NAME(Register Rd, VectorRegister Vs2) {           \\\n+    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);  \\\n+  }\n+\n+  \/\/ Vector Integer Scalar Move Instructions\n+  INSN(vmv_x_s, 0b1010111, 0b010, 0b00000, 0b1, 0b010000);\n+\n+#undef INSN\n+\n+\/\/ r_vm\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, uint32_t imm, VectorMask vm = unmasked) {       \\\n+    guarantee(is_unsigned_imm_in_range(imm, 5, 0), \"imm is invalid\");                              \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6);                         \\\n+  }\n+\n+  \/\/ Vector Single-Width Bit Shift Instructions\n+  INSN(vsra_vi,    0b1010111, 0b011, 0b101001);\n+  INSN(vsrl_vi,    0b1010111, 0b011, 0b101000);\n+  INSN(vsll_vi,    0b1010111, 0b011, 0b100101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs1, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Single-Width Floating-Point Fused Multiply-Add Instructions\n+  INSN(vfnmsub_vv, 0b1010111, 0b001, 0b101011);\n+  INSN(vfmsub_vv,  0b1010111, 0b001, 0b101010);\n+  INSN(vfnmadd_vv, 0b1010111, 0b001, 0b101001);\n+  INSN(vfmadd_vv,  0b1010111, 0b001, 0b101000);\n+  INSN(vfnmsac_vv, 0b1010111, 0b001, 0b101111);\n+  INSN(vfmsac_vv,  0b1010111, 0b001, 0b101110);\n+  INSN(vfmacc_vv,  0b1010111, 0b001, 0b101100);\n+  INSN(vfnmacc_vv, 0b1010111, 0b001, 0b101101);\n+\n+  \/\/ Vector Single-Width Integer Multiply-Add Instructions\n+  INSN(vnmsub_vv, 0b1010111, 0b010, 0b101011);\n+  INSN(vmadd_vv,  0b1010111, 0b010, 0b101001);\n+  INSN(vnmsac_vv, 0b1010111, 0b010, 0b101111);\n+  INSN(vmacc_vv,  0b1010111, 0b010, 0b101101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, Register Rs1, VectorRegister Vs2, VectorMask vm = unmasked) {       \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Single-Width Integer Multiply-Add Instructions\n+  INSN(vnmsub_vx, 0b1010111, 0b110, 0b101011);\n+  INSN(vmadd_vx,  0b1010111, 0b110, 0b101001);\n+  INSN(vnmsac_vx, 0b1010111, 0b110, 0b101111);\n+  INSN(vmacc_vx,  0b1010111, 0b110, 0b101101);\n+\n+  INSN(vrsub_vx,  0b1010111, 0b100, 0b000011);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, FloatRegister Rs1, VectorRegister Vs2, VectorMask vm = unmasked) {  \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Single-Width Floating-Point Fused Multiply-Add Instructions\n+  INSN(vfnmsub_vf, 0b1010111, 0b101, 0b101011);\n+  INSN(vfmsub_vf,  0b1010111, 0b101, 0b101010);\n+  INSN(vfnmadd_vf, 0b1010111, 0b101, 0b101001);\n+  INSN(vfmadd_vf,  0b1010111, 0b101, 0b101000);\n+  INSN(vfnmsac_vf, 0b1010111, 0b101, 0b101111);\n+  INSN(vfmsac_vf,  0b1010111, 0b101, 0b101110);\n+  INSN(vfmacc_vf,  0b1010111, 0b101, 0b101100);\n+  INSN(vfnmacc_vf, 0b1010111, 0b101, 0b101101);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1, VectorMask vm = unmasked) { \\\n+    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Single-Width Floating-Point Reduction Instructions\n+  INSN(vfredsum_vs,   0b1010111, 0b001, 0b000001);\n+  INSN(vfredosum_vs,  0b1010111, 0b001, 0b000011);\n+  INSN(vfredmin_vs,   0b1010111, 0b001, 0b000101);\n+  INSN(vfredmax_vs,   0b1010111, 0b001, 0b000111);\n+\n+  \/\/ Vector Single-Width Integer Reduction Instructions\n+  INSN(vredsum_vs,    0b1010111, 0b010, 0b000000);\n+  INSN(vredand_vs,    0b1010111, 0b010, 0b000001);\n+  INSN(vredor_vs,     0b1010111, 0b010, 0b000010);\n+  INSN(vredxor_vs,    0b1010111, 0b010, 0b000011);\n+  INSN(vredminu_vs,   0b1010111, 0b010, 0b000100);\n+  INSN(vredmin_vs,    0b1010111, 0b010, 0b000101);\n+  INSN(vredmaxu_vs,   0b1010111, 0b010, 0b000110);\n+  INSN(vredmax_vs,    0b1010111, 0b010, 0b000111);\n+\n+  \/\/ Vector Floating-Point Compare Instructions\n+  INSN(vmfle_vv, 0b1010111, 0b001, 0b011001);\n+  INSN(vmflt_vv, 0b1010111, 0b001, 0b011011);\n+  INSN(vmfne_vv, 0b1010111, 0b001, 0b011100);\n+  INSN(vmfeq_vv, 0b1010111, 0b001, 0b011000);\n+\n+  \/\/ Vector Floating-Point Sign-Injection Instructions\n+  INSN(vfsgnjx_vv, 0b1010111, 0b001, 0b001010);\n+  INSN(vfsgnjn_vv, 0b1010111, 0b001, 0b001001);\n+  INSN(vfsgnj_vv,  0b1010111, 0b001, 0b001000);\n+\n+  \/\/ Vector Floating-Point MIN\/MAX Instructions\n+  INSN(vfmax_vv,   0b1010111, 0b001, 0b000110);\n+  INSN(vfmin_vv,   0b1010111, 0b001, 0b000100);\n+\n+  \/\/ Vector Single-Width Floating-Point Multiply\/Divide Instructions\n+  INSN(vfdiv_vv,   0b1010111, 0b001, 0b100000);\n+  INSN(vfmul_vv,   0b1010111, 0b001, 0b100100);\n+\n+  \/\/ Vector Single-Width Floating-Point Add\/Subtract Instructions\n+  INSN(vfsub_vv, 0b1010111, 0b001, 0b000010);\n+  INSN(vfadd_vv, 0b1010111, 0b001, 0b000000);\n+\n+  \/\/ Vector Single-Width Fractional Multiply with Rounding and Saturation\n+  INSN(vsmul_vv, 0b1010111, 0b000, 0b100111);\n+\n+  \/\/ Vector Integer Divide Instructions\n+  INSN(vrem_vv,  0b1010111, 0b010, 0b100011);\n+  INSN(vremu_vv, 0b1010111, 0b010, 0b100010);\n+  INSN(vdiv_vv,  0b1010111, 0b010, 0b100001);\n+  INSN(vdivu_vv, 0b1010111, 0b010, 0b100000);\n+\n+  \/\/ Vector Single-Width Integer Multiply Instructions\n+  INSN(vmulhsu_vv, 0b1010111, 0b010, 0b100110);\n+  INSN(vmulhu_vv,  0b1010111, 0b010, 0b100100);\n+  INSN(vmulh_vv,   0b1010111, 0b010, 0b100111);\n+  INSN(vmul_vv,    0b1010111, 0b010, 0b100101);\n+\n+  \/\/ Vector Integer Min\/Max Instructions\n+  INSN(vmax_vv,  0b1010111, 0b000, 0b000111);\n+  INSN(vmaxu_vv, 0b1010111, 0b000, 0b000110);\n+  INSN(vmin_vv,  0b1010111, 0b000, 0b000101);\n+  INSN(vminu_vv, 0b1010111, 0b000, 0b000100);\n+\n+  \/\/ Vector Integer Comparison Instructions\n+  INSN(vmsle_vv,  0b1010111, 0b000, 0b011101);\n+  INSN(vmsleu_vv, 0b1010111, 0b000, 0b011100);\n+  INSN(vmslt_vv,  0b1010111, 0b000, 0b011011);\n+  INSN(vmsltu_vv, 0b1010111, 0b000, 0b011010);\n+  INSN(vmsne_vv,  0b1010111, 0b000, 0b011001);\n+  INSN(vmseq_vv,  0b1010111, 0b000, 0b011000);\n+\n+  \/\/ Vector Single-Width Bit Shift Instructions\n+  INSN(vsra_vv, 0b1010111, 0b000, 0b101001);\n+  INSN(vsrl_vv, 0b1010111, 0b000, 0b101000);\n+  INSN(vsll_vv, 0b1010111, 0b000, 0b100101);\n+\n+  \/\/ Vector Bitwise Logical Instructions\n+  INSN(vxor_vv, 0b1010111, 0b000, 0b001011);\n+  INSN(vor_vv,  0b1010111, 0b000, 0b001010);\n+  INSN(vand_vv, 0b1010111, 0b000, 0b001001);\n+\n+  \/\/ Vector Single-Width Integer Add and Subtract\n+  INSN(vsub_vv, 0b1010111, 0b000, 0b000010);\n+  INSN(vadd_vv, 0b1010111, 0b000, 0b000000);\n+\n+#undef INSN\n+\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, Register Rs1, VectorMask vm = unmasked) {       \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Integer Divide Instructions\n+  INSN(vrem_vx,  0b1010111, 0b110, 0b100011);\n+  INSN(vremu_vx, 0b1010111, 0b110, 0b100010);\n+  INSN(vdiv_vx,  0b1010111, 0b110, 0b100001);\n+  INSN(vdivu_vx, 0b1010111, 0b110, 0b100000);\n+\n+  \/\/ Vector Single-Width Integer Multiply Instructions\n+  INSN(vmulhsu_vx, 0b1010111, 0b110, 0b100110);\n+  INSN(vmulhu_vx,  0b1010111, 0b110, 0b100100);\n+  INSN(vmulh_vx,   0b1010111, 0b110, 0b100111);\n+  INSN(vmul_vx,    0b1010111, 0b110, 0b100101);\n+\n+  \/\/ Vector Integer Min\/Max Instructions\n+  INSN(vmax_vx,  0b1010111, 0b100, 0b000111);\n+  INSN(vmaxu_vx, 0b1010111, 0b100, 0b000110);\n+  INSN(vmin_vx,  0b1010111, 0b100, 0b000101);\n+  INSN(vminu_vx, 0b1010111, 0b100, 0b000100);\n+\n+  \/\/ Vector Integer Comparison Instructions\n+  INSN(vmsgt_vx,  0b1010111, 0b100, 0b011111);\n+  INSN(vmsgtu_vx, 0b1010111, 0b100, 0b011110);\n+  INSN(vmsle_vx,  0b1010111, 0b100, 0b011101);\n+  INSN(vmsleu_vx, 0b1010111, 0b100, 0b011100);\n+  INSN(vmslt_vx,  0b1010111, 0b100, 0b011011);\n+  INSN(vmsltu_vx, 0b1010111, 0b100, 0b011010);\n+  INSN(vmsne_vx,  0b1010111, 0b100, 0b011001);\n+  INSN(vmseq_vx,  0b1010111, 0b100, 0b011000);\n+\n+  \/\/ Vector Narrowing Integer Right Shift Instructions\n+  INSN(vnsra_wx, 0b1010111, 0b100, 0b101101);\n+  INSN(vnsrl_wx, 0b1010111, 0b100, 0b101100);\n+\n+  \/\/ Vector Single-Width Bit Shift Instructions\n+  INSN(vsra_vx, 0b1010111, 0b100, 0b101001);\n+  INSN(vsrl_vx, 0b1010111, 0b100, 0b101000);\n+  INSN(vsll_vx, 0b1010111, 0b100, 0b100101);\n+\n+  \/\/ Vector Bitwise Logical Instructions\n+  INSN(vxor_vx, 0b1010111, 0b100, 0b001011);\n+  INSN(vor_vx,  0b1010111, 0b100, 0b001010);\n+  INSN(vand_vx, 0b1010111, 0b100, 0b001001);\n+\n+  \/\/ Vector Single-Width Integer Add and Subtract\n+  INSN(vsub_vx, 0b1010111, 0b100, 0b000010);\n+  INSN(vadd_vx, 0b1010111, 0b100, 0b000000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, FloatRegister Rs1, VectorMask vm = unmasked) {  \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n+  }\n+\n+  \/\/ Vector Floating-Point Compare Instructions\n+  INSN(vmfge_vf, 0b1010111, 0b101, 0b011111);\n+  INSN(vmfgt_vf, 0b1010111, 0b101, 0b011101);\n+  INSN(vmfle_vf, 0b1010111, 0b101, 0b011001);\n+  INSN(vmflt_vf, 0b1010111, 0b101, 0b011011);\n+  INSN(vmfne_vf, 0b1010111, 0b101, 0b011100);\n+  INSN(vmfeq_vf, 0b1010111, 0b101, 0b011000);\n+\n+  \/\/ Vector Floating-Point Sign-Injection Instructions\n+  INSN(vfsgnjx_vf, 0b1010111, 0b101, 0b001010);\n+  INSN(vfsgnjn_vf, 0b1010111, 0b101, 0b001001);\n+  INSN(vfsgnj_vf,  0b1010111, 0b101, 0b001000);\n+\n+  \/\/ Vector Floating-Point MIN\/MAX Instructions\n+  INSN(vfmax_vf, 0b1010111, 0b101, 0b000110);\n+  INSN(vfmin_vf, 0b1010111, 0b101, 0b000100);\n+\n+  \/\/ Vector Single-Width Floating-Point Multiply\/Divide Instructions\n+  INSN(vfdiv_vf,  0b1010111, 0b101, 0b100000);\n+  INSN(vfmul_vf,  0b1010111, 0b101, 0b100100);\n+  INSN(vfrdiv_vf, 0b1010111, 0b101, 0b100001);\n+\n+  \/\/ Vector Single-Width Floating-Point Add\/Subtract Instructions\n+  INSN(vfsub_vf,  0b1010111, 0b101, 0b000010);\n+  INSN(vfadd_vf,  0b1010111, 0b101, 0b000000);\n+  INSN(vfrsub_vf, 0b1010111, 0b101, 0b100111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, int32_t imm, VectorMask vm = unmasked) {        \\\n+    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");                                       \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)imm & 0x1f, Vs2, vm, funct6);                           \\\n+  }\n+\n+  INSN(vmsgt_vi,  0b1010111, 0b011, 0b011111);\n+  INSN(vmsgtu_vi, 0b1010111, 0b011, 0b011110);\n+  INSN(vmsle_vi,  0b1010111, 0b011, 0b011101);\n+  INSN(vmsleu_vi, 0b1010111, 0b011, 0b011100);\n+  INSN(vmsne_vi,  0b1010111, 0b011, 0b011001);\n+  INSN(vmseq_vi,  0b1010111, 0b011, 0b011000);\n+  INSN(vxor_vi,   0b1010111, 0b011, 0b001011);\n+  INSN(vor_vi,    0b1010111, 0b011, 0b001010);\n+  INSN(vand_vi,   0b1010111, 0b011, 0b001001);\n+  INSN(vadd_vi,   0b1010111, 0b011, 0b000000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                                                             \\\n+  void NAME(VectorRegister Vd, int32_t imm, VectorRegister Vs2, VectorMask vm = unmasked) {        \\\n+    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");                                       \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6);                         \\\n+  }\n+\n+  INSN(vrsub_vi, 0b1010111, 0b011, 0b000011);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, vm, funct6)                                   \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1) {     \\\n+    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);  \\\n+  }\n+\n+  \/\/ Vector Compress Instruction\n+  INSN(vcompress_vm, 0b1010111, 0b010, 0b1, 0b010111);\n+\n+  \/\/ Vector Mask-Register Logical Instructions\n+  INSN(vmxnor_mm,   0b1010111, 0b010, 0b1, 0b011111);\n+  INSN(vmornot_mm,  0b1010111, 0b010, 0b1, 0b011100);\n+  INSN(vmnor_mm,    0b1010111, 0b010, 0b1, 0b011110);\n+  INSN(vmor_mm,     0b1010111, 0b010, 0b1, 0b011010);\n+  INSN(vmxor_mm,    0b1010111, 0b010, 0b1, 0b011011);\n+  INSN(vmandnot_mm, 0b1010111, 0b010, 0b1, 0b011000);\n+  INSN(vmnand_mm,   0b1010111, 0b010, 0b1, 0b011101);\n+  INSN(vmand_mm,    0b1010111, 0b010, 0b1, 0b011001);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs2, vm, funct6)                            \\\n+  void NAME(VectorRegister Vd, int32_t imm) {                              \\\n+    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");               \\\n+    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6); \\\n+  }\n+\n+  \/\/ Vector Integer Move Instructions\n+  INSN(vmv_v_i, 0b1010111, 0b011, v0, 0b1, 0b010111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n+  void NAME(VectorRegister Vd, FloatRegister Rs1) {                         \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6); \\\n+  }\n+\n+  \/\/ Floating-Point Scalar Move Instructions\n+  INSN(vfmv_s_f, 0b1010111, 0b101, v0, 0b1, 0b010000);\n+  \/\/ Vector Floating-Point Move Instruction\n+  INSN(vfmv_v_f, 0b1010111, 0b101, v0, 0b1, 0b010111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n+  void NAME(VectorRegister Vd, VectorRegister Vs1) {                        \\\n+    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6); \\\n+  }\n+\n+  \/\/ Vector Integer Move Instructions\n+  INSN(vmv_v_v, 0b1010111, 0b000, v0, 0b1, 0b010111);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n+   void NAME(VectorRegister Vd, Register Rs1) {                             \\\n+    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6); \\\n+   }\n+\n+  \/\/ Integer Scalar Move Instructions\n+  INSN(vmv_s_x, 0b1010111, 0b110, v0, 0b1, 0b010000);\n+\n+  \/\/ Vector Integer Move Instructions\n+  INSN(vmv_v_x, 0b1010111, 0b100, v0, 0b1, 0b010111);\n+\n+#undef INSN\n+#undef patch_VArith\n+\n+#define INSN(NAME, op, funct13, funct6)                    \\\n+  void NAME(VectorRegister Vd, VectorMask vm = unmasked) { \\\n+    unsigned insn = 0;                                     \\\n+    patch((address)&insn, 6, 0, op);                       \\\n+    patch((address)&insn, 24, 12, funct13);                \\\n+    patch((address)&insn, 25, vm);                         \\\n+    patch((address)&insn, 31, 26, funct6);                 \\\n+    patch_reg((address)&insn, 7, Vd);                      \\\n+    emit(insn);                                            \\\n+  }\n+\n+  \/\/ Vector Element Index Instruction\n+  INSN(vid_v, 0b1010111, 0b0000010001010, 0b010100);\n+\n+#undef INSN\n+\n+enum Nf {\n+  g1 = 0b000,\n+  g2 = 0b001,\n+  g3 = 0b010,\n+  g4 = 0b011,\n+  g5 = 0b100,\n+  g6 = 0b101,\n+  g7 = 0b110,\n+  g8 = 0b111\n+};\n+\n+#define patch_VLdSt(op, VReg, width, Rs1, Reg_or_umop, vm, mop, mew, nf) \\\n+    unsigned insn = 0;                                                   \\\n+    patch((address)&insn, 6, 0, op);                                     \\\n+    patch((address)&insn, 14, 12, width);                                \\\n+    patch((address)&insn, 24, 20, Reg_or_umop);                          \\\n+    patch((address)&insn, 25, vm);                                       \\\n+    patch((address)&insn, 27, 26, mop);                                  \\\n+    patch((address)&insn, 28, mew);                                      \\\n+    patch((address)&insn, 31, 29, nf);                                   \\\n+    patch_reg((address)&insn, 7, VReg);                                  \\\n+    patch_reg((address)&insn, 15, Rs1);                                  \\\n+    emit(insn)\n+\n+#define INSN(NAME, op, lumop, vm, mop, nf)                                           \\\n+  void NAME(VectorRegister Vd, Register Rs1, uint32_t width = 0, bool mew = false) { \\\n+    guarantee(is_unsigned_imm_in_range(width, 3, 0), \"width is invalid\");            \\\n+    patch_VLdSt(op, Vd, width, Rs1, lumop, vm, mop, mew, nf);                        \\\n+  }\n+\n+  \/\/ Vector Load\/Store Instructions\n+  INSN(vl1r_v, 0b0000111, 0b01000, 0b1, 0b00, g1);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, width, sumop, vm, mop, mew, nf)           \\\n+  void NAME(VectorRegister Vs3, Register Rs1) {                  \\\n+    patch_VLdSt(op, Vs3, width, Rs1, sumop, vm, mop, mew, nf);   \\\n+  }\n+\n+  \/\/ Vector Load\/Store Instructions\n+  INSN(vs1r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g1);\n+\n+#undef INSN\n+\n+\/\/ r2_nfvm\n+#define INSN(NAME, op, width, umop, mop, mew)                         \\\n+  void NAME(VectorRegister Vd_or_Vs3, Register Rs1, Nf nf = g1) {     \\\n+    patch_VLdSt(op, Vd_or_Vs3, width, Rs1, umop, 1, mop, mew, nf);    \\\n+  }\n+\n+  \/\/ Vector Unit-Stride Instructions\n+  INSN(vle1_v, 0b0000111, 0b000, 0b01011, 0b00, 0b0);\n+  INSN(vse1_v, 0b0100111, 0b000, 0b01011, 0b00, 0b0);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, width, umop, mop, mew)                                               \\\n+  void NAME(VectorRegister Vd_or_Vs3, Register Rs1, VectorMask vm = unmasked, Nf nf = g1) { \\\n+    patch_VLdSt(op, Vd_or_Vs3, width, Rs1, umop, vm, mop, mew, nf);                         \\\n+  }\n+\n+  \/\/ Vector Unit-Stride Instructions\n+  INSN(vle8_v,    0b0000111, 0b000, 0b00000, 0b00, 0b0);\n+  INSN(vle16_v,   0b0000111, 0b101, 0b00000, 0b00, 0b0);\n+  INSN(vle32_v,   0b0000111, 0b110, 0b00000, 0b00, 0b0);\n+  INSN(vle64_v,   0b0000111, 0b111, 0b00000, 0b00, 0b0);\n+\n+  \/\/ Vector unit-stride fault-only-first Instructions\n+  INSN(vle8ff_v,  0b0000111, 0b000, 0b10000, 0b00, 0b0);\n+  INSN(vle16ff_v, 0b0000111, 0b101, 0b10000, 0b00, 0b0);\n+  INSN(vle32ff_v, 0b0000111, 0b110, 0b10000, 0b00, 0b0);\n+  INSN(vle64ff_v, 0b0000111, 0b111, 0b10000, 0b00, 0b0);\n+\n+  INSN(vse8_v,  0b0100111, 0b000, 0b00000, 0b00, 0b0);\n+  INSN(vse16_v, 0b0100111, 0b101, 0b00000, 0b00, 0b0);\n+  INSN(vse32_v, 0b0100111, 0b110, 0b00000, 0b00, 0b0);\n+  INSN(vse64_v, 0b0100111, 0b111, 0b00000, 0b00, 0b0);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, width, mop, mew)                                                                  \\\n+  void NAME(VectorRegister Vd, Register Rs1, VectorRegister Vs2, VectorMask vm = unmasked, Nf nf = g1) { \\\n+    patch_VLdSt(op, Vd, width, Rs1, Vs2->encoding_nocheck(), vm, mop, mew, nf);                          \\\n+  }\n+\n+  \/\/ Vector unordered indexed load instructions\n+  INSN(vluxei8_v,  0b0000111, 0b000, 0b01, 0b0);\n+  INSN(vluxei16_v, 0b0000111, 0b101, 0b01, 0b0);\n+  INSN(vluxei32_v, 0b0000111, 0b110, 0b01, 0b0);\n+  INSN(vluxei64_v, 0b0000111, 0b111, 0b01, 0b0);\n+\n+  \/\/ Vector ordered indexed load instructions\n+  INSN(vloxei8_v,  0b0000111, 0b000, 0b11, 0b0);\n+  INSN(vloxei16_v, 0b0000111, 0b101, 0b11, 0b0);\n+  INSN(vloxei32_v, 0b0000111, 0b110, 0b11, 0b0);\n+  INSN(vloxei64_v, 0b0000111, 0b111, 0b11, 0b0);\n+#undef INSN\n+\n+#define INSN(NAME, op, width, mop, mew)                                                                  \\\n+  void NAME(VectorRegister Vd, Register Rs1, Register Rs2, VectorMask vm = unmasked, Nf nf = g1) {       \\\n+    patch_VLdSt(op, Vd, width, Rs1, Rs2->encoding_nocheck(), vm, mop, mew, nf);                          \\\n+  }\n+\n+  \/\/ Vector Strided Instructions\n+  INSN(vlse8_v,  0b0000111, 0b000, 0b10, 0b0);\n+  INSN(vlse16_v, 0b0000111, 0b101, 0b10, 0b0);\n+  INSN(vlse32_v, 0b0000111, 0b110, 0b10, 0b0);\n+  INSN(vlse64_v, 0b0000111, 0b111, 0b10, 0b0);\n+\n+#undef INSN\n+#undef patch_VLdSt\n+\n+\/\/ ====================================\n+\/\/ RISC-V Bit-Manipulation Extension\n+\/\/ ====================================\n+#define INSN(NAME, op, funct3, funct7)                  \\\n+  void NAME(Register Rd, Register Rs1, Register Rs2) {  \\\n+    unsigned insn = 0;                                  \\\n+    patch((address)&insn, 6,  0, op);                   \\\n+    patch((address)&insn, 14, 12, funct3);              \\\n+    patch((address)&insn, 31, 25, funct7);              \\\n+    patch_reg((address)&insn, 7, Rd);                   \\\n+    patch_reg((address)&insn, 15, Rs1);                 \\\n+    patch_reg((address)&insn, 20, Rs2);                 \\\n+    emit(insn);                                         \\\n+  }\n+\n+  INSN(add_uw, 0b0111011, 0b000, 0b0000100);\n+  INSN(rol,    0b0110011, 0b001, 0b0110000);\n+  INSN(rolw,   0b0111011, 0b001, 0b0110000);\n+  INSN(ror,    0b0110011, 0b101, 0b0110000);\n+  INSN(rorw,   0b0111011, 0b101, 0b0110000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct12)                 \\\n+  void NAME(Register Rd, Register Rs1) {                \\\n+    unsigned insn = 0;                                  \\\n+    patch((address)&insn, 6, 0, op);                    \\\n+    patch((address)&insn, 14, 12, funct3);              \\\n+    patch((address)&insn, 31, 20, funct12);             \\\n+    patch_reg((address)&insn, 7, Rd);                   \\\n+    patch_reg((address)&insn, 15, Rs1);                 \\\n+    emit(insn);                                         \\\n+  }\n+\n+  INSN(rev8,   0b0010011, 0b101, 0b011010111000);\n+  INSN(sext_b, 0b0010011, 0b001, 0b011000000100);\n+  INSN(sext_h, 0b0010011, 0b001, 0b011000000101);\n+  INSN(zext_h, 0b0111011, 0b100, 0b000010000000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct6)                  \\\n+  void NAME(Register Rd, Register Rs1, unsigned shamt) {\\\n+    guarantee(shamt <= 0x3f, \"Shamt is invalid\");       \\\n+    unsigned insn = 0;                                  \\\n+    patch((address)&insn, 6, 0, op);                    \\\n+    patch((address)&insn, 14, 12, funct3);              \\\n+    patch((address)&insn, 25, 20, shamt);               \\\n+    patch((address)&insn, 31, 26, funct6);              \\\n+    patch_reg((address)&insn, 7, Rd);                   \\\n+    patch_reg((address)&insn, 15, Rs1);                 \\\n+    emit(insn);                                         \\\n+  }\n+\n+  INSN(rori, 0b0010011, 0b101, 0b011000);\n+\n+#undef INSN\n+\n+#define INSN(NAME, op, funct3, funct7)                  \\\n+  void NAME(Register Rd, Register Rs1, unsigned shamt){ \\\n+    guarantee(shamt <= 0x1f, \"Shamt is invalid\");       \\\n+    unsigned insn = 0;                                  \\\n+    patch((address)&insn, 6, 0, op);                    \\\n+    patch((address)&insn, 14, 12, funct3);              \\\n+    patch((address)&insn, 24, 20, shamt);               \\\n+    patch((address)&insn, 31, 25, funct7);              \\\n+    patch_reg((address)&insn, 7, Rd);                   \\\n+    patch_reg((address)&insn, 15, Rs1);                 \\\n+    emit(insn);                                         \\\n+  }\n+\n+  INSN(roriw, 0b0011011, 0b101, 0b0110000);\n+\n@@ -1241,0 +1987,4 @@\n+  \/\/ RVB pseudo instructions\n+  \/\/ zero extend word\n+  void zext_w(Register Rd, Register Rs);\n+\n@@ -1259,3 +2009,0 @@\n-  #include \"assembler_riscv_v.hpp\"\n-  #include \"assembler_riscv_b.hpp\"\n-\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv.hpp","additions":751,"deletions":4,"binary":false,"changes":755,"status":"modified"},{"patch":"@@ -1,107 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_RISCV_ASSEMBLER_RISCV_B_HPP\n-#define CPU_RISCV_ASSEMBLER_RISCV_B_HPP\n-\n-#define INSN(NAME, op, funct3, funct7)                  \\\n-  void NAME(Register Rd, Register Rs1, Register Rs2) {  \\\n-    unsigned insn = 0;                                  \\\n-    patch((address)&insn, 6,  0, op);                   \\\n-    patch((address)&insn, 14, 12, funct3);              \\\n-    patch((address)&insn, 31, 25, funct7);              \\\n-    patch_reg((address)&insn, 7, Rd);                   \\\n-    patch_reg((address)&insn, 15, Rs1);                 \\\n-    patch_reg((address)&insn, 20, Rs2);                 \\\n-    emit(insn);                                         \\\n-  }\n-\n-  INSN(add_uw, 0b0111011, 0b000, 0b0000100);\n-  INSN(rol,    0b0110011, 0b001, 0b0110000);\n-  INSN(rolw,   0b0111011, 0b001, 0b0110000);\n-  INSN(ror,    0b0110011, 0b101, 0b0110000);\n-  INSN(rorw,   0b0111011, 0b101, 0b0110000);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct12)                 \\\n-  void NAME(Register Rd, Register Rs1) {                \\\n-    unsigned insn = 0;                                  \\\n-    patch((address)&insn, 6, 0, op);                    \\\n-    patch((address)&insn, 14, 12, funct3);              \\\n-    patch((address)&insn, 31, 20, funct12);             \\\n-    patch_reg((address)&insn, 7, Rd);                   \\\n-    patch_reg((address)&insn, 15, Rs1);                 \\\n-    emit(insn);                                         \\\n-  }\n-\n-  INSN(sext_b, 0b0010011, 0b001, 0b011000000100);\n-  INSN(sext_h, 0b0010011, 0b001, 0b011000000101);\n-  INSN(zext_h, 0b0111011, 0b100, 0b000010000000);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct6)                  \\\n-  void NAME(Register Rd, Register Rs1, unsigned shamt) {\\\n-    guarantee(shamt <= 0x3f, \"Shamt is invalid\");       \\\n-    unsigned insn = 0;                                  \\\n-    patch((address)&insn, 6, 0, op);                    \\\n-    patch((address)&insn, 14, 12, funct3);              \\\n-    patch((address)&insn, 25, 20, shamt);               \\\n-    patch((address)&insn, 31, 26, funct6);              \\\n-    patch_reg((address)&insn, 7, Rd);                   \\\n-    patch_reg((address)&insn, 15, Rs1);                 \\\n-    emit(insn);                                         \\\n-  }\n-\n-  INSN(rori, 0b0010011, 0b101, 0b011000);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct7)                  \\\n-  void NAME(Register Rd, Register Rs1, unsigned shamt){ \\\n-    guarantee(shamt <= 0x1f, \"Shamt is invalid\");       \\\n-    unsigned insn = 0;                                  \\\n-    patch((address)&insn, 6, 0, op);                    \\\n-    patch((address)&insn, 14, 12, funct3);              \\\n-    patch((address)&insn, 24, 20, shamt);               \\\n-    patch((address)&insn, 31, 25, funct7);              \\\n-    patch_reg((address)&insn, 7, Rd);                   \\\n-    patch_reg((address)&insn, 15, Rs1);                 \\\n-    emit(insn);                                         \\\n-  }\n-\n-  INSN(roriw, 0b0011011, 0b101, 0b0110000);\n-\n-#undef INSN\n-\n-\/\/ RVB pseudo instructions\n-\/\/ zero extend word\n-void zext_w(Register Rd, Register Rs) {\n-  add_uw(Rd, Rs, zr);\n-}\n-\n-\n-#endif \/\/ CPU_RISCV_ASSEMBLER_RISCV_B_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv_b.hpp","additions":0,"deletions":107,"binary":false,"changes":107,"status":"deleted"},{"patch":"@@ -1,697 +0,0 @@\n-\/*\n- * Copyright (c) 1997, 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2021, Huawei Technologies Co., Ltd. All rights reserved.\n- * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n- *\n- * This code is free software; you can redistribute it and\/or modify it\n- * under the terms of the GNU General Public License version 2 only, as\n- * published by the Free Software Foundation.\n- *\n- * This code is distributed in the hope that it will be useful, but WITHOUT\n- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n- * version 2 for more details (a copy is included in the LICENSE file that\n- * accompanied this code).\n- *\n- * You should have received a copy of the GNU General Public License version\n- * 2 along with this work; if not, write to the Free Software Foundation,\n- * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n- *\n- * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n- * or visit www.oracle.com if you need additional information or have any\n- * questions.\n- *\n- *\/\n-\n-#ifndef CPU_RISCV_ASSEMBLER_RISCV_V_HPP\n-#define CPU_RISCV_ASSEMBLER_RISCV_V_HPP\n-\n-enum SEW {\n-  e8,\n-  e16,\n-  e32,\n-  e64,\n-  RESERVED,\n-};\n-\n-enum LMUL {\n-  mf8 = 0b101,\n-  mf4 = 0b110,\n-  mf2 = 0b111,\n-  m1  = 0b000,\n-  m2  = 0b001,\n-  m4  = 0b010,\n-  m8  = 0b011,\n-};\n-\n-enum VMA {\n-  mu, \/\/ undisturbed\n-  ma, \/\/ agnostic\n-};\n-\n-enum VTA {\n-  tu, \/\/ undisturbed\n-  ta, \/\/ agnostic\n-};\n-\n-static Assembler::SEW elembytes_to_sew(int ebytes) {\n-  assert(ebytes > 0 && ebytes <= 8, \"unsupported element size\");\n-  return (Assembler::SEW) exact_log2(ebytes);\n-}\n-\n-static Assembler::SEW elemtype_to_sew(BasicType etype) {\n-  return Assembler::elembytes_to_sew(type2aelembytes(etype));\n-}\n-\n-#define patch_vtype(hsb, lsb, vlmul, vsew, vta, vma, vill)   \\\n-    if (vill == 1) {                                         \\\n-      guarantee((vlmul | vsew | vta | vma == 0),             \\\n-                \"the other bits in vtype shall be zero\");    \\\n-    }                                                        \\\n-    patch((address)&insn, lsb + 2, lsb, vlmul);              \\\n-    patch((address)&insn, lsb + 5, lsb + 3, vsew);           \\\n-    patch((address)&insn, lsb + 6, vta);                     \\\n-    patch((address)&insn, lsb + 7, vma);                     \\\n-    patch((address)&insn, hsb - 1, lsb + 8, 0);              \\\n-    patch((address)&insn, hsb, vill)\n-\n-#define INSN(NAME, op, funct3)                                            \\\n-  void NAME(Register Rd, Register Rs1, SEW sew, LMUL lmul = m1,           \\\n-            VMA vma = mu, VTA vta = tu, bool vill = false) {              \\\n-    unsigned insn = 0;                                                    \\\n-    patch((address)&insn, 6, 0, op);                                      \\\n-    patch((address)&insn, 14, 12, funct3);                                \\\n-    patch_vtype(30, 20, lmul, sew, vta, vma, vill);                       \\\n-    patch((address)&insn, 31, 0);                                         \\\n-    patch_reg((address)&insn, 7, Rd);                                     \\\n-    patch_reg((address)&insn, 15, Rs1);                                   \\\n-    emit(insn);                                                           \\\n-  }\n-\n-  INSN(vsetvli, 0b1010111, 0b111);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3)                                            \\\n-  void NAME(Register Rd, uint32_t imm, SEW sew, LMUL lmul = m1,           \\\n-            VMA vma = mu, VTA vta = tu, bool vill = false) {              \\\n-    unsigned insn = 0;                                                    \\\n-    guarantee(is_unsigned_imm_in_range(imm, 5, 0), \"imm is invalid\");     \\\n-    patch((address)&insn, 6, 0, op);                                      \\\n-    patch((address)&insn, 14, 12, funct3);                                \\\n-    patch((address)&insn, 19, 15, imm);                                   \\\n-    patch_vtype(29, 20, lmul, sew, vta, vma, vill);                       \\\n-    patch((address)&insn, 31, 30, 0b11);                                  \\\n-    patch_reg((address)&insn, 7, Rd);                                     \\\n-    emit(insn);                                                           \\\n-  }\n-\n-  INSN(vsetivli, 0b1010111, 0b111);\n-\n-#undef INSN\n-\n-#undef patch_vtype\n-\n-#define INSN(NAME, op, funct3, funct7)                          \\\n-  void NAME(Register Rd, Register Rs1, Register Rs2) {          \\\n-    unsigned insn = 0;                                          \\\n-    patch((address)&insn, 6,  0, op);                           \\\n-    patch((address)&insn, 14, 12, funct3);                      \\\n-    patch((address)&insn, 31, 25, funct7);                      \\\n-    patch_reg((address)&insn, 7, Rd);                           \\\n-    patch_reg((address)&insn, 15, Rs1);                         \\\n-    patch_reg((address)&insn, 20, Rs2);                         \\\n-    emit(insn);                                                 \\\n-  }\n-\n-  \/\/ Vector Configuration Instruction\n-  INSN(vsetvl, 0b1010111, 0b111, 0b1000000);\n-\n-#undef INSN\n-\n-enum VectorMask {\n-  v0_t = 0b0,\n-  unmasked = 0b1\n-};\n-\n-#define patch_VArith(op, Reg, funct3, Reg_or_Imm5, Vs2, vm, funct6)            \\\n-    unsigned insn = 0;                                                         \\\n-    patch((address)&insn, 6, 0, op);                                           \\\n-    patch((address)&insn, 14, 12, funct3);                                     \\\n-    patch((address)&insn, 19, 15, Reg_or_Imm5);                                \\\n-    patch((address)&insn, 25, vm);                                             \\\n-    patch((address)&insn, 31, 26, funct6);                                     \\\n-    patch_reg((address)&insn, 7, Reg);                                         \\\n-    patch_reg((address)&insn, 20, Vs2);                                        \\\n-    emit(insn)\n-\n-\/\/ r2_vm\n-#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n-  void NAME(Register Rd, VectorRegister Vs2, VectorMask vm = unmasked) {       \\\n-    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);                        \\\n-  }\n-\n-  \/\/ Vector Mask\n-  INSN(vpopc_m,  0b1010111, 0b010, 0b10000, 0b010000);\n-  INSN(vfirst_m, 0b1010111, 0b010, 0b10001, 0b010000);\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, Vs1, funct6)                                    \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n-    patch_VArith(op, Vd, funct3, Vs1, Vs2, vm, funct6);                        \\\n-  }\n-\n-  \/\/ Vector Integer Extension\n-  INSN(vzext_vf2, 0b1010111, 0b010, 0b00110, 0b010010);\n-  INSN(vzext_vf4, 0b1010111, 0b010, 0b00100, 0b010010);\n-  INSN(vzext_vf8, 0b1010111, 0b010, 0b00010, 0b010010);\n-  INSN(vsext_vf2, 0b1010111, 0b010, 0b00111, 0b010010);\n-  INSN(vsext_vf4, 0b1010111, 0b010, 0b00101, 0b010010);\n-  INSN(vsext_vf8, 0b1010111, 0b010, 0b00011, 0b010010);\n-\n-  \/\/ Vector Mask\n-  INSN(vmsbf_m,   0b1010111, 0b010, 0b00001, 0b010100);\n-  INSN(vmsif_m,   0b1010111, 0b010, 0b00011, 0b010100);\n-  INSN(vmsof_m,   0b1010111, 0b010, 0b00010, 0b010100);\n-  INSN(viota_m,   0b1010111, 0b010, 0b10000, 0b010100);\n-\n-  \/\/ Vector Single-Width Floating-Point\/Integer Type-Convert Instructions\n-  INSN(vfcvt_xu_f_v, 0b1010111, 0b001, 0b00000, 0b010010);\n-  INSN(vfcvt_x_f_v,  0b1010111, 0b001, 0b00001, 0b010010);\n-  INSN(vfcvt_f_xu_v, 0b1010111, 0b001, 0b00010, 0b010010);\n-  INSN(vfcvt_f_x_v,  0b1010111, 0b001, 0b00011, 0b010010);\n-  INSN(vfcvt_rtz_xu_f_v, 0b1010111, 0b001, 0b00110, 0b010010);\n-  INSN(vfcvt_rtz_x_f_v,  0b1010111, 0b001, 0b00111, 0b010010);\n-\n-  \/\/ Vector Floating-Point Instruction\n-  INSN(vfsqrt_v,  0b1010111, 0b001, 0b00000, 0b010011);\n-  INSN(vfclass_v, 0b1010111, 0b001, 0b10000, 0b010011);\n-\n-#undef INSN\n-\n-\/\/ r2rd\n-#define INSN(NAME, op, funct3, simm5, vm, funct6)         \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs2) {      \\\n-    patch_VArith(op, Vd, funct3, simm5, Vs2, vm, funct6); \\\n-  }\n-\n-  \/\/ Vector Whole Vector Register Move\n-  INSN(vmv1r_v, 0b1010111, 0b011, 0b00000, 0b1, 0b100111);\n-  INSN(vmv2r_v, 0b1010111, 0b011, 0b00001, 0b1, 0b100111);\n-  INSN(vmv4r_v, 0b1010111, 0b011, 0b00011, 0b1, 0b100111);\n-  INSN(vmv8r_v, 0b1010111, 0b011, 0b00111, 0b1, 0b100111);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, Vs1, vm, funct6)           \\\n-  void NAME(FloatRegister Rd, VectorRegister Vs2) {       \\\n-    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);   \\\n-  }\n-\n-  \/\/ Vector Floating-Point Move Instruction\n-  INSN(vfmv_f_s, 0b1010111, 0b001, 0b00000, 0b1, 0b010000);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, Vs1, vm, funct6)          \\\n-  void NAME(Register Rd, VectorRegister Vs2) {           \\\n-    patch_VArith(op, Rd, funct3, Vs1, Vs2, vm, funct6);  \\\n-  }\n-\n-  \/\/ Vector Integer Scalar Move Instructions\n-  INSN(vmv_x_s, 0b1010111, 0b010, 0b00000, 0b1, 0b010000);\n-\n-#undef INSN\n-\n-\/\/ r_vm\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs2, uint32_t imm, VectorMask vm = unmasked) {       \\\n-    guarantee(is_unsigned_imm_in_range(imm, 5, 0), \"imm is invalid\");                              \\\n-    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6);                         \\\n-  }\n-\n-  \/\/ Vector Single-Width Bit Shift Instructions\n-  INSN(vsra_vi,    0b1010111, 0b011, 0b101001);\n-  INSN(vsrl_vi,    0b1010111, 0b011, 0b101000);\n-  INSN(vsll_vi,    0b1010111, 0b011, 0b100101);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs1, VectorRegister Vs2, VectorMask vm = unmasked) { \\\n-    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n-  }\n-\n-  \/\/ Vector Single-Width Floating-Point Fused Multiply-Add Instructions\n-  INSN(vfnmsub_vv, 0b1010111, 0b001, 0b101011);\n-  INSN(vfmsub_vv,  0b1010111, 0b001, 0b101010);\n-  INSN(vfnmadd_vv, 0b1010111, 0b001, 0b101001);\n-  INSN(vfmadd_vv,  0b1010111, 0b001, 0b101000);\n-  INSN(vfnmsac_vv, 0b1010111, 0b001, 0b101111);\n-  INSN(vfmsac_vv,  0b1010111, 0b001, 0b101110);\n-  INSN(vfmacc_vv,  0b1010111, 0b001, 0b101100);\n-  INSN(vfnmacc_vv, 0b1010111, 0b001, 0b101101);\n-\n-  \/\/ Vector Single-Width Integer Multiply-Add Instructions\n-  INSN(vnmsub_vv, 0b1010111, 0b010, 0b101011);\n-  INSN(vmadd_vv,  0b1010111, 0b010, 0b101001);\n-  INSN(vnmsac_vv, 0b1010111, 0b010, 0b101111);\n-  INSN(vmacc_vv,  0b1010111, 0b010, 0b101101);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, Register Rs1, VectorRegister Vs2, VectorMask vm = unmasked) {       \\\n-    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n-  }\n-\n-  \/\/ Vector Single-Width Integer Multiply-Add Instructions\n-  INSN(vnmsub_vx, 0b1010111, 0b110, 0b101011);\n-  INSN(vmadd_vx,  0b1010111, 0b110, 0b101001);\n-  INSN(vnmsac_vx, 0b1010111, 0b110, 0b101111);\n-  INSN(vmacc_vx,  0b1010111, 0b110, 0b101101);\n-\n-  INSN(vrsub_vx,  0b1010111, 0b100, 0b000011);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, FloatRegister Rs1, VectorRegister Vs2, VectorMask vm = unmasked) {  \\\n-    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n-  }\n-\n-  \/\/ Vector Single-Width Floating-Point Fused Multiply-Add Instructions\n-  INSN(vfnmsub_vf, 0b1010111, 0b101, 0b101011);\n-  INSN(vfmsub_vf,  0b1010111, 0b101, 0b101010);\n-  INSN(vfnmadd_vf, 0b1010111, 0b101, 0b101001);\n-  INSN(vfmadd_vf,  0b1010111, 0b101, 0b101000);\n-  INSN(vfnmsac_vf, 0b1010111, 0b101, 0b101111);\n-  INSN(vfmsac_vf,  0b1010111, 0b101, 0b101110);\n-  INSN(vfmacc_vf,  0b1010111, 0b101, 0b101100);\n-  INSN(vfnmacc_vf, 0b1010111, 0b101, 0b101101);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1, VectorMask vm = unmasked) { \\\n-    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n-  }\n-\n-  \/\/ Vector Single-Width Floating-Point Reduction Instructions\n-  INSN(vfredsum_vs,   0b1010111, 0b001, 0b000001);\n-  INSN(vfredosum_vs,  0b1010111, 0b001, 0b000011);\n-  INSN(vfredmin_vs,   0b1010111, 0b001, 0b000101);\n-  INSN(vfredmax_vs,   0b1010111, 0b001, 0b000111);\n-\n-  \/\/ Vector Single-Width Integer Reduction Instructions\n-  INSN(vredsum_vs,    0b1010111, 0b010, 0b000000);\n-  INSN(vredand_vs,    0b1010111, 0b010, 0b000001);\n-  INSN(vredor_vs,     0b1010111, 0b010, 0b000010);\n-  INSN(vredxor_vs,    0b1010111, 0b010, 0b000011);\n-  INSN(vredminu_vs,   0b1010111, 0b010, 0b000100);\n-  INSN(vredmin_vs,    0b1010111, 0b010, 0b000101);\n-  INSN(vredmaxu_vs,   0b1010111, 0b010, 0b000110);\n-  INSN(vredmax_vs,    0b1010111, 0b010, 0b000111);\n-\n-  \/\/ Vector Floating-Point Compare Instructions\n-  INSN(vmfle_vv, 0b1010111, 0b001, 0b011001);\n-  INSN(vmflt_vv, 0b1010111, 0b001, 0b011011);\n-  INSN(vmfne_vv, 0b1010111, 0b001, 0b011100);\n-  INSN(vmfeq_vv, 0b1010111, 0b001, 0b011000);\n-\n-  \/\/ Vector Floating-Point Sign-Injection Instructions\n-  INSN(vfsgnjx_vv, 0b1010111, 0b001, 0b001010);\n-  INSN(vfsgnjn_vv, 0b1010111, 0b001, 0b001001);\n-  INSN(vfsgnj_vv,  0b1010111, 0b001, 0b001000);\n-\n-  \/\/ Vector Floating-Point MIN\/MAX Instructions\n-  INSN(vfmax_vv,   0b1010111, 0b001, 0b000110);\n-  INSN(vfmin_vv,   0b1010111, 0b001, 0b000100);\n-\n-  \/\/ Vector Single-Width Floating-Point Multiply\/Divide Instructions\n-  INSN(vfdiv_vv,   0b1010111, 0b001, 0b100000);\n-  INSN(vfmul_vv,   0b1010111, 0b001, 0b100100);\n-\n-  \/\/ Vector Single-Width Floating-Point Add\/Subtract Instructions\n-  INSN(vfsub_vv, 0b1010111, 0b001, 0b000010);\n-  INSN(vfadd_vv, 0b1010111, 0b001, 0b000000);\n-\n-  \/\/ Vector Single-Width Fractional Multiply with Rounding and Saturation\n-  INSN(vsmul_vv, 0b1010111, 0b000, 0b100111);\n-\n-  \/\/ Vector Integer Divide Instructions\n-  INSN(vrem_vv,  0b1010111, 0b010, 0b100011);\n-  INSN(vremu_vv, 0b1010111, 0b010, 0b100010);\n-  INSN(vdiv_vv,  0b1010111, 0b010, 0b100001);\n-  INSN(vdivu_vv, 0b1010111, 0b010, 0b100000);\n-\n-  \/\/ Vector Single-Width Integer Multiply Instructions\n-  INSN(vmulhsu_vv, 0b1010111, 0b010, 0b100110);\n-  INSN(vmulhu_vv,  0b1010111, 0b010, 0b100100);\n-  INSN(vmulh_vv,   0b1010111, 0b010, 0b100111);\n-  INSN(vmul_vv,    0b1010111, 0b010, 0b100101);\n-\n-  \/\/ Vector Integer Min\/Max Instructions\n-  INSN(vmax_vv,  0b1010111, 0b000, 0b000111);\n-  INSN(vmaxu_vv, 0b1010111, 0b000, 0b000110);\n-  INSN(vmin_vv,  0b1010111, 0b000, 0b000101);\n-  INSN(vminu_vv, 0b1010111, 0b000, 0b000100);\n-\n-  \/\/ Vector Integer Comparison Instructions\n-  INSN(vmsle_vv,  0b1010111, 0b000, 0b011101);\n-  INSN(vmsleu_vv, 0b1010111, 0b000, 0b011100);\n-  INSN(vmslt_vv,  0b1010111, 0b000, 0b011011);\n-  INSN(vmsltu_vv, 0b1010111, 0b000, 0b011010);\n-  INSN(vmsne_vv,  0b1010111, 0b000, 0b011001);\n-  INSN(vmseq_vv,  0b1010111, 0b000, 0b011000);\n-\n-  \/\/ Vector Single-Width Bit Shift Instructions\n-  INSN(vsra_vv, 0b1010111, 0b000, 0b101001);\n-  INSN(vsrl_vv, 0b1010111, 0b000, 0b101000);\n-  INSN(vsll_vv, 0b1010111, 0b000, 0b100101);\n-\n-  \/\/ Vector Bitwise Logical Instructions\n-  INSN(vxor_vv, 0b1010111, 0b000, 0b001011);\n-  INSN(vor_vv,  0b1010111, 0b000, 0b001010);\n-  INSN(vand_vv, 0b1010111, 0b000, 0b001001);\n-\n-  \/\/ Vector Single-Width Integer Add and Subtract\n-  INSN(vsub_vv, 0b1010111, 0b000, 0b000010);\n-  INSN(vadd_vv, 0b1010111, 0b000, 0b000000);\n-\n-#undef INSN\n-\n-\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs2, Register Rs1, VectorMask vm = unmasked) {       \\\n-    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n-  }\n-\n-  \/\/ Vector Integer Divide Instructions\n-  INSN(vrem_vx,  0b1010111, 0b110, 0b100011);\n-  INSN(vremu_vx, 0b1010111, 0b110, 0b100010);\n-  INSN(vdiv_vx,  0b1010111, 0b110, 0b100001);\n-  INSN(vdivu_vx, 0b1010111, 0b110, 0b100000);\n-\n-  \/\/ Vector Single-Width Integer Multiply Instructions\n-  INSN(vmulhsu_vx, 0b1010111, 0b110, 0b100110);\n-  INSN(vmulhu_vx,  0b1010111, 0b110, 0b100100);\n-  INSN(vmulh_vx,   0b1010111, 0b110, 0b100111);\n-  INSN(vmul_vx,    0b1010111, 0b110, 0b100101);\n-\n-  \/\/ Vector Integer Min\/Max Instructions\n-  INSN(vmax_vx,  0b1010111, 0b100, 0b000111);\n-  INSN(vmaxu_vx, 0b1010111, 0b100, 0b000110);\n-  INSN(vmin_vx,  0b1010111, 0b100, 0b000101);\n-  INSN(vminu_vx, 0b1010111, 0b100, 0b000100);\n-\n-  \/\/ Vector Integer Comparison Instructions\n-  INSN(vmsgt_vx,  0b1010111, 0b100, 0b011111);\n-  INSN(vmsgtu_vx, 0b1010111, 0b100, 0b011110);\n-  INSN(vmsle_vx,  0b1010111, 0b100, 0b011101);\n-  INSN(vmsleu_vx, 0b1010111, 0b100, 0b011100);\n-  INSN(vmslt_vx,  0b1010111, 0b100, 0b011011);\n-  INSN(vmsltu_vx, 0b1010111, 0b100, 0b011010);\n-  INSN(vmsne_vx,  0b1010111, 0b100, 0b011001);\n-  INSN(vmseq_vx,  0b1010111, 0b100, 0b011000);\n-\n-  \/\/ Vector Narrowing Integer Right Shift Instructions\n-  INSN(vnsra_wx, 0b1010111, 0b100, 0b101101);\n-  INSN(vnsrl_wx, 0b1010111, 0b100, 0b101100);\n-\n-  \/\/ Vector Single-Width Bit Shift Instructions\n-  INSN(vsra_vx, 0b1010111, 0b100, 0b101001);\n-  INSN(vsrl_vx, 0b1010111, 0b100, 0b101000);\n-  INSN(vsll_vx, 0b1010111, 0b100, 0b100101);\n-\n-  \/\/ Vector Bitwise Logical Instructions\n-  INSN(vxor_vx, 0b1010111, 0b100, 0b001011);\n-  INSN(vor_vx,  0b1010111, 0b100, 0b001010);\n-  INSN(vand_vx, 0b1010111, 0b100, 0b001001);\n-\n-  \/\/ Vector Single-Width Integer Add and Subtract\n-  INSN(vsub_vx, 0b1010111, 0b100, 0b000010);\n-  INSN(vadd_vx, 0b1010111, 0b100, 0b000000);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs2, FloatRegister Rs1, VectorMask vm = unmasked) {  \\\n-    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6);                        \\\n-  }\n-\n-  \/\/ Vector Floating-Point Compare Instructions\n-  INSN(vmfge_vf, 0b1010111, 0b101, 0b011111);\n-  INSN(vmfgt_vf, 0b1010111, 0b101, 0b011101);\n-  INSN(vmfle_vf, 0b1010111, 0b101, 0b011001);\n-  INSN(vmflt_vf, 0b1010111, 0b101, 0b011011);\n-  INSN(vmfne_vf, 0b1010111, 0b101, 0b011100);\n-  INSN(vmfeq_vf, 0b1010111, 0b101, 0b011000);\n-\n-  \/\/ Vector Floating-Point Sign-Injection Instructions\n-  INSN(vfsgnjx_vf, 0b1010111, 0b101, 0b001010);\n-  INSN(vfsgnjn_vf, 0b1010111, 0b101, 0b001001);\n-  INSN(vfsgnj_vf,  0b1010111, 0b101, 0b001000);\n-\n-  \/\/ Vector Floating-Point MIN\/MAX Instructions\n-  INSN(vfmax_vf, 0b1010111, 0b101, 0b000110);\n-  INSN(vfmin_vf, 0b1010111, 0b101, 0b000100);\n-\n-  \/\/ Vector Single-Width Floating-Point Multiply\/Divide Instructions\n-  INSN(vfdiv_vf,  0b1010111, 0b101, 0b100000);\n-  INSN(vfmul_vf,  0b1010111, 0b101, 0b100100);\n-  INSN(vfrdiv_vf, 0b1010111, 0b101, 0b100001);\n-\n-  \/\/ Vector Single-Width Floating-Point Add\/Subtract Instructions\n-  INSN(vfsub_vf,  0b1010111, 0b101, 0b000010);\n-  INSN(vfadd_vf,  0b1010111, 0b101, 0b000000);\n-  INSN(vfrsub_vf, 0b1010111, 0b101, 0b100111);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs2, int32_t imm, VectorMask vm = unmasked) {        \\\n-    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");                                       \\\n-    patch_VArith(op, Vd, funct3, (uint32_t)imm & 0x1f, Vs2, vm, funct6);                           \\\n-  }\n-\n-  INSN(vmsgt_vi,  0b1010111, 0b011, 0b011111);\n-  INSN(vmsgtu_vi, 0b1010111, 0b011, 0b011110);\n-  INSN(vmsle_vi,  0b1010111, 0b011, 0b011101);\n-  INSN(vmsleu_vi, 0b1010111, 0b011, 0b011100);\n-  INSN(vmsne_vi,  0b1010111, 0b011, 0b011001);\n-  INSN(vmseq_vi,  0b1010111, 0b011, 0b011000);\n-  INSN(vxor_vi,   0b1010111, 0b011, 0b001011);\n-  INSN(vor_vi,    0b1010111, 0b011, 0b001010);\n-  INSN(vand_vi,   0b1010111, 0b011, 0b001001);\n-  INSN(vadd_vi,   0b1010111, 0b011, 0b000000);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, funct6)                                                             \\\n-  void NAME(VectorRegister Vd, int32_t imm, VectorRegister Vs2, VectorMask vm = unmasked) {        \\\n-    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");                                       \\\n-    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6);                         \\\n-  }\n-\n-  INSN(vrsub_vi, 0b1010111, 0b011, 0b000011);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, vm, funct6)                                   \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs2, VectorRegister Vs1) {     \\\n-    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6);  \\\n-  }\n-\n-  \/\/ Vector Compress Instruction\n-  INSN(vcompress_vm, 0b1010111, 0b010, 0b1, 0b010111);\n-\n-  \/\/ Vector Mask-Register Logical Instructions\n-  INSN(vmxnor_mm,   0b1010111, 0b010, 0b1, 0b011111);\n-  INSN(vmornot_mm,  0b1010111, 0b010, 0b1, 0b011100);\n-  INSN(vmnor_mm,    0b1010111, 0b010, 0b1, 0b011110);\n-  INSN(vmor_mm,     0b1010111, 0b010, 0b1, 0b011010);\n-  INSN(vmxor_mm,    0b1010111, 0b010, 0b1, 0b011011);\n-  INSN(vmandnot_mm, 0b1010111, 0b010, 0b1, 0b011000);\n-  INSN(vmnand_mm,   0b1010111, 0b010, 0b1, 0b011101);\n-  INSN(vmand_mm,    0b1010111, 0b010, 0b1, 0b011001);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, Vs2, vm, funct6)                            \\\n-  void NAME(VectorRegister Vd, int32_t imm) {                              \\\n-    guarantee(is_imm_in_range(imm, 5, 0), \"imm is invalid\");               \\\n-    patch_VArith(op, Vd, funct3, (uint32_t)(imm & 0x1f), Vs2, vm, funct6); \\\n-  }\n-\n-  \/\/ Vector Integer Move Instructions\n-  INSN(vmv_v_i, 0b1010111, 0b011, v0, 0b1, 0b010111);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n-  void NAME(VectorRegister Vd, FloatRegister Rs1) {                         \\\n-    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6); \\\n-  }\n-\n-  \/\/ Floating-Point Scalar Move Instructions\n-  INSN(vfmv_s_f, 0b1010111, 0b101, v0, 0b1, 0b010000);\n-  \/\/ Vector Floating-Point Move Instruction\n-  INSN(vfmv_v_f, 0b1010111, 0b101, v0, 0b1, 0b010111);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n-  void NAME(VectorRegister Vd, VectorRegister Vs1) {                        \\\n-    patch_VArith(op, Vd, funct3, Vs1->encoding_nocheck(), Vs2, vm, funct6); \\\n-  }\n-\n-  \/\/ Vector Integer Move Instructions\n-  INSN(vmv_v_v, 0b1010111, 0b000, v0, 0b1, 0b010111);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, funct3, Vs2, vm, funct6)                             \\\n-   void NAME(VectorRegister Vd, Register Rs1) {                             \\\n-    patch_VArith(op, Vd, funct3, Rs1->encoding_nocheck(), Vs2, vm, funct6); \\\n-   }\n-\n-  \/\/ Integer Scalar Move Instructions\n-  INSN(vmv_s_x, 0b1010111, 0b110, v0, 0b1, 0b010000);\n-\n-  \/\/ Vector Integer Move Instructions\n-  INSN(vmv_v_x, 0b1010111, 0b100, v0, 0b1, 0b010111);\n-\n-#undef INSN\n-#undef patch_VArith\n-\n-#define INSN(NAME, op, funct13, funct6)                    \\\n-  void NAME(VectorRegister Vd, VectorMask vm = unmasked) { \\\n-    unsigned insn = 0;                                     \\\n-    patch((address)&insn, 6, 0, op);                       \\\n-    patch((address)&insn, 24, 12, funct13);                \\\n-    patch((address)&insn, 25, vm);                         \\\n-    patch((address)&insn, 31, 26, funct6);                 \\\n-    patch_reg((address)&insn, 7, Vd);                      \\\n-    emit(insn);                                            \\\n-  }\n-\n-  \/\/ Vector Element Index Instruction\n-  INSN(vid_v, 0b1010111, 0b0000010001010, 0b010100);\n-\n-#undef INSN\n-\n-enum Nf {\n-  g1 = 0b000,\n-  g2 = 0b001,\n-  g3 = 0b010,\n-  g4 = 0b011,\n-  g5 = 0b100,\n-  g6 = 0b101,\n-  g7 = 0b110,\n-  g8 = 0b111\n-};\n-\n-#define patch_VLdSt(op, VReg, width, Rs1, Reg_or_umop, vm, mop, mew, nf) \\\n-    unsigned insn = 0;                                                   \\\n-    patch((address)&insn, 6, 0, op);                                     \\\n-    patch((address)&insn, 14, 12, width);                                \\\n-    patch((address)&insn, 24, 20, Reg_or_umop);                          \\\n-    patch((address)&insn, 25, vm);                                       \\\n-    patch((address)&insn, 27, 26, mop);                                  \\\n-    patch((address)&insn, 28, mew);                                      \\\n-    patch((address)&insn, 31, 29, nf);                                   \\\n-    patch_reg((address)&insn, 7, VReg);                                  \\\n-    patch_reg((address)&insn, 15, Rs1);                                  \\\n-    emit(insn)\n-\n-#define INSN(NAME, op, lumop, vm, mop, nf)                                           \\\n-  void NAME(VectorRegister Vd, Register Rs1, uint32_t width = 0, bool mew = false) { \\\n-    guarantee(is_unsigned_imm_in_range(width, 3, 0), \"width is invalid\");            \\\n-    patch_VLdSt(op, Vd, width, Rs1, lumop, vm, mop, mew, nf);                        \\\n-  }\n-\n-  \/\/ Vector Load\/Store Instructions\n-  INSN(vl1r_v, 0b0000111, 0b01000, 0b1, 0b00, g1);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, width, sumop, vm, mop, mew, nf)           \\\n-  void NAME(VectorRegister Vs3, Register Rs1) {                  \\\n-    patch_VLdSt(op, Vs3, width, Rs1, sumop, vm, mop, mew, nf);   \\\n-  }\n-\n-  \/\/ Vector Load\/Store Instructions\n-  INSN(vs1r_v, 0b0100111, 0b000, 0b01000, 0b1, 0b00, 0b0, g1);\n-\n-#undef INSN\n-\n-\/\/ r2_nfvm\n-#define INSN(NAME, op, width, umop, mop, mew)                         \\\n-  void NAME(VectorRegister Vd_or_Vs3, Register Rs1, Nf nf = g1) {     \\\n-    patch_VLdSt(op, Vd_or_Vs3, width, Rs1, umop, 1, mop, mew, nf);    \\\n-  }\n-\n-  \/\/ Vector Unit-Stride Instructions\n-  INSN(vle1_v, 0b0000111, 0b000, 0b01011, 0b00, 0b0);\n-  INSN(vse1_v, 0b0100111, 0b000, 0b01011, 0b00, 0b0);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, width, umop, mop, mew)                                               \\\n-  void NAME(VectorRegister Vd_or_Vs3, Register Rs1, VectorMask vm = unmasked, Nf nf = g1) { \\\n-    patch_VLdSt(op, Vd_or_Vs3, width, Rs1, umop, vm, mop, mew, nf);                         \\\n-  }\n-\n-  \/\/ Vector Unit-Stride Instructions\n-  INSN(vle8_v,    0b0000111, 0b000, 0b00000, 0b00, 0b0);\n-  INSN(vle16_v,   0b0000111, 0b101, 0b00000, 0b00, 0b0);\n-  INSN(vle32_v,   0b0000111, 0b110, 0b00000, 0b00, 0b0);\n-  INSN(vle64_v,   0b0000111, 0b111, 0b00000, 0b00, 0b0);\n-\n-  \/\/ Vector unit-stride fault-only-first Instructions\n-  INSN(vle8ff_v,  0b0000111, 0b000, 0b10000, 0b00, 0b0);\n-  INSN(vle16ff_v, 0b0000111, 0b101, 0b10000, 0b00, 0b0);\n-  INSN(vle32ff_v, 0b0000111, 0b110, 0b10000, 0b00, 0b0);\n-  INSN(vle64ff_v, 0b0000111, 0b111, 0b10000, 0b00, 0b0);\n-\n-  INSN(vse8_v,  0b0100111, 0b000, 0b00000, 0b00, 0b0);\n-  INSN(vse16_v, 0b0100111, 0b101, 0b00000, 0b00, 0b0);\n-  INSN(vse32_v, 0b0100111, 0b110, 0b00000, 0b00, 0b0);\n-  INSN(vse64_v, 0b0100111, 0b111, 0b00000, 0b00, 0b0);\n-\n-#undef INSN\n-\n-#define INSN(NAME, op, width, mop, mew)                                                                  \\\n-  void NAME(VectorRegister Vd, Register Rs1, VectorRegister Vs2, VectorMask vm = unmasked, Nf nf = g1) { \\\n-    patch_VLdSt(op, Vd, width, Rs1, Vs2->encoding_nocheck(), vm, mop, mew, nf);                          \\\n-  }\n-\n-  \/\/ Vector unordered indexed load instructions\n-  INSN(vluxei8_v,  0b0000111, 0b000, 0b01, 0b0);\n-  INSN(vluxei16_v, 0b0000111, 0b101, 0b01, 0b0);\n-  INSN(vluxei32_v, 0b0000111, 0b110, 0b01, 0b0);\n-  INSN(vluxei64_v, 0b0000111, 0b111, 0b01, 0b0);\n-\n-  \/\/ Vector ordered indexed load instructions\n-  INSN(vloxei8_v,  0b0000111, 0b000, 0b11, 0b0);\n-  INSN(vloxei16_v, 0b0000111, 0b101, 0b11, 0b0);\n-  INSN(vloxei32_v, 0b0000111, 0b110, 0b11, 0b0);\n-  INSN(vloxei64_v, 0b0000111, 0b111, 0b11, 0b0);\n-#undef INSN\n-\n-#define INSN(NAME, op, width, mop, mew)                                                                  \\\n-  void NAME(VectorRegister Vd, Register Rs1, Register Rs2, VectorMask vm = unmasked, Nf nf = g1) {       \\\n-    patch_VLdSt(op, Vd, width, Rs1, Rs2->encoding_nocheck(), vm, mop, mew, nf);                          \\\n-  }\n-\n-  \/\/ Vector Strided Instructions\n-  INSN(vlse8_v,  0b0000111, 0b000, 0b10, 0b0);\n-  INSN(vlse16_v, 0b0000111, 0b101, 0b10, 0b0);\n-  INSN(vlse32_v, 0b0000111, 0b110, 0b10, 0b0);\n-  INSN(vlse64_v, 0b0000111, 0b111, 0b10, 0b0);\n-\n-#undef INSN\n-#undef patch_VLdSt\n-\n-#endif \/\/ CPU_RISCV_ASSEMBLER_RISCV_V_HPP\n","filename":"src\/hotspot\/cpu\/riscv\/assembler_riscv_v.hpp","additions":0,"deletions":697,"binary":false,"changes":697,"status":"deleted"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -181,1 +181,1 @@\n-  grev16(reg, reg);\n+  revb_h(reg, reg);\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1476,35 +1476,12 @@\n-void MacroAssembler::reverseb16(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ This method is only used for grev16\n-  \/\/ Rd = Rs[47:0] Rs[55:48] Rs[63:56]\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1);\n-  srli(Rtmp1, Rs, 48);\n-  andi(Rtmp2, Rtmp1, 0xff);\n-  slli(Rtmp2, Rtmp2, 8);\n-  srli(Rtmp1, Rtmp1, 8);\n-  orr(Rtmp1, Rtmp1, Rtmp2);\n-  slli(Rd, Rs, 16);\n-  orr(Rd, Rd, Rtmp1);\n-}\n-\n-void MacroAssembler::reverseh32(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ This method is only used for grev32\n-  \/\/ Rd[63:0] = Rs[31:0] Rs[47:32] Rs[63:48]\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1);\n-  srli(Rtmp1, Rs, 32);\n-  slli(Rtmp2, Rtmp1, 48);\n-  srli(Rtmp2, Rtmp2, 32);\n-  srli(Rtmp1, Rtmp1, 16);\n-  orr(Rtmp1, Rtmp1, Rtmp2);\n-  slli(Rd, Rs, 32);\n-  orr(Rd, Rd, Rtmp1);\n-}\n-\n-void MacroAssembler::grevh(Register Rd, Register Rs, Register Rtmp) {\n-  \/\/ Reverse bytes in half-word\n-  \/\/ Rd[15:0] = Rs[7:0] Rs[15:8] (sign-extend to 64 bits)\n-  assert_different_registers(Rs, Rtmp);\n-  assert_different_registers(Rd, Rtmp);\n-  srli(Rtmp, Rs, 8);\n-  andi(Rtmp, Rtmp, 0xFF);\n+\/\/ reverse bytes in halfword in lower 16 bits and sign-extend\n+\/\/ Rd[15:0] = Rs[7:0] Rs[15:8] (sign-extend to 64 bits)\n+void MacroAssembler::revb_h_h(Register Rd, Register Rs, Register tmp) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    srai(Rd, Rd, 48);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp);\n+  assert_different_registers(Rd, tmp);\n+  srli(tmp, Rs, 8);\n+  andi(tmp, tmp, 0xFF);\n@@ -1513,1 +1490,1 @@\n-  orr(Rd, Rd, Rtmp);\n+  orr(Rd, Rd, tmp);\n@@ -1516,44 +1493,13 @@\n-void MacroAssembler::grevhu(Register Rd, Register Rs, Register Rtmp) {\n-  \/\/ Reverse bytes in half-word\n-  \/\/ Rd[15:0] = Rs[7:0] Rs[15:8] (zero-extend to 64 bits)\n-  assert_different_registers(Rs, Rtmp);\n-  assert_different_registers(Rd, Rtmp);\n-  srli(Rtmp, Rs, 8);\n-  andi(Rtmp, Rtmp, 0xFF);\n-  andi(Rd, Rs, 0xFF);\n-  slli(Rd, Rd, 8);\n-  orr(Rd, Rd, Rtmp);\n-}\n-\n-void MacroAssembler::grev16w(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ Reverse bytes in half-word (32bit)\n-  \/\/ Rd[31:0] = Rs[23:16] Rs[31:24] Rs[7:0] Rs[15:8] (sign-extend to 64 bits)\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1, Rtmp2);\n-  srli(Rtmp2, Rs, 16);\n-  grevh(Rtmp2, Rtmp2, Rtmp1);\n-  grevhu(Rd, Rs, Rtmp1);\n-  slli(Rtmp2, Rtmp2, 16);\n-  orr(Rd, Rd, Rtmp2);\n-}\n-\n-void MacroAssembler::grev16wu(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ Reverse bytes in half-word (32bit)\n-  \/\/ Rd[31:0] = Rs[23:16] Rs[31:24] Rs[7:0] Rs[15:8] (zero-extend to 64 bits)\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1, Rtmp2);\n-  srli(Rtmp2, Rs, 16);\n-  grevhu(Rtmp2, Rtmp2, Rtmp1);\n-  grevhu(Rd, Rs, Rtmp1);\n-  slli(Rtmp2, Rtmp2, 16);\n-  orr(Rd, Rd, Rtmp2);\n-}\n-\n-void MacroAssembler::grevw(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ Reverse bytes in word (32bit)\n-  \/\/ Rd[31:0] = Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24] (sign-extend to 64 bits)\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1, Rtmp2);\n-  grev16wu(Rd, Rs, Rtmp1, Rtmp2);\n-  slli(Rtmp2, Rd, 48);\n-  srai(Rtmp2, Rtmp2, 32); \/\/ sign-extend\n+\/\/ reverse bytes in lower word and sign-extend\n+\/\/ Rd[31:0] = Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24] (sign-extend to 64 bits)\n+void MacroAssembler::revb_w_w(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    srai(Rd, Rd, 32);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  revb_h_w_u(Rd, Rs, tmp1, tmp2);\n+  slli(tmp2, Rd, 48);\n+  srai(tmp2, tmp2, 32); \/\/ sign-extend\n@@ -1561,1 +1507,1 @@\n-  orr(Rd, Rd, Rtmp2);\n+  orr(Rd, Rd, tmp2);\n@@ -1564,10 +1510,15 @@\n-void MacroAssembler::grevwu(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ Reverse bytes in word (32bit)\n-  \/\/ Rd[31:0] = Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24] (zero-extend to 64 bits)\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1, Rtmp2);\n-  grev16wu(Rd, Rs, Rtmp1, Rtmp2);\n-  slli(Rtmp2, Rd, 48);\n-  srli(Rtmp2, Rtmp2, 32);\n-  srli(Rd, Rd, 16);\n-  orr(Rd, Rd, Rtmp2);\n+\/\/ reverse bytes in halfword in lower 16 bits and zero-extend\n+\/\/ Rd[15:0] = Rs[7:0] Rs[15:8] (zero-extend to 64 bits)\n+void MacroAssembler::revb_h_h_u(Register Rd, Register Rs, Register tmp) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    srli(Rd, Rd, 48);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp);\n+  assert_different_registers(Rd, tmp);\n+  srli(tmp, Rs, 8);\n+  andi(tmp, tmp, 0xFF);\n+  andi(Rd, Rs, 0xFF);\n+  slli(Rd, Rd, 8);\n+  orr(Rd, Rd, tmp);\n@@ -1576,6 +1527,52 @@\n-void MacroAssembler::grev16(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ Reverse bytes in half-word (64bit)\n-  \/\/ Rd[63:0] = Rs[55:48] Rs[63:56] Rs[39:32] Rs[47:40] Rs[23:16] Rs[31:24] Rs[7:0] Rs[15:8]\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1, Rtmp2);\n-  reverseb16(Rd, Rs, Rtmp1, Rtmp2);\n+\/\/ reverse bytes in halfwords in lower 32 bits and zero-extend\n+\/\/ Rd[31:0] = Rs[23:16] Rs[31:24] Rs[7:0] Rs[15:8] (zero-extend to 64 bits)\n+void MacroAssembler::revb_h_w_u(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    rori(Rd, Rd, 32);\n+    roriw(Rd, Rd, 16);\n+    zext_w(Rd, Rd);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  srli(tmp2, Rs, 16);\n+  revb_h_h_u(tmp2, tmp2, tmp1);\n+  revb_h_h_u(Rd, Rs, tmp1);\n+  slli(tmp2, tmp2, 16);\n+  orr(Rd, Rd, tmp2);\n+}\n+\n+\/\/ This method is only used for revb_h\n+\/\/ Rd = Rs[47:0] Rs[55:48] Rs[63:56]\n+void MacroAssembler::revb_h_helper(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1);\n+  srli(tmp1, Rs, 48);\n+  andi(tmp2, tmp1, 0xFF);\n+  slli(tmp2, tmp2, 8);\n+  srli(tmp1, tmp1, 8);\n+  orr(tmp1, tmp1, tmp2);\n+  slli(Rd, Rs, 16);\n+  orr(Rd, Rd, tmp1);\n+}\n+\n+\/\/ reverse bytes in each halfword\n+\/\/ Rd[63:0] = Rs[55:48] Rs[63:56] Rs[39:32] Rs[47:40] Rs[23:16] Rs[31:24] Rs[7:0] Rs[15:8]\n+void MacroAssembler::revb_h(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    assert_different_registers(Rs, tmp1);\n+    assert_different_registers(Rd, tmp1);\n+    rev8(Rd, Rs);\n+    zext_w(tmp1, Rd);\n+    roriw(tmp1, tmp1, 16);\n+    slli(tmp1, tmp1, 32);\n+    srli(Rd, Rd, 32);\n+    roriw(Rd, Rd, 16);\n+    zext_w(Rd, Rd);\n+    orr(Rd, Rd, tmp1);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  revb_h_helper(Rd, Rs, tmp1, tmp2);\n@@ -1583,1 +1580,1 @@\n-    reverseb16(Rd, Rd, Rtmp1, Rtmp2);\n+    revb_h_helper(Rd, Rd, tmp1, tmp2);\n@@ -1587,8 +1584,12 @@\n-void MacroAssembler::grev32(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ Reverse bytes in word (64bit)\n-  \/\/ Rd[63:0] = Rs[39:32] Rs[47:40] Rs[55:48] Rs[63:56] Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24]\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1, Rtmp2);\n-  grev16(Rd, Rs, Rtmp1, Rtmp2);\n-  reverseh32(Rd, Rd, Rtmp1, Rtmp2);\n-  reverseh32(Rd, Rd, Rtmp1, Rtmp2);\n+\/\/ reverse bytes in each word\n+\/\/ Rd[63:0] = Rs[39:32] Rs[47:40] Rs[55:48] Rs[63:56] Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24]\n+void MacroAssembler::revb_w(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    rori(Rd, Rd, 32);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  revb(Rd, Rs, tmp1, tmp2);\n+  ror_imm(Rd, Rd, 32);\n@@ -1597,9 +1598,20 @@\n-void MacroAssembler::grev(Register Rd, Register Rs, Register Rtmp1, Register Rtmp2) {\n-  \/\/ Reverse bytes in double-word (64bit)\n-  \/\/ Rd[63:0] = Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24] Rs[39:32] Rs[47,40] Rs[55,48] Rs[63:56]\n-  assert_different_registers(Rs, Rtmp1, Rtmp2);\n-  assert_different_registers(Rd, Rtmp1, Rtmp2);\n-  grev32(Rd, Rs, Rtmp1, Rtmp2);\n-  slli(Rtmp2, Rd, 32);\n-  srli(Rd, Rd, 32);\n-  orr(Rd, Rd, Rtmp2);\n+\/\/ reverse bytes in doubleword\n+\/\/ Rd[63:0] = Rs[7:0] Rs[15:8] Rs[23:16] Rs[31:24] Rs[39:32] Rs[47,40] Rs[55,48] Rs[63:56]\n+void MacroAssembler::revb(Register Rd, Register Rs, Register tmp1, Register tmp2) {\n+  if (UseRVB) {\n+    rev8(Rd, Rs);\n+    return;\n+  }\n+  assert_different_registers(Rs, tmp1, tmp2);\n+  assert_different_registers(Rd, tmp1, tmp2);\n+  andi(tmp1, Rs, 0xFF);\n+  slli(tmp1, tmp1, 8);\n+  for (int step = 8; step < 56; step += 8) {\n+    srli(tmp2, Rs, step);\n+    andi(tmp2, tmp2, 0xFF);\n+    orr(tmp1, tmp1, tmp2);\n+    slli(tmp1, tmp1, 8);\n+  }\n+  srli(Rd, Rs, 56);\n+  andi(Rd, Rd, 0xFF);\n+  orr(Rd, tmp1, Rd);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":127,"deletions":115,"binary":false,"changes":242,"status":"modified"},{"patch":"@@ -530,13 +530,9 @@\n-  \/\/ grev\n-  void reverseb16(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2= t1);  \/\/ reverse bytes in 16-bit and move to lower\n-  void reverseh32(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2= t1);  \/\/ reverse half-words in 32-bit and move to lower\n-  void grevh(Register Rd, Register Rs, Register Rtmp = t0);                            \/\/ basic reverse bytes in 16-bit halfwords, sign-extend\n-  void grev16w(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2 = t1);    \/\/ reverse bytes in 16-bit halfwords(32), sign-extend\n-  void grevw(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2 = t1);      \/\/ reverse bytes(32), sign-extend\n-  void grev16(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2= t1);      \/\/ reverse bytes in 16-bit halfwords\n-  void grev32(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2= t1);      \/\/ reverse bytes in 32-bit words\n-  void grev(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2 = t1);       \/\/ reverse bytes in 64-bit double-words\n-  void grevhu(Register Rd, Register Rs, Register Rtmp = t0);                           \/\/ basic reverse bytes in 16-bit halfwords, zero-extend\n-  void grev16wu(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2 = t1);   \/\/ reverse bytes in 16-bit halfwords(32), zero-extend\n-  void grevwu(Register Rd, Register Rs, Register Rtmp1 = t0, Register Rtmp2 = t1);     \/\/ reverse bytes(32), zero-extend\n-\n+  \/\/ revb\n+  void revb_h_h(Register Rd, Register Rs, Register tmp = t0);                           \/\/ reverse bytes in halfword in lower 16 bits, sign-extend\n+  void revb_w_w(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2 = t1);      \/\/ reverse bytes in lower word, sign-extend\n+  void revb_h_h_u(Register Rd, Register Rs, Register tmp = t0);                         \/\/ reverse bytes in halfword in lower 16 bits, zero-extend\n+  void revb_h_w_u(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2 = t1);    \/\/ reverse bytes in halfwords in lower 32 bits, zero-extend\n+  void revb_h_helper(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2= t1);  \/\/ reverse bytes in upper 16 bits (48:63) and move to lower\n+  void revb_h(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2= t1);         \/\/ reverse bytes in each halfword\n+  void revb_w(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2= t1);         \/\/ reverse bytes in each word\n+  void revb(Register Rd, Register Rs, Register tmp1 = t0, Register tmp2 = t1);          \/\/ reverse bytes in doubleword\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":9,"deletions":13,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -7561,2 +7561,2 @@\n-  ins_cost(ALU_COST * 17);\n-  format %{ \"grevw  $dst, $src\\t#@bytes_reverse_int\" %}\n+  ins_cost(ALU_COST * 13);\n+  format %{ \"revb_w_w  $dst, $src\\t#@bytes_reverse_int\" %}\n@@ -7565,1 +7565,1 @@\n-    __ grevw(as_Register($dst$$reg), as_Register($src$$reg));\n+    __ revb_w_w(as_Register($dst$$reg), as_Register($src$$reg));\n@@ -7575,2 +7575,2 @@\n-  ins_cost(ALU_COST * 45);\n-  format %{ \"grev  $dst, $src\\t#@bytes_reverse_long\" %}\n+  ins_cost(ALU_COST * 29);\n+  format %{ \"revb  $dst, $src\\t#@bytes_reverse_long\" %}\n@@ -7579,1 +7579,1 @@\n-    __ grev(as_Register($dst$$reg), as_Register($src$$reg));\n+    __ revb(as_Register($dst$$reg), as_Register($src$$reg));\n@@ -7589,1 +7589,1 @@\n-  format %{ \"grevhu  $dst, $src\\t#@bytes_reverse_unsigned_short\" %}\n+  format %{ \"revb_h_h_u  $dst, $src\\t#@bytes_reverse_unsigned_short\" %}\n@@ -7592,1 +7592,1 @@\n-    __ grevhu(as_Register($dst$$reg), as_Register($src$$reg));\n+    __ revb_h_h_u(as_Register($dst$$reg), as_Register($src$$reg));\n@@ -7602,1 +7602,1 @@\n-  format %{ \"grevh  $dst, $src\\t#@bytes_reverse_short\" %}\n+  format %{ \"revb_h_h  $dst, $src\\t#@bytes_reverse_short\" %}\n@@ -7605,1 +7605,1 @@\n-    __ grevh(as_Register($dst$$reg), as_Register($src$$reg));\n+    __ revb_h_h(as_Register($dst$$reg), as_Register($src$$reg));\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -177,0 +177,57 @@\n+%}\n+\n+\/\/ BSWAP instructions\n+instruct bytes_reverse_int_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ReverseBytesI src));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{ \"revb_w_w  $dst, $src\\t#@bytes_reverse_int_rvb\" %}\n+\n+  ins_encode %{\n+    __ revb_w_w(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_long_rvb(iRegLNoSp dst, iRegL src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ReverseBytesL src));\n+\n+  ins_cost(ALU_COST);\n+  format %{ \"rev8  $dst, $src\\t#@bytes_reverse_long_rvb\" %}\n+\n+  ins_encode %{\n+    __ rev8(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_unsigned_short_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ReverseBytesUS src));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{ \"revb_h_h_u  $dst, $src\\t#@bytes_reverse_unsigned_short_rvb\" %}\n+\n+  ins_encode %{\n+    __ revb_h_h_u(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n+%}\n+\n+instruct bytes_reverse_short_rvb(iRegINoSp dst, iRegIorL2I src) %{\n+  predicate(UseRVB);\n+  match(Set dst (ReverseBytesS src));\n+\n+  ins_cost(ALU_COST * 2);\n+  format %{ \"revb_h_h  $dst, $src\\t#@bytes_reverse_short_rvb\" %}\n+\n+  ins_encode %{\n+    __ revb_h_h(as_Register($dst$$reg), as_Register($src$$reg));\n+  %}\n+\n+  ins_pipe(ialu_reg);\n","filename":"src\/hotspot\/cpu\/riscv\/riscv_b.ad","additions":57,"deletions":0,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -4,1 +4,1 @@\n- * Copyright (c) 2020, 2021, Huawei Technologies Co., Ltd. All rights reserved.\n+ * Copyright (c) 2020, 2022, Huawei Technologies Co., Ltd. All rights reserved.\n@@ -300,1 +300,1 @@\n-  __ grevw(x10, x10);\n+  __ revb_w_w(x10, x10);\n@@ -682,1 +682,1 @@\n-  __ grevhu(reg, reg); \/\/ reverse bytes in half-word and zero-extend\n+  __ revb_h_h_u(reg, reg); \/\/ reverse bytes in half-word and zero-extend\n@@ -696,1 +696,1 @@\n-  __ grevhu(x11, x11); \/\/ reverse bytes in half-word and zero-extend\n+  __ revb_h_h_u(x11, x11); \/\/ reverse bytes in half-word and zero-extend\n@@ -713,1 +713,1 @@\n-  __ grevhu(x11, x11); \/\/ reverse bytes in half-word and zero-extend\n+  __ revb_h_h_u(x11, x11); \/\/ reverse bytes in half-word and zero-extend\n@@ -1573,1 +1573,1 @@\n-  __ grev16wu(x11, x11); \/\/ reverse bytes in half-word (32bit) and zero-extend\n+  __ revb_h_w_u(x11, x11); \/\/ reverse bytes in half-word (32bit) and zero-extend\n@@ -1733,1 +1733,1 @@\n-    __ grevh(x12, x12); \/\/ reverse bytes in half-word and sign-extend\n+    __ revb_h_h(x12, x12); \/\/ reverse bytes in half-word and sign-extend\n@@ -1736,1 +1736,1 @@\n-    __ grevw(x12, x12); \/\/ reverse bytes in word and sign-extend\n+    __ revb_w_w(x12, x12); \/\/ reverse bytes in word and sign-extend\n@@ -2011,2 +2011,2 @@\n-  __ grevw(x12, x12); \/\/ reverse bytes in word (32bit) and sign-extend\n-  __ grevw(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(x12, x12); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n@@ -2024,1 +2024,1 @@\n-  __ grevw(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n@@ -2044,1 +2044,1 @@\n-  __ grevw(x10, x10); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(x10, x10); \/\/ reverse bytes in word (32bit) and sign-extend\n@@ -2052,1 +2052,1 @@\n-  __ grev32(x11, x11);\n+  __ revb_w(x11, x11);\n@@ -2075,1 +2075,1 @@\n-  __ grevw(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(x13, x13); \/\/ reverse bytes in word (32bit) and sign-extend\n@@ -2128,1 +2128,1 @@\n-  __ grev32(j, j);\n+  __ revb_w(j, j);\n@@ -2147,1 +2147,1 @@\n-    __ grevw(temp, temp); \/\/ reverse bytes in word (32bit) and sign-extend\n+    __ revb_w_w(temp, temp); \/\/ reverse bytes in word (32bit) and sign-extend\n@@ -2171,1 +2171,1 @@\n-  __ grevw(temp, temp); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(temp, temp); \/\/ reverse bytes in word (32bit) and sign-extend\n@@ -2179,1 +2179,1 @@\n-  __ grevw(j, j); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(j, j); \/\/ reverse bytes in word (32bit) and sign-extend\n@@ -2192,1 +2192,1 @@\n-  __ grevw(j, j); \/\/ reverse bytes in word (32bit) and sign-extend\n+  __ revb_w_w(j, j); \/\/ reverse bytes in word (32bit) and sign-extend\n","filename":"src\/hotspot\/cpu\/riscv\/templateTable_riscv.cpp","additions":19,"deletions":19,"binary":false,"changes":38,"status":"modified"}]}