{"files":[{"patch":"@@ -502,3 +502,5 @@\n-  if (::munmap(addr, length) != 0) {\n-    perror(\"::munmap\");\n-    return false;\n+  while (::munmap(addr, length) != 0) {\n+    if (errno != EINTR) {\n+      perror(\"::munmap\");\n+      return false;\n+    }\n@@ -510,1 +512,1 @@\n-  if (::mmap(addr, length, PROT_READ | PROT_WRITE | (executable ? PROT_EXEC : 0),\n+  while (::mmap(addr, length, PROT_READ | PROT_WRITE | (executable ? PROT_EXEC : 0),\n@@ -512,2 +514,4 @@\n-    fprintf(stderr, \"::mmap %p %zu RW: %m\\n\", addr, length);\n-    return false;\n+    if (errno != EINTR) {\n+      fprintf(stderr, \"::mmap %p %zu RW: %m\\n\", addr, length);\n+      return false;\n+    }\n@@ -519,3 +523,5 @@\n-  if (::mmap(addr, length, PROT_NONE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0) != addr) {\n-    perror(\"::mmap NONE\");\n-    return false;\n+  while (::mmap(addr, length, PROT_NONE, MAP_PRIVATE | MAP_FIXED | MAP_ANONYMOUS, -1, 0) != addr) {\n+    if (errno != EINTR) {\n+      perror(\"::mmap NONE\");\n+      return false;\n+    }\n@@ -550,1 +556,1 @@\n-    if (syscall(SYS_rseq, rseqc->rseq_abi_pointer, rseqc->rseq_abi_size, RSEQ_FLAG_UNREGISTER, rseqc->signature)) {\n+    if (syscall(SYS_rseq, (void *) rseqc->rseq_abi_pointer, rseqc->rseq_abi_size, RSEQ_FLAG_UNREGISTER, rseqc->signature)) {\n@@ -584,1 +590,1 @@\n-    \"mov (%%ecx), %%esi\\n\\t\"\n+    \"mov (%%ebx), %%esi\\n\\t\"\n@@ -631,1 +637,1 @@\n-    if (syscall(SYS_rseq, rseqc->rseq_abi_pointer, rseqc->rseq_abi_size, 0, rseqc->signature) != 0) {\n+    if (syscall(SYS_rseq, (void *) rseqc->rseq_abi_pointer, rseqc->rseq_abi_size, 0, rseqc->signature) != 0) {\n@@ -635,0 +641,8 @@\n+\n+  \/\/ We cannot release this in after_threads_restored(), have to wait\n+  \/\/ until the last thread restores\n+  int dec = Atomic::sub(&persist_waiters, 1);\n+  if (dec == 0) {\n+    FREE_C_HEAP_ARRAY(struct __ptrace_rseq_configuration, rseq_configs);\n+    rseq_configs = nullptr;\n+  }\n@@ -759,4 +773,0 @@\n-#ifdef HAS_RSEQ\n-  FREE_C_HEAP_ARRAY(struct __ptrace_rseq_configuration, rseq_configs);\n-  rseq_configs = nullptr;\n-#endif\n","filename":"src\/hotspot\/os\/linux\/crac_linux.cpp","additions":26,"deletions":16,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -1328,2 +1328,3 @@\n-  void on_restore() override {\n-  #ifdef ASSERT\n+\n+#ifdef ASSERT\n+  void assert_checkpoint() override {\n@@ -1332,1 +1333,4 @@\n-  #endif \/\/ ASSERT\n+  }\n+#endif \/\/ ASSERT\n+\n+  void on_restore() override {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.hpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -628,0 +628,6 @@\n+#ifdef ASSERT\n+  void assert_checkpoint() {\n+    _global_mark_stack.assert_checkpoint();\n+  }\n+#endif\n+\n@@ -629,1 +635,0 @@\n-    DEBUG_ONLY(_global_mark_stack.assert_checkpoint();)\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ConcurrentMark.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -548,0 +548,3 @@\n+#ifdef ASSERT\n+  virtual void assert_checkpoint() {}\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -46,2 +46,0 @@\n-#include <malloc.h>\n-\n@@ -398,1 +396,1 @@\n-  malloc_trim(0);\n+  os::trim_native_heap(nullptr);\n@@ -415,0 +413,1 @@\n+        crac::MemoryPersister::reinit_memory();\n@@ -426,0 +425,7 @@\n+  if (CRPersistMemory) {\n+    \/\/ Before reinit_memory the code must not change memory layout, e.g. mmapping\n+    \/\/ or even malloc'ing anything (malloc running out of space could run short and allocate\n+    \/\/ new regions).\n+    crac::MemoryPersister::reinit_memory();\n+  }\n+\n@@ -436,1 +442,0 @@\n-    Universe::heap()->on_restore();\n@@ -445,0 +450,1 @@\n+    Universe::heap()->assert_checkpoint();\n@@ -447,0 +453,1 @@\n+    Universe::heap()->on_restore();\n@@ -608,0 +615,1 @@\n+  guarantee(sizeof(header) == hdr->self_size, \"Invalid header: restoring 32 bit image with 64 bit JVM or vice versa?\");\n@@ -700,1 +708,2 @@\n-    ssize_t r = ::read(fd, dest + rd, n - rd);\n+    \/\/ without the explicit cast it fails the build on Windows\n+    ssize_t r = (ssize_t) ::read(fd, dest + rd, n - rd);\n","filename":"src\/hotspot\/share\/runtime\/crac.cpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -110,3 +110,3 @@\n-      u_int64_t addr;\n-      u_int64_t length;\n-      u_int64_t offset;\n+      address addr;\n+      size_t length;\n+      size_t offset;\n@@ -138,0 +138,5 @@\n+    \/\/ This method mmaps all memory as non-accessible without loading the data;\n+    \/\/ the purpose is to do this early (e.g. before reading new parameters)\n+    \/\/ to prevent other malloc or other code from accidentally mapping memory\n+    \/\/ in conflicting range.\n+    static void reinit_memory();\n","filename":"src\/hotspot\/share\/runtime\/crac.hpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"memory\/resourceArea.hpp\"\n@@ -98,1 +99,1 @@\n-  unsigned long long *end = (unsigned long long *)((char *) addr + page_size);\n+  unsigned long long *end = (unsigned long long *)((address) addr + page_size);\n@@ -109,1 +110,1 @@\n-  assert(((u_int64_t) addr & (page_size - 1)) == 0, \"Unaligned address %p\", addr);\n+  assert(is_aligned(addr, page_size), \"Unaligned address %p\", addr);\n@@ -111,1 +112,1 @@\n-  assert((mapped_length & (page_size - 1)) == 0, \"Unaligned length %zx at %p (page size %zx)\", mapped_length, addr, page_size);\n+  assert(is_aligned(mapped_length, page_size), \"Unaligned length %zx at %p (page size %zx)\", mapped_length, addr, page_size);\n@@ -114,2 +115,2 @@\n-  char *curr = (char *) addr;\n-  char *end = curr + length;\n+  address curr = (address) addr;\n+  address end = curr + length;\n@@ -118,1 +119,1 @@\n-    char *start = curr;\n+    address start = curr;\n@@ -124,2 +125,2 @@\n-        .addr = (u_int64_t) start,\n-        .length = (u_int64_t) (curr - start),\n+        .addr = start,\n+        .length = (size_t) (curr - start),\n@@ -137,3 +138,3 @@\n-        .addr = (u_int64_t) start,\n-        .length = (u_int64_t) to_write,\n-        .offset = (u_int64_t) offset,\n+        .addr = start,\n+        .length = to_write,\n+        .offset = offset,\n@@ -149,2 +150,2 @@\n-      .addr = (u_int64_t) addr + aligned_length,\n-      .length = (u_int64_t) mapped_length - aligned_length,\n+      .addr = (address) addr + aligned_length,\n+      .length = mapped_length - aligned_length,\n@@ -159,2 +160,2 @@\n-  assert(((u_int64_t) addr & (os::vm_page_size() - 1)) == 0, \"Unaligned address\");\n-  assert((length & (os::vm_page_size() - 1)) == 0, \"Unaligned length\");\n+  assert(is_aligned(addr, os::vm_page_size()), \"Unaligned address\");\n+  assert(is_aligned(length, os::vm_page_size()), \"Unaligned length\");\n@@ -164,0 +165,7 @@\n+#ifdef ASSERT\n+  for (int i = 0; i < _index.length(); ++i) {\n+    const struct record &r = _index.at(i);\n+    assert((address) addr + length <= r.addr || r.addr + r.length <= addr,\n+      \"Overlapping regions %p-%p and %p-%p\", r.addr, r.addr + r.length, addr, (address) addr + length);\n+  }\n+#endif\n@@ -165,2 +173,2 @@\n-    .addr = (u_int64_t) addr,\n-    .length = (u_int64_t) length,\n+    .addr = (address) addr,\n+    .length = length,\n@@ -173,0 +181,11 @@\n+void crac::MemoryPersister::reinit_memory() {\n+  size_t page_size = os::vm_page_size();\n+  for (int i = 0; i < _index.length(); ++i) {\n+    const record &r = _index.at(i);\n+    size_t aligned_length = align_up(r.length, page_size);\n+    if (!map_gap(r.addr, aligned_length)) {\n+      fatal(\"Cannot reinit non-accessible memory at %p-%p\", r.addr, r.addr + aligned_length);\n+    }\n+  }\n+}\n+\n@@ -175,1 +194,1 @@\n-  bool should_map = false;\n+  bool update_protection = false;\n@@ -181,1 +200,1 @@\n-    should_map = true;\n+    update_protection = true;\n@@ -187,1 +206,1 @@\n-    const struct record &r = _index.at(i);\n+    const record &r = _index.at(i);\n@@ -192,2 +211,2 @@\n-        if (!map((void *) r.addr, aligned_length, executable)) {\n-          fatal(\"Cannot remap memory at %p-%p\", (void *) r.addr, (void *)(r.addr + aligned_length));\n+        if (!map(r.addr, aligned_length, executable)) {\n+          fatal(\"Cannot remap memory at %p-%p\", r.addr, r.addr + aligned_length);\n@@ -197,2 +216,3 @@\n-        if (should_map && !map(data, aligned_length, executable)) {\n-          fatal(\"Cannot remap memory at %p-%p\", (void *) r.addr, (void *)(r.addr + aligned_length));\n+        if (update_protection && !os::protect_memory(data, aligned_length,\n+            executable ? os::ProtType::MEM_PROT_RWX : os::ProtType::MEM_PROT_RW)) {\n+          fatal(\"Cannot remap memory at %p-%p\", r.addr, r.addr + aligned_length);\n@@ -202,5 +222,0 @@\n-    } else {\n-      \/\/ In case of RestoreMemoryNoWait the gaps are already mapped in init_userfault()\n-      if (!map_gap((void *) r.addr, aligned_length)) {\n-        fatal(\"Cannot remap non-accessible memory at %p-%p\", (void *) r.addr, (void *)(r.addr + aligned_length));\n-      }\n@@ -212,5 +227,0 @@\n-#ifdef _LP64\n-# define FMT64X \"%lx\"\n-#else\n-# define FMT64X \"%llx\"\n-#endif\n@@ -219,1 +229,0 @@\n-\n@@ -221,2 +230,2 @@\n-  assert(((u_int64_t) addr & (os::vm_page_size() - 1)) == 0, \"Unaligned address %p\", addr);\n-  assert((total & (os::vm_page_size() - 1)) == 0, \"Unaligned length %zx\", total);\n+  assert(is_aligned(addr, os::vm_page_size()), \"Unaligned address %p\", addr);\n+  assert(is_aligned(total, os::vm_page_size()), \"Unaligned length %zx\", total);\n@@ -230,1 +239,1 @@\n-  int at = _index.find_sorted<struct record>(&comparator, { .addr = (u_int64_t) addr }, found);\n+  int at = _index.find_sorted<struct record>(&comparator, { .addr = (address) addr }, found);\n@@ -236,3 +245,3 @@\n-    assert((void   *) r.addr == addr, \"Unexpected address \" FMT64X \", expected %p\", r.addr, addr);\n-    assert(r.flags & Flags::ACCESSIBLE, \"Bad flags for \" FMT64X \": 0x%x\", r.addr, r.flags);\n-    assert(r.length <= used, \"Persisted memory region length does not match at %p: %zu vs. \" FMT64X, addr, used, r.length);\n+    assert((void   *) r.addr == addr, \"Unexpected address %p, expected %p\", r.addr, addr);\n+    assert(r.flags & Flags::ACCESSIBLE, \"Bad flags for %p: 0x%x\", r.addr, r.flags);\n+    assert(r.length <= used, \"Persisted memory region length does not match at %p: 0x%zx vs. 0x%zx\", addr, used, r.length);\n@@ -240,1 +249,1 @@\n-      assert(r.offset != BAD_OFFSET, \"Invalid offset at \" FMT64X, r.addr);\n+      assert(r.offset != BAD_OFFSET, \"Invalid offset at %p\", r.addr);\n@@ -242,1 +251,1 @@\n-      assert(r.offset == BAD_OFFSET, \"Invalid offset at \" FMT64X \": \" FMT64X, r.addr, r.offset);\n+      assert(r.offset == BAD_OFFSET, \"Invalid offset at %p: 0x%zx\", r.addr, r.offset);\n@@ -250,2 +259,2 @@\n-    assert((void *) g.addr == gap_addr, \"Invalid address for the gap region: \" FMT64X \" vs. %p\", g.addr, gap_addr);\n-    assert(g.length == unused, \"Persisted gap length does not match at %p: %zu vs. \" FMT64X, gap_addr, unused, g.length);\n+    assert(g.addr == gap_addr, \"Invalid address for the gap region: %p vs. %p\", g.addr, gap_addr);\n+    assert(g.length == unused, \"Persisted gap length does not match at %p: 0x%zx vs. 0x%zx\", gap_addr, unused, g.length);\n@@ -253,1 +262,1 @@\n-    assert(g.offset == BAD_OFFSET, \"Invalid offset at %p: \" FMT64X, gap_addr, g.offset);\n+    assert(g.offset == BAD_OFFSET, \"Invalid offset at %p: 0x%zx\", gap_addr, g.offset);\n@@ -258,2 +267,2 @@\n-  assert(((u_int64_t) addr & (os::vm_page_size() - 1)) == 0, \"Unaligned address %p\", addr);\n-  assert((length & (os::vm_page_size() - 1)) == 0, \"Unaligned length %zx\", length);\n+  assert(is_aligned(addr, os::vm_page_size()), \"Unaligned address %p\", addr);\n+  assert(is_aligned(length, os::vm_page_size()), \"Unaligned length 0x%zx\", length);\n@@ -263,1 +272,1 @@\n-    int at = _index.find_sorted<struct record>(&comparator, { .addr = (u_int64_t) addr }, found);\n+    int at = _index.find_sorted<struct record>(&comparator, { .addr = (address) addr }, found);\n@@ -266,1 +275,1 @@\n-    assert(r.length == length, \"Persisted memory region length does not match at %p: %zu vs. \" FMT64X, addr, length, r.length);\n+    assert(r.length == length, \"Persisted memory region length does not match at %p: 0x%zx vs. 0x%zx\", addr, length, r.length);\n@@ -268,1 +277,1 @@\n-    assert(r.offset == BAD_OFFSET, \"Invalid offset at %p: \" FMT64X, addr, r.offset);\n+    assert(r.offset == BAD_OFFSET, \"Invalid offset at %p: 0x%zx\", addr, r.offset);\n","filename":"src\/hotspot\/share\/runtime\/crac_memory.cpp","additions":60,"deletions":51,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -58,0 +58,1 @@\n+    uint32_t self_size;\n@@ -119,0 +120,1 @@\n+      (uint32_t) sizeof(struct header),\n","filename":"src\/hotspot\/share\/runtime\/crac_structs.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"}]}