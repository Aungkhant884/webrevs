{"files":[{"patch":"@@ -38,1 +38,1 @@\n-  assert (FKind::is_instance(f), \"\");\n+  assert(FKind::is_instance(f), \"\");\n@@ -117,1 +117,1 @@\n-  assert (FKind::is_instance(f), \"\");\n+  assert(FKind::is_instance(f), \"\");\n@@ -138,2 +138,2 @@\n-  assert (FKind::is_instance(f), \"\");\n-  assert (!caller.is_interpreted_frame()\n+  assert(FKind::is_instance(f), \"\");\n+  assert(!caller.is_interpreted_frame()\n@@ -144,1 +144,1 @@\n-    assert ((intptr_t*)f.at(frame::interpreter_frame_last_sp_offset) == nullptr\n+    assert((intptr_t*)f.at(frame::interpreter_frame_last_sp_offset) == nullptr\n@@ -150,1 +150,1 @@\n-    assert (sp <= fp && fp <= caller.unextended_sp(), \"\");\n+    assert(sp <= fp && fp <= caller.unextended_sp(), \"\");\n@@ -153,1 +153,1 @@\n-    assert (_cont.tail()->is_in_chunk(sp), \"\");\n+    assert(_cont.tail()->is_in_chunk(sp), \"\");\n@@ -168,1 +168,1 @@\n-    assert (_cont.tail()->is_in_chunk(sp), \"\");\n+    assert(_cont.tail()->is_in_chunk(sp), \"\");\n@@ -175,1 +175,1 @@\n-  assert (*(hfp + offset) == *(vfp + offset), \"\");\n+  assert(*(hfp + offset) == *(vfp + offset), \"\");\n@@ -184,2 +184,2 @@\n-  assert (hfp == hf.unextended_sp() + (f.fp() - f.unextended_sp()), \"\");\n-  assert ((f.at(frame::interpreter_frame_last_sp_offset) != 0)\n+  assert(hfp == hf.unextended_sp() + (f.fp() - f.unextended_sp()), \"\");\n+  assert((f.at(frame::interpreter_frame_last_sp_offset) != 0)\n@@ -187,1 +187,1 @@\n-  assert (f.fp() > (intptr_t*)f.at(frame::interpreter_frame_initial_sp_offset), \"\");\n+  assert(f.fp() > (intptr_t*)f.at(frame::interpreter_frame_initial_sp_offset), \"\");\n@@ -200,5 +200,5 @@\n-  assert ((hf.fp() - hf.unextended_sp()) == (f.fp() - f.unextended_sp()), \"\");\n-  assert (hf.unextended_sp() == (intptr_t*)hf.at(frame::interpreter_frame_last_sp_offset), \"\");\n-  assert (hf.unextended_sp() <= (intptr_t*)hf.at(frame::interpreter_frame_initial_sp_offset), \"\");\n-  assert (hf.fp()            >  (intptr_t*)hf.at(frame::interpreter_frame_initial_sp_offset), \"\");\n-  assert (hf.fp()            <= (intptr_t*)hf.at(frame::interpreter_frame_locals_offset), \"\");\n+  assert((hf.fp() - hf.unextended_sp()) == (f.fp() - f.unextended_sp()), \"\");\n+  assert(hf.unextended_sp() == (intptr_t*)hf.at(frame::interpreter_frame_last_sp_offset), \"\");\n+  assert(hf.unextended_sp() <= (intptr_t*)hf.at(frame::interpreter_frame_initial_sp_offset), \"\");\n+  assert(hf.fp()            >  (intptr_t*)hf.at(frame::interpreter_frame_initial_sp_offset), \"\");\n+  assert(hf.fp()            <= (intptr_t*)hf.at(frame::interpreter_frame_locals_offset), \"\");\n@@ -209,2 +209,2 @@\n-  assert (chunk->is_in_chunk(hf.sp() - 1), \"\");\n-  assert (chunk->is_in_chunk(hf.sp() - frame::sender_sp_offset), \"\");\n+  assert(chunk->is_in_chunk(hf.sp() - 1), \"\");\n+  assert(chunk->is_in_chunk(hf.sp() - frame::sender_sp_offset), \"\");\n@@ -221,1 +221,1 @@\n-    assert (!caller.is_empty(), \"\");\n+    assert(!caller.is_empty(), \"\");\n@@ -251,1 +251,1 @@\n-  assert (FKind::is_instance(hf), \"\");\n+  assert(FKind::is_instance(hf), \"\");\n@@ -264,1 +264,1 @@\n-    assert (vsp == unextended_sp, \"\");\n+    assert(vsp == unextended_sp, \"\");\n@@ -269,1 +269,1 @@\n-    assert ((int)offset == locals + frame::sender_sp_offset - 1, \"\");\n+    assert((int)offset == locals + frame::sender_sp_offset - 1, \"\");\n@@ -271,1 +271,1 @@\n-    assert ((intptr_t)f.fp() % 16 == 0, \"\");\n+    assert((intptr_t)f.fp() % 16 == 0, \"\");\n@@ -282,1 +282,1 @@\n-      assert (caller.sp() == vsp + (fsize-argsize), \"\");\n+      assert(caller.sp() == vsp + (fsize-argsize), \"\");\n@@ -287,1 +287,1 @@\n-    assert (hf.cb() != nullptr && hf.oop_map() != nullptr, \"\");\n+    assert(hf.cb() != nullptr && hf.oop_map() != nullptr, \"\");\n","filename":"src\/hotspot\/cpu\/aarch64\/continuation_aarch64.inline.hpp","additions":26,"deletions":26,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -34,2 +34,2 @@\n-  assert (f.raw_pc() == pc, \"f.ra_pc: \" INTPTR_FORMAT \" actual: \" INTPTR_FORMAT, p2i(f.raw_pc()), p2i(pc));\n-  assert (f.fp() == fp, \"f.fp: \" INTPTR_FORMAT \" actual: \" INTPTR_FORMAT, p2i(f.fp()), p2i(fp));\n+  assert(f.raw_pc() == pc, \"f.ra_pc: \" INTPTR_FORMAT \" actual: \" INTPTR_FORMAT, p2i(f.raw_pc()), p2i(pc));\n+  assert(f.fp() == fp, \"f.fp: \" INTPTR_FORMAT \" actual: \" INTPTR_FORMAT, p2i(f.fp()), p2i(fp));\n@@ -53,1 +53,1 @@\n-  assert (f.is_interpreted_frame(), \"\");\n+  assert(f.is_interpreted_frame(), \"\");\n@@ -77,2 +77,2 @@\n-  assert (res == (intptr_t*)f.interpreter_frame_monitor_end() - expression_stack_sz, \"\");\n-  assert (res >= f.unextended_sp(),\n+  assert(res == (intptr_t*)f.interpreter_frame_monitor_end() - expression_stack_sz, \"\");\n+  assert(res >= f.unextended_sp(),\n@@ -83,1 +83,1 @@\n-  \/\/ assert (res == f.unextended_sp(), \"res: \" INTPTR_FORMAT \" unextended_sp: \" INTPTR_FORMAT, p2i(res), p2i(f.unextended_sp() + 1));\n+  \/\/ assert(res == f.unextended_sp(), \"res: \" INTPTR_FORMAT \" unextended_sp: \" INTPTR_FORMAT, p2i(res), p2i(f.unextended_sp() + 1));\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_helpers_aarch64.inline.hpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-  assert (!is_done(), \"\");\n+  assert(!is_done(), \"\");\n@@ -54,1 +54,1 @@\n-  assert (!is_done(), \"\");\n+  assert(!is_done(), \"\");\n@@ -69,1 +69,1 @@\n-  assert (fp != nullptr, \"\");\n+  assert(fp != nullptr, \"\");\n@@ -75,1 +75,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -81,1 +81,1 @@\n-\/\/   assert (frame_kind == chunk_frames::MIXED, \"\");\n+\/\/   assert(frame_kind == chunk_frames::MIXED, \"\");\n@@ -84,1 +84,1 @@\n-\/\/   assert (unextended_sp > callee_fp && unextended_sp >= sp(), \"callee_fp: %p (%d) offset: %ld\", callee_fp, _chunk->to_offset(callee_fp), callee_fp[frame::interpreter_frame_sender_sp_offset]);\n+\/\/   assert(unextended_sp > callee_fp && unextended_sp >= sp(), \"callee_fp: %p (%d) offset: %ld\", callee_fp, _chunk->to_offset(callee_fp), callee_fp[frame::interpreter_frame_sender_sp_offset]);\n@@ -90,1 +90,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -96,1 +96,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -109,1 +109,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -121,1 +121,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -128,1 +128,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n","filename":"src\/hotspot\/cpu\/aarch64\/stackChunkFrameStream_aarch64.inline.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -37,1 +37,1 @@\n-                                     DEBUG_ONLY({ assert (r == rbp->as_VMReg() || r == rbp->as_VMReg()->next(), \"Reg: %s\", r->name()); })\n+                                     DEBUG_ONLY({ assert(r == rbp->as_VMReg() || r == rbp->as_VMReg()->next(), \"Reg: %s\", r->name()); })\n","filename":"src\/hotspot\/cpu\/x86\/smallRegisterMap_x86.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,1 +35,1 @@\n-  assert (!is_done(), \"\");\n+  assert(!is_done(), \"\");\n@@ -54,1 +54,1 @@\n-  assert (!is_done(), \"\");\n+  assert(!is_done(), \"\");\n@@ -69,1 +69,1 @@\n-  assert (fp != nullptr, \"\");\n+  assert(fp != nullptr, \"\");\n@@ -75,1 +75,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -81,1 +81,1 @@\n-\/\/   assert (frame_kind == chunk_frames::MIXED, \"\");\n+\/\/   assert(frame_kind == chunk_frames::MIXED, \"\");\n@@ -84,1 +84,1 @@\n-\/\/   assert (unextended_sp > callee_fp && unextended_sp >= sp(), \"callee_fp: %p (%d) offset: %ld\", callee_fp, _chunk->to_offset(callee_fp), callee_fp[frame::interpreter_frame_sender_sp_offset]);\n+\/\/   assert(unextended_sp > callee_fp && unextended_sp >= sp(), \"callee_fp: %p (%d) offset: %ld\", callee_fp, _chunk->to_offset(callee_fp), callee_fp[frame::interpreter_frame_sender_sp_offset]);\n@@ -90,1 +90,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -96,1 +96,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -109,1 +109,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -121,1 +121,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n@@ -128,1 +128,1 @@\n-  assert (frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n+  assert(frame_kind == chunk_frames::MIXED && is_interpreted(), \"\");\n","filename":"src\/hotspot\/cpu\/x86\/stackChunkFrameStream_x86.inline.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -33,1 +33,4 @@\n-  static void relativize_chunk(oop obj);\n+  \/\/ Relativize the given oop if it is a stack chunk.\n+  static void relativize_stack_chunk(oop obj);\n+  \/\/ Relativize and transform to use a bitmap for future oop iteration for the\n+  \/\/ given oop if it is a stack chunk.\n","filename":"src\/hotspot\/share\/gc\/shared\/continuationGCSupport.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-inline void ContinuationGCSupport::relativize_chunk(oop obj) {\n+inline void ContinuationGCSupport::relativize_stack_chunk(oop obj) {\n@@ -39,1 +39,3 @@\n-  InstanceStackChunkKlass::relativize_chunk(chunk);\n+  if (!chunk->is_gc_mode()) {\n+    chunk->relativize();\n+  }\n@@ -47,2 +49,2 @@\n-  if (!chunk->has_bitmap()) {\n-    InstanceStackChunkKlass::build_bitmap(chunk);\n+  if (!chunk->is_gc_mode()) {\n+    chunk->transform();\n","filename":"src\/hotspot\/share\/gc\/shared\/continuationGCSupport.inline.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -283,1 +283,1 @@\n-  ContinuationGCSupport::relativize_chunk(obj);\n+  ContinuationGCSupport::relativize_stack_chunk(obj);\n","filename":"src\/hotspot\/share\/gc\/z\/zMark.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -92,46 +92,0 @@\n-\/\/ We replace derived pointers with offsets; the converse is done in DerelativizeDerivedPointers\n-class RelativizeDerivedPointers : public DerivedOopClosure {\n-public:\n-  virtual void do_derived_oop(oop* base_loc, derived_pointer* derived_loc) override {\n-    \/\/ The ordering in the following is crucial\n-    OrderAccess::loadload();\n-    oop base = Atomic::load((oop*)base_loc);\n-    if (base == nullptr) {\n-      assert(*derived_loc == derived_pointer(0), \"\");\n-      return;\n-    }\n-    assert(!CompressedOops::is_base(base), \"\");\n-\n-#if INCLUDE_ZGC\n-    if (UseZGC) {\n-      if (ZAddress::is_good(cast_from_oop<uintptr_t>(base))) {\n-        return;\n-      }\n-    }\n-#endif\n-#if INCLUDE_SHENANDOAHGC\n-    if (UseShenandoahGC) {\n-      if (!ShenandoahHeap::heap()->in_collection_set(base)) {\n-        return;\n-      }\n-    }\n-#endif\n-\n-    OrderAccess::loadload();\n-    intptr_t derived_int_val = Atomic::load((intptr_t*)derived_loc);\n-    if (derived_int_val <= 0) {\n-      return;\n-    }\n-\n-    \/\/ at this point, we've seen a non-offset value *after* we've read the base, but we write the offset *before* fixing the base,\n-    \/\/ so we are guaranteed that the value in derived_loc is consistent with base (i.e. points into the object).\n-    intptr_t offset = derived_int_val - cast_from_oop<intptr_t>(base);\n-    if (offset < 0) {\n-      \/\/ It looks as if a derived pointer appears live in the oopMap but isn't pointing into the object.\n-      \/\/ This might be the result of address computation floating above corresponding range check for array access.\n-      offset = -1;\n-    }\n-    Atomic::store((intptr_t*)derived_loc, -offset);\n-  }\n-};\n-\n@@ -161,18 +115,0 @@\n-template <InstanceStackChunkKlass::barrier_type barrier, bool compressedOopsWithBitmap>\n-class BarrierClosure: public OopClosure {\n-  NOT_PRODUCT(intptr_t* _sp;)\n-\n-public:\n-  BarrierClosure(intptr_t* sp) NOT_PRODUCT(: _sp(sp)) {}\n-\n-  virtual void do_oop(oop* p)       override { compressedOopsWithBitmap ? do_oop_work((narrowOop*)p) : do_oop_work(p); }\n-  virtual void do_oop(narrowOop* p) override { do_oop_work(p); }\n-\n-  template <class T> inline void do_oop_work(T* p) {\n-    oop value = (oop)HeapAccess<>::oop_load(p);\n-    if (barrier == InstanceStackChunkKlass::barrier_type::STORE) {\n-      HeapAccess<>::oop_store(p, value);\n-    }\n-  }\n-};\n-\n@@ -232,1 +168,1 @@\n-  iterate_stack(chunk, &closure);\n+  chunk->iterate_stack(&closure);\n@@ -270,1 +206,1 @@\n-  iterate_stack(chunk, &frame_closure);\n+  chunk->iterate_stack(&frame_closure);\n@@ -280,105 +216,0 @@\n-template <chunk_frames frame_kind, typename RegisterMapT>\n-void InstanceStackChunkKlass::relativize_derived_pointers(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n-  RelativizeDerivedPointers derived_closure;\n-  f.iterate_derived_pointers(&derived_closure, map);\n-}\n-\n-template void InstanceStackChunkKlass::relativize_derived_pointers<>(const StackChunkFrameStream<chunk_frames::MIXED>& f, const RegisterMap* map);\n-template void InstanceStackChunkKlass::relativize_derived_pointers<>(const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const RegisterMap* map);\n-template void InstanceStackChunkKlass::relativize_derived_pointers<>(const StackChunkFrameStream<chunk_frames::MIXED>& f, const SmallRegisterMap* map);\n-template void InstanceStackChunkKlass::relativize_derived_pointers<>(const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const SmallRegisterMap* map);\n-\n-template <InstanceStackChunkKlass::barrier_type barrier, chunk_frames frame_kind, typename RegisterMapT>\n-void InstanceStackChunkKlass::do_barriers0(stackChunkOop chunk, const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n-  \/\/ we need to invoke the write barriers so as not to miss oops in old chunks that haven't yet been concurrently scanned\n-  if (f.is_done()) {\n-    return;\n-  }\n-\n-  if (f.is_interpreted()) {\n-    Method* m = f.to_frame().interpreter_frame_method();\n-    m->record_marking_cycle();\n-  } else if (f.is_compiled()) {\n-    nmethod* nm = f.cb()->as_nmethod();\n-    \/\/ The entry barrier takes care of having the right synchronization\n-    \/\/ when keeping the nmethod alive during concurrent execution.\n-    nm->run_nmethod_entry_barrier();\n-    \/\/ there's no need to mark the Method, as class redefinition will walk the CodeCache, noting their Methods\n-  }\n-\n-  assert(!f.is_compiled() || f.oopmap()->has_derived_oops() == f.oopmap()->has_any(OopMapValue::derived_oop_value), \"\");\n-  bool has_derived = f.is_compiled() && f.oopmap()->has_derived_oops();\n-  if (has_derived) {\n-    relativize_derived_pointers(f, map);\n-  }\n-\n-  if (chunk->has_bitmap() && UseCompressedOops) {\n-    BarrierClosure<barrier, true> oops_closure(f.sp());\n-    f.iterate_oops(&oops_closure, map);\n-  } else {\n-    BarrierClosure<barrier, false> oops_closure(f.sp());\n-    f.iterate_oops(&oops_closure, map);\n-  }\n-  OrderAccess::loadload(); \/\/ observing the barriers will prevent derived pointers from being derelativized concurrently\n-}\n-\n-template void InstanceStackChunkKlass::do_barriers0<InstanceStackChunkKlass::barrier_type::LOAD> (stackChunkOop chunk, const StackChunkFrameStream<chunk_frames::MIXED>& f, const RegisterMap* map);\n-template void InstanceStackChunkKlass::do_barriers0<InstanceStackChunkKlass::barrier_type::STORE>(stackChunkOop chunk, const StackChunkFrameStream<chunk_frames::MIXED>& f, const RegisterMap* map);\n-template void InstanceStackChunkKlass::do_barriers0<InstanceStackChunkKlass::barrier_type::LOAD> (stackChunkOop chunk, const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const RegisterMap* map);\n-template void InstanceStackChunkKlass::do_barriers0<InstanceStackChunkKlass::barrier_type::STORE>(stackChunkOop chunk, const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const RegisterMap* map);\n-template void InstanceStackChunkKlass::do_barriers0<InstanceStackChunkKlass::barrier_type::LOAD> (stackChunkOop chunk, const StackChunkFrameStream<chunk_frames::MIXED>& f, const SmallRegisterMap* map);\n-template void InstanceStackChunkKlass::do_barriers0<InstanceStackChunkKlass::barrier_type::STORE>(stackChunkOop chunk, const StackChunkFrameStream<chunk_frames::MIXED>& f, const SmallRegisterMap* map);\n-template void InstanceStackChunkKlass::do_barriers0<InstanceStackChunkKlass::barrier_type::LOAD> (stackChunkOop chunk, const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const SmallRegisterMap* map);\n-template void InstanceStackChunkKlass::do_barriers0<InstanceStackChunkKlass::barrier_type::STORE>(stackChunkOop chunk, const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const SmallRegisterMap* map);\n-\n-template <InstanceStackChunkKlass::barrier_type barrier>\n-class DoBarriersStackClosure {\n-  const stackChunkOop _chunk;\n-\n-public:\n-  DoBarriersStackClosure(stackChunkOop chunk) : _chunk(chunk) {}\n-\n-  template <chunk_frames frame_kind, typename RegisterMapT>\n-  bool do_frame(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n-    InstanceStackChunkKlass::do_barriers0<barrier>(_chunk, f, map);\n-    return true;\n-  }\n-};\n-\n-template <InstanceStackChunkKlass::barrier_type barrier>\n-void InstanceStackChunkKlass::do_barriers(stackChunkOop chunk) {\n-  DoBarriersStackClosure<barrier> closure(chunk);\n-  iterate_stack(chunk, &closure);\n-}\n-\n-class RelativizeStackClosure {\n-  const stackChunkOop _chunk;\n-\n-public:\n-  RelativizeStackClosure(stackChunkOop chunk) : _chunk(chunk) {}\n-\n-  template <chunk_frames frame_kind, typename RegisterMapT>\n-  bool do_frame(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n-    bool has_derived = f.is_compiled() && f.oopmap()->has_derived_oops();\n-    if (has_derived) {\n-      RelativizeDerivedPointers derived_closure;\n-      f.iterate_derived_pointers(&derived_closure, map);\n-    }\n-    return true;\n-  }\n-};\n-\n-void InstanceStackChunkKlass::relativize_chunk(stackChunkOop chunk) {\n-  if (chunk->is_gc_mode()) {\n-    \/\/ Already relativized\n-    return;\n-  }\n-  chunk->set_gc_mode(true);\n-  OrderAccess::storestore();\n-  RelativizeStackClosure closure(chunk);\n-  iterate_stack(chunk, &closure);\n-}\n-\n-template void InstanceStackChunkKlass::do_barriers<InstanceStackChunkKlass::barrier_type::LOAD> (stackChunkOop chunk);\n-template void InstanceStackChunkKlass::do_barriers<InstanceStackChunkKlass::barrier_type::STORE>(stackChunkOop chunk);\n-\n@@ -394,1 +225,1 @@\n-class FixCompressedOopClosure : public OopClosure {\n+class UncompressOopsOopClosure : public OopClosure {\n@@ -396,1 +227,1 @@\n-    assert(UseCompressedOops, \"\");\n+    assert(UseCompressedOops, \"Only needed with compressed oops\");\n@@ -405,79 +236,0 @@\n-enum class oop_kind { NARROW, WIDE };\n-\n-template <oop_kind oops>\n-class BuildBitmapOopClosure : public OopClosure {\n-  intptr_t* const _stack_start;\n-  const BitMap::idx_t _bit_offset;\n-  BitMapView _bm;\n-\n-public:\n-  BuildBitmapOopClosure(intptr_t* stack_start, BitMap::idx_t bit_offset, BitMapView bm)\n-    : _stack_start(stack_start), _bit_offset(bit_offset), _bm(bm) {}\n-\n-  virtual void do_oop(oop* p) override {\n-    assert(p >= (oop*)_stack_start, \"\");\n-    if (oops == oop_kind::NARROW) {\n-      \/\/ Convert all oops to narrow before marking bit\n-      oop obj = *p;\n-      *p = nullptr;\n-      \/\/ assuming little endian\n-      *(narrowOop*)p = CompressedOops::encode(obj);\n-      do_oop((narrowOop*)p);\n-    } else {\n-      BitMap::idx_t index = _bit_offset + (p - (oop*)_stack_start);\n-      assert(!_bm.at(index), \"\");\n-      _bm.set_bit(index);\n-    }\n-  }\n-\n-  virtual void do_oop(narrowOop* p) override {\n-    assert(p >= (narrowOop*)_stack_start, \"\");\n-    BitMap::idx_t index = _bit_offset + (p - (narrowOop*)_stack_start);\n-    assert(!_bm.at(index), \"\");\n-    _bm.set_bit(index);\n-  }\n-};\n-\n-template <oop_kind oops>\n-class BuildBitmapStackClosure {\n-  stackChunkOop _chunk;\n-  const BitMap::idx_t _bit_offset;\n-\n-public:\n-  BuildBitmapStackClosure(stackChunkOop chunk) : _chunk(chunk), _bit_offset(chunk->bit_offset()) {}\n-\n-  template <chunk_frames frame_kind, typename RegisterMapT>\n-  bool do_frame(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n-    if (!_chunk->is_gc_mode() && f.is_compiled() && f.oopmap()->has_derived_oops()) {\n-      RelativizeDerivedPointers derived_oops_closure;\n-      f.iterate_derived_pointers(&derived_oops_closure, map);\n-    }\n-\n-    if (UseChunkBitmaps) {\n-      BuildBitmapOopClosure<oops> oops_closure(_chunk->start_address(), _chunk->bit_offset(), _chunk->bitmap());\n-      f.iterate_oops(&oops_closure, map);\n-    }\n-\n-    return true;\n-  }\n-};\n-\n-void InstanceStackChunkKlass::build_bitmap(stackChunkOop chunk) {\n-  assert(!chunk->has_bitmap(), \"\");\n-  if (UseChunkBitmaps) {\n-    chunk->set_has_bitmap(true);\n-    BitMapView bm = chunk->bitmap();\n-    bm.clear();\n-  }\n-\n-  if (UseCompressedOops) {\n-    BuildBitmapStackClosure<oop_kind::NARROW> closure(chunk);\n-    iterate_stack(chunk, &closure);\n-  } else {\n-    BuildBitmapStackClosure<oop_kind::WIDE> closure(chunk);\n-    iterate_stack(chunk, &closure);\n-  }\n-\n-  chunk->set_gc_mode(true); \/\/ must be set *after* the above closure\n-}\n-\n@@ -487,1 +239,1 @@\n-    FixCompressedOopClosure oop_closure;\n+    UncompressOopsOopClosure oop_closure;\n@@ -524,2 +276,2 @@\n-              \"p: \" INTPTR_FORMAT \" obj: \" INTPTR_FORMAT \" index: \" SIZE_FORMAT \" bit_offset: \" SIZE_FORMAT,\n-              p2i(p), p2i((oopDesc*)obj), index, _chunk->bit_offset());\n+              \"p: \" INTPTR_FORMAT \" obj: \" INTPTR_FORMAT \" index: \" SIZE_FORMAT,\n+              p2i(p), p2i((oopDesc*)obj), index);\n@@ -544,1 +296,1 @@\n-  template <class T> inline void do_oop_work(T* p) {\n+  template <typename T> inline void do_oop_work(T* p) {\n@@ -551,1 +303,1 @@\n-      BitMap::idx_t index = (p - (T*)_chunk->start_address()) + _chunk->bit_offset();\n+      BitMap::idx_t index = _chunk->bit_index_for(p);\n@@ -703,1 +455,1 @@\n-  iterate_stack(chunk, &closure);\n+  chunk->iterate_stack(&closure);\n@@ -737,3 +489,3 @@\n-    assert(chunk->bitmap().size() == chunk->bit_offset() + (size_t)(chunk->stack_size() << (UseCompressedOops ? 1 : 0)),\n-      \"bitmap().size(): %zu bit_offset: %zu stack_size: %d\",\n-      chunk->bitmap().size(), chunk->bit_offset(), chunk->stack_size());\n+    assert(chunk->bitmap().size() == align_up((size_t)(chunk->stack_size() << (UseCompressedOops ? 1 : 0)), BitsPerWord),\n+      \"bitmap().size(): %zu stack_size: %d\",\n+      chunk->bitmap().size(), chunk->stack_size());\n@@ -848,1 +600,1 @@\n-    iterate_stack(c, &closure);\n+    c->iterate_stack(&closure);\n@@ -854,1 +606,1 @@\n-    iterate_stack(c, &describe);\n+    c->iterate_stack(&describe);\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.cpp","additions":15,"deletions":263,"binary":false,"changes":278,"status":"modified"},{"patch":"@@ -101,4 +101,0 @@\n-public:\n-  enum class barrier_type { LOAD, STORE };\n-\n-private:\n@@ -110,1 +106,0 @@\n-  template <barrier_type barrier> friend class DoBarriersStackClosure;\n@@ -132,2 +127,0 @@\n-  \/\/ the *last* bit in the bitmap corresponds to the last word in the stack; this returns the bit index corresponding to the first word\n-  static inline BitMap::idx_t bit_offset(size_t stack_size_in_words);\n@@ -182,8 +175,0 @@\n-  static void relativize_chunk(stackChunkOop chunk);\n-\n-  template <barrier_type>\n-  static void do_barriers(stackChunkOop chunk);\n-\n-  template <barrier_type, chunk_frames frames, typename RegisterMapT>\n-  inline static void do_barriers(stackChunkOop chunk, const StackChunkFrameStream<frames>& f, const RegisterMapT* map);\n-\n@@ -193,2 +178,0 @@\n-  static void build_bitmap(stackChunkOop chunk);\n-\n@@ -217,6 +200,0 @@\n-  template <class StackChunkFrameClosureType>\n-  static inline void iterate_stack(stackChunkOop obj, StackChunkFrameClosureType* closure);\n-\n-  template <chunk_frames frames, class StackChunkFrameClosureType>\n-  static inline void iterate_stack(stackChunkOop obj, StackChunkFrameClosureType* closure);\n-\n@@ -224,6 +201,0 @@\n-\n-  template <chunk_frames frames, typename RegisterMapT>\n-  static void relativize_derived_pointers(const StackChunkFrameStream<frames>& f, const RegisterMapT* map);\n-\n-  template <barrier_type barrier, chunk_frames frames = chunk_frames::MIXED, typename RegisterMapT>\n-  static void do_barriers0(stackChunkOop chunk, const StackChunkFrameStream<frames>& f, const RegisterMapT* map);\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.hpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -64,7 +64,0 @@\n-  size_t size_in_bits = bitmap_size_in_bits(stack_size_in_words);\n-  static const size_t mask = BitsPerWord - 1;\n-  int remainder = (size_in_bits & mask) != 0 ? 1 : 0;\n-  size_t res = (size_in_bits >> LogBitsPerWord) + remainder;\n-  assert (size_in_bits + bit_offset(stack_size_in_words) == (res << LogBitsPerWord), \"\");\n-  return res;\n-}\n@@ -72,4 +65,1 @@\n-inline BitMap::idx_t InstanceStackChunkKlass::bit_offset(size_t stack_size_in_words) {\n-  static const size_t mask = BitsPerWord - 1;\n-  return (BitMap::idx_t)((BitsPerWord - (bitmap_size_in_bits(stack_size_in_words) & mask)) & mask);\n-}\n+  size_t size_in_bits = bitmap_size_in_bits(stack_size_in_words);\n@@ -77,8 +67,1 @@\n-template <InstanceStackChunkKlass::barrier_type barrier, chunk_frames frame_kind, typename RegisterMapT>\n-void InstanceStackChunkKlass::do_barriers(stackChunkOop chunk, const StackChunkFrameStream<frame_kind>& f,\n-                                          const RegisterMapT* map) {\n-  if (frame_kind == chunk_frames::MIXED) {\n-    \/\/ we could freeze deopted frames in slow mode.\n-    f.handle_deopted();\n-  }\n-  do_barriers0<barrier>(chunk, f, map);\n+  return align_up(size_in_bits, BitsPerWord) >> LogBitsPerWord;\n@@ -200,38 +183,0 @@\n-template <class StackChunkFrameClosureType>\n-inline void InstanceStackChunkKlass::iterate_stack(stackChunkOop obj, StackChunkFrameClosureType* closure) {\n-  obj->has_mixed_frames() ? iterate_stack<chunk_frames::MIXED>(obj, closure)\n-                          : iterate_stack<chunk_frames::COMPILED_ONLY>(obj, closure);\n-}\n-\n-template <chunk_frames frame_kind, class StackChunkFrameClosureType>\n-inline void InstanceStackChunkKlass::iterate_stack(stackChunkOop obj, StackChunkFrameClosureType* closure) {\n-  const SmallRegisterMap* map = SmallRegisterMap::instance;\n-  assert (!map->in_cont(), \"\");\n-\n-  StackChunkFrameStream<frame_kind> f(obj);\n-  bool should_continue = true;\n-\n-  if (f.is_stub()) {\n-    RegisterMap full_map((JavaThread*)nullptr, true, false, true);\n-    full_map.set_include_argument_oops(false);\n-\n-    f.next(&full_map);\n-\n-    assert (!f.is_done(), \"\");\n-    assert (f.is_compiled(), \"\");\n-\n-    should_continue = closure->template do_frame<frame_kind>((const StackChunkFrameStream<frame_kind>&)f, &full_map);\n-    f.next(map);\n-    f.handle_deopted(); \/\/ the stub caller might be deoptimized (as it's not at a call)\n-  }\n-  assert (!f.is_stub(), \"\");\n-\n-  for(; should_continue && !f.is_done(); f.next(map)) {\n-    if (frame_kind == chunk_frames::MIXED) {\n-      \/\/ in slow mode we might freeze deoptimized frames\n-      f.handle_deopted();\n-    }\n-    should_continue = closure->template do_frame<frame_kind>((const StackChunkFrameStream<frame_kind>&)f, map);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/instanceStackChunkKlass.inline.hpp","additions":2,"deletions":57,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-  \/\/ if (map->update_map() && should_fix()) InstanceStackChunkKlass::fix_frame<true, false>(fs, map);\n@@ -46,1 +45,0 @@\n-  \/\/ if (!maybe_fix_async_walk(f, map)) return frame();\n@@ -83,19 +81,0 @@\n-\/\/ bool stackChunkOopDesc::maybe_fix_async_walk(frame& f, RegisterMap* map) const {\n-\/\/   if (!Continuation::is_return_barrier_entry(f.pc()))\n-\/\/     return true;\n-\n-\/\/   \/\/ Can happen during async stack walks, where the continuation is in the midst of a freeze\/thaw\n-\/\/   assert(map->is_async(), \"\");\n-\/\/   address pc0 = pc();\n-\n-\/\/   \/\/ we write sp first, then pc; here we read in the opposite order, so if sp is right, so is pc.\n-\/\/   OrderAccess::loadload();\n-\/\/   if (sp() == to_offset(f.sp())) {\n-\/\/     f.set_pc(pc0);\n-\/\/     return true;\n-\/\/   }\n-\/\/   assert(false, \"\");\n-\/\/   log_debug(jvmcont)(\"failed to fix frame during async stackwalk\");\n-\/\/   return false;\n-\/\/ }\n-\n@@ -127,0 +106,247 @@\n+template <stackChunkOopDesc::barrier_type barrier>\n+class DoBarriersStackClosure {\n+  const stackChunkOop _chunk;\n+\n+public:\n+  DoBarriersStackClosure(stackChunkOop chunk) : _chunk(chunk) {}\n+\n+  template <chunk_frames frame_kind, typename RegisterMapT>\n+  bool do_frame(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n+    _chunk->do_barriers0<barrier>(f, map);\n+    return true;\n+  }\n+};\n+\n+template <stackChunkOopDesc::barrier_type barrier>\n+void stackChunkOopDesc::do_barriers() {\n+  DoBarriersStackClosure<barrier> closure(this);\n+  iterate_stack(&closure);\n+}\n+\n+template void stackChunkOopDesc::do_barriers<stackChunkOopDesc::barrier_type::LOAD> ();\n+template void stackChunkOopDesc::do_barriers<stackChunkOopDesc::barrier_type::STORE>();\n+\n+\/\/ We replace derived pointers with offsets; the converse is done in DerelativizeDerivedPointers\n+class RelativizeDerivedPointers : public DerivedOopClosure {\n+public:\n+  virtual void do_derived_oop(oop* base_loc, derived_pointer* derived_loc) override {\n+    \/\/ The ordering in the following is crucial\n+    OrderAccess::loadload();\n+    oop base = Atomic::load((oop*)base_loc);\n+    if (base == nullptr) {\n+      assert(*derived_loc == derived_pointer(0), \"\");\n+      return;\n+    }\n+    assert(!CompressedOops::is_base(base), \"\");\n+\n+#if INCLUDE_ZGC\n+    if (UseZGC) {\n+      if (ZAddress::is_good(cast_from_oop<uintptr_t>(base))) {\n+        return;\n+      }\n+    }\n+#endif\n+#if INCLUDE_SHENANDOAHGC\n+    if (UseShenandoahGC) {\n+      if (!ShenandoahHeap::heap()->in_collection_set(base)) {\n+        return;\n+      }\n+    }\n+#endif\n+\n+    OrderAccess::loadload();\n+    intptr_t derived_int_val = Atomic::load((intptr_t*)derived_loc);\n+    if (derived_int_val <= 0) {\n+      return;\n+    }\n+\n+    \/\/ at this point, we've seen a non-offset value *after* we've read the base, but we write the offset *before* fixing the base,\n+    \/\/ so we are guaranteed that the value in derived_loc is consistent with base (i.e. points into the object).\n+    intptr_t offset = derived_int_val - cast_from_oop<intptr_t>(base);\n+    if (offset < 0) {\n+      \/\/ It looks as if a derived pointer appears live in the oopMap but isn't pointing into the object.\n+      \/\/ This might be the result of address computation floating above corresponding range check for array access.\n+      offset = -1;\n+    }\n+    Atomic::store((intptr_t*)derived_loc, -offset);\n+  }\n+};\n+\n+template <chunk_frames frame_kind, typename RegisterMapT>\n+static void relativize_frame(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n+  bool has_derived = f.is_compiled() && f.oopmap()->has_derived_oops();\n+  if (has_derived) {\n+    RelativizeDerivedPointers derived_closure;\n+    f.iterate_derived_pointers(&derived_closure, map);\n+  }\n+}\n+\n+class RelativizeStackClosure {\n+public:\n+\n+  template <chunk_frames frame_kind, typename RegisterMapT>\n+  bool do_frame(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n+    relativize_frame(f, map);\n+    return true;\n+  }\n+};\n+\n+void stackChunkOopDesc::relativize() {\n+  assert(!is_gc_mode(), \"Should only be called once per chunk\");\n+  set_gc_mode(true);\n+  OrderAccess::storestore();\n+  RelativizeStackClosure closure;\n+  iterate_stack(&closure);\n+}\n+\n+enum class OopKind { Narrow, Wide };\n+\n+template <OopKind kind>\n+class CompressOopsAndBuildBitmapOopClosure : public OopClosure {\n+  stackChunkOop _chunk;\n+  BitMapView _bm;\n+\n+  void convert_oop_to_narrowOop(oop* p) {\n+    oop obj = *p;\n+    *p = nullptr;\n+    *(narrowOop*)p = CompressedOops::encode(obj);\n+  }\n+\n+  template <typename T>\n+  void do_oop_work(T* p) {\n+    BitMap::idx_t index = _chunk->bit_index_for(p);\n+    assert(!_bm.at(index), \"must not be set already\");\n+    _bm.set_bit(index);\n+  }\n+\n+public:\n+  CompressOopsAndBuildBitmapOopClosure(stackChunkOop chunk)\n+    : _chunk(chunk), _bm(chunk->bitmap()) {}\n+\n+  virtual void do_oop(oop* p) override {\n+    if (kind == OopKind::Narrow) {\n+      \/\/ Convert all oops to narrow before marking the oop in the bitmap.\n+      convert_oop_to_narrowOop(p);\n+      do_oop_work((narrowOop*)p);\n+    } else {\n+      do_oop_work(p);\n+    }\n+  }\n+\n+  virtual void do_oop(narrowOop* p) override {\n+    do_oop_work(p);\n+  }\n+};\n+\n+template <OopKind kind>\n+class TransformStackChunkClosure {\n+  stackChunkOop _chunk;\n+\n+public:\n+  TransformStackChunkClosure(stackChunkOop chunk) : _chunk(chunk) {}\n+\n+  template <chunk_frames frame_kind, typename RegisterMapT>\n+  bool do_frame(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n+    \/\/ Relativize derived oops\n+    relativize_frame(f, map);\n+\n+    if (UseChunkBitmaps) {\n+      CompressOopsAndBuildBitmapOopClosure<kind> cl(_chunk);\n+      f.iterate_oops(&cl, map);\n+    }\n+\n+    return true;\n+  }\n+};\n+\n+void stackChunkOopDesc::transform() {\n+  assert(!is_gc_mode(), \"Should only be called once per chunk\");\n+  set_gc_mode(true);\n+\n+  if (UseChunkBitmaps) {\n+    assert(!has_bitmap(), \"Should only be set once\");\n+    set_has_bitmap(true);\n+    bitmap().clear();\n+  }\n+\n+  if (UseCompressedOops) {\n+    TransformStackChunkClosure<OopKind::Narrow> closure(this);\n+    iterate_stack(&closure);\n+  } else {\n+    TransformStackChunkClosure<OopKind::Wide> closure(this);\n+    iterate_stack(&closure);\n+  }\n+}\n+\n+template <stackChunkOopDesc::barrier_type barrier, bool compressedOopsWithBitmap>\n+class BarrierClosure: public OopClosure {\n+  NOT_PRODUCT(intptr_t* _sp;)\n+\n+public:\n+  BarrierClosure(intptr_t* sp) NOT_PRODUCT(: _sp(sp)) {}\n+\n+  virtual void do_oop(oop* p)       override { compressedOopsWithBitmap ? do_oop_work((narrowOop*)p) : do_oop_work(p); }\n+  virtual void do_oop(narrowOop* p) override { do_oop_work(p); }\n+\n+  template <class T> inline void do_oop_work(T* p) {\n+    oop value = (oop)HeapAccess<>::oop_load(p);\n+    if (barrier == stackChunkOopDesc::barrier_type::STORE) {\n+      HeapAccess<>::oop_store(p, value);\n+    }\n+  }\n+};\n+\n+template <stackChunkOopDesc::barrier_type barrier, chunk_frames frame_kind, typename RegisterMapT>\n+void stackChunkOopDesc::do_barriers0(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n+  \/\/ we need to invoke the write barriers so as not to miss oops in old chunks that haven't yet been concurrently scanned\n+  if (f.is_done()) {\n+    return;\n+  }\n+\n+  if (f.is_interpreted()) {\n+    Method* m = f.to_frame().interpreter_frame_method();\n+    m->record_marking_cycle();\n+  } else if (f.is_compiled()) {\n+    nmethod* nm = f.cb()->as_nmethod();\n+    \/\/ The entry barrier takes care of having the right synchronization\n+    \/\/ when keeping the nmethod alive during concurrent execution.\n+    nm->run_nmethod_entry_barrier();\n+    \/\/ there's no need to mark the Method, as class redefinition will walk the CodeCache, noting their Methods\n+  }\n+\n+  assert(!f.is_compiled() || f.oopmap()->has_derived_oops() == f.oopmap()->has_any(OopMapValue::derived_oop_value), \"\");\n+  bool has_derived = f.is_compiled() && f.oopmap()->has_derived_oops();\n+  if (has_derived) {\n+    relativize_derived_pointers(f, map);\n+  }\n+\n+  if (has_bitmap() && UseCompressedOops) {\n+    BarrierClosure<barrier, true> oops_closure(f.sp());\n+    f.iterate_oops(&oops_closure, map);\n+  } else {\n+    BarrierClosure<barrier, false> oops_closure(f.sp());\n+    f.iterate_oops(&oops_closure, map);\n+  }\n+  OrderAccess::loadload(); \/\/ observing the barriers will prevent derived pointers from being derelativized concurrently\n+}\n+\n+template void stackChunkOopDesc::do_barriers0<stackChunkOopDesc::barrier_type::LOAD> (const StackChunkFrameStream<chunk_frames::MIXED>& f, const RegisterMap* map);\n+template void stackChunkOopDesc::do_barriers0<stackChunkOopDesc::barrier_type::STORE>(const StackChunkFrameStream<chunk_frames::MIXED>& f, const RegisterMap* map);\n+template void stackChunkOopDesc::do_barriers0<stackChunkOopDesc::barrier_type::LOAD> (const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const RegisterMap* map);\n+template void stackChunkOopDesc::do_barriers0<stackChunkOopDesc::barrier_type::STORE>(const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const RegisterMap* map);\n+template void stackChunkOopDesc::do_barriers0<stackChunkOopDesc::barrier_type::LOAD> (const StackChunkFrameStream<chunk_frames::MIXED>& f, const SmallRegisterMap* map);\n+template void stackChunkOopDesc::do_barriers0<stackChunkOopDesc::barrier_type::STORE>(const StackChunkFrameStream<chunk_frames::MIXED>& f, const SmallRegisterMap* map);\n+template void stackChunkOopDesc::do_barriers0<stackChunkOopDesc::barrier_type::LOAD> (const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const SmallRegisterMap* map);\n+template void stackChunkOopDesc::do_barriers0<stackChunkOopDesc::barrier_type::STORE>(const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const SmallRegisterMap* map);\n+\n+template <chunk_frames frame_kind, typename RegisterMapT>\n+void stackChunkOopDesc::relativize_derived_pointers(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n+  RelativizeDerivedPointers derived_closure;\n+  f.iterate_derived_pointers(&derived_closure, map);\n+}\n+\n+template void stackChunkOopDesc::relativize_derived_pointers<>(const StackChunkFrameStream<chunk_frames::MIXED>& f, const RegisterMap* map);\n+template void stackChunkOopDesc::relativize_derived_pointers<>(const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const RegisterMap* map);\n+template void stackChunkOopDesc::relativize_derived_pointers<>(const StackChunkFrameStream<chunk_frames::MIXED>& f, const SmallRegisterMap* map);\n+template void stackChunkOopDesc::relativize_derived_pointers<>(const StackChunkFrameStream<chunk_frames::COMPILED_ONLY>& f, const SmallRegisterMap* map);\n+\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.cpp","additions":247,"deletions":21,"binary":false,"changes":268,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"runtime\/stackChunkFrameStream.hpp\"\n@@ -46,0 +47,3 @@\n+public:\n+  enum class barrier_type { LOAD, STORE };\n+\n@@ -47,0 +51,2 @@\n+  template <barrier_type barrier> friend class DoBarriersStackClosure;\n+\n@@ -120,0 +126,12 @@\n+  template <barrier_type>\n+  void do_barriers();\n+\n+  template <barrier_type, chunk_frames frames, typename RegisterMapT>\n+  inline void do_barriers(const StackChunkFrameStream<frames>& f, const RegisterMapT* map);\n+\n+  template <class StackChunkFrameClosureType>\n+  inline void iterate_stack(StackChunkFrameClosureType* closure);\n+\n+  void relativize();\n+  void transform();\n+\n@@ -124,1 +142,0 @@\n-  inline BitMap::idx_t bit_offset() const;\n@@ -160,0 +177,9 @@\n+  template <barrier_type barrier, chunk_frames frames = chunk_frames::MIXED, typename RegisterMapT>\n+  void do_barriers0(const StackChunkFrameStream<frames>& f, const RegisterMapT* map);\n+\n+  template <chunk_frames frames, typename RegisterMapT>\n+  static void relativize_derived_pointers(const StackChunkFrameStream<frames>& f, const RegisterMapT* map);\n+\n+  template <chunk_frames frames, class StackChunkFrameClosureType>\n+  inline void iterate_stack(StackChunkFrameClosureType* closure);\n+\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.hpp","additions":27,"deletions":1,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/smallRegisterMap.inline.hpp\"\n@@ -155,0 +156,47 @@\n+template <stackChunkOopDesc::barrier_type barrier, chunk_frames frame_kind, typename RegisterMapT>\n+void stackChunkOopDesc::do_barriers(const StackChunkFrameStream<frame_kind>& f, const RegisterMapT* map) {\n+  if (frame_kind == chunk_frames::MIXED) {\n+    \/\/ we could freeze deopted frames in slow mode.\n+    f.handle_deopted();\n+  }\n+  do_barriers0<barrier>(f, map);\n+}\n+\n+template <class StackChunkFrameClosureType>\n+inline void stackChunkOopDesc::iterate_stack(StackChunkFrameClosureType* closure) {\n+  has_mixed_frames() ? iterate_stack<chunk_frames::MIXED>(closure)\n+                     : iterate_stack<chunk_frames::COMPILED_ONLY>(closure);\n+}\n+\n+template <chunk_frames frame_kind, class StackChunkFrameClosureType>\n+inline void stackChunkOopDesc::iterate_stack(StackChunkFrameClosureType* closure) {\n+  const SmallRegisterMap* map = SmallRegisterMap::instance;\n+  assert(!map->in_cont(), \"\");\n+\n+  StackChunkFrameStream<frame_kind> f(this);\n+  bool should_continue = true;\n+\n+  if (f.is_stub()) {\n+    RegisterMap full_map((JavaThread*)nullptr, true, false, true);\n+    full_map.set_include_argument_oops(false);\n+\n+    f.next(&full_map);\n+\n+    assert(!f.is_done(), \"\");\n+    assert(f.is_compiled(), \"\");\n+\n+    should_continue = closure->do_frame(f, &full_map);\n+    f.next(map);\n+    f.handle_deopted(); \/\/ the stub caller might be deoptimized (as it's not at a call)\n+  }\n+  assert(!f.is_stub(), \"\");\n+\n+  for(; should_continue && !f.is_done(); f.next(map)) {\n+    if (frame_kind == chunk_frames::MIXED) {\n+      \/\/ in slow mode we might freeze deoptimized frames\n+      f.handle_deopted();\n+    }\n+    should_continue = closure->do_frame(f, map);\n+  }\n+}\n+\n@@ -173,4 +221,0 @@\n-inline BitMap::idx_t stackChunkOopDesc::bit_offset() const {\n-  return InstanceStackChunkKlass::bit_offset(stack_size());\n-}\n-\n@@ -183,1 +227,2 @@\n-  return bit_offset() + (p - (OopT*)start_address());\n+  assert(p >= (OopT*)start_address(), \"Address not in chunk\");\n+  return p - (OopT*)start_address();\n@@ -192,1 +237,1 @@\n-  return (OopT*)start_address() + (index - bit_offset());\n+  return (OopT*)start_address() + index;\n","filename":"src\/hotspot\/share\/oops\/stackChunkOop.inline.hpp","additions":51,"deletions":6,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -1310,1 +1310,1 @@\n-  assert (!is_chunk_available0 || orig_chunk_sp - (chunk->start_address() + chunk_new_sp) == is_chunk_available_size, \"\");\n+  assert(!is_chunk_available0 || orig_chunk_sp - (chunk->start_address() + chunk_new_sp) == is_chunk_available_size, \"\");\n@@ -1835,1 +1835,1 @@\n-    InstanceStackChunkKlass::do_barriers<InstanceStackChunkKlass::barrier_type::STORE>(_cont.tail());\n+    _cont.tail()->do_barriers<stackChunkOopDesc::barrier_type::STORE>();\n@@ -1883,1 +1883,1 @@\n-      chunk = stackChunkOopDesc::cast(allocator.initialize(start));\n+    chunk = stackChunkOopDesc::cast(allocator.initialize(start));\n@@ -1887,1 +1887,1 @@\n-      chunk = stackChunkOopDesc::cast(allocator.allocate()); \/\/ can safepoint\n+    chunk = stackChunkOopDesc::cast(allocator.allocate()); \/\/ can safepoint\n@@ -2705,1 +2705,1 @@\n-    InstanceStackChunkKlass::do_barriers<InstanceStackChunkKlass::barrier_type::STORE>(_cont.tail(), _stream, SmallRegisterMap::instance);\n+    _cont.tail()->do_barriers<stackChunkOopDesc::barrier_type::STORE>(_stream, SmallRegisterMap::instance);\n@@ -2771,1 +2771,1 @@\n-    InstanceStackChunkKlass::do_barriers<InstanceStackChunkKlass::barrier_type::STORE>(_cont.tail(), _stream, SmallRegisterMap::instance);\n+    _cont.tail()->do_barriers<stackChunkOopDesc::barrier_type::STORE>(_stream, SmallRegisterMap::instance);\n@@ -2843,1 +2843,1 @@\n-      InstanceStackChunkKlass::do_barriers<InstanceStackChunkKlass::barrier_type::STORE>(_cont.tail(), _stream, &map);\n+      _cont.tail()->do_barriers<stackChunkOopDesc::barrier_type::STORE>(_stream, &map);\n","filename":"src\/hotspot\/share\/runtime\/continuation.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"runtime\/frame.hpp\"\n@@ -95,1 +96,1 @@\n-  inline int to_offset(stackChunkOop chunk) const { assert(!is_done(), \"\"); return _sp - chunk->start_address(); }\n+  inline int to_offset(stackChunkOop chunk) const;\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-  \/\/ assert (!is_empty(), \"\"); -- allowed to be empty\n+  \/\/ assert(!is_empty(), \"\"); -- allowed to be empty\n@@ -279,0 +279,6 @@\n+template <chunk_frames frame_kind>\n+inline int StackChunkFrameStream<frame_kind>::to_offset(stackChunkOop chunk) const {\n+  assert(!is_done(), \"\");\n+  return _sp - chunk->start_address();\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/stackChunkFrameStream.inline.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"}]}