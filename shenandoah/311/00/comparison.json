{"files":[{"patch":"@@ -1013,1 +1013,1 @@\n-  _heap->generation_sizer()->force_transfer_to_old(1);\n+\n@@ -1015,0 +1015,4 @@\n+  bool transferred = _heap->generation_sizer()->transfer_to_old(1);\n+  if (!transferred) {\n+    log_info(gc, free)(\"Flipped region \" SIZE_FORMAT \" to old reserve, but did not transfer capacity.\", idx);\n+  }\n@@ -1228,8 +1232,7 @@\n-  if (old_reserve > _free_sets.capacity_of(OldCollector)) {\n-    \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n-    \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n-    \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n-    \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n-    if (old_reserve > _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n-      old_reserve = _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes;\n-    }\n+\n+  \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+  \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+  \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+  \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+  if (old_reserve > _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+    old_reserve = _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes;\n@@ -1237,0 +1240,1 @@\n+\n@@ -1256,8 +1260,20 @@\n-    if (_free_sets.in_free_set(idx, Mutator)) {\n-      assert (!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n-      size_t ac = alloc_capacity(r);\n-      assert (ac > 0, \"Membership in free set implies has capacity\");\n-\n-      \/\/ OLD regions that have available memory are already in the old_collector free set\n-      if ((_free_sets.capacity_of(OldCollector) < to_reserve_old) && (r->is_trash() || !r->is_affiliated())) {\n-        _free_sets.move_to_set(idx, OldCollector, alloc_capacity(r));\n+    if (!_free_sets.in_free_set(idx, Mutator)) {\n+      continue;\n+    }\n+\n+    size_t ac = alloc_capacity(r);\n+    assert (ac > 0, \"Membership in free set implies has capacity\");\n+    assert (!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+\n+    bool move_to_old = _free_sets.capacity_of(OldCollector) < to_reserve_old;\n+    bool move_to_young = _free_sets.capacity_of(Collector) < to_reserve;\n+\n+    if (!move_to_old && !move_to_young) {\n+      \/\/ We've satisfied both to_reserve and to_reserved_old\n+      break;\n+    }\n+\n+    if (move_to_old) {\n+      if (r->is_trash() || !r->is_affiliated()) {\n+        \/\/ OLD regions that have available memory are already in the old_collector free set\n+        _free_sets.move_to_set(idx, OldCollector, ac);\n@@ -1265,10 +1281,1 @@\n-      } else if (_free_sets.capacity_of(Collector) < to_reserve) {\n-        \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n-        \/\/ they were entirely empty.  I'm not sure I understand the rational for that.  That alternative behavior would\n-        \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n-        \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n-        _free_sets.move_to_set(idx, Collector, ac);\n-        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n-      } else {\n-        \/\/ We've satisfied both to_reserve and to_reserved_old\n-        break;\n+        continue;\n@@ -1277,0 +1284,9 @@\n+\n+    if (move_to_young) {\n+      \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+      \/\/ they were entirely empty.  I'm not sure I understand the rationale for that.  That alternative behavior would\n+      \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+      \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n+      _free_sets.move_to_set(idx, Collector, ac);\n+      log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":43,"deletions":27,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -1538,1 +1538,1 @@\n-        heap->generation_sizer()->transfer_to_old(old_regions_deficit);\n+        heap->generation_sizer()->force_transfer_to_old(old_regions_deficit);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -780,1 +780,0 @@\n-  _collection_thread_time_s(0.0),\n@@ -856,1 +855,1 @@\n-  assert(_affiliated_region_count > delta, \"Affiliated region count cannot be negative\");\n+  assert(_affiliated_region_count >= delta, \"Affiliated region count cannot be negative\");\n@@ -982,12 +981,0 @@\n-\n-void ShenandoahGeneration::add_collection_time(double time_seconds) {\n-  shenandoah_assert_control_or_vm_thread();\n-  _collection_thread_time_s += time_seconds;\n-}\n-\n-double ShenandoahGeneration::reset_collection_time() {\n-  double t = _collection_thread_time_s;\n-  _collection_thread_time_s = 0.0;\n-  return t;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":1,"deletions":14,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -52,2 +52,0 @@\n-  double _collection_thread_time_s;\n-\n@@ -228,10 +226,0 @@\n-\n-  \/\/ Record the total on-cpu time a thread has spent collecting this\n-  \/\/ generation. This is only called by the control thread (at the start\n-  \/\/ of a collection) and by the VM thread at the end of the collection,\n-  \/\/ so there are no locking concerns.\n-  virtual void add_collection_time(double time_seconds);\n-\n-  \/\/ This returns the accumulated collection time and resets it to zero.\n-  \/\/ This is used to decide which generation should be resized.\n-  double reset_collection_time();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":0,"deletions":12,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -90,9 +90,0 @@\n-void ShenandoahYoungGeneration::add_collection_time(double time_seconds) {\n-  if (is_bootstrap_cycle()) {\n-    \/\/ This is a bootstrap cycle, so attribute time to old gc\n-    ShenandoahHeap::heap()->old_generation()->add_collection_time(time_seconds);\n-  } else {\n-    ShenandoahGeneration::add_collection_time(time_seconds);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -66,2 +66,0 @@\n-  virtual void add_collection_time(double time_seconds) override;\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include <iomanip>\n","filename":"test\/hotspot\/gtest\/gc\/shared\/test_memset_with_concurrent_readers.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"}]}