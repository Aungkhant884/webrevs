{"files":[{"patch":"@@ -33,81 +33,0 @@\n-void ShenandoahCardStats::update_run_work(bool record) {\n-  assert(!(_last_dirty || _last_clean) || (_last_dirty && _dirty_run > 0) || (_last_clean && _clean_run > 0),\n-         \"dirty\/clean run stats inconsistent\");\n-  assert(_dirty_run == 0 || _clean_run == 0, \"Both shouldn't be non-zero\");\n-  if (_dirty_run > _max_dirty_run) {\n-    assert(_last_dirty, \"Error\");\n-    _max_dirty_run = _dirty_run;\n-  } else if (_clean_run > _max_clean_run) {\n-    assert(_last_clean, \"Error\");\n-    _max_clean_run = _clean_run;\n-  }\n-  _dirty_card_cnt += _dirty_run;\n-  _clean_card_cnt += _clean_run;\n-\n-  \/\/ Update local stats\n-  {\n-    assert(_dirty_run <= _cards_in_cluster, \"Error\");\n-    assert(_clean_run <= _cards_in_cluster, \"Error\");\n-    \/\/ Update global stats for distribution of dirty\/clean run lengths\n-    _local_card_stats[DIRTY_RUN].add((double)_dirty_run*100\/(double)_cards_in_cluster);\n-    _local_card_stats[CLEAN_RUN].add((double)_clean_run*100\/(double)_cards_in_cluster);\n-\n-    if (record) {\n-      \/\/ Update global stats for distribution of dirty\/clean cards as a percentage of chunk\n-      _local_card_stats[DIRTY_CARDS].add((double)_dirty_card_cnt*100\/(double)_cards_in_cluster);\n-      _local_card_stats[CLEAN_CARDS].add((double)_clean_card_cnt*100\/(double)_cards_in_cluster);\n-\n-      \/\/ Update global stats for max dirty\/clean run distribution as a percentage of chunk\n-      _local_card_stats[MAX_DIRTY_RUN].add((double)_max_dirty_run*100\/(double)_cards_in_cluster);\n-      _local_card_stats[MAX_CLEAN_RUN].add((double)_max_clean_run*100\/(double)_cards_in_cluster);\n-\n-      \/\/ Update global stats for dirty & clean object counts\n-      _local_card_stats[DIRTY_OBJS].add(_dirty_obj_cnt);\n-      _local_card_stats[CLEAN_OBJS].add(_clean_obj_cnt);\n-      _local_card_stats[DIRTY_SCANS].add(_dirty_scan_cnt);\n-      _local_card_stats[CLEAN_SCANS].add(_clean_scan_cnt);\n-\n-      _local_card_stats[ALTERNATIONS].add(_alternation_cnt);\n-    }\n-  }\n-\n-  if (record) {\n-    \/\/ reset the stats for the next cluster\n-    _dirty_card_cnt = 0;\n-    _clean_card_cnt = 0;\n-\n-    _max_dirty_run = 0;\n-    _max_clean_run = 0;\n-\n-    _dirty_obj_cnt = 0;\n-    _clean_obj_cnt = 0;\n-\n-    _dirty_scan_cnt = 0;\n-    _clean_scan_cnt = 0;\n-\n-    _alternation_cnt = 0;\n-  }\n-  _dirty_run = 0;\n-  _clean_run = 0;\n-  _last_dirty = false;\n-  _last_clean = false;\n-  assert(!record || is_clean(), \"Error\");\n-}\n-\n-bool ShenandoahCardStats::is_clean() {\n-  return\n-    _dirty_card_cnt == 0 &&\n-    _clean_card_cnt == 0 &&\n-    _max_dirty_run == 0 &&\n-    _max_clean_run == 0 &&\n-    _dirty_obj_cnt == 0 &&\n-    _clean_obj_cnt == 0 &&\n-    _dirty_scan_cnt == 0 &&\n-    _clean_scan_cnt == 0 &&\n-    _alternation_cnt == 0 &&\n-    _dirty_run == 0 &&\n-    _clean_run == 0 &&\n-    _last_dirty == false &&\n-    _last_clean == false;\n-}\n-\n@@ -118,2 +37,1 @@\n-      \" dirty objs \" SIZE_FORMAT \", clean objs \" SIZE_FORMAT \",\"\n-      \" dirty scans \" SIZE_FORMAT \", clean scans \" SIZE_FORMAT,\n+      \" dirty scans\/objs \" SIZE_FORMAT,\n@@ -121,2 +39,1 @@\n-      _dirty_obj_cnt, _clean_obj_cnt,\n-      _dirty_scan_cnt, _clean_scan_cnt);\n+      _dirty_scan_obj_cnt);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardStats.cpp","additions":2,"deletions":85,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -32,0 +32,18 @@\n+enum CardStatType {\n+  DIRTY_RUN = 0,\n+  CLEAN_RUN = 1,\n+  DIRTY_CARDS = 2,\n+  CLEAN_CARDS = 3,\n+  MAX_DIRTY_RUN = 4,\n+  MAX_CLEAN_RUN = 5,\n+  DIRTY_SCAN_OBJS = 6,\n+  ALTERNATIONS = 7,\n+  MAX_CARD_STAT_TYPE = 8\n+};\n+\n+enum CardStatLogType {\n+  CARD_STAT_SCAN_RS = 0,\n+  CARD_STAT_UPDATE_REFS = 1,\n+  MAX_CARD_STAT_LOG_TYPE = 2\n+};\n+\n@@ -37,3 +55,0 @@\n-  bool _last_dirty;\n-  bool _last_clean;\n-\n@@ -49,5 +64,1 @@\n-  size_t _dirty_obj_cnt;\n-  size_t _clean_obj_cnt;\n-\n-  size_t _dirty_scan_cnt;\n-  size_t _clean_scan_cnt;\n+  size_t _dirty_scan_obj_cnt;\n@@ -61,2 +72,0 @@\n-    _last_dirty(false),\n-    _last_clean(false),\n@@ -65,2 +74,0 @@\n-    _dirty_run(0),\n-    _clean_run(0),\n@@ -69,4 +76,1 @@\n-    _dirty_obj_cnt(0),\n-    _clean_obj_cnt(0),\n-    _dirty_scan_cnt(0),\n-    _clean_scan_cnt(0),\n+    _dirty_scan_obj_cnt(0),\n@@ -76,28 +80,3 @@\n-private:\n-  void increment_card_cnt_work(bool dirty) {\n-    if (dirty) { \/\/ dirty card\n-      if (_last_dirty) {\n-        assert(_dirty_run > 0 && _clean_run == 0 && !_last_clean, \"Error\");\n-        _dirty_run++;\n-      } else {\n-        if (_last_clean) {\n-          _alternation_cnt++;\n-        }\n-        update_run(false);\n-        _last_dirty = true;\n-        _dirty_run = 1;\n-      }\n-    } else { \/\/ clean card\n-      if (_last_clean) {\n-        assert(_clean_run > 0 && _dirty_run == 0 && !_last_dirty, \"Error\");\n-        _clean_run++;\n-      } else {\n-        if (_last_dirty) {\n-          _alternation_cnt++;\n-        }\n-        update_run(false);\n-        _last_clean = true;\n-        _clean_run = 1;\n-      }\n-    }\n-  }\n+  ~ShenandoahCardStats() {\n+    record();\n+   }\n@@ -105,5 +84,5 @@\n-  inline void increment_obj_cnt_work(bool dirty)  {\n-    assert(!dirty || (_last_dirty && _dirty_run > 0), \"Error\");\n-    assert(dirty  || (_last_clean && _clean_run > 0), \"Error\");\n-    dirty ? _dirty_obj_cnt++ : _clean_obj_cnt++;\n-  }\n+   void record() {\n+    if (ShenandoahEnableCardStats) {\n+      \/\/ Update global stats for distribution of dirty\/clean cards as a percentage of chunk\n+      _local_card_stats[DIRTY_CARDS].add((double)_dirty_card_cnt*100\/(double)_cards_in_cluster);\n+      _local_card_stats[CLEAN_CARDS].add((double)_clean_card_cnt*100\/(double)_cards_in_cluster);\n@@ -111,5 +90,3 @@\n-  inline void increment_scan_cnt_work(bool dirty) {\n-    assert(!dirty || (_last_dirty && _dirty_run > 0), \"Error\");\n-    assert(dirty  || (_last_clean && _clean_run > 0), \"Error\");\n-    dirty ? _dirty_scan_cnt++ : _clean_scan_cnt++;\n-  }\n+      \/\/ Update global stats for max dirty\/clean run distribution as a percentage of chunk\n+      _local_card_stats[MAX_DIRTY_RUN].add((double)_max_dirty_run*100\/(double)_cards_in_cluster);\n+      _local_card_stats[MAX_CLEAN_RUN].add((double)_max_clean_run*100\/(double)_cards_in_cluster);\n@@ -117,1 +94,2 @@\n-  void update_run_work(bool cluster) PRODUCT_RETURN;\n+      \/\/ Update global stats for dirty obj scan counts\n+      _local_card_stats[DIRTY_SCAN_OBJS].add(_dirty_scan_obj_cnt);\n@@ -119,4 +97,2 @@\n-public:\n-  inline void increment_card_cnt(bool dirty) {\n-    if (ShenandoahEnableCardStats) {\n-      increment_card_cnt_work(dirty);\n+      \/\/ Update global stats for alternation counts\n+      _local_card_stats[ALTERNATIONS].add(_alternation_cnt);\n@@ -126,1 +102,2 @@\n-  inline void increment_obj_cnt(bool dirty) {\n+public:\n+  inline void record_dirty_run(size_t len) {\n@@ -128,1 +105,7 @@\n-      increment_obj_cnt_work(dirty);\n+      _alternation_cnt++;\n+      if (len > _max_dirty_run) {\n+        _max_dirty_run = len;\n+      }\n+      _dirty_card_cnt += len;\n+      assert(len <= _cards_in_cluster, \"Error\");\n+      _local_card_stats[DIRTY_RUN].add((double)len*100.0\/(double)_cards_in_cluster);\n@@ -132,1 +115,1 @@\n-  inline void increment_scan_cnt(bool dirty) {\n+  inline void record_clean_run(size_t len) {\n@@ -134,1 +117,7 @@\n-      increment_scan_cnt_work(dirty);\n+      _alternation_cnt++;\n+      if (len > _max_clean_run) {\n+        _max_clean_run = len;\n+      }\n+      _clean_card_cnt += len;\n+      assert(len <= _cards_in_cluster, \"Error\");\n+      _local_card_stats[CLEAN_RUN].add((double)len*100.0\/(double)_cards_in_cluster);\n@@ -138,1 +127,1 @@\n-  inline void update_run(bool record) {\n+  inline void record_scan_obj_cnt(size_t i) {\n@@ -140,1 +129,1 @@\n-      update_run_work(record);\n+      _dirty_scan_obj_cnt += i;\n@@ -144,2 +133,0 @@\n-  bool is_clean() PRODUCT_RETURN0;\n-\n@@ -149,21 +136,0 @@\n-enum CardStatType {\n-  DIRTY_RUN = 0,\n-  CLEAN_RUN = 1,\n-  DIRTY_CARDS = 2,\n-  CLEAN_CARDS = 3,\n-  MAX_DIRTY_RUN = 4,\n-  MAX_CLEAN_RUN = 5,\n-  DIRTY_OBJS = 6,\n-  CLEAN_OBJS = 7,\n-  DIRTY_SCANS = 8,\n-  CLEAN_SCANS= 9,\n-  ALTERNATIONS = 10,\n-  MAX_CARD_STAT_TYPE = 11\n-};\n-\n-enum CardStatLogType {\n-  CARD_STAT_SCAN_RS = 0,\n-  CARD_STAT_UPDATE_REFS = 1,\n-  MAX_CARD_STAT_LOG_TYPE = 2\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardStats.hpp","additions":54,"deletions":88,"binary":false,"changes":142,"status":"modified"},{"patch":"@@ -975,0 +975,2 @@\n+  \/\/ TODO: ysr: revert to an assert\n+  guarantee(adjustment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n@@ -1005,0 +1007,2 @@\n+  \/\/ TODO: ysr: revert to an assert\n+  guarantee(increment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n@@ -1013,0 +1017,2 @@\n+  \/\/ TODO: ysr: revert to an assert\n+  guarantee(decrement % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2712,1 +2712,1 @@\n-                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true, CONCURRENT);\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n@@ -2782,1 +2782,1 @@\n-              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, CONCURRENT, worker_id);\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -593,1 +593,1 @@\n-                                                       HeapWord* start, size_t words, bool write_table, bool is_concurrent) {\n+                                                       HeapWord* start, size_t words, bool write_table) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -409,2 +409,1 @@\n-  void oop_iterate_humongous_slice(OopIterateClosure* cl, bool dirty_only, HeapWord* start, size_t words,\n-                                   bool write_table, bool is_concurrent);\n+  void oop_iterate_humongous_slice(OopIterateClosure* cl, bool dirty_only, HeapWord* start, size_t words, bool write_table);\n@@ -432,0 +431,5 @@\n+  \/\/ Does this region contain this address?\n+  bool contains(HeapWord* p) const {\n+    return (bottom() <= p) && (p < top());\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":6,"deletions":2,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -68,1 +68,2 @@\n-  assert(limit <= tams, \"limit must be less than TAMS\");\n+  assert(limit <= r->top(), \"limit must be less than top\");\n+  assert(addr <= tams, \"addr must be less than TAMS\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -62,5 +62,5 @@\n-  inline bool is_marked(oop) const;\n-  inline bool is_marked_strong(oop obj) const;\n-  inline bool is_marked_weak(oop obj) const;\n-  inline bool is_marked_or_old(oop obj) const;\n-  inline bool is_marked_strong_or_old(oop obj) const;\n+  inline bool is_marked(const oop) const;\n+  inline bool is_marked_strong(const oop obj) const;\n+  inline bool is_marked_weak(const oop obj) const;\n+  inline bool is_marked_or_old(const oop obj) const;\n+  inline bool is_marked_strong_or_old(const oop obj) const;\n@@ -68,1 +68,1 @@\n-  inline HeapWord* get_next_marked_addr(HeapWord* addr, HeapWord* limit) const;\n+  inline HeapWord* get_next_marked_addr(const HeapWord* addr, const HeapWord* limit) const;\n@@ -70,2 +70,2 @@\n-  inline bool allocated_after_mark_start(oop obj) const;\n-  inline bool allocated_after_mark_start(HeapWord* addr) const;\n+  inline bool allocated_after_mark_start(const oop obj) const;\n+  inline bool allocated_after_mark_start(const HeapWord* addr) const;\n@@ -73,1 +73,1 @@\n-  inline HeapWord* top_at_mark_start(ShenandoahHeapRegion* r) const;\n+  inline HeapWord* top_at_mark_start(const ShenandoahHeapRegion* r) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-inline bool ShenandoahMarkingContext::is_marked(oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked(const oop obj) const {\n@@ -44,1 +44,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_strong(oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_strong(const oop obj) const {\n@@ -48,1 +48,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_weak(oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_weak(const oop obj) const {\n@@ -52,1 +52,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_or_old(oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_or_old(const oop obj) const {\n@@ -56,1 +56,1 @@\n-inline bool ShenandoahMarkingContext::is_marked_strong_or_old(oop obj) const {\n+inline bool ShenandoahMarkingContext::is_marked_strong_or_old(const oop obj) const {\n@@ -60,1 +60,1 @@\n-inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(HeapWord* start, HeapWord* limit) const {\n+inline HeapWord* ShenandoahMarkingContext::get_next_marked_addr(const HeapWord* start, const HeapWord* limit) const {\n@@ -64,2 +64,2 @@\n-inline bool ShenandoahMarkingContext::allocated_after_mark_start(oop obj) const {\n-  HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n+inline bool ShenandoahMarkingContext::allocated_after_mark_start(const oop obj) const {\n+  const HeapWord* addr = cast_from_oop<HeapWord*>(obj);\n@@ -69,1 +69,1 @@\n-inline bool ShenandoahMarkingContext::allocated_after_mark_start(HeapWord* addr) const {\n+inline bool ShenandoahMarkingContext::allocated_after_mark_start(const HeapWord* addr) const {\n@@ -72,1 +72,1 @@\n-  bool alloc_after_mark_start = addr >= top_at_mark_start;\n+  const bool alloc_after_mark_start = addr >= top_at_mark_start;\n@@ -115,1 +115,1 @@\n-inline HeapWord* ShenandoahMarkingContext::top_at_mark_start(ShenandoahHeapRegion* r) const {\n+inline HeapWord* ShenandoahMarkingContext::top_at_mark_start(const ShenandoahHeapRegion* r) const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -99,1 +99,1 @@\n-      scanner->process_region_slice(region, assignment._chunk_offset, clusters, end_of_range, &cl, false, _is_concurrent, worker_id);\n+      scanner->process_region_slice(region, assignment._chunk_offset, clusters, end_of_range, &cl, false, worker_id);\n@@ -115,3 +115,3 @@\n-  \/\/ The group size is calculated from the number of regions.  Suppose the entire heap size is N.  The first group processes\n-  \/\/ N\/2 of total heap size.  The second group processes N\/4 of total heap size.  The third group processes N\/2 of total heap\n-  \/\/ size, and so on.  Note that N\/2 + N\/4 + N\/8 + N\/16 + ...  sums to N if expanded to infinite terms.\n+  \/\/ The group size is calculated from the number of regions.  Suppose the heap has N regions.  The first group processes\n+  \/\/ N\/2 regions.  The second group processes N\/4 regions, the third group N\/8 regions and so on.\n+  \/\/ Note that infinite series N\/2 + N\/4 + N\/8 + N\/16 + ...  sums to N.\n@@ -126,1 +126,1 @@\n-  \/\/ a quarter of the remaining heap, the third processes an eight of what remains and so on.  The smallest chunk size\n+  \/\/ half of the remaining heap, the third processes half of what remains and so on.  The smallest chunk size\n@@ -319,0 +319,5 @@\n+\n+ShenandoahVerifyNoYoungRefsClosure::ShenandoahVerifyNoYoungRefsClosure():\n+  _heap(ShenandoahHeap::heap()) {\n+  assert(_heap->mode()->is_generational(), \"Don't use when non-generational\");\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -195,0 +195,1 @@\n+typedef CardTable::CardValue CardValue;\n@@ -204,1 +205,0 @@\n-\n@@ -214,2 +214,2 @@\n-  uint8_t *_byte_map;           \/\/ Points to first entry within the card table\n-  uint8_t *_byte_map_base;      \/\/ Points to byte_map minus the bias computed from address of heap memory\n+  CardValue* _byte_map;         \/\/ Points to first entry within the card table\n+  CardValue* _byte_map_base;    \/\/ Points to byte_map minus the bias computed from address of heap memory\n@@ -218,0 +218,1 @@\n+\n@@ -223,17 +224,18 @@\n-  size_t last_valid_index();\n-  size_t total_cards();\n-  size_t card_index_for_addr(HeapWord *p);\n-  HeapWord *addr_for_card_index(size_t card_index);\n-  bool is_card_dirty(size_t card_index);\n-  bool is_write_card_dirty(size_t card_index);\n-  void mark_card_as_dirty(size_t card_index);\n-  void mark_range_as_dirty(size_t card_index, size_t num_cards);\n-  void mark_card_as_clean(size_t card_index);\n-  void mark_read_card_as_clean(size_t card_index);\n-  void mark_range_as_clean(size_t card_index, size_t num_cards);\n-  bool is_card_dirty(HeapWord *p);\n-  void mark_card_as_dirty(HeapWord *p);\n-  void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n-  void mark_card_as_clean(HeapWord *p);\n-  void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n-  size_t cluster_count();\n+  size_t last_valid_index() const;\n+  size_t total_cards() const;\n+  size_t card_index_for_addr(HeapWord *p) const;\n+  HeapWord *addr_for_card_index(size_t card_index) const;\n+  inline const CardValue* get_card_table_byte_map(bool write_table) const;\n+  inline bool is_card_dirty(size_t card_index) const;\n+  inline bool is_write_card_dirty(size_t card_index) const;\n+  inline void mark_card_as_dirty(size_t card_index);\n+  inline void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  inline void mark_card_as_clean(size_t card_index);\n+  inline void mark_read_card_as_clean(size_t card_index);\n+  inline void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  inline bool is_card_dirty(HeapWord *p) const;\n+  inline void mark_card_as_dirty(HeapWord *p);\n+  inline void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n+  inline void mark_card_as_clean(HeapWord *p);\n+  inline void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n+  inline size_t cluster_count() const;\n@@ -248,1 +250,1 @@\n-    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardTable::CardValue));\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardValue));\n@@ -264,1 +266,1 @@\n-    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardTable::CardValue));\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardValue));\n@@ -391,1 +393,1 @@\n-  \/\/ frequently that last byte.  This is true when number of clean cards is greater than number of dirty cards.\n+  \/\/ frequently than last byte.  This is true when number of clean cards is greater than number of dirty cards.\n@@ -407,1 +409,1 @@\n-  inline void set_has_object_bit(size_t card_index) {\n+  inline void set_starts_object_bit(size_t card_index) {\n@@ -411,1 +413,1 @@\n-  inline void clear_has_object_bit(size_t card_index) {\n+  inline void clear_starts_object_bit(size_t card_index) {\n@@ -416,1 +418,1 @@\n-  inline bool has_object(size_t card_index) {\n+  inline bool starts_object(size_t card_index) const {\n@@ -589,1 +591,1 @@\n-  \/\/     register_object() do pertain to the same card's memory range.  See discussion below to undestand\n+  \/\/     register_object() do pertain to the same card's memory range.  See discussion below to understand\n@@ -614,1 +616,1 @@\n-  \/\/ Reset the has_object() information to false for all cards in the range between from and to.\n+  \/\/ Reset the starts_object() information to false for all cards in the range between from and to.\n@@ -644,1 +646,1 @@\n-  \/\/  3. For following cards spanned entirely by the newly coalesced object, it will change has_object\n+  \/\/  3. For following cards spanned entirely by the newly coalesced object, it will change starts_object\n@@ -664,1 +666,1 @@\n-  \/\/ card noumber for subsequent information lookups and stores.\n+  \/\/ card number for subsequent information lookups and stores.\n@@ -666,4 +668,4 @@\n-  \/\/ If has_object(card_index), this returns the word offset within this card\n-  \/\/ memory at which the first object begins.  If !has_object(card_index), the\n-  \/\/ result is a don't care value.\n-  size_t get_first_start(size_t card_index);\n+  \/\/ If starts_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the first object begins.  If !starts_object(card_index), the\n+  \/\/ result is a don't care value -- asserts in a debug build.\n+  size_t get_first_start(size_t card_index) const;\n@@ -671,2 +673,2 @@\n-  \/\/ If has_object(card_index), this returns the word offset within this card\n-  \/\/ memory at which the last object begins.  If !has_object(card_index), the\n+  \/\/ If starts_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the last object begins.  If !starts_object(card_index), the\n@@ -674,1 +676,1 @@\n-  size_t get_last_start(size_t card_index);\n+  size_t get_last_start(size_t card_index) const;\n@@ -676,0 +678,7 @@\n+\n+  \/\/ Given a card_index, return the starting address of the first block in the heap\n+  \/\/ that straddles into the card. If the card is co-initial with an object, then\n+  \/\/ this would return the starting address of the heap that this card covers.\n+  \/\/ Expects to be called for a card affiliated with the old generation in\n+  \/\/ generational mode.\n+  HeapWord* block_start(size_t card_index) const;\n@@ -707,0 +716,1 @@\n+  \/\/ The types of card metrics that we gather\n@@ -711,2 +721,1 @@\n-   \"dirty_objs\", \"clean_objs\",\n-   \"dirty_scans\", \"clean_scans\",\n+   \"dirty_scan_objs\",\n@@ -716,0 +725,2 @@\n+  \/\/ The statistics are collected and logged separately for\n+  \/\/ card-scans for initial marking, and for updating refs.\n@@ -837,1 +848,1 @@\n-    if (_scc->has_object(card_index)) {\n+    if (_scc->starts_object(card_index)) {\n@@ -851,3 +862,2 @@\n-  \/\/ to scan roots from old gen into young to identify live objects\n-  \/\/ in the young generation. Several worker threads scan different\n-  \/\/ portions of the remembered set by making parallel invocations\n+  \/\/ for references from old gen into young. Several worker threads\n+  \/\/ scan different portions of the remembered set by making parallel invocations\n@@ -883,3 +893,0 @@\n-  template <typename ClosureType>\n-  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops, bool is_concurrent, uint worker_id);\n-\n@@ -888,1 +895,1 @@\n-                               bool use_write_table, bool is_concurrent, uint worker_id);\n+                               bool use_write_table, uint worker_id);\n@@ -892,1 +899,1 @@\n-                                         HeapWord *end_of_range, ClosureType *oops, bool use_write_table, bool is_concurrent);\n+                                         HeapWord *end_of_range, ClosureType *oops, bool use_write_table);\n@@ -896,1 +903,1 @@\n-                                   ClosureType *cl, bool use_write_table, bool is_concurrent, uint worker_id);\n+                                   ClosureType *cl, bool use_write_table, uint worker_id);\n@@ -934,0 +941,3 @@\n+\n+\/\/ A ShenandoahRegionChunk represents a contiguous interval of a ShenandoahHeapRegion, typically representing\n+\/\/ work to be done by a worker thread.\n@@ -935,1 +945,1 @@\n-  ShenandoahHeapRegion *_r;\n+  ShenandoahHeapRegion *_r;      \/\/ The region of which this represents a chunk\n@@ -940,10 +950,11 @@\n-\/\/ ShenandoahRegionChunkIterator divides the total remembered set scanning effort into assignments (ShenandoahRegionChunks)\n-\/\/ that are assigned one at a time to worker threads.  Note that the effort required to scan a range of memory is not\n-\/\/ necessarily a linear function of the size of the range.  Some memory ranges hold only a small number of live objects.\n-\/\/ Some ranges hold primarily primitive (non-pointer data).  We start with larger assignment sizes because larger assignments\n-\/\/ can be processed with less coordination effort.  We expect that the GC worker threads that receive more difficult assignments\n-\/\/ will work longer on those assignments.  Meanwhile, other worker will threads repeatedly accept and complete multiple\n-\/\/ easier assignments.  As the total amount of work remaining to be completed decreases, we decrease the size of assignments\n-\/\/ given to individual threads.  This reduces the likelihood that significant imbalance between worker thread assignments\n-\/\/ will be introduced when there is less meaningful work to be performed by the remaining worker threads while they wait for\n-\/\/ worker threads with difficult assignments to finish.\n+\/\/ ShenandoahRegionChunkIterator divides the total remembered set scanning effort into ShenandoahRegionChunks\n+\/\/ that are assigned one at a time to worker threads. (Here, we use the terms`assignments` and `chunks`\n+\/\/ interchangeably.) Note that the effort required to scan a range of memory is not necessarily a linear\n+\/\/ function of the size of the range.  Some memory ranges hold only a small number of live objects.\n+\/\/ Some ranges hold primarily primitive (non-pointer) data.  We start with larger chunk sizes because larger chunks\n+\/\/ reduce coordination overhead.  We expect that the GC worker threads that receive more difficult assignments\n+\/\/ will work longer on those chunks.  Meanwhile, other worker will threads repeatedly accept and complete multiple\n+\/\/ easier chunks.  As the total amount of work remaining to be completed decreases, we decrease the size of chunks\n+\/\/ given to individual threads.  This reduces the likelihood of significant imbalance between worker thread assignments\n+\/\/ when there is less meaningful work to be performed by the remaining worker threads while they wait for\n+\/\/ worker threads with difficult assignments to finish, reducing the overall duration of the phase.\n@@ -962,1 +973,1 @@\n-  static size_t smallest_chunk_size_words() {\n+  static const size_t smallest_chunk_size_words() {\n@@ -1054,0 +1065,13 @@\n+\n+\/\/ Verify that the oop doesn't point into the young generation\n+class ShenandoahVerifyNoYoungRefsClosure: public BasicOopIterateClosure {\n+  ShenandoahHeap* _heap;\n+  template<class T> void work(T* p);\n+\n+ public:\n+  ShenandoahVerifyNoYoungRefsClosure();\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p)       { work(p); }\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":85,"deletions":61,"binary":false,"changes":146,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-ShenandoahDirectCardMarkRememberedSet::last_valid_index() {\n+ShenandoahDirectCardMarkRememberedSet::last_valid_index() const {\n@@ -45,1 +45,1 @@\n-ShenandoahDirectCardMarkRememberedSet::total_cards() {\n+ShenandoahDirectCardMarkRememberedSet::total_cards() const {\n@@ -50,1 +50,1 @@\n-ShenandoahDirectCardMarkRememberedSet::card_index_for_addr(HeapWord *p) {\n+ShenandoahDirectCardMarkRememberedSet::card_index_for_addr(HeapWord *p) const {\n@@ -54,2 +54,2 @@\n-inline HeapWord *\n-ShenandoahDirectCardMarkRememberedSet::addr_for_card_index(size_t card_index) {\n+inline HeapWord*\n+ShenandoahDirectCardMarkRememberedSet::addr_for_card_index(size_t card_index) const {\n@@ -59,0 +59,7 @@\n+inline const CardValue*\n+ShenandoahDirectCardMarkRememberedSet::get_card_table_byte_map(bool use_write_table) const {\n+  return use_write_table ?\n+           _card_table->write_byte_map()\n+           : _card_table->read_byte_map();\n+}\n+\n@@ -60,2 +67,2 @@\n-ShenandoahDirectCardMarkRememberedSet::is_write_card_dirty(size_t card_index) {\n-  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+ShenandoahDirectCardMarkRememberedSet::is_write_card_dirty(size_t card_index) const {\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n@@ -66,2 +73,2 @@\n-ShenandoahDirectCardMarkRememberedSet::is_card_dirty(size_t card_index) {\n-  uint8_t *bp = &(_card_table->read_byte_map())[card_index];\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(size_t card_index) const {\n+  CardValue* bp = &(_card_table->read_byte_map())[card_index];\n@@ -73,1 +80,1 @@\n-  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n@@ -79,1 +86,1 @@\n-  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n@@ -87,1 +94,1 @@\n-  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n@@ -93,1 +100,1 @@\n-  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  CardValue* bp = &(_card_table->write_byte_map())[card_index];\n@@ -100,1 +107,1 @@\n-ShenandoahDirectCardMarkRememberedSet::is_card_dirty(HeapWord *p) {\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(HeapWord *p) const {\n@@ -102,1 +109,1 @@\n-  uint8_t *bp = &(_card_table->read_byte_map())[index];\n+  CardValue* bp = &(_card_table->read_byte_map())[index];\n@@ -109,1 +116,1 @@\n-  uint8_t *bp = &(_card_table->write_byte_map())[index];\n+  CardValue* bp = &(_card_table->write_byte_map())[index];\n@@ -115,2 +122,2 @@\n-  uint8_t *bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n-  uint8_t *end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  CardValue* bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  CardValue* end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n@@ -129,1 +136,1 @@\n-  uint8_t *bp = &(_card_table->write_byte_map())[index];\n+  CardValue* bp = &(_card_table->write_byte_map())[index];\n@@ -135,1 +142,1 @@\n-  uint8_t *bp = &(_card_table->read_byte_map())[index];\n+  CardValue* bp = &(_card_table->read_byte_map())[index];\n@@ -141,2 +148,2 @@\n-  uint8_t *bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n-  uint8_t *end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  CardValue* bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  CardValue* end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n@@ -153,1 +160,1 @@\n-ShenandoahDirectCardMarkRememberedSet::cluster_count() {\n+ShenandoahDirectCardMarkRememberedSet::cluster_count() const {\n@@ -189,2 +196,2 @@\n-  if (!has_object(card_at_start)) {\n-    set_has_object_bit(card_at_start);\n+  if (!starts_object(card_at_start)) {\n+    set_starts_object_bit(card_at_start);\n@@ -232,1 +239,1 @@\n-      clear_has_object_bit(i);\n+      clear_starts_object_bit(i);\n@@ -236,1 +243,1 @@\n-    if (has_object(card_at_end) && (get_first_start(card_at_end) < follow_offset)) {\n+    if (starts_object(card_at_end) && (get_first_start(card_at_end) < follow_offset)) {\n@@ -243,1 +250,1 @@\n-        clear_has_object_bit(card_at_end);\n+        clear_starts_object_bit(card_at_end);\n@@ -256,2 +263,2 @@\n-ShenandoahCardCluster<RememberedSet>::get_first_start(size_t card_index) {\n-  assert(has_object(card_index), \"Can't get first start because no object starts here\");\n+ShenandoahCardCluster<RememberedSet>::get_first_start(size_t card_index) const {\n+  assert(starts_object(card_index), \"Can't get first start because no object starts here\");\n@@ -263,2 +270,2 @@\n-ShenandoahCardCluster<RememberedSet>::get_last_start(size_t card_index) {\n-  assert(has_object(card_index), \"Can't get last start because no object starts here\");\n+ShenandoahCardCluster<RememberedSet>::get_last_start(size_t card_index) const {\n+  assert(starts_object(card_index), \"Can't get last start because no object starts here\");\n@@ -268,0 +275,75 @@\n+\/\/ Given a card_index, return the starting address of the first block in the heap\n+\/\/ that straddles into this card. If this card is co-initial with an object, then\n+\/\/ this would return the first address of the range that this card covers.\n+\/\/ TODO: collect some stats for the size of walks backward over cards.\n+\/\/ For larger objects, a logarithmic BOT such as used by G1 might make the\n+\/\/ backwards walk potentially faster.\n+template<typename RememberedSet>\n+HeapWord*\n+ShenandoahCardCluster<RememberedSet>::block_start(const size_t card_index) const {\n+\n+  HeapWord* left = _rs->addr_for_card_index(card_index);\n+\n+#ifdef ASSERT\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Do not use in non-generational mode\");\n+  ShenandoahHeapRegion* region = ShenandoahHeap::heap()->heap_region_containing(left);\n+  assert(region->is_old(), \"Do not use for young regions\");\n+  \/\/ For HumongousRegion:s it's more efficient to jump directly to the\n+  \/\/ start region.\n+  assert(!region->is_humongous(), \"Use region->humongous_start_region() instead\");\n+#endif\n+  if (starts_object(card_index) && get_first_start(card_index) == 0) {\n+    \/\/ This card contains a co-initial object; a fortiori, it covers\n+    \/\/ also the case of a card being the first in a region.\n+    return left;\n+  }\n+\n+  HeapWord* p = nullptr;\n+  oop obj = cast_to_oop(p);\n+  ssize_t cur_index = (ssize_t)card_index;\n+  assert(cur_index >= 0, \"Overflow\");\n+  assert(cur_index > 0, \"Should have returned above\");\n+  \/\/ Walk backwards over the cards...\n+  while (--cur_index > 0 && !starts_object(cur_index)) {\n+   \/\/ ... to the one that starts the object\n+  }\n+  \/\/ cur_index should start an object: we should not have walked\n+  \/\/ past the left end of the region.\n+  assert(cur_index >= 0 && (cur_index <= (ssize_t)card_index), \"Error\");\n+  assert(region->bottom() <= _rs->addr_for_card_index(cur_index),\n+         \"Fell off the bottom of containing region\");\n+  assert(starts_object(cur_index), \"Error\");\n+  size_t offset = get_last_start(cur_index);\n+  \/\/ can avoid call via card size arithmetic below instead\n+  p = _rs->addr_for_card_index(cur_index) + offset;\n+  assert(p < left, \"obj should start before left\");\n+  \/\/ While it is safe to ask an object its size in the loop that\n+  \/\/ follows, the loop should never be needed because:\n+  \/\/ [TODO: assert as many of these conditions as one may be able to\n+  \/\/  efficiently check.]\n+  \/\/ 1. we ask this question only for regions in the old generation\n+  \/\/ 2. there is no direct allocation ever by mutators in old generation\n+  \/\/    regions. Only GC will ever allocate in old regions, and then\n+  \/\/    too only during promotion\/evacuation phases. Thus there is no danger\n+  \/\/    of races between reading and writing the object start array, or\n+  \/\/    of asking partially initialized objects their size.\n+  \/\/ 3. only GC asks this question during phases when it is not concurrently\n+  \/\/    evacuating\/promoting, viz. during concurrent root scanning (before\n+  \/\/    the evacuation phase) and during concurrent update refs (after the\n+  \/\/    evacuation phase) of young collections. This is never called\n+  \/\/    during old or global collections.\n+  \/\/ 4. Every allocation under TAMS updates the object start array.\n+  \/\/    [TODO: Compare with the performance of a logarithmic BOT alternative,\n+  \/\/    e.g. as used by G1.]\n+  NOT_PRODUCT(obj = cast_to_oop(p);)\n+#define WALK_FORWARD_IN_BLOCK_START false\n+  while (WALK_FORWARD_IN_BLOCK_START && p + obj->size() < left) {\n+    p += obj->size();\n+  }\n+#undef WALK_FORWARD_IN_BLOCK_START \/\/ false\n+  \/\/ Recall that we already dealt with the co-initial object case above\n+  assert(p < left, \"obj should start before left\");\n+  assert(p + obj->size() > left, \"obj should end after left\");\n+  return p;\n+}\n+\n@@ -351,1 +433,1 @@\n-  if (!_scc->has_object(index)) {\n+  if (!_scc->starts_object(index)) {\n@@ -406,1 +488,1 @@\n-      if (_scc->has_object(end_card_index) &&\n+      if (_scc->starts_object(end_card_index) &&\n@@ -414,1 +496,1 @@\n-      if (_scc->has_object(index)) {\n+      if (_scc->starts_object(index)) {\n@@ -465,8 +547,1 @@\n-template<typename RememberedSet>\n-template <typename ClosureType>\n-inline void ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range,\n-                                                          ClosureType *cl, bool is_concurrent, uint worker_id) {\n-  process_clusters(first_cluster, count, end_of_range, cl, false, is_concurrent, worker_id);\n-}\n-\n-\/\/ Process all objects starting within count clusters beginning with first_cluster for which the start address is\n+\/\/ Process all objects starting within count clusters beginning with first_cluster and for which the start address is\n@@ -474,2 +549,2 @@\n-\/\/ even if its end reaches beyond end_of_range. Object arrays, on the other hand,s are precisely dirtied and only\n-\/\/ the portions of the array on dirty cards need to be scanned.\n+\/\/ even if its end reaches beyond end_of_range. Object arrays, on the other hand, are precisely dirtied and\n+\/\/ only the portions of the array on dirty cards need to be scanned.\n@@ -477,1 +552,1 @@\n-\/\/ Do not CANCEL within process_clusters.  It is assumed that if a worker thread accepts responsbility for processing\n+\/\/ Do not CANCEL within process_clusters.  It is assumed that if a worker thread accepts responsibility for processing\n@@ -479,1 +554,1 @@\n-\/\/ degenerated execution.\n+\/\/ degenerated execution, leading to dangling references.\n@@ -482,6 +557,2 @@\n-void ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range,\n-                                                               ClosureType *cl, bool write_table, bool is_concurrent, uint worker_id) {\n-\n-  \/\/ Unlike traditional Shenandoah marking, the old-gen resident objects that are examined as part of the remembered set are not\n-  \/\/ always themselves marked.  Each such object will be scanned exactly once.  Any young-gen objects referenced from the remembered\n-  \/\/ set will be marked and then subsequently scanned.\n+void ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord* end_of_range,\n+                                                               ClosureType* cl, bool use_write_table, uint worker_id) {\n@@ -495,4 +566,76 @@\n-  \/\/ by marking them (when marking) or evacuating them (when updating refereces).\n-\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahMarkingContext* ctx;\n+  \/\/ by marking them (when marking) or evacuating them (when updating references).\n+\n+  \/\/ start and end addresses of range of objects to be scanned, clipped to end_of_range\n+  const size_t start_card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  const HeapWord* start_addr = _rs->addr_for_card_index(start_card_index);\n+  \/\/ clip at end_of_range (exclusive)\n+  HeapWord* end_addr = MIN2(end_of_range, (HeapWord*)start_addr + (count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster\n+                                                                   * CardTable::card_size_in_words()));\n+  assert(start_addr < end_addr, \"Empty region?\");\n+\n+  const size_t whole_cards = (end_addr - start_addr + CardTable::card_size_in_words() - 1)\/CardTable::card_size_in_words();\n+  const size_t end_card_index = start_card_index + whole_cards - 1;\n+  log_debug(gc, remset)(\"Worker %u: cluster = \" SIZE_FORMAT \" count = \" SIZE_FORMAT \" eor = \" INTPTR_FORMAT\n+                        \" start_addr = \" INTPTR_FORMAT \" end_addr = \" INTPTR_FORMAT \" cards = \" SIZE_FORMAT,\n+                        worker_id, first_cluster, count, p2i(end_of_range), p2i(start_addr), p2i(end_addr), whole_cards);\n+\n+  \/\/ use_write_table states whether we are using the card table that is being\n+  \/\/ marked by the mutators. If false, we are using a snapshot of the card table\n+  \/\/ that is not subject to modifications. Even when this arg is true, and\n+  \/\/ the card table is being actively marked, SATB marking ensures that we need not\n+  \/\/ worry about cards marked after the processing here has passed them.\n+  const CardValue* const ctbm = _rs->get_card_table_byte_map(use_write_table);\n+\n+  \/\/ If old gen evacuation is active, ctx will hold the completed marking of\n+  \/\/ old generation objects. We'll only scan objects that are marked live by\n+  \/\/ the old generation marking. These include objects allocated since the\n+  \/\/ start of old generation marking (being those above TAMS).\n+  const ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  const ShenandoahMarkingContext* ctx = heap->is_old_bitmap_stable() ?\n+                                        heap->marking_context() : nullptr;\n+\n+  \/\/ The region we will scan is the half-open interval [start_addr, end_addr),\n+  \/\/ and lies entirely within a single region.\n+  const ShenandoahHeapRegion* region = ShenandoahHeap::heap()->heap_region_containing(start_addr);\n+  assert(region->contains(end_addr - 1), \"Slice shouldn't cross regions\");\n+\n+  \/\/ This code may have implicit assumptions of examining only old gen regions.\n+  assert(region->is_old(), \"We only expect to be processing old regions\");\n+  assert(!region->is_humongous(), \"Humongous regions can be processed more efficiently;\"\n+                                  \"see process_humongous_clusters()\");\n+  \/\/ tams and ctx below are for old generation marking. As such, young gen roots must\n+  \/\/ consider everything above tams, since it doesn't represent a TAMS for young gen's\n+  \/\/ SATB marking.\n+  const HeapWord* tams = (ctx == nullptr ? region->bottom() : ctx->top_at_mark_start(region));\n+\n+  NOT_PRODUCT(ShenandoahCardStats stats(whole_cards, card_stats(worker_id));)\n+\n+  \/\/ In the case of imprecise marking, we remember the lowest address\n+  \/\/ scanned in a range of dirty cards, as we work our way left from the\n+  \/\/ highest end_addr. This serves as another upper bound on the address we will\n+  \/\/ scan as we move left over each contiguous range of dirty cards.\n+  HeapWord* upper_bound = nullptr;\n+\n+  \/\/ Starting at the right end of the address range, walk backwards accumulating\n+  \/\/ a maximal dirty range of cards, then process those cards.\n+  ssize_t cur_index = (ssize_t) end_card_index;\n+  assert(cur_index > 0, \"Overflow\");\n+  assert(((ssize_t)start_card_index) >= 0, \"Overflow\");\n+  while (cur_index >= (ssize_t)start_card_index) {\n+\n+    \/\/ We'll continue the search starting with the card for the upper bound\n+    \/\/ address identified by the last dirty range that we processed, if any,\n+    \/\/ skipping any cards at higher addresses.\n+    if (upper_bound != nullptr) {\n+      ssize_t right_index = _rs->card_index_for_addr(upper_bound);\n+      assert(right_index > 0, \"Overflow\");\n+      cur_index = MIN2(cur_index, right_index);\n+      assert(upper_bound < end_addr, \"Program logic\");\n+      end_addr  = upper_bound;   \/\/ lower end_addr\n+      upper_bound = nullptr;     \/\/ and clear upper_bound\n+      if (end_addr <= start_addr) {\n+        assert(right_index <= (ssize_t)start_card_index, \"Program logic\");\n+        \/\/ We are done with our cluster\n+        return;\n+      }\n+    }\n@@ -500,5 +643,2 @@\n-  if (heap->is_old_bitmap_stable()) {\n-    ctx = heap->marking_context();\n-  } else {\n-    ctx = nullptr;\n-  }\n+    if (ctbm[cur_index] == CardTable::dirty_card_val()) {\n+      \/\/ ==== BEGIN DIRTY card range processing ====\n@@ -506,23 +646,3 @@\n-  size_t cur_cluster = first_cluster;\n-  size_t cur_count = count;\n-  size_t card_index = cur_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n-  HeapWord* start_of_range = _rs->addr_for_card_index(card_index);\n-  ShenandoahHeapRegion* r = heap->heap_region_containing(start_of_range);\n-  assert(end_of_range <= r->top(), \"process_clusters() examines one region at a time\");\n-\n-  NOT_PRODUCT(ShenandoahCardStats stats(ShenandoahCardCluster<RememberedSet>::CardsPerCluster, card_stats(worker_id));)\n-\n-  while (cur_count-- > 0) {\n-    \/\/ TODO: do we want to check cancellation in inner loop, on every card processed?  That would be more responsive,\n-    \/\/ but require more overhead for checking.\n-    card_index = cur_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n-    size_t end_card_index = card_index + ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n-    cur_cluster++;\n-    size_t next_card_index = 0;\n-\n-    assert(stats.is_clean(), \"Error\");\n-    while (card_index < end_card_index) {    \/\/ TODO: understand why end_of_range is needed.\n-      if (_rs->addr_for_card_index(card_index) > end_of_range) {\n-        cur_count = 0;\n-        card_index = end_card_index;\n-        break;\n+      const size_t dirty_r = cur_index;  \/\/ record right end of dirty range (inclusive)\n+      while (--cur_index >= (ssize_t)start_card_index && ctbm[cur_index] == CardTable::dirty_card_val()) {\n+        \/\/ walk back over contiguous dirty cards to find left end of dirty range (inclusive)\n@@ -530,16 +650,50 @@\n-      bool is_dirty = (write_table)? is_write_card_dirty(card_index): is_card_dirty(card_index);\n-      bool has_object = _scc->has_object(card_index);\n-      NOT_PRODUCT(stats.increment_card_cnt(is_dirty);)\n-      if (is_dirty) {\n-        size_t prev_card_index = card_index;\n-        if (has_object) {\n-          \/\/ Scan all objects that start within this card region.\n-          size_t start_offset = _scc->get_first_start(card_index);\n-          HeapWord *p = _rs->addr_for_card_index(card_index);\n-          HeapWord *card_start = p;\n-          HeapWord *endp = p + CardTable::card_size_in_words();\n-          assert(!r->is_humongous(), \"Process humongous regions elsewhere\");\n-\n-          if (endp > end_of_range) {\n-            endp = end_of_range;\n-            next_card_index = end_card_index;\n+      \/\/ [dirty_l, dirty_r] is a \"maximal\" closed interval range of dirty card indices:\n+      \/\/ it may not be maximal if we are using the write_table, because of concurrent\n+      \/\/ mutations dirtying the card-table. It may also not be maximal if an upper bound\n+      \/\/ was established by the scan of the previous chunk.\n+      const size_t dirty_l = cur_index + 1;   \/\/ record left end of dirty range (inclusive)\n+      \/\/ Check that we identified a boundary on our left\n+      assert(ctbm[dirty_l] == CardTable::dirty_card_val(), \"First card in range should be dirty\");\n+      assert(dirty_l == start_card_index || use_write_table\n+             || ctbm[dirty_l - 1] == CardTable::clean_card_val(),\n+             \"Interval isn't maximal on the left\");\n+      assert(dirty_r >= dirty_l, \"Error\");\n+      assert(ctbm[dirty_r] == CardTable::dirty_card_val(), \"Last card in range should be dirty\");\n+      \/\/ Record alternations, dirty run length, and dirty card count\n+      NOT_PRODUCT(stats.record_dirty_run(dirty_r - dirty_l + 1);)\n+\n+      \/\/ Find first object that starts this range:\n+      \/\/ [left, right) is a maximal right-open interval of dirty cards\n+      HeapWord* left = _rs->addr_for_card_index(dirty_l);        \/\/ inclusive\n+      HeapWord* right = _rs->addr_for_card_index(dirty_r + 1);   \/\/ exclusive\n+      \/\/ Clip right to end_addr established above (still exclusive)\n+      right = MIN2(right, end_addr);\n+      assert(right <= region->top() && end_addr <= region->top(), \"Busted bounds\");\n+      const MemRegion mr(left, right);\n+\n+      \/\/ TODO: cache so as to not call block_start() repeatedly\n+      \/\/ on a very large object. Check: may be upper_bound below\n+      \/\/ and its use above already takes care of this.\n+      HeapWord* p = _scc->block_start(dirty_l);\n+      oop obj = cast_to_oop(p);;\n+\n+      \/\/ PREFIX: The object that straddles into this range of dirty cards\n+      \/\/ from the left may be subject to special treatment unless\n+      \/\/ it is an object array.\n+#define OBJ_MARK_IMPRECISE false   \/\/ indicates that objects are _always_ imprecisely marked:\n+                                   \/\/ this is not true today, as GC closures are precise, but\n+                                   \/\/ compiler & interpreter are imprecise.\n+      if (p < left && !obj->is_objArray()) {\n+        if (OBJ_MARK_IMPRECISE) {\n+          \/\/ If we always dirty non-array objects imprecisely (i.e. only the head),\n+          \/\/ then we'll scan this object when we process the (potentially dirty)\n+          \/\/ card in which it starts, so we can skip over it now.\n+          log_debug(gc, remset)(\"Skipping non-objArray suffix in [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+                                p2i(left), p2i(p + obj->size()));\n+          \/\/ It's tempting to assert that if the card on which it starts is clean,\n+          \/\/ then it must contain no cross-generational pointers. However, that check\n+          \/\/ would be too strong, because such mutations will be found by\n+          \/\/ our SATB barrier processing, and need not be found here.\n+          assert(obj == cast_to_oop(p), \"Inconsistency detected\");\n+          if (ctx == nullptr || p >= tams) {\n+            p += obj->size();\n@@ -547,6 +701,1 @@\n-            \/\/ endp either points to start of next card region, or to the next object that needs to be scanned, which may\n-            \/\/ reside in some successor card region.\n-\n-            \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n-            \/\/ failure if endp points to end of heap.\n-            next_card_index = card_index + (endp - card_start) \/ CardTable::card_size_in_words();\n+            p = ctx->get_next_marked_addr(p, right);\n@@ -554,27 +703,17 @@\n-\n-          p += start_offset;\n-          while (p < endp) {\n-            oop obj = cast_to_oop(p);\n-            NOT_PRODUCT(stats.increment_obj_cnt(is_dirty);)\n-\n-            \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n-            if (!ctx || ctx->is_marked(obj)) {\n-              \/\/ Future TODO:\n-              \/\/ For improved efficiency, we might want to give special handling of obj->is_objArray().  In\n-              \/\/ particular, in that case, we might want to divide the effort for scanning of a very long object array\n-              \/\/ between multiple threads.  Also, skip parts of the array that are not marked as dirty.\n-              if (obj->is_objArray()) {\n-                objArrayOop array = objArrayOop(obj);\n-                int len = array->length();\n-                array->oop_iterate_range(cl, 0, len);\n-                NOT_PRODUCT(stats.increment_scan_cnt(is_dirty);)\n-              } else if (obj->is_instance()) {\n-                obj->oop_iterate(cl);\n-                NOT_PRODUCT(stats.increment_scan_cnt(is_dirty);)\n-              } else {\n-                \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n-                \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n-                \/\/ We skip iterating over the klass pointer since we know that\n-                \/\/ Universe::TypeArrayKlass never moves.\n-                assert (obj->is_typeArray(), \"should be type array\");\n-              }\n+          assert(p > left, \"Should have processed into the range\");\n+        } else {\n+          \/\/ The compiler and interpreter typically dirty the head of an object,\n+          \/\/ but GC may dirty the object precisely. To handle this, we check the\n+          \/\/ head card of the object here and, if dirty, scan the object in its\n+          \/\/ entirety. If we find the head card clean, we'll scan only the\n+          \/\/ portion of the object lying in the dirty card range below,\n+          \/\/ assuming precise marking by GC closures.\n+          const size_t h_index = _rs->card_index_for_addr(p);\n+\t  if (ctbm[h_index] == CardTable::dirty_card_val()) {\n+            \/\/ Scan or skip, and remember for next chunk\n+            upper_bound = p;   \/\/ remember upper bound for next chunk\n+            if (p < start_addr) {\n+              \/\/ if object starts in a previous cluster, it'll be handled\n+              \/\/ in its entirety by the thread processing that cluster; we can\n+              \/\/ skip over it.\n+              assert(obj == cast_to_oop(p), \"Inconsistency detected\");\n@@ -583,4 +722,4 @@\n-              \/\/ This object is not marked so we don't scan it.  Containing region r is initialized above.\n-              HeapWord* tams = ctx->top_at_mark_start(r);\n-              if (p >= tams) {\n-                p += obj->size();\n+              assert(obj == cast_to_oop(p), \"Inconsistency detected\");\n+              if (ctx == nullptr || ctx->is_marked(obj)) {\n+                \/\/ Scan the object in its entirety\n+                p += obj->oop_iterate_size(cl);\n@@ -588,0 +727,2 @@\n+                assert(p < tams, \"Error 1 in ctx\/marking\/tams logic\");\n+                \/\/ Skip over any intermediate dead objects\n@@ -589,0 +730,1 @@\n+                assert(p <= tams, \"Error 2 in ctx\/marking\/tams logic\");\n@@ -591,0 +733,1 @@\n+            assert(p > left, \"Should have processed into the range\");\n@@ -592,5 +735,18 @@\n-          if (p > endp) {\n-            card_index = card_index + (p - card_start) \/ CardTable::card_size_in_words();\n-          } else {                  \/\/ p == endp\n-            card_index = next_card_index;\n-          }\n+        }\n+      }\n+\n+      size_t i = 0;\n+      HeapWord* last_p = nullptr;\n+\n+      \/\/ BODY: Deal with (other) objects in this dirty card range\n+      while (p < right) {\n+        obj = cast_to_oop(p);\n+        \/\/ walk right scanning eligible objects\n+        if (ctx == nullptr || ctx->is_marked(obj)) {\n+          \/\/ we need to remember the last object ptr we scanned, in case we need to\n+          \/\/ complete a partial suffix scan after mr, see below\n+          last_p = p;\n+          \/\/ apply the closure to the oops in the portion of\n+          \/\/ the object within mr.\n+          p += obj->oop_iterate_size(cl, mr);\n+          NOT_PRODUCT(i++);\n@@ -598,2 +754,6 @@\n-          \/\/ Card is dirty but has no object.  Card will have been scanned during scan of a previous cluster.\n-          card_index++;\n+          \/\/ forget the last object pointer we remembered\n+          last_p = nullptr;\n+          assert(p < tams, \"Tams and above are implicitly marked in ctx\");\n+          \/\/ object under tams isn't marked: skip to next live object\n+          p = ctx->get_next_marked_addr(p, tams);\n+          assert(p <= tams, \"Error 3 in ctx\/marking\/tams logic\");\n@@ -601,32 +761,1 @@\n-      } else {\n-        if (has_object) {\n-          \/\/ Card is clean but has object.\n-          \/\/ Scan the last object that starts within this card memory if it spans at least one dirty card within this cluster\n-          \/\/ or if it reaches into the next cluster.\n-          size_t start_offset = _scc->get_last_start(card_index);\n-          HeapWord *card_start = _rs->addr_for_card_index(card_index);\n-          HeapWord *p = card_start + start_offset;\n-          oop obj = cast_to_oop(p);\n-\n-          size_t last_card;\n-          if (!ctx || ctx->is_marked(obj)) {\n-            HeapWord *nextp = p + obj->size();\n-            NOT_PRODUCT(stats.increment_obj_cnt(is_dirty);)\n-\n-            \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n-            \/\/ failure if nextp points to end of heap. Must also not attempt to read past last\n-            \/\/ valid index for card table.\n-            last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words();\n-            last_card = MIN2(last_card, last_valid_index());\n-\n-            bool reaches_next_cluster = (last_card > end_card_index);\n-            bool spans_dirty_within_this_cluster = false;\n-\n-            if (!reaches_next_cluster) {\n-              for (size_t span_card = card_index+1; span_card <= last_card; span_card++) {\n-                if ((write_table)? _rs->is_write_card_dirty(span_card): _rs->is_card_dirty(span_card)) {\n-                  spans_dirty_within_this_cluster = true;\n-                  break;\n-                }\n-              }\n-            }\n+      }\n@@ -634,36 +763,16 @@\n-            \/\/ TODO: only iterate over this object if it spans dirty within this cluster or within following clusters.\n-            \/\/ Code as written is known not to examine a zombie object because either the object is marked, or we are\n-            \/\/ not using the mark-context to differentiate objects, so the object is known to have been coalesced and\n-            \/\/ filled if it is not \"live\".\n-\n-            if (reaches_next_cluster || spans_dirty_within_this_cluster) {\n-              if (obj->is_objArray()) {\n-                objArrayOop array = objArrayOop(obj);\n-                int len = array->length();\n-                array->oop_iterate_range(cl, 0, len);\n-                NOT_PRODUCT(stats.increment_scan_cnt(is_dirty);)\n-              } else if (obj->is_instance()) {\n-                obj->oop_iterate(cl);\n-                NOT_PRODUCT(stats.increment_scan_cnt(is_dirty);)\n-              } else {\n-                \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n-                \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n-                \/\/ We skip iterating over the klass pointer since we know that\n-                \/\/ Universe::TypeArrayKlass never moves.\n-                assert (obj->is_typeArray(), \"should be type array\");\n-              }\n-            }\n-          } else {\n-            \/\/ The object that spans end of this clean card is not marked, so no need to scan it or its\n-            \/\/ unmarked neighbors.  Containing region r is initialized above.\n-            HeapWord* tams = ctx->top_at_mark_start(r);\n-            HeapWord* nextp;\n-            if (p >= tams) {\n-              nextp = p + obj->size();\n-            } else {\n-              nextp = ctx->get_next_marked_addr(p, tams);\n-            }\n-            last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words();\n-          }\n-          \/\/ Increment card_index to account for the spanning object, even if we didn't scan it.\n-          card_index = (last_card > card_index)? last_card: card_index + 1;\n+      \/\/ TODO: if an objArray then only use mr, else just iterate over entire object;\n+      \/\/ that would avoid the special treatment of suffix below.\n+\n+      \/\/ SUFFIX: Fix up a possible incomplete scan at right end of window\n+      \/\/ by scanning the portion of a non-objArray that wasn't done.\n+      if (p > right && last_p != nullptr) {\n+        assert(last_p < right, \"Error\");\n+        \/\/ check if last_p suffix needs scanning\n+        const oop last_obj = cast_to_oop(last_p);\n+        if (!last_obj->is_objArray()) {\n+          \/\/ scan the remaining suffix of the object\n+          const MemRegion last_mr(right, p);\n+          assert(p == last_p + obj->size(), \"Would miss portion of last_obj\");\n+          last_obj->oop_iterate(cl, last_mr);\n+          log_debug(gc, remset)(\"Fixed up non-objArray suffix scan in [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+                                p2i(last_mr.start()), p2i(last_mr.end()));\n@@ -671,2 +780,2 @@\n-          \/\/ Card is clean and has no object.  No need to clean this card.\n-          card_index++;\n+          log_debug(gc, remset)(\"Skipped suffix scan of objArray in [\" INTPTR_FORMAT \", \" INTPTR_FORMAT \")\",\n+                                p2i(right), p2i(p));\n@@ -675,3 +784,19 @@\n-    } \/\/ end of a range of cards in current cluster\n-    NOT_PRODUCT(stats.update_run(true \/* record *\/);)\n-  } \/\/ end of all clusters\n+#undef OBJ_MARK_IMPRECISE \/\/ false\n+      NOT_PRODUCT(stats.record_scan_obj_cnt(i);)\n+\n+      \/\/ ==== END   DIRTY card range processing ====\n+    } else {\n+      \/\/ ==== BEGIN CLEAN card range processing ====\n+\n+      assert(ctbm[cur_index] == CardTable::clean_card_val(), \"Error\");\n+      \/\/ walk back over contiguous clean cards\n+      size_t i = 0;\n+      while (--cur_index >= (ssize_t)start_card_index && ctbm[cur_index] == CardTable::clean_card_val()) {\n+        NOT_PRODUCT(i++);\n+      }\n+      \/\/ Record alternations, clean run length, and clean card count\n+      NOT_PRODUCT(stats.record_clean_run(i);)\n+\n+      \/\/ ==== END CLEAN card range processing ====\n+    }\n+  }\n@@ -686,2 +811,1 @@\n-                                                                    HeapWord *end_of_range, ClosureType *cl, bool write_table,\n-                                                                    bool is_concurrent) {\n+                                                                    HeapWord *end_of_range, ClosureType *cl, bool use_write_table) {\n@@ -698,1 +822,1 @@\n-  start_region->oop_iterate_humongous_slice(cl, true, first_cluster_addr, spanned_words, write_table, is_concurrent);\n+  start_region->oop_iterate_humongous_slice(cl, true, first_cluster_addr, spanned_words, use_write_table);\n@@ -701,0 +825,2 @@\n+\n+\/\/ This method takes a region & determines the end of the region that the worker can scan.\n@@ -706,1 +832,4 @@\n-                                                              bool is_concurrent, uint worker_id) {\n+                                                              uint worker_id) {\n+\n+  \/\/ This is called only for young gen collection, when we scan old gen regions\n+  assert(region->is_old(), \"Expecting an old region\");\n@@ -708,3 +837,0 @@\n-  size_t cluster_size =\n-    CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n-  size_t words = clusters * cluster_size;\n@@ -739,11 +865,4 @@\n-  \/\/ Note that end_of_range may point to the middle of a cluster because region->top() or region->get_update_watermark() may\n-  \/\/ be less than start_of_range + words.\n-\n-  \/\/ We want to assure that our process_clusters() request spans all relevant clusters.  Note that each cluster\n-  \/\/ processed will avoid processing beyond end_of_range.\n-\n-  \/\/ Note that any object that starts between start_of_range and end_of_range, including humongous objects, will\n-  \/\/ be fully processed by process_clusters, even though the object may reach beyond end_of_range.\n-\n-  \/\/ If I am assigned to process a range that starts beyond end_of_range (top or update-watermark), we have no work to do.\n-\n+  \/\/ Note that end_of_range may point to the middle of a cluster because we limit scanning to\n+  \/\/ region->top() or region->get_update_watermark(). We avoid processing past end_of_range.\n+  \/\/ Objects that start between start_of_range and end_of_range, including humongous objects, will\n+  \/\/ be fully processed by process_clusters. In no case should we need to scan past end_of_range.\n@@ -753,1 +872,4 @@\n-      process_humongous_clusters(start_region, start_cluster_no, clusters, end_of_range, cl, use_write_table, is_concurrent);\n+      \/\/ TODO: ysr : This will be called multiple times with same start_region, but different start_cluster_no.\n+      \/\/ Check that it does the right thing here, and doesn't do redundant work. Also see if the call API\/interface\n+      \/\/ can be simplified.\n+      process_humongous_clusters(start_region, start_cluster_no, clusters, end_of_range, cl, use_write_table);\n@@ -755,1 +877,9 @@\n-      process_clusters(start_cluster_no, clusters, end_of_range, cl, use_write_table, is_concurrent, worker_id);\n+      \/\/ TODO: ysr The start_of_range calculated above is discarded and may be calculated again in process_clusters().\n+      \/\/ See if the redundant and wasted calculations can be avoided, and if the call parameters can be cleaned up.\n+      \/\/ It almost sounds like this set of methods needs a working class to stash away some useful info that can be\n+      \/\/ efficiently passed around amongst these methods, as well as related state. Note that we can't use\n+      \/\/ ShenandoahScanRemembered as there seems to be only one instance of that object for the heap which is shared\n+      \/\/ by all workers. Note that there are also task methods which call these which may have per worker storage.\n+      \/\/ We need to be careful however that if the number of workers changes dynamically that state isn't sequestered\n+      \/\/ and become obsolete.\n+      process_clusters(start_cluster_no, clusters, end_of_range, cl, use_write_table, worker_id);\n@@ -793,1 +923,1 @@\n-                                   false \/* is_write_table *\/, false \/* is_concurrent *\/);\n+                                   false \/* use_write_table *\/);\n@@ -795,1 +925,2 @@\n-        process_clusters(start_cluster_no, num_clusters, end_of_range, cl, false \/* is_concurrent *\/, 0);\n+        process_clusters(start_cluster_no, num_clusters, end_of_range, cl,\n+                         false \/* use_write_table *\/, 0 \/* fake worker id *\/);\n@@ -858,1 +989,1 @@\n-  if (_index > _total_chunks) {\n+  if (_index >= _total_chunks) {\n@@ -863,0 +994,2 @@\n+    \/\/ First worker that hits new_index == _total_chunks continues, other\n+    \/\/ contending workers return false.\n@@ -867,0 +1000,1 @@\n+  assert(new_index < _total_chunks, \"Error\");\n@@ -868,0 +1002,1 @@\n+  \/\/ Find the group number for the assigned chunk index\n@@ -871,1 +1006,0 @@\n-\n@@ -892,0 +1026,10 @@\n+\n+template<class T>\n+inline void ShenandoahVerifyNoYoungRefsClosure::work(T* p) {\n+  T o = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(o)) {\n+    oop obj = CompressedOops::decode_not_null(o);\n+    assert(!_heap->is_in_young(obj), \"Found a young ref\");\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":386,"deletions":242,"binary":false,"changes":628,"status":"modified"}]}