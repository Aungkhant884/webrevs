{"files":[{"patch":"@@ -170,1 +170,0 @@\n-\n@@ -182,0 +181,1 @@\n+  collection_set->set_immediate_trash(immediate_garbage);\n@@ -188,0 +188,1 @@\n+      \/\/ We can shrink old_evac_reserve() if the chosen collection set is smaller than maximum allowed.\n@@ -196,86 +197,0 @@\n-    ShenandoahYoungGeneration* young_generation = heap->young_generation();\n-    size_t young_evacuation_reserve = (young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n-\n-    \/\/ At this point, young_generation->available() does not know about recently discovered immediate garbage.\n-    \/\/ What memory it does think to be available is not entirely trustworthy because any available memory associated\n-    \/\/ with a region that is placed into the collection set becomes unavailable when the region is chosen\n-    \/\/ for the collection set.  We'll compute an approximation of young available.  If young_available is zero,\n-    \/\/ we'll need to borrow from old-gen in order to evacuate.  If there's nothing to borrow, we're going to\n-    \/\/ degenerate to full GC.\n-\n-    \/\/ TODO: young_available can include available (between top() and end()) within each young region that is not\n-    \/\/ part of the collection set.  Making this memory available to the young_evacuation_reserve allows a larger\n-    \/\/ young collection set to be chosen when available memory is under extreme pressure.  Implementing this \"improvement\"\n-    \/\/ is tricky, because the incremental construction of the collection set actually changes the amount of memory\n-    \/\/ available to hold evacuated young-gen objects.  As currently implemented, the memory that is available within\n-    \/\/ non-empty regions that are not selected as part of the collection set can be allocated by the mutator while\n-    \/\/ GC is evacuating and updating references.\n-\n-    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-    size_t free_affiliated_regions = immediate_regions + free_regions;\n-    size_t young_available = (free_affiliated_regions + young_generation->free_unaffiliated_regions()) * region_size_bytes;\n-\n-    size_t regions_available_to_loan = 0;\n-\n-    if (heap->mode()->is_generational()) {\n-      \/\/  Now that we've primed the collection set, we can figure out how much memory to reserve for evacuation\n-      \/\/  of young-gen objects.\n-      \/\/\n-      \/\/  YoungEvacuationReserve for young generation: how much memory are we reserving to hold the results\n-      \/\/     of evacuating young collection set regions?  This is typically smaller than the total amount\n-      \/\/     of available memory, and is also smaller than the total amount of marked live memory within\n-      \/\/     young-gen.  This value is the minimum of:\n-      \/\/       1. young_gen->available() + (old_gen->available - (OldEvacuationReserve + PromotionReserve))\n-      \/\/       2. young_gen->capacity() * ShenandoahEvacReserve\n-      \/\/\n-      \/\/     Note that any region added to the collection set will be completely evacuated and its memory will\n-      \/\/     be completely recycled at the end of GC.  The recycled memory will be at least as great as the\n-      \/\/     memory borrowed from old-gen.  Enforce that the amount borrowed from old-gen for YoungEvacuationReserve\n-      \/\/     is an integral number of entire heap regions.\n-      \/\/\n-      young_evacuation_reserve -= heap->get_old_evac_reserve();\n-\n-      \/\/ Though we cannot know the evacuation_supplement until after we have computed the collection set, we do\n-      \/\/ know that every young-gen region added to the collection set will have a net positive impact on available\n-      \/\/ memory within young-gen, since each contributes a positive amount of garbage to available.  Thus, even\n-      \/\/ without knowing the exact composition of the collection set, we can allow young_evacuation_reserve to\n-      \/\/ exceed young_available if there are empty regions available within old-gen to hold the results of evacuation.\n-\n-      ShenandoahGeneration* old_generation = heap->old_generation();\n-\n-      \/\/ Not all of what is currently available within young-gen can be reserved to hold the results of young-gen\n-      \/\/ evacuation.  This is because memory available within any heap region that is placed into the collection set\n-      \/\/ is not available to be allocated during evacuation.  To be safe, we assure that all memory required for evacuation\n-      \/\/ is available within \"virgin\" heap regions.\n-\n-      const size_t available_young_regions = free_regions + immediate_regions + young_generation->free_unaffiliated_regions();\n-      const size_t available_old_regions = old_generation->free_unaffiliated_regions();\n-      size_t already_reserved_old_bytes = heap->get_old_evac_reserve() + heap->get_promotion_reserve();\n-      size_t regions_reserved_for_evac_and_promotion = (already_reserved_old_bytes + region_size_bytes - 1) \/ region_size_bytes;\n-      regions_available_to_loan = available_old_regions - regions_reserved_for_evac_and_promotion;\n-\n-      if (available_young_regions * region_size_bytes < young_evacuation_reserve) {\n-        \/\/ Try to borrow old-gen regions in order to avoid shrinking young_evacuation_reserve\n-        size_t loan_request = young_evacuation_reserve - available_young_regions * region_size_bytes;\n-        size_t loaned_region_request = (loan_request + region_size_bytes - 1) \/ region_size_bytes;\n-        if (loaned_region_request > regions_available_to_loan) {\n-          \/\/ Scale back young_evacuation_reserve to consume all available young and old regions.  After the\n-          \/\/ collection set is chosen, we may get some of this memory back for pacing allocations during evacuation\n-          \/\/ and update refs.\n-          loaned_region_request = regions_available_to_loan;\n-          young_evacuation_reserve = (available_young_regions + loaned_region_request) * region_size_bytes;\n-        } else {\n-          \/\/ No need to scale back young_evacuation_reserve.\n-        }\n-      } else {\n-        \/\/ No need scale back young_evacuation_reserve and no need to borrow from old-gen.  We may even have some\n-        \/\/ available_young_regions to support allocation pacing.\n-      }\n-\n-    } else if (young_evacuation_reserve > young_available) {\n-      \/\/ In non-generational mode, there's no old-gen memory to borrow from\n-      young_evacuation_reserve = young_available;\n-    }\n-\n-    heap->set_young_evac_reserve(young_evacuation_reserve);\n-\n@@ -285,49 +200,0 @@\n-\n-    \/\/ Now compute the evacuation supplement, which is extra memory borrowed from old-gen that can be allocated\n-    \/\/ by mutators while GC is working on evacuation and update-refs.\n-\n-    \/\/ During evacuation and update refs, we will be able to allocate any memory that is currently available\n-    \/\/ plus any memory that can be borrowed on the collateral of the current collection set, reserving a certain\n-    \/\/ percentage of the anticipated replenishment from collection set memory to be allocated during the subsequent\n-    \/\/ concurrent marking effort.  This is how much I can repay.\n-    size_t potential_supplement_regions = collection_set->get_young_region_count();\n-\n-    \/\/ Though I can repay potential_supplement_regions, I can't borrow them unless they are available in old-gen.\n-    if (potential_supplement_regions > regions_available_to_loan) {\n-      potential_supplement_regions = regions_available_to_loan;\n-    }\n-\n-    size_t potential_evac_supplement;\n-\n-    \/\/ How much of the potential_supplement_regions will be consumed by young_evacuation_reserve: borrowed_evac_regions.\n-    const size_t available_unaffiliated_young_regions = young_generation->free_unaffiliated_regions();\n-    const size_t available_affiliated_regions = free_regions + immediate_regions;\n-    const size_t available_young_regions = available_unaffiliated_young_regions + available_affiliated_regions;\n-    size_t young_evac_regions = (young_evacuation_reserve + region_size_bytes - 1) \/ region_size_bytes;\n-    size_t borrowed_evac_regions = (young_evac_regions > available_young_regions)? young_evac_regions - available_young_regions: 0;\n-\n-    potential_supplement_regions -= borrowed_evac_regions;\n-    potential_evac_supplement = potential_supplement_regions * region_size_bytes;\n-\n-    \/\/ Leave some allocation runway for subsequent concurrent mark phase.\n-    potential_evac_supplement = (potential_evac_supplement * ShenandoahBorrowPercent) \/ 100;\n-\n-    heap->set_alloc_supplement_reserve(potential_evac_supplement);\n-\n-    size_t promotion_budget = heap->get_promotion_reserve();\n-    size_t old_evac_budget = heap->get_old_evac_reserve();\n-    size_t alloc_budget_evac_and_update = potential_evac_supplement + young_available;\n-\n-    \/\/ TODO: young_available, which feeds into alloc_budget_evac_and_update is lacking memory available within\n-    \/\/ existing young-gen regions that were not selected for the collection set.  Add this in and adjust the\n-    \/\/ log message (where it says \"empty-region allocation budget\").\n-\n-    log_info(gc, ergo)(\"Memory reserved for evacuation and update-refs includes promotion budget: \" SIZE_FORMAT\n-                       \"%s, young evacuation budget: \" SIZE_FORMAT \"%s, old evacuation budget: \" SIZE_FORMAT\n-                       \"%s, empty-region allocation budget: \" SIZE_FORMAT \"%s, including supplement: \" SIZE_FORMAT \"%s\",\n-                       byte_size_in_proper_unit(promotion_budget), proper_unit_for_byte_size(promotion_budget),\n-                       byte_size_in_proper_unit(young_evacuation_reserve), proper_unit_for_byte_size(young_evacuation_reserve),\n-                       byte_size_in_proper_unit(old_evac_budget), proper_unit_for_byte_size(old_evac_budget),\n-                       byte_size_in_proper_unit(alloc_budget_evac_and_update),\n-                       proper_unit_for_byte_size(alloc_budget_evac_and_update),\n-                       byte_size_in_proper_unit(potential_evac_supplement), proper_unit_for_byte_size(potential_evac_supplement));\n@@ -356,0 +222,3 @@\n+\n+  size_t bytes_evacuated = collection_set->get_bytes_reserved_for_evacuation();\n+  log_info(gc, ergo)(\"Total Evacuation: \" SIZE_FORMAT \"%s\", bytes_evacuated, proper_unit_for_byte_size(bytes_evacuated));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":5,"deletions":136,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -46,0 +46,7 @@\n+\/\/ A bound on memory available for old-gen evacuation has been computed and is fed into the function by\n+\/\/ way of heap->get_evacuation_reserve().  When priming the old-gen collection set, we limit ourselves to\n+\/\/ evacuating no more than heap->get_old_evac_reserve() \/ ShenandoahEvacWaste bytes.\n+\/\/\n+\/\/ If prime_collection_set() does not consume all of this available memory, it will reduce the old_evac_reserve\n+\/\/ and increase the promoted_reserve(), making sure to not promote into the memory\n+\n@@ -54,0 +61,1 @@\n+  size_t collected_old_bytes = 0;\n@@ -63,30 +71,0 @@\n-  size_t max_old_evacuation_bytes = (heap->old_generation()->soft_max_capacity() * ShenandoahOldEvacReserve) \/ 100;\n-  const size_t young_evacuation_bytes = (heap->young_generation()->soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n-  const size_t ratio_bound_on_old_evac_bytes = (young_evacuation_bytes * ShenandoahOldEvacRatioPercent) \/ 100;\n-  if (max_old_evacuation_bytes > ratio_bound_on_old_evac_bytes) {\n-    max_old_evacuation_bytes = ratio_bound_on_old_evac_bytes;\n-  }\n-\n-  \/\/ Usually, old-evacuation is limited by the CPU bounds on effort.  However, it can also be bounded by available\n-  \/\/ memory within old-gen to hold the results of evacuation.  When we are bound by memory availability, we need\n-  \/\/ to account below for the loss of available memory from within each region that is added to the old-gen collection\n-  \/\/ set.\n-  size_t old_available = heap->old_generation()->available();\n-  size_t excess_old_capacity_for_evacuation;\n-  if (max_old_evacuation_bytes > old_available) {\n-    max_old_evacuation_bytes = old_available;\n-    excess_old_capacity_for_evacuation = 0;\n-  } else {\n-    excess_old_capacity_for_evacuation = old_available - max_old_evacuation_bytes;\n-  }\n-\n-  \/\/ promotion_budget_bytes represents an \"arbitrary\" bound on how many bytes can be consumed by young-gen\n-  \/\/ objects promoted into old-gen memory.  We need to avoid a scenario under which promotion of objects\n-  \/\/ depletes old-gen available memory to the point that there is insufficient memory to hold old-gen objects\n-  \/\/ that need to be evacuated from within the old-gen collection set.\n-  \/\/\n-  \/\/ Key idea: if there is not sufficient memory within old-gen to hold an object that wants to be promoted, defer\n-  \/\/ promotion until a subsequent evacuation pass.  Enforcement is provided at the time PLABs and shared allocations\n-  \/\/ in old-gen memory are requested.\n-\n-  const size_t promotion_budget_bytes = heap->get_promotion_reserve();\n@@ -94,2 +72,0 @@\n-  \/\/ old_evacuation_budget is an upper bound on the amount of live memory that can be evacuated.\n-  \/\/\n@@ -100,6 +76,1 @@\n-  \/\/ budget is constrained by availability of free memory.  See remaining_old_evacuation_budget below.\n-\n-  size_t old_evacuation_budget = (size_t) (max_old_evacuation_bytes \/ ShenandoahEvacWaste);\n-\n-  log_info(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s\",\n-                byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget));\n+  \/\/ budget is constrained by availability of free memory.\n@@ -107,0 +78,1 @@\n+  size_t old_evacuation_budget = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n@@ -109,0 +81,3 @@\n+  log_info(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s, candidates: %u\",\n+               byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n+               unprocessed_old_collection_candidates());\n@@ -110,3 +85,2 @@\n-  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent\n-  \/\/ old-gen concurrent marking phase and have not yet been collected is represented by\n-  \/\/ unprocessed_old_collection_candidates()\n+  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates()\n@@ -117,1 +91,0 @@\n-\n@@ -120,4 +93,1 @@\n-    if ((r->get_live_data_bytes() <= remaining_old_evacuation_budget) &&\n-        ((lost_evacuation_capacity + r->free() <= excess_old_capacity_for_evacuation)\n-         || (r->get_live_data_bytes() + r->free() <= remaining_old_evacuation_budget))) {\n-\n+    if (r->get_live_data_bytes() <= remaining_old_evacuation_budget) {\n@@ -129,10 +99,0 @@\n-      if (lost_evacuation_capacity > excess_old_capacity_for_evacuation) {\n-        \/\/ This is slightly conservative because we really only need to remove from the remaining evacuation budget\n-        \/\/ the amount by which lost_evacution_capacity exceeds excess_old_capacity_for_evacuation, but this is relatively\n-        \/\/ rare event and current thought is to be a bit conservative rather than mess up the math on code that is so\n-        \/\/ difficult to test and maintain...\n-\n-        \/\/ Once we have crossed the threshold of lost_evacuation_capacity exceeding excess_old_capacity_for_evacuation,\n-        \/\/ every subsequent iteration of this loop will further decrease remaining_old_evacuation_budget.\n-        remaining_old_evacuation_budget -= r->free();\n-      }\n@@ -142,0 +102,1 @@\n+      collected_old_bytes += r->garbage();\n@@ -149,2 +110,4 @@\n-    log_info(gc)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, \" SIZE_FORMAT \" %s)\",\n-                 included_old_regions, byte_size_in_proper_unit(evacuated_old_bytes), proper_unit_for_byte_size(evacuated_old_bytes));\n+    log_info(gc)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating: \"\n+                 SIZE_FORMAT \"%s, reclaiming: \" SIZE_FORMAT \"%s)\", included_old_regions,\n+                 byte_size_in_proper_unit(evacuated_old_bytes), proper_unit_for_byte_size(evacuated_old_bytes),\n+                 byte_size_in_proper_unit(collected_old_bytes), proper_unit_for_byte_size(collected_old_bytes));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":21,"deletions":58,"binary":false,"changes":79,"status":"modified"},{"patch":"@@ -130,1 +130,1 @@\n-      _heap->retire_plab(plab);\n+      _heap->retire_plab(plab, thread);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+  _old_garbage(0),\n@@ -97,0 +98,1 @@\n+    _old_garbage += r->garbage();\n@@ -118,0 +120,1 @@\n+  _old_garbage = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -60,0 +60,2 @@\n+  size_t                _old_garbage;        \/\/ How many bytes of old garbage are present in a mixed collection set?\n+\n@@ -109,0 +111,2 @@\n+  inline size_t get_old_garbage();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -84,0 +84,4 @@\n+size_t ShenandoahCollectionSet::get_old_garbage() {\n+  return _old_garbage;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -125,1 +125,1 @@\n-      _generation->scan_remembered_set();\n+      _generation->scan_remembered_set(true \/* is_concurrent *\/);\n@@ -225,0 +225,1 @@\n+\n@@ -240,1 +241,2 @@\n-    heap->set_promotion_reserve(0);\n+    heap->set_promoted_reserve(0);\n+    heap->reset_promoted_expended();\n@@ -702,1 +704,1 @@\n-    \/\/ heap->get_promotion_reserve() represents the amount of memory within old-gen's available memory that has\n+    \/\/ heap->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n@@ -752,1 +754,1 @@\n-        log_info(gc, ergo)(\"After generational memory budget adjustments, old avaiable: \" SIZE_FORMAT\n+        log_info(gc, ergo)(\"After generational memory budget adjustments, old available: \" SIZE_FORMAT\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -75,0 +75,1 @@\n+  \/\/ TODO: Why not age during degenerated cycles?  Investigate and fix, or explain why not.\n@@ -289,1 +290,1 @@\n-    heap->set_promotion_reserve(0);\n+    heap->set_promoted_reserve(0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -194,1 +194,1 @@\n-  heap->set_promotion_reserve(0);\n+  heap->set_promoted_reserve(0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -224,1 +224,1 @@\n-void  ShenandoahGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+void ShenandoahGeneration::prepare_regions_and_collection_set(bool concurrent) {\n@@ -227,0 +227,2 @@\n+  ShenandoahCollectionSet* collection_set = heap->collection_set();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -248,2 +250,1 @@\n-    heap->collection_set()->clear();\n-\n+    collection_set->clear();\n@@ -251,0 +252,4 @@\n+    size_t minimum_evacuation_reserve = ShenandoahOldCompactionReserve * region_size_bytes;\n+    size_t avail_evac_reserve_for_loan_to_young_gen = 0;\n+    size_t old_regions_loaned_for_young_evac = 0;\n+    size_t regions_available_to_loan = 0;\n@@ -252,0 +257,2 @@\n+      ShenandoahGeneration* old_generation = heap->old_generation();\n+      ShenandoahYoungGeneration* young_generation = heap->young_generation();\n@@ -258,16 +265,42 @@\n-      \/\/  PromotionReserve for old generation: how much memory are we reserving to hold the results of\n-      \/\/     promoting young-gen objects that have reached tenure age?  This value is not \"critical\".  If we\n-      \/\/     underestimate, certain promotions will simply be deferred.  The basis of this estimate is\n-      \/\/     historical precedent.  Conservatively, budget this value to be twice the amount of memory\n-      \/\/     promoted in previous GC pass.  Whenever the amount promoted during previous GC is zero,\n-      \/\/     including initial passes before any objects have reached tenure age, use live memory within\n-      \/\/     young-gen memory divided by (ShenandoahTenureAge multiplied by InitialTenuringThreshold) as the\n-      \/\/     the very conservative value of this parameter.  Note that during initialization, there is\n-      \/\/     typically plentiful old-gen memory so it's ok to be conservative with the initial estimates\n-      \/\/     of this value.  But PromotionReserve can be no larger than available memory.  In summary, we\n-      \/\/     compute PromotionReserve as the smaller of:\n-      \/\/      1. old_gen->available\n-      \/\/      2. young_gen->capacity() * ShenandoahEvacReserve\n-      \/\/      3. (bytes promoted by previous promotion) * 2 if (bytes promoted by previous promotion) is not zero\n-      \/\/      4. if (bytes promoted by previous promotion) is zero, divide young_gen->used()\n-      \/\/         by (ShenandoahTenureAge * InitialTenuringThreshold)\n+      \/\/ Calculate EvacuationReserve before PromotionReserve.  Evacuation is more critical than promotion.\n+      \/\/ If we cannot evacuate old-gen, we will not be able to reclaim old-gen memory.  Promotions are less\n+      \/\/ critical.  If we cannot promote, there may be degradation of young-gen memory because old objects\n+      \/\/ accumulate there until they can be promoted.  This increases the young-gen marking and evacuation work.\n+\n+      \/\/ Do not fill up old-gen memory with promotions.  Reserve some amount of memory for compaction purposes.\n+      size_t old_evacuation_reserve = 0;\n+      ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+      if (old_heuristics->unprocessed_old_collection_candidates() > 0) {\n+\n+        \/\/ Compute old_evacuation_reserve: how much memory are we reserving to hold the results of\n+        \/\/ evacuating old-gen heap regions?  In order to sustain a consistent pace of young-gen collections,\n+        \/\/ the goal is to maintain a consistent value for this parameter (when the candidate set is not\n+        \/\/ empty).  This value is the minimum of:\n+        \/\/   1. old_gen->available()\n+        \/\/   2. old-gen->capacity() * ShenandoahOldEvacReserve) \/ 100\n+        \/\/       (e.g. old evacuation should be no larger than 5% of old_gen capacity)\n+        \/\/   3. ((young_gen->capacity * ShenandoahEvacReserve \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100\n+        \/\/       (e.g. old evacuation should be no larger than 12% of young-gen evacuation)\n+\n+        old_evacuation_reserve = old_generation->available();\n+        if (old_generation->soft_max_capacity() * ShenandoahOldEvacReserve \/ 100 < old_evacuation_reserve) {\n+          old_evacuation_reserve = old_generation->soft_max_capacity() * ShenandoahOldEvacReserve \/ 100;\n+        }\n+        if (((((young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100) <\n+            old_evacuation_reserve) {\n+          old_evacuation_reserve =\n+            (((young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100;\n+        }\n+      }\n+\n+      if (old_evacuation_reserve < minimum_evacuation_reserve) {\n+        \/\/ Even if there's nothing to be evacuated on this cycle, we still need to reserve this memory for future\n+        \/\/ evacuations.  It is ok to loan this memory to young-gen if we don't need it for evacuation on this pass.\n+        avail_evac_reserve_for_loan_to_young_gen = minimum_evacuation_reserve - old_evacuation_reserve;\n+        old_evacuation_reserve = minimum_evacuation_reserve;\n+      }\n+      heap->set_old_evac_reserve(old_evacuation_reserve);\n+      heap->reset_old_evac_expended();\n+\n+      \/\/ Compute the young evauation reserve: This is how much memory is available for evacuating young-gen objects.\n+      \/\/ We ignore the possible effect of promotions, which reduce demand for young-gen evacuation memory.\n@@ -275,2 +308,3 @@\n-      \/\/     We don't yet know how much live memory.  Inside choose_collection_set(), after it computes live memory,\n-      \/\/     the PromotionReserve may be further reduced.\n+      \/\/ TODO: We could give special treatment to the regions that have reached promotion age, because we know their\n+      \/\/ live data is entirely eligible for promotion.  This knowledge can feed both into calculations of young-gen\n+      \/\/ evacuation reserve and promotion reserve.\n@@ -278,2 +312,4 @@\n-      \/\/      5. live bytes in young-gen divided by (ShenandoahTenureAge * InitialTenuringThreshold\n-      \/\/         if the number of bytes promoted by previous promotion is zero\n+      \/\/  young_evacuation_reserve for young generation: how much memory are we reserving to hold the results\n+      \/\/  of evacuating young collection set regions?  This is typically smaller than the total amount\n+      \/\/  of available memory, and is also smaller than the total amount of marked live memory within\n+      \/\/  young-gen.  This value is the smaller of\n@@ -281,0 +317,70 @@\n+      \/\/    1. (young_gen->capacity() * ShenandoahEvacReserve) \/ 100\n+      \/\/    2. (young_gen->available() + (old_gen->free_region_memory - old_evacuation_reserve);\n+      \/\/\n+      \/\/  ShenandoahEvacReserve represents the configured taget size of the evacuation region.  We can only honor\n+      \/\/  this target if there is memory available to hold the evacuations.  Memory is available if it is already\n+      \/\/  free within young gen, or if it can be borrowed from old gen.  Since we have not yet chosen the collection\n+      \/\/  sets, we do not yet know the exact accounting of how many regions will be freed by this collection pass.\n+      \/\/  What we do know is that there will be at least one evacuated young-gen region for each old-gen region that\n+      \/\/  is loaned to the evacuation effort (because regions to be collected consume more memory than the compacted\n+      \/\/  regions that will replace them).  In summary, if there are old-gen regions that are available to hold the\n+      \/\/  results of young-gen evacuations, it is safe to loan them for this purpose.  At this point, we have not yet\n+      \/\/  established a promotion_reserve.  We'll do that after we choose the collection set and analyze its impact\n+      \/\/  on available memory.\n+      \/\/\n+      \/\/ Though we cannot know the evacuation_supplement until after we have computed the collection set, we do\n+      \/\/ know that every young-gen region added to the collection set will have a net positive impact on available\n+      \/\/ memory within young-gen, since each contributes a positive amount of garbage to available.  Thus, even\n+      \/\/ without knowing the exact composition of the collection set, we can allow young_evacuation_reserve to\n+      \/\/ exceed young_available if there are empty regions available within old-gen to hold the results of evacuation.\n+\n+      size_t young_evacuation_reserve = (young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      \/\/ old evacuation can pack into existing partially used regions.  young evacuation and loans for young allocations\n+      \/\/ need to target regions that do not already hold any old-gen objects.  Round down.\n+      size_t net_available_old_regions = (old_generation->available() - old_evacuation_reserve) \/ region_size_bytes;\n+      regions_available_to_loan = old_generation->free_unaffiliated_regions();\n+      if (regions_available_to_loan > net_available_old_regions) {\n+        regions_available_to_loan = net_available_old_regions;\n+      }\n+      \/\/ Otherwise, the reason regions_available_to_loan is less than net_available_old_regions is because the\n+      \/\/ available memory is scattered between many partially used regions.\n+\n+      if (young_evacuation_reserve > young_generation->available()) {\n+        size_t short_fall = young_evacuation_reserve - young_generation->available();\n+        if (regions_available_to_loan * region_size_bytes >= short_fall) {\n+          old_regions_loaned_for_young_evac = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n+          regions_available_to_loan -= old_regions_loaned_for_young_evac;\n+        } else {\n+          old_regions_loaned_for_young_evac = regions_available_to_loan;\n+          regions_available_to_loan = 0;\n+          young_evacuation_reserve = young_generation->available() + regions_available_to_loan * region_size_bytes;\n+        }\n+      } else {\n+        old_regions_loaned_for_young_evac = 0;\n+      }\n+\n+      heap->set_young_evac_reserve(young_evacuation_reserve);\n+    } else {\n+      \/\/ Not generational mode: limit young evac reserve by young available; no need to establish old_evac_reserve.\n+      ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+      size_t young_evac_reserve = (young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      if (young_evac_reserve > young_generation->available()) {\n+        young_evac_reserve = young_generation->available();\n+      }\n+      heap->set_young_evac_reserve(young_evac_reserve);\n+    }\n+\n+    \/\/ TODO: young_available can include available (between top() and end()) within each young region that is not\n+    \/\/ part of the collection set.  Making this memory available to the young_evacuation_reserve allows a larger\n+    \/\/ young collection set to be chosen when available memory is under extreme pressure.  Implementing this \"improvement\"\n+    \/\/ is tricky, because the incremental construction of the collection set actually changes the amount of memory\n+    \/\/ available to hold evacuated young-gen objects.  As currently implemented, the memory that is available within\n+    \/\/ non-empty regions that are not selected as part of the collection set can be allocated by the mutator while\n+    \/\/ GC is evacuating and updating references.\n+\n+    _heuristics->choose_collection_set(collection_set, heap->old_heuristics());\n+\n+    \/\/ At this point, young_generation->available() knows about recently discovered immediate garbage.  We also\n+    \/\/ know the composition of the chosen collection set.\n+\n+    if (heap->mode()->is_generational()) {\n@@ -283,1 +389,6 @@\n-      size_t promotion_reserve = old_generation->available();\n+      size_t old_evacuation_committed = (size_t) (ShenandoahEvacWaste *\n+                                                  collection_set->get_old_bytes_reserved_for_evacuation());\n+      size_t young_evacuation_commited = (size_t) (ShenandoahEvacWaste *\n+                                                   collection_set->get_young_bytes_reserved_for_evacuation());\n+\n+      size_t immediate_garbage_regions = collection_set->get_immediate_trash() \/ region_size_bytes;\n@@ -285,3 +396,2 @@\n-      size_t max_young_evacuation = (young_generation->soft_max_capacity() * ShenandoahOldEvacReserve) \/ 100;\n-      if (max_young_evacuation < promotion_reserve) {\n-        promotion_reserve = max_young_evacuation;\n+      if (old_evacuation_committed < minimum_evacuation_reserve) {\n+        old_evacuation_committed = minimum_evacuation_reserve;\n@@ -290,0 +400,30 @@\n+      \/\/ Recompute old_regions_loaned_for_young_evac because young-gen collection set may not need all the memory\n+      \/\/ originally reserved.\n+      size_t young_evacuation_reserve_used = ShenandoahEvacWaste * collection_set->get_young_bytes_reserved_for_evacuation();\n+      heap->set_young_evac_reserve(young_evacuation_reserve_used);\n+\n+      \/\/ Adjust old_regions_loaned_for_young_evac to feed into calculations of promotion_reserve\n+      if (young_evacuation_reserve_used > young_generation->available()) {\n+        size_t short_fall = young_evacuation_reserve_used - young_generation->available();\n+        size_t revised_loan_for_young_evacuation = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n+        regions_available_to_loan += old_regions_loaned_for_young_evac;\n+        old_regions_loaned_for_young_evac = revised_loan_for_young_evacuation;\n+        regions_available_to_loan -= old_regions_loaned_for_young_evac;\n+      } else {\n+        regions_available_to_loan += old_regions_loaned_for_young_evac;\n+        old_regions_loaned_for_young_evac = 0;\n+      }\n+\n+      \/\/ Limit promotion_reserve so that we can set aside memory to be loaned from old-gen to young-gen.  This\n+      \/\/ value is not \"critical\".  If we underestimate, certain promotions will simply be deferred.  If we put\n+      \/\/ \"all the rest\" of old-gen memory into the promotion reserve, we'll have nothing left to loan to young-gen\n+      \/\/ during the evac and update phases of GC.  So we \"limit\" the sizes of the promotion budget to be the smaller of:\n+      \/\/\n+      \/\/  1. old_gen->available - old_evacuation_commitment - old_regions_loaned_for_young_evac * region_size_bytes\n+      \/\/  2. if (previously promoted > 0), 4 times the amount of memory promoted in previous GC pass\n+      \/\/  3. young_bytes_reserved_for_evacuation \/ ((0x02 << InitialTenuringThreshold) - 1)\n+      \/\/      (Assume exponential decay of objects, with number of objects of age 0 being twice the number\n+      \/\/       of objects of age 1, which is twice the number of objects of age 2, and so on.  For example,\n+      \/\/       if tenure age is 1, 1\/3 of young evac will be promoted.  If tenure age is 2, 1\/7 of young evac\n+      \/\/       will be promoted.)\n+\n@@ -291,6 +431,15 @@\n-      if (previously_promoted == 0) {\n-        \/\/ Very conservatively, assume linear population decay (rather than more typical exponential) and assume all of\n-        \/\/ used is live.\n-        size_t proposed_reserve = young_generation->used() \/ (ShenandoahAgingCyclePeriod * InitialTenuringThreshold);\n-        if (promotion_reserve > proposed_reserve) {\n-          promotion_reserve = proposed_reserve;\n+#undef KELVIN_TRACE_PROMOTION_BUDGET\n+#ifdef KELVIN_TRACE_PROMOTION_BUDGET\n+      printf(\"PromotionBudget: previously_promoted: \" SIZE_FORMAT \"\\n\", previously_promoted);\n+#endif\n+      assert(old_generation->available() > old_evacuation_committed, \"Cannot evacuate more than available\");\n+      size_t promotion_reserve = (old_generation->available() - old_evacuation_committed -\n+                                  old_regions_loaned_for_young_evac * region_size_bytes);\n+#ifdef KELVIN_TRACE_PROMOTION_BUDGET\n+      printf(\"PromotionBudget: initial promotion_reserve: \" SIZE_FORMAT \", old_avail: \" SIZE_FORMAT\n+             \", old_evac_commit: \" SIZE_FORMAT \", old regions loaned for young_evac: \" SIZE_FORMAT \"\\n\",\n+             promotion_reserve, old_generation->available(), old_evacuation_committed, old_regions_loaned_for_young_evac);\n+#endif\n+      if (previously_promoted > 0) {\n+        if (previously_promoted * 4 < promotion_reserve) {\n+          promotion_reserve = previously_promoted * 4;\n@@ -298,2 +447,21 @@\n-      } else if (previously_promoted * 2 < promotion_reserve) {\n-        promotion_reserve = previously_promoted * 2;\n+      }\n+#ifdef KELVIN_TRACE_PROMOTION_BUDGET\n+      printf(\"PromotionBudget: promotion_reserve limited by previously promoted: \" SIZE_FORMAT \"\\n\",\n+             promotion_reserve);\n+#endif\n+      size_t promotion_divisor = (0x02 << InitialTenuringThreshold) - 1;\n+      size_t young_evacuation_committed = (size_t) (ShenandoahEvacWaste *\n+                                                    collection_set->get_young_bytes_reserved_for_evacuation());\n+      size_t anticipated_promotion = young_evacuation_committed \/ promotion_divisor;\n+      if (anticipated_promotion < promotion_reserve) {\n+        promotion_reserve = anticipated_promotion;\n+      }\n+      heap->set_promoted_reserve(promotion_reserve);\n+#ifdef KELVIN_TRACE_PROMOTION_BUDGET\n+      printf(\"PromotionBudget: promotion_reserve limited by fraction of young evac: \" SIZE_FORMAT\n+             \", young_evac: \" SIZE_FORMAT \", divisor: \" SIZE_FORMAT  \"\\n\",\n+             promotion_reserve, young_evacuation_committed, promotion_divisor);\n+#endif\n+      if (collection_set->get_old_bytes_reserved_for_evacuation() == 0) {\n+        \/\/ Setting old evacuation reserve to zero denotes that there is no old-gen evacuation in this pass.\n+        heap->set_old_evac_reserve(0);\n@@ -302,9 +470,45 @@\n-      heap->set_promotion_reserve(promotion_reserve);\n-      heap->capture_old_usage(old_generation->used());\n-\n-      \/\/  OldEvacuationReserve for old generation: how much memory are we reserving to hold the results of\n-      \/\/     evacuating old-gen heap regions?  In order to sustain a consistent pace of young-gen collections,\n-      \/\/     the goal is to maintain a consistent value for this parameter (when the candidate set is not\n-      \/\/     empty).  This value is the minimum of:\n-      \/\/       1. old_gen->available() - PromotionReserve\n-      \/\/       2. (young_gen->capacity() scaled by ShenandoahEvacReserve) scaled by ShenandoahOldEvacRatioPercent\n+      size_t old_gen_usage_base = old_generation->used() - collection_set->get_old_garbage();\n+      heap->capture_old_usage(old_gen_usage_base);\n+\n+      \/\/ Compute the evacuation supplement, which is extra memory borrowed from old-gen that can be allocated\n+      \/\/ by mutators while GC is working on evacuation and update-refs.  This memory can be temporarily borrowed\n+      \/\/ from old-gen allotment, then repaid at the end of update-refs from the recycled collection set.  After\n+      \/\/ we have computed the collection set based on the parameters established above, we can make additional\n+      \/\/ calculates based on our knowledge of the collection set to determine how much allocation we can allow\n+      \/\/ during the evacuation and update-refs phases of execution.  With full awareness of collection set, we can\n+      \/\/ shrink the values of promotion_reserve, old_evacuation_reserve, and young_evacuation_reserve.  Then, we\n+      \/\/ can compute allocation_supplement as the minimum of:\n+      \/\/\n+      \/\/   1. old_gen->available() -\n+      \/\/        (promotion_reserve + old_evacuation_commitment + old_regions_loaned_for_young_evac * region_size_bytes)\n+      \/\/   2. The replenishment budget (number of regions in collection set - the number of regions already\n+      \/\/         under lien for the young_ewacuation_reserve)\n+      \/\/\n+      \/\/ The possibly revised values are also consulted by the ShenandoahPacer when it establishes pacing parameters\n+      \/\/ for evacuation and update-refs.\n+\n+      \/\/ Recompute regions_available_to_loan based on potential changes since last computation, such as:\n+      \/\/   1. The need to borrow for young_evac may have decreased if we found immediate garbage among young regions\n+      \/\/   2. old_gen->available() may have increased if we found immediate garbage among old regions\n+      \/\/   3. heap->get_old_evac_reserve() may have changed if the old collection set is smaller than budgeted\n+      \/\/   4. heap->get_young_evac_reserve() may have changed if the young collection set is smaller than budgeted\n+      \/\/   5. evacuation_reserve and newly computed promotion_reserve can be packed into existing old regions that\n+      \/\/      are partially used\n+\n+      \/\/ This is the total number of old-gen regions that are available to be temporarily repurposed\n+      size_t regions_available_to_loan = old_generation->free_unaffiliated_regions();\n+\n+      \/\/ This represents the total amount of old-gen memory that is available after handling anticipated promotions\n+      \/\/ and evacuations, represented in terms of a region count.  Note that this is >= regions_available_to_loan because\n+      \/\/ this quantity includes regions that are already partially used.\n+      size_t gross_available_old_regions =\n+        (old_generation->available() - (heap->get_old_evac_reserve() + promotion_reserve)) \/ region_size_bytes;\n+\n+      \/\/ Some portion of regions_available_to_loan may need to be set aside for old-gen evacuations and promotions.\n+      if (regions_available_to_loan > gross_available_old_regions) {\n+        regions_available_to_loan = gross_available_old_regions;\n+      }\n+      \/\/ Else:\n+      \/\/   In many cases, available old-gen memory that is fragmented between many partially used regions will be\n+      \/\/   greater than the sum of the promotion and old-gen evacuation reserves.  In this case, regions_available_to_loan\n+      \/\/   will already be less than gross_available_old_regions and does not need to be downsized.\n@@ -312,2 +516,1 @@\n-      \/\/ Don't reserve for old_evac any more than the memory that is available in old_gen.\n-      size_t old_evacuation_reserve = old_generation->available() - promotion_reserve;\n+      regions_available_to_loan -= old_regions_loaned_for_young_evac;\n@@ -315,3 +518,2 @@\n-      \/\/ Make sure old evacuation is no more than ShenandoahOldEvacRatioPercent of the total evacuation budget.\n-      size_t max_total_evac = (young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n-      size_t max_old_evac_portion = (max_total_evac * ShenandoahOldEvacRatioPercent) \/ 100;\n+      assert(old_regions_loaned_for_young_evac <= collection_set->get_young_region_count(),\n+             \"Cannot loan more regions than will be reclaimed\");\n@@ -319,2 +521,2 @@\n-      if (old_evacuation_reserve > max_old_evac_portion) {\n-        old_evacuation_reserve = max_old_evac_portion;\n+      if (regions_available_to_loan > (collection_set->get_young_region_count() - old_regions_loaned_for_young_evac)) {\n+        regions_available_to_loan = collection_set->get_young_region_count() - old_regions_loaned_for_young_evac;\n@@ -323,8 +525,21 @@\n-      heap->set_old_evac_reserve(old_evacuation_reserve);\n-      heap->reset_old_evac_expended();\n-\n-      \/\/ Compute YoungEvacuationReserve after we prime the collection set with old-gen candidates.  This depends\n-      \/\/ on how much memory old-gen wants to evacuate.  This is done within _heuristics->choose_collection_set().\n-\n-      \/\/ There's no need to pass this information to ShenandoahFreeSet::rebuild().  The GC allocator automatically borrows\n-      \/\/ memory from mutator regions when necessary.\n+      size_t allocation_supplement = regions_available_to_loan * region_size_bytes;\n+      heap->set_alloc_supplement_reserve(allocation_supplement);\n+\n+      size_t promotion_budget = heap->get_promoted_reserve();\n+      size_t old_evac_budget = heap->get_old_evac_reserve();\n+      size_t alloc_budget_evac_and_update = allocation_supplement + young_generation->available();\n+\n+      \/\/ TODO: young_available, which feeds into alloc_budget_evac_and_update is lacking memory available within\n+      \/\/ existing young-gen regions that were not selected for the collection set.  Add this in and adjust the\n+      \/\/ log message (where it says \"empty-region allocation budget\").\n+\n+      log_info(gc, ergo)(\"Memory reserved for evacuation and update-refs includes promotion budget: \" SIZE_FORMAT\n+                         \"%s, young evacuation budget: \" SIZE_FORMAT \"%s, old evacuation budget: \" SIZE_FORMAT\n+                         \"%s, empty-region allocation budget: \" SIZE_FORMAT \"%s, including supplement: \" SIZE_FORMAT \"%s\",\n+                         byte_size_in_proper_unit(promotion_budget), proper_unit_for_byte_size(promotion_budget),\n+                         byte_size_in_proper_unit(young_evacuation_reserve_used),\n+                         proper_unit_for_byte_size(young_evacuation_reserve_used),\n+                         byte_size_in_proper_unit(old_evac_budget), proper_unit_for_byte_size(old_evac_budget),\n+                         byte_size_in_proper_unit(alloc_budget_evac_and_update),\n+                         proper_unit_for_byte_size(alloc_budget_evac_and_update),\n+                         byte_size_in_proper_unit(allocation_supplement), proper_unit_for_byte_size(allocation_supplement));\n@@ -332,21 +547,0 @@\n-\n-    \/\/ The heuristics may consult and\/or change the values of PromotionReserved, OldEvacuationReserved, and\n-    \/\/ YoungEvacuationReserved, all of which are represented in the shared ShenandoahHeap data structure.\n-    _heuristics->choose_collection_set(heap->collection_set(), heap->old_heuristics());\n-\n-    \/\/  EvacuationAllocationSupplement: This represents memory that can be allocated in excess of young_gen->available()\n-    \/\/     during evacuation and update-refs.  This memory can be temporarily borrowed from old-gen allotment, then\n-    \/\/     repaid at the end of update-refs from the recycled collection set.  After we have computed the collection set\n-    \/\/     based on the parameters established above, we can make additional calculates based on our knowledge of the\n-    \/\/     collection set to determine how much allocation we can allow during the evacuation and update-refs phases\n-    \/\/     of execution.  With full awareness of collection set, we can shrink the values of PromotionReserve,\n-    \/\/     OldEvacuationReserve, and YoungEvacuationReserve.  Then, we can compute EvacuationAllocationReserve as the\n-    \/\/     minimum of:\n-    \/\/       1. old_gen->available - (PromotionReserve + OldEvacuationReserve)\n-    \/\/       2. The replenishment budget (number of regions in collection set - the number of regions already\n-    \/\/          under lien for the YoungEvacuationReserve)\n-    \/\/\n-\n-    \/\/ The possibly revised values are also consulted by the ShenandoahPacer when it establishes pacing parameters\n-    \/\/ for evacuation and update-refs.\n-\n@@ -440,1 +634,1 @@\n-void ShenandoahGeneration::scan_remembered_set() {\n+void ShenandoahGeneration::scan_remembered_set(bool is_concurrent) {\n@@ -448,2 +642,2 @@\n-  ShenandoahRegionIterator regions;\n-  ShenandoahScanRememberedTask task(task_queues(), old_gen_task_queues(), rp, &regions);\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n+  ShenandoahScanRememberedTask task(task_queues(), old_gen_task_queues(), rp, &work_list, is_concurrent);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":277,"deletions":83,"binary":false,"changes":360,"status":"modified"},{"patch":"@@ -151,1 +151,1 @@\n-  void scan_remembered_set();\n+  void scan_remembered_set(bool is_concurrent);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -512,1 +512,1 @@\n-  _promotion_reserve(0),\n+  _promoted_reserve(0),\n@@ -904,1 +904,2 @@\n-  size_t new_size = ShenandoahThreadLocalData::plab_size(thread) * 2;\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  size_t future_size = cur_size * 2;\n@@ -908,1 +909,1 @@\n-    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    future_size = MIN2(future_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n@@ -910,2 +911,2 @@\n-  new_size = MIN2(new_size, PLAB::max_size());\n-  new_size = MAX2(new_size, PLAB::min_size());\n+  future_size = MIN2(future_size, PLAB::max_size());\n+  future_size = MAX2(future_size, PLAB::min_size());\n@@ -913,1 +914,1 @@\n-  size_t unalignment = new_size % CardTable::card_size_in_words();\n+  size_t unalignment = cur_size % CardTable::card_size_in_words();\n@@ -915,1 +916,1 @@\n-    new_size = new_size - unalignment + CardTable::card_size_in_words();\n+    cur_size = cur_size - unalignment + CardTable::card_size_in_words();\n@@ -920,1 +921,1 @@\n-  \/\/ heuristics should catch up with them.  Note that the requested new_size may\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n@@ -922,3 +923,2 @@\n-  ShenandoahThreadLocalData::set_plab_size(thread, new_size);\n-\n-  if (new_size < size) {\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n@@ -927,1 +927,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -930,1 +930,0 @@\n-  \/\/ Retire current PLAB, and allocate a new one.\n@@ -932,20 +931,21 @@\n-  \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n-  \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n-  \/\/ aligned with the start of a card's memory range.\n-  retire_plab(plab);\n-\n-  size_t actual_size = 0;\n-  \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n-  \/\/ less than the remaining evacuation need.\n-  HeapWord* plab_buf = allocate_new_plab(min_size, new_size, &actual_size);\n-  if (plab_buf == NULL) {\n-    return NULL;\n-  }\n-\n-  assert (size <= actual_size, \"allocation should fit\");\n-\n-  if (ZeroTLAB) {\n-    \/\/ ..and clear it.\n-    Copy::zero_to_words(plab_buf, actual_size);\n-  } else {\n-    \/\/ ...and zap just allocated object.\n+  if (plab->words_remaining() < PLAB::min_size()) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == NULL) {\n+      return NULL;\n+    }\n+    assert (size <= actual_size, \"allocation should fit\");\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n@@ -953,5 +953,5 @@\n-    \/\/ Skip mangling the space corresponding to the object header to\n-    \/\/ ensure that the returned space is not considered parsable by\n-    \/\/ any concurrent GC thread.\n-    size_t hdr_size = oopDesc::header_size();\n-    Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n@@ -959,2 +959,2 @@\n-  }\n-  plab->set_buf(plab_buf, actual_size);\n+    }\n+    plab->set_buf(plab_buf, actual_size);\n@@ -962,1 +962,9 @@\n-  if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n@@ -965,1 +973,0 @@\n-  return plab->allocate(size);\n@@ -972,1 +979,1 @@\n-void ShenandoahHeap::retire_plab(PLAB* plab) {\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n@@ -976,4 +983,17 @@\n-    Thread* thread = Thread::current();\n-    size_t evacuated = ShenandoahThreadLocalData::get_plab_evacuated(thread);\n-    \/\/ We don't enforce limits on get_plab_promoted(thread).  Promotion uses any memory not required for evacuation.\n-    expend_old_evac(evacuated);\n+    \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+    \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+    \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+    \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+    \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+    \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+    \/\/  1. Some of the plab may have been dedicated to evacuations.\n+    \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+    size_t not_promoted =\n+      ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+    ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+    ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+    ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+    if (not_promoted > 0) {\n+      unexpend_promoted(not_promoted);\n+    }\n@@ -994,0 +1014,10 @@\n+\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  if (!mode()->is_generational()) {\n+    plab->retire();\n+  } else {\n+    Thread* thread = Thread::current();\n+    retire_plab(plab, thread);\n+  }\n+}\n+\n@@ -1162,0 +1192,4 @@\n+  \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+  bool promotion_eligible = false;\n+  bool allow_allocation = true;\n+  bool plab_alloc = false;\n@@ -1165,0 +1199,1 @@\n+  Thread* thread = Thread::current();\n@@ -1176,1 +1211,0 @@\n-\n@@ -1178,14 +1212,11 @@\n-        \/\/ We've already retired this thread's previously exhausted PLAB and have accounted for how that PLAB's\n-        \/\/ memory was allotted.\n-        Thread* thread = Thread::current();\n-        ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n-        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n-\n-        \/\/ Conservatively, assume this entire PLAB will be used for promotion.  Act as if we need to serve the\n-        \/\/ rest of evacuation need from as-yet unallocated old-gen memory.\n-        size_t remaining_evac_need = get_old_evac_reserve() - get_old_evac_expended();\n-        size_t evac_available = old_generation()->adjusted_available() - requested_bytes;\n-        if (remaining_evac_need >= evac_available) {\n-          \/\/ Disable promotions within this thread because the entirety of this PLAB must be available to hold\n-          \/\/ old-gen evacuations.\n-          ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+        plab_alloc = true;\n+\n+        size_t promotion_avail = get_promoted_reserve();\n+        size_t promotion_expended = get_promoted_expended();\n+        if (promotion_expended + requested_bytes > promotion_avail) {\n+          promotion_avail = 0;\n+          if (get_old_evac_reserve() == 0) {\n+            \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+            \/\/ be used for promotions.\n+            allow_allocation = false;\n+          }\n@@ -1193,1 +1224,2 @@\n-          ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+          promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          promotion_eligible = true;\n@@ -1197,5 +1229,12 @@\n-        Thread* thread = Thread::current();\n-        size_t remaining_evac_need = get_old_evac_reserve() - get_old_evac_expended();\n-        size_t evac_available = old_generation()->adjusted_available() - requested_bytes;\n-        if (remaining_evac_need >= evac_available) {\n-          return nullptr;       \/\/ We need to reserve the remaining memory for evacuation so defer the promotion\n+        size_t promotion_avail = get_promoted_reserve();\n+        size_t promotion_expended = get_promoted_expended();\n+        if (promotion_expended + requested_bytes > promotion_avail) {\n+          promotion_avail = 0;\n+        } else {\n+          promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+        }\n+\n+        if (promotion_avail == 0) {\n+          \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+          \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+          return nullptr;\n@@ -1209,2 +1248,1 @@\n-\n-  HeapWord* result = _free_set->allocate(req, in_new_region);\n+  HeapWord* result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n@@ -1213,0 +1251,28 @@\n+      ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+      if (req.is_gc_alloc()) {\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          if (promotion_eligible) {\n+            size_t actual_size = req.actual_size() * HeapWordSize;\n+            \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+            \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+            ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+            expend_promoted(actual_size);\n+            ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+          } else {\n+            \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+#undef KELVIN_MYSTERY\n+#ifdef KELVIN_MYSTERY\n+            printf(PTR_FORMAT \": disabling promotions for requested bytes: \" SIZE_FORMAT \", promotion_expended: \" SIZE_FORMAT\n+                   \", promotion_reserved: \" SIZE_FORMAT \"\\n\",\n+                   p2i(thread), requested_bytes, get_promoted_expended(), get_promoted_reserve());\n+            fflush(stdout);\n+#endif\n+            ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+            ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+          }\n+        } else if (is_promotion) {\n+          \/\/ Shared promotion.  Assume size is requested_bytes.\n+          expend_promoted(requested_bytes);\n+        }\n+      }\n+\n@@ -1232,0 +1298,8 @@\n+  } else {\n+    \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+    if ((req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && req.is_gc_alloc() &&\n+        (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+      \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+      \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+      ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+    }\n@@ -1489,6 +1563,5 @@\n-    \/\/ TODO; Retiring a PLAB disables it so it cannot support future allocations.  This is overkill.  For old-gen\n-    \/\/ regions, the important thing is to make the memory parsable by the remembered-set scanning code that drives\n-    \/\/ the update-refs processing that follows.  After the updating of old-gen references is done, it is ok to carve\n-    \/\/ this remnant object into smaller pieces during the subsequent evacuation pass, as long as the PLAB is made parsable\n-    \/\/ again before the next update-refs phase.\n-    ShenandoahHeap::heap()->retire_plab(plab);\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parseable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n@@ -2405,0 +2478,1 @@\n+\n@@ -2406,0 +2480,1 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n@@ -2408,1 +2483,2 @@\n-  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -2411,1 +2487,2 @@\n-    _regions(regions)\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n@@ -2430,1 +2507,0 @@\n-    ShenandoahHeapRegion* r = _regions->next();\n@@ -2432,0 +2508,1 @@\n+    ShenandoahHeapRegion* r = _regions->next();\n@@ -2441,0 +2518,1 @@\n+      bool region_progress = false;\n@@ -2444,0 +2522,1 @@\n+          region_progress = true;\n@@ -2446,0 +2525,6 @@\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n@@ -2447,46 +2532,1 @@\n-          } else {\n-            \/\/ Old region in a young cycle or mixed cycle.\n-            if (!is_mixed) {\n-              \/\/ This is a young evac..\n-              _heap->card_scan()->process_region(r, &cl, true);\n-            } else {\n-              \/\/ This is a _mixed_evac.\n-              \/\/\n-              \/\/ TODO: For _mixed_evac, consider building an old-gen remembered set that allows restricted updating\n-              \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n-              \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n-              \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n-              \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n-              \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n-              \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n-              \/\/ old-gen heap regions.\n-              if (r->is_humongous()) {\n-                r->oop_iterate_humongous(&cl);\n-              } else {\n-                \/\/ This is a mixed evacuation.  Old regions that are candidates for collection have not been coalesced\n-                \/\/ and filled.  Use mark bits to find objects that need to be updated.\n-                \/\/\n-                \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n-                \/\/ regions which are in the collection set for a particular mixed evacuation.\n-                HeapWord *p = r->bottom();\n-                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, p, update_watermark);\n-\n-                \/\/ Anything beyond update_watermark was allocated during evacuation.  Thus, it is known to not hold\n-                \/\/ references to collection set objects.\n-                while (p < update_watermark) {\n-                  oop obj = cast_to_oop(p);\n-                  if (ctx->is_marked(obj)) {\n-                    objs.do_object(obj);\n-                    p += obj->size();\n-                  } else {\n-                    \/\/ This object is not marked so we don't scan it.\n-                    HeapWord* tams = ctx->top_at_mark_start(r);\n-                    if (p >= tams) {\n-                      p += obj->size();\n-                    } else {\n-                      p = ctx->get_next_marked_addr(p, tams);\n-                    }\n-                  }\n-                }\n-              }\n-            }\n+            region_progress = true;\n@@ -2494,0 +2534,2 @@\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n@@ -2510,1 +2552,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2518,0 +2560,119 @@\n+    if (_heap->mode()->is_generational() && (_heap->active_generation()->generation_mode() != GLOBAL)) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      work_chunk assignment;\n+      bool have_work = _work_chunks->next(&assignment);\n+      RememberedScanner* scanner = _heap->card_scan();\n+      while (have_work) {\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION)) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true, CONCURRENT);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, CONCURRENT);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+        \/\/ Otherwise, this work chunk had nothing for me to do, so do not report pacer progress.\n+\n+        \/\/ Before we take responsibility for another chunk of work, see if cancellation is requested.\n+        if (_heap->check_cancelled_gc_and_yield(CONCURRENT)) {\n+          return;\n+        }\n+        have_work = _work_chunks->next(&assignment);\n+      }\n+    }\n@@ -2523,0 +2684,1 @@\n+  ShenandoahRegionChunkIterator work_list(workers()->active_workers());\n@@ -2525,1 +2687,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2528,1 +2690,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2533,1 +2695,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":290,"deletions":129,"binary":false,"changes":419,"status":"modified"},{"patch":"@@ -353,1 +353,1 @@\n-  \/\/ 2. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promotion_reserve\n+  \/\/ 2. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promoted_reserve\n@@ -367,2 +367,2 @@\n-  size_t _promotion_reserve;           \/\/ Bytes reserved within old-gen to hold the results of promotion\n-\n+  size_t _promoted_reserve;            \/\/ Bytes reserved within old-gen to hold the results of promotion\n+  volatile size_t _promoted_expended;  \/\/ Bytes of old-gen memory expended on promotions\n@@ -371,1 +371,1 @@\n-  size_t _old_evac_expended;           \/\/ Bytes of old-gen memory expended on old-gen evacuations\n+  volatile size_t _old_evac_expended;  \/\/ Bytes of old-gen memory expended on old-gen evacuations\n@@ -429,2 +429,7 @@\n-  inline size_t set_promotion_reserve(size_t new_val);\n-  inline size_t get_promotion_reserve() const;\n+  inline size_t set_promoted_reserve(size_t new_val);\n+  inline size_t get_promoted_reserve() const;\n+\n+  inline void reset_promoted_expended();\n+  inline size_t expend_promoted(size_t increment);\n+  inline size_t unexpend_promoted(size_t decrement);\n+  inline size_t get_promoted_expended();\n@@ -438,1 +443,1 @@\n-  inline size_t get_old_evac_expended() const;\n+  inline size_t get_old_evac_expended();\n@@ -793,0 +798,1 @@\n+  void retire_plab(PLAB* plab, Thread* thread);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":13,"deletions":7,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -304,5 +304,3 @@\n-  if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n-    return NULL;\n-  } else if (plab == NULL) {\n-    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(),\n-           \"Performance: thread should have PLAB: %s\", thread->name());\n+  HeapWord* obj;\n+  if (plab == NULL) {\n+    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(), \"Performance: thread should have PLAB: %s\", thread->name());\n@@ -310,1 +308,3 @@\n-    return NULL;\n+    return nullptr;\n+  } else if (is_promotion && (plab->words_remaining() > 0) && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    return nullptr;\n@@ -312,2 +312,10 @@\n-  HeapWord* obj = plab->allocate(size);\n-  if (obj == NULL) {\n+  \/\/ if plab->word_size() <= 0, thread's plab not yet initialized for this pass, so allow_plab_promotions() is not trustworthy\n+  obj = plab->allocate(size);\n+  if ((obj == nullptr) && (plab->words_remaining() < PLAB::min_size())) {\n+#undef KELVIN_SCRUTINY\n+#ifdef KELVIN_SCRUTINY\n+    printf(PTR_FORMAT \": refreshing plab for allocation of size: \" SIZE_FORMAT \", remaining_words: \" SIZE_FORMAT\n+           \", is_promotion: %s\\n\", p2i(thread), size, plab->words_remaining(), is_promotion? \"true\": \"false\");\n+    fflush(stdout);\n+#endif\n+    \/\/ allocate_from_plab_slow will establish allow_plab_promotions(thread) for future invocations\n@@ -316,0 +324,11 @@\n+#ifdef KELVIN_SCRUTINY\n+  else if (obj == nullptr) {\n+    printf(PTR_FORMAT \": Not refreshing plab for allocation of size: \" SIZE_FORMAT \", remaining_words: \" SIZE_FORMAT\n+           \", is_promotion: %s\\n\", p2i(thread), size, plab->words_remaining(), is_promotion? \"true\": \"false\");\n+    fflush(stdout);\n+  }\n+#endif\n+  \/\/ if plab->words_remaining() >= PLAB::min_size(), just return nullptr so we can use a shared allocation\n+  if (obj == nullptr) {\n+    return nullptr;\n+  }\n@@ -388,0 +407,1 @@\n+#undef KELVIN_SPECIAL_SCRUTINY\n@@ -390,6 +410,45 @@\n-             if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread))) {\n-               \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve.  Try resetting\n-               \/\/ the desired PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n-               ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n-               copy = allocate_from_plab(thread, size, is_promotion);\n-               \/\/ If we still get nullptr, we'll try a shared allocation below.\n+             if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n+                 ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+               \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n+               \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n+               \/\/ where abundance is defined as >= PLAB::min_size().  In the former case, we try resetting the desired\n+               \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n+\n+               \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n+               \/\/ this particular allocation to be shared.\n+\n+               PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+               if (plab->words_remaining() < PLAB::min_size()) {\n+#ifdef KELVIN_SPECIAL_SCRUTINY\n+                 printf(PTR_FORMAT \": plab_alloc failed for size \" SIZE_FORMAT \", remaining words: \" SIZE_FORMAT \", retry with min plab size\\n\",\n+                        p2i(thread), size, plab->words_remaining());\n+                 fflush(stdout);\n+#endif\n+                 ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n+                 copy = allocate_from_plab(thread, size, is_promotion);\n+                 \/\/ If we still get nullptr, we'll try a shared allocation below.\n+                 if (copy == nullptr) {\n+                   \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n+#ifdef KELVIN_SPECIAL_SCRUTINY\n+                   printf(PTR_FORMAT \": disabling plab retres because plab alloc retry failed\\n\", p2i(thread));\n+                   fflush(stdout);\n+#endif\n+                   ShenandoahThreadLocalData::disable_plab_retries(thread);\n+                 }\n+               }\n+               \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n+#ifdef KELVIN_SPECIAL_SCRUTINY\n+               else {\n+                 printf(PTR_FORMAT \": plab_alloc failed for size \" SIZE_FORMAT \", remaining words: \" SIZE_FORMAT \", preserving plab to force shared allocation\\n\",\n+                        p2i(thread), size, plab->words_remaining());\n+                 fflush(stdout);\n+               }\n+#endif\n+             } else if (copy != nullptr) {\n+#ifdef KELVIN_SPECIAL_SCRUTINY\n+               if (!ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+                 printf(PTR_FORMAT \": enabling plab retries because plab alloc succeeded\\n\", p2i(thread));\n+                 fflush(stdout);\n+               }\n+#endif\n+               ShenandoahThreadLocalData::enable_plab_retries(thread);\n@@ -409,3 +468,18 @@\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n-      copy = allocate_memory(req, is_promotion);\n-      alloc_from_lab = false;\n+      if (!is_promotion || (size > PLAB::min_size())) {\n+        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+        copy = allocate_memory(req, is_promotion);\n+        alloc_from_lab = false;\n+#ifdef KELVIN_SPECIAL_SCRUTINY\n+        printf(PTR_FORMAT \": tried shared allocation of size \" SIZE_FORMAT \", is_promotion: %s, %s\\n\",\n+               p2i(thread), size, is_promotion? \"true\": \"false\", (copy == nullptr)? \"failed\": \"succeeded\");\n+        fflush(stdout);\n+#endif\n+      }\n+      \/\/ else, we leave copy equal to NULL, signalling a promotion failure below if appropriate\n+#ifdef KELVIN_SPECIAL_SCRUTINY\n+      else {\n+        printf(PTR_FORMAT \": did NOT try shared allocation of size \" SIZE_FORMAT \", is_promotion: %s\\n\",\n+               p2i(thread), size, is_promotion? \"true\": \"false\");\n+        fflush(stdout);\n+      }\n+#endif\n@@ -591,3 +665,3 @@\n-inline size_t ShenandoahHeap::set_promotion_reserve(size_t new_val) {\n-  size_t orig = _promotion_reserve;\n-  _promotion_reserve = new_val;\n+inline size_t ShenandoahHeap::set_promoted_reserve(size_t new_val) {\n+  size_t orig = _promoted_reserve;\n+  _promoted_reserve = new_val;\n@@ -597,2 +671,2 @@\n-inline size_t ShenandoahHeap::get_promotion_reserve() const {\n-  return _promotion_reserve;\n+inline size_t ShenandoahHeap::get_promoted_reserve() const {\n+  return _promoted_reserve;\n@@ -609,0 +683,1 @@\n+  shenandoah_assert_heaplocked();\n@@ -623,1 +698,2 @@\n-  return _old_evac_reserve;\n+  size_t result = _old_evac_reserve;\n+  return result;\n@@ -627,1 +703,1 @@\n-  _old_evac_expended = 0;\n+  Atomic::store(&_old_evac_expended, (size_t) 0);\n@@ -631,2 +707,17 @@\n-  _old_evac_expended += increment;\n-  return _old_evac_expended;\n+  return Atomic::add(&_old_evac_expended, increment);\n+}\n+\n+inline size_t ShenandoahHeap::get_old_evac_expended() {\n+  return Atomic::load(&_old_evac_expended);\n+}\n+\n+inline void ShenandoahHeap::reset_promoted_expended() {\n+  Atomic::store(&_promoted_expended, (size_t) 0);\n+}\n+\n+inline size_t ShenandoahHeap::expend_promoted(size_t increment) {\n+  return Atomic::add(&_promoted_expended, increment);\n+}\n+\n+inline size_t ShenandoahHeap::unexpend_promoted(size_t decrement) {\n+  return Atomic::sub(&_promoted_expended, decrement);\n@@ -635,2 +726,2 @@\n-inline size_t ShenandoahHeap::get_old_evac_expended() const {\n-  return _old_evac_expended;\n+inline size_t ShenandoahHeap::get_promoted_expended() {\n+  return Atomic::load(&_promoted_expended);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":119,"deletions":28,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -569,0 +569,51 @@\n+\/\/ DO NOT CANCEL.  If this worker thread has accepted responsibility for scanning a particular range of addresses, it\n+\/\/ must finish the work before it can be cancelled.\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice(OopIterateClosure* blk, bool dirty_only,\n+                                                       HeapWord* start, size_t words, bool write_table, bool is_concurrent) {\n+  assert(words % CardTable::card_size_in_words() == 0, \"Humongous iteration must span whole number of cards\");\n+  assert(is_humongous(), \"only humongous region here\");\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Find head.\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  assert(r->is_humongous_start(), \"need humongous head here\");\n+\n+  oop obj = cast_to_oop(r->bottom());\n+  RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+  size_t card_index = scanner->card_index_for_addr(start);\n+  size_t num_cards = words \/ CardTable::card_size_in_words();\n+\n+  if (dirty_only) {\n+    if (write_table) {\n+      while (num_cards-- > 0) {\n+        if (scanner->is_write_card_dirty(card_index++)) {\n+          obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+        }\n+        start += CardTable::card_size_in_words();\n+      }\n+    } else {\n+      while (num_cards-- > 0) {\n+        if (scanner->is_card_dirty(card_index++)) {\n+          obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+        }\n+        start += CardTable::card_size_in_words();\n+      }\n+    }\n+  } else {\n+    \/\/ Scan all data, regardless of whether cards are dirty\n+    while (num_cards-- > 0) {\n+      obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+      start += CardTable::card_size_in_words();\n+    }\n+  }\n+}\n+\n+void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk, HeapWord* start, size_t words) {\n+  assert(is_humongous(), \"only humongous region here\");\n+  \/\/ Find head.\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  assert(r->is_humongous_start(), \"need humongous head here\");\n+  oop obj = cast_to_oop(r->bottom());\n+  obj->oop_iterate(blk, MemRegion(start, start + words));\n+}\n+\n@@ -595,1 +646,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -404,0 +404,7 @@\n+  void oop_iterate_humongous(OopIterateClosure* cl, HeapWord* start, size_t words);\n+\n+  \/\/ Invoke closure on every reference contained within the humongous object that spans this humongous\n+  \/\/ region if the reference is contained within a DIRTY card and the reference is no more than words following\n+  \/\/ start within the humongous object.\n+  void oop_iterate_humongous_slice(OopIterateClosure* cl, bool dirty_only, HeapWord* start, size_t words,\n+                                         bool write_table, bool is_concurrent);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -119,1 +119,1 @@\n-      _generation->scan_remembered_set();\n+      _generation->scan_remembered_set(false \/* is_concurrent *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-                                                           ShenandoahRegionIterator* regions) :\n+                                                           ShenandoahRegionChunkIterator* work_list, bool is_concurrent) :\n@@ -92,1 +92,1 @@\n-  _queue_set(queue_set), _old_queue_set(old_queue_set), _rp(rp), _regions(regions) {}\n+  _queue_set(queue_set), _old_queue_set(old_queue_set), _rp(rp), _work_list(work_list), _is_concurrent(is_concurrent) {}\n@@ -95,3 +95,13 @@\n-  \/\/ This sets up a thread local reference to the worker_id which is necessary\n-  \/\/ the weak reference processor.\n-  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  if (_is_concurrent) {\n+    \/\/ This sets up a thread local reference to the worker_id which is needed by the weak reference processor.\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+    do_work(worker_id);\n+  } else {\n+    \/\/ This sets up a thread local reference to the worker_id which is needed by the weak reference processor.\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    do_work(worker_id);\n+  }\n+}\n+\n+void ShenandoahScanRememberedTask::do_work(uint worker_id) {\n@@ -103,1 +113,2 @@\n-  RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  RememberedScanner* scanner = heap->card_scan();\n@@ -107,3 +118,14 @@\n-  ShenandoahHeapRegion* region = _regions->next();\n-  while (region != NULL) {\n-    log_debug(gc)(\"ShenandoahScanRememberedTask::work(%u), looking at region \" SIZE_FORMAT, worker_id, region->index());\n+  work_chunk assignment;\n+  bool has_work = _work_list->next(&assignment);\n+  while (has_work) {\n+#ifdef ENABLE_REMEMBERED_SET_CANCELLATION\n+    \/\/ This check is currently disabled to avoid crashes that occur\n+    \/\/ when we try to cancel remembered set scanning\n+    if (heap->check_cancelled_gc_and_yield(_is_concurrent)) {\n+      return;\n+    }\n+#endif\n+    ShenandoahHeapRegion* region = assignment._r;\n+    log_debug(gc)(\"ShenandoahScanRememberedTask::do_work(%u), processing slice of region \"\n+                  SIZE_FORMAT \" at offset \" SIZE_FORMAT \", size: \" SIZE_FORMAT,\n+                  worker_id, region->index(), assignment._chunk_offset, assignment._chunk_size);\n@@ -111,1 +133,42 @@\n-      scanner->process_region(region, &cl);\n+      size_t cluster_size =\n+        CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+      size_t clusters = assignment._chunk_size \/ cluster_size;\n+      assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignments must align on cluster boundaries\");\n+      HeapWord* end_of_range = region->bottom() + assignment._chunk_offset + assignment._chunk_size;\n+\n+      \/\/ During concurrent mark, region->top() equals TAMS with respect to the current young-gen pass.  *\/\n+      if (end_of_range > region->top()) {\n+        end_of_range = region->top();\n+      }\n+      scanner->process_region_slice(region, assignment._chunk_offset, clusters, end_of_range, &cl, false, _is_concurrent);\n+    }\n+    has_work = _work_list->next(&assignment);\n+  }\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_group_size() {\n+  \/\/ First group does roughly half of heap, one region at a time.\n+  \/\/ Second group does roughly one quarter of heap, half of a region at a time, and so on.\n+  \/\/ Last group does the remnant of heap, one _smallest_chunk_size at a time.\n+  \/\/ Round down.\n+  return _heap->num_regions() \/ 2;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_first_group_chunk_size() {\n+  size_t words_in_region = ShenandoahHeapRegion::region_size_words();\n+  return words_in_region;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_num_groups() {\n+  size_t total_heap_size = _heap->num_regions() * ShenandoahHeapRegion::region_size_words();\n+  size_t num_groups = 0;\n+  size_t cumulative_group_span = 0;\n+  size_t current_group_span = _first_group_chunk_size * _group_size;\n+  size_t smallest_group_span = _smallest_chunk_size * _group_size;\n+  while ((num_groups < _maximum_groups) && (cumulative_group_span + current_group_span <= total_heap_size)) {\n+    num_groups++;\n+    cumulative_group_span += current_group_span;\n+    if (current_group_span <= smallest_group_span) {\n+      break;\n+    } else {\n+      current_group_span \/= 2;    \/\/ Each group spans half of what the preceding group spanned.\n@@ -113,1 +176,0 @@\n-    region = _regions->next();\n@@ -115,0 +177,123 @@\n+  \/\/ Loop post condition:\n+  \/\/   num_groups <= _maximum_groups\n+  \/\/   cumulative_group_span is the memory spanned by num_groups\n+  \/\/   current_group_span is the span of the last fully populated group (assuming loop iterates at least once)\n+  \/\/   each of num_groups is fully populated with _group_size chunks in each\n+  \/\/ Non post conditions:\n+  \/\/   cumulative_group_span may be less than total_heap size for one or more of the folowing reasons\n+  \/\/   a) The number of regions remaining to be spanned is smaller than a complete group, or\n+  \/\/   b) We have filled up all groups through _maximum_groups and still have not spanned all regions\n+\n+  if (cumulative_group_span < total_heap_size) {\n+    \/\/ We've got more regions to span\n+    if ((num_groups < _maximum_groups) && (current_group_span > smallest_group_span)) {\n+      num_groups++;             \/\/ Place all remaining regions into a new not-full group (chunk_size half that of previous group)\n+    }\n+    \/\/ Else we are unable to create a new group because we've exceed the number of allowed groups or have reached the\n+    \/\/ minimum chunk size.\n+\n+    \/\/ Any remaining regions will be treated as if they are part of the most recently created group.  This group will\n+    \/\/ have more than _group_size chunks within it.\n+  }\n+  return num_groups;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_total_chunks() {\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t unspanned_heap_size = _heap->num_regions() * region_size_words;\n+  size_t num_chunks = 0;\n+  size_t num_groups = 0;\n+  size_t cumulative_group_span = 0;\n+  size_t current_group_span = _first_group_chunk_size * _group_size;\n+  size_t smallest_group_span = _smallest_chunk_size * _group_size;\n+  while (unspanned_heap_size > 0) {\n+    if (current_group_span <= unspanned_heap_size) {\n+      unspanned_heap_size -= current_group_span;\n+      num_chunks += _group_size;\n+      num_groups++;\n+\n+      if (num_groups >= _num_groups) {\n+        \/\/ The last group has more than _group_size entries.\n+        size_t chunk_span = current_group_span \/ _group_size;\n+        size_t extra_chunks = unspanned_heap_size \/ chunk_span;\n+        assert (extra_chunks * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+        num_chunks += extra_chunks;\n+        return num_chunks;\n+      } else if (current_group_span <= smallest_group_span) {\n+        \/\/ We cannot introduce new groups because we've reached the lower bound on group size\n+        size_t chunk_span = _smallest_chunk_size;\n+        size_t extra_chunks = unspanned_heap_size \/ chunk_span;\n+        assert (extra_chunks * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+        num_chunks += extra_chunks;\n+        return num_chunks;\n+      } else {\n+        current_group_span \/= 2;\n+      }\n+    } else {\n+      \/\/ The last group has fewer than _group_size entries.\n+      size_t chunk_span = current_group_span \/ _group_size;\n+      size_t last_group_size = unspanned_heap_size \/ chunk_span;\n+      assert (last_group_size * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+      num_chunks += last_group_size;\n+      return num_chunks;\n+    }\n+  }\n+  return num_chunks;\n+}\n+\n+ShenandoahRegionChunkIterator::ShenandoahRegionChunkIterator(size_t worker_count) :\n+    _heap(ShenandoahHeap::heap()),\n+    _group_size(calc_group_size()),\n+    _first_group_chunk_size(calc_first_group_chunk_size()),\n+    _num_groups(calc_num_groups()),\n+    _total_chunks(calc_total_chunks()),\n+    _index(0)\n+{\n+  assert(_smallest_chunk_size ==\n+         CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster,\n+         \"_smallest_chunk_size is not valid\");\n+\n+  size_t words_in_region = ShenandoahHeapRegion::region_size_words();\n+  size_t group_span = _first_group_chunk_size * _group_size;\n+\n+  _region_index[0] = 0;\n+  _group_offset[0] = 0;\n+  for (size_t i = 1; i < _num_groups; i++) {\n+    _region_index[i] = _region_index[i-1] + (_group_offset[i-1] + group_span) \/ words_in_region;\n+    _group_offset[i] = (_group_offset[i-1] + group_span) % words_in_region;\n+    group_span \/= 2;\n+  }\n+  \/\/ Not necessary, but keeps things tidy\n+  for (size_t i = _num_groups; i < _maximum_groups; i++) {\n+    _region_index[i] = 0;\n+    _group_offset[i] = 0;\n+  }\n+}\n+\n+ShenandoahRegionChunkIterator::ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count) :\n+    _heap(heap),\n+    _group_size(calc_group_size()),\n+    _first_group_chunk_size(calc_first_group_chunk_size()),\n+    _num_groups(calc_num_groups()),\n+    _total_chunks(calc_total_chunks()),\n+    _index(0)\n+{\n+  size_t words_in_region = ShenandoahHeapRegion::region_size_words();\n+  size_t group_span = _first_group_chunk_size * _group_size;\n+\n+  _region_index[0] = 0;\n+  _group_offset[0] = 0;\n+  for (size_t i = 1; i < _num_groups; i++) {\n+    _region_index[i] = _region_index[i-1] + (_group_offset[i-1] + group_span) \/ words_in_region;\n+    _group_offset[i] = (_group_offset[i-1] + group_span) % words_in_region;\n+    group_span \/= 2;\n+  }\n+  \/\/ Not necessary, but keeps things tidy\n+  for (size_t i = _num_groups; i < _maximum_groups; i++) {\n+    _region_index[i] = 0;\n+    _group_offset[i] = 0;\n+  }\n+}\n+\n+void ShenandoahRegionChunkIterator::reset() {\n+  _index = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":196,"deletions":11,"binary":false,"changes":207,"status":"modified"},{"patch":"@@ -212,0 +212,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n@@ -926,0 +927,1 @@\n+  HeapWord* addr_for_cluster(size_t cluster_no);\n@@ -932,0 +934,8 @@\n+  HeapWord* first_object_in_card(size_t card_index) {\n+    if (_scc->has_object(card_index)) {\n+      return addr_for_card_index(card_index) + _scc->get_first_start(card_index);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+\n@@ -970,1 +980,9 @@\n-  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops);\n+  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops, bool is_concurrent);\n+\n+  template <typename ClosureType>\n+  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops,\n+                               bool use_write_table, bool is_concurrent);\n+\n+  template <typename ClosureType>\n+  inline void process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                         HeapWord *end_of_range, ClosureType *oops, bool use_write_table, bool is_concurrent);\n@@ -973,1 +991,1 @@\n-  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops, bool use_write_table);\n+  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl, bool is_concurrent);\n@@ -976,1 +994,1 @@\n-  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl);\n+  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl, bool use_write_table, bool is_concurrent);\n@@ -979,1 +997,2 @@\n-  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl, bool use_write_table);\n+  inline void process_region_slice(ShenandoahHeapRegion* region, size_t offset, size_t clusters, HeapWord* end_of_range,\n+                                   ClosureType *cl, bool use_write_table, bool is_concurrent);\n@@ -1005,0 +1024,75 @@\n+typedef struct ChunkOfRegion {\n+  ShenandoahHeapRegion *_r;\n+  size_t _chunk_offset;          \/\/ HeapWordSize offset\n+  size_t _chunk_size;            \/\/ HeapWordSize qty\n+} work_chunk;\n+\n+class ShenandoahRegionChunkIterator : public StackObj {\n+private:\n+  \/\/ smallest_chunk_size is 64 words per card *\n+  \/\/ ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster.\n+  \/\/ This is computed from CardTable::card_size_in_words() *\n+  \/\/      ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  \/\/ We can't perform this computation here, because of encapsulation and initialization constraints.  We paste\n+  \/\/ the magic number here, and assert that this number matches the intended computation in constructor.\n+  static const size_t _smallest_chunk_size = 64 * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+\n+  \/\/ The total remembered set scanning effort is divided into chunks of work that are assigned to individual worker tasks.\n+  \/\/ The chunks of assigned work are divided into groups, where the size of each group (_group_size) is 4 * the number of\n+  \/\/ worker tasks.  All of the assignments within a group represent the same amount of memory to be scanned.  Each of the\n+  \/\/ assignments within the first group are of size _first_group_chunk_size (typically the ShenandoahHeapRegion size, but\n+  \/\/ possibly smaller.  Each of the assignments within each subsequent group are half the size of the assignments in the\n+  \/\/ preceding group.  The last group may be larger than the others.  Because no group is allowed to have smaller assignments\n+  \/\/ than _smallest_chunk_size, which is 32 KB.\n+\n+  \/\/ Under normal circumstances, no configuration needs more than _maximum_groups (default value of 16).\n+  \/\/\n+\n+  static const size_t _maximum_groups = 16;\n+\n+  const ShenandoahHeap* _heap;\n+\n+  const size_t _group_size;                        \/\/ Number of chunks in each group, equals worker_threads * 8\n+  const size_t _first_group_chunk_size;\n+  const size_t _num_groups;                        \/\/ Number of groups in this configuration\n+  const size_t _total_chunks;\n+\n+  shenandoah_padding(0);\n+  volatile size_t _index;\n+  shenandoah_padding(1);\n+\n+  size_t _region_index[_maximum_groups];\n+  size_t _group_offset[_maximum_groups];\n+\n+\n+  \/\/ No implicit copying: iterators should be passed by reference to capture the state\n+  NONCOPYABLE(ShenandoahRegionChunkIterator);\n+\n+  \/\/ Makes use of _heap.\n+  size_t calc_group_size();\n+\n+  \/\/ Makes use of _group_size, which must be initialized before call.\n+  size_t calc_first_group_chunk_size();\n+\n+  \/\/ Makes use of _group_size and _first_group_chunk_size, both of which must be initialized before call.\n+  size_t calc_num_groups();\n+\n+  \/\/ Makes use of _group_size, _first_group_chunk_size, which must be initialized before call.\n+  size_t calc_total_chunks();\n+\n+public:\n+  ShenandoahRegionChunkIterator(size_t worker_count);\n+  ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count);\n+\n+  \/\/ Reset iterator to default state\n+  void reset();\n+\n+  \/\/ Fills in assignment with next chunk of work and returns true iff there is more work.\n+  \/\/ Otherwise, returns false.  This is multi-thread-safe.\n+  inline bool next(work_chunk *assignment);\n+\n+  \/\/ This is *not* MT safe. However, in the absence of multithreaded access, it\n+  \/\/ can be used to determine if there is more work to do.\n+  inline bool has_next() const;\n+};\n+\n@@ -1012,1 +1106,2 @@\n-  ShenandoahRegionIterator* _regions;\n+  ShenandoahRegionChunkIterator* _work_list;\n+  bool _is_concurrent;\n@@ -1017,2 +1112,2 @@\n-                               ShenandoahRegionIterator* regions);\n-\n+                               ShenandoahRegionChunkIterator* work_list,\n+                               bool is_concurrent);\n@@ -1020,0 +1115,1 @@\n+  void do_work(uint worker_id);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":103,"deletions":7,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -452,1 +452,4 @@\n-        \/\/ offset will be zero if no objects are marked in this card.\n+        \/\/ If there are no marked objects remaining in this region, offset equals tams - base_addr.  If this offset is\n+        \/\/ greater than max_offset, we will immediately exit this loop.  Otherwise, the next iteration of the loop will\n+        \/\/ treat the object at offset as marked and live (because address >= tams) and we will continue iterating object\n+        \/\/ by consulting the size() fields of each.\n@@ -454,1 +457,1 @@\n-    } while (offset > 0 && offset < max_offset);\n+    } while (offset < max_offset);\n@@ -484,2 +487,2 @@\n-                                                          ClosureType *cl) {\n-  process_clusters(first_cluster, count, end_of_range, cl, false);\n+                                                          ClosureType *cl, bool is_concurrent) {\n+  process_clusters(first_cluster, count, end_of_range, cl, false, is_concurrent);\n@@ -489,2 +492,5 @@\n-\/\/ less than end_of_range.  For any such object, process the complete object, even if its end reaches beyond\n-\/\/ end_of_range.\n+\/\/ less than end_of_range.  For any such object, process the complete object, even if its end reaches beyond end_of_range.\n+\n+\/\/ Do not CANCEL within process_clusters.  It is assumed that if a worker thread accepts responsbility for processing\n+\/\/ a chunk of work, it will finish the work it starts.  Otherwise, the chunk of work will be lost in the transition to\n+\/\/ degenerated execution.\n@@ -495,1 +501,1 @@\n-                                                          ClosureType *cl, bool write_table) {\n+                                                          ClosureType *cl, bool write_table, bool is_concurrent) {\n@@ -517,2 +523,5 @@\n-  HeapWord* end_of_clusters = _rs->addr_for_card_index(first_cluster)\n-    + count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster * CardTable::card_size_in_words();\n+  size_t card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  HeapWord *start_of_range = _rs->addr_for_card_index(card_index);\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(start_of_range);\n+  assert(end_of_range <= r->top(), \"process_clusters() examines one region at a time\");\n+\n@@ -520,1 +529,3 @@\n-    size_t card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+    \/\/ TODO: do we want to check cancellation in inner loop, on every card processed?  That would be more responsive,\n+    \/\/ but require more overhead for checking.\n+    card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n@@ -525,0 +536,5 @@\n+      if (_rs->addr_for_card_index(card_index) > end_of_range) {\n+        count = 0;\n+        card_index = end_card_index;\n+        break;\n+      }\n@@ -535,0 +551,2 @@\n+          assert(!r->is_humongous(), \"Process humongous regions elsewhere\");\n+\n@@ -572,2 +590,1 @@\n-              \/\/ This object is not marked so we don't scan it.\n-              ShenandoahHeapRegion* r = heap->heap_region_containing(p);\n+              \/\/ This object is not marked so we don't scan it.  Containing region r is initialized above.\n@@ -621,0 +638,5 @@\n+          \/\/ TODO: only iterate over this object if it spans dirty within this cluster or within following clusters.\n+          \/\/ Code as written is known not to examine a zombie object because either the object is marked, or we are\n+          \/\/ not using the mark-context to differentiate objects, so the object is known to have been coalesced and\n+          \/\/ filled if it is not \"live\".\n+\n@@ -638,2 +660,1 @@\n-          \/\/ unmarked neighbors.\n-          ShenandoahHeapRegion* r = heap->heap_region_containing(p);\n+          \/\/ unmarked neighbors.  Containing region r is initialized above.\n@@ -659,0 +680,3 @@\n+\n+\/\/ Given that this range of clusters is known to span a humongous object spanned by region r, scan the\n+\/\/ portion of the humongous object that corresponds to the specified range.\n@@ -662,2 +686,15 @@\n-ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl) {\n-  process_region(region, cl, false);\n+ShenandoahScanRemembered<RememberedSet>::process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                                                    HeapWord *end_of_range, ClosureType *cl, bool write_table,\n+                                                                    bool is_concurrent) {\n+  ShenandoahHeapRegion* start_region = r->humongous_start_region();\n+  HeapWord* p = start_region->bottom();\n+  oop obj = cast_to_oop(p);\n+  assert(r->is_humongous(), \"Only process humongous regions here\");\n+  assert(start_region->is_humongous_start(), \"Should be start of humongous region\");\n+  assert(p + obj->size() >= end_of_range, \"Humongous object ends before range ends\");\n+\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  HeapWord* first_cluster_addr = _rs->addr_for_card_index(first_card_index);\n+  size_t spanned_words = count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster * CardTable::card_size_in_words();\n+\n+  start_region->oop_iterate_humongous_slice(cl, true, first_cluster_addr, spanned_words, write_table, is_concurrent);\n@@ -669,2 +706,25 @@\n-ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl, bool use_write_table) {\n-  HeapWord *start_of_range = region->bottom();\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl, bool is_concurrent) {\n+  process_region(region, cl, false, is_concurrent);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl,\n+                                                        bool use_write_table, bool is_concurrent) {\n+  size_t cluster_size =\n+    CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t clusters = ShenandoahHeapRegion::region_size_words() \/ cluster_size;\n+  process_region_slice(region, 0, clusters, region->end(), cl, use_write_table, is_concurrent);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region_slice(ShenandoahHeapRegion *region, size_t start_offset, size_t clusters,\n+                                                              HeapWord *end_of_range, ClosureType *cl, bool use_write_table,\n+                                                              bool is_concurrent) {\n+  HeapWord *start_of_range = region->bottom() + start_offset;\n+  size_t cluster_size =\n+    CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t words = clusters * cluster_size;\n@@ -672,0 +732,1 @@\n+  assert(addr_for_cluster(start_cluster_no) == start_of_range, \"process_region_slice range must align on cluster boundary\");\n@@ -679,1 +740,0 @@\n-  HeapWord *end_of_range;\n@@ -682,1 +742,3 @@\n-    end_of_range = region->get_update_watermark();\n+    if (end_of_range > region->get_update_watermark()) {\n+      end_of_range = region->get_update_watermark();\n+    }\n@@ -688,1 +750,3 @@\n-    end_of_range = region->top();\n+    if (end_of_range > region->top()) {\n+      end_of_range = region->top();\n+    }\n@@ -692,1 +756,1 @@\n-                region->index(), p2i(region->bottom()), p2i(end_of_range),\n+                region->index(), p2i(start_of_range), p2i(end_of_range),\n@@ -694,1 +758,4 @@\n-  \/\/ end_of_range may point to the middle of a cluster because region->top() may be different than region->end().\n+\n+  \/\/ Note that end_of_range may point to the middle of a cluster because region->top() or region->get_update_watermark() may\n+  \/\/ be less than start_of_range + words.\n+\n@@ -700,3 +767,0 @@\n-  size_t num_heapwords = end_of_range - start_of_range;\n-  unsigned int cluster_size = CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n-  size_t num_clusters = (size_t) ((num_heapwords - 1 + cluster_size) \/ cluster_size);\n@@ -704,3 +768,9 @@\n-  if (!region->is_humongous_continuation()) {\n-    \/\/ Remembered set scanner\n-    process_clusters(start_cluster_no, num_clusters, end_of_range, cl, use_write_table);\n+  \/\/ If I am assigned to process a range that starts beyond end_of_range (top or update-watermark), we have no work to do.\n+\n+  if (start_of_range < end_of_range) {\n+    if (region->is_humongous()) {\n+      ShenandoahHeapRegion* start_region = region->humongous_start_region();\n+      process_humongous_clusters(start_region, start_cluster_no, clusters, end_of_range, cl, use_write_table, is_concurrent);\n+    } else {\n+      process_clusters(start_cluster_no, clusters, end_of_range, cl, use_write_table, is_concurrent);\n+    }\n@@ -718,0 +788,7 @@\n+template<typename RememberedSet>\n+inline HeapWord*\n+ShenandoahScanRemembered<RememberedSet>::addr_for_cluster(size_t cluster_no) {\n+  size_t card_index = cluster_no * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  return addr_for_card_index(card_index);\n+}\n+\n@@ -734,1 +811,6 @@\n-      process_clusters(start_cluster_no, num_clusters, end_of_range, cl);\n+      if (region->is_humongous()) {\n+        process_humongous_clusters(region->humongous_start_region(), start_cluster_no, num_clusters, end_of_range, cl,\n+                                   false \/* is_write_table *\/, false \/* is_concurrent *\/);\n+      } else {\n+        process_clusters(start_cluster_no, num_clusters, end_of_range, cl, false \/* is_concurrent *\/);\n+      }\n@@ -739,0 +821,39 @@\n+inline bool ShenandoahRegionChunkIterator::has_next() const {\n+  return _index < _total_chunks;\n+}\n+\n+inline bool ShenandoahRegionChunkIterator::next(work_chunk *assignment) {\n+  if (_index > _total_chunks) {\n+    return false;\n+  }\n+  size_t new_index = Atomic::add(&_index, (size_t) 1, memory_order_relaxed);\n+  if (new_index > _total_chunks) {\n+    return false;\n+  }\n+  \/\/ convert to zero-based indexing\n+  new_index--;\n+\n+  size_t group_no = new_index \/ _group_size;\n+  if (group_no + 1 > _num_groups) {\n+    group_no = _num_groups - 1;\n+  }\n+\n+  \/\/ All size computations measured in HeapWord\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t group_region_index = _region_index[group_no];\n+  size_t group_region_offset = _group_offset[group_no];\n+\n+  size_t index_within_group = new_index - (group_no * _group_size);\n+  size_t group_chunk_size = _first_group_chunk_size >> group_no;\n+  size_t offset_of_this_chunk = group_region_offset + index_within_group * group_chunk_size;\n+  size_t regions_spanned_by_chunk_offset = offset_of_this_chunk \/ region_size_words;\n+  size_t region_index = group_region_index + regions_spanned_by_chunk_offset;\n+  size_t offset_within_region = offset_of_this_chunk % region_size_words;\n+\n+  assignment->_r = _heap->get_region(region_index);\n+  assignment->_chunk_offset = offset_within_region;\n+  assignment->_chunk_size = group_chunk_size;\n+\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":151,"deletions":30,"binary":false,"changes":181,"status":"modified"},{"patch":"@@ -59,3 +59,0 @@\n-  size_t _plab_evacuated;\n-  size_t _plab_promoted;\n-\n@@ -66,0 +63,6 @@\n+  size_t _plab_evacuated;\n+  size_t _plab_promoted;\n+  size_t _plab_preallocated_promoted;\n+  bool   _plab_retries_enabled;\n+\n+\n@@ -75,0 +78,2 @@\n+    _disarmed_value(0),\n+    _paced_time(0),\n@@ -77,2 +82,2 @@\n-    _disarmed_value(0),\n-    _paced_time(0) {\n+    _plab_preallocated_promoted(0),\n+    _plab_retries_enabled(true) {\n@@ -158,0 +163,12 @@\n+  static void enable_plab_retries(Thread* thread) {\n+    data(thread)->_plab_retries_enabled = true;\n+  }\n+\n+  static void disable_plab_retries(Thread* thread) {\n+    data(thread)->_plab_retries_enabled = false;\n+  }\n+\n+  static bool plab_retries_enabled(Thread* thread) {\n+    return data(thread)->_plab_retries_enabled;\n+  }\n+\n@@ -202,0 +219,8 @@\n+  static void set_plab_preallocated_promoted(Thread* thread, size_t value) {\n+    data(thread)->_plab_preallocated_promoted = value;\n+  }\n+\n+  static size_t get_plab_preallocated_promoted(Thread* thread) {\n+    return data(thread)->_plab_preallocated_promoted;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":30,"deletions":5,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -270,2 +270,3 @@\n-          \"consulted at the of marking, before selecting the collection \"   \\\n-          \"set.  If available memory at this time is smaller than the \"     \\\n+          \"consulted at the end of marking, before selecting the \"          \\\n+          \"collection set.  \"                                               \\\n+          \"If available memory at this time is smaller than the \"           \\\n@@ -326,1 +327,1 @@\n-  product(uintx, ShenandoahOldEvacReserve, 2, EXPERIMENTAL,                 \\\n+  product(uintx, ShenandoahOldEvacReserve, 5, EXPERIMENTAL,                 \\\n@@ -333,1 +334,1 @@\n-          \"regions included in the collecdtion set is the smaller \"         \\\n+          \"regions included in the collection set is the smaller \"          \\\n@@ -343,1 +344,2 @@\n-          \"than one eighth (12%) of the collection set evacuation \"         \\\n+          \"than one eighth (~12%) of the potential collection set \"         \\\n+          \"evacuation \"                                                     \\\n@@ -513,0 +515,7 @@\n+                                                                            \\\n+  product(uintx, ShenandoahOldCompactionReserve, 8, EXPERIMENTAL,           \\\n+          \"During generational GC, prevent promotions from filling \"        \\\n+          \"this number of heap regions.  These regions are reserved \"       \\\n+          \"for the purpose of supporting compaction of old-gen \"            \\\n+          \"memory.  Otherwise, old-gen memory cannot be compacted.\")        \\\n+          range(0, 128)                                                     \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"}]}