{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -36,0 +37,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -243,1 +245,1 @@\n-  marking = __ AndI(ld, __ ConI(ShenandoahHeap::MARKING));\n+  marking = __ AndI(ld, __ ConI(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n@@ -324,1 +326,1 @@\n-      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::MARKING)) {\n+      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING)) {\n@@ -453,0 +455,86 @@\n+Node* ShenandoahBarrierSetC2::byte_map_base_node(GraphKit* kit) const {\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  CardTable::CardValue* card_table_base = ctbs->card_table()->byte_map_base();\n+  if (card_table_base != nullptr) {\n+    return kit->makecon(TypeRawPtr::make((address)card_table_base));\n+  } else {\n+    return kit->null();\n+  }\n+}\n+\n+void ShenandoahBarrierSetC2::post_barrier(GraphKit* kit,\n+                                          Node* ctl,\n+                                          Node* oop_store,\n+                                          Node* obj,\n+                                          Node* adr,\n+                                          uint  adr_idx,\n+                                          Node* val,\n+                                          BasicType bt,\n+                                          bool use_precise) const {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  \/\/ No store check needed if we're storing a null.\n+  if (val != nullptr && val->is_Con()) {\n+    \/\/ must be either an oop or NULL\n+    const Type* t = val->bottom_type();\n+    if (t == TypePtr::NULL_PTR || t == Type::TOP)\n+      return;\n+  }\n+\n+  if (ReduceInitialCardMarks && obj == kit->just_allocated_object(kit->control())) {\n+    \/\/ We can skip marks on a freshly-allocated object in Eden.\n+    \/\/ Keep this code in sync with new_deferred_store_barrier() in runtime.cpp.\n+    \/\/ That routine informs GC to take appropriate compensating steps,\n+    \/\/ upon a slow-path allocation, so as to make this card-mark\n+    \/\/ elision safe.\n+    return;\n+  }\n+\n+  if (!use_precise) {\n+    \/\/ All card marks for a (non-array) instance are in one place:\n+    adr = obj;\n+  }\n+  \/\/ (Else it's an array (or unknown), and we want more precise card marks.)\n+  assert(adr != nullptr, \"\");\n+\n+  IdealKit ideal(kit, true);\n+\n+  \/\/ Convert the pointer to an int prior to doing math on it\n+  Node* cast = __ CastPX(__ ctrl(), adr);\n+\n+  \/\/ Divide by card size\n+  Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift()) );\n+\n+  \/\/ Combine card table base and card offset\n+  Node* card_adr = __ AddP(__ top(), byte_map_base_node(kit), card_offset );\n+\n+  \/\/ Get the alias_index for raw card-mark memory\n+  int adr_type = Compile::AliasIdxRaw;\n+  Node*   zero = __ ConI(0); \/\/ Dirty card value\n+\n+  if (UseCondCardMark) {\n+    \/\/ The classic GC reference write barrier is typically implemented\n+    \/\/ as a store into the global card mark table.  Unfortunately\n+    \/\/ unconditional stores can result in false sharing and excessive\n+    \/\/ coherence traffic as well as false transactional aborts.\n+    \/\/ UseCondCardMark enables MP \"polite\" conditional card mark\n+    \/\/ stores.  In theory we could relax the load from ctrl() to\n+    \/\/ no_ctrl, but that doesn't buy much latitude.\n+    Node* card_val = __ load( __ ctrl(), card_adr, TypeInt::BYTE, T_BYTE, adr_type);\n+    __ if_then(card_val, BoolTest::ne, zero);\n+  }\n+\n+  \/\/ Smash zero into card\n+  __ store(__ ctrl(), card_adr, zero, T_BYTE, adr_type, MemNode::unordered);\n+\n+  if (UseCondCardMark) {\n+    __ end_if();\n+  }\n+\n+  \/\/ Final sync IdealKit and GraphKit.\n+  kit->final_sync(ideal);\n+}\n+\n@@ -516,0 +604,9 @@\n+\n+    Node* result = BarrierSetC2::store_at_resolved(access, val);\n+\n+    bool anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+    bool is_array = (decorators & IS_ARRAY) != 0;\n+    bool use_precise = is_array || anonymous;\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 adr, adr_idx, val.node(), access.type(), use_precise);\n+    return result;\n@@ -526,0 +623,1 @@\n+    return BarrierSetC2::store_at_resolved(access, val);\n@@ -527,1 +625,0 @@\n-  return BarrierSetC2::store_at_resolved(access, val);\n@@ -598,1 +695,1 @@\n-                                                   Node* new_val, const Type* value_type) const {\n+                                                             Node* new_val, const Type* value_type) const {\n@@ -640,0 +737,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n@@ -695,0 +794,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n@@ -711,0 +812,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), val, T_OBJECT, true);\n@@ -841,1 +944,1 @@\n-      flags |= ShenandoahHeap::MARKING;\n+      flags |= ShenandoahHeap::YOUNG_MARKING;\n@@ -909,3 +1012,19 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n-  if (is_shenandoah_wb_pre_call(n)) {\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  if (is_shenandoah_wb_pre_call(node)) {\n+    shenandoah_eliminate_wb_pre(node, &macro->igvn());\n+  }\n+  if (node->Opcode() == Op_CastP2X && ShenandoahHeap::heap()->mode()->is_generational()) {\n+    Node* shift = node->unique_out();\n+    Node* addp = shift->unique_out();\n+    for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n+      Node* mem = addp->last_out(j);\n+      if (UseCondCardMark && mem->is_Load()) {\n+        assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+        \/\/ The load is checking if the card has been written so\n+        \/\/ replace it with zero to fold the test.\n+        macro->replace_node(mem, macro->intcon(0));\n+        continue;\n+      }\n+      assert(mem->is_Store(), \"store required\");\n+      macro->replace_node(mem, mem->in(MemNode::Memory));\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":127,"deletions":8,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -36,0 +37,3 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n@@ -39,0 +43,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -46,0 +51,1 @@\n+class PLAB;\n@@ -48,0 +54,1 @@\n+class ShenandoahRegulatorThread;\n@@ -50,0 +57,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -51,0 +61,1 @@\n+class ShenandoahOldHeuristics;\n@@ -52,1 +63,0 @@\n-class ShenandoahMode;\n@@ -62,0 +72,1 @@\n+class ShenandoahMode;\n@@ -111,0 +122,10 @@\n+template<ShenandoahGenerationType GENERATION>\n+class ShenandoahGenerationRegionClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  explicit ShenandoahGenerationRegionClosure(ShenandoahHeapRegionClosure* cl) : _cl(cl) {}\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+  virtual bool is_thread_safe() { return _cl->is_thread_safe(); }\n+ private:\n+  ShenandoahHeapRegionClosure* _cl;\n+};\n+\n@@ -128,0 +149,1 @@\n+  friend class ShenandoahOldGC;\n@@ -136,0 +158,4 @@\n+  ShenandoahGeneration* _gc_generation;\n+\n+  \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+  bool _prepare_for_old_mark;\n@@ -142,0 +168,15 @@\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  ShenandoahOldHeuristics* old_heuristics();\n+\n+  bool doing_mixed_evacuations();\n+  bool is_old_bitmap_stable() const;\n+  bool is_gc_generation_young() const;\n+\n@@ -153,2 +194,1 @@\n-  void initialize_mode();\n-  void initialize_heuristics();\n+  void initialize_heuristics_generations();\n@@ -168,0 +208,3 @@\n+  bool verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                               bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste);\n+\n@@ -178,0 +221,8 @@\n+           size_t _promotion_potential;\n+           size_t _promotion_in_place_potential;\n+           size_t _pad_for_promote_in_place;    \/\/ bytes of filler\n+           size_t _promotable_humongous_regions;\n+           size_t _promotable_humongous_usage;\n+           size_t _regular_regions_promoted_in_place;\n+           size_t _regular_usage_promoted_in_place;\n+\n@@ -180,2 +231,0 @@\n-  volatile size_t _used;\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -185,0 +234,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -186,3 +237,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -192,2 +244,0 @@\n-  void increase_allocated(size_t bytes);\n-  size_t bytes_allocated_since_gc_start();\n@@ -230,0 +280,1 @@\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahAffiliation, including FREE status in non-generational mode\n@@ -247,0 +298,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -261,2 +314,2 @@\n-    \/\/ Heap is under marking: needs SATB barriers.\n-    MARKING_BITPOS    = 1,\n+    \/\/ Young regions are under marking: needs SATB barriers.\n+    YOUNG_MARKING_BITPOS    = 1,\n@@ -272,0 +325,3 @@\n+\n+    \/\/ Old regions are under marking, still need SATB barriers.\n+    OLD_MARKING_BITPOS = 5\n@@ -277,1 +333,1 @@\n-    MARKING       = 1 << MARKING_BITPOS,\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n@@ -281,0 +337,1 @@\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -291,0 +348,43 @@\n+  \/\/ TODO: Revisit the following comment.  It may not accurately represent the true behavior when evacuations fail due to\n+  \/\/ difficulty finding memory to hold evacuated objects.\n+  \/\/\n+  \/\/ Note that the typical total expenditure on evacuation is less than the associated evacuation reserve because we generally\n+  \/\/ reserve ShenandoahEvacWaste (> 1.0) times the anticipated evacuation need.  In the case that there is an excessive amount\n+  \/\/ of waste, it may be that one thread fails to grab a new GCLAB, this does not necessarily doom the associated evacuation\n+  \/\/ effort.  If this happens, the requesting thread blocks until some other thread manages to evacuate the offending object.\n+  \/\/ Only after \"all\" threads fail to evacuate an object do we consider the evacuation effort to have failed.\n+\n+  \/\/ How many full-gc cycles have been completed?\n+  volatile size_t _completed_fullgc_cycles;\n+\n+  size_t _promoted_reserve;            \/\/ Bytes reserved within old-gen to hold the results of promotion\n+  volatile size_t _promoted_expended;  \/\/ Bytes of old-gen memory expended on promotions\n+\n+  \/\/ Allocation of old GCLABs (aka PLABs) assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n+  \/\/  is authorized, increment _old_evac_expended by request size.  This allocation ignores old_gen->available().\n+\n+  size_t _old_evac_reserve;            \/\/ Bytes reserved within old-gen to hold evacuated objects from old-gen collection set\n+  volatile size_t _old_evac_expended;  \/\/ Bytes of old-gen memory expended on old-gen evacuations\n+\n+  size_t _young_evac_reserve;          \/\/ Bytes reserved within young-gen to hold evacuated objects from young-gen collection set\n+\n+  size_t _captured_old_usage;          \/\/ What was old usage (bytes) when last captured?\n+\n+  size_t _previous_promotion;          \/\/ Bytes promoted during previous evacuation\n+\n+  bool _upgraded_to_full;\n+\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantitites, stored in _promoted_reserve,\n+  \/\/ _old_evac_reserve, and _young_evac_reserve, are consulted prior to rebuilding the free set (ShenandoahFreeSet)\n+  \/\/ in preparation for evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the\n+  \/\/ collector and old_collector sets to hold if _has_evacuation_reserve_quantities is true.  The other time we\n+  \/\/ rebuild the freeset is at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ _has_evacuation_reserve_quantities is false because we don't yet know how much memory will need to be evacuated\n+  \/\/ in the next GC cycle.  When _has_evacuation_reserve_quantities is false, the free set rebuild operation reserves\n+  \/\/ for the collector and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve,\n+  \/\/ ShenandoahOldEvacReserve, and ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve\n+  \/\/ for old_collector set when not _has_evacuation_reserve_quantities is based in part on anticipated promotion as\n+  \/\/ determined by analysis of live data found during the previous GC pass which is one less than the current tenure age.\n+  bool _has_evacuation_reserve_quantities;\n+\n@@ -298,1 +398,3 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_evacuation_reserve_quantities(bool is_valid);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -307,0 +409,3 @@\n+  void set_prepare_for_old_mark_in_progress(bool cond);\n+  void set_aging_cycle(bool cond);\n+\n@@ -310,0 +415,1 @@\n+  inline bool has_evacuation_reserve_quantities() const;\n@@ -311,0 +417,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -321,0 +429,53 @@\n+  inline bool is_prepare_for_old_mark_in_progress() const;\n+  inline bool is_aging_cycle() const;\n+  inline bool upgraded_to_full() { return _upgraded_to_full; }\n+  inline void start_conc_gc() { _upgraded_to_full = false; }\n+  inline void record_upgrade_to_full() { _upgraded_to_full = true; }\n+\n+  inline size_t capture_old_usage(size_t usage);\n+  inline void set_previous_promotion(size_t promoted_bytes);\n+  inline size_t get_previous_promotion() const;\n+\n+  inline void clear_promotion_potential() { _promotion_potential = 0; };\n+  inline void set_promotion_potential(size_t val) { _promotion_potential = val; };\n+  inline size_t get_promotion_potential() { return _promotion_potential; };\n+\n+  inline void clear_promotion_in_place_potential() { _promotion_in_place_potential = 0; };\n+  inline void set_promotion_in_place_potential(size_t val) { _promotion_in_place_potential = val; };\n+  inline size_t get_promotion_in_place_potential() { return _promotion_in_place_potential; };\n+\n+  inline void set_pad_for_promote_in_place(size_t pad) { _pad_for_promote_in_place = pad; }\n+  inline size_t get_pad_for_promote_in_place() { return _pad_for_promote_in_place; }\n+\n+  inline void reserve_promotable_humongous_regions(size_t region_count) { _promotable_humongous_regions = region_count; }\n+  inline void reserve_promotable_humongous_usage(size_t bytes) { _promotable_humongous_usage = bytes; }\n+  inline void reserve_promotable_regular_regions(size_t region_count) { _regular_regions_promoted_in_place = region_count; }\n+  inline void reserve_promotable_regular_usage(size_t used_bytes) { _regular_usage_promoted_in_place = used_bytes; }\n+\n+  inline size_t get_promotable_humongous_regions() { return _promotable_humongous_regions; }\n+  inline size_t get_promotable_humongous_usage() { return _promotable_humongous_usage; }\n+  inline size_t get_regular_regions_promoted_in_place() { return _regular_regions_promoted_in_place; }\n+  inline size_t get_regular_usage_promoted_in_place() { return _regular_usage_promoted_in_place; }\n+\n+  \/\/ Returns previous value\n+  inline size_t set_promoted_reserve(size_t new_val);\n+  inline size_t get_promoted_reserve() const;\n+  inline void augment_promo_reserve(size_t increment);\n+\n+  inline void reset_promoted_expended();\n+  inline size_t expend_promoted(size_t increment);\n+  inline size_t unexpend_promoted(size_t decrement);\n+  inline size_t get_promoted_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_old_evac_reserve(size_t new_val);\n+  inline size_t get_old_evac_reserve() const;\n+  inline void augment_old_evac_reserve(size_t increment);\n+\n+  inline void reset_old_evac_expended();\n+  inline size_t expend_old_evac(size_t increment);\n+  inline size_t get_old_evac_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_young_evac_reserve(size_t new_val);\n+  inline size_t get_young_evac_reserve() const;\n@@ -323,0 +484,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -333,0 +496,1 @@\n+  double _cancel_requested_time;\n@@ -334,0 +498,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -337,2 +506,0 @@\n-  static address cancelled_gc_addr();\n-\n@@ -342,1 +509,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -344,0 +511,1 @@\n+  void cancel_concurrent_mark();\n@@ -353,4 +521,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n-  void prepare_evacuation(bool concurrent);\n@@ -369,1 +533,0 @@\n-  void rebuild_free_set(bool concurrent);\n@@ -374,0 +537,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -380,0 +544,4 @@\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahGeneration*      _global_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n@@ -381,0 +549,1 @@\n+  ShenandoahRegulatorThread* _regulator_thread;\n@@ -383,1 +552,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -388,1 +556,4 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+  ShenandoahMmuTracker          _mmu_tracker;\n+  ShenandoahGenerationSizer     _generation_sizer;\n@@ -390,1 +561,1 @@\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n@@ -393,0 +564,10 @@\n+  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahOldGeneration*   old_generation()    const { return _old_generation;    }\n+  ShenandoahGeneration*      generation_for(ShenandoahAffiliation affiliation) const;\n+  const ShenandoahGenerationSizer* generation_sizer()  const { return &_generation_sizer;  }\n+\n+  size_t max_size_for(ShenandoahGeneration* generation) const;\n+  size_t min_size_for(ShenandoahGeneration* generation) const;\n+\n@@ -395,1 +576,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -399,1 +579,5 @@\n-  ShenandoahPhaseTimings*    phase_timings()     const { return _phase_timings;     }\n+  ShenandoahPhaseTimings*      phase_timings()   const { return _phase_timings;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return  _evac_tracker;     }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n@@ -408,0 +592,3 @@\n+  MemoryPool*                  _young_gen_memory_pool;\n+  MemoryPool*                  _old_gen_memory_pool;\n+\n@@ -416,1 +603,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -427,8 +614,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -438,0 +617,1 @@\n+  ShenandoahSharedFlag  _is_aging_cycle;\n@@ -453,0 +633,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                          ShenandoahAffiliation new_affiliation);\n+\n@@ -466,1 +649,12 @@\n-  bool is_in(const void* p) const override;\n+  inline bool is_in(const void* p) const override;\n+\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation);\n+\n+  inline ShenandoahAffiliation region_affiliation(size_t index);\n+  inline void set_affiliation(size_t index, ShenandoahAffiliation new_affiliation);\n@@ -520,1 +714,6 @@\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n+  \/\/ How many bytes to transfer between old and young after we have finished recycling collection set regions?\n+  size_t _old_regions_surplus;\n+  size_t _old_regions_deficit;\n+\n+  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region, bool is_promotion);\n+\n@@ -525,0 +724,4 @@\n+  inline HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n@@ -526,1 +729,1 @@\n-  HeapWord* allocate_memory(ShenandoahAllocRequest& request);\n+  HeapWord* allocate_memory(ShenandoahAllocRequest& request, bool is_promotion);\n@@ -532,1 +735,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n@@ -546,0 +749,8 @@\n+  void set_young_lab_region_flags();\n+\n+  inline void set_old_region_surplus(size_t surplus) { _old_regions_surplus = surplus; };\n+  inline void set_old_region_deficit(size_t deficit) { _old_regions_deficit = deficit; };\n+\n+  inline size_t get_old_region_surplus() { return _old_regions_surplus; };\n+  inline size_t get_old_region_deficit() { return _old_regions_deficit; };\n+\n@@ -570,2 +781,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -582,2 +791,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -603,0 +810,5 @@\n+  ShenandoahSharedFlag _old_gen_oom_evac;\n+\n+  inline oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n+  void handle_old_evacuation(HeapWord* obj, size_t words, bool promotion);\n+  void handle_old_evacuation_failure();\n@@ -605,0 +817,3 @@\n+  void handle_promotion_failure();\n+  void report_promotion_failure(Thread* thread, size_t size);\n+\n@@ -615,1 +830,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -623,0 +838,20 @@\n+  inline bool clear_old_evacuation_failure();\n+\n+\/\/ ---------- Generational support\n+\/\/\n+private:\n+  RememberedScanner* _card_scan;\n+\n+public:\n+  inline RememberedScanner* card_scan() { return _card_scan; }\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+  void dirty_cards(HeapWord* start, HeapWord* end);\n+  void clear_cards(HeapWord* start, HeapWord* end);\n+  void mark_card_as_dirty(void* location);\n+  void retire_plab(PLAB* plab);\n+  void retire_plab(PLAB* plab, Thread* thread);\n+  void cancel_old_gc();\n+  bool is_old_gc_active();\n+  void coalesce_and_fill_old_regions();\n+  void adjust_generation_sizes_for_next_cycle(size_t old_xfer_limit, size_t young_cset_regions, size_t old_cset_regions);\n+\n@@ -644,1 +879,8 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+  static inline uint get_object_age(oop obj);\n+\n+  void transfer_old_pointers_from_satb();\n+\n+  void log_heap_status(const char *msg) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":288,"deletions":46,"binary":false,"changes":334,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -45,0 +46,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -46,0 +48,2 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -255,1 +259,1 @@\n-inline void ShenandoahHeap::clear_cancelled_gc() {\n+inline void ShenandoahHeap::clear_cancelled_gc(bool clear_oom_handler) {\n@@ -257,1 +261,9 @@\n-  _oom_evac_handler.clear();\n+  if (_cancel_requested_time > 0) {\n+    double cancel_time = os::elapsedTime() - _cancel_requested_time;\n+    log_info(gc)(\"GC cancellation took %.3fs\", cancel_time);\n+    _cancel_requested_time = 0;\n+  }\n+\n+  if (clear_oom_handler) {\n+    _oom_evac_handler.clear();\n+  }\n@@ -274,1 +286,0 @@\n-  \/\/ Otherwise...\n@@ -278,0 +289,32 @@\n+inline HeapWord* ShenandoahHeap::allocate_from_plab(Thread* thread, size_t size, bool is_promotion) {\n+  assert(UseTLAB, \"TLABs should be enabled\");\n+\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  HeapWord* obj;\n+\n+  if (plab == nullptr) {\n+    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(), \"Performance: thread should have PLAB: %s\", thread->name());\n+    \/\/ No PLABs in this thread, fallback to shared allocation\n+    return nullptr;\n+  } else if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    return nullptr;\n+  }\n+  \/\/ if plab->word_size() <= 0, thread's plab not yet initialized for this pass, so allow_plab_promotions() is not trustworthy\n+  obj = plab->allocate(size);\n+  if ((obj == nullptr) && (plab->words_remaining() < PLAB::min_size())) {\n+    \/\/ allocate_from_plab_slow will establish allow_plab_promotions(thread) for future invocations\n+    obj = allocate_from_plab_slow(thread, size, is_promotion);\n+  }\n+  \/\/ if plab->words_remaining() >= PLAB::min_size(), just return nullptr so we can use a shared allocation\n+  if (obj == nullptr) {\n+    return nullptr;\n+  }\n+\n+  if (is_promotion) {\n+    ShenandoahThreadLocalData::add_to_plab_promoted(thread, size * HeapWordSize);\n+  } else {\n+    ShenandoahThreadLocalData::add_to_plab_evacuated(thread, size * HeapWordSize);\n+  }\n+  return obj;\n+}\n+\n@@ -279,1 +322,2 @@\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(Thread::current())) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n@@ -287,1 +331,2 @@\n-  size_t size = p->size();\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n@@ -289,1 +334,21 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n+  ShenandoahAffiliation target_gen = r->affiliation();\n+  if (mode()->is_generational() && ShenandoahHeap::heap()->is_gc_generation_young() &&\n+      target_gen == YOUNG_GENERATION) {\n+    markWord mark = p->mark();\n+    if (mark.is_marked()) {\n+      \/\/ Already forwarded.\n+      return ShenandoahBarrierSet::resolve_forwarded(p);\n+    }\n+    if (mark.has_displaced_mark_helper()) {\n+      \/\/ We don't want to deal with MT here just to ensure we read the right mark word.\n+      \/\/ Skip the potential promotion attempt for this one.\n+    } else if (r->age() + mark.age() >= InitialTenuringThreshold) {\n+      oop result = try_evacuate_object(p, thread, r, OLD_GENERATION);\n+      if (result != nullptr) {\n+        return result;\n+      }\n+      \/\/ If we failed to promote this aged object, we'll fall through to code below and evacuate to young-gen.\n+    }\n+  }\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n@@ -291,1 +356,6 @@\n-  bool alloc_from_gclab = true;\n+\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n+\/\/ to OLD_GENERATION.\n+inline oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahAffiliation target_gen) {\n+  bool alloc_from_lab = true;\n+  bool has_plab = false;\n@@ -293,0 +363,2 @@\n+  size_t size = p->size();\n+  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n@@ -301,1 +373,45 @@\n-      copy = allocate_from_gclab(thread, size);\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+           copy = allocate_from_gclab(thread, size);\n+           if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+             \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+             \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+             ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+             copy = allocate_from_gclab(thread, size);\n+             \/\/ If we still get nullptr, we'll try a shared allocation below.\n+           }\n+           break;\n+        }\n+        case OLD_GENERATION: {\n+           PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+           if (plab != nullptr) {\n+             has_plab = true;\n+           }\n+           copy = allocate_from_plab(thread, size, is_promotion);\n+           if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n+               ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+             \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n+             \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n+             \/\/ where abundance is defined as >= PLAB::min_size().  In the former case, we try resetting the desired\n+             \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n+\n+             \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n+             \/\/ this particular allocation to be shared.\n+             if (plab->words_remaining() < PLAB::min_size()) {\n+               ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n+               copy = allocate_from_plab(thread, size, is_promotion);\n+               \/\/ If we still get nullptr, we'll try a shared allocation below.\n+               if (copy == nullptr) {\n+                 \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n+                 ShenandoahThreadLocalData::disable_plab_retries(thread);\n+               }\n+             }\n+             \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n+           }\n+           break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n@@ -303,0 +419,1 @@\n+\n@@ -304,3 +421,10 @@\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size);\n-      copy = allocate_memory(req);\n-      alloc_from_gclab = false;\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+        copy = allocate_memory(req, is_promotion);\n+        alloc_from_lab = false;\n+      }\n+      \/\/ else, we leave copy equal to nullptr, signaling a promotion failure below if appropriate.\n+      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n@@ -313,0 +437,14 @@\n+    if (target_gen == OLD_GENERATION) {\n+      assert(mode()->is_generational(), \"Should only be here in generational mode.\");\n+      if (from_region->is_young()) {\n+        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n+        report_promotion_failure(thread, size);\n+        handle_promotion_failure();\n+        return nullptr;\n+      } else {\n+        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n+        \/\/ after the evacuation threads have finished.\n+        handle_old_evacuation_failure();\n+      }\n+    }\n+\n@@ -321,0 +459,1 @@\n+  _evac_tracker->begin_evacuation(thread, size * HeapWordSize);\n@@ -323,1 +462,6 @@\n-  \/\/ Try to install the new forwarding pointer.\n+\n+  if (mode()->is_generational() && target_gen == YOUNG_GENERATION && is_aging_cycle()) {\n+    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n+  }\n+\n+  \/\/ Try to install the new forwarding pointer.\n@@ -330,0 +474,4 @@\n+    _evac_tracker->end_evacuation(thread, size * HeapWordSize, ShenandoahHeap::get_object_age(copy_val));\n+    if (mode()->is_generational() && target_gen == OLD_GENERATION) {\n+      handle_old_evacuation(copy, size, from_region->is_young());\n+    }\n@@ -338,8 +486,23 @@\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n-      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+    if (alloc_from_lab) {\n+       \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+       \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+       \/\/ do this.\n+       switch (target_gen) {\n+         case YOUNG_GENERATION: {\n+             ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+            break;\n+         }\n+         case OLD_GENERATION: {\n+            ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n+            if (is_promotion) {\n+              ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n+            } else {\n+              ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n+            }\n+            break;\n+         }\n+         default: {\n+           ShouldNotReachHere();\n+           break;\n+         }\n+       }\n@@ -347,0 +510,4 @@\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -349,0 +516,1 @@\n+      \/\/ For non-LAB allocations, the object has already been registered\n@@ -355,0 +523,114 @@\n+void ShenandoahHeap::increase_object_age(oop obj, uint additional_age) {\n+  markWord w = obj->has_displaced_mark() ? obj->displaced_mark() : obj->mark();\n+  w = w.set_age(MIN2(markWord::max_age, w.age() + additional_age));\n+  if (obj->has_displaced_mark()) {\n+    obj->set_displaced_mark(w);\n+  } else {\n+    obj->set_mark(w);\n+  }\n+}\n+\n+uint ShenandoahHeap::get_object_age(oop obj) {\n+  markWord w = obj->has_displaced_mark() ? obj->displaced_mark() : obj->mark();\n+  return w.age();\n+}\n+\n+inline bool ShenandoahHeap::clear_old_evacuation_failure() {\n+  return _old_gen_oom_evac.try_unset();\n+}\n+\n+bool ShenandoahHeap::is_in(const void* p) const {\n+  HeapWord* heap_base = (HeapWord*) base();\n+  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n+  return p >= heap_base && p < last_region_end;\n+}\n+\n+inline bool ShenandoahHeap::is_in_active_generation(oop obj) const {\n+  if (!mode()->is_generational()) {\n+    \/\/ everything is the same single generation\n+    return true;\n+  }\n+\n+  if (active_generation() == nullptr) {\n+    \/\/ no collection is happening, only expect this to be called\n+    \/\/ when concurrent processing is active, but that could change\n+    return false;\n+  }\n+\n+  assert(is_in(obj), \"only check if is in active generation for objects (\" PTR_FORMAT \") in heap\", p2i(obj));\n+  assert((active_generation() == (ShenandoahGeneration*) old_generation()) ||\n+         (active_generation() == (ShenandoahGeneration*) young_generation()) ||\n+         (active_generation() == global_generation()), \"Active generation must be old, young, or global\");\n+\n+  size_t index = heap_region_containing(obj)->index();\n+  switch (_affiliations[index]) {\n+  case ShenandoahAffiliation::FREE:\n+    \/\/ Free regions are in Old, Young, Global\n+    return true;\n+  case ShenandoahAffiliation::YOUNG_GENERATION:\n+    \/\/ Young regions are in young_generation and global_generation, not in old_generation\n+    return (active_generation() != (ShenandoahGeneration*) old_generation());\n+  case ShenandoahAffiliation::OLD_GENERATION:\n+    \/\/ Old regions are in old_generation and global_generation, not in young_generation\n+    return (active_generation() != (ShenandoahGeneration*) young_generation());\n+  default:\n+    assert(false, \"Bad affiliation (%d) for region \" SIZE_FORMAT, _affiliations[index], index);\n+    return false;\n+  }\n+}\n+\n+inline bool ShenandoahHeap::is_in_young(const void* p) const {\n+  return is_in(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::YOUNG_GENERATION);\n+}\n+\n+inline bool ShenandoahHeap::is_in_old(const void* p) const {\n+  return is_in(p) && (_affiliations[heap_region_index_containing(p)] == ShenandoahAffiliation::OLD_GENERATION);\n+}\n+\n+inline bool ShenandoahHeap::is_old(oop obj) const {\n+  return is_gc_generation_young() && is_in_old(obj);\n+}\n+\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(const ShenandoahHeapRegion *r) {\n+  return (ShenandoahAffiliation) _affiliations[r->index()];\n+}\n+\n+inline void ShenandoahHeap::assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                                        ShenandoahAffiliation new_affiliation) {\n+  \/\/ A lock is required when changing from FREE to NON-FREE.  Though it may be possible to elide the lock when\n+  \/\/ transitioning from in-use to FREE, the current implementation uses a lock for this transition.  A lock is\n+  \/\/ not required to change from YOUNG to OLD (i.e. when promoting humongous region).\n+  \/\/\n+  \/\/         new_affiliation is:     FREE   YOUNG   OLD\n+  \/\/  orig_affiliation is:  FREE      X       L      L\n+  \/\/                       YOUNG      L       X\n+  \/\/                         OLD      L       X      X\n+  \/\/  X means state transition won't happen (so don't care)\n+  \/\/  L means lock should be held\n+  \/\/  Blank means no lock required because affiliation visibility will not be required until subsequent safepoint\n+  \/\/\n+  \/\/ Note: during full GC, all transitions between states are possible.  During Full GC, we should be in a safepoint.\n+\n+  if ((orig_affiliation == ShenandoahAffiliation::FREE) || (new_affiliation == ShenandoahAffiliation::FREE)) {\n+    shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+  }\n+}\n+\n+inline void ShenandoahHeap::set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation) {\n+#ifdef ASSERT\n+  assert_lock_for_affiliation(region_affiliation(r), new_affiliation);\n+#endif\n+  _affiliations[r->index()] = (uint8_t) new_affiliation;\n+}\n+\n+inline ShenandoahAffiliation ShenandoahHeap::region_affiliation(size_t index) {\n+  return (ShenandoahAffiliation) _affiliations[index];\n+}\n+\n+inline void ShenandoahHeap::set_affiliation(size_t index, ShenandoahAffiliation new_affiliation) {\n+#ifdef ASSERT\n+  assert_lock_for_affiliation(region_affiliation(index), new_affiliation);\n+#endif\n+  _affiliations[index] = (uint8_t) new_affiliation;\n+}\n+\n@@ -370,0 +652,1 @@\n+\n@@ -374,0 +657,4 @@\n+inline bool ShenandoahHeap::has_evacuation_reserve_quantities() const {\n+  return _has_evacuation_reserve_quantities;\n+}\n+\n@@ -375,1 +662,1 @@\n-  return _gc_state.is_unset(MARKING | EVACUATION | UPDATEREFS);\n+  return _gc_state.is_unset(YOUNG_MARKING | OLD_MARKING | EVACUATION | UPDATEREFS);\n@@ -379,1 +666,9 @@\n-  return _gc_state.is_set(MARKING);\n+  return _gc_state.is_set(YOUNG_MARKING | OLD_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_young_mark_in_progress() const {\n+  return _gc_state.is_set(YOUNG_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_old_mark_in_progress() const {\n+  return _gc_state.is_set(OLD_MARKING);\n@@ -418,0 +713,90 @@\n+inline bool ShenandoahHeap::is_aging_cycle() const {\n+  return _is_aging_cycle.is_set();\n+}\n+\n+inline bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return _prepare_for_old_mark;\n+}\n+\n+inline size_t ShenandoahHeap::set_promoted_reserve(size_t new_val) {\n+  size_t orig = _promoted_reserve;\n+  _promoted_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_promoted_reserve() const {\n+  return _promoted_reserve;\n+}\n+\n+\/\/ returns previous value\n+size_t ShenandoahHeap::capture_old_usage(size_t old_usage) {\n+  size_t previous_value = _captured_old_usage;\n+  _captured_old_usage = old_usage;\n+  return previous_value;\n+}\n+\n+void ShenandoahHeap::set_previous_promotion(size_t promoted_bytes) {\n+  shenandoah_assert_heaplocked();\n+  _previous_promotion = promoted_bytes;\n+}\n+\n+size_t ShenandoahHeap::get_previous_promotion() const {\n+  return _previous_promotion;\n+}\n+\n+inline size_t ShenandoahHeap::set_old_evac_reserve(size_t new_val) {\n+  size_t orig = _old_evac_reserve;\n+  _old_evac_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_old_evac_reserve() const {\n+  return _old_evac_reserve;\n+}\n+\n+inline void ShenandoahHeap::augment_old_evac_reserve(size_t increment) {\n+  _old_evac_reserve += increment;\n+}\n+\n+inline void ShenandoahHeap::augment_promo_reserve(size_t increment) {\n+  _promoted_reserve += increment;\n+}\n+\n+inline void ShenandoahHeap::reset_old_evac_expended() {\n+  Atomic::store(&_old_evac_expended, (size_t) 0);\n+}\n+\n+inline size_t ShenandoahHeap::expend_old_evac(size_t increment) {\n+  return Atomic::add(&_old_evac_expended, increment);\n+}\n+\n+inline size_t ShenandoahHeap::get_old_evac_expended() {\n+  return Atomic::load(&_old_evac_expended);\n+}\n+\n+inline void ShenandoahHeap::reset_promoted_expended() {\n+  Atomic::store(&_promoted_expended, (size_t) 0);\n+}\n+\n+inline size_t ShenandoahHeap::expend_promoted(size_t increment) {\n+  return Atomic::add(&_promoted_expended, increment);\n+}\n+\n+inline size_t ShenandoahHeap::unexpend_promoted(size_t decrement) {\n+  return Atomic::sub(&_promoted_expended, decrement);\n+}\n+\n+inline size_t ShenandoahHeap::get_promoted_expended() {\n+  return Atomic::load(&_promoted_expended);\n+}\n+\n+inline size_t ShenandoahHeap::set_young_evac_reserve(size_t new_val) {\n+  size_t orig = _young_evac_reserve;\n+  _young_evac_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_young_evac_reserve() const {\n+  return _young_evac_reserve;\n+}\n+\n@@ -427,2 +812,1 @@\n-  ShenandoahMarkingContext* const ctx = complete_marking_context();\n-  assert(ctx->is_complete(), \"sanity\");\n+  ShenandoahMarkingContext* const ctx = marking_context();\n@@ -559,8 +943,0 @@\n-inline void ShenandoahHeap::mark_complete_marking_context() {\n-  _marking_context->mark_complete();\n-}\n-\n-inline void ShenandoahHeap::mark_incomplete_marking_context() {\n-  _marking_context->mark_incomplete();\n-}\n-\n@@ -576,0 +952,24 @@\n+inline void ShenandoahHeap::clear_cards_for(ShenandoahHeapRegion* region) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_range_as_empty(region->bottom(), pointer_delta(region->end(), region->bottom()));\n+  }\n+}\n+\n+inline void ShenandoahHeap::dirty_cards(HeapWord* start, HeapWord* end) {\n+  assert(mode()->is_generational(), \"Should only be used for generational mode\");\n+  size_t words = pointer_delta(end, start);\n+  _card_scan->mark_range_as_dirty(start, words);\n+}\n+\n+inline void ShenandoahHeap::clear_cards(HeapWord* start, HeapWord* end) {\n+  assert(mode()->is_generational(), \"Should only be used for generational mode\");\n+  size_t words = pointer_delta(end, start);\n+  _card_scan->mark_range_as_clean(start, words);\n+}\n+\n+inline void ShenandoahHeap::mark_card_as_dirty(void* location) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_card_as_dirty((HeapWord*)location);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":432,"deletions":32,"binary":false,"changes":464,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -30,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahAffiliation.hpp\"\n@@ -171,1 +173,2 @@\n-  void make_regular_allocation();\n+  void make_regular_allocation(ShenandoahAffiliation affiliation);\n+  void make_young_maybe();\n@@ -175,2 +178,2 @@\n-  void make_humongous_start_bypass();\n-  void make_humongous_cont_bypass();\n+  void make_humongous_start_bypass(ShenandoahAffiliation affiliation);\n+  void make_humongous_cont_bypass(ShenandoahAffiliation affiliation);\n@@ -201,0 +204,3 @@\n+  inline bool is_young() const;\n+  inline bool is_old() const;\n+  inline bool is_affiliated() const;\n@@ -213,0 +219,4 @@\n+  void clear_young_lab_flags();\n+  void set_young_lab_flag();\n+  bool has_young_lab_flag();\n+\n@@ -235,0 +245,2 @@\n+  HeapWord* _top_before_promoted;\n+\n@@ -237,0 +249,1 @@\n+  HeapWord* _coalesce_and_fill_boundary; \/\/ for old regions not selected as collection set candidates.\n@@ -243,0 +256,3 @@\n+  size_t _plab_allocs;\n+\n+  bool _has_young_lab;\n@@ -249,0 +265,2 @@\n+  uint _age;\n+\n@@ -337,2 +355,10 @@\n-  \/\/ Allocation (return null if full)\n-  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest::Type type);\n+  inline void save_top_before_promote();\n+  inline HeapWord* get_top_before_promote() const { return _top_before_promoted; }\n+  inline void restore_top_before_promote();\n+  inline size_t garbage_before_padded_for_promote() const;\n+\n+  \/\/ Allocation (return nullptr if full)\n+  inline HeapWord* allocate_aligned(size_t word_size, ShenandoahAllocRequest &req, size_t alignment_in_words);\n+\n+  \/\/ Allocation (return nullptr if full)\n+  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest req);\n@@ -359,1 +385,35 @@\n-  void oop_iterate(OopIterateClosure* cl);\n+  inline void begin_preemptible_coalesce_and_fill() {\n+    _coalesce_and_fill_boundary = _bottom;\n+  }\n+\n+  inline void end_preemptible_coalesce_and_fill() {\n+    _coalesce_and_fill_boundary = _end;\n+  }\n+\n+  inline void suspend_coalesce_and_fill(HeapWord* next_focus) {\n+    _coalesce_and_fill_boundary = next_focus;\n+  }\n+\n+  inline HeapWord* resume_coalesce_and_fill() {\n+    return _coalesce_and_fill_boundary;\n+  }\n+\n+  \/\/ Coalesce contiguous spans of garbage objects by filling header and reregistering start locations with remembered set.\n+  \/\/ This is used by old-gen GC following concurrent marking to make old-gen HeapRegions parseable.  Return true iff\n+  \/\/ region is completely coalesced and filled.  Returns false if cancelled before task is complete.\n+  bool oop_fill_and_coalesce();\n+\n+  \/\/ Like oop_fill_and_coalesce(), but without honoring cancellation requests.\n+  bool oop_fill_and_coalesce_without_cancel();\n+\n+  \/\/ During global collections, this service iterates through an old-gen heap region that is not part of collection\n+  \/\/ set to fill and register ranges of dead memory.  Note that live objects were previously registered.  Some dead objects\n+  \/\/ that are subsumed into coalesced ranges of dead memory need to be \"unregistered\".\n+  void global_oop_iterate_and_fill_dead(OopIterateClosure* cl);\n+  void oop_iterate_humongous(OopIterateClosure* cl);\n+  void oop_iterate_humongous(OopIterateClosure* cl, HeapWord* start, size_t words);\n+\n+  \/\/ Invoke closure on every reference contained within the humongous object that spans this humongous\n+  \/\/ region if the reference is contained within a DIRTY card and the reference is no more than words following\n+  \/\/ start within the humongous object.\n+  void oop_iterate_humongous_slice(OopIterateClosure* cl, bool dirty_only, HeapWord* start, size_t words, bool write_table);\n@@ -379,0 +439,1 @@\n+  size_t used_before_promote() const { return byte_size(bottom(), get_top_before_promote()); }\n@@ -381,0 +442,5 @@\n+  \/\/ Does this region contain this address?\n+  bool contains(HeapWord* p) const {\n+    return (bottom() <= p) && (p < top());\n+  }\n+\n@@ -386,0 +452,1 @@\n+  size_t get_plab_allocs() const;\n@@ -391,0 +458,14 @@\n+  inline ShenandoahAffiliation affiliation() const;\n+  inline const char* affiliation_name() const;\n+\n+  void set_affiliation(ShenandoahAffiliation new_affiliation);\n+\n+  uint age()           { return _age; }\n+  void increment_age() { _age++; }\n+  void decrement_age() { if (_age-- == 0) { _age = 0; } }\n+  void reset_age()     { _age = 0; }\n+\n+  \/\/ Register all objects.  Set all remembered set cards to dirty.\n+  void promote_humongous();\n+  void promote_in_place();\n+\n@@ -392,0 +473,1 @@\n+  void decrement_humongous_waste() const;\n@@ -395,2 +477,3 @@\n-  void oop_iterate_objects(OopIterateClosure* cl);\n-  void oop_iterate_humongous(OopIterateClosure* cl);\n+  \/\/ This is an old-region that was not part of the collection set during a GLOBAL collection.  We coalesce the dead\n+  \/\/ objects, but do not need to register the live objects as they are already registered.\n+  void global_oop_iterate_objects_and_fill_dead(OopIterateClosure* cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":91,"deletions":8,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -0,0 +1,1083 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+\n+\/\/ Terminology used within this source file:\n+\/\/\n+\/\/ Card Entry:   This is the information that identifies whether a\n+\/\/               particular card-table entry is Clean or Dirty.  A clean\n+\/\/               card entry denotes that the associated memory does not\n+\/\/               hold references to young-gen memory.\n+\/\/\n+\/\/ Card Region, aka\n+\/\/ Card Memory:  This is the region of memory that is assocated with a\n+\/\/               particular card entry.\n+\/\/\n+\/\/ Card Cluster: A card cluster represents 64 card entries.  A card\n+\/\/               cluster is the minimal amount of work performed at a\n+\/\/               time by a parallel thread.  Note that the work required\n+\/\/               to scan a card cluster is somewhat variable in that the\n+\/\/               required effort depends on how many cards are dirty, how\n+\/\/               many references are held within the objects that span a\n+\/\/               DIRTY card's memory, and on the size of the object\n+\/\/               that spans the end of a DIRTY card's memory (because\n+\/\/               that object, if it's not an array, may need to be scanned in\n+\/\/               its entirety, when the object is imprecisely dirtied. Imprecise\n+\/\/               dirtying is when the card corresponding to the object header\n+\/\/               is dirtied, rather than the card on which the updated field lives).\n+\/\/               To better balance work amongst them, parallel worker threads dynamically\n+\/\/               claim clusters and are flexible in the number of clusters they\n+\/\/               process.\n+\/\/\n+\/\/ A cluster represents a \"natural\" quantum of work to be performed by\n+\/\/ a parallel GC thread's background remembered set scanning efforts.\n+\/\/ The notion of cluster is similar to the notion of stripe in the\n+\/\/ implementation of parallel GC card scanning.  However, a cluster is\n+\/\/ typically smaller than a stripe, enabling finer grain division of\n+\/\/ labor between multiple threads, and potentially better load balancing\n+\/\/ when dirty cards are not uniformly distributed in the heap, as is often\n+\/\/ the case with generational workloads where more recently promoted objects\n+\/\/ may be dirtied more frequently that older objects.\n+\/\/\n+\/\/ For illustration, consider the following possible JVM configurations:\n+\/\/\n+\/\/   Scenario 1:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of a card entry is 512 B\n+\/\/     Each card table entry consumes 1 B\n+\/\/     Assume one long word (8 B)of the card table represents a cluster.\n+\/\/       This long word holds 8 card table entries, spanning a\n+\/\/       total of 8*512 B = 4 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 4 KB = 32 K\n+\/\/\n+\/\/   Scenario 2:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of each card entry is 128 B\n+\/\/     Each card table entry consumes 1 bit\n+\/\/     Assume one int word (4 B) of the card table represents a cluster.\n+\/\/       This int word holds 32 b\/1 b = 32 card table entries, spanning a\n+\/\/       total of 32 * 128 B = 4 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 4 KB = 32 K\n+\/\/\n+\/\/   Scenario 3:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of each card entry is 512 B\n+\/\/     Each card table entry consumes 1 bit\n+\/\/     Assume one long word (8 B) of card table represents a cluster.\n+\/\/       This long word holds 64 b\/ 1 b = 64 card table entries, spanning a\n+\/\/       total of 64 * 512 B = 32 KB of the heap\n+\/\/     The number of clusters per region is 128 MB \/ 32 KB = 4 K\n+\/\/\n+\/\/ At the start of a new young-gen concurrent mark pass, the gang of\n+\/\/ Shenandoah worker threads collaborate in performing the following\n+\/\/ actions:\n+\/\/\n+\/\/  Let old_regions = number of ShenandoahHeapRegion comprising\n+\/\/    old-gen memory\n+\/\/  Let region_size = ShenandoahHeapRegion::region_size_bytes()\n+\/\/    represent the number of bytes in each region\n+\/\/  Let clusters_per_region = region_size \/ 512\n+\/\/  Let rs represent the relevant RememberedSet implementation\n+\/\/    (an instance of ShenandoahDirectCardMarkRememberedSet or an instance\n+\/\/     of a to-be-implemented ShenandoahBufferWithSATBRememberedSet)\n+\/\/\n+\/\/  for each ShenandoahHeapRegion old_region in the whole heap\n+\/\/    determine the cluster number of the first cluster belonging\n+\/\/      to that region\n+\/\/    for each cluster contained within that region\n+\/\/      Assure that exactly one worker thread processes each\n+\/\/      cluster, each thread making a series of invocations of the\n+\/\/      following:\n+\/\/\n+\/\/        rs->process_clusters(worker_id, ReferenceProcessor *,\n+\/\/                             ShenandoahConcurrentMark *, cluster_no, cluster_count,\n+\/\/                             HeapWord *end_of_range, OopClosure *oops);\n+\/\/\n+\/\/  For efficiency, divide up the clusters so that different threads\n+\/\/  are responsible for processing different clusters.  Processing costs\n+\/\/  may vary greatly between clusters for the following reasons:\n+\/\/\n+\/\/        a) some clusters contain mostly dirty cards and other\n+\/\/           clusters contain mostly clean cards\n+\/\/        b) some clusters contain mostly primitive data and other\n+\/\/           clusters contain mostly reference data\n+\/\/        c) some clusters are spanned by very large non-array objects that\n+\/\/           begin in some other cluster.  When a large non-array object\n+\/\/           beginning in a preceding cluster spans large portions of\n+\/\/           this cluster, then because of imprecise dirtying, the\n+\/\/           portion of the object in this cluster may be clean, but\n+\/\/           will need to be processed by the worker responsible for\n+\/\/           this cluster, potentially increasing its work.\n+\/\/        d) in the case that the end of this cluster is spanned by a\n+\/\/           very large non-array object, the worker for this cluster will\n+\/\/           be responsible for processing the portion of the object\n+\/\/           in this cluster.\n+\/\/\n+\/\/ Though an initial division of labor between marking threads may\n+\/\/ assign equal numbers of clusters to be scanned by each thread, it\n+\/\/ should be expected that some threads will finish their assigned\n+\/\/ work before others.  Therefore, some amount of the full remembered\n+\/\/ set scanning effort should be held back and assigned incrementally\n+\/\/ to the threads that end up with excess capacity.  Consider the\n+\/\/ following strategy for dividing labor:\n+\/\/\n+\/\/        1. Assume there are 8 marking threads and 1024 remembered\n+\/\/           set clusters to be scanned.\n+\/\/        2. Assign each thread to scan 64 clusters.  This leaves\n+\/\/           512 (1024 - (8*64)) clusters to still be scanned.\n+\/\/        3. As the 8 server threads complete previous cluster\n+\/\/           scanning assignments, issue each of the next 8 scanning\n+\/\/           assignments as units of 32 additional cluster each.\n+\/\/           In the case that there is high variance in effort\n+\/\/           associated with previous cluster scanning assignments,\n+\/\/           multiples of these next assignments may be serviced by\n+\/\/           the server threads that were previously assigned lighter\n+\/\/           workloads.\n+\/\/        4. Make subsequent scanning assignments as follows:\n+\/\/             a) 8 assignments of size 16 clusters\n+\/\/             b) 8 assignments of size 8 clusters\n+\/\/             c) 16 assignments of size 4 clusters\n+\/\/\n+\/\/    When there is no more remembered set processing work to be\n+\/\/    assigned to a newly idled worker thread, that thread can move\n+\/\/    on to work on other tasks associated with root scanning until such\n+\/\/    time as all clusters have been examined.\n+\/\/\n+\/\/ Remembered set scanning is designed to run concurrently with\n+\/\/ mutator threads, with multiple concurrent workers. Furthermore, the\n+\/\/ current implementation of remembered set scanning never clears a\n+\/\/ card once it has been marked.\n+\/\/\n+\/\/ These limitations will be addressed in future enhancements to the\n+\/\/ existing implementation.\n+\n+#include <stdint.h>\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardStats.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahNumberSeq.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTaskqueue.hpp\"\n+#include \"memory\/iterator.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class ShenandoahReferenceProcessor;\n+class ShenandoahConcurrentMark;\n+class ShenandoahHeap;\n+class ShenandoahRegionIterator;\n+class ShenandoahMarkingContext;\n+\n+class CardTable;\n+typedef CardTable::CardValue CardValue;\n+\n+class ShenandoahDirectCardMarkRememberedSet: public CHeapObj<mtGC> {\n+\n+private:\n+\n+  \/\/ Use symbolic constants defined in cardTable.hpp\n+  \/\/  CardTable::card_shift = 9;\n+  \/\/  CardTable::card_size = 512;\n+  \/\/  CardTable::card_size_in_words = 64;\n+  \/\/  CardTable::clean_card_val()\n+  \/\/  CardTable::dirty_card_val()\n+\n+  ShenandoahHeap *_heap;\n+  ShenandoahCardTable *_card_table;\n+  size_t _card_shift;\n+  size_t _total_card_count;\n+  size_t _cluster_count;\n+  HeapWord *_whole_heap_base;   \/\/ Points to first HeapWord of data contained within heap memory\n+  CardValue* _byte_map;         \/\/ Points to first entry within the card table\n+  CardValue* _byte_map_base;    \/\/ Points to byte_map minus the bias computed from address of heap memory\n+\n+public:\n+\n+  \/\/ count is the number of cards represented by the card table.\n+  ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable *card_table, size_t total_card_count);\n+  ~ShenandoahDirectCardMarkRememberedSet();\n+\n+  \/\/ Card index is zero-based relative to _byte_map.\n+  size_t last_valid_index() const;\n+  size_t total_cards() const;\n+  size_t card_index_for_addr(HeapWord *p) const;\n+  HeapWord *addr_for_card_index(size_t card_index) const;\n+  inline const CardValue* get_card_table_byte_map(bool write_table) const;\n+  inline bool is_card_dirty(size_t card_index) const;\n+  inline bool is_write_card_dirty(size_t card_index) const;\n+  inline void mark_card_as_dirty(size_t card_index);\n+  inline void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  inline void mark_card_as_clean(size_t card_index);\n+  inline void mark_read_card_as_clean(size_t card_index);\n+  inline void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  inline bool is_card_dirty(HeapWord *p) const;\n+  inline void mark_card_as_dirty(HeapWord *p);\n+  inline void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n+  inline void mark_card_as_clean(HeapWord *p);\n+  inline void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n+  inline size_t cluster_count() const;\n+\n+  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n+  \/\/ Not currently used because mutator write barrier does not honor changes to the location of card table.\n+  void swap_remset() {  _card_table->swap_card_tables(); }\n+\n+  void merge_write_table(HeapWord* start, size_t word_count) {\n+    size_t card_index = card_index_for_addr(start);\n+    size_t num_cards = word_count \/ CardTable::card_size_in_words();\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardValue));\n+    intptr_t* read_table_ptr = (intptr_t*) &(_card_table->read_byte_map())[card_index];\n+    intptr_t* write_table_ptr = (intptr_t*) &(_card_table->write_byte_map())[card_index];\n+    for (size_t i = 0; i < iterations; i++) {\n+      intptr_t card_value = *write_table_ptr;\n+      *read_table_ptr++ &= card_value;\n+      write_table_ptr++;\n+    }\n+  }\n+\n+  \/\/ Instead of swap_remset, the current implementation of concurrent remembered set scanning does reset_remset\n+  \/\/ in parallel threads, each invocation processing one entire HeapRegion at a time.  Processing of a region\n+  \/\/ consists of copying the write table to the read table and cleaning the write table.\n+  void reset_remset(HeapWord* start, size_t word_count) {\n+    size_t card_index = card_index_for_addr(start);\n+    size_t num_cards = word_count \/ CardTable::card_size_in_words();\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardValue));\n+    intptr_t* read_table_ptr = (intptr_t*) &(_card_table->read_byte_map())[card_index];\n+    intptr_t* write_table_ptr = (intptr_t*) &(_card_table->write_byte_map())[card_index];\n+    for (size_t i = 0; i < iterations; i++) {\n+      *read_table_ptr++ = *write_table_ptr;\n+      *write_table_ptr++ = CardTable::clean_card_row_val();\n+    }\n+  }\n+\n+  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n+  void clear_old_remset() {  _card_table->clear_read_table(); }\n+\n+};\n+\n+\/\/ A ShenandoahCardCluster represents the minimal unit of work\n+\/\/ performed by independent parallel GC threads during scanning of\n+\/\/ remembered sets.\n+\/\/\n+\/\/ The GC threads that perform card-table remembered set scanning may\n+\/\/ overwrite card-table entries to mark them as clean in the case that\n+\/\/ the associated memory no longer holds references to young-gen\n+\/\/ memory.  Rather than access the card-table entries directly, all GC\n+\/\/ thread access to card-table information is made by way of the\n+\/\/ ShenandoahCardCluster data abstraction.  This abstraction\n+\/\/ effectively manages access to multiple possible underlying\n+\/\/ remembered set implementations, including a traditional card-table\n+\/\/ approach and a SATB-based approach.\n+\/\/\n+\/\/ The API services represent a compromise between efficiency and\n+\/\/ convenience.\n+\/\/\n+\/\/ Multiple GC threads that scan the remembered set\n+\/\/ in parallel.  The desire is to divide the complete scanning effort\n+\/\/ into multiple clusters of work that can be independently processed\n+\/\/ by individual threads without need for synchronizing efforts\n+\/\/ between the work performed by each task.  The term \"cluster\" of\n+\/\/ work is similar to the term \"stripe\" as used in the implementation\n+\/\/ of Parallel GC.\n+\/\/\n+\/\/ Complexity arises when an object to be scanned crosses the boundary\n+\/\/ between adjacent cluster regions.  Here is the protocol that we currently\n+\/\/ follow:\n+\/\/\n+\/\/  1. The thread responsible for scanning the cards in a cluster modifies\n+\/\/     the associated card-table entries. Only cards that are dirty are\n+\/\/     processed, except as described below for the case of objects that\n+\/\/     straddle more than one card.\n+\/\/  2. Object Arrays are precisely dirtied, so only the portion of the obj-array\n+\/\/     that overlaps the range of dirty cards in its cluster are scanned\n+\/\/     by each worker thread. This holds for portions of obj-arrays that extend\n+\/\/     over clusters processed by different workers, with each worked responsible\n+\/\/     for scanning the portion of the obj-array overlapping the dirty cards in\n+\/\/     its cluster.\n+\/\/  3. Non-array objects are precisely dirtied by the interpreter and the compilers\n+\/\/     For such objects that extend over multiple cards, or even multiple clusters,\n+\/\/     the entire object is scanned by the worker that processes the (dirty) card on\n+\/\/     which the object's header lies. (However, GC workers should precisely dirty the\n+\/\/     cards with inter-regional\/inter-generational pointers in the body of this object,\n+\/\/     thus making subsequent scans potentially less expensive.) Such larger non-array\n+\/\/     objects are relatively rare.\n+\/\/\n+\/\/  A possible criticism:\n+\/\/  C. The representation of pointer location descriptive information\n+\/\/     within Klass representations is not designed for efficient\n+\/\/     \"random access\".  An alternative approach to this design would\n+\/\/     be to scan very large objects multiple times, once for each\n+\/\/     cluster that is spanned by the object's range.  This reduces\n+\/\/     unnecessary overscan, but it introduces different sorts of\n+\/\/     overhead effort:\n+\/\/       i) For each spanned cluster, we have to look up the start of\n+\/\/          the crossing object.\n+\/\/      ii) Each time we scan the very large object, we have to\n+\/\/          sequentially walk through its pointer location\n+\/\/          descriptors, skipping over all of the pointers that\n+\/\/          precede the start of the range of addresses that we\n+\/\/          consider relevant.\n+\n+\n+\/\/ Because old-gen heap memory is not necessarily contiguous, and\n+\/\/ because cards are not necessarily maintained for young-gen memory,\n+\/\/ consecutive card numbers do not necessarily correspond to consecutive\n+\/\/ address ranges.  For the traditional direct-card-marking\n+\/\/ implementation of this interface, consecutive card numbers are\n+\/\/ likely to correspond to contiguous regions of memory, but this\n+\/\/ should not be assumed.  Instead, rely only upon the following:\n+\/\/\n+\/\/  1. All card numbers for cards pertaining to the same\n+\/\/     ShenandoahHeapRegion are consecutively numbered.\n+\/\/  2. In the case that neighboring ShenandoahHeapRegions both\n+\/\/     represent old-gen memory, the card regions that span the\n+\/\/     boundary between these neighboring heap regions will be\n+\/\/     consecutively numbered.\n+\/\/  3. (A corollary) In the case that an old-gen object straddles the\n+\/\/     boundary between two heap regions, the card regions that\n+\/\/     correspond to the span of this object will be consecutively\n+\/\/     numbered.\n+\/\/\n+\/\/ ShenandoahCardCluster abstracts access to the remembered set\n+\/\/ and also keeps track of crossing map information to allow efficient\n+\/\/ resolution of object start addresses.\n+\/\/\n+\/\/ ShenandoahCardCluster supports all of the services of\n+\/\/ RememberedSet, plus it supports register_object() and lookup_object().\n+\/\/ Note that we only need to register the start addresses of the object that\n+\/\/ overlays the first address of a card; we need to do this for every card.\n+\/\/ In other words, register_object() checks if the object crosses a card boundary,\n+\/\/ and updates the offset value for each card that the object crosses into.\n+\/\/ For objects that don't straddle cards, nothing needs to be done.\n+\/\/\n+\/\/ The RememberedSet template parameter is intended to represent either\n+\/\/     ShenandoahDirectCardMarkRememberedSet, or a to-be-implemented\n+\/\/     ShenandoahBufferWithSATBRememberedSet.\n+template<typename RememberedSet>\n+class ShenandoahCardCluster: public CHeapObj<mtGC> {\n+\n+private:\n+  RememberedSet *_rs;\n+\n+public:\n+  static const size_t CardsPerCluster = 64;\n+\n+private:\n+  typedef struct cross_map { uint8_t first; uint8_t last; } xmap;\n+  typedef union crossing_info { uint16_t short_word; xmap offsets; } crossing_info;\n+\n+  \/\/ ObjectStartsInCardRegion bit is set within a crossing_info.offsets.start iff at least one object starts within\n+  \/\/ a particular card region.  We pack this bit into start byte under assumption that start byte is accessed less\n+  \/\/ frequently than last byte.  This is true when number of clean cards is greater than number of dirty cards.\n+  static const uint16_t ObjectStartsInCardRegion = 0x80;\n+  static const uint16_t FirstStartBits           = 0x7f;\n+\n+  \/\/ Check that we have enough bits to store the largest possible offset into a card for an object start.\n+  \/\/ The value for maximum card size is based on the constraints for GCCardSizeInBytes in gc_globals.hpp.\n+  static const int MaxCardSize = NOT_LP64(512) LP64_ONLY(1024);\n+  STATIC_ASSERT((MaxCardSize \/ HeapWordSize) - 1 <= FirstStartBits);\n+\n+  crossing_info *object_starts;\n+\n+public:\n+  \/\/ If we're setting first_start, assume the card has an object.\n+  inline void set_first_start(size_t card_index, uint8_t value) {\n+    object_starts[card_index].offsets.first = ObjectStartsInCardRegion | value;\n+  }\n+\n+  inline void set_last_start(size_t card_index, uint8_t value) {\n+    object_starts[card_index].offsets.last = value;\n+  }\n+\n+  inline void set_starts_object_bit(size_t card_index) {\n+    object_starts[card_index].offsets.first |= ObjectStartsInCardRegion;\n+  }\n+\n+  inline void clear_starts_object_bit(size_t card_index) {\n+    object_starts[card_index].offsets.first &= ~ObjectStartsInCardRegion;\n+  }\n+\n+  \/\/ Returns true iff an object is known to start within the card memory associated with card card_index.\n+  inline bool starts_object(size_t card_index) const {\n+    return (object_starts[card_index].offsets.first & ObjectStartsInCardRegion) != 0;\n+  }\n+\n+  inline void clear_objects_in_range(HeapWord *addr, size_t num_words) {\n+    size_t card_index = _rs->card_index_for_addr(addr);\n+    size_t last_card_index = _rs->card_index_for_addr(addr + num_words - 1);\n+    while (card_index <= last_card_index)\n+      object_starts[card_index++].short_word = 0;\n+  }\n+\n+  ShenandoahCardCluster(RememberedSet *rs) {\n+    _rs = rs;\n+    \/\/ TODO: We don't really need object_starts entries for every card entry.  We only need these for\n+    \/\/ the card entries that correspond to old-gen memory.  But for now, let's be quick and dirty.\n+    object_starts = NEW_C_HEAP_ARRAY(crossing_info, rs->total_cards(), mtGC);\n+    for (size_t i = 0; i < rs->total_cards(); i++) {\n+      object_starts[i].short_word = 0;\n+    }\n+  }\n+\n+  ~ShenandoahCardCluster() {\n+    FREE_C_HEAP_ARRAY(crossing_info, object_starts);\n+    object_starts = nullptr;\n+  }\n+\n+  \/\/ There is one entry within the object_starts array for each card entry.\n+  \/\/\n+  \/\/  Suppose multiple garbage objects are coalesced during GC sweep\n+  \/\/  into a single larger \"free segment\".  As each two objects are\n+  \/\/  coalesced together, the start information pertaining to the second\n+  \/\/  object must be removed from the objects_starts array.  If the\n+  \/\/  second object had been been the first object within card memory,\n+  \/\/  the new first object is the object that follows that object if\n+  \/\/  that starts within the same card memory, or NoObject if the\n+  \/\/  following object starts within the following cluster.  If the\n+  \/\/  second object had been the last object in the card memory,\n+  \/\/  replace this entry with the newly coalesced object if it starts\n+  \/\/  within the same card memory, or with NoObject if it starts in a\n+  \/\/  preceding card's memory.\n+  \/\/\n+  \/\/  Suppose a large free segment is divided into a smaller free\n+  \/\/  segment and a new object.  The second part of the newly divided\n+  \/\/  memory must be registered as a new object, overwriting at most\n+  \/\/  one first_start and one last_start entry.  Note that one of the\n+  \/\/  newly divided two objects might be a new GCLAB.\n+  \/\/\n+  \/\/  Suppose postprocessing of a GCLAB finds that the original GCLAB\n+  \/\/  has been divided into N objects.  Each of the N newly allocated\n+  \/\/  objects will be registered, overwriting at most one first_start\n+  \/\/  and one last_start entries.\n+  \/\/\n+  \/\/  No object registration operations are linear in the length of\n+  \/\/  the registered objects.\n+  \/\/\n+  \/\/ Consider further the following observations regarding object\n+  \/\/ registration costs:\n+  \/\/\n+  \/\/   1. The cost is paid once for each old-gen object (Except when\n+  \/\/      an object is demoted and repromoted, in which case we would\n+  \/\/      pay the cost again).\n+  \/\/   2. The cost can be deferred so that there is no urgency during\n+  \/\/      mutator copy-on-first-access promotion.  Background GC\n+  \/\/      threads will update the object_starts array by post-\n+  \/\/      processing the contents of retired PLAB buffers.\n+  \/\/   3. The bet is that these costs are paid relatively rarely\n+  \/\/      because:\n+  \/\/      a) Most objects die young and objects that die in young-gen\n+  \/\/         memory never need to be registered with the object_starts\n+  \/\/         array.\n+  \/\/      b) Most objects that are promoted into old-gen memory live\n+  \/\/         there without further relocation for a relatively long\n+  \/\/         time, so we get a lot of benefit from each investment\n+  \/\/         in registering an object.\n+\n+public:\n+\n+  \/\/ The starting locations of objects contained within old-gen memory\n+  \/\/ are registered as part of the remembered set implementation.  This\n+  \/\/ information is required when scanning dirty card regions that are\n+  \/\/ spanned by objects beginning within preceding card regions.  It\n+  \/\/ is necessary to find the first and last objects that begin within\n+  \/\/ this card region.  Starting addresses of objects are required to\n+  \/\/ find the object headers, and object headers provide information\n+  \/\/ about which fields within the object hold addresses.\n+  \/\/\n+  \/\/ The old-gen memory allocator invokes register_object() for any\n+  \/\/ object that is allocated within old-gen memory.  This identifies\n+  \/\/ the starting addresses of objects that span boundaries between\n+  \/\/ card regions.\n+  \/\/\n+  \/\/ It is not necessary to invoke register_object at the very instant\n+  \/\/ an object is allocated.  It is only necessary to invoke it\n+  \/\/ prior to the next start of a garbage collection concurrent mark\n+  \/\/ or concurrent update-references phase.  An \"ideal\" time to register\n+  \/\/ objects is during post-processing of a GCLAB after the GCLAB is\n+  \/\/ retired due to depletion of its memory.\n+  \/\/\n+  \/\/ register_object() does not perform synchronization.  In the case\n+  \/\/ that multiple threads are registering objects whose starting\n+  \/\/ addresses are within the same cluster, races between these\n+  \/\/ threads may result in corruption of the object-start data\n+  \/\/ structures.  Parallel GC threads should avoid registering objects\n+  \/\/ residing within the same cluster by adhering to the following\n+  \/\/ coordination protocols:\n+  \/\/\n+  \/\/  1. Align thread-local GCLAB buffers with some TBD multiple of\n+  \/\/     card clusters.  The card cluster size is 32 KB.  If the\n+  \/\/     desired GCLAB size is 128 KB, align the buffer on a multiple\n+  \/\/     of 4 card clusters.\n+  \/\/  2. Post-process the contents of GCLAB buffers to register the\n+  \/\/     objects allocated therein.  Allow one GC thread at a\n+  \/\/     time to do the post-processing of each GCLAB.\n+  \/\/  3. Since only one GC thread at a time is registering objects\n+  \/\/     belonging to a particular allocation buffer, no locking\n+  \/\/     is performed when registering these objects.\n+  \/\/  4. Any remnant of unallocated memory within an expended GC\n+  \/\/     allocation buffer is not returned to the old-gen allocation\n+  \/\/     pool until after the GC allocation buffer has been post\n+  \/\/     processed.  Before any remnant memory is returned to the\n+  \/\/     old-gen allocation pool, the GC thread that scanned this GC\n+  \/\/     allocation buffer performs a write-commit memory barrier.\n+  \/\/  5. Background GC threads that perform tenuring of young-gen\n+  \/\/     objects without a GCLAB use a CAS lock before registering\n+  \/\/     each tenured object.  The CAS lock assures both mutual\n+  \/\/     exclusion and memory coherency\/visibility.  Note that an\n+  \/\/     object tenured by a background GC thread will not overlap\n+  \/\/     with any of the clusters that are receiving tenured objects\n+  \/\/     by way of GCLAB buffers.  Multiple independent GC threads may\n+  \/\/     attempt to tenure objects into a shared cluster.  This is why\n+  \/\/     sychronization may be necessary.  Consider the following\n+  \/\/     scenarios:\n+  \/\/\n+  \/\/     a) If two objects are tenured into the same card region, each\n+  \/\/        registration may attempt to modify the first-start or\n+  \/\/        last-start information associated with that card region.\n+  \/\/        Furthermore, because the representations of first-start\n+  \/\/        and last-start information within the object_starts array\n+  \/\/        entry uses different bits of a shared uint_16 to represent\n+  \/\/        each, it is necessary to lock the entire card entry\n+  \/\/        before modifying either the first-start or last-start\n+  \/\/        information within the entry.\n+  \/\/     b) Suppose GC thread X promotes a tenured object into\n+  \/\/        card region A and this tenured object spans into\n+  \/\/        neighboring card region B.  Suppose GC thread Y (not equal\n+  \/\/        to X) promotes a tenured object into cluster B.  GC thread X\n+  \/\/        will update the object_starts information for card A.  No\n+  \/\/        synchronization is required.\n+  \/\/     c) In summary, when background GC threads register objects\n+  \/\/        newly tenured into old-gen memory, they must acquire a\n+  \/\/        mutual exclusion lock on the card that holds the starting\n+  \/\/        address of the newly tenured object.  This can be achieved\n+  \/\/        by using a CAS instruction to assure that the previous\n+  \/\/        values of first-offset and last-offset have not been\n+  \/\/        changed since the same thread inquired as to their most\n+  \/\/        current values.\n+  \/\/\n+  \/\/     One way to minimize the need for synchronization between\n+  \/\/     background tenuring GC threads is for each tenuring GC thread\n+  \/\/     to promote young-gen objects into distinct dedicated cluster\n+  \/\/     ranges.\n+  \/\/  6. The object_starts information is only required during the\n+  \/\/     starting of concurrent marking and concurrent evacuation\n+  \/\/     phases of GC.  Before we start either of these GC phases, the\n+  \/\/     JVM enters a safe point and all GC threads perform\n+  \/\/     commit-write barriers to assure that access to the\n+  \/\/     object_starts information is coherent.\n+\n+\n+  \/\/ Notes on synchronization of register_object():\n+  \/\/\n+  \/\/  1. For efficiency, there is no locking in the implementation of register_object()\n+  \/\/  2. Thus, it is required that users of this service assure that concurrent\/parallel invocations of\n+  \/\/     register_object() do pertain to the same card's memory range.  See discussion below to understand\n+  \/\/     the risks.\n+  \/\/  3. When allocating from a TLAB or GCLAB, the mutual exclusion can be guaranteed by assuring that each\n+  \/\/     LAB's start and end are aligned on card memory boundaries.\n+  \/\/  4. Use the same lock that guarantees exclusivity when performing free-list allocation within heap regions.\n+  \/\/\n+  \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+  \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+  \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+  \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+  \/\/\n+  \/\/ objects being \"concurrently\" allocated:\n+  \/\/    [-----a------][-----b-----][--------------c------------------]\n+  \/\/            [---- card table memory range --------------]\n+  \/\/\n+  \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+  \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+  \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/\n+  \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+  \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+  \/\/ invocations associated with objects that are allocated from \"free lists\" to provide their own mutual exclusion locking\n+  \/\/ mechanism.\n+\n+  \/\/ Reset the starts_object() information to false for all cards in the range between from and to.\n+  void reset_object_range(HeapWord *from, HeapWord *to);\n+\n+  \/\/ register_object() requires that the caller hold the heap lock\n+  \/\/ before calling it.\n+  void register_object(HeapWord* address);\n+\n+  \/\/ register_object_without_lock() does not require that the caller hold\n+  \/\/ the heap lock before calling it, under the assumption that the\n+  \/\/ caller has assure no other thread will endeavor to concurrently\n+  \/\/ register objects that start within the same card's memory region\n+  \/\/ as address.\n+  void register_object_without_lock(HeapWord* address);\n+\n+  \/\/ During the reference updates phase of GC, we walk through each old-gen memory region that was\n+  \/\/ not part of the collection set and we invalidate all unmarked objects.  As part of this effort,\n+  \/\/ we coalesce neighboring dead objects in order to make future remembered set scanning more\n+  \/\/ efficient (since future remembered set scanning of any card region containing consecutive\n+  \/\/ dead objects can skip over all of them at once by reading only a single dead object header\n+  \/\/ instead of having to read the header of each of the coalesced dead objects.\n+  \/\/\n+  \/\/ At some future time, we may implement a further optimization: satisfy future allocation requests\n+  \/\/ by carving new objects out of the range of memory that represents the coalesced dead objects.\n+  \/\/\n+  \/\/ Suppose we want to combine several dead objects into a single coalesced object.  How does this\n+  \/\/ impact our representation of crossing map information?\n+  \/\/  1. If the newly coalesced range is contained entirely within a card range, that card's last\n+  \/\/     start entry either remains the same or it is changed to the start of the coalesced region.\n+  \/\/  2. For the card that holds the start of the coalesced object, it will not impact the first start\n+  \/\/     but it may impact the last start.\n+  \/\/  3. For following cards spanned entirely by the newly coalesced object, it will change starts_object\n+  \/\/     to false (and make first-start and last-start \"undefined\").\n+  \/\/  4. For a following card that is spanned patially by the newly coalesced object, it may change\n+  \/\/     first-start value, but it will not change the last-start value.\n+  \/\/\n+  \/\/ The range of addresses represented by the arguments to coalesce_objects() must represent a range\n+  \/\/ of memory that was previously occupied exactly by one or more previously registered objects.  For\n+  \/\/ convenience, it is legal to invoke coalesce_objects() with arguments that span a single previously\n+  \/\/ registered object.\n+  \/\/\n+  \/\/ The role of coalesce_objects is to change the crossing map information associated with all of the coalesced\n+  \/\/ objects.\n+  void coalesce_objects(HeapWord* address, size_t length_in_words);\n+\n+  \/\/ The typical use case is going to look something like this:\n+  \/\/   for each heapregion that comprises old-gen memory\n+  \/\/     for each card number that corresponds to this heap region\n+  \/\/       scan the objects contained therein if the card is dirty\n+  \/\/ To avoid excessive lookups in a sparse array, the API queries\n+  \/\/ the card number pertaining to a particular address and then uses the\n+  \/\/ card number for subsequent information lookups and stores.\n+\n+  \/\/ If starts_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the first object begins.  If !starts_object(card_index), the\n+  \/\/ result is a don't care value -- asserts in a debug build.\n+  size_t get_first_start(size_t card_index) const;\n+\n+  \/\/ If starts_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the last object begins.  If !starts_object(card_index), the\n+  \/\/ result is a don't care value.\n+  size_t get_last_start(size_t card_index) const;\n+\n+\n+  \/\/ Given a card_index, return the starting address of the first block in the heap\n+  \/\/ that straddles into the card. If the card is co-initial with an object, then\n+  \/\/ this would return the starting address of the heap that this card covers.\n+  \/\/ Expects to be called for a card affiliated with the old generation in\n+  \/\/ generational mode.\n+  HeapWord* block_start(size_t card_index) const;\n+};\n+\n+\/\/ ShenandoahScanRemembered is a concrete class representing the\n+\/\/ ability to scan the old-gen remembered set for references to\n+\/\/ objects residing in young-gen memory.\n+\/\/\n+\/\/ Scanning normally begins with an invocation of numRegions and ends\n+\/\/ after all clusters of all regions have been scanned.\n+\/\/\n+\/\/ Throughout the scanning effort, the number of regions does not\n+\/\/ change.\n+\/\/\n+\/\/ Even though the regions that comprise old-gen memory are not\n+\/\/ necessarily contiguous, the abstraction represented by this class\n+\/\/ identifies each of the old-gen regions with an integer value\n+\/\/ in the range from 0 to (numRegions() - 1) inclusive.\n+\/\/\n+\n+template<typename RememberedSet>\n+class ShenandoahScanRemembered: public CHeapObj<mtGC> {\n+\n+private:\n+  RememberedSet* _rs;\n+  ShenandoahCardCluster<RememberedSet>* _scc;\n+\n+  \/\/ Global card stats (cumulative)\n+  HdrSeq _card_stats_scan_rs[MAX_CARD_STAT_TYPE];\n+  HdrSeq _card_stats_update_refs[MAX_CARD_STAT_TYPE];\n+  \/\/ Per worker card stats (multiplexed by phase)\n+  HdrSeq** _card_stats;\n+\n+  \/\/ The types of card metrics that we gather\n+  const char* _card_stats_name[MAX_CARD_STAT_TYPE] = {\n+   \"dirty_run\", \"clean_run\",\n+   \"dirty_cards\", \"clean_cards\",\n+   \"max_dirty_run\", \"max_clean_run\",\n+   \"dirty_scan_objs\",\n+   \"alternations\"\n+  };\n+\n+  \/\/ The statistics are collected and logged separately for\n+  \/\/ card-scans for initial marking, and for updating refs.\n+  const char* _card_stat_log_type[MAX_CARD_STAT_LOG_TYPE] = {\n+   \"Scan Remembered Set\", \"Update Refs\"\n+  };\n+\n+  int _card_stats_log_counter[2] = {0, 0};\n+\n+public:\n+  \/\/ How to instantiate this object?\n+  \/\/   ShenandoahDirectCardMarkRememberedSet *rs =\n+  \/\/       new ShenandoahDirectCardMarkRememberedSet();\n+  \/\/   scr = new\n+  \/\/     ShenandoahScanRememberd<ShenandoahDirectCardMarkRememberedSet>(rs);\n+  \/\/\n+  \/\/ or, after the planned implementation of\n+  \/\/ ShenandoahBufferWithSATBRememberedSet has been completed:\n+  \/\/\n+  \/\/   ShenandoahBufferWithSATBRememberedSet *rs =\n+  \/\/       new ShenandoahBufferWithSATBRememberedSet();\n+  \/\/   scr = new\n+  \/\/     ShenandoahScanRememberd<ShenandoahBufferWithSATBRememberedSet>(rs);\n+\n+\n+  ShenandoahScanRemembered(RememberedSet *rs) {\n+    _rs = rs;\n+    _scc = new ShenandoahCardCluster<RememberedSet>(rs);\n+\n+    \/\/ We allocate ParallelGCThreads worth even though we usually only\n+    \/\/ use up to ConcGCThreads, because degenerate collections may employ\n+    \/\/ ParallelGCThreads for remembered set scanning.\n+    if (ShenandoahEnableCardStats) {\n+      _card_stats = NEW_C_HEAP_ARRAY(HdrSeq*, ParallelGCThreads, mtGC);\n+      for (uint i = 0; i < ParallelGCThreads; i++) {\n+        _card_stats[i] = new HdrSeq[MAX_CARD_STAT_TYPE];\n+      }\n+    } else {\n+      _card_stats = nullptr;\n+    }\n+  }\n+\n+  ~ShenandoahScanRemembered() {\n+    delete _scc;\n+    if (ShenandoahEnableCardStats) {\n+      for (uint i = 0; i < ParallelGCThreads; i++) {\n+        delete _card_stats[i];\n+      }\n+      FREE_C_HEAP_ARRAY(HdrSeq*, _card_stats);\n+      _card_stats = nullptr;\n+    }\n+    assert(_card_stats == nullptr, \"Error\");\n+  }\n+\n+  HdrSeq* card_stats(uint worker_id) {\n+    assert(worker_id < ParallelGCThreads, \"Error\");\n+    assert(ShenandoahEnableCardStats == (_card_stats != nullptr), \"Error\");\n+    return ShenandoahEnableCardStats ? _card_stats[worker_id] : nullptr;\n+  }\n+\n+  HdrSeq* card_stats_for_phase(CardStatLogType t) {\n+    switch (t) {\n+      case CARD_STAT_SCAN_RS:\n+        return _card_stats_scan_rs;\n+      case CARD_STAT_UPDATE_REFS:\n+        return _card_stats_update_refs;\n+      default:\n+        guarantee(false, \"No such CardStatLogType\");\n+    }\n+    return nullptr; \/\/ Quiet compiler\n+  }\n+\n+  \/\/ TODO:  We really don't want to share all of these APIs with arbitrary consumers of the ShenandoahScanRemembered abstraction.\n+  \/\/ But in the spirit of quick and dirty for the time being, I'm going to go ahead and publish everything for right now.  Some\n+  \/\/ of existing code already depends on having access to these services (because existing code has not been written to honor\n+  \/\/ full abstraction of remembered set scanning.  In the not too distant future, we want to try to make most, if not all, of\n+  \/\/ these services private.  Two problems with publicizing:\n+  \/\/  1. Allowing arbitrary users to reach beneath the hood allows the users to make assumptions about underlying implementation.\n+  \/\/     This will make it more difficult to change underlying implementation at a future time, such as when we eventually experiment\n+  \/\/     with SATB-based implementation of remembered set representation.\n+  \/\/  2. If we carefully control sharing of certain of these services, we can reduce the overhead of synchronization by assuring\n+  \/\/     that all users follow protocols that avoid contention that might require synchronization.  When we publish these APIs, we\n+  \/\/     lose control over who and how the data is accessed.  As a result, we are required to insert more defensive measures into\n+  \/\/     the implementation, including synchronization locks.\n+\n+\n+  \/\/ Card index is zero-based relative to first spanned card region.\n+  size_t last_valid_index();\n+  size_t total_cards();\n+  size_t card_index_for_addr(HeapWord *p);\n+  HeapWord *addr_for_card_index(size_t card_index);\n+  bool is_card_dirty(size_t card_index);\n+  bool is_write_card_dirty(size_t card_index) { return _rs->is_write_card_dirty(card_index); }\n+  void mark_card_as_dirty(size_t card_index);\n+  void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  void mark_card_as_clean(size_t card_index);\n+  void mark_read_card_as_clean(size_t card_index) { _rs->mark_read_card_clean(card_index); }\n+  void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  bool is_card_dirty(HeapWord *p);\n+  void mark_card_as_dirty(HeapWord *p);\n+  void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n+  void mark_card_as_clean(HeapWord *p);\n+  void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n+  size_t cluster_count();\n+\n+  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n+  void swap_remset() { _rs->swap_remset(); }\n+\n+  void reset_remset(HeapWord* start, size_t word_count) { _rs->reset_remset(start, word_count); }\n+\n+  void merge_write_table(HeapWord* start, size_t word_count) { _rs->merge_write_table(start, word_count); }\n+\n+  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n+  void clear_old_remset() { _rs->clear_old_remset(); }\n+\n+  size_t cluster_for_addr(HeapWord *addr);\n+  HeapWord* addr_for_cluster(size_t cluster_no);\n+\n+  void reset_object_range(HeapWord *from, HeapWord *to);\n+  void register_object(HeapWord *addr);\n+  void register_object_without_lock(HeapWord *addr);\n+  void coalesce_objects(HeapWord *addr, size_t length_in_words);\n+\n+  HeapWord* first_object_in_card(size_t card_index) {\n+    if (_scc->starts_object(card_index)) {\n+      return addr_for_card_index(card_index) + _scc->get_first_start(card_index);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+\n+  \/\/ Return true iff this object is \"properly\" registered.\n+  bool verify_registration(HeapWord* address, ShenandoahMarkingContext* ctx);\n+\n+  \/\/ clear the cards to clean, and clear the object_starts info to no objects\n+  void mark_range_as_empty(HeapWord *addr, size_t length_in_words);\n+\n+  \/\/ process_clusters() scans a portion of the remembered set\n+  \/\/ for references from old gen into young. Several worker threads\n+  \/\/ scan different portions of the remembered set by making parallel invocations\n+  \/\/ of process_clusters() with each invocation scanning different\n+  \/\/ \"clusters\" of the remembered set.\n+  \/\/\n+  \/\/ An invocation of process_clusters() examines all of the\n+  \/\/ intergenerational references spanned by `count` clusters starting\n+  \/\/ with `first_cluster`.  The `oops` argument is a worker-thread-local\n+  \/\/ OopClosure that is applied to all \"valid\" references in the remembered set.\n+  \/\/\n+  \/\/ A side-effect of executing process_clusters() is to update the remembered\n+  \/\/ set entries (e.g. marking dirty cards clean if they no longer\n+  \/\/ hold references to young-gen memory).\n+  \/\/\n+  \/\/ An implementation of process_clusters() may choose to efficiently\n+  \/\/ address more typical scenarios in the structure of remembered sets. E.g.\n+  \/\/ in the generational setting, one might expect remembered sets to be very sparse\n+  \/\/ (low mutation rates in the old generation leading to sparse dirty cards,\n+  \/\/ each with very few intergenerational pointers). Specific implementations\n+  \/\/ may choose to degrade gracefully as the sparsity assumption fails to hold,\n+  \/\/ such as when there are sudden spikes in (premature) promotion or in the\n+  \/\/ case of an underprovisioned, poorly-tuned, or poorly-shaped heap.\n+  \/\/\n+  \/\/ At the start of a concurrent young generation marking cycle, we invoke process_clusters\n+  \/\/ with ClosureType ShenandoahInitMarkRootsClosure.\n+  \/\/\n+  \/\/ At the start of a concurrent evacuation phase, we invoke process_clusters with\n+  \/\/ ClosureType ShenandoahEvacuateUpdateRootsClosure.\n+\n+  \/\/ All template expansions require methods to be defined in the inline.hpp file, but larger\n+  \/\/ such methods need not be declared as inline.\n+  template <typename ClosureType>\n+  void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops,\n+                               bool use_write_table, uint worker_id);\n+\n+  template <typename ClosureType>\n+  inline void process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                         HeapWord *end_of_range, ClosureType *oops, bool use_write_table);\n+\n+  template <typename ClosureType>\n+  inline void process_region_slice(ShenandoahHeapRegion* region, size_t offset, size_t clusters, HeapWord* end_of_range,\n+                                   ClosureType *cl, bool use_write_table, uint worker_id);\n+\n+  \/\/ To Do:\n+  \/\/  Create subclasses of ShenandoahInitMarkRootsClosure and\n+  \/\/  ShenandoahEvacuateUpdateRootsClosure and any other closures\n+  \/\/  that need to participate in remembered set scanning.  Within the\n+  \/\/  subclasses, add a (probably templated) instance variable that\n+  \/\/  refers to the associated ShenandoahCardCluster object.  Use this\n+  \/\/  ShenandoahCardCluster instance to \"enhance\" the do_oops\n+  \/\/  processing so that we can:\n+  \/\/\n+  \/\/   1. Avoid processing references that correspond to clean card\n+  \/\/      regions, and\n+  \/\/   2. Set card status to CLEAN when the associated card region no\n+  \/\/      longer holds inter-generatioanal references.\n+  \/\/\n+  \/\/  To enable efficient implementation of these behaviors, we\n+  \/\/  probably also want to add a few fields into the\n+  \/\/  ShenandoahCardCluster object that allow us to precompute and\n+  \/\/  remember the addresses at which card status is going to change\n+  \/\/  from dirty to clean and clean to dirty.  The do_oops\n+  \/\/  implementations will want to update this value each time they\n+  \/\/  cross one of these boundaries.\n+  void roots_do(OopIterateClosure* cl);\n+\n+  \/\/ Log stats related to card\/RS stats for given phase t\n+  void log_card_stats(uint nworkers, CardStatLogType t) PRODUCT_RETURN;\n+private:\n+  \/\/ Log stats for given worker id related into given summary card\/RS stats\n+  void log_worker_card_stats(uint worker_id, HdrSeq* sum_stats) PRODUCT_RETURN;\n+\n+  \/\/ Log given stats\n+  inline void log_card_stats(HdrSeq* stats) PRODUCT_RETURN;\n+\n+  \/\/ Merge the stats from worked_id into the given summary stats, and clear the worker_id's stats.\n+  void merge_worker_card_stats_cumulative(HdrSeq* worker_stats, HdrSeq* sum_stats) PRODUCT_RETURN;\n+};\n+\n+\n+\/\/ A ShenandoahRegionChunk represents a contiguous interval of a ShenandoahHeapRegion, typically representing\n+\/\/ work to be done by a worker thread.\n+struct ShenandoahRegionChunk {\n+  ShenandoahHeapRegion *_r;      \/\/ The region of which this represents a chunk\n+  size_t _chunk_offset;          \/\/ HeapWordSize offset\n+  size_t _chunk_size;            \/\/ HeapWordSize qty\n+};\n+\n+\/\/ ShenandoahRegionChunkIterator divides the total remembered set scanning effort into ShenandoahRegionChunks\n+\/\/ that are assigned one at a time to worker threads. (Here, we use the terms `assignments` and `chunks`\n+\/\/ interchangeably.) Note that the effort required to scan a range of memory is not necessarily a linear\n+\/\/ function of the size of the range.  Some memory ranges hold only a small number of live objects.\n+\/\/ Some ranges hold primarily primitive (non-pointer) data.  We start with larger chunk sizes because larger chunks\n+\/\/ reduce coordination overhead.  We expect that the GC worker threads that receive more difficult assignments\n+\/\/ will work longer on those chunks.  Meanwhile, other worker will threads repeatedly accept and complete multiple\n+\/\/ easier chunks.  As the total amount of work remaining to be completed decreases, we decrease the size of chunks\n+\/\/ given to individual threads.  This reduces the likelihood of significant imbalance between worker thread assignments\n+\/\/ when there is less meaningful work to be performed by the remaining worker threads while they wait for\n+\/\/ worker threads with difficult assignments to finish, reducing the overall duration of the phase.\n+\n+class ShenandoahRegionChunkIterator : public StackObj {\n+private:\n+  \/\/ The largest chunk size is 4 MiB, measured in words.  Otherwise, remembered set scanning may become too unbalanced.\n+  \/\/ If the largest chunk size is too small, there is too much overhead sifting out assignments to individual worker threads.\n+  static const size_t _maximum_chunk_size_words = (4 * 1024 * 1024) \/ HeapWordSize;\n+\n+  static const size_t _clusters_in_smallest_chunk = 4;\n+\n+  \/\/ smallest_chunk_size is 4 clusters.  Each cluster spans 128 KiB.\n+  \/\/ This is computed from CardTable::card_size_in_words() *\n+  \/\/      ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  static size_t smallest_chunk_size_words() {\n+      return _clusters_in_smallest_chunk * CardTable::card_size_in_words() *\n+             ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  }\n+\n+  \/\/ The total remembered set scanning effort is divided into chunks of work that are assigned to individual worker tasks.\n+  \/\/ The chunks of assigned work are divided into groups, where the size of the typical group (_regular_group_size) is half the\n+  \/\/ total number of regions.  The first group may be larger than\n+  \/\/ _regular_group_size in the case that the first group's chunk\n+  \/\/ size is less than the region size.  The last group may be larger\n+  \/\/ than _regular_group_size because no group is allowed to\n+  \/\/ have smaller assignments than _smallest_chunk_size, which is 128 KB.\n+\n+  \/\/ Under normal circumstances, no configuration needs more than _maximum_groups (default value of 16).\n+  \/\/ The first group \"effectively\" processes chunks of size 1 MiB (or smaller for smaller region sizes).\n+  \/\/ The last group processes chunks of size 128 KiB.  There are four groups total.\n+\n+  \/\/ group[0] is 4 MiB chunk size (_maximum_chunk_size_words)\n+  \/\/ group[1] is 2 MiB chunk size\n+  \/\/ group[2] is 1 MiB chunk size\n+  \/\/ group[3] is 512 KiB chunk size\n+  \/\/ group[4] is 256 KiB chunk size\n+  \/\/ group[5] is 128 Kib shunk size (_smallest_chunk_size_words = 4 * 64 * 64\n+  static const size_t _maximum_groups = 6;\n+\n+  const ShenandoahHeap* _heap;\n+\n+  const size_t _regular_group_size;                        \/\/ Number of chunks in each group\n+  const size_t _first_group_chunk_size_b4_rebalance;\n+  const size_t _num_groups;                        \/\/ Number of groups in this configuration\n+  const size_t _total_chunks;\n+\n+  shenandoah_padding(0);\n+  volatile size_t _index;\n+  shenandoah_padding(1);\n+\n+  size_t _region_index[_maximum_groups];           \/\/ The region index for the first region spanned by this group\n+  size_t _group_offset[_maximum_groups];           \/\/ The offset at which group begins within first region spanned by this group\n+  size_t _group_chunk_size[_maximum_groups];       \/\/ The size of each chunk within this group\n+  size_t _group_entries[_maximum_groups];          \/\/ Total chunks spanned by this group and the ones before it.\n+\n+  \/\/ No implicit copying: iterators should be passed by reference to capture the state\n+  NONCOPYABLE(ShenandoahRegionChunkIterator);\n+\n+  \/\/ Makes use of _heap.\n+  size_t calc_regular_group_size();\n+\n+  \/\/ Makes use of _regular_group_size, which must be initialized before call.\n+  size_t calc_first_group_chunk_size_b4_rebalance();\n+\n+  \/\/ Makes use of _regular_group_size and _first_group_chunk_size_b4_rebalance, both of which must be initialized before call.\n+  size_t calc_num_groups();\n+\n+  \/\/ Makes use of _regular_group_size, _first_group_chunk_size_b4_rebalance, which must be initialized before call.\n+  size_t calc_total_chunks();\n+\n+public:\n+  ShenandoahRegionChunkIterator(size_t worker_count);\n+  ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count);\n+\n+  \/\/ Reset iterator to default state\n+  void reset();\n+\n+  \/\/ Fills in assignment with next chunk of work and returns true iff there is more work.\n+  \/\/ Otherwise, returns false.  This is multi-thread-safe.\n+  inline bool next(struct ShenandoahRegionChunk *assignment);\n+\n+  \/\/ This is *not* MT safe. However, in the absence of multithreaded access, it\n+  \/\/ can be used to determine if there is more work to do.\n+  inline bool has_next() const;\n+};\n+\n+typedef ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet> RememberedScanner;\n+\n+class ShenandoahScanRememberedTask : public WorkerTask {\n+ private:\n+  ShenandoahObjToScanQueueSet* _queue_set;\n+  ShenandoahObjToScanQueueSet* _old_queue_set;\n+  ShenandoahReferenceProcessor* _rp;\n+  ShenandoahRegionChunkIterator* _work_list;\n+  bool _is_concurrent;\n+\n+ public:\n+  ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                               ShenandoahObjToScanQueueSet* old_queue_set,\n+                               ShenandoahReferenceProcessor* rp,\n+                               ShenandoahRegionChunkIterator* work_list,\n+                               bool is_concurrent);\n+\n+  void work(uint worker_id);\n+  void do_work(uint worker_id);\n+};\n+\n+\/\/ Verify that the oop doesn't point into the young generation\n+class ShenandoahVerifyNoYoungRefsClosure: public BasicOopIterateClosure {\n+  ShenandoahHeap* _heap;\n+  template<class T> void work(T* p);\n+\n+ public:\n+  ShenandoahVerifyNoYoungRefsClosure();\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p)       { work(p); }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":1083,"deletions":0,"binary":false,"changes":1083,"status":"added"}]}