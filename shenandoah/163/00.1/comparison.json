{"files":[{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -63,1 +64,1 @@\n-        __ mov(rscratch2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING);\n+        __ mov(rscratch2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -80,0 +81,7 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                       Register start, Register count, Register tmp, RegSet saved_regs) {\n+  if (is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n@@ -378,0 +386,25 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+\n+  __ lsr(obj, obj, CardTable::card_shift());\n+\n+  assert(CardTable::dirty_card_val() == 0, \"must be\");\n+\n+  __ load_byte_map_base(rscratch1);\n+\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ ldrb(rscratch2,  Address(obj, rscratch1));\n+    __ cbz(rscratch2, L_already_dirty);\n+    __ strb(zr, Address(obj, rscratch1));\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ strb(zr, Address(obj, rscratch1));\n+  }\n+}\n+\n@@ -414,0 +447,1 @@\n+    store_check(masm, r3);\n@@ -598,0 +632,29 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register start, Register count, Register scratch, RegSet saved_regs) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+\n+  __ cbz(count, L_done); \/\/ zero count - nothing to do\n+\n+  __ lea(end, Address(start, count, Address::lsl(LogBytesPerHeapOop))); \/\/ end = start + count << LogBytesPerHeapOop\n+  __ sub(end, end, BytesPerHeapOop); \/\/ last element address to make inclusive\n+  __ lsr(start, start, CardTable::card_shift());\n+  __ lsr(end, end, CardTable::card_shift());\n+  __ sub(count, end, start); \/\/ number of bytes to copy\n+\n+  __ load_byte_map_base(scratch);\n+  __ add(start, start, scratch);\n+  __ bind(L_loop);\n+  __ strb(zr, Address(start, count));\n+  __ subs(count, count, 1);\n+  __ br(Assembler::GE, L_loop);\n+  __ bind(L_done);\n+}\n+\n@@ -698,1 +761,7 @@\n-  __ tbz(tmp, ShenandoahHeap::MARKING_BITPOS, done);\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    __ tbz(tmp, ShenandoahHeap::YOUNG_MARKING_BITPOS, done);\n+  } else {\n+    __ mov(rscratch2, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n+    __ tst(tmp, rscratch2);\n+    __ br(Assembler::EQ, done);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":71,"deletions":2,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -58,0 +58,2 @@\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n@@ -62,0 +64,2 @@\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators, Register start, Register count, Register scratch, RegSet saved_regs);\n+\n@@ -77,0 +81,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register start, Register count, Register tmp, RegSet saved_regs);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -123,0 +124,24 @@\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+      bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+      bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+\n+      \/\/ We need to squirrel away the original element count because the\n+      \/\/ array copy assembly will destroy the value and we need it for the\n+      \/\/ card marking barrier.\n+#ifdef _LP64\n+      if (!checkcast) {\n+        if (!obj_int) {\n+          \/\/ Save count for barrier\n+          __ movptr(r11, count);\n+        } else if (disjoint) {\n+          \/\/ Save dst in r11 in the disjoint case\n+          __ movq(r11, dst);\n+        }\n+      }\n+#else\n+if (disjoint) {\n+        __ mov(rdx, dst);          \/\/ save 'to'\n+      }\n+#endif\n+    }\n@@ -154,1 +179,1 @@\n-        flags = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING;\n+        flags = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING;\n@@ -184,0 +209,29 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register src, Register dst, Register count) {\n+  bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+  bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+  bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+  Register tmp = rax;\n+\n+if (is_reference_type(type)) {\n+#ifdef _LP64\n+    if (!checkcast) {\n+      if (!obj_int) {\n+        \/\/ Save count for barrier\n+        count = r11;\n+      } else if (disjoint) {\n+        \/\/ Use the saved dst in the disjoint case\n+        dst = r11;\n+      }\n+    } else {\n+      tmp = rscratch1;\n+    }\n+#else\n+    if (disjoint) {\n+      __ mov(dst, rdx); \/\/ restore 'to'\n+    }\n+#endif\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, tmp);\n+  }\n+}\n+\n@@ -227,1 +281,1 @@\n-  __ testb(gc_state, ShenandoahHeap::MARKING);\n+  __ testb(gc_state, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -593,0 +647,45 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  \/\/ Does a store check for the oop in register obj. The content of\n+  \/\/ register obj is destroyed afterwards.\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+\n+  __ shrptr(obj, CardTable::card_shift());\n+\n+  Address card_addr;\n+\n+  \/\/ The calculation for byte_map_base is as follows:\n+  \/\/ byte_map_base = _byte_map - (uintptr_t(low_bound) >> card_shift);\n+  \/\/ So this essentially converts an address to a displacement and it will\n+  \/\/ never need to be relocated. On 64bit however the value may be too\n+  \/\/ large for a 32bit displacement.\n+  intptr_t byte_map_base = (intptr_t)ct->byte_map_base();\n+  if (__ is_simm32(byte_map_base)) {\n+    card_addr = Address(noreg, obj, Address::times_1, byte_map_base);\n+  } else {\n+    \/\/ By doing it as an ExternalAddress 'byte_map_base' could be converted to a rip-relative\n+    \/\/ displacement and done in a single instruction given favorable mapping and a\n+    \/\/ smarter version of as_Address. However, 'ExternalAddress' generates a relocation\n+    \/\/ entry and that entry is not properly handled by the relocation code.\n+    AddressLiteral cardtable((address)byte_map_base, relocInfo::none);\n+    Address index(noreg, obj, Address::times_1);\n+    card_addr = __ as_Address(ArrayAddress(cardtable, index), rscratch1);\n+  }\n+\n+  int dirty = CardTable::dirty_card_val();\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ cmpb(card_addr, dirty);\n+    __ jcc(Assembler::equal, L_already_dirty);\n+    __ movb(card_addr, dirty);\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ movb(card_addr, dirty);\n+  }\n+}\n+\n@@ -634,0 +733,1 @@\n+      \/\/ XXX: store_check missing from upstream\n@@ -635,0 +735,1 @@\n+      store_check(masm, tmp1);\n@@ -830,0 +931,55 @@\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+#define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)\n+\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators, Register addr, Register count, Register tmp) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  intptr_t disp = (intptr_t) ct->byte_map_base();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+  assert_different_registers(addr, end);\n+\n+  __ testl(count, count);\n+  __ jcc(Assembler::zero, L_done); \/\/ zero count - nothing to do\n+\n+\n+#ifdef _LP64\n+  __ leaq(end, Address(addr, count, TIMES_OOP, 0));  \/\/ end == addr+count*oop_size\n+  __ subptr(end, BytesPerHeapOop); \/\/ end - 1 to make inclusive\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end, CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> cards count\n+\n+  __ mov64(tmp, disp);\n+  __ addptr(addr, tmp);\n+__ BIND(L_loop);\n+  __ movb(Address(addr, count, Address::times_1), 0);\n+  __ decrement(count);\n+  __ jcc(Assembler::greaterEqual, L_loop);\n+#else\n+  __ lea(end,  Address(addr, count, Address::times_ptr, -wordSize));\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end,   CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> count\n+__ BIND(L_loop);\n+  Address cardtable(addr, count, Address::times_1, disp);\n+  __ movb(cardtable, 0);\n+  __ decrement(count);\n+  __ jcc(Assembler::greaterEqual, L_loop);\n+#endif\n+\n+__ BIND(L_done);\n+}\n+\n@@ -945,1 +1101,1 @@\n-  __ testb(gc_state, ShenandoahHeap::MARKING);\n+  __ testb(gc_state, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":159,"deletions":3,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  assert(UseG1GC || UseParallelGC || UseSerialGC,\n+  assert(UseG1GC || UseParallelGC || UseSerialGC || UseShenandoahGC,\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -244,1 +245,1 @@\n-  marking = __ AndI(ld, __ ConI(ShenandoahHeap::MARKING));\n+  marking = __ AndI(ld, __ ConI(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n@@ -325,1 +326,1 @@\n-      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::MARKING)) {\n+      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING)) {\n@@ -454,0 +455,92 @@\n+Node* ShenandoahBarrierSetC2::byte_map_base_node(GraphKit* kit) const {\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  CardTable::CardValue* card_table_base = ctbs->card_table()->byte_map_base();\n+  if (card_table_base != NULL) {\n+    return kit->makecon(TypeRawPtr::make((address)card_table_base));\n+  } else {\n+    return kit->null();\n+  }\n+}\n+\n+void ShenandoahBarrierSetC2::post_barrier(GraphKit* kit,\n+                                          Node* ctl,\n+                                          Node* oop_store,\n+                                          Node* obj,\n+                                          Node* adr,\n+                                          uint  adr_idx,\n+                                          Node* val,\n+                                          BasicType bt,\n+                                          bool use_precise) const {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(BarrierSet::barrier_set());\n+  CardTable* ct = ctbs->card_table();\n+  \/\/ No store check needed if we're storing a NULL or an old object\n+  \/\/ (latter case is probably a string constant). The concurrent\n+  \/\/ mark sweep garbage collector, however, needs to have all nonNull\n+  \/\/ oop updates flagged via card-marks.\n+  if (val != NULL && val->is_Con()) {\n+    \/\/ must be either an oop or NULL\n+    const Type* t = val->bottom_type();\n+    if (t == TypePtr::NULL_PTR || t == Type::TOP)\n+      \/\/ stores of null never (?) need barriers\n+      return;\n+  }\n+\n+  if (ReduceInitialCardMarks && obj == kit->just_allocated_object(kit->control())) {\n+    \/\/ We can skip marks on a freshly-allocated object in Eden.\n+    \/\/ Keep this code in sync with new_deferred_store_barrier() in runtime.cpp.\n+    \/\/ That routine informs GC to take appropriate compensating steps,\n+    \/\/ upon a slow-path allocation, so as to make this card-mark\n+    \/\/ elision safe.\n+    return;\n+  }\n+\n+  if (!use_precise) {\n+    \/\/ All card marks for a (non-array) instance are in one place:\n+    adr = obj;\n+  }\n+  \/\/ (Else it's an array (or unknown), and we want more precise card marks.)\n+  assert(adr != NULL, \"\");\n+\n+  IdealKit ideal(kit, true);\n+\n+  \/\/ Convert the pointer to an int prior to doing math on it\n+  Node* cast = __ CastPX(__ ctrl(), adr);\n+\n+  \/\/ Divide by card size\n+  Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift()) );\n+\n+  \/\/ Combine card table base and card offset\n+  Node* card_adr = __ AddP(__ top(), byte_map_base_node(kit), card_offset );\n+\n+  \/\/ Get the alias_index for raw card-mark memory\n+  int adr_type = Compile::AliasIdxRaw;\n+  Node*   zero = __ ConI(0); \/\/ Dirty card value\n+\n+  if (UseCondCardMark) {\n+    \/\/ The classic GC reference write barrier is typically implemented\n+    \/\/ as a store into the global card mark table.  Unfortunately\n+    \/\/ unconditional stores can result in false sharing and excessive\n+    \/\/ coherence traffic as well as false transactional aborts.\n+    \/\/ UseCondCardMark enables MP \"polite\" conditional card mark\n+    \/\/ stores.  In theory we could relax the load from ctrl() to\n+    \/\/ no_ctrl, but that doesn't buy much latitude.\n+    Node* card_val = __ load( __ ctrl(), card_adr, TypeInt::BYTE, T_BYTE, adr_type);\n+    __ if_then(card_val, BoolTest::ne, zero);\n+  }\n+\n+  \/\/ Smash zero into card\n+  __ store(__ ctrl(), card_adr, zero, T_BYTE, adr_type, MemNode::unordered);\n+\n+  if (UseCondCardMark) {\n+    __ end_if();\n+  }\n+\n+  \/\/ Final sync IdealKit and GraphKit.\n+  kit->final_sync(ideal);\n+}\n+\n@@ -520,0 +613,6 @@\n+\n+    Node* result = BarrierSetC2::store_at_resolved(access, val);\n+    bool is_array = (decorators & IS_ARRAY) != 0;\n+    bool use_precise = is_array || anonymous;\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(), adr, adr_idx, val.node(), access.type(), use_precise);\n+    return result;\n@@ -530,0 +629,1 @@\n+    return BarrierSetC2::store_at_resolved(access, val);\n@@ -531,1 +631,0 @@\n-  return BarrierSetC2::store_at_resolved(access, val);\n@@ -602,1 +701,1 @@\n-                                                   Node* new_val, const Type* value_type) const {\n+                                                             Node* new_val, const Type* value_type) const {\n@@ -644,0 +743,1 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(), access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n@@ -699,0 +799,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n@@ -715,0 +817,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), val, T_OBJECT, true);\n@@ -797,1 +901,1 @@\n-        }\n+    }\n@@ -840,1 +944,1 @@\n-      flags |= ShenandoahHeap::MARKING;\n+      flags |= ShenandoahHeap::YOUNG_MARKING;\n@@ -908,3 +1012,20 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n-  if (is_shenandoah_wb_pre_call(n)) {\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  if (is_shenandoah_wb_pre_call(node)) {\n+    shenandoah_eliminate_wb_pre(node, &macro->igvn());\n+  }\n+  if (node->Opcode() == Op_CastP2X && ShenandoahHeap::heap()->mode()->is_generational()) {\n+    assert(node->Opcode() == Op_CastP2X, \"ConvP2XNode required\");\n+     Node *shift = node->unique_out();\n+     Node *addp = shift->unique_out();\n+     for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n+       Node *mem = addp->last_out(j);\n+       if (UseCondCardMark && mem->is_Load()) {\n+         assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+         \/\/ The load is checking if the card has been written so\n+         \/\/ replace it with zero to fold the test.\n+         macro->replace_node(mem, macro->intcon(0));\n+         continue;\n+       }\n+       assert(mem->is_Store(), \"store required\");\n+       macro->replace_node(mem, mem->in(MemNode::Memory));\n+     }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":130,"deletions":9,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -78,0 +78,12 @@\n+  Node* byte_map_base_node(GraphKit* kit) const;\n+\n+  void post_barrier(GraphKit* kit,\n+                    Node* ctl,\n+                    Node* store,\n+                    Node* obj,\n+                    Node* adr,\n+                    uint adr_idx,\n+                    Node* val,\n+                    BasicType bt,\n+                    bool use_precise) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -26,4 +26,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -66,17 +63,0 @@\n-\n-ShenandoahHeuristics* ShenandoahIUMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics == NULL) {\n-    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n-  }\n-  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-    return new ShenandoahAggressiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-    return new ShenandoahStaticHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-    return new ShenandoahAdaptiveHeuristics();\n-  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-    return new ShenandoahCompactHeuristics();\n-  }\n-  vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-  return NULL;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahIUMode.cpp","additions":1,"deletions":21,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -74,0 +74,3 @@\n+  if (heap->mode()->is_generational() && !obj->is_forwarded()) {\n+    msg.append(\"  age: %d\\n\", obj->age());\n+  }\n@@ -388,1 +391,1 @@\n-  ShenandoahMessageBuffer msg(\"Must ba at a Shenandoah safepoint or held %s lock\", lock->name());\n+  ShenandoahMessageBuffer msg(\"Must be at a Shenandoah safepoint or held %s lock\", lock->name());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -37,0 +38,1 @@\n+  ShenandoahCardTable* _card_table;\n@@ -41,1 +43,1 @@\n-  ShenandoahBarrierSet(ShenandoahHeap* heap);\n+  ShenandoahBarrierSet(ShenandoahHeap* heap, MemRegion heap_region);\n@@ -49,0 +51,2 @@\n+  inline ShenandoahCardTable* card_table()  { return _card_table; }\n+\n@@ -114,0 +118,5 @@\n+  template <DecoratorSet decorators, typename T>\n+  void write_ref_field_post(T* field, oop newVal);\n+\n+  void write_ref_array(HeapWord* start, size_t count);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":10,"deletions":1,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/cardTable.hpp\"\n@@ -39,0 +40,2 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"memory\/iterator.inline.hpp\"\n@@ -106,0 +109,1 @@\n+      _heap->is_in_active_generation(obj) &&\n@@ -113,0 +117,1 @@\n+      _heap->is_in_active_generation(obj) &&\n@@ -182,0 +187,8 @@\n+template <DecoratorSet decorators, typename T>\n+inline void ShenandoahBarrierSet::write_ref_field_post(T* field, oop newVal) {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    volatile CardTable::CardValue* byte = card_table()->byte_for(field);\n+    *byte = CardTable::dirty_card_val();\n+  }\n+}\n+\n@@ -245,1 +258,2 @@\n-  shenandoah_assert_marked_if(NULL, value, !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress());\n+  shenandoah_assert_marked_if(NULL, value, !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress() &&\n+                              !(ShenandoahHeap::heap()->is_gc_generation_young() && ShenandoahHeap::heap()->heap_region_containing(value)->is_old()));\n@@ -265,0 +279,1 @@\n+  ShenandoahBarrierSet::barrier_set()->write_ref_field_post<decorators>(addr, value);\n@@ -286,1 +301,3 @@\n-  return bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n+  oop result = bs->oop_cmpxchg(decorators, addr, compare_value, new_value);\n+  bs->write_ref_field_post<decorators>(addr, new_value);\n+  return result;\n@@ -294,1 +311,4 @@\n-  return bs->oop_cmpxchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), compare_value, new_value);\n+  auto addr = AccessInternal::oop_field_addr<decorators>(base, offset);\n+  oop result = bs->oop_cmpxchg(resolved_decorators, addr, compare_value, new_value);\n+  bs->write_ref_field_post<decorators>(addr, new_value);\n+  return result;\n@@ -310,1 +330,3 @@\n-  return bs->oop_xchg(decorators, addr, new_value);\n+  oop result = bs->oop_xchg(decorators, addr, new_value);\n+  bs->write_ref_field_post<decorators>(addr, new_value);\n+  return result;\n@@ -318,1 +340,4 @@\n-  return bs->oop_xchg(resolved_decorators, AccessInternal::oop_field_addr<decorators>(base, offset), new_value);\n+  auto addr = AccessInternal::oop_field_addr<decorators>(base, offset);\n+  oop result = bs->oop_xchg(resolved_decorators, addr, new_value);\n+  bs->write_ref_field_post<decorators>(addr, new_value);\n+  return result;\n@@ -335,0 +360,3 @@\n+  T* src = arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw);\n+  T* dst = arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw);\n+\n@@ -336,4 +364,4 @@\n-  bs->arraycopy_barrier(arrayOopDesc::obj_offset_to_raw(src_obj, src_offset_in_bytes, src_raw),\n-                        arrayOopDesc::obj_offset_to_raw(dst_obj, dst_offset_in_bytes, dst_raw),\n-                        length);\n-  return Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  bs->arraycopy_barrier(src, dst, length);\n+  bool result = Raw::oop_arraycopy_in_heap(src_obj, src_offset_in_bytes, src_raw, dst_obj, dst_offset_in_bytes, dst_raw, length);\n+  bs->write_ref_array((HeapWord*) dst, length);\n+  return result;\n@@ -344,1 +372,3 @@\n-  assert(HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n+  \/\/ We allow forwarding in young generation and marking in old generation\n+  \/\/ to happen simultaneously.\n+  assert(_heap->mode()->is_generational() || HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n@@ -364,1 +394,1 @@\n-      if (ENQUEUE && !ctx->is_marked_strong(obj)) {\n+      if (ENQUEUE && !ctx->is_marked_strong_or_old(obj)) {\n@@ -377,1 +407,1 @@\n-  if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n+  if ((gc_state & ShenandoahHeap::YOUNG_MARKING) != 0) {\n@@ -379,1 +409,4 @@\n-  } else if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n+    return;\n+  }\n+\n+  if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n@@ -384,0 +417,11 @@\n+\n+  if (_heap->mode()->is_generational()) {\n+    assert(ShenandoahSATBBarrier, \"Generational mode assumes SATB mode\");\n+    \/\/ TODO: Could we optimize here by checking that dst is in an old region?\n+    if ((gc_state & ShenandoahHeap::OLD_MARKING) != 0) {\n+      \/\/ Note that we can't do the arraycopy marking using the 'src' array when\n+      \/\/ SATB mode is enabled (so we can't do this as part of the iteration for\n+      \/\/ evacuation or update references).\n+      arraycopy_marking(src, dst, count);\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":57,"deletions":13,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -0,0 +1,119 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+void ShenandoahCardTable::initialize() {\n+  CardTable::initialize();\n+  _write_byte_map = _byte_map;\n+  _write_byte_map_base = _byte_map_base;\n+  const size_t rs_align = _page_size == (size_t) os::vm_page_size() ? 0 :\n+    MAX2(_page_size, (size_t) os::vm_allocation_granularity());\n+\n+  ReservedSpace heap_rs(_byte_map_size, rs_align, _page_size);\n+  if (!heap_rs.is_reserved()) {\n+    vm_exit_during_initialization(\"Could not reserve enough space for second copy of card marking array\");\n+  }\n+  os::commit_memory_or_exit(heap_rs.base(), _byte_map_size, rs_align, false, \"Cannot commit memory for second copy of card table\");\n+\n+  HeapWord* low_bound  = _whole_heap.start();\n+  _read_byte_map = (CardValue*) heap_rs.base();\n+  _read_byte_map_base = _read_byte_map - (uintptr_t(low_bound) >> card_shift());\n+\n+  log_trace(gc, barrier)(\"ShenandoahCardTable::ShenandoahCardTable: \");\n+  log_trace(gc, barrier)(\"    &_read_byte_map[0]: \" INTPTR_FORMAT \"  &_read_byte_map[_last_valid_index]: \" INTPTR_FORMAT,\n+                  p2i(&_read_byte_map[0]), p2i(&_read_byte_map[last_valid_index()]));\n+  log_trace(gc, barrier)(\"    _read_byte_map_base: \" INTPTR_FORMAT, p2i(_read_byte_map_base));\n+\n+  \/\/ TODO: As currently implemented, we do not swap pointers between _read_byte_map and _write_byte_map\n+  \/\/ because the mutator write barrier hard codes the address of the _write_byte_map_base.  Instead,\n+  \/\/ the current implementation simply copies contents of _write_byte_map onto _read_byte_map and cleans\n+  \/\/ the entirety of _write_byte_map at the init_mark safepoint.\n+  \/\/\n+  \/\/ If we choose to modify the mutator write barrier so that we can swap _read_byte_map_base and\n+  \/\/ _write_byte_map_base pointers, we may also have to figure out certain details about how the\n+  \/\/ _guard_region is implemented so that we can replicate the read and write versions of this region.\n+  \/\/\n+  \/\/ Alternatively, we may switch to a SATB-based write barrier and replace the direct card-marking\n+  \/\/ remembered set with something entirely different.\n+\n+  resize_covered_region(_whole_heap);\n+}\n+\n+bool ShenandoahCardTable::is_in_young(const void* obj) const {\n+  return ShenandoahHeap::heap()->is_in_young(obj);\n+}\n+\n+bool ShenandoahCardTable::is_dirty(MemRegion mr) {\n+  for (size_t i = index_for(mr.start()); i <= index_for(mr.end() - 1); i++) {\n+    CardValue* byte = byte_for_index(i);\n+    if (*byte == CardTable::dirty_card_val()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+size_t ShenandoahCardTable::last_valid_index() {\n+  return CardTable::last_valid_index();\n+}\n+\n+void ShenandoahCardTable::clear() {\n+  CardTable::clear(_whole_heap);\n+}\n+\n+\/\/ TODO: This service is not currently used because we are not able to swap _read_byte_map_base and\n+\/\/ _write_byte_map_base pointers.  If we were able to do so, we would invoke clear_read_table \"immediately\"\n+\/\/ following the end of concurrent remembered set scanning so that this read card table would be ready\n+\/\/ to serve as the new write card table at the time these pointer values were next swapped.\n+\/\/\n+\/\/ In the current implementation, the write-table is cleared immediately after its contents is copied to\n+\/\/ the read table, obviating the need for this service.\n+void ShenandoahCardTable::clear_read_table() {\n+  for (size_t i = 0; i < _byte_map_size; i++) {\n+    _read_byte_map[i] = clean_card;\n+  }\n+}\n+\n+\/\/ TODO: This service is not currently used because the mutator write barrier implementation hard codes the\n+\/\/ location of the _write_byte_may_base.  If we change the mutator's write barrier implementation, then we\n+\/\/ may use this service to exchange the roles of the read-card-table and write-card-table.\n+void ShenandoahCardTable::swap_card_tables() {\n+  shenandoah_assert_safepoint();\n+\n+  CardValue* save_value = _read_byte_map;\n+  _read_byte_map = _write_byte_map;\n+  _write_byte_map = save_value;\n+\n+  save_value = _read_byte_map_base;\n+  _read_byte_map_base = _write_byte_map_base;\n+  _write_byte_map_base = save_value;\n+\n+  \/\/ update the superclass instance variables\n+  _byte_map = _write_byte_map;\n+  _byte_map_base = _write_byte_map_base;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":119,"deletions":0,"binary":false,"changes":119,"status":"added"},{"patch":"@@ -0,0 +1,93 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n+\n+#include \"gc\/g1\/g1RegionToSpaceMapper.hpp\"\n+#include \"gc\/shared\/cardTable.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class ShenandoahCardTable: public CardTable {\n+  friend class VMStructs;\n+\n+protected:\n+  \/\/ We maintain two copies of the card table to facilitate concurrent remembered set scanning\n+  \/\/ and concurrent clearing of stale remembered set information.  During the init_mark safepoint,\n+  \/\/ we copy the contents of _write_byte_map to _read_byte_map and clear _write_byte_map.\n+  \/\/\n+  \/\/ Concurrent remembered set scanning reads from _read_byte_map while concurrent mutator write\n+  \/\/ barriers are overwriting cards of the _write_byte_map with DIRTY codes.  Concurrent remembered\n+  \/\/ set scanning also overwrites cards of the _write_byte_map with DIRTY codes whenever it discovers\n+  \/\/ interesting pointers.\n+  \/\/\n+  \/\/ During a concurrent update-references phase, we scan the _write_byte_map concurrently to find\n+  \/\/ all old-gen references that may need to be updated.\n+  \/\/\n+  \/\/ In a future implementation, we may swap the values of _read_byte_map and _write_byte_map during\n+  \/\/ the init-mark safepoint to avoid the need for bulk STW copying and initialization.  Doing so\n+  \/\/ requires a change to the implementation of mutator write barriers as the address of the card\n+  \/\/ table is currently in-lined and hard-coded.\n+  CardValue* _read_byte_map;\n+  CardValue* _write_byte_map;\n+  CardValue* _read_byte_map_base;\n+  CardValue* _write_byte_map_base;\n+\n+public:\n+  ShenandoahCardTable(MemRegion whole_heap) : CardTable(whole_heap) { }\n+\n+  virtual void initialize();\n+\n+  virtual bool is_in_young(const void* obj) const;\n+\n+  bool is_dirty(MemRegion mr);\n+\n+  size_t last_valid_index();\n+\n+  void clear();\n+\n+  void clear_read_table();\n+\n+  \/\/ Exchange the roles of the read and write card tables.\n+  void swap_card_tables();\n+\n+  CardValue* read_byte_map() {\n+    return _read_byte_map;\n+  }\n+\n+  CardValue* write_byte_map() {\n+    return _write_byte_map;\n+  }\n+\n+  CardValue* read_byte_map_base() {\n+    return _read_byte_map_base;\n+  }\n+\n+  CardValue* write_byte_map_base() {\n+    return _write_byte_map_base;\n+  }\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHCARDTABLE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.hpp","additions":93,"deletions":0,"binary":false,"changes":93,"status":"added"},{"patch":"@@ -33,0 +33,3 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -38,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n@@ -43,0 +47,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -51,2 +56,4 @@\n-  _alloc_failure_waiters_lock(Mutex::safepoint-1, \"ShenandoahAllocFailureGC_lock\", true),\n-  _gc_waiters_lock(Mutex::safepoint-1, \"ShenandoahRequestedGC_lock\", true),\n+  _alloc_failure_waiters_lock(Mutex::safepoint - 1, \"ShenandoahAllocFailureGC_lock\", true),\n+  _gc_waiters_lock(Mutex::safepoint - 1, \"ShenandoahRequestedGC_lock\", true),\n+  _control_lock(Mutex::nosafepoint - 1, \"ShenandoahControlGC_lock\", true),\n+  _regulator_lock(Mutex::nosafepoint - 1, \"ShenandoahRegulatorGC_lock\", true),\n@@ -55,0 +62,1 @@\n+  _requested_generation(GenerationMode::GLOBAL),\n@@ -56,1 +64,3 @@\n-  _allocs_seen(0) {\n+  _degen_generation(NULL),\n+  _allocs_seen(0),\n+  _mode(none) {\n@@ -84,0 +94,1 @@\n+  GenerationMode generation = GLOBAL;\n@@ -85,1 +96,0 @@\n-  int sleep = ShenandoahControlIntervalMin;\n@@ -88,1 +98,1 @@\n-  double last_sleep_adjust_time = os::elapsedTime();\n+  uint age_period = 0;\n@@ -97,1 +107,6 @@\n-  ShenandoahHeuristics* heuristics = heap->heuristics();\n+\n+  \/\/ Heuristics are notified of allocation failures here and other outcomes\n+  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n+  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n+  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n+  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n@@ -104,1 +119,1 @@\n-    bool implicit_gc_requested = is_gc_requested && !is_explicit_gc(requested_gc_cause);\n+    bool implicit_gc_requested = is_gc_requested && is_implicit_gc(requested_gc_cause);\n@@ -113,1 +128,1 @@\n-    GCMode mode = none;\n+    set_gc_mode(none);\n@@ -127,1 +142,12 @@\n-      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle()) {\n+      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n+        _degen_generation = heap->mode()->is_generational() ? heap->young_generation() : heap->global_generation();\n+      } else {\n+        assert(_degen_generation != NULL, \"Need to know which generation to resume.\");\n+      }\n+\n+      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n+      generation = _degen_generation->generation_mode();\n+      bool old_gen_evacuation_failed = heap->clear_old_evacuation_failure();\n+\n+      \/\/ Do not bother with degenerated cycle if old generation evacuation failed.\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() && !old_gen_evacuation_failed) {\n@@ -130,1 +156,1 @@\n-        mode = stw_degenerated;\n+        set_gc_mode(stw_degenerated);\n@@ -134,1 +160,2 @@\n-        mode = stw_full;\n+        generation = GLOBAL;\n+        set_gc_mode(stw_full);\n@@ -136,1 +163,0 @@\n-\n@@ -139,0 +165,1 @@\n+      generation = GLOBAL;\n@@ -141,1 +168,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -145,1 +172,1 @@\n-        mode = default_mode;\n+        set_gc_mode(default_mode);\n@@ -147,1 +174,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -150,1 +177,1 @@\n-        mode = stw_full;\n+        set_gc_mode(stw_full);\n@@ -154,0 +181,1 @@\n+      generation = GLOBAL;\n@@ -156,1 +184,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -160,1 +188,1 @@\n-        mode = default_mode;\n+        set_gc_mode(default_mode);\n@@ -163,1 +191,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -166,1 +194,1 @@\n-        mode = stw_full;\n+        set_gc_mode(stw_full);\n@@ -169,5 +197,16 @@\n-      \/\/ Potential normal cycle: ask heuristics if it wants to act\n-      if (heuristics->should_start_gc()) {\n-        mode = default_mode;\n-        cause = default_cause;\n-      }\n+      \/\/ We should only be here if the regulator requested a cycle or if\n+      \/\/ there is an old generation mark in progress.\n+      if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+        if (_requested_generation == OLD && heap->doing_mixed_evacuations()) {\n+          \/\/ If a request to start an old cycle arrived while an old cycle was running, but _before_\n+          \/\/ it chose any regions for evacuation we don't want to start a new old cycle. Rather, we want\n+          \/\/ the heuristic to run a young collection so that we can evacuate some old regions.\n+          assert(!heap->is_concurrent_old_mark_in_progress(), \"Should not be running mixed collections and concurrent marking.\");\n+          generation = YOUNG;\n+        } else {\n+          generation = _requested_generation;\n+        }\n+\n+        \/\/ preemption was requested or this is a regular cycle\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        set_gc_mode(default_mode);\n@@ -175,2 +214,27 @@\n-      \/\/ Ask policy if this cycle wants to process references or unload classes\n-      heap->set_unload_classes(heuristics->should_unload_classes());\n+        \/\/ Don't start a new old marking if there is one already in progress.\n+        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n+          set_gc_mode(servicing_old);\n+        }\n+\n+        if (generation == GLOBAL) {\n+          heap->set_unload_classes(global_heuristics->should_unload_classes());\n+        } else {\n+          heap->set_unload_classes(false);\n+        }\n+\n+        \/\/ Don't want to spin in this loop and start a cycle every time, so\n+        \/\/ clear requested gc cause. This creates a race with callers of the\n+        \/\/ blocking 'request_gc' method, but there it loops and resets the\n+        \/\/ '_requested_gc_cause' until a full cycle is completed.\n+        _requested_gc_cause = GCCause::_no_gc;\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n+        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n+        \/\/ mixed evacuation in progress, so resume working on that.\n+        log_info(gc)(\"Resume old gc: marking=%s, preparing=%s\",\n+                     BOOL_TO_STR(heap->is_concurrent_old_mark_in_progress()),\n+                     BOOL_TO_STR(heap->is_prepare_for_old_mark_in_progress()));\n+\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = OLD;\n+        set_gc_mode(servicing_old);\n+      }\n@@ -180,2 +244,2 @@\n-    \/\/ either implicit or explicit GC request,  or we are requested to do so unconditionally.\n-    if (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n+    \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+    if (generation == GLOBAL && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n@@ -185,1 +249,1 @@\n-    bool gc_requested = (mode != none);\n+    bool gc_requested = (_mode != none);\n@@ -206,12 +270,31 @@\n-      switch (mode) {\n-        case concurrent_normal:\n-          service_concurrent_normal_cycle(cause);\n-          break;\n-        case stw_degenerated:\n-          service_stw_degenerated_cycle(cause, degen_point);\n-          break;\n-        case stw_full:\n-          service_stw_full_cycle(cause);\n-          break;\n-        default:\n-          ShouldNotReachHere();\n+      heap->set_aging_cycle(false);\n+      {\n+        switch (_mode) {\n+          case concurrent_normal: {\n+            if ((generation == YOUNG) && (age_period-- == 0)) {\n+              heap->set_aging_cycle(true);\n+              age_period = ShenandoahAgingCyclePeriod - 1;\n+            }\n+            service_concurrent_normal_cycle(heap, generation, cause);\n+            break;\n+          }\n+          case stw_degenerated: {\n+            if (!service_stw_degenerated_cycle(cause, degen_point)) {\n+              \/\/ The degenerated GC was upgraded to a Full GC\n+              generation = GLOBAL;\n+            }\n+            break;\n+          }\n+          case stw_full: {\n+            service_stw_full_cycle(cause);\n+            break;\n+          }\n+          case servicing_old: {\n+            assert(generation == OLD, \"Expected old generation here\");\n+            service_concurrent_old_cycle(heap, cause);\n+            break;\n+          }\n+          default: {\n+            ShouldNotReachHere();\n+          }\n+        }\n@@ -255,1 +338,2 @@\n-        heuristics->clear_metaspace_oom();\n+        assert(generation == GLOBAL, \"Only unload classes during GLOBAL cycle\");\n+        global_heuristics->clear_metaspace_oom();\n@@ -258,21 +342,1 @@\n-      \/\/ Commit worker statistics to cycle data\n-      heap->phase_timings()->flush_par_workers_to_cycle();\n-      if (ShenandoahPacing) {\n-        heap->pacer()->flush_stats_to_cycle();\n-      }\n-\n-      \/\/ Print GC stats for current cycle\n-      {\n-        LogTarget(Info, gc, stats) lt;\n-        if (lt.is_enabled()) {\n-          ResourceMark rm;\n-          LogStream ls(lt);\n-          heap->phase_timings()->print_cycle_on(&ls);\n-          if (ShenandoahPacing) {\n-            heap->pacer()->print_cycle_on(&ls);\n-          }\n-        }\n-      }\n-\n-      \/\/ Commit statistics to globals\n-      heap->phase_timings()->flush_cycle_to_global();\n+      process_phase_timings(heap);\n@@ -314,8 +378,6 @@\n-    \/\/ Wait before performing the next action. If allocation happened during this wait,\n-    \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n-    \/\/ back off exponentially.\n-    if (_heap_changed.try_unset()) {\n-      sleep = ShenandoahControlIntervalMin;\n-    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n-      sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));\n-      last_sleep_adjust_time = current;\n+    \/\/ Don't wait around if there was an allocation failure - start the next cycle immediately.\n+    if (!is_alloc_failure_gc()) {\n+      \/\/ The timed wait is necessary because this thread has a responsibility to send\n+      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      lock.wait(ShenandoahControlIntervalMax);\n@@ -323,1 +385,0 @@\n-    os::naked_short_sleep(sleep);\n@@ -332,0 +393,186 @@\n+void ShenandoahControlThread::process_phase_timings(const ShenandoahHeap* heap) {\n+\n+  \/\/ Commit worker statistics to cycle data\n+  heap->phase_timings()->flush_par_workers_to_cycle();\n+  if (ShenandoahPacing) {\n+    heap->pacer()->flush_stats_to_cycle();\n+  }\n+\n+  \/\/ Print GC stats for current cycle\n+  {\n+    LogTarget(Info, gc, stats) lt;\n+    if (lt.is_enabled()) {\n+      ResourceMark rm;\n+      LogStream ls(lt);\n+      heap->phase_timings()->print_cycle_on(&ls);\n+      if (ShenandoahPacing) {\n+        heap->pacer()->print_cycle_on(&ls);\n+      }\n+    }\n+  }\n+\n+  \/\/ Commit statistics to globals\n+  heap->phase_timings()->flush_cycle_to_global();\n+}\n+\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old cycles always degenerate to a global cycle.\n+\/\/ Young cycles are degenerated to complete the young cycle.  Young\n+\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n+\/\/ triggered directly by a System.gc() invocation.\n+\/\/\n+\/\/\n+\/\/      +-----+ Idle +-----+-----------+---------------------+\n+\/\/      |         +        |           |                     |\n+\/\/      |         |        |           |                     |\n+\/\/      |         |        v           |                     |\n+\/\/      |         |  Bootstrap Old +-- | ------------+       |\n+\/\/      |         |   +                |             |       |\n+\/\/      |         |   |                |             |       |\n+\/\/      |         v   v                v             v       |\n+\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n+\/\/      |     +  +   ^                            +  +       |\n+\/\/      v     |  |   |                            |  |       |\n+\/\/   Global <-+  |   +----------------------------+  |       |\n+\/\/      +        |                                   |       |\n+\/\/      |        v                                   v       |\n+\/\/      +--->  Global Degen +--------------------> Full <----+\n+\/\/\n+void ShenandoahControlThread::service_concurrent_normal_cycle(\n+  const ShenandoahHeap* heap, const GenerationMode generation, GCCause::Cause cause) {\n+\n+  switch (generation) {\n+    case YOUNG: {\n+      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n+      \/\/ concurrent mark in the old generation. We need to think about promotions\n+      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n+      \/\/ they end up in, but we have to be sure we don't promote into any regions\n+      \/\/ that are in the cset.\n+      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n+      service_concurrent_cycle(heap->young_generation(), cause, false);\n+      heap->young_generation()->log_status();\n+      break;\n+    }\n+    case GLOBAL: {\n+      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n+      service_concurrent_cycle(heap->global_generation(), cause, false);\n+      heap->global_generation()->log_status();\n+      break;\n+    }\n+    case OLD: {\n+      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n+      service_concurrent_old_cycle(heap, cause);\n+      heap->old_generation()->log_status();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void ShenandoahControlThread::service_concurrent_old_cycle(const ShenandoahHeap* heap, GCCause::Cause &cause) {\n+\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+\n+  GCIdMark gc_id_mark;\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  switch (old_generation->state()) {\n+    case ShenandoahOldGeneration::IDLE: {\n+      assert(!heap->is_concurrent_old_mark_in_progress(), \"Old already in progress.\");\n+      assert(old_generation->task_queues()->is_empty(), \"Old mark queues should be empty.\");\n+    }\n+    case ShenandoahOldGeneration::FILLING: {\n+      _allow_old_preemption.set();\n+      ShenandoahGCSession session(cause, old_generation);\n+      old_generation->prepare_gc();\n+      _allow_old_preemption.unset();\n+\n+      if (heap->is_prepare_for_old_mark_in_progress()) {\n+        assert(old_generation->state() == ShenandoahOldGeneration::FILLING, \"Prepare for mark should be in progress.\");\n+        return;\n+      }\n+\n+      assert(old_generation->state() == ShenandoahOldGeneration::BOOTSTRAPPING, \"Finished with filling, should be bootstrapping.\");\n+    }\n+    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      ShenandoahGCSession session(cause, young_generation);\n+      service_concurrent_cycle(heap,young_generation, cause, true);\n+      process_phase_timings(heap);\n+      if (heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled.\");\n+        return;\n+      }\n+\n+      \/\/ Reset the degenerated point. Normally this would happen at the top\n+      \/\/ of the control loop, but here we have just completed a young cycle\n+      \/\/ which has bootstrapped the old concurrent marking.\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle. In order to simplify the debugging\n+      \/\/ effort, the old cycle will ONLY complete the mark phase. No actual\n+      \/\/ collection of the old generation is happening here.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      ShenandoahGCSession session(cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking.\");\n+      }\n+      break;\n+    }\n+    default:\n+      log_error(gc)(\"Unexpected state for old GC: %d\", old_generation->state());\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+bool ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued.\", generation->task_queues()->tasks());\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n+  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n+  \/\/ is allowed to cancel a GC.\n+  ShenandoahOldGC gc(generation, _allow_old_preemption);\n+  if (gc.collect(cause)) {\n+    generation->record_success_concurrent(false);\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    \/\/ It's possible the gc cycle was cancelled after the last time\n+    \/\/ the collection checked for cancellation. In which case, the\n+    \/\/ old gc cycle is still completed, and we have to deal with this\n+    \/\/ cancellation. We set the degeneration point to be outside\n+    \/\/ the cycle because if this is an allocation failure, that is\n+    \/\/ what must be done (there is no degenerated old cycle). If the\n+    \/\/ cancellation was due to a heuristic wanting to start a young\n+    \/\/ cycle, then we are not actually going to a degenerated cycle,\n+    \/\/ so the degenerated point doesn't matter here.\n+    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n+    if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+      heap->shenandoah_policy()->record_interrupted_old();\n+    }\n+    return false;\n+  }\n+  return true;\n+}\n+\n@@ -351,1 +598,1 @@\n-void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n@@ -387,1 +634,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -390,0 +636,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -391,2 +638,1 @@\n-  ShenandoahGCSession session(cause);\n-\n+  ShenandoahGCSession session(cause, generation);\n@@ -395,1 +641,6 @@\n-  ShenandoahConcurrentGC gc;\n+  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n+}\n+\n+void ShenandoahControlThread::service_concurrent_cycle(const ShenandoahHeap* heap, ShenandoahGeneration* generation,\n+                                                       GCCause::Cause &cause, bool do_old_gc_bootstrap) {\n+  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n@@ -398,2 +649,1 @@\n-    heap->heuristics()->record_success_concurrent();\n-    heap->shenandoah_policy()->record_success_concurrent();\n+    generation->record_success_concurrent(gc.abbreviated());\n@@ -403,0 +653,4 @@\n+    assert(generation->generation_mode() != OLD, \"Old GC takes a different control path\");\n+    \/\/ Concurrent young-gen collection degenerates to young\n+    \/\/ collection.  Same for global collections.\n+    _degen_generation = generation;\n@@ -408,7 +662,13 @@\n-  if (heap->cancelled_gc()) {\n-    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n-    if (!in_graceful_shutdown()) {\n-      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n-      _degen_point = point;\n-    }\n+  if (!heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (in_graceful_shutdown()) {\n+    return true;\n+  }\n+\n+  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+\n+  if (is_alloc_failure_gc()) {\n+    _degen_point = point;\n@@ -417,0 +677,16 @@\n+\n+  if (_preemption_requested.is_set()) {\n+    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n+    _preemption_requested.unset();\n+\n+    \/\/ Old generation marking is only cancellable during concurrent marking.\n+    \/\/ Once final mark is complete, the code does not check again for cancellation.\n+    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n+    \/\/ make it to this case. The calling code is responsible for forcing a\n+    \/\/ cancellation due to allocation failure into a degenerated cycle.\n+    _degen_point = point;\n+    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n+    return true;\n+  }\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking.\");\n@@ -425,0 +701,2 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n@@ -426,1 +704,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -431,2 +709,1 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_full();\n+  heap->global_generation()->heuristics()->record_success_full();\n@@ -436,1 +713,1 @@\n-void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n+bool ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n@@ -438,0 +715,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -440,1 +718,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, _degen_generation);\n@@ -442,1 +720,1 @@\n-  ShenandoahDegenGC gc(point);\n+  ShenandoahDegenGC gc(point, _degen_generation);\n@@ -445,2 +723,13 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_degenerated();\n+  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  if (_degen_generation->generation_mode() == GLOBAL) {\n+    assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+    assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+  } else {\n+    assert(_degen_generation->generation_mode() == YOUNG, \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old_generation = (ShenandoahOldGeneration*) heap->old_generation();\n+    if (old_generation->state() == ShenandoahOldGeneration::BOOTSTRAPPING && !gc.upgraded_to_full()) {\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+  }\n+\n+  _degen_generation->heuristics()->record_success_degenerated();\n@@ -448,0 +737,1 @@\n+  return !gc.upgraded_to_full();\n@@ -478,0 +768,4 @@\n+bool ShenandoahControlThread::is_implicit_gc(GCCause::Cause cause) const {\n+  return !is_explicit_gc(cause) && cause != GCCause::_shenandoah_concurrent_gc;\n+}\n+\n@@ -499,0 +793,40 @@\n+bool ShenandoahControlThread::request_concurrent_gc(GenerationMode generation) {\n+  if (_preemption_requested.is_set() || _gc_requested.is_set() || ShenandoahHeap::heap()->cancelled_gc()) {\n+    \/\/ ignore subsequent requests from the heuristics\n+    return false;\n+  }\n+\n+  if (_mode == none) {\n+    _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+    _requested_generation = generation;\n+    notify_control_thread();\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    ml.wait();\n+    return true;\n+  }\n+\n+  if (preempt_old_marking(generation)) {\n+    log_info(gc)(\"Preempting old generation mark to allow %s GC.\", generation_name(generation));\n+    _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+    _requested_generation = generation;\n+    _preemption_requested.set();\n+    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+    notify_control_thread();\n+\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    ml.wait();\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void ShenandoahControlThread::notify_control_thread() {\n+  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _control_lock.notify();\n+}\n+\n+bool ShenandoahControlThread::preempt_old_marking(GenerationMode generation) {\n+  return generation == YOUNG && _allow_old_preemption.try_unset();\n+}\n+\n@@ -518,1 +852,1 @@\n-\n+    notify_control_thread();\n@@ -602,4 +936,0 @@\n-  \/\/ Notify that something had changed.\n-  if (_heap_changed.is_unset()) {\n-    _heap_changed.set();\n-  }\n@@ -640,0 +970,20 @@\n+\n+const char* ShenandoahControlThread::gc_mode_name(ShenandoahControlThread::GCMode mode) {\n+  switch (mode) {\n+    case none:              return \"idle\";\n+    case concurrent_normal: return \"normal\";\n+    case stw_degenerated:   return \"degenerated\";\n+    case stw_full:          return \"full\";\n+    case servicing_old:     return \"old\";\n+    default:                return \"unknown\";\n+  }\n+}\n+\n+void ShenandoahControlThread::set_gc_mode(ShenandoahControlThread::GCMode new_mode) {\n+  if (_mode != new_mode) {\n+    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n+    _mode = new_mode;\n+    MonitorLocker ml(&_regulator_lock, Mutex::_no_safepoint_check_flag);\n+    ml.notify_all();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":450,"deletions":100,"binary":false,"changes":550,"status":"modified"},{"patch":"@@ -60,7 +60,0 @@\n-  typedef enum {\n-    none,\n-    concurrent_normal,\n-    stw_degenerated,\n-    stw_full\n-  } GCMode;\n-\n@@ -69,1 +62,1 @@\n-  \/\/ to make complete explicit cycle for for demanding customers.\n+  \/\/ to make complete explicit cycle for demanding customers.\n@@ -72,0 +65,2 @@\n+  Monitor _control_lock;\n+  Monitor _regulator_lock;\n@@ -76,0 +71,8 @@\n+  typedef enum {\n+    none,\n+    concurrent_normal,\n+    stw_degenerated,\n+    stw_full,\n+    servicing_old\n+  } GCMode;\n+\n@@ -79,0 +82,2 @@\n+  size_t get_gc_id();\n+\n@@ -80,0 +85,2 @@\n+  ShenandoahSharedFlag _allow_old_preemption;\n+  ShenandoahSharedFlag _preemption_requested;\n@@ -83,1 +90,0 @@\n-  ShenandoahSharedFlag _heap_changed;\n@@ -87,0 +93,1 @@\n+  GenerationMode       _requested_generation;\n@@ -88,0 +95,1 @@\n+  ShenandoahGeneration* _degen_generation;\n@@ -94,0 +102,2 @@\n+  volatile GCMode _mode;\n+  shenandoah_padding(3);\n@@ -95,0 +105,1 @@\n+  \/\/ Returns true if the cycle has been cancelled or degenerated.\n@@ -96,1 +107,4 @@\n-  void service_concurrent_normal_cycle(GCCause::Cause cause);\n+\n+  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n+  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool reset_old_bitmap_specially);\n@@ -98,1 +112,4 @@\n-  void service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point);\n+\n+  \/\/ Return true if degenerated cycle finishes normally.  Return false if the degenerated cycle transformed itself\n+  \/\/ into a full GC.\n+  bool service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point);\n@@ -101,0 +118,1 @@\n+  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n@@ -102,0 +120,1 @@\n+  \/\/ Notify threads waiting for GC to complete.\n@@ -103,0 +122,1 @@\n+  \/\/ True if allocation failure flag has been set.\n@@ -107,1 +127,0 @@\n-  size_t get_gc_id();\n@@ -116,0 +135,4 @@\n+  bool is_implicit_gc(GCCause::Cause cause) const;\n+\n+  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n+  bool preempt_old_marking(GenerationMode generation);\n@@ -117,0 +140,1 @@\n+  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n@@ -119,0 +143,2 @@\n+  void process_phase_timings(const ShenandoahHeap* heap);\n+\n@@ -133,0 +159,2 @@\n+  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n+  bool request_concurrent_gc(GenerationMode generation);\n@@ -145,0 +173,19 @@\n+\n+  void service_concurrent_normal_cycle(const ShenandoahHeap* heap,\n+                                       const GenerationMode generation,\n+                                       GCCause::Cause cause);\n+\n+  void service_concurrent_old_cycle(const ShenandoahHeap* heap,\n+                                    GCCause::Cause &cause);\n+\n+  void set_gc_mode(GCMode new_mode);\n+  GCMode gc_mode() {\n+    return _mode;\n+  }\n+\n+ private:\n+  static const char* gc_mode_name(GCMode mode);\n+  void notify_control_thread();\n+\n+  void service_concurrent_cycle(const ShenandoahHeap* heap, ShenandoahGeneration* generation, GCCause::Cause &cause,\n+                                bool do_old_gc_bootstrap);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":59,"deletions":12,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -45,0 +47,1 @@\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -54,0 +57,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -60,0 +64,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -68,0 +73,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -71,0 +78,1 @@\n+\n@@ -75,0 +83,2 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+\n@@ -162,3 +172,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -180,0 +187,4 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_generations();\n+  initialize_heuristics();\n+\n@@ -214,0 +225,25 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize);\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == NULL) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -257,1 +293,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -403,0 +439,1 @@\n+  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n@@ -409,1 +446,12 @@\n-void ShenandoahHeap::initialize_mode() {\n+void ShenandoahHeap::initialize_generations() {\n+  size_t max_capacity_new      = young_generation_capacity(max_capacity());\n+  size_t soft_max_capacity_new = young_generation_capacity(soft_max_capacity());\n+  size_t max_capacity_old      = max_capacity() - max_capacity_new;\n+  size_t soft_max_capacity_old = soft_max_capacity() - soft_max_capacity_new;\n+\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_new, soft_max_capacity_new);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, soft_max_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_max_workers);\n+}\n+\n+void ShenandoahHeap::initialize_heuristics() {\n@@ -417,0 +465,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -434,4 +484,4 @@\n-}\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != NULL, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  if (mode()->is_generational()) {\n+    _young_generation->initialize_heuristics(_gc_mode);\n+    _old_generation->initialize_heuristics(_gc_mode);\n@@ -440,9 +490,1 @@\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n+    ShenandoahEvacWaste = ShenandoahGenerationalEvacWaste;\n@@ -459,0 +501,2 @@\n+  _gc_generation(NULL),\n+  _prepare_for_old_mark(false),\n@@ -462,2 +506,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -470,0 +513,11 @@\n+  _alloc_supplement_reserve(0),\n+  _promoted_reserve(0),\n+  _old_evac_reserve(0),\n+  _old_evac_expended(0),\n+  _young_evac_reserve(0),\n+  _captured_old_usage(0),\n+  _previous_promotion(0),\n+  _cancel_requested_time(0),\n+  _young_generation(NULL),\n+  _global_generation(NULL),\n+  _old_generation(NULL),\n@@ -471,0 +525,1 @@\n+  _regulator_thread(NULL),\n@@ -472,2 +527,0 @@\n-  _gc_mode(NULL),\n-  _heuristics(NULL),\n@@ -480,0 +533,2 @@\n+  _young_gen_memory_pool(NULL),\n+  _old_gen_memory_pool(NULL),\n@@ -485,1 +540,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -493,1 +547,2 @@\n-  _collection_set(NULL)\n+  _collection_set(NULL),\n+  _card_scan(NULL)\n@@ -495,17 +550,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == NULL) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -518,29 +556,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != NULL) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -561,1 +570,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -624,2 +634,0 @@\n-  _heuristics->initialize();\n-\n@@ -629,0 +637,48 @@\n+\n+ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n+  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n+}\n+\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return old_heuristics()->unprocessed_old_collection_candidates() > 0;\n+}\n+\n+bool ShenandoahHeap::is_old_bitmap_stable() const {\n+  ShenandoahOldGeneration::State state = _old_generation->state();\n+  return state != ShenandoahOldGeneration::MARKING\n+      && state != ShenandoahOldGeneration::BOOTSTRAPPING;\n+}\n+\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != NULL && _gc_generation->generation_mode() == YOUNG;\n+}\n+\n+\/\/ There are three JVM parameters for setting young gen capacity:\n+\/\/    NewSize, MaxNewSize, NewRatio.\n+\/\/\n+\/\/ If only NewSize is set, it assigns a fixed size and the other two parameters are ignored.\n+\/\/ Otherwise NewRatio applies.\n+\/\/\n+\/\/ If NewSize is set in any combination, it provides a lower bound.\n+\/\/\n+\/\/ If MaxNewSize is set it provides an upper bound.\n+\/\/ If this bound is smaller than NewSize, it supersedes,\n+\/\/ resulting in a fixed size given by MaxNewSize.\n+size_t ShenandoahHeap::young_generation_capacity(size_t capacity) {\n+  if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+    if (FLAG_IS_CMDLINE(NewSize) && !FLAG_IS_CMDLINE(MaxNewSize) && !FLAG_IS_CMDLINE(NewRatio)) {\n+      capacity = MIN2(NewSize, capacity);\n+    } else {\n+      capacity \/= NewRatio + 1;\n+      if (FLAG_IS_CMDLINE(NewSize)) {\n+        capacity = MAX2(NewSize, capacity);\n+      }\n+      if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+        capacity = MIN2(MaxNewSize, capacity);\n+      }\n+    }\n+  }\n+  \/\/ else, make no adjustment to global capacity\n+  return capacity;\n+}\n+\n@@ -660,4 +716,0 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n-}\n-\n@@ -669,1 +721,1 @@\n-  increase_allocated(bytes);\n+\n@@ -699,0 +751,7 @@\n+\n+  if (mode()->is_generational()) {\n+    size_t soft_max_capacity_young = young_generation_capacity(_soft_max_size);\n+    size_t soft_max_capacity_old = _soft_max_size - soft_max_capacity_young;\n+    _young_generation->set_soft_max_capacity(soft_max_capacity_young);\n+    _old_generation->set_soft_max_capacity(soft_max_capacity_old);\n+  }\n@@ -715,0 +774,23 @@\n+bool ShenandoahHeap::is_in_young(const void* p) const {\n+  return is_in(p) && heap_region_containing(p)->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION;\n+}\n+\n+bool ShenandoahHeap::is_in_old(const void* p) const {\n+  return is_in(p) && heap_region_containing(p)->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION;\n+}\n+\n+bool ShenandoahHeap::is_in_active_generation(oop obj) const {\n+  if (!mode()->is_generational()) {\n+    \/\/ everything is the same single generation\n+    return true;\n+  }\n+\n+  if (active_generation() == NULL) {\n+    \/\/ no collection is happening, only expect this to be called\n+    \/\/ when concurrent processing is active, but that could change\n+    return false;\n+  }\n+\n+  return active_generation()->contains(obj);\n+}\n+\n@@ -742,0 +824,23 @@\n+    regulator_thread()->notify_heap_changed();\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  card_scan()->register_object_wo_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  card_scan()->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    old_generation()->increase_allocated(words * HeapWordSize);\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation_failure() {\n+  if (_old_gen_oom_evac.try_set()) {\n+    log_info(gc)(\"Old gen evac failure.\");\n@@ -745,0 +850,4 @@\n+void ShenandoahHeap::handle_promotion_failure() {\n+  old_heuristics()->handle_promotion_failure();\n+}\n+\n@@ -751,0 +860,6 @@\n+\n+  \/\/ Limit growth of GCLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n@@ -794,0 +909,170 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+  size_t min_size = MAX2(size, PLAB::min_size());\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = PLAB::min_size();\n+  }\n+  size_t future_size = cur_size * 2;\n+  \/\/ Limit growth of PLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    future_size = MIN2(future_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+  future_size = MIN2(future_size, PLAB::max_size());\n+  future_size = MAX2(future_size, PLAB::min_size());\n+\n+  size_t unalignment = future_size % CardTable::card_size_in_words();\n+  if (unalignment != 0) {\n+    future_size = future_size - unalignment + CardTable::card_size_in_words();\n+  }\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < PLAB::min_size()) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == NULL) {\n+      return NULL;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    assert (size <= actual_size, \"allocation should fit\");\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    plab->set_buf(plab_buf, actual_size);\n+\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+    ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  if (not_promoted > 0) {\n+    unexpend_promoted(not_promoted);\n+  }\n+  size_t waste = plab->waste();\n+  HeapWord* top = plab->top();\n+  plab->retire();\n+  if (top != NULL && plab->waste() > waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we\n+    \/\/ need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - waste, p2i(top));\n+    card_scan()->register_object_wo_lock(top);\n+  }\n+}\n+\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n+void ShenandoahHeap::cancel_old_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(_old_generation != NULL, \"Should only have mixed collections in generation mode.\");\n+  log_info(gc)(\"Terminating old gc cycle.\");\n+\n+  \/\/ Stop marking\n+  old_generation()->cancel_marking();\n+  \/\/ Stop coalescing undead objects\n+  set_prepare_for_old_mark_in_progress(false);\n+  \/\/ Stop tracking old regions\n+  old_heuristics()->abandon_collection_candidates();\n+  \/\/ Remove old generation access to young generation mark queues\n+  young_generation()->set_old_gen_task_queues(nullptr);\n+  \/\/ Transition to IDLE now.\n+  _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+}\n+\n+bool ShenandoahHeap::is_old_gc_active() {\n+  return is_concurrent_old_mark_in_progress()\n+         || is_prepare_for_old_mark_in_progress()\n+         || old_heuristics()->unprocessed_old_collection_candidates() > 0\n+         || young_generation()->old_gen_task_queues() != nullptr;\n+}\n+\n+void ShenandoahHeap::coalesce_and_fill_old_regions() {\n+  class ShenandoahGlobalCoalesceAndFill : public ShenandoahHeapRegionClosure {\n+   public:\n+    virtual void heap_region_do(ShenandoahHeapRegion* region) override {\n+      \/\/ old region is not in the collection set and was not immediately trashed\n+      if (region->is_old() && region->is_active() && !region->is_humongous()) {\n+        \/\/ Reset the coalesce and fill boundary because this is a global collect\n+        \/\/ and cannot be preempted by young collects. We want to be sure the entire\n+        \/\/ region is coalesced here and does not resume from a previously interrupted\n+        \/\/ or completed coalescing.\n+        region->begin_preemptible_coalesce_and_fill();\n+        region->oop_fill_and_coalesce();\n+      }\n+    }\n+\n+    virtual bool is_thread_safe() override {\n+      return true;\n+    }\n+  };\n+  ShenandoahGlobalCoalesceAndFill coalesce;\n+  parallel_heap_region_iterate(&coalesce);\n+}\n+\n@@ -798,1 +1083,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -811,1 +1096,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -820,1 +1105,18 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size,\n+                                            size_t word_size,\n+                                            size_t* actual_size) {\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of exceeding the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req, false);\n+  if (res != NULL) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  return res;\n+}\n+\n+\/\/ is_promotion is true iff this allocation is known for sure to hold the result of young-gen evacuation\n+\/\/ to old-gen.  plab allocates arre not known as such, since they may hold old-gen evacuations.\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n@@ -832,1 +1134,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -850,1 +1152,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -856,1 +1158,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -861,1 +1163,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -868,0 +1170,1 @@\n+    regulator_thread()->notify_heap_changed();\n@@ -871,0 +1174,1 @@\n+    ShenandoahGeneration* alloc_generation = generation_for(req.affiliation());\n@@ -873,0 +1177,1 @@\n+    size_t actual_bytes = actual * HeapWordSize;\n@@ -880,0 +1185,1 @@\n+      alloc_generation->increase_allocated(actual_bytes);\n@@ -888,1 +1194,1 @@\n-      increase_used(actual*HeapWordSize);\n+      increase_used(actual_bytes);\n@@ -895,1 +1201,7 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+  \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+  bool promotion_eligible = false;\n+  bool allow_allocation = true;\n+  bool plab_alloc = false;\n+  size_t requested_bytes = req.size() * HeapWordSize;\n+  HeapWord* result = nullptr;\n@@ -897,1 +1209,103 @@\n-  return _free_set->allocate(req, in_new_region);\n+  Thread* thread = Thread::current();\n+  if (mode()->is_generational()) {\n+    if (req.affiliation() == YOUNG_GENERATION) {\n+      if (req.is_mutator_alloc()) {\n+        if (requested_bytes >= young_generation()->adjusted_available()) {\n+          \/\/ We know this is not a GCLAB.  This must be a TLAB or a shared allocation.  Reject the allocation request if\n+          \/\/ exceeds established capacity limits.\n+          return nullptr;\n+        }\n+      }\n+    } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+      assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+      if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+        plab_alloc = true;\n+        size_t promotion_avail = get_promoted_reserve();\n+        size_t promotion_expended = get_promoted_expended();\n+        if (promotion_expended + requested_bytes > promotion_avail) {\n+          promotion_avail = 0;\n+          if (get_old_evac_reserve() == 0) {\n+            \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+            \/\/ be used for promotions.\n+            allow_allocation = false;\n+          }\n+        } else {\n+          promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          promotion_eligible = true;\n+        }\n+      } else if (is_promotion) {\n+        \/\/ This is a shared alloc for promotion\n+        size_t promotion_avail = get_promoted_reserve();\n+        size_t promotion_expended = get_promoted_expended();\n+        if (promotion_expended + requested_bytes > promotion_avail) {\n+          promotion_avail = 0;\n+        } else {\n+          promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+        }\n+\n+        if (promotion_avail == 0) {\n+          \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+          \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+          return nullptr;\n+        }\n+        \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+      } else {\n+        \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n+      }\n+    }\n+  }\n+  result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+  if (result != NULL) {\n+    if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+      ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+      if (req.is_gc_alloc()) {\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          if (promotion_eligible) {\n+            size_t actual_size = req.actual_size() * HeapWordSize;\n+            \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+            \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+            ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+            expend_promoted(actual_size);\n+            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+            ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+          } else {\n+            \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+            ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+            ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+          }\n+        } else if (is_promotion) {\n+          \/\/ Shared promotion.  Assume size is requested_bytes.\n+          expend_promoted(requested_bytes);\n+          assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+        }\n+      }\n+\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+      \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+      \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+      \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+      \/\/ invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      ShenandoahHeap::heap()->card_scan()->register_object(result);\n+    }\n+  } else {\n+    \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+    if ((req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && req.is_gc_alloc() &&\n+        (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+      \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+      \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+      ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+    }\n+  }\n+  return result;\n@@ -903,1 +1317,1 @@\n-  return allocate_memory(req);\n+  return allocate_memory(req, false);\n@@ -912,2 +1326,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -992,0 +1406,1 @@\n+\n@@ -997,0 +1412,60 @@\n+      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n+        break;\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* const _sh;\n+  ShenandoahRegionIterator *_regions;\n+  bool _concurrent;\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent) :\n+    WorkerTask(\"Shenandoah Evacuation\"),\n+    _sh(sh),\n+    _regions(iterator),\n+    _concurrent(concurrent)\n+  {}\n+\n+  void work(uint worker_id) {\n+    if (_concurrent) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    }\n+  }\n+\n+private:\n+  void do_work() {\n+    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n+    ShenandoahHeapRegion* r;\n+    while ((r = _regions->next()) != nullptr) {\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s]\",\n+                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+                    r->is_active()? \"active\": \"inactive\",\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\");\n+      if (r->is_cset()) {\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+        _sh->marked_object_iterate(r, &cl);\n+        if (ShenandoahPacing) {\n+          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+        }\n+      } else if (r->is_young() && r->is_active() && r->is_humongous_start() && (r->age() > InitialTenuringThreshold)) {\n+        \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+        \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+        \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+        r->promote_humongous();\n+      }\n+      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+      \/\/ or is young humongous_start that is too young to be promoted\n@@ -1006,2 +1481,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1034,1 +1515,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1054,0 +1535,1 @@\n+  return required_regions;\n@@ -1063,0 +1545,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != NULL, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1078,0 +1564,11 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != NULL, \"PLAB should be initialized for %s\", thread->name());\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parseable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n@@ -1135,0 +1632,31 @@\n+class ShenandoahTagGCLABClosure : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) {\n+    PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);\n+    assert(gclab != NULL, \"GCLAB should be initialized for %s\", thread->name());\n+    if (gclab->words_remaining() > 0) {\n+      ShenandoahHeapRegion* r = ShenandoahHeap::heap()->heap_region_containing(gclab->allocate(0));\n+      r->set_young_lab_flag();\n+    }\n+  }\n+};\n+\n+void ShenandoahHeap::set_young_lab_region_flags() {\n+  if (!UseTLAB) {\n+    return;\n+  }\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    _regions[i]->clear_young_lab_flags();\n+  }\n+  ShenandoahTagGCLABClosure cl;\n+  workers()->threads_do(&cl);\n+  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n+    cl.do_thread(t);\n+    ThreadLocalAllocBuffer& tlab = t->tlab();\n+    if (tlab.end() != NULL) {\n+      ShenandoahHeapRegion* r = heap_region_containing(tlab.start());\n+      r->set_young_lab_flag();\n+    }\n+  }\n+}\n+\n@@ -1532,23 +2060,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1570,99 +2075,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1679,1 +2085,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1710,4 +2116,44 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state_mask(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(YOUNG_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(YOUNG_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(OLD_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(OLD_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_prepare_for_old_mark_in_progress(bool in_progress) {\n+  \/\/ Unlike other set-gc-state functions, this may happen outside safepoint.\n+  \/\/ Is only set and queried by control thread, so no coherence issues.\n+  _prepare_for_old_mark = in_progress;\n+}\n+\n+void ShenandoahHeap::set_aging_cycle(bool in_progress) {\n+  _is_aging_cycle.set_cond(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1758,0 +2204,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1763,0 +2217,4 @@\n+    _cancel_requested_time = os::elapsedTime();\n+    if (cause == GCCause::_shenandoah_upgrade_to_full_gc) {\n+      _upgraded_to_full = true;\n+    }\n@@ -1773,0 +2231,3 @@\n+  \/\/ Step 0a. Stop requesting collections.\n+  regulator_thread()->stop();\n+\n@@ -1878,4 +2339,0 @@\n-address ShenandoahHeap::cancelled_gc_addr() {\n-  return (address) ShenandoahHeap::heap()->_cancelled_gc.addr_of();\n-}\n-\n@@ -1886,5 +2343,6 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -1956,2 +2414,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2017,0 +2477,2 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n@@ -2018,1 +2480,2 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -2021,1 +2484,3 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n+  {\n@@ -2028,1 +2493,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2031,1 +2496,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2037,1 +2502,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2040,1 +2505,4 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2044,0 +2512,3 @@\n+\n+      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n@@ -2045,1 +2516,31 @@\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (!_heap->mode()->is_generational() || (r->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION)) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          if (_heap->active_generation()->generation_mode() == GLOBAL) {\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 affiliation_name(r->affiliation()), r->index());\n+        }\n@@ -2047,1 +2548,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2055,0 +2556,119 @@\n+    if (_heap->mode()->is_generational() && (_heap->active_generation()->generation_mode() != GLOBAL)) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      bool have_work = _work_chunks->next(&assignment);\n+      RememberedScanner* scanner = _heap->card_scan();\n+      while (have_work) {\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION)) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true, CONCURRENT);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, CONCURRENT);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+        \/\/ Otherwise, this work chunk had nothing for me to do, so do not report pacer progress.\n+\n+        \/\/ Before we take responsibility for another chunk of work, see if cancellation is requested.\n+        if (_heap->check_cancelled_gc_and_yield(CONCURRENT)) {\n+          return;\n+        }\n+        have_work = _work_chunks->next(&assignment);\n+      }\n+    }\n@@ -2060,0 +2680,1 @@\n+  ShenandoahRegionChunkIterator work_list(workers()->active_workers());\n@@ -2062,1 +2683,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2065,1 +2686,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2070,1 +2691,0 @@\n-\n@@ -2073,0 +2693,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2074,0 +2695,1 @@\n+  bool _is_generational;\n@@ -2076,1 +2698,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n+    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -2079,0 +2703,21 @@\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+\n@@ -2081,1 +2726,0 @@\n-\n@@ -2108,1 +2752,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (active_generation()->complete_marking_context());\n@@ -2242,3 +2886,12 @@\n-  _memory_pool = new ShenandoahMemoryPool(this);\n-  _cycle_memory_manager.add_pool(_memory_pool);\n-  _stw_memory_manager.add_pool(_memory_pool);\n+  if (mode()->is_generational()) {\n+    _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+    _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_young_gen_memory_pool);\n+    _cycle_memory_manager.add_pool(_old_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_young_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_old_gen_memory_pool);\n+  } else {\n+    _memory_pool = new ShenandoahMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_memory_pool);\n+    _stw_memory_manager.add_pool(_memory_pool);\n+  }\n@@ -2256,1 +2909,6 @@\n-  memory_pools.append(_memory_pool);\n+  if (mode()->is_generational()) {\n+    memory_pools.append(_young_gen_memory_pool);\n+    memory_pools.append(_old_gen_memory_pool);\n+  } else {\n+    memory_pools.append(_memory_pool);\n+  }\n@@ -2261,1 +2919,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2299,0 +2957,1 @@\n+\n@@ -2326,0 +2985,204 @@\n+\n+void ShenandoahHeap::transfer_old_pointers_from_satb() {\n+  _old_generation->transfer_pointers_from_satb();\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<YOUNG>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit young and free regions\n+  if (region->affiliation() != OLD_GENERATION) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<OLD>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit old and free regions\n+  if (region->affiliation() != YOUNG_GENERATION) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.\n+\/\/ This examines the read_card_table between bottom() and top() since all PLABS are retired\n+\/\/ before the safepoint for init_mark.  Actually, we retire them before update-references and don't\n+\/\/ restore them until the start of evacuation.\n+void ShenandoahHeap::verify_rem_set_at_mark() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(true);\n+  ShenandoahMarkingContext* ctx;\n+\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", doing_mixed_evacuations()? \"mixed\": \"young\");\n+\n+  if (is_old_bitmap_stable() || active_generation()->generation_mode() == GLOBAL) {\n+    ctx = complete_marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && r->is_active()) {\n+      HeapWord* obj_addr = r->bottom();\n+      if (r->is_humongous_start()) {\n+        oop obj = cast_to_oop(obj_addr);\n+        if (!ctx || ctx->is_marked(obj)) {\n+          \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+          \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+          \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+          if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+            obj->oop_iterate(&check_interesting_pointers);\n+          }\n+          \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+        }\n+        \/\/ else, this humongous object is not marked so no need to verify its internal pointers\n+        if (!scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                          \"Verify init-mark remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+      } else if (!r->is_humongous()) {\n+        HeapWord* top = r->top();\n+        while (obj_addr < top) {\n+          oop obj = cast_to_oop(obj_addr);\n+          \/\/ ctx->is_marked() returns true if mark bit set (TAMS not relevant during init mark)\n+          if (!ctx || ctx->is_marked(obj)) {\n+            \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+            \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+            if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+              obj->oop_iterate(&check_interesting_pointers);\n+            }\n+            \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+            if (!scanner->verify_registration(obj_addr, ctx)) {\n+              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                            \"Verify init-mark remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+            }\n+            obj_addr += obj->size();\n+          } else {\n+            \/\/ This object is not live so we don't verify dirty cards contained therein\n+            assert(ctx->top_at_mark_start(r) == top, \"Expect tams == top at start of mark.\");\n+            obj_addr = ctx->get_next_marked_addr(obj_addr, top);\n+          }\n+        }\n+      } \/\/ else, we ignore humongous continuation region\n+    } \/\/ else, this is not an OLD region so we ignore it\n+  } \/\/ all regions have been processed\n+}\n+\n+void ShenandoahHeap::help_verify_region_rem_set(ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx, HeapWord* from,\n+                                                HeapWord* top, HeapWord* registration_watermark, const char* message) {\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(false);\n+\n+  HeapWord* obj_addr = from;\n+  if (r->is_humongous_start()) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      size_t card_index = scanner->card_index_for_addr(obj_addr);\n+      \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+      \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+      \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+      if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+        obj->oop_iterate(&check_interesting_pointers);\n+      }\n+      \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+    }\n+    \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+\n+    if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+      ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL, message,\n+                                       \"object not properly registered\", __FILE__, __LINE__);\n+    }\n+  } else if (!r->is_humongous()) {\n+    while (obj_addr < top) {\n+      oop obj = cast_to_oop(obj_addr);\n+      \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+      if (!ctx || ctx->is_marked(obj)) {\n+        size_t card_index = scanner->card_index_for_addr(obj_addr);\n+        \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+        \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+        if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+          obj->oop_iterate(&check_interesting_pointers);\n+        }\n+        \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+\n+        if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL, message,\n+                                           \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+        obj_addr += obj->size();\n+      } else {\n+        \/\/ This object is not live so we don't verify dirty cards contained therein\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahHeap::verify_rem_set_after_full_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(r, nullptr, r->bottom(), r->top(), r->top(), \"Remembered set violation at end of Full GC\");\n+    }\n+  }\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.  Even though\n+\/\/ the update-references scan of remembered set only examines cards up to update_watermark, the remembered\n+\/\/ set should be valid through top.  This examines the write_card_table between bottom() and top() because\n+\/\/ all PLABS are retired immediately before the start of update refs.\n+void ShenandoahHeap::verify_rem_set_at_update_ref() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (is_old_bitmap_stable() || active_generation()->generation_mode() == GLOBAL) {\n+    ctx = complete_marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(r, ctx, r->bottom(), r->top(), r->get_update_watermark(),\n+                                 \"Remembered set violation at init-update-references\");\n+    }\n+  }\n+}\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahRegionAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":1114,"deletions":251,"binary":false,"changes":1365,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -45,0 +47,1 @@\n+class PLAB;\n@@ -47,0 +50,1 @@\n+class ShenandoahRegulatorThread;\n@@ -49,0 +53,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -50,0 +57,1 @@\n+class ShenandoahOldHeuristics;\n@@ -51,1 +59,0 @@\n-class ShenandoahMode;\n@@ -110,0 +117,10 @@\n+template<GenerationMode GENERATION>\n+class ShenandoahGenerationRegionClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  explicit ShenandoahGenerationRegionClosure(ShenandoahHeapRegionClosure* cl) : _cl(cl) {}\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+  virtual bool is_thread_safe() { return _cl->is_thread_safe(); }\n+ private:\n+  ShenandoahHeapRegionClosure* _cl;\n+};\n+\n@@ -127,0 +144,1 @@\n+  friend class ShenandoahOldGC;\n@@ -135,0 +153,4 @@\n+  ShenandoahGeneration* _gc_generation;\n+\n+  \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+  bool _prepare_for_old_mark;\n@@ -141,0 +163,15 @@\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  ShenandoahOldHeuristics* old_heuristics();\n+\n+  bool doing_mixed_evacuations();\n+  bool is_old_bitmap_stable() const;\n+  bool is_gc_generation_young() const;\n+\n@@ -154,0 +191,1 @@\n+  void initialize_generations();\n@@ -166,0 +204,3 @@\n+  void verify_rem_set_at_mark();\n+  void verify_rem_set_at_update_ref();\n+  void verify_rem_set_after_full_gc();\n@@ -181,1 +222,0 @@\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -184,0 +224,4 @@\n+  static size_t young_generation_capacity(size_t total_capacity);\n+  void help_verify_region_rem_set(ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx,\n+                                  HeapWord* from, HeapWord* top, HeapWord* update_watermark, const char* message);\n+\n@@ -191,2 +235,0 @@\n-  void increase_allocated(size_t bytes);\n-  size_t bytes_allocated_since_gc_start();\n@@ -260,2 +302,2 @@\n-    \/\/ Heap is under marking: needs SATB barriers.\n-    MARKING_BITPOS    = 1,\n+    \/\/ Young regions are under marking: needs SATB barriers.\n+    YOUNG_MARKING_BITPOS    = 1,\n@@ -271,0 +313,3 @@\n+\n+    \/\/ Old regions are under marking, still need SATB barriers.\n+    OLD_MARKING_BITPOS = 5\n@@ -276,1 +321,1 @@\n-    MARKING       = 1 << MARKING_BITPOS,\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n@@ -280,0 +325,1 @@\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -290,0 +336,47 @@\n+  \/\/ _alloc_supplement_reserve is a supplemental budget for new_memory allocations.  During evacuation and update-references,\n+  \/\/ mutator allocation requests are \"authorized\" iff young_gen->available() plus _alloc_supplement_reserve minus\n+  \/\/ _young_evac_reserve is greater than request size.  The values of _alloc_supplement_reserve and _young_evac_reserve\n+  \/\/ are zero except during evacuation and update-reference phases of GC.  Both of these values are established at\n+  \/\/ the start of evacuation, and they remain constant throughout the duration of these two phases of GC.  Since these\n+  \/\/ two values are constant throughout each GC phases, we introduce a new service into ShenandoahGeneration.  This service\n+  \/\/ provides adjusted_available() based on an adjusted capacity.  At the start of evacuation, we adjust young capacity by\n+  \/\/ adding the amount to be borrowed from old-gen and subtracting the _young_evac_reserve, we adjust old capacity by\n+  \/\/ subtracting the amount to be loaned to young-gen.\n+  \/\/\n+  \/\/ We always use adjusted capacities to determine permission to allocate within young and to promote into old.  Note\n+  \/\/ that adjusted capacities equal traditional capacities except during evacuation and update refs.\n+  \/\/\n+  \/\/ During evacuation, we assure that _old_evac_expended does not exceed _old_evac_reserve.\n+  \/\/\n+  \/\/ At the end of update references, we perform the following bookkeeping activities:\n+  \/\/\n+  \/\/ 1. Unadjust the capacity within young-gen and old-gen to undo the effects of borrowing memory from old-gen.  Note that\n+  \/\/    the entirety of the collection set is now available, so allocation capacity naturally increase at this time.\n+  \/\/ 2. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promoted_reserve\n+  \/\/\n+  \/\/ _young_evac_reserve and _old_evac_reserve are only non-zero during evacuation and update-references.\n+  \/\/\n+  \/\/ Allocation of old GCLABs assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n+  \/\/  is authorized, increment _old_evac_expended by request size.  This allocation ignores old_gen->available().\n+  \/\/\n+  \/\/ Note that the typical total expenditure on evacuation is less than the associated evacuation reserve because we generally\n+  \/\/ reserve ShenandoahEvacWaste (> 1.0) times the anticipated evacuation need.  In the case that there is an excessive amount\n+  \/\/ of waste, it may be that one thread fails to grab a new GCLAB, this does not necessarily doom the associated evacuation\n+  \/\/ effort.  If this happens, the requesting thread blocks until some other thread manages to evacuate the offending object.\n+  \/\/ Only after \"all\" threads fail to evacuate an object do we consider the evacuation effort to have failed.\n+\n+  intptr_t _alloc_supplement_reserve;  \/\/ Bytes reserved for young allocations during evac and update refs\n+  size_t _promoted_reserve;            \/\/ Bytes reserved within old-gen to hold the results of promotion\n+  volatile size_t _promoted_expended;  \/\/ Bytes of old-gen memory expended on promotions\n+\n+  size_t _old_evac_reserve;            \/\/ Bytes reserved within old-gen to hold evacuated objects from old-gen collection set\n+  volatile size_t _old_evac_expended;  \/\/ Bytes of old-gen memory expended on old-gen evacuations\n+\n+  size_t _young_evac_reserve;          \/\/ Bytes reserved within young-gen to hold evacuated objects from young-gen collection set\n+\n+  size_t _captured_old_usage;          \/\/ What was old usage (bytes) when last captured?\n+\n+  size_t _previous_promotion;          \/\/ Bytes promoted during previous evacuation\n+\n+  bool _upgraded_to_full;\n+\n@@ -293,0 +386,2 @@\n+\n+\n@@ -297,1 +392,2 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -306,0 +402,2 @@\n+  void set_prepare_for_old_mark_in_progress(bool cond);\n+  void set_aging_cycle(bool cond);\n@@ -310,0 +408,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -320,0 +420,35 @@\n+  inline bool is_prepare_for_old_mark_in_progress() const;\n+  inline bool is_aging_cycle() const;\n+  inline bool upgraded_to_full() { return _upgraded_to_full; }\n+  inline void start_conc_gc() { _upgraded_to_full = false; }\n+  inline void record_upgrade_to_full() { _upgraded_to_full = true; }\n+\n+  inline size_t capture_old_usage(size_t usage);\n+  inline void set_previous_promotion(size_t promoted_bytes);\n+  inline size_t get_previous_promotion() const;\n+\n+  \/\/ Returns previous value\n+  inline size_t set_promoted_reserve(size_t new_val);\n+  inline size_t get_promoted_reserve() const;\n+\n+  inline void reset_promoted_expended();\n+  inline size_t expend_promoted(size_t increment);\n+  inline size_t unexpend_promoted(size_t decrement);\n+  inline size_t get_promoted_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_old_evac_reserve(size_t new_val);\n+  inline size_t get_old_evac_reserve() const;\n+\n+  inline void reset_old_evac_expended();\n+  inline size_t expend_old_evac(size_t increment);\n+  inline size_t get_old_evac_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_young_evac_reserve(size_t new_val);\n+  inline size_t get_young_evac_reserve() const;\n+\n+  \/\/ Returns previous value.  This is a signed value because it is the amount borrowed minus the amount reserved for\n+  \/\/ young-gen evacuation.  In case we cannot borrow much, this value might be negative.\n+  inline intptr_t set_alloc_supplement_reserve(intptr_t new_val);\n+  inline intptr_t get_alloc_supplement_reserve() const;\n@@ -322,0 +457,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -337,0 +474,1 @@\n+  double _cancel_requested_time;\n@@ -338,0 +476,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -341,2 +484,0 @@\n-  static address cancelled_gc_addr();\n-\n@@ -346,1 +487,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -348,0 +489,1 @@\n+  void cancel_concurrent_mark();\n@@ -357,4 +499,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n-  void prepare_evacuation(bool concurrent);\n@@ -384,0 +522,4 @@\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahGeneration*      _global_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n@@ -385,0 +527,1 @@\n+  ShenandoahRegulatorThread* _regulator_thread;\n@@ -387,1 +530,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -395,0 +537,1 @@\n+  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n@@ -397,0 +540,5 @@\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahOldGeneration*   old_generation()    const { return _old_generation;    }\n+  ShenandoahGeneration*      generation_for(ShenandoahRegionAffiliation affiliation) const;\n+\n@@ -399,1 +547,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -412,0 +559,3 @@\n+  MemoryPool*                  _young_gen_memory_pool;\n+  MemoryPool*                  _old_gen_memory_pool;\n+\n@@ -420,1 +570,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support() { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const { return _monitoring_support;    }\n@@ -431,8 +581,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -442,0 +584,1 @@\n+  ShenandoahSharedFlag  _is_aging_cycle;\n@@ -473,0 +616,5 @@\n+  bool is_in_active_generation(oop obj) const;\n+  bool is_in_young(const void* p) const;\n+  bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n@@ -527,1 +675,2 @@\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n+  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region, bool is_promotion);\n+\n@@ -532,0 +681,4 @@\n+  inline HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n@@ -533,1 +686,1 @@\n-  HeapWord* allocate_memory(ShenandoahAllocRequest& request);\n+  HeapWord* allocate_memory(ShenandoahAllocRequest& request, bool is_promotion);\n@@ -553,0 +706,2 @@\n+  void set_young_lab_region_flags();\n+\n@@ -577,2 +732,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -589,2 +742,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -610,0 +761,6 @@\n+  ShenandoahSharedFlag _old_gen_oom_evac;\n+\n+  inline oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahRegionAffiliation target_gen);\n+  void handle_old_evacuation(HeapWord* obj, size_t words, bool promotion);\n+  void handle_old_evacuation_failure();\n+  void handle_promotion_failure();\n@@ -622,1 +779,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -630,0 +787,19 @@\n+  inline bool clear_old_evacuation_failure();\n+\n+\/\/ ---------- Generational support\n+\/\/\n+private:\n+  RememberedScanner* _card_scan;\n+\n+public:\n+  inline RememberedScanner* card_scan() { return _card_scan; }\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+  void dirty_cards(HeapWord* start, HeapWord* end);\n+  void clear_cards(HeapWord* start, HeapWord* end);\n+  void mark_card_as_dirty(void* location);\n+  void retire_plab(PLAB* plab);\n+  void retire_plab(PLAB* plab, Thread* thread);\n+  void cancel_old_gc();\n+  bool is_old_gc_active();\n+  void coalesce_and_fill_old_regions();\n+\n@@ -651,1 +827,3 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n@@ -653,0 +831,1 @@\n+  void transfer_old_pointers_from_satb();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":213,"deletions":34,"binary":false,"changes":247,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2013, 2019, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2013, 2020, Red Hat, Inc. All rights reserved.\n@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -32,0 +33,4 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -46,0 +51,1 @@\n+\n@@ -68,0 +74,2 @@\n+  _plab_allocs(0),\n+  _has_young_lab(false),\n@@ -70,1 +78,3 @@\n-  _update_watermark(start) {\n+  _update_watermark(start),\n+  _affiliation(FREE),\n+  _age(0) {\n@@ -86,1 +96,1 @@\n-void ShenandoahHeapRegion::make_regular_allocation() {\n+void ShenandoahHeapRegion::make_regular_allocation(ShenandoahRegionAffiliation affiliation) {\n@@ -88,1 +98,1 @@\n-\n+  reset_age();\n@@ -93,0 +103,1 @@\n+      set_affiliation(affiliation);\n@@ -102,0 +113,20 @@\n+\/\/ Change affiliation to YOUNG_GENERATION if _state is not _pinned_cset, _regular, or _pinned.  This implements\n+\/\/ behavior previously performed as a side effect of make_regular_bypass().\n+void ShenandoahHeapRegion::make_young_maybe() {\n+ switch (_state) {\n+   case _empty_uncommitted:\n+   case _empty_committed:\n+   case _cset:\n+   case _humongous_start:\n+   case _humongous_cont:\n+     set_affiliation(YOUNG_GENERATION);\n+     return;\n+   case _pinned_cset:\n+   case _regular:\n+   case _pinned:\n+     return;\n+   default:\n+     assert(false, \"Unexpected _state in make_young_maybe\");\n+  }\n+}\n+\n@@ -106,1 +137,1 @@\n-\n+  reset_age();\n@@ -129,0 +160,1 @@\n+  reset_age();\n@@ -140,1 +172,1 @@\n-void ShenandoahHeapRegion::make_humongous_start_bypass() {\n+void ShenandoahHeapRegion::make_humongous_start_bypass(ShenandoahRegionAffiliation affiliation) {\n@@ -143,1 +175,2 @@\n-\n+  set_affiliation(affiliation);\n+  reset_age();\n@@ -158,0 +191,1 @@\n+  reset_age();\n@@ -169,1 +203,1 @@\n-void ShenandoahHeapRegion::make_humongous_cont_bypass() {\n+void ShenandoahHeapRegion::make_humongous_cont_bypass(ShenandoahRegionAffiliation affiliation) {\n@@ -172,1 +206,2 @@\n-\n+  set_affiliation(affiliation);\n+  reset_age();\n@@ -213,0 +248,1 @@\n+      assert(affiliation() != FREE, \"Pinned region should not be FREE\");\n@@ -231,0 +267,1 @@\n+  \/\/ Leave age untouched.  We need to consult the age when we are deciding whether to promote evacuated objects.\n@@ -243,0 +280,1 @@\n+  reset_age();\n@@ -263,1 +301,2 @@\n-  ShenandoahHeap::heap()->complete_marking_context()->reset_top_bitmap(this);\n+  assert(ShenandoahHeap::heap()->active_generation()->is_mark_complete(), \"Marking should be complete here.\");\n+  ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n@@ -268,0 +307,1 @@\n+  reset_age();\n@@ -307,0 +347,1 @@\n+  _plab_allocs = 0;\n@@ -310,1 +351,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs) * HeapWordSize;\n+  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n@@ -321,0 +362,4 @@\n+size_t ShenandoahHeapRegion::get_plab_allocs() const {\n+  return _plab_allocs * HeapWordSize;\n+}\n+\n@@ -365,0 +410,14 @@\n+  switch (_affiliation) {\n+    case ShenandoahRegionAffiliation::FREE:\n+      st->print(\"|F\");\n+      break;\n+    case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+      st->print(\"|Y\");\n+      break;\n+    case ShenandoahRegionAffiliation::OLD_GENERATION:\n+      st->print(\"|O\");\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+  }\n+\n@@ -376,0 +435,3 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    st->print(\"|G \" SIZE_FORMAT_W(5) \"%1s\", byte_size_in_proper_unit(get_plab_allocs()),   proper_unit_for_byte_size(get_plab_allocs()));\n+  }\n@@ -384,1 +446,94 @@\n-void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk) {\n+\/\/ oop_iterate without closure and without cancellation.  always return true.\n+bool ShenandoahHeapRegion::oop_fill_and_coalesce_wo_cancel() {\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != NULL, \"klass should not be NULL\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n+}\n+\n+\/\/ oop_iterate without closure, return true if completed without cancellation\n+bool ShenandoahHeapRegion::oop_fill_and_coalesce() {\n+  HeapWord* obj_addr = resume_coalesce_and_fill();\n+  \/\/ Consider yielding to cancel\/preemption request after this many coalesce operations (skip marked, or coalesce free).\n+  const size_t preemption_stride = 128;\n+\n+  assert(!is_humongous(), \"No need to fill or coalesce humongous regions\");\n+  if (!is_active()) {\n+    end_preemptible_coalesce_and_fill();\n+    return true;\n+  }\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  \/\/ Expect marking to be completed before these threads invoke this service.\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  size_t ops_before_preempt_check = preemption_stride;\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != NULL, \"klass should not be NULL\");\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->card_scan()->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+    if (ops_before_preempt_check-- == 0) {\n+      if (heap->cancelled_gc()) {\n+        suspend_coalesce_and_fill(obj_addr);\n+        return false;\n+      }\n+      ops_before_preempt_check = preemption_stride;\n+    }\n+  }\n+  \/\/ Mark that this region has been coalesced and filled\n+  end_preemptible_coalesce_and_fill();\n+  return true;\n+}\n+\n+void ShenandoahHeapRegion::global_oop_iterate_and_fill_dead(OopIterateClosure* blk) {\n@@ -387,0 +542,2 @@\n+    \/\/ No need to fill dead within humongous regions.  Either the entire region is dead, or the entire region is\n+    \/\/ unchanged.  A humongous region holds no more than one humongous object.\n@@ -389,1 +546,1 @@\n-    oop_iterate_objects(blk);\n+    global_oop_iterate_objects_and_fill_dead(blk);\n@@ -393,2 +550,2 @@\n-void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk) {\n-  assert(! is_humongous(), \"no humongous region here\");\n+void ShenandoahHeapRegion::global_oop_iterate_objects_and_fill_dead(OopIterateClosure* blk) {\n+  assert(!is_humongous(), \"no humongous region here\");\n@@ -396,2 +553,30 @@\n-  HeapWord* t = top();\n-  \/\/ Could call objects iterate, but this is easier.\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  RememberedScanner* rem_set_scanner = heap->card_scan();\n+  \/\/ Objects allocated above TAMS are not marked, but are considered live for purposes of current GC efforts.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  while (obj_addr < t) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != NULL, \"klass should not be NULL\");\n+      \/\/ when promoting an entire region, we have to register the marked objects as well\n+      obj_addr += obj->oop_iterate_size(blk);\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+\n+      \/\/ coalesce_objects() unregisters all but first object subsumed within coalesced range.\n+      rem_set_scanner->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+\n+  \/\/ Any object above TAMS and below top() is considered live.\n+  t = top();\n@@ -404,0 +589,50 @@\n+\/\/ DO NOT CANCEL.  If this worker thread has accepted responsibility for scanning a particular range of addresses, it\n+\/\/ must finish the work before it can be cancelled.\n+void ShenandoahHeapRegion::oop_iterate_humongous_slice(OopIterateClosure* blk, bool dirty_only,\n+                                                       HeapWord* start, size_t words, bool write_table, bool is_concurrent) {\n+  assert(words % CardTable::card_size_in_words() == 0, \"Humongous iteration must span whole number of cards\");\n+  assert(is_humongous(), \"only humongous region here\");\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Find head.\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  assert(r->is_humongous_start(), \"need humongous head here\");\n+  assert(CardTable::card_size_in_words() * (words \/ CardTable::card_size_in_words()) == words,\n+         \"slice must be integral number of cards\");\n+\n+  oop obj = cast_to_oop(r->bottom());\n+  RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+  size_t card_index = scanner->card_index_for_addr(start);\n+  size_t num_cards = words \/ CardTable::card_size_in_words();\n+\n+  if (dirty_only) {\n+    if (write_table) {\n+      while (num_cards-- > 0) {\n+        if (scanner->is_write_card_dirty(card_index++)) {\n+          obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+        }\n+        start += CardTable::card_size_in_words();\n+      }\n+    } else {\n+      while (num_cards-- > 0) {\n+        if (scanner->is_card_dirty(card_index++)) {\n+          obj->oop_iterate(blk, MemRegion(start, start + CardTable::card_size_in_words()));\n+        }\n+        start += CardTable::card_size_in_words();\n+      }\n+    }\n+  } else {\n+    \/\/ Scan all data, regardless of whether cards are dirty\n+    obj->oop_iterate(blk, MemRegion(start, start + num_cards * CardTable::card_size_in_words()));\n+  }\n+}\n+\n+void ShenandoahHeapRegion::oop_iterate_humongous(OopIterateClosure* blk, HeapWord* start, size_t words) {\n+  assert(is_humongous(), \"only humongous region here\");\n+  \/\/ Find head.\n+  ShenandoahHeapRegion* r = humongous_start_region();\n+  assert(r->is_humongous_start(), \"need humongous head here\");\n+  oop obj = cast_to_oop(r->bottom());\n+  obj->oop_iterate(blk, MemRegion(start, start + words));\n+}\n+\n@@ -429,0 +664,8 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  if (affiliation() == YOUNG_GENERATION) {\n+    heap->young_generation()->decrease_used(used());\n+  } else if (affiliation() == OLD_GENERATION) {\n+    heap->old_generation()->decrease_used(used());\n+  }\n+\n@@ -434,1 +677,1 @@\n-  ShenandoahHeap::heap()->marking_context()->reset_top_at_mark_start(this);\n+  heap->marking_context()->reset_top_at_mark_start(this);\n@@ -438,0 +681,1 @@\n+  set_affiliation(FREE);\n@@ -688,0 +932,110 @@\n+\n+void ShenandoahHeapRegion::set_affiliation(ShenandoahRegionAffiliation new_affiliation) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  {\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    log_debug(gc)(\"Setting affiliation of Region \" SIZE_FORMAT \" from %s to %s, top: \" PTR_FORMAT \", TAMS: \" PTR_FORMAT\n+                  \", watermark: \" PTR_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                  index(), affiliation_name(_affiliation), affiliation_name(new_affiliation),\n+                  p2i(top()), p2i(ctx->top_at_mark_start(this)), p2i(_update_watermark), p2i(ctx->top_bitmap(this)));\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ During full gc, heap->complete_marking_context() is not valid, may equal nullptr.\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    size_t idx = this->index();\n+    HeapWord* top_bitmap = ctx->top_bitmap(this);\n+\n+    assert(ctx->is_bitmap_clear_range(top_bitmap, _end),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear between top_bitmap: \" PTR_FORMAT \" and end: \" PTR_FORMAT, idx,\n+           p2i(top_bitmap), p2i(_end));\n+  }\n+#endif\n+\n+  if (_affiliation == new_affiliation) {\n+    return;\n+  }\n+\n+  if (!heap->mode()->is_generational()) {\n+    _affiliation = new_affiliation;\n+    return;\n+  }\n+\n+  log_trace(gc)(\"Changing affiliation of region %zu from %s to %s\",\n+    index(), affiliation_name(_affiliation), affiliation_name(new_affiliation));\n+\n+  if (_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+    heap->young_generation()->decrement_affiliated_region_count();\n+  } else if (_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+    heap->old_generation()->decrement_affiliated_region_count();\n+  }\n+\n+  switch (new_affiliation) {\n+    case FREE:\n+      assert(!has_live(), \"Free region should not have live data\");\n+      break;\n+    case YOUNG_GENERATION:\n+      reset_age();\n+      heap->young_generation()->increment_affiliated_region_count();\n+      break;\n+    case OLD_GENERATION:\n+      heap->old_generation()->increment_affiliated_region_count();\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      return;\n+  }\n+  _affiliation = new_affiliation;\n+}\n+\n+size_t ShenandoahHeapRegion::promote_humongous() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+  assert(is_young(), \"Only young regions can be promoted\");\n+  assert(is_humongous_start(), \"Should not promote humongous continuation in isolation\");\n+  assert(age() >= InitialTenuringThreshold, \"Only promote regions that are sufficiently aged\");\n+\n+  ShenandoahGeneration* old_generation = heap->old_generation();\n+  ShenandoahGeneration* young_generation = heap->young_generation();\n+\n+  oop obj = cast_to_oop(bottom());\n+  assert(marking_context->is_marked(obj), \"promoted humongous object should be alive\");\n+\n+  size_t spanned_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+  size_t index_limit = index() + spanned_regions;\n+\n+  log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, index(), spanned_regions);\n+\n+  \/\/ Since this region may have served previously as OLD, it may hold obsolete object range info.\n+  heap->card_scan()->reset_object_range(bottom(), bottom() + spanned_regions * ShenandoahHeapRegion::region_size_words());\n+  \/\/ Since the humongous region holds only one object, no lock is necessary for this register_object() invocation.\n+  heap->card_scan()->register_object_wo_lock(bottom());\n+\n+  \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n+  \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n+  \/\/ in the last humongous region that is not spanned by obj is currently not used.\n+  for (size_t i = index(); i < index_limit; i++) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                  r->index(), p2i(r->bottom()), p2i(r->top()));\n+    \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n+    r->set_affiliation(OLD_GENERATION);\n+    old_generation->increase_used(r->used());\n+    young_generation->decrease_used(r->used());\n+  }\n+  if (obj->is_typeArray()) {\n+    \/\/ Primitive arrays don't need to be scanned.  See above TODO question about requiring\n+    \/\/ region promotion at safepoint.\n+    log_debug(gc)(\"Clean cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                  index(), p2i(bottom()), p2i(bottom() + obj->size()));\n+    heap->card_scan()->mark_range_as_clean(bottom(), obj->size());\n+  } else {\n+    log_debug(gc)(\"Dirty cards for promoted humongous object (Region \" SIZE_FORMAT \") from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                  index(), p2i(bottom()), p2i(bottom() + obj->size()));\n+    heap->card_scan()->mark_range_as_dirty(bottom(), obj->size());\n+  }\n+  return index_limit - index();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":372,"deletions":18,"binary":false,"changes":390,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -36,11 +37,0 @@\n-ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp) :\n-  MetadataVisitingOopIterateClosure(rp),\n-  _queue(q),\n-  _mark_context(ShenandoahHeap::heap()->marking_context()),\n-  _weak(false)\n-{ }\n-\n-ShenandoahMark::ShenandoahMark() :\n-  _task_queues(ShenandoahHeap::heap()->marking_context()->task_queues()) {\n-}\n-\n@@ -57,1 +47,3 @@\n-  CodeCache::on_gc_marking_cycle_finish();\n+  if (!ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress()) {\n+    CodeCache::on_gc_marking_cycle_finish();\n+  }\n@@ -60,4 +52,7 @@\n-void ShenandoahMark::clear() {\n-  \/\/ Clean up marking stacks.\n-  ShenandoahObjToScanQueueSet* queues = ShenandoahHeap::heap()->marking_context()->task_queues();\n-  queues->clear();\n+ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q) :\n+  MetadataVisitingOopIterateClosure(rp),\n+  _queue(q),\n+  _old_queue(old_q),\n+  _mark_context(ShenandoahHeap::heap()->marking_context()),\n+  _weak(false)\n+{ }\n@@ -65,2 +60,4 @@\n-  \/\/ Cancel SATB buffers.\n-  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+ShenandoahMark::ShenandoahMark(ShenandoahGeneration* generation) :\n+  _generation(generation),\n+  _task_queues(generation->task_queues()),\n+  _old_gen_task_queues(generation->old_gen_task_queues()) {\n@@ -69,2 +66,2 @@\n-template <bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n-void ShenandoahMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req) {\n+template <GenerationMode GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+void ShenandoahMark::mark_loop_prework(uint w, TaskTerminator *t, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req, bool update_refs) {\n@@ -72,0 +69,1 @@\n+  ShenandoahObjToScanQueue* old = get_old_queue(w);\n@@ -78,4 +76,4 @@\n-  if (heap->has_forwarded_objects()) {\n-    using Closure = ShenandoahMarkUpdateRefsClosure;\n-    Closure cl(q, rp);\n-    mark_loop_work<Closure, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n+  if (update_refs) {\n+    using Closure = ShenandoahMarkUpdateRefsClosure<GENERATION>;\n+    Closure cl(q, rp, old);\n+    mark_loop_work<Closure, GENERATION, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n@@ -83,3 +81,3 @@\n-    using Closure = ShenandoahMarkRefsClosure;\n-    Closure cl(q, rp);\n-    mark_loop_work<Closure, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n+    using Closure = ShenandoahMarkRefsClosure<GENERATION>;\n+    Closure cl(q, rp, old);\n+    mark_loop_work<Closure, GENERATION, CANCELLABLE, STRING_DEDUP>(&cl, ld, w, t, req);\n@@ -91,1 +89,21 @@\n-void ShenandoahMark::mark_loop(uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n+template<bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+void ShenandoahMark::mark_loop(GenerationMode generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp, StringDedup::Requests* const req) {\n+  bool update_refs = ShenandoahHeap::heap()->has_forwarded_objects();\n+  switch (generation) {\n+    case YOUNG:\n+      mark_loop_prework<YOUNG, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    case OLD:\n+      \/\/ Old generation collection only performs marking, it should not update references.\n+      mark_loop_prework<OLD, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, false);\n+      break;\n+    case GLOBAL:\n+      mark_loop_prework<GLOBAL, CANCELLABLE, STRING_DEDUP>(worker_id, terminator, rp, req, update_refs);\n+      break;\n+    default:\n+      ShouldNotReachHere();\n+      break;\n+  }\n+}\n+\n+void ShenandoahMark::mark_loop(GenerationMode generation, uint worker_id, TaskTerminator* terminator, ShenandoahReferenceProcessor *rp,\n@@ -96,1 +114,1 @@\n-        mark_loop_prework<true, NO_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, NO_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -99,1 +117,1 @@\n-        mark_loop_prework<true, ENQUEUE_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, ENQUEUE_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -102,1 +120,1 @@\n-        mark_loop_prework<true, ALWAYS_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<true, ALWAYS_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -108,1 +126,1 @@\n-        mark_loop_prework<false, NO_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, NO_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -111,1 +129,1 @@\n-        mark_loop_prework<false, ENQUEUE_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, ENQUEUE_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -114,1 +132,1 @@\n-        mark_loop_prework<false, ALWAYS_DEDUP>(worker_id, terminator, rp, req);\n+        mark_loop<false, ALWAYS_DEDUP>(generation, worker_id, terminator, rp, req);\n@@ -120,1 +138,1 @@\n-template <class T, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n+template <class T, GenerationMode GENERATION, bool CANCELLABLE, StringDedupMode STRING_DEDUP>\n@@ -129,1 +147,2 @@\n-  heap->ref_processor()->set_mark_closure(worker_id, cl);\n+  assert(heap->active_generation()->generation_mode() == GENERATION, \"Sanity\");\n+  heap->active_generation()->ref_processor()->set_mark_closure(worker_id, cl);\n@@ -158,0 +177,1 @@\n+  ShenandoahObjToScanQueue* old = get_old_queue(worker_id);\n@@ -159,1 +179,1 @@\n-  ShenandoahSATBBufferClosure drain_satb(q);\n+  ShenandoahSATBBufferClosure<GENERATION> drain_satb(q, old);\n@@ -169,1 +189,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.cpp","additions":57,"deletions":38,"binary":false,"changes":95,"status":"modified"},{"patch":"@@ -160,5 +160,2 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    ShenandoahKeepAliveClosure cl;\n-    data->oops_do(&cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress() ||\n-             heap->is_concurrent_strong_root_in_progress() ) {\n+  if (heap->is_concurrent_weak_root_in_progress() ||\n+      heap->is_concurrent_strong_root_in_progress()) {\n@@ -167,0 +164,3 @@\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    ShenandoahKeepAliveClosure cl;\n+    data->oops_do(&cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,842 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::last_valid_index() {\n+  return _card_table->last_valid_index();\n+}\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::total_cards() {\n+  return _total_card_count;\n+}\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::card_index_for_addr(HeapWord *p) {\n+  return _card_table->index_for(p);\n+}\n+\n+inline HeapWord *\n+ShenandoahDirectCardMarkRememberedSet::addr_for_card_index(size_t card_index) {\n+  return _whole_heap_base + CardTable::card_size_in_words() * card_index;\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_write_card_dirty(size_t card_index) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(size_t card_index) {\n+  uint8_t *bp = &(_card_table->read_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(size_t card_index) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(size_t card_index, size_t num_cards) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n+    *bp++ = CardTable::dirty_card_val();\n+  }\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_clean(size_t card_index) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(size_t card_index, size_t num_cards) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n+    *bp++ = CardTable::clean_card_val();\n+  }\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(HeapWord *p) {\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->read_byte_map())[index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(HeapWord *p) {\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->write_byte_map())[index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(HeapWord *p, size_t num_heap_words) {\n+  uint8_t *bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  uint8_t *end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to dirty last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size() - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n+    *bp++ = CardTable::dirty_card_val();\n+  }\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_clean(HeapWord *p) {\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->write_byte_map())[index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_read_card_as_clean(size_t index) {\n+  uint8_t *bp = &(_card_table->read_byte_map())[index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(HeapWord *p, size_t num_heap_words) {\n+  uint8_t *bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  uint8_t *end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to clean last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size() - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n+    *bp++ = CardTable::clean_card_val();\n+  }\n+}\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::cluster_count() {\n+  return _cluster_count;\n+}\n+\n+\/\/ No lock required because arguments align with card boundaries.\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::reset_object_range(HeapWord* from, HeapWord* to) {\n+  assert(((((unsigned long long) from) & (CardTable::card_size() - 1)) == 0) &&\n+         ((((unsigned long long) to) & (CardTable::card_size() - 1)) == 0),\n+         \"reset_object_range bounds must align with card boundaries\");\n+  size_t card_at_start = _rs->card_index_for_addr(from);\n+  size_t num_cards = (to - from) \/ CardTable::card_size_in_words();\n+\n+  for (size_t i = 0; i < num_cards; i++) {\n+    object_starts[card_at_start + i].short_word = 0;\n+  }\n+}\n+\n+\/\/ Assume only one thread at a time registers objects pertaining to\n+\/\/ each card-table entry's range of memory.\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::register_object(HeapWord* address) {\n+  shenandoah_assert_heaplocked();\n+\n+  register_object_wo_lock(address);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::register_object_wo_lock(HeapWord* address) {\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord *card_start_address = _rs->addr_for_card_index(card_at_start);\n+  uint8_t offset_in_card = address - card_start_address;\n+\n+  if (!has_object(card_at_start)) {\n+    set_has_object_bit(card_at_start);\n+    set_first_start(card_at_start, offset_in_card);\n+    set_last_start(card_at_start, offset_in_card);\n+  } else {\n+    if (offset_in_card < get_first_start(card_at_start))\n+      set_first_start(card_at_start, offset_in_card);\n+    if (offset_in_card > get_last_start(card_at_start))\n+      set_last_start(card_at_start, offset_in_card);\n+  }\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::coalesce_objects(HeapWord* address, size_t length_in_words) {\n+\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord *card_start_address = _rs->addr_for_card_index(card_at_start);\n+  size_t card_at_end = card_at_start + ((address + length_in_words) - card_start_address) \/ CardTable::card_size_in_words();\n+\n+  if (card_at_start == card_at_end) {\n+    \/\/ There are no changes to the get_first_start array.  Either get_first_start(card_at_start) returns this coalesced object,\n+    \/\/ or it returns an object that precedes the coalesced object.\n+    if (card_start_address + get_last_start(card_at_start) < address + length_in_words) {\n+      uint8_t coalesced_offset = static_cast<uint8_t>(address - card_start_address);\n+      \/\/ The object that used to be the last object starting within this card is being subsumed within the coalesced\n+      \/\/ object.  Since we always coalesce entire objects, this condition only occurs if the last object ends before or at\n+      \/\/ the end of the card's memory range and there is no object following this object.  In this case, adjust last_start\n+      \/\/ to represent the start of the coalesced range.\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ Else, no changes to last_starts information.  Either get_last_start(card_at_start) returns the object that immediately\n+    \/\/ follows the coalesced object, or it returns an object that follows the object immediately following the coalesced object.\n+  } else {\n+    uint8_t coalesced_offset = static_cast<uint8_t>(address - card_start_address);\n+    if (get_last_start(card_at_start) > coalesced_offset) {\n+      \/\/ Existing last start is being coalesced, create new last start\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ otherwise, get_last_start(card_at_start) must equal coalesced_offset\n+\n+    \/\/ All the cards between first and last get cleared.\n+    for (size_t i = card_at_start + 1; i < card_at_end; i++) {\n+      clear_has_object_bit(i);\n+    }\n+\n+    uint8_t follow_offset = static_cast<uint8_t>((address + length_in_words) - _rs->addr_for_card_index(card_at_end));\n+    if (has_object(card_at_end) && (get_first_start(card_at_end) < follow_offset)) {\n+      \/\/ It may be that after coalescing within this last card's memory range, the last card\n+      \/\/ no longer holds an object.\n+      if (get_last_start(card_at_end) >= follow_offset) {\n+        set_first_start(card_at_end, follow_offset);\n+      } else {\n+        \/\/ last_start is being coalesced so this card no longer has any objects.\n+        clear_has_object_bit(card_at_end);\n+      }\n+    }\n+    \/\/ else\n+    \/\/  card_at_end did not have an object, so it still does not have an object, or\n+    \/\/  card_at_end had an object that starts after the coalesced object, so no changes required for card_at_end\n+\n+  }\n+}\n+\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_first_start(size_t card_index) {\n+  assert(has_object(card_index), \"Can't get first start because no object starts here\");\n+  return object_starts[card_index].offsets.first & FirstStartBits;\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_last_start(size_t card_index) {\n+  assert(has_object(card_index), \"Can't get last start because no object starts here\");\n+  return object_starts[card_index].offsets.last;\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::last_valid_index() { return _rs->last_valid_index(); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::total_cards() { return _rs->total_cards(); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::card_index_for_addr(HeapWord *p) { return _rs->card_index_for_addr(p); };\n+\n+template<typename RememberedSet>\n+inline HeapWord *\n+ShenandoahScanRemembered<RememberedSet>::addr_for_card_index(size_t card_index) { return _rs->addr_for_card_index(card_index); }\n+\n+template<typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::is_card_dirty(size_t card_index) { return _rs->is_card_dirty(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_dirty(size_t card_index) { _rs->mark_card_as_dirty(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_dirty(size_t card_index, size_t num_cards) { _rs->mark_range_as_dirty(card_index, num_cards); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_clean(size_t card_index) { _rs->mark_card_as_clean(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_clean(size_t card_index, size_t num_cards) { _rs->mark_range_as_clean(card_index, num_cards); }\n+\n+template<typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::is_card_dirty(HeapWord *p) { return _rs->is_card_dirty(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_dirty(HeapWord *p) { _rs->mark_card_as_dirty(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_dirty(HeapWord *p, size_t num_heap_words) { _rs->mark_range_as_dirty(p, num_heap_words); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_clean(HeapWord *p) { _rs->mark_card_as_clean(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>:: mark_range_as_clean(HeapWord *p, size_t num_heap_words) { _rs->mark_range_as_clean(p, num_heap_words); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::cluster_count() { return _rs->cluster_count(); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::reset_object_range(HeapWord *from, HeapWord *to) {\n+  _scc->reset_object_range(from, to);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::register_object(HeapWord *addr) {\n+  _scc->register_object(addr);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::register_object_wo_lock(HeapWord *addr) {\n+  _scc->register_object_wo_lock(addr);\n+}\n+\n+template <typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::verify_registration(HeapWord* address, ShenandoahMarkingContext* ctx) {\n+\n+  size_t index = card_index_for_addr(address);\n+  if (!_scc->has_object(index)) {\n+    return false;\n+  }\n+  HeapWord* base_addr = addr_for_card_index(index);\n+  size_t offset = _scc->get_first_start(index);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Verify that I can find this object within its enclosing card by scanning forward from first_start.\n+  while (base_addr + offset < address) {\n+    oop obj = cast_to_oop(base_addr + offset);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      offset += obj->size();\n+    } else {\n+      \/\/ If this object is not live, don't trust its size(); all objects above tams are live.\n+      ShenandoahHeapRegion* r = heap->heap_region_containing(obj);\n+      HeapWord* tams = ctx->top_at_mark_start(r);\n+      offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+    }\n+  }\n+  if (base_addr + offset != address){\n+    return false;\n+  }\n+\n+  \/\/ At this point, offset represents object whose registration we are verifying.  We know that at least this object resides\n+  \/\/ within this card's memory.\n+\n+  \/\/ Make sure that last_offset is properly set for the enclosing card, but we can't verify this for\n+  \/\/ candidate collection-set regions during mixed evacuations, so disable this check in general\n+  \/\/ during mixed evacuations.\n+\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(base_addr + offset);\n+  size_t max_offset = r->top() - base_addr;\n+  if (max_offset > CardTable::card_size_in_words()) {\n+    max_offset = CardTable::card_size_in_words();\n+  }\n+  size_t prev_offset;\n+  if (!ctx) {\n+    do {\n+      oop obj = cast_to_oop(base_addr + offset);\n+      prev_offset = offset;\n+      offset += obj->size();\n+    } while (offset < max_offset);\n+    if (_scc->get_last_start(index) != prev_offset) {\n+      return false;\n+    }\n+\n+    \/\/ base + offset represents address of first object that starts on following card, if there is one.\n+\n+    \/\/ Notes: base_addr is addr_for_card_index(index)\n+    \/\/        base_addr + offset is end of the object we are verifying\n+    \/\/        cannot use card_index_for_addr(base_addr + offset) because it asserts arg < end of whole heap\n+    size_t end_card_index = index + offset \/ CardTable::card_size_in_words();\n+\n+    if (end_card_index > index && end_card_index <= _rs->last_valid_index()) {\n+      \/\/ If there is a following object registered on the next card, it should begin where this object ends.\n+      if (_scc->has_object(end_card_index) &&\n+          ((addr_for_card_index(end_card_index) + _scc->get_first_start(end_card_index)) != (base_addr + offset))) {\n+        return false;\n+      }\n+    }\n+\n+    \/\/ Assure that no other objects are registered \"inside\" of this one.\n+    for (index++; index < end_card_index; index++) {\n+      if (_scc->has_object(index)) {\n+        return false;\n+      }\n+    }\n+  } else {\n+    \/\/ This is a mixed evacuation or a global collect: rely on mark bits to identify which objects need to be properly registered\n+    assert(!ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Cannot rely on mark context here.\");\n+    \/\/ If the object reaching or spanning the end of this card's memory is marked, then last_offset for this card\n+    \/\/ should represent this object.  Otherwise, last_offset is a don't care.\n+    ShenandoahHeapRegion* region = heap->heap_region_containing(base_addr + offset);\n+    HeapWord* tams = ctx->top_at_mark_start(region);\n+    oop last_obj = nullptr;\n+    do {\n+      oop obj = cast_to_oop(base_addr + offset);\n+      if (ctx->is_marked(obj)) {\n+        prev_offset = offset;\n+        offset += obj->size();\n+        last_obj = obj;\n+      } else {\n+        offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+        \/\/ If there are no marked objects remaining in this region, offset equals tams - base_addr.  If this offset is\n+        \/\/ greater than max_offset, we will immediately exit this loop.  Otherwise, the next iteration of the loop will\n+        \/\/ treat the object at offset as marked and live (because address >= tams) and we will continue iterating object\n+        \/\/ by consulting the size() fields of each.\n+      }\n+    } while (offset < max_offset);\n+    if (last_obj != nullptr && prev_offset + last_obj->size() >= max_offset) {\n+      \/\/ last marked object extends beyond end of card\n+      if (_scc->get_last_start(index) != prev_offset) {\n+        return false;\n+      }\n+      \/\/ otherwise, the value of _scc->get_last_start(index) is a don't care because it represents a dead object and we\n+      \/\/ cannot verify its context\n+    }\n+  }\n+  return true;\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::coalesce_objects(HeapWord *addr, size_t length_in_words) {\n+  _scc->coalesce_objects(addr, length_in_words);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_empty(HeapWord *addr, size_t length_in_words) {\n+  _rs->mark_range_as_clean(addr, length_in_words);\n+  _scc->clear_objects_in_range(addr, length_in_words);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range,\n+                                                          ClosureType *cl, bool is_concurrent) {\n+  process_clusters(first_cluster, count, end_of_range, cl, false, is_concurrent);\n+}\n+\n+\/\/ Process all objects starting within count clusters beginning with first_cluster for which the start address is\n+\/\/ less than end_of_range.  For any such object, process the complete object, even if its end reaches beyond end_of_range.\n+\n+\/\/ Do not CANCEL within process_clusters.  It is assumed that if a worker thread accepts responsbility for processing\n+\/\/ a chunk of work, it will finish the work it starts.  Otherwise, the chunk of work will be lost in the transition to\n+\/\/ degenerated execution.\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range,\n+                                                          ClosureType *cl, bool write_table, bool is_concurrent) {\n+\n+  \/\/ Unlike traditional Shenandoah marking, the old-gen resident objects that are examined as part of the remembered set are not\n+  \/\/ themselves marked.  Each such object will be scanned only once.  Any young-gen objects referenced from the remembered set will\n+  \/\/ be marked and then subsequently scanned.\n+\n+  \/\/ If old-gen evacuation is active, then MarkingContext for old-gen heap regions is valid.  We use the MarkingContext\n+  \/\/ bits to determine which objects within a DIRTY card need to be scanned.  This is necessary because old-gen heap\n+  \/\/ regions which are in the candidate collection set have not been coalesced and filled.  Thus, these heap regions\n+  \/\/ may contain zombie objects.  Zombie objects are known to be dead, but have not yet been \"collected\".  Scanning\n+  \/\/ zombie objects is unsafe because the Klass pointer is not reliable, objects referenced from a zombie may have been\n+  \/\/ collected and their memory repurposed, and because zombie objects might refer to objects that are themselves dead.\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (heap->is_old_bitmap_stable()) {\n+    ctx = heap->marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  size_t card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  HeapWord *start_of_range = _rs->addr_for_card_index(card_index);\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(start_of_range);\n+  assert(end_of_range <= r->top(), \"process_clusters() examines one region at a time\");\n+\n+  while (count-- > 0) {\n+    \/\/ TODO: do we want to check cancellation in inner loop, on every card processed?  That would be more responsive,\n+    \/\/ but require more overhead for checking.\n+    card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+    size_t end_card_index = card_index + ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+    first_cluster++;\n+    size_t next_card_index = 0;\n+    while (card_index < end_card_index) {\n+      if (_rs->addr_for_card_index(card_index) > end_of_range) {\n+        count = 0;\n+        card_index = end_card_index;\n+        break;\n+      }\n+      bool is_dirty = (write_table)? is_write_card_dirty(card_index): is_card_dirty(card_index);\n+      bool has_object = _scc->has_object(card_index);\n+      if (is_dirty) {\n+        size_t prev_card_index = card_index;\n+        if (has_object) {\n+          \/\/ Scan all objects that start within this card region.\n+          size_t start_offset = _scc->get_first_start(card_index);\n+          HeapWord *p = _rs->addr_for_card_index(card_index);\n+          HeapWord *card_start = p;\n+          HeapWord *endp = p + CardTable::card_size_in_words();\n+          assert(!r->is_humongous(), \"Process humongous regions elsewhere\");\n+\n+          if (endp > end_of_range) {\n+            endp = end_of_range;\n+            next_card_index = end_card_index;\n+          } else {\n+            \/\/ endp either points to start of next card region, or to the next object that needs to be scanned, which may\n+            \/\/ reside in some successor card region.\n+\n+            \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n+            \/\/ failure if endp points to end of heap.\n+            next_card_index = card_index + (endp - card_start) \/ CardTable::card_size_in_words();\n+          }\n+\n+          p += start_offset;\n+          while (p < endp) {\n+            oop obj = cast_to_oop(p);\n+\n+            \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+            if (!ctx || ctx->is_marked(obj)) {\n+              \/\/ Future TODO:\n+              \/\/ For improved efficiency, we might want to give special handling of obj->is_objArray().  In\n+              \/\/ particular, in that case, we might want to divide the effort for scanning of a very long object array\n+              \/\/ between multiple threads.  Also, skip parts of the array that are not marked as dirty.\n+              if (obj->is_objArray()) {\n+                objArrayOop array = objArrayOop(obj);\n+                int len = array->length();\n+                array->oop_iterate_range(cl, 0, len);\n+              } else if (obj->is_instance()) {\n+                obj->oop_iterate(cl);\n+              } else {\n+                \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n+                \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n+                \/\/ We skip iterating over the klass pointer since we know that\n+                \/\/ Universe::TypeArrayKlass never moves.\n+                assert (obj->is_typeArray(), \"should be type array\");\n+              }\n+              p += obj->size();\n+            } else {\n+              \/\/ This object is not marked so we don't scan it.  Containing region r is initialized above.\n+              HeapWord* tams = ctx->top_at_mark_start(r);\n+              if (p >= tams) {\n+                p += obj->size();\n+              } else {\n+                p = ctx->get_next_marked_addr(p, tams);\n+              }\n+            }\n+          }\n+          if (p > endp) {\n+            card_index = card_index + (p - card_start) \/ CardTable::card_size_in_words();\n+          } else {                  \/\/ p == endp\n+            card_index = next_card_index;\n+          }\n+        } else {\n+          \/\/ Card is dirty but has no object.  Card will have been scanned during scan of a previous cluster.\n+          card_index++;\n+        }\n+      } else if (has_object) {\n+        \/\/ Card is clean but has object.\n+\n+        \/\/ Scan the last object that starts within this card memory if it spans at least one dirty card within this cluster\n+        \/\/ or if it reaches into the next cluster.\n+        size_t start_offset = _scc->get_last_start(card_index);\n+        HeapWord *card_start = _rs->addr_for_card_index(card_index);\n+        HeapWord *p = card_start + start_offset;\n+        oop obj = cast_to_oop(p);\n+\n+        size_t last_card;\n+        if (!ctx || ctx->is_marked(obj)) {\n+          HeapWord *nextp = p + obj->size();\n+\n+          \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n+          \/\/ failure if nextp points to end of heap. Must also not attempt to read past last\n+          \/\/ valid index for card table.\n+          last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words();\n+          last_card = MIN2(last_card, last_valid_index());\n+\n+          bool reaches_next_cluster = (last_card > end_card_index);\n+          bool spans_dirty_within_this_cluster = false;\n+\n+          if (!reaches_next_cluster) {\n+            size_t span_card;\n+            for (span_card = card_index+1; span_card <= last_card; span_card++)\n+              if ((write_table)? _rs->is_write_card_dirty(span_card): _rs->is_card_dirty(span_card)) {\n+                spans_dirty_within_this_cluster = true;\n+                break;\n+              }\n+          }\n+\n+          \/\/ TODO: only iterate over this object if it spans dirty within this cluster or within following clusters.\n+          \/\/ Code as written is known not to examine a zombie object because either the object is marked, or we are\n+          \/\/ not using the mark-context to differentiate objects, so the object is known to have been coalesced and\n+          \/\/ filled if it is not \"live\".\n+\n+          if (reaches_next_cluster || spans_dirty_within_this_cluster) {\n+            if (obj->is_objArray()) {\n+              objArrayOop array = objArrayOop(obj);\n+              int len = array->length();\n+              array->oop_iterate_range(cl, 0, len);\n+            } else if (obj->is_instance()) {\n+              obj->oop_iterate(cl);\n+            } else {\n+              \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n+              \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n+              \/\/ We skip iterating over the klass pointer since we know that\n+              \/\/ Universe::TypeArrayKlass never moves.\n+              assert (obj->is_typeArray(), \"should be type array\");\n+            }\n+          }\n+        } else {\n+          \/\/ The object that spans end of this clean card is not marked, so no need to scan it or its\n+          \/\/ unmarked neighbors.  Containing region r is initialized above.\n+          HeapWord* tams = ctx->top_at_mark_start(r);\n+          HeapWord* nextp;\n+          if (p >= tams) {\n+            nextp = p + obj->size();\n+          } else {\n+            nextp = ctx->get_next_marked_addr(p, tams);\n+          }\n+          last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words();\n+        }\n+        \/\/ Increment card_index to account for the spanning object, even if we didn't scan it.\n+        card_index = (last_card > card_index)? last_card: card_index + 1;\n+      } else {\n+        \/\/ Card is clean and has no object.  No need to clean this card.\n+        card_index++;\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ Given that this range of clusters is known to span a humongous object spanned by region r, scan the\n+\/\/ portion of the humongous object that corresponds to the specified range.\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                                                    HeapWord *end_of_range, ClosureType *cl, bool write_table,\n+                                                                    bool is_concurrent) {\n+  ShenandoahHeapRegion* start_region = r->humongous_start_region();\n+  HeapWord* p = start_region->bottom();\n+  oop obj = cast_to_oop(p);\n+  assert(r->is_humongous(), \"Only process humongous regions here\");\n+  assert(start_region->is_humongous_start(), \"Should be start of humongous region\");\n+  assert(p + obj->size() >= end_of_range, \"Humongous object ends before range ends\");\n+\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  HeapWord* first_cluster_addr = _rs->addr_for_card_index(first_card_index);\n+  size_t spanned_words = count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster * CardTable::card_size_in_words();\n+\n+  start_region->oop_iterate_humongous_slice(cl, true, first_cluster_addr, spanned_words, write_table, is_concurrent);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl, bool is_concurrent) {\n+  process_region(region, cl, false, is_concurrent);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl,\n+                                                        bool use_write_table, bool is_concurrent) {\n+  size_t cluster_size =\n+    CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t clusters = ShenandoahHeapRegion::region_size_words() \/ cluster_size;\n+  process_region_slice(region, 0, clusters, region->end(), cl, use_write_table, is_concurrent);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region_slice(ShenandoahHeapRegion *region, size_t start_offset, size_t clusters,\n+                                                              HeapWord *end_of_range, ClosureType *cl, bool use_write_table,\n+                                                              bool is_concurrent) {\n+  HeapWord *start_of_range = region->bottom() + start_offset;\n+  size_t cluster_size =\n+    CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t words = clusters * cluster_size;\n+  size_t start_cluster_no = cluster_for_addr(start_of_range);\n+  assert(addr_for_cluster(start_cluster_no) == start_of_range, \"process_region_slice range must align on cluster boundary\");\n+\n+  \/\/ region->end() represents the end of memory spanned by this region, but not all of this\n+  \/\/   memory is eligible to be scanned because some of this memory has not yet been allocated.\n+  \/\/\n+  \/\/ region->top() represents the end of allocated memory within this region.  Any addresses\n+  \/\/   beyond region->top() should not be scanned as that memory does not hold valid objects.\n+\n+  if (use_write_table) {\n+    \/\/ This is update-refs servicing.\n+    if (end_of_range > region->get_update_watermark()) {\n+      end_of_range = region->get_update_watermark();\n+    }\n+  } else {\n+    \/\/ This is concurrent mark servicing.  Note that TAMS for this region is TAMS at start of old-gen\n+    \/\/ collection.  Here, we need to scan up to TAMS for most recently initiated young-gen collection.\n+    \/\/ Since all LABs are retired at init mark, and since replacement LABs are allocated lazily, and since no\n+    \/\/ promotions occur until evacuation phase, TAMS for most recent young-gen is same as top().\n+    if (end_of_range > region->top()) {\n+      end_of_range = region->top();\n+    }\n+  }\n+\n+  log_debug(gc)(\"Remembered set scan processing Region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT \", using %s table\",\n+                region->index(), p2i(start_of_range), p2i(end_of_range),\n+                use_write_table? \"read\/write (updating)\": \"read (marking)\");\n+\n+  \/\/ Note that end_of_range may point to the middle of a cluster because region->top() or region->get_update_watermark() may\n+  \/\/ be less than start_of_range + words.\n+\n+  \/\/ We want to assure that our process_clusters() request spans all relevant clusters.  Note that each cluster\n+  \/\/ processed will avoid processing beyond end_of_range.\n+\n+  \/\/ Note that any object that starts between start_of_range and end_of_range, including humongous objects, will\n+  \/\/ be fully processed by process_clusters, even though the object may reach beyond end_of_range.\n+\n+  \/\/ If I am assigned to process a range that starts beyond end_of_range (top or update-watermark), we have no work to do.\n+\n+  if (start_of_range < end_of_range) {\n+    if (region->is_humongous()) {\n+      ShenandoahHeapRegion* start_region = region->humongous_start_region();\n+      process_humongous_clusters(start_region, start_cluster_no, clusters, end_of_range, cl, use_write_table, is_concurrent);\n+    } else {\n+      process_clusters(start_cluster_no, clusters, end_of_range, cl, use_write_table, is_concurrent);\n+    }\n+  }\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::cluster_for_addr(HeapWordImpl **addr) {\n+  size_t card_index = _rs->card_index_for_addr(addr);\n+  size_t result = card_index \/ ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  return result;\n+}\n+\n+template<typename RememberedSet>\n+inline HeapWord*\n+ShenandoahScanRemembered<RememberedSet>::addr_for_cluster(size_t cluster_no) {\n+  size_t card_index = cluster_no * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  return addr_for_card_index(card_index);\n+}\n+\n+\/\/ This is used only for debug verification so don't worry about making the scan parallel.\n+template<typename RememberedSet>\n+inline void ShenandoahScanRemembered<RememberedSet>::roots_do(OopIterateClosure* cl) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  for (size_t i = 0, n = heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (region->is_old() && region->is_active() && !region->is_cset()) {\n+      HeapWord* start_of_range = region->bottom();\n+      HeapWord* end_of_range = region->top();\n+      size_t start_cluster_no = cluster_for_addr(start_of_range);\n+      size_t num_heapwords = end_of_range - start_of_range;\n+      unsigned int cluster_size = CardTable::card_size_in_words() *\n+                                  ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+      size_t num_clusters = (size_t) ((num_heapwords - 1 + cluster_size) \/ cluster_size);\n+\n+      \/\/ Remembered set scanner\n+      if (region->is_humongous()) {\n+        process_humongous_clusters(region->humongous_start_region(), start_cluster_no, num_clusters, end_of_range, cl,\n+                                   false \/* is_write_table *\/, false \/* is_concurrent *\/);\n+      } else {\n+        process_clusters(start_cluster_no, num_clusters, end_of_range, cl, false \/* is_concurrent *\/);\n+      }\n+    }\n+  }\n+}\n+\n+inline bool ShenandoahRegionChunkIterator::has_next() const {\n+  return _index < _total_chunks;\n+}\n+\n+inline bool ShenandoahRegionChunkIterator::next(struct ShenandoahRegionChunk *assignment) {\n+  if (_index > _total_chunks) {\n+    return false;\n+  }\n+  size_t new_index = Atomic::add(&_index, (size_t) 1, memory_order_relaxed);\n+  if (new_index > _total_chunks) {\n+    return false;\n+  }\n+  \/\/ convert to zero-based indexing\n+  new_index--;\n+\n+  size_t group_no = new_index \/ _group_size;\n+  if (group_no + 1 > _num_groups) {\n+    group_no = _num_groups - 1;\n+  }\n+\n+  \/\/ All size computations measured in HeapWord\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t group_region_index = _region_index[group_no];\n+  size_t group_region_offset = _group_offset[group_no];\n+\n+  size_t index_within_group = new_index - (group_no * _group_size);\n+  size_t group_chunk_size = _first_group_chunk_size >> group_no;\n+  size_t offset_of_this_chunk = group_region_offset + index_within_group * group_chunk_size;\n+  size_t regions_spanned_by_chunk_offset = offset_of_this_chunk \/ region_size_words;\n+  size_t region_index = group_region_index + regions_spanned_by_chunk_offset;\n+  size_t offset_within_region = offset_of_this_chunk % region_size_words;\n+\n+  assignment->_r = _heap->get_region(region_index);\n+  assignment->_chunk_offset = offset_within_region;\n+  assignment->_chunk_size = group_chunk_size;\n+\n+  return true;\n+}\n+\n+#endif   \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":842,"deletions":0,"binary":false,"changes":842,"status":"added"},{"patch":"@@ -52,1 +52,1 @@\n-    _marking_context(ShenandoahHeap::heap()->complete_marking_context()),\n+    _marking_context(ShenandoahHeap::heap()->marking_context()),\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -36,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -71,0 +74,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -82,1 +86,2 @@\n-    _loc(NULL) {\n+    _loc(NULL),\n+    _generation(NULL) {\n@@ -87,0 +92,5 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->active_generation();\n+      assert(_generation != NULL, \"Expected active generation in this mode\");\n+    }\n@@ -110,1 +120,1 @@\n-      if (_map->par_mark(obj)) {\n+      if ( in_generation(obj) && _map->par_mark(obj)) {\n@@ -117,0 +127,9 @@\n+  bool in_generation(oop obj) {\n+    if (_generation == NULL) {\n+      return true;\n+    }\n+\n+    ShenandoahHeapRegion* region = _heap->heap_region_containing(obj);\n+    return _generation->contains(region);\n+  }\n+\n@@ -127,1 +146,1 @@\n-    ShenandoahHeapRegion *obj_reg = _heap->heap_region_containing(obj);\n+    ShenandoahHeapRegion* obj_reg = _heap->heap_region_containing(obj);\n@@ -138,1 +157,1 @@\n-      HeapWord *obj_addr = cast_from_oop<HeapWord*>(obj);\n+      HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n@@ -167,1 +186,2 @@\n-          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live(),\n+          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live() ||\n+                (obj_reg->is_old() && ShenandoahHeap::heap()->is_gc_generation_young()),\n@@ -202,1 +222,1 @@\n-      HeapWord *fwd_addr = cast_from_oop<HeapWord *>(fwd);\n+      HeapWord* fwd_addr = cast_from_oop<HeapWord* >(fwd);\n@@ -216,1 +236,8 @@\n-\n+    \/\/ We allow for marked or old here for two reasons:\n+    \/\/  1. If this is a young collect, old objects wouldn't be marked. We've\n+    \/\/     recently change the verifier traversal to only follow young objects\n+    \/\/     during a young collect so this _shouldn't_ be necessary.\n+    \/\/  2. At present, we do not clear dead objects from the remembered set.\n+    \/\/     Everything in the remembered set is old (ipso facto), so allowing for\n+    \/\/     'marked_or_old' covers the case of stale objects in rset.\n+    \/\/ TODO: Just use 'is_marked' here.\n@@ -222,1 +249,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->marking_context()->is_marked_or_old(obj),\n@@ -226,1 +253,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked_or_old(obj),\n@@ -230,1 +257,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked_or_old(obj),\n@@ -324,0 +351,2 @@\n+    log_debug(gc)(\"ShenandoahCalculatRegionStatsClosure added \" SIZE_FORMAT \" for %s Region \" SIZE_FORMAT \", yielding: \" SIZE_FORMAT,\n+                  r->used(), r->is_humongous()? \"humongous\": \"regular\", r->index(), _used);\n@@ -333,0 +362,39 @@\n+class ShenandoahGenerationStatsClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  ShenandoahCalculateRegionStatsClosure old;\n+  ShenandoahCalculateRegionStatsClosure young;\n+  ShenandoahCalculateRegionStatsClosure global;\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    switch (r->affiliation()) {\n+      default:\n+        ShouldNotReachHere();\n+        return;\n+      case FREE: return;\n+      case YOUNG_GENERATION:\n+        young.heap_region_do(r);\n+        break;\n+      case OLD_GENERATION:\n+        old.heap_region_do(r);\n+        break;\n+    }\n+    global.heap_region_do(r);\n+  }\n+\n+  static void log_usage(ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    log_debug(gc)(\"Safepoint verification: %s verified usage: \" SIZE_FORMAT \"%s, recorded usage: \" SIZE_FORMAT \"%s\",\n+                  generation->name(),\n+                  byte_size_in_proper_unit(generation->used()), proper_unit_for_byte_size(generation->used()),\n+                  byte_size_in_proper_unit(stats.used()), proper_unit_for_byte_size(stats.used()));\n+  }\n+\n+  static void validate_usage(const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    size_t generation_used = generation->used();\n+    guarantee(stats.used() == generation_used,\n+              \"%s: generation (%s) used size must be consistent: generation-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+              label, generation->name(),\n+              byte_size_in_proper_unit(generation_used), proper_unit_for_byte_size(generation_used),\n+              byte_size_in_proper_unit(stats.used()), proper_unit_for_byte_size(stats.used()));\n+  }\n+};\n+\n@@ -414,2 +482,5 @@\n-    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() == r->used(),\n-           \"Accurate accounting: shared + TLAB + GCLAB = used\");\n+    verify(r, r->get_plab_allocs() <= r->capacity(),\n+           \"PLAB alloc count should not be larger than capacity\");\n+\n+    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() + r->get_plab_allocs() == r->used(),\n+           \"Accurate accounting: shared + TLAB + GCLAB + PLAB = used\");\n@@ -492,1 +563,1 @@\n-  ShenandoahHeap *_heap;\n+  ShenandoahHeap* _heap;\n@@ -497,0 +568,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -510,1 +582,7 @@\n-          _processed(0) {};\n+          _processed(0),\n+          _generation(NULL) {\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->active_generation();\n+      assert(_generation != NULL, \"Expected active generation in this mode.\");\n+    }\n+  };\n@@ -526,0 +604,4 @@\n+        if (!in_generation(r)) {\n+          continue;\n+        }\n+\n@@ -537,1 +619,5 @@\n-  virtual void work_humongous(ShenandoahHeapRegion *r, ShenandoahVerifierStack& stack, ShenandoahVerifyOopClosure& cl) {\n+  bool in_generation(ShenandoahHeapRegion* r) {\n+    return _generation == NULL || _generation->contains(r);\n+  }\n+\n+  virtual void work_humongous(ShenandoahHeapRegion* r, ShenandoahVerifierStack& stack, ShenandoahVerifyOopClosure& cl) {\n@@ -546,1 +632,1 @@\n-  virtual void work_regular(ShenandoahHeapRegion *r, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl) {\n+  virtual void work_regular(ShenandoahHeapRegion* r, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl) {\n@@ -579,1 +665,1 @@\n-  void verify_and_follow(HeapWord *addr, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl, size_t *processed) {\n+  void verify_and_follow(HeapWord* addr, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl, size_t* processed) {\n@@ -609,1 +695,1 @@\n-    if (actual != _expected) {\n+    if (actual != _expected && !(actual & ShenandoahHeap::OLD_MARKING)) {\n@@ -615,1 +701,2 @@\n-void ShenandoahVerifier::verify_at_safepoint(const char *label,\n+void ShenandoahVerifier::verify_at_safepoint(const char* label,\n+                                             VerifyRememberedSet remembered,\n@@ -648,0 +735,4 @@\n+      case _verify_gcstate_updating:\n+        enabled = true;\n+        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::UPDATEREFS;\n+        break;\n@@ -667,1 +758,2 @@\n-      if (actual != expected) {\n+      \/\/ Old generation marking is allowed in all states.\n+      if (actual != expected && !(actual & ShenandoahHeap::OLD_MARKING)) {\n@@ -685,0 +777,1 @@\n+\n@@ -686,0 +779,1 @@\n+\n@@ -700,0 +794,45 @@\n+  log_debug(gc)(\"Safepoint verification finished heap usage verification\");\n+\n+  ShenandoahGeneration* generation;\n+  if (_heap->mode()->is_generational()) {\n+    generation = _heap->active_generation();\n+    guarantee(generation != NULL, \"Need to know which generation to verify.\");\n+  } else {\n+    generation = NULL;\n+  }\n+\n+  if (generation != NULL) {\n+    ShenandoahHeapLocker lock(_heap->lock());\n+\n+    if (remembered == _verify_remembered_for_marking) {\n+      log_debug(gc)(\"Safepoint verification of remembered set at mark\");\n+    } else if (remembered == _verify_remembered_for_updating_references) {\n+      log_debug(gc)(\"Safepoint verification of remembered set at update ref\");\n+    } else if (remembered == _verify_remembered_after_full_gc) {\n+      log_debug(gc)(\"Safepoint verification of remembered set after full gc\");\n+    }\n+\n+    if (remembered == _verify_remembered_for_marking) {\n+      _heap->verify_rem_set_at_mark();\n+    } else if (remembered == _verify_remembered_for_updating_references) {\n+      _heap->verify_rem_set_at_update_ref();\n+    } else if (remembered == _verify_remembered_after_full_gc) {\n+      _heap->verify_rem_set_after_full_gc();\n+    }\n+\n+    ShenandoahGenerationStatsClosure cl;\n+    _heap->heap_region_iterate(&cl);\n+\n+    if (LogTarget(Debug, gc)::is_enabled()) {\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl.global);\n+    }\n+\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->old_generation(), cl.old);\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->young_generation(), cl.young);\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->global_generation(), cl.global);\n+  }\n+\n+  log_debug(gc)(\"Safepoint verification finished remembered set verification\");\n+\n@@ -703,1 +842,5 @@\n-    _heap->heap_region_iterate(&cl);\n+    if (generation != NULL) {\n+      generation->heap_region_iterate(&cl);\n+    } else {\n+      _heap->heap_region_iterate(&cl);\n+    }\n@@ -706,0 +849,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap region closure verification\");\n+\n@@ -730,0 +875,2 @@\n+  log_debug(gc)(\"Safepoint verification finished getting initial reachable set\");\n+\n@@ -747,0 +894,2 @@\n+  log_debug(gc)(\"Safepoint verification finished walking marked objects\");\n+\n@@ -753,0 +902,3 @@\n+      if (generation != NULL && !generation->contains(r)) {\n+        continue;\n+      }\n@@ -776,0 +928,3 @@\n+  log_debug(gc)(\"Safepoint verification finished accumulation of liveness data\");\n+\n+\n@@ -785,0 +940,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -797,0 +953,1 @@\n+          _verify_remembered_for_marking,  \/\/ verify read-only remembered set from bottom() to top()\n@@ -809,0 +966,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -821,0 +979,1 @@\n+          _verify_remembered_disable,                \/\/ do not verify remembered set\n@@ -833,0 +992,1 @@\n+          _verify_remembered_disable, \/\/ do not verify remembered set\n@@ -845,0 +1005,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -857,6 +1018,7 @@\n-          _verify_forwarded_allow,     \/\/ forwarded references allowed\n-          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n-          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n-          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n-          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n-          _verify_gcstate_forwarded    \/\/ evacuation should have produced some forwarded objects\n+          _verify_remembered_for_updating_references,  \/\/ verify read-write remembered set\n+          _verify_forwarded_allow,                     \/\/ forwarded references allowed\n+          _verify_marked_complete,                     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n+          _verify_cset_forwarded,                      \/\/ all cset refs are fully forwarded\n+          _verify_liveness_disable,                    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash,                     \/\/ trash regions have been recycled already\n+          _verify_gcstate_updating                     \/\/ evacuation should have produced some forwarded objects\n@@ -869,0 +1031,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -881,0 +1044,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -893,0 +1057,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -902,0 +1067,13 @@\n+void ShenandoahVerifier::verify_after_generational_fullgc() {\n+  verify_at_safepoint(\n+          \"After Full Generational GC\",\n+          _verify_remembered_after_full_gc,  \/\/ verify read-write remembered set\n+          _verify_forwarded_none,      \/\/ all objects are non-forwarded\n+          _verify_marked_complete,     \/\/ all objects are marked in complete bitmap\n+          _verify_cset_none,           \/\/ no cset references\n+          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash_nocset, \/\/ no trash, no cset\n+          _verify_gcstate_stable       \/\/ full gc cleaned up everything\n+  );\n+}\n+\n@@ -905,0 +1083,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -914,1 +1093,1 @@\n-class ShenandoahVerifyNoForwared : public OopClosure {\n+class ShenandoahVerifyNoForwared : public BasicOopIterateClosure {\n@@ -934,1 +1113,1 @@\n-class ShenandoahVerifyInToSpaceClosure : public OopClosure {\n+class ShenandoahVerifyInToSpaceClosure : public BasicOopIterateClosure {\n@@ -943,1 +1122,1 @@\n-      if (!heap->marking_context()->is_marked(obj)) {\n+      if (!heap->marking_context()->is_marked_or_old(obj)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":209,"deletions":30,"binary":false,"changes":239,"status":"modified"}]}