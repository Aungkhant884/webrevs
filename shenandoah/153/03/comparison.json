{"files":[{"patch":"@@ -125,1 +125,1 @@\n-      _generation->scan_remembered_set();\n+      _generation->scan_remembered_set(true \/* is_concurrent *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -784,1 +784,1 @@\n-void ShenandoahGeneration::scan_remembered_set() {\n+void ShenandoahGeneration::scan_remembered_set(bool is_concurrent) {\n@@ -792,2 +792,2 @@\n-  ShenandoahRegionIterator regions;\n-  ShenandoahScanRememberedTask task(task_queues(), old_gen_task_queues(), rp, &regions);\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n+  ShenandoahScanRememberedTask task(task_queues(), old_gen_task_queues(), rp, &work_list, is_concurrent);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -163,1 +163,1 @@\n-  void scan_remembered_set();\n+  void scan_remembered_set(bool is_concurrent);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2473,0 +2473,1 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n@@ -2475,1 +2476,2 @@\n-  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -2478,1 +2480,2 @@\n-    _regions(regions)\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n@@ -2507,0 +2510,1 @@\n+      bool region_progress = false;\n@@ -2510,0 +2514,1 @@\n+          region_progress = true;\n@@ -2512,0 +2517,6 @@\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n@@ -2513,47 +2524,1 @@\n-          } else {\n-            \/\/ Old region in a young cycle or mixed cycle.\n-            if (!is_mixed) {\n-              \/\/ This is a young evac..\n-              _heap->card_scan()->process_region(r, &cl, true);\n-            } else {\n-              \/\/ This is a _mixed_evac.\n-              \/\/\n-              \/\/ TODO: For _mixed_evac, consider building an old-gen remembered set that allows restricted updating\n-              \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n-              \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n-              \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n-              \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n-              \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n-              \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n-              \/\/ old-gen heap regions.\n-              if (r->is_humongous()) {\n-                \/\/ Need to examine both dirty and clean cards during mixed evac.\n-                r->oop_iterate_humongous(&cl);\n-              } else {\n-                \/\/ This is a mixed evacuation.  Old regions that are candidates for collection have not been coalesced\n-                \/\/ and filled.  Use mark bits to find objects that need to be updated.\n-                \/\/\n-                \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n-                \/\/ regions which are in the collection set for a particular mixed evacuation.\n-                HeapWord *p = r->bottom();\n-                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, p, update_watermark);\n-\n-                \/\/ Anything beyond update_watermark was allocated during evacuation.  Thus, it is known to not hold\n-                \/\/ references to collection set objects.\n-                while (p < update_watermark) {\n-                  oop obj = cast_to_oop(p);\n-                  if (ctx->is_marked(obj)) {\n-                    objs.do_object(obj);\n-                    p += obj->size();\n-                  } else {\n-                    \/\/ This object is not marked so we don't scan it.\n-                    HeapWord* tams = ctx->top_at_mark_start(r);\n-                    if (p >= tams) {\n-                      p += obj->size();\n-                    } else {\n-                      p = ctx->get_next_marked_addr(p, tams);\n-                    }\n-                  }\n-                }\n-              }\n-            }\n+            region_progress = true;\n@@ -2561,0 +2526,2 @@\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n@@ -2577,1 +2544,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2585,0 +2552,119 @@\n+    if (_heap->mode()->is_generational() && (_heap->active_generation()->generation_mode() != GLOBAL)) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      bool have_work = _work_chunks->next(&assignment);\n+      RememberedScanner* scanner = _heap->card_scan();\n+      while (have_work) {\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION)) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true, CONCURRENT);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, CONCURRENT);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+        \/\/ Otherwise, this work chunk had nothing for me to do, so do not report pacer progress.\n+\n+        \/\/ Before we take responsibility for another chunk of work, see if cancellation is requested.\n+        if (_heap->check_cancelled_gc_and_yield(CONCURRENT)) {\n+          return;\n+        }\n+        have_work = _work_chunks->next(&assignment);\n+      }\n+    }\n@@ -2590,0 +2676,1 @@\n+  ShenandoahRegionChunkIterator work_list(workers()->active_workers());\n@@ -2592,1 +2679,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2595,1 +2682,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2600,1 +2687,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":139,"deletions":53,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -119,1 +119,1 @@\n-      _generation->scan_remembered_set();\n+      _generation->scan_remembered_set(false \/* is_concurrent *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-                                                           ShenandoahRegionIterator* regions) :\n+                                                           ShenandoahRegionChunkIterator* work_list, bool is_concurrent) :\n@@ -92,1 +92,1 @@\n-  _queue_set(queue_set), _old_queue_set(old_queue_set), _rp(rp), _regions(regions) {}\n+  _queue_set(queue_set), _old_queue_set(old_queue_set), _rp(rp), _work_list(work_list), _is_concurrent(is_concurrent) {}\n@@ -95,3 +95,13 @@\n-  \/\/ This sets up a thread local reference to the worker_id which is necessary\n-  \/\/ the weak reference processor.\n-  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  if (_is_concurrent) {\n+    \/\/ This sets up a thread local reference to the worker_id which is needed by the weak reference processor.\n+    ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+    ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+    do_work(worker_id);\n+  } else {\n+    \/\/ This sets up a thread local reference to the worker_id which is needed by the weak reference processor.\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    do_work(worker_id);\n+  }\n+}\n+\n+void ShenandoahScanRememberedTask::do_work(uint worker_id) {\n@@ -103,1 +113,2 @@\n-  RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  RememberedScanner* scanner = heap->card_scan();\n@@ -107,3 +118,14 @@\n-  ShenandoahHeapRegion* region = _regions->next();\n-  while (region != NULL) {\n-    log_debug(gc)(\"ShenandoahScanRememberedTask::work(%u), looking at region \" SIZE_FORMAT, worker_id, region->index());\n+  struct ShenandoahRegionChunk assignment;\n+  bool has_work = _work_list->next(&assignment);\n+  while (has_work) {\n+#ifdef ENABLE_REMEMBERED_SET_CANCELLATION\n+    \/\/ This check is currently disabled to avoid crashes that occur\n+    \/\/ when we try to cancel remembered set scanning\n+    if (heap->check_cancelled_gc_and_yield(_is_concurrent)) {\n+      return;\n+    }\n+#endif\n+    ShenandoahHeapRegion* region = assignment._r;\n+    log_debug(gc)(\"ShenandoahScanRememberedTask::do_work(%u), processing slice of region \"\n+                  SIZE_FORMAT \" at offset \" SIZE_FORMAT \", size: \" SIZE_FORMAT,\n+                  worker_id, region->index(), assignment._chunk_offset, assignment._chunk_size);\n@@ -111,1 +133,61 @@\n-      scanner->process_region(region, &cl);\n+      size_t cluster_size =\n+        CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+      size_t clusters = assignment._chunk_size \/ cluster_size;\n+      assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignments must align on cluster boundaries\");\n+      HeapWord* end_of_range = region->bottom() + assignment._chunk_offset + assignment._chunk_size;\n+\n+      \/\/ During concurrent mark, region->top() equals TAMS with respect to the current young-gen pass.  *\/\n+      if (end_of_range > region->top()) {\n+        end_of_range = region->top();\n+      }\n+      scanner->process_region_slice(region, assignment._chunk_offset, clusters, end_of_range, &cl, false, _is_concurrent);\n+    }\n+    has_work = _work_list->next(&assignment);\n+  }\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_group_size() {\n+  \/\/ The group size s calculated from the number of regions.  Every group except the last processes the same number of chunks.\n+  \/\/ The last group processes however many chunks are required to finish the total scanning effort.  The chunk sizes are\n+  \/\/ different for each group.  The intention is that the first group processes roughly half of the heap, the second processes\n+  \/\/ a quarter of the remaining heap, the third processes an eight of what remains and so on.  The smallest chunk size\n+  \/\/ is represented by _smallest_chunk_size.  We do not divide work any smaller than this.\n+  \/\/\n+  \/\/ Note that N\/2 + N\/4 + N\/8 + N\/16 + ...  sums to N if expanded to infinite terms.\n+  return _heap->num_regions() \/ 2;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_first_group_chunk_size() {\n+  size_t words_in_region = ShenandoahHeapRegion::region_size_words();\n+  return words_in_region;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_num_groups() {\n+  size_t total_heap_size = _heap->num_regions() * ShenandoahHeapRegion::region_size_words();\n+  size_t num_groups = 0;\n+  size_t cumulative_group_span = 0;\n+  size_t current_group_span = _first_group_chunk_size * _group_size;\n+  size_t smallest_group_span = _smallest_chunk_size * _group_size;\n+  while ((num_groups < _maximum_groups) && (cumulative_group_span + current_group_span <= total_heap_size)) {\n+    num_groups++;\n+    cumulative_group_span += current_group_span;\n+    if (current_group_span <= smallest_group_span) {\n+      break;\n+    } else {\n+      current_group_span \/= 2;    \/\/ Each group spans half of what the preceding group spanned.\n+    }\n+  }\n+  \/\/ Loop post condition:\n+  \/\/   num_groups <= _maximum_groups\n+  \/\/   cumulative_group_span is the memory spanned by num_groups\n+  \/\/   current_group_span is the span of the last fully populated group (assuming loop iterates at least once)\n+  \/\/   each of num_groups is fully populated with _group_size chunks in each\n+  \/\/ Non post conditions:\n+  \/\/   cumulative_group_span may be less than total_heap size for one or more of the folowing reasons\n+  \/\/   a) The number of regions remaining to be spanned is smaller than a complete group, or\n+  \/\/   b) We have filled up all groups through _maximum_groups and still have not spanned all regions\n+\n+  if (cumulative_group_span < total_heap_size) {\n+    \/\/ We've got more regions to span\n+    if ((num_groups < _maximum_groups) && (current_group_span > smallest_group_span)) {\n+      num_groups++;             \/\/ Place all remaining regions into a new not-full group (chunk_size half that of previous group)\n@@ -113,1 +195,86 @@\n-    region = _regions->next();\n+    \/\/ Else we are unable to create a new group because we've exceed the number of allowed groups or have reached the\n+    \/\/ minimum chunk size.\n+\n+    \/\/ Any remaining regions will be treated as if they are part of the most recently created group.  This group will\n+    \/\/ have more than _group_size chunks within it.\n+  }\n+  return num_groups;\n+}\n+\n+size_t ShenandoahRegionChunkIterator::calc_total_chunks() {\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t unspanned_heap_size = _heap->num_regions() * region_size_words;\n+  size_t num_chunks = 0;\n+  size_t spanned_groups = 0;\n+  size_t cumulative_group_span = 0;\n+  size_t current_group_span = _first_group_chunk_size * _group_size;\n+  size_t smallest_group_span = _smallest_chunk_size * _group_size;\n+  while (unspanned_heap_size > 0) {\n+    if (current_group_span <= unspanned_heap_size) {\n+      unspanned_heap_size -= current_group_span;\n+      num_chunks += _group_size;\n+      spanned_groups++;\n+\n+      \/\/ _num_groups is the number of groups required to span the configured heap size.  We are not allowed\n+      \/\/ to change the number of groups.  The last group is responsible for spanning all chunks not spanned\n+      \/\/ by previously processed groups.\n+      if (spanned_groups >= _num_groups) {\n+        \/\/ The last group has more than _group_size entries.\n+        size_t chunk_span = current_group_span \/ _group_size;\n+        size_t extra_chunks = unspanned_heap_size \/ chunk_span;\n+        assert (extra_chunks * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+        num_chunks += extra_chunks;\n+        return num_chunks;\n+      } else if (current_group_span <= smallest_group_span) {\n+        \/\/ We cannot introduce new groups because we've reached the lower bound on group size\n+        size_t chunk_span = _smallest_chunk_size;\n+        size_t extra_chunks = unspanned_heap_size \/ chunk_span;\n+        assert (extra_chunks * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+        num_chunks += extra_chunks;\n+        return num_chunks;\n+      } else {\n+        current_group_span \/= 2;\n+      }\n+    } else {\n+      \/\/ The last group has fewer than _group_size entries.\n+      size_t chunk_span = current_group_span \/ _group_size;\n+      size_t last_group_size = unspanned_heap_size \/ chunk_span;\n+      assert (last_group_size * chunk_span == unspanned_heap_size, \"Chunks must precisely span regions\");\n+      num_chunks += last_group_size;\n+      return num_chunks;\n+    }\n+  }\n+  return num_chunks;\n+}\n+\n+ShenandoahRegionChunkIterator::ShenandoahRegionChunkIterator(size_t worker_count) :\n+    ShenandoahRegionChunkIterator(ShenandoahHeap::heap(), worker_count)\n+{\n+}\n+\n+ShenandoahRegionChunkIterator::ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count) :\n+    _heap(heap),\n+    _group_size(calc_group_size()),\n+    _first_group_chunk_size(calc_first_group_chunk_size()),\n+    _num_groups(calc_num_groups()),\n+    _total_chunks(calc_total_chunks()),\n+    _index(0)\n+{\n+  assert(_smallest_chunk_size ==\n+         CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster,\n+         \"_smallest_chunk_size is not valid\");\n+\n+  size_t words_in_region = ShenandoahHeapRegion::region_size_words();\n+  size_t group_span = _first_group_chunk_size * _group_size;\n+\n+  _region_index[0] = 0;\n+  _group_offset[0] = 0;\n+  for (size_t i = 1; i < _num_groups; i++) {\n+    _region_index[i] = _region_index[i-1] + (_group_offset[i-1] + group_span) \/ words_in_region;\n+    _group_offset[i] = (_group_offset[i-1] + group_span) % words_in_region;\n+    group_span \/= 2;\n+  }\n+  \/\/ Not necessary, but keeps things tidy\n+  for (size_t i = _num_groups; i < _maximum_groups; i++) {\n+    _region_index[i] = 0;\n+    _group_offset[i] = 0;\n@@ -116,0 +283,4 @@\n+\n+void ShenandoahRegionChunkIterator::reset() {\n+  _index = 0;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":182,"deletions":11,"binary":false,"changes":193,"status":"modified"},{"patch":"@@ -212,0 +212,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n@@ -926,0 +927,1 @@\n+  HeapWord* addr_for_cluster(size_t cluster_no);\n@@ -932,0 +934,8 @@\n+  HeapWord* first_object_in_card(size_t card_index) {\n+    if (_scc->has_object(card_index)) {\n+      return addr_for_card_index(card_index) + _scc->get_first_start(card_index);\n+    } else {\n+      return nullptr;\n+    }\n+  }\n+\n@@ -970,1 +980,10 @@\n-  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops);\n+  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops, bool is_concurrent);\n+\n+  template <typename ClosureType>\n+  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops,\n+                               bool use_write_table, bool is_concurrent);\n+\n+  template <typename ClosureType>\n+  inline void process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                         HeapWord *end_of_range, ClosureType *oops, bool use_write_table, bool is_concurrent);\n+\n@@ -973,1 +992,1 @@\n-  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops, bool use_write_table);\n+  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl, bool is_concurrent);\n@@ -976,1 +995,1 @@\n-  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl);\n+  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl, bool use_write_table, bool is_concurrent);\n@@ -979,1 +998,2 @@\n-  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl, bool use_write_table);\n+  inline void process_region_slice(ShenandoahHeapRegion* region, size_t offset, size_t clusters, HeapWord* end_of_range,\n+                                   ClosureType *cl, bool use_write_table, bool is_concurrent);\n@@ -1005,0 +1025,74 @@\n+struct ShenandoahRegionChunk {\n+  ShenandoahHeapRegion *_r;\n+  size_t _chunk_offset;          \/\/ HeapWordSize offset\n+  size_t _chunk_size;            \/\/ HeapWordSize qty\n+};\n+\n+class ShenandoahRegionChunkIterator : public StackObj {\n+private:\n+  \/\/ smallest_chunk_size is 64 words per card *\n+  \/\/ ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster.\n+  \/\/ This is computed from CardTable::card_size_in_words() *\n+  \/\/      ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  \/\/ We can't perform this computation here, because of encapsulation and initialization constraints.  We paste\n+  \/\/ the magic number here, and assert that this number matches the intended computation in constructor.\n+  static const size_t _smallest_chunk_size = 64 * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+\n+  \/\/ The total remembered set scanning effort is divided into chunks of work that are assigned to individual worker tasks.\n+  \/\/ The chunks of assigned work are divided into groups, where the size of each group (_group_size) is 4 * the number of\n+  \/\/ worker tasks.  All of the assignments within a group represent the same amount of memory to be scanned.  Each of the\n+  \/\/ assignments within the first group are of size _first_group_chunk_size (typically the ShenandoahHeapRegion size, but\n+  \/\/ possibly smaller.  Each of the assignments within each subsequent group are half the size of the assignments in the\n+  \/\/ preceding group.  The last group may be larger than the others.  Because no group is allowed to have smaller assignments\n+  \/\/ than _smallest_chunk_size, which is 32 KB.\n+\n+  \/\/ Under normal circumstances, no configuration needs more than _maximum_groups (default value of 16).\n+\n+  static const size_t _maximum_groups = 16;\n+\n+  const ShenandoahHeap* _heap;\n+\n+  const size_t _group_size;                        \/\/ Number of chunks in each group, equals worker_threads * 8\n+  const size_t _first_group_chunk_size;\n+  const size_t _num_groups;                        \/\/ Number of groups in this configuration\n+  const size_t _total_chunks;\n+\n+  shenandoah_padding(0);\n+  volatile size_t _index;\n+  shenandoah_padding(1);\n+\n+  size_t _region_index[_maximum_groups];\n+  size_t _group_offset[_maximum_groups];\n+\n+\n+  \/\/ No implicit copying: iterators should be passed by reference to capture the state\n+  NONCOPYABLE(ShenandoahRegionChunkIterator);\n+\n+  \/\/ Makes use of _heap.\n+  size_t calc_group_size();\n+\n+  \/\/ Makes use of _group_size, which must be initialized before call.\n+  size_t calc_first_group_chunk_size();\n+\n+  \/\/ Makes use of _group_size and _first_group_chunk_size, both of which must be initialized before call.\n+  size_t calc_num_groups();\n+\n+  \/\/ Makes use of _group_size, _first_group_chunk_size, which must be initialized before call.\n+  size_t calc_total_chunks();\n+\n+public:\n+  ShenandoahRegionChunkIterator(size_t worker_count);\n+  ShenandoahRegionChunkIterator(ShenandoahHeap* heap, size_t worker_count);\n+\n+  \/\/ Reset iterator to default state\n+  void reset();\n+\n+  \/\/ Fills in assignment with next chunk of work and returns true iff there is more work.\n+  \/\/ Otherwise, returns false.  This is multi-thread-safe.\n+  inline bool next(struct ShenandoahRegionChunk *assignment);\n+\n+  \/\/ This is *not* MT safe. However, in the absence of multithreaded access, it\n+  \/\/ can be used to determine if there is more work to do.\n+  inline bool has_next() const;\n+};\n+\n@@ -1012,1 +1106,2 @@\n-  ShenandoahRegionIterator* _regions;\n+  ShenandoahRegionChunkIterator* _work_list;\n+  bool _is_concurrent;\n@@ -1017,1 +1112,2 @@\n-                               ShenandoahRegionIterator* regions);\n+                               ShenandoahRegionChunkIterator* work_list,\n+                               bool is_concurrent);\n@@ -1020,0 +1116,1 @@\n+  void do_work(uint worker_id);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":103,"deletions":6,"binary":false,"changes":109,"status":"modified"},{"patch":"@@ -452,1 +452,4 @@\n-        \/\/ offset will be zero if no objects are marked in this card.\n+        \/\/ If there are no marked objects remaining in this region, offset equals tams - base_addr.  If this offset is\n+        \/\/ greater than max_offset, we will immediately exit this loop.  Otherwise, the next iteration of the loop will\n+        \/\/ treat the object at offset as marked and live (because address >= tams) and we will continue iterating object\n+        \/\/ by consulting the size() fields of each.\n@@ -454,1 +457,1 @@\n-    } while (offset > 0 && offset < max_offset);\n+    } while (offset < max_offset);\n@@ -484,2 +487,2 @@\n-                                                          ClosureType *cl) {\n-  process_clusters(first_cluster, count, end_of_range, cl, false);\n+                                                          ClosureType *cl, bool is_concurrent) {\n+  process_clusters(first_cluster, count, end_of_range, cl, false, is_concurrent);\n@@ -489,2 +492,5 @@\n-\/\/ less than end_of_range.  For any such object, process the complete object, even if its end reaches beyond\n-\/\/ end_of_range.\n+\/\/ less than end_of_range.  For any such object, process the complete object, even if its end reaches beyond end_of_range.\n+\n+\/\/ Do not CANCEL within process_clusters.  It is assumed that if a worker thread accepts responsbility for processing\n+\/\/ a chunk of work, it will finish the work it starts.  Otherwise, the chunk of work will be lost in the transition to\n+\/\/ degenerated execution.\n@@ -495,1 +501,1 @@\n-                                                          ClosureType *cl, bool write_table) {\n+                                                          ClosureType *cl, bool write_table, bool is_concurrent) {\n@@ -517,2 +523,5 @@\n-  HeapWord* end_of_clusters = _rs->addr_for_card_index(first_cluster)\n-    + count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster * CardTable::card_size_in_words();\n+  size_t card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  HeapWord *start_of_range = _rs->addr_for_card_index(card_index);\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(start_of_range);\n+  assert(end_of_range <= r->top(), \"process_clusters() examines one region at a time\");\n+\n@@ -520,1 +529,3 @@\n-    size_t card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+    \/\/ TODO: do we want to check cancellation in inner loop, on every card processed?  That would be more responsive,\n+    \/\/ but require more overhead for checking.\n+    card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n@@ -525,0 +536,5 @@\n+      if (_rs->addr_for_card_index(card_index) > end_of_range) {\n+        count = 0;\n+        card_index = end_card_index;\n+        break;\n+      }\n@@ -535,0 +551,2 @@\n+          assert(!r->is_humongous(), \"Process humongous regions elsewhere\");\n+\n@@ -572,2 +590,1 @@\n-              \/\/ This object is not marked so we don't scan it.\n-              ShenandoahHeapRegion* r = heap->heap_region_containing(p);\n+              \/\/ This object is not marked so we don't scan it.  Containing region r is initialized above.\n@@ -621,0 +638,5 @@\n+          \/\/ TODO: only iterate over this object if it spans dirty within this cluster or within following clusters.\n+          \/\/ Code as written is known not to examine a zombie object because either the object is marked, or we are\n+          \/\/ not using the mark-context to differentiate objects, so the object is known to have been coalesced and\n+          \/\/ filled if it is not \"live\".\n+\n@@ -638,2 +660,1 @@\n-          \/\/ unmarked neighbors.\n-          ShenandoahHeapRegion* r = heap->heap_region_containing(p);\n+          \/\/ unmarked neighbors.  Containing region r is initialized above.\n@@ -659,0 +680,2 @@\n+\/\/ Given that this range of clusters is known to span a humongous object spanned by region r, scan the\n+\/\/ portion of the humongous object that corresponds to the specified range.\n@@ -662,2 +685,15 @@\n-ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl) {\n-  process_region(region, cl, false);\n+ShenandoahScanRemembered<RememberedSet>::process_humongous_clusters(ShenandoahHeapRegion* r, size_t first_cluster, size_t count,\n+                                                                    HeapWord *end_of_range, ClosureType *cl, bool write_table,\n+                                                                    bool is_concurrent) {\n+  ShenandoahHeapRegion* start_region = r->humongous_start_region();\n+  HeapWord* p = start_region->bottom();\n+  oop obj = cast_to_oop(p);\n+  assert(r->is_humongous(), \"Only process humongous regions here\");\n+  assert(start_region->is_humongous_start(), \"Should be start of humongous region\");\n+  assert(p + obj->size() >= end_of_range, \"Humongous object ends before range ends\");\n+\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  HeapWord* first_cluster_addr = _rs->addr_for_card_index(first_card_index);\n+  size_t spanned_words = count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster * CardTable::card_size_in_words();\n+\n+  start_region->oop_iterate_humongous_slice(cl, true, first_cluster_addr, spanned_words, write_table, is_concurrent);\n@@ -669,2 +705,25 @@\n-ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl, bool use_write_table) {\n-  HeapWord *start_of_range = region->bottom();\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl, bool is_concurrent) {\n+  process_region(region, cl, false, is_concurrent);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl,\n+                                                        bool use_write_table, bool is_concurrent) {\n+  size_t cluster_size =\n+    CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t clusters = ShenandoahHeapRegion::region_size_words() \/ cluster_size;\n+  process_region_slice(region, 0, clusters, region->end(), cl, use_write_table, is_concurrent);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region_slice(ShenandoahHeapRegion *region, size_t start_offset, size_t clusters,\n+                                                              HeapWord *end_of_range, ClosureType *cl, bool use_write_table,\n+                                                              bool is_concurrent) {\n+  HeapWord *start_of_range = region->bottom() + start_offset;\n+  size_t cluster_size =\n+    CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t words = clusters * cluster_size;\n@@ -672,0 +731,1 @@\n+  assert(addr_for_cluster(start_cluster_no) == start_of_range, \"process_region_slice range must align on cluster boundary\");\n@@ -679,1 +739,0 @@\n-  HeapWord *end_of_range;\n@@ -682,1 +741,3 @@\n-    end_of_range = region->get_update_watermark();\n+    if (end_of_range > region->get_update_watermark()) {\n+      end_of_range = region->get_update_watermark();\n+    }\n@@ -688,1 +749,3 @@\n-    end_of_range = region->top();\n+    if (end_of_range > region->top()) {\n+      end_of_range = region->top();\n+    }\n@@ -692,1 +755,1 @@\n-                region->index(), p2i(region->bottom()), p2i(end_of_range),\n+                region->index(), p2i(start_of_range), p2i(end_of_range),\n@@ -694,1 +757,4 @@\n-  \/\/ end_of_range may point to the middle of a cluster because region->top() may be different than region->end().\n+\n+  \/\/ Note that end_of_range may point to the middle of a cluster because region->top() or region->get_update_watermark() may\n+  \/\/ be less than start_of_range + words.\n+\n@@ -700,3 +766,0 @@\n-  size_t num_heapwords = end_of_range - start_of_range;\n-  unsigned int cluster_size = CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n-  size_t num_clusters = (size_t) ((num_heapwords - 1 + cluster_size) \/ cluster_size);\n@@ -704,3 +767,9 @@\n-  if (!region->is_humongous_continuation()) {\n-    \/\/ Remembered set scanner\n-    process_clusters(start_cluster_no, num_clusters, end_of_range, cl, use_write_table);\n+  \/\/ If I am assigned to process a range that starts beyond end_of_range (top or update-watermark), we have no work to do.\n+\n+  if (start_of_range < end_of_range) {\n+    if (region->is_humongous()) {\n+      ShenandoahHeapRegion* start_region = region->humongous_start_region();\n+      process_humongous_clusters(start_region, start_cluster_no, clusters, end_of_range, cl, use_write_table, is_concurrent);\n+    } else {\n+      process_clusters(start_cluster_no, clusters, end_of_range, cl, use_write_table, is_concurrent);\n+    }\n@@ -718,0 +787,7 @@\n+template<typename RememberedSet>\n+inline HeapWord*\n+ShenandoahScanRemembered<RememberedSet>::addr_for_cluster(size_t cluster_no) {\n+  size_t card_index = cluster_no * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  return addr_for_card_index(card_index);\n+}\n+\n@@ -734,1 +810,6 @@\n-      process_clusters(start_cluster_no, num_clusters, end_of_range, cl);\n+      if (region->is_humongous()) {\n+        process_humongous_clusters(region->humongous_start_region(), start_cluster_no, num_clusters, end_of_range, cl,\n+                                   false \/* is_write_table *\/, false \/* is_concurrent *\/);\n+      } else {\n+        process_clusters(start_cluster_no, num_clusters, end_of_range, cl, false \/* is_concurrent *\/);\n+      }\n@@ -739,0 +820,39 @@\n+inline bool ShenandoahRegionChunkIterator::has_next() const {\n+  return _index < _total_chunks;\n+}\n+\n+inline bool ShenandoahRegionChunkIterator::next(struct ShenandoahRegionChunk *assignment) {\n+  if (_index > _total_chunks) {\n+    return false;\n+  }\n+  size_t new_index = Atomic::add(&_index, (size_t) 1, memory_order_relaxed);\n+  if (new_index > _total_chunks) {\n+    return false;\n+  }\n+  \/\/ convert to zero-based indexing\n+  new_index--;\n+\n+  size_t group_no = new_index \/ _group_size;\n+  if (group_no + 1 > _num_groups) {\n+    group_no = _num_groups - 1;\n+  }\n+\n+  \/\/ All size computations measured in HeapWord\n+  size_t region_size_words = ShenandoahHeapRegion::region_size_words();\n+  size_t group_region_index = _region_index[group_no];\n+  size_t group_region_offset = _group_offset[group_no];\n+\n+  size_t index_within_group = new_index - (group_no * _group_size);\n+  size_t group_chunk_size = _first_group_chunk_size >> group_no;\n+  size_t offset_of_this_chunk = group_region_offset + index_within_group * group_chunk_size;\n+  size_t regions_spanned_by_chunk_offset = offset_of_this_chunk \/ region_size_words;\n+  size_t region_index = group_region_index + regions_spanned_by_chunk_offset;\n+  size_t offset_within_region = offset_of_this_chunk % region_size_words;\n+\n+  assignment->_r = _heap->get_region(region_index);\n+  assignment->_chunk_offset = offset_within_region;\n+  assignment->_chunk_size = group_chunk_size;\n+\n+  return true;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":150,"deletions":30,"binary":false,"changes":180,"status":"modified"}]}