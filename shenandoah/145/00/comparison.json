{"files":[{"patch":"@@ -88,2 +88,3 @@\n-  size_t max_cset    = (ShenandoahHeap::heap()->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n-  size_t capacity    = ShenandoahHeap::heap()->young_generation()->soft_max_capacity();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t max_cset    = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t capacity    = heap->young_generation()->soft_max_capacity();\n@@ -109,1 +110,1 @@\n-\n+  bool is_generational = heap->mode()->is_generational();\n@@ -112,4 +113,8 @@\n-    size_t biased_garbage = data[idx]._garbage;\n-\n-    size_t new_cset    = cur_cset + r->get_live_data_bytes();\n-\n+    size_t new_cset;\n+    if (is_generational && (r->age() >= InitialTenuringThreshold)) {\n+      \/\/ Entire region will be promoted, This region does not impact young-gen evacuation reserve.  Memory has already\n+      \/\/ been set aside to hold evacuation results as advance_promotion_reserve.\n+      new_cset = cur_cset;\n+    } else {\n+      new_cset = cur_cset + r->get_live_data_bytes();\n+    }\n@@ -142,2 +147,0 @@\n-    } else if (biased_garbage == 0) {\n-      break;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":12,"deletions":9,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -80,1 +80,17 @@\n-void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) {\n+size_t ShenandoahHeuristics::prioritize_aged_regions(size_t old_available, size_t num_regions, bool preselected_regions[]) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t old_consumed = 0;\n+  if (heap->mode()->is_generational()) {\n+    for (size_t i = 0; i < num_regions; i++) {\n+      ShenandoahHeapRegion* region = heap->get_region(i);\n+      if (in_generation(region) && !region->is_empty() && region->is_regular() && (region->age() >= InitialTenuringThreshold)) {\n+        size_t promotion_need = (size_t) (region->get_live_data_bytes() * ShenandoahEvacWaste);\n+        if (old_consumed + promotion_need < old_available) {\n+          old_consumed += promotion_need;\n+          preselected_regions[i] = true;\n+        }\n+      }\n+    }\n+  }\n+  return old_consumed;\n+}\n@@ -82,0 +98,1 @@\n+void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) {\n@@ -134,3 +151,10 @@\n-        if (heap->mode()->is_generational() && (region->age() >= InitialTenuringThreshold)) {\n-          \/\/ Bias selection of regions that have reached tenure age\n-          for (uint j = region->age() - InitialTenuringThreshold; j > 0; j--) {\n+        if (collection_set->is_preselected(i)) {\n+          \/\/ If regions is presected, we know mode()->is_generational() and region->age() >= InitialTenuringThreshold)\n+\n+          \/\/ TODO: Deprecate and\/or refine ShenandoahTenuredRegionUsageBias.  If we preselect the regions, we can just\n+          \/\/ set garbage to \"max\" value, which is the region size rather than doing this extra work to bias selection.\n+          \/\/ May also want to exercise more discretion in prioritize_aged_regions() if we decide there are good reasons\n+          \/\/ to not promote all eligible aged regions on the current GC pass.\n+\n+          \/\/ If we're at tenure age, bias at least once.\n+          for (uint j = region->age() + 1 - InitialTenuringThreshold; j > 0; j--) {\n@@ -253,1 +277,1 @@\n-      size_t already_reserved_old_bytes = heap->get_old_evac_reserve() + heap->get_promotion_reserve();\n+      size_t already_reserved_old_bytes = heap->get_old_evac_reserve() + heap->get_promoted_reserve();\n@@ -317,1 +341,1 @@\n-    size_t promotion_budget = heap->get_promotion_reserve();\n+    size_t promotion_budget = heap->get_promoted_reserve();\n@@ -362,0 +386,4 @@\n+\n+  size_t bytes_evacuated = collection_set->get_bytes_reserved_for_evacuation();\n+  log_info(gc, ergo)(\"Total Evacuation: \" SIZE_FORMAT \"%s\",\n+                     byte_size_in_proper_unit(bytes_evacuated), proper_unit_for_byte_size(bytes_evacuated));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":34,"deletions":6,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -156,0 +156,2 @@\n+  virtual size_t prioritize_aged_regions(size_t old_available, size_t num_regions, bool preselected_regions[]);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -92,1 +92,1 @@\n-  const size_t promotion_budget_bytes = heap->get_promotion_reserve();\n+  const size_t promotion_budget_bytes = heap->get_promoted_reserve();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+  _old_garbage(0),\n@@ -94,0 +95,3 @@\n+    if (r->age() >= InitialTenuringThreshold) {\n+      _young_bytes_to_promote += r->get_live_data_bytes();\n+    }\n@@ -97,0 +101,1 @@\n+    _old_garbage += r->garbage();\n@@ -118,0 +123,1 @@\n+  _old_garbage = 0;\n@@ -126,0 +132,1 @@\n+  _young_bytes_to_promote = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+  size_t                _young_bytes_to_promote;\n@@ -60,0 +61,6 @@\n+  size_t                _old_garbage;        \/\/ How many bytes of old garbage are present in a mixed collection set?\n+\n+  bool*                 _preselected_regions;   \/\/ Points to array identifying which tenure-age regions have been preselected\n+                                                \/\/ for inclusion in collection set.  This field is only valid during brief\n+                                                \/\/ spans of time while collection set is being constructed.\n+\n@@ -105,0 +112,2 @@\n+  inline size_t get_young_bytes_to_be_promoted();\n+\n@@ -109,0 +118,6 @@\n+  inline size_t get_old_garbage();\n+\n+  void establish_preselected(bool *preselected) { _preselected_regions = preselected; }\n+  void abandon_preselected() { _preselected_regions = nullptr; }\n+  bool is_preselected(int region_idx) { return (_preselected_regions != nullptr) && _preselected_regions[region_idx]; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -72,0 +72,4 @@\n+size_t ShenandoahCollectionSet::get_young_bytes_to_be_promoted() {\n+  return _young_bytes_to_promote;\n+}\n+\n@@ -84,0 +88,4 @@\n+size_t ShenandoahCollectionSet::get_old_garbage() {\n+  return _old_garbage;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -244,1 +244,1 @@\n-      heap->set_promotion_reserve(0);\n+      heap->set_promoted_reserve(0);\n@@ -246,1 +246,1 @@\n-    log_info(gc, ergo)(\"At end of concurrent GC, old_available: \" SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+    log_info(gc, ergo)(\"At end of Concurrent GC, old_available: \" SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n@@ -707,1 +707,1 @@\n-    \/\/ heap->get_promotion_reserve() represents the amount of memory within old-gen's available memory that has\n+    \/\/ heap->get_promoted_reserve() represents the amount of memory within old-gen's available memory that has\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -297,2 +297,1 @@\n-    heap->set_promotion_reserve(0);\n-\n+    heap->set_promoted_reserve(0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -201,1 +201,1 @@\n-    heap->set_promotion_reserve(0);\n+    heap->set_promoted_reserve(0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -225,1 +225,0 @@\n-\n@@ -227,0 +226,3 @@\n+  ShenandoahCollectionSet* collection_set = heap->collection_set();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n@@ -250,1 +252,11 @@\n-\n+    size_t minimum_evacuation_reserve = ShenandoahOldCompactionReserve * region_size_bytes;\n+    size_t avail_evac_reserve_for_loan_to_young_gen = 0;\n+    size_t old_regions_loaned_for_young_evac = 0;\n+    size_t regions_available_to_loan = 0;\n+    size_t old_evacuation_reserve = 0;\n+    size_t num_regions = heap->num_regions();\n+    size_t consumed_by_advance_promotion = 0;\n+    bool preselected_regions[num_regions];\n+    for (unsigned int i = 0; i < num_regions; i++) {\n+      preselected_regions[i] = false;\n+    }\n@@ -252,0 +264,2 @@\n+      ShenandoahGeneration* old_generation = heap->old_generation();\n+      ShenandoahYoungGeneration* young_generation = heap->young_generation();\n@@ -254,1 +268,1 @@\n-      \/\/ memory is not yet full (or is in the process of being replaced).  During these tiems especially, it\n+      \/\/ memory is not yet full (or is in the process of being replaced).  During these times especially, it\n@@ -258,16 +272,72 @@\n-      \/\/  PromotionReserve for old generation: how much memory are we reserving to hold the results of\n-      \/\/     promoting young-gen objects that have reached tenure age?  This value is not \"critical\".  If we\n-      \/\/     underestimate, certain promotions will simply be deferred.  The basis of this estimate is\n-      \/\/     historical precedent.  Conservatively, budget this value to be twice the amount of memory\n-      \/\/     promoted in previous GC pass.  Whenever the amount promoted during previous GC is zero,\n-      \/\/     including initial passes before any objects have reached tenure age, use live memory within\n-      \/\/     young-gen memory divided by (ShenandoahTenureAge multiplied by InitialTenuringThreshold) as the\n-      \/\/     the very conservative value of this parameter.  Note that during initialization, there is\n-      \/\/     typically plentiful old-gen memory so it's ok to be conservative with the initial estimates\n-      \/\/     of this value.  But PromotionReserve can be no larger than available memory.  In summary, we\n-      \/\/     compute PromotionReserve as the smaller of:\n-      \/\/      1. old_gen->available\n-      \/\/      2. young_gen->capacity() * ShenandoahEvacReserve\n-      \/\/      3. (bytes promoted by previous promotion) * 2 if (bytes promoted by previous promotion) is not zero\n-      \/\/      4. if (bytes promoted by previous promotion) is zero, divide young_gen->used()\n-      \/\/         by (ShenandoahTenureAge * InitialTenuringThreshold)\n+      \/\/ Calculate EvacuationReserve before PromotionReserve.  Evacuation is more critical than promotion.\n+      \/\/ If we cannot evacuate old-gen, we will not be able to reclaim old-gen memory.  Promotions are less\n+      \/\/ critical.  If we cannot promote, there may be degradation of young-gen memory because old objects\n+      \/\/ accumulate there until they can be promoted.  This increases the young-gen marking and evacuation work.\n+\n+      \/\/ Do not fill up old-gen memory with promotions.  Reserve some amount of memory for compaction purposes.\n+      ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+      if (old_heuristics->unprocessed_old_collection_candidates() > 0) {\n+\n+        \/\/ Compute old_evacuation_reserve: how much memory are we reserving to hold the results of\n+        \/\/ evacuating old-gen heap regions?  In order to sustain a consistent pace of young-gen collections,\n+        \/\/ the goal is to maintain a consistent value for this parameter (when the candidate set is not\n+        \/\/ empty).  This value is the minimum of:\n+        \/\/   1. old_gen->available()\n+        \/\/   2. old-gen->capacity() * ShenandoahOldEvacReserve) \/ 100\n+        \/\/       (e.g. old evacuation should be no larger than 5% of old_gen capacity)\n+        \/\/   3. ((young_gen->capacity * ShenandoahEvacReserve \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100\n+        \/\/       (e.g. old evacuation should be no larger than 12% of young-gen evacuation)\n+\n+        old_evacuation_reserve = old_generation->available();\n+        assert(old_evacuation_reserve > minimum_evacuation_reserve, \"Old-gen available has not been preserved!\");\n+        size_t old_evac_reserve_max = old_generation->soft_max_capacity() * ShenandoahOldEvacReserve \/ 100;\n+        if (old_evac_reserve_max < old_evacuation_reserve) {\n+          old_evacuation_reserve = old_evac_reserve_max;\n+        }\n+        size_t young_evac_reserve_max =\n+          (((young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100;\n+        if (young_evac_reserve_max < old_evacuation_reserve) {\n+          old_evacuation_reserve = young_evac_reserve_max;\n+        }\n+      }\n+\n+      if (minimum_evacuation_reserve > old_generation->available()) {\n+        \/\/ Due to round-off errors during enforcement of minimum_evacuation_reserve during previous GC passes,\n+        \/\/ there can be slight discrepancies here.\n+        minimum_evacuation_reserve = old_generation->available();\n+      }\n+      if (old_evacuation_reserve < minimum_evacuation_reserve) {\n+        \/\/ Even if there's nothing to be evacuated on this cycle, we still need to reserve this memory for future\n+        \/\/ evacuations.  It is ok to loan this memory to young-gen if we don't need it for evacuation on this pass.\n+        avail_evac_reserve_for_loan_to_young_gen = minimum_evacuation_reserve - old_evacuation_reserve;\n+        old_evacuation_reserve = minimum_evacuation_reserve;\n+      }\n+\n+      heap->set_old_evac_reserve(old_evacuation_reserve);\n+      heap->reset_old_evac_expended();\n+\n+      \/\/ Compute the young evauation reserve: This is how much memory is available for evacuating young-gen objects.\n+      \/\/ We ignore the possible effect of promotions, which reduce demand for young-gen evacuation memory.\n+      \/\/\n+      \/\/ TODO: We could give special treatment to the regions that have reached promotion age, because we know their\n+      \/\/ live data is entirely eligible for promotion.  This knowledge can feed both into calculations of young-gen\n+      \/\/ evacuation reserve and promotion reserve.\n+      \/\/\n+      \/\/  young_evacuation_reserve for young generation: how much memory are we reserving to hold the results\n+      \/\/  of evacuating young collection set regions?  This is typically smaller than the total amount\n+      \/\/  of available memory, and is also smaller than the total amount of marked live memory within\n+      \/\/  young-gen.  This value is the smaller of\n+      \/\/\n+      \/\/    1. (young_gen->capacity() * ShenandoahEvacReserve) \/ 100\n+      \/\/    2. (young_gen->available() + old_gen_memory_available_to_be_loaned\n+      \/\/\n+      \/\/  ShenandoahEvacReserve represents the configured taget size of the evacuation region.  We can only honor\n+      \/\/  this target if there is memory available to hold the evacuations.  Memory is available if it is already\n+      \/\/  free within young gen, or if it can be borrowed from old gen.  Since we have not yet chosen the collection\n+      \/\/  sets, we do not yet know the exact accounting of how many regions will be freed by this collection pass.\n+      \/\/  What we do know is that there will be at least one evacuated young-gen region for each old-gen region that\n+      \/\/  is loaned to the evacuation effort (because regions to be collected consume more memory than the compacted\n+      \/\/  regions that will replace them).  In summary, if there are old-gen regions that are available to hold the\n+      \/\/  results of young-gen evacuations, it is safe to loan them for this purpose.  At this point, we have not yet\n+      \/\/  established a promoted_reserve.  We'll do that after we choose the collection set and analyze its impact\n+      \/\/  on available memory.\n@@ -275,2 +345,7 @@\n-      \/\/     We don't yet know how much live memory.  Inside choose_collection_set(), after it computes live memory,\n-      \/\/     the PromotionReserve may be further reduced.\n+      \/\/ We do not know the evacuation_supplement until after we have computed the collection set.  It is not always\n+      \/\/ the case that young-regions inserted into the collection set will result in net decrease of in-use regions\n+      \/\/ because ShenandoahEvacWaste times multiplied by memory within the region may be larger than the region size.\n+      \/\/ The problem is especially relevant to regions that have been inserted into the collection set because they have\n+      \/\/ reached tenure age.  These regions tend to have much higher utilization (e.g. 95%).  These regions also offer\n+      \/\/ a unique opportunity because we know that every live object contained within the region is elgible to be\n+      \/\/ promoted.  Thus, the following implementation treats these regions specially:\n@@ -278,2 +353,5 @@\n-      \/\/      5. live bytes in young-gen divided by (ShenandoahTenureAge * InitialTenuringThreshold\n-      \/\/         if the number of bytes promoted by previous promotion is zero\n+      \/\/  1. Before beginning collection set selection, we tally the total amount of live memory held within regions\n+      \/\/     that are known to have reached tenure age.  If this memory times ShenandoahEvacWaste is available within\n+      \/\/     old-gen memory, establish an advance promotion reserve to hold all or some percentage of these objects.\n+      \/\/     This advance promotion reserve is excluded from memory available for holding old-gen evacuations and cannot\n+      \/\/     be \"loaned\" to young gen.\n@@ -281,0 +359,96 @@\n+      \/\/  2. Tenure-aged regions are included in the collection set iff their evacuation size * ShenandoahEvacWaste fits\n+      \/\/     within the advance promotion reserve.  It is counter productive to evacuate these regions if they cannot be\n+      \/\/     evacuated directly into old-gen memory.  So if there is not sufficient memory to hold copies of their\n+      \/\/     live data right now, we'll just let these regions remain in young for now, to be evacuated by a subsequent\n+      \/\/     evacuation pass.\n+      \/\/\n+      \/\/  3. Next, we calculate a young-gen evacuation budget, which is the smaller of the two quantities mentioned\n+      \/\/     above.  old_gen_memory_available_to_be_loaned is calculated as:\n+      \/\/       old_gen->available - (advance-promotion-reserve + old-gen_evacuation_reserve)\n+      \/\/\n+      \/\/  4. When choosing the collection set, special care is taken to assure that the amount of loaned memory required to\n+      \/\/     hold the results of evacuation is smaller than the total memory occupied by the regions added to the collection\n+      \/\/     set.  We need to take these precautions because we do not know how much memory will be reclaimed by evacuation\n+      \/\/     until after the collection set has been constructed.  The algorithm is as follows:\n+      \/\/\n+      \/\/     a. We feed into the algorithm (i) young available at the start of evacuation and (ii) the amount of memory\n+      \/\/        loaned from old-gen that is available to hold the results of evacuation.\n+      \/\/     b. As candidate regions are added into the young-gen collection set, we maintain accumulations of the amount\n+      \/\/        of memory spanned by the collection set regions and the amount of memory that must be reserved to hold\n+      \/\/        evacuation results (by multiplying live-data size by ShenandoahEvacWaste).  We process candidate regions\n+      \/\/        in order of decreasing amounts of garbage.  We skip over (and do not include into the collection set) any\n+      \/\/        regions that do not satisfy all of the following conditions:\n+      \/\/\n+      \/\/          i. The amount of live data within the region as scaled by ShenandoahEvacWaste must fit within the\n+      \/\/             relevant evacuation reserve (live data of old-gen regions must fit within the old-evac-reserve, live\n+      \/\/             data of young-gen tenure-aged regions must fit within the advance promotion reserve, live data within\n+      \/\/             other young-gen regions must fit within the youn-gen evacuation reserve).\n+      \/\/         ii. The accumulation of memory consumed by evacuation must not exceed the accumulation of memory reclaimed\n+      \/\/             through evacuation by more than young-gen available.\n+      \/\/        iii. Other conditions may be enforced as appropriate for specific heuristics.\n+      \/\/\n+      \/\/       Note that regions are considered for inclusion in the selection set in order of decreasing amounts of garbage.\n+      \/\/       It is possible that a region with a larger amount of garbage will be rejected because it also has a larger\n+      \/\/       amount of live data and some region that follows this region in candidate order is included in the collection\n+      \/\/       set (because it has less live data and thus can fit within the evacuation limits even though it has less\n+      \/\/       garbage).\n+\n+      size_t young_evacuation_reserve = (young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      \/\/ old evacuation can pack into existing partially used regions.  young evacuation and loans for young allocations\n+      \/\/ need to target regions that do not already hold any old-gen objects.  Round down.\n+      regions_available_to_loan = old_generation->free_unaffiliated_regions();\n+      consumed_by_advance_promotion = _heuristics->prioritize_aged_regions(old_generation->available() - old_evacuation_reserve,\n+                                                                           num_regions, preselected_regions);\n+      size_t net_available_old_regions =\n+        (old_generation->available() - old_evacuation_reserve - consumed_by_advance_promotion) \/ region_size_bytes;\n+\n+      if (regions_available_to_loan > net_available_old_regions) {\n+        regions_available_to_loan = net_available_old_regions;\n+      }\n+      \/\/ Otherwise, regions_available_to_loan is less than net_available_old_regions because available memory is\n+      \/\/ scattered between multiple partially used regions.\n+\n+      if (young_evacuation_reserve > young_generation->available()) {\n+        size_t short_fall = young_evacuation_reserve - young_generation->available();\n+        if (regions_available_to_loan * region_size_bytes >= short_fall) {\n+          old_regions_loaned_for_young_evac = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n+          regions_available_to_loan -= old_regions_loaned_for_young_evac;\n+        } else {\n+          old_regions_loaned_for_young_evac = regions_available_to_loan;\n+          regions_available_to_loan = 0;\n+          young_evacuation_reserve = young_generation->available() + old_regions_loaned_for_young_evac * region_size_bytes;\n+        }\n+      } else {\n+        old_regions_loaned_for_young_evac = 0;\n+      }\n+      \/\/ In generational mode, we may end up choosing a young collection set that contains so many promotable objects\n+      \/\/ that there is not sufficient space in old generation to hold the promoted objects.  That is ok because we have\n+      \/\/ assured there is sufficient space in young generation to hold the rejected promotion candidates.  These rejected\n+      \/\/ promotion candidates will presumably be promoted in a future evacuation cycle.\n+      heap->set_young_evac_reserve(young_evacuation_reserve);\n+    } else {\n+      \/\/ Not generational mode: limit young evac reserve by young available; no need to establish old_evac_reserve.\n+      ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+      size_t young_evac_reserve = (young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      if (young_evac_reserve > young_generation->available()) {\n+        young_evac_reserve = young_generation->available();\n+      }\n+      heap->set_young_evac_reserve(young_evac_reserve);\n+    }\n+\n+    \/\/ TODO: young_available can include available (between top() and end()) within each young region that is not\n+    \/\/ part of the collection set.  Making this memory available to the young_evacuation_reserve allows a larger\n+    \/\/ young collection set to be chosen when available memory is under extreme pressure.  Implementing this \"improvement\"\n+    \/\/ is tricky, because the incremental construction of the collection set actually changes the amount of memory\n+    \/\/ available to hold evacuated young-gen objects.  As currently implemented, the memory that is available within\n+    \/\/ non-empty regions that are not selected as part of the collection set can be allocated by the mutator while\n+    \/\/ GC is evacuating and updating references.\n+\n+    collection_set->establish_preselected(preselected_regions);\n+    _heuristics->choose_collection_set(heap->collection_set(), heap->old_heuristics());\n+    collection_set->abandon_preselected();\n+\n+    \/\/ At this point, young_generation->available() knows about recently discovered immediate garbage.  We also\n+    \/\/ know the composition of the chosen collection set.\n+\n+    if (heap->mode()->is_generational()) {\n@@ -283,1 +457,9 @@\n-      size_t promotion_reserve = old_generation->available();\n+      size_t old_evacuation_committed = (size_t) (ShenandoahEvacWaste *\n+                                                  collection_set->get_old_bytes_reserved_for_evacuation());\n+      size_t immediate_garbage_regions = collection_set->get_immediate_trash() \/ region_size_bytes;\n+\n+      if (old_evacuation_committed > old_evacuation_reserve) {\n+        \/\/ This should only happen due to round-off errors when enforcing ShenandoahEvacWaste\n+        assert(old_evacuation_committed < (33 * old_evacuation_reserve) \/ 32, \"Round-off errors should be less than 3.125%%\");\n+        old_evacuation_committed = old_evacuation_reserve;\n+      }\n@@ -285,3 +467,25 @@\n-      size_t max_young_evacuation = (young_generation->soft_max_capacity() * ShenandoahOldEvacReserve) \/ 100;\n-      if (max_young_evacuation < promotion_reserve) {\n-        promotion_reserve = max_young_evacuation;\n+      \/\/ Recompute old_regions_loaned_for_young_evac because young-gen collection set may not need all the memory\n+      \/\/ originally reserved.\n+\n+      size_t young_evacuation_reserve_used =\n+        collection_set->get_young_bytes_reserved_for_evacuation() - collection_set->get_young_bytes_to_be_promoted();\n+      young_evacuation_reserve_used = (size_t) (ShenandoahEvacWaste * young_evacuation_reserve_used);\n+      heap->set_young_evac_reserve(young_evacuation_reserve_used);\n+\n+      \/\/ Adjust old_regions_loaned_for_young_evac to feed into calculations of promoted_reserve\n+      if (young_evacuation_reserve_used > young_generation->available()) {\n+        size_t short_fall = young_evacuation_reserve_used - young_generation->available();\n+\n+        \/\/ region_size_bytes is a power of 2.  loan an integral number of regions.\n+        size_t revised_loan_for_young_evacuation = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n+\n+        \/\/ Undo the previous loan\n+        regions_available_to_loan += old_regions_loaned_for_young_evac;\n+        old_regions_loaned_for_young_evac = revised_loan_for_young_evacuation;\n+        \/\/ And make a new loan\n+        assert(regions_available_to_loan > old_regions_loaned_for_young_evac, \"Cannot loan regions that we do not have\");\n+        regions_available_to_loan -= old_regions_loaned_for_young_evac;\n+      } else {\n+        \/\/ Undo the prevous loan\n+        regions_available_to_loan += old_regions_loaned_for_young_evac;\n+        old_regions_loaned_for_young_evac = 0;\n@@ -290,10 +494,8 @@\n-      size_t previously_promoted = heap->get_previous_promotion();\n-      if (previously_promoted == 0) {\n-        \/\/ Very conservatively, assume linear population decay (rather than more typical exponential) and assume all of\n-        \/\/ used is live.\n-        size_t proposed_reserve = young_generation->used() \/ (ShenandoahAgingCyclePeriod * InitialTenuringThreshold);\n-        if (promotion_reserve > proposed_reserve) {\n-          promotion_reserve = proposed_reserve;\n-        }\n-      } else if (previously_promoted * 2 < promotion_reserve) {\n-        promotion_reserve = previously_promoted * 2;\n+      size_t old_bytes_loaned = old_regions_loaned_for_young_evac * region_size_bytes;\n+      \/\/ Need to enforce that old_evacuation_committed + old_bytes_loaned >= minimum_evacuation_reserve\n+      \/\/ in order to prevent promotion reserve from violating minimum evacuation reserve.\n+      if (old_evacuation_committed + old_bytes_loaned < minimum_evacuation_reserve) {\n+        \/\/ Pretend the old_evacuation_commitment is larger than what will be evacuated to assure that promotions\n+        \/\/ do not fill the minimum_evacuation_reserve.  Note that regions loaned from old-gen will be returned\n+        \/\/ to old-gen before we start a subsequent evacuation.\n+        old_evacuation_committed = minimum_evacuation_reserve - old_bytes_loaned;\n@@ -302,2 +504,7 @@\n-      heap->set_promotion_reserve(promotion_reserve);\n-      heap->capture_old_usage(old_generation->used());\n+      \/\/ Limit promoted_reserve so that we can set aside memory to be loaned from old-gen to young-gen.  This\n+      \/\/ value is not \"critical\".  If we underestimate, certain promotions will simply be deferred.  If we put\n+      \/\/ \"all the rest\" of old-gen memory into the promotion reserve, we'll have nothing left to loan to young-gen\n+      \/\/ during the evac and update phases of GC.  So we \"limit\" the sizes of the promotion budget to be the smaller of:\n+      \/\/\n+      \/\/  1. old_gen->available - (old_evacuation_committed + old_bytes_loaned + consumed_by_advance_promotion)\n+      \/\/  2. young bytes reserved for evacuation\n@@ -305,6 +512,4 @@\n-      \/\/  OldEvacuationReserve for old generation: how much memory are we reserving to hold the results of\n-      \/\/     evacuating old-gen heap regions?  In order to sustain a consistent pace of young-gen collections,\n-      \/\/     the goal is to maintain a consistent value for this parameter (when the candidate set is not\n-      \/\/     empty).  This value is the minimum of:\n-      \/\/       1. old_gen->available() - PromotionReserve\n-      \/\/       2. (young_gen->capacity() scaled by ShenandoahEvacReserve) scaled by ShenandoahOldEvacRatioPercent\n+      assert(old_generation->available() > old_evacuation_committed, \"Cannot evacuate more than available\");\n+      assert(old_generation->available() > old_evacuation_committed + old_bytes_loaned, \"Cannot loan more than available\");\n+      assert(old_generation->available() > old_evacuation_committed + old_bytes_loaned + consumed_by_advance_promotion,\n+             \"Cannot promote more than available\");\n@@ -312,2 +517,2 @@\n-      \/\/ Don't reserve for old_evac any more than the memory that is available in old_gen.\n-      size_t old_evacuation_reserve = old_generation->available() - promotion_reserve;\n+      size_t old_avail = old_generation->available();\n+      size_t promotion_reserve = old_avail - (old_evacuation_committed + consumed_by_advance_promotion + old_bytes_loaned);\n@@ -315,3 +520,2 @@\n-      \/\/ Make sure old evacuation is no more than ShenandoahOldEvacRatioPercent of the total evacuation budget.\n-      size_t max_total_evac = (young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100;\n-      size_t max_old_evac_portion = (max_total_evac * ShenandoahOldEvacRatioPercent) \/ 100;\n+      \/\/ We experimented with constraining promoted_reserve to be no larger than 4 times the size of previously_promoted,\n+      \/\/ but this constraint was too limiting, resulting in failure of legitimate promotions.\n@@ -319,2 +523,13 @@\n-      if (old_evacuation_reserve > max_old_evac_portion) {\n-        old_evacuation_reserve = max_old_evac_portion;\n+      \/\/ We had also experimented with constraining promoted_reserve to be no more than young_evacuation_committed\n+      \/\/ divided by promotion_divisor, where:\n+      \/\/  size_t promotion_divisor = (0x02 << InitialTenuringThreshold) - 1;\n+      \/\/ This also was found to be too limiting, resulting in failure of legitimate promotions.\n+      \/\/\n+      \/\/ Both experiments were conducted in the presence of other bugs which could have been the root cause for\n+      \/\/ the failures identified above as being \"too limiting\".  TODO: conduct new experiments with the more limiting\n+      \/\/ values of young_evacuation_reserved_used.\n+      young_evacuation_reserve_used -= consumed_by_advance_promotion;\n+      if (young_evacuation_reserve_used < promotion_reserve) {\n+        \/\/ Shrink promotion_reserve if its larger than the memory to be consumed by evacuating all young objects in\n+        \/\/ collection set, including anticipated waste.  There's no benefit in using a larger promotion_reserve.\n+        promotion_reserve = young_evacuation_reserve_used;\n@@ -323,5 +538,13 @@\n-      heap->set_old_evac_reserve(old_evacuation_reserve);\n-      heap->reset_old_evac_expended();\n-\n-      \/\/ Compute YoungEvacuationReserve after we prime the collection set with old-gen candidates.  This depends\n-      \/\/ on how much memory old-gen wants to evacuate.  This is done within _heuristics->choose_collection_set().\n+      assert(old_avail >= promotion_reserve + old_evacuation_committed + old_bytes_loaned + consumed_by_advance_promotion,\n+             \"Budget exceeds available old-gen memory\");\n+      log_info(gc, ergo)(\"Old available: \" SIZE_FORMAT \", Original promotion reserve: \" SIZE_FORMAT \", Old evacuation reserve: \"\n+                         SIZE_FORMAT \", Advance promotion reserve supplement: \" SIZE_FORMAT \", Old loaned to young: \" SIZE_FORMAT,\n+                         old_avail, promotion_reserve, old_evacuation_committed, consumed_by_advance_promotion,\n+                         old_regions_loaned_for_young_evac * region_size_bytes);\n+      promotion_reserve += consumed_by_advance_promotion;\n+      heap->set_promoted_reserve(promotion_reserve);\n+      heap->reset_promoted_expended();\n+      if (collection_set->get_old_bytes_reserved_for_evacuation() == 0) {\n+        \/\/ Setting old evacuation reserve to zero denotes that there is no old-gen evacuation in this pass.\n+        heap->set_old_evac_reserve(0);\n+      }\n@@ -329,3 +552,2 @@\n-      \/\/ There's no need to pass this information to ShenandoahFreeSet::rebuild().  The GC allocator automatically borrows\n-      \/\/ memory from mutator regions when necessary.\n-    }\n+      size_t old_gen_usage_base = old_generation->used() - collection_set->get_old_garbage();\n+      heap->capture_old_usage(old_gen_usage_base);\n@@ -333,3 +555,12 @@\n-    \/\/ The heuristics may consult and\/or change the values of PromotionReserved, OldEvacuationReserved, and\n-    \/\/ YoungEvacuationReserved, all of which are represented in the shared ShenandoahHeap data structure.\n-    _heuristics->choose_collection_set(heap->collection_set(), heap->old_heuristics());\n+      \/\/ Compute the evacuation supplement, which is extra memory borrowed from old-gen that can be allocated\n+      \/\/ by mutators while GC is working on evacuation and update-refs.  This memory can be temporarily borrowed\n+      \/\/ from old-gen allotment, then repaid at the end of update-refs from the recycled collection set.  After\n+      \/\/ we have computed the collection set based on the parameters established above, we can make additional\n+      \/\/ loans based on our knowledge of the collection set to determine how much allocation we can allow\n+      \/\/ during the evacuation and update-refs phases of execution.  The total available supplement is the smaller of:\n+      \/\/\n+      \/\/   1. old_gen->available() -\n+      \/\/        (promotion_reserve + old_evacuation_commitment + old_bytes_loaned)\n+      \/\/   2. The replenishment budget (number of regions in collection set - the number of regions already\n+      \/\/         under lien for the young_evacuation_reserve)\n+      \/\/\n@@ -337,15 +568,7 @@\n-    \/\/  EvacuationAllocationSupplement: This represents memory that can be allocated in excess of young_gen->available()\n-    \/\/     during evacuation and update-refs.  This memory can be temporarily borrowed from old-gen allotment, then\n-    \/\/     repaid at the end of update-refs from the recycled collection set.  After we have computed the collection set\n-    \/\/     based on the parameters established above, we can make additional calculates based on our knowledge of the\n-    \/\/     collection set to determine how much allocation we can allow during the evacuation and update-refs phases\n-    \/\/     of execution.  With full awareness of collection set, we can shrink the values of PromotionReserve,\n-    \/\/     OldEvacuationReserve, and YoungEvacuationReserve.  Then, we can compute EvacuationAllocationReserve as the\n-    \/\/     minimum of:\n-    \/\/       1. old_gen->available - (PromotionReserve + OldEvacuationReserve)\n-    \/\/       2. The replenishment budget (number of regions in collection set - the number of regions already\n-    \/\/          under lien for the YoungEvacuationReserve)\n-    \/\/\n-\n-    \/\/ The possibly revised values are also consulted by the ShenandoahPacer when it establishes pacing parameters\n-    \/\/ for evacuation and update-refs.\n+      size_t young_regions_evacuated = collection_set->get_young_region_count();\n+      size_t regions_for_runway = 0;\n+      if (young_regions_evacuated > old_regions_loaned_for_young_evac) {\n+        regions_for_runway = young_regions_evacuated - old_regions_loaned_for_young_evac;\n+        old_regions_loaned_for_young_evac = young_regions_evacuated;\n+        regions_available_to_loan -= regions_for_runway;\n+      }\n@@ -353,0 +576,22 @@\n+      size_t allocation_supplement = regions_for_runway * region_size_bytes;\n+      heap->set_alloc_supplement_reserve(allocation_supplement);\n+\n+      size_t promotion_budget = heap->get_promoted_reserve();\n+      size_t old_evac_budget = heap->get_old_evac_reserve();\n+      size_t alloc_budget_evac_and_update = allocation_supplement + young_generation->available();\n+\n+      \/\/ TODO: young_available, which feeds into alloc_budget_evac_and_update is lacking memory available within\n+      \/\/ existing young-gen regions that were not selected for the collection set.  Add this in and adjust the\n+      \/\/ log message (where it says \"empty-region allocation budget\").\n+\n+      log_info(gc, ergo)(\"Memory reserved for evacuation and update-refs includes promotion budget: \" SIZE_FORMAT\n+                         \"%s, young evacuation budget: \" SIZE_FORMAT \"%s, old evacuation budget: \" SIZE_FORMAT\n+                         \"%s, empty-region allocation budget: \" SIZE_FORMAT \"%s, including supplement: \" SIZE_FORMAT \"%s\",\n+                         byte_size_in_proper_unit(promotion_budget), proper_unit_for_byte_size(promotion_budget),\n+                         byte_size_in_proper_unit(young_evacuation_reserve_used),\n+                         proper_unit_for_byte_size(young_evacuation_reserve_used),\n+                         byte_size_in_proper_unit(old_evac_budget), proper_unit_for_byte_size(old_evac_budget),\n+                         byte_size_in_proper_unit(alloc_budget_evac_and_update),\n+                         proper_unit_for_byte_size(alloc_budget_evac_and_update),\n+                         byte_size_in_proper_unit(allocation_supplement), proper_unit_for_byte_size(allocation_supplement));\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":323,"deletions":78,"binary":false,"changes":401,"status":"modified"},{"patch":"@@ -512,1 +512,1 @@\n-  _promotion_reserve(0),\n+  _promoted_reserve(0),\n@@ -904,1 +904,5 @@\n-  size_t new_size = ShenandoahThreadLocalData::plab_size(thread) * 2;\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = PLAB::min_size();\n+  }\n+  size_t future_size = cur_size * 2;\n@@ -908,1 +912,1 @@\n-    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    future_size = MIN2(future_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n@@ -910,2 +914,2 @@\n-  new_size = MIN2(new_size, PLAB::max_size());\n-  new_size = MAX2(new_size, PLAB::min_size());\n+  future_size = MIN2(future_size, PLAB::max_size());\n+  future_size = MAX2(future_size, PLAB::min_size());\n@@ -913,1 +917,1 @@\n-  size_t unalignment = new_size % CardTable::card_size_in_words();\n+  size_t unalignment = future_size % CardTable::card_size_in_words();\n@@ -915,1 +919,1 @@\n-    new_size = new_size - unalignment + CardTable::card_size_in_words();\n+    future_size = future_size - unalignment + CardTable::card_size_in_words();\n@@ -920,1 +924,1 @@\n-  \/\/ heuristics should catch up with them.  Note that the requested new_size may\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n@@ -922,6 +926,5 @@\n-  ShenandoahThreadLocalData::set_plab_size(thread, new_size);\n-\n-  if (new_size < size) {\n-    \/\/ New size still does not fit the object. Fall back to shared allocation.\n-    \/\/ This avoids retiring perfectly good PLABs, when we encounter a large object.\n-    return NULL;\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n@@ -932,20 +935,23 @@\n-  \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n-  \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n-  \/\/ aligned with the start of a card's memory range.\n-  retire_plab(plab);\n-\n-  size_t actual_size = 0;\n-  \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n-  \/\/ less than the remaining evacuation need.\n-  HeapWord* plab_buf = allocate_new_plab(min_size, new_size, &actual_size);\n-  if (plab_buf == NULL) {\n-    return NULL;\n-  }\n-\n-  assert (size <= actual_size, \"allocation should fit\");\n-\n-  if (ZeroTLAB) {\n-    \/\/ ..and clear it.\n-    Copy::zero_to_words(plab_buf, actual_size);\n-  } else {\n-    \/\/ ...and zap just allocated object.\n+  if (plab->words_remaining() < PLAB::min_size()) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == NULL) {\n+      return NULL;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    assert (size <= actual_size, \"allocation should fit\");\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n@@ -953,5 +959,5 @@\n-    \/\/ Skip mangling the space corresponding to the object header to\n-    \/\/ ensure that the returned space is not considered parsable by\n-    \/\/ any concurrent GC thread.\n-    size_t hdr_size = oopDesc::header_size();\n-    Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n@@ -959,2 +965,2 @@\n-  }\n-  plab->set_buf(plab_buf, actual_size);\n+    }\n+    plab->set_buf(plab_buf, actual_size);\n@@ -962,1 +968,9 @@\n-  if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n@@ -965,1 +979,0 @@\n-  return plab->allocate(size);\n@@ -972,1 +985,1 @@\n-void ShenandoahHeap::retire_plab(PLAB* plab) {\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n@@ -976,4 +989,17 @@\n-    Thread* thread = Thread::current();\n-    size_t evacuated = ShenandoahThreadLocalData::get_plab_evacuated(thread);\n-    \/\/ We don't enforce limits on get_plab_promoted(thread).  Promotion uses any memory not required for evacuation.\n-    expend_old_evac(evacuated);\n+    \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+    \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+    \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+    \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+    \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+    \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+    \/\/  1. Some of the plab may have been dedicated to evacuations.\n+    \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+    size_t not_promoted =\n+      ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+    ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+    ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+    ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+    if (not_promoted > 0) {\n+      unexpend_promoted(not_promoted);\n+    }\n@@ -994,0 +1020,9 @@\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  if (!mode()->is_generational()) {\n+    plab->retire();\n+  } else {\n+    Thread* thread = Thread::current();\n+    retire_plab(plab, thread);\n+  }\n+}\n+\n@@ -1162,0 +1197,4 @@\n+  \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+  bool promotion_eligible = false;\n+  bool allow_allocation = true;\n+  bool plab_alloc = false;\n@@ -1163,1 +1202,1 @@\n-\n+  HeapWord* result = nullptr;\n@@ -1165,0 +1204,1 @@\n+  Thread* thread = Thread::current();\n@@ -1176,1 +1216,0 @@\n-\n@@ -1178,14 +1217,10 @@\n-        \/\/ We've already retired this thread's previously exhausted PLAB and have accounted for how that PLAB's\n-        \/\/ memory was allotted.\n-        Thread* thread = Thread::current();\n-        ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n-        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n-\n-        \/\/ Conservatively, assume this entire PLAB will be used for promotion.  Act as if we need to serve the\n-        \/\/ rest of evacuation need from as-yet unallocated old-gen memory.\n-        size_t remaining_evac_need = get_old_evac_reserve() - get_old_evac_expended();\n-        size_t evac_available = old_generation()->adjusted_available() - requested_bytes;\n-        if (remaining_evac_need >= evac_available) {\n-          \/\/ Disable promotions within this thread because the entirety of this PLAB must be available to hold\n-          \/\/ old-gen evacuations.\n-          ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+        plab_alloc = true;\n+        size_t promotion_avail = get_promoted_reserve();\n+        size_t promotion_expended = get_promoted_expended();\n+        if (promotion_expended + requested_bytes > promotion_avail) {\n+          promotion_avail = 0;\n+          if (get_old_evac_reserve() == 0) {\n+            \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+            \/\/ be used for promotions.\n+            allow_allocation = false;\n+          }\n@@ -1193,1 +1228,2 @@\n-          ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+          promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          promotion_eligible = true;\n@@ -1197,5 +1233,12 @@\n-        Thread* thread = Thread::current();\n-        size_t remaining_evac_need = get_old_evac_reserve() - get_old_evac_expended();\n-        size_t evac_available = old_generation()->adjusted_available() - requested_bytes;\n-        if (remaining_evac_need >= evac_available) {\n-          return nullptr;       \/\/ We need to reserve the remaining memory for evacuation so defer the promotion\n+        size_t promotion_avail = get_promoted_reserve();\n+        size_t promotion_expended = get_promoted_expended();\n+        if (promotion_expended + requested_bytes > promotion_avail) {\n+          promotion_avail = 0;\n+        } else {\n+          promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+        }\n+\n+        if (promotion_avail == 0) {\n+          \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+          \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+          return nullptr;\n@@ -1209,2 +1252,1 @@\n-\n-  HeapWord* result = _free_set->allocate(req, in_new_region);\n+  result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n@@ -1213,0 +1255,23 @@\n+      ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+      if (req.is_gc_alloc()) {\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          if (promotion_eligible) {\n+            size_t actual_size = req.actual_size() * HeapWordSize;\n+            \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+            \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+            ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+            expend_promoted(actual_size);\n+            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+            ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+          } else {\n+            \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+            ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+            ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+          }\n+        } else if (is_promotion) {\n+          \/\/ Shared promotion.  Assume size is requested_bytes.\n+          expend_promoted(requested_bytes);\n+          assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+        }\n+      }\n+\n@@ -1232,0 +1297,8 @@\n+  } else {\n+    \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+    if ((req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && req.is_gc_alloc() &&\n+        (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+      \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+      \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+      ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+    }\n@@ -1489,6 +1562,5 @@\n-    \/\/ TODO; Retiring a PLAB disables it so it cannot support future allocations.  This is overkill.  For old-gen\n-    \/\/ regions, the important thing is to make the memory parsable by the remembered-set scanning code that drives\n-    \/\/ the update-refs processing that follows.  After the updating of old-gen references is done, it is ok to carve\n-    \/\/ this remnant object into smaller pieces during the subsequent evacuation pass, as long as the PLAB is made parsable\n-    \/\/ again before the next update-refs phase.\n-    ShenandoahHeap::heap()->retire_plab(plab);\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parseable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n@@ -2431,1 +2503,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":150,"deletions":79,"binary":false,"changes":229,"status":"modified"},{"patch":"@@ -353,1 +353,1 @@\n-  \/\/ 2. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promotion_reserve\n+  \/\/ 2. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promoted_reserve\n@@ -367,2 +367,2 @@\n-  size_t _promotion_reserve;           \/\/ Bytes reserved within old-gen to hold the results of promotion\n-\n+  size_t _promoted_reserve;            \/\/ Bytes reserved within old-gen to hold the results of promotion\n+  volatile size_t _promoted_expended;  \/\/ Bytes of old-gen memory expended on promotions\n@@ -371,1 +371,1 @@\n-  size_t _old_evac_expended;           \/\/ Bytes of old-gen memory expended on old-gen evacuations\n+  volatile size_t _old_evac_expended;  \/\/ Bytes of old-gen memory expended on old-gen evacuations\n@@ -429,2 +429,7 @@\n-  inline size_t set_promotion_reserve(size_t new_val);\n-  inline size_t get_promotion_reserve() const;\n+  inline size_t set_promoted_reserve(size_t new_val);\n+  inline size_t get_promoted_reserve() const;\n+\n+  inline void reset_promoted_expended();\n+  inline size_t expend_promoted(size_t increment);\n+  inline size_t unexpend_promoted(size_t decrement);\n+  inline size_t get_promoted_expended();\n@@ -438,1 +443,1 @@\n-  inline size_t get_old_evac_expended() const;\n+  inline size_t get_old_evac_expended();\n@@ -793,0 +798,1 @@\n+  void retire_plab(PLAB* plab, Thread* thread);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":13,"deletions":7,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -304,5 +304,3 @@\n-  if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n-    return NULL;\n-  } else if (plab == NULL) {\n-    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(),\n-           \"Performance: thread should have PLAB: %s\", thread->name());\n+  HeapWord* obj;\n+  if (plab == NULL) {\n+    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(), \"Performance: thread should have PLAB: %s\", thread->name());\n@@ -310,1 +308,3 @@\n-    return NULL;\n+    return nullptr;\n+  } else if (is_promotion && (plab->words_remaining() > 0) && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    return nullptr;\n@@ -312,2 +312,4 @@\n-  HeapWord* obj = plab->allocate(size);\n-  if (obj == NULL) {\n+  \/\/ if plab->word_size() <= 0, thread's plab not yet initialized for this pass, so allow_plab_promotions() is not trustworthy\n+  obj = plab->allocate(size);\n+  if ((obj == nullptr) && (plab->words_remaining() < PLAB::min_size())) {\n+    \/\/ allocate_from_plab_slow will establish allow_plab_promotions(thread) for future invocations\n@@ -316,0 +318,5 @@\n+  \/\/ if plab->words_remaining() >= PLAB::min_size(), just return nullptr so we can use a shared allocation\n+  if (obj == nullptr) {\n+    return nullptr;\n+  }\n+\n@@ -353,1 +360,1 @@\n-      \/\/ If we failed to promote this aged object, we'll fall through to code below and evacuat to young-gen.\n+      \/\/ If we failed to promote this aged object, we'll fall through to code below and evacuate to young-gen.\n@@ -364,0 +371,1 @@\n+  bool has_plab = false;\n@@ -389,0 +397,4 @@\n+             PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+             if (plab != nullptr) {\n+               has_plab = true;\n+             }\n@@ -390,6 +402,19 @@\n-             if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread))) {\n-               \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve.  Try resetting\n-               \/\/ the desired PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n-               ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n-               copy = allocate_from_plab(thread, size, is_promotion);\n-               \/\/ If we still get nullptr, we'll try a shared allocation below.\n+             if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread)) &&\n+                 ShenandoahThreadLocalData::plab_retries_enabled(thread)) {\n+               \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve or because\n+               \/\/ the requested object does not fit within the current plab but the plab still has an \"abundance\" of memory,\n+               \/\/ where abundance is defined as >= PLAB::min_size().  In the former case, we try resetting the desired\n+               \/\/ PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n+\n+               \/\/ In this situation, PLAB memory is precious.  We'll try to preserve our existing PLAB by forcing\n+               \/\/ this particular allocation to be shared.\n+               if (plab->words_remaining() < PLAB::min_size()) {\n+                 ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n+                 copy = allocate_from_plab(thread, size, is_promotion);\n+                 \/\/ If we still get nullptr, we'll try a shared allocation below.\n+                 if (copy == nullptr) {\n+                   \/\/ If retry fails, don't continue to retry until we have success (probably in next GC pass)\n+                   ShenandoahThreadLocalData::disable_plab_retries(thread);\n+                 }\n+               }\n+               \/\/ else, copy still equals nullptr.  this causes shared allocation below, preserving this plab for future needs.\n@@ -408,4 +433,10 @@\n-      \/\/ If we failed to allocated in LAB, we'll try a shared allocation.\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n-      copy = allocate_memory(req, is_promotion);\n-      alloc_from_lab = false;\n+      \/\/ If we failed to allocate in LAB, we'll try a shared allocation.\n+      if (!is_promotion || !has_plab || (size > PLAB::min_size())) {\n+        ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+        copy = allocate_memory(req, is_promotion);\n+        alloc_from_lab = false;\n+      }\n+      \/\/ else, we leave copy equal to NULL, signaling a promotion failure below if appropriate.\n+      \/\/ We choose not to promote objects smaller than PLAB::min_size() by way of shared allocations, as this is too\n+      \/\/ costly.  Instead, we'll simply \"evacuate\" to young-gen memory (using a GCLAB) and will promote in a future\n+      \/\/ evacuation pass.  This condition is denoted by: is_promotion && has_plab && (size <= PLAB::min_size())\n@@ -422,0 +453,49 @@\n+\n+        \/\/ We squelch excessive reports to reduce noise in logs.  Squelch enforcement is not \"perfect\" because\n+        \/\/ this same code can be in-lined in multiple contexts, and each context will have its own copy of the static\n+        \/\/ last_report_epoch and this_epoch_report_count variables.\n+        const uint MaxReportsPerEpoch = 4;\n+        static uint last_report_epoch = 0;\n+        static uint epoch_report_count = 0;\n+        PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+        size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n+        const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n+        size_t promotion_reserve;\n+        size_t promotion_expended;\n+        \/\/ We can only query GCId::current() if current thread is a named thread.  If current thread is not a\n+        \/\/ named thread, then we don't even try to squelch the promotion failure report, we don't update the\n+        \/\/ the last_report_epoch, and we don't increment the epoch_report_count\n+        if (thread->is_Named_thread()) {\n+          uint gc_id = GCId::current();\n+          if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n+            {\n+              \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+              ShenandoahHeapLocker locker(lock());\n+              promotion_reserve = get_promoted_reserve();\n+              promotion_expended = get_promoted_expended();\n+            }\n+            log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                               \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT,\n+                               size, plab == nullptr? \"no\": \"yes\",\n+                               words_remaining, promote_enabled, promotion_reserve, promotion_expended);\n+            if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n+              log_info(gc, ergo)(\"Squelching additional promotion failure reports for epoch %d\\n\", last_report_epoch);\n+            } else if (gc_id != last_report_epoch) {\n+              last_report_epoch = gc_id;;\n+              epoch_report_count = 1;\n+            }\n+          }\n+        } else if (epoch_report_count < MaxReportsPerEpoch) {\n+          \/\/ Unnamed threads are much less common than named threads.  In the rare case that an unnamed thread experiences\n+          \/\/ a promotion failure before a named thread within a given epoch, the report for the unnamed thread will be squelched.\n+          {\n+            \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+            ShenandoahHeapLocker locker(lock());\n+            promotion_reserve = get_promoted_reserve();\n+            promotion_expended = get_promoted_expended();\n+          }\n+          log_info(gc, ergo)(\"Promotion failed (unfiltered), size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                             \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT,\n+                             size, plab == nullptr? \"no\": \"yes\",\n+                             words_remaining, promote_enabled, promotion_reserve, promotion_expended);\n+        }\n@@ -591,3 +671,3 @@\n-inline size_t ShenandoahHeap::set_promotion_reserve(size_t new_val) {\n-  size_t orig = _promotion_reserve;\n-  _promotion_reserve = new_val;\n+inline size_t ShenandoahHeap::set_promoted_reserve(size_t new_val) {\n+  size_t orig = _promoted_reserve;\n+  _promoted_reserve = new_val;\n@@ -597,2 +677,2 @@\n-inline size_t ShenandoahHeap::get_promotion_reserve() const {\n-  return _promotion_reserve;\n+inline size_t ShenandoahHeap::get_promoted_reserve() const {\n+  return _promoted_reserve;\n@@ -627,1 +707,1 @@\n-  _old_evac_expended = 0;\n+  Atomic::store(&_old_evac_expended, (size_t) 0);\n@@ -631,2 +711,17 @@\n-  _old_evac_expended += increment;\n-  return _old_evac_expended;\n+  return Atomic::add(&_old_evac_expended, increment);\n+}\n+\n+inline size_t ShenandoahHeap::get_old_evac_expended() {\n+  return Atomic::load(&_old_evac_expended);\n+}\n+\n+inline void ShenandoahHeap::reset_promoted_expended() {\n+  Atomic::store(&_promoted_expended, (size_t) 0);\n+}\n+\n+inline size_t ShenandoahHeap::expend_promoted(size_t increment) {\n+  return Atomic::add(&_promoted_expended, increment);\n+}\n+\n+inline size_t ShenandoahHeap::unexpend_promoted(size_t decrement) {\n+  return Atomic::sub(&_promoted_expended, decrement);\n@@ -635,2 +730,2 @@\n-inline size_t ShenandoahHeap::get_old_evac_expended() const {\n-  return _old_evac_expended;\n+inline size_t ShenandoahHeap::get_promoted_expended() {\n+  return Atomic::load(&_promoted_expended);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":124,"deletions":29,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -59,3 +59,0 @@\n-  size_t _plab_evacuated;\n-  size_t _plab_promoted;\n-\n@@ -66,0 +63,5 @@\n+  size_t _plab_evacuated;\n+  size_t _plab_promoted;\n+  size_t _plab_preallocated_promoted;\n+  bool   _plab_retries_enabled;\n+\n@@ -75,0 +77,2 @@\n+    _disarmed_value(0),\n+    _paced_time(0),\n@@ -77,2 +81,2 @@\n-    _disarmed_value(0),\n-    _paced_time(0) {\n+    _plab_preallocated_promoted(0),\n+    _plab_retries_enabled(true) {\n@@ -158,0 +162,12 @@\n+  static void enable_plab_retries(Thread* thread) {\n+    data(thread)->_plab_retries_enabled = true;\n+  }\n+\n+  static void disable_plab_retries(Thread* thread) {\n+    data(thread)->_plab_retries_enabled = false;\n+  }\n+\n+  static bool plab_retries_enabled(Thread* thread) {\n+    return data(thread)->_plab_retries_enabled;\n+  }\n+\n@@ -202,0 +218,8 @@\n+  static void set_plab_preallocated_promoted(Thread* thread, size_t value) {\n+    data(thread)->_plab_preallocated_promoted = value;\n+  }\n+\n+  static size_t get_plab_preallocated_promoted(Thread* thread) {\n+    return data(thread)->_plab_preallocated_promoted;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":29,"deletions":5,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -523,0 +523,7 @@\n+                                                                            \\\n+  product(uintx, ShenandoahOldCompactionReserve, 8, EXPERIMENTAL,           \\\n+          \"During generational GC, prevent promotions from filling \"        \\\n+          \"this number of heap regions.  These regions are reserved \"       \\\n+          \"for the purpose of supporting compaction of old-gen \"            \\\n+          \"memory.  Otherwise, old-gen memory cannot be compacted.\")        \\\n+          range(0, 128)                                                     \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"}]}