{"files":[{"patch":"@@ -204,0 +204,4 @@\n+  log_debug(gc)(\"should_start_gc? available: \" SIZE_FORMAT \", soft_max_capacity: \" SIZE_FORMAT\n+                \", max_capacity: \" SIZE_FORMAT \", allocated: \" SIZE_FORMAT,\n+                available, capacity, max_capacity, allocated);\n+\n@@ -213,0 +217,4 @@\n+\n+  log_debug(gc)(\"  available adjusted to: \" SIZE_FORMAT \", min_threshold: \" SIZE_FORMAT \", ShenandoahMinFreeThreshold: \" SIZE_FORMAT,\n+                available, min_threshold, ShenandoahMinFreeThreshold);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -204,0 +204,6 @@\n+  \/\/ TODO: Fix implementation of old_generation->bytes_allocated_since_gc_start() to represent bytes promoted since\n+  \/\/ start of most recent OLD collection.\n+\n+  \/\/ Note further: available is the difference between soft_capacity and in_use.  So soft_tail has already been removed\n+  \/\/ from this total.  It is redundant to remove it again below.\n+\n@@ -206,0 +212,4 @@\n+  log_debug(gc)(\"should_start_old_gc? available: \" SIZE_FORMAT \", soft_max_capacity: \" SIZE_FORMAT \", max_capacity: \" SIZE_FORMAT,\n+                available, capacity, max_capacity);\n+  log_debug(gc)(\"  allocated: \" SIZE_FORMAT, allocated);\n+\n@@ -214,1 +224,5 @@\n-  size_t min_threshold = capacity \/ 100 * ShenandoahMinFreeThreshold;\n+  size_t min_threshold = (capacity * ShenandoahMinFreeThreshold) \/ 100;\n+\n+  log_debug(gc)(\"  available adjusted to: \" SIZE_FORMAT \", min_threshold: \" SIZE_FORMAT \", ShenandoahMinFreeThreshold: \" SIZE_FORMAT,\n+                available, min_threshold, ShenandoahMinFreeThreshold);\n+\n@@ -223,1 +237,1 @@\n-  \/\/ Check if are need to learn a bit about the application\n+  \/\/ Check if we need to learn a bit about the application\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveOldHeuristics.cpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -125,0 +125,3 @@\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.\n+    \/\/ This is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each\n+    \/\/ PLAB is aligned with the start of each card's memory range.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -207,4 +207,0 @@\n-    \/\/ Allocation successful, bump stats:\n-    if (req.is_mutator_alloc()) {\n-      increase_used(size * HeapWordSize);\n-    }\n@@ -215,1 +211,11 @@\n-    if (req.is_gc_alloc()) {\n+    \/\/ Allocation successful, bump stats:\n+    if (req.is_mutator_alloc()) {\n+      increase_used(size * HeapWordSize);\n+    } else if (req.is_gc_alloc()) {\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      \/\/ TODO: Making a PLAB parsable involves placing a filler object in its remnant memory but does not require\n+      \/\/ that the PLAB be disabled for all future purposes.  We may want to introduce a new service to make the\n+      \/\/ PLABs parsable while still allowing the PLAB to serve future allocation requests that arise during the\n+      \/\/ next evacuation pass.\n@@ -490,1 +496,1 @@\n-      log_debug(gc)(\"  Setting _mutator_free_bitmap bit for \" SIZE_FORMAT, idx);\n+      log_debug(gc)(\"  Setting Region \" SIZE_FORMAT \" _mutator_free_bitmap bit to true\", idx);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -66,0 +66,61 @@\n+\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n+\/\/ registering all objects between bottom() and top(), and setting remembered set cards to\n+\/\/ DIRTY if they hold interesting pointers.\n+class ShenandoahReconstructRememberedSetTask : public AbstractGangTask {\n+private:\n+  ShenandoahRegionIterator _regions;\n+\n+public:\n+  ShenandoahReconstructRememberedSetTask() :\n+    AbstractGangTask(\"Shenandoah Reset Bitmap\") { }\n+\n+  void work(uint worker_id) {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahHeapRegion* r = _regions.next();\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    RememberedScanner* scanner = heap->card_scan();\n+    ShenandoahSetRememberedCardsToDirtyClosure dirty_cards_for_interesting_pointers;\n+\n+    while (r != NULL) {\n+      if (r->is_old() && r->is_active()) {\n+        HeapWord* obj_addr = r->bottom();\n+        if (r->is_humongous_start()) {\n+          \/\/ First, clear the remembered set\n+          oop obj = cast_to_oop(obj_addr);\n+          size_t size = obj->size();\n+          HeapWord* end_object = r->bottom() + size;\n+\n+          \/\/ First, clear the remembered set\n+          scanner->reset_remset(r->bottom(), size);\n+          size_t region_index = r->index();\n+          ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n+          do {\n+            scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n+            region_index++;\n+            humongous_region = heap->get_region(region_index);\n+          } while (humongous_region->bottom() < end_object);\n+\n+          \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n+          scanner->register_object_wo_lock(obj_addr);\n+          obj->oop_iterate(&dirty_cards_for_interesting_pointers);\n+        } else if (!r->is_humongous()) {\n+          \/\/ First, clear the remembered set\n+          scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+          scanner->reset_object_range(r->bottom(), r->end());\n+\n+          \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n+          HeapWord* t = r->top();\n+          while (obj_addr < t) {\n+            oop obj = cast_to_oop(obj_addr);\n+            size_t size = obj->size();\n+            scanner->register_object_wo_lock(obj_addr);\n+            obj_addr += obj->oop_iterate_size(&dirty_cards_for_interesting_pointers);\n+          }\n+        } \/\/ else, ignore humongous continuation region\n+      }\n+      \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n+      r = _regions.next();\n+    }\n+  }\n+};\n+\n@@ -99,4 +160,0 @@\n-  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-    fatal(\"Full GC not yet supported for generational mode.\");\n-  }\n-\n@@ -122,0 +179,2 @@\n+  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n+  heap->set_gc_generation(heap->global_generation());\n@@ -123,3 +182,0 @@\n-  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-    fatal(\"Full GC not yet supported for generational mode in do_it().\");\n-  }\n@@ -246,0 +302,6 @@\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n+      ShenandoahReconstructRememberedSetTask task;\n+      heap->workers()->run_task(&task);\n+    }\n@@ -261,1 +323,5 @@\n-    heap->verifier()->verify_after_fullgc();\n+    if (heap->mode()->is_generational()) {\n+      heap->verifier()->verify_after_generational_fullgc();\n+    } else {\n+      heap->verifier()->verify_after_fullgc();\n+    }\n@@ -311,0 +377,202 @@\n+class ShenandoahPrepareForCompactionTask : public AbstractGangTask {\n+private:\n+  PreservedMarksSet*        const _preserved_marks;\n+  ShenandoahHeap*           const _heap;\n+  ShenandoahHeapRegionSet** const _worker_slices;\n+  size_t                    const _num_workers;\n+  size_t                    *_old_used;\n+  size_t                    *_young_used;\n+\n+public:\n+  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices,\n+                                     size_t num_workers, size_t* old_used, size_t* young_used);\n+\n+  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n+    \/\/ Empty region: get it into the slice to defragment the slice itself.\n+    \/\/ We could have skipped this without violating correctness, but we really\n+    \/\/ want to compact all live regions to the start of the heap, which sometimes\n+    \/\/ means moving them into the fully empty regions.\n+    if (r->is_empty()) return true;\n+\n+    \/\/ Can move the region, and this is not the humongous region. Humongous\n+    \/\/ moves are special cased here, because their moves are handled separately.\n+    return r->is_stw_move_allowed() && !r->is_humongous();\n+  }\n+\n+  void work(uint worker_id);\n+  void add_to_young_used(size_t worker_id, size_t amount);\n+  void add_to_old_used(size_t worker_id, size_t amount);\n+  size_t young_used();\n+  size_t old_used();\n+};\n+\n+class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n+private:\n+  ShenandoahPrepareForCompactionTask* _compactor;\n+  PreservedMarks*          const _preserved_marks;\n+  ShenandoahHeap*          const _heap;\n+  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n+  int _empty_regions_pos;\n+  ShenandoahHeapRegion*          _old_to_region;\n+  ShenandoahHeapRegion*          _young_to_region;\n+  ShenandoahHeapRegion*          _from_region;\n+  ShenandoahRegionAffiliation    _from_affiliation;\n+  HeapWord*                      _old_compact_point;\n+  HeapWord*                      _young_compact_point;\n+  uint                           _worker_id;\n+\n+public:\n+  ShenandoahPrepareForGenerationalCompactionObjectClosure(ShenandoahPrepareForCompactionTask* compactor,\n+                                                          PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* old_to_region,\n+                                                          ShenandoahHeapRegion* young_to_region, uint worker_id) :\n+      _compactor(compactor),\n+      _preserved_marks(preserved_marks),\n+      _heap(ShenandoahHeap::heap()),\n+      _empty_regions(empty_regions),\n+      _empty_regions_pos(0),\n+      _old_to_region(old_to_region),\n+      _young_to_region(young_to_region),\n+      _from_region(NULL),\n+      _old_compact_point((old_to_region != nullptr)? old_to_region->bottom(): nullptr),\n+      _young_compact_point((young_to_region != nullptr)? young_to_region->bottom(): nullptr),\n+      _worker_id(worker_id) {}\n+\n+  void set_from_region(ShenandoahHeapRegion* from_region) {\n+    _from_region = from_region;\n+    _from_affiliation = from_region->affiliation();\n+    if (_from_region->has_live()) {\n+      if (_from_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+        if (_old_to_region == nullptr) {\n+          _old_to_region = from_region;\n+          _old_compact_point = from_region->bottom();\n+        }\n+      } else {\n+        assert(_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n+        if (_young_to_region == nullptr) {\n+          _young_to_region = from_region;\n+          _young_compact_point = from_region->bottom();\n+        }\n+      }\n+    } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n+  }\n+\n+  void finish() {\n+    finish_old_region();\n+    finish_young_region();\n+  }\n+\n+  void finish_old_region() {\n+    if (_old_to_region != nullptr) {\n+      log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n+                    _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n+      _compactor->add_to_old_used(_worker_id, _old_compact_point - _old_to_region->bottom());\n+      _old_to_region->set_new_top(_old_compact_point);\n+      _old_to_region = nullptr;\n+    }\n+  }\n+\n+  void finish_young_region() {\n+    if (_young_to_region != nullptr) {\n+      log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n+                    _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n+      _compactor->add_to_young_used(_worker_id, _young_compact_point - _young_to_region->bottom());\n+      _young_to_region->set_new_top(_young_compact_point);\n+      _young_to_region = nullptr;\n+    }\n+  }\n+\n+  bool is_compact_same_region() {\n+    return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n+  }\n+\n+  int empty_regions_pos() {\n+    return _empty_regions_pos;\n+  }\n+\n+  void do_object(oop p) {\n+    assert(_from_region != NULL, \"must set before work\");\n+    assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n+           \"Object must reside in _from_region\");\n+    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+\n+    size_t obj_size = p->size();\n+    if (_from_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+      assert(_old_to_region != nullptr, \"_old_to_region should not be NULL when compacting OLD _from_region\");\n+      if (_old_compact_point + obj_size > _old_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n+                      p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _old_to_region.\n+        finish_old_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != NULL, \"must not be NULL\");\n+        _old_to_region = new_to_region;\n+        _old_compact_point = _old_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(NULL, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_old_compact_point));\n+      _old_compact_point += obj_size;\n+    } else {\n+      assert(_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION,\n+             \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n+\n+      assert(_young_to_region != nullptr, \"_young_to_region should not be NULL when compacting YOUNG _from_region\");\n+      if (_young_compact_point + obj_size > _young_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+\n+        log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n+                      p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _young_to_region.\n+        finish_young_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(YOUNG_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != NULL, \"must not be NULL\");\n+        _young_to_region = new_to_region;\n+        _young_compact_point = _young_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(NULL, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_young_compact_point));\n+      _young_compact_point += obj_size;\n+    }\n+  }\n+};\n+\n+\n@@ -339,8 +607,1 @@\n-    if (_heap->mode()->is_generational() && _to_region->affiliation() == FREE) {\n-      \/\/ TODO: Changing this region to young during compaction may not be\n-      \/\/ technically correct here because it completely disregards the ages\n-      \/\/ and origins of the objects being moved. It is, however, certainly\n-      \/\/ more correct than putting live objects into a region without a\n-      \/\/ generational affiliation.\n-      _to_region->set_affiliation(YOUNG_GENERATION);\n-    }\n+    assert(!_heap->mode()->is_generational(), \"Generational GC should use different Closure\");\n@@ -392,5 +653,0 @@\n-class ShenandoahPrepareForCompactionTask : public AbstractGangTask {\n-private:\n-  PreservedMarksSet*        const _preserved_marks;\n-  ShenandoahHeap*           const _heap;\n-  ShenandoahHeapRegionSet** const _worker_slices;\n@@ -398,2 +654,3 @@\n-public:\n-  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices) :\n+ShenandoahPrepareForCompactionTask::ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n+                                                                       ShenandoahHeapRegionSet **worker_slices,\n+                                                                       size_t num_workers, size_t* old_used, size_t* young_used) :\n@@ -401,2 +658,6 @@\n-    _preserved_marks(preserved_marks),\n-    _heap(ShenandoahHeap::heap()), _worker_slices(worker_slices) {\n+    _preserved_marks(preserved_marks), _heap(ShenandoahHeap::heap()),\n+    _worker_slices(worker_slices), _num_workers(num_workers),\n+    _old_used(old_used), _young_used(young_used) {\n+  for (size_t i = 0; i < _num_workers; i++) {\n+    _old_used[i] = 0;\n+    _young_used[i] = 0;\n@@ -404,0 +665,1 @@\n+}\n@@ -405,6 +667,0 @@\n-  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n-    \/\/ Empty region: get it into the slice to defragment the slice itself.\n-    \/\/ We could have skipped this without violating correctness, but we really\n-    \/\/ want to compact all live regions to the start of the heap, which sometimes\n-    \/\/ means moving them into the fully empty regions.\n-    if (r->is_empty()) return true;\n@@ -412,3 +668,8 @@\n-    \/\/ Can move the region, and this is not the humongous region. Humongous\n-    \/\/ moves are special cased here, because their moves are handled separately.\n-    return r->is_stw_move_allowed() && !r->is_humongous();\n+void ShenandoahPrepareForCompactionTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n+  ShenandoahHeapRegionSetIterator it(slice);\n+  ShenandoahHeapRegion* from_region = it.next();\n+  \/\/ No work?\n+  if (from_region == NULL) {\n+    return;\n@@ -417,9 +678,3 @@\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n-    ShenandoahHeapRegionSetIterator it(slice);\n-    ShenandoahHeapRegion* from_region = it.next();\n-    \/\/ No work?\n-    if (from_region == NULL) {\n-       return;\n-    }\n+  \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n+  \/\/ Remember empty regions and reuse them as needed.\n+  ResourceMark rm;\n@@ -427,3 +682,16 @@\n-    \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n-    \/\/ Remember empty regions and reuse them as needed.\n-    ResourceMark rm;\n+  GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n+\n+  if (_heap->mode()->is_generational()) {\n+    ShenandoahHeapRegion* old_to_region = (from_region->is_old())? from_region: nullptr;\n+    ShenandoahHeapRegion* young_to_region = (from_region->is_young())? from_region: nullptr;\n+    ShenandoahPrepareForGenerationalCompactionObjectClosure cl(this, _preserved_marks->get(worker_id), empty_regions,\n+                                                               old_to_region, young_to_region, worker_id);\n+    while (from_region != NULL) {\n+      assert(is_candidate_region(from_region), \"Sanity\");\n+      log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n+                    worker_id, affiliation_name(from_region->affiliation()),\n+                    from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n+      cl.set_from_region(from_region);\n+      if (from_region->has_live()) {\n+        _heap->marked_object_iterate(from_region, &cl);\n+      }\n@@ -431,1 +699,7 @@\n-    GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n+      \/\/ Compacted the region to somewhere else? From-region is empty then.\n+      if (!cl.is_compact_same_region()) {\n+        empty_regions.append(from_region);\n+      }\n+      from_region = it.next();\n+    }\n+    cl.finish();\n@@ -433,0 +707,6 @@\n+    \/\/ Mark all remaining regions as empty\n+    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n+      ShenandoahHeapRegion* r = empty_regions.at(pos);\n+      r->set_new_top(r->bottom());\n+    }\n+  } else {\n@@ -434,1 +714,0 @@\n-\n@@ -437,1 +716,0 @@\n-\n@@ -457,1 +735,39 @@\n-};\n+}\n+\n+\/\/ Accumulate HeapWords of memory used in young-gen memory.\n+void ShenandoahPrepareForCompactionTask::add_to_young_used(size_t worker_id, size_t amount) {\n+  log_debug(gc)(\"Adding to _young_used for worker_id: \" SIZE_FORMAT \", amount: \" SIZE_FORMAT, worker_id, amount);\n+  _young_used[worker_id] += amount;\n+}\n+\n+\/\/ Accumulate HeapWords of memory used in old-gen memory.\n+void ShenandoahPrepareForCompactionTask::add_to_old_used(size_t worker_id, size_t amount) {\n+  log_debug(gc)(\"Adding to _old_used for worker_id: \" SIZE_FORMAT \", amount: \" SIZE_FORMAT, worker_id, amount);\n+  _old_used[worker_id] += amount;\n+}\n+\n+\/\/ Return total number of bytes used in young-gen memory\n+size_t ShenandoahPrepareForCompactionTask::young_used() {\n+  size_t result = 0;\n+  log_debug(gc)(\"Calculating young_used by accumulating worker totals\");\n+  for (size_t i = 0; i < _num_workers; i++) {\n+    log_debug(gc)(\"  worker [\" SIZE_FORMAT \"] contributed \" SIZE_FORMAT, i, _young_used[i]);\n+    result += _young_used[i];\n+  }\n+  result *= HeapWordSize;\n+  log_debug(gc)(\"Accumulated _young_used is: \" SIZE_FORMAT, result);\n+  return result;\n+}\n+\n+\/\/ Return total number of bytes used in old-gen memory\n+size_t ShenandoahPrepareForCompactionTask::old_used() {\n+  size_t result = 0;\n+  log_debug(gc)(\"Calculating old_used by accumulating worker totals\");\n+  for (size_t i = 0; i < _num_workers; i++) {\n+    log_debug(gc)(\"  worker [\" SIZE_FORMAT \"] contributed \" SIZE_FORMAT, i, _old_used[i]);\n+    result += _old_used[i];\n+  }\n+  log_debug(gc)(\"Accumulated _old_used is: \" SIZE_FORMAT, result);\n+  result *= HeapWordSize;\n+  return result;\n+}\n@@ -479,0 +795,11 @@\n+\n+    if (r->is_humongous_start() && heap->mode()->is_generational()) {\n+      oop obj = cast_to_oop(r->bottom());\n+      size_t humongous_bytes = obj->size() * HeapWordSize;\n+      log_debug(gc)(\"Adjusting used for humongous %s object by \" SIZE_FORMAT, r->is_old()? \"OLD\": \"YOUNG\", humongous_bytes);\n+      if (r->is_old()) {\n+        heap->old_generation()->increase_used(humongous_bytes);\n+      } else {\n+        heap->young_generation()->increase_used(humongous_bytes);\n+      }\n+    }\n@@ -735,0 +1062,5 @@\n+  if (heap->mode()->is_generational()) {\n+    heap->young_generation()->clear_used();\n+    heap->old_generation()->clear_used();\n+  }\n+\n@@ -741,1 +1073,4 @@\n-    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices);\n+    size_t num_workers = heap->workers()->total_workers();\n+    size_t old_used[num_workers];\n+    size_t young_used[num_workers];\n+    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices, num_workers, old_used, young_used);\n@@ -743,0 +1078,7 @@\n+\n+    if (heap->mode()->is_generational()) {\n+      log_debug(gc)(\"Usage after compacting regular objects is young: \" SIZE_FORMAT \", old: \" SIZE_FORMAT,\n+                    task.young_used(), task.old_used());\n+      heap->young_generation()->increase_used(task.young_used());\n+      heap->old_generation()->increase_used(task.old_used());\n+    }\n@@ -1093,0 +1435,4 @@\n+    if (heap->mode()->is_generational()) {\n+      log_info(gc)(\"FullGC done: GLOBAL usage: \" SIZE_FORMAT \", young usage: \" SIZE_FORMAT \", old usage: \" SIZE_FORMAT,\n+                    post_compact.get_live(), heap->young_generation()->used(), heap->old_generation()->used());\n+    }\n@@ -1096,4 +1442,0 @@\n-\n-    if (heap->mode()->is_generational()) {\n-      heap->young_generation()->promote_all_regions();\n-    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":397,"deletions":55,"binary":false,"changes":452,"status":"modified"},{"patch":"@@ -325,0 +325,6 @@\n+void ShenandoahGeneration::clear_used() {\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at a safepoint\");\n+  \/\/ Do this atomically to assure visibility to other threads, even though these other threads may be idle \"right now\"..\n+  Atomic::store(&_used, (size_t)0);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -137,0 +137,1 @@\n+  void clear_used();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -857,0 +857,1 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n@@ -879,0 +880,3 @@\n+  \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+  \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+  \/\/ aligned with the start of a card's memory range.\n@@ -914,0 +918,2 @@\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - waste, p2i(top));\n@@ -1250,0 +1256,11 @@\n+    \/\/ TODO; Retiring a PLAB disables it so it cannot support future allocations.  This is overkill.  For old-gen\n+    \/\/ regions, the important thing is to make the memory parsable by the remembered-set scanning code that drives\n+    \/\/ the update-refs processing that follows.  After the updating of old-gen references is done, it is ok to carve\n+    \/\/ this remnant object into smaller pieces during the subsequent evacuation pass, as long as the PLAB is made parsable\n+    \/\/ again before the next update-refs phase.\n+    if (plab->top() != nullptr) {\n+      ShenandoahHeapRegion* r = ShenandoahHeap::heap()->heap_region_containing(plab->top());\n+      log_debug(gc)(\"Retiring plab with top: \" PTR_FORMAT \", hard_end: \" PTR_FORMAT\n+                    \" + AlignmentReserve, in %s Region \" SIZE_FORMAT,\n+                    p2i(plab->top()), p2i(plab->top() + plab->words_remaining()), affiliation_name(r->affiliation()), r->index());\n+    } \/\/ else, don't bother to report retirement\n@@ -2594,0 +2611,2 @@\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", doing_mixed_evacuations()? \"mixed\": \"young\");\n+\n@@ -2626,1 +2645,1 @@\n-          \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+          \/\/ ctx->is_marked() returns true if mark bit set (TAMS not relevant here)\n@@ -2639,0 +2658,5 @@\n+          } \/\/ Else, this object is not live so we don't verify dirty cards contained therein.\n+\n+          if (ctx) {\n+            \/\/ TAMS not relevant here\n+            obj_addr = ctx->get_next_marked_addr(obj_addr, t);\n@@ -2640,8 +2664,1 @@\n-            \/\/ This object is not live so we don't verify dirty cards contained therein\n-            ShenandoahHeapRegion* r = heap_region_containing(obj_addr);\n-            HeapWord* tams = ctx->top_at_mark_start(r);\n-            if (obj_addr >= tams) {\n-              obj_addr += obj->size();\n-            } else {\n-              obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n-            }\n+            obj_addr += obj->size();\n@@ -2655,0 +2672,74 @@\n+void ShenandoahHeap::help_verify_region_rem_set(ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx, HeapWord* from,\n+                                                HeapWord* top, HeapWord* registration_watermark, const char* message) {\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(false);\n+\n+  HeapWord* obj_addr = from;\n+  if (r->is_humongous_start()) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      size_t card_index = scanner->card_index_for_addr(obj_addr);\n+      \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+      \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+      \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+      if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+        obj->oop_iterate(&check_interesting_pointers);\n+      }\n+      \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+    }\n+    \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+\n+    if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, obj->size())) {\n+      ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL, message,\n+                                       \"object not properly registered\", __FILE__, __LINE__);\n+    }\n+  } else if (!r->is_humongous()) {\n+    while (obj_addr < top) {\n+      oop obj = cast_to_oop(obj_addr);\n+      \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+      if (!ctx || ctx->is_marked(obj)) {\n+        size_t card_index = scanner->card_index_for_addr(obj_addr);\n+        \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+        \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+        if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+          obj->oop_iterate(&check_interesting_pointers);\n+        }\n+        \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+\n+        if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, obj->size())) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL, message,\n+                                           \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+      } \/\/ Else, this object is not live so we don't verify dirty cards contained therein.\n+\n+      if (ctx) {\n+        ShenandoahHeapRegion* r = heap_region_containing(obj_addr);\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        if (obj_addr >= tams) {\n+          obj_addr += obj->size();\n+        } else {\n+          obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+        }\n+      } else {\n+        obj_addr += obj->size();\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahHeap::verify_rem_set_after_full_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(r, nullptr, r->bottom(), r->top(), r->top(), \"Remembered set violation at end of Full GC\");\n+    }\n+  }\n+}\n+\n@@ -2664,3 +2755,0 @@\n-  ShenandoahMarkingContext* mark_context = marking_context();\n-  RememberedScanner* scanner = card_scan();\n-  ShenandoahVerifyRemSetClosure check_interesting_pointers(false);\n@@ -2670,1 +2758,1 @@\n-    ctx = mark_context;\n+    ctx = marking_context();\n@@ -2680,79 +2768,3 @@\n-      HeapWord* obj_addr = r->bottom();\n-      if (r->is_humongous_start()) {\n-        oop obj = oop(obj_addr);\n-        if (!ctx || ctx->is_marked(obj)) {\n-          size_t card_index = scanner->card_index_for_addr(obj_addr);\n-          \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n-          \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n-          \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n-          if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n-            obj->oop_iterate(&check_interesting_pointers);\n-          }\n-          \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n-        }\n-        \/\/ else, this humongous object is not live so no need to verify its internal pointers\n-        if (!scanner->verify_registration(obj_addr, obj->size())) {\n-          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n-                                          \"Verify init-update-references remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n-        }\n-      } else if (!r->is_humongous()) {\n-        HeapWord* t = r->get_update_watermark();\n-        while (obj_addr < t) {\n-          oop obj = oop(obj_addr);\n-          \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n-          if (!ctx || ctx->is_marked(obj)) {\n-            size_t card_index = scanner->card_index_for_addr(obj_addr);\n-            \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n-            \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n-            if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n-              obj->oop_iterate(&check_interesting_pointers);\n-            }\n-            \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n-            if (!scanner->verify_registration(obj_addr, obj->size())) {\n-              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n-                                               \"Verify init-update-references remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n-            }\n-            obj_addr += obj->size();\n-          } else {\n-            \/\/ This object is not live so we don't verify dirty cards contained therein\n-            ShenandoahHeapRegion* r = heap_region_containing(obj_addr);\n-            HeapWord* tams = ctx->top_at_mark_start(r);\n-            if (obj_addr >= tams) {\n-              obj_addr += obj->size();\n-            } else {\n-              obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n-            }\n-          }\n-        }\n-        \/\/ Update references only cares about remembered set below update_watermark, but entire remset should be valid\n-        \/\/ We're at safepoint and all LABs have been flushed, so we can parse all the way to top().\n-        t = r->top();\n-        while (obj_addr < t) {\n-          oop obj = oop(obj_addr);\n-          \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n-          if (!ctx || ctx->is_marked(obj)) {\n-            size_t card_index = scanner->card_index_for_addr(obj_addr);\n-            \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n-            \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n-            if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n-              obj->oop_iterate(&check_interesting_pointers);\n-            }\n-            \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n-            if (!scanner->verify_registration(obj_addr, obj->size())) {\n-              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n-                                               \"Verify init-update-references remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n-            }\n-            obj_addr += obj->size();\n-          } else {\n-            \/\/ This object is not live so we don't verify dirty cards contained therein\n-            ShenandoahHeapRegion* r = heap_region_containing(obj_addr);\n-            HeapWord* tams = ctx->top_at_mark_start(r);\n-            if (obj_addr >= tams) {\n-              obj_addr += obj->size();\n-            } else {\n-              obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n-            }\n-          }\n-        }\n-      }\n-    } \/\/ else, we don't care about this region\n+      help_verify_region_rem_set(r, ctx, r->bottom(), r->top(), r->get_update_watermark(),\n+                                 \"Remembered set violation at init-update-references\");\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":104,"deletions":92,"binary":false,"changes":196,"status":"modified"},{"patch":"@@ -208,0 +208,1 @@\n+  void verify_rem_set_after_full_gc();\n@@ -222,0 +223,2 @@\n+  void help_verify_region_rem_set(ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx,\n+                                  HeapWord* from, HeapWord* top, HeapWord* update_watermark, const char* message);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -562,5 +562,1 @@\n-  \/\/ In case top() does not align with a card boundary, it's necessary to fill remainder of memory beyond top().\n-  if (top() < end()) {\n-    ShenandoahHeap::fill_with_object(top(), end() - top());;\n-    rem_set_scanner->register_object_wo_lock(obj_addr);\n-  }\n+  \/\/ Remembered set scanning stops at top() so no need to fill beyond it.\n@@ -861,0 +857,1 @@\n+    \/\/ During full gc, heap->complete_marking_context() is not valid, may equal nullptr.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":2,"deletions":5,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -97,1 +97,0 @@\n-  log_debug(gc)(\"SHR::clear_live_data on %s Region \" SIZE_FORMAT,  affiliation_name(affiliation()), index());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -73,2 +73,2 @@\n-  log_debug(gc)(\"SMC:initialize_top_at_mark_start for region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmaps set to \" PTR_FORMAT,\n-                p2i(r->bottom()), p2i(r->end()), p2i(r->end()));\n+  log_debug(gc)(\"SMC:initialize_top_at_mark_start for Region \" SIZE_FORMAT \", TAMS: \" PTR_FORMAT \", TopOfBitMap: \" PTR_FORMAT,\n+                r->index(), p2i(bottom), p2i(r->end()));\n@@ -85,2 +85,2 @@\n-  log_debug(gc)(\"SMC:clear_bitmap for %s region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmap: \" PTR_FORMAT,\n-                affiliation_name(r->affiliation()), p2i(r->bottom()), p2i(r->end()), p2i(top_bitmap));\n+  log_debug(gc)(\"SMC:clear_bitmap for %s Region \" SIZE_FORMAT \", top_bitmap: \" PTR_FORMAT,\n+                affiliation_name(r->affiliation()), r->index(), p2i(top_bitmap));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -252,0 +252,19 @@\n+class ShenandoahSetRememberedCardsToDirtyClosure : public BasicOopIterateClosure {\n+\n+protected:\n+  ShenandoahHeap* _heap;\n+  RememberedScanner* _scanner;\n+\n+public:\n+\n+  ShenandoahSetRememberedCardsToDirtyClosure() :\n+      _heap(ShenandoahHeap::heap()),\n+      _scanner(_heap->card_scan()) {  }\n+\n+  template<class T>\n+  inline void work(T* p);\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p) { work(p); }\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -73,0 +73,12 @@\n+template<class T>\n+inline void ShenandoahSetRememberedCardsToDirtyClosure::work(T* p) {\n+  T o = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(o)) {\n+    oop obj = CompressedOops::decode_not_null(o);\n+    if (_heap->is_in_young(obj)) {\n+      \/\/ Found interesting pointer.  Mark the containing card as dirty.\n+      _scanner->mark_card_as_dirty((HeapWord*) p);\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -180,0 +180,1 @@\n+  f(full_gc_reconstruct_remembered_set,             \"    Reconstruct Remembered Set\")  \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -495,1 +495,1 @@\n-      \/\/ This object is not live so don't trust its size()\n+      \/\/ If this object is not live, don't trust its size(); all objects above tams are live.\n@@ -509,0 +509,13 @@\n+  \/\/ At this point, offset represents object whose registration we are verifying.  We know that at least this object resides\n+  \/\/ within this card's memory.\n+\n+  \/\/ Make sure that last_offset is properly set for the enclosing card, but we can't verify this for\n+  \/\/ candidate collection-set regions during mixed evacuations, so disable this check in general\n+  \/\/ during mixed evacuations.\n+\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(base_addr + offset);\n+  size_t max_offset = r->top() - base_addr;\n+  if (max_offset > CardTable::card_size_in_words) {\n+    max_offset = CardTable::card_size_in_words;\n+  }\n+  size_t prev_offset;\n@@ -510,6 +523,0 @@\n-    \/\/ Make sure that last_offset is properly set for the enclosing card, but we can't verify this for\n-    \/\/ candidate collection-set regions during mixed evacuations, so disable this check in general\n-    \/\/ during mixed evacuations.\n-    \/\/\n-    \/\/ TODO: could do some additional checking during mixed evacuations if we wanted to work harder.\n-    size_t prev_offset = offset;\n@@ -521,1 +528,1 @@\n-    } while (offset < CardTable::card_size_in_words);\n+    } while (offset < max_offset);\n@@ -545,1 +552,2 @@\n-  }\n+  } else {\n+    \/\/ This is a mixed evacuation: rely on mark bits to identify which objects need to be properly registered\n@@ -547,0 +555,18 @@\n+    \/\/ If the object reaching or spanning the end of this card's memory is marked, then last_offset for this card\n+    \/\/ should represents this object.  Otherwise, last_offset is a don't care.\n+    HeapWord* end_of_interest = base_addr + max_offset;\n+    do {\n+      HeapWord* obj_addr = base_addr + offset;\n+      oop obj = oop(base_addr + offset);\n+      prev_offset = offset;\n+      offset = ctx->get_next_marked_addr(base_addr + offset, end_of_interest) - base_addr;\n+    } while (offset < max_offset);\n+    oop last_obj = oop(base_addr + prev_offset);\n+    if (prev_offset + last_obj->size() >= max_offset) {\n+      if (_scc->get_last_start(index) != prev_offset) {\n+        return false;\n+      }\n+      \/\/ otherwise, the value of _scc->get_last_start(index) is a don't care because it represents a dead object and we\n+      \/\/ cannot verify its context\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":35,"deletions":9,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -348,0 +350,2 @@\n+    log_debug(gc)(\"ShenandoahCalculatRegionStatsClosure added \" SIZE_FORMAT \" for %s Region \" SIZE_FORMAT \", yielding: \" SIZE_FORMAT,\n+                  r->used(), r->is_humongous()? \"humongous\": \"regular\", r->index(), _used);\n@@ -725,0 +729,1 @@\n+\n@@ -740,0 +745,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap usage verification\");\n+\n@@ -751,0 +758,8 @@\n+    if (remembered == _verify_remembered_for_marking) {\n+      log_debug(gc)(\"Safepoint verification of remembered set at mark\");\n+    } else if (remembered == _verify_remembered_for_updating_references) {\n+      log_debug(gc)(\"Safepoint verification of remembered set at update ref\");\n+    } else if (remembered == _verify_remembered_after_full_gc) {\n+      log_debug(gc)(\"Safepoint verification of remembered set after full gc\");\n+    }\n+\n@@ -755,0 +770,2 @@\n+    } else if (remembered == _verify_remembered_after_full_gc) {\n+      _heap->verify_rem_set_after_full_gc();\n@@ -759,0 +776,6 @@\n+\n+    log_debug(gc)(\"Safepoint verification: generation %s usage hereby calculated as: \" SIZE_FORMAT,\n+                  generation->name(), cl.used());\n+    log_debug(gc)(\"                                    previous tabulation of usage: \" SIZE_FORMAT, generation->used());\n+\n+\n@@ -767,0 +790,2 @@\n+  log_debug(gc)(\"Safepoint verification finished remembered set verification\");\n+\n@@ -777,0 +802,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap region closure verification\");\n+\n@@ -802,0 +829,2 @@\n+  log_debug(gc)(\"Safepoint verification finished getting initial reachable set\");\n+\n@@ -819,0 +848,2 @@\n+  log_debug(gc)(\"Safepoint verification finished walking marked objects\");\n+\n@@ -852,0 +883,3 @@\n+  log_debug(gc)(\"Safepoint verification finished accumulation of liveness data\");\n+\n+\n@@ -955,1 +989,1 @@\n-          _verify_remembered_for_updating_references,  \/\/ do not verify remembered set\n+          _verify_remembered_for_updating_references,  \/\/ verify read-write remembered set\n@@ -1008,0 +1042,14 @@\n+void ShenandoahVerifier::verify_after_generational_fullgc() {\n+  verify_at_safepoint(\n+          \"After Full Generational GC\",\n+          _verify_remembered_after_full_gc,  \/\/ verify read-write remembered set\n+          _verify_forwarded_none,      \/\/ all objects are non-forwarded\n+          _verify_marked_complete,     \/\/ all objects are marked in complete bitmap\n+          _verify_cset_none,           \/\/ no cset references\n+          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash_nocset, \/\/ no trash, no cset\n+          _verify_gcstate_stable,       \/\/ full gc cleaned up everything\n+          _verify_all_weak_roots\n+  );\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":49,"deletions":1,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -64,3 +64,3 @@\n-    \/\/ Assure remembered set cards are dirty for every interesting pointer within\n-    \/\/ each ShenandoahHeapRegion between bottom() and top().  This is appropriate at\n-    \/\/ the init_mark safepoint since all TLABS are retired before we reach this code.\n+    \/\/ Assure old objects are registered and remembered set cards within the read-only remembered set are dirty\n+    \/\/ for every interesting pointer within each OLD ShenandoahHeapRegion between bottom() and top().  This is\n+    \/\/ appropriate at the init_mark safepoint since all TLABS are retired before we reach this code.\n@@ -69,3 +69,7 @@\n-    \/\/ Assure remembered set cards are dirty for every interesting pointer within\n-    \/\/ each ShenandoahHeapRegion between bottom() and get_update_watermark()\n-    _verify_remembered_for_updating_references\n+    \/\/ Assure old objects are registered and remembered set cards within the read-write remembered set are dirty\n+    \/\/ for every interesting pointer within each OLD ShenandoahHeapRegion between bottom() and top().\n+    _verify_remembered_for_updating_references,\n+\n+    \/\/ Assure old objects are registered and remembered set cards within the read-write remembered set are dirty\n+    \/\/ for every interesting pointer within each OLD ShenandoahHeapRegion between bottom() and top().\n+    _verify_remembered_after_full_gc\n@@ -209,0 +213,1 @@\n+  void verify_after_generational_fullgc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":11,"deletions":6,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -89,15 +89,0 @@\n-void ShenandoahYoungGeneration::promote_all_regions() {\n-  \/\/ This only happens on a full stw collect. No allocations can happen here.\n-  shenandoah_assert_safepoint();\n-\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  for (size_t index = 0; index < heap->num_regions(); index++) {\n-    ShenandoahHeapRegion* r = heap->get_region(index);\n-    if (r->is_young()) {\n-      r->promote(true);\n-    }\n-  }\n-  assert(_affiliated_region_count == 0, \"young generation must not have affiliated regions after reset\");\n-  _used = 0;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -48,2 +48,0 @@\n-  void promote_all_regions();\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -101,3 +101,4 @@\n-          \"How much heap should be free before most heuristics trigger the \"\\\n-          \"collection, even without other triggers. Provides the safety \"   \\\n-          \"margin for many heuristics. In percents of (soft) max heap size.\")\\\n+          \"Percentage of free heap memory below which most heuristics \"     \\\n+          \"trigger collection independent of other triggers. Provides \"     \\\n+          \"a safety margin for many heuristics. In percents of (soft) \"     \\\n+          \"max heap size.\")                                                 \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"}]}