{"files":[{"patch":"@@ -124,3 +124,0 @@\n-  ShenandoahMarkingContext* const ctx = _generation->complete_marking_context();\n-\n-  size_t remnant_available = 0;\n@@ -152,1 +149,1 @@\n-          \/\/ If regions is presected, we know mode()->is_generational() and region->age() >= InitialTenuringThreshold)\n+          \/\/ If regions is preselected, we know mode()->is_generational() and region->age() >= InitialTenuringThreshold)\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":1,"deletions":4,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -31,0 +30,3 @@\n+#include \"gc\/shenandoah\/shenandoahCollectionSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n@@ -34,9 +36,6 @@\n-    ShenandoahHeuristics(generation),\n-    _old_collection_candidates(0),\n-    _next_old_collection_candidate(0),\n-    _hidden_old_collection_candidates(0),\n-    _hidden_next_old_collection_candidate(0),\n-    _old_coalesce_and_fill_candidates(0),\n-    _first_coalesce_and_fill_candidate(0),\n-    _trigger_heuristic(trigger_heuristic),\n-    _promotion_failed(false)\n+  ShenandoahHeuristics(generation),\n+  _last_old_collection_candidate(0),\n+  _next_old_collection_candidate(0),\n+  _last_old_region(0),\n+  _trigger_heuristic(trigger_heuristic),\n+  _promotion_failed(false)\n@@ -88,1 +87,1 @@\n-  \/\/ Key idea: if there is not sufficient memory within old-gen to hold an object that wants to be promoted, defer\n+  \/\/ Key idea: if there is insufficient memory within old-gen to hold an object that wants to be promoted, defer\n@@ -92,2 +91,0 @@\n-  const size_t promotion_budget_bytes = heap->get_promoted_reserve();\n-\n@@ -185,1 +182,1 @@\n-    if (region->is_regular()) {\n+    if (region->is_regular() || region->is_pinned()) {\n@@ -187,0 +184,1 @@\n+        assert(!region->is_pinned(), \"Pinned region should have live (pinned) objects.\");\n@@ -224,0 +222,1 @@\n+\n@@ -226,0 +225,3 @@\n+  _last_old_region = (uint)cand_idx;\n+  _last_old_collection_candidate = (uint)cand_idx;\n+\n@@ -230,10 +232,2 @@\n-      _hidden_next_old_collection_candidate = 0;\n-      _hidden_old_collection_candidates = (uint)i;\n-      _first_coalesce_and_fill_candidate = (uint)i;\n-      _old_coalesce_and_fill_candidates = (uint)(cand_idx - i);\n-\n-      \/\/ Note that we do not coalesce and fill occupied humongous regions\n-      \/\/ HR: humongous regions, RR: regular regions, CF: coalesce and fill regions\n-      log_info(gc)(\"Old-gen mark evac (\" UINT32_FORMAT \" RR, \" UINT32_FORMAT \" CF)\",\n-                   _hidden_old_collection_candidates, _old_coalesce_and_fill_candidates);\n-      return;\n+      _last_old_collection_candidate = (uint)i;\n+      break;\n@@ -243,6 +237,0 @@\n-  \/\/ If we reach here, all of non-humogous old-gen regions are candidates for collection set.\n-  _hidden_next_old_collection_candidate = 0;\n-  _hidden_old_collection_candidates = (uint)cand_idx;\n-  _first_coalesce_and_fill_candidate = 0;\n-  _old_coalesce_and_fill_candidates = 0;\n-\n@@ -255,1 +243,1 @@\n-               _hidden_old_collection_candidates, _old_coalesce_and_fill_candidates,\n+               _last_old_collection_candidate, _last_old_region - _last_old_collection_candidate,\n@@ -262,5 +250,1 @@\n-\n-  _old_collection_candidates = _hidden_old_collection_candidates;\n-  _next_old_collection_candidate = _hidden_next_old_collection_candidate;\n-\n-  _hidden_old_collection_candidates = 0;\n+  _next_old_collection_candidate = 0;\n@@ -269,1 +253,1 @@\n-uint ShenandoahOldHeuristics::unprocessed_old_or_hidden_collection_candidates() {\n+uint ShenandoahOldHeuristics::last_old_collection_candidate_index() {\n@@ -271,1 +255,1 @@\n-  return _old_collection_candidates + _hidden_old_collection_candidates;\n+  return _last_old_collection_candidate;\n@@ -276,1 +260,1 @@\n-  return _old_collection_candidates;\n+  return _last_old_collection_candidate - _next_old_collection_candidate;\n@@ -280,2 +264,6 @@\n-  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n-  return _region_data[_next_old_collection_candidate]._region;\n+  ShenandoahHeapRegion* next = _region_data[_next_old_collection_candidate]._region;\n+  if (next->is_pinned()) {\n+    \/\/ This region is pinned now and cannot be collected. Swap it to the back of the\n+    \/\/ list of candidates\n+  }\n+  return next;\n@@ -287,1 +275,0 @@\n-  _old_collection_candidates--;\n@@ -290,1 +277,1 @@\n-uint ShenandoahOldHeuristics::old_coalesce_and_fill_candidates() {\n+uint ShenandoahOldHeuristics::last_old_region_index() {\n@@ -292,1 +279,1 @@\n-  return _old_coalesce_and_fill_candidates;\n+  return _last_old_region;\n@@ -297,3 +284,3 @@\n-  uint count = _old_coalesce_and_fill_candidates;\n-  int index = _first_coalesce_and_fill_candidate;\n-  while (count-- > 0) {\n+  uint end = _last_old_region;\n+  uint index = _next_old_collection_candidate;\n+  while (index < end) {\n@@ -305,1 +292,1 @@\n-  _old_collection_candidates = 0;\n+  _last_old_collection_candidate = 0;\n@@ -307,4 +294,1 @@\n-  _hidden_old_collection_candidates = 0;\n-  _hidden_next_old_collection_candidate = 0;\n-  _old_coalesce_and_fill_candidates = 0;\n-  _first_coalesce_and_fill_candidate = 0;\n+  _last_old_region = 0;\n@@ -332,1 +316,1 @@\n-  if (unprocessed_old_or_hidden_collection_candidates() > 0) {\n+  if (unprocessed_old_collection_candidates() > 0) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":37,"deletions":53,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -28,4 +28,1 @@\n-#include \"gc\/shenandoah\/shenandoahCollectionSet.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+\n@@ -34,0 +31,4 @@\n+class ShenandoahCollectionSet;\n+class ShenandoahGeneration;\n+class ShenandoahHeapRegion;\n+\n@@ -38,1 +39,1 @@\n-  \/\/ if (_generation->generation_mode() == OLD) _old_collection_candidates\n+  \/\/ if (_generation->generation_mode() == OLD) _last_old_collection_candidate\n@@ -45,1 +46,1 @@\n-  uint _old_collection_candidates;\n+  uint _last_old_collection_candidate;\n@@ -47,18 +48,1 @@\n-\n-  \/\/ At the time we select the old-gen collection set, _hidden_old_collection_candidates\n-  \/\/ and _hidden_next_old_collection_candidates are set to remember the intended old-gen\n-  \/\/ collection set.  After all old-gen regions not in the old-gen collection set have been\n-  \/\/ coalesced and filled, the content of these variables is copied to _old_collection_candidates\n-  \/\/ and _next_old_collection_candidates so that evacuations can begin evacuating these regions.\n-  uint _hidden_old_collection_candidates;\n-  uint _hidden_next_old_collection_candidate;\n-\n-  \/\/ if (_generation->generation_mode() == OLD)\n-  \/\/  _old_coalesce_and_fill_candidates represents the number of regions\n-  \/\/  that were chosen for the garbage contained therein to be coalesced\n-  \/\/  and filled and _first_coalesce_and_fill_candidate represents the\n-  \/\/  the index of the first such region within the _region_data array.\n-  \/\/ if (_generation->generation_mode() != OLD) these two variables are\n-  \/\/  not used.\n-  uint _old_coalesce_and_fill_candidates;\n-  uint _first_coalesce_and_fill_candidate;\n+  uint _last_old_region;\n@@ -72,3 +56,0 @@\n-  \/\/ Prepare for evacuation of old-gen regions by capturing the mark results of a recently completed concurrent mark pass.\n-  void prepare_for_old_collections();\n-\n@@ -84,0 +65,3 @@\n+  \/\/ Prepare for evacuation of old-gen regions by capturing the mark results of a recently completed concurrent mark pass.\n+  void prepare_for_old_collections();\n+\n@@ -95,1 +79,1 @@\n-  uint unprocessed_old_or_hidden_collection_candidates();\n+  uint last_old_collection_candidate_index();\n@@ -107,1 +91,1 @@\n-  uint old_coalesce_and_fill_candidates();\n+  uint last_old_region_index();\n@@ -111,1 +95,1 @@\n-  \/\/ old_coalesce_and_fill_candidates() entries, or memory may be corrupted when this function overwrites the\n+  \/\/ last_old_region_index() entries, or memory may be corrupted when this function overwrites the\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":14,"deletions":30,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -260,1 +260,1 @@\n-  VM_ShenandoahInitMark op(this, _do_old_gc_bootstrap);\n+  VM_ShenandoahInitMark op(this);\n@@ -569,1 +569,1 @@\n-  _generation->prepare_gc(_do_old_gc_bootstrap);\n+  _generation->prepare_gc();\n@@ -638,0 +638,4 @@\n+    \/\/ TODO: We should be able to pull this out of the safepoint for the bootstrap\n+    \/\/ cycle. The top of an old region will only move when a GC cycle evacuates\n+    \/\/ objects into it. When we start an old cycle, we know that nothing can touch\n+    \/\/ the top of old regions.\n@@ -641,1 +645,0 @@\n-    heap->old_generation()->parallel_heap_region_iterate(&cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -214,1 +215,1 @@\n-          set_gc_mode(marking_old);\n+          set_gc_mode(servicing_old);\n@@ -228,1 +229,1 @@\n-      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n@@ -233,1 +234,1 @@\n-                     BOOL_TO_STR(heap->is_concurrent_prep_for_mixed_evacuation_in_progress()));\n+                     BOOL_TO_STR(heap->is_prepare_for_old_mark_in_progress()));\n@@ -237,1 +238,1 @@\n-        set_gc_mode(marking_old);\n+        set_gc_mode(servicing_old);\n@@ -290,1 +291,1 @@\n-          case marking_old: {\n+          case servicing_old: {\n@@ -292,1 +293,1 @@\n-            resume_concurrent_old_cycle(heap->old_generation(), cause);\n+            service_concurrent_old_cycle(heap, cause);\n@@ -474,6 +475,2 @@\n-  \/\/ Configure the young generation's concurrent mark to put objects in\n-  \/\/ old regions into the concurrent mark queues associated with the old\n-  \/\/ generation. The young cycle will run as normal except that rather than\n-  \/\/ ignore old references it will mark and enqueue them in the old concurrent\n-  \/\/ mark but it will not traverse them.\n-  ShenandoahGeneration* old_generation = heap->old_generation();\n+\n+  ShenandoahOldGeneration* old_generation = (ShenandoahOldGeneration*)heap->old_generation();\n@@ -481,0 +478,7 @@\n+  switch (old_generation->state()) {\n+    case ShenandoahOldGeneration::IDLE: {\n+      assert(!heap->is_concurrent_old_mark_in_progress(), \"Old already in progress.\");\n+      assert(old_generation->task_queues()->is_empty(), \"Old mark queues should be empty.\");\n+    }\n+    case ShenandoahOldGeneration::FILLING: {\n+      _allow_old_preemption.set();\n@@ -482,2 +486,1 @@\n-  assert(!heap->is_concurrent_old_mark_in_progress(), \"Old already in progress.\");\n-  assert(old_generation->task_queues()->is_empty(), \"Old mark queues should be empty.\");\n+      old_generation->prepare_gc();\n@@ -485,4 +488,1 @@\n-  young_generation->set_old_gen_task_queues(old_generation->task_queues());\n-  young_generation->set_mark_incomplete();\n-  old_generation->set_mark_incomplete();\n-  service_concurrent_cycle(young_generation, cause, true);\n+      _allow_old_preemption.unset();\n@@ -490,1 +490,4 @@\n-  process_phase_timings(heap);\n+      if (heap->is_prepare_for_old_mark_in_progress()) {\n+        assert(old_generation->state() == ShenandoahOldGeneration::FILLING, \"Prepare for mark should be in progress.\");\n+        return;\n+      }\n@@ -492,19 +495,15 @@\n-  if (heap->cancelled_gc()) {\n-    \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n-    \/\/ is not going to resume after degenerated young cycle completes.\n-    log_info(gc)(\"Bootstrap cycle for old generation was cancelled.\");\n-  } else {\n-    \/\/ Reset the degenerated point. Normally this would happen at the top\n-    \/\/ of the control loop, but here we have just completed a young cycle\n-    \/\/ which has bootstrapped the old concurrent marking.\n-    _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n-\n-    \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n-    \/\/ and init mark for the concurrent mark. All of that work will have been\n-    \/\/ done by the bootstrapping young cycle. In order to simplify the debugging\n-    \/\/ effort, the old cycle will ONLY complete the mark phase. No actual\n-    \/\/ collection of the old generation is happening here.\n-    set_gc_mode(marking_old);\n-    resume_concurrent_old_cycle(old_generation, cause);\n-  }\n-}\n+      assert(old_generation->state() == ShenandoahOldGeneration::BOOTSTRAPPING, \"Finished with filling, should be bootstrapping.\");\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      service_concurrent_cycle(young_generation, cause, true);\n+      process_phase_timings(heap);\n+      if (heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled.\");\n+        return;\n+      }\n@@ -512,14 +511,18 @@\n-bool ShenandoahControlThread::check_soft_max_changed() const {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n-  size_t old_soft_max = heap->soft_max_capacity();\n-  if (new_soft_max != old_soft_max) {\n-    new_soft_max = MAX2(heap->min_capacity(), new_soft_max);\n-    new_soft_max = MIN2(heap->max_capacity(), new_soft_max);\n-    if (new_soft_max != old_soft_max) {\n-      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n-                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n-                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n-      );\n-      heap->set_soft_max_capacity(new_soft_max);\n-      return true;\n+      \/\/ Reset the degenerated point. Normally this would happen at the top\n+      \/\/ of the control loop, but here we have just completed a young cycle\n+      \/\/ which has bootstrapped the old concurrent marking.\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle. In order to simplify the debugging\n+      \/\/ effort, the old cycle will ONLY complete the mark phase. No actual\n+      \/\/ collection of the old generation is happening here.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking.\");\n+      }\n@@ -527,0 +530,3 @@\n+    default:\n+      log_error(gc)(\"Unexpected state for old GC: %d\", old_generation->state());\n+      ShouldNotReachHere();\n@@ -528,1 +534,0 @@\n-  return false;\n@@ -531,1 +536,1 @@\n-void ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+bool ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n@@ -533,2 +538,1 @@\n-  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress() ||\n-         ShenandoahHeap::heap()->is_concurrent_prep_for_mixed_evacuation_in_progress(),\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(),\n@@ -549,2 +553,1 @@\n-    generation->heuristics()->record_success_concurrent(false);\n-    heap->shenandoah_policy()->record_success_old();\n+    generation->record_success_concurrent(false);\n@@ -567,0 +570,20 @@\n+    return false;\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahControlThread::check_soft_max_changed() const {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n+  size_t old_soft_max = heap->soft_max_capacity();\n+  if (new_soft_max != old_soft_max) {\n+    new_soft_max = MAX2(heap->min_capacity(), new_soft_max);\n+    new_soft_max = MIN2(heap->max_capacity(), new_soft_max);\n+    if (new_soft_max != old_soft_max) {\n+      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n+                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n+                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n+      );\n+      heap->set_soft_max_capacity(new_soft_max);\n+      return true;\n+    }\n@@ -568,0 +591,1 @@\n+  return false;\n@@ -617,2 +641,1 @@\n-    generation->heuristics()->record_success_concurrent(gc.abbreviated());\n-    heap->shenandoah_policy()->record_success_concurrent();\n+    generation->record_success_concurrent(gc.abbreviated());\n@@ -696,0 +719,6 @@\n+  } else {\n+    assert(_degen_generation->generation_mode() == YOUNG, \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old_generation = (ShenandoahOldGeneration*) heap->old_generation();\n+    if (old_generation->state() == ShenandoahOldGeneration::BOOTSTRAPPING && !gc.upgraded_to_full()) {\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n@@ -948,1 +977,1 @@\n-    case marking_old:       return \"old mark\";\n+    case servicing_old:       return \"old mark\";\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":90,"deletions":61,"binary":false,"changes":151,"status":"modified"},{"patch":"@@ -76,1 +76,1 @@\n-    marking_old\n+    servicing_old\n@@ -106,1 +106,1 @@\n-  void resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -322,1 +322,1 @@\n-  _generation->prepare_gc(false);\n+  _generation->prepare_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -207,1 +207,1 @@\n-void ShenandoahGeneration::prepare_gc(bool do_old_gc_bootstrap) {\n+void ShenandoahGeneration::prepare_gc() {\n@@ -210,5 +210,0 @@\n-  if (do_old_gc_bootstrap) {\n-    \/\/ Reset mark bitmap for old regions also.  Note that do_old_gc_bootstrap is only true if this generation is YOUNG.\n-    ShenandoahHeap::heap()->old_generation()->reset_mark_bitmap();\n-  }\n-\n@@ -218,4 +213,0 @@\n-  if (do_old_gc_bootstrap) {\n-    \/\/ Capture top at mark start for both old-gen regions also.  Note that do_old_gc_bootstrap is only true if generation is YOUNG.\n-    ShenandoahHeap::heap()->old_generation()->parallel_heap_region_iterate(&cl);\n-  }\n@@ -235,1 +226,0 @@\n-\n@@ -237,1 +227,0 @@\n-    heap->assert_pinned_region_status();\n@@ -240,2 +229,5 @@\n-      \/\/ Also capture update_watermark for old-gen regions.\n-      ShenandoahCaptureUpdateWaterMarkForOld old_cl(complete_marking_context());\n+      \/\/ We always need to update the watermark for old regions. If there\n+      \/\/ are mixed collections pending, we also need to synchronize the\n+      \/\/ pinned status for old regions. Since we are already visiting every\n+      \/\/ old region here, go ahead and sync the pin status too.\n+      ShenandoahFinalMarkUpdateRegionStateClosure old_cl(nullptr);\n@@ -244,0 +236,2 @@\n+\n+    heap->assert_pinned_region_status();\n@@ -253,1 +247,0 @@\n-    size_t avail_evac_reserve_for_loan_to_young_gen = 0;\n@@ -312,1 +305,0 @@\n-        avail_evac_reserve_for_loan_to_young_gen = minimum_evacuation_reserve - old_evacuation_reserve;\n@@ -319,1 +311,1 @@\n-      \/\/ Compute the young evauation reserve: This is how much memory is available for evacuating young-gen objects.\n+      \/\/ Compute the young evacuation reserve: This is how much memory is available for evacuating young-gen objects.\n@@ -459,1 +451,0 @@\n-      size_t immediate_garbage_regions = collection_set->get_immediate_trash() \/ region_size_bytes;\n@@ -760,0 +751,10 @@\n+\n+void ShenandoahGeneration::record_success_concurrent(bool abbreviated) {\n+  heuristics()->record_success_concurrent(false);\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_concurrent();\n+}\n+\n+void ShenandoahGeneration::record_success_degenerated() {\n+  heuristics()->record_success_degenerated();\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_degenerated();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":20,"deletions":19,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -112,1 +112,1 @@\n-  void prepare_gc(bool do_old_gc_bootstrap);\n+  virtual void prepare_gc();\n@@ -162,0 +162,3 @@\n+\n+  virtual void record_success_concurrent(bool abbreviated);\n+  virtual void record_success_degenerated();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -500,1 +500,1 @@\n-  _prep_for_mixed_evac_in_progress(false),\n+  _prepare_for_old_mark(false),\n@@ -1026,0 +1026,1 @@\n+  ((ShenandoahOldGeneration*) _old_generation)->transition_to(ShenandoahOldGeneration::IDLE);\n@@ -1029,1 +1030,1 @@\n-  set_concurrent_prep_for_mixed_evacuation_in_progress(false);\n+  set_prepare_for_old_mark_in_progress(false);\n@@ -1038,3 +1039,3 @@\n-      || is_concurrent_prep_for_mixed_evacuation_in_progress()\n-      || old_heuristics()->unprocessed_old_or_hidden_collection_candidates() > 0\n-      || young_generation()->old_gen_task_queues() != nullptr;\n+         || is_prepare_for_old_mark_in_progress()\n+         || old_heuristics()->unprocessed_old_collection_candidates() > 0\n+         || young_generation()->old_gen_task_queues() != nullptr;\n@@ -2122,1 +2123,1 @@\n-void ShenandoahHeap::set_concurrent_prep_for_mixed_evacuation_in_progress(bool in_progress) {\n+void ShenandoahHeap::set_prepare_for_old_mark_in_progress(bool in_progress) {\n@@ -2125,5 +2126,1 @@\n-  _prep_for_mixed_evac_in_progress = in_progress;\n-}\n-\n-bool ShenandoahHeap::is_concurrent_prep_for_mixed_evacuation_in_progress() {\n-  return _prep_for_mixed_evac_in_progress;\n+  _prepare_for_old_mark = in_progress;\n@@ -2916,1 +2913,1 @@\n-      is_concurrent_prep_for_mixed_evacuation_in_progress() || active_generation()->generation_mode() == GLOBAL) {\n+    is_prepare_for_old_mark_in_progress() || active_generation()->generation_mode() == GLOBAL) {\n@@ -3051,1 +3048,1 @@\n-      is_concurrent_prep_for_mixed_evacuation_in_progress() || active_generation()->generation_mode() == GLOBAL) {\n+    is_prepare_for_old_mark_in_progress() || active_generation()->generation_mode() == GLOBAL) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":10,"deletions":13,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -154,1 +154,2 @@\n-  bool _prep_for_mixed_evac_in_progress; \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+  \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+  bool _prepare_for_old_mark;\n@@ -400,1 +401,1 @@\n-  void set_concurrent_prep_for_mixed_evacuation_in_progress(bool cond);\n+  void set_prepare_for_old_mark_in_progress(bool cond);\n@@ -418,1 +419,1 @@\n-  bool is_concurrent_prep_for_mixed_evacuation_in_progress();\n+  inline bool is_prepare_for_old_mark_in_progress() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -671,0 +671,4 @@\n+inline bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return _prepare_for_old_mark;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -40,6 +40,8 @@\n-    \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-    \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-    HeapWord *tams = _ctx->top_at_mark_start(r);\n-    HeapWord *top = r->top();\n-    if (top > tams) {\n-      r->increase_live_data_alloc_words(pointer_delta(top, tams));\n+    if (_ctx != nullptr) {\n+      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n+      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+      if (top > tams) {\n+        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n+      }\n@@ -68,2 +70,4 @@\n-    assert(_ctx->top_at_mark_start(r) == r->top(),\n-           \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n+    if (_ctx != nullptr) {\n+      assert(_ctx->top_at_mark_start(r) == r->top(),\n+             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n+    }\n@@ -72,8 +76,0 @@\n-\n-ShenandoahCaptureUpdateWaterMarkForOld::ShenandoahCaptureUpdateWaterMarkForOld(ShenandoahMarkingContext* ctx) :\n-  _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-void ShenandoahCaptureUpdateWaterMarkForOld::heap_region_do(ShenandoahHeapRegion* r) {\n-  \/\/ Remember limit for updating refs. It's guaranteed that we get no from-space-refs written from here on.\n-  r->set_update_watermark_at_safepoint(r->top());\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.cpp","additions":12,"deletions":16,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -38,13 +38,1 @@\n-  ShenandoahFinalMarkUpdateRegionStateClosure(ShenandoahMarkingContext* ctx);\n-\n-  void heap_region_do(ShenandoahHeapRegion* r);\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-class ShenandoahCaptureUpdateWaterMarkForOld : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-public:\n-  ShenandoahCaptureUpdateWaterMarkForOld(ShenandoahMarkingContext* ctx);\n+  explicit ShenandoahFinalMarkUpdateRegionStateClosure(ShenandoahMarkingContext* ctx);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.hpp","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -34,2 +34,0 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n-#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n@@ -39,18 +37,0 @@\n-class ShenandoahConcurrentCoalesceAndFillTask : public WorkerTask {\n-private:\n-  uint _nworkers;\n-  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n-  uint _coalesce_and_fill_region_count;\n-  ShenandoahConcurrentGC* _old_gc;\n-  volatile bool _is_preempted;\n-\n-public:\n-  ShenandoahConcurrentCoalesceAndFillTask(uint nworkers, ShenandoahHeapRegion** coalesce_and_fill_region_array,\n-                                          uint region_count, ShenandoahConcurrentGC* old_gc) :\n-    WorkerTask(\"Shenandoah Concurrent Coalesce and Fill\"),\n-    _nworkers(nworkers),\n-    _coalesce_and_fill_region_array(coalesce_and_fill_region_array),\n-    _coalesce_and_fill_region_count(region_count),\n-    _old_gc(old_gc),\n-    _is_preempted(false) {\n-  }\n@@ -58,20 +38,0 @@\n-  void work(uint worker_id) {\n-    for (uint region_idx = worker_id; region_idx < _coalesce_and_fill_region_count; region_idx += _nworkers) {\n-      ShenandoahHeapRegion* r = _coalesce_and_fill_region_array[region_idx];\n-      if (!r->is_humongous()) {\n-        if (!r->oop_fill_and_coalesce()) {\n-          \/\/ Coalesce and fill has been preempted\n-          Atomic::store(&_is_preempted, true);\n-          return;\n-        }\n-      } else {\n-        \/\/ there's only one object in this region and it's not garbage, so no need to coalesce or fill\n-      }\n-    }\n-  }\n-\n-  \/\/ Value returned from is_completed() is only valid after all worker thread have terminated.\n-  bool is_completed() {\n-    return !Atomic::load(&_is_preempted);\n-  }\n-};\n@@ -82,1 +42,0 @@\n-  _coalesce_and_fill_region_array = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, ShenandoahHeap::heap()->num_regions(), mtGC);\n@@ -91,1 +50,0 @@\n-\n@@ -136,0 +94,1 @@\n+  assert(!heap->is_prepare_for_old_mark_in_progress(), \"Old regions need to be parseable during concurrent mark.\");\n@@ -137,12 +96,5 @@\n-  if (!heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n-    \/\/ Skip over the initial phases of old collect if we're resuming mixed evacuation preparation.\n-    \/\/ Continue concurrent mark, do not reset regions, do not mark roots, do not collect $200.\n-    _allow_preemption.set();\n-    entry_mark();\n-    if (!_allow_preemption.try_unset()) {\n-      \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n-      \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the cancellation.\n-      while (!heap->cancelled_gc()) {\n-        SpinPause();\n-      }\n-    }\n+  \/\/ Enable preemption of old generation mark.\n+  _allow_preemption.set();\n+\n+  \/\/ Continue concurrent mark, do not reset regions, do not mark roots, do not collect $200.\n+  entry_mark();\n@@ -150,2 +102,7 @@\n-    if (heap->cancelled_gc()) {\n-      return false;\n+  \/\/ If we failed to unset the preemption flag, it means another thread has already unset it.\n+  if (!_allow_preemption.try_unset()) {\n+    \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n+    \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the\n+    \/\/ cancellation.\n+    while (!heap->cancelled_gc()) {\n+      SpinPause();\n@@ -153,0 +110,1 @@\n+  }\n@@ -154,2 +112,3 @@\n-    \/\/ Complete marking under STW\n-    vmop_entry_final_mark();\n+  if (heap->cancelled_gc()) {\n+    return false;\n+  }\n@@ -157,3 +116,2 @@\n-    \/\/ We aren't dealing with old generation evacuation yet. Our heuristic\n-    \/\/ should not have built a cset in final mark.\n-    assert(!heap->is_evacuation_in_progress(), \"Old gen evacuations are not supported\");\n+  \/\/ Complete marking under STW\n+  vmop_entry_final_mark();\n@@ -161,5 +119,3 @@\n-    \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n-    if (heap->is_concurrent_weak_root_in_progress()) {\n-      entry_weak_refs();\n-      entry_weak_roots();\n-    }\n+  \/\/ We aren't dealing with old generation evacuation yet. Our heuristic\n+  \/\/ should not have built a cset in final mark.\n+  assert(!heap->is_evacuation_in_progress(), \"Old gen evacuations are not supported\");\n@@ -167,3 +123,5 @@\n-    \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n-    \/\/ the space. This would be the last action if there is nothing to evacuate.\n-    entry_cleanup_early();\n+  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n+    entry_weak_refs();\n+    entry_weak_roots();\n+  }\n@@ -171,4 +129,3 @@\n-    {\n-      ShenandoahHeapLocker locker(heap->lock());\n-      heap->free_set()->log_status();\n-    }\n+  \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  entry_cleanup_early();\n@@ -176,0 +133,4 @@\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->log_status();\n+  }\n@@ -177,6 +138,0 @@\n-    \/\/ TODO: Old marking doesn't support class unloading yet\n-    \/\/ Perform concurrent class unloading\n-    \/\/ if (heap->unload_classes() &&\n-    \/\/     heap->is_concurrent_weak_root_in_progress()) {\n-    \/\/   entry_class_unloading();\n-    \/\/ }\n@@ -184,2 +139,6 @@\n-    heap->set_concurrent_prep_for_mixed_evacuation_in_progress(true);\n-  }\n+  \/\/ TODO: Old marking doesn't support class unloading yet\n+  \/\/ Perform concurrent class unloading\n+  \/\/ if (heap->unload_classes() &&\n+  \/\/     heap->is_concurrent_weak_root_in_progress()) {\n+  \/\/   entry_class_unloading();\n+  \/\/ }\n@@ -196,29 +155,1 @@\n-  \/\/ Coalesce and fill objects _after_ weak root processing and class unloading.\n-  \/\/ Weak root and reference processing makes assertions about unmarked referents\n-  \/\/ that will fail if they've been overwritten with filler objects. There is also\n-  \/\/ a case in the LRB that permits access to from-space objects for the purpose\n-  \/\/ of class unloading that is unlikely to function correctly if the object has\n-  \/\/ been filled.\n-  _allow_preemption.set();\n-\n-  if (heap->cancelled_gc()) {\n-    return false;\n-  }\n-\n-  if (heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n-    if (!entry_coalesce_and_fill()) {\n-      \/\/ If an allocation failure occurs during coalescing, we will run a degenerated\n-      \/\/ cycle for the young generation. This should be a rare event.  Normally, we'll\n-      \/\/ resume the coalesce-and-fill effort after the preempting young-gen GC finishes.\n-      return false;\n-    }\n-  }\n-  if (!_allow_preemption.try_unset()) {\n-    \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n-    \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the cancellation.\n-    while (!heap->cancelled_gc()) {\n-      SpinPause();\n-    }\n-  }\n-  \/\/ Prepare for old evacuations (actual evacuations will happen on subsequent young collects).  This cannot\n-  \/\/ begin until after we have completed coalesce-and-fill.\n+  \/\/ Prepare for old evacuations (actual evacuations will happen on subsequent young collects).\n@@ -229,46 +160,0 @@\n-\n-void ShenandoahOldGC::entry_coalesce_and_fill_message(char *buf, size_t len) const {\n-  \/\/ ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  jio_snprintf(buf, len, \"Coalescing and filling (%s)\", _generation->name());\n-}\n-\n-bool ShenandoahOldGC::op_coalesce_and_fill() {\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n-  WorkerThreads* workers = heap->workers();\n-  uint nworkers = workers->active_workers();\n-\n-  assert(_generation->generation_mode() == OLD, \"Only old-GC does coalesce and fill\");\n-  log_debug(gc)(\"Starting (or resuming) coalesce-and-fill of old heap regions\");\n-  uint coalesce_and_fill_regions_count = old_heuristics->old_coalesce_and_fill_candidates();\n-  assert(coalesce_and_fill_regions_count <= heap->num_regions(), \"Sanity\");\n-  old_heuristics->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n-  ShenandoahConcurrentCoalesceAndFillTask task(nworkers, _coalesce_and_fill_region_array, coalesce_and_fill_regions_count, this);\n-\n-  workers->run_task(&task);\n-  if (task.is_completed()) {\n-    \/\/ Remember that we're done with coalesce-and-fill.\n-    heap->set_concurrent_prep_for_mixed_evacuation_in_progress(false);\n-    return true;\n-  } else {\n-    log_debug(gc)(\"Suspending coalesce-and-fill of old heap regions\");\n-    \/\/ Otherwise, we got preempted before the work was done.\n-    return false;\n-  }\n-}\n-\n-bool ShenandoahOldGC::entry_coalesce_and_fill() {\n-  char msg[1024];\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-\n-  entry_coalesce_and_fill_message(msg, sizeof(msg));\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::coalesce_and_fill);\n-\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n-  EventMark em(\"%s\", msg);\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n-                              \"concurrent coalesce and fill\");\n-\n-  return op_coalesce_and_fill();\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":41,"deletions":156,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n@@ -46,1 +45,0 @@\n-  bool entry_coalesce_and_fill();\n@@ -48,2 +46,0 @@\n-  bool op_coalesce_and_fill();\n-  void entry_coalesce_and_fill_message(char *buf, size_t len) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n@@ -45,0 +47,3 @@\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"utilities\/events.hpp\"\n@@ -127,0 +132,39 @@\n+class ShenandoahConcurrentCoalesceAndFillTask : public WorkerTask {\n+ private:\n+  uint _nworkers;\n+  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n+  uint _coalesce_and_fill_region_count;\n+  volatile bool _is_preempted;\n+\n+ public:\n+  ShenandoahConcurrentCoalesceAndFillTask(uint nworkers, ShenandoahHeapRegion** coalesce_and_fill_region_array,\n+                                          uint region_count) :\n+    WorkerTask(\"Shenandoah Concurrent Coalesce and Fill\"),\n+    _nworkers(nworkers),\n+    _coalesce_and_fill_region_array(coalesce_and_fill_region_array),\n+    _coalesce_and_fill_region_count(region_count),\n+    _is_preempted(false) {\n+  }\n+\n+  void work(uint worker_id) {\n+    for (uint region_idx = worker_id; region_idx < _coalesce_and_fill_region_count; region_idx += _nworkers) {\n+      ShenandoahHeapRegion* r = _coalesce_and_fill_region_array[region_idx];\n+      if (r->is_humongous()) {\n+        \/\/ there's only one object in this region and it's not garbage, so no need to coalesce or fill\n+        continue;\n+      }\n+\n+      if (!r->oop_fill_and_coalesce()) {\n+        \/\/ Coalesce and fill has been preempted\n+        Atomic::store(&_is_preempted, true);\n+        return;\n+      }\n+    }\n+  }\n+\n+  \/\/ Value returned from is_completed() is only valid after all worker thread have terminated.\n+  bool is_completed() {\n+    return !Atomic::load(&_is_preempted);\n+  }\n+};\n+\n@@ -128,1 +172,4 @@\n-  : ShenandoahGeneration(OLD, max_queues, max_capacity, soft_max_capacity) {\n+  : ShenandoahGeneration(OLD, max_queues, max_capacity, soft_max_capacity),\n+    _coalesce_and_fill_region_array(NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, ShenandoahHeap::heap()->num_regions(), mtGC)),\n+    _state(IDLE)\n+{\n@@ -168,0 +215,54 @@\n+void ShenandoahOldGeneration::prepare_gc() {\n+\n+  \/\/ Make the old generation regions parseable, so they can be safely\n+  \/\/ scanned when looking for objects in memory indicated by dirty cards.\n+  entry_coalesce_and_fill();\n+\n+  \/\/ Now that we have made the old generation parseable, it is safe to reset the mark bitmap.\n+  ShenandoahGeneration::prepare_gc();\n+}\n+\n+bool ShenandoahOldGeneration::entry_coalesce_and_fill() {\n+  char msg[1024];\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  ShenandoahConcurrentPhase gc_phase(\"Coalescing and filling (OLD)\", ShenandoahPhaseTimings::coalesce_and_fill);\n+\n+  \/\/ TODO: I don't think we're using these concurrent collection counters correctly.\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent coalesce and fill\");\n+\n+  return coalesce_and_fill();\n+}\n+\n+bool ShenandoahOldGeneration::coalesce_and_fill() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  heap->set_prepare_for_old_mark_in_progress(true);\n+  transition_to(FILLING);\n+\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  WorkerThreads* workers = heap->workers();\n+  uint nworkers = workers->active_workers();\n+\n+  log_debug(gc)(\"Starting (or resuming) coalesce-and-fill of old heap regions\");\n+  uint coalesce_and_fill_regions_count = old_heuristics->last_old_region_index();\n+  assert(coalesce_and_fill_regions_count <= heap->num_regions(), \"Sanity\");\n+  old_heuristics->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n+  ShenandoahConcurrentCoalesceAndFillTask task(nworkers, _coalesce_and_fill_region_array, coalesce_and_fill_regions_count);\n+\n+  workers->run_task(&task);\n+  if (task.is_completed()) {\n+    \/\/ Remember that we're done with coalesce-and-fill.\n+    transition_to(BOOTSTRAPPING);\n+    heap->set_prepare_for_old_mark_in_progress(false);\n+    return true;\n+  } else {\n+    log_debug(gc)(\"Suspending coalesce-and-fill of old heap regions\");\n+    \/\/ Otherwise, we got preempted before the work was done.\n+    return false;\n+  }\n+}\n+\n@@ -197,0 +298,2 @@\n+    \/\/ This doesn't actually choose a collection set, but prepares a list of\n+    \/\/ regions as 'candidates' for inclusion in a mixed collection.\n@@ -203,0 +306,2 @@\n+    \/\/ Though we did not choose a collection set above, we still may have\n+    \/\/ freed up immediate garbage regions so proceed with rebuilding the free set.\n@@ -209,0 +314,7 @@\n+void ShenandoahOldGeneration::transition_to(State new_state) {\n+  if (_state != new_state) {\n+    log_info(gc)(\"Old generation transition from %d to %d\", _state, new_state);\n+    _state = new_state;\n+  }\n+}\n+\n@@ -227,0 +339,5 @@\n+\n+void ShenandoahOldGeneration::record_success_concurrent(bool abbreviated) {\n+  heuristics()->record_success_concurrent(false);\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_old();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":118,"deletions":1,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -51,0 +51,2 @@\n+  virtual void prepare_gc() override;\n+\n@@ -76,0 +78,20 @@\n+\n+  virtual void record_success_concurrent(bool abbreviated) override;\n+\n+  enum State {\n+    IDLE, FILLING, BOOTSTRAPPING, MARKING, WAITING\n+  };\n+\n+  void transition_to(State new_state);\n+\n+  State state() const {\n+    return _state;\n+  }\n+\n+ private:\n+  bool entry_coalesce_and_fill();\n+  bool coalesce_and_fill();\n+\n+  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n+\n+  State _state;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -85,1 +85,1 @@\n-    } else if (mode == ShenandoahControlThread::marking_old) {\n+    } else if (mode == ShenandoahControlThread::servicing_old) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -511,1 +511,1 @@\n-  if (heap->doing_mixed_evacuations() || heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+  if (heap->doing_mixed_evacuations() || heap->is_prepare_for_old_mark_in_progress()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -63,1 +63,0 @@\n-  const bool _do_old_gc_bootstrap;\n@@ -65,1 +64,1 @@\n-  VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc, bool do_old_gc_bootstrap) :\n+  explicit VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc) :\n@@ -67,2 +66,1 @@\n-    _gc(gc),\n-    _do_old_gc_bootstrap(do_old_gc_bootstrap) {};\n+    _gc(gc) {};\n@@ -78,1 +76,1 @@\n-  VM_ShenandoahFinalMarkStartEvac(ShenandoahConcurrentGC* gc) :\n+  explicit VM_ShenandoahFinalMarkStartEvac(ShenandoahConcurrentGC* gc) :\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  if (_old_gen_task_queues != nullptr && in_progress && !heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+  if (_old_gen_task_queues != nullptr && in_progress && !heap->is_prepare_for_old_mark_in_progress()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,191 @@\n+#include \"precompiled.hpp\"\n+#include \"unittest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+\n+\/\/ These tests will all be skipped (unless Shenandoah becomes the default\n+\/\/ collector). To execute these tests, you must enable Shenandoah, which\n+\/\/ is done with:\n+\/\/\n+\/\/ % _JAVA_OPTIONS=\"-XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\" make exploded-test TEST=\"gtest:Shenandoah*\"\n+\/\/\n+\/\/ Please note that these 'unit' tests are really integration tests and rely\n+\/\/ on the JVM being initialized. These tests manipulate the state of the\n+\/\/ collector in ways that are not compatible with a normal collection run.\n+\/\/ If these tests take longer than the minimum time between gc intervals -\n+\/\/ or, more likely, if you have them paused in a debugger longer than this\n+\/\/ interval - you can expect trouble.\n+\n+#define SKIP_IF_NOT_SHENANDOAH() \\\n+    if (!UseShenandoahGC) {      \\\n+      tty->print_cr(\"skipped\");  \\\n+      return;                    \\\n+    }\n+\n+class ShenandoahResetRegions : public ShenandoahHeapRegionClosure {\n+ public:\n+  virtual void heap_region_do(ShenandoahHeapRegion* region) override {\n+    if (!region->is_empty()) {\n+      region->make_trash();\n+      region->make_empty();\n+    }\n+    region->set_affiliation(FREE);\n+    region->clear_live_data();\n+    region->set_top(region->bottom());\n+  }\n+};\n+\n+class ShenandoahOldHeuristicTest : public ::testing::Test {\n+ protected:\n+  ShenandoahHeap* _heap;\n+  ShenandoahOldHeuristics* _heuristics;\n+  ShenandoahCollectionSet* _collection_set;\n+\n+  ShenandoahOldHeuristicTest()\n+    : _heap(ShenandoahHeap::heap()),\n+      _heuristics(_heap->old_heuristics()),\n+      _collection_set(_heap->collection_set()) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahResetRegions reset;\n+    _heap->heap_region_iterate(&reset);\n+    _heuristics->abandon_collection_candidates();\n+    _collection_set->clear();\n+  }\n+\n+  size_t make_garbage(size_t region_idx, size_t garbage_bytes) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->make_regular_allocation(OLD_GENERATION);\n+    region->increase_live_data_alloc_words(1);\n+    region->set_top(region->bottom() + garbage_bytes \/ HeapWordSize);\n+    return region->garbage();\n+  }\n+\n+  size_t create_too_much_garbage_for_one_mixed_evacuation() {\n+    size_t garbage_target = _heap->old_generation()->soft_max_capacity() \/ 2;\n+    size_t garbage_total = 0;\n+    size_t region_idx = 0;\n+    while (garbage_total < garbage_target && region_idx < _heap->num_regions()) {\n+      garbage_total += make_garbage_above_threshold(region_idx++);\n+    }\n+    return garbage_total;\n+  }\n+\n+  void make_pinned(size_t region_idx) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->record_pin();\n+    region->make_pinned();\n+  }\n+\n+  void make_unpinned(size_t region_idx) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->record_unpin();\n+    region->make_unpinned();\n+  }\n+\n+  size_t make_garbage_below_threshold(size_t region_idx) {\n+    return make_garbage(region_idx, collection_threshold() - 100);\n+  }\n+\n+  size_t make_garbage_above_threshold(size_t region_idx) {\n+    return make_garbage(region_idx, collection_threshold() + 100);\n+  }\n+\n+  size_t collection_threshold() const {\n+    return ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold \/ 100;\n+  }\n+};\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_no_old_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+  \n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(0U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(0U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_no_old_region_above_threshold) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ In this case, we have zero regions to add to the collection set,\n+  \/\/ but we will have one region that must still be made parseable.\n+  make_garbage_below_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(1U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(0U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_one_old_region_above_threshold) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  make_garbage_above_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(1U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(1U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(1U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, prime_one_old_region) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t garbage = make_garbage_above_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(garbage, _collection_set->get_old_garbage());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, prime_many_old_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t g1 = make_garbage_above_threshold(100);\n+  size_t g2 = make_garbage_above_threshold(101);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(g1 + g2, _collection_set->get_old_garbage());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, require_multiple_mixed_evacuations) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t garbage = create_too_much_garbage_for_one_mixed_evacuation();\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_LT(_collection_set->get_old_garbage(), garbage);\n+  EXPECT_GT(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, skip_pinned_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  \/\/ A region can be pinned when we chose collection set candidates.\n+  make_pinned(2);\n+  _heuristics->prepare_for_old_collections();\n+\n+  \/\/ We only excluded pinned regions when we actually add regions to the collection set.\n+  ASSERT_EQ(3UL, _heuristics->unprocessed_old_collection_candidates());\n+\n+  \/\/ Here the region is still pinned, so it cannot be added to the collection set.\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  \/\/ The two unpinned regions should be added to the collection set and the pinned\n+  \/\/ region should be retained at the front of the list of candidates as it would be\n+  \/\/ likely to become unpinned by the next mixed collection cycle.\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+}\n\\ No newline at end of file\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":191,"deletions":0,"binary":false,"changes":191,"status":"added"}]}