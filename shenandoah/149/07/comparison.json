{"files":[{"patch":"@@ -127,2 +127,0 @@\n-  ShenandoahMarkingContext* const ctx = _generation->complete_marking_context();\n-\n@@ -163,1 +161,1 @@\n-      bool bm_live = ctx->is_marked(cast_to_oop(region->bottom()));\n+      bool bm_live = heap->complete_marking_context()->is_marked(cast_to_oop(region->bottom()));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,1 +28,3 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectionSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n@@ -33,10 +35,14 @@\n-ShenandoahOldHeuristics::ShenandoahOldHeuristics(ShenandoahGeneration* generation, ShenandoahHeuristics* trigger_heuristic) :\n-    ShenandoahHeuristics(generation),\n-    _old_collection_candidates(0),\n-    _next_old_collection_candidate(0),\n-    _hidden_old_collection_candidates(0),\n-    _hidden_next_old_collection_candidate(0),\n-    _old_coalesce_and_fill_candidates(0),\n-    _first_coalesce_and_fill_candidate(0),\n-    _trigger_heuristic(trigger_heuristic),\n-    _promotion_failed(false)\n+uint ShenandoahOldHeuristics::NOT_FOUND = -1U;\n+\n+ShenandoahOldHeuristics::ShenandoahOldHeuristics(ShenandoahOldGeneration* generation, ShenandoahHeuristics* trigger_heuristic) :\n+  ShenandoahHeuristics(generation),\n+#ifdef ASSERT\n+  _start_candidate(0),\n+#endif\n+  _first_pinned_candidate(NOT_FOUND),\n+  _last_old_collection_candidate(0),\n+  _next_old_collection_candidate(0),\n+  _last_old_region(0),\n+  _trigger_heuristic(trigger_heuristic),\n+  _promotion_failed(false),\n+  _old_generation(generation)\n@@ -44,0 +50,1 @@\n+  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n@@ -51,0 +58,2 @@\n+  _first_pinned_candidate = NOT_FOUND;\n+\n@@ -61,1 +70,1 @@\n-  size_t old_evacuation_budget = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t old_evacuation_budget = (size_t) ((double) heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n@@ -73,0 +82,3 @@\n+    if (r == nullptr) {\n+      break;\n+    }\n@@ -93,0 +105,5 @@\n+  if (_first_pinned_candidate != NOT_FOUND) {\n+    \/\/ Need to deal with pinned regions\n+    slide_pinned_regions_to_front();\n+  }\n+\n@@ -99,0 +116,5 @@\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+  }\n+\n@@ -102,0 +124,58 @@\n+void ShenandoahOldHeuristics::slide_pinned_regions_to_front() {\n+  \/\/ Find the leftmost unpinned region. The region in this slot will have been\n+  \/\/ added to the cset, so we can use it to hold pointers to regions that were\n+  \/\/ pinned when the cset was chosen.\n+  \/\/ [ r p r p p p r ]\n+  \/\/          ^\n+  \/\/          | first r to the left should be in the collection set now.\n+  uint write_index = NOT_FOUND;\n+  for (uint search = _next_old_collection_candidate - 1; search > _first_pinned_candidate; --search) {\n+    ShenandoahHeapRegion* region = _region_data[search]._region;\n+    if (!region->is_pinned()) {\n+      write_index = search;\n+      assert(region->is_cset(), \"Expected unpinned region to be added to the collection set.\");\n+      break;\n+    }\n+  }\n+\n+  if (write_index == NOT_FOUND) {\n+    if (_first_pinned_candidate > 0) {\n+      _next_old_collection_candidate = _first_pinned_candidate;\n+    }\n+    return;\n+  }\n+\n+  \/\/ Find pinned regions to the left and move their pointer into a slot\n+  \/\/ that was pointing at a region that has been added to the cset.\n+  \/\/ [ r p r p p p r ]\n+  \/\/       ^\n+  \/\/       | Write pointer is here. We know this region is already in the cset\n+  \/\/       | so we can clobber it with the next pinned region we find.\n+  for (size_t search = write_index - 1; search > _first_pinned_candidate; --search) {\n+    RegionData& skipped = _region_data[search];\n+    if (skipped._region->is_pinned()) {\n+      RegionData& added_to_cset = _region_data[write_index];\n+      assert(added_to_cset._region->is_cset(), \"Can only overwrite slots used by regions added to the collection set.\");\n+      added_to_cset._region = skipped._region;\n+      added_to_cset._garbage = skipped._garbage;\n+      --write_index;\n+    }\n+  }\n+\n+  \/\/ Everything left should already be in the cset\n+  \/\/ [ r x p p p p r ]\n+  \/\/       ^\n+  \/\/       | next pointer points at the first region which was not added\n+  \/\/       | to the collection set.\n+#ifdef ASSERT\n+  for (size_t check = write_index - 1; check > _start_candidate; --check) {\n+    ShenandoahHeapRegion* region = _region_data[check]._region;\n+    assert(region->is_cset(), \"All regions here should be in the collection set.\");\n+  }\n+  _start_candidate = write_index;\n+#endif\n+\n+  \/\/ Update to read from the leftmost pinned region.\n+  _next_old_collection_candidate = write_index;\n+}\n+\n@@ -132,1 +212,1 @@\n-    if (region->is_regular()) {\n+    if (region->is_regular() || region->is_pinned()) {\n@@ -134,0 +214,1 @@\n+        assert(!region->is_pinned(), \"Pinned region should have live (pinned) objects.\");\n@@ -171,0 +252,1 @@\n+\n@@ -173,0 +255,4 @@\n+  _last_old_region = (uint)cand_idx;\n+  _last_old_collection_candidate = (uint)cand_idx;\n+  _next_old_collection_candidate = 0;\n+\n@@ -174,1 +260,0 @@\n-    candidates_garbage += candidates[i]._garbage;\n@@ -177,10 +262,2 @@\n-      _hidden_next_old_collection_candidate = 0;\n-      _hidden_old_collection_candidates = (uint)i;\n-      _first_coalesce_and_fill_candidate = (uint)i;\n-      _old_coalesce_and_fill_candidates = (uint)(cand_idx - i);\n-\n-      \/\/ Note that we do not coalesce and fill occupied humongous regions\n-      \/\/ HR: humongous regions, RR: regular regions, CF: coalesce and fill regions\n-      log_info(gc)(\"Old-gen mark evac (\" UINT32_FORMAT \" RR, \" UINT32_FORMAT \" CF)\",\n-                   _hidden_old_collection_candidates, _old_coalesce_and_fill_candidates);\n-      return;\n+      _last_old_collection_candidate = (uint)i;\n+      break;\n@@ -188,0 +265,1 @@\n+    candidates_garbage += candidates[i]._garbage;\n@@ -190,6 +268,0 @@\n-  \/\/ If we reach here, all of non-humogous old-gen regions are candidates for collection set.\n-  _hidden_next_old_collection_candidate = 0;\n-  _hidden_old_collection_candidates = (uint)cand_idx;\n-  _first_coalesce_and_fill_candidate = 0;\n-  _old_coalesce_and_fill_candidates = 0;\n-\n@@ -199,13 +271,4 @@\n-  log_info(gc)(\"Old-gen mark evac (\" UINT32_FORMAT \" RR, \" UINT32_FORMAT \" CF), \"\n-               \"Collectable Garbage: \" SIZE_FORMAT \"%s, \"\n-               \"Immediate Garbage: \" SIZE_FORMAT \"%s\",\n-               _hidden_old_collection_candidates, _old_coalesce_and_fill_candidates,\n-               byte_size_in_proper_unit(collectable_garbage), proper_unit_for_byte_size(collectable_garbage),\n-               byte_size_in_proper_unit(immediate_garbage), proper_unit_for_byte_size(immediate_garbage));\n-}\n-\n-void ShenandoahOldHeuristics::start_old_evacuations() {\n-  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n-\n-  _old_collection_candidates = _hidden_old_collection_candidates;\n-  _next_old_collection_candidate = _hidden_next_old_collection_candidate;\n+  log_info(gc)(\"Old-Gen Collectable Garbage: \" SIZE_FORMAT \"%s over \" UINT32_FORMAT \" regions, \"\n+               \"Old-Gen Immediate Garbage: \" SIZE_FORMAT \"%s over \" SIZE_FORMAT \" regions.\",\n+               byte_size_in_proper_unit(collectable_garbage), proper_unit_for_byte_size(collectable_garbage), _last_old_collection_candidate,\n+               byte_size_in_proper_unit(immediate_garbage), proper_unit_for_byte_size(immediate_garbage), immediate_regions);\n@@ -213,1 +276,5 @@\n-  _hidden_old_collection_candidates = 0;\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+  } else {\n+    _old_generation->transition_to(ShenandoahOldGeneration::WAITING);\n+  }\n@@ -216,3 +283,2 @@\n-uint ShenandoahOldHeuristics::unprocessed_old_or_hidden_collection_candidates() {\n-  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n-  return _old_collection_candidates + _hidden_old_collection_candidates;\n+uint ShenandoahOldHeuristics::last_old_collection_candidate_index() {\n+  return _last_old_collection_candidate;\n@@ -222,2 +288,1 @@\n-  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n-  return _old_collection_candidates;\n+  return _last_old_collection_candidate - _next_old_collection_candidate;\n@@ -227,2 +292,14 @@\n-  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n-  return _region_data[_next_old_collection_candidate]._region;\n+  while (_next_old_collection_candidate < _last_old_collection_candidate) {\n+    ShenandoahHeapRegion* next = _region_data[_next_old_collection_candidate]._region;\n+    if (!next->is_pinned()) {\n+      return next;\n+    } else {\n+      assert(next->is_pinned(), \"sanity\");\n+      if (_first_pinned_candidate == NOT_FOUND) {\n+        _first_pinned_candidate = _next_old_collection_candidate;\n+      }\n+    }\n+\n+    _next_old_collection_candidate++;\n+  }\n+  return nullptr;\n@@ -232,1 +309,0 @@\n-  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n@@ -234,1 +310,0 @@\n-  _old_collection_candidates--;\n@@ -237,3 +312,2 @@\n-uint ShenandoahOldHeuristics::old_coalesce_and_fill_candidates() {\n-  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n-  return _old_coalesce_and_fill_candidates;\n+uint ShenandoahOldHeuristics::last_old_region_index() const {\n+  return _last_old_region;\n@@ -242,5 +316,4 @@\n-void ShenandoahOldHeuristics::get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer) {\n-  assert(_generation->generation_mode() == OLD, \"This service only available for old-gc heuristics\");\n-  uint count = _old_coalesce_and_fill_candidates;\n-  int index = _first_coalesce_and_fill_candidate;\n-  while (count-- > 0) {\n+unsigned int ShenandoahOldHeuristics::get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer) {\n+  uint end = _last_old_region;\n+  uint index = _next_old_collection_candidate;\n+  while (index < end) {\n@@ -249,0 +322,1 @@\n+  return _last_old_region - _next_old_collection_candidate;\n@@ -252,1 +326,1 @@\n-  _old_collection_candidates = 0;\n+  _last_old_collection_candidate = 0;\n@@ -254,4 +328,1 @@\n-  _hidden_old_collection_candidates = 0;\n-  _hidden_next_old_collection_candidate = 0;\n-  _old_coalesce_and_fill_candidates = 0;\n-  _first_coalesce_and_fill_candidate = 0;\n+  _last_old_region = 0;\n@@ -277,3 +348,2 @@\n-  \/\/ For example, we could choose to abandon the previous old collection before it has completed evacuations,\n-  \/\/ but this would require that we coalesce and fill all garbage within unevacuated collection-set regions.\n-  if (unprocessed_old_or_hidden_collection_candidates() > 0) {\n+  \/\/ For example, we could choose to abandon the previous old collection before it has completed evacuations.\n+  if (unprocessed_old_collection_candidates() > 0) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":139,"deletions":69,"binary":false,"changes":208,"status":"modified"},{"patch":"@@ -28,4 +28,1 @@\n-#include \"gc\/shenandoah\/shenandoahCollectionSet.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+\n@@ -34,0 +31,4 @@\n+class ShenandoahCollectionSet;\n+class ShenandoahHeapRegion;\n+class ShenandoahOldGeneration;\n+\n@@ -38,8 +39,31 @@\n-  \/\/ if (_generation->generation_mode() == OLD) _old_collection_candidates\n-  \/\/  represent the number of regions selected for collection following the\n-  \/\/  most recently completed old-gen mark that have not yet been selected\n-  \/\/  for evacuation and _next_collection_candidate is the index within\n-  \/\/  _region_data of the next candidate region to be selected for evacuation.\n-  \/\/ if (_generation->generation_mode() != OLD) these two variables are\n-  \/\/  not used.\n-  uint _old_collection_candidates;\n+  static uint NOT_FOUND;\n+\n+  \/\/ After final marking of the old generation, this heuristic will select\n+  \/\/ a set of candidate regions to be included in subsequent mixed collections.\n+  \/\/ The regions are sorted into a `_region_data` array (declared in base\n+  \/\/ class) in decreasing order of garbage. The heuristic will give priority\n+  \/\/ to regions containing more garbage.\n+\n+  \/\/ The following members are used to keep track of which candidate regions\n+  \/\/ have yet to be added to a mixed collection. There is also some special\n+  \/\/ handling for pinned regions, described further below.\n+\n+  \/\/ This points to the first candidate of the current mixed collection. This\n+  \/\/ is only used for an assertion when handling pinned regions.\n+  debug_only(uint _start_candidate);\n+\n+  \/\/ Pinned regions may not be included in the collection set. Any old regions\n+  \/\/ which were pinned at the time when old regions were added to the mixed\n+  \/\/ collection will have been skipped. These regions are still contain garbage,\n+  \/\/ so we want to include them at the start of the list of candidates for the\n+  \/\/ _next_ mixed collection cycle. This variable is the index of the _first_\n+  \/\/ old region which is pinned when the mixed collection set is formed.\n+  uint _first_pinned_candidate;\n+\n+  \/\/ This is the index of the last region which is above the garbage threshold.\n+  \/\/ No regions after this will be considered for inclusion in a mixed collection\n+  \/\/ set.\n+  uint _last_old_collection_candidate;\n+\n+  \/\/ This index points to the first candidate in line to be added to the mixed\n+  \/\/ collection set. It is updated as regions are added to the collection set.\n@@ -48,17 +72,3 @@\n-  \/\/ At the time we select the old-gen collection set, _hidden_old_collection_candidates\n-  \/\/ and _hidden_next_old_collection_candidates are set to remember the intended old-gen\n-  \/\/ collection set.  After all old-gen regions not in the old-gen collection set have been\n-  \/\/ coalesced and filled, the content of these variables is copied to _old_collection_candidates\n-  \/\/ and _next_old_collection_candidates so that evacuations can begin evacuating these regions.\n-  uint _hidden_old_collection_candidates;\n-  uint _hidden_next_old_collection_candidate;\n-\n-  \/\/ if (_generation->generation_mode() == OLD)\n-  \/\/  _old_coalesce_and_fill_candidates represents the number of regions\n-  \/\/  that were chosen for the garbage contained therein to be coalesced\n-  \/\/  and filled and _first_coalesce_and_fill_candidate represents the\n-  \/\/  the index of the first such region within the _region_data array.\n-  \/\/ if (_generation->generation_mode() != OLD) these two variables are\n-  \/\/  not used.\n-  uint _old_coalesce_and_fill_candidates;\n-  uint _first_coalesce_and_fill_candidate;\n+  \/\/ This is the last index in the array of old regions which were active at\n+  \/\/ the end of old final mark.\n+  uint _last_old_region;\n@@ -69,1 +79,2 @@\n-  \/\/ Flag is set when promotion failure is detected (by gc thread), cleared when old generation collection begins (by control thread)\n+  \/\/ Flag is set when promotion failure is detected (by gc thread), and cleared when\n+  \/\/ old generation collection begins (by control thread).\n@@ -72,2 +83,2 @@\n-  \/\/ Prepare for evacuation of old-gen regions by capturing the mark results of a recently completed concurrent mark pass.\n-  void prepare_for_old_collections();\n+  \/\/ Keep a pointer to our generation that we can use without down casting a protected member from the base class.\n+  ShenandoahOldGeneration* _old_generation;\n@@ -80,1 +91,1 @@\n-  ShenandoahOldHeuristics(ShenandoahGeneration* generation, ShenandoahHeuristics* trigger_heuristic);\n+  ShenandoahOldHeuristics(ShenandoahOldGeneration* generation, ShenandoahHeuristics* trigger_heuristic);\n@@ -84,0 +95,3 @@\n+  \/\/ Prepare for evacuation of old-gen regions by capturing the mark results of a recently completed concurrent mark pass.\n+  void prepare_for_old_collections();\n+\n@@ -87,4 +101,0 @@\n-  \/\/ Having coalesced and filled all old-gen heap regions that are not part of the old-gen collection set, begin\n-  \/\/ evacuating the collection set.\n-  void start_old_evacuations();\n-\n@@ -95,1 +105,1 @@\n-  uint unprocessed_old_or_hidden_collection_candidates();\n+  uint last_old_collection_candidate_index();\n@@ -107,1 +117,1 @@\n-  uint old_coalesce_and_fill_candidates();\n+  uint last_old_region_index() const;\n@@ -111,1 +121,1 @@\n-  \/\/ old_coalesce_and_fill_candidates() entries, or memory may be corrupted when this function overwrites the\n+  \/\/ last_old_region_index() entries, or memory may be corrupted when this function overwrites the\n@@ -113,1 +123,1 @@\n-  void get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer);\n+  unsigned int get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer);\n@@ -154,0 +164,2 @@\n+ private:\n+  void slide_pinned_regions_to_front();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":53,"deletions":41,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -261,1 +261,1 @@\n-  VM_ShenandoahInitMark op(this, _do_old_gc_bootstrap);\n+  VM_ShenandoahInitMark op(this);\n@@ -570,1 +570,1 @@\n-  _generation->prepare_gc(_do_old_gc_bootstrap);\n+  _generation->prepare_gc();\n@@ -645,0 +645,4 @@\n+    \/\/ TODO: We should be able to pull this out of the safepoint for the bootstrap\n+    \/\/ cycle. The top of an old region will only move when a GC cycle evacuates\n+    \/\/ objects into it. When we start an old cycle, we know that nothing can touch\n+    \/\/ the top of old regions.\n@@ -648,1 +652,0 @@\n-    heap->old_generation()->parallel_heap_region_iterate(&cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -215,1 +216,1 @@\n-          set_gc_mode(marking_old);\n+          set_gc_mode(servicing_old);\n@@ -229,1 +230,1 @@\n-      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_prepare_for_old_mark_in_progress()) {\n@@ -234,1 +235,1 @@\n-                     BOOL_TO_STR(heap->is_concurrent_prep_for_mixed_evacuation_in_progress()));\n+                     BOOL_TO_STR(heap->is_prepare_for_old_mark_in_progress()));\n@@ -238,1 +239,1 @@\n-        set_gc_mode(marking_old);\n+        set_gc_mode(servicing_old);\n@@ -291,1 +292,1 @@\n-          case marking_old: {\n+          case servicing_old: {\n@@ -293,1 +294,1 @@\n-            resume_concurrent_old_cycle(heap->old_generation(), cause);\n+            service_concurrent_old_cycle(heap, cause);\n@@ -435,3 +436,3 @@\n-\/\/      |     +  +                                   +       |\n-\/\/      v     |  |                                   |       |\n-\/\/   Global <-+  |                                   |       |\n+\/\/      |     +  +   ^                            +  +       |\n+\/\/      v     |  |   |                            |  |       |\n+\/\/   Global <-+  |   +----------------------------+  |       |\n@@ -475,6 +476,2 @@\n-  \/\/ Configure the young generation's concurrent mark to put objects in\n-  \/\/ old regions into the concurrent mark queues associated with the old\n-  \/\/ generation. The young cycle will run as normal except that rather than\n-  \/\/ ignore old references it will mark and enqueue them in the old concurrent\n-  \/\/ mark but it will not traverse them.\n-  ShenandoahGeneration* old_generation = heap->old_generation();\n+\n+  ShenandoahOldGeneration* old_generation = heap->old_generation();\n@@ -483,2 +480,2 @@\n-  assert(!heap->is_concurrent_old_mark_in_progress(), \"Old already in progress.\");\n-  assert(old_generation->task_queues()->is_empty(), \"Old mark queues should be empty.\");\n+  GCIdMark gc_id_mark;\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n@@ -486,4 +483,15 @@\n-  young_generation->set_old_gen_task_queues(old_generation->task_queues());\n-  young_generation->set_mark_incomplete();\n-  old_generation->set_mark_incomplete();\n-  service_concurrent_cycle(young_generation, cause, true);\n+  switch (old_generation->state()) {\n+    case ShenandoahOldGeneration::IDLE: {\n+      assert(!heap->is_concurrent_old_mark_in_progress(), \"Old already in progress.\");\n+      assert(old_generation->task_queues()->is_empty(), \"Old mark queues should be empty.\");\n+    }\n+    case ShenandoahOldGeneration::FILLING: {\n+      _allow_old_preemption.set();\n+      ShenandoahGCSession session(cause, old_generation);\n+      old_generation->prepare_gc();\n+      _allow_old_preemption.unset();\n+\n+      if (heap->is_prepare_for_old_mark_in_progress()) {\n+        assert(old_generation->state() == ShenandoahOldGeneration::FILLING, \"Prepare for mark should be in progress.\");\n+        return;\n+      }\n@@ -491,1 +499,18 @@\n-  process_phase_timings(heap);\n+      assert(old_generation->state() == ShenandoahOldGeneration::BOOTSTRAPPING, \"Finished with filling, should be bootstrapping.\");\n+    }\n+    case ShenandoahOldGeneration::BOOTSTRAPPING: {\n+      \/\/ Configure the young generation's concurrent mark to put objects in\n+      \/\/ old regions into the concurrent mark queues associated with the old\n+      \/\/ generation. The young cycle will run as normal except that rather than\n+      \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+      \/\/ task queues but it will not traverse them.\n+      young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+      ShenandoahGCSession session(cause, young_generation);\n+      service_concurrent_cycle(heap,young_generation, cause, true);\n+      process_phase_timings(heap);\n+      if (heap->cancelled_gc()) {\n+        \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n+        \/\/ is going to resume after degenerated bootstrap cycle completes.\n+        log_info(gc)(\"Bootstrap cycle for old generation was cancelled.\");\n+        return;\n+      }\n@@ -493,19 +518,4 @@\n-  if (heap->cancelled_gc()) {\n-    \/\/ Young generation bootstrap cycle has failed. Concurrent mark for old generation\n-    \/\/ is not going to resume after degenerated young cycle completes.\n-    log_info(gc)(\"Bootstrap cycle for old generation was cancelled.\");\n-  } else {\n-    \/\/ Reset the degenerated point. Normally this would happen at the top\n-    \/\/ of the control loop, but here we have just completed a young cycle\n-    \/\/ which has bootstrapped the old concurrent marking.\n-    _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n-\n-    \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n-    \/\/ and init mark for the concurrent mark. All of that work will have been\n-    \/\/ done by the bootstrapping young cycle. In order to simplify the debugging\n-    \/\/ effort, the old cycle will ONLY complete the mark phase. No actual\n-    \/\/ collection of the old generation is happening here.\n-    set_gc_mode(marking_old);\n-    resume_concurrent_old_cycle(old_generation, cause);\n-  }\n-}\n+      \/\/ Reset the degenerated point. Normally this would happen at the top\n+      \/\/ of the control loop, but here we have just completed a young cycle\n+      \/\/ which has bootstrapped the old concurrent marking.\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n@@ -513,14 +523,15 @@\n-bool ShenandoahControlThread::check_soft_max_changed() const {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n-  size_t old_soft_max = heap->soft_max_capacity();\n-  if (new_soft_max != old_soft_max) {\n-    new_soft_max = MAX2(heap->min_capacity(), new_soft_max);\n-    new_soft_max = MIN2(heap->max_capacity(), new_soft_max);\n-    if (new_soft_max != old_soft_max) {\n-      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n-                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n-                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n-      );\n-      heap->set_soft_max_capacity(new_soft_max);\n-      return true;\n+      \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+      \/\/ and init mark for the concurrent mark. All of that work will have been\n+      \/\/ done by the bootstrapping young cycle. In order to simplify the debugging\n+      \/\/ effort, the old cycle will ONLY complete the mark phase. No actual\n+      \/\/ collection of the old generation is happening here.\n+      set_gc_mode(servicing_old);\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n+    case ShenandoahOldGeneration::MARKING: {\n+      ShenandoahGCSession session(cause, old_generation);\n+      bool marking_complete = resume_concurrent_old_cycle(old_generation, cause);\n+      if (marking_complete) {\n+        assert(old_generation->state() != ShenandoahOldGeneration::MARKING, \"Should not still be marking.\");\n+      }\n+      break;\n@@ -528,0 +539,3 @@\n+    default:\n+      log_error(gc)(\"Unexpected state for old GC: %d\", old_generation->state());\n+      ShouldNotReachHere();\n@@ -529,1 +543,0 @@\n-  return false;\n@@ -532,1 +545,1 @@\n-void ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+bool ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n@@ -534,3 +547,1 @@\n-  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress() ||\n-         ShenandoahHeap::heap()->is_concurrent_prep_for_mixed_evacuation_in_progress(),\n-         \"Old mark or mixed-evac prep should be in progress\");\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n@@ -541,4 +552,0 @@\n-  GCIdMark gc_id_mark;\n-  ShenandoahGCSession session(cause, generation);\n-\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n@@ -550,2 +557,1 @@\n-    generation->heuristics()->record_success_concurrent(false);\n-    heap->shenandoah_policy()->record_success_old();\n+    generation->record_success_concurrent(false);\n@@ -568,0 +574,20 @@\n+    return false;\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahControlThread::check_soft_max_changed() const {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t new_soft_max = Atomic::load(&SoftMaxHeapSize);\n+  size_t old_soft_max = heap->soft_max_capacity();\n+  if (new_soft_max != old_soft_max) {\n+    new_soft_max = MAX2(heap->min_capacity(), new_soft_max);\n+    new_soft_max = MIN2(heap->max_capacity(), new_soft_max);\n+    if (new_soft_max != old_soft_max) {\n+      log_info(gc)(\"Soft Max Heap Size: \" SIZE_FORMAT \"%s -> \" SIZE_FORMAT \"%s\",\n+                   byte_size_in_proper_unit(old_soft_max), proper_unit_for_byte_size(old_soft_max),\n+                   byte_size_in_proper_unit(new_soft_max), proper_unit_for_byte_size(new_soft_max)\n+      );\n+      heap->set_soft_max_capacity(new_soft_max);\n+      return true;\n+    }\n@@ -569,0 +595,1 @@\n+  return false;\n@@ -607,1 +634,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -610,0 +636,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -612,1 +639,0 @@\n-\n@@ -615,0 +641,5 @@\n+  service_concurrent_cycle(heap, generation, cause, do_old_gc_bootstrap);\n+}\n+\n+void ShenandoahControlThread::service_concurrent_cycle(const ShenandoahHeap* heap, ShenandoahGeneration* generation,\n+                                                       GCCause::Cause &cause, bool do_old_gc_bootstrap) {\n@@ -618,2 +649,1 @@\n-    generation->heuristics()->record_success_concurrent(gc.abbreviated());\n-    heap->shenandoah_policy()->record_success_concurrent();\n+    generation->record_success_concurrent(gc.abbreviated());\n@@ -697,0 +727,6 @@\n+  } else {\n+    assert(_degen_generation->generation_mode() == YOUNG, \"Expected degenerated young cycle, if not global.\");\n+    ShenandoahOldGeneration* old_generation = (ShenandoahOldGeneration*) heap->old_generation();\n+    if (old_generation->state() == ShenandoahOldGeneration::BOOTSTRAPPING && !gc.upgraded_to_full()) {\n+      old_generation->transition_to(ShenandoahOldGeneration::MARKING);\n+    }\n@@ -950,1 +986,1 @@\n-    case marking_old:       return \"old mark\";\n+    case servicing_old:     return \"old\";\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":107,"deletions":71,"binary":false,"changes":178,"status":"modified"},{"patch":"@@ -76,1 +76,1 @@\n-    marking_old\n+    servicing_old\n@@ -105,0 +105,1 @@\n+  \/\/ Returns true if the cycle has been cancelled or degenerated.\n@@ -106,1 +107,3 @@\n-  void resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+\n+  \/\/ Returns true if the old generation marking completed (i.e., final mark executed for old generation).\n+  bool resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n@@ -115,0 +118,1 @@\n+  \/\/ Return true if setting the flag which indicates allocation failure succeeds.\n@@ -116,0 +120,1 @@\n+  \/\/ Notify threads waiting for GC to complete.\n@@ -117,0 +122,1 @@\n+  \/\/ True if allocation failure flag has been set.\n@@ -131,0 +137,1 @@\n+  \/\/ Returns true if the old generation marking was interrupted to allow a young cycle.\n@@ -133,0 +140,1 @@\n+  \/\/ Returns true if the soft maximum heap has been changed using management APIs.\n@@ -151,0 +159,1 @@\n+  \/\/ Return true if the request to start a concurrent GC for the given generation succeeded.\n@@ -186,0 +195,3 @@\n+\n+  void service_concurrent_cycle(const ShenandoahHeap* heap, ShenandoahGeneration* generation, GCCause::Cause &cause,\n+                                bool do_old_gc_bootstrap);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":14,"deletions":2,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -322,1 +323,1 @@\n-  _generation->prepare_gc(false);\n+  _generation->prepare_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,1 +26,1 @@\n-\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -64,1 +65,1 @@\n-     _heap(ShenandoahHeap::heap()),\n+    _heap(ShenandoahHeap::heap()),\n@@ -207,1 +208,1 @@\n-void ShenandoahGeneration::prepare_gc(bool do_old_gc_bootstrap) {\n+void ShenandoahGeneration::prepare_gc() {\n@@ -210,5 +211,0 @@\n-  if (do_old_gc_bootstrap) {\n-    \/\/ Reset mark bitmap for old regions also.  Note that do_old_gc_bootstrap is only true if this generation is YOUNG.\n-    ShenandoahHeap::heap()->old_generation()->reset_mark_bitmap();\n-  }\n-\n@@ -218,4 +214,0 @@\n-  if (do_old_gc_bootstrap) {\n-    \/\/ Capture top at mark start for both old-gen regions also.  Note that do_old_gc_bootstrap is only true if generation is YOUNG.\n-    ShenandoahHeap::heap()->old_generation()->parallel_heap_region_iterate(&cl);\n-  }\n@@ -423,1 +415,1 @@\n-    ShenandoahGeneration* old_generation = heap->old_generation();\n+    ShenandoahOldGeneration* old_generation = heap->old_generation();\n@@ -652,1 +644,1 @@\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n+                            ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n@@ -654,1 +646,0 @@\n-\n@@ -656,1 +647,0 @@\n-    heap->assert_pinned_region_status();\n@@ -659,2 +649,5 @@\n-      \/\/ Also capture update_watermark for old-gen regions.\n-      ShenandoahCaptureUpdateWaterMarkForOld old_cl(complete_marking_context());\n+      \/\/ We always need to update the watermark for old regions. If there\n+      \/\/ are mixed collections pending, we also need to synchronize the\n+      \/\/ pinned status for old regions. Since we are already visiting every\n+      \/\/ old region here, go ahead and sync the pin status too.\n+      ShenandoahFinalMarkUpdateRegionStateClosure old_cl(nullptr);\n@@ -663,0 +656,2 @@\n+\n+    heap->assert_pinned_region_status();\n@@ -859,0 +854,10 @@\n+\n+void ShenandoahGeneration::record_success_concurrent(bool abbreviated) {\n+  heuristics()->record_success_concurrent(abbreviated);\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_concurrent();\n+}\n+\n+void ShenandoahGeneration::record_success_degenerated() {\n+  heuristics()->record_success_degenerated();\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_degenerated();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":23,"deletions":18,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -123,2 +123,2 @@\n-  \/\/ Used by concurrent and degenerated GC to reset regions.\n-  void prepare_gc(bool do_old_gc_bootstrap);\n+  \/\/ Called before init mark, expected to prepare regions for marking.\n+  virtual void prepare_gc();\n@@ -126,1 +126,1 @@\n-  \/\/ Return true iff prepared collection set includes at least one old-gen HeapRegion.\n+  \/\/ Called during final mark, chooses collection set, rebuilds free set.\n@@ -174,0 +174,3 @@\n+\n+  virtual void record_success_concurrent(bool abbreviated);\n+  virtual void record_success_degenerated();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -501,1 +501,1 @@\n-  _prep_for_mixed_evac_in_progress(false),\n+  _prepare_for_old_mark(false),\n@@ -645,0 +645,6 @@\n+bool ShenandoahHeap::is_old_bitmap_stable() const {\n+  ShenandoahOldGeneration::State state = _old_generation->state();\n+  return state != ShenandoahOldGeneration::MARKING\n+      && state != ShenandoahOldGeneration::BOOTSTRAPPING;\n+}\n+\n@@ -1033,1 +1039,1 @@\n-  set_concurrent_prep_for_mixed_evacuation_in_progress(false);\n+  set_prepare_for_old_mark_in_progress(false);\n@@ -1038,0 +1044,2 @@\n+  \/\/ Transition to IDLE now.\n+  _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n@@ -1042,3 +1050,3 @@\n-      || is_concurrent_prep_for_mixed_evacuation_in_progress()\n-      || old_heuristics()->unprocessed_old_or_hidden_collection_candidates() > 0\n-      || young_generation()->old_gen_task_queues() != nullptr;\n+         || is_prepare_for_old_mark_in_progress()\n+         || old_heuristics()->unprocessed_old_collection_candidates() > 0\n+         || young_generation()->old_gen_task_queues() != nullptr;\n@@ -2126,1 +2134,1 @@\n-void ShenandoahHeap::set_concurrent_prep_for_mixed_evacuation_in_progress(bool in_progress) {\n+void ShenandoahHeap::set_prepare_for_old_mark_in_progress(bool in_progress) {\n@@ -2129,5 +2137,1 @@\n-  _prep_for_mixed_evac_in_progress = in_progress;\n-}\n-\n-bool ShenandoahHeap::is_concurrent_prep_for_mixed_evacuation_in_progress() {\n-  return _prep_for_mixed_evac_in_progress;\n+  _prepare_for_old_mark = in_progress;\n@@ -2984,1 +2988,1 @@\n-  ((ShenandoahOldGeneration*) _old_generation)->transfer_pointers_from_satb();\n+  _old_generation->transfer_pointers_from_satb();\n@@ -3023,2 +3027,1 @@\n-  if (doing_mixed_evacuations() ||\n-      is_concurrent_prep_for_mixed_evacuation_in_progress() || active_generation()->generation_mode() == GLOBAL) {\n+  if (is_old_bitmap_stable() || active_generation()->generation_mode() == GLOBAL) {\n@@ -3158,2 +3161,1 @@\n-  if (doing_mixed_evacuations() ||\n-      is_concurrent_prep_for_mixed_evacuation_in_progress() || active_generation()->generation_mode() == GLOBAL) {\n+  if (is_old_bitmap_stable() || active_generation()->generation_mode() == GLOBAL) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":18,"deletions":16,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+class ShenandoahOldGeneration;\n@@ -154,1 +155,2 @@\n-  bool _prep_for_mixed_evac_in_progress; \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+  \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+  bool _prepare_for_old_mark;\n@@ -173,1 +175,1 @@\n-\n+  bool is_old_bitmap_stable() const;\n@@ -400,1 +402,1 @@\n-  void set_concurrent_prep_for_mixed_evacuation_in_progress(bool cond);\n+  void set_prepare_for_old_mark_in_progress(bool cond);\n@@ -418,1 +420,1 @@\n-  bool is_concurrent_prep_for_mixed_evacuation_in_progress();\n+  inline bool is_prepare_for_old_mark_in_progress() const;\n@@ -522,1 +524,1 @@\n-  ShenandoahGeneration*      _old_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n@@ -540,1 +542,1 @@\n-  ShenandoahGeneration*      old_generation()    const { return _old_generation;    }\n+  ShenandoahOldGeneration*   old_generation()    const { return _old_generation;    }\n@@ -568,1 +570,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support() { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const { return _monitoring_support;    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":9,"deletions":7,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -674,0 +674,4 @@\n+inline bool ShenandoahHeap::is_prepare_for_old_mark_in_progress() const {\n+  return _prepare_for_old_mark;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,6 +40,13 @@\n-    \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-    \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-    HeapWord *tams = _ctx->top_at_mark_start(r);\n-    HeapWord *top = r->top();\n-    if (top > tams) {\n-      r->increase_live_data_alloc_words(pointer_delta(top, tams));\n+    if (_ctx != nullptr) {\n+      \/\/ _ctx may be null when this closure is used to sync only the pin status\n+      \/\/ update the watermark of old regions. For old regions we cannot reset\n+      \/\/ the TAMS because we rely on that to keep promoted objects alive after\n+      \/\/ old marking is complete.\n+\n+      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n+      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+      if (top > tams) {\n+        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n+      }\n@@ -68,2 +75,2 @@\n-    assert(_ctx->top_at_mark_start(r) == r->top(),\n-           \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n+    assert(_ctx == nullptr || _ctx->top_at_mark_start(r) == r->top(),\n+             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n@@ -72,8 +79,0 @@\n-\n-ShenandoahCaptureUpdateWaterMarkForOld::ShenandoahCaptureUpdateWaterMarkForOld(ShenandoahMarkingContext* ctx) :\n-  _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-void ShenandoahCaptureUpdateWaterMarkForOld::heap_region_do(ShenandoahHeapRegion* r) {\n-  \/\/ Remember limit for updating refs. It's guaranteed that we get no from-space-refs written from here on.\n-  r->set_update_watermark_at_safepoint(r->top());\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -38,13 +38,1 @@\n-  ShenandoahFinalMarkUpdateRegionStateClosure(ShenandoahMarkingContext* ctx);\n-\n-  void heap_region_do(ShenandoahHeapRegion* r);\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-class ShenandoahCaptureUpdateWaterMarkForOld : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-public:\n-  ShenandoahCaptureUpdateWaterMarkForOld(ShenandoahMarkingContext* ctx);\n+  explicit ShenandoahFinalMarkUpdateRegionStateClosure(ShenandoahMarkingContext* ctx);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.hpp","additions":1,"deletions":13,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * Copyright (c) 2022, Amazon.com, Inc. or its affiliates. All rights reserved.\n@@ -34,2 +34,0 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n-#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n@@ -39,18 +37,0 @@\n-class ShenandoahConcurrentCoalesceAndFillTask : public WorkerTask {\n-private:\n-  uint _nworkers;\n-  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n-  uint _coalesce_and_fill_region_count;\n-  ShenandoahConcurrentGC* _old_gc;\n-  volatile bool _is_preempted;\n-\n-public:\n-  ShenandoahConcurrentCoalesceAndFillTask(uint nworkers, ShenandoahHeapRegion** coalesce_and_fill_region_array,\n-                                          uint region_count, ShenandoahConcurrentGC* old_gc) :\n-    WorkerTask(\"Shenandoah Concurrent Coalesce and Fill\"),\n-    _nworkers(nworkers),\n-    _coalesce_and_fill_region_array(coalesce_and_fill_region_array),\n-    _coalesce_and_fill_region_count(region_count),\n-    _old_gc(old_gc),\n-    _is_preempted(false) {\n-  }\n@@ -58,20 +38,0 @@\n-  void work(uint worker_id) {\n-    for (uint region_idx = worker_id; region_idx < _coalesce_and_fill_region_count; region_idx += _nworkers) {\n-      ShenandoahHeapRegion* r = _coalesce_and_fill_region_array[region_idx];\n-      if (!r->is_humongous()) {\n-        if (!r->oop_fill_and_coalesce()) {\n-          \/\/ Coalesce and fill has been preempted\n-          Atomic::store(&_is_preempted, true);\n-          return;\n-        }\n-      } else {\n-        \/\/ there's only one object in this region and it's not garbage, so no need to coalesce or fill\n-      }\n-    }\n-  }\n-\n-  \/\/ Value returned from is_completed() is only valid after all worker thread have terminated.\n-  bool is_completed() {\n-    return !Atomic::load(&_is_preempted);\n-  }\n-};\n@@ -82,7 +42,0 @@\n-  _coalesce_and_fill_region_array = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, ShenandoahHeap::heap()->num_regions(), mtGC);\n-}\n-\n-void ShenandoahOldGC::start_old_evacuations() {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n-  old_heuristics->start_old_evacuations();\n@@ -91,1 +44,0 @@\n-\n@@ -136,0 +88,1 @@\n+  assert(!heap->is_prepare_for_old_mark_in_progress(), \"Old regions need to be parseable during concurrent mark.\");\n@@ -137,12 +90,2 @@\n-  if (!heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n-    \/\/ Skip over the initial phases of old collect if we're resuming mixed evacuation preparation.\n-    \/\/ Continue concurrent mark, do not reset regions, do not mark roots, do not collect $200.\n-    _allow_preemption.set();\n-    entry_mark();\n-    if (!_allow_preemption.try_unset()) {\n-      \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n-      \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the cancellation.\n-      while (!heap->cancelled_gc()) {\n-        SpinPause();\n-      }\n-    }\n+  \/\/ Enable preemption of old generation mark.\n+  _allow_preemption.set();\n@@ -150,2 +93,10 @@\n-    if (heap->cancelled_gc()) {\n-      return false;\n+  \/\/ Continue concurrent mark, do not reset regions, do not mark roots, do not collect $200.\n+  entry_mark();\n+\n+  \/\/ If we failed to unset the preemption flag, it means another thread has already unset it.\n+  if (!_allow_preemption.try_unset()) {\n+    \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n+    \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the\n+    \/\/ cancellation.\n+    while (!heap->cancelled_gc()) {\n+      SpinPause();\n@@ -153,0 +104,1 @@\n+  }\n@@ -154,2 +106,3 @@\n-    \/\/ Complete marking under STW\n-    vmop_entry_final_mark();\n+  if (heap->cancelled_gc()) {\n+    return false;\n+  }\n@@ -157,3 +110,2 @@\n-    \/\/ We aren't dealing with old generation evacuation yet. Our heuristic\n-    \/\/ should not have built a cset in final mark.\n-    assert(!heap->is_evacuation_in_progress(), \"Old gen evacuations are not supported\");\n+  \/\/ Complete marking under STW\n+  vmop_entry_final_mark();\n@@ -161,5 +113,3 @@\n-    \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n-    if (heap->is_concurrent_weak_root_in_progress()) {\n-      entry_weak_refs();\n-      entry_weak_roots();\n-    }\n+  \/\/ We aren't dealing with old generation evacuation yet. Our heuristic\n+  \/\/ should not have built a cset in final mark.\n+  assert(!heap->is_evacuation_in_progress(), \"Old gen evacuations are not supported\");\n@@ -167,3 +117,5 @@\n-    \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n-    \/\/ the space. This would be the last action if there is nothing to evacuate.\n-    entry_cleanup_early();\n+  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n+    entry_weak_refs();\n+    entry_weak_roots();\n+  }\n@@ -171,4 +123,3 @@\n-    {\n-      ShenandoahHeapLocker locker(heap->lock());\n-      heap->free_set()->log_status();\n-    }\n+  \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  entry_cleanup_early();\n@@ -176,0 +127,4 @@\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->log_status();\n+  }\n@@ -177,6 +132,0 @@\n-    \/\/ TODO: Old marking doesn't support class unloading yet\n-    \/\/ Perform concurrent class unloading\n-    \/\/ if (heap->unload_classes() &&\n-    \/\/     heap->is_concurrent_weak_root_in_progress()) {\n-    \/\/   entry_class_unloading();\n-    \/\/ }\n@@ -184,2 +133,6 @@\n-    heap->set_concurrent_prep_for_mixed_evacuation_in_progress(true);\n-  }\n+  \/\/ TODO: Old marking doesn't support class unloading yet\n+  \/\/ Perform concurrent class unloading\n+  \/\/ if (heap->unload_classes() &&\n+  \/\/     heap->is_concurrent_weak_root_in_progress()) {\n+  \/\/   entry_class_unloading();\n+  \/\/ }\n@@ -196,31 +149,0 @@\n-  \/\/ Coalesce and fill objects _after_ weak root processing and class unloading.\n-  \/\/ Weak root and reference processing makes assertions about unmarked referents\n-  \/\/ that will fail if they've been overwritten with filler objects. There is also\n-  \/\/ a case in the LRB that permits access to from-space objects for the purpose\n-  \/\/ of class unloading that is unlikely to function correctly if the object has\n-  \/\/ been filled.\n-  _allow_preemption.set();\n-\n-  if (heap->cancelled_gc()) {\n-    return false;\n-  }\n-\n-  if (heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n-    if (!entry_coalesce_and_fill()) {\n-      \/\/ If an allocation failure occurs during coalescing, we will run a degenerated\n-      \/\/ cycle for the young generation. This should be a rare event.  Normally, we'll\n-      \/\/ resume the coalesce-and-fill effort after the preempting young-gen GC finishes.\n-      return false;\n-    }\n-  }\n-  if (!_allow_preemption.try_unset()) {\n-    \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n-    \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the cancellation.\n-    while (!heap->cancelled_gc()) {\n-      SpinPause();\n-    }\n-  }\n-  \/\/ Prepare for old evacuations (actual evacuations will happen on subsequent young collects).  This cannot\n-  \/\/ begin until after we have completed coalesce-and-fill.\n-  start_old_evacuations();\n-\n@@ -229,46 +151,0 @@\n-\n-void ShenandoahOldGC::entry_coalesce_and_fill_message(char *buf, size_t len) const {\n-  \/\/ ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  jio_snprintf(buf, len, \"Coalescing and filling (%s)\", _generation->name());\n-}\n-\n-bool ShenandoahOldGC::op_coalesce_and_fill() {\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n-  WorkerThreads* workers = heap->workers();\n-  uint nworkers = workers->active_workers();\n-\n-  assert(_generation->generation_mode() == OLD, \"Only old-GC does coalesce and fill\");\n-  log_debug(gc)(\"Starting (or resuming) coalesce-and-fill of old heap regions\");\n-  uint coalesce_and_fill_regions_count = old_heuristics->old_coalesce_and_fill_candidates();\n-  assert(coalesce_and_fill_regions_count <= heap->num_regions(), \"Sanity\");\n-  old_heuristics->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n-  ShenandoahConcurrentCoalesceAndFillTask task(nworkers, _coalesce_and_fill_region_array, coalesce_and_fill_regions_count, this);\n-\n-  workers->run_task(&task);\n-  if (task.is_completed()) {\n-    \/\/ Remember that we're done with coalesce-and-fill.\n-    heap->set_concurrent_prep_for_mixed_evacuation_in_progress(false);\n-    return true;\n-  } else {\n-    log_debug(gc)(\"Suspending coalesce-and-fill of old heap regions\");\n-    \/\/ Otherwise, we got preempted before the work was done.\n-    return false;\n-  }\n-}\n-\n-bool ShenandoahOldGC::entry_coalesce_and_fill() {\n-  char msg[1024];\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-\n-  entry_coalesce_and_fill_message(msg, sizeof(msg));\n-  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::coalesce_and_fill);\n-\n-  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n-  EventMark em(\"%s\", msg);\n-  ShenandoahWorkerScope scope(heap->workers(),\n-                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n-                              \"concurrent coalesce and fill\");\n-\n-  return op_coalesce_and_fill();\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":41,"deletions":165,"binary":false,"changes":206,"status":"modified"},{"patch":"@@ -43,1 +43,0 @@\n-  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n@@ -45,2 +44,0 @@\n-  void start_old_evacuations();\n-  bool entry_coalesce_and_fill();\n@@ -48,2 +45,0 @@\n-  bool op_coalesce_and_fill();\n-  void entry_coalesce_and_fill_message(char *buf, size_t len) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.hpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n@@ -40,0 +42,1 @@\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n@@ -45,0 +48,3 @@\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n@@ -46,0 +52,1 @@\n+#include \"utilities\/events.hpp\"\n@@ -128,0 +135,39 @@\n+class ShenandoahConcurrentCoalesceAndFillTask : public WorkerTask {\n+ private:\n+  uint _nworkers;\n+  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n+  uint _coalesce_and_fill_region_count;\n+  volatile bool _is_preempted;\n+\n+ public:\n+  ShenandoahConcurrentCoalesceAndFillTask(uint nworkers, ShenandoahHeapRegion** coalesce_and_fill_region_array,\n+                                          uint region_count) :\n+    WorkerTask(\"Shenandoah Concurrent Coalesce and Fill\"),\n+    _nworkers(nworkers),\n+    _coalesce_and_fill_region_array(coalesce_and_fill_region_array),\n+    _coalesce_and_fill_region_count(region_count),\n+    _is_preempted(false) {\n+  }\n+\n+  void work(uint worker_id) {\n+    for (uint region_idx = worker_id; region_idx < _coalesce_and_fill_region_count; region_idx += _nworkers) {\n+      ShenandoahHeapRegion* r = _coalesce_and_fill_region_array[region_idx];\n+      if (r->is_humongous()) {\n+        \/\/ there's only one object in this region and it's not garbage, so no need to coalesce or fill\n+        continue;\n+      }\n+\n+      if (!r->oop_fill_and_coalesce()) {\n+        \/\/ Coalesce and fill has been preempted\n+        Atomic::store(&_is_preempted, true);\n+        return;\n+      }\n+    }\n+  }\n+\n+  \/\/ Value returned from is_completed() is only valid after all worker thread have terminated.\n+  bool is_completed() {\n+    return !Atomic::load(&_is_preempted);\n+  }\n+};\n+\n@@ -129,1 +175,4 @@\n-  : ShenandoahGeneration(OLD, max_queues, max_capacity, soft_max_capacity) {\n+  : ShenandoahGeneration(OLD, max_queues, max_capacity, soft_max_capacity),\n+    _coalesce_and_fill_region_array(NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, ShenandoahHeap::heap()->num_regions(), mtGC)),\n+    _state(IDLE)\n+{\n@@ -169,0 +218,60 @@\n+void ShenandoahOldGeneration::prepare_gc() {\n+\n+  \/\/ Make the old generation regions parseable, so they can be safely\n+  \/\/ scanned when looking for objects in memory indicated by dirty cards.\n+  entry_coalesce_and_fill();\n+\n+  \/\/ Now that we have made the old generation parseable, it is safe to reset the mark bitmap.\n+  {\n+    static const char* msg = \"Concurrent reset (OLD)\";\n+    ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::conc_reset_old);\n+    ShenandoahWorkerScope scope(ShenandoahHeap::heap()->workers(),\n+                                ShenandoahWorkerPolicy::calc_workers_for_conc_reset(),\n+                                msg);\n+    ShenandoahGeneration::prepare_gc();\n+  }\n+}\n+\n+bool ShenandoahOldGeneration::entry_coalesce_and_fill() {\n+  char msg[1024];\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  ShenandoahConcurrentPhase gc_phase(\"Coalescing and filling (OLD)\", ShenandoahPhaseTimings::coalesce_and_fill);\n+\n+  \/\/ TODO: I don't think we're using these concurrent collection counters correctly.\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent coalesce and fill\");\n+\n+  return coalesce_and_fill();\n+}\n+\n+bool ShenandoahOldGeneration::coalesce_and_fill() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  heap->set_prepare_for_old_mark_in_progress(true);\n+  transition_to(FILLING);\n+\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  WorkerThreads* workers = heap->workers();\n+  uint nworkers = workers->active_workers();\n+\n+  log_debug(gc)(\"Starting (or resuming) coalesce-and-fill of old heap regions\");\n+  uint coalesce_and_fill_regions_count = old_heuristics->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n+  assert(coalesce_and_fill_regions_count <= heap->num_regions(), \"Sanity\");\n+  ShenandoahConcurrentCoalesceAndFillTask task(nworkers, _coalesce_and_fill_region_array, coalesce_and_fill_regions_count);\n+\n+  workers->run_task(&task);\n+  if (task.is_completed()) {\n+    \/\/ Remember that we're done with coalesce-and-fill.\n+    heap->set_prepare_for_old_mark_in_progress(false);\n+    transition_to(BOOTSTRAPPING);\n+    return true;\n+  } else {\n+    log_debug(gc)(\"Suspending coalesce-and-fill of old heap regions\");\n+    \/\/ Otherwise, we got preempted before the work was done.\n+    return false;\n+  }\n+}\n+\n@@ -198,0 +307,2 @@\n+    \/\/ This doesn't actually choose a collection set, but prepares a list of\n+    \/\/ regions as 'candidates' for inclusion in a mixed collection.\n@@ -204,0 +315,2 @@\n+    \/\/ Though we did not choose a collection set above, we still may have\n+    \/\/ freed up immediate garbage regions so proceed with rebuilding the free set.\n@@ -210,0 +323,99 @@\n+const char* ShenandoahOldGeneration::state_name(State state) {\n+  switch (state) {\n+    case IDLE:          return \"Idle\";\n+    case FILLING:       return \"Coalescing\";\n+    case BOOTSTRAPPING: return \"Bootstrapping\";\n+    case MARKING:       return \"Marking\";\n+    case WAITING:       return \"Waiting\";\n+    default:\n+      ShouldNotReachHere();\n+      return \"Unknown\";\n+  }\n+}\n+\n+void ShenandoahOldGeneration::transition_to(State new_state) {\n+  if (_state != new_state) {\n+    log_info(gc)(\"Old generation transition from %s to %s\", state_name(_state), state_name(new_state));\n+    assert(validate_transition(new_state), \"Invalid state transition.\");\n+    _state = new_state;\n+  }\n+}\n+\n+#ifdef ASSERT\n+\/\/ This diagram depicts the expected state transitions for marking the old generation\n+\/\/ and preparing for old collections. When a young generation cycle executes, the\n+\/\/ remembered set scan must visit objects in old regions. Visiting an object which\n+\/\/ has become dead on previous old cycles will result in crashes. To avoid visiting\n+\/\/ such objects, the remembered set scan will use the old generation mark bitmap when\n+\/\/ possible. It is _not_ possible to use the old generation bitmap when old marking\n+\/\/ is active (bitmap is not complete). For this reason, the old regions are made\n+\/\/ parseable _before_ the old generation bitmap is reset. The diagram does not depict\n+\/\/ global and full collections, both of which cancel any old generation activity.\n+\/\/\n+\/\/                              +-----------------+\n+\/\/               +------------> |      IDLE       |\n+\/\/               |   +--------> |                 |\n+\/\/               |   |          +-----------------+\n+\/\/               |   |            |\n+\/\/               |   |            | Begin Old Mark\n+\/\/               |   |            v\n+\/\/               |   |          +-----------------+     +--------------------+\n+\/\/               |   |          |     FILLING     | <-> |      YOUNG GC      |\n+\/\/               |   |          |                 |     | (RSet Uses Bitmap) |\n+\/\/               |   |          +-----------------+     +--------------------+\n+\/\/               |   |            |\n+\/\/               |   |            | Reset Bitmap\n+\/\/               |   |            v\n+\/\/               |   |          +-----------------+\n+\/\/               |   |          |    BOOTSTRAP    |\n+\/\/               |   |          |                 |\n+\/\/               |   |          +-----------------+\n+\/\/               |   |            |\n+\/\/               |   |            | Continue Marking\n+\/\/               |   |            v\n+\/\/               |   |          +-----------------+     +----------------------+\n+\/\/               |   |          |    MARKING      | <-> |       YOUNG GC       |\n+\/\/               |   +----------|                 |     | (RSet Parses Region) |\n+\/\/               |              +-----------------+     +----------------------+\n+\/\/               |                |\n+\/\/               |                | Has Candidates\n+\/\/               |                v\n+\/\/               |              +-----------------+\n+\/\/               |              |     WAITING     |\n+\/\/               +------------- |                 |\n+\/\/                              +-----------------+\n+\/\/\n+bool ShenandoahOldGeneration::validate_transition(State new_state) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  switch (new_state) {\n+    case IDLE:\n+      assert(!heap->is_concurrent_old_mark_in_progress(), \"Cannot become idle during old mark.\");\n+      assert(_old_heuristics->unprocessed_old_collection_candidates() == 0, \"Cannot become idle with collection candidates\");\n+      assert(!heap->is_prepare_for_old_mark_in_progress(), \"Cannot become idle while making old generation parseable.\");\n+      assert(heap->young_generation()->old_gen_task_queues() == nullptr, \"Cannot become idle when setup for bootstrapping.\");\n+      return true;\n+    case FILLING:\n+      assert(_state == IDLE, \"Cannot begin filling without first being idle.\");\n+      assert(heap->is_prepare_for_old_mark_in_progress(), \"Should be preparing for old mark now.\");\n+      return true;\n+    case BOOTSTRAPPING:\n+      assert(_state == FILLING, \"Cannot reset bitmap without making old regions parseable.\");\n+      \/\/ assert(heap->young_generation()->old_gen_task_queues() != nullptr, \"Cannot bootstrap without old mark queues.\");\n+      assert(!heap->is_prepare_for_old_mark_in_progress(), \"Cannot still be making old regions parseable.\");\n+      return true;\n+    case MARKING:\n+      assert(_state == BOOTSTRAPPING, \"Must have finished bootstrapping before marking.\");\n+      assert(heap->young_generation()->old_gen_task_queues() != nullptr, \"Young generation needs old mark queues.\");\n+      assert(heap->is_concurrent_old_mark_in_progress(), \"Should be marking old now.\");\n+      return true;\n+    case WAITING:\n+      assert(_state == MARKING, \"Cannot have old collection candidates without first marking.\");\n+      assert(_old_heuristics->unprocessed_old_collection_candidates() > 0, \"Must have collection candidates here.\");\n+      return true;\n+    default:\n+      ShouldNotReachHere();\n+      return false;\n+  }\n+}\n+#endif\n+\n@@ -225,1 +437,2 @@\n-  _heuristics = new ShenandoahOldHeuristics(this, trigger);\n+  _old_heuristics = new ShenandoahOldHeuristics(this, trigger);\n+  _heuristics = _old_heuristics;\n@@ -228,0 +441,5 @@\n+\n+void ShenandoahOldGeneration::record_success_concurrent(bool abbreviated) {\n+  heuristics()->record_success_concurrent(abbreviated);\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_success_old();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":220,"deletions":2,"binary":false,"changes":222,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+class ShenandoahOldHeuristics;\n@@ -51,0 +52,2 @@\n+  virtual void prepare_gc() override;\n+\n@@ -76,0 +79,26 @@\n+\n+  virtual void record_success_concurrent(bool abbreviated) override;\n+\n+  enum State {\n+    IDLE, FILLING, BOOTSTRAPPING, MARKING, WAITING\n+  };\n+\n+  static const char* state_name(State state);\n+\n+  void transition_to(State new_state);\n+\n+#ifdef ASSERT\n+  bool validate_transition(State new_state);\n+#endif\n+\n+  State state() const {\n+    return _state;\n+  }\n+\n+ private:\n+  bool entry_coalesce_and_fill();\n+  bool coalesce_and_fill();\n+\n+  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n+  ShenandoahOldHeuristics* _old_heuristics;\n+  State _state;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":29,"deletions":0,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -52,1 +52,1 @@\n-                                                                                       \\\n+  f(conc_reset_old,                                 \"Concurrent Reset (OLD)\")          \\\n@@ -105,1 +105,1 @@\n-  SHENANDOAH_PAR_PHASE_DO(coalesce_and_fill_,       \"    CFOD: \", f)                   \\\n+  SHENANDOAH_PAR_PHASE_DO(coalesce_and_fill_,       \"  CFOD: \", f)                     \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -31,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -85,1 +85,1 @@\n-    } else if (mode == ShenandoahControlThread::marking_old) {\n+    } else if (mode == ShenandoahControlThread::servicing_old) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -517,1 +517,1 @@\n-  if (heap->doing_mixed_evacuations() || heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+  if (heap->is_old_bitmap_stable()) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,0 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -36,0 +35,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -63,1 +63,0 @@\n-  const bool _do_old_gc_bootstrap;\n@@ -65,1 +64,1 @@\n-  VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc, bool do_old_gc_bootstrap) :\n+  explicit VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc) :\n@@ -67,2 +66,1 @@\n-    _gc(gc),\n-    _do_old_gc_bootstrap(do_old_gc_bootstrap) {};\n+    _gc(gc) {};\n@@ -78,1 +76,1 @@\n-  VM_ShenandoahFinalMarkStartEvac(ShenandoahConcurrentGC* gc) :\n+  explicit VM_ShenandoahFinalMarkStartEvac(ShenandoahConcurrentGC* gc) :\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.hpp","additions":3,"deletions":5,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -46,2 +46,2 @@\n-  if (_old_gen_task_queues != nullptr && in_progress && !heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n-    \/\/ This is not a bug. When the young generation marking is complete,\n+  if (_old_gen_task_queues != nullptr && in_progress && !heap->is_prepare_for_old_mark_in_progress()) {\n+    \/\/ This is not a bug. When the bootstrapping marking phase is complete,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,273 @@\n+#include \"precompiled.hpp\"\n+#include \"unittest.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+\n+\/\/ These tests will all be skipped (unless Shenandoah becomes the default\n+\/\/ collector). To execute these tests, you must enable Shenandoah, which\n+\/\/ is done with:\n+\/\/\n+\/\/ % _JAVA_OPTIONS=\"-XX:+UseShenandoahGC -XX:ShenandoahGCMode=generational\" make exploded-test TEST=\"gtest:Shenandoah*\"\n+\/\/\n+\/\/ Please note that these 'unit' tests are really integration tests and rely\n+\/\/ on the JVM being initialized. These tests manipulate the state of the\n+\/\/ collector in ways that are not compatible with a normal collection run.\n+\/\/ If these tests take longer than the minimum time between gc intervals -\n+\/\/ or, more likely, if you have them paused in a debugger longer than this\n+\/\/ interval - you can expect trouble.\n+\n+#define SKIP_IF_NOT_SHENANDOAH() \\\n+    if (!UseShenandoahGC) {      \\\n+      tty->print_cr(\"skipped\");  \\\n+      return;                    \\\n+    }\n+\n+class ShenandoahResetRegions : public ShenandoahHeapRegionClosure {\n+ public:\n+  virtual void heap_region_do(ShenandoahHeapRegion* region) override {\n+    if (!region->is_empty()) {\n+      region->make_trash();\n+      region->make_empty();\n+    }\n+    region->set_affiliation(FREE);\n+    region->clear_live_data();\n+    region->set_top(region->bottom());\n+  }\n+};\n+\n+class ShenandoahOldHeuristicTest : public ::testing::Test {\n+ protected:\n+  ShenandoahHeap* _heap;\n+  ShenandoahOldHeuristics* _heuristics;\n+  ShenandoahCollectionSet* _collection_set;\n+\n+  ShenandoahOldHeuristicTest()\n+    : _heap(ShenandoahHeap::heap()),\n+      _heuristics(_heap->old_heuristics()),\n+      _collection_set(_heap->collection_set()) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahResetRegions reset;\n+    _heap->heap_region_iterate(&reset);\n+    _heap->set_old_evac_reserve(_heap->old_generation()->soft_max_capacity() \/ 4);\n+    _heuristics->abandon_collection_candidates();\n+    _collection_set->clear();\n+  }\n+\n+  size_t make_garbage(size_t region_idx, size_t garbage_bytes) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->make_regular_allocation(OLD_GENERATION);\n+    region->increase_live_data_alloc_words(1);\n+    region->set_top(region->bottom() + garbage_bytes \/ HeapWordSize);\n+    return region->garbage();\n+  }\n+\n+  size_t create_too_much_garbage_for_one_mixed_evacuation() {\n+    size_t garbage_target = _heap->old_generation()->soft_max_capacity() \/ 2;\n+    size_t garbage_total = 0;\n+    size_t region_idx = 0;\n+    while (garbage_total < garbage_target && region_idx < _heap->num_regions()) {\n+      garbage_total += make_garbage_above_threshold(region_idx++);\n+    }\n+    return garbage_total;\n+  }\n+\n+  void make_pinned(size_t region_idx) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->record_pin();\n+    region->make_pinned();\n+  }\n+\n+  void make_unpinned(size_t region_idx) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    ShenandoahHeapRegion* region = _heap->get_region(region_idx);\n+    region->record_unpin();\n+    region->make_unpinned();\n+  }\n+\n+  size_t make_garbage_below_threshold(size_t region_idx) {\n+    return make_garbage(region_idx, collection_threshold() - 100);\n+  }\n+\n+  size_t make_garbage_above_threshold(size_t region_idx) {\n+    return make_garbage(region_idx, collection_threshold() + 100);\n+  }\n+\n+  size_t collection_threshold() const {\n+    return ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold \/ 100;\n+  }\n+};\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_no_old_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(0U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(0U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_no_old_region_above_threshold) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ In this case, we have zero regions to add to the collection set,\n+  \/\/ but we will have one region that must still be made parseable.\n+  make_garbage_below_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(1U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(0U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, select_one_old_region_above_threshold) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  make_garbage_above_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  EXPECT_EQ(1U, _heuristics->last_old_region_index());\n+  EXPECT_EQ(1U, _heuristics->last_old_collection_candidate_index());\n+  EXPECT_EQ(1U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, prime_one_old_region) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t garbage = make_garbage_above_threshold(10);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(garbage, _collection_set->get_old_garbage());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, prime_many_old_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t g1 = make_garbage_above_threshold(100);\n+  size_t g2 = make_garbage_above_threshold(101);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(g1 + g2, _collection_set->get_old_garbage());\n+  EXPECT_EQ(0U, _heuristics->unprocessed_old_collection_candidates());\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, require_multiple_mixed_evacuations) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  size_t garbage = create_too_much_garbage_for_one_mixed_evacuation();\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_LT(_collection_set->get_old_garbage(), garbage);\n+  EXPECT_GT(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, skip_pinned_regions) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  \/\/ A region can be pinned when we chose collection set candidates.\n+  make_pinned(2);\n+  _heuristics->prepare_for_old_collections();\n+\n+  \/\/ We only excluded pinned regions when we actually add regions to the collection set.\n+  ASSERT_EQ(3UL, _heuristics->unprocessed_old_collection_candidates());\n+\n+  \/\/ Here the region is still pinned, so it cannot be added to the collection set.\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  \/\/ The two unpinned regions should be added to the collection set and the pinned\n+  \/\/ region should be retained at the front of the list of candidates as it would be\n+  \/\/ likely to become unpinned by the next mixed collection cycle.\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  \/\/ Simulate another mixed collection after making region 2 unpinned. This time,\n+  \/\/ the now unpinned region should be added to the collection set.\n+  make_unpinned(2);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g2);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, pinned_region_is_first) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  make_pinned(1);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g2 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  make_unpinned(1);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, pinned_region_is_last) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  make_pinned(3);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g2);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 1UL);\n+\n+  make_unpinned(3);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n+\n+TEST_VM_F(ShenandoahOldHeuristicTest, unpinned_region_is_middle) {\n+  SKIP_IF_NOT_SHENANDOAH();\n+\n+  \/\/ Create three old regions with enough garbage to be collected.\n+  size_t g1 = make_garbage_above_threshold(1);\n+  size_t g2 = make_garbage_above_threshold(2);\n+  size_t g3 = make_garbage_above_threshold(3);\n+\n+  make_pinned(1);\n+  make_pinned(3);\n+  _heuristics->prepare_for_old_collections();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g2);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 2UL);\n+\n+  make_unpinned(1);\n+  make_unpinned(3);\n+  _collection_set->clear();\n+  _heuristics->prime_collection_set(_collection_set);\n+\n+  EXPECT_EQ(_collection_set->get_old_garbage(), g1 + g3);\n+  EXPECT_EQ(_heuristics->unprocessed_old_collection_candidates(), 0UL);\n+}\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":273,"deletions":0,"binary":false,"changes":273,"status":"added"}]}