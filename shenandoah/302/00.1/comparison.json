{"files":[{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -24,1 +25,0 @@\n-\n@@ -27,0 +27,4 @@\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n@@ -29,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -34,0 +39,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -61,1 +67,2 @@\n-  _last_trigger(OTHER) { }\n+  _last_trigger(OTHER),\n+  _available(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor) { }\n@@ -89,2 +96,2 @@\n-  size_t free_target = (capacity \/ 100 * ShenandoahMinFreeThreshold) + max_cset;\n-  size_t min_garbage = (free_target > actual_free ? (free_target - actual_free) : 0);\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n+  size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n@@ -93,1 +100,1 @@\n-                     SIZE_FORMAT \"%s, Max CSet: \" SIZE_FORMAT \"%s, Min Garbage: \" SIZE_FORMAT \"%s\",\n+                     SIZE_FORMAT \"%s, Max Evacuation: \" SIZE_FORMAT \"%s, Min Garbage: \" SIZE_FORMAT \"%s\",\n@@ -128,2 +135,2 @@\n-void ShenandoahAdaptiveHeuristics::record_success_concurrent() {\n-  ShenandoahHeuristics::record_success_concurrent();\n+void ShenandoahAdaptiveHeuristics::record_success_concurrent(bool abbreviated) {\n+  ShenandoahHeuristics::record_success_concurrent(abbreviated);\n@@ -133,3 +140,10 @@\n-  _available.add(available);\n-  if (_available.sd() > 0) {\n-    z_score = (available - _available.avg()) \/ _available.sd();\n+  double available_sd = _available.sd();\n+  if (available_sd > 0) {\n+    double available_avg = _available.avg();\n+    z_score = (double(available) - available_avg) \/ available_sd;\n+    log_debug(gc, ergo)(\"%s Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n+                        _space_info->name(),\n+                        byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                        z_score,\n+                        byte_size_in_proper_unit(available_avg), proper_unit_for_byte_size(available_avg),\n+                        byte_size_in_proper_unit(available_sd), proper_unit_for_byte_size(available_sd));\n@@ -139,5 +153,1 @@\n-  log_debug(gc, ergo)(\"Available: \" SIZE_FORMAT \" %sB, z-score=%.3f. Average available: %.1f %sB +\/- %.1f %sB.\",\n-                      byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n-                      z_score,\n-                      byte_size_in_proper_unit(_available.avg()), proper_unit_for_byte_size(_available.avg()),\n-                      byte_size_in_proper_unit(_available.sd()), proper_unit_for_byte_size(_available.sd()));\n+  _available.add(double(available));\n@@ -199,1 +209,0 @@\n-  size_t max_capacity = _space_info->max_capacity();\n@@ -201,1 +210,1 @@\n-  size_t available = _space_info->available();\n+  size_t available = _space_info->soft_available();\n@@ -204,3 +213,3 @@\n-  \/\/ Make sure the code below treats available without the soft tail.\n-  size_t soft_tail = max_capacity - capacity;\n-  available = (available > soft_tail) ? (available - soft_tail) : 0;\n+  log_debug(gc)(\"should_start_gc (%s)? available: \" SIZE_FORMAT \", soft_max_capacity: \" SIZE_FORMAT\n+                \", allocated: \" SIZE_FORMAT,\n+                _space_info->name(), available, capacity, allocated);\n@@ -212,1 +221,1 @@\n-  size_t min_threshold = capacity \/ 100 * ShenandoahMinFreeThreshold;\n+  size_t min_threshold = min_free_threshold();\n@@ -214,2 +223,2 @@\n-    log_info(gc)(\"Trigger: Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n-                 byte_size_in_proper_unit(available),     proper_unit_for_byte_size(available),\n+    log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\", _space_info->name(),\n+                 byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n@@ -220,0 +229,1 @@\n+  \/\/ Check if we need to learn a bit about the application\n@@ -224,3 +234,3 @@\n-      log_info(gc)(\"Trigger: Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n-                   _gc_times_learned + 1, max_learn,\n-                   byte_size_in_proper_unit(available),      proper_unit_for_byte_size(available),\n+      log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n+                   _space_info->name(), _gc_times_learned + 1, max_learn,\n+                   byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n@@ -231,1 +241,28 @@\n-\n+  \/\/  Rationale:\n+  \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n+  \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n+  \/\/\n+  \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n+  \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n+  \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n+  \/\/       accomodation to give us enough runway to recalibrate our \"average allocation rate\".\n+  \/\/\n+  \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n+  \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n+  \/\/       enough runway to recalibrate our \"average allocation rate\".\n+  \/\/\n+  \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n+  \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n+  \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n+  \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n+  \/\/\n+  \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n+  \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n+  \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n+  \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n+  \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n+  \/\/          of how this will behave.\n+  \/\/\n+  \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n+  \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n+  \/\/    computations an independent effort.\n@@ -233,1 +270,1 @@\n-  \/\/   1. Some space to absorb allocation spikes\n+  \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n@@ -243,1 +280,1 @@\n-  double avg_cycle_time = _gc_time_history->davg() + (_margin_of_error_sd * _gc_time_history->dsd());\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n@@ -245,0 +282,3 @@\n+  log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+                _space_info->name(),\n+          avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n@@ -246,2 +286,3 @@\n-    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n-                 avg_cycle_time * 1000,\n+    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+                 \" to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n+                 _space_info->name(), avg_cycle_time * 1000,\n@@ -251,1 +292,0 @@\n-\n@@ -257,1 +297,0 @@\n-\n@@ -264,2 +303,2 @@\n-    log_info(gc)(\"Trigger: Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n-                 avg_cycle_time * 1000,\n+    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                 _space_info->name(), avg_cycle_time * 1000,\n@@ -302,0 +341,7 @@\n+size_t ShenandoahAdaptiveHeuristics::min_free_threshold() {\n+  \/\/ Note that soft_max_capacity() \/ 100 * min_free_threshold is smaller than max_capacity() \/ 100 * min_free_threshold.\n+  \/\/ We want to behave conservatively here, so use max_capacity().  By returning a larger value, we cause the GC to\n+  \/\/ trigger when the remaining amount of free shrinks below the larger threshold.\n+  return _space_info->max_capacity() \/ 100 * ShenandoahMinFreeThreshold;\n+}\n+\n@@ -356,4 +402,0 @@\n-double ShenandoahAllocationRate::instantaneous_rate(size_t allocated) const {\n-  return instantaneous_rate(os::elapsedTime(), allocated);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":81,"deletions":39,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,2 @@\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"memory\/allocation.hpp\"\n@@ -31,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -40,1 +44,0 @@\n-  double instantaneous_rate(size_t allocated) const;\n@@ -43,1 +46,0 @@\n-\n@@ -77,1 +79,1 @@\n-  void record_success_concurrent();\n+  void record_success_concurrent(bool abbreviated);\n@@ -142,0 +144,2 @@\n+\n+  size_t min_free_threshold();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,2 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahCollectionSet.inline.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -36,0 +35,1 @@\n+#include \"utilities\/quickSort.hpp\"\n@@ -37,0 +37,1 @@\n+\/\/ sort by decreasing garbage (so most garbage comes first)\n@@ -38,1 +39,1 @@\n-  if (a._garbage > b._garbage)\n+  if (a._u._garbage > b._u._garbage)\n@@ -40,1 +41,1 @@\n-  else if (a._garbage < b._garbage)\n+  else if (a._u._garbage < b._u._garbage)\n@@ -50,0 +51,1 @@\n+  _guaranteed_gc_interval(0),\n@@ -54,1 +56,1 @@\n-  _gc_time_history(new TruncatedSeq(10, ShenandoahAdaptiveDecayFactor)),\n+  _gc_cycle_time_history(new TruncatedSeq(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor)),\n@@ -73,1 +75,1 @@\n-  assert(collection_set->count() == 0, \"Must be empty\");\n+  assert(collection_set->is_empty(), \"Must be empty\");\n@@ -116,1 +118,1 @@\n-        candidates[cand_idx]._garbage = garbage;\n+        candidates[cand_idx]._u._garbage = garbage;\n@@ -154,0 +156,4 @@\n+  } else {\n+    \/\/ We are going to skip evacuation and update refs because we reclaimed\n+    \/\/ sufficient amounts of immediate garbage.\n+    heap->shenandoah_policy()->record_abbreviated_cycle();\n@@ -157,1 +163,0 @@\n-\n@@ -162,2 +167,2 @@\n-                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \"\n-                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%)\",\n+                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions, \"\n+                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions\",\n@@ -172,0 +177,1 @@\n+                     immediate_regions,\n@@ -175,1 +181,2 @@\n-                     cset_percent);\n+                     cset_percent,\n+                     collection_set->count());\n@@ -194,1 +201,1 @@\n-  if (ShenandoahGuaranteedGCInterval > 0) {\n+  if (_guaranteed_gc_interval > 0) {\n@@ -196,3 +203,3 @@\n-    if (last_time_ms > ShenandoahGuaranteedGCInterval) {\n-      log_info(gc)(\"Trigger: Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n-                   last_time_ms, ShenandoahGuaranteedGCInterval);\n+    if (last_time_ms > _guaranteed_gc_interval) {\n+      log_info(gc)(\"Trigger (%s): Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n+                   _space_info->name(), last_time_ms, _guaranteed_gc_interval);\n@@ -212,1 +219,1 @@\n-          \"In range before adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+         \"In range before adjustment: \" INTX_FORMAT, _gc_time_penalties);\n@@ -224,1 +231,1 @@\n-          \"In range after adjustment: \" INTX_FORMAT, _gc_time_penalties);\n+         \"In range after adjustment: \" INTX_FORMAT, _gc_time_penalties);\n@@ -227,1 +234,1 @@\n-void ShenandoahHeuristics::record_success_concurrent() {\n+void ShenandoahHeuristics::record_success_concurrent(bool abbreviated) {\n@@ -231,2 +238,4 @@\n-  _gc_time_history->add(time_since_last_gc());\n-  _gc_times_learned++;\n+  if (!(abbreviated && ShenandoahAdaptiveIgnoreShortCycles)) {\n+    _gc_cycle_time_history->add(elapsed_cycle_time());\n+    _gc_times_learned++;\n+  }\n@@ -289,1 +298,1 @@\n-double ShenandoahHeuristics::time_since_last_gc() const {\n+double ShenandoahHeuristics::elapsed_cycle_time() const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":30,"deletions":21,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,1 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n@@ -73,0 +73,2 @@\n+  static const uint Moving_Average_Samples = 10; \/\/ Number of samples to store in moving averages\n+\n@@ -75,1 +77,4 @@\n-    size_t _garbage;\n+    union {\n+      size_t _garbage;          \/\/ Not used by old-gen heuristics.\n+      size_t _live_data;        \/\/ Only used for old-gen heuristics, which prioritizes retention of _live_data over garbage reclaim\n+    } _u;\n@@ -81,0 +86,12 @@\n+  \/\/ Depending on generation mode, region data represents the results of the relevant\n+  \/\/ most recently completed marking pass:\n+  \/\/   - in GLOBAL mode, global marking pass\n+  \/\/   - in OLD mode,    old-gen marking pass\n+  \/\/   - in YOUNG mode,  young-gen marking pass\n+  \/\/\n+  \/\/ Note that there is some redundancy represented in region data because\n+  \/\/ each instance is an array large enough to hold all regions. However,\n+  \/\/ any region in young-gen is not in old-gen. And any time we are\n+  \/\/ making use of the GLOBAL data, there is no need to maintain the\n+  \/\/ YOUNG or OLD data. Consider this redundancy of data structure to\n+  \/\/ have negligible cost unless proven otherwise.\n@@ -86,0 +103,2 @@\n+  size_t _guaranteed_gc_interval;\n+\n@@ -91,1 +110,1 @@\n-  TruncatedSeq* _gc_time_history;\n+  TruncatedSeq* _gc_cycle_time_history;\n@@ -98,0 +117,4 @@\n+  \/\/ TODO: We need to enhance this API to give visibility to accompanying old-gen evacuation effort.\n+  \/\/ In the case that the old-gen evacuation effort is small or zero, the young-gen heuristics\n+  \/\/ should feel free to dedicate increased efforts to young-gen evacuation.\n+\n@@ -112,0 +135,4 @@\n+  void set_guaranteed_gc_interval(size_t guaranteed_gc_interval) {\n+    _guaranteed_gc_interval = guaranteed_gc_interval;\n+  }\n+\n@@ -120,1 +147,1 @@\n-  virtual void record_success_concurrent();\n+  virtual void record_success_concurrent(bool abbreviated);\n@@ -141,1 +168,1 @@\n-  double time_since_last_gc() const;\n+  double elapsed_cycle_time() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":32,"deletions":5,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -0,0 +1,580 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectionSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"utilities\/quickSort.hpp\"\n+\n+#define BYTES_FORMAT    SIZE_FORMAT \"%s\"\n+#define FORMAT_BYTES(b) byte_size_in_proper_unit(b), proper_unit_for_byte_size(b)\n+\n+uint ShenandoahOldHeuristics::NOT_FOUND = -1U;\n+\n+\/\/ sort by increasing live (so least live comes first)\n+int ShenandoahOldHeuristics::compare_by_live(RegionData a, RegionData b) {\n+  if (a._u._live_data < b._u._live_data)\n+    return -1;\n+  else if (a._u._live_data > b._u._live_data)\n+    return 1;\n+  else return 0;\n+}\n+\n+ShenandoahOldHeuristics::ShenandoahOldHeuristics(ShenandoahOldGeneration* generation) :\n+  ShenandoahHeuristics(generation),\n+  _first_pinned_candidate(NOT_FOUND),\n+  _last_old_collection_candidate(0),\n+  _next_old_collection_candidate(0),\n+  _last_old_region(0),\n+  _live_bytes_in_unprocessed_candidates(0),\n+  _old_generation(generation),\n+  _cannot_expand_trigger(false),\n+  _fragmentation_trigger(false),\n+  _growth_trigger(false) {\n+}\n+\n+bool ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    return false;\n+  }\n+\n+  _first_pinned_candidate = NOT_FOUND;\n+\n+  uint included_old_regions = 0;\n+  size_t evacuated_old_bytes = 0;\n+  size_t collected_old_bytes = 0;\n+\n+  \/\/ If a region is put into the collection set, then this region's free (not yet used) bytes are no longer\n+  \/\/ \"available\" to hold the results of other evacuations.  This may cause a decrease in the remaining amount\n+  \/\/ of memory that can still be evacuated.  We address this by reducing the evacuation budget by the amount\n+  \/\/ of live memory in that region and by the amount of unallocated memory in that region if the evacuation\n+  \/\/ budget is constrained by availability of free memory.\n+  size_t old_evacuation_budget = (size_t) ((double) heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n+  size_t unfragmented_available = _old_generation->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  size_t fragmented_available;\n+  size_t excess_fragmented_available;\n+\n+  if (unfragmented_available > old_evacuation_budget) {\n+    unfragmented_available = old_evacuation_budget;\n+    fragmented_available = 0;\n+    excess_fragmented_available = 0;\n+  } else {\n+    assert(_old_generation->available() >= old_evacuation_budget, \"Cannot budget more than is available\");\n+    fragmented_available = _old_generation->available() - unfragmented_available;\n+    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n+    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n+      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n+      fragmented_available -= excess_fragmented_available;\n+    }\n+  }\n+\n+  size_t remaining_old_evacuation_budget = old_evacuation_budget;\n+  log_info(gc)(\"Choose old regions for mixed collection: old evacuation budget: \" SIZE_FORMAT \"%s, candidates: %u\",\n+               byte_size_in_proper_unit(old_evacuation_budget), proper_unit_for_byte_size(old_evacuation_budget),\n+               unprocessed_old_collection_candidates());\n+\n+  size_t lost_evacuation_capacity = 0;\n+\n+  \/\/ The number of old-gen regions that were selected as candidates for collection at the end of the most recent old-gen\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n+  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n+  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n+  while (unprocessed_old_collection_candidates() > 0) {\n+    \/\/ Old collection candidates are sorted in order of decreasing garbage contained therein.\n+    ShenandoahHeapRegion* r = next_old_collection_candidate();\n+    if (r == nullptr) {\n+      break;\n+    }\n+\n+    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n+    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n+\n+    size_t live_data_for_evacuation = r->get_live_data_bytes();\n+    size_t lost_available = r->free();\n+\n+    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n+      if (lost_available < excess_fragmented_available) {\n+        excess_fragmented_available -= lost_available;\n+        lost_evacuation_capacity -= lost_available;\n+        lost_available  = 0;\n+      } else {\n+        lost_available -= excess_fragmented_available;\n+        lost_evacuation_capacity -= excess_fragmented_available;\n+        excess_fragmented_available = 0;\n+      }\n+    }\n+    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n+    if ((lost_available > 0) && (fragmented_available > 0)) {\n+      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n+        fragmented_available -= scaled_loss;\n+        scaled_loss = 0;\n+      } else {\n+        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n+        \/\/ to decrement scaled_loss\n+      }\n+    }\n+    if (scaled_loss > 0) {\n+      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n+      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n+      if (live_data_for_evacuation > unfragmented_available) {\n+        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n+        break;\n+      } else {\n+        unfragmented_available -= live_data_for_evacuation;\n+      }\n+    } else {\n+      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n+      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n+      size_t evacuation_need = live_data_for_evacuation;\n+\n+      if (evacuation_need > fragmented_available) {\n+        evacuation_need -= fragmented_available;\n+        fragmented_available = 0;\n+      } else {\n+        fragmented_available -= evacuation_need;\n+        evacuation_need = 0;\n+      }\n+      if (evacuation_need > unfragmented_available) {\n+        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n+        break;\n+      } else {\n+        unfragmented_available -= evacuation_need;\n+        \/\/ dead code: evacuation_need == 0;\n+      }\n+    }\n+    collection_set->add_region(r);\n+    included_old_regions++;\n+    evacuated_old_bytes += live_data_for_evacuation;\n+    collected_old_bytes += r->garbage();\n+    consume_old_collection_candidate();\n+  }\n+\n+  if (_first_pinned_candidate != NOT_FOUND) {\n+    \/\/ Need to deal with pinned regions\n+    slide_pinned_regions_to_front();\n+  }\n+  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n+  if (included_old_regions > 0) {\n+    log_info(gc)(\"Old-gen piggyback evac (\" UINT32_FORMAT \" regions, evacuating \" SIZE_FORMAT \"%s, reclaiming: \" SIZE_FORMAT \"%s)\",\n+                 included_old_regions,\n+                 byte_size_in_proper_unit(evacuated_old_bytes), proper_unit_for_byte_size(evacuated_old_bytes),\n+                 byte_size_in_proper_unit(collected_old_bytes), proper_unit_for_byte_size(collected_old_bytes));\n+  }\n+\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    \/\/ We have added the last of our collection candidates to a mixed collection.\n+    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n+    clear_triggers();\n+    _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+  } else if (included_old_regions == 0) {\n+    \/\/ We have candidates, but none were included for evacuation - are they all pinned?\n+    \/\/ or did we just not have enough room for any of them in this collection set?\n+    \/\/ We don't want a region with a stuck pin to prevent subsequent old collections, so\n+    \/\/ if they are all pinned we transition to a state that will allow us to make these uncollected\n+    \/\/ (pinned) regions parseable.\n+    if (all_candidates_are_pinned()) {\n+      log_info(gc)(\"All candidate regions \" UINT32_FORMAT \" are pinned\", unprocessed_old_collection_candidates());\n+      _old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_FILL);\n+    } else {\n+      log_info(gc)(\"No regions selected for mixed collection. \"\n+                   \"Old evacuation budget: \" BYTES_FORMAT \", Remaining evacuation budget: \" BYTES_FORMAT\n+                   \", Lost capacity: \" BYTES_FORMAT\n+                   \", Next candidate: \" UINT32_FORMAT \", Last candidate: \" UINT32_FORMAT,\n+                   FORMAT_BYTES(heap->get_old_evac_reserve()),\n+                   FORMAT_BYTES(remaining_old_evacuation_budget),\n+                   FORMAT_BYTES(lost_evacuation_capacity),\n+                   _next_old_collection_candidate, _last_old_collection_candidate);\n+    }\n+  }\n+\n+  return (included_old_regions > 0);\n+}\n+\n+bool ShenandoahOldHeuristics::all_candidates_are_pinned() {\n+#ifdef ASSERT\n+  if (uint(os::random()) % 100 < ShenandoahCoalesceChance) {\n+    return true;\n+  }\n+#endif\n+\n+  for (uint i = _next_old_collection_candidate; i < _last_old_collection_candidate; ++i) {\n+    ShenandoahHeapRegion* region = _region_data[i]._region;\n+    if (!region->is_pinned()) {\n+      return false;\n+    }\n+  }\n+  return true;\n+}\n+\n+void ShenandoahOldHeuristics::slide_pinned_regions_to_front() {\n+  \/\/ Find the first unpinned region to the left of the next region that\n+  \/\/ will be added to the collection set. These regions will have been\n+  \/\/ added to the cset, so we can use them to hold pointers to regions\n+  \/\/ that were pinned when the cset was chosen.\n+  \/\/ [ r p r p p p r r ]\n+  \/\/     ^         ^ ^\n+  \/\/     |         | | pointer to next region to add to a mixed collection is here.\n+  \/\/     |         | first r to the left should be in the collection set now.\n+  \/\/     | first pinned region, we don't need to look past this\n+  uint write_index = NOT_FOUND;\n+  for (uint search = _next_old_collection_candidate - 1; search > _first_pinned_candidate; --search) {\n+    ShenandoahHeapRegion* region = _region_data[search]._region;\n+    if (!region->is_pinned()) {\n+      write_index = search;\n+      assert(region->is_cset(), \"Expected unpinned region to be added to the collection set.\");\n+      break;\n+    }\n+  }\n+\n+  \/\/ If we could not find an unpinned region, it means there are no slots available\n+  \/\/ to move up the pinned regions. In this case, we just reset our next index in the\n+  \/\/ hopes that some of these regions will become unpinned before the next mixed\n+  \/\/ collection. We may want to bailout of here instead, as it should be quite\n+  \/\/ rare to have so many pinned regions and may indicate something is wrong.\n+  if (write_index == NOT_FOUND) {\n+    assert(_first_pinned_candidate != NOT_FOUND, \"Should only be here if there are pinned regions.\");\n+    _next_old_collection_candidate = _first_pinned_candidate;\n+    return;\n+  }\n+\n+  \/\/ Find pinned regions to the left and move their pointer into a slot\n+  \/\/ that was pointing at a region that has been added to the cset (or was pointing\n+  \/\/ to a pinned region that we've already moved up). We are done when the leftmost\n+  \/\/ pinned region has been slid up.\n+  \/\/ [ r p r x p p p r ]\n+  \/\/         ^       ^\n+  \/\/         |       | next region for mixed collections\n+  \/\/         | Write pointer is here. We know this region is already in the cset\n+  \/\/         | so we can clobber it with the next pinned region we find.\n+  for (int32_t search = (int32_t)write_index - 1; search >= (int32_t)_first_pinned_candidate; --search) {\n+    RegionData& skipped = _region_data[search];\n+    if (skipped._region->is_pinned()) {\n+      RegionData& available_slot = _region_data[write_index];\n+      available_slot._region = skipped._region;\n+      available_slot._u._live_data = skipped._u._live_data;\n+      --write_index;\n+    }\n+  }\n+\n+  \/\/ Update to read from the leftmost pinned region. Plus one here because we decremented\n+  \/\/ the write index to hold the next found pinned region. We are just moving it back now\n+  \/\/ to point to the first pinned region.\n+  _next_old_collection_candidate = write_index + 1;\n+}\n+\n+void ShenandoahOldHeuristics::prepare_for_old_collections() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  size_t cand_idx = 0;\n+  size_t total_garbage = 0;\n+  size_t num_regions = heap->num_regions();\n+  size_t immediate_garbage = 0;\n+  size_t immediate_regions = 0;\n+  size_t live_data = 0;\n+\n+  RegionData* candidates = _region_data;\n+  for (size_t i = 0; i < num_regions; i++) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (!_old_generation->contains(region)) {\n+      continue;\n+    }\n+\n+    size_t garbage = region->garbage();\n+    size_t live_bytes = region->get_live_data_bytes();\n+    total_garbage += garbage;\n+    live_data += live_bytes;\n+\n+    if (region->is_regular() || region->is_pinned()) {\n+      if (!region->has_live()) {\n+        assert(!region->is_pinned(), \"Pinned region should have live (pinned) objects.\");\n+        region->make_trash_immediate();\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+      } else {\n+        region->begin_preemptible_coalesce_and_fill();\n+        candidates[cand_idx]._region = region;\n+        candidates[cand_idx]._u._live_data = live_bytes;\n+        cand_idx++;\n+      }\n+    } else if (region->is_humongous_start()) {\n+      if (!region->has_live()) {\n+        \/\/ The humongous object is dead, we can just return this region and the continuations\n+        \/\/ immediately to the freeset - no evacuations are necessary here. The continuations\n+        \/\/ will be made into trash by this method, so they'll be skipped by the 'is_regular'\n+        \/\/ check above, but we still need to count the start region.\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+        size_t region_count = heap->trash_humongous_region_at(region);\n+        log_debug(gc)(\"Trashed \" SIZE_FORMAT \" regions for humongous object.\", region_count);\n+      }\n+    } else if (region->is_trash()) {\n+      \/\/ Count humongous objects made into trash here.\n+      immediate_regions++;\n+      immediate_garbage += garbage;\n+    }\n+  }\n+\n+  _old_generation->set_live_bytes_after_last_mark(live_data);\n+\n+  \/\/ TODO: Consider not running mixed collects if we recovered some threshold percentage of memory from immediate garbage.\n+  \/\/ This would be similar to young and global collections shortcutting evacuation, though we'd probably want a separate\n+  \/\/ threshold for the old generation.\n+\n+  \/\/ Unlike young, we are more interested in efficiently packing OLD-gen than in reclaiming garbage first.  We sort by live-data.\n+  \/\/ Some regular regions may have been promoted in place with no garbage but also with very little live data.  When we \"compact\"\n+  \/\/ old-gen, we want to pack these underutilized regions together so we can have more unaffiliated (unfragmented) free regions\n+  \/\/ in old-gen.\n+  QuickSort::sort<RegionData>(candidates, cand_idx, compare_by_live, false);\n+\n+  \/\/ Any old-gen region that contains (ShenandoahOldGarbageThreshold (default value 25)% garbage or more is to be\n+  \/\/ added to the list of candidates for subsequent mixed evacuations.\n+  \/\/\n+  \/\/ TODO: allow ShenandoahOldGarbageThreshold to be determined adaptively, by heuristics.\n+\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ The convention is to collect regions that have more than this amount of garbage.\n+  const size_t garbage_threshold = region_size_bytes * ShenandoahOldGarbageThreshold \/ 100;\n+\n+  \/\/ Enlightened interpretation: collect regions that have less than this amount of live.\n+  const size_t live_threshold = region_size_bytes - garbage_threshold;\n+\n+  size_t candidates_garbage = 0;\n+  _last_old_region = (uint)cand_idx;\n+  _last_old_collection_candidate = (uint)cand_idx;\n+  _next_old_collection_candidate = 0;\n+\n+  size_t unfragmented = 0;\n+\n+  for (size_t i = 0; i < cand_idx; i++) {\n+    size_t live = candidates[i]._u._live_data;\n+    if (live > live_threshold) {\n+      \/\/ Candidates are sorted in increasing order of live data, so no regions after this will be below the threshold.\n+      _last_old_collection_candidate = (uint)i;\n+      break;\n+    }\n+    size_t region_garbage = candidates[i]._region->garbage();\n+    size_t region_free = candidates[i]._region->free();\n+    candidates_garbage += region_garbage;\n+    unfragmented += region_free;\n+  }\n+\n+  \/\/ Note that we do not coalesce and fill occupied humongous regions\n+  \/\/ HR: humongous regions, RR: regular regions, CF: coalesce and fill regions\n+  size_t collectable_garbage = immediate_garbage + candidates_garbage;\n+  size_t old_candidates = _last_old_collection_candidate;\n+  log_info(gc)(\"Old-Gen Collectable Garbage: \" SIZE_FORMAT \"%s \"\n+               \"consolidated with free: \" SIZE_FORMAT \"%s, over \" SIZE_FORMAT \" regions, \"\n+               \"Old-Gen Immediate Garbage: \" SIZE_FORMAT \"%s over \" SIZE_FORMAT \" regions.\",\n+               byte_size_in_proper_unit(collectable_garbage), proper_unit_for_byte_size(collectable_garbage),\n+               byte_size_in_proper_unit(unfragmented),        proper_unit_for_byte_size(unfragmented), old_candidates,\n+               byte_size_in_proper_unit(immediate_garbage),   proper_unit_for_byte_size(immediate_garbage), immediate_regions);\n+  size_t mixed_evac_live = old_candidates * region_size_bytes - (candidates_garbage + unfragmented);\n+  set_unprocessed_old_collection_candidates_live_memory(mixed_evac_live);\n+  if (unprocessed_old_collection_candidates() == 0) {\n+    _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+  } else {\n+    _old_generation->transition_to(ShenandoahOldGeneration::WAITING_FOR_EVAC);\n+  }\n+}\n+\n+size_t ShenandoahOldHeuristics::unprocessed_old_collection_candidates_live_memory() const {\n+  return _live_bytes_in_unprocessed_candidates;\n+}\n+\n+void ShenandoahOldHeuristics::set_unprocessed_old_collection_candidates_live_memory(size_t initial_live) {\n+  _live_bytes_in_unprocessed_candidates = initial_live;\n+}\n+\n+void ShenandoahOldHeuristics::decrease_unprocessed_old_collection_candidates_live_memory(size_t evacuated_live) {\n+  assert(evacuated_live <= _live_bytes_in_unprocessed_candidates, \"Cannot evacuate more than was present\");\n+  _live_bytes_in_unprocessed_candidates -= evacuated_live;\n+}\n+\n+\/\/ Used by unit test: test_shenandoahOldHeuristic.cpp\n+uint ShenandoahOldHeuristics::last_old_collection_candidate_index() const {\n+  return _last_old_collection_candidate;\n+}\n+\n+uint ShenandoahOldHeuristics::unprocessed_old_collection_candidates() const {\n+  return _last_old_collection_candidate - _next_old_collection_candidate;\n+}\n+\n+ShenandoahHeapRegion* ShenandoahOldHeuristics::next_old_collection_candidate() {\n+  while (_next_old_collection_candidate < _last_old_collection_candidate) {\n+    ShenandoahHeapRegion* next = _region_data[_next_old_collection_candidate]._region;\n+    if (!next->is_pinned()) {\n+      return next;\n+    } else {\n+      assert(next->is_pinned(), \"sanity\");\n+      if (_first_pinned_candidate == NOT_FOUND) {\n+        _first_pinned_candidate = _next_old_collection_candidate;\n+      }\n+    }\n+\n+    _next_old_collection_candidate++;\n+  }\n+  return nullptr;\n+}\n+\n+void ShenandoahOldHeuristics::consume_old_collection_candidate() {\n+  _next_old_collection_candidate++;\n+}\n+\n+uint ShenandoahOldHeuristics::last_old_region_index() const {\n+  return _last_old_region;\n+}\n+\n+unsigned int ShenandoahOldHeuristics::get_coalesce_and_fill_candidates(ShenandoahHeapRegion** buffer) {\n+  uint end = _last_old_region;\n+  uint index = _next_old_collection_candidate;\n+  while (index < end) {\n+    *buffer++ = _region_data[index++]._region;\n+  }\n+  return (_last_old_region - _next_old_collection_candidate);\n+}\n+\n+void ShenandoahOldHeuristics::abandon_collection_candidates() {\n+  _last_old_collection_candidate = 0;\n+  _next_old_collection_candidate = 0;\n+  _last_old_region = 0;\n+}\n+\n+void ShenandoahOldHeuristics::record_cycle_end() {\n+  this->ShenandoahHeuristics::record_cycle_end();\n+  clear_triggers();\n+}\n+\n+void ShenandoahOldHeuristics::trigger_old_has_grown() {\n+  _growth_trigger = true;\n+}\n+\n+\n+void ShenandoahOldHeuristics::clear_triggers() {\n+  \/\/ Clear any triggers that were set during mixed evacuations.  Conditions may be different now that this phase has finished.\n+  _cannot_expand_trigger = false;\n+  _fragmentation_trigger = false;\n+  _growth_trigger = false;\n+ }\n+\n+bool ShenandoahOldHeuristics::should_start_gc() {\n+  \/\/ Cannot start a new old-gen GC until previous one has finished.\n+  \/\/\n+  \/\/ Future refinement: under certain circumstances, we might be more sophisticated about this choice.\n+  \/\/ For example, we could choose to abandon the previous old collection before it has completed evacuations.\n+  if (!_old_generation->can_start_gc()) {\n+    return false;\n+  }\n+\n+  if (_cannot_expand_trigger) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t old_gen_capacity = _old_generation->max_capacity();\n+    size_t heap_capacity = heap->capacity();\n+    double percent = percent_of(old_gen_capacity, heap_capacity);\n+    log_info(gc)(\"Trigger (OLD): Expansion failure, current size: \" SIZE_FORMAT \"%s which is %.1f%% of total heap size\",\n+                 byte_size_in_proper_unit(old_gen_capacity), proper_unit_for_byte_size(old_gen_capacity), percent);\n+    return true;\n+  }\n+\n+  if (_fragmentation_trigger) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t used = _old_generation->used();\n+    size_t used_regions_size = _old_generation->used_regions_size();\n+    size_t used_regions = _old_generation->used_regions();\n+    assert(used_regions_size > used_regions, \"Cannot have more used than used regions\");\n+    size_t fragmented_free = used_regions_size - used;\n+    double percent = percent_of(fragmented_free, used_regions_size);\n+    log_info(gc)(\"Trigger (OLD): Old has become fragmented: \"\n+                 SIZE_FORMAT \"%s available bytes spread between \" SIZE_FORMAT \" regions (%.1f%% free)\",\n+                 byte_size_in_proper_unit(fragmented_free), proper_unit_for_byte_size(fragmented_free), used_regions, percent);\n+    return true;\n+  }\n+\n+  if (_growth_trigger) {\n+    \/\/ Growth may be falsely triggered during mixed evacuations, before the mixed-evacuation candidates have been\n+    \/\/ evacuated.  Before acting on a false trigger, we check to confirm the trigger condition is still satisfied.\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t current_usage = _old_generation->used();\n+    size_t trigger_threshold = _old_generation->usage_trigger_threshold();\n+    if (current_usage > trigger_threshold) {\n+      size_t live_at_previous_old = _old_generation->get_live_bytes_after_last_mark();\n+      double percent_growth = percent_of(current_usage - live_at_previous_old, live_at_previous_old);\n+      log_info(gc)(\"Trigger (OLD): Old has overgrown, live at end of previous OLD marking: \"\n+                   SIZE_FORMAT \"%s, current usage: \" SIZE_FORMAT \"%s, percent growth: %.1f%%\",\n+                   byte_size_in_proper_unit(live_at_previous_old), proper_unit_for_byte_size(live_at_previous_old),\n+                   byte_size_in_proper_unit(current_usage), proper_unit_for_byte_size(current_usage), percent_growth);\n+      return true;\n+    } else {\n+      _growth_trigger = false;\n+    }\n+  }\n+\n+  \/\/ Otherwise, defer to inherited heuristic for gc trigger.\n+  return this->ShenandoahHeuristics::should_start_gc();\n+}\n+\n+void ShenandoahOldHeuristics::record_success_concurrent(bool abbreviated) {\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n+  this->ShenandoahHeuristics::record_success_concurrent(abbreviated);\n+}\n+\n+void ShenandoahOldHeuristics::record_success_degenerated() {\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n+  this->ShenandoahHeuristics::record_success_degenerated();\n+}\n+\n+void ShenandoahOldHeuristics::record_success_full() {\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n+  this->ShenandoahHeuristics::record_success_full();\n+}\n+\n+const char* ShenandoahOldHeuristics::name() {\n+  return \"Old\";\n+}\n+\n+bool ShenandoahOldHeuristics::is_diagnostic() {\n+  return false;\n+}\n+\n+bool ShenandoahOldHeuristics::is_experimental() {\n+  return true;\n+}\n+\n+void ShenandoahOldHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* set,\n+                                                                    ShenandoahHeuristics::RegionData* data,\n+                                                                    size_t data_size, size_t free) {\n+  ShouldNotReachHere();\n+}\n+\n+\n+#undef BYTES_FORMAT\n+#undef FORMAT_BYTES\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":580,"deletions":0,"binary":false,"changes":580,"status":"added"},{"patch":"@@ -39,0 +39,1 @@\n+  virtual const char* name() const = 0;\n@@ -41,0 +42,1 @@\n+  virtual size_t soft_available() const = 0;\n@@ -42,0 +44,1 @@\n+  virtual size_t used() const = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,243 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+\n+#include \"utilities\/quickSort.hpp\"\n+\n+ShenandoahYoungHeuristics::ShenandoahYoungHeuristics(ShenandoahYoungGeneration* generation)\n+        : ShenandoahGenerationalHeuristics(generation) {\n+  assert(!generation->is_old(), \"Young heuristics only accept the young generation\");\n+}\n+\n+\n+void ShenandoahYoungHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                      RegionData* data, size_t size,\n+                                                                      size_t actual_free) {\n+  \/\/ The logic for cset selection in adaptive is as follows:\n+  \/\/\n+  \/\/   1. We cannot get cset larger than available free space. Otherwise we guarantee OOME\n+  \/\/      during evacuation, and thus guarantee full GC. In practice, we also want to let\n+  \/\/      application to allocate something. This is why we limit CSet to some fraction of\n+  \/\/      available space. In non-overloaded heap, max_cset would contain all plausible candidates\n+  \/\/      over garbage threshold.\n+  \/\/\n+  \/\/   2. We should not get cset too low so that free threshold would not be met right\n+  \/\/      after the cycle. Otherwise we get back-to-back cycles for no reason if heap is\n+  \/\/      too fragmented. In non-overloaded non-fragmented heap min_garbage would be around zero.\n+  \/\/\n+  \/\/ Therefore, we start by sorting the regions by garbage. Then we unconditionally add the best candidates\n+  \/\/ before we meet min_garbage. Then we add all candidates that fit with a garbage threshold before\n+  \/\/ we hit max_cset. When max_cset is hit, we terminate the cset selection. Note that in this scheme,\n+  \/\/ ShenandoahGarbageThreshold is the soft threshold which would be ignored until min_garbage is hit.\n+\n+  \/\/ In generational mode, the sort order within the data array is not strictly descending amounts of garbage.  In\n+  \/\/ particular, regions that have reached tenure age will be sorted into this array before younger regions that contain\n+  \/\/ more garbage.  This represents one of the reasons why we keep looking at regions even after we decide, for example,\n+  \/\/ to exclude one of the regions because it might require evacuation of too much live data.\n+\n+  \/\/ Better select garbage-first regions\n+  QuickSort::sort<RegionData>(data, (int) size, compare_by_garbage, false);\n+\n+  size_t cur_young_garbage = add_preselected_regions_to_collection_set(cset, data, size);\n+\n+  choose_young_collection_set(cset, data, size, actual_free, cur_young_garbage);\n+\n+  log_cset_composition(cset);\n+}\n+\n+void ShenandoahYoungHeuristics::choose_young_collection_set(ShenandoahCollectionSet* cset,\n+                                                            const RegionData* data,\n+                                                            size_t size, size_t actual_free,\n+                                                            size_t cur_young_garbage) const {\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  size_t capacity = heap->young_generation()->max_capacity();\n+  size_t garbage_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahGarbageThreshold \/ 100;\n+  size_t ignore_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahIgnoreGarbageThreshold \/ 100;\n+\n+  \/\/ This is young-gen collection or a mixed evacuation.\n+  \/\/ If this is mixed evacuation, the old-gen candidate regions have already been added.\n+  size_t max_cset = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t cur_cset = 0;\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n+  size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n+\n+  log_info(gc, ergo)(\n+          \"Adaptive CSet Selection for YOUNG. Max Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n+          byte_size_in_proper_unit(max_cset), proper_unit_for_byte_size(max_cset),\n+          byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+\n+  for (size_t idx = 0; idx < size; idx++) {\n+    ShenandoahHeapRegion* r = data[idx]._region;\n+    if (cset->is_preselected(r->index())) {\n+      continue;\n+    }\n+    if (r->age() < InitialTenuringThreshold) {\n+      size_t new_cset = cur_cset + r->get_live_data_bytes();\n+      size_t region_garbage = r->garbage();\n+      size_t new_garbage = cur_young_garbage + region_garbage;\n+      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+      assert(r->is_young(), \"Only young candidates expected in the data array\");\n+      if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+        cur_cset = new_cset;\n+        cur_young_garbage = new_garbage;\n+        cset->add_region(r);\n+      }\n+    }\n+    \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n+    \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects or because\n+    \/\/ they are to be promoted in place.\n+  }\n+}\n+\n+\n+bool ShenandoahYoungHeuristics::should_start_gc() {\n+  \/\/ inherited triggers have already decided to start a cycle, so no further evaluation is required\n+  if (ShenandoahAdaptiveHeuristics::should_start_gc()) {\n+    return true;\n+  }\n+\n+  \/\/ Get through promotions and mixed evacuations as quickly as possible.  These cycles sometimes require significantly\n+  \/\/ more time than traditional young-generation cycles so start them up as soon as possible.  This is a \"mitigation\"\n+  \/\/ for the reality that old-gen and young-gen activities are not truly \"concurrent\".  If there is old-gen work to\n+  \/\/ be done, we start up the young-gen GC threads so they can do some of this old-gen work.  As implemented, promotion\n+  \/\/ gets priority over old-gen marking.\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t promo_potential = heap->get_promotion_potential();\n+  if (promo_potential > 0) {\n+    \/\/ Detect unsigned arithmetic underflow\n+    assert(promo_potential < heap->capacity(), \"Sanity\");\n+    log_info(gc)(\"Trigger (%s): expedite promotion of \" SIZE_FORMAT \"%s\",\n+                 _space_info->name(),\n+                 byte_size_in_proper_unit(promo_potential),\n+                 proper_unit_for_byte_size(promo_potential));\n+    return true;\n+  }\n+\n+  size_t promo_in_place_potential = heap->get_promotion_in_place_potential();\n+  if (promo_in_place_potential > 0) {\n+    \/\/ Detect unsigned arithmetic underflow\n+    assert(promo_in_place_potential < heap->capacity(), \"Sanity\");\n+    log_info(gc)(\"Trigger (%s): expedite promotion in place of \" SIZE_FORMAT \"%s\",\n+                 _space_info->name(),\n+                 byte_size_in_proper_unit(promo_in_place_potential),\n+                 proper_unit_for_byte_size(promo_in_place_potential));\n+    return true;\n+  }\n+\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  size_t mixed_candidates = old_heuristics->unprocessed_old_collection_candidates();\n+  if (mixed_candidates > 0) {\n+    \/\/ We need to run young GC in order to open up some free heap regions so we can finish mixed evacuations.\n+    log_info(gc)(\"Trigger (%s): expedite mixed evacuation of \" SIZE_FORMAT \" regions\",\n+                 _space_info->name(), mixed_candidates);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+\/\/ Return a conservative estimate of how much memory can be allocated before we need to start GC. The estimate is based\n+\/\/ on memory that is currently available within young generation plus all of the memory that will be added to the young\n+\/\/ generation at the end of the current cycle (as represented by young_regions_to_be_reclaimed) and on the anticipated\n+\/\/ amount of time required to perform a GC.\n+size_t ShenandoahYoungHeuristics::bytes_of_allocation_runway_before_gc_trigger(size_t young_regions_to_be_reclaimed) {\n+  size_t capacity = _space_info->soft_max_capacity();\n+  size_t usage = _space_info->used();\n+  size_t available = (capacity > usage)? capacity - usage: 0;\n+  size_t allocated = _space_info->bytes_allocated_since_gc_start();\n+\n+  size_t available_young_collected = ShenandoahHeap::heap()->collection_set()->get_young_available_bytes_collected();\n+  size_t anticipated_available =\n+          available + young_regions_to_be_reclaimed * ShenandoahHeapRegion::region_size_bytes() - available_young_collected;\n+  size_t spike_headroom = capacity * ShenandoahAllocSpikeFactor \/ 100;\n+  size_t penalties      = capacity * _gc_time_penalties \/ 100;\n+\n+  double rate = _allocation_rate.sample(allocated);\n+\n+  \/\/ At what value of available, would avg and spike triggers occur?\n+  \/\/  if allocation_headroom < avg_cycle_time * avg_alloc_rate, then we experience avg trigger\n+  \/\/  if allocation_headroom < avg_cycle_time * rate, then we experience spike trigger if is_spiking\n+  \/\/\n+  \/\/ allocation_headroom =\n+  \/\/     0, if penalties > available or if penalties + spike_headroom > available\n+  \/\/     available - penalties - spike_headroom, otherwise\n+  \/\/\n+  \/\/ so we trigger if available - penalties - spike_headroom < avg_cycle_time * avg_alloc_rate, which is to say\n+  \/\/                  available < avg_cycle_time * avg_alloc_rate + penalties + spike_headroom\n+  \/\/            or if available < penalties + spike_headroom\n+  \/\/\n+  \/\/ since avg_cycle_time * avg_alloc_rate > 0, the first test is sufficient to test both conditions\n+  \/\/\n+  \/\/ thus, evac_slack_avg is MIN2(0,  available - avg_cycle_time * avg_alloc_rate + penalties + spike_headroom)\n+  \/\/\n+  \/\/ similarly, evac_slack_spiking is MIN2(0, available - avg_cycle_time * rate + penalties + spike_headroom)\n+  \/\/ but evac_slack_spiking is only relevant if is_spiking, as defined below.\n+\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+\n+  \/\/ TODO: Consider making conservative adjustments to avg_cycle_time, such as: (avg_cycle_time *= 2) in cases where\n+  \/\/ we expect a longer-than-normal GC duration.  This includes mixed evacuations, evacuation that perform promotion\n+  \/\/ including promotion in place, and OLD GC bootstrap cycles.  It has been observed that these cycles sometimes\n+  \/\/ require twice or more the duration of \"normal\" GC cycles.  We have experimented with this approach.  While it\n+  \/\/ does appear to reduce the frequency of degenerated cycles due to late triggers, it also has the effect of reducing\n+  \/\/ evacuation slack so that there is less memory available to be transferred to OLD.  The result is that we\n+  \/\/ throttle promotion and it takes too long to move old objects out of the young generation.\n+\n+  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+  size_t evac_slack_avg;\n+  if (anticipated_available > avg_cycle_time * avg_alloc_rate + penalties + spike_headroom) {\n+    evac_slack_avg = anticipated_available - (avg_cycle_time * avg_alloc_rate + penalties + spike_headroom);\n+  } else {\n+    \/\/ we have no slack because it's already time to trigger\n+    evac_slack_avg = 0;\n+  }\n+\n+  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+  size_t evac_slack_spiking;\n+  if (is_spiking) {\n+    if (anticipated_available > avg_cycle_time * rate + penalties + spike_headroom) {\n+      evac_slack_spiking = anticipated_available - (avg_cycle_time * rate + penalties + spike_headroom);\n+    } else {\n+      \/\/ we have no slack because it's already time to trigger\n+      evac_slack_spiking = 0;\n+    }\n+  } else {\n+    evac_slack_spiking = evac_slack_avg;\n+  }\n+\n+  size_t threshold = min_free_threshold();\n+  size_t evac_min_threshold = (anticipated_available > threshold)? anticipated_available - threshold: 0;\n+  return MIN3(evac_slack_spiking, evac_slack_avg, evac_min_threshold);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":243,"deletions":0,"binary":false,"changes":243,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+\n+ShenandoahHeuristics* ShenandoahMode::initialize_heuristics(ShenandoahSpaceInfo* space_info) const {\n+  if (ShenandoahGCHeuristics == nullptr) {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n+  }\n+\n+  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n+    return new ShenandoahAggressiveHeuristics(space_info);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n+    return new ShenandoahStaticHeuristics(space_info);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n+    return new ShenandoahAdaptiveHeuristics(space_info);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n+    return new ShenandoahCompactHeuristics(space_info);\n+  } else {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.cpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -29,0 +30,2 @@\n+#include \"runtime\/java.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -30,0 +33,1 @@\n+class ShenandoahSpaceInfo;\n@@ -51,1 +55,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const = 0;\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahSpaceInfo* space_info) const;\n@@ -55,0 +59,1 @@\n+  virtual bool is_generational() { return false; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.hpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n@@ -31,1 +33,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n@@ -59,1 +60,2 @@\n-ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics() const {\n+\n+ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics(ShenandoahSpaceInfo* space_info) const {\n@@ -63,1 +65,1 @@\n-  return new ShenandoahPassiveHeuristics(ShenandoahHeap::heap());\n+  return new ShenandoahPassiveHeuristics(space_info);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -33,2 +33,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n-\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahSpaceInfo* space_info) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,240 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahSpaceInfo.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahLock.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkingContext.hpp\"\n+\n+class ShenandoahHeapRegion;\n+class ShenandoahHeapRegionClosure;\n+class ShenandoahReferenceProcessor;\n+class ShenandoahHeap;\n+class ShenandoahMode;\n+\n+class ShenandoahGeneration : public CHeapObj<mtGC>, public ShenandoahSpaceInfo {\n+  friend class VMStructs;\n+private:\n+  ShenandoahGenerationType const _type;\n+\n+  \/\/ Marking task queues and completeness\n+  ShenandoahObjToScanQueueSet* _task_queues;\n+  ShenandoahSharedFlag _is_marking_complete;\n+\n+  ShenandoahReferenceProcessor* const _ref_processor;\n+\n+  double _collection_thread_time_s;\n+\n+  size_t _affiliated_region_count;\n+\n+  \/\/ How much free memory is left in the last region of humongous objects.\n+  \/\/ This is _not_ included in used, but it _is_ deducted from available,\n+  \/\/ which gives the heuristics a more accurate view of how much memory remains\n+  \/\/ for allocation. This figure is also included the heap status logging.\n+  \/\/ The units are bytes. The value is only changed on a safepoint or under the\n+  \/\/ heap lock.\n+  size_t _humongous_waste;\n+\n+protected:\n+  \/\/ Usage\n+\n+  volatile size_t _used;\n+  volatile size_t _bytes_allocated_since_gc_start;\n+  size_t _max_capacity;\n+  size_t _soft_max_capacity;\n+\n+  ShenandoahHeuristics* _heuristics;\n+\n+private:\n+  \/\/ Compute evacuation budgets prior to choosing collection set.\n+  void compute_evacuation_budgets(ShenandoahHeap* heap,\n+                                  bool* preselected_regions,\n+                                  ShenandoahCollectionSet* collection_set,\n+                                  size_t& consumed_by_advance_promotion);\n+\n+  \/\/ Adjust evacuation budgets after choosing collection set.\n+  void adjust_evacuation_budgets(ShenandoahHeap* heap,\n+                                 ShenandoahCollectionSet* collection_set,\n+                                 size_t consumed_by_advance_promotion);\n+\n+  \/\/ Preselect for inclusion into the collection set regions whose age is\n+  \/\/ at or above tenure age and which contain more than ShenandoahOldGarbageThreshold\n+  \/\/ amounts of garbage.\n+  \/\/\n+  \/\/ A side effect performed by this function is to tally up the number of regions and\n+  \/\/ the number of live bytes that we plan to promote-in-place during the current GC cycle.\n+  \/\/ This information, which is stored with an invocation of heap->set_promotion_in_place_potential(),\n+  \/\/ feeds into subsequent decisions about when to trigger the next GC and may identify\n+  \/\/ special work to be done during this GC cycle if we choose to abbreviate it.\n+  \/\/\n+  \/\/ Returns bytes of old-gen memory consumed by selected aged regions\n+  size_t select_aged_regions(size_t old_available,\n+                             size_t num_regions, bool\n+                             candidate_regions_for_promotion_by_copy[]);\n+\n+  size_t available(size_t capacity) const;\n+\n+ public:\n+  ShenandoahGeneration(ShenandoahGenerationType type,\n+                       uint max_workers,\n+                       size_t max_capacity,\n+                       size_t soft_max_capacity);\n+  ~ShenandoahGeneration();\n+\n+  bool is_young() const  { return _type == YOUNG; }\n+  bool is_old() const    { return _type == OLD; }\n+  bool is_global() const { return _type == GLOBAL_GEN || _type == GLOBAL_NON_GEN; }\n+\n+  inline ShenandoahGenerationType type() const { return _type; }\n+\n+  inline ShenandoahHeuristics* heuristics() const { return _heuristics; }\n+\n+  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n+\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode);\n+\n+  size_t soft_max_capacity() const override { return _soft_max_capacity; }\n+  size_t max_capacity() const override      { return _max_capacity; }\n+  virtual size_t used_regions() const;\n+  virtual size_t used_regions_size() const;\n+  virtual size_t free_unaffiliated_regions() const;\n+  size_t used() const override { return _used; }\n+  size_t available() const override;\n+\n+  \/\/ Returns the memory available based on the _soft_ max heap capacity (soft_max_heap - used).\n+  \/\/ The soft max heap size may be adjusted lower than the max heap size to cause the trigger\n+  \/\/ to believe it has less memory available than is _really_ available. Lowering the soft\n+  \/\/ max heap size will cause the adaptive heuristic to run more frequent cycles.\n+  size_t soft_available() const override;\n+\n+  size_t bytes_allocated_since_gc_start() const override;\n+  void reset_bytes_allocated_since_gc_start();\n+  void increase_allocated(size_t bytes);\n+\n+  \/\/ These methods change the capacity of the region by adding or subtracting the given number of bytes from the current\n+  \/\/ capacity.\n+  void increase_capacity(size_t increment);\n+  void decrease_capacity(size_t decrement);\n+\n+  void set_soft_max_capacity(size_t soft_max_capacity) {\n+    _soft_max_capacity = soft_max_capacity;\n+  }\n+\n+  void log_status(const char* msg) const;\n+\n+  \/\/ Used directly by FullGC\n+  void reset_mark_bitmap();\n+\n+  \/\/ Used by concurrent and degenerated GC to reset remembered set.\n+  void swap_remembered_set();\n+\n+  \/\/ Update the read cards with the state of the write table (write table is not cleared).\n+  void merge_write_table();\n+\n+  \/\/ Called before init mark, expected to prepare regions for marking.\n+  virtual void prepare_gc();\n+\n+  \/\/ Called during final mark, chooses collection set, rebuilds free set.\n+  virtual void prepare_regions_and_collection_set(bool concurrent);\n+\n+  \/\/ Cancel marking (used by Full collect and when cancelling cycle).\n+  virtual void cancel_marking();\n+\n+  \/\/ Return true if this region is affiliated with this generation.\n+  virtual bool contains(ShenandoahHeapRegion* region) const = 0;\n+\n+  \/\/ Return true if this object is affiliated with this generation.\n+  virtual bool contains(oop obj) const = 0;\n+\n+  \/\/ Apply closure to all regions affiliated with this generation.\n+  virtual void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) = 0;\n+\n+  \/\/ Apply closure to all regions affiliated with this generation (single threaded).\n+  virtual void heap_region_iterate(ShenandoahHeapRegionClosure* cl) = 0;\n+\n+  \/\/ This is public to support cancellation of marking when a Full cycle is started.\n+  virtual void set_concurrent_mark_in_progress(bool in_progress) = 0;\n+\n+  \/\/ Check the bitmap only for regions belong to this generation.\n+  bool is_bitmap_clear();\n+\n+  \/\/ We need to track the status of marking for different generations.\n+  bool is_mark_complete();\n+  void set_mark_complete();\n+  void set_mark_incomplete();\n+\n+  ShenandoahMarkingContext* complete_marking_context();\n+\n+  \/\/ Task queues\n+  ShenandoahObjToScanQueueSet* task_queues() const { return _task_queues; }\n+  virtual void reserve_task_queues(uint workers);\n+  virtual ShenandoahObjToScanQueueSet* old_gen_task_queues() const;\n+\n+  \/\/ Scan remembered set at start of concurrent young-gen marking.\n+  void scan_remembered_set(bool is_concurrent);\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t increment_affiliated_region_count();\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t decrement_affiliated_region_count();\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t increase_affiliated_region_count(size_t delta);\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t decrease_affiliated_region_count(size_t delta);\n+\n+  void establish_usage(size_t num_regions, size_t num_bytes, size_t humongous_waste);\n+\n+  void increase_used(size_t bytes);\n+  void decrease_used(size_t bytes);\n+\n+  void increase_humongous_waste(size_t bytes);\n+  void decrease_humongous_waste(size_t bytes);\n+  size_t get_humongous_waste() const { return _humongous_waste; }\n+\n+  virtual bool is_concurrent_mark_in_progress() = 0;\n+  void confirm_heuristics_mode();\n+\n+  virtual void record_success_concurrent(bool abbreviated);\n+  virtual void record_success_degenerated();\n+\n+  \/\/ Record the total on-cpu time a thread has spent collecting this\n+  \/\/ generation. This is only called by the control thread (at the start\n+  \/\/ of a collection) and by the VM thread at the end of the collection,\n+  \/\/ so there are no locking concerns.\n+  virtual void add_collection_time(double time_seconds);\n+\n+  \/\/ This returns the accumulated collection time and resets it to zero.\n+  \/\/ This is used to decide which generation should be resized.\n+  double reset_collection_time();\n+};\n+\n+#endif \/\/ SHARE_VM_GC_SHENANDOAH_SHENANDOAHGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":240,"deletions":0,"binary":false,"changes":240,"status":"added"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -38,0 +39,3 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -39,0 +43,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -45,0 +50,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -46,0 +52,1 @@\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -55,0 +62,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -61,0 +69,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -68,0 +77,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -71,0 +82,2 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n@@ -162,3 +175,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -180,0 +190,3 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_heuristics_generations();\n+\n@@ -218,0 +231,25 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize);\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == nullptr) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -265,1 +303,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -347,0 +385,1 @@\n+  _affiliations = NEW_C_HEAP_ARRAY(uint8_t, _num_regions, mtGC);\n@@ -352,0 +391,1 @@\n+\n@@ -363,0 +403,2 @@\n+\n+      _affiliations[i] = ShenandoahAffiliation::FREE;\n@@ -367,0 +409,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -368,1 +411,3 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+    _free_set->rebuild(young_cset_regions, old_cset_regions);\n@@ -430,0 +475,1 @@\n+  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n@@ -436,1 +482,31 @@\n-void ShenandoahHeap::initialize_mode() {\n+size_t ShenandoahHeap::max_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return _generation_sizer.max_young_size();\n+    case OLD:\n+      return max_capacity() - _generation_sizer.min_young_size();\n+    case GLOBAL_GEN:\n+    case GLOBAL_NON_GEN:\n+      return max_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+size_t ShenandoahHeap::min_size_for(ShenandoahGeneration* generation) const {\n+  switch (generation->type()) {\n+    case YOUNG:\n+      return _generation_sizer.min_young_size();\n+    case OLD:\n+      return max_capacity() - _generation_sizer.max_young_size();\n+    case GLOBAL_GEN:\n+    case GLOBAL_NON_GEN:\n+      return min_capacity();\n+    default:\n+      ShouldNotReachHere();\n+      return 0;\n+  }\n+}\n+\n+void ShenandoahHeap::initialize_heuristics_generations() {\n@@ -444,0 +520,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -461,4 +539,9 @@\n-}\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != nullptr, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n+  \/\/ Max capacity is the maximum _allowed_ capacity. That is, the maximum allowed capacity\n+  \/\/ for old would be total heap - minimum capacity of young. This means the sum of the maximum\n+  \/\/ allowed for old and young could exceed the total heap size. It remains the case that the\n+  \/\/ _actual_ capacity of young + old = total.\n+  _generation_sizer.heap_size_changed(max_capacity());\n+  size_t initial_capacity_young = _generation_sizer.max_young_size();\n+  size_t max_capacity_young = _generation_sizer.max_young_size();\n+  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n@@ -467,10 +550,6 @@\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n-  }\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_gc_mode->is_generational(), _max_workers, max_capacity(), max_capacity());\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  _young_generation->initialize_heuristics(_gc_mode);\n+  _old_generation->initialize_heuristics(_gc_mode);\n@@ -486,0 +565,2 @@\n+  _gc_generation(nullptr),\n+  _prepare_for_old_mark(false),\n@@ -487,1 +568,2 @@\n-  _used(0),\n+  _promotion_potential(0),\n+  _promotion_in_place_potential(0),\n@@ -489,2 +571,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -496,0 +577,1 @@\n+  _affiliations(nullptr),\n@@ -497,0 +579,12 @@\n+  _promoted_reserve(0),\n+  _old_evac_reserve(0),\n+  _old_evac_expended(0),\n+  _young_evac_reserve(0),\n+  _captured_old_usage(0),\n+  _previous_promotion(0),\n+  _upgraded_to_full(false),\n+  _has_evacuation_reserve_quantities(false),\n+  _cancel_requested_time(0),\n+  _young_generation(nullptr),\n+  _global_generation(nullptr),\n+  _old_generation(nullptr),\n@@ -498,0 +592,1 @@\n+  _regulator_thread(nullptr),\n@@ -499,2 +594,0 @@\n-  _gc_mode(nullptr),\n-  _heuristics(nullptr),\n@@ -505,0 +598,3 @@\n+  _evac_tracker(new ShenandoahEvacuationTracker()),\n+  _mmu_tracker(),\n+  _generation_sizer(&_mmu_tracker),\n@@ -507,0 +603,2 @@\n+  _young_gen_memory_pool(nullptr),\n+  _old_gen_memory_pool(nullptr),\n@@ -512,1 +610,2 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n+  _old_regions_surplus(0),\n+  _old_regions_deficit(0),\n@@ -520,1 +619,2 @@\n-  _collection_set(nullptr)\n+  _collection_set(nullptr),\n+  _card_scan(nullptr)\n@@ -522,17 +622,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == nullptr) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -545,29 +628,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != nullptr) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -588,1 +642,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -639,0 +694,2 @@\n+  _mmu_tracker.initialize();\n+\n@@ -652,2 +709,0 @@\n-  _heuristics->initialize();\n-\n@@ -657,0 +712,21 @@\n+\n+ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n+  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n+}\n+\n+ShenandoahYoungHeuristics* ShenandoahHeap::young_heuristics() {\n+  return (ShenandoahYoungHeuristics*) _young_generation->heuristics();\n+}\n+\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return _old_generation->state() == ShenandoahOldGeneration::WAITING_FOR_EVAC;\n+}\n+\n+bool ShenandoahHeap::is_old_bitmap_stable() const {\n+  return _old_generation->is_mark_complete();\n+}\n+\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != nullptr && _gc_generation->is_young();\n+}\n+\n@@ -658,1 +734,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -665,4 +741,0 @@\n-size_t ShenandoahHeap::available() const {\n-  return free_set()->available();\n-}\n-\n@@ -679,2 +751,43 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n@@ -683,2 +796,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -687,3 +803,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -692,2 +810,5 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -696,4 +817,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -701,1 +822,3 @@\n-  increase_allocated(bytes);\n+}\n+\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -704,2 +827,2 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n+    if (waste > 0) {\n+      pacer()->claim_for_alloc(waste, true);\n@@ -741,6 +864,0 @@\n-bool ShenandoahHeap::is_in(const void* p) const {\n-  HeapWord* heap_base = (HeapWord*) base();\n-  HeapWord* last_region_end = heap_base + ShenandoahHeapRegion::region_size_words() * num_regions();\n-  return p >= heap_base && p < last_region_end;\n-}\n-\n@@ -774,0 +891,65 @@\n+    regulator_thread()->notify_heap_changed();\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  card_scan()->register_object_without_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  card_scan()->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    old_generation()->increase_allocated(words * HeapWordSize);\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation_failure() {\n+  if (_old_gen_oom_evac.try_set()) {\n+    log_info(gc)(\"Old gen evac failure.\");\n+  }\n+}\n+\n+void ShenandoahHeap::report_promotion_failure(Thread* thread, size_t size) {\n+  \/\/ We squelch excessive reports to reduce noise in logs.\n+  const size_t MaxReportsPerEpoch = 4;\n+  static size_t last_report_epoch = 0;\n+  static size_t epoch_report_count = 0;\n+\n+  size_t promotion_reserve;\n+  size_t promotion_expended;\n+\n+  size_t gc_id = control_thread()->get_gc_id();\n+\n+  if ((gc_id != last_report_epoch) || (epoch_report_count++ < MaxReportsPerEpoch)) {\n+    {\n+      \/\/ Promotion failures should be very rare.  Invest in providing useful diagnostic info.\n+      ShenandoahHeapLocker locker(lock());\n+      promotion_reserve = get_promoted_reserve();\n+      promotion_expended = get_promoted_expended();\n+    }\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    size_t words_remaining = (plab == nullptr)? 0: plab->words_remaining();\n+    const char* promote_enabled = ShenandoahThreadLocalData::allow_plab_promotions(thread)? \"enabled\": \"disabled\";\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_capacity = old_gen->max_capacity();\n+    size_t old_usage = old_gen->used();\n+    size_t old_free_regions = old_gen->free_unaffiliated_regions();\n+\n+    log_info(gc, ergo)(\"Promotion failed, size \" SIZE_FORMAT \", has plab? %s, PLAB remaining: \" SIZE_FORMAT\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n+                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n+                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n+                       old_capacity, old_usage, old_free_regions);\n+\n+    if ((gc_id == last_report_epoch) && (epoch_report_count >= MaxReportsPerEpoch)) {\n+      log_info(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n+    } else if (gc_id != last_report_epoch) {\n+      last_report_epoch = gc_id;;\n+      epoch_report_count = 1;\n+    }\n@@ -783,0 +965,8 @@\n+\n+  \/\/ Limit growth of GCLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    log_debug(gc, free)(\"Allocate new gclab: \" SIZE_FORMAT \", \" SIZE_FORMAT, new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+    new_size = MIN2(new_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+\n@@ -794,0 +984,1 @@\n+    log_debug(gc, free)(\"New gclab size (\" SIZE_FORMAT \") is too small for \" SIZE_FORMAT, new_size, size);\n@@ -826,0 +1017,260 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+  size_t min_size = MAX2(size, PLAB::min_size());\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.\n+  size_t cur_size = ShenandoahThreadLocalData::plab_size(thread);\n+  if (cur_size == 0) {\n+    cur_size = PLAB::min_size();\n+  }\n+  size_t future_size = cur_size * 2;\n+  \/\/ Limit growth of PLABs to ShenandoahMaxEvacLABRatio * the minimum size.  This enables more equitable distribution of\n+  \/\/ available evacuation buidget between the many threads that are coordinating in the evacuation effort.\n+  if (ShenandoahMaxEvacLABRatio > 0) {\n+    future_size = MIN2(future_size, PLAB::min_size() * ShenandoahMaxEvacLABRatio);\n+  }\n+  future_size = MIN2(future_size, PLAB::max_size());\n+  future_size = MAX2(future_size, PLAB::min_size());\n+\n+  size_t unalignment = future_size % CardTable::card_size_in_words();\n+  if (unalignment != 0) {\n+    future_size = future_size - unalignment + CardTable::card_size_in_words();\n+  }\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested cur_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, future_size);\n+  if (cur_size < size) {\n+    \/\/ The PLAB to be allocated is still not large enough to hold the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs in order to represent a single large object allocation.\n+    return nullptr;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab->words_remaining() < PLAB::min_size()) {\n+    \/\/ Retire current PLAB, and allocate a new one.\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+    \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+    \/\/ aligned with the start of a card's memory range.\n+    retire_plab(plab, thread);\n+\n+    size_t actual_size = 0;\n+    \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+    \/\/ less than the remaining evacuation need.  It also adjusts plab_preallocated and expend_promoted if appropriate.\n+    HeapWord* plab_buf = allocate_new_plab(min_size, cur_size, &actual_size);\n+    if (plab_buf == nullptr) {\n+      if (min_size == PLAB::min_size()) {\n+        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return NULL;\n+    } else {\n+      ShenandoahThreadLocalData::enable_plab_retries(thread);\n+    }\n+    assert (size <= actual_size, \"allocation should fit\");\n+    if (ZeroTLAB) {\n+      \/\/ ..and clear it.\n+      Copy::zero_to_words(plab_buf, actual_size);\n+    } else {\n+      \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+      \/\/ Skip mangling the space corresponding to the object header to\n+      \/\/ ensure that the returned space is not considered parsable by\n+      \/\/ any concurrent GC thread.\n+      size_t hdr_size = oopDesc::header_size();\n+      Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+    }\n+    plab->set_buf(plab_buf, actual_size);\n+    if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+      return nullptr;\n+    }\n+    return plab->allocate(size);\n+  } else {\n+    \/\/ If there's still at least min_size() words available within the current plab, don't retire it.  Let's gnaw\n+    \/\/ away on this plab as long as we can.  Meanwhile, return nullptr to force this particular allocation request\n+    \/\/ to be satisfied with a shared allocation.  By packing more promotions into the previously allocated PLAB, we\n+    \/\/ reduce the likelihood of evacuation failures, and we we reduce the need for downsizing our PLABs.\n+    return nullptr;\n+  }\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahHeap::retire_plab(PLAB* plab, Thread* thread) {\n+  \/\/ We don't enforce limits on plab_evacuated.  We let it consume all available old-gen memory in order to reduce\n+  \/\/ probability of an evacuation failure.  We do enforce limits on promotion, to make sure that excessive promotion\n+  \/\/ does not result in an old-gen evacuation failure.  Note that a failed promotion is relatively harmless.  Any\n+  \/\/ object that fails to promote in the current cycle will be eligible for promotion in a subsequent cycle.\n+\n+  \/\/ When the plab was instantiated, its entirety was treated as if the entire buffer was going to be dedicated to\n+  \/\/ promotions.  Now that we are retiring the buffer, we adjust for the reality that the plab is not entirely promotions.\n+  \/\/  1. Some of the plab may have been dedicated to evacuations.\n+  \/\/  2. Some of the plab may have been abandoned due to waste (at the end of the plab).\n+  size_t not_promoted =\n+    ShenandoahThreadLocalData::get_plab_preallocated_promoted(thread) - ShenandoahThreadLocalData::get_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+  ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+  ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+  if (not_promoted > 0) {\n+    unexpend_promoted(not_promoted);\n+  }\n+  size_t waste = plab->waste();\n+  HeapWord* top = plab->top();\n+  plab->retire();\n+  if (top != nullptr && plab->waste() > waste && is_in_old(top)) {\n+    \/\/ If retiring the plab created a filler object, then we\n+    \/\/ need to register it with our card scanner so it can\n+    \/\/ safely walk the region backing the plab.\n+    log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                  plab->waste() - waste, p2i(top));\n+    card_scan()->register_object_without_lock(top);\n+  }\n+}\n+\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  Thread* thread = Thread::current();\n+  retire_plab(plab, thread);\n+}\n+\n+void ShenandoahHeap::cancel_old_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(_old_generation != nullptr, \"Should only have mixed collections in generation mode.\");\n+  log_info(gc)(\"Terminating old gc cycle.\");\n+\n+  \/\/ Stop marking\n+  old_generation()->cancel_marking();\n+  \/\/ Stop coalescing undead objects\n+  set_prepare_for_old_mark_in_progress(false);\n+  \/\/ Stop tracking old regions\n+  old_heuristics()->abandon_collection_candidates();\n+  \/\/ Remove old generation access to young generation mark queues\n+  young_generation()->set_old_gen_task_queues(nullptr);\n+  \/\/ Transition to IDLE now.\n+  _old_generation->transition_to(ShenandoahOldGeneration::IDLE);\n+}\n+\n+bool ShenandoahHeap::is_old_gc_active() {\n+  return _old_generation->state() != ShenandoahOldGeneration::IDLE;\n+}\n+\n+void ShenandoahHeap::coalesce_and_fill_old_regions() {\n+  class ShenandoahGlobalCoalesceAndFill : public ShenandoahHeapRegionClosure {\n+   public:\n+    virtual void heap_region_do(ShenandoahHeapRegion* region) override {\n+      \/\/ old region is not in the collection set and was not immediately trashed\n+      if (region->is_old() && region->is_active() && !region->is_humongous()) {\n+        \/\/ Reset the coalesce and fill boundary because this is a global collect\n+        \/\/ and cannot be preempted by young collects. We want to be sure the entire\n+        \/\/ region is coalesced here and does not resume from a previously interrupted\n+        \/\/ or completed coalescing.\n+        region->begin_preemptible_coalesce_and_fill();\n+        region->oop_fill_and_coalesce();\n+      }\n+    }\n+\n+    virtual bool is_thread_safe() override {\n+      return true;\n+    }\n+  };\n+  ShenandoahGlobalCoalesceAndFill coalesce;\n+  parallel_heap_region_iterate(&coalesce);\n+}\n+\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old\n+void ShenandoahHeap::adjust_generation_sizes_for_next_cycle(\n+  size_t xfer_limit, size_t young_cset_regions, size_t old_cset_regions) {\n+\n+  \/\/ Make sure old-generation is large enough, but no larger, than is necessary to hold mixed evacuations\n+  \/\/ and promotions if we anticipate either.\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t promo_load = get_promotion_potential();\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations\n+  size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  size_t old_reserve = 0;\n+  size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n+  bool doing_mixed = (mixed_candidates > 0);\n+  bool doing_promotions = promo_load > 0;\n+\n+  \/\/ round down\n+  size_t max_old_region_xfer = xfer_limit \/ region_size_bytes;\n+\n+  \/\/ We can limit the reserve to the size of anticipated promotions\n+  size_t max_old_reserve = young_reserve * ShenandoahOldEvacRatioPercent \/ (100 - ShenandoahOldEvacRatioPercent);\n+  \/\/ Here's the algebra:\n+  \/\/  TotalEvacuation = OldEvacuation + YoungEvacuation\n+  \/\/  OldEvacuation = TotalEvacuation*(ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * (ShenandoahOldEvacRatioPercent\/100)\/(1 - ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * ShenandoahOldEvacRatioPercent\/(100 - ShenandoahOldEvacRatioPercent)\n+\n+  size_t reserve_for_mixed, reserve_for_promo;\n+  if (doing_mixed) {\n+    assert(old_generation()->available() >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n+           \"Unaffiliated available must be less than total available\");\n+\n+    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n+    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n+    size_t max_evac_need = (size_t)\n+      (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n+    size_t old_fragmented_available =\n+      old_generation()->available() - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    reserve_for_mixed = max_evac_need + old_fragmented_available;\n+    if (reserve_for_mixed > max_old_reserve) {\n+      reserve_for_mixed = max_old_reserve;\n+    }\n+  } else {\n+    reserve_for_mixed = 0;\n+  }\n+\n+  size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n+  if (doing_promotions) {\n+    \/\/ We're only promoting and we have a maximum bound on the amount to be promoted\n+    reserve_for_promo = (size_t) (promo_load * ShenandoahPromoEvacWaste);\n+    if (reserve_for_promo > available_for_promotions) {\n+      reserve_for_promo = available_for_promotions;\n+    }\n+  } else {\n+    reserve_for_promo = 0;\n+  }\n+  old_reserve = reserve_for_mixed + reserve_for_promo;\n+  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+  size_t old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  size_t young_available = young_generation()->available() + young_cset_regions * region_size_bytes;\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+  if (old_available >= old_reserve) {\n+    size_t old_excess = old_available - old_reserve;\n+    size_t excess_regions = old_excess \/ region_size_bytes;\n+    size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n+    size_t unaffiliated_old = unaffiliated_old_regions * region_size_bytes;\n+    if (unaffiliated_old_regions < excess_regions) {\n+      \/\/ We'll give only unaffiliated old to young, which is known to be less than the excess.\n+      old_region_surplus = unaffiliated_old_regions;\n+    } else {\n+      \/\/ unaffiliated_old_regions > excess_regions, so we only give away the excess.\n+      old_region_surplus = excess_regions;\n+    }\n+  } else {\n+    \/\/ We need to request transfer from YOUNG.  Ignore that this will directly impact young_generation()->max_capacity(),\n+    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n+    size_t old_need = old_reserve - old_available;\n+    \/\/ Round up the number of regions needed from YOUNG\n+    old_region_deficit = (old_need + region_size_bytes - 1) \/ region_size_bytes;\n+  }\n+  if (old_region_deficit > max_old_region_xfer) {\n+    \/\/ If we're running short on young-gen memory, limit the xfer.  Old-gen collection activities will be curtailed\n+    \/\/ if the budget is smaller than desired.\n+    old_region_deficit = max_old_region_xfer;\n+  }\n+  set_old_region_surplus(old_region_surplus);\n+  set_old_region_deficit(old_region_deficit);\n+}\n+\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -830,1 +1281,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -843,1 +1294,21 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n+  if (res != nullptr) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  return res;\n+}\n+\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size,\n+                                            size_t word_size,\n+                                            size_t* actual_size) {\n+  \/\/ Align requested sizes to card sized multiples\n+  size_t words_in_card = CardTable::card_size_in_words();\n+  size_t align_mask = ~(words_in_card - 1);\n+  min_size = (min_size + words_in_card - 1) & align_mask;\n+  word_size = (word_size + words_in_card - 1) & align_mask;\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req, false);\n@@ -852,1 +1323,3 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n+\/\/ is_promotion is true iff this allocation is known for sure to hold the result of young-gen evacuation\n+\/\/ to old-gen.  plab allocates are not known as such, since they may hold old-gen evacuations.\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n@@ -864,1 +1337,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -875,2 +1348,1 @@\n-    \/\/ Full GC, which means we want to try more than ShenandoahFullGCThreshold times.\n-\n+    \/\/ Full GC.\n@@ -878,1 +1350,1 @@\n-\n+    size_t original_fullgc_count = shenandoah_policy()->get_fullgc_count();\n@@ -882,1 +1354,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -884,2 +1356,2 @@\n-\n-    while (result == nullptr && tries <= ShenandoahFullGCThreshold) {\n+    while (result == nullptr &&\n+           ((shenandoah_policy()->get_fullgc_count() == original_fullgc_count) || (tries <= ShenandoahOOMGCRetries))) {\n@@ -888,1 +1360,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -890,1 +1362,0 @@\n-\n@@ -893,1 +1364,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -900,0 +1371,5 @@\n+    regulator_thread()->notify_heap_changed();\n+  }\n+\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n@@ -902,0 +1378,4 @@\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -911,2 +1391,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-\n@@ -919,2 +1397,0 @@\n-    } else {\n-      increase_used(actual*HeapWordSize);\n@@ -927,3 +1403,182 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahHeapLocker locker(lock());\n-  return _free_set->allocate(req, in_new_region);\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+  bool try_smaller_lab_size = false;\n+  size_t smaller_lab_size;\n+  {\n+    \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+    bool promotion_eligible = false;\n+    bool allow_allocation = true;\n+    bool plab_alloc = false;\n+    size_t requested_bytes = req.size() * HeapWordSize;\n+    HeapWord* result = nullptr;\n+    ShenandoahHeapLocker locker(lock());\n+    Thread* thread = Thread::current();\n+\n+    if (mode()->is_generational()) {\n+      if (req.affiliation() == YOUNG_GENERATION) {\n+        if (req.is_mutator_alloc()) {\n+          size_t young_words_available = young_generation()->available() \/ HeapWordSize;\n+          if (ShenandoahElasticTLAB && req.is_lab_alloc() && (req.min_size() < young_words_available)) {\n+            \/\/ Allow ourselves to try a smaller lab size even if requested_bytes <= young_available.  We may need a smaller\n+            \/\/ lab size because young memory has become too fragmented.\n+            try_smaller_lab_size = true;\n+            smaller_lab_size = (young_words_available < req.size())? young_words_available: req.size();\n+          } else if (req.size() > young_words_available) {\n+            \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+            log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                               \", young words available: \" SIZE_FORMAT, req.type_string(),\n+                               HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_words_available);\n+            return nullptr;\n+          }\n+        }\n+      } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+        if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+          plab_alloc = true;\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+            if (get_old_evac_reserve() == 0) {\n+              \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+              \/\/ be used for promotions.\n+              allow_allocation = false;\n+            }\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+            promotion_eligible = true;\n+          }\n+        } else if (is_promotion) {\n+          \/\/ This is a shared alloc for promotion\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          }\n+          if (promotion_avail == 0) {\n+            \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+            \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+            return nullptr;\n+          }\n+          \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+        } else {\n+          \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n+        }\n+      }\n+    } \/\/ This ends the is_generational() block\n+\n+    \/\/ First try the original request.  If TLAB request size is greater than available, allocate() will attempt to downsize\n+    \/\/ request to fit within available memory.\n+    result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+    if (result != nullptr) {\n+      if (req.is_old()) {\n+        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+        if (req.is_gc_alloc()) {\n+          bool disable_plab_promotions = false;\n+          if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+            if (promotion_eligible) {\n+              size_t actual_size = req.actual_size() * HeapWordSize;\n+              \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+              \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+              if (get_promoted_expended() + actual_size <= get_promoted_reserve()) {\n+                \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+                \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+                ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+                expend_promoted(actual_size);\n+                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+              } else {\n+                disable_plab_promotions = true;\n+              }\n+            } else {\n+              disable_plab_promotions = true;\n+            }\n+            if (disable_plab_promotions) {\n+              \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+              ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+              ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+            }\n+          } else if (is_promotion) {\n+            \/\/ Shared promotion.  Assume size is requested_bytes.\n+            expend_promoted(requested_bytes);\n+            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+          }\n+        }\n+\n+        \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+        \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+        \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+        \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+        \/\/\n+        \/\/ objects being \"concurrently\" allocated:\n+        \/\/    [-----a------][-----b-----][--------------c------------------]\n+        \/\/            [---- card table memory range --------------]\n+        \/\/\n+        \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+        \/\/   wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+        \/\/   allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+        \/\/   allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+        \/\/   card region.\n+        \/\/\n+        \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+        \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+        \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+        ShenandoahHeap::heap()->card_scan()->register_object(result);\n+      }\n+    } else {\n+      \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+      if (req.is_old() && req.is_gc_alloc() && (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+        \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+        \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+        ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+      }\n+    }\n+    if ((result != nullptr) || !try_smaller_lab_size) {\n+      return result;\n+    }\n+    \/\/ else, fall through to try_smaller_lab_size\n+  } \/\/ This closes the block that holds the heap lock, releasing the lock.\n+\n+  \/\/ We failed to allocate the originally requested lab size.  Let's see if we can allocate a smaller lab size.\n+  if (req.size() == smaller_lab_size) {\n+    \/\/ If we were already trying to allocate min size, no value in attempting to repeat the same.  End the recursion.\n+    return nullptr;\n+  }\n+\n+  \/\/ We arrive here if the tlab allocation request can be resized to fit within young_available\n+  assert((req.affiliation() == YOUNG_GENERATION) && req.is_lab_alloc() && req.is_mutator_alloc() &&\n+         (smaller_lab_size < req.size()), \"Only shrink allocation request size for TLAB allocations\");\n+\n+  \/\/ By convention, ShenandoahAllocationRequest is primarily read-only.  The only mutable instance data is represented by\n+  \/\/ actual_size(), which is overwritten with the size of the allocaion when the allocation request is satisfied.  We use a\n+  \/\/ recursive call here rather than introducing new methods to mutate the existing ShenandoahAllocationRequest argument.\n+  \/\/ Mutation of the existing object might result in astonishing results if calling contexts assume the content of immutable\n+  \/\/ fields remain constant.  The original TLAB allocation request was for memory that exceeded the current capacity.  We'll\n+  \/\/ attempt to allocate a smaller TLAB.  If this is successful, we'll update actual_size() of our incoming\n+  \/\/ ShenandoahAllocRequest.  If the recursive request fails, we'll simply return nullptr.\n+\n+  \/\/ Note that we've relinquished the HeapLock and some other thread may perform additional allocation before our recursive\n+  \/\/ call reacquires the lock.  If that happens, we will need another recursive call to further reduce the size of our request\n+  \/\/ for each time another thread allocates young memory during the brief intervals that the heap lock is available to\n+  \/\/ interfering threads.  We expect this interference to be rare.  The recursion bottoms out when young_available is\n+  \/\/ smaller than req.min_size().  The inner-nested call to allocate_memory_under_lock() uses the same min_size() value\n+  \/\/ as this call, but it uses a preferred size() that is smaller than our preferred size, and is no larger than what we most\n+  \/\/ recently saw as the memory currently available within the young generation.\n+\n+  \/\/ TODO: At the expense of code clarity, we could rewrite this recursive solution to use iteration.  We need at most one\n+  \/\/ extra instance of the ShenandoahAllocRequest, which we can re-initialize multiple times inside a loop, with one iteration\n+  \/\/ of the loop required for each time the existing solution would recurse.  An iterative solution would be more efficient\n+  \/\/ in CPU time and stack memory utilization.  The expectation is that it is very rare that we would recurse more than once\n+  \/\/ so making this change is not currently seen as a high priority.\n+\n+  ShenandoahAllocRequest smaller_req = ShenandoahAllocRequest::for_tlab(req.min_size(), smaller_lab_size);\n+\n+  \/\/ Note that shrinking the preferred size gets us past the gatekeeper that checks whether there's available memory to\n+  \/\/ satisfy the allocation request.  The reality is the actual TLAB size is likely to be even smaller, because it will\n+  \/\/ depend on how much memory is available within mutator regions that are not yet fully used.\n+  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region, is_promotion);\n+  if (result != nullptr) {\n+    req.set_actual_size(smaller_req.actual_size());\n+  }\n+  return result;\n@@ -935,1 +1590,1 @@\n-  return allocate_memory(req);\n+  return allocate_memory(req, false);\n@@ -944,2 +1599,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1024,0 +1679,1 @@\n+\n@@ -1029,0 +1685,83 @@\n+      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n+        break;\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* const _sh;\n+  ShenandoahRegionIterator *_regions;\n+  bool _concurrent;\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent) :\n+    WorkerTask(\"Shenandoah Evacuation\"),\n+    _sh(sh),\n+    _regions(iterator),\n+    _concurrent(concurrent)\n+  {}\n+\n+  void work(uint worker_id) {\n+    if (_concurrent) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    }\n+  }\n+\n+private:\n+  void do_work() {\n+    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n+    ShenandoahHeapRegion* r;\n+    ShenandoahMarkingContext* const ctx = ShenandoahHeap::heap()->marking_context();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n+    while ((r = _regions->next()) != nullptr) {\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n+                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+                    r->is_active()? \"active\": \"inactive\",\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+                    r->is_cset()? \"cset\": \"not-cset\");\n+\n+      if (r->is_cset()) {\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+        _sh->marked_object_iterate(r, &cl);\n+        if (ShenandoahPacing) {\n+          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+        }\n+      } else if (r->is_young() && r->is_active() && (r->age() >= InitialTenuringThreshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        if (r->is_humongous_start()) {\n+          \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+          \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+          \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+          r->promote_humongous();\n+        } else if (r->is_regular() && (r->get_top_before_promote() != nullptr)) {\n+          assert(r->garbage_before_padded_for_promote() < old_garbage_threshold,\n+                 \"Region \" SIZE_FORMAT \" has too much garbage for promotion\", r->index());\n+          assert(r->get_top_before_promote() == tams,\n+                 \"Region \" SIZE_FORMAT \" has been used for allocations before promotion\", r->index());\n+          \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+          \/\/ the LRB to copy on reference fetch.\n+          r->promote_in_place();\n+        }\n+        \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+        \/\/ more garbage than ShenandoahOldGarbageTrheshold, we'll promote by evacuation.  If there is room for evacuation\n+        \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+        \/\/ by evacuation in some future GC cycle.\n+\n+        \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+        \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n+      }\n+      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+      \/\/ or is young humongous_start that is too young to be promoted\n@@ -1038,2 +1777,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1069,1 +1814,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1089,0 +1834,1 @@\n+  return required_regions;\n@@ -1098,0 +1844,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1113,0 +1863,11 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != nullptr, \"PLAB should be initialized for %s\", thread->name());\n+\n+    \/\/ There are two reasons to retire all plabs between old-gen evacuation passes.\n+    \/\/  1. We need to make the plab memory parseable by remembered-set scanning.\n+    \/\/  2. We need to establish a trustworthy UpdateWaterMark value within each old-gen heap region\n+    ShenandoahHeap::heap()->retire_plab(plab, thread);\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n@@ -1170,0 +1931,31 @@\n+class ShenandoahTagGCLABClosure : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) {\n+    PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);\n+    assert(gclab != nullptr, \"GCLAB should be initialized for %s\", thread->name());\n+    if (gclab->words_remaining() > 0) {\n+      ShenandoahHeapRegion* r = ShenandoahHeap::heap()->heap_region_containing(gclab->allocate(0));\n+      r->set_young_lab_flag();\n+    }\n+  }\n+};\n+\n+void ShenandoahHeap::set_young_lab_region_flags() {\n+  if (!UseTLAB) {\n+    return;\n+  }\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    _regions[i]->clear_young_lab_flags();\n+  }\n+  ShenandoahTagGCLABClosure cl;\n+  workers()->threads_do(&cl);\n+  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n+    cl.do_thread(t);\n+    ThreadLocalAllocBuffer& tlab = t->tlab();\n+    if (tlab.end() != nullptr) {\n+      ShenandoahHeapRegion* r = heap_region_containing(tlab.start());\n+      r->set_young_lab_flag();\n+    }\n+  }\n+}\n+\n@@ -1173,3 +1965,7 @@\n-    \/\/ With Elastic TLABs, return the max allowed size, and let the allocation path\n-    \/\/ figure out the safe size for current allocation.\n-    return ShenandoahHeapRegion::max_tlab_size_bytes();\n+    if (mode()->is_generational()) {\n+      return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n+    } else {\n+      \/\/ With Elastic TLABs, return the max allowed size, and let the allocation path\n+      \/\/ figure out the safe size for current allocation.\n+      return ShenandoahHeapRegion::max_tlab_size_bytes();\n+    }\n@@ -1218,0 +2014,4 @@\n+  if (_shenandoah_policy->is_at_shutdown()) {\n+    return;\n+  }\n+\n@@ -1219,0 +2019,1 @@\n+  tcl->do_thread(_regulator_thread);\n@@ -1238,0 +2039,4 @@\n+    ls.cr();\n+\n+    evac_tracker()->print_global_on(&ls);\n+\n@@ -1243,0 +2048,18 @@\n+void ShenandoahHeap::on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+  set_gc_cause(cause);\n+  set_gc_generation(generation);\n+\n+  shenandoah_policy()->record_cycle_start();\n+  generation->heuristics()->record_cycle_start();\n+}\n+\n+void ShenandoahHeap::on_cycle_end(ShenandoahGeneration* generation) {\n+  generation->heuristics()->record_cycle_end();\n+  if (mode()->is_generational() && (generation->is_global() || upgraded_to_full())) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    young_generation()->heuristics()->record_cycle_end();\n+    old_generation()->heuristics()->record_cycle_end();\n+  }\n+  set_gc_cause(GCCause::_no_gc);\n+}\n+\n@@ -1564,23 +2387,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1602,99 +2402,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1711,1 +2412,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1742,4 +2443,48 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state_mask(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  _has_evacuation_reserve_quantities = is_valid;\n+}\n+\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(YOUNG_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(YOUNG_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(OLD_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(OLD_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_prepare_for_old_mark_in_progress(bool in_progress) {\n+  \/\/ Unlike other set-gc-state functions, this may happen outside safepoint.\n+  \/\/ Is only set and queried by control thread, so no coherence issues.\n+  _prepare_for_old_mark = in_progress;\n+}\n+\n+void ShenandoahHeap::set_aging_cycle(bool in_progress) {\n+  _is_aging_cycle.set_cond(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1778,0 +2523,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1783,0 +2536,4 @@\n+    _cancel_requested_time = os::elapsedTime();\n+    if (cause == GCCause::_shenandoah_upgrade_to_full_gc) {\n+      _upgraded_to_full = true;\n+    }\n@@ -1793,1 +2550,1 @@\n-  \/\/ Step 0. Notify policy to disable event recording.\n+  \/\/ Step 1. Notify policy to disable event recording and prevent visiting gc threads during shutdown\n@@ -1796,1 +2553,4 @@\n-  \/\/ Step 1. Notify control thread that we are in shutdown.\n+  \/\/ Step 2. Stop requesting collections.\n+  regulator_thread()->stop();\n+\n+  \/\/ Step 3. Notify control thread that we are in shutdown.\n@@ -1801,1 +2561,1 @@\n-  \/\/ Step 2. Notify GC workers that we are cancelling GC.\n+  \/\/ Step 4. Notify GC workers that we are cancelling GC.\n@@ -1804,1 +2564,1 @@\n-  \/\/ Step 3. Wait until GC worker exits normally.\n+  \/\/ Step 5. Wait until GC worker exits normally.\n@@ -1898,4 +2658,0 @@\n-address ShenandoahHeap::cancelled_gc_addr() {\n-  return (address) ShenandoahHeap::heap()->_cancelled_gc.addr_of();\n-}\n-\n@@ -1906,5 +2662,6 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() const {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -1975,2 +2732,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2036,0 +2795,2 @@\n+  ShenandoahRegionChunkIterator* _work_chunks;\n+\n@@ -2037,1 +2798,2 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  explicit ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions,\n+                                        ShenandoahRegionChunkIterator* work_chunks) :\n@@ -2040,1 +2802,3 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _work_chunks(work_chunks)\n+  {\n@@ -2047,1 +2811,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2050,1 +2814,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2056,1 +2820,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2058,0 +2822,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n@@ -2059,1 +2833,4 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2063,0 +2840,3 @@\n+\n+      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n+      bool region_progress = false;\n@@ -2064,1 +2844,31 @@\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (!_heap->mode()->is_generational() || r->is_young()) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          region_progress = true;\n+        } else if (r->is_old()) {\n+          if (_heap->active_generation()->is_global()) {\n+            \/\/ Note that GLOBAL collection is not as effectively balanced as young and mixed cycles.  This is because\n+            \/\/ concurrent GC threads are parceled out entire heap regions of work at a time and there\n+            \/\/ is no \"catchup phase\" consisting of remembered set scanning, during which parcels of work are smaller\n+            \/\/ and more easily distributed more fairly across threads.\n+\n+            \/\/ TODO: Consider an improvement to load balance GLOBAL GC.\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+            region_progress = true;\n+          }\n+          \/\/ Otherwise, this is an old region in a young or mixed cycle.  Process it during a second phase, below.\n+          \/\/ Don't bother to report pacing progress in this case.\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 r->affiliation_name(), r->index());\n+        }\n@@ -2066,1 +2876,1 @@\n-      if (ShenandoahPacing) {\n+      if (region_progress && ShenandoahPacing) {\n@@ -2074,0 +2884,114 @@\n+\n+    if (_heap->mode()->is_generational() && !_heap->active_generation()->is_global()) {\n+      \/\/ Since this is generational and not GLOBAL, we have to process the remembered set.  There's no remembered\n+      \/\/ set processing if not in generational mode or if GLOBAL mode.\n+\n+      \/\/ After this thread has exhausted its traditional update-refs work, it continues with updating refs within remembered set.\n+      \/\/ The remembered set workload is better balanced between threads, so threads that are \"behind\" can catch up with other\n+      \/\/ threads during this phase, allowing all threads to work more effectively in parallel.\n+      struct ShenandoahRegionChunk assignment;\n+      RememberedScanner* scanner = _heap->card_scan();\n+\n+      while (!_heap->check_cancelled_gc_and_yield(CONCURRENT) && _work_chunks->next(&assignment)) {\n+        \/\/ Keep grabbing next work chunk to process until finished, or asked to yield\n+        ShenandoahHeapRegion* r = assignment._r;\n+        if (r->is_active() && !r->is_cset() && r->is_old()) {\n+          HeapWord* start_of_range = r->bottom() + assignment._chunk_offset;\n+          HeapWord* end_of_range = r->get_update_watermark();\n+          if (end_of_range > start_of_range + assignment._chunk_size) {\n+            end_of_range = start_of_range + assignment._chunk_size;\n+          }\n+\n+          \/\/ Old region in a young cycle or mixed cycle.\n+          if (is_mixed) {\n+            \/\/ TODO: For mixed evac, consider building an old-gen remembered set that allows restricted updating\n+            \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+            \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+            \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+            \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+            \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+            \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+            \/\/ old-gen heap regions.\n+\n+            if (r->is_humongous()) {\n+              if (start_of_range < end_of_range) {\n+                \/\/ Need to examine both dirty and clean cards during mixed evac.\n+                r->oop_iterate_humongous_slice(&cl, false, start_of_range, assignment._chunk_size, true);\n+              }\n+            } else {\n+              \/\/ Since this is mixed evacuation, old regions that are candidates for collection have not been coalesced\n+              \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+              \/\/\n+              \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+              \/\/ regions which are in the collection set for a particular mixed evacuation.\n+              if (start_of_range < end_of_range) {\n+                HeapWord* p = nullptr;\n+                size_t card_index = scanner->card_index_for_addr(start_of_range);\n+                \/\/ In case last object in my range spans boundary of my chunk, I may need to scan all the way to top()\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, start_of_range, r->top());\n+\n+                \/\/ Any object that begins in a previous range is part of a different scanning assignment.  Any object that\n+                \/\/ starts after end_of_range is also not my responsibility.  (Either allocated during evacuation, so does\n+                \/\/ not hold pointers to from-space, or is beyond the range of my assigned work chunk.)\n+\n+                \/\/ Find the first object that begins in my range, if there is one.\n+                p = start_of_range;\n+                oop obj = cast_to_oop(p);\n+                HeapWord* tams = ctx->top_at_mark_start(r);\n+                if (p >= tams) {\n+                  \/\/ We cannot use ctx->is_marked(obj) to test whether an object begins at this address.  Instead,\n+                  \/\/ we need to use the remembered set crossing map to advance p to the first object that starts\n+                  \/\/ within the enclosing card.\n+\n+                  while (true) {\n+                    HeapWord* first_object = scanner->first_object_in_card(card_index);\n+                    if (first_object != nullptr) {\n+                      p = first_object;\n+                      break;\n+                    } else if (scanner->addr_for_card_index(card_index + 1) < end_of_range) {\n+                      card_index++;\n+                    } else {\n+                      \/\/ Force the loop that follows to immediately terminate.\n+                      p = end_of_range;\n+                      break;\n+                    }\n+                  }\n+                  obj = cast_to_oop(p);\n+                  \/\/ Note: p may be >= end_of_range\n+                } else if (!ctx->is_marked(obj)) {\n+                  p = ctx->get_next_marked_addr(p, tams);\n+                  obj = cast_to_oop(p);\n+                  \/\/ If there are no more marked objects before tams, this returns tams.\n+                  \/\/ Note that tams is either >= end_of_range, or tams is the start of an object that is marked.\n+                }\n+                while (p < end_of_range) {\n+                  \/\/ p is known to point to the beginning of marked object obj\n+                  objs.do_object(obj);\n+                  HeapWord* prev_p = p;\n+                  p += obj->size();\n+                  if (p < tams) {\n+                    p = ctx->get_next_marked_addr(p, tams);\n+                    \/\/ If there are no more marked objects before tams, this returns tams.  Note that tams is\n+                    \/\/ either >= end_of_range, or tams is the start of an object that is marked.\n+                  }\n+                  assert(p != prev_p, \"Lack of forward progress\");\n+                  obj = cast_to_oop(p);\n+                }\n+              }\n+            }\n+          } else {\n+            \/\/ This is a young evac..\n+            if (start_of_range < end_of_range) {\n+              size_t cluster_size =\n+                CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+              size_t clusters = assignment._chunk_size \/ cluster_size;\n+              assert(clusters * cluster_size == assignment._chunk_size, \"Chunk assignment must align on cluster boundaries\");\n+              scanner->process_region_slice(r, assignment._chunk_offset, clusters, end_of_range, &cl, true, worker_id);\n+            }\n+          }\n+          if (ShenandoahPacing && (start_of_range < end_of_range)) {\n+            _heap->pacer()->report_updaterefs(pointer_delta(end_of_range, start_of_range));\n+          }\n+        }\n+      }\n+    }\n@@ -2079,0 +3003,2 @@\n+  uint nworkers = workers()->active_workers();\n+  ShenandoahRegionChunkIterator work_list(nworkers);\n@@ -2081,1 +3007,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, &work_list);\n@@ -2084,1 +3010,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, &work_list);\n@@ -2087,0 +3013,3 @@\n+  if (ShenandoahEnableCardStats && card_scan()!=nullptr) { \/\/ generational check proxy\n+    card_scan()->log_card_stats(nworkers, CARD_STAT_UPDATE_REFS);\n+  }\n@@ -2089,1 +3018,0 @@\n-\n@@ -2092,0 +3020,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2093,0 +3022,1 @@\n+  bool _is_generational;\n@@ -2095,1 +3025,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n+    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -2098,0 +3030,21 @@\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young() && r->is_active()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+\n@@ -2100,1 +3053,0 @@\n-\n@@ -2127,1 +3079,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (active_generation()->complete_marking_context());\n@@ -2142,6 +3094,52 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational()) {\n+    assert(verify_generation_usage(true, old_generation()->used_regions(),\n+                                   old_generation()->used(), old_generation()->get_humongous_waste(),\n+                                   true, young_generation()->used_regions(),\n+                                   young_generation()->used(), young_generation()->get_humongous_waste()),\n+           \"Generation accounts are inaccurate\");\n+\n+    \/\/ The computation of bytes_of_allocation_runway_before_gc_trigger is quite conservative so consider all of this\n+    \/\/ available for transfer to old. Note that transfer of humongous regions does not impact available.\n+    size_t allocation_runway = young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    adjust_generation_sizes_for_next_cycle(allocation_runway, young_cset_regions, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational()) {\n+    size_t old_available = old_generation()->available();\n+    size_t old_unaffiliated_available = old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    size_t old_fragmented_available;\n+    assert(old_available >= old_unaffiliated_available, \"unaffiliated available is a subset of total available\");\n+    old_fragmented_available = old_available - old_unaffiliated_available;\n+\n+    size_t old_capacity = old_generation()->max_capacity();\n+    size_t heap_capacity = capacity();\n+    if ((old_capacity > heap_capacity \/ 8) && (old_fragmented_available > old_capacity \/ 8)) {\n+      old_heuristics()->trigger_old_is_fragmented();\n+    }\n+\n+    size_t old_used = old_generation()->used() + old_generation()->get_humongous_waste();\n+    size_t trigger_threshold = old_generation()->usage_trigger_threshold();\n+    \/\/ Detects unsigned arithmetic underflow\n+    assert(old_used < ShenandoahHeap::heap()->capacity(), \"Old used must be less than heap capacity\");\n+\n+    if (old_used > trigger_threshold) {\n+      old_heuristics()->trigger_old_has_grown();\n+    }\n@@ -2262,3 +3260,12 @@\n-  _memory_pool = new ShenandoahMemoryPool(this);\n-  _cycle_memory_manager.add_pool(_memory_pool);\n-  _stw_memory_manager.add_pool(_memory_pool);\n+  if (mode()->is_generational()) {\n+    _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+    _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_young_gen_memory_pool);\n+    _cycle_memory_manager.add_pool(_old_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_young_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_old_gen_memory_pool);\n+  } else {\n+    _memory_pool = new ShenandoahMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_memory_pool);\n+    _stw_memory_manager.add_pool(_memory_pool);\n+  }\n@@ -2276,1 +3283,6 @@\n-  memory_pools.append(_memory_pool);\n+  if (mode()->is_generational()) {\n+    memory_pools.append(_young_gen_memory_pool);\n+    memory_pools.append(_old_gen_memory_pool);\n+  } else {\n+    memory_pools.append(_memory_pool);\n+  }\n@@ -2281,1 +3293,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2319,0 +3331,1 @@\n+\n@@ -2346,0 +3359,104 @@\n+\n+void ShenandoahHeap::transfer_old_pointers_from_satb() {\n+  _old_generation->transfer_pointers_from_satb();\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<YOUNG>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit young and free regions\n+  if (!region->is_old()) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<OLD>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit old and free regions\n+  if (!region->is_young()) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL_NON_GEN>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+bool ShenandoahHeap::verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                                             bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste) {\n+  size_t tally_old_regions = 0;\n+  size_t tally_old_bytes = 0;\n+  size_t tally_old_waste = 0;\n+  size_t tally_young_regions = 0;\n+  size_t tally_young_bytes = 0;\n+  size_t tally_young_waste = 0;\n+\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_old()) {\n+      tally_old_regions++;\n+      tally_old_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_old_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    } else if (r->is_young()) {\n+      tally_young_regions++;\n+      tally_young_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_young_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    }\n+  }\n+  if (verify_young &&\n+      ((young_regions != tally_young_regions) || (young_bytes != tally_young_bytes) || (young_waste != tally_young_waste))) {\n+    return false;\n+  } else if (verify_old &&\n+             ((old_regions != tally_old_regions) || (old_bytes != tally_old_bytes) || (old_waste != tally_old_waste))) {\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n+\n+void ShenandoahHeap::log_heap_status(const char* msg) const {\n+  if (mode()->is_generational()) {\n+    young_generation()->log_status(msg);\n+    old_generation()->log_status(msg);\n+  } else {\n+    global_generation()->log_status(msg);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":1416,"deletions":299,"binary":false,"changes":1715,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -37,0 +38,3 @@\n+#include \"gc\/shenandoah\/shenandoahEvacTracker.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGenerationType.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMmuTracker.hpp\"\n@@ -40,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -47,0 +52,1 @@\n+class PLAB;\n@@ -49,0 +55,1 @@\n+class ShenandoahRegulatorThread;\n@@ -51,0 +58,3 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n+class ShenandoahOldGeneration;\n@@ -52,0 +62,2 @@\n+class ShenandoahOldHeuristics;\n+class ShenandoahYoungHeuristics;\n@@ -53,1 +65,0 @@\n-class ShenandoahMode;\n@@ -63,0 +74,1 @@\n+class ShenandoahMode;\n@@ -112,0 +124,10 @@\n+template<ShenandoahGenerationType GENERATION>\n+class ShenandoahGenerationRegionClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  explicit ShenandoahGenerationRegionClosure(ShenandoahHeapRegionClosure* cl) : _cl(cl) {}\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+  virtual bool is_thread_safe() { return _cl->is_thread_safe(); }\n+ private:\n+  ShenandoahHeapRegionClosure* _cl;\n+};\n+\n@@ -120,1 +142,1 @@\n-class ShenandoahHeap : public CollectedHeap, public ShenandoahSpaceInfo {\n+class ShenandoahHeap : public CollectedHeap {\n@@ -129,0 +151,1 @@\n+  friend class ShenandoahOldGC;\n@@ -137,0 +160,4 @@\n+  ShenandoahGeneration* _gc_generation;\n+\n+  \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+  bool _prepare_for_old_mark;\n@@ -143,0 +170,16 @@\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  ShenandoahOldHeuristics* old_heuristics();\n+  ShenandoahYoungHeuristics* young_heuristics();\n+\n+  bool doing_mixed_evacuations();\n+  bool is_old_bitmap_stable() const;\n+  bool is_gc_generation_young() const;\n+\n@@ -154,2 +197,1 @@\n-  void initialize_mode();\n-  void initialize_heuristics();\n+  void initialize_heuristics_generations();\n@@ -169,0 +211,3 @@\n+  bool verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                               bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste);\n+\n@@ -179,0 +224,8 @@\n+           size_t _promotion_potential;\n+           size_t _promotion_in_place_potential;\n+           size_t _pad_for_promote_in_place;    \/\/ bytes of filler\n+           size_t _promotable_humongous_regions;\n+           size_t _promotable_humongous_usage;\n+           size_t _regular_regions_promoted_in_place;\n+           size_t _regular_usage_promoted_in_place;\n+\n@@ -181,2 +234,0 @@\n-  volatile size_t _used;\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -186,0 +237,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -187,3 +240,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -193,2 +247,0 @@\n-  void increase_allocated(size_t bytes);\n-  size_t bytes_allocated_since_gc_start() const override;\n@@ -200,1 +252,1 @@\n-  size_t soft_max_capacity() const override;\n+  size_t soft_max_capacity() const;\n@@ -205,1 +257,0 @@\n-  size_t available()         const override;\n@@ -232,0 +283,1 @@\n+  uint8_t* _affiliations;       \/\/ Holds array of enum ShenandoahAffiliation, including FREE status in non-generational mode\n@@ -249,0 +301,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -263,2 +317,2 @@\n-    \/\/ Heap is under marking: needs SATB barriers.\n-    MARKING_BITPOS    = 1,\n+    \/\/ Young regions are under marking: needs SATB barriers.\n+    YOUNG_MARKING_BITPOS    = 1,\n@@ -274,0 +328,3 @@\n+\n+    \/\/ Old regions are under marking, still need SATB barriers.\n+    OLD_MARKING_BITPOS = 5\n@@ -279,1 +336,1 @@\n-    MARKING       = 1 << MARKING_BITPOS,\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n@@ -283,0 +340,1 @@\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -293,0 +351,43 @@\n+  \/\/ TODO: Revisit the following comment.  It may not accurately represent the true behavior when evacuations fail due to\n+  \/\/ difficulty finding memory to hold evacuated objects.\n+  \/\/\n+  \/\/ Note that the typical total expenditure on evacuation is less than the associated evacuation reserve because we generally\n+  \/\/ reserve ShenandoahEvacWaste (> 1.0) times the anticipated evacuation need.  In the case that there is an excessive amount\n+  \/\/ of waste, it may be that one thread fails to grab a new GCLAB, this does not necessarily doom the associated evacuation\n+  \/\/ effort.  If this happens, the requesting thread blocks until some other thread manages to evacuate the offending object.\n+  \/\/ Only after \"all\" threads fail to evacuate an object do we consider the evacuation effort to have failed.\n+\n+  \/\/ How many full-gc cycles have been completed?\n+  volatile size_t _completed_fullgc_cycles;\n+\n+  size_t _promoted_reserve;            \/\/ Bytes reserved within old-gen to hold the results of promotion\n+  volatile size_t _promoted_expended;  \/\/ Bytes of old-gen memory expended on promotions\n+\n+  \/\/ Allocation of old GCLABs (aka PLABs) assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n+  \/\/  is authorized, increment _old_evac_expended by request size.  This allocation ignores old_gen->available().\n+\n+  size_t _old_evac_reserve;            \/\/ Bytes reserved within old-gen to hold evacuated objects from old-gen collection set\n+  volatile size_t _old_evac_expended;  \/\/ Bytes of old-gen memory expended on old-gen evacuations\n+\n+  size_t _young_evac_reserve;          \/\/ Bytes reserved within young-gen to hold evacuated objects from young-gen collection set\n+\n+  size_t _captured_old_usage;          \/\/ What was old usage (bytes) when last captured?\n+\n+  size_t _previous_promotion;          \/\/ Bytes promoted during previous evacuation\n+\n+  bool _upgraded_to_full;\n+\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantitites, stored in _promoted_reserve,\n+  \/\/ _old_evac_reserve, and _young_evac_reserve, are consulted prior to rebuilding the free set (ShenandoahFreeSet)\n+  \/\/ in preparation for evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the\n+  \/\/ collector and old_collector sets to hold if _has_evacuation_reserve_quantities is true.  The other time we\n+  \/\/ rebuild the freeset is at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ _has_evacuation_reserve_quantities is false because we don't yet know how much memory will need to be evacuated\n+  \/\/ in the next GC cycle.  When _has_evacuation_reserve_quantities is false, the free set rebuild operation reserves\n+  \/\/ for the collector and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve,\n+  \/\/ ShenandoahOldEvacReserve, and ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve\n+  \/\/ for old_collector set when not _has_evacuation_reserve_quantities is based in part on anticipated promotion as\n+  \/\/ determined by analysis of live data found during the previous GC pass which is one less than the current tenure age.\n+  bool _has_evacuation_reserve_quantities;\n+\n@@ -300,1 +401,3 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_evacuation_reserve_quantities(bool is_valid);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -309,0 +412,3 @@\n+  void set_prepare_for_old_mark_in_progress(bool cond);\n+  void set_aging_cycle(bool cond);\n+\n@@ -312,0 +418,1 @@\n+  inline bool has_evacuation_reserve_quantities() const;\n@@ -313,0 +420,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -323,0 +432,53 @@\n+  inline bool is_prepare_for_old_mark_in_progress() const;\n+  inline bool is_aging_cycle() const;\n+  inline bool upgraded_to_full() { return _upgraded_to_full; }\n+  inline void start_conc_gc() { _upgraded_to_full = false; }\n+  inline void record_upgrade_to_full() { _upgraded_to_full = true; }\n+\n+  inline size_t capture_old_usage(size_t usage);\n+  inline void set_previous_promotion(size_t promoted_bytes);\n+  inline size_t get_previous_promotion() const;\n+\n+  inline void clear_promotion_potential() { _promotion_potential = 0; };\n+  inline void set_promotion_potential(size_t val) { _promotion_potential = val; };\n+  inline size_t get_promotion_potential() { return _promotion_potential; };\n+\n+  inline void clear_promotion_in_place_potential() { _promotion_in_place_potential = 0; };\n+  inline void set_promotion_in_place_potential(size_t val) { _promotion_in_place_potential = val; };\n+  inline size_t get_promotion_in_place_potential() { return _promotion_in_place_potential; };\n+\n+  inline void set_pad_for_promote_in_place(size_t pad) { _pad_for_promote_in_place = pad; }\n+  inline size_t get_pad_for_promote_in_place() { return _pad_for_promote_in_place; }\n+\n+  inline void reserve_promotable_humongous_regions(size_t region_count) { _promotable_humongous_regions = region_count; }\n+  inline void reserve_promotable_humongous_usage(size_t bytes) { _promotable_humongous_usage = bytes; }\n+  inline void reserve_promotable_regular_regions(size_t region_count) { _regular_regions_promoted_in_place = region_count; }\n+  inline void reserve_promotable_regular_usage(size_t used_bytes) { _regular_usage_promoted_in_place = used_bytes; }\n+\n+  inline size_t get_promotable_humongous_regions() { return _promotable_humongous_regions; }\n+  inline size_t get_promotable_humongous_usage() { return _promotable_humongous_usage; }\n+  inline size_t get_regular_regions_promoted_in_place() { return _regular_regions_promoted_in_place; }\n+  inline size_t get_regular_usage_promoted_in_place() { return _regular_usage_promoted_in_place; }\n+\n+  \/\/ Returns previous value\n+  inline size_t set_promoted_reserve(size_t new_val);\n+  inline size_t get_promoted_reserve() const;\n+  inline void augment_promo_reserve(size_t increment);\n+\n+  inline void reset_promoted_expended();\n+  inline size_t expend_promoted(size_t increment);\n+  inline size_t unexpend_promoted(size_t decrement);\n+  inline size_t get_promoted_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_old_evac_reserve(size_t new_val);\n+  inline size_t get_old_evac_reserve() const;\n+  inline void augment_old_evac_reserve(size_t increment);\n+\n+  inline void reset_old_evac_expended();\n+  inline size_t expend_old_evac(size_t increment);\n+  inline size_t get_old_evac_expended();\n+\n+  \/\/ Returns previous value\n+  inline size_t set_young_evac_reserve(size_t new_val);\n+  inline size_t get_young_evac_reserve() const;\n@@ -325,0 +487,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -335,0 +499,1 @@\n+  double _cancel_requested_time;\n@@ -336,0 +501,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -339,2 +509,0 @@\n-  static address cancelled_gc_addr();\n-\n@@ -344,1 +512,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -346,0 +514,1 @@\n+  void cancel_concurrent_mark();\n@@ -355,4 +524,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n-  void prepare_evacuation(bool concurrent);\n@@ -371,1 +536,0 @@\n-  void rebuild_free_set(bool concurrent);\n@@ -376,0 +540,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -382,0 +547,4 @@\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahGeneration*      _global_generation;\n+  ShenandoahOldGeneration*   _old_generation;\n+\n@@ -383,0 +552,1 @@\n+  ShenandoahRegulatorThread* _regulator_thread;\n@@ -385,1 +555,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -390,1 +559,4 @@\n-  ShenandoahPhaseTimings*    _phase_timings;\n+  ShenandoahPhaseTimings*       _phase_timings;\n+  ShenandoahEvacuationTracker*  _evac_tracker;\n+  ShenandoahMmuTracker          _mmu_tracker;\n+  ShenandoahGenerationSizer     _generation_sizer;\n@@ -392,1 +564,1 @@\n-  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n@@ -395,0 +567,10 @@\n+  ShenandoahControlThread*   control_thread()          { return _control_thread;    }\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahOldGeneration*   old_generation()    const { return _old_generation;    }\n+  ShenandoahGeneration*      generation_for(ShenandoahAffiliation affiliation) const;\n+  const ShenandoahGenerationSizer* generation_sizer()  const { return &_generation_sizer;  }\n+\n+  size_t max_size_for(ShenandoahGeneration* generation) const;\n+  size_t min_size_for(ShenandoahGeneration* generation) const;\n+\n@@ -397,1 +579,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -401,1 +582,5 @@\n-  ShenandoahPhaseTimings*    phase_timings()     const { return _phase_timings;     }\n+  ShenandoahPhaseTimings*      phase_timings()   const { return _phase_timings;     }\n+  ShenandoahEvacuationTracker* evac_tracker()    const { return  _evac_tracker;     }\n+\n+  void on_cycle_start(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void on_cycle_end(ShenandoahGeneration* generation);\n@@ -410,0 +595,3 @@\n+  MemoryPool*                  _young_gen_memory_pool;\n+  MemoryPool*                  _old_gen_memory_pool;\n+\n@@ -418,1 +606,1 @@\n-  ShenandoahMonitoringSupport* monitoring_support()          { return _monitoring_support;    }\n+  ShenandoahMonitoringSupport* monitoring_support() const    { return _monitoring_support;    }\n@@ -429,8 +617,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -440,0 +620,1 @@\n+  ShenandoahSharedFlag  _is_aging_cycle;\n@@ -455,0 +636,3 @@\n+  inline void assert_lock_for_affiliation(ShenandoahAffiliation orig_affiliation,\n+                                          ShenandoahAffiliation new_affiliation);\n+\n@@ -468,1 +652,12 @@\n-  bool is_in(const void* p) const override;\n+  inline bool is_in(const void* p) const override;\n+\n+  inline bool is_in_active_generation(oop obj) const;\n+  inline bool is_in_young(const void* p) const;\n+  inline bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n+  inline ShenandoahAffiliation region_affiliation(const ShenandoahHeapRegion* r);\n+  inline void set_affiliation(ShenandoahHeapRegion* r, ShenandoahAffiliation new_affiliation);\n+\n+  inline ShenandoahAffiliation region_affiliation(size_t index);\n+  inline void set_affiliation(size_t index, ShenandoahAffiliation new_affiliation);\n@@ -522,1 +717,6 @@\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n+  \/\/ How many bytes to transfer between old and young after we have finished recycling collection set regions?\n+  size_t _old_regions_surplus;\n+  size_t _old_regions_deficit;\n+\n+  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region, bool is_promotion);\n+\n@@ -527,0 +727,4 @@\n+  inline HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n@@ -528,1 +732,1 @@\n-  HeapWord* allocate_memory(ShenandoahAllocRequest& request);\n+  HeapWord* allocate_memory(ShenandoahAllocRequest& request, bool is_promotion);\n@@ -534,1 +738,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n@@ -548,0 +752,8 @@\n+  void set_young_lab_region_flags();\n+\n+  inline void set_old_region_surplus(size_t surplus) { _old_regions_surplus = surplus; };\n+  inline void set_old_region_deficit(size_t deficit) { _old_regions_deficit = deficit; };\n+\n+  inline size_t get_old_region_surplus() { return _old_regions_surplus; };\n+  inline size_t get_old_region_deficit() { return _old_regions_deficit; };\n+\n@@ -572,2 +784,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -584,2 +794,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -605,0 +813,5 @@\n+  ShenandoahSharedFlag _old_gen_oom_evac;\n+\n+  inline oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahAffiliation target_gen);\n+  void handle_old_evacuation(HeapWord* obj, size_t words, bool promotion);\n+  void handle_old_evacuation_failure();\n@@ -607,0 +820,2 @@\n+  void report_promotion_failure(Thread* thread, size_t size);\n+\n@@ -617,1 +832,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -625,0 +840,20 @@\n+  inline bool clear_old_evacuation_failure();\n+\n+\/\/ ---------- Generational support\n+\/\/\n+private:\n+  RememberedScanner* _card_scan;\n+\n+public:\n+  inline RememberedScanner* card_scan() { return _card_scan; }\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+  void dirty_cards(HeapWord* start, HeapWord* end);\n+  void clear_cards(HeapWord* start, HeapWord* end);\n+  void mark_card_as_dirty(void* location);\n+  void retire_plab(PLAB* plab);\n+  void retire_plab(PLAB* plab, Thread* thread);\n+  void cancel_old_gc();\n+  bool is_old_gc_active();\n+  void coalesce_and_fill_old_regions();\n+  void adjust_generation_sizes_for_next_cycle(size_t old_xfer_limit, size_t young_cset_regions, size_t old_cset_regions);\n+\n@@ -646,1 +881,8 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+  static inline uint get_object_age(oop obj);\n+\n+  void transfer_old_pointers_from_satb();\n+\n+  void log_heap_status(const char *msg) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":291,"deletions":49,"binary":false,"changes":340,"status":"modified"}]}