{"files":[{"patch":"@@ -2,1 +2,1 @@\n-project=jdk\n+project=shenandoah\n@@ -7,1 +7,1 @@\n-error=author,committer,reviewers,merge,issues,executable,symlink,message,hg-tag,whitespace,problemlists\n+error=author,committer,reviewers,executable,symlink,message,hg-tag,whitespace,problemlists\n@@ -21,4 +21,1 @@\n-[checks \"merge\"]\n-message=Merge\n-\n-reviewers=1\n+committers=1\n@@ -31,3 +28,0 @@\n-[checks \"issues\"]\n-pattern=^([124-8][0-9]{6}): (\\S.*)$\n-\n","filename":".jcheck\/conf","additions":3,"deletions":9,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -63,1 +64,1 @@\n-        __ mov(rscratch2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING);\n+        __ mov(rscratch2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -80,0 +81,7 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                       Register start, Register count, Register tmp, RegSet saved_regs) {\n+  if (is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n@@ -376,0 +384,25 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+\n+  __ lsr(obj, obj, CardTable::card_shift());\n+\n+  assert(CardTable::dirty_card_val() == 0, \"must be\");\n+\n+  __ load_byte_map_base(rscratch1);\n+\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ ldrb(rscratch2,  Address(obj, rscratch1));\n+    __ cbz(rscratch2, L_already_dirty);\n+    __ strb(zr, Address(obj, rscratch1));\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ strb(zr, Address(obj, rscratch1));\n+  }\n+}\n+\n@@ -412,0 +445,1 @@\n+    store_check(masm, r3);\n@@ -596,0 +630,29 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register start, Register count, Register scratch, RegSet saved_regs) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+\n+  __ cbz(count, L_done); \/\/ zero count - nothing to do\n+\n+  __ lea(end, Address(start, count, Address::lsl(LogBytesPerHeapOop))); \/\/ end = start + count << LogBytesPerHeapOop\n+  __ sub(end, end, BytesPerHeapOop); \/\/ last element address to make inclusive\n+  __ lsr(start, start, CardTable::card_shift());\n+  __ lsr(end, end, CardTable::card_shift());\n+  __ sub(count, end, start); \/\/ number of bytes to copy\n+\n+  __ load_byte_map_base(scratch);\n+  __ add(start, start, scratch);\n+  __ bind(L_loop);\n+  __ strb(zr, Address(start, count));\n+  __ subs(count, count, 1);\n+  __ br(Assembler::GE, L_loop);\n+  __ bind(L_done);\n+}\n+\n@@ -696,1 +759,7 @@\n-  __ tbz(tmp, ShenandoahHeap::MARKING_BITPOS, done);\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    __ tbz(tmp, ShenandoahHeap::YOUNG_MARKING_BITPOS, done);\n+  } else {\n+    __ mov(rscratch2, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n+    __ tst(tmp, rscratch2);\n+    __ br(Assembler::EQ, done);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":71,"deletions":2,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -123,0 +124,24 @@\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+      bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+      bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+\n+      \/\/ We need to squirrel away the original element count because the\n+      \/\/ array copy assembly will destroy the value and we need it for the\n+      \/\/ card marking barrier.\n+#ifdef _LP64\n+      if (!checkcast) {\n+        if (!obj_int) {\n+          \/\/ Save count for barrier\n+          __ movptr(r11, count);\n+        } else if (disjoint) {\n+          \/\/ Save dst in r11 in the disjoint case\n+          __ movq(r11, dst);\n+        }\n+      }\n+#else\n+if (disjoint) {\n+        __ mov(rdx, dst);          \/\/ save 'to'\n+      }\n+#endif\n+    }\n@@ -154,1 +179,1 @@\n-        flags = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING;\n+        flags = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING;\n@@ -184,0 +209,29 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register src, Register dst, Register count) {\n+  bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+  bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+  bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+  Register tmp = rax;\n+\n+if (is_reference_type(type)) {\n+#ifdef _LP64\n+    if (!checkcast) {\n+      if (!obj_int) {\n+        \/\/ Save count for barrier\n+        count = r11;\n+      } else if (disjoint) {\n+        \/\/ Use the saved dst in the disjoint case\n+        dst = r11;\n+      }\n+    } else {\n+      tmp = rscratch1;\n+    }\n+#else\n+    if (disjoint) {\n+      __ mov(dst, rdx); \/\/ restore 'to'\n+    }\n+#endif\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, tmp);\n+  }\n+}\n+\n@@ -227,1 +281,1 @@\n-  __ testb(gc_state, ShenandoahHeap::MARKING);\n+  __ testb(gc_state, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -593,0 +647,45 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  \/\/ Does a store check for the oop in register obj. The content of\n+  \/\/ register obj is destroyed afterwards.\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+\n+  __ shrptr(obj, CardTable::card_shift());\n+\n+  Address card_addr;\n+\n+  \/\/ The calculation for byte_map_base is as follows:\n+  \/\/ byte_map_base = _byte_map - (uintptr_t(low_bound) >> card_shift);\n+  \/\/ So this essentially converts an address to a displacement and it will\n+  \/\/ never need to be relocated. On 64bit however the value may be too\n+  \/\/ large for a 32bit displacement.\n+  intptr_t byte_map_base = (intptr_t)ct->byte_map_base();\n+  if (__ is_simm32(byte_map_base)) {\n+    card_addr = Address(noreg, obj, Address::times_1, byte_map_base);\n+  } else {\n+    \/\/ By doing it as an ExternalAddress 'byte_map_base' could be converted to a rip-relative\n+    \/\/ displacement and done in a single instruction given favorable mapping and a\n+    \/\/ smarter version of as_Address. However, 'ExternalAddress' generates a relocation\n+    \/\/ entry and that entry is not properly handled by the relocation code.\n+    AddressLiteral cardtable((address)byte_map_base, relocInfo::none);\n+    Address index(noreg, obj, Address::times_1);\n+    card_addr = __ as_Address(ArrayAddress(cardtable, index));\n+  }\n+\n+  int dirty = CardTable::dirty_card_val();\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ cmpb(card_addr, dirty);\n+    __ jcc(Assembler::equal, L_already_dirty);\n+    __ movb(card_addr, dirty);\n+    __ bind(L_already_dirty);\n+  } else {\n+    __ movb(card_addr, dirty);\n+  }\n+}\n+\n@@ -594,1 +693,1 @@\n-              Address dst, Register val, Register tmp1, Register tmp2) {\n+                                             Address dst, Register val, Register tmp1, Register tmp2) {\n@@ -636,0 +735,1 @@\n+      store_check(masm, tmp1);\n@@ -831,0 +931,55 @@\n+#ifdef PRODUCT\n+#define BLOCK_COMMENT(str) \/* nothing *\/\n+#else\n+#define BLOCK_COMMENT(str) __ block_comment(str)\n+#endif\n+\n+#define BIND(label) bind(label); BLOCK_COMMENT(#label \":\")\n+\n+#define TIMES_OOP (UseCompressedOops ? Address::times_4 : Address::times_8)\n+\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators, Register addr, Register count, Register tmp) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  intptr_t disp = (intptr_t) ct->byte_map_base();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+  assert_different_registers(addr, end);\n+\n+  __ testl(count, count);\n+  __ jcc(Assembler::zero, L_done); \/\/ zero count - nothing to do\n+\n+\n+#ifdef _LP64\n+  __ leaq(end, Address(addr, count, TIMES_OOP, 0));  \/\/ end == addr+count*oop_size\n+  __ subptr(end, BytesPerHeapOop); \/\/ end - 1 to make inclusive\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end, CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> cards count\n+\n+  __ mov64(tmp, disp);\n+  __ addptr(addr, tmp);\n+__ BIND(L_loop);\n+  __ movb(Address(addr, count, Address::times_1), 0);\n+  __ decrement(count);\n+  __ jcc(Assembler::greaterEqual, L_loop);\n+#else\n+  __ lea(end,  Address(addr, count, Address::times_ptr, -wordSize));\n+  __ shrptr(addr, CardTable::card_shift());\n+  __ shrptr(end,   CardTable::card_shift());\n+  __ subptr(end, addr); \/\/ end --> count\n+__ BIND(L_loop);\n+  Address cardtable(addr, count, Address::times_1, disp);\n+  __ movb(cardtable, 0);\n+  __ decrement(count);\n+  __ jcc(Assembler::greaterEqual, L_loop);\n+#endif\n+\n+__ BIND(L_done);\n+}\n+\n@@ -946,1 +1101,1 @@\n-  __ testb(gc_state, ShenandoahHeap::MARKING);\n+  __ testb(gc_state, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":159,"deletions":4,"binary":false,"changes":163,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-  assert(UseG1GC || UseParallelGC || UseSerialGC,\n+  assert(UseG1GC || UseParallelGC || UseSerialGC || UseShenandoahGC,\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -192,1 +192,1 @@\n-    size_t delta = pointer_delta(p, _byte_map_base, sizeof(CardValue));\n+    size_t delta =  (((uintptr_t) p) - ((uintptr_t) _byte_map_base)) \/ sizeof(CardValue);\n","filename":"src\/hotspot\/share\/gc\/shared\/cardTable.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -140,0 +140,4 @@\n+\n+  HeapWord* top() {\n+    return _top;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/plab.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -196,0 +197,10 @@\n+\n+  if (access.is_oop()) {\n+    DecoratorSet decorators = access.decorators();\n+    bool is_array = (decorators & IS_ARRAY) != 0;\n+    bool on_anonymous = (decorators & ON_UNKNOWN_OOP_REF) != 0;\n+\n+    bool precise = is_array || on_anonymous;\n+    LIR_Opr post_addr = precise ? access.resolved_addr() : access.base().opr();\n+    post_barrier(access, post_addr, value);\n+  }\n@@ -294,0 +305,61 @@\n+\n+void ShenandoahBarrierSetC1::post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  DecoratorSet decorators = access.decorators();\n+  LIRGenerator* gen = access.gen();\n+  bool in_heap = (decorators & IN_HEAP) != 0;\n+  if (!in_heap) {\n+    return;\n+  }\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  CardTable* ct = ctbs->card_table();\n+  LIR_Const* card_table_base = new LIR_Const(ct->byte_map_base());\n+  if (addr->is_address()) {\n+    LIR_Address* address = addr->as_address_ptr();\n+    \/\/ ptr cannot be an object because we use this barrier for array card marks\n+    \/\/ and addr can point in the middle of an array.\n+    LIR_Opr ptr = gen->new_pointer_register();\n+    if (!address->index()->is_valid() && address->disp() == 0) {\n+      __ move(address->base(), ptr);\n+    } else {\n+      assert(address->disp() != max_jint, \"lea doesn't support patched addresses!\");\n+      __ leal(addr, ptr);\n+    }\n+    addr = ptr;\n+  }\n+  assert(addr->is_register(), \"must be a register at this point\");\n+\n+  LIR_Opr tmp = gen->new_pointer_register();\n+  if (TwoOperandLIRForm) {\n+    __ move(addr, tmp);\n+    __ unsigned_shift_right(tmp, CardTable::card_shift(), tmp);\n+  } else {\n+    __ unsigned_shift_right(addr, CardTable::card_shift(), tmp);\n+  }\n+\n+  LIR_Address* card_addr;\n+  if (gen->can_inline_as_constant(card_table_base)) {\n+    card_addr = new LIR_Address(tmp, card_table_base->as_jint(), T_BYTE);\n+  } else {\n+    card_addr = new LIR_Address(tmp, gen->load_constant(card_table_base), T_BYTE);\n+  }\n+\n+  LIR_Opr dirty = LIR_OprFact::intConst(CardTable::dirty_card_val());\n+  if (UseCondCardMark) {\n+    LIR_Opr cur_value = gen->new_register(T_INT);\n+    __ move(card_addr, cur_value);\n+\n+    LabelObj* L_already_dirty = new LabelObj();\n+    __ cmp(lir_cond_equal, cur_value, dirty);\n+    __ branch(lir_cond_equal, L_already_dirty->label());\n+    __ move(dirty, card_addr);\n+    __ branch_destination(L_already_dirty->label());\n+  } else {\n+    __ move(dirty, card_addr);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.cpp","additions":72,"deletions":0,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -245,0 +245,2 @@\n+  void post_barrier(LIRAccess& access, LIR_Opr addr, LIR_Opr new_val);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -244,1 +245,1 @@\n-  marking = __ AndI(ld, __ ConI(ShenandoahHeap::MARKING));\n+  marking = __ AndI(ld, __ ConI(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n@@ -325,1 +326,1 @@\n-      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::MARKING)) {\n+      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING)) {\n@@ -454,0 +455,92 @@\n+Node* ShenandoahBarrierSetC2::byte_map_base_node(GraphKit* kit) const {\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  CardTable::CardValue* card_table_base = ctbs->card_table()->byte_map_base();\n+  if (card_table_base != NULL) {\n+    return kit->makecon(TypeRawPtr::make((address)card_table_base));\n+  } else {\n+    return kit->null();\n+  }\n+}\n+\n+void ShenandoahBarrierSetC2::post_barrier(GraphKit* kit,\n+                                          Node* ctl,\n+                                          Node* oop_store,\n+                                          Node* obj,\n+                                          Node* adr,\n+                                          uint  adr_idx,\n+                                          Node* val,\n+                                          BasicType bt,\n+                                          bool use_precise) const {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(BarrierSet::barrier_set());\n+  CardTable* ct = ctbs->card_table();\n+  \/\/ No store check needed if we're storing a NULL or an old object\n+  \/\/ (latter case is probably a string constant). The concurrent\n+  \/\/ mark sweep garbage collector, however, needs to have all nonNull\n+  \/\/ oop updates flagged via card-marks.\n+  if (val != NULL && val->is_Con()) {\n+    \/\/ must be either an oop or NULL\n+    const Type* t = val->bottom_type();\n+    if (t == TypePtr::NULL_PTR || t == Type::TOP)\n+      \/\/ stores of null never (?) need barriers\n+      return;\n+  }\n+\n+  if (ReduceInitialCardMarks && obj == kit->just_allocated_object(kit->control())) {\n+    \/\/ We can skip marks on a freshly-allocated object in Eden.\n+    \/\/ Keep this code in sync with new_deferred_store_barrier() in runtime.cpp.\n+    \/\/ That routine informs GC to take appropriate compensating steps,\n+    \/\/ upon a slow-path allocation, so as to make this card-mark\n+    \/\/ elision safe.\n+    return;\n+  }\n+\n+  if (!use_precise) {\n+    \/\/ All card marks for a (non-array) instance are in one place:\n+    adr = obj;\n+  }\n+  \/\/ (Else it's an array (or unknown), and we want more precise card marks.)\n+  assert(adr != NULL, \"\");\n+\n+  IdealKit ideal(kit, true);\n+\n+  \/\/ Convert the pointer to an int prior to doing math on it\n+  Node* cast = __ CastPX(__ ctrl(), adr);\n+\n+  \/\/ Divide by card size\n+  Node* card_offset = __ URShiftX( cast, __ ConI(CardTable::card_shift()) );\n+\n+  \/\/ Combine card table base and card offset\n+  Node* card_adr = __ AddP(__ top(), byte_map_base_node(kit), card_offset );\n+\n+  \/\/ Get the alias_index for raw card-mark memory\n+  int adr_type = Compile::AliasIdxRaw;\n+  Node*   zero = __ ConI(0); \/\/ Dirty card value\n+\n+  if (UseCondCardMark) {\n+    \/\/ The classic GC reference write barrier is typically implemented\n+    \/\/ as a store into the global card mark table.  Unfortunately\n+    \/\/ unconditional stores can result in false sharing and excessive\n+    \/\/ coherence traffic as well as false transactional aborts.\n+    \/\/ UseCondCardMark enables MP \"polite\" conditional card mark\n+    \/\/ stores.  In theory we could relax the load from ctrl() to\n+    \/\/ no_ctrl, but that doesn't buy much latitude.\n+    Node* card_val = __ load( __ ctrl(), card_adr, TypeInt::BYTE, T_BYTE, adr_type);\n+    __ if_then(card_val, BoolTest::ne, zero);\n+  }\n+\n+  \/\/ Smash zero into card\n+  __ store(__ ctrl(), card_adr, zero, T_BYTE, adr_type, MemNode::unordered);\n+\n+  if (UseCondCardMark) {\n+    __ end_if();\n+  }\n+\n+  \/\/ Final sync IdealKit and GraphKit.\n+  kit->final_sync(ideal);\n+}\n+\n@@ -520,0 +613,6 @@\n+\n+    Node* result = BarrierSetC2::store_at_resolved(access, val);\n+    bool is_array = (decorators & IS_ARRAY) != 0;\n+    bool use_precise = is_array || anonymous;\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(), adr, adr_idx, val.node(), access.type(), use_precise);\n+    return result;\n@@ -530,0 +629,1 @@\n+    return BarrierSetC2::store_at_resolved(access, val);\n@@ -531,1 +631,0 @@\n-  return BarrierSetC2::store_at_resolved(access, val);\n@@ -602,1 +701,1 @@\n-                                                   Node* new_val, const Type* value_type) const {\n+                                                             Node* new_val, const Type* value_type) const {\n@@ -644,0 +743,1 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(), access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n@@ -699,0 +799,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), new_val, T_OBJECT, true);\n@@ -715,0 +817,2 @@\n+    post_barrier(kit, kit->control(), access.raw_access(), access.base(),\n+                 access.addr().node(), access.alias_idx(), val, T_OBJECT, true);\n@@ -797,1 +901,1 @@\n-        }\n+    }\n@@ -840,1 +944,1 @@\n-      flags |= ShenandoahHeap::MARKING;\n+      flags |= ShenandoahHeap::YOUNG_MARKING;\n@@ -908,3 +1012,20 @@\n-void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* n) const {\n-  if (is_shenandoah_wb_pre_call(n)) {\n-    shenandoah_eliminate_wb_pre(n, &macro->igvn());\n+void ShenandoahBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  if (is_shenandoah_wb_pre_call(node)) {\n+    shenandoah_eliminate_wb_pre(node, &macro->igvn());\n+  }\n+  if (node->Opcode() == Op_CastP2X && ShenandoahHeap::heap()->mode()->is_generational()) {\n+    assert(node->Opcode() == Op_CastP2X, \"ConvP2XNode required\");\n+     Node *shift = node->unique_out();\n+     Node *addp = shift->unique_out();\n+     for (DUIterator_Last jmin, j = addp->last_outs(jmin); j >= jmin; --j) {\n+       Node *mem = addp->last_out(j);\n+       if (UseCondCardMark && mem->is_Load()) {\n+         assert(mem->Opcode() == Op_LoadB, \"unexpected code shape\");\n+         \/\/ The load is checking if the card has been written so\n+         \/\/ replace it with zero to fold the test.\n+         macro->replace_node(mem, macro->intcon(0));\n+         continue;\n+       }\n+       assert(mem->is_Store(), \"store required\");\n+       macro->replace_node(mem, mem->in(MemNode::Memory));\n+     }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":130,"deletions":9,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -1481,1 +1481,1 @@\n-    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, ShenandoahHeap::MARKING);\n+    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, (ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,53 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+\n+ShenandoahHeuristics* ShenandoahMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n+  if (ShenandoahGCHeuristics == NULL) {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n+  }\n+\n+  if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n+    return new ShenandoahAggressiveHeuristics(generation);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n+    return new ShenandoahStaticHeuristics(generation);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n+    return new ShenandoahAdaptiveHeuristics(generation);\n+  } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n+    return new ShenandoahCompactHeuristics(generation);\n+  } else {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n+  }\n+\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"added"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n@@ -58,1 +58,1 @@\n-ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics() const {\n+ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n@@ -62,1 +62,1 @@\n-  return new ShenandoahPassiveHeuristics();\n+  return new ShenandoahPassiveHeuristics(generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -181,0 +181,2 @@\n+  CardTable::initialize_card_size();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,115 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+void ShenandoahCardTable::initialize() {\n+  CardTable::initialize();\n+  _write_byte_map = _byte_map;\n+  _write_byte_map_base = _byte_map_base;\n+  const size_t rs_align = _page_size == (size_t) os::vm_page_size() ? 0 :\n+    MAX2(_page_size, (size_t) os::vm_allocation_granularity());\n+\n+  ReservedSpace heap_rs(_byte_map_size, rs_align, _page_size);\n+  if (!heap_rs.is_reserved()) {\n+    vm_exit_during_initialization(\"Could not reserve enough space for second copy of card marking array\");\n+  }\n+  os::commit_memory_or_exit(heap_rs.base(), _byte_map_size, rs_align, false, \"Cannot commit memory for second copy of card table\");\n+\n+  HeapWord* low_bound  = _whole_heap.start();\n+  _read_byte_map = (CardValue*) heap_rs.base();\n+  _read_byte_map_base = _read_byte_map - (uintptr_t(low_bound) >> card_shift());\n+\n+  log_trace(gc, barrier)(\"ShenandoahCardTable::ShenandoahCardTable: \");\n+  log_trace(gc, barrier)(\"    &_read_byte_map[0]: \" INTPTR_FORMAT \"  &_read_byte_map[_last_valid_index]: \" INTPTR_FORMAT,\n+                  p2i(&_read_byte_map[0]), p2i(&_read_byte_map[_last_valid_index]));\n+  log_trace(gc, barrier)(\"    _read_byte_map_base: \" INTPTR_FORMAT, p2i(_read_byte_map_base));\n+\n+  \/\/ TODO: As currently implemented, we do not swap pointers between _read_byte_map and _write_byte_map\n+  \/\/ because the mutator write barrier hard codes the address of the _write_byte_map_base.  Instead,\n+  \/\/ the current implementation simply copies contents of _write_byte_map onto _read_byte_map and cleans\n+  \/\/ the entirety of _write_byte_map at the init_mark safepoint.\n+  \/\/\n+  \/\/ If we choose to modify the mutator write barrier so that we can swap _read_byte_map_base and\n+  \/\/ _write_byte_map_base pointers, we may also have to figure out certain details about how the\n+  \/\/ _guard_region is implemented so that we can replicate the read and write versions of this region.\n+  \/\/\n+  \/\/ Alternatively, we may switch to a SATB-based write barrier and replace the direct card-marking\n+  \/\/ remembered set with something entirely different.\n+\n+  resize_covered_region(_whole_heap);\n+}\n+\n+bool ShenandoahCardTable::is_in_young(oop obj) const {\n+  return ShenandoahHeap::heap()->is_in_young(obj);\n+}\n+\n+bool ShenandoahCardTable::is_dirty(MemRegion mr) {\n+  for (size_t i = index_for(mr.start()); i <= index_for(mr.end() - 1); i++) {\n+    CardValue* byte = byte_for_index(i);\n+    if (*byte == CardTable::dirty_card_val()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void ShenandoahCardTable::clear() {\n+  CardTable::clear(_whole_heap);\n+}\n+\n+\/\/ TODO: This service is not currently used because we are not able to swap _read_byte_map_base and\n+\/\/ _write_byte_map_base pointers.  If we were able to do so, we would invoke clear_read_table \"immediately\"\n+\/\/ following the end of concurrent remembered set scanning so that this read card table would be ready\n+\/\/ to serve as the new write card table at the time these pointer values were next swapped.\n+\/\/\n+\/\/ In the current implementation, the write-table is cleared immediately after its contents is copied to\n+\/\/ the read table, obviating the need for this service.\n+void ShenandoahCardTable::clear_read_table() {\n+  for (size_t i = 0; i < _byte_map_size; i++) {\n+    _read_byte_map[i] = clean_card;\n+  }\n+}\n+\n+\/\/ TODO: This service is not currently used because the mutator write barrier implementation hard codes the\n+\/\/ location of the _write_byte_may_base.  If we change the mutator's write barrier implementation, then we\n+\/\/ may use this service to exchange the roles of the read-card-table and write-card-table.\n+void ShenandoahCardTable::swap_card_tables() {\n+  shenandoah_assert_safepoint();\n+\n+  CardValue* save_value = _read_byte_map;\n+  _read_byte_map = _write_byte_map;\n+  _write_byte_map = save_value;\n+\n+  save_value = _read_byte_map_base;\n+  _read_byte_map_base = _write_byte_map_base;\n+  _write_byte_map_base = save_value;\n+\n+  \/\/ update the superclass instance variables\n+  _byte_map = _write_byte_map;\n+  _byte_map_base = _write_byte_map_base;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":115,"deletions":0,"binary":false,"changes":115,"status":"added"},{"patch":"@@ -49,1 +49,1 @@\n-  return _mark_context->is_marked(obj);\n+  return _mark_context->is_marked_or_old(obj);\n@@ -61,1 +61,1 @@\n-  return _mark_context->is_marked(obj);\n+  return _mark_context->is_marked_or_old(obj);\n@@ -85,1 +85,1 @@\n-  assert(!ShenandoahHeap::heap()->has_forwarded_objects(), \"Not expected\");\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress() || !ShenandoahHeap::heap()->has_forwarded_objects(), \"Not expected\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahClosures.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,0 +33,3 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -87,3 +90,6 @@\n-ShenandoahConcurrentGC::ShenandoahConcurrentGC() :\n-  _mark(),\n-  _degen_point(ShenandoahDegenPoint::_degenerated_unset) {\n+ShenandoahConcurrentGC::ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap) :\n+  _mark(generation),\n+  _degen_point(ShenandoahDegenPoint::_degenerated_unset),\n+  _mixed_evac (false),\n+  _do_old_gc_bootstrap(do_old_gc_bootstrap),\n+  _generation(generation) {\n@@ -96,4 +102,0 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n-}\n-\n@@ -102,0 +104,2 @@\n+  heap->start_conc_gc();\n+\n@@ -112,0 +116,12 @@\n+\n+    \/\/ Reset task queue stats here, rather than in mark_concurrent_roots\n+    \/\/ because remembered set scan will `push` oops into the queues and\n+    \/\/ resetting after this happens will lose those counts.\n+    TASKQUEUE_STATS_ONLY(_mark.task_queues()->reset_taskqueue_stats());\n+\n+    \/\/ Concurrent remembered set scanning\n+    if (_generation->generation_mode() == YOUNG) {\n+      ShenandoahConcurrentPhase gc_phase(\"Concurrent remembered set scanning\", ShenandoahPhaseTimings::init_scan_rset);\n+      _generation->scan_remembered_set();\n+    }\n+\n@@ -114,1 +130,1 @@\n-    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle)) return false;\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_roots)) return false;\n@@ -124,0 +140,13 @@\n+  \/\/ If GC was cancelled before final mark, then the safepoint operation will do nothing\n+  \/\/ and the concurrent mark will still be in progress. In this case it is safe to resume\n+  \/\/ the degenerated cycle from the marking phase. On the other hand, if the GC is cancelled\n+  \/\/ after final mark (but before this check), then the final mark safepoint operation\n+  \/\/ will have finished the mark (setting concurrent mark in progress to false). Final mark\n+  \/\/ will also have setup state (in concurrent stack processing) that will not be safe to\n+  \/\/ resume from the marking phase in the degenerated cycle. That is, if the cancellation\n+  \/\/ occurred after final mark, we must resume the degenerated cycle after the marking phase.\n+  if (_generation->is_concurrent_mark_in_progress() && check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) {\n+    assert(!heap->is_concurrent_weak_root_in_progress(), \"Weak roots should not be in progress when concurrent mark is in progress\");\n+    return false;\n+  }\n+\n@@ -136,1 +165,2 @@\n-  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.  Note that\n+  \/\/ we will not age young-gen objects in the case that we skip evacuation.\n@@ -157,0 +187,6 @@\n+  \/\/ Global marking has completed. We need to fill in any unmarked objects in the old generation\n+  \/\/ so that subsequent remembered set scans will not walk pointers into reclaimed memory.\n+  if (!heap->cancelled_gc() && heap->mode()->is_generational() && _generation->generation_mode() == GLOBAL) {\n+    entry_global_coalesce_and_fill();\n+  }\n+\n@@ -179,1 +215,2 @@\n-    vmop_entry_final_roots();\n+    \/\/ We chose not to evacuate because we found sufficient immediate garbage.\n+    vmop_entry_final_roots(heap->is_aging_cycle());\n@@ -181,0 +218,29 @@\n+  size_t old_available, young_available;\n+  {\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    ShenandoahGeneration* old_gen = heap->old_generation();\n+    ShenandoahHeapLocker locker(heap->lock());\n+\n+    size_t old_usage_before_evac = heap->capture_old_usage(0);\n+    size_t old_usage_now = old_gen->used();\n+    size_t promoted_bytes = old_usage_now - old_usage_before_evac;\n+    heap->set_previous_promotion(promoted_bytes);\n+\n+    young_gen->unadjust_available();\n+    old_gen->unadjust_available();\n+    young_gen->increase_used(heap->get_young_evac_expended());\n+    \/\/ No need to old_gen->increase_used().  That was done when plabs were allocated, accounting for both old evacs and promotions.\n+\n+    young_available = young_gen->adjusted_available();\n+    old_available = old_gen->adjusted_available();\n+\n+    heap->set_alloc_supplement_reserve(0);\n+    heap->set_young_evac_reserve(0);\n+    heap->reset_young_evac_expended();\n+    heap->set_old_evac_reserve(0);\n+    heap->reset_old_evac_expended();\n+    heap->set_promotion_reserve(0);\n+  }\n+  log_info(gc, ergo)(\"At end of concurrent GC, old_available: \" SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                     byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                     byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n@@ -191,1 +257,1 @@\n-  VM_ShenandoahInitMark op(this);\n+  VM_ShenandoahInitMark op(this, _do_old_gc_bootstrap);\n@@ -225,1 +291,1 @@\n-void ShenandoahConcurrentGC::vmop_entry_final_roots() {\n+void ShenandoahConcurrentGC::vmop_entry_final_roots(bool increment_region_ages) {\n@@ -232,1 +298,1 @@\n-  VM_ShenandoahFinalRoots op(this);\n+  VM_ShenandoahFinalRoots op(this, increment_region_ages);\n@@ -237,1 +303,2 @@\n-  const char* msg = init_mark_event_message();\n+  char msg[1024];\n+  init_mark_event_message(msg, sizeof(msg));\n@@ -245,0 +312,8 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()\n+    && (_generation->generation_mode() == YOUNG || (_generation->generation_mode() == GLOBAL && ShenandoahVerify))) {\n+    \/\/ The current implementation of swap_remembered_set() copies the write-card-table\n+    \/\/ to the read-card-table. The remembered sets are also swapped for GLOBAL collections\n+    \/\/ so that the verifier works with the correct copy of the card table when verifying.\n+    _generation->swap_remembered_set();\n+  }\n+\n@@ -249,1 +324,2 @@\n-  const char* msg = final_mark_event_message();\n+  char msg[1024];\n+  final_mark_event_message(msg, sizeof(msg));\n@@ -320,0 +396,1 @@\n+  char msg[1024];\n@@ -322,1 +399,1 @@\n-  const char* msg = conc_mark_event_message();\n+  conc_mark_event_message(msg, sizeof(msg));\n@@ -477,0 +554,15 @@\n+void ShenandoahConcurrentGC::entry_global_coalesce_and_fill() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  const char* msg = \"Coalescing and filling old regions in global collect\";\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::coalesce_and_fill);\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent coalesce and fill\");\n+\n+  op_global_coalesce_and_fill();\n+}\n+\n@@ -482,2 +574,1 @@\n-\n-  heap->prepare_gc();\n+  _generation->prepare_gc(_do_old_gc_bootstrap);\n@@ -496,1 +587,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -514,2 +606,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -526,1 +618,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -528,1 +620,2 @@\n-  {\n+  if (_do_old_gc_bootstrap) {\n+    \/\/ Update region state for both young and old regions\n@@ -532,0 +625,6 @@\n+    heap->old_generation()->parallel_heap_region_iterate(&cl);\n+  } else {\n+    \/\/ Update region state for only young regions\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_update_region_states);\n+    ShenandoahInitMarkUpdateRegionStateClosure cl;\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -535,1 +634,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n@@ -541,0 +640,1 @@\n+\n@@ -578,1 +678,31 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    \/\/ The collection set is chosen by prepare_regions_and_collection_set().\n+    \/\/\n+    \/\/ TODO: Under severe memory overload conditions that can be checked here, we may want to limit\n+    \/\/ the inclusion of old-gen candidates within the collection set.  This would allow us to prioritize efforts on\n+    \/\/ evacuating young-gen,  This remediation is most appropriate when old-gen availability is very high (so there\n+    \/\/ are negligible negative impacts from delaying completion of old-gen evacuation) and when young-gen collections\n+    \/\/ are \"under duress\" (as signalled by very low availability of memory within young-gen, indicating that\/ young-gen\n+    \/\/ collections are not triggering frequently enough).\n+    bool mixed_evac = _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+\n+    \/\/ Upon return from prepare_regions_and_collection_set(), certain parameters have been established to govern the\n+    \/\/ evacuation efforts that are about to begin.  In particular:\n+    \/\/\n+    \/\/ heap->get_promotion_reserve() represents the amount of memory within old-gen's available memory that has\n+    \/\/   been set aside to hold objects promoted from young-gen memory.  This represents an estimated percentage\n+    \/\/   of the live young-gen memory within the collection set.  If there is more data ready to be promoted than\n+    \/\/   can fit within this reserve, the promotion of some objects will be deferred until a subsequent evacuation\n+    \/\/   pass.\n+    \/\/\n+    \/\/ heap->get_old_evac_reserve() represents the amount of memory within old-gen's available memory that has been\n+    \/\/  set aside to hold objects evacuated from the old-gen collection set.\n+    \/\/\n+    \/\/ heap->get_young_evac_reserve() represents the amount of memory within young-gen's available memory that has\n+    \/\/  been set aside to hold objects evacuated from the young-gen collection set.  Conservatively, this value\n+    \/\/  equals the entire amount of live young-gen memory within the collection set, even though some of this memory\n+    \/\/  will likely be promoted.\n+    \/\/\n+    \/\/ heap->get_alloc_supplement_reserve() represents the amount of old-gen memory that can be allocated during evacuation\n+    \/\/ and update-refs phases of gc.  The young evacuation reserve has already been removed from this quantity.\n+\n+    heap->set_mixed_evac(mixed_evac);\n@@ -605,0 +735,14 @@\n+      if (heap->mode()->is_generational()) {\n+        \/\/ Calculate the temporary evacuation allowance supplement to young-gen memory capacity (for allocations\n+        \/\/ and young-gen evacuations).\n+        size_t young_available = heap->young_generation()->adjust_available(heap->get_alloc_supplement_reserve());\n+        \/\/ old_available is memory that can hold promotions and evacuations.  Subtract out the memory that is being\n+        \/\/ loaned for young-gen allocations or evacuations.\n+        size_t old_available = heap->old_generation()->adjust_available(-heap->get_alloc_supplement_reserve());\n+\n+        log_info(gc, ergo)(\"After generational memory budget adjustments, old avaiable: \" SIZE_FORMAT\n+                           \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                           byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                           byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+      }\n+\n@@ -673,1 +817,1 @@\n-  heap->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n+  _generation->ref_processor()->process_references(ShenandoahPhaseTimings::conc_weak_refs, heap->workers(), true \/* concurrent *\/);\n@@ -700,2 +844,9 @@\n-      shenandoah_assert_correct(p, obj);\n-      ShenandoahHeap::atomic_clear_oop(p, obj);\n+      if (_heap->is_in_active_generation(obj)) {\n+        \/\/ TODO: This worries me. Here we are asserting that an unmarked from-space object is 'correct'.\n+        \/\/ Normally, I would call this a bogus assert, but there seems to be a legitimate use-case for\n+        \/\/ accessing from-space objects during class unloading. However, the from-space object may have\n+        \/\/ been \"filled\". We've made no effort to prevent old generation classes being unloaded by young\n+        \/\/ gen (and vice-versa).\n+        shenandoah_assert_correct(p, obj);\n+        ShenandoahHeap::atomic_clear_oop(p, obj);\n+      }\n@@ -926,1 +1077,3 @@\n-\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_before_updaterefs();\n+  }\n@@ -971,1 +1124,1 @@\n-    heap->clear_cancelled_gc();\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -981,0 +1134,7 @@\n+  if (heap->is_concurrent_old_mark_in_progress()) {\n+    \/\/ Purge the SATB buffers, transferring any valid, old pointers to the\n+    \/\/ old generation mark queue. From here on, no mutator will have access\n+    \/\/ to anything that will be trashed and recycled.\n+    heap->purge_old_satb_buffers(false \/* abandon *\/);\n+  }\n+\n@@ -984,0 +1144,4 @@\n+  \/\/ Aging_cycle is only relevant during evacuation cycle for individual objects and during final mark for\n+  \/\/ entire regions.  Both of these relevant operations occur before final update refs.\n+  heap->set_aging_cycle(false);\n+\n@@ -1003,0 +1167,4 @@\n+void ShenandoahConcurrentGC::op_global_coalesce_and_fill() {\n+  ShenandoahHeap::heap()->coalesce_and_fill_old_regions();\n+}\n+\n@@ -1011,1 +1179,1 @@\n-const char* ShenandoahConcurrentGC::init_mark_event_message() const {\n+void ShenandoahConcurrentGC::init_mark_event_message(char* buf, size_t len) const {\n@@ -1015,1 +1183,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    jio_snprintf(buf, len, \"Pause Init Mark (%s) (unload classes)\", _generation->name());\n@@ -1017,1 +1185,1 @@\n-    return \"Pause Init Mark\";\n+    jio_snprintf(buf, len, \"Pause Init Mark (%s)\", _generation->name());\n@@ -1021,1 +1189,1 @@\n-const char* ShenandoahConcurrentGC::final_mark_event_message() const {\n+void ShenandoahConcurrentGC::final_mark_event_message(char* buf, size_t len) const {\n@@ -1023,1 +1191,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark (unless old gen concurrent mark is running)\");\n@@ -1025,1 +1194,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    jio_snprintf(buf, len, \"Pause Final Mark (%s) (unload classes)\", _generation->name());\n@@ -1027,1 +1196,1 @@\n-    return \"Pause Final Mark\";\n+    jio_snprintf(buf, len, \"Pause Final Mark (%s)\", _generation->name());\n@@ -1031,1 +1200,1 @@\n-const char* ShenandoahConcurrentGC::conc_mark_event_message() const {\n+void ShenandoahConcurrentGC::conc_mark_event_message(char* buf, size_t len) const {\n@@ -1033,1 +1202,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark (unless old gen concurrent mark is running\");\n@@ -1035,1 +1205,1 @@\n-    return \"Concurrent marking (unload classes)\";\n+    jio_snprintf(buf, len, \"Concurrent marking (%s) (unload classes)\", _generation->name());\n@@ -1037,1 +1207,1 @@\n-    return \"Concurrent marking\";\n+    jio_snprintf(buf, len, \"Concurrent marking (%s)\", _generation->name());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":211,"deletions":41,"binary":false,"changes":252,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -45,0 +47,1 @@\n+template <GenerationMode GENERATION>\n@@ -59,2 +62,1 @@\n-    ShenandoahObjToScanQueue* q = _cm->get_queue(worker_id);\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -63,1 +65,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n+    _cm->mark_loop(GENERATION, worker_id, _terminator, rp,\n@@ -96,0 +98,1 @@\n+template<GenerationMode GENERATION>\n@@ -111,1 +114,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -117,0 +120,1 @@\n+      ShenandoahObjToScanQueue* old = _cm->get_old_queue(worker_id);\n@@ -118,1 +122,1 @@\n-      ShenandoahSATBBufferClosure cl(q);\n+      ShenandoahSATBBufferClosure<GENERATION> cl(q, old);\n@@ -123,1 +127,1 @@\n-      ShenandoahMarkRefsClosure             mark_cl(q, rp);\n+      ShenandoahMarkRefsClosure<GENERATION> mark_cl(q, rp, old);\n@@ -128,1 +132,1 @@\n-    _cm->mark_loop(worker_id, _terminator, rp,\n+    _cm->mark_loop(GENERATION, worker_id, _terminator, rp,\n@@ -136,2 +140,2 @@\n-ShenandoahConcurrentMark::ShenandoahConcurrentMark() :\n-  ShenandoahMark() {}\n+ShenandoahConcurrentMark::ShenandoahConcurrentMark(ShenandoahGeneration* generation) :\n+  ShenandoahMark(generation) {}\n@@ -140,0 +144,1 @@\n+template<GenerationMode GENERATION>\n@@ -145,0 +150,1 @@\n+  ShenandoahObjToScanQueueSet* const  _old_queue_set;\n@@ -149,0 +155,1 @@\n+                                    ShenandoahObjToScanQueueSet* old,\n@@ -155,4 +162,6 @@\n-ShenandoahMarkConcurrentRootsTask::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n-                                                                     ShenandoahReferenceProcessor* rp,\n-                                                                     ShenandoahPhaseTimings::Phase phase,\n-                                                                     uint nworkers) :\n+template<GenerationMode GENERATION>\n+ShenandoahMarkConcurrentRootsTask<GENERATION>::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n+                                                                                 ShenandoahObjToScanQueueSet* old,\n+                                                                                 ShenandoahReferenceProcessor* rp,\n+                                                                                 ShenandoahPhaseTimings::Phase phase,\n+                                                                                 uint nworkers) :\n@@ -162,0 +171,1 @@\n+  _old_queue_set(old),\n@@ -166,1 +176,2 @@\n-void ShenandoahMarkConcurrentRootsTask::work(uint worker_id) {\n+template<GenerationMode GENERATION>\n+void ShenandoahMarkConcurrentRootsTask<GENERATION>::work(uint worker_id) {\n@@ -169,1 +180,2 @@\n-  ShenandoahMarkRefsClosure cl(q, _rp);\n+  ShenandoahObjToScanQueue* old = _old_queue_set == NULL ? NULL : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp, old);\n@@ -177,2 +189,0 @@\n-  TASKQUEUE_STATS_ONLY(task_queues()->reset_taskqueue_stats());\n-\n@@ -180,5 +190,19 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n-  task_queues()->reserve(workers->active_workers());\n-  ShenandoahMarkConcurrentRootsTask task(task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n-\n-  workers->run_task(&task);\n+  ShenandoahReferenceProcessor* rp = _generation->ref_processor();\n+  _generation->reserve_task_queues(workers->active_workers());\n+  switch (_generation->generation_mode()) {\n+    case YOUNG: {\n+      ShenandoahMarkConcurrentRootsTask<YOUNG> task(task_queues(), old_task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL: {\n+      assert(old_task_queues() == NULL, \"Global mark should not have old gen mark queues.\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(task_queues(), old_task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      workers->run_task(&task);\n+      break;\n+    }\n+    default:\n+      \/\/ Intentionally haven't added OLD here. We use a YOUNG generation\n+      \/\/ cycle to bootstrap concurrent old marking.\n+      ShouldNotReachHere();\n+  }\n@@ -209,3 +233,22 @@\n-    TaskTerminator terminator(nworkers, task_queues());\n-    ShenandoahConcurrentMarkingTask task(this, &terminator);\n-    workers->run_task(&task);\n+    switch (_generation->generation_mode()) {\n+      case YOUNG: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<YOUNG> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case OLD: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<OLD> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      case GLOBAL: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<GLOBAL> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n+      default:\n+        ShouldNotReachHere();\n+    }\n@@ -238,3 +281,2 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->set_concurrent_mark_in_progress(false);\n-  heap->mark_complete_marking_context();\n+  _generation->set_concurrent_mark_in_progress(false);\n+  _generation->set_mark_complete();\n@@ -258,4 +300,19 @@\n-  ShenandoahFinalMarkingTask task(this, &terminator, ShenandoahStringDedup::is_enabled());\n-  heap->workers()->run_task(&task);\n-  assert(task_queues()->is_empty(), \"Should be empty\");\n-}\n+  switch (_generation->generation_mode()) {\n+    case YOUNG:{\n+      ShenandoahFinalMarkingTask<YOUNG> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case OLD:{\n+      ShenandoahFinalMarkingTask<OLD> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    case GLOBAL:{\n+      ShenandoahFinalMarkingTask<GLOBAL> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -265,4 +322,1 @@\n-void ShenandoahConcurrentMark::cancel() {\n-  clear();\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n-  rp->abandon_partial_discovery();\n+  assert(task_queues()->is_empty(), \"Should be empty\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":90,"deletions":36,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -32,0 +32,2 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -37,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n@@ -42,0 +45,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -50,2 +54,3 @@\n-  _alloc_failure_waiters_lock(Mutex::safepoint-1, \"ShenandoahAllocFailureGC_lock\", true),\n-  _gc_waiters_lock(Mutex::safepoint-1, \"ShenandoahRequestedGC_lock\", true),\n+  _alloc_failure_waiters_lock(Mutex::safepoint - 1, \"ShenandoahAllocFailureGC_lock\", true),\n+  _gc_waiters_lock(Mutex::safepoint - 1, \"ShenandoahRequestedGC_lock\", true),\n+  _control_lock(Mutex::nosafepoint - 1, \"ShenandoahControlGC_lock\", true),\n@@ -54,0 +59,1 @@\n+  _requested_generation(GenerationMode::GLOBAL),\n@@ -55,1 +61,3 @@\n-  _allocs_seen(0) {\n+  _degen_generation(NULL),\n+  _allocs_seen(0),\n+  _mode(none) {\n@@ -83,0 +91,1 @@\n+  GenerationMode generation = GLOBAL;\n@@ -84,1 +93,0 @@\n-  int sleep = ShenandoahControlIntervalMin;\n@@ -87,1 +95,1 @@\n-  double last_sleep_adjust_time = os::elapsedTime();\n+  uint age_period = 0;\n@@ -96,1 +104,6 @@\n-  ShenandoahHeuristics* heuristics = heap->heuristics();\n+\n+  \/\/ Heuristics are notified of allocation failures here and other outcomes\n+  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n+  \/\/ degenerated cycle should be 'promoted' to a full cycle. The decision to\n+  \/\/ trigger a cycle or not is evaluated on the regulator thread.\n+  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n@@ -103,1 +116,1 @@\n-    bool implicit_gc_requested = is_gc_requested && !is_explicit_gc(requested_gc_cause);\n+    bool implicit_gc_requested = is_gc_requested && is_implicit_gc(requested_gc_cause);\n@@ -112,1 +125,1 @@\n-    GCMode mode = none;\n+    set_gc_mode(none);\n@@ -126,1 +139,12 @@\n-      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle()) {\n+      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n+        _degen_generation = heap->mode()->is_generational() ? heap->young_generation() : heap->global_generation();\n+      } else {\n+        assert(_degen_generation != NULL, \"Need to know which generation to resume.\");\n+      }\n+\n+      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n+      generation = _degen_generation->generation_mode();\n+      bool old_gen_evacuation_failed = heap->clear_old_evacuation_failure();\n+\n+      \/\/ Do not bother with degenerated cycle if old generation evacuation failed.\n+      if (ShenandoahDegeneratedGC && heuristics->should_degenerate_cycle() && !old_gen_evacuation_failed) {\n@@ -129,1 +153,1 @@\n-        mode = stw_degenerated;\n+        set_gc_mode(stw_degenerated);\n@@ -133,1 +157,2 @@\n-        mode = stw_full;\n+        generation = GLOBAL;\n+        set_gc_mode(stw_full);\n@@ -135,1 +160,0 @@\n-\n@@ -137,0 +161,1 @@\n+      generation = GLOBAL;\n@@ -140,1 +165,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -144,1 +169,1 @@\n-        mode = default_mode;\n+        set_gc_mode(default_mode);\n@@ -146,1 +171,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -149,1 +174,1 @@\n-        mode = stw_full;\n+        set_gc_mode(stw_full);\n@@ -153,0 +178,1 @@\n+      generation = GLOBAL;\n@@ -155,1 +181,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -159,1 +185,1 @@\n-        mode = default_mode;\n+        set_gc_mode(default_mode);\n@@ -162,1 +188,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -165,1 +191,1 @@\n-        mode = stw_full;\n+        set_gc_mode(stw_full);\n@@ -168,4 +194,24 @@\n-      \/\/ Potential normal cycle: ask heuristics if it wants to act\n-      if (heuristics->should_start_gc()) {\n-        mode = default_mode;\n-        cause = default_cause;\n+      \/\/ We should only be here if the regulator requested a cycle or if\n+      \/\/ there is an old generation mark in progress.\n+      if (_requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+        \/\/ preemption was requested or this is a regular cycle\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = _requested_generation;\n+        set_gc_mode(default_mode);\n+\n+        \/\/ Don't start a new old marking if there is one already in progress.\n+        if (generation == OLD && heap->is_concurrent_old_mark_in_progress()) {\n+          set_gc_mode(marking_old);\n+        }\n+\n+        if (generation == GLOBAL) {\n+          heap->set_unload_classes(global_heuristics->should_unload_classes());\n+        } else {\n+          heap->set_unload_classes(false);\n+        }\n+      } else if (heap->is_concurrent_old_mark_in_progress() || heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+        \/\/ Nobody asked us to do anything, but we have an old-generation mark or old-generation preparation for\n+        \/\/ mixed evacuation in progress, so resume working on that.\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = OLD;\n+        set_gc_mode(marking_old);\n@@ -174,2 +220,5 @@\n-      \/\/ Ask policy if this cycle wants to process references or unload classes\n-      heap->set_unload_classes(heuristics->should_unload_classes());\n+      \/\/ Don't want to spin in this loop and start a cycle every time, so\n+      \/\/ clear requested gc cause. This creates a race with callers of the\n+      \/\/ blocking 'request_gc' method, but there it loops and resets the\n+      \/\/ '_requested_gc_cause' until a full cycle is completed.\n+      _requested_gc_cause = GCCause::_no_gc;\n@@ -179,2 +228,2 @@\n-    \/\/ either implicit or explicit GC request,  or we are requested to do so unconditionally.\n-    if (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n+    \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+    if (generation == GLOBAL && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n@@ -184,1 +233,1 @@\n-    bool gc_requested = (mode != none);\n+    bool gc_requested = (_mode != none);\n@@ -205,12 +254,31 @@\n-      switch (mode) {\n-        case concurrent_normal:\n-          service_concurrent_normal_cycle(cause);\n-          break;\n-        case stw_degenerated:\n-          service_stw_degenerated_cycle(cause, degen_point);\n-          break;\n-        case stw_full:\n-          service_stw_full_cycle(cause);\n-          break;\n-        default:\n-          ShouldNotReachHere();\n+      heap->set_aging_cycle(false);\n+      {\n+        switch (_mode) {\n+          case concurrent_normal: {\n+            if ((generation == YOUNG) && (age_period-- == 0)) {\n+              heap->set_aging_cycle(true);\n+              age_period = ShenandoahAgingCyclePeriod - 1;\n+            }\n+            service_concurrent_normal_cycle(heap, generation, cause);\n+            break;\n+          }\n+          case stw_degenerated: {\n+            if (!service_stw_degenerated_cycle(cause, degen_point)) {\n+              \/\/ The degenerated GC was upgraded to a Full GC\n+              generation = GLOBAL;\n+            }\n+            break;\n+          }\n+          case stw_full: {\n+            service_stw_full_cycle(cause);\n+            break;\n+          }\n+          case marking_old: {\n+            assert(generation == OLD, \"Expected old generation here\");\n+            resume_concurrent_old_cycle(heap->old_generation(), cause);\n+            break;\n+          }\n+          default: {\n+            ShouldNotReachHere();\n+          }\n+        }\n@@ -254,1 +322,2 @@\n-        heuristics->clear_metaspace_oom();\n+        assert(generation == GLOBAL, \"Only unload classes during GLOBAL cycle\");\n+        global_heuristics->clear_metaspace_oom();\n@@ -313,8 +382,6 @@\n-    \/\/ Wait before performing the next action. If allocation happened during this wait,\n-    \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n-    \/\/ back off exponentially.\n-    if (_heap_changed.try_unset()) {\n-      sleep = ShenandoahControlIntervalMin;\n-    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n-      sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));\n-      last_sleep_adjust_time = current;\n+    \/\/ Don't wait around if there was an allocation failure - start the next cycle immediately.\n+    if (!is_alloc_failure_gc()) {\n+      \/\/ The timed wait is necessary because this thread has a responsibility to send\n+      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+      lock.wait(ShenandoahControlIntervalMax);\n@@ -322,1 +389,0 @@\n-    os::naked_short_sleep(sleep);\n@@ -331,0 +397,94 @@\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old cycles always degenerate to a global cycle.\n+\/\/ Young cycles are degenerated to complete the young cycle.  Young\n+\/\/ and old degen may upgrade to Full GC.  Full GC may also be\n+\/\/ triggered directly by a System.gc() invocation.\n+\/\/\n+\/\/\n+\/\/      +-----+ Idle +-----+-----------+---------------------+\n+\/\/      |         +        |           |                     |\n+\/\/      |         |        |           |                     |\n+\/\/      |         |        v           |                     |\n+\/\/      |         |  Bootstrap Old +-- | ------------+       |\n+\/\/      |         |   +                |             |       |\n+\/\/      |         |   |                |             |       |\n+\/\/      |         v   v                v             v       |\n+\/\/      |    Resume Old <----------+ Young +--> Young Degen  |\n+\/\/      |     +  +                                   +       |\n+\/\/      v     |  |                                   |       |\n+\/\/   Global <-+  |                                   |       |\n+\/\/      +        |                                   |       |\n+\/\/      |        v                                   v       |\n+\/\/      +--->  Global Degen +--------------------> Full <----+\n+\/\/\n+void ShenandoahControlThread::service_concurrent_normal_cycle(\n+  const ShenandoahHeap* heap, const GenerationMode generation, GCCause::Cause cause) {\n+\n+  switch (generation) {\n+    case YOUNG: {\n+      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n+      \/\/ concurrent mark in the old generation. We need to think about promotions\n+      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n+      \/\/ they end up in, but we have to be sure we don't promote into any regions\n+      \/\/ that are in the cset.\n+      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n+      service_concurrent_cycle(heap->young_generation(), cause, false);\n+      heap->young_generation()->log_status();\n+      break;\n+    }\n+    case GLOBAL: {\n+      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n+      service_concurrent_cycle(heap->global_generation(), cause, false);\n+      heap->global_generation()->log_status();\n+      break;\n+    }\n+    case OLD: {\n+      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n+      service_concurrent_old_cycle(heap, cause);\n+      heap->old_generation()->log_status();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void ShenandoahControlThread::service_concurrent_old_cycle(const ShenandoahHeap* heap, GCCause::Cause &cause) {\n+  \/\/ Configure the young generation's concurrent mark to put objects in\n+  \/\/ old regions into the concurrent mark queues associated with the old\n+  \/\/ generation. The young cycle will run as normal except that rather than\n+  \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+  \/\/ mark but it will not traverse them.\n+  ShenandoahGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+\n+  assert(!heap->is_concurrent_old_mark_in_progress(), \"Old already in progress.\");\n+  assert(old_generation->task_queues()->is_empty(), \"Old mark queues should be empty.\");\n+\n+  young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+  young_generation->set_mark_incomplete();\n+  old_generation->set_mark_incomplete();\n+  service_concurrent_cycle(young_generation, cause, true);\n+  if (!heap->cancelled_gc()) {\n+    \/\/ Reset the degenerated point. Normally this would happen at the top\n+    \/\/ of the control loop, but here we have just completed a young cycle\n+    \/\/ which has bootstrapped the old concurrent marking.\n+    _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+    \/\/ TODO: Bit of a hack here to keep the phase timings happy as we transition\n+    \/\/ to concurrent old marking. We need to revisit this.\n+    heap->phase_timings()->flush_par_workers_to_cycle();\n+    heap->phase_timings()->flush_cycle_to_global();\n+\n+    \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+    \/\/ and init mark for the concurrent mark. All of that work will have been\n+    \/\/ done by the bootstrapping young cycle. In order to simplify the debugging\n+    \/\/ effort, the old cycle will ONLY complete the mark phase. No actual\n+    \/\/ collection of the old generation is happening here.\n+    set_gc_mode(marking_old);\n+    resume_concurrent_old_cycle(old_generation, cause);\n+  }\n+}\n+\n@@ -350,1 +510,40 @@\n-void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause) {\n+void ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress() ||\n+         ShenandoahHeap::heap()->is_concurrent_prep_for_mixed_evacuation_in_progress(),\n+         \"Old mark or mixed-evac prep should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued.\", generation->task_queues()->tasks());\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  ShenandoahGCSession session(cause, generation);\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  \/\/ We can only tolerate being cancelled during concurrent marking or during preparation for mixed\n+  \/\/ evacuation. This flag here (passed by reference) is used to control precisely where the regulator\n+  \/\/ is allowed to cancel a GC.\n+  ShenandoahOldGC gc(generation, _allow_old_preemption);\n+  if (gc.collect(cause)) {\n+    \/\/ Old collection is complete, the young generation no longer needs this\n+    \/\/ reference to the old concurrent mark so clean it up.\n+    heap->young_generation()->set_old_gen_task_queues(NULL);\n+    generation->heuristics()->record_success_concurrent();\n+    heap->shenandoah_policy()->record_success_concurrent();\n+  }\n+\n+  if (heap->cancelled_gc()) {\n+    \/\/ It's possible the gc cycle was cancelled after the last time\n+    \/\/ the collection checked for cancellation. In which case, the\n+    \/\/ old gc cycle is still completed, and we have to deal with this\n+    \/\/ cancellation. We set the degeneration point to be outside\n+    \/\/ the cycle because if this is an allocation failure, that is\n+    \/\/ what must be done (there is no degenerated old cycle). If the\n+    \/\/ cancellation was due to a heuristic wanting to start a young\n+    \/\/ cycle, then we are not actually going to a degenerated cycle,\n+    \/\/ so the degenerated point doesn't matter here.\n+    check_cancellation_or_degen(ShenandoahGC::_degenerated_outside_cycle);\n+  }\n+}\n+\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause, bool do_old_gc_bootstrap) {\n@@ -390,1 +589,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, generation);\n@@ -394,1 +593,1 @@\n-  ShenandoahConcurrentGC gc;\n+  ShenandoahConcurrentGC gc(generation, do_old_gc_bootstrap);\n@@ -397,1 +596,1 @@\n-    heap->heuristics()->record_success_concurrent();\n+    generation->heuristics()->record_success_concurrent();\n@@ -402,0 +601,4 @@\n+    assert(generation->generation_mode() != OLD, \"Old GC takes a different control path\");\n+    \/\/ Concurrent young-gen collection degenerates to young\n+    \/\/ collection.  Same for global collections.\n+    _degen_generation = generation;\n@@ -407,7 +610,5 @@\n-  if (heap->cancelled_gc()) {\n-    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n-    if (!in_graceful_shutdown()) {\n-      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n-      _degen_point = point;\n-    }\n+  if (!heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (in_graceful_shutdown()) {\n@@ -416,0 +617,24 @@\n+\n+  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+\n+  if (is_alloc_failure_gc()) {\n+    _degen_point = point;\n+    return true;\n+  }\n+\n+  if (_preemption_requested.is_set()) {\n+    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n+    _preemption_requested.unset();\n+\n+    \/\/ Old generation marking is only cancellable during concurrent marking.\n+    \/\/ Once final mark is complete, the code does not check again for cancellation.\n+    \/\/ If old generation was cancelled for an allocation failure, we wouldn't\n+    \/\/ make it to this case. The calling code is responsible for forcing a\n+    \/\/ cancellation due to allocation failure into a degenerated cycle.\n+    _degen_point = point;\n+    heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n+    return true;\n+  }\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking.\");\n@@ -424,0 +649,2 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n@@ -425,1 +652,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -430,2 +657,1 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_full();\n+  heap->global_generation()->heuristics()->record_success_full();\n@@ -435,1 +661,1 @@\n-void ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n+bool ShenandoahControlThread::service_stw_degenerated_cycle(GCCause::Cause cause, ShenandoahGC::ShenandoahDegenPoint point) {\n@@ -437,0 +663,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -439,1 +666,6 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, _degen_generation);\n+\n+  ShenandoahDegenGC gc(point, _degen_generation);\n+\n+  \/\/ Just in case degenerated cycle preempted old-gen marking, clear the old-gen task queues.\n+  heap->young_generation()->set_old_gen_task_queues(NULL);\n@@ -441,1 +673,0 @@\n-  ShenandoahDegenGC gc(point);\n@@ -444,2 +675,5 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_degenerated();\n+  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+  assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+\n+  _degen_generation->heuristics()->record_success_degenerated();\n@@ -447,0 +681,1 @@\n+  return !gc.upgraded_to_full();\n@@ -477,0 +712,4 @@\n+bool ShenandoahControlThread::is_implicit_gc(GCCause::Cause cause) const {\n+  return !is_explicit_gc(cause) && cause != GCCause::_shenandoah_concurrent_gc;\n+}\n+\n@@ -496,0 +735,35 @@\n+bool ShenandoahControlThread::request_concurrent_gc(GenerationMode generation) {\n+  if (_preemption_requested.is_set() || _gc_requested.is_set() || ShenandoahHeap::heap()->cancelled_gc()) {\n+    \/\/ ignore subsequent requests from the heuristics\n+    return false;\n+  }\n+\n+  if (_mode == none) {\n+    _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+    _requested_generation = generation;\n+    notify_control_thread();\n+    return true;\n+  }\n+\n+  if (preempt_old_marking(generation)) {\n+    log_info(gc)(\"Preempting old generation mark to allow young GC.\");\n+    _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+    _requested_generation = generation;\n+    _preemption_requested.set();\n+    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+    notify_control_thread();\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+void ShenandoahControlThread::notify_control_thread() {\n+  MonitorLocker locker(&_control_lock, Mutex::_no_safepoint_check_flag);\n+  _control_lock.notify();\n+}\n+\n+bool ShenandoahControlThread::preempt_old_marking(GenerationMode generation) {\n+  return generation == YOUNG && _allow_old_preemption.try_unset();\n+}\n+\n@@ -515,1 +789,1 @@\n-\n+    notify_control_thread();\n@@ -599,4 +873,0 @@\n-  \/\/ Notify that something had changed.\n-  if (_heap_changed.is_unset()) {\n-    _heap_changed.set();\n-  }\n@@ -647,0 +917,18 @@\n+\n+const char* ShenandoahControlThread::gc_mode_name(ShenandoahControlThread::GCMode mode) {\n+  switch (mode) {\n+    case none:              return \"idle\";\n+    case concurrent_normal: return \"normal\";\n+    case stw_degenerated:   return \"degenerated\";\n+    case stw_full:          return \"full\";\n+    case marking_old:       return \"old mark\";\n+    default:                return \"unknown\";\n+  }\n+}\n+\n+void ShenandoahControlThread::set_gc_mode(ShenandoahControlThread::GCMode new_mode) {\n+  if (_mode != new_mode) {\n+    log_info(gc)(\"Transition from: %s to: %s\", gc_mode_name(_mode), gc_mode_name(new_mode));\n+    _mode = new_mode;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":363,"deletions":75,"binary":false,"changes":438,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -40,0 +41,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -45,1 +47,1 @@\n-ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point) :\n+ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation) :\n@@ -47,1 +49,3 @@\n-  _degen_point(degen_point) {\n+  _degen_point(degen_point),\n+  _generation(generation),\n+  _upgraded_to_full(false) {\n@@ -63,1 +67,2 @@\n-  const char* msg = degen_event_message(_degen_point);\n+  char msg[1024];\n+  degen_event_message(_degen_point, msg, sizeof(msg));\n@@ -68,0 +73,4 @@\n+  \/\/ In case degenerated GC preempted evacuation or update-refs, clear the aging cycle now.  No harm in clearing it\n+  \/\/ redundantly if it is already clear.  We don't age during degenerated cycles.\n+  heap->set_aging_cycle(false);\n+\n@@ -82,1 +91,9 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n+\n+  \/\/ We can't easily clear the old mark in progress flag because it must be done\n+  \/\/ on a safepoint (not sure if that is a hard requirement). At any rate, once\n+  \/\/ we are in a degenerated cycle, there should be no more old marking.\n+  if (heap->is_concurrent_old_mark_in_progress()) {\n+    heap->old_generation()->cancel_marking();\n+  }\n+  assert(heap->old_generation()->task_queues()->is_empty(), \"Old gen task queues should be empty.\");\n@@ -98,1 +115,11 @@\n-      \/\/\n+\n+      \/\/ Note that we can only do this for \"outside-cycle\" degens, otherwise we would risk\n+      \/\/ changing the cycle parameters mid-cycle during concurrent -> degenerated handover.\n+      heap->set_unload_classes((!heap->mode()->is_generational() || _generation->generation_mode() == GLOBAL) && _generation->heuristics()->can_unload_classes());\n+\n+      if (_generation->generation_mode() == YOUNG || (_generation->generation_mode() == GLOBAL && ShenandoahVerify)) {\n+        \/\/ Swap remembered sets for young, or if the verifier will run during a global collect\n+        _generation->swap_remembered_set();\n+      }\n+\n+    case _degenerated_roots:\n@@ -102,2 +129,1 @@\n-        ShenandoahConcurrentMark::cancel();\n-        heap->set_concurrent_mark_in_progress(false);\n+        heap->cancel_concurrent_mark();\n@@ -106,3 +132,8 @@\n-      \/\/ Note that we can only do this for \"outside-cycle\" degens, otherwise we would risk\n-      \/\/ changing the cycle parameters mid-cycle during concurrent -> degenerated handover.\n-      heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+      if (_degen_point == ShenandoahDegenPoint::_degenerated_roots) {\n+        \/\/ We only need this if the concurrent cycle has already swapped the card tables.\n+        \/\/ Marking will use the 'read' table, but interesting pointers may have been\n+        \/\/ recorded in the 'write' table in the time between the cancelled concurrent cycle\n+        \/\/ and this degenerated cycle. These pointers need to be included the 'read' table\n+        \/\/ used to scan the remembered set during the STW mark which follows here.\n+        _generation->merge_write_table();\n+      }\n@@ -130,0 +161,5 @@\n+\n+      if (heap->mode()->is_generational() && _generation->generation_mode() == GLOBAL) {\n+        op_global_coalesce_and_fill();\n+      }\n+\n@@ -151,1 +187,0 @@\n-\n@@ -196,0 +231,18 @@\n+  if (heap->mode()->is_generational()) {\n+    \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up transient state.\n+    \/\/ Otherwise, these actions have no effect.\n+\n+    heap->young_generation()->unadjust_available();\n+    heap->old_generation()->unadjust_available();\n+    heap->young_generation()->increase_used(heap->get_young_evac_expended());\n+    \/\/ No need to old_gen->increase_used().  That was done when plabs were allocated, accounting for both old evacs and promotions.\n+\n+    heap->set_alloc_supplement_reserve(0);\n+    heap->set_young_evac_reserve(0);\n+    heap->reset_young_evac_expended();\n+    heap->set_old_evac_reserve(0);\n+    heap->reset_old_evac_expended();\n+    heap->set_promotion_reserve(0);\n+\n+  }\n+\n@@ -218,1 +271,1 @@\n-  ShenandoahHeap::heap()->prepare_gc();\n+  _generation->prepare_gc(false);\n@@ -224,2 +277,1 @@\n-  ShenandoahSTWMark mark(false \/*full gc*\/);\n-  mark.clear();\n+  ShenandoahSTWMark mark(_generation, false \/*full gc*\/);\n@@ -230,1 +282,1 @@\n-  ShenandoahConcurrentMark mark;\n+  ShenandoahConcurrentMark mark(_generation);\n@@ -242,0 +294,1 @@\n+\n@@ -243,1 +296,1 @@\n-  heap->prepare_regions_and_collection_set(false \/*concurrent*\/);\n+  _generation->prepare_regions_and_collection_set(false \/*concurrent*\/);\n@@ -277,0 +330,4 @@\n+void ShenandoahDegenGC::op_global_coalesce_and_fill() {\n+  ShenandoahHeap::heap()->coalesce_and_fill_old_regions();\n+}\n+\n@@ -327,3 +384,1 @@\n-  log_info(gc)(\"Cannot finish degeneration, upgrading to Full GC\");\n-  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n-\n+  upgrade_to_full();\n@@ -335,1 +390,1 @@\n-  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n+  upgrade_to_full();\n@@ -340,16 +395,12 @@\n-const char* ShenandoahDegenGC::degen_event_message(ShenandoahDegenPoint point) const {\n-  switch (point) {\n-    case _degenerated_unset:\n-      return \"Pause Degenerated GC (<UNSET>)\";\n-    case _degenerated_outside_cycle:\n-      return \"Pause Degenerated GC (Outside of Cycle)\";\n-    case _degenerated_mark:\n-      return \"Pause Degenerated GC (Mark)\";\n-    case _degenerated_evac:\n-      return \"Pause Degenerated GC (Evacuation)\";\n-    case _degenerated_updaterefs:\n-      return \"Pause Degenerated GC (Update Refs)\";\n-    default:\n-      ShouldNotReachHere();\n-      return \"ERROR\";\n-  }\n+void ShenandoahDegenGC::degen_event_message(ShenandoahDegenPoint point, char* buf, size_t len) const {\n+  jio_snprintf(buf, len, \"Pause Degenerated %s GC (%s)\", _generation->name(), ShenandoahGC::degen_point_to_string(point));\n+}\n+\n+void ShenandoahDegenGC::upgrade_to_full() {\n+  log_info(gc)(\"Degenerate GC upgrading to Full GC\");\n+  ShenandoahHeap::heap()->shenandoah_policy()->record_degenerated_upgrade_to_full();\n+  _upgraded_to_full = true;\n+}\n+\n+bool ShenandoahDegenGC::upgraded_to_full() {\n+  return _upgraded_to_full;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":87,"deletions":36,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -31,0 +32,2 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -64,0 +67,17 @@\n+HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahRegionAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region) {\n+  for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+    size_t idx = c - 1;\n+    if (is_collector_free(idx)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != NULL) {\n+          return result;\n+        }\n+      }\n+    }\n+  }\n+  return NULL;\n+}\n+\n@@ -77,0 +97,2 @@\n+  \/\/ Overwrite with non-zero (non-NULL) values only if necessary for allocation bookkeeping.\n+\n@@ -80,1 +102,0 @@\n-\n@@ -84,0 +105,1 @@\n+          \/\/ try_allocate_in() increases used if the allocation is successful.\n@@ -95,2 +117,2 @@\n-    case ShenandoahAllocRequest::_alloc_shared_gc: {\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      \/\/ GCLABs are for evacuation so we must be in evacuation phase.  If this allocation is successful, increment\n+      \/\/ the relevant evac_expended rather than used value.\n@@ -98,9 +120,13 @@\n-      \/\/ Fast-path: try to allocate in the collector view first\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_collector_free(idx)) {\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n-          if (result != NULL) {\n-            return result;\n-          }\n-        }\n+    case ShenandoahAllocRequest::_alloc_plab:\n+      \/\/ PLABs always reside in old-gen and are only allocated during evacuation phase.\n+\n+    case ShenandoahAllocRequest::_alloc_shared_gc: {\n+      \/\/ First try to fit into a region that is already in use in the same generation.\n+      HeapWord* result = allocate_with_affiliation(req.affiliation(), req, in_new_region);\n+      if (result != NULL) {\n+        return result;\n+      }\n+      \/\/ Then try a free region that is dedicated to GC allocations.\n+      result = allocate_with_affiliation(FREE, req, in_new_region);\n+      if (result != NULL) {\n+        return result;\n@@ -114,1 +140,1 @@\n-      \/\/ Try to steal the empty region from the mutator view\n+      \/\/ Try to steal an empty region from the mutator view.\n@@ -132,1 +158,0 @@\n-\n@@ -138,1 +163,0 @@\n-\n@@ -152,0 +176,27 @@\n+  if (r->affiliation() == ShenandoahRegionAffiliation::FREE) {\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n+\n+    \/\/ Any OLD region allocated during concurrent coalesce-and-fill does not need to be coalesced and filled because\n+    \/\/ all objects allocated within this region are above TAMS (and thus are implicitly marked).  In case this is an\n+    \/\/ OLD region and concurrent preparation for mixed evacuations visits this region before the start of the next\n+    \/\/ old-gen concurrent mark (i.e. this region is allocated following the start of old-gen concurrent mark but before\n+    \/\/ concurrent preparations for mixed evacuations are completed), we mark this region as not requiring any\n+    \/\/ coalesce-and-fill processing.  This code is only necessary if req.affiliation() is OLD, but harmless if not.\n+    r->end_preemptible_coalesce_and_fill();\n+    ctx->capture_top_at_mark_start(r);\n+\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+\n+    \/\/ Leave top_bitmap alone.  The first time a heap region is put into service, top_bitmap should equal end.\n+    \/\/ Thereafter, it should represent the upper bound on parts of the bitmap that need to be cleared.\n+    log_debug(gc)(\"NOT clearing bitmap for region \" SIZE_FORMAT \", top_bitmap: \"\n+                  PTR_FORMAT \" at transition from FREE to %s\",\n+                  r->index(), p2i(ctx->top_bitmap(r)), affiliation_name(req.affiliation()));\n+  } else if (r->affiliation() != req.affiliation()) {\n+    return NULL;\n+  }\n+\n@@ -157,0 +208,1 @@\n+  \/\/ req.size() is in words, r->free() is in bytes.\n@@ -158,3 +210,44 @@\n-    size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n-    if (size > free) {\n-      size = free;\n+    if (req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+      \/\/ Need to assure that plabs are aligned on multiple of card region.\n+      size_t free = r->free();\n+      size_t usable_free = (free \/ CardTable::card_size()) << CardTable::card_shift();\n+      if ((free != usable_free) && (free - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+        \/\/ We'll have to add another card's memory to the padding\n+        usable_free -= CardTable::card_size();\n+      }\n+      free \/= HeapWordSize;\n+      usable_free \/= HeapWordSize;\n+      if (size > usable_free) {\n+        size = usable_free;\n+      }\n+      if (size >= req.min_size()) {\n+        result = r->allocate_aligned(size, req, CardTable::card_size());\n+        if (result != nullptr && free > usable_free) {\n+          \/\/ Account for the alignment padding\n+          size_t padding = (free - usable_free) * HeapWordSize;\n+          increase_used(padding);\n+          assert(r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION, \"All PLABs reside in old-gen\");\n+          _heap->old_generation()->increase_used(padding);\n+          \/\/ For verification consistency, we need to report this padding to _heap\n+          _heap->increase_used(padding);\n+        }\n+      }\n+    } else {\n+      \/\/ This is a GCLAB or a TLAB allocation\n+      size_t free = align_down(r->free() >> LogHeapWordSize, MinObjAlignment);\n+      if (size > free) {\n+        size = free;\n+      }\n+      if (size >= req.min_size()) {\n+        result = r->allocate(size, req);\n+        assert (result != NULL, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+      }\n+    }\n+  } else if (req.is_lab_alloc() && req.type() == ShenandoahAllocRequest::_alloc_plab) {\n+    size_t free = r->free();\n+    size_t usable_free = (free \/ CardTable::card_size()) << CardTable::card_shift();\n+    free \/= HeapWordSize;\n+    usable_free \/= HeapWordSize;\n+    if ((free != usable_free) && (free - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+      \/\/ We'll have to add another card's memory to the padding\n+      usable_free -= CardTable::card_size();\n@@ -162,3 +255,13 @@\n-    if (size >= req.min_size()) {\n-      result = r->allocate(size, req.type());\n-      assert (result != NULL, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+    if (size <= usable_free) {\n+      assert(size % CardTable::card_size_in_words() == 0, \"PLAB size must be multiple of remembered set card size\");\n+\n+      result = r->allocate_aligned(size, req, CardTable::card_size());\n+      if (result != nullptr) {\n+        \/\/ Account for the alignment padding\n+        size_t padding = (free - usable_free) * HeapWordSize;\n+        increase_used(padding);\n+        assert(r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION, \"All PLABs reside in old-gen\");\n+        _heap->old_generation()->increase_used(padding);\n+        \/\/ For verification consistency, we need to report this padding to _heap\n+        _heap->increase_used(padding);\n+      }\n@@ -167,1 +270,1 @@\n-    result = r->allocate(size, req.type());\n+    result = r->allocate(size, req);\n@@ -171,0 +274,3 @@\n+    \/\/ Record actual allocation size\n+    req.set_actual_size(size);\n+\n@@ -173,0 +279,2 @@\n+      \/\/ Mutator allocations always pull from young gen.\n+      _heap->young_generation()->increase_used(size * HeapWordSize);\n@@ -174,1 +282,11 @@\n-    }\n+    } else {\n+      \/\/ assert(req.is_gc_alloc(), \"Should be gc_alloc since req wasn't mutator alloc\");\n+\n+      \/\/ For GC allocations, we advance update_watermark because the objects relocated into this memory during\n+      \/\/ evacuation are not updated during evacuation.  For both young and old regions r, it is essential that all\n+      \/\/ PLABs be made parsable at the end of evacuation.  This is enabled by retiring all plabs at end of evacuation.\n+      \/\/ TODO: Making a PLAB parsable involves placing a filler object in its remnant memory but does not require\n+      \/\/ that the PLAB be disabled for all future purposes.  We may want to introduce a new service to make the\n+      \/\/ PLABs parsable while still allowing the PLAB to serve future allocation requests that arise during the\n+      \/\/ next evacuation pass.\n+      r->set_update_watermark(r->top());\n@@ -176,2 +294,7 @@\n-    \/\/ Record actual allocation size\n-    req.set_actual_size(size);\n+      if (r->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+        \/\/ This is either a GCLAB or it is a shared evacuation allocation.  In either case, we expend young evac.\n+        \/\/ At end of update refs, we'll add expended young evac into young_gen->used.  We hide this usage\n+        \/\/ from current accounting because memory reserved for evacuation is not part of adjusted capacity.\n+        _heap->expend_young_evac(size * HeapWordSize);\n+      } else {\n+        assert(r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION, \"GC Alloc was not YOUNG so must be OLD\");\n@@ -179,2 +302,4 @@\n-    if (req.is_gc_alloc()) {\n-      r->set_update_watermark(r->top());\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+        _heap->old_generation()->increase_used(size * HeapWordSize);\n+        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      }\n@@ -183,1 +308,0 @@\n-\n@@ -189,2 +313,3 @@\n-    \/\/ almost-full regions precede the fully-empty region where we want allocate the entire TLAB.\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations\n+    \/\/ almost-full regions precede the fully-empty region where we want to allocate the entire TLAB.\n+    \/\/ TODO: Record first fully-empty region, and use that for large allocations and\/or organize\n+    \/\/ available free segments within regions for more efficient searches for \"good fit\".\n@@ -197,0 +322,1 @@\n+        _heap->generation_for(req.affiliation())->increase_allocated(waste);\n@@ -285,0 +411,1 @@\n+  ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -308,1 +435,14 @@\n-    r->set_top(r->bottom() + used_words);\n+    r->set_affiliation(req.affiliation());\n+    r->set_update_watermark(r->bottom());\n+    r->set_top(r->bottom());    \/\/ Set top to bottom so we can capture TAMS\n+    ctx->capture_top_at_mark_start(r);\n+    r->set_top(r->bottom() + used_words); \/\/ Then change top to reflect allocation of humongous object.\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+\n+    \/\/ Leave top_bitmap alone.  The first time a heap region is put into service, top_bitmap should equal end.\n+    \/\/ Thereafter, it should represent the upper bound on parts of the bitmap that need to be cleared.\n+    \/\/ ctx->clear_bitmap(r);\n+    log_debug(gc)(\"NOT clearing bitmap for Humongous region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmap: \"\n+                  PTR_FORMAT \" at transition from FREE to %s\",\n+                  p2i(r->bottom()), p2i(r->end()), p2i(ctx->top_bitmap(r)), affiliation_name(req.affiliation()));\n@@ -316,0 +456,5 @@\n+  if (req.affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+    _heap->young_generation()->increase_used(words_size * HeapWordSize);\n+  } else if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+    _heap->old_generation()->increase_used(words_size * HeapWordSize);\n+  }\n@@ -319,1 +464,3 @@\n-    _heap->notify_mutator_alloc_words(ShenandoahHeapRegion::region_size_words() - remainder, true);\n+    size_t waste = ShenandoahHeapRegion::region_size_words() - remainder;\n+    _heap->notify_mutator_alloc_words(waste, true);\n+    _heap->generation_for(req.affiliation())->increase_allocated(waste * HeapWordSize);\n@@ -387,0 +534,4 @@\n+\n+  \/\/ We do not ensure that the region is no longer trash,\n+  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n@@ -409,0 +560,1 @@\n+  log_debug(gc)(\"Rebuilding FreeSet\");\n@@ -422,0 +574,2 @@\n+\n+      log_debug(gc)(\"  Setting Region \" SIZE_FORMAT \" _mutator_free_bitmap bit to true\", idx);\n@@ -426,1 +580,1 @@\n-  size_t to_reserve = _heap->max_capacity() \/ 100 * ShenandoahEvacReserve;\n+  size_t to_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n@@ -439,0 +593,1 @@\n+      log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n@@ -541,0 +696,1 @@\n+  \/\/ Allocation request is known to satisfy all memory budgeting constraints.\n@@ -547,0 +703,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":190,"deletions":33,"binary":false,"changes":223,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -53,0 +54,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -63,0 +65,63 @@\n+\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n+\/\/ registering all objects between bottom() and top(), and setting remembered set cards to\n+\/\/ DIRTY if they hold interesting pointers.\n+class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n+private:\n+  ShenandoahRegionIterator _regions;\n+\n+public:\n+  ShenandoahReconstructRememberedSetTask() :\n+    WorkerTask(\"Shenandoah Reset Bitmap\") { }\n+\n+  void work(uint worker_id) {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahHeapRegion* r = _regions.next();\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    RememberedScanner* scanner = heap->card_scan();\n+    ShenandoahSetRememberedCardsToDirtyClosure dirty_cards_for_interesting_pointers;\n+\n+    while (r != NULL) {\n+      if (r->is_old() && r->is_active()) {\n+        HeapWord* obj_addr = r->bottom();\n+        if (r->is_humongous_start()) {\n+          \/\/ First, clear the remembered set\n+          oop obj = cast_to_oop(obj_addr);\n+          size_t size = obj->size();\n+          HeapWord* end_object = r->bottom() + size;\n+\n+          \/\/ First, clear the remembered set for all spanned humongous regions\n+          size_t num_regions = (size + ShenandoahHeapRegion::region_size_words() - 1) \/ ShenandoahHeapRegion::region_size_words();\n+          size_t region_span = num_regions * ShenandoahHeapRegion::region_size_words();\n+          scanner->reset_remset(r->bottom(), region_span);\n+          size_t region_index = r->index();\n+          ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n+          while (num_regions-- != 0) {\n+            scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n+            region_index++;\n+            humongous_region = heap->get_region(region_index);\n+          }\n+\n+          \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n+          scanner->register_object_wo_lock(obj_addr);\n+          obj->oop_iterate(&dirty_cards_for_interesting_pointers);\n+        } else if (!r->is_humongous()) {\n+          \/\/ First, clear the remembered set\n+          scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+          scanner->reset_object_range(r->bottom(), r->end());\n+\n+          \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n+          HeapWord* t = r->top();\n+          while (obj_addr < t) {\n+            oop obj = cast_to_oop(obj_addr);\n+            size_t size = obj->size();\n+            scanner->register_object_wo_lock(obj_addr);\n+            obj_addr += obj->oop_iterate_size(&dirty_cards_for_interesting_pointers);\n+          }\n+        } \/\/ else, ignore humongous continuation region\n+      }\n+      \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n+      r = _regions.next();\n+    }\n+  }\n+};\n+\n@@ -116,0 +181,15 @@\n+  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n+  heap->set_gc_generation(heap->global_generation());\n+\n+  \/\/ There will be no concurrent allocations during full GC so reset these coordination variables.\n+  heap->young_generation()->unadjust_available();\n+  heap->old_generation()->unadjust_available();\n+  heap->young_generation()->increase_used(heap->get_young_evac_expended());\n+  \/\/ No need to old_gen->increase_used().  That was done when plabs were allocated, accounting for both old evacs and promotions.\n+\n+  heap->set_alloc_supplement_reserve(0);\n+  heap->set_young_evac_reserve(0);\n+  heap->reset_young_evac_expended();\n+  heap->set_old_evac_reserve(0);\n+  heap->reset_old_evac_expended();\n+  heap->set_promotion_reserve(0);\n@@ -159,1 +239,1 @@\n-    \/\/ b. Cancel concurrent mark, if in progress\n+    \/\/ b. Cancel all concurrent marks, if in progress\n@@ -161,2 +241,1 @@\n-      ShenandoahConcurrentGC::cancel();\n-      heap->set_concurrent_mark_in_progress(false);\n+      heap->cancel_concurrent_mark();\n@@ -172,1 +251,1 @@\n-    heap->reset_mark_bitmap();\n+    heap->global_generation()->reset_mark_bitmap();\n@@ -174,1 +253,1 @@\n-    assert(!heap->marking_context()->is_complete(), \"sanity\");\n+    assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n@@ -177,1 +256,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -190,0 +269,1 @@\n+    \/\/ TODO: Do we need to explicitly retire PLABs?\n@@ -232,0 +312,6 @@\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n+      ShenandoahReconstructRememberedSetTask task;\n+      heap->workers()->run_task(&task);\n+    }\n@@ -247,1 +333,5 @@\n-    heap->verifier()->verify_after_fullgc();\n+    if (heap->mode()->is_generational()) {\n+      heap->verifier()->verify_after_generational_fullgc();\n+    } else {\n+      heap->verifier()->verify_after_fullgc();\n+    }\n@@ -268,2 +358,4 @@\n-    _ctx->capture_top_at_mark_start(r);\n-    r->clear_live_data();\n+    if (r->affiliation() != FREE) {\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n@@ -271,0 +363,2 @@\n+\n+  bool is_thread_safe() { return true; }\n@@ -280,1 +374,1 @@\n-  heap->heap_region_iterate(&cl);\n+  heap->parallel_heap_region_iterate(&cl);\n@@ -282,1 +376,1 @@\n-  heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n@@ -284,1 +378,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -288,1 +382,1 @@\n-  ShenandoahSTWMark mark(true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n@@ -293,0 +387,227 @@\n+class ShenandoahPrepareForCompactionTask : public WorkerTask {\n+private:\n+  PreservedMarksSet*        const _preserved_marks;\n+  ShenandoahHeap*           const _heap;\n+  ShenandoahHeapRegionSet** const _worker_slices;\n+  size_t                    const _num_workers;\n+\n+public:\n+  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices,\n+                                     size_t num_workers);\n+\n+  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n+    \/\/ Empty region: get it into the slice to defragment the slice itself.\n+    \/\/ We could have skipped this without violating correctness, but we really\n+    \/\/ want to compact all live regions to the start of the heap, which sometimes\n+    \/\/ means moving them into the fully empty regions.\n+    if (r->is_empty()) return true;\n+\n+    \/\/ Can move the region, and this is not the humongous region. Humongous\n+    \/\/ moves are special cased here, because their moves are handled separately.\n+    return r->is_stw_move_allowed() && !r->is_humongous();\n+  }\n+\n+  void work(uint worker_id);\n+};\n+\n+class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n+private:\n+  ShenandoahPrepareForCompactionTask* _compactor;\n+  PreservedMarks*          const _preserved_marks;\n+  ShenandoahHeap*          const _heap;\n+\n+  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n+  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n+  \/\/ region has not been compacted onto itself.\n+  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n+  int _empty_regions_pos;\n+  ShenandoahHeapRegion*          _old_to_region;\n+  ShenandoahHeapRegion*          _young_to_region;\n+  ShenandoahHeapRegion*          _from_region;\n+  ShenandoahRegionAffiliation    _from_affiliation;\n+  HeapWord*                      _old_compact_point;\n+  HeapWord*                      _young_compact_point;\n+  uint                           _worker_id;\n+\n+public:\n+  ShenandoahPrepareForGenerationalCompactionObjectClosure(ShenandoahPrepareForCompactionTask* compactor,\n+                                                          PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* old_to_region,\n+                                                          ShenandoahHeapRegion* young_to_region, uint worker_id) :\n+      _compactor(compactor),\n+      _preserved_marks(preserved_marks),\n+      _heap(ShenandoahHeap::heap()),\n+      _empty_regions(empty_regions),\n+      _empty_regions_pos(0),\n+      _old_to_region(old_to_region),\n+      _young_to_region(young_to_region),\n+      _from_region(NULL),\n+      _old_compact_point((old_to_region != nullptr)? old_to_region->bottom(): nullptr),\n+      _young_compact_point((young_to_region != nullptr)? young_to_region->bottom(): nullptr),\n+      _worker_id(worker_id) {}\n+\n+  void set_from_region(ShenandoahHeapRegion* from_region) {\n+    _from_region = from_region;\n+    _from_affiliation = from_region->affiliation();\n+    if (_from_region->has_live()) {\n+      if (_from_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+        if (_old_to_region == nullptr) {\n+          _old_to_region = from_region;\n+          _old_compact_point = from_region->bottom();\n+        }\n+      } else {\n+        assert(_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n+        if (_young_to_region == nullptr) {\n+          _young_to_region = from_region;\n+          _young_compact_point = from_region->bottom();\n+        }\n+      }\n+    } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n+  }\n+\n+  void finish() {\n+    finish_old_region();\n+    finish_young_region();\n+  }\n+\n+  void finish_old_region() {\n+    if (_old_to_region != nullptr) {\n+      log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n+                    _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n+      _old_to_region->set_new_top(_old_compact_point);\n+      _old_to_region = nullptr;\n+    }\n+  }\n+\n+  void finish_young_region() {\n+    if (_young_to_region != nullptr) {\n+      log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n+                    _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n+      _young_to_region->set_new_top(_young_compact_point);\n+      _young_to_region = nullptr;\n+    }\n+  }\n+\n+  bool is_compact_same_region() {\n+    return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n+  }\n+\n+  int empty_regions_pos() {\n+    return _empty_regions_pos;\n+  }\n+\n+  void do_object(oop p) {\n+    assert(_from_region != NULL, \"must set before work\");\n+    assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n+           \"Object must reside in _from_region\");\n+    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+\n+    size_t obj_size = p->size();\n+    uint from_region_age = _from_region->age();\n+    uint object_age = p->age();\n+\n+    bool promote_object = false;\n+    if ((_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) &&\n+        (from_region_age + object_age > InitialTenuringThreshold)) {\n+      if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+        finish_old_region();\n+        _old_to_region = nullptr;\n+      }\n+      if (_old_to_region == nullptr) {\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+          _old_to_region = new_to_region;\n+          _old_compact_point = _old_to_region->bottom();\n+          promote_object = true;\n+        }\n+        \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n+        \/\/ we leave promote_object as false, deferring the promotion.\n+      } else {\n+        promote_object = true;\n+      }\n+    }\n+\n+    if (promote_object || (_from_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION)) {\n+      assert(_old_to_region != nullptr, \"_old_to_region should not be NULL when evacuating to OLD region\");\n+      if (_old_compact_point + obj_size > _old_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n+                      p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _old_to_region.\n+        finish_old_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != NULL, \"must not be NULL\");\n+        _old_to_region = new_to_region;\n+        _old_compact_point = _old_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(NULL, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_old_compact_point));\n+      _old_compact_point += obj_size;\n+    } else {\n+      assert(_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION,\n+             \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n+      assert(_young_to_region != nullptr, \"_young_to_region should not be NULL when compacting YOUNG _from_region\");\n+\n+      \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n+      \/\/ tenuring progress.\n+      _heap->increase_object_age(p, from_region_age + 1);\n+\n+      if (_young_compact_point + obj_size > _young_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n+                      p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _young_to_region.\n+        finish_young_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(YOUNG_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != NULL, \"must not be NULL\");\n+        _young_to_region = new_to_region;\n+        _young_compact_point = _young_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(NULL, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_young_compact_point));\n+      _young_compact_point += obj_size;\n+    }\n+  }\n+};\n+\n+\n@@ -321,0 +642,1 @@\n+    assert(!_heap->mode()->is_generational(), \"Generational GC should use different Closure\");\n@@ -366,7 +688,3 @@\n-class ShenandoahPrepareForCompactionTask : public WorkerTask {\n-private:\n-  PreservedMarksSet*        const _preserved_marks;\n-  ShenandoahHeap*           const _heap;\n-  ShenandoahHeapRegionSet** const _worker_slices;\n-public:\n-  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices) :\n+ShenandoahPrepareForCompactionTask::ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n+                                                                       ShenandoahHeapRegionSet **worker_slices,\n+                                                                       size_t num_workers) :\n@@ -374,2 +692,12 @@\n-    _preserved_marks(preserved_marks),\n-    _heap(ShenandoahHeap::heap()), _worker_slices(worker_slices) {\n+    _preserved_marks(preserved_marks), _heap(ShenandoahHeap::heap()),\n+    _worker_slices(worker_slices), _num_workers(num_workers) { }\n+\n+\n+void ShenandoahPrepareForCompactionTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n+  ShenandoahHeapRegionSetIterator it(slice);\n+  ShenandoahHeapRegion* from_region = it.next();\n+  \/\/ No work?\n+  if (from_region == NULL) {\n+    return;\n@@ -379,11 +707,3 @@\n-  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n-    \/\/ Empty region: get it into the slice to defragment the slice itself.\n-    \/\/ We could have skipped this without violating correctness, but we really\n-    \/\/ want to compact all live regions to the start of the heap, which sometimes\n-    \/\/ means moving them into the fully empty regions.\n-    if (r->is_empty()) return true;\n-\n-    \/\/ Can move the region, and this is not the humongous region. Humongous\n-    \/\/ moves are special cased here, because their moves are handled separately.\n-    return r->is_stw_move_allowed() && !r->is_humongous();\n-  }\n+  \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n+  \/\/ Remember empty regions and reuse them as needed.\n+  ResourceMark rm;\n@@ -391,9 +711,1 @@\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n-    ShenandoahHeapRegionSetIterator it(slice);\n-    ShenandoahHeapRegion* from_region = it.next();\n-    \/\/ No work?\n-    if (from_region == NULL) {\n-       return;\n-    }\n+  GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n@@ -401,3 +713,14 @@\n-    \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n-    \/\/ Remember empty regions and reuse them as needed.\n-    ResourceMark rm;\n+  if (_heap->mode()->is_generational()) {\n+    ShenandoahHeapRegion* old_to_region = (from_region->is_old())? from_region: nullptr;\n+    ShenandoahHeapRegion* young_to_region = (from_region->is_young())? from_region: nullptr;\n+    ShenandoahPrepareForGenerationalCompactionObjectClosure cl(this, _preserved_marks->get(worker_id), empty_regions,\n+                                                               old_to_region, young_to_region, worker_id);\n+    while (from_region != NULL) {\n+      assert(is_candidate_region(from_region), \"Sanity\");\n+      log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n+                    worker_id, affiliation_name(from_region->affiliation()),\n+                    from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n+      cl.set_from_region(from_region);\n+      if (from_region->has_live()) {\n+        _heap->marked_object_iterate(from_region, &cl);\n+      }\n@@ -405,1 +728,7 @@\n-    GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n+      \/\/ Compacted the region to somewhere else? From-region is empty then.\n+      if (!cl.is_compact_same_region()) {\n+        empty_regions.append(from_region);\n+      }\n+      from_region = it.next();\n+    }\n+    cl.finish();\n@@ -407,0 +736,6 @@\n+    \/\/ Mark all remaining regions as empty\n+    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n+      ShenandoahHeapRegion* r = empty_regions.at(pos);\n+      r->set_new_top(r->bottom());\n+    }\n+  } else {\n@@ -408,1 +743,0 @@\n-\n@@ -411,1 +745,0 @@\n-\n@@ -431,1 +764,1 @@\n-};\n+}\n@@ -450,0 +783,1 @@\n+  log_debug(gc)(\"Full GC calculating target humongous objects from end \" SIZE_FORMAT, to_end);\n@@ -516,17 +850,22 @@\n-    if (r->is_humongous_start()) {\n-      oop humongous_obj = cast_to_oop(r->bottom());\n-      if (!_ctx->is_marked(humongous_obj)) {\n-        assert(!r->has_live(),\n-               \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n-        _heap->trash_humongous_region_at(r);\n-      } else {\n-        assert(r->has_live(),\n-               \"Region \" SIZE_FORMAT \" should have live\", r->index());\n-      }\n-    } else if (r->is_humongous_continuation()) {\n-      \/\/ If we hit continuation, the non-live humongous starts should have been trashed already\n-      assert(r->humongous_start_region()->has_live(),\n-             \"Region \" SIZE_FORMAT \" should have live\", r->index());\n-    } else if (r->is_regular()) {\n-      if (!r->has_live()) {\n-        r->make_trash_immediate();\n+    if (r->affiliation() != FREE) {\n+      if (r->is_humongous_start()) {\n+        oop humongous_obj = cast_to_oop(r->bottom());\n+        if (!_ctx->is_marked(humongous_obj)) {\n+          assert(!r->has_live(),\n+                 \"Humongous Start %s Region \" SIZE_FORMAT \" is not marked, should not have live\",\n+                 affiliation_name(r->affiliation()),  r->index());\n+          log_debug(gc)(\"Trashing immediate humongous region \" SIZE_FORMAT \" because not marked\", r->index());\n+          _heap->trash_humongous_region_at(r);\n+        } else {\n+          assert(r->has_live(),\n+                 \"Humongous Start %s Region \" SIZE_FORMAT \" should have live\", affiliation_name(r->affiliation()),  r->index());\n+        }\n+      } else if (r->is_humongous_continuation()) {\n+        \/\/ If we hit continuation, the non-live humongous starts should have been trashed already\n+        assert(r->humongous_start_region()->has_live(),\n+               \"Humongous Continuation %s Region \" SIZE_FORMAT \" should have live\", affiliation_name(r->affiliation()),  r->index());\n+      } else if (r->is_regular()) {\n+        if (!r->has_live()) {\n+          log_debug(gc)(\"Trashing immediate regular region \" SIZE_FORMAT \" because has no live\", r->index());\n+          r->make_trash_immediate();\n+        }\n@@ -535,0 +874,2 @@\n+    \/\/ else, ignore this FREE region.\n+    \/\/ TODO: change iterators so they do not process FREE regions.\n@@ -701,0 +1042,5 @@\n+  if (heap->mode()->is_generational()) {\n+    heap->young_generation()->clear_used();\n+    heap->old_generation()->clear_used();\n+  }\n+\n@@ -707,1 +1053,4 @@\n-    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices);\n+    size_t num_workers = heap->max_workers();\n+\n+    ResourceMark rm;\n+    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices, num_workers);\n@@ -917,0 +1266,9 @@\n+    \/\/ Update final usage for generations\n+    if (_heap->mode()->is_generational() && live != 0) {\n+      if (r->is_young()) {\n+        _heap->young_generation()->increase_used(live);\n+      } else if (r->is_old()) {\n+        _heap->old_generation()->increase_used(live);\n+      }\n+    }\n+\n@@ -954,0 +1312,3 @@\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT,\n+                    old_start, new_start);\n+\n@@ -962,0 +1323,1 @@\n+        ShenandoahRegionAffiliation original_affiliation = r->affiliation();\n@@ -971,1 +1333,1 @@\n-            r->make_humongous_start_bypass();\n+            r->make_humongous_start_bypass(original_affiliation);\n@@ -973,1 +1335,1 @@\n-            r->make_humongous_cont_bypass();\n+            r->make_humongous_cont_bypass(original_affiliation);\n@@ -1052,0 +1414,5 @@\n+    if (heap->mode()->is_generational()) {\n+      heap->young_generation()->clear_used();\n+      heap->old_generation()->clear_used();\n+    }\n+\n@@ -1055,0 +1422,4 @@\n+    if (heap->mode()->is_generational()) {\n+      log_info(gc)(\"FullGC done: GLOBAL usage: \" SIZE_FORMAT \", young usage: \" SIZE_FORMAT \", old usage: \" SIZE_FORMAT,\n+                    post_compact.get_live(), heap->young_generation()->used(), heap->old_generation()->used());\n+    }\n@@ -1060,1 +1431,1 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":441,"deletions":70,"binary":false,"changes":511,"status":"modified"},{"patch":"@@ -42,0 +42,2 @@\n+    case _degenerated_roots:\n+      return \"Roots\";\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGC.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -43,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -44,0 +46,1 @@\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n@@ -53,0 +56,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -59,0 +63,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -67,0 +72,2 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -70,0 +77,1 @@\n+\n@@ -74,0 +82,2 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+\n@@ -160,3 +170,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -178,0 +185,4 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_generations();\n+  initialize_heuristics();\n+\n@@ -212,0 +223,25 @@\n+  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this, _heap_region));\n+\n+  \/\/\n+  \/\/ After reserving the Java heap, create the card table, barriers, and workers, in dependency order\n+  \/\/\n+  if (mode()->is_generational()) {\n+    ShenandoahDirectCardMarkRememberedSet *rs;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize) - 1;\n+    rs = new ShenandoahDirectCardMarkRememberedSet(ShenandoahBarrierSet::barrier_set()->card_table(), card_count);\n+    _card_scan = new ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet>(rs);\n+  }\n+\n+  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n+  if (_workers == NULL) {\n+    vm_exit_during_initialization(\"Failed necessary allocation.\");\n+  } else {\n+    _workers->initialize_workers();\n+  }\n+\n+  if (ParallelGCThreads > 1) {\n+    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\", ParallelGCThreads);\n+    _safepoint_workers->initialize_workers();\n+  }\n+\n@@ -255,1 +291,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -401,0 +437,1 @@\n+  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n@@ -407,1 +444,12 @@\n-void ShenandoahHeap::initialize_mode() {\n+void ShenandoahHeap::initialize_generations() {\n+  size_t max_capacity_new      = young_generation_capacity(max_capacity());\n+  size_t soft_max_capacity_new = young_generation_capacity(soft_max_capacity());\n+  size_t max_capacity_old      = max_capacity() - max_capacity_new;\n+  size_t soft_max_capacity_old = soft_max_capacity() - soft_max_capacity_new;\n+\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_new, soft_max_capacity_new);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, soft_max_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_max_workers);\n+}\n+\n+void ShenandoahHeap::initialize_heuristics() {\n@@ -415,0 +463,2 @@\n+    } else if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+      _gc_mode = new ShenandoahGenerationalMode();\n@@ -432,14 +482,4 @@\n-}\n-\n-void ShenandoahHeap::initialize_heuristics() {\n-  assert(_gc_mode != NULL, \"Must be initialized\");\n-  _heuristics = _gc_mode->initialize_heuristics();\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  if (mode()->is_generational()) {\n+    _young_generation->initialize_heuristics(_gc_mode);\n+    _old_generation->initialize_heuristics(_gc_mode);\n@@ -457,0 +497,4 @@\n+  _gc_generation(NULL),\n+  _mixed_evac(false),\n+  _prep_for_mixed_evac_in_progress(false),\n+  _evacuation_allowance(0),\n@@ -460,2 +504,1 @@\n-  _bytes_allocated_since_gc_start(0),\n-  _max_workers(MAX2(ConcGCThreads, ParallelGCThreads)),\n+  _max_workers(MAX3(ConcGCThreads, ParallelGCThreads, 1U)),\n@@ -468,0 +511,12 @@\n+  _alloc_supplement_reserve(0),\n+  _promotion_reserve(0),\n+  _old_evac_reserve(0),\n+  _old_evac_expended(0),\n+  _young_evac_reserve(0),\n+  _young_evac_expended(0),\n+  _captured_old_usage(0),\n+  _previous_promotion(0),\n+  _cancel_requested_time(0),\n+  _young_generation(NULL),\n+  _global_generation(NULL),\n+  _old_generation(NULL),\n@@ -469,0 +524,1 @@\n+  _regulator_thread(NULL),\n@@ -470,2 +526,0 @@\n-  _gc_mode(NULL),\n-  _heuristics(NULL),\n@@ -478,0 +532,2 @@\n+  _young_gen_memory_pool(NULL),\n+  _old_gen_memory_pool(NULL),\n@@ -483,1 +539,0 @@\n-  _ref_processor(new ShenandoahReferenceProcessor(MAX2(_max_workers, 1U))),\n@@ -491,1 +546,2 @@\n-  _collection_set(NULL)\n+  _collection_set(NULL),\n+  _card_scan(NULL)\n@@ -493,17 +549,0 @@\n-  \/\/ Initialize GC mode early, so we can adjust barrier support\n-  initialize_mode();\n-  BarrierSet::set_barrier_set(new ShenandoahBarrierSet(this));\n-\n-  _max_workers = MAX2(_max_workers, 1U);\n-  _workers = new ShenandoahWorkerThreads(\"Shenandoah GC Threads\", _max_workers);\n-  if (_workers == NULL) {\n-    vm_exit_during_initialization(\"Failed necessary allocation.\");\n-  } else {\n-    _workers->initialize_workers();\n-  }\n-\n-  if (ParallelGCThreads > 1) {\n-    _safepoint_workers = new ShenandoahWorkerThreads(\"Safepoint Cleanup Thread\",\n-                                                ParallelGCThreads);\n-    _safepoint_workers->initialize_workers();\n-  }\n@@ -516,29 +555,0 @@\n-class ShenandoahResetBitmapTask : public WorkerTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    WorkerTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != NULL) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -559,1 +569,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -622,2 +633,0 @@\n-  _heuristics->initialize();\n-\n@@ -627,0 +636,39 @@\n+\n+ShenandoahOldHeuristics* ShenandoahHeap::old_heuristics() {\n+  return (ShenandoahOldHeuristics*) _old_generation->heuristics();\n+}\n+\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return old_heuristics()->unprocessed_old_collection_candidates() > 0;\n+}\n+\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != NULL && _gc_generation->generation_mode() == YOUNG;\n+}\n+\n+\/\/ There are three JVM parameters for setting young gen capacity:\n+\/\/    NewSize, MaxNewSize, NewRatio.\n+\/\/\n+\/\/ If only NewSize is set, it assigns a fixed size and the other two parameters are ignored.\n+\/\/ Otherwise NewRatio applies.\n+\/\/\n+\/\/ If NewSize is set in any combination, it provides a lower bound.\n+\/\/\n+\/\/ If MaxNewSize is set it provides an upper bound.\n+\/\/ If this bound is smaller than NewSize, it supersedes,\n+\/\/ resulting in a fixed size given by MaxNewSize.\n+size_t ShenandoahHeap::young_generation_capacity(size_t capacity) {\n+  if (FLAG_IS_CMDLINE(NewSize) && !FLAG_IS_CMDLINE(MaxNewSize) && !FLAG_IS_CMDLINE(NewRatio)) {\n+    capacity = MIN2(NewSize, capacity);\n+  } else {\n+    capacity \/= NewRatio + 1;\n+    if (FLAG_IS_CMDLINE(NewSize)) {\n+      capacity = MAX2(NewSize, capacity);\n+    }\n+    if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+      capacity = MIN2(MaxNewSize, capacity);\n+    }\n+  }\n+  return capacity;\n+}\n+\n@@ -658,4 +706,0 @@\n-void ShenandoahHeap::increase_allocated(size_t bytes) {\n-  Atomic::add(&_bytes_allocated_since_gc_start, bytes, memory_order_relaxed);\n-}\n-\n@@ -667,1 +711,1 @@\n-  increase_allocated(bytes);\n+\n@@ -697,0 +741,7 @@\n+\n+  if (mode()->is_generational()) {\n+    size_t soft_max_capacity_young = young_generation_capacity(_soft_max_size);\n+    size_t soft_max_capacity_old = _soft_max_size - soft_max_capacity_young;\n+    _young_generation->set_soft_max_capacity(soft_max_capacity_young);\n+    _old_generation->set_soft_max_capacity(soft_max_capacity_old);\n+  }\n@@ -713,0 +764,23 @@\n+bool ShenandoahHeap::is_in_young(const void* p) const {\n+  return is_in(p) && heap_region_containing(p)->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION;\n+}\n+\n+bool ShenandoahHeap::is_in_old(const void* p) const {\n+  return is_in(p) && heap_region_containing(p)->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION;\n+}\n+\n+bool ShenandoahHeap::is_in_active_generation(oop obj) const {\n+  if (!mode()->is_generational()) {\n+    \/\/ everything is the same single generation\n+    return true;\n+  }\n+\n+  if (active_generation() == NULL) {\n+    \/\/ no collection is happening, only expect this to be called\n+    \/\/ when concurrent processing is active, but that could change\n+    return false;\n+  }\n+\n+  return active_generation()->contains(obj);\n+}\n+\n@@ -740,0 +814,17 @@\n+    regulator_thread()->notify_heap_changed();\n+  }\n+}\n+\n+void ShenandoahHeap::handle_old_evacuation(HeapWord* obj, size_t words, bool promotion) {\n+  \/\/ Only register the copy of the object that won the evacuation race.\n+  card_scan()->register_object_wo_lock(obj);\n+\n+  \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+  \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+  \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+  \/\/ that hold interesting pointers right now.\n+  card_scan()->mark_range_as_dirty(obj, words);\n+\n+  if (promotion) {\n+    \/\/ This evacuation was a promotion, track this as allocation against old gen\n+    old_generation()->increase_allocated(words * HeapWordSize);\n@@ -743,0 +834,10 @@\n+void ShenandoahHeap::handle_old_evacuation_failure() {\n+  if (_old_gen_oom_evac.try_set()) {\n+    log_info(gc)(\"Old gen evac failure.\");\n+  }\n+}\n+\n+void ShenandoahHeap::handle_promotion_failure() {\n+  old_heuristics()->handle_promotion_failure();\n+}\n+\n@@ -792,0 +893,119 @@\n+\/\/ Establish a new PLAB and allocate size HeapWords within it.\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion) {\n+  \/\/ New object should fit the PLAB size\n+  size_t min_size = MAX2(size, PLAB::min_size());\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.\n+  size_t new_size = ShenandoahThreadLocalData::plab_size(thread) * 2;\n+  new_size = MIN2(new_size, PLAB::max_size());\n+  new_size = MAX2(new_size, PLAB::min_size());\n+\n+  size_t unalignment = new_size % CardTable::card_size_in_words();\n+  if (unalignment != 0) {\n+    new_size = new_size - unalignment + CardTable::card_size_in_words();\n+  }\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.  Note that the requested new_size may\n+  \/\/ not be honored, but we remember that this is the preferred size.\n+  ShenandoahThreadLocalData::set_plab_size(thread, new_size);\n+\n+  if (new_size < size) {\n+    \/\/ New size still does not fit the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs, when we encounter a large object.\n+    return NULL;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.  This\n+  \/\/ is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each PLAB is\n+  \/\/ aligned with the start of a card's memory range.\n+  retire_plab(plab);\n+\n+  size_t actual_size = 0;\n+  \/\/ allocate_new_plab resets plab_evacuated and plab_promoted and disables promotions if old-gen available is\n+  \/\/ less than the remaining evacuation need.\n+  HeapWord* plab_buf = allocate_new_plab(min_size, new_size, &actual_size);\n+  if (plab_buf == NULL) {\n+    return NULL;\n+  }\n+\n+  assert (size <= actual_size, \"allocation should fit\");\n+\n+  if (ZeroTLAB) {\n+    \/\/ ..and clear it.\n+    Copy::zero_to_words(plab_buf, actual_size);\n+  } else {\n+    \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+    \/\/ Skip mangling the space corresponding to the object header to\n+    \/\/ ensure that the returned space is not considered parsable by\n+    \/\/ any concurrent GC thread.\n+    size_t hdr_size = oopDesc::header_size();\n+    Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+  }\n+  plab->set_buf(plab_buf, actual_size);\n+\n+  if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    return nullptr;\n+  }\n+  return plab->allocate(size);\n+}\n+\n+\/\/ TODO: It is probably most efficient to register all objects (both promotions and evacuations) that were allocated within\n+\/\/ this plab at the time we retire the plab.  A tight registration loop will run within both code and data caches.  This change\n+\/\/ would allow smaller and faster in-line implementation of alloc_from_plab().  Since plabs are aligned on card-table boundaries,\n+\/\/ this object registration loop can be performed without acquiring a lock.\n+void ShenandoahHeap::retire_plab(PLAB* plab) {\n+  if (!mode()->is_generational()) {\n+    plab->retire();\n+  } else {\n+    Thread* thread = Thread::current();\n+    size_t evacuated = ShenandoahThreadLocalData::get_plab_evacuated(thread);\n+    \/\/ We don't enforce limits on get_plab_promoted(thread).  Promotion uses any memory not required for evacuation.\n+    expend_old_evac(evacuated);\n+    size_t waste = plab->waste();\n+    HeapWord* top = plab->top();\n+    plab->retire();\n+    if (top != NULL && plab->waste() > waste && is_in_old(top)) {\n+      \/\/ If retiring the plab created a filler object, then we\n+      \/\/ need to register it with our card scanner so it can\n+      \/\/ safely walk the region backing the plab.\n+      log_debug(gc)(\"retire_plab() is registering remnant of size \" SIZE_FORMAT \" at \" PTR_FORMAT,\n+                    plab->waste() - waste, p2i(top));\n+      card_scan()->register_object_wo_lock(top);\n+    }\n+  }\n+}\n+\n+void ShenandoahHeap::cancel_mixed_collections() {\n+  assert(_old_generation != NULL, \"Should only have mixed collections in generation mode.\");\n+  old_heuristics()->abandon_collection_candidates();\n+}\n+\n+void ShenandoahHeap::coalesce_and_fill_old_regions() {\n+  class ShenandoahGlobalCoalesceAndFill : public ShenandoahHeapRegionClosure {\n+   public:\n+    virtual void heap_region_do(ShenandoahHeapRegion* region) override {\n+      \/\/ old region is not in the collection set and was not immediately trashed\n+      if (region->is_old() && region->is_active() && !region->is_humongous()) {\n+        \/\/ Reset the coalesce and fill boundary because this is a global collect\n+        \/\/ and cannot be preempted by young collects. We want to be sure the entire\n+        \/\/ region is coalesced here and does not resume from a previously interrupted\n+        \/\/ or completed coalescing.\n+        region->begin_preemptible_coalesce_and_fill();\n+        region->oop_fill_and_coalesce();\n+      }\n+    }\n+\n+    virtual bool is_thread_safe() override {\n+      return true;\n+    }\n+  };\n+  ShenandoahGlobalCoalesceAndFill coalesce;\n+  parallel_heap_region_iterate(&coalesce);\n+}\n+\n@@ -796,1 +1016,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -809,1 +1029,1 @@\n-  HeapWord* res = allocate_memory(req);\n+  HeapWord* res = allocate_memory(req, false);\n@@ -818,1 +1038,18 @@\n-HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req) {\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size,\n+                                            size_t word_size,\n+                                            size_t* actual_size) {\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  \/\/ Note that allocate_memory() sets a thread-local flag to prohibit further promotions by this thread\n+  \/\/ if we are at risk of exceeding the old-gen evacuation budget.\n+  HeapWord* res = allocate_memory(req, false);\n+  if (res != NULL) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  return res;\n+}\n+\n+\/\/ is_promotion is true iff this allocation is known for sure to hold the result of young-gen evacuation\n+\/\/ to old-gen.  plab allocates arre not known as such, since they may hold old-gen evacuations.\n+HeapWord* ShenandoahHeap::allocate_memory(ShenandoahAllocRequest& req, bool is_promotion) {\n@@ -830,1 +1067,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -848,1 +1085,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -854,1 +1091,1 @@\n-      result = allocate_memory_under_lock(req, in_new_region);\n+      result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -859,1 +1096,1 @@\n-    result = allocate_memory_under_lock(req, in_new_region);\n+    result = allocate_memory_under_lock(req, in_new_region, is_promotion);\n@@ -866,0 +1103,1 @@\n+    regulator_thread()->notify_heap_changed();\n@@ -869,0 +1107,1 @@\n+    ShenandoahGeneration* alloc_generation = generation_for(req.affiliation());\n@@ -871,0 +1110,1 @@\n+    size_t actual_bytes = actual * HeapWordSize;\n@@ -878,0 +1118,1 @@\n+      alloc_generation->increase_allocated(actual_bytes);\n@@ -886,1 +1127,1 @@\n-      increase_used(actual*HeapWordSize);\n+      increase_used(actual_bytes);\n@@ -893,1 +1134,3 @@\n-HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region) {\n+HeapWord* ShenandoahHeap::allocate_memory_under_lock(ShenandoahAllocRequest& req, bool& in_new_region, bool is_promotion) {\n+  size_t requested_bytes = req.size() * HeapWordSize;\n+\n@@ -895,1 +1138,88 @@\n-  return _free_set->allocate(req, in_new_region);\n+  if (mode()->is_generational()) {\n+    if (req.affiliation() == YOUNG_GENERATION) {\n+      if (req.type() == ShenandoahAllocRequest::_alloc_gclab) {\n+        if (requested_bytes + get_young_evac_expended() > get_young_evac_reserve()) {\n+          \/\/ This should only happen if evacuation waste is too low.  Rejecting one thread's request for GCLAB does not\n+          \/\/ necessarily result in failure of the evacuation effort.  A different thread may be able to copy from-space object.\n+\n+          \/\/ TODO: Should we really fail here in the case that there is sufficient memory to allow us to allocate a gclab\n+          \/\/ beyond the young_evac_reserve?  Seems it would be better to take away from mutator allocation budget if this\n+          \/\/ prevents fall-back to full GC in order to recover from failed evacuation.\n+          return nullptr;\n+        }\n+        \/\/ else, there is sufficient memory to allocate this GCLAB so do nothing here.\n+      } else if (req.is_gc_alloc()) {\n+        \/\/ This is a shared alloc for purposes of evacuation.\n+        if (requested_bytes + get_young_evac_expended() > get_young_evac_reserve()) {\n+          \/\/ TODO: Should we really fail here in the case that there is sufficient memory to allow us to allocate a gclab\n+          \/\/ beyond the young_evac_reserve?  Seems it would be better to take away from mutator allocation budget if this\n+          \/\/ prevents fall-back to full GC in order to recover from failed evacuation.\n+          return nullptr;\n+        } else {\n+          \/\/ There is sufficient memory to allocate this shared evacuation object.\n+        }\n+      }  else if (requested_bytes >= young_generation()->adjusted_available()) {\n+        \/\/ We know this is not a GCLAB.  This must be a TLAB or a shared allocation.  Reject the allocation request if\n+        \/\/ exceeds established capacity limits.\n+        return nullptr;\n+      }\n+    } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+      assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n+\n+      if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+        \/\/ We've already retired this thread's previously exhausted PLAB and have accounted for how that PLAB's\n+        \/\/ memory was allotted.\n+        Thread* thread = Thread::current();\n+        ShenandoahThreadLocalData::reset_plab_evacuated(thread);\n+        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+\n+        \/\/ Conservatively, assume this entire PLAB will be used for promotion.  Act as if we need to serve the\n+        \/\/ rest of evacuation need from as-yet unallocated old-gen memory.\n+        size_t remaining_evac_need = get_old_evac_reserve() - get_old_evac_expended();\n+        size_t evac_available = old_generation()->adjusted_available() - requested_bytes;\n+        if (remaining_evac_need >= evac_available) {\n+          \/\/ Disable promotions within this thread because the entirety of this PLAB must be available to hold\n+          \/\/ old-gen evacuations.\n+          ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+        } else {\n+          ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+        }\n+      } else if (is_promotion) {\n+        \/\/ This is a shared alloc for promotion\n+        Thread* thread = Thread::current();\n+        size_t remaining_evac_need = get_old_evac_reserve() - get_old_evac_expended();\n+        size_t evac_available = old_generation()->adjusted_available() - requested_bytes;\n+        if (remaining_evac_need >= evac_available) {\n+          return nullptr;       \/\/ We need to reserve the remaining memory for evacuation so defer the promotion\n+        }\n+        \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+      } else {\n+        \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n+      }\n+    }\n+  }\n+\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+  if (result != NULL) {\n+    if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+      \/\/\n+      \/\/ objects being \"concurrently\" allocated:\n+      \/\/    [-----a------][-----b-----][--------------c------------------]\n+      \/\/            [---- card table memory range --------------]\n+      \/\/\n+      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+      \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+      \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+      \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+      \/\/\n+      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+      \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+      \/\/ invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+      ShenandoahHeap::heap()->card_scan()->register_object(result);\n+    }\n+  }\n+  return result;\n@@ -901,1 +1231,1 @@\n-  return allocate_memory(req);\n+  return allocate_memory(req, false);\n@@ -910,2 +1240,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -990,0 +1320,1 @@\n+\n@@ -995,0 +1326,60 @@\n+      if (_sh->check_cancelled_gc_and_yield(_concurrent)) {\n+        break;\n+      }\n+    }\n+  }\n+};\n+\n+\/\/ Unlike ShenandoahEvacuationTask, this iterates over all regions rather than just the collection set.\n+\/\/ This is needed in order to promote humongous start regions if age() >= tenure threshold.\n+class ShenandoahGenerationalEvacuationTask : public WorkerTask {\n+private:\n+  ShenandoahHeap* const _sh;\n+  ShenandoahRegionIterator *_regions;\n+  bool _concurrent;\n+public:\n+  ShenandoahGenerationalEvacuationTask(ShenandoahHeap* sh,\n+                                       ShenandoahRegionIterator* iterator,\n+                                       bool concurrent) :\n+    WorkerTask(\"Shenandoah Evacuation\"),\n+    _sh(sh),\n+    _regions(iterator),\n+    _concurrent(concurrent)\n+  {}\n+\n+  void work(uint worker_id) {\n+    if (_concurrent) {\n+      ShenandoahConcurrentWorkerSession worker_session(worker_id);\n+      ShenandoahSuspendibleThreadSetJoiner stsj(ShenandoahSuspendibleWorkers);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    } else {\n+      ShenandoahParallelWorkerSession worker_session(worker_id);\n+      ShenandoahEvacOOMScope oom_evac_scope;\n+      do_work();\n+    }\n+  }\n+\n+private:\n+  void do_work() {\n+    ShenandoahConcurrentEvacuateRegionObjectClosure cl(_sh);\n+    ShenandoahHeapRegion* r;\n+    while ((r = _regions->next()) != nullptr) {\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s]\",\n+                    r->is_old()? \"old\": r->is_young()? \"young\": \"free\", r->index(), r->age(),\n+                    r->is_active()? \"active\": \"inactive\",\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\");\n+      if (r->is_cset()) {\n+        assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+        _sh->marked_object_iterate(r, &cl);\n+        if (ShenandoahPacing) {\n+          _sh->pacer()->report_evac(r->used() >> LogHeapWordSize);\n+        }\n+      } else if (r->is_young() && r->is_active() && r->is_humongous_start() && (r->age() > InitialTenuringThreshold)) {\n+        \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+        \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+        \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+        r->promote_humongous();\n+      }\n+      \/\/ else, region is free, or OLD, or not in collection set, or humongous_continuation,\n+      \/\/ or is young humongous_start that is too young to be promoted\n@@ -1004,2 +1395,8 @@\n-  ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n-  workers()->run_task(&task);\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    ShenandoahRegionIterator regions;\n+    ShenandoahGenerationalEvacuationTask task(this, &regions, concurrent);\n+    workers()->run_task(&task);\n+  } else {\n+    ShenandoahEvacuationTask task(this, _collection_set, concurrent);\n+    workers()->run_task(&task);\n+  }\n@@ -1032,1 +1429,1 @@\n-void ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n+size_t ShenandoahHeap::trash_humongous_region_at(ShenandoahHeapRegion* start) {\n@@ -1052,0 +1449,1 @@\n+  return required_regions;\n@@ -1061,0 +1459,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != NULL, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1076,0 +1478,12 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != NULL, \"PLAB should be initialized for %s\", thread->name());\n+    \/\/ TODO; Retiring a PLAB disables it so it cannot support future allocations.  This is overkill.  For old-gen\n+    \/\/ regions, the important thing is to make the memory parsable by the remembered-set scanning code that drives\n+    \/\/ the update-refs processing that follows.  After the updating of old-gen references is done, it is ok to carve\n+    \/\/ this remnant object into smaller pieces during the subsequent evacuation pass, as long as the PLAB is made parsable\n+    \/\/ again before the next update-refs phase.\n+    ShenandoahHeap::heap()->retire_plab(plab);\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n@@ -1133,0 +1547,31 @@\n+class ShenandoahTagGCLABClosure : public ThreadClosure {\n+public:\n+  void do_thread(Thread* thread) {\n+    PLAB* gclab = ShenandoahThreadLocalData::gclab(thread);\n+    assert(gclab != NULL, \"GCLAB should be initialized for %s\", thread->name());\n+    if (gclab->words_remaining() > 0) {\n+      ShenandoahHeapRegion* r = ShenandoahHeap::heap()->heap_region_containing(gclab->allocate(0));\n+      r->set_young_lab_flag();\n+    }\n+  }\n+};\n+\n+void ShenandoahHeap::set_young_lab_region_flags() {\n+  if (!UseTLAB) {\n+    return;\n+  }\n+  for (size_t i = 0; i < _num_regions; i++) {\n+    _regions[i]->clear_young_lab_flags();\n+  }\n+  ShenandoahTagGCLABClosure cl;\n+  workers()->threads_do(&cl);\n+  for (JavaThreadIteratorWithHandle jtiwh; JavaThread *t = jtiwh.next(); ) {\n+    cl.do_thread(t);\n+    ThreadLocalAllocBuffer& tlab = t->tlab();\n+    if (tlab.end() != NULL) {\n+      ShenandoahHeapRegion* r = heap_region_containing(tlab.start());\n+      r->set_young_lab_flag();\n+    }\n+  }\n+}\n+\n@@ -1529,23 +1974,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1567,99 +1989,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1676,1 +1999,1 @@\n-  ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n+  active_generation()->ref_processor()->process_references(phase, workers(), false \/* concurrent *\/);\n@@ -1707,4 +2030,48 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n-  assert(!has_forwarded_objects(), \"Not expected before\/after mark phase\");\n-  set_gc_state_mask(MARKING, in_progress);\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(YOUNG_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(YOUNG_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(OLD_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(OLD_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_prep_for_mixed_evacuation_in_progress(bool in_progress) {\n+  \/\/ Unlike other set-gc-state functions, this may happen outside safepoint.\n+  \/\/ Is only set and queried by control thread, so no coherence issues.\n+  _prep_for_mixed_evac_in_progress = in_progress;\n+}\n+\n+bool ShenandoahHeap::is_concurrent_prep_for_mixed_evacuation_in_progress() {\n+  return _prep_for_mixed_evac_in_progress;\n+}\n+\n+void ShenandoahHeap::set_aging_cycle(bool in_progress) {\n+  _is_aging_cycle.set_cond(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  }\n@@ -1755,0 +2122,8 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+}\n+\n@@ -1760,0 +2135,4 @@\n+    _cancel_requested_time = os::elapsedTime();\n+    if (cause == GCCause::_shenandoah_upgrade_to_full_gc) {\n+      _upgraded_to_full = true;\n+    }\n@@ -1770,0 +2149,3 @@\n+  \/\/ Step 0a. Stop requesting collections.\n+  regulator_thread()->stop();\n+\n@@ -1874,4 +2256,0 @@\n-address ShenandoahHeap::cancelled_gc_addr() {\n-  return (address) ShenandoahHeap::heap()->_cancelled_gc.addr_of();\n-}\n-\n@@ -1882,5 +2260,6 @@\n-size_t ShenandoahHeap::bytes_allocated_since_gc_start() {\n-  return Atomic::load(&_bytes_allocated_since_gc_start);\n-}\n-\n-  Atomic::store(&_bytes_allocated_since_gc_start, (size_t)0);\n+  if (mode()->is_generational()) {\n+    young_generation()->reset_bytes_allocated_since_gc_start();\n+    old_generation()->reset_bytes_allocated_since_gc_start();\n+  }\n+\n+  global_generation()->reset_bytes_allocated_since_gc_start();\n@@ -1956,2 +2335,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2017,0 +2398,2 @@\n+  bool _mixed_evac;             \/\/ true iff most recent evacuation includes old-gen HeapRegions\n+\n@@ -2018,1 +2401,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions, bool mixed_evac) :\n@@ -2021,1 +2404,3 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _mixed_evac(mixed_evac)\n+  {\n@@ -2028,1 +2413,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2031,1 +2416,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2037,1 +2422,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2040,1 +2425,5 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+\n+    \/\/ We update references for global, old, and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n+    bool is_mixed = _heap->collection_set()->has_old_regions();\n@@ -2044,0 +2433,2 @@\n+\n+      log_debug(gc)(\"ShenandoahUpdateHeapRefsTask::do_work(%u) looking at region \" SIZE_FORMAT, worker_id, r->index());\n@@ -2045,1 +2436,67 @@\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        if (!_heap->mode()->is_generational() || (r->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION)) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        } else if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          if (_heap->active_generation()->generation_mode() == GLOBAL) {\n+            _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+          } else {\n+            \/\/ Old region in a young cycle or mixed cycle.\n+            if (!_mixed_evac) {\n+              \/\/ This is a young evac..\n+              _heap->card_scan()->process_region(r, &cl, true);\n+            } else {\n+              \/\/ This is a _mixed_evac.\n+              \/\/\n+              \/\/ TODO: For _mixed_evac, consider building an old-gen remembered set that allows restricted updating\n+              \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+              \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+              \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+              \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+              \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+              \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+              \/\/ old-gen heap regions.\n+              if (r->is_humongous()) {\n+                r->oop_iterate_humongous(&cl);\n+              } else {\n+                \/\/ This is a mixed evacuation.  Old regions that are candidates for collection have not been coalesced\n+                \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+                \/\/\n+                \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+                \/\/ regions which are in the collection set for a particular mixed evacuation.\n+                HeapWord *p = r->bottom();\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, p, update_watermark);\n+\n+                \/\/ Anything beyond update_watermark was allocated during evacuation.  Thus, it is known to not hold\n+                \/\/ references to collection set objects.\n+                while (p < update_watermark) {\n+                  oop obj = cast_to_oop(p);\n+                  if (ctx->is_marked(obj)) {\n+                    objs.do_object(obj);\n+                    p += obj->size();\n+                  } else {\n+                    \/\/ This object is not marked so we don't scan it.\n+                    HeapWord* tams = ctx->top_at_mark_start(r);\n+                    if (p >= tams) {\n+                      p += obj->size();\n+                    } else {\n+                      p = ctx->get_next_marked_addr(p, tams);\n+                    }\n+                  }\n+                }\n+              }\n+            }\n+          }\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 affiliation_name(r->affiliation()), r->index());\n+        }\n@@ -2062,1 +2519,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, _mixed_evac);\n@@ -2065,1 +2522,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, _mixed_evac);\n@@ -2073,0 +2530,1 @@\n+  ShenandoahMarkingContext* _ctx;\n@@ -2074,0 +2532,1 @@\n+  bool _is_generational;\n@@ -2076,1 +2535,3 @@\n-  ShenandoahFinalUpdateRefsUpdateRegionStateClosure() : _lock(ShenandoahHeap::heap()->lock()) {}\n+  ShenandoahFinalUpdateRefsUpdateRegionStateClosure(\n+    ShenandoahMarkingContext* ctx) : _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()),\n+                                     _is_generational(ShenandoahHeap::heap()->mode()->is_generational()) { }\n@@ -2079,0 +2540,21 @@\n+\n+    \/\/ Maintenance of region age must follow evacuation in order to account for evacuation allocations within survivor\n+    \/\/ regions.  We consult region age during the subsequent evacuation to determine whether certain objects need to\n+    \/\/ be promoted.\n+    if (_is_generational && r->is_young()) {\n+      HeapWord *tams = _ctx->top_at_mark_start(r);\n+      HeapWord *top = r->top();\n+\n+      \/\/ Allocations move the watermark when top moves.  However compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top).\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else if (ShenandoahHeap::heap()->is_aging_cycle()) {\n+        r->increment_age();\n+      }\n+    }\n+\n@@ -2081,1 +2563,0 @@\n-\n@@ -2108,1 +2589,1 @@\n-    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl;\n+    ShenandoahFinalUpdateRefsUpdateRegionStateClosure cl (young_generation()->complete_marking_context());\n@@ -2242,3 +2723,12 @@\n-  _memory_pool = new ShenandoahMemoryPool(this);\n-  _cycle_memory_manager.add_pool(_memory_pool);\n-  _stw_memory_manager.add_pool(_memory_pool);\n+  if (mode()->is_generational()) {\n+    _young_gen_memory_pool = new ShenandoahYoungGenMemoryPool(this);\n+    _old_gen_memory_pool = new ShenandoahOldGenMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_young_gen_memory_pool);\n+    _cycle_memory_manager.add_pool(_old_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_young_gen_memory_pool);\n+    _stw_memory_manager.add_pool(_old_gen_memory_pool);\n+  } else {\n+    _memory_pool = new ShenandoahMemoryPool(this);\n+    _cycle_memory_manager.add_pool(_memory_pool);\n+    _stw_memory_manager.add_pool(_memory_pool);\n+  }\n@@ -2256,1 +2746,6 @@\n-  memory_pools.append(_memory_pool);\n+  if (mode()->is_generational()) {\n+    memory_pools.append(_young_gen_memory_pool);\n+    memory_pools.append(_old_gen_memory_pool);\n+  } else {\n+    memory_pools.append(_memory_pool);\n+  }\n@@ -2261,1 +2756,1 @@\n-  return _memory_pool->get_memory_usage();\n+  return MemoryUsage(_initial_size, used(), committed(), max_capacity());\n@@ -2299,0 +2794,1 @@\n+\n@@ -2308,0 +2804,206 @@\n+\n+void ShenandoahHeap::purge_old_satb_buffers(bool abandon) {\n+  ((ShenandoahOldGeneration*)_old_generation)->purge_satb_buffers(abandon);\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<YOUNG>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit young and free regions\n+  if (region->affiliation() != OLD_GENERATION) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<OLD>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit old and free regions\n+  if (region->affiliation() != YOUNG_GENERATION) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.\n+\/\/ This examines the read_card_table between bottom() and top() since all PLABS are retired\n+\/\/ before the safepoint for init_mark.  Actually, we retire them before update-references and don't\n+\/\/ restore them until the start of evacuation.\n+void ShenandoahHeap::verify_rem_set_at_mark() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(true);\n+  ShenandoahMarkingContext* ctx;\n+\n+  log_debug(gc)(\"Verifying remembered set at %s mark\", doing_mixed_evacuations()? \"mixed\": \"young\");\n+\n+  if (doing_mixed_evacuations() ||\n+      is_concurrent_prep_for_mixed_evacuation_in_progress() || active_generation()->generation_mode() == GLOBAL) {\n+    ctx = complete_marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && r->is_active()) {\n+      HeapWord* obj_addr = r->bottom();\n+      if (r->is_humongous_start()) {\n+        oop obj = cast_to_oop(obj_addr);\n+        if (!ctx || ctx->is_marked(obj)) {\n+          \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+          \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+          \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+          if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+            obj->oop_iterate(&check_interesting_pointers);\n+          }\n+          \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+        }\n+        \/\/ else, this humongous object is not marked so no need to verify its internal pointers\n+        if (!scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                          \"Verify init-mark remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+      } else if (!r->is_humongous()) {\n+        HeapWord* top = r->top();\n+        while (obj_addr < top) {\n+          oop obj = cast_to_oop(obj_addr);\n+          \/\/ ctx->is_marked() returns true if mark bit set (TAMS not relevant during init mark)\n+          if (!ctx || ctx->is_marked(obj)) {\n+            \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+            \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+            if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+              obj->oop_iterate(&check_interesting_pointers);\n+            }\n+            \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+            if (!scanner->verify_registration(obj_addr, ctx)) {\n+              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                            \"Verify init-mark remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+            }\n+            obj_addr += obj->size();\n+          } else {\n+            \/\/ This object is not live so we don't verify dirty cards contained therein\n+            assert(ctx->top_at_mark_start(r) == top, \"Expect tams == top at start of mark.\");\n+            obj_addr = ctx->get_next_marked_addr(obj_addr, top);\n+          }\n+        }\n+      } \/\/ else, we ignore humongous continuation region\n+    } \/\/ else, this is not an OLD region so we ignore it\n+  } \/\/ all regions have been processed\n+}\n+\n+void ShenandoahHeap::help_verify_region_rem_set(ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx, HeapWord* from,\n+                                                HeapWord* top, HeapWord* registration_watermark, const char* message) {\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(false);\n+\n+  HeapWord* obj_addr = from;\n+  if (r->is_humongous_start()) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      size_t card_index = scanner->card_index_for_addr(obj_addr);\n+      \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+      \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+      \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+      if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+        obj->oop_iterate(&check_interesting_pointers);\n+      }\n+      \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+    }\n+    \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+\n+    if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+      ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL, message,\n+                                       \"object not properly registered\", __FILE__, __LINE__);\n+    }\n+  } else if (!r->is_humongous()) {\n+    while (obj_addr < top) {\n+      oop obj = cast_to_oop(obj_addr);\n+      \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+      if (!ctx || ctx->is_marked(obj)) {\n+        size_t card_index = scanner->card_index_for_addr(obj_addr);\n+        \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+        \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+        if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+          obj->oop_iterate(&check_interesting_pointers);\n+        }\n+        \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+\n+        if ((obj_addr < registration_watermark) && !scanner->verify_registration(obj_addr, ctx)) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL, message,\n+                                           \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+        obj_addr += obj->size();\n+      } else {\n+        \/\/ This object is not live so we don't verify dirty cards contained therein\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+      }\n+    }\n+  }\n+}\n+\n+void ShenandoahHeap::verify_rem_set_after_full_gc() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(r, nullptr, r->bottom(), r->top(), r->top(), \"Remembered set violation at end of Full GC\");\n+    }\n+  }\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.  Even though\n+\/\/ the update-references scan of remembered set only examines cards up to update_watermark, the remembered\n+\/\/ set should be valid through top.  This examines the write_card_table between bottom() and top() because\n+\/\/ all PLABS are retired immediately before the start of update refs.\n+void ShenandoahHeap::verify_rem_set_at_update_ref() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (doing_mixed_evacuations() ||\n+      is_concurrent_prep_for_mixed_evacuation_in_progress() || active_generation()->generation_mode() == GLOBAL) {\n+    ctx = complete_marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && !r->is_cset()) {\n+      help_verify_region_rem_set(r, ctx, r->bottom(), r->top(), r->get_update_watermark(),\n+                                 \"Remembered set violation at init-update-references\");\n+    }\n+  }\n+}\n+\n+ShenandoahGeneration* ShenandoahHeap::generation_for(ShenandoahRegionAffiliation affiliation) const {\n+  if (!mode()->is_generational()) {\n+    return global_generation();\n+  } else if (affiliation == YOUNG_GENERATION) {\n+    return young_generation();\n+  } else if (affiliation == OLD_GENERATION) {\n+    return old_generation();\n+  }\n+\n+  ShouldNotReachHere();\n+  return nullptr;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":952,"deletions":250,"binary":false,"changes":1202,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -45,0 +47,1 @@\n+class PLAB;\n@@ -47,0 +50,1 @@\n+class ShenandoahRegulatorThread;\n@@ -49,0 +53,2 @@\n+class ShenandoahGeneration;\n+class ShenandoahYoungGeneration;\n@@ -50,0 +56,1 @@\n+class ShenandoahOldHeuristics;\n@@ -51,1 +58,0 @@\n-class ShenandoahMode;\n@@ -110,0 +116,10 @@\n+template<GenerationMode GENERATION>\n+class ShenandoahGenerationRegionClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  explicit ShenandoahGenerationRegionClosure(ShenandoahHeapRegionClosure* cl) : _cl(cl) {}\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+  virtual bool is_thread_safe() { return _cl->is_thread_safe(); }\n+ private:\n+  ShenandoahHeapRegionClosure* _cl;\n+};\n+\n@@ -127,0 +143,1 @@\n+  friend class ShenandoahOldGC;\n@@ -135,0 +152,6 @@\n+  ShenandoahGeneration* _gc_generation;\n+\n+  bool _mixed_evac;                      \/\/ true iff most recent evac included at least one old-gen HeapRegion\n+  bool _prep_for_mixed_evac_in_progress; \/\/ true iff we are concurrently coalescing and filling old-gen HeapRegions\n+\n+  size_t _evacuation_allowance;          \/\/ amount by which young-gen usage may temporarily exceed young-gen capacity\n@@ -141,0 +164,19 @@\n+  ShenandoahGeneration* active_generation() const {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  void set_mixed_evac(bool mixed_evac) {\n+    _mixed_evac = mixed_evac;\n+  }\n+\n+  ShenandoahOldHeuristics* old_heuristics();\n+\n+  bool doing_mixed_evacuations();\n+\n+  bool is_gc_generation_young() const;\n+\n@@ -154,0 +196,1 @@\n+  void initialize_generations();\n@@ -166,0 +209,3 @@\n+  void verify_rem_set_at_mark();\n+  void verify_rem_set_at_update_ref();\n+  void verify_rem_set_after_full_gc();\n@@ -181,1 +227,0 @@\n-  volatile size_t _bytes_allocated_since_gc_start;\n@@ -184,0 +229,4 @@\n+  static size_t young_generation_capacity(size_t total_capacity);\n+  void help_verify_region_rem_set(ShenandoahHeapRegion* r, ShenandoahMarkingContext* ctx,\n+                                  HeapWord* from, HeapWord* top, HeapWord* update_watermark, const char* message);\n+\n@@ -191,2 +240,0 @@\n-  void increase_allocated(size_t bytes);\n-  size_t bytes_allocated_since_gc_start();\n@@ -260,2 +307,2 @@\n-    \/\/ Heap is under marking: needs SATB barriers.\n-    MARKING_BITPOS    = 1,\n+    \/\/ Young regions are under marking: needs SATB barriers.\n+    YOUNG_MARKING_BITPOS    = 1,\n@@ -271,0 +318,3 @@\n+\n+    \/\/ Old regions are under marking, still need SATB barriers.\n+    OLD_MARKING_BITPOS = 5\n@@ -276,1 +326,1 @@\n-    MARKING       = 1 << MARKING_BITPOS,\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n@@ -280,0 +330,1 @@\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -290,0 +341,56 @@\n+  \/\/ _alloc_supplement_reserve is a supplemental budget for new_memory allocations.  During evacuation and update-references,\n+  \/\/ mutator allocation requests are \"authorized\" iff young_gen->available() plus _alloc_supplement_reserve minus\n+  \/\/ _young_evac_reserve is greater than request size.  The values of _alloc_supplement_reserve and _young_evac_reserve\n+  \/\/ are zero except during evacuation and update-reference phases of GC.  Both of these values are established at\n+  \/\/ the start of evacuation, and they remain constant throughout the duration of these two phases of GC.  Since these\n+  \/\/ two values are constant throughout each GC phases, we introduce a new service into ShenandoahGeneration.  This service\n+  \/\/ provides adjusted_available() based on an adjusted capacity.  At the start of evacuation, we adjust young capacity by\n+  \/\/ adding the amount to be borrowed from old-gen and subtracting the _young_evac_reserve, we adjust old capacity by\n+  \/\/ subtracting the amount to be loaned to young-gen.  At the end of update-refs, we unadjust the capacity of each generation,\n+  \/\/ and add _young_evac_expended to young-gen used.\n+  \/\/\n+  \/\/ We always use adjusted capacities to determine permission to allocate within young and to promote into old.  Note\n+  \/\/ that adjusted capacities equal traditional capacities except during evacuation and update refs.\n+  \/\/\n+  \/\/ During evacuation, we assure that _young_evac_expended does not exceed _young_evac_reserve and that _old_evac_expended\n+  \/\/ does not exceed _old_evac_reserve.  GCLAB allocations do not immediately affect used within the young generation\n+  \/\/ since the adjusted capacity already accounts for the entire evacuation reserve.  Each GCLAB allocations increments\n+  \/\/ _young_evac_expended rather than incrementing the affiliated generation's used value.\n+  \/\/\n+  \/\/ At the end of update references, we perform the following bookkeeping activities:\n+  \/\/\n+  \/\/ 1. Unadjust the capacity within young-gen and old-gen to undo the effects of borrowing memory from old-gen.  Note that\n+  \/\/    the entirety of the collection set is now available, so allocation capacity naturally increase at this time.\n+  \/\/ 2. Increase young_gen->used() by _young_evac_expended.  This represents memory consumed by evacutions from young-gen.\n+  \/\/ 3. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promotion_reserve\n+  \/\/\n+  \/\/ _young_evac_reserve and _old_evac_reserve are only non-zero during evacuation and update-references.\n+  \/\/\n+  \/\/ Allocation of young GCLABs assures that _young_evac_expended + request-size < _young_evac_reserved.  If the allocation\n+  \/\/  is authorized, increment _young_evac_expended by request size.  This allocation ignores young_gen->available().\n+  \/\/\n+  \/\/ Allocation of old GCLABs assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n+  \/\/  is authorized, increment _old_evac_expended by request size.  This allocation ignores old_gen->available().\n+  \/\/\n+  \/\/ Note that the typical total expenditure on evacuation is less than the associated evacuation reserve because we generally\n+  \/\/ reserve ShenandoahEvacWaste (> 1.0) times the anticipated evacuation need.  In the case that there is an excessive amount\n+  \/\/ of waste, it may be that one thread fails to grab a new GCLAB, this does not necessarily doom the associated evacuation\n+  \/\/ effort.  If this happens, the requesting thread blocks until some other thread manages to evacuate the offending object.\n+  \/\/ Only after \"all\" threads fail to evacuate an object do we consider the evacuation effort to have failed.\n+\n+  intptr_t _alloc_supplement_reserve;  \/\/ Bytes reserved for young allocations during evac and update refs\n+  size_t _promotion_reserve;           \/\/ Bytes reserved within old-gen to hold the results of promotion\n+\n+\n+  size_t _old_evac_reserve;            \/\/ Bytes reserved within old-gen to hold evacuated objects from old-gen collection set\n+  size_t _old_evac_expended;           \/\/ Bytes of old-gen memory expended on old-gen evacuations\n+\n+  size_t _young_evac_reserve;          \/\/ Bytes reserved within young-gen to hold evacuated objects from young-gen collection set\n+  size_t _young_evac_expended;         \/\/ Bytes old-gen memory has been expended on young-gen evacuations\n+\n+  size_t _captured_old_usage;          \/\/ What was old usage (bytes) when last captured?\n+\n+  size_t _previous_promotion;          \/\/ Bytes promoted during previous evacuation\n+\n+  bool _upgraded_to_full;\n+\n@@ -293,0 +400,2 @@\n+\n+\n@@ -297,1 +406,2 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -306,0 +416,2 @@\n+  void set_concurrent_prep_for_mixed_evacuation_in_progress(bool cond);\n+  void set_aging_cycle(bool cond);\n@@ -310,0 +422,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -320,0 +434,34 @@\n+  bool is_concurrent_prep_for_mixed_evacuation_in_progress();\n+  inline bool is_aging_cycle() const;\n+  inline bool upgraded_to_full() { return _upgraded_to_full; }\n+  inline void start_conc_gc() { _upgraded_to_full = false; }\n+  inline void record_upgrade_to_full() { _upgraded_to_full = true; }\n+\n+  inline size_t capture_old_usage(size_t usage);\n+  inline void set_previous_promotion(size_t promoted_bytes);\n+  inline size_t get_previous_promotion() const;\n+\n+  \/\/ Returns previous value\n+  inline size_t set_promotion_reserve(size_t new_val);\n+  inline size_t get_promotion_reserve() const;\n+\n+  \/\/ Returns previous value\n+  inline size_t set_old_evac_reserve(size_t new_val);\n+  inline size_t get_old_evac_reserve() const;\n+\n+  inline void reset_old_evac_expended();\n+  inline size_t expend_old_evac(size_t increment);\n+  inline size_t get_old_evac_expended() const;\n+\n+  \/\/ Returns previous value\n+  inline size_t set_young_evac_reserve(size_t new_val);\n+  inline size_t get_young_evac_reserve() const;\n+\n+  inline void reset_young_evac_expended();\n+  inline size_t expend_young_evac(size_t increment);\n+  inline size_t get_young_evac_expended() const;\n+\n+  \/\/ Returns previous value.  This is a signed value because it is the amount borrowed minus the amount reserved for\n+  \/\/ young-gen evacuation.  In case we cannot borrow much, this value might be negative.\n+  inline intptr_t set_alloc_supplement_reserve(intptr_t new_val);\n+  inline intptr_t get_alloc_supplement_reserve() const;\n@@ -322,0 +470,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -337,0 +487,1 @@\n+  double _cancel_requested_time;\n@@ -338,0 +489,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n@@ -341,2 +497,0 @@\n-  static address cancelled_gc_addr();\n-\n@@ -346,1 +500,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -348,0 +502,1 @@\n+  void cancel_concurrent_mark();\n@@ -357,4 +512,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n-  void prepare_evacuation(bool concurrent);\n@@ -384,0 +535,4 @@\n+  ShenandoahYoungGeneration* _young_generation;\n+  ShenandoahGeneration*      _global_generation;\n+  ShenandoahGeneration*      _old_generation;\n+\n@@ -385,0 +540,1 @@\n+  ShenandoahRegulatorThread* _regulator_thread;\n@@ -387,1 +543,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -395,0 +550,1 @@\n+  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n@@ -397,0 +553,5 @@\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n+  ShenandoahGeneration*      global_generation() const { return _global_generation; }\n+  ShenandoahGeneration*      old_generation()    const { return _old_generation;    }\n+  ShenandoahGeneration*      generation_for(ShenandoahRegionAffiliation affiliation) const;\n+\n@@ -399,1 +560,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -412,0 +572,3 @@\n+  MemoryPool*                  _young_gen_memory_pool;\n+  MemoryPool*                  _old_gen_memory_pool;\n+\n@@ -431,8 +594,0 @@\n-\/\/ ---------- Reference processing\n-\/\/\n-private:\n-  ShenandoahReferenceProcessor* const _ref_processor;\n-\n-public:\n-  ShenandoahReferenceProcessor* ref_processor() { return _ref_processor; }\n-\n@@ -442,0 +597,1 @@\n+  ShenandoahSharedFlag  _is_aging_cycle;\n@@ -473,0 +629,5 @@\n+  bool is_in_active_generation(oop obj) const;\n+  bool is_in_young(const void* p) const;\n+  bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n+\n@@ -526,1 +687,2 @@\n-  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region);\n+  HeapWord* allocate_memory_under_lock(ShenandoahAllocRequest& request, bool& in_new_region, bool is_promotion);\n+\n@@ -531,0 +693,4 @@\n+  inline HeapWord* allocate_from_plab(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size, bool is_promotion);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n@@ -532,1 +698,1 @@\n-  HeapWord* allocate_memory(ShenandoahAllocRequest& request);\n+  HeapWord* allocate_memory(ShenandoahAllocRequest& request, bool is_promotion);\n@@ -552,0 +718,2 @@\n+  void set_young_lab_region_flags();\n+\n@@ -576,2 +744,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -588,2 +754,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -609,0 +773,6 @@\n+  ShenandoahSharedFlag _old_gen_oom_evac;\n+\n+  inline oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahRegionAffiliation target_gen);\n+  void handle_old_evacuation(HeapWord* obj, size_t words, bool promotion);\n+  void handle_old_evacuation_failure();\n+  void handle_promotion_failure();\n@@ -621,1 +791,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -629,0 +799,17 @@\n+  inline bool clear_old_evacuation_failure();\n+\n+\/\/ ---------- Generational support\n+\/\/\n+private:\n+  RememberedScanner* _card_scan;\n+\n+public:\n+  inline RememberedScanner* card_scan() { return _card_scan; }\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n+  void dirty_cards(HeapWord* start, HeapWord* end);\n+  void clear_cards(HeapWord* start, HeapWord* end);\n+  void mark_card_as_dirty(void* location);\n+  void retire_plab(PLAB* plab);\n+  void cancel_mixed_collections();\n+  void coalesce_and_fill_old_regions();\n+\n@@ -650,1 +837,3 @@\n-  void trash_humongous_region_at(ShenandoahHeapRegion *r);\n+  size_t trash_humongous_region_at(ShenandoahHeapRegion *r);\n+\n+  static inline void increase_object_age(oop obj, uint additional_age);\n@@ -652,0 +841,1 @@\n+  void purge_old_satb_buffers(bool abandon);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":223,"deletions":33,"binary":false,"changes":256,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -45,0 +46,2 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -267,1 +270,1 @@\n-inline void ShenandoahHeap::clear_cancelled_gc() {\n+inline void ShenandoahHeap::clear_cancelled_gc(bool clear_oom_handler) {\n@@ -269,1 +272,9 @@\n-  _oom_evac_handler.clear();\n+  if (_cancel_requested_time > 0) {\n+    double cancel_time = os::elapsedTime() - _cancel_requested_time;\n+    log_info(gc)(\"GC cancellation took %.3fs\", cancel_time);\n+    _cancel_requested_time = 0;\n+  }\n+\n+  if (clear_oom_handler) {\n+    _oom_evac_handler.clear();\n+  }\n@@ -286,1 +297,0 @@\n-  \/\/ Otherwise...\n@@ -290,0 +300,24 @@\n+inline HeapWord* ShenandoahHeap::allocate_from_plab(Thread* thread, size_t size, bool is_promotion) {\n+  assert(UseTLAB, \"TLABs should be enabled\");\n+\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+    return NULL;\n+  } else if (plab == NULL) {\n+    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(),\n+           \"Performance: thread should have PLAB: %s\", thread->name());\n+    \/\/ No PLABs in this thread, fallback to shared allocation\n+    return NULL;\n+  }\n+  HeapWord* obj = plab->allocate(size);\n+  if (obj == NULL) {\n+    obj = allocate_from_plab_slow(thread, size, is_promotion);\n+  }\n+  if (is_promotion) {\n+    ShenandoahThreadLocalData::add_to_plab_promoted(thread, size * HeapWordSize);\n+  } else {\n+    ShenandoahThreadLocalData::add_to_plab_evacuated(thread, size * HeapWordSize);\n+  }\n+  return obj;\n+}\n+\n@@ -291,1 +325,2 @@\n-  if (ShenandoahThreadLocalData::is_oom_during_evac(Thread::current())) {\n+  assert(thread == Thread::current(), \"Expected thread parameter to be current thread.\");\n+  if (ShenandoahThreadLocalData::is_oom_during_evac(thread)) {\n@@ -299,1 +334,2 @@\n-  size_t size = p->size();\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n@@ -301,1 +337,21 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n+  ShenandoahRegionAffiliation target_gen = r->affiliation();\n+  if (mode()->is_generational() && ShenandoahHeap::heap()->is_gc_generation_young() &&\n+      target_gen == YOUNG_GENERATION && ShenandoahPromoteTenuredObjects) {\n+    markWord mark = p->mark();\n+    if (mark.is_marked()) {\n+      \/\/ Already forwarded.\n+      return ShenandoahBarrierSet::resolve_forwarded(p);\n+    }\n+    if (mark.has_displaced_mark_helper()) {\n+      \/\/ We don't want to deal with MT here just to ensure we read the right mark word.\n+      \/\/ Skip the potential promotion attempt for this one.\n+    } else if (r->age() + mark.age() >= InitialTenuringThreshold) {\n+      oop result = try_evacuate_object(p, thread, r, OLD_GENERATION);\n+      if (result != NULL) {\n+        return result;\n+      }\n+      \/\/ If we failed to promote this aged object, we'll fall through to code below and evacuat to young-gen.\n+    }\n+  }\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n@@ -303,1 +359,5 @@\n-  bool alloc_from_gclab = true;\n+\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n+\/\/ to OLD_GENERATION.\n+inline oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region,\n+                                               ShenandoahRegionAffiliation target_gen) {\n+  bool alloc_from_lab = true;\n@@ -305,0 +365,2 @@\n+  size_t size = p->size();\n+  bool is_promotion = (target_gen == OLD_GENERATION) && from_region->is_young();\n@@ -313,1 +375,30 @@\n-      copy = allocate_from_gclab(thread, size);\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+           copy = allocate_from_gclab(thread, size);\n+           if ((copy == nullptr) && (size < ShenandoahThreadLocalData::gclab_size(thread))) {\n+             \/\/ GCLAB allocation failed because we are bumping up against the limit on young evacuation reserve.  Try resetting\n+             \/\/ the desired GCLAB size and retry GCLAB allocation to avoid cascading of shared memory allocations.\n+             ShenandoahThreadLocalData::set_gclab_size(thread, PLAB::min_size());\n+             copy = allocate_from_gclab(thread, size);\n+             \/\/ If we still get nullptr, we'll try a shared allocation below.\n+           }\n+           break;\n+        }\n+        case OLD_GENERATION: {\n+           if (ShenandoahUsePLAB) {\n+             copy = allocate_from_plab(thread, size, is_promotion);\n+             if ((copy == nullptr) && (size < ShenandoahThreadLocalData::plab_size(thread))) {\n+               \/\/ PLAB allocation failed because we are bumping up against the limit on old evacuation reserve.  Try resetting\n+               \/\/ the desired PLAB size and retry PLAB allocation to avoid cascading of shared memory allocations.\n+               ShenandoahThreadLocalData::set_plab_size(thread, PLAB::min_size());\n+               copy = allocate_from_plab(thread, size, is_promotion);\n+               \/\/ If we still get nullptr, we'll try a shared allocation below.\n+             }\n+           }\n+           break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n@@ -315,0 +406,1 @@\n+\n@@ -316,3 +408,4 @@\n-      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size);\n-      copy = allocate_memory(req);\n-      alloc_from_gclab = false;\n+      \/\/ If we failed to allocated in LAB, we'll try a shared allocation.\n+      ShenandoahAllocRequest req = ShenandoahAllocRequest::for_shared_gc(size, target_gen);\n+      copy = allocate_memory(req, is_promotion);\n+      alloc_from_lab = false;\n@@ -325,0 +418,13 @@\n+    if (target_gen == OLD_GENERATION) {\n+      assert(mode()->is_generational(), \"Should only be here in generational mode.\");\n+      if (from_region->is_young()) {\n+        \/\/ Signal that promotion failed. Will evacuate this old object somewhere in young gen.\n+        handle_promotion_failure();\n+        return NULL;\n+      } else {\n+        \/\/ Remember that evacuation to old gen failed. We'll want to trigger a full gc to recover from this\n+        \/\/ after the evacuation threads have finished.\n+        handle_old_evacuation_failure();\n+      }\n+    }\n+\n@@ -335,1 +441,2 @@\n-  \/\/ Try to install the new forwarding pointer.\n+\n+  \/\/ Try to install the new forwarding pointer.\n@@ -340,0 +447,11 @@\n+    if (mode()->is_generational()) {\n+      if (target_gen == OLD_GENERATION) {\n+        handle_old_evacuation(copy, size, from_region->is_young());\n+      } else if (target_gen == YOUNG_GENERATION) {\n+        if (is_aging_cycle()) {\n+          ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n+        }\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n@@ -348,8 +466,23 @@\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n-      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+    if (alloc_from_lab) {\n+       \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+       \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+       \/\/ do this.\n+       switch (target_gen) {\n+         case YOUNG_GENERATION: {\n+             ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+            break;\n+         }\n+         case OLD_GENERATION: {\n+            ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n+            if (is_promotion) {\n+              ShenandoahThreadLocalData::subtract_from_plab_promoted(thread, size * HeapWordSize);\n+            } else {\n+              ShenandoahThreadLocalData::subtract_from_plab_evacuated(thread, size * HeapWordSize);\n+            }\n+            break;\n+         }\n+         default: {\n+           ShouldNotReachHere();\n+           break;\n+         }\n+       }\n@@ -357,0 +490,3 @@\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n@@ -359,0 +495,1 @@\n+      \/\/ For non-LAB allocations, the object has already been registered\n@@ -365,0 +502,18 @@\n+void ShenandoahHeap::increase_object_age(oop obj, uint additional_age) {\n+  markWord w = obj->has_displaced_mark() ? obj->displaced_mark() : obj->mark();\n+  w = w.set_age(MIN2(markWord::max_age, w.age() + additional_age));\n+  if (obj->has_displaced_mark()) {\n+    obj->set_displaced_mark(w);\n+  } else {\n+    obj->set_mark(w);\n+  }\n+}\n+\n+inline bool ShenandoahHeap::clear_old_evacuation_failure() {\n+  return _old_gen_oom_evac.try_unset();\n+}\n+\n+inline bool ShenandoahHeap::is_old(oop obj) const {\n+  return is_gc_generation_young() && is_in_old(obj);\n+}\n+\n@@ -385,1 +540,1 @@\n-  return _gc_state.is_unset(MARKING | EVACUATION | UPDATEREFS);\n+  return _gc_state.is_unset(YOUNG_MARKING | OLD_MARKING | EVACUATION | UPDATEREFS);\n@@ -389,1 +544,9 @@\n-  return _gc_state.is_set(MARKING);\n+  return _gc_state.is_set(YOUNG_MARKING | OLD_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_young_mark_in_progress() const {\n+  return _gc_state.is_set(YOUNG_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_old_mark_in_progress() const {\n+  return _gc_state.is_set(OLD_MARKING);\n@@ -428,0 +591,85 @@\n+inline bool ShenandoahHeap::is_aging_cycle() const {\n+  return _is_aging_cycle.is_set();\n+}\n+\n+inline size_t ShenandoahHeap::set_promotion_reserve(size_t new_val) {\n+  size_t orig = _promotion_reserve;\n+  _promotion_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_promotion_reserve() const {\n+  return _promotion_reserve;\n+}\n+\n+\/\/ returns previous value\n+size_t ShenandoahHeap::capture_old_usage(size_t old_usage) {\n+  size_t previous_value = _captured_old_usage;\n+  _captured_old_usage = old_usage;\n+  return previous_value;\n+}\n+\n+void ShenandoahHeap::set_previous_promotion(size_t promoted_bytes) {\n+  _previous_promotion = promoted_bytes;\n+}\n+\n+size_t ShenandoahHeap::get_previous_promotion() const {\n+  return _previous_promotion;\n+}\n+\n+inline size_t ShenandoahHeap::set_old_evac_reserve(size_t new_val) {\n+  size_t orig = _old_evac_reserve;\n+  _old_evac_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_old_evac_reserve() const {\n+  return _old_evac_reserve;\n+}\n+\n+inline void ShenandoahHeap::reset_old_evac_expended() {\n+  _old_evac_expended = 0;\n+}\n+\n+inline size_t ShenandoahHeap::expend_old_evac(size_t increment) {\n+  _old_evac_expended += increment;\n+  return _old_evac_expended;\n+}\n+\n+inline size_t ShenandoahHeap::get_old_evac_expended() const {\n+  return _old_evac_expended;\n+}\n+\n+inline size_t ShenandoahHeap::set_young_evac_reserve(size_t new_val) {\n+  size_t orig = _young_evac_reserve;\n+  _young_evac_reserve = new_val;\n+  return orig;\n+}\n+\n+inline size_t ShenandoahHeap::get_young_evac_reserve() const {\n+  return _young_evac_reserve;\n+}\n+\n+inline void ShenandoahHeap::reset_young_evac_expended() {\n+  _young_evac_expended = 0;\n+}\n+\n+inline size_t ShenandoahHeap::expend_young_evac(size_t increment) {\n+  _young_evac_expended += increment;\n+  return _young_evac_expended;\n+}\n+\n+inline size_t ShenandoahHeap::get_young_evac_expended() const {\n+  return _young_evac_expended;\n+}\n+\n+inline intptr_t ShenandoahHeap::set_alloc_supplement_reserve(intptr_t new_val) {\n+  intptr_t orig = _alloc_supplement_reserve;\n+  _alloc_supplement_reserve = new_val;\n+  return orig;\n+}\n+\n+inline intptr_t ShenandoahHeap::get_alloc_supplement_reserve() const {\n+  return _alloc_supplement_reserve;\n+}\n+\n@@ -437,2 +685,1 @@\n-  ShenandoahMarkingContext* const ctx = complete_marking_context();\n-  assert(ctx->is_complete(), \"sanity\");\n+  ShenandoahMarkingContext* const ctx = marking_context();\n@@ -569,8 +816,0 @@\n-inline void ShenandoahHeap::mark_complete_marking_context() {\n-  _marking_context->mark_complete();\n-}\n-\n-inline void ShenandoahHeap::mark_incomplete_marking_context() {\n-  _marking_context->mark_incomplete();\n-}\n-\n@@ -586,0 +825,24 @@\n+inline void ShenandoahHeap::clear_cards_for(ShenandoahHeapRegion* region) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_range_as_empty(region->bottom(), pointer_delta(region->end(), region->bottom()));\n+  }\n+}\n+\n+inline void ShenandoahHeap::dirty_cards(HeapWord* start, HeapWord* end) {\n+  assert(mode()->is_generational(), \"Should only be used for generational mode\");\n+  size_t words = pointer_delta(end, start);\n+  _card_scan->mark_range_as_dirty(start, words);\n+}\n+\n+inline void ShenandoahHeap::clear_cards(HeapWord* start, HeapWord* end) {\n+  assert(mode()->is_generational(), \"Should only be used for generational mode\");\n+  size_t words = pointer_delta(end, start);\n+  _card_scan->mark_range_as_clean(start, words);\n+}\n+\n+inline void ShenandoahHeap::mark_card_as_dirty(void* location) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_card_as_dirty((HeapWord*)location);\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":295,"deletions":32,"binary":false,"changes":327,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-\n+#include \"logging\/log.hpp\"\n@@ -52,0 +52,8 @@\n+inline bool ShenandoahMarkingContext::is_marked_or_old(oop obj) const {\n+  return is_marked(obj) || ShenandoahHeap::heap()->is_old(obj);\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_strong_or_old(oop obj) const {\n+  return is_marked_strong(obj) || ShenandoahHeap::heap()->is_old(obj);\n+}\n+\n@@ -69,13 +77,32 @@\n-  size_t idx = r->index();\n-  HeapWord* old_tams = _top_at_mark_starts_base[idx];\n-  HeapWord* new_tams = r->top();\n-\n-  assert(new_tams >= old_tams,\n-         \"Region \" SIZE_FORMAT\", TAMS updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n-         idx, p2i(old_tams), p2i(new_tams));\n-  assert(is_bitmap_clear_range(old_tams, new_tams),\n-         \"Region \" SIZE_FORMAT \", bitmap should be clear while adjusting TAMS: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n-         idx, p2i(old_tams), p2i(new_tams));\n-\n-  _top_at_mark_starts_base[idx] = new_tams;\n-  _top_bitmaps[idx] = new_tams;\n+  if (r->affiliation() != FREE) {\n+    size_t idx = r->index();\n+    HeapWord* old_tams = _top_at_mark_starts_base[idx];\n+    HeapWord* new_tams = r->top();\n+\n+    assert(new_tams >= old_tams,\n+           \"Region \" SIZE_FORMAT\", TAMS updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(old_tams), p2i(new_tams));\n+    assert((new_tams == r->bottom()) || (old_tams == r->bottom()) || (new_tams >= _top_bitmaps[idx]),\n+           \"Region \" SIZE_FORMAT\", top_bitmaps updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(_top_bitmaps[idx]), p2i(new_tams));\n+    assert(old_tams == r->bottom() || is_bitmap_clear_range(old_tams, new_tams),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear while adjusting TAMS: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(old_tams), p2i(new_tams));\n+\n+    log_debug(gc)(\"Capturing TAMS for %s Region \" SIZE_FORMAT \", was: %llx, now: %llx\\n\",\n+                  affiliation_name(r->affiliation()), idx, (unsigned long long) old_tams, (unsigned long long) new_tams);\n+\n+    if ((old_tams == r->bottom()) && (new_tams > old_tams)) {\n+      log_debug(gc)(\"Clearing mark bitmap for %s Region \" SIZE_FORMAT \" while capturing TAMS\",\n+                    affiliation_name(r->affiliation()), idx);\n+\n+      clear_bitmap(r);\n+    }\n+\n+    _top_at_mark_starts_base[idx] = new_tams;\n+    if (new_tams > r->bottom()) {\n+      \/\/ In this case, new_tams is greater than old _top_bitmaps[idx]\n+      _top_bitmaps[idx] = new_tams;\n+    }\n+  }\n+  \/\/ else, FREE regions do not need their TAMS updated\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":41,"deletions":14,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -158,5 +158,2 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    ShenandoahKeepAliveClosure cl;\n-    data->oops_do(&cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress() ||\n-             heap->is_concurrent_strong_root_in_progress()) {\n+  if (heap->is_concurrent_weak_root_in_progress() ||\n+      heap->is_concurrent_strong_root_in_progress()) {\n@@ -165,0 +162,3 @@\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    ShenandoahKeepAliveClosure cl;\n+    data->oops_do(&cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,265 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahWorkerPolicy.hpp\"\n+#include \"prims\/jvmtiTagMap.hpp\"\n+#include \"utilities\/events.hpp\"\n+\n+class ShenandoahConcurrentCoalesceAndFillTask : public WorkerTask {\n+private:\n+  uint _nworkers;\n+  ShenandoahHeapRegion** _coalesce_and_fill_region_array;\n+  uint _coalesce_and_fill_region_count;\n+  ShenandoahConcurrentGC* _old_gc;\n+  volatile bool _is_preempted;\n+\n+public:\n+  ShenandoahConcurrentCoalesceAndFillTask(uint nworkers, ShenandoahHeapRegion** coalesce_and_fill_region_array,\n+                                          uint region_count, ShenandoahConcurrentGC* old_gc) :\n+    WorkerTask(\"Shenandoah Concurrent Coalesce and Fill\"),\n+    _nworkers(nworkers),\n+    _coalesce_and_fill_region_array(coalesce_and_fill_region_array),\n+    _coalesce_and_fill_region_count(region_count),\n+    _old_gc(old_gc),\n+    _is_preempted(false) {\n+  }\n+\n+  void work(uint worker_id) {\n+    for (uint region_idx = worker_id; region_idx < _coalesce_and_fill_region_count; region_idx += _nworkers) {\n+      ShenandoahHeapRegion* r = _coalesce_and_fill_region_array[region_idx];\n+      if (!r->is_humongous()) {\n+        if (!r->oop_fill_and_coalesce()) {\n+          \/\/ Coalesce and fill has been preempted\n+          Atomic::store(&_is_preempted, true);\n+          return;\n+        }\n+      } else {\n+        \/\/ there's only one object in this region and it's not garbage, so no need to coalesce or fill\n+      }\n+    }\n+  }\n+\n+  \/\/ Value returned from is_completed() is only valid after all worker thread have terminated.\n+  bool is_completed() {\n+    return !Atomic::load(&_is_preempted);\n+  }\n+};\n+\n+\n+ShenandoahOldGC::ShenandoahOldGC(ShenandoahGeneration* generation, ShenandoahSharedFlag& allow_preemption) :\n+    ShenandoahConcurrentGC(generation, false), _allow_preemption(allow_preemption) {\n+  _coalesce_and_fill_region_array = NEW_C_HEAP_ARRAY(ShenandoahHeapRegion*, ShenandoahHeap::heap()->num_regions(), mtGC);\n+}\n+\n+void ShenandoahOldGC::start_old_evacuations() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  old_heuristics->start_old_evacuations();\n+}\n+\n+\n+\/\/ Final mark for old-gen is different than for young or old, so we\n+\/\/ override the implementation.\n+void ShenandoahOldGC::op_final_mark() {\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should be at safepoint\");\n+  assert(!heap->has_forwarded_objects(), \"No forwarded objects on this path\");\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_roots_no_forwarded();\n+  }\n+\n+  if (!heap->cancelled_gc()) {\n+    assert(_mark.generation()->generation_mode() == OLD, \"Generation of Old-Gen GC should be OLD\");\n+    _mark.finish_mark();\n+    assert(!heap->cancelled_gc(), \"STW mark cannot OOM\");\n+\n+    \/\/ We need to do this because weak root cleaning reports the number of dead handles\n+    JvmtiTagMap::set_needs_cleaning();\n+\n+    _generation->prepare_regions_and_collection_set(true);\n+\n+    heap->set_unload_classes(false);\n+    heap->prepare_concurrent_roots();\n+\n+    \/\/ Believe verification following old-gen concurrent mark needs to be different than verification following\n+    \/\/ young-gen concurrent mark, so am commenting this out for now:\n+    \/\/   if (ShenandoahVerify) {\n+    \/\/     heap->verifier()->verify_after_concmark();\n+    \/\/   }\n+\n+    if (VerifyAfterGC) {\n+      Universe::verify();\n+    }\n+  }\n+}\n+\n+bool ShenandoahOldGC::collect(GCCause::Cause cause) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  if (!heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+    \/\/ Skip over the initial phases of old collect if we're resuming mixed evacuation preparation.\n+    \/\/ Continue concurrent mark, do not reset regions, do not mark roots, do not collect $200.\n+    _allow_preemption.set();\n+    entry_mark();\n+    if (!_allow_preemption.try_unset()) {\n+      \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n+      \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the cancellation.\n+      while (!heap->cancelled_gc()) {\n+        SpinPause();\n+      }\n+    }\n+\n+    if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) {\n+      return false;\n+    }\n+\n+    \/\/ Complete marking under STW\n+    vmop_entry_final_mark();\n+\n+    \/\/ We aren't dealing with old generation evacuation yet. Our heuristic\n+    \/\/ should not have built a cset in final mark.\n+    assert(!heap->is_evacuation_in_progress(), \"Old gen evacuations are not supported\");\n+\n+    \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n+    if (heap->is_concurrent_weak_root_in_progress()) {\n+      entry_weak_refs();\n+      entry_weak_roots();\n+    }\n+\n+    \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n+    \/\/ the space. This would be the last action if there is nothing to evacuate.\n+    entry_cleanup_early();\n+\n+    {\n+      ShenandoahHeapLocker locker(heap->lock());\n+      heap->free_set()->log_status();\n+    }\n+\n+\n+    \/\/ TODO: Old marking doesn't support class unloading yet\n+    \/\/ Perform concurrent class unloading\n+    \/\/ if (heap->unload_classes() &&\n+    \/\/     heap->is_concurrent_weak_root_in_progress()) {\n+    \/\/   entry_class_unloading();\n+    \/\/ }\n+\n+    heap->set_concurrent_prep_for_mixed_evacuation_in_progress(true);\n+  }\n+\n+  \/\/ Coalesce and fill objects _after_ weak root processing and class unloading.\n+  \/\/ Weak root and reference processing makes assertions about unmarked referents\n+  \/\/ that will fail if they've been overwritten with filler objects. There is also\n+  \/\/ a case in the LRB that permits access to from-space objects for the purpose\n+  \/\/ of class unloading that is unlikely to function correctly if the object has\n+  \/\/ been filled.\n+\n+  _allow_preemption.set();\n+\n+  if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_evac)) {\n+    return false;\n+  }\n+\n+  assert(!heap->is_concurrent_strong_root_in_progress(), \"No evacuations during old gc.\");\n+\n+  vmop_entry_final_roots(false);\n+\n+  if (heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+    if (!entry_coalesce_and_fill()) {\n+      \/\/ If old-gen degenerates instead of resuming, we'll just start up an out-of-cycle degenerated GC.\n+      \/\/ This should be a rare event.  Normally, we'll resume the coalesce-and-fill effort after the\n+      \/\/ preempting young-gen GC finishes.\n+      check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_outside_cycle);\n+      return false;\n+    }\n+  }\n+  if (!_allow_preemption.try_unset()) {\n+    \/\/ The regulator thread has unset the preemption guard. That thread will shortly cancel\n+    \/\/ the gc, but the control thread is now racing it. Wait until this thread sees the cancellation.\n+    while (!heap->cancelled_gc()) {\n+      SpinPause();\n+    }\n+  }\n+  \/\/ Prepare for old evacuations (actual evacuations will happen on subsequent young collects).  This cannot\n+  \/\/ begin until after we have completed coalesce-and-fill.\n+  start_old_evacuations();\n+\n+  return true;\n+}\n+\n+void ShenandoahOldGC::entry_coalesce_and_fill_message(char *buf, size_t len) const {\n+  \/\/ ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  jio_snprintf(buf, len, \"Coalescing and filling (%s)\", _generation->name());\n+}\n+\n+bool ShenandoahOldGC::op_coalesce_and_fill() {\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  WorkerThreads* workers = heap->workers();\n+  uint nworkers = workers->active_workers();\n+\n+  assert(_generation->generation_mode() == OLD, \"Only old-GC does coalesce and fill\");\n+  log_debug(gc)(\"Starting (or resuming) coalesce-and-fill of old heap regions\");\n+  uint coalesce_and_fill_regions_count = old_heuristics->old_coalesce_and_fill_candidates();\n+  assert(coalesce_and_fill_regions_count <= heap->num_regions(), \"Sanity\");\n+  old_heuristics->get_coalesce_and_fill_candidates(_coalesce_and_fill_region_array);\n+  ShenandoahConcurrentCoalesceAndFillTask task(nworkers, _coalesce_and_fill_region_array, coalesce_and_fill_regions_count, this);\n+\n+  workers->run_task(&task);\n+  if (task.is_completed()) {\n+    \/\/ Remember that we're done with coalesce-and-fill.\n+    heap->set_concurrent_prep_for_mixed_evacuation_in_progress(false);\n+    return true;\n+  } else {\n+    log_debug(gc)(\"Suspending coalesce-and-fill of old heap regions\");\n+    \/\/ Otherwise, we got preempted before the work was done.\n+    return false;\n+  }\n+}\n+\n+bool ShenandoahOldGC::entry_coalesce_and_fill() {\n+  char msg[1024];\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+\n+  entry_coalesce_and_fill_message(msg, sizeof(msg));\n+  ShenandoahConcurrentPhase gc_phase(msg, ShenandoahPhaseTimings::coalesce_and_fill);\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+  EventMark em(\"%s\", msg);\n+  ShenandoahWorkerScope scope(heap->workers(),\n+                              ShenandoahWorkerPolicy::calc_workers_for_conc_marking(),\n+                              \"concurrent coalesce and fill\");\n+\n+  return op_coalesce_and_fill();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":265,"deletions":0,"binary":false,"changes":265,"status":"added"},{"patch":"@@ -0,0 +1,224 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shared\/strongRootsScope.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahAsserts.hpp\"\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMark.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahStringDedup.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+\n+class ShenandoahFlushAllSATB : public ThreadClosure {\n+ private:\n+  SATBMarkQueueSet& _satb_qset;\n+  uintx _claim_token;\n+\n+ public:\n+  explicit ShenandoahFlushAllSATB(SATBMarkQueueSet& satb_qset) :\n+    _satb_qset(satb_qset),\n+    _claim_token(Threads::thread_claim_token()) { }\n+\n+  void do_thread(Thread* thread) {\n+    if (thread->claim_threads_do(true, _claim_token)) {\n+      \/\/ Transfer any partial buffer to the qset for completed buffer processing.\n+      _satb_qset.flush_queue(ShenandoahThreadLocalData::satb_mark_queue(thread));\n+    }\n+  }\n+};\n+\n+class ShenandoahProcessOldSATB : public SATBBufferClosure {\n+ private:\n+  ShenandoahObjToScanQueue* _queue;\n+  ShenandoahHeap* _heap;\n+  ShenandoahMarkingContext* const _mark_context;\n+\n+ public:\n+  size_t _trashed_oops;\n+\n+  explicit ShenandoahProcessOldSATB(ShenandoahObjToScanQueue* q) :\n+    _queue(q),\n+    _heap(ShenandoahHeap::heap()),\n+    _mark_context(_heap->marking_context()),\n+    _trashed_oops(0) {}\n+\n+  void do_buffer(void **buffer, size_t size) {\n+    assert(size == 0 || !_heap->has_forwarded_objects() || _heap->is_concurrent_old_mark_in_progress(), \"Forwarded objects are not expected here\");\n+    for (size_t i = 0; i < size; ++i) {\n+      oop *p = (oop *) &buffer[i];\n+      ShenandoahHeapRegion* region = _heap->heap_region_containing(*p);\n+      if (region->is_old()) {\n+        if (!region->is_trash()) {\n+          ShenandoahMark::mark_through_ref<oop, OLD>(p, _queue, NULL, _mark_context, false);\n+        } else {\n+          ++_trashed_oops;\n+        }\n+      }\n+    }\n+  }\n+};\n+\n+class ShenandoahPurgeSATBTask : public WorkerTask {\n+private:\n+  ShenandoahObjToScanQueueSet* _mark_queues;\n+\n+public:\n+  volatile size_t _trashed_oops;\n+\n+  explicit ShenandoahPurgeSATBTask(ShenandoahObjToScanQueueSet* queues) :\n+    WorkerTask(\"Purge SATB\"),\n+    _mark_queues(queues),\n+    _trashed_oops(0) {\n+    Threads::change_thread_claim_token();\n+  }\n+\n+  ~ShenandoahPurgeSATBTask() {\n+    if (_trashed_oops > 0) {\n+      log_info(gc)(\"Purged \" SIZE_FORMAT \" oops from old generation SATB buffers.\", _trashed_oops);\n+    }\n+  }\n+\n+  void work(uint worker_id) {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahSATBMarkQueueSet &satb_queues = ShenandoahBarrierSet::satb_mark_queue_set();\n+    ShenandoahFlushAllSATB flusher(satb_queues);\n+    Threads::threads_do(&flusher);\n+\n+    ShenandoahObjToScanQueue* mark_queue = _mark_queues->queue(worker_id);\n+    ShenandoahProcessOldSATB processor(mark_queue);\n+    while (satb_queues.apply_closure_to_completed_buffer(&processor)) {}\n+\n+    Atomic::add(&_trashed_oops, processor._trashed_oops);\n+  }\n+};\n+\n+ShenandoahOldGeneration::ShenandoahOldGeneration(uint max_queues, size_t max_capacity, size_t soft_max_capacity)\n+  : ShenandoahGeneration(OLD, max_queues, max_capacity, soft_max_capacity) {\n+  \/\/ Always clear references for old generation\n+  ref_processor()->set_soft_reference_policy(true);\n+}\n+\n+const char* ShenandoahOldGeneration::name() const {\n+  return \"OLD\";\n+}\n+\n+bool ShenandoahOldGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return region->affiliation() != YOUNG_GENERATION;\n+}\n+\n+void ShenandoahOldGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahGenerationRegionClosure<OLD> old_regions(cl);\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(&old_regions);\n+}\n+\n+void ShenandoahOldGeneration::heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahGenerationRegionClosure<OLD> old_regions(cl);\n+  ShenandoahHeap::heap()->heap_region_iterate(&old_regions);\n+}\n+\n+void ShenandoahOldGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap::heap()->set_concurrent_old_mark_in_progress(in_progress);\n+}\n+\n+bool ShenandoahOldGeneration::is_concurrent_mark_in_progress() {\n+  return ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress();\n+}\n+\n+void ShenandoahOldGeneration::purge_satb_buffers(bool abandon) {\n+  ShenandoahHeap *heap = ShenandoahHeap::heap();\n+  shenandoah_assert_safepoint();\n+  assert(heap->is_concurrent_old_mark_in_progress(), \"Only necessary during old marking.\");\n+\n+  if (abandon) {\n+    ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+  } else {\n+    uint nworkers = heap->workers()->active_workers();\n+    StrongRootsScope scope(nworkers);\n+\n+    ShenandoahPurgeSATBTask purge_satb_task(task_queues());\n+    heap->workers()->run_task(&purge_satb_task);\n+  }\n+}\n+\n+bool ShenandoahOldGeneration::contains(oop obj) const {\n+  return ShenandoahHeap::heap()->is_in_old(obj);\n+}\n+\n+bool ShenandoahOldGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(!heap->is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states : ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n+    ShenandoahFinalMarkUpdateRegionStateClosure cl(complete_marking_context());\n+\n+    parallel_heap_region_iterate(&cl);\n+    heap->assert_pinned_region_status();\n+  }\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset : ShenandoahPhaseTimings::degen_gc_choose_cset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heuristics()->choose_collection_set(nullptr, nullptr);\n+  }\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset : ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->rebuild();\n+  }\n+  return false;\n+}\n+\n+ShenandoahHeuristics* ShenandoahOldGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  assert(ShenandoahOldGCHeuristics != NULL, \"ShenandoahOldGCHeuristics should not equal NULL\");\n+  ShenandoahHeuristics* trigger;\n+  if (strcmp(ShenandoahOldGCHeuristics, \"static\") == 0) {\n+    trigger = new ShenandoahStaticHeuristics(this);\n+  } else if (strcmp(ShenandoahOldGCHeuristics, \"adaptive\") == 0) {\n+    trigger = new ShenandoahAdaptiveHeuristics(this);\n+  } else if (strcmp(ShenandoahOldGCHeuristics, \"compact\") == 0) {\n+    trigger = new ShenandoahCompactHeuristics(this);\n+  } else {\n+    vm_exit_during_initialization(\"Unknown -XX:ShenandoahOldGCHeuristics option (must be one of: static, adaptive, compact)\");\n+    ShouldNotReachHere();\n+    return NULL;\n+  }\n+  trigger->set_guaranteed_gc_interval(ShenandoahGuaranteedOldGCInterval);\n+  _heuristics = new ShenandoahOldHeuristics(this, trigger);\n+  return _heuristics;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":224,"deletions":0,"binary":false,"changes":224,"status":"added"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp\"\n@@ -260,0 +261,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -276,0 +278,5 @@\n+  if (!heap->is_in_active_generation(referent)) {\n+    log_trace(gc,ref)(\"Referent outside of active generation: \" PTR_FORMAT, p2i(referent));\n+    return false;\n+  }\n+\n@@ -363,1 +370,2 @@\n-  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s)\", p2i(reference), reference_type_name(type));\n+  log_trace(gc, ref)(\"Encountered Reference: \" PTR_FORMAT \" (%s, %s)\",\n+          p2i(reference), reference_type_name(type), affiliation_name(reference));\n@@ -378,1 +386,1 @@\n-#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -380,3 +388,1 @@\n-  assert(referent == NULL || ShenandoahHeap::heap()->marking_context()->is_marked(referent),\n-         \"only drop references with alive referents\");\n-#endif\n+  assert(referent == NULL || heap->marking_context()->is_marked(referent), \"only drop references with alive referents\");\n@@ -387,0 +393,8 @@\n+  \/\/ When this reference was discovered, it would not have been marked. If it ends up surviving\n+  \/\/ the cycle, we need to dirty the card if the reference is old and the referent is young.  Note\n+  \/\/ that if the reference is not dropped, then its pointer to the referent will be nulled before\n+  \/\/ evacuation begins so card does not need to be dirtied.\n+  if (heap->mode()->is_generational() && heap->is_in_old(reference) && heap->is_in_young(referent)) {\n+    \/\/ Note: would be sufficient to mark only the card that holds the start of this Reference object.\n+    heap->card_scan()->mark_range_as_dirty(cast_from_oop<HeapWord*>(reference), reference->size());\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -39,0 +40,1 @@\n+template<GenerationMode GENERATION>\n@@ -40,1 +42,1 @@\n-private:\n+ private:\n@@ -45,2 +47,3 @@\n-  inline void do_oop_work(T* p);\n-public:\n+    inline void do_oop_work(T* p);\n+\n+ public:\n@@ -53,3 +56,4 @@\n-ShenandoahInitMarkRootsClosure::ShenandoahInitMarkRootsClosure(ShenandoahObjToScanQueue* q) :\n-  _queue(q),\n-  _mark_context(ShenandoahHeap::heap()->marking_context()) {\n+template<GenerationMode GENERATION>\n+ShenandoahInitMarkRootsClosure<GENERATION>::ShenandoahInitMarkRootsClosure(ShenandoahObjToScanQueue* q) :\n+_queue(q),\n+_mark_context(ShenandoahHeap::heap()->marking_context()) {\n@@ -58,0 +62,1 @@\n+template <GenerationMode GENERATION>\n@@ -59,2 +64,3 @@\n-void ShenandoahInitMarkRootsClosure::do_oop_work(T* p) {\n-  ShenandoahMark::mark_through_ref<T>(p, _queue, _mark_context, false);\n+void ShenandoahInitMarkRootsClosure<GENERATION>::do_oop_work(T* p) {\n+  \/\/ Only called from STW mark, should not be used to bootstrap old generation marking.\n+  ShenandoahMark::mark_through_ref<T, GENERATION>(p, _queue, nullptr, _mark_context, false);\n@@ -83,2 +89,2 @@\n-ShenandoahSTWMark::ShenandoahSTWMark(bool full_gc) :\n-  ShenandoahMark(),\n+ShenandoahSTWMark::ShenandoahSTWMark(ShenandoahGeneration* generation, bool full_gc) :\n+  ShenandoahMark(generation),\n@@ -86,1 +92,1 @@\n-  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), ShenandoahHeap::heap()->marking_context()->task_queues()),\n+  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), task_queues()),\n@@ -94,1 +100,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->active_generation()->ref_processor();\n@@ -111,0 +117,5 @@\n+    if (_generation->generation_mode() == YOUNG) {\n+      \/\/ But only scan the remembered set for young generation.\n+      _generation->scan_remembered_set();\n+    }\n+\n@@ -118,1 +129,1 @@\n-  heap->mark_complete_marking_context();\n+  _generation->set_mark_complete();\n@@ -126,2 +137,14 @@\n-  ShenandoahInitMarkRootsClosure  init_mark(task_queues()->queue(worker_id));\n-  _root_scanner.roots_do(&init_mark, worker_id);\n+  switch (_generation->generation_mode()) {\n+    case GLOBAL: {\n+      ShenandoahInitMarkRootsClosure<GLOBAL> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case YOUNG: {\n+      ShenandoahInitMarkRootsClosure<YOUNG> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -133,1 +156,1 @@\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n+  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->active_generation()->ref_processor();\n@@ -136,1 +159,2 @@\n-  mark_loop(worker_id, &_terminator, rp,\n+  mark_loop(_generation->generation_mode(),\n+            worker_id, &_terminator, rp,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.cpp","additions":41,"deletions":17,"binary":false,"changes":58,"status":"modified"},{"patch":"@@ -0,0 +1,115 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+\n+ShenandoahDirectCardMarkRememberedSet::ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable* card_table, size_t total_card_count) {\n+  _heap = ShenandoahHeap::heap();\n+  _card_table = card_table;\n+  _total_card_count = total_card_count;\n+  _cluster_count = total_card_count \/ ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  _card_shift = CardTable::card_shift();\n+\n+  _byte_map = _card_table->byte_for_index(0);\n+\n+  _whole_heap_base = _card_table->addr_for(_byte_map);\n+  _whole_heap_end = _whole_heap_base + total_card_count * CardTable::card_size();\n+\n+  _byte_map_base = _byte_map - (uintptr_t(_whole_heap_base) >> _card_shift);\n+\n+  _overreach_map = (uint8_t *) malloc(total_card_count);\n+  _overreach_map_base = (_overreach_map -\n+                         (uintptr_t(_whole_heap_base) >> _card_shift));\n+\n+  assert(total_card_count % ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster == 0, \"Invalid card count.\");\n+  assert(total_card_count > 0, \"Card count cannot be zero.\");\n+  \/\/ assert(_overreach_cards != NULL);\n+}\n+\n+ShenandoahDirectCardMarkRememberedSet::~ShenandoahDirectCardMarkRememberedSet() {\n+  free(_overreach_map);\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::initialize_overreach(size_t first_cluster, size_t count) {\n+\n+  \/\/ We can make this run faster in the future by explicitly\n+  \/\/ unrolling the loop and doing wide writes if the compiler\n+  \/\/ doesn't do this for us.\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  uint8_t* omp = &_overreach_map[first_card_index];\n+  uint8_t* endp = omp + count * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  while (omp < endp)\n+    *omp++ = CardTable::clean_card_val();\n+}\n+\n+void ShenandoahDirectCardMarkRememberedSet::merge_overreach(size_t first_cluster, size_t count) {\n+\n+  \/\/ We can make this run faster in the future by explicitly unrolling the loop and doing wide writes if the compiler\n+  \/\/ doesn't do this for us.\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  uint8_t* bmp = &_byte_map[first_card_index];\n+  uint8_t* endp = bmp + count * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  uint8_t* omp = &_overreach_map[first_card_index];\n+\n+  \/\/ dirty_card is 0, clean card is 0xff; if either *bmp or *omp is dirty, we need to mark it as dirty\n+  while (bmp < endp)\n+    *bmp++ &= *omp++;\n+}\n+\n+ShenandoahScanRememberedTask::ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                                                           ShenandoahObjToScanQueueSet* old_queue_set,\n+                                                           ShenandoahReferenceProcessor* rp,\n+                                                           ShenandoahRegionIterator* regions) :\n+  WorkerTask(\"Scan Remembered Set\"),\n+  _queue_set(queue_set), _old_queue_set(old_queue_set), _rp(rp), _regions(regions) {}\n+\n+void ShenandoahScanRememberedTask::work(uint worker_id) {\n+  \/\/ This sets up a thread local reference to the worker_id which is necessary\n+  \/\/ the weak reference processor.\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahWorkerTimingsTracker x(ShenandoahPhaseTimings::init_scan_rset, ShenandoahPhaseTimings::ScanClusters, worker_id);\n+\n+  ShenandoahObjToScanQueue* q = _queue_set->queue(worker_id);\n+  ShenandoahObjToScanQueue* old = _old_queue_set == NULL ? NULL : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<YOUNG> cl(q, _rp, old);\n+  RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+\n+  \/\/ set up thread local closure for shen ref processor\n+  _rp->set_mark_closure(worker_id, &cl);\n+  ShenandoahHeapRegion* region = _regions->next();\n+  while (region != NULL) {\n+    log_debug(gc)(\"ShenandoahScanRememberedTask::work(%u), looking at region \" SIZE_FORMAT, worker_id, region->index());\n+    if (region->affiliation() == OLD_GENERATION) {\n+      scanner->process_region(region, &cl);\n+    }\n+    region = _regions->next();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":115,"deletions":0,"binary":false,"changes":115,"status":"added"},{"patch":"@@ -0,0 +1,1021 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+\n+\/\/ Terminology used within this source file:\n+\/\/\n+\/\/ Card Entry:   This is the information that identifies whether a\n+\/\/               particular card-table entry is Clean or Dirty.  A clean\n+\/\/               card entry denotes that the associated memory does not\n+\/\/               hold references to young-gen memory.\n+\/\/\n+\/\/ Card Region, aka\n+\/\/ Card Memory:  This is the region of memory that is assocated with a\n+\/\/               particular card entry.\n+\/\/\n+\/\/ Card Cluster: A card cluster represents 64 card entries.  A card\n+\/\/               cluster is the minimal amount of work performed at a\n+\/\/               time by a parallel thread.  Note that the work required\n+\/\/               to scan a card cluster is somewhat variable in that the\n+\/\/               required effort depends on how many cards are dirty, how\n+\/\/               many references are held within the objects that span a\n+\/\/               DIRTY card's memory, and on the size of the object\n+\/\/               that spans the end of a DIRTY card's memory (because\n+\/\/               that object will be scanned in its entirety). For these\n+\/\/               reasons, it is advisable for the multiple worker threads\n+\/\/               to be flexible in the number of clusters to be\n+\/\/               processed by each thread.\n+\/\/\n+\/\/ A cluster represents a \"natural\" quantum of work to be performed by\n+\/\/ a parallel GC thread's background remembered set scanning efforts.\n+\/\/ The notion of cluster is similar to the notion of stripe in the\n+\/\/ implementation of parallel GC card scanning.  However, a cluster is\n+\/\/ typically smaller than a stripe, enabling finer grain division of\n+\/\/ labor between multiple threads.\n+\/\/\n+\/\/ For illustration, consider the following possible JVM configurations:\n+\/\/\n+\/\/   Scenario 1:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of a card entry is 512 B\n+\/\/     Each card table entry consumes 1 B\n+\/\/     Assume one long word of card table entries represents a cluster.\n+\/\/       This long word holds 8 card table entries, spanning a\n+\/\/       total of 4KB\n+\/\/     The number of clusters per region is 128 MB \/ 4 KB = 32K\n+\/\/\n+\/\/   Scenario 2:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of each card entry is 128 B\n+\/\/     Each card table entry consumes 1 bit\n+\/\/     Assume one int word of card tables represents a cluster.\n+\/\/       This int word holds 32 card table entries, spanning a\n+\/\/       total of 4KB\n+\/\/     The number of clusters per region is 128 MB \/ 4 KB = 32K\n+\/\/\n+\/\/   Scenario 3:\n+\/\/     RegionSize is 128 MB\n+\/\/     Span of each card entry is 512 B\n+\/\/     Each card table entry consumes 1 bit\n+\/\/     Assume one long word of card tables represents a cluster.\n+\/\/       This long word holds 64 card table entries, spanning a\n+\/\/       total of 32 KB\n+\/\/     The number of clusters per region is 128 MB \/ 32 KB = 4K\n+\/\/\n+\/\/ At the start of a new young-gen concurrent mark pass, the gang of\n+\/\/ Shenandoah worker threads collaborate in performing the following\n+\/\/ actions:\n+\/\/\n+\/\/  Let old_regions = number of ShenandoahHeapRegion comprising\n+\/\/    old-gen memory\n+\/\/  Let region_size = ShenandoahHeapRegion::region_size_bytes()\n+\/\/    represent the number of bytes in each region\n+\/\/  Let clusters_per_region = region_size \/ 512\n+\/\/  Let rs represent the relevant RememberedSet implementation\n+\/\/    (an instance of ShenandoahDirectCardMarkRememberedSet or an instance\n+\/\/     of a to-be-implemented ShenandoahBufferWithSATBRememberedSet)\n+\/\/\n+\/\/  for each ShenandoahHeapRegion old_region in the whole heap\n+\/\/    determine the cluster number of the first cluster belonging\n+\/\/      to that region\n+\/\/    for each cluster contained within that region\n+\/\/      Assure that exactly one worker thread initializes each\n+\/\/      cluster of overreach memory by invoking:\n+\/\/\n+\/\/        rs->initialize_overreach(cluster_no, cluster_count)\n+\/\/\n+\/\/      in separate threads.  (Divide up the clusters so that\n+\/\/      different threads are responsible for initializing different\n+\/\/      clusters.  Initialization cost is essentially identical for\n+\/\/      each cluster.)\n+\/\/\n+\/\/  Next, we repeat the process for invocations of process_clusters.\n+\/\/  for each ShenandoahHeapRegion old_region in the whole heap\n+\/\/    determine the cluster number of the first cluster belonging\n+\/\/      to that region\n+\/\/    for each cluster contained within that region\n+\/\/      Assure that exactly one worker thread processes each\n+\/\/      cluster, each thread making a series of invocations of the\n+\/\/      following:\n+\/\/\n+\/\/        rs->process_clusters(worker_id, ReferenceProcessor *,\n+\/\/                             ShenandoahConcurrentMark *, cluster_no, cluster_count,\n+\/\/                             HeapWord *end_of_range, OopClosure *oops);\n+\/\/\n+\/\/  For efficiency, divide up the clusters so that different threads\n+\/\/  are responsible for processing different clusters.  Processing costs\n+\/\/  may vary greatly between clusters for the following reasons:\n+\/\/\n+\/\/        a) some clusters contain mostly dirty cards and other\n+\/\/           clusters contain mostly clean cards\n+\/\/        b) some clusters contain mostly primitive data and other\n+\/\/           clusters contain mostly reference data\n+\/\/        c) some clusters are spanned by very large objects that\n+\/\/           begin in some other cluster.  When a large object\n+\/\/           beginning in a preceding cluster spans large portions of\n+\/\/           this cluster, the processing of this cluster gets a\n+\/\/           \"free ride\" because the thread responsible for processing\n+\/\/           the cluster that holds the object's header does the\n+\/\/           processing.\n+\/\/        d) in the case that the end of this cluster is spanned by a\n+\/\/           very large object, the processing of this cluster will\n+\/\/           be responsible for examining the entire object,\n+\/\/           potentially requiring this thread to process large amounts\n+\/\/           of memory pertaining to other clusters.\n+\/\/\n+\/\/ Though an initial division of labor between marking threads may\n+\/\/ assign equal numbers of clusters to be scanned by each thread, it\n+\/\/ should be expected that some threads will finish their assigned\n+\/\/ work before others.  Therefore, some amount of the full remembered\n+\/\/ set scanning effort should be held back and assigned incrementally\n+\/\/ to the threads that end up with excess capacity.  Consider the\n+\/\/ following strategy for dividing labor:\n+\/\/\n+\/\/        1. Assume there are 8 marking threads and 1024 remembered\n+\/\/           set clusters to be scanned.\n+\/\/        2. Assign each thread to scan 64 clusters.  This leaves\n+\/\/           512 (1024 - (8*64)) clusters to still be scanned.\n+\/\/        3. As the 8 server threads complete previous cluster\n+\/\/           scanning assignments, issue each of the next 8 scanning\n+\/\/           assignments as units of 32 additional cluster each.\n+\/\/           In the case that there is high variance in effort\n+\/\/           associated with previous cluster scanning assignments,\n+\/\/           multiples of these next assignments may be serviced by\n+\/\/           the server threads that were previously assigned lighter\n+\/\/           workloads.\n+\/\/        4. Make subsequent scanning assignments as follows:\n+\/\/             a) 8 assignments of size 16 clusters\n+\/\/             b) 8 assignments of size 8 clusters\n+\/\/             c) 16 assignments of size 4 clusters\n+\/\/\n+\/\/    When there is no more remembered set processing work to be\n+\/\/    assigned to a newly idled worker thread, that thread can move\n+\/\/    on to work on other tasks associated with root scanning until such\n+\/\/    time as all clusters have been examined.\n+\/\/\n+\/\/  Once all clusters have been processed, the gang of GC worker\n+\/\/  threads collaborate to merge the overreach data.\n+\/\/\n+\/\/  for each ShenandoahHeapRegion old_region in the whole heap\n+\/\/    determine the cluster number of the first cluster belonging\n+\/\/      to that region\n+\/\/    for each cluster contained within that region\n+\/\/      Assure that exactly one worker thread initializes each\n+\/\/      cluster of overreach memory by invoking:\n+\/\/\n+\/\/        rs->merge_overreach(cluster_no, cluster_count)\n+\/\/\n+\/\/      in separate threads.  (Divide up the clusters so that\n+\/\/      different threads are responsible for merging different\n+\/\/      clusters.  Merging cost is essentially identical for\n+\/\/      each cluster.)\n+\/\/\n+\/\/ Though remembered set scanning is designed to run concurrently with\n+\/\/ mutator threads, the current implementation of remembered set\n+\/\/ scanning runs in parallel during a GC safepoint.  Furthermore, the\n+\/\/ current implementation of remembered set scanning never clears a\n+\/\/ card once it has been marked.  Since the current implementation\n+\/\/ never clears marked pages, the current implementation does not\n+\/\/ invoke initialize_overreach() or merge_overreach().\n+\/\/\n+\/\/ These limitations will be addressed in future enhancements to the\n+\/\/ existing implementation.\n+\n+#include <stdint.h>\n+#include \"memory\/iterator.hpp\"\n+#include \"gc\/shared\/workerThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTaskqueue.hpp\"\n+\n+class ShenandoahReferenceProcessor;\n+class ShenandoahConcurrentMark;\n+class ShenandoahHeap;\n+class ShenandoahRegionIterator;\n+class ShenandoahMarkingContext;\n+\n+class CardTable;\n+\n+class ShenandoahDirectCardMarkRememberedSet: public CHeapObj<mtGC> {\n+\n+private:\n+\n+  \/\/ Use symbolic constants defined in cardTable.hpp\n+  \/\/  CardTable::card_shift = 9;\n+  \/\/  CardTable::card_size = 512;\n+  \/\/  CardTable::card_size_in_words = 64;\n+\n+  \/\/  CardTable::clean_card_val()\n+  \/\/  CardTable::dirty_card_val()\n+\n+  ShenandoahHeap *_heap;\n+  ShenandoahCardTable *_card_table;\n+  size_t _card_shift;\n+  size_t _total_card_count;\n+  size_t _cluster_count;\n+  HeapWord *_whole_heap_base;   \/\/ Points to first HeapWord of data contained within heap memory\n+  HeapWord *_whole_heap_end;\n+  uint8_t *_byte_map;           \/\/ Points to first entry within the card table\n+  uint8_t *_byte_map_base;      \/\/ Points to byte_map minus the bias computed from address of heap memory\n+  uint8_t *_overreach_map;      \/\/ Points to first entry within the overreach card table\n+  uint8_t *_overreach_map_base; \/\/ Points to overreach_map minus the bias computed from address of heap memory\n+\n+  uint64_t _wide_clean_value;\n+\n+public:\n+  \/\/ count is the number of cards represented by the card table.\n+  ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable *card_table, size_t total_card_count);\n+  ~ShenandoahDirectCardMarkRememberedSet();\n+\n+  \/\/ Card index is zero-based relative to _byte_map.\n+  size_t total_cards();\n+  size_t card_index_for_addr(HeapWord *p);\n+  HeapWord *addr_for_card_index(size_t card_index);\n+  bool is_card_dirty(size_t card_index);\n+  bool is_write_card_dirty(size_t card_index);\n+  void mark_card_as_dirty(size_t card_index);\n+  void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  void mark_card_as_clean(size_t card_index);\n+  void mark_read_card_as_clean(size_t card_index);\n+  void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  void mark_overreach_card_as_dirty(size_t card_index);\n+  bool is_card_dirty(HeapWord *p);\n+  void mark_card_as_dirty(HeapWord *p);\n+  void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n+  void mark_card_as_clean(HeapWord *p);\n+  void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n+  void mark_overreach_card_as_dirty(void *p);\n+  size_t cluster_count();\n+\n+  \/\/ Called by multiple GC threads at start of concurrent mark and evacuation phases.  Each parallel GC thread typically\n+  \/\/ initializes a different subranges of all overreach entries.\n+  void initialize_overreach(size_t first_cluster, size_t count);\n+\n+  \/\/ Called by GC thread at end of concurrent mark or evacuation phase.  Each parallel GC thread typically merges different\n+  \/\/ subranges of all overreach entries.\n+  void merge_overreach(size_t first_cluster, size_t count);\n+\n+  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n+  \/\/ Not currently used because mutator write barrier does not honor changes to the location of card table.\n+  void swap_remset() {  _card_table->swap_card_tables(); }\n+\n+  void merge_write_table(HeapWord* start, size_t word_count) {\n+    size_t card_index = card_index_for_addr(start);\n+    size_t num_cards = word_count \/ CardTable::card_size_in_words();\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardTable::CardValue));\n+    intptr_t* read_table_ptr = (intptr_t*) &(_card_table->read_byte_map())[card_index];\n+    intptr_t* write_table_ptr = (intptr_t*) &(_card_table->write_byte_map())[card_index];\n+    for (size_t i = 0; i < iterations; i++) {\n+      intptr_t card_value = *write_table_ptr;\n+      *read_table_ptr++ &= card_value;\n+      write_table_ptr++;\n+    }\n+  }\n+\n+  HeapWord* whole_heap_base() { return _whole_heap_base; }\n+  HeapWord* whole_heap_end() { return _whole_heap_end; }\n+\n+  \/\/ Instead of swap_remset, the current implementation of concurrent remembered set scanning does reset_remset\n+  \/\/ in parallel threads, each invocation processing one entire HeapRegion at a time.  Processing of a region\n+  \/\/ consists of copying the write table to the read table and cleaning the write table.\n+  void reset_remset(HeapWord* start, size_t word_count) {\n+    size_t card_index = card_index_for_addr(start);\n+    size_t num_cards = word_count \/ CardTable::card_size_in_words();\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardTable::CardValue));\n+    intptr_t* read_table_ptr = (intptr_t*) &(_card_table->read_byte_map())[card_index];\n+    intptr_t* write_table_ptr = (intptr_t*) &(_card_table->write_byte_map())[card_index];\n+    for (size_t i = 0; i < iterations; i++) {\n+      *read_table_ptr++ = *write_table_ptr;\n+      *write_table_ptr++ = CardTable::clean_card_row_val();\n+    }\n+  }\n+\n+  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n+  void clear_old_remset() {  _card_table->clear_read_table(); }\n+\n+};\n+\n+\/\/ A ShenandoahCardCluster represents the minimal unit of work\n+\/\/ performed by independent parallel GC threads during scanning of\n+\/\/ remembered sets.\n+\/\/\n+\/\/ The GC threads that perform card-table remembered set scanning may\n+\/\/ overwrite card-table entries to mark them as clean in the case that\n+\/\/ the associated memory no longer holds references to young-gen\n+\/\/ memory.  Rather than access the card-table entries directly, all GC\n+\/\/ thread access to card-table information is made by way of the\n+\/\/ ShenandoahCardCluster data abstraction.  This abstraction\n+\/\/ effectively manages access to multiple possible underlying\n+\/\/ remembered set implementations, including a traditional card-table\n+\/\/ approach and a SATB-based approach.\n+\/\/\n+\/\/ The API services represent a compromise between efficiency and\n+\/\/ convenience.\n+\/\/\n+\/\/ In the initial implementation, we assume that scanning of card\n+\/\/ table entries occurs only while the JVM is at a safe point.  Thus,\n+\/\/ there is no synchronization required between GC threads that are\n+\/\/ scanning card-table entries and marking certain entries that were\n+\/\/ previously dirty as clean, and mutator threads which would possibly\n+\/\/ be marking certain card-table entries as dirty.\n+\/\/\n+\/\/ There is however a need to implement concurrency control and memory\n+\/\/ coherency between multiple GC threads that scan the remembered set\n+\/\/ in parallel.  The desire is to divide the complete scanning effort\n+\/\/ into multiple clusters of work that can be independently processed\n+\/\/ by individual threads without need for synchronizing efforts\n+\/\/ between the work performed by each task.  The term \"cluster\" of\n+\/\/ work is similar to the term \"stripe\" as used in the implementation\n+\/\/ of Parallel GC.\n+\/\/\n+\/\/ Complexity arises when an object to be scanned crosses the boundary\n+\/\/ between adjacent cluster regions.  Here is the protocol that is\n+\/\/ followed:\n+\/\/\n+\/\/  1. We implement a supplemental data structure known as the overreach\n+\/\/     card table.  The thread that is responsible for scanning each\n+\/\/     cluster of card-table entries is granted exclusive access to\n+\/\/     modify the associated card-table entries.  In the case that a\n+\/\/     thread scans a very large object that reaches into one or more\n+\/\/     following clusters, that thread has exclusive access to the\n+\/\/     overreach card table for all of the entries belonging to the\n+\/\/     following clusters that are spanned by this large object.\n+\/\/     After all clusters have been scanned, the scanning threads\n+\/\/     briefly synchronize to merge the contents of the overreach\n+\/\/     entries with the traditional card table entries using logical-\n+\/\/     and operations.\n+\/\/  2. Every object is scanned in its \"entirety\" by the thread that is\n+\/\/     responsible for the cluster that holds its starting address.\n+\/\/     Entirety is in quotes because there are various situations in\n+\/\/     which some portions of the object will not be scanned by this\n+\/\/     thread:\n+\/\/     a) If an object spans multiple card regions, all of which are\n+\/\/        contained within the same cluster, the scanning thread\n+\/\/        consults the existing card-table entries and does not scan\n+\/\/        portions of the object that are not currently dirty.\n+\/\/     b) For any cluster that is spanned in its entirety by a very\n+\/\/        large object, the GC thread that scans this object assumes\n+\/\/        full responsibility for maintenance of the associated\n+\/\/        card-table entries.\n+\/\/     c) If a cluster is partially spanned by an object originating\n+\/\/        in a preceding cluster, the portion of the object that\n+\/\/        partially spans the following cluster is scanned in its\n+\/\/        entirety (because the thread that is responsible for\n+\/\/        scanning the object cannot rely upon the card-table entries\n+\/\/        associated with the following cluster).  Whenever references\n+\/\/        to young-gen memory are found within the scanned data, the\n+\/\/        associated overreach card table entries are marked as dirty\n+\/\/        by the scanning thread.\n+\/\/  3. If a cluster is spanned in its entirety by an object that\n+\/\/     originates within a preceding cluster's memory, the thread\n+\/\/     assigned to examine this cluster does absolutely nothing.  The\n+\/\/     thread assigned to scan the cluster that holds the object's\n+\/\/     starting address takes full responsibility for scanning the\n+\/\/     entire object and updating the associated card-table entries.\n+\/\/  4. If a cluster is spanned partially by an object that originates\n+\/\/     within a preceding cluster's memory, the thread assigned to\n+\/\/     examine this cluster marks the card-table entry as clean for\n+\/\/     each card table that is fully spanned by this overreaching\n+\/\/     object.  If a card-table entry's memory is partially spanned\n+\/\/     by the overreaching object, the thread sets the card-table\n+\/\/     entry to clean if it was previously dirty and if the portion\n+\/\/     of the card-table entry's memory that is not spanned by the\n+\/\/     overreaching object does not hold pointers to young-gen\n+\/\/     memory.\n+\/\/  5. While examining a particular card belonging to a particular\n+\/\/     cluster, if an object reaches beyond the end of its card\n+\/\/     memory, the thread \"scans\" all portions of the object that\n+\/\/     correspond to DIRTY card entries within the current cluster and\n+\/\/     all portions of the object that reach into following clustesr.\n+\/\/     After this object is scanned, continue scanning with the memory\n+\/\/     that follows this object if this memory pertains to the same\n+\/\/     cluster.  Otherwise, consider this cluster's memory to have\n+\/\/     been fully examined.\n+\/\/\n+\/\/ Discussion:\n+\/\/  Though this design results from careful consideration of multiple\n+\/\/  design objectives, it is subject to various criticisms.  Some\n+\/\/  discussion of the design choices is provided here:\n+\/\/\n+\/\/  1. Note that remembered sets are a heuristic technique to avoid\n+\/\/     the need to scan all of old-gen memory with each young-gen\n+\/\/     collection.  If we sometimes scan a bit more memory than is\n+\/\/     absolutely necessary, that should be considered a reasonable\n+\/\/     compromise.  This compromise is already present in the sizing\n+\/\/     of card table memory areas.  Note that a single dirty pointer\n+\/\/     within a 512-byte card region forces the \"unnecessary\" scanning\n+\/\/     of 63 = ((512 - 8 = 504) \/ 8) pointers.\n+\/\/  2. One undesirable aspect of this design is that we sometimes have\n+\/\/     to scan large amounts of memory belonging to very large\n+\/\/     objects, even for parts of the very large object that do not\n+\/\/     correspond to dirty card table entries.  Note that this design\n+\/\/     limits the amount of non-dirty scanning that might have to\n+\/\/     be performed for these very large objects.  In particular, only\n+\/\/     the last part of the very large object that extends into but\n+\/\/     does not completely span a particular cluster is unnecessarily\n+\/\/     scanned.  Thus, for each very large object, the maximum\n+\/\/     over-scan is the size of memory spanned by a single cluster.\n+\/\/  3. The representation of pointer location descriptive information\n+\/\/     within Klass representations is not designed for efficient\n+\/\/     \"random access\".  An alternative approach to this design would\n+\/\/     be to scan very large objects multiple times, once for each\n+\/\/     cluster that is spanned by the object's range.  This reduces\n+\/\/     unnecessary overscan, but it introduces different sorts of\n+\/\/     overhead effort:\n+\/\/       i) For each spanned cluster, we have to look up the start of\n+\/\/          the crossing object.\n+\/\/      ii) Each time we scan the very large object, we have to\n+\/\/          sequentially walk through its pointer location\n+\/\/          descriptors, skipping over all of the pointers that\n+\/\/          precede the start of the range of addresses that we\n+\/\/          consider relevant.\n+\n+\n+\/\/ Because old-gen heap memory is not necessarily contiguous, and\n+\/\/ because cards are not necessarily maintained for young-gen memory,\n+\/\/ consecutive card numbers do not necessarily correspond to consecutive\n+\/\/ address ranges.  For the traditional direct-card-marking\n+\/\/ implementation of this interface, consecutive card numbers are\n+\/\/ likely to correspond to contiguous regions of memory, but this\n+\/\/ should not be assumed.  Instead, rely only upon the following:\n+\/\/\n+\/\/  1. All card numbers for cards pertaining to the same\n+\/\/     ShenandoahHeapRegion are consecutively numbered.\n+\/\/  2. In the case that neighboring ShenandoahHeapRegions both\n+\/\/     represent old-gen memory, the card regions that span the\n+\/\/     boundary between these neighboring heap regions will be\n+\/\/     consecutively numbered.\n+\/\/  3. (A corollary) In the case that an old-gen object spans the\n+\/\/     boundary between two heap regions, the card regions that\n+\/\/     correspond to the span of this object will be consecutively\n+\/\/     numbered.\n+\n+\n+\/\/ ShenandoahCardCluster abstracts access to the remembered set\n+\/\/ and also keeps track of crossing map information to allow efficient\n+\/\/ resolution of object start addresses.\n+\/\/\n+\/\/ ShenandoahCardCluster supports all of the services of\n+\/\/ RememberedSet, plus it supports register_object() and lookup_object().\n+\/\/\n+\/\/ There are two situations under which we need to know the location\n+\/\/ at which the object spanning the start of a particular card-table\n+\/\/ memory region begins:\n+\/\/\n+\/\/ 1. When we begin to scan dirty card memory that is not the\n+\/\/    first card region within a cluster, and the object that\n+\/\/    crosses into this card memory was not previously scanned,\n+\/\/    we need to find where that object starts so we can scan it.\n+\/\/    (Asides: if the objects starts within a previous cluster, it\n+\/\/     has already been scanned.  If the object starts within this\n+\/\/     cluster and it spans at least one card region that is dirty\n+\/\/     and precedes this card region within the cluster, then it has\n+\/\/     already been scanned.)\n+\/\/ 2. When we are otherwise done scanning a complete cluster, if the\n+\/\/    last object within the cluster reaches into the following\n+\/\/    cluster, we need to scan this object.  Thus, we need to find\n+\/\/    its starting location.\n+\/\/\n+\/\/ The RememberedSet template parameter is intended to represent either\n+\/\/     ShenandoahDirectCardMarkRememberedSet, or a to-be-implemented\n+\/\/     ShenandoahBufferWithSATBRememberedSet.\n+template<typename RememberedSet>\n+class ShenandoahCardCluster: public CHeapObj<mtGC> {\n+\n+private:\n+  RememberedSet *_rs;\n+\n+public:\n+  static const size_t CardsPerCluster = 64;\n+\n+private:\n+  typedef struct cross_map { uint8_t first; uint8_t last; } xmap;\n+  typedef union crossing_info { uint16_t short_word; xmap offsets; } crossing_info;\n+\n+  \/\/ ObjectStartsInCardRegion bit is set within a crossing_info.offsets.start iff at least one object starts within\n+  \/\/ a particular card region.  We pack this bit into start byte under assumption that start byte is accessed less\n+  \/\/ frequently that last byte.  This is true when number of clean cards is greater than number of dirty cards.\n+  static const uint16_t ObjectStartsInCardRegion = 0x80;\n+  static const uint16_t FirstStartBits           = 0x3f;\n+\n+  crossing_info *object_starts;\n+\n+public:\n+  \/\/ If we're setting first_start, assume the card has an object.\n+  inline void set_first_start(size_t card_index, uint8_t value) {\n+    object_starts[card_index].offsets.first = ObjectStartsInCardRegion | value;\n+  }\n+\n+  inline void set_last_start(size_t card_index, uint8_t value) {\n+    object_starts[card_index].offsets.last = value;\n+  }\n+\n+  inline void set_has_object_bit(size_t card_index) {\n+    object_starts[card_index].offsets.first |= ObjectStartsInCardRegion;\n+  }\n+\n+  inline void clear_has_object_bit(size_t card_index) {\n+    object_starts[card_index].offsets.first &= ~ObjectStartsInCardRegion;\n+  }\n+\n+  \/\/ Returns true iff an object is known to start within the card memory associated with card card_index.\n+  inline bool has_object(size_t card_index) {\n+    return (object_starts[card_index].offsets.first & ObjectStartsInCardRegion) != 0;\n+  }\n+\n+  inline void clear_objects_in_range(HeapWord *addr, size_t num_words) {\n+    size_t card_index = _rs->card_index_for_addr(addr);\n+    size_t last_card_index = _rs->card_index_for_addr(addr + num_words - 1);\n+    while (card_index <= last_card_index)\n+      object_starts[card_index++].short_word = 0;\n+  }\n+\n+  ShenandoahCardCluster(RememberedSet *rs) {\n+    _rs = rs;\n+    \/\/ TODO: We don't really need object_starts entries for every card entry.  We only need these for\n+    \/\/ the card entries that correspond to old-gen memory.  But for now, let's be quick and dirty.\n+    object_starts = (crossing_info *) malloc(rs->total_cards() * sizeof(crossing_info));\n+    if (object_starts == nullptr)\n+      fatal(\"Insufficient memory for initializing heap\");\n+    for (size_t i = 0; i < rs->total_cards(); i++)\n+      object_starts[i].short_word = 0;\n+  }\n+\n+  ~ShenandoahCardCluster() {\n+    if (object_starts != nullptr)\n+      free(object_starts);\n+    object_starts = nullptr;\n+  }\n+\n+  \/\/ There is one entry within the object_starts array for each card entry.\n+  \/\/\n+  \/\/ In the most recent implementation of ShenandoahScanRemembered::process_clusters(),\n+  \/\/ there is no need for the get_crossing_object_start() method function, so there is no\n+  \/\/ need to maintain the following information.  The comment is left in place for now in\n+  \/\/ case we find it necessary to add support for this service at a later time.\n+  \/\/\n+  \/\/ Bits 0x7fff: If no object starts within this card region, the\n+  \/\/              remaining bits of the object_starts array represent\n+  \/\/              the absolute word offset within the enclosing\n+  \/\/              cluster's memory of the starting address for the\n+  \/\/              object that spans the start of this card region's\n+  \/\/              memory.  If the spanning object begins in memory\n+  \/\/              that precedes this card region's cluster, the value\n+  \/\/              stored in these bits is the special value 0x7fff.\n+  \/\/              (Note that the maximum value required to represent a\n+  \/\/              spanning object from within the current cluster is\n+  \/\/              ((63 * 64) - 8), which equals 0x0fbf.\n+  \/\/\n+  \/\/ In the absence of the need to support get_crossing_object_start(),\n+  \/\/ here is discussion of performance:\n+  \/\/\n+  \/\/  Suppose multiple garbage objects are coalesced during GC sweep\n+  \/\/  into a single larger \"free segment\".  As each two objects are\n+  \/\/  coalesced together, the start information pertaining to the second\n+  \/\/  object must be removed from the objects_starts array.  If the\n+  \/\/  second object had been been the first object within card memory,\n+  \/\/  the new first object is the object that follows that object if\n+  \/\/  that starts within the same card memory, or NoObject if the\n+  \/\/  following object starts within the following cluster.  If the\n+  \/\/  second object had been the last object in the card memory,\n+  \/\/  replace this entry with the newly coalesced object if it starts\n+  \/\/  within the same card memory, or with NoObject if it starts in a\n+  \/\/  preceding card's memory.\n+  \/\/\n+  \/\/  Suppose a large free segment is divided into a smaller free\n+  \/\/  segment and a new object.  The second part of the newly divided\n+  \/\/  memory must be registered as a new object, overwriting at most\n+  \/\/  one first_start and one last_start entry.  Note that one of the\n+  \/\/  newly divided two objects might be a new GCLAB.\n+  \/\/\n+  \/\/  Suppose postprocessing of a GCLAB finds that the original GCLAB\n+  \/\/  has been divided into N objects.  Each of the N newly allocated\n+  \/\/  objects will be registered, overwriting at most one first_start\n+  \/\/  and one last_start entries.\n+  \/\/\n+  \/\/  No object registration operations are linear in the length of\n+  \/\/  the registered objects.\n+  \/\/\n+  \/\/ Consider further the following observations regarding object\n+  \/\/ registration costs:\n+  \/\/\n+  \/\/   1. The cost is paid once for each old-gen object (Except when\n+  \/\/      an object is demoted and repromoted, in which case we would\n+  \/\/      pay the cost again).\n+  \/\/   2. The cost can be deferred so that there is no urgency during\n+  \/\/      mutator copy-on-first-access promotion.  Background GC\n+  \/\/      threads will update the object_starts array by post-\n+  \/\/      processing the contents of retired PLAB buffers.\n+  \/\/   3. The bet is that these costs are paid relatively rarely\n+  \/\/      because:\n+  \/\/      a) Most objects die young and objects that die in young-gen\n+  \/\/         memory never need to be registered with the object_starts\n+  \/\/         array.\n+  \/\/      b) Most objects that are promoted into old-gen memory live\n+  \/\/         there without further relocation for a relatively long\n+  \/\/         time, so we get a lot of benefit from each investment\n+  \/\/         in registering an object.\n+\n+public:\n+\n+  \/\/ The starting locations of objects contained within old-gen memory\n+  \/\/ are registered as part of the remembered set implementation.  This\n+  \/\/ information is required when scanning dirty card regions that are\n+  \/\/ spanned by objects beginning within preceding card regions.  It\n+  \/\/ is necessary to find the first and last objects that begin within\n+  \/\/ this card region.  Starting addresses of objects are required to\n+  \/\/ find the object headers, and object headers provide information\n+  \/\/ about which fields within the object hold addresses.\n+  \/\/\n+  \/\/ The old-gen memory allocator invokes register_object() for any\n+  \/\/ object that is allocated within old-gen memory.  This identifies\n+  \/\/ the starting addresses of objects that span boundaries between\n+  \/\/ card regions.\n+  \/\/\n+  \/\/ It is not necessary to invoke register_object at the very instant\n+  \/\/ an object is allocated.  It is only necessary to invoke it\n+  \/\/ prior to the next start of a garbage collection concurrent mark\n+  \/\/ or concurrent update-references phase.  An \"ideal\" time to register\n+  \/\/ objects is during post-processing of a GCLAB after the GCLAB is\n+  \/\/ retired due to depletion of its memory.\n+  \/\/\n+  \/\/ register_object() does not perform synchronization.  In the case\n+  \/\/ that multiple threads are registering objects whose starting\n+  \/\/ addresses are within the same cluster, races between these\n+  \/\/ threads may result in corruption of the object-start data\n+  \/\/ structures.  Parallel GC threads should avoid registering objects\n+  \/\/ residing within the same cluster by adhering to the following\n+  \/\/ coordination protocols:\n+  \/\/\n+  \/\/  1. Align thread-local GCLAB buffers with some TBD multiple of\n+  \/\/     card clusters.  The card cluster size is 32 KB.  If the\n+  \/\/     desired GCLAB size is 128 KB, align the buffer on a multiple\n+  \/\/     of 4 card clusters.\n+  \/\/  2. Post-process the contents of GCLAB buffers to register the\n+  \/\/     objects allocated therein.  Allow one GC thread at a\n+  \/\/     time to do the post-processing of each GCLAB.\n+  \/\/  3. Since only one GC thread at a time is registering objects\n+  \/\/     belonging to a particular allocation buffer, no locking\n+  \/\/     is performed when registering these objects.\n+  \/\/  4. Any remnant of unallocated memory within an expended GC\n+  \/\/     allocation buffer is not returned to the old-gen allocation\n+  \/\/     pool until after the GC allocation buffer has been post\n+  \/\/     processed.  Before any remnant memory is returned to the\n+  \/\/     old-gen allocation pool, the GC thread that scanned this GC\n+  \/\/     allocation buffer performs a write-commit memory barrier.\n+  \/\/  5. Background GC threads that perform tenuring of young-gen\n+  \/\/     objects without a GCLAB use a CAS lock before registering\n+  \/\/     each tenured object.  The CAS lock assures both mutual\n+  \/\/     exclusion and memory coherency\/visibility.  Note that an\n+  \/\/     object tenured by a background GC thread will not overlap\n+  \/\/     with any of the clusters that are receiving tenured objects\n+  \/\/     by way of GCLAB buffers.  Multiple independent GC threads may\n+  \/\/     attempt to tenure objects into a shared cluster.  This is why\n+  \/\/     sychronization may be necessary.  Consider the following\n+  \/\/     scenarios:\n+  \/\/\n+  \/\/     a) If two objects are tenured into the same card region, each\n+  \/\/        registration may attempt to modify the first-start or\n+  \/\/        last-start information associated with that card region.\n+  \/\/        Furthermore, because the representations of first-start\n+  \/\/        and last-start information within the object_starts array\n+  \/\/        entry uses different bits of a shared uint_16 to represent\n+  \/\/        each, it is necessary to lock the entire card entry\n+  \/\/        before modifying either the first-start or last-start\n+  \/\/        information within the entry.\n+  \/\/     b) Suppose GC thread X promotes a tenured object into\n+  \/\/        card region A and this tenured object spans into\n+  \/\/        neighboring card region B.  Suppose GC thread Y (not equal\n+  \/\/        to X) promotes a tenured object into cluster B.  GC thread X\n+  \/\/        will update the object_starts information for card A.  No\n+  \/\/        synchronization is required.\n+  \/\/     c) In summary, when background GC threads register objects\n+  \/\/        newly tenured into old-gen memory, they must acquire a\n+  \/\/        mutual exclusion lock on the card that holds the starting\n+  \/\/        address of the newly tenured object.  This can be achieved\n+  \/\/        by using a CAS instruction to assure that the previous\n+  \/\/        values of first-offset and last-offset have not been\n+  \/\/        changed since the same thread inquired as to their most\n+  \/\/        current values.\n+  \/\/\n+  \/\/     One way to minimize the need for synchronization between\n+  \/\/     background tenuring GC threads is for each tenuring GC thread\n+  \/\/     to promote young-gen objects into distinct dedicated cluster\n+  \/\/     ranges.\n+  \/\/  6. The object_starts information is only required during the\n+  \/\/     starting of concurrent marking and concurrent evacuation\n+  \/\/     phases of GC.  Before we start either of these GC phases, the\n+  \/\/     JVM enters a safe point and all GC threads perform\n+  \/\/     commit-write barriers to assure that access to the\n+  \/\/     object_starts information is coherent.\n+\n+\n+  \/\/ Notes on synchronization of register_object():\n+  \/\/\n+  \/\/  1. For efficiency, there is no locking in the implementation of register_object()\n+  \/\/  2. Thus, it is required that users of this service assure that concurrent\/parallel invocations of\n+  \/\/     register_object() do pertain to the same card's memory range.  See discussion below to undestand\n+  \/\/     the risks.\n+  \/\/  3. When allocating from a TLAB or GCLAB, the mutual exclusion can be guaranteed by assuring that each\n+  \/\/     LAB's start and end are aligned on card memory boundaries.\n+  \/\/  4. Use the same lock that guarantees exclusivity when performing free-list allocation within heap regions.\n+  \/\/\n+  \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+  \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+  \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+  \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+  \/\/\n+  \/\/ objects being \"concurrently\" allocated:\n+  \/\/    [-----a------][-----b-----][--------------c------------------]\n+  \/\/            [---- card table memory range --------------]\n+  \/\/\n+  \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+  \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+  \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/\n+  \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+  \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+  \/\/ invocations associated with objects that are allocated from \"free lists\" to provide their own mutual exclusion locking\n+  \/\/ mechanism.\n+\n+  \/\/ Reset the has_object() information to false for all cards in the range between from and to.\n+  void reset_object_range(HeapWord *from, HeapWord *to);\n+\n+  \/\/ register_object() requires that the caller hold the heap lock\n+  \/\/ before calling it.\n+  void register_object(HeapWord* address);\n+\n+  \/\/ register_object_wo_lock() does not require that the caller hold\n+  \/\/ the heap lock before calling it, under the assumption that the\n+  \/\/ caller has assure no other thread will endeavor to concurrently\n+  \/\/ register objects that start within the same card's memory region\n+  \/\/ as address.\n+  void register_object_wo_lock(HeapWord* address);\n+\n+  \/\/ During the reference updates phase of GC, we walk through each old-gen memory region that was\n+  \/\/ not part of the collection set and we invalidate all unmarked objects.  As part of this effort,\n+  \/\/ we coalesce neighboring dead objects in order to make future remembered set scanning more\n+  \/\/ efficient (since future remembered set scanning of any card region containing consecutive\n+  \/\/ dead objects can skip over all of them at once by reading only a single dead object header\n+  \/\/ instead of having to read the header of each of the coalesced dead objects.\n+  \/\/\n+  \/\/ At some future time, we may implement a further optimization: satisfy future allocation requests\n+  \/\/ by carving new objects out of the range of memory that represents the coalesced dead objects.\n+  \/\/\n+  \/\/ Suppose we want to combine several dead objects into a single coalesced object.  How does this\n+  \/\/ impact our representation of crossing map information?\n+  \/\/  1. If the newly coalesced range is contained entirely within a card range, that card's last\n+  \/\/     start entry either remains the same or it is changed to the start of the coalesced region.\n+  \/\/  2. For the card that holds the start of the coalesced object, it will not impact the first start\n+  \/\/     but it may impact the last start.\n+  \/\/  3. For following cards spanned entirely by the newly coalesced object, it will change has_object\n+  \/\/     to false (and make first-start and last-start \"undefined\").\n+  \/\/  4. For a following card that is spanned patially by the newly coalesced object, it may change\n+  \/\/     first-start value, but it will not change the last-start value.\n+  \/\/\n+  \/\/ The range of addresses represented by the arguments to coalesce_objects() must represent a range\n+  \/\/ of memory that was previously occupied exactly by one or more previously registered objects.  For\n+  \/\/ convenience, it is legal to invoke coalesce_objects() with arguments that span a single previously\n+  \/\/ registered object.\n+  \/\/\n+  \/\/ The role of coalesce_objects is to change the crossing map information associated with all of the coalesced\n+  \/\/ objects.\n+  void coalesce_objects(HeapWord* address, size_t length_in_words);\n+\n+  \/\/ The typical use case is going to look something like this:\n+  \/\/   for each heapregion that comprises old-gen memory\n+  \/\/     for each card number that corresponds to this heap region\n+  \/\/       scan the objects contained therein if the card is dirty\n+  \/\/ To avoid excessive lookups in a sparse array, the API queries\n+  \/\/ the card number pertaining to a particular address and then uses the\n+  \/\/ card noumber for subsequent information lookups and stores.\n+\n+  \/\/ If has_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the first object begins.  If !has_object(card_index), the\n+  \/\/ result is a don't care value.\n+  size_t get_first_start(size_t card_index);\n+\n+  \/\/ If has_object(card_index), this returns the word offset within this card\n+  \/\/ memory at which the last object begins.  If !has_object(card_index), the\n+  \/\/ result is a don't care value.\n+  size_t get_last_start(size_t card_index);\n+\n+};\n+\n+\/\/ ShenandoahScanRemembered is a concrete class representing the\n+\/\/ ability to scan the old-gen remembered set for references to\n+\/\/ objects residing in young-gen memory.\n+\/\/\n+\/\/ Scanning normally begins with an invocation of numRegions and ends\n+\/\/ after all clusters of all regions have been scanned.\n+\/\/\n+\/\/ Throughout the scanning effort, the number of regions does not\n+\/\/ change.\n+\/\/\n+\/\/ Even though the regions that comprise old-gen memory are not\n+\/\/ necessarily contiguous, the abstraction represented by this class\n+\/\/ identifies each of the old-gen regions with an integer value\n+\/\/ in the range from 0 to (numRegions() - 1) inclusive.\n+\/\/\n+\n+template<typename RememberedSet>\n+class ShenandoahScanRemembered: public CHeapObj<mtGC> {\n+\n+private:\n+\n+  RememberedSet* _rs;\n+  ShenandoahCardCluster<RememberedSet>* _scc;\n+\n+public:\n+  \/\/ How to instantiate this object?\n+  \/\/   ShenandoahDirectCardMarkRememberedSet *rs =\n+  \/\/       new ShenandoahDirectCardMarkRememberedSet();\n+  \/\/   scr = new\n+  \/\/     ShenandoahScanRememberd<ShenandoahDirectCardMarkRememberedSet>(rs);\n+  \/\/\n+  \/\/ or, after the planned implementation of\n+  \/\/ ShenandoahBufferWithSATBRememberedSet has been completed:\n+  \/\/\n+  \/\/   ShenandoahBufferWithSATBRememberedSet *rs =\n+  \/\/       new ShenandoahBufferWithSATBRememberedSet();\n+  \/\/   scr = new\n+  \/\/     ShenandoahScanRememberd<ShenandoahBufferWithSATBRememberedSet>(rs);\n+\n+\n+  ShenandoahScanRemembered(RememberedSet *rs) {\n+    _rs = rs;\n+    _scc = new ShenandoahCardCluster<RememberedSet>(rs);\n+  }\n+\n+  ~ShenandoahScanRemembered() {\n+    delete _scc;\n+  }\n+\n+  \/\/ TODO:  We really don't want to share all of these APIs with arbitrary consumers of the ShenandoahScanRemembered abstraction.\n+  \/\/ But in the spirit of quick and dirty for the time being, I'm going to go ahead and publish everything for right now.  Some\n+  \/\/ of existing code already depends on having access to these services (because existing code has not been written to honor\n+  \/\/ full abstraction of remembered set scanning.  In the not too distant future, we want to try to make most, if not all, of\n+  \/\/ these services private.  Two problems with publicizing:\n+  \/\/  1. Allowing arbitrary users to reach beneath the hood allows the users to make assumptions about underlying implementation.\n+  \/\/     This will make it more difficult to change underlying implementation at a future time, such as when we eventually experiment\n+  \/\/     with SATB-based implementation of remembered set representation.\n+  \/\/  2. If we carefully control sharing of certain of these services, we can reduce the overhead of synchronization by assuring\n+  \/\/     that all users follow protocols that avoid contention that might require synchronization.  When we publish these APIs, we\n+  \/\/     lose control over who and how the data is accessed.  As a result, we are required to insert more defensive measures into\n+  \/\/     the implementation, including synchronization locks.\n+\n+\n+  \/\/ Card index is zero-based relative to first spanned card region.\n+  size_t total_cards();\n+  size_t card_index_for_addr(HeapWord *p);\n+  HeapWord *addr_for_card_index(size_t card_index);\n+  bool is_card_dirty(size_t card_index);\n+  bool is_write_card_dirty(size_t card_index) { return _rs->is_write_card_dirty(card_index); }\n+  void mark_card_as_dirty(size_t card_index);\n+  void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  void mark_card_as_clean(size_t card_index);\n+  void mark_read_card_as_clean(size_t card_index) { _rs->mark_read_card_clean(card_index); }\n+  void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  void mark_overreach_card_as_dirty(size_t card_index);\n+  bool is_card_dirty(HeapWord *p);\n+  void mark_card_as_dirty(HeapWord *p);\n+  void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n+  void mark_card_as_clean(HeapWord *p);\n+  void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n+  void mark_overreach_card_as_dirty(void *p);\n+  size_t cluster_count();\n+  void initialize_overreach(size_t first_cluster, size_t count);\n+  void merge_overreach(size_t first_cluster, size_t count);\n+\n+  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n+  void swap_remset() { _rs->swap_remset(); }\n+\n+  void reset_remset(HeapWord* start, size_t word_count) { _rs->reset_remset(start, word_count); }\n+\n+  void merge_write_table(HeapWord* start, size_t word_count) { _rs->merge_write_table(start, word_count); }\n+\n+  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n+  void clear_old_remset() { _rs->clear_old_remset(); }\n+\n+  size_t cluster_for_addr(HeapWord *addr);\n+\n+  void reset_object_range(HeapWord *from, HeapWord *to);\n+  void register_object(HeapWord *addr);\n+  void register_object_wo_lock(HeapWord *addr);\n+  void coalesce_objects(HeapWord *addr, size_t length_in_words);\n+\n+  \/\/ Return true iff this object is \"properly\" registered.\n+  bool verify_registration(HeapWord* address, ShenandoahMarkingContext* ctx);\n+\n+  \/\/ clear the cards to clean, and clear the object_starts info to no objects\n+  void mark_range_as_empty(HeapWord *addr, size_t length_in_words);\n+\n+  \/\/ process_clusters() scans a portion of the remembered set during a JVM\n+  \/\/ safepoint as part of the root scanning activities that serve to\n+  \/\/ initiate concurrent scanning and concurrent evacuation.  Multiple\n+  \/\/ threads may scan different portions of the remembered set by\n+  \/\/ making parallel invocations of process_clusters() with each\n+  \/\/ invocation scanning different clusters of the remembered set.\n+  \/\/\n+  \/\/ An invocation of process_clusters() examines all of the\n+  \/\/ intergenerational references spanned by count clusters starting\n+  \/\/ with first_cluster.  The oops argument is assumed to represent a\n+  \/\/ thread-local OopClosure into which addresses of intergenerational\n+  \/\/ pointer values will be accumulated for the purposes of root scanning.\n+  \/\/\n+  \/\/ A side effect of executing process_clusters() is to update the card\n+  \/\/ table entries, marking dirty cards as clean if they no longer\n+  \/\/ hold references to young-gen memory.  (THIS IS NOT YET IMPLEMENTED.)\n+  \/\/\n+  \/\/ The implementation of process_clusters() is designed to efficiently\n+  \/\/ minimize work in the large majority of cases for which the\n+  \/\/ associated cluster has very few dirty card-table entries.\n+  \/\/\n+  \/\/ At initialization of concurrent marking, invoke process_clusters with\n+  \/\/ ClosureType equal to ShenandoahInitMarkRootsClosure.\n+  \/\/\n+  \/\/ At initialization of concurrent evacuation, invoke process_clusters with\n+  \/\/ ClosureType equal to ShenandoahEvacuateUpdateRootsClosure.\n+\n+  \/\/ This is big enough it probably shouldn't be in-lined.  On the other hand, there are only a few places this\n+  \/\/ code is called from, so it might as well be in-lined.  The \"real\" reason I'm inlining at the moment is because\n+  \/\/ the template expansions were making it difficult for the link\/loader to resolve references to the template-\n+  \/\/ parameterized implementations of this service.\n+  template <typename ClosureType>\n+  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops);\n+\n+  template <typename ClosureType>\n+  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops, bool use_write_table);\n+\n+  template <typename ClosureType>\n+  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl);\n+\n+  template <typename ClosureType>\n+  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl, bool use_write_table);\n+\n+  \/\/ To Do:\n+  \/\/  Create subclasses of ShenandoahInitMarkRootsClosure and\n+  \/\/  ShenandoahEvacuateUpdateRootsClosure and any other closures\n+  \/\/  that need to participate in remembered set scanning.  Within the\n+  \/\/  subclasses, add a (probably templated) instance variable that\n+  \/\/  refers to the associated ShenandoahCardCluster object.  Use this\n+  \/\/  ShenandoahCardCluster instance to \"enhance\" the do_oops\n+  \/\/  processing so that we can:\n+  \/\/\n+  \/\/   1. Avoid processing references that correspond to clean card\n+  \/\/      regions, and\n+  \/\/   2. Set card status to CLEAN when the associated card region no\n+  \/\/      longer holds inter-generatioanal references.\n+  \/\/\n+  \/\/  To enable efficient implementation of these behaviors, we\n+  \/\/  probably also want to add a few fields into the\n+  \/\/  ShenandoahCardCluster object that allow us to precompute and\n+  \/\/  remember the addresses at which card status is going to change\n+  \/\/  from dirty to clean and clean to dirty.  The do_oops\n+  \/\/  implementations will want to update this value each time they\n+  \/\/  cross one of these boundaries.\n+  void roots_do(OopIterateClosure* cl);\n+};\n+\n+typedef ShenandoahScanRemembered<ShenandoahDirectCardMarkRememberedSet> RememberedScanner;\n+\n+class ShenandoahScanRememberedTask : public WorkerTask {\n+ private:\n+  ShenandoahObjToScanQueueSet* _queue_set;\n+  ShenandoahObjToScanQueueSet* _old_queue_set;\n+  ShenandoahReferenceProcessor* _rp;\n+  ShenandoahRegionIterator* _regions;\n+ public:\n+  ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                               ShenandoahObjToScanQueueSet* old_queue_set,\n+                               ShenandoahReferenceProcessor* rp,\n+                               ShenandoahRegionIterator* regions);\n+\n+  void work(uint worker_id);\n+};\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":1021,"deletions":0,"binary":false,"changes":1021,"status":"added"},{"patch":"@@ -0,0 +1,739 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. or its affiliates.  All rights reserved.\n+ *\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n+\n+#include \"memory\/iterator.hpp\"\n+#include \"oops\/oop.hpp\"\n+#include \"oops\/objArrayOop.hpp\"\n+#include \"gc\/shared\/collectorCounters.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::total_cards() {\n+  return _total_card_count;\n+}\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::card_index_for_addr(HeapWord *p) {\n+  return _card_table->index_for(p);\n+}\n+\n+inline HeapWord *\n+ShenandoahDirectCardMarkRememberedSet::addr_for_card_index(size_t card_index) {\n+  return _whole_heap_base + CardTable::card_size_in_words() * card_index;\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_write_card_dirty(size_t card_index) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(size_t card_index) {\n+  uint8_t *bp = &(_card_table->read_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(size_t card_index) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(size_t card_index, size_t num_cards) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n+    *bp++ = CardTable::dirty_card_val();\n+  }\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_clean(size_t card_index) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(size_t card_index, size_t num_cards) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n+    *bp++ = CardTable::clean_card_val();\n+  }\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_overreach_card_as_dirty(size_t card_index) {\n+  uint8_t *bp = &_overreach_map[card_index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(HeapWord *p) {\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->read_byte_map())[index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(HeapWord *p) {\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->write_byte_map())[index];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(HeapWord *p, size_t num_heap_words) {\n+  uint8_t *bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  uint8_t *end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to dirty last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size() - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n+    *bp++ = CardTable::dirty_card_val();\n+  }\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_clean(HeapWord *p) {\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->write_byte_map())[index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_read_card_as_clean(size_t index) {\n+  uint8_t *bp = &(_card_table->read_byte_map())[index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(HeapWord *p, size_t num_heap_words) {\n+  uint8_t *bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  uint8_t *end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to clean last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size() - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n+    *bp++ = CardTable::clean_card_val();\n+  }\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_overreach_card_as_dirty(void *p) {\n+  uint8_t *bp = &_overreach_map_base[uintptr_t(p) >> _card_shift];\n+  bp[0] = CardTable::dirty_card_val();\n+}\n+\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::cluster_count() {\n+  return _cluster_count;\n+}\n+\n+\/\/ No lock required because arguments align with card boundaries.\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::reset_object_range(HeapWord* from, HeapWord* to) {\n+  assert(((((unsigned long long) from) & (CardTable::card_size() - 1)) == 0) &&\n+         ((((unsigned long long) to) & (CardTable::card_size() - 1)) == 0),\n+         \"reset_object_range bounds must align with card boundaries\");\n+  size_t card_at_start = _rs->card_index_for_addr(from);\n+  size_t num_cards = (to - from) \/ CardTable::card_size_in_words();\n+\n+  for (size_t i = 0; i < num_cards; i++) {\n+    object_starts[card_at_start + i].short_word = 0;\n+  }\n+}\n+\n+\/\/ Assume only one thread at a time registers objects pertaining to\n+\/\/ each card-table entry's range of memory.\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::register_object(HeapWord* address) {\n+  shenandoah_assert_heaplocked();\n+\n+  register_object_wo_lock(address);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::register_object_wo_lock(HeapWord* address) {\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord *card_start_address = _rs->addr_for_card_index(card_at_start);\n+  uint8_t offset_in_card = address - card_start_address;\n+\n+  if (!has_object(card_at_start)) {\n+    set_has_object_bit(card_at_start);\n+    set_first_start(card_at_start, offset_in_card);\n+    set_last_start(card_at_start, offset_in_card);\n+  } else {\n+    if (offset_in_card < get_first_start(card_at_start))\n+      set_first_start(card_at_start, offset_in_card);\n+    if (offset_in_card > get_last_start(card_at_start))\n+      set_last_start(card_at_start, offset_in_card);\n+  }\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::coalesce_objects(HeapWord* address, size_t length_in_words) {\n+\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord *card_start_address = _rs->addr_for_card_index(card_at_start);\n+  size_t card_at_end = card_at_start + ((address + length_in_words) - card_start_address) \/ CardTable::card_size_in_words();\n+\n+  if (card_at_start == card_at_end) {\n+    \/\/ There are no changes to the get_first_start array.  Either get_first_start(card_at_start) returns this coalesced object,\n+    \/\/ or it returns an object that precedes the coalesced object.\n+    if (card_start_address + get_last_start(card_at_start) < address + length_in_words) {\n+      uint8_t coalesced_offset = static_cast<uint8_t>(address - card_start_address);\n+      \/\/ The object that used to be the last object starting within this card is being subsumed within the coalesced\n+      \/\/ object.  Since we always coalesce entire objects, this condition only occurs if the last object ends before or at\n+      \/\/ the end of the card's memory range and there is no object following this object.  In this case, adjust last_start\n+      \/\/ to represent the start of the coalesced range.\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ Else, no changes to last_starts information.  Either get_last_start(card_at_start) returns the object that immediately\n+    \/\/ follows the coalesced object, or it returns an object that follows the object immediately following the coalesced object.\n+  } else {\n+    uint8_t coalesced_offset = static_cast<uint8_t>(address - card_start_address);\n+    if (get_last_start(card_at_start) > coalesced_offset) {\n+      \/\/ Existing last start is being coalesced, create new last start\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ otherwise, get_last_start(card_at_start) must equal coalesced_offset\n+\n+    \/\/ All the cards between first and last get cleared.\n+    for (size_t i = card_at_start + 1; i < card_at_end; i++) {\n+      clear_has_object_bit(i);\n+    }\n+\n+    uint8_t follow_offset = static_cast<uint8_t>((address + length_in_words) - _rs->addr_for_card_index(card_at_end));\n+    if (has_object(card_at_end) && (get_first_start(card_at_end) < follow_offset)) {\n+      \/\/ It may be that after coalescing within this last card's memory range, the last card\n+      \/\/ no longer holds an object.\n+      if (get_last_start(card_at_end) >= follow_offset) {\n+        set_first_start(card_at_end, follow_offset);\n+      } else {\n+        \/\/ last_start is being coalesced so this card no longer has any objects.\n+        clear_has_object_bit(card_at_end);\n+      }\n+    }\n+    \/\/ else\n+    \/\/  card_at_end did not have an object, so it still does not have an object, or\n+    \/\/  card_at_end had an object that starts after the coalesced object, so no changes required for card_at_end\n+\n+  }\n+}\n+\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_first_start(size_t card_index) {\n+  assert(has_object(card_index), \"Can't get first start because no object starts here\");\n+  return object_starts[card_index].offsets.first & FirstStartBits;\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_last_start(size_t card_index) {\n+  assert(has_object(card_index), \"Can't get last start because no object starts here\");\n+  return object_starts[card_index].offsets.last;\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::total_cards() { return _rs->total_cards(); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::card_index_for_addr(HeapWord *p) { return _rs->card_index_for_addr(p); };\n+\n+template<typename RememberedSet>\n+inline HeapWord *\n+ShenandoahScanRemembered<RememberedSet>::addr_for_card_index(size_t card_index) { return _rs->addr_for_card_index(card_index); }\n+\n+template<typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::is_card_dirty(size_t card_index) { return _rs->is_card_dirty(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_dirty(size_t card_index) { _rs->mark_card_as_dirty(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_dirty(size_t card_index, size_t num_cards) { _rs->mark_range_as_dirty(card_index, num_cards); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_clean(size_t card_index) { _rs->mark_card_as_clean(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_clean(size_t card_index, size_t num_cards) { _rs->mark_range_as_clean(card_index, num_cards); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>:: mark_overreach_card_as_dirty(size_t card_index) { _rs->mark_overreach_card_as_dirty(card_index); }\n+\n+template<typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::is_card_dirty(HeapWord *p) { return _rs->is_card_dirty(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_dirty(HeapWord *p) { _rs->mark_card_as_dirty(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_dirty(HeapWord *p, size_t num_heap_words) { _rs->mark_range_as_dirty(p, num_heap_words); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_clean(HeapWord *p) { _rs->mark_card_as_clean(p); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>:: mark_range_as_clean(HeapWord *p, size_t num_heap_words) { _rs->mark_range_as_clean(p, num_heap_words); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_overreach_card_as_dirty(void *p) { _rs->mark_overreach_card_as_dirty(p); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::cluster_count() { return _rs->cluster_count(); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::initialize_overreach(size_t first_cluster, size_t count) { _rs->initialize_overreach(first_cluster, count); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::merge_overreach(size_t first_cluster, size_t count) { _rs->merge_overreach(first_cluster, count); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::reset_object_range(HeapWord *from, HeapWord *to) {\n+  _scc->reset_object_range(from, to);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::register_object(HeapWord *addr) {\n+  _scc->register_object(addr);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::register_object_wo_lock(HeapWord *addr) {\n+  _scc->register_object_wo_lock(addr);\n+}\n+\n+template <typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::verify_registration(HeapWord* address, ShenandoahMarkingContext* ctx) {\n+\n+  size_t index = card_index_for_addr(address);\n+  if (!_scc->has_object(index)) {\n+    return false;\n+  }\n+  HeapWord* base_addr = addr_for_card_index(index);\n+  size_t offset = _scc->get_first_start(index);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Verify that I can find this object within its enclosing card by scanning forward from first_start.\n+  while (base_addr + offset < address) {\n+    oop obj = cast_to_oop(base_addr + offset);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      offset += obj->size();\n+    } else {\n+      \/\/ If this object is not live, don't trust its size(); all objects above tams are live.\n+      ShenandoahHeapRegion* r = heap->heap_region_containing(obj);\n+      HeapWord* tams = ctx->top_at_mark_start(r);\n+      offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+    }\n+  }\n+  if (base_addr + offset != address){\n+    return false;\n+  }\n+\n+  \/\/ At this point, offset represents object whose registration we are verifying.  We know that at least this object resides\n+  \/\/ within this card's memory.\n+\n+  \/\/ Make sure that last_offset is properly set for the enclosing card, but we can't verify this for\n+  \/\/ candidate collection-set regions during mixed evacuations, so disable this check in general\n+  \/\/ during mixed evacuations.\n+\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(base_addr + offset);\n+  size_t max_offset = r->top() - base_addr;\n+  if (max_offset > CardTable::card_size_in_words()) {\n+    max_offset = CardTable::card_size_in_words();\n+  }\n+  size_t prev_offset;\n+  if (!ctx) {\n+    do {\n+      oop obj = cast_to_oop(base_addr + offset);\n+      prev_offset = offset;\n+      offset += obj->size();\n+    } while (offset < max_offset);\n+    if (_scc->get_last_start(index) != prev_offset) {\n+      return false;\n+    }\n+\n+    \/\/ base + offset represents address of first object that starts on following card, if there is one.\n+\n+    \/\/ Notes: base_addr is addr_for_card_index(index)\n+    \/\/        base_addr + offset is end of the object we are verifying\n+    \/\/        cannot use card_index_for_addr(base_addr + offset) because it asserts arg < end of whole heap\n+    size_t end_card_index = index + offset \/ CardTable::card_size_in_words();\n+\n+    if (end_card_index > index) {\n+      \/\/ If there is a following object registered on the next card, it should begin where this object ends.\n+      if ((base_addr + offset < _rs->whole_heap_end()) && _scc->has_object(end_card_index) &&\n+          ((addr_for_card_index(end_card_index) + _scc->get_first_start(end_card_index)) != (base_addr + offset))) {\n+        return false;\n+      }\n+    }\n+\n+    \/\/ Assure that no other objects are registered \"inside\" of this one.\n+    for (index++; index < end_card_index; index++) {\n+      if (_scc->has_object(index)) {\n+        return false;\n+      }\n+    }\n+  } else {\n+    \/\/ This is a mixed evacuation or a global collect: rely on mark bits to identify which objects need to be properly registered\n+    assert(!ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Cannot rely on mark context here.\");\n+    \/\/ If the object reaching or spanning the end of this card's memory is marked, then last_offset for this card\n+    \/\/ should represent this object.  Otherwise, last_offset is a don't care.\n+    ShenandoahHeapRegion* region = heap->heap_region_containing(base_addr + offset);\n+    HeapWord* tams = ctx->top_at_mark_start(region);\n+    oop last_obj = nullptr;\n+    do {\n+      oop obj = cast_to_oop(base_addr + offset);\n+      if (ctx->is_marked(obj)) {\n+        prev_offset = offset;\n+        offset += obj->size();\n+        last_obj = obj;\n+      } else {\n+        offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+        \/\/ offset will be zero if no objects are marked in this card.\n+      }\n+    } while (offset > 0 && offset < max_offset);\n+    if (last_obj != nullptr && prev_offset + last_obj->size() >= max_offset) {\n+      \/\/ last marked object extends beyond end of card\n+      if (_scc->get_last_start(index) != prev_offset) {\n+        return false;\n+      }\n+      \/\/ otherwise, the value of _scc->get_last_start(index) is a don't care because it represents a dead object and we\n+      \/\/ cannot verify its context\n+    }\n+  }\n+  return true;\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::coalesce_objects(HeapWord *addr, size_t length_in_words) {\n+  _scc->coalesce_objects(addr, length_in_words);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_empty(HeapWord *addr, size_t length_in_words) {\n+  _rs->mark_range_as_clean(addr, length_in_words);\n+  _scc->clear_objects_in_range(addr, length_in_words);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range,\n+                                                          ClosureType *cl) {\n+  process_clusters(first_cluster, count, end_of_range, cl, false);\n+}\n+\n+\/\/ Process all objects starting within count clusters beginning with first_cluster for which the start address is\n+\/\/ less than end_of_range.  For any such object, process the complete object, even if its end reaches beyond\n+\/\/ end_of_range.\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range,\n+                                                          ClosureType *cl, bool write_table) {\n+\n+  \/\/ Unlike traditional Shenandoah marking, the old-gen resident objects that are examined as part of the remembered set are not\n+  \/\/ themselves marked.  Each such object will be scanned only once.  Any young-gen objects referenced from the remembered set will\n+  \/\/ be marked and then subsequently scanned.\n+\n+  \/\/ If old-gen evacuation is active, then MarkingContext for old-gen heap regions is valid.  We use the MarkingContext\n+  \/\/ bits to determine which objects within a DIRTY card need to be scanned.  This is necessary because old-gen heap\n+  \/\/ regions which are in the candidate collection set have not been coalesced and filled.  Thus, these heap regions\n+  \/\/ may contain zombie objects.  Zombie objects are known to be dead, but have not yet been \"collected\".  Scanning\n+  \/\/ zombie objects is unsafe because the Klass pointer is not reliable, objects referenced from a zombie may have been\n+  \/\/ collected and their memory repurposed, and because zombie objects might refer to objects that are themselves dead.\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (heap->doing_mixed_evacuations() || heap->is_concurrent_prep_for_mixed_evacuation_in_progress()) {\n+    ctx = heap->marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  HeapWord* end_of_clusters = _rs->addr_for_card_index(first_cluster)\n+    + count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster * CardTable::card_size_in_words();\n+  while (count-- > 0) {\n+    size_t card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+    size_t end_card_index = card_index + ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+    first_cluster++;\n+    size_t next_card_index = 0;\n+    while (card_index < end_card_index) {\n+      bool is_dirty = (write_table)? is_write_card_dirty(card_index): is_card_dirty(card_index);\n+      bool has_object = _scc->has_object(card_index);\n+      if (is_dirty) {\n+        size_t prev_card_index = card_index;\n+        if (has_object) {\n+          \/\/ Scan all objects that start within this card region.\n+          size_t start_offset = _scc->get_first_start(card_index);\n+          HeapWord *p = _rs->addr_for_card_index(card_index);\n+          HeapWord *card_start = p;\n+          HeapWord *endp = p + CardTable::card_size_in_words();\n+          if (endp > end_of_range) {\n+            endp = end_of_range;\n+            next_card_index = end_card_index;\n+          } else {\n+            \/\/ endp either points to start of next card region, or to the next object that needs to be scanned, which may\n+            \/\/ reside in some successor card region.\n+\n+            \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n+            \/\/ failure if endp points to end of heap.\n+            next_card_index = card_index + (endp - card_start) \/ CardTable::card_size_in_words();\n+          }\n+\n+          p += start_offset;\n+          while (p < endp) {\n+            oop obj = cast_to_oop(p);\n+\n+            \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+            if (!ctx || ctx->is_marked(obj)) {\n+              \/\/ Future TODO:\n+              \/\/ For improved efficiency, we might want to give special handling of obj->is_objArray().  In\n+              \/\/ particular, in that case, we might want to divide the effort for scanning of a very long object array\n+              \/\/ between multiple threads.  Also, skip parts of the array that are not marked as dirty.\n+              if (obj->is_objArray()) {\n+                objArrayOop array = objArrayOop(obj);\n+                int len = array->length();\n+                array->oop_iterate_range(cl, 0, len);\n+              } else if (obj->is_instance()) {\n+                obj->oop_iterate(cl);\n+              } else {\n+                \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n+                \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n+                \/\/ We skip iterating over the klass pointer since we know that\n+                \/\/ Universe::TypeArrayKlass never moves.\n+                assert (obj->is_typeArray(), \"should be type array\");\n+              }\n+              p += obj->size();\n+            } else {\n+              \/\/ This object is not marked so we don't scan it.\n+              ShenandoahHeapRegion* r = heap->heap_region_containing(p);\n+              HeapWord* tams = ctx->top_at_mark_start(r);\n+              if (p >= tams) {\n+                p += obj->size();\n+              } else {\n+                p = ctx->get_next_marked_addr(p, tams);\n+              }\n+            }\n+          }\n+          if (p > endp) {\n+            card_index = card_index + (p - card_start) \/ CardTable::card_size_in_words();\n+          } else {                  \/\/ p == endp\n+            card_index = next_card_index;\n+          }\n+        } else {\n+          \/\/ Card is dirty but has no object.  Card will have been scanned during scan of a previous cluster.\n+          card_index++;\n+        }\n+      } else if (has_object) {\n+        \/\/ Card is clean but has object.\n+\n+        \/\/ Scan the last object that starts within this card memory if it spans at least one dirty card within this cluster\n+        \/\/ or if it reaches into the next cluster.\n+        size_t start_offset = _scc->get_last_start(card_index);\n+        HeapWord *card_start = _rs->addr_for_card_index(card_index);\n+        HeapWord *p = card_start + start_offset;\n+        oop obj = cast_to_oop(p);\n+\n+        size_t last_card;\n+        if (!ctx || ctx->is_marked(obj)) {\n+          HeapWord *nextp = p + obj->size();\n+\n+          \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n+          \/\/ failure if nextp points to end of heap.\n+          last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words();\n+\n+          bool reaches_next_cluster = (last_card > end_card_index);\n+          bool spans_dirty_within_this_cluster = false;\n+\n+          if (!reaches_next_cluster) {\n+            size_t span_card;\n+            for (span_card = card_index+1; span_card <= last_card; span_card++)\n+              if ((write_table)? _rs->is_write_card_dirty(span_card): _rs->is_card_dirty(span_card)) {\n+                spans_dirty_within_this_cluster = true;\n+                break;\n+              }\n+          }\n+\n+          if (reaches_next_cluster || spans_dirty_within_this_cluster) {\n+            if (obj->is_objArray()) {\n+              objArrayOop array = objArrayOop(obj);\n+              int len = array->length();\n+              array->oop_iterate_range(cl, 0, len);\n+            } else if (obj->is_instance()) {\n+              obj->oop_iterate(cl);\n+            } else {\n+              \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n+              \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n+              \/\/ We skip iterating over the klass pointer since we know that\n+              \/\/ Universe::TypeArrayKlass never moves.\n+              assert (obj->is_typeArray(), \"should be type array\");\n+            }\n+          }\n+        } else {\n+          \/\/ The object that spans end of this clean card is not marked, so no need to scan it or its\n+          \/\/ unmarked neighbors.\n+          ShenandoahHeapRegion* r = heap->heap_region_containing(p);\n+          HeapWord* tams = ctx->top_at_mark_start(r);\n+          HeapWord* nextp;\n+          if (p >= tams) {\n+            nextp = p + obj->size();\n+          } else {\n+            nextp = ctx->get_next_marked_addr(p, tams);\n+          }\n+          last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words();\n+        }\n+        \/\/ Increment card_index to account for the spanning object, even if we didn't scan it.\n+        card_index = (last_card > card_index)? last_card: card_index + 1;\n+      } else {\n+        \/\/ Card is clean and has no object.  No need to clean this card.\n+        card_index++;\n+      }\n+    }\n+  }\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl) {\n+  process_region(region, cl, false);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl, bool use_write_table) {\n+  HeapWord *start_of_range = region->bottom();\n+  size_t start_cluster_no = cluster_for_addr(start_of_range);\n+\n+  \/\/ region->end() represents the end of memory spanned by this region, but not all of this\n+  \/\/   memory is eligible to be scanned because some of this memory has not yet been allocated.\n+  \/\/\n+  \/\/ region->top() represents the end of allocated memory within this region.  Any addresses\n+  \/\/   beyond region->top() should not be scanned as that memory does not hold valid objects.\n+\n+  HeapWord *end_of_range;\n+  if (use_write_table) {\n+    \/\/ This is update-refs servicing.\n+    end_of_range = region->get_update_watermark();\n+  } else {\n+    \/\/ This is concurrent mark servicing.  Note that TAMS for this region is TAMS at start of old-gen\n+    \/\/ collection.  Here, we need to scan up to TAMS for most recently initiated young-gen collection.\n+    \/\/ Since all LABs are retired at init mark, and since replacement LABs are allocated lazily, and since no\n+    \/\/ promotions occur until evacuation phase, TAMS for most recent young-gen is same as top().\n+    end_of_range = region->top();\n+  }\n+\n+  log_debug(gc)(\"Remembered set scan processing Region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT \", using %s table\",\n+                region->index(), p2i(region->bottom()), p2i(end_of_range),\n+                use_write_table? \"read\/write (updating)\": \"read (marking)\");\n+  \/\/ end_of_range may point to the middle of a cluster because region->top() may be different than region->end().\n+  \/\/ We want to assure that our process_clusters() request spans all relevant clusters.  Note that each cluster\n+  \/\/ processed will avoid processing beyond end_of_range.\n+\n+  \/\/ Note that any object that starts between start_of_range and end_of_range, including humongous objects, will\n+  \/\/ be fully processed by process_clusters, even though the object may reach beyond end_of_range.\n+  size_t num_heapwords = end_of_range - start_of_range;\n+  unsigned int cluster_size = CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t num_clusters = (size_t) ((num_heapwords - 1 + cluster_size) \/ cluster_size);\n+\n+  if (!region->is_humongous_continuation()) {\n+    \/\/ Remembered set scanner\n+    process_clusters(start_cluster_no, num_clusters, end_of_range, cl, use_write_table);\n+  }\n+}\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::cluster_for_addr(HeapWordImpl **addr) {\n+  size_t card_index = _rs->card_index_for_addr(addr);\n+  size_t result = card_index \/ ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  return result;\n+}\n+\n+\/\/ This is used only for debug verification so don't worry about making the scan parallel.\n+template<typename RememberedSet>\n+inline void ShenandoahScanRemembered<RememberedSet>::roots_do(OopIterateClosure* cl) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  for (size_t i = 0, n = heap->num_regions(); i < n; ++i) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (region->is_old() && region->is_active() && !region->is_cset()) {\n+      HeapWord* start_of_range = region->bottom();\n+      HeapWord* end_of_range = region->top();\n+      size_t start_cluster_no = cluster_for_addr(start_of_range);\n+      size_t num_heapwords = end_of_range - start_of_range;\n+      unsigned int cluster_size = CardTable::card_size_in_words() *\n+                                  ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+      size_t num_clusters = (size_t) ((num_heapwords - 1 + cluster_size) \/ cluster_size);\n+\n+      \/\/ Remembered set scanner\n+      process_clusters(start_cluster_no, num_clusters, end_of_range, cl);\n+    }\n+  }\n+}\n+\n+#endif   \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBEREDINLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":739,"deletions":0,"binary":false,"changes":739,"status":"added"},{"patch":"@@ -44,0 +44,1 @@\n+  bool                    _plab_allows_promotion; \/\/ If false, no more promotion by this thread during this evacuation phase.\n@@ -45,0 +46,3 @@\n+\n+  \/\/ Thread-local allocation buffer for object evacuations.\n+  \/\/ In generational mode, it is exclusive to the young generation.\n@@ -47,0 +51,12 @@\n+\n+  \/\/ Thread-local allocation buffer only used in generational mode.\n+  \/\/ Used both by mutator threads and by GC worker threads\n+  \/\/ for evacuations within the old generation and\n+  \/\/ for promotions from the young generation into the old generation.\n+  PLAB* _plab;\n+  size_t _plab_size;\n+\n+  size_t _plab_evacuated;\n+  size_t _plab_promoted;\n+\n+  uint  _worker_id;\n@@ -57,0 +73,4 @@\n+    _plab(NULL),\n+    _plab_size(0),\n+    _plab_evacuated(0),\n+    _plab_promoted(0),\n@@ -69,0 +89,4 @@\n+    if (_plab != NULL) {\n+      ShenandoahHeap::heap()->retire_plab(_plab);\n+      delete _plab;\n+    }\n@@ -106,0 +130,2 @@\n+    data(thread)->_plab = new PLAB(PLAB::min_size());\n+    data(thread)->_plab_size = 0;\n@@ -120,0 +146,56 @@\n+  static PLAB* plab(Thread* thread) {\n+    return data(thread)->_plab;\n+  }\n+\n+  static size_t plab_size(Thread* thread) {\n+    return data(thread)->_plab_size;\n+  }\n+\n+  static void set_plab_size(Thread* thread, size_t v) {\n+    data(thread)->_plab_size = v;\n+  }\n+\n+  static void enable_plab_promotions(Thread* thread) {\n+    data(thread)->_plab_allows_promotion = true;\n+  }\n+\n+  static void disable_plab_promotions(Thread* thread) {\n+    data(thread)->_plab_allows_promotion = false;\n+  }\n+\n+  static bool allow_plab_promotions(Thread* thread) {\n+    return data(thread)->_plab_allows_promotion;\n+  }\n+\n+  static void reset_plab_evacuated(Thread* thread) {\n+    data(thread)->_plab_evacuated = 0;\n+  }\n+\n+  static void add_to_plab_evacuated(Thread* thread, size_t increment) {\n+    data(thread)->_plab_evacuated += increment;\n+  }\n+\n+  static void subtract_from_plab_evacuated(Thread* thread, size_t increment) {\n+    data(thread)->_plab_evacuated -= increment;\n+  }\n+\n+  static size_t get_plab_evacuated(Thread* thread) {\n+    return data(thread)->_plab_evacuated;\n+  }\n+\n+  static void reset_plab_promoted(Thread* thread) {\n+    data(thread)->_plab_promoted = 0;\n+  }\n+\n+  static void add_to_plab_promoted(Thread* thread, size_t increment) {\n+    data(thread)->_plab_promoted += increment;\n+  }\n+\n+  static void subtract_from_plab_promoted(Thread* thread, size_t increment) {\n+    data(thread)->_plab_promoted -= increment;\n+  }\n+\n+  static size_t get_plab_promoted(Thread* thread) {\n+    return data(thread)->_plab_promoted;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":82,"deletions":0,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -41,1 +43,1 @@\n-ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause) :\n+ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation) :\n@@ -43,0 +45,1 @@\n+  _generation(generation),\n@@ -48,0 +51,1 @@\n+  _heap->set_gc_generation(generation);\n@@ -53,1 +57,1 @@\n-  _heap->heuristics()->record_cycle_start();\n+  generation->heuristics()->record_cycle_start();\n@@ -66,0 +70,1 @@\n+\n@@ -67,1 +72,8 @@\n-  _heap->heuristics()->record_cycle_end();\n+\n+  _generation->heuristics()->record_cycle_end();\n+  if (_heap->mode()->is_generational() &&\n+      ((_generation->generation_mode() == GLOBAL) || _heap->upgraded_to_full())) {\n+    \/\/ If we just completed a GLOBAL GC, claim credit for completion of young-gen and old-gen GC as well\n+    _heap->young_generation()->heuristics()->record_cycle_end();\n+    _heap->old_generation()->heuristics()->record_cycle_end();\n+  }\n@@ -70,1 +82,1 @@\n-  _tracer->report_gc_reference_stats(_heap->ref_processor()->reference_process_stats());\n+  _tracer->report_gc_reference_stats(_generation->ref_processor()->reference_process_stats());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":16,"deletions":4,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+class ShenandoahGeneration;\n@@ -48,0 +49,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -53,1 +55,1 @@\n-  ShenandoahGCSession(GCCause::Cause cause);\n+  ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -32,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -36,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -70,0 +73,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -81,1 +85,2 @@\n-    _loc(NULL) {\n+    _loc(NULL),\n+    _generation(NULL) {\n@@ -86,0 +91,5 @@\n+\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->active_generation();\n+      assert(_generation != NULL, \"Expected active generation in this mode\");\n+    }\n@@ -109,1 +119,1 @@\n-      if (_map->par_mark(obj)) {\n+      if ( in_generation(obj) && _map->par_mark(obj)) {\n@@ -116,0 +126,9 @@\n+  bool in_generation(oop obj) {\n+    if (_generation == NULL) {\n+      return true;\n+    }\n+\n+    ShenandoahHeapRegion* region = _heap->heap_region_containing(obj);\n+    return _generation->contains(region);\n+  }\n+\n@@ -126,1 +145,1 @@\n-    ShenandoahHeapRegion *obj_reg = _heap->heap_region_containing(obj);\n+    ShenandoahHeapRegion* obj_reg = _heap->heap_region_containing(obj);\n@@ -137,1 +156,1 @@\n-      HeapWord *obj_addr = cast_from_oop<HeapWord*>(obj);\n+      HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n@@ -166,1 +185,2 @@\n-          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live(),\n+          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live() ||\n+                (obj_reg->is_old() && ShenandoahHeap::heap()->is_gc_generation_young()),\n@@ -201,1 +221,1 @@\n-      HeapWord *fwd_addr = cast_from_oop<HeapWord *>(fwd);\n+      HeapWord* fwd_addr = cast_from_oop<HeapWord* >(fwd);\n@@ -215,1 +235,8 @@\n-\n+    \/\/ We allow for marked or old here for two reasons:\n+    \/\/  1. If this is a young collect, old objects wouldn't be marked. We've\n+    \/\/     recently change the verifier traversal to only follow young objects\n+    \/\/     during a young collect so this _shouldn't_ be necessary.\n+    \/\/  2. At present, we do not clear dead objects from the remembered set.\n+    \/\/     Everything in the remembered set is old (ipso facto), so allowing for\n+    \/\/     'marked_or_old' covers the case of stale objects in rset.\n+    \/\/ TODO: Just use 'is_marked' here.\n@@ -221,1 +248,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->marking_context()->is_marked_or_old(obj),\n@@ -225,1 +252,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked_or_old(obj),\n@@ -229,1 +256,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked_or_old(obj),\n@@ -323,0 +350,2 @@\n+    log_debug(gc)(\"ShenandoahCalculatRegionStatsClosure added \" SIZE_FORMAT \" for %s Region \" SIZE_FORMAT \", yielding: \" SIZE_FORMAT,\n+                  r->used(), r->is_humongous()? \"humongous\": \"regular\", r->index(), _used);\n@@ -332,0 +361,48 @@\n+class ShenandoahGenerationStatsClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  ShenandoahCalculateRegionStatsClosure old;\n+  ShenandoahCalculateRegionStatsClosure young;\n+  ShenandoahCalculateRegionStatsClosure global;\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) override {\n+    switch (r->affiliation()) {\n+      default:\n+        ShouldNotReachHere();\n+        return;\n+      case FREE: return;\n+      case YOUNG_GENERATION:\n+        young.heap_region_do(r);\n+        break;\n+      case OLD_GENERATION:\n+        old.heap_region_do(r);\n+        break;\n+    }\n+    global.heap_region_do(r);\n+  }\n+\n+  static void log_usage(ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    log_debug(gc)(\"Safepoint verification: %s verified usage: \" SIZE_FORMAT \"%s, recorded usage: \" SIZE_FORMAT \"%s\",\n+                  generation->name(),\n+                  byte_size_in_proper_unit(generation->used()), proper_unit_for_byte_size(generation->used()),\n+                  byte_size_in_proper_unit(stats.used()), proper_unit_for_byte_size(stats.used()));\n+  }\n+\n+  static void validate_usage(const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    size_t generation_used;\n+    if (generation->generation_mode() == YOUNG) {\n+      \/\/ young_evac_expended is \"usually zero\".  If it is non-zero, this means we are doing evacuation or updating references\n+      \/\/ and young-gen memory that holds the results of evacuation is being temporarily hidden from the usage accounting,\n+      \/\/ so we add it back in here to make verification happy.\n+      generation_used = generation->used() + ShenandoahHeap::heap()->get_young_evac_expended();\n+    } else {\n+      generation_used = generation->used();\n+    }\n+\n+    guarantee(stats.used() == generation_used,\n+              \"%s: generation (%s) used size must be consistent: generation-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+              label, generation->name(),\n+              byte_size_in_proper_unit(generation_used), proper_unit_for_byte_size(generation_used),\n+              byte_size_in_proper_unit(stats.used()), proper_unit_for_byte_size(stats.used()));\n+  }\n+};\n+\n@@ -413,2 +490,5 @@\n-    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() == r->used(),\n-           \"Accurate accounting: shared + TLAB + GCLAB = used\");\n+    verify(r, r->get_plab_allocs() <= r->capacity(),\n+           \"PLAB alloc count should not be larger than capacity\");\n+\n+    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() + r->get_plab_allocs() == r->used(),\n+           \"Accurate accounting: shared + TLAB + GCLAB + PLAB = used\");\n@@ -491,1 +571,1 @@\n-  ShenandoahHeap *_heap;\n+  ShenandoahHeap* _heap;\n@@ -496,0 +576,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -509,1 +590,7 @@\n-          _processed(0) {};\n+          _processed(0),\n+          _generation(NULL) {\n+    if (_heap->mode()->is_generational()) {\n+      _generation = _heap->active_generation();\n+      assert(_generation != NULL, \"Expected active generation in this mode.\");\n+    }\n+  };\n@@ -525,0 +612,4 @@\n+        if (!in_generation(r)) {\n+          continue;\n+        }\n+\n@@ -536,1 +627,5 @@\n-  virtual void work_humongous(ShenandoahHeapRegion *r, ShenandoahVerifierStack& stack, ShenandoahVerifyOopClosure& cl) {\n+  bool in_generation(ShenandoahHeapRegion* r) {\n+    return _generation == NULL || _generation->contains(r);\n+  }\n+\n+  virtual void work_humongous(ShenandoahHeapRegion* r, ShenandoahVerifierStack& stack, ShenandoahVerifyOopClosure& cl) {\n@@ -545,1 +640,1 @@\n-  virtual void work_regular(ShenandoahHeapRegion *r, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl) {\n+  virtual void work_regular(ShenandoahHeapRegion* r, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl) {\n@@ -578,1 +673,1 @@\n-  void verify_and_follow(HeapWord *addr, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl, size_t *processed) {\n+  void verify_and_follow(HeapWord* addr, ShenandoahVerifierStack &stack, ShenandoahVerifyOopClosure &cl, size_t* processed) {\n@@ -608,1 +703,1 @@\n-    if (actual != _expected) {\n+    if (actual != _expected && !(actual & ShenandoahHeap::OLD_MARKING)) {\n@@ -614,1 +709,2 @@\n-void ShenandoahVerifier::verify_at_safepoint(const char *label,\n+void ShenandoahVerifier::verify_at_safepoint(const char* label,\n+                                             VerifyRememberedSet remembered,\n@@ -647,0 +743,4 @@\n+      case _verify_gcstate_updating:\n+        enabled = true;\n+        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::UPDATEREFS;\n+        break;\n@@ -666,1 +766,2 @@\n-      if (actual != expected) {\n+      \/\/ Old generation marking is allowed in all states.\n+      if (actual != expected && !(actual & ShenandoahHeap::OLD_MARKING)) {\n@@ -684,0 +785,1 @@\n+\n@@ -685,0 +787,1 @@\n+\n@@ -699,0 +802,45 @@\n+  log_debug(gc)(\"Safepoint verification finished heap usage verification\");\n+\n+  ShenandoahGeneration* generation;\n+  if (_heap->mode()->is_generational()) {\n+    generation = _heap->active_generation();\n+    guarantee(generation != NULL, \"Need to know which generation to verify.\");\n+  } else {\n+    generation = NULL;\n+  }\n+\n+  if (generation != NULL) {\n+    ShenandoahHeapLocker lock(_heap->lock());\n+\n+    if (remembered == _verify_remembered_for_marking) {\n+      log_debug(gc)(\"Safepoint verification of remembered set at mark\");\n+    } else if (remembered == _verify_remembered_for_updating_references) {\n+      log_debug(gc)(\"Safepoint verification of remembered set at update ref\");\n+    } else if (remembered == _verify_remembered_after_full_gc) {\n+      log_debug(gc)(\"Safepoint verification of remembered set after full gc\");\n+    }\n+\n+    if (remembered == _verify_remembered_for_marking) {\n+      _heap->verify_rem_set_at_mark();\n+    } else if (remembered == _verify_remembered_for_updating_references) {\n+      _heap->verify_rem_set_at_update_ref();\n+    } else if (remembered == _verify_remembered_after_full_gc) {\n+      _heap->verify_rem_set_after_full_gc();\n+    }\n+\n+    ShenandoahGenerationStatsClosure cl;\n+    _heap->heap_region_iterate(&cl);\n+\n+    if (LogTarget(Debug, gc)::is_enabled()) {\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::log_usage(_heap->global_generation(), cl.global);\n+    }\n+\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->old_generation(), cl.old);\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->young_generation(), cl.young);\n+    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->global_generation(), cl.global);\n+  }\n+\n+  log_debug(gc)(\"Safepoint verification finished remembered set verification\");\n+\n@@ -702,1 +850,5 @@\n-    _heap->heap_region_iterate(&cl);\n+    if (generation != NULL) {\n+      generation->heap_region_iterate(&cl);\n+    } else {\n+      _heap->heap_region_iterate(&cl);\n+    }\n@@ -705,0 +857,2 @@\n+  log_debug(gc)(\"Safepoint verification finished heap region closure verification\");\n+\n@@ -729,0 +883,2 @@\n+  log_debug(gc)(\"Safepoint verification finished getting initial reachable set\");\n+\n@@ -746,0 +902,2 @@\n+  log_debug(gc)(\"Safepoint verification finished walking marked objects\");\n+\n@@ -752,0 +910,3 @@\n+      if (generation != NULL && !generation->contains(r)) {\n+        continue;\n+      }\n@@ -776,0 +937,3 @@\n+  log_debug(gc)(\"Safepoint verification finished accumulation of liveness data\");\n+\n+\n@@ -785,0 +949,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -797,0 +962,1 @@\n+          _verify_remembered_for_marking,  \/\/ verify read-only remembered set from bottom() to top()\n@@ -809,0 +975,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -821,0 +988,1 @@\n+          _verify_remembered_disable,                \/\/ do not verify remembered set\n@@ -833,0 +1001,1 @@\n+          _verify_remembered_disable, \/\/ do not verify remembered set\n@@ -845,0 +1014,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -857,6 +1027,7 @@\n-          _verify_forwarded_allow,     \/\/ forwarded references allowed\n-          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n-          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n-          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n-          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n-          _verify_gcstate_forwarded    \/\/ evacuation should have produced some forwarded objects\n+          _verify_remembered_for_updating_references,  \/\/ verify read-write remembered set\n+          _verify_forwarded_allow,                     \/\/ forwarded references allowed\n+          _verify_marked_complete,                     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n+          _verify_cset_forwarded,                      \/\/ all cset refs are fully forwarded\n+          _verify_liveness_disable,                    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash,                     \/\/ trash regions have been recycled already\n+          _verify_gcstate_updating                     \/\/ evacuation should have produced some forwarded objects\n@@ -869,0 +1040,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -881,0 +1053,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -893,0 +1066,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -902,0 +1076,13 @@\n+void ShenandoahVerifier::verify_after_generational_fullgc() {\n+  verify_at_safepoint(\n+          \"After Full Generational GC\",\n+          _verify_remembered_after_full_gc,  \/\/ verify read-write remembered set\n+          _verify_forwarded_none,      \/\/ all objects are non-forwarded\n+          _verify_marked_complete,     \/\/ all objects are marked in complete bitmap\n+          _verify_cset_none,           \/\/ no cset references\n+          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash_nocset, \/\/ no trash, no cset\n+          _verify_gcstate_stable       \/\/ full gc cleaned up everything\n+  );\n+}\n+\n@@ -905,0 +1092,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -914,1 +1102,1 @@\n-class ShenandoahVerifyNoForwared : public OopClosure {\n+class ShenandoahVerifyNoForwared : public BasicOopIterateClosure {\n@@ -934,1 +1122,1 @@\n-class ShenandoahVerifyInToSpaceClosure : public OopClosure {\n+class ShenandoahVerifyInToSpaceClosure : public BasicOopIterateClosure {\n@@ -943,1 +1131,1 @@\n-      if (!heap->marking_context()->is_marked(obj)) {\n+      if (!heap->marking_context()->is_marked_or_old(obj)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":218,"deletions":30,"binary":false,"changes":248,"status":"modified"},{"patch":"@@ -96,14 +96,0 @@\n-class FileLocker : public StackObj {\n-private:\n-  FILE *_file;\n-\n-public:\n-  FileLocker(FILE *file) : _file(file) {\n-    os::flockfile(_file);\n-  }\n-\n-  ~FileLocker() {\n-    os::funlockfile(_file);\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/logging\/logFileStreamOutput.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -105,0 +105,14 @@\n+class FileLocker : public StackObj {\n+private:\n+    FILE *_file;\n+\n+public:\n+    FileLocker(FILE *file) : _file(file) {\n+      os::flockfile(_file);\n+    }\n+\n+    ~FileLocker() {\n+      os::funlockfile(_file);\n+    }\n+};\n+\n","filename":"src\/hotspot\/share\/logging\/logFileStreamOutput.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -300,0 +300,9 @@\n+tier1_gc_shenandoah_generational = \\\n+  gc\/shenandoah\/generational\/\n+\n+# No tier 2 tests for shenandoah_generational at this time\n+tier2_gc_shenandoah_generational =\n+\n+# No tier 3 tests for shenandoah_generational at this time\n+tier3_gc_shenandoah_generational =\n+\n@@ -331,0 +340,1 @@\n+# include shenandoah generational tests in tier3 shenandoah\n@@ -337,0 +347,1 @@\n+  :hotspot_gc_shenandoah_generational \\\n@@ -344,0 +355,5 @@\n+hotspot_gc_shenandoah_generational = \\\n+  :tier1_gc_shenandoah_generational \\\n+  :tier2_gc_shenandoah_generational \\\n+  :tier3_gc_shenandoah_generational\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"}]}