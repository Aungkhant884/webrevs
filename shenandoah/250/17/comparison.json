{"files":[{"patch":"@@ -1202,1 +1202,0 @@\n-  heap->rebuild_free_set(true \/*concurrent*\/);\n@@ -1204,0 +1203,1 @@\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -41,5 +41,5 @@\n-ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n-  _heap(heap),\n-  _mutator_free_bitmap(max_regions, mtGC),\n-  _collector_free_bitmap(max_regions, mtGC),\n-  _max(max_regions)\n+\n+ShenandoahSetsOfFree::ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _free_set(free_set),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes())\n@@ -47,0 +47,1 @@\n+  _membership = NEW_C_HEAP_ARRAY(MemoryReserve, max_regions, mtGC);\n@@ -50,3 +51,9 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n-  shenandoah_assert_heaplocked();\n-  _used += num_bytes;\n+ShenandoahSetsOfFree::~ShenandoahSetsOfFree() {\n+  FREE_C_HEAP_ARRAY(MemoryReserve, _membership);\n+}\n+\n+\n+void ShenandoahSetsOfFree::clear_internal() {\n+  for (size_t idx = 0; idx < _max; idx++) {\n+    _membership[idx] = NotFree;\n+  }\n@@ -54,2 +61,17 @@\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+  for (size_t idx = 0; idx < NumFreeSets; idx++) {\n+    _left_mosts[idx] = _max;\n+    _right_mosts[idx] = 0;\n+    _left_mosts_empty[idx] = _max;\n+    _right_mosts_empty[idx] = 0;\n+    _capacity_of[idx] = 0;\n+    _used_by[idx] = 0;\n+  }\n+\n+  _left_to_right_bias[Mutator] = true;\n+  _left_to_right_bias[Collector] = false;\n+  _left_to_right_bias[OldCollector] = false;\n+\n+  _region_counts[Mutator] = 0;\n+  _region_counts[Collector] = 0;\n+  _region_counts[OldCollector] = 0;\n+  _region_counts[NotFree] = _max;\n@@ -58,4 +80,2 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  return _mutator_free_bitmap.at(idx);\n+void ShenandoahSetsOfFree::clear_all() {\n+  clear_internal();\n@@ -64,4 +84,6 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  return _collector_free_bitmap.at(idx);\n+void ShenandoahSetsOfFree::increase_used(MemoryReserve which_set, size_t bytes) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"Set must correspond to a valid freeset\");\n+  _used_by[which_set] += bytes;\n+  assert (_used_by[which_set] <= _capacity_of[which_set],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used_by[which_set], _capacity_of[which_set], bytes);\n@@ -70,7 +92,20 @@\n-\/\/ This is a temporary solution to work around a shortcoming with the existing free set implementation.\n-\/\/ TODO:\n-\/\/   Remove this function after restructing FreeSet representation.  A problem in the existing implementation is that old-gen\n-\/\/   regions are not considered to reside within the is_collector_free range.\n-\/\/\n-HeapWord* ShenandoahFreeSet::allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahAffiliation affiliation = ShenandoahAffiliation::OLD_GENERATION;\n+inline void ShenandoahSetsOfFree::shrink_bounds_if_touched(MemoryReserve set, size_t idx) {\n+  if (idx == _left_mosts[set]) {\n+    while ((_left_mosts[set] < _max) && !in_free_set(_left_mosts[set], set)) {\n+      _left_mosts[set]++;\n+    }\n+    if (_left_mosts_empty[set] < _left_mosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when left_mosts_empty is requested.\n+      _left_mosts_empty[set] = _left_mosts[set];\n+    }\n+  }\n+  if (idx == _right_mosts[set]) {\n+    while (_right_mosts[set] > 0 && !in_free_set(_right_mosts[set], set)) {\n+      _right_mosts[set]--;\n+    }\n+    if (_right_mosts_empty[set] > _right_mosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when right_mosts_empty is requested.\n+      _right_mosts_empty[set] = _right_mosts[set];\n+    }\n+  }\n+}\n@@ -78,2 +113,16 @@\n-  size_t rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n-  size_t leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+inline void ShenandoahSetsOfFree::expand_bounds_maybe(MemoryReserve set, size_t idx, size_t region_capacity) {\n+  if (region_capacity == _region_size_bytes) {\n+    if (_left_mosts_empty[set] > idx) {\n+      _left_mosts_empty[set] = idx;\n+    }\n+    if (_right_mosts_empty[set] < idx) {\n+      _right_mosts_empty[set] = idx;\n+    }\n+  }\n+  if (_left_mosts[set] > idx) {\n+    _left_mosts[set] = idx;\n+  }\n+  if (_right_mosts[set] < idx) {\n+    _right_mosts[set] = idx;\n+  }\n+}\n@@ -81,9 +130,315 @@\n-  for (size_t c = rightmost + 1; c > leftmost; c--) {\n-    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-    size_t idx = c - 1;\n-    ShenandoahHeapRegion* r = _heap->get_region(idx);\n-    if (r->affiliation() == affiliation && !r->is_humongous()) {\n-      if (!r->is_cset() && !has_no_alloc_capacity(r)) {\n-        HeapWord* result = try_allocate_in(r, req, in_new_region);\n-        if (result != nullptr) {\n-          return result;\n+void ShenandoahSetsOfFree::remove_from_free_sets(size_t idx) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  MemoryReserve orig_set = membership(idx);\n+  assert (orig_set > NotFree && orig_set < NumFreeSets, \"Cannot remove from free sets if not already free\");\n+  _membership[idx] = NotFree;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[NotFree]++;\n+}\n+\n+\n+void ShenandoahSetsOfFree::make_free(size_t idx, MemoryReserve which_set, size_t region_capacity) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (_membership[idx] == NotFree, \"Cannot make free if already free\");\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  _membership[idx] = which_set;\n+  _capacity_of[which_set] += region_capacity;\n+  expand_bounds_maybe(which_set, idx, region_capacity);\n+\n+  _region_counts[NotFree]--;\n+  _region_counts[which_set]++;\n+}\n+\n+void ShenandoahSetsOfFree::move_to_set(size_t idx, MemoryReserve new_set, size_t region_capacity) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert ((new_set > NotFree) && (new_set < NumFreeSets), \"New set must be valid\");\n+  MemoryReserve orig_set = _membership[idx];\n+  assert ((orig_set > NotFree) && (orig_set < NumFreeSets), \"Cannot move free unless already free\");\n+  \/\/ Expected transitions:\n+  \/\/  During rebuild: Mutator => Collector\n+  \/\/                  Mutator empty => Collector\n+  \/\/  During flip_to_gc:\n+  \/\/                  Mutator empty => Collector\n+  \/\/                  Mutator empty => Old Collector\n+  assert (((region_capacity < _region_size_bytes) && (orig_set == Mutator) && (new_set == Collector)) ||\n+\t  ((region_capacity == _region_size_bytes) && (orig_set == Mutator) && (new_set == Collector || new_set == OldCollector)),\n+\t  \"Unexpected movement between sets\");\n+\n+  _membership[idx] = new_set;\n+  _capacity_of[orig_set] -= region_capacity;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _capacity_of[new_set] += region_capacity;\n+  expand_bounds_maybe(new_set, idx, region_capacity);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[new_set]++;\n+}\n+\n+inline MemoryReserve ShenandoahSetsOfFree::membership(size_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  return _membership[idx];\n+}\n+\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+inline bool ShenandoahSetsOfFree::in_free_set(size_t idx, MemoryReserve test_set) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  if (_membership[idx] == test_set) {\n+    assert (test_set == NotFree || _free_set->alloc_capacity(idx) > 0, \"Free regions must have alloc capacity\");\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+inline size_t ShenandoahSetsOfFree::left_most(MemoryReserve which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _left_mosts[which_set];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    assert (in_free_set(idx, which_set), \"left-most region must be free\");\n+    return idx;\n+  }\n+}\n+\n+inline size_t ShenandoahSetsOfFree::right_most(MemoryReserve which_set) const {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _right_mosts[which_set];\n+  assert ((_left_mosts[which_set] == _max) || in_free_set(idx, which_set), \"right-most region must be free\");\n+  return idx;\n+}\n+\n+size_t ShenandoahSetsOfFree::left_most_empty(MemoryReserve which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (size_t idx = _left_mosts_empty[which_set]; idx < _max; idx++) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _left_mosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _left_mosts_empty[which_set] = _max;\n+  _right_mosts_empty[which_set] = 0;\n+  return _max;\n+}\n+\n+inline size_t ShenandoahSetsOfFree::right_most_empty(MemoryReserve which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (intptr_t idx = _right_mosts_empty[which_set]; idx >= 0; idx--) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _right_mosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _left_mosts_empty[which_set] = _max;\n+  _right_mosts_empty[which_set] = 0;\n+  return 0;\n+}\n+\n+inline bool ShenandoahSetsOfFree::alloc_from_left_bias(MemoryReserve which_set) {\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  return _left_to_right_bias[which_set];\n+}\n+\n+void ShenandoahSetsOfFree::establish_alloc_bias(MemoryReserve which_set) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+\n+  size_t middle = (_left_mosts[which_set] + _right_mosts[which_set]) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (size_t index = _left_mosts[which_set]; index < middle; index++) {\n+    if (in_free_set(index, which_set)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (size_t index = middle; index <= _right_mosts[which_set]; index++) {\n+    if (in_free_set(index, which_set)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+  \/\/ In the future, we may modify Full GC so that it slides old objects to the end of the heap and young objects to the start\n+  \/\/ of the heap. If this is done, we can always search survivor Collector and OldCollector regions right to left.\n+  _left_to_right_bias[which_set] = (available_in_second_half > available_in_first_half);\n+}\n+\n+#ifdef ASSERT\n+void ShenandoahSetsOfFree::assert_bounds() {\n+\n+  size_t left_mosts[NumFreeSets];\n+  size_t right_mosts[NumFreeSets];\n+  size_t empty_left_mosts[NumFreeSets];\n+  size_t empty_right_mosts[NumFreeSets];\n+\n+  for (int i = 0; i < NumFreeSets; i++) {\n+    left_mosts[i] = _max;\n+    empty_left_mosts[i] = _max;\n+    right_mosts[i] = 0;\n+    empty_right_mosts[i] = 0;\n+  }\n+\n+  for (size_t i = 0; i < _max; i++) {\n+    MemoryReserve set = membership(i);\n+    switch (set) {\n+      case NotFree:\n+        break;\n+\n+      case Mutator:\n+      case Collector:\n+      case OldCollector:\n+      {\n+        size_t capacity = _free_set->alloc_capacity(i);\n+        bool is_empty = (capacity == _region_size_bytes);\n+        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        if (i < left_mosts[set]) {\n+          left_mosts[set] = i;\n+        }\n+        if (is_empty && (i < empty_left_mosts[set])) {\n+          empty_left_mosts[set] = i;\n+        }\n+        if (i > right_mosts[set]) {\n+          right_mosts[set] = i;\n+        }\n+        if (is_empty && (i > empty_right_mosts[set])) {\n+          empty_right_mosts[set] = i;\n+        }\n+        break;\n+      }\n+\n+      case NumFreeSets:\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  }\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (left_most(Mutator) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, left_most(Mutator),  _max);\n+  assert (right_most(Mutator) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, right_most(Mutator),  _max);\n+\n+  assert (left_most(Mutator) == _max || in_free_set(left_most(Mutator), Mutator),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  left_most(Mutator));\n+  assert (left_most(Mutator) == _max || in_free_set(right_most(Mutator), Mutator),\n+          \"rightmost region should be free: \" SIZE_FORMAT, right_most(Mutator));\n+\n+  \/\/ If Mutator set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  size_t beg_off = left_mosts[Mutator];\n+  size_t end_off = right_mosts[Mutator];\n+  assert (beg_off >= left_most(Mutator),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(Mutator));\n+  assert (end_off <= right_most(Mutator),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(Mutator));\n+\n+  beg_off = empty_left_mosts[Mutator];\n+  end_off = empty_right_mosts[Mutator];\n+  assert (beg_off >= left_most_empty(Mutator),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most_empty(Mutator));\n+  assert (end_off <= right_most_empty(Mutator),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most_empty(Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (left_most(Collector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, left_most(Collector),  _max);\n+  assert (right_most(Collector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, right_most(Collector),  _max);\n+\n+  assert (left_most(Collector) == _max || in_free_set(left_most(Collector), Collector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  left_most(Collector));\n+  assert (left_most(Collector) == _max || in_free_set(right_most(Collector), Collector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, right_most(Collector));\n+\n+  \/\/ If Collector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = left_mosts[Collector];\n+  end_off = right_mosts[Collector];\n+  assert (beg_off >= left_most(Collector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(Collector));\n+  assert (end_off <= right_most(Collector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(Collector));\n+\n+  beg_off = empty_left_mosts[Collector];\n+  end_off = empty_right_mosts[Collector];\n+  assert (beg_off >= left_most_empty(Collector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most_empty(Collector));\n+  assert (end_off <= right_most_empty(Collector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most_empty(Collector));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (left_most(OldCollector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, left_most(OldCollector),  _max);\n+  assert (right_most(OldCollector) < _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, right_most(OldCollector),  _max);\n+\n+  assert (left_most(OldCollector) == _max || in_free_set(left_most(OldCollector), OldCollector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  left_most(OldCollector));\n+  assert (left_most(OldCollector) == _max || in_free_set(right_most(OldCollector), OldCollector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, right_most(OldCollector));\n+\n+  \/\/ If OldCollector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = left_mosts[OldCollector];\n+  end_off = right_mosts[OldCollector];\n+  assert (beg_off >= left_most(OldCollector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(OldCollector));\n+  assert (end_off <= right_most(OldCollector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(OldCollector));\n+\n+  beg_off = empty_left_mosts[OldCollector];\n+  end_off = empty_right_mosts[OldCollector];\n+  assert (beg_off >= left_most_empty(OldCollector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most_empty(OldCollector));\n+  assert (end_off <= right_most_empty(OldCollector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most_empty(OldCollector));\n+}\n+#endif\n+\n+\n+ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n+  _heap(heap),\n+  _free_sets(max_regions, this)\n+{\n+  clear_internal();\n+}\n+\n+\/\/ This allocates from a region within the old_collector_set.  If affiliation equals OLD, the allocation must be taken\n+\/\/ from a region that is_old().  Otherwise, affiliation should be FREE, in which case this will put a previously unaffiliated\n+\/\/ region into service.\n+HeapWord* ShenandoahFreeSet::allocate_old_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                           ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  size_t rightmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.right_most_empty(OldCollector): _free_sets.right_most(OldCollector);\n+  size_t leftmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.left_most_empty(OldCollector): _free_sets.left_most(OldCollector);\n+  if (_free_sets.alloc_from_left_bias(OldCollector)) {\n+    \/\/ This mode picks up stragglers left by a full GC\n+    for (size_t idx = leftmost; idx <= rightmost; idx++) {\n+      if (_free_sets.in_free_set(idx, OldCollector)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  } else {\n+    \/\/ This mode picks up stragglers left by a previous concurrent GC\n+    for (size_t count = rightmost + 1; count > leftmost; count--) {\n+      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      size_t idx = count - 1;\n+      if (_free_sets.in_free_set(idx, OldCollector)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n@@ -98,1 +453,6 @@\n-  for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+  shenandoah_assert_heaplocked();\n+  size_t rightmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.right_most_empty(Collector): _free_sets.right_most(Collector);\n+  size_t leftmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.left_most_empty(Collector): _free_sets.left_most(Collector);\n+  for (size_t c = rightmost + 1; c > leftmost; c--) {\n@@ -101,1 +461,1 @@\n-    if (is_collector_free(idx)) {\n+    if (_free_sets.in_free_set(idx, Collector)) {\n@@ -116,0 +476,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -160,1 +522,1 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n+      for (size_t idx = _free_sets.left_most(Mutator); idx <= _free_sets.right_most(Mutator); idx++) {\n@@ -162,1 +524,1 @@\n-        if (is_mutator_free(idx) && (allow_new_region || r->is_affiliated())) {\n+        if (_free_sets.in_free_set(idx, Mutator) && (allow_new_region || r->is_affiliated())) {\n@@ -184,1 +546,1 @@\n-        for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+        for (size_t c = _free_sets.right_most(Collector) + 1; c > _free_sets.left_most(Collector); c--) {\n@@ -186,1 +548,1 @@\n-          if (is_collector_free(idx)) {\n+          if (_free_sets.in_free_set(idx, Collector)) {\n@@ -197,3 +559,1 @@\n-          \/\/ TODO: this is a work around to address a deficiency in FreeSet representation.  A better solution fixes\n-          \/\/ the FreeSet implementation to deal more efficiently with old-gen regions as being in the \"collector free set\"\n-          result = allocate_with_old_affiliation(req, in_new_region);\n+          result = allocate_old_with_affiliation(req.affiliation(), req, in_new_region);\n@@ -208,1 +568,5 @@\n-          result = allocate_with_affiliation(FREE, req, in_new_region);\n+          if (req.is_old()) {\n+            result = allocate_old_with_affiliation(FREE, req, in_new_region);\n+          } else {\n+            result = allocate_with_affiliation(FREE, req, in_new_region);\n+          }\n@@ -220,0 +584,20 @@\n+      \/\/ TODO:\n+      \/\/ if (!allow_new_region && req.is_old() && (young_generation->adjusted_unaffiliated_regions() > 0)) {\n+      \/\/   transfer a region from young to old;\n+      \/\/   allow_new_region = true;\n+      \/\/   heap->set_old_evac_reserve(heap->get_old_evac_reserve() + region_size_bytes);\n+      \/\/ }\n+      \/\/\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+\n+      \/\/ Also TODO:\n+      \/\/ if (GC is idle (out of cycle) and mutator allocation fails and there is memory reserved in Collector\n+      \/\/ or OldCollector sets, transfer a region of memory so that we can satisfy the allocation request, and\n+      \/\/ immediately trigger the start of GC.  Is better to satisfy the allocation than to trigger out-of-cycle\n+      \/\/ allocation failure (even if this means we have a little less memory to handle evacuations during the\n+      \/\/ subsequent GC pass).\n+\n@@ -222,1 +606,1 @@\n-        for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n+        for (size_t c = _free_sets.right_most_empty(Mutator) + 1; c > _free_sets.left_most_empty(Mutator); c--) {\n@@ -224,1 +608,1 @@\n-          if (is_mutator_free(idx)) {\n+          if (_free_sets.in_free_set(idx, Mutator)) {\n@@ -227,1 +611,5 @@\n-              flip_to_gc(r);\n+              if (req.is_old()) {\n+                flip_to_old_gc(r);\n+              } else {\n+                flip_to_gc(r);\n+              }\n@@ -250,2 +638,1 @@\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n-\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n@@ -337,2 +724,1 @@\n-          increase_used(padding);\n-          assert(r->is_old(), \"All PLABs reside in old-gen\");\n+           assert(r->is_old(), \"All PLABs reside in old-gen\");\n@@ -389,1 +775,0 @@\n-        increase_used(padding);\n@@ -410,1 +795,1 @@\n-      increase_used(size * HeapWordSize);\n+      _free_sets.increase_used(Mutator, size * HeapWordSize);\n@@ -430,2 +815,2 @@\n-  if (result == nullptr || has_no_alloc_capacity(r)) {\n-    \/\/ Region cannot afford this or future allocations. Retire it.\n+  if (result == nullptr || alloc_capacity(r) < PLAB::min_size() * HeapWordSize) {\n+    \/\/ Region cannot afford this and is likely to not afford future allocations. Retire it.\n@@ -434,1 +819,1 @@\n-    \/\/ fit, but the next small one would, we are risking to inflate scan times when lots of\n+    \/\/ fit but the next small one would, we are risking to inflate scan times when lots of\n@@ -436,2 +821,0 @@\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations and\/or organize\n-    \/\/ available free segments within regions for more efficient searches for \"good fit\".\n@@ -440,0 +823,1 @@\n+    size_t idx = r->index();\n@@ -443,1 +827,1 @@\n-        increase_used(waste);\n+        _free_sets.increase_used(Mutator, waste);\n@@ -447,0 +831,4 @@\n+      assert(_free_sets.membership(idx) == Mutator, \"Must be mutator free: \" SIZE_FORMAT, idx);\n+    } else {\n+      assert(_free_sets.membership(idx) == Collector || _free_sets.membership(idx) == OldCollector,\n+             \"Must be collector or old-collector free: \" SIZE_FORMAT, idx);\n@@ -448,9 +836,3 @@\n-\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n-    }\n-    assert_bounds();\n+    \/\/ This region is no longer considered free (in any set)\n+    _free_sets.remove_from_free_sets(idx);\n+    _free_sets.assert_bounds();\n@@ -461,32 +843,0 @@\n-bool ShenandoahFreeSet::touches_bounds(size_t num) const {\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n-}\n-\n-void ShenandoahFreeSet::recompute_bounds() {\n-  \/\/ Reset to the most pessimistic case:\n-  _mutator_rightmost = _max - 1;\n-  _mutator_leftmost = 0;\n-  _collector_rightmost = _max - 1;\n-  _collector_leftmost = 0;\n-\n-  \/\/ ...and adjust from there\n-  adjust_bounds();\n-}\n-\n-void ShenandoahFreeSet::adjust_bounds() {\n-  \/\/ Rewind both mutator bounds until the next bit.\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n-    _mutator_leftmost++;\n-  }\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n-    _mutator_rightmost--;\n-  }\n-  \/\/ Rewind both collector bounds until the next bit.\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n-    _collector_leftmost++;\n-  }\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n-    _collector_rightmost--;\n-  }\n-}\n-\n@@ -505,1 +855,1 @@\n-    if (num > mutator_count() || (num > avail_young_regions)) {\n+    if (num > _free_sets.count(Mutator) || (num > avail_young_regions)) {\n@@ -509,1 +859,1 @@\n-    if (num > mutator_count()) {\n+    if (num > _free_sets.count(Mutator)) {\n@@ -517,1 +867,1 @@\n-  size_t beg = _mutator_leftmost;\n+  size_t beg = _free_sets.left_most(Mutator);\n@@ -521,1 +871,1 @@\n-    if (end >= _max) {\n+    if (end >= _free_sets.max()) {\n@@ -528,1 +878,1 @@\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n+    if (!_free_sets.in_free_set(end, Mutator) || !can_allocate_from(_heap->get_region(end))) {\n@@ -568,13 +918,1 @@\n-    r->set_update_watermark(r->bottom());\n-    r->set_top(r->bottom());    \/\/ Set top to bottom so we can capture TAMS\n-    ctx->capture_top_at_mark_start(r);\n-    r->set_top(r->bottom() + used_words); \/\/ Then change top to reflect allocation of humongous object.\n-    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n-    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n-\n-    \/\/ Leave top_bitmap alone.  The first time a heap region is put into service, top_bitmap should equal end.\n-    \/\/ Thereafter, it should represent the upper bound on parts of the bitmap that need to be cleared.\n-    \/\/ ctx->clear_bitmap(r);\n-    log_debug(gc, free)(\"NOT clearing bitmap for Humongous region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmap: \"\n-                        PTR_FORMAT \" at transition from FREE to %s\",\n-                        p2i(r->bottom()), p2i(r->end()), p2i(ctx->top_bitmap(r)), req.affiliation_name());\n+    r->set_top(r->bottom() + used_words);\n@@ -583,1 +921,1 @@\n-    _mutator_free_bitmap.clear_bit(r->index());\n+    _free_sets.remove_from_free_sets(r->index());\n@@ -586,1 +924,1 @@\n-  increase_used(total_humongous_size);\n+  _free_sets.increase_used(Mutator, total_humongous_size);\n@@ -599,1 +937,0 @@\n-\n@@ -606,7 +943,1 @@\n-\n-  \/\/ Allocated at left\/rightmost? Move the bounds appropriately.\n-  if (beg == _mutator_leftmost || end == _mutator_rightmost) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n-\n+  _free_sets.assert_bounds();\n@@ -617,1 +948,4 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n@@ -621,1 +955,6 @@\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n+size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n+size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -630,1 +969,10 @@\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n+bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+bool ShenandoahFreeSet::has_alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r) > 0;\n+}\n+\n+bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -655,1 +1003,1 @@\n-void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n@@ -658,1 +1006,1 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n@@ -661,4 +1009,11 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n+  size_t region_capacity = alloc_capacity(r);\n+  _free_sets.move_to_set(idx, OldCollector, region_capacity);\n+  _free_sets.assert_bounds();\n+\n+  \/\/ We do not ensure that the region is no longer trash,\n+  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n+void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n@@ -666,1 +1021,2 @@\n-  _capacity -= alloc_capacity(r);\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n@@ -668,4 +1024,3 @@\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n+  size_t region_capacity = alloc_capacity(r);\n+  _free_sets.move_to_set(idx, Collector, region_capacity);\n+  _free_sets.assert_bounds();\n@@ -684,8 +1039,1 @@\n-  _mutator_free_bitmap.clear();\n-  _collector_free_bitmap.clear();\n-  _mutator_leftmost = _max;\n-  _mutator_rightmost = 0;\n-  _collector_leftmost = _max;\n-  _collector_rightmost = 0;\n-  _capacity = 0;\n-  _used = 0;\n+  _free_sets.clear_internal();\n@@ -694,3 +1042,6 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n+\/\/ This function places all is_old() regions that have allocation capacity into the old_collector set.  It places\n+\/\/ all other regions (not is_old()) that have allocation capacity into the mutator_set.  Subsequently, we will\n+\/\/ move some of the mutator regions into the collector set or old_collector set with the intent of packing\n+\/\/ old_collector memory into the highest (rightmost) addresses of the heap and the collector memory into the\n+\/\/ next highest addresses of the heap, with mutator memory consuming the lowest addresses of the heap.\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity() {\n@@ -698,1 +1049,0 @@\n-  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -702,4 +1052,2 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n-\n-      \/\/ Do not add regions that would surely fail allocation\n-      if (has_no_alloc_capacity(region)) continue;\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n+      assert(_free_sets.in_free_set(idx, NotFree), \"We are about to make region free; it should not be free already\");\n@@ -707,2 +1055,2 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      if (alloc_capacity(region) < PLAB::min_size() * HeapWordSize) continue;\n@@ -710,4 +1058,10 @@\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n-\n-      log_debug(gc, free)(\"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n+      if (region->is_old()) {\n+        _free_sets.make_free(idx, OldCollector, alloc_capacity(region));\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT  \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to old collector set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      } else {\n+        _free_sets.make_free(idx, Mutator, alloc_capacity(region));\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator set\",\n@@ -715,1 +1069,2 @@\n-               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      }\n@@ -718,0 +1073,8 @@\n+}\n+\n+void ShenandoahFreeSet::rebuild() {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -719,1 +1082,7 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.\n+  find_regions_with_alloc_capacity();\n+\n+  \/\/ Evac reserve: reserve trailing space for evacuations, with regions reserved for old evacuations placed to the right\n+  \/\/ of regions reserved of young evacuations.\n+  size_t young_reserve, old_reserve;\n@@ -721,2 +1090,2 @@\n-    size_t to_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    reserve_regions(to_reserve);\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -724,2 +1093,1 @@\n-    size_t young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    \/\/ Note that all allocations performed from old-gen are performed by GC, generally using PLABs for both\n+    \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n@@ -727,6 +1095,12 @@\n-    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons within\n-    \/\/ each PLAB.  We do not reserve any of old-gen memory in order to facilitate the loaning of old-gen memory\n-    \/\/ to young-gen purposes.\n-    size_t old_reserve = 0;\n-    size_t to_reserve = young_reserve + old_reserve;\n-    reserve_regions(to_reserve);\n+    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons for\n+    \/\/ each PLAB's available memory.\n+    if (_heap->has_evacuation_reserve_quantities()) {\n+      \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+      young_reserve = _heap->get_young_evac_reserve();\n+      old_reserve = _heap->get_promoted_reserve() + _heap->get_old_evac_reserve();\n+    } else {\n+      \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+      young_reserve = (_heap->young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      old_reserve = MAX2((_heap->old_generation()->max_capacity() * ShenandoahOldEvacReserve) \/ 100,\n+                         ShenandoahOldCompactionReserve * ShenandoahHeapRegion::region_size_bytes());\n+    }\n@@ -734,0 +1108,1 @@\n+  reserve_regions(young_reserve, old_reserve);\n@@ -735,2 +1110,3 @@\n-  recompute_bounds();\n-  assert_bounds();\n+  _free_sets.establish_alloc_bias(OldCollector);\n+  _free_sets.assert_bounds();\n+  log_status();\n@@ -739,3 +1115,6 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n-  size_t reserved = 0;\n-\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve, and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old) {\n@@ -743,12 +1122,21 @@\n-    if (reserved >= to_reserve) break;\n-\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n-      log_debug(gc, free)(\"  Shifting Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to collector free set\",\n-                          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-                               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (_free_sets.in_free_set(idx, Mutator)) {\n+      assert (!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+      size_t ac = alloc_capacity(r);\n+      assert (ac > 0, \"Membership in free set implies has capacity\");\n+\n+      \/\/ OLD regions that have available memory are already in the old_collector free set\n+      if ((_free_sets.capacity_of(OldCollector) < to_reserve_old) && (r->is_trash() || !r->is_affiliated())) {\n+        _free_sets.move_to_set(idx, OldCollector, alloc_capacity(r));\n+        log_debug(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+      } else if (_free_sets.capacity_of(Collector) < to_reserve) {\n+        \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+        \/\/ they were entirely empty.  I'm not sure I understand the rational for that.  That alternative behavior would\n+        \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+        \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n+        _free_sets.move_to_set(idx, Collector, ac);\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n+      } else {\n+        \/\/ We've satisfied both to_reserve and to_reserved_old\n+        break;\n+      }\n@@ -762,0 +1150,74 @@\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  {\n+#define BUFFER_SIZE 80\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_info(gc, free)(\"FreeSet map legend:\"\n+                       \" M:mutator_free C:collector_free O:old_collector_free\"\n+                       \" H:humongous ~:retired old _:retired young\");\n+    log_info(gc, free)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n+                       _free_sets.left_most(Mutator), _free_sets.right_most(Mutator),\n+                       _free_sets.left_most(Collector), _free_sets.right_most(Collector),\n+                       _free_sets.left_most(OldCollector), _free_sets.right_most(OldCollector),\n+                       _free_sets.alloc_from_left_bias(OldCollector)? \"left to right\": \"right to left\");\n+\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_info(gc, free)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (_free_sets.in_free_set(i, Mutator)) {\n+        assert(!r->is_old(), \"Old regions should not be in mutator_free set\");\n+        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'M': 'm';\n+      } else if (_free_sets.in_free_set(i, Collector)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'C': 'c';\n+      } else if (_free_sets.in_free_set(i, OldCollector)) {\n+        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'O': 'o';\n+      } else if (r->is_humongous()) {\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n+      } else {\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young += region_size_bytes;\n+        }\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_info(gc, free)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+    size_t total_young = retired_young + retired_young_humongous;\n+    size_t total_old = retired_old + retired_old_humongous;\n+    log_info(gc, free)(\"Retired young: \" SIZE_FORMAT \"%s (including humongous: \" SIZE_FORMAT \"%s), old: \" SIZE_FORMAT\n+                       \"%s (including humongous: \" SIZE_FORMAT \"%s)\",\n+                       byte_size_in_proper_unit(total_young),             proper_unit_for_byte_size(total_young),\n+                       byte_size_in_proper_unit(retired_young_humongous), proper_unit_for_byte_size(retired_young_humongous),\n+                       byte_size_in_proper_unit(total_old),               proper_unit_for_byte_size(total_old),\n+                       byte_size_in_proper_unit(retired_old_humongous),   proper_unit_for_byte_size(retired_old_humongous));\n+  }\n+#endif\n+\n@@ -777,2 +1239,2 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (is_mutator_free(idx)) {\n+      for (size_t idx = _free_sets.left_most(Mutator); idx <= _free_sets.right_most(Mutator); idx++) {\n+        if (_free_sets.in_free_set(idx, Mutator)) {\n@@ -781,1 +1243,0 @@\n-\n@@ -783,1 +1244,0 @@\n-\n@@ -794,1 +1254,0 @@\n-\n@@ -797,1 +1256,0 @@\n-\n@@ -806,0 +1264,4 @@\n+      assert(free == total_free, \"Sum of free within mutator regions (\" SIZE_FORMAT\n+             \") should match mutator capacity (\" SIZE_FORMAT \") minus mutator used (\" SIZE_FORMAT \")\",\n+             total_free, capacity(), used());\n+\n@@ -822,2 +1284,2 @@\n-      if (mutator_count() > 0) {\n-        frag_int = (100 * (total_used \/ mutator_count()) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_free_sets.count(Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _free_sets.count(Mutator)) \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -828,2 +1290,2 @@\n-      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT \" \",\n-               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), mutator_count());\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), _free_sets.count(Mutator));\n@@ -837,2 +1299,2 @@\n-      for (size_t idx = _collector_leftmost; idx <= _collector_rightmost; idx++) {\n-        if (is_collector_free(idx)) {\n+      for (size_t idx = _free_sets.left_most(Collector); idx <= _free_sets.right_most(Collector); idx++) {\n+        if (_free_sets.in_free_set(idx, Collector)) {\n@@ -846,0 +1308,10 @@\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n@@ -847,1 +1319,10 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\",\n+      for (size_t idx = _free_sets.left_most(OldCollector); idx <= _free_sets.right_most(OldCollector); idx++) {\n+        if (_free_sets.in_free_set(idx, OldCollector)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n@@ -857,1 +1338,1 @@\n-  assert_bounds();\n+  _free_sets.assert_bounds();\n@@ -885,2 +1366,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (index < _max && is_mutator_free(index)) {\n+  for (size_t index = _free_sets.left_most(Mutator); index <= _free_sets.right_most(Mutator); index++) {\n+    if (index < _free_sets.max() && _free_sets.in_free_set(index, Mutator)) {\n@@ -899,3 +1380,3 @@\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", mutator_count());\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Mutator));\n+  for (size_t index = _free_sets.left_most(Mutator); index <= _free_sets.right_most(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -905,3 +1386,3 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", collector_count());\n-  for (size_t index = _collector_leftmost; index <= _collector_rightmost; index++) {\n-    if (is_collector_free(index)) {\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Collector));\n+  for (size_t index = _free_sets.left_most(Collector); index <= _free_sets.right_most(Collector); index++) {\n+    if (_free_sets.in_free_set(index, Collector)) {\n@@ -911,0 +1392,8 @@\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(OldCollector));\n+    for (size_t index = _free_sets.left_most(OldCollector); index <= _free_sets.right_most(OldCollector); index++) {\n+      if (_free_sets.in_free_set(index, OldCollector)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n+    }\n+  }\n@@ -939,2 +1428,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  for (size_t index = _free_sets.left_most(Mutator); index <= _free_sets.right_most(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -977,2 +1466,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (is_mutator_free(index)) {\n+  for (size_t index = _free_sets.left_most(Mutator); index <= _free_sets.right_most(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -1003,27 +1492,0 @@\n-#ifdef ASSERT\n-void ShenandoahFreeSet::assert_bounds() const {\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance\n-  \/\/ would suffer.\n-  assert (_mutator_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_leftmost,  _max);\n-  assert (_mutator_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_rightmost, _max);\n-\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n-\n-  size_t beg_off = _mutator_free_bitmap.find_first_set_bit(0);\n-  size_t end_off = _mutator_free_bitmap.find_first_set_bit(_mutator_rightmost + 1);\n-  assert (beg_off >= _mutator_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _mutator_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _mutator_rightmost);\n-\n-  assert (_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_leftmost,  _max);\n-  assert (_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_rightmost, _max);\n-\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n-\n-  beg_off = _collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _collector_free_bitmap.find_first_set_bit(_collector_rightmost + 1);\n-  assert (beg_off >= _collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _collector_rightmost);\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":710,"deletions":248,"binary":false,"changes":958,"status":"modified"},{"patch":"@@ -32,1 +32,11 @@\n-class ShenandoahFreeSet : public CHeapObj<mtGC> {\n+enum MemoryReserve : uint8_t {\n+  NotFree,\n+  Mutator,\n+  Collector,\n+  OldCollector,\n+  NumFreeSets\n+};\n+\n+class ShenandoahSetsOfFree {\n+  friend class ShenandoahFreeSet;\n+\n@@ -34,3 +44,0 @@\n-  ShenandoahHeap* const _heap;\n-  CHeapBitMap _mutator_free_bitmap;\n-  CHeapBitMap _collector_free_bitmap;\n@@ -38,0 +45,37 @@\n+  ShenandoahFreeSet* _free_set;\n+  size_t _region_size_bytes;\n+  MemoryReserve* _membership;\n+  size_t _left_mosts[NumFreeSets];\n+  size_t _right_mosts[NumFreeSets];\n+  size_t _left_mosts_empty[NumFreeSets];\n+  size_t _right_mosts_empty[NumFreeSets];\n+  size_t _capacity_of[NumFreeSets];\n+  size_t _used_by[NumFreeSets];\n+  bool _left_to_right_bias[NumFreeSets];\n+  size_t _region_counts[NumFreeSets];\n+\n+  inline void shrink_bounds_if_touched(MemoryReserve set, size_t idx);\n+  inline void expand_bounds_maybe(MemoryReserve set, size_t idx, size_t capacity);\n+\n+  \/\/ Restore all state variables to initial default state.\n+  void clear_internal();\n+\n+public:\n+  ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set);\n+  ~ShenandoahSetsOfFree();\n+\n+  \/\/ Make all regions NotFree and reset all bounds\n+  void clear_all();\n+\n+  \/\/ Remove or retire region idx from all free sets.  Requires that idx is in a free set.  This does not affect capacity.\n+  void remove_from_free_sets(size_t idx);\n+\n+  \/\/ Place region idx into free set which_set.  Requires that idx is currently NotFree.\n+  void make_free(size_t idx, MemoryReserve which_set, size_t region_capacity);\n+\n+  \/\/ Place region idx into free set new_set.  Requires that idx is currently not NotFRee.\n+  void move_to_set(size_t idx, MemoryReserve new_set, size_t region_capacity);\n+\n+  \/\/ Returns the MemoryReserve affiliation of region idx, or NotFree if this region is not currently free.  This does\n+  \/\/ not enforce that free_set membership implies allocation capacity.\n+  inline MemoryReserve membership(size_t idx) const;\n@@ -39,4 +83,3 @@\n-  \/\/ Left-most and right-most region indexes. There are no free regions outside\n-  \/\/ of [left-most; right-most] index intervals\n-  size_t _mutator_leftmost, _mutator_rightmost;\n-  size_t _collector_leftmost, _collector_rightmost;\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+  inline bool in_free_set(size_t idx, MemoryReserve which_set) const;\n@@ -44,2 +87,5 @@\n-  size_t _capacity;\n-  size_t _used;\n+  \/\/ Each of the following four methods returns _max to indicate absence of requested region.\n+  inline size_t left_most(MemoryReserve which_set) const;\n+  inline size_t right_most(MemoryReserve which_set) const;\n+  size_t left_most_empty(MemoryReserve which_set);\n+  size_t right_most_empty(MemoryReserve which_set);\n@@ -47,1 +93,1 @@\n-  void assert_bounds() const NOT_DEBUG_RETURN;\n+  inline void increase_used(MemoryReserve which_set, size_t bytes);\n@@ -49,2 +95,48 @@\n-  bool is_mutator_free(size_t idx) const;\n-  bool is_collector_free(size_t idx) const;\n+  inline size_t capacity_of(MemoryReserve which_set) const {\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    return _capacity_of[which_set];\n+  }\n+\n+  inline size_t used_by(MemoryReserve which_set) const {\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    return _used_by[which_set];\n+  }\n+\n+  inline size_t max() const { return _max; }\n+\n+  inline size_t count(MemoryReserve which_set) const { return _region_counts[which_set]; }\n+\n+  \/\/ Return true iff regions for allocation from this set should be peformed left to right.  Otherwise, allocate\n+  \/\/ from right to left.\n+  inline bool alloc_from_left_bias(MemoryReserve which_set);\n+\n+  \/\/ Determine whether we prefer to allocate from left to right or from right to left for this free-set.\n+  void establish_alloc_bias(MemoryReserve which_set);\n+\n+  \/\/ Assure leftmost, rightmost, leftmost_empty, and rightmost_empty bounds are valid for all free sets.\n+  \/\/ valid bounds honor all of the following (where max is the number of heap regions):\n+  \/\/   if the set is empty, leftmost equals max and rightmost equals 0\n+  \/\/   Otherwise (the set is not empty):\n+  \/\/     0 <= leftmost < max and 0 <= rightmost < max\n+  \/\/     the region at leftmost is in the set\n+  \/\/     the region at rightmost is in the set\n+  \/\/     rightmost >= leftmost\n+  \/\/     for every idx that is in the set {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  \/\/   if the set has no empty regions, leftmost_empty equals max and right_most_empty equals 0\n+  \/\/   Otherwise (the region has empty regions):\n+  \/\/     0 <= lefmost_empty < max and 0 <= rightmost_empty < max\n+  \/\/     rightmost_empty >= leftmost_empty\n+  \/\/     for every idx that is in the set and is empty {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  void assert_bounds() NOT_DEBUG_RETURN;\n+};\n+\n+class ShenandoahFreeSet : public CHeapObj<mtGC> {\n+private:\n+  ShenandoahHeap* const _heap;\n+  ShenandoahSetsOfFree _free_sets;\n@@ -53,0 +145,3 @@\n+\n+  \/\/ Satisfy young-generation or single-generation collector allocation request req by finding memory that matches\n+  \/\/ affiliation, which either equals req.affiliation or FREE.  We know req.is_young().\n@@ -54,1 +149,4 @@\n-  HeapWord* allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Satisfy allocation request req by finding memory that matches affiliation, which either equals req.affiliation\n+  \/\/ or FREE. We know req.is_old().\n+  HeapWord* allocate_old_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -66,0 +164,1 @@\n+  void flip_to_old_gc(ShenandoahHeapRegion* r);\n@@ -67,6 +166,0 @@\n-  void recompute_bounds();\n-  void adjust_bounds();\n-  bool touches_bounds(size_t num) const;\n-\n-  \/\/ Used of free set represents the amount of is_mutator_free set that has been consumed since most recent rebuild.\n-  void increase_used(size_t amount);\n@@ -77,3 +170,4 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r);\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n-  bool has_no_alloc_capacity(ShenandoahHeapRegion *r);\n+  bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  bool has_alloc_capacity(size_t idx) const;\n+  bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n+  bool has_no_alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -84,5 +178,2 @@\n-  \/\/ Number of regions dedicated to GC allocations (for evacuation or promotion) that are currently free\n-  size_t collector_count() const { return _collector_free_bitmap.count_one_bits(); }\n-\n-  \/\/ Number of regions dedicated to mutator allocations that are currently free\n-  size_t mutator_count()   const { return _mutator_free_bitmap.count_one_bits();   }\n+  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  size_t alloc_capacity(size_t idx) const;\n@@ -97,5 +188,5 @@\n-  size_t capacity()  const { return _capacity; }\n-  size_t used()      const { return _used;     }\n-  size_t available() const {\n-    assert(_used <= _capacity, \"must use less than capacity\");\n-    return _capacity - _used;\n+  inline size_t capacity()  const { return _free_sets.capacity_of(Mutator); }\n+  inline size_t used()      const { return _free_sets.used_by(Mutator);     }\n+  inline size_t available() const {\n+    assert(used() <= capacity(), \"must use less than capacity\");\n+    return capacity() - used();\n@@ -112,1 +203,2 @@\n-  void reserve_regions(size_t to_reserve);\n+  void find_regions_with_alloc_capacity();\n+  void reserve_regions(size_t young_reserve, size_t old_reserve);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":126,"deletions":34,"binary":false,"changes":160,"status":"modified"},{"patch":"@@ -1051,0 +1051,5 @@\n+\/\/ TODO:\n+\/\/  Consider compacting old-gen objects toward the high end of memory and young-gen objects towards the low-end\n+\/\/  of memory.  As currently implemented, all regions are compacted toward the low-end of memory.  This creates more\n+\/\/  fragmentation of the heap, because old-gen regions get scattered among low-address regions such that it becomes\n+\/\/  more difficult to find contiguous regions for humongous objects.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -836,0 +836,2 @@\n+  \/\/ Freeset construction uses reserve quantities if they are valid\n+  heap->set_evacuation_reserve_quantities(true);\n@@ -842,0 +844,1 @@\n+  heap->set_evacuation_reserve_quantities(false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -559,0 +559,2 @@\n+  _upgraded_to_full(false),\n+  _has_evacuation_reserve_quantities(false),\n@@ -2248,0 +2250,4 @@\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  _has_evacuation_reserve_quantities = is_valid;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -380,0 +380,14 @@\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantitites, stored in _promoted_reserve,\n+  \/\/ _old_evac_reserve, and _young_evac_reserve, are consulted prior to rebuilding the free set (ShenandoahFreeSet)\n+  \/\/ in preparation for evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the\n+  \/\/ collector and old_collector sets to hold if _has_evacuation_reserve_quantities is true.  The other time we\n+  \/\/ rebuild the freeset is at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ _has_evacuation_reserve_quantities is false because we don't yet know how much memory will need to be evacuated\n+  \/\/ in the next GC cycle.  When _has_evacuation_reserve_quantities is false, the free set rebuild operation reserves\n+  \/\/ for the collector and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve,\n+  \/\/ ShenandoahOldEvacReserve, and ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve\n+  \/\/ for old_collector set when not _has_evacuation_reserve_quantities is based in part on anticipated promotion as\n+  \/\/ determined by analysis of live data found during the previous GC pass which is one less than the current tenure age.\n+  bool _has_evacuation_reserve_quantities;\n+\n@@ -383,2 +397,0 @@\n-\n-\n@@ -386,0 +398,1 @@\n+\n@@ -389,0 +402,1 @@\n+  void set_evacuation_reserve_quantities(bool is_valid);\n@@ -404,0 +418,1 @@\n+  inline bool has_evacuation_reserve_quantities() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":17,"deletions":2,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -664,0 +664,1 @@\n+\n@@ -668,0 +669,4 @@\n+inline bool ShenandoahHeap::has_evacuation_reserve_quantities() const {\n+  return _has_evacuation_reserve_quantities;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"}]}