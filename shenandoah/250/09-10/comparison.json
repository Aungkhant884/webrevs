{"files":[{"patch":"@@ -41,6 +41,5 @@\n-ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n-  _heap(heap),\n-  _mutator_free_bitmap(max_regions, mtGC),\n-  _collector_free_bitmap(max_regions, mtGC),\n-  _old_collector_free_bitmap(max_regions, mtGC),\n-  _max(max_regions)\n+\n+ShenandoahSetsOfFree::ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set) :\n+    _max(max_regions),\n+    _free_set(free_set),\n+    _region_size_bytes(ShenandoahHeapRegion::region_size_bytes())\n@@ -48,0 +47,1 @@\n+  _membership = NEW_C_HEAP_ARRAY(MemoryReserve, max_regions, mtGC);\n@@ -51,1 +51,30 @@\n-inline void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n+ShenandoahSetsOfFree::~ShenandoahSetsOfFree() {\n+  FREE_C_HEAP_ARRAY(MemoryReserve, _membership);\n+}\n+\n+\n+void ShenandoahSetsOfFree::clear_internal() {\n+  for (size_t idx = 0; idx < _max; idx++) {\n+    _membership[idx] = NotFree;\n+  }\n+\n+  for (size_t idx = 0; idx < NumFreeSets; idx++) {\n+    _left_mosts[idx] = _max;\n+    _right_mosts[idx] = 0;\n+    _left_mosts_empty[idx] = _max;\n+    _right_mosts_empty[idx] = 0;\n+    _capacity_of[idx] = 0;\n+    _used_by[idx] = 0;\n+  }\n+\n+  _left_to_right_bias[Mutator] = true;\n+  _left_to_right_bias[Collector] = false;\n+  _left_to_right_bias[OldCollector] = false;\n+\n+  _region_counts[Mutator] = 0;\n+  _region_counts[Collector] = 0;\n+  _region_counts[OldCollector] = 0;\n+  _region_counts[NotFree] = _max;\n+}\n+\n+void ShenandoahSetsOfFree::clear_all() {\n@@ -53,3 +82,1 @@\n-  _used += num_bytes;\n-  assert(_used <= _capacity, \"must not use (\" SIZE_FORMAT \") more than we have (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n-         _used, _capacity, num_bytes);\n+  clear_internal();\n@@ -58,10 +85,27 @@\n-template <MemoryReserve SET> inline bool ShenandoahFreeSet::probe_set(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  switch(SET) {\n-    case Mutator:\n-      return _mutator_free_bitmap.at(idx);\n-    case Collector:\n-      return _collector_free_bitmap.at(idx);\n-    case OldCollector:\n-      return _old_collector_free_bitmap.at(idx);\n+void ShenandoahSetsOfFree::increase_used(MemoryReserve which_set, size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"Set must correspond to a valid freeset\");\n+  _used_by[which_set] += bytes;\n+  assert (_used_by[which_set] <= _capacity_of[which_set],\n+          \"Must not use (\" SIZE_FORMAT \") more than capacity (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+          _used_by[which_set], _capacity_of[which_set], bytes);\n+}\n+                       \n+inline void ShenandoahSetsOfFree::shrink_bounds_if_touched(MemoryReserve set, size_t idx) {\n+  if (idx = _left_mosts[set]) {\n+    while ((_left_mosts[set] < _max) && !in_free_set(_left_mosts[set], set)) {\n+      _left_mosts[set]++;\n+    }\n+    if (_left_mosts_empty[set] < _left_mosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when left_mosts_empty is requested.\n+      _left_mosts_empty[set] = _left_mosts[set];\n+    }\n+  }\n+  if (idx == _right_mosts[set]) {\n+    while (_right_mosts[set] > 0 && !in_free_set(_right_mosts[set], set)) {\n+      _right_mosts[set]--;\n+    }\n+    if (_right_mosts_empty[set] > _right_mosts[set]) {\n+      \/\/ This gets us closer to where we need to be; we'll scan further when right_mosts_empty is requested.\n+      _right_mosts_empty[set] = _right_mosts[set];\n+    }\n@@ -71,14 +115,14 @@\n-template <MemoryReserve SET> inline bool ShenandoahFreeSet::in_set(size_t idx) const {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  bool is_free;\n-  switch(SET) {\n-    case Mutator:\n-      is_free = _mutator_free_bitmap.at(idx);\n-      break;\n-    case Collector:\n-      is_free = _collector_free_bitmap.at(idx);;\n-      break;\n-    case OldCollector:\n-      is_free = _old_collector_free_bitmap.at(idx);\n-      break;\n+inline void ShenandoahSetsOfFree::expand_bounds_maybe(MemoryReserve set, size_t idx, size_t region_capacity) {\n+  if (region_capacity == _region_size_bytes) {\n+    if (_left_mosts_empty[set] > idx) {\n+      _left_mosts_empty[set] = idx;\n+    }\n+    if (_right_mosts_empty[set] < idx) {\n+      _right_mosts_empty[set] = idx;\n+    }\n+  }\n+  if (_left_mosts[set] > idx) {\n+    _left_mosts[set] = idx;\n+  }\n+  if (_right_mosts[set] < idx) {\n+    _right_mosts[set] = idx;\n@@ -86,2 +130,0 @@\n-  assert(!is_free || has_alloc_capacity(idx), \"Free set should contain useful regions\");\n-  return is_free;\n@@ -90,28 +132,61 @@\n-template <MemoryReserve SET> inline void ShenandoahFreeSet::expand_bounds_maybe(size_t idx) {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  switch(SET) {\n-    case Mutator:\n-      if (idx < _mutator_leftmost) {\n-        _mutator_leftmost = idx;\n-      }\n-      if (idx > _mutator_rightmost) {\n-        _mutator_rightmost = idx;\n-      }\n-      break;\n-    case Collector:\n-      if (idx < _collector_leftmost) {\n-        _collector_leftmost = idx;\n-      }\n-      if (idx > _collector_rightmost) {\n-        _collector_rightmost = idx;\n-      }\n-      break;\n-    case OldCollector:\n-      if (idx < _old_collector_leftmost) {\n-        _old_collector_leftmost = idx;\n-      }\n-      if (idx > _old_collector_rightmost) {\n-        _old_collector_rightmost = idx;\n-      }\n-      break;\n+void ShenandoahSetsOfFree::remove_from_free_sets(size_t idx) {\n+  shenandoah_assert_heaplocked();\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  MemoryReserve orig_set = membership(idx);\n+  assert (orig_set > NotFree && orig_set < NumFreeSets, \"Cannot remove from free sets if not already free\");\n+  _membership[idx] = NotFree;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[NotFree]++;\n+}\n+\n+\n+void ShenandoahSetsOfFree::make_free(size_t idx, MemoryReserve which_set, size_t region_capacity) {\n+  shenandoah_assert_heaplocked();\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  assert (_membership[idx] == NotFree, \"Cannot make free if already free\");\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  _membership[idx] = which_set;\n+  _capacity_of[which_set] += region_capacity;\n+  expand_bounds_maybe(which_set, idx, region_capacity);\n+\n+  _region_counts[NotFree]--;\n+  _region_counts[which_set]++;\n+}\n+\n+void ShenandoahSetsOfFree::move_to_set(size_t idx, MemoryReserve new_set, size_t region_capacity) {\n+  shenandoah_assert_heaplocked();\n+  assert (region_capacity == _region_size_bytes, \"Only move entirely empty regions\");\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  MemoryReserve orig_set = _membership[idx];\n+  assert ((orig_set > NotFree) && (orig_set < NumFreeSets), \"Cannot move free unless already free\");\n+  assert ((new_set > NotFree) && (new_set < NumFreeSets), \"New set must be valid\");\n+\n+  _membership[idx] = new_set;\n+  _capacity_of[orig_set] -= region_capacity;\n+  shrink_bounds_if_touched(orig_set, idx);\n+\n+  _capacity_of[new_set] += region_capacity;\n+  expand_bounds_maybe(new_set, idx, region_capacity);\n+\n+  _region_counts[orig_set]--;\n+  _region_counts[new_set]++;\n+}\n+\n+inline MemoryReserve ShenandoahSetsOfFree::membership(size_t idx) const {\n+  shenandoah_assert_heaplocked();\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  return _membership[idx];\n+}\n+\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+inline bool ShenandoahSetsOfFree::in_free_set(size_t idx, MemoryReserve test_set) const {\n+  shenandoah_assert_heaplocked();\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT, idx, _max);\n+  if (_membership[idx] == test_set) {\n+    assert (test_set == NotFree || _free_set->alloc_capacity(idx) > 0, \"Free regions must have alloc capacity\");\n+    return true;\n+  } else {\n+    return false;\n@@ -121,17 +196,9 @@\n-template <MemoryReserve SET> inline void ShenandoahFreeSet::add_to_set(size_t idx) {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  assert(has_alloc_capacity(idx), \"Regions added to free set should have allocation capacity\");\n-  switch(SET) {\n-    case Mutator:\n-      assert(!_collector_free_bitmap.at(idx) && !_old_collector_free_bitmap.at(idx), \"Freeset membership is mutually exclusive\");\n-      _mutator_free_bitmap.set_bit(idx);\n-      break;\n-    case Collector:\n-      assert(!_mutator_free_bitmap.at(idx) && !_old_collector_free_bitmap.at(idx), \"Freeset membership is mutually exclusive\");\n-      _collector_free_bitmap.set_bit(idx);\n-      break;\n-    case OldCollector:\n-      assert(!_mutator_free_bitmap.at(idx) && !_collector_free_bitmap.at(idx), \"Freeset membership is mutually exclusive\");\n-      _old_collector_free_bitmap.set_bit(idx);\n-      break;\n+inline size_t ShenandoahSetsOfFree::left_most(MemoryReserve which_set) const {\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _left_mosts[which_set];\n+  if (idx >= _max) {\n+    return _max;\n+  } else {\n+    assert (in_free_set(idx, which_set), \"left-most region must be free\");\n+    return idx;\n@@ -139,1 +206,0 @@\n-  expand_bounds_maybe<SET>(idx);\n@@ -142,13 +208,9 @@\n-template <MemoryReserve SET> inline void ShenandoahFreeSet::remove_from_set(size_t idx) {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  switch(SET) {\n-    case Mutator:\n-      _mutator_free_bitmap.clear_bit(idx);\n-      break;\n-    case Collector:\n-      _collector_free_bitmap.clear_bit(idx);\n-      break;\n-    case OldCollector:\n-      _old_collector_free_bitmap.clear_bit(idx);\n-      break;\n+inline size_t ShenandoahSetsOfFree::right_most(MemoryReserve which_set) const {\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  size_t idx = _right_mosts[which_set];\n+  if ((idx == 0) && (_membership[idx] != which_set)) {\n+    return _max;\n+  } else {\n+    assert (in_free_set(idx, which_set), \"right-most region must be free\");\n+    return idx;\n@@ -156,1 +218,0 @@\n-  adjust_bounds_if_touched<SET>(idx);\n@@ -159,22 +220,95 @@\n-\/\/ If idx represents a mutator bound, recompute the mutator bounds, returning true iff bounds were adjusted.\n-template <MemoryReserve SET> bool ShenandoahFreeSet::adjust_bounds_if_touched(size_t idx) {\n-  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n-          idx, _max, _mutator_leftmost, _mutator_rightmost);\n-  switch(SET) {\n-    case Mutator:\n-      if (idx == _mutator_leftmost || idx == _mutator_rightmost) {\n-        \/\/ Rewind both mutator bounds until the next bit.\n-        while (_mutator_leftmost < _max && !_mutator_free_bitmap.at(_mutator_leftmost)) {\n-          _mutator_leftmost++;\n-        }\n-        while (_mutator_rightmost > 0 && !_mutator_free_bitmap.at(_mutator_rightmost)) {\n-          _mutator_rightmost--;\n-        }\n-        return true;\n-      }\n-      break;        \n-    case Collector:\n-      if (idx == _collector_leftmost || idx == _collector_rightmost) {\n-        \/\/ Rewind both collector bounds until the next bit.\n-        while (_collector_leftmost < _max && !_collector_free_bitmap.at(_collector_leftmost)) {\n-          _collector_leftmost++;\n+size_t ShenandoahSetsOfFree::left_most_empty(MemoryReserve which_set) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (size_t idx = _left_mosts_empty[which_set]; idx < _max; idx++) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _left_mosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _left_mosts_empty[which_set] = _max;\n+  _right_mosts_empty[which_set] = 0;\n+  return _max;\n+}\n+\n+inline size_t ShenandoahSetsOfFree::right_most_empty(MemoryReserve which_set) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  for (intptr_t idx = _right_mosts_empty[which_set]; idx >= 0; idx--) {\n+    if ((membership(idx) == which_set) && (_free_set->alloc_capacity(idx) == _region_size_bytes)) {\n+      _right_mosts_empty[which_set] = idx;\n+      return idx;\n+    }\n+  }\n+  _left_mosts_empty[which_set] = _max;\n+  _right_mosts_empty[which_set] = 0;\n+  return _max;\n+}\n+\n+inline bool ShenandoahSetsOfFree::alloc_from_left_bias(MemoryReserve which_set) {\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+  return _left_to_right_bias[which_set];\n+}\n+\n+void ShenandoahSetsOfFree::establish_alloc_bias(MemoryReserve which_set) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  shenandoah_assert_heaplocked();\n+  assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+\n+  size_t middle = (_left_mosts[which_set] + _right_mosts[which_set]) \/ 2;\n+  size_t available_in_first_half = 0;\n+  size_t available_in_second_half = 0;\n+\n+  for (size_t index = _left_mosts[which_set]; index < middle; index++) {\n+    if (in_free_set(index, which_set)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_first_half += r->free();\n+    }\n+  }\n+  for (size_t index = middle; index <= _right_mosts[which_set]; index++) {\n+    if (in_free_set(index, which_set)) {\n+      ShenandoahHeapRegion* r = heap->get_region(index);\n+      available_in_second_half += r->free();\n+    }\n+  }\n+\n+  \/\/ We desire to first consume the sparsely distributed regions in order that the remaining regions are densely packed.\n+  \/\/ Densely packing regions reduces the effort to search for a region that has sufficient memory to satisfy a new allocation\n+  \/\/ request.  Regions become sparsely distributed following a Full GC, which tends to slide all regions to the front of the\n+  \/\/ heap rather than allowing survivor regions to remain at the high end of the heap where we intend for them to congregate.\n+  \/\/ In the future, we may modify Full GC so that it slides old objects to the end of the heap and young objects to the start\n+  \/\/ of the heap. If this is done, we can always search survivor Collector and OldCollector regions right to left.\n+  _left_to_right_bias[which_set] = (available_in_second_half > available_in_first_half);\n+}\n+\n+#ifdef ASSERT\n+void ShenandoahSetsOfFree::assert_bounds() {\n+\n+  size_t left_mosts[NumFreeSets];\n+  size_t right_mosts[NumFreeSets];\n+  size_t empty_left_mosts[NumFreeSets];\n+  size_t empty_right_mosts[NumFreeSets];\n+\n+  for (int i = 0; i < NumFreeSets; i++) {\n+    left_mosts[i] = _max;\n+    empty_left_mosts[i] = _max;\n+    right_mosts[i] = 0;\n+    empty_right_mosts[i] = 0;\n+  }\n+\n+  for (size_t i = 0; i < _max; i++) {\n+    MemoryReserve set = membership(i);\n+    switch (set) {\n+      case NotFree:\n+        break;\n+\n+      case Mutator:\n+      case Collector:\n+      case OldCollector:\n+      {\n+        size_t capacity = _free_set->alloc_capacity(i);\n+        bool is_empty = (capacity == _region_size_bytes);\n+        assert(capacity > 0, \"free regions must have allocation capacity\");\n+        if (i < left_mosts[set]) {\n+          left_mosts[set] = i;\n@@ -182,2 +316,2 @@\n-        while (_collector_rightmost > 0 && !_collector_free_bitmap.at(_collector_rightmost)) {\n-          _collector_rightmost--;\n+        if (is_empty && (i < empty_left_mosts[set])) {\n+          empty_left_mosts[set] = i;\n@@ -185,8 +319,2 @@\n-        return true;\n-      }\n-      break;        \n-    case OldCollector:\n-      if (idx == _old_collector_leftmost || idx == _old_collector_rightmost) {\n-        \/\/ Rewind both old_collector bounds until the next bit.\n-        while (_old_collector_leftmost < _max && !_old_collector_free_bitmap.at(_old_collector_leftmost)) {\n-          _old_collector_leftmost++;\n+        if (i > right_mosts[i]) {\n+          right_mosts[set] = i;\n@@ -194,2 +322,2 @@\n-        while (_old_collector_rightmost > 0 && !_old_collector_free_bitmap.at(_old_collector_rightmost)) {\n-          _old_collector_rightmost--;\n+        if (is_empty && (i > empty_right_mosts[set])) {\n+          empty_right_mosts[set] = i;\n@@ -197,1 +325,1 @@\n-        return true;\n+        break;\n@@ -199,1 +327,5 @@\n-      break;        \n+\n+      case NumFreeSets:\n+      default:\n+        ShouldNotReachHere();\n+    }\n@@ -201,1 +333,81 @@\n-  return false;\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (left_most(Mutator) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, left_most(Mutator),  _max);\n+  assert (right_most(Mutator) <= _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, right_most(Mutator),  _max);\n+\n+  assert (left_most(Mutator) == _max || in_free_set(left_most(Mutator), Mutator),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  left_most(Mutator));\n+  assert (left_most(Mutator) == _max || in_free_set(right_most(Mutator), Mutator),\n+          \"rightmost region should be free: \" SIZE_FORMAT, right_most(Mutator));\n+\n+  \/\/ If Mutator set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  size_t beg_off = left_mosts[Mutator];\n+  size_t end_off = right_mosts[Mutator];\n+  assert (beg_off >= left_most(Mutator),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(Mutator));\n+  assert (end_off <= right_most(Mutator),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(Mutator));\n+\n+  beg_off = empty_left_mosts[Mutator];\n+  end_off = empty_right_mosts[Mutator];\n+  assert (beg_off >= left_most_empty(Mutator),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(Mutator));\n+  assert (end_off <= right_most_empty(Mutator),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(Mutator));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (left_most(Collector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, left_most(Collector),  _max);\n+  assert (right_most(Collector) <= _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, right_most(Collector),  _max);\n+\n+  assert (left_most(Collector) == _max || in_free_set(left_most(Collector), Collector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  left_most(Collector));\n+  assert (left_most(Collector) == _max || in_free_set(right_most(Collector), Collector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, right_most(Collector));\n+\n+  \/\/ If Collector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = left_mosts[Collector];\n+  end_off = right_mosts[Collector];\n+  assert (beg_off >= left_most(Collector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(Collector));\n+  assert (end_off <= right_most(Collector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(Collector));\n+\n+  beg_off = empty_left_mosts[Collector];\n+  end_off = empty_right_mosts[Collector];\n+  assert (beg_off >= left_most_empty(Collector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(Collector));\n+  assert (end_off <= right_most_empty(Collector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(Collector));\n+\n+  \/\/ Performance invariants. Failing these would not break the free set, but performance would suffer.\n+  assert (left_most(OldCollector) <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, left_most(OldCollector),  _max);\n+  assert (right_most(OldCollector) <= _max, \"rightmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, right_most(OldCollector),  _max);\n+\n+  assert (left_most(OldCollector) == _max || in_free_set(left_most(OldCollector), OldCollector),\n+          \"leftmost region should be free: \" SIZE_FORMAT,  left_most(OldCollector));\n+  assert (left_most(OldCollector) == _max || in_free_set(right_most(OldCollector), OldCollector),\n+          \"rightmost region should be free: \" SIZE_FORMAT, right_most(OldCollector));\n+\n+  \/\/ If OldCollector set is empty, leftmosts will both equal max, rightmosts will both equal zero.  Likewise for empty region sets.\n+  beg_off = left_mosts[OldCollector];\n+  end_off = right_mosts[OldCollector];\n+  assert (beg_off >= left_most(OldCollector),\n+          \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(OldCollector));\n+  assert (end_off <= right_most(OldCollector),\n+          \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(OldCollector));\n+\n+  beg_off = empty_left_mosts[OldCollector];\n+  end_off = empty_right_mosts[OldCollector];\n+  assert (beg_off >= left_most_empty(OldCollector),\n+          \"free empty regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, left_most(OldCollector));\n+  assert (end_off <= right_most_empty(OldCollector),\n+          \"free empty regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, right_most(OldCollector));\n+}\n+#endif\n+\n+\n+ShenandoahFreeSet::ShenandoahFreeSet(ShenandoahHeap* heap, size_t max_regions) :\n+  _heap(heap),\n+  _free_sets(max_regions, this)\n+{\n+  clear_internal();\n@@ -210,3 +422,5 @@\n-  size_t rightmost = _old_collector_rightmost;\n-  size_t leftmost = _old_collector_leftmost;\n-  if (_old_collector_search_left_to_right) {\n+  size_t rightmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.right_most_empty(OldCollector): _free_sets.right_most(OldCollector);\n+  size_t leftmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.left_most_empty(OldCollector): _free_sets.left_most(OldCollector);\n+  if (_free_sets.alloc_from_left_bias(OldCollector)) {\n@@ -214,3 +428,3 @@\n-    for (size_t c = leftmost; c <= rightmost; c++) {\n-      if (in_set<OldCollector>(c)) {\n-        ShenandoahHeapRegion* r = _heap->get_region(c);\n+    for (size_t idx = leftmost; idx <= rightmost; idx++) {\n+      if (_free_sets.in_free_set(idx, OldCollector)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n@@ -228,1 +442,1 @@\n-    for (size_t c = rightmost + 1; c > leftmost; c--) {\n+    for (size_t count = rightmost + 1; count > leftmost; count--) {\n@@ -230,2 +444,2 @@\n-      size_t idx = c - 1;\n-      if (in_set<OldCollector>(idx)) {\n+      size_t idx = count - 1;\n+      if (_free_sets.in_free_set(idx, OldCollector)) {\n@@ -248,1 +462,5 @@\n-  for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+  size_t rightmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.right_most_empty(Collector): _free_sets.right_most(Collector);\n+  size_t leftmost =\n+    (affiliation == ShenandoahAffiliation::FREE)? _free_sets.left_most_empty(Collector): _free_sets.left_most(Collector);\n+  for (size_t c = rightmost + 1; c > leftmost; c--) {\n@@ -251,1 +469,1 @@\n-    if (in_set<Collector>(idx)) {\n+    if (_free_sets.in_free_set(idx, Collector)) {\n@@ -312,1 +530,1 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n+      for (size_t idx = _free_sets.left_most(Mutator); idx <= _free_sets.right_most(Mutator); idx++) {\n@@ -314,1 +532,1 @@\n-        if (in_set<Mutator>(idx) && (allow_new_region || r->is_affiliated())) {\n+        if (_free_sets.in_free_set(idx, Mutator) && (allow_new_region || r->is_affiliated())) {\n@@ -336,1 +554,1 @@\n-        for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+        for (size_t c = _free_sets.right_most(Collector) + 1; c > _free_sets.left_most(Collector); c--) {\n@@ -338,1 +556,1 @@\n-          if (in_set<Collector>(idx)) {\n+          if (_free_sets.in_free_set(idx, Collector)) {\n@@ -389,1 +607,1 @@\n-        for (size_t c = _mutator_rightmost + 1; c > _mutator_leftmost; c--) {\n+        for (size_t c = _free_sets.right_most(Mutator) + 1; c > _free_sets.left_most(Mutator); c--) {\n@@ -391,1 +609,1 @@\n-          if (in_set<Mutator>(idx)) {\n+          if (_free_sets.in_free_set(idx, Mutator)) {\n@@ -578,1 +796,1 @@\n-      increase_used(size * HeapWordSize);\n+      _free_sets.increase_used(Mutator, size * HeapWordSize);\n@@ -598,1 +816,1 @@\n-  if (result == nullptr || has_no_alloc_capacity(r)) {\n+  if (result == nullptr || alloc_capacity(r) < PLAB::min_size() * HeapWordSize) {\n@@ -604,2 +822,0 @@\n-    \/\/ TODO: Record first fully-empty region, and use that for large allocations and\/or organize\n-    \/\/ available free segments within regions for more efficient searches for \"good fit\".\n@@ -612,1 +828,1 @@\n-        increase_used(waste);\n+        _free_sets.increase_used(Mutator, waste);\n@@ -616,40 +832,4 @@\n-      assert(probe_set<Mutator>(idx), \"Must be mutator free: \" SIZE_FORMAT, idx);\n-      remove_from_set<Mutator>(idx);\n-      assert(!in_set<Collector>(idx) && !in_set<OldCollector>(idx), \"Region cannot be in multiple free sets\");\n-    } else if (r->free() < PLAB::min_size() * HeapWordSize) {\n-      \/\/ Permanently retire this region if there's room for a fill object.  By permanently retiring the region,\n-      \/\/ we simplify future allocation efforts.  Regions with \"very limited\" available will not be added to \n-      \/\/ future collector or old_collector sets.  This allows the sets to be more represented more compactly, with\n-      \/\/ a smaller delta between leftmost and rightmost indexes.  It reduces the effort required to find a region\n-      \/\/ with sufficient memory to satisfy future allocation requests.  It reduces the need to rediscover that\n-      \/\/ this region has insufficient memory, eliminates the need to retire the region multiple times, and\n-      \/\/ reduces the need to adjust bounds each time a region is retired.\n-      size_t waste = r->free();\n-      HeapWord* fill_addr = r->top();\n-      size_t fill_size = waste \/ HeapWordSize;\n-      if (fill_size >= ShenandoahHeap::min_fill_size()) {\n-        ShenandoahHeap::fill_with_object(fill_addr, fill_size);\n-        r->set_top(r->end());\n-        \/\/ Since we have filled the waste with an empty object, account for increased usage\n-        _heap->increase_used(waste);\n-      } else {\n-        \/\/ We'll retire the region until the freeset is rebuilt. Since retiement is not permanent, we do not account for waste.\n-        waste = 0;\n-      }\n-      if (probe_set<OldCollector>(idx)) {\n-        assert(_heap->mode()->is_generational(), \"Old collector free regions only present in generational mode\");\n-        if (waste > 0) {\n-          _heap->old_generation()->increase_used(waste);\n-          _heap->card_scan()->register_object(fill_addr);\n-        }\n-        remove_from_set<OldCollector>(idx);\n-        assert(!in_set<Collector>(idx) && !in_set<Mutator>(idx), \"Region cannot be in multiple free sets\");\n-      } else {\n-        assert(probe_set<Collector>(idx), \"Region that is not mutator free must be collector free or old collector free\");\n-        if ((waste > 0) && _heap->mode()->is_generational()) {\n-          _heap->young_generation()->increase_used(waste);\n-        }\n-        \/\/ This applies to both generational and non-generational mode\n-        remove_from_set<Collector>(idx);\n-        assert(!in_set<Mutator>(idx) && !in_set<OldCollector>(idx), \"Region cannot be in multiple free sets\");\n-      }\n+      assert(_free_sets.membership(idx) == Mutator, \"Must be mutator free: \" SIZE_FORMAT, idx);\n+    } else {\n+      assert(_free_sets.membership(idx) == Collector || _free_sets.membership(idx) == OldCollector,\n+             \"Must be collector or old-collector free: \" SIZE_FORMAT, idx);\n@@ -657,1 +837,3 @@\n-    assert_bounds();\n+    \/\/ This region is no longer considered free (in any set)\n+    _free_sets.remove_from_free_sets(idx);\n+    _free_sets.assert_bounds();\n@@ -662,70 +844,0 @@\n-bool ShenandoahFreeSet::touches_bounds(size_t num) const {\n-  return (num == _collector_leftmost || num == _collector_rightmost ||\n-          num == _old_collector_leftmost || num == _old_collector_rightmost ||\n-          num == _mutator_leftmost || num == _mutator_rightmost);\n-}\n-\n-\/\/ Recompute bounds is onl\n-void ShenandoahFreeSet::recompute_bounds() {\n-  \/\/ Reset to the most pessimistic case:\n-  _mutator_rightmost = _max - 1;\n-  _mutator_leftmost = 0;\n-  _collector_rightmost = _max - 1;\n-  _collector_leftmost = 0;\n-  _old_collector_rightmost = _max - 1;\n-  _old_collector_leftmost = 0;\n-\n-  \/\/ ...and adjust from there\n-  adjust_bounds();\n-  if (_heap->mode()->is_generational()) {\n-    size_t old_collector_middle = (_old_collector_leftmost + _old_collector_rightmost) \/ 2;\n-    size_t old_collector_available_in_first_half = 0;\n-    size_t old_collector_available_in_second_half = 0;\n-\n-    for (size_t index = _old_collector_leftmost; index < old_collector_middle; index++) {\n-      if (in_set<OldCollector>(index)) {\n-        ShenandoahHeapRegion* r = _heap->get_region(index);\n-        old_collector_available_in_first_half += r->free();\n-      }\n-    }\n-    for (size_t index = old_collector_middle; index <= _old_collector_rightmost; index++) {\n-      if (in_set<OldCollector>(index)) {\n-        ShenandoahHeapRegion* r = _heap->get_region(index);\n-        old_collector_available_in_second_half += r->free();\n-      }\n-    }\n-    \/\/ We desire to first consume the sparsely distributed old-collector regions in order that the remaining old-collector\n-    \/\/ regions are densely packed.  Densely packing old-collector regions reduces the effort to search for a region that\n-    \/\/ has sufficient memory to satisfy a new allocation request.  Old-collector regions become sparsely distributed following\n-    \/\/ a Full GC, which tends to slide old-gen regions to the front of the heap rather than allowing them to remain\n-    \/\/ at the end of the heap where we intend for them to congregate.  In the future, we may modify Full GC so that it\n-    \/\/ slides old objects to the end of the heap and young objects to the start of the heap. If this is done, we can\n-    \/\/ always search right to left.\n-    _old_collector_search_left_to_right = (old_collector_available_in_second_half > old_collector_available_in_first_half);\n-  }\n-}\n-\n-void ShenandoahFreeSet::adjust_bounds() {\n-  \/\/ Rewind both mutator bounds until the next bit.\n-  while (_mutator_leftmost < _max && !in_set<Mutator>(_mutator_leftmost)) {\n-    _mutator_leftmost++;\n-  }\n-  while (_mutator_rightmost > 0 && !in_set<Mutator>(_mutator_rightmost)) {\n-    _mutator_rightmost--;\n-  }\n-  \/\/ Rewind both collector bounds until the next bit.\n-  while (_collector_leftmost < _max && !in_set<Collector>(_collector_leftmost)) {\n-    _collector_leftmost++;\n-  }\n-  while (_collector_rightmost > 0 && !in_set<Collector>(_collector_rightmost)) {\n-    _collector_rightmost--;\n-  }\n-  \/\/ Rewind both old collector bounds until the next bit.\n-  while (_old_collector_leftmost < _max && !in_set<OldCollector>(_old_collector_leftmost)) {\n-    _old_collector_leftmost++;\n-  }\n-  while (_old_collector_rightmost > 0 && !in_set<OldCollector>(_old_collector_rightmost)) {\n-    _old_collector_rightmost--;\n-  }\n-}\n-\n@@ -744,1 +856,1 @@\n-    if (num > mutator_count() || (num > avail_young_regions)) {\n+    if (num > _free_sets.count(Mutator) || (num > avail_young_regions)) {\n@@ -748,1 +860,1 @@\n-    if (num > mutator_count()) {\n+    if (num > _free_sets.count(Mutator)) {\n@@ -756,1 +868,1 @@\n-  size_t beg = _mutator_leftmost;\n+  size_t beg = _free_sets.left_most(Mutator);\n@@ -760,1 +872,1 @@\n-    if (end >= _max) {\n+    if (end >= _free_sets.max()) {\n@@ -767,1 +879,1 @@\n-    if (!in_set<Mutator>(end) || !can_allocate_from(_heap->get_region(end))) {\n+    if (!_free_sets.in_free_set(end, Mutator) || !can_allocate_from(_heap->get_region(end))) {\n@@ -821,1 +933,1 @@\n-    remove_from_set<Mutator>(r->index());\n+    _free_sets.remove_from_free_sets(r->index());\n@@ -824,1 +936,1 @@\n-  increase_used(total_humongous_size);\n+  _free_sets.increase_used(Mutator, total_humongous_size);\n@@ -837,1 +949,0 @@\n-\n@@ -844,7 +955,1 @@\n-\n-  \/\/ Allocated at left\/rightmost? Move the bounds appropriately.\n-  if (beg == _mutator_leftmost || end == _mutator_rightmost) {\n-    adjust_bounds();\n-  }\n-  assert_bounds();\n-\n+  _free_sets.assert_bounds();\n@@ -862,0 +967,5 @@\n+size_t ShenandoahFreeSet::alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r);\n+}\n+\n@@ -908,1 +1018,1 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n@@ -911,3 +1021,0 @@\n-  remove_from_set<Mutator>(idx);\n-  add_to_set<OldCollector>(idx);\n-\n@@ -915,3 +1022,2 @@\n-  _capacity -= region_capacity;\n-  _old_capacity += region_capacity;\n-  assert_bounds();\n+  _free_sets.move_to_set(idx, OldCollector, region_capacity);\n+  _free_sets.assert_bounds();\n@@ -927,1 +1033,1 @@\n-  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(_free_sets.in_free_set(idx, Mutator), \"Should be in mutator view\");\n@@ -930,5 +1036,3 @@\n-  remove_from_set<Mutator>(idx);\n-  add_to_set<Collector>(idx);\n-\n-  _capacity -= alloc_capacity(r);\n-  assert_bounds();\n+  size_t region_capacity = alloc_capacity(r);\n+  _free_sets.move_to_set(idx, Collector, region_capacity);\n+  _free_sets.assert_bounds();\n@@ -947,12 +1051,1 @@\n-  _mutator_free_bitmap.clear();\n-  _collector_free_bitmap.clear();\n-  _old_collector_free_bitmap.clear();\n-  _mutator_leftmost = _max;\n-  _mutator_rightmost = 0;\n-  _collector_leftmost = _max;\n-  _collector_rightmost = 0;\n-  _old_collector_leftmost = _max;\n-  _old_collector_rightmost = 0;\n-  _capacity = 0;\n-  _old_capacity = 0;\n-  _used = 0;\n+  _free_sets.clear_all();\n@@ -972,0 +1065,1 @@\n+      assert(!_free_sets.in_free_set(idx, NotFree), \"We are about to add it, it shouldn't be there already\");\n@@ -973,2 +1067,2 @@\n-      \/\/ Do not add regions that would surely fail allocation\n-      if (has_no_alloc_capacity(region)) continue;\n+      \/\/ Do not add regions that would almost surely fail allocation\n+      if (alloc_capacity(region) < PLAB::min_size() * HeapWordSize) continue;\n@@ -977,3 +1071,1 @@\n-        _old_capacity += alloc_capacity(region);\n-        assert(!in_set<OldCollector>(idx), \"We are about to add it, it shouldn't be there already\");\n-        add_to_set<OldCollector>(idx);\n+        _free_sets.make_free(idx, OldCollector, alloc_capacity(region));\n@@ -984,1 +1076,0 @@\n-\n@@ -986,3 +1077,1 @@\n-        _capacity += alloc_capacity(region);\n-        assert(!in_set<Mutator>(idx), \"We are about to add it, it shouldn't be there already\");\n-        add_to_set<Mutator>(idx);\n+        _free_sets.make_free(idx, Mutator, alloc_capacity(region));\n@@ -1033,6 +1122,2 @@\n-  \/\/ TODO: bounds have been computed incrementally so we do not NEED to recompute bounds.  However, we currently\n-  \/\/ rely on recompute_bounds to decide whether old-collector allocations should be performed from left to right\n-  \/\/ or right to left.  Refactor this code to eliminate redundant computation of bounds and reduce effort required\n-  \/\/ for deciding direction of old-collector allocations.\n-  recompute_bounds();\n-  assert_bounds();\n+  _free_sets.establish_alloc_bias(OldCollector);\n+  _free_sets.assert_bounds();\n@@ -1048,1 +1133,0 @@\n-  size_t reserved = 0;\n@@ -1051,2 +1135,5 @@\n-    if (_mutator_free_bitmap.at(idx) && (alloc_capacity(r) > 0)) {\n-      assert(!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+    if (_free_sets.in_free_set(idx, Mutator)) {\n+      assert (!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+      size_t ac = alloc_capacity(r);\n+      assert (ac > 0, \"Membership in free set implies has capacity\");\n+\n@@ -1054,6 +1141,2 @@\n-      if ((_old_capacity < to_reserve_old) && (r->is_trash() || !r->is_affiliated())) {\n-        remove_from_set<Mutator>(idx);\n-        add_to_set<OldCollector>(idx);\n-        size_t ac = alloc_capacity(r);\n-        _capacity -= ac;\n-        _old_capacity += ac;\n+      if ((_free_sets.capacity_of(OldCollector) < to_reserve_old) && (r->is_trash() || !r->is_affiliated())) {\n+        _free_sets.move_to_set(idx, OldCollector, alloc_capacity(r));\n@@ -1061,1 +1144,1 @@\n-      } else if (reserved < to_reserve) {\n+      } else if (_free_sets.capacity_of(Collector) < to_reserve) {\n@@ -1066,5 +1149,1 @@\n-        remove_from_set<Mutator>(idx);\n-        add_to_set<Collector>(idx);\n-        size_t ac = alloc_capacity(r);\n-        _capacity -= ac;\n-        reserved += ac;\n+        _free_sets.move_to_set(idx, Collector, ac);\n@@ -1096,2 +1175,2 @@\n-    log_info(gc, free)(\"FreeSet map legend (see source for unexpected codes: *, $, !, #):\\n\"\n-                       \" m:mutator_free c:collector_free C:old_collector_free\"\n+    log_info(gc, free)(\"FreeSet map legend:\"\n+                       \" mM:mutator_free cC:collector_free oO:old_collector_free\"\n@@ -1102,3 +1181,5 @@\n-                       _mutator_leftmost, _mutator_rightmost, _collector_leftmost, _collector_rightmost,\n-                       _old_collector_leftmost, _old_collector_rightmost,\n-                       _old_collector_search_left_to_right? \"left to right\": \"right to left\");\n+                       _free_sets.left_most(Mutator), _free_sets.right_most(Mutator),\n+                       _free_sets.left_most(Collector), _free_sets.right_most(Collector),\n+                       _free_sets.left_most(OldCollector), _free_sets.right_most(OldCollector),\n+                       _free_sets.alloc_from_left_bias(OldCollector)? \"left to right\": \"right to left\");\n+\n@@ -1111,11 +1192,1 @@\n-      if (in_set<Mutator>(i) && in_set<Collector>(i) && in_set<OldCollector>(i)) {\n-        buffer[idx] = '*';\n-      } else if (in_set<Mutator>(i) && in_set<Collector>(i)) {\n-        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n-        buffer[idx] = '$';\n-      } else if (in_set<Mutator>(i) && in_set<OldCollector>(i)) {\n-        \/\/ Note that young regions may be in the old_collector_free set.\n-        buffer[idx] = '!';\n-      } else if (in_set<Collector>(i) && in_set<OldCollector>(i)) {\n-        buffer[idx] = '#';\n-      } else if (in_set<Mutator>(i)) {\n+      if (_free_sets.in_free_set(i, Mutator)) {\n@@ -1123,2 +1194,2 @@\n-        buffer[idx] = 'm';\n-      } else if (in_set<Collector>(i)) {\n+        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'M': 'm';\n+      } else if (_free_sets.in_free_set(i, Collector)) {\n@@ -1126,3 +1197,3 @@\n-        buffer[idx] = 'c';\n-      } else if (in_set<OldCollector>(i)) {\n-        buffer[idx] = 'C';\n+        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'C': 'c';\n+      } else if (_free_sets.in_free_set(i, OldCollector)) {\n+        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'O': 'o';\n@@ -1145,1 +1216,0 @@\n-\n@@ -1181,2 +1251,2 @@\n-      for (size_t idx = _mutator_leftmost; idx <= _mutator_rightmost; idx++) {\n-        if (in_set<Mutator>(idx)) {\n+      for (size_t idx = _free_sets.left_most(Mutator); idx <= _free_sets.right_most(Mutator); idx++) {\n+        if (_free_sets.in_free_set(idx, Mutator)) {\n@@ -1185,1 +1255,0 @@\n-\n@@ -1187,1 +1256,0 @@\n-\n@@ -1198,1 +1266,0 @@\n-\n@@ -1229,2 +1296,2 @@\n-      if (mutator_count() > 0) {\n-        frag_int = (100 * (total_used \/ mutator_count()) \/ ShenandoahHeapRegion::region_size_bytes());\n+      if (_free_sets.count(Mutator) > 0) {\n+        frag_int = (100 * (total_used \/ _free_sets.count(Mutator)) \/ ShenandoahHeapRegion::region_size_bytes());\n@@ -1236,1 +1303,1 @@\n-               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), mutator_count());\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used), _free_sets.count(Mutator));\n@@ -1244,2 +1311,2 @@\n-      for (size_t idx = _collector_leftmost; idx <= _collector_rightmost; idx++) {\n-        if (in_set<Collector>(idx)) {\n+      for (size_t idx = _free_sets.left_most(Collector); idx <= _free_sets.right_most(Collector); idx++) {\n+        if (_free_sets.in_free_set(idx, Collector)) {\n@@ -1264,2 +1331,2 @@\n-      for (size_t idx = _old_collector_leftmost; idx <= _old_collector_rightmost; idx++) {\n-        if (in_set<OldCollector>(idx)) {\n+      for (size_t idx = _free_sets.left_most(OldCollector); idx <= _free_sets.right_most(OldCollector); idx++) {\n+        if (_free_sets.in_free_set(idx, OldCollector)) {\n@@ -1283,1 +1350,1 @@\n-  assert_bounds();\n+  _free_sets.assert_bounds();\n@@ -1311,2 +1378,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (index < _max && in_set<Mutator>(index)) {\n+  for (size_t index = _free_sets.left_most(Mutator); index <= _free_sets.right_most(Mutator); index++) {\n+    if (index < _free_sets.max() && _free_sets.in_free_set(index, Mutator)) {\n@@ -1325,3 +1392,3 @@\n-  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", mutator_count());\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (in_set<Mutator>(index)) {\n+  out->print_cr(\"Mutator Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Mutator));\n+  for (size_t index = _free_sets.left_most(Mutator); index <= _free_sets.right_most(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -1331,3 +1398,3 @@\n-  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", collector_count());\n-  for (size_t index = _collector_leftmost; index <= _collector_rightmost; index++) {\n-    if (in_set<Collector>(index)) {\n+  out->print_cr(\"Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(Collector));\n+  for (size_t index = _free_sets.left_most(Collector); index <= _free_sets.right_most(Collector); index++) {\n+    if (_free_sets.in_free_set(index, Collector)) {\n@@ -1337,0 +1404,8 @@\n+  if (_heap->mode()->is_generational()) {\n+    out->print_cr(\"Old Collector Free Set: \" SIZE_FORMAT \"\", _free_sets.count(OldCollector));\n+    for (size_t index = _free_sets.left_most(OldCollector); index <= _free_sets.right_most(OldCollector); index++) {\n+      if (_free_sets.in_free_set(index, OldCollector)) {\n+        _heap->get_region(index)->print_on(out);\n+      }\n+    }\n+  }\n@@ -1365,2 +1440,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (in_set<Mutator>(index)) {\n+  for (size_t index = _free_sets.left_most(Mutator); index <= _free_sets.right_most(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -1403,2 +1478,2 @@\n-  for (size_t index = _mutator_leftmost; index <= _mutator_rightmost; index++) {\n-    if (in_set<Mutator>(index)) {\n+  for (size_t index = _free_sets.left_most(Mutator); index <= _free_sets.right_most(Mutator); index++) {\n+    if (_free_sets.in_free_set(index, Mutator)) {\n@@ -1429,38 +1504,0 @@\n-#ifdef ASSERT\n-void ShenandoahFreeSet::assert_bounds() const {\n-  \/\/ Performance invariants. Failing these would not break the free set, but performance\n-  \/\/ would suffer.\n-  assert (_mutator_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_leftmost,  _max);\n-  assert (_mutator_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _mutator_rightmost, _max);\n-\n-  assert (_mutator_leftmost == _max || in_set<Mutator>(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || in_set<Mutator>(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n-\n-  size_t beg_off = _mutator_free_bitmap.find_first_set_bit(0);\n-  size_t end_off = _mutator_free_bitmap.find_first_set_bit(_mutator_rightmost + 1);\n-  assert (beg_off >= _mutator_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _mutator_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _mutator_rightmost);\n-\n-  assert (_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_leftmost,  _max);\n-  assert (_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _collector_rightmost, _max);\n-\n-  assert (_collector_leftmost == _max || in_set<Collector>(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || in_set<Collector>(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n-\n-  beg_off = _collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _collector_free_bitmap.find_first_set_bit(_collector_rightmost + 1);\n-  assert (beg_off >= _collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _collector_rightmost);\n-\n-  assert (_old_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _old_collector_leftmost,  _max);\n-  assert (_old_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _old_collector_rightmost, _max);\n-\n-  assert (_old_collector_leftmost == _max || in_set<OldCollector>(_old_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _old_collector_leftmost);\n-  assert (_old_collector_rightmost == 0   || in_set<OldCollector>(_old_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _old_collector_rightmost);\n-\n-  beg_off = _old_collector_free_bitmap.find_first_set_bit(0);\n-  end_off = _old_collector_free_bitmap.find_first_set_bit(_old_collector_rightmost + 1);\n-  assert (beg_off >= _old_collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _old_collector_leftmost);\n-  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _old_collector_rightmost);\n-}\n-#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":458,"deletions":421,"binary":false,"changes":879,"status":"modified"},{"patch":"@@ -32,1 +32,2 @@\n-enum MemoryReserve {\n+enum MemoryReserve : uint8_t {\n+  NotFree,\n@@ -35,1 +36,2 @@\n-  OldCollector\n+  OldCollector,\n+  NumFreeSets\n@@ -38,1 +40,1 @@\n-class ShenandoahFreeSet : public CHeapObj<mtGC> {\n+class ShenandoahSetsOfFree {\n@@ -40,16 +42,0 @@\n-\n-  ShenandoahHeap* const _heap;\n-  CHeapBitMap _mutator_free_bitmap;\n-\n-  \/\/ The _collector_free regions hold survivor objects within young-generation and within traditional single-generation\n-  \/\/ collections.  In general, the _collector_free regions are at the high end of memory and mutator-free regions are at\n-  \/\/ the low-end of memory.  In generational mode, the young survivor regions are typically recycled after the region reaches\n-  \/\/ tenure age.  In the case that a young survivor region reaches tenure age and has sufficiently low amount of garbage,\n-  \/\/ the region will be promoted in place.  This means the region will simply be relabled as an old-generation region and\n-  \/\/ will not be evacuated until an old-generation collection chooses to do so.\n-  CHeapBitMap _collector_free_bitmap;\n-\n-  \/\/ We keep the _old_collector regions separate from the young collector regions.  This allows us to pack the old regions\n-  \/\/ further to the right than the young collector regions.  This is desirable because the old collector regions are recycled\n-  \/\/ even less frequently than the young survivor regions.\n-  CHeapBitMap _old_collector_free_bitmap;\n@@ -57,0 +43,17 @@\n+  ShenandoahFreeSet* _free_set;\n+  size_t _region_size_bytes;\n+  MemoryReserve* _membership;\n+  size_t _left_mosts[NumFreeSets];\n+  size_t _right_mosts[NumFreeSets];\n+  size_t _left_mosts_empty[NumFreeSets];\n+  size_t _right_mosts_empty[NumFreeSets];\n+  size_t _capacity_of[NumFreeSets];\n+  size_t _used_by[NumFreeSets];\n+  bool _left_to_right_bias[NumFreeSets];\n+  size_t _region_counts[NumFreeSets];\n+\n+  inline void shrink_bounds_if_touched(MemoryReserve set, size_t idx);\n+  inline void expand_bounds_maybe(MemoryReserve set, size_t idx, size_t capacity);\n+\n+  \/\/ Restore all state variables to initial default state.\n+  void clear_internal();\n@@ -58,8 +61,3 @@\n-  \/\/ Left-most and right-most region indexes. There are no free regions outside of [left-most; right-most] index intervals.\n-  \/\/ For a free set of a given kind (mutator, collector, old_collector), we maintain left and right indices to limit\n-  \/\/ searching. The intervals represented by these extremal indices designate the lowest and highest indices at which\n-  \/\/ that kind of free region exists. These intervals may overlap. In particular, it is quite common for the collector\n-  \/\/ free interval to overlap the mutator free interval on one side (the low end) and the old_collector free interval\n-  \/\/ on the other (the high end).  It is also possible for the mutator interval to overlap the old_collector free\n-  \/\/ interval.\n-  size_t _mutator_leftmost, _mutator_rightmost;\n+public:\n+  ShenandoahSetsOfFree(size_t max_regions, ShenandoahFreeSet* free_set);\n+  ~ShenandoahSetsOfFree();\n@@ -67,2 +65,2 @@\n-  size_t _collector_leftmost, _collector_rightmost;\n-  size_t _old_collector_leftmost, _old_collector_rightmost;\n+  \/\/ Make all regions NotFree and reset all bounds\n+  void clear_all();\n@@ -70,3 +68,2 @@\n-  \/\/ _capacity represents the amount of memory that can be allocated within the mutator set at the time of the\n-  \/\/ most recent rebuild, as adjusted for the flipping of regions from mutator set to collector set or old collector set.\n-  size_t _capacity;\n+  \/\/ Remove or retire region idx from all free sets.  Requires that idx is in a free set.  This does not affect capacity.\n+  void remove_from_free_sets(size_t idx);\n@@ -74,4 +71,2 @@\n-  \/\/ _used represents the amount of memory allocated within the mutator set since the time of the most recent rebuild.\n-  \/\/ _used feeds into certain ShenandoanPacing decisions.  There is no need to track of the memory consumed from\n-  \/\/ within the collector and old_collector sets.\n-  size_t _used;\n+  \/\/ Place region idx into free set which_set.  Requires that idx is currently NotFree.\n+  void make_free(size_t idx, MemoryReserve which_set, size_t region_capacity);\n@@ -79,3 +74,2 @@\n-  \/\/ _old_capacity represents the amount of memory that can be allocated within the old collector set at the time\n-  \/\/ of the most recent rebuild, as adjusted for the flipping of regions from mutator set to old collector set.\n-  size_t _old_capacity;\n+  \/\/ Place region idx into free set new_set.  Requires that idx is currently not NotFRee.\n+  void move_to_set(size_t idx, MemoryReserve new_set, size_t region_capacity);\n@@ -83,3 +77,3 @@\n-  \/\/ There is no need to compute young collector capacity.  And there is not need to consult _old_capacity once we\n-  \/\/ have successfully reserved the evacuation (old_collector and collector sets) requested at rebuild time.\n-  \/\/ TODO: A cleaner abstraction might encapsulate capacity (and used) information within a refactored set abstraction.\n+  \/\/ Returns the MemoryReserve affiliation of region idx, or NotFree if this region is not currently free.  This does\n+  \/\/ not enforce that free_set membership implies allocation capacity.\n+  inline MemoryReserve membership(size_t idx) const;\n@@ -87,0 +81,3 @@\n+  \/\/ Returns true iff region idx is in the test_set free_set.  Before returning true, asserts that the free\n+  \/\/ set is not empty.  Requires that test_set != NotFree or NumFreeSets.\n+  inline bool in_free_set(size_t idx, MemoryReserve which_set) const;\n@@ -88,4 +85,5 @@\n-  \/\/ When old_collector_set regions sparsely populate the lower address ranges of the heap, we search from left to\n-  \/\/ right in order to consume (and remove from the old_collector set range) these sparsely distributed regions.\n-  \/\/ This allows us to more quickly condense the range of addresses that represent old_collector_free regions.\n-  bool _old_collector_search_left_to_right = true;\n+  \/\/ Each of the following four methods returns _max to indicate absence of requested region.\n+  inline size_t left_most(MemoryReserve which_set) const;\n+  inline size_t right_most(MemoryReserve which_set) const;\n+  size_t left_most_empty(MemoryReserve which_set);\n+  size_t right_most_empty(MemoryReserve which_set);\n@@ -93,1 +91,26 @@\n-  \/\/ Assure leftmost and rightmost bounds are valid for the mutator_is_free, collector_is_free, and old_collector_is_free sets.\n+  inline void increase_used(MemoryReserve which_set, size_t bytes);\n+\n+  inline size_t capacity_of(MemoryReserve which_set) const {\n+    shenandoah_assert_heaplocked();\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    return _capacity_of[which_set];\n+  }\n+\n+  inline size_t used_by(MemoryReserve which_set) const {\n+    shenandoah_assert_heaplocked();\n+    assert (which_set > NotFree && which_set < NumFreeSets, \"selected free set must be valid\");\n+    return _used_by[which_set];\n+  }\n+\n+  inline size_t max() const { return _max; }\n+\n+  inline size_t count(MemoryReserve which_set) const { return _region_counts[which_set]; }\n+\n+  \/\/ Return true iff regions for allocation from this set should be peformed left to right.  Otherwise, allocate\n+  \/\/ from right to left.\n+  inline bool alloc_from_left_bias(MemoryReserve which_set);\n+\n+  \/\/ Determine whether we prefer to allocate from left to right or from right to left for this free-set.\n+  void establish_alloc_bias(MemoryReserve which_set);\n+\n+  \/\/ Assure leftmost, rightmost, leftmost_empty, and rightmost_empty bounds are valid for all free sets.\n@@ -105,17 +128,10 @@\n-  void assert_bounds() const NOT_DEBUG_RETURN;\n-\n-  \/\/ Every region is in exactly one of four sets: mutator_free, collector_free, old_collector_free, not_free.\n-  \/\/ Insofar as the free-set abstraction is concerned, we are only interested in regions that are free so we provide no\n-  \/\/ mechanism to directly inquire as to whether a region is not_free.  not_free membership is implied by not member of\n-  \/\/ mutator_free, collector_free and old_collector_free sets.\n-  \/\/\n-  \/\/ in_set() implies that the region has allocation capacity (i.e. is not yet fully allocated) as assured by assertions.\n-  \/\/\n-  \/\/ TODO: a future implementation may replace the three bitmaps with a single array of enums to simplify the representation\n-  \/\/ of membership within these four mutually exclusive sets.\n-\n-  template <MemoryReserve SET> inline bool in_set(size_t idx) const;\n-\n-  \/\/ The following probe routine mimics the behavior is in_set() but does not assert that regions have allocation capacity.\n-  \/\/ This probe routine is used in assertions enforced during certain state transitions.\n-  template <MemoryReserve SET> inline bool probe_set(size_t idx) const;\n+  \/\/   if the set has no empty regions, leftmost_empty equals max and right_most_empty equals 0\n+  \/\/   Otherwise (the region has empty regions):\n+  \/\/     0 <= lefmost_empty < max and 0 <= rightmost_empty < max\n+  \/\/     rightmost_empty >= leftmost_empty\n+  \/\/     for every idx that is in the set and is empty {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n+  void assert_bounds() NOT_DEBUG_RETURN;\n+};\n@@ -123,3 +139,4 @@\n-  \/\/ The next two methods change set membership of regions\n-  template <MemoryReserve SET> inline void add_to_set(size_t idx);\n-  template <MemoryReserve SSET> inline void remove_from_set(size_t idx);\n+class ShenandoahFreeSet : public CHeapObj<mtGC> {\n+private:\n+  ShenandoahHeap* const _heap;\n+  ShenandoahSetsOfFree _free_sets;\n@@ -149,19 +166,0 @@\n-  \/\/ Compute left-most and right-most indexes for the mutator_is_free, collector_is_free, and old_collector_is_free sets.\n-  void recompute_bounds();\n-\n-  \/\/ Adjust left-most and right-most indexes for the mutator_is_free, collector_is_free, and old_collector_is_free sets\n-  \/\/  following minor changes to at least one set membership.\n-  void adjust_bounds();\n-\n-  \/\/ Adjust left-most and right-most indexes for the <SET> free set after adding region idx to this set.\n-  template <MemoryReserve SET> inline void expand_bounds_maybe(size_t idx);\n-\n-  \/\/ Adjust left-most and right-most indexes for the <SET> free set after removing region idx from this set.\n-  template <MemoryReserve SET> bool adjust_bounds_if_touched(size_t idx);\n-\n-  \/\/ Return true iff region idx was the left-most or right-most index for one of the three free sets.\n-  bool touches_bounds(size_t idx) const;\n-\n-   \/\/ Used of free set represents the amount of is_mutator_free set that has been consumed since most recent rebuild.\n-  void increase_used(size_t amount);\n-\n@@ -173,1 +171,0 @@\n-  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -181,8 +178,2 @@\n-  \/\/ Number of regions dedicated to GC allocations (for evacuation) that are at least partially free\n-  size_t collector_count() const { return _collector_free_bitmap.count_one_bits(); }\n-\n-  \/\/ Number of regions dedicated to Old GC allocations (for evacuation or promotion) that are at least partially free\n-  size_t old_collector_count() const { return _old_collector_free_bitmap.count_one_bits(); }\n-\n-  \/\/ Number of regions dedicated to mutator allocations that are at least partially free\n-  size_t mutator_count()   const { return _mutator_free_bitmap.count_one_bits();   }\n+  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  size_t alloc_capacity(size_t idx) const;\n@@ -197,5 +188,5 @@\n-  size_t capacity()  const { return _capacity; }\n-  size_t used()      const { return _used;     }\n-  size_t available() const {\n-    assert(_used <= _capacity, \"must use less than capacity\");\n-    return _capacity - _used;\n+  inline size_t capacity()  const { return _free_sets.capacity_of(Mutator); }\n+  inline size_t used()      const { return _free_sets.used_by(Mutator);     }\n+  inline size_t available() const {\n+    assert(used() <= capacity(), \"must use less than capacity\");\n+    return capacity() - used();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":91,"deletions":100,"binary":false,"changes":191,"status":"modified"}]}