{"files":[{"patch":"@@ -1202,1 +1202,0 @@\n-  heap->rebuild_free_set(true \/*concurrent*\/);\n@@ -1204,0 +1203,1 @@\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+  _old_collector_free_bitmap(max_regions, mtGC),\n@@ -50,1 +51,1 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n+inline void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n@@ -53,0 +54,3 @@\n+  assert(_used <= _capacity, \"must not use (\" SIZE_FORMAT \") more than we have (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+         _used, _capacity, num_bytes);\n+}\n@@ -54,2 +58,11 @@\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+template <MemoryReserve SET> inline bool ShenandoahFreeSet::probe_set(size_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n+          idx, _max, _collector_leftmost, _collector_rightmost);\n+  switch(SET) {\n+    case Mutator:\n+      return _mutator_free_bitmap.at(idx);\n+    case Collector:\n+      return _collector_free_bitmap.at(idx);\n+    case OldCollector:\n+      return _old_collector_free_bitmap.at(idx);\n+  }\n@@ -58,1 +71,1 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n+template <MemoryReserve SET> inline bool ShenandoahFreeSet::in_set(size_t idx) const {\n@@ -61,1 +74,14 @@\n-  return _mutator_free_bitmap.at(idx);\n+  bool is_free;\n+  switch(SET) {\n+    case Mutator:\n+      is_free = _mutator_free_bitmap.at(idx);\n+      break;\n+    case Collector:\n+      is_free = _collector_free_bitmap.at(idx);;\n+      break;\n+    case OldCollector:\n+      is_free = _old_collector_free_bitmap.at(idx);\n+      break;\n+  }\n+  assert(!is_free || has_alloc_capacity(idx), \"Free set should contain useful regions\");\n+  return is_free;\n@@ -64,1 +90,1 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n+template <MemoryReserve SET> inline void ShenandoahFreeSet::expand_bounds_maybe(size_t idx) {\n@@ -66,2 +92,27 @@\n-          idx, _max, _collector_leftmost, _collector_rightmost);\n-  return _collector_free_bitmap.at(idx);\n+          idx, _max, _mutator_leftmost, _mutator_rightmost);\n+  switch(SET) {\n+    case Mutator:\n+      if (idx < _mutator_leftmost) {\n+        _mutator_leftmost = idx;\n+      }\n+      if (idx > _mutator_rightmost) {\n+        _mutator_rightmost = idx;\n+      }\n+      break;\n+    case Collector:\n+      if (idx < _collector_leftmost) {\n+        _collector_leftmost = idx;\n+      }\n+      if (idx > _collector_rightmost) {\n+        _collector_rightmost = idx;\n+      }\n+      break;\n+    case OldCollector:\n+      if (idx < _old_collector_leftmost) {\n+        _old_collector_leftmost = idx;\n+      }\n+      if (idx > _old_collector_rightmost) {\n+        _old_collector_rightmost = idx;\n+      }\n+      break;\n+  }\n@@ -70,7 +121,20 @@\n-\/\/ This is a temporary solution to work around a shortcoming with the existing free set implementation.\n-\/\/ TODO:\n-\/\/   Remove this function after restructing FreeSet representation.  A problem in the existing implementation is that old-gen\n-\/\/   regions are not considered to reside within the is_collector_free range.\n-\/\/\n-HeapWord* ShenandoahFreeSet::allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahAffiliation affiliation = ShenandoahAffiliation::OLD_GENERATION;\n+template <MemoryReserve SET> inline void ShenandoahFreeSet::add_to_set(size_t idx) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n+          idx, _max, _mutator_leftmost, _mutator_rightmost);\n+  assert(has_alloc_capacity(idx), \"Regions added to free set should have allocation capacity\");\n+  switch(SET) {\n+    case Mutator:\n+      assert(!_collector_free_bitmap.at(idx) && !_old_collector_free_bitmap.at(idx), \"Freeset membership is mutually exclusive\");\n+      _mutator_free_bitmap.set_bit(idx);\n+      break;\n+    case Collector:\n+      assert(!_mutator_free_bitmap.at(idx) && !_old_collector_free_bitmap.at(idx), \"Freeset membership is mutually exclusive\");\n+      _collector_free_bitmap.set_bit(idx);\n+      break;\n+    case OldCollector:\n+      assert(!_mutator_free_bitmap.at(idx) && !_collector_free_bitmap.at(idx), \"Freeset membership is mutually exclusive\");\n+      _old_collector_free_bitmap.set_bit(idx);\n+      break;\n+  }\n+  expand_bounds_maybe<SET>(idx);\n+}\n@@ -78,2 +142,16 @@\n-  size_t rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n-  size_t leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+template <MemoryReserve SET> inline void ShenandoahFreeSet::remove_from_set(size_t idx) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n+          idx, _max, _mutator_leftmost, _mutator_rightmost);\n+  switch(SET) {\n+    case Mutator:\n+      _mutator_free_bitmap.clear_bit(idx);\n+      break;\n+    case Collector:\n+      _collector_free_bitmap.clear_bit(idx);\n+      break;\n+    case OldCollector:\n+      _old_collector_free_bitmap.clear_bit(idx);\n+      break;\n+  }\n+  adjust_bounds_if_touched<SET>(idx);\n+}\n@@ -81,9 +159,80 @@\n-  for (size_t c = rightmost + 1; c > leftmost; c--) {\n-    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-    size_t idx = c - 1;\n-    ShenandoahHeapRegion* r = _heap->get_region(idx);\n-    if (r->affiliation() == affiliation && !r->is_humongous()) {\n-      if (!r->is_cset() && !has_no_alloc_capacity(r)) {\n-        HeapWord* result = try_allocate_in(r, req, in_new_region);\n-        if (result != nullptr) {\n-          return result;\n+\/\/ If idx represents a mutator bound, recompute the mutator bounds, returning true iff bounds were adjusted.\n+template <MemoryReserve SET> bool ShenandoahFreeSet::adjust_bounds_if_touched(size_t idx) {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n+          idx, _max, _mutator_leftmost, _mutator_rightmost);\n+  switch(SET) {\n+    case Mutator:\n+      if (idx == _mutator_leftmost || idx == _mutator_rightmost) {\n+        \/\/ Rewind both mutator bounds until the next bit.\n+        while (_mutator_leftmost < _max && !_mutator_free_bitmap.at(_mutator_leftmost)) {\n+          _mutator_leftmost++;\n+        }\n+        while (_mutator_rightmost > 0 && !_mutator_free_bitmap.at(_mutator_rightmost)) {\n+          _mutator_rightmost--;\n+        }\n+        return true;\n+      }\n+      break;        \n+    case Collector:\n+      if (idx == _collector_leftmost || idx == _collector_rightmost) {\n+        \/\/ Rewind both collector bounds until the next bit.\n+        while (_collector_leftmost < _max && !_collector_free_bitmap.at(_collector_leftmost)) {\n+          _collector_leftmost++;\n+        }\n+        while (_collector_rightmost > 0 && !_collector_free_bitmap.at(_collector_rightmost)) {\n+          _collector_rightmost--;\n+        }\n+        return true;\n+      }\n+      break;        \n+    case OldCollector:\n+      if (idx == _old_collector_leftmost || idx == _old_collector_rightmost) {\n+        \/\/ Rewind both old_collector bounds until the next bit.\n+        while (_old_collector_leftmost < _max && !_old_collector_free_bitmap.at(_old_collector_leftmost)) {\n+          _old_collector_leftmost++;\n+        }\n+        while (_old_collector_rightmost > 0 && !_old_collector_free_bitmap.at(_old_collector_rightmost)) {\n+          _old_collector_rightmost--;\n+        }\n+        return true;\n+      }\n+      break;        \n+  }\n+  return false;\n+}\n+\n+\/\/ This allocates from a region within the old_collector_set.  If affiliation equals OLD, the allocation must be taken\n+\/\/ from a region that is_old().  Otherwise, affiliation should be FREE, in which case this will put a previously unaffiliated\n+\/\/ region into service.\n+HeapWord* ShenandoahFreeSet::allocate_old_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                           ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  size_t rightmost = _old_collector_rightmost;\n+  size_t leftmost = _old_collector_leftmost;\n+  if (_old_collector_search_left_to_right) {\n+    \/\/ This mode picks up stragglers left by a full GC\n+    for (size_t c = leftmost; c <= rightmost; c++) {\n+      if (in_set<OldCollector>(c)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(c);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  } else {\n+    \/\/ This mode picks up stragglers left by a previous concurrent GC\n+    for (size_t c = rightmost + 1; c > leftmost; c--) {\n+      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      size_t idx = c - 1;\n+      if (in_set<OldCollector>(idx)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n@@ -98,0 +247,1 @@\n+  shenandoah_assert_heaplocked();\n@@ -101,1 +251,1 @@\n-    if (is_collector_free(idx)) {\n+    if (in_set<Collector>(idx)) {\n@@ -116,0 +266,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -162,1 +314,1 @@\n-        if (is_mutator_free(idx) && (allow_new_region || r->is_affiliated())) {\n+        if (in_set<Mutator>(idx) && (allow_new_region || r->is_affiliated())) {\n@@ -186,1 +338,1 @@\n-          if (is_collector_free(idx)) {\n+          if (in_set<Collector>(idx)) {\n@@ -197,3 +349,1 @@\n-          \/\/ TODO: this is a work around to address a deficiency in FreeSet representation.  A better solution fixes\n-          \/\/ the FreeSet implementation to deal more efficiently with old-gen regions as being in the \"collector free set\"\n-          result = allocate_with_old_affiliation(req, in_new_region);\n+          result = allocate_old_with_affiliation(req.affiliation(), req, in_new_region);\n@@ -208,1 +358,5 @@\n-          result = allocate_with_affiliation(FREE, req, in_new_region);\n+          if (req.is_old()) {\n+            result = allocate_old_with_affiliation(FREE, req, in_new_region);\n+          } else {\n+            result = allocate_with_affiliation(FREE, req, in_new_region);\n+          }\n@@ -220,0 +374,13 @@\n+      \/\/ TODO:\n+      \/\/ if (!allow_new_region && req.is_old() && (young_generation->adjusted_unaffiliated_regions() > 0)) {\n+      \/\/   transfer a region from young to old;\n+      \/\/   allow_new_region = true;\n+      \/\/   heap->set_old_evac_reserve(heap->get_old_evac_reserve() + region_size_bytes);\n+      \/\/ }\n+      \/\/\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+\n@@ -224,1 +391,1 @@\n-          if (is_mutator_free(idx)) {\n+          if (in_set<Mutator>(idx)) {\n@@ -227,1 +394,5 @@\n-              flip_to_gc(r);\n+              if (req.is_old()) {\n+                flip_to_old_gc(r);\n+              } else {\n+                flip_to_gc(r);\n+              }\n@@ -250,2 +421,1 @@\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n-\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n@@ -337,2 +507,1 @@\n-          increase_used(padding);\n-          assert(r->is_old(), \"All PLABs reside in old-gen\");\n+           assert(r->is_old(), \"All PLABs reside in old-gen\");\n@@ -389,1 +558,0 @@\n-        increase_used(padding);\n@@ -431,1 +599,1 @@\n-    \/\/ Region cannot afford this or future allocations. Retire it.\n+    \/\/ Region cannot afford this and is likely to not afford future allocations. Retire it.\n@@ -434,1 +602,1 @@\n-    \/\/ fit, but the next small one would, we are risking to inflate scan times when lots of\n+    \/\/ fit but the next small one would, we are risking to inflate scan times when lots of\n@@ -440,0 +608,1 @@\n+    size_t idx = r->index();\n@@ -447,8 +616,40 @@\n-    }\n-\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n+      assert(probe_set<Mutator>(idx), \"Must be mutator free: \" SIZE_FORMAT, idx);\n+      remove_from_set<Mutator>(idx);\n+      assert(!in_set<Collector>(idx) && !in_set<OldCollector>(idx), \"Region cannot be in multiple free sets\");\n+    } else if (r->free() < PLAB::min_size() * HeapWordSize) {\n+      \/\/ Permanently retire this region if there's room for a fill object.  By permanently retiring the region,\n+      \/\/ we simplify future allocation efforts.  Regions with \"very limited\" available will not be added to \n+      \/\/ future collector or old_collector sets.  This allows the sets to be more represented more compactly, with\n+      \/\/ a smaller delta between leftmost and rightmost indexes.  It reduces the effort required to find a region\n+      \/\/ with sufficient memory to satisfy future allocation requests.  It reduces the need to rediscover that\n+      \/\/ this region has insufficient memory, eliminates the need to retire the region multiple times, and\n+      \/\/ reduces the need to adjust bounds each time a region is retired.\n+      size_t waste = r->free();\n+      HeapWord* fill_addr = r->top();\n+      size_t fill_size = waste \/ HeapWordSize;\n+      if (fill_size >= ShenandoahHeap::min_fill_size()) {\n+        ShenandoahHeap::fill_with_object(fill_addr, fill_size);\n+        r->set_top(r->end());\n+        \/\/ Since we have filled the waste with an empty object, account for increased usage\n+        _heap->increase_used(waste);\n+      } else {\n+        \/\/ We'll retire the region until the freeset is rebuilt. Since retiement is not permanent, we do not account for waste.\n+        waste = 0;\n+      }\n+      if (probe_set<OldCollector>(idx)) {\n+        assert(_heap->mode()->is_generational(), \"Old collector free regions only present in generational mode\");\n+        if (waste > 0) {\n+          _heap->old_generation()->increase_used(waste);\n+          _heap->card_scan()->register_object(fill_addr);\n+        }\n+        remove_from_set<OldCollector>(idx);\n+        assert(!in_set<Collector>(idx) && !in_set<Mutator>(idx), \"Region cannot be in multiple free sets\");\n+      } else {\n+        assert(probe_set<Collector>(idx), \"Region that is not mutator free must be collector free or old collector free\");\n+        if ((waste > 0) && _heap->mode()->is_generational()) {\n+          _heap->young_generation()->increase_used(waste);\n+        }\n+        \/\/ This applies to both generational and non-generational mode\n+        remove_from_set<Collector>(idx);\n+        assert(!in_set<Mutator>(idx) && !in_set<OldCollector>(idx), \"Region cannot be in multiple free sets\");\n+      }\n@@ -462,1 +663,3 @@\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n+  return (num == _collector_leftmost || num == _collector_rightmost ||\n+          num == _old_collector_leftmost || num == _old_collector_rightmost ||\n+          num == _mutator_leftmost || num == _mutator_rightmost);\n@@ -465,0 +668,1 @@\n+\/\/ Recompute bounds is onl\n@@ -471,0 +675,2 @@\n+  _old_collector_rightmost = _max - 1;\n+  _old_collector_leftmost = 0;\n@@ -474,0 +680,26 @@\n+  if (_heap->mode()->is_generational()) {\n+    size_t old_collector_middle = (_old_collector_leftmost + _old_collector_rightmost) \/ 2;\n+    size_t old_collector_available_in_first_half = 0;\n+    size_t old_collector_available_in_second_half = 0;\n+\n+    for (size_t index = _old_collector_leftmost; index < old_collector_middle; index++) {\n+      if (in_set<OldCollector>(index)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(index);\n+        old_collector_available_in_first_half += r->free();\n+      }\n+    }\n+    for (size_t index = old_collector_middle; index <= _old_collector_rightmost; index++) {\n+      if (in_set<OldCollector>(index)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(index);\n+        old_collector_available_in_second_half += r->free();\n+      }\n+    }\n+    \/\/ We desire to first consume the sparsely distributed old-collector regions in order that the remaining old-collector\n+    \/\/ regions are densely packed.  Densely packing old-collector regions reduces the effort to search for a region that\n+    \/\/ has sufficient memory to satisfy a new allocation request.  Old-collector regions become sparsely distributed following\n+    \/\/ a Full GC, which tends to slide old-gen regions to the front of the heap rather than allowing them to remain\n+    \/\/ at the end of the heap where we intend for them to congregate.  In the future, we may modify Full GC so that it\n+    \/\/ slides old objects to the end of the heap and young objects to the start of the heap. If this is done, we can\n+    \/\/ always search right to left.\n+    _old_collector_search_left_to_right = (old_collector_available_in_second_half > old_collector_available_in_first_half);\n+  }\n@@ -478,1 +710,1 @@\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n+  while (_mutator_leftmost < _max && !in_set<Mutator>(_mutator_leftmost)) {\n@@ -481,1 +713,1 @@\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n+  while (_mutator_rightmost > 0 && !in_set<Mutator>(_mutator_rightmost)) {\n@@ -485,1 +717,1 @@\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n+  while (_collector_leftmost < _max && !in_set<Collector>(_collector_leftmost)) {\n@@ -488,1 +720,1 @@\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n+  while (_collector_rightmost > 0 && !in_set<Collector>(_collector_rightmost)) {\n@@ -491,0 +723,7 @@\n+  \/\/ Rewind both old collector bounds until the next bit.\n+  while (_old_collector_leftmost < _max && !in_set<OldCollector>(_old_collector_leftmost)) {\n+    _old_collector_leftmost++;\n+  }\n+  while (_old_collector_rightmost > 0 && !in_set<OldCollector>(_old_collector_rightmost)) {\n+    _old_collector_rightmost--;\n+  }\n@@ -528,1 +767,1 @@\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n+    if (!in_set<Mutator>(end) || !can_allocate_from(_heap->get_region(end))) {\n@@ -581,1 +820,0 @@\n-\n@@ -583,1 +821,1 @@\n-    _mutator_free_bitmap.clear_bit(r->index());\n+    remove_from_set<Mutator>(r->index());\n@@ -617,1 +855,4 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n@@ -621,1 +862,1 @@\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n+size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -630,1 +871,10 @@\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n+bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+bool ShenandoahFreeSet::has_alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r) > 0;\n+}\n+\n+bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -655,0 +905,19 @@\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  remove_from_set<Mutator>(idx);\n+  add_to_set<OldCollector>(idx);\n+\n+  size_t region_capacity = alloc_capacity(r);\n+  _capacity -= region_capacity;\n+  _old_capacity += region_capacity;\n+  assert_bounds();\n+\n+  \/\/ We do not ensure that the region is no longer trash,\n+  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n@@ -661,4 +930,2 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n+  remove_from_set<Mutator>(idx);\n+  add_to_set<Collector>(idx);\n@@ -667,4 +934,0 @@\n-\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n@@ -686,0 +949,1 @@\n+  _old_collector_free_bitmap.clear();\n@@ -690,0 +954,2 @@\n+  _old_collector_leftmost = _max;\n+  _old_collector_rightmost = 0;\n@@ -691,0 +957,1 @@\n+  _old_capacity = 0;\n@@ -694,3 +961,6 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n+\/\/ This function places all is_old() regions that have allocation capacity into the old_collector set.  It places\n+\/\/ all other regions (not is_old()) that have allocation capacity into the mutator_set.  Subsequently, we will\n+\/\/ move some of the mutator regions into the collector set or old_collector set with the intent of packing\n+\/\/ old_collector memory into the highest (rightmost) addresses of the heap and the collector memory into the\n+\/\/ next highest addresses of the heap, with mutator memory consuming the lowest addresses of the heap.\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity() {\n@@ -698,1 +968,0 @@\n-  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -702,1 +971,1 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -707,5 +976,8 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n-\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+      if (region->is_old()) {\n+        _old_capacity += alloc_capacity(region);\n+        assert(!in_set<OldCollector>(idx), \"We are about to add it, it shouldn't be there already\");\n+        add_to_set<OldCollector>(idx);\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT  \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to old collector set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -713,1 +985,6 @@\n-      log_debug(gc, free)(\"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n+      } else {\n+        _capacity += alloc_capacity(region);\n+        assert(!in_set<Mutator>(idx), \"We are about to add it, it shouldn't be there already\");\n+        add_to_set<Mutator>(idx);\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator set\",\n@@ -715,1 +992,2 @@\n-               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      }\n@@ -718,0 +996,12 @@\n+}\n+\n+void ShenandoahFreeSet::rebuild() {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n+\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.\n+  find_regions_with_alloc_capacity();\n@@ -719,1 +1009,3 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n+  \/\/ Evac reserve: reserve trailing space for evacuations, with regions reserved for old evacuations placed to the right\n+  \/\/ of regions reserved of young evacuations.\n+  size_t young_reserve, old_reserve;\n@@ -721,2 +1013,2 @@\n-    size_t to_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    reserve_regions(to_reserve);\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -724,2 +1016,1 @@\n-    size_t young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    \/\/ Note that all allocations performed from old-gen are performed by GC, generally using PLABs for both\n+    \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n@@ -727,6 +1018,12 @@\n-    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons within\n-    \/\/ each PLAB.  We do not reserve any of old-gen memory in order to facilitate the loaning of old-gen memory\n-    \/\/ to young-gen purposes.\n-    size_t old_reserve = 0;\n-    size_t to_reserve = young_reserve + old_reserve;\n-    reserve_regions(to_reserve);\n+    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons for\n+    \/\/ each PLAB's available memory.\n+    if (_heap->has_evacuation_reserve_quantities()) {\n+      \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+      young_reserve = _heap->get_young_evac_reserve();\n+      old_reserve = _heap->get_promoted_reserve() + _heap->get_old_evac_reserve();\n+    } else {\n+      \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+      young_reserve = (_heap->young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      old_reserve = MAX2((_heap->old_generation()->max_capacity() * ShenandoahOldEvacReserve) \/ 100,\n+                         ShenandoahOldCompactionReserve * ShenandoahHeapRegion::region_size_bytes());\n+    }\n@@ -734,0 +1031,1 @@\n+  reserve_regions(young_reserve, old_reserve);\n@@ -735,0 +1033,4 @@\n+  \/\/ TODO: bounds have been computed incrementally so we do not NEED to recompute bounds.  However, we currently\n+  \/\/ rely on recompute_bounds to decide whether old-collector allocations should be performed from left to right\n+  \/\/ or right to left.  Refactor this code to eliminate redundant computation of bounds and reduce effort required\n+  \/\/ for deciding direction of old-collector allocations.\n@@ -737,0 +1039,1 @@\n+  log_status();\n@@ -739,1 +1042,6 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve, and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old) {\n@@ -741,1 +1049,0 @@\n-\n@@ -743,12 +1050,26 @@\n-    if (reserved >= to_reserve) break;\n-\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n-      log_debug(gc, free)(\"  Shifting Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to collector free set\",\n-                          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-                               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (_mutator_free_bitmap.at(idx) && (alloc_capacity(r) > 0)) {\n+      assert(!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+      \/\/ OLD regions that have available memory are already in the old_collector free set\n+      if ((_old_capacity < to_reserve_old) && (r->is_trash() || !r->is_affiliated())) {\n+        remove_from_set<Mutator>(idx);\n+        add_to_set<OldCollector>(idx);\n+        size_t ac = alloc_capacity(r);\n+        _capacity -= ac;\n+        _old_capacity += ac;\n+        log_debug(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+      } else if (reserved < to_reserve) {\n+        \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+        \/\/ they were entirely empty.  I'm not sure I understand the rational for that.  That alternative behavior would\n+        \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+        \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n+        remove_from_set<Mutator>(idx);\n+        add_to_set<Collector>(idx);\n+        size_t ac = alloc_capacity(r);\n+        _capacity -= ac;\n+        reserved += ac;\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n+      } else {\n+        \/\/ We've satisfied both to_reserve and to_reserved_old\n+        break;\n+      }\n@@ -762,0 +1083,83 @@\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  {\n+#define BUFFER_SIZE 80\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_info(gc, free)(\"FreeSet map legend (see source for unexpected codes: *, $, !, #):\\n\"\n+                       \" m:mutator_free c:collector_free C:old_collector_free\"\n+                       \" h:humongous young H:humongous old ~:retired old _:retired young\");\n+    log_info(gc, free)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n+                       _mutator_leftmost, _mutator_rightmost, _collector_leftmost, _collector_rightmost,\n+                       _old_collector_leftmost, _old_collector_rightmost,\n+                       _old_collector_search_left_to_right? \"left to right\": \"right to left\");\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_info(gc, free)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (in_set<Mutator>(i) && in_set<Collector>(i) && in_set<OldCollector>(i)) {\n+        buffer[idx] = '*';\n+      } else if (in_set<Mutator>(i) && in_set<Collector>(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = '$';\n+      } else if (in_set<Mutator>(i) && in_set<OldCollector>(i)) {\n+        \/\/ Note that young regions may be in the old_collector_free set.\n+        buffer[idx] = '!';\n+      } else if (in_set<Collector>(i) && in_set<OldCollector>(i)) {\n+        buffer[idx] = '#';\n+      } else if (in_set<Mutator>(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in mutator_free set\");\n+        buffer[idx] = 'm';\n+      } else if (in_set<Collector>(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = 'c';\n+      } else if (in_set<OldCollector>(i)) {\n+        buffer[idx] = 'C';\n+      } else if (r->is_humongous()) {\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n+      } else {\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young += region_size_bytes;\n+        }\n+\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_info(gc, free)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+    size_t total_young = retired_young + retired_young_humongous;\n+    size_t total_old = retired_old + retired_old_humongous;\n+    log_info(gc, free)(\"Retired young: \" SIZE_FORMAT \"%s (including humongous: \" SIZE_FORMAT \"%s), old: \" SIZE_FORMAT\n+                       \"%s (including humongous: \" SIZE_FORMAT \"%s)\",\n+                       byte_size_in_proper_unit(total_young),             proper_unit_for_byte_size(total_young),\n+                       byte_size_in_proper_unit(retired_young_humongous), proper_unit_for_byte_size(retired_young_humongous),\n+                       byte_size_in_proper_unit(total_old),               proper_unit_for_byte_size(total_old),\n+                       byte_size_in_proper_unit(retired_old_humongous),   proper_unit_for_byte_size(retired_old_humongous));\n+  }\n+#endif\n+\n@@ -778,1 +1182,1 @@\n-        if (is_mutator_free(idx)) {\n+        if (in_set<Mutator>(idx)) {\n@@ -797,1 +1201,0 @@\n-\n@@ -806,0 +1209,4 @@\n+      assert(free == total_free, \"Sum of free within mutator regions (\" SIZE_FORMAT\n+             \") should match mutator capacity (\" SIZE_FORMAT \") minus mutator used (\" SIZE_FORMAT \")\",\n+             total_free, capacity(), used());\n+\n@@ -828,1 +1235,1 @@\n-      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT \" \",\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n@@ -838,1 +1245,1 @@\n-        if (is_collector_free(idx)) {\n+        if (in_set<Collector>(idx)) {\n@@ -846,0 +1253,10 @@\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n@@ -847,1 +1264,10 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\",\n+      for (size_t idx = _old_collector_leftmost; idx <= _old_collector_rightmost; idx++) {\n+        if (in_set<OldCollector>(idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n@@ -886,1 +1312,1 @@\n-    if (index < _max && is_mutator_free(index)) {\n+    if (index < _max && in_set<Mutator>(index)) {\n@@ -901,1 +1327,1 @@\n-    if (is_mutator_free(index)) {\n+    if (in_set<Mutator>(index)) {\n@@ -907,1 +1333,1 @@\n-    if (is_collector_free(index)) {\n+    if (in_set<Collector>(index)) {\n@@ -940,1 +1366,1 @@\n-    if (is_mutator_free(index)) {\n+    if (in_set<Mutator>(index)) {\n@@ -978,1 +1404,1 @@\n-    if (is_mutator_free(index)) {\n+    if (in_set<Mutator>(index)) {\n@@ -1010,2 +1436,2 @@\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n+  assert (_mutator_leftmost == _max || in_set<Mutator>(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n+  assert (_mutator_rightmost == 0   || in_set<Mutator>(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n@@ -1021,2 +1447,2 @@\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n+  assert (_collector_leftmost == _max || in_set<Collector>(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n+  assert (_collector_rightmost == 0   || in_set<Collector>(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n@@ -1028,0 +1454,11 @@\n+\n+  assert (_old_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _old_collector_leftmost,  _max);\n+  assert (_old_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _old_collector_rightmost, _max);\n+\n+  assert (_old_collector_leftmost == _max || in_set<OldCollector>(_old_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _old_collector_leftmost);\n+  assert (_old_collector_rightmost == 0   || in_set<OldCollector>(_old_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _old_collector_rightmost);\n+\n+  beg_off = _old_collector_free_bitmap.find_first_set_bit(0);\n+  end_off = _old_collector_free_bitmap.find_first_set_bit(_old_collector_rightmost + 1);\n+  assert (beg_off >= _old_collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _old_collector_leftmost);\n+  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _old_collector_rightmost);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":557,"deletions":120,"binary":false,"changes":677,"status":"modified"},{"patch":"@@ -32,0 +32,6 @@\n+enum MemoryReserve {\n+  Mutator,\n+  Collector,\n+  OldCollector\n+};\n+\n@@ -34,0 +40,1 @@\n+\n@@ -36,0 +43,7 @@\n+\n+  \/\/ The _collector_free regions hold survivor objects within young-generation and within traditional single-generation\n+  \/\/ collections.  In general, the _collector_free regions are at the high end of memory and mutator-free regions are at\n+  \/\/ the low-end of memory.  In generational mode, the young survivor regions are typically recycled after the region reaches\n+  \/\/ tenure age.  In the case that a young survivor region reaches tenure age and has sufficiently low amount of garbage,\n+  \/\/ the region will be promoted in place.  This means the region will simply be relabled as an old-generation region and\n+  \/\/ will not be evacuated until an old-generation collection chooses to do so.\n@@ -37,0 +51,5 @@\n+\n+  \/\/ We keep the _old_collector regions separate from the young collector regions.  This allows us to pack the old regions\n+  \/\/ further to the right than the young collector regions.  This is desirable because the old collector regions are recycled\n+  \/\/ even less frequently than the young survivor regions.\n+  CHeapBitMap _old_collector_free_bitmap;\n@@ -39,2 +58,7 @@\n-  \/\/ Left-most and right-most region indexes. There are no free regions outside\n-  \/\/ of [left-most; right-most] index intervals\n+  \/\/ Left-most and right-most region indexes. There are no free regions outside of [left-most; right-most] index intervals.\n+  \/\/ For a free set of a given kind (mutator, collector, old_collector), we maintain left and right indices to limit\n+  \/\/ searching. The intervals represented by these extremal indices designate the lowest and highest indices at which\n+  \/\/ that kind of free region exists. These intervals may overlap. In particular, it is quite common for the collector\n+  \/\/ free interval to overlap the mutator free interval on one side (the low end) and the old_collector free interval\n+  \/\/ on the other (the high end).  It is also possible for the mutator interval to overlap the old_collector free\n+  \/\/ interval.\n@@ -42,0 +66,1 @@\n+\n@@ -43,0 +68,1 @@\n+  size_t _old_collector_leftmost, _old_collector_rightmost;\n@@ -44,0 +70,2 @@\n+  \/\/ _capacity represents the amount of memory that can be allocated within the mutator set at the time of the\n+  \/\/ most recent rebuild, as adjusted for the flipping of regions from mutator set to collector set or old collector set.\n@@ -45,0 +73,4 @@\n+\n+  \/\/ _used represents the amount of memory allocated within the mutator set since the time of the most recent rebuild.\n+  \/\/ _used feeds into certain ShenandoanPacing decisions.  There is no need to track of the memory consumed from\n+  \/\/ within the collector and old_collector sets.\n@@ -47,0 +79,26 @@\n+  \/\/ _old_capacity represents the amount of memory that can be allocated within the old collector set at the time\n+  \/\/ of the most recent rebuild, as adjusted for the flipping of regions from mutator set to old collector set.\n+  size_t _old_capacity;\n+\n+  \/\/ There is no need to compute young collector capacity.  And there is not need to consult _old_capacity once we\n+  \/\/ have successfully reserved the evacuation (old_collector and collector sets) requested at rebuild time.\n+  \/\/ TODO: A cleaner abstraction might encapsulate capacity (and used) information within a refactored set abstraction.\n+\n+\n+  \/\/ When old_collector_set regions sparsely populate the lower address ranges of the heap, we search from left to\n+  \/\/ right in order to consume (and remove from the old_collector set range) these sparsely distributed regions.\n+  \/\/ This allows us to more quickly condense the range of addresses that represent old_collector_free regions.\n+  bool _old_collector_search_left_to_right = true;\n+\n+  \/\/ Assure leftmost and rightmost bounds are valid for the mutator_is_free, collector_is_free, and old_collector_is_free sets.\n+  \/\/ valid bounds honor all of the following (where max is the number of heap regions):\n+  \/\/   if the set is empty, leftmost equals max and rightmost equals 0\n+  \/\/   Otherwise (the set is not empty):\n+  \/\/     0 <= leftmost < max and 0 <= rightmost < max\n+  \/\/     the region at leftmost is in the set\n+  \/\/     the region at rightmost is in the set\n+  \/\/     rightmost >= leftmost\n+  \/\/     for every idx that is in the set {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n@@ -49,2 +107,19 @@\n-  bool is_mutator_free(size_t idx) const;\n-  bool is_collector_free(size_t idx) const;\n+  \/\/ Every region is in exactly one of four sets: mutator_free, collector_free, old_collector_free, not_free.\n+  \/\/ Insofar as the free-set abstraction is concerned, we are only interested in regions that are free so we provide no\n+  \/\/ mechanism to directly inquire as to whether a region is not_free.  not_free membership is implied by not member of\n+  \/\/ mutator_free, collector_free and old_collector_free sets.\n+  \/\/\n+  \/\/ in_set() implies that the region has allocation capacity (i.e. is not yet fully allocated) as assured by assertions.\n+  \/\/\n+  \/\/ TODO: a future implementation may replace the three bitmaps with a single array of enums to simplify the representation\n+  \/\/ of membership within these four mutually exclusive sets.\n+\n+  template <MemoryReserve SET> inline bool in_set(size_t idx) const;\n+\n+  \/\/ The following probe routine mimics the behavior is in_set() but does not assert that regions have allocation capacity.\n+  \/\/ This probe routine is used in assertions enforced during certain state transitions.\n+  template <MemoryReserve SET> inline bool probe_set(size_t idx) const;\n+\n+  \/\/ The next two methods change set membership of regions\n+  template <MemoryReserve SET> inline void add_to_set(size_t idx);\n+  template <MemoryReserve SSET> inline void remove_from_set(size_t idx);\n@@ -53,0 +128,3 @@\n+\n+  \/\/ Satisfy young-generation or single-generation collector allocation request req by finding memory that matches\n+  \/\/ affiliation, which either equals req.affiliation or FREE.  We know req.is_young().\n@@ -54,1 +132,4 @@\n-  HeapWord* allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Satisfy allocation request req by finding memory that matches affiliation, which either equals req.affiliation\n+  \/\/ or FREE. We know req.is_old().\n+  HeapWord* allocate_old_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -66,0 +147,1 @@\n+  void flip_to_old_gc(ShenandoahHeapRegion* r);\n@@ -67,0 +149,1 @@\n+  \/\/ Compute left-most and right-most indexes for the mutator_is_free, collector_is_free, and old_collector_is_free sets.\n@@ -68,0 +151,3 @@\n+\n+  \/\/ Adjust left-most and right-most indexes for the mutator_is_free, collector_is_free, and old_collector_is_free sets\n+  \/\/  following minor changes to at least one set membership.\n@@ -69,1 +155,0 @@\n-  bool touches_bounds(size_t num) const;\n@@ -71,1 +156,10 @@\n-  \/\/ Used of free set represents the amount of is_mutator_free set that has been consumed since most recent rebuild.\n+  \/\/ Adjust left-most and right-most indexes for the <SET> free set after adding region idx to this set.\n+  template <MemoryReserve SET> inline void expand_bounds_maybe(size_t idx);\n+\n+  \/\/ Adjust left-most and right-most indexes for the <SET> free set after removing region idx from this set.\n+  template <MemoryReserve SET> bool adjust_bounds_if_touched(size_t idx);\n+\n+  \/\/ Return true iff region idx was the left-most or right-most index for one of the three free sets.\n+  bool touches_bounds(size_t idx) const;\n+\n+   \/\/ Used of free set represents the amount of is_mutator_free set that has been consumed since most recent rebuild.\n@@ -73,0 +167,1 @@\n+\n@@ -77,3 +172,5 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r);\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n-  bool has_no_alloc_capacity(ShenandoahHeapRegion *r);\n+  bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  bool has_alloc_capacity(size_t idx) const;\n+  bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n+  bool has_no_alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -84,1 +181,1 @@\n-  \/\/ Number of regions dedicated to GC allocations (for evacuation or promotion) that are currently free\n+  \/\/ Number of regions dedicated to GC allocations (for evacuation) that are at least partially free\n@@ -87,1 +184,4 @@\n-  \/\/ Number of regions dedicated to mutator allocations that are currently free\n+  \/\/ Number of regions dedicated to Old GC allocations (for evacuation or promotion) that are at least partially free\n+  size_t old_collector_count() const { return _old_collector_free_bitmap.count_one_bits(); }\n+\n+  \/\/ Number of regions dedicated to mutator allocations that are at least partially free\n@@ -112,1 +212,2 @@\n-  void reserve_regions(size_t to_reserve);\n+  void find_regions_with_alloc_capacity();\n+  void reserve_regions(size_t young_reserve, size_t old_reserve);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":114,"deletions":13,"binary":false,"changes":127,"status":"modified"},{"patch":"@@ -1051,0 +1051,5 @@\n+\/\/ TODO:\n+\/\/  Consider compacting old-gen objects toward the high end of memory and young-gen objects towards the low-end\n+\/\/  of memory.  As currently implemented, all regions are compacted toward the low-end of memory.  This creates more\n+\/\/  fragmentation of the heap, because old-gen regions get scattered among low-address regions such that it becomes\n+\/\/  more difficult to find contiguous regions for humongous objects.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -836,0 +836,2 @@\n+  \/\/ Freeset construction uses reserve quantities if they are valid\n+  heap->set_evacuation_reserve_quantities(true);\n@@ -842,0 +844,1 @@\n+  heap->set_evacuation_reserve_quantities(false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -559,0 +559,2 @@\n+  _upgraded_to_full(false),\n+  _has_evacuation_reserve_quantities(false),\n@@ -2248,0 +2250,4 @@\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  _has_evacuation_reserve_quantities = is_valid;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -380,0 +380,14 @@\n+  \/\/ At the end of final mark, but before we begin evacuating, heuristics calculate how much memory is required to\n+  \/\/ hold the results of evacuating to young-gen and to old-gen.  These quantitites, stored in _promoted_reserve,\n+  \/\/ _old_evac_reserve, and _young_evac_reserve, are consulted prior to rebuilding the free set (ShenandoahFreeSet)\n+  \/\/ in preparation for evacuation.  When the free set is rebuilt, we make sure to reserve sufficient memory in the\n+  \/\/ collector and old_collector sets to hold if _has_evacuation_reserve_quantities is true.  The other time we\n+  \/\/ rebuild the freeset is at the end of GC, as we prepare to idle GC until the next trigger.  In this case,\n+  \/\/ _has_evacuation_reserve_quantities is false because we don't yet know how much memory will need to be evacuated\n+  \/\/ in the next GC cycle.  When _has_evacuation_reserve_quantities is false, the free set rebuild operation reserves\n+  \/\/ for the collector and old_collector sets based on alternative mechanisms, such as ShenandoahEvacReserve,\n+  \/\/ ShenandoahOldEvacReserve, and ShenandoahOldCompactionReserve.  In a future planned enhancement, the reserve\n+  \/\/ for old_collector set when not _has_evacuation_reserve_quantities is based in part on anticipated promotion as\n+  \/\/ determined by analysis of live data found during the previous GC pass which is one less than the current tenure age.\n+  bool _has_evacuation_reserve_quantities;\n+\n@@ -383,2 +397,0 @@\n-\n-\n@@ -386,0 +398,1 @@\n+\n@@ -389,0 +402,1 @@\n+  void set_evacuation_reserve_quantities(bool is_valid);\n@@ -404,0 +418,1 @@\n+  inline bool has_evacuation_reserve_quantities() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":17,"deletions":2,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -664,0 +664,1 @@\n+\n@@ -668,0 +669,4 @@\n+inline bool ShenandoahHeap::has_evacuation_reserve_quantities() const {\n+  return _has_evacuation_reserve_quantities;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"}]}