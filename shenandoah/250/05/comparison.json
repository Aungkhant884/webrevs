{"files":[{"patch":"@@ -33,1 +33,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1205,1 +1205,0 @@\n-  heap->rebuild_free_set(true \/*concurrent*\/);\n@@ -1207,0 +1206,1 @@\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+  _old_collector_free_bitmap(max_regions, mtGC),\n@@ -50,1 +51,1 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n+inline void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n@@ -53,0 +54,3 @@\n+  assert(_used <= _capacity, \"must not use (\" SIZE_FORMAT \") more than we have (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+         _used, _capacity, num_bytes);\n+}\n@@ -54,2 +58,2 @@\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+inline bool ShenandoahFreeSet::peek_is_mutator_free(size_t idx) const {\n+  return _mutator_free_bitmap.at(idx);\n@@ -58,1 +62,1 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n+inline bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n@@ -61,0 +65,2 @@\n+  assert(!_mutator_free_bitmap.at(idx) || (alloc_capacity(ShenandoahHeap::heap()->get_region(idx)) > 0),\n+         \"mutator_free implies available memory in region \" SIZE_FORMAT, idx);\n@@ -64,1 +70,5 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n+inline bool ShenandoahFreeSet::peek_is_collector_free(size_t idx) const {\n+  return _collector_free_bitmap.at(idx);\n+}\n+\n+inline bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n@@ -67,0 +77,2 @@\n+  assert(!_collector_free_bitmap.at(idx) || (alloc_capacity(ShenandoahHeap::heap()->get_region(idx)) > 0),\n+         \"collector_free implies available memory in region \" SIZE_FORMAT, idx);\n@@ -70,7 +82,3 @@\n-\/\/ This is a temporary solution to work around a shortcoming with the existing free set implementation.\n-\/\/ TODO:\n-\/\/   Remove this function after restructing FreeSet representation.  A problem in the existing implementation is that old-gen\n-\/\/   regions are not considered to reside within the is_collector_free range.\n-\/\/\n-HeapWord* ShenandoahFreeSet::allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahAffiliation affiliation = ShenandoahAffiliation::OLD_GENERATION;\n+inline bool ShenandoahFreeSet::peek_is_old_collector_free(size_t idx) const {\n+  return _old_collector_free_bitmap.at(idx);\n+}\n@@ -78,2 +86,7 @@\n-  size_t rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n-  size_t leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+inline bool ShenandoahFreeSet::is_old_collector_free(size_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n+          idx, _max, _old_collector_leftmost, _old_collector_rightmost);\n+  assert(!_old_collector_free_bitmap.at(idx) || (alloc_capacity(ShenandoahHeap::heap()->get_region(idx)) > 0),\n+         \"old_collector_free implies available memory in region \" SIZE_FORMAT, idx);\n+  return _old_collector_free_bitmap.at(idx);\n+}\n@@ -81,9 +94,69 @@\n-  for (size_t c = rightmost + 1; c > leftmost; c--) {\n-    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-    size_t idx = c - 1;\n-    ShenandoahHeapRegion* r = _heap->get_region(idx);\n-    if (r->affiliation() == affiliation && !r->is_humongous()) {\n-      if (!r->is_cset() && !has_no_alloc_capacity(r)) {\n-        HeapWord* result = try_allocate_in(r, req, in_new_region);\n-        if (result != nullptr) {\n-          return result;\n+inline void ShenandoahFreeSet::set_mutator_free(size_t idx) {\n+  assert(alloc_capacity(ShenandoahHeap::heap()->get_region(idx)) > 0, \"mutator_free implies available memory in region \"\n+         SIZE_FORMAT, idx);\n+  assert(!is_old_collector_free(idx) && !is_collector_free(idx), \"Freeset membership is mutually exclusive\");\n+  _mutator_free_bitmap.set_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::clear_mutator_free(size_t idx) {\n+  _mutator_free_bitmap.clear_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::set_collector_free(size_t idx) {\n+  assert(alloc_capacity(ShenandoahHeap::heap()->get_region(idx)) > 0, \"mutator_free implies available memory in region \"\n+         SIZE_FORMAT, idx);\n+  assert(!is_mutator_free(idx) && !is_old_collector_free(idx), \"Freeset membership is mutually exclusive\");\n+  _collector_free_bitmap.set_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::clear_collector_free(size_t idx) {\n+  _collector_free_bitmap.clear_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::set_old_collector_free(size_t idx) {\n+  assert(alloc_capacity(ShenandoahHeap::heap()->get_region(idx)) > 0, \"old_collector_free implies available memory in region \"\n+         SIZE_FORMAT, idx);\n+  assert(!is_mutator_free(idx) && !is_collector_free(idx), \"Freeset membership is mutually exclusive\");\n+  _old_collector_free_bitmap.set_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::clear_old_collector_free(size_t idx) {\n+  _old_collector_free_bitmap.clear_bit(idx);\n+}\n+\n+\n+HeapWord* ShenandoahFreeSet::allocate_old_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                           ShenandoahAllocRequest& req, bool& in_new_region) {\n+  size_t rightmost = _old_collector_rightmost;\n+  size_t leftmost = _old_collector_leftmost;\n+  if (_old_collector_search_left_to_right) {\n+    \/\/ This mode picks up stragglers following a full GC\n+    for (size_t c = leftmost; c <= rightmost; c++) {\n+      if (is_old_collector_free(c)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(c);\n+        assert(r->is_trash() || (r->affiliation() == ShenandoahAffiliation::FREE)\n+               || (r->affiliation() == ShenandoahAffiliation::OLD_GENERATION),\n+               \"is_old_collector_free region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  } else {\n+    \/\/ This mode picks up stragglers from a previous concurrent GC\n+    for (size_t c = rightmost + 1; c > leftmost; c--) {\n+      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      size_t idx = c - 1;\n+      if (is_old_collector_free(idx)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || (r->affiliation() == ShenandoahAffiliation::FREE)\n+               || (r->affiliation() == ShenandoahAffiliation::OLD_GENERATION),\n+               \"is_old_collector_free region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n@@ -197,3 +270,1 @@\n-          \/\/ TODO: this is a work around to address a deficiency in FreeSet representation.  A better solution fixes\n-          \/\/ the FreeSet implementation to deal more efficiently with old-gen regions as being in the \"collector free set\"\n-          result = allocate_with_old_affiliation(req, in_new_region);\n+          result = allocate_old_with_affiliation(req.affiliation(), req, in_new_region);\n@@ -208,1 +279,5 @@\n-          result = allocate_with_affiliation(FREE, req, in_new_region);\n+          if (req.affiliation() == ShenandoahAffiliation::OLD_GENERATION) {\n+            result = allocate_old_with_affiliation(FREE, req, in_new_region);\n+          } else {\n+            result = allocate_with_affiliation(FREE, req, in_new_region);\n+          }\n@@ -220,0 +295,13 @@\n+      \/\/ TODO:\n+      \/\/ if (!allow_new_region && req.is_old() && (young_generation->adjusted_unaffiliated_regions() > 0)) {\n+      \/\/   transfer a region from young to old;\n+      \/\/   allow_new_region = true;\n+      \/\/   heap->set_old_evac_reserve(heap->get_old_evac_reserve() + region_size_bytes);\n+      \/\/ }\n+      \/\/\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+\n@@ -227,1 +315,5 @@\n-              flip_to_gc(r);\n+              if (req.affiliation() == ShenandoahAffiliation::OLD_GENERATION) {\n+                flip_to_old_gc(r);\n+              } else {\n+                flip_to_gc(r);\n+              }\n@@ -337,2 +429,1 @@\n-          increase_used(padding);\n-          assert(r->is_old(), \"All PLABs reside in old-gen\");\n+           assert(r->is_old(), \"All PLABs reside in old-gen\");\n@@ -389,1 +480,0 @@\n-        increase_used(padding);\n@@ -440,0 +530,1 @@\n+    size_t idx = r->index();\n@@ -447,8 +538,35 @@\n-    }\n-\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n+      assert(peek_is_mutator_free(idx), \"Must be mutator free: \" SIZE_FORMAT, idx);\n+      clear_mutator_free(idx);\n+      assert(!is_collector_free(idx) && !is_old_collector_free(idx), \"Region cannot be in multiple free sets\");\n+      adjust_mutator_bounds_if_touched(idx);\n+    } else if (r->free() < PLAB::min_size() * HeapWordSize) {\n+      \/\/ Permanently retire this region if there's room for a fill object\n+      size_t waste = r->free();\n+      HeapWord* fill_addr = r->top();\n+      size_t fill_size = waste \/ HeapWordSize;\n+      if (fill_size >= ShenandoahHeap::min_fill_size()) {\n+        ShenandoahHeap::fill_with_object(fill_addr, fill_size);\n+        r->set_top(r->end());\n+        \/\/ Since we have filled the waste with an empty object, account for increased usage\n+        _heap->increase_used(waste);\n+      } else {\n+        waste = 0;              \/\/ if we don't make fill object, then the waste is not permanent\n+      }\n+      if (peek_is_old_collector_free(idx)) {\n+        assert(_heap->mode()->is_generational(), \"Old collector free regions only present in generational mode\");\n+        if (waste > 0) {\n+          _heap->old_generation()->increase_used(waste);\n+          _heap->card_scan()->register_object(fill_addr);\n+        }\n+        clear_old_collector_free(idx);\n+        assert(!is_collector_free(idx) && !is_mutator_free(idx), \"Region cannot be in multiple free sets\");\n+        adjust_old_collector_bounds_if_touched(idx);\n+      } else if (peek_is_collector_free(idx)) {\n+        if ((waste > 0) && _heap->mode()->is_generational()) {\n+          _heap->young_generation()->increase_used(waste);\n+        }\n+        \/\/ This applies to both generational and non-generational mode\n+        clear_collector_free(idx);\n+        assert(!is_mutator_free(idx) && !is_old_collector_free(idx), \"Region cannot be in multiple free sets\");\n+        adjust_collector_bounds_if_touched(idx);\n+      }\n@@ -461,0 +579,48 @@\n+\/\/ If idx represents a mutator bound, recompute the mutator bounds, returning true iff bounds were adjusted.\n+bool ShenandoahFreeSet::adjust_mutator_bounds_if_touched(size_t idx) {\n+  if (idx == _mutator_leftmost || idx == _mutator_rightmost) {\n+    \/\/ Rewind both mutator bounds until the next bit.\n+    while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n+      _mutator_leftmost++;\n+    }\n+    while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n+      _mutator_rightmost--;\n+    }\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ If idx represents an old collector bound, recompute the old collector bounds, returning true iff bounds were adjusted.\n+bool ShenandoahFreeSet::adjust_old_collector_bounds_if_touched(size_t idx) {\n+  if (idx == _old_collector_leftmost || idx == _old_collector_rightmost) {\n+    \/\/ Rewind both old collector bounds until the next bit.\n+    while (_old_collector_leftmost < _max && !is_old_collector_free(_old_collector_leftmost)) {\n+      _old_collector_leftmost++;\n+    }\n+    while (_old_collector_rightmost > 0 && !is_old_collector_free(_old_collector_rightmost)) {\n+      _old_collector_rightmost--;\n+    }\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ If idx represents a collector bound, recompute the collector bounds, returning true iff bounds were adjusted.\n+bool ShenandoahFreeSet::adjust_collector_bounds_if_touched(size_t idx) {\n+  if (idx == _collector_leftmost || idx == _collector_rightmost) {\n+    \/\/ Rewind both old collector bounds until the next bit.\n+    while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n+      _collector_leftmost++;\n+    }\n+    while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n+      _collector_rightmost--;\n+    }\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n@@ -462,1 +628,3 @@\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n+  return (num == _collector_leftmost || num == _collector_rightmost ||\n+          num == _old_collector_leftmost || num == _old_collector_rightmost ||\n+          num == _mutator_leftmost || num == _mutator_rightmost);\n@@ -471,0 +639,2 @@\n+  _old_collector_rightmost = _max - 1;\n+  _old_collector_leftmost = 0;\n@@ -474,0 +644,45 @@\n+  if (_heap->mode()->is_generational()) {\n+    size_t old_collector_middle = (_old_collector_leftmost + _old_collector_rightmost) \/ 2;\n+    size_t old_collector_available_in_first_half = 0;\n+    size_t old_collector_available_in_second_half = 0;\n+\n+    for (size_t index = _old_collector_leftmost; index < old_collector_middle; index++) {\n+      if (is_old_collector_free(index)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(index);\n+        old_collector_available_in_first_half += r->free();\n+      }\n+    }\n+    for (size_t index = old_collector_middle; index <= _old_collector_rightmost; index++) {\n+      if (is_old_collector_free(index)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(index);\n+        old_collector_available_in_second_half += r->free();\n+      }\n+    }\n+    _old_collector_search_left_to_right = (old_collector_available_in_second_half > old_collector_available_in_first_half);\n+  }\n+}\n+\n+bool ShenandoahFreeSet::expand_collector_bounds_maybe(size_t idx) {\n+  bool result = false;\n+  if (idx < _collector_leftmost) {\n+    _collector_leftmost = idx;\n+    result = true;\n+  }\n+  if (idx > _collector_rightmost) {\n+    _collector_rightmost = idx;\n+    result = true;\n+  }\n+  return result;\n+}\n+\n+bool ShenandoahFreeSet::expand_old_collector_bounds_maybe(size_t idx) {\n+  bool result = false;\n+  if (idx < _old_collector_leftmost) {\n+    _old_collector_leftmost = idx;\n+    result = true;\n+  }\n+  if (idx > _old_collector_rightmost) {\n+    _old_collector_rightmost = idx;\n+    result = true;\n+  }\n+  return result;\n@@ -491,0 +706,7 @@\n+  \/\/ Rewind both old collector bounds until the next bit.\n+  while (_old_collector_leftmost < _max && !is_old_collector_free(_old_collector_leftmost)) {\n+    _old_collector_leftmost++;\n+  }\n+  while (_old_collector_rightmost > 0 && !is_old_collector_free(_old_collector_rightmost)) {\n+    _old_collector_rightmost--;\n+  }\n@@ -581,2 +803,1 @@\n-\n-    _mutator_free_bitmap.clear_bit(r->index());\n+    clear_mutator_free(r->index());\n@@ -607,1 +828,4 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n@@ -611,1 +835,1 @@\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n+size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -620,1 +844,1 @@\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n+bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -645,1 +869,1 @@\n-void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n@@ -651,4 +875,3 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n+  clear_mutator_free(idx);\n+  set_old_collector_free(idx);\n+  bool result = expand_old_collector_bounds_maybe(idx);\n@@ -657,0 +880,2 @@\n+  adjust_mutator_bounds_if_touched(idx);\n+  assert_bounds();\n@@ -658,3 +883,17 @@\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n+  \/\/ We do not ensure that the region is no longer trash,\n+  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n+void ShenandoahFreeSet::flip_to_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  clear_mutator_free(idx);\n+  set_collector_free(idx);\n+  bool result = expand_collector_bounds_maybe(idx);\n+\n+  _capacity -= alloc_capacity(r);\n+  adjust_mutator_bounds_if_touched(idx);\n@@ -676,0 +915,1 @@\n+  _old_collector_free_bitmap.clear();\n@@ -680,0 +920,2 @@\n+  _old_collector_leftmost = _max;\n+  _old_collector_rightmost = 0;\n@@ -681,0 +923,1 @@\n+  _old_capacity = 0;\n@@ -691,0 +934,7 @@\n+\n+    \/\/ We move all young available regions into mutator_free set and then we take back the regions we need for our\n+    \/\/ reserve.  This allows us to \"compact\" the collector_free (survivor) regions at the high end of the heap.\n+    clear_mutator_free(idx);\n+    clear_collector_free(idx);\n+    clear_old_collector_free(idx);\n+\n@@ -692,1 +942,1 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -697,5 +947,7 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n-\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+      if (region->is_old()) {\n+        _old_capacity += alloc_capacity(region);\n+        assert(!is_old_collector_free(idx), \"We are about to add it, it shouldn't be there already\");\n+        set_old_collector_free(idx);\n+        log_debug(gc)(\"  Setting Region \" SIZE_FORMAT \" _old_collector_free_bitmap bit to true\", idx);\n+      } else {\n+        _capacity += alloc_capacity(region);\n@@ -703,1 +955,5 @@\n-      log_debug(gc, free)(\"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n+        assert(_used <= _capacity, \"must not use more than we have\");\n+        assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n+        set_mutator_free(idx);\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n@@ -705,1 +961,2 @@\n-               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      }\n@@ -710,0 +967,1 @@\n+  size_t young_reserve, old_reserve;\n@@ -711,2 +969,2 @@\n-    size_t to_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    reserve_regions(to_reserve);\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -714,1 +972,1 @@\n-    size_t young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+\n@@ -720,3 +978,10 @@\n-    size_t old_reserve = 0;\n-    size_t to_reserve = young_reserve + old_reserve;\n-    reserve_regions(to_reserve);\n+    if (_heap->has_evacuation_reserve_quantities()) {\n+      \/\/ We are rebuilding at the end of final mark, having established evacuation budgets for this GC pass.\n+      young_reserve = _heap->get_young_evac_reserve();\n+      old_reserve = _heap->get_promoted_reserve() + _heap->get_old_evac_reserve();\n+    } else {\n+      \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+      young_reserve = (_heap->young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      old_reserve = MAX2((_heap->old_generation()->max_capacity() * ShenandoahOldEvacReserve) \/ 100,\n+                         ShenandoahOldCompactionReserve * ShenandoahHeapRegion::region_size_bytes());\n+    }\n@@ -724,1 +989,1 @@\n-\n+  reserve_regions(young_reserve, old_reserve);\n@@ -727,0 +992,1 @@\n+  log_status();\n@@ -729,1 +995,1 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old) {\n@@ -731,1 +997,0 @@\n-\n@@ -733,12 +998,26 @@\n-    if (reserved >= to_reserve) break;\n-\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n-      log_debug(gc, free)(\"  Shifting Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to collector free set\",\n-                          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-                               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (_mutator_free_bitmap.at(idx) && (alloc_capacity(r) > 0)) {\n+      assert(!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+      \/\/ OLD regions that have available memory are already in the old_collector free set\n+      if ((_old_capacity < to_reserve_old) && (r->is_trash() || (r->affiliation() == ShenandoahAffiliation::FREE))) {\n+        clear_mutator_free(idx);\n+        set_old_collector_free(idx);\n+        size_t ac = alloc_capacity(r);\n+        _capacity -= ac;\n+        _old_capacity += ac;\n+        log_debug(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+      } else if (reserved < to_reserve) {\n+        \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+        \/\/ they were entirely empty.  I'm not sure I understand the rational for that.  That alternative behavior would\n+        \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+        \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n+        clear_mutator_free(idx);\n+        set_collector_free(idx);\n+        size_t ac = alloc_capacity(r);\n+        _capacity -= ac;\n+        reserved += ac;\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n+      } else {\n+        \/\/ We've satisfied both to_reserve and to_reserved_old\n+        break;\n+      }\n@@ -752,0 +1031,84 @@\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  {\n+#define BUFFER_SIZE 80\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_info(gc, ergo)(\"FreeSet map legend (see source for unexpected codes: *, $, !, #):\\n\"\n+                       \" m:mutator_free c:collector_free C:old_collector_free\"\n+                       \" h:humongous young H:humongous old ~:retired old _:retired young\");\n+    log_info(gc, ergo)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n+                       _mutator_leftmost, _mutator_rightmost, _collector_leftmost, _collector_rightmost,\n+                       _old_collector_leftmost, _old_collector_rightmost,\n+                       _old_collector_search_left_to_right? \"left to right\": \"right to left\");\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_info(gc, ergo)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (is_mutator_free(i) && is_collector_free(i) && is_old_collector_free(i)) {\n+        buffer[idx] = '*';\n+      } else if (is_mutator_free(i) && is_collector_free(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = '$';\n+      } else if (is_mutator_free(i) && is_old_collector_free(i)) {\n+        \/\/ Note that young regions may be in the old_collector_free set.\n+        buffer[idx] = '!';\n+      } else if (is_collector_free(i) && is_old_collector_free(i)) {\n+        buffer[idx] = '#';\n+      } else if (is_mutator_free(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in mutator_free set\");\n+        buffer[idx] = 'm';\n+      } else if (is_collector_free(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = 'c';\n+      } else if (is_old_collector_free(i)) {\n+        buffer[idx] = 'C';\n+      }\n+      else if (r->is_humongous()) {\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n+      } else {\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young += region_size_bytes;\n+        }\n+\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_info(gc, ergo)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+    size_t total_young = retired_young + retired_young_humongous;\n+    size_t total_old = retired_old + retired_old_humongous;\n+    log_info(gc, ergo)(\"Retired young: \" SIZE_FORMAT \"%s (including humongous: \" SIZE_FORMAT \"%s), old: \" SIZE_FORMAT\n+                       \"%s (including humongous: \" SIZE_FORMAT \"%s)\",\n+                       byte_size_in_proper_unit(total_young),             proper_unit_for_byte_size(total_young),\n+                       byte_size_in_proper_unit(retired_young_humongous), proper_unit_for_byte_size(retired_young_humongous),\n+                       byte_size_in_proper_unit(total_old),               proper_unit_for_byte_size(total_old),\n+                       byte_size_in_proper_unit(retired_old_humongous),   proper_unit_for_byte_size(retired_old_humongous));\n+  }\n+#endif\n+\n@@ -787,1 +1150,0 @@\n-\n@@ -796,0 +1158,4 @@\n+      assert(free == total_free, \"Sum of free within mutator regions (\" SIZE_FORMAT\n+             \") should match mutator capacity (\" SIZE_FORMAT \") minus mutator used (\" SIZE_FORMAT \")\",\n+             total_free, capacity(), used());\n+\n@@ -818,1 +1184,1 @@\n-      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT \" \",\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n@@ -836,0 +1202,10 @@\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n@@ -837,1 +1213,10 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\",\n+      for (size_t idx = _old_collector_leftmost; idx <= _old_collector_rightmost; idx++) {\n+        if (is_old_collector_free(idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n@@ -1018,0 +1403,11 @@\n+\n+  assert (_old_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _old_collector_leftmost,  _max);\n+  assert (_old_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _old_collector_rightmost, _max);\n+\n+  assert (_old_collector_leftmost == _max || is_old_collector_free(_old_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _old_collector_leftmost);\n+  assert (_old_collector_rightmost == 0   || is_old_collector_free(_old_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _old_collector_rightmost);\n+\n+  beg_off = _old_collector_free_bitmap.find_first_set_bit(0);\n+  end_off = _old_collector_free_bitmap.find_first_set_bit(_old_collector_rightmost + 1);\n+  assert (beg_off >= _old_collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _old_collector_leftmost);\n+  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _old_collector_rightmost);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":481,"deletions":85,"binary":false,"changes":566,"status":"modified"},{"patch":"@@ -37,0 +37,5 @@\n+  \/\/ We keep the _old_collector regions separate from the young collector regions.  This allows us to pack the old regions\n+  \/\/ further to the right than the young collector regions.  This is desirable because the old collector regions are recycled\n+  \/\/ even less frequently than the young survivor regions.  In generational mode, the young survivor regions are typically\n+  \/\/ recycled after tenure age GC passes.\n+  CHeapBitMap _old_collector_free_bitmap;\n@@ -43,0 +48,1 @@\n+  size_t _old_collector_leftmost, _old_collector_rightmost;\n@@ -45,0 +51,1 @@\n+  size_t _old_capacity;\n@@ -47,0 +54,6 @@\n+  \/\/ When is_old_collector_free regions sparsely populate the lower address ranges of the heap, we search from left to\n+  \/\/ right in order to consume (and remove from the is_old_collector_free range) these sparsely distributed regions.\n+  \/\/ This allows us to more quickly condense the range of addresses that represent old_collector_free regions.\n+  bool _old_collector_search_left_to_right = true;\n+\n+\n@@ -49,2 +62,16 @@\n-  bool is_mutator_free(size_t idx) const;\n-  bool is_collector_free(size_t idx) const;\n+  inline bool is_mutator_free(size_t idx) const;\n+  inline bool is_collector_free(size_t idx) const;\n+  inline bool is_old_collector_free(size_t idx) const;\n+\n+  \/\/ Routines that do not assert non-empty free, for use in assertions and during state transitions\n+  inline bool peek_is_mutator_free(size_t idx) const;\n+  inline bool peek_is_collector_free(size_t idx) const;\n+  inline bool peek_is_old_collector_free(size_t idx) const;\n+\n+  inline void set_mutator_free(size_t idx);\n+  inline void set_collector_free(size_t idx);\n+  inline void set_old_collector_free(size_t idx);\n+\n+  inline void clear_mutator_free(size_t idx);\n+  inline void clear_collector_free(size_t idx);\n+  inline void clear_old_collector_free(size_t idx);\n@@ -54,1 +81,1 @@\n-  HeapWord* allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region);\n+  HeapWord* allocate_old_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -66,0 +93,1 @@\n+  void flip_to_old_gc(ShenandoahHeapRegion* r);\n@@ -69,0 +97,3 @@\n+  bool adjust_mutator_bounds_if_touched(size_t idx);\n+  bool adjust_collector_bounds_if_touched(size_t idx);\n+  bool adjust_old_collector_bounds_if_touched(size_t idx);\n@@ -70,0 +101,2 @@\n+  bool expand_collector_bounds_maybe(size_t idx);\n+  bool expand_old_collector_bounds_maybe(size_t idx);\n@@ -71,1 +104,1 @@\n-  void increase_used(size_t amount);\n+  inline void increase_used(size_t amount);\n@@ -76,3 +109,3 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r);\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n-  bool has_no_alloc_capacity(ShenandoahHeapRegion *r);\n+  bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  bool has_no_alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -83,1 +116,1 @@\n-  \/\/ Number of regions dedicated to GC allocations (for evacuation or promotion) that are currently free\n+  \/\/ Number of regions dedicated to GC allocations (for evacuation) that are currently free\n@@ -86,0 +119,3 @@\n+  \/\/ Number of regions dedicated to Old GC allocations (for evacuation or promotion) that are currently free\n+  size_t old_collector_count() const { return _old_collector_free_bitmap.count_one_bits(); }\n+\n@@ -111,1 +147,1 @@\n-  void reserve_regions(size_t to_reserve);\n+  void reserve_regions(size_t young_reserve, size_t old_reserve);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":45,"deletions":9,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -1056,0 +1056,5 @@\n+\/\/ TODO:\n+\/\/  Consider compacting old-gen objects toward the high end of memory and young-gen objects towards the low-end\n+\/\/  of memory.  As currently implemented, all regions are compacted toward the low-end of memory.  This creates more\n+\/\/  fragmentation of the heap, because old-gen regions get scattered among low-address regions such that it becomes\n+\/\/  more difficult to find contiguous regions for humongous objects.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -834,0 +834,2 @@\n+  \/\/ Freeset construction uses reserve quantities if they are valid\n+  heap->set_evacuation_reserve_quantities(true);\n@@ -840,0 +842,1 @@\n+  heap->set_evacuation_reserve_quantities(false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2242,0 +2242,4 @@\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  set_gc_state_mask(VALID_EVACUATION_RESERVE_QUANTITIES, is_valid);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -320,1 +320,4 @@\n-    OLD_MARKING_BITPOS = 5\n+    OLD_MARKING_BITPOS = 5,\n+\n+    \/\/ The evacuation reserves for old-gen and young-gen are available\n+    VALID_EVACUATION_RESERVE_QUANTITIES_BITPOS = 6\n@@ -330,1 +333,2 @@\n-    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS,\n+    VALID_EVACUATION_RESERVE_QUANTITIES = 1 << VALID_EVACUATION_RESERVE_QUANTITIES_BITPOS\n@@ -388,0 +392,2 @@\n+  bool _has_evacuation_reserve_quantities;\n+\n@@ -394,0 +400,1 @@\n+\n@@ -397,0 +404,1 @@\n+  void set_evacuation_reserve_quantities(bool is_valid);\n@@ -412,0 +420,1 @@\n+  inline bool has_evacuation_reserve_quantities() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -664,0 +664,1 @@\n+\n@@ -668,0 +669,4 @@\n+inline bool ShenandoahHeap::has_evacuation_reserve_quantities() const {\n+  return _gc_state.is_set(VALID_EVACUATION_RESERVE_QUANTITIES);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"}]}