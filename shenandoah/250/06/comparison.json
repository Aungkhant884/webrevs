{"files":[{"patch":"@@ -1205,1 +1205,0 @@\n-  heap->rebuild_free_set(true \/*concurrent*\/);\n@@ -1207,0 +1206,1 @@\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+  _old_collector_free_bitmap(max_regions, mtGC),\n@@ -50,1 +51,1 @@\n-void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n+inline void ShenandoahFreeSet::increase_used(size_t num_bytes) {\n@@ -53,0 +54,3 @@\n+  assert(_used <= _capacity, \"must not use (\" SIZE_FORMAT \") more than we have (\" SIZE_FORMAT \") after increase by \" SIZE_FORMAT,\n+         _used, _capacity, num_bytes);\n+}\n@@ -54,2 +58,2 @@\n-  assert(_used <= _capacity, \"must not use more than we have: used: \" SIZE_FORMAT\n-         \", capacity: \" SIZE_FORMAT \", num_bytes: \" SIZE_FORMAT, _used, _capacity, num_bytes);\n+inline bool ShenandoahFreeSet::probe_mutator_set(size_t idx) const {\n+  return _mutator_free_bitmap.at(idx);\n@@ -58,1 +62,2 @@\n-bool ShenandoahFreeSet::is_mutator_free(size_t idx) const {\n+inline bool ShenandoahFreeSet::in_mutator_set(size_t idx) const {\n+  bool is_mutator_free = _mutator_free_bitmap.at(idx);\n@@ -61,1 +66,6 @@\n-  return _mutator_free_bitmap.at(idx);\n+  assert(!is_mutator_free || has_alloc_capacity(idx), \"Mutator free set should contain useful regions\");\n+  return is_mutator_free;\n+}\n+\n+inline bool ShenandoahFreeSet::probe_collector_set(size_t idx) const {\n+  return _collector_free_bitmap.at(idx);\n@@ -64,1 +74,2 @@\n-bool ShenandoahFreeSet::is_collector_free(size_t idx) const {\n+inline bool ShenandoahFreeSet::in_collector_set(size_t idx) const {\n+  bool is_collector_free = _collector_free_bitmap.at(idx);\n@@ -67,1 +78,2 @@\n-  return _collector_free_bitmap.at(idx);\n+  assert(!is_collector_free || has_alloc_capacity(idx), \"Collector free set should contain useful regions\");\n+  return is_collector_free;\n@@ -70,7 +82,3 @@\n-\/\/ This is a temporary solution to work around a shortcoming with the existing free set implementation.\n-\/\/ TODO:\n-\/\/   Remove this function after restructing FreeSet representation.  A problem in the existing implementation is that old-gen\n-\/\/   regions are not considered to reside within the is_collector_free range.\n-\/\/\n-HeapWord* ShenandoahFreeSet::allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahAffiliation affiliation = ShenandoahAffiliation::OLD_GENERATION;\n+inline bool ShenandoahFreeSet::probe_old_collector_set(size_t idx) const {\n+  return _old_collector_free_bitmap.at(idx);\n+}\n@@ -78,2 +86,7 @@\n-  size_t rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n-  size_t leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+inline bool ShenandoahFreeSet::in_old_collector_set(size_t idx) const {\n+  bool is_old_collector_free = _old_collector_free_bitmap.at(idx);\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n+          idx, _max, _old_collector_leftmost, _old_collector_rightmost);\n+  assert(!is_old_collector_free || has_alloc_capacity(idx), \"Old collector free set should contain useful regions\");\n+  return is_old_collector_free;\n+}\n@@ -81,9 +94,63 @@\n-  for (size_t c = rightmost + 1; c > leftmost; c--) {\n-    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-    size_t idx = c - 1;\n-    ShenandoahHeapRegion* r = _heap->get_region(idx);\n-    if (r->affiliation() == affiliation && !r->is_humongous()) {\n-      if (!r->is_cset() && !has_no_alloc_capacity(r)) {\n-        HeapWord* result = try_allocate_in(r, req, in_new_region);\n-        if (result != nullptr) {\n-          return result;\n+inline void ShenandoahFreeSet::add_to_mutator_set(size_t idx) {\n+  assert(has_alloc_capacity(idx), \"Mutator free set should contain useful regions\");\n+  assert(!in_old_collector_set(idx) && !in_collector_set(idx), \"Freeset membership is mutually exclusive\");\n+  _mutator_free_bitmap.set_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::remove_from_mutator_set(size_t idx) {\n+  _mutator_free_bitmap.clear_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::add_to_collector_set(size_t idx) {\n+  assert(has_alloc_capacity(idx), \"Collector free set should contain useful regions\");\n+  assert(!in_mutator_set(idx) && !in_old_collector_set(idx), \"Freeset membership is mutually exclusive\");\n+  _collector_free_bitmap.set_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::remove_from_collector_set(size_t idx) {\n+  _collector_free_bitmap.clear_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::add_to_old_collector_set(size_t idx) {\n+  assert(has_alloc_capacity(idx), \"Old collector free set should contain useful regions\");\n+  assert(!in_mutator_set(idx) && !in_collector_set(idx), \"Freeset membership is mutually exclusive\");\n+  _old_collector_free_bitmap.set_bit(idx);\n+}\n+\n+inline void ShenandoahFreeSet::remove_from_old_collector_set(size_t idx) {\n+  _old_collector_free_bitmap.clear_bit(idx);\n+}\n+\n+\n+HeapWord* ShenandoahFreeSet::allocate_old_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                           ShenandoahAllocRequest& req, bool& in_new_region) {\n+  shenandoah_assert_heaplocked();\n+  size_t rightmost = _old_collector_rightmost;\n+  size_t leftmost = _old_collector_leftmost;\n+  if (_old_collector_search_left_to_right) {\n+    \/\/ This mode picks up stragglers left by a full GC\n+    for (size_t c = leftmost; c <= rightmost; c++) {\n+      if (in_old_collector_set(c)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(c);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  } else {\n+    \/\/ This mode picks up stragglers left by a previous concurrent GC\n+    for (size_t c = rightmost + 1; c > leftmost; c--) {\n+      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      size_t idx = c - 1;\n+      if (in_old_collector_set(idx)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || !r->is_affiliated() || r->is_old(), \"old_collector_set region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+          if (result != nullptr) {\n+            return result;\n+          }\n@@ -98,0 +165,1 @@\n+  shenandoah_assert_heaplocked();\n@@ -101,1 +169,1 @@\n-    if (is_collector_free(idx)) {\n+    if (in_collector_set(idx)) {\n@@ -116,0 +184,2 @@\n+  shenandoah_assert_heaplocked();\n+\n@@ -162,1 +232,1 @@\n-        if (is_mutator_free(idx) && (allow_new_region || r->is_affiliated())) {\n+        if (in_mutator_set(idx) && (allow_new_region || r->is_affiliated())) {\n@@ -186,1 +256,1 @@\n-          if (is_collector_free(idx)) {\n+          if (in_collector_set(idx)) {\n@@ -197,3 +267,1 @@\n-          \/\/ TODO: this is a work around to address a deficiency in FreeSet representation.  A better solution fixes\n-          \/\/ the FreeSet implementation to deal more efficiently with old-gen regions as being in the \"collector free set\"\n-          result = allocate_with_old_affiliation(req, in_new_region);\n+          result = allocate_old_with_affiliation(req.affiliation(), req, in_new_region);\n@@ -208,1 +276,5 @@\n-          result = allocate_with_affiliation(FREE, req, in_new_region);\n+          if (req.is_old()) {\n+            result = allocate_old_with_affiliation(FREE, req, in_new_region);\n+          } else {\n+            result = allocate_with_affiliation(FREE, req, in_new_region);\n+          }\n@@ -220,0 +292,13 @@\n+      \/\/ TODO:\n+      \/\/ if (!allow_new_region && req.is_old() && (young_generation->adjusted_unaffiliated_regions() > 0)) {\n+      \/\/   transfer a region from young to old;\n+      \/\/   allow_new_region = true;\n+      \/\/   heap->set_old_evac_reserve(heap->get_old_evac_reserve() + region_size_bytes);\n+      \/\/ }\n+      \/\/\n+      \/\/ We should expand old-gen if this can prevent an old-gen evacuation failure.  We don't care so much about\n+      \/\/ promotion failures since they can be mitigated in a subsequent GC pass.  Would be nice to know if this\n+      \/\/ allocation request is for evacuation or promotion.  Individual threads limit their use of PLAB memory for\n+      \/\/ promotions, so we already have an assurance that any additional memory set aside for old-gen will be used\n+      \/\/ only for old-gen evacuations.\n+\n@@ -224,1 +309,1 @@\n-          if (is_mutator_free(idx)) {\n+          if (in_mutator_set(idx)) {\n@@ -227,1 +312,5 @@\n-              flip_to_gc(r);\n+              if (req.is_old()) {\n+                flip_to_old_gc(r);\n+              } else {\n+                flip_to_gc(r);\n+              }\n@@ -250,2 +339,1 @@\n-  assert (!has_no_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n-\n+  assert (has_alloc_capacity(r), \"Performance: should avoid full regions on this path: \" SIZE_FORMAT, r->index());\n@@ -337,2 +425,1 @@\n-          increase_used(padding);\n-          assert(r->is_old(), \"All PLABs reside in old-gen\");\n+           assert(r->is_old(), \"All PLABs reside in old-gen\");\n@@ -389,1 +476,0 @@\n-        increase_used(padding);\n@@ -440,0 +526,1 @@\n+    size_t idx = r->index();\n@@ -447,8 +534,37 @@\n-    }\n-\n-    size_t num = r->index();\n-    _collector_free_bitmap.clear_bit(num);\n-    _mutator_free_bitmap.clear_bit(num);\n-    \/\/ Touched the bounds? Need to update:\n-    if (touches_bounds(num)) {\n-      adjust_bounds();\n+      assert(probe_mutator_set(idx), \"Must be mutator free: \" SIZE_FORMAT, idx);\n+      remove_from_mutator_set(idx);\n+      assert(!in_collector_set(idx) && !in_old_collector_set(idx), \"Region cannot be in multiple free sets\");\n+      adjust_mutator_bounds_if_touched(idx);\n+    } else if (r->free() < PLAB::min_size() * HeapWordSize) {\n+      \/\/ Permanently retire this region if there's room for a fill object\n+      size_t waste = r->free();\n+      HeapWord* fill_addr = r->top();\n+      size_t fill_size = waste \/ HeapWordSize;\n+      if (fill_size >= ShenandoahHeap::min_fill_size()) {\n+        ShenandoahHeap::fill_with_object(fill_addr, fill_size);\n+        r->set_top(r->end());\n+        \/\/ Since we have filled the waste with an empty object, account for increased usage\n+        _heap->increase_used(waste);\n+      } else {\n+        \/\/ We'll retire the region until the freeset is rebuilt. Since retiement is not permanent, we do not account for waste.\n+        waste = 0;\n+      }\n+      if (probe_old_collector_set(idx)) {\n+        assert(_heap->mode()->is_generational(), \"Old collector free regions only present in generational mode\");\n+        if (waste > 0) {\n+          _heap->old_generation()->increase_used(waste);\n+          _heap->card_scan()->register_object(fill_addr);\n+        }\n+        remove_from_old_collector_set(idx);\n+        assert(!in_collector_set(idx) && !in_mutator_set(idx), \"Region cannot be in multiple free sets\");\n+        adjust_old_collector_bounds_if_touched(idx);\n+      } else {\n+        assert(probe_collector_set(idx), \"Region that is not mutator free must be collector free or old collector free\");\n+        if ((waste > 0) && _heap->mode()->is_generational()) {\n+          _heap->young_generation()->increase_used(waste);\n+        }\n+        \/\/ This applies to both generational and non-generational mode\n+        remove_from_collector_set(idx);\n+        assert(!in_mutator_set(idx) && !in_old_collector_set(idx), \"Region cannot be in multiple free sets\");\n+        adjust_collector_bounds_if_touched(idx);\n+      }\n@@ -461,0 +577,48 @@\n+\/\/ If idx represents a mutator bound, recompute the mutator bounds, returning true iff bounds were adjusted.\n+bool ShenandoahFreeSet::adjust_mutator_bounds_if_touched(size_t idx) {\n+  if (idx == _mutator_leftmost || idx == _mutator_rightmost) {\n+    \/\/ Rewind both mutator bounds until the next bit.\n+    while (_mutator_leftmost < _max && !in_mutator_set(_mutator_leftmost)) {\n+      _mutator_leftmost++;\n+    }\n+    while (_mutator_rightmost > 0 && !in_mutator_set(_mutator_rightmost)) {\n+      _mutator_rightmost--;\n+    }\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ If idx represents an old collector bound, recompute the old collector bounds, returning true iff bounds were adjusted.\n+bool ShenandoahFreeSet::adjust_old_collector_bounds_if_touched(size_t idx) {\n+  if (idx == _old_collector_leftmost || idx == _old_collector_rightmost) {\n+    \/\/ Rewind both old collector bounds until the next bit.\n+    while (_old_collector_leftmost < _max && !in_old_collector_set(_old_collector_leftmost)) {\n+      _old_collector_leftmost++;\n+    }\n+    while (_old_collector_rightmost > 0 && !in_old_collector_set(_old_collector_rightmost)) {\n+      _old_collector_rightmost--;\n+    }\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n+\/\/ If idx represents a collector bound, recompute the collector bounds, returning true iff bounds were adjusted.\n+bool ShenandoahFreeSet::adjust_collector_bounds_if_touched(size_t idx) {\n+  if (idx == _collector_leftmost || idx == _collector_rightmost) {\n+    \/\/ Rewind both old collector bounds until the next bit.\n+    while (_collector_leftmost < _max && !in_collector_set(_collector_leftmost)) {\n+      _collector_leftmost++;\n+    }\n+    while (_collector_rightmost > 0 && !in_collector_set(_collector_rightmost)) {\n+      _collector_rightmost--;\n+    }\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n@@ -462,1 +626,3 @@\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n+  return (num == _collector_leftmost || num == _collector_rightmost ||\n+          num == _old_collector_leftmost || num == _old_collector_rightmost ||\n+          num == _mutator_leftmost || num == _mutator_rightmost);\n@@ -471,0 +637,2 @@\n+  _old_collector_rightmost = _max - 1;\n+  _old_collector_leftmost = 0;\n@@ -474,0 +642,44 @@\n+  if (_heap->mode()->is_generational()) {\n+    size_t old_collector_middle = (_old_collector_leftmost + _old_collector_rightmost) \/ 2;\n+    size_t old_collector_available_in_first_half = 0;\n+    size_t old_collector_available_in_second_half = 0;\n+\n+    for (size_t index = _old_collector_leftmost; index < old_collector_middle; index++) {\n+      if (in_old_collector_set(index)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(index);\n+        old_collector_available_in_first_half += r->free();\n+      }\n+    }\n+    for (size_t index = old_collector_middle; index <= _old_collector_rightmost; index++) {\n+      if (in_old_collector_set(index)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(index);\n+        old_collector_available_in_second_half += r->free();\n+      }\n+    }\n+    \/\/ We desire to first consume the sparsely distributed old-collector regions in order that the remaining old-collector\n+    \/\/ regions are densely packed.  Densely packing old-collector regions reduces the effort to search for a region that\n+    \/\/ has sufficient memory to satisfy a new allocation request.  Old-collector regions become sparsely distributed following\n+    \/\/ a Full GC, which tends to slide old-gen regions to the front of the heap rather than allowing them to remain\n+    \/\/ at the end of the heap where we intend for them to congregate.  In the future, we may modify Full GC so that it\n+    \/\/ slides old objects to the end of the heap and young objects to the start of the heap. If this is done, we can\n+    \/\/ always search right to left.\n+    _old_collector_search_left_to_right = (old_collector_available_in_second_half > old_collector_available_in_first_half);\n+  }\n+}\n+\n+void ShenandoahFreeSet::expand_collector_bounds_maybe(size_t idx) {\n+  if (idx < _collector_leftmost) {\n+    _collector_leftmost = idx;\n+  }\n+  if (idx > _collector_rightmost) {\n+    _collector_rightmost = idx;\n+  }\n+}\n+\n+void ShenandoahFreeSet::expand_old_collector_bounds_maybe(size_t idx) {\n+  if (idx < _old_collector_leftmost) {\n+    _old_collector_leftmost = idx;\n+  }\n+  if (idx > _old_collector_rightmost) {\n+    _old_collector_rightmost = idx;\n+  }\n@@ -478,1 +690,1 @@\n-  while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n+  while (_mutator_leftmost < _max && !in_mutator_set(_mutator_leftmost)) {\n@@ -481,1 +693,1 @@\n-  while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n+  while (_mutator_rightmost > 0 && !in_mutator_set(_mutator_rightmost)) {\n@@ -485,1 +697,1 @@\n-  while (_collector_leftmost < _max && !is_collector_free(_collector_leftmost)) {\n+  while (_collector_leftmost < _max && !in_collector_set(_collector_leftmost)) {\n@@ -488,1 +700,1 @@\n-  while (_collector_rightmost > 0 && !is_collector_free(_collector_rightmost)) {\n+  while (_collector_rightmost > 0 && !in_collector_set(_collector_rightmost)) {\n@@ -491,0 +703,7 @@\n+  \/\/ Rewind both old collector bounds until the next bit.\n+  while (_old_collector_leftmost < _max && !in_old_collector_set(_old_collector_leftmost)) {\n+    _old_collector_leftmost++;\n+  }\n+  while (_old_collector_rightmost > 0 && !in_old_collector_set(_old_collector_rightmost)) {\n+    _old_collector_rightmost--;\n+  }\n@@ -528,1 +747,1 @@\n-    if (!is_mutator_free(end) || !can_allocate_from(_heap->get_region(end))) {\n+    if (!in_mutator_set(end) || !can_allocate_from(_heap->get_region(end))) {\n@@ -581,2 +800,1 @@\n-\n-    _mutator_free_bitmap.clear_bit(r->index());\n+    remove_from_mutator_set(r->index());\n@@ -607,1 +825,4 @@\n-bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) {\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n+bool ShenandoahFreeSet::can_allocate_from(ShenandoahHeapRegion *r) const {\n@@ -611,1 +832,1 @@\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n+size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -620,1 +841,10 @@\n-bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) {\n+bool ShenandoahFreeSet::has_alloc_capacity(ShenandoahHeapRegion *r) const {\n+  return alloc_capacity(r) > 0;\n+}\n+\n+bool ShenandoahFreeSet::has_alloc_capacity(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return alloc_capacity(r) > 0;\n+}\n+\n+bool ShenandoahFreeSet::has_no_alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -645,0 +875,21 @@\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+  remove_from_mutator_set(idx);\n+  add_to_old_collector_set(idx);\n+  expand_old_collector_bounds_maybe(idx);\n+\n+  size_t region_capacity = alloc_capacity(r);\n+  _capacity -= region_capacity;\n+  _old_capacity += region_capacity;\n+  adjust_mutator_bounds_if_touched(idx);\n+  assert_bounds();\n+\n+  \/\/ We do not ensure that the region is no longer trash,\n+  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n@@ -651,4 +902,3 @@\n-  _mutator_free_bitmap.clear_bit(idx);\n-  _collector_free_bitmap.set_bit(idx);\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n+  remove_from_mutator_set(idx);\n+  add_to_collector_set(idx);\n+  expand_collector_bounds_maybe(idx);\n@@ -657,4 +907,1 @@\n-\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n+  adjust_mutator_bounds_if_touched(idx);\n@@ -676,0 +923,1 @@\n+  _old_collector_free_bitmap.clear();\n@@ -680,0 +928,2 @@\n+  _old_collector_leftmost = _max;\n+  _old_collector_rightmost = 0;\n@@ -681,0 +931,1 @@\n+  _old_capacity = 0;\n@@ -684,3 +935,6 @@\n-void ShenandoahFreeSet::rebuild() {\n-  shenandoah_assert_heaplocked();\n-  clear();\n+\/\/ This function places all is_old() regions that have allocation capacity into the old_collector set.  It places\n+\/\/ all other regions (not is_old()) that have allocation capacity into the mutator_set.  Subsequently, we will\n+\/\/ move some of the mutator regions into the collector set or old_collector set with the intent of packing\n+\/\/ old_collector memory into the highest (rightmost) addresses of the heap and the collector memory into the\n+\/\/ next highest addresses of the heap, with mutator memory consuming the lowest addresses of the heap.\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity() {\n@@ -688,1 +942,0 @@\n-  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -692,1 +945,1 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -697,5 +950,8 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n-\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n+      if (region->is_old()) {\n+        _old_capacity += alloc_capacity(region);\n+        assert(!in_old_collector_set(idx), \"We are about to add it, it shouldn't be there already\");\n+        add_to_old_collector_set(idx);\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT  \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to old collector set\",\n+          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n@@ -703,1 +959,6 @@\n-      log_debug(gc, free)(\"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n+      } else {\n+        _capacity += alloc_capacity(region);\n+        assert(!in_mutator_set(idx), \"We are about to add it, it shouldn't be there already\");\n+        add_to_mutator_set(idx);\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator set\",\n@@ -705,1 +966,2 @@\n-               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      }\n@@ -708,0 +970,8 @@\n+}\n+\n+void ShenandoahFreeSet::rebuild() {\n+  shenandoah_assert_heaplocked();\n+  \/\/ This resets all state information, removing all regions from all sets.\n+  clear();\n+\n+  log_debug(gc, free)(\"Rebuilding FreeSet\");\n@@ -709,1 +979,7 @@\n-  \/\/ Evac reserve: reserve trailing space for evacuations\n+  \/\/ This places regions that have alloc_capacity into the old_collector set if they identify as is_old() or the\n+  \/\/ mutator set otherwise.\n+  find_regions_with_alloc_capacity();\n+\n+  \/\/ Evac reserve: reserve trailing space for evacuations, with regions reserved for old evacuations placed to the right\n+  \/\/ of regions reserved of young evacuations.\n+  size_t young_reserve, old_reserve;\n@@ -711,2 +987,2 @@\n-    size_t to_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    reserve_regions(to_reserve);\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -714,2 +990,1 @@\n-    size_t young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    \/\/ Note that all allocations performed from old-gen are performed by GC, generally using PLABs for both\n+    \/\/ All allocations taken from the old collector set are performed by GC, generally using PLABs for both\n@@ -717,6 +992,12 @@\n-    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons within\n-    \/\/ each PLAB.  We do not reserve any of old-gen memory in order to facilitate the loaning of old-gen memory\n-    \/\/ to young-gen purposes.\n-    size_t old_reserve = 0;\n-    size_t to_reserve = young_reserve + old_reserve;\n-    reserve_regions(to_reserve);\n+    \/\/ which is reserved for promotion is enforced using thread-local variables that prescribe intentons for\n+    \/\/ each PLAB's available memory.\n+    if (_heap->has_evacuation_reserve_quantities()) {\n+      \/\/ We are rebuilding at the end of final mark, having already established evacuation budgets for this GC pass.\n+      young_reserve = _heap->get_young_evac_reserve();\n+      old_reserve = _heap->get_promoted_reserve() + _heap->get_old_evac_reserve();\n+    } else {\n+      \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+      young_reserve = (_heap->young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      old_reserve = MAX2((_heap->old_generation()->max_capacity() * ShenandoahOldEvacReserve) \/ 100,\n+                         ShenandoahOldCompactionReserve * ShenandoahHeapRegion::region_size_bytes());\n+    }\n@@ -724,1 +1005,1 @@\n-\n+  reserve_regions(young_reserve, old_reserve);\n@@ -727,0 +1008,1 @@\n+  log_status();\n@@ -729,1 +1011,6 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+\/\/ Having placed all regions that have allocation capacity into the mutator set if they identify as is_young()\n+\/\/ or into the old collector set if they identify as is_old(), move some of these regions from the mutator set\n+\/\/ into the collector set or old collector set in order to assure that the memory available for allocations within\n+\/\/ the collector set is at least to_reserve, and the memory available for allocations within the old collector set\n+\/\/ is at least to_reserve_old.\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old) {\n@@ -731,1 +1018,0 @@\n-\n@@ -733,12 +1019,26 @@\n-    if (reserved >= to_reserve) break;\n-\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n-      log_debug(gc, free)(\"  Shifting Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to collector free set\",\n-                          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-                               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (_mutator_free_bitmap.at(idx) && (alloc_capacity(r) > 0)) {\n+      assert(!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+      \/\/ OLD regions that have available memory are already in the old_collector free set\n+      if ((_old_capacity < to_reserve_old) && (r->is_trash() || !r->is_affiliated())) {\n+        remove_from_mutator_set(idx);\n+        add_to_old_collector_set(idx);\n+        size_t ac = alloc_capacity(r);\n+        _capacity -= ac;\n+        _old_capacity += ac;\n+        log_debug(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+      } else if (reserved < to_reserve) {\n+        \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+        \/\/ they were entirely empty.  I'm not sure I understand the rational for that.  That alternative behavior would\n+        \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+        \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n+        remove_from_mutator_set(idx);\n+        add_to_collector_set(idx);\n+        size_t ac = alloc_capacity(r);\n+        _capacity -= ac;\n+        reserved += ac;\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n+      } else {\n+        \/\/ We've satisfied both to_reserve and to_reserved_old\n+        break;\n+      }\n@@ -752,0 +1052,83 @@\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  {\n+#define BUFFER_SIZE 80\n+    size_t retired_old = 0;\n+    size_t retired_old_humongous = 0;\n+    size_t retired_young = 0;\n+    size_t retired_young_humongous = 0;\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_info(gc, free)(\"FreeSet map legend (see source for unexpected codes: *, $, !, #):\\n\"\n+                       \" m:mutator_free c:collector_free C:old_collector_free\"\n+                       \" h:humongous young H:humongous old ~:retired old _:retired young\");\n+    log_info(gc, free)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"] allocates from %s\",\n+                       _mutator_leftmost, _mutator_rightmost, _collector_leftmost, _collector_rightmost,\n+                       _old_collector_leftmost, _old_collector_rightmost,\n+                       _old_collector_search_left_to_right? \"left to right\": \"right to left\");\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_info(gc, free)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (in_mutator_set(i) && in_collector_set(i) && in_old_collector_set(i)) {\n+        buffer[idx] = '*';\n+      } else if (in_mutator_set(i) && in_collector_set(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = '$';\n+      } else if (in_mutator_set(i) && in_old_collector_set(i)) {\n+        \/\/ Note that young regions may be in the old_collector_free set.\n+        buffer[idx] = '!';\n+      } else if (in_collector_set(i) && in_old_collector_set(i)) {\n+        buffer[idx] = '#';\n+      } else if (in_mutator_set(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in mutator_free set\");\n+        buffer[idx] = 'm';\n+      } else if (in_collector_set(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = 'c';\n+      } else if (in_old_collector_set(i)) {\n+        buffer[idx] = 'C';\n+      } else if (r->is_humongous()) {\n+        if (r->is_old()) {\n+          buffer[idx] = 'H';\n+          retired_old_humongous += region_size_bytes;\n+        } else {\n+          buffer[idx] = 'h';\n+          retired_young_humongous += region_size_bytes;\n+        }\n+      } else {\n+        if (r->is_old()) {\n+          buffer[idx] = '~';\n+          retired_old += region_size_bytes;\n+        } else {\n+          buffer[idx] = '_';\n+          retired_young += region_size_bytes;\n+        }\n+\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_info(gc, free)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+    size_t total_young = retired_young + retired_young_humongous;\n+    size_t total_old = retired_old + retired_old_humongous;\n+    log_info(gc, free)(\"Retired young: \" SIZE_FORMAT \"%s (including humongous: \" SIZE_FORMAT \"%s), old: \" SIZE_FORMAT\n+                       \"%s (including humongous: \" SIZE_FORMAT \"%s)\",\n+                       byte_size_in_proper_unit(total_young),             proper_unit_for_byte_size(total_young),\n+                       byte_size_in_proper_unit(retired_young_humongous), proper_unit_for_byte_size(retired_young_humongous),\n+                       byte_size_in_proper_unit(total_old),               proper_unit_for_byte_size(total_old),\n+                       byte_size_in_proper_unit(retired_old_humongous),   proper_unit_for_byte_size(retired_old_humongous));\n+  }\n+#endif\n+\n@@ -768,1 +1151,1 @@\n-        if (is_mutator_free(idx)) {\n+        if (in_mutator_set(idx)) {\n@@ -787,1 +1170,0 @@\n-\n@@ -796,0 +1178,4 @@\n+      assert(free == total_free, \"Sum of free within mutator regions (\" SIZE_FORMAT\n+             \") should match mutator capacity (\" SIZE_FORMAT \") minus mutator used (\" SIZE_FORMAT \")\",\n+             total_free, capacity(), used());\n+\n@@ -818,1 +1204,1 @@\n-      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT \" \",\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n@@ -828,1 +1214,1 @@\n-        if (is_collector_free(idx)) {\n+        if (in_collector_set(idx)) {\n@@ -836,0 +1222,10 @@\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n@@ -837,1 +1233,10 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\",\n+      for (size_t idx = _old_collector_leftmost; idx <= _old_collector_rightmost; idx++) {\n+        if (in_old_collector_set(idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n@@ -876,1 +1281,1 @@\n-    if (index < _max && is_mutator_free(index)) {\n+    if (index < _max && in_mutator_set(index)) {\n@@ -891,1 +1296,1 @@\n-    if (is_mutator_free(index)) {\n+    if (in_mutator_set(index)) {\n@@ -897,1 +1302,1 @@\n-    if (is_collector_free(index)) {\n+    if (in_collector_set(index)) {\n@@ -930,1 +1335,1 @@\n-    if (is_mutator_free(index)) {\n+    if (in_mutator_set(index)) {\n@@ -968,1 +1373,1 @@\n-    if (is_mutator_free(index)) {\n+    if (in_mutator_set(index)) {\n@@ -1000,2 +1405,2 @@\n-  assert (_mutator_leftmost == _max || is_mutator_free(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n-  assert (_mutator_rightmost == 0   || is_mutator_free(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n+  assert (_mutator_leftmost == _max || in_mutator_set(_mutator_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _mutator_leftmost);\n+  assert (_mutator_rightmost == 0   || in_mutator_set(_mutator_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _mutator_rightmost);\n@@ -1011,2 +1416,2 @@\n-  assert (_collector_leftmost == _max || is_collector_free(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n-  assert (_collector_rightmost == 0   || is_collector_free(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n+  assert (_collector_leftmost == _max || in_collector_set(_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _collector_leftmost);\n+  assert (_collector_rightmost == 0   || in_collector_set(_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _collector_rightmost);\n@@ -1018,0 +1423,11 @@\n+\n+  assert (_old_collector_leftmost <= _max, \"leftmost in bounds: \"  SIZE_FORMAT \" < \" SIZE_FORMAT, _old_collector_leftmost,  _max);\n+  assert (_old_collector_rightmost < _max, \"rightmost in bounds: \" SIZE_FORMAT \" < \" SIZE_FORMAT, _old_collector_rightmost, _max);\n+\n+  assert (_old_collector_leftmost == _max || in_old_collector_set(_old_collector_leftmost),  \"leftmost region should be free: \" SIZE_FORMAT,  _old_collector_leftmost);\n+  assert (_old_collector_rightmost == 0   || in_old_collector_set(_old_collector_rightmost), \"rightmost region should be free: \" SIZE_FORMAT, _old_collector_rightmost);\n+\n+  beg_off = _old_collector_free_bitmap.find_first_set_bit(0);\n+  end_off = _old_collector_free_bitmap.find_first_set_bit(_old_collector_rightmost + 1);\n+  assert (beg_off >= _old_collector_leftmost, \"free regions before the leftmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT, beg_off, _old_collector_leftmost);\n+  assert (end_off == _max,      \"free regions past the rightmost: \" SIZE_FORMAT \", bound \" SIZE_FORMAT,  end_off, _old_collector_rightmost);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":534,"deletions":118,"binary":false,"changes":652,"status":"modified"},{"patch":"@@ -36,0 +36,7 @@\n+\n+  \/\/ The _collector_free regions hold survivor objects within young-generation and within traditional single-generation\n+  \/\/ collections.  In general, the _collector_free regions are at the high end of memory and mutator-free regions are at\n+  \/\/ the low-end of memory.  In generational mode, the young survivor regions are typically recycled after the region reaches\n+  \/\/ tenure age.  In the case that a young survivor region reaches tenure age and has sufficiently low amount of garbage,\n+  \/\/ the region will be promoted in place.  This means the region will simply be relabled as an old-generation region and\n+  \/\/ will not be evacuated until an old-generation collection chooses to do so.\n@@ -37,0 +44,5 @@\n+\n+  \/\/ We keep the _old_collector regions separate from the young collector regions.  This allows us to pack the old regions\n+  \/\/ further to the right than the young collector regions.  This is desirable because the old collector regions are recycled\n+  \/\/ even less frequently than the young survivor regions.\n+  CHeapBitMap _old_collector_free_bitmap;\n@@ -39,2 +51,3 @@\n-  \/\/ Left-most and right-most region indexes. There are no free regions outside\n-  \/\/ of [left-most; right-most] index intervals\n+  \/\/ Left-most and right-most region indexes. There are no free regions outside of [left-most; right-most] index intervals.\n+  \/\/ The sets are not necessarily contiguous.  It is common for collector_is_free regions to reside within the mutator_is_free\n+  \/\/ range, and for _old_collector_is_free regions to reside within the collector_is_free range.\n@@ -43,0 +56,1 @@\n+  size_t _old_collector_leftmost, _old_collector_rightmost;\n@@ -44,0 +58,2 @@\n+  \/\/ _capacity represents the amount of memory that can be allocated within the mutator set at the time of the\n+  \/\/ most recent rebuild, as adjusted for the flipping of regions from mutator set to collector set or old collector set.\n@@ -45,0 +61,4 @@\n+\n+  \/\/ _used represents the amount of memory allocated within the mutator set since the time of the most recent rebuild.\n+  \/\/ _used feeds into certain ShenandoanPacing decisions.  There is no need to track of the memory consumed from\n+  \/\/ within the collector and old_collector sets.\n@@ -47,0 +67,26 @@\n+  \/\/ _old_capacity represents the amount of memory that can be allocated within the old collector set at the time\n+  \/\/ of the most recent rebuild, as adjusted for the flipping of regions from mutator set to old collector set.\n+  size_t _old_capacity;\n+\n+  \/\/ There is no need to compute young collector capacity.  And there is not need to consult _old_capacity once we\n+  \/\/ have successfully reserved the evacuation (old_collector and collector sets) requested at rebuild time.\n+  \/\/ TODO: A cleaner abstraction might encapsulate capacity (and used) information within a refactored set abstraction.\n+\n+\n+  \/\/ When old_collector_set regions sparsely populate the lower address ranges of the heap, we search from left to\n+  \/\/ right in order to consume (and remove from the old_collector set range) these sparsely distributed regions.\n+  \/\/ This allows us to more quickly condense the range of addresses that represent old_collector_free regions.\n+  bool _old_collector_search_left_to_right = true;\n+\n+  \/\/ Assure leftmost and rightmost bounds are valid for the mutator_is_free, collector_is_free, and old_collector_is_free sets.\n+  \/\/ valid bounds honor all of the following (where max is the number of heap regions):\n+  \/\/   if the set is empty, leftmost equals max and rightmost equals 0\n+  \/\/   Otherwise (the set is not empty):\n+  \/\/     0 <= leftmost < max and 0 <= rightmost < max\n+  \/\/     the region at leftmost is in the set\n+  \/\/     the region at rightmost is in the set\n+  \/\/     rightmost >= leftmost\n+  \/\/     for every idx that is in the set {\n+  \/\/       idx >= leftmost &&\n+  \/\/       idx <= rightmost\n+  \/\/     }\n@@ -49,2 +95,28 @@\n-  bool is_mutator_free(size_t idx) const;\n-  bool is_collector_free(size_t idx) const;\n+  \/\/ Every region is in exactly one of four sets: mutator_free, collector_free, old_collector_free, not_free.\n+  \/\/ Insofar as the free-set abstraction is concerned, we are only interested in regions that are free so we provide no\n+  \/\/ mechanism to directly inquire as to whether a region is not_free.  not_free membership is implied by not member of\n+  \/\/ mutator_free, collector_free and old_collector_free sets.\n+  \/\/\n+  \/\/ in_xx_set() implies that the region has allocation capacity (i.e. is not yet fully allocated).  Assertions enforce\n+  \/\/ that in_xx_set(idx) implies has_alloc_capacity(idx).\n+  \/\/\n+  \/\/ TODO: a future implementation may replace the three bitmaps with a single array of enums to simplify the representation\n+  \/\/ of membership within these four mutually exclusive sets.\n+  inline bool in_mutator_set(size_t idx) const;\n+  inline bool in_collector_set(size_t idx) const;\n+  inline bool in_old_collector_set(size_t idx) const;\n+\n+  \/\/ The following three probe routines mimic the behavior is in_mutator_set(), in_collector_set() and in_old_collector_set()\n+  \/\/ but do not assert that the regions have allocation capacity.  These probe routines are used in assertions enforced\n+  \/\/ during certain state transitions.\n+  inline bool probe_mutator_set(size_t idx) const;\n+  inline bool probe_collector_set(size_t idx) const;\n+  inline bool probe_old_collector_set(size_t idx) const;\n+\n+  inline void add_to_mutator_set(size_t idx);\n+  inline void add_to_collector_set(size_t idx);\n+  inline void add_to_old_collector_set(size_t idx);\n+\n+  inline void remove_from_mutator_set(size_t idx);\n+  inline void remove_from_collector_set(size_t idx);\n+  inline void remove_from_old_collector_set(size_t idx);\n@@ -53,0 +125,3 @@\n+\n+  \/\/ Satisfy young-generation or single-generation collector allocation request req by finding memory that matches\n+  \/\/ affiliation, which either equals req.affiliation or FREE.  We know req.is_young().\n@@ -54,1 +129,4 @@\n-  HeapWord* allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region);\n+\n+  \/\/ Satisfy allocation request req by finding memory that matches affiliation, which either equals req.affiliation\n+  \/\/ or FREE. We know req.is_old().\n+  HeapWord* allocate_old_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -66,0 +144,1 @@\n+  void flip_to_old_gc(ShenandoahHeapRegion* r);\n@@ -67,0 +146,1 @@\n+  \/\/ Compute left-most and right-most indexes for the mutator_is_free, collector_is_free, and old_collector_is_free sets.\n@@ -68,0 +148,3 @@\n+\n+  \/\/ Adjust left-most and right-most indexes for the mutator_is_free, collector_is_free, and old_collector_is_free sets\n+  \/\/  following minor changes to at least one set membership.\n@@ -69,1 +152,0 @@\n-  bool touches_bounds(size_t num) const;\n@@ -71,1 +153,19 @@\n-  void increase_used(size_t amount);\n+  \/\/ Adjust left-most and right-most indexes for the mutator_is_free set after removing region idx from this set.\n+  bool adjust_mutator_bounds_if_touched(size_t idx);\n+\n+  \/\/ Adjust left-most and right-most indexes for the collector_is_free set after removing region idx from this set.\n+  bool adjust_collector_bounds_if_touched(size_t idx);\n+\n+  \/\/ Adjust left-most and right-most indexes for the old_collector_is_free set after removing region idx from this set.\n+  bool adjust_old_collector_bounds_if_touched(size_t idx);\n+\n+  \/\/ Return true iff region idx was the left-most or right-most index for one of the three free sets.\n+  bool touches_bounds(size_t idx) const;\n+\n+  \/\/ Adjust left-most and right-most indexes for the collector_is_free set after adding region idx to this set.\n+  void expand_collector_bounds_maybe(size_t idx);\n+\n+  \/\/ Adjust left-most and right-most indexes for the old_collector_is_free set after adding region idx to this set.\n+  void expand_old_collector_bounds_maybe(size_t idx);\n+\n+  inline void increase_used(size_t amount);\n@@ -76,3 +176,5 @@\n-  bool can_allocate_from(ShenandoahHeapRegion *r);\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n-  bool has_no_alloc_capacity(ShenandoahHeapRegion *r);\n+  bool can_allocate_from(ShenandoahHeapRegion *r) const;\n+  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n+  bool has_alloc_capacity(size_t idx) const;\n+  bool has_alloc_capacity(ShenandoahHeapRegion *r) const;\n+  bool has_no_alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -83,1 +185,1 @@\n-  \/\/ Number of regions dedicated to GC allocations (for evacuation or promotion) that are currently free\n+  \/\/ Number of regions dedicated to GC allocations (for evacuation) that are at least partially free\n@@ -86,1 +188,4 @@\n-  \/\/ Number of regions dedicated to mutator allocations that are currently free\n+  \/\/ Number of regions dedicated to Old GC allocations (for evacuation or promotion) that are at least partially free\n+  size_t old_collector_count() const { return _old_collector_free_bitmap.count_one_bits(); }\n+\n+  \/\/ Number of regions dedicated to mutator allocations that are at least partially free\n@@ -111,1 +216,2 @@\n-  void reserve_regions(size_t to_reserve);\n+  void find_regions_with_alloc_capacity();\n+  void reserve_regions(size_t young_reserve, size_t old_reserve);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":119,"deletions":13,"binary":false,"changes":132,"status":"modified"},{"patch":"@@ -1056,0 +1056,5 @@\n+\/\/ TODO:\n+\/\/  Consider compacting old-gen objects toward the high end of memory and young-gen objects towards the low-end\n+\/\/  of memory.  As currently implemented, all regions are compacted toward the low-end of memory.  This creates more\n+\/\/  fragmentation of the heap, because old-gen regions get scattered among low-address regions such that it becomes\n+\/\/  more difficult to find contiguous regions for humongous objects.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -834,0 +834,2 @@\n+  \/\/ Freeset construction uses reserve quantities if they are valid\n+  heap->set_evacuation_reserve_quantities(true);\n@@ -840,0 +842,1 @@\n+  heap->set_evacuation_reserve_quantities(false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -554,0 +554,2 @@\n+  _upgraded_to_full(false),\n+  _has_evacuation_reserve_quantities(false),\n@@ -2242,0 +2244,4 @@\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  _has_evacuation_reserve_quantities = is_valid;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -320,1 +320,1 @@\n-    OLD_MARKING_BITPOS = 5\n+    OLD_MARKING_BITPOS = 5,\n@@ -330,1 +330,1 @@\n-    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS,\n@@ -388,0 +388,2 @@\n+  bool _has_evacuation_reserve_quantities;\n+\n@@ -391,2 +393,0 @@\n-\n-\n@@ -394,0 +394,1 @@\n+\n@@ -397,0 +398,1 @@\n+  void set_evacuation_reserve_quantities(bool is_valid);\n@@ -412,0 +414,1 @@\n+  inline bool has_evacuation_reserve_quantities() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -664,0 +664,1 @@\n+\n@@ -668,0 +669,4 @@\n+inline bool ShenandoahHeap::has_evacuation_reserve_quantities() const {\n+  return _has_evacuation_reserve_quantities;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"}]}