{"files":[{"patch":"@@ -51,1 +51,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1208,1 +1208,0 @@\n-  heap->rebuild_free_set(true \/*concurrent*\/);\n@@ -1210,0 +1209,1 @@\n+  heap->rebuild_free_set(true \/*concurrent*\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,0 +40,2 @@\n+#undef KELVIN_MONITOR\n+\n@@ -44,0 +46,1 @@\n+  _old_collector_free_bitmap(max_regions, mtGC),\n@@ -69,10 +72,5 @@\n-\/\/ This is a temporary solution to work around a shortcoming with the existing free set implementation.\n-\/\/ TODO:\n-\/\/   Remove this function after restructing FreeSet representation.  A problem in the existing implementation is that old-gen\n-\/\/   regions are not considered to reside within the is_collector_free range.\n-\/\/\n-HeapWord* ShenandoahFreeSet::allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region) {\n-  ShenandoahRegionAffiliation affiliation = ShenandoahRegionAffiliation::OLD_GENERATION;\n-\n-  size_t rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n-  size_t leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+bool ShenandoahFreeSet::is_old_collector_free(size_t idx) const {\n+  assert (idx < _max, \"index is sane: \" SIZE_FORMAT \" < \" SIZE_FORMAT \" (left: \" SIZE_FORMAT \", right: \" SIZE_FORMAT \")\",\n+          idx, _max, _old_collector_leftmost, _old_collector_rightmost);\n+  return _old_collector_free_bitmap.at(idx);\n+}\n@@ -80,9 +78,66 @@\n-  for (size_t c = rightmost + 1; c > leftmost; c--) {\n-    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-    size_t idx = c - 1;\n-    ShenandoahHeapRegion* r = _heap->get_region(idx);\n-    if (r->affiliation() == affiliation && !r->is_humongous()) {\n-      if (!r->is_cset() && !has_no_alloc_capacity(r)) {\n-        HeapWord* result = try_allocate_in(r, req, in_new_region);\n-        if (result != nullptr) {\n-          return result;\n+HeapWord* ShenandoahFreeSet::allocate_old_with_affiliation(ShenandoahRegionAffiliation affiliation,\n+                                                           ShenandoahAllocRequest& req, bool& in_new_region) {\n+  size_t rightmost = _old_collector_rightmost;\n+  size_t leftmost = _old_collector_leftmost;\n+#ifdef KELVIN_MONITOR\n+  size_t old_regions_examined = 0;\n+  size_t region_with_most_avail = 0;\n+  size_t avail_in_largest_region = 0;\n+#endif\n+  if (_old_collector_search_left_to_right) {\n+    \/\/ This mode picks up stragglers following a full GC\n+    for (size_t c = leftmost; c <= rightmost; c++) {\n+      if (is_old_collector_free(c)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(c);\n+        assert(r->is_trash() || (r->affiliation() == ShenandoahRegionAffiliation::FREE)\n+               || (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION),\n+               \"is_old_collector_free region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+#ifdef KELVIN_MONITOR\n+          size_t region_available = r->end() - r->top();\n+          if ((old_regions_examined++ == 0) || (region_available > avail_in_largest_region)) {\n+            region_with_most_avail = c;\n+            avail_in_largest_region = region_available;\n+          }\n+#endif\n+          if (result != nullptr) {\n+#ifdef KELVIN_MONITOR\n+            log_info(gc, ergo)(\"aowa succeeds for %s size: \" SIZE_FORMAT \", min_size: \" SIZE_FORMAT \", actual_size: \" SIZE_FORMAT\n+                               \", in region \" SIZE_FORMAT \", remaining available: \" SIZE_FORMAT,\n+                               req.is_lab_alloc()? \"PLAB\": \"shared\", req.size(), req.is_lab_alloc()? req.min_size(): req.size(),\n+                               req.actual_size(), r->index(), r->end() - r->top());\n+#endif\n+            return result;\n+          }\n+        }\n+      }\n+    }\n+  } else {\n+    \/\/ This mode picks up stragglers from a previous concurrent GC\n+    for (size_t c = rightmost + 1; c > leftmost; c--) {\n+      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+      size_t idx = c - 1;\n+      if (is_old_collector_free(idx)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(idx);\n+        assert(r->is_trash() || (r->affiliation() == ShenandoahRegionAffiliation::FREE)\n+               || (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION),\n+               \"is_old_collector_free region has bad affiliation\");\n+        if (r->affiliation() == affiliation) {\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n+#ifdef KELVIN_MONITOR\n+          size_t region_available = r->end() - r->top();\n+          if ((old_regions_examined++ == 0) || (region_available > avail_in_largest_region)) {\n+            region_with_most_avail = idx;\n+            avail_in_largest_region = region_available;\n+          }\n+#endif\n+          if (result != nullptr) {\n+#ifdef KELVIN_MONITOR\n+            log_info(gc, ergo)(\"aowa succeeds for %s size: \" SIZE_FORMAT \", min_size: \" SIZE_FORMAT \", actual_size: \" SIZE_FORMAT\n+                               \", in region \" SIZE_FORMAT \", remaining availalble: \" SIZE_FORMAT,\n+                               req.is_lab_alloc()? \"PLAB\": \"shared\", req.size(), req.is_lab_alloc()? req.min_size(): req.size(),\n+                               req.actual_size(), r->index(), r->end() - r->top());\n+#endif\n+            return result;\n+          }\n@@ -93,0 +148,6 @@\n+#ifdef KELVIN_MONITOR\n+  log_info(gc, ergo)(\"aowa failed for %s size: \" SIZE_FORMAT \", min_size: \" SIZE_FORMAT \", scanned \" SIZE_FORMAT\n+                     \" from \" SIZE_FORMAT \" to \" SIZE_FORMAT \", largest available: \" SIZE_FORMAT \" at region \" SIZE_FORMAT,\n+                     req.is_lab_alloc()? \"PLAB\": \"shared\", req.size(), req.is_lab_alloc()? req.min_size(): req.size(),\n+                     old_regions_examined, leftmost, rightmost, avail_in_largest_region, region_with_most_avail);\n+#endif\n@@ -194,3 +255,1 @@\n-          \/\/ TODO: this is a work around to address a deficiency in FreeSet representation.  A better solution fixes\n-          \/\/ the FreeSet implementation to deal more efficiently with old-gen regions as being in the \"collector free set\"\n-          result = allocate_with_old_affiliation(req, in_new_region);\n+          result = allocate_old_with_affiliation(req.affiliation(), req, in_new_region);\n@@ -205,1 +264,5 @@\n-          result = allocate_with_affiliation(FREE, req, in_new_region);\n+          if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+            result = allocate_old_with_affiliation(FREE, req, in_new_region);\n+          } else {\n+            result = allocate_with_affiliation(FREE, req, in_new_region);\n+          }\n@@ -224,1 +287,5 @@\n-              flip_to_gc(r);\n+              if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+                flip_to_old_gc(r);\n+              } else {\n+                flip_to_gc(r);\n+              }\n@@ -444,0 +511,17 @@\n+    } else if (r->free() < PLAB::min_size() * HeapWordSize) {\n+      \/\/ Permanently retire this region if there's room for a fill object\n+      if (r->free() >= ShenandoahHeap::min_fill_size()) {\n+        size_t waste = r->free();\n+        HeapWord* fill_addr = r->top();\n+        size_t fill_size = waste \/ HeapWordSize;\n+        ShenandoahHeap::fill_with_object(fill_addr, fill_size);\n+        r->set_top(r->end());\n+        \/\/ Since we have filled the waste with an empty object, account for increased usage\n+        _heap->increase_used(waste);\n+        if (_heap->mode()->is_generational()) {\n+          _heap->generation_for(req.affiliation())->increase_used(waste);\n+          if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+            _heap->card_scan()->register_object(fill_addr);\n+          }\n+        }\n+      }\n@@ -447,0 +531,1 @@\n+    _old_collector_free_bitmap.clear_bit(num);\n@@ -451,0 +536,8 @@\n+#ifdef KELVIN_MONITOR\n+      if (!req.is_mutator_alloc()) {\n+        \/\/ I only want to see retiring of _is_collector_free and\n+        \/\/ _is_old_collector_free regions\n+        log_info(gc, ergo)(\"try_allocate_in() retiring region \" SIZE_FORMAT \" with free: \" SIZE_FORMAT\n+                           \" from all sets, and adjusting bounds\", num, r->free());\n+      }\n+#endif\n@@ -458,0 +551,16 @@\n+\/\/ If idx represents a mutator bound, recompute the mutator bounds, returning true iff bounds were adjusted.\n+bool ShenandoahFreeSet::adjust_mutator_bounds_if_touched(size_t idx) {\n+  if (idx == _mutator_leftmost || idx == _mutator_rightmost) {\n+    \/\/ Rewind both mutator bounds until the next bit.\n+    while (_mutator_leftmost < _max && !is_mutator_free(_mutator_leftmost)) {\n+      _mutator_leftmost++;\n+    }\n+    while (_mutator_rightmost > 0 && !is_mutator_free(_mutator_rightmost)) {\n+      _mutator_rightmost--;\n+    }\n+    return true;\n+  } else {\n+    return false;\n+  }\n+}\n+\n@@ -459,1 +568,3 @@\n-  return num == _collector_leftmost || num == _collector_rightmost || num == _mutator_leftmost || num == _mutator_rightmost;\n+  return (num == _collector_leftmost || num == _collector_rightmost ||\n+          num == _old_collector_leftmost || num == _old_collector_rightmost ||\n+          num == _mutator_leftmost || num == _mutator_rightmost);\n@@ -468,0 +579,2 @@\n+  _old_collector_rightmost = _max - 1;\n+  _old_collector_leftmost = 0;\n@@ -471,0 +584,45 @@\n+  if (_heap->mode()->is_generational()) {\n+    size_t old_collector_middle = (_old_collector_leftmost + _old_collector_rightmost) \/ 2;\n+    size_t old_collector_available_in_first_half = 0;\n+    size_t old_collector_available_in_second_half = 0;\n+\n+    for (size_t index = _old_collector_leftmost; index < old_collector_middle; index++) {\n+      if (is_old_collector_free(index)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(index);\n+        old_collector_available_in_first_half += r->free();\n+      }\n+    }\n+    for (size_t index = old_collector_middle; index <= _old_collector_rightmost; index++) {\n+      if (is_old_collector_free(index)) {\n+        ShenandoahHeapRegion* r = _heap->get_region(index);\n+        old_collector_available_in_second_half += r->free();\n+      }\n+    }\n+    _old_collector_search_left_to_right = (old_collector_available_in_second_half > old_collector_available_in_first_half);\n+  }\n+}\n+\n+bool ShenandoahFreeSet::expand_collector_bounds_maybe(size_t idx) {\n+  bool result = false;\n+  if (idx < _collector_leftmost) {\n+    _collector_leftmost = idx;\n+    result = true;\n+  }\n+  if (idx > _collector_rightmost) {\n+    _collector_rightmost = idx;\n+    result = true;\n+  }\n+  return result;\n+}\n+\n+bool ShenandoahFreeSet::expand_old_collector_bounds_maybe(size_t idx) {\n+  bool result = false;\n+  if (idx < _old_collector_leftmost) {\n+    _old_collector_leftmost = idx;\n+    result = true;\n+  }\n+  if (idx > _old_collector_rightmost) {\n+    _old_collector_rightmost = idx;\n+    result = true;\n+  }\n+  return result;\n@@ -474,0 +632,9 @@\n+#ifdef KELVIN_MONITOR\n+  size_t original_m_left = _mutator_leftmost;\n+  size_t original_m_right = _mutator_rightmost;\n+  size_t original_c_left = _collector_leftmost;\n+  size_t original_c_right = _collector_rightmost;\n+  size_t original_oc_left = _old_collector_leftmost;\n+  size_t original_oc_right = _old_collector_rightmost;\n+#endif\n+\n@@ -488,0 +655,19 @@\n+  \/\/ Rewind both old collector bounds until the next bit.\n+  while (_old_collector_leftmost < _max && !is_old_collector_free(_old_collector_leftmost)) {\n+    _old_collector_leftmost++;\n+  }\n+  while (_old_collector_rightmost > 0 && !is_old_collector_free(_old_collector_rightmost)) {\n+    _old_collector_rightmost--;\n+  }\n+#ifdef KELVIN_MONITOR\n+  if ((original_c_left != _collector_leftmost) || (original_c_right != _collector_rightmost) ||\n+      (original_oc_left != _old_collector_leftmost) || (original_oc_right != _old_collector_rightmost)) {\n+    log_info(gc, ergo)(\"adjust_bounds for mutator [\" SIZE_FORMAT \"-\" SIZE_FORMAT \"] => [\" SIZE_FORMAT \"-\" SIZE_FORMAT\n+                       \"], for collector [\" SIZE_FORMAT \"-\" SIZE_FORMAT \"] -> [\" SIZE_FORMAT \"-\" SIZE_FORMAT\n+                       \"], for old collector [\" SIZE_FORMAT \"-\" SIZE_FORMAT \"] -> [\" SIZE_FORMAT \"-\" SIZE_FORMAT \"]\",\n+                       original_m_left, original_m_right, _mutator_leftmost, _mutator_rightmost,\n+                       original_c_left, original_c_right, _collector_leftmost, _collector_rightmost,\n+                       original_oc_left, original_oc_right, _old_collector_leftmost, _old_collector_rightmost);\n+    \/\/ Don't report mutator adjustments.  They are too frequent.\n+  }\n+#endif\n@@ -596,0 +782,3 @@\n+#ifdef KELVIN_MONITOR\n+    log_info(gc, ergo)(\"alloc_contiguous(\" SIZE_FORMAT \"-\" SIZE_FORMAT \") is adjusting bounds\", beg, end);\n+#endif\n@@ -604,0 +793,3 @@\n+\/\/ Returns true iff this region is entirely available, either because it is empty() or because it has been found to represent\n+\/\/ immediate trash and we'll be able to immediately recycle it.  Note that we cannot recycle immediate trash if\n+\/\/ concurrent weak root processing is in progress.\n@@ -642,0 +834,28 @@\n+void ShenandoahFreeSet::flip_to_old_gc(ShenandoahHeapRegion* r) {\n+  size_t idx = r->index();\n+\n+  assert(_mutator_free_bitmap.at(idx), \"Should be in mutator view\");\n+  assert(can_allocate_from(r), \"Should not be allocated\");\n+\n+#ifdef KELVIN_MONITOR\n+  size_t original_left = _old_collector_leftmost;\n+  size_t original_right = _old_collector_rightmost;\n+#endif\n+  _mutator_free_bitmap.clear_bit(idx);\n+  _old_collector_free_bitmap.set_bit(idx);\n+  bool result = expand_old_collector_bounds_maybe(idx);\n+#ifdef KELVIN_MONITOR\n+  log_info(gc, ergo)(\"Flipping region \" SIZE_FORMAT \" to OLD GC, %s collector range: [\" SIZE_FORMAT \"-\" SIZE_FORMAT \"] to [\"\n+                     SIZE_FORMAT \"-\" SIZE_FORMAT \"]\", idx, result? \"expanded\": \"preserved\",\n+ original_left, original_right, _old_collector_leftmost, _old_collector_rightmost);\n+#endif\n+\n+  _capacity -= alloc_capacity(r);\n+  adjust_mutator_bounds_if_touched(idx);\n+  assert_bounds();\n+\n+  \/\/ We do not ensure that the region is no longer trash,\n+  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ to recycle trash before attempting to allocate anything in the region.\n+}\n+\n@@ -648,0 +868,4 @@\n+#ifdef KELVIN_MONITOR\n+  size_t original_left = _collector_leftmost;\n+  size_t original_right = _collector_rightmost;\n+#endif\n@@ -650,2 +874,6 @@\n-  _collector_leftmost = MIN2(idx, _collector_leftmost);\n-  _collector_rightmost = MAX2(idx, _collector_rightmost);\n+  bool result = expand_collector_bounds_maybe(idx);\n+#ifdef KELVIN_MONITOR\n+  log_info(gc, ergo)(\"Flipping region \" SIZE_FORMAT \" to GC, collector range: [\" SIZE_FORMAT \"-\" SIZE_FORMAT \"] to [\"\n+                     SIZE_FORMAT \"-\" SIZE_FORMAT \"]\",\n+                     idx, original_left, original_right, _collector_leftmost, _collector_rightmost);\n+#endif\n@@ -654,4 +882,1 @@\n-\n-  if (touches_bounds(idx)) {\n-    adjust_bounds();\n-  }\n+  adjust_mutator_bounds_if_touched(idx);\n@@ -673,0 +898,1 @@\n+  _old_collector_free_bitmap.clear();\n@@ -677,0 +903,2 @@\n+  _old_collector_leftmost = _max;\n+  _old_collector_rightmost = 0;\n@@ -678,0 +906,1 @@\n+  _old_capacity = 0;\n@@ -686,0 +915,3 @@\n+#ifdef KELVIN_MONITOR\n+  log_info(gc, ergo)(\"Rebuilding FreeSet\");\n+#endif\n@@ -688,0 +920,13 @@\n+#ifdef KELVIN_MONITOR\n+    bool was_collector_free = false;\n+    if (is_collector_free(idx)) {\n+      was_collector_free = true;\n+    }\n+#endif\n+\n+    \/\/ We move all young available regions into mutator_free set and then we take back the regions we need for our\n+    \/\/ reserve.  This allows us to \"compact\" the collector_free (survivor) regions at the high end of the heap.\n+    _mutator_free_bitmap.clear_bit(idx);\n+    _collector_free_bitmap.clear_bit(idx);\n+    _old_collector_free_bitmap.clear_bit(idx);\n+\n@@ -689,1 +934,1 @@\n-      assert(!region->is_cset(), \"Shouldn't be adding those to the free set\");\n+      assert(!region->is_cset(), \"Shouldn't be adding cset regions to the free set\");\n@@ -691,0 +936,5 @@\n+#ifdef KELVIN_MONITOR_X\n+      if (has_no_alloc_capacity(region)) {\n+        log_info(gc, ergo)(\"Region \" SIZE_FORMAT \" not part of FreeSet because it has no alloc capacity\", region->index());\n+      }\n+#endif\n@@ -694,7 +944,18 @@\n-      _capacity += alloc_capacity(region);\n-      assert(_used <= _capacity, \"must not use more than we have\");\n-\n-      assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n-      _mutator_free_bitmap.set_bit(idx);\n-\n-      log_debug(gc, free)(\"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n+      if (region->is_old()) {\n+        _old_capacity += alloc_capacity(region);\n+        assert(!is_old_collector_free(idx), \"We are about to add it, it shouldn't be there already\");\n+        _old_collector_free_bitmap.set_bit(idx);\n+        log_debug(gc)(\"  Setting Region \" SIZE_FORMAT \" _old_collector_free_bitmap bit to true\", idx);\n+      } else {\n+        _capacity += alloc_capacity(region);\n+        assert(_used <= _capacity, \"must not use more than we have\");\n+        assert(!is_mutator_free(idx), \"We are about to add it, it shouldn't be there already\");\n+        _mutator_free_bitmap.set_bit(idx);\n+#ifdef KELVIN_MONITOR\n+        if (was_collector_free) {\n+          log_info(gc, ergo)(\"Treating Region \" SIZE_FORMAT \" as _mutator_free and collector_free!  region capacity: \" SIZE_FORMAT\n+                             \", total capacity: \" SIZE_FORMAT, idx, alloc_capacity(region), _capacity);\n+        }\n+#endif\n+        log_debug(gc, free)(\n+          \"  Adding Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to mutator free set\",\n@@ -702,1 +963,7 @@\n-               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+          byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+      }\n+    }\n+#ifdef KELVIN_MONITOR_X\n+    else {\n+      log_info(gc, ergo)(\"Region \" SIZE_FORMAT \" not part of FreeSet because allocation not allowed and region is not trash\",\n+                         region->index());\n@@ -704,0 +971,1 @@\n+#endif\n@@ -706,0 +974,5 @@\n+#ifdef KELVIN_MONITOR\n+  log_info(gc, ergo)(\"After rebuild but before reserve\");\n+  log_status();\n+#endif\n+\n@@ -707,0 +980,1 @@\n+  size_t young_reserve, old_reserve;\n@@ -708,2 +982,2 @@\n-    size_t to_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n-    reserve_regions(to_reserve);\n+    young_reserve = (_heap->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    old_reserve = 0;\n@@ -711,1 +985,1 @@\n-    size_t young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+\n@@ -717,3 +991,18 @@\n-    size_t old_reserve = 0;\n-    size_t to_reserve = young_reserve + old_reserve;\n-    reserve_regions(to_reserve);\n+    if (_heap->has_evacuation_reserve_quantities()) {\n+      \/\/ We are rebuilding at the end of final mark, having established evacuation budgets for this GC pass.\n+      young_reserve = _heap->get_young_evac_reserve();\n+      old_reserve = _heap->get_promoted_reserve() + _heap->get_old_evac_reserve();\n+#ifdef KELVIN_MONITOR\n+      log_info(gc, ergo)(\"Freeset for this evacuation, young reserve: \" SIZE_FORMAT \", old reserve: \" SIZE_FORMAT,\n+                         young_reserve, old_reserve);\n+#endif\n+    } else {\n+      \/\/ We are rebuilding at end of GC, so we set aside budgets specified on command line (or defaults)\n+      young_reserve = (_heap->young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+      old_reserve = MAX2((_heap->old_generation()->max_capacity() * ShenandoahOldEvacReserve) \/ 100,\n+                         ShenandoahOldCompactionReserve * ShenandoahHeapRegion::region_size_bytes());\n+#ifdef KELVIN_MONITOR\n+      log_info(gc, ergo)(\"Freeset for next evacuation, young reserve: \" SIZE_FORMAT \", old reserve: \" SIZE_FORMAT,\n+                         young_reserve, old_reserve);\n+#endif\n+    }\n@@ -721,1 +1010,1 @@\n-\n+  reserve_regions(young_reserve, old_reserve);\n@@ -724,0 +1013,5 @@\n+#ifdef KELVIN_MONITOR\n+  log_info(gc, ergo)(\"After rebuild and reserve and recomputing bounds, allocate old left to right is: %s\",\n+                     _old_collector_search_left_to_right? \"true\": \"false\");\n+#endif\n+  log_status();\n@@ -726,1 +1020,1 @@\n-void ShenandoahFreeSet::reserve_regions(size_t to_reserve) {\n+void ShenandoahFreeSet::reserve_regions(size_t to_reserve, size_t to_reserve_old) {\n@@ -728,1 +1022,7 @@\n-\n+#ifdef KELVIN_MONITOR\n+  size_t original_old_capacity = _old_capacity;\n+  size_t leftmost_reserved = 0;\n+  size_t rightmost_reserved = 0;\n+  size_t leftmost_old_reserved = 0;\n+  size_t rightmost_old_reserved = 0;\n+#endif\n@@ -730,12 +1030,46 @@\n-    if (reserved >= to_reserve) break;\n-\n-    ShenandoahHeapRegion* region = _heap->get_region(idx);\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n-      _mutator_free_bitmap.clear_bit(idx);\n-      _collector_free_bitmap.set_bit(idx);\n-      size_t ac = alloc_capacity(region);\n-      _capacity -= ac;\n-      reserved += ac;\n-      log_debug(gc, free)(\"  Shifting Region \" SIZE_FORMAT \" (Free: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s) to collector free set\",\n-                          idx, byte_size_in_proper_unit(region->free()), proper_unit_for_byte_size(region->free()),\n-                               byte_size_in_proper_unit(region->used()), proper_unit_for_byte_size(region->used()));\n+#ifdef KELVIN_MONITOR_X\n+    if (reserved <= to_reserve) {\n+      log_info(gc, ergo)(\"Reserving \" SIZE_FORMAT \" more, region \" SIZE_FORMAT \" has capacity: \" SIZE_FORMAT,\n+                         to_reserve - reserved, idx, alloc_capacity(_heap->get_region(idx)));\n+    } else {\n+      log_info(gc, ergo)(\"Resevations are done\");\n+    }\n+#endif\n+    ShenandoahHeapRegion* r = _heap->get_region(idx);\n+    if (_mutator_free_bitmap.at(idx) && (alloc_capacity(r) > 0)) {\n+      assert(!r->is_old(), \"mutator_is_free regions should not be affiliated OLD\");\n+      \/\/ OLD regions that have available memory are already in the old_collector free set\n+      if ((_old_capacity < to_reserve_old) && (r->is_trash() || (r->affiliation() == ShenandoahRegionAffiliation::FREE))) {\n+        _mutator_free_bitmap.clear_bit(idx);\n+        _old_collector_free_bitmap.set_bit(idx);\n+        size_t ac = alloc_capacity(r);\n+        _capacity -= ac;\n+        _old_capacity += ac;\n+#ifdef KELVIN_MONITOR\n+        leftmost_old_reserved = idx;\n+        if (rightmost_old_reserved == 0) {\n+          rightmost_old_reserved = idx;\n+        }\n+#endif\n+        log_debug(gc, free)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to old_collector_free\", idx);\n+      } else if (reserved < to_reserve) {\n+        \/\/ Note: In a previous implementation, regions were only placed into the survivor space (collector_is_free) if\n+        \/\/ they were entirely empty.  I'm not sure I understand the rational for that.  That alternative behavior would\n+        \/\/ tend to mix survivor objects with ephemeral objects, making it more difficult to reclaim the memory for the\n+        \/\/ ephemeral objects.  It also delays aging of regions, causing promotion in place to be delayed.\n+        _mutator_free_bitmap.clear_bit(idx);\n+        _collector_free_bitmap.set_bit(idx);\n+        size_t ac = alloc_capacity(r);\n+        _capacity -= ac;\n+        reserved += ac;\n+#ifdef KELVIN_MONITOR\n+        leftmost_reserved = idx;\n+        if (rightmost_reserved == 0) {\n+          rightmost_reserved = idx;\n+        }\n+#endif\n+        log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n+      } else {\n+        \/\/ We've satisfied both to_reserve and to_reserved_old\n+        break;\n+      }\n@@ -744,0 +1078,8 @@\n+#ifdef KELVIN_MONITOR\n+  log_info(gc, ergo)(\"Successfully reserved for young: \" SIZE_FORMAT \" between \" SIZE_FORMAT \" and \" SIZE_FORMAT\n+                     \", reserved for old: \" SIZE_FORMAT \" between \" SIZE_FORMAT \" and \" SIZE_FORMAT\n+                     \" (of which \" SIZE_FORMAT \" is scattered)\"\n+                     \", with remaining allocation capacity: \" SIZE_FORMAT,\n+                     reserved, leftmost_reserved, rightmost_reserved,\n+                     _old_capacity, leftmost_old_reserved, rightmost_old_reserved, original_old_capacity, _capacity);\n+#endif\n@@ -749,0 +1091,57 @@\n+#ifdef ASSERT\n+  \/\/ Dump of the FreeSet details is only enabled if assertions are enabled\n+  {\n+#define BUFFER_SIZE 80\n+    char buffer[BUFFER_SIZE];\n+    for (uint i = 0; i < BUFFER_SIZE; i++) {\n+      buffer[i] = '\\0';\n+    }\n+    log_info(gc, ergo)(\"FreeSet map legend (see source for unexpected codes: *, $, !, #):\\n\"\n+                       \" m - mutator_free, c - collector_free, C - old_collector_free,\"\n+                       \" h - humongous young, H - humongous old, ~ - retired old, _ - retired young\");\n+    log_info(gc, ergo)(\" mutator free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \" collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"], \"\n+                       \"old collector free range [\" SIZE_FORMAT \"..\" SIZE_FORMAT \"]\",\n+                       _mutator_leftmost, _mutator_rightmost, _collector_leftmost, _collector_rightmost,\n+                       _old_collector_leftmost, _old_collector_rightmost);\n+    for (uint i = 0; i < _heap->num_regions(); i++) {\n+      ShenandoahHeapRegion *r = _heap->get_region(i);\n+      uint idx = i % 64;\n+      if ((i != 0) && (idx == 0)) {\n+        log_info(gc, ergo)(\" %6u: %s\", i-64, buffer);\n+      }\n+      if (is_mutator_free(i) && is_collector_free(i) && is_old_collector_free(i)) {\n+        buffer[idx] = '*';\n+      } else if (is_mutator_free(i) && is_collector_free(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = '$';\n+      } else if (is_mutator_free(i) && is_old_collector_free(i)) {\n+        \/\/ Note that young regions may be in the old_collector_free set.\n+        buffer[idx] = '!';\n+      } else if (is_collector_free(i) && is_old_collector_free(i)) {\n+        buffer[idx] = '#';\n+      } else if (is_mutator_free(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in mutator_free set\");\n+        buffer[idx] = 'm';\n+      } else if (is_collector_free(i)) {\n+        assert(!r->is_old(), \"Old regions should not be in collector_free set\");\n+        buffer[idx] = 'c';\n+      } else if (is_old_collector_free(i)) {\n+        buffer[idx] = 'C';\n+      }\n+      else if (r->is_humongous()) {\n+        buffer[idx] = (r->is_old())? 'H': 'h';\n+      } else {\n+        buffer[idx] = (r->is_old())? '~': '_';\n+      }\n+    }\n+    uint remnant = _heap->num_regions() % 64;\n+    if (remnant > 0) {\n+      buffer[remnant] = '\\0';\n+    } else {\n+      remnant = 64;\n+    }\n+    log_info(gc, ergo)(\" %6u: %s\", (uint) (_heap->num_regions() - remnant), buffer);\n+  }\n+#endif\n+\n@@ -815,1 +1214,1 @@\n-      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT \" \",\n+      ls.print(\"Used: \" SIZE_FORMAT \"%s, Mutator Free: \" SIZE_FORMAT,\n@@ -833,0 +1232,10 @@\n+      ls.print(\" Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n+               byte_size_in_proper_unit(total_free), proper_unit_for_byte_size(total_free),\n+               byte_size_in_proper_unit(max),        proper_unit_for_byte_size(max),\n+               byte_size_in_proper_unit(total_used), proper_unit_for_byte_size(total_used));\n+    }\n+\n+    if (_heap->mode()->is_generational()) {\n+      size_t max = 0;\n+      size_t total_free = 0;\n+      size_t total_used = 0;\n@@ -834,1 +1243,10 @@\n-      ls.print_cr(\"Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s, Used: \" SIZE_FORMAT \"%s\",\n+      for (size_t idx = _old_collector_leftmost; idx <= _old_collector_rightmost; idx++) {\n+        if (is_old_collector_free(idx)) {\n+          ShenandoahHeapRegion *r = _heap->get_region(idx);\n+          size_t free = alloc_capacity(r);\n+          max = MAX2(max, free);\n+          total_free += free;\n+          total_used += r->used();\n+        }\n+      }\n+      ls.print_cr(\" Old Collector Reserve: \" SIZE_FORMAT \"%s, Max: \" SIZE_FORMAT \"%s; Used: \" SIZE_FORMAT \"%s\",\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":481,"deletions":63,"binary":false,"changes":544,"status":"modified"},{"patch":"@@ -37,0 +37,5 @@\n+  \/\/ We keep the _old_collector regions separate from the young collector regions.  This allows us to pack the old regions\n+  \/\/ further to the right than the young collector regions.  This is desirable because the old collector regions are recycled\n+  \/\/ even less frequently than the young survivor regions.  In generational mode, the young survivor regions are typically\n+  \/\/ recycled after tenure age GC passes.\n+  CHeapBitMap _old_collector_free_bitmap;\n@@ -43,0 +48,1 @@\n+  size_t _old_collector_leftmost, _old_collector_rightmost;\n@@ -45,0 +51,1 @@\n+  size_t _old_capacity;\n@@ -47,0 +54,6 @@\n+  \/\/ When is_old_collector_free regions sparsely populate the lower address ranges of the heap, we search from left to\n+  \/\/ right in order to consume (and remove from the is_old_collector_free range) these sparsely distributed regions.\n+  \/\/ This allows us to more quickly condense the range of addresses that represent old_collector_free regions.\n+  bool _old_collector_search_left_to_right = true;\n+\n+\n@@ -51,0 +64,1 @@\n+  bool is_old_collector_free(size_t idx) const;\n@@ -54,1 +68,2 @@\n-  HeapWord* allocate_with_old_affiliation(ShenandoahAllocRequest& req, bool& in_new_region);\n+  HeapWord* allocate_old_with_affiliation(ShenandoahRegionAffiliation affiliation,\n+                                          ShenandoahAllocRequest& req, bool& in_new_region);\n@@ -66,0 +81,1 @@\n+  void flip_to_old_gc(ShenandoahHeapRegion* r);\n@@ -69,0 +85,1 @@\n+  bool adjust_mutator_bounds_if_touched(size_t idx);\n@@ -70,0 +87,2 @@\n+  bool expand_collector_bounds_maybe(size_t idx);\n+  bool expand_old_collector_bounds_maybe(size_t idx);\n@@ -83,1 +102,1 @@\n-  \/\/ Number of regions dedicated to GC allocations (for evacuation or promotion) that are currently free\n+  \/\/ Number of regions dedicated to GC allocations (for evacuation) that are currently free\n@@ -86,0 +105,3 @@\n+  \/\/ Number of regions dedicated to Old GC allocations (for evacuation or promotion) that are currently free\n+  size_t old_collector_count() const { return _old_collector_free_bitmap.count_one_bits(); }\n+\n@@ -111,1 +133,1 @@\n-  void reserve_regions(size_t to_reserve);\n+  void reserve_regions(size_t young_reserve, size_t old_reserve);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -1053,0 +1053,5 @@\n+\/\/ TODO:\n+\/\/  Consider compacting old-gen objects toward the high end of memory and young-gen objects towards the low-end\n+\/\/  of memory.  As currently implemented, all regions are compacted toward the low-end of memory.  This creates more\n+\/\/  fragmentation of the heap, because old-gen regions get scattered among low-address regions such that it becomes\n+\/\/  more difficult to find contiguous regions for humongous objects.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -831,0 +831,2 @@\n+  \/\/ Freeset construction uses reserve quantities if they are valid\n+  heap->set_evacuation_reserve_quantities(true);\n@@ -837,0 +839,1 @@\n+  heap->set_evacuation_reserve_quantities(false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2243,0 +2243,4 @@\n+void ShenandoahHeap::set_evacuation_reserve_quantities(bool is_valid) {\n+  set_gc_state_mask(VALID_EVACUATION_RESERVE_QUANTITIES, is_valid);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -319,1 +319,4 @@\n-    OLD_MARKING_BITPOS = 5\n+    OLD_MARKING_BITPOS = 5,\n+\n+    \/\/ The evacuation reserves for old-gen and young-gen are available\n+    VALID_EVACUATION_RESERVE_QUANTITIES_BITPOS = 6\n@@ -329,1 +332,2 @@\n-    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS,\n+    VALID_EVACUATION_RESERVE_QUANTITIES = 1 << VALID_EVACUATION_RESERVE_QUANTITIES_BITPOS\n@@ -387,0 +391,2 @@\n+  bool _has_evacuation_reserve_quantities;\n+\n@@ -393,0 +399,1 @@\n+\n@@ -396,0 +403,1 @@\n+  void set_evacuation_reserve_quantities(bool is_valid);\n@@ -411,0 +419,1 @@\n+  inline bool has_evacuation_reserve_quantities() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -665,0 +665,1 @@\n+\n@@ -669,0 +670,4 @@\n+inline bool ShenandoahHeap::has_evacuation_reserve_quantities() const {\n+  return _gc_state.is_set(VALID_EVACUATION_RESERVE_QUANTITIES);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"}]}