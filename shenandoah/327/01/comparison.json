{"files":[{"patch":"@@ -63,0 +63,7 @@\n+const size_t ShenandoahAdaptiveHeuristics::SPIKE_ACCELERATION_SAMPLE_SIZE = 3;\n+\n+\/\/ Separately, we keep track of the average gc time.  We track the most recent GC_TIME_SAMPLE_SIZE GC times in order to\n+\/\/ detect changing trends in the time required to perform GC.  If the number of samples is too large, we will not be as\n+\/\/ responsive to change trends, as the best-fit line will look more like an average.\n+const size_t ShenandoahAdaptiveHeuristics::GC_TIME_SAMPLE_SIZE = 4;\n+\n@@ -68,3 +75,27 @@\n-  _available(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor) { }\n-\n-ShenandoahAdaptiveHeuristics::~ShenandoahAdaptiveHeuristics() {}\n+  _available(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor),\n+  _gc_time_first_sample_index(0),\n+  _gc_time_num_samples(0),\n+  _gc_time_timestamps(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_samples(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_xy(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_xx(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_sum_of_timestamps(0),\n+  _gc_time_sum_of_samples(0),\n+  _gc_time_sum_of_xy(0),\n+  _gc_time_sum_of_xx(0),\n+  _gc_time_m(0.0),\n+  _gc_time_b(0.0),\n+  _gc_time_sd(0.0),\n+  _spike_acceleration_first_sample_index(0),\n+  _spike_acceleration_num_samples(0),\n+  _spike_acceleration_rate_samples(NEW_C_HEAP_ARRAY(double, SPIKE_ACCELERATION_SAMPLE_SIZE, mtGC)),\n+  _spike_acceleration_rate_timestamps(NEW_C_HEAP_ARRAY(double, SPIKE_ACCELERATION_SAMPLE_SIZE, mtGC)) { }\n+\n+ShenandoahAdaptiveHeuristics::~ShenandoahAdaptiveHeuristics() {\n+  FREE_C_HEAP_ARRAY(double, _spike_acceleration_rate_samples);\n+  FREE_C_HEAP_ARRAY(double, _spike_acceleration_rate_timestamps);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_timestamps);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_samples);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_xy);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_xx);\n+}\n@@ -135,0 +166,85 @@\n+#undef KELVIN_NEW_CODE\n+void ShenandoahAdaptiveHeuristics::add_gc_time(double timestamp, double gc_time) {\n+\n+  \/\/ Update best-fit linear predictor of GC time\n+  uint index = (_gc_time_first_sample_index + _gc_time_num_samples) % GC_TIME_SAMPLE_SIZE;\n+  if (_gc_time_num_samples == GC_TIME_SAMPLE_SIZE) {\n+    _gc_time_sum_of_timestamps -= _gc_time_timestamps[index];\n+    _gc_time_sum_of_samples -= _gc_time_samples[index];\n+    _gc_time_sum_of_xy -= _gc_time_xy[index];\n+    _gc_time_sum_of_xx -= _gc_time_xx[index];\n+  }\n+  _gc_time_timestamps[index] = timestamp;\n+  _gc_time_samples[index] = gc_time;\n+  _gc_time_xy[index] = timestamp * gc_time;\n+  _gc_time_xx[index] = timestamp * timestamp;\n+\n+  _gc_time_sum_of_timestamps += _gc_time_timestamps[index];\n+  _gc_time_sum_of_samples += _gc_time_samples[index];\n+  _gc_time_sum_of_xy += _gc_time_xy[index];\n+  _gc_time_sum_of_xx += _gc_time_xx[index];\n+\n+  if (_gc_time_num_samples < GC_TIME_SAMPLE_SIZE) {\n+    _gc_time_num_samples++;\n+  } else {\n+    _gc_time_first_sample_index = (_gc_time_first_sample_index + 1) % GC_TIME_SAMPLE_SIZE;\n+  }\n+\n+  if (_gc_time_num_samples == 1) {\n+    \/\/ The predictor is constant (horizontal line)\n+    _gc_time_m = 0;\n+    _gc_time_b = gc_time;\n+    _gc_time_sd = 0.0;\n+  } else if (_gc_time_num_samples == 2) {\n+    \/\/ Two points define a line\n+    double delta_y = gc_time - _gc_time_samples[_gc_time_first_sample_index];\n+    double delta_x = timestamp - _gc_time_timestamps[_gc_time_first_sample_index];\n+\n+    _gc_time_m = delta_y \/ delta_x;\n+\n+#ifdef KELVIN_NEW_CODE\n+    log_info(gc)(\"For 2 samples with first index: %u: delta_y is %.3f = %.3f - %.3f, delta_x is %.3f = %.3f - %.3f\",\n+                 _gc_time_first_sample_index,\n+                 delta_y, timestamp, _gc_time_timestamps[_gc_time_first_sample_index],\n+                 delta_x, gc_time, _gc_time_samples[_gc_time_first_sample_index]);\n+#endif\n+    \/\/ y = mx + b\n+    \/\/ so b = y0 - mx0\n+    _gc_time_b = gc_time - _gc_time_m * timestamp;\n+    _gc_time_sd = 0.0;\n+  } else {\n+    _gc_time_m = ((_gc_time_num_samples * _gc_time_sum_of_xy - _gc_time_sum_of_timestamps * _gc_time_sum_of_samples) \/\n+                  (_gc_time_num_samples * _gc_time_sum_of_xx - _gc_time_sum_of_timestamps * _gc_time_sum_of_timestamps));\n+    _gc_time_b = (_gc_time_sum_of_samples - _gc_time_m * _gc_time_sum_of_timestamps) \/ _gc_time_num_samples;\n+#ifdef KELVIN_NEW_CODE\n+    log_info(gc)(\"num_samples: %u, sum_of_xy: %.3f, sum of timestamps: %.3f, sum_of_samples: %.3f, sum_of_xx: %.3f\",\n+                 _gc_time_num_samples, _gc_time_sum_of_xy, _gc_time_sum_of_timestamps,\n+                 _gc_time_sum_of_samples, _gc_time_sum_of_xx);\n+    log_info(gc)(\"%14s%14s%14s%14s\", \"actual x\", \"actual y\", \"predict y\", \"deviation\");\n+#endif\n+\n+    double sum_of_squared_deviations = 0.0;\n+    for (size_t i = 0; i < _gc_time_num_samples; i++) {\n+      uint index = (_gc_time_first_sample_index + i) % GC_TIME_SAMPLE_SIZE;\n+      double x = _gc_time_timestamps[index];\n+      double predicted_y = _gc_time_m * x + _gc_time_b;\n+      double deviation = predicted_y - _gc_time_samples[index];\n+\n+#ifdef KELVIN_NEW_CODE\n+      log_info(gc)(\"%14.3f%14.3f%14.3f%14.3f\", x, _gc_time_samples[index], predicted_y, deviation);\n+#endif\n+      sum_of_squared_deviations = deviation * deviation;\n+    }\n+    _gc_time_sd = sqrt(sum_of_squared_deviations \/ _gc_time_num_samples);\n+  }\n+#ifdef KELVIN_NEW_CODE\n+  log_info(gc)(\"@ %.3f, add gc_time[%u]: %.3f, new slope: %.3f, new y-intercept: %.3f, _gc_time_sd: %.3f\",\n+               timestamp, _gc_time_num_samples, gc_time, _gc_time_m, _gc_time_b, _gc_time_sd);\n+#endif\n+}\n+\n+double ShenandoahAdaptiveHeuristics::predict_gc_time(double timestamp_at_start) {\n+  double result = _gc_time_m * timestamp_at_start + _gc_time_b + _gc_time_sd * _margin_of_error_sd;;\n+  return result;\n+}\n+\n@@ -138,0 +254,4 @@\n+  if (!abbreviated) {\n+    add_gc_time(_cycle_start, elapsed_cycle_time());\n+  }\n+\n@@ -208,0 +328,30 @@\n+#undef KELVIN_TRACE_INFRA\n+#ifdef KELVIN_TRACE_INFRA\n+\n+static double _history_timestamp;\n+static double _history_interval;\n+\n+void ShenandoahAdaptiveHeuristics::timestamp_for_sample(double timestamp, double next_interval) {\n+  _history_timestamp = timestamp;\n+  _history_interval = next_interval;\n+}\n+\n+#define HISTORY_SIZE 512\n+\n+static size_t _history_oldest = 0;\n+static size_t _history_count = 0;\n+static double _historic_avg_cycle_time[HISTORY_SIZE];\n+static double _historic_predicted_cycle_time[HISTORY_SIZE];\n+static double _historic_avg_alloc_rate[HISTORY_SIZE];\n+static double _historic_spike_alloc_rate[HISTORY_SIZE];\n+static double _historic_spike_threshold[HISTORY_SIZE];\n+static double _historic_timestamp[HISTORY_SIZE];\n+static double _historic_interval[HISTORY_SIZE];\n+static unsigned long _historic_headroom[HISTORY_SIZE];\n+static unsigned long _historic_allocated[HISTORY_SIZE];\n+static unsigned long _historic_available[HISTORY_SIZE];\n+static unsigned long _historic_accelerated_consumption[HISTORY_SIZE];\n+static ShenandoahAllocationRate _historic_rates[HISTORY_SIZE];\n+\/\/ _last_sample_time, _last_sample_value, _interval_sec\n+#endif\n+\n@@ -273,1 +423,0 @@\n-\n@@ -279,0 +428,5 @@\n+#undef KELVIN_NOISE\n+#ifdef KELVIN_NOISE\n+  log_info(gc)(\"available: \" SIZE_FORMAT \", spike_headroom: \" SIZE_FORMAT \", penalties: \" SIZE_FORMAT \" becomes allocation_headroom: \" SIZE_FORMAT,\n+               available, spike_headroom, penalties, allocation_headroom);\n+#endif\n@@ -280,0 +434,1 @@\n+  double now =  _allocation_rate.last_sample_time();\n@@ -281,0 +436,10 @@\n+  double predicted_gc_time = predict_gc_time(now);\n+  double planned_gc_time;\n+  bool planned_gc_time_is_average;\n+  if (predicted_gc_time > avg_cycle_time) {\n+    planned_gc_time = predicted_gc_time;\n+    planned_gc_time_is_average = false;\n+  } else {\n+    planned_gc_time = avg_cycle_time;\n+    planned_gc_time_is_average = true;\n+  }\n@@ -282,5 +447,5 @@\n-  log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n-                _space_info->name(),\n-          avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n-  if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n-    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+  log_debug(gc)(\"%s: average GC time: %.2f ms, predicted GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+                _space_info->name(), avg_cycle_time * 1000, predicted_gc_time * 1000,\n+                byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n+  if (planned_gc_time > allocation_headroom \/ avg_alloc_rate) {\n+    log_info(gc)(\"Trigger (%s): Planned %s GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n@@ -288,1 +453,1 @@\n-                 _space_info->name(), avg_cycle_time * 1000,\n+                 _space_info->name(), planned_gc_time_is_average? \"(from average)\": \"(by linear prediction)\", planned_gc_time * 1000, \n@@ -298,0 +463,5 @@\n+#undef KELVIN_TRACE\n+#ifdef KELVIN_TRACE\n+    _history_count = 0;\n+    _history_oldest = 0;\n+#endif\n@@ -302,3 +472,4 @@\n-  if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n-    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n-                 _space_info->name(), avg_cycle_time * 1000,\n+  if (is_spiking && planned_gc_time > allocation_headroom \/ rate) {\n+    log_info(gc)(\"Trigger (%s): Planned %s GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s)\"\n+                 \" to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                 _space_info->name(), planned_gc_time_is_average? \"(from average)\": \"(by linear prediction)\", planned_gc_time * 1000,\n@@ -308,0 +479,16 @@\n+#ifdef KELVIN_TRACE\n+    log_info(gc)(\"Prehistory for instantaneous trigger at time %0.3f\", _history_timestamp);\n+    log_info(gc)(\"%10s:%9s%16s%18s%18s%16s%12s%12s%12s%16s%16s\", \"time_stamp\", \"interval\", \"avg_cycle_time\", \"avg_alloc_rate\", \"spike_alloc_rate\", \"spike_threshold\", \"headroom\", \"allocated\", \"available\", \"predicted_cycle\", \"accel_consumption\");\n+    for (uint i = 0; i < _history_count; i++) {\n+      uint index = _history_oldest + i;\n+      if (index >= HISTORY_SIZE) index = 0;\n+      log_info(gc)(\"%10.3f:%9.3f%16.3f%18.3f%18.3f%16.3f%12lu%12lu%12lu%16.3f%16lu\",\n+                   _historic_timestamp[index], _historic_interval[index],\n+                   _historic_avg_cycle_time[index], _historic_avg_alloc_rate[index], _historic_spike_alloc_rate[index],\n+                   _historic_spike_threshold[index], _historic_headroom[index], _historic_allocated[index],\n+                   _historic_available[index], _historic_predicted_cycle_time[index], _historic_accelerated_consumption[index]);\n+    }\n+    log_info(gc)(\"%10.3f:%9.3f%16.3f%18.3f%18.3f%16.3f%12lu%12lu%12lu\",\n+                 _history_timestamp, _history_interval, avg_cycle_time * 1000, avg_alloc_rate, rate,\n+                 _spike_threshold_sd, (unsigned long) allocation_headroom, (unsigned long) allocated, (unsigned long) available);\n+#endif\n@@ -312,1 +499,125 @@\n-  return ShenandoahHeuristics::should_start_gc();\n+  \/\/ Allocation rates may accelerate quickly during certain execution phase changes or due to unexpected growth in client demand\n+  \/\/ for a service.  While unbounded quadratic growth of consumption does not fully model this scenario, it is a much better\n+  \/\/ approximation than constant allocation rate within the domain of interest.\n+  \/\/\n+  \/\/ The SPIKE trigger above is not robust against rapidly changing allocation rates.  We have observed situations\n+  \/\/ such as the following:\n+  \/\/\n+  \/\/    Sample Time (s)      Allocation Rate (MB\/s)       Headroom (GB)\n+  \/\/       101.807                       0.0                  26.93\n+  \/\/       101.907                     477.6                  26.85\n+  \/\/       102.007                   3,206.0                  26.35\n+  \/\/       102.108                  23,797.8                  24.19   <--- accelerated spike triggers here\n+  \/\/       102.208                  24,164.5                  21.83\n+  \/\/       102.309                  23,965.0                  19.47\n+  \/\/       102.409                  24,624.35                 17.05   <--- without accelerated spike detection, we trigger here\n+  \/\/\n+  \/\/ The late trigger results in degenerated GC\n+  \/\/\n+  \/\/ The domain of interest is the sampling interval (in this case 100 ms) plus the average gc cycle time (in this case 750 ms).\n+  \/\/ The question we can ask at time 102.108 is:\n+  \/\/\n+  \/\/    Assume allocation rate is accelerating at a constant rate.  If we postpone the spike trigger until the subsequent\n+  \/\/    sample point, will there be enough memory to satisfy allocations that occur during the anticipated concurrent GC\n+  \/\/    cycle?  If not, we should trigger right now.\n+  \/\/\n+  \/\/ Outline of this heuristic triggering technique:\n+  \/\/\n+  \/\/  1. We remember the three most recent samples of spike allocation rate r0, r1, r2 samples at t0, t1, and t2\n+  \/\/  2. if r1 < r0 or r2 < r1, approximate Acceleration = 0.0, Rate = Max(r0, r1, r2)\n+  \/\/  3. Otherwise, use least squares method to compute best-fit line through rate vs time\n+  \/\/  4. The slope of this line represents Acceleration. The y-intercept of this line represents \"initial rate\"\n+  \/\/  5. Calculate modeled CurrentRate by substituting (t2 - t0) for t in the computed best-fit lint\n+  \/\/  6. Use Consumption = CurrentRate * GCTime + 1\/2 * Acceleration * GCTime * GCTime\n+  \/\/     (See High School physics discussions on constant acceleration: D = v0 * t + 1\/2 * a * t^2)\n+  \/\/  7. if Consumption exceeds headroom, trigger now\n+\n+  \/\/ Though larger sample size would improve quality of predictor, it would delay our trigger response as well.  A\n+  \/\/ SPIKE_ACCELERATION_SAMPLE_SIZE of 2 might work, but that would be more vulnerable to noise.\n+\n+  size_t consumption_accelerated = 0;\n+  if (rate > 0.0) {\n+    \/\/ We just collected a new spike allocation rate sample\n+\n+    uint new_sample_index = (_spike_acceleration_first_sample_index + _spike_acceleration_num_samples) % SPIKE_ACCELERATION_SAMPLE_SIZE;\n+\n+    _spike_acceleration_rate_samples[new_sample_index] = rate;\n+    _spike_acceleration_rate_timestamps[new_sample_index] = now;\n+    if (_spike_acceleration_num_samples == SPIKE_ACCELERATION_SAMPLE_SIZE) {\n+      _spike_acceleration_first_sample_index++;\n+      if (_spike_acceleration_first_sample_index == SPIKE_ACCELERATION_SAMPLE_SIZE) {\n+        _spike_acceleration_first_sample_index = 0;\n+      }\n+    } else {\n+      _spike_acceleration_num_samples++;\n+    }\n+\n+    \/\/ Do not require that the last sample is_spiking.  That makes us behave too conservatively.\n+    double acceleration;\n+    double current_alloc_rate;\n+#undef KELVIN_TRACE_CONSUMPTION\n+#ifdef KELVIN_TRACE_CONSUMPTION\n+    consumption_accelerated = accelerated_consumption(acceleration, current_alloc_rate, planned_gc_time,\n+                                                      available, spike_headroom, penalties, allocation_headroom);\n+#else\n+    consumption_accelerated = accelerated_consumption(acceleration, current_alloc_rate, planned_gc_time);\n+#endif\n+    if (consumption_accelerated > allocation_headroom) {\n+      size_t size_t_acceleration = (size_t) acceleration;\n+      size_t size_t_alloc_rate = (size_t) current_alloc_rate;\n+      log_info(gc)(\"Trigger (%s): Accelerated consumption (\" SIZE_FORMAT \"%s) exceeds free headroom (\" SIZE_FORMAT \"%s) at \"\n+                   \"current rate (\" SIZE_FORMAT \"%s\/s) with acceleration (\" SIZE_FORMAT \"%s\/s\/s) for planned %s GC time (%.2f ms)\",\n+                   _space_info->name(),\n+                   byte_size_in_proper_unit(consumption_accelerated), proper_unit_for_byte_size(consumption_accelerated),\n+                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                   byte_size_in_proper_unit(size_t_alloc_rate), proper_unit_for_byte_size(size_t_alloc_rate),\n+                   byte_size_in_proper_unit(size_t_acceleration), proper_unit_for_byte_size(size_t_acceleration),\n+                   planned_gc_time_is_average? \"(from average)\": \"(by linear prediction)\", planned_gc_time * 1000);\n+      _spike_acceleration_num_samples = 0;\n+      _spike_acceleration_first_sample_index = 0;\n+\n+      \/\/ Count this as a form of RATE trigger for purposes of adjusting heuristic triggering configuration because this\n+      \/\/ trigger is influenced more by margin_of_error_sd than by spike_threshold_sd.\n+      _last_trigger = RATE;\n+      return true;\n+    }\n+  }\n+\n+#ifdef KELVIN_TRACE\n+  if (ShenandoahHeuristics::should_start_gc()) {\n+    _history_count = 0;\n+    _history_oldest = 0;\n+    return true;\n+  } else {\n+    size_t _history_index = (_history_oldest + _history_count) % HISTORY_SIZE;\n+    _historic_avg_cycle_time[_history_index] = avg_cycle_time * 1000;\n+    _historic_predicted_cycle_time[_history_index] = predicted_gc_time * 1000;\n+    _historic_avg_alloc_rate[_history_index] = avg_alloc_rate;\n+    _historic_spike_alloc_rate[_history_index] = rate;\n+    _historic_spike_threshold[_history_index] = _spike_threshold_sd;\n+    _historic_timestamp[_history_index] = _history_timestamp;\n+    _historic_interval[_history_index] = _history_interval;\n+    _historic_headroom[_history_index] = (unsigned long) allocation_headroom;\n+    _historic_allocated[_history_index] = (unsigned long) allocated;\n+    _historic_available[_history_index] = (unsigned long) available;\n+    _historic_accelerated_consumption[_history_index] = (unsigned long) consumption_accelerated;\n+    if (_history_count < HISTORY_SIZE) {\n+      \/\/ no wrap around, so no need to adjust _history_oldest\n+      _history_count++;\n+    } else {\n+      \/\/ We're already full.  Don't increment _history_count.  Do increment _history_oldest.\n+      _history_oldest++;\n+      if (_history_oldest >= HISTORY_SIZE) {\n+        _history_oldest = 0;\n+      }\n+    }\n+    return false;\n+  }\n+#endif\n+  if (ShenandoahHeuristics::should_start_gc()) {\n+    _spike_acceleration_num_samples = 0;\n+    _spike_acceleration_first_sample_index = 0;\n+    return true;\n+  } else {\n+    return false;\n+  }\n@@ -331,0 +642,81 @@\n+\/\/ This is only called if a new rate sample has been gathered (e.g. ten times per second).\n+\/\/ There is no adjustment for standard deviation of the accelerated rate prediction.\n+#ifdef KELVIN_TRACE_CONSUMPTION\n+size_t ShenandoahAdaptiveHeuristics::accelerated_consumption(double& acceleration, double& current_rate, double predicted_cycle_time,\n+                                                             size_t available, size_t spike_headroom, size_t penalties, size_t headroom) const\n+#else\n+size_t ShenandoahAdaptiveHeuristics::accelerated_consumption(double& acceleration, double& current_rate, double predicted_cycle_time) const\n+#endif                                                             \n+{\n+  double *x_array = (double *) alloca(SPIKE_ACCELERATION_SAMPLE_SIZE * sizeof(double));\n+  double *y_array = (double *) alloca(SPIKE_ACCELERATION_SAMPLE_SIZE * sizeof(double));\n+  double x_sum = 0.0;\n+  double y_sum = 0.0;\n+  double y_max = 0.0;\n+\n+  for (uint i = 0; i < _spike_acceleration_num_samples; i++) {\n+    uint index = (_spike_acceleration_first_sample_index + i) % SPIKE_ACCELERATION_SAMPLE_SIZE;\n+    x_array[i] = _spike_acceleration_rate_timestamps[index];\n+    x_sum += x_array[i];\n+    y_array[i] = _spike_acceleration_rate_samples[index];\n+    if (y_array[i] > y_max) {\n+      y_max = y_array[i];\n+    }\n+    y_sum += y_array[i];\n+  }\n+  bool spikes_increasing = true;\n+  for (uint i = 1; i < _spike_acceleration_num_samples; i++) {\n+    if (y_array[i] <= y_array[i-1]) {\n+      spikes_increasing = false;\n+      break;\n+    }\n+  }\n+  if (!spikes_increasing || (_spike_acceleration_num_samples < SPIKE_ACCELERATION_SAMPLE_SIZE)) {\n+    \/\/ bytes allocated per second.  Use the max among collected samples.\n+    current_rate = y_max;\n+    acceleration = 0.0;\n+  } else {\n+    double *xy_array = (double *) alloca(SPIKE_ACCELERATION_SAMPLE_SIZE * sizeof(double));\n+    double *x2_array = (double *) alloca(SPIKE_ACCELERATION_SAMPLE_SIZE * sizeof(double));\n+    double xy_sum = 0.0;\n+    double x2_sum = 0.0;\n+    for (uint i = 0; i < SPIKE_ACCELERATION_SAMPLE_SIZE; i++) {\n+      xy_array[i] = x_array[i] * y_array[i];\n+      xy_sum += xy_array[i];\n+      x2_array[i] = x_array[i] * x_array[i];\n+      x2_sum += x2_array[i];\n+    }\n+    \/\/ Find the best-fit least-squares linear representation of rate vs time\n+    double m;                 \/* slope *\/\n+    double b;                 \/* y-intercept *\/\n+    m = (SPIKE_ACCELERATION_SAMPLE_SIZE * xy_sum - x_sum * y_sum) \/ (SPIKE_ACCELERATION_SAMPLE_SIZE * x2_sum - x_sum * x_sum);\n+    b = (y_sum - m * x_sum) \/ SPIKE_ACCELERATION_SAMPLE_SIZE;\n+    acceleration = m;\n+    current_rate = m * x_array[SPIKE_ACCELERATION_SAMPLE_SIZE - 1] + b;\n+#ifdef KELVIN_TRACE_CONSUMPTION\n+    log_info(gc)(\"Best-fit line has m %.3f, b: %.3f\", m, b);\n+    log_info(gc)(\"%8s%16s%16s%16s\", \"sample\", \"timestamp\", \"predicted rate\", \"actual rate\");\n+    for (uint i = 0; i < SPIKE_ACCELERATION_SAMPLE_SIZE; i++) {\n+      log_info(gc)(\"%8d%16.3f%16.3f%16.3f\", i, x_array[i], m * x_array[i] + b, y_array[i]);\n+    }\n+#endif\n+  }\n+\n+  double time_delta = _allocation_rate.interval() + predicted_cycle_time;\n+  size_t bytes_to_be_consumed = (size_t) (current_rate * time_delta + 0.5 * acceleration * time_delta * time_delta);\n+#ifdef KELVIN_TRACE_CONSUMPTION\n+  if (acceleration > 0.0) {\n+    log_info(gc)(\"Consume \" SIZE_FORMAT \"%s after %.3fs vs. headroom \" SIZE_FORMAT \"%s=\" SIZE_FORMAT \"%s-(\" SIZE_FORMAT \"%s+\" SIZE_FORMAT \"%s)\",\n+                 byte_size_in_proper_unit(bytes_to_be_consumed), proper_unit_for_byte_size(bytes_to_be_consumed),\n+                 time_delta,\n+                 byte_size_in_proper_unit(headroom), proper_unit_for_byte_size(headroom),\n+                 byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                 byte_size_in_proper_unit(spike_headroom), proper_unit_for_byte_size(spike_headroom),\n+                 byte_size_in_proper_unit(penalties), proper_unit_for_byte_size(penalties));\n+  }\n+#endif\n+  return bytes_to_be_consumed;\n+}\n+\n+#undef KELVIN_DEBUG_SPIKE_THRESHOLD\n+\n@@ -333,0 +725,4 @@\n+#ifdef KELVIN_DEBUG_SPIKE_THRESHOLD\n+  \/\/ I want to see what causes us to increase margin of error, and what causes us to shrink it.\n+  log_info(gc)(\"KELVIN adjusts margin of error sd to %.2f\", _margin_of_error_sd);\n+#endif\n@@ -338,0 +734,4 @@\n+#ifdef KELVIN_DEBUG_SPIKE_THRESHOLD\n+  \/\/ I want to see what causes us to increase the threshold, and what causes us to shrink it.\n+  log_info(gc)(\"KELVIN adjusts spike threshold to %.2f\", _spike_threshold_sd);\n+#endif\n@@ -392,2 +792,4 @@\n-    \/\/ There is a small chance that that rate has already been sampled, but it\n-    \/\/ seems not to matter in practice.\n+    \/\/ There is a small chance that that rate has already been sampled, but it seems not to matter in practice.\n+    \/\/ z_score reports how close this measure is to the average.  A value between -1 and 1 means we are within 1\n+    \/\/ standard deviation.  A value between -3 and +3 means we are within 3 standard deviations.  We care only if\n+    \/\/  the spike is above the mean.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":418,"deletions":16,"binary":false,"changes":434,"status":"modified"},{"patch":"@@ -46,0 +46,6 @@\n+  double interval() const {\n+    return _interval_sec;\n+  }\n+  double last_sample_time() const {\n+    return _last_sample_time;\n+  }\n@@ -84,1 +90,4 @@\n-\n+#undef KELVIN_TRACE_HEADER\n+#ifdef KELVIN_TRACE_HEADER\n+  void timestamp_for_sample(double timestamp, double interval);\n+#endif\n@@ -102,0 +111,3 @@\n+  const static size_t SPIKE_ACCELERATION_SAMPLE_SIZE;\n+  const static size_t GC_TIME_SAMPLE_SIZE;\n+\n@@ -116,0 +128,7 @@\n+#undef KELVIN_TRACE_CONSUMPTION_X\n+#ifdef KELVIN_TRACE_CONSUMPTION_X\n+        size_t accelerated_consumption(double& acceleration, double& current_rate, double predicted_cycle_time,\n+                                       size_t available, size_t spike_headroom, size_t penalties, size_t headroom) const;\n+#else\n+  size_t accelerated_consumption(double& acceleration, double& current_rate, double predicted_cycle_time) const;\n+#endif\n@@ -131,1 +150,1 @@\n-  \/\/ of concurrent GCs.\n+  \/\/ of concurrent GCs because more scenarios will be seen as spiking.\n@@ -145,0 +164,25 @@\n+  \/\/ Keep track of GC_TIME_SAMPLE_SIZE most recent concurrent GC cycle times\n+  uint _gc_time_first_sample_index;\n+  uint _gc_time_num_samples;\n+  double* const _gc_time_timestamps;\n+  double* const _gc_time_samples;\n+  double* const _gc_time_xy;    \/\/ timestamp * sample\n+  double* const _gc_time_xx;    \/\/ timestamp squared\n+  double _gc_time_sum_of_timestamps;\n+  double _gc_time_sum_of_samples;\n+  double _gc_time_sum_of_xy;\n+  double _gc_time_sum_of_xx;\n+\n+  double _gc_time_m;            \/\/ slope\n+  double _gc_time_b;            \/\/ y-intercept\n+  double _gc_time_sd;           \/\/ sd on deviance from prediction\n+\n+  void add_gc_time(double timestamp_at_start, double duration);\n+  double predict_gc_time(double timestamp_at_start);\n+\n+  \/\/ Keep track of SPIKE_ACCELERATION_SAMPLE_SIZE most recent spike allocation rate measurements\n+  uint _spike_acceleration_first_sample_index;\n+  uint _spike_acceleration_num_samples;\n+  double* const _spike_acceleration_rate_samples;\n+  double* const _spike_acceleration_rate_timestamps;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":46,"deletions":2,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -85,0 +85,7 @@\n+#undef KELVIN_TRACE\n+#ifdef KELVIN_TRACE\n+  if (which_set == Mutator) {\n+    log_info(gc, free)(\"Mutator CON$UME$: \" SIZE_FORMAT \", remaining available: \" SIZE_FORMAT,\n+                       bytes, _capacity_of[Mutator] - _used_by[Mutator]);\n+  }\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -26,0 +26,2 @@\n+#undef KELVIN_TRACE\n+\n@@ -33,0 +35,3 @@\n+#ifdef KELVIN_TRACE\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#endif\n@@ -67,0 +72,6 @@\n+#undef KELVIN_TRACE\n+#ifdef KELVIN_TRACE\n+static double _most_recent_timestamp;\n+static double _next_sleep_interval;\n+#endif\n+\n@@ -70,0 +81,4 @@\n+#ifdef KELVIN_TRACE\n+  ShenandoahAdaptiveHeuristics* adaptive_heuristics =\n+         (ShenandoahAdaptiveHeuristics*)ShenandoahHeap::heap()->young_heuristics();\n+#endif\n@@ -76,0 +91,3 @@\n+#ifdef KELVIN_TRACE\n+          log_info(gc)(\"Acceptance after sleeping %.3f following timestamp %.3f\", _next_sleep_interval, _most_recent_timestamp);\n+#endif\n@@ -79,0 +97,3 @@\n+#ifdef KELVIN_TRACE\n+        adaptive_heuristics->timestamp_for_sample(_most_recent_timestamp, _next_sleep_interval);\n+#endif\n@@ -80,0 +101,3 @@\n+#ifdef KELVIN_TRACE\n+          log_info(gc)(\"Acceptance after sleeping %.3f following timestamp %.3f\", _next_sleep_interval, _most_recent_timestamp);\n+#endif\n@@ -82,0 +106,3 @@\n+#ifdef KELVIN_TRACE\n+          log_info(gc)(\"Acceptance after sleeping %.3f following timestamp %.3f\", _next_sleep_interval, _most_recent_timestamp);\n+#endif\n@@ -86,0 +113,3 @@\n+#ifdef KELVIN_TRACE\n+      adaptive_heuristics->timestamp_for_sample(_most_recent_timestamp, _next_sleep_interval);\n+#endif\n@@ -87,0 +117,3 @@\n+#ifdef KELVIN_TRACE\n+        log_info(gc)(\"Acceptance after sleeping %.3f following timestamp %.3f\", _next_sleep_interval, _most_recent_timestamp);\n+#endif\n@@ -131,1 +164,3 @@\n-\n+#ifdef KELVIN_TRACE\n+  _most_recent_timestamp = current;\n+#endif\n@@ -138,0 +173,3 @@\n+#ifdef KELVIN_TRACE\n+  _next_sleep_interval = _sleep;\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":39,"deletions":1,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -179,1 +179,1 @@\n-  product(uintx, ShenandoahLearningSteps, 10, EXPERIMENTAL,                 \\\n+  product(uintx, ShenandoahLearningSteps, 5, EXPERIMENTAL,                  \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}