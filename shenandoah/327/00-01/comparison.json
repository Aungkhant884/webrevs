{"files":[{"patch":"@@ -63,1 +63,6 @@\n-const size_t ShenandoahAdaptiveHeuristics::SAMPLE_SIZE = 3;\n+const size_t ShenandoahAdaptiveHeuristics::SPIKE_ACCELERATION_SAMPLE_SIZE = 3;\n+\n+\/\/ Separately, we keep track of the average gc time.  We track the most recent GC_TIME_SAMPLE_SIZE GC times in order to\n+\/\/ detect changing trends in the time required to perform GC.  If the number of samples is too large, we will not be as\n+\/\/ responsive to change trends, as the best-fit line will look more like an average.\n+const size_t ShenandoahAdaptiveHeuristics::GC_TIME_SAMPLE_SIZE = 4;\n@@ -71,4 +76,17 @@\n-  _first_sample_index(0),\n-  _num_samples(0),\n-  _rate_samples(NEW_C_HEAP_ARRAY(double, SAMPLE_SIZE, mtGC)),\n-  _rate_timestamps(NEW_C_HEAP_ARRAY(double, SAMPLE_SIZE, mtGC)) { }\n+  _gc_time_first_sample_index(0),\n+  _gc_time_num_samples(0),\n+  _gc_time_timestamps(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_samples(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_xy(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_xx(NEW_C_HEAP_ARRAY(double, GC_TIME_SAMPLE_SIZE, mtGC)),\n+  _gc_time_sum_of_timestamps(0),\n+  _gc_time_sum_of_samples(0),\n+  _gc_time_sum_of_xy(0),\n+  _gc_time_sum_of_xx(0),\n+  _gc_time_m(0.0),\n+  _gc_time_b(0.0),\n+  _gc_time_sd(0.0),\n+  _spike_acceleration_first_sample_index(0),\n+  _spike_acceleration_num_samples(0),\n+  _spike_acceleration_rate_samples(NEW_C_HEAP_ARRAY(double, SPIKE_ACCELERATION_SAMPLE_SIZE, mtGC)),\n+  _spike_acceleration_rate_timestamps(NEW_C_HEAP_ARRAY(double, SPIKE_ACCELERATION_SAMPLE_SIZE, mtGC)) { }\n@@ -77,2 +95,6 @@\n-  FREE_C_HEAP_ARRAY(double, _rate_samples);\n-  FREE_C_HEAP_ARRAY(double, _rate_timestamps);\n+  FREE_C_HEAP_ARRAY(double, _spike_acceleration_rate_samples);\n+  FREE_C_HEAP_ARRAY(double, _spike_acceleration_rate_timestamps);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_timestamps);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_samples);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_xy);\n+  FREE_C_HEAP_ARRAY(double, _gc_time_xx);\n@@ -144,0 +166,85 @@\n+#undef KELVIN_NEW_CODE\n+void ShenandoahAdaptiveHeuristics::add_gc_time(double timestamp, double gc_time) {\n+\n+  \/\/ Update best-fit linear predictor of GC time\n+  uint index = (_gc_time_first_sample_index + _gc_time_num_samples) % GC_TIME_SAMPLE_SIZE;\n+  if (_gc_time_num_samples == GC_TIME_SAMPLE_SIZE) {\n+    _gc_time_sum_of_timestamps -= _gc_time_timestamps[index];\n+    _gc_time_sum_of_samples -= _gc_time_samples[index];\n+    _gc_time_sum_of_xy -= _gc_time_xy[index];\n+    _gc_time_sum_of_xx -= _gc_time_xx[index];\n+  }\n+  _gc_time_timestamps[index] = timestamp;\n+  _gc_time_samples[index] = gc_time;\n+  _gc_time_xy[index] = timestamp * gc_time;\n+  _gc_time_xx[index] = timestamp * timestamp;\n+\n+  _gc_time_sum_of_timestamps += _gc_time_timestamps[index];\n+  _gc_time_sum_of_samples += _gc_time_samples[index];\n+  _gc_time_sum_of_xy += _gc_time_xy[index];\n+  _gc_time_sum_of_xx += _gc_time_xx[index];\n+\n+  if (_gc_time_num_samples < GC_TIME_SAMPLE_SIZE) {\n+    _gc_time_num_samples++;\n+  } else {\n+    _gc_time_first_sample_index = (_gc_time_first_sample_index + 1) % GC_TIME_SAMPLE_SIZE;\n+  }\n+\n+  if (_gc_time_num_samples == 1) {\n+    \/\/ The predictor is constant (horizontal line)\n+    _gc_time_m = 0;\n+    _gc_time_b = gc_time;\n+    _gc_time_sd = 0.0;\n+  } else if (_gc_time_num_samples == 2) {\n+    \/\/ Two points define a line\n+    double delta_y = gc_time - _gc_time_samples[_gc_time_first_sample_index];\n+    double delta_x = timestamp - _gc_time_timestamps[_gc_time_first_sample_index];\n+\n+    _gc_time_m = delta_y \/ delta_x;\n+\n+#ifdef KELVIN_NEW_CODE\n+    log_info(gc)(\"For 2 samples with first index: %u: delta_y is %.3f = %.3f - %.3f, delta_x is %.3f = %.3f - %.3f\",\n+                 _gc_time_first_sample_index,\n+                 delta_y, timestamp, _gc_time_timestamps[_gc_time_first_sample_index],\n+                 delta_x, gc_time, _gc_time_samples[_gc_time_first_sample_index]);\n+#endif\n+    \/\/ y = mx + b\n+    \/\/ so b = y0 - mx0\n+    _gc_time_b = gc_time - _gc_time_m * timestamp;\n+    _gc_time_sd = 0.0;\n+  } else {\n+    _gc_time_m = ((_gc_time_num_samples * _gc_time_sum_of_xy - _gc_time_sum_of_timestamps * _gc_time_sum_of_samples) \/\n+                  (_gc_time_num_samples * _gc_time_sum_of_xx - _gc_time_sum_of_timestamps * _gc_time_sum_of_timestamps));\n+    _gc_time_b = (_gc_time_sum_of_samples - _gc_time_m * _gc_time_sum_of_timestamps) \/ _gc_time_num_samples;\n+#ifdef KELVIN_NEW_CODE\n+    log_info(gc)(\"num_samples: %u, sum_of_xy: %.3f, sum of timestamps: %.3f, sum_of_samples: %.3f, sum_of_xx: %.3f\",\n+                 _gc_time_num_samples, _gc_time_sum_of_xy, _gc_time_sum_of_timestamps,\n+                 _gc_time_sum_of_samples, _gc_time_sum_of_xx);\n+    log_info(gc)(\"%14s%14s%14s%14s\", \"actual x\", \"actual y\", \"predict y\", \"deviation\");\n+#endif\n+\n+    double sum_of_squared_deviations = 0.0;\n+    for (size_t i = 0; i < _gc_time_num_samples; i++) {\n+      uint index = (_gc_time_first_sample_index + i) % GC_TIME_SAMPLE_SIZE;\n+      double x = _gc_time_timestamps[index];\n+      double predicted_y = _gc_time_m * x + _gc_time_b;\n+      double deviation = predicted_y - _gc_time_samples[index];\n+\n+#ifdef KELVIN_NEW_CODE\n+      log_info(gc)(\"%14.3f%14.3f%14.3f%14.3f\", x, _gc_time_samples[index], predicted_y, deviation);\n+#endif\n+      sum_of_squared_deviations = deviation * deviation;\n+    }\n+    _gc_time_sd = sqrt(sum_of_squared_deviations \/ _gc_time_num_samples);\n+  }\n+#ifdef KELVIN_NEW_CODE\n+  log_info(gc)(\"@ %.3f, add gc_time[%u]: %.3f, new slope: %.3f, new y-intercept: %.3f, _gc_time_sd: %.3f\",\n+               timestamp, _gc_time_num_samples, gc_time, _gc_time_m, _gc_time_b, _gc_time_sd);\n+#endif\n+}\n+\n+double ShenandoahAdaptiveHeuristics::predict_gc_time(double timestamp_at_start) {\n+  double result = _gc_time_m * timestamp_at_start + _gc_time_b + _gc_time_sd * _margin_of_error_sd;;\n+  return result;\n+}\n+\n@@ -147,0 +254,4 @@\n+  if (!abbreviated) {\n+    add_gc_time(_cycle_start, elapsed_cycle_time());\n+  }\n+\n@@ -193,4 +304,0 @@\n-#define KELVIN_SEE_STATS\n-#ifdef KELVIN_SEE_STATS\n-    log_info(gc)(\"kelvin adjusts last trigger with arg: %.5f\", z_score \/ -100);\n-#endif\n@@ -221,2 +328,2 @@\n-#undef KELVIN_TRACE\n-#ifdef KELVIN_TRACE\n+#undef KELVIN_TRACE_INFRA\n+#ifdef KELVIN_TRACE_INFRA\n@@ -237,0 +344,1 @@\n+static double _historic_predicted_cycle_time[HISTORY_SIZE];\n@@ -245,0 +353,1 @@\n+static unsigned long _historic_accelerated_consumption[HISTORY_SIZE];\n@@ -314,1 +423,0 @@\n-\n@@ -320,0 +428,5 @@\n+#undef KELVIN_NOISE\n+#ifdef KELVIN_NOISE\n+  log_info(gc)(\"available: \" SIZE_FORMAT \", spike_headroom: \" SIZE_FORMAT \", penalties: \" SIZE_FORMAT \" becomes allocation_headroom: \" SIZE_FORMAT,\n+               available, spike_headroom, penalties, allocation_headroom);\n+#endif\n@@ -321,0 +434,1 @@\n+  double now =  _allocation_rate.last_sample_time();\n@@ -322,0 +436,10 @@\n+  double predicted_gc_time = predict_gc_time(now);\n+  double planned_gc_time;\n+  bool planned_gc_time_is_average;\n+  if (predicted_gc_time > avg_cycle_time) {\n+    planned_gc_time = predicted_gc_time;\n+    planned_gc_time_is_average = false;\n+  } else {\n+    planned_gc_time = avg_cycle_time;\n+    planned_gc_time_is_average = true;\n+  }\n@@ -323,5 +447,5 @@\n-  log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n-                _space_info->name(),\n-          avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n-  if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n-    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+  log_debug(gc)(\"%s: average GC time: %.2f ms, predicted GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+                _space_info->name(), avg_cycle_time * 1000, predicted_gc_time * 1000,\n+                byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n+  if (planned_gc_time > allocation_headroom \/ avg_alloc_rate) {\n+    log_info(gc)(\"Trigger (%s): Planned %s GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n@@ -329,1 +453,1 @@\n-                 _space_info->name(), avg_cycle_time * 1000,\n+                 _space_info->name(), planned_gc_time_is_average? \"(from average)\": \"(by linear prediction)\", planned_gc_time * 1000, \n@@ -339,0 +463,1 @@\n+#undef KELVIN_TRACE\n@@ -347,3 +472,4 @@\n-  if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n-    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n-                 _space_info->name(), avg_cycle_time * 1000,\n+  if (is_spiking && planned_gc_time > allocation_headroom \/ rate) {\n+    log_info(gc)(\"Trigger (%s): Planned %s GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s)\"\n+                 \" to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                 _space_info->name(), planned_gc_time_is_average? \"(from average)\": \"(by linear prediction)\", planned_gc_time * 1000,\n@@ -355,1 +481,1 @@\n-    log_info(gc)(\"%10s:%9s%16s%18s%18s%16s%12s%12s%12s\", \"time_stamp\", \"interval\", \"avg_cycle_time\", \"avg_alloc_rate\", \"spike_alloc_rate\", \"spike_threshold\", \"headroom\", \"allocated\", \"available\");\n+    log_info(gc)(\"%10s:%9s%16s%18s%18s%16s%12s%12s%12s%16s%16s\", \"time_stamp\", \"interval\", \"avg_cycle_time\", \"avg_alloc_rate\", \"spike_alloc_rate\", \"spike_threshold\", \"headroom\", \"allocated\", \"available\", \"predicted_cycle\", \"accel_consumption\");\n@@ -359,1 +485,2 @@\n-      log_info(gc)(\"%10.3f:%9.3f%16.3f%18.3f%18.3f%16.3f%12lu%12lu%12lu\", _historic_timestamp[index], _historic_interval[index],\n+      log_info(gc)(\"%10.3f:%9.3f%16.3f%18.3f%18.3f%16.3f%12lu%12lu%12lu%16.3f%16lu\",\n+                   _historic_timestamp[index], _historic_interval[index],\n@@ -361,1 +488,2 @@\n-                   _historic_spike_threshold[index], _historic_headroom[index], _historic_allocated[index], _historic_available[index]);\n+                   _historic_spike_threshold[index], _historic_headroom[index], _historic_allocated[index],\n+                   _historic_available[index], _historic_predicted_cycle_time[index], _historic_accelerated_consumption[index]);\n@@ -408,1 +536,1 @@\n-  \/\/ SAMPLE_SIZE of 2 might work, but that would be more vulnerable to noise.\n+  \/\/ SPIKE_ACCELERATION_SAMPLE_SIZE of 2 might work, but that would be more vulnerable to noise.\n@@ -410,0 +538,1 @@\n+  size_t consumption_accelerated = 0;\n@@ -413,1 +542,1 @@\n-    uint new_sample_index = (_first_sample_index + _num_samples) % SAMPLE_SIZE;\n+    uint new_sample_index = (_spike_acceleration_first_sample_index + _spike_acceleration_num_samples) % SPIKE_ACCELERATION_SAMPLE_SIZE;\n@@ -415,6 +544,6 @@\n-    _rate_samples[new_sample_index] = rate;\n-    _rate_timestamps[new_sample_index] = _allocation_rate.last_sample_time();\n-    if (_num_samples == SAMPLE_SIZE) {\n-      _first_sample_index++;\n-      if (_first_sample_index == SAMPLE_SIZE) {\n-        _first_sample_index = 0;\n+    _spike_acceleration_rate_samples[new_sample_index] = rate;\n+    _spike_acceleration_rate_timestamps[new_sample_index] = now;\n+    if (_spike_acceleration_num_samples == SPIKE_ACCELERATION_SAMPLE_SIZE) {\n+      _spike_acceleration_first_sample_index++;\n+      if (_spike_acceleration_first_sample_index == SPIKE_ACCELERATION_SAMPLE_SIZE) {\n+        _spike_acceleration_first_sample_index = 0;\n@@ -423,1 +552,1 @@\n-      _num_samples++;\n+      _spike_acceleration_num_samples++;\n@@ -426,23 +555,28 @@\n-    if (is_spiking) {\n-      double acceleration;\n-      double current_alloc_rate;\n-      size_t consumption = accelerated_consumption(acceleration, current_alloc_rate, avg_cycle_time);\n-      if (consumption > allocation_headroom) {\n-        size_t size_t_acceleration = (size_t) acceleration;\n-        size_t size_t_alloc_rate = (size_t) current_alloc_rate;\n-        log_info(gc)(\"Trigger (%s): Accelerated consumption (\" SIZE_FORMAT \"%s) exceeds free headroom (\" SIZE_FORMAT \"%s) at \"\n-                     \"current rate (\" SIZE_FORMAT \"%s\/s) with acceleration (\" SIZE_FORMAT \"%s\/s\/s) for average GC time (%.2f ms)\",\n-                     _space_info->name(),\n-                     byte_size_in_proper_unit(consumption), proper_unit_for_byte_size(consumption),\n-                     byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-                     byte_size_in_proper_unit(size_t_alloc_rate), proper_unit_for_byte_size(size_t_alloc_rate),\n-                     byte_size_in_proper_unit(size_t_acceleration), proper_unit_for_byte_size(size_t_acceleration),\n-                     avg_cycle_time * 1000);\n-        _num_samples = 0;\n-        _first_sample_index = 0;\n-\n-        \/\/ Count this as an OTHER trigger: we do NOT want to adjust spike threshold or margin of error since\n-        \/\/  acceleration of allocation rate is intended to be relatively rare.\n-        _last_trigger = OTHER;\n-        return true;\n-      }\n+    \/\/ Do not require that the last sample is_spiking.  That makes us behave too conservatively.\n+    double acceleration;\n+    double current_alloc_rate;\n+#undef KELVIN_TRACE_CONSUMPTION\n+#ifdef KELVIN_TRACE_CONSUMPTION\n+    consumption_accelerated = accelerated_consumption(acceleration, current_alloc_rate, planned_gc_time,\n+                                                      available, spike_headroom, penalties, allocation_headroom);\n+#else\n+    consumption_accelerated = accelerated_consumption(acceleration, current_alloc_rate, planned_gc_time);\n+#endif\n+    if (consumption_accelerated > allocation_headroom) {\n+      size_t size_t_acceleration = (size_t) acceleration;\n+      size_t size_t_alloc_rate = (size_t) current_alloc_rate;\n+      log_info(gc)(\"Trigger (%s): Accelerated consumption (\" SIZE_FORMAT \"%s) exceeds free headroom (\" SIZE_FORMAT \"%s) at \"\n+                   \"current rate (\" SIZE_FORMAT \"%s\/s) with acceleration (\" SIZE_FORMAT \"%s\/s\/s) for planned %s GC time (%.2f ms)\",\n+                   _space_info->name(),\n+                   byte_size_in_proper_unit(consumption_accelerated), proper_unit_for_byte_size(consumption_accelerated),\n+                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                   byte_size_in_proper_unit(size_t_alloc_rate), proper_unit_for_byte_size(size_t_alloc_rate),\n+                   byte_size_in_proper_unit(size_t_acceleration), proper_unit_for_byte_size(size_t_acceleration),\n+                   planned_gc_time_is_average? \"(from average)\": \"(by linear prediction)\", planned_gc_time * 1000);\n+      _spike_acceleration_num_samples = 0;\n+      _spike_acceleration_first_sample_index = 0;\n+\n+      \/\/ Count this as a form of RATE trigger for purposes of adjusting heuristic triggering configuration because this\n+      \/\/ trigger is influenced more by margin_of_error_sd than by spike_threshold_sd.\n+      _last_trigger = RATE;\n+      return true;\n@@ -460,0 +594,1 @@\n+    _historic_predicted_cycle_time[_history_index] = predicted_gc_time * 1000;\n@@ -468,0 +603,1 @@\n+    _historic_accelerated_consumption[_history_index] = (unsigned long) consumption_accelerated;\n@@ -482,2 +618,2 @@\n-    _num_samples = 0;\n-    _first_sample_index = 0;\n+    _spike_acceleration_num_samples = 0;\n+    _spike_acceleration_first_sample_index = 0;\n@@ -506,3 +642,11 @@\n-size_t ShenandoahAdaptiveHeuristics::accelerated_consumption(double& acceleration, double& current_rate, double avg_cycle_time) {\n-  double *x_array = (double *) alloca(SAMPLE_SIZE * sizeof(double));\n-  double *y_array = (double *) alloca(SAMPLE_SIZE * sizeof(double));\n+\/\/ This is only called if a new rate sample has been gathered (e.g. ten times per second).\n+\/\/ There is no adjustment for standard deviation of the accelerated rate prediction.\n+#ifdef KELVIN_TRACE_CONSUMPTION\n+size_t ShenandoahAdaptiveHeuristics::accelerated_consumption(double& acceleration, double& current_rate, double predicted_cycle_time,\n+                                                             size_t available, size_t spike_headroom, size_t penalties, size_t headroom) const\n+#else\n+size_t ShenandoahAdaptiveHeuristics::accelerated_consumption(double& acceleration, double& current_rate, double predicted_cycle_time) const\n+#endif                                                             \n+{\n+  double *x_array = (double *) alloca(SPIKE_ACCELERATION_SAMPLE_SIZE * sizeof(double));\n+  double *y_array = (double *) alloca(SPIKE_ACCELERATION_SAMPLE_SIZE * sizeof(double));\n@@ -511,0 +655,1 @@\n+  double y_max = 0.0;\n@@ -512,12 +657,3 @@\n-#undef KELVIN_TRACE_CONSUMPTION\n-#ifdef KELVIN_TRACE_CONSUMPTION\n-  log_info(gc)(\"accelerated_consumption(), num_samples: %u\", _num_samples);\n-  for (uint i = 0; i < _num_samples; i++) {\n-    uint index = (_first_sample_index + i) % SAMPLE_SIZE;\n-    log_info(gc)(\"sample[%d]: %.3f @ time %.3f\", i, _rate_samples[index], _rate_timestamps[index]);\n-  }\n-#endif\n-\n-  for (uint i = 0; i < _num_samples; i++) {\n-    uint index = (_first_sample_index + i) % SAMPLE_SIZE;\n-    x_array[i] = _rate_timestamps[index];\n+  for (uint i = 0; i < _spike_acceleration_num_samples; i++) {\n+    uint index = (_spike_acceleration_first_sample_index + i) % SPIKE_ACCELERATION_SAMPLE_SIZE;\n+    x_array[i] = _spike_acceleration_rate_timestamps[index];\n@@ -525,1 +661,4 @@\n-    y_array[i] = _rate_samples[index];\n+    y_array[i] = _spike_acceleration_rate_samples[index];\n+    if (y_array[i] > y_max) {\n+      y_max = y_array[i];\n+    }\n@@ -529,1 +668,1 @@\n-  for (uint i = 1; i < _num_samples; i++) {\n+  for (uint i = 1; i < _spike_acceleration_num_samples; i++) {\n@@ -535,1 +674,3 @@\n-  if (!spikes_increasing || (_num_samples < SAMPLE_SIZE)) {\n+  if (!spikes_increasing || (_spike_acceleration_num_samples < SPIKE_ACCELERATION_SAMPLE_SIZE)) {\n+    \/\/ bytes allocated per second.  Use the max among collected samples.\n+    current_rate = y_max;\n@@ -537,1 +678,0 @@\n-    current_rate = y_sum \/ _num_samples;\n@@ -539,2 +679,2 @@\n-    double *xy_array = (double *) alloca(SAMPLE_SIZE * sizeof(double));\n-    double *x2_array = (double *) alloca(SAMPLE_SIZE * sizeof(double));\n+    double *xy_array = (double *) alloca(SPIKE_ACCELERATION_SAMPLE_SIZE * sizeof(double));\n+    double *x2_array = (double *) alloca(SPIKE_ACCELERATION_SAMPLE_SIZE * sizeof(double));\n@@ -543,1 +683,1 @@\n-    for (uint i = 0; i < SAMPLE_SIZE; i++) {\n+    for (uint i = 0; i < SPIKE_ACCELERATION_SAMPLE_SIZE; i++) {\n@@ -552,2 +692,2 @@\n-    m = (SAMPLE_SIZE * xy_sum - x_sum * y_sum) \/ (SAMPLE_SIZE * x2_sum - x_sum * x_sum);\n-    b = (y_sum - m * x_sum) \/ SAMPLE_SIZE;\n+    m = (SPIKE_ACCELERATION_SAMPLE_SIZE * xy_sum - x_sum * y_sum) \/ (SPIKE_ACCELERATION_SAMPLE_SIZE * x2_sum - x_sum * x_sum);\n+    b = (y_sum - m * x_sum) \/ SPIKE_ACCELERATION_SAMPLE_SIZE;\n@@ -555,1 +695,1 @@\n-    current_rate = m * x_array[SAMPLE_SIZE - 1] + b;\n+    current_rate = m * x_array[SPIKE_ACCELERATION_SAMPLE_SIZE - 1] + b;\n@@ -558,2 +698,3 @@\n-    for (uint i = 0; i < SAMPLE_SIZE; i++) {\n-      log_info(gc)(\"sample[%d] timestamp: %12.3f, predicted rate: %12.3f\", i, x_array[i], m * x_array[i] + b);\n+    log_info(gc)(\"%8s%16s%16s%16s\", \"sample\", \"timestamp\", \"predicted rate\", \"actual rate\");\n+    for (uint i = 0; i < SPIKE_ACCELERATION_SAMPLE_SIZE; i++) {\n+      log_info(gc)(\"%8d%16.3f%16.3f%16.3f\", i, x_array[i], m * x_array[i] + b, y_array[i]);\n@@ -564,1 +705,1 @@\n-  double time_delta = _allocation_rate.interval() + avg_cycle_time;\n+  double time_delta = _allocation_rate.interval() + predicted_cycle_time;\n@@ -567,3 +708,9 @@\n-  log_info(gc)(\"accelerated_consumption() acceleration: %0.3f, current_rate: %0.3f, time_delta: %0.3f returning \" SIZE_FORMAT \"%s\",\n-               acceleration, current_rate, time_delta,\n-               byte_size_in_proper_unit(bytes_to_be_consumed), proper_unit_for_byte_size(bytes_to_be_consumed));\n+  if (acceleration > 0.0) {\n+    log_info(gc)(\"Consume \" SIZE_FORMAT \"%s after %.3fs vs. headroom \" SIZE_FORMAT \"%s=\" SIZE_FORMAT \"%s-(\" SIZE_FORMAT \"%s+\" SIZE_FORMAT \"%s)\",\n+                 byte_size_in_proper_unit(bytes_to_be_consumed), proper_unit_for_byte_size(bytes_to_be_consumed),\n+                 time_delta,\n+                 byte_size_in_proper_unit(headroom), proper_unit_for_byte_size(headroom),\n+                 byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                 byte_size_in_proper_unit(spike_headroom), proper_unit_for_byte_size(spike_headroom),\n+                 byte_size_in_proper_unit(penalties), proper_unit_for_byte_size(penalties));\n+  }\n@@ -574,0 +721,2 @@\n+#undef KELVIN_DEBUG_SPIKE_THRESHOLD\n+\n@@ -576,0 +725,4 @@\n+#ifdef KELVIN_DEBUG_SPIKE_THRESHOLD\n+  \/\/ I want to see what causes us to increase margin of error, and what causes us to shrink it.\n+  log_info(gc)(\"KELVIN adjusts margin of error sd to %.2f\", _margin_of_error_sd);\n+#endif\n@@ -581,0 +734,4 @@\n+#ifdef KELVIN_DEBUG_SPIKE_THRESHOLD\n+  \/\/ I want to see what causes us to increase the threshold, and what causes us to shrink it.\n+  log_info(gc)(\"KELVIN adjusts spike threshold to %.2f\", _spike_threshold_sd);\n+#endif\n@@ -637,2 +794,2 @@\n-    \/\/ standard deviation.  A value between -3 and +3 meands we are within 3 standard deviations.  In our use\n-    \/\/ case, we only care if the spike is above the mean.\n+    \/\/ standard deviation.  A value between -3 and +3 means we are within 3 standard deviations.  We care only if\n+    \/\/  the spike is above the mean.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":250,"deletions":93,"binary":false,"changes":343,"status":"modified"},{"patch":"@@ -90,2 +90,2 @@\n-#undef KELVIN_TRACE\n-#ifdef KELVIN_TRACE\n+#undef KELVIN_TRACE_HEADER\n+#ifdef KELVIN_TRACE_HEADER\n@@ -111,1 +111,2 @@\n-  const static size_t SAMPLE_SIZE;\n+  const static size_t SPIKE_ACCELERATION_SAMPLE_SIZE;\n+  const static size_t GC_TIME_SAMPLE_SIZE;\n@@ -115,1 +116,0 @@\n-\n@@ -128,2 +128,7 @@\n-  size_t accelerated_consumption(double& acceleration, double& current_rate, double avg_cycle_time);\n-\n+#undef KELVIN_TRACE_CONSUMPTION_X\n+#ifdef KELVIN_TRACE_CONSUMPTION_X\n+        size_t accelerated_consumption(double& acceleration, double& current_rate, double predicted_cycle_time,\n+                                       size_t available, size_t spike_headroom, size_t penalties, size_t headroom) const;\n+#else\n+  size_t accelerated_consumption(double& acceleration, double& current_rate, double predicted_cycle_time) const;\n+#endif\n@@ -145,1 +150,1 @@\n-  \/\/ of concurrent GCs.\n+  \/\/ of concurrent GCs because more scenarios will be seen as spiking.\n@@ -159,5 +164,24 @@\n-  \/\/ Keep track of SAMPLE_SIZE most recent spike allocation rate measurements\n-  uint _first_sample_index;\n-  uint _num_samples;\n-  double* const _rate_samples;\n-  double* const _rate_timestamps;\n+  \/\/ Keep track of GC_TIME_SAMPLE_SIZE most recent concurrent GC cycle times\n+  uint _gc_time_first_sample_index;\n+  uint _gc_time_num_samples;\n+  double* const _gc_time_timestamps;\n+  double* const _gc_time_samples;\n+  double* const _gc_time_xy;    \/\/ timestamp * sample\n+  double* const _gc_time_xx;    \/\/ timestamp squared\n+  double _gc_time_sum_of_timestamps;\n+  double _gc_time_sum_of_samples;\n+  double _gc_time_sum_of_xy;\n+  double _gc_time_sum_of_xx;\n+\n+  double _gc_time_m;            \/\/ slope\n+  double _gc_time_b;            \/\/ y-intercept\n+  double _gc_time_sd;           \/\/ sd on deviance from prediction\n+\n+  void add_gc_time(double timestamp_at_start, double duration);\n+  double predict_gc_time(double timestamp_at_start);\n+\n+  \/\/ Keep track of SPIKE_ACCELERATION_SAMPLE_SIZE most recent spike allocation rate measurements\n+  uint _spike_acceleration_first_sample_index;\n+  uint _spike_acceleration_num_samples;\n+  double* const _spike_acceleration_rate_samples;\n+  double* const _spike_acceleration_rate_timestamps;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":36,"deletions":12,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#undef KELVIN_TRACE\n@@ -90,0 +91,3 @@\n+#ifdef KELVIN_TRACE\n+          log_info(gc)(\"Acceptance after sleeping %.3f following timestamp %.3f\", _next_sleep_interval, _most_recent_timestamp);\n+#endif\n@@ -97,0 +101,3 @@\n+#ifdef KELVIN_TRACE\n+          log_info(gc)(\"Acceptance after sleeping %.3f following timestamp %.3f\", _next_sleep_interval, _most_recent_timestamp);\n+#endif\n@@ -99,0 +106,3 @@\n+#ifdef KELVIN_TRACE\n+          log_info(gc)(\"Acceptance after sleeping %.3f following timestamp %.3f\", _next_sleep_interval, _most_recent_timestamp);\n+#endif\n@@ -107,0 +117,3 @@\n+#ifdef KELVIN_TRACE\n+        log_info(gc)(\"Acceptance after sleeping %.3f following timestamp %.3f\", _next_sleep_interval, _most_recent_timestamp);\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"}]}