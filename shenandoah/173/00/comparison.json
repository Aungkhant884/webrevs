{"files":[{"patch":"@@ -1135,1 +1135,1 @@\n-\/\/ to old-gen.  plab allocates arre not known as such, since they may hold old-gen evacuations.\n+\/\/ to old-gen.  plab allocates are not known as such, since they may hold old-gen evacuations.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -112,3 +112,11 @@\n-size_t ShenandoahRegionChunkIterator::calc_group_size() {\n-  \/\/ The group size s calculated from the number of regions.  Every group except the last processes the same number of chunks.\n-  \/\/ The last group processes however many chunks are required to finish the total scanning effort.  The chunk sizes are\n+size_t ShenandoahRegionChunkIterator::calc_regular_group_size() {\n+  \/\/ The group size is calculated from the number of regions.  Suppose the entire heap size is N.  The first group processes\n+  \/\/ N\/2 of total heap size.  The second group processes N\/4 of total heap size.  The third group processes N\/2 of total heap\n+  \/\/ size, and so on.  Note that N\/2 + N\/4 + N\/8 + N\/16 + ...  sums to N if expanded to infinite terms.\n+  \/\/\n+  \/\/ The normal group size is the number of regions \/ 2.\n+  \/\/\n+  \/\/ In the case that the region_size_words is greater than _maximum_chunk_size_words, the first group_size is\n+  \/\/ larger than the normal group size because each chunk in the group will be smaller than the region size.\n+  \/\/\n+  \/\/ The last group also has more than the normal entries because it finishes the total scanning effort.  The chunk sizes are\n@@ -117,1 +125,1 @@\n-  \/\/ is represented by _smallest_chunk_size.  We do not divide work any smaller than this.\n+  \/\/ is represented by _smallest_chunk_size_words.  We do not divide work any smaller than this.\n@@ -119,2 +127,3 @@\n-  \/\/ Note that N\/2 + N\/4 + N\/8 + N\/16 + ...  sums to N if expanded to infinite terms.\n-  return _heap->num_regions() \/ 2;\n+\n+  size_t group_size = _heap->num_regions() \/ 2;\n+  return group_size;\n@@ -123,3 +132,3 @@\n-size_t ShenandoahRegionChunkIterator::calc_first_group_chunk_size() {\n-  size_t words_in_region = ShenandoahHeapRegion::region_size_words();\n-  return words_in_region;\n+size_t ShenandoahRegionChunkIterator::calc_first_group_chunk_size_b4_rebalance() {\n+  size_t words_in_first_chunk = ShenandoahHeapRegion::region_size_words();\n+  return words_in_first_chunk;\n@@ -132,2 +141,2 @@\n-  size_t current_group_span = _first_group_chunk_size * _group_size;\n-  size_t smallest_group_span = _smallest_chunk_size * _group_size;\n+  size_t current_group_span = _first_group_chunk_size_b4_rebalance * _regular_group_size;\n+  size_t smallest_group_span = _smallest_chunk_size_words * _regular_group_size;\n@@ -147,1 +156,1 @@\n-  \/\/   each of num_groups is fully populated with _group_size chunks in each\n+  \/\/   each of num_groups is fully populated with _regular_group_size chunks in each\n@@ -162,1 +171,1 @@\n-    \/\/ have more than _group_size chunks within it.\n+    \/\/ have more than _regular_group_size chunks within it.\n@@ -171,1 +180,0 @@\n-  size_t spanned_groups = 0;\n@@ -173,2 +181,19 @@\n-  size_t current_group_span = _first_group_chunk_size * _group_size;\n-  size_t smallest_group_span = _smallest_chunk_size * _group_size;\n+  size_t current_group_span = _first_group_chunk_size_b4_rebalance * _regular_group_size;\n+  size_t smallest_group_span = _smallest_chunk_size_words * _regular_group_size;\n+\n+  \/\/ The first group gets special handling because the first chunk size can be no larger than _largest_chunk_size_words\n+  if (region_size_words > _maximum_chunk_size_words) {\n+    \/\/ In the case that we shrink the first group's chunk size, certain other groups will also be subsumed within the first group\n+    size_t effective_chunk_size = _first_group_chunk_size_b4_rebalance;\n+    while (effective_chunk_size >= _maximum_chunk_size_words) {\n+      num_chunks += current_group_span \/ _maximum_chunk_size_words;\n+      unspanned_heap_size -= current_group_span;\n+      effective_chunk_size \/= 2;\n+      current_group_span \/= 2;\n+    }\n+  } else {\n+    num_chunks = _regular_group_size;\n+    unspanned_heap_size -= current_group_span;\n+    current_group_span \/= 2;\n+  }\n+  size_t spanned_groups = 1;\n@@ -178,1 +203,1 @@\n-      num_chunks += _group_size;\n+      num_chunks += _regular_group_size;\n@@ -185,2 +210,2 @@\n-        \/\/ The last group has more than _group_size entries.\n-        size_t chunk_span = current_group_span \/ _group_size;\n+        \/\/ The last group has more than _regular_group_size entries.\n+        size_t chunk_span = current_group_span \/ _regular_group_size;\n@@ -192,2 +217,3 @@\n-        \/\/ We cannot introduce new groups because we've reached the lower bound on group size\n-        size_t chunk_span = _smallest_chunk_size;\n+        \/\/ We cannot introduce new groups because we've reached the lower bound on group size.  So this last\n+        \/\/ group may hold extra chunks.\n+        size_t chunk_span = _smallest_chunk_size_words;\n@@ -202,2 +228,2 @@\n-      \/\/ The last group has fewer than _group_size entries.\n-      size_t chunk_span = current_group_span \/ _group_size;\n+      \/\/ This last group has fewer than _regular_group_size entries.\n+      size_t chunk_span = current_group_span \/ _regular_group_size;\n@@ -220,2 +246,2 @@\n-    _group_size(calc_group_size()),\n-    _first_group_chunk_size(calc_first_group_chunk_size()),\n+    _regular_group_size(calc_regular_group_size()),\n+    _first_group_chunk_size_b4_rebalance(calc_first_group_chunk_size_b4_rebalance()),\n@@ -226,3 +252,6 @@\n-  assert(_smallest_chunk_size ==\n-         CardTable::card_size_in_words() * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster,\n-         \"_smallest_chunk_size is not valid\");\n+  assert(_smallest_chunk_size_words == _clusters_in_smallest_chunk * CardTable::card_size_in_words()\n+         * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster, \"_smallest_chunk_size is not valid\");\n+  assert(_num_groups <= _maximum_groups,\n+         \"The number of remembered set scanning groups must be less than or equal to maximum groups\");\n+  assert(_smallest_chunk_size_words << (_maximum_groups - 1) == _maximum_chunk_size_words,\n+         \"Maximum number of groups needs to span maximum chunk size to smallest chunk size\");\n@@ -231,2 +260,0 @@\n-  size_t group_span = _first_group_chunk_size * _group_size;\n-\n@@ -235,0 +262,18 @@\n+  if (words_in_region > _maximum_chunk_size_words) {\n+    \/\/ In the case that we shrink the first group's chunk size, certain other groups will also be subsumed within the first group\n+    size_t num_chunks = 0;\n+    size_t effective_chunk_size = _first_group_chunk_size_b4_rebalance;\n+    size_t  current_group_span = effective_chunk_size * _regular_group_size;\n+    while (effective_chunk_size >= _maximum_chunk_size_words) {\n+      num_chunks += current_group_span \/ _maximum_chunk_size_words;\n+      effective_chunk_size \/= 2;\n+      current_group_span \/= 2;\n+    }\n+    _group_entries[0] = num_chunks;\n+    _group_chunk_size[0] = _maximum_chunk_size_words;\n+  } else {\n+    _group_entries[0] = _regular_group_size;\n+    _group_chunk_size[0] = _first_group_chunk_size_b4_rebalance;\n+  }\n+\n+  size_t previous_group_span = _group_entries[0] * _group_chunk_size[0];\n@@ -236,3 +281,9 @@\n-    _region_index[i] = _region_index[i-1] + (_group_offset[i-1] + group_span) \/ words_in_region;\n-    _group_offset[i] = (_group_offset[i-1] + group_span) % words_in_region;\n-    group_span \/= 2;\n+    size_t previous_group_entries = (i == 1)? _group_entries[0]: (_group_entries[i-1] - _group_entries[i-2]);\n+    _group_chunk_size[i] = _group_chunk_size[i-1] \/ 2;\n+    size_t chunks_in_group = _regular_group_size;\n+    size_t this_group_span = _group_chunk_size[i] * chunks_in_group;\n+    size_t total_span_of_groups = previous_group_span + this_group_span;\n+    _region_index[i] = previous_group_span \/ words_in_region;\n+    _group_offset[i] = previous_group_span % words_in_region;\n+    _group_entries[i] = _group_entries[i-1] + _regular_group_size;\n+    previous_group_span = total_span_of_groups;\n@@ -240,0 +291,11 @@\n+  if (_group_entries[_num_groups-1] < _total_chunks) {\n+    assert((_total_chunks - _group_entries[_num_groups-1]) * _group_chunk_size[_num_groups-1] + previous_group_span ==\n+           heap->num_regions() * words_in_region, \"Total region chunks (\" SIZE_FORMAT\n+           \") do not span total heap regions (\" SIZE_FORMAT \")\", _total_chunks, _heap->num_regions());\n+    previous_group_span += (_total_chunks - _group_entries[_num_groups-1]) * _group_chunk_size[_num_groups-1];\n+    _group_entries[_num_groups-1] = _total_chunks;\n+  }\n+  assert(previous_group_span == heap->num_regions() * words_in_region, \"Total region chunks (\" SIZE_FORMAT\n+         \") do not span total heap regions (\" SIZE_FORMAT \"): \" SIZE_FORMAT \" does not equal \" SIZE_FORMAT,\n+         _total_chunks, _heap->num_regions(), previous_group_span, heap->num_regions() * words_in_region);\n+\n@@ -244,0 +306,2 @@\n+    _group_entries[i] = _group_entries[i-1];\n+    _group_chunk_size[i] = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":97,"deletions":33,"binary":false,"changes":130,"status":"modified"},{"patch":"@@ -1011,2 +1011,9 @@\n-  \/\/ smallest_chunk_size is 64 words per card *\n-  \/\/ ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster.\n+  \/\/ The largest chunk size is 4 MiB, measured in words.  Otherwise, remembered set scanning may become too unbalanced.\n+  \/\/ If the largest chunk size is too small, there is too much overhead sifting out assignments to individual worker threads.\n+  static const size_t _maximum_chunk_size_words = (4 * 1024 * 1024) \/ HeapWordSize;\n+\n+  static const size_t _clusters_in_smallest_chunk = 4;\n+  static const size_t _assumed_words_in_card = 64;\n+\n+  \/\/ smallest_chunk_size is 4 clusters (i.e. 128 KiB).  Note that there are 64 words per card and there are 64 cards per\n+  \/\/ cluster.  Each cluster spans 128 KiB.\n@@ -1017,1 +1024,2 @@\n-  static const size_t _smallest_chunk_size = 64 * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  static const size_t _smallest_chunk_size_words = (_clusters_in_smallest_chunk * _assumed_words_in_card *\n+                                                    ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster);\n@@ -1020,6 +1028,6 @@\n-  \/\/ The chunks of assigned work are divided into groups, where the size of each group (_group_size) is 4 * the number of\n-  \/\/ worker tasks.  All of the assignments within a group represent the same amount of memory to be scanned.  Each of the\n-  \/\/ assignments within the first group are of size _first_group_chunk_size (typically the ShenandoahHeapRegion size, but\n-  \/\/ possibly smaller.  Each of the assignments within each subsequent group are half the size of the assignments in the\n-  \/\/ preceding group.  The last group may be larger than the others.  Because no group is allowed to have smaller assignments\n-  \/\/ than _smallest_chunk_size, which is 32 KB.\n+  \/\/ The chunks of assigned work are divided into groups, where the size of the typical group (_regular_group_size) is half the\n+  \/\/ total number of regions.  The first group may be larger than\n+  \/\/ _regular_group_size in the case that the first group's chunk\n+  \/\/ size is less than the region size.  The last group may be larger\n+  \/\/ than _regular_group_size because no group is allowed to\n+  \/\/ have smaller assignments than _smallest_chunk_size, which is 128 KB.\n@@ -1028,0 +1036,2 @@\n+  \/\/ The first group \"effectively\" processes chunks of size 1 MiB (or smaller for smaller region sizes).\n+  \/\/ The last group processes chunks of size 128 KiB.  There are four groups total.\n@@ -1029,1 +1039,7 @@\n-  static const size_t _maximum_groups = 16;\n+  \/\/ group[0] is 4 MiB chunk size (_maximum_chunk_size_words)\n+  \/\/ group[1] is 2 MiB chunk size\n+  \/\/ group[2] is 1 MiB chunk size\n+  \/\/ group[3] is 512 KiB chunk size\n+  \/\/ group[4] is 256 KiB chunk size\n+  \/\/ group[5] is 128 Kib shunk size (_smallest_chunk_size_words = 4 * 64 * 64\n+  static const size_t _maximum_groups = 6;\n@@ -1033,2 +1049,2 @@\n-  const size_t _group_size;                        \/\/ Number of chunks in each group, equals worker_threads * 8\n-  const size_t _first_group_chunk_size;\n+  const size_t _regular_group_size;                        \/\/ Number of chunks in each group\n+  const size_t _first_group_chunk_size_b4_rebalance;\n@@ -1042,3 +1058,4 @@\n-  size_t _region_index[_maximum_groups];\n-  size_t _group_offset[_maximum_groups];\n-\n+  size_t _region_index[_maximum_groups];           \/\/ The region index for the first region spanned by this group\n+  size_t _group_offset[_maximum_groups];           \/\/ The offset at which group begins within first region spanned by this group\n+  size_t _group_chunk_size[_maximum_groups];       \/\/ The size of each chunk within this group\n+  size_t _group_entries[_maximum_groups];          \/\/ Total chunks spanned by this group and the ones before it.\n@@ -1050,1 +1067,1 @@\n-  size_t calc_group_size();\n+  size_t calc_regular_group_size();\n@@ -1052,2 +1069,2 @@\n-  \/\/ Makes use of _group_size, which must be initialized before call.\n-  size_t calc_first_group_chunk_size();\n+  \/\/ Makes use of _regular_group_size, which must be initialized before call.\n+  size_t calc_first_group_chunk_size_b4_rebalance();\n@@ -1055,1 +1072,1 @@\n-  \/\/ Makes use of _group_size and _first_group_chunk_size, both of which must be initialized before call.\n+  \/\/ Makes use of _regular_group_size and _first_group_chunk_size_b4_rebalance, both of which must be initialized before call.\n@@ -1058,1 +1075,1 @@\n-  \/\/ Makes use of _group_size, _first_group_chunk_size, which must be initialized before call.\n+  \/\/ Makes use of _regular_group_size, _first_group_chunk_size_b4_rebalance, which must be initialized before call.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":37,"deletions":20,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -681,1 +681,0 @@\n-\n@@ -818,4 +817,5 @@\n-  size_t group_no = new_index \/ _group_size;\n-  if (group_no + 1 > _num_groups) {\n-    group_no = _num_groups - 1;\n-  }\n+  size_t group_no;\n+  for (group_no = 0; new_index >= _group_entries[group_no]; group_no++)\n+    ;\n+\n+  assert(group_no < _num_groups, \"Cannot have group no greater or equal to _num_groups\");\n@@ -828,2 +828,2 @@\n-  size_t index_within_group = new_index - (group_no * _group_size);\n-  size_t group_chunk_size = _first_group_chunk_size >> group_no;\n+  size_t index_within_group = (group_no == 0)? new_index: new_index - _group_entries[group_no - 1];\n+  size_t group_chunk_size = _group_chunk_size[group_no];\n@@ -832,1 +832,0 @@\n-  size_t region_index = group_region_index + regions_spanned_by_chunk_offset;\n@@ -835,0 +834,2 @@\n+  size_t region_index = group_region_index + regions_spanned_by_chunk_offset;\n+\n@@ -838,1 +839,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"}]}