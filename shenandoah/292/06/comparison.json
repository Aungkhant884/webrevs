{"files":[{"patch":"@@ -25,1 +25,0 @@\n-\n@@ -28,0 +27,4 @@\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeapStats.hpp\"\n@@ -30,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n@@ -32,1 +36,0 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -34,2 +37,0 @@\n-#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"runtime\/globals_extension.hpp\"\n@@ -61,2 +63,3 @@\n-ShenandoahAdaptiveHeuristics::ShenandoahAdaptiveHeuristics(ShenandoahGeneration* generation) :\n-  ShenandoahHeuristics(generation),\n+ShenandoahAdaptiveHeuristics::ShenandoahAdaptiveHeuristics(ShenandoahHeapStats* heap_stats) :\n+  ShenandoahHeuristics(),\n+  _heap_stats(heap_stats),\n@@ -74,2 +77,0 @@\n-  size_t ignore_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahIgnoreGarbageThreshold \/ 100;\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -94,13 +95,4 @@\n-  \/\/ In generational mode, the sort order within the data array is not strictly descending amounts of garbage.  In\n-  \/\/ particular, regions that have reached tenure age will be sorted into this array before younger regions that contain\n-  \/\/ more garbage.  This represents one of the reasons why we keep looking at regions even after we decide, for example,\n-  \/\/ to exclude one of the regions because it might require evacuation of too much live data.\n-  \/\/ TODO: Split it in the separate methods for clarity.\n-  bool is_generational = heap->mode()->is_generational();\n-  bool is_global = _generation->is_global();\n-  size_t capacity = heap->young_generation()->max_capacity();\n-\n-  \/\/ cur_young_garbage represents the amount of memory to be reclaimed from young-gen.  In the case that live objects\n-  \/\/ are known to be promoted out of young-gen, we count this as cur_young_garbage because this memory is reclaimed\n-  \/\/ from young-gen and becomes available to serve future young-gen allocation requests.\n-  size_t cur_young_garbage = 0;\n+  size_t capacity    = ShenandoahHeap::heap()->soft_max_capacity();\n+  size_t max_cset    = (size_t)((1.0 * capacity \/ 100 * ShenandoahEvacReserve) \/ ShenandoahEvacWaste);\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n+  size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n@@ -108,107 +100,1 @@\n-  \/\/ Better select garbage-first regions\n-  QuickSort::sort<RegionData>(data, (int)size, compare_by_garbage, false);\n-\n-  if (is_generational) {\n-    for (size_t idx = 0; idx < size; idx++) {\n-      ShenandoahHeapRegion* r = data[idx]._region;\n-      if (cset->is_preselected(r->index())) {\n-        assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n-        \/\/ Entire region will be promoted, This region does not impact young-gen or old-gen evacuation reserve.\n-        \/\/ This region has been pre-selected and its impact on promotion reserve is already accounted for.\n-\n-        \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n-        \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n-        \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n-        \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n-        \/\/ within youn-gen memory.\n-\n-        cur_young_garbage += r->garbage();\n-        cset->add_region(r);\n-      }\n-    }\n-    if (is_global) {\n-      size_t max_young_cset    = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n-      size_t young_cur_cset = 0;\n-      size_t max_old_cset    = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n-      size_t old_cur_cset = 0;\n-      size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n-      size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n-\n-      log_info(gc, ergo)(\"Adaptive CSet Selection for GLOBAL. Max Young Evacuation: \" SIZE_FORMAT\n-                         \"%s, Max Old Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n-                         byte_size_in_proper_unit(max_young_cset),    proper_unit_for_byte_size(max_young_cset),\n-                         byte_size_in_proper_unit(max_old_cset),    proper_unit_for_byte_size(max_old_cset),\n-                         byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n-\n-      for (size_t idx = 0; idx < size; idx++) {\n-        ShenandoahHeapRegion* r = data[idx]._region;\n-        if (cset->is_preselected(r->index())) {\n-          continue;\n-        }\n-        bool add_region = false;\n-        if (r->is_old()) {\n-          size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n-          if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n-            add_region = true;\n-            old_cur_cset = new_cset;\n-          }\n-        } else if (r->age() < InitialTenuringThreshold) {\n-          size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n-          size_t region_garbage = r->garbage();\n-          size_t new_garbage = cur_young_garbage + region_garbage;\n-          bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-          if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-            add_region = true;\n-            young_cur_cset = new_cset;\n-            cur_young_garbage = new_garbage;\n-          }\n-        }\n-        \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n-        \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects.\n-\n-        if (add_region) {\n-          cset->add_region(r);\n-        }\n-      }\n-    } else {\n-      \/\/ This is young-gen collection or a mixed evacuation.  If this is mixed evacuation, the old-gen candidate regions\n-      \/\/ have already been added.\n-      size_t max_cset    = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n-      size_t cur_cset = 0;\n-      size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n-      size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n-\n-      log_info(gc, ergo)(\"Adaptive CSet Selection for YOUNG. Max Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n-                         byte_size_in_proper_unit(max_cset),    proper_unit_for_byte_size(max_cset),\n-                         byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n-\n-      for (size_t idx = 0; idx < size; idx++) {\n-        ShenandoahHeapRegion* r = data[idx]._region;\n-        if (cset->is_preselected(r->index())) {\n-          continue;\n-        }\n-        if  (r->age() < InitialTenuringThreshold) {\n-          size_t new_cset = cur_cset + r->get_live_data_bytes();\n-          size_t region_garbage = r->garbage();\n-          size_t new_garbage = cur_young_garbage + region_garbage;\n-          bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-          assert(r->is_young(), \"Only young candidates expected in the data array\");\n-          if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-            cur_cset = new_cset;\n-            cur_young_garbage = new_garbage;\n-            cset->add_region(r);\n-          }\n-        }\n-        \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n-        \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects or because\n-        \/\/ they are to be promoted in place.\n-      }\n-    }\n-  } else {\n-    \/\/ Traditional Shenandoah (non-generational)\n-    size_t capacity    = ShenandoahHeap::heap()->max_capacity();\n-    size_t max_cset    = (size_t)((1.0 * capacity \/ 100 * ShenandoahEvacReserve) \/ ShenandoahEvacWaste);\n-    size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n-    size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n-\n-    log_info(gc, ergo)(\"Adaptive CSet Selection. Target Free: \" SIZE_FORMAT \"%s, Actual Free: \"\n+  log_info(gc, ergo)(\"Adaptive CSet Selection. Target Free: \" SIZE_FORMAT \"%s, Actual Free: \"\n@@ -221,2 +107,5 @@\n-    size_t cur_cset = 0;\n-    size_t cur_garbage = 0;\n+  \/\/ Better select garbage-first regions\n+  QuickSort::sort<RegionData>(data, (int)size, compare_by_garbage, false);\n+\n+  size_t cur_cset = 0;\n+  size_t cur_garbage = 0;\n@@ -224,2 +113,2 @@\n-    for (size_t idx = 0; idx < size; idx++) {\n-      ShenandoahHeapRegion* r = data[idx]._region;\n+  for (size_t idx = 0; idx < size; idx++) {\n+    ShenandoahHeapRegion* r = data[idx]._region;\n@@ -227,2 +116,2 @@\n-      size_t new_cset    = cur_cset + r->get_live_data_bytes();\n-      size_t new_garbage = cur_garbage + r->garbage();\n+    size_t new_cset    = cur_cset + r->get_live_data_bytes();\n+    size_t new_garbage = cur_garbage + r->garbage();\n@@ -230,3 +119,3 @@\n-      if (new_cset > max_cset) {\n-        break;\n-      }\n+    if (new_cset > max_cset) {\n+      break;\n+    }\n@@ -234,5 +123,4 @@\n-      if ((new_garbage < min_garbage) || (r->garbage() > garbage_threshold)) {\n-        cset->add_region(r);\n-        cur_cset = new_cset;\n-        cur_garbage = new_garbage;\n-      }\n+    if ((new_garbage < min_garbage) || (r->garbage() > garbage_threshold)) {\n+      cset->add_region(r);\n+      cur_cset = new_cset;\n+      cur_garbage = new_garbage;\n@@ -241,10 +129,0 @@\n-\n-  size_t collected_old = cset->get_old_bytes_reserved_for_evacuation();\n-  size_t collected_promoted = cset->get_young_bytes_to_be_promoted();\n-  size_t collected_young = cset->get_young_bytes_reserved_for_evacuation();\n-\n-  log_info(gc, ergo)(\"Chosen CSet evacuates young: \" SIZE_FORMAT \"%s (of which at least: \" SIZE_FORMAT \"%s are to be promoted), \"\n-                     \"old: \" SIZE_FORMAT \"%s\",\n-                     byte_size_in_proper_unit(collected_young),    proper_unit_for_byte_size(collected_young),\n-                     byte_size_in_proper_unit(collected_promoted), proper_unit_for_byte_size(collected_promoted),\n-                     byte_size_in_proper_unit(collected_old),      proper_unit_for_byte_size(collected_old));\n@@ -261,1 +139,1 @@\n-  size_t available = MIN2(_generation->available(), ShenandoahHeap::heap()->free_set()->available());\n+  size_t available = _heap_stats->available();\n@@ -269,2 +147,2 @@\n-                        _generation->name(),\n-                        byte_size_in_proper_unit(available),     proper_unit_for_byte_size(available),\n+                        _heap_stats->name(),\n+                        byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n@@ -273,1 +151,1 @@\n-                        byte_size_in_proper_unit(available_sd),  proper_unit_for_byte_size(available_sd));\n+                        byte_size_in_proper_unit(available_sd), proper_unit_for_byte_size(available_sd));\n@@ -331,78 +209,0 @@\n-\/\/ Return a conservative estimate of how much memory can be allocated before we need to start GC. The estimate is based\n-\/\/ on memory that is currently available within young generation plus all of the memory that will be added to the young\n-\/\/ generation at the end of the current cycle (as represented by young_regions_to_be_reclaimed) and on the anticipated\n-\/\/ amount of time required to perform a GC.\n-size_t ShenandoahAdaptiveHeuristics::bytes_of_allocation_runway_before_gc_trigger(size_t young_regions_to_be_reclaimed) {\n-  assert(_generation->is_young(), \"Only meaningful for young-gen heuristic\");\n-\n-  size_t max_capacity = _generation->max_capacity();\n-  size_t capacity = _generation->soft_max_capacity();\n-  size_t usage = _generation->used();\n-  size_t available = (capacity > usage)? capacity - usage: 0;\n-  size_t allocated = _generation->bytes_allocated_since_gc_start();\n-\n-  size_t available_young_collected = ShenandoahHeap::heap()->collection_set()->get_young_available_bytes_collected();\n-  size_t anticipated_available =\n-    available + young_regions_to_be_reclaimed * ShenandoahHeapRegion::region_size_bytes() - available_young_collected;\n-  size_t allocation_headroom = anticipated_available;\n-  size_t spike_headroom = capacity * ShenandoahAllocSpikeFactor \/ 100;\n-  size_t penalties      = capacity * _gc_time_penalties \/ 100;\n-\n-  double rate = _allocation_rate.sample(allocated);\n-\n-  \/\/ At what value of available, would avg and spike triggers occur?\n-  \/\/  if allocation_headroom < avg_cycle_time * avg_alloc_rate, then we experience avg trigger\n-  \/\/  if allocation_headroom < avg_cycle_time * rate, then we experience spike trigger if is_spiking\n-  \/\/\n-  \/\/ allocation_headroom =\n-  \/\/     0, if penalties > available or if penalties + spike_headroom > available\n-  \/\/     available - penalties - spike_headroom, otherwise\n-  \/\/\n-  \/\/ so we trigger if available - penalties - spike_headroom < avg_cycle_time * avg_alloc_rate, which is to say\n-  \/\/                  available < avg_cycle_time * avg_alloc_rate + penalties + spike_headroom\n-  \/\/            or if available < penalties + spike_headroom\n-  \/\/\n-  \/\/ since avg_cycle_time * avg_alloc_rate > 0, the first test is sufficient to test both conditions\n-  \/\/\n-  \/\/ thus, evac_slack_avg is MIN2(0,  available - avg_cycle_time * avg_alloc_rate + penalties + spike_headroom)\n-  \/\/\n-  \/\/ similarly, evac_slack_spiking is MIN2(0, available - avg_cycle_time * rate + penalties + spike_headroom)\n-  \/\/ but evac_slack_spiking is only relevant if is_spiking, as defined below.\n-\n-  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n-\n-  \/\/ TODO: Consider making conservative adjustments to avg_cycle_time, such as: (avg_cycle_time *= 2) in cases where\n-  \/\/ we expect a longer-than-normal GC duration.  This includes mixed evacuations, evacuation that perform promotion\n-  \/\/ including promotion in place, and OLD GC bootstrap cycles.  It has been observed that these cycles sometimes\n-  \/\/ require twice or more the duration of \"normal\" GC cycles.  We have experimented with this approach.  While it\n-  \/\/ does appear to reduce the frequency of degenerated cycles due to late triggers, it also has the effect of reducing\n-  \/\/ evacuation slack so that there is less memory available to be transferred to OLD.  The result is that we\n-  \/\/ throttle promotion and it takes too long to move old objects out of the young generation.\n-\n-  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n-  size_t evac_slack_avg;\n-  if (anticipated_available > avg_cycle_time * avg_alloc_rate + penalties + spike_headroom) {\n-    evac_slack_avg = anticipated_available - (avg_cycle_time * avg_alloc_rate + penalties + spike_headroom);\n-  } else {\n-    \/\/ we have no slack because it's already time to trigger\n-    evac_slack_avg = 0;\n-  }\n-\n-  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n-  size_t evac_slack_spiking;\n-  if (is_spiking) {\n-    if (anticipated_available > avg_cycle_time * rate + penalties + spike_headroom) {\n-      evac_slack_spiking = anticipated_available - (avg_cycle_time * rate + penalties + spike_headroom);\n-    } else {\n-      \/\/ we have no slack because it's already time to trigger\n-      evac_slack_spiking = 0;\n-    }\n-  } else {\n-    evac_slack_spiking = evac_slack_avg;\n-  }\n-\n-  size_t threshold = min_free_threshold();\n-  size_t evac_min_threshold = (anticipated_available > threshold)? anticipated_available - threshold: 0;\n-  return MIN3(evac_slack_spiking, evac_slack_avg, evac_min_threshold);\n-}\n-\n@@ -410,3 +210,3 @@\n-  size_t capacity = _generation->soft_max_capacity();\n-  size_t available = _generation->soft_available();\n-  size_t allocated = _generation->bytes_allocated_since_gc_start();\n+  size_t capacity = _heap_stats->soft_max_capacity();\n+  size_t available = _heap_stats->soft_available();\n+  size_t allocated = _heap_stats->bytes_allocated_since_gc_start();\n@@ -416,11 +216,1 @@\n-                _generation->name(), available, capacity, allocated);\n-\n-  \/\/ The collector reserve may eat into what the mutator is allowed to use. Make sure we are looking\n-  \/\/ at what is available to the mutator when deciding whether to start a GC.\n-  size_t usable = ShenandoahHeap::heap()->free_set()->available();\n-  if (usable < available) {\n-    log_debug(gc)(\"Usable (\" SIZE_FORMAT \"%s) is less than available (\" SIZE_FORMAT \"%s)\",\n-                  byte_size_in_proper_unit(usable),    proper_unit_for_byte_size(usable),\n-                  byte_size_in_proper_unit(available), proper_unit_for_byte_size(available));\n-    available = usable;\n-  }\n+                _heap_stats->name(), available, capacity, allocated);\n@@ -432,6 +222,15 @@\n-  \/\/ OLD generation is maintained to be as small as possible.  Depletion-of-free-pool triggers do not apply to old generation.\n-  if (!_generation->is_old()) {\n-    size_t min_threshold = min_free_threshold();\n-    if (available < min_threshold) {\n-      log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n-                   _generation->name(),\n+  size_t min_threshold = min_free_threshold();\n+  if (available < min_threshold) {\n+    log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\", _heap_stats->name(),\n+                 byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                 byte_size_in_proper_unit(min_threshold), proper_unit_for_byte_size(min_threshold));\n+    return true;\n+  }\n+\n+  \/\/ Check if we need to learn a bit about the application\n+  const size_t max_learn = ShenandoahLearningSteps;\n+  if (_gc_times_learned < max_learn) {\n+    size_t init_threshold = capacity \/ 100 * ShenandoahInitFreeThreshold;\n+    if (available < init_threshold) {\n+      log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n+                   _heap_stats->name(), _gc_times_learned + 1, max_learn,\n@@ -439,1 +238,1 @@\n-                   byte_size_in_proper_unit(min_threshold),       proper_unit_for_byte_size(min_threshold));\n+                   byte_size_in_proper_unit(init_threshold), proper_unit_for_byte_size(init_threshold));\n@@ -442,0 +241,33 @@\n+  }\n+  \/\/  Rationale:\n+  \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n+  \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n+  \/\/\n+  \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n+  \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n+  \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n+  \/\/       accomodation to give us enough runway to recalibrate our \"average allocation rate\".\n+  \/\/\n+  \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n+  \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n+  \/\/       enough runway to recalibrate our \"average allocation rate\".\n+  \/\/\n+  \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n+  \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n+  \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n+  \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n+  \/\/\n+  \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n+  \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n+  \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n+  \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n+  \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n+  \/\/          of how this will behave.\n+  \/\/\n+  \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n+  \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n+  \/\/    computations an independent effort.\n+  \/\/ Check if allocation headroom is still okay. This also factors in:\n+  \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n+  \/\/   2. Accumulated penalties from Degenerated and Full GC\n+  size_t allocation_headroom = available;\n@@ -443,76 +275,2 @@\n-    \/\/ Check if we need to learn a bit about the application\n-    const size_t max_learn = ShenandoahLearningSteps;\n-    if (_gc_times_learned < max_learn) {\n-      size_t init_threshold = capacity \/ 100 * ShenandoahInitFreeThreshold;\n-      if (available < init_threshold) {\n-        log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\"\n-                     SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n-                     _generation->name(), _gc_times_learned + 1, max_learn,\n-                     byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n-                     byte_size_in_proper_unit(init_threshold),      proper_unit_for_byte_size(init_threshold));\n-        return true;\n-      }\n-    }\n-\n-    \/\/  Rationale:\n-    \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n-    \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n-    \/\/\n-    \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n-    \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n-    \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n-    \/\/       accomodation to give us enough runway to recalibrate our \"average allocation rate\".\n-    \/\/\n-    \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n-    \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n-    \/\/       enough runway to recalibrate our \"average allocation rate\".\n-    \/\/\n-    \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n-    \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n-    \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n-    \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n-    \/\/\n-    \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n-    \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n-    \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n-    \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n-    \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n-    \/\/          of how this will behave.\n-    \/\/\n-    \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n-    \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n-    \/\/    computations an independent effort.\n-\n-\n-    \/\/ Check if allocation headroom is still okay. This also factors in:\n-    \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n-    \/\/   2. Accumulated penalties from Degenerated and Full GC\n-\n-    size_t allocation_headroom = available;\n-    size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n-    size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n-\n-    allocation_headroom -= MIN2(allocation_headroom, penalties);\n-    allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n-\n-    double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n-    double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n-    log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n-                  _generation->name(),\n-                  avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n-\n-    if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n-\n-      log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n-                   \" to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n-                   _generation->name(), avg_cycle_time * 1000,\n-                   byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate),\n-                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-                   _margin_of_error_sd);\n-\n-      log_info(gc, ergo)(\"Free headroom: \" SIZE_FORMAT \"%s (free) - \" SIZE_FORMAT \"%s (spike) - \"\n-                         SIZE_FORMAT \"%s (penalties) = \" SIZE_FORMAT \"%s\",\n-                         byte_size_in_proper_unit(available),           proper_unit_for_byte_size(available),\n-                         byte_size_in_proper_unit(spike_headroom),      proper_unit_for_byte_size(spike_headroom),\n-                         byte_size_in_proper_unit(penalties),           proper_unit_for_byte_size(penalties),\n-                         byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n+  size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n+  size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n@@ -520,3 +278,2 @@\n-      _last_trigger = RATE;\n-      return true;\n-    }\n+  allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n+  allocation_headroom -= MIN2(allocation_headroom, penalties);\n@@ -524,11 +281,20 @@\n-    bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n-    if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n-      log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s)\"\n-                   \" to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n-                   _generation->name(), avg_cycle_time * 1000,\n-                   byte_size_in_proper_unit(rate), proper_unit_for_byte_size(rate),\n-                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-                   _spike_threshold_sd);\n-      _last_trigger = SPIKE;\n-      return true;\n-    }\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+  log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+                _heap_stats->name(),\n+          avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n+  if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n+    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+                 \" to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n+                 _heap_stats->name(), avg_cycle_time * 1000,\n+                 byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate),\n+                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                 _margin_of_error_sd);\n+    log_info(gc, ergo)(\"Free headroom: \" SIZE_FORMAT \"%s (free) - \" SIZE_FORMAT \"%s (spike) - \" SIZE_FORMAT \"%s (penalties) = \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(available),           proper_unit_for_byte_size(available),\n+                       byte_size_in_proper_unit(spike_headroom),      proper_unit_for_byte_size(spike_headroom),\n+                       byte_size_in_proper_unit(penalties),           proper_unit_for_byte_size(penalties),\n+                       byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n+    _last_trigger = RATE;\n+    return true;\n+  }\n@@ -536,32 +302,9 @@\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    if (heap->mode()->is_generational()) {\n-      \/\/ Get through promotions and mixed evacuations as quickly as possible.  These cycles sometimes require significantly\n-      \/\/ more time than traditional young-generation cycles so start them up as soon as possible.  This is a \"mitigation\"\n-      \/\/ for the reality that old-gen and young-gen activities are not truly \"concurrent\".  If there is old-gen work to\n-      \/\/ be done, we start up the young-gen GC threads so they can do some of this old-gen work.  As implemented, promotion\n-      \/\/ gets priority over old-gen marking.\n-\n-      size_t promo_potential = heap->get_promotion_potential();\n-      size_t promo_in_place_potential = heap->get_promotion_in_place_potential();\n-      ShenandoahOldHeuristics* old_heuristics = (ShenandoahOldHeuristics*) heap->old_generation()->heuristics();\n-      size_t mixed_candidates = old_heuristics->unprocessed_old_collection_candidates();\n-      if (promo_potential > 0) {\n-        \/\/ Detect unsigned arithmetic underflow\n-        assert(promo_potential < heap->capacity(), \"Sanity\");\n-        log_info(gc)(\"Trigger (%s): expedite promotion of \" SIZE_FORMAT \"%s\",\n-                     _generation->name(), byte_size_in_proper_unit(promo_potential), proper_unit_for_byte_size(promo_potential));\n-        return true;\n-      } else if (promo_in_place_potential > 0) {\n-        \/\/ Detect unsigned arithmetic underflow\n-        assert(promo_in_place_potential < heap->capacity(), \"Sanity\");\n-        log_info(gc)(\"Trigger (%s): expedite promotion in place of \" SIZE_FORMAT \"%s\", _generation->name(),\n-                     byte_size_in_proper_unit(promo_in_place_potential),\n-                     proper_unit_for_byte_size(promo_in_place_potential));\n-        return true;\n-      } else if (mixed_candidates > 0) {\n-        \/\/ We need to run young GC in order to open up some free heap regions so we can finish mixed evacuations.\n-        log_info(gc)(\"Trigger (%s): expedite mixed evacuation of \" SIZE_FORMAT \" regions\",\n-                     _generation->name(), mixed_candidates);\n-        return true;\n-      }\n-    }\n+  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+  if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n+    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                 _heap_stats->name(), avg_cycle_time * 1000,\n+                 byte_size_in_proper_unit(rate), proper_unit_for_byte_size(rate),\n+                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                 _spike_threshold_sd);\n+    _last_trigger = SPIKE;\n+    return true;\n@@ -569,0 +312,1 @@\n+\n@@ -598,0 +342,7 @@\n+size_t ShenandoahAdaptiveHeuristics::min_free_threshold() {\n+  \/\/ Note that soft_max_capacity() \/ 100 * min_free_threshold is smaller than max_capacity() \/ 100 * min_free_threshold.\n+  \/\/ We want to behave conservatively here, so use max_capacity().  By returning a larger value, we cause the GC to\n+  \/\/ trigger when the remaining amount of free shrinks below the larger threshold.\n+  return _heap_stats->max_capacity() \/ 100 * ShenandoahMinFreeThreshold;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":128,"deletions":377,"binary":false,"changes":505,"status":"modified"},{"patch":"@@ -29,0 +29,4 @@\n+#include \"runtime\/globals_extension.hpp\"\n+#include \"memory\/allocation.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeapStats.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n@@ -55,1 +59,1 @@\n-  ShenandoahAdaptiveHeuristics(ShenandoahGeneration* generation);\n+  ShenandoahAdaptiveHeuristics(ShenandoahHeapStats* heap_stats);\n@@ -74,2 +78,0 @@\n-  virtual size_t bytes_of_allocation_runway_before_gc_trigger(size_t young_regions_to_be_recycled);\n-\n@@ -103,0 +105,3 @@\n+protected:\n+  ShenandoahHeapStats* _heap_stats;\n+\n@@ -107,1 +112,1 @@\n-  \/\/ tend to over estimate the rate at which mutators will deplete the\n+  \/\/ tend to overestimate the rate at which mutators will deplete the\n@@ -130,0 +135,2 @@\n+\n+  size_t min_free_threshold();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":11,"deletions":4,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -35,6 +35,1 @@\n-ShenandoahAggressiveHeuristics::ShenandoahAggressiveHeuristics(ShenandoahGeneration* generation) :\n-  ShenandoahHeuristics(generation) {\n-\n-  assert(!ShenandoahHeap::heap()->mode()->is_generational(),\n-         \"Aggressive heuristics is not available in generational mode\");\n-\n+ShenandoahAggressiveHeuristics::ShenandoahAggressiveHeuristics() : ShenandoahHeuristics() {\n@@ -57,2 +52,0 @@\n-  \/\/ Note that there is no bound on collection set size. If we try to collect too much memory,\n-  \/\/ we'll get an allocation failure during collection and slide to degenerated GC.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-  ShenandoahAggressiveHeuristics(ShenandoahGeneration* generation);\n+  ShenandoahAggressiveHeuristics();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,1 +31,1 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -36,2 +36,1 @@\n-ShenandoahCompactHeuristics::ShenandoahCompactHeuristics(ShenandoahGeneration* generation) :\n-  ShenandoahHeuristics(generation) {\n+ShenandoahCompactHeuristics::ShenandoahCompactHeuristics() : ShenandoahHeuristics() {\n@@ -50,4 +49,5 @@\n-  size_t max_capacity = _generation->max_capacity();\n-  size_t capacity     = _generation->soft_max_capacity();\n-  size_t usage        = _generation->used();\n-  size_t available    = (capacity > usage)? capacity - usage: 0;\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  size_t max_capacity = heap->max_capacity();\n+  size_t capacity = heap->soft_max_capacity();\n+  size_t available = heap->free_set()->available();\n@@ -60,1 +60,1 @@\n-  size_t min_threshold = min_free_threshold();\n+  size_t min_threshold = capacity \/ 100 * ShenandoahMinFreeThreshold;\n@@ -69,1 +69,1 @@\n-  size_t bytes_allocated = _generation->bytes_allocated_since_gc_start();\n+  size_t bytes_allocated = heap->bytes_allocated_since_gc_start();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  ShenandoahCompactHeuristics(ShenandoahGeneration* generation);\n+  ShenandoahCompactHeuristics();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,268 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectionSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+\n+#include \"logging\/log.hpp\"\n+\n+ShenandoahGenerationalHeuristics::ShenandoahGenerationalHeuristics(ShenandoahGeneration* generation)\n+        : ShenandoahAdaptiveHeuristics(generation), _generation(generation) {\n+}\n+\n+void ShenandoahGenerationalHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n+  assert(collection_set->is_empty(), \"Must be empty\");\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+\n+  \/\/ Check all pinned regions have updated status before choosing the collection set.\n+  heap->assert_pinned_region_status();\n+\n+  \/\/ Step 1. Build up the region candidates we care about, rejecting losers and accepting winners right away.\n+\n+  size_t num_regions = heap->num_regions();\n+\n+  RegionData* candidates = _region_data;\n+\n+  size_t cand_idx = 0;\n+  size_t preselected_candidates = 0;\n+\n+  size_t total_garbage = 0;\n+\n+  size_t immediate_garbage = 0;\n+  size_t immediate_regions = 0;\n+\n+  size_t free = 0;\n+  size_t free_regions = 0;\n+\n+  \/\/ This counts number of humongous regions that we intend to promote in this cycle.\n+  size_t humongous_regions_promoted = 0;\n+  \/\/ This counts bytes of memory used by humongous regions to be promoted in place.\n+  size_t humongous_bytes_promoted = 0;\n+  \/\/ This counts number of regular regions that will be promoted in place.\n+  size_t regular_regions_promoted_in_place = 0;\n+  \/\/ This counts bytes of memory used by regular regions to be promoted in place.\n+  size_t regular_regions_promoted_usage = 0;\n+\n+  for (size_t i = 0; i < num_regions; i++) {\n+    ShenandoahHeapRegion* region = heap->get_region(i);\n+    if (!_generation->contains(region)) {\n+      continue;\n+    }\n+    size_t garbage = region->garbage();\n+    total_garbage += garbage;\n+    if (region->is_empty()) {\n+      free_regions++;\n+      free += ShenandoahHeapRegion::region_size_bytes();\n+    } else if (region->is_regular()) {\n+      if (!region->has_live()) {\n+        \/\/ We can recycle it right away and put it in the free set.\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+        region->make_trash_immediate();\n+      } else {\n+        bool is_candidate;\n+        \/\/ This is our candidate for later consideration.\n+        if (collection_set->is_preselected(i)) {\n+          \/\/ If !is_generational, we cannot ask if is_preselected.  If is_preselected, we know\n+          \/\/   region->age() >= InitialTenuringThreshold).\n+          is_candidate = true;\n+          preselected_candidates++;\n+          \/\/ Set garbage value to maximum value to force this into the sorted collection set.\n+          garbage = region_size_bytes;\n+        } else if (region->is_young() && (region->age() >= InitialTenuringThreshold)) {\n+          \/\/ Note that for GLOBAL GC, region may be OLD, and OLD regions do not qualify for pre-selection\n+\n+          \/\/ This region is old enough to be promoted but it was not preselected, either because its garbage is below\n+          \/\/ ShenandoahOldGarbageThreshold so it will be promoted in place, or because there is not sufficient room\n+          \/\/ in old gen to hold the evacuated copies of this region's live data.  In both cases, we choose not to\n+          \/\/ place this region into the collection set.\n+          if (region->get_top_before_promote() != nullptr) {\n+            regular_regions_promoted_in_place++;\n+            regular_regions_promoted_usage += region->used_before_promote();\n+          }\n+          is_candidate = false;\n+        } else {\n+          is_candidate = true;\n+        }\n+        if (is_candidate) {\n+          candidates[cand_idx]._region = region;\n+          candidates[cand_idx]._u._garbage = garbage;\n+          cand_idx++;\n+        }\n+      }\n+    } else if (region->is_humongous_start()) {\n+      \/\/ Reclaim humongous regions here, and count them as the immediate garbage\n+#ifdef ASSERT\n+      bool reg_live = region->has_live();\n+      bool bm_live = heap->complete_marking_context()->is_marked(cast_to_oop(region->bottom()));\n+      assert(reg_live == bm_live,\n+             \"Humongous liveness and marks should agree. Region live: %s; Bitmap live: %s; Region Live Words: \" SIZE_FORMAT,\n+             BOOL_TO_STR(reg_live), BOOL_TO_STR(bm_live), region->get_live_data_words());\n+#endif\n+      if (!region->has_live()) {\n+        heap->trash_humongous_region_at(region);\n+\n+        \/\/ Count only the start. Continuations would be counted on \"trash\" path\n+        immediate_regions++;\n+        immediate_garbage += garbage;\n+      } else {\n+        if (region->is_young() && region->age() >= InitialTenuringThreshold) {\n+          oop obj = cast_to_oop(region->bottom());\n+          size_t humongous_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+          humongous_regions_promoted += humongous_regions;\n+          humongous_bytes_promoted += obj->size() * HeapWordSize;\n+        }\n+      }\n+    } else if (region->is_trash()) {\n+      \/\/ Count in just trashed collection set, during coalesced CM-with-UR\n+      immediate_regions++;\n+      immediate_garbage += garbage;\n+    }\n+  }\n+  heap->reserve_promotable_humongous_regions(humongous_regions_promoted);\n+  heap->reserve_promotable_humongous_usage(humongous_bytes_promoted);\n+  heap->reserve_promotable_regular_regions(regular_regions_promoted_in_place);\n+  heap->reserve_promotable_regular_usage(regular_regions_promoted_usage);\n+  log_info(gc, ergo)(\"Planning to promote in place \" SIZE_FORMAT \" humongous regions and \" SIZE_FORMAT\n+                     \" regular regions, spanning a total of \" SIZE_FORMAT \" used bytes\",\n+                     humongous_regions_promoted, regular_regions_promoted_in_place,\n+                     humongous_regions_promoted * ShenandoahHeapRegion::region_size_bytes() +\n+                     regular_regions_promoted_usage);\n+\n+  \/\/ Step 2. Look back at garbage statistics, and decide if we want to collect anything,\n+  \/\/ given the amount of immediately reclaimable garbage. If we do, figure out the collection set.\n+\n+  assert (immediate_garbage <= total_garbage,\n+          \"Cannot have more immediate garbage than total garbage: \" SIZE_FORMAT \"%s vs \" SIZE_FORMAT \"%s\",\n+          byte_size_in_proper_unit(immediate_garbage), proper_unit_for_byte_size(immediate_garbage),\n+          byte_size_in_proper_unit(total_garbage), proper_unit_for_byte_size(total_garbage));\n+\n+  size_t immediate_percent = (total_garbage == 0) ? 0 : (immediate_garbage * 100 \/ total_garbage);\n+\n+  bool doing_promote_in_place = (humongous_regions_promoted + regular_regions_promoted_in_place > 0);\n+  if (doing_promote_in_place || (preselected_candidates > 0) || (immediate_percent <= ShenandoahImmediateThreshold)) {\n+    \/\/ Only young collections need to prime the collection set.\n+    if (_generation->is_young()) {\n+      heap->old_heuristics()->prime_collection_set(collection_set);\n+    }\n+\n+    \/\/ Call the subclasses to add young-gen regions into the collection set.\n+    choose_collection_set_from_regiondata(collection_set, candidates, cand_idx, immediate_garbage + free);\n+  } else {\n+    \/\/ We are going to skip evacuation and update refs because we reclaimed\n+    \/\/ sufficient amounts of immediate garbage.\n+    heap->shenandoah_policy()->record_abbreviated_cycle();\n+  }\n+\n+  if (collection_set->has_old_regions()) {\n+    heap->shenandoah_policy()->record_mixed_cycle();\n+  }\n+\n+  size_t cset_percent = (total_garbage == 0) ? 0 : (collection_set->garbage() * 100 \/ total_garbage);\n+  size_t collectable_garbage = collection_set->garbage() + immediate_garbage;\n+  size_t collectable_garbage_percent = (total_garbage == 0) ? 0 : (collectable_garbage * 100 \/ total_garbage);\n+\n+  log_info(gc, ergo)(\"Collectable Garbage: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \"\n+                     \"Immediate: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions, \"\n+                     \"CSet: \" SIZE_FORMAT \"%s (\" SIZE_FORMAT \"%%), \" SIZE_FORMAT \" regions\",\n+\n+                     byte_size_in_proper_unit(collectable_garbage),\n+                     proper_unit_for_byte_size(collectable_garbage),\n+                     collectable_garbage_percent,\n+\n+                     byte_size_in_proper_unit(immediate_garbage),\n+                     proper_unit_for_byte_size(immediate_garbage),\n+                     immediate_percent,\n+                     immediate_regions,\n+\n+                     byte_size_in_proper_unit(collection_set->garbage()),\n+                     proper_unit_for_byte_size(collection_set->garbage()),\n+                     cset_percent,\n+                     collection_set->count());\n+\n+  if (collection_set->garbage() > 0) {\n+    size_t young_evac_bytes = collection_set->get_young_bytes_reserved_for_evacuation();\n+    size_t promote_evac_bytes = collection_set->get_young_bytes_to_be_promoted();\n+    size_t old_evac_bytes = collection_set->get_old_bytes_reserved_for_evacuation();\n+    size_t total_evac_bytes = young_evac_bytes + promote_evac_bytes + old_evac_bytes;\n+    log_info(gc, ergo)(\"Evacuation Targets: YOUNG: \" SIZE_FORMAT \"%s, \"\n+                       \"PROMOTE: \" SIZE_FORMAT \"%s, \"\n+                       \"OLD: \" SIZE_FORMAT \"%s, \"\n+                       \"TOTAL: \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(young_evac_bytes), proper_unit_for_byte_size(young_evac_bytes),\n+                       byte_size_in_proper_unit(promote_evac_bytes), proper_unit_for_byte_size(promote_evac_bytes),\n+                       byte_size_in_proper_unit(old_evac_bytes), proper_unit_for_byte_size(old_evac_bytes),\n+                       byte_size_in_proper_unit(total_evac_bytes), proper_unit_for_byte_size(total_evac_bytes));\n+  }\n+}\n+\n+\n+size_t ShenandoahGenerationalHeuristics::add_preselected_regions_to_collection_set(ShenandoahCollectionSet* cset,\n+                                                                                   const RegionData* data,\n+                                                                                   size_t size) const {\n+  \/\/ cur_young_garbage represents the amount of memory to be reclaimed from young-gen.  In the case that live objects\n+  \/\/ are known to be promoted out of young-gen, we count this as cur_young_garbage because this memory is reclaimed\n+  \/\/ from young-gen and becomes available to serve future young-gen allocation requests.\n+  size_t cur_young_garbage = 0;\n+  for (size_t idx = 0; idx < size; idx++) {\n+    ShenandoahHeapRegion* r = data[idx]._region;\n+    if (cset->is_preselected(r->index())) {\n+      assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n+      \/\/ Entire region will be promoted, This region does not impact young-gen or old-gen evacuation reserve.\n+      \/\/ This region has been pre-selected and its impact on promotion reserve is already accounted for.\n+\n+      \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n+      \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n+      \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n+      \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n+      \/\/ within young-gen memory.\n+\n+      cur_young_garbage += r->garbage();\n+      cset->add_region(r);\n+    }\n+  }\n+  return cur_young_garbage;\n+}\n+\n+void ShenandoahGenerationalHeuristics::log_cset_composition(ShenandoahCollectionSet* cset) const {\n+  size_t collected_old = cset->get_old_bytes_reserved_for_evacuation();\n+  size_t collected_promoted = cset->get_young_bytes_to_be_promoted();\n+  size_t collected_young = cset->get_young_bytes_reserved_for_evacuation();\n+\n+  log_info(gc, ergo)(\n+          \"Chosen CSet evacuates young: \" SIZE_FORMAT \"%s (of which at least: \" SIZE_FORMAT \"%s are to be promoted), \"\n+          \"old: \" SIZE_FORMAT \"%s\",\n+          byte_size_in_proper_unit(collected_young), proper_unit_for_byte_size(collected_young),\n+          byte_size_in_proper_unit(collected_promoted), proper_unit_for_byte_size(collected_promoted),\n+          byte_size_in_proper_unit(collected_old), proper_unit_for_byte_size(collected_old));\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.cpp","additions":268,"deletions":0,"binary":false,"changes":268,"status":"added"},{"patch":"@@ -0,0 +1,59 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGENERATIONALHEURISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGENERATIONALHEURISTICS_HPP\n+\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+\n+class ShenandoahGeneration;\n+\n+\/*\n+ * This class serves as the base class for heuristics used to trigger and\n+ * choose the collection sets for young and global collections. It leans\n+ * heavily on the existing functionality of ShenandoahAdaptiveHeuristics.\n+ *\n+ * It differs from the base class primarily in that choosing the collection\n+ * set is responsible for mixed collections and in-place promotions of tenured\n+ * regions.\n+ *\/\n+class ShenandoahGenerationalHeuristics : public ShenandoahAdaptiveHeuristics {\n+\n+public:\n+  explicit ShenandoahGenerationalHeuristics(ShenandoahGeneration* generation);\n+\n+  void choose_collection_set(ShenandoahCollectionSet* collection_set) override;\n+protected:\n+  ShenandoahGeneration* _generation;\n+\n+  size_t add_preselected_regions_to_collection_set(ShenandoahCollectionSet* cset,\n+                                                   const RegionData* data,\n+                                                   size_t size) const;\n+\n+  void log_cset_composition(ShenandoahCollectionSet* cset) const;\n+};\n+\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGENERATIONALHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,130 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+\n+#include \"utilities\/quickSort.hpp\"\n+\n+ShenandoahGlobalHeuristics::ShenandoahGlobalHeuristics(ShenandoahGlobalGeneration* generation)\n+        : ShenandoahGenerationalHeuristics(generation) {\n+}\n+\n+\n+void ShenandoahGlobalHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                       RegionData* data, size_t size,\n+                                                                       size_t actual_free) {\n+  \/\/ The logic for cset selection in adaptive is as follows:\n+  \/\/\n+  \/\/   1. We cannot get cset larger than available free space. Otherwise we guarantee OOME\n+  \/\/      during evacuation, and thus guarantee full GC. In practice, we also want to let\n+  \/\/      application to allocate something. This is why we limit CSet to some fraction of\n+  \/\/      available space. In non-overloaded heap, max_cset would contain all plausible candidates\n+  \/\/      over garbage threshold.\n+  \/\/\n+  \/\/   2. We should not get cset too low so that free threshold would not be met right\n+  \/\/      after the cycle. Otherwise we get back-to-back cycles for no reason if heap is\n+  \/\/      too fragmented. In non-overloaded non-fragmented heap min_garbage would be around zero.\n+  \/\/\n+  \/\/ Therefore, we start by sorting the regions by garbage. Then we unconditionally add the best candidates\n+  \/\/ before we meet min_garbage. Then we add all candidates that fit with a garbage threshold before\n+  \/\/ we hit max_cset. When max_cset is hit, we terminate the cset selection. Note that in this scheme,\n+  \/\/ ShenandoahGarbageThreshold is the soft threshold which would be ignored until min_garbage is hit.\n+\n+  \/\/ In generational mode, the sort order within the data array is not strictly descending amounts of garbage.  In\n+  \/\/ particular, regions that have reached tenure age will be sorted into this array before younger regions that contain\n+  \/\/ more garbage.  This represents one of the reasons why we keep looking at regions even after we decide, for example,\n+  \/\/ to exclude one of the regions because it might require evacuation of too much live data.\n+\n+\n+\n+  \/\/ Better select garbage-first regions\n+  QuickSort::sort<RegionData>(data, (int) size, compare_by_garbage, false);\n+\n+  size_t cur_young_garbage = add_preselected_regions_to_collection_set(cset, data, size);\n+\n+  choose_global_collection_set(cset, data, size, actual_free, cur_young_garbage);\n+\n+  log_cset_composition(cset);\n+}\n+\n+\n+void ShenandoahGlobalHeuristics::choose_global_collection_set(ShenandoahCollectionSet* cset,\n+                                                              const ShenandoahHeuristics::RegionData* data,\n+                                                              size_t size, size_t actual_free,\n+                                                              size_t cur_young_garbage) const {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t capacity = heap->young_generation()->max_capacity();\n+  size_t garbage_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahGarbageThreshold \/ 100;\n+  size_t ignore_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahIgnoreGarbageThreshold \/ 100;\n+\n+  size_t max_young_cset = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t young_cur_cset = 0;\n+  size_t max_old_cset = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n+  size_t old_cur_cset = 0;\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_young_cset;\n+  size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n+\n+  log_info(gc, ergo)(\"Adaptive CSet Selection for GLOBAL. Max Young Evacuation: \" SIZE_FORMAT\n+                     \"%s, Max Old Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n+                     byte_size_in_proper_unit(max_young_cset), proper_unit_for_byte_size(max_young_cset),\n+                     byte_size_in_proper_unit(max_old_cset), proper_unit_for_byte_size(max_old_cset),\n+                     byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+\n+  for (size_t idx = 0; idx < size; idx++) {\n+    ShenandoahHeapRegion* r = data[idx]._region;\n+    if (cset->is_preselected(r->index())) {\n+      continue;\n+    }\n+    bool add_region = false;\n+    if (r->is_old()) {\n+      size_t new_cset = old_cur_cset + r->get_live_data_bytes();\n+      if ((new_cset <= max_old_cset) && (r->garbage() > garbage_threshold)) {\n+        add_region = true;\n+        old_cur_cset = new_cset;\n+      }\n+    } else if (r->age() < InitialTenuringThreshold) {\n+      size_t new_cset = young_cur_cset + r->get_live_data_bytes();\n+      size_t region_garbage = r->garbage();\n+      size_t new_garbage = cur_young_garbage + region_garbage;\n+      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+      if ((new_cset <= max_young_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+        add_region = true;\n+        young_cur_cset = new_cset;\n+        cur_young_garbage = new_garbage;\n+      }\n+    }\n+    \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n+    \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects.\n+\n+    if (add_region) {\n+      cset->add_region(r);\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.cpp","additions":130,"deletions":0,"binary":false,"changes":130,"status":"added"},{"patch":"@@ -0,0 +1,54 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGLOBALHEURISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGLOBALHEURISTICS_HPP\n+\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp\"\n+\n+class ShenandoahGlobalGeneration;\n+\n+\/*\n+ * This is a specialization of the generational heuristics which is aware\n+ * of old and young regions and respects the configured evacuation parameters\n+ * for such regions during a global collection of a generational heap.\n+ *\/\n+class ShenandoahGlobalHeuristics : public ShenandoahGenerationalHeuristics {\n+public:\n+  ShenandoahGlobalHeuristics(ShenandoahGlobalGeneration* generation);\n+\n+  void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                             RegionData* data, size_t size,\n+                                             size_t actual_free) override;\n+\n+private:\n+  void choose_global_collection_set(ShenandoahCollectionSet* cset,\n+                                    const ShenandoahHeuristics::RegionData* data,\n+                                    size_t size, size_t actual_free,\n+                                    size_t cur_young_garbage) const;\n+};\n+\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHGLOBALHEURISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.hpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -0,0 +1,41 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHHEAPCHARACTERISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHHEAPCHARACTERISTICS_HPP\n+\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+class ShenandoahHeapStats {\n+public:\n+  virtual const char* name() const = 0;\n+  virtual size_t soft_max_capacity() const = 0;\n+  virtual size_t max_capacity() const = 0;\n+  virtual size_t used() const = 0;\n+  virtual size_t available() const = 0;\n+  virtual size_t soft_available() const = 0;\n+  virtual size_t bytes_allocated_since_gc_start() const = 0;\n+};\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHHEAPCHARACTERISTICS_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeapStats.hpp","additions":41,"deletions":0,"binary":false,"changes":41,"status":"added"},{"patch":"@@ -28,2 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n-#include \"gc\/shenandoah\/shenandoahCollectionSet.inline.hpp\"\n@@ -31,2 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -35,2 +31,0 @@\n-#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n-#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -38,1 +32,0 @@\n-#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -44,14 +37,0 @@\n-inline void assert_no_in_place_promotions() {\n-#ifdef ASSERT\n-  class ShenandoahNoInPlacePromotions : public ShenandoahHeapRegionClosure {\n-  public:\n-    void heap_region_do(ShenandoahHeapRegion *r) override {\n-      assert(r->get_top_before_promote() == nullptr,\n-             \"Region \" SIZE_FORMAT \" should not be ready for in-place promotion\", r->index());\n-    }\n-  } cl;\n-  ShenandoahHeap::heap()->heap_region_iterate(&cl);\n-#endif\n-}\n-\n-\n@@ -67,21 +46,11 @@\n-\/\/ sort by increasing live (so least live comes first)\n-int ShenandoahHeuristics::compare_by_live(RegionData a, RegionData b) {\n-  if (a._u._live_data < b._u._live_data)\n-    return -1;\n-  else if (a._u._live_data > b._u._live_data)\n-    return 1;\n-  else return 0;\n-}\n-\n-ShenandoahHeuristics::ShenandoahHeuristics(ShenandoahGeneration* generation) :\n-  _generation(generation),\n-  _region_data(nullptr),\n-  _degenerated_cycles_in_a_row(0),\n-  _successful_cycles_in_a_row(0),\n-  _guaranteed_gc_interval(0),\n-  _cycle_start(os::elapsedTime()),\n-  _last_cycle_end(0),\n-  _gc_times_learned(0),\n-  _gc_time_penalties(0),\n-  _gc_cycle_time_history(new TruncatedSeq(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor)),\n-  _metaspace_oom()\n+ShenandoahHeuristics::ShenandoahHeuristics() :\n+        _region_data(nullptr),\n+        _degenerated_cycles_in_a_row(0),\n+        _successful_cycles_in_a_row(0),\n+        _guaranteed_gc_interval(0),\n+        _cycle_start(os::elapsedTime()),\n+        _last_cycle_end(0),\n+        _gc_times_learned(0),\n+        _gc_time_penalties(0),\n+        _gc_cycle_time_history(new TruncatedSeq(Moving_Average_Samples, ShenandoahAdaptiveDecayFactor)),\n+        _metaspace_oom()\n@@ -104,168 +73,2 @@\n-typedef struct {\n-  ShenandoahHeapRegion* _region;\n-  size_t _live_data;\n-} AgedRegionData;\n-\n-static int compare_by_aged_live(AgedRegionData a, AgedRegionData b) {\n-  if (a._live_data < b._live_data)\n-    return -1;\n-  else if (a._live_data > b._live_data)\n-    return 1;\n-  else return 0;\n-}\n-\n-\/\/ Preselect for inclusion into the collection set regions whose age is at or above tenure age which contain more than\n-\/\/ ShenandoahOldGarbageThreshold amounts of garbage.  We identify these regions by setting the appropriate entry of\n-\/\/ candidate_regions_for_promotion_by_copy[] to true.  All entries are initialized to false before calling this\n-\/\/ function.\n-\/\/\n-\/\/ During the subsequent selection of the collection set, we give priority to these promotion set candidates.\n-\/\/ Without this prioritization, we found that the aged regions tend to be ignored because they typically have\n-\/\/ much less garbage and much more live data than the recently allocated \"eden\" regions.  When aged regions are\n-\/\/ repeatedly excluded from the collection set, the amount of live memory within the young generation tends to\n-\/\/ accumulate and this has the undesirable side effect of causing young-generation collections to require much more\n-\/\/ CPU and wall-clock time.\n-\/\/\n-\/\/ A second benefit of treating aged regions differently than other regions during collection set selection is\n-\/\/ that this allows us to more accurately budget memory to hold the results of evacuation.  Memory for evacuation\n-\/\/ of aged regions must be reserved in the old generations.  Memory for evacuation of all other regions must be\n-\/\/ reserved in the young generation.\n-\/\/\n-\/\/ A side effect performed by this function is to tally up the number of regions and the number of live bytes\n-\/\/ that we plan to promote-in-place during the current GC cycle.  This information, which is stored with\n-\/\/ an invocation of heap->set_promotion_in_place_potential(), feeds into subsequent decisions about when to\n-\/\/ trigger the next GC and may identify special work to be done during this GC cycle if we choose to abbreviate it.\n-\/\/\n-\/\/ Returns bytes of old-gen memory consumed by selected aged regions\n-size_t ShenandoahHeuristics::select_aged_regions(size_t old_available, size_t num_regions,\n-                                                 bool candidate_regions_for_promotion_by_copy[]) {\n-\n-  \/\/ There should be no regions configured for subsequent in-place-promotions carried over from the previous cycle.\n-  assert_no_in_place_promotions();\n-\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  assert(heap->mode()->is_generational(), \"Only in generational mode\");\n-  ShenandoahMarkingContext* const ctx = heap->marking_context();\n-  size_t old_consumed = 0;\n-  size_t promo_potential = 0;\n-  size_t anticipated_promote_in_place_live = 0;\n-\n-  heap->clear_promotion_in_place_potential();\n-  heap->clear_promotion_potential();\n-  size_t candidates = 0;\n-  size_t candidates_live = 0;\n-  size_t old_garbage_threshold = (ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold) \/ 100;\n-  size_t promote_in_place_regions = 0;\n-  size_t promote_in_place_live = 0;\n-  size_t promote_in_place_pad = 0;\n-  size_t anticipated_candidates = 0;\n-  size_t anticipated_promote_in_place_regions = 0;\n-\n-  \/\/ Sort the promotion-eligible regions according to live-data-bytes so that we can first reclaim regions that require\n-  \/\/ less evacuation effort.  This prioritizes garbage first, expanding the allocation pool before we begin the work of\n-  \/\/ reclaiming regions that require more effort.\n-  AgedRegionData* sorted_regions = (AgedRegionData*) alloca(num_regions * sizeof(AgedRegionData));\n-  for (size_t i = 0; i < num_regions; i++) {\n-    ShenandoahHeapRegion* r = heap->get_region(i);\n-    if (r->is_empty() || !r->has_live() || !r->is_young() || !r->is_regular()) {\n-      continue;\n-    }\n-    if (r->age() >= InitialTenuringThreshold) {\n-      if ((r->garbage() < old_garbage_threshold)) {\n-        HeapWord* tams = ctx->top_at_mark_start(r);\n-        HeapWord* original_top = r->top();\n-        if (tams == original_top) {\n-          \/\/ No allocations from this region have been made during concurrent mark. It meets all the criteria\n-          \/\/ for in-place-promotion. Though we only need the value of top when we fill the end of the region,\n-          \/\/ we use this field to indicate that this region should be promoted in place during the evacuation\n-          \/\/ phase.\n-          r->save_top_before_promote();\n-\n-          size_t remnant_size = r->free() \/ HeapWordSize;\n-          if (remnant_size > ShenandoahHeap::min_fill_size()) {\n-            ShenandoahHeap::fill_with_object(original_top, remnant_size);\n-            \/\/ Fill the remnant memory within this region to assure no allocations prior to promote in place.  Otherwise,\n-            \/\/ newly allocated objects will not be parseable when promote in place tries to register them.  Furthermore, any\n-            \/\/ new allocations would not necessarily be eligible for promotion.  This addresses both issues.\n-            r->set_top(r->end());\n-            promote_in_place_pad += remnant_size * HeapWordSize;\n-          } else {\n-            \/\/ Since the remnant is so small that it cannot be filled, we don't have to worry about any accidental\n-            \/\/ allocations occurring within this region before the region is promoted in place.\n-          }\n-          promote_in_place_regions++;\n-          promote_in_place_live += r->get_live_data_bytes();\n-        }\n-        \/\/ Else, we do not promote this region (either in place or by copy) because it has received new allocations.\n-\n-        \/\/ During evacuation, we exclude from promotion regions for which age > tenure threshold, garbage < garbage-threshold,\n-        \/\/  and get_top_before_promote() != tams\n-      } else {\n-        \/\/ After sorting and selecting best candidates below, we may decide to exclude this promotion-eligible region\n-        \/\/ from the current collection sets.  If this happens, we will consider this region as part of the anticipated\n-        \/\/ promotion potential for the next GC pass.\n-        size_t live_data = r->get_live_data_bytes();\n-        candidates_live += live_data;\n-        sorted_regions[candidates]._region = r;\n-        sorted_regions[candidates++]._live_data = live_data;\n-      }\n-    } else {\n-      \/\/ We only anticipate to promote regular regions if garbage() is above threshold.  Tenure-aged regions with less\n-      \/\/ garbage are promoted in place.  These take a different path to old-gen.  Note that certain regions that are\n-      \/\/ excluded from anticipated promotion because their garbage content is too low (causing us to anticipate that\n-      \/\/ the region would be promoted in place) may be eligible for evacuation promotion by the time promotion takes\n-      \/\/ place during a subsequent GC pass because more garbage is found within the region between now and then.  This\n-      \/\/ should not happen if we are properly adapting the tenure age.  The theory behind adaptive tenuring threshold\n-      \/\/ is to choose the youngest age that demonstrates no \"significant\" futher loss of population since the previous\n-      \/\/ age.  If not this, we expect the tenure age to demonstrate linear population decay for at least two population\n-      \/\/ samples, whereas we expect to observe exponetial population decay for ages younger than the tenure age.\n-      \/\/\n-      \/\/ In the case that certain regions which were anticipated to be promoted in place need to be promoted by\n-      \/\/ evacuation, it may be the case that there is not sufficient reserve within old-gen to hold evacuation of\n-      \/\/ these regions.  The likely outcome is that these regions will not be selected for evacuation or promotion\n-      \/\/ in the current cycle and we will anticipate that they will be promoted in the next cycle.  This will cause\n-      \/\/ us to reserve more old-gen memory so that these objects can be promoted in the subsequent cycle.\n-      \/\/\n-      \/\/ TODO:\n-      \/\/   If we are auto-tuning the tenure age and regions that were anticipated to be promoted in place end up\n-      \/\/   being promoted by evacuation, this event should feed into the tenure-age-selection heuristic so that\n-      \/\/   the tenure age can be increased.\n-      if (heap->is_aging_cycle() && (r->age() + 1 == InitialTenuringThreshold)) {\n-        if (r->garbage() >= old_garbage_threshold) {\n-          anticipated_candidates++;\n-          promo_potential += r->get_live_data_bytes();\n-        }\n-        else {\n-          anticipated_promote_in_place_regions++;\n-          anticipated_promote_in_place_live += r->get_live_data_bytes();\n-        }\n-      }\n-    }\n-    \/\/ Note that we keep going even if one region is excluded from selection.\n-    \/\/ Subsequent regions may be selected if they have smaller live data.\n-  }\n-  \/\/ Sort in increasing order according to live data bytes.  Note that candidates represents the number of regions\n-  \/\/ that qualify to be promoted by evacuation.\n-  if (candidates > 0) {\n-    QuickSort::sort<AgedRegionData>(sorted_regions, candidates, compare_by_aged_live, false);\n-    for (size_t i = 0; i < candidates; i++) {\n-      size_t region_live_data = sorted_regions[i]._live_data;\n-      size_t promotion_need = (size_t) (region_live_data * ShenandoahPromoEvacWaste);\n-      if (old_consumed + promotion_need <= old_available) {\n-        ShenandoahHeapRegion* region = sorted_regions[i]._region;\n-        old_consumed += promotion_need;\n-        candidate_regions_for_promotion_by_copy[region->index()] = true;\n-      } else {\n-        \/\/ We rejected this promotable region from the collection set because we had no room to hold its copy.\n-        \/\/ Add this region to promo potential for next GC.\n-        promo_potential += region_live_data;\n-      }\n-      \/\/ We keep going even if one region is excluded from selection because we need to accumulate all eligible\n-      \/\/ regions that are not preselected into promo_potential\n-    }\n-  }\n-  heap->set_pad_for_promote_in_place(promote_in_place_pad);\n-  heap->set_promotion_potential(promo_potential);\n-  heap->set_promotion_in_place_potential(anticipated_promote_in_place_live);\n-  return old_consumed;\n-}\n+void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set) {\n+  assert(collection_set->is_empty(), \"Must be empty\");\n@@ -273,1 +76,0 @@\n-void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) {\n@@ -275,5 +77,0 @@\n-  bool is_generational = heap->mode()->is_generational();\n-  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n-\n-  assert(collection_set->count() == 0, \"Must be empty\");\n-  assert(!is_generational || !_generation->is_old(), \"Old GC invokes ShenandoahOldHeuristics::choose_collection_set()\");\n@@ -291,1 +88,0 @@\n-  size_t preselected_candidates = 0;\n@@ -301,9 +97,1 @@\n-  size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n-  \/\/ This counts number of humongous regions that we intend to promote in this cycle.\n-  size_t humongous_regions_promoted = 0;\n-  \/\/ This counts bytes of memory used by hunongous regions to be promoted in place.\n-  size_t humongous_bytes_promoted = 0;\n-  \/\/ This counts number of regular regions that will be promoted in place.\n-  size_t regular_regions_promoted_in_place = 0;\n-  \/\/ This counts bytes of memory used by regular regions to be promoted in place.\n-  size_t regular_regions_promoted_usage = 0;\n+  ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n@@ -313,3 +101,1 @@\n-    if (is_generational && !in_generation(region)) {\n-      continue;\n-    }\n+\n@@ -318,0 +104,1 @@\n+\n@@ -328,2 +115,0 @@\n-        assert(!_generation->is_old(), \"OLD is handled elsewhere\");\n-        bool is_candidate;\n@@ -331,27 +116,3 @@\n-        if (is_generational && collection_set->is_preselected(i)) {\n-          \/\/ If !is_generational, we cannot ask if is_preselected.  If is_preselected, we know\n-          \/\/   region->age() >= InitialTenuringThreshold).\n-          is_candidate = true;\n-          preselected_candidates++;\n-          \/\/ Set garbage value to maximum value to force this into the sorted collection set.\n-          garbage = region_size_bytes;\n-        } else if (is_generational && region->is_young() && (region->age() >= InitialTenuringThreshold)) {\n-          \/\/ Note that for GLOBAL GC, region may be OLD, and OLD regions do not qualify for pre-selection\n-\n-          \/\/ This region is old enough to be promoted but it was not preselected, either because its garbage is below\n-          \/\/ ShenandoahOldGarbageThreshold so it will be promoted in place, or because there is not sufficient room\n-          \/\/ in old gen to hold the evacuated copies of this region's live data.  In both cases, we choose not to\n-          \/\/ place this region into the collection set.\n-          if (region->get_top_before_promote() != nullptr) {\n-            regular_regions_promoted_in_place++;\n-            regular_regions_promoted_usage += region->used_before_promote();\n-          }\n-          is_candidate = false;\n-        } else {\n-          is_candidate = true;\n-        }\n-        if (is_candidate) {\n-          candidates[cand_idx]._region = region;\n-          candidates[cand_idx]._u._garbage = garbage;\n-          cand_idx++;\n-        }\n+        candidates[cand_idx]._region = region;\n+        candidates[cand_idx]._u._garbage = garbage;\n+        cand_idx++;\n@@ -363,1 +124,1 @@\n-      bool bm_live = heap->complete_marking_context()->is_marked(cast_to_oop(region->bottom()));\n+      bool bm_live = ctx->is_marked(cast_to_oop(region->bottom()));\n@@ -374,7 +135,0 @@\n-      } else {\n-        if (region->is_young() && region->age() >= InitialTenuringThreshold) {\n-          oop obj = cast_to_oop(region->bottom());\n-          size_t humongous_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n-          humongous_regions_promoted += humongous_regions;\n-          humongous_bytes_promoted += obj->size() * HeapWordSize;\n-        }\n@@ -388,8 +142,0 @@\n-  heap->reserve_promotable_humongous_regions(humongous_regions_promoted);\n-  heap->reserve_promotable_humongous_usage(humongous_bytes_promoted);\n-  heap->reserve_promotable_regular_regions(regular_regions_promoted_in_place);\n-  heap->reserve_promotable_regular_usage(regular_regions_promoted_usage);\n-  log_info(gc, ergo)(\"Planning to promote in place \" SIZE_FORMAT \" humongous regions and \" SIZE_FORMAT\n-                     \" regular regions, spanning a total of \" SIZE_FORMAT \" used bytes\",\n-                     humongous_regions_promoted, regular_regions_promoted_in_place,\n-                     humongous_regions_promoted * ShenandoahHeapRegion::region_size_bytes() + regular_regions_promoted_usage);\n@@ -406,11 +152,0 @@\n-  collection_set->set_immediate_trash(immediate_garbage);\n-\n-  ShenandoahGeneration* young_gen = heap->young_generation();\n-  bool doing_promote_in_place = (humongous_regions_promoted + regular_regions_promoted_in_place > 0);\n-  if (doing_promote_in_place || (preselected_candidates > 0) || (immediate_percent <= ShenandoahImmediateThreshold)) {\n-    if (old_heuristics != nullptr) {\n-      old_heuristics->prime_collection_set(collection_set);\n-    } else {\n-      \/\/ This is a global collection and does not need to prime cset\n-      assert(_generation->is_global(), \"Expected global collection here\");\n-    }\n@@ -418,1 +153,1 @@\n-    \/\/ Call the subclasses to add young-gen regions into the collection set.\n+  if (immediate_percent <= ShenandoahImmediateThreshold) {\n@@ -426,4 +161,0 @@\n-  if (collection_set->has_old_regions()) {\n-    heap->shenandoah_policy()->record_mixed_cycle();\n-  }\n-\n@@ -451,15 +182,0 @@\n-\n-  if (collection_set->garbage() > 0) {\n-    size_t young_evac_bytes   = collection_set->get_young_bytes_reserved_for_evacuation();\n-    size_t promote_evac_bytes = collection_set->get_young_bytes_to_be_promoted();\n-    size_t old_evac_bytes     = collection_set->get_old_bytes_reserved_for_evacuation();\n-    size_t total_evac_bytes   = young_evac_bytes + promote_evac_bytes + old_evac_bytes;\n-    log_info(gc, ergo)(\"Evacuation Targets: YOUNG: \" SIZE_FORMAT \"%s, \"\n-                       \"PROMOTE: \" SIZE_FORMAT \"%s, \"\n-                       \"OLD: \" SIZE_FORMAT \"%s, \"\n-                       \"TOTAL: \" SIZE_FORMAT \"%s\",\n-                       byte_size_in_proper_unit(young_evac_bytes),   proper_unit_for_byte_size(young_evac_bytes),\n-                       byte_size_in_proper_unit(promote_evac_bytes), proper_unit_for_byte_size(promote_evac_bytes),\n-                       byte_size_in_proper_unit(old_evac_bytes),     proper_unit_for_byte_size(old_evac_bytes),\n-                       byte_size_in_proper_unit(total_evac_bytes),   proper_unit_for_byte_size(total_evac_bytes));\n-  }\n@@ -487,2 +203,2 @@\n-      log_info(gc)(\"Trigger (%s): Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n-                   _generation->name(), last_time_ms, _guaranteed_gc_interval);\n+      log_info(gc)(\"Trigger: Time since last GC (%.0f ms) is larger than guaranteed interval (\" UINTX_FORMAT \" ms)\",\n+                   last_time_ms, _guaranteed_gc_interval);\n@@ -550,4 +266,0 @@\n-  reset_gc_learning();\n-}\n-\n-void ShenandoahHeuristics::reset_gc_learning() {\n@@ -585,6 +297,0 @@\n-size_t ShenandoahHeuristics::bytes_of_allocation_runway_before_gc_trigger(size_t young_regions_to_be_recycled) {\n-  assert(false, \"Only implemented for young Adaptive Heuristics\");\n-  return 0;\n-}\n-\n-\n@@ -594,15 +300,0 @@\n-\n-bool ShenandoahHeuristics::in_generation(ShenandoahHeapRegion* region) {\n-  return _generation->is_global()\n-          || (_generation->is_young() && region->is_young())\n-          || (_generation->is_old()   && region->is_old());\n-}\n-\n-size_t ShenandoahHeuristics::min_free_threshold() {\n-  assert(!_generation->is_old(), \"min_free_threshold is only relevant to young GC\");\n-  size_t min_free_threshold = ShenandoahMinFreeThreshold;\n-  \/\/ Note that soft_max_capacity() \/ 100 * min_free_threshold is smaller than max_capacity() \/ 100 * min_free_threshold.\n-  \/\/ We want to behave conservatively here, so use max_capacity().  By returning a larger value, we cause the GC to\n-  \/\/ trigger when the remaining amount of free shrinks below the larger threshold.\n-  return _generation->max_capacity() \/ 100 * min_free_threshold;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":23,"deletions":332,"binary":false,"changes":355,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeapStats.hpp\"\n@@ -60,2 +61,0 @@\n-class ShenandoahGeneration;\n-class ShenandoahOldHeuristics;\n@@ -79,2 +78,0 @@\n-  ShenandoahGeneration* _generation;\n-\n@@ -112,8 +109,0 @@\n-  \/\/ Compare by live is used to prioritize compaction of old-gen regions.  With old-gen compaction, the goal is\n-  \/\/ to tightly pack long-lived objects into available regions.  In most cases, there has not been an accumulation\n-  \/\/ of garbage within old-gen regions.  The more likely opportunity will be to combine multiple sparsely populated\n-  \/\/ old-gen regions which may have been promoted in place into a smaller number of densely packed old-gen regions.\n-  \/\/ This improves subsequent allocation efficiency and reduces the likelihood of allocation failure (including\n-  \/\/ humongous allocation failure) due to fragmentation of the available old-gen allocation pool\n-  static int compare_by_live(RegionData a, RegionData b);\n-\n@@ -130,4 +119,0 @@\n-  bool in_generation(ShenandoahHeapRegion* region);\n-\n-  size_t min_free_threshold();\n-\n@@ -135,1 +120,1 @@\n-  ShenandoahHeuristics(ShenandoahGeneration* generation);\n+  ShenandoahHeuristics();\n@@ -146,4 +131,0 @@\n-  uint degenerated_cycles_in_a_row() {\n-    return _degenerated_cycles_in_a_row;\n-  }\n-\n@@ -168,5 +149,1 @@\n-  virtual void reset_gc_learning();\n-\n-  virtual size_t select_aged_regions(size_t old_available, size_t num_regions, bool candidate_regions_for_promotion_by_copy[]);\n-\n-  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics);\n+  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set);\n@@ -183,2 +160,0 @@\n-  virtual size_t bytes_of_allocation_runway_before_gc_trigger(size_t region_to_be_recycled);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":3,"deletions":28,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -39,2 +39,11 @@\n-ShenandoahOldHeuristics::ShenandoahOldHeuristics(ShenandoahOldGeneration* generation, ShenandoahHeuristics* trigger_heuristic) :\n-  ShenandoahHeuristics(generation),\n+\/\/ sort by increasing live (so least live comes first)\n+int ShenandoahOldHeuristics::compare_by_live(RegionData a, RegionData b) {\n+  if (a._u._live_data < b._u._live_data)\n+    return -1;\n+  else if (a._u._live_data > b._u._live_data)\n+    return 1;\n+  else return 0;\n+}\n+\n+ShenandoahOldHeuristics::ShenandoahOldHeuristics(ShenandoahOldGeneration* generation) :\n+  ShenandoahHeuristics(),\n@@ -45,1 +54,1 @@\n-  _trigger_heuristic(trigger_heuristic),\n+  _live_bytes_in_unprocessed_candidates(0),\n@@ -47,1 +56,0 @@\n-  _promotion_failed(false),\n@@ -50,3 +58,1 @@\n-  _growth_trigger(false)\n-{\n-  assert(_generation->is_old(), \"This service only available for old-gc heuristics\");\n+  _growth_trigger(false) {\n@@ -285,10 +291,0 @@\n-\/\/ Both arguments are don't cares for old-gen collections\n-void ShenandoahOldHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set,\n-                                                    ShenandoahOldHeuristics* old_heuristics) {\n-  assert(collection_set == nullptr, \"Expect null\");\n-  assert(old_heuristics == nullptr, \"Expect null\");\n-  \/\/ Old-gen doesn't actually choose a collection set to be evacuated by its own gang of worker tasks.\n-  \/\/ Instead, it computes the set of regions to be evacuated by subsequent young-gen evacuation passes.\n-  prepare_for_old_collections();\n-}\n-\n@@ -296,1 +292,0 @@\n-  assert(_generation->is_old(), \"This service only available for old-gc heuristics\");\n@@ -309,1 +304,1 @@\n-    if (!in_generation(region)) {\n+    if (!_old_generation->contains(region)) {\n@@ -370,1 +365,1 @@\n-  \/\/ Englightened interpretation: collect regions that have less than this amount of live.\n+  \/\/ Enlightened interpretation: collect regions that have less than this amount of live.\n@@ -425,3 +420,2 @@\n-\n-\/\/ TODO: Unused?\n-uint ShenandoahOldHeuristics::last_old_collection_candidate_index() {\n+\/\/ Used by unit test: test_shenandoahOldHeuristic.cpp\n+uint ShenandoahOldHeuristics::last_old_collection_candidate_index() const {\n@@ -431,1 +425,1 @@\n-uint ShenandoahOldHeuristics::unprocessed_old_collection_candidates() {\n+uint ShenandoahOldHeuristics::unprocessed_old_collection_candidates() const {\n@@ -475,8 +469,0 @@\n-void ShenandoahOldHeuristics::handle_promotion_failure() {\n-  _promotion_failed = true;\n-}\n-\n-void ShenandoahOldHeuristics::record_cycle_start() {\n-  _trigger_heuristic->record_cycle_start();\n-}\n-\n@@ -484,1 +470,1 @@\n-  _trigger_heuristic->record_cycle_end();\n+  this->ShenandoahHeuristics::record_cycle_end();\n@@ -495,1 +481,0 @@\n-  _promotion_failed = false;\n@@ -514,1 +499,1 @@\n-    double percent = 100.0 * ((double) old_gen_capacity) \/ heap_capacity;\n+    double percent = percent_of(old_gen_capacity, heap_capacity);\n@@ -527,1 +512,1 @@\n-    double percent = 100.0 * ((double) fragmented_free) \/ used_regions_size;\n+    double percent = percent_of(fragmented_free, used_regions_size);\n@@ -542,1 +527,1 @@\n-      double percent_growth = 100.0 * ((double) current_usage - live_at_previous_old) \/ live_at_previous_old;\n+      double percent_growth = percent_of(current_usage - live_at_previous_old, live_at_previous_old);\n@@ -553,6 +538,2 @@\n-  \/\/ Otherwise, defer to configured heuristic for gc trigger.\n-  return _trigger_heuristic->should_start_gc();\n-}\n-\n-bool ShenandoahOldHeuristics::should_degenerate_cycle() {\n-  return _trigger_heuristic->should_degenerate_cycle();\n+  \/\/ Otherwise, defer to inherited heuristic for gc trigger.\n+  return this->ShenandoahHeuristics::should_start_gc();\n@@ -562,1 +543,1 @@\n-  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n@@ -564,1 +545,1 @@\n-  _trigger_heuristic->record_success_concurrent(abbreviated);\n+  this->ShenandoahHeuristics::record_success_concurrent(abbreviated);\n@@ -568,1 +549,1 @@\n-  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n@@ -570,1 +551,1 @@\n-  _trigger_heuristic->record_success_degenerated();\n+  this->ShenandoahHeuristics::record_success_degenerated();\n@@ -574,1 +555,1 @@\n-  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  \/\/ Forget any triggers that occurred while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n@@ -576,25 +557,1 @@\n-  _trigger_heuristic->record_success_full();\n-}\n-\n-void ShenandoahOldHeuristics::record_allocation_failure_gc() {\n-  _trigger_heuristic->record_allocation_failure_gc();\n-}\n-\n-void ShenandoahOldHeuristics::record_requested_gc() {\n-  _trigger_heuristic->record_requested_gc();\n-}\n-\n-void ShenandoahOldHeuristics::reset_gc_learning() {\n-  _trigger_heuristic->reset_gc_learning();\n-}\n-\n-bool ShenandoahOldHeuristics::can_unload_classes() {\n-  return _trigger_heuristic->can_unload_classes();\n-}\n-\n-bool ShenandoahOldHeuristics::can_unload_classes_normal() {\n-  return _trigger_heuristic->can_unload_classes_normal();\n-}\n-\n-bool ShenandoahOldHeuristics::should_unload_classes() {\n-  return _trigger_heuristic->should_unload_classes();\n+  this->ShenandoahHeuristics::record_success_full();\n@@ -604,3 +561,1 @@\n-  static char name[128];\n-  jio_snprintf(name, sizeof(name), \"%s (OLD)\", _trigger_heuristic->name());\n-  return name;\n+  return \"Old\";\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":31,"deletions":76,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -35,0 +35,13 @@\n+\/*\n+ * This heuristic is responsible for choosing a set of candidates for inclusion\n+ * in mixed collections. These candidates are chosen when marking of the old\n+ * generation is complete. Note that this list of candidates may live through\n+ * several mixed collections.\n+ *\n+ * This heuristic is also responsible for triggering old collections. It has its\n+ * own collection of triggers to decide whether to start an old collection. It does\n+ * _not_ use any of the functionality from the adaptive heuristics for triggers.\n+ * It also does not use any of the functionality from the heuristics base classes\n+ * to choose the collection set. For these reasons, it does not extend from\n+ * ShenandoahGenerationalHeuristics.\n+ *\/\n@@ -75,3 +88,0 @@\n-  \/\/ This can be the 'static' or 'adaptive' heuristic.\n-  ShenandoahHeuristics* _trigger_heuristic;\n-\n@@ -81,4 +91,0 @@\n-  \/\/ Flag is set when promotion failure is detected (by gc thread), and cleared when\n-  \/\/ old generation collection begins (by control thread).\n-  volatile bool _promotion_failed;\n-\n@@ -91,0 +97,8 @@\n+  \/\/ Compare by live is used to prioritize compaction of old-gen regions.  With old-gen compaction, the goal is\n+  \/\/ to tightly pack long-lived objects into available regions.  In most cases, there has not been an accumulation\n+  \/\/ of garbage within old-gen regions.  The more likely opportunity will be to combine multiple sparsely populated\n+  \/\/ old-gen regions which may have been promoted in place into a smaller number of densely packed old-gen regions.\n+  \/\/ This improves subsequent allocation efficiency and reduces the likelihood of allocation failure (including\n+  \/\/ humongous allocation failure) due to fragmentation of the available old-gen allocation pool\n+  static int compare_by_live(RegionData a, RegionData b);\n+\n@@ -96,3 +110,1 @@\n-  ShenandoahOldHeuristics(ShenandoahOldGeneration* generation, ShenandoahHeuristics* trigger_heuristic);\n-\n-  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) override;\n+  ShenandoahOldHeuristics(ShenandoahOldGeneration* generation);\n@@ -107,1 +119,1 @@\n-  uint unprocessed_old_collection_candidates();\n+  uint unprocessed_old_collection_candidates() const;\n@@ -117,1 +129,1 @@\n-  uint last_old_collection_candidate_index();\n+  uint last_old_collection_candidate_index() const;\n@@ -141,5 +153,0 @@\n-  \/\/ Promotion failure does not currently trigger old-gen collections.  Often, promotion failures occur because\n-  \/\/ old-gen is sized too small rather than because it is necessary to collect old gen.  We keep the method\n-  \/\/ here in case we decide to feed this signal to sizing or triggering heuristics in the future.\n-  void handle_promotion_failure();\n-\n@@ -152,17 +159,1 @@\n-  virtual void record_cycle_start() override;\n-\n-  virtual void record_cycle_end() override;\n-\n-  virtual bool should_start_gc() override;\n-\n-  virtual bool should_degenerate_cycle() override;\n-\n-  virtual void record_success_concurrent(bool abbreviated) override;\n-\n-  virtual void record_success_degenerated() override;\n-\n-  virtual void record_success_full() override;\n-\n-  virtual void record_allocation_failure_gc() override;\n-\n-  virtual void record_requested_gc() override;\n+  void record_cycle_end() override;\n@@ -170,1 +161,1 @@\n-  virtual void reset_gc_learning() override;\n+  bool should_start_gc() override;\n@@ -172,1 +163,1 @@\n-  virtual bool can_unload_classes() override;\n+  void record_success_concurrent(bool abbreviated) override;\n@@ -174,1 +165,1 @@\n-  virtual bool can_unload_classes_normal() override;\n+  void record_success_degenerated() override;\n@@ -176,1 +167,1 @@\n-  virtual bool should_unload_classes() override;\n+  void record_success_full() override;\n@@ -178,1 +169,1 @@\n-  virtual const char* name() override;\n+  const char* name() override;\n@@ -180,1 +171,1 @@\n-  virtual bool is_diagnostic() override;\n+  bool is_diagnostic() override;\n@@ -182,1 +173,1 @@\n-  virtual bool is_experimental() override;\n+  bool is_experimental() override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":32,"deletions":41,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -32,2 +32,0 @@\n-  ShenandoahPassiveHeuristics(ShenandoahGeneration* generation)\n-    : ShenandoahHeuristics(generation) {}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -32,1 +33,0 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -36,2 +36,1 @@\n-ShenandoahStaticHeuristics::ShenandoahStaticHeuristics(ShenandoahGeneration* generation) :\n-  ShenandoahHeuristics(generation) {\n+ShenandoahStaticHeuristics::ShenandoahStaticHeuristics() : ShenandoahHeuristics() {\n@@ -45,3 +44,5 @@\n-  size_t max_capacity = _generation->max_capacity();\n-  size_t capacity     = _generation->soft_max_capacity();\n-  size_t available    = _generation->available();\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  size_t max_capacity = heap->max_capacity();\n+  size_t capacity = heap->soft_max_capacity();\n+  size_t available = heap->free_set()->available();\n@@ -53,1 +54,1 @@\n-  size_t threshold_available = min_free_threshold();\n+  size_t threshold_available = capacity \/ 100 * ShenandoahMinFreeThreshold;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.cpp","additions":8,"deletions":7,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  ShenandoahStaticHeuristics(ShenandoahGeneration* generation);\n+  ShenandoahStaticHeuristics();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,244 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCollectorPolicy.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+\n+#include \"utilities\/quickSort.hpp\"\n+#include \"shenandoahAdaptiveHeuristics.hpp\"\n+\n+ShenandoahYoungHeuristics::ShenandoahYoungHeuristics(ShenandoahYoungGeneration* generation)\n+        : ShenandoahGenerationalHeuristics(generation) {\n+  assert(!generation->is_old(), \"Young heuristics only accept the young generation\");\n+}\n+\n+\n+void ShenandoahYoungHeuristics::choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                                                      RegionData* data, size_t size,\n+                                                                      size_t actual_free) {\n+  \/\/ The logic for cset selection in adaptive is as follows:\n+  \/\/\n+  \/\/   1. We cannot get cset larger than available free space. Otherwise we guarantee OOME\n+  \/\/      during evacuation, and thus guarantee full GC. In practice, we also want to let\n+  \/\/      application to allocate something. This is why we limit CSet to some fraction of\n+  \/\/      available space. In non-overloaded heap, max_cset would contain all plausible candidates\n+  \/\/      over garbage threshold.\n+  \/\/\n+  \/\/   2. We should not get cset too low so that free threshold would not be met right\n+  \/\/      after the cycle. Otherwise we get back-to-back cycles for no reason if heap is\n+  \/\/      too fragmented. In non-overloaded non-fragmented heap min_garbage would be around zero.\n+  \/\/\n+  \/\/ Therefore, we start by sorting the regions by garbage. Then we unconditionally add the best candidates\n+  \/\/ before we meet min_garbage. Then we add all candidates that fit with a garbage threshold before\n+  \/\/ we hit max_cset. When max_cset is hit, we terminate the cset selection. Note that in this scheme,\n+  \/\/ ShenandoahGarbageThreshold is the soft threshold which would be ignored until min_garbage is hit.\n+\n+  \/\/ In generational mode, the sort order within the data array is not strictly descending amounts of garbage.  In\n+  \/\/ particular, regions that have reached tenure age will be sorted into this array before younger regions that contain\n+  \/\/ more garbage.  This represents one of the reasons why we keep looking at regions even after we decide, for example,\n+  \/\/ to exclude one of the regions because it might require evacuation of too much live data.\n+\n+  \/\/ Better select garbage-first regions\n+  QuickSort::sort<RegionData>(data, (int) size, compare_by_garbage, false);\n+\n+  size_t cur_young_garbage = add_preselected_regions_to_collection_set(cset, data, size);\n+\n+  choose_young_collection_set(cset, data, size, actual_free, cur_young_garbage);\n+\n+  log_cset_composition(cset);\n+}\n+\n+void ShenandoahYoungHeuristics::choose_young_collection_set(ShenandoahCollectionSet* cset,\n+                                                            const RegionData* data,\n+                                                            size_t size, size_t actual_free,\n+                                                            size_t cur_young_garbage) const {\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  size_t capacity = heap->young_generation()->max_capacity();\n+  size_t garbage_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahGarbageThreshold \/ 100;\n+  size_t ignore_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahIgnoreGarbageThreshold \/ 100;\n+\n+  \/\/ This is young-gen collection or a mixed evacuation.\n+  \/\/ If this is mixed evacuation, the old-gen candidate regions have already been added.\n+  size_t max_cset = (size_t) (heap->get_young_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t cur_cset = 0;\n+  size_t free_target = (capacity * ShenandoahMinFreeThreshold) \/ 100 + max_cset;\n+  size_t min_garbage = (free_target > actual_free) ? (free_target - actual_free) : 0;\n+\n+  log_info(gc, ergo)(\n+          \"Adaptive CSet Selection for YOUNG. Max Evacuation: \" SIZE_FORMAT \"%s, Actual Free: \" SIZE_FORMAT \"%s.\",\n+          byte_size_in_proper_unit(max_cset), proper_unit_for_byte_size(max_cset),\n+          byte_size_in_proper_unit(actual_free), proper_unit_for_byte_size(actual_free));\n+\n+  for (size_t idx = 0; idx < size; idx++) {\n+    ShenandoahHeapRegion* r = data[idx]._region;\n+    if (cset->is_preselected(r->index())) {\n+      continue;\n+    }\n+    if (r->age() < InitialTenuringThreshold) {\n+      size_t new_cset = cur_cset + r->get_live_data_bytes();\n+      size_t region_garbage = r->garbage();\n+      size_t new_garbage = cur_young_garbage + region_garbage;\n+      bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+      assert(r->is_young(), \"Only young candidates expected in the data array\");\n+      if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+        cur_cset = new_cset;\n+        cur_young_garbage = new_garbage;\n+        cset->add_region(r);\n+      }\n+    }\n+    \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n+    \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects or because\n+    \/\/ they are to be promoted in place.\n+  }\n+}\n+\n+\n+bool ShenandoahYoungHeuristics::should_start_gc() {\n+  \/\/ inherited triggers have already decided to start a cycle, so no further evaluation is required\n+  if (ShenandoahAdaptiveHeuristics::should_start_gc()) {\n+    return true;\n+  }\n+\n+  \/\/ Get through promotions and mixed evacuations as quickly as possible.  These cycles sometimes require significantly\n+  \/\/ more time than traditional young-generation cycles so start them up as soon as possible.  This is a \"mitigation\"\n+  \/\/ for the reality that old-gen and young-gen activities are not truly \"concurrent\".  If there is old-gen work to\n+  \/\/ be done, we start up the young-gen GC threads so they can do some of this old-gen work.  As implemented, promotion\n+  \/\/ gets priority over old-gen marking.\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  size_t promo_potential = heap->get_promotion_potential();\n+  if (promo_potential > 0) {\n+    \/\/ Detect unsigned arithmetic underflow\n+    assert(promo_potential < heap->capacity(), \"Sanity\");\n+    log_info(gc)(\"Trigger (%s): expedite promotion of \" SIZE_FORMAT \"%s\",\n+                 _heap_stats->name(),\n+                 byte_size_in_proper_unit(promo_potential),\n+                 proper_unit_for_byte_size(promo_potential));\n+    return true;\n+  }\n+\n+  size_t promo_in_place_potential = heap->get_promotion_in_place_potential();\n+  if (promo_in_place_potential > 0) {\n+    \/\/ Detect unsigned arithmetic underflow\n+    assert(promo_in_place_potential < heap->capacity(), \"Sanity\");\n+    log_info(gc)(\"Trigger (%s): expedite promotion in place of \" SIZE_FORMAT \"%s\",\n+                 _heap_stats->name(),\n+                 byte_size_in_proper_unit(promo_in_place_potential),\n+                 proper_unit_for_byte_size(promo_in_place_potential));\n+    return true;\n+  }\n+\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  size_t mixed_candidates = old_heuristics->unprocessed_old_collection_candidates();\n+  if (mixed_candidates > 0) {\n+    \/\/ We need to run young GC in order to open up some free heap regions so we can finish mixed evacuations.\n+    log_info(gc)(\"Trigger (%s): expedite mixed evacuation of \" SIZE_FORMAT \" regions\",\n+                 _heap_stats->name(), mixed_candidates);\n+    return true;\n+  }\n+\n+  return false;\n+}\n+\n+\/\/ Return a conservative estimate of how much memory can be allocated before we need to start GC. The estimate is based\n+\/\/ on memory that is currently available within young generation plus all of the memory that will be added to the young\n+\/\/ generation at the end of the current cycle (as represented by young_regions_to_be_reclaimed) and on the anticipated\n+\/\/ amount of time required to perform a GC.\n+size_t ShenandoahYoungHeuristics::bytes_of_allocation_runway_before_gc_trigger(size_t young_regions_to_be_reclaimed) {\n+  size_t capacity = _heap_stats->soft_max_capacity();\n+  size_t usage = _heap_stats->used();\n+  size_t available = (capacity > usage)? capacity - usage: 0;\n+  size_t allocated = _heap_stats->bytes_allocated_since_gc_start();\n+\n+  size_t available_young_collected = ShenandoahHeap::heap()->collection_set()->get_young_available_bytes_collected();\n+  size_t anticipated_available =\n+          available + young_regions_to_be_reclaimed * ShenandoahHeapRegion::region_size_bytes() - available_young_collected;\n+  size_t spike_headroom = capacity * ShenandoahAllocSpikeFactor \/ 100;\n+  size_t penalties      = capacity * _gc_time_penalties \/ 100;\n+\n+  double rate = _allocation_rate.sample(allocated);\n+\n+  \/\/ At what value of available, would avg and spike triggers occur?\n+  \/\/  if allocation_headroom < avg_cycle_time * avg_alloc_rate, then we experience avg trigger\n+  \/\/  if allocation_headroom < avg_cycle_time * rate, then we experience spike trigger if is_spiking\n+  \/\/\n+  \/\/ allocation_headroom =\n+  \/\/     0, if penalties > available or if penalties + spike_headroom > available\n+  \/\/     available - penalties - spike_headroom, otherwise\n+  \/\/\n+  \/\/ so we trigger if available - penalties - spike_headroom < avg_cycle_time * avg_alloc_rate, which is to say\n+  \/\/                  available < avg_cycle_time * avg_alloc_rate + penalties + spike_headroom\n+  \/\/            or if available < penalties + spike_headroom\n+  \/\/\n+  \/\/ since avg_cycle_time * avg_alloc_rate > 0, the first test is sufficient to test both conditions\n+  \/\/\n+  \/\/ thus, evac_slack_avg is MIN2(0,  available - avg_cycle_time * avg_alloc_rate + penalties + spike_headroom)\n+  \/\/\n+  \/\/ similarly, evac_slack_spiking is MIN2(0, available - avg_cycle_time * rate + penalties + spike_headroom)\n+  \/\/ but evac_slack_spiking is only relevant if is_spiking, as defined below.\n+\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+\n+  \/\/ TODO: Consider making conservative adjustments to avg_cycle_time, such as: (avg_cycle_time *= 2) in cases where\n+  \/\/ we expect a longer-than-normal GC duration.  This includes mixed evacuations, evacuation that perform promotion\n+  \/\/ including promotion in place, and OLD GC bootstrap cycles.  It has been observed that these cycles sometimes\n+  \/\/ require twice or more the duration of \"normal\" GC cycles.  We have experimented with this approach.  While it\n+  \/\/ does appear to reduce the frequency of degenerated cycles due to late triggers, it also has the effect of reducing\n+  \/\/ evacuation slack so that there is less memory available to be transferred to OLD.  The result is that we\n+  \/\/ throttle promotion and it takes too long to move old objects out of the young generation.\n+\n+  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+  size_t evac_slack_avg;\n+  if (anticipated_available > avg_cycle_time * avg_alloc_rate + penalties + spike_headroom) {\n+    evac_slack_avg = anticipated_available - (avg_cycle_time * avg_alloc_rate + penalties + spike_headroom);\n+  } else {\n+    \/\/ we have no slack because it's already time to trigger\n+    evac_slack_avg = 0;\n+  }\n+\n+  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+  size_t evac_slack_spiking;\n+  if (is_spiking) {\n+    if (anticipated_available > avg_cycle_time * rate + penalties + spike_headroom) {\n+      evac_slack_spiking = anticipated_available - (avg_cycle_time * rate + penalties + spike_headroom);\n+    } else {\n+      \/\/ we have no slack because it's already time to trigger\n+      evac_slack_spiking = 0;\n+    }\n+  } else {\n+    evac_slack_spiking = evac_slack_avg;\n+  }\n+\n+  size_t threshold = min_free_threshold();\n+  size_t evac_min_threshold = (anticipated_available > threshold)? anticipated_available - threshold: 0;\n+  return MIN3(evac_slack_spiking, evac_slack_avg, evac_min_threshold);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.cpp","additions":244,"deletions":0,"binary":false,"changes":244,"status":"added"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#ifndef SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHYOUNGHEURISTICS_HPP\n+#define SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHYOUNGHEURISTICS_HPP\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGenerationalHeuristics.hpp\"\n+\n+class ShenandoahYoungGeneration;\n+\n+\/*\n+ * This is a specialization of the generational heuristic which chooses\n+ * young regions for evacuation. This heuristic also has additional triggers\n+ * designed to expedite mixed collections and promotions.\n+ *\/\n+class ShenandoahYoungHeuristics : public ShenandoahGenerationalHeuristics {\n+public:\n+  explicit ShenandoahYoungHeuristics(ShenandoahYoungGeneration* generation);\n+\n+\n+  void choose_collection_set_from_regiondata(ShenandoahCollectionSet* cset,\n+                                             RegionData* data, size_t size,\n+                                             size_t actual_free) override;\n+\n+  bool should_start_gc() override;\n+\n+  size_t bytes_of_allocation_runway_before_gc_trigger(size_t young_regions_to_be_reclaimed);\n+\n+private:\n+  void choose_young_collection_set(ShenandoahCollectionSet* cset,\n+                                   const RegionData* data,\n+                                   size_t size, size_t actual_free,\n+                                   size_t cur_young_garbage) const;\n+\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_HEURISTICS_SHENANDOAHYOUNGHEURISTICS_HPP\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -26,1 +26,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n@@ -34,0 +34,5 @@\n+\n+#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined PPC64)\n+  vm_exit_during_initialization(\"Shenandoah Generational GC is not supported on this platform.\");\n+#endif\n+\n@@ -54,17 +59,0 @@\n-\n-ShenandoahHeuristics* ShenandoahGenerationalMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n-  if (ShenandoahGCHeuristics == nullptr) {\n-    vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option (null)\");\n-  }\n-\n-  if (strcmp(ShenandoahGCHeuristics, \"adaptive\") != 0) {\n-    vm_exit_during_initialization(\"Generational mode requires the (default) adaptive heuristic\");\n-  }\n-\n-#if !(defined AARCH64 || defined AMD64 || defined IA32 || defined PPC64)\n-  vm_exit_during_initialization(\"Shenandoah Generational GC is not supported on this platform.\");\n-#endif\n-\n-  return new ShenandoahAdaptiveHeuristics(generation);\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.cpp","additions":6,"deletions":18,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeapStats.hpp\"\n@@ -31,1 +32,0 @@\n-#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -34,1 +34,1 @@\n-ShenandoahHeuristics* ShenandoahMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n+ShenandoahHeuristics* ShenandoahMode::initialize_heuristics(ShenandoahHeapStats* heap_info) const {\n@@ -40,1 +40,1 @@\n-    return new ShenandoahAggressiveHeuristics(generation);\n+    return new ShenandoahAggressiveHeuristics();\n@@ -42,1 +42,1 @@\n-    return new ShenandoahStaticHeuristics(generation);\n+    return new ShenandoahStaticHeuristics();\n@@ -44,1 +44,1 @@\n-    return new ShenandoahAdaptiveHeuristics(generation);\n+    return new ShenandoahAdaptiveHeuristics(heap_info);\n@@ -46,1 +46,1 @@\n-    return new ShenandoahCompactHeuristics(generation);\n+    return new ShenandoahCompactHeuristics();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-class ShenandoahGeneration;\n+class ShenandoahHeapStats;\n@@ -35,1 +35,0 @@\n-class ShenandoahOldHeuristics;\n@@ -56,1 +55,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahHeapStats* heap_info) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeapStats.hpp\"\n@@ -58,1 +59,2 @@\n-ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n+\n+ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics(ShenandoahHeapStats* heap_info) const {\n@@ -62,1 +64,1 @@\n-  return new ShenandoahPassiveHeuristics(generation);\n+  return new ShenandoahPassiveHeuristics();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahHeapStats* heap_info) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -52,1 +52,0 @@\n-  size_t                _immediate_trash;\n@@ -104,3 +103,0 @@\n-  inline size_t get_immediate_trash();\n-  inline void set_immediate_trash(size_t immediate_trash);\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -57,8 +57,0 @@\n-void ShenandoahCollectionSet::set_immediate_trash(size_t immediate_trash) {\n-  _immediate_trash = immediate_trash;\n-}\n-\n-size_t ShenandoahCollectionSet::get_immediate_trash() {\n-  return _immediate_trash;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.inline.hpp","additions":0,"deletions":8,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -40,0 +40,2 @@\n+#include \"utilities\/quickSort.hpp\"\n+\n@@ -139,1 +141,1 @@\n-size_t ShenandoahGeneration::bytes_allocated_since_gc_start() {\n+size_t ShenandoahGeneration::bytes_allocated_since_gc_start() const {\n@@ -324,1 +326,1 @@\n-  consumed_by_advance_promotion = _heuristics->select_aged_regions(old_promo_reserve, num_regions, preselected_regions);\n+  consumed_by_advance_promotion = select_aged_regions(old_promo_reserve, num_regions, preselected_regions);\n@@ -445,0 +447,175 @@\n+typedef struct {\n+  ShenandoahHeapRegion* _region;\n+  size_t _live_data;\n+} AgedRegionData;\n+\n+static int compare_by_aged_live(AgedRegionData a, AgedRegionData b) {\n+  if (a._live_data < b._live_data)\n+    return -1;\n+  else if (a._live_data > b._live_data)\n+    return 1;\n+  else return 0;\n+}\n+\n+inline void assert_no_in_place_promotions() {\n+#ifdef ASSERT\n+  class ShenandoahNoInPlacePromotions : public ShenandoahHeapRegionClosure {\n+  public:\n+    void heap_region_do(ShenandoahHeapRegion *r) override {\n+      assert(r->get_top_before_promote() == nullptr,\n+             \"Region \" SIZE_FORMAT \" should not be ready for in-place promotion\", r->index());\n+    }\n+  } cl;\n+  ShenandoahHeap::heap()->heap_region_iterate(&cl);\n+#endif\n+}\n+\n+\/\/ Preselect for inclusion into the collection set regions whose age is at or above tenure age which contain more than\n+\/\/ ShenandoahOldGarbageThreshold amounts of garbage.  We identify these regions by setting the appropriate entry of\n+\/\/ candidate_regions_for_promotion_by_copy[] to true.  All entries are initialized to false before calling this\n+\/\/ function.\n+\/\/\n+\/\/ During the subsequent selection of the collection set, we give priority to these promotion set candidates.\n+\/\/ Without this prioritization, we found that the aged regions tend to be ignored because they typically have\n+\/\/ much less garbage and much more live data than the recently allocated \"eden\" regions.  When aged regions are\n+\/\/ repeatedly excluded from the collection set, the amount of live memory within the young generation tends to\n+\/\/ accumulate and this has the undesirable side effect of causing young-generation collections to require much more\n+\/\/ CPU and wall-clock time.\n+\/\/\n+\/\/ A second benefit of treating aged regions differently than other regions during collection set selection is\n+\/\/ that this allows us to more accurately budget memory to hold the results of evacuation.  Memory for evacuation\n+\/\/ of aged regions must be reserved in the old generations.  Memory for evacuation of all other regions must be\n+\/\/ reserved in the young generation.\n+size_t ShenandoahGeneration::select_aged_regions(size_t old_available, size_t num_regions,\n+                                                 bool candidate_regions_for_promotion_by_copy[]) {\n+\n+  \/\/ There should be no regions configured for subsequent in-place-promotions carried over from the previous cycle.\n+  assert_no_in_place_promotions();\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(heap->mode()->is_generational(), \"Only in generational mode\");\n+  ShenandoahMarkingContext* const ctx = heap->marking_context();\n+  size_t old_consumed = 0;\n+  size_t promo_potential = 0;\n+  size_t anticipated_promote_in_place_live = 0;\n+\n+  heap->clear_promotion_in_place_potential();\n+  heap->clear_promotion_potential();\n+  size_t candidates = 0;\n+  size_t candidates_live = 0;\n+  size_t old_garbage_threshold = (ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold) \/ 100;\n+  size_t promote_in_place_regions = 0;\n+  size_t promote_in_place_live = 0;\n+  size_t promote_in_place_pad = 0;\n+  size_t anticipated_candidates = 0;\n+  size_t anticipated_promote_in_place_regions = 0;\n+\n+  \/\/ Sort the promotion-eligible regions according to live-data-bytes so that we can first reclaim regions that require\n+  \/\/ less evacuation effort.  This prioritizes garbage first, expanding the allocation pool before we begin the work of\n+  \/\/ reclaiming regions that require more effort.\n+  AgedRegionData* sorted_regions = (AgedRegionData*) alloca(num_regions * sizeof(AgedRegionData));\n+  for (size_t i = 0; i < num_regions; i++) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    if (r->is_empty() || !r->has_live() || !r->is_young() || !r->is_regular()) {\n+      continue;\n+    }\n+    if (r->age() >= InitialTenuringThreshold) {\n+      if ((r->garbage() < old_garbage_threshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        HeapWord* original_top = r->top();\n+        if (tams == original_top) {\n+          \/\/ No allocations from this region have been made during concurrent mark. It meets all the criteria\n+          \/\/ for in-place-promotion. Though we only need the value of top when we fill the end of the region,\n+          \/\/ we use this field to indicate that this region should be promoted in place during the evacuation\n+          \/\/ phase.\n+          r->save_top_before_promote();\n+\n+          size_t remnant_size = r->free() \/ HeapWordSize;\n+          if (remnant_size > ShenandoahHeap::min_fill_size()) {\n+            ShenandoahHeap::fill_with_object(original_top, remnant_size);\n+            \/\/ Fill the remnant memory within this region to assure no allocations prior to promote in place.  Otherwise,\n+            \/\/ newly allocated objects will not be parseable when promote in place tries to register them.  Furthermore, any\n+            \/\/ new allocations would not necessarily be eligible for promotion.  This addresses both issues.\n+            r->set_top(r->end());\n+            promote_in_place_pad += remnant_size * HeapWordSize;\n+          } else {\n+            \/\/ Since the remnant is so small that it cannot be filled, we don't have to worry about any accidental\n+            \/\/ allocations occurring within this region before the region is promoted in place.\n+          }\n+          promote_in_place_regions++;\n+          promote_in_place_live += r->get_live_data_bytes();\n+        }\n+        \/\/ Else, we do not promote this region (either in place or by copy) because it has received new allocations.\n+\n+        \/\/ During evacuation, we exclude from promotion regions for which age > tenure threshold, garbage < garbage-threshold,\n+        \/\/  and get_top_before_promote() != tams\n+      } else {\n+        \/\/ After sorting and selecting best candidates below, we may decide to exclude this promotion-eligible region\n+        \/\/ from the current collection sets.  If this happens, we will consider this region as part of the anticipated\n+        \/\/ promotion potential for the next GC pass.\n+        size_t live_data = r->get_live_data_bytes();\n+        candidates_live += live_data;\n+        sorted_regions[candidates]._region = r;\n+        sorted_regions[candidates++]._live_data = live_data;\n+      }\n+    } else {\n+      \/\/ We only anticipate to promote regular regions if garbage() is above threshold.  Tenure-aged regions with less\n+      \/\/ garbage are promoted in place.  These take a different path to old-gen.  Note that certain regions that are\n+      \/\/ excluded from anticipated promotion because their garbage content is too low (causing us to anticipate that\n+      \/\/ the region would be promoted in place) may be eligible for evacuation promotion by the time promotion takes\n+      \/\/ place during a subsequent GC pass because more garbage is found within the region between now and then.  This\n+      \/\/ should not happen if we are properly adapting the tenure age.  The theory behind adaptive tenuring threshold\n+      \/\/ is to choose the youngest age that demonstrates no \"significant\" futher loss of population since the previous\n+      \/\/ age.  If not this, we expect the tenure age to demonstrate linear population decay for at least two population\n+      \/\/ samples, whereas we expect to observe exponetial population decay for ages younger than the tenure age.\n+      \/\/\n+      \/\/ In the case that certain regions which were anticipated to be promoted in place need to be promoted by\n+      \/\/ evacuation, it may be the case that there is not sufficient reserve within old-gen to hold evacuation of\n+      \/\/ these regions.  The likely outcome is that these regions will not be selected for evacuation or promotion\n+      \/\/ in the current cycle and we will anticipate that they will be promoted in the next cycle.  This will cause\n+      \/\/ us to reserve more old-gen memory so that these objects can be promoted in the subsequent cycle.\n+      \/\/\n+      \/\/ TODO:\n+      \/\/   If we are auto-tuning the tenure age and regions that were anticipated to be promoted in place end up\n+      \/\/   being promoted by evacuation, this event should feed into the tenure-age-selection heuristic so that\n+      \/\/   the tenure age can be increased.\n+      if (heap->is_aging_cycle() && (r->age() + 1 == InitialTenuringThreshold)) {\n+        if (r->garbage() >= old_garbage_threshold) {\n+          anticipated_candidates++;\n+          promo_potential += r->get_live_data_bytes();\n+        }\n+        else {\n+          anticipated_promote_in_place_regions++;\n+          anticipated_promote_in_place_live += r->get_live_data_bytes();\n+        }\n+      }\n+    }\n+    \/\/ Note that we keep going even if one region is excluded from selection.\n+    \/\/ Subsequent regions may be selected if they have smaller live data.\n+  }\n+  \/\/ Sort in increasing order according to live data bytes.  Note that candidates represents the number of regions\n+  \/\/ that qualify to be promoted by evacuation.\n+  if (candidates > 0) {\n+    QuickSort::sort<AgedRegionData>(sorted_regions, candidates, compare_by_aged_live, false);\n+    for (size_t i = 0; i < candidates; i++) {\n+      size_t region_live_data = sorted_regions[i]._live_data;\n+      size_t promotion_need = (size_t) (region_live_data * ShenandoahPromoEvacWaste);\n+      if (old_consumed + promotion_need <= old_available) {\n+        ShenandoahHeapRegion* region = sorted_regions[i]._region;\n+        old_consumed += promotion_need;\n+        candidate_regions_for_promotion_by_copy[region->index()] = true;\n+      } else {\n+        \/\/ We rejected this promotable region from the collection set because we had no room to hold its copy.\n+        \/\/ Add this region to promo potential for next GC.\n+        promo_potential += region_live_data;\n+      }\n+      \/\/ We keep going even if one region is excluded from selection because we need to accumulate all eligible\n+      \/\/ regions that are not preselected into promo_potential\n+    }\n+  }\n+  heap->set_pad_for_promote_in_place(promote_in_place_pad);\n+  heap->set_promotion_potential(promo_potential);\n+  heap->set_promotion_in_place_potential(anticipated_promote_in_place_live);\n+  return old_consumed;\n+}\n+\n@@ -490,1 +667,1 @@\n-      _heuristics->choose_collection_set(collection_set, heap->old_heuristics());\n+      _heuristics->choose_collection_set(collection_set);\n@@ -496,1 +673,1 @@\n-      _heuristics->choose_collection_set(collection_set, heap->old_heuristics());\n+      _heuristics->choose_collection_set(collection_set);\n@@ -702,3 +879,1 @@\n-  size_t in_use = used() + get_humongous_waste();\n-  size_t capacity = max_capacity();\n-  return in_use > capacity ? 0 : capacity - in_use;\n+  return available(max_capacity());\n@@ -708,0 +883,4 @@\n+  return available(soft_max_capacity());\n+}\n+\n+size_t ShenandoahGeneration::available(size_t capacity) const {\n@@ -709,2 +888,11 @@\n-  size_t soft_capacity = soft_max_capacity();\n-  return in_use > soft_capacity ? 0 : soft_capacity - in_use;\n+  size_t available = in_use > capacity ? 0 : capacity - in_use;\n+  \/\/ The collector reserve may eat into what the mutator is allowed to use. Make sure we are looking\n+  \/\/ at what is available to the mutator when deciding whether to start a GC.\n+  size_t usable = ShenandoahHeap::heap()->free_set()->available();\n+  if (usable < available) {\n+    log_debug(gc)(\"Usable (\" SIZE_FORMAT \"%s) is less than available (\" SIZE_FORMAT \"%s)\",\n+                  byte_size_in_proper_unit(usable), proper_unit_for_byte_size(usable),\n+                  byte_size_in_proper_unit(available), proper_unit_for_byte_size(available));\n+    available = usable;\n+  }\n+  return available;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":197,"deletions":9,"binary":false,"changes":206,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeapStats.hpp\"\n@@ -40,1 +41,1 @@\n-class ShenandoahGeneration : public CHeapObj<mtGC> {\n+class ShenandoahGeneration : public CHeapObj<mtGC>, public ShenandoahHeapStats {\n@@ -85,0 +86,17 @@\n+  \/\/ Preselect for inclusion into the collection set regions whose age is\n+  \/\/ at or above tenure age and which contain more than ShenandoahOldGarbageThreshold\n+  \/\/ amounts of garbage.\n+  \/\/\n+  \/\/ A side effect performed by this function is to tally up the number of regions and\n+  \/\/ the number of live bytes that we plan to promote-in-place during the current GC cycle.\n+  \/\/ This information, which is stored with an invocation of heap->set_promotion_in_place_potential(),\n+  \/\/ feeds into subsequent decisions about when to trigger the next GC and may identify\n+  \/\/ special work to be done during this GC cycle if we choose to abbreviate it.\n+  \/\/\n+  \/\/ Returns bytes of old-gen memory consumed by selected aged regions\n+  size_t select_aged_regions(size_t old_available,\n+                             size_t num_regions, bool\n+                             candidate_regions_for_promotion_by_copy[]);\n+\n+  size_t available(size_t capacity) const;\n+\n@@ -102,2 +120,0 @@\n-  virtual const char* name() const = 0;\n-\n@@ -106,2 +122,2 @@\n-  virtual size_t soft_max_capacity() const { return _soft_max_capacity; }\n-  virtual size_t max_capacity() const      { return _max_capacity; }\n+  size_t soft_max_capacity() const override { return _soft_max_capacity; }\n+  size_t max_capacity() const override      { return _max_capacity; }\n@@ -111,2 +127,2 @@\n-  virtual size_t used() const { return _used; }\n-  virtual size_t available() const;\n+  size_t used() const override { return _used; }\n+  size_t available() const override;\n@@ -118,1 +134,1 @@\n-  size_t soft_available() const;\n+  size_t soft_available() const override;\n@@ -120,1 +136,1 @@\n-  size_t bytes_allocated_since_gc_start();\n+  size_t bytes_allocated_since_gc_start() const override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":25,"deletions":9,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahGlobalHeuristics.hpp\"\n@@ -32,1 +33,1 @@\n-#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+\n@@ -89,0 +90,12 @@\n+\n+ShenandoahHeuristics* ShenandoahGlobalGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  if (gc_mode->is_generational()) {\n+    _heuristics = new ShenandoahGlobalHeuristics(this);\n+  } else {\n+    _heuristics = gc_mode->initialize_heuristics(this);\n+  }\n+\n+  _heuristics->set_guaranteed_gc_interval(ShenandoahGuaranteedGCInterval);\n+  confirm_heuristics_mode();\n+  return _heuristics;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.cpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -61,0 +61,2 @@\n+\n+  ShenandoahHeuristics* initialize_heuristics(ShenandoahMode* gc_mode) override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -39,0 +39,2 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n@@ -86,2 +88,0 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n-\n@@ -717,0 +717,4 @@\n+ShenandoahYoungHeuristics* ShenandoahHeap::young_heuristics() {\n+  return (ShenandoahYoungHeuristics*) _young_generation->heuristics();\n+}\n+\n@@ -913,4 +917,0 @@\n-void ShenandoahHeap::handle_promotion_failure() {\n-  old_heuristics()->handle_promotion_failure();\n-}\n-\n@@ -2662,0 +2662,5 @@\n+size_t ShenandoahHeap::bytes_allocated_since_gc_start() {\n+  assert(!mode()->is_generational(), \"This is used for heuristics that are not compatible with generational mode\");\n+  return global_generation()->bytes_allocated_since_gc_start();\n+}\n+\n@@ -3111,1 +3116,1 @@\n-    size_t allocation_runway = young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n+    size_t allocation_runway = young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(young_cset_regions);\n@@ -3135,1 +3140,1 @@\n-      ((ShenandoahOldHeuristics *) old_generation()->heuristics())->trigger_old_is_fragmented();\n+      old_heuristics()->trigger_old_is_fragmented();\n@@ -3144,1 +3149,1 @@\n-      ((ShenandoahOldHeuristics *) old_generation()->heuristics())->trigger_old_has_grown();\n+      old_heuristics()->trigger_old_has_grown();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":14,"deletions":9,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+class ShenandoahYoungHeuristics;\n@@ -178,0 +179,1 @@\n+  ShenandoahYoungHeuristics* young_heuristics();\n@@ -245,0 +247,1 @@\n+  size_t bytes_allocated_since_gc_start();\n@@ -817,1 +820,0 @@\n-  void handle_promotion_failure();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -442,1 +442,0 @@\n-        handle_promotion_failure();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n@@ -152,1 +153,1 @@\n-  size_t allocation_runway = heap->young_generation()->heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n+  size_t allocation_runway = heap->young_heuristics()->bytes_of_allocation_runway_before_gc_trigger(0);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -334,1 +334,1 @@\n-    heuristics()->choose_collection_set(nullptr, nullptr);\n+    _old_heuristics->prepare_for_old_collections();\n@@ -465,15 +465,2 @@\n-  assert(ShenandoahOldGCHeuristics != nullptr, \"ShenandoahOldGCHeuristics should not be unset\");\n-  ShenandoahHeuristics* trigger;\n-  if (strcmp(ShenandoahOldGCHeuristics, \"static\") == 0) {\n-    trigger = new ShenandoahStaticHeuristics(this);\n-  } else if (strcmp(ShenandoahOldGCHeuristics, \"adaptive\") == 0) {\n-    trigger = new ShenandoahAdaptiveHeuristics(this);\n-  } else if (strcmp(ShenandoahOldGCHeuristics, \"compact\") == 0) {\n-    trigger = new ShenandoahCompactHeuristics(this);\n-  } else {\n-    vm_exit_during_initialization(\"Unknown -XX:ShenandoahOldGCHeuristics option (must be one of: static, adaptive, compact)\");\n-    ShouldNotReachHere();\n-    return nullptr;\n-  }\n-  trigger->set_guaranteed_gc_interval(ShenandoahGuaranteedOldGCInterval);\n-  _old_heuristics = new ShenandoahOldHeuristics(this, trigger);\n+  _old_heuristics = new ShenandoahOldHeuristics(this);\n+  _old_heuristics->set_guaranteed_gc_interval(ShenandoahGuaranteedOldGCInterval);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":3,"deletions":16,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -181,2 +181,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n@@ -191,0 +189,1 @@\n+class ShenandoahHeapRegion;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahYoungHeuristics.hpp\"\n@@ -84,1 +84,1 @@\n-  _heuristics = gc_mode->initialize_heuristics(this);\n+  _heuristics = new ShenandoahYoungHeuristics(this);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -81,6 +81,0 @@\n-  product(ccstr, ShenandoahOldGCHeuristics, \"adaptive\", EXPERIMENTAL,       \\\n-          \"Similar to ShenandoahGCHeuristics, but applied to the old \"      \\\n-          \"generation. This configuration is only used to trigger old \"     \\\n-          \"collections and does not change how regions are selected \"       \\\n-          \"for collection.\")                                                \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"}]}