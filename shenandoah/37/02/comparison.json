{"files":[{"patch":"@@ -75,1 +75,3 @@\n-void ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) {\n+\/\/ Returns true iff the chosen collection set includes old-gen regions\n+bool ShenandoahHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) {\n+  bool result = false;\n@@ -165,1 +167,3 @@\n-      old_heuristics->prime_collection_set(collection_set);\n+      if (old_heuristics->prime_collection_set(collection_set)) {\n+        result = true;\n+      }\n@@ -194,0 +198,1 @@\n+  return result;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -143,1 +143,2 @@\n-  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics);\n+  \/\/ Return true iff the chosen collection set includes at least one old-gen region.\n+  virtual bool choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-void ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n+bool ShenandoahOldHeuristics::prime_collection_set(ShenandoahCollectionSet* collection_set) {\n@@ -144,0 +144,1 @@\n+  return (included_old_regions > 0);\n@@ -146,4 +147,2 @@\n-\n-void ShenandoahOldHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) {\n-  assert(collection_set->count() == 0, \"Must be empty\");\n-\n+\/\/ Both arguments are don't cares for old-gen collections\n+bool ShenandoahOldHeuristics::choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics) {\n@@ -153,0 +152,1 @@\n+  return false;\n@@ -215,1 +215,1 @@\n-      log_info(gc)(\"Old-gen mark evac (%llu RR), %llu CF)\",\n+      log_info(gc)(\"Old-gen mark evac (%llu RR, %llu CF)\",\n@@ -230,1 +230,1 @@\n-  log_info(gc)(\"Old-gen mark evac (%llu RR), %llu CF)\",\n+  log_info(gc)(\"Old-gen mark evac (%llu RR, %llu CF)\",\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -72,1 +72,2 @@\n-  virtual void choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics);\n+  \/\/ Return true iff chosen collection set includes at least one old-gen HeapRegion.\n+  virtual bool choose_collection_set(ShenandoahCollectionSet* collection_set, ShenandoahOldHeuristics* old_heuristics);\n@@ -74,1 +75,2 @@\n-  void prime_collection_set(ShenandoahCollectionSet* set);\n+  \/\/ Return true iff the collection set is primed with at least one old-gen region.\n+  bool prime_collection_set(ShenandoahCollectionSet* set);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -161,1 +161,1 @@\n-  card_table()->dirty_MemRegion(MemRegion(aligned_start, aligned_end));\n+  _heap->card_scan()->mark_range_as_dirty(aligned_start, (aligned_end - aligned_start));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n@@ -26,1 +26,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -28,0 +27,2 @@\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n@@ -31,0 +32,32 @@\n+  _write_byte_map = _byte_map;\n+  _write_byte_map_base = _byte_map_base;\n+  const size_t rs_align = _page_size == (size_t) os::vm_page_size() ? 0 :\n+    MAX2(_page_size, (size_t) os::vm_allocation_granularity());\n+\n+  ReservedSpace heap_rs(_byte_map_size, rs_align, false);\n+  if (!heap_rs.is_reserved()) {\n+    vm_exit_during_initialization(\"Could not reserve enough space for second copy of card marking array\");\n+  }\n+  os::commit_memory_or_exit(heap_rs.base(), _byte_map_size, rs_align, false, \"Cannot commit memory for second copy of card table\");\n+\n+  HeapWord* low_bound  = _whole_heap.start();\n+  _read_byte_map = (CardValue*) heap_rs.base();\n+  _read_byte_map_base = _read_byte_map - (uintptr_t(low_bound) >> card_shift);\n+\n+  log_trace(gc, barrier)(\"ShenandoahCardTable::ShenandoahCardTable: \");\n+  log_trace(gc, barrier)(\"    &_read_byte_map[0]: \" INTPTR_FORMAT \"  &_read_byte_map[_last_valid_index]: \" INTPTR_FORMAT,\n+                  p2i(&_read_byte_map[0]), p2i(&_read_byte_map[_last_valid_index]));\n+  log_trace(gc, barrier)(\"    _read_byte_map_base: \" INTPTR_FORMAT, p2i(_read_byte_map_base));\n+\n+  \/\/ TODO: As currently implemented, we do not swap pointers between _read_byte_map and _write_byte_map\n+  \/\/ because the mutator write barrier hard codes the address of the _write_byte_map_base.  Instead,\n+  \/\/ the current implementation simply copies contents of _write_byte_map onto _read_byte_map and cleans\n+  \/\/ the entirety of _write_byte_map at the init_mark safepoint.\n+  \/\/\n+  \/\/ If we choose to modify the mutator write barrier so that we can swap _read_byte_map_base and\n+  \/\/ _write_byte_map_base pointers, we may also have to figure out certain details about how the\n+  \/\/ _guard_region is implemented so that we can replicate the read and write versions of this region.\n+  \/\/\n+  \/\/ Alternatively, we may switch to a SATB-based write barrier and replace the direct card-marking\n+  \/\/ remembered set with something entirely different.\n+\n@@ -51,0 +84,32 @@\n+\n+\/\/ TODO: This service is not currently used because we are not able to swap _read_byte_map_base and\n+\/\/ _write_byte_map_base pointers.  If we were able to do so, we would invoke clear_read_table \"immediately\"\n+\/\/ following the end of concurrent remembered set scanning so that this read card table would be ready\n+\/\/ to serve as the new write card table at the time these pointer values were next swapped.\n+\/\/\n+\/\/ In the current implementation, the write-table is cleared immediately after its contents is copied to\n+\/\/ the read table, obviating the need for this service.\n+void ShenandoahCardTable::clear_read_table() {\n+  for (size_t i = 0; i < _byte_map_size; i++) {\n+    _read_byte_map[i] = clean_card;\n+  }\n+}\n+\n+\/\/ TODO: This service is not currently used because the mutator write barrier implementation hard codes the\n+\/\/ location of the _write_byte_may_base.  If we change the mutator's write barrier implementation, then we\n+\/\/ may use this service to exchange the roles of the read-card-table and write-card-table.\n+void ShenandoahCardTable::swap_card_tables() {\n+  shenandoah_assert_safepoint();\n+\n+  CardValue* save_value = _read_byte_map;\n+  _read_byte_map = _write_byte_map;\n+  _write_byte_map = save_value;\n+\n+  save_value = _read_byte_map_base;\n+  _read_byte_map_base = _write_byte_map_base;\n+  _write_byte_map_base = save_value;\n+\n+  \/\/ update the superclass instance variables\n+  _byte_map = _write_byte_map;\n+  _byte_map_base = _write_byte_map_base;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":67,"deletions":2,"binary":false,"changes":69,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n@@ -36,0 +36,22 @@\n+protected:\n+  \/\/ We maintain two copies of the card table to facilitate concurrent remembered set scanning\n+  \/\/ and concurrent clearing of stale remembered set information.  During the init_mark safepoint,\n+  \/\/ we copy the contents of _write_byte_map to _read_byte_map and clear _write_byte_map.\n+  \/\/\n+  \/\/ Concurrent remembered set scanning reads from _read_byte_map while concurrent mutator write\n+  \/\/ barriers are overwriting cards of the _write_byte_map with DIRTY codes.  Concurrent remembered\n+  \/\/ set scanning also overwrites cards of the _write_byte_map with DIRTY codes whenever it discovers\n+  \/\/ interesting pointers.\n+  \/\/\n+  \/\/ During a concurrent update-references phase, we scan the _write_byte_map concurrently to find\n+  \/\/ all old-gen references that may need to be updated.\n+  \/\/\n+  \/\/ In a future implementation, we may swap the values of _read_byte_map and _write_byte_map during\n+  \/\/ the init-mark safepoint to avoid the need for bulk STW copying and initialization.  Doing so\n+  \/\/ requires a change to the implementation of mutator write barriers as the address of the card\n+  \/\/ table is currently in-lined and hard-coded.\n+  CardValue* _read_byte_map;\n+  CardValue* _write_byte_map;\n+  CardValue* _read_byte_map_base;\n+  CardValue* _write_byte_map_base;\n+\n@@ -37,1 +59,1 @@\n-  ShenandoahCardTable(MemRegion whole_heap): CardTable(whole_heap) { }\n+  ShenandoahCardTable(MemRegion whole_heap) : CardTable(whole_heap) { }\n@@ -46,0 +68,21 @@\n+\n+  void clear_read_table();\n+\n+  \/\/ Exchange the roles of the read and write card tables.\n+  void swap_card_tables();\n+\n+  CardValue* read_byte_map() {\n+    return _read_byte_map;\n+  }\n+\n+  CardValue* write_byte_map() {\n+    return _write_byte_map;\n+  }\n+\n+  CardValue* read_byte_map_base() {\n+    return _read_byte_map_base;\n+  }\n+\n+  CardValue* write_byte_map_base() {\n+    return _write_byte_map_base;\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.hpp","additions":45,"deletions":2,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+  _mixed_evac (false),\n@@ -69,1 +70,6 @@\n-    \/\/ Concurrent mark roots\n+  \/\/ Concurrent remembered set scanning\n+  if (_generation->generation_mode() == YOUNG) {\n+    _generation->scan_remembered_set();\n+  }\n+\n+  \/\/ Concurrent mark roots\n@@ -151,1 +157,1 @@\n-  VM_ShenandoahInitMark op(this);\n+  VM_ShenandoahInitMark op(this, _do_old_gc_bootstrap);\n@@ -195,0 +201,6 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational() && (_generation->generation_mode() == YOUNG)) {\n+    \/\/ The current implementation of swap_remembered_set() copies the write-card-table\n+    \/\/ to the read-card-table.\n+    _generation->swap_remembered_set();\n+  }\n+\n@@ -438,6 +450,1 @@\n-\n-  if (_do_old_gc_bootstrap) {\n-    heap->global_generation()->prepare_gc();\n-  } else {\n-    _generation->prepare_gc();\n-  }\n+  _generation->prepare_gc(_do_old_gc_bootstrap);\n@@ -456,1 +463,2 @@\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n+      \/\/ reset, so it is very likely we don't need to do another write here.  Since most regions\n+      \/\/ are not \"active\", this path is relatively rare.\n@@ -493,0 +501,1 @@\n+    heap->old_generation()->parallel_heap_region_iterate(&cl);\n@@ -508,4 +517,0 @@\n-  if (_generation->generation_mode() == YOUNG) {\n-    _generation->scan_remembered_set();\n-  }\n-\n@@ -549,1 +554,2 @@\n-    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    bool mixed_evac = _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    heap->set_mixed_evac(mixed_evac);\n@@ -912,1 +918,3 @@\n-\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_before_updaterefs();\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":23,"deletions":15,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -46,1 +46,1 @@\n-private:\n+protected:\n@@ -48,0 +48,2 @@\n+\n+private:\n@@ -49,0 +51,1 @@\n+  bool                        _mixed_evac; \/\/ true iff most recent evacuation includes old-gen HeapRegions\n@@ -55,1 +58,1 @@\n-  ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap = false);\n+  ShenandoahConcurrentGC(ShenandoahGeneration* generation, bool do_old_gc_bootstrap);\n@@ -92,0 +95,1 @@\n+  virtual void op_final_mark();\n@@ -104,1 +108,0 @@\n-  void op_final_mark();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -420,1 +420,1 @@\n-      \/\/ that are in the cset (more of an issue for Milestone-8 to worry about).\n+      \/\/ that are in the cset.\n@@ -458,1 +458,0 @@\n-\n@@ -460,1 +459,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -225,1 +225,1 @@\n-  _generation->prepare_gc();\n+  _generation->prepare_gc(false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -170,2 +170,5 @@\n-    \/\/ This free region might have garbage in its remembered set representation.\n-    _heap->clear_cards_for(r);\n+    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+      \/\/ This free region might have garbage in its remembered set representation.\n+      _heap->clear_cards_for(r);\n+    }\n@@ -173,0 +176,11 @@\n+    r->set_update_watermark(r->bottom());\n+    ctx->capture_top_at_mark_start(r);\n+\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+\n+    \/\/ Leave top_bitmap alone.  The first time a heap region is put into service, top_bitmap should equal end.\n+    \/\/ Thereafter, it should represent the upper bound on parts of the bitmap that need to be cleared.\n+    log_debug(gc)(\"NOT clearing bitmap for region \" SIZE_FORMAT \", top_bitmap: \"\n+                  PTR_FORMAT \" at transition from FREE to %s\",\n+                  r->index(), p2i(ctx->top_bitmap(r)), affiliation_name(req.affiliation()));\n@@ -316,0 +330,1 @@\n+  ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n@@ -339,1 +354,0 @@\n-    r->set_top(r->bottom() + used_words);\n@@ -341,0 +355,13 @@\n+    r->set_update_watermark(r->bottom());\n+    r->set_top(r->bottom());    \/\/ Set top to bottom so we can capture TAMS\n+    ctx->capture_top_at_mark_start(r);\n+    r->set_top(r->bottom() + used_words); \/\/ Then change top to reflect allocation of humongous object.\n+    assert(ctx->top_at_mark_start(r) == r->bottom(), \"Newly established allocation region starts with TAMS equal to bottom\");\n+    assert(ctx->is_bitmap_clear_range(ctx->top_bitmap(r), r->end()), \"Bitmap above top_bitmap() must be clear\");\n+\n+    \/\/ Leave top_bitmap alone.  The first time a heap region is put into service, top_bitmap should equal end.\n+    \/\/ Thereafter, it should represent the upper bound on parts of the bitmap that need to be cleared.\n+    \/\/ ctx->clear_bitmap(r);\n+    log_debug(gc)(\"NOT clearing bitmap for Humongous region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmap: \"\n+                  PTR_FORMAT \" at transition from FREE to %s\",\n+                  p2i(r->bottom()), p2i(r->end()), p2i(ctx->top_bitmap(r)), affiliation_name(req.affiliation()));\n@@ -451,0 +478,1 @@\n+  log_debug(gc)(\"Rebuilding FreeSet\");\n@@ -464,0 +492,2 @@\n+\n+      log_debug(gc)(\"  Setting _mutator_free_bitmap bit for \" SIZE_FORMAT, idx);\n@@ -481,0 +511,1 @@\n+      log_debug(gc)(\"  Shifting region \" SIZE_FORMAT \" from mutator_free to collector_free\", idx);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":34,"deletions":3,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -53,0 +53,7 @@\n+\n+  \/\/ While holding the heap lock, allocate memory for a single object which is to be entirely contained\n+  \/\/ within a single HeapRegion as characterized by req.  The req.size() value is known to be less than or\n+  \/\/ equal to ShenandoahHeapRegion::humongous_threshold_words().  The caller of allocate_single is responsible\n+  \/\/ for registering the resulting object and setting the remembered set card values as appropriate.  The\n+  \/\/ most common case is that we are allocating a PLAB in which case object registering and card dirtying\n+  \/\/ is managed after the PLAB is divided into individual objects.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -123,0 +123,3 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    fatal(\"Full GC not yet supported for generational mode in do_it().\");\n+  }\n@@ -199,0 +202,1 @@\n+    \/\/ TODO: Do we need to explicitly retire PLABs?\n@@ -278,2 +282,4 @@\n-    _ctx->capture_top_at_mark_start(r);\n-    r->clear_live_data();\n+    if (r->affiliation() != FREE) {\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n@@ -470,0 +476,1 @@\n+  log_debug(gc)(\"Full GC calculating target humongous objects from end \" SIZE_FORMAT, to_end);\n@@ -536,17 +543,22 @@\n-    if (r->is_humongous_start()) {\n-      oop humongous_obj = oop(r->bottom());\n-      if (!_ctx->is_marked(humongous_obj)) {\n-        assert(!r->has_live(),\n-               \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n-        _heap->trash_humongous_region_at(r);\n-      } else {\n-        assert(r->has_live(),\n-               \"Region \" SIZE_FORMAT \" should have live\", r->index());\n-      }\n-    } else if (r->is_humongous_continuation()) {\n-      \/\/ If we hit continuation, the non-live humongous starts should have been trashed already\n-      assert(r->humongous_start_region()->has_live(),\n-             \"Region \" SIZE_FORMAT \" should have live\", r->index());\n-    } else if (r->is_regular()) {\n-      if (!r->has_live()) {\n-        r->make_trash_immediate();\n+    if (r->affiliation() != FREE) {\n+      if (r->is_humongous_start()) {\n+        oop humongous_obj = oop(r->bottom());\n+        if (!_ctx->is_marked(humongous_obj)) {\n+          assert(!r->has_live(),\n+                 \"Humongous Start %s Region \" SIZE_FORMAT \" is not marked, should not have live\",\n+                 affiliation_name(r->affiliation()),  r->index());\n+          log_debug(gc)(\"Trashing immediate humongous region \" SIZE_FORMAT \" because not marked\", r->index());\n+          _heap->trash_humongous_region_at(r);\n+        } else {\n+          assert(r->has_live(),\n+                 \"Humongous Start %s Region \" SIZE_FORMAT \" should have live\", affiliation_name(r->affiliation()),  r->index());\n+        }\n+      } else if (r->is_humongous_continuation()) {\n+        \/\/ If we hit continuation, the non-live humongous starts should have been trashed already\n+        assert(r->humongous_start_region()->has_live(),\n+               \"Humongous Continuation %s Region \" SIZE_FORMAT \" should have live\", affiliation_name(r->affiliation()),  r->index());\n+      } else if (r->is_regular()) {\n+        if (!r->has_live()) {\n+          log_debug(gc)(\"Trashing immediate regular region \" SIZE_FORMAT \" because has no live\", r->index());\n+          r->make_trash_immediate();\n+        }\n@@ -555,0 +567,2 @@\n+    \/\/ else, ignore this FREE region.\n+    \/\/ TODO: change iterators so they do not process FREE regions.\n@@ -974,0 +988,3 @@\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT,\n+                    old_start, new_start);\n+\n@@ -982,0 +999,1 @@\n+        ShenandoahRegionAffiliation original_affiliation = r->affiliation();\n@@ -991,1 +1009,1 @@\n-            r->make_humongous_start_bypass();\n+            r->make_humongous_start_bypass(original_affiliation);\n@@ -993,1 +1011,1 @@\n-            r->make_humongous_cont_bypass();\n+            r->make_humongous_cont_bypass(original_affiliation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":39,"deletions":21,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -67,1 +67,19 @@\n-        _ctx->clear_bitmap(region);\n+      _ctx->clear_bitmap(region);\n+    }\n+  }\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n+class ShenandoahSquirrelAwayCardTable: public ShenandoahHeapRegionClosure {\n+ private:\n+  ShenandoahHeap* _heap;\n+  RememberedScanner* _scanner;\n+ public:\n+  ShenandoahSquirrelAwayCardTable() :\n+    _heap(ShenandoahHeap::heap()),\n+    _scanner(_heap->card_scan()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* region) {\n+    if (region->is_old()) {\n+      _scanner->reset_remset(region->bottom(), ShenandoahHeapRegion::region_size_words());\n@@ -138,1 +156,15 @@\n-void ShenandoahGeneration::prepare_gc() {\n+\/\/ The ideal is to swap the remembered set so the safepoint effort is no more than a few pointer manipulations.\n+\/\/ However, limitations in the implementation of the mutator write-barrier make it difficult to simply change the\n+\/\/ location of the card table.  So the interim implementation of swap_remembered_set will copy the write-table\n+\/\/ onto the read-table and will then clear the write-table.\n+void ShenandoahGeneration::swap_remembered_set() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+  shenandoah_assert_safepoint();\n+\n+  \/\/ TODO: Eventually, we want replace this with a constant-time exchange of pointers.\n+  ShenandoahSquirrelAwayCardTable task;\n+  heap->old_generation()->parallel_heap_region_iterate(&task);\n+}\n+\n+void ShenandoahGeneration::prepare_gc(bool do_old_gc_bootstrap) {\n@@ -141,0 +173,4 @@\n+  if (do_old_gc_bootstrap) {\n+    \/\/ Reset mark bitmap for old regions also.  Note that do_old_gc_bootstrap is only true if this generation is YOUNG.\n+    ShenandoahHeap::heap()->old_generation()->reset_mark_bitmap();\n+  }\n@@ -145,0 +181,4 @@\n+  if (do_old_gc_bootstrap) {\n+    \/\/ Capture top at mark start for both old-gen regions also.  Note that do_old_gc_bootstrap is only true if generation is YOUNG.\n+    ShenandoahHeap::heap()->old_generation()->parallel_heap_region_iterate(&cl);\n+  }\n@@ -147,1 +187,3 @@\n-void ShenandoahGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+\/\/ Returns true iff the chosen collection set includes a mix of young-gen and old-gen regions.\n+bool ShenandoahGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+  bool result;\n@@ -150,0 +192,1 @@\n+  assert(generation_mode() != OLD, \"Only YOUNG and GLOBAL GC perform evacuations\");\n@@ -154,1 +197,0 @@\n-    parallel_heap_region_iterate(&cl);\n@@ -156,0 +198,1 @@\n+    parallel_heap_region_iterate(&cl);\n@@ -157,0 +200,4 @@\n+\n+    \/\/ Also capture update_watermark for old-gen regions.\n+    ShenandoahCaptureUpdateWaterMarkForOld old_cl(complete_marking_context());\n+    heap->old_generation()->parallel_heap_region_iterate(&old_cl);\n@@ -164,1 +211,1 @@\n-    _heuristics->choose_collection_set(heap->collection_set(), heap->old_heuristics());\n+    result = _heuristics->choose_collection_set(heap->collection_set(), heap->old_heuristics());\n@@ -173,0 +220,1 @@\n+  return result;\n@@ -181,2 +229,3 @@\n-    if (contains(r)) {\n-      if (heap->is_bitmap_slice_committed(r) && !context->is_bitmap_clear_range(r->bottom(), r->end())) {\n+    if (contains(r) && (r->affiliation() != FREE)) {\n+      if (heap->is_bitmap_slice_committed(r) && (context->top_at_mark_start(r) > r->bottom()) &&\n+          !context->is_bitmap_clear_range(r->bottom(), r->end())) {\n@@ -249,1 +298,0 @@\n-  shenandoah_assert_safepoint();\n@@ -256,1 +304,1 @@\n-  ShenandoahGCPhase phase(ShenandoahPhaseTimings::init_scan_rset);\n+  ShenandoahConcurrentPhase gc_phase(\"Concurrent remembered set scanning\", ShenandoahPhaseTimings::init_scan_rset);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":57,"deletions":9,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -84,0 +84,3 @@\n+  \/\/ Used by concurrent and degenerated GC to reset remembered set.\n+  void swap_remembered_set();\n+\n@@ -85,2 +88,4 @@\n-  virtual void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n+  virtual void prepare_gc(bool do_old_gc_bootstrap);\n+\n+  \/\/ Return true iff prepared collection set includes at least one old-gen HeapRegion.\n+  bool prepare_regions_and_collection_set(bool concurrent);\n@@ -118,0 +123,1 @@\n+  \/\/ Scan remembered set at start of concurrent young-gen marking. *\/\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -90,2 +90,2 @@\n-void ShenandoahGlobalGeneration::prepare_gc() {\n-  ShenandoahGeneration::prepare_gc();\n+void ShenandoahGlobalGeneration::prepare_gc(bool do_old_gc_bootstrap) {\n+  ShenandoahGeneration::prepare_gc(do_old_gc_bootstrap);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,1 +45,1 @@\n-  virtual void prepare_gc();\n+  virtual void prepare_gc(bool do_old_gc_bootstrap);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -82,0 +82,2 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp\"\n+\n@@ -224,1 +226,2 @@\n-    size_t card_count = ShenandoahBarrierSet::barrier_set()->card_table()->cards_required(heap_rs.size() \/ HeapWordSize) - 1;\n+    ShenandoahCardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n+    size_t card_count = card_table->cards_required(heap_rs.size() \/ HeapWordSize) - 1;\n@@ -482,1 +485,0 @@\n-  \/\/ ojo: want to instantiate a ShenandoahOldHeuristics object here\n@@ -497,0 +499,1 @@\n+  _mixed_evac(false),\n@@ -624,0 +627,4 @@\n+bool ShenandoahHeap::doing_mixed_evacuations() {\n+  return (_old_heuristics->unprocessed_old_collection_candidates() > 0);\n+}\n+\n@@ -893,1 +900,1 @@\n-    card_scan()->register_object(top);\n+    card_scan()->register_object_wo_lock(top);\n@@ -1020,0 +1027,20 @@\n+  if (result != NULL && req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+    \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+    \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+    \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+    \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+    \/\/\n+    \/\/ objects being \"concurrently\" allocated:\n+    \/\/    [-----a------][-----b-----][--------------c------------------]\n+    \/\/            [---- card table memory range --------------]\n+    \/\/\n+    \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+    \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+    \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+    \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+    \/\/\n+    \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+    \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+    \/\/ invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+    ShenandoahHeap::heap()->card_scan()->register_object(result);\n+  }\n@@ -2108,0 +2135,2 @@\n+  bool _mixed_evac;             \/\/ true iff most recent evacuation includes old-gen HeapRegions\n+\n@@ -2109,1 +2138,1 @@\n-  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions) :\n+  ShenandoahUpdateHeapRefsTask(ShenandoahRegionIterator* regions, bool mixed_evac) :\n@@ -2112,1 +2141,3 @@\n-    _regions(regions) {\n+    _regions(regions),\n+    _mixed_evac(mixed_evac)\n+  {\n@@ -2119,1 +2150,1 @@\n-      do_work<ShenandoahConcUpdateRefsClosure>();\n+      do_work<ShenandoahConcUpdateRefsClosure>(worker_id);\n@@ -2122,1 +2153,1 @@\n-      do_work<ShenandoahSTWUpdateRefsClosure>();\n+      do_work<ShenandoahSTWUpdateRefsClosure>(worker_id);\n@@ -2128,1 +2159,1 @@\n-  void do_work() {\n+  void do_work(uint worker_id) {\n@@ -2131,0 +2162,1 @@\n+\n@@ -2138,1 +2170,0 @@\n-\n@@ -2140,1 +2171,1 @@\n-        if (!_heap->mode()->is_generational() || r->affiliation() == YOUNG_GENERATION) {\n+        if (!_heap->mode()->is_generational() || (r->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION)) {\n@@ -2142,6 +2173,8 @@\n-        } else if (r->affiliation() == OLD_GENERATION) {\n-          if (!_heap->is_gc_generation_young() || is_mixed) {\n-            \/\/ Old region in global or mixed cycle (in which case, old regions should be marked).\n-            \/\/ We need to make sure that the next remembered set scan does not iterate over dead objects\n-            \/\/ which haven't had their references updated.\n-            r->oop_iterate(&cl, \/*fill_dead_objects*\/ true, \/* reregister_coalesced_objects *\/ true);\n+        } else if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          if (_heap->active_generation()->generation_mode() == GLOBAL) {\n+            \/\/ This code is only relevant to GLOBAL GC.  With OLD GC, all coalescing and filling is done before any relevant\n+            \/\/ evacuations.\n+\n+            \/\/ This is an old region in a global cycle.  Make sure that the next cycle does not iterate over dead objects\n+            \/\/ which haven't had their references updated.  This is not a promotion.\n+            r->global_oop_iterate_and_fill_dead(&cl);\n@@ -2149,5 +2182,48 @@\n-            \/\/ Old region in a young cycle with no old regions.\n-            if (!ShenandoahUseSimpleCardScanning) {\n-              _heap->card_scan()->process_region(r, &cl);\n-            } else if (ShenandoahBarrierSet::barrier_set()->card_table()->is_dirty(MemRegion(r->bottom(), r->top()))) {\n-              update_all_references(&cl, r, update_watermark);\n+            \/\/ Old region in a young cycle or mixed cycle.\n+            if (ShenandoahUseSimpleCardScanning) {\n+              if (ShenandoahBarrierSet::barrier_set()->card_table()->is_dirty(MemRegion(r->bottom(), r->top()))) {\n+                update_all_references(&cl, r, update_watermark );\n+              }\n+            } else if (!_mixed_evac) {\n+              \/\/ This is a young evac..\n+              _heap->card_scan()->process_region(r, &cl, true);\n+            } else {\n+              \/\/ This is a _mixed_evac.\n+              \/\/\n+              \/\/ TODO: For _mixed_evac, consider building an old-gen remembered set that allows restricted updating\n+              \/\/ within old-gen HeapRegions.  This remembered set can be constructed by old-gen concurrent marking\n+              \/\/ and augmented by card marking.  For example, old-gen concurrent marking can remember for each old-gen\n+              \/\/ card which other old-gen regions it refers to: none, one-other specifically, multiple-other non-specific.\n+              \/\/ Update-references when _mixed_evac processess each old-gen memory range that has a traditional DIRTY\n+              \/\/ card or if the \"old-gen remembered set\" indicates that this card holds pointers specifically to an\n+              \/\/ old-gen region in the most recent collection set, or if this card holds pointers to other non-specific\n+              \/\/ old-gen heap regions.\n+              if (r->is_humongous()) {\n+                r->oop_iterate_humongous(&cl);\n+              } else {\n+                \/\/ This is a mixed evacuation.  Old regions that are candidates for collection have not been coalesced\n+                \/\/ and filled.  Use mark bits to find objects that need to be updated.\n+                \/\/\n+                \/\/ Future TODO: establish a second remembered set to identify which old-gen regions point to other old-gen\n+                \/\/ regions which are in the collection set for a particular mixed evacuation.\n+                HeapWord *p = r->bottom();\n+                ShenandoahObjectToOopBoundedClosure<T> objs(&cl, p, update_watermark);\n+\n+                \/\/ Anything beyond update_watermark was allocated during evacuation.  Thus, it is known to not hold\n+                \/\/ references to collection set objects.\n+                while (p < update_watermark) {\n+                  oop obj = oop(p);\n+                  if (ctx->is_marked(obj)) {\n+                    objs.do_object(obj);\n+                    p += obj->size();\n+                  } else {\n+                    \/\/ This object is not marked so we don't scan it.\n+                    HeapWord* tams = ctx->top_at_mark_start(r);\n+                    if (p >= tams) {\n+                      p += obj->size();\n+                    } else {\n+                      p = ctx->get_next_marked_addr(p, tams);\n+                    }\n+                  }\n+                }\n+              }\n@@ -2156,0 +2232,14 @@\n+        } else {\n+          \/\/ Because updating of references runs concurrently, it is possible that a FREE inactive region transitions\n+          \/\/ to a non-free active region while this loop is executing.  Whenever this happens, the changing of a region's\n+          \/\/ active status may propagate at a different speed than the changing of the region's affiliation.\n+\n+          \/\/ When we reach this control point, it is because a race has allowed a region's is_active() status to be seen\n+          \/\/ by this thread before the region's affiliation() is seen by this thread.\n+\n+          \/\/ It's ok for this race to occur because the newly transformed region does not have any references to be\n+          \/\/ updated.\n+\n+          assert(r->get_update_watermark() == r->bottom(),\n+                 \"%s Region \" SIZE_FORMAT \" is_active but not recognized as YOUNG or OLD so must be newly transitioned from FREE\",\n+                 affiliation_name(r->affiliation()), r->index());\n@@ -2192,1 +2282,1 @@\n-    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<true> task(&_update_refs_iterator, _mixed_evac);\n@@ -2195,1 +2285,1 @@\n-    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator);\n+    ShenandoahUpdateHeapRefsTask<false> task(&_update_refs_iterator, _mixed_evac);\n@@ -2444,0 +2534,1 @@\n+\n@@ -2478,0 +2569,182 @@\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.\n+\/\/ This examines the read_card_table between bottom() and top() since all PLABS are retired\n+\/\/ before the safepoint for init_mark.  Actually, we retire them before update-references and don't\n+\/\/ restore them until the start of evacuation.\n+void ShenandoahHeap::verify_rem_set_at_mark() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+  ShenandoahMarkingContext* mark_context = marking_context();\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(true);\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (doing_mixed_evacuations()) {\n+    ctx = mark_context;\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old()) {\n+      HeapWord* obj_addr = r->bottom();\n+      if (r->is_humongous_start()) {\n+        oop obj = oop(obj_addr);\n+        if (!ctx || ctx->is_marked(obj)) {\n+          \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+          \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+          \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+          if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+            obj->oop_iterate(&check_interesting_pointers);\n+          }\n+          \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+        }\n+        \/\/ else, this humongous object is not marked so no need to verify its internal pointers\n+        if (!scanner->verify_registration(obj_addr, obj->size())) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                          \"Verify init-mark remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+      } else if (!r->is_humongous()) {\n+        HeapWord* t = r->top();\n+        while (obj_addr < t) {\n+          oop obj = oop(obj_addr);\n+          \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+          if (!ctx || ctx->is_marked(obj)) {\n+            \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+            \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+            if (!scanner->is_card_dirty(obj_addr) || obj->is_objArray()) {\n+              obj->oop_iterate(&check_interesting_pointers);\n+            }\n+            \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+            if (!scanner->verify_registration(obj_addr, obj->size())) {\n+              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                            \"Verify init-mark remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+            }\n+            obj_addr += obj->size();\n+          } else {\n+            \/\/ This object is not live so we don't verify dirty cards contained therein\n+            ShenandoahHeapRegion* r = heap_region_containing(obj_addr);\n+            HeapWord* tams = ctx->top_at_mark_start(r);\n+            if (obj_addr >= tams) {\n+              obj_addr += obj->size();\n+            } else {\n+              obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+            }\n+          }\n+        }\n+      } \/\/ else, we ignore humongous continuation region\n+    } \/\/ else, this is not an OLD region so we ignore it\n+  } \/\/ all regions have been processed\n+}\n+\n+\/\/ Assure that the remember set has a dirty card everywhere there is an interesting pointer.  Even though\n+\/\/ the update-references scan of remembered set only examines cards up to update_watermark, the remembered\n+\/\/ set should be valid through top.  This examines the write_card_table between bottom() and top() because\n+\/\/ all PLABS are retired immediately before the start of update refs.\n+void ShenandoahHeap::verify_rem_set_at_update_ref() {\n+  shenandoah_assert_safepoint();\n+  assert(mode()->is_generational(), \"Only verify remembered set for generational operational modes\");\n+\n+  ShenandoahRegionIterator iterator;\n+  ShenandoahMarkingContext* mark_context = marking_context();\n+  RememberedScanner* scanner = card_scan();\n+  ShenandoahVerifyRemSetClosure check_interesting_pointers(false);\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (doing_mixed_evacuations()) {\n+    ctx = mark_context;\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  while (iterator.has_next()) {\n+    ShenandoahHeapRegion* r = iterator.next();\n+    if (r == nullptr)\n+      break;\n+    if (r->is_old() && !r->is_cset()) {\n+      HeapWord* obj_addr = r->bottom();\n+      if (r->is_humongous_start()) {\n+        oop obj = oop(obj_addr);\n+        if (!ctx || ctx->is_marked(obj)) {\n+          size_t card_index = scanner->card_index_for_addr(obj_addr);\n+          \/\/ For humongous objects, the typical object is an array, so the following checks may be overkill\n+          \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+          \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+          if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+            obj->oop_iterate(&check_interesting_pointers);\n+          }\n+          \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+        }\n+        \/\/ else, this humongous object is not live so no need to verify its internal pointers\n+        if (!scanner->verify_registration(obj_addr, obj->size())) {\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                          \"Verify init-update-references remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+        }\n+      } else if (!r->is_humongous()) {\n+        HeapWord* t = r->get_update_watermark();\n+        while (obj_addr < t) {\n+          oop obj = oop(obj_addr);\n+          \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+          if (!ctx || ctx->is_marked(obj)) {\n+            size_t card_index = scanner->card_index_for_addr(obj_addr);\n+            \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+            \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+            if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+              obj->oop_iterate(&check_interesting_pointers);\n+            }\n+            \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+            if (!scanner->verify_registration(obj_addr, obj->size())) {\n+              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                               \"Verify init-update-references remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+            }\n+            obj_addr += obj->size();\n+          } else {\n+            \/\/ This object is not live so we don't verify dirty cards contained therein\n+            ShenandoahHeapRegion* r = heap_region_containing(obj_addr);\n+            HeapWord* tams = ctx->top_at_mark_start(r);\n+            if (obj_addr >= tams) {\n+              obj_addr += obj->size();\n+            } else {\n+              obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+            }\n+          }\n+        }\n+        \/\/ Update references only cares about remembered set below update_watermark, but entire remset should be valid\n+        \/\/ We're at safepoint and all LABs have been flushed, so we can parse all the way to top().\n+        t = r->top();\n+        while (obj_addr < t) {\n+          oop obj = oop(obj_addr);\n+          \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+          if (!ctx || ctx->is_marked(obj)) {\n+            size_t card_index = scanner->card_index_for_addr(obj_addr);\n+            \/\/ For regular objects (not object arrays), if the card holding the start of the object is dirty,\n+            \/\/ we do not need to verify that cards spanning interesting pointers within this object are dirty.\n+            if (!scanner->is_write_card_dirty(card_index) || obj->is_objArray()) {\n+              obj->oop_iterate(&check_interesting_pointers);\n+            }\n+            \/\/ else, object's start is marked dirty and obj is not an objArray, so any interesting pointers are covered\n+            if (!scanner->verify_registration(obj_addr, obj->size())) {\n+              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, NULL,\n+                                               \"Verify init-update-references remembered set violation\", \"object not properly registered\", __FILE__, __LINE__);\n+            }\n+            obj_addr += obj->size();\n+          } else {\n+            \/\/ This object is not live so we don't verify dirty cards contained therein\n+            ShenandoahHeapRegion* r = heap_region_containing(obj_addr);\n+            HeapWord* tams = ctx->top_at_mark_start(r);\n+            if (obj_addr >= tams) {\n+              obj_addr += obj->size();\n+            } else {\n+              obj_addr = ctx->get_next_marked_addr(obj_addr, tams);\n+            }\n+          }\n+        }\n+      }\n+    } \/\/ else, we don't care about this region\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":296,"deletions":23,"binary":false,"changes":319,"status":"modified"},{"patch":"@@ -152,0 +152,1 @@\n+  bool _mixed_evac;             \/\/ true iff most recent evac included at least one old-gen HeapRegion\n@@ -167,0 +168,4 @@\n+  void set_mixed_evac(bool mixed_evac) {\n+    _mixed_evac = mixed_evac;\n+  }\n+\n@@ -171,0 +176,2 @@\n+  bool doing_mixed_evacuations();\n+\n@@ -198,0 +205,2 @@\n+  void verify_rem_set_at_mark();\n+  void verify_rem_set_at_update_ref();\n@@ -374,0 +383,5 @@\n+\n+  \/\/ Returns true if cancel request was successfully communicated.\n+  \/\/ Returns false if some other thread already communicated cancel\n+  \/\/ request.  A true return value does not mean GC has been\n+  \/\/ cancelled, only that the process of cancelling GC has begun.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -233,0 +233,3 @@\n+  if (mode()->is_generational() && obj != NULL) {\n+    ShenandoahHeap::heap()->card_scan()->register_object_wo_lock(obj);\n+  }\n@@ -269,0 +272,2 @@\n+\/\/ try_evacuate_object registers the object and dirties the associated remembered set information when evacuating\n+\/\/ to OLD_GENERATION.\n@@ -341,2 +346,10 @@\n-      ShenandoahBarrierSet::barrier_set()->card_table()->dirty_MemRegion(MemRegion(copy, size));\n-      card_scan()->register_object(copy);\n+      if (alloc_from_lab) {\n+        card_scan()->register_object_wo_lock(copy);\n+      }\n+      \/\/ else, allocate_memory_under_lock() has already registered the object\n+\n+      \/\/ Mark the entire range of the evacuated object as dirty.  At next remembered set scan,\n+      \/\/ we will clear dirty bits that do not hold interesting pointers.  It's more efficient to\n+      \/\/ do this in batch, in a background GC thread than to try to carefully dirty only cards\n+      \/\/ that hold interesting pointers right now.\n+      card_scan()->mark_range_as_dirty(copy, size);\n@@ -377,3 +390,1 @@\n-      if (target_gen == OLD_GENERATION) {\n-        card_scan()->register_object(copy);\n-      }\n+      \/\/ For non-LAB allocations, the object has already been registered\n@@ -647,1 +658,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":16,"deletions":6,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -158,1 +158,1 @@\n-void ShenandoahHeapRegion::make_humongous_start_bypass() {\n+void ShenandoahHeapRegion::make_humongous_start_bypass(ShenandoahRegionAffiliation affiliation) {\n@@ -161,0 +161,1 @@\n+  set_affiliation(affiliation);\n@@ -188,1 +189,1 @@\n-void ShenandoahHeapRegion::make_humongous_cont_bypass() {\n+void ShenandoahHeapRegion::make_humongous_cont_bypass(ShenandoahRegionAffiliation affiliation) {\n@@ -191,0 +192,1 @@\n+  set_affiliation(affiliation);\n@@ -286,1 +288,3 @@\n-  ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n+  \/\/ Leave top_bitmap alone.  If it is greater than bottom(), then we still need to clear between bottom() and top_bitmap()\n+  \/\/ when this FREE region is repurposed for YOUNG or OLD.\n+  \/\/ ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n@@ -427,1 +431,0 @@\n-  HeapWord* t = top();\n@@ -434,0 +437,6 @@\n+  \/\/ All objects above TAMS are considered live even though their mark bits will not be set.  Note that young-\n+  \/\/ gen evacuations that interrupt a long-running old-gen concurrent mark may promote objects into old-gen\n+  \/\/ while the old-gen concurrent marking is ongoing.  These newly promoted objects will reside above TAMS\n+  \/\/ and will be treated as live during the current old-gen marking pass, even though they will not be\n+  \/\/ explicitly marked.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n@@ -437,0 +446,1 @@\n+  assert(heap->active_generation()->generation_mode() == OLD, \"sanity\");\n@@ -456,2 +466,1 @@\n-\n-void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk, bool fill_dead_objects, bool reregister_coalesced_objects) {\n+void ShenandoahHeapRegion::global_oop_iterate_and_fill_dead(OopIterateClosure* blk) {\n@@ -460,5 +469,2 @@\n-    \/\/ TODO: This doesn't look right.  This registers objects if !reregister, and it isn't filling if fill_dead_objects.\n-    \/\/ Furthermore, register and fill should be done after iterating.\n-    if (fill_dead_objects && !reregister_coalesced_objects) {\n-      ShenandoahHeap::heap()->card_scan()->register_object(bottom());\n-    }\n+    \/\/ No need to fill dead within humongous regions.  Either the entire region is dead, or the entire region is\n+    \/\/ unchanged.  A humongous region holds no more than one humongous object.\n@@ -467,1 +473,1 @@\n-    oop_iterate_objects(blk, fill_dead_objects, reregister_coalesced_objects);\n+    global_oop_iterate_objects_and_fill_dead(blk);\n@@ -471,1 +477,1 @@\n-void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk, bool fill_dead_objects, bool reregister_coalesced_objects) {\n+void ShenandoahHeapRegion::global_oop_iterate_objects_and_fill_dead(OopIterateClosure* blk) {\n@@ -474,1 +480,0 @@\n-  HeapWord* t = top();\n@@ -476,3 +481,11 @@\n-  if (!fill_dead_objects) {\n-    while (obj_addr < t) {\n-      oop obj = oop(obj_addr);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  RememberedScanner* rem_set_scanner = heap->card_scan();\n+  \/\/ Objects allocated above TAMS are not marked, but are considered live for purposes of current GC efforts.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  while (obj_addr < t) {\n+    oop obj = oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n@@ -480,0 +493,1 @@\n+      \/\/ when promoting an entire region, we have to register the marked objects as well\n@@ -481,0 +495,10 @@\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+\n+      \/\/ coalesce_objects() unregisters all but first object subsumed within coalesced range.\n+      rem_set_scanner->coalesce_objects(obj_addr, fill_size);\n+      obj_addr = next_marked_obj;\n@@ -482,27 +506,43 @@\n-  } else {\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* marking_context = heap->marking_context();\n-    assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n-    HeapWord* tams = marking_context->top_at_mark_start(this);\n-\n-    while (obj_addr < t) {\n-      oop obj = oop(obj_addr);\n-      if (marking_context->is_marked(obj)) {\n-        assert(obj->klass() != NULL, \"klass should not be NULL\");\n-        if (!reregister_coalesced_objects) {\n-          heap->card_scan()->register_object(obj_addr);\n-        }\n-        obj_addr += obj->oop_iterate_size(blk);\n-      } else {\n-        \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n-        HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, tams);\n-        assert(next_marked_obj <= tams, \"next marked object cannot exceed top at mark start\");\n-        size_t fill_size = next_marked_obj - obj_addr;\n-        ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n-        if (reregister_coalesced_objects) {\n-          heap->card_scan()->coalesce_objects(obj_addr, fill_size);\n-        } else {              \/\/ establish new crossing map information\n-          heap->card_scan()->register_object(obj_addr);\n-        }\n-        obj_addr = next_marked_obj;\n-      }\n+  }\n+\n+  \/\/ Any object above TAMS and below top() is considered live.\n+  t = top();\n+  while (obj_addr < t) {\n+    oop obj = oop(obj_addr);\n+    obj_addr += obj->oop_iterate_size(blk);\n+  }\n+}\n+\n+\/\/ This function does not set card dirty bits.  The decision of which cards to dirty is best\n+\/\/ made in the caller's context.\n+void ShenandoahHeapRegion::fill_dead_and_register_for_promotion() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  HeapWord* obj_addr = bottom();\n+  RememberedScanner* rem_set_scanner = heap->card_scan();\n+  \/\/ Objects allocated above TAMS are not marked, but are considered live for purposes of current GC efforts.\n+  HeapWord* t = marking_context->top_at_mark_start(this);\n+\n+  assert(!is_humongous(), \"no humongous region here\");\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  \/\/ end() might be overkill as end of range, but top() may not align with card boundary.\n+  rem_set_scanner->reset_object_range(bottom(), end());\n+  while (obj_addr < t) {\n+    oop obj = oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != NULL, \"klass should not be NULL\");\n+      \/\/ when promoting an entire region, we have to register the marked objects as well\n+      rem_set_scanner->register_object_wo_lock(obj_addr);\n+      obj_addr += obj->size();\n+    } else {\n+      \/\/ Object is not marked.  Coalesce and fill dead object with dead neighbors.\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, t);\n+      assert(next_marked_obj <= t, \"next marked object cannot exceed top\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      assert(fill_size >= (size_t) oopDesc::header_size(),\n+             \"fill size \" SIZE_FORMAT \" for obj @ \" PTR_FORMAT \", next_marked: \" PTR_FORMAT \", TAMS: \" PTR_FORMAT \" is too small\",\n+             fill_size, p2i(obj_addr), p2i(next_marked_obj), p2i(t));\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      rem_set_scanner->register_object_wo_lock(obj_addr);\n+      obj_addr = next_marked_obj;\n@@ -511,0 +551,16 @@\n+\n+  \/\/ Any object above TAMS and below top() is considered live.\n+  t = top();\n+  while (obj_addr < t) {\n+    oop obj = oop(obj_addr);\n+    assert(obj->klass() != NULL, \"klass should not be NULL\");\n+    \/\/ when promoting an entire region, we have to register the marked objects as well\n+    rem_set_scanner->register_object_wo_lock(obj_addr);\n+    obj_addr += obj->size();\n+  }\n+\n+  \/\/ In case top() does not align with a card boundary, it's necessary to fill remainder of memory beyond top().\n+  if (top() < end()) {\n+    ShenandoahHeap::fill_with_object(top(), end() - top());;\n+    rem_set_scanner->register_object_wo_lock(obj_addr);\n+  }\n@@ -792,0 +848,21 @@\n+\n+  {\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    log_debug(gc)(\"Setting affiliation of Region \" SIZE_FORMAT \" from %s to %s, top: \" PTR_FORMAT \", TAMS: \" PTR_FORMAT\n+                  \", watermark: \" PTR_FORMAT \", top_bitmap: \" PTR_FORMAT \"\\n\",\n+                  index(), affiliation_name(_affiliation), affiliation_name(new_affiliation),\n+                  p2i(top()), p2i(ctx->top_at_mark_start(this)), p2i(this->get_update_watermark()), p2i(ctx->top_bitmap(this)));\n+  }\n+\n+#ifdef ASSERT\n+  {\n+    ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+    size_t idx = this->index();\n+    HeapWord* top_bitmap = ctx->top_bitmap(this);\n+\n+    assert(ctx->is_bitmap_clear_range(top_bitmap, _end),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear between top_bitmap: \" PTR_FORMAT \" and end: \" PTR_FORMAT, idx,\n+           p2i(top_bitmap), p2i(_end));\n+  }\n+#endif\n+\n@@ -810,1 +887,0 @@\n-  CardTable* card_table = ShenandoahBarrierSet::barrier_set()->card_table();\n@@ -814,1 +890,0 @@\n-      card_table->clear_MemRegion(MemRegion(_bottom, _end));\n@@ -830,27 +905,2 @@\n-class UpdateCardValuesClosure : public BasicOopIterateClosure {\n-private:\n-  void update_card_value(void* address, oop obj) {\n-    if (ShenandoahHeap::heap()->is_in_young(obj)) {\n-      ShenandoahHeap::heap()->mark_card_as_dirty(address);\n-    }\n-  }\n-\n-public:\n-  void do_oop(oop* p) {\n-    oop obj = *p;\n-    if (obj != NULL) {\n-      update_card_value(p, obj);\n-    }\n-  }\n-\n-  void do_oop(narrowOop* p) {\n-    narrowOop o = RawAccess<>::oop_load(p);\n-    if (!CompressedOops::is_null(o)) {\n-      oop obj = CompressedOops::decode_not_null(o);\n-      assert(oopDesc::is_oop(obj), \"must be a valid oop\");\n-      update_card_value(p, obj);\n-    }\n-  }\n-};\n-\n-size_t ShenandoahHeapRegion::promote() {\n+size_t ShenandoahHeapRegion::promote(bool promoting_all) {\n+  \/\/ TODO: Not sure why region promotion must be performed at safepoint.  Reconsider this requirement.\n@@ -859,0 +909,17 @@\n+  \/\/ Note that region promotion occurs at a safepoint following all evacuation.  When a region is promoted, we leave\n+  \/\/ its TAMS and update_watermark information as is.\n+  \/\/\n+  \/\/ Note that update_watermark represents the state of this region as of the moment at which the most recent evacuation\n+  \/\/ began.  The value of update_watermark is the same for old regions and young regions, as both participate equally in\n+  \/\/ the processes of a mixed evacuation.\n+  \/\/\n+  \/\/ The meaning of TAMS is different for young-gen and old-gen regions.  For a young-gen region, TAMS represents\n+  \/\/ top() at start of most recent young-gen concurrent mark.  For an old-gen region, TAMS represents top() at start\n+  \/\/ of most recent old-gen concurrent mark().  In the case that a young-gen heap region is promoted into old-gen,\n+  \/\/ we can preserve its TAMS information with the following understandings:\n+  \/\/   1. The most recent young-GC concurrent mark phase began at the same time or after the most recent old-GC\n+  \/\/      concurrent mark phase.\n+  \/\/   2. After the region is promoted, it is still the case that any object within the region that is beneath TAMS\n+  \/\/      and is considered alive for the current old GC pass will be \"marked\" within the current marking context, and\n+  \/\/      any object within the region that is above TAMS will be considered alive for the current old GC pass.  Objects\n+  \/\/      that were dead at promotion time will all reside below TAMS and will be unmarked.\n@@ -864,1 +931,0 @@\n-  UpdateCardValuesClosure update_card_values;\n@@ -871,1 +937,3 @@\n-    heap->card_scan()->register_object(bottom());\n+\n+    \/\/ Since the humongous region holds only one object, no lock is necessary for this register_object() invocation.\n+    heap->card_scan()->register_object_wo_lock(bottom());\n@@ -873,0 +941,4 @@\n+\n+    \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n+    \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n+    \/\/ in the last humongous region that is not spanned by obj is currently not used.\n@@ -879,2 +951,2 @@\n-        heap->card_scan()->register_object(r->top());\n-        heap->clear_cards(r->top(), r->end());\n+        heap->card_scan()->register_object_wo_lock(r->top());\n+        heap->card_scan()->mark_range_as_clean(top(), r->end() - r->top());\n@@ -882,1 +954,1 @@\n-      heap->dirty_cards(r->bottom(), r->top());\n+      \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n@@ -884,0 +956,2 @@\n+      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", dirtying cards from \" SIZE_FORMAT \" to \" SIZE_FORMAT,\n+                    i, (size_t) r->bottom(), (size_t) r->top());\n@@ -887,0 +961,10 @@\n+    if (promoting_all || obj->is_typeArray()) {\n+      \/\/ Primitive arrays don't need to be scanned.  Likewise, if we are promoting_all, there's nothing\n+      \/\/ left in young-gen, so there can exist no \"interesting\" pointers.  See above TODO question about requiring\n+      \/\/ region promotion at safepoint.  If we're not at a safepoint, then we can't really \"promote all\" without\n+      \/\/ directing new allocations to old-gen.  That's probably not what we want.  The whole \"promote-all strategy\"\n+      \/\/ probably needs to be revisited at some future point.\n+      heap->card_scan()->mark_range_as_clean(bottom(), obj->size());\n+    } else {\n+      heap->card_scan()->mark_range_as_dirty(bottom(), obj->size());\n+    }\n@@ -889,1 +973,1 @@\n-    log_debug(gc)(\"promoting region \" SIZE_FORMAT \", from \" SIZE_FORMAT \" to \" SIZE_FORMAT,\n+    log_debug(gc)(\"promoting region \" SIZE_FORMAT \", dirtying cards from \" SIZE_FORMAT \" to \" SIZE_FORMAT,\n@@ -893,0 +977,13 @@\n+    fill_dead_and_register_for_promotion();\n+    \/\/ Rather than scanning entire contents of the promoted region right now to determine which\n+    \/\/ cards to mark as dirty, we just mark them all as dirty (unless promoting_all).  Later, when we\n+    \/\/ scan the remembered set, we will clear cards that are found to not contain live references to\n+    \/\/ young memory.  Ultimately, this approach is more efficient as it only scans the \"dirty\" cards\n+    \/\/ once and the clean cards once.  The alternative approach of scanning all cards now and then\n+    \/\/ scanning dirty cards again at next concurrent mark pass scans the clean cards once and the dirty\n+    \/\/ cards twice.\n+    if (promoting_all) {\n+      heap->card_scan()->mark_range_as_clean(bottom(), top() - bottom());\n+    } else {\n+      heap->card_scan()->mark_range_as_dirty(bottom(), top() - bottom());\n+    }\n@@ -896,7 +993,0 @@\n-\n-    \/\/ In terms of card marking, We could just set the whole occupied range in this region to dirty instead of iterating here.\n-    \/\/ Card scanning could correct false positives later and that would be more efficient.\n-    \/\/ But oop_iterate_objects() has other, indispensable effects: filling dead objects and registering object starts.\n-    \/\/ So while we are already doing this here, we may as well also set more precise card values.\n-    heap->dirty_cards(bottom(), top());\n-    oop_iterate_objects(&update_card_values, \/*fill_dead_objects*\/ true, \/* reregister_coalesced_objects *\/ false);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":176,"deletions":86,"binary":false,"changes":262,"status":"modified"},{"patch":"@@ -175,2 +175,2 @@\n-  void make_humongous_start_bypass();\n-  void make_humongous_cont_bypass();\n+  void make_humongous_start_bypass(ShenandoahRegionAffiliation affiliation);\n+  void make_humongous_cont_bypass(ShenandoahRegionAffiliation affiliation);\n@@ -370,1 +370,2 @@\n-  \/\/ coalesce contiguous spans of garbage objects by filling header and reregistering start locations with remembered set.\n+  \/\/ Coalesce contiguous spans of garbage objects by filling header and reregistering start locations with remembered set.\n+  \/\/ This is used by old-gen GC following concurrent marking to make old-gen HeapRegions parseable.\n@@ -373,1 +374,4 @@\n-  void oop_iterate(OopIterateClosure* cl, bool fill_dead_objects = false, bool reregister_coalesced_objects = false);\n+  \/\/ During global collections, this service iterates through an old-gen heap region that is not part of collection\n+  \/\/ set to fill and register ranges of dead memory.  Note that live objects were previously registered.  Some dead objects\n+  \/\/ that are subsumed into coalesced ranges of dead memory need to be \"unregistered\".\n+  void global_oop_iterate_and_fill_dead(OopIterateClosure* cl);\n@@ -417,2 +421,6 @@\n-  \/\/ If this is a humongous start, returns the number of regions in the object.\n-  size_t promote();\n+  \/\/ Adjusts remembered set information by setting all cards to clean if promoting all, setting\n+  \/\/ all cards to dirty otherwise.\n+  \/\/\n+  \/\/ Returns the number of regions promoted, which is generally one, but may be greater than 1 if\n+  \/\/ this is humongous region with multiple continuations.\n+  size_t promote(bool promoting_all);\n@@ -424,2 +432,7 @@\n-  void oop_iterate_objects(OopIterateClosure* cl, bool fill_dead_objects, bool reregister_coalesced_objects);\n-  void oop_iterate_objects(bool fill_dead_objects, bool reregister_coalesced_objects);\n+  \/\/ This is an old-region that was not part of the collection set during a GLOBAL collection.  We coalesce the dead\n+  \/\/ objects, but do not need to register the live objects as they are already registered.\n+  void global_oop_iterate_objects_and_fill_dead(OopIterateClosure* cl);\n+\n+  \/\/ Process the contents of a region when it is being promoted en masse by registering each marked object, coalescing\n+  \/\/ contiguous ranges of unmarked objects into registered dead objects.  Do not touch card marks.\n+  void fill_dead_and_register_for_promotion();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":21,"deletions":8,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -91,1 +91,2 @@\n-         \"can't have more live data than used: \" SIZE_FORMAT \", \" SIZE_FORMAT, live_bytes, used_bytes);\n+         \"%s Region \" SIZE_FORMAT \" can't have more live data than used: \" SIZE_FORMAT \", \" SIZE_FORMAT \" after adding \" SIZE_FORMAT,\n+         affiliation_name(affiliation()), index(), live_bytes, used_bytes, s * HeapWordSize);\n@@ -96,0 +97,1 @@\n+  log_debug(gc)(\"SHR::clear_live_data on %s Region \" SIZE_FORMAT,  affiliation_name(affiliation()), index());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -181,1 +181,1 @@\n-               bool cancellable, bool strdedup) {\n+                               bool cancellable, bool strdedup) {\n@@ -215,14 +215,0 @@\n-template<>\n-bool ShenandoahMark::in_generation<YOUNG>(oop obj) {\n-  return ShenandoahHeap::heap()->is_in_young(obj);\n-}\n-\n-template<>\n-bool ShenandoahMark::in_generation<OLD>(oop obj) {\n-  return ShenandoahHeap::heap()->is_in_old(obj);\n-}\n-\n-template<>\n-bool ShenandoahMark::in_generation<GLOBAL>(oop obj) {\n-  return true;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.cpp","additions":1,"deletions":15,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -74,0 +74,2 @@\n+  inline ShenandoahGeneration* generation() { return _generation; };\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -106,0 +106,1 @@\n+    assert(region->affiliation() != FREE, \"Do not count live data within Free Regular Region \" SIZE_FORMAT, region_idx);\n@@ -120,0 +121,1 @@\n+    assert(region->affiliation() != FREE, \"Do not count live data within FREE Humongous Start Region \" SIZE_FORMAT, region_idx);\n@@ -123,0 +125,1 @@\n+      assert(chain_reg->affiliation() != FREE, \"Do not count live data within FREE Humongous Continuation Region \" SIZE_FORMAT, i);\n@@ -259,0 +262,13 @@\n+template<GenerationMode GENERATION>\n+bool ShenandoahMark::in_generation(oop obj) {\n+  \/\/ Each in-line expansion of in_generation() resolves GENERATION at compile time.\n+  if (GENERATION == YOUNG)\n+    return ShenandoahHeap::heap()->is_in_young(obj);\n+  else if (GENERATION == OLD)\n+    return ShenandoahHeap::heap()->is_in_old(obj);\n+  else if (GENERATION == GLOBAL)\n+    return true;\n+  else\n+    return false;\n+}\n+\n@@ -267,1 +283,0 @@\n-\n@@ -271,0 +286,18 @@\n+      if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+        \/\/ TODO: As implemented herein, GLOBAL collections reconstruct the card table during GLOBAL concurrent\n+        \/\/ marking. Note that the card table is cleaned at init_mark time so it needs to be reconstructed to support\n+        \/\/ future young-gen collections.  It might be better to reconstruct card table in\n+        \/\/ ShenandoahHeapRegion::global_oop_iterate_and_fill_dead.  We could either mark all live memory as dirty, or could\n+        \/\/ use the GLOBAL update-refs scanning of pointers to determine precisely which cards to flag as dirty.\n+        \/\/\n+        if ((GENERATION == YOUNG) && ShenandoahHeap::heap()->is_in(p) && ShenandoahHeap::heap()->is_in_old(p)) {\n+          RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+          \/\/ Mark card as dirty because remembered set scanning still finds interesting pointer.\n+          ShenandoahHeap::heap()->mark_card_as_dirty((HeapWord*)p);\n+        } else if ((GENERATION == GLOBAL) && in_generation<YOUNG>(obj) &&\n+                   ShenandoahHeap::heap()->is_in(p) && ShenandoahHeap::heap()->is_in_old(p)) {\n+          RememberedScanner* scanner = ShenandoahHeap::heap()->card_scan();\n+          \/\/ Mark card as dirty because GLOBAL marking finds interesting pointer.\n+          ShenandoahHeap::heap()->mark_card_as_dirty((HeapWord*)p);\n+        }\n+      }\n@@ -277,0 +310,2 @@\n+      \/\/ TODO:  Rethink this: may be redundant with dirtying of cards identified during young-gen remembered set scanning\n+      \/\/ and by mutator write barriers.  Assert\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.inline.hpp","additions":36,"deletions":1,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -46,0 +46,14 @@\n+bool ShenandoahMarkBitMap::is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const {\n+  \/\/ Similar to get_next_marked_addr(), without assertion.\n+  \/\/ Round addr up to a possible object boundary to be safe.\n+  if (start == end) {\n+    return true;\n+  }\n+  size_t const addr_offset = address_to_index(align_up(start, HeapWordSize << LogMinObjAlignment));\n+  size_t const limit_offset = address_to_index(end);\n+  size_t const next_offset = get_next_one_offset(addr_offset, limit_offset);\n+  HeapWord* result = index_to_address(next_offset);\n+  return (result == end);\n+}\n+\n+\n@@ -48,0 +62,5 @@\n+#ifdef ASSERT\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahHeapRegion* r = heap->heap_region_containing(addr);\n+  ShenandoahMarkingContext* ctx = heap->marking_context();\n+  HeapWord* tams = ctx->top_at_mark_start(r);\n@@ -49,0 +68,3 @@\n+  assert(limit <= tams, \"limit must be less than TAMS\");\n+#endif\n+\n@@ -53,1 +75,2 @@\n-  return index_to_address(nextOffset);\n+  HeapWord* result = index_to_address(nextOffset);\n+  return result;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.cpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -162,0 +162,2 @@\n+  bool is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkBitMap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -89,0 +89,7 @@\n+ShenandoahCaptureUpdateWaterMarkForOld::ShenandoahCaptureUpdateWaterMarkForOld(ShenandoahMarkingContext* ctx) :\n+  _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()) {}\n+\n+void ShenandoahCaptureUpdateWaterMarkForOld::heap_region_do(ShenandoahHeapRegion* r) {\n+  \/\/ Remember limit for updating refs. It's guaranteed that we get no from-space-refs written from here on.\n+  r->set_update_watermark_at_safepoint(r->top());\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -45,0 +45,12 @@\n+class ShenandoahCaptureUpdateWaterMarkForOld : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahMarkingContext* const _ctx;\n+  ShenandoahHeapLock* const _lock;\n+public:\n+  ShenandoahCaptureUpdateWaterMarkForOld(ShenandoahMarkingContext* ctx);\n+\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.hpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-    if (heap->is_bitmap_slice_committed(r) && !is_bitmap_clear_range(r->bottom(), r->end())) {\n+    if ((r->affiliation() != FREE) && heap->is_bitmap_slice_committed(r) && !is_bitmap_clear_range(r->bottom(), r->end())) {\n@@ -50,2 +50,13 @@\n-bool ShenandoahMarkingContext::is_bitmap_clear_range(HeapWord* start, HeapWord* end) const {\n-  return _mark_bit_map.get_next_marked_addr(start, end) == end;\n+bool ShenandoahMarkingContext::is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const {\n+  if (start < end) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    size_t start_idx = heap->heap_region_index_containing(start);\n+    size_t end_idx = heap->heap_region_index_containing(end - 1);\n+    while (start_idx <= end_idx) {\n+      ShenandoahHeapRegion* r = heap->get_region(start_idx);\n+      if (!heap->is_bitmap_slice_committed(r))\n+        return true;\n+      start_idx++;\n+    }\n+  }\n+  return _mark_bit_map.is_bitmap_clear_range(start, end);\n@@ -57,0 +68,1 @@\n+\n@@ -58,1 +70,9 @@\n-  _top_bitmaps[idx] = bottom;\n+  \/\/ Arrange that the first time we use this bitmap, we clean from bottom to end.\n+  _top_bitmaps[idx] = r->end();\n+\n+  log_debug(gc)(\"SMC:initialize_top_at_mark_start for region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmaps set to \" PTR_FORMAT,\n+                p2i(r->bottom()), p2i(r->end()), p2i(r->end()));\n+}\n+\n+HeapWord* ShenandoahMarkingContext::top_bitmap(ShenandoahHeapRegion* r) {\n+  return _top_bitmaps[r->index()];\n@@ -64,3 +84,12 @@\n-  if (top_bitmap > bottom) {\n-    _mark_bit_map.clear_range_large(MemRegion(bottom, top_bitmap));\n-    _top_bitmaps[r->index()] = bottom;\n+\n+  log_debug(gc)(\"SMC:clear_bitmap for %s region [\" PTR_FORMAT \", \" PTR_FORMAT \"], top_bitmap: \" PTR_FORMAT,\n+                affiliation_name(r->affiliation()), p2i(r->bottom()), p2i(r->end()), p2i(top_bitmap));\n+\n+  if (r->affiliation() != FREE) {\n+    if (top_bitmap > bottom) {\n+      _mark_bit_map.clear_range_large(MemRegion(bottom, top_bitmap));\n+      _top_bitmaps[r->index()] = bottom;\n+    }\n+    r->clear_live_data();\n+    assert(is_bitmap_clear_range(bottom, r->end()),\n+           \"Region \" SIZE_FORMAT \" should have no marks in bitmap\", r->index());\n@@ -68,2 +97,2 @@\n-  assert(is_bitmap_clear_range(bottom, r->end()),\n-         \"Region \" SIZE_FORMAT \" should have no marks in bitmap\", r->index());\n+  \/\/ heap iterators include FREE regions, which don't need to be cleared.\n+  \/\/ TODO: would be better for certain iterators to not include FREE regions.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.cpp","additions":38,"deletions":9,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -78,0 +78,2 @@\n+  HeapWord* top_bitmap(ShenandoahHeapRegion* r);\n+\n@@ -82,1 +84,1 @@\n-  bool is_bitmap_clear_range(HeapWord* start, HeapWord* end) const;\n+  bool is_bitmap_clear_range(const HeapWord* start, const HeapWord* end) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -76,13 +76,32 @@\n-  size_t idx = r->index();\n-  HeapWord* old_tams = _top_at_mark_starts_base[idx];\n-  HeapWord* new_tams = r->top();\n-\n-  assert(new_tams >= old_tams,\n-         \"Region \" SIZE_FORMAT\", TAMS updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n-         idx, p2i(old_tams), p2i(new_tams));\n-  assert(is_bitmap_clear_range(old_tams, new_tams),\n-         \"Region \" SIZE_FORMAT \", bitmap should be clear while adjusting TAMS: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n-         idx, p2i(old_tams), p2i(new_tams));\n-\n-  _top_at_mark_starts_base[idx] = new_tams;\n-  _top_bitmaps[idx] = new_tams;\n+  if (r->affiliation() != FREE) {\n+    size_t idx = r->index();\n+    HeapWord* old_tams = _top_at_mark_starts_base[idx];\n+    HeapWord* new_tams = r->top();\n+\n+    assert(new_tams >= old_tams,\n+           \"Region \" SIZE_FORMAT\", TAMS updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(old_tams), p2i(new_tams));\n+    assert((new_tams == r->bottom()) || (old_tams == r->bottom()) || (new_tams >= _top_bitmaps[idx]),\n+           \"Region \" SIZE_FORMAT\", top_bitmaps updates should be monotonic: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(_top_bitmaps[idx]), p2i(new_tams));\n+    assert(old_tams == r->bottom() || is_bitmap_clear_range(old_tams, new_tams),\n+           \"Region \" SIZE_FORMAT \", bitmap should be clear while adjusting TAMS: \" PTR_FORMAT \" -> \" PTR_FORMAT,\n+           idx, p2i(old_tams), p2i(new_tams));\n+\n+    log_debug(gc)(\"Capturing TAMS for %s Region \" SIZE_FORMAT \", was: %llx, now: %llx\\n\",\n+                  affiliation_name(r->affiliation()), idx, (unsigned long long) old_tams, (unsigned long long) new_tams);\n+\n+    if ((old_tams == r->bottom()) && (new_tams > old_tams)) {\n+      log_debug(gc)(\"Clearing mark bitmap for %s Region \" SIZE_FORMAT \" while capturing TAMS\",\n+                    affiliation_name(r->affiliation()), idx);\n+\n+      clear_bitmap(r);\n+    }\n+\n+    _top_at_mark_starts_base[idx] = new_tams;\n+    if (new_tams > r->bottom()) {\n+      \/\/ In this case, new_tams is greater than old _top_bitmaps[idx]\n+      _top_bitmaps[idx] = new_tams;\n+    }\n+  }\n+  \/\/ else, FREE regions do not need their TAMS updated\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":32,"deletions":13,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -39,2 +39,0 @@\n-  \/\/ remember nworkers, coalesce_and_fill_region_array,coalesce_and_fill_regions_count\n-\n@@ -70,1 +68,1 @@\n-  ShenandoahConcurrentGC(generation), _allow_preemption(allow_preemption) {\n+    ShenandoahConcurrentGC(generation, false), _allow_preemption(allow_preemption) {\n@@ -81,0 +79,41 @@\n+\n+\/\/ Final mark for old-gen is different than for young or old, so we\n+\/\/ override the implementation.\n+void ShenandoahOldGC::op_final_mark() {\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"Should be at safepoint\");\n+  assert(!heap->has_forwarded_objects(), \"No forwarded objects on this path\");\n+\n+  if (ShenandoahVerify) {\n+    heap->verifier()->verify_roots_no_forwarded();\n+  }\n+\n+  if (!heap->cancelled_gc()) {\n+    assert(_mark.generation()->generation_mode() == OLD, \"Generation of Old-Gen GC should be OLD\");\n+    _mark.finish_mark();\n+    assert(!heap->cancelled_gc(), \"STW mark cannot OOM\");\n+\n+    \/\/ Believe notifying JVMTI that the tagmap table will need cleaning is not relevant following old-gen mark\n+    \/\/ so commenting out for now:\n+    \/\/   JvmtiTagMap::set_needs_cleaning();\n+\n+    {\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::choose_cset);\n+      ShenandoahHeapLocker locker(heap->lock());\n+      \/\/ Old-gen choose_collection_set() does not directly manipulate heap->collection_set() so no need to clear it.\n+      _generation->heuristics()->choose_collection_set(nullptr, nullptr);\n+    }\n+\n+    \/\/ Believe verification following old-gen concurrent mark needs to be different than verification following\n+    \/\/ young-gen concurrent mark, so am commenting this out for now:\n+    \/\/   if (ShenandoahVerify) {\n+    \/\/     heap->verifier()->verify_after_concmark();\n+    \/\/   }\n+\n+    if (VerifyAfterGC) {\n+      Universe::verify();\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":42,"deletions":3,"binary":false,"changes":45,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n@@ -37,0 +38,4 @@\n+\n+ protected:\n+  virtual void op_final_mark();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -233,0 +233,19 @@\n+class ShenandoahVerifyRemSetClosure : public BasicOopIterateClosure {\n+  protected:\n+  bool _init_mark;\n+  ShenandoahHeap* _heap;\n+  RememberedScanner* _scanner;\n+\n+  public:\n+\/\/ Argument distinguishes between initial mark or start of update refs verification.\n+  ShenandoahVerifyRemSetClosure(bool init_mark) :\n+      _init_mark(init_mark),\n+      _heap(ShenandoahHeap::heap()),\n+      _scanner(_heap->card_scan()) {  }\n+  template<class T>\n+  inline void work(T* p);\n+\n+  virtual void do_oop(narrowOop* p) { work(p); }\n+  virtual void do_oop(oop* p) { work(p); }\n+};\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -55,0 +55,18 @@\n+template<class T>\n+inline void ShenandoahVerifyRemSetClosure::work(T* p) {\n+  T o = RawAccess<>::oop_load(p);\n+  if (!CompressedOops::is_null(o)) {\n+    oop obj = CompressedOops::decode_not_null(o);\n+    if (_heap->is_in_young(obj)) {\n+      size_t card_index = _scanner->card_index_for_addr((HeapWord*) p);\n+      if (_init_mark && !_scanner->is_card_dirty(card_index)) {\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, NULL,\n+                                        \"Verify init-mark remembered set violation\", \"clean card should be dirty\", __FILE__, __LINE__);\n+      } else if (!_init_mark && !_scanner->is_write_card_dirty(card_index)) {\n+        ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, p, NULL,\n+                                        \"Verify init-update-refs remembered set violation\", \"clean card should be dirty\", __FILE__, __LINE__);\n+      }\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -389,0 +389,2 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  oop referent = reference_referent<T>(reference);\n@@ -390,1 +392,1 @@\n-         ShenandoahHeap::heap()->marking_context()->is_marked(reference_referent<T>(reference)), \"only drop references with alive referents\");\n+         heap->marking_context()->is_marked(reference_referent<T>(reference)), \"only drop references with alive referents\");\n@@ -395,0 +397,8 @@\n+  \/\/ When this reference was discovered, it would not have been marked. If it ends up surviving\n+  \/\/ the cycle, we need to dirty the card if the reference is old and the referent is young.  Note\n+  \/\/ that if the reference is not dropped, then its pointer to the referent will be nulled before\n+  \/\/ evacuation begins so card does not need to be dirtied.\n+  if (heap->mode()->is_generational() && heap->is_old(reference) && heap->is_in_young(referent)) {\n+    \/\/ Note: would be sufficient to mark only the card that holds the start of this Reference object.\n+    heap->card_scan()->mark_range_as_dirty(cast_from_oop<HeapWord*>(reference), reference->size());\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahReferenceProcessor.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-ShenandoahDirectCardMarkRememberedSet::ShenandoahDirectCardMarkRememberedSet(CardTable* card_table, size_t total_card_count) {\n+ShenandoahDirectCardMarkRememberedSet::ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable* card_table, size_t total_card_count) {\n@@ -107,1 +107,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -239,1 +239,1 @@\n-  CardTable *_card_table;\n+  ShenandoahCardTable *_card_table;\n@@ -250,0 +250,2 @@\n+  uint64_t _wide_clean_value;\n+\n@@ -252,1 +254,1 @@\n-  ShenandoahDirectCardMarkRememberedSet(CardTable *card_table, size_t total_card_count);\n+  ShenandoahDirectCardMarkRememberedSet(ShenandoahCardTable *card_table, size_t total_card_count);\n@@ -260,0 +262,1 @@\n+  bool is_write_card_dirty(size_t card_index);\n@@ -263,0 +266,1 @@\n+  void mark_read_card_as_clean(size_t card_index);\n@@ -280,0 +284,26 @@\n+\n+  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n+  \/\/ Not currently used because mutator write barrier does not honor changes to the location of card table.\n+  void swap_remset() {  _card_table->swap_card_tables(); }\n+\n+  HeapWord* whole_heap_base() { return _whole_heap_base; }\n+  HeapWord* whole_heap_end() { return _whole_heap_end; }\n+\n+  \/\/ Instead of swap_remset, the current implementation of concurrent remembered set scanning does reset_remset\n+  \/\/ in parallel threads, each invocation processing one entire HeapRegion at a time.  Processing of a region\n+  \/\/ consists of copying the write table to the read table and cleaning the write table.\n+  void reset_remset(HeapWord* start, size_t word_count) {\n+    size_t card_index = card_index_for_addr(start);\n+    size_t num_cards = word_count \/ CardTable::card_size_in_words;\n+    size_t iterations = num_cards \/ (sizeof (intptr_t) \/ sizeof (CardTable::CardValue));\n+    intptr_t* read_table_ptr = (intptr_t*) &(_card_table->read_byte_map())[card_index];\n+    intptr_t* write_table_ptr = (intptr_t*) &(_card_table->write_byte_map())[card_index];\n+    for (size_t i = 0; i < iterations; i++) {\n+      *read_table_ptr++ = *write_table_ptr;\n+      *write_table_ptr++ = CardTable::clean_card_row_val();\n+    }\n+  }\n+\n+  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n+  void clear_old_remset() {  _card_table->clear_read_table(); }\n+\n@@ -650,1 +680,1 @@\n-  \/\/ or concurrent evacuation phase.  An \"ideal\" time to register\n+  \/\/ or concurrent update-references phase.  An \"ideal\" time to register\n@@ -725,1 +755,9 @@\n-  \/\/ Synchronization thoughts from kelvin:\n+  \/\/ Notes on synchronization of register_object():\n+  \/\/\n+  \/\/  1. For efficiency, there is no locking in the implementation of register_object()\n+  \/\/  2. Thus, it is required that users of this service assure that concurrent\/parallel invocations of\n+  \/\/     register_object() do pertain to the same card's memory range.  See discussion below to undestand\n+  \/\/     the risks.\n+  \/\/  3. When allocating from a TLAB or GCLAB, the mutual exclusion can be guaranteed by assuring that each\n+  \/\/     LAB's start and end are aligned on card memory boundaries.\n+  \/\/  4. Use the same lock that guarantees exclusivity when performing free-list allocation within heap regions.\n@@ -727,5 +765,18 @@\n-  \/\/ previously, I had contemplated a more complex implementation of\n-  \/\/ object registration, which had to touch every card spanned by the\n-  \/\/ registered object.  But the current implementation is much simpler,\n-  \/\/ and only has to touch the card that contains the start of the\n-  \/\/ object.\n+  \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+  \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+  \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+  \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+  \/\/\n+  \/\/ objects being \"concurrently\" allocated:\n+  \/\/    [-----a------][-----b-----][--------------c------------------]\n+  \/\/            [---- card table memory range --------------]\n+  \/\/\n+  \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+  \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+  \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/\n+  \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+  \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+  \/\/ invocations associated with objects that are allocated from \"free lists\" to provide their own mutual exclusion locking\n+  \/\/ mechanism.\n@@ -733,4 +784,2 @@\n-  \/\/ if I were careful to assure that every GCLAB aligns with the start\n-  \/\/ of a card region and spanned a multiple of the card region size,\n-  \/\/ then the post-processing of each GCLAB with regards to\n-  \/\/ register_object() invocations can proceed without synchronization.\n+  \/\/ Reset the has_object() information to false for all cards in the range between from and to.\n+  void reset_object_range(HeapWord *from, HeapWord *to);\n@@ -738,5 +787,2 @@\n-  \/\/ But right now, we're not even using GCLABs  We are doing shared\n-  \/\/ allocations.  But, we must hold a lock while we are doing these, so\n-  \/\/ maybe I just piggy back on the lock that we already hold for\n-  \/\/ managing the free lists and register each object newly allocated by\n-  \/\/ the shared allocator.\n+  \/\/ register_object() requires that the caller hold the heap lock\n+  \/\/ before calling it.\n@@ -745,0 +791,7 @@\n+  \/\/ register_object_wo_lock() does not require that the caller hold\n+  \/\/ the heap lock before calling it, under the assumption that the\n+  \/\/ caller has assure no other thread will endeavor to concurrently\n+  \/\/ register objects that start within the same card's memory region\n+  \/\/ as address.\n+  void register_object_wo_lock(HeapWord* address);\n+\n@@ -755,3 +808,0 @@\n-  \/\/ In its current implementation, unregister_object() serves the needs of coalescing objects.\n-  \/\/\n-\n@@ -760,1 +810,1 @@\n-  \/\/  1. If the newly coalesced region is contained entirely within a single region, that region's last\n+  \/\/  1. If the newly coalesced range is contained entirely within a card range, that card's last\n@@ -762,1 +812,1 @@\n-  \/\/  2. For the region that holds the start of the coalesced object, it will not impact the first start\n+  \/\/  2. For the card that holds the start of the coalesced object, it will not impact the first start\n@@ -764,1 +814,1 @@\n-  \/\/  3. For following regions spanned entirely by the newly coalesced object, it will change has_object\n+  \/\/  3. For following cards spanned entirely by the newly coalesced object, it will change has_object\n@@ -766,1 +816,1 @@\n-  \/\/  4. For a following region that is spanned patially by the newly coalesced object, it may change\n+  \/\/  4. For a following card that is spanned patially by the newly coalesced object, it may change\n@@ -773,0 +823,3 @@\n+  \/\/\n+  \/\/ The role of coalesce_objects is to change the crossing map information associated with all of the coalesced\n+  \/\/ objects.\n@@ -805,8 +858,0 @@\n-\/\/ In an initial implementation, remembered set scanning happens\n-\/\/ during a HotSpot safepoint.  This greatly simplifies the\n-\/\/ implementation and improves efficiency of remembered set scanning,\n-\/\/ but this design choice increases pause times experienced at the\n-\/\/ start of concurrent marking and concurrent evacuation.  Pause times\n-\/\/ will be especially long if old-gen memory holds many pointers to\n-\/\/ young-gen memory.\n-\/\/\n@@ -877,0 +922,1 @@\n+  bool is_write_card_dirty(size_t card_index) { return _rs->is_write_card_dirty(card_index); }\n@@ -880,0 +926,1 @@\n+  void mark_read_card_as_clean(size_t card_index) { _rs->mark_read_card_clean(card_index); }\n@@ -892,0 +939,8 @@\n+  \/\/ Called by GC thread at start of concurrent mark to exchange roles of read and write remembered sets.\n+  void swap_remset() { _rs->swap_remset(); }\n+\n+  void reset_remset(HeapWord* start, size_t word_count) { _rs->reset_remset(start, word_count); }\n+\n+  \/\/ Called by GC thread after scanning old remembered set in order to prepare for next GC pass\n+  void clear_old_remset() { _rs->clear_old_remset(); }\n+\n@@ -893,0 +948,2 @@\n+\n+  void reset_object_range(HeapWord *from, HeapWord *to);\n@@ -894,0 +951,1 @@\n+  void register_object_wo_lock(HeapWord *addr);\n@@ -896,0 +954,3 @@\n+  \/\/ Return true iff this object is \"properly\" registered.\n+  bool verify_registration(HeapWord* address, size_t size_in_words);\n+\n@@ -933,0 +994,3 @@\n+  template <typename ClosureType>\n+  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops, bool use_write_table);\n+\n@@ -936,0 +1000,3 @@\n+  template <typename ClosureType>\n+  inline void process_region(ShenandoahHeapRegion* region, ClosureType *cl, bool use_write_table);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":100,"deletions":33,"binary":false,"changes":133,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/collectorCounters.hpp\"\n@@ -52,0 +53,6 @@\n+inline bool\n+ShenandoahDirectCardMarkRememberedSet::is_write_card_dirty(size_t card_index) {\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  return (bp[0] == CardTable::dirty_card_val());\n+}\n+\n@@ -54,1 +61,1 @@\n-  uint8_t *bp = &_byte_map[card_index];\n+  uint8_t *bp = &(_card_table->read_byte_map())[card_index];\n@@ -60,1 +67,1 @@\n-  uint8_t *bp = &_byte_map[card_index];\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n@@ -66,2 +73,2 @@\n-  uint8_t *bp = &_byte_map[card_index];\n-  while (num_cards-- > 0)\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n@@ -69,0 +76,1 @@\n+  }\n@@ -73,1 +81,1 @@\n-  uint8_t *bp = &_byte_map[card_index];\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n@@ -79,2 +87,2 @@\n-  uint8_t *bp = &_byte_map[card_index];\n-  while (num_cards-- > 0)\n+  uint8_t *bp = &(_card_table->write_byte_map())[card_index];\n+  while (num_cards-- > 0) {\n@@ -82,0 +90,1 @@\n+  }\n@@ -92,1 +101,2 @@\n-  uint8_t *bp = &_byte_map_base[uintptr_t(p) >> _card_shift];\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->read_byte_map())[index];\n@@ -98,1 +108,2 @@\n-  uint8_t *bp = &_byte_map_base[uintptr_t(p) >> _card_shift];\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->write_byte_map())[index];\n@@ -104,3 +115,7 @@\n-  uint8_t *bp = &_byte_map_base[uintptr_t(p) >> _card_shift];\n-  uint8_t *end_bp = &_byte_map_base[uintptr_t(p + num_heap_words) >> _card_shift];\n-  while (bp < end_bp)\n+  uint8_t *bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  uint8_t *end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to dirty last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n@@ -108,0 +123,1 @@\n+  }\n@@ -112,1 +128,8 @@\n-  uint8_t *bp = &_byte_map_base[uintptr_t(p) >> _card_shift];\n+  size_t index = card_index_for_addr(p);\n+  uint8_t *bp = &(_card_table->write_byte_map())[index];\n+  bp[0] = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_read_card_as_clean(size_t index) {\n+  uint8_t *bp = &(_card_table->read_byte_map())[index];\n@@ -118,3 +141,7 @@\n-  uint8_t *bp = &_byte_map_base[uintptr_t(p) >> _card_shift];\n-  uint8_t *end_bp = &_byte_map_base[uintptr_t(p + num_heap_words) >> _card_shift];\n-  while (bp < end_bp)\n+  uint8_t *bp = &(_card_table->write_byte_map_base())[uintptr_t(p) >> _card_shift];\n+  uint8_t *end_bp = &(_card_table->write_byte_map_base())[uintptr_t(p + num_heap_words) >> _card_shift];\n+  \/\/ If (p + num_heap_words) is not aligned on card boundary, we also need to clean last card.\n+  if (((unsigned long long) (p + num_heap_words)) & (CardTable::card_size - 1)) {\n+    end_bp++;\n+  }\n+  while (bp < end_bp) {\n@@ -122,0 +149,1 @@\n+  }\n@@ -135,0 +163,17 @@\n+\/\/ No lock required because arguments align with card boundaries.\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::reset_object_range(HeapWord* from, HeapWord* to) {\n+  assert(((((unsigned long long) from) & (CardTable::card_size - 1)) == 0) &&\n+         ((((unsigned long long) to) & (CardTable::card_size - 1)) == 0),\n+         \"reset_object_range bounds must align with card boundaries\");\n+  size_t card_at_start = _rs->card_index_for_addr(from);\n+  size_t num_cards = (to - from) \/ CardTable::card_size_in_words;\n+\n+  for (size_t i = 0; i < num_cards; i++) {\n+    object_starts[card_at_start + i] = 0;\n+  }\n+}\n+\n+\/\/ Assume only one thread at a time registers objects pertaining to\n+\/\/ each card-table entry's range of memory.\n@@ -138,0 +183,7 @@\n+  shenandoah_assert_heaplocked();\n+  register_object_wo_lock(address);\n+}\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahCardCluster<RememberedSet>::register_object_wo_lock(HeapWord* address) {\n@@ -400,0 +452,5 @@\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::reset_object_range(HeapWord *from, HeapWord *to) {\n+  _scc->reset_object_range(from, to);\n+}\n@@ -407,0 +464,86 @@\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::register_object_wo_lock(HeapWord *addr) {\n+  _scc->register_object_wo_lock(addr);\n+}\n+\n+template <typename RememberedSet>\n+inline bool\n+ShenandoahScanRemembered<RememberedSet>::verify_registration(HeapWord* address, size_t size_in_words) {\n+\n+  size_t index = card_index_for_addr(address);\n+  if (!_scc->has_object(index)) {\n+    return false;\n+  }\n+  HeapWord* base_addr = addr_for_card_index(index);\n+  size_t offset = _scc->get_first_start(index);\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (heap->doing_mixed_evacuations()) {\n+    ctx = heap->marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  \/\/ Verify that I can find this object within its enclosing card by scanning forward from first_start.\n+  while (base_addr + offset < address) {\n+    oop obj = oop(base_addr + offset);\n+    if (!ctx || ctx->is_marked(obj)) {\n+      offset += obj->size();\n+    } else {\n+      \/\/ This object is not live so don't trust its size()\n+      ShenandoahHeapRegion* r = heap->heap_region_containing(base_addr + offset);\n+      HeapWord* tams = ctx->top_at_mark_start(r);\n+      if (base_addr + offset >= tams) {\n+        offset += obj->size();\n+      } else {\n+        offset = ctx->get_next_marked_addr(base_addr + offset, tams) - base_addr;\n+      }\n+    }\n+  }\n+  if (base_addr + offset != address){\n+    return false;\n+  }\n+\n+  if (!ctx) {\n+    \/\/ Make sure that last_offset is properly set for the enclosing card, but we can't verify this for\n+    \/\/ candidate collection-set regions during mixed evacuations, so disable this check in general\n+    \/\/ during mixed evacuations.\n+    \/\/\n+    \/\/ TODO: could do some additional checking during mixed evacuations if we wanted to work harder.\n+    size_t prev_offset = offset;\n+    do {\n+      HeapWord* obj_addr = base_addr + offset;\n+      oop obj = oop(base_addr + offset);\n+      prev_offset = offset;\n+      offset += obj->size();\n+    } while (offset < CardTable::card_size_in_words);\n+    if (_scc->get_last_start(index) != prev_offset) {\n+      return false;\n+    }\n+\n+    \/\/ base + offset represents address of first object that starts on following card, if there is one.\n+\n+    \/\/ Notes: base_addr is addr_for_card_index(index)\n+    \/\/        base_addr + offset is end of the object we are verifying\n+    \/\/        cannot use card_index_for_addr(base_addr + offset) because it asserts arg < end of whole heap\n+    size_t end_card_index = index + offset \/ CardTable::card_size_in_words;\n+\n+    \/\/ If there is a following object registered, it should begin where this object ends.\n+    if ((base_addr + offset < _rs->whole_heap_end()) && _scc->has_object(end_card_index) &&\n+        ((addr_for_card_index(end_card_index) + _scc->get_first_start(end_card_index)) != (base_addr + offset))) {\n+      return false;\n+    }\n+\n+    \/\/ Assure that no other objects are registered \"inside\" of this one.\n+    for (index++; index < end_card_index; index++) {\n+      if (_scc->has_object(index)) {\n+        return false;\n+      }\n+    }\n+  }\n+\n+  return true;\n+}\n+\n@@ -425,0 +568,8 @@\n+  process_clusters(first_cluster, count, end_of_range, cl, false);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range,\n+                                                          ClosureType *cl, bool write_table) {\n@@ -430,0 +581,19 @@\n+  \/\/ If old-gen evacuation is active, then MarkingContext for old-gen heap regions is valid.  We use the MarkingContext\n+  \/\/ bits to determine which objects within a DIRTY card need to be scanned.  This is necessary because old-gen heap\n+  \/\/ regions which are in the candidate collection set have not been coalesced and filled.  Thus, these heap regions\n+  \/\/ may contain zombie objects.  Zombie objects are known to be dead, but have not yet been \"collected\".  Scanning\n+  \/\/ zombie objects is unsafe because the Klass pointer is not reliable, objects referenced from a zombie may have been\n+  \/\/ collected and their memory repurposed, and because zombie objects might refer to objects that are themselves dead.\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+  ShenandoahMarkingContext* ctx;\n+\n+  if (heap->doing_mixed_evacuations()) {\n+    ctx = heap->marking_context();\n+  } else {\n+    ctx = nullptr;\n+  }\n+\n+  HeapWord* end_of_clusters = _rs->addr_for_card_index(first_cluster)\n+    + count * ShenandoahCardCluster<RememberedSet>::CardsPerCluster * CardTable::card_size_in_words;\n@@ -433,1 +603,0 @@\n-\n@@ -437,2 +606,1 @@\n-\n-      bool is_dirty = _rs->is_card_dirty(card_index);\n+      bool is_dirty = (write_table)? is_write_card_dirty(card_index): is_card_dirty(card_index);\n@@ -440,1 +608,0 @@\n-\n@@ -442,0 +609,1 @@\n+        size_t prev_card_index = card_index;\n@@ -464,10 +632,20 @@\n-            \/\/ Future TODO:\n-            \/\/ For improved efficiency, we might want to give special handling of obj->is_objArray().  In\n-            \/\/ particular, in that case, we might want to divide the effort for scanning of a very long object array\n-            \/\/ between multiple threads.\n-            if (obj->is_objArray()) {\n-              objArrayOop array = objArrayOop(obj);\n-              int len = array->length();\n-              array->oop_iterate_range(cl, 0, len);\n-            } else if (obj->is_instance()) {\n-              obj->oop_iterate(cl);\n+            \/\/ ctx->is_marked() returns true if mark bit set or if obj above TAMS.\n+            if (!ctx || ctx->is_marked(obj)) {\n+              \/\/ Future TODO:\n+              \/\/ For improved efficiency, we might want to give special handling of obj->is_objArray().  In\n+              \/\/ particular, in that case, we might want to divide the effort for scanning of a very long object array\n+              \/\/ between multiple threads.\n+              if (obj->is_objArray()) {\n+                objArrayOop array = objArrayOop(obj);\n+                int len = array->length();\n+                array->oop_iterate_range(cl, 0, len);\n+              } else if (obj->is_instance()) {\n+                obj->oop_iterate(cl);\n+              } else {\n+                \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n+                \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n+                \/\/ We skip iterating over the klass pointer since we know that\n+                \/\/ Universe::TypeArrayKlass never moves.\n+                assert (obj->is_typeArray(), \"should be type array\");\n+              }\n+              p += obj->size();\n@@ -475,5 +653,8 @@\n-              \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n-              \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n-              \/\/ We skip iterating over the klass pointer since we know that\n-              \/\/ Universe::TypeArrayKlass never moves.\n-              assert (obj->is_typeArray(), \"should be type array\");\n+              \/\/ This object is not marked so we don't scan it.\n+              ShenandoahHeapRegion* r = heap->heap_region_containing(p);\n+              HeapWord* tams = ctx->top_at_mark_start(r);\n+              if (p >= tams) {\n+                p += obj->size();\n+              } else {\n+                p = ctx->get_next_marked_addr(p, tams);\n+              }\n@@ -481,1 +662,0 @@\n-            p += obj->size();\n@@ -483,1 +663,1 @@\n-          if (p > endp)\n+          if (p > endp) {\n@@ -485,1 +665,1 @@\n-          else                  \/\/ p == endp\n+          } else {                  \/\/ p == endp\n@@ -487,0 +667,1 @@\n+          }\n@@ -488,1 +669,1 @@\n-          \/\/ otherwise, this card will have been scanned during scan of a previous cluster.\n+          \/\/ Card is dirty but has no object.  Card will have been scanned during scan of a previous cluster.\n@@ -492,0 +673,1 @@\n+        \/\/ Card is clean but has object.\n@@ -499,1 +681,0 @@\n-        HeapWord *nextp = p + obj->size();\n@@ -501,3 +682,3 @@\n-        \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n-        \/\/ failure if nextp points to end of heap.\n-        size_t last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words;\n+        size_t last_card;\n+        if (!ctx || ctx->is_marked(obj)) {\n+          HeapWord *nextp = p + obj->size();\n@@ -505,2 +686,3 @@\n-        bool reaches_next_cluster = (last_card > end_card_index);\n-        bool spans_dirty_within_this_cluster = false;\n+          \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n+          \/\/ failure if nextp points to end of heap.\n+          last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words;\n@@ -508,8 +690,2 @@\n-        if (!reaches_next_cluster) {\n-          size_t span_card;\n-          for (span_card = card_index+1; span_card <= last_card; span_card++)\n-            if (_rs->is_card_dirty(span_card)) {\n-              spans_dirty_within_this_cluster = true;\n-              break;\n-            }\n-        }\n+          bool reaches_next_cluster = (last_card > end_card_index);\n+          bool spans_dirty_within_this_cluster = false;\n@@ -517,7 +693,32 @@\n-        if (reaches_next_cluster || spans_dirty_within_this_cluster) {\n-          if (obj->is_objArray()) {\n-            objArrayOop array = objArrayOop(obj);\n-            int len = array->length();\n-            array->oop_iterate_range(cl, 0, len);\n-          } else if (obj->is_instance()) {\n-            obj->oop_iterate(cl);\n+          if (!reaches_next_cluster) {\n+            size_t span_card;\n+            for (span_card = card_index+1; span_card <= last_card; span_card++)\n+              if ((write_table)? _rs->is_write_card_dirty(span_card): _rs->is_card_dirty(span_card)) {\n+                spans_dirty_within_this_cluster = true;\n+                break;\n+              }\n+          }\n+\n+          if (reaches_next_cluster || spans_dirty_within_this_cluster) {\n+            if (obj->is_objArray()) {\n+              objArrayOop array = objArrayOop(obj);\n+              int len = array->length();\n+              array->oop_iterate_range(cl, 0, len);\n+            } else if (obj->is_instance()) {\n+              obj->oop_iterate(cl);\n+            } else {\n+              \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n+              \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n+              \/\/ We skip iterating over the klass pointer since we know that\n+              \/\/ Universe::TypeArrayKlass never moves.\n+              assert (obj->is_typeArray(), \"should be type array\");\n+            }\n+          }\n+        } else {\n+          \/\/ The object that spans end of this clean card is not marked, so no need to scan it or its\n+          \/\/ unmarked neighbors.\n+          ShenandoahHeapRegion* r = heap->heap_region_containing(p);\n+          HeapWord* tams = ctx->top_at_mark_start(r);\n+          HeapWord* nextp;\n+          if (p >= tams) {\n+            nextp = p + obj->size();\n@@ -525,5 +726,1 @@\n-            \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n-            \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n-            \/\/ We skip iterating over the klass pointer since we know that\n-            \/\/ Universe::TypeArrayKlass never moves.\n-            assert (obj->is_typeArray(), \"should be type array\");\n+            nextp = ctx->get_next_marked_addr(p, tams);\n@@ -531,0 +728,1 @@\n+          last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words;\n@@ -535,0 +733,1 @@\n+        \/\/ Card is clean and has no object.  No need to clean this card.\n@@ -545,0 +744,7 @@\n+  process_region(region, cl, false);\n+}\n+\n+template<typename RememberedSet>\n+template <typename ClosureType>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::process_region(ShenandoahHeapRegion *region, ClosureType *cl, bool use_write_table) {\n@@ -553,1 +759,0 @@\n-  HeapWord *end_of_range = region->top();\n@@ -555,1 +760,13 @@\n-  \/\/ end_of_range may point to the middle of a cluster because region->top() may be different than region->end.\n+  HeapWord *end_of_range;\n+  if (use_write_table) {\n+    \/\/ This is update-refs servicing.\n+    end_of_range = region->get_update_watermark();\n+  } else {\n+    \/\/ This is concurrent mark servicing.  Note that TAMS for this region is TAMS at start of old-gen\n+    \/\/ collection.  Here, we need to scan up to TAMS for most recently initiated young-gen collection.\n+    \/\/ Since all LABs are retired at init mark, and since replacement LABs are allocated lazily, and since no\n+    \/\/ promotions occur until evacuation phase, TAMS for most recent young-gen is same as top().\n+    end_of_range = region->top();\n+  }\n+\n+  \/\/ end_of_range may point to the middle of a cluster because region->top() may be different than region->end().\n@@ -560,2 +777,1 @@\n-  unsigned int cluster_size = CardTable::card_size_in_words *\n-    ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  unsigned int cluster_size = CardTable::card_size_in_words * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n@@ -565,1 +781,1 @@\n-  process_clusters(start_cluster_no, num_clusters, end_of_range, cl);\n+  process_clusters(start_cluster_no, num_clusters, end_of_range, cl, use_write_table);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":286,"deletions":70,"binary":false,"changes":356,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+  const bool _do_old_gc_bootstrap;\n@@ -63,1 +64,1 @@\n-  VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc) :\n+  VM_ShenandoahInitMark(ShenandoahConcurrentGC* gc, bool do_old_gc_bootstrap) :\n@@ -65,1 +66,2 @@\n-    _gc(gc) {};\n+    _gc(gc),\n+    _do_old_gc_bootstrap(do_old_gc_bootstrap) {};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVMOperations.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -661,0 +661,1 @@\n+                                             VerifyRememberedSet remembered,\n@@ -690,0 +691,4 @@\n+      case _verify_gcstate_updating:\n+        enabled = true;\n+        expected = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::UPDATEREFS;\n+        break;\n@@ -746,0 +751,6 @@\n+    if (remembered == _verify_remembered_for_marking) {\n+      _heap->verify_rem_set_at_mark();\n+    } else if (remembered == _verify_remembered_for_updating_references) {\n+      _heap->verify_rem_set_at_update_ref();\n+    }\n+\n@@ -850,0 +861,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -863,0 +875,1 @@\n+          _verify_remembered_for_marking,  \/\/ verify read-only remembered set from bottom() to top()\n@@ -876,0 +889,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -894,0 +908,1 @@\n+          _verify_remembered_disable,                \/\/ do not verify remembered set\n@@ -912,0 +927,1 @@\n+          _verify_remembered_disable, \/\/ do not verify remembered set\n@@ -925,0 +941,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -938,6 +955,7 @@\n-          _verify_forwarded_allow,     \/\/ forwarded references allowed\n-          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n-          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n-          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n-          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n-          _verify_gcstate_forwarded,   \/\/ evacuation should have produced some forwarded objects\n+          _verify_remembered_for_updating_references,  \/\/ do not verify remembered set\n+          _verify_forwarded_allow,                     \/\/ forwarded references allowed\n+          _verify_marked_complete,                     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n+          _verify_cset_forwarded,                      \/\/ all cset refs are fully forwarded\n+          _verify_liveness_disable,                    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash,                     \/\/ trash regions have been recycled already\n+          _verify_gcstate_updating,                    \/\/ evacuation is done, objects are forwarded, updating in process\n@@ -951,0 +969,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -964,0 +983,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -977,0 +997,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n@@ -990,0 +1011,1 @@\n+          _verify_remembered_disable,  \/\/ do not verify remembered set\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":28,"deletions":6,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -60,0 +60,14 @@\n+  typedef enum {\n+    \/\/ Disable remembered set verification.\n+    _verify_remembered_disable,\n+\n+    \/\/ Assure remembered set cards are dirty for every interesting pointer within\n+    \/\/ each ShenandoahHeapRegion between bottom() and top().  This is appropriate at\n+    \/\/ the init_mark safepoint since all TLABS are retired before we reach this code.\n+    _verify_remembered_for_marking,\n+\n+    \/\/ Assure remembered set cards are dirty for every interesting pointer within\n+    \/\/ each ShenandoahHeapRegion between bottom() and get_update_watermark()\n+    _verify_remembered_for_updating_references\n+  } VerifyRememberedSet;\n+\n@@ -136,1 +150,4 @@\n-    _verify_gcstate_evacuation\n+    _verify_gcstate_evacuation,\n+\n+    \/\/ Evacuation is done, objects are forwarded, updating is in progress\n+    _verify_gcstate_updating\n@@ -170,0 +187,1 @@\n+                           VerifyRememberedSet remembered,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":19,"deletions":1,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -34,2 +34,0 @@\n-#undef TRACE_PROMOTION\n-\n@@ -72,3 +70,4 @@\n-        \/\/ The above condition filtered out humongous continuations, among other states.\n-        \/\/ Here we rely on promote() below promoting related continuation regions when encountering a homongous start.\n-        size_t promoted = r->promote();\n+        \/\/ The thread that first encounters a humongous start region promotes the associated humonogous continuations,\n+        \/\/ so we do not process humongous continuations directly.  Below, we rely on promote() to promote related\n+        \/\/ continuation regions when encountering a homongous start.\n+        size_t promoted = r->promote(false);\n@@ -98,1 +97,1 @@\n-      r->promote();\n+      r->promote(true);\n@@ -103,6 +102,0 @@\n-\n-  \/\/ HEY! Better to use a service of ShenandoahScanRemembered for the following.\n-\n-  \/\/ We can clear the entire card table here because we've just promoted all\n-  \/\/ young regions to old, so there can be no old->young pointers at this point.\n-  ShenandoahBarrierSet::barrier_set()->card_table()->clear();\n@@ -112,0 +105,1 @@\n+  \/\/ TODO: why not test for equals YOUNG_GENERATION?  As written, returns true for regions that are FREE\n@@ -116,0 +110,1 @@\n+  \/\/ Just iterate over the young generation here.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":7,"deletions":12,"binary":false,"changes":19,"status":"modified"}]}