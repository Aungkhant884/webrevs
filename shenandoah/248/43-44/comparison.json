{"files":[{"patch":"@@ -111,0 +111,7 @@\n+\n+  \/\/ Compare by live is used to prioritize compaction of old-gen regions.  With old-gen compaction, the goal is\n+  \/\/ to tightly pack long-lived objects into available regions.  In most cases, there has not been an accumulation\n+  \/\/ of garbage within old-gen regions.  The more likely opportunity will be to combine multiple sparsely populated\n+  \/\/ old-gen regions which may have been promoted in place into a smaller number of densely packed old-gen regions.\n+  \/\/ This improves subsequent allocation efficiency and reduces the likelihood of allocation failure (including\n+  \/\/ humongous allocation failure) due to fragmentation of the available old-gen allocation pool\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -71,0 +71,18 @@\n+  size_t unfragmented_available = heap->old_generation()->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  size_t fragmented_available;\n+  size_t excess_fragmented_available;\n+\n+  if (unfragmented_available > old_evacuation_budget) {\n+    unfragmented_available = old_evacuation_budget;\n+    fragmented_available = 0;\n+    excess_fragmented_available = 0;\n+  } else {\n+    assert(heap->old_generation()->available() > old_evacuation_budget, \"Cannot budget more than is available\");\n+    fragmented_available = heap->old_generation()->available() - unfragmented_available;\n+    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n+    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n+      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n+      fragmented_available -= excess_fragmented_available;\n+    }\n+  }\n+\n@@ -77,1 +95,3 @@\n-  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates()\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n+  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n+  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n@@ -85,3 +105,3 @@\n-    \/\/ If we choose region r to be collected, then we need to decrease the capacity to hold other evacuations by\n-    \/\/ the size of r's free memory.\n-    size_t lost_evacuation_capacity = r->free();\n+    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n+    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n+\n@@ -89,8 +109,30 @@\n-    if (live_data_for_evacuation + lost_evacuation_capacity <= remaining_old_evacuation_budget) {\n-      \/\/ Decrement remaining evacuation budget by bytes that will be copied.\n-      remaining_old_evacuation_budget -= (live_data_for_evacuation + lost_evacuation_capacity);\n-      collection_set->add_region(r);\n-      included_old_regions++;\n-      evacuated_old_bytes += live_data_for_evacuation;\n-      collected_old_bytes += r->garbage();\n-      consume_old_collection_candidate();\n+    size_t lost_available = r->free();\n+\n+    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n+      if (lost_available < excess_fragmented_available) {\n+        excess_fragmented_available -= lost_available;\n+        lost_available  = 0;\n+      } else {\n+        lost_available -= excess_fragmented_available;\n+        excess_fragmented_available = 0;\n+      }\n+    }\n+    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n+    if ((lost_available > 0) && (fragmented_available > 0)) {\n+      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n+        fragmented_available -= scaled_loss;\n+        scaled_loss = 0;\n+      } else {\n+        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n+        \/\/ to decrement scaled_loss\n+      }\n+    }\n+    if (scaled_loss > 0) {\n+      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n+      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n+      if (live_data_for_evacuation > unfragmented_available) {\n+        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n+        break;\n+      } else {\n+        unfragmented_available -= live_data_for_evacuation;\n+      }\n@@ -98,1 +140,18 @@\n-      break;\n+      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n+      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n+      size_t evacuation_need = live_data_for_evacuation;\n+\n+      if (evacuation_need > fragmented_available) {\n+        evacuation_need -= fragmented_available;\n+        fragmented_available = 0;\n+      } else {\n+        fragmented_available -= evacuation_need;\n+        evacuation_need = 0;\n+      }\n+      if (evacuation_need > unfragmented_available) {\n+        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n+        break;\n+      } else {\n+        unfragmented_available -= evacuation_need;\n+        \/\/ dead code: evacuation_need == 0;  \n+      }\n@@ -100,0 +159,5 @@\n+    collection_set->add_region(r);\n+    included_old_regions++;\n+    evacuated_old_bytes += live_data_for_evacuation;\n+    collected_old_bytes += r->garbage();\n+    consume_old_collection_candidate();\n@@ -106,1 +170,1 @@\n-\n+  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n@@ -286,0 +350,2 @@\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n@@ -287,1 +353,1 @@\n-  const size_t garbage_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold \/ 100;\n+  const size_t garbage_threshold = region_size_bytes * ShenandoahOldGarbageThreshold \/ 100;\n@@ -290,1 +356,1 @@\n-  const size_t live_threshold = ShenandoahHeapRegion::region_size_bytes() - garbage_threshold;\n+  const size_t live_threshold = region_size_bytes - garbage_threshold;\n@@ -322,1 +388,2 @@\n-\n+  size_t mixed_evac_live = old_candidates * region_size_bytes - (candidates_garbage + unfragmented);\n+  set_unprocessed_old_collection_candidates_live_memory(mixed_evac_live);\n@@ -330,0 +397,14 @@\n+size_t ShenandoahOldHeuristics::unprocessed_old_collection_candidates_live_memory() const {\n+  return _live_bytes_in_unprocessed_candidates;\n+}\n+\n+void ShenandoahOldHeuristics::set_unprocessed_old_collection_candidates_live_memory(size_t initial_live) {\n+  _live_bytes_in_unprocessed_candidates = initial_live;\n+}\n+\n+void ShenandoahOldHeuristics::decrease_unprocessed_old_collection_candidates_live_memory(size_t evacuated_live) {\n+  assert(evacuated_live <= _live_bytes_in_unprocessed_candidates, \"Cannot evacuate more than was present\");\n+  _live_bytes_in_unprocessed_candidates -= evacuated_live;\n+}\n+\n+\n@@ -392,0 +473,5 @@\n+void ShenandoahOldHeuristics::trigger_old_has_grown() {\n+  _growth_trigger = true;\n+}\n+\n+\n@@ -436,0 +522,2 @@\n+    \/\/ Growth may be falsely triggered during mixed evacuations, before the mixed-evacuation candidates have been\n+    \/\/ evacuated.  Before acting on a false trigger, we check to confirm the trigger condition is still satisfied.\n@@ -439,7 +527,12 @@\n-    size_t live_at_previous_old = old_gen->get_live_bytes_after_last_mark();\n-    double percent_growth = 100.0 * ((double) current_usage - live_at_previous_old) \/ live_at_previous_old;\n-    log_info(gc)(\"Trigger (OLD): Old has overgrown, live at end of previous OLD marking: \"\n-                 SIZE_FORMAT \"%s, current usage: \" SIZE_FORMAT \"%s, percent growth: %.1f%%\",\n-                 byte_size_in_proper_unit(live_at_previous_old), proper_unit_for_byte_size(live_at_previous_old),\n-                 byte_size_in_proper_unit(current_usage), proper_unit_for_byte_size(current_usage), percent_growth);\n-    return true;\n+    size_t trigger_threshold = old_gen->usage_trigger_threshold();\n+    if (current_usage > trigger_threshold) {\n+      size_t live_at_previous_old = old_gen->get_live_bytes_after_last_mark();\n+      double percent_growth = 100.0 * ((double) current_usage - live_at_previous_old) \/ live_at_previous_old;\n+      log_info(gc)(\"Trigger (OLD): Old has overgrown, live at end of previous OLD marking: \"\n+                   SIZE_FORMAT \"%s, current usage: \" SIZE_FORMAT \"%s, percent growth: %.1f%%\",\n+                   byte_size_in_proper_unit(live_at_previous_old), proper_unit_for_byte_size(live_at_previous_old),\n+                   byte_size_in_proper_unit(current_usage), proper_unit_for_byte_size(current_usage), percent_growth);\n+      return true;\n+    } else {\n+      _growth_trigger = false;\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":117,"deletions":24,"binary":false,"changes":141,"status":"modified"},{"patch":"@@ -72,0 +72,3 @@\n+  \/\/ How much live data must be evacuated from within the unprocessed mixed evacuation candidates?\n+  size_t _live_bytes_in_unprocessed_candidates;\n+\n@@ -106,0 +109,7 @@\n+  \/\/ How much live memory must be evacuated from within old-collection candidates that have not yet been processed?\n+  size_t unprocessed_old_collection_candidates_live_memory() const;\n+\n+  void set_unprocessed_old_collection_candidates_live_memory(size_t initial_live);\n+\n+  void decrease_unprocessed_old_collection_candidates_live_memory(size_t evacuated_live);\n+\n@@ -131,3 +141,3 @@\n-  \/\/ Notify the heuristic of promotion failures. The promotion attempt will be skipped and the object will\n-  \/\/ be evacuated into the young generation. The collection should complete normally, but we want to schedule\n-  \/\/ an old collection as soon as possible.\n+  \/\/ Promotion failure does not currently trigger old-gen collections.  Often, promotion failures occur because\n+  \/\/ old-gen is sized too small rather than because it is necessary to collect old gen.  We keep the method\n+  \/\/ here in case we decide to feed this signal to sizing or triggering heuristics in the future.\n@@ -138,1 +148,2 @@\n-  void trigger_old_has_grown() { _growth_trigger = true; }\n+  void trigger_old_has_grown();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -455,6 +455,1 @@\n-  \/\/ TODO: the region that has been promoted in place may have been previously identified as is_collector_free or\n-  \/\/ is_mutator_free.  When we restructure the implementation of ShenandoahFreeSet to give special handling to\n-  \/\/ is_old_collector_free, we should also enforce that the region to be promoted, which is YOUNG and has no\n-  \/\/ available memory after its promote-in-place-pad has been inserted above original top, is identified as neither\n-  \/\/ is_mutator_free nor is_collector_free nor is_old_collector_free.\n-\n+  size_t capacity = alloc_capacity(region);\n@@ -462,2 +457,3 @@\n-  if (alloc_capacity(region) >= PLAB::min_size() * HeapWordSize) {\n-    _free_sets.make_free(idx, OldCollector, alloc_capacity(region));\n+  if (capacity >= PLAB::min_size() * HeapWordSize) {\n+    _free_sets.make_free(idx, OldCollector, capacity);\n+    _heap->augment_promo_reserve(capacity);\n@@ -600,7 +596,5 @@\n-      \/\/ TODO:\n-      \/\/ if (!allow_new_region && req.is_old() && (young_generation->adjusted_unaffiliated_regions() > 0)) {\n-      \/\/   transfer a region from young to old;\n-      \/\/   allow_new_region = true;\n-      \/\/   heap->set_old_evac_reserve(heap->get_old_evac_reserve() + region_size_bytes);\n-      \/\/ }\n-      \/\/\n+      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+        \/\/ This allows us to flip a mutator region to old_collector\n+        allow_new_region = true;\n+      }\n+\n@@ -997,0 +991,1 @@\n+  \/\/ Note: can_allocate_from(r) means r is entirely empty\n@@ -1002,3 +997,3 @@\n-\n-  \/\/ We do not ensure that the region is no longer trash,\n-  \/\/ relying on try_allocate_in(), which always comes next,\n+  _heap->generation_sizer()->force_transfer_to_old(1);\n+  _heap->augment_old_evac_reserve(region_capacity);\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n@@ -1018,2 +1013,1 @@\n-  \/\/ We do not ensure that the region is no longer trash,\n-  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n@@ -1039,1 +1033,0 @@\n-  young_cset_regions = 0;\n@@ -1041,1 +1034,1 @@\n-\n+  young_cset_regions = 0;\n@@ -1047,1 +1040,1 @@\n-\told_cset_regions++;\n+        old_cset_regions++;\n@@ -1049,2 +1042,2 @@\n-\tassert(region->is_young(), \"Trashed region should be old or young\");\n-\tyoung_cset_regions++;\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n@@ -1053,1 +1046,0 @@\n-\n@@ -1078,1 +1070,1 @@\n-\/\/ Return the amount of young-gen memory that is about to be reycled\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n@@ -1080,1 +1072,0 @@\n-  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":19,"deletions":28,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -1206,2 +1206,3 @@\n-    \/\/ only have to evacuate the live memory within mixed candidate.\n-    size_t max_evac_need = (size_t) (mixed_candidates * region_size_bytes * ShenandoahOldEvacWaste);\n+    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n+    size_t max_evac_need = (size_t)\n+      (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n@@ -1230,1 +1231,0 @@\n-\n@@ -1256,1 +1256,2 @@\n-    \/\/ If we're running short on young-gen memory, limit the xfer\n+    \/\/ If we're running short on young-gen memory, limit the xfer.  Old-gen collection activities will be curtailed\n+    \/\/ if the budget is smaller than desired.\n@@ -1258,1 +1259,0 @@\n-    \/\/ old-gen collection activities will be curtailed if the budget is smaller than desired.\n@@ -3067,0 +3067,1 @@\n+#ifdef KELVIN_DEPRECATE\n@@ -3081,0 +3082,2 @@\n+      \/\/ usage, affiliated region counts, and humongous waste are now accounted when the regions are promoted\n+\n@@ -3085,0 +3088,1 @@\n+\n@@ -3086,0 +3090,1 @@\n+        \/\/ Regions that were promoted in place were transferred at the time they were promoted.\n@@ -3089,0 +3094,3 @@\n+\n+      \/\/ usage, affiliated region counts, and humongous waste are now accounted when the regions are promoted\n+\n@@ -3093,0 +3101,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":14,"deletions":5,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -461,0 +461,1 @@\n+  inline void augment_promo_reserve(size_t increment);\n@@ -470,0 +471,1 @@\n+  inline void augment_old_evac_reserve(size_t increment);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -757,0 +757,8 @@\n+inline void ShenandoahHeap::augment_old_evac_reserve(size_t increment) {\n+  _old_evac_reserve += increment;\n+}\n+\n+inline void ShenandoahHeap::augment_promo_reserve(size_t increment) {\n+  _promoted_reserve += increment;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -970,0 +970,2 @@\n+#ifdef KELVIN_DEPRECATE\n+  \/\/ my plan is to get rid of the defer_affiliated_region_count_updates argument so I can immediately update\n@@ -971,0 +973,1 @@\n+#endif\n@@ -976,1 +979,0 @@\n-\n@@ -989,1 +991,1 @@\n-               \"Number of young regions cannot exceed adjusted capacity\");\n+               \"Number of young regions cannot exceed capacity\");\n@@ -997,1 +999,1 @@\n-               \"Number of old regions cannot exceed adjusted capacity\");\n+               \"Number of old regions cannot exceed capacity\");\n@@ -1003,0 +1005,1 @@\n+#ifdef KELVIN_DEPRECATE\n@@ -1006,0 +1009,1 @@\n+#endif\n@@ -1024,0 +1028,1 @@\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -1028,2 +1033,0 @@\n-    set_affiliation(OLD_GENERATION, true);\n-\n@@ -1036,0 +1039,3 @@\n+    size_t region_capacity = free();\n+    size_t region_used = used();\n+\n@@ -1040,6 +1046,0 @@\n-    size_t promoted_used = this->used();\n-    size_t promoted_free = this->free();\n-    size_t promo_reserve = heap->get_promoted_reserve() + promoted_free;\n-    young_gen->decrease_used(promoted_used);\n-    young_gen->decrement_affiliated_region_count();\n-\n@@ -1052,3 +1052,0 @@\n-    \/\/\n-    \/\/ Note that we will rebalance the generation sizes at the end of this GC cycle.\n-    heap->generation_sizer()->force_transfer_to_old(1);\n@@ -1056,1 +1053,1 @@\n-    heap->free_set()->add_old_collector_free_region(this);\n+    young_gen->decrease_used(region_used);\n@@ -1058,2 +1055,2 @@\n-    old_gen->increment_affiliated_region_count();\n-    old_gen->increase_used(promoted_used);\n+    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n+    heap->generation_sizer()->force_transfer_to_old(1);\n@@ -1061,3 +1058,3 @@\n-    \/\/ Might as well make the free memory within newly promoted region available to hold promotions that we were not able\n-    \/\/ to budget for previously.\n-    heap->set_promoted_reserve(promo_reserve);\n+    \/\/ set_affiliation() increments affiliated_regions for OLD, decrements for YOUNG\n+    set_affiliation(OLD_GENERATION, true);\n+    old_gen->increase_used(region_used);\n@@ -1065,1 +1062,2 @@\n-    \/\/ TODO: adjust bounds in the free set\n+    \/\/ add_old_collector_free_region() increases promoted_reserve() if available space exceeds PLAB::min_size()\n+    heap->free_set()->add_old_collector_free_region(this);\n@@ -1117,1 +1115,3 @@\n-  size_t spanned_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+  size_t used_bytes = obj->size() * HeapWordSize;\n+  size_t spanned_regions = ShenandoahHeapRegion::required_regions(used_bytes);\n+  size_t humongous_waste = spanned_regions * ShenandoahHeapRegion::region_size_bytes() - obj->size() * HeapWordSize;\n@@ -1128,0 +1128,6 @@\n+    young_generation->decrease_used(used_bytes);\n+    young_generation->decrease_humongous_waste(humongous_waste);\n+\n+    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n+    heap->generation_sizer()->force_transfer_to_old(spanned_regions);\n+\n@@ -1138,0 +1144,3 @@\n+\n+    old_generation->increase_used(used_bytes);\n+    old_generation->increase_humongous_waste(humongous_waste);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":31,"deletions":22,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -932,0 +932,4 @@\n+#ifdef KELVIN_DEPRECATE\n+    \/\/ I think I can also deprecate second argument to validate_usage:\n+    \/\/ that is always false\n+\n@@ -936,1 +940,3 @@\n-    } else if  (sizeness == _verify_size_adjusted_for_padding) {\n+    } else \n+#endif\n+    if (sizeness == _verify_size_adjusted_for_padding) {\n@@ -940,2 +946,1 @@\n-    }\n-    else if (sizeness == _verify_size_exact) {\n+    } else if (sizeness == _verify_size_exact) {\n@@ -1065,2 +1070,3 @@\n-            \"Before Mark\",\n-            _verify_remembered_before_marking,  \/\/ verify read-only remembered set from bottom() to top()\n+          \"Before Mark\",\n+          _verify_remembered_before_marking,\n+                                       \/\/ verify read-only remembered set from bottom() to top()\n@@ -1082,1 +1088,2 @@\n-          _verify_marked_complete_except_references, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n+          _verify_marked_complete_except_references,\n+                                       \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n@@ -1129,2 +1136,1 @@\n-          _verify_size_adjusted_for_deferred_accounting,\n-                                       \/\/ expect generation and heap sizes to match after adjustments for promote in place\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -1139,9 +1145,7 @@\n-          _verify_forwarded_allow,                     \/\/ forwarded references allowed\n-          _verify_marked_complete,                     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n-          _verify_cset_forwarded,                      \/\/ all cset refs are fully forwarded\n-          _verify_liveness_disable,                    \/\/ no reliable liveness data anymore\n-          _verify_regions_notrash,                     \/\/ trash regions have been recycled already\n-          _verify_size_adjusted_for_deferred_accounting,\n-                                                       \/\/ expect generation and heap sizes to match after adjustments\n-                                                       \/\/  for promote in place\n-          _verify_gcstate_updating                     \/\/ evacuation should have produced some forwarded objects\n+          _verify_forwarded_allow,     \/\/ forwarded references allowed\n+          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n+          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n+          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_updating     \/\/ evacuation should have produced some forwarded objects\n@@ -1161,2 +1165,1 @@\n-          _verify_size_adjusted_for_deferred_accounting,\n-                                       \/\/ expect generation and heap sizes to match after adjustments for promote in place\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -1176,1 +1179,1 @@\n-          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":23,"deletions":20,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -153,1 +153,1 @@\n-\n+#ifdef KELVIN_DEPRECATE\n@@ -156,0 +156,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -296,1 +296,1 @@\n-  product(double, ShenandoahOldEvacWaste, 1.6, EXPERIMENTAL,                \\\n+  product(double, ShenandoahOldEvacWaste, 1.4, EXPERIMENTAL,                \\\n@@ -305,1 +305,1 @@\n-          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"How much waste promotions produce within the reserved space. \"   \\\n@@ -307,1 +307,1 @@\n-          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"evacuation conflicts, at expense of promoting less on each \"     \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"}]}