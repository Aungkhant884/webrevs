{"files":[{"patch":"@@ -33,0 +33,2 @@\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -59,3 +61,0 @@\n-\/\/ TODO: Provide comment here or remove if not used\n-const uint ShenandoahAdaptiveHeuristics::MINIMUM_RESIZE_INTERVAL = 10;\n-\n@@ -113,0 +112,17 @@\n+    for (size_t idx = 0; idx < size; idx++) {\n+      ShenandoahHeapRegion* r = data[idx]._region;\n+      if (cset->is_preselected(r->index())) {\n+        assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n+        \/\/ Entire region will be promoted, This region does not impact young-gen or old-gen evacuation reserve.\n+        \/\/ This region has been pre-selected and its impact on promotion reserve is already accounted for.\n+\n+        \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n+        \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n+        \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n+        \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n+        \/\/ within youn-gen memory.\n+\n+        cur_young_garbage += r->garbage();\n+        cset->add_region(r);\n+      }\n+    }\n@@ -116,1 +132,1 @@\n-      size_t max_old_cset    = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n+      size_t max_old_cset    = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n@@ -129,0 +145,3 @@\n+        if (cset->is_preselected(r->index())) {\n+          continue;\n+        }\n@@ -136,11 +155,0 @@\n-        } else if (cset->is_preselected(r->index())) {\n-          assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n-          \/\/ Entire region will be promoted, This region does not impact young-gen or old-gen evacuation reserve.\n-          \/\/ This region has been pre-selected and its impact on promotion reserve is already accounted for.\n-          add_region = true;\n-          \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n-          \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n-          \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n-          \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n-          \/\/ within youn-gen memory.\n-          cur_young_garbage += r->used();\n@@ -179,28 +187,12 @@\n-        bool add_region = false;\n-\n-        if (!r->is_old()) {\n-          if (cset->is_preselected(r->index())) {\n-            assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n-            \/\/ Entire region will be promoted, This region does not impact young-gen evacuation reserve.  Memory has already\n-            \/\/ been set aside to hold evacuation results as advance_promotion_reserve.\n-            add_region = true;\n-            \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n-            \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n-            \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n-            \/\/ within youn-gen memory\n-            cur_young_garbage += r->get_live_data_bytes();\n-          } else if  (r->age() < InitialTenuringThreshold) {\n-            size_t new_cset = cur_cset + r->get_live_data_bytes();\n-            size_t region_garbage = r->garbage();\n-            size_t new_garbage = cur_young_garbage + region_garbage;\n-            bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-            if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-              add_region = true;\n-              cur_cset = new_cset;\n-              cur_young_garbage = new_garbage;\n-            }\n-          }\n-          \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n-          \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects.\n-\n-          if (add_region) {\n+        if (cset->is_preselected(r->index())) {\n+          continue;\n+        }\n+        if  (r->age() < InitialTenuringThreshold) {\n+          size_t new_cset = cur_cset + r->get_live_data_bytes();\n+          size_t region_garbage = r->garbage();\n+          size_t new_garbage = cur_young_garbage + region_garbage;\n+          bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+          assert(r->is_young(), \"Only young candidates expected in the data array\");\n+          if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+            cur_cset = new_cset;\n+            cur_young_garbage = new_garbage;\n@@ -210,0 +202,3 @@\n+        \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n+        \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects or because\n+        \/\/ they are to be promoted in place.\n@@ -214,1 +209,1 @@\n-    size_t capacity    = ShenandoahHeap::heap()->soft_max_capacity();\n+    size_t capacity    = ShenandoahHeap::heap()->max_capacity();\n@@ -246,0 +241,10 @@\n+\n+  size_t collected_old = cset->get_old_bytes_reserved_for_evacuation();\n+  size_t collected_promoted = cset->get_young_bytes_to_be_promoted();\n+  size_t collected_young = cset->get_young_bytes_reserved_for_evacuation();\n+\n+  log_info(gc, ergo)(\"Chosen CSet evacuates young: \" SIZE_FORMAT \"%s (of which at least: \" SIZE_FORMAT \"%s are to be promoted), \"\n+                     \"old: \" SIZE_FORMAT \"%s\",\n+                     byte_size_in_proper_unit(collected_young),    proper_unit_for_byte_size(collected_young),\n+                     byte_size_in_proper_unit(collected_promoted), proper_unit_for_byte_size(collected_promoted),\n+                     byte_size_in_proper_unit(collected_old),      proper_unit_for_byte_size(collected_old));\n@@ -251,1 +256,0 @@\n-  ++_cycles_since_last_resize;\n@@ -327,0 +331,75 @@\n+\/\/ Return conservative estimate of how much memory can be allocated before we need to start GC\n+size_t ShenandoahAdaptiveHeuristics::evac_slack(size_t young_regions_to_be_reclaimed) {\n+  assert(_generation->is_young(), \"evac_slack is only meaningful for young-gen heuristic\");\n+\n+  size_t max_capacity = _generation->max_capacity();\n+  size_t capacity = _generation->soft_max_capacity();\n+  size_t usage = _generation->used();\n+  size_t available = (capacity > usage)? capacity - usage: 0;\n+  size_t allocated = _generation->bytes_allocated_since_gc_start();\n+\n+  size_t available_young_collected = ShenandoahHeap::heap()->collection_set()->get_young_available_bytes_collected();\n+  size_t anticipated_available =\n+    available + young_regions_to_be_reclaimed * ShenandoahHeapRegion::region_size_bytes() - available_young_collected;\n+  size_t allocation_headroom = anticipated_available;\n+  size_t spike_headroom = capacity * ShenandoahAllocSpikeFactor \/ 100;\n+  size_t penalties      = capacity * _gc_time_penalties \/ 100;\n+\n+  double rate = _allocation_rate.sample(allocated);\n+\n+  \/\/ At what value of available, would avg and spike triggers occur?\n+  \/\/  if allocation_headroom < avg_cycle_time * avg_alloc_rate, then we experience avg trigger\n+  \/\/  if allocation_headroom < avg_cycle_time * rate, then we experience spike trigger if is_spiking\n+  \/\/\n+  \/\/ allocation_headroom =\n+  \/\/     0, if penalties > available or if penalties + spike_headroom > available\n+  \/\/     available - penalties - spike_headroom, otherwise\n+  \/\/\n+  \/\/ so we trigger if available - penalties - spike_headroom < avg_cycle_time * avg_alloc_rate, which is to say\n+  \/\/                  available < avg_cycle_time * avg_alloc_rate + penalties + spike_headroom\n+  \/\/            or if available < penalties + spike_headroom\n+  \/\/\n+  \/\/ since avg_cycle_time * avg_alloc_rate > 0, the first test is sufficient to test both conditions\n+  \/\/\n+  \/\/ thus, avg_evac_slack is MIN2(0,  available - avg_cycle_time * avg_alloc_rate + penalties + spike_headroom)\n+  \/\/\n+  \/\/ similarly, spike_evac_slack is MIN2(0, available - avg_cycle_time * rate + penalties + spike_headroom)\n+  \/\/ but spike_evac_slack is only relevant if is_spiking, as defined below.\n+\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+\n+  \/\/ TODO: Consider making conservative adjustments to avg_cycle_time, such as: (avg_cycle_time *= 2) in cases where\n+  \/\/ we expect a longer-than-normal GC duration.  This includes mixed evacuations, evacuation that perform promotion\n+  \/\/ including promotion in place, and OLD GC bootstrap cycles.  It has been observed that these cycles sometimes\n+  \/\/ require twice or more the duration of \"normal\" GC cycles.  We have experimented with this approach.  While it\n+  \/\/ does appear to reduce the frequency of degenerated cycles due to late triggers, it also has the effect of reducing\n+  \/\/ evacuation slack so that there is less memory available to be transferred to OLD.  The result is that we\n+  \/\/ throttle promotion and it takes too long to move old objects out of the young generation.\n+\n+  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+  size_t evac_slack_avg;\n+  if (anticipated_available > avg_cycle_time * avg_alloc_rate + penalties + spike_headroom) {\n+    evac_slack_avg = anticipated_available - (avg_cycle_time * avg_alloc_rate + penalties + spike_headroom);\n+  } else {\n+    \/\/ we have no slack because it's already time to trigger\n+    evac_slack_avg = 0;\n+  }\n+\n+  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+  size_t evac_slack_spiking;\n+  if (is_spiking) {\n+    if (anticipated_available > avg_cycle_time * rate + penalties + spike_headroom) {\n+      evac_slack_spiking = anticipated_available - (avg_cycle_time * rate + penalties + spike_headroom);\n+    } else {\n+      \/\/ we have no slack because it's already time to trigger\n+      evac_slack_spiking = 0;\n+    }\n+  } else {\n+    evac_slack_spiking = evac_slack_avg;\n+  }\n+\n+  size_t threshold = min_free_threshold();\n+  size_t evac_min_threshold = (anticipated_available > threshold)? anticipated_available - threshold: 0;\n+  return MIN3(evac_slack_spiking, evac_slack_avg, evac_min_threshold);\n+}\n+\n@@ -350,19 +429,8 @@\n-  size_t min_threshold = min_free_threshold();\n-\n-  if (available < min_threshold) {\n-    log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n-                 _generation->name(),\n-                 byte_size_in_proper_unit(available),     proper_unit_for_byte_size(available),\n-                 byte_size_in_proper_unit(min_threshold), proper_unit_for_byte_size(min_threshold));\n-    return resize_and_evaluate();\n-  }\n-\n-  \/\/ Check if we need to learn a bit about the application\n-  const size_t max_learn = ShenandoahLearningSteps;\n-  if (_gc_times_learned < max_learn) {\n-    size_t init_threshold = capacity \/ 100 * ShenandoahInitFreeThreshold;\n-    if (available < init_threshold) {\n-      log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n-                   _generation->name(), _gc_times_learned + 1, max_learn,\n-                   byte_size_in_proper_unit(available),       proper_unit_for_byte_size(available),\n-                   byte_size_in_proper_unit(init_threshold),  proper_unit_for_byte_size(init_threshold));\n+  \/\/ OLD generation is maintained to be as small as possible.  Depletion-of-free-pool triggers do not apply to old generation.\n+  if (!_generation->is_old()) {\n+    size_t min_threshold = min_free_threshold();\n+    if (available < min_threshold) {\n+      log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n+                   _generation->name(),\n+                   byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                   byte_size_in_proper_unit(min_threshold),       proper_unit_for_byte_size(min_threshold));\n@@ -371,1 +439,0 @@\n-  }\n@@ -373,48 +440,13 @@\n-  \/\/  Rationale:\n-  \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n-  \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n-  \/\/\n-  \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n-  \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n-  \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n-  \/\/       accommodation to give us enough runway to recalibrate our \"average allocation rate\".\n-  \/\/\n-  \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n-  \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n-  \/\/       enough runway to recalibrate our \"average allocation rate\".\n-  \/\/\n-  \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n-  \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n-  \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n-  \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n-  \/\/\n-  \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n-  \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n-  \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n-  \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n-  \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n-  \/\/          of how this will behave.\n-  \/\/\n-  \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n-  \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n-  \/\/    computations an independent effort.\n-\n-\n-  \/\/ TODO: Account for inherent delays in responding to GC triggers\n-  \/\/  1. It has been observed that delays of 200 ms or greater are common between the moment we return true from should_start_gc()\n-  \/\/     and the moment at which we begin execution of the concurrent reset phase.  Add this time into the calculation of\n-  \/\/     avg_cycle_time below.  (What is \"this time\"?  Perhaps we should remember recent history of this delay for the\n-  \/\/     running workload and use the maximum delay recently seen for \"this time\".)\n-  \/\/  2. The frequency of inquiries to should_start_gc() is adaptive, ranging between ShenandoahControlIntervalMin and\n-  \/\/     ShenandoahControlIntervalMax.  The current control interval (or the max control interval) should also be added into\n-  \/\/     the calculation of avg_cycle_time below.\n-\n-  \/\/ Check if allocation headroom is still okay. This also factors in:\n-  \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n-  \/\/   2. Accumulated penalties from Degenerated and Full GC\n-  size_t allocation_headroom = available;\n-  size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n-  size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n-\n-  allocation_headroom -= MIN2(allocation_headroom, penalties);\n-  allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n+    \/\/ Check if we need to learn a bit about the application\n+    const size_t max_learn = ShenandoahLearningSteps;\n+    if (_gc_times_learned < max_learn) {\n+      size_t init_threshold = capacity \/ 100 * ShenandoahInitFreeThreshold;\n+      if (available < init_threshold) {\n+        log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\"\n+                     SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n+                     _generation->name(), _gc_times_learned + 1, max_learn,\n+                     byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                     byte_size_in_proper_unit(init_threshold),      proper_unit_for_byte_size(init_threshold));\n+        return true;\n+      }\n+    }\n@@ -422,1 +454,28 @@\n-  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+    \/\/  Rationale:\n+    \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n+    \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n+    \/\/\n+    \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n+    \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n+    \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n+    \/\/       accomodation to give us enough runway to recalibrate our \"average allocation rate\".\n+    \/\/\n+    \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n+    \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n+    \/\/       enough runway to recalibrate our \"average allocation rate\".\n+    \/\/\n+    \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n+    \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n+    \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n+    \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n+    \/\/\n+    \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n+    \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n+    \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n+    \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n+    \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n+    \/\/          of how this will behave.\n+    \/\/\n+    \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n+    \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n+    \/\/    computations an independent effort.\n@@ -424,21 +483,0 @@\n-  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n-  log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n-          _generation->name(), avg_cycle_time * 1000,\n-          byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n-\n-  if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n-    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n-                 _generation->name(), avg_cycle_time * 1000,\n-                 byte_size_in_proper_unit(avg_alloc_rate),      proper_unit_for_byte_size(avg_alloc_rate),\n-                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-                 _margin_of_error_sd);\n-\n-    log_info(gc, ergo)(\"Free headroom: \" SIZE_FORMAT \"%s (free) - \" SIZE_FORMAT \"%s (spike) - \" SIZE_FORMAT \"%s (penalties) = \" SIZE_FORMAT \"%s\",\n-                       byte_size_in_proper_unit(available),           proper_unit_for_byte_size(available),\n-                       byte_size_in_proper_unit(spike_headroom),      proper_unit_for_byte_size(spike_headroom),\n-                       byte_size_in_proper_unit(penalties),           proper_unit_for_byte_size(penalties),\n-                       byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n-\n-    _last_trigger = RATE;\n-    return resize_and_evaluate();\n-  }\n@@ -446,10 +484,3 @@\n-  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n-  if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n-    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n-                 _generation->name(), avg_cycle_time * 1000,\n-                 byte_size_in_proper_unit(rate), proper_unit_for_byte_size(rate),\n-                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-                 _spike_threshold_sd);\n-    _last_trigger = SPIKE;\n-    return resize_and_evaluate();\n-  }\n+    \/\/ Check if allocation headroom is still okay. This also factors in:\n+    \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n+    \/\/   2. Accumulated penalties from Degenerated and Full GC\n@@ -457,2 +488,3 @@\n-  return ShenandoahHeuristics::should_start_gc();\n-}\n+    size_t allocation_headroom = available;\n+    size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n+    size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n@@ -460,6 +492,2 @@\n-bool ShenandoahAdaptiveHeuristics::resize_and_evaluate() {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  if (!heap->mode()->is_generational()) {\n-    \/\/ We only attempt to resize the generations in generational mode.\n-    return true;\n-  }\n+    allocation_headroom -= MIN2(allocation_headroom, penalties);\n+    allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n@@ -467,5 +495,5 @@\n-  if (_cycles_since_last_resize <= MINIMUM_RESIZE_INTERVAL) {\n-    log_info(gc, ergo)(\"Not resizing %s for another \" UINT32_FORMAT \" cycles\",\n-            _generation->name(), _cycles_since_last_resize);\n-    return true;\n-  }\n+    double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+    double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+    log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+                  _generation->name(),\n+                  avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n@@ -473,5 +501,15 @@\n-  if (!heap->generation_sizer()->transfer_capacity(_generation)) {\n-    \/\/ We could not enlarge our generation, so we must start a gc cycle.\n-    log_info(gc, ergo)(\"Could not increase size of %s, begin gc cycle\", _generation->name());\n-    return true;\n-  }\n+    if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n+\n+      log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+                   \" to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n+                   _generation->name(), avg_cycle_time * 1000,\n+                   byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate),\n+                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                   _margin_of_error_sd);\n+\n+      log_info(gc, ergo)(\"Free headroom: \" SIZE_FORMAT \"%s (free) - \" SIZE_FORMAT \"%s (spike) - \"\n+                         SIZE_FORMAT \"%s (penalties) = \" SIZE_FORMAT \"%s\",\n+                         byte_size_in_proper_unit(available),           proper_unit_for_byte_size(available),\n+                         byte_size_in_proper_unit(spike_headroom),      proper_unit_for_byte_size(spike_headroom),\n+                         byte_size_in_proper_unit(penalties),           proper_unit_for_byte_size(penalties),\n+                         byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n@@ -479,2 +517,50 @@\n-  log_info(gc)(\"Increased size of %s generation, re-evaluate trigger criteria\", _generation->name());\n-  return should_start_gc();\n+      _last_trigger = RATE;\n+      return true;\n+    }\n+\n+    bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+    if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n+      log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s)\"\n+                   \" to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                   _generation->name(), avg_cycle_time * 1000,\n+                   byte_size_in_proper_unit(rate), proper_unit_for_byte_size(rate),\n+                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                   _spike_threshold_sd);\n+      _last_trigger = SPIKE;\n+      return true;\n+    }\n+\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    if (heap->mode()->is_generational()) {\n+      \/\/ Get through promotions and mixed evacuations as quickly as possible.  These cycles sometimes require significantly\n+      \/\/ more time than traditional young-generation cycles so start them up as soon as possible.  This is a \"mitigation\"\n+      \/\/ for the reality that old-gen and young-gen activities are not truly \"concurrent\".  If there is old-gen work to\n+      \/\/ be done, we start up the young-gen GC threads so they can do some of this old-gen work.  As implemented, promotion\n+      \/\/ gets priority over old-gen marking.\n+\n+      size_t promo_potential = heap->get_promotion_potential();\n+      size_t promo_in_place_potential = heap->get_promotion_in_place_potential();\n+      ShenandoahOldHeuristics* old_heuristics = (ShenandoahOldHeuristics*) heap->old_generation()->heuristics();\n+      size_t mixed_candidates = old_heuristics->unprocessed_old_collection_candidates();\n+      if (promo_potential > 0) {\n+        \/\/ Detect unsigned arithmetic underflow\n+        assert(promo_potential < heap->capacity(), \"Sanity\");\n+        log_info(gc)(\"Trigger (%s): expedite promotion of \" SIZE_FORMAT \"%s\",\n+                     _generation->name(), byte_size_in_proper_unit(promo_potential), proper_unit_for_byte_size(promo_potential));\n+        return true;\n+      } else if (promo_in_place_potential > 0) {\n+        \/\/ Detect unsigned arithmetic underflow\n+        assert(promo_in_place_potential < heap->capacity(), \"Sanity\");\n+        log_info(gc)(\"Trigger (%s): expedite promotion in place of \" SIZE_FORMAT \"%s\", _generation->name(),\n+                     byte_size_in_proper_unit(promo_in_place_potential),\n+                     proper_unit_for_byte_size(promo_in_place_potential));\n+        return true;\n+      } else if (mixed_candidates > 0) {\n+        \/\/ We need to run young GC in order to open up some free heap regions so we can finish mixed evacuations.\n+        log_info(gc)(\"Trigger (%s): expedite mixed evacuation of \" SIZE_FORMAT \" regions\",\n+                     _generation->name(), mixed_candidates);\n+        return true;\n+      }\n+    }\n+  }\n+  return ShenandoahHeuristics::should_start_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":251,"deletions":165,"binary":false,"changes":416,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-\n@@ -75,0 +74,2 @@\n+  virtual size_t evac_slack(size_t young_regions_to_be_recycled);\n+\n@@ -88,7 +89,0 @@\n-  \/\/ At least this many cycles must execute before the heuristic will attempt\n-  \/\/ to resize its generation. This is to prevent the heuristic from rapidly\n-  \/\/ maxing out the generation size (which only forces the collector for the\n-  \/\/ other generation to run more frequently, defeating the purpose of improving\n-  \/\/ MMU).\n-  const static uint MINIMUM_RESIZE_INTERVAL;\n-\n@@ -109,2 +103,0 @@\n-  bool resize_and_evaluate();\n-\n@@ -138,4 +130,0 @@\n-\n-  \/\/ Do not attempt to resize the generation for this heuristic until this\n-  \/\/ value is greater than MINIMUM_RESIZE_INTERVAL.\n-  uint _cycles_since_last_resize;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":2,"deletions":14,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -52,1 +52,2 @@\n-  size_t available    = _generation->available();\n+  size_t usage        = _generation->used();\n+  size_t available    = (capacity > usage)? capacity - usage: 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"utilities\/quickSort.hpp\"\n@@ -43,0 +44,1 @@\n+\/\/ sort by decreasing garbage (so most garbage comes first)\n@@ -44,1 +46,1 @@\n-  if (a._garbage > b._garbage)\n+  if (a._u._garbage > b._u._garbage)\n@@ -46,1 +48,10 @@\n-  else if (a._garbage < b._garbage)\n+  else if (a._u._garbage < b._u._garbage)\n+    return 1;\n+  else return 0;\n+}\n+\n+\/\/ sort by increasing live (so least live comes first)\n+int ShenandoahHeuristics::compare_by_live(RegionData a, RegionData b) {\n+  if (a._u._live_data < b._u._live_data)\n+    return -1;\n+  else if (a._u._live_data > b._u._live_data)\n@@ -79,1 +90,15 @@\n-size_t ShenandoahHeuristics::select_aged_regions(size_t old_available, size_t num_regions, bool* preselected_regions) {\n+typedef struct {\n+  ShenandoahHeapRegion* _region;\n+  size_t _live_data;\n+} AgedRegionData;\n+\n+static int compare_by_aged_live(AgedRegionData a, AgedRegionData b) {\n+  if (a._live_data < b._live_data)\n+    return -1;\n+  else if (a._live_data > b._live_data)\n+    return 1;\n+  else return 0;\n+}\n+\n+\/\/ Returns bytes of old-gen memory consumed by selected aged regions\n+size_t ShenandoahHeuristics::select_aged_regions(size_t old_available, size_t num_regions, bool preselected_regions[]) {\n@@ -82,1 +107,1 @@\n-\n+  ShenandoahMarkingContext* const ctx = heap->marking_context();\n@@ -84,0 +109,18 @@\n+  size_t promo_potential = 0;\n+  size_t anticipated_promote_in_place_live = 0;\n+\n+  heap->clear_promotion_in_place_potential();\n+  heap->clear_promotion_potential();\n+  size_t candidates = 0;\n+  size_t candidates_live = 0;\n+  size_t old_garbage_threshold = (ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold) \/ 100;\n+  size_t promote_in_place_regions = 0;\n+  size_t promote_in_place_live = 0;\n+  size_t promote_in_place_pad = 0;\n+  size_t anticipated_candidates = 0;\n+  size_t anticipated_promote_in_place_regions = 0;\n+\n+  \/\/ Sort the promotion-eligible regions according to live-data-bytes so that we can first reclaim regions that require\n+  \/\/ less evacuation effort.  This prioritizes garbage first, expanding the allocation pool before we begin the work of\n+  \/\/ reclaiming regions that require more effort.\n+  AgedRegionData* sorted_regions = (AgedRegionData*) alloca(num_regions * sizeof(AgedRegionData));\n@@ -85,4 +128,82 @@\n-    ShenandoahHeapRegion* region = heap->get_region(i);\n-    if (in_generation(region) && !region->is_empty() && region->is_regular() && (region->age() >= InitialTenuringThreshold)) {\n-      size_t promotion_need = (size_t) (region->get_live_data_bytes() * ShenandoahEvacWaste);\n-      if (old_consumed + promotion_need < old_available) {\n+    ShenandoahHeapRegion* r = heap->get_region(i);\n+    if (r->is_empty() || !r->has_live() || !r->is_young() || !r->is_regular()) {\n+      continue;\n+    }\n+    if (r->age() >= InitialTenuringThreshold) {\n+      r->save_top_before_promote();\n+      if ((r->garbage() < old_garbage_threshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        HeapWord* original_top = r->top();\n+        if (tams == original_top) {\n+          \/\/ Fill the remnant memory within this region to assure no allocations prior to promote in place.  Otherwise,\n+          \/\/ newly allocated objects will not be parseable when promote in place tries to register them.  Furthermore, any\n+          \/\/ new allocations would not necessarily be eligible for promotion.  This addresses both issues.\n+          size_t remnant_size = r->free() \/ HeapWordSize;\n+          if (remnant_size > ShenandoahHeap::min_fill_size()) {\n+            ShenandoahHeap::fill_with_object(original_top, remnant_size);\n+            r->set_top(r->end());\n+            promote_in_place_pad += remnant_size * HeapWordSize;\n+          } else {\n+            \/\/ Since the remnant is so small that it cannot be filled, we don't have to worry about any accidental\n+            \/\/ allocations occuring within this region before the region is promoted in place.\n+          }\n+          promote_in_place_regions++;\n+          promote_in_place_live += r->get_live_data_bytes();\n+        }\n+        \/\/ Else, we do not promote this region (either in place or by copy) because it has received new allocations.\n+\n+        \/\/ During evacuation, we exclude from promotion regions for which age > tenure threshold, garbage < garbage-threshold,\n+        \/\/  and get_top_before_promote() != tams\n+      } else {\n+        \/\/ After sorting and selecting best candidates below, we may decide to exclude this promotion-eligible region\n+        \/\/ from the current collection sets.  If this happens, we will consider this region as part of the anticipated\n+        \/\/ promotion potential for the next GC pass.\n+        size_t live_data = r->get_live_data_bytes();\n+        candidates_live += live_data;\n+        sorted_regions[candidates]._region = r;\n+        sorted_regions[candidates++]._live_data = live_data;\n+      }\n+    } else {\n+      \/\/ We only anticipate to promote regular regions if garbage() is above threshold.  Tenure-aged regions with less\n+      \/\/ garbage are promoted in place.  These take a different path to old-gen.  Note that certain regions that are\n+      \/\/ excluded from anticipated promotion because their garbage content is too low (causing us to anticipate that\n+      \/\/ the region would be promoted in place) may be eligible for evacuation promotion by the time promotion takes\n+      \/\/ place during a subsequent GC pass because more garbage is found within the region between now and then.  This\n+      \/\/ should not happen if we are properly adapting the tenure age.  The theory behind adaptive tenuring threshold\n+      \/\/ is to choose the youngest age that demonstrates no \"significant\" futher loss of population since the previous\n+      \/\/ age.  If not this, we expect the tenure age to demonstrate linear population decay for at least two population\n+      \/\/ samples, whereas we expect to observe exponetial population decay for ages younger than the tenure age.\n+      \/\/\n+      \/\/ In the case that certain regions which were anticipated to be promoted in place need to be promoted by\n+      \/\/ evacuation, it may be the case that there is not sufficient reserve within old-gen to hold evacuation of\n+      \/\/ these regions.  The likely outcome is that these regions will not be selected for evacuation or promotion\n+      \/\/ in the current cycle and we will anticipate that they will be promoted in the next cycle.  This will cause\n+      \/\/ us to reserve more old-gen memory so that these objects can be promoted in the subsequent cycle.\n+      \/\/\n+      \/\/ TODO:\n+      \/\/   If we are auto-tuning the tenure age and regions that were anticipated to be promoted in place end up\n+      \/\/   being promoted by evacuation, this event should feed into the tenure-age-selection heuristic so that\n+      \/\/   the tenure age can be increased.\n+      if (heap->is_aging_cycle() && (r->age() + 1 == InitialTenuringThreshold)) {\n+        if (r->garbage() >= old_garbage_threshold) {\n+          anticipated_candidates++;\n+          promo_potential += r->get_live_data_bytes();\n+        }\n+        else {\n+          anticipated_promote_in_place_regions++;\n+          anticipated_promote_in_place_live += r->get_live_data_bytes();\n+        }\n+      }\n+    }\n+    \/\/ Note that we keep going even if one region is excluded from selection.\n+    \/\/ Subsequent regions may be selected if they have smaller live data.\n+  }\n+  \/\/ Sort in increasing order according to live data bytes.  Note that candidates represents the number of regions\n+  \/\/ that qualify to be promoted by evacuation.\n+  if (candidates > 0) {\n+    QuickSort::sort<AgedRegionData>(sorted_regions, candidates, compare_by_aged_live, false);\n+    for (size_t i = 0; i < candidates; i++) {\n+      size_t region_live_data = sorted_regions[i]._live_data;\n+      size_t promotion_need = (size_t) (region_live_data * ShenandoahPromoEvacWaste);\n+      if (old_consumed + promotion_need <= old_available) {\n+        ShenandoahHeapRegion* region = sorted_regions[i]._region;\n@@ -90,1 +211,5 @@\n-        preselected_regions[i] = true;\n+        preselected_regions[region->index()] = true;\n+      } else {\n+        \/\/ We rejected this promotable region from the collection set because we had no room to hold its copy.\n+        \/\/ Add this region to promo potential for next GC.\n+        promo_potential += region_live_data;\n@@ -92,2 +217,2 @@\n-      \/\/ Note that we keep going even if one region is excluded from selection.\n-      \/\/ Subsequent regions may be selected if they have smaller live data.\n+      \/\/ We keep going even if one region is excluded from selection because we need to accumulate all eligible\n+      \/\/ regions that are not preselected into promo_potential\n@@ -96,0 +221,3 @@\n+  heap->set_pad_for_promote_in_place(promote_in_place_pad);\n+  heap->set_promotion_potential(promo_potential);\n+  heap->set_promotion_in_place_potential(anticipated_promote_in_place_live);\n@@ -102,0 +230,1 @@\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -116,0 +245,1 @@\n+  size_t preselected_candidates = 0;\n@@ -125,0 +255,10 @@\n+  size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n+  \/\/ This counts number of humongous regions that we intend to promote in this cycle.\n+  size_t humongous_regions_promoted = 0;\n+  \/\/ This counts bytes of memory used by hunongous regions to be promoted in place.\n+  size_t humongous_bytes_promoted = 0;\n+  \/\/ This counts number of regular regions that will be promoted in place.\n+  size_t regular_regions_promoted_in_place = 0;\n+  \/\/ This counts bytes of memory used by regular regions to be promoted in place.\n+  size_t regular_regions_promoted_usage = 0;\n+\n@@ -130,1 +270,0 @@\n-\n@@ -144,0 +283,1 @@\n+        bool is_candidate;\n@@ -145,1 +285,0 @@\n-        candidates[cand_idx]._region = region;\n@@ -147,2 +286,25 @@\n-          \/\/ If region is preselected, we know mode()->is_generational() and region->age() >= InitialTenuringThreshold)\n-          garbage = ShenandoahHeapRegion::region_size_bytes();\n+          \/\/ If !is_generational, we cannot ask if is_preselected.  If is_preselected, we know\n+          \/\/   region->age() >= InitialTenuringThreshold).\n+          is_candidate = true;\n+          preselected_candidates++;\n+          \/\/ Set garbage value to maximum value to force this into the sorted collection set.\n+          garbage = region_size_bytes;\n+        } else if (is_generational && region->is_young() && (region->age() >= InitialTenuringThreshold)) {\n+          \/\/ Note that for GLOBAL GC, region may be OLD, and OLD regions do not qualify for pre-selection\n+\n+          \/\/ This region is old enough to be promoted but it was not preselected, either because its garbage is below\n+          \/\/ ShenandoahOldGarbageThreshold so it will be promoted in place, or because there is not sufficient room\n+          \/\/ in old gen to hold the evacuated copies of this region's live data.  In both cases, we choose not to\n+          \/\/ place this region into the collection set.\n+          if (region->garbage_before_padded_for_promote() < old_garbage_threshold) {\n+            regular_regions_promoted_in_place++;\n+            regular_regions_promoted_usage += region->used_before_promote();\n+          }\n+          is_candidate = false;\n+        } else {\n+          is_candidate = true;\n+        }\n+        if (is_candidate) {\n+          candidates[cand_idx]._region = region;\n+          candidates[cand_idx]._u._garbage = garbage;\n+          cand_idx++;\n@@ -150,2 +312,0 @@\n-        candidates[cand_idx]._garbage = garbage;\n-        cand_idx++;\n@@ -154,1 +314,0 @@\n-\n@@ -169,0 +328,7 @@\n+      } else {\n+        if (region->is_young() && region->age() >= InitialTenuringThreshold) {\n+          oop obj = cast_to_oop(region->bottom());\n+          size_t humongous_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+          humongous_regions_promoted += humongous_regions;\n+          humongous_bytes_promoted += obj->size() * HeapWordSize;\n+        }\n@@ -176,0 +342,9 @@\n+  heap->reserve_promotable_humongous_regions(humongous_regions_promoted);\n+  heap->reserve_promotable_humongous_usage(humongous_bytes_promoted);\n+  heap->reserve_promotable_regular_regions(regular_regions_promoted_in_place);\n+  heap->reserve_promotable_regular_usage(regular_regions_promoted_usage);\n+\n+  log_info(gc, ergo)(\"Planning to promote in place \" SIZE_FORMAT \" humongous regions and \" SIZE_FORMAT\n+                     \" regular regions, spanning a total of \" SIZE_FORMAT \" used bytes\",\n+                     humongous_regions_promoted, regular_regions_promoted_in_place,\n+                     humongous_regions_promoted * ShenandoahHeapRegion::region_size_bytes() + regular_regions_promoted_usage);\n@@ -188,1 +363,3 @@\n-  if (immediate_percent <= ShenandoahImmediateThreshold) {\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  bool doing_promote_in_place = (humongous_regions_promoted + regular_regions_promoted_in_place > 0);\n+  if (doing_promote_in_place || (preselected_candidates > 0) || (immediate_percent <= ShenandoahImmediateThreshold)) {\n@@ -362,0 +539,6 @@\n+size_t ShenandoahHeuristics::evac_slack(size_t young_regions_to_be_recycled) {\n+  assert(false, \"evac_slack() only implemented for young Adaptive Heuristics\");\n+  return 0;\n+}\n+\n+\n@@ -373,2 +556,6 @@\n-  size_t min_free_threshold = _generation->is_old() ? ShenandoahOldMinFreeThreshold : ShenandoahMinFreeThreshold;\n-  return _generation->soft_max_capacity() \/ 100 * min_free_threshold;\n+  assert(!_generation->is_old(), \"min_free_threshold is only relevant to young GC\");\n+  size_t min_free_threshold = ShenandoahMinFreeThreshold;\n+  \/\/ Note that soft_max_capacity() \/ 100 * min_free_threshold is smaller than max_capacity() \/ 100 * min_free_threshold.\n+  \/\/ We want to behave conservatively here, so use max_capacity().  By returning a larger value, we cause the GC to\n+  \/\/ trigger when the remaining amount of free shrinks below the larger threshold.\n+  return _generation->max_capacity() \/ 100 * min_free_threshold;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":208,"deletions":21,"binary":false,"changes":229,"status":"modified"},{"patch":"@@ -73,1 +73,4 @@\n-    size_t _garbage;\n+    union {\n+      size_t _garbage;          \/\/ Not used by old-gen heuristics.\n+      size_t _live_data;        \/\/ Only used for old-gen heuristics, which prioritizes retention of _live_data over garbage reclaim\n+    } _u;\n@@ -109,0 +112,8 @@\n+  \/\/ Compare by live is used to prioritize compaction of old-gen regions.  With old-gen compaction, the goal is\n+  \/\/ to tightly pack long-lived objects into available regions.  In most cases, there has not been an accumulation\n+  \/\/ of garbage within old-gen regions.  The more likely opportunity will be to combine multiple sparsely populated\n+  \/\/ old-gen regions which may have been promoted in place into a smaller number of densely packed old-gen regions.\n+  \/\/ This improves subsequent allocation efficiency and reduces the likelihood of allocation failure (including\n+  \/\/ humongous allocation failure) due to fragmentation of the available old-gen allocation pool\n+  static int compare_by_live(RegionData a, RegionData b);\n+\n@@ -172,0 +183,2 @@\n+  virtual size_t evac_slack(size_t region_to_be_recycled);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":14,"deletions":1,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+  _old_generation(generation),\n@@ -47,1 +48,3 @@\n-  _old_generation(generation)\n+  _cannot_expand_trigger(false),\n+  _fragmentation_trigger(false),\n+  _growth_trigger(false)\n@@ -53,0 +56,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -59,1 +63,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -69,1 +72,19 @@\n-  size_t old_evacuation_budget = (size_t) ((double) heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t old_evacuation_budget = (size_t) ((double) heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n+  size_t unfragmented_available = heap->old_generation()->free_unaffiliated_regions() * ShenandoahHeapRegion::region_size_bytes();\n+  size_t fragmented_available;\n+  size_t excess_fragmented_available;\n+\n+  if (unfragmented_available > old_evacuation_budget) {\n+    unfragmented_available = old_evacuation_budget;\n+    fragmented_available = 0;\n+    excess_fragmented_available = 0;\n+  } else {\n+    assert(heap->old_generation()->available() > old_evacuation_budget, \"Cannot budget more than is available\");\n+    fragmented_available = heap->old_generation()->available() - unfragmented_available;\n+    assert(fragmented_available + unfragmented_available >= old_evacuation_budget, \"Budgets do not add up\");\n+    if (fragmented_available + unfragmented_available > old_evacuation_budget) {\n+      excess_fragmented_available = (fragmented_available + unfragmented_available) - old_evacuation_budget;\n+      fragmented_available -= excess_fragmented_available;\n+    }\n+  }\n+\n@@ -71,1 +92,0 @@\n-  size_t lost_evacuation_capacity = 0;\n@@ -76,0 +96,2 @@\n+  size_t lost_evacuation_capacity = 0;\n+\n@@ -77,1 +99,3 @@\n-  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates()\n+  \/\/ concurrent marking phase and have not yet been collected is represented by unprocessed_old_collection_candidates().\n+  \/\/ Candidate regions are ordered according to increasing amount of live data.  If there is not sufficient room to\n+  \/\/ evacuate region N, then there is no need to even consider evacuating region N+1.\n@@ -85,15 +109,36 @@\n-    \/\/ If we choose region r to be collected, then we need to decrease the capacity to hold other evacuations by\n-    \/\/ the size of r's free memory.\n-\n-    \/\/ It's probably overkill to compensate with lost_evacuation_capacity.\n-    \/\/ But it's the safe thing to do and has minimal impact on content of primed collection set.\n-    size_t live = r->get_live_data_bytes();\n-    if (live + lost_evacuation_capacity <= remaining_old_evacuation_budget) {\n-      \/\/ Decrement remaining evacuation budget by bytes that will be copied.\n-      lost_evacuation_capacity += r->free();\n-      remaining_old_evacuation_budget -= live;\n-      collection_set->add_region(r);\n-      included_old_regions++;\n-      evacuated_old_bytes += live;\n-      collected_old_bytes += r->garbage();\n-      consume_old_collection_candidate();\n+    \/\/ If region r is evacuated to fragmented memory (to free memory within a partially used region), then we need\n+    \/\/ to decrease the capacity of the fragmented memory by the scaled loss.\n+\n+    size_t live_data_for_evacuation = r->get_live_data_bytes();\n+    size_t lost_available = r->free();\n+\n+    if ((lost_available > 0) && (excess_fragmented_available > 0)) {\n+      if (lost_available < excess_fragmented_available) {\n+        excess_fragmented_available -= lost_available;\n+        lost_evacuation_capacity -= lost_available;\n+        lost_available  = 0;\n+      } else {\n+        lost_available -= excess_fragmented_available;\n+        lost_evacuation_capacity -= excess_fragmented_available;\n+        excess_fragmented_available = 0;\n+      }\n+    }\n+    size_t scaled_loss = (size_t) ((double) lost_available \/ ShenandoahOldEvacWaste);\n+    if ((lost_available > 0) && (fragmented_available > 0)) {\n+      if (scaled_loss + live_data_for_evacuation < fragmented_available) {\n+        fragmented_available -= scaled_loss;\n+        scaled_loss = 0;\n+      } else {\n+        \/\/ We will have to allocate this region's evacuation memory from unfragmented memory, so don't bother\n+        \/\/ to decrement scaled_loss\n+      }\n+    }\n+    if (scaled_loss > 0) {\n+      \/\/ We were not able to account for the lost free memory within fragmented memory, so we need to take this\n+      \/\/ allocation out of unfragmented memory.  Unfragmented memory does not need to account for loss of free.\n+      if (live_data_for_evacuation > unfragmented_available) {\n+        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n+        break;\n+      } else {\n+        unfragmented_available -= live_data_for_evacuation;\n+      }\n@@ -101,1 +146,18 @@\n-      break;\n+      \/\/ Since scaled_loss == 0, we have accounted for the loss of free memory, so we can allocate from either\n+      \/\/ fragmented or unfragmented available memory.  Use up the fragmented memory budget first.\n+      size_t evacuation_need = live_data_for_evacuation;\n+\n+      if (evacuation_need > fragmented_available) {\n+        evacuation_need -= fragmented_available;\n+        fragmented_available = 0;\n+      } else {\n+        fragmented_available -= evacuation_need;\n+        evacuation_need = 0;\n+      }\n+      if (evacuation_need > unfragmented_available) {\n+        \/\/ There is not room to evacuate this region or any that come after it in within the candidates array.\n+        break;\n+      } else {\n+        unfragmented_available -= evacuation_need;\n+        \/\/ dead code: evacuation_need == 0;\n+      }\n@@ -103,0 +165,5 @@\n+    collection_set->add_region(r);\n+    included_old_regions++;\n+    evacuated_old_bytes += live_data_for_evacuation;\n+    collected_old_bytes += r->garbage();\n+    consume_old_collection_candidate();\n@@ -109,1 +176,1 @@\n-\n+  decrease_unprocessed_old_collection_candidates_live_memory(evacuated_old_bytes);\n@@ -119,0 +186,2 @@\n+    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n+    clear_triggers();\n@@ -205,1 +274,1 @@\n-      available_slot._garbage = skipped._garbage;\n+      available_slot._u._live_data = skipped._u._live_data;\n@@ -235,0 +304,1 @@\n+  size_t live_data = 0;\n@@ -244,0 +314,1 @@\n+    size_t live_bytes = region->get_live_data_bytes();\n@@ -245,0 +316,1 @@\n+    live_data += live_bytes;\n@@ -255,1 +327,1 @@\n-        candidates[cand_idx]._garbage = garbage;\n+        candidates[cand_idx]._u._live_data = live_bytes;\n@@ -276,0 +348,2 @@\n+  ((ShenandoahOldGeneration*) (heap->old_generation()))->set_live_bytes_after_last_mark(live_data);\n+\n@@ -280,2 +354,5 @@\n-  \/\/ Prioritize regions to select garbage-first regions\n-  QuickSort::sort<RegionData>(candidates, cand_idx, compare_by_garbage, false);\n+  \/\/ Unlike young, we are more interested in efficiently packing OLD-gen than in reclaiming garbage first.  We sort by live-data.\n+  \/\/ Some regular regions may have been promoted in place with no garbage but also with very little live data.  When we \"compact\"\n+  \/\/ old-gen, we want to pack these underutilized regions together so we can have more unaffiliated (unfragmented) free regions\n+  \/\/ in old-gen.\n+  QuickSort::sort<RegionData>(candidates, cand_idx, compare_by_live, false);\n@@ -283,2 +360,2 @@\n-  \/\/ Any old-gen region that contains (ShenandoahOldGarbageThreshold (default value 25))% garbage or more is to\n-  \/\/ be evacuated.\n+  \/\/ Any old-gen region that contains (ShenandoahOldGarbageThreshold (default value 25)% garbage or more is to be\n+  \/\/ added to the list of candidates for subsequent mixed evacuations.\n@@ -288,0 +365,7 @@\n+  const size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  \/\/ The convention is to collect regions that have more than this amount of garbage.\n+  const size_t garbage_threshold = region_size_bytes * ShenandoahOldGarbageThreshold \/ 100;\n+\n+  \/\/ Englightened interpretation: collect regions that have less than this amount of live.\n+  const size_t live_threshold = region_size_bytes - garbage_threshold;\n@@ -289,1 +373,0 @@\n-  const size_t garbage_threshold = ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold \/ 100;\n@@ -295,0 +378,2 @@\n+  size_t unfragmented = 0;\n+\n@@ -296,2 +381,3 @@\n-    if (candidates[i]._garbage < garbage_threshold) {\n-      \/\/ Candidates are sorted in decreasing order of garbage, so no regions after this will be above the threshold\n+    size_t live = candidates[i]._u._live_data;\n+    if (live > live_threshold) {\n+      \/\/ Candidates are sorted in increasing order of live data, so no regions after this will be below the threshold.\n@@ -301,1 +387,4 @@\n-    candidates_garbage += candidates[i]._garbage;\n+    size_t region_garbage = candidates[i]._region->garbage();\n+    size_t region_free = candidates[i]._region->free();\n+    candidates_garbage += region_garbage;\n+    unfragmented += region_free;\n@@ -307,1 +396,3 @@\n-  log_info(gc)(\"Old-Gen Collectable Garbage: \" SIZE_FORMAT \"%s over \" UINT32_FORMAT \" regions, \"\n+  size_t old_candidates = _last_old_collection_candidate;\n+  log_info(gc)(\"Old-Gen Collectable Garbage: \" SIZE_FORMAT \"%s \"\n+               \"consolidated with free: \" SIZE_FORMAT \"%s, over \" SIZE_FORMAT \" regions, \"\n@@ -309,3 +400,5 @@\n-               byte_size_in_proper_unit(collectable_garbage), proper_unit_for_byte_size(collectable_garbage), _last_old_collection_candidate,\n-               byte_size_in_proper_unit(immediate_garbage),   proper_unit_for_byte_size(immediate_garbage),   immediate_regions);\n-\n+               byte_size_in_proper_unit(collectable_garbage), proper_unit_for_byte_size(collectable_garbage),\n+               byte_size_in_proper_unit(unfragmented),        proper_unit_for_byte_size(unfragmented), old_candidates,\n+               byte_size_in_proper_unit(immediate_garbage),   proper_unit_for_byte_size(immediate_garbage), immediate_regions);\n+  size_t mixed_evac_live = old_candidates * region_size_bytes - (candidates_garbage + unfragmented);\n+  set_unprocessed_old_collection_candidates_live_memory(mixed_evac_live);\n@@ -319,0 +412,14 @@\n+size_t ShenandoahOldHeuristics::unprocessed_old_collection_candidates_live_memory() const {\n+  return _live_bytes_in_unprocessed_candidates;\n+}\n+\n+void ShenandoahOldHeuristics::set_unprocessed_old_collection_candidates_live_memory(size_t initial_live) {\n+  _live_bytes_in_unprocessed_candidates = initial_live;\n+}\n+\n+void ShenandoahOldHeuristics::decrease_unprocessed_old_collection_candidates_live_memory(size_t evacuated_live) {\n+  assert(evacuated_live <= _live_bytes_in_unprocessed_candidates, \"Cannot evacuate more than was present\");\n+  _live_bytes_in_unprocessed_candidates -= evacuated_live;\n+}\n+\n+\n@@ -359,1 +466,1 @@\n-  return _last_old_region - _next_old_collection_candidate;\n+  return (_last_old_region - _next_old_collection_candidate);\n@@ -369,6 +476,0 @@\n-  if (!_promotion_failed) {\n-    if (ShenandoahHeap::heap()->generation_sizer()->transfer_capacity(_old_generation)) {\n-      log_info(gc)(\"Increased size of old generation due to promotion failure.\");\n-    }\n-    \/\/ TODO: Increase tenuring threshold to push back on promotions.\n-  }\n@@ -379,1 +480,0 @@\n-  _promotion_failed = false;\n@@ -385,0 +485,5 @@\n+  clear_triggers();\n+}\n+\n+void ShenandoahOldHeuristics::trigger_old_has_grown() {\n+  _growth_trigger = true;\n@@ -387,0 +492,9 @@\n+\n+void ShenandoahOldHeuristics::clear_triggers() {\n+  \/\/ Clear any triggers that were set during mixed evacuations.  Conditions may be different now that this phase has finished.\n+  _promotion_failed = false;\n+  _cannot_expand_trigger = false;\n+  _fragmentation_trigger = false;\n+  _growth_trigger = false;\n+ }\n+\n@@ -396,4 +510,8 @@\n-  \/\/ If there's been a promotion failure (and we don't have regions already scheduled for evacuation),\n-  \/\/ start a new old generation collection.\n-  if (_promotion_failed) {\n-    log_info(gc)(\"Trigger: Promotion Failure\");\n+  if (_cannot_expand_trigger) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    size_t old_gen_capacity = old_gen->max_capacity();\n+    size_t heap_capacity = heap->capacity();\n+    double percent = 100.0 * ((double) old_gen_capacity) \/ heap_capacity;\n+    log_info(gc)(\"Trigger (OLD): Expansion failure, current size: \" SIZE_FORMAT \"%s which is %.1f%% of total heap size\",\n+                 byte_size_in_proper_unit(old_gen_capacity), proper_unit_for_byte_size(old_gen_capacity), percent);\n@@ -403,0 +521,35 @@\n+  if (_fragmentation_trigger) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    size_t used = old_gen->used();\n+    size_t used_regions_size = old_gen->used_regions_size();\n+    size_t used_regions = old_gen->used_regions();\n+    assert(used_regions_size > used_regions, \"Cannot have more used than used regions\");\n+    size_t fragmented_free = used_regions_size - used;\n+    double percent = 100.0 * ((double) fragmented_free) \/ used_regions_size;\n+    log_info(gc)(\"Trigger (OLD): Old has become fragmented: \"\n+                 SIZE_FORMAT \"%s available bytes spread between \" SIZE_FORMAT \" regions (%.1f%% free)\",\n+                 byte_size_in_proper_unit(fragmented_free), proper_unit_for_byte_size(fragmented_free), used_regions, percent);\n+    return true;\n+  }\n+\n+  if (_growth_trigger) {\n+    \/\/ Growth may be falsely triggered during mixed evacuations, before the mixed-evacuation candidates have been\n+    \/\/ evacuated.  Before acting on a false trigger, we check to confirm the trigger condition is still satisfied.\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    size_t current_usage = old_gen->used();\n+    size_t trigger_threshold = old_gen->usage_trigger_threshold();\n+    if (current_usage > trigger_threshold) {\n+      size_t live_at_previous_old = old_gen->get_live_bytes_after_last_mark();\n+      double percent_growth = 100.0 * ((double) current_usage - live_at_previous_old) \/ live_at_previous_old;\n+      log_info(gc)(\"Trigger (OLD): Old has overgrown, live at end of previous OLD marking: \"\n+                   SIZE_FORMAT \"%s, current usage: \" SIZE_FORMAT \"%s, percent growth: %.1f%%\",\n+                   byte_size_in_proper_unit(live_at_previous_old), proper_unit_for_byte_size(live_at_previous_old),\n+                   byte_size_in_proper_unit(current_usage), proper_unit_for_byte_size(current_usage), percent_growth);\n+      return true;\n+    } else {\n+      _growth_trigger = false;\n+    }\n+  }\n+\n@@ -412,0 +565,2 @@\n+  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n@@ -416,0 +571,2 @@\n+  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n@@ -420,0 +577,2 @@\n+  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  clear_triggers();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":207,"deletions":48,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -72,0 +72,3 @@\n+  \/\/ How much live data must be evacuated from within the unprocessed mixed evacuation candidates?\n+  size_t _live_bytes_in_unprocessed_candidates;\n+\n@@ -75,0 +78,3 @@\n+  \/\/ Keep a pointer to our generation that we can use without down casting a protected member from the base class.\n+  ShenandoahOldGeneration* _old_generation;\n+\n@@ -79,2 +85,5 @@\n-  \/\/ Keep a pointer to our generation that we can use without down casting a protected member from the base class.\n-  ShenandoahOldGeneration* _old_generation;\n+  \/\/ Flags are set when promotion failure is detected (by gc thread), and cleared when\n+  \/\/ old generation collection begins (by control thread).  Flags are set and cleared at safepoints.\n+  bool _cannot_expand_trigger;\n+  bool _fragmentation_trigger;\n+  bool _growth_trigger;\n@@ -100,0 +109,7 @@\n+  \/\/ How much live memory must be evacuated from within old-collection candidates that have not yet been processed?\n+  size_t unprocessed_old_collection_candidates_live_memory() const;\n+\n+  void set_unprocessed_old_collection_candidates_live_memory(size_t initial_live);\n+\n+  void decrease_unprocessed_old_collection_candidates_live_memory(size_t evacuated_live);\n+\n@@ -125,3 +141,3 @@\n-  \/\/ Notify the heuristic of promotion failures. The promotion attempt will be skipped and the object will\n-  \/\/ be evacuated into the young generation. The collection should complete normally, but we want to schedule\n-  \/\/ an old collection as soon as possible.\n+  \/\/ Promotion failure does not currently trigger old-gen collections.  Often, promotion failures occur because\n+  \/\/ old-gen is sized too small rather than because it is necessary to collect old gen.  We keep the method\n+  \/\/ here in case we decide to feed this signal to sizing or triggering heuristics in the future.\n@@ -130,0 +146,6 @@\n+  void trigger_cannot_expand() { _cannot_expand_trigger = true; };\n+  void trigger_old_is_fragmented() { _fragmentation_trigger = true; }\n+  void trigger_old_has_grown();\n+\n+  void clear_triggers();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":27,"deletions":5,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -93,2 +93,1 @@\n-\n-  size_t live = r->get_live_data_bytes();\n+  size_t live    = r->get_live_data_bytes();\n@@ -96,1 +95,1 @@\n-\n+  size_t free    = r->free();\n@@ -100,0 +99,1 @@\n+    _young_available_bytes_collected += free;\n@@ -107,0 +107,1 @@\n+    _old_available_bytes_collected += free;\n@@ -120,0 +121,1 @@\n+\n@@ -143,0 +145,3 @@\n+  _young_available_bytes_collected = 0;\n+  _old_available_bytes_collected = 0;\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":8,"deletions":3,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -69,0 +69,5 @@\n+  \/\/ When a region having memory available to be allocated is added to the collection set, the region's available memory\n+  \/\/ should be subtracted from what's available.\n+  size_t                _young_available_bytes_collected;\n+  size_t                _old_available_bytes_collected;\n+\n@@ -114,0 +119,4 @@\n+  size_t get_young_available_bytes_collected() { return _young_available_bytes_collected; }\n+\n+  size_t get_old_available_bytes_collected() { return _old_available_bytes_collected; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -220,0 +220,2 @@\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n@@ -221,0 +223,5 @@\n+    bool success;\n+    size_t region_xfer;\n+    const char* region_destination;\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    ShenandoahGeneration* old_gen = heap->old_generation();\n@@ -222,2 +229,0 @@\n-      ShenandoahYoungGeneration* young_gen = heap->young_generation();\n-      ShenandoahGeneration* old_gen = heap->old_generation();\n@@ -226,0 +231,21 @@\n+      size_t old_region_surplus = heap->get_old_region_surplus();\n+      size_t old_region_deficit = heap->get_old_region_deficit();\n+      if (old_region_surplus) {\n+        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        region_destination = \"young\";\n+        region_xfer = old_region_surplus;\n+      } else if (old_region_deficit) {\n+        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+        region_destination = \"old\";\n+        region_xfer = old_region_deficit;\n+        if (!success) {\n+          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+        }\n+      } else {\n+        region_destination = \"none\";\n+        region_xfer = 0;\n+        success = true;\n+      }\n+      heap->set_old_region_surplus(0);\n+      heap->set_old_region_deficit(0);\n+\n@@ -230,7 +256,0 @@\n-\n-      young_gen->unadjust_available();\n-      old_gen->unadjust_available();\n-      \/\/ No need to old_gen->increase_used().\n-      \/\/ That was done when plabs were allocated, accounting for both old evacs and promotions.\n-\n-      heap->set_alloc_supplement_reserve(0);\n@@ -242,0 +261,9 @@\n+\n+    \/\/ Report outside the heap lock\n+    size_t young_available = young_gen->available();\n+    size_t old_available = old_gen->available();\n+    log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                       SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                       success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                       byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                       byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n@@ -736,3 +764,0 @@\n-    \/\/\n-    \/\/ heap->get_alloc_supplement_reserve() represents the amount of old-gen memory that can be allocated during evacuation\n-    \/\/ and update-refs phases of gc.  The young evacuation reserve has already been removed from this quantity.\n@@ -743,7 +768,14 @@\n-    if (!heap->collection_set()->is_empty()) {\n-      LogTarget(Debug, gc, cset) lt;\n-      if (lt.is_enabled()) {\n-        ResourceMark rm;\n-        LogStream ls(lt);\n-        heap->collection_set()->print_on(&ls);\n-      }\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGeneration* young_gen = heap->young_generation();\n+      size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+      size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n+      if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+        \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+        \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+        LogTarget(Debug, gc, cset) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n@@ -751,3 +783,4 @@\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_before_evacuation();\n-      }\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+        \/\/ TODO: we do not need to run update-references following evacuation if collection_set->is_empty().\n@@ -755,3 +788,3 @@\n-      heap->set_evacuation_in_progress(true);\n-      \/\/ From here on, we need to update references.\n-      heap->set_has_forwarded_objects(true);\n+        heap->set_evacuation_in_progress(true);\n+        \/\/ From here on, we need to update references.\n+        heap->set_has_forwarded_objects(true);\n@@ -759,5 +792,5 @@\n-      \/\/ Verify before arming for concurrent processing.\n-      \/\/ Otherwise, verification can trigger stack processing.\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_during_evacuation();\n-      }\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n@@ -765,18 +798,3 @@\n-      \/\/ Arm nmethods\/stack for concurrent processing\n-      ShenandoahCodeRoots::arm_nmethods();\n-      ShenandoahStackWatermark::change_epoch_id();\n-\n-      if (heap->mode()->is_generational()) {\n-        \/\/ Calculate the temporary evacuation allowance supplement to young-gen memory capacity (for allocations\n-        \/\/ and young-gen evacuations).\n-        intptr_t adjustment = heap->get_alloc_supplement_reserve();\n-        size_t young_available = heap->young_generation()->adjust_available(adjustment);\n-        \/\/ old_available is memory that can hold promotions and evacuations.  Subtract out the memory that is being\n-        \/\/ loaned for young-gen allocations or evacuations.\n-        size_t old_available = heap->old_generation()->adjust_available(-adjustment);\n-\n-        log_info(gc, ergo)(\"After generational memory budget adjustments, old available: \" SIZE_FORMAT\n-                           \"%s, young_available: \" SIZE_FORMAT \"%s\",\n-                           byte_size_in_proper_unit(old_available),   proper_unit_for_byte_size(old_available),\n-                           byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n-      }\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        ShenandoahCodeRoots::arm_nmethods();\n+        ShenandoahStackWatermark::change_epoch_id();\n@@ -784,2 +802,11 @@\n-      if (ShenandoahPacing) {\n-        heap->pacer()->setup_for_evac();\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -788,3 +815,8 @@\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_after_concmark();\n-      }\n+      \/\/ Not is_generational()\n+      if (!heap->collection_set()->is_empty()) {\n+        LogTarget(Info, gc, ergo) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n@@ -792,2 +824,29 @@\n-      if (VerifyAfterGC) {\n-        Universe::verify();\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+\n+        heap->set_evacuation_in_progress(true);\n+        \/\/ From here on, we need to update references.\n+        heap->set_has_forwarded_objects(true);\n+\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n+\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        ShenandoahCodeRoots::arm_nmethods();\n+        ShenandoahStackWatermark::change_epoch_id();\n+\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -815,0 +874,1 @@\n+  ShenandoahThreadLocalData::enable_plab_promotions(thread);\n@@ -828,0 +888,3 @@\n+    Thread* worker_thread = Thread::current();\n+    ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n+\n@@ -1203,1 +1266,0 @@\n-  heap->adjust_generation_sizes();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":118,"deletions":56,"binary":false,"changes":174,"status":"modified"},{"patch":"@@ -116,0 +116,1 @@\n+  bool old_bootstrap_requested = false;\n@@ -210,0 +211,6 @@\n+        } else if (_requested_generation == OLD && !old_bootstrap_requested) {\n+          \/\/ Arrange to perform a young GC immediately followed by a bootstrap OLD GC.  OLD GC typically requires more\n+          \/\/ than twice the time required for YOUNG GC, so we run a YOUNG GC to replenish the YOUNG allocation pool before\n+          \/\/ we start the longer OLD GC effort.\n+          old_bootstrap_requested = true;\n+          generation = YOUNG;\n@@ -211,0 +218,3 @@\n+          \/\/ if (old_bootstrap_requested && (_requested_generation == OLD)), this starts the bootstrap GC that\n+          \/\/  immediately follows the preparatory young GC.\n+          \/\/ But we will abandon the planned bootstrap GC if a GLOBAL GC has been now been requested.\n@@ -212,0 +222,1 @@\n+          old_bootstrap_requested = false;\n@@ -213,1 +224,0 @@\n-\n@@ -394,4 +404,12 @@\n-      \/\/ The timed wait is necessary because this thread has a responsibility to send\n-      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n-      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n-      lock.wait(ShenandoahControlIntervalMax);\n+      if (old_bootstrap_requested) {\n+        _requested_generation = OLD;\n+        _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+      } else {\n+        \/\/ The timed wait is necessary because this thread has a responsibility to send\n+        \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+        MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+        lock.wait(ShenandoahControlIntervalMax);\n+      }\n+    } else {\n+      \/\/ in case of alloc_failure, abandon any plans to do immediate OLD Bootstrap\n+      old_bootstrap_requested = false;\n@@ -460,1 +478,1 @@\n-void ShenandoahControlThread::service_concurrent_normal_cycle(const ShenandoahHeap* heap,\n+void ShenandoahControlThread::service_concurrent_normal_cycle(ShenandoahHeap* heap,\n@@ -464,0 +482,1 @@\n+  ShenandoahGeneration* the_generation = nullptr;\n@@ -472,1 +491,2 @@\n-      service_concurrent_cycle(heap->young_generation(), cause, false);\n+      the_generation = heap->young_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n@@ -477,0 +497,1 @@\n+      the_generation = heap->old_generation();\n@@ -482,1 +503,2 @@\n-      service_concurrent_cycle(heap->global_generation(), cause, false);\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n@@ -487,1 +509,2 @@\n-      service_concurrent_cycle(heap->global_generation(), cause, false);\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n@@ -493,14 +516,0 @@\n-  const char* msg;\n-  if (heap->mode()->is_generational()) {\n-    if (heap->cancelled_gc()) {\n-      msg = (generation == YOUNG) ? \"At end of Interrupted Concurrent Young GC\" :\n-            \"At end of Interrupted Concurrent Bootstrap GC\";\n-    } else {\n-      msg = (generation == YOUNG) ? \"At end of Concurrent Young GC\" :\n-            \"At end of Concurrent Bootstrap GC\";\n-    }\n-  } else {\n-    msg = heap->cancelled_gc() ? \"At end of cancelled GC\" :\n-                                 \"At end of GC\";\n-  }\n-  heap->log_heap_status(msg);\n@@ -509,1 +518,1 @@\n-void ShenandoahControlThread::service_concurrent_old_cycle(const ShenandoahHeap* heap, GCCause::Cause &cause) {\n+void ShenandoahControlThread::service_concurrent_old_cycle(ShenandoahHeap* heap, GCCause::Cause &cause) {\n@@ -565,1 +574,1 @@\n-      service_concurrent_cycle(heap,young_generation, cause, true);\n+      service_concurrent_cycle(heap, young_generation, cause, true);\n@@ -591,0 +600,2 @@\n+          heap->mmu_tracker()->record_old_marking_increment(old_generation, GCId::current(), true,\n+                                                            heap->collection_set()->has_old_regions());\n@@ -594,0 +605,2 @@\n+        heap->mmu_tracker()->record_old_marking_increment(old_generation, GCId::current(), false,\n+                                                          heap->collection_set()->has_old_regions());\n@@ -700,1 +713,1 @@\n-void ShenandoahControlThread::service_concurrent_cycle(const ShenandoahHeap* heap,\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahHeap* heap,\n@@ -716,0 +729,26 @@\n+  const char* msg;\n+  if (heap->mode()->is_generational()) {\n+    if (heap->cancelled_gc()) {\n+      msg = (generation->is_young()) ? \"At end of Interrupted Concurrent Young GC\" :\n+                                       \"At end of Interrupted Concurrent Bootstrap GC\";\n+    } else {\n+      msg = (generation->is_young()) ? \"At end of Concurrent Young GC\" :\n+                                       \"At end of Concurrent Bootstrap GC\";\n+      \/\/ We only record GC results if GC was successful\n+      ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n+      if (generation->is_young()) {\n+        if (heap->collection_set()->has_old_regions()) {\n+          bool mixed_is_done = (heap->old_heuristics()->unprocessed_old_collection_candidates() == 0);\n+          mmu_tracker->record_mixed(generation, get_gc_id(), mixed_is_done);\n+        } else {\n+          mmu_tracker->record_young(generation, get_gc_id());\n+        }\n+      } else {\n+        mmu_tracker->record_bootstrap(generation, get_gc_id(), heap->collection_set()->has_old_regions());\n+      }\n+    }\n+  } else {\n+    msg = heap->cancelled_gc() ? \"At end of cancelled GC\" :\n+                                 \"At end of GC\";\n+  }\n+  heap->log_heap_status(msg);\n@@ -931,1 +970,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":65,"deletions":27,"binary":false,"changes":92,"status":"modified"},{"patch":"@@ -91,0 +91,1 @@\n+  ShenandoahSharedFlag _humongous_alloc_failure_gc;\n@@ -176,1 +177,1 @@\n-  void service_concurrent_normal_cycle(const ShenandoahHeap* heap,\n+  void service_concurrent_normal_cycle(ShenandoahHeap* heap,\n@@ -180,1 +181,1 @@\n-  void service_concurrent_old_cycle(const ShenandoahHeap* heap,\n+  void service_concurrent_old_cycle(ShenandoahHeap* heap,\n@@ -194,1 +195,1 @@\n-  void service_concurrent_cycle(const ShenandoahHeap* heap,\n+  void service_concurrent_cycle(ShenandoahHeap* heap,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -60,1 +60,5 @@\n-    heap->log_heap_status(\"At end of Degenerated GC\");\n+    bool is_bootstrap_gc = heap->is_concurrent_old_mark_in_progress() && _generation->is_young();\n+    heap->mmu_tracker()->record_degenerated(_generation, GCId::current(), is_bootstrap_gc,\n+                                            !heap->collection_set()->has_old_regions());\n+    const char* msg = is_bootstrap_gc? \"At end of Degenerated Boostrap Old GC\": \"At end of Degenerated GC\";\n+    heap->log_heap_status(msg);\n@@ -275,0 +279,35 @@\n+      \/\/ We defer generation resizing actions until after cset regions have been recycled.\n+      if (heap->mode()->is_generational()) {\n+        size_t old_region_surplus = heap->get_old_region_surplus();\n+        size_t old_region_deficit = heap->get_old_region_deficit();\n+        bool success;\n+        size_t region_xfer;\n+        const char* region_destination;\n+        if (old_region_surplus) {\n+          region_xfer = old_region_surplus;\n+          region_destination = \"young\";\n+          success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        } else if (old_region_deficit) {\n+          region_xfer = old_region_surplus;\n+          region_destination = \"old\";\n+          success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+          if (!success) {\n+            ((ShenandoahOldHeuristics *) heap->old_generation()->heuristics())->trigger_cannot_expand();\n+          }\n+        } else {\n+          region_destination = \"none\";\n+          region_xfer = 0;\n+          success = true;\n+        }\n+\n+        size_t young_available = heap->young_generation()->available();\n+        size_t old_available = heap->old_generation()->available();\n+        log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                           SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                           success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                           byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                           byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+\n+        heap->set_old_region_surplus(0);\n+        heap->set_old_region_deficit(0);\n+      }\n@@ -281,9 +320,2 @@\n-    \/\/ In case degeneration interrupted concurrent evacuation or update references,\n-    \/\/ we need to clean up transient state. Otherwise, these actions have no effect.\n-\n-    heap->young_generation()->unadjust_available();\n-    heap->old_generation()->unadjust_available();\n-    \/\/ No need to old_gen->increase_used(). That was done when plabs were allocated,\n-    \/\/ accounting for both old evacs and promotions.\n-\n-    heap->set_alloc_supplement_reserve(0);\n+    \/\/ In case degeneration interrupted concurrent evacuation or update references, we need to clean up transient state.\n+    \/\/ Otherwise, these actions have no effect.\n@@ -294,2 +326,0 @@\n-\n-    heap->adjust_generation_sizes();\n@@ -357,1 +387,10 @@\n-  if (!heap->collection_set()->is_empty()) {\n+  size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+  size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n+  if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+    \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+    \/\/ Degenerated evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+    if (ShenandoahVerify) {\n+      heap->verifier()->verify_before_evacuation();\n+    }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":52,"deletions":13,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -41,1 +41,0 @@\n-\n@@ -165,3 +164,10 @@\n-  assert (((region_capacity < _region_size_bytes) && (orig_set == Mutator) && (new_set == Collector)) ||\n-          ((region_capacity == _region_size_bytes) && (orig_set == Mutator) && (new_set == Collector || new_set == OldCollector)),\n-          \"Unexpected movement between sets\");\n+  \/\/ At start of update refs:\n+  \/\/                  Collector => Mutator\n+  \/\/                  OldCollector Empty => Mutator\n+  assert (((region_capacity <= _region_size_bytes) &&\n+           ((orig_set == Mutator) && (new_set == Collector)) ||\n+           ((orig_set == Collector) && (new_set == Mutator))) ||\n+          ((region_capacity == _region_size_bytes) &&\n+           ((orig_set == Mutator) && (new_set == Collector)) ||\n+           ((orig_set == OldCollector) && (new_set == Mutator)) ||\n+           (new_set == OldCollector)), \"Unexpected movement between sets\");\n@@ -401,1 +407,0 @@\n-\n@@ -415,0 +420,1 @@\n+\n@@ -453,1 +459,13 @@\n-HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region) {\n+void ShenandoahFreeSet::add_old_collector_free_region(ShenandoahHeapRegion* region) {\n+  shenandoah_assert_heaplocked();\n+  size_t idx = region->index();\n+  size_t capacity = alloc_capacity(region);\n+  assert(_free_sets.membership(idx) == NotFree, \"Regions promoted in place should not be in any free set\");\n+  if (capacity >= PLAB::min_size() * HeapWordSize) {\n+    _free_sets.make_free(idx, OldCollector, capacity);\n+    _heap->augment_promo_reserve(capacity);\n+  }\n+}\n+\n+HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahAffiliation affiliation,\n+                                                       ShenandoahAllocRequest& req, bool& in_new_region) {\n@@ -472,1 +490,2 @@\n-  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT, shenandoah_affiliation_name(affiliation), p2i(&req));\n+  log_debug(gc, free)(\"Could not allocate collector region with affiliation: %s for request \" PTR_FORMAT,\n+                      shenandoah_affiliation_name(affiliation), p2i(&req));\n@@ -497,2 +516,2 @@\n-        \/\/ Note: unsigned result from adjusted_unaffiliated_regions() will never be less than zero, but it may equal zero.\n-        if (_heap->old_generation()->adjusted_unaffiliated_regions() <= 0) {\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n@@ -504,2 +523,2 @@\n-        \/\/ Note: unsigned result from adjusted_unaffiliated_regions() will never be less than zero, but it may equal zero.\n-        if (_heap->young_generation()->adjusted_unaffiliated_regions() <= 0) {\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n@@ -518,1 +537,0 @@\n-\n@@ -527,2 +545,3 @@\n-          HeapWord* result = try_allocate_in(r, req, in_new_region);\n-          if (result != nullptr) {\n+          HeapWord* result;\n+          size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+          if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n@@ -579,1 +598,0 @@\n-\n@@ -585,7 +603,5 @@\n-      \/\/ TODO:\n-      \/\/ if (!allow_new_region && req.is_old() && (young_generation->adjusted_unaffiliated_regions() > 0)) {\n-      \/\/   transfer a region from young to old;\n-      \/\/   allow_new_region = true;\n-      \/\/   heap->set_old_evac_reserve(heap->get_old_evac_reserve() + region_size_bytes);\n-      \/\/ }\n-      \/\/\n+      if (!allow_new_region && req.is_old() && (_heap->young_generation()->free_unaffiliated_regions() > 0)) {\n+        \/\/ This allows us to flip a mutator region to old_collector\n+        allow_new_region = true;\n+      }\n+\n@@ -709,0 +725,3 @@\n+      _heap->old_generation()->increment_affiliated_region_count();\n+    } else {\n+      _heap->young_generation()->increment_affiliated_region_count();\n@@ -730,0 +749,2 @@\n+      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+      assert(_free_sets.in_free_set(r->index(), OldCollector), \"PLABS must be allocated in old_collector_free regions\");\n@@ -841,1 +862,1 @@\n-    size_t avail_young_regions = generation->adjusted_unaffiliated_regions();\n+    size_t avail_young_regions = generation->free_unaffiliated_regions();\n@@ -911,0 +932,2 @@\n+  _heap->young_generation()->increase_affiliated_region_count(num);\n+\n@@ -928,0 +951,5 @@\n+bool ShenandoahFreeSet::can_allocate_from(size_t idx) const {\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+  return can_allocate_from(r);\n+}\n+\n@@ -979,0 +1007,1 @@\n+  \/\/ Note: can_allocate_from(r) means r is entirely empty\n@@ -984,3 +1013,3 @@\n-\n-  \/\/ We do not ensure that the region is no longer trash,\n-  \/\/ relying on try_allocate_in(), which always comes next,\n+  _heap->generation_sizer()->force_transfer_to_old(1);\n+  _heap->augment_old_evac_reserve(region_capacity);\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n@@ -1000,2 +1029,1 @@\n-  \/\/ We do not ensure that the region is no longer trash,\n-  \/\/ relying on try_allocate_in(), which always comes next,\n+  \/\/ We do not ensure that the region is no longer trash, relying on try_allocate_in(), which always comes next,\n@@ -1019,1 +1047,1 @@\n-void ShenandoahFreeSet::find_regions_with_alloc_capacity() {\n+void ShenandoahFreeSet::find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions) {\n@@ -1021,0 +1049,2 @@\n+  old_cset_regions = 0;\n+  young_cset_regions = 0;\n@@ -1023,0 +1053,9 @@\n+    if (region->is_trash()) {\n+      \/\/ Trashed regions represent regions that had been in the collection set but have not yet been \"cleaned up\".\n+      if (region->is_old()) {\n+        old_cset_regions++;\n+      } else {\n+        assert(region->is_young(), \"Trashed region should be old or young\");\n+        young_cset_regions++;\n+      }\n+    }\n@@ -1047,1 +1086,64 @@\n-void ShenandoahFreeSet::rebuild() {\n+\/\/ Move no more than cset_regions from the existing Collector and OldCollector free sets to the Mutator free set.\n+\/\/ This is called from outside the heap lock.\n+void ShenandoahFreeSet::move_collector_sets_to_mutator(size_t max_xfer_regions) {\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t collector_empty_xfer = 0;\n+  size_t collector_not_empty_xfer = 0;\n+  size_t old_collector_empty_xfer = 0;\n+\n+  \/\/ Process empty regions within the Collector free set\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(Collector) <= _free_sets.rightmost_empty(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost_empty(Collector);\n+         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(Collector)); idx++) {\n+      if (_free_sets.in_free_set(idx, Collector) && can_allocate_from(idx)) {\n+        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        collector_empty_xfer += region_size_bytes;\n+      }\n+    }\n+  }\n+\n+  \/\/ Process empty regions within the OldCollector free set\n+  size_t old_collector_regions = 0;\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost_empty(OldCollector) <= _free_sets.rightmost_empty(OldCollector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost_empty(OldCollector);\n+         (max_xfer_regions > 0) && (idx <= _free_sets.rightmost_empty(OldCollector)); idx++) {\n+      if (_free_sets.in_free_set(idx, OldCollector) && can_allocate_from(idx)) {\n+        _free_sets.move_to_set(idx, Mutator, region_size_bytes);\n+        max_xfer_regions--;\n+        old_collector_empty_xfer += region_size_bytes;\n+        old_collector_regions++;\n+      }\n+    }\n+  }\n+  if (old_collector_regions > 0) {\n+    _heap->generation_sizer()->transfer_to_young(old_collector_regions);\n+  }\n+\n+  \/\/ If there are any non-empty regions within Collector set, we can also move them to the Mutator free set\n+  if ((max_xfer_regions > 0) && (_free_sets.leftmost(Collector) <= _free_sets.rightmost(Collector))) {\n+    ShenandoahHeapLocker locker(_heap->lock());\n+    for (size_t idx = _free_sets.leftmost(Collector); (max_xfer_regions > 0) && (idx <= _free_sets.rightmost(Collector)); idx++) {\n+      size_t alloc_capacity = this->alloc_capacity(idx);\n+      if (_free_sets.in_free_set(idx, Collector) && (alloc_capacity > 0)) {\n+        _free_sets.move_to_set(idx, Mutator, alloc_capacity);\n+        max_xfer_regions--;\n+        collector_not_empty_xfer += alloc_capacity;\n+      }\n+    }\n+  }\n+\n+  size_t collector_xfer = collector_empty_xfer + collector_not_empty_xfer;\n+  size_t total_xfer = collector_xfer + old_collector_empty_xfer;\n+  log_info(gc, free)(\"At start of update refs, moving \" SIZE_FORMAT \"%s to Mutator free set from Collector Reserve (\"\n+                     SIZE_FORMAT \"%s) and from Old Collector Reserve (\" SIZE_FORMAT \"%s)\",\n+                     byte_size_in_proper_unit(total_xfer), proper_unit_for_byte_size(total_xfer),\n+                     byte_size_in_proper_unit(collector_xfer), proper_unit_for_byte_size(collector_xfer),\n+                     byte_size_in_proper_unit(old_collector_empty_xfer), proper_unit_for_byte_size(old_collector_empty_xfer));\n+}\n+\n+\n+\/\/ Overwrite arguments to represent the amount of memory in each generation that is about to be recycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions) {\n@@ -1051,1 +1153,0 @@\n-\n@@ -1056,1 +1157,44 @@\n-  find_regions_with_alloc_capacity();\n+  find_regions_with_alloc_capacity(young_cset_regions, old_cset_regions);\n+}\n+\n+void ShenandoahFreeSet::rebuild(size_t young_cset_regions, size_t old_cset_regions) {\n+  shenandoah_assert_heaplocked();\n+  size_t young_reserve, old_reserve;\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  size_t old_capacity = _heap->old_generation()->max_capacity();\n+  size_t old_available = _heap->old_generation()->available();\n+  size_t old_unaffiliated_regions = _heap->old_generation()->free_unaffiliated_regions();\n+  size_t young_capacity = _heap->young_generation()->max_capacity();\n+  size_t young_available = _heap->young_generation()->available();\n+  size_t young_unaffiliated_regions = _heap->young_generation()->free_unaffiliated_regions();\n+\n+  old_unaffiliated_regions += old_cset_regions;\n+  old_available += old_cset_regions * region_size_bytes;\n+  young_unaffiliated_regions += young_cset_regions;\n+  young_available += young_cset_regions * region_size_bytes;\n+\n+  \/\/ Consult old-region surplus and deficit to make adjustments to current generation capacities and availability.\n+  \/\/ The generation region transfers take place after we rebuild.\n+  size_t old_region_surplus = _heap->get_old_region_surplus();\n+  size_t old_region_deficit = _heap->get_old_region_deficit();\n+\n+  if (old_region_surplus > 0) {\n+    size_t xfer_bytes = old_region_surplus * region_size_bytes;\n+    assert(old_region_surplus <= old_unaffiliated_regions, \"Cannot transfer regions that are affiliated\");\n+    old_capacity -= xfer_bytes;\n+    old_available -= xfer_bytes;\n+    old_unaffiliated_regions -= old_region_surplus;\n+    young_capacity += xfer_bytes;\n+    young_available += xfer_bytes;\n+    young_unaffiliated_regions += old_region_surplus;\n+  } else if (old_region_deficit > 0) {\n+    size_t xfer_bytes = old_region_deficit * region_size_bytes;\n+    assert(old_region_deficit <= young_unaffiliated_regions, \"Cannot transfer regions that are affiliated\");\n+    old_capacity += xfer_bytes;\n+    old_available += xfer_bytes;\n+    old_unaffiliated_regions += old_region_deficit;\n+    young_capacity -= xfer_bytes;;\n+    young_available -= xfer_bytes;\n+    young_unaffiliated_regions -= old_region_deficit;\n+  }\n@@ -1060,1 +1204,0 @@\n-  size_t young_reserve, old_reserve;\n@@ -1073,0 +1216,3 @@\n+      assert(old_reserve <= old_available,\n+             \"Cannot reserve (\" SIZE_FORMAT \" + \" SIZE_FORMAT\") more OLD than is available: \" SIZE_FORMAT,\n+             _heap->get_promoted_reserve(), _heap->get_old_evac_reserve(), old_available);\n@@ -1075,3 +1221,5 @@\n-      young_reserve = (_heap->young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-      old_reserve = MAX2((_heap->old_generation()->max_capacity() * ShenandoahOldEvacReserve) \/ 100,\n-                         ShenandoahOldCompactionReserve * ShenandoahHeapRegion::region_size_bytes());\n+      young_reserve = (young_capacity * ShenandoahEvacReserve) \/ 100;\n+      \/\/ The auto-sizer has already made old-gen large enough to hold all anticipated evacuations and promotions.\n+      \/\/ Affiliated old-gen regions are already in the OldCollector free set.  Add in the relevant number of\n+      \/\/ unaffiliated regions.\n+      old_reserve = old_available;\n@@ -1080,1 +1228,12 @@\n-  reserve_regions(young_reserve, old_reserve);\n+  if (old_reserve > _free_sets.capacity_of(OldCollector)) {\n+    \/\/ Old available regions that have less than PLAB::min_size() of available memory are not placed into the OldCollector\n+    \/\/ free set.  Because of this, old_available may not have enough memory to represent the intended reserve.  Adjust\n+    \/\/ the reserve downward to account for this possibility. This loss is part of the reason why the original budget\n+    \/\/ was adjusted with ShenandoahOldEvacWaste and ShenandoahOldPromoWaste multipliers.\n+    if (old_reserve > _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes) {\n+      old_reserve = _free_sets.capacity_of(OldCollector) + old_unaffiliated_regions * region_size_bytes;\n+    }\n+  }\n+  if (young_reserve > young_unaffiliated_regions * region_size_bytes) {\n+    young_reserve = young_unaffiliated_regions * region_size_bytes;\n+  }\n@@ -1082,0 +1241,1 @@\n+  reserve_regions(young_reserve, old_reserve);\n@@ -1093,1 +1253,2 @@\n-  for (size_t idx = _heap->num_regions() - 1; idx > 0; idx--) {\n+  for (size_t i = _heap->num_regions(); i > 0; i--) {\n+    size_t idx = i - 1;\n@@ -1131,0 +1292,11 @@\n+    size_t retired_young_waste = 0;\n+    size_t retired_old_waste = 0;\n+    size_t consumed_collector = 0;\n+    size_t consumed_old_collector = 0;\n+    size_t consumed_mutator = 0;\n+    size_t available_old = 0;\n+    size_t available_young = 0;\n+    size_t available_mutator = 0;\n+    size_t available_collector = 0;\n+    size_t available_old_collector = 0;\n+\n@@ -1154,1 +1326,4 @@\n-        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'M': 'm';\n+        size_t capacity = alloc_capacity(r);\n+        available_mutator += capacity;\n+        consumed_mutator += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'M': 'm';\n@@ -1157,1 +1332,4 @@\n-        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'C': 'c';\n+        size_t capacity = alloc_capacity(r);\n+        available_collector += capacity;\n+        consumed_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'C': 'c';\n@@ -1159,1 +1337,4 @@\n-        buffer[idx] = (alloc_capacity(r) == region_size_bytes)? 'O': 'o';\n+        size_t capacity = alloc_capacity(r);\n+        available_old_collector += capacity;\n+        consumed_old_collector += region_size_bytes - capacity;\n+        buffer[idx] = (capacity == region_size_bytes)? 'O': 'o';\n@@ -1171,0 +1352,1 @@\n+          retired_old_waste += alloc_capacity(r);\n@@ -1174,0 +1356,1 @@\n+          retired_young_waste += alloc_capacity(r);\n@@ -1187,6 +1370,0 @@\n-    log_info(gc, free)(\"Retired young: \" SIZE_FORMAT \"%s (including humongous: \" SIZE_FORMAT \"%s), old: \" SIZE_FORMAT\n-                       \"%s (including humongous: \" SIZE_FORMAT \"%s)\",\n-                       byte_size_in_proper_unit(total_young),             proper_unit_for_byte_size(total_young),\n-                       byte_size_in_proper_unit(retired_young_humongous), proper_unit_for_byte_size(retired_young_humongous),\n-                       byte_size_in_proper_unit(total_old),               proper_unit_for_byte_size(total_old),\n-                       byte_size_in_proper_unit(retired_old_humongous),   proper_unit_for_byte_size(retired_old_humongous));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":224,"deletions":47,"binary":false,"changes":271,"status":"modified"},{"patch":"@@ -75,1 +75,1 @@\n-  \/\/ Place region idx into free set new_set.  Requires that idx is currently not NotFRee.\n+  \/\/ Place region idx into free set new_set.  Requires that idx is currently not NotFree.\n@@ -175,0 +175,8 @@\n+  void adjust_bounds_for_additional_old_collector_free_region(size_t idx);\n+\n+  void recompute_bounds();\n+  void adjust_bounds();\n+  bool touches_bounds(size_t num) const;\n+\n+  \/\/ Used of free set represents the amount of is_mutator_free set that has been consumed since most recent rebuild.\n+  void increase_used(size_t amount);\n@@ -180,0 +188,1 @@\n+  bool can_allocate_from(size_t idx) const;\n@@ -191,1 +200,5 @@\n-  void rebuild();\n+  void prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions);\n+  void rebuild(size_t young_cset_regions, size_t old_cset_regions);\n+  void move_collector_sets_to_mutator(size_t cset_regions);\n+\n+  void add_old_collector_free_region(ShenandoahHeapRegion* region);\n@@ -212,1 +225,1 @@\n-  void find_regions_with_alloc_capacity();\n+  void find_regions_with_alloc_capacity(size_t &young_cset_regions, size_t &old_cset_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":16,"deletions":3,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -178,0 +178,1 @@\n+    heap->mmu_tracker()->record_full(heap->global_generation(), GCId::current());\n@@ -182,1 +183,1 @@\n-    assert(heap->old_generation()->used_regions_size() <= heap->old_generation()->adjusted_capacity(),\n+    assert(heap->old_generation()->used_regions_size() <= heap->old_generation()->max_capacity(),\n@@ -184,1 +185,1 @@\n-    assert(heap->young_generation()->used_regions_size() <= heap->young_generation()->adjusted_capacity(),\n+    assert(heap->young_generation()->used_regions_size() <= heap->young_generation()->max_capacity(),\n@@ -186,0 +187,6 @@\n+\n+    assert((heap->young_generation()->used() + heap->young_generation()->get_humongous_waste())\n+           <= heap->young_generation()->used_regions_size(), \"Young consumed can be no larger than span of affiliated regions\");\n+    assert((heap->old_generation()->used() + heap->old_generation()->get_humongous_waste())\n+           <= heap->old_generation()->used_regions_size(), \"Old consumed can be no larger than span of affiliated regions\");\n+\n@@ -202,3 +209,0 @@\n-    \/\/ Defer unadjust_available() invocations until after Full GC finishes its efforts because Full GC makes use\n-    \/\/ of young-gen memory that may have been loaned from old-gen.\n-\n@@ -206,1 +210,0 @@\n-    heap->set_alloc_supplement_reserve(0);\n@@ -345,2 +348,0 @@\n-  heap->adjust_generation_sizes();\n-\n@@ -360,4 +361,1 @@\n-  \/\/ Having reclaimed all dead memory, it is now safe to restore capacities to original values.\n-  heap->young_generation()->unadjust_available();\n-  heap->old_generation()->unadjust_available();\n-\n+  \/\/ Humongous regions are promoted on demand and are accounted for by normal Full GC mechanisms.\n@@ -1343,1 +1341,0 @@\n-\n@@ -1504,4 +1501,19 @@\n-    log_info(gc)(\"FullGC done: global usage: \" SIZE_FORMAT \"%s, young usage: \" SIZE_FORMAT \"%s, old usage: \" SIZE_FORMAT \"%s\",\n-                 byte_size_in_proper_unit(heap->global_generation()->used()), proper_unit_for_byte_size(heap->global_generation()->used()),\n-                 byte_size_in_proper_unit(heap->young_generation()->used()),  proper_unit_for_byte_size(heap->young_generation()->used()),\n-                 byte_size_in_proper_unit(heap->old_generation()->used()),    proper_unit_for_byte_size(heap->old_generation()->used()));\n+    if (heap->mode()->is_generational()) {\n+      size_t old_usage = heap->old_generation()->used_regions_size();\n+      size_t old_capacity = heap->old_generation()->max_capacity();\n+\n+      assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must aligh with region size\");\n+      assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must aligh with region size\");\n+\n+      if (old_capacity > old_usage) {\n+        size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n+        heap->generation_sizer()->transfer_to_young(excess_old_regions);\n+      } else if (old_capacity < old_usage) {\n+        size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n+        heap->generation_sizer()->transfer_to_old(old_regions_deficit);\n+      }\n+\n+      log_info(gc)(\"FullGC done: young usage: \" SIZE_FORMAT \"%s, old usage: \" SIZE_FORMAT \"%s\",\n+                   byte_size_in_proper_unit(heap->young_generation()->used()), proper_unit_for_byte_size(heap->young_generation()->used()),\n+                   byte_size_in_proper_unit(heap->old_generation()->used()),   proper_unit_for_byte_size(heap->old_generation()->used()));\n+    }\n@@ -1509,1 +1521,59 @@\n-    heap->free_set()->rebuild();\n+    size_t young_cset_regions, old_cset_regions;\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+\n+    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n+    \/\/ no longer have objects separated by age into distinct regions.\n+\n+    \/\/ TODO: Do we need to fix FullGC so that it maintains aged segregation of objects into distinct regions?\n+    \/\/       A partial solution would be to remember how many objects are of tenure age following Full GC, but\n+    \/\/       this is probably suboptimal, because most of these objects will not reside in a region that will be\n+    \/\/       selected for the next evacuation phase.\n+\n+    \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n+    heap->clear_promotion_potential();\n+    heap->clear_promotion_in_place_potential();\n+\n+    if (heap->mode()->is_generational()) {\n+      \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n+      heap->adjust_generation_sizes_for_next_cycle(0, 0, 0);\n+    }\n+    heap->free_set()->rebuild(young_cset_regions, old_cset_regions);\n+\n+    \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+    \/\/ abbreviated cycle.\n+    if (heap->mode()->is_generational()) {\n+      bool success;\n+      size_t region_xfer;\n+      const char* region_destination;\n+      ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+      ShenandoahGeneration* old_gen = heap->old_generation();\n+\n+      size_t old_region_surplus = heap->get_old_region_surplus();\n+      size_t old_region_deficit = heap->get_old_region_deficit();\n+      if (old_region_surplus) {\n+        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        region_destination = \"young\";\n+        region_xfer = old_region_surplus;\n+      } else if (old_region_deficit) {\n+        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+        region_destination = \"old\";\n+        region_xfer = old_region_deficit;\n+        if (!success) {\n+          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+        }\n+      } else {\n+        region_destination = \"none\";\n+        region_xfer = 0;\n+        success = true;\n+      }\n+      heap->set_old_region_surplus(0);\n+      heap->set_old_region_deficit(0);\n+      size_t young_available = young_gen->available();\n+      size_t old_available = old_gen->available();\n+      log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                         SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                         success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                         byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                         byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+    }\n+    heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n@@ -1511,1 +1581,0 @@\n-  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":88,"deletions":19,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -228,1 +228,0 @@\n-  assert(heap->mode()->is_generational(), \"Only generational mode uses evacuation budgets.\");\n@@ -252,23 +251,0 @@\n-  if (heap->doing_mixed_evacuations()) {\n-    \/\/ Compute old_evacuation_reserve: how much memory are we reserving to hold the results of\n-    \/\/ evacuating old-gen heap regions?  In order to sustain a consistent pace of young-gen collections,\n-    \/\/ the goal is to maintain a consistent value for this parameter (when the candidate set is not\n-    \/\/ empty).  This value is the minimum of:\n-    \/\/   1. old_gen->available()\n-    \/\/   2. old-gen->capacity() * ShenandoahOldEvacReserve) \/ 100\n-    \/\/       (e.g. old evacuation should be no larger than 5% of old_gen capacity)\n-    \/\/   3. ((young_gen->capacity * ShenandoahEvacReserve \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100\n-    \/\/       (e.g. old evacuation should be no larger than 12% of young-gen evacuation)\n-    old_evacuation_reserve = old_generation->available();\n-    \/\/ This assertion has been disabled because we expect this code to be replaced by 05\/2023\n-    \/\/ assert(old_evacuation_reserve > minimum_evacuation_reserve, \"Old-gen available has not been preserved!\");\n-    size_t old_evac_reserve_max = old_generation->max_capacity() * ShenandoahOldEvacReserve \/ 100;\n-    if (old_evac_reserve_max < old_evacuation_reserve) {\n-      old_evacuation_reserve = old_evac_reserve_max;\n-    }\n-    young_evac_reserve_max =\n-      (((young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100;\n-    if (young_evac_reserve_max < old_evacuation_reserve) {\n-      old_evacuation_reserve = young_evac_reserve_max;\n-    }\n-  }\n@@ -276,5 +252,1 @@\n-  if (minimum_evacuation_reserve > old_generation->available()) {\n-    \/\/ Due to round-off errors during enforcement of minimum_evacuation_reserve during previous GC passes,\n-    \/\/ there can be slight discrepancies here.\n-    minimum_evacuation_reserve = old_generation->available();\n-  }\n+  \/\/ First priority is to reclaim the easy garbage out of young-gen.\n@@ -282,89 +254,6 @@\n-  heap->set_old_evac_reserve(old_evacuation_reserve);\n-  heap->reset_old_evac_expended();\n-\n-  \/\/ Compute the young evacuation reserve: This is how much memory is available for evacuating young-gen objects.\n-  \/\/ We ignore the possible effect of promotions, which reduce demand for young-gen evacuation memory.\n-  \/\/\n-  \/\/ TODO: We could give special treatment to the regions that have reached promotion age, because we know their\n-  \/\/ live data is entirely eligible for promotion.  This knowledge can feed both into calculations of young-gen\n-  \/\/ evacuation reserve and promotion reserve.\n-  \/\/\n-  \/\/  young_evacuation_reserve for young generation: how much memory are we reserving to hold the results\n-  \/\/  of evacuating young collection set regions?  This is typically smaller than the total amount\n-  \/\/  of available memory, and is also smaller than the total amount of marked live memory within\n-  \/\/  young-gen.  This value is the smaller of\n-  \/\/\n-  \/\/    1. (young_gen->capacity() * ShenandoahEvacReserve) \/ 100\n-  \/\/    2. (young_gen->available() + old_gen_memory_available_to_be_loaned\n-  \/\/\n-  \/\/  ShenandoahEvacReserve represents the configured target size of the evacuation region.  We can only honor\n-  \/\/  this target if there is memory available to hold the evacuations.  Memory is available if it is already\n-  \/\/  free within young gen, or if it can be borrowed from old gen.  Since we have not yet chosen the collection\n-  \/\/  sets, we do not yet know the exact accounting of how many regions will be freed by this collection pass.\n-  \/\/  What we do know is that there will be at least one evacuated young-gen region for each old-gen region that\n-  \/\/  is loaned to the evacuation effort (because regions to be collected consume more memory than the compacted\n-  \/\/  regions that will replace them).  In summary, if there are old-gen regions that are available to hold the\n-  \/\/  results of young-gen evacuations, it is safe to loan them for this purpose.  At this point, we have not yet\n-  \/\/  established a promoted_reserve.  We'll do that after we choose the collection set and analyze its impact\n-  \/\/  on available memory.\n-  \/\/\n-  \/\/ We do not know the evacuation_supplement until after we have computed the collection set.  It is not always\n-  \/\/ the case that young-regions inserted into the collection set will result in net decrease of in-use regions\n-  \/\/ because ShenandoahEvacWaste times multiplied by memory within the region may be larger than the region size.\n-  \/\/ The problem is especially relevant to regions that have been inserted into the collection set because they have\n-  \/\/ reached tenure age.  These regions tend to have much higher utilization (e.g. 95%).  These regions also offer\n-  \/\/ a unique opportunity because we know that every live object contained within the region is elgible to be\n-  \/\/ promoted.  Thus, the following implementation treats these regions specially:\n-  \/\/\n-  \/\/  1. Before beginning collection set selection, we tally the total amount of live memory held within regions\n-  \/\/     that are known to have reached tenure age.  If this memory times ShenandoahEvacWaste is available within\n-  \/\/     old-gen memory, establish an advance promotion reserve to hold all or some percentage of these objects.\n-  \/\/     This advance promotion reserve is excluded from memory available for holding old-gen evacuations and cannot\n-  \/\/     be \"loaned\" to young gen.\n-  \/\/\n-  \/\/  2. Tenure-aged regions are included in the collection set iff their evacuation size * ShenandoahEvacWaste fits\n-  \/\/     within the advance promotion reserve.  It is counter productive to evacuate these regions if they cannot be\n-  \/\/     evacuated directly into old-gen memory.  So if there is not sufficient memory to hold copies of their\n-  \/\/     live data right now, we'll just let these regions remain in young for now, to be evacuated by a subsequent\n-  \/\/     evacuation pass.\n-  \/\/\n-  \/\/  3. Next, we calculate a young-gen evacuation budget, which is the smaller of the two quantities mentioned\n-  \/\/     above.  old_gen_memory_available_to_be_loaned is calculated as:\n-  \/\/       old_gen->available - (advance-promotion-reserve + old-gen_evacuation_reserve)\n-  \/\/\n-  \/\/  4. When choosing the collection set, special care is taken to assure that the amount of loaned memory required to\n-  \/\/     hold the results of evacuation is smaller than the total memory occupied by the regions added to the collection\n-  \/\/     set.  We need to take these precautions because we do not know how much memory will be reclaimed by evacuation\n-  \/\/     until after the collection set has been constructed.  The algorithm is as follows:\n-  \/\/\n-  \/\/     a. We feed into the algorithm (i) young available at the start of evacuation and (ii) the amount of memory\n-  \/\/        loaned from old-gen that is available to hold the results of evacuation.\n-  \/\/     b. As candidate regions are added into the young-gen collection set, we maintain accumulations of the amount\n-  \/\/        of memory spanned by the collection set regions and the amount of memory that must be reserved to hold\n-  \/\/        evacuation results (by multiplying live-data size by ShenandoahEvacWaste).  We process candidate regions\n-  \/\/        in order of decreasing amounts of garbage.  We skip over (and do not include into the collection set) any\n-  \/\/        regions that do not satisfy all of the following conditions:\n-  \/\/\n-  \/\/          i. The amount of live data within the region as scaled by ShenandoahEvacWaste must fit within the\n-  \/\/             relevant evacuation reserve (live data of old-gen regions must fit within the old-evac-reserve, live\n-  \/\/             data of young-gen tenure-aged regions must fit within the advance promotion reserve, live data within\n-  \/\/             other young-gen regions must fit within the youn-gen evacuation reserve).\n-  \/\/         ii. The accumulation of memory consumed by evacuation must not exceed the accumulation of memory reclaimed\n-  \/\/             through evacuation by more than young-gen available.\n-  \/\/        iii. Other conditions may be enforced as appropriate for specific heuristics.\n-  \/\/\n-  \/\/       Note that regions are considered for inclusion in the selection set in order of decreasing amounts of garbage.\n-  \/\/       It is possible that a region with a larger amount of garbage will be rejected because it also has a larger\n-  \/\/       amount of live data and some region that follows this region in candidate order is included in the collection\n-  \/\/       set (because it has less live data and thus can fit within the evacuation limits even though it has less\n-  \/\/       garbage).\n-\n-  size_t young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-  \/\/ old evacuation can pack into existing partially used regions.  young evacuation and loans for young allocations\n-  \/\/ need to target regions that do not already hold any old-gen objects.  Round down.\n-  regions_available_to_loan = old_generation->free_unaffiliated_regions();\n-\n-  size_t required_evacuation_reserve;\n-  \/\/ Memory evacuated from old-gen on this pass will be available to hold old-gen evacuations in next pass.\n-  if (old_evacuation_reserve > minimum_evacuation_reserve) {\n-    required_evacuation_reserve = 0;\n+  \/\/ maximum_young_evacuation_reserve is upper bound on memory to be evacuated out of young\n+  size_t maximum_young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  size_t young_evacuation_reserve = maximum_young_evacuation_reserve;\n+  size_t excess_young;\n+  if (young_generation->available() > young_evacuation_reserve) {\n+    excess_young = young_generation->available() - young_evacuation_reserve;\n@@ -372,1 +261,2 @@\n-    required_evacuation_reserve = minimum_evacuation_reserve - old_evacuation_reserve;\n+    young_evacuation_reserve = young_generation->available();\n+    excess_young = 0;\n@@ -374,8 +264,45 @@\n-\n-  consumed_by_advance_promotion = _heuristics->select_aged_regions(\n-    old_generation->available() - old_evacuation_reserve - required_evacuation_reserve, num_regions, preselected_regions);\n-  size_t net_available_old_regions =\n-    (old_generation->available() - old_evacuation_reserve - consumed_by_advance_promotion) \/ region_size_bytes;\n-\n- if (regions_available_to_loan > net_available_old_regions) {\n-    regions_available_to_loan = net_available_old_regions;\n+  size_t unaffiliated_young = young_generation->free_unaffiliated_regions() * region_size_bytes;\n+  if (excess_young > unaffiliated_young) {\n+    excess_young = unaffiliated_young;\n+  } else {\n+    \/\/ round down to multiple of region size\n+    excess_young \/= region_size_bytes;\n+    excess_young *= region_size_bytes;\n+  }\n+  \/\/ excess_young is available to be transferred to OLD.  Assume that OLD will not request any more than had\n+  \/\/ already been set aside for its promotion and evacuation needs at the end of previous GC.  No need to\n+  \/\/ hold back memory for allocation runway.\n+\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+\n+  \/\/ maximum_old_evacuation_reserve is an upper bound on memory evacuated from old and evacuated to old (promoted).\n+  size_t maximum_old_evacuation_reserve =\n+    maximum_young_evacuation_reserve * ShenandoahOldEvacRatioPercent \/ (100 - ShenandoahOldEvacRatioPercent);\n+  \/\/ Here's the algebra:\n+  \/\/  TotalEvacuation = OldEvacuation + YoungEvacuation\n+  \/\/  OldEvacuation = TotalEvacuation * (ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * (ShenandoahOldEvacRatioPercent\/100)\/(1 - ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * ShenandoahOldEvacRatioPercent\/(100 - ShenandoahOldEvacRatioPercent)\n+\n+  if (maximum_old_evacuation_reserve > old_generation->available()) {\n+    maximum_old_evacuation_reserve = old_generation->available();\n+  }\n+\n+  \/\/ Second priority is to reclaim garbage out of old-gen if there are old-gen collection candidates.  Third priority\n+  \/\/ is to promote as much as we have room to promote.  However, if old-gen memory is in short supply, this means young\n+  \/\/ GC is operating under \"duress\" and was unable to transfer the memory that we would normally expect.  In this case,\n+  \/\/ old-gen will refrain from compacting itself in order to allow a quicker young-gen cycle (by avoiding the update-refs\n+  \/\/ through ALL of old-gen).  If there is some memory available in old-gen, we will use this for promotions as promotions\n+  \/\/ do not add to the update-refs burden of GC.\n+\n+  size_t old_promo_reserve;\n+  if (old_heuristics->unprocessed_old_collection_candidates() > 0) {\n+    \/\/ We reserved all old-gen memory at end of previous GC to hold anticipated evacuations to old-gen.  If this is\n+    \/\/ mixed evacuation, reserve all of this memory for compaction of old-gen and do not promote.  Prioritize compaction\n+    \/\/ over promotion in order to defragment OLD so that it will be better prepared to efficiently receive promoted memory.\n+    old_evacuation_reserve = maximum_old_evacuation_reserve;\n+    old_promo_reserve = 0;\n+  } else {\n+    \/\/ Make all old-evacuation memory for promotion, but if we can't use it all for promotion, we'll allow some evacuation.\n+    old_evacuation_reserve = 0;\n+    old_promo_reserve = maximum_old_evacuation_reserve;\n@@ -384,2 +311,8 @@\n-  \/\/ Otherwise, regions_available_to_loan is less than net_available_old_regions because available memory is\n-  \/\/ scattered between multiple partially used regions.\n+  \/\/ We see too many old-evacuation failures if we force ourselves to evacuate into regions that are not initially empty.\n+  \/\/ So we limit the old-evacuation reserve to unfragmented memory.  Even so, old-evacuation is free to fill in nooks and\n+  \/\/ crannies within existing partially used regions and it generally tries to do so.\n+  size_t old_free_regions = old_generation->free_unaffiliated_regions();\n+  size_t old_free_unfragmented = old_free_regions * region_size_bytes;\n+  if (old_evacuation_reserve > old_free_unfragmented) {\n+    size_t delta = old_evacuation_reserve - old_free_unfragmented;\n+    old_evacuation_reserve -= delta;\n@@ -387,12 +320,2 @@\n-  if (young_evacuation_reserve > young_generation->available()) {\n-    size_t short_fall = young_evacuation_reserve - young_generation->available();\n-    if (regions_available_to_loan * region_size_bytes >= short_fall) {\n-      old_regions_loaned_for_young_evac = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n-      regions_available_to_loan -= old_regions_loaned_for_young_evac;\n-    } else {\n-      old_regions_loaned_for_young_evac = regions_available_to_loan;\n-      regions_available_to_loan = 0;\n-      young_evacuation_reserve = young_generation->available() + old_regions_loaned_for_young_evac * region_size_bytes;\n-      \/\/ In this case, there's no memory available for new allocations while evacuating and updating, unless we\n-      \/\/ find more old-gen memory to borrow below.\n-    }\n+    \/\/ Let promo consume fragments of old-gen memory.\n+    old_promo_reserve += delta;\n@@ -400,5 +323,0 @@\n-  \/\/ In generational mode, we may end up choosing a young collection set that contains so many promotable objects\n-  \/\/ that there is not sufficient space in old generation to hold the promoted objects.  That is ok because we have\n-  \/\/ assured there is sufficient space in young generation to hold the rejected promotion candidates.  These rejected\n-  \/\/ promotion candidates will presumably be promoted in a future evacuation cycle.\n-  heap->set_young_evac_reserve(young_evacuation_reserve);\n@@ -406,0 +324,11 @@\n+  consumed_by_advance_promotion = _heuristics->select_aged_regions(old_promo_reserve, num_regions, preselected_regions);\n+  assert(consumed_by_advance_promotion <= maximum_old_evacuation_reserve, \"Cannot promote more than available old-gen memory\");\n+  if (consumed_by_advance_promotion < old_promo_reserve) {\n+    \/\/ If we're in a global collection, this memory can be used for old evacuations\n+    old_evacuation_reserve += old_promo_reserve - consumed_by_advance_promotion;\n+  }\n+  heap->set_young_evac_reserve(young_evacuation_reserve);\n+  heap->set_old_evac_reserve(old_evacuation_reserve);\n+  heap->set_promoted_reserve(consumed_by_advance_promotion);\n+\n+  \/\/ There is no need to expand OLD because all memory used here was set aside at end of previous GC\n@@ -428,2 +357,0 @@\n-  assert(heap->mode()->is_generational(), \"Only generational mode uses evacuation budgets.\");\n-  size_t old_regions_loaned_for_young_evac, regions_available_to_loan;\n@@ -433,21 +360,0 @@\n-  size_t old_evacuated = collection_set->get_old_bytes_reserved_for_evacuation();\n-  size_t old_evacuated_committed = (size_t) (ShenandoahEvacWaste * old_evacuated);\n-  size_t old_evacuation_reserve = heap->get_old_evac_reserve();\n-  \/\/ Immediate garbage found during choose_collection_set() is all young\n-  size_t immediate_garbage = collection_set->get_immediate_trash();\n-  size_t old_available = old_generation->available();\n-  size_t young_available = young_generation->available() + immediate_garbage;\n-  size_t loaned_regions = 0;\n-  size_t available_loan_remnant = 0; \/\/ loaned memory that is not yet dedicated to any particular budget\n-\n-  \/\/ We expect this code to be replaced by 05\/01\/23.\n-  \/\/\n-  \/\/ assert(((consumed_by_advance_promotion * 33) \/ 32) >= collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste,\n-  \/\/       \"Advance promotion (\" SIZE_FORMAT \") should be at least young_bytes_to_be_promoted (\" SIZE_FORMAT\n-  \/\/       \")* ShenandoahEvacWaste, totalling: \" SIZE_FORMAT \", within round-off errors of up to 3.125%%\",\n-  \/\/       consumed_by_advance_promotion, collection_set->get_young_bytes_to_be_promoted(),\n-  \/\/       (size_t) (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste));\n-\n-  \/\/ assert(consumed_by_advance_promotion <= (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste * 33) \/ 32,\n-  \/\/       \"Round-off errors should be less than 3.125%%, consumed by advance: \" SIZE_FORMAT \", promoted: \" SIZE_FORMAT,\n-  \/\/       consumed_by_advance_promotion, (size_t) (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste));\n@@ -455,0 +361,1 @@\n+  \/\/ Preselected regions have been inserted into the collection set, so we no longer need the preselected array.\n@@ -457,0 +364,4 @@\n+  size_t old_evacuated = collection_set->get_old_bytes_reserved_for_evacuation();\n+  size_t old_evacuated_committed = (size_t) (ShenandoahOldEvacWaste * old_evacuated);\n+  size_t old_evacuation_reserve = heap->get_old_evac_reserve();\n+\n@@ -458,1 +369,1 @@\n-    \/\/ This should only happen due to round-off errors when enforcing ShenandoahEvacWaste\n+    \/\/ This should only happen due to round-off errors when enforcing ShenandoahOldEvacWaste\n@@ -463,0 +374,1 @@\n+    \/\/ Leave old_evac_reserve as previously configured\n@@ -464,1 +376,1 @@\n-    \/\/ This may happen if the old-gen collection consumes less than full budget.\n+    \/\/ This happens if the old-gen collection consumes less than full budget.\n@@ -469,4 +381,2 @@\n-  \/\/ Recompute old_regions_loaned_for_young_evac because young-gen collection set may not need all the memory\n-  \/\/ originally reserved.\n-  size_t young_promoted = collection_set->get_young_bytes_to_be_promoted();\n-  size_t young_promoted_reserve_used = (size_t) (ShenandoahEvacWaste * young_promoted);\n+  size_t young_advance_promoted = collection_set->get_young_bytes_to_be_promoted();\n+  size_t young_advance_promoted_reserve_used = (size_t) (ShenandoahPromoEvacWaste * young_advance_promoted);\n@@ -477,149 +387,2 @@\n-  \/\/ We'll invoke heap->set_young_evac_reserve() further below, after we make additional adjustments to its value\n-\n-  \/\/ Adjust old_regions_loaned_for_young_evac to feed into calculations of promoted_reserve\n-  if (young_evacuated_reserve_used > young_available) {\n-    size_t short_fall = young_evacuated_reserve_used - young_available;\n-\n-    \/\/ region_size_bytes is a power of 2.  loan an integral number of regions.\n-    size_t revised_loan_for_young_evacuation = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n-\n-    \/\/ available_loan_remnant represents memory loaned from old-gen but not required for young evacuation.\n-    \/\/ This is the excess loaned memory that results from rounding the required loan up to an integral number\n-    \/\/ of heap regions.  This will be dedicated to alloc_supplement below.\n-    available_loan_remnant = (revised_loan_for_young_evacuation * region_size_bytes) - short_fall;\n-\n-    \/\/ We previously loaned more than was required by young-gen evacuation.  So claw some of this memory back.\n-    old_regions_loaned_for_young_evac = revised_loan_for_young_evacuation;\n-    loaned_regions = old_regions_loaned_for_young_evac;\n-  } else {\n-    \/\/ Undo the previous loan, if any.\n-    old_regions_loaned_for_young_evac = 0;\n-    loaned_regions = 0;\n-  }\n-\n-  size_t old_bytes_loaned_for_young_evac = old_regions_loaned_for_young_evac * region_size_bytes - available_loan_remnant;\n-\n-  \/\/ Recompute regions_available_to_loan based on possible changes to old_regions_loaned_for_young_evac and\n-  \/\/ old_evacuation_reserve.\n-\n-  \/\/ Any decrease in old_regions_loaned_for_young_evac are immediately available to be loaned\n-  \/\/ However, a change to old_evacuation_reserve() is not necessarily available to loan, because this memory may\n-  \/\/ reside within many fragments scattered throughout old-gen.\n-\n-  regions_available_to_loan = old_generation->free_unaffiliated_regions();\n-  size_t working_old_available = old_generation->available();\n-\n-  assert(regions_available_to_loan * region_size_bytes <= working_old_available,\n-         \"Regions available to loan  must be less than available memory\");\n-\n-  \/\/ fragmented_old_total is the amount of memory in old-gen beyond regions_available_to_loan that is otherwise not\n-  \/\/ yet dedicated to a particular budget.  This memory can be used for promotion_reserve.\n-  size_t fragmented_old_total = working_old_available - regions_available_to_loan * region_size_bytes;\n-\n-  \/\/ fragmented_old_usage is the memory that is dedicated to holding evacuated old-gen objects, which does not need\n-  \/\/ to be an integral number of regions.\n-  size_t fragmented_old_usage = old_evacuated_committed + consumed_by_advance_promotion;\n-\n-  if (fragmented_old_total >= fragmented_old_usage) {\n-    \/\/ Seems this will be rare.  In this case, all of the memory required for old-gen evacuations and promotions can be\n-    \/\/ taken from the existing fragments within old-gen.  Reduce this fragmented total by this amount.\n-    fragmented_old_total -= fragmented_old_usage;\n-    \/\/ And reduce regions_available_to_loan by the regions dedicated to young_evac.\n-    regions_available_to_loan -= old_regions_loaned_for_young_evac;\n-  } else {\n-    \/\/ In this case, we need to dedicate some of the regions_available_to_loan to hold the results of old-gen evacuations\n-    \/\/ and promotions.\n-\n-    size_t unaffiliated_memory_required_for_old = fragmented_old_usage - fragmented_old_total;\n-    size_t unaffiliated_regions_used_by_old = (unaffiliated_memory_required_for_old + region_size_bytes - 1) \/ region_size_bytes;\n-    regions_available_to_loan -= (unaffiliated_regions_used_by_old + old_regions_loaned_for_young_evac);\n-\n-    size_t memory_for_promotions_and_old_evac = fragmented_old_total + unaffiliated_regions_used_by_old;\n-    size_t memory_required_for_promotions_and_old_evac = fragmented_old_usage;\n-    size_t excess_fragmented = memory_for_promotions_and_old_evac - memory_required_for_promotions_and_old_evac;\n-    fragmented_old_total = excess_fragmented;\n-  }\n-\n-  \/\/ Subtract from working_old_available old_evacuated_committed and consumed_by_advance_promotion\n-  working_old_available -= fragmented_old_usage;\n-  \/\/ And also subtract out the regions loaned for young evacuation\n-  working_old_available -= old_regions_loaned_for_young_evac * region_size_bytes;\n-\n-  \/\/ Assure that old_evacuated_committed + old_bytes_loaned_for_young_evac >= the minimum evacuation reserve\n-  \/\/ in order to prevent promotion reserve from violating minimum evacuation reserve.\n-  size_t old_regions_reserved_for_alloc_supplement = 0;\n-  size_t old_bytes_reserved_for_alloc_supplement = 0;\n-  size_t reserved_bytes_for_future_old_evac = 0;\n-\n-  old_bytes_reserved_for_alloc_supplement = available_loan_remnant;\n-  available_loan_remnant = 0;\n-\n-  \/\/ Memory that has been loaned for young evacuations and old-gen regions in the current mixed-evacuation collection\n-  \/\/ set will be available to hold future old-gen evacuations.  If this memory is less than the desired amount of memory\n-  \/\/ set aside for old-gen compaction reserve, try to set aside additional memory so that it will be available during\n-  \/\/ the next mixed evacuation cycle.  Note that memory loaned to young-gen for allocation supplement is excluded from\n-  \/\/ the old-gen promotion reserve.\n-  size_t future_evac_reserve_regions = old_regions_loaned_for_young_evac + collection_set->get_old_region_count();\n-  size_t collected_regions = collection_set->get_young_region_count();\n-\n-  if (future_evac_reserve_regions < ShenandoahOldCompactionReserve) {\n-    \/\/ Require that we loan more memory for holding young evacuations to assure that we have adequate reserves to receive\n-    \/\/ old-gen evacuations during subsequent collections.  Loaning this memory for an allocation supplement does not\n-    \/\/ satisfy our needs because newly allocated objects are not necessarily counter-balanced by reclaimed collection\n-    \/\/ set regions.\n-\n-    \/\/ Put this memory into reserve by identifying it as old_regions_loaned_for_young_evac\n-    size_t additional_regions_to_loan = ShenandoahOldCompactionReserve - future_evac_reserve_regions;\n-\n-    \/\/ We can loan additional regions to be repaid from the anticipated recycling of young collection set regions\n-    \/\/ provided that these regions are currently available within old-gen memory.\n-    size_t collected_regions_to_loan;\n-    if (collected_regions >= additional_regions_to_loan) {\n-      collected_regions_to_loan = additional_regions_to_loan;\n-      additional_regions_to_loan = 0;\n-    } else if (collected_regions > 0) {\n-      collected_regions_to_loan = collected_regions;\n-      additional_regions_to_loan -= collected_regions_to_loan;\n-    } else {\n-      collected_regions_to_loan = 0;\n-    }\n-\n-    if (collected_regions_to_loan > 0) {\n-      \/\/ We're evacuating at least this many regions, it's ok to use these regions for allocation supplement since\n-      \/\/ we'll be able to repay the loan at end of this GC pass, assuming the regions are available.\n-      if (collected_regions_to_loan > regions_available_to_loan) {\n-        collected_regions_to_loan = regions_available_to_loan;\n-      }\n-      old_bytes_reserved_for_alloc_supplement += collected_regions_to_loan * region_size_bytes;\n-      regions_available_to_loan -= collected_regions_to_loan;\n-      loaned_regions += collected_regions_to_loan;\n-      working_old_available -= collected_regions_to_loan * region_size_bytes;\n-    }\n-\n-    \/\/ If there's still memory that we want to exclude from the current promotion reserve, but we are unable to loan\n-    \/\/ this memory because fully empty old-gen regions are not available, decrement the working_old_available to make\n-    \/\/ sure that this memory is not used to hold the results of old-gen evacuation.\n-    if (additional_regions_to_loan > regions_available_to_loan) {\n-      size_t unloaned_regions = additional_regions_to_loan - regions_available_to_loan;\n-      size_t unloaned_bytes = unloaned_regions * region_size_bytes;\n-\n-      if (working_old_available < unloaned_bytes) {\n-        \/\/ We're in dire straits.  We won't be able to reserve all the memory that we want to make available for the\n-        \/\/ next old-gen evacuation.  We'll reserve as much of it as possible.  Setting working_old_available to zero\n-        \/\/ means there will be no promotion except for the advance promotion.  Note that if some advance promotion fails,\n-        \/\/ the object will be evacuated to young-gen so we should still end up reclaiming the entire advance promotion\n-        \/\/ collection set.\n-        reserved_bytes_for_future_old_evac = working_old_available;\n-        working_old_available = 0;\n-      } else {\n-        reserved_bytes_for_future_old_evac = unloaned_bytes;\n-        working_old_available -= unloaned_bytes;\n-      }\n-      size_t regions_reserved_for_future_old_evac =\n-        (reserved_bytes_for_future_old_evac + region_size_bytes - 1) \/ region_size_bytes;\n-\n-      if (regions_reserved_for_future_old_evac < regions_available_to_loan) {\n-        regions_available_to_loan -= regions_reserved_for_future_old_evac;\n-      } else {\n-        regions_available_to_loan = 0;\n-      }\n+  assert(young_evacuated_reserve_used <= young_generation->available(), \"Cannot evacuate more than is available in young\");\n+  heap->set_young_evac_reserve(young_evacuated_reserve_used);\n@@ -627,3 +390,18 @@\n-      \/\/ Since we're in dire straits, zero out fragmented_old_total so this won't be used for promotion;\n-      if (working_old_available > fragmented_old_total) {\n-        working_old_available -= fragmented_old_total;\n+  size_t old_available = old_generation->available();\n+  \/\/ Now that we've established the collection set, we know how much memory is really required by old-gen for evacuation\n+  \/\/ and promotion reserves.  Try shrinking OLD now in case that gives us a bit more runway for mutator allocations during\n+  \/\/ evac and update phases.\n+  size_t old_consumed = old_evacuated_committed + young_advance_promoted_reserve_used;\n+  assert(old_available >= old_consumed, \"Cannot consume more than is available\");\n+  size_t excess_old = old_available - old_consumed;\n+  size_t unaffiliated_old_regions = old_generation->free_unaffiliated_regions();\n+  size_t unaffiliated_old = unaffiliated_old_regions * region_size_bytes;\n+  assert(old_available >= unaffiliated_old, \"Unaffiliated old is a subset of old available\");\n+\n+  \/\/ Make sure old_evac_committed is unaffiliated\n+  if (old_evacuated_committed > 0) {\n+    if (unaffiliated_old > old_evacuated_committed) {\n+      size_t giveaway = unaffiliated_old - old_evacuated_committed;\n+      size_t giveaway_regions = giveaway \/ region_size_bytes;  \/\/ round down\n+      if (giveaway_regions > 0) {\n+        excess_old = MIN2(excess_old, giveaway_regions * region_size_bytes);\n@@ -631,1 +409,1 @@\n-        working_old_available = 0;\n+        excess_old = 0;\n@@ -633,1 +411,2 @@\n-      fragmented_old_total = 0;\n+    } else {\n+      excess_old = 0;\n@@ -637,46 +416,12 @@\n-  \/\/ Establish young_evac_reserve so that this young-gen memory is not used for new allocations, allowing the memory\n-  \/\/ to be returned to old-gen as soon as the current collection set regions are reclaimed.\n-  heap->set_young_evac_reserve(young_evacuated_reserve_used);\n-\n-  \/\/ Limit promoted_reserve so that we can set aside memory to be loaned from old-gen to young-gen.  This\n-  \/\/ value is not \"critical\".  If we underestimate, certain promotions will simply be deferred.  If we put\n-  \/\/ \"all the rest\" of old-gen memory into the promotion reserve, we'll have nothing left to loan to young-gen\n-  \/\/ during the evac and update phases of GC.  So we \"limit\" the sizes of the promotion budget to be the smaller of:\n-  \/\/\n-  \/\/  1. old_available\n-  \/\/     (old_available is old_gen->available() -\n-  \/\/      (old_evacuated_committed + consumed_by_advance_promotion + loaned_for_young_evac + reserved_for_alloc_supplement))\n-  \/\/  2. young bytes reserved for evacuation (we can't promote more than young is evacuating)\n-  size_t promotion_reserve = working_old_available;\n-\n-  \/\/ We experimented with constraining promoted_reserve to be no larger than 4 times the size of previously_promoted,\n-  \/\/ but this constraint was too limiting, resulting in failure of legitimate promotions.  This was tried before we\n-  \/\/ had special handling in place for advance promotion.  We should retry now that advance promotion is handled\n-  \/\/ specially.\n-\n-  \/\/ We had also experimented with constraining promoted_reserve to be no more than young_evacuation_committed\n-  \/\/ divided by promotion_divisor, where:\n-  \/\/  size_t promotion_divisor = (0x02 << InitialTenuringThreshold) - 1;\n-  \/\/ This also was found to be too limiting, resulting in failure of legitimate promotions.\n-  \/\/\n-  \/\/ Both experiments were conducted in the presence of other bugs which could have been the root cause for\n-  \/\/ the failures identified above as being \"too limiting\".  TODO: conduct new experiments with the more limiting\n-  \/\/ values of young_evacuation_reserved_used.\n-\n-  \/\/ young_evacuation_reserve_used already excludes bytes known to be promoted, which equals consumed_by_advance_promotion\n-  if (young_evacuated_reserve_used < promotion_reserve) {\n-    \/\/ Shrink promotion_reserve if it is larger than the memory to be consumed by evacuating all young objects in\n-    \/\/ collection set, including anticipated waste.  There's no benefit in using a larger promotion_reserve.\n-    \/\/ young_evacuation_reserve_used does not include live memory within tenure-aged regions.\n-    promotion_reserve = young_evacuated_reserve_used;\n-  }\n-  assert(working_old_available >= promotion_reserve, \"Cannot reserve for promotion more than is available\");\n-  working_old_available -= promotion_reserve;\n-  \/\/ Having reserved this memory for promotion, the regions are no longer available to be loaned.\n-  size_t regions_consumed_by_promotion_reserve = (promotion_reserve + region_size_bytes - 1) \/ region_size_bytes;\n-  if (regions_consumed_by_promotion_reserve > regions_available_to_loan) {\n-    \/\/ This can happen if the promotion reserve makes use of memory that is fragmented between many partially available\n-    \/\/ old-gen regions.\n-    regions_available_to_loan = 0;\n-  } else {\n-    regions_available_to_loan -= regions_consumed_by_promotion_reserve;\n+  \/\/ If we find that OLD has excess regions, give them back to YOUNG now to reduce likelihood we run out of allocation\n+  \/\/ runway during evacuation and update-refs.\n+  size_t regions_to_xfer = 0;\n+  if (excess_old > unaffiliated_old) {\n+    \/\/ we can give back unaffiliated_old (all of unaffiliated is excess)\n+    if (unaffiliated_old_regions > 0) {\n+      regions_to_xfer = unaffiliated_old_regions;\n+    }\n+  } else if (unaffiliated_old_regions > 0) {\n+    \/\/ excess_old < unaffiliated old: we can give back MIN(excess_old\/region_size_bytes, unaffiliated_old_regions)\n+    size_t excess_regions = excess_old \/ region_size_bytes;\n+    size_t regions_to_xfer = MIN2(excess_regions, unaffiliated_old_regions);\n@@ -685,13 +430,6 @@\n-  log_debug(gc)(\"old_gen->available(): \" SIZE_FORMAT \" divided between promotion reserve: \" SIZE_FORMAT\n-                \", old evacuation reserve: \" SIZE_FORMAT \", advance promotion reserve supplement: \" SIZE_FORMAT\n-                \", old loaned for young evacuation: \" SIZE_FORMAT \", old reserved for alloc supplement: \" SIZE_FORMAT,\n-                old_generation->available(), promotion_reserve, old_evacuated_committed, consumed_by_advance_promotion,\n-                old_regions_loaned_for_young_evac * region_size_bytes, old_bytes_reserved_for_alloc_supplement);\n-\n-  promotion_reserve += consumed_by_advance_promotion;\n-  heap->set_promoted_reserve(promotion_reserve);\n-\n-  heap->reset_promoted_expended();\n-  if (collection_set->get_old_bytes_reserved_for_evacuation() == 0) {\n-    \/\/ Setting old evacuation reserve to zero denotes that there is no old-gen evacuation in this pass.\n-    heap->set_old_evac_reserve(0);\n+  if (regions_to_xfer > 0) {\n+    bool result = heap->generation_sizer()->transfer_to_young(regions_to_xfer);\n+    assert(excess_old > regions_to_xfer * region_size_bytes, \"Cannot xfer more than excess old\");\n+    excess_old -= regions_to_xfer * region_size_bytes;\n+    log_info(gc, ergo)(\"%s transferred \" SIZE_FORMAT \" excess regions to young before start of evacuation\",\n+                       result? \"Successfully\": \"Unsuccessfully\", regions_to_xfer);\n@@ -700,78 +438,5 @@\n-  size_t old_gen_usage_base = old_generation->used() - collection_set->get_old_garbage();\n-  heap->capture_old_usage(old_gen_usage_base);\n-\n-  \/\/ Compute additional evacuation supplement, which is extra memory borrowed from old-gen that can be allocated\n-  \/\/ by mutators while GC is working on evacuation and update-refs.  This memory can be temporarily borrowed\n-  \/\/ from old-gen allotment, then repaid at the end of update-refs from the recycled collection set.  After\n-  \/\/ we have computed the collection set based on the parameters established above, we can make additional\n-  \/\/ loans based on our knowledge of the collection set to determine how much allocation we can allow\n-  \/\/ during the evacuation and update-refs phases of execution.  The total available supplement is the result\n-  \/\/ of adding old_bytes_reserved_for_alloc_supplement to the smaller of:\n-  \/\/\n-  \/\/   1. regions_available_to_loan * region_size_bytes\n-  \/\/   2. The replenishment budget (number of regions in collection set - the number of regions already\n-  \/\/         under lien for the young_evacuation_reserve)\n-  \/\/\n-\n-  \/\/ Regardless of how many regions may be available to be loaned, we can loan no more regions than\n-  \/\/ the total number of young regions to be evacuated.  Call this the regions_for_runway.\n-\n-  if (regions_available_to_loan > 0 && (collected_regions > loaned_regions)) {\n-    assert(regions_available_to_loan * region_size_bytes <= working_old_available,\n-           \"regions_available_to_loan should not exceed working_old_available\");\n-\n-    size_t additional_regions_to_loan = collected_regions - loaned_regions;\n-    if (additional_regions_to_loan > regions_available_to_loan) {\n-      additional_regions_to_loan = regions_available_to_loan;\n-    }\n-    loaned_regions += additional_regions_to_loan;\n-    old_bytes_reserved_for_alloc_supplement += additional_regions_to_loan * region_size_bytes;\n-    working_old_available -= additional_regions_to_loan * region_size_bytes;\n-  }\n-  size_t allocation_supplement = old_bytes_reserved_for_alloc_supplement + old_bytes_loaned_for_young_evac;\n-  assert(allocation_supplement % ShenandoahHeapRegion::region_size_bytes() == 0,\n-         \"allocation_supplement must be multiple of region size\");\n-\n-  heap->set_alloc_supplement_reserve(allocation_supplement);\n-\n-  \/\/ TODO: young_available, which feeds into alloc_budget_evac_and_update is lacking memory available within\n-  \/\/ existing young-gen regions that were not selected for the collection set.  Add this in and adjust the\n-  \/\/ log message (where it says \"empty-region allocation budget\").\n-\n-\n-  log_debug(gc)(\"Memory reserved for young evacuation: \" SIZE_FORMAT \"%s for evacuating \" SIZE_FORMAT\n-                \"%s out of young available: \" SIZE_FORMAT \"%s\",\n-                byte_size_in_proper_unit(young_evacuated_reserve_used),\n-                proper_unit_for_byte_size(young_evacuated_reserve_used),\n-                byte_size_in_proper_unit(young_evacuated), proper_unit_for_byte_size(young_evacuated),\n-                byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n-\n-  log_debug(gc)(\"Memory reserved for old evacuation: \" SIZE_FORMAT \"%s for evacuating \" SIZE_FORMAT\n-                \"%s out of old available: \" SIZE_FORMAT \"%s\",\n-                byte_size_in_proper_unit(old_evacuated), proper_unit_for_byte_size(old_evacuated),\n-                byte_size_in_proper_unit(old_evacuated), proper_unit_for_byte_size(old_evacuated),\n-                byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available));\n-\n-  size_t regular_promotion = promotion_reserve - consumed_by_advance_promotion;\n-  size_t excess =\n-    old_available - (old_evacuation_reserve + promotion_reserve + old_bytes_loaned_for_young_evac + allocation_supplement);\n-  log_info(gc, ergo)(\"Old available: \" SIZE_FORMAT \"%s is partitioned into old evacuation budget: \" SIZE_FORMAT\n-                     \"%s, aged region promotion budget: \" SIZE_FORMAT\n-                     \"%s, regular region promotion budget: \" SIZE_FORMAT\n-                     \"%s, loaned for young evacuation: \" SIZE_FORMAT\n-                     \"%s, loaned for young allocations: \" SIZE_FORMAT\n-                     \"%s, excess: \" SIZE_FORMAT \"%s\",\n-                     byte_size_in_proper_unit(old_available),\n-                     proper_unit_for_byte_size(old_available),\n-                     byte_size_in_proper_unit(old_evacuation_reserve),\n-                     proper_unit_for_byte_size(old_evacuation_reserve),\n-                     byte_size_in_proper_unit(consumed_by_advance_promotion),\n-                     proper_unit_for_byte_size(consumed_by_advance_promotion),\n-                     byte_size_in_proper_unit(regular_promotion),\n-                     proper_unit_for_byte_size(regular_promotion),\n-                     byte_size_in_proper_unit(old_bytes_loaned_for_young_evac),\n-                     proper_unit_for_byte_size(old_bytes_loaned_for_young_evac),\n-                     byte_size_in_proper_unit(allocation_supplement),\n-                     proper_unit_for_byte_size(allocation_supplement),\n-                     byte_size_in_proper_unit(excess),\n-                     proper_unit_for_byte_size(excess));\n+  \/\/ Add in the excess_old memory to hold unanticipated promotions, if any.  If there are more unanticipated\n+  \/\/ promotions than fit in reserved memory, they will be deferred until a future GC pass.\n+  size_t total_promotion_reserve = young_advance_promoted_reserve_used + excess_old;\n+  heap->set_promoted_reserve(total_promotion_reserve);\n+  heap->reset_promoted_expended();\n@@ -825,1 +490,0 @@\n-\n@@ -842,1 +506,5 @@\n-    heap->free_set()->rebuild();\n+    size_t young_cset_regions, old_cset_regions;\n+\n+    \/\/ We are preparing for evacuation.  At this time, we ignore cset region tallies.\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+    heap->free_set()->rebuild(young_cset_regions, old_cset_regions);\n@@ -900,1 +568,1 @@\n-  _adjusted_capacity(max_capacity), _heuristics(nullptr) {\n+  _heuristics(nullptr) {\n@@ -958,0 +626,22 @@\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         \"used + humongous cannot exceed regions\");\n+  return _affiliated_region_count;\n+}\n+\n+size_t ShenandoahGeneration::increase_affiliated_region_count(size_t delta) {\n+  shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+  _affiliated_region_count += delta;\n+  return _affiliated_region_count;\n+}\n+\n+size_t ShenandoahGeneration::decrease_affiliated_region_count(size_t delta) {\n+  shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+  assert(_affiliated_region_count > delta, \"Affiliated region count cannot be negative\");\n+\n+  _affiliated_region_count -= delta;\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         \"used + humongous cannot exceed regions\");\n@@ -970,5 +660,5 @@\n-}\n-\n-void ShenandoahGeneration::decrease_used(size_t bytes) {\n-  assert(_used >= bytes, \"cannot reduce bytes used by generation below zero\");\n-  Atomic::sub(&_used, bytes);\n+  \/\/ This detects arithmetic wraparound on _used.  Non-generational mode does not keep track of _affiliated_region_count\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         \"used cannot exceed regions\");\n@@ -985,1 +675,0 @@\n-    assert(_humongous_waste >= bytes, \"Waste cannot be negative\");\n@@ -992,0 +681,13 @@\n+void ShenandoahGeneration::decrease_used(size_t bytes) {\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used >= bytes), \"cannot reduce bytes used by generation below zero\");\n+  Atomic::sub(&_used, bytes);\n+\n+  \/\/ Non-generational mode does not maintain affiliated region counts\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() >= _used),\n+         \"Affiliated regions must hold more than what is currently used\");\n+}\n+\n@@ -999,1 +701,1 @@\n-    result = 0;                 \/\/ If old-gen is loaning regions to young-gen, affiliated regions may exceed capacity temporarily.\n+    result = 0;\n@@ -1022,31 +724,0 @@\n-size_t ShenandoahGeneration::adjust_available(intptr_t adjustment) {\n-  assert(adjustment % ShenandoahHeapRegion::region_size_bytes() == 0,\n-        \"Adjustment to generation size must be multiple of region size\");\n-  _adjusted_capacity = max_capacity() + adjustment;\n-  return _adjusted_capacity;\n-}\n-\n-size_t ShenandoahGeneration::unadjust_available() {\n-  _adjusted_capacity = max_capacity();\n-  return _adjusted_capacity;\n-}\n-\n-size_t ShenandoahGeneration::adjusted_available() const {\n-  size_t in_use = used() + get_humongous_waste();\n-  size_t capacity = _adjusted_capacity;\n-  return in_use > capacity ? 0 : capacity - in_use;\n-}\n-\n-size_t ShenandoahGeneration::adjusted_capacity() const {\n-  return _adjusted_capacity;\n-}\n-\n-size_t ShenandoahGeneration::adjusted_unaffiliated_regions() const {\n-  \/\/ This assertion has been disabled because we expect this code to be replaced by 05\/2023\n-  \/\/ assert(adjusted_capacity() >= used_regions_size(), \"adjusted_unaffiliated_regions() cannot return negative\");\n-  assert((adjusted_capacity() - used_regions_size()) % ShenandoahHeapRegion::region_size_bytes() == 0,\n-         \"adjusted capacity (\" SIZE_FORMAT \") and used regions size (\" SIZE_FORMAT \") should be multiples of region_size_bytes\",\n-         adjusted_capacity(), used_regions_size());\n-  return (adjusted_capacity() - used_regions_size()) \/ ShenandoahHeapRegion::region_size_bytes();\n-}\n-\n@@ -1055,7 +726,8 @@\n-  assert(_max_capacity + increment <= ShenandoahHeap::heap()->max_size_for(this), \"Cannot increase generation capacity beyond maximum.\");\n-  assert(increment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n-  \/\/ TODO: ysr: remove this check and warning\n-  if (increment % ShenandoahHeapRegion::region_size_bytes() != 0) {\n-    log_warning(gc)(\"Increment (\" INTPTR_FORMAT \") should be a multiple of region size (\" SIZE_FORMAT \")\",\n-                    increment, ShenandoahHeapRegion::region_size_bytes());\n-  }\n+\n+  \/\/ We do not enforce that new capacity >= heap->max_size_for(this).  The maximum generation size is treated as a rule of thumb\n+  \/\/ which may be violated during certain transitions, such as when we are forcing transfers for the purpose of promoting regions\n+  \/\/ in place.\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_max_capacity + increment <= ShenandoahHeap::heap()->max_capacity()), \"Generation cannot be larger than heap size\");\n+  assert(increment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Generation capacity must be multiple of region size\");\n@@ -1063,1 +735,6 @@\n-  _adjusted_capacity += increment;\n+\n+  \/\/ This detects arithmetic wraparound on _used\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() >= _used),\n+         \"Affiliated regions must hold more than what is currently used\");\n@@ -1068,7 +745,8 @@\n-  assert(_max_capacity - decrement >= ShenandoahHeap::heap()->min_size_for(this), \"Cannot decrease generation capacity beyond minimum.\");\n-  assert(decrement % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n-  \/\/ TODO: ysr: remove this check and warning\n-  if (decrement % ShenandoahHeapRegion::region_size_bytes() != 0) {\n-    log_warning(gc)(\"Decrement (\" INTPTR_FORMAT \") should be a multiple of region size (\" SIZE_FORMAT \")\",\n-                    decrement, ShenandoahHeapRegion::region_size_bytes());\n-  }\n+\n+  \/\/ We do not enforce that new capacity >= heap->min_size_for(this).  The minimum generation size is treated as a rule of thumb\n+  \/\/ which may be violated during certain transitions, such as when we are forcing transfers for the purpose of promoting regions\n+  \/\/ in place.\n+  assert(decrement % ShenandoahHeapRegion::region_size_bytes() == 0, \"Generation capacity must be multiple of region size\");\n+  assert(_max_capacity >= decrement, \"Generation capacity cannot be negative\");\n+  assert(_soft_max_capacity >= decrement, \"Generation soft capacity cannot be negative\");\n+\n@@ -1076,1 +754,13 @@\n-  _adjusted_capacity -= decrement;\n+\n+  \/\/ This detects arithmetic wraparound on _used\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() >= _used),\n+         \"Affiliated regions must hold more than what is currently used\");\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used <= _max_capacity), \"Cannot use more than capacity\");\n+  \/\/ TODO: REMOVE IS_GLOBAL() QUALIFIER AFTER WE FIX GLOBAL AFFILIATED REGION ACCOUNTING\n+  assert(is_global() || ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() <= _max_capacity),\n+         \"Cannot use more than capacity\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":213,"deletions":523,"binary":false,"changes":736,"status":"modified"},{"patch":"@@ -53,2 +53,0 @@\n-protected:\n-  \/\/ Usage\n@@ -64,0 +62,4 @@\n+\n+protected:\n+  \/\/ Usage\n+\n@@ -69,2 +71,0 @@\n-  size_t _adjusted_capacity;\n-\n@@ -120,16 +120,0 @@\n-  \/\/ During evacuation and update-refs, some memory may be shifted between generations.  In particular, memory\n-  \/\/ may be loaned by old-gen to young-gen based on the promise the loan will be promptly repaid from the memory reclaimed\n-  \/\/ when the current collection set is recycled.  The capacity adjustment also takes into consideration memory that is\n-  \/\/ set aside within each generation to hold the results of evacuation, but not promotion, into that region.  Promotions\n-  \/\/ into old-gen are bounded by adjusted_available() whereas evacuations into old-gen are pre-committed.\n-  size_t adjusted_available() const;\n-  size_t adjusted_capacity() const;\n-\n-  \/\/ This is the number of FREE regions that are eligible to be affiliated with this generation according to the current\n-  \/\/ adjusted capacity.\n-  size_t adjusted_unaffiliated_regions() const;\n-\n-  \/\/ Both of following return new value of available\n-  size_t adjust_available(intptr_t adjustment);\n-  size_t unadjust_available();\n-\n@@ -208,0 +192,6 @@\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t increase_affiliated_region_count(size_t delta);\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t decrease_affiliated_region_count(size_t delta);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":10,"deletions":20,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -382,0 +382,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -383,1 +384,3 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+    _free_set->rebuild(young_cset_regions, old_cset_regions);\n@@ -522,2 +525,1 @@\n-  _global_generation = new ShenandoahGlobalGeneration(_gc_mode->is_generational(), _max_workers, soft_max_capacity(), soft_max_capacity());\n-\n+  _global_generation = new ShenandoahGlobalGeneration(_gc_mode->is_generational(), _max_workers, max_capacity(), max_capacity());\n@@ -525,6 +527,2 @@\n-  if (mode()->is_generational()) {\n-    _young_generation->initialize_heuristics(_gc_mode);\n-    _old_generation->initialize_heuristics(_gc_mode);\n-\n-    ShenandoahEvacWaste = ShenandoahGenerationalEvacWaste;\n-  }\n+  _young_generation->initialize_heuristics(_gc_mode);\n+  _old_generation->initialize_heuristics(_gc_mode);\n@@ -543,0 +541,2 @@\n+  _promotion_potential(0),\n+  _promotion_in_place_potential(0),\n@@ -552,1 +552,0 @@\n-  _alloc_supplement_reserve(0),\n@@ -584,0 +583,2 @@\n+  _old_regions_surplus(0),\n+  _old_regions_deficit(0),\n@@ -822,0 +823,2 @@\n+#ifdef KELVIN_DEPRECATE\n+  \/\/ soft_max affects heuristic triggers, but has no impact on generation sizes\n@@ -828,0 +831,1 @@\n+#endif\n@@ -896,3 +900,1 @@\n-  \/\/ We squelch excessive reports to reduce noise in logs.  Squelch enforcement is not \"perfect\" because\n-  \/\/ this same code can be in-lined in multiple contexts, and each context will have its own copy of the static\n-  \/\/ last_report_epoch and this_epoch_report_count variables.\n+  \/\/ We squelch excessive reports to reduce noise in logs.\n@@ -918,0 +920,4 @@\n+    ShenandoahGeneration* old_gen = old_generation();\n+    size_t old_capacity = old_gen->max_capacity();\n+    size_t old_usage = old_gen->used();\n+    size_t old_free_regions = old_gen->free_unaffiliated_regions();\n@@ -920,3 +926,6 @@\n-                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT,\n-                       size, plab == nullptr? \"no\": \"yes\",\n-                       words_remaining, promote_enabled, promotion_reserve, promotion_expended);\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n+                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n+                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n+                       old_capacity, old_usage, old_free_regions);\n+\n@@ -1032,1 +1041,0 @@\n-\n@@ -1040,1 +1048,6 @@\n-      return nullptr;\n+      if (min_size == PLAB::min_size()) {\n+        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return NULL;\n@@ -1059,1 +1072,0 @@\n-\n@@ -1157,3 +1169,43 @@\n-bool ShenandoahHeap::adjust_generation_sizes() {\n-  if (mode()->is_generational()) {\n-    return _generation_sizer.adjust_generation_sizes();\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old\n+void ShenandoahHeap::adjust_generation_sizes_for_next_cycle(\n+  size_t xfer_limit, size_t young_cset_regions, size_t old_cset_regions) {\n+\n+  \/\/ Make sure old-generation is large enough, but no larger, than is necessary to hold mixed evacuations\n+  \/\/ and promotions if we anticipate either.\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t promo_load = get_promotion_potential();\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations\n+  size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  size_t old_reserve = 0;\n+  size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n+  bool doing_mixed = (mixed_candidates > 0);\n+  bool doing_promotions = promo_load > 0;\n+\n+  \/\/ round down\n+  size_t max_old_region_xfer = xfer_limit \/ region_size_bytes;\n+\n+  \/\/ We can limit the reserve to the size of anticipated promotions\n+  size_t max_old_reserve = young_reserve * ShenandoahOldEvacRatioPercent \/ (100 - ShenandoahOldEvacRatioPercent);\n+  \/\/ Here's the algebra:\n+  \/\/  TotalEvacuation = OldEvacuation + YoungEvacuation\n+  \/\/  OldEvacuation = TotalEvacuation*(ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * (ShenandoahOldEvacRatioPercent\/100)\/(1 - ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * ShenandoahOldEvacRatioPercent\/(100 - ShenandoahOldEvacRatioPercent)\n+\n+  size_t reserve_for_mixed, reserve_for_promo;\n+  if (doing_mixed) {\n+    assert(old_generation()->available() >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n+           \"Unaffiliated available must be less than total available\");\n+\n+    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n+    \/\/ may not evacuate the entirety of unprocessed candidates in a single mixed evacuation.\n+    size_t max_evac_need = (size_t)\n+      (old_heuristics()->unprocessed_old_collection_candidates_live_memory() * ShenandoahOldEvacWaste);\n+    size_t old_fragmented_available =\n+      old_generation()->available() - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    reserve_for_mixed = max_evac_need + old_fragmented_available;\n+    if (reserve_for_mixed > max_old_reserve) {\n+      reserve_for_mixed = max_old_reserve;\n+    }\n+  } else {\n+    reserve_for_mixed = 0;\n@@ -1161,1 +1213,43 @@\n-  return false;\n+\n+  size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n+  if (doing_promotions) {\n+    \/\/ We're only promoting and we have a maximum bound on the amount to be promoted\n+    reserve_for_promo = (size_t) (promo_load * ShenandoahPromoEvacWaste);\n+    if (reserve_for_promo > available_for_promotions) {\n+      reserve_for_promo = available_for_promotions;\n+    }\n+  } else {\n+    reserve_for_promo = 0;\n+  }\n+  old_reserve = reserve_for_mixed + reserve_for_promo;\n+  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+  size_t old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  size_t young_available = young_generation()->available() + young_cset_regions * region_size_bytes;\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+  if (old_available >= old_reserve) {\n+    size_t old_excess = old_available - old_reserve;\n+    size_t excess_regions = old_excess \/ region_size_bytes;\n+    size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n+    size_t unaffiliated_old = unaffiliated_old_regions * region_size_bytes;\n+    if (unaffiliated_old_regions < excess_regions) {\n+      \/\/ We'll give only unaffiliated old to young, which is known to be less than the excess.\n+      old_region_surplus = unaffiliated_old_regions;\n+    } else {\n+      \/\/ unaffiliated_old_regions > excess_regions, so we only give away the excess.\n+      old_region_surplus = excess_regions;\n+    }\n+  } else {\n+    \/\/ We need to request transfer from YOUNG.  Ignore that this will directly impact young_generation()->max_capacity(),\n+    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n+    size_t old_need = old_reserve - old_available;\n+    \/\/ Round up the number of regions needed from YOUNG\n+    old_region_deficit = (old_need + region_size_bytes - 1) \/ region_size_bytes;\n+  }\n+  if (old_region_deficit > max_old_region_xfer) {\n+    \/\/ If we're running short on young-gen memory, limit the xfer.  Old-gen collection activities will be curtailed\n+    \/\/ if the budget is smaller than desired.\n+    old_region_deficit = max_old_region_xfer;\n+  }\n+  set_old_region_surplus(old_region_surplus);\n+  set_old_region_deficit(old_region_deficit);\n@@ -1196,1 +1290,1 @@\n-  \/\/ if we are at risk of exceeding the old-gen evacuation budget.\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n@@ -1300,14 +1394,12 @@\n-          size_t young_available = young_generation()->adjusted_available();\n-          if (requested_bytes > young_available) {\n-            \/\/ We know this is not a GCLAB.  This must be a TLAB or a shared allocation.\n-            if (req.is_lab_alloc() && (young_available >= req.min_size())) {\n-              try_smaller_lab_size = true;\n-              smaller_lab_size = young_available \/ HeapWordSize;\n-            } else {\n-              \/\/ Can't allocate because even min_size() is larger than remaining young_available\n-              log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n-                                 \", young available: \" SIZE_FORMAT,\n-                                 req.is_lab_alloc()? \"TLAB\": \"shared\",\n-                                 HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_available);\n-              return nullptr;\n-            }\n+          size_t young_words_available = young_generation()->available() \/ HeapWordSize;\n+          if (ShenandoahElasticTLAB && req.is_lab_alloc() && (req.min_size() < young_words_available)) {\n+            \/\/ Allow ourselves to try a smaller lab size even if requested_bytes <= young_available.  We may need a smaller\n+            \/\/ lab size because young memory has become too fragmented.\n+            try_smaller_lab_size = true;\n+            smaller_lab_size = (young_words_available < req.size())? young_words_available: req.size();\n+          } else if (req.size() > young_words_available) {\n+            \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+            log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                               \", young words available: \" SIZE_FORMAT, req.type_string(),\n+                               HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_words_available);\n+            return nullptr;\n@@ -1354,9 +1446,14 @@\n-    if (!try_smaller_lab_size) {\n-      result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n-      if (result != nullptr) {\n-        if (req.is_old()) {\n-          ShenandoahThreadLocalData::reset_plab_promoted(thread);\n-          if (req.is_gc_alloc()) {\n-            if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n-              if (promotion_eligible) {\n-                size_t actual_size = req.actual_size() * HeapWordSize;\n+    \/\/ First try the original request.  If TLAB request size is greater than available, allocate() will attempt to downsize\n+    \/\/ request to fit within available memory.\n+    result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+    if (result != nullptr) {\n+      if (req.is_old()) {\n+        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+        if (req.is_gc_alloc()) {\n+          bool disable_plab_promotions = false;\n+          if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+            if (promotion_eligible) {\n+              size_t actual_size = req.actual_size() * HeapWordSize;\n+              \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+              \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+              if (get_promoted_expended() + actual_size <= get_promoted_reserve()) {\n@@ -1367,2 +1464,1 @@\n-                \/\/ This assert has been disabled because we expect this code to be replaced by 05\/2023.\n-                \/\/ assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n@@ -1371,3 +1467,1 @@\n-                \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n-                ShenandoahThreadLocalData::disable_plab_promotions(thread);\n-                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+                disable_plab_promotions = true;\n@@ -1375,4 +1469,7 @@\n-            } else if (is_promotion) {\n-              \/\/ Shared promotion.  Assume size is requested_bytes.\n-              expend_promoted(requested_bytes);\n-              assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+            } else {\n+              disable_plab_promotions = true;\n+            }\n+            if (disable_plab_promotions) {\n+              \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+              ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+              ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n@@ -1380,0 +1477,4 @@\n+          } else if (is_promotion) {\n+            \/\/ Shared promotion.  Assume size is requested_bytes.\n+            expend_promoted(requested_bytes);\n+            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n@@ -1381,27 +1482,0 @@\n-\n-          \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n-          \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n-          \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n-          \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n-          \/\/\n-          \/\/ objects being \"concurrently\" allocated:\n-          \/\/    [-----a------][-----b-----][--------------c------------------]\n-          \/\/            [---- card table memory range --------------]\n-          \/\/\n-          \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n-          \/\/   wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n-          \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n-          \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n-          \/\/\n-          \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n-          \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n-          \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n-          ShenandoahHeap::heap()->card_scan()->register_object(result);\n-        }\n-      } else {\n-        \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n-        if (req.is_old() && req.is_gc_alloc() &&\n-            (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n-          \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n-          \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n-          ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n@@ -1409,0 +1483,20 @@\n+\n+        \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+        \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+        \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+        \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+        \/\/\n+        \/\/ objects being \"concurrently\" allocated:\n+        \/\/    [-----a------][-----b-----][--------------c------------------]\n+        \/\/            [---- card table memory range --------------]\n+        \/\/\n+        \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+        \/\/   wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+        \/\/   allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+        \/\/   allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+        \/\/   card region.\n+        \/\/\n+        \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+        \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+        \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+        ShenandoahHeap::heap()->card_scan()->register_object(result);\n@@ -1410,0 +1504,9 @@\n+    } else {\n+      \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+      if (req.is_old() && req.is_gc_alloc() && (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+        \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+        \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+        ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+      }\n+    }\n+    if ((result != nullptr) || !try_smaller_lab_size) {\n@@ -1412,2 +1515,8 @@\n-    \/\/ else, try_smaller_lab_size is true so we fall through and recurse with a smaller lab size\n-  } \/\/ This closes the block that holds the heap lock.  This releases the lock.\n+    \/\/ else, fall through to try_smaller_lab_size\n+  } \/\/ This closes the block that holds the heap lock, releasing the lock.\n+\n+  \/\/ We failed to allocate the originally requested lab size.  Let's see if we can allocate a smaller lab size.\n+  if (req.size() == smaller_lab_size) {\n+    \/\/ If we were already trying to allocate min size, no value in attempting to repeat the same.  End the recursion.\n+    return nullptr;\n+  }\n@@ -1592,0 +1701,3 @@\n+    ShenandoahMarkingContext* const ctx = ShenandoahHeap::heap()->marking_context();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n@@ -1593,1 +1705,1 @@\n-      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s]\",\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n@@ -1596,1 +1708,3 @@\n-                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\");\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+                    r->is_cset()? \"cset\": \"not-cset\");\n+\n@@ -1603,13 +1717,11 @@\n-      } else if (r->is_young() && r->is_active() && r->is_humongous_start() && (r->age() > InitialTenuringThreshold)) {\n-        \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n-        \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n-        \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n-        if (r->promote_humongous() == 0) {\n-          \/\/ We chose not to promote because old-gen is out of memory.  Report and handle the promotion failure because\n-          \/\/ this suggests need for expanding old-gen and\/or performing collection of old-gen.\n-          ShenandoahHeap* heap = ShenandoahHeap::heap();\n-          oop obj = cast_to_oop(r->bottom());\n-          size_t size = obj->size();\n-          Thread* thread = Thread::current();\n-          heap->report_promotion_failure(thread, size);\n-          heap->handle_promotion_failure();\n+      } else if (r->is_young() && r->is_active() && (r->age() >= InitialTenuringThreshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        if (r->is_humongous_start()) {\n+          \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+          \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+          \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+          r->promote_humongous();\n+        } else if (r->is_regular() && (r->garbage_before_padded_for_promote() < old_garbage_threshold) && (r->get_top_before_promote() == tams)) {\n+          \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+          \/\/ the LRB to copy on reference fetch.\n+          r->promote_in_place();\n@@ -1617,0 +1729,7 @@\n+        \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+        \/\/ more garbage than ShenandoahOldGarbageTrheshold, we'll promote by evacuation.  If there is room for evacuation\n+        \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+        \/\/ by evacuation in some future GC cycle.\n+\n+        \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+        \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n@@ -1815,1 +1934,1 @@\n-      return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->adjusted_available());\n+      return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n@@ -1903,4 +2022,0 @@\n-\n-  \/\/ When a cycle starts, attribute any thread activity when the collector\n-  \/\/ is idle to the global generation.\n-  _mmu_tracker.record(global_generation());\n@@ -1911,1 +2026,0 @@\n-\n@@ -1918,3 +2032,0 @@\n-\n-  \/\/ When a cycle ends, the thread activity is attributed to the respective generation\n-  _mmu_tracker.record(generation);\n@@ -2679,0 +2790,10 @@\n+    if (CONCURRENT && (worker_id == 0)) {\n+      \/\/ We ask the first worker to replenish the Mutator free set by moving regions previously reserved to hold the\n+      \/\/ results of evacuation.  These reserves are no longer necessary because evacuation has completed.\n+      size_t cset_regions = _heap->collection_set()->count();\n+      \/\/ We cannot transfer any more regions than will be reclaimed when the existing collection set is recycled, because\n+      \/\/ we need the reclaimed collection set regions to replenish the collector reserves\n+      _heap->free_set()->move_collector_sets_to_mutator(cset_regions);\n+    }\n+    \/\/ If !CONCURRENT, there's no value in expanding Mutator free set\n+\n@@ -2941,6 +3062,52 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational()) {\n+    assert(verify_generation_usage(true, old_generation()->used_regions(),\n+                                   old_generation()->used(), old_generation()->get_humongous_waste(),\n+                                   true, young_generation()->used_regions(),\n+                                   young_generation()->used(), young_generation()->get_humongous_waste()),\n+           \"Generation accounts are inaccurate\");\n+\n+    \/\/ The computation of evac_slack is quite conservative so consider all of this available for transfer to old.\n+    \/\/ Note that transfer of humongous regions does not impact available.\n+    size_t evac_slack = young_generation()->heuristics()->evac_slack(young_cset_regions);\n+    adjust_generation_sizes_for_next_cycle(evac_slack, young_cset_regions, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational()) {\n+    size_t old_available = old_generation()->available();\n+    size_t old_unaffiliated_available = old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    size_t old_fragmented_available;\n+    assert(old_available >= old_unaffiliated_available, \"unaffiliated available is a subset of total available\");\n+    old_fragmented_available = old_available - old_unaffiliated_available;\n+\n+    size_t old_capacity = old_generation()->max_capacity();\n+    size_t heap_capacity = capacity();\n+    if ((old_capacity > heap_capacity \/ 8) && (old_fragmented_available > old_capacity \/ 8)) {\n+      ((ShenandoahOldHeuristics *) old_generation()->heuristics())->trigger_old_is_fragmented();\n+    }\n+\n+    size_t old_used = old_generation()->used() + old_generation()->get_humongous_waste();\n+    size_t trigger_threshold = old_generation()->usage_trigger_threshold();\n+    \/\/ Detects unsigned arithmetic underflow\n+    assert(old_used < ShenandoahHeap::heap()->capacity(), \"Old used must be less than heap capacity\");\n+\n+    if (old_used > trigger_threshold) {\n+      ((ShenandoahOldHeuristics *) old_generation()->heuristics())->trigger_old_has_grown();\n+    }\n@@ -3190,0 +3357,51 @@\n+bool ShenandoahHeap::verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                                             bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste) {\n+  size_t tally_old_regions = 0;\n+  size_t tally_old_bytes = 0;\n+  size_t tally_old_waste = 0;\n+  size_t tally_young_regions = 0;\n+  size_t tally_young_bytes = 0;\n+  size_t tally_young_waste = 0;\n+\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_old()) {\n+      tally_old_regions++;\n+      tally_old_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_old_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    } else if (r->is_young()) {\n+      tally_young_regions++;\n+      tally_young_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_young_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    }\n+  }\n+  if (verify_young &&\n+      ((young_regions != tally_young_regions) || (young_bytes != tally_young_bytes) || (young_waste != tally_young_waste))) {\n+    return false;\n+  } else if (verify_old &&\n+             ((old_regions != tally_old_regions) || (old_bytes != tally_old_bytes) || (old_waste != tally_old_waste))) {\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":333,"deletions":115,"binary":false,"changes":448,"status":"modified"},{"patch":"@@ -208,0 +208,3 @@\n+  bool verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                               bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste);\n+\n@@ -218,0 +221,8 @@\n+           size_t _promotion_potential;\n+           size_t _promotion_in_place_potential;\n+           size_t _pad_for_promote_in_place;    \/\/ bytes of filler\n+           size_t _promotable_humongous_regions;\n+           size_t _promotable_humongous_usage;\n+           size_t _regular_regions_promoted_in_place;\n+           size_t _regular_usage_promoted_in_place;\n+\n@@ -287,0 +298,2 @@\n+  inline ShenandoahMmuTracker* const mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -335,25 +348,2 @@\n-  \/\/ _alloc_supplement_reserve is a supplemental budget for new_memory allocations.  During evacuation and update-references,\n-  \/\/ mutator allocation requests are \"authorized\" iff young_gen->available() plus _alloc_supplement_reserve minus\n-  \/\/ _young_evac_reserve is greater than request size.  The values of _alloc_supplement_reserve and _young_evac_reserve\n-  \/\/ are zero except during evacuation and update-reference phases of GC.  Both of these values are established at\n-  \/\/ the start of evacuation, and they remain constant throughout the duration of these two phases of GC.  Since these\n-  \/\/ two values are constant throughout each GC phases, we introduce a new service into ShenandoahGeneration.  This service\n-  \/\/ provides adjusted_available() based on an adjusted capacity.  At the start of evacuation, we adjust young capacity by\n-  \/\/ adding the amount to be borrowed from old-gen and subtracting the _young_evac_reserve, we adjust old capacity by\n-  \/\/ subtracting the amount to be loaned to young-gen.\n-  \/\/\n-  \/\/ We always use adjusted capacities to determine permission to allocate within young and to promote into old.  Note\n-  \/\/ that adjusted capacities equal traditional capacities except during evacuation and update refs.\n-  \/\/\n-  \/\/ During evacuation, we assure that _old_evac_expended does not exceed _old_evac_reserve.\n-  \/\/\n-  \/\/ At the end of update references, we perform the following bookkeeping activities:\n-  \/\/\n-  \/\/ 1. Unadjust the capacity within young-gen and old-gen to undo the effects of borrowing memory from old-gen.  Note that\n-  \/\/    the entirety of the collection set is now available, so allocation capacity naturally increase at this time.\n-  \/\/ 2. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promoted_reserve\n-  \/\/\n-  \/\/ _young_evac_reserve and _old_evac_reserve are only non-zero during evacuation and update-references.\n-  \/\/\n-  \/\/ Allocation of old GCLABs assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n-  \/\/  is authorized, increment _old_evac_expended by request size.  This allocation ignores old_gen->available().\n+  \/\/ TODO: Revisit the following comment.  It may not accurately represent the true behavior when evacuations fail due to\n+  \/\/ difficulty finding memory to hold evacuated objects.\n@@ -367,1 +357,0 @@\n-  intptr_t _alloc_supplement_reserve;  \/\/ Bytes reserved for young allocations during evac and update refs\n@@ -371,0 +360,3 @@\n+  \/\/ Allocation of old GCLABs (aka PLABs) assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n+  \/\/  is authorized, increment _old_evac_expended by request size.  This allocation ignores old_gen->available().\n+\n@@ -418,0 +410,1 @@\n+\n@@ -444,0 +437,21 @@\n+  inline void clear_promotion_potential() { _promotion_potential = 0; };\n+  inline void set_promotion_potential(size_t val) { _promotion_potential = val; };\n+  inline size_t get_promotion_potential() { return _promotion_potential; };\n+\n+  inline void clear_promotion_in_place_potential() { _promotion_in_place_potential = 0; };\n+  inline void set_promotion_in_place_potential(size_t val) { _promotion_in_place_potential = val; };\n+  inline size_t get_promotion_in_place_potential() { return _promotion_in_place_potential; };\n+\n+  inline void set_pad_for_promote_in_place(size_t pad) { _pad_for_promote_in_place = pad; }\n+  inline size_t get_pad_for_promote_in_place() { return _pad_for_promote_in_place; }\n+\n+  inline void reserve_promotable_humongous_regions(size_t region_count) { _promotable_humongous_regions = region_count; }\n+  inline void reserve_promotable_humongous_usage(size_t bytes) { _promotable_humongous_usage = bytes; }\n+  inline void reserve_promotable_regular_regions(size_t region_count) { _regular_regions_promoted_in_place = region_count; }\n+  inline void reserve_promotable_regular_usage(size_t used_bytes) { _regular_usage_promoted_in_place = used_bytes; }\n+\n+  inline size_t get_promotable_humongous_regions() { return _promotable_humongous_regions; }\n+  inline size_t get_promotable_humongous_usage() { return _promotable_humongous_usage; }\n+  inline size_t get_regular_regions_promoted_in_place() { return _regular_regions_promoted_in_place; }\n+  inline size_t get_regular_usage_promoted_in_place() { return _regular_usage_promoted_in_place; }\n+\n@@ -447,0 +461,1 @@\n+  inline void augment_promo_reserve(size_t increment);\n@@ -456,0 +471,1 @@\n+  inline void augment_old_evac_reserve(size_t increment);\n@@ -465,5 +481,0 @@\n-  \/\/ Returns previous value.  This is a signed value because it is the amount borrowed minus the amount reserved for\n-  \/\/ young-gen evacuation.  In case we cannot borrow much, this value might be negative.\n-  inline intptr_t set_alloc_supplement_reserve(intptr_t new_val);\n-  inline intptr_t get_alloc_supplement_reserve() const;\n-\n@@ -520,1 +531,0 @@\n-  void rebuild_free_set(bool concurrent);\n@@ -525,0 +535,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -701,0 +712,4 @@\n+  \/\/ How many bytes to transfer between old and young after we have finished recycling collection set regions?\n+  size_t _old_regions_surplus;\n+  size_t _old_regions_deficit;\n+\n@@ -734,0 +749,6 @@\n+  inline void set_old_region_surplus(size_t surplus) { _old_regions_surplus = surplus; };\n+  inline void set_old_region_deficit(size_t deficit) { _old_regions_deficit = deficit; };\n+\n+  inline size_t get_old_region_surplus() { return _old_regions_surplus; };\n+  inline size_t get_old_region_deficit() { return _old_regions_deficit; };\n+\n@@ -833,1 +854,1 @@\n-  bool adjust_generation_sizes();\n+  void adjust_generation_sizes_for_next_cycle(size_t old_xfer_limit, size_t young_cset_regions, size_t old_cset_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":54,"deletions":33,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -294,0 +294,1 @@\n+\n@@ -298,1 +299,1 @@\n-  } else if (is_promotion && (plab->words_remaining() > 0) && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+  } else if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n@@ -385,1 +386,0 @@\n-\n@@ -513,0 +513,1 @@\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -756,0 +757,8 @@\n+inline void ShenandoahHeap::augment_old_evac_reserve(size_t increment) {\n+  _old_evac_reserve += increment;\n+}\n+\n+inline void ShenandoahHeap::augment_promo_reserve(size_t increment) {\n+  _promoted_reserve += increment;\n+}\n+\n@@ -794,10 +803,0 @@\n-inline intptr_t ShenandoahHeap::set_alloc_supplement_reserve(intptr_t new_val) {\n-  intptr_t orig = _alloc_supplement_reserve;\n-  _alloc_supplement_reserve = new_val;\n-  return orig;\n-}\n-\n-inline intptr_t ShenandoahHeap::get_alloc_supplement_reserve() const {\n-  return _alloc_supplement_reserve;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":11,"deletions":12,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -105,1 +106,1 @@\n-      set_affiliation(affiliation);\n+      assert(this->affiliation() == affiliation, \"Region affiliation should already be established\");\n@@ -125,1 +126,7 @@\n-     set_affiliation(YOUNG_GENERATION);\n+     if (affiliation() != YOUNG_GENERATION) {\n+       if (is_old()) {\n+         ShenandoahHeap::heap()->old_generation()->decrement_affiliated_region_count();\n+       }\n+       set_affiliation(YOUNG_GENERATION);\n+       ShenandoahHeap::heap()->young_generation()->increment_affiliated_region_count();\n+     }\n@@ -178,0 +185,1 @@\n+  \/\/ Don't bother to account for affiliated regions during Full GC.  We recompute totals at end.\n@@ -210,0 +218,1 @@\n+  \/\/ Don't bother to account for affiliated regions during Full GC.  We recompute totals at end.\n@@ -472,0 +481,1 @@\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n@@ -517,0 +527,1 @@\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -568,0 +579,1 @@\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n@@ -569,1 +581,0 @@\n-\n@@ -673,0 +684,1 @@\n+  ShenandoahHeap::heap()->generation_for(affiliation())->decrement_affiliated_region_count();\n@@ -674,1 +686,0 @@\n-\n@@ -960,0 +971,2 @@\n+    log_trace(gc)(\"Changing affiliation of region %zu from %s to %s\",\n+                  index(), affiliation_name(), shenandoah_affiliation_name(new_affiliation));\n@@ -964,10 +977,0 @@\n-  log_trace(gc)(\"Changing affiliation of region %zu from %s to %s\",\n-    index(), shenandoah_affiliation_name(region_affiliation), shenandoah_affiliation_name(new_affiliation));\n-\n-  if (region_affiliation == ShenandoahAffiliation::YOUNG_GENERATION) {\n-    heap->young_generation()->decrement_affiliated_region_count();\n-  } else if (region_affiliation == ShenandoahAffiliation::OLD_GENERATION) {\n-    heap->old_generation()->decrement_affiliated_region_count();\n-  }\n-\n-  size_t regions;\n@@ -980,6 +983,0 @@\n-      regions = heap->young_generation()->increment_affiliated_region_count();\n-      \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n-      \/\/ restored upon completion of Full GC.\n-      assert(heap->is_full_gc_in_progress() ||\n-             (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->young_generation()->adjusted_capacity()),\n-             \"Number of young regions cannot exceed adjusted capacity\");\n@@ -988,6 +985,2 @@\n-      regions = heap->old_generation()->increment_affiliated_region_count();\n-      \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n-      \/\/ restored upon completion of Full GC.\n-      assert(heap->is_full_gc_in_progress() ||\n-             (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->old_generation()->adjusted_capacity()),\n-             \"Number of old regions cannot exceed adjusted capacity\");\n+      \/\/ TODO: should we reset_age() for OLD as well?  Examine invocations of set_affiliation(). Some contexts redundantly\n+      \/\/       invoke reset_age().\n@@ -1002,2 +995,84 @@\n-\/\/ Returns number of regions promoted, or zero if we choose not to promote.\n-size_t ShenandoahHeapRegion::promote_humongous() {\n+\/\/ When we promote a region in place, we can continue to use the established marking context to guide subsequent remembered\n+\/\/ set scans of this region's content.  The region will be coalesced and filled prior to the next old-gen marking effort.\n+\/\/ We identify the entirety of the region as DIRTY to force the next remembered set scan to identify the \"interesting poitners\"\n+\/\/ contained herein.\n+void ShenandoahHeapRegion::promote_in_place() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  HeapWord* tams = marking_context->top_at_mark_start(this);\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+  assert(is_young(), \"Only young regions can be promoted\");\n+  assert(is_regular(), \"Use different service to promote humongous regions\");\n+  assert(age() >= InitialTenuringThreshold, \"Only promote regions that are sufficiently aged\");\n+\n+  ShenandoahOldGeneration* old_gen = heap->old_generation();\n+  ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+\n+    HeapWord* update_watermark = get_update_watermark();\n+\n+    \/\/ Now that this region is affiliated with old, we can allow it to receive allocations, though it may not be in the\n+    \/\/ is_collector_free range.\n+    restore_top_before_promote();\n+\n+    size_t region_capacity = free();\n+    size_t region_used = used();\n+\n+    \/\/ The update_watermark was likely established while we had the artificially high value of top.  Make it sane now.\n+    assert(update_watermark >= top(), \"original top cannot exceed preserved update_watermark\");\n+    set_update_watermark(top());\n+\n+    \/\/ Unconditionally transfer one region from young to old to represent the newly promoted region.\n+    \/\/ This expands old and shrinks new by the size of one region.  Strictly, we do not \"need\" to expand old\n+    \/\/ if there are already enough unaffiliated regions in old to account for this newly promoted region.\n+    \/\/ However, if we do not transfer the capacities, we end up reducing the amount of memory that would have\n+    \/\/ otherwise been available to hold old evacuations, because old available is max_capacity - used and now\n+    \/\/ we would be trading a fully empty region for a partially used region.\n+\n+    young_gen->decrease_used(region_used);\n+    young_gen->decrement_affiliated_region_count();\n+\n+    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n+    heap->generation_sizer()->force_transfer_to_old(1);\n+    set_affiliation(OLD_GENERATION);\n+\n+    old_gen->increment_affiliated_region_count();\n+    old_gen->increase_used(region_used);\n+\n+    \/\/ add_old_collector_free_region() increases promoted_reserve() if available space exceeds PLAB::min_size()\n+    heap->free_set()->add_old_collector_free_region(this);\n+  }\n+\n+  assert(top() == tams, \"Cannot promote regions in place if top has advanced beyond TAMS\");\n+\n+  \/\/ Since this region may have served previously as OLD, it may hold obsolete object range info.\n+  heap->card_scan()->reset_object_range(bottom(), end());\n+  heap->card_scan()->mark_range_as_dirty(bottom(), top() - bottom());\n+\n+  HeapWord* obj_addr = bottom();\n+  while (obj_addr < tams) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != NULL, \"klass should not be NULL\");\n+      \/\/ This thread is responsible for registering all objects in this region.  No need for lock.\n+      heap->card_scan()->register_object_without_lock(obj_addr);\n+      obj_addr += obj->size();\n+    } else {\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, tams);\n+      assert(next_marked_obj <= tams, \"next marked object cannot exceed tams\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->card_scan()->register_object_without_lock(obj_addr);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+\n+  \/\/ We do not need to scan above TAMS because top equals tams\n+  assert(obj_addr == tams, \"Expect loop to terminate when obj_addr equals tams\");\n+}\n+\n+void ShenandoahHeapRegion::promote_humongous() {\n@@ -1023,2 +1098,3 @@\n-\n-  size_t spanned_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+  size_t used_bytes = obj->size() * HeapWordSize;\n+  size_t spanned_regions = ShenandoahHeapRegion::required_regions(used_bytes);\n+  size_t humongous_waste = spanned_regions * ShenandoahHeapRegion::region_size_bytes() - obj->size() * HeapWordSize;\n@@ -1026,1 +1102,0 @@\n-\n@@ -1031,16 +1106,0 @@\n-    size_t available_old_regions = old_generation->adjusted_unaffiliated_regions();\n-    if (spanned_regions <= available_old_regions) {\n-      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, index(), spanned_regions);\n-\n-      \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n-      \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n-      \/\/ in the last humongous region that is not spanned by obj is currently not used.\n-      for (size_t i = index(); i < index_limit; i++) {\n-        ShenandoahHeapRegion* r = heap->get_region(i);\n-        log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n-                      r->index(), p2i(r->bottom()), p2i(r->top()));\n-        \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n-        old_generation->increase_used(r->used());\n-        young_generation->decrease_used(r->used());\n-        r->set_affiliation(OLD_GENERATION);\n-      }\n@@ -1048,19 +1107,20 @@\n-      ShenandoahHeapRegion* tail = heap->get_region(index_limit - 1);\n-      size_t waste = tail->free();\n-      if (waste != 0) {\n-        old_generation->increase_humongous_waste(waste);\n-        young_generation->decrease_humongous_waste(waste);\n-      }\n-      \/\/ Then fall through to finish the promotion after releasing the heap lock.\n-    } else {\n-      \/\/ There are not enough available old regions to promote this humongous region at this time, so defer promotion.\n-      \/\/ TODO: Consider allowing the promotion now, with the expectation that we can resize and\/or collect OLD\n-      \/\/ momentarily to address the transient violation of budgets.  Some problems that need to be addressed in order\n-      \/\/ to allow transient violation of capacity budgets are:\n-      \/\/  1. Various size_t subtractions assume usage is less than capacity, and thus assume there will be no\n-      \/\/     arithmetic underflow when we subtract usage from capacity.  The results of such size_t subtractions\n-      \/\/     would need to be guarded and special handling provided.\n-      \/\/  2. ShenandoahVerifier enforces that usage is less than capacity.  If we are going to relax this constraint,\n-      \/\/     we need to think about what conditions allow the constraint to be violated and document and implement the\n-      \/\/     changes.\n-      return 0;\n+    \/\/ We promote humongous objects unconditionally, without checking for availability.  We adjust\n+    \/\/ usage totals, including humongous waste, after evacuation is done.\n+    log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, index(), spanned_regions);\n+\n+    young_generation->decrease_used(used_bytes);\n+    young_generation->decrease_humongous_waste(humongous_waste);\n+    young_generation->decrease_affiliated_region_count(spanned_regions);\n+\n+    \/\/ transfer_to_old() increases capacity of old and decreases capacity of young\n+    heap->generation_sizer()->force_transfer_to_old(spanned_regions);\n+\n+    \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n+    \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n+    \/\/ in the last humongous region that is not spanned by obj is currently not used.\n+    for (size_t i = index(); i < index_limit; i++) {\n+      ShenandoahHeapRegion* r = heap->get_region(i);\n+      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                    r->index(), p2i(r->bottom()), p2i(r->top()));\n+      \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n+      r->set_affiliation(OLD_GENERATION);\n@@ -1068,0 +1128,4 @@\n+\n+    old_generation->increase_affiliated_region_count(spanned_regions);\n+    old_generation->increase_used(used_bytes);\n+    old_generation->increase_humongous_waste(humongous_waste);\n@@ -1085,1 +1149,0 @@\n-  return index_limit - index();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":130,"deletions":67,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -245,0 +245,2 @@\n+  HeapWord* _top_before_promoted;\n+\n@@ -353,0 +355,5 @@\n+  inline void save_top_before_promote();\n+  inline HeapWord* get_top_before_promote() const { return _top_before_promoted; }\n+  inline void restore_top_before_promote();\n+  inline size_t garbage_before_padded_for_promote() const;\n+\n@@ -432,0 +439,1 @@\n+  size_t used_before_promote() const { return byte_size(bottom(), get_top_before_promote()); }\n@@ -460,2 +468,3 @@\n-  \/\/ Sets all remembered set cards to dirty.  Returns the number of regions spanned by the associated humongous object.\n-  size_t promote_humongous();\n+  \/\/ Register all objects.  Set all remembered set cards to dirty.\n+  void promote_humongous();\n+  void promote_in_place();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+      assert(pad_words >= ShenandoahHeap::min_fill_size(), \"pad_words expanded above to meet size constraint\");\n@@ -193,0 +194,11 @@\n+inline size_t ShenandoahHeapRegion::garbage_before_padded_for_promote() const {\n+  size_t used_before_promote = byte_size(bottom(), get_top_before_promote());\n+  assert(get_top_before_promote() != nullptr, \"top before promote should not equal null\");\n+  assert(used_before_promote >= get_live_data_bytes(),\n+         \"Live Data must be a subset of used before promotion live: \" SIZE_FORMAT \" used: \" SIZE_FORMAT,\n+         get_live_data_bytes(), used_before_promote);\n+  size_t result = used_before_promote - get_live_data_bytes();\n+  return result;\n+\n+}\n+\n@@ -243,0 +255,13 @@\n+inline void ShenandoahHeapRegion::save_top_before_promote() {\n+  _top_before_promoted = _top;\n+}\n+\n+inline void ShenandoahHeapRegion::restore_top_before_promote() {\n+  _top = _top_before_promoted;\n+#ifdef ASSERT\n+  _top_before_promoted = nullptr;\n+#endif\n+ }\n+\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-    log_info(gc, init)(\"Young Generation Initial Size: \" SIZE_FORMAT \"%s\",\n+    log_info(gc, init)(\"Young Generation Soft Size: \" SIZE_FORMAT \"%s\",\n@@ -57,1 +57,1 @@\n-    log_info(gc, init)(\"Old Generation Initial Size: \" SIZE_FORMAT \"%s\",\n+    log_info(gc, init)(\"Old Generation Soft Size: \" SIZE_FORMAT \"%s\",\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-\n@@ -56,1 +55,18 @@\n-double ShenandoahMmuTracker::gc_thread_time_seconds() {\n+ShenandoahMmuTracker::ShenandoahMmuTracker() :\n+    _most_recent_timestamp(0.0),\n+    _most_recent_gc_time(0.0),\n+    _most_recent_gcu(0.0),\n+    _most_recent_mutator_time(0.0),\n+    _most_recent_mu(0.0),\n+    _most_recent_periodic_time_stamp(0.0),\n+    _most_recent_periodic_gc_time(0.0),\n+    _most_recent_periodic_mutator_time(0.0),\n+    _mmu_periodic_task(new ShenandoahMmuTask(this)) {\n+}\n+\n+ShenandoahMmuTracker::~ShenandoahMmuTracker() {\n+  _mmu_periodic_task->disenroll();\n+  delete _mmu_periodic_task;\n+}\n+\n+void ShenandoahMmuTracker::fetch_cpu_times(double &gc_time, double &mutator_time) {\n@@ -61,2 +77,2 @@\n-  return double(cl.total_time) \/ NANOSECS_PER_SEC;\n-}\n+  double most_recent_gc_thread_time = double(cl.total_time) \/ NANOSECS_PER_SEC;\n+  gc_time = most_recent_gc_thread_time;\n@@ -64,1 +80,0 @@\n-double ShenandoahMmuTracker::process_time_seconds() {\n@@ -67,2 +82,27 @@\n-  if (valid) {\n-    return process_user_time + process_system_time;\n+  assert(valid, \"don't know why this would not be valid\");\n+  mutator_time =(process_user_time + process_system_time) - most_recent_gc_thread_time;\n+}\n+\n+void ShenandoahMmuTracker::update_utilization(ShenandoahGeneration* generation, size_t gcid, const char *msg) {\n+  double current = os::elapsedTime();\n+  _most_recent_gcid = gcid;\n+  _most_recent_is_full = false;\n+\n+  if (gcid == 0) {\n+    fetch_cpu_times(_most_recent_gc_time, _most_recent_mutator_time);\n+\n+    _most_recent_timestamp = current;\n+  } else {\n+    double gc_cycle_period = current - _most_recent_timestamp;\n+    _most_recent_timestamp = current;\n+\n+    double gc_thread_time, mutator_thread_time;\n+    fetch_cpu_times(gc_thread_time, mutator_thread_time);\n+    double gc_time = gc_thread_time - _most_recent_gc_time;\n+    _most_recent_gc_time = gc_thread_time;\n+    _most_recent_gcu = gc_time \/ (_active_processors * gc_cycle_period);\n+    double mutator_time = mutator_thread_time - _most_recent_mutator_time;\n+    _most_recent_mutator_time = mutator_thread_time;\n+    _most_recent_mu = mutator_time \/ (_active_processors * gc_cycle_period);\n+    log_info(gc, ergo)(\"At end of %s: GCU: %.1f%%, MU: %.1f%% during period of %.3fs\",\n+                       msg, _most_recent_gcu * 100, _most_recent_mu * 100, gc_cycle_period);\n@@ -70,1 +110,0 @@\n-  return 0.0;\n@@ -73,6 +112,2 @@\n-ShenandoahMmuTracker::ShenandoahMmuTracker() :\n-    _generational_reference_time_s(0.0),\n-    _process_reference_time_s(0.0),\n-    _collector_reference_time_s(0.0),\n-    _mmu_periodic_task(new ShenandoahMmuTask(this)),\n-    _mmu_average(10, ShenandoahAdaptiveDecayFactor) {\n+void ShenandoahMmuTracker::record_young(ShenandoahGeneration* generation, size_t gcid) {\n+  update_utilization(generation, gcid, \"Concurrent Young GC\");\n@@ -81,3 +116,34 @@\n-ShenandoahMmuTracker::~ShenandoahMmuTracker() {\n-  _mmu_periodic_task->disenroll();\n-  delete _mmu_periodic_task;\n+void ShenandoahMmuTracker::record_bootstrap(ShenandoahGeneration* generation, size_t gcid, bool candidates_for_mixed) {\n+  \/\/ Not likely that this will represent an \"ideal\" GCU, but doesn't hurt to try\n+  update_utilization(generation, gcid, \"Bootstrap Old GC\");\n+}\n+\n+void ShenandoahMmuTracker::record_old_marking_increment(ShenandoahGeneration* generation, size_t gcid, bool old_marking_done,\n+                                                        bool has_old_candidates) {\n+  \/\/ No special processing for old marking\n+  double now = os::elapsedTime();\n+  double duration = now - _most_recent_timestamp;\n+\n+  double gc_time, mutator_time;\n+  fetch_cpu_times(gc_time, mutator_time);\n+  double gcu = (gc_time - _most_recent_gc_time) \/ duration;\n+  double mu = (mutator_time - _most_recent_mutator_time) \/ duration;\n+  log_info(gc, ergo)(\"At end of %s: GCU: %.1f%%, MU: %.1f%% for duration %.3fs (totals to be subsumed in next gc report)\",\n+                     old_marking_done? \"last OLD marking increment\": \"OLD marking increment\",\n+                     gcu * 100, mu * 100, duration);\n+}\n+\n+void ShenandoahMmuTracker::record_mixed(ShenandoahGeneration* generation, size_t gcid, bool is_mixed_done) {\n+  update_utilization(generation, gcid, \"Mixed Concurrent GC\");\n+}\n+\n+void ShenandoahMmuTracker::record_degenerated(ShenandoahGeneration* generation,\n+                                              size_t gcid, bool is_old_bootstrap, bool is_mixed_done) {\n+  if ((gcid == _most_recent_gcid) && _most_recent_is_full) {\n+    \/\/ Do nothing.  This is a redundant recording for the full gc that just completed.\n+    \/\/ TODO: avoid making the call to record_degenerated() in the case that this degenerated upgraded to full gc.\n+  } else if (is_old_bootstrap) {\n+    update_utilization(generation, gcid, \"Degenerated Bootstrap Old GC\");\n+  } else {\n+    update_utilization(generation, gcid, \"Degenerated Young GC\");\n+  }\n@@ -86,6 +152,3 @@\n-void ShenandoahMmuTracker::record(ShenandoahGeneration* generation) {\n-  shenandoah_assert_control_or_vm_thread();\n-  double collector_time_s = gc_thread_time_seconds();\n-  double elapsed_gc_time_s = collector_time_s - _generational_reference_time_s;\n-  generation->add_collection_time(elapsed_gc_time_s);\n-  _generational_reference_time_s = collector_time_s;\n+void ShenandoahMmuTracker::record_full(ShenandoahGeneration* generation, size_t gcid) {\n+  update_utilization(generation, gcid, \"Full GC\");\n+  _most_recent_is_full = true;\n@@ -96,6 +159,3 @@\n-  double process_time_s = process_time_seconds();\n-  double elapsed_process_time_s = process_time_s - _process_reference_time_s;\n-  if (elapsed_process_time_s <= 0.01) {\n-    \/\/ No cpu time for this interval?\n-    return;\n-  }\n+  double current = os::elapsedTime();\n+  double time_delta = current - _most_recent_periodic_time_stamp;\n+  _most_recent_periodic_time_stamp = current;\n@@ -103,7 +163,12 @@\n-  _process_reference_time_s = process_time_s;\n-  double collector_time_s = gc_thread_time_seconds();\n-  double elapsed_collector_time_s = collector_time_s - _collector_reference_time_s;\n-  _collector_reference_time_s = collector_time_s;\n-  double minimum_mutator_utilization = ((elapsed_process_time_s - elapsed_collector_time_s) \/ elapsed_process_time_s) * 100;\n-  _mmu_average.add(minimum_mutator_utilization);\n-  log_info(gc)(\"Average MMU = %.3f\", _mmu_average.davg());\n+  double gc_time, mutator_time;\n+  fetch_cpu_times(gc_time, mutator_time);\n+\n+  double gc_delta = gc_time - _most_recent_periodic_gc_time;\n+  _most_recent_periodic_gc_time = gc_time;\n+\n+  double mutator_delta = mutator_time - _most_recent_periodic_mutator_time;\n+  _most_recent_periodic_mutator_time = mutator_time;\n+\n+  double mu = mutator_delta \/ (_active_processors * time_delta);\n+  double gcu = gc_delta \/ (_active_processors * time_delta);\n+  log_info(gc)(\"Periodic Sample: GCU = %.3f%%, MU = %.3f%% during most recent %.1fs\", gcu * 100, mu * 100, time_delta);\n@@ -113,3 +178,5 @@\n-  _process_reference_time_s = process_time_seconds();\n-  _generational_reference_time_s = gc_thread_time_seconds();\n-  _collector_reference_time_s = _generational_reference_time_s;\n+  \/\/ initialize static data\n+  _active_processors = os::initial_active_processor_count();\n+\n+  double _most_recent_periodic_time_stamp = os::elapsedTime();\n+  fetch_cpu_times(_most_recent_periodic_gc_time, _most_recent_periodic_mutator_time);\n@@ -163,1 +230,1 @@\n-  return MAX2(uint(min_young_regions), 1U);\n+  return MAX2(min_young_regions, (size_t) 1U);\n@@ -168,1 +235,1 @@\n-  return MAX2(uint(max_young_regions), 1U);\n+  return MAX2(max_young_regions, (size_t) 1U);\n@@ -205,10 +272,2 @@\n-bool ShenandoahGenerationSizer::adjust_generation_sizes() const {\n-  shenandoah_assert_generational();\n-  if (!use_adaptive_sizing()) {\n-    return false;\n-  }\n-\n-  if (_mmu_tracker->average() >= double(GCTimeRatio)) {\n-    return false;\n-  }\n-\n+\/\/ Returns true iff transfer is successful\n+bool ShenandoahGenerationSizer::transfer_to_old(size_t regions) const {\n@@ -216,11 +275,4 @@\n-  ShenandoahOldGeneration *old = heap->old_generation();\n-  ShenandoahYoungGeneration *young = heap->young_generation();\n-  ShenandoahGeneration *global = heap->global_generation();\n-  double old_time_s = old->reset_collection_time();\n-  double young_time_s = young->reset_collection_time();\n-  double global_time_s = global->reset_collection_time();\n-\n-  const double transfer_threshold = 3.0;\n-  double delta = young_time_s - old_time_s;\n-\n-  log_info(gc)(\"Thread Usr+Sys YOUNG = %.3f, OLD = %.3f, GLOBAL = %.3f\", young_time_s, old_time_s, global_time_s);\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t bytes_to_transfer = regions * region_size_bytes;\n@@ -228,2 +280,5 @@\n-  if (abs(delta) <= transfer_threshold) {\n-    log_info(gc, ergo)(\"Difference (%.3f) for thread utilization for each generation is under threshold (%.3f)\", abs(delta), transfer_threshold);\n+  if (young_gen->free_unaffiliated_regions() < regions) {\n+    return false;\n+  } else if (old_gen->max_capacity() + bytes_to_transfer > heap->max_size_for(old_gen)) {\n+    return false;\n+  } else if (young_gen->max_capacity() - bytes_to_transfer < heap->min_size_for(young_gen)) {\n@@ -231,5 +286,0 @@\n-  }\n-\n-  if (delta > 0) {\n-    \/\/ young is busier than old, increase size of young to raise MMU\n-    return transfer_capacity(old, young);\n@@ -237,2 +287,7 @@\n-    \/\/ old is busier than young, increase size of old to raise MMU\n-    return transfer_capacity(young, old);\n+    young_gen->decrease_capacity(bytes_to_transfer);\n+    old_gen->increase_capacity(bytes_to_transfer);\n+    size_t new_size = old_gen->max_capacity();\n+    log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" SIZE_FORMAT \"%s\",\n+                 regions, young_gen->name(), old_gen->name(),\n+                 byte_size_in_proper_unit(new_size), proper_unit_for_byte_size(new_size));\n+    return true;\n@@ -242,8 +297,15 @@\n-bool ShenandoahGenerationSizer::transfer_capacity(ShenandoahGeneration* target) const {\n-  ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock());\n-  if (target->is_young()) {\n-    return transfer_capacity(ShenandoahHeap::heap()->old_generation(), target);\n-  } else {\n-    assert(target->is_old(), \"Expected old generation, if not young.\");\n-    return transfer_capacity(ShenandoahHeap::heap()->young_generation(), target);\n-  }\n+\/\/ This is used when promoting humongous or highly utilized regular regions in place.  It is not required in this situation\n+\/\/ that the transferred regions be unaffiliated.\n+void ShenandoahGenerationSizer::force_transfer_to_old(size_t regions) const {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t bytes_to_transfer = regions * region_size_bytes;\n+\n+  young_gen->decrease_capacity(bytes_to_transfer);\n+  old_gen->increase_capacity(bytes_to_transfer);\n+  size_t new_size = old_gen->max_capacity();\n+  log_info(gc)(\"Forcing transfer of \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" SIZE_FORMAT \"%s\",\n+               regions, young_gen->name(), old_gen->name(),\n+               byte_size_in_proper_unit(new_size), proper_unit_for_byte_size(new_size));\n@@ -252,2 +314,0 @@\n-bool ShenandoahGenerationSizer::transfer_capacity(ShenandoahGeneration* from, ShenandoahGeneration* to) const {\n-  shenandoah_assert_heaplocked_or_safepoint();\n@@ -255,12 +315,6 @@\n-  size_t available_regions = from->free_unaffiliated_regions();\n-  if (available_regions <= 0) {\n-    log_info(gc)(\"%s has no regions available for transfer to %s\", from->name(), to->name());\n-    return false;\n-  }\n-\n-  size_t regions_to_transfer = MAX2(1u, uint(double(available_regions) * _resize_increment));\n-  if (from->is_young()) {\n-    regions_to_transfer = adjust_transfer_from_young(from, regions_to_transfer);\n-  } else {\n-    regions_to_transfer = adjust_transfer_to_young(to, regions_to_transfer);\n-  }\n+bool ShenandoahGenerationSizer::transfer_to_young(size_t regions) const {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t bytes_to_transfer = regions * region_size_bytes;\n@@ -268,4 +322,1 @@\n-  if (regions_to_transfer == 0) {\n-    log_info(gc)(\"No capacity available to transfer from: %s (\" SIZE_FORMAT \"%s) to: %s (\" SIZE_FORMAT \"%s)\",\n-                  from->name(), byte_size_in_proper_unit(from->max_capacity()), proper_unit_for_byte_size(from->max_capacity()),\n-                  to->name(), byte_size_in_proper_unit(to->max_capacity()), proper_unit_for_byte_size(to->max_capacity()));\n+  if (old_gen->free_unaffiliated_regions() < regions) {\n@@ -273,0 +324,12 @@\n+  } else if (young_gen->max_capacity() + bytes_to_transfer > heap->max_size_for(young_gen)) {\n+    return false;\n+  } else if (old_gen->max_capacity() - bytes_to_transfer < heap->min_size_for(old_gen)) {\n+    return false;\n+  } else {\n+    old_gen->decrease_capacity(bytes_to_transfer);\n+    young_gen->increase_capacity(bytes_to_transfer);\n+    size_t new_size = young_gen->max_capacity();\n+    log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" SIZE_FORMAT \"%s\",\n+                 regions, old_gen->name(), young_gen->name(),\n+                 byte_size_in_proper_unit(new_size), proper_unit_for_byte_size(new_size));\n+    return true;\n@@ -274,35 +337,0 @@\n-\n-  log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s\", regions_to_transfer, from->name(), to->name());\n-  from->decrease_capacity(regions_to_transfer * ShenandoahHeapRegion::region_size_bytes());\n-  to->increase_capacity(regions_to_transfer * ShenandoahHeapRegion::region_size_bytes());\n-  return true;\n-}\n-\n-size_t ShenandoahGenerationSizer::adjust_transfer_from_young(ShenandoahGeneration* from, size_t regions_to_transfer) const {\n-  assert(from->is_young(), \"Expect to transfer from young\");\n-  size_t young_capacity_regions = from->max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n-  size_t new_young_regions = young_capacity_regions - regions_to_transfer;\n-  size_t minimum_young_regions = min_young_regions();\n-  \/\/ Check that we are not going to violate the minimum size constraint.\n-  if (new_young_regions < minimum_young_regions) {\n-    assert(minimum_young_regions <= young_capacity_regions, \"Young is under minimum capacity.\");\n-    \/\/ If the transfer violates the minimum size and there is still some capacity to transfer,\n-    \/\/ adjust the transfer to take the size to the minimum. Note that this may be zero.\n-    regions_to_transfer = young_capacity_regions - minimum_young_regions;\n-  }\n-  return regions_to_transfer;\n-}\n-\n-size_t ShenandoahGenerationSizer::adjust_transfer_to_young(ShenandoahGeneration* to, size_t regions_to_transfer) const {\n-  assert(to->is_young(), \"Can only transfer between young and old.\");\n-  size_t young_capacity_regions = to->max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n-  size_t new_young_regions = young_capacity_regions + regions_to_transfer;\n-  size_t maximum_young_regions = max_young_regions();\n-  \/\/ Check that we are not going to violate the maximum size constraint.\n-  if (new_young_regions > maximum_young_regions) {\n-    assert(maximum_young_regions >= young_capacity_regions, \"Young is over maximum capacity\");\n-    \/\/ If the transfer violates the maximum size and there is still some capacity to transfer,\n-    \/\/ adjust the transfer to take the size to the maximum. Note that this may be zero.\n-    regions_to_transfer = maximum_young_regions - young_capacity_regions;\n-  }\n-  return regions_to_transfer;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.cpp","additions":160,"deletions":132,"binary":false,"changes":292,"status":"modified"},{"patch":"@@ -53,0 +53,14 @@\n+private:\n+  \/\/ These variables hold recent snapshots of cumulative quantities that are used for calculating\n+  \/\/ CPU time consumed by GC and mutator threads during each GC cycle.\n+  double _most_recent_timestamp;\n+  double _most_recent_gc_time;\n+  double _most_recent_gcu;\n+  double _most_recent_mutator_time;\n+  double _most_recent_mu;\n+\n+  \/\/ These variables hold recent snapshots of cumulative quantities that are used for reporting\n+  \/\/ periodic consumption of CPU time by GC and mutator threads.\n+  double _most_recent_periodic_time_stamp;\n+  double _most_recent_periodic_gc_time;\n+  double _most_recent_periodic_mutator_time;\n@@ -54,3 +68,4 @@\n-  double _generational_reference_time_s;\n-  double _process_reference_time_s;\n-  double _collector_reference_time_s;\n+  size_t _most_recent_gcid;\n+  uint _active_processors;\n+\n+  bool _most_recent_is_full;\n@@ -61,2 +76,2 @@\n-  static double gc_thread_time_seconds();\n-  static double process_time_seconds();\n+  void update_utilization(ShenandoahGeneration* generation, size_t gcid, const char* msg);\n+  static void fetch_cpu_times(double &gc_time, double &mutator_time);\n@@ -71,6 +86,13 @@\n-  \/\/ This is called at the start and end of a GC cycle. The GC thread times\n-  \/\/ will be accumulated in this generation. Note that the bootstrap cycle\n-  \/\/ for an old collection should be counted against the old generation.\n-  \/\/ When the collector is idle, it still runs a regulator and a control.\n-  \/\/ The times for these threads are attributed to the global generation.\n-  void record(ShenandoahGeneration* generation);\n+  \/\/ At completion of each GC cycle (not including interrupted cycles), we invoke one of the following to record the\n+  \/\/ GC utilization during this cycle.  Incremental efforts spent in an interrupted GC cycle will be accumulated into\n+  \/\/ the CPU time reports for the subsequent completed [degenerated or full] GC cycle.\n+  \/\/\n+  \/\/ We may redundantly record degen and full in the case that a degen upgrades to full.  When this happens, we will invoke\n+  \/\/ both record_full() and record_degenerated() with the same value of gcid.  record_full() is called first and the log\n+  \/\/ reports such a cycle as a FULL cycle.\n+  void record_young(ShenandoahGeneration* generation, size_t gcid);\n+  void record_bootstrap(ShenandoahGeneration* generation, size_t gcid, bool has_old_candidates);\n+  void record_old_marking_increment(ShenandoahGeneration* generation, size_t gcid, bool old_marking_done, bool has_old_candidates);\n+  void record_mixed(ShenandoahGeneration* generation, size_t gcid, bool is_mixed_done);\n+  void record_full(ShenandoahGeneration* generation, size_t gcid);\n+  void record_degenerated(ShenandoahGeneration* generation, size_t gcid, bool is_old_boostrap, bool is_mixed_done);\n@@ -81,1 +103,0 @@\n-  \/\/ This method also logs the average MMU.\n@@ -83,4 +104,0 @@\n-\n-  double average() {\n-    return _mmu_average.davg();\n-  }\n@@ -117,8 +134,0 @@\n-  \/\/ These two methods are responsible for enforcing the minimum and maximum\n-  \/\/ constraints for the size of the generations.\n-  size_t adjust_transfer_from_young(ShenandoahGeneration* from, size_t regions_to_transfer) const;\n-  size_t adjust_transfer_to_young(ShenandoahGeneration* to, size_t regions_to_transfer) const;\n-\n-  \/\/ This will attempt to transfer capacity from one generation to the other. It\n-  \/\/ returns true if a transfer is made, false otherwise.\n-  bool transfer_capacity(ShenandoahGeneration* from, ShenandoahGeneration* to) const;\n@@ -148,13 +157,5 @@\n-  \/\/ This is invoked at the end of a collection. This happens on a safepoint\n-  \/\/ to avoid any races with allocators (and to avoid interfering with\n-  \/\/ allocators by taking the heap lock). The amount of capacity to move\n-  \/\/ from one generation to another is controlled by YoungGenerationSizeIncrement\n-  \/\/ and defaults to 20% of the available capacity of the donor generation.\n-  \/\/ The minimum and maximum sizes of the young generation are controlled by\n-  \/\/ ShenandoahMinYoungPercentage and ShenandoahMaxYoungPercentage, respectively.\n-  \/\/ The method returns true when an adjustment is made, false otherwise.\n-  bool adjust_generation_sizes() const;\n-\n-  \/\/ This may be invoked by a heuristic (from regulator thread) before it\n-  \/\/ decides to run a collection.\n-  bool transfer_capacity(ShenandoahGeneration* target) const;\n+  bool transfer_to_young(size_t regions) const;\n+  bool transfer_to_old(size_t regions) const;\n+\n+  \/\/ force transfer is used when we promote humongous objects.  May violate min\/max limits on generation sizes\n+  void force_transfer_to_old(size_t regions) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.hpp","additions":38,"deletions":37,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -149,0 +150,43 @@\n+  \/\/ We do not rebuild_free following increments of old marking because memory has not been reclaimed..  However, we may\n+  \/\/ need to transfer memory to OLD in order to efficiently support the mixed evacuations that might immediately follow.\n+  size_t evac_slack = heap->young_generation()->heuristics()->evac_slack(0);\n+  heap->adjust_generation_sizes_for_next_cycle(evac_slack, 0, 0);\n+\n+  bool success;\n+  size_t region_xfer;\n+  const char* region_destination;\n+  ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+\n+    size_t old_region_surplus = heap->get_old_region_surplus();\n+    size_t old_region_deficit = heap->get_old_region_deficit();\n+    if (old_region_surplus) {\n+      success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+      region_destination = \"young\";\n+      region_xfer = old_region_surplus;\n+    } else if (old_region_deficit) {\n+      success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+      region_destination = \"old\";\n+      region_xfer = old_region_deficit;\n+      if (!success) {\n+        ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+      }\n+    } else {\n+      region_destination = \"none\";\n+      region_xfer = 0;\n+      success = true;\n+    }\n+    heap->set_old_region_surplus(0);\n+    heap->set_old_region_deficit(0);\n+  }\n+\n+  \/\/ Report outside the heap lock\n+  size_t young_available = young_gen->available();\n+  size_t old_available = old_gen->available();\n+  log_info(gc, ergo)(\"After old marking finished, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                     SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                     success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                     byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                     byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -177,1 +177,2 @@\n-    _state(IDLE)\n+    _state(IDLE),\n+    _growth_before_compaction(INITIAL_GROWTH_BEFORE_COMPACTION)\n@@ -179,0 +180,1 @@\n+  _live_bytes_after_last_mark = ShenandoahHeap::heap()->capacity() * INITIAL_LIVE_FRACTION \/ FRACTIONAL_DENOMINATOR;\n@@ -183,0 +185,16 @@\n+size_t ShenandoahOldGeneration::get_live_bytes_after_last_mark() const {\n+  return _live_bytes_after_last_mark;\n+}\n+\n+void ShenandoahOldGeneration::set_live_bytes_after_last_mark(size_t bytes) {\n+  _live_bytes_after_last_mark = bytes;\n+  if (_growth_before_compaction > MINIMUM_GROWTH_BEFORE_COMPACTION) {\n+    _growth_before_compaction \/= 2;\n+  }\n+}\n+\n+size_t ShenandoahOldGeneration::usage_trigger_threshold() const {\n+  size_t result = _live_bytes_after_last_mark + (_live_bytes_after_last_mark * _growth_before_compaction) \/ FRACTIONAL_DENOMINATOR;\n+  return result;\n+}\n+\n@@ -258,0 +276,1 @@\n+\n@@ -325,1 +344,5 @@\n-    heap->free_set()->rebuild();\n+    size_t cset_young_regions, cset_old_regions;\n+    heap->free_set()->prepare_to_rebuild(cset_young_regions, cset_old_regions);\n+    \/\/ This is just old-gen completion.  No future budgeting required here.  The only reason to rebuild the freeset here\n+    \/\/ is in case there was any immediate old garbage identified.\n+    heap->free_set()->rebuild(cset_young_regions, cset_old_regions);\n@@ -408,1 +431,2 @@\n-      assert(_old_heuristics->unprocessed_old_collection_candidates() == 0, \"Cannot become idle with collection candidates\");\n+      assert(!heap->mode()->is_generational() ||\n+             (_old_heuristics->unprocessed_old_collection_candidates() == 0), \"Cannot become idle with collection candidates\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":27,"deletions":3,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -93,0 +93,22 @@\n+  static const size_t FRACTIONAL_DENOMINATOR = 64536;\n+\n+  \/\/ During initialization of the JVM, we search for the correct old-gen size by initally performing old-gen\n+  \/\/ collection when old-gen usage is 50% more (INITIAL_GROWTH_BEFORE_COMPACTION) than the initial old-gen size\n+  \/\/ estimate (3.125% of heap).  The next old-gen trigger occurs when old-gen grows 25% larger than its live\n+  \/\/ memory at the end of the first old-gen collection.  Then we trigger again when old-gen growns 12.5%\n+  \/\/ more than its live memory at the end of the previous old-gen collection.  Thereafter, we trigger each time\n+  \/\/ old-gen grows more than 12.5% following the end of its previous old-gen collection.\n+  static const size_t INITIAL_GROWTH_BEFORE_COMPACTION = FRACTIONAL_DENOMINATOR \/ 2;          \/\/  50.0%\n+  static const size_t MINIMUM_GROWTH_BEFORE_COMPACTION = FRACTIONAL_DENOMINATOR \/ 8;          \/\/  12.5%\n+\n+  \/\/ INITIAL_LIVE_FRACTION represents the initial guess of how large old-gen should be.  We estimate that old-gen\n+  \/\/ needs to consume 3.125% of the total heap size.  And we \"pretend\" that we start out with this amount of live\n+  \/\/ old-gen memory.  The first old-collection trigger will occur when old-gen occupies 50% more than this initial\n+  \/\/ approximation of the old-gen memory requirement, in other words when old-gen usage is 150% of 3.125%, which\n+  \/\/ is 4.6875% of the total heap size.\n+  static const uint16_t INITIAL_LIVE_FRACTION = FRACTIONAL_DENOMINATOR \/ 32;                    \/\/   3.125%\n+  size_t _live_bytes_after_last_mark;\n+  size_t _growth_before_compaction; \/\/ How much growth in usage before we trigger old collection, per 65_536\n+\n+  void validate_transition(State new_state) NOT_DEBUG_RETURN;\n+\n@@ -98,0 +120,7 @@\n+  void transition_to(State new_state);\n+\n+  size_t get_live_bytes_after_last_mark() const;\n+  void set_live_bytes_after_last_mark(size_t new_live);\n+\n+  size_t usage_trigger_threshold() const;\n+\n@@ -103,3 +132,0 @@\n-\n-  void transition_to(State new_state);\n-  void validate_transition(State new_state) NOT_DEBUG_RETURN;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -143,1 +143,2 @@\n-  return _old_heuristics->should_start_gc() && _control_thread->request_concurrent_gc(OLD);\n+  return !ShenandoahHeap::heap()->doing_mixed_evacuations() && !ShenandoahHeap::heap()->collection_set()->has_old_regions() &&\n+    _old_heuristics->should_start_gc() && _control_thread->request_concurrent_gc(OLD);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -406,1 +406,3 @@\n-  static void validate_usage(const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+  static void validate_usage(const bool adjust_for_padding, const bool adjust_for_deferred_accounting,\n+                             const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -408,0 +410,24 @@\n+    size_t generation_used_regions = generation->used_regions();\n+    if (adjust_for_deferred_accounting) {\n+      ShenandoahGeneration* young_generation = heap->young_generation();\n+      size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+      size_t humongous_bytes_promoted = heap->get_promotable_humongous_usage();\n+      size_t total_regions_promoted = humongous_regions_promoted;\n+      size_t bytes_promoted_in_place = 0;\n+      if (total_regions_promoted > 0) {\n+        bytes_promoted_in_place = humongous_bytes_promoted;\n+      }\n+      if (generation->is_young()) {\n+        generation_used -= bytes_promoted_in_place;\n+        generation_used_regions -= total_regions_promoted;\n+      } else if (generation->is_old()) {\n+        generation_used += bytes_promoted_in_place;\n+        generation_used_regions += total_regions_promoted;\n+      }\n+      \/\/ else, global validation doesn't care where the promoted-in-place data is tallied.\n+    }\n+    if (adjust_for_padding && (generation->is_young() || generation->is_global())) {\n+      size_t pad = ShenandoahHeap::heap()->get_pad_for_promote_in_place();\n+      generation_used += pad;\n+    }\n+\n@@ -414,1 +440,1 @@\n-    guarantee(stats.regions() == generation->used_regions(),\n+    guarantee(stats.regions() == generation_used_regions,\n@@ -418,6 +444,17 @@\n-\/\/ This check is disabled because of known issues with this feature. We expect this code to be updated by 05\/2023.\n-\/\/    size_t capacity = generation->adjusted_capacity();\n-\/\/    guarantee(stats.span() <= capacity,\n-\/\/              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") must not exceed current capacity (\" SIZE_FORMAT \"%s)\",\n-\/\/              label, generation->name(), stats.regions(),\n-\/\/              byte_size_in_proper_unit(capacity), proper_unit_for_byte_size(capacity));\n+    size_t generation_capacity = generation->max_capacity();\n+    size_t humongous_regions_promoted = 0;\n+    if (adjust_for_deferred_accounting) {\n+      humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+      size_t transferred_regions = humongous_regions_promoted;\n+      if (generation->is_old()) {\n+        \/\/ Promoted-in-place regions are labeled as old, but generation->max_capacity() has not yet been increased\n+        generation_capacity += transferred_regions * ShenandoahHeapRegion::region_size_bytes();\n+      } else if (generation->is_young()) {\n+        \/\/ Promoted-in-place regions are labeled as old, but generation->max_capacity() has not yet been decreased\n+        generation_capacity -= transferred_regions * ShenandoahHeapRegion::region_size_bytes();\n+      }\n+    }\n+    guarantee(stats.span() <= generation_capacity,\n+              \"%s: generation (%s) size spanned by regions (\" SIZE_FORMAT \") must not exceed current capacity (\" SIZE_FORMAT \"%s)\",\n+              label, generation->name(), stats.regions(),\n+              byte_size_in_proper_unit(generation_capacity), proper_unit_for_byte_size(generation_capacity));\n@@ -426,0 +463,14 @@\n+    if (adjust_for_deferred_accounting) {\n+      size_t promoted_humongous_bytes = heap->get_promotable_humongous_usage();\n+      size_t promoted_regions_span = humongous_regions_promoted * ShenandoahHeapRegion::region_size_bytes();\n+      assert(promoted_regions_span >= promoted_humongous_bytes, \"sanity\");\n+      size_t promoted_waste = promoted_regions_span - promoted_humongous_bytes;\n+      if (generation->is_old()) {\n+        \/\/ Promoted-in-place regions are labeled as old, but generation->get_humongous_waste() has not yet been increased\n+        humongous_waste += promoted_waste;\n+      } else if (generation->is_young()) {\n+        \/\/ Promoted-in-place regions are labeled as old, but generation->get_humongous_waste() has not yet been decreased\n+        assert(humongous_waste >= promoted_waste, \"Cannot promote in place more waste than exists in young\");\n+        humongous_waste -= promoted_waste;\n+      }\n+    }\n@@ -746,0 +797,1 @@\n+                                             VerifySize sizeness,\n@@ -798,0 +850,1 @@\n+      \/\/ Old generation marking is allowed in all states.\n@@ -816,7 +869,14 @@\n-    size_t heap_used = _heap->used();\n-    guarantee(cl.used() == heap_used,\n-              \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n-              label,\n-              byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n-              byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n-\n+    size_t heap_used;\n+    if (_heap->mode()->is_generational() && (sizeness == _verify_size_adjusted_for_padding)) {\n+      \/\/ Prior to evacuation, regular regions that are to be evacuated in place are padded to prevent further allocations\n+      heap_used = _heap->used() + _heap->get_pad_for_promote_in_place();\n+    } else if (sizeness != _verify_size_disable) {\n+      heap_used = _heap->used();\n+    }\n+    if (sizeness != _verify_size_disable) {\n+      guarantee(cl.used() == heap_used,\n+                \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+                label,\n+                byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n+                byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n+    }\n@@ -872,3 +932,20 @@\n-    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->old_generation(),    cl.old);\n-    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->young_generation(),  cl.young);\n-    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->global_generation(), cl.global);\n+#ifdef KELVIN_DEPRECATE\n+    \/\/ I think I can also deprecate second argument to validate_usage:\n+    \/\/ that is always false\n+\n+    if (sizeness == _verify_size_adjusted_for_deferred_accounting) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, true, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, true, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->global_generation(), cl.global);\n+    } else\n+#endif\n+    if (sizeness == _verify_size_adjusted_for_padding) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, false, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, false, label, _heap->global_generation(), cl.global);\n+    } else if (sizeness == _verify_size_exact) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->global_generation(), cl.global);\n+    }\n+    \/\/ else: sizeness must equal _verify_size_disable\n@@ -986,0 +1063,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -992,2 +1070,3 @@\n-            \"Before Mark\",\n-            _verify_remembered_before_marking,  \/\/ verify read-only remembered set from bottom() to top()\n+          \"Before Mark\",\n+          _verify_remembered_before_marking,\n+                                       \/\/ verify read-only remembered set from bottom() to top()\n@@ -999,0 +1078,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -1008,1 +1088,2 @@\n-          _verify_marked_complete_except_references, \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n+          _verify_marked_complete_except_references,\n+                                       \/\/ bitmaps as precise as we can get, except dangling j.l.r.Refs\n@@ -1012,0 +1093,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -1025,0 +1107,2 @@\n+          _verify_size_adjusted_for_padding,         \/\/ expect generation and heap sizes to match after adjustments\n+                                                     \/\/  for promote in place padding\n@@ -1038,0 +1122,1 @@\n+          _verify_size_disable,       \/\/ we don't know how much of promote-in-place work has been completed\n@@ -1051,0 +1136,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -1059,6 +1145,7 @@\n-          _verify_forwarded_allow,                     \/\/ forwarded references allowed\n-          _verify_marked_complete,                     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n-          _verify_cset_forwarded,                      \/\/ all cset refs are fully forwarded\n-          _verify_liveness_disable,                    \/\/ no reliable liveness data anymore\n-          _verify_regions_notrash,                     \/\/ trash regions have been recycled already\n-          _verify_gcstate_updating                     \/\/ evacuation should have produced some forwarded objects\n+          _verify_forwarded_allow,     \/\/ forwarded references allowed\n+          _verify_marked_complete,     \/\/ bitmaps might be stale, but alloc-after-mark should be well\n+          _verify_cset_forwarded,      \/\/ all cset refs are fully forwarded\n+          _verify_liveness_disable,    \/\/ no reliable liveness data anymore\n+          _verify_regions_notrash,     \/\/ trash regions have been recycled already\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n+          _verify_gcstate_updating     \/\/ evacuation should have produced some forwarded objects\n@@ -1068,0 +1155,1 @@\n+\/\/ We have not yet cleanup (reclaimed) the collection set\n@@ -1077,0 +1165,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -1090,0 +1179,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -1103,0 +1193,1 @@\n+          _verify_size_disable,        \/\/ if we degenerate during evacuation, usage not valid: padding and deferred accounting\n@@ -1116,0 +1207,1 @@\n+          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n@@ -1317,1 +1409,1 @@\n-          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr,\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, nullptr, nullptr,\n@@ -1333,1 +1425,1 @@\n-              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr,\n+              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, nullptr, nullptr,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":121,"deletions":29,"binary":false,"changes":150,"status":"modified"},{"patch":"@@ -144,0 +144,15 @@\n+  typedef enum {\n+    \/\/ Disable size verification\n+    _verify_size_disable,\n+\n+    \/\/ Enforce exact consistency\n+    _verify_size_exact,\n+\n+    \/\/ Expect promote-in-place adjustments: padding inserted to temporarily prevent further allocation in regular regions\n+    _verify_size_adjusted_for_padding,\n+#ifdef KELVIN_DEPRECATE\n+    \/\/ Expect promote-in-place adjustments: usage within regions promoted in place is transferred at end of update refs\n+    _verify_size_adjusted_for_deferred_accounting\n+#endif\n+  } VerifySize;\n+\n@@ -192,0 +207,1 @@\n+                           VerifySize sizeness,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-  product(uintx, ShenandoahOldGarbageThreshold, 10, EXPERIMENTAL,           \\\n+  product(uintx, ShenandoahOldGarbageThreshold, 15, EXPERIMENTAL,           \\\n@@ -131,7 +131,0 @@\n-  product(uintx, ShenandoahOldMinFreeThreshold, 5, EXPERIMENTAL,            \\\n-          \"Percentage of free old generation heap memory below which most \" \\\n-          \"heuristics trigger collection independent of other triggers. \"   \\\n-          \"Provides a safety margin for many heuristics. In percents of \"   \\\n-          \"(soft) max heap size.\")                                          \\\n-          range(0,100)                                                      \\\n-                                                                            \\\n@@ -213,2 +206,2 @@\n-  product(uintx, ShenandoahGuaranteedYoungGCInterval, 5*60*1000, EXPERIMENTAL, \\\n-          \"Run a collection of the young generation at least this often. \"  \\\n+  product(uintx, ShenandoahGuaranteedYoungGCInterval, 30*1000,  EXPERIMENTAL,  \\\n+          \"Run a collection of the young generation at least this often. \"    \\\n@@ -303,11 +296,14 @@\n-  product(double, ShenandoahGenerationalEvacWaste, 2.0, EXPERIMENTAL,       \\\n-          \"For generational mode, how much waste evacuations produce \"      \\\n-          \"within the reserved space.  Larger values make evacuations \"     \\\n-          \"more resilient against evacuation conflicts, at expense of \"     \\\n-          \"evacuating less on each GC cycle.  Smaller values increase \"     \\\n-          \"the risk of evacuation failures, which will trigger \"            \\\n-          \"stop-the-world Full GC passes.  The default value for \"          \\\n-          \"generational mode is 2.0.  The reason for the higher default \"   \\\n-          \"value in generational mode is because generational mode \"        \\\n-          \"enforces the evacuation budget, triggering degenerated GC \"      \\\n-          \"which upgrades to full GC whenever the budget is exceeded.\")     \\\n+  product(double, ShenandoahOldEvacWaste, 1.4, EXPERIMENTAL,                \\\n+          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahPromoEvacWaste, 1.2, EXPERIMENTAL,              \\\n+          \"How much waste promotions produce within the reserved space. \"   \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of promoting less on each \"     \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n@@ -342,15 +338,1 @@\n-  product(uintx, ShenandoahOldEvacReserve, 2, EXPERIMENTAL,                 \\\n-          \"How much of old-generation heap to reserve for old-generation \"  \\\n-          \"evacuations.  Larger values allow GC to evacuate more live \"     \\\n-          \"old-generation objects on every cycle, while potentially \"       \\\n-          \"creating greater impact on the cadence at which the young- \"     \\\n-          \"generation allocation pool is replenished.  During mixed \"       \\\n-          \"evacuations, the bound on amount of old-generation heap \"        \\\n-          \"regions included in the collecdtion set is the smaller \"         \\\n-          \"of the quantities specified by this parameter and the \"          \\\n-          \"size of ShenandoahEvacReserve as adjusted by the value of \"      \\\n-          \"ShenandoahOldEvacRatioPercent.  In percents of total \"           \\\n-          \"old-generation heap size.\")                                      \\\n-          range(1,100)                                                      \\\n-                                                                            \\\n-  product(uintx, ShenandoahOldEvacRatioPercent, 12, EXPERIMENTAL,           \\\n+  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n@@ -358,4 +340,8 @@\n-          \"a percent ratio.  The default value 12 denotes that no more \"    \\\n-          \"than one eighth (12%) of the collection set evacuation \"         \\\n-          \"workload may be comprised of old-gen heap regions.  A larger \"   \\\n-          \"value allows a smaller number of mixed evacuations to process \"  \\\n+          \"a percent ratio.  The default value 75 denotes that no more \"    \\\n+          \"than 75% of the collection set evacuation \"                      \\\n+          \"workload may be evacuate to old-gen heap regions.  This limits \" \\\n+          \"both the promotion of aged regions and the compaction of \"       \\\n+          \"existing old regions.  A value of 75 denotes that the normal \"   \\\n+          \"young-gen evacuation is increased by up to four fold. \"          \\\n+          \"A larger value allows quicker promotion and allows\"              \\\n+          \"a smaller number of mixed evacuations to process \"               \\\n@@ -381,1 +367,1 @@\n-  product(uintx, ShenandoahMaxYoungPercentage, 80, EXPERIMENTAL,            \\\n+  product(uintx, ShenandoahMaxYoungPercentage, 100, EXPERIMENTAL,           \\\n@@ -434,1 +420,1 @@\n-  product(uintx, ShenandoahFullGCThreshold, 3, EXPERIMENTAL,                \\\n+  product(uintx, ShenandoahFullGCThreshold, 64, EXPERIMENTAL,               \\\n@@ -521,14 +507,0 @@\n-  product(uintx, ShenandoahBorrowPercent, 30, EXPERIMENTAL,                 \\\n-          \"During evacuation and reference updating in generational \"       \\\n-          \"mode, new allocations are allowed to borrow from old-gen \"       \\\n-          \"memory up to ShenandoahBorrowPercent \/ 100 amount of the \"       \\\n-          \"young-generation content of the current collection set.  \"       \\\n-          \"Any memory borrowed from old-gen during evacuation and \"         \\\n-          \"update-references phases of GC will be repaid from the \"         \\\n-          \"abundance of young-gen memory produced when the collection \"     \\\n-          \"set is recycled at the end of updating references.  The \"        \\\n-          \"default value of 30 reserves 70% of the to-be-reclaimed \"        \\\n-          \"young collection set memory to be allocated during the \"         \\\n-          \"subsequent concurrent mark phase of GC.\")                        \\\n-          range(0, 100)                                                     \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":28,"deletions":56,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -86,5 +86,0 @@\n-gc\/shenandoah\/oom\/TestThreadFailure.java 8306335 generic-all\n-gc\/shenandoah\/oom\/TestClassLoaderLeak.java 8306336 generic-all\n-gc\/stress\/gclocker\/TestGCLockerWithShenandoah.java#generational 8306341 generic-all\n-gc\/TestAllocHumongousFragment.java#generational 8306342 generic-all\n-\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"}]}