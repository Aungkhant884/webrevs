{"files":[{"patch":"@@ -33,0 +33,2 @@\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -59,2 +61,0 @@\n-const uint ShenandoahAdaptiveHeuristics::MINIMUM_RESIZE_INTERVAL = 10;\n-\n@@ -110,0 +110,1 @@\n+\n@@ -111,0 +112,17 @@\n+    for (size_t idx = 0; idx < size; idx++) {\n+      ShenandoahHeapRegion* r = data[idx]._region;\n+      if (cset->is_preselected(r->index())) {\n+        assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n+        \/\/ Entire region will be promoted, This region does not impact young-gen or old-gen evacuation reserve.\n+        \/\/ This region has been pre-selected and its impact on promotion reserve is already accounted for.\n+\n+        \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n+        \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n+        \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n+        \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n+        \/\/ within youn-gen memory.\n+\n+        cur_young_garbage += r->garbage();\n+        cset->add_region(r);\n+      }\n+    }\n@@ -114,1 +132,1 @@\n-      size_t max_old_cset    = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n+      size_t max_old_cset    = (size_t) (heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n@@ -127,0 +145,3 @@\n+        if (cset->is_preselected(r->index())) {\n+          continue;\n+        }\n@@ -134,11 +155,0 @@\n-        } else if (cset->is_preselected(r->index())) {\n-          assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n-          \/\/ Entire region will be promoted, This region does not impact young-gen or old-gen evacuation reserve.\n-          \/\/ This region has been pre-selected and its impact on promotion reserve is already accounted for.\n-          add_region = true;\n-          \/\/ r->used() is r->garbage() + r->get_live_data_bytes()\n-          \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n-          \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n-          \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n-          \/\/ within youn-gen memory.\n-          cur_young_garbage += r->used();\n@@ -177,28 +187,12 @@\n-        bool add_region = false;\n-\n-        if (!r->is_old()) {\n-          if (cset->is_preselected(r->index())) {\n-            assert(r->age() >= InitialTenuringThreshold, \"Preselected regions must have tenure age\");\n-            \/\/ Entire region will be promoted, This region does not impact young-gen evacuation reserve.  Memory has already\n-            \/\/ been set aside to hold evacuation results as advance_promotion_reserve.\n-            add_region = true;\n-            \/\/ Since all live data in this region is being evacuated from young-gen, it is as if this memory\n-            \/\/ is garbage insofar as young-gen is concerned.  Counting this as garbage reduces the need to\n-            \/\/ reclaim highly utilized young-gen regions just for the sake of finding min_garbage to reclaim\n-            \/\/ within youn-gen memory\n-            cur_young_garbage += r->get_live_data_bytes();\n-          } else if  (r->age() < InitialTenuringThreshold) {\n-            size_t new_cset = cur_cset + r->get_live_data_bytes();\n-            size_t region_garbage = r->garbage();\n-            size_t new_garbage = cur_young_garbage + region_garbage;\n-            bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n-            if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n-              add_region = true;\n-              cur_cset = new_cset;\n-              cur_young_garbage = new_garbage;\n-            }\n-          }\n-          \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n-          \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects.\n-\n-          if (add_region) {\n+        if (cset->is_preselected(r->index())) {\n+          continue;\n+        }\n+        if  (r->age() < InitialTenuringThreshold) {\n+          size_t new_cset = cur_cset + r->get_live_data_bytes();\n+          size_t region_garbage = r->garbage();\n+          size_t new_garbage = cur_young_garbage + region_garbage;\n+          bool add_regardless = (region_garbage > ignore_threshold) && (new_garbage < min_garbage);\n+          assert(r->is_young(), \"Only young candidates expected in the data array\");\n+          if ((new_cset <= max_cset) && (add_regardless || (region_garbage > garbage_threshold))) {\n+            cur_cset = new_cset;\n+            cur_young_garbage = new_garbage;\n@@ -208,0 +202,3 @@\n+        \/\/ Note that we do not add aged regions if they were not pre-selected.  The reason they were not preselected\n+        \/\/ is because there is not sufficient room in old-gen to hold their to-be-promoted live objects or because\n+        \/\/ they are to be promoted in place.\n@@ -244,0 +241,10 @@\n+\n+  size_t collected_old = cset->get_old_bytes_reserved_for_evacuation();\n+  size_t collected_promoted = cset->get_young_bytes_to_be_promoted();\n+  size_t collected_young = cset->get_young_bytes_reserved_for_evacuation();\n+\n+  log_info(gc, ergo)(\"Chosen CSet evacuates young: \" SIZE_FORMAT \"%s (of which at least: \" SIZE_FORMAT \"%s are to be promoted), \"\n+                     \"old: \" SIZE_FORMAT \"%s\",\n+                     byte_size_in_proper_unit(collected_young),    proper_unit_for_byte_size(collected_young),\n+                     byte_size_in_proper_unit(collected_promoted), proper_unit_for_byte_size(collected_promoted),\n+                     byte_size_in_proper_unit(collected_old),      proper_unit_for_byte_size(collected_old));\n@@ -249,1 +256,0 @@\n-  ++_cycles_since_last_resize;\n@@ -324,0 +330,74 @@\n+\/\/ Return conservative estimate of how much memory can be allocated before we need to start GC\n+size_t ShenandoahAdaptiveHeuristics::evac_slack(size_t young_regions_to_be_reclaimed) {\n+  assert(_generation->is_young(), \"evac_slack is only meaningful for young-gen heuristic\");\n+\n+  size_t max_capacity = _generation->max_capacity();\n+  size_t capacity = _generation->soft_max_capacity();\n+  size_t available = _generation->available();\n+  size_t allocated = _generation->bytes_allocated_since_gc_start();\n+\n+  size_t available_young_collected = ShenandoahHeap::heap()->collection_set()->get_young_available_bytes_collected();\n+  size_t anticipated_available =\n+    available + young_regions_to_be_reclaimed * ShenandoahHeapRegion::region_size_bytes() - available_young_collected;\n+  size_t allocation_headroom = anticipated_available;\n+  size_t spike_headroom = capacity * ShenandoahAllocSpikeFactor \/ 100;\n+  size_t penalties      = capacity * _gc_time_penalties \/ 100;\n+\n+  double rate = _allocation_rate.sample(allocated);\n+\n+  \/\/ At what value of available, would avg and spike triggers occur?\n+  \/\/  if allocation_headroom < avg_cycle_time * avg_alloc_rate, then we experience avg trigger\n+  \/\/  if allocation_headroom < avg_cycle_time * rate, then we experience spike trigger if is_spiking\n+  \/\/\n+  \/\/ allocation_headroom =\n+  \/\/     0, if penalties > available or if penalties + spike_headroom > available\n+  \/\/     available - penalties - spike_headroom, otherwise\n+  \/\/\n+  \/\/ so we trigger if available - penalties - spike_headroom < avg_cycle_time * avg_alloc_rate, which is to say\n+  \/\/                  available < avg_cycle_time * avg_alloc_rate + penalties + spike_headroom\n+  \/\/            or if available < penalties + spike_headroom\n+  \/\/\n+  \/\/ since avg_cycle_time * avg_alloc_rate > 0, the first test is sufficient to test both conditions\n+  \/\/\n+  \/\/ thus, avg_evac_slack is MIN2(0,  available - avg_cycle_time * avg_alloc_rate + penalties + spike_headroom)\n+  \/\/\n+  \/\/ similarly, spike_evac_slack is MIN2(0, available - avg_cycle_time * rate + penalties + spike_headroom)\n+  \/\/ but spike_evac_slack is only relevant if is_spiking, as defined below.\n+\n+  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+\n+  \/\/ TODO: Consider making conservative adjustments to avg_cycle_time, such as: (avg_cycle_time *= 2) in cases where\n+  \/\/ we expect a longer-than-normal GC duration.  This includes mixed evacuations, evacuation that perform promotion\n+  \/\/ including promotion in place, and OLD GC bootstrap cycles.  It has been observed that these cycles sometimes\n+  \/\/ require twice or more the duration of \"normal\" GC cycles.  We have experimented with this approach.  While it\n+  \/\/ does appear to reduce the frequency of degenerated cycles due to late triggers, it also has the effect of reducing\n+  \/\/ evacuation slack so that there is less memory available to be transferred to OLD.  The result is that we\n+  \/\/ throttle promotion and it takes too long to move old objects out of the young generation.\n+\n+  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+  size_t evac_slack_avg;\n+  if (anticipated_available > avg_cycle_time * avg_alloc_rate + penalties + spike_headroom) {\n+    evac_slack_avg = anticipated_available - (avg_cycle_time * avg_alloc_rate + penalties + spike_headroom);\n+  } else {\n+    \/\/ we have no slack because it's already time to trigger\n+    evac_slack_avg = 0;\n+  }\n+\n+  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+  size_t evac_slack_spiking;\n+  if (is_spiking) {\n+    if (anticipated_available > avg_cycle_time * rate + penalties + spike_headroom) {\n+      evac_slack_spiking = anticipated_available - (avg_cycle_time * rate + penalties + spike_headroom);\n+    } else {\n+      \/\/ we have no slack because it's already time to trigger\n+      evac_slack_spiking = 0;\n+    }\n+  } else {\n+    evac_slack_spiking = evac_slack_avg;\n+  }\n+\n+  size_t threshold = min_free_threshold();\n+  size_t evac_min_threshold = (anticipated_available > threshold)? anticipated_available - threshold: 0;\n+  return MIN3(evac_slack_spiking, evac_slack_avg, evac_min_threshold);\n+}\n+\n@@ -350,7 +430,2 @@\n-  if (available < min_threshold) {\n-    log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n-                 _generation->name(),\n-                 byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n-                 byte_size_in_proper_unit(min_threshold),       proper_unit_for_byte_size(min_threshold));\n-    return resize_and_evaluate();\n-  }\n+  \/\/ OLD generation is maintained to be as small as possible.  Depletion-of-free-pool triggers do not apply to old generation.\n+  if (!_generation->is_old()) {\n@@ -358,7 +433,3 @@\n-  \/\/ Check if we need to learn a bit about the application\n-  const size_t max_learn = ShenandoahLearningSteps;\n-  if (_gc_times_learned < max_learn) {\n-    size_t init_threshold = capacity \/ 100 * ShenandoahInitFreeThreshold;\n-    if (available < init_threshold) {\n-      log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\" SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n-                   _generation->name(), _gc_times_learned + 1, max_learn,\n+    if (available < min_threshold) {\n+      log_info(gc)(\"Trigger (%s): Free (\" SIZE_FORMAT \"%s) is below minimum threshold (\" SIZE_FORMAT \"%s)\",\n+                   _generation->name(),\n@@ -366,1 +437,1 @@\n-                   byte_size_in_proper_unit(init_threshold),      proper_unit_for_byte_size(init_threshold));\n+                   byte_size_in_proper_unit(min_threshold),       proper_unit_for_byte_size(min_threshold));\n@@ -369,1 +440,0 @@\n-  }\n@@ -371,48 +441,13 @@\n-  \/\/  Rationale:\n-  \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n-  \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n-  \/\/\n-  \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n-  \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n-  \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n-  \/\/       accomodation to give us enough runway to recalibrate our \"average allocation rate\".\n-  \/\/\n-  \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n-  \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n-  \/\/       enough runway to recalibrate our \"average allocation rate\".\n-  \/\/\n-  \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n-  \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n-  \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n-  \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n-  \/\/\n-  \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n-  \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n-  \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n-  \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n-  \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n-  \/\/          of how this will behave.\n-  \/\/\n-  \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n-  \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n-  \/\/    computations an independent effort.\n-\n-\n-  \/\/ TODO: Account for inherent delays in responding to GC triggers\n-  \/\/  1. It has been observed that delays of 200 ms or greater are common between the moment we return true from should_start_gc()\n-  \/\/     and the moment at which we begin execution of the concurrent reset phase.  Add this time into the calculation of\n-  \/\/     avg_cycle_time below.  (What is \"this time\"?  Perhaps we should remember recent history of this delay for the\n-  \/\/     running workload and use the maximum delay recently seen for \"this time\".)\n-  \/\/  2. The frequency of inquiries to should_start_gc() is adaptive, ranging between ShenandoahControlIntervalMin and\n-  \/\/     ShenandoahControlIntervalMax.  The current control interval (or the max control interval) should also be added into\n-  \/\/     the calculation of avg_cycle_time below.\n-\n-  \/\/ Check if allocation headroom is still okay. This also factors in:\n-  \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n-  \/\/   2. Accumulated penalties from Degenerated and Full GC\n-  size_t allocation_headroom = available;\n-  size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n-  size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n-\n-  allocation_headroom -= MIN2(allocation_headroom, penalties);\n-  allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n+    \/\/ Check if we need to learn a bit about the application\n+    const size_t max_learn = ShenandoahLearningSteps;\n+    if (_gc_times_learned < max_learn) {\n+      size_t init_threshold = capacity \/ 100 * ShenandoahInitFreeThreshold;\n+      if (available < init_threshold) {\n+        log_info(gc)(\"Trigger (%s): Learning \" SIZE_FORMAT \" of \" SIZE_FORMAT \". Free (\"\n+                     SIZE_FORMAT \"%s) is below initial threshold (\" SIZE_FORMAT \"%s)\",\n+                     _generation->name(), _gc_times_learned + 1, max_learn,\n+                     byte_size_in_proper_unit(available), proper_unit_for_byte_size(available),\n+                     byte_size_in_proper_unit(init_threshold),      proper_unit_for_byte_size(init_threshold));\n+        return true;\n+      }\n+    }\n@@ -420,1 +455,28 @@\n-  double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+    \/\/  Rationale:\n+    \/\/    The idea is that there is an average allocation rate and there are occasional abnormal bursts (or spikes) of\n+    \/\/    allocations that exceed the average allocation rate.  What do these spikes look like?\n+    \/\/\n+    \/\/    1. At certain phase changes, we may discard large amounts of data and replace it with large numbers of newly\n+    \/\/       allocated objects.  This \"spike\" looks more like a phase change.  We were in steady state at M bytes\/sec\n+    \/\/       allocation rate and now we're in a \"reinitialization phase\" that looks like N bytes\/sec.  We need the \"spike\"\n+    \/\/       accomodation to give us enough runway to recalibrate our \"average allocation rate\".\n+    \/\/\n+    \/\/   2. The typical workload changes.  \"Suddenly\", our typical workload of N TPS increases to N+delta TPS.  This means\n+    \/\/       our average allocation rate needs to be adjusted.  Once again, we need the \"spike\" accomodation to give us\n+    \/\/       enough runway to recalibrate our \"average allocation rate\".\n+    \/\/\n+    \/\/    3. Though there is an \"average\" allocation rate, a given workload's demand for allocation may be very bursty.  We\n+    \/\/       allocate a bunch of LABs during the 5 ms that follow completion of a GC, then we perform no more allocations for\n+    \/\/       the next 150 ms.  It seems we want the \"spike\" to represent the maximum divergence from average within the\n+    \/\/       period of time between consecutive evaluation of the should_start_gc() service.  Here's the thinking:\n+    \/\/\n+    \/\/       a) Between now and the next time I ask whether should_start_gc(), we might experience a spike representing\n+    \/\/          the anticipated burst of allocations.  If that would put us over budget, then we should start GC immediately.\n+    \/\/       b) Between now and the anticipated depletion of allocation pool, there may be two or more bursts of allocations.\n+    \/\/          If there are more than one of these bursts, we can \"approximate\" that these will be separated by spans of\n+    \/\/          time with very little or no allocations so the \"average\" allocation rate should be a suitable approximation\n+    \/\/          of how this will behave.\n+    \/\/\n+    \/\/    For cases 1 and 2, we need to \"quickly\" recalibrate the average allocation rate whenever we detect a change\n+    \/\/    in operation mode.  We want some way to decide that the average rate has changed.  Make average allocation rate\n+    \/\/    computations an independent effort.\n@@ -422,3 +484,0 @@\n-  double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n-  log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n-    _generation->name(), avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n@@ -426,1 +485,0 @@\n-  if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n@@ -428,5 +486,0 @@\n-    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n-                 _generation->name(), avg_cycle_time * 1000,\n-                 byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate),\n-                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-                 _margin_of_error_sd);\n@@ -434,5 +487,0 @@\n-    log_info(gc, ergo)(\"Free headroom: \" SIZE_FORMAT \"%s (free) - \" SIZE_FORMAT \"%s (spike) - \" SIZE_FORMAT \"%s (penalties) = \" SIZE_FORMAT \"%s\",\n-                       byte_size_in_proper_unit(available),           proper_unit_for_byte_size(available),\n-                       byte_size_in_proper_unit(spike_headroom),      proper_unit_for_byte_size(spike_headroom),\n-                       byte_size_in_proper_unit(penalties),           proper_unit_for_byte_size(penalties),\n-                       byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n@@ -440,3 +488,3 @@\n-    _last_trigger = RATE;\n-    return resize_and_evaluate();\n-  }\n+    \/\/ Check if allocation headroom is still okay. This also factors in:\n+    \/\/   1. Some space to absorb allocation spikes (ShenandoahAllocSpikeFactor)\n+    \/\/   2. Accumulated penalties from Degenerated and Full GC\n@@ -444,11 +492,3 @@\n-  bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n-  if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n-    log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s) to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n-                 _generation->name(), avg_cycle_time * 1000,\n-                 byte_size_in_proper_unit(rate), proper_unit_for_byte_size(rate),\n-                 byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n-\n-                 _spike_threshold_sd);\n-    _last_trigger = SPIKE;\n-    return resize_and_evaluate();\n-  }\n+    size_t allocation_headroom = available;\n+    size_t spike_headroom = capacity \/ 100 * ShenandoahAllocSpikeFactor;\n+    size_t penalties      = capacity \/ 100 * _gc_time_penalties;\n@@ -456,2 +496,2 @@\n-  return ShenandoahHeuristics::should_start_gc();\n-}\n+    allocation_headroom -= MIN2(allocation_headroom, penalties);\n+    allocation_headroom -= MIN2(allocation_headroom, spike_headroom);\n@@ -459,6 +499,5 @@\n-bool ShenandoahAdaptiveHeuristics::resize_and_evaluate() {\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  if (!heap->mode()->is_generational()) {\n-    \/\/ We only attempt to resize the generations in generational mode.\n-    return true;\n-  }\n+    double avg_cycle_time = _gc_cycle_time_history->davg() + (_margin_of_error_sd * _gc_cycle_time_history->dsd());\n+    double avg_alloc_rate = _allocation_rate.upper_bound(_margin_of_error_sd);\n+    log_debug(gc)(\"%s: average GC time: %.2f ms, allocation rate: %.0f %s\/s\",\n+                  _generation->name(),\n+                  avg_cycle_time * 1000, byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate));\n@@ -466,5 +505,1 @@\n-  if (_cycles_since_last_resize <= MINIMUM_RESIZE_INTERVAL) {\n-    log_info(gc, ergo)(\"Not resizing %s for another \" UINT32_FORMAT \" cycles.\",\n-        _generation->name(),  _cycles_since_last_resize);\n-    return true;\n-  }\n+    if (avg_cycle_time > allocation_headroom \/ avg_alloc_rate) {\n@@ -472,5 +507,13 @@\n-  if (!heap->generation_sizer()->transfer_capacity(_generation)) {\n-    \/\/ We could not enlarge our generation, so we must start a gc cycle.\n-    log_info(gc, ergo)(\"Could not increase size of %s, begin gc cycle.\", _generation->name());\n-    return true;\n-  }\n+      log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for average allocation rate (%.0f %sB\/s)\"\n+                   \" to deplete free headroom (\" SIZE_FORMAT \"%s) (margin of error = %.2f)\",\n+                   _generation->name(), avg_cycle_time * 1000,\n+                   byte_size_in_proper_unit(avg_alloc_rate), proper_unit_for_byte_size(avg_alloc_rate),\n+                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                   _margin_of_error_sd);\n+\n+      log_info(gc, ergo)(\"Free headroom: \" SIZE_FORMAT \"%s (free) - \" SIZE_FORMAT \"%s (spike) - \"\n+                         SIZE_FORMAT \"%s (penalties) = \" SIZE_FORMAT \"%s\",\n+                         byte_size_in_proper_unit(available),           proper_unit_for_byte_size(available),\n+                         byte_size_in_proper_unit(spike_headroom),      proper_unit_for_byte_size(spike_headroom),\n+                         byte_size_in_proper_unit(penalties),           proper_unit_for_byte_size(penalties),\n+                         byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom));\n@@ -478,2 +521,50 @@\n-  log_info(gc)(\"Increased size of %s generation, re-evaluate trigger criteria\", _generation->name());\n-  return should_start_gc();\n+      _last_trigger = RATE;\n+      return true;\n+    }\n+\n+    bool is_spiking = _allocation_rate.is_spiking(rate, _spike_threshold_sd);\n+    if (is_spiking && avg_cycle_time > allocation_headroom \/ rate) {\n+      log_info(gc)(\"Trigger (%s): Average GC time (%.2f ms) is above the time for instantaneous allocation rate (%.0f %sB\/s)\"\n+                   \" to deplete free headroom (\" SIZE_FORMAT \"%s) (spike threshold = %.2f)\",\n+                   _generation->name(), avg_cycle_time * 1000,\n+                   byte_size_in_proper_unit(rate), proper_unit_for_byte_size(rate),\n+                   byte_size_in_proper_unit(allocation_headroom), proper_unit_for_byte_size(allocation_headroom),\n+                   _spike_threshold_sd);\n+      _last_trigger = SPIKE;\n+      return true;\n+    }\n+\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    if (heap->mode()->is_generational()) {\n+      \/\/ Get through promotions and mixed evacuations as quickly as possible.  These cycles sometimes require significantly\n+      \/\/ more time than traditional young-generation cycles so start them up as soon as possible.  This is a \"mitigation\"\n+      \/\/ for the reality that old-gen and young-gen activities are not truly \"concurrent\".  If there is old-gen work to\n+      \/\/ be done, we start up the young-gen GC threads so they can do some of this old-gen work.  As implemented, promotion\n+      \/\/ gets priority over old-gen marking.\n+\n+      size_t promo_potential = heap->get_promotion_potential();\n+      size_t promo_in_place_potential = heap->get_promotion_in_place_potential();\n+      ShenandoahOldHeuristics* old_heuristics = (ShenandoahOldHeuristics*) heap->old_generation()->heuristics();\n+      size_t mixed_candidates = old_heuristics->unprocessed_old_collection_candidates();\n+      if (promo_potential > 0) {\n+        \/\/ Detect unsigned arithmetic underflow\n+        assert(promo_potential < heap->capacity(), \"Sanity\");\n+        log_info(gc)(\"Trigger (%s): expedite promotion of \" SIZE_FORMAT \"%s\",\n+                     _generation->name(), byte_size_in_proper_unit(promo_potential), proper_unit_for_byte_size(promo_potential));\n+        return true;\n+      } else if (promo_in_place_potential > 0) {\n+        \/\/ Detect unsigned arithmetic underflow\n+        assert(promo_in_place_potential < heap->capacity(), \"Sanity\");\n+        log_info(gc)(\"Trigger (%s): expedite promotion in place of \" SIZE_FORMAT \"%s\", _generation->name(),\n+                     byte_size_in_proper_unit(promo_in_place_potential),\n+                     proper_unit_for_byte_size(promo_in_place_potential));\n+        return true;\n+      } else if (mixed_candidates > 0) {\n+        \/\/ We need to run young GC in order to open up some free heap regions so we can finish mixed evacuations.\n+        log_info(gc)(\"Trigger (%s): expedite mixed evacuation of \" SIZE_FORMAT \" regions\",\n+                     _generation->name(), mixed_candidates);\n+        return true;\n+      }\n+    }\n+  }\n+  return ShenandoahHeuristics::should_start_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":247,"deletions":156,"binary":false,"changes":403,"status":"modified"},{"patch":"@@ -42,1 +42,0 @@\n-\n@@ -75,0 +74,2 @@\n+  virtual size_t evac_slack(size_t young_regions_to_be_recycled);\n+\n@@ -88,7 +89,0 @@\n-  \/\/ At least this many cycles must execute before the heuristic will attempt\n-  \/\/ to resize its generation. This is to prevent the heuristic from rapidly\n-  \/\/ maxing out the generation size (which only forces the collector for the\n-  \/\/ other generation to run more frequently, defeating the purpose of improving\n-  \/\/ MMU).\n-  const static uint MINIMUM_RESIZE_INTERVAL;\n-\n@@ -109,2 +103,0 @@\n-  bool resize_and_evaluate();\n-\n@@ -138,4 +130,0 @@\n-\n-  \/\/ Do not attempt to resize the generation for this heuristic until this\n-  \/\/ value is greater than MINIMUM_RESIZE_INTERVAL.\n-  uint _cycles_since_last_resize;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":2,"deletions":14,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"utilities\/quickSort.hpp\"\n@@ -43,0 +44,1 @@\n+\/\/ sort by decreasing garbage (so most garbage comes first)\n@@ -44,1 +46,1 @@\n-  if (a._garbage > b._garbage)\n+  if (a._u._garbage > b._u._garbage)\n@@ -46,1 +48,10 @@\n-  else if (a._garbage < b._garbage)\n+  else if (a._u._garbage < b._u._garbage)\n+    return 1;\n+  else return 0;\n+}\n+\n+\/\/ sort by increasing live (so least live comes first)\n+int ShenandoahHeuristics::compare_by_live(RegionData a, RegionData b) {\n+  if (a._u._live_data < b._u._live_data)\n+    return -1;\n+  else if (a._u._live_data > b._u._live_data)\n@@ -79,0 +90,14 @@\n+typedef struct {\n+  ShenandoahHeapRegion* _region;\n+  size_t _live_data;\n+} AgedRegionData;\n+\n+static int compare_by_aged_live(AgedRegionData a, AgedRegionData b) {\n+  if (a._live_data < b._live_data)\n+    return -1;\n+  else if (a._live_data > b._live_data)\n+    return 1;\n+  else return 0;\n+}\n+\n+\/\/ Returns bytes of old-gen memory consumed by selected aged regions\n@@ -81,0 +106,1 @@\n+  ShenandoahMarkingContext* const ctx = heap->marking_context();\n@@ -82,0 +108,2 @@\n+  size_t promo_potential = 0;\n+  size_t anticipated_promote_in_place_live = 0;\n@@ -83,0 +111,15 @@\n+    heap->clear_promotion_in_place_potential();\n+    heap->clear_promotion_potential();\n+    size_t candidates = 0;\n+    size_t candidates_live = 0;\n+    size_t old_garbage_threshold = (ShenandoahHeapRegion::region_size_bytes() * ShenandoahOldGarbageThreshold) \/ 100;\n+    size_t promote_in_place_regions = 0;\n+    size_t promote_in_place_live = 0;\n+    size_t promote_in_place_pad = 0;\n+    size_t anticipated_candidates = 0;\n+    size_t anticipated_promote_in_place_regions = 0;\n+\n+    \/\/ Sort the promotion-eligible regions according to live-data-bytes so that we can first reclaim regions that require\n+    \/\/ less evacuation effort.  This prioritizes garbage first, expanding the allocation pool before we begin the work of\n+    \/\/ reclaiming regions that require more effort.\n+    AgedRegionData* sorted_regions = (AgedRegionData*) alloca(num_regions * sizeof(AgedRegionData));\n@@ -84,4 +127,80 @@\n-      ShenandoahHeapRegion* region = heap->get_region(i);\n-      if (in_generation(region) && !region->is_empty() && region->is_regular() && (region->age() >= InitialTenuringThreshold)) {\n-        size_t promotion_need = (size_t) (region->get_live_data_bytes() * ShenandoahEvacWaste);\n-        if (old_consumed + promotion_need < old_available) {\n+      ShenandoahHeapRegion* r = heap->get_region(i);\n+      if (r->is_empty() || !r->has_live() || !r->is_young() || !r->is_regular()) {\n+        continue;\n+      }\n+      if (r->age() >= InitialTenuringThreshold) {\n+        r->save_top_before_promote();\n+        if ((r->garbage() < old_garbage_threshold)) {\n+          HeapWord* tams = ctx->top_at_mark_start(r);\n+          HeapWord* original_top = r->top();\n+          if (tams == original_top) {\n+            \/\/ Fill the remnant memory within this region to assure no allocations prior to promote in place.  Otherwise,\n+            \/\/ newly allocated objects will not be parseable when promote in place tries to register them.  Furthermore, any\n+            \/\/ new allocations would not necessarily be eligible for promotion.  This addresses both issues.\n+            size_t remnant_size = r->free() \/ HeapWordSize;\n+            if (remnant_size > ShenandoahHeap::min_fill_size()) {\n+              ShenandoahHeap::fill_with_object(original_top, remnant_size);\n+              r->set_top(r->end());\n+              promote_in_place_pad += remnant_size * HeapWordSize;\n+            } else {\n+              \/\/ Since the remnant is so small that it cannot be filled, we don't have to worry about any accidental\n+              \/\/ allocations occuring within this region before the region is promoted in place.\n+            }\n+            promote_in_place_regions++;\n+            promote_in_place_live += r->get_live_data_bytes();\n+          }\n+          \/\/ Else, we do not promote this region (either in place or by copy) because it has received new allocations.\n+\n+          \/\/ During evacuation, we exclude from promotion regions for which age > tenure threshold, garbage < garbage-threshold,\n+          \/\/  and get_top_before_promote() != tams\n+        } else {\n+          \/\/ After sorting and selecting best candidates below, we may decide to exclude this promotion-eligible region\n+          \/\/ from the current collection sets.  If this happens, we will consider this region as part of the anticipated\n+          \/\/ promotion potential for the next GC pass.\n+          size_t live_data = r->get_live_data_bytes();\n+          candidates_live += live_data;\n+          sorted_regions[candidates]._region = r;\n+          sorted_regions[candidates++]._live_data = live_data;\n+        }\n+      } else {\n+        \/\/ We only anticipate to promote regular regions if garbage() is above threshold.  Tenure-aged regions with less\n+        \/\/ garbage are promoted in place.  These take a different path to old-gen.  Note that certain regions that are\n+        \/\/ excluded from anticipated promotion because their garbage content is too low (causing us to anticipate that\n+        \/\/ the region would be promoted in place) may be eligible for evacuation promotion by the time promotion takes\n+        \/\/ place during a subsequent GC pass because more garbage is found within the region between now and then.  This\n+        \/\/ should not happen if we are properly adapting the tenure age.  The theory behind adaptive tenuring threshold\n+        \/\/ is to choose the youngest age that demonstrates no \"significant\" futher loss of population since the previous\n+        \/\/ age.  If not this, we expect the tenure age to demonstrate linear population decay for at least two population\n+        \/\/ samples, whereas we expect to observe exponetial population decay for ages younger than the tenure age.\n+        \/\/\n+        \/\/ In the case that certain regions which were anticipated to be promoted in place need to be promoted by\n+        \/\/ evacuation, it may be the case that there is not sufficient reserve within old-gen to hold evacuation of\n+        \/\/ these regions.  The likely outcome is that these regions will not be selected for evacuation or promotion\n+        \/\/ in the current cycle and we will anticipate that they will be promoted in the next cycle.  This will cause\n+        \/\/ us to reserve more old-gen memory so that these objects can be promoted in the subsequent cycle.\n+        \/\/\n+        \/\/ TODO:\n+        \/\/   If we are auto-tuning the tenure age and regions that were anticipated to be promoted in place end up\n+        \/\/   being promoted by evacuation, this event should feed into the tenure-age-selection heuristic so that\n+        \/\/   the tenure age can be increased.\n+        if (r->age() + 1 == InitialTenuringThreshold) {\n+          if (r->garbage() >= old_garbage_threshold) {\n+            anticipated_candidates++;\n+            promo_potential += r->get_live_data_bytes();\n+          }\n+          else {\n+            anticipated_promote_in_place_regions++;\n+            anticipated_promote_in_place_live += r->get_live_data_bytes();\n+          }\n+        }\n+      }\n+    }\n+    \/\/ Sort in increasing order according to live data bytes.  Note that candidates represents the number of regions\n+    \/\/ that qualify to be promoted by evacuation.\n+    if (candidates > 0) {\n+      QuickSort::sort<AgedRegionData>(sorted_regions, candidates, compare_by_aged_live, false);\n+      for (size_t i = 0; i < candidates; i++) {\n+        size_t region_live_data = sorted_regions[i]._live_data;\n+        size_t promotion_need = (size_t) (region_live_data * ShenandoahPromoEvacWaste);\n+        if (old_consumed + promotion_need <= old_available) {\n+          ShenandoahHeapRegion* region = sorted_regions[i]._region;\n@@ -89,1 +208,5 @@\n-          preselected_regions[i] = true;\n+          preselected_regions[region->index()] = true;\n+        } else {\n+          \/\/ We rejected this promotable region from the collection set because we had no room to hold its copy.\n+          \/\/ Add this region to promo potential for next GC.\n+          promo_potential += region_live_data;\n@@ -91,2 +214,2 @@\n-        \/\/ Note that we keep going even if one region is excluded from selection.  Subsequent regions may be selected\n-        \/\/ if they have smaller live data.\n+        \/\/ We keep going even if one region is excluded from selection because we need to accumulate all eligible\n+        \/\/ regions that are not preselected into promo_potential\n@@ -95,0 +218,3 @@\n+    heap->set_pad_for_promote_in_place(promote_in_place_pad);\n+    heap->set_promotion_potential(promo_potential);\n+    heap->set_promotion_in_place_potential(anticipated_promote_in_place_live);\n@@ -102,0 +228,1 @@\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -116,0 +243,1 @@\n+  size_t preselected_candidates = 0;\n@@ -126,0 +254,10 @@\n+  size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n+  \/\/ This counts number of humongous regions that we intend to promote in this cycle.\n+  size_t humongous_regions_promoted = 0;\n+  \/\/ This counts bytes of memory used by hunongous regions to be promoted in place.\n+  size_t humongous_bytes_promoted = 0;\n+  \/\/ This counts number of regular regions that will be promoted in place.\n+  size_t regular_regions_promoted_in_place = 0;\n+  \/\/ This counts bytes of memory used by regular regions to be promoted in place.\n+  size_t regular_regions_promoted_usage = 0;\n+\n@@ -131,1 +269,0 @@\n-\n@@ -146,0 +283,1 @@\n+        bool is_candidate;\n@@ -147,1 +285,0 @@\n-        candidates[cand_idx]._region = region;\n@@ -149,2 +286,25 @@\n-          \/\/ If region is preselected, we know mode()->is_generational() and region->age() >= InitialTenuringThreshold)\n-          garbage = ShenandoahHeapRegion::region_size_bytes();\n+          \/\/ If !is_generational, we cannot ask if is_preselected.  If is_preselected, we know\n+          \/\/   region->age() >= InitialTenuringThreshold).\n+          is_candidate = true;\n+          preselected_candidates++;\n+          \/\/ Set garbage value to maximum value to force this into the sorted collection set.\n+          garbage = region_size_bytes;\n+        } else if (is_generational && region->is_young() && (region->age() >= InitialTenuringThreshold)) {\n+          \/\/ Note that for GLOBAL GC, region may be OLD, and OLD regions do not qualify for pre-selection\n+\n+          \/\/ This region is old enough to be promoted but it was not preselected, either because its garbage is below\n+          \/\/ ShenandoahOldGarbageThreshold so it will be promoted in place, or because there is not sufficient room\n+          \/\/ in old gen to hold the evacuated copies of this region's live data.  In both cases, we choose not to\n+          \/\/ place this region into the collection set.\n+          if (region->garbage_before_padded_for_promote() < old_garbage_threshold) {\n+            regular_regions_promoted_in_place++;\n+            regular_regions_promoted_usage += region->used_before_promote();\n+          }\n+          is_candidate = false;\n+        } else {\n+          is_candidate = true;\n+        }\n+        if (is_candidate) {\n+          candidates[cand_idx]._region = region;\n+          candidates[cand_idx]._u._garbage = garbage;\n+          cand_idx++;\n@@ -152,2 +312,0 @@\n-        candidates[cand_idx]._garbage = garbage;\n-        cand_idx++;\n@@ -156,1 +314,0 @@\n-\n@@ -173,0 +330,6 @@\n+        if (region->is_young() && region->age() >= InitialTenuringThreshold) {\n+          oop obj = cast_to_oop(region->bottom());\n+          size_t humongous_regions = ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+          humongous_regions_promoted += humongous_regions;\n+          humongous_bytes_promoted += obj->size() * HeapWordSize;\n+        }\n@@ -182,0 +345,9 @@\n+  heap->reserve_promotable_humongous_regions(humongous_regions_promoted);\n+  heap->reserve_promotable_humongous_usage(humongous_bytes_promoted);\n+  heap->reserve_promotable_regular_regions(regular_regions_promoted_in_place);\n+  heap->reserve_promotable_regular_usage(regular_regions_promoted_usage);\n+\n+  log_info(gc, ergo)(\"Planning to promote in place \" SIZE_FORMAT \" humongous regions and \" SIZE_FORMAT\n+                     \" regular regions, spanning a total of \" SIZE_FORMAT \" used bytes\",\n+                     humongous_regions_promoted, regular_regions_promoted_in_place,\n+                     humongous_regions_promoted * ShenandoahHeapRegion::region_size_bytes() + regular_regions_promoted_usage);\n@@ -194,1 +366,3 @@\n-  if (immediate_percent <= ShenandoahImmediateThreshold) {\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  bool doing_promote_in_place = (humongous_regions_promoted + regular_regions_promoted_in_place > 0);\n+  if (doing_promote_in_place || (preselected_candidates > 0) || (immediate_percent <= ShenandoahImmediateThreshold)) {\n@@ -198,1 +372,1 @@\n-    \/\/ else, this is global collection and doesn't need to prime_collection_set\n+    \/\/ else, this is non-generational or global collection and doesn't need to prime_collection_set\n@@ -234,1 +408,1 @@\n-  if (collection_set->garbage() > 0) {\n+  if (!collection_set->is_empty()) {\n@@ -239,0 +413,1 @@\n+\n@@ -367,0 +542,6 @@\n+size_t ShenandoahHeuristics::evac_slack(size_t young_regions_to_be_recycled) {\n+  assert(false, \"evac_slack() only implemented for young Adaptive Heuristics\");\n+  return 0;\n+}\n+\n+\n@@ -378,4 +559,1 @@\n-  size_t min_free_threshold =\n-      _generation->generation_mode() == GenerationMode::OLD\n-          ? ShenandoahOldMinFreeThreshold\n-          : ShenandoahMinFreeThreshold;\n+  size_t min_free_threshold = ShenandoahMinFreeThreshold;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":201,"deletions":23,"binary":false,"changes":224,"status":"modified"},{"patch":"@@ -73,1 +73,4 @@\n-    size_t _garbage;\n+    union {\n+      size_t _garbage;          \/\/ Not used by old-gen heuristics.\n+      size_t _live_data;        \/\/ Only used for old-gen heuristics, which prioritizes retention of _live_data over garbage reclaim\n+    } _u;\n@@ -109,0 +112,1 @@\n+  static int compare_by_live(RegionData a, RegionData b);\n@@ -173,0 +177,2 @@\n+  virtual size_t evac_slack(size_t region_to_be_recycled);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+  _old_generation(generation),\n@@ -45,1 +46,3 @@\n-  _old_generation(generation)\n+  _cannot_expand_trigger(false),\n+  _fragmentation_trigger(false),\n+  _growth_trigger(false)\n@@ -51,0 +54,1 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -57,1 +61,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -67,1 +70,1 @@\n-  size_t old_evacuation_budget = (size_t) ((double) heap->get_old_evac_reserve() \/ ShenandoahEvacWaste);\n+  size_t old_evacuation_budget = (size_t) ((double) heap->get_old_evac_reserve() \/ ShenandoahOldEvacWaste);\n@@ -69,1 +72,0 @@\n-  size_t lost_evacuation_capacity = 0;\n@@ -85,4 +87,3 @@\n-\n-    \/\/ It's probably overkill to compensate with lost_evacuation_capacity.  But it's the safe thing to do and\n-    \/\/  has minimal impact on content of primed collection set.\n-    if (r->get_live_data_bytes() + lost_evacuation_capacity <= remaining_old_evacuation_budget) {\n+    size_t lost_evacuation_capacity = r->free();\n+    size_t live_data_for_evacuation = r->get_live_data_bytes();\n+    if (live_data_for_evacuation + lost_evacuation_capacity <= remaining_old_evacuation_budget) {\n@@ -90,2 +91,1 @@\n-      lost_evacuation_capacity += r->free();\n-      remaining_old_evacuation_budget -= r->get_live_data_bytes();\n+      remaining_old_evacuation_budget -= (live_data_for_evacuation + lost_evacuation_capacity);\n@@ -94,1 +94,1 @@\n-      evacuated_old_bytes += r->get_live_data_bytes();\n+      evacuated_old_bytes += live_data_for_evacuation;\n@@ -116,0 +116,2 @@\n+    \/\/ Any triggers that occurred during mixed evacuations may no longer be valid.  They can retrigger if appropriate.\n+    clear_triggers();\n@@ -193,1 +195,1 @@\n-      available_slot._garbage = skipped._garbage;\n+      available_slot._u._live_data = skipped._u._live_data;\n@@ -223,0 +225,1 @@\n+  size_t live_data = 0;\n@@ -232,0 +235,1 @@\n+    size_t live_bytes = region->get_live_data_bytes();\n@@ -233,0 +237,1 @@\n+    live_data += live_bytes;\n@@ -243,1 +248,1 @@\n-        candidates[cand_idx]._garbage = garbage;\n+        candidates[cand_idx]._u._live_data = live_bytes;\n@@ -264,0 +269,2 @@\n+  ((ShenandoahOldGeneration*) (heap->old_generation()))->set_live_bytes_after_last_mark(live_data);\n+\n@@ -268,2 +275,5 @@\n-  \/\/ Prioritize regions to select garbage-first regions\n-  QuickSort::sort<RegionData>(candidates, cand_idx, compare_by_garbage, false);\n+  \/\/ Unlike young, we are more interested in efficiently packing OLD-gen than in reclaiming garbage first.  We sort by live-data.\n+  \/\/ Note that regular regions may be promoted in place with no garbage but also with very little live data.  When we \"compact\"\n+  \/\/ old-gen, we want to pack these underutilized regions together so we can have more unaffiliated (unfragmented) free regions\n+  \/\/ in old-gen.\n+  QuickSort::sort<RegionData>(candidates, cand_idx, compare_by_live, false);\n@@ -276,1 +286,1 @@\n-\n+  \/\/ The convention is to collect regions that have more than this amount of garbage.\n@@ -278,0 +288,4 @@\n+\n+  \/\/ Englightened interpretation: collect regions that have less than this amount of live.\n+  const size_t live_threshold = ShenandoahHeapRegion::region_size_bytes() - garbage_threshold;\n+\n@@ -283,0 +297,2 @@\n+  size_t unfragmented = 0;\n+\n@@ -284,2 +300,4 @@\n-    if (candidates[i]._garbage < garbage_threshold) {\n-      \/\/ Candidates are sorted in decreasing order of garbage, so no regions after this will be above the threshold\n+    size_t region_garbage = candidates[i]._region->garbage();\n+    size_t unused = ShenandoahHeapRegion::region_size_bytes() - candidates[i]._u._live_data;\n+    if (unused < garbage_threshold) {\n+      \/\/ Candidates are sorted in increasing order of live data, so no regions after this will be below the threshold.\n@@ -289,1 +307,2 @@\n-    candidates_garbage += candidates[i]._garbage;\n+    candidates_garbage += region_garbage;\n+    unfragmented += unused;\n@@ -295,1 +314,2 @@\n-  log_info(gc)(\"Old-Gen Collectable Garbage: \" SIZE_FORMAT \"%s over \" UINT32_FORMAT \" regions, \"\n+  log_info(gc)(\"Old-Gen Collectable Garbage: \" SIZE_FORMAT \"%s consolidated with free: \"\n+               SIZE_FORMAT \"%s, over \" UINT32_FORMAT \" regions, \"\n@@ -297,1 +317,2 @@\n-               byte_size_in_proper_unit(collectable_garbage), proper_unit_for_byte_size(collectable_garbage), _last_old_collection_candidate,\n+               byte_size_in_proper_unit(collectable_garbage), proper_unit_for_byte_size(collectable_garbage),\n+               byte_size_in_proper_unit(unfragmented), proper_unit_for_byte_size(unfragmented), _last_old_collection_candidate,\n@@ -346,1 +367,1 @@\n-  return _last_old_region - _next_old_collection_candidate;\n+  return (_last_old_region - _next_old_collection_candidate);\n@@ -356,6 +377,0 @@\n-  if (!_promotion_failed) {\n-    if (ShenandoahHeap::heap()->generation_sizer()->transfer_capacity(_old_generation)) {\n-      log_info(gc)(\"Increased size of old generation due to promotion failure.\");\n-    }\n-    \/\/ TODO: Increase tenuring threshold to push back on promotions.\n-  }\n@@ -366,1 +381,0 @@\n-  _promotion_failed = false;\n@@ -372,0 +386,1 @@\n+  clear_triggers();\n@@ -374,0 +389,8 @@\n+void ShenandoahOldHeuristics::clear_triggers() {\n+  \/\/ Clear any triggers that were set during mixed evacuations.  Conditions may be different now that this phase has finished.\n+  _promotion_failed = false;\n+  _cannot_expand_trigger = false;\n+  _fragmentation_trigger = false;\n+  _growth_trigger = false;\n+ }\n+\n@@ -383,4 +406,36 @@\n-  \/\/ If there's been a promotion failure (and we don't have regions already scheduled for evacuation),\n-  \/\/ start a new old generation collection.\n-  if (_promotion_failed) {\n-    log_info(gc)(\"Trigger: Promotion Failure\");\n+  if (_cannot_expand_trigger) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    size_t old_gen_capacity = old_gen->max_capacity();\n+    size_t heap_capacity = heap->capacity();\n+    double percent = 100.0 * ((double) old_gen_capacity) \/ heap_capacity;\n+    log_info(gc)(\"Trigger (OLD): Expansion failure, current size: \" SIZE_FORMAT \"%s which is %.1f%% of total heap size\",\n+                 byte_size_in_proper_unit(old_gen_capacity), proper_unit_for_byte_size(old_gen_capacity), percent);\n+    return true;\n+  }\n+\n+  if (_fragmentation_trigger) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    size_t used = old_gen->used();\n+    size_t used_regions_size = old_gen->used_regions_size();\n+    size_t used_regions = old_gen->used_regions();\n+    assert(used_regions_size > used_regions, \"Cannot have more used than used regions\");\n+    size_t fragmented_free = used_regions_size - used;\n+    double percent = 100.0 * ((double) fragmented_free) \/ used_regions_size;\n+    log_info(gc)(\"Trigger (OLD): Old has become fragmented: \"\n+                 SIZE_FORMAT \"%s available bytes spread between \" SIZE_FORMAT \" regions (%.1f%% free)\",\n+                 byte_size_in_proper_unit(fragmented_free), proper_unit_for_byte_size(fragmented_free), used_regions, percent);\n+    return true;\n+  }\n+\n+  if (_growth_trigger) {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahOldGeneration* old_gen = heap->old_generation();\n+    size_t current_usage = old_gen->used();\n+    size_t live_at_previous_old = old_gen->get_live_bytes_after_last_mark();\n+    double percent_growth = 100.0 * ((double) current_usage - live_at_previous_old) \/ live_at_previous_old;\n+    log_info(gc)(\"Trigger (OLD): Old has overgrown, live at end of previous OLD marking: \"\n+                 SIZE_FORMAT \"%s, current usage: \" SIZE_FORMAT \"%s, percent growth: %.1f%%\",\n+                 byte_size_in_proper_unit(live_at_previous_old), proper_unit_for_byte_size(live_at_previous_old),\n+                 byte_size_in_proper_unit(current_usage), proper_unit_for_byte_size(current_usage), percent_growth);\n@@ -399,0 +454,5 @@\n+  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  _promotion_failed = false;\n+  _cannot_expand_trigger = false;\n+  _fragmentation_trigger = false;\n+  _growth_trigger = false;\n@@ -403,0 +463,5 @@\n+  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  _promotion_failed = false;\n+  _cannot_expand_trigger = false;\n+  _fragmentation_trigger = false;\n+  _growth_trigger = false;\n@@ -407,0 +472,5 @@\n+  \/\/ Forget any triggers that occured while OLD GC was ongoing.  If we really need to start another, it will retrigger.\n+  _promotion_failed = false;\n+  _cannot_expand_trigger = false;\n+  _fragmentation_trigger = false;\n+  _growth_trigger = false;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.cpp","additions":103,"deletions":33,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -75,0 +75,3 @@\n+  \/\/ Keep a pointer to our generation that we can use without down casting a protected member from the base class.\n+  ShenandoahOldGeneration* _old_generation;\n+\n@@ -79,2 +82,5 @@\n-  \/\/ Keep a pointer to our generation that we can use without down casting a protected member from the base class.\n-  ShenandoahOldGeneration* _old_generation;\n+  \/\/ Flags are set when promotion failure is detected (by gc thread), and cleared when\n+  \/\/ old generation collection begins (by control thread).  Flags are set and cleared at safepoints.\n+  bool _cannot_expand_trigger;\n+  bool _fragmentation_trigger;\n+  bool _growth_trigger;\n@@ -130,0 +136,5 @@\n+  void trigger_cannot_expand() { _cannot_expand_trigger = true; };\n+  void trigger_old_is_fragmented() { _fragmentation_trigger = true; }\n+  void trigger_old_has_grown() { _growth_trigger = true; }\n+  void clear_triggers();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahOldHeuristics.hpp","additions":13,"deletions":2,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -171,0 +171,4 @@\n+\n+#define shenandoah_assert_control_thread() \\\n+                    assert(Thread::current() == ShenandoahHeap::heap()->control_thread(), \"Expected control thread\")\n+\n@@ -227,0 +231,1 @@\n+#define shenandoah_assert_control_thread()\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -93,1 +93,0 @@\n-\n@@ -97,0 +96,1 @@\n+    _young_available_bytes_collected += r->free();\n@@ -104,0 +104,1 @@\n+    _old_available_bytes_collected += r->free();\n@@ -117,0 +118,1 @@\n+\n@@ -140,0 +142,3 @@\n+  _young_available_bytes_collected = 0;\n+  _old_available_bytes_collected = 0;\n+\n@@ -190,0 +195,2 @@\n+#ifdef ASSERT\n+  \/\/ If Assertions are on, we'll dump each region selected for the collection set.  Otherwise, too much detail.\n@@ -198,0 +205,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.cpp","additions":9,"deletions":1,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -67,0 +67,5 @@\n+  \/\/ When a region having memory available to be allocated is added to the collection set, the region's available memory\n+  \/\/ should be subtracted from what's available.\n+  size_t                _young_available_bytes_collected;\n+  size_t                _old_available_bytes_collected;\n+\n@@ -112,0 +117,4 @@\n+  size_t get_young_available_bytes_collected() { return _young_available_bytes_collected; }\n+\n+  size_t get_old_available_bytes_collected() { return _old_available_bytes_collected; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCollectionSet.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -221,0 +221,2 @@\n+  \/\/ We defer generation resizing actions until after cset regions have been recycled.  We do this even following an\n+  \/\/ abbreviated cycle.\n@@ -222,1 +224,5 @@\n-    size_t old_available, young_available;\n+    bool success;\n+    size_t region_xfer;\n+    const char* region_destination;\n+    ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+    ShenandoahGeneration* old_gen = heap->old_generation();\n@@ -224,2 +230,0 @@\n-      ShenandoahYoungGeneration* young_gen = heap->young_generation();\n-      ShenandoahGeneration* old_gen = heap->old_generation();\n@@ -228,0 +232,21 @@\n+      size_t old_region_surplus = heap->get_old_region_surplus();\n+      size_t old_region_deficit = heap->get_old_region_deficit();\n+      if (old_region_surplus) {\n+        success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        region_destination = \"young\";\n+        region_xfer = old_region_surplus;\n+      } else if (old_region_deficit) {\n+        success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+        region_destination = \"old\";\n+        region_xfer = old_region_deficit;\n+        if (!success) {\n+          ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+        }\n+      } else {\n+        region_destination = \"none\";\n+        region_xfer = 0;\n+        success = true;\n+      }\n+      heap->set_old_region_surplus(0);\n+      heap->set_old_region_deficit(0);\n+\n@@ -233,9 +258,0 @@\n-      young_gen->unadjust_available();\n-      old_gen->unadjust_available();\n-      \/\/ No need to old_gen->increase_used().\n-      \/\/ That was done when plabs were allocated, accounting for both old evacs and promotions.\n-\n-      young_available = young_gen->adjusted_available();\n-      old_available = old_gen->adjusted_available();\n-\n-      heap->set_alloc_supplement_reserve(0);\n@@ -247,0 +263,9 @@\n+\n+    \/\/ Report outside the heap lock\n+    size_t young_available = young_gen->available();\n+    size_t old_available = old_gen->available();\n+    log_info(gc, ergo)(\"After cleanup, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                       SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                       success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                       byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                       byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n@@ -743,3 +768,0 @@\n-    \/\/\n-    \/\/ heap->get_alloc_supplement_reserve() represents the amount of old-gen memory that can be allocated during evacuation\n-    \/\/ and update-refs phases of gc.  The young evacuation reserve has already been removed from this quantity.\n@@ -750,42 +772,45 @@\n-    if (!heap->collection_set()->is_empty()) {\n-      LogTarget(Debug, gc, cset) lt;\n-      if (lt.is_enabled()) {\n-        ResourceMark rm;\n-        LogStream ls(lt);\n-        heap->collection_set()->print_on(&ls);\n-      }\n-\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_before_evacuation();\n-      }\n-\n-      heap->set_evacuation_in_progress(true);\n-      \/\/ From here on, we need to update references.\n-      heap->set_has_forwarded_objects(true);\n-\n-      \/\/ Verify before arming for concurrent processing.\n-      \/\/ Otherwise, verification can trigger stack processing.\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_during_evacuation();\n-      }\n-\n-      \/\/ Arm nmethods\/stack for concurrent processing\n-      ShenandoahCodeRoots::arm_nmethods();\n-      ShenandoahStackWatermark::change_epoch_id();\n-\n-      if (heap->mode()->is_generational()) {\n-        \/\/ Calculate the temporary evacuation allowance supplement to young-gen memory capacity (for allocations\n-        \/\/ and young-gen evacuations).\n-        size_t young_available = heap->young_generation()->adjust_available(heap->get_alloc_supplement_reserve());\n-        \/\/ old_available is memory that can hold promotions and evacuations.  Subtract out the memory that is being\n-        \/\/ loaned for young-gen allocations or evacuations.\n-        size_t old_available = heap->old_generation()->adjust_available(-heap->get_alloc_supplement_reserve());\n-\n-        log_info(gc, ergo)(\"After generational memory budget adjustments, old available: \" SIZE_FORMAT\n-                           \"%s, young_available: \" SIZE_FORMAT \"%s\",\n-                           byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n-                           byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n-      }\n-\n-      if (ShenandoahPacing) {\n-        heap->pacer()->setup_for_evac();\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGeneration* young_gen = heap->young_generation();\n+      size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+      size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n+      if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+        \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+        \/\/ Concurrent evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+        LogTarget(Debug, gc, cset) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n+\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+        \/\/ TODO: we do not need to run update-references following evacuation if collection_set->is_empty().\n+\n+        heap->set_evacuation_in_progress(true);\n+        \/\/ From here on, we need to update references.\n+        heap->set_has_forwarded_objects(true);\n+\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n+\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        ShenandoahCodeRoots::arm_nmethods();\n+        ShenandoahStackWatermark::change_epoch_id();\n+\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -794,6 +819,38 @@\n-      if (ShenandoahVerify) {\n-        heap->verifier()->verify_after_concmark();\n-      }\n-\n-      if (VerifyAfterGC) {\n-        Universe::verify();\n+      \/\/ Not is_generational()\n+      if (!heap->collection_set()->is_empty()) {\n+        LogTarget(Info, gc, ergo) lt;\n+        if (lt.is_enabled()) {\n+          ResourceMark rm;\n+          LogStream ls(lt);\n+          heap->collection_set()->print_on(&ls);\n+        }\n+\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_before_evacuation();\n+        }\n+\n+        heap->set_evacuation_in_progress(true);\n+        \/\/ From here on, we need to update references.\n+        heap->set_has_forwarded_objects(true);\n+\n+        \/\/ Verify before arming for concurrent processing.\n+        \/\/ Otherwise, verification can trigger stack processing.\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_during_evacuation();\n+        }\n+\n+        \/\/ Arm nmethods\/stack for concurrent processing\n+        ShenandoahCodeRoots::arm_nmethods();\n+        ShenandoahStackWatermark::change_epoch_id();\n+\n+        if (ShenandoahPacing) {\n+          heap->pacer()->setup_for_evac();\n+        }\n+      } else {\n+        if (ShenandoahVerify) {\n+          heap->verifier()->verify_after_concmark();\n+        }\n+\n+        if (VerifyAfterGC) {\n+          Universe::verify();\n+        }\n@@ -821,0 +878,1 @@\n+  ShenandoahThreadLocalData::enable_plab_promotions(thread);\n@@ -834,0 +892,3 @@\n+    Thread* worker_thread = Thread::current();\n+    ShenandoahThreadLocalData::enable_plab_promotions(worker_thread);\n+\n@@ -1209,1 +1270,0 @@\n-  heap->adjust_generation_sizes();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":124,"deletions":64,"binary":false,"changes":188,"status":"modified"},{"patch":"@@ -115,0 +115,1 @@\n+  bool old_bootstrap_requested = false;\n@@ -208,0 +209,6 @@\n+        } else if (_requested_generation == OLD && !old_bootstrap_requested) {\n+          \/\/ Arrange to perform a young GC immediately followed by a bootstrap OLD GC.  OLD GC typically requires more\n+          \/\/ than twice the time required for YOUNG GC, so we run a YOUNG GC to replenish the YOUNG allocation pool before\n+          \/\/ we start the longer OLD GC effort.\n+          old_bootstrap_requested = true;\n+          generation = YOUNG;\n@@ -209,0 +216,3 @@\n+          \/\/ if (old_bootstrap_requested && (_requested_generation == OLD)), this starts the bootstrap GC that\n+          \/\/  immediately follows the preparatory young GC.\n+          \/\/ But we will abandon the planned bootstrap GC if a GLOBAL GC has been now been requested.\n@@ -210,0 +220,1 @@\n+          old_bootstrap_requested = false;\n@@ -211,1 +222,0 @@\n-\n@@ -394,4 +404,12 @@\n-      \/\/ The timed wait is necessary because this thread has a responsibility to send\n-      \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n-      MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n-      lock.wait(ShenandoahControlIntervalMax);\n+      if (old_bootstrap_requested) {\n+        _requested_generation = OLD;\n+        _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+      } else {\n+        \/\/ The timed wait is necessary because this thread has a responsibility to send\n+        \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+        MonitorLocker lock(&_control_lock, Mutex::_no_safepoint_check_flag);\n+        lock.wait(ShenandoahControlIntervalMax);\n+      }\n+    } else {\n+      \/\/ in case of alloc_failure, abandon any plans to do immediate OLD Bootstrap\n+      old_bootstrap_requested = false;\n@@ -463,1 +481,1 @@\n-  const ShenandoahHeap* heap, const GenerationMode generation, GCCause::Cause cause) {\n+  ShenandoahHeap* heap, const GenerationMode generation, GCCause::Cause cause) {\n@@ -465,0 +483,1 @@\n+  ShenandoahGeneration* the_generation = nullptr;\n@@ -473,1 +492,2 @@\n-      service_concurrent_cycle(heap->young_generation(), cause, false);\n+      the_generation = heap->young_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n@@ -478,1 +498,2 @@\n-      service_concurrent_cycle(heap->global_generation(), cause, false);\n+      the_generation = heap->global_generation();\n+      service_concurrent_cycle(the_generation, cause, false);\n@@ -483,0 +504,1 @@\n+      the_generation = heap->old_generation();\n@@ -492,1 +514,2 @@\n-      msg = (generation == YOUNG)? \"At end of Interrupted Concurrent Young GC\": \"At end of Interrupted Concurrent Bootstrap GC\";\n+      msg = (generation == YOUNG)?\n+        \"At end of Interrupted Concurrent Young GC\": \"At end of Interrupted Concurrent Bootstrap Old GC\";\n@@ -494,1 +517,13 @@\n-      msg = (generation == YOUNG)? \"At end of Concurrent Young GC\": \"At end of Concurrent Bootstrap GC\";\n+      msg = (generation == YOUNG)? \"At end of Concurrent Young GC\": \"At end of Concurrent Bootstrap Old GC\";\n+      \/\/ We only record GC results if GC was successful\n+      ShenandoahMmuTracker* mmu_tracker = heap->mmu_tracker();\n+      if (generation == YOUNG) {\n+        if (heap->collection_set()->has_old_regions()) {\n+          bool mixed_is_done = (heap->old_heuristics()->unprocessed_old_collection_candidates() == 0);\n+          mmu_tracker->record_mixed(the_generation, GCId::current(), mixed_is_done);\n+        } else {\n+          mmu_tracker->record_young(the_generation, GCId::current());\n+        }\n+      } else {\n+        mmu_tracker->record_bootstrap(the_generation, GCId::current(), heap->collection_set()->has_old_regions());\n+      }\n@@ -502,1 +537,1 @@\n-void ShenandoahControlThread::service_concurrent_old_cycle(const ShenandoahHeap* heap, GCCause::Cause &cause) {\n+void ShenandoahControlThread::service_concurrent_old_cycle(ShenandoahHeap* heap, GCCause::Cause &cause) {\n@@ -565,0 +600,2 @@\n+          heap->mmu_tracker()->record_old_marking_increment(old_generation, GCId::current(), true,\n+                                                            heap->collection_set()->has_old_regions());\n@@ -568,0 +605,2 @@\n+        heap->mmu_tracker()->record_old_marking_increment(old_generation, GCId::current(), false,\n+                                                          heap->collection_set()->has_old_regions());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":50,"deletions":11,"binary":false,"changes":61,"status":"modified"},{"patch":"@@ -175,1 +175,1 @@\n-  void service_concurrent_normal_cycle(const ShenandoahHeap* heap,\n+  void service_concurrent_normal_cycle(ShenandoahHeap* heap,\n@@ -179,1 +179,1 @@\n-  void service_concurrent_old_cycle(const ShenandoahHeap* heap,\n+  void service_concurrent_old_cycle(ShenandoahHeap* heap,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -60,1 +60,5 @@\n-    heap->log_heap_status(\"At end of Degenerated GC\");\n+    bool is_bootstrap_gc = heap->is_concurrent_old_mark_in_progress() && _generation->is_young();\n+    heap->mmu_tracker()->record_degenerated(_generation, GCId::current(), is_bootstrap_gc,\n+                                            !heap->collection_set()->has_old_regions());\n+    const char* msg = is_bootstrap_gc? \"At end of Degenerated Boostrap Old GC\": \"At end of Degenerated GC\";\n+    heap->log_heap_status(msg);\n@@ -273,0 +277,15 @@\n+      \/\/ We defer generation resizing actions until after cset regions have been recycled.\n+      if (heap->mode()->is_generational()) {\n+        size_t old_region_surplus = heap->get_old_region_surplus();\n+        size_t old_region_deficit = heap->get_old_region_deficit();\n+        if (old_region_surplus) {\n+          bool success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+        } else if (old_region_deficit) {\n+          bool success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+          if (!success) {\n+            ((ShenandoahOldHeuristics *) heap->old_generation()->heuristics())->trigger_cannot_expand();\n+          }\n+        }\n+        heap->set_old_region_surplus(0);\n+        heap->set_old_region_deficit(0);\n+      }\n@@ -281,6 +300,0 @@\n-\n-    heap->young_generation()->unadjust_available();\n-    heap->old_generation()->unadjust_available();\n-    \/\/ No need to old_gen->increase_used().  That was done when plabs were allocated, accounting for both old evacs and promotions.\n-\n-    heap->set_alloc_supplement_reserve(0);\n@@ -291,2 +304,0 @@\n-\n-    heap->adjust_generation_sizes();\n@@ -354,1 +365,10 @@\n-  if (!heap->collection_set()->is_empty()) {\n+  size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+  size_t regular_regions_promoted_in_place = heap->get_regular_regions_promoted_in_place();\n+  if (!heap->collection_set()->is_empty() || (humongous_regions_promoted + regular_regions_promoted_in_place > 0)) {\n+    \/\/ Even if the collection set is empty, we need to do evacuation if there are regions to be promoted in place.\n+    \/\/ Degenerated evacuation takes responsibility for registering objects and setting the remembered set cards to dirty.\n+\n+    if (ShenandoahVerify) {\n+      heap->verifier()->verify_before_evacuation();\n+    }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":30,"deletions":10,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -40,0 +40,7 @@\n+\/\/ In the existing implementatation, plab allocations are taken from is-mutator-free regions and\/or existing old regions.\n+\/\/\n+\/\/ The plan is to introduce a new is_old_collector_free qualifier in order to distinguish the regions dedicated to old-promotions\n+\/\/ and evacuations from regions that are dedicated to mutator allocations and young-evacuations.  This macro symbol identifies\n+\/\/ some of the code segments that will be affected when we incorporate this change.\n+#define REMOVE_WHEN_FREESET_DOES_OLD_COLLECTED_FREE 1\n+\n@@ -53,0 +60,6 @@\n+#ifndef REMOVE_WHEN_FREESET_DOES_OLD_COLLECTED_FREE\n+  \/\/ The intention as originally implemented is that free-set capacity represents the budget for mutator allocations.\n+  \/\/ This assert is currently disabled because plab and old shared allocations are taken from mutator-free regions and\/or\n+  \/\/ from existing old-gen regions which are not mutator-free and may not be collector-free.  Work is in progress to\n+  \/\/ create a new mode for regions, which is \"old-collector-free\".  When that's implemented and we properly distinguish\n+  \/\/ regions that are dedicated to old-gen allocations, we'll reenable this assert.\n@@ -55,0 +68,42 @@\n+#endif\n+}\n+\n+void ShenandoahFreeSet::add_old_collector_free_region(ShenandoahHeapRegion* region) {\n+  shenandoah_assert_heaplocked();\n+  size_t idx = region->index();\n+  \/\/ TODO: the region that has been promoted in place may have been previously identified as is_collector_free or\n+  \/\/ is_mutator_free.  When we restructure the implementation of ShenandoahFreeSet to give special handling to\n+  \/\/ is_old_collector_free, we should also enforce that the region to be promoted, which is YOUNG and has no\n+  \/\/ available memory after its promote-in-place-pad has been inserted above original top, is identified as neither\n+  \/\/ is_mutator_free nor is_collecor_free nor is_old_collector_free.\n+\n+  \/\/ TODO: we really want to label this as old_collector_free but that is not yet implemented.\n+  if (!is_mutator_free(idx)) {\n+  } else {\n+    if (is_collector_free(idx)) {\n+      _collector_free_bitmap.clear_bit(idx);\n+      if (touches_bounds(idx)) {\n+        adjust_bounds();\n+      }\n+    }\n+    _mutator_free_bitmap.set_bit(idx);\n+    adjust_bounds_for_additional_old_collector_free_region(idx);\n+  }\n+  \/\/ Else, there's nothing to do.  Region is already identified is_mutator_free and the bounds are already set as such.\n+}\n+\n+void ShenandoahFreeSet::adjust_bounds_for_additional_old_collector_free_region(size_t idx) {\n+  \/\/ TODO: this should modify _old_collector_leftmost and _old_collector_rightmost, when they are implemented,\n+  ShenandoahHeapRegion* r = _heap->get_region(idx);\n+\n+  \/\/ TODO: add available to _old_collector_capacity rather than _capacity, because we'll mark this region as is_old_collector_free\n+  _capacity += r->free();\n+  \/\/ Only adjust _mutator_leftmost and _mutator_rightmost.\n+  \/\/ In the case that there were previously zero is_mutator_free regions, we will have to adjust both leftmost and rightmost\n+  \/\/ because leftmost will equal _max and rightmost will equal 0.\n+  if (idx < _mutator_leftmost) {\n+    _mutator_leftmost = idx;\n+  }\n+  if (idx > _mutator_rightmost) {\n+    _mutator_rightmost = idx;\n+  }\n@@ -57,0 +112,1 @@\n+\n@@ -73,0 +129,2 @@\n+\/\/   Eventually, we'll keep a separate range for old_collector_free_range and we'll give preference to allocating from\n+\/\/     a region that is_old_collector_free().  If that's not available, we may try to flip a region that is_mutator_free.\n@@ -77,2 +135,4 @@\n-  size_t rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n-  size_t leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+  size_t o_rightmost = MAX2(_collector_rightmost, _mutator_rightmost);\n+  size_t o_leftmost = MIN2(_collector_leftmost, _mutator_leftmost);\n+  size_t rightmost = _heap->num_regions() - 1;\n+  size_t leftmost = 0;\n@@ -80,0 +140,1 @@\n+  size_t min_size = req.is_lab_alloc()? req.min_size(): req.size();\n@@ -85,1 +146,1 @@\n-      if (!r->is_cset() && !has_no_alloc_capacity(r)) {\n+      if (!r->is_cset() && (alloc_capacity(r) >= min_size)) {\n@@ -133,2 +194,2 @@\n-        \/\/ Note: unsigned result from adjusted_unaffiliated_regions() will never be less than zero, but it may equal zero.\n-        if (_heap->old_generation()->adjusted_unaffiliated_regions() <= 0) {\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->old_generation()->free_unaffiliated_regions() <= 0) {\n@@ -140,2 +201,2 @@\n-        \/\/ Note: unsigned result from adjusted_unaffiliated_regions() will never be less than zero, but it may equal zero.\n-        if (_heap->young_generation()->adjusted_unaffiliated_regions() <= 0) {\n+        \/\/ Note: unsigned result from free_unaffiliated_regions() will never be less than zero, but it may equal zero.\n+        if (_heap->young_generation()->free_unaffiliated_regions() <= 0) {\n@@ -152,1 +213,0 @@\n-\n@@ -161,2 +221,3 @@\n-          HeapWord* result = try_allocate_in(r, req, in_new_region);\n-          if (result != nullptr) {\n+          HeapWord* result;\n+          size_t min_size = (req.type() == ShenandoahAllocRequest::_alloc_tlab)? req.min_size(): req.size();\n+          if ((alloc_capacity(r) >= min_size) && ((result = try_allocate_in(r, req, in_new_region)) != nullptr)) {\n@@ -211,1 +272,0 @@\n-\n@@ -241,1 +301,1 @@\n-      ShouldNotReachHere();\n+      assert(false, \"The request type %d is unrecognized\", req.type());\n@@ -256,1 +316,1 @@\n-    r->set_affiliation(req.affiliation());\n+    r->set_affiliation(req.affiliation(), false);\n@@ -288,0 +348,3 @@\n+#ifndef REMOVE_WHEN_FREESET_DOES_OLD_COLLECTED_FREE\n+      assert(is_old_collector_free(r->index()), \"PLABS must be allocated in old_collector_free regions\");\n+#endif\n@@ -322,0 +385,3 @@\n+#ifdef REMOVE_WHEN_FREESET_DOES_OLD_COLLECTED_FREE\n+        bool was_mutator_free = is_mutator_free(r->index());\n+#endif\n@@ -334,1 +400,6 @@\n-          increase_used(padding);\n+#ifdef REMOVE_WHEN_FREESET_DOES_OLD_COLLECTED_FREE\n+          \/\/ PLABS reside in old-gen.  Their regions should not be is_mutator_free.  So we should not increase_used().\n+          if (was_mutator_free) {\n+            increase_used(padding);\n+          }\n+#endif\n@@ -377,0 +448,1 @@\n+      bool was_mutator_free = is_mutator_free(r->index());\n@@ -386,1 +458,7 @@\n-        increase_used(padding);\n+\n+#ifdef REMOVE_WHEN_FREESET_DOES_OLD_COLLECTED_FREE\n+        \/\/ PLAB allocations are collector_is_free.  We only increase_Used for mutator allocations.\n+        if (was_mutator_free) {\n+          increase_used(padding);\n+        }\n+#endif\n@@ -406,1 +484,3 @@\n-      generation->increase_used(size * HeapWordSize);\n+      if (_heap->mode()->is_generational()) {\n+        generation->increase_used(size * HeapWordSize);\n+      }\n@@ -419,4 +499,6 @@\n-      generation->increase_used(size * HeapWordSize);\n-      if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n-        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n-        \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+      if (_heap->mode()->is_generational()) {\n+        generation->increase_used(size * HeapWordSize);\n+        if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"old-gen allocations use PLAB or shared allocation\");\n+          \/\/ for plabs, we'll sort the difference between evac and promotion usage when we retire the plab\n+        }\n@@ -444,0 +526,17 @@\n+    } else if (r->free() < PLAB::min_size() * HeapWordSize) {\n+      \/\/ Permanently retire this region if there's room for a fill object\n+      size_t waste = r->free();\n+      size_t fill_size = waste \/ HeapWordSize;\n+      if (fill_size >= ShenandoahHeap::min_fill_size()) {\n+        HeapWord* fill_addr = r->top();\n+        ShenandoahHeap::fill_with_object(fill_addr, fill_size);\n+        r->set_top(r->end());\n+        \/\/ Since we have filled the waste with an empty object, account for increased usage\n+        _heap->increase_used(waste);\n+        if (_heap->mode()->is_generational()) {\n+          _heap->generation_for(req.affiliation())->increase_used(waste);\n+          if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+            _heap->card_scan()->register_object(fill_addr);\n+          }\n+        }\n+      }\n@@ -501,1 +600,1 @@\n-    size_t avail_young_regions = generation->adjusted_unaffiliated_regions();\n+    size_t avail_young_regions = generation->free_unaffiliated_regions();\n@@ -564,1 +663,1 @@\n-    r->set_affiliation(req.affiliation());\n+    r->set_affiliation(req.affiliation(), false);\n@@ -582,4 +681,13 @@\n-  \/\/ While individual regions report their true use, all humongous regions are\n-  \/\/ marked used in the free set.\n-  increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n-  generation->increase_used(words_size * HeapWordSize);\n+  \/\/ While individual regions report their true use, all humongous regions are marked used in the free set.\n+  size_t total_humongous_size = ShenandoahHeapRegion::region_size_bytes() * num;\n+  increase_used(total_humongous_size);\n+  if (_heap->mode()->is_generational()) {\n+    size_t humongous_waste = total_humongous_size - words_size * HeapWordSize;\n+    if (req.affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+      _heap->young_generation()->increase_used(words_size * HeapWordSize);\n+      _heap->young_generation()->increase_humongous_waste(humongous_waste);\n+    } else if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+      _heap->old_generation()->increase_used(words_size * HeapWordSize);\n+      _heap->old_generation()->increase_humongous_waste(humongous_waste);\n+    }\n+  }\n@@ -608,1 +716,1 @@\n-size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) {\n+size_t ShenandoahFreeSet::alloc_capacity(ShenandoahHeapRegion *r) const {\n@@ -681,1 +789,3 @@\n-void ShenandoahFreeSet::rebuild() {\n+\/\/ Return the amount of young-gen memory that is about to be reycled\n+void ShenandoahFreeSet::prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions) {\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n@@ -684,0 +794,2 @@\n+  young_cset_regions = 0;\n+  old_cset_regions = 0;\n@@ -693,0 +805,12 @@\n+      if (region->is_old() && !region->is_trash()) {\n+        continue;\n+      }\n+\n+      if (region->is_trash()) {\n+        if (region->is_young()) {\n+          young_cset_regions++;\n+        } else {\n+          assert(region->is_old(), \"Better be old if not young\");\n+          old_cset_regions++;\n+        }\n+      }\n@@ -699,1 +823,0 @@\n-\n@@ -705,0 +828,6 @@\n+}\n+\n+\/\/ If young_reserve equals zero, compute young reserve from ShenandoahEvacReserve.  Otherwise, use the value supplied\n+\/\/ as input (which may be smaller than ShenandoanEvacReserve, as calculated from the know size of collection set.\n+void ShenandoahFreeSet::rebuild(size_t young_reserve) {\n+  shenandoah_assert_heaplocked();\n@@ -711,1 +840,3 @@\n-    size_t young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    if (young_reserve == 0) {\n+      young_reserve = (_heap->young_generation()->max_capacity() \/ 100) * ShenandoahEvacReserve;\n+    }\n@@ -717,0 +848,3 @@\n+\n+    \/\/ All old allocations are performed by the GC rather than the mutator, so these allocations need to be\n+    \/\/ satisfied by is_collector_free regions.\n@@ -728,4 +862,2 @@\n-\n-  for (size_t idx = _heap->num_regions() - 1; idx > 0; idx--) {\n-    if (reserved >= to_reserve) break;\n-\n+  for (size_t count = _heap->num_regions(); count > 0; count--) {\n+    size_t idx = count - 1;\n@@ -733,1 +865,2 @@\n-    if (_mutator_free_bitmap.at(idx) && can_allocate_from(region)) {\n+    if (reserved >= to_reserve) break;\n+    if (_mutator_free_bitmap.at(idx) && (region->free() > 0)) {\n@@ -871,1 +1004,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":168,"deletions":36,"binary":false,"changes":204,"status":"modified"},{"patch":"@@ -67,0 +67,2 @@\n+  void adjust_bounds_for_additional_old_collector_free_region(size_t idx);\n+\n@@ -71,0 +73,1 @@\n+  \/\/ Used of free set represents the amount of is_mutator_free set that has been consumed since most recent rebuild.\n@@ -77,1 +80,1 @@\n-  size_t alloc_capacity(ShenandoahHeapRegion *r);\n+  size_t alloc_capacity(ShenandoahHeapRegion *r) const;\n@@ -90,1 +93,4 @@\n-  void rebuild();\n+  void prepare_to_rebuild(size_t &young_cset_regions, size_t &old_cset_regions);\n+  void rebuild(size_t young_reserve);\n+\n+  void add_old_collector_free_region(ShenandoahHeapRegion* region);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -178,0 +178,1 @@\n+    heap->mmu_tracker()->record_full(heap->global_generation(), GCId::current());\n@@ -182,1 +183,1 @@\n-    assert(heap->old_generation()->used_regions_size() <= heap->old_generation()->adjusted_capacity(),\n+    assert(heap->old_generation()->used_regions_size() <= heap->old_generation()->soft_max_capacity(),\n@@ -184,1 +185,1 @@\n-    assert(heap->young_generation()->used_regions_size() <= heap->young_generation()->adjusted_capacity(),\n+    assert(heap->young_generation()->used_regions_size() <= heap->young_generation()->soft_max_capacity(),\n@@ -186,0 +187,6 @@\n+\n+    assert((heap->young_generation()->used() + heap->young_generation()->get_humongous_waste())\n+           <= heap->young_generation()->used_regions_size(), \"Young consumed can be no larger than span of affiliated regions\");\n+    assert((heap->old_generation()->used() + heap->old_generation()->get_humongous_waste())\n+           <= heap->old_generation()->used_regions_size(), \"Old consumed can be no larger than span of affiliated regions\");\n+\n@@ -202,6 +209,1 @@\n-    \/\/ Defer unadjust_available() invocations until after Full GC finishes its efforts because Full GC makes use\n-    \/\/ of young-gen memory that may have been loaned from old-gen.\n-\n-    \/\/ No need to old_gen->increase_used().  That was done when plabs were allocated, accounting for both old evacs and promotions.\n-\n-    heap->set_alloc_supplement_reserve(0);\n+    \/\/ No need for old_gen->increase_used() as this was done when plabs were allocated.\n@@ -345,2 +347,0 @@\n-  heap->adjust_generation_sizes();\n-\n@@ -364,4 +364,1 @@\n-  \/\/ Having reclaimed all dead memory, it is now safe to restore capacities to original values.\n-  heap->young_generation()->unadjust_available();\n-  heap->old_generation()->unadjust_available();\n-\n+  \/\/ Humongous regions are promoted on demand and are accounted for by normal Full GC mechanisms.\n@@ -550,1 +547,1 @@\n-          new_to_region->set_affiliation(OLD_GENERATION);\n+          new_to_region->set_affiliation(OLD_GENERATION, false);\n@@ -576,1 +573,1 @@\n-          new_to_region->set_affiliation(OLD_GENERATION);\n+          new_to_region->set_affiliation(OLD_GENERATION, false);\n@@ -621,1 +618,1 @@\n-          new_to_region->set_affiliation(YOUNG_GENERATION);\n+          new_to_region->set_affiliation(YOUNG_GENERATION, false);\n@@ -1337,1 +1334,0 @@\n-\n@@ -1494,0 +1490,15 @@\n+\n+      size_t old_usage = heap->old_generation()->used_regions_size();\n+      size_t old_capacity = heap->old_generation()->max_capacity();\n+\n+      assert(old_usage % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old usage must aligh with region size\");\n+      assert(old_capacity % ShenandoahHeapRegion::region_size_bytes() == 0, \"Old capacity must aligh with region size\");\n+\n+      if (old_capacity > old_usage) {\n+        size_t excess_old_regions = (old_capacity - old_usage) \/ ShenandoahHeapRegion::region_size_bytes();\n+        heap->generation_sizer()->transfer_to_young(excess_old_regions);\n+      } else if (old_capacity < old_usage) {\n+        size_t old_regions_deficit = (old_usage - old_capacity) \/ ShenandoahHeapRegion::region_size_bytes();\n+        heap->generation_sizer()->transfer_to_old(old_regions_deficit);\n+      }\n+\n@@ -1499,2 +1510,7 @@\n-    heap->free_set()->rebuild();\n-  }\n+    size_t young_cset_regions, old_cset_regions;\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+\n+    \/\/ We do not separately promote humongous after Full GC.  These have been handled by separate mechanism.\n+\n+    \/\/ We also do not expand old generation size following Full GC because we have scrambled age populations and\n+    \/\/ no longer have object separted by age into distinct regions.\n@@ -1502,0 +1518,15 @@\n+    \/\/ TODO: Do we need to fix FullGC so that it maintains aged segregation of objects into distinct regions?\n+    \/\/       A partial solution would be to remember how many objects are of tenure age following Full GC, but\n+    \/\/       this is probably suboptimal, because most of these objects will not reside in a region that will be\n+    \/\/       selected for the next evacuation phase.\n+\n+    \/\/ In case this Full GC resulted from degeneration, clear the tally on anticipated promotion.\n+    heap->clear_promotion_potential();\n+    heap->clear_promotion_in_place_potential();\n+\n+    if (heap->mode()->is_generational()) {\n+      \/\/ Invoke this in case we are able to transfer memory from OLD to YOUNG.\n+      heap->adjust_generation_sizes_for_next_cycle(0, 0, 0);\n+    }\n+    heap->free_set()->rebuild(0);\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":51,"deletions":20,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -165,1 +165,1 @@\n-  size_t v_adjusted_avail = adjusted_available();\n+  size_t v_humongous_waste = get_humongous_waste();\n@@ -167,3 +167,2 @@\n-                   \"soft capacity: \" SIZE_FORMAT \"%s, max capacity: \" SIZE_FORMAT \"%s, available: \" SIZE_FORMAT \"%s, \"\n-                   \"adjusted available: \" SIZE_FORMAT \"%s\",\n-                   msg, name(),\n+                   \"humongous waste: \" SIZE_FORMAT \"%s, soft capacity: \" SIZE_FORMAT \"%s, max capacity: \" SIZE_FORMAT \"%s, \"\n+                   \"available: \" SIZE_FORMAT \"%s\", msg, name(),\n@@ -172,0 +171,1 @@\n+                   byte_size_in_proper_unit(v_humongous_waste), proper_unit_for_byte_size(v_humongous_waste),\n@@ -174,2 +174,3 @@\n-                   byte_size_in_proper_unit(v_available), proper_unit_for_byte_size(v_available),\n-                   byte_size_in_proper_unit(v_adjusted_avail), proper_unit_for_byte_size(v_adjusted_avail));\n+                   byte_size_in_proper_unit(v_available), proper_unit_for_byte_size(v_available));\n+  \/\/ This detects arithmetic underflow of unsigned usage value\n+  assert(v_used <= ShenandoahHeap::heap()->capacity(), \"Generation capacity must be less than heap capacity\");\n@@ -253,22 +254,0 @@\n-  if (heap->doing_mixed_evacuations()) {\n-    \/\/ Compute old_evacuation_reserve: how much memory are we reserving to hold the results of\n-    \/\/ evacuating old-gen heap regions?  In order to sustain a consistent pace of young-gen collections,\n-    \/\/ the goal is to maintain a consistent value for this parameter (when the candidate set is not\n-    \/\/ empty).  This value is the minimum of:\n-    \/\/   1. old_gen->available()\n-    \/\/   2. old-gen->capacity() * ShenandoahOldEvacReserve) \/ 100\n-    \/\/       (e.g. old evacuation should be no larger than 5% of old_gen capacity)\n-    \/\/   3. ((young_gen->capacity * ShenandoahEvacReserve \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100\n-    \/\/       (e.g. old evacuation should be no larger than 12% of young-gen evacuation)\n-    old_evacuation_reserve = old_generation->available();\n-    assert(old_evacuation_reserve > minimum_evacuation_reserve, \"Old-gen available has not been preserved!\");\n-    size_t old_evac_reserve_max = old_generation->soft_max_capacity() * ShenandoahOldEvacReserve \/ 100;\n-    if (old_evac_reserve_max < old_evacuation_reserve) {\n-      old_evacuation_reserve = old_evac_reserve_max;\n-    }\n-    young_evac_reserve_max =\n-      (((young_generation->soft_max_capacity() * ShenandoahEvacReserve) \/ 100) * ShenandoahOldEvacRatioPercent) \/ 100;\n-    if (young_evac_reserve_max < old_evacuation_reserve) {\n-      old_evacuation_reserve = young_evac_reserve_max;\n-    }\n-  }\n@@ -276,5 +255,1 @@\n-  if (minimum_evacuation_reserve > old_generation->available()) {\n-    \/\/ Due to round-off errors during enforcement of minimum_evacuation_reserve during previous GC passes,\n-    \/\/ there can be slight discrepancies here.\n-    minimum_evacuation_reserve = old_generation->available();\n-  }\n+  \/\/ First priority is to reclaim the easy garbage out of young-gen.\n@@ -282,89 +257,6 @@\n-  heap->set_old_evac_reserve(old_evacuation_reserve);\n-  heap->reset_old_evac_expended();\n-\n-  \/\/ Compute the young evacuation reserve: This is how much memory is available for evacuating young-gen objects.\n-  \/\/ We ignore the possible effect of promotions, which reduce demand for young-gen evacuation memory.\n-  \/\/\n-  \/\/ TODO: We could give special treatment to the regions that have reached promotion age, because we know their\n-  \/\/ live data is entirely eligible for promotion.  This knowledge can feed both into calculations of young-gen\n-  \/\/ evacuation reserve and promotion reserve.\n-  \/\/\n-  \/\/  young_evacuation_reserve for young generation: how much memory are we reserving to hold the results\n-  \/\/  of evacuating young collection set regions?  This is typically smaller than the total amount\n-  \/\/  of available memory, and is also smaller than the total amount of marked live memory within\n-  \/\/  young-gen.  This value is the smaller of\n-  \/\/\n-  \/\/    1. (young_gen->capacity() * ShenandoahEvacReserve) \/ 100\n-  \/\/    2. (young_gen->available() + old_gen_memory_available_to_be_loaned\n-  \/\/\n-  \/\/  ShenandoahEvacReserve represents the configured target size of the evacuation region.  We can only honor\n-  \/\/  this target if there is memory available to hold the evacuations.  Memory is available if it is already\n-  \/\/  free within young gen, or if it can be borrowed from old gen.  Since we have not yet chosen the collection\n-  \/\/  sets, we do not yet know the exact accounting of how many regions will be freed by this collection pass.\n-  \/\/  What we do know is that there will be at least one evacuated young-gen region for each old-gen region that\n-  \/\/  is loaned to the evacuation effort (because regions to be collected consume more memory than the compacted\n-  \/\/  regions that will replace them).  In summary, if there are old-gen regions that are available to hold the\n-  \/\/  results of young-gen evacuations, it is safe to loan them for this purpose.  At this point, we have not yet\n-  \/\/  established a promoted_reserve.  We'll do that after we choose the collection set and analyze its impact\n-  \/\/  on available memory.\n-  \/\/\n-  \/\/ We do not know the evacuation_supplement until after we have computed the collection set.  It is not always\n-  \/\/ the case that young-regions inserted into the collection set will result in net decrease of in-use regions\n-  \/\/ because ShenandoahEvacWaste times multiplied by memory within the region may be larger than the region size.\n-  \/\/ The problem is especially relevant to regions that have been inserted into the collection set because they have\n-  \/\/ reached tenure age.  These regions tend to have much higher utilization (e.g. 95%).  These regions also offer\n-  \/\/ a unique opportunity because we know that every live object contained within the region is elgible to be\n-  \/\/ promoted.  Thus, the following implementation treats these regions specially:\n-  \/\/\n-  \/\/  1. Before beginning collection set selection, we tally the total amount of live memory held within regions\n-  \/\/     that are known to have reached tenure age.  If this memory times ShenandoahEvacWaste is available within\n-  \/\/     old-gen memory, establish an advance promotion reserve to hold all or some percentage of these objects.\n-  \/\/     This advance promotion reserve is excluded from memory available for holding old-gen evacuations and cannot\n-  \/\/     be \"loaned\" to young gen.\n-  \/\/\n-  \/\/  2. Tenure-aged regions are included in the collection set iff their evacuation size * ShenandoahEvacWaste fits\n-  \/\/     within the advance promotion reserve.  It is counter productive to evacuate these regions if they cannot be\n-  \/\/     evacuated directly into old-gen memory.  So if there is not sufficient memory to hold copies of their\n-  \/\/     live data right now, we'll just let these regions remain in young for now, to be evacuated by a subsequent\n-  \/\/     evacuation pass.\n-  \/\/\n-  \/\/  3. Next, we calculate a young-gen evacuation budget, which is the smaller of the two quantities mentioned\n-  \/\/     above.  old_gen_memory_available_to_be_loaned is calculated as:\n-  \/\/       old_gen->available - (advance-promotion-reserve + old-gen_evacuation_reserve)\n-  \/\/\n-  \/\/  4. When choosing the collection set, special care is taken to assure that the amount of loaned memory required to\n-  \/\/     hold the results of evacuation is smaller than the total memory occupied by the regions added to the collection\n-  \/\/     set.  We need to take these precautions because we do not know how much memory will be reclaimed by evacuation\n-  \/\/     until after the collection set has been constructed.  The algorithm is as follows:\n-  \/\/\n-  \/\/     a. We feed into the algorithm (i) young available at the start of evacuation and (ii) the amount of memory\n-  \/\/        loaned from old-gen that is available to hold the results of evacuation.\n-  \/\/     b. As candidate regions are added into the young-gen collection set, we maintain accumulations of the amount\n-  \/\/        of memory spanned by the collection set regions and the amount of memory that must be reserved to hold\n-  \/\/        evacuation results (by multiplying live-data size by ShenandoahEvacWaste).  We process candidate regions\n-  \/\/        in order of decreasing amounts of garbage.  We skip over (and do not include into the collection set) any\n-  \/\/        regions that do not satisfy all of the following conditions:\n-  \/\/\n-  \/\/          i. The amount of live data within the region as scaled by ShenandoahEvacWaste must fit within the\n-  \/\/             relevant evacuation reserve (live data of old-gen regions must fit within the old-evac-reserve, live\n-  \/\/             data of young-gen tenure-aged regions must fit within the advance promotion reserve, live data within\n-  \/\/             other young-gen regions must fit within the youn-gen evacuation reserve).\n-  \/\/         ii. The accumulation of memory consumed by evacuation must not exceed the accumulation of memory reclaimed\n-  \/\/             through evacuation by more than young-gen available.\n-  \/\/        iii. Other conditions may be enforced as appropriate for specific heuristics.\n-  \/\/\n-  \/\/       Note that regions are considered for inclusion in the selection set in order of decreasing amounts of garbage.\n-  \/\/       It is possible that a region with a larger amount of garbage will be rejected because it also has a larger\n-  \/\/       amount of live data and some region that follows this region in candidate order is included in the collection\n-  \/\/       set (because it has less live data and thus can fit within the evacuation limits even though it has less\n-  \/\/       garbage).\n-\n-  size_t young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n-  \/\/ old evacuation can pack into existing partially used regions.  young evacuation and loans for young allocations\n-  \/\/ need to target regions that do not already hold any old-gen objects.  Round down.\n-  regions_available_to_loan = old_generation->free_unaffiliated_regions();\n-\n-  size_t required_evacuation_reserve;\n-  \/\/ Memory evacuated from old-gen on this pass will be available to hold old-gen evacuations in next pass.\n-  if (old_evacuation_reserve > minimum_evacuation_reserve) {\n-    required_evacuation_reserve = 0;\n+  \/\/ maximum_old_evacuations limits the sum of old collections and promotions to old\n+  size_t maximum_young_evacuation_reserve = (young_generation->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  size_t young_evacuation_reserve = maximum_young_evacuation_reserve;\n+  size_t excess_young;\n+  if (young_generation->available() > young_evacuation_reserve) {\n+    excess_young = young_generation->available() - young_evacuation_reserve;\n@@ -372,1 +264,2 @@\n-    required_evacuation_reserve = minimum_evacuation_reserve - old_evacuation_reserve;\n+    young_evacuation_reserve = young_generation->available();\n+    excess_young = 0;\n@@ -374,8 +267,45 @@\n-\n-  consumed_by_advance_promotion = _heuristics->select_aged_regions(\n-    old_generation->available() - old_evacuation_reserve - required_evacuation_reserve, num_regions, preselected_regions);\n-  size_t net_available_old_regions =\n-    (old_generation->available() - old_evacuation_reserve - consumed_by_advance_promotion) \/ region_size_bytes;\n-\n- if (regions_available_to_loan > net_available_old_regions) {\n-    regions_available_to_loan = net_available_old_regions;\n+  size_t unaffiliated_young = young_generation->free_unaffiliated_regions() * region_size_bytes;\n+  if (excess_young > unaffiliated_young) {\n+    excess_young = unaffiliated_young;\n+  } else {\n+    \/\/ round down to multiple of region size\n+    excess_young \/= region_size_bytes;\n+    excess_young *= region_size_bytes;\n+  }\n+  \/\/ excess_young is available to be transferred to OLD.  Assume that OLD will not request any more than had\n+  \/\/ already been set aside for its promotion and evacuation needs at the end of previous GC.  No need to\n+  \/\/ hold back memory for allocation runway.\n+\n+  ShenandoahOldHeuristics* old_heuristics = heap->old_heuristics();\n+\n+  \/\/ maximum_old_evacuation_reserve is an upper bound on memory evacuated from old and evacuated to old (promoted).\n+  size_t maximum_old_evacuation_reserve =\n+    maximum_young_evacuation_reserve * ShenandoahOldEvacRatioPercent \/ (100 - ShenandoahOldEvacRatioPercent);\n+  \/\/ Here's the algebra:\n+  \/\/  TotalEvacuation = OldEvacuation + YoungEvacuation\n+  \/\/  OldEvacuation = TotalEvacuation * (ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * (ShenandoahOldEvacRatioPercent\/100)\/(1 - ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * ShenandoahOldEvacRatioPercent\/(100 - ShenandoahOldEvacRatioPercent)\n+\n+  if (maximum_old_evacuation_reserve > old_generation->available()) {\n+    maximum_old_evacuation_reserve = old_generation->available();\n+  }\n+\n+  \/\/ Second priority is to reclaim garbage out of old-gen if there are old-gen collection candidates.  Third priority\n+  \/\/ is to promote as much as we have room to promote.  However, if old-gen memory is in short supply, this means young\n+  \/\/ GC is operating under \"duress\" and was unable to transfer the memory that we would normally expect.  In this case,\n+  \/\/ old-gen will refrain from compacting itself in order to allow a quicker young-gen cycle (by avoiding the update-refs\n+  \/\/ through ALL of old-gen).  If there is some memory available in old-gen, we will use this for promotions as promotions\n+  \/\/ do not add to the update-refs burden of GC.\n+\n+  size_t old_promo_reserve;\n+  if (old_heuristics->unprocessed_old_collection_candidates() > 0) {\n+    \/\/ We reserved all old-gen memory at end of previous GC to hold anticipated evacuations to old-gen.  If this is\n+    \/\/ mixed evacuation, reserve all of this memory for compaction of old-gen and do not promote.  Prioritize compaction\n+    \/\/ over promotion in order to defragment OLD so that it will be better prepared to efficiently receive promoted memory.\n+    old_evacuation_reserve = maximum_old_evacuation_reserve;\n+    old_promo_reserve = 0;\n+  } else {\n+    \/\/ Make all old-evacuation memory for promotion, but if we can't use it all for promotion, we'll allow some evacuation.\n+    old_evacuation_reserve = 0;\n+    old_promo_reserve = maximum_old_evacuation_reserve;\n@@ -384,2 +314,8 @@\n-  \/\/ Otherwise, regions_available_to_loan is less than net_available_old_regions because available memory is\n-  \/\/ scattered between multiple partially used regions.\n+  \/\/ We see too many old-evacuation failures if we force ourselves to evacuate into regions that are not initially empty.\n+  \/\/ So we limit the old-evacuation reserve to unfragmented memory.  Even so, old-evacuation is free to fill in nooks and\n+  \/\/ crannies within existing partially used regions and it generally tries to do so.\n+  size_t old_free_regions = old_generation->free_unaffiliated_regions();\n+  size_t old_free_unfragmented = old_free_regions * region_size_bytes;\n+  if (old_evacuation_reserve > old_free_unfragmented) {\n+    size_t delta = old_evacuation_reserve - old_free_unfragmented;\n+    old_evacuation_reserve -= delta;\n@@ -387,12 +323,2 @@\n-  if (young_evacuation_reserve > young_generation->available()) {\n-    size_t short_fall = young_evacuation_reserve - young_generation->available();\n-    if (regions_available_to_loan * region_size_bytes >= short_fall) {\n-      old_regions_loaned_for_young_evac = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n-      regions_available_to_loan -= old_regions_loaned_for_young_evac;\n-    } else {\n-      old_regions_loaned_for_young_evac = regions_available_to_loan;\n-      regions_available_to_loan = 0;\n-      young_evacuation_reserve = young_generation->available() + old_regions_loaned_for_young_evac * region_size_bytes;\n-      \/\/ In this case, there's no memory available for new allocations while evacuating and updating, unless we\n-      \/\/ find more old-gen memory to borrow below.\n-    }\n+    \/\/ Let promo consume fragments of old-gen memory.\n+    old_promo_reserve += delta;\n@@ -400,5 +326,0 @@\n-  \/\/ In generational mode, we may end up choosing a young collection set that contains so many promotable objects\n-  \/\/ that there is not sufficient space in old generation to hold the promoted objects.  That is ok because we have\n-  \/\/ assured there is sufficient space in young generation to hold the rejected promotion candidates.  These rejected\n-  \/\/ promotion candidates will presumably be promoted in a future evacuation cycle.\n-  heap->set_young_evac_reserve(young_evacuation_reserve);\n@@ -406,0 +327,11 @@\n+  consumed_by_advance_promotion = _heuristics->select_aged_regions(old_promo_reserve, num_regions, preselected_regions);\n+  assert(consumed_by_advance_promotion <= maximum_old_evacuation_reserve, \"Cannot promote more than available old-gen memory\");\n+  if (consumed_by_advance_promotion < old_promo_reserve) {\n+    \/\/ If we're in a global collection, this memory can be used for old evacuations\n+    old_evacuation_reserve += old_promo_reserve - consumed_by_advance_promotion;\n+  }\n+  heap->set_young_evac_reserve(young_evacuation_reserve);\n+  heap->set_old_evac_reserve(old_evacuation_reserve);\n+  heap->set_promoted_reserve(consumed_by_advance_promotion);\n+\n+  \/\/ There is no need to expand OLD because all memory used here was set aside at end of previous GC\n@@ -408,1 +340,1 @@\n-\/\/ Having chosen the collection set, adjust the budgets for generatioal mode based on its composition.  Note\n+\/\/ Having chosen the collection set, adjust the budgets for generational mode based on its composition.  Note\n@@ -429,1 +361,1 @@\n-  size_t old_regions_loaned_for_young_evac, regions_available_to_loan;\n+\n@@ -433,19 +365,1 @@\n-  size_t old_evacuated = collection_set->get_old_bytes_reserved_for_evacuation();\n-  size_t old_evacuated_committed = (size_t) (ShenandoahEvacWaste * old_evacuated);\n-  size_t old_evacuation_reserve = heap->get_old_evac_reserve();\n-  \/\/ Immediate garbage found during choose_collection_set() is all young\n-  size_t immediate_garbage = collection_set->get_immediate_trash();\n-  size_t old_available = old_generation->available();\n-  size_t young_available = young_generation->available() + immediate_garbage;\n-  size_t loaned_regions = 0;\n-  size_t available_loan_remnant = 0; \/\/ loaned memory that is not yet dedicated to any particular budget\n-\n-  assert(((consumed_by_advance_promotion * 33) \/ 32) >= collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste,\n-         \"Advance promotion (\" SIZE_FORMAT \") should be at least young_bytes_to_be_promoted (\" SIZE_FORMAT\n-         \")* ShenandoahEvacWaste, totalling: \" SIZE_FORMAT \", within round-off errors of up to 3.125%%\",\n-         consumed_by_advance_promotion, collection_set->get_young_bytes_to_be_promoted(),\n-         (size_t) (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste));\n-\n-  assert(consumed_by_advance_promotion <= (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste * 33) \/ 32,\n-         \"Round-off errors should be less than 3.125%%, consumed by advance: \" SIZE_FORMAT \", promoted: \" SIZE_FORMAT,\n-         consumed_by_advance_promotion, (size_t) (collection_set->get_young_bytes_to_be_promoted() * ShenandoahEvacWaste));\n+  \/\/ With auto-sizing of generations, adjust_evacuation_budgets is much simpler\n@@ -453,0 +367,1 @@\n+  \/\/ Preselected regions have been inserted into the collection set, so we no longer need the preselected array.\n@@ -455,0 +370,4 @@\n+  size_t old_evacuated = collection_set->get_old_bytes_reserved_for_evacuation();\n+  size_t old_evacuated_committed = (size_t) (ShenandoahOldEvacWaste * old_evacuated);\n+  size_t old_evacuation_reserve = heap->get_old_evac_reserve();\n+\n@@ -456,1 +375,1 @@\n-    \/\/ This should only happen due to round-off errors when enforcing ShenandoahEvacWaste\n+    \/\/ This should only happen due to round-off errors when enforcing ShenandoahOldEvacWaste\n@@ -461,0 +380,1 @@\n+    \/\/ Leave old_evac_reserve as previously configured\n@@ -462,1 +382,1 @@\n-    \/\/ This may happen if the old-gen collection consumes less than full budget.\n+    \/\/ This happens if the old-gen collection consumes less than full budget.\n@@ -467,4 +387,2 @@\n-  \/\/ Recompute old_regions_loaned_for_young_evac because young-gen collection set may not need all the memory\n-  \/\/ originally reserved.\n-  size_t young_promoted = collection_set->get_young_bytes_to_be_promoted();\n-  size_t young_promoted_reserve_used = (size_t) (ShenandoahEvacWaste * young_promoted);\n+  size_t young_advance_promoted = collection_set->get_young_bytes_to_be_promoted();\n+  size_t young_advance_promoted_reserve_used = (size_t) (ShenandoahPromoEvacWaste * young_advance_promoted);\n@@ -475,151 +393,2 @@\n-  \/\/ We'll invoke heap->set_young_evac_reserve() further below, after we make additional adjustments to its value\n-\n-  \/\/ Adjust old_regions_loaned_for_young_evac to feed into calculations of promoted_reserve\n-  if (young_evacuated_reserve_used > young_available) {\n-    size_t short_fall = young_evacuated_reserve_used - young_available;\n-\n-    \/\/ region_size_bytes is a power of 2.  loan an integral number of regions.\n-    size_t revised_loan_for_young_evacuation = (short_fall + region_size_bytes - 1) \/ region_size_bytes;\n-\n-    \/\/ available_loan_remnant represents memory loaned from old-gen but not required for young evacuation.\n-    \/\/ This is the excess loaned memory that results from rounding the required loan up to an integral number\n-    \/\/ of heap regions.  This will be dedicated to alloc_supplement below.\n-    available_loan_remnant = (revised_loan_for_young_evacuation * region_size_bytes) - short_fall;\n-\n-    \/\/ We previously loaned more than was required by young-gen evacuation.  So claw some of this memory back.\n-    old_regions_loaned_for_young_evac = revised_loan_for_young_evacuation;\n-    loaned_regions = old_regions_loaned_for_young_evac;\n-  } else {\n-    \/\/ Undo the prevous loan, if any.\n-    old_regions_loaned_for_young_evac = 0;\n-    loaned_regions = 0;\n-  }\n-\n-  size_t old_bytes_loaned_for_young_evac = old_regions_loaned_for_young_evac * region_size_bytes - available_loan_remnant;\n-\n-  \/\/ Recompute regions_available_to_loan based on possible changes to old_regions_loaned_for_young_evac and\n-  \/\/ old_evacuation_reserve.\n-\n-  \/\/ Any decrease in old_regions_loaned_for_young_evac are immediately available to be loaned\n-  \/\/ However, a change to old_evacuation_reserve() is not necessarily available to loan, because this memory may\n-  \/\/ reside within many fragments scattered throughout old-gen.\n-\n-  regions_available_to_loan = old_generation->free_unaffiliated_regions();\n-  size_t working_old_available = old_generation->available();\n-\n-  assert(regions_available_to_loan * region_size_bytes <= working_old_available,\n-         \"Regions available to loan  must be less than available memory\");\n-\n-  \/\/ fragmented_old_total is the amount of memory in old-gen beyond regions_available_to_loan that is otherwise not\n-  \/\/ yet dedicated to a particular budget.  This memory can be used for promotion_reserve.\n-  size_t fragmented_old_total = working_old_available - regions_available_to_loan * region_size_bytes;\n-\n-  \/\/ fragmented_old_usage is the memory that is dedicated to holding evacuated old-gen objects, which does not need\n-  \/\/ to be an integral number of regions.\n-  size_t fragmented_old_usage = old_evacuated_committed + consumed_by_advance_promotion;\n-\n-\n-\n-  if (fragmented_old_total >= fragmented_old_usage) {\n-    \/\/ Seems this will be rare.  In this case, all of the memory required for old-gen evacuations and promotions can be\n-    \/\/ taken from the existing fragments within old-gen.  Reduce this fragmented total by this amount.\n-    fragmented_old_total -= fragmented_old_usage;\n-    \/\/ And reduce regions_available_to_loan by the regions dedicated to young_evac.\n-    regions_available_to_loan -= old_regions_loaned_for_young_evac;\n-  } else {\n-    \/\/ In this case, we need to dedicate some of the regions_available_to_loan to hold the results of old-gen evacuations\n-    \/\/ and promotions.\n-\n-    size_t unaffiliated_memory_required_for_old = fragmented_old_usage - fragmented_old_total;\n-    size_t unaffiliated_regions_used_by_old = (unaffiliated_memory_required_for_old + region_size_bytes - 1) \/ region_size_bytes;\n-    regions_available_to_loan -= (unaffiliated_regions_used_by_old + old_regions_loaned_for_young_evac);\n-\n-    size_t memory_for_promotions_and_old_evac = fragmented_old_total + unaffiliated_regions_used_by_old;\n-    size_t memory_required_for_promotions_and_old_evac = fragmented_old_usage;\n-    size_t excess_fragmented = memory_for_promotions_and_old_evac - memory_required_for_promotions_and_old_evac;\n-    fragmented_old_total = excess_fragmented;\n-  }\n-\n-  \/\/ Subtract from working_old_available old_evacuated_committed and consumed_by_advance_promotion\n-  working_old_available -= fragmented_old_usage;\n-  \/\/ And also subtract out the regions loaned for young evacuation\n-  working_old_available -= old_regions_loaned_for_young_evac * region_size_bytes;\n-\n-  \/\/ Assure that old_evacuated_committed + old_bytes_loaned_for_young_evac >= the minimum evacuation reserve\n-  \/\/ in order to prevent promotion reserve from violating minimum evacuation reserve.\n-  size_t old_regions_reserved_for_alloc_supplement = 0;\n-  size_t old_bytes_reserved_for_alloc_supplement = 0;\n-  size_t reserved_bytes_for_future_old_evac = 0;\n-\n-  old_bytes_reserved_for_alloc_supplement = available_loan_remnant;\n-  available_loan_remnant = 0;\n-\n-  \/\/ Memory that has been loaned for young evacuations and old-gen regions in the current mixed-evacuation collection\n-  \/\/ set will be available to hold future old-gen evacuations.  If this memory is less than the desired amount of memory\n-  \/\/ set aside for old-gen compaction reserve, try to set aside additional memory so that it will be available during\n-  \/\/ the next mixed evacuation cycle.  Note that memory loaned to young-gen for allocation supplement is excluded from\n-  \/\/ the old-gen promotion reserve.\n-  size_t future_evac_reserve_regions = old_regions_loaned_for_young_evac + collection_set->get_old_region_count();\n-  size_t collected_regions = collection_set->get_young_region_count();\n-\n-  if (future_evac_reserve_regions < ShenandoahOldCompactionReserve) {\n-    \/\/ Require that we loan more memory for holding young evacuations to assure that we have adequate reserves to receive\n-    \/\/ old-gen evacuations during subsequent collections.  Loaning this memory for an allocation supplement does not\n-    \/\/ satisfy our needs because newly allocated objects are not necessarily counter-balanced by reclaimed collection\n-    \/\/ set regions.\n-\n-    \/\/ Put this memory into reserve by identifying it as old_regions_loaned_for_young_evac\n-    size_t additional_regions_to_loan = ShenandoahOldCompactionReserve - future_evac_reserve_regions;\n-\n-    \/\/ We can loan additional regions to be repaid from the anticipated recycling of young collection set regions\n-    \/\/ provided that these regions are currently available within old-gen memory.\n-    size_t collected_regions_to_loan;\n-    if (collected_regions >= additional_regions_to_loan) {\n-      collected_regions_to_loan = additional_regions_to_loan;\n-      additional_regions_to_loan = 0;\n-    } else if (collected_regions > 0) {\n-      collected_regions_to_loan = collected_regions;\n-      additional_regions_to_loan -= collected_regions_to_loan;\n-    } else {\n-      collected_regions_to_loan = 0;\n-    }\n-\n-    if (collected_regions_to_loan > 0) {\n-      \/\/ We're evacuating at least this many regions, it's ok to use these regions for allocation supplement since\n-      \/\/ we'll be able to repay the loan at end of this GC pass, assuming the regions are available.\n-      if (collected_regions_to_loan > regions_available_to_loan) {\n-        collected_regions_to_loan = regions_available_to_loan;\n-      }\n-      old_bytes_reserved_for_alloc_supplement += collected_regions_to_loan * region_size_bytes;\n-      regions_available_to_loan -= collected_regions_to_loan;\n-      loaned_regions += collected_regions_to_loan;\n-      working_old_available -= collected_regions_to_loan * region_size_bytes;\n-    }\n-\n-    \/\/ If there's still memory that we want to exclude from the current promotion reserve, but we are unable to loan\n-    \/\/ this memory because fully empty old-gen regions are not available, decrement the working_old_available to make\n-    \/\/ sure that this memory is not used to hold the results of old-gen evacuation.\n-    if (additional_regions_to_loan > regions_available_to_loan) {\n-      size_t unloaned_regions = additional_regions_to_loan - regions_available_to_loan;\n-      size_t unloaned_bytes = unloaned_regions * region_size_bytes;\n-\n-      if (working_old_available < unloaned_bytes) {\n-        \/\/ We're in dire straits.  We won't be able to reserve all the memory that we want to make available for the\n-        \/\/ next old-gen evacuation.  We'll reserve as much of it as possible.  Setting working_old_available to zero\n-        \/\/ means there will be no promotion except for the advance promotion.  Note that if some advance promotion fails,\n-        \/\/ the object will be evacuated to young-gen so we should still end up reclaiming the entire advance promotion\n-        \/\/ collection set.\n-        reserved_bytes_for_future_old_evac = working_old_available;\n-        working_old_available = 0;\n-      } else {\n-        reserved_bytes_for_future_old_evac = unloaned_bytes;\n-        working_old_available -= unloaned_bytes;\n-      }\n-      size_t regions_reserved_for_future_old_evac =\n-        (reserved_bytes_for_future_old_evac + region_size_bytes - 1) \/ region_size_bytes;\n-\n-      if (regions_reserved_for_future_old_evac < regions_available_to_loan) {\n-        regions_available_to_loan -= regions_reserved_for_future_old_evac;\n-      } else {\n-        regions_available_to_loan = 0;\n-      }\n+  assert(young_evacuated_reserve_used <= young_generation->available(), \"Cannot evacuate more than is available in young\");\n+  heap->set_young_evac_reserve(young_evacuated_reserve_used);\n@@ -627,3 +396,18 @@\n-      \/\/ Since we're in dire straits, zero out fragmented_old_total so this won't be used for promotion;\n-      if (working_old_available > fragmented_old_total) {\n-        working_old_available -= fragmented_old_total;\n+  size_t old_available = old_generation->available();\n+  \/\/ Now that we've established the collection set, we know how much memory is really required by old-gen for evacuation\n+  \/\/ and promotion reserves.  Try shrinking OLD now in case that gives us a bit more runway for mutator allocations during\n+  \/\/ evac and update phases.\n+  size_t old_consumed = old_evacuated_committed + young_advance_promoted_reserve_used;\n+  assert(old_available >= old_consumed, \"Cannot consume more than is available\");\n+  size_t excess_old = old_available - old_consumed;\n+  size_t unaffiliated_old_regions = old_generation->free_unaffiliated_regions();\n+  size_t unaffiliated_old = unaffiliated_old_regions * region_size_bytes;\n+  assert(old_available >= unaffiliated_old, \"Unaffiliated old is a subset of old available\");\n+\n+  \/\/ Make sure old_evac_committed is unaffiliated\n+  if (old_evacuated_committed > 0) {\n+    if (unaffiliated_old > old_evacuated_committed) {\n+      size_t giveaway = unaffiliated_old - old_evacuated_committed;\n+      size_t giveaway_regions = giveaway \/ region_size_bytes;  \/\/ round down\n+      if (giveaway_regions > 0) {\n+        excess_old = MIN2(excess_old, giveaway_regions * region_size_bytes);\n@@ -631,1 +415,1 @@\n-        working_old_available = 0;\n+        excess_old = 0;\n@@ -633,1 +417,2 @@\n-      fragmented_old_total = 0;\n+    } else {\n+      excess_old = 0;\n@@ -637,46 +422,12 @@\n-  \/\/ Establish young_evac_reserve so that this young-gen memory is not used for new allocations, allowing the memory\n-  \/\/ to be returned to old-gen as soon as the current collection set regions are reclaimed.\n-  heap->set_young_evac_reserve(young_evacuated_reserve_used);\n-\n-  \/\/ Limit promoted_reserve so that we can set aside memory to be loaned from old-gen to young-gen.  This\n-  \/\/ value is not \"critical\".  If we underestimate, certain promotions will simply be deferred.  If we put\n-  \/\/ \"all the rest\" of old-gen memory into the promotion reserve, we'll have nothing left to loan to young-gen\n-  \/\/ during the evac and update phases of GC.  So we \"limit\" the sizes of the promotion budget to be the smaller of:\n-  \/\/\n-  \/\/  1. old_available\n-  \/\/     (old_available is old_gen->available() -\n-  \/\/      (old_evacuated_committed + consumed_by_advance_promotion + loaned_for_young_evac + reserved_for_alloc_supplement))\n-  \/\/  2. young bytes reserved for evacuation (we can't promote more than young is evacuating)\n-  size_t promotion_reserve = working_old_available;\n-\n-  \/\/ We experimented with constraining promoted_reserve to be no larger than 4 times the size of previously_promoted,\n-  \/\/ but this constraint was too limiting, resulting in failure of legitimate promotions.  This was tried before we\n-  \/\/ had special handling in place for advance promotion.  We should retry now that advance promotion is handled\n-  \/\/ specially.\n-\n-  \/\/ We had also experimented with constraining promoted_reserve to be no more than young_evacuation_committed\n-  \/\/ divided by promotion_divisor, where:\n-  \/\/  size_t promotion_divisor = (0x02 << InitialTenuringThreshold) - 1;\n-  \/\/ This also was found to be too limiting, resulting in failure of legitimate promotions.\n-  \/\/\n-  \/\/ Both experiments were conducted in the presence of other bugs which could have been the root cause for\n-  \/\/ the failures identified above as being \"too limiting\".  TODO: conduct new experiments with the more limiting\n-  \/\/ values of young_evacuation_reserved_used.\n-\n-  \/\/ young_evacuation_reserve_used already excludes bytes known to be promoted, which equals consumed_by_advance_promotion\n-  if (young_evacuated_reserve_used < promotion_reserve) {\n-    \/\/ Shrink promotion_reserve if it is larger than the memory to be consumed by evacuating all young objects in\n-    \/\/ collection set, including anticipated waste.  There's no benefit in using a larger promotion_reserve.\n-    \/\/ young_evacuation_reserve_used does not include live memory within tenure-aged regions.\n-    promotion_reserve = young_evacuated_reserve_used;\n-  }\n-  assert(working_old_available >= promotion_reserve, \"Cannot reserve for promotion more than is available\");\n-  working_old_available -= promotion_reserve;\n-  \/\/ Having reserved this memory for promotion, the regions are no longer available to be loaned.\n-  size_t regions_consumed_by_promotion_reserve = (promotion_reserve + region_size_bytes - 1) \/ region_size_bytes;\n-  if (regions_consumed_by_promotion_reserve > regions_available_to_loan) {\n-    \/\/ This can happen if the promotion reserve makes use of memory that is fragmented between many partially available\n-    \/\/ old-gen regions.\n-    regions_available_to_loan = 0;\n-  } else {\n-    regions_available_to_loan -= regions_consumed_by_promotion_reserve;\n+  \/\/ If we find that OLD has excess regions, give them back to YOUNG now to reduce likelihood we run out of allocation\n+  \/\/ runway during evacuation and update-refs.\n+  size_t regions_to_xfer = 0;\n+  if (excess_old > unaffiliated_old) {\n+    \/\/ we can give back unaffiliated_old (all of unaffiliated is excess)\n+    if (unaffiliated_old_regions > 0) {\n+      regions_to_xfer = unaffiliated_old_regions;\n+    }\n+  } else if (unaffiliated_old_regions > 0) {\n+    \/\/ excess_old < unaffiliated old: we can give back MIN(excess_old\/region_size_bytes, unaffiliated_old_regions)\n+    size_t excess_regions = excess_old \/ region_size_bytes;\n+    size_t regions_to_xfer = MIN2(excess_regions, unaffiliated_old_regions);\n@@ -685,13 +436,6 @@\n-  log_debug(gc)(\"old_gen->available(): \" SIZE_FORMAT \" divided between promotion reserve: \" SIZE_FORMAT\n-                \", old evacuation reserve: \" SIZE_FORMAT \", advance promotion reserve supplement: \" SIZE_FORMAT\n-                \", old loaned for young evacuation: \" SIZE_FORMAT \", old reserved for alloc supplement: \" SIZE_FORMAT,\n-                old_generation->available(), promotion_reserve, old_evacuated_committed, consumed_by_advance_promotion,\n-                old_regions_loaned_for_young_evac * region_size_bytes, old_bytes_reserved_for_alloc_supplement);\n-\n-  promotion_reserve += consumed_by_advance_promotion;\n-  heap->set_promoted_reserve(promotion_reserve);\n-\n-  heap->reset_promoted_expended();\n-  if (collection_set->get_old_bytes_reserved_for_evacuation() == 0) {\n-    \/\/ Setting old evacuation reserve to zero denotes that there is no old-gen evacuation in this pass.\n-    heap->set_old_evac_reserve(0);\n+  if (regions_to_xfer > 0) {\n+    bool result = heap->generation_sizer()->transfer_to_young(regions_to_xfer);\n+    assert(excess_old > regions_to_xfer * region_size_bytes, \"Cannot xfer more than excess old\");\n+    excess_old -= regions_to_xfer * region_size_bytes;\n+    log_info(gc, ergo)(\"%s transferred \" SIZE_FORMAT \" excess regions to young before start of evacuation\",\n+                       result? \"Successfully\": \"Unsuccessfully\", regions_to_xfer);\n@@ -700,73 +444,5 @@\n-  size_t old_gen_usage_base = old_generation->used() - collection_set->get_old_garbage();\n-  heap->capture_old_usage(old_gen_usage_base);\n-\n-  \/\/ Compute additional evacuation supplement, which is extra memory borrowed from old-gen that can be allocated\n-  \/\/ by mutators while GC is working on evacuation and update-refs.  This memory can be temporarily borrowed\n-  \/\/ from old-gen allotment, then repaid at the end of update-refs from the recycled collection set.  After\n-  \/\/ we have computed the collection set based on the parameters established above, we can make additional\n-  \/\/ loans based on our knowledge of the collection set to determine how much allocation we can allow\n-  \/\/ during the evacuation and update-refs phases of execution.  The total available supplement is the result\n-  \/\/ of adding old_bytes_reserved_for_alloc_supplement to the smaller of:\n-  \/\/\n-  \/\/   1. regions_available_to_loan * region_size_bytes\n-  \/\/   2. The replenishment budget (number of regions in collection set - the number of regions already\n-  \/\/         under lien for the young_evacuation_reserve)\n-  \/\/\n-\n-  \/\/ Regardless of how many regions may be available to be loaned, we can loan no more regions than\n-  \/\/ the total number of young regions to be evacuated.  Call this the regions_for_runway.\n-\n-  if (regions_available_to_loan > 0 && (collected_regions > loaned_regions)) {\n-    assert(regions_available_to_loan * region_size_bytes <= working_old_available,\n-           \"regions_available_to_loan should not exceed working_old_available\");\n-\n-    size_t additional_regions_to_loan = collected_regions - loaned_regions;\n-    if (additional_regions_to_loan > regions_available_to_loan) {\n-      additional_regions_to_loan = regions_available_to_loan;\n-    }\n-    loaned_regions += additional_regions_to_loan;\n-    old_bytes_reserved_for_alloc_supplement += additional_regions_to_loan * region_size_bytes;\n-    working_old_available -= additional_regions_to_loan * region_size_bytes;\n-  }\n-  size_t allocation_supplement = old_bytes_reserved_for_alloc_supplement + old_bytes_loaned_for_young_evac;\n-  assert(allocation_supplement % ShenandoahHeapRegion::region_size_bytes() == 0,\n-         \"allocation_supplement must be multiple of region size\");\n-\n-  heap->set_alloc_supplement_reserve(allocation_supplement);\n-\n-  \/\/ TODO: young_available, which feeds into alloc_budget_evac_and_update is lacking memory available within\n-  \/\/ existing young-gen regions that were not selected for the collection set.  Add this in and adjust the\n-  \/\/ log message (where it says \"empty-region allocation budget\").\n-\n-\n-  log_debug(gc)(\"Memory reserved for young evacuation: \" SIZE_FORMAT \"%s for evacuating \" SIZE_FORMAT\n-                \"%s out of young available: \" SIZE_FORMAT \"%s\",\n-                byte_size_in_proper_unit(young_evacuated_reserve_used),\n-                proper_unit_for_byte_size(young_evacuated_reserve_used),\n-                byte_size_in_proper_unit(young_evacuated), proper_unit_for_byte_size(young_evacuated),\n-                byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n-\n-  log_debug(gc)(\"Memory reserved for old evacuation: \" SIZE_FORMAT \"%s for evacuating \" SIZE_FORMAT\n-                \"%s out of old available: \" SIZE_FORMAT \"%s\",\n-                byte_size_in_proper_unit(old_evacuated), proper_unit_for_byte_size(old_evacuated),\n-                byte_size_in_proper_unit(old_evacuated), proper_unit_for_byte_size(old_evacuated),\n-                byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available));\n-\n-  size_t regular_promotion = promotion_reserve - consumed_by_advance_promotion;\n-  size_t excess =\n-    old_available - (old_evacuation_reserve + promotion_reserve + old_bytes_loaned_for_young_evac + allocation_supplement);\n-  log_info(gc, ergo)(\"Old available: \" SIZE_FORMAT \"%s is partitioned into old evacuation budget: \" SIZE_FORMAT\n-                     \"%s, aged region promotion budget: \" SIZE_FORMAT\n-                     \"%s, regular region promotion budget: \" SIZE_FORMAT\n-                     \"%s, loaned for young evacuation: \" SIZE_FORMAT\n-                     \"%s, loaned for young allocations: \" SIZE_FORMAT\n-                     \"%s, excess: \" SIZE_FORMAT \"%s\",\n-                     byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n-                     byte_size_in_proper_unit(old_evacuation_reserve), proper_unit_for_byte_size(old_evacuation_reserve),\n-                     byte_size_in_proper_unit(consumed_by_advance_promotion),\n-                     proper_unit_for_byte_size(consumed_by_advance_promotion),\n-                     byte_size_in_proper_unit(regular_promotion), proper_unit_for_byte_size(regular_promotion),\n-                     byte_size_in_proper_unit(old_bytes_loaned_for_young_evac),\n-                     proper_unit_for_byte_size(old_bytes_loaned_for_young_evac),\n-                     byte_size_in_proper_unit(allocation_supplement), proper_unit_for_byte_size(allocation_supplement),\n-                     byte_size_in_proper_unit(excess), proper_unit_for_byte_size(excess));\n+  \/\/ Add in the excess_old memory to hold unanticipated promotions, if any.  If there are more unanticipated\n+  \/\/ promotions than fit in reserved memory, they will be deferred until a future GC pass.\n+  size_t total_promotion_reserve = young_advance_promoted_reserve_used + excess_old;\n+  heap->set_promoted_reserve(total_promotion_reserve);\n+  heap->reset_promoted_expended();\n@@ -820,1 +496,0 @@\n-\n@@ -830,1 +505,0 @@\n-\n@@ -835,1 +509,5 @@\n-    heap->free_set()->rebuild();\n+    size_t young_cset_regions, old_cset_regions;\n+\n+    \/\/ We are preparing for evacuation.  At this time, we ignore cset region tallies.\n+    heap->free_set()->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+    heap->free_set()->rebuild((size_t)(collection_set->get_young_bytes_reserved_for_evacuation() * ShenandoahEvacWaste));\n@@ -890,1 +568,1 @@\n-  _affiliated_region_count(0), _used(0), _bytes_allocated_since_gc_start(0),\n+  _affiliated_region_count(0), _humongous_waste(0), _used(0), _bytes_allocated_since_gc_start(0),\n@@ -892,1 +570,1 @@\n-  _adjusted_capacity(soft_max_capacity), _heuristics(nullptr) {\n+  _heuristics(nullptr) {\n@@ -950,0 +628,20 @@\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         \"used + humongous cannot exceed regions\");\n+  return _affiliated_region_count;\n+}\n+\n+size_t ShenandoahGeneration::increase_affiliated_region_count(size_t delta) {\n+  shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+  _affiliated_region_count += delta;\n+  return _affiliated_region_count;\n+}\n+\n+size_t ShenandoahGeneration::decrease_affiliated_region_count(size_t delta) {\n+  shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+  assert(_affiliated_region_count > delta, \"Affiliated region count cannot be negative\");\n+\n+  _affiliated_region_count -= delta;\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         \"used + humongous cannot exceed regions\");\n@@ -957,1 +655,1 @@\n-  \/\/ future improvement: _humongous_waste = humongous_waste;\n+  _humongous_waste = humongous_waste;\n@@ -967,0 +665,1 @@\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Only generational mode accounts for used within generations\");\n@@ -968,0 +667,25 @@\n+  \/\/ This detects arithmetic wraparound on _used.  Non-generational mode does not keep track of _affiliated_region_count\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+         \"used cannot exceed regions\");\n+}\n+\n+void ShenandoahGeneration::increase_humongous_waste(size_t bytes) {\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Only generational mode accounts for used within generations\");\n+  if (bytes > 0) {\n+    shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+    _humongous_waste += bytes;\n+    assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+           (_used + _humongous_waste <= _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes()),\n+           \"waste cannot exceed regions\");\n+  }\n+}\n+\n+void ShenandoahGeneration::decrease_humongous_waste(size_t bytes) {\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Only generational mode accounts for used within generations\");\n+  if (bytes > 0) {\n+    shenandoah_assert_heaplocked_or_fullgc_safepoint();\n+    assert(ShenandoahHeap::heap()->is_full_gc_in_progress() || (_humongous_waste >= bytes),\n+           \"Waste (\" SIZE_FORMAT \") cannot be negative (after subtracting \" SIZE_FORMAT \")\", _humongous_waste, bytes);\n+    _humongous_waste -= bytes;\n+  }\n@@ -971,1 +695,3 @@\n-  assert(_used >= bytes, \"cannot reduce bytes used by generation below zero\");\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Only generational mode accounts for used within generations\");\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+\t (_used >= bytes), \"cannot reduce bytes used by generation below zero\");\n@@ -973,0 +699,5 @@\n+\n+  \/\/ Non-generational mode does not maintain affiliated region counts\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() >= _used),\n+         \"Affiliated regions must hold more than what is currently used\");\n@@ -982,1 +713,1 @@\n-    result = 0;                 \/\/ If old-gen is loaning regions to young-gen, affiliated regions may exceed capacity temporarily.\n+    result = 0;\n@@ -994,1 +725,1 @@\n-  size_t in_use = used();\n+  size_t in_use = used() + get_humongous_waste();\n@@ -999,35 +730,0 @@\n-size_t ShenandoahGeneration::adjust_available(intptr_t adjustment) {\n-  \/\/ TODO: ysr: remove this check & warning\n-  if (adjustment % ShenandoahHeapRegion::region_size_bytes() != 0) {\n-    log_warning(gc)(\"Adjustment (\" INTPTR_FORMAT \") should be a multiple of region size (\" SIZE_FORMAT \")\",\n-                    adjustment, ShenandoahHeapRegion::region_size_bytes());\n-  }\n-  assert(adjustment % ShenandoahHeapRegion::region_size_bytes() == 0,\n-         \"Adjustment to generation size must be multiple of region size\");\n-  _adjusted_capacity = soft_max_capacity() + adjustment;\n-  return _adjusted_capacity;\n-}\n-\n-size_t ShenandoahGeneration::unadjust_available() {\n-  _adjusted_capacity = soft_max_capacity();\n-  return _adjusted_capacity;\n-}\n-\n-size_t ShenandoahGeneration::adjusted_available() const {\n-  size_t in_use = used();\n-  size_t capacity = _adjusted_capacity;\n-  return in_use > capacity ? 0 : capacity - in_use;\n-}\n-\n-size_t ShenandoahGeneration::adjusted_capacity() const {\n-  return _adjusted_capacity;\n-}\n-\n-size_t ShenandoahGeneration::adjusted_unaffiliated_regions() const {\n-  assert(adjusted_capacity() >= used_regions_size(), \"adjusted_unaffiliated_regions() cannot return negative\");\n-  assert((adjusted_capacity() - used_regions_size()) % ShenandoahHeapRegion::region_size_bytes() == 0,\n-         \"adjusted capacity (\" SIZE_FORMAT \") and used regions size (\" SIZE_FORMAT \") should be multiples of region_size_bytes\",\n-         adjusted_capacity(), used_regions_size());\n-  return (adjusted_capacity() - used_regions_size()) \/ ShenandoahHeapRegion::region_size_bytes();\n-}\n-\n@@ -1035,0 +731,1 @@\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Only generational mode accounts for used within generations\");\n@@ -1036,7 +733,7 @@\n-  assert(_max_capacity + increment <= ShenandoahHeap::heap()->max_size_for(this), \"Cannot increase generation capacity beyond maximum.\");\n-  assert(increment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n-  \/\/ TODO: ysr: remove this check and warning\n-  if (increment % ShenandoahHeapRegion::region_size_bytes() != 0) {\n-    log_warning(gc)(\"Increment (\" INTPTR_FORMAT \") should be a multiple of region size (\" SIZE_FORMAT \")\",\n-                    increment, ShenandoahHeapRegion::region_size_bytes());\n-  }\n+\n+  \/\/ We do not enforce that new capacity >= heap->max_size_for(this).  The maximum generation size is treated as a rule of thumb\n+  \/\/ which may be violated during certain transitions, such as when we are forcing transfers for the purpose of promoting regions\n+  \/\/ in place.\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+\t (_max_capacity + increment <= ShenandoahHeap::heap()->max_capacity()), \"Generation cannot be larger than heap size\");\n+  assert(increment % ShenandoahHeapRegion::region_size_bytes() == 0, \"Generation capacity must be multiple of region size\");\n@@ -1045,1 +742,5 @@\n-  _adjusted_capacity += increment;\n+\n+  \/\/ This detects arithmetic wraparound on _used\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+\t (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() >= _used),\n+         \"Affiliated regions must hold more than what is currently used\");\n@@ -1049,0 +750,1 @@\n+  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Only generational mode accounts for used within generations\");\n@@ -1050,7 +752,8 @@\n-  assert(_max_capacity - decrement >= ShenandoahHeap::heap()->min_size_for(this), \"Cannot decrease generation capacity beyond minimum.\");\n-  assert(decrement % ShenandoahHeapRegion::region_size_bytes() == 0, \"Region-sized changes only\");\n-  \/\/ TODO: ysr: remove this check and warning\n-  if (decrement % ShenandoahHeapRegion::region_size_bytes() != 0) {\n-    log_warning(gc)(\"Decrement (\" INTPTR_FORMAT \") should be a multiple of region size (\" SIZE_FORMAT \")\",\n-                    decrement, ShenandoahHeapRegion::region_size_bytes());\n-  }\n+\n+  \/\/ We do not enforce that new capacity >= heap->min_size_for(this).  The minimum generation size is treated as a rule of thumb\n+  \/\/ which may be violated during certain transitions, such as when we are forcing transfers for the purpose of promoting regions\n+  \/\/ in place.\n+  assert(decrement % ShenandoahHeapRegion::region_size_bytes() == 0, \"Generation capacity must be multiple of region size\");\n+  assert(_max_capacity >= decrement, \"Generation capacity cannot be negative\");\n+  assert(_soft_max_capacity >= decrement, \"Generation soft capacity cannot be negative\");\n+\n@@ -1059,1 +762,10 @@\n-  _adjusted_capacity -= decrement;\n+\n+  \/\/ This detects arithmetic wraparound on _used\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+\t (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() >= _used),\n+         \"Affiliated regions must hold more than what is currently used\");\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+\t (_used <= _max_capacity), \"Cannot use more than capacity\");\n+  assert(ShenandoahHeap::heap()->is_full_gc_in_progress() ||\n+         (_affiliated_region_count * ShenandoahHeapRegion::region_size_bytes() <= _max_capacity),\n+         \"Cannot use more than capacity\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":237,"deletions":525,"binary":false,"changes":762,"status":"modified"},{"patch":"@@ -51,0 +51,3 @@\n+  size_t _affiliated_region_count;\n+  size_t _humongous_waste;      \/\/ how much waste for padding in humongous objects\n+\n@@ -53,1 +56,0 @@\n-  size_t _affiliated_region_count;\n@@ -59,2 +61,0 @@\n-  size_t _adjusted_capacity;\n-\n@@ -98,16 +98,0 @@\n-  \/\/ During evacuation and update-refs, some memory may be shifted between generations.  In particular, memory\n-  \/\/ may be loaned by old-gen to young-gen based on the promise the loan will be promptly repaid from the memory reclaimed\n-  \/\/ when the current collection set is recycled.  The capacity adjustment also takes into consideration memory that is\n-  \/\/ set aside within each generation to hold the results of evacuation, but not promotion, into that region.  Promotions\n-  \/\/ into old-gen are bounded by adjusted_available() whereas evacuations into old-gen are pre-committed.\n-  virtual size_t adjusted_available() const;\n-  virtual size_t adjusted_capacity() const;\n-\n-  \/\/ This is the number of FREE regions that are eligible to be affiliated with this generation according to the current\n-  \/\/ adjusted capacity.\n-  virtual size_t adjusted_unaffiliated_regions() const;\n-\n-  \/\/ Both of following return new value of available\n-  virtual size_t adjust_available(intptr_t adjustment);\n-  virtual size_t unadjust_available();\n-\n@@ -186,0 +170,6 @@\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t increase_affiliated_region_count(size_t delta);\n+\n+  \/\/ Return the updated value of affiliated_region_count\n+  size_t decrease_affiliated_region_count(size_t delta);\n+\n@@ -192,0 +182,4 @@\n+  void increase_humongous_waste(size_t bytes);\n+  void decrease_humongous_waste(size_t bytes);\n+  size_t get_humongous_waste() const { return _humongous_waste; }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":13,"deletions":19,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -382,0 +382,1 @@\n+    size_t young_cset_regions, old_cset_regions;\n@@ -383,1 +384,3 @@\n-    _free_set->rebuild();\n+    \/\/ We are initializing free set.  We ignore cset region tallies.\n+    _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+    _free_set->rebuild(0);\n@@ -479,9 +482,15 @@\n-  _generation_sizer.heap_size_changed(soft_max_capacity());\n-  size_t initial_capacity_young = _generation_sizer.max_young_size();\n-  size_t max_capacity_young = _generation_sizer.max_young_size();\n-  size_t initial_capacity_old = max_capacity() - max_capacity_young;\n-  size_t max_capacity_old = max_capacity() - initial_capacity_young;\n-\n-  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n-  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n-  _global_generation = new ShenandoahGlobalGeneration(_max_workers, soft_max_capacity(), soft_max_capacity());\n+  if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+    _generation_sizer.heap_size_changed(soft_max_capacity());\n+    size_t initial_capacity_young = _generation_sizer.max_young_size();\n+    size_t max_capacity_young = _generation_sizer.max_young_size();\n+    size_t initial_capacity_old = max_capacity() - max_capacity_young;\n+    size_t max_capacity_old = max_capacity() - initial_capacity_young;\n+\n+    _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_young, initial_capacity_young);\n+    _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, initial_capacity_old);\n+    _global_generation = new ShenandoahGlobalGeneration(_max_workers, soft_max_capacity(), soft_max_capacity());\n+  } else {\n+    _young_generation = new ShenandoahYoungGeneration(_max_workers, soft_max_capacity(), soft_max_capacity());\n+    _old_generation = new ShenandoahOldGeneration(_max_workers, 0L, 0L);\n+    _global_generation = new ShenandoahGlobalGeneration(_max_workers, soft_max_capacity(), soft_max_capacity());\n+  }\n@@ -522,2 +531,0 @@\n-\n-    ShenandoahEvacWaste = ShenandoahGenerationalEvacWaste;\n@@ -537,0 +544,2 @@\n+  _promotion_potential(0),\n+  _promotion_in_place_potential(0),\n@@ -547,1 +556,0 @@\n-  _alloc_supplement_reserve(0),\n@@ -837,3 +845,1 @@\n-  \/\/ We squelch excessive reports to reduce noise in logs.  Squelch enforcement is not \"perfect\" because\n-  \/\/ this same code can be in-lined in multiple contexts, and each context will have its own copy of the static\n-  \/\/ last_report_epoch and this_epoch_report_count variables.\n+  \/\/ We squelch excessive reports to reduce noise in logs.\n@@ -859,0 +865,5 @@\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahGeneration* old_gen = heap->old_generation();\n+    size_t old_capacity = old_gen->max_capacity();\n+    size_t old_usage = old_gen->used();\n+    size_t old_free_regions = old_gen->free_unaffiliated_regions();\n@@ -861,3 +872,6 @@\n-                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT,\n-                       size, plab == nullptr? \"no\": \"yes\",\n-                       words_remaining, promote_enabled, promotion_reserve, promotion_expended);\n+                       \", plab promotions %s, promotion reserve: \" SIZE_FORMAT \", promotion expended: \" SIZE_FORMAT\n+                       \", old capacity: \" SIZE_FORMAT \", old_used: \" SIZE_FORMAT \", old unaffiliated regions: \" SIZE_FORMAT,\n+                       size * HeapWordSize, plab == nullptr? \"no\": \"yes\",\n+                       words_remaining * HeapWordSize, promote_enabled, promotion_reserve, promotion_expended,\n+                       old_capacity, old_usage, old_free_regions);\n+\n@@ -973,1 +987,0 @@\n-\n@@ -981,1 +994,6 @@\n-      return nullptr;\n+      if (min_size == PLAB::min_size()) {\n+        \/\/ Disable plab promotions for this thread because we cannot even allocate a plab of minimal size.  This allows us\n+        \/\/ to fail faster on subsequent promotion attempts.\n+        ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+      }\n+      return NULL;\n@@ -1000,1 +1018,0 @@\n-\n@@ -1098,3 +1115,42 @@\n-bool ShenandoahHeap::adjust_generation_sizes() {\n-  if (mode()->is_generational()) {\n-    return _generation_sizer.adjust_generation_sizes();\n+\/\/ xfer_limit is the maximum we're able to transfer from young to old\n+void ShenandoahHeap::adjust_generation_sizes_for_next_cycle(\n+  size_t xfer_limit, size_t young_cset_regions, size_t old_cset_regions) {\n+\n+  \/\/ Make sure old-generation is large enough, but no larger, than is necessary to hold mixed evacuations\n+  \/\/ and promotions if we anticipate either.\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t promo_load = get_promotion_potential();\n+  \/\/ The free set will reserve this amount of memory to hold young evacuations\n+  size_t young_reserve = (young_generation()->max_capacity() * ShenandoahEvacReserve) \/ 100;\n+  size_t old_reserve = 0;\n+  size_t mixed_candidates = old_heuristics()->unprocessed_old_collection_candidates();\n+  bool doing_mixed = (mixed_candidates > 0);\n+  bool doing_promotions = promo_load > 0;\n+\n+  \/\/ round down\n+  size_t max_old_region_xfer = xfer_limit \/ region_size_bytes;\n+\n+  \/\/ We can limit the reserve to the size of anticipated promotions\n+  size_t max_old_reserve = young_reserve * ShenandoahOldEvacRatioPercent \/ (100 - ShenandoahOldEvacRatioPercent);\n+  \/\/ Here's the algebra:\n+  \/\/  TotalEvacuation = OldEvacuation + YoungEvacuation\n+  \/\/  OldEvacuation = TotalEvacuation*(ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * (ShenandoahOldEvacRatioPercent\/100)\/(1 - ShenandoahOldEvacRatioPercent\/100)\n+  \/\/  OldEvacuation = YoungEvacuation * ShenandoahOldEvacRatioPercent\/(100 - ShenandoahOldEvacRatioPercent)\n+\n+  size_t reserve_for_mixed, reserve_for_promo;\n+  if (doing_mixed) {\n+    assert(old_generation()->available() >= old_generation()->free_unaffiliated_regions() * region_size_bytes,\n+           \"Unaffiliated available must be less than total available\");\n+\n+    \/\/ We want this much memory to be unfragmented in order to reliably evacuate old.  This is conservative because we\n+    \/\/ only have to evacuate the live memory within mixed candidate.\n+    size_t max_evac_need = (size_t) (mixed_candidates * region_size_bytes * ShenandoahOldEvacWaste);\n+    size_t old_fragmented_available =\n+      old_generation()->available() - old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    reserve_for_mixed = max_evac_need + old_fragmented_available;\n+    if (reserve_for_mixed > max_old_reserve) {\n+      reserve_for_mixed = max_old_reserve;\n+    }\n+  } else {\n+    reserve_for_mixed = 0;\n@@ -1102,1 +1158,45 @@\n-  return false;\n+\n+  size_t available_for_promotions = max_old_reserve - reserve_for_mixed;\n+  if (doing_promotions) {\n+    \/\/ We're only promoting and we have a maximum bound on the amount to be promoted\n+    reserve_for_promo = (size_t) (promo_load * ShenandoahPromoEvacWaste);\n+    if (reserve_for_promo > available_for_promotions) {\n+      reserve_for_promo = available_for_promotions;\n+    }\n+  } else {\n+    reserve_for_promo = 0;\n+  }\n+  old_reserve = reserve_for_mixed + reserve_for_promo;\n+  assert(old_reserve <= max_old_reserve, \"cannot reserve more than max for old evacuations\");\n+\n+  size_t old_available = old_generation()->available() + old_cset_regions * region_size_bytes;\n+  size_t young_available = young_generation()->available() + young_cset_regions * region_size_bytes;\n+  size_t old_region_deficit = 0;\n+  size_t old_region_surplus = 0;\n+  if (old_available >= old_reserve) {\n+    size_t old_excess = old_available - old_reserve;\n+    size_t excess_regions = old_excess \/ region_size_bytes;\n+    size_t unaffiliated_old_regions = old_generation()->free_unaffiliated_regions() + old_cset_regions;\n+    size_t unaffiliated_old = unaffiliated_old_regions * region_size_bytes;\n+    if (unaffiliated_old_regions < excess_regions) {\n+      \/\/ We'll give only unaffiliated old to young, which is known to be less than the excess.\n+      old_region_surplus = unaffiliated_old_regions;\n+    } else {\n+      \/\/ unaffiliated_old_regions > excess_regions, so we only give away the excess.\n+      old_region_surplus = excess_regions;\n+    }\n+  } else {\n+    \/\/ We need to request transfer from YOUNG.  Ignore that this will directly impact young_generation()->max_capacity(),\n+    \/\/ indirectly impacting young_reserve and old_reserve.  These computations are conservative.\n+    size_t old_need = old_reserve - old_available;\n+    \/\/ Round up the number of regions needed from YOUNG\n+    old_region_deficit = (old_need + region_size_bytes - 1) \/ region_size_bytes;\n+  }\n+\n+  if (old_region_deficit > max_old_region_xfer) {\n+    \/\/ If we're running short on young-gen memory, limit the xfer\n+    old_region_deficit = max_old_region_xfer;\n+    \/\/ old-gen collection activities will be curtailed if the budget is smaller than desired.\n+  }\n+  set_old_region_surplus(old_region_surplus);\n+  set_old_region_deficit(old_region_deficit);\n@@ -1137,1 +1237,1 @@\n-  \/\/ if we are at risk of exceeding the old-gen evacuation budget.\n+  \/\/ if we are at risk of infringing on the old-gen evacuation budget.\n@@ -1240,14 +1340,12 @@\n-          size_t young_available = young_generation()->adjusted_available();\n-          if (requested_bytes > young_available) {\n-            \/\/ We know this is not a GCLAB.  This must be a TLAB or a shared allocation.\n-            if (req.is_lab_alloc() && (young_available >= req.min_size())) {\n-              try_smaller_lab_size = true;\n-              smaller_lab_size = young_available \/ HeapWordSize;\n-            } else {\n-              \/\/ Can't allocate because even min_size() is larger than remaining young_available\n-              log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n-                                 \", young available: \" SIZE_FORMAT,\n-                                 req.is_lab_alloc()? \"TLAB\": \"shared\",\n-                                 HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_available);\n-              return nullptr;\n-            }\n+          size_t young_words_available = young_generation()->available() \/ HeapWordSize;\n+          if (ShenandoahElasticTLAB && req.is_lab_alloc() && (req.min_size() < young_words_available)) {\n+            \/\/ Allow ourselves to try a smaller lab size even if requested_bytes <= young_available.  We may need a smaller\n+            \/\/ lab size because young memory has become too fragmented.\n+            try_smaller_lab_size = true;\n+            smaller_lab_size = (young_words_available < req.size())? young_words_available: req.size();\n+          } else if (req.size() > young_words_available) {\n+            \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+            log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                               \", young words available: \" SIZE_FORMAT, req.type_string(),\n+                               HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_words_available);\n+            return nullptr;\n@@ -1294,9 +1392,14 @@\n-    if (!try_smaller_lab_size) {\n-      result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n-      if (result != nullptr) {\n-        if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n-          ShenandoahThreadLocalData::reset_plab_promoted(thread);\n-          if (req.is_gc_alloc()) {\n-            if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n-              if (promotion_eligible) {\n-                size_t actual_size = req.actual_size() * HeapWordSize;\n+    \/\/ First try the original request.  If TLAB request size is greater than available, allocate() will attempt to downsize\n+    \/\/ request to fit within available memory.\n+    result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+    if (result != nullptr) {\n+      if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+        ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+        if (req.is_gc_alloc()) {\n+          bool disable_plab_promotions = false;\n+          if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+            if (promotion_eligible) {\n+              size_t actual_size = req.actual_size() * HeapWordSize;\n+              \/\/ The actual size of the allocation may be larger than the requested bytes (due to alignment on card boundaries).\n+              \/\/ If this puts us over our promotion budget, we need to disable future PLAB promotions for this thread.\n+              if (get_promoted_expended() + actual_size <= get_promoted_reserve()) {\n@@ -1307,1 +1410,0 @@\n-                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n@@ -1310,3 +1412,1 @@\n-                \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n-                ShenandoahThreadLocalData::disable_plab_promotions(thread);\n-                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+                disable_plab_promotions = true;\n@@ -1314,4 +1414,2 @@\n-            } else if (is_promotion) {\n-              \/\/ Shared promotion.  Assume size is requested_bytes.\n-              expend_promoted(requested_bytes);\n-              assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+            } else {\n+              disable_plab_promotions = true;\n@@ -1319,0 +1417,9 @@\n+            if (disable_plab_promotions) {\n+              \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+              ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+              ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+            }\n+          } else if (is_promotion) {\n+            \/\/ Shared promotion.  Assume size is requested_bytes.\n+            expend_promoted(requested_bytes);\n+            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n@@ -1320,27 +1427,0 @@\n-\n-          \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n-          \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n-          \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n-          \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n-          \/\/\n-          \/\/ objects being \"concurrently\" allocated:\n-          \/\/    [-----a------][-----b-----][--------------c------------------]\n-          \/\/            [---- card table memory range --------------]\n-          \/\/\n-          \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n-          \/\/   wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n-          \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n-          \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n-          \/\/\n-          \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n-          \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n-          \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n-          ShenandoahHeap::heap()->card_scan()->register_object(result);\n-        }\n-      } else {\n-        \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n-        if ((req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && req.is_gc_alloc() &&\n-            (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n-          \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n-          \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n-          ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n@@ -1348,0 +1428,20 @@\n+\n+        \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+        \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+        \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+        \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+        \/\/\n+        \/\/ objects being \"concurrently\" allocated:\n+        \/\/    [-----a------][-----b-----][--------------c------------------]\n+        \/\/            [---- card table memory range --------------]\n+        \/\/\n+        \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+        \/\/   wants to set the starts-object, first-start, and last-start attributes of the preceding card region.\n+        \/\/   allocation of object b wants to set the starts-object, first-start, and last-start attributes of this card region.\n+        \/\/   allocation of object c also wants to set the starts-object, first-start, and last-start attributes of this\n+        \/\/   card region.\n+        \/\/\n+        \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+        \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+        \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+        ShenandoahHeap::heap()->card_scan()->register_object(result);\n@@ -1349,0 +1449,10 @@\n+    } else {\n+      \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+      if ((req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && req.is_gc_alloc() &&\n+          (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+        \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+        \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+        ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+      }\n+    }\n+    if ((result != nullptr) || !try_smaller_lab_size) {\n@@ -1351,2 +1461,8 @@\n-    \/\/ else, try_smaller_lab_size is true so we fall through and recurse with a smaller lab size\n-  } \/\/ This closes the block that holds the heap lock.  This releases the lock.\n+    \/\/ else, fall through to try_smaller_lab_size\n+  } \/\/ This closes the block that holds the heap lock, releasing the lock.\n+\n+  \/\/ We failed to allocate the originally requested lab size.  Let's see if we can allocate a smaller lab size.\n+  if (req.size() == smaller_lab_size) {\n+    \/\/ If we were already trying to allocate min size, no value in attempting to repeat the same.  End the recursion.\n+    return nullptr;\n+  }\n@@ -1531,0 +1647,3 @@\n+    ShenandoahMarkingContext* const ctx = ShenandoahHeap::heap()->marking_context();\n+    size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+    size_t old_garbage_threshold = (region_size_bytes * ShenandoahOldGarbageThreshold) \/ 100;\n@@ -1532,1 +1651,1 @@\n-      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s]\",\n+      log_debug(gc)(\"GenerationalEvacuationTask do_work(), looking at %s region \" SIZE_FORMAT \", (age: %d) [%s, %s, %s]\",\n@@ -1535,1 +1654,3 @@\n-                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\");\n+                    r->is_humongous()? (r->is_humongous_start()? \"humongous_start\": \"humongous_continuation\"): \"regular\",\n+                    r->is_cset()? \"cset\": \"not-cset\");\n+\n@@ -1542,13 +1663,11 @@\n-      } else if (r->is_young() && r->is_active() && r->is_humongous_start() && (r->age() > InitialTenuringThreshold)) {\n-        \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n-        \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n-        \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n-        if (r->promote_humongous() == 0) {\n-          \/\/ We chose not to promote because old-gen is out of memory.  Report and handle the promotion failure because\n-          \/\/ this suggests need for expanding old-gen and\/or performing collection of old-gen.\n-          ShenandoahHeap* heap = ShenandoahHeap::heap();\n-          oop obj = cast_to_oop(r->bottom());\n-          size_t size = obj->size();\n-          Thread* thread = Thread::current();\n-          heap->report_promotion_failure(thread, size);\n-          heap->handle_promotion_failure();\n+      } else if (r->is_young() && r->is_active() && (r->age() >= InitialTenuringThreshold)) {\n+        HeapWord* tams = ctx->top_at_mark_start(r);\n+        if (r->is_humongous_start()) {\n+          \/\/ We promote humongous_start regions along with their affiliated continuations during evacuation rather than\n+          \/\/ doing this work during a safepoint.  We cannot put humongous regions into the collection set because that\n+          \/\/ triggers the load-reference barrier (LRB) to copy on reference fetch.\n+          r->promote_humongous();\n+        } else if (r->is_regular() && (r->garbage_before_padded_for_promote() < old_garbage_threshold) && (r->get_top_before_promote() == tams)) {\n+          \/\/ Likewise, we cannot put promote-in-place regions into the collection set because that would also trigger\n+          \/\/ the LRB to copy on reference fetch.\n+          r->promote_in_place();\n@@ -1556,0 +1675,7 @@\n+        \/\/ Aged humongous continuation regions are handled with their start region.  If an aged regular region has\n+        \/\/ more garbage than ShenandoahOldGarbageTrheshold, we'll promote by evacuation.  If there is room for evacuation\n+        \/\/ in this cycle, the region will be in the collection set.  If there is not room, the region will be promoted\n+        \/\/ by evacuation in some future GC cycle.\n+\n+        \/\/ If an aged regular region has received allocations during the current cycle, we do not promote because the\n+        \/\/ newly allocated objects do not have appropriate age; this region's age will be reset to zero at end of cycle.\n@@ -1754,1 +1880,1 @@\n-      return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->adjusted_available());\n+      return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->available());\n@@ -1801,0 +1927,6 @@\n+void ShenandoahHeap::mutator_threads_do(ThreadClosure* tcl) const {\n+  ShenandoahJavaThreadsIterator _java_threads(ShenandoahPhaseTimings::thread_iteration_roots, 1);\n+  \/\/ Just use one worker_id for now\n+  _java_threads.threads_do(tcl, 0);\n+}\n+\n@@ -1845,4 +1977,0 @@\n-\n-  \/\/ When a cycle starts, attribute any thread activity when the collector\n-  \/\/ is idle to the global generation.\n-  _mmu_tracker.record(global_generation());\n@@ -1853,1 +1981,0 @@\n-\n@@ -1861,3 +1988,0 @@\n-\n-  \/\/ When a cycle ends, the thread activity is attributed to the respective generation\n-  _mmu_tracker.record(generation);\n@@ -2892,6 +3016,83 @@\n-  {\n-    ShenandoahGCPhase phase(concurrent ?\n-                            ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n-                            ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n+  ShenandoahGCPhase phase(concurrent ?\n+                          ShenandoahPhaseTimings::final_update_refs_rebuild_freeset :\n+                          ShenandoahPhaseTimings::degen_gc_final_update_refs_rebuild_freeset);\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  ShenandoahHeapLocker locker(lock());\n+  size_t young_cset_regions, old_cset_regions;\n+  _free_set->prepare_to_rebuild(young_cset_regions, old_cset_regions);\n+\n+  if (mode()->is_generational()) {\n+    \/\/ Promote aged humongous regions.  We know that all of the regions to be transferred exist in young.\n+    size_t humongous_regions_promoted = get_promotable_humongous_regions();\n+    size_t humongous_bytes_promoted = get_promotable_humongous_usage();\n+    size_t humongous_waste_promoted =\n+      humongous_regions_promoted * ShenandoahHeapRegion::region_size_bytes() - humongous_bytes_promoted;\n+    size_t regular_regions_promoted_in_place = get_regular_regions_promoted_in_place();\n+    size_t total_regions_promoted = humongous_regions_promoted;\n+    size_t bytes_promoted_in_place = 0;\n+    if (total_regions_promoted > 0) {\n+      bytes_promoted_in_place = humongous_bytes_promoted;\n+      log_info(gc, ergo)(\"Promoted \" SIZE_FORMAT \" humongous and \" SIZE_FORMAT \" regular regions in place\"\n+                         \", representing total usage of \" SIZE_FORMAT,\n+                         humongous_regions_promoted, regular_regions_promoted_in_place, bytes_promoted_in_place);\n+      size_t free_old_regions = old_generation()->free_unaffiliated_regions();\n+      \/\/ Decrease usage within young before we transfer capacity to old in order to avoid certain assertion failures.\n+      young_generation()->decrease_humongous_waste(humongous_waste_promoted);\n+      young_generation()->decrease_used(bytes_promoted_in_place);\n+      young_generation()->decrease_affiliated_region_count(total_regions_promoted);\n+      if (free_old_regions < total_regions_promoted) {\n+        size_t needed_regions = total_regions_promoted - free_old_regions;\n+        generation_sizer()->force_transfer_to_old(needed_regions);\n+      }\n+      old_generation()->increase_affiliated_region_count(total_regions_promoted);\n+      old_generation()->increase_used(bytes_promoted_in_place);\n+      old_generation()->increase_humongous_waste(humongous_waste_promoted);\n+    }\n+    assert(verify_generation_usage(true, old_generation()->used_regions(),\n+                                   old_generation()->used(), old_generation()->get_humongous_waste(),\n+                                   true, young_generation()->used_regions(),\n+                                   young_generation()->used(), young_generation()->get_humongous_waste()),\n+           \"Generation accounts are inaccurate\");\n+\n+    \/\/ The computation of evac_slack is quite conservative so consider all of this available for transfer to old.\n+    \/\/ Note that transfer of humongous regions does not impact available.\n+    size_t evac_slack = young_generation()->heuristics()->evac_slack(young_cset_regions);\n+    adjust_generation_sizes_for_next_cycle(evac_slack, young_cset_regions, old_cset_regions);\n+\n+    \/\/ Total old_available may have been expanded to hold anticipated promotions.  We trigger if the fragmented available\n+    \/\/ memory represents more than 16 regions worth of data.  Note that fragmentation may increase when we promote regular\n+    \/\/ regions in place when many of these regular regions have an abundant amount of available memory within them.  Fragmentation\n+    \/\/ will decrease as promote-by-copy consumes the available memory within these partially consumed regions.\n+    \/\/\n+    \/\/ We consider old-gen to have excessive fragmentation if more than 12.5% of old-gen is free memory that resides\n+    \/\/ within partially consumed regions of memory.\n+  }\n+  \/\/ Rebuild free set based on adjusted generation sizes.\n+  _free_set->rebuild(0);\n+\n+  if (mode()->is_generational()) {\n+    size_t old_available = old_generation()->available();\n+    size_t old_unaffiliated_available = old_generation()->free_unaffiliated_regions() * region_size_bytes;\n+    size_t old_fragmented_available;\n+    assert(old_available >= old_unaffiliated_available, \"unaffiliated available is a subset of total available\");\n+    if (old_available >= old_unaffiliated_available) {\n+      old_fragmented_available = old_available - old_unaffiliated_available;\n+    } else {\n+      \/\/ WE SHOULD NOT NEED THIS CONDITIONAL CODE, BUT KELVIN HAS NOT\n+      \/\/ YET FIGURED OUT HOW THIS CONDITION IS VIOLATED.\n+      old_fragmented_available = 0;\n+    }\n+    size_t old_capacity = old_generation()->max_capacity();\n+    size_t heap_capacity = capacity();\n+    if ((old_capacity > heap_capacity \/ 8) && (old_fragmented_available > old_capacity \/ 8)) {\n+      ((ShenandoahOldHeuristics *) old_generation()->heuristics())->trigger_old_is_fragmented();\n+    }\n+\n+    size_t old_used = old_generation()->used() + old_generation()->get_humongous_waste();\n+    size_t trigger_threshold = old_generation()->usage_trigger_threshold();\n+    \/\/ Detects unsigned arithmetic underflow\n+    assert(old_used < ShenandoahHeap::heap()->capacity(), \"Old used must be less than heap capacity\");\n+\n+    if (old_used > trigger_threshold) {\n+      ((ShenandoahOldHeuristics *) old_generation()->heuristics())->trigger_old_has_grown();\n+    }\n@@ -3136,0 +3337,51 @@\n+bool ShenandoahHeap::verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                                             bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste) {\n+  size_t tally_old_regions = 0;\n+  size_t tally_old_bytes = 0;\n+  size_t tally_old_waste = 0;\n+  size_t tally_young_regions = 0;\n+  size_t tally_young_bytes = 0;\n+  size_t tally_young_waste = 0;\n+\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  for (size_t i = 0; i < num_regions(); i++) {\n+    ShenandoahHeapRegion* r = get_region(i);\n+    if (r->is_old()) {\n+      tally_old_regions++;\n+      tally_old_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_old_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    } else if (r->is_young()) {\n+      tally_young_regions++;\n+      tally_young_bytes += r->used();\n+      if (r->is_humongous()) {\n+        ShenandoahHeapRegion* start = r->humongous_start_region();\n+        HeapWord* obj_addr = start->bottom();\n+        oop obj = cast_to_oop(obj_addr);\n+        size_t word_size = obj->size();\n+        HeapWord* end_addr = obj_addr + word_size;\n+        if (end_addr <= r->end()) {\n+          tally_young_waste += (r->end() - end_addr) * HeapWordSize;\n+        }\n+      }\n+    }\n+  }\n+  if (verify_young &&\n+      ((young_regions != tally_young_regions) || (young_bytes != tally_young_bytes) || (young_waste != tally_young_waste))) {\n+    return false;\n+  } else if (verify_old &&\n+             ((old_regions != tally_old_regions) || (old_bytes != tally_old_bytes) || (old_waste != tally_old_waste))) {\n+    return false;\n+  } else {\n+    return true;\n+  }\n+}\n+\n@@ -3177,1 +3429,1 @@\n-          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr,\n+          ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, nullptr, nullptr,\n@@ -3193,1 +3445,1 @@\n-              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, obj_addr, nullptr,\n+              ShenandoahAsserts::print_failure(ShenandoahAsserts::_safe_all, obj, nullptr, nullptr,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":371,"deletions":119,"binary":false,"changes":490,"status":"modified"},{"patch":"@@ -212,0 +212,3 @@\n+  bool verify_generation_usage(bool verify_old, size_t old_regions, size_t old_bytes, size_t old_waste,\n+                               bool verify_young, size_t young_regions, size_t young_bytes, size_t young_waste);\n+\n@@ -222,0 +225,8 @@\n+           size_t _promotion_potential;\n+           size_t _promotion_in_place_potential;\n+           size_t _pad_for_promote_in_place;    \/\/ bytes of filler\n+           size_t _promotable_humongous_regions;\n+           size_t _promotable_humongous_usage;\n+           size_t _regular_regions_promoted_in_place;\n+           size_t _regular_usage_promoted_in_place;\n+\n@@ -266,0 +277,1 @@\n+  void mutator_threads_do(ThreadClosure* tcl) const;\n@@ -292,0 +304,2 @@\n+  inline ShenandoahMmuTracker* mmu_tracker() { return &_mmu_tracker; };\n+\n@@ -319,1 +333,4 @@\n-    OLD_MARKING_BITPOS = 5\n+    OLD_MARKING_BITPOS = 5,\n+\n+    \/\/ Old (mixed) evacations are enabled.\n+    MIXED_EVACUATIONS_ENABLED_BITPOS = 6\n@@ -329,1 +346,2 @@\n-    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS,\n+    MIXED_EVACUATIONS_ENABLED = 1 << MIXED_EVACUATIONS_ENABLED_BITPOS\n@@ -340,24 +358,1 @@\n-  \/\/ _alloc_supplement_reserve is a supplemental budget for new_memory allocations.  During evacuation and update-references,\n-  \/\/ mutator allocation requests are \"authorized\" iff young_gen->available() plus _alloc_supplement_reserve minus\n-  \/\/ _young_evac_reserve is greater than request size.  The values of _alloc_supplement_reserve and _young_evac_reserve\n-  \/\/ are zero except during evacuation and update-reference phases of GC.  Both of these values are established at\n-  \/\/ the start of evacuation, and they remain constant throughout the duration of these two phases of GC.  Since these\n-  \/\/ two values are constant throughout each GC phases, we introduce a new service into ShenandoahGeneration.  This service\n-  \/\/ provides adjusted_available() based on an adjusted capacity.  At the start of evacuation, we adjust young capacity by\n-  \/\/ adding the amount to be borrowed from old-gen and subtracting the _young_evac_reserve, we adjust old capacity by\n-  \/\/ subtracting the amount to be loaned to young-gen.\n-  \/\/\n-  \/\/ We always use adjusted capacities to determine permission to allocate within young and to promote into old.  Note\n-  \/\/ that adjusted capacities equal traditional capacities except during evacuation and update refs.\n-  \/\/\n-  \/\/ During evacuation, we assure that _old_evac_expended does not exceed _old_evac_reserve.\n-  \/\/\n-  \/\/ At the end of update references, we perform the following bookkeeping activities:\n-  \/\/\n-  \/\/ 1. Unadjust the capacity within young-gen and old-gen to undo the effects of borrowing memory from old-gen.  Note that\n-  \/\/    the entirety of the collection set is now available, so allocation capacity naturally increase at this time.\n-  \/\/ 2. Clear (reset to zero) _alloc_supplement_reserve, _young_evac_reserve, _old_evac_reserve, and _promoted_reserve\n-  \/\/\n-  \/\/ _young_evac_reserve and _old_evac_reserve are only non-zero during evacuation and update-references.\n-  \/\/\n-  \/\/ Allocation of old GCLABs assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n+  \/\/ Allocation of old GCLABs (aka PLABs) assures that _old_evac_expended + request-size < _old_evac_reserved.  If the allocation\n@@ -366,0 +361,2 @@\n+  \/\/ TODO: The following comment is not entirely accurate, I believe.  Maybe it is not at all accurate.\n+  \/\/\n@@ -372,1 +369,0 @@\n-  intptr_t _alloc_supplement_reserve;  \/\/ Bytes reserved for young allocations during evac and update refs\n@@ -390,2 +386,0 @@\n-\n-\n@@ -409,0 +403,1 @@\n+\n@@ -434,0 +429,21 @@\n+  inline void clear_promotion_potential() { _promotion_potential = 0; };\n+  inline void set_promotion_potential(size_t val) { _promotion_potential = val; };\n+  inline size_t get_promotion_potential() { return _promotion_potential; };\n+\n+  inline void clear_promotion_in_place_potential() { _promotion_in_place_potential = 0; };\n+  inline void set_promotion_in_place_potential(size_t val) { _promotion_in_place_potential = val; };\n+  inline size_t get_promotion_in_place_potential() { return _promotion_in_place_potential; };\n+\n+  inline void set_pad_for_promote_in_place(size_t pad) { _pad_for_promote_in_place = pad; }\n+  inline size_t get_pad_for_promote_in_place() { return _pad_for_promote_in_place; }\n+\n+  inline void reserve_promotable_humongous_regions(size_t region_count) { _promotable_humongous_regions = region_count; }\n+  inline void reserve_promotable_humongous_usage(size_t bytes) { _promotable_humongous_usage = bytes; }\n+  inline void reserve_promotable_regular_regions(size_t region_count) { _regular_regions_promoted_in_place = region_count; }\n+  inline void reserve_promotable_regular_usage(size_t used_bytes) { _regular_usage_promoted_in_place = used_bytes; }\n+\n+  inline size_t get_promotable_humongous_regions() { return _promotable_humongous_regions; }\n+  inline size_t get_promotable_humongous_usage() { return _promotable_humongous_usage; }\n+  inline size_t get_regular_regions_promoted_in_place() { return _regular_regions_promoted_in_place; }\n+  inline size_t get_regular_usage_promoted_in_place() { return _regular_usage_promoted_in_place; }\n+\n@@ -455,5 +471,0 @@\n-  \/\/ Returns previous value.  This is a signed value because it is the amount borrowed minus the amount reserved for\n-  \/\/ young-gen evacuation.  In case we cannot borrow much, this value might be negative.\n-  inline intptr_t set_alloc_supplement_reserve(intptr_t new_val);\n-  inline intptr_t get_alloc_supplement_reserve() const;\n-\n@@ -515,1 +526,0 @@\n-  void rebuild_free_set(bool concurrent);\n@@ -520,0 +530,1 @@\n+  void rebuild_free_set(bool concurrent);\n@@ -696,0 +707,4 @@\n+  \/\/ How many bytes to transfer between old and young after we have finished recycling collection set regions?\n+  size_t _old_regions_surplus;\n+  size_t _old_regions_deficit;\n+\n@@ -729,0 +744,6 @@\n+  inline void set_old_region_surplus(size_t surplus) { _old_regions_surplus = surplus; };\n+  inline void set_old_region_deficit(size_t deficit) { _old_regions_deficit = deficit; };\n+\n+  inline size_t get_old_region_surplus() { return _old_regions_surplus; };\n+  inline size_t get_old_region_deficit() { return _old_regions_deficit; };\n+\n@@ -828,1 +849,1 @@\n-  bool adjust_generation_sizes();\n+  void adjust_generation_sizes_for_next_cycle(size_t old_xfer_limit, size_t young_cset_regions, size_t old_cset_regions);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":57,"deletions":36,"binary":false,"changes":93,"status":"modified"},{"patch":"@@ -307,0 +307,1 @@\n+\n@@ -311,1 +312,1 @@\n-  } else if (is_promotion && (plab->words_remaining() > 0) && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n+  } else if (is_promotion && !ShenandoahThreadLocalData::allow_plab_promotions(thread)) {\n@@ -398,1 +399,0 @@\n-\n@@ -526,0 +526,1 @@\n+      assert(size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -803,10 +804,0 @@\n-inline intptr_t ShenandoahHeap::set_alloc_supplement_reserve(intptr_t new_val) {\n-  intptr_t orig = _alloc_supplement_reserve;\n-  _alloc_supplement_reserve = new_val;\n-  return orig;\n-}\n-\n-inline intptr_t ShenandoahHeap::get_alloc_supplement_reserve() const {\n-  return _alloc_supplement_reserve;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":3,"deletions":12,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n@@ -105,1 +106,1 @@\n-      set_affiliation(affiliation);\n+      set_affiliation(affiliation, false);\n@@ -125,1 +126,1 @@\n-     set_affiliation(YOUNG_GENERATION);\n+     set_affiliation(YOUNG_GENERATION, false);\n@@ -178,1 +179,1 @@\n-  set_affiliation(affiliation);\n+  set_affiliation(affiliation, false);\n@@ -209,1 +210,1 @@\n-  set_affiliation(affiliation);\n+  set_affiliation(affiliation, false);\n@@ -285,2 +286,0 @@\n-    case _cset:\n-      \/\/ Reclaiming cset regions\n@@ -289,2 +288,11 @@\n-      \/\/ Reclaiming humongous regions\n-    case _regular:\n+    {\n+      \/\/ Reclaiming humongous regions and reclaim humongous waste.  When this region is eventually recycled, we'll reclaim\n+      \/\/ its used memory.  At recycle time, we no longer recognize this as a humongous region.\n+      if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+        decrement_humongous_waste();\n+      }\n+    }\n+\n+  case _cset:\n+      \/\/ Reclaiming cset regions\n+  case _regular:\n@@ -480,0 +488,1 @@\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n@@ -525,0 +534,1 @@\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated object known to be larger than min_size\");\n@@ -576,0 +586,1 @@\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n@@ -577,1 +588,0 @@\n-\n@@ -670,1 +680,8 @@\n-  heap->generation_for(affiliation())->decrease_used(used());\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    \/\/ It may be that humongous regions are never recycled directly because they may be converted into trash before they\n+    \/\/ are recycled.  If this is universally true, we can replace the following with an assert(!is_humongous()).\n+    if (is_humongous()) {\n+      decrement_humongous_waste();\n+    }\n+    heap->generation_for(affiliation())->decrease_used(used());\n+  }\n@@ -681,1 +698,1 @@\n-  set_affiliation(FREE);\n+  set_affiliation(FREE, false);\n@@ -938,1 +955,2 @@\n-void ShenandoahHeapRegion::set_affiliation(ShenandoahRegionAffiliation new_affiliation) {\n+void ShenandoahHeapRegion::set_affiliation(ShenandoahRegionAffiliation new_affiliation,\n+                                           bool defer_affiliated_region_count_updates) {\n@@ -940,1 +958,0 @@\n-\n@@ -975,5 +992,6 @@\n-  if (region_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n-    heap->young_generation()->decrement_affiliated_region_count();\n-  } else if (region_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n-    heap->old_generation()->decrement_affiliated_region_count();\n-  }\n+  if (!defer_affiliated_region_count_updates) {\n+    if (region_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+      heap->young_generation()->decrement_affiliated_region_count();\n+    } else if (region_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+      heap->old_generation()->decrement_affiliated_region_count();\n+    }\n@@ -981,25 +999,28 @@\n-  size_t regions;\n-  switch (new_affiliation) {\n-    case FREE:\n-      assert(!has_live(), \"Free region should not have live data\");\n-      break;\n-    case YOUNG_GENERATION:\n-      reset_age();\n-      regions = heap->young_generation()->increment_affiliated_region_count();\n-      \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n-      \/\/ restored upon completion of Full GC.\n-      assert(heap->is_full_gc_in_progress() ||\n-             (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->young_generation()->adjusted_capacity()),\n-             \"Number of young regions cannot exceed adjusted capacity\");\n-      break;\n-    case OLD_GENERATION:\n-      regions = heap->old_generation()->increment_affiliated_region_count();\n-      \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n-      \/\/ restored upon completion of Full GC.\n-      assert(heap->is_full_gc_in_progress() ||\n-             (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->old_generation()->adjusted_capacity()),\n-             \"Number of old regions cannot exceed adjusted capacity\");\n-      break;\n-    default:\n-      ShouldNotReachHere();\n-      return;\n+    size_t regions;\n+    switch (new_affiliation) {\n+      case FREE:\n+        assert(!has_live(), \"Free region should not have live data\");\n+        break;\n+      case YOUNG_GENERATION:\n+        reset_age();\n+        regions = heap->young_generation()->increment_affiliated_region_count();\n+        \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n+        \/\/ restored upon completion of Full GC.\n+        assert(heap->is_full_gc_in_progress() ||\n+               (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->young_generation()->soft_max_capacity()),\n+               \"Number of young regions cannot exceed adjusted capacity\");\n+        break;\n+      case OLD_GENERATION:\n+        regions = heap->old_generation()->increment_affiliated_region_count();\n+        \/\/ During Full GC, we allow temporary violation of this requirement.  We enforce that this condition is\n+        \/\/ restored upon completion of Full GC.\n+        assert(heap->is_full_gc_in_progress() ||\n+               (regions * ShenandoahHeapRegion::region_size_bytes() <= heap->old_generation()->soft_max_capacity()),\n+               \"Number of old regions cannot exceed adjusted capacity\");\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+        return;\n+    }\n+  } else if (new_affiliation == YOUNG_GENERATION) {\n+    reset_age();\n@@ -1010,2 +1031,89 @@\n-\/\/ Returns number of regions promoted, or zero if we choose not to promote.\n-size_t ShenandoahHeapRegion::promote_humongous() {\n+\/\/ When we promote a region in place, we can continue to use the established marking context to guide subsequent remembered\n+\/\/ set scans of this region's content.  The region will be coalesced and filled prior to the next old-gen marking effort.\n+\/\/ We identify the entirety of the region as DIRTY to force the next remembered set scan to identify the \"interesting poitners\"\n+\/\/ contained herein.\n+void ShenandoahHeapRegion::promote_in_place() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  HeapWord* tams = marking_context->top_at_mark_start(this);\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+  assert(is_young(), \"Only young regions can be promoted\");\n+  assert(is_regular(), \"Use different service to promote humongous regions\");\n+  assert(age() >= InitialTenuringThreshold, \"Only promote regions that are sufficiently aged\");\n+\n+  ShenandoahOldGeneration* old_gen = heap->old_generation();\n+  ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+\n+    set_affiliation(OLD_GENERATION, true);\n+\n+    HeapWord* update_watermark = get_update_watermark();\n+\n+    \/\/ Now that this region is affiliated with old, we can allow it to receive allocations, though it may not be in the\n+    \/\/ is_collector_free range.\n+    restore_top_before_promote();\n+\n+    \/\/ The update_watermark was likely established while we had the artificially high value of top.  Make it sane now.\n+    assert(update_watermark >= top(), \"original top cannot exceed preserved update_watermark\");\n+    set_update_watermark(top());\n+\n+    size_t promoted_used = this->used();\n+    size_t promoted_free = this->free();\n+    size_t promo_reserve = heap->get_promoted_reserve() + promoted_free;\n+    young_gen->decrease_used(promoted_used);\n+    young_gen->decrement_affiliated_region_count();\n+\n+    \/\/ Unconditionally transfer one region from young to old to represent the newly promoted region.\n+    \/\/ This expands old and shrinks new by the size of one region.  Strictly, we do not \"need\" to expand old\n+    \/\/ if there are already enough unaffiliated regions in old to account for this newly promoted region.\n+    \/\/ However, if we do not transfer the capacities, we end up reducing the amount of memory that would have\n+    \/\/ otherwise been available to hold old evacuations, because old available is max_capacity - used and now\n+    \/\/ we would be trading a fully empty region for a partially used region.\n+    \/\/\n+    \/\/ Note that we will rebalance the generation sizes at the end of this GC cycle.\n+    heap->generation_sizer()->force_transfer_to_old(1);\n+\n+    heap->free_set()->add_old_collector_free_region(this);\n+\n+    old_gen->increment_affiliated_region_count();\n+    old_gen->increase_used(promoted_used);\n+\n+    \/\/ Might as well make the free memory within newly promoted region available to hold promotions that we were not able\n+    \/\/ to budget for previously.\n+    heap->set_promoted_reserve(promo_reserve);\n+\n+    \/\/ TODO: adjust bounds in the free set\n+  }\n+\n+  assert(top() == tams, \"Cannot promote regions in place if top has advanced beyond TAMS\");\n+\n+  \/\/ Since this region may have served previously as OLD, it may hold obsolete object range info.\n+  heap->card_scan()->reset_object_range(bottom(), end());\n+  heap->card_scan()->mark_range_as_dirty(bottom(), top() - bottom());\n+\n+  HeapWord* obj_addr = bottom();\n+  while (obj_addr < tams) {\n+    oop obj = cast_to_oop(obj_addr);\n+    if (marking_context->is_marked(obj)) {\n+      assert(obj->klass() != NULL, \"klass should not be NULL\");\n+      \/\/ This thread is responsible for registering all objects in this region.  No need for lock.\n+      heap->card_scan()->register_object_wo_lock(obj_addr);\n+      obj_addr += obj->size();\n+    } else {\n+      HeapWord* next_marked_obj = marking_context->get_next_marked_addr(obj_addr, tams);\n+      assert(next_marked_obj <= tams, \"next marked object cannot exceed tams\");\n+      size_t fill_size = next_marked_obj - obj_addr;\n+      assert(fill_size >= ShenandoahHeap::min_fill_size(), \"previously allocated objects known to be larger than min_size\");\n+      ShenandoahHeap::fill_with_object(obj_addr, fill_size);\n+      heap->card_scan()->register_object_wo_lock(obj_addr);\n+      obj_addr = next_marked_obj;\n+    }\n+  }\n+\n+  \/\/ We do not need to scan above TAMS because top equals tams\n+  assert(obj_addr == tams, \"Expect loop to terminate when obj_addr equals tams\");\n+}\n+\n+void ShenandoahHeapRegion::promote_humongous() {\n@@ -1031,1 +1139,0 @@\n-\n@@ -1034,1 +1141,0 @@\n-\n@@ -1039,29 +1145,14 @@\n-    size_t available_old_regions = old_generation->adjusted_unaffiliated_regions();\n-    if (spanned_regions <= available_old_regions) {\n-      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, index(), spanned_regions);\n-\n-      \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n-      \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n-      \/\/ in the last humongous region that is not spanned by obj is currently not used.\n-      for (size_t i = index(); i < index_limit; i++) {\n-        ShenandoahHeapRegion* r = heap->get_region(i);\n-        log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n-                      r->index(), p2i(r->bottom()), p2i(r->top()));\n-        \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n-        r->set_affiliation(OLD_GENERATION);\n-        old_generation->increase_used(r->used());\n-        young_generation->decrease_used(r->used());\n-      }\n-      \/\/ Then fall through to finish the promotion after releasing the heap lock.\n-    } else {\n-      \/\/ There are not enough available old regions to promote this humongous region at this time, so defer promotion.\n-      \/\/ TODO: Consider allowing the promotion now, with the expectation that we can resize and\/or collect OLD\n-      \/\/ momentarily to address the transient violation of budgets.  Some problems that need to be addressed in order\n-      \/\/ to allow transient violation of capacity budgets are:\n-      \/\/  1. Various size_t subtractions assume usage is less than capacity, and thus assume there will be no\n-      \/\/     arithmetic underflow when we subtract usage from capacity.  The results of such size_t subtractions\n-      \/\/     would need to be guarded and special handling provided.\n-      \/\/  2. ShenandoahVerifier enforces that usage is less than capacity.  If we are going to relax this constraint,\n-      \/\/     we need to think about what conditions allow the constraint to be violated and document and implement the\n-      \/\/     changes.\n-      return 0;\n+\n+    \/\/ We promote humongous objects unconditionally, without checking for availability.  We adjust\n+    \/\/ usage totals after evacuation is done.\n+    log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", spanning \" SIZE_FORMAT, index(), spanned_regions);\n+\n+    \/\/ For this region and each humongous continuation region spanned by this humongous object, change\n+    \/\/ affiliation to OLD_GENERATION and adjust the generation-use tallies.  The remnant of memory\n+    \/\/ in the last humongous region that is not spanned by obj is currently not used.\n+    for (size_t i = index(); i < index_limit; i++) {\n+      ShenandoahHeapRegion* r = heap->get_region(i);\n+      log_debug(gc)(\"promoting humongous region \" SIZE_FORMAT \", from \" PTR_FORMAT \" to \" PTR_FORMAT,\n+                    r->index(), p2i(r->bottom()), p2i(r->top()));\n+      \/\/ We mark the entire humongous object's range as dirty after loop terminates, so no need to dirty the range here\n+      r->set_affiliation(OLD_GENERATION, true);\n@@ -1086,1 +1177,17 @@\n-  return index_limit - index();\n+}\n+\n+void ShenandoahHeapRegion::decrement_humongous_waste() const {\n+  ShenandoahHeapRegion* start = humongous_start_region();\n+  HeapWord* obj_addr = start->bottom();\n+  oop obj = cast_to_oop(obj_addr);\n+  size_t word_size = obj->size();\n+  HeapWord* end_addr = obj_addr + word_size;\n+  if (end_addr < end()) {\n+    size_t humongous_waste = (end() - end_addr) * HeapWordSize;\n+    if (is_old()) {\n+      ShenandoahHeap::heap()->old_generation()->decrease_humongous_waste(humongous_waste);\n+    } else {\n+      ShenandoahHeap::heap()->young_generation()->decrease_humongous_waste(humongous_waste);\n+    }\n+  }\n+  \/\/ else, this region is entirely spanned by humongous object so contributes no humongous waste\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":184,"deletions":77,"binary":false,"changes":261,"status":"modified"},{"patch":"@@ -243,0 +243,2 @@\n+  HeapWord* _top_before_promoted;\n+\n@@ -351,0 +353,5 @@\n+  inline void save_top_before_promote();\n+  inline HeapWord* get_top_before_promote() const { return _top_before_promoted; }\n+  inline void restore_top_before_promote();\n+  inline size_t garbage_before_padded_for_promote() const;\n+\n@@ -430,0 +437,1 @@\n+  size_t used_before_promote() const { return byte_size(bottom(), get_top_before_promote()); }\n@@ -450,1 +458,1 @@\n-  void set_affiliation(ShenandoahRegionAffiliation new_affiliation);\n+  void set_affiliation(ShenandoahRegionAffiliation new_affiliation, bool defer_affiliated_region_count_updates);\n@@ -457,2 +465,3 @@\n-  \/\/ Sets all remembered set cards to dirty.  Returns the number of regions spanned by the associated humongous object.\n-  size_t promote_humongous();\n+  \/\/ Register all objects.  Set all remembered set cards to dirty.\n+  void promote_humongous();\n+  void promote_in_place();\n@@ -461,0 +470,1 @@\n+  void decrement_humongous_waste() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":13,"deletions":3,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+      assert(pad_words >= ShenandoahHeap::min_fill_size(), \"pad_words expanded above to meet size constraint\");\n@@ -192,0 +193,11 @@\n+inline size_t ShenandoahHeapRegion::garbage_before_padded_for_promote() const {\n+  size_t used_before_promote = byte_size(bottom(), get_top_before_promote());\n+  assert(get_top_before_promote() != nullptr, \"top before promote should not equal null\");\n+  assert(used_before_promote >= get_live_data_bytes(),\n+         \"Live Data must be a subset of used before promotion live: \" SIZE_FORMAT \" used: \" SIZE_FORMAT,\n+         get_live_data_bytes(), used_before_promote);\n+  size_t result = used_before_promote - get_live_data_bytes();\n+  return result;\n+\n+}\n+\n@@ -234,0 +246,13 @@\n+inline void ShenandoahHeapRegion::save_top_before_promote() {\n+  _top_before_promoted = _top;\n+}\n+\n+inline void ShenandoahHeapRegion::restore_top_before_promote() {\n+  _top = _top_before_promoted;\n+#ifdef ASSERT\n+  _top_before_promoted = nullptr;\n+#endif\n+ }\n+\n+\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-\n@@ -56,1 +55,18 @@\n-double ShenandoahMmuTracker::gc_thread_time_seconds() {\n+ShenandoahMmuTracker::ShenandoahMmuTracker() :\n+    _most_recent_timestamp(0.0),\n+    _most_recent_gc_time(0.0),\n+    _most_recent_gcu(0.0),\n+    _most_recent_mutator_time(0.0),\n+    _most_recent_mu(0.0),\n+    _most_recent_periodic_time_stamp(0.0),\n+    _most_recent_periodic_gc_time(0.0),\n+    _most_recent_periodic_mutator_time(0.0),\n+    _mmu_periodic_task(new ShenandoahMmuTask(this)) {\n+}\n+\n+ShenandoahMmuTracker::~ShenandoahMmuTracker() {\n+  _mmu_periodic_task->disenroll();\n+  delete _mmu_periodic_task;\n+}\n+\n+void ShenandoahMmuTracker::fetch_cpu_times(double &gc_time, double &mutator_time) {\n@@ -61,1 +77,7 @@\n-  return double(cl.total_time) \/ NANOSECS_PER_SEC;\n+  double most_recent_gc_thread_time = double(cl.total_time) \/ NANOSECS_PER_SEC;\n+  gc_time = most_recent_gc_thread_time;\n+\n+  double process_real_time(0.0), process_user_time(0.0), process_system_time(0.0);\n+  bool valid = os::getTimesSecs(&process_real_time, &process_user_time, &process_system_time);\n+  assert(valid, \"don't know why this would not be valid\");\n+  mutator_time =(process_user_time + process_system_time) - most_recent_gc_thread_time;\n@@ -73,6 +95,24 @@\n-ShenandoahMmuTracker::ShenandoahMmuTracker() :\n-    _generational_reference_time_s(0.0),\n-    _process_reference_time_s(0.0),\n-    _collector_reference_time_s(0.0),\n-    _mmu_periodic_task(new ShenandoahMmuTask(this)),\n-    _mmu_average(10, ShenandoahAdaptiveDecayFactor) {\n+void ShenandoahMmuTracker::help_record_concurrent(ShenandoahGeneration* generation, uint gcid, const char *msg) {\n+  double current = os::elapsedTime();\n+  _most_recent_gcid = gcid;\n+  _most_recent_is_full = false;\n+\n+  if (gcid == 0) {\n+    fetch_cpu_times(_most_recent_gc_time, _most_recent_mutator_time);\n+\n+    _most_recent_timestamp = current;\n+  } else {\n+    double gc_cycle_duration = current - _most_recent_timestamp;\n+    _most_recent_timestamp = current;\n+\n+    double gc_thread_time, mutator_thread_time;\n+    fetch_cpu_times(gc_thread_time, mutator_thread_time);\n+    double gc_time = gc_thread_time - _most_recent_gc_time;\n+    _most_recent_gc_time = gc_thread_time;\n+    _most_recent_gcu = gc_time \/ (_active_processors * gc_cycle_duration);\n+    double mutator_time = mutator_thread_time - _most_recent_mutator_time;\n+    _most_recent_mutator_time = mutator_thread_time;\n+    _most_recent_mu = mutator_time \/ (_active_processors * gc_cycle_duration);\n+    log_info(gc, ergo)(\"At end of %s: GCU: %.1f%%, MU: %.1f%% for duration %.3fs\",\n+                       msg, _most_recent_gcu * 100, _most_recent_mu * 100, gc_cycle_duration);\n+  }\n@@ -81,3 +121,2 @@\n-ShenandoahMmuTracker::~ShenandoahMmuTracker() {\n-  _mmu_periodic_task->disenroll();\n-  delete _mmu_periodic_task;\n+void ShenandoahMmuTracker::record_young(ShenandoahGeneration* generation, uint gcid) {\n+  help_record_concurrent(generation, gcid, \"Concurrent Young GC\");\n@@ -86,6 +125,47 @@\n-void ShenandoahMmuTracker::record(ShenandoahGeneration* generation) {\n-  shenandoah_assert_control_or_vm_thread();\n-  double collector_time_s = gc_thread_time_seconds();\n-  double elapsed_gc_time_s = collector_time_s - _generational_reference_time_s;\n-  generation->add_collection_time(elapsed_gc_time_s);\n-  _generational_reference_time_s = collector_time_s;\n+void ShenandoahMmuTracker::record_bootstrap(ShenandoahGeneration* generation, uint gcid, bool candidates_for_mixed) {\n+  \/\/ Not likely that this will represent an \"ideal\" GCU, but doesn't hurt to try\n+  help_record_concurrent(generation, gcid, \"Bootstrap Old GC\");\n+  if (candidates_for_mixed) {\n+    _doing_mixed_evacuations = true;\n+  }\n+  \/\/ Else, there are no candidates for mixed evacuations, so we are not going to do mixed evacuations.\n+}\n+\n+void ShenandoahMmuTracker::record_old_marking_increment(ShenandoahGeneration* generation, uint gcid, bool old_marking_done,\n+                                                        bool has_old_candidates) {\n+  \/\/ No special processing for old marking\n+  double duration = os::elapsedTime() - _most_recent_timestamp;\n+  double gc_time, mutator_time;\n+  fetch_cpu_times(gc_time, mutator_time);\n+  double gcu = (gc_time - _most_recent_gc_time) \/ duration;\n+  double mu = (mutator_time - _most_recent_mutator_time) \/ duration;\n+  if (has_old_candidates) {\n+    _doing_mixed_evacuations = true;\n+  }\n+  log_info(gc, ergo)(\"At end of %s: GC Utilization: %.1f%% for duration %.3fs (which is subsumed in next concurrent gc report)\",\n+                     old_marking_done? \"last OLD marking increment\": \"OLD marking increment\",\n+                     _most_recent_gcu * 100, duration);\n+}\n+\n+void ShenandoahMmuTracker::record_mixed(ShenandoahGeneration* generation, uint gcid, bool is_mixed_done) {\n+  help_record_concurrent(generation, gcid, \"Mixed Concurrent GC\");\n+}\n+\n+void ShenandoahMmuTracker::record_degenerated(ShenandoahGeneration* generation,\n+                                              uint gcid, bool is_old_bootstrap, bool is_mixed_done) {\n+  if ((gcid == _most_recent_gcid) && _most_recent_is_full) {\n+    \/\/ Do nothing.  This is a redundant recording for the full gc that just completed.\n+  } else if (is_old_bootstrap) {\n+    help_record_concurrent(generation, gcid, \"Degenerated Bootstrap Old GC\");\n+    if (!is_mixed_done) {\n+      _doing_mixed_evacuations = true;\n+    }\n+  } else {\n+    help_record_concurrent(generation, gcid, \"Degenerated Young GC\");\n+  }\n+}\n+\n+void ShenandoahMmuTracker::record_full(ShenandoahGeneration* generation, uint gcid) {\n+  help_record_concurrent(generation, gcid, \"Full GC\");\n+  _most_recent_is_full = true;\n+  _doing_mixed_evacuations = false;\n@@ -96,6 +176,6 @@\n-  double process_time_s = process_time_seconds();\n-  double elapsed_process_time_s = process_time_s - _process_reference_time_s;\n-  if (elapsed_process_time_s <= 0.01) {\n-    \/\/ No cpu time for this interval?\n-    return;\n-  }\n+  double current = os::elapsedTime();\n+  double time_delta = current - _most_recent_periodic_time_stamp;\n+  _most_recent_periodic_time_stamp = current;\n+\n+  double gc_time, mutator_time;\n+  fetch_cpu_times(gc_time, mutator_time);\n@@ -103,7 +183,9 @@\n-  _process_reference_time_s = process_time_s;\n-  double collector_time_s = gc_thread_time_seconds();\n-  double elapsed_collector_time_s = collector_time_s - _collector_reference_time_s;\n-  _collector_reference_time_s = collector_time_s;\n-  double minimum_mutator_utilization = ((elapsed_process_time_s - elapsed_collector_time_s) \/ elapsed_process_time_s) * 100;\n-  _mmu_average.add(minimum_mutator_utilization);\n-  log_info(gc)(\"Average MMU = %.3f\", _mmu_average.davg());\n+  double gc_delta = gc_time - _most_recent_periodic_gc_time;\n+  _most_recent_periodic_gc_time = gc_time;\n+\n+  double mutator_delta = mutator_time - _most_recent_periodic_mutator_time;\n+  _most_recent_periodic_mutator_time = mutator_time;\n+\n+  double mu = mutator_delta \/ (_active_processors * time_delta);\n+  double gcu = gc_delta \/ (_active_processors * time_delta);\n+  log_info(gc)(\"Periodic Sample: Average GCU = %.3f%%, Average MU = %.3f%%\", gcu * 100, mu * 100);\n@@ -113,3 +195,5 @@\n-  _process_reference_time_s = process_time_seconds();\n-  _generational_reference_time_s = gc_thread_time_seconds();\n-  _collector_reference_time_s = _generational_reference_time_s;\n+  \/\/ initialize static data\n+  _active_processors = os::initial_active_processor_count();\n+\n+  double _most_recent_periodic_time_stamp = os::elapsedTime();\n+  fetch_cpu_times(_most_recent_periodic_gc_time, _most_recent_periodic_mutator_time);\n@@ -163,1 +247,1 @@\n-  return MAX2(uint(min_young_regions), 1U);\n+  return MAX2(min_young_regions, (size_t) 1U);\n@@ -168,1 +252,1 @@\n-  return MAX2(uint(max_young_regions), 1U);\n+  return MAX2(max_young_regions, (size_t) 1U);\n@@ -201,0 +285,1 @@\n+\/\/ TODO: this is not hooked anywhere.\n@@ -205,10 +290,2 @@\n-bool ShenandoahGenerationSizer::adjust_generation_sizes() const {\n-  shenandoah_assert_generational();\n-  if (!use_adaptive_sizing()) {\n-    return false;\n-  }\n-\n-  if (_mmu_tracker->average() >= double(GCTimeRatio)) {\n-    return false;\n-  }\n-\n+\/\/ Returns true iff transfer is successful\n+bool ShenandoahGenerationSizer::transfer_to_old(size_t regions) const {\n@@ -216,6 +293,4 @@\n-  ShenandoahOldGeneration *old = heap->old_generation();\n-  ShenandoahYoungGeneration *young = heap->young_generation();\n-  ShenandoahGeneration *global = heap->global_generation();\n-  double old_time_s = old->reset_collection_time();\n-  double young_time_s = young->reset_collection_time();\n-  double global_time_s = global->reset_collection_time();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t bytes_to_transfer = regions * region_size_bytes;\n@@ -223,7 +298,5 @@\n-  const double transfer_threshold = 3.0;\n-  double delta = young_time_s - old_time_s;\n-\n-  log_info(gc)(\"Thread Usr+Sys YOUNG = %.3f, OLD = %.3f, GLOBAL = %.3f\", young_time_s, old_time_s, global_time_s);\n-\n-  if (abs(delta) <= transfer_threshold) {\n-    log_info(gc, ergo)(\"Difference (%.3f) for thread utilization for each generation is under threshold (%.3f)\", abs(delta), transfer_threshold);\n+  if (young_gen->free_unaffiliated_regions() < regions) {\n+    return false;\n+  } else if (old_gen->max_capacity() + bytes_to_transfer > heap->max_size_for(old_gen)) {\n+    return false;\n+  } else if (young_gen->max_capacity() - bytes_to_transfer < heap->min_size_for(young_gen)) {\n@@ -231,5 +304,0 @@\n-  }\n-\n-  if (delta > 0) {\n-    \/\/ young is busier than old, increase size of young to raise MMU\n-    return transfer_capacity(old, young);\n@@ -237,2 +305,7 @@\n-    \/\/ old is busier than young, increase size of old to raise MMU\n-    return transfer_capacity(young, old);\n+    young_gen->decrease_capacity(bytes_to_transfer);\n+    old_gen->increase_capacity(bytes_to_transfer);\n+    size_t new_size = old_gen->max_capacity();\n+    log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" SIZE_FORMAT \"%s\",\n+                 regions, young_gen->name(), old_gen->name(),\n+                 byte_size_in_proper_unit(new_size), proper_unit_for_byte_size(new_size));\n+    return true;\n@@ -242,8 +315,15 @@\n-bool ShenandoahGenerationSizer::transfer_capacity(ShenandoahGeneration* target) const {\n-  ShenandoahHeapLocker locker(ShenandoahHeap::heap()->lock());\n-  if (target->is_young()) {\n-    return transfer_capacity(ShenandoahHeap::heap()->old_generation(), target);\n-  } else {\n-    assert(target->is_old(), \"Expected old generation, if not young.\");\n-    return transfer_capacity(ShenandoahHeap::heap()->young_generation(), target);\n-  }\n+\/\/ This is used when promoting humongous or highly utilized regular regions in place.  It is not required in this situation\n+\/\/ that the transferred regions be unaffiliated.  In fact, we are transferring regions that already have high utilization.\n+void ShenandoahGenerationSizer::force_transfer_to_old(size_t regions) const {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t bytes_to_transfer = regions * region_size_bytes;\n+\n+  young_gen->decrease_capacity(bytes_to_transfer);\n+  old_gen->increase_capacity(bytes_to_transfer);\n+  size_t new_size = old_gen->max_capacity();\n+  log_info(gc)(\"Forcing transfer of \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" SIZE_FORMAT \"%s\",\n+               regions, young_gen->name(), old_gen->name(),\n+               byte_size_in_proper_unit(new_size), proper_unit_for_byte_size(new_size));\n@@ -252,2 +332,0 @@\n-bool ShenandoahGenerationSizer::transfer_capacity(ShenandoahGeneration* from, ShenandoahGeneration* to) const {\n-  shenandoah_assert_heaplocked_or_safepoint();\n@@ -255,12 +333,6 @@\n-  size_t available_regions = from->free_unaffiliated_regions();\n-  if (available_regions <= 0) {\n-    log_info(gc)(\"%s has no regions available for transfer to %s\", from->name(), to->name());\n-    return false;\n-  }\n-\n-  size_t regions_to_transfer = MAX2(1u, uint(double(available_regions) * _resize_increment));\n-  if (from->generation_mode() == YOUNG) {\n-    regions_to_transfer = adjust_transfer_from_young(from, regions_to_transfer);\n-  } else {\n-    regions_to_transfer = adjust_transfer_to_young(to, regions_to_transfer);\n-  }\n+bool ShenandoahGenerationSizer::transfer_to_young(size_t regions) const {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  ShenandoahGeneration* young_gen = heap->young_generation();\n+  size_t region_size_bytes = ShenandoahHeapRegion::region_size_bytes();\n+  size_t bytes_to_transfer = regions * region_size_bytes;\n@@ -268,4 +340,1 @@\n-  if (regions_to_transfer == 0) {\n-    log_info(gc)(\"No capacity available to transfer from: %s (\" SIZE_FORMAT \"%s) to: %s (\" SIZE_FORMAT \"%s)\",\n-                  from->name(), byte_size_in_proper_unit(from->max_capacity()), proper_unit_for_byte_size(from->max_capacity()),\n-                  to->name(), byte_size_in_proper_unit(to->max_capacity()), proper_unit_for_byte_size(to->max_capacity()));\n+  if (old_gen->free_unaffiliated_regions() < regions) {\n@@ -273,0 +342,12 @@\n+  } else if (young_gen->max_capacity() + bytes_to_transfer > heap->max_size_for(young_gen)) {\n+    return false;\n+  } else if (old_gen->max_capacity() - bytes_to_transfer < heap->min_size_for(old_gen)) {\n+    return false;\n+  } else {\n+    old_gen->decrease_capacity(bytes_to_transfer);\n+    young_gen->increase_capacity(bytes_to_transfer);\n+    size_t new_size = young_gen->max_capacity();\n+    log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s, yielding increased size: \" SIZE_FORMAT \"%s\",\n+                 regions, old_gen->name(), young_gen->name(),\n+                 byte_size_in_proper_unit(new_size), proper_unit_for_byte_size(new_size));\n+    return true;\n@@ -274,35 +355,0 @@\n-\n-  log_info(gc)(\"Transfer \" SIZE_FORMAT \" region(s) from %s to %s\", regions_to_transfer, from->name(), to->name());\n-  from->decrease_capacity(regions_to_transfer * ShenandoahHeapRegion::region_size_bytes());\n-  to->increase_capacity(regions_to_transfer * ShenandoahHeapRegion::region_size_bytes());\n-  return true;\n-}\n-\n-size_t ShenandoahGenerationSizer::adjust_transfer_from_young(ShenandoahGeneration* from, size_t regions_to_transfer) const {\n-  assert(from->generation_mode() == YOUNG, \"Expect to transfer from young\");\n-  size_t young_capacity_regions = from->max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n-  size_t new_young_regions = young_capacity_regions - regions_to_transfer;\n-  size_t minimum_young_regions = min_young_regions();\n-  \/\/ Check that we are not going to violate the minimum size constraint.\n-  if (new_young_regions < minimum_young_regions) {\n-    assert(minimum_young_regions <= young_capacity_regions, \"Young is under minimum capacity.\");\n-    \/\/ If the transfer violates the minimum size and there is still some capacity to transfer,\n-    \/\/ adjust the transfer to take the size to the minimum. Note that this may be zero.\n-    regions_to_transfer = young_capacity_regions - minimum_young_regions;\n-  }\n-  return regions_to_transfer;\n-}\n-\n-size_t ShenandoahGenerationSizer::adjust_transfer_to_young(ShenandoahGeneration* to, size_t regions_to_transfer) const {\n-  assert(to->generation_mode() == YOUNG, \"Can only transfer between young and old.\");\n-  size_t young_capacity_regions = to->max_capacity() \/ ShenandoahHeapRegion::region_size_bytes();\n-  size_t new_young_regions = young_capacity_regions + regions_to_transfer;\n-  size_t maximum_young_regions = max_young_regions();\n-  \/\/ Check that we are not going to violate the maximum size constraint.\n-  if (new_young_regions > maximum_young_regions) {\n-    assert(maximum_young_regions >= young_capacity_regions, \"Young is over maximum capacity\");\n-    \/\/ If the transfer violates the maximum size and there is still some capacity to transfer,\n-    \/\/ adjust the transfer to take the size to the maximum. Note that this may be zero.\n-    regions_to_transfer = maximum_young_regions - young_capacity_regions;\n-  }\n-  return regions_to_transfer;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.cpp","additions":173,"deletions":127,"binary":false,"changes":300,"status":"modified"},{"patch":"@@ -53,0 +53,12 @@\n+private:\n+  \/\/ For reporting utilization during most recent GC cycle\n+  double _most_recent_timestamp;\n+  double _most_recent_gc_time;\n+  double _most_recent_gcu;\n+  double _most_recent_mutator_time;\n+  double _most_recent_mu;\n+\n+  \/\/ For periodic MU\/GCU reports\n+  double _most_recent_periodic_time_stamp;\n+  double _most_recent_periodic_gc_time;\n+  double _most_recent_periodic_mutator_time;\n@@ -54,3 +66,5 @@\n-  double _generational_reference_time_s;\n-  double _process_reference_time_s;\n-  double _collector_reference_time_s;\n+  uint _most_recent_gcid;\n+  uint _active_processors;\n+\n+  bool _most_recent_is_full;\n+  bool _doing_mixed_evacuations;\n@@ -61,1 +75,1 @@\n-  static double gc_thread_time_seconds();\n+  void help_record_concurrent(ShenandoahGeneration* generation, uint gcid, const char* msg);\n@@ -63,0 +77,1 @@\n+  static void fetch_cpu_times(double &gc_time, double &mutator_time);\n@@ -71,6 +86,11 @@\n-  \/\/ This is called at the start and end of a GC cycle. The GC thread times\n-  \/\/ will be accumulated in this generation. Note that the bootstrap cycle\n-  \/\/ for an old collection should be counted against the old generation.\n-  \/\/ When the collector is idle, it still runs a regulator and a control.\n-  \/\/ The times for these threads are attributed to the global generation.\n-  void record(ShenandoahGeneration* generation);\n+  \/\/ At completion of each GC cycle (not including interrupted cycles), we invoke one of the following to record the\n+  \/\/ GC utilization during this cycle.\n+  \/\/\n+  \/\/ We may redundantly record degen and full, in which case the gcid will repeat.  We log these as FULL.\n+  \/\/ Full gets reported first.\n+  void record_young(ShenandoahGeneration* generation, uint gcid);\n+  void record_bootstrap(ShenandoahGeneration* generation, uint gcid, bool has_old_candidates);\n+  void record_old_marking_increment(ShenandoahGeneration* generation, uint gcid, bool old_marking_done, bool has_old_candidates);\n+  void record_mixed(ShenandoahGeneration* generation, uint gcid, bool is_mixed_done);\n+  void record_full(ShenandoahGeneration* generation, uint gcid);\n+  void record_degenerated(ShenandoahGeneration* generation, uint gcid, bool is_old_boostrap, bool is_mixed_done);\n@@ -81,1 +101,0 @@\n-  \/\/ This method also logs the average MMU.\n@@ -83,4 +102,0 @@\n-\n-  double average() {\n-    return _mmu_average.davg();\n-  }\n@@ -117,8 +132,0 @@\n-  \/\/ These two methods are responsible for enforcing the minimum and maximum\n-  \/\/ constraints for the size of the generations.\n-  size_t adjust_transfer_from_young(ShenandoahGeneration* from, size_t regions_to_transfer) const;\n-  size_t adjust_transfer_to_young(ShenandoahGeneration* to, size_t regions_to_transfer) const;\n-\n-  \/\/ This will attempt to transfer capacity from one generation to the other. It\n-  \/\/ returns true if a transfer is made, false otherwise.\n-  bool transfer_capacity(ShenandoahGeneration* from, ShenandoahGeneration* to) const;\n@@ -148,13 +155,5 @@\n-  \/\/ This is invoked at the end of a collection. This happens on a safepoint\n-  \/\/ to avoid any races with allocators (and to avoid interfering with\n-  \/\/ allocators by taking the heap lock). The amount of capacity to move\n-  \/\/ from one generation to another is controlled by YoungGenerationSizeIncrement\n-  \/\/ and defaults to 20% of the available capacity of the donor generation.\n-  \/\/ The minimum and maximum sizes of the young generation are controlled by\n-  \/\/ ShenandoahMinYoungPercentage and ShenandoahMaxYoungPercentage, respectively.\n-  \/\/ The method returns true when an adjustment is made, false otherwise.\n-  bool adjust_generation_sizes() const;\n-\n-  \/\/ This may be invoked by a heuristic (from regulator thread) before it\n-  \/\/ decides to run a collection.\n-  bool transfer_capacity(ShenandoahGeneration* target) const;\n+  bool transfer_to_young(size_t regions) const;\n+  bool transfer_to_old(size_t regions) const;\n+\n+  \/\/ force transfer is used when we promote humongous objects.  May violate min\/max limits on generation sizes\n+  void force_transfer_to_old(size_t regions) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMmuTracker.hpp","additions":35,"deletions":36,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -149,0 +150,43 @@\n+  \/\/ We do not rebuild_free following increments of old marking because memory has not been reclaimed..  However, we may\n+  \/\/ need to transfer memory to OLD in order to efficiently support the mixed evacuations that might immediately follow.\n+  size_t evac_slack = heap->young_generation()->heuristics()->evac_slack(0);\n+  heap->adjust_generation_sizes_for_next_cycle(evac_slack, 0, 0);\n+\n+  bool success;\n+  size_t region_xfer;\n+  const char* region_destination;\n+  ShenandoahYoungGeneration* young_gen = heap->young_generation();\n+  ShenandoahGeneration* old_gen = heap->old_generation();\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+\n+    size_t old_region_surplus = heap->get_old_region_surplus();\n+    size_t old_region_deficit = heap->get_old_region_deficit();\n+    if (old_region_surplus) {\n+      success = heap->generation_sizer()->transfer_to_young(old_region_surplus);\n+      region_destination = \"young\";\n+      region_xfer = old_region_surplus;\n+    } else if (old_region_deficit) {\n+      success = heap->generation_sizer()->transfer_to_old(old_region_deficit);\n+      region_destination = \"old\";\n+      region_xfer = old_region_deficit;\n+      if (!success) {\n+        ((ShenandoahOldHeuristics *) old_gen->heuristics())->trigger_cannot_expand();\n+      }\n+    } else {\n+      region_destination = \"none\";\n+      region_xfer = 0;\n+      success = true;\n+    }\n+    heap->set_old_region_surplus(0);\n+    heap->set_old_region_deficit(0);\n+  }\n+\n+  \/\/ Report outside the heap lock\n+  size_t young_available = young_gen->available();\n+  size_t old_available = old_gen->available();\n+  log_info(gc, ergo)(\"After old marking finished, %s \" SIZE_FORMAT \" regions to %s to prepare for next gc, old available: \"\n+                     SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                     success? \"successfully transferred\": \"failed to transfer\", region_xfer, region_destination,\n+                     byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                     byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":44,"deletions":0,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -173,1 +173,2 @@\n-    _state(IDLE)\n+    _state(IDLE),\n+    _growth_before_compaction(INITIAL_GROWTH_BEFORE_COMPACTION)\n@@ -175,0 +176,1 @@\n+  _live_bytes_after_last_mark = ShenandoahHeap::heap()->capacity() * INITIAL_LIVE_FRACTION \/ FRACTIONAL_DENOMINATOR;\n@@ -179,0 +181,17 @@\n+size_t ShenandoahOldGeneration::get_live_bytes_after_last_mark() const {\n+  return _live_bytes_after_last_mark;\n+}\n+\n+void ShenandoahOldGeneration::set_live_bytes_after_last_mark(size_t bytes) {\n+  _live_bytes_after_last_mark = bytes;\n+  if (_growth_before_compaction > MINIMUM_GROWTH_BEFORE_COMPACTION) {\n+    _growth_before_compaction \/= 2;\n+  }\n+}\n+\n+size_t ShenandoahOldGeneration::usage_trigger_threshold() const {\n+  size_t result = _live_bytes_after_last_mark + (_live_bytes_after_last_mark * _growth_before_compaction) \/ FRACTIONAL_DENOMINATOR;\n+  return result;\n+}\n+\n+\n@@ -258,0 +277,1 @@\n+\n@@ -320,1 +340,5 @@\n-    heap->free_set()->rebuild();\n+    size_t cset_young_regions, cset_old_regions;\n+    heap->free_set()->prepare_to_rebuild(cset_young_regions, cset_old_regions);\n+    \/\/ This is just old-gen completion.  No future budgeting required here.  The only reason to rebuild the freeset here\n+    \/\/ is in case there was any immediate old garbage identified.\n+    heap->free_set()->rebuild(0);\n@@ -403,1 +427,2 @@\n-      assert(_old_heuristics->unprocessed_old_collection_candidates() == 0, \"Cannot become idle with collection candidates\");\n+      assert(!heap->mode()->is_generational() ||\n+             (_old_heuristics->unprocessed_old_collection_candidates() == 0), \"Cannot become idle with collection candidates\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":28,"deletions":3,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -98,0 +98,5 @@\n+  size_t get_live_bytes_after_last_mark() const;\n+  void set_live_bytes_after_last_mark(size_t new_live);\n+\n+  size_t usage_trigger_threshold() const;\n+\n@@ -103,0 +108,18 @@\n+  static const size_t FRACTIONAL_DENOMINATOR = 64536;\n+\n+  \/\/ During initialization of the JVM, we search for the correct old-gen size by initally performing old-gen\n+  \/\/ collection when old-gen usage is 50% more (INITIAL_GROWTH_BEFORE_COMPACTION) than the initial old-gen size\n+  \/\/ estimate (3.125% of heap).  The next old-gen trigger occurs when old-gen grows 25% larger than its live\n+  \/\/ memory at the end of the first old-gen collection.  Then we trigger again when old-gen growns 12.5%\n+  \/\/ more than its live memory at the end of the previous old-gen collection.  Thereafter, we trigger each time\n+  \/\/ old-gen grows more than 12.5% following the end of its previous old-gen collection.\n+  static const size_t INITIAL_GROWTH_BEFORE_COMPACTION = FRACTIONAL_DENOMINATOR \/ 2;          \/\/  50.0%\n+  static const size_t MINIMUM_GROWTH_BEFORE_COMPACTION = FRACTIONAL_DENOMINATOR \/ 8;          \/\/  12.5%\n+\n+  \/\/ INITIAL_LIVE_FRACTION represents the initial guess of how large old-gen should be.  We estimate that old-gen\n+  \/\/ needs to consume 3.125% of the total heap size.  And we \"pretend\" that we start out with this amount of live\n+  \/\/ old-gen memory.  The first old-collection trigger will occur when old-gen occupies 50% more than this initial\n+  \/\/ approximation of the old-gen memory requirement, in other words when old-gen usage is 150% of 3.125%, which\n+  \/\/ is 4.6875% of the total heap size.\n+  static const uint16_t INITIAL_LIVE_FRACTION = FRACTIONAL_DENOMINATOR \/ 32;                    \/\/   3.125%\n+\n@@ -109,0 +132,2 @@\n+  size_t _live_bytes_after_last_mark;\n+  size_t _growth_before_compaction; \/\/ How much growth in usage before we trigger old collection, per 65_536\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -192,0 +192,1 @@\n+  f(thread_iteration_roots,                         \"Java Thread Iteration\")           \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -143,1 +143,2 @@\n-  return _old_heuristics->should_start_gc() && _control_thread->request_concurrent_gc(OLD);\n+  return !ShenandoahHeap::heap()->doing_mixed_evacuations() && !ShenandoahHeap::heap()->collection_set()->has_old_regions() &&\n+    _old_heuristics->should_start_gc() && _control_thread->request_concurrent_gc(OLD);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -381,1 +381,2 @@\n-      case FREE: return;\n+      case FREE:\n+        return;\n@@ -399,1 +400,2 @@\n-  static void validate_usage(const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n+  static void validate_usage(const bool adjust_for_padding, const bool adjust_for_deferred_accounting,\n+                             const char* label, ShenandoahGeneration* generation, ShenandoahCalculateRegionStatsClosure& stats) {\n@@ -401,0 +403,25 @@\n+    size_t generation_used_regions = generation->used_regions();\n+    if (adjust_for_deferred_accounting) {\n+      ShenandoahHeap* heap = ShenandoahHeap::heap();\n+      ShenandoahGeneration* young_generation = heap->young_generation();\n+      size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+      size_t humongous_bytes_promoted = heap->get_promotable_humongous_usage();\n+      size_t total_regions_promoted = humongous_regions_promoted;\n+      size_t bytes_promoted_in_place = 0;\n+      if (total_regions_promoted > 0) {\n+        bytes_promoted_in_place = humongous_bytes_promoted;\n+      }\n+      if (generation->is_young()) {\n+        generation_used -= bytes_promoted_in_place;\n+        generation_used_regions -= total_regions_promoted;\n+      } else if (generation->is_old()) {\n+        generation_used += bytes_promoted_in_place;\n+        generation_used_regions += total_regions_promoted;\n+      }\n+      \/\/ else, global validation doesn't care where the promoted-in-place data is tallied.\n+    }\n+    if (adjust_for_padding && (generation->is_young() || generation->is_global())) {\n+      size_t pad = ShenandoahHeap::heap()->get_pad_for_promote_in_place();\n+      generation_used += pad;\n+    }\n+\n@@ -407,1 +434,1 @@\n-    guarantee(stats.regions() == generation->used_regions(),\n+    guarantee(stats.regions() == generation_used_regions,\n@@ -411,2 +438,8 @@\n-    size_t capacity = generation->adjusted_capacity();\n-    guarantee(stats.span() <= capacity,\n+    size_t generation_capacity = generation->soft_max_capacity();\n+    if (adjust_for_deferred_accounting) {\n+      ShenandoahHeap* heap = ShenandoahHeap::heap();\n+      size_t humongous_regions_promoted = heap->get_promotable_humongous_regions();\n+      size_t transferred_regions = humongous_regions_promoted;\n+      generation_capacity += transferred_regions * ShenandoahHeapRegion::region_size_bytes();\n+    }\n+    guarantee(stats.span() <= generation_capacity,\n@@ -415,2 +448,1 @@\n-              byte_size_in_proper_unit(capacity), proper_unit_for_byte_size(capacity));\n-\n+              byte_size_in_proper_unit(generation_capacity), proper_unit_for_byte_size(generation_capacity));\n@@ -713,1 +745,1 @@\n-    char actual = ShenandoahThreadLocalData::gc_state(t);\n+    char actual = ShenandoahThreadLocalData::gc_state(t) & ~ShenandoahHeap::MIXED_EVACUATIONS_ENABLED;\n@@ -725,0 +757,1 @@\n+                                             VerifySize sizeness,\n@@ -776,1 +809,1 @@\n-      char actual = _heap->gc_state();\n+      char actual = _heap->gc_state() & ~ShenandoahHeap::MIXED_EVACUATIONS_ENABLED;\n@@ -796,7 +829,14 @@\n-    size_t heap_used = _heap->used();\n-    guarantee(cl.used() == heap_used,\n-              \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n-              label,\n-              byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n-              byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n-\n+    size_t heap_used;\n+    if (_heap->mode()->is_generational() && (sizeness == _verify_size_adjusted_for_padding)) {\n+      \/\/ Prior to evacuation, regular regions that are to be evacuated in place are padded to prevent further allocations\n+      heap_used = _heap->used() + _heap->get_pad_for_promote_in_place();\n+    } else if (sizeness != _verify_size_disable) {\n+      heap_used = _heap->used();\n+    }\n+    if (sizeness != _verify_size_disable) {\n+      guarantee(cl.used() == heap_used,\n+                \"%s: heap used size must be consistent: heap-used = \" SIZE_FORMAT \"%s, regions-used = \" SIZE_FORMAT \"%s\",\n+                label,\n+                byte_size_in_proper_unit(heap_used), proper_unit_for_byte_size(heap_used),\n+                byte_size_in_proper_unit(cl.used()), proper_unit_for_byte_size(cl.used()));\n+    }\n@@ -849,3 +889,15 @@\n-    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->old_generation(), cl.old);\n-    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->young_generation(), cl.young);\n-    ShenandoahGenerationStatsClosure::validate_usage(label, _heap->global_generation(), cl.global);\n+    if (sizeness == _verify_size_adjusted_for_deferred_accounting) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, true, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, true, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->global_generation(), cl.global);\n+    } else if  (sizeness == _verify_size_adjusted_for_padding) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, false, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(true, false, label, _heap->global_generation(), cl.global);\n+    }\n+    else if (sizeness == _verify_size_exact) {\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->old_generation(), cl.old);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->young_generation(), cl.young);\n+      ShenandoahGenerationStatsClosure::validate_usage(false, false, label, _heap->global_generation(), cl.global);\n+    }\n+    \/\/ else: sizeness must equal _verify_size_disable\n@@ -963,0 +1015,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -976,0 +1029,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -989,0 +1043,1 @@\n+          _verify_size_exact,          \/\/ expect generation and heap sizes to match exactly\n@@ -1002,0 +1057,2 @@\n+          _verify_size_adjusted_for_padding,         \/\/ expect generation and heap sizes to match after adjustments\n+                                                     \/\/  for promote in place padding\n@@ -1015,0 +1072,1 @@\n+          _verify_size_disable,       \/\/ we don't know how much of promote-in-place work has been completed\n@@ -1028,0 +1086,2 @@\n+          _verify_size_adjusted_for_deferred_accounting,\n+                                       \/\/ expect generation and heap sizes to match after adjustments for promote in place\n@@ -1041,0 +1101,3 @@\n+          _verify_size_adjusted_for_deferred_accounting,\n+                                                       \/\/ expect generation and heap sizes to match after adjustments\n+                                                       \/\/  for promote in place\n@@ -1045,0 +1108,1 @@\n+\/\/ We have not yet cleanup (reclaimed) the collection set\n@@ -1054,0 +1118,2 @@\n+          _verify_size_adjusted_for_deferred_accounting,\n+                                       \/\/ expect generation and heap sizes to match after adjustments for promote in place\n@@ -1067,0 +1133,1 @@\n+          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n@@ -1080,0 +1147,1 @@\n+          _verify_size_disable,        \/\/ if we degenerate during evacuation, usage not valid: padding and deferred accounting\n@@ -1093,0 +1161,1 @@\n+          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n@@ -1106,0 +1175,1 @@\n+          _verify_size_exact,           \/\/ expect generation and heap sizes to match exactly\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":89,"deletions":19,"binary":false,"changes":108,"status":"modified"},{"patch":"@@ -144,0 +144,14 @@\n+  typedef enum {\n+    \/\/ Disable size verification\n+    _verify_size_disable,\n+\n+    \/\/ Enforce exact consistency\n+    _verify_size_exact,\n+\n+    \/\/ Expect promote-in-place adjustments: padding inserted to temporarily prevent further allocation in regular regions\n+    _verify_size_adjusted_for_padding,\n+\n+    \/\/ Expect promote-in-place adjustments: usage within regions promoted in place is transferred at end of update refs\n+    _verify_size_adjusted_for_deferred_accounting\n+  } VerifySize;\n+\n@@ -192,0 +206,1 @@\n+                           VerifySize sizeness,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.hpp","additions":15,"deletions":0,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -100,1 +100,1 @@\n-  product(uintx, ShenandoahOldGarbageThreshold, 10, EXPERIMENTAL,           \\\n+  product(uintx, ShenandoahOldGarbageThreshold, 15, EXPERIMENTAL,           \\\n@@ -131,7 +131,0 @@\n-  product(uintx, ShenandoahOldMinFreeThreshold, 5, EXPERIMENTAL,            \\\n-          \"Percentage of free old generation heap memory below which most \" \\\n-          \"heuristics trigger collection independent of other triggers. \"   \\\n-          \"Provides a safety margin for many heuristics. In percents of \"   \\\n-          \"(soft) max heap size.\")                                          \\\n-          range(0,100)                                                      \\\n-                                                                            \\\n@@ -213,1 +206,1 @@\n-  product(uintx, ShenandoahGuaranteedYoungGCInterval, 5*60*1000,  EXPERIMENTAL,  \\\n+  product(uintx, ShenandoahGuaranteedYoungGCInterval, 15*1000,  EXPERIMENTAL,  \\\n@@ -302,11 +295,14 @@\n-  product(double, ShenandoahGenerationalEvacWaste, 2.0, EXPERIMENTAL,       \\\n-          \"For generational mode, how much waste evacuations produce \"      \\\n-          \"within the reserved space.  Larger values make evacuations \"     \\\n-          \"more resilient against evacuation conflicts, at expense of \"     \\\n-          \"evacuating less on each GC cycle.  Smaller values increase \"     \\\n-          \"the risk of evacuation failures, which will trigger \"            \\\n-          \"stop-the-world Full GC passes.  The default value for \"          \\\n-          \"generational mode is 2.0.  The reason for the higher default \"   \\\n-          \"value in generational mode is because generational mode \"        \\\n-          \"enforces the evacuation budget, triggering degenerated GC \"      \\\n-          \"which upgrades to full GC whenever the budget is exceeded.\")     \\\n+  product(double, ShenandoahOldEvacWaste, 1.6, EXPERIMENTAL,                \\\n+          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n+          range(1.0,100.0)                                                  \\\n+                                                                            \\\n+  product(double, ShenandoahPromoEvacWaste, 1.2, EXPERIMENTAL,              \\\n+          \"How much waste evacuations produce within the reserved space. \"  \\\n+          \"Larger values make evacuations more resilient against \"          \\\n+          \"evacuation conflicts, at expense of evacuating less on each \"    \\\n+          \"GC cycle.  Smaller values increase the risk of evacuation \"      \\\n+          \"failures, which will trigger stop-the-world Full GC passes.\")    \\\n@@ -341,15 +337,1 @@\n-  product(uintx, ShenandoahOldEvacReserve, 2, EXPERIMENTAL,                 \\\n-          \"How much of old-generation heap to reserve for old-generation \"  \\\n-          \"evacuations.  Larger values allow GC to evacuate more live \"     \\\n-          \"old-generation objects on every cycle, while potentially \"       \\\n-          \"creating greater impact on the cadence at which the young- \"     \\\n-          \"generation allocation pool is replenished.  During mixed \"       \\\n-          \"evacuations, the bound on amount of old-generation heap \"        \\\n-          \"regions included in the collecdtion set is the smaller \"         \\\n-          \"of the quantities specified by this parameter and the \"          \\\n-          \"size of ShenandoahEvacReserve as adjusted by the value of \"      \\\n-          \"ShenandoahOldEvacRatioPercent.  In percents of total \"           \\\n-          \"old-generation heap size.\")                                      \\\n-          range(1,100)                                                      \\\n-                                                                            \\\n-  product(uintx, ShenandoahOldEvacRatioPercent, 12, EXPERIMENTAL,           \\\n+  product(uintx, ShenandoahOldEvacRatioPercent, 75, EXPERIMENTAL,           \\\n@@ -357,4 +339,8 @@\n-          \"a percent ratio.  The default value 12 denotes that no more \"    \\\n-          \"than one eighth (12%) of the collection set evacuation \"         \\\n-          \"workload may be comprised of old-gen heap regions.  A larger \"   \\\n-          \"value allows a smaller number of mixed evacuations to process \"  \\\n+          \"a percent ratio.  The default value 75 denotes that no more \"    \\\n+          \"than 75% of the collection set evacuation \"                      \\\n+          \"workload may be evacuate to old-gen heap regions.  This limits \" \\\n+          \"both the promotion of aged regions and the compaction of \"       \\\n+          \"existing old regions.  A value of 75 denotes that the normal \"   \\\n+          \"young-gen evacuation is increased by up to four fold. \"          \\\n+          \"A larger value allows quicker promotion and allows\"              \\\n+          \"a smaller number of mixed evacuations to process \"               \\\n@@ -374,1 +360,1 @@\n-  product(uintx, ShenandoahMinYoungPercentage, 20,                                \\\n+  product(uintx, ShenandoahMinYoungPercentage, 20,                          \\\n@@ -380,1 +366,1 @@\n-  product(uintx, ShenandoahMaxYoungPercentage, 80,                                \\\n+  product(uintx, ShenandoahMaxYoungPercentage, 100,                         \\\n@@ -520,14 +506,0 @@\n-  product(uintx, ShenandoahBorrowPercent, 30, EXPERIMENTAL,                 \\\n-          \"During evacuation and reference updating in generational \"       \\\n-          \"mode, new allocations are allowed to borrow from old-gen \"       \\\n-          \"memory up to ShenandoahBorrowPercent \/ 100 amount of the \"       \\\n-          \"young-generation content of the current collection set.  \"       \\\n-          \"Any memory borrowed from old-gen during evacuation and \"         \\\n-          \"update-references phases of GC will be repaid from the \"         \\\n-          \"abundance of young-gen memory produced when the collection \"     \\\n-          \"set is recycled at the end of updating references.  The \"        \\\n-          \"default value of 30 reserves 70% of the to-be-reclaimed \"        \\\n-          \"young collection set memory to be allocated during the \"         \\\n-          \"subsequent concurrent mark phase of GC.\")                        \\\n-          range(0, 100)                                                     \\\n-                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":27,"deletions":55,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-    region->set_affiliation(FREE);\n+    region->set_affiliation(FREE, false);\n","filename":"test\/hotspot\/gtest\/gc\/shenandoah\/test_shenandoahOldHeuristic.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}