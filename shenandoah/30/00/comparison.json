{"files":[{"patch":"@@ -35,1 +35,1 @@\n-    _alloc_shared_gc,   \/\/ Allocate common, outside of GCLAB\n+    _alloc_shared_gc,   \/\/ Allocate common, outside of GCLAB\/PLAB\n@@ -38,0 +38,1 @@\n+    _alloc_plab,        \/\/ Allocate PLAB\n@@ -51,0 +52,2 @@\n+      case _alloc_plab:\n+        return \"PLAB\";\n@@ -84,0 +87,4 @@\n+  static inline ShenandoahAllocRequest for_plab(size_t min_size, size_t requested_size) {\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_plab, ShenandoahRegionAffiliation::OLD_GENERATION);\n+  }\n+\n@@ -128,0 +135,1 @@\n+      case _alloc_plab:\n@@ -142,0 +150,1 @@\n+      case _alloc_plab:\n@@ -154,0 +163,1 @@\n+      case _alloc_plab:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAllocRequest.hpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -38,1 +38,0 @@\n-\n@@ -68,0 +67,17 @@\n+HeapWord* ShenandoahFreeSet::allocate_with_affiliation(ShenandoahRegionAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region) {\n+  for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n+    \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n+    size_t idx = c - 1;\n+    if (is_collector_free(idx)) {\n+      ShenandoahHeapRegion* r = _heap->get_region(idx);\n+      if (r->affiliation() == affiliation) {\n+        HeapWord* result = try_allocate_in(r, req, in_new_region);\n+        if (result != NULL) {\n+          return result;\n+        }\n+      }\n+    }\n+  }\n+  return NULL;\n+}\n+\n@@ -99,0 +115,1 @@\n+    case ShenandoahAllocRequest::_alloc_plab:\n@@ -100,27 +117,9 @@\n-      \/\/ size_t is unsigned, need to dodge underflow when _leftmost = 0\n-\n-      \/\/ Fast-path: try to allocate in the collector view first\n-      for (size_t c = _collector_rightmost + 1; c > _collector_leftmost; c--) {\n-        size_t idx = c - 1;\n-        if (is_collector_free(idx)) {\n-          ShenandoahHeapRegion* r = _heap->get_region(idx);\n-          if (r->is_young() && req.is_old()) {\n-            \/\/ We don't want to cannibalize a young region to satisfy\n-            \/\/ an evacuation from an old region.\n-            continue;\n-          }\n-          HeapWord* result = try_allocate_in(r, req, in_new_region);\n-          if (result != NULL) {\n-            if (r->is_old()) {\n-              \/\/ HEY! This is a very coarse card marking. We hope to repair\n-              \/\/ such cards during remembered set scanning.\n-\n-              \/\/ HEY! To support full generality with alternative remembered set implementations,\n-              \/\/ is preferable to not make direct access to the current card_table implementation.\n-              \/\/  Try ShenandoahHeap::heap()->card_scan()->mark_range_as_dirty(result, req.actual_size());\n-\n-              ShenandoahBarrierSet::barrier_set()->card_table()->dirty_MemRegion(MemRegion(result, req.actual_size()));\n-            }\n-            return result;\n-          }\n-        }\n+      \/\/ First try to fit into a region that is already in use in the same generation.\n+      HeapWord* result = allocate_with_affiliation(req.affiliation(), req, in_new_region);\n+      if (result != NULL) {\n+        return result;\n+      }\n+      \/\/ Then try a free region that is dedicated to GC allocations.\n+      result = allocate_with_affiliation(FREE, req, in_new_region);\n+      if (result != NULL) {\n+        return result;\n@@ -134,1 +133,1 @@\n-      \/\/ Try to steal the empty region from the mutator view\n+      \/\/ Try to steal an empty region from the mutator view.\n@@ -140,4 +139,0 @@\n-            if (r->is_young() && req.is_old()) {\n-              continue;\n-            }\n-\n@@ -147,10 +142,0 @@\n-              if (r->is_old()) {\n-                \/\/ HEY! This is a very coarse card marking. We hope to repair\n-                \/\/ such cards during remembered set scanning.\n-\n-                \/\/ HEY! To support full generality with alternative remembered set implementations,\n-                \/\/ is preferable to not make direct access to the current card_table implementation.\n-                \/\/  Try ShenandoahHeap::heap()->card_scan()->mark_range_as_dirty(result, req.actual_size());\n-\n-                ShenandoahBarrierSet::barrier_set()->card_table()->dirty_MemRegion(MemRegion(result, req.actual_size()));\n-              }\n@@ -166,1 +151,0 @@\n-\n@@ -172,1 +156,0 @@\n-\n@@ -205,1 +188,1 @@\n-      result = r->allocate(size, req.type());\n+      result = r->allocate(size, req);\n@@ -209,1 +192,1 @@\n-    result = r->allocate(size, req.type());\n+    result = r->allocate(size, req);\n@@ -442,0 +425,6 @@\n+\n+  shenandoah_assert_heaplocked();\n+  try_recycle_trashed(r);\n+\n+  assert(r->is_empty(), \"Region must be empty after flipping from mutator to GC.\");\n+  r->set_affiliation(FREE);\n@@ -602,0 +591,1 @@\n+      case ShenandoahAllocRequest::_alloc_plab:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":37,"deletions":47,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+  HeapWord* allocate_with_affiliation(ShenandoahRegionAffiliation affiliation, ShenandoahAllocRequest& req, bool& in_new_region);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -334,1 +334,1 @@\n-      \/\/ HEY! Changing this region to young during compaction may not be\n+      \/\/ TODO: Changing this region to young during compaction may not be\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -834,0 +834,49 @@\n+HeapWord* ShenandoahHeap::allocate_from_plab_slow(Thread* thread, size_t size) {\n+  \/\/ New object should fit the PLAB size\n+  size_t min_size = MAX2(size, PLAB::min_size());\n+\n+  \/\/ Figure out size of new PLAB, looking back at heuristics. Expand aggressively.\n+  size_t new_size = ShenandoahThreadLocalData::plab_size(thread) * 2;\n+  new_size = MIN2(new_size, PLAB::max_size());\n+  new_size = MAX2(new_size, PLAB::min_size());\n+\n+  \/\/ Record new heuristic value even if we take any shortcut. This captures\n+  \/\/ the case when moderately-sized objects always take a shortcut. At some point,\n+  \/\/ heuristics should catch up with them.\n+  ShenandoahThreadLocalData::set_plab_size(thread, new_size);\n+\n+  if (new_size < size) {\n+    \/\/ New size still does not fit the object. Fall back to shared allocation.\n+    \/\/ This avoids retiring perfectly good PLABs, when we encounter a large object.\n+    return NULL;\n+  }\n+\n+  \/\/ Retire current PLAB, and allocate a new one.\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  plab->retire();\n+\n+  size_t actual_size = 0;\n+  HeapWord* plab_buf = allocate_new_plab(min_size, new_size, &actual_size);\n+  if (plab_buf == NULL) {\n+    return NULL;\n+  }\n+\n+  assert (size <= actual_size, \"allocation should fit\");\n+\n+  if (ZeroTLAB) {\n+    \/\/ ..and clear it.\n+    Copy::zero_to_words(plab_buf, actual_size);\n+  } else {\n+    \/\/ ...and zap just allocated object.\n+#ifdef ASSERT\n+    \/\/ Skip mangling the space corresponding to the object header to\n+    \/\/ ensure that the returned space is not considered parsable by\n+    \/\/ any concurrent GC thread.\n+    size_t hdr_size = oopDesc::header_size();\n+    Copy::fill_to_words(plab_buf + hdr_size, actual_size - hdr_size, badHeapWordVal);\n+#endif \/\/ ASSERT\n+  }\n+  plab->set_buf(plab_buf, actual_size);\n+  return plab->allocate(size);\n+}\n+\n@@ -860,0 +909,13 @@\n+HeapWord* ShenandoahHeap::allocate_new_plab(size_t min_size,\n+                                            size_t word_size,\n+                                            size_t* actual_size) {\n+  ShenandoahAllocRequest req = ShenandoahAllocRequest::for_plab(min_size, word_size);\n+  HeapWord* res = allocate_memory(req);\n+  if (res != NULL) {\n+    *actual_size = req.actual_size();\n+  } else {\n+    *actual_size = 0;\n+  }\n+  return res;\n+}\n+\n@@ -955,1 +1017,1 @@\n-  \/\/ invocations to be \"mutually exclusive\".  Later, when we use GCLABs to allocate memory for promotions and evacuations,\n+  \/\/ invocations to be \"mutually exclusive\".  Later, when we use GCLABs and PLABs to allocate memory for promotions and evacuations,\n@@ -957,6 +1019,7 @@\n-  \/\/   1. The GCLAB is allocated by this (or similar) function, while holding the global lock.\n-  \/\/   2. The GCLAB is registered as a single object.\n-  \/\/\/  3. The GCLAB is always aligned at the start of a card memory range and is always a multiple of the card-table memory range size\n-  \/\/   3. Individual allocations carved from the GCLAB are not immediately registered\n-  \/\/   4. When the GCLAB is eventually retired, all of the objects allocated within the GCLAB are registered in batch by a\n-  \/\/      single thread.  No further synchronization is required because no other allocations will pertain to the same\n+  \/\/   1. The GCLAB\/PLAB is allocated by this (or similar) function, while holding the global lock.\n+  \/\/   2. The GCLAB\/PLAB is always aligned at the start of a card memory range\n+  \/\/      and is always a multiple of the card-table memory range size.\n+  \/\/   3. Individual allocations carved from a GCLAB\/PLAB are not immediately registered.\n+  \/\/   4. A PLAB is registered as a single object.\n+  \/\/   5. When a PLAB is eventually retired, all of the objects allocated within the GCLAB\/PLAB are registered in batch by a\n+   \/\/      single thread.  No further synchronization is required because no other allocations will pertain to the same\n@@ -965,5 +1028,6 @@\n-  \/\/ The other case that needs special handling is promotion of regions en masse.  When the region is promoted, all objects contained\n-  \/\/ within the region are registered.  Since the region is a multiple of card-table memory range sizes, there is no need for\n-  \/\/ synchronization.  It might be nice to figure out how to allow multiple threads to work together to register all of the objects in\n-  \/\/ a promoted region, or at least try to balance the efforts so that different gc threads work on registering the objects of\n-  \/\/ different heap regions.  But that effort will come later.\n+  \/\/ The other case that needs special handling is region promotion.  When a region is promoted, all objects contained\n+  \/\/ in it are registered.  Since the region is a multiple of card table memory range sizes, there is no need for\n+  \/\/ synchronization.\n+  \/\/ TODO: figure out how to allow multiple threads to work together to register all of the objects in\n+  \/\/ a promoted region, or at least try to balance the efforts so that different GC threads work\n+  \/\/ on registering the objects of different heap regions.\n@@ -971,1 +1035,1 @@\n-  if (result != NULL && req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+  if (mode()->is_generational() && result != NULL && req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n@@ -1141,0 +1205,4 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != NULL, \"PLAB should be initialized for %s\", thread->name());\n+    assert(plab->words_remaining() == 0, \"PLAB should not need retirement\");\n@@ -1156,0 +1224,7 @@\n+\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    assert(plab != NULL, \"PLAB should be initialized for %s\", thread->name());\n+    plab->retire();\n+    if (_resize && ShenandoahThreadLocalData::plab_size(thread) > 0) {\n+      ShenandoahThreadLocalData::set_plab_size(thread, 0);\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":88,"deletions":13,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -564,0 +564,1 @@\n+\n@@ -568,0 +569,4 @@\n+  inline HeapWord* allocate_from_plab(Thread* thread, size_t size);\n+  HeapWord* allocate_from_plab_slow(Thread* thread, size_t size);\n+  HeapWord* allocate_new_plab(size_t min_size, size_t word_size, size_t* actual_size);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -213,1 +214,0 @@\n-  \/\/ Otherwise...\n@@ -217,0 +217,17 @@\n+inline HeapWord* ShenandoahHeap::allocate_from_plab(Thread* thread, size_t size) {\n+  assert(UseTLAB, \"TLABs should be enabled\");\n+\n+  PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+  if (plab == NULL) {\n+    assert(!thread->is_Java_thread() && !thread->is_Worker_thread(),\n+           \"Performance: thread should have PLAB: %s\", thread->name());\n+    \/\/ No PLABs in this thread, fallback to shared allocation\n+    return NULL;\n+  }\n+  HeapWord* obj = plab->allocate(size);\n+  if (obj != NULL) {\n+    return obj;\n+  }\n+  return allocate_from_plab_slow(thread, size);\n+}\n+\n@@ -243,7 +260,0 @@\n-        \/\/ TODO: Just marking the cards covering this object dirty\n-        \/\/ may overall be less efficient than scanning it now for references to young gen\n-        \/\/ or other alternatives like deferred card marking or scanning.\n-        \/\/ We should revisit this.\n-        \/\/ Furthermore, the object start should be registered for remset scanning.\n-        MemRegion mr(cast_from_oop<HeapWord*>(result), result->size());\n-        ShenandoahBarrierSet::barrier_set()->card_table()->invalidate(mr);\n@@ -258,1 +268,1 @@\n-  bool alloc_from_gclab = true;\n+  bool alloc_from_lab = true;\n@@ -268,2 +278,17 @@\n-    if (UseTLAB && target_gen == YOUNG_GENERATION) {\n-      copy = allocate_from_gclab(thread, size);\n+    if (UseTLAB) {\n+      switch (target_gen) {\n+        case YOUNG_GENERATION: {\n+           copy = allocate_from_gclab(thread, size);\n+           break;\n+        }\n+        case OLD_GENERATION: {\n+           if (ShenandoahUsePLAB) {\n+             copy = allocate_from_plab(thread, size);\n+           }\n+           break;\n+        }\n+        default: {\n+          ShouldNotReachHere();\n+          break;\n+        }\n+      }\n@@ -274,1 +299,1 @@\n-      alloc_from_gclab = false;\n+      alloc_from_lab = false;\n@@ -313,0 +338,4 @@\n+    if (target_gen == OLD_GENERATION) {\n+      ShenandoahBarrierSet::barrier_set()->card_table()->dirty_MemRegion(MemRegion(copy, size));\n+      card_scan()->register_object(copy);\n+    }\n@@ -322,8 +351,18 @@\n-    \/\/\n-    \/\/ For GCLAB allocations, it is enough to rollback the allocation ptr. Either the next\n-    \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n-    \/\/ do this. For non-GCLAB allocations, we have no way to retract the allocation, and\n-    \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n-    \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n-    if (alloc_from_gclab) {\n-      ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+    if (alloc_from_lab) {\n+       \/\/ For LAB allocations, it is enough to rollback the allocation ptr. Either the next\n+       \/\/ object will overwrite this stale copy, or the filler object on LAB retirement will\n+       \/\/ do this.\n+       switch (target_gen) {\n+         case YOUNG_GENERATION: {\n+             ShenandoahThreadLocalData::gclab(thread)->undo_allocation(copy, size);\n+            break;\n+         }\n+         case OLD_GENERATION: {\n+            ShenandoahThreadLocalData::plab(thread)->undo_allocation(copy, size);\n+            break;\n+         }\n+         default: {\n+           ShouldNotReachHere();\n+           break;\n+         }\n+       }\n@@ -331,0 +370,3 @@\n+      \/\/ For non-LAB allocations, we have no way to retract the allocation, and\n+      \/\/ have to explicitly overwrite the copy with the filler object. With that overwrite,\n+      \/\/ we have to keep the fwdptr initialized and pointing to our (stale) copy.\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":62,"deletions":20,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+  _plab_allocs(0),\n@@ -76,1 +77,1 @@\n-  _affiliation(ShenandoahRegionAffiliation::FREE),\n+  _affiliation(FREE),\n@@ -94,1 +95,1 @@\n-void ShenandoahHeapRegion::make_regular_allocation() {\n+void ShenandoahHeapRegion::make_regular_allocation(ShenandoahRegionAffiliation affiliation) {\n@@ -101,0 +102,1 @@\n+      set_affiliation(affiliation);\n@@ -122,0 +124,6 @@\n+      \/\/ TODO: Changing this region to young during compaction may not be\n+      \/\/ technically correct here because it completely disregards the ages\n+      \/\/ and origins of the objects being moved. It is, however, certainly\n+      \/\/ more correct than putting live objects into a region without a\n+      \/\/ generational affiliation.\n+      set_affiliation(YOUNG_GENERATION);\n@@ -223,0 +231,1 @@\n+      assert(affiliation() != FREE, \"Pinned region should not be FREE\");\n@@ -321,0 +330,1 @@\n+  _plab_allocs = 0;\n@@ -324,1 +334,1 @@\n-  return used() - (_tlab_allocs + _gclab_allocs) * HeapWordSize;\n+  return used() - (_tlab_allocs + _gclab_allocs + _plab_allocs) * HeapWordSize;\n@@ -335,0 +345,4 @@\n+size_t ShenandoahHeapRegion::get_plab_allocs() const {\n+  return _plab_allocs * HeapWordSize;\n+}\n+\n@@ -400,0 +414,3 @@\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    st->print(\"|G \" SIZE_FORMAT_W(5) \"%1s\", byte_size_in_proper_unit(get_plab_allocs()),   proper_unit_for_byte_size(get_plab_allocs()));\n+  }\n@@ -516,1 +533,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":20,"deletions":4,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -171,1 +171,1 @@\n-  void make_regular_allocation();\n+  void make_regular_allocation(ShenandoahRegionAffiliation affiliation);\n@@ -245,0 +245,1 @@\n+  size_t _plab_allocs;\n@@ -342,1 +343,1 @@\n-  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest::Type type);\n+  inline HeapWord* allocate(size_t word_size, ShenandoahAllocRequest req);\n@@ -391,0 +392,1 @@\n+  size_t get_plab_allocs() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-HeapWord* ShenandoahHeapRegion::allocate(size_t size, ShenandoahAllocRequest::Type type) {\n+HeapWord* ShenandoahHeapRegion::allocate(size_t size, ShenandoahAllocRequest req) {\n@@ -39,2 +39,2 @@\n-    make_regular_allocation();\n-    adjust_alloc_metadata(type, size);\n+    make_regular_allocation(req.affiliation());\n+    adjust_alloc_metadata(req.type(), size);\n@@ -66,0 +66,3 @@\n+    case ShenandoahAllocRequest::_alloc_plab:\n+      _plab_allocs += size;\n+      break;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -114,0 +114,1 @@\n+          data |= ((100 * r->get_plab_allocs() \/ rs)     & PERCENT_MASK) << PLAB_SHIFT;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -54,1 +54,1 @@\n- * - bits 35-41  <reserved>\n+ * - bits 35-41  plab allocated memory in percent\n@@ -73,0 +73,1 @@\n+  static const jlong PLAB_SHIFT        = 35;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -50,0 +50,2 @@\n+  PLAB* _plab;\n+  size_t _plab_size;\n@@ -61,0 +63,2 @@\n+    _plab(NULL),\n+    _plab_size(0),\n@@ -74,0 +78,3 @@\n+    if (_plab != NULL) {\n+      delete _plab;\n+    }\n@@ -121,0 +128,2 @@\n+    data(thread)->_plab = new PLAB(PLAB::min_size());\n+    data(thread)->_plab_size = 0;\n@@ -135,0 +144,12 @@\n+  static PLAB* plab(Thread* thread) {\n+    return data(thread)->_plab;\n+  }\n+\n+  static size_t plab_size(Thread* thread) {\n+    return data(thread)->_plab_size;\n+  }\n+\n+  static void set_plab_size(Thread* thread, size_t v) {\n+    data(thread)->_plab_size = v;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahThreadLocalData.hpp","additions":21,"deletions":0,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -438,2 +438,5 @@\n-    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() == r->used(),\n-           \"Accurate accounting: shared + TLAB + GCLAB = used\");\n+    verify(r, r->get_plab_allocs() <= r->capacity(),\n+           \"PLAB alloc count should not be larger than capacity\");\n+\n+    verify(r, r->get_shared_allocs() + r->get_tlab_allocs() + r->get_gclab_allocs() + r->get_plab_allocs() == r->used(),\n+           \"Accurate accounting: shared + TLAB + GCLAB + PLAB = used\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -227,0 +227,4 @@\n+  product(bool, ShenandoahUsePLAB, true, DIAGNOSTIC,                        \\\n+          \"Use PLABs for object promotions with Shenandoah, \"               \\\n+          \"if in generational mode and UseTLAB is also set.\")               \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"}]}