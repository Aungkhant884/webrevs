{"files":[{"patch":"@@ -40,0 +40,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -93,2 +94,0 @@\n-  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n-\n@@ -117,0 +116,1 @@\n+  __ block_comment(\"arraycopy_prologue (shenandoahgc) {\");\n@@ -133,1 +133,1 @@\n-                              : ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING;\n+                              : ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING;\n@@ -190,0 +190,10 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register dst, Register count,\n+                                                       Register preserve) {\n+  if (is_reference_type(type)) {\n+    __ block_comment(\"arraycopy_epilogue (shenandoahgc) {\");\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, preserve);\n+    __ block_comment(\"} arraycopy_epilogue (shenandoahgc)\");\n+  }\n+}\n+\n@@ -223,1 +233,1 @@\n-  __ andi_(tmp1, tmp1, ShenandoahHeap::MARKING);\n+  __ andi_(tmp1, tmp1, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -589,0 +599,21 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register base, RegisterOrConstant ind_or_offs, Register tmp) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+  assert_different_registers(base, tmp, R0);\n+\n+  if (ind_or_offs.is_constant()) {\n+    __ add_const_optimized(base, base, ind_or_offs.as_constant(), tmp);\n+  } else {\n+    __ add(base, ind_or_offs.as_register(), base);\n+  }\n+\n+  __ load_const_optimized(tmp, (address)ct->byte_map_base(), R0);\n+  __ srdi(base, base, CardTable::card_shift());\n+  __ li(R0, CardTable::dirty_card_val());\n+  __ stbx(R0, tmp, base);\n+}\n+\n@@ -611,0 +642,5 @@\n+\n+  \/\/ No need for post barrier if storing NULL\n+  if (is_reference_type(type) && val != noreg) {\n+    store_check(masm, base, ind_or_offs, tmp1);\n+  }\n@@ -760,0 +796,32 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register addr, Register count, Register preserve) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  ShenandoahBarrierSet* bs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = bs->card_table();\n+  assert_different_registers(addr, count, R0);\n+\n+  Label Lskip_loop, Lstore_loop;\n+\n+  __ sldi_(count, count, LogBytesPerHeapOop);\n+  __ beq(CCR0, Lskip_loop); \/\/ zero length\n+  __ addi(count, count, -BytesPerHeapOop);\n+  __ add(count, addr, count);\n+  \/\/ Use two shifts to clear out those low order two bits! (Cannot opt. into 1.)\n+  __ srdi(addr, addr, CardTable::card_shift());\n+  __ srdi(count, count, CardTable::card_shift());\n+  __ subf(count, addr, count);\n+  __ add_const_optimized(addr, addr, (address)ct->byte_map_base(), R0);\n+  __ addi(count, count, 1);\n+  __ li(R0, 0);\n+  __ mtctr(count);\n+  \/\/ Byte store loop\n+  __ bind(Lstore_loop);\n+  __ stb(R0, 0, addr);\n+  __ addi(addr, addr, 1);\n+  __ bdnz(Lstore_loop);\n+  __ bind(Lskip_loop);\n+}\n+\n@@ -895,1 +963,1 @@\n-  __ andi_(R12_tmp2, R12_tmp2, ShenandoahHeap::MARKING);\n+  __ andi_(R12_tmp2, R12_tmp2, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/ppc\/gc\/shenandoah\/shenandoahBarrierSetAssembler_ppc.cpp","additions":73,"deletions":5,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -40,1 +41,1 @@\n-AgeTable::AgeTable(bool global) {\n+AgeTable::AgeTable(bool global) : _use_perf_data(UsePerfData && global) {\n@@ -44,1 +45,1 @@\n-  if (UsePerfData && global) {\n+  if (_use_perf_data) {\n@@ -73,1 +74,1 @@\n-void AgeTable::merge(AgeTable* subTable) {\n+void AgeTable::merge(const AgeTable* subTable) {\n@@ -108,19 +109,4 @@\n-  if (log_is_enabled(Trace, gc, age) || UsePerfData || AgeTableTracer::is_tenuring_distribution_event_enabled()) {\n-    log_trace(gc, age)(\"Age table with threshold %u (max threshold \" UINTX_FORMAT \")\",\n-                       tenuring_threshold, MaxTenuringThreshold);\n-\n-    size_t total = 0;\n-    uint age = 1;\n-    while (age < table_size) {\n-      size_t wordSize = sizes[age];\n-      total += wordSize;\n-      if (wordSize > 0) {\n-        log_trace(gc, age)(\"- age %3u: \" SIZE_FORMAT_W(10) \" bytes, \" SIZE_FORMAT_W(10) \" total\",\n-                            age, wordSize * oopSize, total * oopSize);\n-      }\n-      AgeTableTracer::send_tenuring_distribution_event(age, wordSize * oopSize);\n-      if (UsePerfData) {\n-        _perf_sizes[age]->set_value(wordSize * oopSize);\n-      }\n-      age++;\n-    }\n+  LogTarget(Trace, gc, age) lt;\n+  if (lt.is_enabled() || _use_perf_data || AgeTableTracer::is_tenuring_distribution_event_enabled()) {\n+    LogStream st(lt);\n+    print_on(&st, tenuring_threshold);\n@@ -130,0 +116,20 @@\n+void AgeTable::print_on(outputStream* st, uint tenuring_threshold) {\n+  st->print_cr(\"Age table with threshold %u (max threshold \" UINTX_FORMAT \")\",\n+           tenuring_threshold, MaxTenuringThreshold);\n+\n+  size_t total = 0;\n+  uint age = 1;\n+  while (age < table_size) {\n+    size_t word_size = sizes[age];\n+    total += word_size;\n+    if (word_size > 0) {\n+      st->print_cr(\"- age %3u: \" SIZE_FORMAT_W(10) \" bytes, \" SIZE_FORMAT_W(10) \" total\",\n+                   age, word_size * oopSize, total * oopSize);\n+    }\n+    AgeTableTracer::send_tenuring_distribution_event(age, word_size * oopSize);\n+    if (_use_perf_data) {\n+      _perf_sizes[age]->set_value(word_size * oopSize);\n+    }\n+    age++;\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/ageTable.cpp","additions":28,"deletions":22,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-ShenandoahBarrierSet::ShenandoahBarrierSet(ShenandoahHeap* heap) :\n+ShenandoahBarrierSet::ShenandoahBarrierSet(ShenandoahHeap* heap, MemRegion heap_region) :\n@@ -55,0 +55,4 @@\n+  if (heap->mode()->is_generational()) {\n+    _card_table = new ShenandoahCardTable(heap_region);\n+    _card_table->initialize();\n+  }\n@@ -127,0 +131,8 @@\n+    PLAB* plab = ShenandoahThreadLocalData::plab(thread);\n+    \/\/ CAUTION: retire_plab may register the remnant filler object with the remembered set scanner without a lock.\n+    \/\/ This is safe iff it is assured that each PLAB is a whole-number multiple of card-mark memory size and each\n+    \/\/ PLAB is aligned with the start of each card's memory range.\n+    if (plab != NULL) {\n+      _heap->retire_plab(plab);\n+    }\n+\n@@ -145,0 +157,21 @@\n+\n+void ShenandoahBarrierSet::write_ref_array(HeapWord* start, size_t count) {\n+  if (!_heap->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  HeapWord* end = (HeapWord*)((char*) start + (count * heapOopSize));\n+  \/\/ In the case of compressed oops, start and end may potentially be misaligned;\n+  \/\/ so we need to conservatively align the first downward (this is not\n+  \/\/ strictly necessary for current uses, but a case of good hygiene and,\n+  \/\/ if you will, aesthetics) and the second upward (this is essential for\n+  \/\/ current uses) to a HeapWord boundary, so we mark all cards overlapping\n+  \/\/ this write.\n+  HeapWord* aligned_start = align_down(start, HeapWordSize);\n+  HeapWord* aligned_end   = align_up  (end,   HeapWordSize);\n+  \/\/ If compressed oops were not being used, these should already be aligned\n+  assert(UseCompressedOops || (aligned_start == start && aligned_end == end),\n+         \"Expected heap word alignment of start and end\");\n+  _heap->card_scan()->mark_range_as_dirty(aligned_start, (aligned_end - aligned_start));\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":34,"deletions":1,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -46,0 +47,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -54,0 +56,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -65,0 +68,63 @@\n+\/\/ After Full GC is done, reconstruct the remembered set by iterating over OLD regions,\n+\/\/ registering all objects between bottom() and top(), and setting remembered set cards to\n+\/\/ DIRTY if they hold interesting pointers.\n+class ShenandoahReconstructRememberedSetTask : public WorkerTask {\n+private:\n+  ShenandoahRegionIterator _regions;\n+\n+public:\n+  ShenandoahReconstructRememberedSetTask() :\n+    WorkerTask(\"Shenandoah Reset Bitmap\") { }\n+\n+  void work(uint worker_id) {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahHeapRegion* r = _regions.next();\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    RememberedScanner* scanner = heap->card_scan();\n+    ShenandoahSetRememberedCardsToDirtyClosure dirty_cards_for_interesting_pointers;\n+\n+    while (r != NULL) {\n+      if (r->is_old() && r->is_active()) {\n+        HeapWord* obj_addr = r->bottom();\n+        if (r->is_humongous_start()) {\n+          \/\/ First, clear the remembered set\n+          oop obj = cast_to_oop(obj_addr);\n+          size_t size = obj->size();\n+          HeapWord* end_object = r->bottom() + size;\n+\n+          \/\/ First, clear the remembered set for all spanned humongous regions\n+          size_t num_regions = (size + ShenandoahHeapRegion::region_size_words() - 1) \/ ShenandoahHeapRegion::region_size_words();\n+          size_t region_span = num_regions * ShenandoahHeapRegion::region_size_words();\n+          scanner->reset_remset(r->bottom(), region_span);\n+          size_t region_index = r->index();\n+          ShenandoahHeapRegion* humongous_region = heap->get_region(region_index);\n+          while (num_regions-- != 0) {\n+            scanner->reset_object_range(humongous_region->bottom(), humongous_region->end());\n+            region_index++;\n+            humongous_region = heap->get_region(region_index);\n+          }\n+\n+          \/\/ Then register the humongous object and DIRTY relevant remembered set cards\n+          scanner->register_object_wo_lock(obj_addr);\n+          obj->oop_iterate(&dirty_cards_for_interesting_pointers);\n+        } else if (!r->is_humongous()) {\n+          \/\/ First, clear the remembered set\n+          scanner->reset_remset(r->bottom(), ShenandoahHeapRegion::region_size_words());\n+          scanner->reset_object_range(r->bottom(), r->end());\n+\n+          \/\/ Then iterate over all objects, registering object and DIRTYing relevant remembered set cards\n+          HeapWord* t = r->top();\n+          while (obj_addr < t) {\n+            oop obj = cast_to_oop(obj_addr);\n+            size_t size = obj->size();\n+            scanner->register_object_wo_lock(obj_addr);\n+            obj_addr += obj->oop_iterate_size(&dirty_cards_for_interesting_pointers);\n+          }\n+        } \/\/ else, ignore humongous continuation region\n+      }\n+      \/\/ else, this region is FREE or YOUNG or inactive and we can ignore it.\n+      r = _regions.next();\n+    }\n+  }\n+};\n+\n@@ -102,0 +168,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -109,1 +176,7 @@\n-\n+  if (heap->mode()->is_generational()) {\n+    size_t old_available = heap->old_generation()->available();\n+    size_t young_available = heap->young_generation()->available();\n+    log_info(gc, ergo)(\"At end of Full GC, old_available: \" SIZE_FORMAT \"%s, young_available: \" SIZE_FORMAT \"%s\",\n+                       byte_size_in_proper_unit(old_available), proper_unit_for_byte_size(old_available),\n+                       byte_size_in_proper_unit(young_available), proper_unit_for_byte_size(young_available));\n+  }\n@@ -121,0 +194,18 @@\n+  \/\/ Since we may arrive here from degenerated GC failure of either young or old, establish generation as GLOBAL.\n+  heap->set_gc_generation(heap->global_generation());\n+\n+  if (heap->mode()->is_generational()) {\n+    \/\/ There will be no concurrent allocations during full GC so reset these coordination variables.\n+    heap->young_generation()->unadjust_available();\n+    heap->old_generation()->unadjust_available();\n+    \/\/ No need to old_gen->increase_used().  That was done when plabs were allocated, accounting for both old evacs and promotions.\n+\n+    heap->set_alloc_supplement_reserve(0);\n+    heap->set_young_evac_reserve(0);\n+    heap->set_old_evac_reserve(0);\n+    heap->reset_old_evac_expended();\n+    heap->set_promoted_reserve(0);\n+\n+    \/\/ Full GC supersedes any marking or coalescing in old generation.\n+    heap->cancel_old_gc();\n+  }\n@@ -164,1 +255,1 @@\n-    \/\/ b. Cancel concurrent mark, if in progress\n+    \/\/ b. Cancel all concurrent marks, if in progress\n@@ -166,2 +257,1 @@\n-      ShenandoahConcurrentGC::cancel();\n-      heap->set_concurrent_mark_in_progress(false);\n+      heap->cancel_concurrent_mark();\n@@ -177,1 +267,1 @@\n-    heap->reset_mark_bitmap();\n+    heap->global_generation()->reset_mark_bitmap();\n@@ -179,1 +269,1 @@\n-    assert(!heap->marking_context()->is_complete(), \"sanity\");\n+    assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n@@ -182,1 +272,1 @@\n-    ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+    ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -195,0 +285,1 @@\n+    \/\/ TODO: Do we need to explicitly retire PLABs?\n@@ -237,0 +328,6 @@\n+\n+    if (heap->mode()->is_generational()) {\n+      ShenandoahGCPhase phase(ShenandoahPhaseTimings::full_gc_reconstruct_remembered_set);\n+      ShenandoahReconstructRememberedSetTask task;\n+      heap->workers()->run_task(&task);\n+    }\n@@ -252,1 +349,5 @@\n-    heap->verifier()->verify_after_fullgc();\n+    if (heap->mode()->is_generational()) {\n+      heap->verifier()->verify_after_generational_fullgc();\n+    } else {\n+      heap->verifier()->verify_after_fullgc();\n+    }\n@@ -273,2 +374,4 @@\n-    _ctx->capture_top_at_mark_start(r);\n-    r->clear_live_data();\n+    if (r->affiliation() != FREE) {\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n@@ -276,0 +379,2 @@\n+\n+  bool is_thread_safe() { return true; }\n@@ -285,1 +390,1 @@\n-  heap->heap_region_iterate(&cl);\n+  heap->parallel_heap_region_iterate(&cl);\n@@ -287,1 +392,1 @@\n-  heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n@@ -289,1 +394,1 @@\n-  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahReferenceProcessor* rp = heap->global_generation()->ref_processor();\n@@ -293,1 +398,1 @@\n-  ShenandoahSTWMark mark(true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n@@ -298,0 +403,227 @@\n+class ShenandoahPrepareForCompactionTask : public WorkerTask {\n+private:\n+  PreservedMarksSet*        const _preserved_marks;\n+  ShenandoahHeap*           const _heap;\n+  ShenandoahHeapRegionSet** const _worker_slices;\n+  size_t                    const _num_workers;\n+\n+public:\n+  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices,\n+                                     size_t num_workers);\n+\n+  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n+    \/\/ Empty region: get it into the slice to defragment the slice itself.\n+    \/\/ We could have skipped this without violating correctness, but we really\n+    \/\/ want to compact all live regions to the start of the heap, which sometimes\n+    \/\/ means moving them into the fully empty regions.\n+    if (r->is_empty()) return true;\n+\n+    \/\/ Can move the region, and this is not the humongous region. Humongous\n+    \/\/ moves are special cased here, because their moves are handled separately.\n+    return r->is_stw_move_allowed() && !r->is_humongous();\n+  }\n+\n+  void work(uint worker_id);\n+};\n+\n+class ShenandoahPrepareForGenerationalCompactionObjectClosure : public ObjectClosure {\n+private:\n+  ShenandoahPrepareForCompactionTask* _compactor;\n+  PreservedMarks*          const _preserved_marks;\n+  ShenandoahHeap*          const _heap;\n+\n+  \/\/ _empty_regions is a thread-local list of heap regions that have been completely emptied by this worker thread's\n+  \/\/ compaction efforts.  The worker thread that drives these efforts adds compacted regions to this list if the\n+  \/\/ region has not been compacted onto itself.\n+  GrowableArray<ShenandoahHeapRegion*>& _empty_regions;\n+  int _empty_regions_pos;\n+  ShenandoahHeapRegion*          _old_to_region;\n+  ShenandoahHeapRegion*          _young_to_region;\n+  ShenandoahHeapRegion*          _from_region;\n+  ShenandoahRegionAffiliation    _from_affiliation;\n+  HeapWord*                      _old_compact_point;\n+  HeapWord*                      _young_compact_point;\n+  uint                           _worker_id;\n+\n+public:\n+  ShenandoahPrepareForGenerationalCompactionObjectClosure(ShenandoahPrepareForCompactionTask* compactor,\n+                                                          PreservedMarks* preserved_marks,\n+                                                          GrowableArray<ShenandoahHeapRegion*>& empty_regions,\n+                                                          ShenandoahHeapRegion* old_to_region,\n+                                                          ShenandoahHeapRegion* young_to_region, uint worker_id) :\n+      _compactor(compactor),\n+      _preserved_marks(preserved_marks),\n+      _heap(ShenandoahHeap::heap()),\n+      _empty_regions(empty_regions),\n+      _empty_regions_pos(0),\n+      _old_to_region(old_to_region),\n+      _young_to_region(young_to_region),\n+      _from_region(NULL),\n+      _old_compact_point((old_to_region != nullptr)? old_to_region->bottom(): nullptr),\n+      _young_compact_point((young_to_region != nullptr)? young_to_region->bottom(): nullptr),\n+      _worker_id(worker_id) {}\n+\n+  void set_from_region(ShenandoahHeapRegion* from_region) {\n+    _from_region = from_region;\n+    _from_affiliation = from_region->affiliation();\n+    if (_from_region->has_live()) {\n+      if (_from_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+        if (_old_to_region == nullptr) {\n+          _old_to_region = from_region;\n+          _old_compact_point = from_region->bottom();\n+        }\n+      } else {\n+        assert(_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION, \"from_region must be OLD or YOUNG\");\n+        if (_young_to_region == nullptr) {\n+          _young_to_region = from_region;\n+          _young_compact_point = from_region->bottom();\n+        }\n+      }\n+    } \/\/ else, we won't iterate over this _from_region so we don't need to set up to region to hold copies\n+  }\n+\n+  void finish() {\n+    finish_old_region();\n+    finish_young_region();\n+  }\n+\n+  void finish_old_region() {\n+    if (_old_to_region != nullptr) {\n+      log_debug(gc)(\"Planned compaction into Old Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT \" tabulated by worker %u\",\n+                    _old_to_region->index(), _old_compact_point - _old_to_region->bottom(), _worker_id);\n+      _old_to_region->set_new_top(_old_compact_point);\n+      _old_to_region = nullptr;\n+    }\n+  }\n+\n+  void finish_young_region() {\n+    if (_young_to_region != nullptr) {\n+      log_debug(gc)(\"Worker %u planned compaction into Young Region \" SIZE_FORMAT \", used: \" SIZE_FORMAT,\n+                    _worker_id, _young_to_region->index(), _young_compact_point - _young_to_region->bottom());\n+      _young_to_region->set_new_top(_young_compact_point);\n+      _young_to_region = nullptr;\n+    }\n+  }\n+\n+  bool is_compact_same_region() {\n+    return (_from_region == _old_to_region) || (_from_region == _young_to_region);\n+  }\n+\n+  int empty_regions_pos() {\n+    return _empty_regions_pos;\n+  }\n+\n+  void do_object(oop p) {\n+    assert(_from_region != NULL, \"must set before work\");\n+    assert((_from_region->bottom() <= cast_from_oop<HeapWord*>(p)) && (cast_from_oop<HeapWord*>(p) < _from_region->top()),\n+           \"Object must reside in _from_region\");\n+    assert(_heap->complete_marking_context()->is_marked(p), \"must be marked\");\n+    assert(!_heap->complete_marking_context()->allocated_after_mark_start(p), \"must be truly marked\");\n+\n+    size_t obj_size = p->size();\n+    uint from_region_age = _from_region->age();\n+    uint object_age = p->age();\n+\n+    bool promote_object = false;\n+    if ((_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) &&\n+        (from_region_age + object_age >= InitialTenuringThreshold)) {\n+      if ((_old_to_region != nullptr) && (_old_compact_point + obj_size > _old_to_region->end())) {\n+        finish_old_region();\n+        _old_to_region = nullptr;\n+      }\n+      if (_old_to_region == nullptr) {\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          ShenandoahHeapRegion* new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+          _old_to_region = new_to_region;\n+          _old_compact_point = _old_to_region->bottom();\n+          promote_object = true;\n+        }\n+        \/\/ Else this worker thread does not yet have any empty regions into which this aged object can be promoted so\n+        \/\/ we leave promote_object as false, deferring the promotion.\n+      } else {\n+        promote_object = true;\n+      }\n+    }\n+\n+    if (promote_object || (_from_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION)) {\n+      assert(_old_to_region != nullptr, \"_old_to_region should not be NULL when evacuating to OLD region\");\n+      if (_old_compact_point + obj_size > _old_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing old region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _old_to_region->index(),\n+                      p2i(_old_compact_point), obj_size, p2i(_old_compact_point + obj_size), p2i(_old_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _old_to_region.\n+        finish_old_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(OLD_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _old_to_region, we know that the _old_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _old_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != NULL, \"must not be NULL\");\n+        _old_to_region = new_to_region;\n+        _old_compact_point = _old_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_old_compact_point + obj_size <= _old_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(NULL, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_old_compact_point));\n+      _old_compact_point += obj_size;\n+    } else {\n+      assert(_from_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION,\n+             \"_from_region must be OLD_GENERATION or YOUNG_GENERATION\");\n+      assert(_young_to_region != nullptr, \"_young_to_region should not be NULL when compacting YOUNG _from_region\");\n+\n+      \/\/ After full gc compaction, all regions have age 0.  Embed the region's age into the object's age in order to preserve\n+      \/\/ tenuring progress.\n+      _heap->increase_object_age(p, from_region_age + 1);\n+\n+      if (_young_compact_point + obj_size > _young_to_region->end()) {\n+        ShenandoahHeapRegion* new_to_region;\n+\n+        log_debug(gc)(\"Worker %u finishing young region \" SIZE_FORMAT \", compact_point: \" PTR_FORMAT \", obj_size: \" SIZE_FORMAT\n+                      \", &compact_point[obj_size]: \" PTR_FORMAT \", region end: \" PTR_FORMAT,  _worker_id, _young_to_region->index(),\n+                      p2i(_young_compact_point), obj_size, p2i(_young_compact_point + obj_size), p2i(_young_to_region->end()));\n+\n+        \/\/ Object does not fit.  Get a new _young_to_region.\n+        finish_young_region();\n+        if (_empty_regions_pos < _empty_regions.length()) {\n+          new_to_region = _empty_regions.at(_empty_regions_pos);\n+          _empty_regions_pos++;\n+          new_to_region->set_affiliation(YOUNG_GENERATION);\n+        } else {\n+          \/\/ If we've exhausted the previously selected _young_to_region, we know that the _young_to_region is distinct\n+          \/\/ from _from_region.  That's because there is always room for _from_region to be compacted into itself.\n+          \/\/ Since we're out of empty regions, let's use _from_region to hold the results of its own compaction.\n+          new_to_region = _from_region;\n+        }\n+\n+        assert(new_to_region != _young_to_region, \"must not reuse same OLD to-region\");\n+        assert(new_to_region != NULL, \"must not be NULL\");\n+        _young_to_region = new_to_region;\n+        _young_compact_point = _young_to_region->bottom();\n+      }\n+\n+      \/\/ Object fits into current region, record new location:\n+      assert(_young_compact_point + obj_size <= _young_to_region->end(), \"must fit\");\n+      shenandoah_assert_not_forwarded(NULL, p);\n+      _preserved_marks->push_if_necessary(p, p->mark());\n+      p->forward_to(cast_to_oop(_young_compact_point));\n+      _young_compact_point += obj_size;\n+    }\n+  }\n+};\n+\n+\n@@ -326,0 +658,1 @@\n+    assert(!_heap->mode()->is_generational(), \"Generational GC should use different Closure\");\n@@ -371,7 +704,3 @@\n-class ShenandoahPrepareForCompactionTask : public WorkerTask {\n-private:\n-  PreservedMarksSet*        const _preserved_marks;\n-  ShenandoahHeap*           const _heap;\n-  ShenandoahHeapRegionSet** const _worker_slices;\n-public:\n-  ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks, ShenandoahHeapRegionSet **worker_slices) :\n+ShenandoahPrepareForCompactionTask::ShenandoahPrepareForCompactionTask(PreservedMarksSet *preserved_marks,\n+                                                                       ShenandoahHeapRegionSet **worker_slices,\n+                                                                       size_t num_workers) :\n@@ -380,2 +709,12 @@\n-    _preserved_marks(preserved_marks),\n-    _heap(ShenandoahHeap::heap()), _worker_slices(worker_slices) {\n+    _preserved_marks(preserved_marks), _heap(ShenandoahHeap::heap()),\n+    _worker_slices(worker_slices), _num_workers(num_workers) { }\n+\n+\n+void ShenandoahPrepareForCompactionTask::work(uint worker_id) {\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+  ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n+  ShenandoahHeapRegionSetIterator it(slice);\n+  ShenandoahHeapRegion* from_region = it.next();\n+  \/\/ No work?\n+  if (from_region == NULL) {\n+    return;\n@@ -384,6 +723,3 @@\n-  static bool is_candidate_region(ShenandoahHeapRegion* r) {\n-    \/\/ Empty region: get it into the slice to defragment the slice itself.\n-    \/\/ We could have skipped this without violating correctness, but we really\n-    \/\/ want to compact all live regions to the start of the heap, which sometimes\n-    \/\/ means moving them into the fully empty regions.\n-    if (r->is_empty()) return true;\n+  \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n+  \/\/ Remember empty regions and reuse them as needed.\n+  ResourceMark rm;\n@@ -391,4 +727,1 @@\n-    \/\/ Can move the region, and this is not the humongous region. Humongous\n-    \/\/ moves are special cased here, because their moves are handled separately.\n-    return r->is_stw_move_allowed() && !r->is_humongous();\n-  }\n+  GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n@@ -396,13 +729,14 @@\n-  void work(uint worker_id) {\n-    ShenandoahParallelWorkerSession worker_session(worker_id);\n-    ShenandoahHeapRegionSet* slice = _worker_slices[worker_id];\n-    ShenandoahHeapRegionSetIterator it(slice);\n-    ShenandoahHeapRegion* from_region = it.next();\n-    \/\/ No work?\n-    if (from_region == NULL) {\n-       return;\n-    }\n-\n-    \/\/ Sliding compaction. Walk all regions in the slice, and compact them.\n-    \/\/ Remember empty regions and reuse them as needed.\n-    ResourceMark rm;\n+  if (_heap->mode()->is_generational()) {\n+    ShenandoahHeapRegion* old_to_region = (from_region->is_old())? from_region: nullptr;\n+    ShenandoahHeapRegion* young_to_region = (from_region->is_young())? from_region: nullptr;\n+    ShenandoahPrepareForGenerationalCompactionObjectClosure cl(this, _preserved_marks->get(worker_id), empty_regions,\n+                                                               old_to_region, young_to_region, worker_id);\n+    while (from_region != NULL) {\n+      assert(is_candidate_region(from_region), \"Sanity\");\n+      log_debug(gc)(\"Worker %u compacting %s Region \" SIZE_FORMAT \" which had used \" SIZE_FORMAT \" and %s live\",\n+                    worker_id, affiliation_name(from_region->affiliation()),\n+                    from_region->index(), from_region->used(), from_region->has_live()? \"has\": \"does not have\");\n+      cl.set_from_region(from_region);\n+      if (from_region->has_live()) {\n+        _heap->marked_object_iterate(from_region, &cl);\n+      }\n@@ -410,1 +744,7 @@\n-    GrowableArray<ShenandoahHeapRegion*> empty_regions((int)_heap->num_regions());\n+      \/\/ Compacted the region to somewhere else? From-region is empty then.\n+      if (!cl.is_compact_same_region()) {\n+        empty_regions.append(from_region);\n+      }\n+      from_region = it.next();\n+    }\n+    cl.finish();\n@@ -412,0 +752,6 @@\n+    \/\/ Mark all remaining regions as empty\n+    for (int pos = cl.empty_regions_pos(); pos < empty_regions.length(); ++pos) {\n+      ShenandoahHeapRegion* r = empty_regions.at(pos);\n+      r->set_new_top(r->bottom());\n+    }\n+  } else {\n@@ -413,1 +759,0 @@\n-\n@@ -416,1 +761,0 @@\n-\n@@ -436,1 +780,1 @@\n-};\n+}\n@@ -455,0 +799,1 @@\n+  log_debug(gc)(\"Full GC calculating target humongous objects from end \" SIZE_FORMAT, to_end);\n@@ -493,0 +838,1 @@\n+    bool is_generational = _heap->mode()->is_generational();\n@@ -497,0 +843,1 @@\n+      \/\/ Leave afffiliation unchanged.\n@@ -521,17 +868,22 @@\n-    if (r->is_humongous_start()) {\n-      oop humongous_obj = cast_to_oop(r->bottom());\n-      if (!_ctx->is_marked(humongous_obj)) {\n-        assert(!r->has_live(),\n-               \"Region \" SIZE_FORMAT \" is not marked, should not have live\", r->index());\n-        _heap->trash_humongous_region_at(r);\n-      } else {\n-        assert(r->has_live(),\n-               \"Region \" SIZE_FORMAT \" should have live\", r->index());\n-      }\n-    } else if (r->is_humongous_continuation()) {\n-      \/\/ If we hit continuation, the non-live humongous starts should have been trashed already\n-      assert(r->humongous_start_region()->has_live(),\n-             \"Region \" SIZE_FORMAT \" should have live\", r->index());\n-    } else if (r->is_regular()) {\n-      if (!r->has_live()) {\n-        r->make_trash_immediate();\n+    if (r->affiliation() != FREE) {\n+      if (r->is_humongous_start()) {\n+        oop humongous_obj = cast_to_oop(r->bottom());\n+        if (!_ctx->is_marked(humongous_obj)) {\n+          assert(!r->has_live(),\n+                 \"Humongous Start %s Region \" SIZE_FORMAT \" is not marked, should not have live\",\n+                 affiliation_name(r->affiliation()),  r->index());\n+          log_debug(gc)(\"Trashing immediate humongous region \" SIZE_FORMAT \" because not marked\", r->index());\n+          _heap->trash_humongous_region_at(r);\n+        } else {\n+          assert(r->has_live(),\n+                 \"Humongous Start %s Region \" SIZE_FORMAT \" should have live\", affiliation_name(r->affiliation()),  r->index());\n+        }\n+      } else if (r->is_humongous_continuation()) {\n+        \/\/ If we hit continuation, the non-live humongous starts should have been trashed already\n+        assert(r->humongous_start_region()->has_live(),\n+               \"Humongous Continuation %s Region \" SIZE_FORMAT \" should have live\", affiliation_name(r->affiliation()),  r->index());\n+      } else if (r->is_regular()) {\n+        if (!r->has_live()) {\n+          log_debug(gc)(\"Trashing immediate regular region \" SIZE_FORMAT \" because has no live\", r->index());\n+          r->make_trash_immediate();\n+        }\n@@ -540,0 +892,2 @@\n+    \/\/ else, ignore this FREE region.\n+    \/\/ TODO: change iterators so they do not process FREE regions.\n@@ -706,0 +1060,5 @@\n+  if (heap->mode()->is_generational()) {\n+    heap->young_generation()->clear_used();\n+    heap->old_generation()->clear_used();\n+  }\n+\n@@ -712,1 +1071,4 @@\n-    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices);\n+    size_t num_workers = heap->max_workers();\n+\n+    ResourceMark rm;\n+    ShenandoahPrepareForCompactionTask task(_preserved_marks, worker_slices, num_workers);\n@@ -786,0 +1148,7 @@\n+      if (r->is_pinned() && r->is_old() && r->is_active() && !r->is_humongous()) {\n+        \/\/ Pinned regions are not compacted so they may still hold unmarked objects with\n+        \/\/ reference to reclaimed memory. Remembered set scanning will crash if it attempts\n+        \/\/ to iterate the oops in these objects.\n+        r->begin_preemptible_coalesce_and_fill();\n+        r->oop_fill_and_coalesce_wo_cancel();\n+      }\n@@ -898,0 +1267,1 @@\n+    bool is_generational = _heap->mode()->is_generational();\n@@ -912,0 +1282,4 @@\n+      if (!is_generational) {\n+        r->make_young_maybe();\n+      }\n+      \/\/ else, generational mode compaction has already established affiliation.\n@@ -926,0 +1300,9 @@\n+    \/\/ Update final usage for generations\n+    if (is_generational && live != 0) {\n+      if (r->is_young()) {\n+        _heap->young_generation()->increase_used(live);\n+      } else if (r->is_old()) {\n+        _heap->old_generation()->increase_used(live);\n+      }\n+    }\n+\n@@ -963,2 +1346,7 @@\n-      Copy::aligned_conjoint_words(r->bottom(), heap->get_region(new_start)->bottom(), words_size);\n-      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(r->bottom()));\n+      ContinuationGCSupport::relativize_stack_chunk(cast_to_oop<HeapWord*>(heap->get_region(old_start)->bottom()));\n+      log_debug(gc)(\"Full GC compaction moves humongous object from region \" SIZE_FORMAT \" to region \" SIZE_FORMAT,\n+                    old_start, new_start);\n+\n+      Copy::aligned_conjoint_words(heap->get_region(old_start)->bottom(),\n+                                   heap->get_region(new_start)->bottom(),\n+                                   words_size);\n@@ -970,0 +1358,1 @@\n+        ShenandoahRegionAffiliation original_affiliation = r->affiliation();\n@@ -972,0 +1361,1 @@\n+          \/\/ Leave humongous region affiliation unchanged.\n@@ -979,1 +1369,1 @@\n-            r->make_humongous_start_bypass();\n+            r->make_humongous_start_bypass(original_affiliation);\n@@ -981,1 +1371,1 @@\n-            r->make_humongous_cont_bypass();\n+            r->make_humongous_cont_bypass(original_affiliation);\n@@ -1060,0 +1450,5 @@\n+    if (heap->mode()->is_generational()) {\n+      heap->young_generation()->clear_used();\n+      heap->old_generation()->clear_used();\n+    }\n+\n@@ -1063,0 +1458,4 @@\n+    if (heap->mode()->is_generational()) {\n+      log_info(gc)(\"FullGC done: GLOBAL usage: \" SIZE_FORMAT \", young usage: \" SIZE_FORMAT \", old usage: \" SIZE_FORMAT,\n+                    post_compact.get_live(), heap->young_generation()->used(), heap->old_generation()->used());\n+    }\n@@ -1068,1 +1467,1 @@\n-  heap->clear_cancelled_gc();\n+  heap->clear_cancelled_gc(true \/* clear oom handler *\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":472,"deletions":73,"binary":false,"changes":545,"status":"modified"},{"patch":"@@ -77,14 +77,0 @@\n-class FileLocker : public StackObj {\n-private:\n-  FILE *_file;\n-\n-public:\n-  FileLocker(FILE *file) : _file(file) {\n-    os::flockfile(_file);\n-  }\n-\n-  ~FileLocker() {\n-    os::funlockfile(_file);\n-  }\n-};\n-\n","filename":"src\/hotspot\/share\/logging\/logFileStreamOutput.cpp","additions":0,"deletions":14,"binary":false,"changes":14,"status":"modified"}]}