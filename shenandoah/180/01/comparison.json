{"files":[{"patch":"@@ -136,1 +136,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -865,1 +865,1 @@\n-      log_info(gc, ergo)(\"Squelching additional promotion failure reports for epoch \" SIZE_FORMAT, last_report_epoch);\n+      log_info(gc, ergo)(\"Squelching additional promotion failure reports for current epoch\");\n@@ -1105,0 +1105,1 @@\n+\/\/ Called from stubs in JIT code or interpreter\n@@ -1172,1 +1173,0 @@\n-\n@@ -1174,1 +1174,0 @@\n-\n@@ -1180,1 +1179,0 @@\n-\n@@ -1186,1 +1184,0 @@\n-\n@@ -1228,35 +1225,29 @@\n-  \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n-  bool promotion_eligible = false;\n-  bool allow_allocation = true;\n-  bool plab_alloc = false;\n-  size_t requested_bytes = req.size() * HeapWordSize;\n-  HeapWord* result = nullptr;\n-  ShenandoahHeapLocker locker(lock());\n-  Thread* thread = Thread::current();\n-  if (mode()->is_generational()) {\n-    if (req.affiliation() == YOUNG_GENERATION) {\n-      if (req.is_mutator_alloc()) {\n-        if (requested_bytes >= young_generation()->adjusted_available()) {\n-          \/\/ We know this is not a GCLAB.  This must be a TLAB or a shared allocation.  Reject the allocation request if\n-          \/\/ exceeds established capacity limits.\n-\n-          \/\/ TODO: if ShenandoahElasticTLAB and req.is_lab_alloc(), we should endeavor to shrink the TLAB request\n-          \/\/ in order to avoid allocation failure and degeneration of GC.\n-\n-          log_info(gc, ergo)(\"Rejecting mutator alloc of \" SIZE_FORMAT \" because young available is: \" SIZE_FORMAT,\n-                             requested_bytes, young_generation()->adjusted_available());\n-          return nullptr;\n-        }\n-      }\n-    } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n-      assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n-      if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n-        plab_alloc = true;\n-        size_t promotion_avail = get_promoted_reserve();\n-        size_t promotion_expended = get_promoted_expended();\n-        if (promotion_expended + requested_bytes > promotion_avail) {\n-          promotion_avail = 0;\n-          if (get_old_evac_reserve() == 0) {\n-            \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n-            \/\/ be used for promotions.\n-            allow_allocation = false;\n+  bool try_smaller_lab_size = false;\n+  size_t smaller_lab_size;\n+  {\n+    \/\/ promotion_eligible pertains only to PLAB allocations, denoting that the PLAB is allowed to allocate for promotions.\n+    bool promotion_eligible = false;\n+    bool allow_allocation = true;\n+    bool plab_alloc = false;\n+    size_t requested_bytes = req.size() * HeapWordSize;\n+    HeapWord* result = nullptr;\n+    ShenandoahHeapLocker locker(lock());\n+    Thread* thread = Thread::current();\n+\n+    if (mode()->is_generational()) {\n+      if (req.affiliation() == YOUNG_GENERATION) {\n+        if (req.is_mutator_alloc()) {\n+          size_t young_available = young_generation()->adjusted_available();\n+          if (requested_bytes > young_available) {\n+            \/\/ We know this is not a GCLAB.  This must be a TLAB or a shared allocation.\n+            if (req.is_lab_alloc() && (young_available >= req.min_size())) {\n+              try_smaller_lab_size = true;\n+              smaller_lab_size = young_available \/ HeapWordSize;\n+            } else {\n+              \/\/ Can't allocate because even min_size() is larger than remaining young_available\n+              log_info(gc, ergo)(\"Unable to shrink %s alloc request of minimum size: \" SIZE_FORMAT\n+                                 \", young available: \" SIZE_FORMAT,\n+                                 req.is_lab_alloc()? \"TLAB\": \"shared\",\n+                                 HeapWordSize * (req.is_lab_alloc()? req.min_size(): req.size()), young_available);\n+              return nullptr;\n+            }\n@@ -1264,18 +1255,0 @@\n-        } else {\n-          promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n-          promotion_eligible = true;\n-        }\n-      } else if (is_promotion) {\n-        \/\/ This is a shared alloc for promotion\n-        size_t promotion_avail = get_promoted_reserve();\n-        size_t promotion_expended = get_promoted_expended();\n-        if (promotion_expended + requested_bytes > promotion_avail) {\n-          promotion_avail = 0;\n-        } else {\n-          promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n-        }\n-\n-        if (promotion_avail == 0) {\n-          \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n-          \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n-          return nullptr;\n@@ -1283,11 +1256,2 @@\n-        \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n-      } else {\n-        \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n-      }\n-    }\n-  }\n-  result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n-  if (result != NULL) {\n-    if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n-      ShenandoahThreadLocalData::reset_plab_promoted(thread);\n-      if (req.is_gc_alloc()) {\n+      } else {                    \/\/ reg.affiliation() == OLD_GENERATION\n+        assert(req.type() != ShenandoahAllocRequest::_alloc_gclab, \"GCLAB pertains only to young-gen memory\");\n@@ -1295,8 +1259,10 @@\n-          if (promotion_eligible) {\n-            size_t actual_size = req.actual_size() * HeapWordSize;\n-            \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n-            \/\/ When we retire this plab, we'll unexpend what we don't really use.\n-            ShenandoahThreadLocalData::enable_plab_promotions(thread);\n-            expend_promoted(actual_size);\n-            assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n-            ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+          plab_alloc = true;\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+            if (get_old_evac_reserve() == 0) {\n+              \/\/ There are no old-gen evacuations in this pass.  There's no value in creating a plab that cannot\n+              \/\/ be used for promotions.\n+              allow_allocation = false;\n+            }\n@@ -1304,3 +1270,2 @@\n-            \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n-            ShenandoahThreadLocalData::disable_plab_promotions(thread);\n-            ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+            promotion_eligible = true;\n@@ -1309,3 +1274,16 @@\n-          \/\/ Shared promotion.  Assume size is requested_bytes.\n-          expend_promoted(requested_bytes);\n-          assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+          \/\/ This is a shared alloc for promotion\n+          size_t promotion_avail = get_promoted_reserve();\n+          size_t promotion_expended = get_promoted_expended();\n+          if (promotion_expended + requested_bytes > promotion_avail) {\n+            promotion_avail = 0;\n+          } else {\n+            promotion_avail = promotion_avail - (promotion_expended + requested_bytes);\n+          }\n+          if (promotion_avail == 0) {\n+            \/\/ We need to reserve the remaining memory for evacuation.  Reject this allocation.  The object will be\n+            \/\/ evacuated to young-gen memory and promoted during a future GC pass.\n+            return nullptr;\n+          }\n+          \/\/ Else, we'll allow the allocation to proceed.  (Since we hold heap lock, the tested condition remains true.)\n+        } else {\n+          \/\/ This is a shared allocation for evacuation.  Memory has already been reserved for this purpose.\n@@ -1314,0 +1292,28 @@\n+    } \/\/ This ends the is_generational() block\n+\n+    if (!try_smaller_lab_size) {\n+      result = (allow_allocation)? _free_set->allocate(req, in_new_region): nullptr;\n+      if (result != NULL) {\n+        if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+          ShenandoahThreadLocalData::reset_plab_promoted(thread);\n+          if (req.is_gc_alloc()) {\n+            if (req.type() ==  ShenandoahAllocRequest::_alloc_plab) {\n+              if (promotion_eligible) {\n+                size_t actual_size = req.actual_size() * HeapWordSize;\n+                \/\/ Assume the entirety of this PLAB will be used for promotion.  This prevents promotion from overreach.\n+                \/\/ When we retire this plab, we'll unexpend what we don't really use.\n+                ShenandoahThreadLocalData::enable_plab_promotions(thread);\n+                expend_promoted(actual_size);\n+                assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, actual_size);\n+              } else {\n+                \/\/ Disable promotions in this thread because entirety of this PLAB must be available to hold old-gen evacuations.\n+                ShenandoahThreadLocalData::disable_plab_promotions(thread);\n+                ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+              }\n+            } else if (is_promotion) {\n+              \/\/ Shared promotion.  Assume size is requested_bytes.\n+              expend_promoted(requested_bytes);\n+              assert(get_promoted_expended() <= get_promoted_reserve(), \"Do not expend more promotion than budgeted\");\n+            }\n+          }\n@@ -1315,26 +1321,29 @@\n-      \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n-      \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n-      \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n-      \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n-      \/\/\n-      \/\/ objects being \"concurrently\" allocated:\n-      \/\/    [-----a------][-----b-----][--------------c------------------]\n-      \/\/            [---- card table memory range --------------]\n-      \/\/\n-      \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n-      \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n-      \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n-      \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n-      \/\/\n-      \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n-      \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n-      \/\/ invocations to be \"mutually exclusive\" with respect to each card's memory range.\n-      ShenandoahHeap::heap()->card_scan()->register_object(result);\n-    }\n-  } else {\n-    \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n-    if ((req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && req.is_gc_alloc() &&\n-        (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n-      \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n-      \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n-      ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+          \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+          \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+          \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+          \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+          \/\/\n+          \/\/ objects being \"concurrently\" allocated:\n+          \/\/    [-----a------][-----b-----][--------------c------------------]\n+          \/\/            [---- card table memory range --------------]\n+          \/\/\n+          \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that allocation of object a\n+          \/\/   wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+          \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+          \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+          \/\/\n+          \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as\n+          \/\/ last-start representing object b while first-start represents object c.  This is why we need to require all\n+          \/\/ register_object() invocations to be \"mutually exclusive\" with respect to each card's memory range.\n+          ShenandoahHeap::heap()->card_scan()->register_object(result);\n+        }\n+      } else {\n+        \/\/ The allocation failed.  If this was a plab allocation, We've already retired it and no longer have a plab.\n+        if ((req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && req.is_gc_alloc() &&\n+            (req.type() == ShenandoahAllocRequest::_alloc_plab)) {\n+          \/\/ We don't need to disable PLAB promotions because there is no PLAB.  We leave promotions enabled because\n+          \/\/ this allows the surrounding infrastructure to retry alloc_plab_slow() with a smaller PLAB size.\n+          ShenandoahThreadLocalData::set_plab_preallocated_promoted(thread, 0);\n+        }\n+      }\n+      return result;\n@@ -1342,0 +1351,37 @@\n+    \/\/ else, try_smaller_lab_size is true so we fall through and recurse with a smaller lab size\n+  } \/\/ This closes the block that holds the heap lock.  This releases the lock.\n+\n+  \/\/ We arrive here if the tlab allocation request can be resized to fit within young_available\n+  assert((req.affiliation() == YOUNG_GENERATION) && req.is_lab_alloc() && req.is_mutator_alloc() &&\n+         (smaller_lab_size < req.size()), \"Only shrink allocation request size for TLAB allocations\");\n+\n+  \/\/ By convention, ShenandoahAllocationRequest is primarily read-only.  The only mutable instance data is represented by\n+  \/\/ actual_size(), which is overwritten with the size of the allocaion when the allocation request is satisfied.  We use a\n+  \/\/ recursive call here rather than introducing new methods to mutate the existing ShenandoahAllocationRequest argument.\n+  \/\/ Mutation of the existing object might result in astonishing results if calling contexts assume the content of immutable\n+  \/\/ fields remain constant.  The original TLAB allocation request was for memory that exceeded the current capacity.  We'll\n+  \/\/ attempt to allocate a smaller TLAB.  If this is successful, we'll update actual_size() of our incoming\n+  \/\/ ShenandoahAllocRequest.  If the recursive request fails, we'll simply return nullptr.\n+\n+  \/\/ Note that we've relinquished the HeapLock and some other thread may perform additional allocation before our recursive\n+  \/\/ call reacquires the lock.  If that happens, we will need another recursive call to further reduce the size of our request\n+  \/\/ for each time another thread allocates young memory during the brief intervals that the heap lock is available to\n+  \/\/ interfering threads.  We expect this interference to be rare.  The recursion bottoms out when young_available is\n+  \/\/ smaller than req.min_size().  The inner-nested call to allocate_memory_under_lock() uses the same min_size() value\n+  \/\/ as this call, but it uses a preferred size() that is smaller than our preferred size, and is no larger than what we most\n+  \/\/ recently saw as the memory currently available within the young generation.\n+\n+  \/\/ TODO: At the expense of code clarity, we could rewrite this recursive solution to use iteration.  We need at most one\n+  \/\/ extra instance of the ShenandoahAllocRequest, which we can re-initialize multiple times inside a loop, with one iteration\n+  \/\/ of the loop required for each time the existing solution would recurse.  An iterative solution would be more efficient\n+  \/\/ in CPU time and stack memory utilization.  The expectation is that it is very rare that we would recurse more than once\n+  \/\/ so making this change is not currently seen as a high priority.\n+\n+  ShenandoahAllocRequest smaller_req = ShenandoahAllocRequest::for_tlab(req.min_size(), smaller_lab_size);\n+\n+  \/\/ Note that shrinking the preferred size gets us past the gatekeeper that checks whether there's available memory to\n+  \/\/ satisfy the allocation request.  The reality is the actual TLAB size is likely to be even smaller, because it will\n+  \/\/ depend on how much memory is available within mutator regions that are not yet fully used.\n+  HeapWord* result = allocate_memory_under_lock(smaller_req, in_new_region, is_promotion);\n+  if (result != nullptr) {\n+    req.set_actual_size(smaller_req.actual_size());\n@@ -1707,3 +1753,7 @@\n-    \/\/ With Elastic TLABs, return the max allowed size, and let the allocation path\n-    \/\/ figure out the safe size for current allocation.\n-    return ShenandoahHeapRegion::max_tlab_size_bytes();\n+    if (mode()->is_generational()) {\n+      return MIN2(ShenandoahHeapRegion::max_tlab_size_bytes(), young_generation()->adjusted_available());\n+    } else {\n+      \/\/ With Elastic TLABs, return the max allowed size, and let the allocation path\n+      \/\/ figure out the safe size for current allocation.\n+      return ShenandoahHeapRegion::max_tlab_size_bytes();\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":162,"deletions":112,"binary":false,"changes":274,"status":"modified"}]}