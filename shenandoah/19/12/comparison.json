{"files":[{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -90,0 +91,2 @@\n+\n+      post_barrier(access, access.resolved_addr(), new_value.result());\n@@ -93,1 +96,8 @@\n-  return BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  LIR_Opr result =  BarrierSetC1::atomic_cmpxchg_at_resolved(access, cmp_value, new_value);\n+\n+  if (access.is_oop()) {\n+    post_barrier(access, access.resolved_addr(), new_value.result());\n+  }\n+\n+  return result;\n@@ -121,0 +131,1 @@\n+    post_barrier(access, access.resolved_addr(), result);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_aarch64.cpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -63,1 +64,1 @@\n-        __ mov(rscratch2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING);\n+        __ mov(rscratch2, ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::OLD_MARKING);\n@@ -80,0 +81,7 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                                       Register start, Register count, Register tmp, RegSet saved_regs) {\n+  if (is_oop) {\n+    gen_write_ref_array_post_barrier(masm, decorators, start, count, tmp, saved_regs);\n+  }\n+}\n+\n@@ -369,0 +377,29 @@\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+      return;\n+  }\n+\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n+  CardTable* ct = ctbs->card_table();\n+\n+  __ lsr(obj, obj, CardTable::card_shift);\n+\n+  assert(CardTable::dirty_card_val() == 0, \"must be\");\n+\n+  __ load_byte_map_base(rscratch1);\n+\n+  if (UseCondCardMark) {\n+    Label L_already_dirty;\n+    __ membar(Assembler::StoreLoad);\n+    __ ldrb(rscratch2,  Address(obj, rscratch1));\n+    __ cbz(rscratch2, L_already_dirty);\n+    __ strb(zr, Address(obj, rscratch1));\n+    __ bind(L_already_dirty);\n+  } else {\n+    \/\/ if (ct->scanned_concurrently()) {\n+    \/\/  __ membar(Assembler::StoreStore);\n+    \/\/ }\n+    __ strb(zr, Address(obj, rscratch1));\n+  }\n+}\n+\n@@ -405,0 +442,1 @@\n+    store_check(masm, r3);\n@@ -589,0 +627,33 @@\n+void ShenandoahBarrierSetAssembler::gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators,\n+                                                                     Register start, Register count, Register scratch, RegSet saved_regs) {\n+  if (!ShenandoahHeap::heap()->mode()->is_generational()) {\n+    return;\n+  }\n+\n+  BarrierSet* bs = BarrierSet::barrier_set();\n+  CardTableBarrierSet* ctbs = barrier_set_cast<CardTableBarrierSet>(bs);\n+  CardTable* ct = ctbs->card_table();\n+\n+  Label L_loop, L_done;\n+  const Register end = count;\n+\n+  __ cbz(count, L_done); \/\/ zero count - nothing to do\n+\n+  __ lea(end, Address(start, count, Address::lsl(LogBytesPerHeapOop))); \/\/ end = start + count << LogBytesPerHeapOop\n+  __ sub(end, end, BytesPerHeapOop); \/\/ last element address to make inclusive\n+  __ lsr(start, start, CardTable::card_shift);\n+  __ lsr(end, end, CardTable::card_shift);\n+  __ sub(count, end, start); \/\/ number of bytes to copy\n+\n+  __ load_byte_map_base(scratch);\n+  __ add(start, start, scratch);\n+  \/\/ if (ct->scanned_concurrently()) {\n+  \/\/   __ membar(__ StoreStore);\n+  \/\/ }\n+  __ bind(L_loop);\n+  __ strb(zr, Address(start, count));\n+  __ subs(count, count, 1);\n+  __ br(Assembler::GE, L_loop);\n+  __ bind(L_done);\n+}\n+\n@@ -689,1 +760,1 @@\n-  __ mov(rscratch2, ShenandoahHeap::MARKING);\n+  __ mov(rscratch2, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.cpp","additions":73,"deletions":2,"binary":false,"changes":75,"status":"modified"},{"patch":"@@ -57,0 +57,2 @@\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n@@ -61,0 +63,2 @@\n+  void gen_write_ref_array_post_barrier(MacroAssembler* masm, DecoratorSet decorators, Register start, Register count, Register scratch, RegSet saved_regs);\n+\n@@ -74,0 +78,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, bool is_oop,\n+                                  Register start, Register count, Register tmp, RegSet saved_regs);\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shenandoah\/shenandoahBarrierSetAssembler_aarch64.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -90,0 +90,2 @@\n+\n+      post_barrier(access, access.resolved_addr(), new_value.result());\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/c1\/shenandoahBarrierSetC1_x86.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -155,1 +155,1 @@\n-        flags = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::MARKING;\n+        flags = ShenandoahHeap::HAS_FORWARDED | ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING;\n@@ -185,0 +185,33 @@\n+void ShenandoahBarrierSetAssembler::arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                                       Register src, Register dst, Register count) {\n+  bool checkcast = (decorators & ARRAYCOPY_CHECKCAST) != 0;\n+  bool disjoint = (decorators & ARRAYCOPY_DISJOINT) != 0;\n+  bool obj_int = type == T_OBJECT LP64_ONLY(&& UseCompressedOops);\n+#ifdef _LP64\n+  Register tmp = rscratch1;\n+#else\n+  Register tmp = rax;\n+#endif\n+\n+if (is_reference_type(type)) {\n+#ifdef _LP64\n+#if COMPILER2_OR_JVMCI\n+    if (VM_Version::supports_avx512vlbw() && MaxVectorSize  >= 32 && !checkcast) {\n+      if (!obj_int) {\n+        \/\/ Save count for barrier\n+        count = r11;\n+      } else if (disjoint) {\n+        \/\/ Use the saved dst in the disjoint case\n+        dst = r11;\n+      }\n+    }\n+#  endif\n+#else\n+    if (disjoint) {\n+      __ mov(dst, rdx); \/\/ restore 'to'\n+    }\n+#endif\n+    gen_write_ref_array_post_barrier(masm, decorators, dst, count, tmp);\n+  }\n+}\n+\n@@ -228,1 +261,1 @@\n-  __ testb(gc_state, ShenandoahHeap::MARKING);\n+  __ testb(gc_state, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n@@ -590,1 +623,1 @@\n-void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj, Address dst) {\n+void ShenandoahBarrierSetAssembler::store_check(MacroAssembler* masm, Register obj) {\n@@ -597,1 +630,0 @@\n-  BarrierSet* bs = BarrierSet::barrier_set();\n@@ -599,1 +631,1 @@\n-  ShenandoahBarrierSet* ctbs = barrier_set_cast<ShenandoahBarrierSet>(bs);\n+  ShenandoahBarrierSet* ctbs = ShenandoahBarrierSet::barrier_set();\n@@ -682,1 +714,1 @@\n-      store_check(masm, tmp1, dst);\n+      store_check(masm, tmp1);\n@@ -1046,1 +1078,1 @@\n-  __ testb(gc_state, ShenandoahHeap::MARKING);\n+  __ testb(gc_state, ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.cpp","additions":39,"deletions":7,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -61,1 +61,2 @@\n-  void store_check(MacroAssembler* masm, Register obj, Address dst);\n+  void store_check(MacroAssembler* masm, Register obj);\n+\n@@ -80,0 +81,2 @@\n+  virtual void arraycopy_epilogue(MacroAssembler* masm, DecoratorSet decorators, BasicType type,\n+                                  Register src, Register dst, Register count);\n","filename":"src\/hotspot\/cpu\/x86\/gc\/shenandoah\/shenandoahBarrierSetAssembler_x86.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -245,1 +245,1 @@\n-  marking = __ AndI(ld, __ ConI(ShenandoahHeap::MARKING));\n+  marking = __ AndI(ld, __ ConI(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n@@ -326,1 +326,1 @@\n-      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::MARKING)) {\n+      cmpx->in(1)->in(2) == phase->intcon(ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING)) {\n@@ -953,1 +953,1 @@\n-      flags |= ShenandoahHeap::MARKING;\n+      flags |= ShenandoahHeap::YOUNG_MARKING;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahBarrierSetC2.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1503,1 +1503,1 @@\n-    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, ShenandoahHeap::MARKING);\n+    test_gc_state(ctrl, raw_mem, heap_stable_ctrl, phase, (ShenandoahHeap::YOUNG_MARKING | ShenandoahHeap::OLD_MARKING));\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/c2\/shenandoahSupport.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -57,2 +57,2 @@\n-ShenandoahAdaptiveHeuristics::ShenandoahAdaptiveHeuristics() :\n-  ShenandoahHeuristics(),\n+ShenandoahAdaptiveHeuristics::ShenandoahAdaptiveHeuristics(ShenandoahGeneration* generation) :\n+  ShenandoahHeuristics(generation),\n@@ -87,1 +87,1 @@\n-  size_t capacity    = ShenandoahHeap::heap()->soft_max_capacity();\n+  size_t capacity    = _generation->soft_max_capacity();\n@@ -199,5 +199,4 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n-  size_t allocated = heap->bytes_allocated_since_gc_start();\n+  size_t max_capacity = _generation->max_capacity();\n+  size_t capacity = _generation->soft_max_capacity();\n+  size_t available = _generation->available();\n+  size_t allocated = _generation->bytes_allocated_since_gc_start();\n@@ -221,0 +220,1 @@\n+  \/\/ Check if are need to learn a bit about the application\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -56,1 +56,1 @@\n-  ShenandoahAdaptiveHeuristics();\n+  ShenandoahAdaptiveHeuristics(ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,1 +34,1 @@\n-ShenandoahAggressiveHeuristics::ShenandoahAggressiveHeuristics() : ShenandoahHeuristics() {\n+ShenandoahAggressiveHeuristics::ShenandoahAggressiveHeuristics(ShenandoahGeneration* generation) : ShenandoahHeuristics(generation) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  ShenandoahAggressiveHeuristics();\n+  ShenandoahAggressiveHeuristics(ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,1 +35,2 @@\n-ShenandoahCompactHeuristics::ShenandoahCompactHeuristics() : ShenandoahHeuristics() {\n+ShenandoahCompactHeuristics::ShenandoahCompactHeuristics(ShenandoahGeneration* generation) :\n+  ShenandoahHeuristics(generation) {\n@@ -48,5 +49,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n+  size_t max_capacity = _generation->max_capacity();\n+  size_t capacity = _generation->soft_max_capacity();\n+  size_t available = _generation->available();\n@@ -68,1 +67,1 @@\n-  size_t bytes_allocated = heap->bytes_allocated_since_gc_start();\n+  size_t bytes_allocated = _generation->bytes_allocated_since_gc_start();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.cpp","additions":7,"deletions":8,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  ShenandoahCompactHeuristics();\n+  ShenandoahCompactHeuristics(ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -29,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -33,0 +35,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -45,1 +48,1 @@\n-ShenandoahHeuristics::ShenandoahHeuristics() :\n+ShenandoahHeuristics::ShenandoahHeuristics(ShenandoahGeneration* generation) :\n@@ -54,1 +57,2 @@\n-  _metaspace_oom()\n+  _metaspace_oom(),\n+  _generation(generation)\n@@ -95,1 +99,1 @@\n-  ShenandoahMarkingContext* const ctx = heap->complete_marking_context();\n+  ShenandoahMarkingContext* const ctx = _generation->complete_marking_context();\n@@ -99,1 +103,1 @@\n-    if (heap->mode()->is_generational() && region->affiliation() != ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+    if (!in_generation(region)) {\n@@ -110,1 +114,1 @@\n-      if (!region->has_live()) {\n+      if (!region->has_live() && !heap->mode()->is_generational()) {\n@@ -115,1 +119,6 @@\n-      } else {\n+      } else if (_generation->generation_mode() != OLD) {\n+        \/\/ HEY! At this stage in development our concurrent old\n+        \/\/ marking does NOT complete the subsequent phases of the collection\n+        \/\/ and we don't want regions stuck in the 'in_cset' state because\n+        \/\/ various asserts will trip.\n+\n@@ -122,0 +131,1 @@\n+\n@@ -294,0 +304,6 @@\n+\n+bool ShenandoahHeuristics::in_generation(ShenandoahHeapRegion* region) {\n+  return (_generation->generation_mode() == GLOBAL)\n+      || (_generation->generation_mode() == YOUNG && region->affiliation() == YOUNG_GENERATION)\n+      || (_generation->generation_mode() == OLD && region->affiliation() == OLD_GENERATION);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.cpp","additions":22,"deletions":6,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -87,0 +87,2 @@\n+  ShenandoahGeneration* _generation;\n+\n@@ -95,0 +97,2 @@\n+  bool in_generation(ShenandoahHeapRegion* region);\n+\n@@ -96,1 +100,1 @@\n-  ShenandoahHeuristics();\n+  ShenandoahHeuristics(ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,3 @@\n+  ShenandoahPassiveHeuristics(ShenandoahGeneration* generation)\n+    : ShenandoahHeuristics(generation) {}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahPassiveHeuristics.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,1 +30,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -32,0 +31,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -35,1 +35,2 @@\n-ShenandoahStaticHeuristics::ShenandoahStaticHeuristics() : ShenandoahHeuristics() {\n+ShenandoahStaticHeuristics::ShenandoahStaticHeuristics(ShenandoahGeneration* generation) :\n+  ShenandoahHeuristics(generation) {\n@@ -43,5 +44,3 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n-\n-  size_t max_capacity = heap->max_capacity();\n-  size_t capacity = heap->soft_max_capacity();\n-  size_t available = heap->free_set()->available();\n+  size_t max_capacity = _generation->max_capacity();\n+  size_t capacity = _generation->soft_max_capacity();\n+  size_t available = _generation->available();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.cpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  ShenandoahStaticHeuristics();\n+  ShenandoahStaticHeuristics(ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * Copyright (c) 2019, 2020, Red Hat, Inc. All rights reserved.\n@@ -26,4 +26,0 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n@@ -34,1 +30,4 @@\n-#include \"runtime\/java.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"logging\/log.hpp\"\n+#include \"logging\/logTag.hpp\"\n+#include \"runtime\/globals_extension.hpp\"\n@@ -45,0 +44,3 @@\n+  \/\/ HEY! Disabled while M7 work is in progress.\n+  FLAG_SET_ERGO(ShenandoahUnloadClassesFrequency, 0);\n+\n@@ -52,18 +54,0 @@\n-\n-ShenandoahHeuristics* ShenandoahGenerationalMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics != NULL) {\n-    if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-      return new ShenandoahAggressiveHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-      return new ShenandoahStaticHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-      return new ShenandoahAdaptiveHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-      return new ShenandoahCompactHeuristics();\n-    } else {\n-      vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return NULL;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.cpp","additions":8,"deletions":24,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahGenerationalMode.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -26,4 +26,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -59,18 +56,0 @@\n-\n-ShenandoahHeuristics* ShenandoahIUMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics != NULL) {\n-    if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-      return new ShenandoahAggressiveHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-      return new ShenandoahStaticHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-      return new ShenandoahAdaptiveHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-      return new ShenandoahCompactHeuristics();\n-    } else {\n-      vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return NULL;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahIUMode.cpp","additions":1,"deletions":22,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -35,2 +35,0 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahIUMode.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,50 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+\n+ShenandoahHeuristics* ShenandoahMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n+\n+  if (ShenandoahGCHeuristics != NULL) {\n+    if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n+      return new ShenandoahAggressiveHeuristics(generation);\n+    } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n+      return new ShenandoahStaticHeuristics(generation);\n+    } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n+      return new ShenandoahAdaptiveHeuristics(generation);\n+    } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n+      return new ShenandoahCompactHeuristics(generation);\n+    } else {\n+      vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n+    }\n+  }\n+  ShouldNotReachHere();\n+  return NULL;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.cpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/java.hpp\"\n@@ -30,0 +31,1 @@\n+class ShenandoahGeneration;\n@@ -51,1 +53,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const = 0;\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahMode.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -30,1 +31,0 @@\n-#include \"runtime\/globals_extension.hpp\"\n@@ -55,1 +55,1 @@\n-ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics() const {\n+ShenandoahHeuristics* ShenandoahPassiveMode::initialize_heuristics(ShenandoahGeneration* generation) const {\n@@ -57,1 +57,1 @@\n-    return new ShenandoahPassiveHeuristics();\n+    return new ShenandoahPassiveHeuristics(generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -33,2 +33,1 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n-\n+  virtual ShenandoahHeuristics* initialize_heuristics(ShenandoahGeneration* generation) const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahPassiveMode.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -26,4 +26,1 @@\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAdaptiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahAggressiveHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahCompactHeuristics.hpp\"\n-#include \"gc\/shenandoah\/heuristics\/shenandoahStaticHeuristics.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n@@ -52,18 +49,0 @@\n-\n-ShenandoahHeuristics* ShenandoahSATBMode::initialize_heuristics() const {\n-  if (ShenandoahGCHeuristics != NULL) {\n-    if (strcmp(ShenandoahGCHeuristics, \"aggressive\") == 0) {\n-      return new ShenandoahAggressiveHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"static\") == 0) {\n-      return new ShenandoahStaticHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"adaptive\") == 0) {\n-      return new ShenandoahAdaptiveHeuristics();\n-    } else if (strcmp(ShenandoahGCHeuristics, \"compact\") == 0) {\n-      return new ShenandoahCompactHeuristics();\n-    } else {\n-      vm_exit_during_initialization(\"Unknown -XX:ShenandoahGCHeuristics option\");\n-    }\n-  }\n-  ShouldNotReachHere();\n-  return NULL;\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahSATBMode.cpp","additions":1,"deletions":22,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-  virtual ShenandoahHeuristics* initialize_heuristics() const;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/mode\/shenandoahSATBMode.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-    return ShenandoahAllocRequest(min_size, requested_size, _alloc_tlab, YOUNG_GENERATION);\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_tlab, ShenandoahRegionAffiliation::YOUNG_GENERATION);\n@@ -86,1 +86,1 @@\n-    return ShenandoahAllocRequest(min_size, requested_size, _alloc_gclab, YOUNG_GENERATION);\n+    return ShenandoahAllocRequest(min_size, requested_size, _alloc_gclab, ShenandoahRegionAffiliation::YOUNG_GENERATION);\n@@ -94,1 +94,1 @@\n-    return ShenandoahAllocRequest(0, requested_size, _alloc_shared, YOUNG_GENERATION);\n+    return ShenandoahAllocRequest(0, requested_size, _alloc_shared, ShenandoahRegionAffiliation::YOUNG_GENERATION);\n@@ -169,0 +169,8 @@\n+  bool is_old() {\n+    return _affiliation == OLD_GENERATION;\n+  }\n+\n+  bool is_young() {\n+    return _affiliation == YOUNG_GENERATION;\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAllocRequest.hpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -159,0 +159,10 @@\n+  if (strcmp(ShenandoahGCMode, \"generational\") == 0) {\n+    \/\/ When we fill in dead objects during update refs, we use oop::size,\n+    \/\/ which depends on the klass being loaded. However, if these dead objects\n+    \/\/ were the last referrers to the klass, it will be unloaded and we'll\n+    \/\/ crash. Class unloading is disabled until we're able to sort this out.\n+    FLAG_SET_DEFAULT(ClassUnloading, false);\n+    FLAG_SET_DEFAULT(ClassUnloadingWithConcurrentMark, false);\n+    FLAG_SET_DEFAULT(ShenandoahUnloadClassesFrequency, 0);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahArguments.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -74,0 +74,3 @@\n+  if (heap->mode()->is_generational() && !obj->is_forwarded()) {\n+    msg.append(\"  age: %d\\n\", obj->age());\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAsserts.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -140,0 +140,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-  inline CardTable* card_table()  { return _card_table; }\n+  inline ShenandoahCardTable* card_table()  { return _card_table; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -243,1 +243,2 @@\n-  shenandoah_assert_marked_if(NULL, value, !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress());\n+  shenandoah_assert_marked_if(NULL, value, !CompressedOops::is_null(value) && ShenandoahHeap::heap()->is_evacuation_in_progress() &&\n+                              !(ShenandoahHeap::heap()->is_gc_generation_young() && ShenandoahHeap::heap()->heap_region_containing(value)->is_old()));\n@@ -355,1 +356,3 @@\n-  assert(HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n+  \/\/ We allow forwarding in young generation and marking in old generation\n+  \/\/ to happen simultaneously.\n+  assert(_heap->mode()->is_generational() || HAS_FWD == _heap->has_forwarded_objects(), \"Forwarded object status is sane\");\n@@ -375,1 +378,1 @@\n-      if (ENQUEUE && !ctx->is_marked_strong(obj)) {\n+      if (ENQUEUE && !ctx->is_marked_strong_or_old(obj)) {\n@@ -388,1 +391,1 @@\n-  if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n+  if ((gc_state & ShenandoahHeap::YOUNG_MARKING) != 0) {\n@@ -390,1 +393,4 @@\n-  } else if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n+    return;\n+  }\n+\n+  if ((gc_state & ShenandoahHeap::EVACUATION) != 0) {\n@@ -395,0 +401,11 @@\n+\n+  if (_heap->mode()->is_generational()) {\n+    assert(ShenandoahSATBBarrier, \"Generational mode assumes SATB mode\");\n+    \/\/ HEY! Could we optimize here by checking that dst is in an old region?\n+    if ((gc_state & ShenandoahHeap::OLD_MARKING) != 0) {\n+      \/\/ Note that we can't do the arraycopy marking using the 'src' array when\n+      \/\/ SATB mode is enabled (so we can't do this as part of the iteration for\n+      \/\/ evacuation or update references).\n+      arraycopy_marking(src, dst, count);\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSet.inline.hpp","additions":22,"deletions":5,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -105,0 +105,4 @@\n+  \/\/ We only need to handle YOUNG_MARKING here because the clone barrier\n+  \/\/ is only invoked during marking if Shenandoah is in incremental update\n+  \/\/ mode. OLD_MARKING should only happen when Shenandoah is in generational\n+  \/\/ mode, which uses the SATB write barrier.\n@@ -106,1 +110,1 @@\n-  if ((gc_state & ShenandoahHeap::MARKING) != 0) {\n+  if ((gc_state & ShenandoahHeap::YOUNG_MARKING) != 0) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahBarrierSetClone.inline.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n@@ -32,0 +33,18 @@\n+\n+bool ShenandoahCardTable::is_in_young(oop obj) const {\n+  return ShenandoahHeap::heap()->is_in_young(obj);\n+}\n+\n+bool ShenandoahCardTable::is_dirty(MemRegion mr) {\n+  for (size_t i = index_for(mr.start()); i <= index_for(mr.end() - 1); i++) {\n+    CardValue* byte = byte_for_index(i);\n+    if (*byte == CardTable::dirty_card_val()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+void ShenandoahCardTable::clear() {\n+  CardTable::clear(_whole_heap);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -41,4 +41,5 @@\n-  inline bool is_in_young(oop obj) const {\n-    ShouldNotReachHere();\n-    return false;\n-  }\n+  virtual bool is_in_young(oop obj) const;\n+\n+  bool is_dirty(MemRegion mr);\n+\n+  void clear();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahCardTable.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-  return _mark_context->is_marked(obj);\n+  return _mark_context->is_marked_or_old(obj);\n@@ -60,1 +60,1 @@\n-  return _mark_context->is_marked(obj);\n+  return _mark_context->is_marked_or_old(obj);\n@@ -84,1 +84,1 @@\n-  assert(!ShenandoahHeap::heap()->has_forwarded_objects(), \"Not expected\");\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress() || !ShenandoahHeap::heap()->has_forwarded_objects(), \"Not expected\");\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahClosures.inline.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-  _mark(),\n+  _mark(generation),\n@@ -59,4 +59,0 @@\n-void ShenandoahConcurrentGC::cancel() {\n-  ShenandoahConcurrentMark::cancel();\n-}\n-\n@@ -189,1 +185,2 @@\n-  const char* msg = init_mark_event_message();\n+  char msg[1024];\n+  init_mark_event_message(msg, sizeof(msg));\n@@ -201,1 +198,2 @@\n-  const char* msg = final_mark_event_message();\n+  char msg[1024];\n+  final_mark_event_message(msg, sizeof(msg));\n@@ -264,0 +262,1 @@\n+  char msg[1024];\n@@ -266,1 +265,1 @@\n-  const char* msg = conc_mark_event_message();\n+  conc_mark_event_message(msg, sizeof(msg));\n@@ -439,1 +438,1 @@\n-  heap->prepare_gc();\n+  _generation->prepare_gc();\n@@ -470,2 +469,2 @@\n-  assert(heap->marking_context()->is_bitmap_clear(), \"need clear marking bitmap\");\n-  assert(!heap->marking_context()->is_complete(), \"should not be complete\");\n+  assert(_generation->is_bitmap_clear(), \"need clear marking bitmap\");\n+  assert(!_generation->is_mark_complete(), \"should not be complete\");\n@@ -482,1 +481,1 @@\n-  heap->set_concurrent_mark_in_progress(true);\n+  _generation->set_concurrent_mark_in_progress(true);\n@@ -487,1 +486,1 @@\n-    heap->parallel_heap_region_iterate(&cl);\n+    _generation->parallel_heap_region_iterate(&cl);\n@@ -497,0 +496,5 @@\n+\n+  if (_generation->generation_mode() == YOUNG) {\n+    _generation->scan_remembered_set();\n+  }\n+\n@@ -511,1 +515,1 @@\n-  _mark.mark_concurrent_roots(_generation);\n+  _mark.mark_concurrent_roots();\n@@ -515,1 +519,1 @@\n-  _mark.concurrent_mark(_generation);\n+  _mark.concurrent_mark();\n@@ -528,1 +532,1 @@\n-    _mark.finish_mark(_generation);\n+    _mark.finish_mark();\n@@ -534,1 +538,1 @@\n-    heap->prepare_regions_and_collection_set(true \/*concurrent*\/);\n+    _generation->prepare_regions_and_collection_set(true \/*concurrent*\/);\n@@ -978,1 +982,1 @@\n-const char* ShenandoahConcurrentGC::init_mark_event_message() const {\n+void ShenandoahConcurrentGC::init_mark_event_message(char* buf, size_t len) const {\n@@ -982,1 +986,1 @@\n-    return \"Pause Init Mark (unload classes)\";\n+    jio_snprintf(buf, len, \"Pause Init Mark (%s) (unload classes)\", _generation->name());\n@@ -984,1 +988,1 @@\n-    return \"Pause Init Mark\";\n+    jio_snprintf(buf, len, \"Pause Init Mark (%s)\", _generation->name());\n@@ -988,1 +992,1 @@\n-const char* ShenandoahConcurrentGC::final_mark_event_message() const {\n+void ShenandoahConcurrentGC::final_mark_event_message(char* buf, size_t len) const {\n@@ -990,1 +994,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects during final mark (unless old gen concurrent mark is running)\");\n@@ -992,1 +997,1 @@\n-    return \"Pause Final Mark (unload classes)\";\n+    jio_snprintf(buf, len, \"Pause Final Mark (%s) (unload classes)\", _generation->name());\n@@ -994,1 +999,1 @@\n-    return \"Pause Final Mark\";\n+    jio_snprintf(buf, len, \"Pause Final Mark (%s)\", _generation->name());\n@@ -998,1 +1003,1 @@\n-const char* ShenandoahConcurrentGC::conc_mark_event_message() const {\n+void ShenandoahConcurrentGC::conc_mark_event_message(char* buf, size_t len) const {\n@@ -1000,1 +1005,2 @@\n-  assert(!heap->has_forwarded_objects(), \"Should not have forwarded objects here\");\n+  assert(!heap->has_forwarded_objects() || heap->is_concurrent_old_mark_in_progress(),\n+         \"Should not have forwarded objects concurrent mark (unless old gen concurrent mark is running\");\n@@ -1002,1 +1008,1 @@\n-    return \"Concurrent marking (unload classes)\";\n+    jio_snprintf(buf, len, \"Concurrent marking (%s) (unload classes)\", _generation->name());\n@@ -1004,1 +1010,1 @@\n-    return \"Concurrent marking\";\n+    jio_snprintf(buf, len, \"Concurrent marking (%s)\", _generation->name());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.cpp","additions":34,"deletions":28,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -56,2 +56,0 @@\n-  \/\/ Cancel ongoing concurrent GC\n-  static void cancel();\n@@ -62,0 +60,2 @@\n+\n+protected:\n@@ -63,0 +63,2 @@\n+\n+private:\n@@ -77,0 +79,2 @@\n+\n+protected:\n@@ -85,0 +89,2 @@\n+\n+private:\n@@ -112,3 +118,3 @@\n-  const char* init_mark_event_message() const;\n-  const char* final_mark_event_message() const;\n-  const char* conc_mark_event_message() const;\n+  void init_mark_event_message(char* buf, size_t len) const;\n+  void final_mark_event_message(char* buf, size_t len) const;\n+  void conc_mark_event_message(char* buf, size_t len) const;\n@@ -116,0 +122,1 @@\n+protected:\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentGC.hpp","additions":12,"deletions":5,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -89,1 +89,0 @@\n-    ShenandoahObjToScanQueue* q = _cm->get_queue(worker_id);\n@@ -145,0 +144,1 @@\n+      ShenandoahObjToScanQueue* old = _cm->get_old_queue(worker_id);\n@@ -146,1 +146,1 @@\n-      ShenandoahSATBBufferClosure<GENERATION> cl(q);\n+      ShenandoahSATBBufferClosure<GENERATION> cl(q, old);\n@@ -151,1 +151,1 @@\n-      ShenandoahMarkRefsClosure<GENERATION> mark_cl(q, rp);\n+      ShenandoahMarkRefsClosure<GENERATION> mark_cl(q, rp, old);\n@@ -165,2 +165,2 @@\n-ShenandoahConcurrentMark::ShenandoahConcurrentMark() :\n-  ShenandoahMark() {}\n+ShenandoahConcurrentMark::ShenandoahConcurrentMark(ShenandoahGeneration* generation) :\n+  ShenandoahMark(generation) {}\n@@ -172,1 +172,0 @@\n-  ShenandoahConcurrentMark*           _scm;\n@@ -176,0 +175,1 @@\n+  ShenandoahObjToScanQueueSet* const  _old_queue_set;\n@@ -179,2 +179,2 @@\n-  ShenandoahMarkConcurrentRootsTask(ShenandoahConcurrentMark* scm,\n-                                    ShenandoahObjToScanQueueSet* qs,\n+  ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n+                                    ShenandoahObjToScanQueueSet* old,\n@@ -188,2 +188,2 @@\n-ShenandoahMarkConcurrentRootsTask<GENERATION>::ShenandoahMarkConcurrentRootsTask(ShenandoahConcurrentMark* scm,\n-                                                                                 ShenandoahObjToScanQueueSet* qs,\n+ShenandoahMarkConcurrentRootsTask<GENERATION>::ShenandoahMarkConcurrentRootsTask(ShenandoahObjToScanQueueSet* qs,\n+                                                                                 ShenandoahObjToScanQueueSet* old,\n@@ -196,0 +196,1 @@\n+  _old_queue_set(old),\n@@ -204,1 +205,2 @@\n-  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp);\n+  ShenandoahObjToScanQueue* old = _old_queue_set == NULL ? NULL : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<GENERATION> cl(q, _rp, old);\n@@ -208,1 +210,1 @@\n-void ShenandoahConcurrentMark::mark_concurrent_roots(ShenandoahGeneration* generation) {\n+void ShenandoahConcurrentMark::mark_concurrent_roots() {\n@@ -216,2 +218,2 @@\n-  task_queues()->reserve(workers->active_workers());\n-  switch (generation->generation_mode()) {\n+  _generation->reserve_task_queues(workers->active_workers());\n+  switch (_generation->generation_mode()) {\n@@ -219,1 +221,1 @@\n-      ShenandoahMarkConcurrentRootsTask<YOUNG> task(this, task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      ShenandoahMarkConcurrentRootsTask<YOUNG> task(task_queues(), old_task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n@@ -224,1 +226,2 @@\n-      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(this, task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n+      assert(old_task_queues() == NULL, \"Global mark should not have old gen mark queues.\");\n+      ShenandoahMarkConcurrentRootsTask<GLOBAL> task(task_queues(), old_task_queues(), rp, ShenandoahPhaseTimings::conc_mark_roots, workers->active_workers());\n@@ -229,0 +232,2 @@\n+      \/\/ Intentionally haven't added OLD here. We use a YOUNG generation\n+      \/\/ cycle to bootstrap concurrent old marking.\n@@ -246,1 +251,1 @@\n-void ShenandoahConcurrentMark::concurrent_mark(ShenandoahGeneration* generation) {\n+void ShenandoahConcurrentMark::concurrent_mark() {\n@@ -255,1 +260,1 @@\n-    switch (generation->generation_mode()) {\n+    switch (_generation->generation_mode()) {\n@@ -262,0 +267,6 @@\n+      case OLD: {\n+        TaskTerminator terminator(nworkers, task_queues());\n+        ShenandoahConcurrentMarkingTask<OLD> task(this, &terminator);\n+        workers->run_task(&task);\n+        break;\n+      }\n@@ -289,1 +300,1 @@\n-void ShenandoahConcurrentMark::finish_mark(ShenandoahGeneration* generation) {\n+void ShenandoahConcurrentMark::finish_mark() {\n@@ -292,1 +303,1 @@\n-  finish_mark_work(generation);\n+  finish_mark_work();\n@@ -297,3 +308,2 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->set_concurrent_mark_in_progress(false);\n-  heap->mark_complete_marking_context();\n+  _generation->set_concurrent_mark_in_progress(false);\n+  _generation->set_mark_complete();\n@@ -302,1 +312,1 @@\n-void ShenandoahConcurrentMark::finish_mark_work(ShenandoahGeneration* generation) {\n+void ShenandoahConcurrentMark::finish_mark_work() {\n@@ -318,1 +328,1 @@\n-  switch (generation->generation_mode()) {\n+  switch (_generation->generation_mode()) {\n@@ -324,0 +334,5 @@\n+    case OLD:{\n+      ShenandoahFinalMarkingTask<OLD> task(this, &terminator, ShenandoahStringDedup::is_enabled());\n+      heap->workers()->run_task(&task);\n+      break;\n+    }\n@@ -336,7 +351,0 @@\n-\n-\n-void ShenandoahConcurrentMark::cancel() {\n-  clear();\n-  ShenandoahReferenceProcessor* rp = ShenandoahHeap::heap()->ref_processor();\n-  rp->abandon_partial_discovery();\n-}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.cpp","additions":40,"deletions":32,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-  ShenandoahConcurrentMark();\n+  ShenandoahConcurrentMark(ShenandoahGeneration* generation);\n@@ -44,1 +44,1 @@\n-  void mark_concurrent_roots(ShenandoahGeneration* generation);\n+  void mark_concurrent_roots();\n@@ -47,3 +47,1 @@\n-  void concurrent_mark(ShenandoahGeneration* generation);\n-  \/\/ Finish mark at a safepoint\n-  void finish_mark(ShenandoahGeneration* generation);\n+  void concurrent_mark();\n@@ -51,1 +49,2 @@\n-  static void cancel();\n+  \/\/ Finish mark at a safepoint\n+  void finish_mark();\n@@ -54,1 +53,1 @@\n-  void finish_mark_work(ShenandoahGeneration* generation);\n+  void finish_mark_work();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahConcurrentMark.hpp","additions":6,"deletions":7,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -48,0 +49,1 @@\n+#include \"shenandoahOldGC.hpp\"\n@@ -55,0 +57,1 @@\n+  _requested_generation(GenerationMode::GLOBAL),\n@@ -56,0 +59,1 @@\n+  _degen_generation(NULL),\n@@ -84,0 +88,1 @@\n+  GenerationMode generation = GLOBAL;\n@@ -85,1 +90,0 @@\n-  int sleep = ShenandoahControlIntervalMin;\n@@ -88,1 +92,0 @@\n-  double last_sleep_adjust_time = os::elapsedTime();\n@@ -97,1 +100,7 @@\n-  ShenandoahHeuristics* heuristics = heap->heuristics();\n+\n+  \/\/ HEY! heuristics are notified of allocation failures here and other outcomes\n+  \/\/ of the cycle. They're also used here to control whether the Nth consecutive\n+  \/\/ degenerated cycle should be 'promoted' to a full cycle. This changes the\n+  \/\/ threading model for them somewhat, as they are now evaluated on a separate\n+  \/\/ thread.\n+  ShenandoahHeuristics* global_heuristics = heap->global_generation()->heuristics();\n@@ -101,2 +110,2 @@\n-    bool explicit_gc_requested = _gc_requested.is_set() &&  is_explicit_gc(_requested_gc_cause);\n-    bool implicit_gc_requested = _gc_requested.is_set() && !is_explicit_gc(_requested_gc_cause);\n+    bool explicit_gc_requested = _gc_requested.is_set() && is_explicit_gc(_requested_gc_cause);\n+    bool implicit_gc_requested = _gc_requested.is_set() && is_implicit_gc(_requested_gc_cause);\n@@ -125,0 +134,9 @@\n+      if (degen_point == ShenandoahGC::_degenerated_outside_cycle) {\n+        _degen_generation = heap->global_generation();\n+      } else {\n+        assert(_degen_generation != NULL, \"Need to know which generation to resume.\");\n+      }\n+\n+      ShenandoahHeuristics* heuristics = _degen_generation->heuristics();\n+      generation = _degen_generation->generation_mode();\n+\n@@ -134,1 +152,0 @@\n-\n@@ -136,0 +153,1 @@\n+      generation = GLOBAL;\n@@ -139,1 +157,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -145,1 +163,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -151,0 +169,1 @@\n+      generation = GLOBAL;\n@@ -154,1 +173,1 @@\n-      heuristics->record_requested_gc();\n+      global_heuristics->record_requested_gc();\n@@ -161,1 +180,1 @@\n-        heap->set_unload_classes(heuristics->can_unload_classes());\n+        heap->set_unload_classes(global_heuristics->can_unload_classes());\n@@ -167,2 +186,6 @@\n-      \/\/ Potential normal cycle: ask heuristics if it wants to act\n-      if (heuristics->should_start_gc()) {\n+      \/\/ We should only be here if the regulator requested a cycle or if\n+      \/\/ there is an old generation mark in progress.\n+      if (_preemption_requested.try_unset() || _requested_gc_cause == GCCause::_shenandoah_concurrent_gc) {\n+        \/\/ preemption was requested or this is a regular cycle\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = _requested_generation;\n@@ -170,1 +193,9 @@\n-        cause = default_cause;\n+        if (generation == GLOBAL) {\n+          heap->set_unload_classes(global_heuristics->should_unload_classes());\n+        } else {\n+          heap->set_unload_classes(false);\n+        }\n+      } else if (heap->is_concurrent_old_mark_in_progress()) {\n+        cause = GCCause::_shenandoah_concurrent_gc;\n+        generation = OLD;\n+        mode = resume_old;\n@@ -173,2 +204,5 @@\n-      \/\/ Ask policy if this cycle wants to process references or unload classes\n-      heap->set_unload_classes(heuristics->should_unload_classes());\n+      \/\/ Don't want to spin in this loop and start a cycle every time, so\n+      \/\/ clear requested gc cause. This creates a race with callers of the\n+      \/\/ blocking 'request_gc' method, but there it loops and resets the\n+      \/\/ '_requested_gc_cause' until a full cycle is completed.\n+      _requested_gc_cause = GCCause::_no_gc;\n@@ -178,2 +212,2 @@\n-    \/\/ either implicit or explicit GC request,  or we are requested to do so unconditionally.\n-    if (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs) {\n+    \/\/ either implicit or explicit GC request, or we are requested to do so unconditionally.\n+    if (generation == GLOBAL && (alloc_failure_pending || implicit_gc_requested || explicit_gc_requested || ShenandoahAlwaysClearSoftRefs)) {\n@@ -187,2 +221,0 @@\n-      \/\/ GC is starting, bump the internal ID\n-      update_gc_id();\n@@ -205,8 +237,13 @@\n-      switch (mode) {\n-        case concurrent_normal:\n-          if (heap->mode()->is_generational()) {\n-            \/\/ TODO: Only young collections for now.\n-            \/\/ We'll add old collections later.\n-            service_concurrent_normal_cycle(cause, heap->young_generation());\n-          } else {\n-            service_concurrent_normal_cycle(cause, heap->global_generation());\n+      {\n+        switch (mode) {\n+          case concurrent_normal: {\n+            service_concurrent_normal_cycle(heap, generation, cause);\n+            break;\n+          }\n+          case stw_degenerated: {\n+            service_stw_degenerated_cycle(cause, degen_point);\n+            break;\n+          }\n+          case stw_full: {\n+            service_stw_full_cycle(cause);\n+            break;\n@@ -214,9 +251,9 @@\n-          break;\n-        case stw_degenerated:\n-          service_stw_degenerated_cycle(cause, degen_point);\n-          break;\n-        case stw_full:\n-          service_stw_full_cycle(cause);\n-          break;\n-        default:\n-          ShouldNotReachHere();\n+          case resume_old: {\n+            assert(generation == OLD, \"Expected old generation here\");\n+            resume_concurrent_old_cycle(heap->old_generation(), cause);\n+            break;\n+          }\n+          default: {\n+            ShouldNotReachHere();\n+          }\n+        }\n@@ -225,0 +262,3 @@\n+      \/\/ GC is finished, bump the internal ID before notifying waiters.\n+      update_gc_id();\n+\n@@ -260,1 +300,3 @@\n-        heuristics->clear_metaspace_oom();\n+        \/\/ HEY! Should we do this if the cycle was cancelled\/degenerated?\n+        assert(generation != YOUNG, \"should not unload classes in young cycle\");\n+        global_heuristics->clear_metaspace_oom();\n@@ -319,10 +361,5 @@\n-    \/\/ Wait before performing the next action. If allocation happened during this wait,\n-    \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n-    \/\/ back off exponentially.\n-    if (_heap_changed.try_unset()) {\n-      sleep = ShenandoahControlIntervalMin;\n-    } else if ((current - last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n-      sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, sleep * 2));\n-      last_sleep_adjust_time = current;\n-    }\n-    os::naked_short_sleep(sleep);\n+    \/\/ HEY! kemperw would like to have this thread sleep on a timed wait so it\n+    \/\/ could be explicitly woken when there is something to do. The timed wait\n+    \/\/ is necessary because this thread has a responsibility to send\n+    \/\/ 'alloc_words' to the pacer when it does not perform a GC.\n+    os::naked_short_sleep(ShenandoahControlIntervalMin);\n@@ -337,0 +374,100 @@\n+\/\/ Young and old concurrent cycles are initiated by the regulator. Implicit\n+\/\/ and explicit GC requests are handled by the controller thread and always\n+\/\/ run a global cycle (which is concurrent by default, but may be overridden\n+\/\/ by command line options). Old and young cycles always degenerate to a\n+\/\/ global cycle. Since the mark data for young and old cycles is partial,\n+\/\/ degeneration in these cycles must begin with a global (complete) marking.\n+\/\/\n+\/\/\n+\/\/              +-----------+  Idle  +-----------+\n+\/\/              |               +                |\n+\/\/              v               |                v\n+\/\/                              |\n+\/\/            Young             |               Old +------> Young (bootstrap)\n+\/\/              +               v                +             +\n+\/\/              |                                |             |\n+\/\/              |             Global             |             |\n+\/\/              |               +                |             |\n+\/\/              |               |                |             |\n+\/\/              |               |                |             |\n+\/\/              v               v                |             v\n+\/\/                                               |\n+\/\/            Degen           Degen   <----------+           Degen\n+\/\/            Young           Global                         Young\n+\/\/\n+void ShenandoahControlThread::service_concurrent_normal_cycle(\n+  const ShenandoahHeap* heap, const GenerationMode generation, GCCause::Cause cause) {\n+\n+  switch (generation) {\n+    case YOUNG: {\n+      \/\/ Run a young cycle. This might or might not, have interrupted an ongoing\n+      \/\/ concurrent mark in the old generation. We need to think about promotions\n+      \/\/ in this case. Promoted objects should be above the TAMS in the old regions\n+      \/\/ they end up in, but we have to be sure we don't promote into any regions\n+      \/\/ that are in the cset (more of an issue for Milestone-8 to worry about).\n+      log_info(gc, ergo)(\"Start GC cycle (YOUNG)\");\n+      service_concurrent_cycle(heap->young_generation(), cause);\n+      heap->young_generation()->log_status();\n+      break;\n+    }\n+    case GLOBAL: {\n+      log_info(gc, ergo)(\"Start GC cycle (GLOBAL)\");\n+      service_concurrent_cycle(heap->global_generation(), cause);\n+      heap->global_generation()->log_status();\n+      break;\n+    }\n+    case OLD: {\n+      log_info(gc, ergo)(\"Start GC cycle (OLD)\");\n+      service_concurrent_old_cycle(heap, cause);\n+      heap->old_generation()->log_status();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+void ShenandoahControlThread::service_concurrent_old_cycle(const ShenandoahHeap* heap, GCCause::Cause &cause) {\n+  \/\/ Configure the young generation's concurrent mark to put objects in\n+  \/\/ old regions into the concurrent mark queues associated with the old\n+  \/\/ generation. The young cycle will run as normal except that rather than\n+  \/\/ ignore old references it will mark and enqueue them in the old concurrent\n+  \/\/ mark but it will not traverse them.\n+  ShenandoahGeneration* old_generation = heap->old_generation();\n+  ShenandoahYoungGeneration* young_generation = heap->young_generation();\n+\n+  young_generation->set_old_gen_task_queues(old_generation->task_queues());\n+\n+  old_generation->set_mark_incomplete();\n+\n+  service_concurrent_cycle(young_generation, cause);\n+\n+  \/\/ Young generation no longer needs this reference to the old concurrent\n+  \/\/ mark so clean it up.\n+  young_generation->set_old_gen_task_queues(NULL);\n+\n+  if (heap->cancelled_gc()) {\n+    \/\/ Bootstrap cycle was cancelled. Now we expect to run a degenerated\n+    \/\/ young cycle. Clear anything out of old generation mark queues.\n+    old_generation->task_queues()->clear();\n+    old_generation->set_mark_incomplete();\n+  } else {\n+    \/\/ Reset the degenerated point. Normally this would happen at the top\n+    \/\/ of the control loop, but here we have just completed a young cycle\n+    \/\/ which has bootstrapped the old concurrent marking.\n+    _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+\n+    \/\/ Bit of a hack here to keep the phase timings happy as we transition\n+    \/\/ to concurrent old marking. We need to revisit this.\n+    heap->phase_timings()->flush_par_workers_to_cycle();\n+    heap->phase_timings()->flush_cycle_to_global();\n+\n+    \/\/ From here we will 'resume' the old concurrent mark. This will skip reset\n+    \/\/ and init mark for the concurrent mark. All of that work will have been\n+    \/\/ done by the bootstrapping young cycle. In order to simplify the debugging\n+    \/\/ effort, the old cycle will ONLY complete the mark phase. No actual\n+    \/\/ collection of the old generation is happening here.\n+    resume_concurrent_old_cycle(old_generation, cause);\n+  }\n+}\n+\n@@ -356,1 +493,35 @@\n-void ShenandoahControlThread::service_concurrent_normal_cycle(GCCause::Cause cause, ShenandoahGeneration* generation) {\n+void ShenandoahControlThread::resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n+  assert(ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress(), \"Old mark should be in progress\");\n+  log_debug(gc)(\"Resuming old generation with \" UINT32_FORMAT \" marking tasks queued.\", generation->task_queues()->tasks());\n+\n+  _allow_old_preemption.set();\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  GCIdMark gc_id_mark;\n+  ShenandoahGCSession session(cause, generation);\n+\n+  TraceCollectorStats tcs(heap->monitoring_support()->concurrent_collection_counters());\n+\n+  ShenandoahOldGC gc(generation);\n+  if (gc.collect(cause)) {\n+    \/\/ Cycle is complete\n+    generation->heuristics()->record_success_concurrent();\n+    heap->shenandoah_policy()->record_success_concurrent();\n+  } else {\n+    assert(heap->cancelled_gc(), \"Old marking must be interrupted.\");\n+    check_cancellation_or_degen(gc.degen_point());\n+    if (is_alloc_failure_gc()) {\n+      \/\/ Cancelled for degeneration, not just to run a young cycle.\n+      \/\/ We can't complete a global cycle with the partial marking\n+      \/\/ information in the old generation mark queues so we force\n+      \/\/ the degenerated cycle to be global and from outside the cycle.\n+      generation->task_queues()->clear();\n+      generation->set_mark_incomplete();\n+      _degen_point = ShenandoahGC::_degenerated_outside_cycle;\n+    }\n+  }\n+  _allow_old_preemption.unset();\n+}\n+\n+void ShenandoahControlThread::service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause) {\n@@ -396,1 +567,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, generation);\n@@ -403,1 +574,1 @@\n-    heap->heuristics()->record_success_concurrent();\n+    generation->heuristics()->record_success_concurrent();\n@@ -408,0 +579,1 @@\n+    _degen_generation = generation;\n@@ -413,5 +585,29 @@\n-  if (heap->cancelled_gc()) {\n-    assert (is_alloc_failure_gc() || in_graceful_shutdown(), \"Cancel GC either for alloc failure GC, or gracefully exiting\");\n-    if (!in_graceful_shutdown()) {\n-      assert (_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n-              \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+  if (!heap->cancelled_gc()) {\n+    return false;\n+  }\n+\n+  if (in_graceful_shutdown()) {\n+    return true;\n+  }\n+\n+  assert(_degen_point == ShenandoahGC::_degenerated_outside_cycle,\n+         \"Should not be set yet: %s\", ShenandoahGC::degen_point_to_string(_degen_point));\n+\n+  if (is_alloc_failure_gc()) {\n+    _degen_point = point;\n+    return true;\n+  }\n+\n+  if (_preemption_requested.is_set()) {\n+    assert(_requested_generation == YOUNG, \"Only young GCs may preempt old.\");\n+    assert(_requested_gc_cause == GCCause::_shenandoah_concurrent_gc, \"Should be normal concurrent young gc.\");\n+\n+    if (point != ShenandoahGC::_degenerated_mark) {\n+      \/\/ Okay, cancel is set in the heap now whether we are in concurrent old marking\n+      \/\/ or not. Hopefully, we still are, but we can't guarantee when this thread\n+      \/\/ observes the cancellation. If this thread is still in concurrent marking,\n+      \/\/ then we can suspend without degeneration. Otherwise, we have to degenerate\n+      \/\/ the cycle - we can't just blithely ignore the cancellation because it\n+      \/\/ affects the machinery deeper than us. We could try to abandon the cycle\n+      \/\/ entirely, but that would be a new code path.\n+      \/\/ HEY! We don't actually handle this case in run_service!\n@@ -419,0 +615,7 @@\n+    } else {\n+      \/\/ We were cancelled during the concurrent mark of old gen. Normally, the\n+      \/\/ degenerated or full cycle would clear the cancellation request, so we\n+      \/\/ need to clear it here so the cycle interrupting us will run. Note that\n+      \/\/ when this condition is met we will _not_ consume _degen_point and will\n+      \/\/ begin a concurrent young cycle.\n+      heap->clear_cancelled_gc(false \/* clear oom handler *\/);\n@@ -422,0 +625,2 @@\n+\n+  fatal(\"Cancel GC either for alloc failure GC, or gracefully exiting, or to pause old generation marking.\");\n@@ -430,0 +635,5 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  if (heap->mode()->is_generational()) {\n+    fatal(\"Full GC not yet supported for generational mode.\");\n+  }\n+\n@@ -431,1 +641,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, heap->global_generation());\n@@ -436,2 +646,1 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_full();\n+  heap->global_generation()->heuristics()->record_success_full();\n@@ -443,0 +652,1 @@\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n@@ -445,1 +655,1 @@\n-  ShenandoahGCSession session(cause);\n+  ShenandoahGCSession session(cause, _degen_generation);\n@@ -447,1 +657,1 @@\n-  ShenandoahDegenGC gc(point);\n+  ShenandoahDegenGC gc(point, _degen_generation);\n@@ -450,2 +660,5 @@\n-  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n-  heap->heuristics()->record_success_degenerated();\n+  assert(heap->young_generation()->task_queues()->is_empty(), \"Unexpected young generation marking tasks\");\n+  assert(heap->old_generation()->task_queues()->is_empty(), \"Unexpected old generation marking tasks\");\n+  assert(heap->global_generation()->task_queues()->is_empty(), \"Unexpected global generation marking tasks\");\n+\n+  _degen_generation->heuristics()->record_success_degenerated();\n@@ -483,0 +696,4 @@\n+bool ShenandoahControlThread::is_implicit_gc(GCCause::Cause cause) const {\n+  return !is_explicit_gc(cause) && cause != GCCause::_shenandoah_concurrent_gc;\n+}\n+\n@@ -501,0 +718,20 @@\n+void ShenandoahControlThread::request_concurrent_gc(GenerationMode generation) {\n+  if (_preemption_requested.is_set() || _gc_requested.is_set()) {\n+    \/\/ ignore subsequent requests from the heuristics\n+    return;\n+  }\n+\n+  _requested_gc_cause = GCCause::_shenandoah_concurrent_gc;\n+  _requested_generation = generation;\n+\n+  if (preempt_old_marking(generation)) {\n+    log_info(gc)(\"Preempting old generation mark to allow young GC.\");\n+    _preemption_requested.set();\n+    ShenandoahHeap::heap()->cancel_gc(GCCause::_shenandoah_concurrent_gc);\n+  }\n+}\n+\n+bool ShenandoahControlThread::preempt_old_marking(GenerationMode generation) {\n+  return generation == YOUNG && _allow_old_preemption.try_unset();\n+}\n+\n@@ -598,4 +835,0 @@\n-  \/\/ Notify that something had changed.\n-  if (_heap_changed.is_unset()) {\n-    _heap_changed.set();\n-  }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.cpp","additions":299,"deletions":66,"binary":false,"changes":365,"status":"modified"},{"patch":"@@ -64,1 +64,2 @@\n-    stw_full\n+    stw_full,\n+    resume_old\n@@ -79,0 +80,2 @@\n+  size_t get_gc_id();\n+\n@@ -80,0 +83,2 @@\n+  ShenandoahSharedFlag _allow_old_preemption;\n+  ShenandoahSharedFlag _preemption_requested;\n@@ -83,1 +88,0 @@\n-  ShenandoahSharedFlag _heap_changed;\n@@ -87,0 +91,1 @@\n+  GenerationMode       _requested_generation;\n@@ -88,0 +93,1 @@\n+  ShenandoahGeneration* _degen_generation;\n@@ -96,1 +102,2 @@\n-  void service_concurrent_normal_cycle(GCCause::Cause cause, ShenandoahGeneration* generation);\n+  void resume_concurrent_old_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n+  void service_concurrent_cycle(ShenandoahGeneration* generation, GCCause::Cause cause);\n@@ -107,1 +114,0 @@\n-  size_t get_gc_id();\n@@ -116,0 +122,3 @@\n+  bool is_implicit_gc(GCCause::Cause cause) const;\n+\n+  bool preempt_old_marking(GenerationMode generation);\n@@ -133,0 +142,1 @@\n+  void request_concurrent_gc(GenerationMode generation);\n@@ -151,0 +161,7 @@\n+\n+  void service_concurrent_normal_cycle(const ShenandoahHeap* heap,\n+                                       const GenerationMode generation,\n+                                       GCCause::Cause cause);\n+\n+  void service_concurrent_old_cycle(const ShenandoahHeap* heap,\n+                                    GCCause::Cause &cause);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahControlThread.hpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -45,1 +46,1 @@\n-ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point) :\n+ShenandoahDegenGC::ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation) :\n@@ -47,1 +48,2 @@\n-  _degen_point(degen_point) {\n+  _degen_point(degen_point),\n+  _generation(generation) {\n@@ -84,0 +86,8 @@\n+  \/\/ We can't easily clear the old mark in progress flag because it must be done\n+  \/\/ on a safepoint (not sure if that is a hard requirement). At any rate, once\n+  \/\/ we are in a degenerated cycle, there should be no more old marking.\n+  assert(heap->old_generation()->task_queues()->is_empty(), \"Old gen task queues should be empty.\");\n+  if (heap->is_concurrent_old_mark_in_progress()) {\n+    heap->set_concurrent_old_mark_in_progress(false);\n+  }\n+\n@@ -102,2 +112,1 @@\n-        ShenandoahConcurrentMark::cancel();\n-        heap->set_concurrent_mark_in_progress(false);\n+        heap->cancel_concurrent_mark();\n@@ -108,1 +117,1 @@\n-      heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+      heap->set_unload_classes(_generation->heuristics()->can_unload_classes());\n@@ -216,1 +225,1 @@\n-  ShenandoahHeap::heap()->prepare_gc();\n+  _generation->prepare_gc();\n@@ -222,2 +231,1 @@\n-  ShenandoahSTWMark mark(false \/*full gc*\/);\n-  mark.clear();\n+  ShenandoahSTWMark mark(_generation, false \/*full gc*\/);\n@@ -228,2 +236,2 @@\n-  ShenandoahConcurrentMark mark;\n-  mark.finish_mark(ShenandoahHeap::heap()->global_generation());\n+  ShenandoahConcurrentMark mark(_generation);\n+  mark.finish_mark();\n@@ -240,0 +248,1 @@\n+\n@@ -241,1 +250,1 @@\n-  heap->prepare_regions_and_collection_set(false \/*concurrent*\/);\n+  _generation->prepare_regions_and_collection_set(false \/*concurrent*\/);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.cpp","additions":20,"deletions":11,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+class ShenandoahGeneration;\n@@ -36,0 +37,1 @@\n+  ShenandoahGeneration* _generation;\n@@ -38,1 +40,1 @@\n-  ShenandoahDegenGC(ShenandoahDegenPoint degen_point);\n+  ShenandoahDegenGC(ShenandoahDegenPoint degen_point, ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahDegeneratedGC.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shenandoah\/shenandoahBarrierSet.hpp\"\n@@ -31,0 +32,2 @@\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -35,0 +38,2 @@\n+#undef DEBUG_TRACE\n+\n@@ -102,1 +107,7 @@\n-          HeapWord* result = try_allocate_in(_heap->get_region(idx), req, in_new_region);\n+          ShenandoahHeapRegion* r = _heap->get_region(idx);\n+          if (r->is_young() && req.is_old()) {\n+            \/\/ We don't want to cannibalize a young region to satisfy\n+            \/\/ an evacuation from an old region.\n+            continue;\n+          }\n+          HeapWord* result = try_allocate_in(r, req, in_new_region);\n@@ -104,0 +115,10 @@\n+            if (r->is_old()) {\n+              \/\/ HEY! This is a very coarse card marking. We hope to repair\n+              \/\/ such cards during remembered set scanning.\n+\n+              \/\/ HEY! To support full generality with alternative remembered set implementations,\n+              \/\/ is preferable to not make direct access to the current card_table implementation.\n+              \/\/  Try ShenandoahHeap::heap()->card_scan()->mark_range_as_dirty(result, req.actual_size());\n+\n+              ShenandoahBarrierSet::barrier_set()->card_table()->dirty_MemRegion(MemRegion(result, req.actual_size()));\n+            }\n@@ -120,0 +141,4 @@\n+            if (r->is_young() && req.is_old()) {\n+              continue;\n+            }\n+\n@@ -123,0 +148,10 @@\n+              if (r->is_old()) {\n+                \/\/ HEY! This is a very coarse card marking. We hope to repair\n+                \/\/ such cards during remembered set scanning.\n+\n+                \/\/ HEY! To support full generality with alternative remembered set implementations,\n+                \/\/ is preferable to not make direct access to the current card_table implementation.\n+                \/\/  Try ShenandoahHeap::heap()->card_scan()->mark_range_as_dirty(result, req.actual_size());\n+\n+                ShenandoahBarrierSet::barrier_set()->card_table()->dirty_MemRegion(MemRegion(result, req.actual_size()));\n+              }\n@@ -142,0 +177,17 @@\n+#ifdef DEBUG_TRACE\n+static const char *affiliation_name(ShenandoahRegionAffiliation a) {\n+\n+  switch (a) {\n+    case ShenandoahRegionAffiliation::FREE:\n+        return \"FREE\";\n+        break;\n+    case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n+        return \"YOUNG_GENERATION\";\n+    case ShenandoahRegionAffiliation::OLD_GENERATION:\n+        return \"OLD_GENERATION\";\n+    default:\n+        return \"UnrecognizedAffiliation\";\n+  }\n+}\n+#endif\n+\n@@ -152,1 +204,9 @@\n-  if (r->affiliation() == FREE) {\n+  if (r->affiliation() == ShenandoahRegionAffiliation::FREE) {\n+#ifdef DEBUG_TRACE\n+    printf(\"try_allocate_in(), converting region @ (%llx, %llx, %llx) to %s\\n\",\n+           (unsigned long long) r->bottom(), (unsigned long long) r->top(), (unsigned long long) r->end(),\n+           affiliation_name(req.affiliation()));\n+    fflush(stdout);\n+#endif\n+    \/\/ This free region might have garbage in its remembered set representation.\n+    _heap->clear_cards_for(r);\n@@ -188,0 +248,6 @@\n+\n+    if (r->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+      _heap->young_generation()->increase_used(size * HeapWordSize);\n+    } else if (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+      _heap->old_generation()->increase_used(size * HeapWordSize);\n+    }\n@@ -315,0 +381,6 @@\n+#ifdef DEBUG_TRACE\n+    printf(\"allocate_contiguous(), setting region (%llx, %llx, %llx) to %s\\n\",\n+           (unsigned long long) r->bottom(), (unsigned long long) r->top(), (unsigned long long) r->end(),\n+           affiliation_name(req.affiliation()));\n+    fflush(stdout);\n+#endif\n@@ -324,0 +396,6 @@\n+  if (req.affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+    _heap->young_generation()->increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n+  } else if (req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+    _heap->old_generation()->increase_used(ShenandoahHeapRegion::region_size_bytes() * num);\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":80,"deletions":2,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -52,0 +52,1 @@\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n@@ -162,1 +163,1 @@\n-    \/\/ b. Cancel concurrent mark, if in progress\n+    \/\/ b. Cancel all concurrent marks, if in progress\n@@ -164,2 +165,1 @@\n-      ShenandoahConcurrentGC::cancel();\n-      heap->set_concurrent_mark_in_progress(false);\n+      heap->cancel_concurrent_mark();\n@@ -175,1 +175,1 @@\n-    heap->reset_mark_bitmap();\n+    heap->global_generation()->reset_mark_bitmap();\n@@ -177,1 +177,1 @@\n-    assert(!heap->marking_context()->is_complete(), \"sanity\");\n+    assert(!heap->global_generation()->is_mark_complete(), \"sanity\");\n@@ -277,0 +277,2 @@\n+\n+  bool is_thread_safe() { return true; }\n@@ -286,1 +288,1 @@\n-  heap->heap_region_iterate(&cl);\n+  heap->parallel_heap_region_iterate(&cl);\n@@ -288,1 +290,1 @@\n-  heap->set_unload_classes(heap->heuristics()->can_unload_classes());\n+  heap->set_unload_classes(heap->global_generation()->heuristics()->can_unload_classes());\n@@ -294,1 +296,1 @@\n-  ShenandoahSTWMark mark(true \/*full_gc*\/);\n+  ShenandoahSTWMark mark(heap->global_generation(), true \/*full_gc*\/);\n@@ -327,0 +329,8 @@\n+    if (_heap->mode()->is_generational() && _to_region->affiliation() == FREE) {\n+      \/\/ HEY! Changing this region to young during compaction may not be\n+      \/\/ technically correct here because it completely disregards the ages\n+      \/\/ and origins of the objects being moved. It is, however, certainly\n+      \/\/ more correct than putting live objects into a region without a\n+      \/\/ generational affiliation.\n+      _to_region->set_affiliation(YOUNG_GENERATION);\n+    }\n@@ -1064,0 +1074,4 @@\n+\n+    if (heap->mode()->is_generational()) {\n+      heap->young_generation()->promote_all_regions();\n+    }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":22,"deletions":8,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -0,0 +1,277 @@\n+\/*\n+ * Copyright (c) 2001, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkClosures.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMonitoringSupport.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTaskqueue.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+\n+class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n+ private:\n+  ShenandoahMarkingContext* const _ctx;\n+ public:\n+  ShenandoahResetUpdateRegionStateClosure() :\n+    _ctx(ShenandoahHeap::heap()->marking_context()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* r) {\n+    if (r->is_active()) {\n+      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n+      \/\/ anyway to capture any updates that happened since now.\n+      _ctx->capture_top_at_mark_start(r);\n+      r->clear_live_data();\n+    }\n+  }\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n+class ShenandoahResetBitmapTask : public ShenandoahHeapRegionClosure {\n+ private:\n+  ShenandoahHeap* _heap;\n+  ShenandoahMarkingContext* const _ctx;\n+ public:\n+  ShenandoahResetBitmapTask() :\n+    _heap(ShenandoahHeap::heap()),\n+    _ctx(_heap->marking_context()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* region) {\n+    if (_heap->is_bitmap_slice_committed(region)) {\n+        _ctx->clear_bitmap(region);\n+    }\n+  }\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n+void ShenandoahGeneration::initialize_heuristics(ShenandoahMode* gc_mode) {\n+  _heuristics = gc_mode->initialize_heuristics(this);\n+\n+  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n+    vm_exit_during_initialization(\n+            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n+                    _heuristics->name()));\n+  }\n+  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n+    vm_exit_during_initialization(\n+            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n+                    _heuristics->name()));\n+  }\n+}\n+\n+size_t ShenandoahGeneration::bytes_allocated_since_gc_start() {\n+  return ShenandoahHeap::heap()->bytes_allocated_since_gc_start();\n+}\n+\n+void ShenandoahGeneration::log_status() const {\n+  typedef LogTarget(Info, gc, ergo) LogGcInfo;\n+\n+  if (!LogGcInfo::is_enabled()) {\n+    return;\n+  }\n+\n+  \/\/ Not under a lock here, so read each of these once to make sure\n+  \/\/ byte size in proper unit and proper unit for byte size are consistent.\n+  size_t v_used = used();\n+  size_t v_used_regions = used_regions_size();\n+  size_t v_soft_max_capacity = soft_max_capacity();\n+  size_t v_max_capacity = max_capacity();\n+  size_t v_available = available();\n+  LogGcInfo::print(\"%s generation used: \" SIZE_FORMAT \"%s, used regions: \" SIZE_FORMAT \"%s, \"\n+                   \"soft capacity: \" SIZE_FORMAT \"%s, max capacity: \" SIZE_FORMAT \" %s, available: \" SIZE_FORMAT \" %s\",\n+                   name(),\n+                   byte_size_in_proper_unit(v_used), proper_unit_for_byte_size(v_used),\n+                   byte_size_in_proper_unit(v_used_regions), proper_unit_for_byte_size(v_used_regions),\n+                   byte_size_in_proper_unit(v_soft_max_capacity), proper_unit_for_byte_size(v_soft_max_capacity),\n+                   byte_size_in_proper_unit(v_max_capacity), proper_unit_for_byte_size(v_max_capacity),\n+                   byte_size_in_proper_unit(v_available), proper_unit_for_byte_size(v_available));\n+}\n+\n+void ShenandoahGeneration::reset_mark_bitmap() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->assert_gc_workers(heap->workers()->active_workers());\n+\n+  set_mark_incomplete();\n+\n+  ShenandoahResetBitmapTask task;\n+  parallel_heap_region_iterate(&task);\n+}\n+\n+void ShenandoahGeneration::prepare_gc() {\n+  reset_mark_bitmap();\n+\n+  ShenandoahResetUpdateRegionStateClosure cl;\n+  parallel_heap_region_iterate(&cl);\n+}\n+\n+void ShenandoahGeneration::prepare_regions_and_collection_set(bool concurrent) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  assert(!heap->is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n+                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n+    ShenandoahFinalMarkUpdateRegionStateClosure cl(complete_marking_context());\n+    parallel_heap_region_iterate(&cl);\n+\n+    heap->assert_pinned_region_status();\n+  }\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n+                            ShenandoahPhaseTimings::degen_gc_choose_cset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->collection_set()->clear();\n+    _heuristics->choose_collection_set(heap->collection_set());\n+  }\n+\n+  {\n+    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n+                            ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->rebuild();\n+  }\n+}\n+\n+bool ShenandoahGeneration::is_bitmap_clear() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* context = heap->marking_context();\n+  size_t num_regions = heap->num_regions();\n+  for (size_t idx = 0; idx < num_regions; idx++) {\n+    ShenandoahHeapRegion* r = heap->get_region(idx);\n+    if (contains(r)) {\n+      if (heap->is_bitmap_slice_committed(r) && !context->is_bitmap_clear_range(r->bottom(), r->end())) {\n+        return false;\n+      }\n+    }\n+  }\n+  return true;\n+}\n+\n+bool ShenandoahGeneration::is_mark_complete() {\n+  return _is_marking_complete.is_set();\n+}\n+\n+void ShenandoahGeneration::set_mark_complete() {\n+  _is_marking_complete.set();\n+}\n+\n+void ShenandoahGeneration::set_mark_incomplete() {\n+  _is_marking_complete.unset();\n+}\n+\n+ShenandoahMarkingContext* ShenandoahGeneration::complete_marking_context() {\n+  assert(is_mark_complete(), \"Marking must be completed.\");\n+  return ShenandoahHeap::heap()->marking_context();\n+}\n+\n+void ShenandoahGeneration::cancel_marking() {\n+  if (is_concurrent_mark_in_progress()) {\n+    set_concurrent_mark_in_progress(false);\n+  }\n+  _task_queues->clear();\n+}\n+\n+ShenandoahGeneration::ShenandoahGeneration(GenerationMode generation_mode,\n+                                           uint max_queues,\n+                                           size_t max_capacity,\n+                                           size_t soft_max_capacity) :\n+  _generation_mode(generation_mode),\n+  _task_queues(new ShenandoahObjToScanQueueSet(max_queues)),\n+  _affiliated_region_count(0), _used(0),\n+  _max_capacity(max_capacity), _soft_max_capacity(soft_max_capacity) {\n+  _is_marking_complete.set();\n+  assert(max_queues > 0, \"At least one queue\");\n+  for (uint i = 0; i < max_queues; ++i) {\n+    ShenandoahObjToScanQueue* task_queue = new ShenandoahObjToScanQueue();\n+    task_queue->initialize();\n+    _task_queues->register_queue(i, task_queue);\n+  }\n+}\n+\n+ShenandoahGeneration::~ShenandoahGeneration() {\n+  for (uint i = 0; i < _task_queues->size(); ++i) {\n+    ShenandoahObjToScanQueue* q = _task_queues->queue(i);\n+    delete q;\n+  }\n+  delete _task_queues;\n+}\n+\n+void ShenandoahGeneration::reserve_task_queues(uint workers) {\n+  _task_queues->reserve(workers);\n+}\n+\n+ShenandoahObjToScanQueueSet* ShenandoahGeneration::old_gen_task_queues() const {\n+  return nullptr;\n+}\n+\n+void ShenandoahGeneration::scan_remembered_set() {\n+  shenandoah_assert_safepoint();\n+  assert(generation_mode() == YOUNG, \"Should only scan remembered set for young generation.\");\n+\n+  ShenandoahHeap* const heap = ShenandoahHeap::heap();\n+  \/\/ TODO: Add a phase for rset scan.\n+  \/\/ ShenandoahGCPhase phase(ShenandoahPhaseTimings::finish_mark);\n+  uint nworkers = heap->workers()->active_workers();\n+  reserve_task_queues(nworkers);\n+\n+  ShenandoahReferenceProcessor* rp = heap->ref_processor();\n+  ShenandoahRegionIterator regions;\n+  ShenandoahScanRememberedTask task(task_queues(), old_gen_task_queues(), rp, &regions);\n+  heap->workers()->run_task(&task);\n+}\n+\n+void ShenandoahGeneration::increment_affiliated_region_count() {\n+  _affiliated_region_count++;\n+}\n+\n+void ShenandoahGeneration::decrement_affiliated_region_count() {\n+  _affiliated_region_count--;\n+}\n+\n+void ShenandoahGeneration::increase_used(size_t bytes) {\n+  shenandoah_assert_heaplocked();\n+  _used += bytes;\n+}\n+\n+void ShenandoahGeneration::decrease_used(size_t bytes) {\n+  shenandoah_assert_heaplocked_or_safepoint();\n+  assert(_used >= bytes, \"cannot reduce bytes used by generation below zero\");\n+  _used -= bytes;\n+}\n+\n+size_t ShenandoahGeneration::used_regions_size() const {\n+  return _affiliated_region_count * ShenandoahHeapRegion::region_size_bytes();\n+}\n+\n+size_t ShenandoahGeneration::available() const {\n+  size_t in_use = used();\n+  size_t soft_capacity = soft_max_capacity();\n+  return in_use > soft_capacity ? 0 : soft_capacity - in_use;\n+}\n\\ No newline at end of file\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":277,"deletions":0,"binary":false,"changes":277,"status":"added"},{"patch":"@@ -29,1 +29,5 @@\n-#include \"gc\/shenandoah\/shenandoahConcurrentMark.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahLock.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkingContext.hpp\"\n+\n+class ShenandoahHeapRegion;\n@@ -34,0 +38,12 @@\n+  ShenandoahHeuristics* _heuristics;\n+\n+  \/\/ Marking task queues and completeness\n+  ShenandoahObjToScanQueueSet* _task_queues;\n+  ShenandoahSharedFlag _is_marking_complete;\n+\n+protected:\n+  \/\/ Usage\n+  size_t _affiliated_region_count;\n+  size_t _used;\n+  size_t _max_capacity;\n+  size_t _soft_max_capacity;\n@@ -36,3 +52,2 @@\n-  explicit ShenandoahGeneration(GenerationMode generation_mode) :\n-    _generation_mode(generation_mode) {\n-  }\n+  ShenandoahGeneration(GenerationMode generation_mode, uint max_queues, size_t max_capacity, size_t soft_max_capacity);\n+  ~ShenandoahGeneration();\n@@ -41,0 +56,66 @@\n+\n+  inline ShenandoahHeuristics* heuristics() const { return _heuristics; }\n+\n+  virtual const char* name() const = 0;\n+\n+  void initialize_heuristics(ShenandoahMode* gc_mode);\n+\n+  virtual size_t soft_max_capacity() const { return _soft_max_capacity; }\n+  virtual size_t max_capacity() const      { return _max_capacity; }\n+  virtual size_t used_regions_size() const;\n+  virtual size_t used() const { return _used; }\n+  virtual size_t available() const;\n+\n+  void set_soft_max_capacity(size_t soft_max_capacity) {\n+    _soft_max_capacity = soft_max_capacity;\n+  }\n+\n+  virtual size_t bytes_allocated_since_gc_start();\n+\n+  void log_status() const;\n+\n+  \/\/ Used directly by FullGC\n+  void reset_mark_bitmap();\n+\n+  \/\/ Used by concurrent and degenerated GC to reset regions.\n+  void prepare_gc();\n+  void prepare_regions_and_collection_set(bool concurrent);\n+\n+  \/\/ Cancel marking (used by Full collect and when cancelling cycle).\n+  void cancel_marking();\n+\n+  \/\/ Return true if this region is affiliated with this generation.\n+  virtual bool contains(ShenandoahHeapRegion* region) const = 0;\n+\n+  \/\/ Apply closure to all regions affiliated with this generation.\n+  virtual void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) = 0;\n+\n+  \/\/ This is public to support cancellation of marking when a Full cycle is started.\n+  virtual void set_concurrent_mark_in_progress(bool in_progress) = 0;\n+\n+  \/\/ Check the bitmap only for regions belong to this generation.\n+  bool is_bitmap_clear();\n+\n+  \/\/ We need to track the status of marking for different generations.\n+  bool is_mark_complete();\n+  void set_mark_complete();\n+  void set_mark_incomplete();\n+\n+  ShenandoahMarkingContext* complete_marking_context();\n+\n+  \/\/ Task queues\n+  ShenandoahObjToScanQueueSet* task_queues() const { return _task_queues; }\n+  virtual void reserve_task_queues(uint workers);\n+  virtual ShenandoahObjToScanQueueSet* old_gen_task_queues() const;\n+\n+  void scan_remembered_set();\n+\n+  void increment_affiliated_region_count();\n+  void decrement_affiliated_region_count();\n+\n+  void increase_used(size_t bytes);\n+  void decrease_used(size_t bytes);\n+\n+ protected:\n+\n+  virtual bool is_concurrent_mark_in_progress() = 0;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":85,"deletions":4,"binary":false,"changes":89,"status":"modified"},{"patch":"@@ -0,0 +1,76 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGlobalGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+\n+const char* ShenandoahGlobalGeneration::name() const {\n+  return \"GLOBAL\";\n+}\n+\n+size_t ShenandoahGlobalGeneration::max_capacity() const {\n+  return ShenandoahHeap::heap()->max_capacity();\n+}\n+\n+size_t ShenandoahGlobalGeneration::used_regions_size() const {\n+  return ShenandoahHeap::heap()->capacity();\n+}\n+\n+size_t ShenandoahGlobalGeneration::soft_max_capacity() const {\n+  return ShenandoahHeap::heap()->soft_max_capacity();\n+}\n+\n+size_t ShenandoahGlobalGeneration::used() const {\n+  return ShenandoahHeap::heap()->used();\n+}\n+\n+size_t ShenandoahGlobalGeneration::available() const {\n+  return ShenandoahHeap::heap()->free_set()->available();\n+}\n+\n+void ShenandoahGlobalGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_young_mark_in_progress(in_progress);\n+  heap->set_concurrent_old_mark_in_progress(in_progress);\n+}\n+\n+bool ShenandoahGlobalGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return true;\n+}\n+\n+void ShenandoahGlobalGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(cl);\n+}\n+\n+bool ShenandoahGlobalGeneration::is_concurrent_mark_in_progress() {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  return heap->is_concurrent_mark_in_progress();\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.cpp","additions":76,"deletions":0,"binary":false,"changes":76,"status":"added"},{"patch":"@@ -33,1 +33,20 @@\n-  ShenandoahGlobalGeneration() : ShenandoahGeneration(GLOBAL) { }\n+  ShenandoahGlobalGeneration(uint max_queues)\n+  : ShenandoahGeneration(GLOBAL, max_queues, 0, 0) { }\n+\n+public:\n+  virtual const char* name() const;\n+\n+  virtual size_t max_capacity() const;\n+  virtual size_t soft_max_capacity() const;\n+  virtual size_t used_regions_size() const;\n+  virtual size_t used() const;\n+  virtual size_t available() const;\n+\n+  virtual void set_concurrent_mark_in_progress(bool in_progress);\n+\n+  bool contains(ShenandoahHeapRegion* region) const;\n+\n+  void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl);\n+\n+ protected:\n+  bool is_concurrent_mark_in_progress();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":20,"deletions":1,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n@@ -55,0 +56,1 @@\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n@@ -162,3 +164,0 @@\n-  \/\/ Now we know the number of regions, initialize the heuristics.\n-  initialize_heuristics();\n-\n@@ -180,0 +179,4 @@\n+  \/\/ Now we know the number of regions and heap sizes, initialize the heuristics.\n+  initialize_generations();\n+  initialize_heuristics();\n+\n@@ -286,1 +289,1 @@\n-  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions, _max_workers);\n+  _marking_context = new ShenandoahMarkingContext(_heap_region, _bitmap_region, _num_regions);\n@@ -433,0 +436,1 @@\n+  _regulator_thread = new ShenandoahRegulatorThread(_control_thread);\n@@ -439,0 +443,11 @@\n+void ShenandoahHeap::initialize_generations() {\n+  size_t max_capacity_new      = young_generation_capacity(max_capacity());\n+  size_t soft_max_capacity_new = young_generation_capacity(soft_max_capacity());\n+  size_t max_capacity_old      = max_capacity() - max_capacity_new;\n+  size_t soft_max_capacity_old = soft_max_capacity() - soft_max_capacity_new;\n+\n+  _young_generation = new ShenandoahYoungGeneration(_max_workers, max_capacity_new, soft_max_capacity_new);\n+  _old_generation = new ShenandoahOldGeneration(_max_workers, max_capacity_old, soft_max_capacity_old);\n+  _global_generation = new ShenandoahGlobalGeneration(_max_workers);\n+}\n+\n@@ -467,12 +482,3 @@\n-  _heuristics = _gc_mode->initialize_heuristics();\n-\n-  if (_heuristics->is_diagnostic() && !UnlockDiagnosticVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is diagnostic, and must be enabled via -XX:+UnlockDiagnosticVMOptions.\",\n-                    _heuristics->name()));\n-  }\n-  if (_heuristics->is_experimental() && !UnlockExperimentalVMOptions) {\n-    vm_exit_during_initialization(\n-            err_msg(\"Heuristics \\\"%s\\\" is experimental, and must be enabled via -XX:+UnlockExperimentalVMOptions.\",\n-                    _heuristics->name()));\n-  }\n+  _global_generation->initialize_heuristics(_gc_mode);\n+  _young_generation->initialize_heuristics(_gc_mode);\n+  _old_generation->initialize_heuristics(_gc_mode);\n@@ -488,0 +494,1 @@\n+  _gc_generation(NULL),\n@@ -499,2 +506,3 @@\n-  _young_generation(new ShenandoahYoungGeneration()),\n-  _global_generation(new ShenandoahGlobalGeneration()),\n+  _young_generation(NULL),\n+  _global_generation(NULL),\n+  _old_generation(NULL),\n@@ -502,0 +510,1 @@\n+  _regulator_thread(NULL),\n@@ -503,1 +512,0 @@\n-  _heuristics(NULL),\n@@ -532,29 +540,0 @@\n-class ShenandoahResetBitmapTask : public AbstractGangTask {\n-private:\n-  ShenandoahRegionIterator _regions;\n-\n-public:\n-  ShenandoahResetBitmapTask() :\n-    AbstractGangTask(\"Shenandoah Reset Bitmap\") {}\n-\n-  void work(uint worker_id) {\n-    ShenandoahHeapRegion* region = _regions.next();\n-    ShenandoahHeap* heap = ShenandoahHeap::heap();\n-    ShenandoahMarkingContext* const ctx = heap->marking_context();\n-    while (region != NULL) {\n-      if (heap->is_bitmap_slice_committed(region)) {\n-        ctx->clear_bitmap(region);\n-      }\n-      region = _regions.next();\n-    }\n-  }\n-};\n-\n-void ShenandoahHeap::reset_mark_bitmap() {\n-  assert_gc_workers(_workers->active_workers());\n-  mark_incomplete_marking_context();\n-\n-  ShenandoahResetBitmapTask task;\n-  _workers->run_task(&task);\n-}\n-\n@@ -575,1 +554,2 @@\n-  if (is_concurrent_mark_in_progress())        st->print(\"marking, \");\n+  if (is_concurrent_old_mark_in_progress())    st->print(\"old marking, \");\n+  if (is_concurrent_young_mark_in_progress())  st->print(\"young marking, \");\n@@ -638,2 +618,0 @@\n-  _heuristics->initialize();\n-\n@@ -643,0 +621,30 @@\n+bool ShenandoahHeap::is_gc_generation_young() const {\n+  return _gc_generation != NULL && _gc_generation->generation_mode() == YOUNG;\n+}\n+\n+\/\/ There are three JVM parameters for setting young gen capacity:\n+\/\/    NewSize, MaxNewSize, NewRatio.\n+\/\/\n+\/\/ If only NewSize is set, it assigns a fixed size and the other two parameters are ignored.\n+\/\/ Otherwise NewRatio applies.\n+\/\/\n+\/\/ If NewSize is set in any combination, it provides a lower bound.\n+\/\/\n+\/\/ If MaxNewSize is set it provides an upper bound.\n+\/\/ If this bound is smaller than NewSize, it supersedes,\n+\/\/ resulting in a fixed size given by MaxNewSize.\n+size_t ShenandoahHeap::young_generation_capacity(size_t capacity) {\n+  if (FLAG_IS_CMDLINE(NewSize) && !FLAG_IS_CMDLINE(MaxNewSize) && !FLAG_IS_CMDLINE(NewRatio)) {\n+    capacity = MIN2(NewSize, capacity);\n+  } else {\n+    capacity \/= NewRatio + 1;\n+    if (FLAG_IS_CMDLINE(NewSize)) {\n+      capacity = MAX2(NewSize, capacity);\n+    }\n+    if (FLAG_IS_CMDLINE(MaxNewSize)) {\n+      capacity = MIN2(MaxNewSize, capacity);\n+    }\n+  }\n+  return capacity;\n+}\n+\n@@ -714,0 +722,7 @@\n+\n+  if (mode()->is_generational()) {\n+    size_t soft_max_capacity_young = young_generation_capacity(_soft_max_size);\n+    size_t soft_max_capacity_old = _soft_max_size - soft_max_capacity_young;\n+    _young_generation->set_soft_max_capacity(soft_max_capacity_young);\n+    _old_generation->set_soft_max_capacity(soft_max_capacity_old);\n+  }\n@@ -731,1 +746,5 @@\n-  return heap_region_containing(p)->affiliation() == YOUNG_GENERATION;\n+  return heap_region_containing(p)->affiliation() == ShenandoahRegionAffiliation::YOUNG_GENERATION;\n+}\n+\n+bool ShenandoahHeap::is_in_old(const void* p) const {\n+  return heap_region_containing(p)->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION;\n@@ -761,0 +780,1 @@\n+    regulator_thread()->notify_heap_changed();\n@@ -887,0 +907,1 @@\n+    regulator_thread()->notify_heap_changed();\n@@ -916,1 +937,37 @@\n-  return _free_set->allocate(req, in_new_region);\n+  HeapWord* result = _free_set->allocate(req, in_new_region);\n+  \/\/ Register the newly allocated object while we're holding the global lock since there's no synchronization\n+  \/\/ built in to the implementation of register_object().  There are potential races when multiple independent\n+  \/\/ threads are allocating objects, some of which might span the same card region.  For example, consider\n+  \/\/ a card table's memory region within which three objects are being allocated by three different threads:\n+  \/\/\n+  \/\/ objects being \"concurrently\" allocated:\n+  \/\/    [-----a------][-----b-----][--------------c------------------]\n+  \/\/            [---- card table memory range --------------]\n+  \/\/\n+  \/\/ Before any objects are allocated, this card's memory range holds no objects.  Note that:\n+  \/\/   allocation of object a wants to set the has-object, first-start, and last-start attributes of the preceding card region.\n+  \/\/   allocation of object b wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/   allocation of object c also wants to set the has-object, first-start, and last-start attributes of this card region.\n+  \/\/\n+  \/\/ The thread allocating b and the thread allocating c can \"race\" in various ways, resulting in confusion, such as last-start\n+  \/\/ representing object b while first-start represents object c.  This is why we need to require all register_object()\n+  \/\/ invocations to be \"mutually exclusive\".  Later, when we use GCLABs to allocate memory for promotions and evacuations,\n+  \/\/ the protocol may work something like the following:\n+  \/\/   1. The GCLAB is allocated by this (or similar) function, while holding the global lock.\n+  \/\/   2. The GCLAB is registered as a single object.\n+  \/\/\/  3. The GCLAB is always aligned at the start of a card memory range and is always a multiple of the card-table memory range size\n+  \/\/   3. Individual allocations carved from the GCLAB are not immediately registered\n+  \/\/   4. When the GCLAB is eventually retired, all of the objects allocated within the GCLAB are registered in batch by a\n+  \/\/      single thread.  No further synchronization is required because no other allocations will pertain to the same\n+  \/\/      card-table memory ranges.\n+  \/\/\n+  \/\/ The other case that needs special handling is promotion of regions en masse.  When the region is promoted, all objects contained\n+  \/\/ within the region are registered.  Since the region is a multiple of card-table memory range sizes, there is no need for\n+  \/\/ synchronization.  It might be nice to figure out how to allow multiple threads to work together to register all of the objects in\n+  \/\/ a promoted region, or at least try to balance the efforts so that different gc threads work on registering the objects of\n+  \/\/ different heap regions.  But that effort will come later.\n+  \/\/\n+  if (result != NULL && req.affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+    ShenandoahHeap::heap()->card_scan()->register_object(result);\n+  }\n+  return result;\n@@ -931,2 +988,2 @@\n-  if (heuristics()->can_unload_classes()) {\n-    ShenandoahHeuristics* h = heuristics();\n+  ShenandoahHeuristics* h = global_generation()->heuristics();\n+  if (h->can_unload_classes()) {\n@@ -1010,1 +1067,2 @@\n-      assert(r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n+      \/\/ Generational mode doesn't support immediate collection\n+      assert(_sh->mode()->is_generational() || r->has_live(), \"Region \" SIZE_FORMAT \" should have been reclaimed early\", r->index());\n@@ -1550,23 +1608,0 @@\n-class ShenandoahInitMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahInitMarkUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-    if (r->is_active()) {\n-      \/\/ Check if region needs updating its TAMS. We have updated it already during concurrent\n-      \/\/ reset, so it is very likely we don't need to do another write here.\n-      if (_ctx->top_at_mark_start(r) != r->top()) {\n-        _ctx->capture_top_at_mark_start(r);\n-      }\n-    } else {\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should already have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n@@ -1582,99 +1617,0 @@\n-class ShenandoahResetUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-public:\n-  ShenandoahResetUpdateRegionStateClosure() : _ctx(ShenandoahHeap::heap()->marking_context()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ Reset live data and set TAMS optimistically. We would recheck these under the pause\n-      \/\/ anyway to capture any updates that happened since now.\n-      r->clear_live_data();\n-      _ctx->capture_top_at_mark_start(r);\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_gc() {\n-  reset_mark_bitmap();\n-\n-  ShenandoahResetUpdateRegionStateClosure cl;\n-  parallel_heap_region_iterate(&cl);\n-}\n-\n-class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n-private:\n-  ShenandoahMarkingContext* const _ctx;\n-  ShenandoahHeapLock* const _lock;\n-\n-public:\n-  ShenandoahFinalMarkUpdateRegionStateClosure() :\n-    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n-\n-  void heap_region_do(ShenandoahHeapRegion* r) {\n-    if (r->is_active()) {\n-      \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n-      \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n-      HeapWord *tams = _ctx->top_at_mark_start(r);\n-      HeapWord *top = r->top();\n-      if (top > tams) {\n-        r->increase_live_data_alloc_words(pointer_delta(top, tams));\n-      }\n-\n-      \/\/ We are about to select the collection set, make sure it knows about\n-      \/\/ current pinning status. Also, this allows trashing more regions that\n-      \/\/ now have their pinning status dropped.\n-      if (r->is_pinned()) {\n-        if (r->pin_count() == 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_unpinned();\n-        }\n-      } else {\n-        if (r->pin_count() > 0) {\n-          ShenandoahHeapLocker locker(_lock);\n-          r->make_pinned();\n-        }\n-      }\n-\n-      \/\/ Remember limit for updating refs. It's guaranteed that we get no\n-      \/\/ from-space-refs written from here on.\n-      r->set_update_watermark_at_safepoint(r->top());\n-    } else {\n-      assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n-      assert(_ctx->top_at_mark_start(r) == r->top(),\n-             \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n-    }\n-  }\n-\n-  bool is_thread_safe() { return true; }\n-};\n-\n-void ShenandoahHeap::prepare_regions_and_collection_set(bool concurrent) {\n-  assert(!is_full_gc_in_progress(), \"Only for concurrent and degenerated GC\");\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_update_region_states :\n-                                         ShenandoahPhaseTimings::degen_gc_final_update_region_states);\n-    ShenandoahFinalMarkUpdateRegionStateClosure cl;\n-    parallel_heap_region_iterate(&cl);\n-\n-    assert_pinned_region_status();\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::choose_cset :\n-                                         ShenandoahPhaseTimings::degen_gc_choose_cset);\n-    ShenandoahHeapLocker locker(lock());\n-    _collection_set->clear();\n-    heuristics()->choose_collection_set(_collection_set);\n-  }\n-\n-  {\n-    ShenandoahGCPhase phase(concurrent ? ShenandoahPhaseTimings::final_rebuild_freeset :\n-                                         ShenandoahPhaseTimings::degen_gc_final_rebuild_freeset);\n-    ShenandoahHeapLocker locker(lock());\n-    _free_set->rebuild();\n-  }\n-}\n-\n@@ -1723,1 +1659,11 @@\n-void ShenandoahHeap::set_concurrent_mark_in_progress(bool in_progress) {\n+void ShenandoahHeap::set_concurrent_young_mark_in_progress(bool in_progress) {\n+  if (has_forwarded_objects()) {\n+    set_gc_state_mask(YOUNG_MARKING | UPDATEREFS, in_progress);\n+  } else {\n+    set_gc_state_mask(YOUNG_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::set_concurrent_old_mark_in_progress(bool in_progress) {\n@@ -1725,1 +1671,1 @@\n-    set_gc_state_mask(MARKING | UPDATEREFS, in_progress);\n+    set_gc_state_mask(OLD_MARKING | UPDATEREFS, in_progress);\n@@ -1727,1 +1673,19 @@\n-    set_gc_state_mask(MARKING, in_progress);\n+    set_gc_state_mask(OLD_MARKING, in_progress);\n+  }\n+\n+  manage_satb_barrier(in_progress);\n+}\n+\n+void ShenandoahHeap::manage_satb_barrier(bool active) {\n+  if (is_concurrent_mark_in_progress()) {\n+    \/\/ Ignore request to deactivate barrier while concurrent mark is in progress.\n+    \/\/ Do not attempt to re-activate the barrier if it is already active.\n+    if (active && !ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n+  } else {\n+    \/\/ No concurrent marking is in progress so honor request to deactivate,\n+    \/\/ but only if the barrier is already active.\n+    if (!active && ShenandoahBarrierSet::satb_mark_queue_set().is_active()) {\n+      ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(active, !active);\n+    }\n@@ -1729,1 +1693,0 @@\n-  ShenandoahBarrierSet::satb_mark_queue_set().set_active_all_threads(in_progress, !in_progress);\n@@ -1778,0 +1741,11 @@\n+void ShenandoahHeap::cancel_concurrent_mark() {\n+  _young_generation->cancel_marking();\n+  _old_generation->cancel_marking();\n+  _global_generation->cancel_marking();\n+\n+  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+\n+  \/\/ HEY! Previously, only ShenandoahConcurrentMark::cancel (static) cleared ref processor.\n+  ref_processor()->abandon_partial_discovery();\n+}\n+\n@@ -1793,0 +1767,3 @@\n+  \/\/ Step 0a. Stop requesting collections.\n+  regulator_thread()->stop();\n+\n@@ -1902,4 +1879,0 @@\n-address ShenandoahHeap::cancelled_gc_addr() {\n-  return (address) ShenandoahHeap::heap()->_cancelled_gc.addr_of();\n-}\n-\n@@ -1981,2 +1954,4 @@\n-    assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n-           \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    if (active_generation()->contains(r)) {\n+      assert((r->is_pinned() && r->pin_count() > 0) || (!r->is_pinned() && r->pin_count() == 0),\n+             \"Region \" SIZE_FORMAT \" pinning status is inconsistent\", i);\n+    }\n@@ -2065,1 +2040,3 @@\n-    ShenandoahMarkingContext* const ctx = _heap->complete_marking_context();\n+    \/\/ We update references for global and young collections.\n+    assert(_heap->active_generation()->is_mark_complete(), \"Expected complete marking\");\n+    ShenandoahMarkingContext* const ctx = _heap->marking_context();\n@@ -2071,47 +2048,38 @@\n-      \/\/ Eventually, scanning of old-gen memory regions for the purpose of updating references can happen\n-      \/\/ concurrently.  This can be done during concurrent evacuation of roots for example.\n-      if (r->is_active() && (r->affiliation() == ShenandoahRegionAffiliation::OLD_GENERATION) && !r->is_cset()) {\n-\n-        \/\/ Note that we use this code even if we are doing an old-gen collection and we have a bitmap to\n-        \/\/ represent marked objects within the heap region.\n-        \/\/\n-        \/\/ It is necessary to process all objects rather than just the marked objects during update-refs of\n-        \/\/ an old-gen region as part of an old-gen collection.  Otherwise, a subseqent update-refs scan of\n-        \/\/ the same region will see stale pointers and crash.\n-        \/\/\n-        \/\/   r->top() represents the upper end of memory that has been allocated within this region.\n-        \/\/       As new objects are allocated, the value of r->top() increases to accomodate each new\n-        \/\/       object.\n-        \/\/   At the start of evacuation, \"update_watermark\" is initalized to represent the value of top().\n-        \/\/       Objects newly allocated during evacuation do not need to be visited during update-refs\n-        \/\/       because the to-space invariant which is in force throughout evacuation assures that no from-space\n-        \/\/       pointer is written to any newly allocated object.  In the case that survivor objects are evacuated\n-        \/\/       into this region during evacuation, the region's watermark is incremented to represent the end of\n-        \/\/       the memory range known to hold newly evacuated objects.  Regions that receive evacuated objects\n-        \/\/       are distinct from regions that serve new object allocation requests.  A region's watermark is not\n-        \/\/       increased when objects are newly allocated within that region during evacuation.\n-\n-        HeapWord *p = r->bottom();\n-        ShenandoahObjectToOopBoundedClosure<T> objs(&cl, p, update_watermark);\n-\n-        \/\/ TODO: This code assumes every object ever allocated within this old-gen region is still live.  If we\n-        \/\/ allow a sweep phase to turn garbage objects into free memory regions, we need to modify this code to\n-        \/\/ skip over and\/or synchronize access to these free memory regions.  There might be races, for example,\n-        \/\/ if we are trying to scan one of these free memory regions while a different thread is trying to\n-        \/\/ allocate from within a free region.\n-        \/\/\n-        \/\/ Alternative approaches are also under consideration.  For example:\n-        \/\/  1. Coalesce, fill, and register each range of contiguous dead objects so that subsequent updating of\n-        \/\/     references can be done more efficiently.\n-        \/\/  2. Retain the mark bitmap from the most recently completed old GC effort and use this bitmap to allow\n-        \/\/     skipping over objects that were not live as of the most recently completed old-gen GC effort.\n-\n-        \/\/ Anything beyond update_watermark does not need to be updated.\n-        while (p < update_watermark) {\n-          oop obj = oop(p);\n-\n-          \/\/ The invocation of do_object() is \"borrowed\" from the implementation of\n-          \/\/ ShenandoahHeap::marked_object_iterate(), which is called by _heap->marked_object_oop_iterate().\n-          objs.do_object(obj);\n-          p += obj->size();\n-\n+      if (r->is_active() && !r->is_cset()) {\n+        if (r->affiliation() == YOUNG_GENERATION || !_heap->mode()->is_generational()) {\n+          _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n+        } else {\n+          assert(r->affiliation() == OLD_GENERATION, \"Should not be updating references on FREE regions\");\n+          if (!_heap->is_gc_generation_young()) {\n+            \/\/ Old region in a global cycle.\n+            \/\/ We need to make sure that the next cycle does not iterate over dead objects\n+            \/\/ which haven't had their references updated.\n+            r->oop_iterate(&cl, \/*fill_dead_objects*\/ true, \/* reregister_coalesced_objects *\/ true);\n+          } else if (ShenandoahBarrierSet::barrier_set()->card_table()->is_dirty(MemRegion(r->bottom(), r->top()))) {\n+            \/\/ Old region in a young cycle.\n+            if (r->is_humongous()) {\n+              r->oop_iterate_humongous(&cl);\n+            } else {\n+              \/\/ We don't have liveness information about this region.\n+              \/\/ Therefore we process all objects, rather than just marked ones.\n+              \/\/ Otherwise subsequent traversals will encounter stale pointers.\n+\n+              \/\/ HEY! kelvin thinks we don't have to update refs throughout the entire region r.  We only need\n+              \/\/ to update refs for objects that span dirty cards.  The code in process clusters does that.  We cannot invoke process_clusters\n+              \/\/ because that's designed to process transitive closure of live objects.  Here, we are just looking\n+              \/\/ one level deep in each of the relevant regions.  But we can copy and paste some of the code from\n+              \/\/ there.\n+\n+              \/\/ HEY moreover!  Need to figure out how regions are partitioned between worker threads.  Is it possible\n+              \/\/ that each region is being processed redundantly by each worker thread?\n+\n+              HeapWord *p = r->bottom();\n+              ShenandoahObjectToOopBoundedClosure<T> objs(&cl, p, update_watermark);\n+              \/\/ Anything beyond update_watermark is not yet allocated or initialized.\n+              while (p < update_watermark) {\n+                oop obj = oop(p);\n+                objs.do_object(obj);\n+                p += obj->size();\n+              }\n+            }\n+          }\n@@ -2120,3 +2088,0 @@\n-      else if (r->is_active() && !r->is_cset()) {\n-        _heap->marked_object_oop_iterate(r, &cl, update_watermark);\n-      }\n@@ -2206,0 +2171,7 @@\n+\n+  \/\/ HEY! this code and rebuild free set used to be in op_final_updaterefs\n+  if (mode()->is_generational() && is_gc_generation_young() && ShenandoahPromoteTenuredRegions) {\n+    ShenandoahGCPhase phase(ShenandoahPhaseTimings::final_update_refs_promote_tenured_regions);\n+    ShenandoahHeapLocker locker(lock());\n+    young_generation()->promote_tenured_regions();\n+  }\n@@ -2392,0 +2364,21 @@\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<YOUNG>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit young and free regions\n+  if (region->affiliation() != OLD_GENERATION) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<OLD>::heap_region_do(ShenandoahHeapRegion* region) {\n+  \/\/ Visit old and free regions\n+  if (region->affiliation() != YOUNG_GENERATION) {\n+    _cl->heap_region_do(region);\n+  }\n+}\n+\n+template<>\n+void ShenandoahGenerationRegionClosure<GLOBAL>::heap_region_do(ShenandoahHeapRegion* region) {\n+  _cl->heap_region_do(region);\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":232,"deletions":239,"binary":false,"changes":471,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+class ShenandoahRegulatorThread;\n@@ -51,0 +52,1 @@\n+class ShenandoahYoungGeneration;\n@@ -106,0 +108,6 @@\n+enum GenerationMode {\n+  YOUNG,\n+  OLD,\n+  GLOBAL\n+};\n+\n@@ -112,0 +120,10 @@\n+template<GenerationMode GENERATION>\n+class ShenandoahGenerationRegionClosure : public ShenandoahHeapRegionClosure {\n+ public:\n+  explicit ShenandoahGenerationRegionClosure(ShenandoahHeapRegionClosure* cl) : _cl(cl) {}\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+  virtual bool is_thread_safe() { return _cl->is_thread_safe(); }\n+ private:\n+  ShenandoahHeapRegionClosure* _cl;\n+};\n+\n@@ -136,0 +154,1 @@\n+  ShenandoahGeneration* _gc_generation;\n@@ -142,0 +161,11 @@\n+  ShenandoahGeneration* active_generation() {\n+    \/\/ last or latest generation might be a better name here.\n+    return _gc_generation;\n+  }\n+\n+  void set_gc_generation(ShenandoahGeneration* generation) {\n+    _gc_generation = generation;\n+  }\n+\n+  bool is_gc_generation_young() const;\n+\n@@ -154,0 +184,1 @@\n+  void initialize_generations();\n@@ -179,0 +210,2 @@\n+  static size_t young_generation_capacity(size_t total_capacity);\n+\n@@ -255,2 +288,2 @@\n-    \/\/ Heap is under marking: needs SATB barriers.\n-    MARKING_BITPOS    = 1,\n+    \/\/ Young regions are under marking: needs SATB barriers.\n+    YOUNG_MARKING_BITPOS    = 1,\n@@ -263,0 +296,3 @@\n+\n+    \/\/ Old regions are under marking, still need SATB barriers.\n+    OLD_MARKING_BITPOS = 4\n@@ -268,1 +304,1 @@\n-    MARKING       = 1 << MARKING_BITPOS,\n+    YOUNG_MARKING = 1 << YOUNG_MARKING_BITPOS,\n@@ -271,0 +307,1 @@\n+    OLD_MARKING   = 1 << OLD_MARKING_BITPOS\n@@ -289,1 +326,2 @@\n-  void set_concurrent_mark_in_progress(bool in_progress);\n+  void set_concurrent_young_mark_in_progress(bool in_progress);\n+  void set_concurrent_old_mark_in_progress(bool in_progress);\n@@ -302,0 +340,2 @@\n+  inline bool is_concurrent_young_mark_in_progress() const;\n+  inline bool is_concurrent_old_mark_in_progress() const;\n@@ -314,0 +354,2 @@\n+  void manage_satb_barrier(bool active);\n+\n@@ -333,2 +375,0 @@\n-  static address cancelled_gc_addr();\n-\n@@ -338,1 +378,1 @@\n-  inline void clear_cancelled_gc();\n+  inline void clear_cancelled_gc(bool clear_oom_handler = true);\n@@ -340,0 +380,1 @@\n+  void cancel_concurrent_mark();\n@@ -349,3 +390,0 @@\n-  \/\/ Reset bitmap, prepare regions for new GC cycle\n-  void prepare_gc();\n-  void prepare_regions_and_collection_set(bool concurrent);\n@@ -353,1 +391,0 @@\n-  void prepare_evacuation(bool concurrent);\n@@ -377,1 +414,1 @@\n-  ShenandoahGeneration*      _young_generation;\n+  ShenandoahYoungGeneration* _young_generation;\n@@ -379,0 +416,1 @@\n+  ShenandoahGeneration*      _old_generation;\n@@ -380,0 +418,1 @@\n+  ShenandoahRegulatorThread* _regulator_thread;\n@@ -382,1 +421,0 @@\n-  ShenandoahHeuristics*      _heuristics;\n@@ -390,0 +428,1 @@\n+  ShenandoahRegulatorThread* regulator_thread()        { return _regulator_thread;  }\n@@ -392,1 +431,1 @@\n-  ShenandoahGeneration*      young_generation()  const { return _young_generation;  }\n+  ShenandoahYoungGeneration* young_generation()  const { return _young_generation;  }\n@@ -394,0 +433,2 @@\n+  ShenandoahGeneration*      old_generation()    const { return _old_generation;    }\n+\n@@ -396,1 +437,0 @@\n-  ShenandoahHeuristics*      heuristics()        const { return _heuristics;        }\n@@ -471,0 +511,2 @@\n+  bool is_in_old(const void* p) const;\n+  inline bool is_old(oop pobj) const;\n@@ -575,2 +617,0 @@\n-  inline void mark_complete_marking_context();\n-  inline void mark_incomplete_marking_context();\n@@ -587,2 +627,0 @@\n-  void reset_mark_bitmap();\n-\n@@ -609,0 +647,2 @@\n+  inline oop try_evacuate_object(oop src, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahRegionAffiliation target_gen);\n+\n@@ -620,1 +660,1 @@\n-  \/\/ Evacuates object src. Returns the evacuated object, either evacuated\n+  \/\/ Evacuates or promotes object src. Returns the evacuated object, either evacuated\n@@ -635,1 +675,1 @@\n-\n+  void clear_cards_for(ShenandoahHeapRegion* region);\n@@ -654,0 +694,2 @@\n+  static inline void increase_object_age(oop obj, uint additional_age);\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":63,"deletions":21,"binary":false,"changes":84,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n@@ -185,1 +186,1 @@\n-inline void ShenandoahHeap::clear_cancelled_gc() {\n+inline void ShenandoahHeap::clear_cancelled_gc(bool clear_oom_handler) {\n@@ -187,1 +188,3 @@\n-  _oom_evac_handler.clear();\n+  if (clear_oom_handler) {\n+    _oom_evac_handler.clear();\n+  }\n@@ -217,3 +220,9 @@\n-  ShenandoahRegionAffiliation target_gen = heap_region_containing(p)->affiliation();\n-  if (target_gen == YOUNG_GENERATION) {\n-    if (ShenandoahForwarding::is_forwarded(p)) {\n+  ShenandoahHeapRegion* r = heap_region_containing(p);\n+  assert(!r->is_humongous(), \"never evacuate humongous objects\");\n+\n+  ShenandoahRegionAffiliation target_gen = r->affiliation();\n+  if (mode()->is_generational() && ShenandoahHeap::heap()->is_gc_generation_young() &&\n+      target_gen == YOUNG_GENERATION && ShenandoahPromoteTenuredObjects) {\n+    markWord mark = p->mark();\n+    if (mark.is_marked()) {\n+      \/\/ Already forwarded.\n@@ -221,3 +230,16 @@\n-    } else if (p->mark().age() > InitialTenuringThreshold) {\n-      \/\/tty->print_cr(\"promoting object: \" PTR_FORMAT, p2i(p));\n-      \/\/target_gen = OLD_GEN;\n+    }\n+    if (mark.has_displaced_mark_helper()) {\n+      \/\/ We don't want to deal with MT here just to ensure we read the right mark word.\n+      \/\/ Skip the potential promotion attempt for this one.\n+    } else if (mark.age() >= InitialTenuringThreshold) {\n+      oop result = try_evacuate_object(p, thread, r, OLD_GENERATION);\n+      if (result != NULL) {\n+        \/\/ TODO: Just marking the cards covering this object dirty\n+        \/\/ may overall be less efficient than scanning it now for references to young gen\n+        \/\/ or other alternatives like deferred card marking or scanning.\n+        \/\/ We should revisit this.\n+        \/\/ Furthermore, the object start should be registered for remset scanning.\n+        MemRegion mr(cast_from_oop<HeapWord*>(result), result->size());\n+        ShenandoahBarrierSet::barrier_set()->card_table()->invalidate(mr);\n+        return result;\n+      }\n@@ -226,0 +248,2 @@\n+  return try_evacuate_object(p, thread, r, target_gen);\n+}\n@@ -227,2 +251,1 @@\n-  assert(!heap_region_containing(p)->is_humongous(), \"never evacuate humongous objects\");\n-\n+inline oop ShenandoahHeap::try_evacuate_object(oop p, Thread* thread, ShenandoahHeapRegion* from_region, ShenandoahRegionAffiliation target_gen) {\n@@ -252,0 +275,5 @@\n+    if (target_gen == OLD_GENERATION && from_region->affiliation() == YOUNG_GENERATION) {\n+      \/\/ Indicate that a promotion attempt failed.\n+      return NULL;\n+    }\n+\n@@ -262,1 +290,0 @@\n-  \/\/ Try to install the new forwarding pointer.\n@@ -264,0 +291,14 @@\n+  if (target_gen == YOUNG_GENERATION) {\n+    \/\/ Increment the age in young copies, absorbing region age.\n+    \/\/ (Only retired regions will have more than zero age to pass along.)\n+\n+    ShenandoahHeap::increase_object_age(copy_val, from_region->age() + 1);\n+\n+    \/\/ Note that p may have been forwarded by another thread,\n+    \/\/ anywhere between here and the check above for forwarding.\n+    \/\/ In that case try_update_forwardee() below will not be successful\n+    \/\/ and the increment we just performed will simply be forgotten,\n+    \/\/ but it will have succeeded in said other thread.\n+  }\n+\n+  \/\/ Try to install the new forwarding pointer.\n@@ -268,12 +309,0 @@\n-\n-    \/\/ Hey!  This code showed up in a merge conflict.  It has \"nothing\" to do with the patch that\n-    \/\/ was merged, so kdnilsen is leaving it in place as is.  However, it looks to me like the object's\n-    \/\/ age should be incremented before the copy is committed to avoid the need for synchronization here.\n-    \/\/\n-    \/\/ kdnilsen believes the following code is replaced\/relocated in a subsequent commit.\n-\n-    \/\/ Increment age in young copies.\n-    if (target_gen == YOUNG_GENERATION) {\n-      copy_val->incr_age();\n-    }\n-\n@@ -293,0 +322,2 @@\n+\n+\n@@ -304,0 +335,15 @@\n+void ShenandoahHeap::increase_object_age(oop obj, uint additional_age) {\n+  markWord w = obj->has_displaced_mark() ? obj->displaced_mark() : obj->mark();\n+  w = w.set_age(MIN2(markWord::max_age, w.age() + additional_age));\n+  if (obj->has_displaced_mark()) {\n+    obj->set_displaced_mark(w);\n+  } else {\n+    obj->set_mark(w);\n+  }\n+}\n+\n+\n+inline bool ShenandoahHeap::is_old(oop obj) const {\n+  return is_gc_generation_young() && is_in_old(obj);\n+}\n+\n@@ -324,1 +370,1 @@\n-  return _gc_state.is_unset(MARKING | EVACUATION | UPDATEREFS);\n+  return _gc_state.is_unset(YOUNG_MARKING | OLD_MARKING | EVACUATION | UPDATEREFS);\n@@ -328,1 +374,9 @@\n-  return _gc_state.is_set(MARKING);\n+  return _gc_state.is_set(YOUNG_MARKING | OLD_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_young_mark_in_progress() const {\n+  return _gc_state.is_set(YOUNG_MARKING);\n+}\n+\n+inline bool ShenandoahHeap::is_concurrent_old_mark_in_progress() const {\n+  return _gc_state.is_set(OLD_MARKING);\n@@ -376,2 +430,3 @@\n-  ShenandoahMarkingContext* const ctx = complete_marking_context();\n-  assert(ctx->is_complete(), \"sanity\");\n+  ShenandoahMarkingContext* const ctx = marking_context();\n+  \/\/ HEY! All callers (at the time of this writing) have already asserted the mark context is complete.\n+  \/\/ assert(ctx->is_complete(), \"sanity\");\n@@ -508,8 +563,0 @@\n-inline void ShenandoahHeap::mark_complete_marking_context() {\n-  _marking_context->mark_complete();\n-}\n-\n-inline void ShenandoahHeap::mark_incomplete_marking_context() {\n-  _marking_context->mark_incomplete();\n-}\n-\n@@ -525,0 +572,6 @@\n+inline void ShenandoahHeap::clear_cards_for(ShenandoahHeapRegion* region) {\n+  if (mode()->is_generational()) {\n+    _card_scan->mark_range_as_empty(region->bottom(), (uint32_t) (region->end() - region->bottom()));\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":88,"deletions":35,"binary":false,"changes":123,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n@@ -32,0 +33,3 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -45,0 +49,3 @@\n+#include \"shenandoahGeneration.hpp\"\n+\n+#undef DEBUG_TRACE\n@@ -71,1 +78,2 @@\n-  _affiliation(ShenandoahRegionAffiliation::FREE) {\n+  _affiliation(ShenandoahRegionAffiliation::FREE),\n+  _age(0) {\n@@ -90,1 +98,1 @@\n-\n+  reset_age();\n@@ -108,1 +116,1 @@\n-\n+  reset_age();\n@@ -131,0 +139,1 @@\n+  reset_age();\n@@ -145,1 +154,1 @@\n-\n+  reset_age();\n@@ -160,0 +169,1 @@\n+  reset_age();\n@@ -174,1 +184,1 @@\n-\n+  reset_age();\n@@ -233,0 +243,1 @@\n+  reset_age();\n@@ -245,0 +256,1 @@\n+  reset_age();\n@@ -265,1 +277,2 @@\n-  ShenandoahHeap::heap()->complete_marking_context()->reset_top_bitmap(this);\n+  assert(ShenandoahHeap::heap()->active_generation()->is_mark_complete(), \"Marking should be complete here.\");\n+  ShenandoahHeap::heap()->marking_context()->reset_top_bitmap(this);\n@@ -270,0 +283,1 @@\n+  reset_age();\n@@ -326,0 +340,3 @@\n+  if (_affiliation == FREE) {\n+    \/\/assert(_live_data == 0, \"Setting non-zero live data (%zu) on FREE region\", s);\n+  }\n@@ -367,1 +384,1 @@\n-    case FREE:\n+    case ShenandoahRegionAffiliation::FREE:\n@@ -370,1 +387,1 @@\n-    case YOUNG_GENERATION:\n+    case ShenandoahRegionAffiliation::YOUNG_GENERATION:\n@@ -373,1 +390,1 @@\n-    case OLD_GENERATION:\n+    case ShenandoahRegionAffiliation::OLD_GENERATION:\n@@ -394,1 +411,1 @@\n-void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk) {\n+void ShenandoahHeapRegion::oop_iterate(OopIterateClosure* blk, bool fill_dead_objects, bool reregister_coalesced_objects) {\n@@ -397,0 +414,3 @@\n+    if (fill_dead_objects && !reregister_coalesced_objects) {\n+      ShenandoahHeap::heap()->card_scan()->register_object(bottom());\n+    }\n@@ -399,1 +419,1 @@\n-    oop_iterate_objects(blk);\n+    oop_iterate_objects(blk, fill_dead_objects, reregister_coalesced_objects);\n@@ -403,2 +423,2 @@\n-void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk) {\n-  assert(! is_humongous(), \"no humongous region here\");\n+void ShenandoahHeapRegion::oop_iterate_objects(OopIterateClosure* blk, bool fill_dead_objects, bool reregister_coalesced_objects) {\n+  assert(!is_humongous(), \"no humongous region here\");\n@@ -407,4 +427,49 @@\n-  \/\/ Could call objects iterate, but this is easier.\n-  while (obj_addr < t) {\n-    oop obj = oop(obj_addr);\n-    obj_addr += obj->oop_iterate_size(blk);\n+\n+  if (!fill_dead_objects) {\n+    while (obj_addr < t) {\n+      oop obj = oop(obj_addr);\n+      assert(obj->klass() != NULL, \"klass should not be NULL\");\n+      obj_addr += obj->oop_iterate_size(blk);\n+    }\n+  } else {\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahMarkingContext* marking_context = heap->marking_context();\n+    assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+    HeapWord* fill_addr = NULL;\n+    size_t fill_size = 0;\n+    while (obj_addr < t) {\n+      oop obj = oop(obj_addr);\n+      if (marking_context->is_marked(obj)) {\n+        if (fill_addr != NULL) {\n+           if (reregister_coalesced_objects) { \/\/ change existing crossing map information\n+            heap->card_scan()->coalesce_objects(fill_addr, fill_size);\n+          } else {              \/\/ establish new crossing map information\n+             heap->card_scan()->register_object(fill_addr);\n+          }\n+          ShenandoahHeap::fill_with_object(fill_addr, fill_size);\n+          fill_addr = NULL;\n+        }\n+        assert(obj->klass() != NULL, \"klass should not be NULL\");\n+        if (!reregister_coalesced_objects)\n+          heap->card_scan()->register_object(obj_addr);\n+        obj_addr += obj->oop_iterate_size(blk);\n+      } else {\n+        int size = obj->size();\n+        if (fill_addr == NULL) {\n+          fill_addr = obj_addr;\n+          fill_size = size;\n+        } else {\n+          fill_size += size;\n+        }\n+        obj_addr += size;\n+      }\n+    }\n+    if (fill_addr != NULL) {\n+      ShenandoahHeap::fill_with_object(fill_addr, fill_size);\n+      if (reregister_coalesced_objects) { \/\/ change existing crossing map information\n+        heap->card_scan()->coalesce_objects(fill_addr, fill_size);\n+      } else {              \/\/ establish new crossing map information\n+        heap->card_scan()->register_object(fill_addr);\n+      }\n+    }\n@@ -439,0 +504,8 @@\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  if (affiliation() == YOUNG_GENERATION) {\n+    heap->young_generation()->decrease_used(used());\n+  } else if (affiliation() == OLD_GENERATION) {\n+    heap->old_generation()->decrease_used(used());\n+  }\n+\n@@ -444,1 +517,1 @@\n-  ShenandoahHeap::heap()->marking_context()->reset_top_at_mark_start(this);\n+  heap->marking_context()->reset_top_at_mark_start(this);\n@@ -448,1 +521,6 @@\n-  _affiliation = ShenandoahRegionAffiliation::FREE;\n+#ifdef DEBUG_TRACE\n+  printf(\"SHR::recycle(), setting region (%llx, %llx, %llx) to FREE\\n\",\n+         (unsigned long long) bottom(), (unsigned long long) top(), (unsigned long long) end());\n+  fflush(stdout);\n+#endif\n+  set_affiliation(FREE);\n@@ -683,24 +761,4 @@\n-class UpdateOopCardValueClosure : public BasicOopIterateClosure {\n-  CardTable* _card_table;\n-\n-public:\n-  UpdateOopCardValueClosure(CardTable *card_table) : _card_table(card_table) { }\n-\n-  void do_oop(oop* p) {\n-    if (ShenandoahHeap::heap()->is_in_young(*p)) {\n-      volatile CardTable::CardValue* card_value = _card_table->byte_for(*p);\n-      *card_value = CardTable::dirty_card_val();\n-    }\n-  }\n-\n-  void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n-  }\n-};\n-\n-class UpdateObjectCardValuesClosure : public BasicOopIterateClosure {\n-  UpdateOopCardValueClosure* _oop_closure;\n-\n-public:\n-  UpdateObjectCardValuesClosure(UpdateOopCardValueClosure *oop_closure) :\n-    _oop_closure(oop_closure) {\n+void ShenandoahHeapRegion::set_affiliation(ShenandoahRegionAffiliation new_affiliation) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  if (_affiliation == new_affiliation) {\n+    return;\n@@ -709,2 +767,3 @@\n-  void do_oop(oop* p) {\n-    (*p)->oop_iterate(_oop_closure);\n+  if (!heap->mode()->is_generational()) {\n+    _affiliation = new_affiliation;\n+    return;\n@@ -713,2 +772,4 @@\n-  void do_oop(narrowOop* p) {\n-    ShouldNotReachHere();\n+  if (_affiliation == ShenandoahRegionAffiliation::YOUNG_GENERATION) {\n+    heap->young_generation()->decrement_affiliated_region_count();\n+  } else if (_affiliation == ShenandoahRegionAffiliation::OLD_GENERATION) {\n+    heap->old_generation()->decrement_affiliated_region_count();\n@@ -716,1 +777,0 @@\n-};\n@@ -718,4 +778,0 @@\n-void ShenandoahHeapRegion::set_affiliation(ShenandoahRegionAffiliation new_affiliation) {\n-  if (_affiliation == new_affiliation) {\n-    return;\n-  }\n@@ -725,0 +781,1 @@\n+      assert(!has_live(), \"Free region should not have live data\");\n@@ -728,0 +785,2 @@\n+      reset_age();\n+      heap->young_generation()->increment_affiliated_region_count();\n@@ -730,8 +789,1 @@\n-      if (_affiliation == YOUNG_GENERATION) {\n-        assert(SafepointSynchronize::is_at_safepoint(), \"old gen card values must be updated in a safepoint\");\n-        card_table->clear_MemRegion(MemRegion(_bottom, _end));\n-\n-        UpdateOopCardValueClosure oop_closure(card_table);\n-        UpdateObjectCardValuesClosure object_closure(&oop_closure);\n-        oop_iterate(&object_closure);\n-      }\n+      heap->old_generation()->increment_affiliated_region_count();\n@@ -745,0 +797,86 @@\n+\n+class UpdateCardValuesClosure : public BasicOopIterateClosure {\n+private:\n+  void update_card_value(void* address, oop obj) {\n+    if (ShenandoahHeap::heap()->is_in_young(obj)) {\n+      volatile CardTable::CardValue* card_value = ShenandoahBarrierSet::barrier_set()->card_table()->byte_for(address);\n+      *card_value = CardTable::dirty_card_val();\n+    }\n+  }\n+\n+public:\n+  void do_oop(oop* p) {\n+    oop obj = *p;\n+    if (obj != NULL) {\n+      update_card_value(p, obj);\n+    }\n+  }\n+\n+  void do_oop(narrowOop* p) {\n+    narrowOop o = RawAccess<>::oop_load(p);\n+    if (!CompressedOops::is_null(o)) {\n+      oop obj = CompressedOops::decode_not_null(o);\n+      assert(oopDesc::is_oop(obj), \"must be a valid oop\");\n+      update_card_value(p, obj);\n+    }\n+  }\n+};\n+\n+void ShenandoahHeapRegion::promote() {\n+  assert(SafepointSynchronize::is_at_safepoint(), \"must be at a safepoint\");\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahMarkingContext* marking_context = heap->marking_context();\n+  assert(heap->active_generation()->is_mark_complete(), \"sanity\");\n+\n+  UpdateCardValuesClosure update_card_values;\n+\n+  if (is_humongous_start()) {\n+    oop obj = oop(bottom());\n+    assert(marking_context->is_marked(obj), \"promoted humongous object should be alive\");\n+\n+    size_t index_limit = index() + ShenandoahHeapRegion::required_regions(obj->size() * HeapWordSize);\n+    heap->card_scan()->register_object(bottom());\n+    for (size_t i = index(); i < index_limit; i++) {\n+      ShenandoahHeapRegion* r = heap->get_region(i);\n+      log_debug(gc)(\"promoting region \" SIZE_FORMAT \", clear cards from \" SIZE_FORMAT \" to \" SIZE_FORMAT,\n+        r->index(), (size_t) r->bottom(), (size_t) r->top());\n+\n+      ShenandoahBarrierSet::barrier_set()->card_table()->clear_MemRegion(MemRegion(r->bottom(), r->end()));\n+#ifdef DEBUG_TRACE\n+      printf(\"promoting humongous region (%llx, %llx, %llx), setting affiliation to OLD_GENERATION\\n\",\n+             (unsigned long long) r->bottom(), (unsigned long long) r->top(), (unsigned long long) r->end());\n+      fflush(stdout);\n+#endif\n+      r->set_affiliation(OLD_GENERATION);\n+    }\n+    \/\/ HEY!  Better to call ShenandoahHeap::heap()->card_scan()->mark_range_as_clean(r->bottom(), obj->size())\n+    \/\/  and skip the calls to clear_MemRegion() above.\n+\n+    \/\/ Iterate over all humongous regions that are spanned by the humongous object obj.  The remnant\n+    \/\/ of memory in the last humongous region that is not spanned by obj is currently not used.\n+    obj->oop_iterate(&update_card_values);\n+  } else {\n+    log_debug(gc)(\"promoting region \" SIZE_FORMAT \", clear cards from \" SIZE_FORMAT \" to \" SIZE_FORMAT,\n+      index(), (size_t) bottom(), (size_t) top());\n+    assert(!is_humongous_continuation(), \"should not promote humongous object continuation in isolation\");\n+\n+    \/\/ Rather than scanning entire contents of the promoted region right now to determine which\n+    \/\/ cards to mark dirty, we just mark them all as dirty.  Later, when we scan the remembered\n+    \/\/ set, we will clear cards that are found to not contain live references to young memory.\n+    \/\/ Ultimately, this approach is more efficient as it only scans the \"dirty\" cards once and\n+    \/\/ the clean cards once.  The alternative approach of scanning all cards now and then scanning\n+    \/\/ dirty cards again at next concurrent mark pass scans the clean cards once and the dirty\n+    \/\/ cards twice.\n+\n+    \/\/ HEY!  Better to call ShenandoahHeap::heap()->card_scan()->mark_range_as_dirty(r->bottom(), obj->size());\n+    ShenandoahBarrierSet::barrier_set()->card_table()->dirty_MemRegion(MemRegion(bottom(), end()));\n+#ifdef DEBUG_TRACE\n+    printf(\"promoting normal region (%llx, %llx, %llx), setting affiliation to OLD_GENERATION\\n\",\n+           (unsigned long long) bottom(), (unsigned long long) top(), (unsigned long long) end());\n+    fflush(stdout);\n+#endif\n+    set_affiliation(OLD_GENERATION);\n+    oop_iterate_objects(&update_card_values, \/*fill_dead_objects*\/ true, \/* reregister_coalesced_objects *\/ false);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":198,"deletions":60,"binary":false,"changes":258,"status":"modified"},{"patch":"@@ -201,0 +201,2 @@\n+  bool is_young()                  const { return _affiliation == YOUNG_GENERATION; }\n+  bool is_old()                    const { return _affiliation == OLD_GENERATION; }\n@@ -250,0 +252,1 @@\n+  uint _age;\n@@ -360,1 +363,2 @@\n-  void oop_iterate(OopIterateClosure* cl);\n+  void oop_iterate(OopIterateClosure* cl, bool fill_dead_objects = false, bool reregister_coalesced_objects = false);\n+  void oop_iterate_humongous(OopIterateClosure* cl);\n@@ -398,0 +402,6 @@\n+  uint age()           { return _age; }\n+  void increment_age() { if (_age < markWord::max_age) { _age++; } }\n+  void reset_age()     { _age = 0; }\n+\n+  void promote();\n+\n@@ -402,2 +412,1 @@\n-  void oop_iterate_objects(OopIterateClosure* cl);\n-  void oop_iterate_humongous(OopIterateClosure* cl);\n+  void oop_iterate_objects(OopIterateClosure* cl, bool fill_dead_objects, bool reregister_coalesced_objects);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.hpp","additions":12,"deletions":3,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -33,0 +33,2 @@\n+#undef DEBUG_TRACE\n+\n@@ -109,0 +111,7 @@\n+\n+#ifdef DEBUG_TRACE\n+  printf(\"SHR::garbage(), used: %lld\\n\", (long long) used());\n+  fflush(stdout);\n+  printf(\"   get_live_data_bytes: %lld\\n\", (long long) get_live_data_bytes());\n+  fflush(stdout);\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -85,0 +85,1 @@\n+      \/\/ HEY! Update visualizer to render concurrent old marking\n@@ -88,0 +89,4 @@\n+      if (heap->is_gc_generation_young())              status |= 1 << 3;\n+      if (heap->is_degenerated_gc_in_progress())       status |= 1 << 4;\n+      if (heap->is_full_gc_in_progress())              status |= 1 << 5;\n+\n@@ -105,0 +110,3 @@\n+\n+          data |= (r->age() & AGE_MASK) << AGE_SHIFT;\n+          data |= (r->affiliation() & AFFILIATION_MASK) << AFFILIATION_SHIFT;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -56,1 +56,2 @@\n- * - bits 51-57  <reserved>\n+ * - bits 51-55  age\n+ * - bits 56-57  affiliation: 0 = free, young = 1, old = 2\n@@ -62,2 +63,4 @@\n-  static const jlong PERCENT_MASK = 0x7f;\n-  static const jlong STATUS_MASK  = 0x3f;\n+  static const jlong PERCENT_MASK      = 0x7f;\n+  static const jlong AGE_MASK          = 0x1f;\n+  static const jlong AFFILIATION_MASK  = 0x03;\n+  static const jlong STATUS_MASK       = 0x3f;\n@@ -65,7 +68,8 @@\n-  static const jlong USED_SHIFT   = 0;\n-  static const jlong LIVE_SHIFT   = 7;\n-  static const jlong TLAB_SHIFT   = 14;\n-  static const jlong GCLAB_SHIFT  = 21;\n-  static const jlong SHARED_SHIFT = 28;\n-\n-  static const jlong STATUS_SHIFT = 58;\n+  static const jlong USED_SHIFT        = 0;\n+  static const jlong LIVE_SHIFT        = 7;\n+  static const jlong TLAB_SHIFT        = 14;\n+  static const jlong GCLAB_SHIFT       = 21;\n+  static const jlong SHARED_SHIFT      = 28;\n+  static const jlong AGE_SHIFT         = 51;\n+  static const jlong AFFILIATION_SHIFT = 56;\n+  static const jlong STATUS_SHIFT      = 58;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegionCounters.hpp","additions":14,"deletions":10,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -44,1 +45,1 @@\n-                     heap->heuristics()->name());\n+                     heap->global_generation()->heuristics()->name());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahInitLogger.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -37,1 +38,1 @@\n-ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp) :\n+ShenandoahMarkRefsSuperClosure::ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q,  ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old_q) :\n@@ -40,0 +41,1 @@\n+  _old_queue(old_q),\n@@ -44,11 +46,4 @@\n-ShenandoahMark::ShenandoahMark() :\n-  _task_queues(ShenandoahHeap::heap()->marking_context()->task_queues()) {\n-}\n-\n-void ShenandoahMark::clear() {\n-  \/\/ Clean up marking stacks.\n-  ShenandoahObjToScanQueueSet* queues = ShenandoahHeap::heap()->marking_context()->task_queues();\n-  queues->clear();\n-\n-  \/\/ Cancel SATB buffers.\n-  ShenandoahBarrierSet::satb_mark_queue_set().abandon_partial_marking();\n+ShenandoahMark::ShenandoahMark(ShenandoahGeneration* generation) :\n+  _generation(generation),\n+  _task_queues(generation->task_queues()),\n+  _old_gen_task_queues(generation->old_gen_task_queues()) {\n@@ -59,1 +54,1 @@\n-                                       bool strdedup) {\n+                                       bool strdedup, bool update_refs) {\n@@ -61,0 +56,1 @@\n+  ShenandoahObjToScanQueue* old = get_old_queue(w);\n@@ -68,1 +64,1 @@\n-    if (heap->has_forwarded_objects()) {\n+    if (update_refs) {\n@@ -70,1 +66,1 @@\n-        ShenandoahMarkUpdateRefsMetadataDedupClosure<GENERATION> cl(q, rp);\n+        ShenandoahMarkUpdateRefsMetadataDedupClosure<GENERATION> cl(q, rp, old);\n@@ -73,1 +69,1 @@\n-        ShenandoahMarkUpdateRefsMetadataClosure<GENERATION> cl(q, rp);\n+        ShenandoahMarkUpdateRefsMetadataClosure<GENERATION> cl(q, rp, old);\n@@ -78,1 +74,1 @@\n-        ShenandoahMarkRefsMetadataDedupClosure<GENERATION> cl(q, rp);\n+        ShenandoahMarkRefsMetadataDedupClosure<GENERATION> cl(q, rp, old);\n@@ -81,1 +77,1 @@\n-        ShenandoahMarkRefsMetadataClosure<GENERATION> cl(q, rp);\n+        ShenandoahMarkRefsMetadataClosure<GENERATION> cl(q, rp, old);\n@@ -86,1 +82,1 @@\n-    if (heap->has_forwarded_objects()) {\n+    if (update_refs) {\n@@ -88,1 +84,1 @@\n-        ShenandoahMarkUpdateRefsDedupClosure<GENERATION> cl(q, rp);\n+        ShenandoahMarkUpdateRefsDedupClosure<GENERATION> cl(q, rp, old);\n@@ -91,1 +87,1 @@\n-        ShenandoahMarkUpdateRefsClosure<GENERATION> cl(q, rp);\n+        ShenandoahMarkUpdateRefsClosure<GENERATION> cl(q, rp, old);\n@@ -96,1 +92,1 @@\n-        ShenandoahMarkRefsDedupClosure<GENERATION> cl(q, rp);\n+        ShenandoahMarkRefsDedupClosure<GENERATION> cl(q, rp, old);\n@@ -99,1 +95,1 @@\n-        ShenandoahMarkRefsClosure<GENERATION> cl(q, rp);\n+        ShenandoahMarkRefsClosure<GENERATION> cl(q, rp, old);\n@@ -146,0 +142,1 @@\n+  ShenandoahObjToScanQueue* old = get_old_queue(worker_id);\n@@ -147,1 +144,1 @@\n-  ShenandoahSATBBufferClosure<GENERATION> drain_satb(q);\n+  ShenandoahSATBBufferClosure<GENERATION> drain_satb(q, old);\n@@ -185,0 +182,1 @@\n+  bool update_refs = ShenandoahHeap::heap()->has_forwarded_objects();\n@@ -188,1 +186,10 @@\n-        mark_loop_prework<YOUNG, true>(worker_id, terminator, rp, strdedup);\n+        mark_loop_prework<YOUNG, true>(worker_id, terminator, rp, strdedup, update_refs);\n+      } else {\n+        mark_loop_prework<YOUNG, false>(worker_id, terminator, rp, strdedup, update_refs);\n+      }\n+      break;\n+    }\n+    case OLD: {\n+      \/\/ Old generation collection only performs marking, it should to update references.\n+      if (cancellable) {\n+        mark_loop_prework<OLD, true>(worker_id, terminator, rp, strdedup, false);\n@@ -190,1 +197,1 @@\n-        mark_loop_prework<YOUNG, false>(worker_id, terminator, rp, strdedup);\n+        mark_loop_prework<OLD, false>(worker_id, terminator, rp, strdedup, false);\n@@ -196,1 +203,1 @@\n-        mark_loop_prework<GLOBAL, true>(worker_id, terminator, rp, strdedup);\n+        mark_loop_prework<GLOBAL, true>(worker_id, terminator, rp, strdedup, update_refs);\n@@ -198,1 +205,1 @@\n-        mark_loop_prework<GLOBAL, false>(worker_id, terminator, rp, strdedup);\n+        mark_loop_prework<GLOBAL, false>(worker_id, terminator, rp, strdedup, update_refs);\n@@ -207,0 +214,15 @@\n+\n+template<>\n+bool ShenandoahMark::in_generation<YOUNG>(oop obj) {\n+  return ShenandoahHeap::heap()->is_in_young(obj);\n+}\n+\n+template<>\n+bool ShenandoahMark::in_generation<OLD>(oop obj) {\n+  return ShenandoahHeap::heap()->is_in_old(obj);\n+}\n+\n+template<>\n+bool ShenandoahMark::in_generation<GLOBAL>(oop obj) {\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.cpp","additions":50,"deletions":28,"binary":false,"changes":78,"status":"modified"},{"patch":"@@ -32,2 +32,0 @@\n-class ShenandoahCMDrainMarkingStackClosure;\n-\n@@ -54,1 +52,0 @@\n-  friend class ShenandoahCMDrainMarkingStackClosure;\n@@ -57,0 +54,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -58,0 +56,1 @@\n+  ShenandoahObjToScanQueueSet* const _old_gen_task_queues;\n@@ -60,1 +59,1 @@\n-  ShenandoahMark();\n+  ShenandoahMark(ShenandoahGeneration* generation);\n@@ -64,3 +63,1 @@\n-  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak);\n-\n-  static void clear();\n+  static inline void mark_through_ref(T* p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old, ShenandoahMarkingContext* const mark_context, bool weak);\n@@ -70,0 +67,4 @@\n+  ShenandoahObjToScanQueueSet* old_task_queues() {\n+    return _old_gen_task_queues;\n+  }\n+\n@@ -71,0 +72,1 @@\n+  inline ShenandoahObjToScanQueue* get_old_queue(uint index) const;\n@@ -89,1 +91,10 @@\n-  void mark_loop_prework(uint worker_id, TaskTerminator *terminator, ShenandoahReferenceProcessor *rp, bool strdedup);\n+  void mark_loop_prework(uint worker_id, TaskTerminator *terminator,\n+                         ShenandoahReferenceProcessor *rp, bool strdedup, bool update_refs);\n+\n+  template <GenerationMode GENERATION>\n+  static bool in_generation(oop obj);\n+\n+  template<StringDedupMode STRING_DEDUP>\n+  static void mark_ref(ShenandoahObjToScanQueue* q,\n+                       ShenandoahMarkingContext* const mark_context, bool weak,\n+                       oop obj);\n@@ -94,0 +105,1 @@\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.hpp","additions":20,"deletions":8,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -51,1 +51,2 @@\n-  ShenandoahMark::mark_through_ref<T, GENERATION, NO_DEDUP>(p, _queue, _mark_context, false);\n+  \/\/ Only called from STW mark, should not be used to bootstrap old generation marking.\n+  ShenandoahMark::mark_through_ref<T, GENERATION, NO_DEDUP>(p, _queue, nullptr, _mark_context, false);\n@@ -58,0 +59,4 @@\n+  \/\/ HEY! This will push array chunks into the mark queue with no regard for\n+  \/\/ generations. I don't think it will break anything, but the young generation\n+  \/\/ scan might end up processing some old generation array chunks.\n+\n@@ -224,0 +229,1 @@\n+  ShenandoahObjToScanQueue* _old;\n@@ -227,1 +233,1 @@\n-  ShenandoahSATBBufferClosure(ShenandoahObjToScanQueue* q) :\n+  ShenandoahSATBBufferClosure(ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old) :\n@@ -229,0 +235,1 @@\n+    _old(old),\n@@ -235,1 +242,1 @@\n-    assert(size == 0 || !_heap->has_forwarded_objects(), \"Forwarded objects are not expected here\");\n+    assert(size == 0 || !_heap->has_forwarded_objects() || _heap->is_concurrent_old_mark_in_progress(), \"Forwarded objects are not expected here\");\n@@ -247,1 +254,1 @@\n-      ShenandoahMark::mark_through_ref<oop, GENERATION, STRING_DEDUP>(p, _queue, _mark_context, false);\n+      ShenandoahMark::mark_through_ref<oop, GENERATION, STRING_DEDUP>(p, _queue, _old, _mark_context, false);\n@@ -253,1 +260,1 @@\n-inline void ShenandoahMark::mark_through_ref(T *p, ShenandoahObjToScanQueue* q, ShenandoahMarkingContext* const mark_context, bool weak) {\n+inline void ShenandoahMark::mark_through_ref(T *p, ShenandoahObjToScanQueue* q, ShenandoahObjToScanQueue* old, ShenandoahMarkingContext* const mark_context, bool weak) {\n@@ -261,17 +268,6 @@\n-    if (GENERATION != YOUNG || ShenandoahHeap::heap()->is_in_young(obj)) {\n-      bool skip_live = false;\n-      bool marked;\n-      if (weak) {\n-        marked = mark_context->mark_weak(obj);\n-      } else {\n-        marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n-      }\n-      if (marked) {\n-        bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n-        assert(pushed, \"overflow queue should always succeed pushing\");\n-\n-        if ((STRING_DEDUP == ENQUEUE_DEDUP) && ShenandoahStringDedup::is_candidate(obj)) {\n-          assert(ShenandoahStringDedup::is_enabled(), \"Must be enabled\");\n-          ShenandoahStringDedup::enqueue_candidate(obj);\n-        }\n-      }\n+    if (in_generation<GENERATION>(obj)) {\n+      mark_ref<STRING_DEDUP>(q, mark_context, weak, obj);\n+      shenandoah_assert_marked(p, obj);\n+    } else if (old != nullptr) {\n+      mark_ref<STRING_DEDUP>(old, mark_context, weak, obj);\n+      shenandoah_assert_marked(p, obj);\n@@ -279,0 +275,2 @@\n+  }\n+}\n@@ -280,1 +278,19 @@\n-    shenandoah_assert_marked(p, obj);\n+template<StringDedupMode STRING_DEDUP>\n+void ShenandoahMark::mark_ref(ShenandoahObjToScanQueue* q,\n+                              ShenandoahMarkingContext* const mark_context,\n+                              bool weak, oop obj) {\n+  bool skip_live = false;\n+  bool marked;\n+  if (weak) {\n+    marked = mark_context->mark_weak(obj);\n+  } else {\n+    marked = mark_context->mark_strong(obj, \/* was_upgraded = *\/ skip_live);\n+  }\n+  if (marked) {\n+    bool pushed = q->push(ShenandoahMarkTask(obj, skip_live, weak));\n+    assert(pushed, \"overflow queue should always succeed pushing\");\n+\n+    if ((STRING_DEDUP == ENQUEUE_DEDUP) && ShenandoahStringDedup::is_candidate(obj)) {\n+      assert(ShenandoahStringDedup::is_enabled(), \"Must be enabled\");\n+      ShenandoahStringDedup::enqueue_candidate(obj);\n+    }\n@@ -291,0 +307,8 @@\n+\n+ShenandoahObjToScanQueue* ShenandoahMark::get_old_queue(uint index) const {\n+  if (_old_gen_task_queues != nullptr) {\n+    return _old_gen_task_queues->queue(index);\n+  }\n+  return nullptr;\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMark.inline.hpp","additions":47,"deletions":23,"binary":false,"changes":70,"status":"modified"},{"patch":"@@ -0,0 +1,54 @@\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahMarkClosures.hpp\"\n+\n+void ShenandoahFinalMarkUpdateRegionStateClosure::heap_region_do(ShenandoahHeapRegion* r) {\n+  if (r->is_active()) {\n+    \/\/ All allocations past TAMS are implicitly live, adjust the region data.\n+    \/\/ Bitmaps\/TAMS are swapped at this point, so we need to poll complete bitmap.\n+    HeapWord *tams = _ctx->top_at_mark_start(r);\n+    HeapWord *top = r->top();\n+    if (top > tams) {\n+      r->increase_live_data_alloc_words(pointer_delta(top, tams));\n+    }\n+\n+    \/\/ We are about to select the collection set, make sure it knows about\n+    \/\/ current pinning status. Also, this allows trashing more regions that\n+    \/\/ now have their pinning status dropped.\n+    if (r->is_pinned()) {\n+      if (r->pin_count() == 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_unpinned();\n+      }\n+    } else {\n+      if (r->pin_count() > 0) {\n+        ShenandoahHeapLocker locker(_lock);\n+        r->make_pinned();\n+      }\n+    }\n+\n+    if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+      \/\/ HEY! Allocations move the watermark when top moves, however compacting\n+      \/\/ objects will sometimes lower top beneath the watermark, after which,\n+      \/\/ attempts to read the watermark will assert out (watermark should not be\n+      \/\/ higher than top). I think the right way™ to check for new allocations\n+      \/\/ is to compare top with the TAMS as is done earlier in this function.\n+      \/\/ if (r->top() != r->get_update_watermark()) {\n+      if (top > tams) {\n+        \/\/ There have been allocations in this region since the start of the cycle.\n+        \/\/ Any objects new to this region must not assimilate elevated age.\n+        r->reset_age();\n+      } else {\n+        r->increment_age();\n+      }\n+    }\n+\n+    \/\/ Remember limit for updating refs. It's guaranteed that we get no\n+    \/\/ from-space-refs written from here on.\n+    r->set_update_watermark_at_safepoint(r->top());\n+  } else {\n+    assert(!r->has_live(), \"Region \" SIZE_FORMAT \" should have no live data\", r->index());\n+    assert(_ctx->top_at_mark_start(r) == r->top(),\n+           \"Region \" SIZE_FORMAT \" should have correct TAMS\", r->index());\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.cpp","additions":54,"deletions":0,"binary":false,"changes":54,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contac    _ctx(ShenandoahHeap::heap()->complete_marking_context()), _lock(ShenandoahHeap::heap()->lock()) {}\n+ Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHMARKCLOSURES_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHMARKCLOSURES_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahSharedVariables.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkingContext.hpp\"\n+#include \"gc\/shenandoah\/shenandoahMarkClosures.hpp\"\n+\n+class ShenandoahFinalMarkUpdateRegionStateClosure : public ShenandoahHeapRegionClosure {\n+private:\n+  ShenandoahMarkingContext* const _ctx;\n+  ShenandoahHeapLock* const _lock;\n+public:\n+  ShenandoahFinalMarkUpdateRegionStateClosure(ShenandoahMarkingContext* ctx) :\n+    _ctx(ctx), _lock(ShenandoahHeap::heap()->lock()) {}\n+\n+  void heap_region_do(ShenandoahHeapRegion* r);\n+\n+  bool is_thread_safe() { return true; }\n+};\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHMARKCLOSURES_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkClosures.hpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -28,1 +28,0 @@\n-#include \"gc\/shenandoah\/shenandoahHeapRegion.inline.hpp\"\n@@ -30,2 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahTaskqueue.inline.hpp\"\n-#include \"utilities\/stack.inline.hpp\"\n@@ -33,1 +30,1 @@\n-ShenandoahMarkingContext::ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions, uint max_queues) :\n+ShenandoahMarkingContext::ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions) :\n@@ -38,16 +35,1 @@\n-                      ((uintx) heap_region.start() >> ShenandoahHeapRegion::region_size_bytes_shift())),\n-  _task_queues(new ShenandoahObjToScanQueueSet(max_queues)) {\n-  assert(max_queues > 0, \"At least one queue\");\n-  for (uint i = 0; i < max_queues; ++i) {\n-    ShenandoahObjToScanQueue* task_queue = new ShenandoahObjToScanQueue();\n-    task_queue->initialize();\n-    _task_queues->register_queue(i, task_queue);\n-  }\n-}\n-\n-ShenandoahMarkingContext::~ShenandoahMarkingContext() {\n-  for (uint i = 0; i < _task_queues->size(); ++i) {\n-    ShenandoahObjToScanQueue* q = _task_queues->queue(i);\n-    delete q;\n-  }\n-  delete _task_queues;\n+                      ((uintx) heap_region.start() >> ShenandoahHeapRegion::region_size_bytes_shift())) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.cpp","additions":2,"deletions":20,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -50,3 +50,0 @@\n-  \/\/ Marking task queues\n-  ShenandoahObjToScanQueueSet* _task_queues;\n-\n@@ -54,2 +51,1 @@\n-  ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions, uint max_queues);\n-  ~ShenandoahMarkingContext();\n+  ShenandoahMarkingContext(MemRegion heap_region, MemRegion bitmap_region, size_t num_regions);\n@@ -69,0 +65,2 @@\n+  inline bool is_marked_or_old(oop obj) const;\n+  inline bool is_marked_strong_or_old(oop obj) const;\n@@ -89,3 +87,0 @@\n-\n-  \/\/ Task queues\n-  ShenandoahObjToScanQueueSet* task_queues() const { return _task_queues; }\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.hpp","additions":3,"deletions":8,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -51,0 +51,8 @@\n+inline bool ShenandoahMarkingContext::is_marked_or_old(oop obj) const {\n+  return is_marked(obj) || ShenandoahHeap::heap()->is_old(obj);\n+}\n+\n+inline bool ShenandoahMarkingContext::is_marked_strong_or_old(oop obj) const {\n+  return is_marked_strong(obj) || ShenandoahHeap::heap()->is_old(obj);\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahMarkingContext.inline.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -158,5 +158,2 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    ShenandoahKeepAliveClosure cl;\n-    data->oops_do(&cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress() ||\n-             heap->is_concurrent_strong_root_in_progress()) {\n+  if (heap->is_concurrent_weak_root_in_progress() ||\n+      heap->is_concurrent_strong_root_in_progress()) {\n@@ -165,0 +162,3 @@\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    ShenandoahKeepAliveClosure cl;\n+    data->oops_do(&cl);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahNMethod.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -0,0 +1,59 @@\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+\n+ShenandoahOldGC::ShenandoahOldGC(ShenandoahGeneration* generation) :\n+  ShenandoahConcurrentGC(generation) {}\n+\n+bool ShenandoahOldGC::collect(GCCause::Cause cause) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+\n+  \/\/ Continue concurrent mark, do not reset regions, do not mark roots, do not collect $200.\n+  entry_mark();\n+  if (check_cancellation_and_abort(ShenandoahDegenPoint::_degenerated_mark)) return false;\n+\n+  \/\/ Complete marking under STW\n+  vmop_entry_final_mark();\n+\n+  \/\/ We aren't dealing with old generation evacuation yet. Our heuristic\n+  \/\/ should not have built a cset in final mark.\n+  assert(!heap->is_evacuation_in_progress(), \"Old gen evacuations are not supported\");\n+\n+  \/\/ Concurrent stack processing\n+  if (heap->is_evacuation_in_progress()) {\n+    entry_thread_roots();\n+  }\n+\n+  \/\/ Process weak roots that might still point to regions that would be broken by cleanup\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n+    entry_weak_refs();\n+    entry_weak_roots();\n+  }\n+\n+  \/\/ Final mark might have reclaimed some immediate garbage, kick cleanup to reclaim\n+  \/\/ the space. This would be the last action if there is nothing to evacuate.\n+  entry_cleanup_early();\n+\n+  {\n+    ShenandoahHeapLocker locker(heap->lock());\n+    heap->free_set()->log_status();\n+  }\n+\n+  \/\/ Perform concurrent class unloading\n+  if (heap->unload_classes() &&\n+      heap->is_concurrent_weak_root_in_progress()) {\n+    entry_class_unloading();\n+  }\n+\n+  \/\/ Processing strong roots\n+  \/\/ This may be skipped if there is nothing to update\/evacuate.\n+  \/\/ If so, strong_root_in_progress would be unset.\n+  if (heap->is_concurrent_strong_root_in_progress()) {\n+    entry_strong_roots();\n+  }\n+\n+  entry_rendezvous_roots();\n+  return true;\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.cpp","additions":59,"deletions":0,"binary":false,"changes":59,"status":"added"},{"patch":"@@ -0,0 +1,18 @@\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n+\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentMark.hpp\"\n+#include \"gc\/shenandoah\/shenandoahConcurrentGC.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+\n+class ShenandoahGeneration;\n+\n+class ShenandoahOldGC : public ShenandoahConcurrentGC {\n+ public:\n+  ShenandoahOldGC(ShenandoahGeneration* generation);\n+  bool collect(GCCause::Cause cause);\n+};\n+\n+\n+#endif \/\/SHARE_GC_SHENANDOAH_SHENANDOAHOLDGC_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGC.hpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"added"},{"patch":"@@ -0,0 +1,30 @@\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeapRegion.hpp\"\n+#include \"gc\/shenandoah\/shenandoahOldGeneration.hpp\"\n+\n+ShenandoahOldGeneration::ShenandoahOldGeneration(uint max_queues, size_t max_capacity, size_t soft_max_capacity)\n+  : ShenandoahGeneration(OLD, max_queues, max_capacity, soft_max_capacity) {}\n+\n+const char* ShenandoahOldGeneration::name() const {\n+  return \"OLD\";\n+}\n+\n+bool ShenandoahOldGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return region->affiliation() != YOUNG_GENERATION;\n+}\n+\n+void ShenandoahOldGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  ShenandoahGenerationRegionClosure<OLD> old_regions(cl);\n+  ShenandoahHeap::heap()->parallel_heap_region_iterate(&old_regions);\n+}\n+\n+void ShenandoahOldGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap::heap()->set_concurrent_old_mark_in_progress(in_progress);\n+}\n+\n+bool ShenandoahOldGeneration::is_concurrent_mark_in_progress() {\n+  return ShenandoahHeap::heap()->is_concurrent_old_mark_in_progress();\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.cpp","additions":30,"deletions":0,"binary":false,"changes":30,"status":"added"},{"patch":"@@ -0,0 +1,22 @@\n+\n+#ifndef SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n+#define SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n+\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+\n+class ShenandoahOldGeneration : public ShenandoahGeneration {\n+ public:\n+  ShenandoahOldGeneration(uint max_queues, size_t max_capacity, size_t soft_max_capacity);\n+\n+  const char* name() const;\n+\n+  bool contains(ShenandoahHeapRegion* region) const;\n+  void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl);\n+  void set_concurrent_mark_in_progress(bool in_progress);\n+\n+ protected:\n+  bool is_concurrent_mark_in_progress();\n+};\n+\n+\n+#endif \/\/SHARE_VM_GC_SHENANDOAH_SHENANDOAHOLDGENERATION_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOldGeneration.hpp","additions":22,"deletions":0,"binary":false,"changes":22,"status":"added"},{"patch":"@@ -36,5 +36,0 @@\n-enum GenerationMode {\n-  YOUNG,\n-  GLOBAL\n-};\n-\n@@ -49,0 +44,1 @@\n+  ShenandoahObjToScanQueue* _old_queue;\n@@ -57,1 +53,1 @@\n-  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp);\n+  ShenandoahMarkRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp,  ShenandoahObjToScanQueue* old_queue = NULL);\n@@ -76,2 +72,2 @@\n-  ShenandoahMarkUpdateRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp),\n+  ShenandoahMarkUpdateRefsSuperClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old),\n@@ -90,2 +86,2 @@\n-  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkUpdateRefsSuperClosure(q, rp) {}\n+  ShenandoahMarkUpdateRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkUpdateRefsSuperClosure(q, rp, old) {}\n@@ -105,2 +101,2 @@\n-  ShenandoahMarkUpdateRefsDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkUpdateRefsSuperClosure(q, rp) {}\n+  ShenandoahMarkUpdateRefsDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkUpdateRefsSuperClosure(q, rp, old) {}\n@@ -120,2 +116,2 @@\n-  ShenandoahMarkUpdateRefsMetadataClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkUpdateRefsSuperClosure(q, rp) {}\n+  ShenandoahMarkUpdateRefsMetadataClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkUpdateRefsSuperClosure(q, rp, old) {}\n@@ -135,2 +131,2 @@\n-  ShenandoahMarkUpdateRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkUpdateRefsSuperClosure(q, rp) {}\n+  ShenandoahMarkUpdateRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkUpdateRefsSuperClosure(q, rp, old) {}\n@@ -150,2 +146,2 @@\n-  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp) {};\n+  ShenandoahMarkRefsClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old) {};\n@@ -165,2 +161,2 @@\n-  ShenandoahMarkRefsDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp) {};\n+  ShenandoahMarkRefsDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old) {};\n@@ -180,2 +176,2 @@\n-  ShenandoahMarkRefsMetadataClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp) {};\n+  ShenandoahMarkRefsMetadataClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old) {};\n@@ -195,2 +191,2 @@\n-  ShenandoahMarkRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp) :\n-    ShenandoahMarkRefsSuperClosure(q, rp) {};\n+  ShenandoahMarkRefsMetadataDedupClosure(ShenandoahObjToScanQueue* q, ShenandoahReferenceProcessor* rp, ShenandoahObjToScanQueue* old = NULL) :\n+    ShenandoahMarkRefsSuperClosure(q, rp, old) {};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.hpp","additions":20,"deletions":24,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -33,1 +33,1 @@\n-  ShenandoahMark::mark_through_ref<T, GENERATION, STRING_DEDUP>(p, _queue, _mark_context, _weak);\n+  ShenandoahMark::mark_through_ref<T, GENERATION, STRING_DEDUP>(p, _queue, _old_queue, _mark_context, _weak);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahOopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -102,2 +102,2 @@\n-  f(init_update_refs_gross,                         \"Pause Init  Update Refs (G)\")     \\\n-  f(init_update_refs,                               \"Pause Init  Update Refs (N)\")     \\\n+  f(init_update_refs_gross,                         \"Pause Init Update Refs (G)\")      \\\n+  f(init_update_refs,                               \"Pause Init Update Refs (N)\")      \\\n@@ -115,0 +115,1 @@\n+  f(final_update_refs_promote_tenured_regions,      \"  Promote Tenured Regions\")       \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahPhaseTimings.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -0,0 +1,158 @@\n+\/*\n+ * Copyright (c), Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/mode\/shenandoahMode.hpp\"\n+#include \"gc\/shenandoah\/shenandoahControlThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahRegulatorThread.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"logging\/log.hpp\"\n+\n+static ShenandoahHeuristics* get_heuristics(ShenandoahGeneration* nullable) {\n+  return nullable != NULL ? nullable->heuristics() : NULL;\n+}\n+\n+ShenandoahRegulatorThread::ShenandoahRegulatorThread(ShenandoahControlThread* control_thread) :\n+  ConcurrentGCThread(),\n+  _control_thread(control_thread),\n+  _last_young_cycle(0),\n+  _last_old_cycle(0),\n+  _last_cycle(0),\n+  _sleep(ShenandoahControlIntervalMin),\n+  _last_sleep_adjust_time(os::elapsedTime()) {\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  _old_heuristics = get_heuristics(heap->old_generation());\n+  _young_heuristics = get_heuristics(heap->young_generation());\n+  _global_heuristics = get_heuristics(heap->global_generation());\n+\n+  create_and_start();\n+}\n+\n+void ShenandoahRegulatorThread::run_service() {\n+  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n+    if (ShenandoahAllowOldMarkingPreemption) {\n+      regulate_concurrent_cycles();\n+    } else {\n+      regulate_interleaved_cycles();\n+    }\n+  } else {\n+    regulate_heap();\n+  }\n+\n+  log_info(gc)(\"%s: Done.\", name());\n+}\n+\n+void ShenandoahRegulatorThread::regulate_concurrent_cycles() {\n+  assert(_young_heuristics != NULL, \"Need young heuristics.\");\n+  assert(_old_heuristics != NULL, \"Need old heuristics.\");\n+\n+  while (!should_terminate()) {\n+    if (should_start_old_cycle()) {\n+      log_info(gc)(\"Heuristics requesting old collection.\");\n+      _last_old_cycle = _control_thread->get_gc_id();\n+      _control_thread->request_concurrent_gc(OLD);\n+    } else if (should_start_young_cycle()) {\n+      log_info(gc)(\"Heuristics requesting young collection.\");\n+      _last_young_cycle = _control_thread->get_gc_id();\n+      _control_thread->request_concurrent_gc(YOUNG);\n+    }\n+\n+    regulator_sleep();\n+  }\n+}\n+\n+void ShenandoahRegulatorThread::regulate_interleaved_cycles() {\n+  assert(_young_heuristics != NULL, \"Need young heuristics.\");\n+  assert(_global_heuristics != NULL, \"Need global heuristics.\");\n+\n+  while (!should_terminate()) {\n+    if (should_start_cycle(_young_heuristics, _last_cycle)) {\n+      log_info(gc)(\"Heuristics requesting young collection.\");\n+      _last_cycle = _control_thread->get_gc_id();\n+      _control_thread->request_concurrent_gc(YOUNG);\n+    } else if (should_start_cycle(_global_heuristics, _last_cycle)) {\n+      log_info(gc)(\"Heuristics requesting global collection.\");\n+      _last_cycle = _control_thread->get_gc_id();\n+      _control_thread->request_concurrent_gc(GLOBAL);\n+    }\n+\n+    regulator_sleep();\n+  }\n+}\n+\n+void ShenandoahRegulatorThread::regulate_heap() {\n+  assert(_global_heuristics != NULL, \"Need global heuristics.\");\n+\n+  while (!should_terminate()) {\n+    if (should_start_cycle(_global_heuristics, _last_cycle)) {\n+      _last_cycle = _control_thread->get_gc_id();\n+      _control_thread->request_concurrent_gc(GLOBAL);\n+    }\n+\n+    regulator_sleep();\n+  }\n+}\n+\n+void ShenandoahRegulatorThread::regulator_sleep() {\n+  \/\/ Wait before performing the next action. If allocation happened during this wait,\n+  \/\/ we exit sooner, to let heuristics re-evaluate new conditions. If we are at idle,\n+  \/\/ back off exponentially.\n+  double current = os::elapsedTime();\n+\n+  if (_heap_changed.try_unset()) {\n+    _sleep = ShenandoahControlIntervalMin;\n+  } else if ((current - _last_sleep_adjust_time) * 1000 > ShenandoahControlIntervalAdjustPeriod){\n+    _sleep = MIN2<int>(ShenandoahControlIntervalMax, MAX2(1, _sleep * 2));\n+    _last_sleep_adjust_time = current;\n+  }\n+\n+  os::naked_short_sleep(_sleep);\n+}\n+\n+bool ShenandoahRegulatorThread::should_start_young_cycle() {\n+  return ShenandoahHeap::heap()->mode()->is_generational()\n+      && should_start_cycle(_young_heuristics, _last_young_cycle);\n+}\n+\n+bool ShenandoahRegulatorThread::should_start_old_cycle() {\n+  return should_start_cycle(_old_heuristics, _last_old_cycle);\n+}\n+\n+bool ShenandoahRegulatorThread::should_start_cycle(ShenandoahHeuristics* heuristics, size_t last_cycle_started) {\n+  \/\/ We want to hold the last heuristic down so that it doesn't repeatedly try to start\n+  \/\/ a cycle. We might consider invoking 'should_start_gc' because in some cases it has\n+  \/\/ side effects (like sampling the allocation rate). On the other hand, the right thing™\n+  \/\/ to do is probably factor allocation rate sampling outside of heuristics evaluation.\n+  return (last_cycle_started == 0 || _control_thread->get_gc_id() > last_cycle_started)\n+      && heuristics->should_start_gc();\n+}\n+\n+void ShenandoahRegulatorThread::stop_service() {\n+  log_info(gc)(\"%s: Stop requested.\", name());\n+}\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.cpp","additions":158,"deletions":0,"binary":false,"changes":158,"status":"added"},{"patch":"@@ -0,0 +1,91 @@\n+\/*\n+ * Copyright (c) 2021, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n+\n+#include \"gc\/shared\/concurrentGCThread.hpp\"\n+#include \"gc\/shared\/gcCause.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"runtime\/mutex.hpp\"\n+\n+\/*\n+ * The purpose of this class (and thread) is to allow us to continue\n+ * to evaluate heuristics during a garbage collection. This is necessary\n+ * to allow young generation collections to interrupt and old generation\n+ * collection which is in-progress. This puts heuristic triggers on the\n+ * same footing as other gc requests (alloc failure, System.gc, etc.).\n+ * However, this regulator does not block after submitting a gc request.\n+ *\n+ * We could use a PeriodicTask for this, but this thread will sleep longer\n+ * when the allocation rate is lower and PeriodicTasks cannot adjust their\n+ * sleep time.\n+ *\/\n+class ShenandoahRegulatorThread: public ConcurrentGCThread {\n+  friend class VMStructs;\n+\n+ public:\n+  explicit ShenandoahRegulatorThread(ShenandoahControlThread* control_thread);\n+\n+  char* name() const { return (char*)\"ShenandoahRegulatorThread\";}\n+\n+  \/\/ This is called from allocation path, and thus should be fast.\n+  void notify_heap_changed() {\n+    \/\/ Notify that something had changed.\n+    if (_heap_changed.is_unset()) {\n+      _heap_changed.set();\n+    }\n+  }\n+\n+ protected:\n+  void run_service();\n+  void stop_service();\n+\n+ private:\n+  void regulate_interleaved_cycles();\n+  void regulate_concurrent_cycles();\n+  void regulate_heap();\n+\n+  bool should_start_young_cycle();\n+  bool should_start_old_cycle();\n+  bool should_start_cycle(ShenandoahHeuristics* heuristics, size_t last_cycle_started);\n+\n+  ShenandoahSharedFlag _heap_changed;\n+  ShenandoahControlThread* _control_thread;\n+  ShenandoahHeuristics* _young_heuristics;\n+  ShenandoahHeuristics* _old_heuristics;\n+  ShenandoahHeuristics* _global_heuristics;\n+\n+  size_t _last_young_cycle;\n+  size_t _last_old_cycle;\n+  size_t _last_cycle;\n+\n+  int _sleep;\n+  double _last_sleep_adjust_time;\n+\n+  void regulator_sleep();\n+};\n+\n+\n+#endif \/\/ SHARE_GC_SHENANDOAH_SHENANDOAHREGULATORTHREAD_HPP\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahRegulatorThread.hpp","additions":91,"deletions":0,"binary":false,"changes":91,"status":"added"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -59,2 +60,2 @@\n-ShenandoahSTWMark::ShenandoahSTWMark(bool full_gc) :\n-  ShenandoahMark(),\n+ShenandoahSTWMark::ShenandoahSTWMark(ShenandoahGeneration* generation, bool full_gc) :\n+  ShenandoahMark(generation),\n@@ -62,1 +63,1 @@\n-  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), ShenandoahHeap::heap()->marking_context()->task_queues()),\n+  _terminator(ShenandoahHeap::heap()->workers()->active_workers(), task_queues()),\n@@ -94,1 +95,1 @@\n-  heap->mark_complete_marking_context();\n+  heap->global_generation()->set_mark_complete();\n@@ -102,2 +103,15 @@\n-  ShenandoahInitMarkRootsClosure<GLOBAL> init_mark(task_queues()->queue(worker_id));\n-  _root_scanner.roots_do(&init_mark, worker_id);\n+  switch (_generation->generation_mode()) {\n+    case GLOBAL: {\n+      ShenandoahInitMarkRootsClosure<GLOBAL> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      break;\n+    }\n+    case YOUNG: {\n+      ShenandoahInitMarkRootsClosure<YOUNG> init_mark(task_queues()->queue(worker_id));\n+      _root_scanner.roots_do(&init_mark, worker_id);\n+      _generation->scan_remembered_set();\n+      break;\n+    }\n+    default:\n+      ShouldNotReachHere();\n+  }\n@@ -111,1 +125,2 @@\n-  mark_loop(GLOBAL, worker_id, &_terminator, rp,\n+  mark_loop(_generation->generation_mode(),\n+            worker_id, &_terminator, rp,\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.cpp","additions":22,"deletions":7,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+class ShenandoahGeneration;\n@@ -40,1 +41,1 @@\n- ShenandoahSTWMark(bool full_gc);\n+ ShenandoahSTWMark(ShenandoahGeneration* generation, bool full_gc);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahSTWMark.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,1 +29,0 @@\n-#include \"gc\/shenandoah\/shenandoahScanRemembered.hpp\"\n@@ -31,0 +30,3 @@\n+#include \"gc\/shenandoah\/shenandoahOopClosures.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahReferenceProcessor.hpp\"\n+#include \"gc\/shenandoah\/shenandoahScanRemembered.inline.hpp\"\n@@ -36,1 +38,1 @@\n-  _cluster_count = uint32_t(total_card_count \/ ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster);\n+  _cluster_count = total_card_count \/ ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n@@ -59,1 +61,1 @@\n-void ShenandoahDirectCardMarkRememberedSet::initialize_overreach(uint32_t first_cluster, uint32_t count) {\n+void ShenandoahDirectCardMarkRememberedSet::initialize_overreach(size_t first_cluster, size_t count) {\n@@ -64,1 +66,1 @@\n-  uint32_t first_card_index = first_cluster * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n@@ -71,1 +73,1 @@\n-void ShenandoahDirectCardMarkRememberedSet::merge_overreach(uint32_t first_cluster, uint32_t count) {\n+void ShenandoahDirectCardMarkRememberedSet::merge_overreach(size_t first_cluster, size_t count) {\n@@ -75,1 +77,1 @@\n-  uint32_t first_card_index = first_cluster * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+  size_t first_card_index = first_cluster * ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n@@ -84,0 +86,49 @@\n+\n+ShenandoahScanRememberedTask::ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                                                           ShenandoahObjToScanQueueSet* old_queue_set,\n+                                                           ShenandoahReferenceProcessor* rp,\n+                                                           ShenandoahRegionIterator* regions) :\n+  AbstractGangTask(\"Scan Remembered Set\"),\n+  _queue_set(queue_set), _old_queue_set(old_queue_set), _rp(rp), _regions(regions) {}\n+\n+void ShenandoahScanRememberedTask::work(uint worker_id) {\n+  \/\/ This sets up a thread local reference to the worker_id which is necessary\n+  \/\/ the weak reference processor.\n+  ShenandoahParallelWorkerSession worker_session(worker_id);\n+\n+  ShenandoahObjToScanQueue* q = _queue_set->queue(worker_id);\n+  ShenandoahObjToScanQueue* old = _old_queue_set == NULL ? NULL : _old_queue_set->queue(worker_id);\n+  ShenandoahMarkRefsClosure<YOUNG> cl(q, _rp, old);\n+  RememberedScanner *rs = ShenandoahHeap::heap()->card_scan();\n+\n+  \/\/ set up thread local closure for shen ref processor\n+  _rp->set_mark_closure(worker_id, &cl);\n+\n+  ShenandoahHeapRegion* region = _regions->next();\n+  while (region != NULL) {\n+    if (region->affiliation() == OLD_GENERATION) {\n+      HeapWord *start_of_range = region->bottom();\n+      size_t start_cluster_no = rs->cluster_for_addr(start_of_range);\n+\n+      \/\/ region->end() represents the end of memory spanned by this region, but not all of this\n+      \/\/   memory is eligible to be scanned because some of this memory has not yet been allocated.\n+      \/\/\n+      \/\/ region->top() represents the end of allocated memory within this region.  Any addresses\n+      \/\/   beyond region->top() should not be scanned as that memory does not hold valid objects.\n+      HeapWord *end_of_range = region->top();\n+\n+      \/\/ end_of_range may point to the middle of a cluster because region->top() may be different than region->end.\n+      \/\/ We want to assure that our process_clusters() request spans all relevant clusters.  Note that each cluster\n+      \/\/ processed will avoid processing beyond end_of_range.\n+\n+      size_t num_heapwords = end_of_range - start_of_range;\n+      unsigned int cluster_size = CardTable::card_size_in_words *\n+        ShenandoahCardCluster<ShenandoahDirectCardMarkRememberedSet>::CardsPerCluster;\n+      size_t num_clusters = (size_t) ((num_heapwords - 1 + cluster_size) \/ cluster_size);\n+\n+      \/\/ Remembered set scanner\n+      rs->process_clusters(start_cluster_no, num_clusters, end_of_range, &cl);\n+    }\n+    region = _regions->next();\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.cpp","additions":57,"deletions":6,"binary":false,"changes":63,"status":"modified"},{"patch":"@@ -26,8 +26,3 @@\n-\n-#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n-#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n-\n-#include <stdint.h>\n-#include \"memory\/iterator.hpp\"\n-#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n-\n+\/\/ During development of this new feature, we want the option to test\n+\/\/ with and without, and to compare performance before and after.\n+#define FAST_REMEMBERED_SET_SCANNING\n@@ -122,1 +117,1 @@\n-\/\/  Next, we repeat the process for invocations of process_Clusters.\n+\/\/  Next, we repeat the process for invocations of process_clusters.\n@@ -214,0 +209,9 @@\n+#ifndef SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+#define SHARE_GC_SHENANDOAH_SHENANDOAHSCANREMEMBERED_HPP\n+\n+#include <stdint.h>\n+#include \"memory\/iterator.hpp\"\n+#include \"gc\/shared\/workgroup.hpp\"\n+#include \"gc\/shenandoah\/shenandoahCardTable.hpp\"\n+#include \"gc\/shenandoah\/shenandoahTaskqueue.hpp\"\n+\n@@ -217,0 +221,2 @@\n+class ShenandoahRegionIterator;\n+\n@@ -233,1 +239,1 @@\n-  uint32_t _card_shift;\n+  size_t _card_shift;\n@@ -235,1 +241,1 @@\n-  uint32_t _cluster_count;\n+  size_t _cluster_count;\n@@ -249,6 +255,9 @@\n-  uint32_t card_index_for_addr(HeapWord *p);\n-  HeapWord *addr_for_card_index(uint32_t card_index);\n-  bool is_card_dirty(uint32_t card_index);\n-  void mark_card_as_dirty(uint32_t card_index);\n-  void mark_card_as_clean(uint32_t card_index);\n-  void mark_overreach_card_as_dirty(uint32_t card_index);\n+  size_t total_cards();\n+  size_t card_index_for_addr(HeapWord *p);\n+  HeapWord *addr_for_card_index(size_t card_index);\n+  bool is_card_dirty(size_t card_index);\n+  void mark_card_as_dirty(size_t card_index);\n+  void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  void mark_card_as_clean(size_t card_index);\n+  void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  void mark_overreach_card_as_dirty(size_t card_index);\n@@ -257,0 +266,1 @@\n+  void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n@@ -258,0 +268,1 @@\n+  void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n@@ -259,1 +270,1 @@\n-  uint32_t cluster_count();\n+  size_t cluster_count();\n@@ -263,1 +274,1 @@\n-  void initialize_overreach(uint32_t first_cluster, uint32_t count);\n+  void initialize_overreach(size_t first_cluster, size_t count);\n@@ -267,1 +278,1 @@\n-  void merge_overreach(uint32_t first_cluster, uint32_t count);\n+  void merge_overreach(size_t first_cluster, size_t count);\n@@ -461,1 +472,42 @@\n-  static const uint32_t CardsPerCluster = 64;\n+  static const size_t CardsPerCluster = 64;\n+\n+#ifdef FAST_REMEMBERED_SET_SCANNING\n+private:\n+  \/\/ This bit is set iff at least one object starts within a\n+  \/\/ particular card region.\n+  static const uint16_t ObjectStartsInCardRegion = 0x8000;\n+  static const uint16_t FirstStartBits = 0x003f;\n+  static const uint16_t LastStartBits = 0x0fc0;\n+  static const uint16_t FirstStartShift = 0;\n+  static const uint16_t LastStartShift = 6;\n+\n+  static const uint16_t CardOffsetMultiplier = 8;\n+\n+  uint16_t *object_starts;\n+\n+public:\n+  inline void set_first_start(size_t card_index, uint8_t value) {\n+    object_starts[card_index] &= ~FirstStartBits;\n+    object_starts[card_index] |= (FirstStartBits & (value << FirstStartShift));\n+  }\n+\n+  inline void set_last_start(size_t card_index, uint8_t value) {\n+    object_starts[card_index] &= ~LastStartBits;\n+    object_starts[card_index] |= (LastStartBits & (value << LastStartShift));\n+  }\n+\n+  inline void set_has_object_bit(size_t card_index) {\n+    object_starts[card_index] |= ObjectStartsInCardRegion;\n+  }\n+\n+  inline void clear_has_object_bit(size_t card_index) {\n+    object_starts[card_index] &= ~ObjectStartsInCardRegion;\n+  }\n+\n+  inline void clear_objects_in_range(HeapWord *addr, size_t num_words) {\n+    size_t card_index = _rs->card_index_for_addr(addr);\n+    size_t last_card_index = _rs->card_index_for_addr(addr + num_words - 1);\n+    while (card_index <= last_card_index)\n+      object_starts[card_index++] = 0;\n+  }\n+#endif  \/\/ FAST_REMEMBERED_SET_SCANNING\n@@ -465,0 +517,9 @@\n+#ifdef FAST_REMEMBERED_SET_SCANNING\n+    \/\/ HEY!  We don't really need object_starts entries for every card entry.  We only need these for the\n+    \/\/ the card entries that correspond to old-gen memory.  But for now, let's be quick and dirty.\n+    object_starts = (uint16_t *) malloc(rs->total_cards() * sizeof(uint16_t));\n+    if (object_starts == NULL)\n+      fatal(\"Insufficient memory for initializing heap\");\n+    for (size_t i = 0; i < rs->total_cards(); i++)\n+      object_starts[i] = 0;\n+#endif\n@@ -468,0 +529,2 @@\n+    if (object_starts != NULL)\n+      free(object_starts);\n@@ -482,0 +545,1 @@\n+  \/\/ Bits 0x7000: Reserved for future uses\n@@ -551,15 +615,2 @@\n-  const uint32_t CardByteOffsetMultiplier = 8;\n-  const uint32_t CardWordOffsetMultiplier = 1;\n-\n-#ifdef IMPLEMENT_THIS_OPTIMIZATION_LATER\n-\n-  \/\/ This bit is set iff at least one object starts within a\n-  \/\/ particular card region.\n-  const uint_16 ObjectStartsInCardRegion = 0x8000;\n-  const uint_16 FirstStartBits = 0x003f;\n-  const uint_16 LastStartBits = 0x0fc0;\n-  const uint_16 FirstStartShift = 0;\n-  const uint_16 LastStartShift = 6;\n-  const uint_16 CrossingObjectOverflow = 0x7fff;\n-\n-  uint_16 *object_starts;\n+  const size_t CardByteOffsetMultiplier = 8;\n+  const size_t CardWordOffsetMultiplier = 1;\n@@ -568,9 +619,3 @@\n-  inline void set_first_start(uint32_t card_index, uint8_t value) {\n-    object_starts[card_index] &= ~FirstStartBits;\n-    object_starts[card_index] |= (FirstStartBits & (value << FirstStartShift));\n-  }\n-\n-  inline void set_last_start(uint32_t card_index, uint8_t value) {\n-    object_starts[card_index] &= ~LastStartBits;\n-    object_starts[card_index] |= (LastStartBits & (value << LastStartShift));\n-  }\n+#ifdef CROSSING_OFFSETS_NO_LONGER_NEEDED\n+private:\n+  const uint16_t CrossingObjectOverflow = 0x7fff;\n@@ -578,3 +623,1 @@\n-  inline void set_has_object_bit(uint32_t card_index) {\n-    object_starts[card_index] |= ObjectStartsInCardRegion;\n-  }\n+public:\n@@ -583,1 +626,1 @@\n-  inline void set_crossing_object_start(uint32_t card_index, uint_16 crossing_offset) {\n+  inline void set_crossing_object_start(size_t card_index, uint16_t crossing_offset) {\n@@ -586,1 +629,1 @@\n-#endif  \/\/ IMPLEMENT_THIS_OPTIMIZATION_LATER\n+#endif  \/\/ CROSSING_OFFSETS_NO_LONGER_NEEDED\n@@ -588,1 +631,0 @@\n-public:\n@@ -681,93 +723,50 @@\n-  static void register_object(void *address, uint32_t length_in_bytes) {\n-\n-#ifdef IMPLEMENT_THIS_OPTIMIZATION_LATER\n-    uint32_t card_at_start = cardAtAddress(address);\n-    \/\/ The end of this object is contained within this card\n-    uint32_t card_at_end = cardAtAddress(address + length_in_bytes);\n-\n-    uint32_t cluster_at_start = clusterAtCard(card_at_start);\n-    uint32_t cluster_at_end = clusterAtCard(card_at_end);\n-\n-    void *card_start_address = addressAtCard(card_at_start);\n-    void *cluster_start_address = addressAtCluster(cluster_at_start);\n-\n-    uint8_t offset_in_card =\n-      (address - card_start_address) \/ CardOffsetMultiplier;\n-\n-    if (!getHashObjectBit(card_at_start)) {\n-      set_has_object_bit(card_at_start);\n-      set_first_start(card_at_start, offset_in_card);\n-      set_last_start(card_at_start, offset_in_card);\n-    } else {\n-      if (offset_in_card < get_first_start(card_at_start))\n-        set_first_start(card_at_start, offset_in_card);\n-      if (offset_in_card > get_last_start(card_at_start))\n-        set_last_start(card_at_last, offset_in_card);\n-    }\n-\n-#ifdef SUPPORT_FOR_GET_CROSSING_OBJECT_START_NO_LONGER_REQUIRED\n-\n-    \/\/ What is the last card within the current cluster?\n-    uint32_t next_cluster = cluster_at_start + 1;\n-    uint32_t card_at_next_cluster = cardAtCluster(next_cluster);\n-    uint32_t last_card_of_this_cluster = card_at_next_cluster - 1;\n-    uint32_t last_card_in_cluster = ((card_at_end < last_card_of_this_cluster) ? card_at_end: last_card_of_this_cluster);\n-\n-    uint_16 crossing_map_within_cluster = address - cluster_at_start;\n-\n-    for (uint32_t card_to_update = card_at_start + 1; card_to_update < last_card_in_cluster; card_to_update++)\n-      set_crossing_object_start(card_to_update, crossing_map_within_cluster);\n-\n-    \/\/ If the last card region of this cluster is completely spanned\n-    \/\/ by this new object,  set its crossing object start as well.\n-    if (cluster_at_end > cluster_at_start) {\n-      set_crossing_object_start(card_to_update++, crossing_map_within_cluster);\n-\n-      \/\/ Now, we have to update all spanned cards that reside within the\n-      \/\/ following clusters.\n-      while (card_to_update < card_at_end)\n-        set_crossing_object_start(card_to_update++, CrossingObjectOverflow);\n-\n-      \/\/ Cases:\n-      \/\/\n-      \/\/ 1. The region for card_at_end is not spanned at all because the\n-      \/\/    end of the new object aligns with the start of the last\n-      \/\/    card region.  In this case, we do nothing with the\n-      \/\/    card_at_end entry of the object_starts array.\n-      \/\/\n-      \/\/ 2. The region for card_at_end is spanned partially.  In this\n-      \/\/    case, we do nothing with the card_at_end entry of the\n-      \/\/    object_starts array.\n-      \/\/\n-      \/\/ 3. Do not need to consider the case that the region for\n-      \/\/    card_at_end is spanned entirely.  If the last region\n-      \/\/    spanned by a newly registered object is spanned in its\n-      \/\/    entirety, then card_at_end will identify the card region\n-      \/\/    that follows the object rather than the last region spanned\n-      \/\/    by the object.\n-\n-      \/\/ Bottom line: no further work to be done in any of these cases.\n-    }\n-    \/\/ Otherwise, the last card region of this cluster is not\n-    \/\/ completely spanned by this new object.  Leave the object_starts\n-    \/\/ array entry alone.  Two cases:\n-    \/\/\n-    \/\/  a) This newly registered object represents a consolidation of\n-    \/\/     multiple smaller objects, not including the object that\n-    \/\/     follows the consolidation.\n-    \/\/  b) This newly registered object is created by splitting a\n-    \/\/     previously existing object.  In this case, the other\n-    \/\/     objects resulting from the split will be registered\n-    \/\/     separately before it is necessary to lookup any information\n-    \/\/     within the object_starts array.\n-    \/\/\n-\n-#endif  \/\/ SUPPORT_FOR_GET_CROSSING_OBJECT_START_NO_LONGER_REQUIRED\n-\n-#else   \/\/ IMPLEMENT_THIS_OPTIMIZATION_LATER\n-\n-    \/\/ Do nothing for now as we have a brute-force implementation\n-    \/\/ of findSpanningObject().\n-\n-#endif  \/\/ IMPLEMENT_THIS_OPTIMIZATION_LATER\n-  }\n+\n+  \/\/ Synchronization thoughts from kelvin:\n+  \/\/\n+  \/\/ previously, I had contemplated a more complex implementation of\n+  \/\/ object registration, which had to touch every card spanned by the\n+  \/\/ registered object.  But the current implementation is much simpler,\n+  \/\/ and only has to touch the card that contains the start of the\n+  \/\/ object.\n+\n+  \/\/ if I were careful to assure that every GCLAB aligns with the start\n+  \/\/ of a card region and spanned a multiple of the card region size,\n+  \/\/ then the post-processing of each GCLAB with regards to\n+  \/\/ register_object() invocations can proceed without synchronization.\n+\n+  \/\/ But right now, we're not even using GCLABs  We are doing shared\n+  \/\/ allocations.  But, we must hold a lock while we are doing these, so\n+  \/\/ maybe I just piggy back on the lock that we already hold for\n+  \/\/ managing the free lists and register each object newly allocated by\n+  \/\/ the shared allocator.\n+  void register_object(HeapWord* address);\n+\n+  \/\/ During the reference updates phase of GC, we walk through each old-gen memory region that was\n+  \/\/ not part of the collection set and we invalidate all unmarked objects.  As part of this effort,\n+  \/\/ we coalesce neighboring dead objects in order to make future remembered set scanning more\n+  \/\/ efficient (since future remembered set scanning of any card region containing consecutive\n+  \/\/ dead objects can skip over all of them at once by reading only a single dead object header\n+  \/\/ instead of having to read the header of each of the coalesced dead objects.\n+  \/\/\n+  \/\/ At some future time, we may implement a further optimization: satisfy future allocation requests\n+  \/\/ by carving new objects out of the range of memory that represents the coalesced dead objects.\n+  \/\/\n+  \/\/ In its current implementation, unregister_object() serves the needs of coalescing objects.\n+  \/\/\n+\n+  \/\/ Suppose we want to combine several dead objects into a single coalesced object.  How does this\n+  \/\/ impact our representation of crossing map information?\n+  \/\/  1. If the newly coalesced region is contained entirely within a single region, that region's last\n+  \/\/     start entry either remains the same or it is changed to the start of the coalesced region.\n+  \/\/  2. For the region that holds the start of the coalesced object, it will not impact the first start\n+  \/\/     but it may impact the last start.\n+  \/\/  3. For following regions spanned entirely by the newly coalesced object, it will change has_object\n+  \/\/     to false (and make first-start and last-start \"undefined\").\n+  \/\/  4. For a following region that is spanned patially by the newly coalesced object, it may change\n+  \/\/     first-start value, but it will not change the last-start value.\n+  \/\/\n+  \/\/ The range of addresses represented by the arguments to coalesce_objects() must represent a range\n+  \/\/ of memory that was previously occupied exactly by one or more previously registered objects.  For\n+  \/\/ convenience, it is legal to invoke coalesce_objects() with arguments that span a single previously\n+  \/\/ registered object.\n+  void coalesce_objects(HeapWord* address, size_t length_in_words);\n@@ -776,1 +775,1 @@\n-  \/\/   for each heapregion that comprises olg-gen memory\n+  \/\/   for each heapregion that comprises old-gen memory\n@@ -787,1 +786,1 @@\n-  bool has_object(uint32_t card_index);\n+  bool has_object(size_t card_index);\n@@ -790,2 +789,3 @@\n-  \/\/ memory at which the first object begins.\n-  uint32_t get_first_start(uint32_t card_index);\n+  \/\/ memory at which the first object begins.  If !has_object(card_index), the\n+  \/\/ result is a don't care value.\n+  size_t get_first_start(size_t card_index);\n@@ -794,2 +794,3 @@\n-  \/\/ memory at which the last object begins.\n-  uint32_t get_last_start(uint32_t card_index);\n+  \/\/ memory at which the last object begins.  If !has_object(card_index), the\n+  \/\/ result is a don't care value.\n+  size_t get_last_start(size_t card_index);\n@@ -797,20 +798,0 @@\n-  \/\/ If !has_object(card_index), this returns the offset within the\n-  \/\/ enclosing cluster at which the object spanning the start of this\n-  \/\/ card memory begins, or returns 0x7fff if the spanning object\n-  \/\/ starts before the enclosing cluster.\n-  uint32_t get_crossing_object_start(uint32_t card_index);\n-\n-  \/\/ Card index is zero-based relative to first spanned card region.\n-  uint32_t card_index_for_addr(HeapWord *p);\n-  HeapWord *addr_for_card_index(uint32_t card_index);\n-  bool is_card_dirty(uint32_t card_index);\n-  void mark_card_as_dirty(uint32_t card_index);\n-  void mark_card_as_clean(uint32_t card_index);\n-  void mark_overreach_card_as_dirty(uint32_t card_index);\n-  bool is_card_dirty(HeapWord *p);\n-  void mark_card_as_dirty(HeapWord *p);\n-  void mark_card_as_clean(HeapWord *p);\n-  void mark_overreach_card_as_dirty(void *p);\n-  uint32_t cluster_count();\n-  void initialize_overreach(uint32_t first_cluster, uint32_t count);\n-  void merge_overreach(uint32_t first_cluster, uint32_t count);\n@@ -848,1 +829,2 @@\n-  ShenandoahCardCluster<RememberedSet> *_scc;\n+  RememberedSet* _rs;\n+  ShenandoahCardCluster<RememberedSet>* _scc;\n@@ -866,2 +848,49 @@\n-  ShenandoahScanRemembered(RememberedSet *rs);\n-  ~ShenandoahScanRemembered();\n+  ShenandoahScanRemembered(RememberedSet *rs) {\n+    _rs = rs;\n+    _scc = new ShenandoahCardCluster<RememberedSet>(rs);\n+  }\n+\n+  ~ShenandoahScanRemembered() {\n+    delete _scc;\n+  }\n+\n+  \/\/ HEY!  We really don't want to share all of these APIs with arbitrary consumers of the ShenandoahScanRemembered abstraction.\n+  \/\/ But in the spirit of quick and dirty for the time being, I'm going to go ahead and publish everything for right now.  Some\n+  \/\/ of existing code already depends on having access to these services (because existing code has not been written to honor\n+  \/\/ full abstraction of remembered set scanning.  In the not too distant future, we want to try to make most, if not all, of\n+  \/\/ these services private.  Two problems with publicizing:\n+  \/\/  1. Allowing arbitrary users to reach beneath the hood allows the users to make assumptions about underlying implementation.\n+  \/\/     This will make it more difficult to change underlying implementation at a future time, such as when we eventually experiment\n+  \/\/     with SATB-based implementation of remembered set representation.\n+  \/\/  2. If we carefully control sharing of certain of these services, we can reduce the overhead of synchronization by assuring\n+  \/\/     that all users follow protocols that avoid contention that might require synchronization.  When we publish these APIs, we\n+  \/\/     lose control over who and how the data is accessed.  As a result, we are required to insert more defensive measures into\n+  \/\/     the implementation, including synchronization locks.\n+\n+\n+  \/\/ Card index is zero-based relative to first spanned card region.\n+  size_t total_cards();\n+  size_t card_index_for_addr(HeapWord *p);\n+  HeapWord *addr_for_card_index(size_t card_index);\n+  bool is_card_dirty(size_t card_index);\n+  void mark_card_as_dirty(size_t card_index);\n+  void mark_range_as_dirty(size_t card_index, size_t num_cards);\n+  void mark_card_as_clean(size_t card_index);\n+  void mark_range_as_clean(size_t card_index, size_t num_cards);\n+  void mark_overreach_card_as_dirty(size_t card_index);\n+  bool is_card_dirty(HeapWord *p);\n+  void mark_card_as_dirty(HeapWord *p);\n+  void mark_range_as_dirty(HeapWord *p, size_t num_heap_words);\n+  void mark_card_as_clean(HeapWord *p);\n+  void mark_range_as_clean(HeapWord *p, size_t num_heap_words);\n+  void mark_overreach_card_as_dirty(void *p);\n+  size_t cluster_count();\n+  void initialize_overreach(size_t first_cluster, size_t count);\n+  void merge_overreach(size_t first_cluster, size_t count);\n+\n+  size_t cluster_for_addr(HeapWord *addr);\n+  void register_object(HeapWord *addr);\n+  void coalesce_objects(HeapWord *addr, size_t length_in_words);\n+\n+  \/\/ clear the cards to clean, and clear the object_starts info to no objects\n+  void mark_range_as_empty(HeapWord *addr, size_t length_in_words);\n@@ -901,2 +930,1 @@\n-  void process_clusters(uint worker_id, ShenandoahReferenceProcessor* rp, ShenandoahConcurrentMark* cm, uint32_t first_cluster, uint32_t count,\n-                        HeapWord *end_of_range, ClosureType *oops);\n+  inline void process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range, ClosureType *oops);\n@@ -904,1 +932,0 @@\n-  uint32_t cluster_for_addr(HeapWord *addr);\n@@ -932,0 +959,14 @@\n+class ShenandoahScanRememberedTask : public AbstractGangTask {\n+ private:\n+  ShenandoahObjToScanQueueSet* _queue_set;\n+  ShenandoahObjToScanQueueSet* _old_queue_set;\n+  ShenandoahReferenceProcessor* _rp;\n+  ShenandoahRegionIterator* _regions;\n+ public:\n+  ShenandoahScanRememberedTask(ShenandoahObjToScanQueueSet* queue_set,\n+                               ShenandoahObjToScanQueueSet* old_queue_set,\n+                               ShenandoahReferenceProcessor* rp,\n+                               ShenandoahRegionIterator* regions);\n+\n+  void work(uint worker_id);\n+};\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.hpp","additions":217,"deletions":176,"binary":false,"changes":393,"status":"modified"},{"patch":"@@ -37,1 +37,6 @@\n-inline uint32_t\n+inline size_t\n+ShenandoahDirectCardMarkRememberedSet::total_cards() {\n+  return _total_card_count;\n+}\n+\n+inline size_t\n@@ -39,1 +44,1 @@\n-  return (uint32_t) _card_table->index_for(p);\n+  return _card_table->index_for(p);\n@@ -43,1 +48,1 @@\n-ShenandoahDirectCardMarkRememberedSet::addr_for_card_index(uint32_t card_index) {\n+ShenandoahDirectCardMarkRememberedSet::addr_for_card_index(size_t card_index) {\n@@ -48,1 +53,1 @@\n-ShenandoahDirectCardMarkRememberedSet::is_card_dirty(uint32_t card_index) {\n+ShenandoahDirectCardMarkRememberedSet::is_card_dirty(size_t card_index) {\n@@ -54,1 +59,1 @@\n-ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(uint32_t card_index) {\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_dirty(size_t card_index) {\n@@ -60,1 +65,8 @@\n-ShenandoahDirectCardMarkRememberedSet::mark_card_as_clean(uint32_t card_index) {\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(size_t card_index, size_t num_cards) {\n+  uint8_t *bp = &_byte_map[card_index];\n+  while (num_cards-- > 0)\n+    *bp++ = CardTable::dirty_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_card_as_clean(size_t card_index) {\n@@ -66,2 +78,8 @@\n-ShenandoahDirectCardMarkRememberedSet::mark_overreach_card_as_dirty(\n-    uint32_t card_index) {\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(size_t card_index, size_t num_cards) {\n+  uint8_t *bp = &_byte_map[card_index];\n+  while (num_cards-- > 0)\n+    *bp++ = CardTable::clean_card_val();\n+}\n+\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_overreach_card_as_dirty(size_t card_index) {\n@@ -84,0 +102,8 @@\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_dirty(HeapWord *p, size_t num_heap_words) {\n+  uint8_t *bp = &_byte_map_base[uintptr_t(p) >> _card_shift];\n+  uint8_t *end_bp = &_byte_map_base[uintptr_t(p + num_heap_words) >> _card_shift];\n+  while (bp < end_bp)\n+    *bp++ = CardTable::dirty_card_val();\n+}\n+\n@@ -90,0 +116,8 @@\n+inline void\n+ShenandoahDirectCardMarkRememberedSet::mark_range_as_clean(HeapWord *p, size_t num_heap_words) {\n+  uint8_t *bp = &_byte_map_base[uintptr_t(p) >> _card_shift];\n+  uint8_t *end_bp = &_byte_map_base[uintptr_t(p + num_heap_words) >> _card_shift];\n+  while (bp < end_bp)\n+    *bp++ = CardTable::clean_card_val();\n+}\n+\n@@ -96,1 +130,1 @@\n-inline uint32_t\n+inline size_t\n@@ -101,1 +135,0 @@\n-\n@@ -103,3 +136,16 @@\n-inline uint32_t\n-ShenandoahCardCluster<RememberedSet>::card_index_for_addr(HeapWord *p) {\n-  return _rs->card_index_for_addr(p);\n+inline void\n+ShenandoahCardCluster<RememberedSet>::register_object(HeapWord* address) {\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord *card_start_address = _rs->addr_for_card_index(card_at_start);\n+  uint8_t offset_in_card = address - card_start_address;\n+\n+  if ((object_starts[card_at_start] & ObjectStartsInCardRegion) == 0) {\n+    set_has_object_bit(card_at_start);\n+    set_first_start(card_at_start, offset_in_card);\n+    set_last_start(card_at_start, offset_in_card);\n+  } else {\n+    if (offset_in_card < get_first_start(card_at_start))\n+      set_first_start(card_at_start, offset_in_card);\n+    if (offset_in_card > get_last_start(card_at_start))\n+      set_last_start(card_at_start, offset_in_card);\n+  }\n@@ -109,2 +155,47 @@\n-inline bool\n-ShenandoahCardCluster<RememberedSet>::has_object(uint32_t card_index) {\n+inline void\n+ShenandoahCardCluster<RememberedSet>::coalesce_objects(HeapWord* address, size_t length_in_words) {\n+#ifdef FAST_REMEMBERED_SET_SCANNING\n+  size_t card_at_start = _rs->card_index_for_addr(address);\n+  HeapWord *card_start_address = _rs->addr_for_card_index(card_at_start);\n+  size_t card_at_end = card_at_start + ((address + length_in_words) - card_start_address) \/ CardTable::card_size_in_words;\n+\n+  if (card_at_start == card_at_end) {\n+    \/\/ No changes to object_starts array.  Either:\n+    \/\/  get_first_start(card_at_start) returns this coalesced object,\n+    \/\/    or it returns an object that precedes the coalesced object.\n+    \/\/  get_last_start(card_at_start) returns the object that immediately follows the coalesced object,\n+    \/\/    or it returns an object that comes after the object immediately following the coalesced object.\n+  } else {\n+    uint8_t coalesced_offset = static_cast<uint8_t>(address - card_start_address);\n+    if (get_last_start(card_at_start) > coalesced_offset) {\n+      \/\/ Existing last start is being coalesced, create new last start\n+      set_last_start(card_at_start, coalesced_offset);\n+    }\n+    \/\/ otherwise, get_last_start(card_at_start) must equal coalesced_offset\n+\n+    \/\/ All the cards between first and last get cleared.\n+    for (size_t i = card_at_start + 1; i < card_at_end; i++) {\n+      clear_has_object_bit(i);\n+    }\n+\n+    uint8_t follow_offset = static_cast<uint8_t>((address + length_in_words) - _rs->addr_for_card_index(card_at_end));\n+    if (has_object(card_at_end) && (get_first_start(card_at_end) < follow_offset)) {\n+      \/\/ It may be that after coalescing within this last card's memory range, the last card\n+      \/\/ no longer holds an object.\n+      if (get_last_start(card_at_end) >= follow_offset) {\n+        set_first_start(card_at_end, follow_offset);\n+      } else {\n+        \/\/ last_start is being coalesced so this card no longer has any objects.\n+        clear_has_object_bit(card_at_end);\n+      }\n+    }\n+    \/\/ else\n+    \/\/  card_at_end did not have an object, so it still does not have an object, or\n+    \/\/  card_at_end had an object that starts after the coalesced object, so no changes required for card_at_end\n+\n+  }\n+#else  \/\/ FAST_REMEMBERED_SET_SCANNING\n+  \/\/ Do nothing for now as we have a brute-force implementation\n+  \/\/ of findSpanningObject().\n+#endif \/\/ FAST_REMEMBERED_SET_SCANNING\n+}\n@@ -112,1 +203,0 @@\n-  HeapWord *addr = _rs->addr_for_card_index(card_index);\n@@ -114,0 +204,9 @@\n+template<typename RememberedSet>\n+inline bool\n+ShenandoahCardCluster<RememberedSet>::has_object(size_t card_index) {\n+#ifdef FAST_REMEMBERED_SET_SCANNING\n+  if (object_starts[card_index] & ObjectStartsInCardRegion)\n+    return true;\n+  else\n+    return false;\n+#else \/\/ FAST_REMEMBERED_SET_SCANNING'\n@@ -115,0 +214,1 @@\n+  HeapWord *addr = _rs->addr_for_card_index(card_index);\n@@ -117,1 +217,1 @@\n-  \/\/ Apparently, region->block_start(addr) is not robust to inquiries beyond top() and it crashes.\n+  \/\/ region->block_start(addr) is not robust to inquiries beyond top() and it crashes.\n@@ -121,0 +221,5 @@\n+  \/\/ region->block_start(addr) is also not robust to inquiries within a humongous continuation region.\n+  \/\/ if region is humongous continuation, no object starts within it.\n+  if (region->is_humongous_continuation())\n+    return false;\n+\n@@ -144,0 +249,1 @@\n+#endif \/\/ FAST_REMEMBERED_SET_SCANNING'\n@@ -147,2 +253,6 @@\n-inline uint32_t\n-ShenandoahCardCluster<RememberedSet>::get_first_start(uint32_t card_index) {\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_first_start(size_t card_index) {\n+#ifdef FAST_REMEMBERED_SET_SCANNING\n+  assert(object_starts[card_index] & ObjectStartsInCardRegion, \"Can't get first start because no object starts here\");\n+  return (object_starts[card_index] & FirstStartBits) >> FirstStartShift;\n+#else  \/\/ FAST_REMEMBERED_SET_SCANNING\n@@ -168,0 +278,1 @@\n+#endif  \/\/ FAST_REMEMBERED_SET_SCANNING\n@@ -171,2 +282,6 @@\n-inline uint32_t\n-ShenandoahCardCluster<RememberedSet>::get_last_start(uint32_t card_index) {\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_last_start(size_t card_index) {\n+#ifdef FAST_REMEMBERED_SET_SCANNING\n+  assert(object_starts[card_index] & ObjectStartsInCardRegion, \"Can't get last start because no objects starts here\");\n+  return (object_starts[card_index] & LastStartBits) >> LastStartShift;\n+#else  \/\/ FAST_REMEMBERED_SET_SCANNING\n@@ -191,0 +306,1 @@\n+#endif  \/\/ FAST_REMEMBERED_SET_SCANNING\n@@ -193,0 +309,1 @@\n+#ifdef CROSSING_OFFSETS_NO_LONGER_NEEDED\n@@ -194,2 +311,2 @@\n-inline uint32_t\n-ShenandoahCardCluster<RememberedSet>::get_crossing_object_start(uint32_t card_index) {\n+inline size_t\n+ShenandoahCardCluster<RememberedSet>::get_crossing_object_start(size_t card_index) {\n@@ -197,1 +314,1 @@\n-  uint32_t cluster_no = card_index \/ ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  size_t cluster_no = card_index \/ ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n@@ -209,0 +326,9 @@\n+#endif\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::total_cards() { return _rs->total_cards(); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::card_index_for_addr(HeapWord *p) { return _rs->card_index_for_addr(p); };\n@@ -212,3 +338,1 @@\n-ShenandoahCardCluster<RememberedSet>::addr_for_card_index(uint32_t card_index) {\n-  return _rs->addr_for_card_index(card_index);\n-}\n+ShenandoahScanRemembered<RememberedSet>::addr_for_card_index(size_t card_index) { return _rs->addr_for_card_index(card_index); }\n@@ -218,3 +342,1 @@\n-ShenandoahCardCluster<RememberedSet>::is_card_dirty(uint32_t card_index) {\n-  return _rs->is_card_dirty(card_index);\n-}\n+ShenandoahScanRemembered<RememberedSet>::is_card_dirty(size_t card_index) { return _rs->is_card_dirty(card_index); }\n@@ -224,3 +346,1 @@\n-ShenandoahCardCluster<RememberedSet>::mark_card_as_dirty(uint32_t card_index) {\n-  return _rs->mark_card_as_dirty(card_index);\n-}\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_dirty(size_t card_index) { _rs->mark_card_as_dirty(card_index); }\n@@ -230,3 +350,1 @@\n-ShenandoahCardCluster<RememberedSet>::mark_card_as_clean(uint32_t card_index) {\n-  return _rs->mark_card_as_clean(card_index);\n-}\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_dirty(size_t card_index, size_t num_cards) { _rs->mark_range_as_dirty(card_index, num_cards); }\n@@ -236,3 +354,9 @@\n-ShenandoahCardCluster<RememberedSet>::mark_overreach_card_as_dirty(uint32_t card_index) {\n-  return _rs->mark_overreach_card_as_dirty(card_index);\n-}\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_clean(size_t card_index) { _rs->mark_card_as_clean(card_index); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_clean(size_t card_index, size_t num_cards) { _rs->mark_range_as_clean(card_index, num_cards); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>:: mark_overreach_card_as_dirty(size_t card_index) { _rs->mark_overreach_card_as_dirty(card_index); }\n@@ -242,3 +366,1 @@\n-ShenandoahCardCluster<RememberedSet>::is_card_dirty(HeapWord *p) {\n-  return _rs->is_card_dirty(p);\n-}\n+ShenandoahScanRemembered<RememberedSet>::is_card_dirty(HeapWord *p) { return _rs->is_card_dirty(p); }\n@@ -248,3 +370,1 @@\n-ShenandoahCardCluster<RememberedSet>::mark_card_as_dirty(HeapWord *p) {\n-  return _rs->mark_card_as_dirty(p);\n-}\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_dirty(HeapWord *p) { _rs->mark_card_as_dirty(p); }\n@@ -254,3 +374,1 @@\n-ShenandoahCardCluster<RememberedSet>::mark_card_as_clean(HeapWord *p) {\n-  return _rs->mark_card_as_clean(p);\n-}\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_dirty(HeapWord *p, size_t num_heap_words) { _rs->mark_range_as_dirty(p, num_heap_words); }\n@@ -260,3 +378,1 @@\n-ShenandoahCardCluster<RememberedSet>::mark_overreach_card_as_dirty(void *p) {\n-  return _rs->mark_overreach_card_as_dirty(p);\n-}\n+ShenandoahScanRemembered<RememberedSet>::mark_card_as_clean(HeapWord *p) { _rs->mark_card_as_clean(p); }\n@@ -265,4 +381,2 @@\n-inline uint32_t\n-ShenandoahCardCluster<RememberedSet>::cluster_count() {\n-  return _rs->cluster_count();\n-}\n+inline void\n+ShenandoahScanRemembered<RememberedSet>:: mark_range_as_clean(HeapWord *p, size_t num_heap_words) { _rs->mark_range_as_clean(p, num_heap_words); }\n@@ -272,3 +386,14 @@\n-ShenandoahCardCluster<RememberedSet>::initialize_overreach(uint32_t first_cluster, uint32_t count) {\n-  return _rs->initialize_overreach(first_cluster, count);\n-}\n+ShenandoahScanRemembered<RememberedSet>::mark_overreach_card_as_dirty(void *p) { _rs->mark_overreach_card_as_dirty(p); }\n+\n+template<typename RememberedSet>\n+inline size_t\n+ShenandoahScanRemembered<RememberedSet>::cluster_count() { return _rs->cluster_count(); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::initialize_overreach(size_t first_cluster, size_t count) { _rs->initialize_overreach(first_cluster, count); }\n+\n+template<typename RememberedSet>\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::merge_overreach(size_t first_cluster, size_t count) { _rs->merge_overreach(first_cluster, count); }\n+\n@@ -278,2 +403,2 @@\n-ShenandoahCardCluster<RememberedSet>::merge_overreach(uint32_t first_cluster, uint32_t count) {\n-  return _rs->merge_overreach(first_cluster, count);\n+ShenandoahScanRemembered<RememberedSet>::register_object(HeapWord *addr) {\n+  _scc->register_object(addr);\n@@ -283,2 +408,3 @@\n-ShenandoahScanRemembered<RememberedSet>::ShenandoahScanRemembered(RememberedSet *rs) {\n-  _scc = new ShenandoahCardCluster<RememberedSet>(rs);\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::coalesce_objects(HeapWord *addr, size_t length_in_words) {\n+  _scc->coalesce_objects(addr, length_in_words);\n@@ -288,2 +414,4 @@\n-ShenandoahScanRemembered<RememberedSet>::~ShenandoahScanRemembered() {\n-  delete _scc;\n+inline void\n+ShenandoahScanRemembered<RememberedSet>::mark_range_as_empty(HeapWord *addr, size_t length_in_words) {\n+  _rs->mark_range_as_clean(addr, length_in_words);\n+  _scc->clear_objects_in_range(addr, length_in_words);\n@@ -295,3 +423,2 @@\n-ShenandoahScanRemembered<RememberedSet>::process_clusters(uint worker_id, ShenandoahReferenceProcessor* rp, ShenandoahConcurrentMark* cm,\n-                                                          uint32_t first_cluster, uint32_t count, HeapWord *end_of_range,\n-                                                          ClosureType *oops) {\n+ShenandoahScanRemembered<RememberedSet>::process_clusters(size_t first_cluster, size_t count, HeapWord *end_of_range,\n+                                                          ClosureType *cl) {\n@@ -304,2 +431,2 @@\n-    uint32_t card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n-    uint32_t end_card_index = card_index + ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+    size_t card_index = first_cluster * ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+    size_t end_card_index = card_index + ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n@@ -308,1 +435,1 @@\n-    int next_card_index = 0;\n+    size_t next_card_index = 0;\n@@ -310,2 +437,3 @@\n-      int is_dirty = _scc->is_card_dirty(card_index);\n-      int has_object = _scc->has_object(card_index);\n+\n+      bool is_dirty = _rs->is_card_dirty(card_index);\n+      bool has_object = _scc->has_object(card_index);\n@@ -316,2 +444,3 @@\n-          uint32_t start_offset = _scc->get_first_start(card_index);\n-          HeapWord *p = _scc->addr_for_card_index(card_index);\n+          size_t start_offset = _scc->get_first_start(card_index);\n+          HeapWord *p = _rs->addr_for_card_index(card_index);\n+          HeapWord *card_start = p;\n@@ -325,1 +454,4 @@\n-            next_card_index = _scc->card_index_for_addr(endp);\n+\n+            \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n+            \/\/ failure if endp points to end of heap.\n+            next_card_index = card_index + (endp - card_start) \/ CardTable::card_size_in_words;\n@@ -331,0 +463,1 @@\n+\n@@ -336,2 +469,0 @@\n-              ShenandoahObjToScanQueue* q = cm->get_queue(worker_id);\n-              ShenandoahMarkRefsClosure<YOUNG> cl(q, rp);\n@@ -340,1 +471,3 @@\n-              array->oop_iterate_range(&cl, 0, len);\n+              array->oop_iterate_range(cl, 0, len);\n+            } else if (obj->is_instance()) {\n+              obj->oop_iterate(cl);\n@@ -342,1 +475,5 @@\n-              oops->do_oop(&obj);\n+              \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n+              \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n+              \/\/ We skip iterating over the klass pointer since we know that\n+              \/\/ Universe::TypeArrayKlass never moves.\n+              assert (obj->is_typeArray(), \"should be type array\");\n@@ -346,1 +483,4 @@\n-          card_index = next_card_index;\n+          if (p > endp)\n+            card_index = card_index + (p - card_start) \/ CardTable::card_size_in_words;\n+          else                  \/\/ p == endp\n+            card_index = next_card_index;\n@@ -351,1 +491,2 @@\n-      } else if (_scc->has_object(card_index)) {\n+      } else if (has_object) {\n+\n@@ -354,2 +495,3 @@\n-        uint32_t start_offset = _scc->get_last_start(card_index);\n-        HeapWord *p = _scc->addr_for_card_index(card_index) + start_offset;\n+        size_t start_offset = _scc->get_last_start(card_index);\n+        HeapWord *card_start = _rs->addr_for_card_index(card_index);\n+        HeapWord *p = card_start + start_offset;\n@@ -358,1 +500,4 @@\n-        uint32_t last_card = _scc->card_index_for_addr(nextp);\n+\n+        \/\/ Can't use _scc->card_index_for_addr(endp) here because it crashes with assertion\n+        \/\/ failure if nextp points to end of heap.\n+        size_t last_card = card_index + (nextp - card_start) \/ CardTable::card_size_in_words;\n@@ -364,3 +509,3 @@\n-          uint32_t span_card;\n-          for (span_card = card_index+1; span_card < end_card_index; span_card++)\n-            if (_scc->is_card_dirty(span_card)) {\n+          size_t span_card;\n+          for (span_card = card_index+1; span_card <= last_card; span_card++)\n+            if (_rs->is_card_dirty(span_card)) {\n@@ -371,0 +516,1 @@\n+\n@@ -373,2 +519,0 @@\n-            ShenandoahObjToScanQueue* q = cm->get_queue(worker_id); \/\/ kelvin to confirm: get_queue wants worker_id\n-            ShenandoahMarkRefsClosure<YOUNG> cl(q, rp);\n@@ -377,1 +521,3 @@\n-            array->oop_iterate_range(&cl, 0, len);\n+            array->oop_iterate_range(cl, 0, len);\n+          } else if (obj->is_instance()) {\n+            obj->oop_iterate(cl);\n@@ -379,1 +525,5 @@\n-            oops->do_oop(&obj);\n+            \/\/ Case 3: Primitive array. Do nothing, no oops there. We use the same\n+            \/\/ performance tweak TypeArrayKlass::oop_oop_iterate_impl is using:\n+            \/\/ We skip iterating over the klass pointer since we know that\n+            \/\/ Universe::TypeArrayKlass never moves.\n+            assert (obj->is_typeArray(), \"should be type array\");\n@@ -392,1 +542,1 @@\n-inline uint32_t\n+inline size_t\n@@ -394,2 +544,2 @@\n-  uint32_t card_index = _scc->card_index_for_addr(addr);\n-  uint32_t result = card_index \/ ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n+  size_t card_index = _rs->card_index_for_addr(addr);\n+  size_t result = card_index \/ ShenandoahCardCluster<RememberedSet>::CardsPerCluster;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahScanRemembered.inline.hpp","additions":244,"deletions":94,"binary":false,"changes":338,"status":"modified"},{"patch":"@@ -78,3 +78,1 @@\n-    if (_heap->is_concurrent_mark_in_progress()) {\n-      return &_keep_alive_cl;\n-    } else if (_heap->is_concurrent_weak_root_in_progress()) {\n+    if (_heap->is_concurrent_weak_root_in_progress()) {\n@@ -83,0 +81,2 @@\n+    } else if (_heap->is_concurrent_mark_in_progress()) {\n+      return &_keep_alive_cl;\n@@ -95,8 +95,1 @@\n-  if (heap->is_concurrent_mark_in_progress()) {\n-    \/\/ We need to reset all TLABs because they might be below the TAMS, and we need to mark\n-    \/\/ the objects in them. Do not let mutators allocate any new objects in their current TLABs.\n-    \/\/ It is also a good place to resize the TLAB sizes for future allocations.\n-    retire_tlab();\n-\n-    _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n-  } else if (heap->is_concurrent_weak_root_in_progress()) {\n+  if (heap->is_concurrent_weak_root_in_progress()) {\n@@ -111,0 +104,7 @@\n+    _jt->oops_do_no_frames(closure_from_context(context), &_cb_cl);\n+  } else if (heap->is_concurrent_mark_in_progress()) {\n+    \/\/ We need to reset all TLABs because they might be below the TAMS, and we need to mark\n+    \/\/ the objects in them. Do not let mutators allocate any new objects in their current TLABs.\n+    \/\/ It is also a good place to resize the TLAB sizes for future allocations.\n+    retire_tlab();\n+\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahStackWatermark.cpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-    _marking_context(ShenandoahHeap::heap()->complete_marking_context()),\n+    _marking_context(ShenandoahHeap::heap()->marking_context()),\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUnload.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -34,0 +34,1 @@\n+#include \"gc\/shenandoah\/shenandoahGeneration.hpp\"\n@@ -41,1 +42,1 @@\n-ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause) :\n+ShenandoahGCSession::ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation) :\n@@ -43,0 +44,1 @@\n+  _generation(generation),\n@@ -48,0 +50,1 @@\n+  _heap->set_gc_generation(generation);\n@@ -53,1 +56,1 @@\n-  _heap->heuristics()->record_cycle_start();\n+  generation->heuristics()->record_cycle_start();\n@@ -67,1 +70,1 @@\n-  _heap->heuristics()->record_cycle_end();\n+  _generation->heuristics()->record_cycle_end();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.cpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+class ShenandoahGeneration;\n@@ -47,0 +48,1 @@\n+  ShenandoahGeneration* const _generation;\n@@ -52,1 +54,1 @@\n-  ShenandoahGCSession(GCCause::Cause cause);\n+  ShenandoahGCSession(GCCause::Cause cause, ShenandoahGeneration* generation);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahUtils.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -166,1 +166,2 @@\n-          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live(),\n+          check(ShenandoahAsserts::_safe_oop, obj, obj_reg->has_live() ||\n+                (obj_reg->is_old() && ShenandoahHeap::heap()->is_gc_generation_young()),\n@@ -221,1 +222,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->marking_context()->is_marked_or_old(obj),\n@@ -225,1 +226,1 @@\n-        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked(obj),\n+        check(ShenandoahAsserts::_safe_all, obj, _heap->complete_marking_context()->is_marked_or_old(obj),\n@@ -611,1 +612,1 @@\n-    if (actual != _expected) {\n+    if (actual != _expected && !(actual & ShenandoahHeap::OLD_MARKING)) {\n@@ -658,1 +659,2 @@\n-      if (actual != expected) {\n+      \/\/ Old generation marking is allowed in all states.\n+      if (actual != expected && !(actual & ShenandoahHeap::OLD_MARKING)) {\n@@ -745,0 +747,4 @@\n+      if (r->is_old() && _heap->is_gc_generation_young()) {\n+        \/\/ Old regions don't have computed live data during young collections.\n+        continue;\n+      }\n@@ -957,1 +963,1 @@\n-      if (!heap->marking_context()->is_marked(obj)) {\n+      if (!heap->marking_context()->is_marked_or_old(obj)) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":12,"deletions":6,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -0,0 +1,139 @@\n+\/*\n+ * Copyright (c) 2020, Red Hat, Inc. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+#include \"precompiled.hpp\"\n+\n+#include \"gc\/shenandoah\/shenandoahFreeSet.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.hpp\"\n+#include \"gc\/shenandoah\/shenandoahHeap.inline.hpp\"\n+#include \"gc\/shenandoah\/shenandoahUtils.hpp\"\n+#include \"gc\/shenandoah\/shenandoahVerifier.hpp\"\n+#include \"gc\/shenandoah\/shenandoahYoungGeneration.hpp\"\n+#include \"gc\/shenandoah\/heuristics\/shenandoahHeuristics.hpp\"\n+\n+#undef TRACE_PROMOTION\n+\n+ShenandoahYoungGeneration::ShenandoahYoungGeneration(uint max_queues, size_t max_capacity, size_t soft_max_capacity) :\n+  ShenandoahGeneration(YOUNG, max_queues, max_capacity, soft_max_capacity),\n+  _old_gen_task_queues(nullptr) {\n+}\n+\n+const char* ShenandoahYoungGeneration::name() const {\n+  return \"YOUNG\";\n+}\n+\n+void ShenandoahYoungGeneration::set_concurrent_mark_in_progress(bool in_progress) {\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  heap->set_concurrent_young_mark_in_progress(in_progress);\n+  if (_old_gen_task_queues != nullptr && in_progress) {\n+    \/\/ This is not a bug. When the young generation marking is complete,\n+    \/\/ the old generation marking is still in progress.\n+    heap->set_concurrent_old_mark_in_progress(in_progress);\n+  }\n+}\n+\n+class ShenandoahPromoteTenuredRegionsTask : public AbstractGangTask {\n+private:\n+  ShenandoahRegionIterator* _regions;\n+public:\n+  volatile size_t _used;\n+  volatile size_t _promoted;\n+\n+  ShenandoahPromoteTenuredRegionsTask(ShenandoahRegionIterator* regions) :\n+    AbstractGangTask(\"Shenandoah Promote Tenured Regions\"),\n+    _regions(regions),\n+    _used(0),\n+    _promoted(0) {\n+  }\n+\n+  void work(uint worker_id) {\n+    ShenandoahParallelWorkerSession worker_session(worker_id);\n+    ShenandoahHeapRegion* r = _regions->next();\n+    while (r != NULL) {\n+      if (r->is_young()) {\n+        if (r->age() >= InitialTenuringThreshold && !r->is_humongous_continuation()) {\n+          r->promote();\n+          Atomic::inc(&_promoted);\n+        } else {\n+          Atomic::add(&_used, r->used());\n+        }\n+      }\n+      r = _regions->next();\n+    }\n+  }\n+};\n+\n+void ShenandoahYoungGeneration::promote_tenured_regions() {\n+  ShenandoahRegionIterator regions;\n+  ShenandoahPromoteTenuredRegionsTask task(&regions);\n+  ShenandoahHeap::heap()->workers()->run_task(&task);\n+  _used = task._used;\n+  log_info(gc)(\"Promoted \" SIZE_FORMAT \" regions.\", task._promoted);\n+}\n+\n+void ShenandoahYoungGeneration::promote_all_regions() {\n+  \/\/ This only happens on a full stw collect. No allocations can happen here.\n+  shenandoah_assert_safepoint();\n+\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  for (size_t index = 0; index < heap->num_regions(); index++) {\n+    ShenandoahHeapRegion* r = heap->get_region(index);\n+    if (r->is_young()) {\n+      r->promote();\n+    }\n+  }\n+  assert(_affiliated_region_count == 0, \"young generation must not have affiliated regions after reset\");\n+  _used = 0;\n+\n+  \/\/ HEY! Better to use a service of ShenandoahScanRemembered for the following.\n+\n+  \/\/ We can clear the entire card table here because we've just promoted all\n+  \/\/ young regions to old, so there can be no old->young pointers at this point.\n+  ShenandoahBarrierSet::barrier_set()->card_table()->clear();\n+}\n+\n+bool ShenandoahYoungGeneration::contains(ShenandoahHeapRegion* region) const {\n+  return region->affiliation() != OLD_GENERATION;\n+}\n+\n+void ShenandoahYoungGeneration::parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl) {\n+  if (_old_gen_task_queues != NULL) {\n+    \/\/ No generation filter on regions, we need to iterate all the regions.\n+    ShenandoahHeap::heap()->parallel_heap_region_iterate(cl);\n+  } else {\n+    \/\/ Just the young generations here.\n+    ShenandoahGenerationRegionClosure<YOUNG> young_regions(cl);\n+    ShenandoahHeap::heap()->parallel_heap_region_iterate(&young_regions);\n+  }\n+}\n+\n+bool ShenandoahYoungGeneration::is_concurrent_mark_in_progress() {\n+  return ShenandoahHeap::heap()->is_concurrent_young_mark_in_progress();\n+}\n+\n+void ShenandoahYoungGeneration::reserve_task_queues(uint workers) {\n+  ShenandoahGeneration::reserve_task_queues(workers);\n+  if (_old_gen_task_queues != NULL) {\n+    _old_gen_task_queues->reserve(workers);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.cpp","additions":139,"deletions":0,"binary":false,"changes":139,"status":"added"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, Amazon.com, Inc. and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, Amazon.com, Inc. or its affiliates. All rights reserved.\n@@ -31,0 +31,3 @@\n+private:\n+  ShenandoahObjToScanQueueSet* _old_gen_task_queues;\n+\n@@ -32,1 +35,24 @@\n-  ShenandoahYoungGeneration() : ShenandoahGeneration(YOUNG) { }\n+  ShenandoahYoungGeneration(uint max_queues, size_t max_capacity, size_t max_soft_capacity);\n+\n+  virtual const char* name() const;\n+\n+  virtual void set_concurrent_mark_in_progress(bool in_progress);\n+  virtual void parallel_heap_region_iterate(ShenandoahHeapRegionClosure* cl);\n+\n+  bool contains(ShenandoahHeapRegion* region) const;\n+\n+  void promote_tenured_regions();\n+  void promote_all_regions();\n+\n+  void set_old_gen_task_queues(ShenandoahObjToScanQueueSet* old_gen_queues) {\n+    _old_gen_task_queues = old_gen_queues;\n+  }\n+\n+  ShenandoahObjToScanQueueSet* old_gen_task_queues() const {\n+    return _old_gen_task_queues;\n+  }\n+\n+  virtual void reserve_task_queues(uint workers);\n+\n+ protected:\n+  bool is_concurrent_mark_in_progress();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahYoungGeneration.hpp","additions":28,"deletions":2,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -364,2 +364,17 @@\n-          \"might degrade performance.\")\n-\n+          \"might degrade performance.\")                                     \\\n+                                                                            \\\n+  product(bool, ShenandoahUseSimpleCardScanning, false, DIAGNOSTIC,         \\\n+          \"Testing: use simplified, very inefficient but much less complex\" \\\n+          \" card table scanning.\")                                          \\\n+                                                                            \\\n+  product(bool, ShenandoahPromoteTenuredObjects, true, DIAGNOSTIC,          \\\n+          \"Turn on\/off evacuating individual tenured young objects \"        \\\n+          \" to the old generation.\")                                        \\\n+                                                                            \\\n+  product(bool, ShenandoahPromoteTenuredRegions, true, DIAGNOSTIC,          \\\n+          \"Turn on\/off transitioning tenured young regions \"                \\\n+          \" to the old generation.\")                                        \\\n+                                                                            \\\n+  product(bool, ShenandoahAllowOldMarkingPreemption, true, DIAGNOSTIC,      \\\n+          \"Allow young generation collections to suspend concurrent\"        \\\n+          \" marking in the old generation.\")\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoah_globals.hpp","additions":17,"deletions":2,"binary":false,"changes":19,"status":"modified"}]}