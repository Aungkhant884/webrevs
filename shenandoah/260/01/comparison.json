{"files":[{"patch":"@@ -62,0 +62,1 @@\n+  \/\/ When ShenandoahElasticTLAB is enabled, the request cannot be made smaller than _min_size.\n@@ -63,0 +64,2 @@\n+\n+  \/\/ The size of the request in words.\n@@ -64,0 +67,2 @@\n+\n+  \/\/ The allocation may be increased for padding or decreased to fit in the remaining space of a region.\n@@ -65,0 +70,8 @@\n+\n+  \/\/ For a humongous object, the _waste is the amount of free memory in the last region.\n+  \/\/ For other requests, the _waste will be non-zero if the request enountered one or more regions\n+  \/\/ with less memory than _min_size. This waste does not contribute to the used memory for\n+  \/\/ the heap, but it does contribute to the allocation rate for heuristics.\n+  size_t _waste;\n+\n+  \/\/ This is the type of the request.\n@@ -66,0 +79,2 @@\n+\n+  \/\/ This is the generation which the request is targeting.\n@@ -67,0 +82,1 @@\n+\n@@ -68,0 +84,1 @@\n+  \/\/ Check that this is set before being read.\n@@ -73,1 +90,1 @@\n-          _actual_size(0), _alloc_type(_alloc_type), _affiliation(affiliation)\n+          _actual_size(0), _waste(0), _alloc_type(_alloc_type), _affiliation(affiliation)\n@@ -100,1 +117,1 @@\n-  inline size_t size() {\n+  inline size_t size() const {\n@@ -104,1 +121,1 @@\n-  inline Type type() {\n+  inline Type type() const {\n@@ -108,1 +125,1 @@\n-  inline const char* type_string() {\n+  inline const char* type_string() const {\n@@ -112,1 +129,1 @@\n-  inline size_t min_size() {\n+  inline size_t min_size() const {\n@@ -117,1 +134,1 @@\n-  inline size_t actual_size() {\n+  inline size_t actual_size() const {\n@@ -130,1 +147,9 @@\n-  inline bool is_mutator_alloc() {\n+  inline size_t waste() const {\n+    return _waste;\n+  }\n+\n+  inline void set_waste(size_t v) {\n+    _waste = v;\n+  }\n+\n+  inline bool is_mutator_alloc() const {\n@@ -145,1 +170,1 @@\n-  inline bool is_gc_alloc() {\n+  inline bool is_gc_alloc() const {\n@@ -160,1 +185,1 @@\n-  inline bool is_lab_alloc() {\n+  inline bool is_lab_alloc() const {\n@@ -175,1 +200,1 @@\n-  bool is_old() {\n+  bool is_old() const {\n@@ -179,1 +204,1 @@\n-  bool is_young() {\n+  bool is_young() const {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahAllocRequest.hpp","additions":36,"deletions":11,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -249,0 +249,52 @@\n+\/\/ This work method takes an argument corresponding to the number of bytes\n+\/\/ free in a region, and returns the largest amount in heapwords that can be allocated\n+\/\/ such that both of the following conditions are satisfied:\n+\/\/\n+\/\/ 1. it is a multiple of card size\n+\/\/ 2. any remaining shard may be filled with a filler object\n+\/\/\n+\/\/ The idea is that the allocation starts and ends at card boundaries. Because\n+\/\/ a region ('s end) is card-aligned, the remainder shard that must be filled is\n+\/\/ at the start of the free space.\n+\/\/\n+\/\/ This is merely a helper method to use for the purpose of such a calculation.\n+size_t get_usable_free_words(size_t free_bytes) {\n+  \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n+  \/\/      free is 514\n+  \/\/      usable_free is 512, which is decreased to 0\n+  size_t usable_free = (free_bytes \/ CardTable::card_size()) << CardTable::card_shift();\n+  assert(usable_free <= free_bytes, \"Sanity check\");\n+  if ((free_bytes != usable_free) && (free_bytes - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n+    \/\/ After aligning to card multiples, the remainder would be smaller than\n+    \/\/ the minimum filler object, so we'll need to take away another card's\n+    \/\/ worth to construct a filler object.\n+    if (usable_free >= CardTable::card_size()) {\n+      usable_free -= CardTable::card_size();\n+    } else {\n+      assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n+    }\n+  }\n+\n+  return usable_free \/ HeapWordSize;\n+}\n+\n+\/\/ Given a size argument, which is a multiple of card size, a request struct\n+\/\/ for a PLAB, and an old region, return a pointer to the allocated space for\n+\/\/ a PLAB which is card-aligned and where any remaining shard in the region\n+\/\/ has been suitably filled by a filler object.\n+\/\/ It is assumed (and assertion-checked) that such an allocation is always possible.\n+HeapWord* ShenandoahFreeSet::allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r) {\n+  assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+  assert(r->is_old(), \"All PLABs reside in old-gen\");\n+  assert(!req.is_mutator_alloc(), \"PLABs should not be allocated by mutators.\");\n+  assert(size % CardTable::card_size_in_words() == 0, \"size must be multiple of card table size, was \" SIZE_FORMAT, size);\n+\n+  HeapWord* result = r->allocate_aligned(size, req, CardTable::card_size());\n+  assert(result != nullptr, \"Allocation cannot fail\");\n+  assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n+  assert(req.actual_size() == size, \"Should not have needed to adjust size for PLAB.\");\n+  assert(((uintptr_t) result) % CardTable::card_size_in_words() == 0, \"PLAB start must align with card boundary\");\n+\n+  return result;\n+}\n+\n@@ -252,2 +304,1 @@\n-  if (_heap->is_concurrent_weak_root_in_progress() &&\n-      r->is_trash()) {\n+  if (_heap->is_concurrent_weak_root_in_progress() && r->is_trash()) {\n@@ -256,0 +307,1 @@\n+\n@@ -281,1 +333,0 @@\n-  size_t size = req.size();\n@@ -291,1 +342,0 @@\n-      assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n@@ -293,23 +343,8 @@\n-      size_t free = r->free();\n-      \/\/ e.g. card_size is 512, card_shift is 9, min_fill_size() is 8\n-      \/\/      free is 514\n-      \/\/      usable_free is 512, which is decreased to 0\n-      size_t usable_free = (free \/ CardTable::card_size()) << CardTable::card_shift();\n-      if ((free != usable_free) && (free - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n-        \/\/ We'll have to add another card's memory to the padding\n-        if (usable_free >= CardTable::card_size()) {\n-          usable_free -= CardTable::card_size();\n-        } else {\n-          assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n-        }\n-      }\n-      free \/= HeapWordSize;\n-      usable_free \/= HeapWordSize;\n-      size_t remnant = size % CardTable::card_size_in_words();\n-      if (remnant > 0) {\n-        \/\/ Since we have Elastic TLABs, align size up.  This is consistent with aligning min_size up.\n-        size = size - remnant + CardTable::card_size_in_words();\n-      }\n-      if (size > usable_free) {\n-        size = usable_free;\n-        assert(size % CardTable::card_size_in_words() == 0, \"usable_free is a multiple of card table size\");\n+      \/\/ Since we have Elastic TLABs, align sizes up. They may be decreased to fit in the usable\n+      \/\/ memory remaining in the region (which will also be aligned to cards).\n+      size_t adjusted_size = align_up(req.size(), CardTable::card_size_in_words());\n+      size_t adjusted_min_size = align_up(req.min_size(), CardTable::card_size_in_words());\n+      size_t usable_free = get_usable_free_words(r->free());\n+\n+      if (adjusted_size > usable_free) {\n+        adjusted_size = usable_free;\n@@ -318,25 +353,2 @@\n-      size_t adjusted_min_size = req.min_size();\n-      remnant = adjusted_min_size % CardTable::card_size_in_words();\n-      if (remnant > 0) {\n-        \/\/ Round up adjusted_min_size to a multiple of alignment size\n-        adjusted_min_size = adjusted_min_size - remnant + CardTable::card_size_in_words();\n-      }\n-      if (size >= adjusted_min_size) {\n-        result = r->allocate_aligned(size, req, CardTable::card_size());\n-        assert(result != nullptr, \"Allocation cannot fail\");\n-        size = req.actual_size();\n-        assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n-        \/\/ actual_size() will be set to size below.\n-        assert((result == nullptr) || (size % CardTable::card_size_in_words() == 0),\n-               \"PLAB size must be multiple of card size\");\n-        assert((result == nullptr) || (((uintptr_t) result) % CardTable::card_size_in_words() == 0),\n-               \"PLAB start must align with card boundary\");\n-        if (free > usable_free) {\n-          \/\/ Account for the alignment padding\n-          size_t padding = (free - usable_free) * HeapWordSize;\n-          increase_used(padding);\n-          assert(r->is_old(), \"All PLABs reside in old-gen\");\n-          _heap->old_generation()->increase_used(padding);\n-          \/\/ For verification consistency, we need to report this padding to _heap\n-          _heap->increase_used(padding);\n-        }\n+      if (adjusted_size >= adjusted_min_size) {\n+        result = allocate_aligned_plab(adjusted_size, req, r);\n@@ -347,0 +359,1 @@\n+      size_t adjusted_size = req.size();\n@@ -348,2 +361,2 @@\n-      if (size > free) {\n-        size = free;\n+      if (adjusted_size > free) {\n+        adjusted_size = free;\n@@ -351,7 +364,4 @@\n-      if (size >= req.min_size()) {\n-        result = r->allocate(size, req);\n-        if (result != nullptr) {\n-          \/\/ Record actual allocation size\n-          req.set_actual_size(size);\n-        }\n-        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, size);\n+      if (adjusted_size >= req.min_size()) {\n+        result = r->allocate(adjusted_size, req);\n+        assert (result != nullptr, \"Allocation must succeed: free \" SIZE_FORMAT \", actual \" SIZE_FORMAT, free, adjusted_size);\n+        req.set_actual_size(adjusted_size);\n@@ -360,1 +370,1 @@\n-                           \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), size, req.min_size());\n+                           \" because min_size() is \" SIZE_FORMAT, req.size(), r->index(), adjusted_size, req.min_size());\n@@ -364,1 +374,1 @@\n-    assert(_heap->mode()->is_generational(), \"PLABs are only for generational mode\");\n+\n@@ -366,13 +376,2 @@\n-    size_t free = r->free();\n-    size_t usable_free = (free \/ CardTable::card_size()) << CardTable::card_shift();\n-    free \/= HeapWordSize;\n-    usable_free \/= HeapWordSize;\n-    if ((free != usable_free) && (free - usable_free < ShenandoahHeap::min_fill_size() * HeapWordSize)) {\n-      \/\/ We'll have to add another card's memory to the padding\n-      if (usable_free > CardTable::card_size_in_words()) {\n-        usable_free -= CardTable::card_size_in_words();\n-      } else {\n-        assert(usable_free == 0, \"usable_free is a multiple of card_size and card_size > min_fill_size\");\n-      }\n-    }\n-    assert(size % CardTable::card_size_in_words() == 0, \"PLAB size must be multiple of remembered set card size\");\n+    size_t size = req.size();\n+    size_t usable_free = get_usable_free_words(r->free());\n@@ -380,15 +379,1 @@\n-      result = r->allocate_aligned(size, req, CardTable::card_size());\n-      size = req.actual_size();\n-      assert(result != nullptr, \"Allocation cannot fail\");\n-      assert(r->top() <= r->end(), \"Allocation cannot span end of region\");\n-      assert(req.actual_size() % CardTable::card_size_in_words() == 0, \"PLAB start must align with card boundary\");\n-      assert(((uintptr_t) result) % CardTable::card_size_in_words() == 0, \"PLAB start must align with card boundary\");\n-      if (free > usable_free) {\n-        \/\/ Account for the alignment padding\n-        size_t padding = (free - usable_free) * HeapWordSize;\n-        increase_used(padding);\n-        assert(r->is_old(), \"All PLABs reside in old-gen\");\n-        _heap->old_generation()->increase_used(padding);\n-        \/\/ For verification consistency, we need to report this padding to _heap\n-        _heap->increase_used(padding);\n-      }\n+      result = allocate_aligned_plab(size, req, r);\n@@ -397,0 +382,1 @@\n+    size_t size = req.size();\n@@ -408,3 +394,1 @@\n-      assert(req.is_young(), \"Mutator allocations always come from young generation.\");\n-      generation->increase_used(size * HeapWordSize);\n-      increase_used(size * HeapWordSize);\n+      increase_used(req.actual_size() * HeapWordSize);\n@@ -422,1 +406,0 @@\n-      generation->increase_used(size * HeapWordSize);\n@@ -444,2 +427,2 @@\n-        generation->increase_allocated(waste);\n-        _heap->notify_mutator_alloc_words(waste >> LogHeapWordSize, true);\n+        \/\/ This one request could cause several regions to be \"retired\", so we must accumulate the waste\n+        req.set_waste((waste >> LogHeapWordSize) + req.waste());\n@@ -587,19 +570,0 @@\n-  if (_heap->mode()->is_generational()) {\n-    size_t humongous_waste = total_humongous_size - words_size * HeapWordSize;\n-    _heap->global_generation()->increase_used(words_size * HeapWordSize);\n-    _heap->global_generation()->increase_humongous_waste(humongous_waste);\n-    if (req.is_young()) {\n-      _heap->young_generation()->increase_used(words_size * HeapWordSize);\n-      _heap->young_generation()->increase_humongous_waste(humongous_waste);\n-    } else if (req.is_old()) {\n-      _heap->old_generation()->increase_used(words_size * HeapWordSize);\n-      _heap->old_generation()->increase_humongous_waste(humongous_waste);\n-    }\n-  }\n-\n-  if (remainder != 0) {\n-    \/\/ Record this remainder as allocation waste\n-    size_t waste = ShenandoahHeapRegion::region_size_words() - remainder;\n-    _heap->notify_mutator_alloc_words(waste, true);\n-    generation->increase_allocated(waste * HeapWordSize);\n-  }\n@@ -614,0 +578,3 @@\n+  if (remainder != 0) {\n+    req.set_waste(ShenandoahHeapRegion::region_size_words() - remainder);\n+  }\n@@ -636,1 +603,0 @@\n-    _heap->decrease_used(r->used());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.cpp","additions":83,"deletions":117,"binary":false,"changes":200,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+  HeapWord* allocate_aligned_plab(size_t size, ShenandoahAllocRequest& req, ShenandoahHeapRegion* r);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFreeSet.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1333,1 +1333,1 @@\n-    } else if (_is_generational) {\n+    } else {\n@@ -1353,3 +1353,14 @@\n-    assert(_is_generational, \"Only update generation usage if generational\");\n-    _heap->old_generation()->establish_usage(_old_regions, _old_usage, _old_humongous_waste);\n-    _heap->young_generation()->establish_usage(_young_regions, _young_usage, _young_humongous_waste);\n+    if (_is_generational) {\n+      _heap->old_generation()->establish_usage(_old_regions, _old_usage, _old_humongous_waste);\n+      _heap->young_generation()->establish_usage(_young_regions, _young_usage, _young_humongous_waste);\n+    } else {\n+      assert(_old_regions == 0, \"Old regions only expected in generational mode\");\n+      assert(_old_usage == 0, \"Old usage only expected in generational mode\");\n+      assert(_old_humongous_waste == 0, \"Old humongous waste only expected in generational mode\");\n+    }\n+\n+    \/\/ In generational mode, global usage should be the sum of young and old. This is also true\n+    \/\/ for non-generational modes except that there are no old regions.\n+    _heap->global_generation()->establish_usage(_old_regions + _young_regions,\n+                                                _old_usage + _young_usage,\n+                                                _old_humongous_waste + _young_humongous_waste);\n@@ -1496,8 +1507,5 @@\n-    heap->set_used(post_compact.get_live());\n-    if (heap->mode()->is_generational()) {\n-      post_compact.update_generation_usage();\n-      log_info(gc)(\"FullGC done: GLOBAL usage: \" SIZE_FORMAT \"%s, young usage: \" SIZE_FORMAT \"%s, old usage: \" SIZE_FORMAT \"%s\",\n-                   byte_size_in_proper_unit(post_compact.get_live()),          proper_unit_for_byte_size(post_compact.get_live()),\n-                   byte_size_in_proper_unit(heap->young_generation()->used()), proper_unit_for_byte_size(heap->young_generation()->used()),\n-                   byte_size_in_proper_unit(heap->old_generation()->used()),   proper_unit_for_byte_size(heap->old_generation()->used()));\n-    }\n+    post_compact.update_generation_usage();\n+    log_info(gc)(\"FullGC done: global usage: \" SIZE_FORMAT \"%s, young usage: \" SIZE_FORMAT \"%s, old usage: \" SIZE_FORMAT \"%s\",\n+                 byte_size_in_proper_unit(heap->global_generation()->used()), proper_unit_for_byte_size(heap->global_generation()->used()),\n+                 byte_size_in_proper_unit(heap->young_generation()->used()),  proper_unit_for_byte_size(heap->young_generation()->used()),\n+                 byte_size_in_proper_unit(heap->old_generation()->used()),    proper_unit_for_byte_size(heap->old_generation()->used()));\n@@ -1509,1 +1517,0 @@\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahFullGC.cpp","additions":20,"deletions":13,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -966,7 +966,0 @@\n-void ShenandoahGeneration::clear_used() {\n-  assert(ShenandoahHeap::heap()->mode()->is_generational(), \"Only generational mode accounts for generational usage\");\n-  assert(ShenandoahSafepoint::is_at_shenandoah_safepoint(), \"must be at a safepoint\");\n-  \/\/ Do this atomically to assure visibility to other threads, even though these other threads may be idle \"right now\"..\n-  Atomic::store(&_used, (size_t)0);\n-}\n-\n@@ -984,2 +977,1 @@\n-    shenandoah_assert_heaplocked_or_fullgc_safepoint();\n-    _humongous_waste += bytes;\n+    Atomic::add(&_humongous_waste, bytes);\n@@ -991,1 +983,0 @@\n-    shenandoah_assert_heaplocked_or_fullgc_safepoint();\n@@ -995,1 +986,1 @@\n-    _humongous_waste -= bytes;\n+    Atomic::sub(&_humongous_waste, bytes);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.cpp","additions":2,"deletions":11,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -203,1 +203,0 @@\n-  void clear_used();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGeneration.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -56,4 +56,0 @@\n-size_t ShenandoahGlobalGeneration::used() const {\n-  return ShenandoahHeap::heap()->used();\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -45,1 +45,0 @@\n-  virtual size_t used() const override;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahGlobalGeneration.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shenandoah\/shenandoahAllocRequest.hpp\"\n@@ -80,0 +81,1 @@\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -542,1 +544,0 @@\n-  _used(0),\n@@ -701,1 +702,1 @@\n-  return Atomic::load(&_used);\n+  return global_generation()->used();\n@@ -718,2 +719,50 @@\n-void ShenandoahHeap::increase_used(size_t bytes) {\n-  Atomic::add(&_used, bytes, memory_order_relaxed);\n+\/\/ For tracking usage based on allocations, it should be the case that:\n+\/\/ * The sum of regions::used == heap::used\n+\/\/ * The sum of a generation's regions::used == generation::used\n+\/\/ * The sum of a generation's humongous regions::free == generation::humongous_waste\n+\/\/ These invariants are checked by the verifier on GC safepoints.\n+\/\/\n+\/\/ Additional notes:\n+\/\/ * When a mutator's allocation request causes a region to be retired, the\n+\/\/   free memory left in that region is considered waste. It does not contribute\n+\/\/   to the usage, but it _does_ contribute to allocation rate.\n+\/\/ * The bottom of a PLAB must be aligned on card size. In some cases this will\n+\/\/   require padding in front of the PLAB (a filler object). Because this padding\n+\/\/   is included in the region's used memory we include the padding in the usage\n+\/\/   accounting as waste.\n+\/\/ * Mutator allocations are used to compute an allocation rate. They are also\n+\/\/   sent to the Pacer for those purposes.\n+\/\/ * There are three sources of waste:\n+\/\/  1. The padding used to align a PLAB on card size\n+\/\/  2. Region's free is less than minimum TLAB size and is retired\n+\/\/  3. The unused portion of memory in the last region of a humongous object\n+void ShenandoahHeap::increase_used(const ShenandoahAllocRequest& req) {\n+  size_t actual_bytes = req.actual_size() * HeapWordSize;\n+  size_t wasted_bytes = req.waste() * HeapWordSize;\n+  ShenandoahGeneration* generation = generation_for(req.affiliation());\n+\n+  if (req.is_gc_alloc()) {\n+    assert(wasted_bytes == 0 || req.type() == ShenandoahAllocRequest::_alloc_plab, \"Only PLABs have waste\");\n+    increase_used(generation, actual_bytes + wasted_bytes);\n+  } else {\n+    assert(req.is_mutator_alloc(), \"Expected mutator alloc here\");\n+    \/\/ padding and actual size both count towards allocation counter\n+    generation->increase_allocated(actual_bytes + wasted_bytes);\n+\n+    \/\/ only actual size counts toward usage for mutator allocations\n+    increase_used(generation, actual_bytes);\n+\n+    \/\/ notify pacer of both actual size and waste\n+    notify_mutator_alloc_words(req.actual_size(), req.waste());\n+\n+    if (wasted_bytes > 0 && req.actual_size() > ShenandoahHeapRegion::humongous_threshold_words()) {\n+      increase_humongous_waste(generation,wasted_bytes);\n+    }\n+  }\n+}\n+\n+void ShenandoahHeap::increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_humongous_waste(bytes);\n+  }\n@@ -722,2 +771,5 @@\n-void ShenandoahHeap::set_used(size_t bytes) {\n-  Atomic::store(&_used, bytes);\n+void ShenandoahHeap::decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_humongous_waste(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_humongous_waste(bytes);\n+  }\n@@ -726,3 +778,5 @@\n-void ShenandoahHeap::decrease_used(size_t bytes) {\n-  assert(used() >= bytes, \"never decrease heap size by more than we've left\");\n-  Atomic::sub(&_used, bytes, memory_order_relaxed);\n+void ShenandoahHeap::increase_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->increase_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->increase_used(bytes);\n+  }\n@@ -731,4 +785,4 @@\n-void ShenandoahHeap::notify_mutator_alloc_words(size_t words, bool waste) {\n-  size_t bytes = words * HeapWordSize;\n-  if (!waste) {\n-    increase_used(bytes);\n+void ShenandoahHeap::decrease_used(ShenandoahGeneration* generation, size_t bytes) {\n+  generation->decrease_used(bytes);\n+  if (!generation->is_global()) {\n+    global_generation()->decrease_used(bytes);\n@@ -736,0 +790,1 @@\n+}\n@@ -737,0 +792,1 @@\n+void ShenandoahHeap::notify_mutator_alloc_words(size_t words, size_t waste) {\n@@ -739,3 +795,1 @@\n-    if (waste) {\n-      pacer()->claim_for_alloc(words, true);\n-    }\n+    pacer()->claim_for_alloc(waste, true);\n@@ -1201,0 +1255,8 @@\n+  if (result == nullptr) {\n+    req.set_actual_size(0);\n+  }\n+\n+  \/\/ This is called regardless of the outcome of the allocation to account\n+  \/\/ for any waste created by retiring regions with this request.\n+  increase_used(req);\n+\n@@ -1202,1 +1264,0 @@\n-    ShenandoahGeneration* alloc_generation = generation_for(req.affiliation());\n@@ -1205,1 +1266,0 @@\n-    size_t actual_bytes = actual * HeapWordSize;\n@@ -1212,3 +1272,0 @@\n-      notify_mutator_alloc_words(actual, false);\n-      alloc_generation->increase_allocated(actual_bytes);\n-\n@@ -1221,2 +1278,0 @@\n-    } else {\n-      increase_used(actual_bytes);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":78,"deletions":23,"binary":false,"changes":101,"status":"modified"},{"patch":"@@ -220,1 +220,0 @@\n-  volatile size_t _used;\n@@ -224,0 +223,2 @@\n+  void increase_used(const ShenandoahAllocRequest& req);\n+\n@@ -225,3 +226,4 @@\n-  void increase_used(size_t bytes);\n-  void decrease_used(size_t bytes);\n-  void set_used(size_t bytes);\n+  void increase_used(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_used(ShenandoahGeneration* generation, size_t bytes);\n+  void increase_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n+  void decrease_humongous_waste(ShenandoahGeneration* generation, size_t bytes);\n@@ -706,1 +708,1 @@\n-  void notify_mutator_alloc_words(size_t words, bool waste);\n+  void notify_mutator_alloc_words(size_t words, size_t waste);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.hpp","additions":7,"deletions":5,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -290,3 +290,1 @@\n-      if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-        decrement_humongous_waste();\n-      }\n+      decrement_humongous_waste();\n@@ -661,1 +659,0 @@\n-  ShenandoahHeap* heap = ShenandoahHeap::heap();\n@@ -663,4 +660,3 @@\n-\n-  if (ShenandoahHeap::heap()->mode()->is_generational()) {\n-    heap->generation_for(affiliation())->decrease_used(used());\n-  }\n+  ShenandoahHeap* heap = ShenandoahHeap::heap();\n+  ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+  heap->decrease_used(generation, used());\n@@ -1096,2 +1092,3 @@\n-    ShenandoahHeap::heap()->generation_for(affiliation())->decrease_humongous_waste(waste_bytes);\n-    ShenandoahHeap::heap()->global_generation()->decrease_humongous_waste(waste_bytes);\n+    ShenandoahHeap* heap = ShenandoahHeap::heap();\n+    ShenandoahGeneration* generation = heap->generation_for(affiliation());\n+    heap->decrease_humongous_waste(generation, waste_bytes);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.cpp","additions":7,"deletions":10,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -96,0 +96,1 @@\n+    req.set_waste(pad_words);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeapRegion.inline.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -35,1 +35,0 @@\n-  volatile_nonstatic_field(ShenandoahHeap, _used,                  size_t)                            \\\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/vmStructs_shenandoah.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"}]}