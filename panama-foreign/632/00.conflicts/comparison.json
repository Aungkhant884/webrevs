{"files":[{"patch":"@@ -32,0 +32,1 @@\n+<<<<<<< HEAD\n@@ -33,0 +34,3 @@\n+=======\n+#include \"compiler\/oopMap.hpp\"\n+>>>>>>> 1e1574c9cb6186fd0087019cef81b6c67d26db29\n@@ -292,0 +296,1 @@\n+<<<<<<< HEAD\n@@ -300,0 +305,3 @@\n+=======\n+void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp) {\n+>>>>>>> 1e1574c9cb6186fd0087019cef81b6c67d26db29\n@@ -301,2 +309,2 @@\n-    lea(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n-    ldar(rscratch1, rscratch1);\n+    lea(tmp, Address(rthread, JavaThread::polling_word_offset()));\n+    ldar(tmp, tmp);\n@@ -304,1 +312,1 @@\n-    ldr(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n+    ldr(tmp, Address(rthread, JavaThread::polling_word_offset()));\n@@ -309,1 +317,1 @@\n-    cmp(in_nmethod ? sp : rfp, rscratch1);\n+    cmp(in_nmethod ? sp : rfp, tmp);\n@@ -312,1 +320,11 @@\n-    tbnz(rscratch1, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+    tbnz(tmp, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+  }\n+}\n+\n+void MacroAssembler::rt_call(address dest, Register tmp) {\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb) {\n+    far_call(RuntimeAddress(dest));\n+  } else {\n+    lea(tmp, RuntimeAddress(dest));\n+    blr(tmp);\n@@ -2145,1 +2163,1 @@\n-  tbz(r0, 0, not_weak);    \/\/ Test for jweak tag.\n+  tbz(value, 0, not_weak);    \/\/ Test for jweak tag.\n@@ -5205,1 +5223,1 @@\n-void MacroAssembler::verify_sve_vector_length() {\n+void MacroAssembler::verify_sve_vector_length(Register tmp) {\n@@ -5209,3 +5227,3 @@\n-  movw(rscratch1, zr);\n-  sve_inc(rscratch1, B);\n-  subsw(zr, rscratch1, VM_Version::get_initial_sve_vector_length());\n+  movw(tmp, zr);\n+  sve_inc(tmp, B);\n+  subsw(zr, tmp, VM_Version::get_initial_sve_vector_length());\n@@ -5272,0 +5290,176 @@\n+\n+\/\/ The java_calling_convention describes stack locations as ideal slots on\n+\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n+\/\/ (like the placement of the register window) the slots must be biased by\n+\/\/ the following value.\n+static int reg2offset_in(VMReg r) {\n+  \/\/ Account for saved rfp and lr\n+  \/\/ This should really be in_preserve_stack_slots\n+  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n+}\n+\n+static int reg2offset_out(VMReg r) {\n+  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n+}\n+\n+\/\/ On 64 bit we will store integer like items to the stack as\n+\/\/ 64 bits items (Aarch64 abi) even though java would only store\n+\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n+\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      sxtw(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ An oop arg. Must pass a handle not the oop itself\n+void MacroAssembler::object_move(\n+                        OopMap* map,\n+                        int oop_handle_offset,\n+                        int framesize_in_slots,\n+                        VMRegPair src,\n+                        VMRegPair dst,\n+                        bool is_receiver,\n+                        int* receiver_offset) {\n+\n+  \/\/ must pass a handle. First figure out the location we use as a handle\n+\n+  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n+\n+  \/\/ See if oop is NULL if it is we need no handle\n+\n+  if (src.first()->is_stack()) {\n+\n+    \/\/ Oop is already on the stack as an argument\n+    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n+    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n+    if (is_receiver) {\n+      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n+    }\n+\n+    ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+    lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n+    \/\/ conditionally move a NULL\n+    cmp(rscratch1, zr);\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  } else {\n+\n+    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+\n+    const Register rOop = src.first()->as_Register();\n+    int oop_slot;\n+    if (rOop == j_rarg0)\n+      oop_slot = 0;\n+    else if (rOop == j_rarg1)\n+      oop_slot = 1;\n+    else if (rOop == j_rarg2)\n+      oop_slot = 2;\n+    else if (rOop == j_rarg3)\n+      oop_slot = 3;\n+    else if (rOop == j_rarg4)\n+      oop_slot = 4;\n+    else if (rOop == j_rarg5)\n+      oop_slot = 5;\n+    else if (rOop == j_rarg6)\n+      oop_slot = 6;\n+    else {\n+      assert(rOop == j_rarg7, \"wrong register\");\n+      oop_slot = 7;\n+    }\n+\n+    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n+    int offset = oop_slot*VMRegImpl::stack_slot_size;\n+\n+    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n+    \/\/ Store oop in handle area, may be NULL\n+    str(rOop, Address(sp, offset));\n+    if (is_receiver) {\n+      *receiver_offset = offset;\n+    }\n+\n+    cmp(rOop, zr);\n+    lea(rHandle, Address(sp, offset));\n+    \/\/ conditionally move a NULL\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  }\n+\n+  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n+  if (dst.first()->is_stack()) {\n+    str(rHandle, Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A float arg may have to do float reg int reg conversion\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+ if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldrw(tmp, Address(rfp, reg2offset_in(src.first())));\n+      strw(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ldrs(dst.first()->as_FloatRegister(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      strs(src.first()->as_FloatRegister(), Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A long move\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      mov(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\n+\/\/ A double move\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst, Register tmp) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldr(tmp, Address(rfp, reg2offset_in(src.first())));\n+      str(tmp, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ldrd(dst.first()->as_FloatRegister(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      strd(src.first()->as_FloatRegister(), Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":204,"deletions":10,"binary":false,"changes":214,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+<<<<<<< HEAD\n@@ -31,0 +32,3 @@\n+=======\n+#include \"code\/vmreg.hpp\"\n+>>>>>>> 1e1574c9cb6186fd0087019cef81b6c67d26db29\n@@ -35,0 +39,2 @@\n+class OopMap;\n+\n@@ -107,1 +113,2 @@\n-  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod);\n+  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp = rscratch1);\n+  void rt_call(address dest, Register tmp = rscratch1);\n@@ -706,0 +713,14 @@\n+  \/\/ support for argument shuffling\n+  void move32_64(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void float_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void long_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void double_move(VMRegPair src, VMRegPair dst, Register tmp = rscratch1);\n+  void object_move(\n+                   OopMap* map,\n+                   int oop_handle_offset,\n+                   int framesize_in_slots,\n+                   VMRegPair src,\n+                   VMRegPair dst,\n+                   bool is_receiver,\n+                   int* receiver_offset);\n+\n@@ -954,1 +975,1 @@\n-  void verify_sve_vector_length();\n+  void verify_sve_vector_length(Register tmp = rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":23,"deletions":2,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2019, Arm Limited. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Arm Limited. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -29,0 +30,7 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -32,9 +40,10 @@\n-\/\/ 1. Create buffer according to layout\n-\/\/ 2. Load registers & stack args into buffer\n-\/\/ 3. Call upcall helper with upcall handler instance & buffer pointer (C++ ABI)\n-\/\/ 4. Load return value from buffer into foreign ABI registers\n-\/\/ 5. Return\n-address ProgrammableUpcallHandler::generate_upcall_stub(jobject rec, jobject jabi, jobject jlayout) {\n-  ResourceMark rm;\n-  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n-  const BufferLayout layout = ForeignGlobals::parse_buffer_layout(jlayout);\n+\/\/ for callee saved regs, according to the caller's ABI\n+static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n+  int size = 0;\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n@@ -42,1 +51,7 @@\n-  CodeBuffer buffer(\"upcall_stub\", 1024, upcall_stub_size);\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      \/\/ Only the lower 64 bits of vector registers need to be preserved.\n+      size += 8; \/\/ bytes\n+    }\n+  }\n@@ -44,1 +59,2 @@\n-  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  return size;\n+}\n@@ -46,2 +62,4 @@\n-  \/\/ stub code\n-  __ enter();\n+static void preserve_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to save it here\n@@ -49,2 +67,1 @@\n-  \/\/ save pointer to JNI receiver handle into constant segment\n-  Address rec_adr = InternalAddress(__ address_constant((address)rec));\n+  int offset = reg_save_area_offset;\n@@ -52,1 +69,9 @@\n-  assert(abi._stack_alignment_bytes % 16 == 0, \"stack must be 16 byte aligned\");\n+  __ block_comment(\"{ preserve_callee_saved_regs \");\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ str(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n@@ -54,1 +79,10 @@\n-  __ sub(sp, sp, (int) align_up(layout.buffer_size, abi._stack_alignment_bytes));\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ strd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} preserve_callee_saved_regs \");\n+}\n@@ -56,3 +90,16 @@\n-  \/\/ TODO: This stub only uses registers which are caller-save in the\n-  \/\/       standard C ABI. If this is called from a different ABI then\n-  \/\/       we need to save registers here according to abi.is_volatile_reg.\n+static void restore_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to restore it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ restore_callee_saved_regs \");\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ldr(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n@@ -60,4 +107,6 @@\n-  for (int i = 0; i < abi._integer_argument_registers.length(); i++) {\n-    Register reg = abi._integer_argument_registers.at(i);\n-    ssize_t offset = layout.arguments_integer + i * sizeof(uintptr_t);\n-    __ str(reg, Address(sp, offset));\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ldrd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n@@ -66,4 +115,54 @@\n-  for (int i = 0; i < abi._vector_argument_registers.length(); i++) {\n-    FloatRegister reg = abi._vector_argument_registers.at(i);\n-    ssize_t offset = layout.arguments_vector + i * float_reg_size;\n-    __ strq(reg, Address(sp, offset));\n+  __ block_comment(\"} restore_callee_saved_regs \");\n+}\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+  ResourceMark rm;\n+  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n+  CodeBuffer buffer(\"upcall_stub_linkToNative\", \/* code_size = *\/ 2048, \/* locs_size = *\/ 1024);\n+\n+  Register shuffle_reg = r19;\n+  JavaCallConv out_conv;\n+  NativeCallConv in_conv(call_regs._arg_regs, call_regs._args_length);\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &in_conv, &out_conv, shuffle_reg->as_VMReg());\n+  int stack_slots = SharedRuntime::out_preserve_stack_slots() + arg_shuffle.out_arg_stack_slots();\n+  int out_arg_area = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ out_arg_area (for stack arguments) doubles as shadow space for native calls.\n+  \/\/ make sure it is big enough.\n+  if (out_arg_area < frame::arg_reg_save_area_bytes) {\n+    out_arg_area = frame::arg_reg_save_area_bytes;\n+  }\n+\n+  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  RegSpiller arg_spilller(call_regs._arg_regs, call_regs._args_length);\n+  RegSpiller result_spiller(call_regs._ret_regs, call_regs._rets_length);\n+  \/\/ To spill receiver during deopt\n+  int deopt_spill_size = 1 * BytesPerWord;\n+\n+  int shuffle_area_offset    = 0;\n+  int deopt_spill_offset     = shuffle_area_offset    + out_arg_area;\n+  int res_save_area_offset   = deopt_spill_offset     + deopt_spill_size;\n+  int arg_save_area_offset   = res_save_area_offset   + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset   = arg_save_area_offset   + arg_spilller.spill_size_bytes();\n+  int frame_data_offset      = reg_save_area_offset   + reg_save_area_size;\n+  int frame_bottom_offset    = frame_data_offset      + sizeof(OptimizedEntryBlob::FrameData);\n+\n+  int ret_buf_offset = -1;\n+  if (needs_return_buffer) {\n+    ret_buf_offset = frame_bottom_offset;\n+    frame_bottom_offset += ret_buf_size;\n@@ -72,0 +171,1 @@\n+<<<<<<< HEAD\n@@ -76,0 +176,4 @@\n+=======\n+  int frame_size = frame_bottom_offset;\n+  frame_size = align_up(frame_size, StackAlignmentInBytes);\n+>>>>>>> 1e1574c9cb6186fd0087019cef81b6c67d26db29\n@@ -77,4 +181,45 @@\n-  \/\/ Call upcall helper\n-  __ ldr(c_rarg0, rec_adr);\n-  __ mov(c_rarg1, sp);\n-  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::attach_thread_and_do_upcall));\n+  \/\/ The space we have allocated will look like:\n+  \/\/\n+  \/\/\n+  \/\/ FP-> |                     |\n+  \/\/      |---------------------| = frame_bottom_offset = frame_size\n+  \/\/      | (optional)          |\n+  \/\/      | ret_buf             |\n+  \/\/      |---------------------| = ret_buf_offset\n+  \/\/      |                     |\n+  \/\/      | FrameData           |\n+  \/\/      |---------------------| = frame_data_offset\n+  \/\/      |                     |\n+  \/\/      | reg_save_area       |\n+  \/\/      |---------------------| = reg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | arg_save_area       |\n+  \/\/      |---------------------| = arg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | res_save_area       |\n+  \/\/      |---------------------| = res_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | deopt_spill         |\n+  \/\/      |---------------------| = deopt_spill_offset\n+  \/\/      |                     |\n+  \/\/ SP-> | out_arg_area        |   needs to be at end for shadow space\n+  \/\/\n+  \/\/\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  address start = __ pc();\n+  __ enter(); \/\/ set up frame\n+  assert((abi._stack_alignment_bytes % 16) == 0, \"must be 16 byte aligned\");\n+  \/\/ allocate frame (frame_size is also aligned, so stack is still aligned)\n+  __ sub(sp, sp, frame_size);\n+\n+  \/\/ we have to always spill args since we need to do a call to get the thread\n+  \/\/ (and maybe attach it).\n+  arg_spilller.generate_spill(_masm, arg_save_area_offset);\n+  preserve_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  __ block_comment(\"{ on_entry\");\n+  __ lea(c_rarg0, Address(sp, frame_data_offset));\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::on_entry));\n@@ -82,0 +227,3 @@\n+  __ mov(rthread, r0);\n+  __ reinit_heapbase();\n+  __ block_comment(\"} on_entry\");\n@@ -83,3 +231,5 @@\n-  for (int i = 0; i < abi._integer_return_registers.length(); i++) {\n-    ssize_t offs = layout.returns_integer + i * sizeof(uintptr_t);\n-    __ ldr(abi._integer_return_registers.at(i), Address(sp, offs));\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_spilller.generate_fill(_masm, arg_save_area_offset);\n+  if (needs_return_buffer) {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(abi._ret_buf_addr_reg, Address(sp, ret_buf_offset));\n@@ -87,0 +237,11 @@\n+  arg_shuffle.generate(_masm, shuffle_reg->as_VMReg(), abi._shadow_space_bytes, 0);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ block_comment(\"{ receiver \");\n+  __ movptr(shuffle_reg, (intptr_t)receiver);\n+  __ resolve_jobject(shuffle_reg, rthread, rscratch2);\n+  __ mov(j_rarg0, shuffle_reg);\n+  __ block_comment(\"} receiver \");\n+\n+  __ mov_metadata(rmethod, entry);\n+  __ str(rmethod, Address(rthread, JavaThread::callee_target_offset())); \/\/ just in case callee is deoptimized\n@@ -88,4 +249,46 @@\n-  for (int i = 0; i < abi._vector_return_registers.length(); i++) {\n-    FloatRegister reg = abi._vector_return_registers.at(i);\n-    ssize_t offs = layout.returns_vector + i * float_reg_size;\n-    __ ldrq(reg, Address(sp, offs));\n+  __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+  __ blr(rscratch1);\n+\n+    \/\/ return value shuffle\n+  if (!needs_return_buffer) {\n+#ifdef ASSERT\n+    if (call_regs._rets_length == 1) { \/\/ 0 or 1\n+      VMReg j_expected_result_reg;\n+      switch (ret_type) {\n+        case T_BOOLEAN:\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_CHAR:\n+        case T_INT:\n+        case T_LONG:\n+        j_expected_result_reg = r0->as_VMReg();\n+        break;\n+        case T_FLOAT:\n+        case T_DOUBLE:\n+          j_expected_result_reg = v0->as_VMReg();\n+          break;\n+        default:\n+          fatal(\"unexpected return type: %s\", type2name(ret_type));\n+      }\n+      \/\/ No need to move for now, since CallArranger can pick a return type\n+      \/\/ that goes in the same reg for both CCs. But, at least assert they are the same\n+      assert(call_regs._ret_regs[0] == j_expected_result_reg,\n+      \"unexpected result register: %s != %s\", call_regs._ret_regs[0]->name(), j_expected_result_reg->name());\n+    }\n+#endif\n+  } else {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(rscratch1, Address(sp, ret_buf_offset));\n+    int offset = 0;\n+    for (int i = 0; i < call_regs._rets_length; i++) {\n+      VMReg reg = call_regs._ret_regs[i];\n+      if (reg->is_Register()) {\n+        __ ldr(reg->as_Register(), Address(rscratch1, offset));\n+        offset += 8;\n+      } else if (reg->is_FloatRegister()) {\n+        __ ldrd(reg->as_FloatRegister(), Address(rscratch1, offset));\n+        offset += 16; \/\/ needs to match VECTOR_REG_SIZE in AArch64Architecture (Java)\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n@@ -94,0 +297,13 @@\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n+\n+  __ block_comment(\"{ on_exit\");\n+  __ lea(c_rarg0, Address(sp, frame_data_offset));\n+  \/\/ stack already aligned\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::on_exit));\n+  __ blr(rscratch1);\n+  __ block_comment(\"} on_exit\");\n+\n+  restore_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n+\n@@ -97,1 +313,1 @@\n-  __ flush();\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n@@ -99,1 +315,1 @@\n-  BufferBlob* blob = BufferBlob::create(\"upcall_stub\", &buffer);\n+  __ block_comment(\"{ exception handler\");\n@@ -101,2 +317,1 @@\n-  return blob->code_begin();\n-}\n+  intptr_t exception_handler_offset = __ pc() - start;\n@@ -104,4 +319,8 @@\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n+  \/\/ Native caller has no idea how to handle exceptions,\n+  \/\/ so we just crash here. Up to callee to catch exceptions.\n+  __ verify_oop(r0);\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::handle_uncaught_exception));\n+  __ blr(rscratch1);\n+  __ should_not_reach_here();\n+\n+  __ block_comment(\"} exception handler\");\n@@ -109,2 +328,22 @@\n-bool ProgrammableUpcallHandler::supports_optimized_upcalls() {\n-  return false;\n+  _masm->flush();\n+\n+#ifndef PRODUCT\n+  stringStream ss;\n+  ss.print(\"optimized_upcall_stub_%s\", entry->signature()->as_C_string());\n+  const char* name = _masm->code_string(ss.as_string());\n+#else \/\/ PRODUCT\n+  const char* name = \"optimized_upcall_stub\";\n+#endif \/\/ PRODUCT\n+\n+  OptimizedEntryBlob* blob\n+    = OptimizedEntryBlob::create(name,\n+                                 &buffer,\n+                                 exception_handler_offset,\n+                                 receiver,\n+                                 in_ByteSize(frame_data_offset));\n+\n+  if (TraceOptimizedUpcallStubs) {\n+    blob->print_on(tty);\n+  }\n+\n+  return blob->code_begin();\n","filename":"src\/hotspot\/cpu\/aarch64\/universalUpcallHandler_aarch64.cpp","additions":290,"deletions":51,"binary":false,"changes":341,"status":"modified"}]}