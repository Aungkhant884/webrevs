{"files":[{"patch":"@@ -26,0 +26,1 @@\n+#include \"code\/vmreg.inline.hpp\"\n@@ -29,0 +30,2 @@\n+#include \"oops\/oopCast.inline.hpp\"\n+#include \"opto\/matcher.hpp\"\n@@ -31,0 +34,1 @@\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -50,1 +54,1 @@\n-  objArrayOop inputStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.inputStorage_offset));\n+  objArrayOop inputStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.inputStorage_offset));\n@@ -54,1 +58,1 @@\n-  objArrayOop outputStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.outputStorage_offset));\n+  objArrayOop outputStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.outputStorage_offset));\n@@ -58,1 +62,1 @@\n-  objArrayOop volatileStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.volatileStorage_offset));\n+  objArrayOop volatileStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.volatileStorage_offset));\n@@ -76,1 +80,1 @@\n-  typeArrayOop input_offsets = cast<typeArrayOop>(layout_oop->obj_field(BL.input_type_offsets_offset));\n+  typeArrayOop input_offsets = oop_cast<typeArrayOop>(layout_oop->obj_field(BL.input_type_offsets_offset));\n@@ -80,1 +84,1 @@\n-  typeArrayOop output_offsets = cast<typeArrayOop>(layout_oop->obj_field(BL.output_type_offsets_offset));\n+  typeArrayOop output_offsets = oop_cast<typeArrayOop>(layout_oop->obj_field(BL.output_type_offsets_offset));\n@@ -93,0 +97,93 @@\n+\n+enum class RegType {\n+  INTEGER = 0,\n+  VECTOR = 1,\n+  STACK = 3\n+};\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  switch(static_cast<RegType>(type)) {\n+    case RegType::INTEGER: return ::as_Register(index)->as_VMReg();\n+    case RegType::VECTOR: return ::as_FloatRegister(index)->as_VMReg();\n+    case RegType::STACK: return VMRegImpl::stack2reg(index LP64_ONLY(* 2));\n+  }\n+  return VMRegImpl::Bad();\n+}\n+\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  if (reg->is_Register()) {\n+    return 8;\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      return Matcher::scalable_vector_reg_size(T_BYTE);\n+    }\n+    return 16;\n+  }\n+  return 0; \/\/ stack and BAD\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->spill(reg->as_Register(), true, offset);\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      masm->spill_sve_vector(reg->as_FloatRegister(), offset, Matcher::scalable_vector_reg_size(T_BYTE));\n+    } else {\n+      masm->spill(reg->as_FloatRegister(), masm->Q, offset);\n+    }\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->unspill(reg->as_Register(), true, offset);\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      masm->unspill_sve_vector(reg->as_FloatRegister(), offset, Matcher::scalable_vector_reg_size(T_BYTE));\n+    } else {\n+      masm->unspill(reg->as_FloatRegister(), masm->Q, offset);\n+    }\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm) const {\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    masm->block_comment(err_msg(\"bt=%s\", null_safe_string(type2name(arg_bt))));\n+    switch (arg_bt) {\n+      case T_BOOLEAN:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_CHAR:\n+      case T_INT:\n+        masm->move32_64(from_vmreg, to_vmreg);\n+        break;\n+\n+      case T_FLOAT:\n+        masm->float_move(from_vmreg, to_vmreg);\n+        break;\n+\n+      case T_DOUBLE:\n+        masm->double_move(from_vmreg, to_vmreg);\n+        break;\n+\n+      case T_LONG :\n+        masm->long_move(from_vmreg, to_vmreg);\n+        break;\n+\n+      default:\n+        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/foreign_globals_aarch64.cpp","additions":102,"deletions":5,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"compiler\/oopMap.hpp\"\n@@ -295,1 +296,1 @@\n-void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod) {\n+void MacroAssembler::safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp) {\n@@ -297,2 +298,2 @@\n-    lea(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n-    ldar(rscratch1, rscratch1);\n+    lea(tmp, Address(rthread, JavaThread::polling_word_offset()));\n+    ldar(tmp, tmp);\n@@ -300,1 +301,1 @@\n-    ldr(rscratch1, Address(rthread, JavaThread::polling_word_offset()));\n+    ldr(tmp, Address(rthread, JavaThread::polling_word_offset()));\n@@ -305,1 +306,1 @@\n-    cmp(in_nmethod ? sp : rfp, rscratch1);\n+    cmp(in_nmethod ? sp : rfp, tmp);\n@@ -308,1 +309,11 @@\n-    tbnz(rscratch1, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+    tbnz(tmp, log2i_exact(SafepointMechanism::poll_bit()), slow_path);\n+  }\n+}\n+\n+void MacroAssembler::rt_call(address dest, Register tmp) {\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb) {\n+    far_call(RuntimeAddress(dest));\n+  } else {\n+    lea(tmp, RuntimeAddress(dest));\n+    blr(tmp);\n@@ -5110,1 +5121,1 @@\n-void MacroAssembler::verify_sve_vector_length() {\n+void MacroAssembler::verify_sve_vector_length(Register tmp) {\n@@ -5114,3 +5125,3 @@\n-  movw(rscratch1, zr);\n-  sve_inc(rscratch1, B);\n-  subsw(zr, rscratch1, VM_Version::get_initial_sve_vector_length());\n+  movw(tmp, zr);\n+  sve_inc(tmp, B);\n+  subsw(zr, tmp, VM_Version::get_initial_sve_vector_length());\n@@ -5159,0 +5170,180 @@\n+\n+\/\/ The java_calling_convention describes stack locations as ideal slots on\n+\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n+\/\/ (like the placement of the register window) the slots must be biased by\n+\/\/ the following value.\n+static int reg2offset_in(VMReg r) {\n+  \/\/ Account for saved rfp and lr\n+  \/\/ This should really be in_preserve_stack_slots\n+  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n+}\n+\n+static int reg2offset_out(VMReg r) {\n+  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n+}\n+\n+\/\/ On 64 bit we will store integer like items to the stack as\n+\/\/ 64 bits items (Aarch64 abi) even though java would only store\n+\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n+\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n+void MacroAssembler::move32_64(VMRegPair src, VMRegPair dst) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+      str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      sxtw(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\/\/ An oop arg. Must pass a handle not the oop itself\n+void MacroAssembler::object_move(\n+                        OopMap* map,\n+                        int oop_handle_offset,\n+                        int framesize_in_slots,\n+                        VMRegPair src,\n+                        VMRegPair dst,\n+                        bool is_receiver,\n+                        int* receiver_offset) {\n+\n+  \/\/ must pass a handle. First figure out the location we use as a handle\n+\n+  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n+\n+  \/\/ See if oop is NULL if it is we need no handle\n+\n+  if (src.first()->is_stack()) {\n+\n+    \/\/ Oop is already on the stack as an argument\n+    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n+    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n+    if (is_receiver) {\n+      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n+    }\n+\n+    ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+    lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n+    \/\/ conditionally move a NULL\n+    cmp(rscratch1, zr);\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  } else {\n+\n+    \/\/ Oop is in an a register we must store it to the space we reserve\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+\n+    const Register rOop = src.first()->as_Register();\n+    int oop_slot;\n+    if (rOop == j_rarg0)\n+      oop_slot = 0;\n+    else if (rOop == j_rarg1)\n+      oop_slot = 1;\n+    else if (rOop == j_rarg2)\n+      oop_slot = 2;\n+    else if (rOop == j_rarg3)\n+      oop_slot = 3;\n+    else if (rOop == j_rarg4)\n+      oop_slot = 4;\n+    else if (rOop == j_rarg5)\n+      oop_slot = 5;\n+    else if (rOop == j_rarg6)\n+      oop_slot = 6;\n+    else {\n+      assert(rOop == j_rarg7, \"wrong register\");\n+      oop_slot = 7;\n+    }\n+\n+    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n+    int offset = oop_slot*VMRegImpl::stack_slot_size;\n+\n+    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n+    \/\/ Store oop in handle area, may be NULL\n+    str(rOop, Address(sp, offset));\n+    if (is_receiver) {\n+      *receiver_offset = offset;\n+    }\n+\n+    cmp(rOop, zr);\n+    lea(rHandle, Address(sp, offset));\n+    \/\/ conditionally move a NULL\n+    csel(rHandle, zr, rHandle, Assembler::EQ);\n+  }\n+\n+  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n+  if (dst.first()->is_stack()) {\n+    str(rHandle, Address(sp, reg2offset_out(dst.first())));\n+  }\n+}\n+\n+\/\/ A float arg may have to do float reg int reg conversion\n+void MacroAssembler::float_move(VMRegPair src, VMRegPair dst) {\n+  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n+         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldrw(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+      strw(rscratch1, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      ShouldNotReachHere();\n+  }\n+}\n+\n+\/\/ A long move\n+void MacroAssembler::long_move(VMRegPair src, VMRegPair dst) {\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      \/\/ stack to stack\n+      ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+      str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      \/\/ stack to reg\n+      ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n+    }\n+  } else if (dst.first()->is_stack()) {\n+    \/\/ reg to stack\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+    str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n+  } else {\n+    if (dst.first() != src.first()) {\n+      mov(dst.first()->as_Register(), src.first()->as_Register());\n+    }\n+  }\n+}\n+\n+\n+\/\/ A double move\n+void MacroAssembler::double_move(VMRegPair src, VMRegPair dst) {\n+  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n+         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n+  if (src.first()->is_stack()) {\n+    if (dst.first()->is_stack()) {\n+      ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n+      str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n+    } else {\n+      ShouldNotReachHere();\n+    }\n+  } else if (src.first() != dst.first()) {\n+    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n+      fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n+    else\n+      ShouldNotReachHere();\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":201,"deletions":10,"binary":false,"changes":211,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"code\/vmreg.hpp\"\n@@ -34,0 +35,2 @@\n+class OopMap;\n+\n@@ -106,1 +109,2 @@\n-  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod);\n+  void safepoint_poll(Label& slow_path, bool at_return, bool acquire, bool in_nmethod, Register tmp = rscratch1);\n+  void rt_call(address dest, Register tmp = rscratch1);\n@@ -701,0 +705,14 @@\n+  \/\/ support for argument shuffling\n+  void move32_64(VMRegPair src, VMRegPair dst);\n+  void float_move(VMRegPair src, VMRegPair dst);\n+  void long_move(VMRegPair src, VMRegPair dst);\n+  void double_move(VMRegPair src, VMRegPair dst);\n+  void object_move(\n+                   OopMap* map,\n+                   int oop_handle_offset,\n+                   int framesize_in_slots,\n+                   VMRegPair src,\n+                   VMRegPair dst,\n+                   bool is_receiver,\n+                   int* receiver_offset);\n+\n@@ -949,1 +967,1 @@\n-  void verify_sve_vector_length();\n+  void verify_sve_vector_length(Register tmp = rscratch1);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":20,"deletions":2,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -261,0 +261,15 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_invoke_NativeEntryPoint::invoker_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ br(temp_target);\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -273,1 +288,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n@@ -282,4 +297,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -289,0 +301,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":20,"deletions":5,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -59,0 +59,3 @@\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -245,14 +245,0 @@\n-\/\/ The java_calling_convention describes stack locations as ideal slots on\n-\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n-\/\/ (like the placement of the register window) the slots must be biased by\n-\/\/ the following value.\n-static int reg2offset_in(VMReg r) {\n-  \/\/ Account for saved rfp and lr\n-  \/\/ This should really be in_preserve_stack_slots\n-  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n-}\n-\n-static int reg2offset_out(VMReg r) {\n-  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n-}\n-\n@@ -884,166 +870,0 @@\n-\/\/ On 64 bit we will store integer like items to the stack as\n-\/\/ 64 bits items (Aarch64 abi) even though java would only store\n-\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n-\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n-static void move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ sxtw(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\/\/ An oop arg. Must pass a handle not the oop itself\n-static void object_move(MacroAssembler* masm,\n-                        OopMap* map,\n-                        int oop_handle_offset,\n-                        int framesize_in_slots,\n-                        VMRegPair src,\n-                        VMRegPair dst,\n-                        bool is_receiver,\n-                        int* receiver_offset) {\n-\n-  \/\/ must pass a handle. First figure out the location we use as a handle\n-\n-  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n-\n-  \/\/ See if oop is NULL if it is we need no handle\n-\n-  if (src.first()->is_stack()) {\n-\n-    \/\/ Oop is already on the stack as an argument\n-    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n-    if (is_receiver) {\n-      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n-    }\n-\n-    __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-    __ lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n-    \/\/ conditionally move a NULL\n-    __ cmp(rscratch1, zr);\n-    __ csel(rHandle, zr, rHandle, Assembler::EQ);\n-  } else {\n-\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n-\n-    const Register rOop = src.first()->as_Register();\n-    int oop_slot;\n-    if (rOop == j_rarg0)\n-      oop_slot = 0;\n-    else if (rOop == j_rarg1)\n-      oop_slot = 1;\n-    else if (rOop == j_rarg2)\n-      oop_slot = 2;\n-    else if (rOop == j_rarg3)\n-      oop_slot = 3;\n-    else if (rOop == j_rarg4)\n-      oop_slot = 4;\n-    else if (rOop == j_rarg5)\n-      oop_slot = 5;\n-    else if (rOop == j_rarg6)\n-      oop_slot = 6;\n-    else {\n-      assert(rOop == j_rarg7, \"wrong register\");\n-      oop_slot = 7;\n-    }\n-\n-    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n-    int offset = oop_slot*VMRegImpl::stack_slot_size;\n-\n-    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n-    \/\/ Store oop in handle area, may be NULL\n-    __ str(rOop, Address(sp, offset));\n-    if (is_receiver) {\n-      *receiver_offset = offset;\n-    }\n-\n-    __ cmp(rOop, zr);\n-    __ lea(rHandle, Address(sp, offset));\n-    \/\/ conditionally move a NULL\n-    __ csel(rHandle, zr, rHandle, Assembler::EQ);\n-  }\n-\n-  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n-  if (dst.first()->is_stack()) {\n-    __ str(rHandle, Address(sp, reg2offset_out(dst.first())));\n-  }\n-}\n-\n-\/\/ A float arg may have to do float reg int reg conversion\n-static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n-         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ ldrw(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ strw(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (src.first() != dst.first()) {\n-    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n-      __ fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n-    else\n-      ShouldNotReachHere();\n-  }\n-}\n-\n-\/\/ A long move\n-static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ mov(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\n-\/\/ A double move\n-static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n-         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (src.first() != dst.first()) {\n-    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n-      __ fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n-    else\n-      ShouldNotReachHere();\n-  }\n-}\n-\n@@ -1177,11 +997,0 @@\n-\n-static void rt_call(MacroAssembler* masm, address dest) {\n-  CodeBlob *cb = CodeCache::find_blob(dest);\n-  if (cb) {\n-    __ far_call(RuntimeAddress(dest));\n-  } else {\n-    __ lea(rscratch1, RuntimeAddress(dest));\n-    __ blr(rscratch1);\n-  }\n-}\n-\n@@ -1227,1 +1036,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1229,0 +1038,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = r19;  \/\/ known to be free at this point\n@@ -1665,3 +1477,3 @@\n-        object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n-                    ((i == 0) && (!is_static)),\n-                    &receiver_offset);\n+        __ object_move(map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n+                       ((i == 0) && (!is_static)),\n+                       &receiver_offset);\n@@ -1674,1 +1486,1 @@\n-        float_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ float_move(in_regs[i], out_regs[c_arg]);\n@@ -1682,1 +1494,1 @@\n-        double_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ double_move(in_regs[i], out_regs[c_arg]);\n@@ -1687,1 +1499,1 @@\n-        long_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ long_move(in_regs[i], out_regs[c_arg]);\n@@ -1694,1 +1506,1 @@\n-        move32_64(masm, in_regs[i], out_regs[c_arg]);\n+        __ move32_64(in_regs[i], out_regs[c_arg]);\n@@ -1825,1 +1637,1 @@\n-  rt_call(masm, native_func);\n+  __ rt_call(native_func);\n@@ -2052,1 +1864,1 @@\n-    rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n+    __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n@@ -2079,1 +1891,1 @@\n-  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n@@ -3087,251 +2899,0 @@\n-\/\/ ---------------------------------------------------------------\n-\n-class NativeInvokerGenerator : public StubCodeGenerator {\n-  address _call_target;\n-  int _shadow_space_bytes;\n-\n-  const GrowableArray<VMReg>& _input_registers;\n-  const GrowableArray<VMReg>& _output_registers;\n-\n-  int _frame_complete;\n-  int _framesize;\n-  OopMapSet* _oop_maps;\n-public:\n-  NativeInvokerGenerator(CodeBuffer* buffer,\n-                         address call_target,\n-                         int shadow_space_bytes,\n-                         const GrowableArray<VMReg>& input_registers,\n-                         const GrowableArray<VMReg>& output_registers)\n-   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n-     _call_target(call_target),\n-     _shadow_space_bytes(shadow_space_bytes),\n-     _input_registers(input_registers),\n-     _output_registers(output_registers),\n-     _frame_complete(0),\n-     _framesize(0),\n-     _oop_maps(NULL) {\n-    assert(_output_registers.length() <= 1\n-           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n-  }\n-\n-  void generate();\n-\n-  int spill_size_in_bytes() const {\n-    if (_output_registers.length() == 0) {\n-      return 0;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    if (reg->is_Register()) {\n-      return 8;\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        return Matcher::scalable_vector_reg_size(T_BYTE);\n-      }\n-      return 16;\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-\n-  void spill_output_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ spill(reg->as_Register(), true, 0);\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        __ spill_sve_vector(reg->as_FloatRegister(), 0, Matcher::scalable_vector_reg_size(T_BYTE));\n-      } else {\n-        __ spill(reg->as_FloatRegister(), __ Q, 0);\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  void fill_output_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ unspill(reg->as_Register(), true, 0);\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        __ unspill_sve_vector(reg->as_FloatRegister(), 0, Matcher::scalable_vector_reg_size(T_BYTE));\n-      } else {\n-        __ unspill(reg->as_FloatRegister(), __ Q, 0);\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  int frame_complete() const {\n-    return _frame_complete;\n-  }\n-\n-  int framesize() const {\n-    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n-  }\n-\n-  OopMapSet* oop_maps() const {\n-    return _oop_maps;\n-  }\n-\n-private:\n-#ifdef ASSERT\n-  bool target_uses_register(VMReg reg) {\n-    return _input_registers.contains(reg) || _output_registers.contains(reg);\n-  }\n-#endif\n-};\n-\n-static const int native_invoker_code_size = 1024;\n-\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  int locs_size  = 64;\n-  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n-  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n-  g.generate();\n-  code.log_section_sizes(\"nep_invoker_blob\");\n-\n-  RuntimeStub* stub =\n-    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n-                                  &code,\n-                                  g.frame_complete(),\n-                                  g.framesize(),\n-                                  g.oop_maps(), false);\n-  return stub;\n-}\n-\n-void NativeInvokerGenerator::generate() {\n-  assert(!(target_uses_register(rscratch1->as_VMReg())\n-           || target_uses_register(rscratch2->as_VMReg())\n-           || target_uses_register(rthread->as_VMReg())),\n-         \"Register conflict\");\n-\n-  enum layout {\n-    rbp_off,\n-    rbp_off2,\n-    return_off,\n-    return_off2,\n-    framesize \/\/ inclusive of return address\n-  };\n-\n-  assert(_shadow_space_bytes == 0, \"not expecting shadow space on AArch64\");\n-  _framesize = align_up(framesize + (spill_size_in_bytes() >> LogBytesPerInt), 4);\n-  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n-\n-  _oop_maps  = new OopMapSet();\n-  MacroAssembler* masm = _masm;\n-\n-  address start = __ pc();\n-\n-  __ enter();\n-\n-  \/\/ lr and fp are already in place\n-  __ sub(sp, rfp, ((unsigned)_framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-  _frame_complete = __ pc() - start;\n-\n-  address the_pc = __ pc();\n-  __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n-  OopMap* map = new OopMap(_framesize, 0);\n-  _oop_maps->add_gc_map(the_pc - start, map);\n-\n-  \/\/ State transition\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n-\n-  rt_call(masm, _call_target);\n-\n-  __ mov(rscratch1, _thread_in_native_trans);\n-  __ strw(rscratch1, Address(rthread, JavaThread::thread_state_offset()));\n-\n-  \/\/ Force this write out before the read below\n-  __ membar(Assembler::LoadLoad | Assembler::LoadStore |\n-            Assembler::StoreLoad | Assembler::StoreStore);\n-\n-  __ verify_sve_vector_length();\n-\n-  Label L_after_safepoint_poll;\n-  Label L_safepoint_poll_slow_path;\n-\n-  __ safepoint_poll(L_safepoint_poll_slow_path, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n-\n-  __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n-  __ cbnzw(rscratch1, L_safepoint_poll_slow_path);\n-\n-  __ bind(L_after_safepoint_poll);\n-\n-  \/\/ change thread state\n-  __ mov(rscratch1, _thread_in_Java);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n-\n-  __ block_comment(\"reguard stack check\");\n-  Label L_reguard;\n-  Label L_after_reguard;\n-  __ ldrb(rscratch1, Address(rthread, JavaThread::stack_guard_state_offset()));\n-  __ cmpw(rscratch1, StackOverflow::stack_guard_yellow_reserved_disabled);\n-  __ br(Assembler::EQ, L_reguard);\n-  __ bind(L_after_reguard);\n-\n-  __ reset_last_Java_frame(true);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(lr);\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n-  __ bind(L_safepoint_poll_slow_path);\n-\n-  \/\/ Need to save the native result registers around any runtime calls.\n-  spill_output_registers();\n-\n-  __ mov(c_rarg0, rthread);\n-  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n-  __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-  __ blr(rscratch1);\n-\n-  fill_output_registers();\n-\n-  __ b(L_after_safepoint_poll);\n-  __ block_comment(\"} L_safepoint_poll_slow_path\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_reguard\");\n-  __ bind(L_reguard);\n-\n-  spill_output_registers();\n-\n-  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n-\n-  fill_output_registers();\n-\n-  __ b(L_after_reguard);\n-\n-  __ block_comment(\"} L_reguard\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ flush();\n-}\n@@ -3339,0 +2900,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":15,"deletions":453,"binary":false,"changes":468,"status":"modified"},{"patch":"@@ -28,0 +28,4 @@\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"logging\/logStream.hpp\"\n@@ -128,0 +132,251 @@\n+\n+\/\/ ---------------------------------------------------------------\n+\n+class NativeInvokerGenerator : public StubCodeGenerator {\n+  BasicType* _signature;\n+  int _num_args;\n+  BasicType _ret_bt;\n+  int _shadow_space_bytes;\n+\n+  const GrowableArray<VMReg>& _input_registers;\n+  const GrowableArray<VMReg>& _output_registers;\n+\n+  int _frame_complete;\n+  int _framesize;\n+  OopMapSet* _oop_maps;\n+public:\n+  NativeInvokerGenerator(CodeBuffer* buffer,\n+                         BasicType* signature,\n+                         int num_args,\n+                         BasicType ret_bt,\n+                         int shadow_space_bytes,\n+                         const GrowableArray<VMReg>& input_registers,\n+                         const GrowableArray<VMReg>& output_registers)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _signature(signature),\n+     _num_args(num_args),\n+     _ret_bt(ret_bt),\n+     _shadow_space_bytes(shadow_space_bytes),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers),\n+     _frame_complete(0),\n+     _framesize(0),\n+     _oop_maps(NULL) {\n+    assert(_output_registers.length() <= 1\n+           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n+  }\n+\n+  void generate();\n+\n+  int frame_complete() const {\n+    return _frame_complete;\n+  }\n+\n+  int framesize() const {\n+    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n+  }\n+\n+  OopMapSet* oop_maps() const {\n+    return _oop_maps;\n+  }\n+\n+private:\n+#ifdef ASSERT\n+  bool target_uses_register(VMReg reg) {\n+    return _input_registers.contains(reg) || _output_registers.contains(reg);\n+  }\n+#endif\n+};\n+\n+static const int native_invoker_code_size = 1024;\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      int shadow_space_bytes,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers) {\n+  int locs_size  = 64;\n+  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n+  NativeInvokerGenerator g(&code, signature, num_args, ret_bt, shadow_space_bytes, input_registers, output_registers);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n+\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n+                                  &code,\n+                                  g.frame_complete(),\n+                                  g.framesize(),\n+                                  g.oop_maps(), false);\n+\n+  if (TraceNativeInvokers) {\n+    stub->print_on(tty);\n+  }\n+\n+  return stub;\n+}\n+\n+void NativeInvokerGenerator::generate() {\n+  \/\/ we can't use rscratch1 because it is r8, and used by the ABI\n+  Register tmp1 = r9;\n+  Register tmp2 = r10;\n+  assert(!target_uses_register(tmp1->as_VMReg()), \"conflict\");\n+  assert(!target_uses_register(tmp2->as_VMReg()), \"conflict\");\n+  assert(!target_uses_register(rthread->as_VMReg()), \"conflict\");\n+\n+  enum layout {\n+    rfp_off,\n+    rfp_off2,\n+    lr_off,\n+    lr_off2,\n+    framesize \/\/ inclusive of return address\n+    \/\/ The following are also computed dynamically:\n+    \/\/ spill area for return value\n+    \/\/ out arg area (e.g. for stack args)\n+  };\n+\n+  Register input_addr_reg = tmp1;\n+  JavaCallConv in_conv;\n+  DowncallNativeCallConv out_conv(_input_registers, input_addr_reg->as_VMReg());\n+  ArgumentShuffle arg_shuffle(_signature, _num_args, _signature, _num_args, &in_conv, &out_conv, r19->as_VMReg());\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  RegSpiller out_reg_spiller(_output_registers);\n+  int spill_offset = 0;\n+\n+  assert(_shadow_space_bytes == 0, \"not expecting shadow space on AArch64\");\n+  _framesize = align_up(framesize\n+    + (out_reg_spiller.spill_size_bytes() >> LogBytesPerInt)\n+    + arg_shuffle.out_arg_stack_slots(), 4);\n+  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n+\n+  _oop_maps  = new OopMapSet();\n+  MacroAssembler* masm = _masm;\n+\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ lr and fp are already in place\n+  __ sub(sp, rfp, ((unsigned)_framesize-4) << LogBytesPerInt); \/\/ prolog\n+\n+  _frame_complete = __ pc() - start;\n+\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(sp, rfp, the_pc, tmp1);\n+  OopMap* map = new OopMap(_framesize, 0);\n+  _oop_maps->add_gc_map(the_pc - start, map);\n+\n+  \/\/ State transition\n+  __ mov(tmp1, _thread_in_native);\n+  __ lea(tmp2, Address(rthread, JavaThread::thread_state_offset()));\n+  __ stlrw(tmp1, tmp2);\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_shuffle.generate(_masm);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ blr(input_addr_reg);\n+\n+  \/\/ Unpack native results.\n+  switch (_ret_bt) {\n+    case T_BOOLEAN: __ c2bool(r0);                     break;\n+    case T_CHAR   : __ ubfx(r0, r0, 0, 16);            break;\n+    case T_BYTE   : __ sbfx(r0, r0, 0, 8);             break;\n+    case T_SHORT  : __ sbfx(r0, r0, 0, 16);            break;\n+    case T_INT    : __ sbfx(r0, r0, 0, 32);            break;\n+    case T_DOUBLE :\n+    case T_FLOAT  :\n+      \/\/ Result is in v0 we'll save as needed\n+      break;\n+    case T_VOID: break;\n+    case T_LONG: break;\n+    default       : ShouldNotReachHere();\n+  }\n+\n+  __ mov(tmp1, _thread_in_native_trans);\n+  __ strw(tmp1, Address(rthread, JavaThread::thread_state_offset()));\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(Assembler::LoadLoad | Assembler::LoadStore |\n+            Assembler::StoreLoad | Assembler::StoreStore);\n+\n+  __ verify_sve_vector_length(tmp1);\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/, tmp1);\n+\n+  __ ldrw(tmp1, Address(rthread, JavaThread::suspend_flags_offset()));\n+  __ cbnzw(tmp1, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ mov(tmp1, _thread_in_Java);\n+  __ lea(tmp2, Address(rthread, JavaThread::thread_state_offset()));\n+  __ stlrw(tmp1, tmp2);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ ldrb(tmp1, Address(rthread, JavaThread::stack_guard_state_offset()));\n+  __ cmpw(tmp1, StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ br(Assembler::EQ, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(true);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(lr);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+\n+  \/\/ Need to save the native result registers around any runtime calls.\n+  out_reg_spiller.generate_spill(_masm, spill_offset);\n+\n+  __ mov(c_rarg0, rthread);\n+  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+  __ lea(tmp1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n+  __ blr(tmp1);\n+\n+  out_reg_spiller.generate_fill(_masm, spill_offset);\n+\n+  __ b(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+\n+  out_reg_spiller.generate_spill(_masm, spill_offset);\n+\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages), tmp1);\n+\n+  out_reg_spiller.generate_fill(_masm, spill_offset);\n+\n+  __ b(L_after_reguard);\n+\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n+}\n+\n+bool ProgrammableInvoker::supports_native_invoker() {\n+  return true;\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/universalNativeInvoker_aarch64.cpp","additions":255,"deletions":0,"binary":false,"changes":255,"status":"modified"},{"patch":"@@ -54,13 +54,0 @@\n-\n-#define INTEGER_TYPE 0\n-#define VECTOR_TYPE 1\n-#define STACK_TYPE 3\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  switch(type) {\n-    case INTEGER_TYPE: return ::as_Register(index)->as_VMReg();\n-    case VECTOR_TYPE: return ::as_FloatRegister(index)->as_VMReg();\n-    case STACK_TYPE: return VMRegImpl::stack2reg(index LP64_ONLY(* 2));\n-  }\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/aarch64\/vmreg_aarch64.cpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -42,0 +42,5 @@\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+}\n","filename":"src\/hotspot\/cpu\/arm\/foreign_globals_arm.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1874,10 +1874,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/arm\/sharedRuntime_arm.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -32,0 +32,14 @@\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      int shadow_space_bytes,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers) {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+bool ProgrammableInvoker::supports_native_invoker() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/arm\/universalNativeInvoker_arm.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -52,5 +52,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  Unimplemented();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/arm\/vmreg_arm.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -44,0 +44,5 @@\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/foreign_globals_ppc.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -310,0 +310,6 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  __ stop(\"Should not reach here\");\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -327,4 +333,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -333,0 +336,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/ppc\/methodHandles_ppc.cpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -62,0 +62,3 @@\n+\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n","filename":"src\/hotspot\/cpu\/ppc\/methodHandles_ppc.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1625,1 +1625,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1627,0 +1627,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = R19_method;  \/\/ known to be free at this point\n@@ -3436,10 +3439,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/ppc\/sharedRuntime_ppc.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -33,0 +33,14 @@\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      int shadow_space_bytes,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers) {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+bool ProgrammableInvoker::supports_native_invoker() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/universalNativeInvoker_ppc.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -51,5 +51,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  Unimplemented();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/ppc\/vmreg_ppc.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -42,0 +42,5 @@\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+}\n","filename":"src\/hotspot\/cpu\/s390\/foreign_globals_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -351,0 +351,6 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  __ should_not_reach_here();\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -364,1 +370,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : Z_ARG1),\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : Z_ARG1),\n@@ -377,4 +383,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -385,0 +388,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/s390\/methodHandles_s390.cpp","additions":11,"deletions":5,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -62,0 +62,3 @@\n+\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n","filename":"src\/hotspot\/cpu\/s390\/methodHandles_s390.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -922,0 +922,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = Z_R9;  \/\/ known to be free at this point\n@@ -923,1 +926,1 @@\n-    guarantee(special_dispatch == vmIntrinsics::_invokeBasic || special_dispatch == vmIntrinsics::_linkToNative,\n+    guarantee(special_dispatch == vmIntrinsics::_invokeBasic,\n@@ -3461,10 +3464,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  Unimplemented();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":4,"deletions":11,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -32,0 +32,14 @@\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      int shadow_space_bytes,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers) {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+bool ProgrammableInvoker::supports_native_invoker() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/s390\/universalNativeInvoker_s390.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -50,5 +50,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  Unimplemented();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/s390\/vmreg_s390.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"oops\/oopCast.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -49,1 +51,1 @@\n-  objArrayOop inputStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.inputStorage_offset));\n+  objArrayOop inputStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.inputStorage_offset));\n@@ -53,1 +55,1 @@\n-  objArrayOop outputStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.outputStorage_offset));\n+  objArrayOop outputStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.outputStorage_offset));\n@@ -56,1 +58,1 @@\n-  objArrayOop subarray = cast<objArrayOop>(outputStorage->obj_at(X87_TYPE));\n+  objArrayOop subarray = oop_cast<objArrayOop>(outputStorage->obj_at(X87_TYPE));\n@@ -59,1 +61,1 @@\n-  objArrayOop volatileStorage = cast<objArrayOop>(abi_oop->obj_field(ABI.volatileStorage_offset));\n+  objArrayOop volatileStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.volatileStorage_offset));\n@@ -77,1 +79,1 @@\n-  typeArrayOop input_offsets = cast<typeArrayOop>(layout_oop->obj_field(BL.input_type_offsets_offset));\n+  typeArrayOop input_offsets = oop_cast<typeArrayOop>(layout_oop->obj_field(BL.input_type_offsets_offset));\n@@ -81,1 +83,1 @@\n-  typeArrayOop output_offsets = cast<typeArrayOop>(layout_oop->obj_field(BL.output_type_offsets_offset));\n+  typeArrayOop output_offsets = oop_cast<typeArrayOop>(layout_oop->obj_field(BL.output_type_offsets_offset));\n@@ -93,2 +95,2 @@\n-  objArrayOop arg_regs_oop = cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.arg_regs_offset));\n-  objArrayOop ret_regs_oop = cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.ret_regs_offset));\n+  objArrayOop arg_regs_oop = oop_cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.arg_regs_offset));\n+  objArrayOop ret_regs_oop = oop_cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.ret_regs_offset));\n@@ -107,1 +109,1 @@\n-    result._arg_regs[i] = VMRegImpl::vmStorageToVMReg(type, index);\n+    result._arg_regs[i] = vmstorage_to_vmreg(type, index);\n@@ -114,1 +116,1 @@\n-    result._ret_regs[i] = VMRegImpl::vmStorageToVMReg(type, index);\n+    result._ret_regs[i] = vmstorage_to_vmreg(type, index);\n@@ -119,0 +121,89 @@\n+\n+enum class RegType {\n+  INTEGER = 0,\n+  VECTOR = 1,\n+  X87 = 2,\n+  STACK = 3\n+};\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  switch(static_cast<RegType>(type)) {\n+    case RegType::INTEGER: return ::as_Register(index)->as_VMReg();\n+    case RegType::VECTOR: return ::as_XMMRegister(index)->as_VMReg();\n+    case RegType::STACK: return VMRegImpl::stack2reg(index LP64_ONLY(* 2)); \/\/ numbering on x64 goes per 64-bits\n+    case RegType::X87: break;\n+  }\n+  return VMRegImpl::Bad();\n+}\n+\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  if (reg->is_Register()) {\n+    return 8;\n+  } else if (reg->is_XMMRegister()) {\n+    return 16;\n+  }\n+  return 0; \/\/ stack and BAD\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->movptr(Address(rsp, offset), reg->as_Register());\n+  } else if (reg->is_XMMRegister()) {\n+    masm->movdqu(Address(rsp, offset), reg->as_XMMRegister());\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->movptr(reg->as_Register(), Address(rsp, offset));\n+  } else if (reg->is_XMMRegister()) {\n+    masm->movdqu(reg->as_XMMRegister(), Address(rsp, offset));\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm) const {\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    masm->block_comment(err_msg(\"bt=%s\", null_safe_string(type2name(arg_bt))));\n+    switch (arg_bt) {\n+      case T_BOOLEAN:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_CHAR:\n+      case T_INT:\n+        masm->move32_64(from_vmreg, to_vmreg);\n+        break;\n+\n+      case T_FLOAT:\n+        if (to_vmreg.first()->is_Register()) { \/\/ Windows vararg call\n+          masm->movq(to_vmreg.first()->as_Register(), from_vmreg.first()->as_XMMRegister());\n+        } else {\n+          masm->float_move(from_vmreg, to_vmreg);\n+        }\n+        break;\n+\n+      case T_DOUBLE:\n+        if (to_vmreg.first()->is_Register()) { \/\/ Windows vararg call\n+          masm->movq(to_vmreg.first()->as_Register(), from_vmreg.first()->as_XMMRegister());\n+        } else {\n+          masm->double_move(from_vmreg, to_vmreg);\n+        }\n+        break;\n+\n+      case T_LONG:\n+        masm->long_move(from_vmreg, to_vmreg);\n+        break;\n+\n+      default:\n+        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86.cpp","additions":101,"deletions":10,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -30,0 +30,2 @@\n+class outputStream;\n+\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -941,1 +941,2 @@\n-      assert(dst.is_single_reg(), \"not a stack pair\");\n+      assert(dst.is_single_reg(), \"not a stack pair: (%s, %s), (%s, %s)\",\n+       src.first()->name(), src.second()->name(), dst.first()->name(), dst.second()->name());\n@@ -946,1 +947,1 @@\n-    movq(dst.first()->as_Register(), Address(rbp, reg2offset_out(src.first())));\n+    movq(dst.first()->as_Register(), Address(rbp, reg2offset_in(src.first())));\n@@ -972,1 +973,1 @@\n-    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_out(src.first())));\n+    movdbl(dst.first()->as_XMMRegister(), Address(rbp, reg2offset_in(src.first())));\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -206,0 +206,15 @@\n+void MethodHandles::jump_to_native_invoker(MacroAssembler* _masm, Register nep_reg, Register temp_target) {\n+  BLOCK_COMMENT(\"jump_to_native_invoker {\");\n+  assert_different_registers(nep_reg, temp_target);\n+  assert(nep_reg != noreg, \"required register\");\n+\n+  \/\/ Load the invoker, as NEP -> .invoker\n+  __ verify_oop(nep_reg);\n+  __ access_load_at(T_ADDRESS, IN_HEAP, temp_target,\n+                    Address(nep_reg, NONZERO(jdk_internal_invoke_NativeEntryPoint::invoker_offset_in_bytes())),\n+                    noreg, noreg);\n+\n+  __ jmp(temp_target);\n+  BLOCK_COMMENT(\"} jump_to_native_invoker\");\n+}\n+\n@@ -317,1 +332,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : j_rarg0), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : j_rarg0), \"only valid assignment\");\n@@ -327,1 +342,1 @@\n-    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic ? noreg : rcx), \"only valid assignment\");\n+    assert(receiver_reg == (iid == vmIntrinsics::_linkToStatic || iid == vmIntrinsics::_linkToNative ? noreg : rcx), \"only valid assignment\");\n@@ -339,4 +354,1 @@\n-  if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n-    if (iid == vmIntrinsics::_linkToNative) {\n-      assert(for_compiler_entry, \"only compiler entry is supported\");\n-    }\n+  if (iid == vmIntrinsics::_invokeBasic) {\n@@ -345,1 +357,3 @@\n-\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    assert(for_compiler_entry, \"only compiler entry is supported\");\n+    jump_to_native_invoker(_masm, member_reg, temp1);\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":21,"deletions":7,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -58,0 +58,3 @@\n+  static void jump_to_native_invoker(MacroAssembler* _masm,\n+                                     Register nep_reg, Register temp);\n+\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2987,10 +2987,0 @@\n-\n-#ifdef COMPILER2\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  ShouldNotCallThis();\n-  return nullptr;\n-}\n-#endif\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"compiler\/disassembler.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -1490,1 +1492,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1492,0 +1494,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = rbx;  \/\/ known to be free at this point\n@@ -3242,257 +3247,0 @@\n-#ifdef COMPILER2\n-static const int native_invoker_code_size = MethodHandles::adapter_code_size;\n-\n-class NativeInvokerGenerator : public StubCodeGenerator {\n-  address _call_target;\n-  int _shadow_space_bytes;\n-\n-  const GrowableArray<VMReg>& _input_registers;\n-  const GrowableArray<VMReg>& _output_registers;\n-\n-  int _frame_complete;\n-  int _framesize;\n-  OopMapSet* _oop_maps;\n-public:\n-  NativeInvokerGenerator(CodeBuffer* buffer,\n-                         address call_target,\n-                         int shadow_space_bytes,\n-                         const GrowableArray<VMReg>& input_registers,\n-                         const GrowableArray<VMReg>& output_registers)\n-   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n-     _call_target(call_target),\n-     _shadow_space_bytes(shadow_space_bytes),\n-     _input_registers(input_registers),\n-     _output_registers(output_registers),\n-     _frame_complete(0),\n-     _framesize(0),\n-     _oop_maps(NULL) {\n-    assert(_output_registers.length() <= 1\n-           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n-\n-  }\n-\n-  void generate();\n-\n-  int spill_size_in_bytes() const {\n-    if (_output_registers.length() == 0) {\n-      return 0;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    if (reg->is_Register()) {\n-      return 8;\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        return 64;\n-      } else if (UseAVX >= 1) {\n-        return 32;\n-      } else {\n-        return 16;\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-\n-  void spill_out_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ movptr(Address(rsp, 0), reg->as_Register());\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        __ evmovdqul(Address(rsp, 0), reg->as_XMMRegister(), Assembler::AVX_512bit);\n-      } else if (UseAVX >= 1) {\n-        __ vmovdqu(Address(rsp, 0), reg->as_XMMRegister());\n-      } else {\n-        __ movdqu(Address(rsp, 0), reg->as_XMMRegister());\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  void fill_out_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ movptr(reg->as_Register(), Address(rsp, 0));\n-    } else if (reg->is_XMMRegister()) {\n-      if (UseAVX >= 3) {\n-        __ evmovdqul(reg->as_XMMRegister(), Address(rsp, 0), Assembler::AVX_512bit);\n-      } else if (UseAVX >= 1) {\n-        __ vmovdqu(reg->as_XMMRegister(), Address(rsp, 0));\n-      } else {\n-        __ movdqu(reg->as_XMMRegister(), Address(rsp, 0));\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  int frame_complete() const {\n-    return _frame_complete;\n-  }\n-\n-  int framesize() const {\n-    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n-  }\n-\n-  OopMapSet* oop_maps() const {\n-    return _oop_maps;\n-  }\n-\n-private:\n-#ifdef ASSERT\n-bool target_uses_register(VMReg reg) {\n-  return _input_registers.contains(reg) || _output_registers.contains(reg);\n-}\n-#endif\n-};\n-\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  int locs_size  = 64;\n-  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n-  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n-  g.generate();\n-  code.log_section_sizes(\"nep_invoker_blob\");\n-\n-  RuntimeStub* stub =\n-    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n-                                  &code,\n-                                  g.frame_complete(),\n-                                  g.framesize(),\n-                                  g.oop_maps(), false);\n-  return stub;\n-}\n-\n-void NativeInvokerGenerator::generate() {\n-  assert(!(target_uses_register(r15_thread->as_VMReg()) || target_uses_register(rscratch1->as_VMReg())), \"Register conflict\");\n-\n-  enum layout {\n-    rbp_off,\n-    rbp_off2,\n-    return_off,\n-    return_off2,\n-    framesize \/\/ inclusive of return address\n-  };\n-\n-  _framesize = align_up(framesize + ((_shadow_space_bytes + spill_size_in_bytes()) >> LogBytesPerInt), 4);\n-  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n-\n-  _oop_maps  = new OopMapSet();\n-  MacroAssembler* masm = _masm;\n-\n-  address start = __ pc();\n-\n-  __ enter();\n-\n-  \/\/ return address and rbp are already in place\n-  __ subptr(rsp, (_framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-  _frame_complete = __ pc() - start;\n-\n-  address the_pc = __ pc();\n-\n-  __ set_last_Java_frame(rsp, rbp, (address)the_pc);\n-  OopMap* map = new OopMap(_framesize, 0);\n-  _oop_maps->add_gc_map(the_pc - start, map);\n-\n-  \/\/ State transition\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n-\n-  __ call(RuntimeAddress(_call_target));\n-\n-  __ restore_cpu_control_state_after_jni();\n-\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native_trans);\n-\n-  \/\/ Force this write out before the read below\n-  __ membar(Assembler::Membar_mask_bits(\n-          Assembler::LoadLoad | Assembler::LoadStore |\n-          Assembler::StoreLoad | Assembler::StoreStore));\n-\n-  Label L_after_safepoint_poll;\n-  Label L_safepoint_poll_slow_path;\n-\n-  __ safepoint_poll(L_safepoint_poll_slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n-  __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n-  __ jcc(Assembler::notEqual, L_safepoint_poll_slow_path);\n-\n-  __ bind(L_after_safepoint_poll);\n-\n-  \/\/ change thread state\n-  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_Java);\n-\n-  __ block_comment(\"reguard stack check\");\n-  Label L_reguard;\n-  Label L_after_reguard;\n-  __ cmpl(Address(r15_thread, JavaThread::stack_guard_state_offset()), StackOverflow::stack_guard_yellow_reserved_disabled);\n-  __ jcc(Assembler::equal, L_reguard);\n-  __ bind(L_after_reguard);\n-\n-  __ reset_last_Java_frame(r15_thread, true);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(0);\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n-  __ bind(L_safepoint_poll_slow_path);\n-  __ vzeroupper();\n-\n-  spill_out_registers();\n-\n-  __ mov(c_rarg0, r15_thread);\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  fill_out_registers();\n-\n-  __ jmp(L_after_safepoint_poll);\n-  __ block_comment(\"} L_safepoint_poll_slow_path\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_reguard\");\n-  __ bind(L_reguard);\n-  __ vzeroupper();\n-\n-  spill_out_registers();\n-\n-  __ mov(r12, rsp); \/\/ remember sp\n-  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n-  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n-  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n-  __ mov(rsp, r12); \/\/ restore sp\n-  __ reinit_heapbase();\n-\n-  fill_out_registers();\n-\n-  __ jmp(L_after_reguard);\n-\n-  __ block_comment(\"} L_reguard\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ flush();\n-}\n-#endif \/\/ COMPILER2\n-\n@@ -3896,9 +3644,0 @@\n-void SharedRuntime::compute_move_order(const BasicType* in_sig_bt,\n-                                       int total_in_args, const VMRegPair* in_regs,\n-                                       int total_out_args, VMRegPair* out_regs,\n-                                       GrowableArray<int>& arg_order,\n-                                       VMRegPair tmp_vmreg) {\n-  ComputeMoveOrder order(total_in_args, in_regs,\n-                         total_out_args, out_regs,\n-                         in_sig_bt, arg_order, tmp_vmreg);\n-}\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":6,"deletions":267,"binary":false,"changes":273,"status":"modified"},{"patch":"@@ -35,0 +35,14 @@\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      int shadow_space_bytes,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers) {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+bool ProgrammableInvoker::supports_native_invoker() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/universalNativeInvoker_x86_32.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -28,0 +29,1 @@\n+#include \"prims\/foreign_globals.inline.hpp\"\n@@ -29,0 +31,2 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n@@ -148,0 +152,262 @@\n+\n+static const int native_invoker_code_size = 1024;\n+\n+class NativeInvokerGenerator : public StubCodeGenerator {\n+  BasicType* _signature;\n+  int _num_args;\n+  BasicType _ret_bt;\n+  int _shadow_space_bytes;\n+\n+  const GrowableArray<VMReg>& _input_registers;\n+  const GrowableArray<VMReg>& _output_registers;\n+\n+  int _frame_complete;\n+  int _framesize;\n+  OopMapSet* _oop_maps;\n+public:\n+  NativeInvokerGenerator(CodeBuffer* buffer,\n+                         BasicType* signature,\n+                         int num_args,\n+                         BasicType ret_bt,\n+                         int shadow_space_bytes,\n+                         const GrowableArray<VMReg>& input_registers,\n+                         const GrowableArray<VMReg>& output_registers)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _signature(signature),\n+     _num_args(num_args),\n+     _ret_bt(ret_bt),\n+     _shadow_space_bytes(shadow_space_bytes),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers),\n+     _frame_complete(0),\n+     _framesize(0),\n+     _oop_maps(NULL) {\n+    assert(_output_registers.length() <= 1\n+           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n+\n+  }\n+\n+  void generate();\n+\n+  int frame_complete() const {\n+    return _frame_complete;\n+  }\n+\n+  int framesize() const {\n+    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n+  }\n+\n+  OopMapSet* oop_maps() const {\n+    return _oop_maps;\n+  }\n+\n+private:\n+#ifdef ASSERT\n+bool target_uses_register(VMReg reg) {\n+  return _input_registers.contains(reg) || _output_registers.contains(reg);\n+}\n+#endif\n+};\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      int shadow_space_bytes,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers) {\n+  int locs_size  = 64;\n+  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n+  NativeInvokerGenerator g(&code, signature, num_args, ret_bt, shadow_space_bytes, input_registers, output_registers);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n+\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n+                                  &code,\n+                                  g.frame_complete(),\n+                                  g.framesize(),\n+                                  g.oop_maps(), false);\n+\n+  if (TraceNativeInvokers) {\n+    stub->print_on(tty);\n+  }\n+\n+  return stub;\n+}\n+\n+void NativeInvokerGenerator::generate() {\n+  assert(!(target_uses_register(r15_thread->as_VMReg()) || target_uses_register(rscratch1->as_VMReg())), \"Register conflict\");\n+\n+  enum layout {\n+    rbp_off,\n+    rbp_off2,\n+    return_off,\n+    return_off2,\n+    framesize_base \/\/ inclusive of return address\n+    \/\/ The following are also computed dynamically:\n+    \/\/ shadow space\n+    \/\/ spill area\n+    \/\/ out arg area (e.g. for stack args)\n+  };\n+\n+  Register input_addr_reg = rscratch1;\n+  JavaCallConv in_conv;\n+  DowncallNativeCallConv out_conv(_input_registers, input_addr_reg->as_VMReg());\n+  ArgumentShuffle arg_shuffle(_signature, _num_args, _signature, _num_args, &in_conv, &out_conv, rbx->as_VMReg());\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ in bytes\n+  int allocated_frame_size = 0;\n+  allocated_frame_size += arg_shuffle.out_arg_stack_slots() << LogBytesPerInt;\n+  allocated_frame_size += _shadow_space_bytes;\n+\n+  RegSpiller out_reg_spiller(_output_registers);\n+  int spill_rsp_offset = 0;\n+\n+  \/\/ spill area can be shared with the above, so we take the max of the 2\n+  allocated_frame_size = out_reg_spiller.spill_size_bytes() > allocated_frame_size\n+    ? out_reg_spiller.spill_size_bytes()\n+    : allocated_frame_size;\n+  allocated_frame_size = align_up(allocated_frame_size, 16);\n+  \/\/ _framesize is in 32-bit stack slots:\n+  _framesize += framesize_base + (allocated_frame_size >> LogBytesPerInt);\n+  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n+\n+  _oop_maps  = new OopMapSet();\n+  MacroAssembler* masm = _masm;\n+\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ return address and rbp are already in place\n+  __ subptr(rsp, allocated_frame_size); \/\/ prolog\n+\n+  _frame_complete = __ pc() - start;\n+\n+  address the_pc = __ pc();\n+\n+  __ block_comment(\"{ thread java2native\");\n+  __ set_last_Java_frame(rsp, rbp, (address)the_pc);\n+  OopMap* map = new OopMap(_framesize, 0);\n+  _oop_maps->add_gc_map(the_pc - start, map);\n+\n+  \/\/ State transition\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  __ block_comment(\"} thread java2native\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_shuffle.generate(_masm);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ call(input_addr_reg);\n+\n+  \/\/ Unpack native results.\n+  switch (_ret_bt) {\n+    case T_BOOLEAN: __ c2bool(rax);            break;\n+    case T_CHAR   : __ movzwl(rax, rax);       break;\n+    case T_BYTE   : __ sign_extend_byte (rax); break;\n+    case T_SHORT  : __ sign_extend_short(rax); break;\n+    case T_INT    : \/* nothing to do *\/        break;\n+    case T_DOUBLE :\n+    case T_FLOAT  :\n+      \/\/ Result is in xmm0 we'll save as needed\n+      break;\n+    case T_VOID: break;\n+    case T_LONG: break;\n+    default       : ShouldNotReachHere();\n+  }\n+\n+  __ block_comment(\"{ thread native2java\");\n+  __ restore_cpu_control_state_after_jni();\n+\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native_trans);\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(Assembler::Membar_mask_bits(\n+          Assembler::LoadLoad | Assembler::LoadStore |\n+          Assembler::StoreLoad | Assembler::StoreStore));\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n+  __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n+  __ jcc(Assembler::notEqual, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_Java);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ cmpl(Address(r15_thread, JavaThread::stack_guard_state_offset()), StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ jcc(Assembler::equal, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(r15_thread, true);\n+  __ block_comment(\"} thread native2java\");\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+  __ vzeroupper();\n+\n+  out_reg_spiller.generate_spill(_masm, spill_rsp_offset);\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(r12, rsp); \/\/ remember sp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n+  __ mov(rsp, r12); \/\/ restore sp\n+  __ reinit_heapbase();\n+\n+  out_reg_spiller.generate_fill(_masm, spill_rsp_offset);\n+\n+  __ jmp(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+  __ vzeroupper();\n+\n+  out_reg_spiller.generate_spill(_masm, spill_rsp_offset);\n+\n+  __ mov(r12, rsp); \/\/ remember sp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n+  __ mov(rsp, r12); \/\/ restore sp\n+  __ reinit_heapbase();\n+\n+  out_reg_spiller.generate_fill(_masm, spill_rsp_offset);\n+\n+  __ jmp(L_after_reguard);\n+\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n+}\n+\n+bool ProgrammableInvoker::supports_native_invoker() {\n+  return true;\n+}\n","filename":"src\/hotspot\/cpu\/x86\/universalNativeInvoker_x86_64.cpp","additions":266,"deletions":0,"binary":false,"changes":266,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"prims\/foreign_globals.inline.hpp\"\n@@ -154,201 +155,0 @@\n-struct ArgMove {\n-  BasicType bt;\n-  VMRegPair from;\n-  VMRegPair to;\n-\n-  bool is_identity() const {\n-      return from.first() == to.first() && from.second() == to.second();\n-  }\n-};\n-\n-static GrowableArray<ArgMove> compute_argument_shuffle(Method* entry, int& out_arg_size_bytes, const CallRegs& conv, BasicType& ret_type) {\n-  assert(entry->is_static(), \"\");\n-\n-  \/\/ Fill in the signature array, for the calling-convention call.\n-  const int total_out_args = entry->size_of_parameters();\n-  assert(total_out_args > 0, \"receiver arg \");\n-\n-  BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_out_args);\n-  VMRegPair* out_regs = NEW_RESOURCE_ARRAY(VMRegPair, total_out_args);\n-\n-  {\n-    int i = 0;\n-    SignatureStream ss(entry->signature());\n-    for (; !ss.at_return_type(); ss.next()) {\n-      out_sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n-      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)\n-        out_sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n-    }\n-    assert(i == total_out_args, \"\");\n-    ret_type = ss.type();\n-  }\n-\n-  int out_arg_slots = SharedRuntime::java_calling_convention(out_sig_bt, out_regs, total_out_args);\n-\n-  const int total_in_args = total_out_args - 1; \/\/ skip receiver\n-  BasicType* in_sig_bt  = NEW_RESOURCE_ARRAY(BasicType, total_in_args);\n-  VMRegPair* in_regs    = NEW_RESOURCE_ARRAY(VMRegPair, total_in_args);\n-\n-  for (int i = 0; i < total_in_args ; i++ ) {\n-    in_sig_bt[i] = out_sig_bt[i+1]; \/\/ skip receiver\n-  }\n-\n-  \/\/ Now figure out where the args must be stored and how much stack space they require.\n-  conv.calling_convention(in_sig_bt, in_regs, total_in_args);\n-\n-  GrowableArray<int> arg_order(2 * total_in_args);\n-\n-  VMRegPair tmp_vmreg;\n-  tmp_vmreg.set2(rbx->as_VMReg());\n-\n-  \/\/ Compute a valid move order, using tmp_vmreg to break any cycles\n-  SharedRuntime::compute_move_order(in_sig_bt,\n-                                    total_in_args, in_regs,\n-                                    total_out_args, out_regs,\n-                                    arg_order,\n-                                    tmp_vmreg);\n-\n-  GrowableArray<ArgMove> arg_order_vmreg(total_in_args); \/\/ conservative\n-\n-#ifdef ASSERT\n-  bool reg_destroyed[RegisterImpl::number_of_registers];\n-  bool freg_destroyed[XMMRegisterImpl::number_of_registers];\n-  for ( int r = 0 ; r < RegisterImpl::number_of_registers ; r++ ) {\n-    reg_destroyed[r] = false;\n-  }\n-  for ( int f = 0 ; f < XMMRegisterImpl::number_of_registers ; f++ ) {\n-    freg_destroyed[f] = false;\n-  }\n-#endif \/\/ ASSERT\n-\n-  for (int i = 0; i < arg_order.length(); i += 2) {\n-    int in_arg  = arg_order.at(i);\n-    int out_arg = arg_order.at(i + 1);\n-\n-    assert(in_arg != -1 || out_arg != -1, \"\");\n-    BasicType arg_bt = (in_arg != -1 ? in_sig_bt[in_arg] : out_sig_bt[out_arg]);\n-    switch (arg_bt) {\n-      case T_BOOLEAN:\n-      case T_BYTE:\n-      case T_SHORT:\n-      case T_CHAR:\n-      case T_INT:\n-      case T_FLOAT:\n-        break; \/\/ process\n-\n-      case T_LONG:\n-      case T_DOUBLE:\n-        assert(in_arg  == -1 || (in_arg  + 1 < total_in_args  &&  in_sig_bt[in_arg  + 1] == T_VOID), \"bad arg list: %d\", in_arg);\n-        assert(out_arg == -1 || (out_arg + 1 < total_out_args && out_sig_bt[out_arg + 1] == T_VOID), \"bad arg list: %d\", out_arg);\n-        break; \/\/ process\n-\n-      case T_VOID:\n-        continue; \/\/ skip\n-\n-      default:\n-        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n-    }\n-\n-    ArgMove move;\n-    move.bt   = arg_bt;\n-    move.from = (in_arg != -1 ? in_regs[in_arg] : tmp_vmreg);\n-    move.to   = (out_arg != -1 ? out_regs[out_arg] : tmp_vmreg);\n-\n-    if(move.is_identity()) {\n-      continue; \/\/ useless move\n-    }\n-\n-#ifdef ASSERT\n-    if (in_arg != -1) {\n-      if (in_regs[in_arg].first()->is_Register()) {\n-        assert(!reg_destroyed[in_regs[in_arg].first()->as_Register()->encoding()], \"destroyed reg!\");\n-      } else if (in_regs[in_arg].first()->is_XMMRegister()) {\n-        assert(!freg_destroyed[in_regs[in_arg].first()->as_XMMRegister()->encoding()], \"destroyed reg!\");\n-      }\n-    }\n-    if (out_arg != -1) {\n-      if (out_regs[out_arg].first()->is_Register()) {\n-        reg_destroyed[out_regs[out_arg].first()->as_Register()->encoding()] = true;\n-      } else if (out_regs[out_arg].first()->is_XMMRegister()) {\n-        freg_destroyed[out_regs[out_arg].first()->as_XMMRegister()->encoding()] = true;\n-      }\n-    }\n-#endif \/* ASSERT *\/\n-\n-    arg_order_vmreg.push(move);\n-  }\n-\n-  int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;\n-  out_arg_size_bytes = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n-\n-  return arg_order_vmreg;\n-}\n-\n-static const char* null_safe_string(const char* str) {\n-  return str == nullptr ? \"NULL\" : str;\n-}\n-\n-#ifdef ASSERT\n-static void print_arg_moves(const GrowableArray<ArgMove>& arg_moves, Method* entry) {\n-  LogTarget(Trace, foreign) lt;\n-  if (lt.is_enabled()) {\n-    ResourceMark rm;\n-    LogStream ls(lt);\n-    ls.print_cr(\"Argument shuffle for %s {\", entry->name_and_sig_as_C_string());\n-    for (int i = 0; i < arg_moves.length(); i++) {\n-      ArgMove arg_mv = arg_moves.at(i);\n-      BasicType arg_bt     = arg_mv.bt;\n-      VMRegPair from_vmreg = arg_mv.from;\n-      VMRegPair to_vmreg   = arg_mv.to;\n-\n-      ls.print(\"Move a %s from (\", null_safe_string(type2name(arg_bt)));\n-      from_vmreg.first()->print_on(&ls);\n-      ls.print(\",\");\n-      from_vmreg.second()->print_on(&ls);\n-      ls.print(\") to \");\n-      to_vmreg.first()->print_on(&ls);\n-      ls.print(\",\");\n-      to_vmreg.second()->print_on(&ls);\n-      ls.print_cr(\")\");\n-    }\n-    ls.print_cr(\"}\");\n-  }\n-}\n-#endif\n-\n-static void save_native_arguments(MacroAssembler* _masm, const CallRegs& conv, int arg_save_area_offset) {\n-  __ block_comment(\"{ save_native_args \");\n-  int store_offset = arg_save_area_offset;\n-  for (int i = 0; i < conv._args_length; i++) {\n-    VMReg reg = conv._arg_regs[i];\n-    if (reg->is_Register()) {\n-      __ movptr(Address(rsp, store_offset), reg->as_Register());\n-      store_offset += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      __ movdqu(Address(rsp, store_offset), reg->as_XMMRegister());\n-      store_offset += 16;\n-    }\n-    \/\/ do nothing for stack\n-  }\n-  __ block_comment(\"} save_native_args \");\n-}\n-\n-static void restore_native_arguments(MacroAssembler* _masm, const CallRegs& conv, int arg_save_area_offset) {\n-  __ block_comment(\"{ restore_native_args \");\n-  int load_offset = arg_save_area_offset;\n-  for (int i = 0; i < conv._args_length; i++) {\n-    VMReg reg = conv._arg_regs[i];\n-    if (reg->is_Register()) {\n-      __ movptr(reg->as_Register(), Address(rsp, load_offset));\n-      load_offset += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      __ movdqu(reg->as_XMMRegister(), Address(rsp, load_offset));\n-      load_offset += 16;\n-    }\n-    \/\/ do nothing for stack\n-  }\n-  __ block_comment(\"} restore_native_args \");\n-}\n-\n@@ -389,69 +189,0 @@\n-static int compute_arg_save_area_size(const CallRegs& conv) {\n-  int result_size = 0;\n-  for (int i = 0; i < conv._args_length; i++) {\n-    VMReg reg = conv._arg_regs[i];\n-    if (reg->is_Register()) {\n-      result_size += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      result_size += 16;\n-    }\n-    \/\/ do nothing for stack\n-  }\n-  return result_size;\n-}\n-\n-static int compute_res_save_area_size(const CallRegs& conv) {\n-  int result_size = 0;\n-  for (int i = 0; i < conv._rets_length; i++) {\n-    VMReg reg = conv._ret_regs[i];\n-    if (reg->is_Register()) {\n-      result_size += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      result_size += 16;\n-    } else {\n-      ShouldNotReachHere(); \/\/ unhandled type\n-    }\n-  }\n-  return result_size;\n-}\n-\n-static void save_java_result(MacroAssembler* _masm, const CallRegs& conv, int res_save_area_offset) {\n-  int offset = res_save_area_offset;\n-  __ block_comment(\"{ save java result \");\n-  for (int i = 0; i < conv._rets_length; i++) {\n-    VMReg reg = conv._ret_regs[i];\n-    if (reg->is_Register()) {\n-      __ movptr(Address(rsp, offset), reg->as_Register());\n-      offset += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      __ movdqu(Address(rsp, offset), reg->as_XMMRegister());\n-      offset += 16;\n-    } else {\n-      ShouldNotReachHere(); \/\/ unhandled type\n-    }\n-  }\n-  __ block_comment(\"} save java result \");\n-}\n-\n-static void restore_java_result(MacroAssembler* _masm, const CallRegs& conv, int res_save_area_offset) {\n-  int offset = res_save_area_offset;\n-  __ block_comment(\"{ restore java result \");\n-  for (int i = 0; i < conv._rets_length; i++) {\n-    VMReg reg = conv._ret_regs[i];\n-    if (reg->is_Register()) {\n-      __ movptr(reg->as_Register(), Address(rsp, offset));\n-      offset += 8;\n-    } else if (reg->is_XMMRegister()) {\n-      \/\/ Java API doesn't support vector args\n-      __ movdqu(reg->as_XMMRegister(), Address(rsp, offset));\n-      offset += 16;\n-    } else {\n-      ShouldNotReachHere(); \/\/ unhandled type\n-    }\n-  }\n-  __ block_comment(\"} restore java result \");\n-}\n-\n@@ -549,41 +280,0 @@\n-\n-static void shuffle_arguments(MacroAssembler* _masm, const GrowableArray<ArgMove>& arg_moves) {\n-  for (int i = 0; i < arg_moves.length(); i++) {\n-    ArgMove arg_mv = arg_moves.at(i);\n-    BasicType arg_bt     = arg_mv.bt;\n-    VMRegPair from_vmreg = arg_mv.from;\n-    VMRegPair to_vmreg   = arg_mv.to;\n-\n-    assert(\n-      !((from_vmreg.first()->is_Register() && to_vmreg.first()->is_XMMRegister())\n-      || (from_vmreg.first()->is_XMMRegister() && to_vmreg.first()->is_Register())),\n-       \"move between gp and fp reg not supported\");\n-\n-    __ block_comment(err_msg(\"bt=%s\", null_safe_string(type2name(arg_bt))));\n-    switch (arg_bt) {\n-      case T_BOOLEAN:\n-      case T_BYTE:\n-      case T_SHORT:\n-      case T_CHAR:\n-      case T_INT:\n-       __ move32_64(from_vmreg, to_vmreg);\n-       break;\n-\n-      case T_FLOAT:\n-        __ float_move(from_vmreg, to_vmreg);\n-        break;\n-\n-      case T_DOUBLE:\n-        __ double_move(from_vmreg, to_vmreg);\n-        break;\n-\n-      case T_LONG :\n-        __ long_move(from_vmreg, to_vmreg);\n-        break;\n-\n-      default:\n-        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n-    }\n-  }\n-}\n-\n@@ -597,2 +287,2 @@\n-  const CallRegs conv = ForeignGlobals::parse_call_regs(jconv);\n-  assert(conv._rets_length <= 1, \"no multi reg returns\");\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n+  assert(call_regs._rets_length <= 1, \"no multi reg returns\");\n@@ -604,1 +294,6 @@\n-  int out_arg_area = -1;\n+  assert(entry->is_static(), \"static only\");\n+  \/\/ Fill in the signature array, for the calling-convention call.\n+  const int total_out_args = entry->size_of_parameters();\n+  assert(total_out_args > 0, \"receiver arg\");\n+\n+  BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_out_args);\n@@ -606,3 +301,28 @@\n-  GrowableArray<ArgMove> arg_moves = compute_argument_shuffle(entry, out_arg_area, conv, ret_type);\n-  assert(out_arg_area != -1, \"Should have been set\");\n-  DEBUG_ONLY(print_arg_moves(arg_moves, entry);)\n+  {\n+    int i = 0;\n+    SignatureStream ss(entry->signature());\n+    for (; !ss.at_return_type(); ss.next()) {\n+      out_sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)\n+        out_sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+    }\n+    assert(i == total_out_args, \"\");\n+    ret_type = ss.type();\n+  }\n+  \/\/ skip receiver\n+  BasicType* in_sig_bt = out_sig_bt + 1;\n+  int total_in_args = total_out_args - 1;\n+\n+  JavaCallConv out_conv;\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &call_regs, &out_conv, rbx->as_VMReg());\n+  int stack_slots = SharedRuntime::out_preserve_stack_slots() + arg_shuffle.out_arg_stack_slots();\n+  int out_arg_area = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n@@ -617,2 +337,2 @@\n-  int arg_save_area_size = compute_arg_save_area_size(conv);\n-  int res_save_area_size = compute_res_save_area_size(conv);\n+  RegSpiller arg_spilller(call_regs._arg_regs, call_regs._args_length);\n+  RegSpiller result_spiller(call_regs._ret_regs, call_regs._rets_length);\n@@ -625,2 +345,2 @@\n-  int arg_save_area_offset   = res_save_area_offset   + res_save_area_size;\n-  int reg_save_area_offset   = arg_save_area_offset   + arg_save_area_size;\n+  int arg_save_area_offset   = res_save_area_offset   + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset   = arg_save_area_offset   + arg_spilller.spill_size_bytes();\n@@ -672,1 +392,1 @@\n-  save_native_arguments(_masm, conv, arg_save_area_offset);\n+  arg_spilller.generate_spill(_masm, arg_save_area_offset);\n@@ -686,3 +406,2 @@\n-  \/\/ TODO merge these somehow\n-  restore_native_arguments(_masm, conv, arg_save_area_offset);\n-  shuffle_arguments(_masm, arg_moves);\n+  arg_spilller.generate_fill(_masm, arg_save_area_offset);\n+  arg_shuffle.generate(_masm);\n@@ -702,1 +421,1 @@\n-  save_java_result(_masm, conv, res_save_area_offset);\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n@@ -714,1 +433,1 @@\n-  restore_java_result(_masm, conv, res_save_area_offset);\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n@@ -718,1 +437,1 @@\n-  if (conv._rets_length == 1) { \/\/ 0 or 1\n+  if (call_regs._rets_length == 1) { \/\/ 0 or 1\n@@ -738,2 +457,2 @@\n-    assert(conv._ret_regs[0] == j_expected_result_reg,\n-     \"unexpected result register: %s != %s\", conv._ret_regs[0]->name(), j_expected_result_reg->name());\n+    assert(call_regs._ret_regs[0] == j_expected_result_reg,\n+     \"unexpected result register: %s != %s\", call_regs._ret_regs[0]->name(), j_expected_result_reg->name());\n","filename":"src\/hotspot\/cpu\/x86\/universalUpcallHandler_x86_64.cpp","additions":49,"deletions":330,"binary":false,"changes":379,"status":"modified"},{"patch":"@@ -69,15 +69,0 @@\n-\n-#define INTEGER_TYPE 0\n-#define VECTOR_TYPE 1\n-#define X87_TYPE 2\n-#define STACK_TYPE 3\n-\n-\/\/TODO: Case for KRegisters\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  switch(type) {\n-    case INTEGER_TYPE: return ::as_Register(index)->as_VMReg();\n-    case VECTOR_TYPE: return ::as_XMMRegister(index)->as_VMReg();\n-    case STACK_TYPE: return VMRegImpl::stack2reg(index LP64_ONLY(* 2)); \/\/ numbering on x64 goes per 64-bits\n-  }\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/x86\/vmreg_x86.cpp","additions":0,"deletions":15,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -42,0 +42,5 @@\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+}\n","filename":"src\/hotspot\/cpu\/zero\/foreign_globals_zero.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -31,0 +31,14 @@\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      int shadow_space_bytes,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers) {\n+  Unimplemented();\n+  return nullptr;\n+}\n+\n+bool ProgrammableInvoker::supports_native_invoker() {\n+  return false;\n+}\n","filename":"src\/hotspot\/cpu\/zero\/universalNativeInvoker_zero.cpp","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -64,5 +64,0 @@\n-\n-VMReg VMRegImpl::vmStorageToVMReg(int type, int index) {\n-  ShouldNotCallThis();\n-  return VMRegImpl::Bad();\n-}\n","filename":"src\/hotspot\/cpu\/zero\/vmreg_zero.cpp","additions":0,"deletions":5,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1026,2 +1026,1 @@\n-                            RTMState  rtm_state,\n-                            const GrowableArrayView<RuntimeStub*>& native_invokers) {\n+                            RTMState  rtm_state) {\n@@ -1116,2 +1115,1 @@\n-                               compiler, task()->comp_level(),\n-                               native_invokers);\n+                               compiler, task()->comp_level());\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":2,"deletions":4,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -382,2 +382,1 @@\n-                       RTMState                  rtm_state = NoRTM,\n-                       const GrowableArrayView<RuntimeStub*>& native_invokers = GrowableArrayView<RuntimeStub*>::EMPTY);\n+                       RTMState                  rtm_state = NoRTM);\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -3866,0 +3866,1 @@\n+int jdk_internal_invoke_NativeEntryPoint::_invoker_offset;\n@@ -3873,1 +3874,2 @@\n-  macro(_name_offset,            k, \"name\",           string_signature, false);\n+  macro(_name_offset,            k, \"name\",           string_signature, false); \\\n+  macro(_invoker_offset,         k, \"invoker\",        long_signature, false);\n@@ -3914,0 +3916,4 @@\n+jlong jdk_internal_invoke_NativeEntryPoint::invoker(oop entry) {\n+  return entry->long_field(_invoker_offset);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1051,0 +1051,1 @@\n+  static int _invoker_offset;\n@@ -1064,0 +1065,1 @@\n+  static jlong      invoker(oop entry);\n@@ -1079,0 +1081,1 @@\n+  static int invoker_offset_in_bytes()         { return _invoker_offset;         }\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -566,1 +566,0 @@\n-    nmethod* ptr = (nmethod *)cb;\n@@ -568,1 +567,1 @@\n-    if (ptr->has_dependencies()) {\n+    if (((nmethod *)cb)->has_dependencies()) {\n@@ -571,1 +570,0 @@\n-    ptr->free_native_invokers();\n","filename":"src\/hotspot\/share\/code\/codeCache.cpp","additions":1,"deletions":3,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -504,2 +504,1 @@\n-  int comp_level,\n-  const GrowableArrayView<RuntimeStub*>& native_invokers\n+  int comp_level\n@@ -527,1 +526,0 @@\n-      + align_up(checked_cast<int>(native_invokers.data_size_in_bytes()), oopSize)\n@@ -543,2 +541,1 @@\n-            comp_level,\n-            native_invokers\n+            comp_level\n@@ -632,2 +629,1 @@\n-    _native_invokers_offset     = _dependencies_offset;\n-    _handler_table_offset    = _native_invokers_offset;\n+    _handler_table_offset    = _dependencies_offset;\n@@ -729,2 +725,1 @@\n-  int comp_level,\n-  const GrowableArrayView<RuntimeStub*>& native_invokers\n+  int comp_level\n@@ -807,2 +802,1 @@\n-    _native_invokers_offset  = _dependencies_offset  + align_up((int)dependencies->size_in_bytes(), oopSize);\n-    _handler_table_offset    = _native_invokers_offset + align_up(checked_cast<int>(native_invokers.data_size_in_bytes()), oopSize);\n+    _handler_table_offset    = _dependencies_offset  + align_up((int)dependencies->size_in_bytes(), oopSize);\n@@ -830,4 +824,0 @@\n-    if (native_invokers.is_nonempty()) { \/\/ can not get address of zero-length array\n-      \/\/ Copy native stubs\n-      memcpy(native_invokers_begin(), native_invokers.adr_at(0), native_invokers.data_size_in_bytes());\n-    }\n@@ -998,4 +988,0 @@\n-    if (printmethod && native_invokers_begin() < native_invokers_end()) {\n-      print_native_invokers();\n-      tty->print_cr(\"- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \");\n-    }\n@@ -1062,6 +1048,0 @@\n-void nmethod::free_native_invokers() {\n-  for (RuntimeStub** it = native_invokers_begin(); it < native_invokers_end(); it++) {\n-    CodeCache::free(*it);\n-  }\n-}\n-\n@@ -2717,8 +2697,0 @@\n-void nmethod::print_native_invokers() {\n-  ResourceMark m;       \/\/ in case methods get printed via debugger\n-  tty->print_cr(\"Native invokers:\");\n-  for (RuntimeStub** itt = native_invokers_begin(); itt < native_invokers_end(); itt++) {\n-    (*itt)->print_on(tty);\n-  }\n-}\n-\n","filename":"src\/hotspot\/share\/code\/nmethod.cpp","additions":5,"deletions":33,"binary":false,"changes":38,"status":"modified"},{"patch":"@@ -210,1 +210,0 @@\n-  int _native_invokers_offset;\n@@ -312,2 +311,1 @@\n-          int comp_level,\n-          const GrowableArrayView<RuntimeStub*>& native_invokers\n+          int comp_level\n@@ -361,2 +359,1 @@\n-                              int comp_level,\n-                              const GrowableArrayView<RuntimeStub*>& native_invokers = GrowableArrayView<RuntimeStub*>::EMPTY\n+                              int comp_level\n@@ -411,3 +408,1 @@\n-  address dependencies_end      () const          { return           header_begin() + _native_invokers_offset ; }\n-  RuntimeStub** native_invokers_begin() const     { return (RuntimeStub**)(header_begin() + _native_invokers_offset) ; }\n-  RuntimeStub** native_invokers_end  () const     { return (RuntimeStub**)(header_begin() + _handler_table_offset); }\n+  address dependencies_end      () const          { return           header_begin() + _handler_table_offset ; }\n@@ -529,2 +524,0 @@\n-  void free_native_invokers();\n-\n@@ -667,1 +660,0 @@\n-  void print_native_invokers();\n","filename":"src\/hotspot\/share\/code\/nmethod.hpp","additions":3,"deletions":11,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -144,2 +144,0 @@\n-  static VMReg vmStorageToVMReg(int type, int index);\n-\n","filename":"src\/hotspot\/share\/code\/vmreg.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1745,1 +1745,1 @@\n-                                 compiler, comp_level, GrowableArrayView<RuntimeStub*>::EMPTY,\n+                                 compiler, comp_level,\n","filename":"src\/hotspot\/share\/jvmci\/jvmciRuntime.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,50 @@\n+\/*\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_OOPS_OOPCAST_INLINE_HPP\n+#define SHARE_OOPS_OOPCAST_INLINE_HPP\n+\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+template<typename T>\n+static bool is_oop_type(oop theOop) {\n+  static_assert(sizeof(T) == 0, \"No is_oop_type specialization found for this type\");\n+  return false;\n+}\n+template<>\n+inline bool is_oop_type<instanceOop>(oop theOop) { return theOop->is_instance(); }\n+template<>\n+inline bool is_oop_type<arrayOop>(oop theOop) { return theOop->is_array(); }\n+template<>\n+inline bool is_oop_type<objArrayOop>(oop theOop) { return theOop->is_objArray(); }\n+template<>\n+inline bool is_oop_type<typeArrayOop>(oop theOop) { return theOop->is_typeArray(); }\n+\n+template<typename R>\n+R oop_cast(oop theOop) {\n+  assert(is_oop_type<R>(theOop), \"Invalid cast\");\n+  return (R) theOop;\n+}\n+\n+#endif \/\/ SHARE_OOPS_OOPCAST_INLINE_HPP\n","filename":"src\/hotspot\/share\/oops\/oopCast.inline.hpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -1213,1 +1213,1 @@\n-      Node* addr_n = kit.argument(1); \/\/ target address\n+      Node* addr_n = kit.argument(0); \/\/ target address\n@@ -1218,1 +1218,1 @@\n-        const TypeLong* addr_t = addr_n->bottom_type()->is_long();\n+\n@@ -1220,1 +1220,0 @@\n-        address addr = (address) addr_t->get_con();\n@@ -1222,1 +1221,10 @@\n-        return new NativeCallGenerator(callee, addr, nep);\n+\n+        if (!nep->need_transition()) {\n+          const TypeLong* addr_t = addr_n->bottom_type()->is_long();\n+          address addr = (address) addr_t->get_con();\n+\n+          return new NativeCallGenerator(callee, addr, nep);\n+        } else {\n+          print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n+                        \"Not inlining non-trivial call\");\n+        }\n@@ -1225,1 +1233,1 @@\n-                               \"NativeEntryPoint not constant\");\n+                               \"NativeEntryPoint or address not constant\");\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciNativeEntryPoint.hpp\"\n@@ -1086,2 +1087,4 @@\n-      if (in(TypeFunc::Parms + callee->arg_size() - 1)->Opcode() == Op_ConP \/* NEP *\/\n-          && in(TypeFunc::Parms + 1)->Opcode() == Op_ConL \/* address *\/) {\n+      Node* nep_node = in(TypeFunc::Parms + callee->arg_size() - 1);\n+      if (nep_node->Opcode() == Op_ConP \/* NEP *\/\n+          && in(TypeFunc::Parms + 0)->Opcode() == Op_ConL \/* address *\/\n+          && !nep_node->bottom_type()->is_oopptr()->const_oop()->as_native_entry_point()->need_transition()) {\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":5,"deletions":2,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -839,1 +839,0 @@\n-  const bool _need_transition;\n@@ -845,2 +844,1 @@\n-                 int shadow_space_bytes,\n-                 bool need_transition)\n+                 int shadow_space_bytes)\n@@ -848,2 +846,1 @@\n-      _ret_regs(ret_regs), _shadow_space_bytes(shadow_space_bytes),\n-      _need_transition(need_transition)\n+      _ret_regs(ret_regs), _shadow_space_bytes(shadow_space_bytes)\n@@ -855,1 +852,1 @@\n-  virtual bool  guaranteed_safepoint()  { return _need_transition; }\n+  virtual bool  guaranteed_safepoint()  { return false; }\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":3,"deletions":6,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -598,1 +598,0 @@\n-                  _native_invokers(comp_arena(), 1, 0, NULL),\n@@ -888,1 +887,0 @@\n-    _native_invokers(),\n@@ -4875,4 +4873,0 @@\n-void Compile::add_native_invoker(RuntimeStub* stub) {\n-  _native_invokers.append(stub);\n-}\n-\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -393,2 +393,0 @@\n-  GrowableArray<RuntimeStub*>   _native_invokers;\n-\n@@ -961,4 +959,0 @@\n-  void add_native_invoker(RuntimeStub* stub);\n-\n-  const GrowableArray<RuntimeStub*> native_invokers() const { return _native_invokers; }\n-\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":0,"deletions":6,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2589,0 +2589,2 @@\n+  assert(!nep->need_transition(), \"only trivial calls\");\n+\n@@ -2590,5 +2592,5 @@\n-  \/\/ [MethodHandle fallback, long addr, HALF addr, ... args , NativeEntryPoint nep]\n-  \/\/                                             |          |\n-  \/\/                                             V          V\n-  \/\/                                             [ ... args ]\n-  uint n_filtered_args = nargs - 4; \/\/ -fallback, -addr (2), -nep;\n+  \/\/ [long addr, HALF addr, ... args , NativeEntryPoint nep]\n+  \/\/                      |          |\n+  \/\/                      V          V\n+  \/\/                      [ ... args ]\n+  uint n_filtered_args = nargs - 3; \/\/ -addr (2), -nep;\n@@ -2604,1 +2606,1 @@\n-      uint vm_unfiltered_arg_pos = vm_arg_pos + 3; \/\/ +3 to skip fallback handle argument and addr (2 since long)\n+      uint vm_unfiltered_arg_pos = vm_arg_pos + 2; \/\/ +2 to skip addr (2 since long)\n@@ -2640,12 +2642,4 @@\n-  if (nep->need_transition()) {\n-    RuntimeStub* invoker = SharedRuntime::make_native_invoker(call_addr,\n-                                                              nep->shadow_space(),\n-                                                              arg_regs, ret_regs);\n-    if (invoker == NULL) {\n-      C->record_failure(\"native invoker not implemented on this platform\");\n-      return NULL;\n-    }\n-    C->add_native_invoker(invoker);\n-    call_addr = invoker->code_begin();\n-  }\n-  assert(call_addr != NULL, \"sanity\");\n+  CallNode* call = new CallNativeNode(new_call_type, call_addr, nep->name(), TypePtr::BOTTOM,\n+                            arg_regs,\n+                            ret_regs,\n+                            nep->shadow_space());\n@@ -2653,9 +2647,1 @@\n-  CallNativeNode* call = new CallNativeNode(new_call_type, call_addr, nep->name(), TypePtr::BOTTOM,\n-                                            arg_regs,\n-                                            ret_regs,\n-                                            nep->shadow_space(),\n-                                            nep->need_transition());\n-\n-  if (call->_need_transition) {\n-    add_safepoint_edges(call);\n-  }\n+  assert(call != nullptr, \"'call' was not set\");\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":13,"deletions":27,"binary":false,"changes":40,"status":"modified"},{"patch":"@@ -3400,2 +3400,1 @@\n-                                     C->rtm_state(),\n-                                     C->native_invokers());\n+                                     C->rtm_state());\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"prims\/foreign_globals.inline.hpp\"\n@@ -102,1 +103,1 @@\n-void CallRegs::calling_convention(BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt) const {\n+int CallRegs::calling_convention(BasicType* sig_bt, VMRegPair *regs, int num_args) const {\n@@ -104,1 +105,1 @@\n-  for (uint i = 0; i < argcnt; i++) {\n+  for (int i = 0; i < num_args; i++) {\n@@ -113,1 +114,1 @@\n-        parm_regs[i].set1(_arg_regs[src_pos++]);\n+        regs[i].set1(_arg_regs[src_pos++]);\n@@ -117,1 +118,1 @@\n-        assert((i + 1) < argcnt && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+        assert((i + 1) < num_args && sig_bt[i + 1] == T_VOID, \"expecting half\");\n@@ -119,1 +120,1 @@\n-        parm_regs[i].set2(_arg_regs[src_pos++]);\n+        regs[i].set2(_arg_regs[src_pos++]);\n@@ -123,1 +124,1 @@\n-        parm_regs[i].set_bad();\n+        regs[i].set_bad();\n@@ -130,0 +131,277 @@\n+  return 0; \/\/ assumed unused\n+}\n+\n+int RegSpiller::compute_spill_area(const VMReg* regs, int num_regs) {\n+  int result_size = 0;\n+  for (int i = 0; i < num_regs; i++) {\n+    result_size += pd_reg_size(regs[i]);\n+  }\n+  return result_size;\n+}\n+\n+void RegSpiller::generate(MacroAssembler* masm, int rsp_offset, bool spill) const {\n+  int offset = rsp_offset;\n+  for (int i = 0; i < _num_regs; i++) {\n+    VMReg reg = _regs[i];\n+    if (spill) {\n+      pd_store_reg(masm, offset, reg);\n+    } else {\n+      pd_load_reg(masm, offset, reg);\n+    }\n+    offset += pd_reg_size(reg);\n+  }\n+}\n+\n+void ArgumentShuffle::print_on(outputStream* os) const {\n+  os->print_cr(\"Argument shuffle {\");\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    os->print(\"Move a %s from (\", null_safe_string(type2name(arg_bt)));\n+    from_vmreg.first()->print_on(os);\n+    os->print(\",\");\n+    from_vmreg.second()->print_on(os);\n+    os->print(\") to (\");\n+    to_vmreg.first()->print_on(os);\n+    os->print(\",\");\n+    to_vmreg.second()->print_on(os);\n+    os->print_cr(\")\");\n+  }\n+  os->print_cr(\"Stack argument slots: %d\", _out_arg_stack_slots);\n+  os->print_cr(\"}\");\n+}\n+\n+int DowncallNativeCallConv::calling_convention(BasicType* sig_bt, VMRegPair* out_regs, int num_args) const {\n+  out_regs[0].set2(_input_addr_reg); \/\/ address\n+  out_regs[1].set_bad(); \/\/ upper half\n+\n+  int src_pos = 0;\n+  int stk_slots = 0;\n+  for (int i = 2; i < num_args; i++) { \/\/ skip address (2)\n+    switch (sig_bt[i]) {\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_INT:\n+      case T_FLOAT: {\n+        VMReg reg = _input_regs.at(src_pos++);\n+        out_regs[i].set1(reg);\n+        if (reg->is_stack())\n+          stk_slots += 2;\n+        break;\n+      }\n+      case T_LONG:\n+      case T_DOUBLE: {\n+        assert((i + 1) < num_args && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+        VMReg reg = _input_regs.at(src_pos);\n+        out_regs[i].set2(reg);\n+        src_pos += 2; \/\/ skip BAD as well\n+        if (reg->is_stack())\n+          stk_slots += 2;\n+        break;\n+      }\n+      case T_VOID: \/\/ Halves of longs and doubles\n+        assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+        out_regs[i].set_bad();\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n+  return stk_slots;\n+}\n+\n+\/\/ based on ComputeMoveOrder from x86_64 shared runtime code.\n+\/\/ with some changes.\n+class ForeignCMO: public StackObj {\n+  class MoveOperation: public ResourceObj {\n+    friend class ForeignCMO;\n+   private:\n+    VMRegPair        _src;\n+    VMRegPair        _dst;\n+    bool             _processed;\n+    MoveOperation*  _next;\n+    MoveOperation*  _prev;\n+    BasicType        _bt;\n+\n+    static int get_id(VMRegPair r) {\n+      return r.first()->value();\n+    }\n+\n+   public:\n+    MoveOperation(VMRegPair src, VMRegPair dst, BasicType bt):\n+      _src(src)\n+    , _dst(dst)\n+    , _processed(false)\n+    , _next(NULL)\n+    , _prev(NULL)\n+    , _bt(bt) {\n+    }\n+\n+    int src_id() const          { return get_id(_src); }\n+    int dst_id() const          { return get_id(_dst); }\n+    MoveOperation* next() const { return _next; }\n+    MoveOperation* prev() const { return _prev; }\n+    void set_processed()        { _processed = true; }\n+    bool is_processed() const   { return _processed; }\n+\n+    \/\/ insert\n+    void break_cycle(VMRegPair temp_register) {\n+      \/\/ create a new store following the last store\n+      \/\/ to move from the temp_register to the original\n+      MoveOperation* new_store = new MoveOperation(temp_register, _dst, _bt);\n+\n+      \/\/ break the cycle of links and insert new_store at the end\n+      \/\/ break the reverse link.\n+      MoveOperation* p = prev();\n+      assert(p->next() == this, \"must be\");\n+      _prev = NULL;\n+      p->_next = new_store;\n+      new_store->_prev = p;\n+\n+      \/\/ change the original store to save it's value in the temp.\n+      _dst = temp_register;\n+    }\n+\n+    void link(GrowableArray<MoveOperation*>& killer) {\n+      \/\/ link this store in front the store that it depends on\n+      MoveOperation* n = killer.at_grow(src_id(), NULL);\n+      if (n != NULL) {\n+        assert(_next == NULL && n->_prev == NULL, \"shouldn't have been set yet\");\n+        _next = n;\n+        n->_prev = this;\n+      }\n+    }\n+\n+    Move as_move() {\n+      return {_bt, _src, _dst};\n+    }\n+  };\n+\n+ private:\n+  GrowableArray<MoveOperation*> _edges;\n+  GrowableArray<Move> _moves;\n+\n+ public:\n+  ForeignCMO(int total_in_args, const VMRegPair* in_regs, int total_out_args, VMRegPair* out_regs,\n+             const BasicType* in_sig_bt, VMRegPair tmp_vmreg) : _edges(total_in_args), _moves(total_in_args) {\n+    assert(total_out_args >= total_in_args, \"can only add prefix args\");\n+    \/\/ Note that total_out_args args can be greater than total_in_args in the case of upcalls.\n+    \/\/ There will be a leading MH receiver arg in the out args in that case.\n+    \/\/\n+    \/\/ Leading args in the out args will be ignored below because we iterate from the end of\n+    \/\/ the register arrays until !(in_idx >= 0), and total_in_args is smaller.\n+    \/\/\n+    \/\/ Stub code adds a move for the receiver to j_rarg0 (and potential other prefix args) manually.\n+    for (int in_idx = total_in_args - 1, out_idx = total_out_args - 1; in_idx >= 0; in_idx--, out_idx--) {\n+      BasicType bt = in_sig_bt[in_idx];\n+      assert(bt != T_ARRAY, \"array not expected\");\n+      VMRegPair in_reg = in_regs[in_idx];\n+      VMRegPair out_reg = out_regs[out_idx];\n+\n+      if (out_reg.first()->is_stack()) {\n+        \/\/ Move operations where the dest is the stack can all be\n+        \/\/ scheduled first since they can't interfere with the other moves.\n+        \/\/ The input and output stack spaces are distinct from each other.\n+        Move move{bt, in_reg, out_reg};\n+        _moves.push(move);\n+      } else if (in_reg.first() == out_reg.first()\n+                 || bt == T_VOID) {\n+        \/\/ 1. Can skip non-stack identity moves.\n+        \/\/\n+        \/\/ 2. Upper half of long or double (T_VOID).\n+        \/\/    Don't need to do anything.\n+        continue;\n+      } else {\n+        _edges.append(new MoveOperation(in_reg, out_reg, bt));\n+      }\n+    }\n+    \/\/ Break any cycles in the register moves and emit the in the\n+    \/\/ proper order.\n+    compute_store_order(tmp_vmreg);\n+  }\n+\n+  \/\/ Walk the edges breaking cycles between moves.  The result list\n+  \/\/ can be walked in order to produce the proper set of loads\n+  void compute_store_order(VMRegPair temp_register) {\n+    \/\/ Record which moves kill which values\n+    GrowableArray<MoveOperation*> killer; \/\/ essentially a map of register id -> MoveOperation*\n+    for (int i = 0; i < _edges.length(); i++) {\n+      MoveOperation* s = _edges.at(i);\n+      assert(killer.at_grow(s->dst_id(), NULL) == NULL,\n+             \"multiple moves with the same register as destination\");\n+      killer.at_put_grow(s->dst_id(), s, NULL);\n+    }\n+    assert(killer.at_grow(MoveOperation::get_id(temp_register), NULL) == NULL,\n+           \"make sure temp isn't in the registers that are killed\");\n+\n+    \/\/ create links between loads and stores\n+    for (int i = 0; i < _edges.length(); i++) {\n+      _edges.at(i)->link(killer);\n+    }\n+\n+    \/\/ at this point, all the move operations are chained together\n+    \/\/ in one or more doubly linked lists.  Processing them backwards finds\n+    \/\/ the beginning of the chain, forwards finds the end.  If there's\n+    \/\/ a cycle it can be broken at any point,  so pick an edge and walk\n+    \/\/ backward until the list ends or we end where we started.\n+    for (int e = 0; e < _edges.length(); e++) {\n+      MoveOperation* s = _edges.at(e);\n+      if (!s->is_processed()) {\n+        MoveOperation* start = s;\n+        \/\/ search for the beginning of the chain or cycle\n+        while (start->prev() != NULL && start->prev() != s) {\n+          start = start->prev();\n+        }\n+        if (start->prev() == s) {\n+          start->break_cycle(temp_register);\n+        }\n+        \/\/ walk the chain forward inserting to store list\n+        while (start != NULL) {\n+          _moves.push(start->as_move());\n+\n+          start->set_processed();\n+          start = start->next();\n+        }\n+      }\n+    }\n+  }\n+\n+  GrowableArray<Move> moves() {\n+    return _moves;\n+  }\n+};\n+\n+ArgumentShuffle::ArgumentShuffle(\n+    BasicType* in_sig_bt,\n+    int num_in_args,\n+    BasicType* out_sig_bt,\n+    int num_out_args,\n+    const CallConvClosure* input_conv,\n+    const CallConvClosure* output_conv,\n+    VMReg shuffle_temp) {\n+\n+  VMRegPair* in_regs = NEW_RESOURCE_ARRAY(VMRegPair, num_in_args);\n+  input_conv->calling_convention(in_sig_bt, in_regs, num_in_args);\n+\n+  VMRegPair* out_regs = NEW_RESOURCE_ARRAY(VMRegPair, num_out_args);\n+  _out_arg_stack_slots = output_conv->calling_convention(out_sig_bt, out_regs, num_out_args);\n+\n+  VMRegPair tmp_vmreg;\n+  tmp_vmreg.set2(shuffle_temp);\n+\n+  \/\/ Compute a valid move order, using tmp_vmreg to break any cycles.\n+  \/\/ Note that ForeignCMO ignores the upper half of our VMRegPairs.\n+  \/\/ We are not moving Java values here, only register-sized values,\n+  \/\/ so we shouldn't have to worry about the upper half any ways.\n+  \/\/ This should work fine on 32-bit as well, since we would only be\n+  \/\/ moving 32-bit sized values (i.e. low-level MH shouldn't take any double\/long).\n+  ForeignCMO order(num_in_args, in_regs,\n+                   num_out_args, out_regs,\n+                   in_sig_bt, tmp_vmreg);\n+  _moves = order.moves();\n","filename":"src\/hotspot\/share\/prims\/foreign_globals.cpp","additions":284,"deletions":6,"binary":false,"changes":290,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -34,1 +35,6 @@\n-struct CallRegs {\n+class CallConvClosure {\n+public:\n+  virtual int calling_convention(BasicType* sig_bt, VMRegPair* regs, int num_args) const = 0;\n+};\n+\n+struct CallRegs : public CallConvClosure {\n@@ -41,1 +47,1 @@\n-  void calling_convention(BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt) const;\n+  int calling_convention(BasicType* sig_bt, VMRegPair* regs, int num_args) const override;\n@@ -77,3 +83,0 @@\n-  template<typename R>\n-  static R cast(oop theOop);\n-\n@@ -90,0 +93,74 @@\n+\n+  static VMReg vmstorage_to_vmreg(int type, int index);\n+};\n+\n+\n+\n+class JavaCallConv : public CallConvClosure {\n+public:\n+  int calling_convention(BasicType* sig_bt, VMRegPair* regs, int num_args) const override {\n+    return SharedRuntime::java_calling_convention(sig_bt, regs, num_args);\n+  }\n+};\n+\n+class DowncallNativeCallConv : public CallConvClosure {\n+  const GrowableArray<VMReg>& _input_regs;\n+  VMReg _input_addr_reg;\n+public:\n+  DowncallNativeCallConv(const GrowableArray<VMReg>& input_regs, VMReg input_addr_reg)\n+   : _input_regs(input_regs),\n+   _input_addr_reg(input_addr_reg) {}\n+\n+  int calling_convention(BasicType* sig_bt, VMRegPair* out_regs, int num_args) const override;\n+};\n+\n+class RegSpiller {\n+  const VMReg* _regs;\n+  int _num_regs;\n+  int _spill_size_bytes;\n+public:\n+  RegSpiller(const VMReg* regs, int num_regs) :\n+    _regs(regs), _num_regs(num_regs),\n+    _spill_size_bytes(compute_spill_area(regs, num_regs)) {\n+  }\n+  RegSpiller(const GrowableArray<VMReg>& regs) : RegSpiller(regs.data(), regs.length()) {\n+  }\n+\n+  int spill_size_bytes() const { return _spill_size_bytes; }\n+  void generate_spill(MacroAssembler* masm, int rsp_offset) const { return generate(masm, rsp_offset, true); }\n+  void generate_fill(MacroAssembler* masm, int rsp_offset) const { return generate(masm, rsp_offset, false); }\n+\n+private:\n+  static int compute_spill_area(const VMReg* regs, int num_regs);\n+  void generate(MacroAssembler* masm, int rsp_offset, bool is_spill) const;\n+\n+  static int pd_reg_size(VMReg reg);\n+  static void pd_store_reg(MacroAssembler* masm, int offset, VMReg reg);\n+  static void pd_load_reg(MacroAssembler* masm, int offset, VMReg reg);\n+};\n+\n+struct Move {\n+  BasicType bt;\n+  VMRegPair from;\n+  VMRegPair to;\n+};\n+\n+class ArgumentShuffle {\n+private:\n+  GrowableArray<Move> _moves;\n+  int _out_arg_stack_slots;\n+public:\n+  ArgumentShuffle(\n+    BasicType* in_sig_bt, int num_in_args,\n+    BasicType* out_sig_bt, int num_out_args,\n+    const CallConvClosure* input_conv, const CallConvClosure* output_conv,\n+    VMReg shuffle_temp);\n+\n+  int out_arg_stack_slots() const { return _out_arg_stack_slots; }\n+  void generate(MacroAssembler* masm) const {\n+    pd_generate(masm);\n+  }\n+\n+  void print_on(outputStream* os) const;\n+private:\n+  void pd_generate(MacroAssembler* masm) const;\n","filename":"src\/hotspot\/share\/prims\/foreign_globals.hpp","additions":82,"deletions":5,"binary":false,"changes":87,"status":"modified"},{"patch":"@@ -31,16 +31,1 @@\n-\n-template<typename T>\n-static bool check_type(oop theOop) {\n-  static_assert(sizeof(T) == 0, \"No check_type specialization found for this type\");\n-  return false;\n-}\n-template<>\n-inline bool check_type<objArrayOop>(oop theOop) { return theOop->is_objArray(); }\n-template<>\n-inline bool check_type<typeArrayOop>(oop theOop) { return theOop->is_typeArray(); }\n-\n-template<typename R>\n-R ForeignGlobals::cast(oop theOop) {\n-  assert(check_type<R>(theOop), \"Invalid cast\");\n-  return (R) theOop;\n-}\n+#include \"oops\/oopCast.inline.hpp\"\n@@ -50,1 +35,1 @@\n-  objArrayOop subarray = cast<objArrayOop>(jarray->obj_at(type_index));\n+  objArrayOop subarray = oop_cast<objArrayOop>(jarray->obj_at(type_index));\n@@ -59,0 +44,4 @@\n+inline const char* null_safe_string(const char* str) {\n+  return str == nullptr ? \"NULL\" : str;\n+}\n+\n","filename":"src\/hotspot\/share\/prims\/foreign_globals.inline.hpp","additions":6,"deletions":17,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -27,0 +27,2 @@\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"classfile\/javaClasses.inline.hpp\"\n@@ -28,0 +30,9 @@\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"oops\/typeArrayOop.hpp\"\n+#include \"oops\/oopCast.inline.hpp\"\n+#include \"prims\/foreign_globals.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"prims\/universalNativeInvoker.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n@@ -30,1 +41,89 @@\n-  return VMRegImpl::vmStorageToVMReg(type, index)->value();\n+  return ForeignGlobals::vmstorage_to_vmreg(type, index)->value();\n+JNI_END\n+\n+JNI_ENTRY(jlong, NEP_makeInvoker(JNIEnv* env, jclass _unused, jobject method_type, jint shadow_space_bytes,\n+                                 jlongArray arg_moves, jlongArray ret_moves))\n+  ResourceMark rm;\n+\n+  \/\/ Note: the method_type's first param is the target address, but we don't have\n+  \/\/ and entry for that in the arg_moves array.\n+  \/\/ we need an entry for that in the basic type at least, so we can later\n+  \/\/ generate the right argument shuffle\n+\n+  oop type = JNIHandles::resolve(method_type);\n+  \/\/ does not contain entry for address:\n+  typeArrayOop arg_moves_oop = oop_cast<typeArrayOop>(JNIHandles::resolve(arg_moves));\n+  typeArrayOop ret_moves_oop = oop_cast<typeArrayOop>(JNIHandles::resolve(ret_moves));\n+  \/\/ contains address:\n+  int pcount = java_lang_invoke_MethodType::ptype_count(type);\n+  int pslots = java_lang_invoke_MethodType::ptype_slot_count(type);\n+  \/\/ contains address:\n+  BasicType* basic_type = NEW_RESOURCE_ARRAY(BasicType, pslots);\n+  \/\/ address\n+  basic_type[0] = T_LONG;\n+  basic_type[1] = T_VOID;\n+\n+  \/\/ does not contain entry for address:\n+  GrowableArray<VMReg> input_regs(pslots);\n+\n+  int num_args = 2;\n+  for (int i = 1; i < pcount; i++) { \/\/ skip addr\n+    oop type_oop = java_lang_invoke_MethodType::ptype(type, i);\n+    assert(java_lang_Class::is_primitive(type_oop), \"Only primitives expected\");\n+    BasicType bt = java_lang_Class::primitive_type(type_oop);\n+    basic_type[num_args] = bt;\n+    input_regs.push(VMRegImpl::as_VMReg(arg_moves_oop->long_at(i - 1))); \/\/ address missing in moves\n+    num_args++;\n+\n+    if (bt == BasicType::T_DOUBLE || bt == BasicType::T_LONG) {\n+      basic_type[num_args] = T_VOID;\n+      input_regs.push(VMRegImpl::Bad()); \/\/ half of double\/long\n+      num_args++;\n+    }\n+  }\n+\n+  GrowableArray<VMReg> output_regs(pslots);\n+\n+  jint outs = ret_moves_oop->length();\n+  assert(outs <= 1, \"No multi-reg returns\");\n+  BasicType ret_bt = T_VOID;\n+  if (outs == 1) {\n+    oop type_oop = java_lang_invoke_MethodType::rtype(type);\n+    ret_bt = java_lang_Class::primitive_type(type_oop);\n+\n+    output_regs.push(VMRegImpl::as_VMReg(ret_moves_oop->long_at(0)));\n+    if (ret_bt == BasicType::T_DOUBLE || ret_bt == BasicType::T_LONG) {\n+      output_regs.push(VMRegImpl::Bad()); \/\/ half of double\/long\n+    }\n+  }\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    ls.print_cr(\"Generating native invoker {\");\n+    ls.print(\"BasicType { \");\n+    for (int i = 0; i < num_args; i++) {\n+      ls.print(\"%s, \", null_safe_string(type2name(basic_type[i])));\n+    }\n+    ls.print_cr(\"}\");\n+    ls.print_cr(\"shadow_space_bytes = %d\", shadow_space_bytes);\n+    ls.print(\"input_registers { \");\n+    for (int i = 0; i < input_regs.length(); i++) {\n+      VMReg reg = input_regs.at(i);\n+      ls.print(\"%s (\" INTPTR_FORMAT \"), \", reg->name(), reg->value());\n+    }\n+    ls.print_cr(\"}\");\n+      ls.print(\"output_registers { \");\n+    for (int i = 0; i < output_regs.length(); i++) {\n+      VMReg reg = output_regs.at(i);\n+      ls.print(\"%s (\" INTPTR_FORMAT \"), \", reg->name(), reg->value());\n+    }\n+    ls.print_cr(\"}\");\n+    ls.print_cr(\"}\");\n+  }\n+#endif\n+\n+  return (jlong) ProgrammableInvoker::make_native_invoker(\n+    basic_type, num_args, ret_bt, shadow_space_bytes, input_regs, output_regs)->code_begin();\n@@ -38,0 +137,1 @@\n+  {CC \"makeInvoker\", CC \"(Ljava\/lang\/invoke\/MethodType;I[J[J)J\", FN_PTR(NEP_makeInvoker)},\n@@ -45,1 +145,1 @@\n-JNI_END\n\\ No newline at end of file\n+JNI_END\n","filename":"src\/hotspot\/share\/prims\/nativeEntryPoint.cpp","additions":102,"deletions":2,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -53,0 +53,4 @@\n+JVM_ENTRY(jboolean, PI_SupportsNativeInvoker(JNIEnv *env, jclass unused))\n+  return (jboolean) ProgrammableInvoker::supports_native_invoker();\n+JVM_END\n+\n@@ -59,1 +63,2 @@\n-  {CC \"generateAdapter\", CC \"(\" FOREIGN_ABI \"\/ABIDescriptor;\" FOREIGN_ABI \"\/BufferLayout;\" \")J\", FN_PTR(PI_generateAdapter)}\n+  {CC \"generateAdapter\", CC \"(\" FOREIGN_ABI \"\/ABIDescriptor;\" FOREIGN_ABI \"\/BufferLayout;\" \")J\", FN_PTR(PI_generateAdapter)},\n+  {CC \"supportsNativeInvoker\", CC \"()Z\", FN_PTR(PI_SupportsNativeInvoker)},\n","filename":"src\/hotspot\/share\/prims\/universalNativeInvoker.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+class RuntimeStub;\n@@ -48,0 +49,8 @@\n+\n+  static RuntimeStub* make_native_invoker(BasicType*,\n+                                          int num_args,\n+                                          BasicType ret_bt,\n+                                          int shadow_space_bytes,\n+                                          const GrowableArray<VMReg>& input_registers,\n+                                          const GrowableArray<VMReg>& output_registers);\n+  static bool supports_native_invoker();\n","filename":"src\/hotspot\/share\/prims\/universalNativeInvoker.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2049,0 +2049,3 @@\n+                                                                            \\\n+  develop(bool, TraceNativeInvokers, false,                                 \\\n+                \"Trace Panama native invoker generation\")                   \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -513,13 +513,0 @@\n-#ifdef COMPILER2\n-  static RuntimeStub* make_native_invoker(address call_target,\n-                                          int shadow_space_bytes,\n-                                          const GrowableArray<VMReg>& input_registers,\n-                                          const GrowableArray<VMReg>& output_registers);\n-#endif\n-\n-  static void compute_move_order(const BasicType* in_sig_bt,\n-                                 int total_in_args, const VMRegPair* in_regs,\n-                                 int total_out_args, VMRegPair* out_regs,\n-                                 GrowableArray<int>& arg_order,\n-                                 VMRegPair tmp_vmreg);\n-\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.hpp","additions":0,"deletions":13,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -144,0 +144,8 @@\n+  E* data() {\n+    return _data;\n+  }\n+\n+  const E* data() const {\n+    return _data;\n+  }\n+\n","filename":"src\/hotspot\/share\/utilities\/growableArray.hpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -1468,2 +1468,2 @@\n-            public MethodHandle nativeMethodHandle(NativeEntryPoint nep, MethodHandle fallback) {\n-                return NativeMethodHandle.make(nep, fallback);\n+            public MethodHandle nativeMethodHandle(NativeEntryPoint nep) {\n+                return NativeMethodHandle.make(nep);\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleImpl.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -44,1 +44,0 @@\n-    final MethodHandle fallback;\n@@ -46,1 +45,1 @@\n-    private NativeMethodHandle(MethodType type, LambdaForm form, MethodHandle fallback, NativeEntryPoint nep) {\n+    private NativeMethodHandle(MethodType type, LambdaForm form, NativeEntryPoint nep) {\n@@ -48,1 +47,0 @@\n-        this.fallback = fallback;\n@@ -55,1 +53,1 @@\n-    public static MethodHandle make(NativeEntryPoint nep, MethodHandle fallback) {\n+    public static MethodHandle make(NativeEntryPoint nep) {\n@@ -60,2 +58,0 @@\n-        if (type != fallback.type())\n-            throw new IllegalArgumentException(\"Type of fallback must match: \" + type + \" != \" + fallback.type());\n@@ -64,1 +60,1 @@\n-        return new NativeMethodHandle(type, lform, fallback, nep);\n+        return new NativeMethodHandle(type, lform, nep);\n@@ -91,2 +87,2 @@\n-        MethodType linkerType = mtype.insertParameterTypes(0, MethodHandle.class)\n-                .appendParameterTypes(Object.class);\n+        MethodType linkerType = mtype\n+                .appendParameterTypes(Object.class); \/\/ NEP\n@@ -103,1 +99,0 @@\n-        final int GET_FALLBACK = nameCursor++;\n@@ -106,0 +101,1 @@\n+\n@@ -108,1 +104,1 @@\n-        names[GET_FALLBACK] = new LambdaForm.Name(Lazy.NF_internalFallback, names[NMH_THIS]);\n+\n@@ -110,0 +106,1 @@\n+\n@@ -111,3 +108,1 @@\n-        \/\/ Need to pass fallback here so we can call it without destroying the receiver register!!\n-        outArgs[0] = names[GET_FALLBACK];\n-        System.arraycopy(names, ARG_BASE, outArgs, 1, mtype.parameterCount());\n+        System.arraycopy(names, ARG_BASE, outArgs, 0, mtype.parameterCount());\n@@ -116,0 +111,1 @@\n+\n@@ -126,1 +122,1 @@\n-        return new NativeMethodHandle(mt, lf, fallback, nep);\n+        return new NativeMethodHandle(mt, lf, nep);\n@@ -139,5 +135,0 @@\n-    @ForceInline\n-    static MethodHandle internalFallback(Object mh) {\n-        return ((NativeMethodHandle)mh).fallback;\n-    }\n-\n@@ -152,2 +143,0 @@\n-        static final NamedFunction\n-                NF_internalFallback;\n@@ -161,2 +150,0 @@\n-                        NF_internalFallback = new NamedFunction(\n-                                THIS_CLASS.getDeclaredMethod(\"internalFallback\", Object.class))\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/NativeMethodHandle.java","additions":11,"deletions":24,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -131,1 +131,0 @@\n-     * @param fallback the fallback handle\n@@ -134,1 +133,1 @@\n-    MethodHandle nativeMethodHandle(NativeEntryPoint nep, MethodHandle fallback);\n+    MethodHandle nativeMethodHandle(NativeEntryPoint nep);\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangInvokeAccess.java","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -29,0 +29,3 @@\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n@@ -30,0 +33,1 @@\n+import java.util.concurrent.ConcurrentHashMap;\n@@ -50,0 +54,6 @@\n+    private final long invoker;\n+\n+    private static final Map<CacheKey, Long> INVOKER_CACHE = new ConcurrentHashMap<>();\n+    private record CacheKey(MethodType mt, int shadowSpaceBytes,\n+                            List<VMStorageProxy> argMoves, List<VMStorageProxy> retMoves) {}\n+\n@@ -51,1 +61,1 @@\n-                     boolean needTransition, MethodType methodType, String name) {\n+                     boolean needTransition, MethodType methodType, String name, long invoker) {\n@@ -58,0 +68,1 @@\n+        this.invoker = invoker;\n@@ -67,2 +78,13 @@\n-        return new NativeEntryPoint(abi.shadowSpaceBytes(), encodeVMStorages(argMoves), encodeVMStorages(returnMoves),\n-                needTransition, methodType, name);\n+        assert (methodType.parameterType(0) == long.class) : \"Address expected\";\n+\n+        int shadowSpaceBytes = abi.shadowSpaceBytes();\n+        long[] encArgMoves = encodeVMStorages(argMoves);\n+        long[] encRetMoves = encodeVMStorages(returnMoves);\n+\n+        CacheKey key = new CacheKey(methodType, abi.shadowSpaceBytes(),\n+                Arrays.asList(argMoves), Arrays.asList(returnMoves));\n+        long invoker = INVOKER_CACHE.computeIfAbsent(key, k ->\n+            makeInvoker(methodType, shadowSpaceBytes, encArgMoves, encRetMoves));\n+\n+        return new NativeEntryPoint(shadowSpaceBytes, encArgMoves, encRetMoves,\n+                needTransition, methodType, name, invoker);\n@@ -81,0 +103,2 @@\n+    private static native long makeInvoker(MethodType methodType, int shadowSpaceBytes, long[] encArgMoves, long[] encRetMoves);\n+\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/invoke\/NativeEntryPoint.java","additions":27,"deletions":3,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -138,4 +138,1 @@\n-        MethodHandle handle = insertArguments(MH_INVOKE_MOVES.bindTo(this), 2, argMoves, retMoves);\n-        MethodHandle collector = makeCollectorHandle(leafType);\n-        handle = collectArguments(handle, 1, collector);\n-        handle = handle.asType(leafTypeWithAddress);\n+        MethodHandle handle;\n@@ -145,1 +142,1 @@\n-        if (USE_INTRINSICS && isSimple && !usesStackArgs) {\n+        if (USE_INTRINSICS && isSimple && !usesStackArgs && supportsNativeInvoker()) {\n@@ -147,1 +144,1 @@\n-                \"native_call\",\n+                \"native_invoker_\" + leafType.descriptorString(),\n@@ -155,1 +152,6 @@\n-            handle = JLIA.nativeMethodHandle(nep, handle);\n+            handle = JLIA.nativeMethodHandle(nep);\n+        } else {\n+            handle = insertArguments(MH_INVOKE_MOVES.bindTo(this), 2, argMoves, retMoves);\n+            MethodHandle collector = makeCollectorHandle(leafType);\n+            handle = collectArguments(handle, 1, collector);\n+            handle = handle.asType(leafTypeWithAddress);\n@@ -357,0 +359,1 @@\n+    static native boolean supportsNativeInvoker();\n","filename":"src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/ProgrammableInvoker.java","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -25,0 +25,24 @@\n+\/*\n+ * @test id=scope\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+ * @build NativeTestHelper CallGeneratorHelper TestUpcall\n+ *\n+ * @run testng\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:-VerifyDependencies\n+ *   --enable-native-access=ALL-UNNAMED -Dgenerator.sample.factor=17\n+ *   -DUPCALL_TEST_TYPE=SCOPE\n+ *   TestUpcall\n+ *\/\n+\n+\/*\n+ * @test id=no_scope\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+ * @build NativeTestHelper CallGeneratorHelper TestUpcall\n+ *\n+ * @run testng\/othervm -XX:+IgnoreUnrecognizedVMOptions -XX:-VerifyDependencies\n+ *   --enable-native-access=ALL-UNNAMED -Dgenerator.sample.factor=17\n+ *   -DUPCALL_TEST_TYPE=NO_SCOPE\n+ *   TestUpcall\n+ *\/\n+\n","filename":"test\/jdk\/java\/foreign\/TestUpcall.java","additions":24,"deletions":0,"binary":false,"changes":24,"status":"modified"}]}