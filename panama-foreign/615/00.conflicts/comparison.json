{"files":[{"patch":"@@ -282,14 +282,0 @@\n-\/\/ The java_calling_convention describes stack locations as ideal slots on\n-\/\/ a frame with no abi restrictions. Since we must observe abi restrictions\n-\/\/ (like the placement of the register window) the slots must be biased by\n-\/\/ the following value.\n-static int reg2offset_in(VMReg r) {\n-  \/\/ Account for saved rfp and lr\n-  \/\/ This should really be in_preserve_stack_slots\n-  return (r->reg2stack() + 4) * VMRegImpl::stack_slot_size;\n-}\n-\n-static int reg2offset_out(VMReg r) {\n-  return (r->reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;\n-}\n-\n@@ -921,166 +907,0 @@\n-\/\/ On 64 bit we will store integer like items to the stack as\n-\/\/ 64 bits items (Aarch64 abi) even though java would only store\n-\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n-\/\/ So this routine will do 32->32 on 32bit and 32->64 on 64bit\n-static void move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ ldrsw(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ sxtw(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\/\/ An oop arg. Must pass a handle not the oop itself\n-static void object_move(MacroAssembler* masm,\n-                        OopMap* map,\n-                        int oop_handle_offset,\n-                        int framesize_in_slots,\n-                        VMRegPair src,\n-                        VMRegPair dst,\n-                        bool is_receiver,\n-                        int* receiver_offset) {\n-\n-  \/\/ must pass a handle. First figure out the location we use as a handle\n-\n-  Register rHandle = dst.first()->is_stack() ? rscratch2 : dst.first()->as_Register();\n-\n-  \/\/ See if oop is NULL if it is we need no handle\n-\n-  if (src.first()->is_stack()) {\n-\n-    \/\/ Oop is already on the stack as an argument\n-    int offset_in_older_frame = src.first()->reg2stack() + SharedRuntime::out_preserve_stack_slots();\n-    map->set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));\n-    if (is_receiver) {\n-      *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;\n-    }\n-\n-    __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-    __ lea(rHandle, Address(rfp, reg2offset_in(src.first())));\n-    \/\/ conditionally move a NULL\n-    __ cmp(rscratch1, zr);\n-    __ csel(rHandle, zr, rHandle, Assembler::EQ);\n-  } else {\n-\n-    \/\/ Oop is in an a register we must store it to the space we reserve\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n-\n-    const Register rOop = src.first()->as_Register();\n-    int oop_slot;\n-    if (rOop == j_rarg0)\n-      oop_slot = 0;\n-    else if (rOop == j_rarg1)\n-      oop_slot = 1;\n-    else if (rOop == j_rarg2)\n-      oop_slot = 2;\n-    else if (rOop == j_rarg3)\n-      oop_slot = 3;\n-    else if (rOop == j_rarg4)\n-      oop_slot = 4;\n-    else if (rOop == j_rarg5)\n-      oop_slot = 5;\n-    else if (rOop == j_rarg6)\n-      oop_slot = 6;\n-    else {\n-      assert(rOop == j_rarg7, \"wrong register\");\n-      oop_slot = 7;\n-    }\n-\n-    oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;\n-    int offset = oop_slot*VMRegImpl::stack_slot_size;\n-\n-    map->set_oop(VMRegImpl::stack2reg(oop_slot));\n-    \/\/ Store oop in handle area, may be NULL\n-    __ str(rOop, Address(sp, offset));\n-    if (is_receiver) {\n-      *receiver_offset = offset;\n-    }\n-\n-    __ cmp(rOop, zr);\n-    __ lea(rHandle, Address(sp, offset));\n-    \/\/ conditionally move a NULL\n-    __ csel(rHandle, zr, rHandle, Assembler::EQ);\n-  }\n-\n-  \/\/ If arg is on the stack then place it otherwise it is already in correct reg.\n-  if (dst.first()->is_stack()) {\n-    __ str(rHandle, Address(sp, reg2offset_out(dst.first())));\n-  }\n-}\n-\n-\/\/ A float arg may have to do float reg int reg conversion\n-static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n-         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ ldrw(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ strw(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (src.first() != dst.first()) {\n-    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n-      __ fmovs(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n-    else\n-      ShouldNotReachHere();\n-  }\n-}\n-\n-\/\/ A long move\n-static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      \/\/ stack to stack\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      \/\/ stack to reg\n-      __ ldr(dst.first()->as_Register(), Address(rfp, reg2offset_in(src.first())));\n-    }\n-  } else if (dst.first()->is_stack()) {\n-    \/\/ reg to stack\n-    \/\/ Do we really have to sign extend???\n-    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n-    __ str(src.first()->as_Register(), Address(sp, reg2offset_out(dst.first())));\n-  } else {\n-    if (dst.first() != src.first()) {\n-      __ mov(dst.first()->as_Register(), src.first()->as_Register());\n-    }\n-  }\n-}\n-\n-\n-\/\/ A double move\n-static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {\n-  assert(src.first()->is_stack() && dst.first()->is_stack() ||\n-         src.first()->is_reg() && dst.first()->is_reg(), \"Unexpected error\");\n-  if (src.first()->is_stack()) {\n-    if (dst.first()->is_stack()) {\n-      __ ldr(rscratch1, Address(rfp, reg2offset_in(src.first())));\n-      __ str(rscratch1, Address(sp, reg2offset_out(dst.first())));\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  } else if (src.first() != dst.first()) {\n-    if (src.is_single_phys_reg() && dst.is_single_phys_reg())\n-      __ fmovd(dst.first()->as_FloatRegister(), src.first()->as_FloatRegister());\n-    else\n-      ShouldNotReachHere();\n-  }\n-}\n-\n@@ -1152,0 +972,1 @@\n+<<<<<<< HEAD\n@@ -1161,0 +982,63 @@\n+=======\n+\/\/ Unpack an array argument into a pointer to the body and the length\n+\/\/ if the array is non-null, otherwise pass 0 for both.\n+static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) { Unimplemented(); }\n+\n+\n+class ComputeMoveOrder: public StackObj {\n+  class MoveOperation: public ResourceObj {\n+    friend class ComputeMoveOrder;\n+   private:\n+    VMRegPair        _src;\n+    VMRegPair        _dst;\n+    int              _src_index;\n+    int              _dst_index;\n+    bool             _processed;\n+    MoveOperation*  _next;\n+    MoveOperation*  _prev;\n+\n+    static int get_id(VMRegPair r) { Unimplemented(); return 0; }\n+\n+   public:\n+    MoveOperation(int src_index, VMRegPair src, int dst_index, VMRegPair dst):\n+      _src(src)\n+    , _dst(dst)\n+    , _src_index(src_index)\n+    , _dst_index(dst_index)\n+    , _processed(false)\n+    , _next(NULL)\n+    , _prev(NULL) { Unimplemented(); }\n+\n+    VMRegPair src() const              { Unimplemented(); return _src; }\n+    int src_id() const                 { Unimplemented(); return 0; }\n+    int src_index() const              { Unimplemented(); return 0; }\n+    VMRegPair dst() const              { Unimplemented(); return _src; }\n+    void set_dst(int i, VMRegPair dst) { Unimplemented(); }\n+    int dst_index() const              { Unimplemented(); return 0; }\n+    int dst_id() const                 { Unimplemented(); return 0; }\n+    MoveOperation* next() const        { Unimplemented(); return 0; }\n+    MoveOperation* prev() const        { Unimplemented(); return 0; }\n+    void set_processed()               { Unimplemented(); }\n+    bool is_processed() const          { Unimplemented(); return 0; }\n+\n+    \/\/ insert\n+    void break_cycle(VMRegPair temp_register) { Unimplemented(); }\n+\n+    void link(GrowableArray<MoveOperation*>& killer) { Unimplemented(); }\n+  };\n+\n+ private:\n+  GrowableArray<MoveOperation*> edges;\n+\n+ public:\n+  ComputeMoveOrder(int total_in_args, VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,\n+                    BasicType* in_sig_bt, GrowableArray<int>& arg_order, VMRegPair tmp_vmreg) { Unimplemented(); }\n+\n+  \/\/ Collected all the move operations\n+  void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) { Unimplemented(); }\n+\n+  \/\/ Walk the edges breaking cycles between moves.  The result list\n+  \/\/ can be walked in order to produce the proper set of loads\n+  GrowableArray<MoveOperation*>* get_store_order(VMRegPair temp_register) { Unimplemented(); return 0; }\n+};\n+>>>>>>> 68e97308327340b18167def38490f741c2a7d532\n@@ -1201,1 +1085,1 @@\n-  } else if (iid == vmIntrinsics::_invokeBasic || iid == vmIntrinsics::_linkToNative) {\n+  } else if (iid == vmIntrinsics::_invokeBasic) {\n@@ -1203,0 +1087,3 @@\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+    member_reg = r19;  \/\/ known to be free at this point\n@@ -1535,0 +1422,1 @@\n+<<<<<<< HEAD\n@@ -1538,0 +1426,6 @@\n+=======\n+        assert(!is_critical_native, \"no oop arguments\");\n+        __ object_move(map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],\n+                       ((i == 0) && (!is_static)),\n+                       &receiver_offset);\n+>>>>>>> 68e97308327340b18167def38490f741c2a7d532\n@@ -1544,1 +1438,1 @@\n-        float_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ float_move(in_regs[i], out_regs[c_arg]);\n@@ -1552,1 +1446,1 @@\n-        double_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ double_move(in_regs[i], out_regs[c_arg]);\n@@ -1557,1 +1451,1 @@\n-        long_move(masm, in_regs[i], out_regs[c_arg]);\n+        __ long_move(in_regs[i], out_regs[c_arg]);\n@@ -1564,1 +1458,1 @@\n-        move32_64(masm, in_regs[i], out_regs[c_arg]);\n+        __ move32_64(in_regs[i], out_regs[c_arg]);\n@@ -1692,1 +1586,1 @@\n-  rt_call(masm, native_func);\n+  __ rt_call(native_func);\n@@ -1901,1 +1795,1 @@\n-    rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n+    __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));\n@@ -1928,1 +1822,1 @@\n-  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n@@ -2936,251 +2830,0 @@\n-\/\/ ---------------------------------------------------------------\n-\n-class NativeInvokerGenerator : public StubCodeGenerator {\n-  address _call_target;\n-  int _shadow_space_bytes;\n-\n-  const GrowableArray<VMReg>& _input_registers;\n-  const GrowableArray<VMReg>& _output_registers;\n-\n-  int _frame_complete;\n-  int _framesize;\n-  OopMapSet* _oop_maps;\n-public:\n-  NativeInvokerGenerator(CodeBuffer* buffer,\n-                         address call_target,\n-                         int shadow_space_bytes,\n-                         const GrowableArray<VMReg>& input_registers,\n-                         const GrowableArray<VMReg>& output_registers)\n-   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n-     _call_target(call_target),\n-     _shadow_space_bytes(shadow_space_bytes),\n-     _input_registers(input_registers),\n-     _output_registers(output_registers),\n-     _frame_complete(0),\n-     _framesize(0),\n-     _oop_maps(NULL) {\n-    assert(_output_registers.length() <= 1\n-           || (_output_registers.length() == 2 && !_output_registers.at(1)->is_valid()), \"no multi-reg returns\");\n-  }\n-\n-  void generate();\n-\n-  int spill_size_in_bytes() const {\n-    if (_output_registers.length() == 0) {\n-      return 0;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    if (reg->is_Register()) {\n-      return 8;\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        return Matcher::scalable_vector_reg_size(T_BYTE);\n-      }\n-      return 16;\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-    return 0;\n-  }\n-\n-  void spill_output_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ spill(reg->as_Register(), true, 0);\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        __ spill_sve_vector(reg->as_FloatRegister(), 0, Matcher::scalable_vector_reg_size(T_BYTE));\n-      } else {\n-        __ spill(reg->as_FloatRegister(), __ Q, 0);\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  void fill_output_registers() {\n-    if (_output_registers.length() == 0) {\n-      return;\n-    }\n-    VMReg reg = _output_registers.at(0);\n-    assert(reg->is_reg(), \"must be a register\");\n-    MacroAssembler* masm = _masm;\n-    if (reg->is_Register()) {\n-      __ unspill(reg->as_Register(), true, 0);\n-    } else if (reg->is_FloatRegister()) {\n-      bool use_sve = Matcher::supports_scalable_vector();\n-      if (use_sve) {\n-        __ unspill_sve_vector(reg->as_FloatRegister(), 0, Matcher::scalable_vector_reg_size(T_BYTE));\n-      } else {\n-        __ unspill(reg->as_FloatRegister(), __ Q, 0);\n-      }\n-    } else {\n-      ShouldNotReachHere();\n-    }\n-  }\n-\n-  int frame_complete() const {\n-    return _frame_complete;\n-  }\n-\n-  int framesize() const {\n-    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n-  }\n-\n-  OopMapSet* oop_maps() const {\n-    return _oop_maps;\n-  }\n-\n-private:\n-#ifdef ASSERT\n-  bool target_uses_register(VMReg reg) {\n-    return _input_registers.contains(reg) || _output_registers.contains(reg);\n-  }\n-#endif\n-};\n-\n-static const int native_invoker_code_size = 1024;\n-\n-RuntimeStub* SharedRuntime::make_native_invoker(address call_target,\n-                                                int shadow_space_bytes,\n-                                                const GrowableArray<VMReg>& input_registers,\n-                                                const GrowableArray<VMReg>& output_registers) {\n-  int locs_size  = 64;\n-  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n-  NativeInvokerGenerator g(&code, call_target, shadow_space_bytes, input_registers, output_registers);\n-  g.generate();\n-  code.log_section_sizes(\"nep_invoker_blob\");\n-\n-  RuntimeStub* stub =\n-    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n-                                  &code,\n-                                  g.frame_complete(),\n-                                  g.framesize(),\n-                                  g.oop_maps(), false);\n-  return stub;\n-}\n-\n-void NativeInvokerGenerator::generate() {\n-  assert(!(target_uses_register(rscratch1->as_VMReg())\n-           || target_uses_register(rscratch2->as_VMReg())\n-           || target_uses_register(rthread->as_VMReg())),\n-         \"Register conflict\");\n-\n-  enum layout {\n-    rbp_off,\n-    rbp_off2,\n-    return_off,\n-    return_off2,\n-    framesize \/\/ inclusive of return address\n-  };\n-\n-  assert(_shadow_space_bytes == 0, \"not expecting shadow space on AArch64\");\n-  _framesize = align_up(framesize + (spill_size_in_bytes() >> LogBytesPerInt), 4);\n-  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n-\n-  _oop_maps  = new OopMapSet();\n-  MacroAssembler* masm = _masm;\n-\n-  address start = __ pc();\n-\n-  __ enter();\n-\n-  \/\/ lr and fp are already in place\n-  __ sub(sp, rfp, ((unsigned)_framesize-4) << LogBytesPerInt); \/\/ prolog\n-\n-  _frame_complete = __ pc() - start;\n-\n-  address the_pc = __ pc();\n-  __ set_last_Java_frame(sp, rfp, the_pc, rscratch1);\n-  OopMap* map = new OopMap(_framesize, 0);\n-  _oop_maps->add_gc_map(the_pc - start, map);\n-\n-  \/\/ State transition\n-  __ mov(rscratch1, _thread_in_native);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n-\n-  rt_call(masm, _call_target);\n-\n-  __ mov(rscratch1, _thread_in_native_trans);\n-  __ strw(rscratch1, Address(rthread, JavaThread::thread_state_offset()));\n-\n-  \/\/ Force this write out before the read below\n-  __ membar(Assembler::LoadLoad | Assembler::LoadStore |\n-            Assembler::StoreLoad | Assembler::StoreStore);\n-\n-  __ verify_sve_vector_length();\n-\n-  Label L_after_safepoint_poll;\n-  Label L_safepoint_poll_slow_path;\n-\n-  __ safepoint_poll(L_safepoint_poll_slow_path, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/);\n-\n-  __ ldrw(rscratch1, Address(rthread, JavaThread::suspend_flags_offset()));\n-  __ cbnzw(rscratch1, L_safepoint_poll_slow_path);\n-\n-  __ bind(L_after_safepoint_poll);\n-\n-  \/\/ change thread state\n-  __ mov(rscratch1, _thread_in_Java);\n-  __ lea(rscratch2, Address(rthread, JavaThread::thread_state_offset()));\n-  __ stlrw(rscratch1, rscratch2);\n-\n-  __ block_comment(\"reguard stack check\");\n-  Label L_reguard;\n-  Label L_after_reguard;\n-  __ ldrb(rscratch1, Address(rthread, JavaThread::stack_guard_state_offset()));\n-  __ cmpw(rscratch1, StackOverflow::stack_guard_yellow_reserved_disabled);\n-  __ br(Assembler::EQ, L_reguard);\n-  __ bind(L_after_reguard);\n-\n-  __ reset_last_Java_frame(true);\n-\n-  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n-  __ ret(lr);\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n-  __ bind(L_safepoint_poll_slow_path);\n-\n-  \/\/ Need to save the native result registers around any runtime calls.\n-  spill_output_registers();\n-\n-  __ mov(c_rarg0, rthread);\n-  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n-  __ lea(rscratch1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n-  __ blr(rscratch1);\n-\n-  fill_output_registers();\n-\n-  __ b(L_after_safepoint_poll);\n-  __ block_comment(\"} L_safepoint_poll_slow_path\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ block_comment(\"{ L_reguard\");\n-  __ bind(L_reguard);\n-\n-  spill_output_registers();\n-\n-  rt_call(masm, CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages));\n-\n-  fill_output_registers();\n-\n-  __ b(L_after_reguard);\n-\n-  __ block_comment(\"} L_reguard\");\n-\n-  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n-\n-  __ flush();\n-}\n@@ -3188,0 +2831,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":83,"deletions":439,"binary":false,"changes":522,"status":"modified"}]}