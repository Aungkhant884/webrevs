{"files":[{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2019, Arm Limited. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Arm Limited. All rights reserved.\n@@ -97,2 +97,20 @@\n-  ShouldNotCallThis();\n-  return {};\n+  oop conv_oop = JNIHandles::resolve_non_null(jconv);\n+  objArrayOop arg_regs_oop = oop_cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.arg_regs_offset));\n+  objArrayOop ret_regs_oop = oop_cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.ret_regs_offset));\n+\n+  CallRegs result;\n+  result._args_length = arg_regs_oop->length();\n+  result._arg_regs = NEW_RESOURCE_ARRAY(VMReg, result._args_length);\n+\n+  result._rets_length = ret_regs_oop->length();\n+  result._ret_regs = NEW_RESOURCE_ARRAY(VMReg, result._rets_length);\n+\n+  for (int i = 0; i < result._args_length; i++) {\n+    result._arg_regs[i] = parse_vmstorage(arg_regs_oop->obj_at(i));\n+  }\n+\n+  for (int i = 0; i < result._rets_length; i++) {\n+    result._ret_regs[i] = parse_vmstorage(ret_regs_oop->obj_at(i));\n+  }\n+\n+  return result;\n","filename":"src\/hotspot\/cpu\/aarch64\/foreign_globals_aarch64.cpp","additions":22,"deletions":4,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -118,0 +118,2 @@\n+    } else if (is_optimized_entry_frame()) {\n+      return fp_safe;\n@@ -214,0 +216,2 @@\n+    } else if (sender_blob->is_optimized_entry_blob()) {\n+      return false;\n@@ -366,2 +370,4 @@\n-  ShouldNotCallThis();\n-  return nullptr;\n+  assert(frame.is_optimized_entry_frame(), \"wrong frame\");\n+  \/\/ need unextended_sp here, since normal sp is wrong for interpreter callees\n+  return reinterpret_cast<OptimizedEntryBlob::FrameData*>(\n+    reinterpret_cast<char*>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n@@ -371,2 +377,4 @@\n-  ShouldNotCallThis();\n-  return false;\n+  assert(is_optimized_entry_frame(), \"must be optimzed entry frame\");\n+  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  return jfa->last_Java_sp() == NULL;\n@@ -376,2 +384,19 @@\n-  ShouldNotCallThis();\n-  return {};\n+  assert(map != NULL, \"map must be set\");\n+  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+  \/\/ Java frame called from C; skip all C frames and return top C\n+  \/\/ frame of that chunk as the sender\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  assert(!optimized_entry_frame_is_first(), \"must have a frame anchor to go back to\");\n+  assert(jfa->last_Java_sp() > sp(), \"must be above this frame on stack\");\n+  \/\/ Since we are walking the stack now this nested anchor is obviously walkable\n+  \/\/ even if it wasn't when it was stacked.\n+  if (!jfa->walkable()) {\n+    \/\/ Capture _last_Java_pc (if needed) and mark anchor walkable.\n+    jfa->capture_last_Java_pc();\n+  }\n+  map->clear();\n+  assert(map->include_argument_oops(), \"should be set by clear\");\n+  vmassert(jfa->last_Java_pc() != NULL, \"not walkable\");\n+  frame fr(jfa->last_Java_sp(), jfa->last_Java_fp(), jfa->last_Java_pc());\n+\n+  return fr;\n@@ -510,0 +535,2 @@\n+  if (is_optimized_entry_frame())\n+    return sender_for_optimized_entry_frame(map);\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":33,"deletions":6,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -2078,1 +2078,1 @@\n-  tbz(r0, 0, not_weak);    \/\/ Test for jweak tag.\n+  tbz(value, 0, not_weak);    \/\/ Test for jweak tag.\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2019, Arm Limited. All rights reserved.\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Arm Limited. All rights reserved.\n@@ -27,0 +27,1 @@\n+#include \"logging\/logStream.hpp\"\n@@ -29,0 +30,7 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"vmreg_aarch64.inline.hpp\"\n@@ -103,3 +111,285 @@\n-address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject mh, Method* entry, jobject jabi, jobject jconv) {\n-  ShouldNotCallThis();\n-  return nullptr;\n+\/\/ for callee saved regs, according to the caller's ABI\n+static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n+  int size = 0;\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      \/\/ Only the lower 64 bits of vector registers need to be preserved.\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  return size;\n+}\n+\n+static void preserve_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to save it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ preserve_callee_saved_regs \");\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ str(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ strd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} preserve_callee_saved_regs \");\n+}\n+\n+static void restore_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to restore it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ restore_callee_saved_regs \");\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ldr(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ldrd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} restore_callee_saved_regs \");\n+}\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry, jobject jabi, jobject jconv) {\n+  ResourceMark rm;\n+  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n+  assert(call_regs._rets_length <= 1, \"no multi reg returns\");\n+  CodeBuffer buffer(\"upcall_stub_linkToNative\", \/* code_size = *\/ 2048, \/* locs_size = *\/ 1024);\n+\n+  assert(entry->is_static(), \"static only\");\n+  \/\/ Fill in the signature array, for the calling-convention call.\n+  const int total_out_args = entry->size_of_parameters();\n+  assert(total_out_args > 0, \"receiver arg\");\n+\n+  BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_out_args);\n+  BasicType ret_type;\n+  {\n+    int i = 0;\n+    SignatureStream ss(entry->signature());\n+    for (; !ss.at_return_type(); ss.next()) {\n+      out_sig_bt[i++] = ss.type();  \/\/ Collect remaining bits of signature\n+      if (ss.type() == T_LONG || ss.type() == T_DOUBLE)\n+        out_sig_bt[i++] = T_VOID;   \/\/ Longs & doubles take 2 Java slots\n+    }\n+    assert(i == total_out_args, \"\");\n+    ret_type = ss.type();\n+  }\n+  \/\/ skip receiver\n+  BasicType* in_sig_bt = out_sig_bt + 1;\n+  int total_in_args = total_out_args - 1;\n+\n+  Register shuffle_reg = r19;\n+  JavaCallConv out_conv;\n+  NativeCallConv in_conv(call_regs._arg_regs, call_regs._args_length);\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &in_conv, &out_conv, shuffle_reg->as_VMReg());\n+  int stack_slots = SharedRuntime::out_preserve_stack_slots() + arg_shuffle.out_arg_stack_slots();\n+  int out_arg_area = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ out_arg_area (for stack arguments) doubles as shadow space for native calls.\n+  \/\/ make sure it is big enough.\n+  if (out_arg_area < frame::arg_reg_save_area_bytes) {\n+    out_arg_area = frame::arg_reg_save_area_bytes;\n+  }\n+\n+  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  RegSpiller arg_spilller(call_regs._arg_regs, call_regs._args_length);\n+  RegSpiller result_spiller(call_regs._ret_regs, call_regs._rets_length);\n+  \/\/ To spill receiver during deopt\n+  int deopt_spill_size = 1 * BytesPerWord;\n+\n+  int shuffle_area_offset    = 0;\n+  int deopt_spill_offset     = shuffle_area_offset    + out_arg_area;\n+  int res_save_area_offset   = deopt_spill_offset     + deopt_spill_size;\n+  int arg_save_area_offset   = res_save_area_offset   + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset   = arg_save_area_offset   + arg_spilller.spill_size_bytes();\n+  int frame_data_offset      = reg_save_area_offset   + reg_save_area_size;\n+  int frame_bottom_offset    = frame_data_offset      + sizeof(OptimizedEntryBlob::FrameData);\n+\n+  int frame_size = frame_bottom_offset;\n+  frame_size = align_up(frame_size, StackAlignmentInBytes);\n+\n+  \/\/ The space we have allocated will look like:\n+  \/\/\n+  \/\/\n+  \/\/ FP-> |                     |\n+  \/\/      |---------------------| = frame_bottom_offset = frame_size\n+  \/\/      |                     |\n+  \/\/      | FrameData           |\n+  \/\/      |---------------------| = frame_data_offset\n+  \/\/      |                     |\n+  \/\/      | reg_save_area       |\n+  \/\/      |---------------------| = reg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | arg_save_area       |\n+  \/\/      |---------------------| = arg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | res_save_area       |\n+  \/\/      |---------------------| = res_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | deopt_spill         |\n+  \/\/      |---------------------| = deopt_spill_offset\n+  \/\/      |                     |\n+  \/\/ SP-> | out_arg_area        |   needs to be at end for shadow space\n+  \/\/\n+  \/\/\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  address start = __ pc();\n+  __ enter(); \/\/ set up frame\n+  assert((abi._stack_alignment_bytes % 16) == 0, \"must be 16 byte aligned\");\n+  \/\/ allocate frame (frame_size is also aligned, so stack is still aligned)\n+  __ sub(sp, sp, frame_size);\n+\n+  \/\/ we have to always spill args since we need to do a call to get the thread\n+  \/\/ (and maybe attach it).\n+  arg_spilller.generate_spill(_masm, arg_save_area_offset);\n+  preserve_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  __ block_comment(\"{ on_entry\");\n+  __ lea(c_rarg0, Address(sp, frame_data_offset));\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::on_entry));\n+  __ blr(rscratch1);\n+  __ mov(rthread, r0);\n+  __ reinit_heapbase();\n+  __ block_comment(\"} on_entry\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_spilller.generate_fill(_masm, arg_save_area_offset);\n+  arg_shuffle.generate(_masm, shuffle_reg->as_VMReg(), abi._shadow_space_bytes, 0);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ block_comment(\"{ receiver \");\n+  __ movptr(shuffle_reg, (intptr_t)receiver);\n+  __ resolve_jobject(shuffle_reg, rthread, rscratch2);\n+  __ mov(j_rarg0, shuffle_reg);\n+  __ block_comment(\"} receiver \");\n+\n+  __ mov_metadata(rmethod, entry);\n+  __ str(rmethod, Address(rthread, JavaThread::callee_target_offset())); \/\/ just in case callee is deoptimized\n+\n+  __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+  __ blr(rscratch1);\n+\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n+\n+  __ block_comment(\"{ on_exit\");\n+  __ lea(c_rarg0, Address(sp, frame_data_offset));\n+  \/\/ stack already aligned\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::on_exit));\n+  __ blr(rscratch1);\n+  __ block_comment(\"} on_exit\");\n+\n+  restore_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n+\n+  \/\/ return value shuffle\n+#ifdef ASSERT\n+  if (call_regs._rets_length == 1) { \/\/ 0 or 1\n+    VMReg j_expected_result_reg;\n+    switch (ret_type) {\n+      case T_BOOLEAN:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_CHAR:\n+      case T_INT:\n+      case T_LONG:\n+       j_expected_result_reg = r0->as_VMReg();\n+       break;\n+      case T_FLOAT:\n+      case T_DOUBLE:\n+        j_expected_result_reg = v0->as_VMReg();\n+        break;\n+      default:\n+        fatal(\"unexpected return type: %s\", type2name(ret_type));\n+    }\n+    \/\/ No need to move for now, since CallArranger can pick a return type\n+    \/\/ that goes in the same reg for both CCs. But, at least assert they are the same\n+    assert(call_regs._ret_regs[0] == j_expected_result_reg,\n+     \"unexpected result register: %s != %s\", call_regs._ret_regs[0]->name(), j_expected_result_reg->name());\n+  }\n+#endif\n+\n+  __ leave();\n+  __ ret(lr);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ exception handler\");\n+\n+  intptr_t exception_handler_offset = __ pc() - start;\n+\n+  \/\/ Native caller has no idea how to handle exceptions,\n+  \/\/ so we just crash here. Up to callee to catch exceptions.\n+  __ verify_oop(r0);\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::handle_uncaught_exception));\n+  __ blr(rscratch1);\n+  __ should_not_reach_here();\n+\n+  __ block_comment(\"} exception handler\");\n+\n+  _masm->flush();\n+\n+#ifndef PRODUCT\n+  stringStream ss;\n+  ss.print(\"optimized_upcall_stub_%s\", entry->signature()->as_C_string());\n+  const char* name = _masm->code_string(ss.as_string());\n+#else \/\/ PRODUCT\n+  const char* name = \"optimized_upcall_stub\";\n+#endif \/\/ PRODUCT\n+\n+  OptimizedEntryBlob* blob = OptimizedEntryBlob::create(name, &buffer, exception_handler_offset, receiver, in_ByteSize(frame_data_offset));\n+\n+  if (TraceOptimizedUpcallStubs) {\n+    blob->print_on(tty);\n+  }\n+\n+  return blob->code_begin();\n@@ -109,1 +399,1 @@\n-  return false;\n+  return true;\n","filename":"src\/hotspot\/cpu\/aarch64\/universalUpcallHandler_aarch64.cpp","additions":296,"deletions":6,"binary":false,"changes":302,"status":"modified"},{"patch":"@@ -228,2 +228,2 @@\n-BufferBlob::BufferBlob(const char* name, int size)\n-: RuntimeBlob(name, sizeof(BufferBlob), size, CodeOffsets::frame_never_safe, \/*locs_size:*\/ 0)\n+BufferBlob::BufferBlob(const char* name, int header_size, int size)\n+: RuntimeBlob(name, header_size, size, CodeOffsets::frame_never_safe, \/*locs_size:*\/ 0)\n@@ -243,1 +243,1 @@\n-    blob = new (size) BufferBlob(name, size);\n+    blob = new (size) BufferBlob(name, sizeof(BufferBlob), size);\n@@ -252,2 +252,2 @@\n-BufferBlob::BufferBlob(const char* name, int size, CodeBuffer* cb)\n-  : RuntimeBlob(name, cb, sizeof(BufferBlob), size, CodeOffsets::frame_never_safe, 0, NULL)\n+BufferBlob::BufferBlob(const char* name, int header_size, int size, CodeBuffer* cb)\n+  : RuntimeBlob(name, cb, header_size, size, CodeOffsets::frame_never_safe, 0, NULL)\n@@ -264,1 +264,1 @@\n-    blob = new (size) BufferBlob(name, size, cb);\n+    blob = new (size) BufferBlob(name, sizeof(BufferBlob), size, cb);\n@@ -293,1 +293,1 @@\n-  BufferBlob(\"I2C\/C2I adapters\", size, cb) {\n+  BufferBlob(\"I2C\/C2I adapters\", sizeof(AdapterBlob), size, cb) {\n@@ -323,1 +323,1 @@\n-  BufferBlob(name, size) {\n+  BufferBlob(name, sizeof(VtableBlob), size) {\n@@ -724,1 +724,1 @@\n-  BufferBlob(name, size, cb),\n+  BufferBlob(name, sizeof(OptimizedEntryBlob), size, cb),\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":9,"deletions":9,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -404,2 +404,2 @@\n-  BufferBlob(const char* name, int size);\n-  BufferBlob(const char* name, int size, CodeBuffer* cb);\n+  BufferBlob(const char* name, int header_size, int size);\n+  BufferBlob(const char* name, int header_size, int size, CodeBuffer* cb);\n@@ -468,1 +468,1 @@\n-  MethodHandlesAdapterBlob(int size)                 : BufferBlob(\"MethodHandles adapters\", size) {}\n+  MethodHandlesAdapterBlob(int size): BufferBlob(\"MethodHandles adapters\", sizeof(MethodHandlesAdapterBlob), size) {}\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -106,2 +106,0 @@\n-  MACOS_AARCH64_ONLY(thread->enable_wx(WXExec));\n-\n@@ -116,2 +114,0 @@\n-  MACOS_AARCH64_ONLY(thread->enable_wx(WXWrite));\n-\n","filename":"src\/hotspot\/share\/prims\/universalUpcallHandler.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1548,1 +1548,1 @@\n-      ::new (blob) BufferBlob(\"WB::DummyBlob\", full_size);\n+      ::new (blob) BufferBlob(\"WB::DummyBlob\", sizeof(BufferBlob), full_size);\n","filename":"src\/hotspot\/share\/prims\/whitebox.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -540,0 +540,1 @@\n+java\/foreign\/TestUpcall.java#stack 8275584 macosx-aarch64\n","filename":"test\/jdk\/ProblemList.txt","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"}]}