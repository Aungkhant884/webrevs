{"files":[{"patch":"@@ -0,0 +1,168 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"runtime\/jniHandles.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"oops\/typeArrayOop.inline.hpp\"\n+#include \"oops\/oopCast.inline.hpp\"\n+#include \"opto\/matcher.hpp\"\n+#include \"prims\/foreign_globals.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+\n+bool ABIDescriptor::is_volatile_reg(Register reg) const {\n+  return _integer_argument_registers.contains(reg)\n+    || _integer_additional_volatile_registers.contains(reg);\n+}\n+\n+bool ABIDescriptor::is_volatile_reg(FloatRegister reg) const {\n+    return _vector_argument_registers.contains(reg)\n+        || _vector_additional_volatile_registers.contains(reg);\n+}\n+\n+#define INTEGER_TYPE 0\n+#define VECTOR_TYPE 1\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n+  oop abi_oop = JNIHandles::resolve_non_null(jabi);\n+  ABIDescriptor abi;\n+  constexpr Register (*to_Register)(int) = as_Register;\n+\n+  objArrayOop inputStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.inputStorage_offset));\n+  loadArray(inputStorage, INTEGER_TYPE, abi._integer_argument_registers, to_Register);\n+  loadArray(inputStorage, VECTOR_TYPE, abi._vector_argument_registers, as_FloatRegister);\n+\n+  objArrayOop outputStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.outputStorage_offset));\n+  loadArray(outputStorage, INTEGER_TYPE, abi._integer_return_registers, to_Register);\n+  loadArray(outputStorage, VECTOR_TYPE, abi._vector_return_registers, as_FloatRegister);\n+\n+  objArrayOop volatileStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.volatileStorage_offset));\n+  loadArray(volatileStorage, INTEGER_TYPE, abi._integer_additional_volatile_registers, to_Register);\n+  loadArray(volatileStorage, VECTOR_TYPE, abi._vector_additional_volatile_registers, as_FloatRegister);\n+\n+  abi._stack_alignment_bytes = abi_oop->int_field(ABI.stackAlignment_offset);\n+  abi._shadow_space_bytes = abi_oop->int_field(ABI.shadowSpace_offset);\n+\n+  abi._target_addr_reg = parse_vmstorage(abi_oop->obj_field(ABI.targetAddrStorage_offset))->as_Register();\n+  abi._ret_buf_addr_reg = parse_vmstorage(abi_oop->obj_field(ABI.retBufAddrStorage_offset))->as_Register();\n+\n+  return abi;\n+}\n+\n+enum class RegType {\n+  INTEGER = 0,\n+  VECTOR = 1,\n+  STACK = 3\n+};\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  switch(static_cast<RegType>(type)) {\n+    case RegType::INTEGER: return ::as_Register(index)->as_VMReg();\n+    case RegType::VECTOR: return ::as_FloatRegister(index)->as_VMReg();\n+    case RegType::STACK: return VMRegImpl::stack2reg(index LP64_ONLY(* 2));\n+  }\n+  return VMRegImpl::Bad();\n+}\n+\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  if (reg->is_Register()) {\n+    return 8;\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      return Matcher::scalable_vector_reg_size(T_BYTE);\n+    }\n+    return 16;\n+  }\n+  return 0; \/\/ stack and BAD\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->spill(reg->as_Register(), true, offset);\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      masm->spill_sve_vector(reg->as_FloatRegister(), offset, Matcher::scalable_vector_reg_size(T_BYTE));\n+    } else {\n+      masm->spill(reg->as_FloatRegister(), masm->Q, offset);\n+    }\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->unspill(reg->as_Register(), true, offset);\n+  } else if (reg->is_FloatRegister()) {\n+    bool use_sve = Matcher::supports_scalable_vector();\n+    if (use_sve) {\n+      masm->unspill_sve_vector(reg->as_FloatRegister(), offset, Matcher::scalable_vector_reg_size(T_BYTE));\n+    } else {\n+      masm->unspill(reg->as_FloatRegister(), masm->Q, offset);\n+    }\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+  assert(in_stk_bias == 0 && out_stk_bias == 0, \"bias not implemented\");\n+  Register tmp_reg = tmp->as_Register();\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    masm->block_comment(err_msg(\"bt=%s\", null_safe_string(type2name(arg_bt))));\n+    switch (arg_bt) {\n+      case T_BOOLEAN:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_CHAR:\n+      case T_INT:\n+        masm->move32_64(from_vmreg, to_vmreg, tmp_reg);\n+        break;\n+\n+      case T_FLOAT:\n+        masm->float_move(from_vmreg, to_vmreg, tmp_reg);\n+        break;\n+\n+      case T_DOUBLE:\n+        masm->double_move(from_vmreg, to_vmreg, tmp_reg);\n+        break;\n+\n+      case T_LONG :\n+        masm->long_move(from_vmreg, to_vmreg, tmp_reg);\n+        break;\n+\n+      default:\n+        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/foreign_globals_aarch64.cpp","additions":168,"deletions":0,"binary":false,"changes":168,"status":"added"},{"patch":"@@ -121,0 +121,1 @@\n+<<<<<<< HEAD\n@@ -122,0 +123,3 @@\n+=======\n+    } else if (is_optimized_entry_frame()) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -225,0 +229,1 @@\n+<<<<<<< HEAD\n@@ -226,0 +231,3 @@\n+=======\n+    } else if (sender_blob->is_optimized_entry_blob()) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -375,0 +383,1 @@\n+<<<<<<< HEAD\n@@ -385,0 +394,12 @@\n+=======\n+OptimizedEntryBlob::FrameData* OptimizedEntryBlob::frame_data_for_frame(const frame& frame) const {\n+  assert(frame.is_optimized_entry_frame(), \"wrong frame\");\n+  \/\/ need unextended_sp here, since normal sp is wrong for interpreter callees\n+  return reinterpret_cast<OptimizedEntryBlob::FrameData*>(\n+    reinterpret_cast<char*>(frame.unextended_sp()) + in_bytes(_frame_data_offset));\n+}\n+\n+bool frame::optimized_entry_frame_is_first() const {\n+  assert(is_optimized_entry_frame(), \"must be optimzed entry frame\");\n+  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -389,0 +410,1 @@\n+<<<<<<< HEAD\n@@ -402,0 +424,19 @@\n+=======\n+frame frame::sender_for_optimized_entry_frame(RegisterMap* map) const {\n+  assert(map != NULL, \"map must be set\");\n+  OptimizedEntryBlob* blob = _cb->as_optimized_entry_blob();\n+  \/\/ Java frame called from C; skip all C frames and return top C\n+  \/\/ frame of that chunk as the sender\n+  JavaFrameAnchor* jfa = blob->jfa_for_frame(*this);\n+  assert(!optimized_entry_frame_is_first(), \"must have a frame anchor to go back to\");\n+  assert(jfa->last_Java_sp() > sp(), \"must be above this frame on stack\");\n+  \/\/ Since we are walking the stack now this nested anchor is obviously walkable\n+  \/\/ even if it wasn't when it was stacked.\n+  if (!jfa->walkable()) {\n+    \/\/ Capture _last_Java_pc (if needed) and mark anchor walkable.\n+    jfa->capture_last_Java_pc();\n+  }\n+  map->clear();\n+  assert(map->include_argument_oops(), \"should be set by clear\");\n+  vmassert(jfa->last_Java_pc() != NULL, \"not walkable\");\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -475,0 +516,49 @@\n+<<<<<<< HEAD\n+=======\n+\n+    \/\/ Since the prolog does the save and restore of FP there is no\n+    \/\/ oopmap for it so we must fill in its location as if there was\n+    \/\/ an oopmap entry since if our caller was compiled code there\n+    \/\/ could be live jvm state in it.\n+    update_map_with_saved_link(map, saved_fp_addr);\n+  }\n+\n+  return frame(l_sender_sp, unextended_sp, *saved_fp_addr, sender_pc);\n+}\n+\n+\/\/------------------------------------------------------------------------------\n+\/\/ frame::sender_raw\n+frame frame::sender_raw(RegisterMap* map) const {\n+  \/\/ Default is we done have to follow them. The sender_for_xxx will\n+  \/\/ update it accordingly\n+   map->set_include_argument_oops(false);\n+\n+  if (is_entry_frame())\n+    return sender_for_entry_frame(map);\n+  if (is_optimized_entry_frame())\n+    return sender_for_optimized_entry_frame(map);\n+  if (is_interpreted_frame())\n+    return sender_for_interpreter_frame(map);\n+  assert(_cb == CodeCache::find_blob(pc()),\"Must be the same\");\n+\n+  \/\/ This test looks odd: why is it not is_compiled_frame() ?  That's\n+  \/\/ because stubs also have OOP maps.\n+  if (_cb != NULL) {\n+    return sender_for_compiled_frame(map);\n+  }\n+\n+  \/\/ Must be native-compiled frame, i.e. the marshaling code for native\n+  \/\/ methods that exists in the core system.\n+\n+  \/\/ Native code may or may not have signed the return address, we have no way to be sure or what\n+  \/\/ signing methods they used. Instead, just ensure the stripped value is used.\n+\n+  return frame(sender_sp(), link(), sender_pc());\n+}\n+\n+frame frame::sender(RegisterMap* map) const {\n+  frame result = sender_raw(map);\n+\n+  if (map->process_frames()) {\n+    StackWatermarkSet::on_iteration(map->thread(), result);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/cpu\/aarch64\/frame_aarch64.cpp","additions":90,"deletions":0,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -315,0 +315,13 @@\n+<<<<<<< HEAD\n+=======\n+  }\n+}\n+\n+void MacroAssembler::rt_call(address dest, Register tmp) {\n+  CodeBlob *cb = CodeCache::find_blob(dest);\n+  if (cb) {\n+    far_call(RuntimeAddress(dest));\n+  } else {\n+    lea(tmp, RuntimeAddress(dest));\n+    blr(tmp);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -5538,0 +5551,1 @@\n+<<<<<<< HEAD\n@@ -5541,0 +5555,5 @@\n+=======\n+\/\/ On 64 bit we will store integer like items to the stack as\n+\/\/ 64 bits items (Aarch64 abi) even though java would only store\n+\/\/ 32bits for a parameter. On 32bit it will simply be 32 bits\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -5554,0 +5573,5 @@\n+<<<<<<< HEAD\n+=======\n+    \/\/ Do we really have to sign extend???\n+    \/\/ __ movslq(src.first()->as_Register(), src.first()->as_Register());\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -5683,0 +5707,1 @@\n+<<<<<<< HEAD\n@@ -5684,0 +5709,3 @@\n+=======\n+  if (src.first()->is_stack()) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -269,0 +269,1 @@\n+<<<<<<< HEAD\n@@ -270,0 +271,3 @@\n+=======\n+                    Address(nep_reg, NONZERO(jdk_internal_invoke_NativeEntryPoint::invoker_offset_in_bytes())),\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/cpu\/aarch64\/methodHandles_aarch64.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,322 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"compiler\/oopMap.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"prims\/universalNativeInvoker.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+\n+#define __ _masm->\n+\n+class NativeInvokerGenerator : public StubCodeGenerator {\n+  BasicType* _signature;\n+  int _num_args;\n+  BasicType _ret_bt;\n+  const ABIDescriptor& _abi;\n+\n+  const GrowableArray<VMReg>& _input_registers;\n+  const GrowableArray<VMReg>& _output_registers;\n+\n+  bool _needs_return_buffer;\n+\n+  int _frame_complete;\n+  int _framesize;\n+  OopMapSet* _oop_maps;\n+public:\n+  NativeInvokerGenerator(CodeBuffer* buffer,\n+                         BasicType* signature,\n+                         int num_args,\n+                         BasicType ret_bt,\n+                         const ABIDescriptor& abi,\n+                         const GrowableArray<VMReg>& input_registers,\n+                         const GrowableArray<VMReg>& output_registers,\n+                         bool needs_return_buffer)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _signature(signature),\n+     _num_args(num_args),\n+     _ret_bt(ret_bt),\n+     _abi(abi),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers),\n+     _needs_return_buffer(needs_return_buffer),\n+     _frame_complete(0),\n+     _framesize(0),\n+     _oop_maps(NULL) {\n+  }\n+\n+  void generate();\n+\n+  int frame_complete() const {\n+    return _frame_complete;\n+  }\n+\n+  int framesize() const {\n+    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n+  }\n+\n+  OopMapSet* oop_maps() const {\n+    return _oop_maps;\n+  }\n+};\n+\n+static const int native_invoker_code_size = 1024;\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+  int locs_size  = 64;\n+  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n+  NativeInvokerGenerator g(&code, signature, num_args, ret_bt, abi, input_registers, output_registers, needs_return_buffer);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n+\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n+                                  &code,\n+                                  g.frame_complete(),\n+                                  g.framesize(),\n+                                  g.oop_maps(), false);\n+\n+  if (TraceNativeInvokers) {\n+    stub->print_on(tty);\n+  }\n+\n+  return stub;\n+}\n+\n+void NativeInvokerGenerator::generate() {\n+  enum layout {\n+    rfp_off,\n+    rfp_off2,\n+    lr_off,\n+    lr_off2,\n+    framesize \/\/ inclusive of return address\n+    \/\/ The following are also computed dynamically:\n+    \/\/ spill area for return value\n+    \/\/ out arg area (e.g. for stack args)\n+  };\n+\n+  \/\/ we can't use rscratch1 because it is r8, and used by the ABI\n+  Register tmp1 = r9;\n+  Register tmp2 = r10;\n+\n+  Register shuffle_reg = r19;\n+  JavaCallConv in_conv;\n+  NativeCallConv out_conv(_input_registers);\n+  ArgumentShuffle arg_shuffle(_signature, _num_args, _signature, _num_args, &in_conv, &out_conv, shuffle_reg->as_VMReg());\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  int allocated_frame_size = 0;\n+  if (_needs_return_buffer) {\n+    allocated_frame_size += 8; \/\/ for address spill\n+  }\n+  allocated_frame_size += arg_shuffle.out_arg_stack_slots() <<LogBytesPerInt;\n+  assert(_abi._shadow_space_bytes == 0, \"not expecting shadow space on AArch64\");\n+\n+  int ret_buf_addr_sp_offset = -1;\n+  if (_needs_return_buffer) {\n+     \/\/ in sync with the above\n+     ret_buf_addr_sp_offset = allocated_frame_size - 8;\n+  }\n+\n+  RegSpiller out_reg_spiller(_output_registers);\n+  int spill_offset = -1;\n+\n+  if (!_needs_return_buffer) {\n+    spill_offset = 0;\n+    \/\/ spill area can be shared with the above, so we take the max of the 2\n+    allocated_frame_size = out_reg_spiller.spill_size_bytes() > allocated_frame_size\n+      ? out_reg_spiller.spill_size_bytes()\n+      : allocated_frame_size;\n+  }\n+\n+  _framesize = align_up(framesize\n+    + (allocated_frame_size >> LogBytesPerInt), 4);\n+  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n+\n+  _oop_maps  = new OopMapSet();\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ lr and fp are already in place\n+  __ sub(sp, rfp, ((unsigned)_framesize-4) << LogBytesPerInt); \/\/ prolog\n+\n+  _frame_complete = __ pc() - start;\n+\n+  address the_pc = __ pc();\n+  __ set_last_Java_frame(sp, rfp, the_pc, tmp1);\n+  OopMap* map = new OopMap(_framesize, 0);\n+  _oop_maps->add_gc_map(the_pc - start, map);\n+\n+  \/\/ State transition\n+  __ mov(tmp1, _thread_in_native);\n+  __ lea(tmp2, Address(rthread, JavaThread::thread_state_offset()));\n+  __ stlrw(tmp1, tmp2);\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_shuffle.generate(_masm, shuffle_reg->as_VMReg(), 0, _abi._shadow_space_bytes);\n+  if (_needs_return_buffer) {\n+    assert(ret_buf_addr_sp_offset != -1, \"no return buffer addr spill\");\n+    __ str(_abi._ret_buf_addr_reg, Address(sp, ret_buf_addr_sp_offset));\n+  }\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ blr(_abi._target_addr_reg);\n+  \/\/ this call is assumed not to have killed rthread\n+\n+  if (!_needs_return_buffer) {\n+    \/\/ Unpack native results.\n+    switch (_ret_bt) {\n+      case T_BOOLEAN: __ c2bool(r0);                     break;\n+      case T_CHAR   : __ ubfx(r0, r0, 0, 16);            break;\n+      case T_BYTE   : __ sbfx(r0, r0, 0, 8);             break;\n+      case T_SHORT  : __ sbfx(r0, r0, 0, 16);            break;\n+      case T_INT    : __ sbfx(r0, r0, 0, 32);            break;\n+      case T_DOUBLE :\n+      case T_FLOAT  :\n+        \/\/ Result is in v0 we'll save as needed\n+        break;\n+      case T_VOID: break;\n+      case T_LONG: break;\n+      default       : ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(ret_buf_addr_sp_offset != -1, \"no return buffer addr spill\");\n+    __ ldr(tmp1, Address(sp, ret_buf_addr_sp_offset));\n+    int offset = 0;\n+    for (int i = 0; i < _output_registers.length(); i++) {\n+      VMReg reg = _output_registers.at(i);\n+      if (reg->is_Register()) {\n+        __ str(reg->as_Register(), Address(tmp1, offset));\n+        offset += 8;\n+      } else if(reg->is_FloatRegister()) {\n+        __ strd(reg->as_FloatRegister(), Address(tmp1, offset));\n+        offset += 16;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  __ mov(tmp1, _thread_in_native_trans);\n+  __ strw(tmp1, Address(rthread, JavaThread::thread_state_offset()));\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(Assembler::LoadLoad | Assembler::LoadStore |\n+            Assembler::StoreLoad | Assembler::StoreStore);\n+\n+  __ verify_sve_vector_length(tmp1);\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, true \/* at_return *\/, true \/* acquire *\/, false \/* in_nmethod *\/, tmp1);\n+\n+  __ ldrw(tmp1, Address(rthread, JavaThread::suspend_flags_offset()));\n+  __ cbnzw(tmp1, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ mov(tmp1, _thread_in_Java);\n+  __ lea(tmp2, Address(rthread, JavaThread::thread_state_offset()));\n+  __ stlrw(tmp1, tmp2);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ ldrb(tmp1, Address(rthread, JavaThread::stack_guard_state_offset()));\n+  __ cmpw(tmp1, StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ br(Assembler::EQ, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(true);\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(lr);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+\n+  if (!_needs_return_buffer) {\n+    \/\/ Need to save the native result registers around any runtime calls.\n+    out_reg_spiller.generate_spill(_masm, spill_offset);\n+  }\n+\n+  __ mov(c_rarg0, rthread);\n+  assert(frame::arg_reg_save_area_bytes == 0, \"not expecting frame reg save area\");\n+  __ lea(tmp1, RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n+  __ blr(tmp1);\n+\n+  if (!_needs_return_buffer) {\n+    out_reg_spiller.generate_fill(_masm, spill_offset);\n+  }\n+\n+  __ b(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+\n+  if (!_needs_return_buffer) {\n+    out_reg_spiller.generate_spill(_masm, spill_offset);\n+  }\n+\n+  __ rt_call(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages), tmp1);\n+\n+  if (!_needs_return_buffer) {\n+    out_reg_spiller.generate_fill(_masm, spill_offset);\n+  }\n+\n+  __ b(L_after_reguard);\n+\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/universalNativeInvoker_aarch64.cpp","additions":322,"deletions":0,"binary":false,"changes":322,"status":"added"},{"patch":"@@ -0,0 +1,343 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2021, Arm Limited. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"prims\/universalUpcallHandler.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+#include \"vmreg_aarch64.inline.hpp\"\n+\n+#define __ _masm->\n+\n+\/\/ for callee saved regs, according to the caller's ABI\n+static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n+  int size = 0;\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      \/\/ Only the lower 64 bits of vector registers need to be preserved.\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  return size;\n+}\n+\n+static void preserve_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to save it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ preserve_callee_saved_regs \");\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ str(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ strd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} preserve_callee_saved_regs \");\n+}\n+\n+static void restore_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to restore it here\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ restore_callee_saved_regs \");\n+  for (int i = 0; i < RegisterImpl::number_of_registers; i++) {\n+    Register reg = as_Register(i);\n+    if (reg == rfp || reg == sp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ldr(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  for (int i = 0; i < FloatRegisterImpl::number_of_registers; i++) {\n+    FloatRegister reg = as_FloatRegister(i);\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ ldrd(reg, Address(sp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  __ block_comment(\"} restore_callee_saved_regs \");\n+}\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+  ResourceMark rm;\n+  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n+  CodeBuffer buffer(\"upcall_stub_linkToNative\", \/* code_size = *\/ 2048, \/* locs_size = *\/ 1024);\n+\n+  Register shuffle_reg = r19;\n+  JavaCallConv out_conv;\n+  NativeCallConv in_conv(call_regs._arg_regs, call_regs._args_length);\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &in_conv, &out_conv, shuffle_reg->as_VMReg());\n+  int stack_slots = SharedRuntime::out_preserve_stack_slots() + arg_shuffle.out_arg_stack_slots();\n+  int out_arg_area = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ out_arg_area (for stack arguments) doubles as shadow space for native calls.\n+  \/\/ make sure it is big enough.\n+  if (out_arg_area < frame::arg_reg_save_area_bytes) {\n+    out_arg_area = frame::arg_reg_save_area_bytes;\n+  }\n+\n+  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  RegSpiller arg_spilller(call_regs._arg_regs, call_regs._args_length);\n+  RegSpiller result_spiller(call_regs._ret_regs, call_regs._rets_length);\n+  \/\/ To spill receiver during deopt\n+  int deopt_spill_size = 1 * BytesPerWord;\n+\n+  int shuffle_area_offset    = 0;\n+  int deopt_spill_offset     = shuffle_area_offset    + out_arg_area;\n+  int res_save_area_offset   = deopt_spill_offset     + deopt_spill_size;\n+  int arg_save_area_offset   = res_save_area_offset   + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset   = arg_save_area_offset   + arg_spilller.spill_size_bytes();\n+  int frame_data_offset      = reg_save_area_offset   + reg_save_area_size;\n+  int frame_bottom_offset    = frame_data_offset      + sizeof(OptimizedEntryBlob::FrameData);\n+\n+  int ret_buf_offset = -1;\n+  if (needs_return_buffer) {\n+    ret_buf_offset = frame_bottom_offset;\n+    frame_bottom_offset += ret_buf_size;\n+  }\n+\n+  int frame_size = frame_bottom_offset;\n+  frame_size = align_up(frame_size, StackAlignmentInBytes);\n+\n+  \/\/ The space we have allocated will look like:\n+  \/\/\n+  \/\/\n+  \/\/ FP-> |                     |\n+  \/\/      |---------------------| = frame_bottom_offset = frame_size\n+  \/\/      | (optional)          |\n+  \/\/      | ret_buf             |\n+  \/\/      |---------------------| = ret_buf_offset\n+  \/\/      |                     |\n+  \/\/      | FrameData           |\n+  \/\/      |---------------------| = frame_data_offset\n+  \/\/      |                     |\n+  \/\/      | reg_save_area       |\n+  \/\/      |---------------------| = reg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | arg_save_area       |\n+  \/\/      |---------------------| = arg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | res_save_area       |\n+  \/\/      |---------------------| = res_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | deopt_spill         |\n+  \/\/      |---------------------| = deopt_spill_offset\n+  \/\/      |                     |\n+  \/\/ SP-> | out_arg_area        |   needs to be at end for shadow space\n+  \/\/\n+  \/\/\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  address start = __ pc();\n+  __ enter(); \/\/ set up frame\n+  assert((abi._stack_alignment_bytes % 16) == 0, \"must be 16 byte aligned\");\n+  \/\/ allocate frame (frame_size is also aligned, so stack is still aligned)\n+  __ sub(sp, sp, frame_size);\n+\n+  \/\/ we have to always spill args since we need to do a call to get the thread\n+  \/\/ (and maybe attach it).\n+  arg_spilller.generate_spill(_masm, arg_save_area_offset);\n+  preserve_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  __ block_comment(\"{ on_entry\");\n+  __ lea(c_rarg0, Address(sp, frame_data_offset));\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::on_entry));\n+  __ blr(rscratch1);\n+  __ mov(rthread, r0);\n+  __ reinit_heapbase();\n+  __ block_comment(\"} on_entry\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_spilller.generate_fill(_masm, arg_save_area_offset);\n+  if (needs_return_buffer) {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(abi._ret_buf_addr_reg, Address(sp, ret_buf_offset));\n+  }\n+  arg_shuffle.generate(_masm, shuffle_reg->as_VMReg(), abi._shadow_space_bytes, 0);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ block_comment(\"{ receiver \");\n+  __ movptr(shuffle_reg, (intptr_t)receiver);\n+  __ resolve_jobject(shuffle_reg, rthread, rscratch2);\n+  __ mov(j_rarg0, shuffle_reg);\n+  __ block_comment(\"} receiver \");\n+\n+  __ mov_metadata(rmethod, entry);\n+  __ str(rmethod, Address(rthread, JavaThread::callee_target_offset())); \/\/ just in case callee is deoptimized\n+\n+  __ ldr(rscratch1, Address(rmethod, Method::from_compiled_offset()));\n+  __ blr(rscratch1);\n+\n+    \/\/ return value shuffle\n+  if (!needs_return_buffer) {\n+#ifdef ASSERT\n+    if (call_regs._rets_length == 1) { \/\/ 0 or 1\n+      VMReg j_expected_result_reg;\n+      switch (ret_type) {\n+        case T_BOOLEAN:\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_CHAR:\n+        case T_INT:\n+        case T_LONG:\n+        j_expected_result_reg = r0->as_VMReg();\n+        break;\n+        case T_FLOAT:\n+        case T_DOUBLE:\n+          j_expected_result_reg = v0->as_VMReg();\n+          break;\n+        default:\n+          fatal(\"unexpected return type: %s\", type2name(ret_type));\n+      }\n+      \/\/ No need to move for now, since CallArranger can pick a return type\n+      \/\/ that goes in the same reg for both CCs. But, at least assert they are the same\n+      assert(call_regs._ret_regs[0] == j_expected_result_reg,\n+      \"unexpected result register: %s != %s\", call_regs._ret_regs[0]->name(), j_expected_result_reg->name());\n+    }\n+#endif\n+  } else {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(rscratch1, Address(sp, ret_buf_offset));\n+    int offset = 0;\n+    for (int i = 0; i < call_regs._rets_length; i++) {\n+      VMReg reg = call_regs._ret_regs[i];\n+      if (reg->is_Register()) {\n+        __ ldr(reg->as_Register(), Address(rscratch1, offset));\n+        offset += 8;\n+      } else if (reg->is_FloatRegister()) {\n+        __ ldrd(reg->as_FloatRegister(), Address(rscratch1, offset));\n+        offset += 16; \/\/ needs to match VECTOR_REG_SIZE in AArch64Architecture (Java)\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n+\n+  __ block_comment(\"{ on_exit\");\n+  __ lea(c_rarg0, Address(sp, frame_data_offset));\n+  \/\/ stack already aligned\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::on_exit));\n+  __ blr(rscratch1);\n+  __ block_comment(\"} on_exit\");\n+\n+  restore_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n+\n+  __ leave();\n+  __ ret(lr);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ exception handler\");\n+\n+  intptr_t exception_handler_offset = __ pc() - start;\n+\n+  \/\/ Native caller has no idea how to handle exceptions,\n+  \/\/ so we just crash here. Up to callee to catch exceptions.\n+  __ verify_oop(r0);\n+  __ movptr(rscratch1, CAST_FROM_FN_PTR(uint64_t, ProgrammableUpcallHandler::handle_uncaught_exception));\n+  __ blr(rscratch1);\n+  __ should_not_reach_here();\n+\n+  __ block_comment(\"} exception handler\");\n+\n+  _masm->flush();\n+\n+#ifndef PRODUCT\n+  stringStream ss;\n+  ss.print(\"optimized_upcall_stub_%s\", entry->signature()->as_C_string());\n+  const char* name = _masm->code_string(ss.as_string());\n+#else \/\/ PRODUCT\n+  const char* name = \"optimized_upcall_stub\";\n+#endif \/\/ PRODUCT\n+\n+  OptimizedEntryBlob* blob\n+    = OptimizedEntryBlob::create(name,\n+                                 &buffer,\n+                                 exception_handler_offset,\n+                                 receiver,\n+                                 in_ByteSize(frame_data_offset));\n+\n+  if (TraceOptimizedUpcallStubs) {\n+    blob->print_on(tty);\n+  }\n+\n+  return blob->code_begin();\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/universalUpcallHandler_aarch64.cpp","additions":343,"deletions":0,"binary":false,"changes":343,"status":"added"},{"patch":"@@ -28,0 +28,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/arm\/downcallLinker_arm.cpp\n@@ -35,0 +36,9 @@\n+=======\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/arm\/universalNativeInvoker_arm.cpp\n","filename":"src\/hotspot\/cpu\/arm\/downcallLinker_arm.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/arm\/foreignGlobals_arm.cpp\n@@ -37,0 +38,3 @@\n+=======\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/arm\/foreign_globals_arm.cpp\n@@ -41,0 +45,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/arm\/foreignGlobals_arm.cpp\n@@ -56,0 +61,5 @@\n+=======\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/arm\/foreign_globals_arm.cpp\n","filename":"src\/hotspot\/cpu\/arm\/foreignGlobals_arm.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/arm\/upcallLinker_arm.cpp\n@@ -34,0 +35,8 @@\n+=======\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/arm\/universalUpcallHandle_arm.cpp\n","filename":"src\/hotspot\/cpu\/arm\/upcallLinker_arm.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/ppc\/continuationEntry_ppc.inline.hpp\n@@ -38,0 +39,3 @@\n+=======\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/s390\/foreign_globals_s390.cpp\n@@ -42,0 +46,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/ppc\/continuationEntry_ppc.inline.hpp\n@@ -44,0 +49,5 @@\n+=======\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/s390\/foreign_globals_s390.cpp\n","filename":"src\/hotspot\/cpu\/ppc\/continuationEntry_ppc.inline.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.cpp\n@@ -39,0 +40,3 @@\n+=======\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/ppc\/foreign_globals_ppc.cpp\n@@ -43,0 +47,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.cpp\n@@ -58,0 +63,5 @@\n+=======\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/ppc\/foreign_globals_ppc.cpp\n","filename":"src\/hotspot\/cpu\/ppc\/foreignGlobals_ppc.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/ppc\/stackChunkOop_ppc.inline.hpp\n@@ -29,0 +30,9 @@\n+=======\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/s390\/universalNativeInvoker_s390.cpp\n","filename":"src\/hotspot\/cpu\/ppc\/stackChunkOop_ppc.inline.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/ppc\/upcallLinker_ppc.cpp\n@@ -35,0 +36,8 @@\n+=======\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/ppc\/universalUpcallHandle_ppc.cpp\n","filename":"src\/hotspot\/cpu\/ppc\/upcallLinker_ppc.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -923,0 +923,1 @@\n+<<<<<<< HEAD\n@@ -925,0 +926,4 @@\n+=======\n+  } else if (iid == vmIntrinsics::_linkToNative) {\n+    member_arg_pos = method->size_of_parameters() - 1;  \/\/ trailing NativeEntryPoint argument\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/cpu\/s390\/sharedRuntime_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/s390\/upcallLinker_s390.cpp\n@@ -34,0 +35,8 @@\n+=======\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/s390\/universalUpcallHandle_s390.cpp\n","filename":"src\/hotspot\/cpu\/s390\/upcallLinker_s390.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -30,0 +30,7 @@\n+<<<<<<< HEAD:src\/hotspot\/cpu\/x86\/foreignGlobals_x86.hpp\n+=======\n+class outputStream;\n+\n+constexpr size_t xmm_reg_size = 16; \/\/ size of XMM reg\n+\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/x86\/foreign_globals_x86.hpp\n","filename":"src\/hotspot\/cpu\/x86\/foreignGlobals_x86.hpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -0,0 +1,161 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+#include \"oops\/typeArrayOop.inline.hpp\"\n+#include \"oops\/oopCast.inline.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+\n+bool ABIDescriptor::is_volatile_reg(Register reg) const {\n+    return _integer_argument_registers.contains(reg)\n+        || _integer_additional_volatile_registers.contains(reg);\n+}\n+\n+bool ABIDescriptor::is_volatile_reg(XMMRegister reg) const {\n+    return _vector_argument_registers.contains(reg)\n+        || _vector_additional_volatile_registers.contains(reg);\n+}\n+\n+#define INTEGER_TYPE 0\n+#define VECTOR_TYPE 1\n+#define X87_TYPE 2\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor_impl(jobject jabi) const {\n+  oop abi_oop = JNIHandles::resolve_non_null(jabi);\n+  ABIDescriptor abi;\n+\n+  objArrayOop inputStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.inputStorage_offset));\n+  loadArray(inputStorage, INTEGER_TYPE, abi._integer_argument_registers, as_Register);\n+  loadArray(inputStorage, VECTOR_TYPE, abi._vector_argument_registers, as_XMMRegister);\n+\n+  objArrayOop outputStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.outputStorage_offset));\n+  loadArray(outputStorage, INTEGER_TYPE, abi._integer_return_registers, as_Register);\n+  loadArray(outputStorage, VECTOR_TYPE, abi._vector_return_registers, as_XMMRegister);\n+  objArrayOop subarray = oop_cast<objArrayOop>(outputStorage->obj_at(X87_TYPE));\n+  abi._X87_return_registers_noof = subarray->length();\n+\n+  objArrayOop volatileStorage = oop_cast<objArrayOop>(abi_oop->obj_field(ABI.volatileStorage_offset));\n+  loadArray(volatileStorage, INTEGER_TYPE, abi._integer_additional_volatile_registers, as_Register);\n+  loadArray(volatileStorage, VECTOR_TYPE, abi._vector_additional_volatile_registers, as_XMMRegister);\n+\n+  abi._stack_alignment_bytes = abi_oop->int_field(ABI.stackAlignment_offset);\n+  abi._shadow_space_bytes = abi_oop->int_field(ABI.shadowSpace_offset);\n+\n+  abi._target_addr_reg = parse_vmstorage(abi_oop->obj_field(ABI.targetAddrStorage_offset))->as_Register();\n+  abi._ret_buf_addr_reg = parse_vmstorage(abi_oop->obj_field(ABI.retBufAddrStorage_offset))->as_Register();\n+\n+  return abi;\n+}\n+\n+enum class RegType {\n+  INTEGER = 0,\n+  VECTOR = 1,\n+  X87 = 2,\n+  STACK = 3\n+};\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  switch(static_cast<RegType>(type)) {\n+    case RegType::INTEGER: return ::as_Register(index)->as_VMReg();\n+    case RegType::VECTOR: return ::as_XMMRegister(index)->as_VMReg();\n+    case RegType::STACK: return VMRegImpl::stack2reg(index LP64_ONLY(* 2)); \/\/ numbering on x64 goes per 64-bits\n+    case RegType::X87: break;\n+  }\n+  return VMRegImpl::Bad();\n+}\n+\n+int RegSpiller::pd_reg_size(VMReg reg) {\n+  if (reg->is_Register()) {\n+    return 8;\n+  } else if (reg->is_XMMRegister()) {\n+    return 16;\n+  }\n+  return 0; \/\/ stack and BAD\n+}\n+\n+void RegSpiller::pd_store_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->movptr(Address(rsp, offset), reg->as_Register());\n+  } else if (reg->is_XMMRegister()) {\n+    masm->movdqu(Address(rsp, offset), reg->as_XMMRegister());\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void RegSpiller::pd_load_reg(MacroAssembler* masm, int offset, VMReg reg) {\n+  if (reg->is_Register()) {\n+    masm->movptr(reg->as_Register(), Address(rsp, offset));\n+  } else if (reg->is_XMMRegister()) {\n+    masm->movdqu(reg->as_XMMRegister(), Address(rsp, offset));\n+  } else {\n+    \/\/ stack and BAD\n+  }\n+}\n+\n+void ArgumentShuffle::pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+  Register tmp_reg = tmp->as_Register();\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    masm->block_comment(err_msg(\"bt=%s\", null_safe_string(type2name(arg_bt))));\n+    switch (arg_bt) {\n+      case T_BOOLEAN:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_CHAR:\n+      case T_INT:\n+        masm->move32_64(from_vmreg, to_vmreg, tmp_reg, in_stk_bias, out_stk_bias);\n+        break;\n+\n+      case T_FLOAT:\n+        if (to_vmreg.first()->is_Register()) { \/\/ Windows vararg call\n+          masm->movq(to_vmreg.first()->as_Register(), from_vmreg.first()->as_XMMRegister());\n+        } else {\n+          masm->float_move(from_vmreg, to_vmreg, tmp_reg, in_stk_bias, out_stk_bias);\n+        }\n+        break;\n+\n+      case T_DOUBLE:\n+        if (to_vmreg.first()->is_Register()) { \/\/ Windows vararg call\n+          masm->movq(to_vmreg.first()->as_Register(), from_vmreg.first()->as_XMMRegister());\n+        } else {\n+          masm->double_move(from_vmreg, to_vmreg, tmp_reg, in_stk_bias, out_stk_bias);\n+        }\n+        break;\n+\n+      case T_LONG:\n+        masm->long_move(from_vmreg, to_vmreg, tmp_reg, in_stk_bias, out_stk_bias);\n+        break;\n+\n+      default:\n+        fatal(\"found in upcall args: %s\", type2name(arg_bt));\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/foreign_globals_x86.cpp","additions":161,"deletions":0,"binary":false,"changes":161,"status":"added"},{"patch":"@@ -924,0 +924,1 @@\n+<<<<<<< HEAD\n@@ -925,0 +926,3 @@\n+=======\n+       src.first()->name(), src.second()->name(), dst.first()->name(), dst.second()->name());\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -214,0 +214,1 @@\n+<<<<<<< HEAD\n@@ -215,0 +216,3 @@\n+=======\n+                    Address(nep_reg, NONZERO(jdk_internal_invoke_NativeEntryPoint::invoker_offset_in_bytes())),\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/cpu\/x86\/methodHandles_x86.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,331 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"prims\/universalNativeInvoker.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/stubCodeGenerator.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+\n+#define __ _masm->\n+\n+class NativeInvokerGenerator : public StubCodeGenerator {\n+  BasicType* _signature;\n+  int _num_args;\n+  BasicType _ret_bt;\n+\n+  const ABIDescriptor& _abi;\n+  const GrowableArray<VMReg>& _input_registers;\n+  const GrowableArray<VMReg>& _output_registers;\n+\n+  bool _needs_return_buffer;\n+\n+  int _frame_complete;\n+  int _framesize;\n+  OopMapSet* _oop_maps;\n+public:\n+  NativeInvokerGenerator(CodeBuffer* buffer,\n+                         BasicType* signature,\n+                         int num_args,\n+                         BasicType ret_bt,\n+                         const ABIDescriptor& abi,\n+                         const GrowableArray<VMReg>& input_registers,\n+                         const GrowableArray<VMReg>& output_registers,\n+                         bool needs_return_buffer)\n+   : StubCodeGenerator(buffer, PrintMethodHandleStubs),\n+     _signature(signature),\n+     _num_args(num_args),\n+     _ret_bt(ret_bt),\n+     _abi(abi),\n+     _input_registers(input_registers),\n+     _output_registers(output_registers),\n+     _needs_return_buffer(needs_return_buffer),\n+     _frame_complete(0),\n+     _framesize(0),\n+     _oop_maps(NULL) {\n+  }\n+\n+  void generate();\n+\n+  int frame_complete() const {\n+    return _frame_complete;\n+  }\n+\n+  int framesize() const {\n+    return (_framesize >> (LogBytesPerWord - LogBytesPerInt));\n+  }\n+\n+  OopMapSet* oop_maps() const {\n+    return _oop_maps;\n+  }\n+};\n+\n+static const int native_invoker_code_size = 1024;\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+  int locs_size  = 64;\n+  CodeBuffer code(\"nep_invoker_blob\", native_invoker_code_size, locs_size);\n+  NativeInvokerGenerator g(&code, signature, num_args, ret_bt, abi, input_registers, output_registers, needs_return_buffer);\n+  g.generate();\n+  code.log_section_sizes(\"nep_invoker_blob\");\n+\n+  RuntimeStub* stub =\n+    RuntimeStub::new_runtime_stub(\"nep_invoker_blob\",\n+                                  &code,\n+                                  g.frame_complete(),\n+                                  g.framesize(),\n+                                  g.oop_maps(), false);\n+\n+  if (TraceNativeInvokers) {\n+    stub->print_on(tty);\n+  }\n+\n+  return stub;\n+}\n+\n+void NativeInvokerGenerator::generate() {\n+  enum layout {\n+    rbp_off,\n+    rbp_off2,\n+    return_off,\n+    return_off2,\n+    framesize_base \/\/ inclusive of return address\n+    \/\/ The following are also computed dynamically:\n+    \/\/ shadow space\n+    \/\/ spill area\n+    \/\/ out arg area (e.g. for stack args)\n+  };\n+\n+  Register shufffle_reg = rbx;\n+  JavaCallConv in_conv;\n+  NativeCallConv out_conv(_input_registers);\n+  ArgumentShuffle arg_shuffle(_signature, _num_args, _signature, _num_args, &in_conv, &out_conv, shufffle_reg->as_VMReg());\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ in bytes\n+  int allocated_frame_size = 0;\n+  if (_needs_return_buffer) {\n+    allocated_frame_size += 8; \/\/ store address\n+  }\n+  allocated_frame_size += arg_shuffle.out_arg_stack_slots() << LogBytesPerInt;\n+  allocated_frame_size += _abi._shadow_space_bytes;\n+\n+  int ret_buf_addr_rsp_offset = -1;\n+  if (_needs_return_buffer) {\n+    \/\/ the above\n+    ret_buf_addr_rsp_offset = allocated_frame_size - 8;\n+  }\n+\n+  \/\/ when we don't use a return buffer we need to spill the return value around our slowpath calls\n+  \/\/ when we use a return buffer case this SHOULD be unused.\n+  RegSpiller out_reg_spiller(_output_registers);\n+  int spill_rsp_offset = -1;\n+\n+  if (!_needs_return_buffer) {\n+    spill_rsp_offset = 0;\n+    \/\/ spill area can be shared with the above, so we take the max of the 2\n+    allocated_frame_size = out_reg_spiller.spill_size_bytes() > allocated_frame_size\n+      ? out_reg_spiller.spill_size_bytes()\n+      : allocated_frame_size;\n+  }\n+  allocated_frame_size = align_up(allocated_frame_size, 16);\n+  \/\/ _framesize is in 32-bit stack slots:\n+  _framesize += framesize_base + (allocated_frame_size >> LogBytesPerInt);\n+  assert(is_even(_framesize\/2), \"sp not 16-byte aligned\");\n+\n+  _oop_maps  = new OopMapSet();\n+  address start = __ pc();\n+\n+  __ enter();\n+\n+  \/\/ return address and rbp are already in place\n+  __ subptr(rsp, allocated_frame_size); \/\/ prolog\n+\n+  _frame_complete = __ pc() - start;\n+\n+  address the_pc = __ pc();\n+\n+  __ block_comment(\"{ thread java2native\");\n+  __ set_last_Java_frame(rsp, rbp, (address)the_pc);\n+  OopMap* map = new OopMap(_framesize, 0);\n+  _oop_maps->add_gc_map(the_pc - start, map);\n+\n+  \/\/ State transition\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);\n+  __ block_comment(\"} thread java2native\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_shuffle.generate(_masm, shufffle_reg->as_VMReg(), 0, _abi._shadow_space_bytes);\n+  if (_needs_return_buffer) {\n+    \/\/ spill our return buffer address\n+    assert(ret_buf_addr_rsp_offset != -1, \"no return buffer addr spill\");\n+    __ movptr(Address(rsp, ret_buf_addr_rsp_offset), _abi._ret_buf_addr_reg);\n+  }\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ call(_abi._target_addr_reg);\n+  \/\/ this call is assumed not to have killed r15_thread\n+\n+  if (!_needs_return_buffer) {\n+    \/\/ FIXME: this assumes we return in rax\/xmm0, which might not be the case\n+    \/\/ Unpack native results.\n+    switch (_ret_bt) {\n+      case T_BOOLEAN: __ c2bool(rax);            break;\n+      case T_CHAR   : __ movzwl(rax, rax);       break;\n+      case T_BYTE   : __ sign_extend_byte (rax); break;\n+      case T_SHORT  : __ sign_extend_short(rax); break;\n+      case T_INT    : \/* nothing to do *\/        break;\n+      case T_DOUBLE :\n+      case T_FLOAT  :\n+        \/\/ Result is in xmm0 we'll save as needed\n+        break;\n+      case T_VOID: break;\n+      case T_LONG: break;\n+      default       : ShouldNotReachHere();\n+    }\n+  } else {\n+    assert(ret_buf_addr_rsp_offset != -1, \"no return buffer addr spill\");\n+    __ movptr(rscratch1, Address(rsp, ret_buf_addr_rsp_offset));\n+    int offset = 0;\n+    for (int i = 0; i < _output_registers.length(); i++) {\n+      VMReg reg = _output_registers.at(i);\n+      if (reg->is_Register()) {\n+        __ movptr(Address(rscratch1, offset), reg->as_Register());\n+        offset += 8;\n+      } else if (reg->is_XMMRegister()) {\n+        __ movdqu(Address(rscratch1, offset), reg->as_XMMRegister());\n+        offset += 16;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  __ block_comment(\"{ thread native2java\");\n+  __ restore_cpu_control_state_after_jni();\n+\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native_trans);\n+\n+  \/\/ Force this write out before the read below\n+  __ membar(Assembler::Membar_mask_bits(\n+          Assembler::LoadLoad | Assembler::LoadStore |\n+          Assembler::StoreLoad | Assembler::StoreStore));\n+\n+  Label L_after_safepoint_poll;\n+  Label L_safepoint_poll_slow_path;\n+\n+  __ safepoint_poll(L_safepoint_poll_slow_path, r15_thread, true \/* at_return *\/, false \/* in_nmethod *\/);\n+  __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);\n+  __ jcc(Assembler::notEqual, L_safepoint_poll_slow_path);\n+\n+  __ bind(L_after_safepoint_poll);\n+\n+  \/\/ change thread state\n+  __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_Java);\n+\n+  __ block_comment(\"reguard stack check\");\n+  Label L_reguard;\n+  Label L_after_reguard;\n+  __ cmpl(Address(r15_thread, JavaThread::stack_guard_state_offset()), StackOverflow::stack_guard_yellow_reserved_disabled);\n+  __ jcc(Assembler::equal, L_reguard);\n+  __ bind(L_after_reguard);\n+\n+  __ reset_last_Java_frame(r15_thread, true);\n+  __ block_comment(\"} thread native2java\");\n+\n+  __ leave(); \/\/ required for proper stackwalking of RuntimeStub frame\n+  __ ret(0);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_safepoint_poll_slow_path\");\n+  __ bind(L_safepoint_poll_slow_path);\n+  __ vzeroupper();\n+\n+  if(!_needs_return_buffer) {\n+    out_reg_spiller.generate_spill(_masm, spill_rsp_offset);\n+  }\n+\n+  __ mov(c_rarg0, r15_thread);\n+  __ mov(r12, rsp); \/\/ remember sp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));\n+  __ mov(rsp, r12); \/\/ restore sp\n+  __ reinit_heapbase();\n+\n+  if(!_needs_return_buffer) {\n+    out_reg_spiller.generate_fill(_masm, spill_rsp_offset);\n+  }\n+\n+  __ jmp(L_after_safepoint_poll);\n+  __ block_comment(\"} L_safepoint_poll_slow_path\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ L_reguard\");\n+  __ bind(L_reguard);\n+  __ vzeroupper();\n+\n+  if(!_needs_return_buffer) {\n+    out_reg_spiller.generate_spill(_masm, spill_rsp_offset);\n+  }\n+\n+  __ mov(r12, rsp); \/\/ remember sp\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows\n+  __ andptr(rsp, -16); \/\/ align stack as required by ABI\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));\n+  __ mov(rsp, r12); \/\/ restore sp\n+  __ reinit_heapbase();\n+\n+  if(!_needs_return_buffer) {\n+    out_reg_spiller.generate_fill(_masm, spill_rsp_offset);\n+  }\n+\n+  __ jmp(L_after_reguard);\n+\n+  __ block_comment(\"} L_reguard\");\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ flush();\n+}\n","filename":"src\/hotspot\/cpu\/x86\/universalNativeInvoker_x86_64.cpp","additions":331,"deletions":0,"binary":false,"changes":331,"status":"added"},{"patch":"@@ -0,0 +1,401 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"asm\/macroAssembler.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"code\/vmreg.inline.hpp\"\n+#include \"compiler\/disassembler.hpp\"\n+#include \"logging\/logStream.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"prims\/universalUpcallHandler.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/signature.hpp\"\n+#include \"runtime\/stubRoutines.hpp\"\n+#include \"utilities\/formatBuffer.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n+\n+#define __ _masm->\n+\n+static bool is_valid_XMM(XMMRegister reg) {\n+  return reg->is_valid() && (UseAVX >= 3 || (reg->encoding() < 16)); \/\/ why is this not covered by is_valid()?\n+}\n+\n+\/\/ for callee saved regs, according to the caller's ABI\n+static int compute_reg_save_area_size(const ABIDescriptor& abi) {\n+  int size = 0;\n+  for (Register reg = as_Register(0); reg->is_valid(); reg = reg->successor()) {\n+    if (reg == rbp || reg == rsp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      size += 8; \/\/ bytes\n+    }\n+  }\n+\n+  for (XMMRegister reg = as_XMMRegister(0); is_valid_XMM(reg); reg = reg->successor()) {\n+    if (!abi.is_volatile_reg(reg)) {\n+      if (UseAVX >= 3) {\n+        size += 64; \/\/ bytes\n+      } else if (UseAVX >= 1) {\n+        size += 32;\n+      } else {\n+        size += 16;\n+      }\n+    }\n+  }\n+\n+#ifndef _WIN64\n+  \/\/ for mxcsr\n+  size += 8;\n+#endif\n+\n+  return size;\n+}\n+\n+constexpr int MXCSR_MASK = 0xFFC0;  \/\/ Mask out any pending exceptions\n+\n+static void preserve_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to save it here\n+  \/\/ 2. save mxcsr on non-windows platforms\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ preserve_callee_saved_regs \");\n+  for (Register reg = as_Register(0); reg->is_valid(); reg = reg->successor()) {\n+    if (reg == rbp || reg == rsp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ movptr(Address(rsp, offset), reg);\n+      offset += 8;\n+    }\n+  }\n+\n+  for (XMMRegister reg = as_XMMRegister(0); is_valid_XMM(reg); reg = reg->successor()) {\n+    if (!abi.is_volatile_reg(reg)) {\n+      if (UseAVX >= 3) {\n+        __ evmovdqul(Address(rsp, offset), reg, Assembler::AVX_512bit);\n+        offset += 64;\n+      } else if (UseAVX >= 1) {\n+        __ vmovdqu(Address(rsp, offset), reg);\n+        offset += 32;\n+      } else {\n+        __ movdqu(Address(rsp, offset), reg);\n+        offset += 16;\n+      }\n+    }\n+  }\n+\n+#ifndef _WIN64\n+  {\n+    const Address mxcsr_save(rsp, offset);\n+    Label skip_ldmx;\n+    __ stmxcsr(mxcsr_save);\n+    __ movl(rax, mxcsr_save);\n+    __ andl(rax, MXCSR_MASK);    \/\/ Only check control and mask bits\n+    ExternalAddress mxcsr_std(StubRoutines::x86::addr_mxcsr_std());\n+    __ cmp32(rax, mxcsr_std);\n+    __ jcc(Assembler::equal, skip_ldmx);\n+    __ ldmxcsr(mxcsr_std);\n+    __ bind(skip_ldmx);\n+  }\n+#endif\n+\n+  __ block_comment(\"} preserve_callee_saved_regs \");\n+}\n+\n+static void restore_callee_saved_registers(MacroAssembler* _masm, const ABIDescriptor& abi, int reg_save_area_offset) {\n+  \/\/ 1. iterate all registers in the architecture\n+  \/\/     - check if they are volatile or not for the given abi\n+  \/\/     - if NOT, we need to restore it here\n+  \/\/ 2. restore mxcsr on non-windows platforms\n+\n+  int offset = reg_save_area_offset;\n+\n+  __ block_comment(\"{ restore_callee_saved_regs \");\n+  for (Register reg = as_Register(0); reg->is_valid(); reg = reg->successor()) {\n+    if (reg == rbp || reg == rsp) continue; \/\/ saved\/restored by prologue\/epilogue\n+    if (!abi.is_volatile_reg(reg)) {\n+      __ movptr(reg, Address(rsp, offset));\n+      offset += 8;\n+    }\n+  }\n+\n+  for (XMMRegister reg = as_XMMRegister(0); is_valid_XMM(reg); reg = reg->successor()) {\n+    if (!abi.is_volatile_reg(reg)) {\n+      if (UseAVX >= 3) {\n+        __ evmovdqul(reg, Address(rsp, offset), Assembler::AVX_512bit);\n+        offset += 64;\n+      } else if (UseAVX >= 1) {\n+        __ vmovdqu(reg, Address(rsp, offset));\n+        offset += 32;\n+      } else {\n+        __ movdqu(reg, Address(rsp, offset));\n+        offset += 16;\n+      }\n+    }\n+  }\n+\n+#ifndef _WIN64\n+  const Address mxcsr_save(rsp, offset);\n+  __ ldmxcsr(mxcsr_save);\n+#endif\n+\n+  __ block_comment(\"} restore_callee_saved_regs \");\n+}\n+\/\/ Register is a class, but it would be assigned numerical value.\n+\/\/ \"0\" is assigned for rax and for xmm0. Thus we need to ignore -Wnonnull.\n+PRAGMA_DIAG_PUSH\n+PRAGMA_NONNULL_IGNORED\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+  const ABIDescriptor abi = ForeignGlobals::parse_abi_descriptor(jabi);\n+  const CallRegs call_regs = ForeignGlobals::parse_call_regs(jconv);\n+  CodeBuffer buffer(\"upcall_stub_linkToNative\", \/* code_size = *\/ 2048, \/* locs_size = *\/ 1024);\n+\n+  Register shuffle_reg = rbx;\n+  JavaCallConv out_conv;\n+  NativeCallConv in_conv(call_regs._arg_regs, call_regs._args_length);\n+  ArgumentShuffle arg_shuffle(in_sig_bt, total_in_args, out_sig_bt, total_out_args, &in_conv, &out_conv, shuffle_reg->as_VMReg());\n+  int stack_slots = SharedRuntime::out_preserve_stack_slots() + arg_shuffle.out_arg_stack_slots();\n+  int out_arg_area = align_up(stack_slots * VMRegImpl::stack_slot_size, StackAlignmentInBytes);\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    arg_shuffle.print_on(&ls);\n+  }\n+#endif\n+\n+  \/\/ out_arg_area (for stack arguments) doubles as shadow space for native calls.\n+  \/\/ make sure it is big enough.\n+  if (out_arg_area < frame::arg_reg_save_area_bytes) {\n+    out_arg_area = frame::arg_reg_save_area_bytes;\n+  }\n+\n+  int reg_save_area_size = compute_reg_save_area_size(abi);\n+  RegSpiller arg_spiller(call_regs._arg_regs, call_regs._args_length);\n+  RegSpiller result_spiller(call_regs._ret_regs, call_regs._rets_length);\n+\n+  int shuffle_area_offset    = 0;\n+  int res_save_area_offset   = shuffle_area_offset    + out_arg_area;\n+  int arg_save_area_offset   = res_save_area_offset   + result_spiller.spill_size_bytes();\n+  int reg_save_area_offset   = arg_save_area_offset   + arg_spiller.spill_size_bytes();\n+  int frame_data_offset      = reg_save_area_offset   + reg_save_area_size;\n+  int frame_bottom_offset    = frame_data_offset      + sizeof(OptimizedEntryBlob::FrameData);\n+\n+  int ret_buf_offset = -1;\n+  if (needs_return_buffer) {\n+    ret_buf_offset = frame_bottom_offset;\n+    frame_bottom_offset += ret_buf_size;\n+  }\n+\n+  int frame_size = frame_bottom_offset;\n+  frame_size = align_up(frame_size, StackAlignmentInBytes);\n+\n+  \/\/ Ok The space we have allocated will look like:\n+  \/\/\n+  \/\/\n+  \/\/ FP-> |                     |\n+  \/\/      |---------------------| = frame_bottom_offset = frame_size\n+  \/\/      | (optional)          |\n+  \/\/      | ret_buf             |\n+  \/\/      |---------------------| = ret_buf_offset\n+  \/\/      |                     |\n+  \/\/      | FrameData           |\n+  \/\/      |---------------------| = frame_data_offset\n+  \/\/      |                     |\n+  \/\/      | reg_save_area       |\n+  \/\/      |---------------------| = reg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | arg_save_area       |\n+  \/\/      |---------------------| = arg_save_are_offset\n+  \/\/      |                     |\n+  \/\/      | res_save_area       |\n+  \/\/      |---------------------| = res_save_are_offset\n+  \/\/      |                     |\n+  \/\/ SP-> | out_arg_area        |   needs to be at end for shadow space\n+  \/\/\n+  \/\/\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  MacroAssembler* _masm = new MacroAssembler(&buffer);\n+  address start = __ pc();\n+  __ enter(); \/\/ set up frame\n+  if ((abi._stack_alignment_bytes % 16) != 0) {\n+    \/\/ stack alignment of caller is not a multiple of 16\n+    __ andptr(rsp, -StackAlignmentInBytes); \/\/ align stack\n+  }\n+  \/\/ allocate frame (frame_size is also aligned, so stack is still aligned)\n+  __ subptr(rsp, frame_size);\n+\n+  \/\/ we have to always spill args since we need to do a call to get the thread\n+  \/\/ (and maybe attach it).\n+  arg_spiller.generate_spill(_masm, arg_save_area_offset);\n+\n+  preserve_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  __ block_comment(\"{ on_entry\");\n+  __ vzeroupper();\n+  __ lea(c_rarg0, Address(rsp, frame_data_offset));\n+  \/\/ stack already aligned\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, ProgrammableUpcallHandler::on_entry)));\n+  __ movptr(r15_thread, rax);\n+  __ reinit_heapbase();\n+  __ block_comment(\"} on_entry\");\n+\n+  __ block_comment(\"{ argument shuffle\");\n+  arg_spiller.generate_fill(_masm, arg_save_area_offset);\n+  if (needs_return_buffer) {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(abi._ret_buf_addr_reg, Address(rsp, ret_buf_offset));\n+  }\n+  arg_shuffle.generate(_masm, shuffle_reg->as_VMReg(), abi._shadow_space_bytes, 0);\n+  __ block_comment(\"} argument shuffle\");\n+\n+  __ block_comment(\"{ receiver \");\n+  __ movptr(rscratch1, (intptr_t)receiver);\n+  __ resolve_jobject(rscratch1, r15_thread, rscratch2);\n+  __ movptr(j_rarg0, rscratch1);\n+  __ block_comment(\"} receiver \");\n+\n+  __ mov_metadata(rbx, entry);\n+  __ movptr(Address(r15_thread, JavaThread::callee_target_offset()), rbx); \/\/ just in case callee is deoptimized\n+\n+  __ call(Address(rbx, Method::from_compiled_offset()));\n+\n+  \/\/ return value shuffle\n+  if (!needs_return_buffer) {\n+#ifdef ASSERT\n+    if (call_regs._rets_length == 1) { \/\/ 0 or 1\n+      VMReg j_expected_result_reg;\n+      switch (ret_type) {\n+        case T_BOOLEAN:\n+        case T_BYTE:\n+        case T_SHORT:\n+        case T_CHAR:\n+        case T_INT:\n+        case T_LONG:\n+        j_expected_result_reg = rax->as_VMReg();\n+        break;\n+        case T_FLOAT:\n+        case T_DOUBLE:\n+          j_expected_result_reg = xmm0->as_VMReg();\n+          break;\n+        default:\n+          fatal(\"unexpected return type: %s\", type2name(ret_type));\n+      }\n+      \/\/ No need to move for now, since CallArranger can pick a return type\n+      \/\/ that goes in the same reg for both CCs. But, at least assert they are the same\n+      assert(call_regs._ret_regs[0] == j_expected_result_reg,\n+      \"unexpected result register: %s != %s\", call_regs._ret_regs[0]->name(), j_expected_result_reg->name());\n+    }\n+#endif\n+  } else {\n+    assert(ret_buf_offset != -1, \"no return buffer allocated\");\n+    __ lea(rscratch1, Address(rsp, ret_buf_offset));\n+    int offset = 0;\n+    for (int i = 0; i < call_regs._rets_length; i++) {\n+      VMReg reg = call_regs._ret_regs[i];\n+      if (reg->is_Register()) {\n+        __ movptr(reg->as_Register(), Address(rscratch1, offset));\n+        offset += 8;\n+      } else if (reg->is_XMMRegister()) {\n+        __ movdqu(reg->as_XMMRegister(), Address(rscratch1, offset));\n+        offset += 16;\n+      } else {\n+        ShouldNotReachHere();\n+      }\n+    }\n+  }\n+\n+  result_spiller.generate_spill(_masm, res_save_area_offset);\n+\n+  __ block_comment(\"{ on_exit\");\n+  __ vzeroupper();\n+  __ lea(c_rarg0, Address(rsp, frame_data_offset));\n+  \/\/ stack already aligned\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, ProgrammableUpcallHandler::on_exit)));\n+  __ reinit_heapbase();\n+  __ block_comment(\"} on_exit\");\n+\n+  restore_callee_saved_registers(_masm, abi, reg_save_area_offset);\n+\n+  result_spiller.generate_fill(_masm, res_save_area_offset);\n+\n+  __ leave();\n+  __ ret(0);\n+\n+  \/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n+\n+  __ block_comment(\"{ exception handler\");\n+\n+  intptr_t exception_handler_offset = __ pc() - start;\n+\n+  \/\/ TODO: this is always the same, can we bypass and call handle_uncaught_exception directly?\n+\n+  \/\/ native caller has no idea how to handle exceptions\n+  \/\/ we just crash here. Up to callee to catch exceptions.\n+  __ verify_oop(rax);\n+  __ vzeroupper();\n+  __ mov(c_rarg0, rax);\n+  __ andptr(rsp, -StackAlignmentInBytes); \/\/ align stack as required by ABI\n+  __ subptr(rsp, frame::arg_reg_save_area_bytes); \/\/ windows (not really needed)\n+  __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, ProgrammableUpcallHandler::handle_uncaught_exception)));\n+  __ should_not_reach_here();\n+\n+  __ block_comment(\"} exception handler\");\n+\n+  _masm->flush();\n+\n+\n+#ifndef PRODUCT\n+  stringStream ss;\n+  ss.print(\"optimized_upcall_stub_%s\", entry->signature()->as_C_string());\n+  const char* name = _masm->code_string(ss.as_string());\n+#else \/\/ PRODUCT\n+  const char* name = \"optimized_upcall_stub\";\n+#endif \/\/ PRODUCT\n+\n+  OptimizedEntryBlob* blob\n+    = OptimizedEntryBlob::create(name,\n+                                 &buffer,\n+                                 exception_handler_offset,\n+                                 receiver,\n+                                 in_ByteSize(frame_data_offset));\n+\n+  if (TraceOptimizedUpcallStubs) {\n+    blob->print_on(tty);\n+  }\n+\n+  return blob->code_begin();\n+}\n+PRAGMA_DIAG_POP\n","filename":"src\/hotspot\/cpu\/x86\/universalUpcallHandler_x86_64.cpp","additions":401,"deletions":0,"binary":false,"changes":401,"status":"added"},{"patch":"@@ -0,0 +1,35 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"prims\/universalUpcallHandler.hpp\"\n+\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+  ShouldNotCallThis();\n+  return nullptr;\n+}\n","filename":"src\/hotspot\/cpu\/zero\/universalUpcallHandle_zero.cpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"added"},{"patch":"@@ -1054,0 +1054,1 @@\n+<<<<<<< HEAD\n@@ -1056,0 +1057,2 @@\n+=======\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/ci\/ciEnv.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -395,0 +395,1 @@\n+<<<<<<< HEAD\n@@ -397,0 +398,2 @@\n+=======\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/ci\/ciEnv.hpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4240,0 +4240,1 @@\n+<<<<<<< HEAD\n@@ -4248,0 +4249,20 @@\n+=======\n+int jdk_internal_invoke_NativeEntryPoint::_shadow_space_offset;\n+int jdk_internal_invoke_NativeEntryPoint::_argMoves_offset;\n+int jdk_internal_invoke_NativeEntryPoint::_returnMoves_offset;\n+int jdk_internal_invoke_NativeEntryPoint::_need_transition_offset;\n+int jdk_internal_invoke_NativeEntryPoint::_method_type_offset;\n+int jdk_internal_invoke_NativeEntryPoint::_name_offset;\n+int jdk_internal_invoke_NativeEntryPoint::_invoker_offset;\n+\n+#define NEP_FIELDS_DO(macro) \\\n+  macro(_shadow_space_offset,    k, \"shadowSpace\",    int_signature, false); \\\n+  macro(_argMoves_offset,        k, \"argMoves\",       long_array_signature, false); \\\n+  macro(_returnMoves_offset,     k, \"returnMoves\",    long_array_signature, false); \\\n+  macro(_need_transition_offset, k, \"needTransition\", bool_signature, false); \\\n+  macro(_method_type_offset,     k, \"methodType\",     java_lang_invoke_MethodType_signature, false); \\\n+  macro(_name_offset,            k, \"name\",           string_signature, false); \\\n+  macro(_invoker_offset,         k, \"invoker\",        long_signature, false);\n+\n+bool jdk_internal_invoke_NativeEntryPoint::is_instance(oop obj) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -4396,0 +4417,4 @@\n+jlong jdk_internal_invoke_NativeEntryPoint::invoker(oop entry) {\n+  return entry->long_field(_invoker_offset);\n+}\n+\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.cpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -1287,0 +1287,1 @@\n+<<<<<<< HEAD\n@@ -1288,0 +1289,4 @@\n+=======\n+  static int _name_offset;\n+  static int _invoker_offset;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -1296,0 +1301,1 @@\n+<<<<<<< HEAD\n@@ -1297,0 +1303,4 @@\n+=======\n+  static oop        name(oop entry);\n+  static jlong      invoker(oop entry);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -1306,0 +1316,1 @@\n+<<<<<<< HEAD\n@@ -1392,0 +1403,9 @@\n+=======\n+  static int shadow_space_offset_in_bytes()    { return _shadow_space_offset;    }\n+  static int argMoves_offset_in_bytes()        { return _argMoves_offset;        }\n+  static int returnMoves_offset_in_bytes()     { return _returnMoves_offset;     }\n+  static int need_transition_offset_in_bytes() { return _need_transition_offset; }\n+  static int method_type_offset_in_bytes()     { return _method_type_offset;     }\n+  static int name_offset_in_bytes()            { return _name_offset;            }\n+  static int invoker_offset_in_bytes()         { return _invoker_offset;         }\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/classfile\/javaClasses.hpp","additions":20,"deletions":0,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -728,0 +728,1 @@\n+<<<<<<< HEAD\n@@ -732,0 +733,6 @@\n+=======\n+OptimizedEntryBlob::OptimizedEntryBlob(const char* name, CodeBuffer* cb, int size,\n+                                       intptr_t exception_handler_offset,\n+                                       jobject receiver, ByteSize frame_data_offset) :\n+  RuntimeBlob(name, cb, sizeof(OptimizedEntryBlob), size, CodeOffsets::frame_never_safe, 0 \/* no frame size *\/,\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -739,0 +746,1 @@\n+<<<<<<< HEAD\n@@ -746,0 +754,9 @@\n+=======\n+void* OptimizedEntryBlob::operator new(size_t s, unsigned size) throw() {\n+  return CodeCache::allocate(size, CodeBlobType::NonNMethod);\n+}\n+\n+OptimizedEntryBlob* OptimizedEntryBlob::create(const char* name, CodeBuffer* cb,\n+                                               intptr_t exception_handler_offset,\n+                                               jobject receiver, ByteSize frame_data_offset) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -752,0 +769,1 @@\n+<<<<<<< HEAD\n@@ -753,0 +771,3 @@\n+=======\n+    blob = new (size) OptimizedEntryBlob(name, cb, size,\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -758,0 +779,1 @@\n+<<<<<<< HEAD\n@@ -759,0 +781,3 @@\n+=======\n+  trace_new_stub(blob, \"OptimizedEntryBlob\");\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -771,0 +796,1 @@\n+<<<<<<< HEAD\n@@ -772,0 +798,3 @@\n+=======\n+void OptimizedEntryBlob::free(OptimizedEntryBlob* blob) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -777,0 +806,1 @@\n+<<<<<<< HEAD\n@@ -787,0 +817,12 @@\n+=======\n+void OptimizedEntryBlob::preserve_callee_argument_oops(frame fr, const RegisterMap* reg_map, OopClosure* f) {\n+  \/\/ do nothing for now\n+}\n+\n+\/\/ Misc.\n+void OptimizedEntryBlob::verify() {\n+  \/\/ unimplemented\n+}\n+\n+void OptimizedEntryBlob::print_on(outputStream* st) const {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -791,0 +833,1 @@\n+<<<<<<< HEAD\n@@ -793,0 +836,4 @@\n+=======\n+void OptimizedEntryBlob::print_value_on(outputStream* st) const {\n+  st->print_cr(\"OptimizedEntryBlob (\" INTPTR_FORMAT  \") used for %s\", p2i(this), name());\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/code\/codeBlob.cpp","additions":47,"deletions":0,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -70,0 +70,1 @@\n+<<<<<<< HEAD\n@@ -71,0 +72,3 @@\n+=======\n+\/\/   OptimizedEntryBlob  : Used for upcalls from native code\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -758,0 +762,1 @@\n+<<<<<<< HEAD\n@@ -761,0 +766,4 @@\n+=======\n+class OptimizedEntryBlob: public RuntimeBlob {\n+  friend class ProgrammableUpcallHandler;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -766,0 +775,1 @@\n+<<<<<<< HEAD\n@@ -767,0 +777,3 @@\n+=======\n+  OptimizedEntryBlob(const char* name, CodeBuffer* cb, int size,\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -783,0 +796,1 @@\n+<<<<<<< HEAD\n@@ -788,0 +802,7 @@\n+=======\n+  static OptimizedEntryBlob* create(const char* name, CodeBuffer* cb,\n+                                    intptr_t exception_handler_offset,\n+                                    jobject receiver, ByteSize frame_data_offset);\n+\n+  static void free(OptimizedEntryBlob* blob);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -795,0 +816,1 @@\n+<<<<<<< HEAD\n@@ -796,0 +818,3 @@\n+=======\n+  virtual bool is_optimized_entry_blob() const override { return true; }\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/code\/codeBlob.hpp","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/code\/codeBlob.inline.hpp\n@@ -37,0 +38,10 @@\n+=======\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+  ShouldNotCallThis();\n+  return {};\n+}\n+\n+VMReg ForeignGlobals::vmstorage_to_vmreg(int type, int index) {\n+  Unimplemented();\n+  return VMRegImpl::Bad();\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/zero\/foreign_globals_zero.cpp\n","filename":"src\/hotspot\/share\/code\/codeBlob.inline.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -2,0 +2,1 @@\n+<<<<<<< HEAD\n@@ -3,0 +4,3 @@\n+=======\n+ * Copyright (c) 2021, Oracle and\/or its affiliates. All rights reserved.\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/oops\/oopCast.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1131,0 +1131,1 @@\n+<<<<<<< HEAD\n@@ -1133,0 +1134,26 @@\n+=======\n+    {\n+      Node* addr_n = kit.argument(0); \/\/ target address\n+      Node* nep_n = kit.argument(callee->arg_size() - 1); \/\/ NativeEntryPoint\n+      \/\/ This check needs to be kept in sync with the one in CallStaticJavaNode::Ideal\n+      if (addr_n->Opcode() == Op_ConL && nep_n->Opcode() == Op_ConP) {\n+        input_not_const = false;\n+\n+        const TypeOopPtr* nep_t = nep_n->bottom_type()->is_oopptr();\n+        ciNativeEntryPoint* nep = nep_t->const_oop()->as_native_entry_point();\n+\n+        if (!nep->need_transition()) {\n+          const TypeLong* addr_t = addr_n->bottom_type()->is_long();\n+          address addr = (address) addr_t->get_con();\n+\n+          return new NativeCallGenerator(callee, addr, nep);\n+        } else {\n+          print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n+                        \"Not inlining non-trivial call\");\n+        }\n+      } else {\n+        print_inlining_failure(C, callee, jvms->depth() - 1, jvms->bci(),\n+                               \"NativeEntryPoint or address not constant\");\n+      }\n+    }\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/opto\/callGenerator.cpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"ci\/ciNativeEntryPoint.hpp\"\n@@ -1086,0 +1087,1 @@\n+<<<<<<< HEAD\n@@ -1087,0 +1089,9 @@\n+=======\n+      Node* nep_node = in(TypeFunc::Parms + callee->arg_size() - 1);\n+      if (nep_node->Opcode() == Op_ConP \/* NEP *\/\n+          && in(TypeFunc::Parms + 0)->Opcode() == Op_ConL \/* address *\/\n+          && !nep_node->bottom_type()->is_oopptr()->const_oop()->as_native_entry_point()->need_transition()) {\n+        phase->C->prepend_late_inline(cg);\n+        set_generator(NULL);\n+      }\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -824,0 +824,36 @@\n+<<<<<<< HEAD\n+=======\n+\/\/------------------------------CallNativeNode-----------------------------------\n+\/\/ Make a direct call into a foreign function with an arbitrary ABI\n+\/\/ safepoints\n+class CallNativeNode : public CallNode {\n+  friend class MachCallNativeNode;\n+  virtual bool cmp( const Node &n ) const;\n+  virtual uint size_of() const;\n+  static void print_regs(const GrowableArray<VMReg>& regs, outputStream* st);\n+public:\n+  GrowableArray<VMReg> _arg_regs;\n+  GrowableArray<VMReg> _ret_regs;\n+  const int _shadow_space_bytes;\n+\n+  CallNativeNode(const TypeFunc* tf, address addr, const char* name,\n+                 const TypePtr* adr_type,\n+                 const GrowableArray<VMReg>& arg_regs,\n+                 const GrowableArray<VMReg>& ret_regs,\n+                 int shadow_space_bytes)\n+    : CallNode(tf, addr, adr_type), _arg_regs(arg_regs),\n+      _ret_regs(ret_regs), _shadow_space_bytes(shadow_space_bytes)\n+  {\n+    init_class_id(Class_CallNative);\n+    _name = name;\n+  }\n+  virtual int   Opcode() const;\n+  virtual bool  guaranteed_safepoint()  { return false; }\n+  virtual Node* match(const ProjNode *proj, const Matcher *m);\n+  virtual void  calling_convention( BasicType* sig_bt, VMRegPair *parm_regs, uint argcnt ) const;\n+#ifndef PRODUCT\n+  virtual void  dump_spec(outputStream *st) const;\n+#endif\n+};\n+\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/opto\/callnode.hpp","additions":36,"deletions":0,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -2565,0 +2565,103 @@\n+<<<<<<< HEAD\n+=======\n+\/\/-----------------------------make_native_call-------------------------------\n+Node* GraphKit::make_native_call(address call_addr, const TypeFunc* call_type, uint nargs, ciNativeEntryPoint* nep) {\n+  assert(!nep->need_transition(), \"only trivial calls\");\n+\n+  \/\/ Select just the actual call args to pass on\n+  \/\/ [long addr, HALF addr, ... args , NativeEntryPoint nep]\n+  \/\/                      |          |\n+  \/\/                      V          V\n+  \/\/                      [ ... args ]\n+  uint n_filtered_args = nargs - 3; \/\/ -addr (2), -nep;\n+  ResourceMark rm;\n+  Node** argument_nodes = NEW_RESOURCE_ARRAY(Node*, n_filtered_args);\n+  const Type** arg_types = TypeTuple::fields(n_filtered_args);\n+  GrowableArray<VMReg> arg_regs(C->comp_arena(), n_filtered_args, n_filtered_args, VMRegImpl::Bad());\n+\n+  VMReg* argRegs = nep->argMoves();\n+  {\n+    for (uint vm_arg_pos = 0, java_arg_read_pos = 0;\n+        vm_arg_pos < n_filtered_args; vm_arg_pos++) {\n+      uint vm_unfiltered_arg_pos = vm_arg_pos + 2; \/\/ +2 to skip addr (2 since long)\n+      Node* node = argument(vm_unfiltered_arg_pos);\n+      const Type* type = call_type->domain()->field_at(TypeFunc::Parms + vm_unfiltered_arg_pos);\n+      VMReg reg = type == Type::HALF\n+        ? VMRegImpl::Bad()\n+        : argRegs[java_arg_read_pos++];\n+\n+      argument_nodes[vm_arg_pos] = node;\n+      arg_types[TypeFunc::Parms + vm_arg_pos] = type;\n+      arg_regs.at_put(vm_arg_pos, reg);\n+    }\n+  }\n+\n+  uint n_returns = call_type->range()->cnt() - TypeFunc::Parms;\n+  GrowableArray<VMReg> ret_regs(C->comp_arena(), n_returns, n_returns, VMRegImpl::Bad());\n+  const Type** ret_types = TypeTuple::fields(n_returns);\n+\n+  VMReg* retRegs = nep->returnMoves();\n+  {\n+    for (uint vm_ret_pos = 0, java_ret_read_pos = 0;\n+        vm_ret_pos < n_returns; vm_ret_pos++) { \/\/ 0 or 1\n+      const Type* type = call_type->range()->field_at(TypeFunc::Parms + vm_ret_pos);\n+      VMReg reg = type == Type::HALF\n+        ? VMRegImpl::Bad()\n+        : retRegs[java_ret_read_pos++];\n+\n+      ret_regs.at_put(vm_ret_pos, reg);\n+      ret_types[TypeFunc::Parms + vm_ret_pos] = type;\n+    }\n+  }\n+\n+  const TypeFunc* new_call_type = TypeFunc::make(\n+    TypeTuple::make(TypeFunc::Parms + n_filtered_args, arg_types),\n+    TypeTuple::make(TypeFunc::Parms + n_returns, ret_types)\n+  );\n+\n+  CallNode* call = new CallNativeNode(new_call_type, call_addr, nep->name(), TypePtr::BOTTOM,\n+                            arg_regs,\n+                            ret_regs,\n+                            nep->shadow_space());\n+\n+  assert(call != nullptr, \"'call' was not set\");\n+\n+  set_predefined_input_for_runtime_call(call);\n+\n+  for (uint i = 0; i < n_filtered_args; i++) {\n+    call->init_req(i + TypeFunc::Parms, argument_nodes[i]);\n+  }\n+\n+  Node* c = gvn().transform(call);\n+  assert(c == call, \"cannot disappear\");\n+\n+  set_predefined_output_for_runtime_call(call);\n+\n+  Node* ret;\n+  if (method() == NULL || method()->return_type()->basic_type() == T_VOID) {\n+    ret = top();\n+  } else {\n+    ret =  gvn().transform(new ProjNode(call, TypeFunc::Parms));\n+    \/\/ Unpack native results if needed\n+    \/\/ Need this method type since it's unerased\n+    switch (nep->method_type()->rtype()->basic_type()) {\n+      case T_CHAR:\n+        ret = _gvn.transform(new AndINode(ret, _gvn.intcon(0xFFFF)));\n+        break;\n+      case T_BYTE:\n+        ret = sign_extend_byte(ret);\n+        break;\n+      case T_SHORT:\n+        ret = sign_extend_short(ret);\n+        break;\n+      default: \/\/ do nothing\n+        break;\n+    }\n+  }\n+\n+  push_node(method()->return_type()->basic_type(), ret);\n+\n+  return call;\n+}\n+\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/opto\/graphKit.cpp","additions":103,"deletions":0,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -3361,0 +3361,1 @@\n+<<<<<<< HEAD\n@@ -3363,0 +3364,2 @@\n+=======\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/opto\/output.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/prims\/foreignGlobals.inline.hpp\n@@ -36,0 +37,3 @@\n+=======\n+void ForeignGlobals::loadArray(objArrayOop jarray, int type_index, GrowableArray<T>& array, Func converter) const {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/share\/prims\/foreign_globals.inline.hpp\n","filename":"src\/hotspot\/share\/prims\/foreignGlobals.inline.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,397 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"foreign_globals.hpp\"\n+#include \"classfile\/symbolTable.hpp\"\n+#include \"classfile\/systemDictionary.hpp\"\n+#include \"classfile\/vmSymbols.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"runtime\/fieldDescriptor.inline.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+\n+#define FOREIGN_ABI \"jdk\/internal\/foreign\/abi\/\"\n+\n+static int field_offset(InstanceKlass* cls, const char* fieldname, Symbol* sigsym) {\n+  TempNewSymbol fieldnamesym = SymbolTable::new_symbol(fieldname, (int)strlen(fieldname));\n+  fieldDescriptor fd;\n+  bool success = cls->find_field(fieldnamesym, sigsym, false, &fd);\n+  assert(success, \"Field not found\");\n+  return fd.offset();\n+}\n+\n+static InstanceKlass* find_InstanceKlass(const char* name, TRAPS) {\n+  TempNewSymbol sym = SymbolTable::new_symbol(name, (int)strlen(name));\n+  Klass* k = SystemDictionary::resolve_or_null(sym, Handle(), Handle(), THREAD);\n+  assert(k != nullptr, \"Can not find class: %s\", name);\n+  return InstanceKlass::cast(k);\n+}\n+\n+const ForeignGlobals& ForeignGlobals::instance() {\n+  static ForeignGlobals globals; \/\/ thread-safe lazy init-once (since C++11)\n+  return globals;\n+}\n+\n+const ABIDescriptor ForeignGlobals::parse_abi_descriptor(jobject jabi) {\n+  return instance().parse_abi_descriptor_impl(jabi);\n+}\n+\n+const CallRegs ForeignGlobals::parse_call_regs(jobject jconv) {\n+  return instance().parse_call_regs_impl(jconv);\n+}\n+\n+ForeignGlobals::ForeignGlobals() {\n+  JavaThread* current_thread = JavaThread::current();\n+  ResourceMark rm(current_thread);\n+\n+  \/\/ ABIDescriptor\n+  InstanceKlass* k_ABI = find_InstanceKlass(FOREIGN_ABI \"ABIDescriptor\", current_thread);\n+  const char* strVMSArrayArray = \"[[L\" FOREIGN_ABI \"VMStorage;\";\n+  Symbol* symVMSArrayArray = SymbolTable::new_symbol(strVMSArrayArray);\n+  ABI.inputStorage_offset = field_offset(k_ABI, \"inputStorage\", symVMSArrayArray);\n+  ABI.outputStorage_offset = field_offset(k_ABI, \"outputStorage\", symVMSArrayArray);\n+  ABI.volatileStorage_offset = field_offset(k_ABI, \"volatileStorage\", symVMSArrayArray);\n+  ABI.stackAlignment_offset = field_offset(k_ABI, \"stackAlignment\", vmSymbols::int_signature());\n+  ABI.shadowSpace_offset = field_offset(k_ABI, \"shadowSpace\", vmSymbols::int_signature());\n+  const char* strVMS = \"L\" FOREIGN_ABI \"VMStorage;\";\n+  Symbol* symVMS = SymbolTable::new_symbol(strVMS);\n+  ABI.targetAddrStorage_offset = field_offset(k_ABI, \"targetAddrStorage\", symVMS);\n+  ABI.retBufAddrStorage_offset = field_offset(k_ABI, \"retBufAddrStorage\", symVMS);\n+\n+  \/\/ VMStorage\n+  InstanceKlass* k_VMS = find_InstanceKlass(FOREIGN_ABI \"VMStorage\", current_thread);\n+  VMS.index_offset = field_offset(k_VMS, \"index\", vmSymbols::int_signature());\n+  VMS.type_offset = field_offset(k_VMS, \"type\", vmSymbols::int_signature());\n+\n+  \/\/ CallRegs\n+  const char* strVMSArray = \"[L\" FOREIGN_ABI \"VMStorage;\";\n+  Symbol* symVMSArray = SymbolTable::new_symbol(strVMSArray);\n+  InstanceKlass* k_CC = find_InstanceKlass(FOREIGN_ABI \"ProgrammableUpcallHandler$CallRegs\", current_thread);\n+  CallConvOffsets.arg_regs_offset = field_offset(k_CC, \"argRegs\", symVMSArray);\n+  CallConvOffsets.ret_regs_offset = field_offset(k_CC, \"retRegs\", symVMSArray);\n+}\n+\n+const CallRegs ForeignGlobals::parse_call_regs_impl(jobject jconv) const {\n+  oop conv_oop = JNIHandles::resolve_non_null(jconv);\n+  objArrayOop arg_regs_oop = oop_cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.arg_regs_offset));\n+  objArrayOop ret_regs_oop = oop_cast<objArrayOop>(conv_oop->obj_field(CallConvOffsets.ret_regs_offset));\n+\n+  CallRegs result;\n+  result._args_length = arg_regs_oop->length();\n+  result._arg_regs = NEW_RESOURCE_ARRAY(VMReg, result._args_length);\n+\n+  result._rets_length = ret_regs_oop->length();\n+  result._ret_regs = NEW_RESOURCE_ARRAY(VMReg, result._rets_length);\n+\n+  for (int i = 0; i < result._args_length; i++) {\n+    result._arg_regs[i] = parse_vmstorage(arg_regs_oop->obj_at(i));\n+  }\n+\n+  for (int i = 0; i < result._rets_length; i++) {\n+    result._ret_regs[i] = parse_vmstorage(ret_regs_oop->obj_at(i));\n+  }\n+\n+  return result;\n+}\n+\n+VMReg ForeignGlobals::parse_vmstorage(oop storage) const {\n+  jint index = storage->int_field(VMS.index_offset);\n+  jint type = storage->int_field(VMS.type_offset);\n+  return vmstorage_to_vmreg(type, index);\n+}\n+\n+int RegSpiller::compute_spill_area(const VMReg* regs, int num_regs) {\n+  int result_size = 0;\n+  for (int i = 0; i < num_regs; i++) {\n+    result_size += pd_reg_size(regs[i]);\n+  }\n+  return result_size;\n+}\n+\n+void RegSpiller::generate(MacroAssembler* masm, int rsp_offset, bool spill) const {\n+  assert(rsp_offset != -1, \"rsp_offset should be set\");\n+  int offset = rsp_offset;\n+  for (int i = 0; i < _num_regs; i++) {\n+    VMReg reg = _regs[i];\n+    if (spill) {\n+      pd_store_reg(masm, offset, reg);\n+    } else {\n+      pd_load_reg(masm, offset, reg);\n+    }\n+    offset += pd_reg_size(reg);\n+  }\n+}\n+\n+void ArgumentShuffle::print_on(outputStream* os) const {\n+  os->print_cr(\"Argument shuffle {\");\n+  for (int i = 0; i < _moves.length(); i++) {\n+    Move move = _moves.at(i);\n+    BasicType arg_bt     = move.bt;\n+    VMRegPair from_vmreg = move.from;\n+    VMRegPair to_vmreg   = move.to;\n+\n+    os->print(\"Move a %s from (\", null_safe_string(type2name(arg_bt)));\n+    from_vmreg.first()->print_on(os);\n+    os->print(\",\");\n+    from_vmreg.second()->print_on(os);\n+    os->print(\") to (\");\n+    to_vmreg.first()->print_on(os);\n+    os->print(\",\");\n+    to_vmreg.second()->print_on(os);\n+    os->print_cr(\")\");\n+  }\n+  os->print_cr(\"Stack argument slots: %d\", _out_arg_stack_slots);\n+  os->print_cr(\"}\");\n+}\n+\n+int NativeCallConv::calling_convention(BasicType* sig_bt, VMRegPair* out_regs, int num_args) const {\n+  int src_pos = 0;\n+  int stk_slots = 0;\n+  for (int i = 0; i < num_args; i++) {\n+    switch (sig_bt[i]) {\n+      case T_BOOLEAN:\n+      case T_CHAR:\n+      case T_BYTE:\n+      case T_SHORT:\n+      case T_INT:\n+      case T_FLOAT: {\n+        assert(src_pos < _input_regs_length, \"oob\");\n+        VMReg reg = _input_regs[src_pos++];\n+        out_regs[i].set1(reg);\n+        if (reg->is_stack())\n+          stk_slots += 2;\n+        break;\n+      }\n+      case T_LONG:\n+      case T_DOUBLE: {\n+        assert((i + 1) < num_args && sig_bt[i + 1] == T_VOID, \"expecting half\");\n+        assert(src_pos < _input_regs_length, \"oob\");\n+        VMReg reg = _input_regs[src_pos++];\n+        out_regs[i].set2(reg);\n+        if (reg->is_stack())\n+          stk_slots += 2;\n+        break;\n+      }\n+      case T_VOID: \/\/ Halves of longs and doubles\n+        assert(i != 0 && (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), \"expecting half\");\n+        out_regs[i].set_bad();\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+        break;\n+    }\n+  }\n+  return stk_slots;\n+}\n+\n+\/\/ based on ComputeMoveOrder from x86_64 shared runtime code.\n+\/\/ with some changes.\n+class ForeignCMO: public StackObj {\n+  class MoveOperation: public ResourceObj {\n+    friend class ForeignCMO;\n+   private:\n+    VMRegPair        _src;\n+    VMRegPair        _dst;\n+    bool             _processed;\n+    MoveOperation*  _next;\n+    MoveOperation*  _prev;\n+    BasicType        _bt;\n+\n+    static int get_id(VMRegPair r) {\n+      return r.first()->value();\n+    }\n+\n+   public:\n+    MoveOperation(VMRegPair src, VMRegPair dst, BasicType bt):\n+      _src(src)\n+    , _dst(dst)\n+    , _processed(false)\n+    , _next(NULL)\n+    , _prev(NULL)\n+    , _bt(bt) {\n+    }\n+\n+    int src_id() const          { return get_id(_src); }\n+    int dst_id() const          { return get_id(_dst); }\n+    MoveOperation* next() const { return _next; }\n+    MoveOperation* prev() const { return _prev; }\n+    void set_processed()        { _processed = true; }\n+    bool is_processed() const   { return _processed; }\n+\n+    \/\/ insert\n+    void break_cycle(VMRegPair temp_register) {\n+      \/\/ create a new store following the last store\n+      \/\/ to move from the temp_register to the original\n+      MoveOperation* new_store = new MoveOperation(temp_register, _dst, _bt);\n+\n+      \/\/ break the cycle of links and insert new_store at the end\n+      \/\/ break the reverse link.\n+      MoveOperation* p = prev();\n+      assert(p->next() == this, \"must be\");\n+      _prev = NULL;\n+      p->_next = new_store;\n+      new_store->_prev = p;\n+\n+      \/\/ change the original store to save it's value in the temp.\n+      _dst = temp_register;\n+    }\n+\n+    void link(GrowableArray<MoveOperation*>& killer) {\n+      \/\/ link this store in front the store that it depends on\n+      MoveOperation* n = killer.at_grow(src_id(), NULL);\n+      if (n != NULL) {\n+        assert(_next == NULL && n->_prev == NULL, \"shouldn't have been set yet\");\n+        _next = n;\n+        n->_prev = this;\n+      }\n+    }\n+\n+    Move as_move() {\n+      return {_bt, _src, _dst};\n+    }\n+  };\n+\n+ private:\n+  GrowableArray<MoveOperation*> _edges;\n+  GrowableArray<Move> _moves;\n+\n+ public:\n+  ForeignCMO(int total_in_args, const VMRegPair* in_regs, int total_out_args, VMRegPair* out_regs,\n+             const BasicType* in_sig_bt, VMRegPair tmp_vmreg) : _edges(total_in_args), _moves(total_in_args) {\n+    assert(total_out_args >= total_in_args, \"can only add prefix args\");\n+    \/\/ Note that total_out_args args can be greater than total_in_args in the case of upcalls.\n+    \/\/ There will be a leading MH receiver arg in the out args in that case.\n+    \/\/\n+    \/\/ Leading args in the out args will be ignored below because we iterate from the end of\n+    \/\/ the register arrays until !(in_idx >= 0), and total_in_args is smaller.\n+    \/\/\n+    \/\/ Stub code adds a move for the receiver to j_rarg0 (and potential other prefix args) manually.\n+    for (int in_idx = total_in_args - 1, out_idx = total_out_args - 1; in_idx >= 0; in_idx--, out_idx--) {\n+      BasicType bt = in_sig_bt[in_idx];\n+      assert(bt != T_ARRAY, \"array not expected\");\n+      VMRegPair in_reg = in_regs[in_idx];\n+      VMRegPair out_reg = out_regs[out_idx];\n+\n+      if (out_reg.first()->is_stack()) {\n+        \/\/ Move operations where the dest is the stack can all be\n+        \/\/ scheduled first since they can't interfere with the other moves.\n+        \/\/ The input and output stack spaces are distinct from each other.\n+        Move move{bt, in_reg, out_reg};\n+        _moves.push(move);\n+      } else if (in_reg.first() == out_reg.first()\n+                 || bt == T_VOID) {\n+        \/\/ 1. Can skip non-stack identity moves.\n+        \/\/\n+        \/\/ 2. Upper half of long or double (T_VOID).\n+        \/\/    Don't need to do anything.\n+        continue;\n+      } else {\n+        _edges.append(new MoveOperation(in_reg, out_reg, bt));\n+      }\n+    }\n+    \/\/ Break any cycles in the register moves and emit the in the\n+    \/\/ proper order.\n+    compute_store_order(tmp_vmreg);\n+  }\n+\n+  \/\/ Walk the edges breaking cycles between moves.  The result list\n+  \/\/ can be walked in order to produce the proper set of loads\n+  void compute_store_order(VMRegPair temp_register) {\n+    \/\/ Record which moves kill which values\n+    GrowableArray<MoveOperation*> killer; \/\/ essentially a map of register id -> MoveOperation*\n+    for (int i = 0; i < _edges.length(); i++) {\n+      MoveOperation* s = _edges.at(i);\n+      assert(killer.at_grow(s->dst_id(), NULL) == NULL,\n+             \"multiple moves with the same register as destination\");\n+      killer.at_put_grow(s->dst_id(), s, NULL);\n+    }\n+    assert(killer.at_grow(MoveOperation::get_id(temp_register), NULL) == NULL,\n+           \"make sure temp isn't in the registers that are killed\");\n+\n+    \/\/ create links between loads and stores\n+    for (int i = 0; i < _edges.length(); i++) {\n+      _edges.at(i)->link(killer);\n+    }\n+\n+    \/\/ at this point, all the move operations are chained together\n+    \/\/ in one or more doubly linked lists.  Processing them backwards finds\n+    \/\/ the beginning of the chain, forwards finds the end.  If there's\n+    \/\/ a cycle it can be broken at any point,  so pick an edge and walk\n+    \/\/ backward until the list ends or we end where we started.\n+    for (int e = 0; e < _edges.length(); e++) {\n+      MoveOperation* s = _edges.at(e);\n+      if (!s->is_processed()) {\n+        MoveOperation* start = s;\n+        \/\/ search for the beginning of the chain or cycle\n+        while (start->prev() != NULL && start->prev() != s) {\n+          start = start->prev();\n+        }\n+        if (start->prev() == s) {\n+          start->break_cycle(temp_register);\n+        }\n+        \/\/ walk the chain forward inserting to store list\n+        while (start != NULL) {\n+          _moves.push(start->as_move());\n+\n+          start->set_processed();\n+          start = start->next();\n+        }\n+      }\n+    }\n+  }\n+\n+  GrowableArray<Move> moves() {\n+    return _moves;\n+  }\n+};\n+\n+ArgumentShuffle::ArgumentShuffle(\n+    BasicType* in_sig_bt,\n+    int num_in_args,\n+    BasicType* out_sig_bt,\n+    int num_out_args,\n+    const CallConvClosure* input_conv,\n+    const CallConvClosure* output_conv,\n+    VMReg shuffle_temp) {\n+\n+  VMRegPair* in_regs = NEW_RESOURCE_ARRAY(VMRegPair, num_in_args);\n+  input_conv->calling_convention(in_sig_bt, in_regs, num_in_args);\n+\n+  VMRegPair* out_regs = NEW_RESOURCE_ARRAY(VMRegPair, num_out_args);\n+  _out_arg_stack_slots = output_conv->calling_convention(out_sig_bt, out_regs, num_out_args);\n+\n+  VMRegPair tmp_vmreg;\n+  tmp_vmreg.set2(shuffle_temp);\n+\n+  \/\/ Compute a valid move order, using tmp_vmreg to break any cycles.\n+  \/\/ Note that ForeignCMO ignores the upper half of our VMRegPairs.\n+  \/\/ We are not moving Java values here, only register-sized values,\n+  \/\/ so we shouldn't have to worry about the upper half any ways.\n+  \/\/ This should work fine on 32-bit as well, since we would only be\n+  \/\/ moving 32-bit sized values (i.e. low-level MH shouldn't take any double\/long).\n+  ForeignCMO order(num_in_args, in_regs,\n+                   num_out_args, out_regs,\n+                   in_sig_bt, tmp_vmreg);\n+  _moves = order.moves();\n+}\n","filename":"src\/hotspot\/share\/prims\/foreign_globals.cpp","additions":397,"deletions":0,"binary":false,"changes":397,"status":"added"},{"patch":"@@ -0,0 +1,163 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_PRIMS_FOREIGN_GLOBALS\n+#define SHARE_PRIMS_FOREIGN_GLOBALS\n+\n+#include \"code\/vmreg.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+#include CPU_HEADER(foreign_globals)\n+\n+class CallConvClosure {\n+public:\n+  virtual int calling_convention(BasicType* sig_bt, VMRegPair* regs, int num_args) const = 0;\n+};\n+\n+struct CallRegs {\n+  VMReg* _arg_regs;\n+  int _args_length;\n+\n+  VMReg* _ret_regs;\n+  int _rets_length;\n+};\n+\n+class ForeignGlobals {\n+private:\n+  struct {\n+    int inputStorage_offset;\n+    int outputStorage_offset;\n+    int volatileStorage_offset;\n+    int stackAlignment_offset;\n+    int shadowSpace_offset;\n+    int targetAddrStorage_offset;\n+    int retBufAddrStorage_offset;\n+  } ABI;\n+\n+  struct {\n+    int index_offset;\n+    int type_offset;\n+  } VMS;\n+\n+  struct {\n+    int arg_regs_offset;\n+    int ret_regs_offset;\n+  } CallConvOffsets;\n+\n+  ForeignGlobals();\n+\n+  static const ForeignGlobals& instance();\n+\n+  template<typename T, typename Func>\n+  void loadArray(objArrayOop jarray, int type_index, GrowableArray<T>& array, Func converter) const;\n+\n+  const ABIDescriptor parse_abi_descriptor_impl(jobject jabi) const;\n+  const CallRegs parse_call_regs_impl(jobject jconv) const;\n+\n+  VMReg parse_vmstorage(oop storage) const;\n+public:\n+  static const ABIDescriptor parse_abi_descriptor(jobject jabi);\n+  static const CallRegs parse_call_regs(jobject jconv);\n+\n+  static VMReg vmstorage_to_vmreg(int type, int index);\n+};\n+\n+\n+\n+class JavaCallConv : public CallConvClosure {\n+public:\n+  int calling_convention(BasicType* sig_bt, VMRegPair* regs, int num_args) const override {\n+    return SharedRuntime::java_calling_convention(sig_bt, regs, num_args);\n+  }\n+};\n+\n+class NativeCallConv : public CallConvClosure {\n+  const VMReg* _input_regs;\n+  int _input_regs_length;\n+public:\n+  NativeCallConv(const VMReg* input_regs, int input_regs_length) :\n+    _input_regs(input_regs),\n+    _input_regs_length(input_regs_length) {\n+  }\n+  NativeCallConv(const GrowableArray<VMReg>& input_regs)\n+   : NativeCallConv(input_regs.data(), input_regs.length()) {}\n+\n+  int calling_convention(BasicType* sig_bt, VMRegPair* out_regs, int num_args) const override;\n+};\n+\n+class RegSpiller {\n+  const VMReg* _regs;\n+  int _num_regs;\n+  int _spill_size_bytes;\n+public:\n+  RegSpiller(const VMReg* regs, int num_regs) :\n+    _regs(regs), _num_regs(num_regs),\n+    _spill_size_bytes(compute_spill_area(regs, num_regs)) {\n+  }\n+  RegSpiller(const GrowableArray<VMReg>& regs) : RegSpiller(regs.data(), regs.length()) {\n+  }\n+\n+  int spill_size_bytes() const { return _spill_size_bytes; }\n+  void generate_spill(MacroAssembler* masm, int rsp_offset) const { return generate(masm, rsp_offset, true); }\n+  void generate_fill(MacroAssembler* masm, int rsp_offset) const { return generate(masm, rsp_offset, false); }\n+\n+private:\n+  static int compute_spill_area(const VMReg* regs, int num_regs);\n+  void generate(MacroAssembler* masm, int rsp_offset, bool is_spill) const;\n+\n+  static int pd_reg_size(VMReg reg);\n+  static void pd_store_reg(MacroAssembler* masm, int offset, VMReg reg);\n+  static void pd_load_reg(MacroAssembler* masm, int offset, VMReg reg);\n+};\n+\n+struct Move {\n+  BasicType bt;\n+  VMRegPair from;\n+  VMRegPair to;\n+};\n+\n+class ArgumentShuffle {\n+private:\n+  GrowableArray<Move> _moves;\n+  int _out_arg_stack_slots;\n+public:\n+  ArgumentShuffle(\n+    BasicType* in_sig_bt, int num_in_args,\n+    BasicType* out_sig_bt, int num_out_args,\n+    const CallConvClosure* input_conv, const CallConvClosure* output_conv,\n+    VMReg shuffle_temp);\n+\n+  int out_arg_stack_slots() const { return _out_arg_stack_slots; }\n+  void generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const {\n+    pd_generate(masm, tmp, in_stk_bias, out_stk_bias);\n+  }\n+\n+  void print_on(outputStream* os) const;\n+private:\n+  void pd_generate(MacroAssembler* masm, VMReg tmp, int in_stk_bias, int out_stk_bias) const;\n+};\n+\n+#endif \/\/ SHARE_PRIMS_FOREIGN_GLOBALS\n","filename":"src\/hotspot\/share\/prims\/foreign_globals.hpp","additions":163,"deletions":0,"binary":false,"changes":163,"status":"added"},{"patch":"@@ -33,0 +33,1 @@\n+<<<<<<< HEAD\n@@ -39,0 +40,12 @@\n+=======\n+#include \"prims\/foreign_globals.inline.hpp\"\n+#include \"prims\/universalNativeInvoker.hpp\"\n+#include \"runtime\/jniHandles.inline.hpp\"\n+\n+JNI_LEAF(jlong, NEP_vmStorageToVMReg(JNIEnv* env, jclass _unused, jint type, jint index))\n+  return ForeignGlobals::vmstorage_to_vmreg(type, index)->value();\n+JNI_END\n+\n+JNI_ENTRY(jlong, NEP_makeInvoker(JNIEnv* env, jclass _unused, jobject method_type, jobject jabi,\n+                                 jlongArray arg_moves, jlongArray ret_moves, jboolean needs_return_buffer))\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -43,0 +56,1 @@\n+<<<<<<< HEAD\n@@ -45,0 +59,4 @@\n+=======\n+  typeArrayOop arg_moves_oop = oop_cast<typeArrayOop>(JNIHandles::resolve(arg_moves));\n+  typeArrayOop ret_moves_oop = oop_cast<typeArrayOop>(JNIHandles::resolve(ret_moves));\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -55,0 +73,1 @@\n+<<<<<<< HEAD\n@@ -56,0 +75,3 @@\n+=======\n+    input_regs.push(VMRegImpl::as_VMReg(arg_moves_oop->long_at(i)));\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -60,0 +82,1 @@\n+<<<<<<< HEAD\n@@ -62,0 +85,4 @@\n+=======\n+      \/\/ NativeCallConv ignores them, but they are needed\n+      \/\/ for JavaCallConv\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -73,0 +100,1 @@\n+<<<<<<< HEAD\n@@ -88,0 +116,35 @@\n+=======\n+    output_regs.push(VMRegImpl::as_VMReg(ret_moves_oop->long_at(i)));\n+  }\n+\n+#ifdef ASSERT\n+  LogTarget(Trace, panama) lt;\n+  if (lt.is_enabled()) {\n+    ResourceMark rm;\n+    LogStream ls(lt);\n+    ls.print_cr(\"Generating native invoker {\");\n+    ls.print(\"BasicType { \");\n+    for (int i = 0; i < pslots; i++) {\n+      ls.print(\"%s, \", null_safe_string(type2name(basic_type[i])));\n+    }\n+    ls.print_cr(\"}\");\n+    ls.print_cr(\"shadow_space_bytes = %d\", abi._shadow_space_bytes);\n+    ls.print(\"input_registers { \");\n+    for (int i = 0; i < input_regs.length(); i++) {\n+      VMReg reg = input_regs.at(i);\n+      ls.print(\"%s (\" INTPTR_FORMAT \"), \", reg->name(), reg->value());\n+    }\n+    ls.print_cr(\"}\");\n+      ls.print(\"output_registers { \");\n+    for (int i = 0; i < output_regs.length(); i++) {\n+      VMReg reg = output_regs.at(i);\n+      ls.print(\"%s (\" INTPTR_FORMAT \"), \", reg->name(), reg->value());\n+    }\n+    ls.print_cr(\"}\");\n+    ls.print_cr(\"}\");\n+  }\n+#endif\n+\n+  return (jlong) ProgrammableInvoker::make_native_invoker(\n+    basic_type, pslots, ret_bt, abi, input_regs, output_regs, needs_return_buffer)->code_begin();\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -97,0 +160,1 @@\n+<<<<<<< HEAD\n@@ -99,0 +163,4 @@\n+=======\n+  {CC \"vmStorageToVMReg\", CC \"(II)J\", FN_PTR(NEP_vmStorageToVMReg)},\n+  {CC \"makeInvoker\", CC \"(Ljava\/lang\/invoke\/MethodType;Ljdk\/internal\/invoke\/ABIDescriptorProxy;[J[JZ)J\", FN_PTR(NEP_makeInvoker)},\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -109,0 +177,1 @@\n+<<<<<<< HEAD\n@@ -110,0 +179,3 @@\n+=======\n+            \"register jdk.internal.invoke.NativeEntryPoint natives\");\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/prims\/nativeEntryPoint.cpp","additions":72,"deletions":0,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -205,0 +205,1 @@\n+<<<<<<< HEAD\n@@ -206,0 +207,3 @@\n+=======\n+  void JNICALL JVM_RegisterProgrammableUpcallHandlerMethods(JNIEnv *env, jclass unsafecls);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -223,0 +227,1 @@\n+<<<<<<< HEAD\n@@ -225,0 +230,4 @@\n+=======\n+  { CC\"Java_jdk_internal_foreign_abi_ProgrammableUpcallHandler_registerNatives\",      NULL, FN_PTR(JVM_RegisterProgrammableUpcallHandlerMethods) },\n+  { CC\"Java_jdk_internal_invoke_NativeEntryPoint_registerNatives\",      NULL, FN_PTR(JVM_RegisterNativeEntryPointMethods) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/prims\/nativeLookup.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -0,0 +1,42 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_VM_PRIMS_UNIVERSALNATIVEINVOKER_HPP\n+#define SHARE_VM_PRIMS_UNIVERSALNATIVEINVOKER_HPP\n+\n+#include \"prims\/foreign_globals.hpp\"\n+\n+class RuntimeStub;\n+\n+class ProgrammableInvoker: AllStatic {\n+public:\n+  static RuntimeStub* make_native_invoker(BasicType*,\n+                                          int num_args,\n+                                          BasicType ret_bt,\n+                                          const ABIDescriptor& abi,\n+                                          const GrowableArray<VMReg>& input_registers,\n+                                          const GrowableArray<VMReg>& output_registers,\n+                                          bool needs_return_buffer);\n+};\n+\n+#endif \/\/ SHARE_VM_PRIMS_UNIVERSALNATIVEINVOKER_HPP\n","filename":"src\/hotspot\/share\/prims\/universalNativeInvoker.hpp","additions":42,"deletions":0,"binary":false,"changes":42,"status":"added"},{"patch":"@@ -0,0 +1,50 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_VM_PRIMS_UNIVERSALUPCALLHANDLER_HPP\n+#define SHARE_VM_PRIMS_UNIVERSALUPCALLHANDLER_HPP\n+\n+#include \"asm\/codeBuffer.hpp\"\n+#include \"code\/codeBlob.hpp\"\n+#include \"prims\/foreign_globals.hpp\"\n+\n+class JavaThread;\n+\n+class ProgrammableUpcallHandler {\n+private:\n+  static void handle_uncaught_exception(oop exception);\n+  static JavaThread* maybe_attach_and_get_thread();\n+  static void detach_current_thread();\n+\n+  static JavaThread* on_entry(OptimizedEntryBlob::FrameData* context);\n+  static void on_exit(OptimizedEntryBlob::FrameData* context);\n+public:\n+  static address generate_optimized_upcall_stub(jobject mh, Method* entry,\n+                                                BasicType* in_sig_bt, int total_in_args,\n+                                                BasicType* out_sig_bt, int total_out_args,\n+                                                BasicType ret_type,\n+                                                jobject jabi, jobject jconv,\n+                                                bool needs_return_buffer, int ret_buf_size);\n+};\n+\n+#endif \/\/ SHARE_VM_PRIMS_UNIVERSALUPCALLHANDLER_HPP\n","filename":"src\/hotspot\/share\/prims\/universalUpcallHandler.hpp","additions":50,"deletions":0,"binary":false,"changes":50,"status":"added"},{"patch":"@@ -58,0 +58,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/prims\/upcallLinker.cpp\n@@ -61,0 +62,6 @@\n+=======\n+\n+APPROVED_CPP_THREAD_LOCAL UpcallContext threadContext;\n+\n+JavaThread* ProgrammableUpcallHandler::maybe_attach_and_get_thread() {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/share\/prims\/universalUpcallHandler.cpp\n@@ -75,0 +82,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/prims\/upcallLinker.cpp\n@@ -76,0 +84,3 @@\n+=======\n+JavaThread* ProgrammableUpcallHandler::on_entry(OptimizedEntryBlob::FrameData* context) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/share\/prims\/universalUpcallHandler.cpp\n@@ -136,0 +147,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/prims\/upcallLinker.cpp\n@@ -137,0 +149,3 @@\n+=======\n+void ProgrammableUpcallHandler::handle_uncaught_exception(oop exception) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/share\/prims\/universalUpcallHandler.cpp\n@@ -144,0 +159,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/prims\/upcallLinker.cpp\n@@ -145,0 +161,3 @@\n+=======\n+JVM_ENTRY(jlong, PUH_AllocateOptimizedUpcallStub(JNIEnv *env, jclass unused, jobject mh, jobject abi, jobject conv,\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/share\/prims\/universalUpcallHandler.cpp\n@@ -180,0 +199,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/prims\/upcallLinker.cpp\n@@ -181,0 +201,3 @@\n+=======\n+  return (jlong) ProgrammableUpcallHandler::generate_optimized_upcall_stub(\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/share\/prims\/universalUpcallHandler.cpp\n@@ -187,0 +210,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/prims\/upcallLinker.cpp\n@@ -189,0 +213,4 @@\n+=======\n+static JNINativeMethod PUH_methods[] = {\n+  {CC \"allocateOptimizedUpcallStub\", CC \"(\" \"Ljava\/lang\/invoke\/MethodHandle;\" \"L\" FOREIGN_ABI \"ABIDescriptor;\" \"L\" FOREIGN_ABI \"ProgrammableUpcallHandler$CallRegs;\" \"ZJ)J\", FN_PTR(PUH_AllocateOptimizedUpcallStub)},\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/share\/prims\/universalUpcallHandler.cpp\n","filename":"src\/hotspot\/share\/prims\/upcallLinker.cpp","additions":28,"deletions":0,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+<<<<<<< HEAD\n@@ -36,0 +37,3 @@\n+=======\n+  OptimizedEntryBlob::free(cb->as_optimized_entry_blob());\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/hotspot\/share\/prims\/upcallStubs.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+<<<<<<< HEAD:src\/hotspot\/share\/runtime\/threadIdentifier.cpp\n@@ -42,0 +43,11 @@\n+=======\n+address ProgrammableUpcallHandler::generate_optimized_upcall_stub(jobject receiver, Method* entry,\n+                                                                  BasicType* in_sig_bt, int total_in_args,\n+                                                                  BasicType* out_sig_bt, int total_out_args,\n+                                                                  BasicType ret_type,\n+                                                                  jobject jabi, jobject jconv,\n+                                                                  bool needs_return_buffer, int ret_buf_size) {\n+  ShouldNotCallThis();\n+  return nullptr;\n+}\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/x86\/universalUpcallHandler_x86_32.cpp\n","filename":"src\/hotspot\/share\/runtime\/threadIdentifier.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -1588,0 +1588,9 @@\n+<<<<<<< HEAD\n+=======\n+            public VarHandle memoryAccessVarHandle(Class<?> carrier, boolean skipAlignmentMaskCheck, long alignmentMask,\n+                                                   ByteOrder order) {\n+                return VarHandles.makeMemoryAddressViewHandle(carrier, skipAlignmentMaskCheck, alignmentMask, order);\n+            }\n+\n+            @Override\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/java.base\/share\/classes\/java\/lang\/invoke\/MethodHandleImpl.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -136,0 +136,10 @@\n+<<<<<<< HEAD\n+=======\n+\n+    \/**\n+     * Ensure given method handle is customized\n+     *\n+     * @param mh the method handle\n+     *\/\n+    void ensureCustomized(MethodHandle mh);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/access\/JavaLangInvokeAccess.java","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -57,0 +57,11 @@\n+<<<<<<< HEAD:src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ABIDescriptor.java\n+    }\n+\n+    public VMStorage targetAddrStorage() {\n+        return targetAddrStorage;\n+=======\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/ABIDescriptor.java\n+    }\n+\n+    public VMStorage retBufAddrStorage() {\n+        return retBufAddrStorage;\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/ABIDescriptor.java","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -278,0 +278,1 @@\n+<<<<<<< HEAD:src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java\n@@ -281,0 +282,4 @@\n+=======\n+        if (destIndex < sourceIndex)\n+            sourceIndex--;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java\n@@ -317,0 +322,183 @@\n+<<<<<<< HEAD:src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java\n+=======\n+    static MethodHandle wrapWithAllocator(MethodHandle specializedHandle,\n+                                          int allocatorPos, long allocationSize,\n+                                          boolean upcall) {\n+        \/\/ insert try-finally to close the NativeScope used for Binding.Copy\n+        MethodHandle closer;\n+        int insertPos;\n+        if (specializedHandle.type().returnType() == void.class) {\n+            if (!upcall) {\n+                closer = empty(methodType(void.class, Throwable.class)); \/\/ (Throwable) -> void\n+            } else {\n+                closer = MH_HANDLE_UNCAUGHT_EXCEPTION;\n+            }\n+            insertPos = 1;\n+        } else {\n+            closer = identity(specializedHandle.type().returnType()); \/\/ (V) -> V\n+            if (!upcall) {\n+                closer = dropArguments(closer, 0, Throwable.class); \/\/ (Throwable, V) -> V\n+            } else {\n+                closer = collectArguments(closer, 0, MH_HANDLE_UNCAUGHT_EXCEPTION); \/\/ (Throwable, V) -> V\n+            }\n+            insertPos = 2;\n+        }\n+\n+        \/\/ downcalls get the leading SegmentAllocator param as well\n+        if (!upcall) {\n+            closer = dropArguments(closer, insertPos++, SegmentAllocator.class); \/\/ (Throwable, V?, SegmentAllocator, NativeSymbol) -> V\/void\n+        }\n+\n+        closer = collectArguments(closer, insertPos, MH_CLOSE_CONTEXT); \/\/ (Throwable, V?, SegmentAllocator?, BindingContext) -> V\/void\n+\n+        MethodHandle contextFactory;\n+\n+        if (allocationSize > 0) {\n+            contextFactory = MethodHandles.insertArguments(MH_MAKE_CONTEXT_BOUNDED_ALLOCATOR, 0, allocationSize);\n+        } else if (upcall) {\n+            contextFactory = MH_MAKE_CONTEXT_NO_ALLOCATOR;\n+        } else {\n+            \/\/ this path is probably never used now, since ProgrammableInvoker never calls this routine with bufferCopySize == 0\n+            contextFactory = constant(Binding.Context.class, Binding.Context.DUMMY);\n+        }\n+\n+        specializedHandle = tryFinally(specializedHandle, closer);\n+        specializedHandle = collectArguments(specializedHandle, allocatorPos, contextFactory);\n+        return specializedHandle;\n+    }\n+\n+    @ForceInline\n+    @SuppressWarnings(\"fallthrough\")\n+    public static void acquire(Scoped[] args) {\n+        ResourceScope scope4 = null;\n+        ResourceScope scope3 = null;\n+        ResourceScope scope2 = null;\n+        ResourceScope scope1 = null;\n+        ResourceScope scope0 = null;\n+        switch (args.length) {\n+            default:\n+                \/\/ slow path, acquire all remaining addressable parameters in isolation\n+                for (int i = 5 ; i < args.length ; i++) {\n+                    acquire(args[i].scope());\n+                }\n+            \/\/ fast path, acquire only scopes not seen in other parameters\n+            case 5:\n+                scope4 = args[4].scope();\n+                acquire(scope4);\n+            case 4:\n+                scope3 = args[3].scope();\n+                if (scope3 != scope4)\n+                    acquire(scope3);\n+            case 3:\n+                scope2 = args[2].scope();\n+                if (scope2 != scope3 && scope2 != scope4)\n+                    acquire(scope2);\n+            case 2:\n+                scope1 = args[1].scope();\n+                if (scope1 != scope2 && scope1 != scope3 && scope1 != scope4)\n+                    acquire(scope1);\n+            case 1:\n+                scope0 = args[0].scope();\n+                if (scope0 != scope1 && scope0 != scope2 && scope0 != scope3 && scope0 != scope4)\n+                    acquire(scope0);\n+            case 0: break;\n+        }\n+    }\n+\n+    @ForceInline\n+    @SuppressWarnings(\"fallthrough\")\n+    public static void release(Scoped[] args) {\n+        ResourceScope scope4 = null;\n+        ResourceScope scope3 = null;\n+        ResourceScope scope2 = null;\n+        ResourceScope scope1 = null;\n+        ResourceScope scope0 = null;\n+        switch (args.length) {\n+            default:\n+                \/\/ slow path, release all remaining addressable parameters in isolation\n+                for (int i = 5 ; i < args.length ; i++) {\n+                    release(args[i].scope());\n+                }\n+            \/\/ fast path, release only scopes not seen in other parameters\n+            case 5:\n+                scope4 = args[4].scope();\n+                release(scope4);\n+            case 4:\n+                scope3 = args[3].scope();\n+                if (scope3 != scope4)\n+                    release(scope3);\n+            case 3:\n+                scope2 = args[2].scope();\n+                if (scope2 != scope3 && scope2 != scope4)\n+                    release(scope2);\n+            case 2:\n+                scope1 = args[1].scope();\n+                if (scope1 != scope2 && scope1 != scope3 && scope1 != scope4)\n+                    release(scope1);\n+            case 1:\n+                scope0 = args[0].scope();\n+                if (scope0 != scope1 && scope0 != scope2 && scope0 != scope3 && scope0 != scope4)\n+                    release(scope0);\n+            case 0: break;\n+        }\n+    }\n+\n+    @ForceInline\n+    private static void acquire(ResourceScope scope) {\n+        ((ResourceScopeImpl)scope).acquire0();\n+    }\n+\n+    @ForceInline\n+    private static void release(ResourceScope scope) {\n+        ((ResourceScopeImpl)scope).release0();\n+    }\n+\n+    \/*\n+     * This method adds a try\/finally block to a downcall method handle, to make sure that all by-reference\n+     * parameters (including the target address of the native function) are kept alive for the duration of\n+     * the downcall.\n+     *\/\n+    public static MethodHandle wrapDowncall(MethodHandle downcallHandle, FunctionDescriptor descriptor) {\n+        boolean hasReturn = descriptor.returnLayout().isPresent();\n+        MethodHandle tryBlock = downcallHandle;\n+        MethodHandle cleanup = hasReturn ?\n+                MethodHandles.identity(downcallHandle.type().returnType()) :\n+                MethodHandles.empty(MethodType.methodType(void.class));\n+        int addressableCount = 0;\n+        List<UnaryOperator<MethodHandle>> adapters = new ArrayList<>();\n+        for (int i = 0 ; i < downcallHandle.type().parameterCount() ; i++) {\n+            Class<?> ptype = downcallHandle.type().parameterType(i);\n+            if (ptype == Addressable.class || ptype == NativeSymbol.class) {\n+                addressableCount++;\n+            } else {\n+                int pos = i;\n+                adapters.add(mh -> dropArguments(mh, pos, ptype));\n+            }\n+        }\n+\n+        if (addressableCount > 0) {\n+            cleanup = dropArguments(cleanup, 0, Throwable.class);\n+\n+            MethodType adapterType = MethodType.methodType(void.class);\n+            for (int i = 0 ; i < addressableCount ; i++) {\n+                adapterType = adapterType.appendParameterTypes(i == 0 ? NativeSymbol.class : Addressable.class);\n+            }\n+\n+            MethodHandle acquireHandle = ACQUIRE_MH.asCollector(Scoped[].class, addressableCount).asType(adapterType);\n+            MethodHandle releaseHandle = RELEASE_MH.asCollector(Scoped[].class, addressableCount).asType(adapterType);\n+\n+            for (UnaryOperator<MethodHandle> adapter : adapters) {\n+                acquireHandle = adapter.apply(acquireHandle);\n+                releaseHandle = adapter.apply(releaseHandle);\n+            }\n+\n+            tryBlock = foldArguments(tryBlock, acquireHandle);\n+            cleanup = collectArguments(cleanup, hasReturn ? 2 : 1, releaseHandle);\n+\n+            return tryFinally(tryBlock, cleanup);\n+        } else {\n+            return downcallHandle;\n+        }\n+    }\n+\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java\n@@ -330,0 +518,15 @@\n+<<<<<<< HEAD:src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java\n+=======\n+    }\n+\n+    \/\/ lazy init MH_ALLOC and MH_FREE handles\n+    private static class AllocHolder {\n+\n+        private static final CLinker SYS_LINKER = getSystemLinker();\n+\n+        static final MethodHandle MH_MALLOC = SYS_LINKER.downcallHandle(CLinker.systemCLinker().lookup(\"malloc\").get(),\n+                FunctionDescriptor.of(ADDRESS, JAVA_LONG));\n+\n+        static final MethodHandle MH_FREE = SYS_LINKER.downcallHandle(CLinker.systemCLinker().lookup(\"free\").get(),\n+                FunctionDescriptor.ofVoid(ADDRESS));\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/SharedUtils.java","additions":203,"deletions":0,"binary":false,"changes":203,"status":"modified"},{"patch":"@@ -169,0 +169,1 @@\n+<<<<<<< HEAD:src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/AArch64Architecture.java\n@@ -170,0 +171,3 @@\n+=======\n+                targetAddrStorage, retBufAddrStorage);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/AArch64Architecture.java\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/AArch64Architecture.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+<<<<<<< HEAD:src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/linux\/LinuxAArch64Linker.java\n@@ -57,0 +58,8 @@\n+=======\n+    public final MethodHandle downcallHandle(FunctionDescriptor function) {\n+        Objects.requireNonNull(function);\n+        MethodType type = SharedUtils.inferMethodType(function, false);\n+        MethodHandle handle = CallArranger.LINUX.arrangeDowncall(type, function);\n+        handle = SharedUtils.maybeInsertAllocator(handle);\n+        return SharedUtils.wrapDowncall(handle, function);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/linux\/LinuxAArch64Linker.java\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/linux\/LinuxAArch64Linker.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+<<<<<<< HEAD:src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/macos\/MacOsAArch64Linker.java\n@@ -57,0 +58,8 @@\n+=======\n+    public final MethodHandle downcallHandle(FunctionDescriptor function) {\n+        Objects.requireNonNull(function);\n+        MethodType type = SharedUtils.inferMethodType(function, false);\n+        MethodHandle handle = CallArranger.MACOS.arrangeDowncall(type, function);\n+        handle = SharedUtils.maybeInsertAllocator(handle);\n+        return SharedUtils.wrapDowncall(handle, function);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/macos\/MacOsAArch64Linker.java\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/aarch64\/macos\/MacOsAArch64Linker.java","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -156,0 +156,1 @@\n+<<<<<<< HEAD:src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/X86_64Architecture.java\n@@ -157,0 +158,3 @@\n+=======\n+                targetAddrStorage, retBufAddrStorage);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/X86_64Architecture.java\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/X86_64Architecture.java","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -0,0 +1,111 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.internal.invoke;\n+\n+import java.lang.invoke.MethodType;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+\/**\n+ * This class describes a native call, including arguments\/return shuffle moves, PC entry point and\n+ * various other info which are relevant when the call will be intrinsified by C2.\n+ *\/\n+public class NativeEntryPoint {\n+    static {\n+        registerNatives();\n+    }\n+\n+    private final int shadowSpace;\n+\n+    \/\/ encoded as VMRegImpl*\n+    private final long[] argMoves;\n+    private final long[] returnMoves;\n+\n+    private final boolean needTransition;\n+    private final MethodType methodType; \/\/ C2 sees erased version (byte -> int), so need this explicitly\n+    private final String name;\n+\n+    private final long invoker;\n+\n+    private static final Map<CacheKey, Long> INVOKER_CACHE = new ConcurrentHashMap<>();\n+    private record CacheKey(MethodType mt, int shadowSpaceBytes,\n+                            List<VMStorageProxy> argMoves, List<VMStorageProxy> retMoves) {}\n+\n+    private NativeEntryPoint(int shadowSpace, long[] argMoves, long[] returnMoves,\n+                     boolean needTransition, MethodType methodType, String name, long invoker) {\n+        this.shadowSpace = shadowSpace;\n+        this.argMoves = Objects.requireNonNull(argMoves);\n+        this.returnMoves = Objects.requireNonNull(returnMoves);\n+        this.needTransition = needTransition;\n+        this.methodType = methodType;\n+        this.name = name;\n+        this.invoker = invoker;\n+    }\n+\n+    public static NativeEntryPoint make(String name, ABIDescriptorProxy abi,\n+                                        VMStorageProxy[] argMoves, VMStorageProxy[] returnMoves,\n+                                        boolean needTransition, MethodType methodType, boolean needsReturnBuffer) {\n+        if (returnMoves.length > 1 != needsReturnBuffer) {\n+            throw new IllegalArgumentException(\"Multiple register return, but needsReturnBuffer was false\");\n+        }\n+\n+        assert (methodType.parameterType(0) == long.class) : \"Address expected\";\n+        assert (!needsReturnBuffer || methodType.parameterType(1) == long.class) : \"IMR address expected\";\n+\n+        int shadowSpaceBytes = abi.shadowSpaceBytes();\n+        long[] encArgMoves = encodeVMStorages(argMoves);\n+        long[] encRetMoves = encodeVMStorages(returnMoves);\n+\n+        CacheKey key = new CacheKey(methodType, abi.shadowSpaceBytes(),\n+                Arrays.asList(argMoves), Arrays.asList(returnMoves));\n+        long invoker = INVOKER_CACHE.computeIfAbsent(key, k ->\n+            makeInvoker(methodType, abi, encArgMoves, encRetMoves, needsReturnBuffer));\n+\n+        return new NativeEntryPoint(shadowSpaceBytes, encArgMoves, encRetMoves,\n+                needTransition, methodType, name, invoker);\n+    }\n+\n+    private static long[] encodeVMStorages(VMStorageProxy[] moves) {\n+        long[] out = new long[moves.length];\n+        for (int i = 0; i < moves.length; i++) {\n+            out[i] = vmStorageToVMReg(moves[i].type(), moves[i].index());\n+        }\n+        return out;\n+    }\n+\n+    private static native long vmStorageToVMReg(int type, int index);\n+\n+    private static native long makeInvoker(MethodType methodType, ABIDescriptorProxy abi, long[] encArgMoves, long[] encRetMoves, boolean needsReturnBuffer);\n+\n+    public MethodType type() {\n+        return methodType;\n+    }\n+\n+    private static native void registerNatives();\n+}\n","filename":"src\/java.base\/share\/classes\/jdk\/internal\/invoke\/NativeEntryPoint.java","additions":111,"deletions":0,"binary":false,"changes":111,"status":"added"},{"patch":"@@ -0,0 +1,114 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi;\n+\n+import jdk.incubator.foreign.FunctionDescriptor;\n+\n+import java.lang.invoke.MethodType;\n+import java.util.List;\n+import java.util.stream.Stream;\n+\n+public class CallingSequence {\n+    private final MethodType mt;\n+    private final FunctionDescriptor desc;\n+    private final boolean isTrivial;\n+    private final boolean needsReturnBuffer;\n+    private final long returnBufferSize;\n+    private final long allocationSize;\n+\n+    private final List<Binding> returnBindings;\n+    private final List<List<Binding>> argumentBindings;\n+\n+    public CallingSequence(MethodType mt, FunctionDescriptor desc,\n+                           boolean isTrivial, boolean needsReturnBuffer, long returnBufferSize, long allocationSize,\n+                           List<List<Binding>> argumentBindings, List<Binding> returnBindings) {\n+        this.mt = mt;\n+        this.desc = desc;\n+        this.isTrivial = isTrivial;\n+        this.needsReturnBuffer = needsReturnBuffer;\n+        this.returnBufferSize = returnBufferSize;\n+        this.allocationSize = allocationSize;\n+        this.returnBindings = returnBindings;\n+        this.argumentBindings = argumentBindings;\n+    }\n+\n+    public int argumentCount() {\n+        return argumentBindings.size();\n+    }\n+\n+    public List<Binding> argumentBindings(int i) {\n+        return argumentBindings.get(i);\n+    }\n+\n+    public Stream<Binding> argumentBindings() {\n+        return argumentBindings.stream().flatMap(List::stream);\n+    }\n+\n+    public List<Binding> returnBindings() {\n+        return returnBindings;\n+    }\n+\n+    public String asString() {\n+        StringBuilder sb = new StringBuilder();\n+\n+        sb.append(\"CallingSequence: {\\n\");\n+        sb.append(\"  MethodType: \").append(mt);\n+        sb.append(\"  FunctionDescriptor: \").append(desc);\n+        sb.append(\"  Argument Bindings:\\n\");\n+        for (int i = 0; i < mt.parameterCount(); i++) {\n+            sb.append(\"    \").append(i).append(\": \").append(argumentBindings.get(i)).append(\"\\n\");\n+        }\n+        if (mt.returnType() != void.class) {\n+            sb.append(\"    \").append(\"Return: \").append(returnBindings).append(\"\\n\");\n+        }\n+        sb.append(\"}\\n\");\n+\n+        return sb.toString();\n+    }\n+\n+    public MethodType methodType() {\n+        return mt;\n+    }\n+\n+    public FunctionDescriptor functionDesc() {\n+        return desc;\n+    }\n+\n+    public boolean isTrivial() {\n+        return isTrivial;\n+    }\n+\n+    public boolean needsReturnBuffer() {\n+        return needsReturnBuffer;\n+    }\n+\n+    public long returnBufferSize() {\n+        return returnBufferSize;\n+    }\n+\n+    public long allocationSize() {\n+        return allocationSize;\n+    }\n+}\n","filename":"src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/CallingSequence.java","additions":114,"deletions":0,"binary":false,"changes":114,"status":"added"},{"patch":"@@ -0,0 +1,214 @@\n+\/*\n+ *  Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ *  DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ *  This code is free software; you can redistribute it and\/or modify it\n+ *  under the terms of the GNU General Public License version 2 only, as\n+ *  published by the Free Software Foundation.  Oracle designates this\n+ *  particular file as subject to the \"Classpath\" exception as provided\n+ *  by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ *  This code is distributed in the hope that it will be useful, but WITHOUT\n+ *  ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ *  FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ *  version 2 for more details (a copy is included in the LICENSE file that\n+ *  accompanied this code).\n+ *\n+ *  You should have received a copy of the GNU General Public License version\n+ *  2 along with this work; if not, write to the Free Software Foundation,\n+ *  Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ *  Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ *  or visit www.oracle.com if you need additional information or have any\n+ *  questions.\n+ *\/\n+package jdk.internal.foreign.abi;\n+\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.MemoryLayout;\n+import jdk.incubator.foreign.MemorySegment;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.ValueLayout;\n+import jdk.internal.foreign.Utils;\n+import sun.security.action.GetPropertyAction;\n+\n+import java.lang.invoke.MethodType;\n+import java.util.ArrayDeque;\n+import java.util.ArrayList;\n+import java.util.Deque;\n+import java.util.EnumSet;\n+import java.util.List;\n+import java.util.Set;\n+\n+import static jdk.internal.foreign.abi.Binding.Tag.*;\n+\n+public class CallingSequenceBuilder {\n+    private static final boolean VERIFY_BINDINGS = Boolean.parseBoolean(\n+            GetPropertyAction.privilegedGetProperty(\"jdk.incubator.foreign.VERIFY_BINDINGS\", \"true\"));\n+\n+    private final ABIDescriptor abi;\n+\n+    private boolean isTrivial;\n+    private final boolean forUpcall;\n+    private final List<List<Binding>> inputBindings = new ArrayList<>();\n+    private List<Binding> outputBindings = List.of();\n+\n+    private MethodType mt = MethodType.methodType(void.class);\n+    private FunctionDescriptor desc = FunctionDescriptor.ofVoid();\n+\n+    public CallingSequenceBuilder(ABIDescriptor abi, boolean forUpcall) {\n+        this.abi = abi;\n+        this.forUpcall = forUpcall;\n+    }\n+\n+    public final CallingSequenceBuilder addArgumentBindings(Class<?> carrier, MemoryLayout layout,\n+                                                            List<Binding> bindings) {\n+        addArgumentBinding(inputBindings.size(), carrier, layout, bindings);\n+        return this;\n+    }\n+\n+    private void addArgumentBinding(int index, Class<?> carrier, MemoryLayout layout, List<Binding> bindings) {\n+        verifyBindings(true, carrier, bindings);\n+        inputBindings.add(index, bindings);\n+        mt = mt.insertParameterTypes(index, carrier);\n+        desc = desc.insertArgumentLayouts(index, layout);\n+    }\n+\n+    public CallingSequenceBuilder setReturnBindings(Class<?> carrier, MemoryLayout layout,\n+                                                    List<Binding> bindings) {\n+        verifyBindings(false, carrier, bindings);\n+        this.outputBindings = bindings;\n+        mt = mt.changeReturnType(carrier);\n+        desc = desc.changeReturnLayout(layout);\n+        return this;\n+    }\n+\n+    public CallingSequenceBuilder setTrivial(boolean isTrivial) {\n+        this.isTrivial = isTrivial;\n+        return this;\n+    }\n+\n+    private boolean needsReturnBuffer() {\n+        return outputBindings.stream()\n+            .filter(Binding.Move.class::isInstance)\n+            .count() > 1;\n+    }\n+\n+    public CallingSequence build() {\n+        boolean needsReturnBuffer = needsReturnBuffer();\n+        long returnBufferSize = needsReturnBuffer ? computeReturnBuferSize() : 0;\n+        long allocationSize = computeAllocationSize() + returnBufferSize;\n+        if (!forUpcall) {\n+            addArgumentBinding(0, NativeSymbol.class, ValueLayout.ADDRESS, List.of(\n+                Binding.unboxAddress(NativeSymbol.class),\n+                Binding.vmStore(abi.targetAddrStorage(), long.class)));\n+            if (needsReturnBuffer) {\n+                addArgumentBinding(0, MemorySegment.class, ValueLayout.ADDRESS, List.of(\n+                    Binding.unboxAddress(MemorySegment.class),\n+                    Binding.vmStore(abi.retBufAddrStorage(), long.class)));\n+            }\n+        } else if (needsReturnBuffer) { \/\/ forUpcall == true\n+            addArgumentBinding(0, MemorySegment.class, ValueLayout.ADDRESS, List.of(\n+                Binding.vmLoad(abi.retBufAddrStorage(), long.class),\n+                Binding.boxAddress(),\n+                Binding.toSegment(returnBufferSize)));\n+        }\n+        return new CallingSequence(mt, desc, isTrivial, needsReturnBuffer, returnBufferSize, allocationSize, inputBindings, outputBindings);\n+    }\n+\n+    private long computeAllocationSize() {\n+        \/\/ FIXME: > 16 bytes alignment might need extra space since the\n+        \/\/ starting address of the allocator might be un-aligned.\n+        long size = 0;\n+        for (List<Binding> bindings : inputBindings) {\n+            for (Binding b : bindings) {\n+                if (b instanceof Binding.Copy copy) {\n+                    size = Utils.alignUp(size, copy.alignment());\n+                    size += copy.size();\n+                } else if (b instanceof Binding.Allocate allocate) {\n+                    size = Utils.alignUp(size, allocate.alignment());\n+                    size += allocate.size();\n+                }\n+            }\n+        }\n+        return size;\n+    }\n+\n+    private long computeReturnBuferSize() {\n+        return outputBindings.stream()\n+                .filter(Binding.Move.class::isInstance)\n+                .map(Binding.Move.class::cast)\n+                .map(Binding.Move::storage)\n+                .map(VMStorage::type)\n+                .mapToLong(abi.arch::typeSize)\n+                .sum();\n+    }\n+\n+    private void verifyBindings(boolean forArguments, Class<?> carrier, List<Binding> bindings) {\n+        if (VERIFY_BINDINGS) {\n+            if (forUpcall == forArguments) {\n+                verifyBoxBindings(carrier, bindings);\n+            } else {\n+                verifyUnboxBindings(carrier, bindings);\n+            }\n+        }\n+    }\n+\n+    private static final Set<Binding.Tag> UNBOX_TAGS = EnumSet.of(\n+        VM_STORE,\n+        \/\/VM_LOAD,\n+        \/\/BUFFER_STORE,\n+        BUFFER_LOAD,\n+        COPY_BUFFER,\n+        \/\/ALLOC_BUFFER,\n+        \/\/BOX_ADDRESS,\n+        UNBOX_ADDRESS,\n+        \/\/TO_SEGMENT,\n+        DUP\n+    );\n+\n+    private static void verifyUnboxBindings(Class<?> inType, List<Binding> bindings) {\n+        Deque<Class<?>> stack = new ArrayDeque<>();\n+        stack.push(inType);\n+\n+        for (Binding b : bindings) {\n+            if (!UNBOX_TAGS.contains(b.tag()))\n+                throw new IllegalArgumentException(\"Unexpected operator: \" + b);\n+            b.verify(stack);\n+        }\n+\n+        if (!stack.isEmpty()) {\n+            throw new IllegalArgumentException(\"Stack must be empty after recipe\");\n+        }\n+    }\n+\n+    private static final Set<Binding.Tag> BOX_TAGS = EnumSet.of(\n+        \/\/VM_STORE,\n+        VM_LOAD,\n+        BUFFER_STORE,\n+        \/\/BUFFER_LOAD,\n+        COPY_BUFFER,\n+        ALLOC_BUFFER,\n+        BOX_ADDRESS,\n+        \/\/UNBOX_ADDRESS,\n+        TO_SEGMENT,\n+        DUP\n+    );\n+\n+    private static void verifyBoxBindings(Class<?> expectedOutType, List<Binding> bindings) {\n+        Deque<Class<?>> stack = new ArrayDeque<>();\n+\n+        for (Binding b : bindings) {\n+            if (!BOX_TAGS.contains(b.tag()))\n+                throw new IllegalArgumentException(\"Unexpected operator: \" + b);\n+            b.verify(stack);\n+        }\n+\n+        if (stack.size() != 1) {\n+            throw new IllegalArgumentException(\"Stack must contain exactly 1 value\");\n+        }\n+\n+        Class<?> actualOutType = stack.pop();\n+        SharedUtils.checkType(actualOutType, expectedOutType);\n+    }\n+}\n","filename":"src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/CallingSequenceBuilder.java","additions":214,"deletions":0,"binary":false,"changes":214,"status":"added"},{"patch":"@@ -0,0 +1,323 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi;\n+\n+import jdk.incubator.foreign.MemoryHandles;\n+import jdk.incubator.foreign.MemoryLayout;\n+import jdk.incubator.foreign.MemorySegment;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.SegmentAllocator;\n+import jdk.incubator.foreign.ValueLayout;\n+import jdk.internal.access.JavaLangInvokeAccess;\n+import jdk.internal.access.SharedSecrets;\n+import jdk.internal.invoke.NativeEntryPoint;\n+import jdk.internal.invoke.VMStorageProxy;\n+import sun.security.action.GetPropertyAction;\n+\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.invoke.MethodType;\n+import java.lang.invoke.VarHandle;\n+import java.nio.ByteOrder;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Stream;\n+\n+import static java.lang.invoke.MethodHandles.collectArguments;\n+import static java.lang.invoke.MethodHandles.dropArguments;\n+import static java.lang.invoke.MethodHandles.foldArguments;\n+import static java.lang.invoke.MethodHandles.identity;\n+import static java.lang.invoke.MethodHandles.insertArguments;\n+import static java.lang.invoke.MethodType.methodType;\n+\n+\/**\n+ * This class implements native call invocation through a so called 'universal adapter'. A universal adapter takes\n+ * an array of longs together with a call 'recipe', which is used to move the arguments in the right places as\n+ * expected by the system ABI.\n+ *\/\n+public class ProgrammableInvoker {\n+    private static final boolean USE_SPEC = Boolean.parseBoolean(\n+        GetPropertyAction.privilegedGetProperty(\"jdk.internal.foreign.ProgrammableInvoker.USE_SPEC\", \"true\"));\n+\n+    private static final JavaLangInvokeAccess JLIA = SharedSecrets.getJavaLangInvokeAccess();\n+\n+    private static final MethodHandle MH_INVOKE_INTERP_BINDINGS;\n+    private static final MethodHandle MH_WRAP_ALLOCATOR;\n+    private static final MethodHandle MH_ALLOCATE_RETURN_BUFFER;\n+    private static final MethodHandle MH_CHECK_SYMBOL;\n+\n+    private static final MethodHandle EMPTY_OBJECT_ARRAY_HANDLE = MethodHandles.constant(Object[].class, new Object[0]);\n+\n+    static {\n+        try {\n+            MethodHandles.Lookup lookup = MethodHandles.lookup();\n+            MH_INVOKE_INTERP_BINDINGS = lookup.findVirtual(ProgrammableInvoker.class, \"invokeInterpBindings\",\n+                    methodType(Object.class, SegmentAllocator.class, Object[].class, InvocationData.class));\n+            MH_WRAP_ALLOCATOR = lookup.findStatic(Binding.Context.class, \"ofAllocator\",\n+                    methodType(Binding.Context.class, SegmentAllocator.class));\n+            MH_ALLOCATE_RETURN_BUFFER = lookup.findStatic(ProgrammableInvoker.class, \"allocateReturnBuffer\",\n+                    methodType(MemorySegment.class, Binding.Context.class, long.class));\n+            MH_CHECK_SYMBOL = lookup.findStatic(SharedUtils.class, \"checkSymbol\",\n+                    methodType(void.class, NativeSymbol.class));\n+        } catch (ReflectiveOperationException e) {\n+            throw new RuntimeException(e);\n+        }\n+    }\n+\n+    private final ABIDescriptor abi;\n+    private final CallingSequence callingSequence;\n+\n+    public ProgrammableInvoker(ABIDescriptor abi, CallingSequence callingSequence) {\n+        this.abi = abi;\n+        this.callingSequence = callingSequence;\n+    }\n+\n+    public MethodHandle getBoundMethodHandle() {\n+        Binding.VMStore[] argMoves = argMoveBindingsStream(callingSequence).toArray(Binding.VMStore[]::new);\n+        Class<?>[] argMoveTypes = Arrays.stream(argMoves).map(Binding.VMStore::type).toArray(Class<?>[]::new);\n+\n+        Binding.VMLoad[] retMoves = retMoveBindings(callingSequence);\n+        Class<?> returnType = retMoves.length == 1 ? retMoves[0].type() : void.class;\n+\n+        MethodType leafType = methodType(returnType, argMoveTypes);\n+\n+        NativeEntryPoint nep = NativeEntryPoint.make(\n+            \"native_invoker_\" + leafType.descriptorString(),\n+            abi,\n+            toStorageArray(argMoves),\n+            toStorageArray(retMoves),\n+            !callingSequence.isTrivial(),\n+            leafType,\n+            callingSequence.needsReturnBuffer()\n+        );\n+        MethodHandle handle = JLIA.nativeMethodHandle(nep);\n+\n+        if (USE_SPEC) {\n+            handle = specialize(handle);\n+         } else {\n+            Map<VMStorage, Integer> argIndexMap = SharedUtils.indexMap(argMoves);\n+            Map<VMStorage, Integer> retIndexMap = SharedUtils.indexMap(retMoves);\n+\n+            InvocationData invData = new InvocationData(handle, argIndexMap, retIndexMap);\n+            handle = insertArguments(MH_INVOKE_INTERP_BINDINGS.bindTo(this), 2, invData);\n+            MethodType interpType = callingSequence.methodType();\n+            if (callingSequence.needsReturnBuffer()) {\n+                \/\/ Return buffer is supplied by invokeInterpBindings\n+                assert interpType.parameterType(0) == MemorySegment.class;\n+                interpType.dropParameterTypes(0, 1);\n+            }\n+            MethodHandle collectorInterp = makeCollectorHandle(interpType);\n+            handle = collectArguments(handle, 1, collectorInterp);\n+            handle = handle.asType(handle.type().changeReturnType(interpType.returnType()));\n+         }\n+\n+        assert handle.type().parameterType(0) == SegmentAllocator.class;\n+        assert handle.type().parameterType(1) == NativeSymbol.class;\n+        handle = foldArguments(handle, 1, MH_CHECK_SYMBOL);\n+\n+        handle = SharedUtils.swapArguments(handle, 0, 1); \/\/ normalize parameter order\n+\n+        return handle;\n+    }\n+\n+    private static MemorySegment allocateReturnBuffer(Binding.Context context, long size) {\n+        return context.allocator().allocate(size);\n+    }\n+\n+    \/\/ Funnel from type to Object[]\n+    private static MethodHandle makeCollectorHandle(MethodType type) {\n+        return type.parameterCount() == 0\n+            ? EMPTY_OBJECT_ARRAY_HANDLE\n+            : identity(Object[].class)\n+                .asCollector(Object[].class, type.parameterCount())\n+                .asType(type.changeReturnType(Object[].class));\n+    }\n+\n+    private Stream<Binding.VMStore> argMoveBindingsStream(CallingSequence callingSequence) {\n+        return callingSequence.argumentBindings()\n+                .filter(Binding.VMStore.class::isInstance)\n+                .map(Binding.VMStore.class::cast);\n+    }\n+\n+    private Binding.VMLoad[] retMoveBindings(CallingSequence callingSequence) {\n+        return retMoveBindingsStream(callingSequence).toArray(Binding.VMLoad[]::new);\n+    }\n+\n+    private Stream<Binding.VMLoad> retMoveBindingsStream(CallingSequence callingSequence) {\n+        return callingSequence.returnBindings().stream()\n+                .filter(Binding.VMLoad.class::isInstance)\n+                .map(Binding.VMLoad.class::cast);\n+    }\n+\n+    private VMStorageProxy[] toStorageArray(Binding.Move[] moves) {\n+        return Arrays.stream(moves).map(Binding.Move::storage).toArray(VMStorage[]::new);\n+    }\n+\n+    private MethodHandle specialize(MethodHandle leafHandle) {\n+        MethodType highLevelType = callingSequence.methodType();\n+\n+        int argInsertPos = 0;\n+        int argContextPos = 0;\n+\n+        MethodHandle specializedHandle = dropArguments(leafHandle, argContextPos, Binding.Context.class);\n+        for (int i = 0; i < highLevelType.parameterCount(); i++) {\n+            List<Binding> bindings = callingSequence.argumentBindings(i);\n+            argInsertPos += bindings.stream().filter(Binding.VMStore.class::isInstance).count() + 1;\n+            \/\/ We interpret the bindings in reverse since we have to construct a MethodHandle from the bottom up\n+            for (int j = bindings.size() - 1; j >= 0; j--) {\n+                Binding binding = bindings.get(j);\n+                if (binding.tag() == Binding.Tag.VM_STORE) {\n+                    argInsertPos--;\n+                } else {\n+                    specializedHandle = binding.specialize(specializedHandle, argInsertPos, argContextPos);\n+                }\n+            }\n+        }\n+\n+        if (highLevelType.returnType() != void.class) {\n+            MethodHandle returnFilter = identity(highLevelType.returnType());\n+            int retBufPos = -1;\n+            long retBufReadOffset = -1;\n+            int retContextPos = 0;\n+            int retInsertPos = 1;\n+            if (callingSequence.needsReturnBuffer()) {\n+                retBufPos = 0;\n+                retBufReadOffset = callingSequence.returnBufferSize();\n+                retContextPos++;\n+                retInsertPos++;\n+                returnFilter = dropArguments(returnFilter, retBufPos, MemorySegment.class);\n+            }\n+            returnFilter = dropArguments(returnFilter, retContextPos, Binding.Context.class);\n+            List<Binding> bindings = callingSequence.returnBindings();\n+            for (int j = bindings.size() - 1; j >= 0; j--) {\n+                Binding binding = bindings.get(j);\n+                if (callingSequence.needsReturnBuffer() && binding.tag() == Binding.Tag.VM_LOAD) {\n+                    \/\/ spacial case this, since we need to update retBufReadOffset as well\n+                    Binding.VMLoad load = (Binding.VMLoad) binding;\n+                    ValueLayout layout = MemoryLayout.valueLayout(load.type(), ByteOrder.nativeOrder()).withBitAlignment(8);\n+                    \/\/ since we iterate the bindings in reverse, we have to compute the offset in reverse as well\n+                    retBufReadOffset -= abi.arch.typeSize(load.storage().type());\n+                    MethodHandle loadHandle = MemoryHandles.insertCoordinates(MemoryHandles.varHandle(layout), 1, retBufReadOffset)\n+                            .toMethodHandle(VarHandle.AccessMode.GET);\n+\n+                    returnFilter = MethodHandles.collectArguments(returnFilter, retInsertPos, loadHandle);\n+                    assert returnFilter.type().parameterType(retInsertPos - 1) == MemorySegment.class;\n+                    assert returnFilter.type().parameterType(retInsertPos - 2) == MemorySegment.class;\n+                    returnFilter = SharedUtils.mergeArguments(returnFilter, retBufPos, retInsertPos);\n+                    \/\/ to (... MemorySegment, MemorySegment, <primitive>, ...)\n+                    \/\/ from (... MemorySegment, MemorySegment, ...)\n+                    retInsertPos -= 2; \/\/ set insert pos back to the first MS (later DUP binding will merge the 2 MS)\n+                } else {\n+                    returnFilter = binding.specialize(returnFilter, retInsertPos, retContextPos);\n+                    if (callingSequence.needsReturnBuffer() && binding.tag() == Binding.Tag.BUFFER_STORE) {\n+                        \/\/ from (... MemorySegment, ...)\n+                        \/\/ to (... MemorySegment, MemorySegment, <primitive>, ...)\n+                        retInsertPos += 2; \/\/ set insert pos to <primitive>\n+                        assert returnFilter.type().parameterType(retInsertPos - 1) == MemorySegment.class;\n+                        assert returnFilter.type().parameterType(retInsertPos - 2) == MemorySegment.class;\n+                    }\n+                }\n+            }\n+            \/\/ (R, Context (ret)) -> (MemorySegment?, Context (ret), MemorySegment?, Context (arg), ...)\n+            specializedHandle = MethodHandles.collectArguments(returnFilter, retInsertPos, specializedHandle);\n+            if (callingSequence.needsReturnBuffer()) {\n+                \/\/ (MemorySegment, Context (ret), Context (arg), MemorySegment,  ...) -> (MemorySegment, Context (ret), Context (arg), ...)\n+                specializedHandle = SharedUtils.mergeArguments(specializedHandle, retBufPos, retBufPos + 3);\n+\n+                \/\/ allocate the return buffer from the binding context, and then merge the 2 allocator args\n+                MethodHandle retBufAllocHandle = MethodHandles.insertArguments(MH_ALLOCATE_RETURN_BUFFER, 1, callingSequence.returnBufferSize());\n+                \/\/ (MemorySegment, Context (ret), Context (arg), ...) -> (Context (arg), Context (ret), Context (arg), ...)\n+                specializedHandle = MethodHandles.filterArguments(specializedHandle, retBufPos, retBufAllocHandle);\n+                \/\/ (Context (arg), Context (ret), Context (arg), ...) -> (Context (ret), Context (arg), ...)\n+                specializedHandle = SharedUtils.mergeArguments(specializedHandle, argContextPos + 1, retBufPos); \/\/ +1 to skip return context\n+            }\n+            \/\/ (Context (ret), Context (arg), ...) -> (SegmentAllocator, Context (arg), ...)\n+            specializedHandle = MethodHandles.filterArguments(specializedHandle, 0, MH_WRAP_ALLOCATOR);\n+        } else {\n+            specializedHandle = MethodHandles.dropArguments(specializedHandle, 0, SegmentAllocator.class);\n+        }\n+\n+        \/\/ now bind the internal context parameter\n+\n+        argContextPos++; \/\/ skip over the return SegmentAllocator (inserted by the above code)\n+        specializedHandle = SharedUtils.wrapWithAllocator(specializedHandle, argContextPos, callingSequence.allocationSize(), false);\n+        return specializedHandle;\n+    }\n+\n+    private record InvocationData(MethodHandle leaf, Map<VMStorage, Integer> argIndexMap, Map<VMStorage, Integer> retIndexMap) {}\n+\n+    Object invokeInterpBindings(SegmentAllocator allocator, Object[] args, InvocationData invData) throws Throwable {\n+        Binding.Context unboxContext = callingSequence.allocationSize() != 0\n+                ? Binding.Context.ofBoundedAllocator(callingSequence.allocationSize())\n+                : Binding.Context.DUMMY;\n+        try (unboxContext) {\n+            MemorySegment returnBuffer = null;\n+\n+            \/\/ do argument processing, get Object[] as result\n+            Object[] leafArgs = new Object[invData.leaf.type().parameterCount()];\n+            if (callingSequence.needsReturnBuffer()) {\n+                \/\/ we supply the return buffer (argument array does not contain it)\n+                Object[] prefixedArgs = new Object[args.length + 1];\n+                returnBuffer = unboxContext.allocator().allocate(callingSequence.returnBufferSize());\n+                prefixedArgs[0] = returnBuffer;\n+                System.arraycopy(args, 0, prefixedArgs, 1, args.length);\n+                args = prefixedArgs;\n+            }\n+            for (int i = 0; i < args.length; i++) {\n+                Object arg = args[i];\n+                BindingInterpreter.unbox(arg, callingSequence.argumentBindings(i),\n+                        (storage, type, value) -> {\n+                            leafArgs[invData.argIndexMap.get(storage)] = value;\n+                        }, unboxContext);\n+            }\n+\n+            \/\/ call leaf\n+            Object o = invData.leaf.invokeWithArguments(leafArgs);\n+\n+            \/\/ return value processing\n+            if (o == null) {\n+                if (!callingSequence.needsReturnBuffer()) {\n+                    return null;\n+                }\n+                MemorySegment finalReturnBuffer = returnBuffer;\n+                return BindingInterpreter.box(callingSequence.returnBindings(),\n+                        new BindingInterpreter.LoadFunc() {\n+                            int retBufReadOffset = 0;\n+                            @Override\n+                            public Object load(VMStorage storage, Class<?> type) {\n+                                Object result1 = SharedUtils.read(finalReturnBuffer.asSlice(retBufReadOffset), type);\n+                                retBufReadOffset += abi.arch.typeSize(storage.type());\n+                                return result1;\n+                            }\n+                        }, Binding.Context.ofAllocator(allocator));\n+            } else {\n+                return BindingInterpreter.box(callingSequence.returnBindings(), (storage, type) -> o,\n+                        Binding.Context.ofAllocator(allocator));\n+            }\n+        }\n+    }\n+}\n+\n","filename":"src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/ProgrammableInvoker.java","additions":323,"deletions":0,"binary":false,"changes":323,"status":"added"},{"patch":"@@ -0,0 +1,287 @@\n+\/*\n+ * Copyright (c) 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+package jdk.internal.foreign.abi;\n+\n+import jdk.incubator.foreign.MemoryHandles;\n+import jdk.incubator.foreign.MemoryLayout;\n+import jdk.incubator.foreign.MemorySegment;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.ResourceScope;\n+import jdk.incubator.foreign.ValueLayout;\n+import jdk.internal.access.JavaLangInvokeAccess;\n+import jdk.internal.access.SharedSecrets;\n+import sun.security.action.GetPropertyAction;\n+\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.invoke.MethodType;\n+import java.lang.invoke.VarHandle;\n+import java.nio.ByteOrder;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.stream.Stream;\n+\n+import static java.lang.invoke.MethodHandles.collectArguments;\n+import static java.lang.invoke.MethodHandles.dropArguments;\n+import static java.lang.invoke.MethodHandles.empty;\n+import static java.lang.invoke.MethodHandles.exactInvoker;\n+import static java.lang.invoke.MethodHandles.identity;\n+import static java.lang.invoke.MethodHandles.insertArguments;\n+import static java.lang.invoke.MethodHandles.lookup;\n+import static java.lang.invoke.MethodType.methodType;\n+import static jdk.internal.foreign.abi.SharedUtils.mergeArguments;\n+import static sun.security.action.GetBooleanAction.privilegedGetProperty;\n+\n+public class ProgrammableUpcallHandler {\n+    private static final boolean DEBUG =\n+        privilegedGetProperty(\"jdk.internal.foreign.ProgrammableUpcallHandler.DEBUG\");\n+    private static final boolean USE_SPEC = Boolean.parseBoolean(\n+        GetPropertyAction.privilegedGetProperty(\"jdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC\", \"true\"));\n+\n+    private static final MethodHandle MH_invokeInterpBindings;\n+\n+    private static final JavaLangInvokeAccess JLI = SharedSecrets.getJavaLangInvokeAccess();\n+\n+    static {\n+        try {\n+            MethodHandles.Lookup lookup = lookup();\n+            MH_invokeInterpBindings = lookup.findStatic(ProgrammableUpcallHandler.class, \"invokeInterpBindings\",\n+                    methodType(Object.class, Object[].class, InvocationData.class));\n+        } catch (ReflectiveOperationException e) {\n+            throw new InternalError(e);\n+        }\n+    }\n+\n+    public static NativeSymbol make(ABIDescriptor abi, MethodHandle target, CallingSequence callingSequence, ResourceScope scope) {\n+        Binding.VMLoad[] argMoves = argMoveBindings(callingSequence);\n+        Binding.VMStore[] retMoves = retMoveBindings(callingSequence);\n+\n+        Class<?> llReturn = retMoves.length == 1 ? retMoves[0].type() : void.class;\n+        Class<?>[] llParams = Arrays.stream(argMoves).map(Binding.Move::type).toArray(Class<?>[]::new);\n+        MethodType llType = methodType(llReturn, llParams);\n+\n+        MethodHandle doBindings;\n+        if (USE_SPEC) {\n+            doBindings = specializedBindingHandle(target, callingSequence, llReturn, abi);\n+            assert doBindings.type() == llType;\n+        } else {\n+            Map<VMStorage, Integer> argIndices = SharedUtils.indexMap(argMoves);\n+            Map<VMStorage, Integer> retIndices = SharedUtils.indexMap(retMoves);\n+            int spreaderCount = callingSequence.methodType().parameterCount();\n+            if (callingSequence.needsReturnBuffer()) {\n+                spreaderCount--; \/\/ return buffer is dropped from the argument list\n+            }\n+            target = target.asSpreader(Object[].class, spreaderCount);\n+            InvocationData invData = new InvocationData(target, argIndices, retIndices, callingSequence, retMoves, abi);\n+            doBindings = insertArguments(MH_invokeInterpBindings, 1, invData);\n+            doBindings = doBindings.asCollector(Object[].class, llType.parameterCount());\n+            doBindings = doBindings.asType(llType);\n+        }\n+\n+        checkPrimitive(doBindings.type());\n+        JLI.ensureCustomized(doBindings);\n+        VMStorage[] args = Arrays.stream(argMoves).map(Binding.Move::storage).toArray(VMStorage[]::new);\n+        VMStorage[] rets = Arrays.stream(retMoves).map(Binding.Move::storage).toArray(VMStorage[]::new);\n+        CallRegs conv = new CallRegs(args, rets);\n+        long entryPoint = allocateOptimizedUpcallStub(doBindings, abi, conv,\n+                callingSequence.needsReturnBuffer(), callingSequence.returnBufferSize());\n+        return UpcallStubs.makeUpcall(entryPoint, scope);\n+    }\n+\n+    private static void checkPrimitive(MethodType type) {\n+        if (!type.returnType().isPrimitive()\n+                || type.parameterList().stream().anyMatch(p -> !p.isPrimitive()))\n+            throw new IllegalArgumentException(\"MethodHandle type must be primitive: \" + type);\n+    }\n+\n+    private static Stream<Binding.VMLoad> argMoveBindingsStream(CallingSequence callingSequence) {\n+        return callingSequence.argumentBindings()\n+                .filter(Binding.VMLoad.class::isInstance)\n+                .map(Binding.VMLoad.class::cast);\n+    }\n+\n+    private static Binding.VMLoad[] argMoveBindings(CallingSequence callingSequence) {\n+        return argMoveBindingsStream(callingSequence)\n+                .toArray(Binding.VMLoad[]::new);\n+    }\n+\n+    private static Binding.VMStore[] retMoveBindings(CallingSequence callingSequence) {\n+        return callingSequence.returnBindings().stream()\n+                .filter(Binding.VMStore.class::isInstance)\n+                .map(Binding.VMStore.class::cast)\n+                .toArray(Binding.VMStore[]::new);\n+    }\n+\n+    private static MethodHandle specializedBindingHandle(MethodHandle target, CallingSequence callingSequence,\n+                                                         Class<?> llReturn, ABIDescriptor abi) {\n+        MethodType highLevelType = callingSequence.methodType();\n+\n+        MethodHandle specializedHandle = target; \/\/ initial\n+\n+        \/\/ we handle returns first since IMR adds an extra parameter that needs to be specialized as well\n+        if (llReturn != void.class || callingSequence.needsReturnBuffer()) {\n+            int retAllocatorPos = -1; \/\/ assumed not needed\n+            int retInsertPos;\n+            MethodHandle filter;\n+            if (callingSequence.needsReturnBuffer()) {\n+                retInsertPos = 1;\n+                filter = empty(methodType(void.class, MemorySegment.class));\n+            } else {\n+                retInsertPos = 0;\n+                filter = identity(llReturn);\n+            }\n+            long retBufWriteOffset = callingSequence.returnBufferSize();\n+            List<Binding> bindings = callingSequence.returnBindings();\n+            for (int j = bindings.size() - 1; j >= 0; j--) {\n+                Binding binding = bindings.get(j);\n+                if (callingSequence.needsReturnBuffer() && binding.tag() == Binding.Tag.VM_STORE) {\n+                    Binding.VMStore store = (Binding.VMStore) binding;\n+                    ValueLayout layout = MemoryLayout.valueLayout(store.type(), ByteOrder.nativeOrder()).withBitAlignment(8);\n+                    \/\/ since we iterate the bindings in reverse, we have to compute the offset in reverse as well\n+                    retBufWriteOffset -= abi.arch.typeSize(store.storage().type());\n+                    MethodHandle storeHandle = MemoryHandles.insertCoordinates(MemoryHandles.varHandle(layout), 1, retBufWriteOffset)\n+                            .toMethodHandle(VarHandle.AccessMode.SET);\n+                    filter = collectArguments(filter, retInsertPos, storeHandle);\n+                    filter = mergeArguments(filter, retInsertPos - 1, retInsertPos);\n+                } else {\n+                    filter = binding.specialize(filter, retInsertPos, retAllocatorPos);\n+                }\n+            }\n+            specializedHandle = collectArguments(filter, retInsertPos, specializedHandle);\n+        }\n+\n+        int argAllocatorPos = 0;\n+        int argInsertPos = 1;\n+        specializedHandle = dropArguments(specializedHandle, argAllocatorPos, Binding.Context.class);\n+        for (int i = 0; i < highLevelType.parameterCount(); i++) {\n+            MethodHandle filter = identity(highLevelType.parameterType(i));\n+            int filterAllocatorPos = 0;\n+            int filterInsertPos = 1; \/\/ +1 for allocator\n+            filter = dropArguments(filter, filterAllocatorPos, Binding.Context.class);\n+\n+            List<Binding> bindings = callingSequence.argumentBindings(i);\n+            for (int j = bindings.size() - 1; j >= 0; j--) {\n+                Binding binding = bindings.get(j);\n+                filter = binding.specialize(filter, filterInsertPos, filterAllocatorPos);\n+            }\n+            specializedHandle = MethodHandles.collectArguments(specializedHandle, argInsertPos, filter);\n+            specializedHandle = mergeArguments(specializedHandle, argAllocatorPos, argInsertPos + filterAllocatorPos);\n+            argInsertPos += filter.type().parameterCount() - 1; \/\/ -1 for allocator\n+        }\n+\n+        specializedHandle = SharedUtils.wrapWithAllocator(specializedHandle, argAllocatorPos, callingSequence.allocationSize(), true);\n+\n+        return specializedHandle;\n+    }\n+\n+    private record InvocationData(MethodHandle leaf,\n+                                  Map<VMStorage, Integer> argIndexMap,\n+                                  Map<VMStorage, Integer> retIndexMap,\n+                                  CallingSequence callingSequence,\n+                                  Binding.VMStore[] retMoves,\n+                                  ABIDescriptor abi) {}\n+\n+    private static Object invokeInterpBindings(Object[] lowLevelArgs, InvocationData invData) throws Throwable {\n+        Binding.Context allocator = invData.callingSequence.allocationSize() != 0\n+                ? Binding.Context.ofBoundedAllocator(invData.callingSequence.allocationSize())\n+                : Binding.Context.ofScope();\n+        try (allocator) {\n+            \/\/\/ Invoke interpreter, got array of high-level arguments back\n+            Object[] highLevelArgs = new Object[invData.callingSequence.methodType().parameterCount()];\n+            for (int i = 0; i < highLevelArgs.length; i++) {\n+                highLevelArgs[i] = BindingInterpreter.box(invData.callingSequence.argumentBindings(i),\n+                        (storage, type) -> lowLevelArgs[invData.argIndexMap.get(storage)], allocator);\n+            }\n+\n+            MemorySegment returnBuffer = null;\n+            if (invData.callingSequence.needsReturnBuffer()) {\n+                \/\/ this one is for us\n+                returnBuffer = (MemorySegment) highLevelArgs[0];\n+                Object[] newArgs = new Object[highLevelArgs.length - 1];\n+                System.arraycopy(highLevelArgs, 1, newArgs, 0, newArgs.length);\n+                highLevelArgs = newArgs;\n+            }\n+\n+            if (DEBUG) {\n+                System.err.println(\"Java arguments:\");\n+                System.err.println(Arrays.toString(highLevelArgs).indent(2));\n+            }\n+\n+            \/\/ invoke our target\n+            Object o = invData.leaf.invoke(highLevelArgs);\n+\n+            if (DEBUG) {\n+                System.err.println(\"Java return:\");\n+                System.err.println(Objects.toString(o).indent(2));\n+            }\n+\n+            Object[] returnValues = new Object[invData.retIndexMap.size()];\n+            if (invData.leaf.type().returnType() != void.class) {\n+                BindingInterpreter.unbox(o, invData.callingSequence.returnBindings(),\n+                        (storage, type, value) -> returnValues[invData.retIndexMap.get(storage)] = value, null);\n+            }\n+\n+            if (returnValues.length == 0) {\n+                return null;\n+            } else if (returnValues.length == 1) {\n+                return returnValues[0];\n+            } else {\n+                assert invData.callingSequence.needsReturnBuffer();\n+\n+                Binding.VMStore[] retMoves = invData.callingSequence.returnBindings().stream()\n+                        .filter(Binding.VMStore.class::isInstance)\n+                        .map(Binding.VMStore.class::cast)\n+                        .toArray(Binding.VMStore[]::new);\n+\n+                assert returnValues.length == retMoves.length;\n+                int retBufWriteOffset = 0;\n+                for (int i = 0; i < retMoves.length; i++) {\n+                    Binding.VMStore store = retMoves[i];\n+                    Object value = returnValues[i];\n+                    SharedUtils.writeOverSized(returnBuffer.asSlice(retBufWriteOffset), store.type(), value);\n+                    retBufWriteOffset += invData.abi.arch.typeSize(store.storage().type());\n+                }\n+                return null;\n+            }\n+        } catch(Throwable t) {\n+            SharedUtils.handleUncaughtException(t);\n+            return null;\n+        }\n+    }\n+\n+    \/\/ used for transporting data into native code\n+    private static record CallRegs(VMStorage[] argRegs, VMStorage[] retRegs) {}\n+\n+    static native long allocateOptimizedUpcallStub(MethodHandle mh, ABIDescriptor abi, CallRegs conv,\n+                                                   boolean needsReturnBuffer, long returnBufferSize);\n+\n+    private static native void registerNatives();\n+    static {\n+        registerNatives();\n+    }\n+}\n","filename":"src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/ProgrammableUpcallHandler.java","additions":287,"deletions":0,"binary":false,"changes":287,"status":"added"},{"patch":"@@ -0,0 +1,97 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi.x64.sysv;\n+\n+\n+import jdk.incubator.foreign.CLinker;\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.MemoryAddress;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.ResourceScope;\n+import jdk.incubator.foreign.VaList;\n+import jdk.internal.foreign.abi.SharedUtils;\n+\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodType;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+\/**\n+ * ABI implementation based on System V ABI AMD64 supplement v.0.99.6\n+ *\/\n+public final class SysVx64Linker implements CLinker {\n+    public static final int MAX_INTEGER_ARGUMENT_REGISTERS = 6;\n+    public static final int MAX_INTEGER_RETURN_REGISTERS = 2;\n+    public static final int MAX_VECTOR_ARGUMENT_REGISTERS = 8;\n+    public static final int MAX_VECTOR_RETURN_REGISTERS = 2;\n+    public static final int MAX_X87_RETURN_REGISTERS = 2;\n+\n+    private static SysVx64Linker instance;\n+\n+    static final long ADDRESS_SIZE = 64; \/\/ bits\n+\n+    public static SysVx64Linker getInstance() {\n+        if (instance == null) {\n+            instance = new SysVx64Linker();\n+        }\n+        return instance;\n+    }\n+\n+    public static VaList newVaList(Consumer<VaList.Builder> actions, ResourceScope scope) {\n+        SysVVaList.Builder builder = SysVVaList.builder(scope);\n+        actions.accept(builder);\n+        return builder.build();\n+    }\n+\n+    @Override\n+    public final MethodHandle downcallHandle(FunctionDescriptor function) {\n+        Objects.requireNonNull(function);\n+        MethodType type = SharedUtils.inferMethodType(function, false);\n+        MethodHandle handle = CallArranger.arrangeDowncall(type, function);\n+        handle = SharedUtils.maybeInsertAllocator(handle);\n+        return SharedUtils.wrapDowncall(handle, function);\n+    }\n+\n+    @Override\n+    public final NativeSymbol upcallStub(MethodHandle target, FunctionDescriptor function, ResourceScope scope) {\n+        Objects.requireNonNull(scope);\n+        Objects.requireNonNull(target);\n+        Objects.requireNonNull(function);\n+        SharedUtils.checkExceptions(target);\n+        MethodType type = SharedUtils.inferMethodType(function, true);\n+        if (!type.equals(target.type())) {\n+            throw new IllegalArgumentException(\"Wrong method handle type: \" + target.type());\n+        }\n+        return CallArranger.arrangeUpcall(target, target.type(), function, scope);\n+    }\n+\n+    public static VaList newVaListOfAddress(MemoryAddress ma, ResourceScope scope) {\n+        return SysVVaList.ofAddress(ma, scope);\n+    }\n+\n+    public static VaList emptyVaList() {\n+        return SysVVaList.empty();\n+    }\n+}\n","filename":"src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/sysv\/SysVx64Linker.java","additions":97,"deletions":0,"binary":false,"changes":97,"status":"added"},{"patch":"@@ -0,0 +1,100 @@\n+\/*\n+ * Copyright (c) 2020, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.  Oracle designates this\n+ * particular file as subject to the \"Classpath\" exception as provided\n+ * by Oracle in the LICENSE file that accompanied this code.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.internal.foreign.abi.x64.windows;\n+\n+import jdk.incubator.foreign.CLinker;\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.MemoryAddress;\n+import jdk.incubator.foreign.MemorySegment;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.ResourceScope;\n+import jdk.incubator.foreign.VaList;\n+import jdk.internal.foreign.abi.SharedUtils;\n+\n+import java.lang.invoke.MethodHandle;\n+import java.lang.invoke.MethodHandles;\n+import java.lang.invoke.MethodType;\n+import java.util.Objects;\n+import java.util.function.Consumer;\n+\n+\/**\n+ * ABI implementation based on Windows ABI AMD64 supplement v.0.99.6\n+ *\/\n+public final class Windowsx64Linker implements CLinker {\n+\n+    public static final int MAX_INTEGER_ARGUMENT_REGISTERS = 4;\n+    public static final int MAX_INTEGER_RETURN_REGISTERS = 1;\n+    public static final int MAX_VECTOR_ARGUMENT_REGISTERS = 4;\n+    public static final int MAX_VECTOR_RETURN_REGISTERS = 1;\n+    public static final int MAX_REGISTER_ARGUMENTS = 4;\n+    public static final int MAX_REGISTER_RETURNS = 1;\n+\n+    private static Windowsx64Linker instance;\n+\n+    static final long ADDRESS_SIZE = 64; \/\/ bits\n+\n+    public static Windowsx64Linker getInstance() {\n+        if (instance == null) {\n+            instance = new Windowsx64Linker();\n+        }\n+        return instance;\n+    }\n+\n+    public static VaList newVaList(Consumer<VaList.Builder> actions, ResourceScope scope) {\n+        WinVaList.Builder builder = WinVaList.builder(scope);\n+        actions.accept(builder);\n+        return builder.build();\n+    }\n+\n+    @Override\n+    public final MethodHandle downcallHandle(FunctionDescriptor function) {\n+        Objects.requireNonNull(function);\n+        MethodType type = SharedUtils.inferMethodType(function, false);\n+        MethodHandle handle = CallArranger.arrangeDowncall(type, function);\n+        handle = SharedUtils.maybeInsertAllocator(handle);\n+        return SharedUtils.wrapDowncall(handle, function);\n+    }\n+\n+    @Override\n+    public final NativeSymbol upcallStub(MethodHandle target, FunctionDescriptor function, ResourceScope scope) {\n+        Objects.requireNonNull(scope);\n+        Objects.requireNonNull(target);\n+        Objects.requireNonNull(function);\n+        SharedUtils.checkExceptions(target);\n+        MethodType type = SharedUtils.inferMethodType(function, true);\n+        if (!type.equals(target.type())) {\n+            throw new IllegalArgumentException(\"Wrong method handle type: \" + target.type());\n+        }\n+        return CallArranger.arrangeUpcall(target, target.type(), function, scope);\n+    }\n+\n+    public static VaList newVaListOfAddress(MemoryAddress ma, ResourceScope scope) {\n+        return WinVaList.ofAddress(ma, scope);\n+    }\n+\n+    public static VaList emptyVaList() {\n+        return WinVaList.empty();\n+    }\n+}\n","filename":"src\/jdk.incubator.foreign\/share\/classes\/jdk\/internal\/foreign\/abi\/x64\/windows\/Windowsx64Linker.java","additions":100,"deletions":0,"binary":false,"changes":100,"status":"added"},{"patch":"@@ -24,0 +24,1 @@\n+<<<<<<< HEAD:test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\/NestHostOldInfApp.java\n@@ -28,0 +29,15 @@\n+=======\n+\n+#include \"precompiled.hpp\"\n+#include \"prims\/universalNativeInvoker.hpp\"\n+\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+  Unimplemented();\n+  return nullptr;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/zero\/universalNativeInvoker_zero.cpp\n","filename":"test\/hotspot\/jtreg\/runtime\/cds\/appcds\/test-classes\/NestHostOldInfApp.java","additions":16,"deletions":0,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -488,0 +488,4 @@\n+<<<<<<< HEAD\n+=======\n+java\/foreign\/TestMismatch.java 8249684 macosx-all\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/ProblemList.txt","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -40,0 +40,1 @@\n+<<<<<<< HEAD\n@@ -45,0 +46,9 @@\n+=======\n+import jdk.incubator.foreign.CLinker;\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.GroupLayout;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.ResourceScope;\n+import jdk.incubator.foreign.SymbolLookup;\n+import jdk.incubator.foreign.MemoryLayout;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -69,0 +79,1 @@\n+<<<<<<< HEAD\n@@ -73,0 +84,28 @@\n+=======\n+        NativeSymbol addr = LOOKUP.lookup(fName).get();\n+        FunctionDescriptor descriptor = function(ret, paramTypes, fields);\n+        Object[] args = makeArgs(paramTypes, fields, checks);\n+        try (ResourceScope scope = ResourceScope.newSharedScope()) {\n+            boolean needsScope = descriptor.returnLayout().map(GroupLayout.class::isInstance).orElse(false);\n+            SegmentAllocator allocator = needsScope ?\n+                    SegmentAllocator.newNativeArena(scope) :\n+                    THROWING_ALLOCATOR;\n+            Object res = doCall(addr, allocator, descriptor, args);\n+            if (ret == Ret.NON_VOID) {\n+                checks.forEach(c -> c.accept(res));\n+                if (needsScope) {\n+                    \/\/ check that return struct has indeed been allocated in the native scope\n+                    assertEquals(((MemorySegment) res).scope(), scope);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Test(dataProvider=\"functions\", dataProviderClass=CallGeneratorHelper.class)\n+    public void testDowncallStack(int count, String fName, Ret ret, List<ParamType> paramTypes, List<StructFieldType> fields) throws Throwable {\n+        List<Consumer<Object>> checks = new ArrayList<>();\n+        NativeSymbol addr = LOOKUP.lookup(\"s\" + fName).get();\n+        FunctionDescriptor descriptor = functionStack(ret, paramTypes, fields);\n+        Object[] args = makeArgsStack(paramTypes, fields, checks);\n+        try (ResourceScope scope = ResourceScope.newSharedScope()) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/java\/foreign\/TestDowncall.java","additions":39,"deletions":0,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+<<<<<<< HEAD\n@@ -84,0 +85,33 @@\n+=======\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * Note: to run this test manually, you need to build the tests first to get native\n+ * libraries compiled, and then execute it with plain jtreg, like:\n+ *\n+ *  $ bin\/jtreg -jdk:<path-to-tested-jdk> \\\n+ *              -nativepath:<path-to-build-dir>\/support\/test\/jdk\/jtreg\/native\/manual\/lib\/ \\\n+ *              -concurrency:auto \\\n+ *              .\/test\/jdk\/java\/foreign\/TestMatrix.java\n+ *\/\n+\n+\/* @test id=UpcallHighArity-FF\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -90,0 +124,1 @@\n+<<<<<<< HEAD\n@@ -95,0 +130,7 @@\n+=======\n+ *   -Djdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC=false\n+ *   TestUpcallHighArity\n+ *\/\n+\n+\/* @test id=UpcallHighArity-TF\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -101,0 +143,1 @@\n+<<<<<<< HEAD\n@@ -106,0 +149,7 @@\n+=======\n+ *   -Djdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC=false\n+ *   TestUpcallHighArity\n+ *\/\n+\n+\/* @test id=UpcallHighArity-FT\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -112,0 +162,1 @@\n+<<<<<<< HEAD\n@@ -118,0 +169,7 @@\n+=======\n+ *   -Djdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC=true\n+ *   TestUpcallHighArity\n+ *\/\n+\n+\/* @test id=UpcallHighArity-TT\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -124,0 +182,1 @@\n+<<<<<<< HEAD\n@@ -132,0 +191,10 @@\n+=======\n+ *   -Djdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC=true\n+ *   TestUpcallHighArity\n+ *\/\n+\n+\/* @test id=Downcall-F\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+ * @build NativeTestHelper CallGeneratorHelper TestDowncall\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -136,0 +205,1 @@\n+<<<<<<< HEAD\n@@ -144,0 +214,9 @@\n+=======\n+ *   TestDowncall\n+ *\/\n+\n+\/* @test id=Downcall-T\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+ * @build NativeTestHelper CallGeneratorHelper TestDowncall\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -148,0 +227,1 @@\n+<<<<<<< HEAD\n@@ -155,0 +235,8 @@\n+=======\n+ *   TestDowncall\n+ *\/\n+\n+\/* @test id=UpcallScope-FF\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -161,0 +249,1 @@\n+<<<<<<< HEAD\n@@ -167,0 +256,8 @@\n+=======\n+ *   TestUpcallScope\n+ *\/\n+\n+\/* @test id=UpcallScope-TF\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -173,0 +270,1 @@\n+<<<<<<< HEAD\n@@ -179,0 +277,8 @@\n+=======\n+ *   TestUpcallScope\n+ *\/\n+\n+\/* @test id=UpcallScope-FT\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -185,0 +291,1 @@\n+<<<<<<< HEAD\n@@ -191,0 +298,8 @@\n+=======\n+ *   TestUpcallScope\n+ *\/\n+\n+\/* @test id=UpcallScope-TT\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -197,0 +312,1 @@\n+<<<<<<< HEAD\n@@ -203,0 +319,8 @@\n+=======\n+ *   TestUpcallScope\n+ *\/\n+\n+\/* @test id=UpcallAsync-FF\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -205,1 +329,1 @@\n- * @run testng\/othervm\/native\/manual\n+ * @run testng\/othervm\/native\/manual\/timeout=960\n@@ -209,0 +333,1 @@\n+<<<<<<< HEAD\n@@ -215,0 +340,58 @@\n+=======\n+ *   TestUpcallAsync\n+ *\/\n+\n+\/* @test id=UpcallAsync-TF\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n+ * @build NativeTestHelper CallGeneratorHelper TestUpcallBase\n+ *\n+ * @run testng\/othervm\/native\/manual\/timeout=960\n+ *   --enable-native-access=ALL-UNNAMED\n+ *   -Djdk.internal.foreign.ProgrammableInvoker.USE_SPEC=true\n+ *   -Djdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC=false\n+<<<<<<< HEAD\n+=======\n+ *   TestUpcallAsync\n+ *\/\n+\n+\/* @test id=UpcallAsync-FT\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+ * @build NativeTestHelper CallGeneratorHelper TestUpcallBase\n+ *\n+ * @run testng\/othervm\/native\/manual\/timeout=960\n+ *   --enable-native-access=ALL-UNNAMED\n+ *   -Djdk.internal.foreign.ProgrammableInvoker.USE_SPEC=false\n+ *   -Djdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC=true\n+ *   TestUpcallAsync\n+ *\/\n+\n+\/* @test id=UpcallAsync-TT\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+ * @build NativeTestHelper CallGeneratorHelper TestUpcallBase\n+ *\n+ * @run testng\/othervm\/native\/manual\/timeout=960\n+ *   --enable-native-access=ALL-UNNAMED\n+ *   -Djdk.internal.foreign.ProgrammableInvoker.USE_SPEC=true\n+ *   -Djdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC=true\n+ *   TestUpcallAsync\n+ *\/\n+\n+\/* @test id=UpcallStack-FF\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+ * @build NativeTestHelper CallGeneratorHelper TestUpcallBase\n+ *\n+ * @run testng\/othervm\/native\/manual\n+ *   --enable-native-access=ALL-UNNAMED\n+ *   -Djdk.internal.foreign.ProgrammableInvoker.USE_SPEC=false\n+ *   -Djdk.internal.foreign.ProgrammableUpcallHandler.USE_SPEC=false\n+ *   TestUpcallStack\n+ *\/\n+\n+\/* @test id=UpcallStack-TF\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n@@ -221,0 +404,1 @@\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -225,0 +409,1 @@\n+<<<<<<< HEAD\n@@ -227,0 +412,4 @@\n+=======\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -237,0 +426,1 @@\n+<<<<<<< HEAD\n@@ -239,0 +429,4 @@\n+=======\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -247,0 +441,1 @@\n+\n","filename":"test\/jdk\/java\/foreign\/TestMatrix.java","additions":196,"deletions":1,"binary":false,"changes":197,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+<<<<<<< HEAD\n@@ -28,0 +29,4 @@\n+=======\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -35,0 +40,1 @@\n+<<<<<<< HEAD\n@@ -42,0 +48,9 @@\n+=======\n+import jdk.incubator.foreign.CLinker;\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.MemoryLayout;\n+import jdk.incubator.foreign.MemorySegment;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.ResourceScope;\n+import jdk.incubator.foreign.SegmentAllocator;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -63,0 +78,1 @@\n+<<<<<<< HEAD\n@@ -69,0 +85,8 @@\n+=======\n+        NativeSymbol addr = LOOKUP.lookup(fName).get();\n+        try (ResourceScope scope = ResourceScope.newSharedScope()) {\n+            SegmentAllocator allocator = SegmentAllocator.newNativeArena(scope);\n+            FunctionDescriptor descriptor = function(ret, paramTypes, fields);\n+            MethodHandle mh = downcallHandle(ABI, addr, allocator, descriptor);\n+            Object[] args = makeArgs(ResourceScope.newImplicitScope(), ret, paramTypes, fields, returnChecks, argChecks);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -75,0 +99,1 @@\n+<<<<<<< HEAD\n@@ -76,0 +101,3 @@\n+=======\n+            NativeSymbol callback = ABI.upcallStub(mh.asType(CLinker.upcallType(callbackDesc)), callbackDesc, scope);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -96,0 +124,1 @@\n+<<<<<<< HEAD\n@@ -97,0 +126,3 @@\n+=======\n+                            LOOKUP.lookup(symbol).orElseThrow(),\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -104,0 +136,1 @@\n+<<<<<<< HEAD\n@@ -105,0 +138,3 @@\n+=======\n+            NativeSymbol invokerSymbol = LOOKUP.lookup(symbol).orElseThrow();\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/java\/foreign\/TestUpcallAsync.java","additions":36,"deletions":0,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -25,0 +25,1 @@\n+<<<<<<< HEAD\n@@ -32,0 +33,10 @@\n+=======\n+import jdk.incubator.foreign.CLinker;\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.SymbolLookup;\n+import jdk.incubator.foreign.MemoryLayout;\n+import jdk.incubator.foreign.MemorySegment;\n+\n+import jdk.incubator.foreign.ResourceScope;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -48,0 +59,1 @@\n+<<<<<<< HEAD\n@@ -49,0 +61,4 @@\n+=======\n+    static CLinker ABI = CLinker.systemCLinker();\n+    static final SymbolLookup LOOKUP = SymbolLookup.loaderLookup();\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -63,0 +79,1 @@\n+<<<<<<< HEAD\n@@ -68,0 +85,7 @@\n+=======\n+    private static NativeSymbol DUMMY_STUB;\n+\n+    @BeforeClass\n+    void setup() {\n+        DUMMY_STUB = ABI.upcallStub(DUMMY, FunctionDescriptor.ofVoid(), ResourceScope.newImplicitScope());\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -83,0 +107,1 @@\n+<<<<<<< HEAD\n@@ -88,0 +113,7 @@\n+=======\n+    static Object[] makeArgs(ResourceScope scope, Ret ret, List<ParamType> params, List<StructFieldType> fields, List<Consumer<Object>> checks, List<Consumer<Object[]>> argChecks) throws ReflectiveOperationException {\n+        return makeArgs(scope, ret, params, fields, checks, argChecks, List.of());\n+    }\n+\n+    static Object[] makeArgs(ResourceScope scope, Ret ret, List<ParamType> params, List<StructFieldType> fields, List<Consumer<Object>> checks, List<Consumer<Object[]>> argChecks, List<MemoryLayout> prefix) throws ReflectiveOperationException {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -96,0 +128,1 @@\n+<<<<<<< HEAD\n@@ -101,0 +134,7 @@\n+=======\n+        args[argNum] = makeCallback(scope, ret, params, fields, checks, argChecks, prefix);\n+        return args;\n+    }\n+\n+    static NativeSymbol makeCallback(ResourceScope scope, Ret ret, List<ParamType> params, List<StructFieldType> fields, List<Consumer<Object>> checks, List<Consumer<Object[]>> argChecks, List<MemoryLayout> prefix) {\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -143,0 +183,1 @@\n+<<<<<<< HEAD\n@@ -144,0 +185,3 @@\n+=======\n+        return ABI.upcallStub(mh, func, scope);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -150,0 +194,1 @@\n+<<<<<<< HEAD\n@@ -151,0 +196,3 @@\n+=======\n+                MemorySegment copy = MemorySegment.allocateNative(ms.byteSize(), ResourceScope.newImplicitScope());\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/java\/foreign\/TestUpcallBase.java","additions":48,"deletions":0,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+<<<<<<< HEAD\n@@ -28,0 +29,4 @@\n+=======\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -35,0 +40,1 @@\n+<<<<<<< HEAD\n@@ -38,0 +44,5 @@\n+=======\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.ResourceScope;\n+import jdk.incubator.foreign.SegmentAllocator;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -55,0 +66,1 @@\n+<<<<<<< HEAD\n@@ -60,0 +72,7 @@\n+=======\n+        NativeSymbol addr = LOOKUP.lookup(fName).get();\n+        try (ResourceScope scope = ResourceScope.newConfinedScope()) {\n+            SegmentAllocator allocator = SegmentAllocator.newNativeArena(scope);\n+            MethodHandle mh = downcallHandle(ABI, addr, allocator, function(ret, paramTypes, fields));\n+            Object[] args = makeArgs(scope, ret, paramTypes, fields, returnChecks, argChecks);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/java\/foreign\/TestUpcallScope.java","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+<<<<<<< HEAD\n@@ -28,0 +29,4 @@\n+=======\n+ * @requires ((os.arch == \"amd64\" | os.arch == \"x86_64\") & sun.arch.data.model == \"64\") | os.arch == \"aarch64\"\n+ * @modules jdk.incubator.foreign\/jdk.internal.foreign\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -35,0 +40,1 @@\n+<<<<<<< HEAD\n@@ -39,0 +45,6 @@\n+=======\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.NativeSymbol;\n+import jdk.incubator.foreign.ResourceScope;\n+import jdk.incubator.foreign.SegmentAllocator;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -56,0 +68,1 @@\n+<<<<<<< HEAD\n@@ -61,0 +74,7 @@\n+=======\n+        NativeSymbol addr = LOOKUP.lookup(\"s\" + fName).get();\n+        try (ResourceScope scope = ResourceScope.newConfinedScope()) {\n+            SegmentAllocator allocator = SegmentAllocator.newNativeArena(scope);\n+            MethodHandle mh = downcallHandle(ABI, addr, allocator, functionStack(ret, paramTypes, fields));\n+            Object[] args = makeArgsStack(scope, ret, paramTypes, fields, returnChecks, argChecks);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -74,0 +94,1 @@\n+<<<<<<< HEAD\n@@ -76,0 +97,4 @@\n+=======\n+    static Object[] makeArgsStack(ResourceScope scope, Ret ret, List<ParamType> params, List<StructFieldType> fields, List<Consumer<Object>> checks, List<Consumer<Object[]>> argChecks) throws ReflectiveOperationException {\n+        return makeArgs(scope, ret, params, fields, checks, argChecks, STACK_PREFIX_LAYOUTS);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/java\/foreign\/TestUpcallStack.java","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+<<<<<<< HEAD\n@@ -40,0 +41,7 @@\n+=======\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.MemoryAddress;\n+import jdk.incubator.foreign.MemoryLayout;\n+import jdk.incubator.foreign.MemorySegment;\n+import jdk.incubator.foreign.NativeSymbol;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -48,0 +56,1 @@\n+<<<<<<< HEAD\n@@ -49,0 +58,3 @@\n+=======\n+import static jdk.incubator.foreign.ValueLayout.ADDRESS;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -66,0 +78,1 @@\n+<<<<<<< HEAD\n@@ -71,0 +84,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) }\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -90,0 +110,1 @@\n+<<<<<<< HEAD\n@@ -95,0 +116,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -120,0 +148,1 @@\n+<<<<<<< HEAD\n@@ -125,0 +154,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -142,0 +178,1 @@\n+<<<<<<< HEAD\n@@ -147,0 +184,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -202,0 +246,1 @@\n+<<<<<<< HEAD\n@@ -207,0 +252,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -233,0 +285,1 @@\n+<<<<<<< HEAD\n@@ -238,0 +291,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, NativeSymbol.class, MemoryAddress.class));\n+        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(ADDRESS, C_POINTER));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -257,0 +317,1 @@\n+<<<<<<< HEAD\n@@ -258,0 +319,3 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, MemorySegment.class, NativeSymbol.class));\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -262,0 +326,1 @@\n+<<<<<<< HEAD\n@@ -263,0 +328,3 @@\n+=======\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) }\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -286,0 +354,1 @@\n+<<<<<<< HEAD\n@@ -287,0 +356,3 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, MemorySegment.class, NativeSymbol.class));\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -291,0 +363,1 @@\n+<<<<<<< HEAD\n@@ -292,0 +365,3 @@\n+=======\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -324,0 +400,1 @@\n+<<<<<<< HEAD\n@@ -329,0 +406,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -378,0 +462,1 @@\n+<<<<<<< HEAD\n@@ -383,0 +468,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -402,0 +494,1 @@\n+<<<<<<< HEAD\n@@ -403,0 +496,3 @@\n+=======\n+        FunctionDescriptor fdExpected = FunctionDescriptor.ofVoid(ADDRESS, C_INT).asVariadic(C_INT, C_FLOAT);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -407,0 +503,1 @@\n+<<<<<<< HEAD\n@@ -408,0 +505,3 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -412,0 +512,1 @@\n+<<<<<<< HEAD\n@@ -413,0 +514,3 @@\n+=======\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -425,0 +529,1 @@\n+<<<<<<< HEAD\n@@ -426,0 +531,3 @@\n+=======\n+        FunctionDescriptor fdExpected = FunctionDescriptor.ofVoid(ADDRESS, C_INT).asVariadic(C_INT, C_FLOAT);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -430,0 +538,1 @@\n+<<<<<<< HEAD\n@@ -431,0 +540,3 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -435,0 +547,1 @@\n+<<<<<<< HEAD\n@@ -436,0 +549,3 @@\n+=======\n+            { unboxAddress(NativeSymbol.class), vmStore(r9, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestAarch64CallArranger.java","additions":116,"deletions":0,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+<<<<<<< HEAD\n@@ -40,0 +41,7 @@\n+=======\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.MemoryAddress;\n+import jdk.incubator.foreign.MemoryLayout;\n+import jdk.incubator.foreign.MemorySegment;\n+import jdk.incubator.foreign.NativeSymbol;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -49,0 +57,1 @@\n+<<<<<<< HEAD\n@@ -50,0 +59,3 @@\n+=======\n+import static jdk.incubator.foreign.ValueLayout.ADDRESS;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -67,0 +79,1 @@\n+<<<<<<< HEAD\n@@ -72,0 +85,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -95,0 +115,1 @@\n+<<<<<<< HEAD\n@@ -100,0 +121,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -126,0 +154,1 @@\n+<<<<<<< HEAD\n@@ -131,0 +160,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -156,0 +192,1 @@\n+<<<<<<< HEAD\n@@ -161,0 +198,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -186,0 +230,1 @@\n+<<<<<<< HEAD\n@@ -191,0 +236,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -211,0 +263,1 @@\n+<<<<<<< HEAD\n@@ -216,0 +269,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -242,0 +302,1 @@\n+<<<<<<< HEAD\n@@ -247,0 +308,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -277,0 +345,1 @@\n+<<<<<<< HEAD\n@@ -282,0 +351,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -334,0 +410,1 @@\n+<<<<<<< HEAD\n@@ -339,0 +416,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -377,0 +461,1 @@\n+<<<<<<< HEAD\n@@ -382,0 +467,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -399,0 +491,1 @@\n+<<<<<<< HEAD\n@@ -404,0 +497,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.appendArgumentLayouts(C_LONG).insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -458,0 +558,1 @@\n+<<<<<<< HEAD\n@@ -459,0 +560,3 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.appendParameterTypes(long.class).insertParameterTypes(0, MemorySegment.class, NativeSymbol.class));\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -463,0 +567,1 @@\n+<<<<<<< HEAD\n@@ -464,0 +569,3 @@\n+=======\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -490,0 +598,1 @@\n+<<<<<<< HEAD\n@@ -495,0 +604,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, NativeSymbol.class, MemoryAddress.class, long.class));\n+        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(ADDRESS, C_POINTER, C_LONG));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestSysVCallArranger.java","additions":116,"deletions":0,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -36,0 +36,1 @@\n+<<<<<<< HEAD\n@@ -41,0 +42,7 @@\n+=======\n+import jdk.incubator.foreign.FunctionDescriptor;\n+import jdk.incubator.foreign.MemoryAddress;\n+import jdk.incubator.foreign.MemoryLayout;\n+import jdk.incubator.foreign.MemorySegment;\n+import jdk.incubator.foreign.NativeSymbol;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -48,0 +56,1 @@\n+<<<<<<< HEAD\n@@ -49,0 +58,3 @@\n+=======\n+import static jdk.incubator.foreign.ValueLayout.ADDRESS;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -65,0 +77,1 @@\n+<<<<<<< HEAD\n@@ -70,0 +83,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) }\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -82,0 +102,1 @@\n+<<<<<<< HEAD\n@@ -87,0 +108,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -104,0 +132,1 @@\n+<<<<<<< HEAD\n@@ -109,0 +138,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -128,0 +164,1 @@\n+<<<<<<< HEAD\n@@ -133,0 +170,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -159,0 +203,1 @@\n+<<<<<<< HEAD\n@@ -164,0 +209,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -191,0 +243,1 @@\n+<<<<<<< HEAD\n@@ -192,0 +245,3 @@\n+=======\n+                ADDRESS, C_INT, C_DOUBLE).asVariadic(C_INT, C_DOUBLE, C_DOUBLE);\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -196,0 +252,1 @@\n+<<<<<<< HEAD\n@@ -201,0 +258,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fdExpected);\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -230,0 +294,1 @@\n+<<<<<<< HEAD\n@@ -235,0 +300,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -260,0 +332,1 @@\n+<<<<<<< HEAD\n@@ -265,0 +338,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -291,0 +371,1 @@\n+<<<<<<< HEAD\n@@ -296,0 +377,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -312,0 +400,1 @@\n+<<<<<<< HEAD\n@@ -317,0 +406,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -336,0 +432,1 @@\n+<<<<<<< HEAD\n@@ -341,0 +438,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), MethodType.methodType(void.class, NativeSymbol.class, MemoryAddress.class));\n+        assertEquals(callingSequence.functionDesc(), FunctionDescriptor.ofVoid(ADDRESS, C_POINTER));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n@@ -365,0 +469,1 @@\n+<<<<<<< HEAD\n@@ -370,0 +475,7 @@\n+=======\n+        assertEquals(callingSequence.methodType(), mt.insertParameterTypes(0, NativeSymbol.class));\n+        assertEquals(callingSequence.functionDesc(), fd.insertArgumentLayouts(0, ADDRESS));\n+\n+        checkArgumentBindings(callingSequence, new Binding[][]{\n+            { unboxAddress(NativeSymbol.class), vmStore(r10, long.class) },\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c\n","filename":"test\/jdk\/java\/foreign\/callarranger\/TestWindowsCallArranger.java","additions":112,"deletions":0,"binary":false,"changes":112,"status":"modified"},{"patch":"@@ -26,0 +26,1 @@\n+<<<<<<< HEAD:test\/jdk\/jdk\/internal\/vm\/Continuation\/libBasicJNI.c\n@@ -31,0 +32,11 @@\n+=======\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+  Unimplemented();\n+  return nullptr;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/x86\/universalNativeInvoker_x86_32.cpp\n","filename":"test\/jdk\/jdk\/internal\/vm\/Continuation\/libBasicJNI.c","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -33,0 +33,12 @@\n+<<<<<<< HEAD:test\/jdk\/jdk\/jfr\/jvm\/PlaceholderEventWriter.java\n+=======\n+RuntimeStub* ProgrammableInvoker::make_native_invoker(BasicType* signature,\n+                                                      int num_args,\n+                                                      BasicType ret_bt,\n+                                                      const ABIDescriptor& abi,\n+                                                      const GrowableArray<VMReg>& input_registers,\n+                                                      const GrowableArray<VMReg>& output_registers,\n+                                                      bool needs_return_buffer) {\n+  Unimplemented();\n+  return nullptr;\n+>>>>>>> b4e38e04a3aefec656763e4c5f6b3d0d14fb0d6c:src\/hotspot\/cpu\/ppc\/universalNativeInvoker_ppc.cpp\n","filename":"test\/jdk\/jdk\/jfr\/jvm\/PlaceholderEventWriter.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"}]}