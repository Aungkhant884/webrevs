{"files":[{"patch":"@@ -1356,1 +1356,6 @@\n-    copy_memory(aligned, s, d, count, rscratch1, size);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      bool add_entry = !is_oop && (!aligned || sizeof(jlong) == size);\n+      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      copy_memory(aligned, s, d, count, rscratch1, size);\n+    }\n@@ -1422,1 +1427,6 @@\n-    copy_memory(aligned, s, d, count, rscratch1, -size);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      bool add_entry = !is_oop && (!aligned || sizeof(jlong) == size);\n+      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      copy_memory(aligned, s, d, count, rscratch1, -size);\n+    }\n@@ -5921,0 +5931,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 8\n@@ -5922,0 +5933,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":16,"deletions":2,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -927,1 +927,1 @@\n-  int generate_forward_aligned_copy_loop(Register from, Register to, Register count, int bytes_per_count) {\n+  int generate_forward_aligned_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool unsafe_copy = false) {\n@@ -953,2 +953,5 @@\n-    \/\/ predecrease to exit when there is less than count_per_loop\n-    __ sub_32(count, count, count_per_loop);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);\n+      \/\/ predecrease to exit when there is less than count_per_loop\n+      __ sub_32(count, count, count_per_loop);\n@@ -956,2 +959,2 @@\n-    if (pld_offset != 0) {\n-      pld_offset = (pld_offset < 0) ? -pld_offset : pld_offset;\n+      if (pld_offset != 0) {\n+        pld_offset = (pld_offset < 0) ? -pld_offset : pld_offset;\n@@ -959,1 +962,1 @@\n-      prefetch(from, to, 0);\n+        prefetch(from, to, 0);\n@@ -961,6 +964,6 @@\n-      if (prefetch_before) {\n-        \/\/ If prefetch is done ahead, final PLDs that overflow the\n-        \/\/ copied area can be easily avoided. 'count' is predecreased\n-        \/\/ by the prefetch distance to optimize the inner loop and the\n-        \/\/ outer loop skips the PLD.\n-        __ subs_32(count, count, (bytes_per_loop+pld_offset)\/bytes_per_count);\n+        if (prefetch_before) {\n+          \/\/ If prefetch is done ahead, final PLDs that overflow the\n+          \/\/ copied area can be easily avoided. 'count' is predecreased\n+          \/\/ by the prefetch distance to optimize the inner loop and the\n+          \/\/ outer loop skips the PLD.\n+          __ subs_32(count, count, (bytes_per_loop+pld_offset)\/bytes_per_count);\n@@ -968,3 +971,3 @@\n-        \/\/ skip prefetch for small copies\n-        __ b(L_skip_pld, lt);\n-      }\n+          \/\/ skip prefetch for small copies\n+          __ b(L_skip_pld, lt);\n+        }\n@@ -972,6 +975,6 @@\n-      int offset = ArmCopyCacheLineSize;\n-      while (offset <= pld_offset) {\n-        prefetch(from, to, offset);\n-        offset += ArmCopyCacheLineSize;\n-      };\n-    }\n+        int offset = ArmCopyCacheLineSize;\n+        while (offset <= pld_offset) {\n+          prefetch(from, to, offset);\n+          offset += ArmCopyCacheLineSize;\n+        };\n+      }\n@@ -979,3 +982,3 @@\n-    {\n-      \/\/ 32-bit ARM note: we have tried implementing loop unrolling to skip one\n-      \/\/ PLD with 64 bytes cache line but the gain was not significant.\n+      {\n+        \/\/ 32-bit ARM note: we have tried implementing loop unrolling to skip one\n+        \/\/ PLD with 64 bytes cache line but the gain was not significant.\n@@ -983,3 +986,3 @@\n-      Label L_copy_loop;\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_copy_loop);\n+        Label L_copy_loop;\n+        __ align(OptoLoopAlignment);\n+        __ BIND(L_copy_loop);\n@@ -987,4 +990,4 @@\n-      if (prefetch_before) {\n-        prefetch(from, to, bytes_per_loop + pld_offset);\n-        __ BIND(L_skip_pld);\n-      }\n+        if (prefetch_before) {\n+          prefetch(from, to, bytes_per_loop + pld_offset);\n+          __ BIND(L_skip_pld);\n+        }\n@@ -992,10 +995,10 @@\n-      if (split_read) {\n-        \/\/ Split the register set in two sets so that there is less\n-        \/\/ latency between LDM and STM (R3-R6 available while R7-R10\n-        \/\/ still loading) and less register locking issue when iterating\n-        \/\/ on the first LDM.\n-        __ ldmia(from, RegisterSet(R3, R6), writeback);\n-        __ ldmia(from, RegisterSet(R7, R10), writeback);\n-      } else {\n-        __ ldmia(from, RegisterSet(R3, R10), writeback);\n-      }\n+        if (split_read) {\n+          \/\/ Split the register set in two sets so that there is less\n+          \/\/ latency between LDM and STM (R3-R6 available while R7-R10\n+          \/\/ still loading) and less register locking issue when iterating\n+          \/\/ on the first LDM.\n+          __ ldmia(from, RegisterSet(R3, R6), writeback);\n+          __ ldmia(from, RegisterSet(R7, R10), writeback);\n+        } else {\n+          __ ldmia(from, RegisterSet(R3, R10), writeback);\n+        }\n@@ -1003,1 +1006,1 @@\n-      __ subs_32(count, count, count_per_loop);\n+        __ subs_32(count, count, count_per_loop);\n@@ -1005,3 +1008,3 @@\n-      if (prefetch_after) {\n-        prefetch(from, to, pld_offset, bytes_per_loop);\n-      }\n+        if (prefetch_after) {\n+          prefetch(from, to, pld_offset, bytes_per_loop);\n+        }\n@@ -1009,6 +1012,6 @@\n-      if (split_write) {\n-        __ stmia(to, RegisterSet(R3, R6), writeback);\n-        __ stmia(to, RegisterSet(R7, R10), writeback);\n-      } else {\n-        __ stmia(to, RegisterSet(R3, R10), writeback);\n-      }\n+        if (split_write) {\n+          __ stmia(to, RegisterSet(R3, R6), writeback);\n+          __ stmia(to, RegisterSet(R7, R10), writeback);\n+        } else {\n+          __ stmia(to, RegisterSet(R3, R10), writeback);\n+        }\n@@ -1016,1 +1019,1 @@\n-      __ b(L_copy_loop, ge);\n+        __ b(L_copy_loop, ge);\n@@ -1018,4 +1021,5 @@\n-      if (prefetch_before) {\n-        \/\/ the inner loop may end earlier, allowing to skip PLD for the last iterations\n-        __ cmn_32(count, (bytes_per_loop + pld_offset)\/bytes_per_count);\n-        __ b(L_skip_pld, ge);\n+        if (prefetch_before) {\n+          \/\/ the inner loop may end earlier, allowing to skip PLD for the last iterations\n+          __ cmn_32(count, (bytes_per_loop + pld_offset)\/bytes_per_count);\n+          __ b(L_skip_pld, ge);\n+        }\n@@ -1023,3 +1027,2 @@\n-    }\n-    BLOCK_COMMENT(\"Remaining bytes:\");\n-    \/\/ still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes\n+      BLOCK_COMMENT(\"Remaining bytes:\");\n+      \/\/ still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes\n@@ -1027,2 +1030,2 @@\n-    \/\/ __ add(count, count, ...); \/\/ addition useless for the bit tests\n-    assert (pld_offset % bytes_per_loop == 0, \"decreasing count by pld_offset before loop must not change tested bits\");\n+      \/\/ __ add(count, count, ...); \/\/ addition useless for the bit tests\n+      assert (pld_offset % bytes_per_loop == 0, \"decreasing count by pld_offset before loop must not change tested bits\");\n@@ -1030,3 +1033,3 @@\n-    __ tst(count, 16 \/ bytes_per_count);\n-    __ ldmia(from, RegisterSet(R3, R6), writeback, ne); \/\/ copy 16 bytes\n-    __ stmia(to, RegisterSet(R3, R6), writeback, ne);\n+      __ tst(count, 16 \/ bytes_per_count);\n+      __ ldmia(from, RegisterSet(R3, R6), writeback, ne); \/\/ copy 16 bytes\n+      __ stmia(to, RegisterSet(R3, R6), writeback, ne);\n@@ -1034,3 +1037,3 @@\n-    __ tst(count, 8 \/ bytes_per_count);\n-    __ ldmia(from, RegisterSet(R3, R4), writeback, ne); \/\/ copy 8 bytes\n-    __ stmia(to, RegisterSet(R3, R4), writeback, ne);\n+      __ tst(count, 8 \/ bytes_per_count);\n+      __ ldmia(from, RegisterSet(R3, R4), writeback, ne); \/\/ copy 8 bytes\n+      __ stmia(to, RegisterSet(R3, R4), writeback, ne);\n@@ -1038,5 +1041,5 @@\n-    if (bytes_per_count <= 4) {\n-      __ tst(count, 4 \/ bytes_per_count);\n-      __ ldr(R3, Address(from, 4, post_indexed), ne); \/\/ copy 4 bytes\n-      __ str(R3, Address(to, 4, post_indexed), ne);\n-    }\n+      if (bytes_per_count <= 4) {\n+        __ tst(count, 4 \/ bytes_per_count);\n+        __ ldr(R3, Address(from, 4, post_indexed), ne); \/\/ copy 4 bytes\n+        __ str(R3, Address(to, 4, post_indexed), ne);\n+      }\n@@ -1044,5 +1047,5 @@\n-    if (bytes_per_count <= 2) {\n-      __ tst(count, 2 \/ bytes_per_count);\n-      __ ldrh(R3, Address(from, 2, post_indexed), ne); \/\/ copy 2 bytes\n-      __ strh(R3, Address(to, 2, post_indexed), ne);\n-    }\n+      if (bytes_per_count <= 2) {\n+        __ tst(count, 2 \/ bytes_per_count);\n+        __ ldrh(R3, Address(from, 2, post_indexed), ne); \/\/ copy 2 bytes\n+        __ strh(R3, Address(to, 2, post_indexed), ne);\n+      }\n@@ -1050,4 +1053,5 @@\n-    if (bytes_per_count == 1) {\n-      __ tst(count, 1);\n-      __ ldrb(R3, Address(from, 1, post_indexed), ne);\n-      __ strb(R3, Address(to, 1, post_indexed), ne);\n+      if (bytes_per_count == 1) {\n+        __ tst(count, 1);\n+        __ ldrb(R3, Address(from, 1, post_indexed), ne);\n+        __ strb(R3, Address(to, 1, post_indexed), ne);\n+      }\n@@ -1082,1 +1086,1 @@\n-  int generate_backward_aligned_copy_loop(Register end_from, Register end_to, Register count, int bytes_per_count) {\n+  int generate_backward_aligned_copy_loop(Register end_from, Register end_to, Register count, int bytes_per_count, bool unsafe_copy = false) {\n@@ -1098,1 +1102,4 @@\n-    __ sub_32(count, count, count_per_loop);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);\n+      __ sub_32(count, count, count_per_loop);\n@@ -1100,2 +1107,2 @@\n-    const bool prefetch_before = pld_offset < 0;\n-    const bool prefetch_after = pld_offset > 0;\n+      const bool prefetch_before = pld_offset < 0;\n+      const bool prefetch_after = pld_offset > 0;\n@@ -1103,1 +1110,1 @@\n-    Label L_skip_pld;\n+      Label L_skip_pld;\n@@ -1105,2 +1112,2 @@\n-    if (pld_offset != 0) {\n-      pld_offset = (pld_offset < 0) ? -pld_offset : pld_offset;\n+      if (pld_offset != 0) {\n+        pld_offset = (pld_offset < 0) ? -pld_offset : pld_offset;\n@@ -1108,1 +1115,1 @@\n-      prefetch(end_from, end_to, -wordSize);\n+        prefetch(end_from, end_to, -wordSize);\n@@ -1110,4 +1117,4 @@\n-      if (prefetch_before) {\n-        __ subs_32(count, count, (bytes_per_loop + pld_offset) \/ bytes_per_count);\n-        __ b(L_skip_pld, lt);\n-      }\n+        if (prefetch_before) {\n+          __ subs_32(count, count, (bytes_per_loop + pld_offset) \/ bytes_per_count);\n+          __ b(L_skip_pld, lt);\n+        }\n@@ -1115,6 +1122,6 @@\n-      int offset = ArmCopyCacheLineSize;\n-      while (offset <= pld_offset) {\n-        prefetch(end_from, end_to, -(wordSize + offset));\n-        offset += ArmCopyCacheLineSize;\n-      };\n-    }\n+        int offset = ArmCopyCacheLineSize;\n+        while (offset <= pld_offset) {\n+          prefetch(end_from, end_to, -(wordSize + offset));\n+          offset += ArmCopyCacheLineSize;\n+        };\n+      }\n@@ -1122,3 +1129,3 @@\n-    {\n-      \/\/ 32-bit ARM note: we have tried implementing loop unrolling to skip one\n-      \/\/ PLD with 64 bytes cache line but the gain was not significant.\n+      {\n+        \/\/ 32-bit ARM note: we have tried implementing loop unrolling to skip one\n+        \/\/ PLD with 64 bytes cache line but the gain was not significant.\n@@ -1126,3 +1133,3 @@\n-      Label L_copy_loop;\n-      __ align(OptoLoopAlignment);\n-      __ BIND(L_copy_loop);\n+        Label L_copy_loop;\n+        __ align(OptoLoopAlignment);\n+        __ BIND(L_copy_loop);\n@@ -1130,4 +1137,4 @@\n-      if (prefetch_before) {\n-        prefetch(end_from, end_to, -(wordSize + bytes_per_loop + pld_offset));\n-        __ BIND(L_skip_pld);\n-      }\n+        if (prefetch_before) {\n+          prefetch(end_from, end_to, -(wordSize + bytes_per_loop + pld_offset));\n+          __ BIND(L_skip_pld);\n+        }\n@@ -1135,6 +1142,6 @@\n-      if (split_read) {\n-        __ ldmdb(end_from, RegisterSet(R7, R10), writeback);\n-        __ ldmdb(end_from, RegisterSet(R3, R6), writeback);\n-      } else {\n-        __ ldmdb(end_from, RegisterSet(R3, R10), writeback);\n-      }\n+        if (split_read) {\n+          __ ldmdb(end_from, RegisterSet(R7, R10), writeback);\n+          __ ldmdb(end_from, RegisterSet(R3, R6), writeback);\n+        } else {\n+          __ ldmdb(end_from, RegisterSet(R3, R10), writeback);\n+        }\n@@ -1142,1 +1149,1 @@\n-      __ subs_32(count, count, count_per_loop);\n+        __ subs_32(count, count, count_per_loop);\n@@ -1144,3 +1151,3 @@\n-      if (prefetch_after) {\n-        prefetch(end_from, end_to, -(wordSize + pld_offset), -bytes_per_loop);\n-      }\n+        if (prefetch_after) {\n+          prefetch(end_from, end_to, -(wordSize + pld_offset), -bytes_per_loop);\n+        }\n@@ -1148,6 +1155,6 @@\n-      if (split_write) {\n-        __ stmdb(end_to, RegisterSet(R7, R10), writeback);\n-        __ stmdb(end_to, RegisterSet(R3, R6), writeback);\n-      } else {\n-        __ stmdb(end_to, RegisterSet(R3, R10), writeback);\n-      }\n+        if (split_write) {\n+          __ stmdb(end_to, RegisterSet(R7, R10), writeback);\n+          __ stmdb(end_to, RegisterSet(R3, R6), writeback);\n+        } else {\n+          __ stmdb(end_to, RegisterSet(R3, R10), writeback);\n+        }\n@@ -1155,1 +1162,1 @@\n-      __ b(L_copy_loop, ge);\n+        __ b(L_copy_loop, ge);\n@@ -1157,3 +1164,4 @@\n-      if (prefetch_before) {\n-        __ cmn_32(count, (bytes_per_loop + pld_offset)\/bytes_per_count);\n-        __ b(L_skip_pld, ge);\n+        if (prefetch_before) {\n+          __ cmn_32(count, (bytes_per_loop + pld_offset)\/bytes_per_count);\n+          __ b(L_skip_pld, ge);\n+        }\n@@ -1161,3 +1169,2 @@\n-    }\n-    BLOCK_COMMENT(\"Remaining bytes:\");\n-    \/\/ still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes\n+      BLOCK_COMMENT(\"Remaining bytes:\");\n+      \/\/ still 0..bytes_per_loop-1 aligned bytes to copy, count already decreased by (at least) bytes_per_loop bytes\n@@ -1165,2 +1172,2 @@\n-    \/\/ __ add(count, count, ...); \/\/ addition useless for the bit tests\n-    assert (pld_offset % bytes_per_loop == 0, \"decreasing count by pld_offset before loop must not change tested bits\");\n+      \/\/ __ add(count, count, ...); \/\/ addition useless for the bit tests\n+      assert (pld_offset % bytes_per_loop == 0, \"decreasing count by pld_offset before loop must not change tested bits\");\n@@ -1168,3 +1175,3 @@\n-    __ tst(count, 16 \/ bytes_per_count);\n-    __ ldmdb(end_from, RegisterSet(R3, R6), writeback, ne); \/\/ copy 16 bytes\n-    __ stmdb(end_to, RegisterSet(R3, R6), writeback, ne);\n+      __ tst(count, 16 \/ bytes_per_count);\n+      __ ldmdb(end_from, RegisterSet(R3, R6), writeback, ne); \/\/ copy 16 bytes\n+      __ stmdb(end_to, RegisterSet(R3, R6), writeback, ne);\n@@ -1172,3 +1179,3 @@\n-    __ tst(count, 8 \/ bytes_per_count);\n-    __ ldmdb(end_from, RegisterSet(R3, R4), writeback, ne); \/\/ copy 8 bytes\n-    __ stmdb(end_to, RegisterSet(R3, R4), writeback, ne);\n+      __ tst(count, 8 \/ bytes_per_count);\n+      __ ldmdb(end_from, RegisterSet(R3, R4), writeback, ne); \/\/ copy 8 bytes\n+      __ stmdb(end_to, RegisterSet(R3, R4), writeback, ne);\n@@ -1176,5 +1183,5 @@\n-    if (bytes_per_count <= 4) {\n-      __ tst(count, 4 \/ bytes_per_count);\n-      __ ldr(R3, Address(end_from, -4, pre_indexed), ne); \/\/ copy 4 bytes\n-      __ str(R3, Address(end_to, -4, pre_indexed), ne);\n-    }\n+      if (bytes_per_count <= 4) {\n+        __ tst(count, 4 \/ bytes_per_count);\n+        __ ldr(R3, Address(end_from, -4, pre_indexed), ne); \/\/ copy 4 bytes\n+        __ str(R3, Address(end_to, -4, pre_indexed), ne);\n+      }\n@@ -1182,5 +1189,5 @@\n-    if (bytes_per_count <= 2) {\n-      __ tst(count, 2 \/ bytes_per_count);\n-      __ ldrh(R3, Address(end_from, -2, pre_indexed), ne); \/\/ copy 2 bytes\n-      __ strh(R3, Address(end_to, -2, pre_indexed), ne);\n-    }\n+      if (bytes_per_count <= 2) {\n+        __ tst(count, 2 \/ bytes_per_count);\n+        __ ldrh(R3, Address(end_from, -2, pre_indexed), ne); \/\/ copy 2 bytes\n+        __ strh(R3, Address(end_to, -2, pre_indexed), ne);\n+      }\n@@ -1188,4 +1195,5 @@\n-    if (bytes_per_count == 1) {\n-      __ tst(count, 1);\n-      __ ldrb(R3, Address(end_from, -1, pre_indexed), ne);\n-      __ strb(R3, Address(end_to, -1, pre_indexed), ne);\n+      if (bytes_per_count == 1) {\n+        __ tst(count, 1);\n+        __ ldrb(R3, Address(end_from, -1, pre_indexed), ne);\n+        __ strb(R3, Address(end_to, -1, pre_indexed), ne);\n+      }\n@@ -1193,1 +1201,0 @@\n-\n@@ -1748,1 +1755,1 @@\n-  void copy_small_array(Register from, Register to, Register count, Register tmp, Register tmp2, int bytes_per_count, bool forward, Label & entry) {\n+  void copy_small_array(Register from, Register to, Register count, Register tmp, Register tmp2, int bytes_per_count, bool forward, Label & entry, bool unsafe_copy = false) {\n@@ -1751,8 +1758,12 @@\n-    __ align(OptoLoopAlignment);\n-    Label L_small_loop;\n-    __ BIND(L_small_loop);\n-    store_one(tmp, to, bytes_per_count, forward, al, tmp2);\n-    __ BIND(entry); \/\/ entry point\n-    __ subs(count, count, 1);\n-    load_one(tmp, from, bytes_per_count, forward, ge, tmp2);\n-    __ b(L_small_loop, ge);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);\n+      __ align(OptoLoopAlignment);\n+      Label L_small_loop;\n+      __ BIND(L_small_loop);\n+      store_one(tmp, to, bytes_per_count, forward, al, tmp2);\n+      __ BIND(entry); \/\/ entry point\n+      __ subs(count, count, 1);\n+      load_one(tmp, from, bytes_per_count, forward, ge, tmp2);\n+      __ b(L_small_loop, ge);\n+    }\n@@ -1875,1 +1886,1 @@\n-  int align_dst_and_generate_shifted_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool forward) {\n+  int align_dst_and_generate_shifted_copy_loop(Register from, Register to, Register count, int bytes_per_count, bool forward, bool unsafe_copy = false) {\n@@ -1885,1 +1896,0 @@\n-    load_one(Rval, from, wordSize, forward);\n@@ -1887,14 +1897,33 @@\n-    switch (bytes_per_count) {\n-      case 2:\n-        min_copy = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n-        break;\n-      case 1:\n-      {\n-        Label L1, L2, L3;\n-        int min_copy1, min_copy2, min_copy3;\n-\n-        Label L_loop_finished;\n-\n-        if (forward) {\n-            __ tbz(to, 0, L2);\n-            __ tbz(to, 1, L1);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, unsafe_copy, true);\n+      load_one(Rval, from, wordSize, forward);\n+\n+      switch (bytes_per_count) {\n+        case 2:\n+          min_copy = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n+          break;\n+        case 1:\n+        {\n+          Label L1, L2, L3;\n+          int min_copy1, min_copy2, min_copy3;\n+\n+          Label L_loop_finished;\n+\n+          if (forward) {\n+              __ tbz(to, 0, L2);\n+              __ tbz(to, 1, L1);\n+\n+              __ BIND(L3);\n+              min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);\n+              __ b(L_loop_finished);\n+\n+              __ BIND(L1);\n+              min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);\n+              __ b(L_loop_finished);\n+\n+              __ BIND(L2);\n+              min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n+          } else {\n+              __ tbz(to, 0, L2);\n+              __ tbnz(to, 1, L3);\n@@ -1902,3 +1931,3 @@\n-            __ BIND(L3);\n-            min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);\n-            __ b(L_loop_finished);\n+              __ BIND(L1);\n+              min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);\n+              __ b(L_loop_finished);\n@@ -1906,3 +1935,3 @@\n-            __ BIND(L1);\n-            min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);\n-            __ b(L_loop_finished);\n+               __ BIND(L3);\n+              min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);\n+              __ b(L_loop_finished);\n@@ -1910,5 +1939,3 @@\n-            __ BIND(L2);\n-            min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n-        } else {\n-            __ tbz(to, 0, L2);\n-            __ tbnz(to, 1, L3);\n+             __ BIND(L2);\n+              min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n+          }\n@@ -1916,3 +1943,1 @@\n-            __ BIND(L1);\n-            min_copy1 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 1, bytes_per_count, forward);\n-            __ b(L_loop_finished);\n+          min_copy = MAX2(MAX2(min_copy1, min_copy2), min_copy3);\n@@ -1920,3 +1945,1 @@\n-             __ BIND(L3);\n-            min_copy3 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 3, bytes_per_count, forward);\n-            __ b(L_loop_finished);\n+          __ BIND(L_loop_finished);\n@@ -1924,2 +1947,1 @@\n-           __ BIND(L2);\n-            min_copy2 = align_dst_and_generate_shifted_copy_loop(from, to, count, Rval, 2, bytes_per_count, forward);\n+          break;\n@@ -1927,6 +1949,3 @@\n-\n-        min_copy = MAX2(MAX2(min_copy1, min_copy2), min_copy3);\n-\n-        __ BIND(L_loop_finished);\n-\n-        break;\n+        default:\n+          ShouldNotReachHere();\n+          break;\n@@ -1934,3 +1953,0 @@\n-      default:\n-        ShouldNotReachHere();\n-        break;\n@@ -1938,1 +1954,0 @@\n-\n@@ -1962,0 +1977,7 @@\n+  address generate_unsafecopy_common_error_exit() {\n+    address start_pc = __ pc();\n+      __ mov(R0, 0);\n+      __ ret();\n+    return start_pc;\n+  }\n+\n@@ -2032,2 +2054,7 @@\n-    int count_required_to_align = from_is_aligned ? 0 : align_src(from, to, count, tmp1, bytes_per_count, forward);\n-    assert (small_copy_limit >= count_required_to_align, \"alignment could exhaust count\");\n+    int count_required_to_align = 0;\n+    {\n+      \/\/ UnsafeCopyMemoryMark page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      count_required_to_align = from_is_aligned ? 0 : align_src(from, to, count, tmp1, bytes_per_count, forward);\n+      assert (small_copy_limit >= count_required_to_align, \"alignment could exhaust count\");\n+    }\n@@ -2063,1 +2090,1 @@\n-      min_copy = generate_forward_aligned_copy_loop (from, to, count, bytes_per_count);\n+      min_copy = generate_forward_aligned_copy_loop(from, to, count, bytes_per_count, !aligned \/*add UnsafeCopyMemory entry*\/);\n@@ -2065,1 +2092,1 @@\n-      min_copy = generate_backward_aligned_copy_loop(from, to, count, bytes_per_count);\n+      min_copy = generate_backward_aligned_copy_loop(from, to, count, bytes_per_count, !aligned \/*add UnsafeCopyMemory entry*\/);\n@@ -2076,1 +2103,1 @@\n-      copy_small_array(from, to, count, tmp1, tmp2, bytes_per_count, forward, L_small_array \/* entry *\/);\n+      copy_small_array(from, to, count, tmp1, tmp2, bytes_per_count, forward, L_small_array \/* entry *\/, !aligned \/*add UnsafeCopyMemory entry*\/);\n@@ -2087,1 +2114,1 @@\n-      int min_copy_shifted = align_dst_and_generate_shifted_copy_loop(from, to, count, bytes_per_count, forward);\n+      int min_copy_shifted = align_dst_and_generate_shifted_copy_loop(from, to, count, bytes_per_count, forward, !aligned \/*add UnsafeCopyMemory entry*\/);\n@@ -2872,0 +2899,3 @@\n+    address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();\n+    UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);\n+\n@@ -3054,0 +3084,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 32\n@@ -3055,0 +3086,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/arm\/stubGenerator_arm.cpp","additions":250,"deletions":216,"binary":false,"changes":466,"status":"modified"},{"patch":"@@ -955,0 +955,14 @@\n+  \/\/ This is common errorexit stub for UnsafeCopyMemory.\n+  address generate_unsafecopy_common_error_exit() {\n+    address start_pc = __ pc();\n+    Register tmp1 = R6_ARG4;\n+    \/\/ probably copy stub would have changed value reset it.\n+    if (VM_Version::has_mfdscr()) {\n+      __ load_const_optimized(tmp1, VM_Version::_dscr_val);\n+      __ mtdscr(tmp1);\n+    }\n+    __ li(R3_RET, 0); \/\/ return 0\n+    __ blr();\n+    return start_pc;\n+  }\n+\n@@ -992,0 +1006,3 @@\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n@@ -993,4 +1010,4 @@\n-    \/\/ Don't try anything fancy if arrays don't have many elements.\n-    __ li(tmp3, 0);\n-    __ cmpwi(CCR0, R5_ARG3, 17);\n-    __ ble(CCR0, l_6); \/\/ copy 4 at a time\n+      \/\/ Don't try anything fancy if arrays don't have many elements.\n+      __ li(tmp3, 0);\n+      __ cmpwi(CCR0, R5_ARG3, 17);\n+      __ ble(CCR0, l_6); \/\/ copy 4 at a time\n@@ -998,18 +1015,4 @@\n-    if (!aligned) {\n-      __ xorr(tmp1, R3_ARG1, R4_ARG2);\n-      __ andi_(tmp1, tmp1, 3);\n-      __ bne(CCR0, l_6); \/\/ If arrays don't have the same alignment mod 4, do 4 element copy.\n-\n-      \/\/ Copy elements if necessary to align to 4 bytes.\n-      __ neg(tmp1, R3_ARG1); \/\/ Compute distance to alignment boundary.\n-      __ andi_(tmp1, tmp1, 3);\n-      __ beq(CCR0, l_2);\n-\n-      __ subf(R5_ARG3, tmp1, R5_ARG3);\n-      __ bind(l_9);\n-      __ lbz(tmp2, 0, R3_ARG1);\n-      __ addic_(tmp1, tmp1, -1);\n-      __ stb(tmp2, 0, R4_ARG2);\n-      __ addi(R3_ARG1, R3_ARG1, 1);\n-      __ addi(R4_ARG2, R4_ARG2, 1);\n-      __ bne(CCR0, l_9);\n+      if (!aligned) {\n+        __ xorr(tmp1, R3_ARG1, R4_ARG2);\n+        __ andi_(tmp1, tmp1, 3);\n+        __ bne(CCR0, l_6); \/\/ If arrays don't have the same alignment mod 4, do 4 element copy.\n@@ -1017,2 +1020,4 @@\n-      __ bind(l_2);\n-    }\n+        \/\/ Copy elements if necessary to align to 4 bytes.\n+        __ neg(tmp1, R3_ARG1); \/\/ Compute distance to alignment boundary.\n+        __ andi_(tmp1, tmp1, 3);\n+        __ beq(CCR0, l_2);\n@@ -1020,4 +1025,11 @@\n-    \/\/ copy 8 elements at a time\n-    __ xorr(tmp2, R3_ARG1, R4_ARG2); \/\/ skip if src & dest have differing alignment mod 8\n-    __ andi_(tmp1, tmp2, 7);\n-    __ bne(CCR0, l_7); \/\/ not same alignment -> to or from is aligned -> copy 8\n+        __ subf(R5_ARG3, tmp1, R5_ARG3);\n+        __ bind(l_9);\n+        __ lbz(tmp2, 0, R3_ARG1);\n+        __ addic_(tmp1, tmp1, -1);\n+        __ stb(tmp2, 0, R4_ARG2);\n+        __ addi(R3_ARG1, R3_ARG1, 1);\n+        __ addi(R4_ARG2, R4_ARG2, 1);\n+        __ bne(CCR0, l_9);\n+\n+        __ bind(l_2);\n+      }\n@@ -1025,3 +1037,4 @@\n-    \/\/ copy a 2-element word if necessary to align to 8 bytes\n-    __ andi_(R0, R3_ARG1, 7);\n-    __ beq(CCR0, l_7);\n+      \/\/ copy 8 elements at a time\n+      __ xorr(tmp2, R3_ARG1, R4_ARG2); \/\/ skip if src & dest have differing alignment mod 8\n+      __ andi_(tmp1, tmp2, 7);\n+      __ bne(CCR0, l_7); \/\/ not same alignment -> to or from is aligned -> copy 8\n@@ -1029,8 +1042,3 @@\n-    __ lwzx(tmp2, R3_ARG1, tmp3);\n-    __ addi(R5_ARG3, R5_ARG3, -4);\n-    __ stwx(tmp2, R4_ARG2, tmp3);\n-    { \/\/ FasterArrayCopy\n-      __ addi(R3_ARG1, R3_ARG1, 4);\n-      __ addi(R4_ARG2, R4_ARG2, 4);\n-    }\n-    __ bind(l_7);\n+      \/\/ copy a 2-element word if necessary to align to 8 bytes\n+      __ andi_(R0, R3_ARG1, 7);\n+      __ beq(CCR0, l_7);\n@@ -1038,3 +1046,8 @@\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 31);\n-      __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 32 elements remain\n+      __ lwzx(tmp2, R3_ARG1, tmp3);\n+      __ addi(R5_ARG3, R5_ARG3, -4);\n+      __ stwx(tmp2, R4_ARG2, tmp3);\n+      { \/\/ FasterArrayCopy\n+        __ addi(R3_ARG1, R3_ARG1, 4);\n+        __ addi(R4_ARG2, R4_ARG2, 4);\n+      }\n+      __ bind(l_7);\n@@ -1042,3 +1055,3 @@\n-      __ srdi(tmp1, R5_ARG3, 5);\n-      __ andi_(R5_ARG3, R5_ARG3, 31);\n-      __ mtctr(tmp1);\n+      { \/\/ FasterArrayCopy\n+        __ cmpwi(CCR0, R5_ARG3, 31);\n+        __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 32 elements remain\n@@ -1046,1 +1059,3 @@\n-     if (!VM_Version::has_vsx()) {\n+        __ srdi(tmp1, R5_ARG3, 5);\n+        __ andi_(R5_ARG3, R5_ARG3, 31);\n+        __ mtctr(tmp1);\n@@ -1048,15 +1063,1 @@\n-      __ bind(l_8);\n-      \/\/ Use unrolled version for mass copying (copy 32 elements a time)\n-      \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-      \/\/ Therefore, the following sequence is made for the good of both.\n-      __ ld(tmp1, 0, R3_ARG1);\n-      __ ld(tmp2, 8, R3_ARG1);\n-      __ ld(tmp3, 16, R3_ARG1);\n-      __ ld(tmp4, 24, R3_ARG1);\n-      __ std(tmp1, 0, R4_ARG2);\n-      __ std(tmp2, 8, R4_ARG2);\n-      __ std(tmp3, 16, R4_ARG2);\n-      __ std(tmp4, 24, R4_ARG2);\n-      __ addi(R3_ARG1, R3_ARG1, 32);\n-      __ addi(R4_ARG2, R4_ARG2, 32);\n-      __ bdnz(l_8);\n+       if (!VM_Version::has_vsx()) {\n@@ -1064,1 +1065,15 @@\n-    } else { \/\/ Processor supports VSX, so use it to mass copy.\n+        __ bind(l_8);\n+        \/\/ Use unrolled version for mass copying (copy 32 elements a time)\n+        \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n+        \/\/ Therefore, the following sequence is made for the good of both.\n+        __ ld(tmp1, 0, R3_ARG1);\n+        __ ld(tmp2, 8, R3_ARG1);\n+        __ ld(tmp3, 16, R3_ARG1);\n+        __ ld(tmp4, 24, R3_ARG1);\n+        __ std(tmp1, 0, R4_ARG2);\n+        __ std(tmp2, 8, R4_ARG2);\n+        __ std(tmp3, 16, R4_ARG2);\n+        __ std(tmp4, 24, R4_ARG2);\n+        __ addi(R3_ARG1, R3_ARG1, 32);\n+        __ addi(R4_ARG2, R4_ARG2, 32);\n+        __ bdnz(l_8);\n@@ -1066,2 +1081,1 @@\n-      \/\/ Prefetch the data into the L2 cache.\n-      __ dcbt(R3_ARG1, 0);\n+      } else { \/\/ Processor supports VSX, so use it to mass copy.\n@@ -1069,5 +1083,2 @@\n-      \/\/ If supported set DSCR pre-fetch to deepest.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-        __ mtdscr(tmp2);\n-      }\n+        \/\/ Prefetch the data into the L2 cache.\n+        __ dcbt(R3_ARG1, 0);\n@@ -1075,1 +1086,5 @@\n-      __ li(tmp1, 16);\n+        \/\/ If supported set DSCR pre-fetch to deepest.\n+        if (VM_Version::has_mfdscr()) {\n+          __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n+          __ mtdscr(tmp2);\n+        }\n@@ -1077,4 +1092,1 @@\n-      \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n-      \/\/ loop contains < 8 instructions that fit inside a single\n-      \/\/ i-cache sector.\n-      __ align(32);\n+        __ li(tmp1, 16);\n@@ -1082,10 +1094,4 @@\n-      __ bind(l_10);\n-      \/\/ Use loop with VSX load\/store instructions to\n-      \/\/ copy 32 elements a time.\n-      __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n-      __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n-      __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src + 16\n-      __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst + 16\n-      __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32\n-      __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32\n-      __ bdnz(l_10);                       \/\/ Dec CTR and loop if not zero.\n+        \/\/ Backbranch target aligned to 32-byte. Not 16-byte align as\n+        \/\/ loop contains < 8 instructions that fit inside a single\n+        \/\/ i-cache sector.\n+        __ align(32);\n@@ -1093,5 +1099,10 @@\n-      \/\/ Restore DSCR pre-fetch value.\n-      if (VM_Version::has_mfdscr()) {\n-        __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-        __ mtdscr(tmp2);\n-      }\n+        __ bind(l_10);\n+        \/\/ Use loop with VSX load\/store instructions to\n+        \/\/ copy 32 elements a time.\n+        __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load src\n+        __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst\n+        __ lxvd2x(tmp_vsr2, tmp1, R3_ARG1);  \/\/ Load src + 16\n+        __ stxvd2x(tmp_vsr2, tmp1, R4_ARG2); \/\/ Store to dst + 16\n+        __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32\n+        __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32\n+        __ bdnz(l_10);                       \/\/ Dec CTR and loop if not zero.\n@@ -1099,2 +1110,5 @@\n-    } \/\/ VSX\n-   } \/\/ FasterArrayCopy\n+        \/\/ Restore DSCR pre-fetch value.\n+        if (VM_Version::has_mfdscr()) {\n+          __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n+          __ mtdscr(tmp2);\n+        }\n@@ -1102,1 +1116,2 @@\n-    __ bind(l_6);\n+      } \/\/ VSX\n+     } \/\/ FasterArrayCopy\n@@ -1104,6 +1119,1 @@\n-    \/\/ copy 4 elements at a time\n-    __ cmpwi(CCR0, R5_ARG3, 4);\n-    __ blt(CCR0, l_1);\n-    __ srdi(tmp1, R5_ARG3, 2);\n-    __ mtctr(tmp1); \/\/ is > 0\n-    __ andi_(R5_ARG3, R5_ARG3, 3);\n+      __ bind(l_6);\n@@ -1111,10 +1121,6 @@\n-    { \/\/ FasterArrayCopy\n-      __ addi(R3_ARG1, R3_ARG1, -4);\n-      __ addi(R4_ARG2, R4_ARG2, -4);\n-      __ bind(l_3);\n-      __ lwzu(tmp2, 4, R3_ARG1);\n-      __ stwu(tmp2, 4, R4_ARG2);\n-      __ bdnz(l_3);\n-      __ addi(R3_ARG1, R3_ARG1, 4);\n-      __ addi(R4_ARG2, R4_ARG2, 4);\n-    }\n+      \/\/ copy 4 elements at a time\n+      __ cmpwi(CCR0, R5_ARG3, 4);\n+      __ blt(CCR0, l_1);\n+      __ srdi(tmp1, R5_ARG3, 2);\n+      __ mtctr(tmp1); \/\/ is > 0\n+      __ andi_(R5_ARG3, R5_ARG3, 3);\n@@ -1122,4 +1128,10 @@\n-    \/\/ do single element copy\n-    __ bind(l_1);\n-    __ cmpwi(CCR0, R5_ARG3, 0);\n-    __ beq(CCR0, l_4);\n+      { \/\/ FasterArrayCopy\n+        __ addi(R3_ARG1, R3_ARG1, -4);\n+        __ addi(R4_ARG2, R4_ARG2, -4);\n+        __ bind(l_3);\n+        __ lwzu(tmp2, 4, R3_ARG1);\n+        __ stwu(tmp2, 4, R4_ARG2);\n+        __ bdnz(l_3);\n+        __ addi(R3_ARG1, R3_ARG1, 4);\n+        __ addi(R4_ARG2, R4_ARG2, 4);\n+      }\n@@ -1127,4 +1139,4 @@\n-    { \/\/ FasterArrayCopy\n-      __ mtctr(R5_ARG3);\n-      __ addi(R3_ARG1, R3_ARG1, -1);\n-      __ addi(R4_ARG2, R4_ARG2, -1);\n+      \/\/ do single element copy\n+      __ bind(l_1);\n+      __ cmpwi(CCR0, R5_ARG3, 0);\n+      __ beq(CCR0, l_4);\n@@ -1132,4 +1144,10 @@\n-      __ bind(l_5);\n-      __ lbzu(tmp2, 1, R3_ARG1);\n-      __ stbu(tmp2, 1, R4_ARG2);\n-      __ bdnz(l_5);\n+      { \/\/ FasterArrayCopy\n+        __ mtctr(R5_ARG3);\n+        __ addi(R3_ARG1, R3_ARG1, -1);\n+        __ addi(R4_ARG2, R4_ARG2, -1);\n+\n+        __ bind(l_5);\n+        __ lbzu(tmp2, 1, R3_ARG1);\n+        __ stbu(tmp2, 1, R4_ARG2);\n+        __ bdnz(l_5);\n+      }\n@@ -1170,9 +1188,11 @@\n-\n-    __ b(l_2);\n-    __ bind(l_1);\n-    __ stbx(tmp1, R4_ARG2, R5_ARG3);\n-    __ bind(l_2);\n-    __ addic_(R5_ARG3, R5_ARG3, -1);\n-    __ lbzx(tmp1, R3_ARG1, R5_ARG3);\n-    __ bge(CCR0, l_1);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      __ b(l_2);\n+      __ bind(l_1);\n+      __ stbx(tmp1, R4_ARG2, R5_ARG3);\n+      __ bind(l_2);\n+      __ addic_(R5_ARG3, R5_ARG3, -1);\n+      __ lbzx(tmp1, R3_ARG1, R5_ARG3);\n+      __ bge(CCR0, l_1);\n+    }\n@@ -1255,0 +1275,7 @@\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      \/\/ don't try anything fancy if arrays don't have many elements\n+      __ li(tmp3, 0);\n+      __ cmpwi(CCR0, R5_ARG3, 9);\n+      __ ble(CCR0, l_6); \/\/ copy 2 at a time\n@@ -1256,9 +1283,4 @@\n-    \/\/ don't try anything fancy if arrays don't have many elements\n-    __ li(tmp3, 0);\n-    __ cmpwi(CCR0, R5_ARG3, 9);\n-    __ ble(CCR0, l_6); \/\/ copy 2 at a time\n-\n-    if (!aligned) {\n-      __ xorr(tmp1, R3_ARG1, R4_ARG2);\n-      __ andi_(tmp1, tmp1, 3);\n-      __ bne(CCR0, l_6); \/\/ if arrays don't have the same alignment mod 4, do 2 element copy\n+      if (!aligned) {\n+        __ xorr(tmp1, R3_ARG1, R4_ARG2);\n+        __ andi_(tmp1, tmp1, 3);\n+        __ bne(CCR0, l_6); \/\/ if arrays don't have the same alignment mod 4, do 2 element copy\n@@ -1266,1 +1288,1 @@\n-      \/\/ At this point it is guaranteed that both, from and to have the same alignment mod 4.\n+        \/\/ At this point it is guaranteed that both, from and to have the same alignment mod 4.\n@@ -1268,3 +1290,3 @@\n-      \/\/ Copy 1 element if necessary to align to 4 bytes.\n-      __ andi_(tmp1, R3_ARG1, 3);\n-      __ beq(CCR0, l_2);\n+        \/\/ Copy 1 element if necessary to align to 4 bytes.\n+        __ andi_(tmp1, R3_ARG1, 3);\n+        __ beq(CCR0, l_2);\n@@ -1272,6 +1294,6 @@\n-      __ lhz(tmp2, 0, R3_ARG1);\n-      __ addi(R3_ARG1, R3_ARG1, 2);\n-      __ sth(tmp2, 0, R4_ARG2);\n-      __ addi(R4_ARG2, R4_ARG2, 2);\n-      __ addi(R5_ARG3, R5_ARG3, -1);\n-      __ bind(l_2);\n+        __ lhz(tmp2, 0, R3_ARG1);\n+        __ addi(R3_ARG1, R3_ARG1, 2);\n+        __ sth(tmp2, 0, R4_ARG2);\n+        __ addi(R4_ARG2, R4_ARG2, 2);\n+        __ addi(R5_ARG3, R5_ARG3, -1);\n+        __ bind(l_2);\n@@ -1279,1 +1301,1 @@\n-      \/\/ At this point the positions of both, from and to, are at least 4 byte aligned.\n+        \/\/ At this point the positions of both, from and to, are at least 4 byte aligned.\n@@ -1281,5 +1303,5 @@\n-      \/\/ Copy 4 elements at a time.\n-      \/\/ Align to 8 bytes, but only if both, from and to, have same alignment mod 8.\n-      __ xorr(tmp2, R3_ARG1, R4_ARG2);\n-      __ andi_(tmp1, tmp2, 7);\n-      __ bne(CCR0, l_7); \/\/ not same alignment mod 8 -> copy 4, either from or to will be unaligned\n+        \/\/ Copy 4 elements at a time.\n+        \/\/ Align to 8 bytes, but only if both, from and to, have same alignment mod 8.\n+        __ xorr(tmp2, R3_ARG1, R4_ARG2);\n+        __ andi_(tmp1, tmp2, 7);\n+        __ bne(CCR0, l_7); \/\/ not same alignment mod 8 -> copy 4, either from or to will be unaligned\n@@ -1287,3 +1309,3 @@\n-      \/\/ Copy a 2-element word if necessary to align to 8 bytes.\n-      __ andi_(R0, R3_ARG1, 7);\n-      __ beq(CCR0, l_7);\n+        \/\/ Copy a 2-element word if necessary to align to 8 bytes.\n+        __ andi_(R0, R3_ARG1, 7);\n+        __ beq(CCR0, l_7);\n@@ -1291,6 +1313,7 @@\n-      __ lwzx(tmp2, R3_ARG1, tmp3);\n-      __ addi(R5_ARG3, R5_ARG3, -2);\n-      __ stwx(tmp2, R4_ARG2, tmp3);\n-      { \/\/ FasterArrayCopy\n-        __ addi(R3_ARG1, R3_ARG1, 4);\n-        __ addi(R4_ARG2, R4_ARG2, 4);\n+        __ lwzx(tmp2, R3_ARG1, tmp3);\n+        __ addi(R5_ARG3, R5_ARG3, -2);\n+        __ stwx(tmp2, R4_ARG2, tmp3);\n+        { \/\/ FasterArrayCopy\n+          __ addi(R3_ARG1, R3_ARG1, 4);\n+          __ addi(R4_ARG2, R4_ARG2, 4);\n+        }\n@@ -1298,16 +1321,0 @@\n-    }\n-\n-    __ bind(l_7);\n-\n-    \/\/ Copy 4 elements at a time; either the loads or the stores can\n-    \/\/ be unaligned if aligned == false.\n-\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 15);\n-      __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 16 elements remain\n-\n-      __ srdi(tmp1, R5_ARG3, 4);\n-      __ andi_(R5_ARG3, R5_ARG3, 15);\n-      __ mtctr(tmp1);\n-\n-      if (!VM_Version::has_vsx()) {\n@@ -1315,15 +1322,1 @@\n-        __ bind(l_8);\n-        \/\/ Use unrolled version for mass copying (copy 16 elements a time).\n-        \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n-        \/\/ Therefore, the following sequence is made for the good of both.\n-        __ ld(tmp1, 0, R3_ARG1);\n-        __ ld(tmp2, 8, R3_ARG1);\n-        __ ld(tmp3, 16, R3_ARG1);\n-        __ ld(tmp4, 24, R3_ARG1);\n-        __ std(tmp1, 0, R4_ARG2);\n-        __ std(tmp2, 8, R4_ARG2);\n-        __ std(tmp3, 16, R4_ARG2);\n-        __ std(tmp4, 24, R4_ARG2);\n-        __ addi(R3_ARG1, R3_ARG1, 32);\n-        __ addi(R4_ARG2, R4_ARG2, 32);\n-        __ bdnz(l_8);\n+      __ bind(l_7);\n@@ -1331,1 +1324,2 @@\n-      } else { \/\/ Processor supports VSX, so use it to mass copy.\n+      \/\/ Copy 4 elements at a time; either the loads or the stores can\n+      \/\/ be unaligned if aligned == false.\n@@ -1333,2 +1327,59 @@\n-        \/\/ Prefetch src data into L2 cache.\n-        __ dcbt(R3_ARG1, 0);\n+      { \/\/ FasterArrayCopy\n+        __ cmpwi(CCR0, R5_ARG3, 15);\n+        __ ble(CCR0, l_6); \/\/ copy 2 at a time if less than 16 elements remain\n+\n+        __ srdi(tmp1, R5_ARG3, 4);\n+        __ andi_(R5_ARG3, R5_ARG3, 15);\n+        __ mtctr(tmp1);\n+\n+        if (!VM_Version::has_vsx()) {\n+\n+          __ bind(l_8);\n+          \/\/ Use unrolled version for mass copying (copy 16 elements a time).\n+          \/\/ Load feeding store gets zero latency on Power6, however not on Power5.\n+          \/\/ Therefore, the following sequence is made for the good of both.\n+          __ ld(tmp1, 0, R3_ARG1);\n+          __ ld(tmp2, 8, R3_ARG1);\n+          __ ld(tmp3, 16, R3_ARG1);\n+          __ ld(tmp4, 24, R3_ARG1);\n+          __ std(tmp1, 0, R4_ARG2);\n+          __ std(tmp2, 8, R4_ARG2);\n+          __ std(tmp3, 16, R4_ARG2);\n+          __ std(tmp4, 24, R4_ARG2);\n+          __ addi(R3_ARG1, R3_ARG1, 32);\n+          __ addi(R4_ARG2, R4_ARG2, 32);\n+          __ bdnz(l_8);\n+\n+        } else { \/\/ Processor supports VSX, so use it to mass copy.\n+\n+          \/\/ Prefetch src data into L2 cache.\n+          __ dcbt(R3_ARG1, 0);\n+\n+          \/\/ If supported set DSCR pre-fetch to deepest.\n+          if (VM_Version::has_mfdscr()) {\n+            __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n+            __ mtdscr(tmp2);\n+          }\n+          __ li(tmp1, 16);\n+\n+          \/\/ Backbranch target aligned to 32-byte. It's not aligned 16-byte\n+          \/\/ as loop contains < 8 instructions that fit inside a single\n+          \/\/ i-cache sector.\n+          __ align(32);\n+\n+          __ bind(l_9);\n+          \/\/ Use loop with VSX load\/store instructions to\n+          \/\/ copy 16 elements a time.\n+          __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load from src.\n+          __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst.\n+          __ lxvd2x(tmp_vsr2, R3_ARG1, tmp1);  \/\/ Load from src + 16.\n+          __ stxvd2x(tmp_vsr2, R4_ARG2, tmp1); \/\/ Store to dst + 16.\n+          __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32.\n+          __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32.\n+          __ bdnz(l_9);                        \/\/ Dec CTR and loop if not zero.\n+\n+          \/\/ Restore DSCR pre-fetch value.\n+          if (VM_Version::has_mfdscr()) {\n+            __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n+            __ mtdscr(tmp2);\n+          }\n@@ -1336,4 +1387,0 @@\n-        \/\/ If supported set DSCR pre-fetch to deepest.\n-        if (VM_Version::has_mfdscr()) {\n-          __ load_const_optimized(tmp2, VM_Version::_dscr_val | 7);\n-          __ mtdscr(tmp2);\n@@ -1341,1 +1388,2 @@\n-        __ li(tmp1, 16);\n+      } \/\/ FasterArrayCopy\n+      __ bind(l_6);\n@@ -1343,4 +1391,6 @@\n-        \/\/ Backbranch target aligned to 32-byte. It's not aligned 16-byte\n-        \/\/ as loop contains < 8 instructions that fit inside a single\n-        \/\/ i-cache sector.\n-        __ align(32);\n+      \/\/ copy 2 elements at a time\n+      { \/\/ FasterArrayCopy\n+        __ cmpwi(CCR0, R5_ARG3, 2);\n+        __ blt(CCR0, l_1);\n+        __ srdi(tmp1, R5_ARG3, 1);\n+        __ andi_(R5_ARG3, R5_ARG3, 1);\n@@ -1348,10 +1398,3 @@\n-        __ bind(l_9);\n-        \/\/ Use loop with VSX load\/store instructions to\n-        \/\/ copy 16 elements a time.\n-        __ lxvd2x(tmp_vsr1, R3_ARG1);        \/\/ Load from src.\n-        __ stxvd2x(tmp_vsr1, R4_ARG2);       \/\/ Store to dst.\n-        __ lxvd2x(tmp_vsr2, R3_ARG1, tmp1);  \/\/ Load from src + 16.\n-        __ stxvd2x(tmp_vsr2, R4_ARG2, tmp1); \/\/ Store to dst + 16.\n-        __ addi(R3_ARG1, R3_ARG1, 32);       \/\/ Update src+=32.\n-        __ addi(R4_ARG2, R4_ARG2, 32);       \/\/ Update dsc+=32.\n-        __ bdnz(l_9);                        \/\/ Dec CTR and loop if not zero.\n+        __ addi(R3_ARG1, R3_ARG1, -4);\n+        __ addi(R4_ARG2, R4_ARG2, -4);\n+        __ mtctr(tmp1);\n@@ -1359,5 +1402,4 @@\n-        \/\/ Restore DSCR pre-fetch value.\n-        if (VM_Version::has_mfdscr()) {\n-          __ load_const_optimized(tmp2, VM_Version::_dscr_val);\n-          __ mtdscr(tmp2);\n-        }\n+        __ bind(l_3);\n+        __ lwzu(tmp2, 4, R3_ARG1);\n+        __ stwu(tmp2, 4, R4_ARG2);\n+        __ bdnz(l_3);\n@@ -1365,0 +1407,2 @@\n+        __ addi(R3_ARG1, R3_ARG1, 4);\n+        __ addi(R4_ARG2, R4_ARG2, 4);\n@@ -1366,13 +1410,0 @@\n-    } \/\/ FasterArrayCopy\n-    __ bind(l_6);\n-\n-    \/\/ copy 2 elements at a time\n-    { \/\/ FasterArrayCopy\n-      __ cmpwi(CCR0, R5_ARG3, 2);\n-      __ blt(CCR0, l_1);\n-      __ srdi(tmp1, R5_ARG3, 1);\n-      __ andi_(R5_ARG3, R5_ARG3, 1);\n-\n-      __ addi(R3_ARG1, R3_ARG1, -4);\n-      __ addi(R4_ARG2, R4_ARG2, -4);\n-      __ mtctr(tmp1);\n@@ -1380,4 +1411,4 @@\n-      __ bind(l_3);\n-      __ lwzu(tmp2, 4, R3_ARG1);\n-      __ stwu(tmp2, 4, R4_ARG2);\n-      __ bdnz(l_3);\n+      \/\/ do single element copy\n+      __ bind(l_1);\n+      __ cmpwi(CCR0, R5_ARG3, 0);\n+      __ beq(CCR0, l_4);\n@@ -1385,2 +1416,10 @@\n-      __ addi(R3_ARG1, R3_ARG1, 4);\n-      __ addi(R4_ARG2, R4_ARG2, 4);\n+      { \/\/ FasterArrayCopy\n+        __ mtctr(R5_ARG3);\n+        __ addi(R3_ARG1, R3_ARG1, -2);\n+        __ addi(R4_ARG2, R4_ARG2, -2);\n+\n+        __ bind(l_5);\n+        __ lhzu(tmp2, 2, R3_ARG1);\n+        __ sthu(tmp2, 2, R4_ARG2);\n+        __ bdnz(l_5);\n+      }\n@@ -1389,15 +1428,0 @@\n-    \/\/ do single element copy\n-    __ bind(l_1);\n-    __ cmpwi(CCR0, R5_ARG3, 0);\n-    __ beq(CCR0, l_4);\n-\n-    { \/\/ FasterArrayCopy\n-      __ mtctr(R5_ARG3);\n-      __ addi(R3_ARG1, R3_ARG1, -2);\n-      __ addi(R4_ARG2, R4_ARG2, -2);\n-\n-      __ bind(l_5);\n-      __ lhzu(tmp2, 2, R3_ARG1);\n-      __ sthu(tmp2, 2, R4_ARG2);\n-      __ bdnz(l_5);\n-    }\n@@ -1435,9 +1459,12 @@\n-    __ sldi(tmp1, R5_ARG3, 1);\n-    __ b(l_2);\n-    __ bind(l_1);\n-    __ sthx(tmp2, R4_ARG2, tmp1);\n-    __ bind(l_2);\n-    __ addic_(tmp1, tmp1, -2);\n-    __ lhzx(tmp2, R3_ARG1, tmp1);\n-    __ bge(CCR0, l_1);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      __ sldi(tmp1, R5_ARG3, 1);\n+      __ b(l_2);\n+      __ bind(l_1);\n+      __ sthx(tmp2, R4_ARG2, tmp1);\n+      __ bind(l_2);\n+      __ addic_(tmp1, tmp1, -2);\n+      __ lhzx(tmp2, R3_ARG1, tmp1);\n+      __ bge(CCR0, l_1);\n+    }\n@@ -1591,1 +1618,5 @@\n-    generate_disjoint_int_copy_core(aligned);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_disjoint_int_copy_core(aligned);\n+    }\n@@ -1739,2 +1770,5 @@\n-\n-    generate_conjoint_int_copy_core(aligned);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_conjoint_int_copy_core(aligned);\n+    }\n@@ -1862,1 +1896,5 @@\n-    generate_disjoint_long_copy_core(aligned);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_disjoint_long_copy_core(aligned);\n+    }\n@@ -1866,1 +1904,1 @@\n-    return start;\n+  return start;\n@@ -1989,2 +2027,5 @@\n-    generate_conjoint_long_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_conjoint_long_copy_core(aligned);\n+    }\n@@ -3011,0 +3052,3 @@\n+    address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();\n+    UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);\n+\n@@ -3582,0 +3626,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 8\n@@ -3583,0 +3628,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/ppc\/stubGenerator_ppc.cpp","additions":319,"deletions":271,"binary":false,"changes":590,"status":"modified"},{"patch":"@@ -1079,0 +1079,11 @@\n+  address generate_unsafecopy_common_error_exit() {\n+    address start_pc = __ pc();\n+    if (UseBlockCopy) {\n+      __ wrasi(G0, Assembler::ASI_PRIMARY_NOFAULT);\n+      __ membar(Assembler::StoreLoad);\n+    }\n+    __ retl();\n+    __ delayed()->mov(G0, O0); \/\/ return 0\n+    return start_pc;\n+  }\n+\n@@ -1110,4 +1121,3 @@\n-    \/\/ for short arrays, just do single element copy\n-    __ cmp(count, 23); \/\/ 16 + 7\n-    __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);\n-    __ delayed()->mov(G0, offset);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n@@ -1115,30 +1125,4 @@\n-    if (aligned) {\n-      \/\/ 'aligned' == true when it is known statically during compilation\n-      \/\/ of this arraycopy call site that both 'from' and 'to' addresses\n-      \/\/ are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).\n-      \/\/\n-      \/\/ Aligned arrays have 4 bytes alignment in 32-bits VM\n-      \/\/ and 8 bytes - in 64-bits VM. So we do it only for 32-bits VM\n-      \/\/\n-    } else {\n-      \/\/ copy bytes to align 'to' on 8 byte boundary\n-      __ andcc(to, 7, G1); \/\/ misaligned bytes\n-      __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n-      __ delayed()->neg(G1);\n-      __ inc(G1, 8);       \/\/ bytes need to copy to next 8-bytes alignment\n-      __ sub(count, G1, count);\n-    __ BIND(L_align);\n-      __ ldub(from, 0, O3);\n-      __ deccc(G1);\n-      __ inc(from);\n-      __ stb(O3, to, 0);\n-      __ br(Assembler::notZero, false, Assembler::pt, L_align);\n-      __ delayed()->inc(to);\n-    __ BIND(L_skip_alignment);\n-    }\n-    if (!aligned) {\n-      \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n-      \/\/ the same alignment mod 8, otherwise fall through to the next\n-      \/\/ code for aligned copy.\n-      \/\/ The compare above (count >= 23) guarantes 'count' >= 16 bytes.\n-      \/\/ Also jump over aligned copy after the copy with shift completed.\n+      \/\/ for short arrays, just do single element copy\n+      __ cmp(count, 23); \/\/ 16 + 7\n+      __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);\n+      __ delayed()->mov(G0, offset);\n@@ -1146,2 +1130,33 @@\n-      copy_16_bytes_forward_with_shift(from, to, count, 0, L_copy_byte);\n-    }\n+      if (aligned) {\n+        \/\/ 'aligned' == true when it is known statically during compilation\n+        \/\/ of this arraycopy call site that both 'from' and 'to' addresses\n+        \/\/ are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).\n+        \/\/\n+        \/\/ Aligned arrays have 4 bytes alignment in 32-bits VM\n+        \/\/ and 8 bytes - in 64-bits VM. So we do it only for 32-bits VM\n+        \/\/\n+      } else {\n+        \/\/ copy bytes to align 'to' on 8 byte boundary\n+        __ andcc(to, 7, G1); \/\/ misaligned bytes\n+        __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n+        __ delayed()->neg(G1);\n+        __ inc(G1, 8);       \/\/ bytes need to copy to next 8-bytes alignment\n+        __ sub(count, G1, count);\n+      __ BIND(L_align);\n+        __ ldub(from, 0, O3);\n+        __ deccc(G1);\n+        __ inc(from);\n+        __ stb(O3, to, 0);\n+        __ br(Assembler::notZero, false, Assembler::pt, L_align);\n+        __ delayed()->inc(to);\n+      __ BIND(L_skip_alignment);\n+      }\n+      if (!aligned) {\n+        \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n+        \/\/ the same alignment mod 8, otherwise fall through to the next\n+        \/\/ code for aligned copy.\n+        \/\/ The compare above (count >= 23) guarantes 'count' >= 16 bytes.\n+        \/\/ Also jump over aligned copy after the copy with shift completed.\n+\n+        copy_16_bytes_forward_with_shift(from, to, count, 0, L_copy_byte);\n+      }\n@@ -1149,1 +1164,1 @@\n-    \/\/ Both array are 8 bytes aligned, copy 16 bytes at a time\n+      \/\/ Both array are 8 bytes aligned, copy 16 bytes at a time\n@@ -1152,1 +1167,1 @@\n-     generate_disjoint_long_copy_core(aligned);\n+      generate_disjoint_long_copy_core(aligned);\n@@ -1155,10 +1170,11 @@\n-    \/\/ copy tailing bytes\n-    __ BIND(L_copy_byte);\n-      __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_byte_loop);\n-      __ ldub(from, offset, O3);\n-      __ deccc(count);\n-      __ stb(O3, to, offset);\n-      __ brx(Assembler::notZero, false, Assembler::pt, L_copy_byte_loop);\n-      __ delayed()->inc(offset);\n+      \/\/ copy tailing bytes\n+      __ BIND(L_copy_byte);\n+        __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_copy_byte_loop);\n+        __ ldub(from, offset, O3);\n+        __ deccc(count);\n+        __ stb(O3, to, offset);\n+        __ brx(Assembler::notZero, false, Assembler::pt, L_copy_byte_loop);\n+        __ delayed()->inc(offset);\n+    }\n@@ -1210,1 +1226,3 @@\n-    __ add(to, count, end_to);       \/\/ offset after last copied element\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n@@ -1212,4 +1230,1 @@\n-    \/\/ for short arrays, just do single element copy\n-    __ cmp(count, 23); \/\/ 16 + 7\n-    __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);\n-    __ delayed()->add(from, count, end_from);\n+      __ add(to, count, end_to);       \/\/ offset after last copied element\n@@ -1217,3 +1232,4 @@\n-    {\n-      \/\/ Align end of arrays since they could be not aligned even\n-      \/\/ when arrays itself are aligned.\n+      \/\/ for short arrays, just do single element copy\n+      __ cmp(count, 23); \/\/ 16 + 7\n+      __ brx(Assembler::less, false, Assembler::pn, L_copy_byte);\n+      __ delayed()->add(from, count, end_from);\n@@ -1221,25 +1237,3 @@\n-      \/\/ copy bytes to align 'end_to' on 8 byte boundary\n-      __ andcc(end_to, 7, G1); \/\/ misaligned bytes\n-      __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n-      __ delayed()->nop();\n-      __ sub(count, G1, count);\n-    __ BIND(L_align);\n-      __ dec(end_from);\n-      __ dec(end_to);\n-      __ ldub(end_from, 0, O3);\n-      __ deccc(G1);\n-      __ brx(Assembler::notZero, false, Assembler::pt, L_align);\n-      __ delayed()->stb(O3, end_to, 0);\n-    __ BIND(L_skip_alignment);\n-    }\n-    if (aligned) {\n-      \/\/ Both arrays are aligned to 8-bytes in 64-bits VM.\n-      \/\/ The 'count' is decremented in copy_16_bytes_backward_with_shift()\n-      \/\/ in unaligned case.\n-      __ dec(count, 16);\n-    } else {\n-      \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n-      \/\/ the same alignment mod 8, otherwise jump to the next\n-      \/\/ code for aligned copy (and substracting 16 from 'count' before jump).\n-      \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n-      \/\/ Also jump over aligned copy after the copy with shift completed.\n+      {\n+        \/\/ Align end of arrays since they could be not aligned even\n+        \/\/ when arrays itself are aligned.\n@@ -1247,2 +1241,53 @@\n-      copy_16_bytes_backward_with_shift(end_from, end_to, count, 16,\n-                                        L_aligned_copy, L_copy_byte);\n+        \/\/ copy bytes to align 'end_to' on 8 byte boundary\n+        __ andcc(end_to, 7, G1); \/\/ misaligned bytes\n+        __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n+        __ delayed()->nop();\n+        __ sub(count, G1, count);\n+      __ BIND(L_align);\n+        __ dec(end_from);\n+        __ dec(end_to);\n+        __ ldub(end_from, 0, O3);\n+        __ deccc(G1);\n+        __ brx(Assembler::notZero, false, Assembler::pt, L_align);\n+        __ delayed()->stb(O3, end_to, 0);\n+      __ BIND(L_skip_alignment);\n+      }\n+      if (aligned) {\n+        \/\/ Both arrays are aligned to 8-bytes in 64-bits VM.\n+        \/\/ The 'count' is decremented in copy_16_bytes_backward_with_shift()\n+        \/\/ in unaligned case.\n+        __ dec(count, 16);\n+      } else {\n+        \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n+        \/\/ the same alignment mod 8, otherwise jump to the next\n+        \/\/ code for aligned copy (and substracting 16 from 'count' before jump).\n+        \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n+        \/\/ Also jump over aligned copy after the copy with shift completed.\n+\n+       copy_16_bytes_backward_with_shift(end_from, end_to, count, 16,\n+                                          L_aligned_copy, L_copy_byte);\n+      }\n+      \/\/ copy 4 elements (16 bytes) at a time\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_aligned_copy);\n+        __ dec(end_from, 16);\n+        __ ldx(end_from, 8, O3);\n+        __ ldx(end_from, 0, O4);\n+        __ dec(end_to, 16);\n+        __ deccc(count, 16);\n+        __ stx(O3, end_to, 8);\n+        __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);\n+        __ delayed()->stx(O4, end_to, 0);\n+        __ inc(count, 16);\n+\n+      \/\/ copy 1 element (2 bytes) at a time\n+      __ BIND(L_copy_byte);\n+        __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_copy_byte_loop);\n+        __ dec(end_from);\n+        __ dec(end_to);\n+        __ ldub(end_from, 0, O4);\n+        __ deccc(count);\n+        __ brx(Assembler::greater, false, Assembler::pt, L_copy_byte_loop);\n+        __ delayed()->stb(O4, end_to, 0);\n@@ -1250,24 +1295,0 @@\n-    \/\/ copy 4 elements (16 bytes) at a time\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_aligned_copy);\n-      __ dec(end_from, 16);\n-      __ ldx(end_from, 8, O3);\n-      __ ldx(end_from, 0, O4);\n-      __ dec(end_to, 16);\n-      __ deccc(count, 16);\n-      __ stx(O3, end_to, 8);\n-      __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);\n-      __ delayed()->stx(O4, end_to, 0);\n-      __ inc(count, 16);\n-\n-    \/\/ copy 1 element (2 bytes) at a time\n-    __ BIND(L_copy_byte);\n-      __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_byte_loop);\n-      __ dec(end_from);\n-      __ dec(end_to);\n-      __ ldub(end_from, 0, O4);\n-      __ deccc(count);\n-      __ brx(Assembler::greater, false, Assembler::pt, L_copy_byte_loop);\n-      __ delayed()->stb(O4, end_to, 0);\n@@ -1314,4 +1335,7 @@\n-    \/\/ for short arrays, just do single element copy\n-    __ cmp(count, 11); \/\/ 8 + 3  (22 bytes)\n-    __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);\n-    __ delayed()->mov(G0, offset);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      \/\/ for short arrays, just do single element copy\n+      __ cmp(count, 11); \/\/ 8 + 3  (22 bytes)\n+      __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);\n+      __ delayed()->mov(G0, offset);\n@@ -1319,18 +1343,40 @@\n-    if (aligned) {\n-      \/\/ 'aligned' == true when it is known statically during compilation\n-      \/\/ of this arraycopy call site that both 'from' and 'to' addresses\n-      \/\/ are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).\n-      \/\/\n-      \/\/ Aligned arrays have 4 bytes alignment in 32-bits VM\n-      \/\/ and 8 bytes - in 64-bits VM.\n-      \/\/\n-    } else {\n-      \/\/ copy 1 element if necessary to align 'to' on an 4 bytes\n-      __ andcc(to, 3, G0);\n-      __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n-      __ delayed()->lduh(from, 0, O3);\n-      __ inc(from, 2);\n-      __ inc(to, 2);\n-      __ dec(count);\n-      __ sth(O3, to, -2);\n-    __ BIND(L_skip_alignment);\n+      if (aligned) {\n+        \/\/ 'aligned' == true when it is known statically during compilation\n+        \/\/ of this arraycopy call site that both 'from' and 'to' addresses\n+        \/\/ are HeapWordSize aligned (see LibraryCallKit::basictype2arraycopy()).\n+        \/\/\n+        \/\/ Aligned arrays have 4 bytes alignment in 32-bits VM\n+        \/\/ and 8 bytes - in 64-bits VM.\n+        \/\/\n+      } else {\n+        \/\/ copy 1 element if necessary to align 'to' on an 4 bytes\n+        __ andcc(to, 3, G0);\n+        __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n+        __ delayed()->lduh(from, 0, O3);\n+        __ inc(from, 2);\n+        __ inc(to, 2);\n+        __ dec(count);\n+        __ sth(O3, to, -2);\n+      __ BIND(L_skip_alignment);\n+\n+        \/\/ copy 2 elements to align 'to' on an 8 byte boundary\n+        __ andcc(to, 7, G0);\n+        __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);\n+        __ delayed()->lduh(from, 0, O3);\n+        __ dec(count, 2);\n+        __ lduh(from, 2, O4);\n+        __ inc(from, 4);\n+        __ inc(to, 4);\n+        __ sth(O3, to, -4);\n+        __ sth(O4, to, -2);\n+      __ BIND(L_skip_alignment2);\n+      }\n+      if (!aligned) {\n+        \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n+        \/\/ the same alignment mod 8, otherwise fall through to the next\n+        \/\/ code for aligned copy.\n+        \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n+        \/\/ Also jump over aligned copy after the copy with shift completed.\n+\n+        copy_16_bytes_forward_with_shift(from, to, count, 1, L_copy_2_bytes);\n+      }\n@@ -1338,18 +1384,5 @@\n-      \/\/ copy 2 elements to align 'to' on an 8 byte boundary\n-      __ andcc(to, 7, G0);\n-      __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);\n-      __ delayed()->lduh(from, 0, O3);\n-      __ dec(count, 2);\n-      __ lduh(from, 2, O4);\n-      __ inc(from, 4);\n-      __ inc(to, 4);\n-      __ sth(O3, to, -4);\n-      __ sth(O4, to, -2);\n-    __ BIND(L_skip_alignment2);\n-    }\n-    if (!aligned) {\n-      \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n-      \/\/ the same alignment mod 8, otherwise fall through to the next\n-      \/\/ code for aligned copy.\n-      \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n-      \/\/ Also jump over aligned copy after the copy with shift completed.\n+      \/\/ Both array are 8 bytes aligned, copy 16 bytes at a time\n+        __ and3(count, 3, G4); \/\/ Save\n+        __ srl(count, 2, count);\n+       generate_disjoint_long_copy_core(aligned);\n+        __ mov(G4, count); \/\/ restore\n@@ -1357,1 +1390,10 @@\n-      copy_16_bytes_forward_with_shift(from, to, count, 1, L_copy_2_bytes);\n+      \/\/ copy 1 element at a time\n+      __ BIND(L_copy_2_bytes);\n+        __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_copy_2_bytes_loop);\n+        __ lduh(from, offset, O3);\n+        __ deccc(count);\n+        __ sth(O3, to, offset);\n+        __ brx(Assembler::notZero, false, Assembler::pt, L_copy_2_bytes_loop);\n+        __ delayed()->inc(offset, 2);\n@@ -1360,17 +1402,0 @@\n-    \/\/ Both array are 8 bytes aligned, copy 16 bytes at a time\n-      __ and3(count, 3, G4); \/\/ Save\n-      __ srl(count, 2, count);\n-     generate_disjoint_long_copy_core(aligned);\n-      __ mov(G4, count); \/\/ restore\n-\n-    \/\/ copy 1 element at a time\n-    __ BIND(L_copy_2_bytes);\n-      __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_2_bytes_loop);\n-      __ lduh(from, offset, O3);\n-      __ deccc(count);\n-      __ sth(O3, to, offset);\n-      __ brx(Assembler::notZero, false, Assembler::pt, L_copy_2_bytes_loop);\n-      __ delayed()->inc(offset, 2);\n-\n@@ -1642,8 +1667,0 @@\n-    __ sllx(count, LogBytesPerShort, byte_count);\n-    __ add(to, byte_count, end_to);  \/\/ offset after last copied element\n-\n-    \/\/ for short arrays, just do single element copy\n-    __ cmp(count, 11); \/\/ 8 + 3  (22 bytes)\n-    __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);\n-    __ delayed()->add(from, byte_count, end_from);\n-\n@@ -1651,2 +1668,2 @@\n-      \/\/ Align end of arrays since they could be not aligned even\n-      \/\/ when arrays itself are aligned.\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n@@ -1654,9 +1671,2 @@\n-      \/\/ copy 1 element if necessary to align 'end_to' on an 4 bytes\n-      __ andcc(end_to, 3, G0);\n-      __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n-      __ delayed()->lduh(end_from, -2, O3);\n-      __ dec(end_from, 2);\n-      __ dec(end_to, 2);\n-      __ dec(count);\n-      __ sth(O3, end_to, 0);\n-    __ BIND(L_skip_alignment);\n+      __ sllx(count, LogBytesPerShort, byte_count);\n+      __ add(to, byte_count, end_to);  \/\/ offset after last copied element\n@@ -1664,23 +1674,42 @@\n-      \/\/ copy 2 elements to align 'end_to' on an 8 byte boundary\n-      __ andcc(end_to, 7, G0);\n-      __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);\n-      __ delayed()->lduh(end_from, -2, O3);\n-      __ dec(count, 2);\n-      __ lduh(end_from, -4, O4);\n-      __ dec(end_from, 4);\n-      __ dec(end_to, 4);\n-      __ sth(O3, end_to, 2);\n-      __ sth(O4, end_to, 0);\n-    __ BIND(L_skip_alignment2);\n-    }\n-    if (aligned) {\n-      \/\/ Both arrays are aligned to 8-bytes in 64-bits VM.\n-      \/\/ The 'count' is decremented in copy_16_bytes_backward_with_shift()\n-      \/\/ in unaligned case.\n-      __ dec(count, 8);\n-    } else {\n-      \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n-      \/\/ the same alignment mod 8, otherwise jump to the next\n-      \/\/ code for aligned copy (and substracting 8 from 'count' before jump).\n-      \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n-      \/\/ Also jump over aligned copy after the copy with shift completed.\n+      \/\/ for short arrays, just do single element copy\n+      __ cmp(count, 11); \/\/ 8 + 3  (22 bytes)\n+      __ brx(Assembler::less, false, Assembler::pn, L_copy_2_bytes);\n+      __ delayed()->add(from, byte_count, end_from);\n+\n+      {\n+        \/\/ Align end of arrays since they could be not aligned even\n+        \/\/ when arrays itself are aligned.\n+\n+        \/\/ copy 1 element if necessary to align 'end_to' on an 4 bytes\n+        __ andcc(end_to, 3, G0);\n+        __ br(Assembler::zero, false, Assembler::pt, L_skip_alignment);\n+        __ delayed()->lduh(end_from, -2, O3);\n+        __ dec(end_from, 2);\n+        __ dec(end_to, 2);\n+        __ dec(count);\n+        __ sth(O3, end_to, 0);\n+      __ BIND(L_skip_alignment);\n+\n+        \/\/ copy 2 elements to align 'end_to' on an 8 byte boundary\n+        __ andcc(end_to, 7, G0);\n+        __ br(Assembler::zero, false, Assembler::pn, L_skip_alignment2);\n+        __ delayed()->lduh(end_from, -2, O3);\n+        __ dec(count, 2);\n+        __ lduh(end_from, -4, O4);\n+        __ dec(end_from, 4);\n+        __ dec(end_to, 4);\n+        __ sth(O3, end_to, 2);\n+        __ sth(O4, end_to, 0);\n+      __ BIND(L_skip_alignment2);\n+      }\n+      if (aligned) {\n+        \/\/ Both arrays are aligned to 8-bytes in 64-bits VM.\n+        \/\/ The 'count' is decremented in copy_16_bytes_backward_with_shift()\n+        \/\/ in unaligned case.\n+        __ dec(count, 8);\n+      } else {\n+        \/\/ Copy with shift 16 bytes per iteration if arrays do not have\n+        \/\/ the same alignment mod 8, otherwise jump to the next\n+        \/\/ code for aligned copy (and substracting 8 from 'count' before jump).\n+        \/\/ The compare above (count >= 11) guarantes 'count' >= 16 bytes.\n+        \/\/ Also jump over aligned copy after the copy with shift completed.\n@@ -1688,1 +1717,1 @@\n-      copy_16_bytes_backward_with_shift(end_from, end_to, count, 8,\n+        copy_16_bytes_backward_with_shift(end_from, end_to, count, 8,\n@@ -1690,0 +1719,24 @@\n+      }\n+      \/\/ copy 4 elements (16 bytes) at a time\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_aligned_copy);\n+        __ dec(end_from, 16);\n+        __ ldx(end_from, 8, O3);\n+        __ ldx(end_from, 0, O4);\n+        __ dec(end_to, 16);\n+        __ deccc(count, 8);\n+        __ stx(O3, end_to, 8);\n+        __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);\n+        __ delayed()->stx(O4, end_to, 0);\n+        __ inc(count, 8);\n+\n+      \/\/ copy 1 element (2 bytes) at a time\n+      __ BIND(L_copy_2_bytes);\n+        __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n+      __ BIND(L_copy_2_bytes_loop);\n+        __ dec(end_from, 2);\n+        __ dec(end_to, 2);\n+        __ lduh(end_from, 0, O4);\n+        __ deccc(count);\n+        __ brx(Assembler::greater, false, Assembler::pt, L_copy_2_bytes_loop);\n+        __ delayed()->sth(O4, end_to, 0);\n@@ -1691,24 +1744,0 @@\n-    \/\/ copy 4 elements (16 bytes) at a time\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_aligned_copy);\n-      __ dec(end_from, 16);\n-      __ ldx(end_from, 8, O3);\n-      __ ldx(end_from, 0, O4);\n-      __ dec(end_to, 16);\n-      __ deccc(count, 8);\n-      __ stx(O3, end_to, 8);\n-      __ brx(Assembler::greaterEqual, false, Assembler::pt, L_aligned_copy);\n-      __ delayed()->stx(O4, end_to, 0);\n-      __ inc(count, 8);\n-\n-    \/\/ copy 1 element (2 bytes) at a time\n-    __ BIND(L_copy_2_bytes);\n-      __ cmp_and_br_short(count, 0, Assembler::equal, Assembler::pt, L_exit);\n-    __ BIND(L_copy_2_bytes_loop);\n-      __ dec(end_from, 2);\n-      __ dec(end_to, 2);\n-      __ lduh(end_from, 0, O4);\n-      __ deccc(count);\n-      __ brx(Assembler::greater, false, Assembler::pt, L_copy_2_bytes_loop);\n-      __ delayed()->sth(O4, end_to, 0);\n-\n@@ -1873,3 +1902,5 @@\n-\n-    generate_disjoint_int_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_disjoint_int_copy_core(aligned);\n+    }\n@@ -2008,3 +2039,5 @@\n-\n-    generate_conjoint_int_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false);\n+      generate_conjoint_int_copy_core(aligned);\n+    }\n@@ -2159,2 +2192,5 @@\n-    generate_disjoint_long_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, true, false);\n+      generate_disjoint_long_copy_core(aligned);\n+    }\n@@ -2235,3 +2271,5 @@\n-\n-    generate_conjoint_long_copy_core(aligned);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue at UnsafeCopyMemory common_error_exit\n+      UnsafeCopyMemoryMark ucmm(this, true, false);\n+      generate_conjoint_long_copy_core(aligned);\n+    }\n@@ -2932,0 +2970,3 @@\n+    address ucm_common_error_exit       =  generate_unsafecopy_common_error_exit();\n+    UnsafeCopyMemory::set_common_exit_stub_pc(ucm_common_error_exit);\n+\n@@ -5824,0 +5865,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 8\n@@ -5825,0 +5867,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/sparc\/stubGenerator_sparc.cpp","additions":288,"deletions":243,"binary":false,"changes":531,"status":"modified"},{"patch":"@@ -792,0 +792,2 @@\n+    case 0x6F: \/\/ movdq\n+    case 0x7F: \/\/ movdq\n@@ -4277,0 +4279,1 @@\n+\n","filename":"src\/hotspot\/cpu\/x86\/assembler_x86.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -892,42 +892,27 @@\n-\n-    __ subptr(to, from); \/\/ to --> to_from\n-    __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n-    __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n-    if (!UseUnalignedLoadStores && !aligned && (t == T_BYTE || t == T_SHORT)) {\n-      \/\/ align source address at 4 bytes address boundary\n-      if (t == T_BYTE) {\n-        \/\/ One byte misalignment happens only for byte arrays\n-        __ testl(from, 1);\n-        __ jccb(Assembler::zero, L_skip_align1);\n-        __ movb(rax, Address(from, 0));\n-        __ movb(Address(from, to_from, Address::times_1, 0), rax);\n-        __ increment(from);\n-        __ decrement(count);\n-      __ BIND(L_skip_align1);\n-      }\n-      \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n-      __ testl(from, 2);\n-      __ jccb(Assembler::zero, L_skip_align2);\n-      __ movw(rax, Address(from, 0));\n-      __ movw(Address(from, to_from, Address::times_1, 0), rax);\n-      __ addptr(from, 2);\n-      __ subl(count, 1<<(shift-1));\n-    __ BIND(L_skip_align2);\n-    }\n-    if (!VM_Version::supports_mmx()) {\n-      __ mov(rax, count);      \/\/ save 'count'\n-      __ shrl(count, shift); \/\/ bytes count\n-      __ addptr(to_from, from);\/\/ restore 'to'\n-      __ rep_mov();\n-      __ subptr(to_from, from);\/\/ restore 'to_from'\n-      __ mov(count, rax);      \/\/ restore 'count'\n-      __ jmpb(L_copy_2_bytes); \/\/ all dwords were copied\n-    } else {\n-      if (!UseUnalignedLoadStores) {\n-        \/\/ align to 8 bytes, we know we are 4 byte aligned to start\n-        __ testptr(from, 4);\n-        __ jccb(Assembler::zero, L_copy_64_bytes);\n-        __ movl(rax, Address(from, 0));\n-        __ movl(Address(from, to_from, Address::times_1, 0), rax);\n-        __ addptr(from, 4);\n-        __ subl(count, 1<<shift);\n+    {\n+      bool add_entry = (t != T_OBJECT && (!aligned || t == T_INT));\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      __ subptr(to, from); \/\/ to --> to_from\n+      __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n+      __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n+      if (!UseUnalignedLoadStores && !aligned && (t == T_BYTE || t == T_SHORT)) {\n+        \/\/ align source address at 4 bytes address boundary\n+        if (t == T_BYTE) {\n+          \/\/ One byte misalignment happens only for byte arrays\n+          __ testl(from, 1);\n+          __ jccb(Assembler::zero, L_skip_align1);\n+          __ movb(rax, Address(from, 0));\n+          __ movb(Address(from, to_from, Address::times_1, 0), rax);\n+          __ increment(from);\n+          __ decrement(count);\n+        __ BIND(L_skip_align1);\n+        }\n+        \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n+        __ testl(from, 2);\n+        __ jccb(Assembler::zero, L_skip_align2);\n+        __ movw(rax, Address(from, 0));\n+        __ movw(Address(from, to_from, Address::times_1, 0), rax);\n+        __ addptr(from, 2);\n+        __ subl(count, 1<<(shift-1));\n+      __ BIND(L_skip_align2);\n@@ -935,8 +920,8 @@\n-    __ BIND(L_copy_64_bytes);\n-      __ mov(rax, count);\n-      __ shrl(rax, shift+1);  \/\/ 8 bytes chunk count\n-      \/\/\n-      \/\/ Copy 8-byte chunks through MMX registers, 8 per iteration of the loop\n-      \/\/\n-      if (UseXMMForArrayCopy) {\n-        xmm_copy_forward(from, to_from, rax);\n+      if (!VM_Version::supports_mmx()) {\n+        __ mov(rax, count);      \/\/ save 'count'\n+        __ shrl(count, shift); \/\/ bytes count\n+        __ addptr(to_from, from);\/\/ restore 'to'\n+        __ rep_mov();\n+        __ subptr(to_from, from);\/\/ restore 'to_from'\n+        __ mov(count, rax);      \/\/ restore 'count'\n+        __ jmpb(L_copy_2_bytes); \/\/ all dwords were copied\n@@ -944,1 +929,20 @@\n-        mmx_copy_forward(from, to_from, rax);\n+        if (!UseUnalignedLoadStores) {\n+          \/\/ align to 8 bytes, we know we are 4 byte aligned to start\n+          __ testptr(from, 4);\n+          __ jccb(Assembler::zero, L_copy_64_bytes);\n+          __ movl(rax, Address(from, 0));\n+          __ movl(Address(from, to_from, Address::times_1, 0), rax);\n+          __ addptr(from, 4);\n+          __ subl(count, 1<<shift);\n+         }\n+      __ BIND(L_copy_64_bytes);\n+        __ mov(rax, count);\n+        __ shrl(rax, shift+1);  \/\/ 8 bytes chunk count\n+        \/\/\n+        \/\/ Copy 8-byte chunks through MMX registers, 8 per iteration of the loop\n+        \/\/\n+        if (UseXMMForArrayCopy) {\n+          xmm_copy_forward(from, to_from, rax);\n+        } else {\n+          mmx_copy_forward(from, to_from, rax);\n+        }\n@@ -946,24 +950,26 @@\n-    }\n-    \/\/ copy tailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(count, 1<<shift);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rax, Address(from, 0));\n-    __ movl(Address(from, to_from, Address::times_1, 0), rax);\n-    if (t == T_BYTE || t == T_SHORT) {\n-      __ addptr(from, 4);\n-    __ BIND(L_copy_2_bytes);\n-      \/\/ copy tailing word\n-      __ testl(count, 1<<(shift-1));\n-      __ jccb(Assembler::zero, L_copy_byte);\n-      __ movw(rax, Address(from, 0));\n-      __ movw(Address(from, to_from, Address::times_1, 0), rax);\n-      if (t == T_BYTE) {\n-        __ addptr(from, 2);\n-      __ BIND(L_copy_byte);\n-        \/\/ copy tailing byte\n-        __ testl(count, 1);\n-        __ jccb(Assembler::zero, L_exit);\n-        __ movb(rax, Address(from, 0));\n-        __ movb(Address(from, to_from, Address::times_1, 0), rax);\n-      __ BIND(L_exit);\n+      \/\/ copy tailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(count, 1<<shift);\n+      __ jccb(Assembler::zero, L_copy_2_bytes);\n+      __ movl(rax, Address(from, 0));\n+      __ movl(Address(from, to_from, Address::times_1, 0), rax);\n+      if (t == T_BYTE || t == T_SHORT) {\n+        __ addptr(from, 4);\n+      __ BIND(L_copy_2_bytes);\n+        \/\/ copy tailing word\n+        __ testl(count, 1<<(shift-1));\n+        __ jccb(Assembler::zero, L_copy_byte);\n+        __ movw(rax, Address(from, 0));\n+        __ movw(Address(from, to_from, Address::times_1, 0), rax);\n+        if (t == T_BYTE) {\n+          __ addptr(from, 2);\n+        __ BIND(L_copy_byte);\n+          \/\/ copy tailing byte\n+          __ testl(count, 1);\n+          __ jccb(Assembler::zero, L_exit);\n+          __ movb(rax, Address(from, 0));\n+          __ movb(Address(from, to_from, Address::times_1, 0), rax);\n+        __ BIND(L_exit);\n+        } else {\n+        __ BIND(L_copy_byte);\n+        }\n@@ -971,1 +977,1 @@\n-      __ BIND(L_copy_byte);\n+      __ BIND(L_copy_2_bytes);\n@@ -973,2 +979,0 @@\n-    } else {\n-    __ BIND(L_copy_2_bytes);\n@@ -977,0 +981,3 @@\n+    if (VM_Version::supports_mmx() && !UseXMMForArrayCopy) {\n+      __ emms();\n+    }\n@@ -1082,22 +1089,5 @@\n-    \/\/ copy from high to low\n-    __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n-    __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n-    if (t == T_BYTE || t == T_SHORT) {\n-      \/\/ Align the end of destination array at 4 bytes address boundary\n-      __ lea(end, Address(dst, count, sf, 0));\n-      if (t == T_BYTE) {\n-        \/\/ One byte misalignment happens only for byte arrays\n-        __ testl(end, 1);\n-        __ jccb(Assembler::zero, L_skip_align1);\n-        __ decrement(count);\n-        __ movb(rdx, Address(from, count, sf, 0));\n-        __ movb(Address(to, count, sf, 0), rdx);\n-      __ BIND(L_skip_align1);\n-      }\n-      \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n-      __ testl(end, 2);\n-      __ jccb(Assembler::zero, L_skip_align2);\n-      __ subptr(count, 1<<(shift-1));\n-      __ movw(rdx, Address(from, count, sf, 0));\n-      __ movw(Address(to, count, sf, 0), rdx);\n-    __ BIND(L_skip_align2);\n+    {\n+      bool add_entry = (t != T_OBJECT && (!aligned || t == T_INT));\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, add_entry, true);\n+      \/\/ copy from high to low\n@@ -1105,25 +1095,23 @@\n-      __ jcc(Assembler::below, L_copy_4_bytes);\n-    }\n-\n-    if (!VM_Version::supports_mmx()) {\n-      __ std();\n-      __ mov(rax, count); \/\/ Save 'count'\n-      __ mov(rdx, to);    \/\/ Save 'to'\n-      __ lea(rsi, Address(from, count, sf, -4));\n-      __ lea(rdi, Address(to  , count, sf, -4));\n-      __ shrptr(count, shift); \/\/ bytes count\n-      __ rep_mov();\n-      __ cld();\n-      __ mov(count, rax); \/\/ restore 'count'\n-      __ andl(count, (1<<shift)-1);      \/\/ mask the number of rest elements\n-      __ movptr(from, Address(rsp, 12+4)); \/\/ reread 'from'\n-      __ mov(to, rdx);   \/\/ restore 'to'\n-      __ jmpb(L_copy_2_bytes); \/\/ all dword were copied\n-   } else {\n-      \/\/ Align to 8 bytes the end of array. It is aligned to 4 bytes already.\n-      __ testptr(end, 4);\n-      __ jccb(Assembler::zero, L_copy_8_bytes);\n-      __ subl(count, 1<<shift);\n-      __ movl(rdx, Address(from, count, sf, 0));\n-      __ movl(Address(to, count, sf, 0), rdx);\n-      __ jmpb(L_copy_8_bytes);\n+      __ jcc(Assembler::below, L_copy_4_bytes); \/\/ use unsigned cmp\n+      if (t == T_BYTE || t == T_SHORT) {\n+        \/\/ Align the end of destination array at 4 bytes address boundary\n+        __ lea(end, Address(dst, count, sf, 0));\n+        if (t == T_BYTE) {\n+          \/\/ One byte misalignment happens only for byte arrays\n+          __ testl(end, 1);\n+          __ jccb(Assembler::zero, L_skip_align1);\n+          __ decrement(count);\n+          __ movb(rdx, Address(from, count, sf, 0));\n+          __ movb(Address(to, count, sf, 0), rdx);\n+        __ BIND(L_skip_align1);\n+        }\n+        \/\/ Two bytes misalignment happens only for byte and short (char) arrays\n+        __ testl(end, 2);\n+        __ jccb(Assembler::zero, L_skip_align2);\n+        __ subptr(count, 1<<(shift-1));\n+        __ movw(rdx, Address(from, count, sf, 0));\n+        __ movw(Address(to, count, sf, 0), rdx);\n+      __ BIND(L_skip_align2);\n+        __ cmpl(count, 2<<shift); \/\/ Short arrays (< 8 bytes) copy by element\n+        __ jcc(Assembler::below, L_copy_4_bytes);\n+      }\n@@ -1131,6 +1119,14 @@\n-      __ align(OptoLoopAlignment);\n-      \/\/ Move 8 bytes\n-    __ BIND(L_copy_8_bytes_loop);\n-      if (UseXMMForArrayCopy) {\n-        __ movq(xmm0, Address(from, count, sf, 0));\n-        __ movq(Address(to, count, sf, 0), xmm0);\n+      if (!VM_Version::supports_mmx()) {\n+        __ std();\n+        __ mov(rax, count); \/\/ Save 'count'\n+        __ mov(rdx, to);    \/\/ Save 'to'\n+        __ lea(rsi, Address(from, count, sf, -4));\n+        __ lea(rdi, Address(to  , count, sf, -4));\n+        __ shrptr(count, shift); \/\/ bytes count\n+        __ rep_mov();\n+        __ cld();\n+        __ mov(count, rax); \/\/ restore 'count'\n+        __ andl(count, (1<<shift)-1);      \/\/ mask the number of rest elements\n+        __ movptr(from, Address(rsp, 12+4)); \/\/ reread 'from'\n+        __ mov(to, rdx);   \/\/ restore 'to'\n+        __ jmpb(L_copy_2_bytes); \/\/ all dword were copied\n@@ -1138,2 +1134,25 @@\n-        __ movq(mmx0, Address(from, count, sf, 0));\n-        __ movq(Address(to, count, sf, 0), mmx0);\n+        \/\/ Align to 8 bytes the end of array. It is aligned to 4 bytes already.\n+        __ testptr(end, 4);\n+        __ jccb(Assembler::zero, L_copy_8_bytes);\n+        __ subl(count, 1<<shift);\n+        __ movl(rdx, Address(from, count, sf, 0));\n+        __ movl(Address(to, count, sf, 0), rdx);\n+        __ jmpb(L_copy_8_bytes);\n+\n+        __ align(OptoLoopAlignment);\n+        \/\/ Move 8 bytes\n+      __ BIND(L_copy_8_bytes_loop);\n+        if (UseXMMForArrayCopy) {\n+          __ movq(xmm0, Address(from, count, sf, 0));\n+          __ movq(Address(to, count, sf, 0), xmm0);\n+        } else {\n+          __ movq(mmx0, Address(from, count, sf, 0));\n+          __ movq(Address(to, count, sf, 0), mmx0);\n+        }\n+      __ BIND(L_copy_8_bytes);\n+        __ subl(count, 2<<shift);\n+        __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n+        __ addl(count, 2<<shift);\n+        if (!UseXMMForArrayCopy) {\n+          __ emms();\n+        }\n@@ -1141,6 +1160,29 @@\n-    __ BIND(L_copy_8_bytes);\n-      __ subl(count, 2<<shift);\n-      __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n-      __ addl(count, 2<<shift);\n-      if (!UseXMMForArrayCopy) {\n-        __ emms();\n+    __ BIND(L_copy_4_bytes);\n+      \/\/ copy prefix qword\n+      __ testl(count, 1<<shift);\n+      __ jccb(Assembler::zero, L_copy_2_bytes);\n+      __ movl(rdx, Address(from, count, sf, -4));\n+      __ movl(Address(to, count, sf, -4), rdx);\n+\n+      if (t == T_BYTE || t == T_SHORT) {\n+          __ subl(count, (1<<shift));\n+        __ BIND(L_copy_2_bytes);\n+          \/\/ copy prefix dword\n+          __ testl(count, 1<<(shift-1));\n+          __ jccb(Assembler::zero, L_copy_byte);\n+          __ movw(rdx, Address(from, count, sf, -2));\n+          __ movw(Address(to, count, sf, -2), rdx);\n+          if (t == T_BYTE) {\n+            __ subl(count, 1<<(shift-1));\n+          __ BIND(L_copy_byte);\n+            \/\/ copy prefix byte\n+            __ testl(count, 1);\n+            __ jccb(Assembler::zero, L_exit);\n+            __ movb(rdx, Address(from, 0));\n+            __ movb(Address(to, 0), rdx);\n+          __ BIND(L_exit);\n+          } else {\n+          __ BIND(L_copy_byte);\n+          }\n+      } else {\n+      __ BIND(L_copy_2_bytes);\n@@ -1149,6 +1191,0 @@\n-  __ BIND(L_copy_4_bytes);\n-    \/\/ copy prefix qword\n-    __ testl(count, 1<<shift);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rdx, Address(from, count, sf, -4));\n-    __ movl(Address(to, count, sf, -4), rdx);\n@@ -1156,22 +1192,2 @@\n-    if (t == T_BYTE || t == T_SHORT) {\n-        __ subl(count, (1<<shift));\n-      __ BIND(L_copy_2_bytes);\n-        \/\/ copy prefix dword\n-        __ testl(count, 1<<(shift-1));\n-        __ jccb(Assembler::zero, L_copy_byte);\n-        __ movw(rdx, Address(from, count, sf, -2));\n-        __ movw(Address(to, count, sf, -2), rdx);\n-        if (t == T_BYTE) {\n-          __ subl(count, 1<<(shift-1));\n-        __ BIND(L_copy_byte);\n-          \/\/ copy prefix byte\n-          __ testl(count, 1);\n-          __ jccb(Assembler::zero, L_exit);\n-          __ movb(rdx, Address(from, 0));\n-          __ movb(Address(to, 0), rdx);\n-        __ BIND(L_exit);\n-        } else {\n-        __ BIND(L_copy_byte);\n-        }\n-    } else {\n-    __ BIND(L_copy_2_bytes);\n+    if (VM_Version::supports_mmx() && !UseXMMForArrayCopy) {\n+      __ emms();\n@@ -1179,1 +1195,0 @@\n-\n@@ -1215,4 +1230,10 @@\n-    __ subptr(to, from); \/\/ to --> to_from\n-    if (VM_Version::supports_mmx()) {\n-      if (UseXMMForArrayCopy) {\n-        xmm_copy_forward(from, to_from, count);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, true, true);\n+      __ subptr(to, from); \/\/ to --> to_from\n+      if (VM_Version::supports_mmx()) {\n+        if (UseXMMForArrayCopy) {\n+          xmm_copy_forward(from, to_from, count);\n+        } else {\n+          mmx_copy_forward(from, to_from, count);\n+        }\n@@ -1220,1 +1241,9 @@\n-        mmx_copy_forward(from, to_from, count);\n+        __ jmpb(L_copy_8_bytes);\n+        __ align(OptoLoopAlignment);\n+      __ BIND(L_copy_8_bytes_loop);\n+        __ fild_d(Address(from, 0));\n+        __ fistp_d(Address(from, to_from, Address::times_1));\n+        __ addptr(from, 8);\n+      __ BIND(L_copy_8_bytes);\n+        __ decrement(count);\n+        __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n@@ -1222,10 +1251,3 @@\n-    } else {\n-      __ jmpb(L_copy_8_bytes);\n-      __ align(OptoLoopAlignment);\n-    __ BIND(L_copy_8_bytes_loop);\n-      __ fild_d(Address(from, 0));\n-      __ fistp_d(Address(from, to_from, Address::times_1));\n-      __ addptr(from, 8);\n-    __ BIND(L_copy_8_bytes);\n-      __ decrement(count);\n-      __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n+    }\n+    if (VM_Version::supports_mmx() && !UseXMMForArrayCopy) {\n+      __ emms();\n@@ -1270,1 +1292,3 @@\n-    __ jmpb(L_copy_8_bytes);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, true, true);\n@@ -1272,6 +1296,12 @@\n-    __ align(OptoLoopAlignment);\n-  __ BIND(L_copy_8_bytes_loop);\n-    if (VM_Version::supports_mmx()) {\n-      if (UseXMMForArrayCopy) {\n-        __ movq(xmm0, Address(from, count, Address::times_8));\n-        __ movq(Address(to, count, Address::times_8), xmm0);\n+      __ jmpb(L_copy_8_bytes);\n+\n+      __ align(OptoLoopAlignment);\n+    __ BIND(L_copy_8_bytes_loop);\n+      if (VM_Version::supports_mmx()) {\n+        if (UseXMMForArrayCopy) {\n+          __ movq(xmm0, Address(from, count, Address::times_8));\n+          __ movq(Address(to, count, Address::times_8), xmm0);\n+        } else {\n+          __ movq(mmx0, Address(from, count, Address::times_8));\n+          __ movq(Address(to, count, Address::times_8), mmx0);\n+        }\n@@ -1279,2 +1309,2 @@\n-        __ movq(mmx0, Address(from, count, Address::times_8));\n-        __ movq(Address(to, count, Address::times_8), mmx0);\n+        __ fild_d(Address(from, count, Address::times_8));\n+        __ fistp_d(Address(to, count, Address::times_8));\n@@ -1282,7 +1312,3 @@\n-    } else {\n-      __ fild_d(Address(from, count, Address::times_8));\n-      __ fistp_d(Address(to, count, Address::times_8));\n-    }\n-  __ BIND(L_copy_8_bytes);\n-    __ decrement(count);\n-    __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n+    __ BIND(L_copy_8_bytes);\n+      __ decrement(count);\n+      __ jcc(Assembler::greaterEqual, L_copy_8_bytes_loop);\n@@ -1290,0 +1316,1 @@\n+    }\n@@ -3948,1 +3975,1 @@\n-\n+#define UCM_TABLE_MAX_ENTRIES 8\n@@ -3950,0 +3977,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_32.cpp","additions":230,"deletions":200,"binary":false,"changes":430,"status":"modified"},{"patch":"@@ -1492,1 +1492,0 @@\n-\n@@ -1541,44 +1540,47 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(byte_count, count);\n-    __ shrptr(count, 3); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count); \/\/ make the count negative\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(byte_count, 4);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 4);\n-    __ addptr(end_to, 4);\n-\n-    \/\/ Check for and copy trailing word\n-  __ BIND(L_copy_2_bytes);\n-    __ testl(byte_count, 2);\n-    __ jccb(Assembler::zero, L_copy_byte);\n-    __ movw(rax, Address(end_from, 8));\n-    __ movw(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 2);\n-    __ addptr(end_to, 2);\n-\n-    \/\/ Check for and copy trailing byte\n-  __ BIND(L_copy_byte);\n-    __ testl(byte_count, 1);\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movb(rax, Address(end_from, 8));\n-    __ movb(Address(end_to, 8), rax);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(byte_count, count);\n+      __ shrptr(count, 3); \/\/ count => qword_count\n+\n+      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+      __ negptr(qword_count); \/\/ make the count negative\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+      __ increment(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+      \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(byte_count, 4);\n+      __ jccb(Assembler::zero, L_copy_2_bytes);\n+      __ movl(rax, Address(end_from, 8));\n+      __ movl(Address(end_to, 8), rax);\n+\n+      __ addptr(end_from, 4);\n+      __ addptr(end_to, 4);\n+\n+      \/\/ Check for and copy trailing word\n+    __ BIND(L_copy_2_bytes);\n+      __ testl(byte_count, 2);\n+      __ jccb(Assembler::zero, L_copy_byte);\n+      __ movw(rax, Address(end_from, 8));\n+      __ movw(Address(end_to, 8), rax);\n+\n+      __ addptr(end_from, 2);\n+      __ addptr(end_to, 2);\n+\n+      \/\/ Check for and copy trailing byte\n+    __ BIND(L_copy_byte);\n+      __ testl(byte_count, 1);\n+      __ jccb(Assembler::zero, L_exit);\n+      __ movb(rax, Address(end_from, 8));\n+      __ movb(Address(end_to, 8), rax);\n+    }\n@@ -1586,0 +1588,1 @@\n+    address ucme_exit_pc = __ pc();\n@@ -1593,4 +1596,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n-\n+    {\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+      __ jmp(L_copy_4_bytes);\n+    }\n@@ -1641,35 +1646,38 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(byte_count, count);\n-    __ shrptr(count, 3);   \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.\n-\n-    \/\/ Check for and copy trailing byte\n-    __ testl(byte_count, 1);\n-    __ jcc(Assembler::zero, L_copy_2_bytes);\n-    __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n-    __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n-    __ decrement(byte_count); \/\/ Adjust for possible trailing word\n-\n-    \/\/ Check for and copy trailing word\n-  __ BIND(L_copy_2_bytes);\n-    __ testl(byte_count, 2);\n-    __ jcc(Assembler::zero, L_copy_4_bytes);\n-    __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n-    __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(byte_count, 4);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, qword_count, Address::times_8));\n-    __ movl(Address(to, qword_count, Address::times_8), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(byte_count, count);\n+      __ shrptr(count, 3);   \/\/ count => qword_count\n+\n+      \/\/ Copy from high to low addresses.\n+\n+      \/\/ Check for and copy trailing byte\n+      __ testl(byte_count, 1);\n+      __ jcc(Assembler::zero, L_copy_2_bytes);\n+      __ movb(rax, Address(from, byte_count, Address::times_1, -1));\n+      __ movb(Address(to, byte_count, Address::times_1, -1), rax);\n+      __ decrement(byte_count); \/\/ Adjust for possible trailing word\n+\n+      \/\/ Check for and copy trailing word\n+    __ BIND(L_copy_2_bytes);\n+      __ testl(byte_count, 2);\n+      __ jcc(Assembler::zero, L_copy_4_bytes);\n+      __ movw(rax, Address(from, byte_count, Address::times_1, -2));\n+      __ movw(Address(to, byte_count, Address::times_1, -2), rax);\n+\n+      \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(byte_count, 4);\n+      __ jcc(Assembler::zero, L_copy_bytes);\n+      __ movl(rax, Address(from, qword_count, Address::times_8));\n+      __ movl(Address(to, qword_count, Address::times_8), rax);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+      __ decrement(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -1683,3 +1691,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -1743,37 +1754,40 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(word_count, count);\n-    __ shrptr(count, 2); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Original 'dest' is trashed, so we can't use it as a\n-    \/\/ base register for a possible trailing word copy\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(word_count, 2);\n-    __ jccb(Assembler::zero, L_copy_2_bytes);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-\n-    __ addptr(end_from, 4);\n-    __ addptr(end_to, 4);\n-\n-    \/\/ Check for and copy trailing word\n-  __ BIND(L_copy_2_bytes);\n-    __ testl(word_count, 1);\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movw(rax, Address(end_from, 8));\n-    __ movw(Address(end_to, 8), rax);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(word_count, count);\n+      __ shrptr(count, 2); \/\/ count => qword_count\n+\n+      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+      __ negptr(qword_count);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+      __ increment(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+      \/\/ Original 'dest' is trashed, so we can't use it as a\n+      \/\/ base register for a possible trailing word copy\n+\n+      \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(word_count, 2);\n+      __ jccb(Assembler::zero, L_copy_2_bytes);\n+      __ movl(rax, Address(end_from, 8));\n+      __ movl(Address(end_to, 8), rax);\n+\n+      __ addptr(end_from, 4);\n+      __ addptr(end_to, 4);\n+\n+      \/\/ Check for and copy trailing word\n+    __ BIND(L_copy_2_bytes);\n+      __ testl(word_count, 1);\n+      __ jccb(Assembler::zero, L_exit);\n+      __ movw(rax, Address(end_from, 8));\n+      __ movw(Address(end_to, 8), rax);\n+    }\n@@ -1781,0 +1795,1 @@\n+    address ucme_exit_pc = __ pc();\n@@ -1788,3 +1803,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n+    {\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, false, ucme_exit_pc);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+      __ jmp(L_copy_4_bytes);\n+    }\n@@ -1857,27 +1875,30 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(word_count, count);\n-    __ shrptr(count, 2); \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-    \/\/ Check for and copy trailing word\n-    __ testl(word_count, 1);\n-    __ jccb(Assembler::zero, L_copy_4_bytes);\n-    __ movw(rax, Address(from, word_count, Address::times_2, -2));\n-    __ movw(Address(to, word_count, Address::times_2, -2), rax);\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(word_count, 2);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, qword_count, Address::times_8));\n-    __ movl(Address(to, qword_count, Address::times_8), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(word_count, count);\n+      __ shrptr(count, 2); \/\/ count => qword_count\n+\n+      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+      \/\/ Check for and copy trailing word\n+      __ testl(word_count, 1);\n+      __ jccb(Assembler::zero, L_copy_4_bytes);\n+      __ movw(rax, Address(from, word_count, Address::times_2, -2));\n+      __ movw(Address(to, word_count, Address::times_2, -2), rax);\n+\n+     \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(word_count, 2);\n+      __ jcc(Assembler::zero, L_copy_bytes);\n+      __ movl(rax, Address(from, qword_count, Address::times_8));\n+      __ movl(Address(to, qword_count, Address::times_8), rax);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+      __ decrement(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -1891,3 +1912,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !aligned, true);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -1964,24 +1988,27 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(dword_count, count);\n-    __ shrptr(count, 1); \/\/ count => qword_count\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n-    \/\/ Check for and copy trailing dword\n-  __ BIND(L_copy_4_bytes);\n-    __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n-    __ jccb(Assembler::zero, L_exit);\n-    __ movl(rax, Address(end_from, 8));\n-    __ movl(Address(end_to, 8), rax);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(dword_count, count);\n+      __ shrptr(count, 1); \/\/ count => qword_count\n+\n+      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+      __ negptr(qword_count);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+      __ increment(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+\n+      \/\/ Check for and copy trailing dword\n+    __ BIND(L_copy_4_bytes);\n+      __ testl(dword_count, 1); \/\/ Only byte test since the value is 0 or 1\n+      __ jccb(Assembler::zero, L_exit);\n+      __ movl(rax, Address(end_from, 8));\n+      __ movl(Address(end_to, 8), rax);\n+    }\n@@ -1989,0 +2016,1 @@\n+    address ucme_exit_pc = __ pc();\n@@ -1997,3 +2025,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-    __ jmp(L_copy_4_bytes);\n+    {\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, false, ucme_exit_pc);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+      __ jmp(L_copy_4_bytes);\n+    }\n@@ -2060,20 +2091,23 @@\n-    \/\/ 'from', 'to' and 'count' are now valid\n-    __ movptr(dword_count, count);\n-    __ shrptr(count, 1); \/\/ count => qword_count\n-\n-    \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n-\n-    \/\/ Check for and copy trailing dword\n-    __ testl(dword_count, 1);\n-    __ jcc(Assembler::zero, L_copy_bytes);\n-    __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n-    __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ 'from', 'to' and 'count' are now valid\n+      __ movptr(dword_count, count);\n+      __ shrptr(count, 1); \/\/ count => qword_count\n+\n+      \/\/ Copy from high to low addresses.  Use 'to' as scratch.\n+\n+      \/\/ Check for and copy trailing dword\n+      __ testl(dword_count, 1);\n+      __ jcc(Assembler::zero, L_copy_bytes);\n+      __ movl(rax, Address(from, dword_count, Address::times_4, -4));\n+      __ movl(Address(to, dword_count, Address::times_4, -4), rax);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+      __ decrement(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -2090,2 +2124,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -2161,14 +2199,17 @@\n-\n-    \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n-    __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n-    __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n-    __ negptr(qword_count);\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n-    __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n-    __ increment(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n-\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+\n+      \/\/ Copy from low to high addresses.  Use 'to' as scratch.\n+      __ lea(end_from, Address(from, qword_count, Address::times_8, -8));\n+      __ lea(end_to,   Address(to,   qword_count, Address::times_8, -8));\n+      __ negptr(qword_count);\n+      __ jmp(L_copy_bytes);\n+\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(end_from, qword_count, Address::times_8, 8));\n+      __ movq(Address(end_to, qword_count, Address::times_8, 8), rax);\n+      __ increment(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -2186,2 +2227,6 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_forward(end_from, end_to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -2254,0 +2299,3 @@\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n@@ -2255,8 +2303,1 @@\n-    __ jmp(L_copy_bytes);\n-\n-    \/\/ Copy trailing qwords\n-  __ BIND(L_copy_8_bytes);\n-    __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n-    __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n-    __ decrement(qword_count);\n-    __ jcc(Assembler::notZero, L_copy_8_bytes);\n+      __ jmp(L_copy_bytes);\n@@ -2264,0 +2305,7 @@\n+      \/\/ Copy trailing qwords\n+    __ BIND(L_copy_8_bytes);\n+      __ movq(rax, Address(from, qword_count, Address::times_8, -8));\n+      __ movq(Address(to, qword_count, Address::times_8, -8), rax);\n+      __ decrement(qword_count);\n+      __ jcc(Assembler::notZero, L_copy_8_bytes);\n+    }\n@@ -2274,0 +2322,3 @@\n+    {\n+      \/\/ UnsafeCopyMemory page error: continue after ucm\n+      UnsafeCopyMemoryMark ucmm(this, !is_oop && !aligned, true);\n@@ -2275,3 +2326,3 @@\n-    \/\/ Copy in multi-bytes chunks\n-    copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n-\n+      \/\/ Copy in multi-bytes chunks\n+      copy_bytes_backward(from, to, qword_count, rax, L_copy_bytes, L_copy_8_bytes);\n+    }\n@@ -6093,0 +6144,1 @@\n+#define UCM_TABLE_MAX_ENTRIES 16\n@@ -6094,0 +6146,3 @@\n+  if (UnsafeCopyMemory::_table == NULL) {\n+    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":288,"deletions":233,"binary":false,"changes":521,"status":"modified"},{"patch":"@@ -2595,1 +2595,5 @@\n-      if ((thread->thread_state() == _thread_in_vm &&\n+\n+      bool is_unsafe_arraycopy = (thread->thread_state() == _thread_in_native || in_java) && UnsafeCopyMemory::contains_pc(pc);\n+      if (((thread->thread_state() == _thread_in_vm ||\n+           thread->thread_state() == _thread_in_native ||\n+           is_unsafe_arraycopy) &&\n@@ -2598,1 +2602,5 @@\n-        return Handle_Exception(exceptionInfo, SharedRuntime::handle_unsafe_access(thread, (address)Assembler::locate_next_instruction(pc)));\n+        address next_pc =  Assembler::locate_next_instruction(pc);\n+        if (is_unsafe_arraycopy) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n+        return Handle_Exception(exceptionInfo, SharedRuntime::handle_unsafe_access(thread, next_pc));\n","filename":"src\/hotspot\/os\/windows\/os_windows.cpp","additions":10,"deletions":2,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -444,1 +444,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -446,0 +447,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -464,1 +468,2 @@\n-      else if (thread->thread_state() == _thread_in_vm &&\n+      else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n@@ -467,0 +472,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n","filename":"src\/hotspot\/os_cpu\/aix_ppc\/os_aix_ppc.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -592,1 +592,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc);\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -594,0 +595,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -668,1 +672,2 @@\n-    } else if (thread->thread_state() == _thread_in_vm &&\n+    } else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n@@ -672,0 +677,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n","filename":"src\/hotspot\/os_cpu\/bsd_x86\/os_bsd_x86.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -196,1 +196,2 @@\n-    else*\/ if (thread->thread_state() == _thread_in_vm &&\n+    else*\/ if ((thread->thread_state() == _thread_in_vm ||\n+               thread->thread_state() == _thread_in_native) &&\n","filename":"src\/hotspot\/os_cpu\/bsd_zero\/os_bsd_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -375,1 +375,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -377,0 +378,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -395,1 +399,2 @@\n-    } else if (thread->thread_state() == _thread_in_vm &&\n+    } else if ((thread->thread_state() == _thread_in_vm ||\n+                 thread->thread_state() == _thread_in_native) &&\n@@ -399,0 +404,3 @@\n+      if (UnsafeCopyMemory::contains_pc(pc)) {\n+        next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+      }\n","filename":"src\/hotspot\/os_cpu\/linux_aarch64\/os_linux_aarch64.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2008, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2008, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -388,1 +388,1 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        if ((nm != NULL && nm->has_unsafe_access()) || (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc))) {\n@@ -402,1 +402,2 @@\n-    } else if (thread->thread_state() == _thread_in_vm &&\n+    } else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n@@ -422,0 +423,3 @@\n+    if (UnsafeCopyMemory::contains_pc(pc)) {\n+      next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+    }\n","filename":"src\/hotspot\/os_cpu\/linux_arm\/os_linux_arm.cpp","additions":7,"deletions":3,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -473,1 +473,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -475,0 +476,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -489,1 +493,2 @@\n-      else if (thread->thread_state() == _thread_in_vm &&\n+      else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n@@ -492,0 +497,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n@@ -493,1 +501,1 @@\n-        os::Linux::ucontext_set_pc(uc, pc + 4);\n+        os::Linux::ucontext_set_pc(uc, next_pc);\n","filename":"src\/hotspot\/os_cpu\/linux_ppc\/os_linux_ppc.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -471,1 +471,2 @@\n-      } else if (thread->thread_state() == _thread_in_vm &&\n+      } else if ((thread->thread_state() == _thread_in_vm ||\n+                  thread->thread_state() == _thread_in_native) &&\n","filename":"src\/hotspot\/os_cpu\/linux_s390\/os_linux_s390.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -388,1 +388,5 @@\n-  if (nm != NULL && nm->has_unsafe_access()) {\n+  bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+  if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n+    if (is_unsafe_arraycopy) {\n+      npc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+    }\n@@ -554,1 +558,2 @@\n-        thread->thread_state() == _thread_in_vm &&\n+        (thread->thread_state() == _thread_in_vm ||\n+         thread->thread_state() == _thread_in_native) &&\n@@ -556,0 +561,3 @@\n+      if (UnsafeCopyMemory::contains_pc(pc)) {\n+        npc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+      }\n","filename":"src\/hotspot\/os_cpu\/linux_sparc\/os_linux_sparc.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -439,1 +439,2 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc);\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n@@ -441,0 +442,3 @@\n+          if (is_unsafe_arraycopy) {\n+            next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n@@ -487,3 +491,4 @@\n-    } else if (thread->thread_state() == _thread_in_vm &&\n-               sig == SIGBUS && \/* info->si_code == BUS_OBJERR && *\/\n-               thread->doing_unsafe_access()) {\n+    } else if ((thread->thread_state() == _thread_in_vm ||\n+                thread->thread_state() == _thread_in_native) &&\n+               (sig == SIGBUS && \/* info->si_code == BUS_OBJERR && *\/\n+               thread->doing_unsafe_access())) {\n@@ -491,0 +496,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n","filename":"src\/hotspot\/os_cpu\/linux_x86\/os_linux_x86.cpp","additions":13,"deletions":5,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -210,1 +210,2 @@\n-    else*\/ if (thread->thread_state() == _thread_in_vm &&\n+    else*\/ if ((thread->thread_state() == _thread_in_vm ||\n+               thread->thread_state() == _thread_in_native) &&\n","filename":"src\/hotspot\/os_cpu\/linux_zero\/os_linux_zero.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -439,1 +439,2 @@\n-    if (thread->thread_state() == _thread_in_vm) {\n+    if (thread->thread_state() == _thread_in_vm ||\n+        thread->thread_state() == _thread_in_native) {\n@@ -441,0 +442,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          npc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n@@ -479,1 +483,5 @@\n-        if (nm != NULL && nm->has_unsafe_access()) {\n+        bool is_unsafe_arraycopy = (thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc));\n+        if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy) {\n+          if (is_unsafe_arraycopy) {\n+            npc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+          }\n","filename":"src\/hotspot\/os_cpu\/solaris_sparc\/os_solaris_sparc.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n@@ -520,1 +520,2 @@\n-    if (thread->thread_state() == _thread_in_vm) {\n+    if (thread->thread_state() == _thread_in_vm ||\n+         thread->thread_state() == _thread_in_native) {\n@@ -523,0 +524,3 @@\n+        if (UnsafeCopyMemory::contains_pc(pc)) {\n+          next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+        }\n@@ -539,1 +543,2 @@\n-          if (nm != NULL && nm->has_unsafe_access()) {\n+          bool is_unsafe_arraycopy = thread->doing_unsafe_access() && UnsafeCopyMemory::contains_pc(pc);\n+          if ((nm != NULL && nm->has_unsafe_access()) || is_unsafe_arraycopy)) {\n@@ -541,0 +546,3 @@\n+            if (is_unsafe_arraycopy) {\n+              next_pc = UnsafeCopyMemory::page_error_continue_pc(pc);\n+            }\n","filename":"src\/hotspot\/os_cpu\/solaris_x86\/os_solaris_x86.cpp","additions":11,"deletions":3,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -4252,0 +4252,8 @@\n+  Node* thread = _gvn.transform(new ThreadLocalNode());\n+  Node* doing_unsafe_access_addr = basic_plus_adr(top(), thread, in_bytes(JavaThread::doing_unsafe_access_offset()));\n+  BasicType doing_unsafe_access_bt = T_BYTE;\n+  assert((sizeof(bool) * CHAR_BIT) == 8, \"not implemented\");\n+\n+  \/\/ update volatile field\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(1), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n+\n@@ -4260,0 +4268,2 @@\n+  store_to_memory(control(), doing_unsafe_access_addr, intcon(0), doing_unsafe_access_bt, Compile::AliasIdxRaw, MemNode::unordered);\n+\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -151,0 +151,19 @@\n+\/**\n+ * Helper class to wrap memory accesses in JavaThread::doing_unsafe_access()\n+ *\/\n+class GuardUnsafeAccess {\n+  JavaThread* _thread;\n+\n+public:\n+  GuardUnsafeAccess(JavaThread* thread) : _thread(thread) {\n+    \/\/ native\/off-heap access which may raise SIGBUS if accessing\n+    \/\/ memory mapped file data in a region of the file which has\n+    \/\/ been truncated and is now invalid.\n+    _thread->set_doing_unsafe_access(true);\n+  }\n+\n+  ~GuardUnsafeAccess() {\n+    _thread->set_doing_unsafe_access(false);\n+  }\n+};\n+\n@@ -192,19 +211,0 @@\n-  \/**\n-   * Helper class to wrap memory accesses in JavaThread::doing_unsafe_access()\n-   *\/\n-  class GuardUnsafeAccess {\n-    JavaThread* _thread;\n-\n-  public:\n-    GuardUnsafeAccess(JavaThread* thread) : _thread(thread) {\n-      \/\/ native\/off-heap access which may raise SIGBUS if accessing\n-      \/\/ memory mapped file data in a region of the file which has\n-      \/\/ been truncated and is now invalid\n-      _thread->set_doing_unsafe_access(true);\n-    }\n-\n-    ~GuardUnsafeAccess() {\n-      _thread->set_doing_unsafe_access(false);\n-    }\n-  };\n-\n@@ -402,2 +402,8 @@\n-\n-  Copy::conjoint_memory_atomic(src, dst, sz);\n+  {\n+    GuardUnsafeAccess guard(thread);\n+    if (StubRoutines::unsafe_arraycopy() != NULL) {\n+      StubRoutines::UnsafeArrayCopy_stub()(src, dst, sz);\n+    } else {\n+      Copy::conjoint_memory_atomic(src, dst, sz);\n+    }\n+  }\n@@ -419,1 +425,5 @@\n-    Copy::conjoint_swap(src, dst, sz, esz);\n+    {\n+      JavaThread* thread = JavaThread::thread_from_jni_environment(env);\n+      GuardUnsafeAccess guard(thread);\n+      Copy::conjoint_swap(src, dst, sz, esz);\n+    }\n@@ -430,1 +440,4 @@\n-      Copy::conjoint_swap(src, dst, sz, esz);\n+      {\n+        GuardUnsafeAccess guard(thread);\n+        Copy::conjoint_swap(src, dst, sz, esz);\n+      }\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":36,"deletions":23,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -41,0 +41,4 @@\n+UnsafeCopyMemory* UnsafeCopyMemory::_table                      = NULL;\n+int UnsafeCopyMemory::_table_length                             = 0;\n+int UnsafeCopyMemory::_table_max_length                         = 0;\n+address UnsafeCopyMemory::_common_exit_stub_pc                  = NULL;\n@@ -116,1 +120,0 @@\n-\n@@ -180,0 +183,25 @@\n+void UnsafeCopyMemory::create_table(int max_size) {\n+  UnsafeCopyMemory::_table = new UnsafeCopyMemory[max_size];\n+  UnsafeCopyMemory::_table_max_length = max_size;\n+}\n+\n+bool UnsafeCopyMemory::contains_pc(address pc) {\n+  for (int i = 0; i < UnsafeCopyMemory::_table_length; i++) {\n+    UnsafeCopyMemory* entry = &UnsafeCopyMemory::_table[i];\n+    if (pc >= entry->start_pc() && pc < entry->end_pc()) {\n+      return true;\n+    }\n+  }\n+  return false;\n+}\n+\n+address UnsafeCopyMemory::page_error_continue_pc(address pc) {\n+  for (int i = 0; i < UnsafeCopyMemory::_table_length; i++) {\n+    UnsafeCopyMemory* entry = &UnsafeCopyMemory::_table[i];\n+    if (pc >= entry->start_pc() && pc < entry->end_pc()) {\n+      return entry->error_exit_pc();\n+    }\n+  }\n+  return NULL;\n+}\n+\n@@ -572,0 +600,22 @@\n+\n+UnsafeCopyMemoryMark::UnsafeCopyMemoryMark(StubCodeGenerator* cgen, bool add_entry, bool continue_at_scope_end, address error_exit_pc) {\n+  _cgen = cgen;\n+  _ucm_entry = NULL;\n+  if (add_entry) {\n+    address err_exit_pc = NULL;\n+    if (!continue_at_scope_end) {\n+      err_exit_pc = error_exit_pc != NULL ? error_exit_pc : UnsafeCopyMemory::common_exit_stub_pc();\n+    }\n+    assert(err_exit_pc != NULL || continue_at_scope_end, \"error exit not set\");\n+    _ucm_entry = UnsafeCopyMemory::add_to_table(_cgen->assembler()->pc(), NULL, err_exit_pc);\n+  }\n+}\n+\n+UnsafeCopyMemoryMark::~UnsafeCopyMemoryMark() {\n+  if (_ucm_entry != NULL) {\n+    _ucm_entry->set_end_pc(_cgen->assembler()->pc());\n+    if (_ucm_entry->error_exit_pc() == NULL) {\n+      _ucm_entry->set_error_exit_pc(_cgen->assembler()->pc());\n+    }\n+  }\n+}\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.cpp","additions":51,"deletions":1,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -77,0 +77,45 @@\n+class UnsafeCopyMemory : public CHeapObj<mtCode> {\n+ private:\n+  address _start_pc;\n+  address _end_pc;\n+  address _error_exit_pc;\n+ public:\n+  static address           _common_exit_stub_pc;\n+  static UnsafeCopyMemory* _table;\n+  static int               _table_length;\n+  static int               _table_max_length;\n+  UnsafeCopyMemory() : _start_pc(NULL), _end_pc(NULL), _error_exit_pc(NULL) {}\n+  void    set_start_pc(address pc)      { _start_pc = pc; }\n+  void    set_end_pc(address pc)        { _end_pc = pc; }\n+  void    set_error_exit_pc(address pc) { _error_exit_pc = pc; }\n+  address start_pc()      const { return _start_pc; }\n+  address end_pc()        const { return _end_pc; }\n+  address error_exit_pc() const { return _error_exit_pc; }\n+\n+  static void    set_common_exit_stub_pc(address pc) { _common_exit_stub_pc = pc; }\n+  static address common_exit_stub_pc()               { return _common_exit_stub_pc; }\n+\n+  static UnsafeCopyMemory* add_to_table(address start_pc, address end_pc, address error_exit_pc) {\n+    guarantee(_table_length < _table_max_length, \"Incorrect UnsafeCopyMemory::_table_max_length\");\n+    UnsafeCopyMemory* entry = &_table[_table_length];\n+    entry->set_start_pc(start_pc);\n+    entry->set_end_pc(end_pc);\n+    entry->set_error_exit_pc(error_exit_pc);\n+\n+    _table_length++;\n+    return entry;\n+  }\n+\n+  static bool    contains_pc(address pc);\n+  static address page_error_continue_pc(address pc);\n+  static void    create_table(int max_size);\n+};\n+\n+class UnsafeCopyMemoryMark : public StackObj {\n+ private:\n+  UnsafeCopyMemory*  _ucm_entry;\n+  StubCodeGenerator* _cgen;\n+ public:\n+  UnsafeCopyMemoryMark(StubCodeGenerator* cgen, bool add_entry, bool continue_at_scope_end, address error_exit_pc = NULL);\n+  ~UnsafeCopyMemoryMark();\n+};\n@@ -313,1 +358,0 @@\n-\n@@ -317,1 +361,5 @@\n-  static address unsafe_arraycopy()    { return _unsafe_arraycopy; }\n+  static address unsafe_arraycopy()     { return _unsafe_arraycopy; }\n+\n+  typedef void (*UnsafeArrayCopyStub)(const void* src, void* dst, size_t count);\n+  static UnsafeArrayCopyStub UnsafeArrayCopy_stub()         { return CAST_TO_FN_PTR(UnsafeArrayCopyStub,  _unsafe_arraycopy); }\n+\n","filename":"src\/hotspot\/share\/runtime\/stubRoutines.hpp","additions":50,"deletions":2,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -1797,0 +1797,1 @@\n+  static ByteSize doing_unsafe_access_offset() { return byte_offset_of(JavaThread, _doing_unsafe_access); }\n","filename":"src\/hotspot\/share\/runtime\/thread.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -0,0 +1,155 @@\n+\/*\n+ * Copyright (c) 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+\/*\n+ * @test\n+ * @bug 8191278\n+ * @requires os.family != \"windows\"\n+ * @summary Check that SIGBUS errors caused by memory accesses in Unsafe_CopyMemory()\n+ * and UnsafeCopySwapMemory() get converted to java.lang.InternalError exceptions.\n+ * @modules java.base\/jdk.internal.misc\n+ *\n+ * @library \/test\/lib\n+ * @build sun.hotspot.WhiteBox\n+ * @run main ClassFileInstaller sun.hotspot.WhiteBox\n+ *      sun.hotspot.WhiteBox$WhiteBoxPermission\n+ *\n+ * @run main\/othervm -XX:CompileCommand=exclude,*InternalErrorTest.main -XX:CompileCommand=inline,*.get -XX:CompileCommand=inline,*Unsafe.* -Xbootclasspath\/a:.  -XX:+UnlockDiagnosticVMOptions -XX:+WhiteBoxAPI InternalErrorTest\n+ *\/\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.RandomAccessFile;\n+import java.lang.reflect.Field;\n+import java.lang.reflect.Method;\n+import java.nio.MappedByteBuffer;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import jdk.internal.misc.Unsafe;\n+import sun.hotspot.WhiteBox;\n+\n+\/\/ Test that illegal memory access errors in Unsafe_CopyMemory0() and\n+\/\/ UnsafeCopySwapMemory() that cause SIGBUS errors result in\n+\/\/ java.lang.InternalError exceptions, not JVM crashes.\n+public class InternalErrorTest {\n+\n+    private static final Unsafe unsafe = Unsafe.getUnsafe();\n+    private static final int pageSize = WhiteBox.getWhiteBox().getVMPageSize();\n+    private static final String expectedErrorMsg = \"fault occurred in a recent unsafe memory access\";\n+    private static final String failureMsg1 = \"InternalError not thrown\";\n+    private static final String failureMsg2 = \"Wrong InternalError: \";\n+\n+    public static void main(String[] args) throws Throwable {\n+        Unsafe unsafe = Unsafe.getUnsafe();\n+\n+        String currentDir = System.getProperty(\"test.classes\");\n+        File file = new File(currentDir, \"tmpFile.txt\");\n+\n+        StringBuilder s = new StringBuilder();\n+        for (int i = 1; i < pageSize + 1000; i++) {\n+            s.append(\"1\");\n+        }\n+        Files.write(file.toPath(), s.toString().getBytes());\n+        FileChannel fileChannel = new RandomAccessFile(file, \"r\").getChannel();\n+        MappedByteBuffer buffer =\n+            fileChannel.map(FileChannel.MapMode.READ_ONLY, 0, fileChannel.size());\n+\n+        \/\/ Get address of mapped memory.\n+        long mapAddr = 0;\n+        try {\n+            Field af = java.nio.Buffer.class.getDeclaredField(\"address\");\n+            af.setAccessible(true);\n+            mapAddr = af.getLong(buffer);\n+        } catch (Exception f) {\n+            throw f;\n+        }\n+        long allocMem = unsafe.allocateMemory(4000);\n+\n+        for (int i = 0; i < 3; i++) {\n+            test(buffer, unsafe, mapAddr, allocMem, i);\n+        }\n+\n+        Files.write(file.toPath(), \"2\".getBytes());\n+        buffer.position(buffer.position() + pageSize);\n+        for (int i = 0; i < 3; i++) {\n+            try {\n+                test(buffer, unsafe, mapAddr, allocMem, i);\n+                WhiteBox.getWhiteBox().forceSafepoint();\n+                throw new RuntimeException(failureMsg1);\n+            } catch (InternalError e) {\n+                if (!e.getMessage().contains(expectedErrorMsg)) {\n+                    throw new RuntimeException(failureMsg2 + e.getMessage());\n+                }\n+            }\n+        }\n+\n+        Method m = InternalErrorTest.class.getMethod(\"test\", MappedByteBuffer.class, Unsafe.class, long.class, long.class, int.class);\n+        WhiteBox.getWhiteBox().enqueueMethodForCompilation(m, 3);\n+\n+        for (int i = 0; i < 3; i++) {\n+            try {\n+                test(buffer, unsafe, mapAddr, allocMem, i);\n+                WhiteBox.getWhiteBox().forceSafepoint();\n+                throw new RuntimeException(failureMsg1);\n+            } catch (InternalError e) {\n+                if (!e.getMessage().contains(expectedErrorMsg)) {\n+                    throw new RuntimeException(failureMsg2 + e.getMessage());\n+                }\n+            }\n+        }\n+\n+        WhiteBox.getWhiteBox().enqueueMethodForCompilation(m, 4);\n+\n+        for (int i = 0; i < 3; i++) {\n+            try {\n+                test(buffer, unsafe, mapAddr, allocMem, i);\n+                WhiteBox.getWhiteBox().forceSafepoint();\n+                throw new RuntimeException(failureMsg1);\n+            } catch (InternalError e) {\n+                if (!e.getMessage().contains(expectedErrorMsg)) {\n+                    throw new RuntimeException(failureMsg2 + e.getMessage());\n+                }\n+            }\n+        }\n+\n+        System.out.println(\"Success\");\n+    }\n+\n+    public static void test(MappedByteBuffer buffer, Unsafe unsafe, long mapAddr, long allocMem, int type) {\n+        switch (type) {\n+            case 0:\n+                \/\/ testing Unsafe.copyMemory, trying to access a word from next page after truncation.\n+                buffer.get(new byte[8]);\n+                break;\n+            case 1:\n+                \/\/ testing Unsafe.copySwapMemory, trying to access next  page after truncation.\n+                unsafe.copySwapMemory(null, mapAddr + pageSize, new byte[4000], 16, 2000, 2);\n+                break;\n+            case 2:\n+                \/\/ testing Unsafe.copySwapMemory, trying to access next  page after truncation.\n+                unsafe.copySwapMemory(null, mapAddr + pageSize, null, allocMem, 2000, 2);\n+                break;\n+        }\n+    }\n+\n+}\n","filename":"test\/hotspot\/jtreg\/runtime\/Unsafe\/InternalErrorTest.java","additions":155,"deletions":0,"binary":false,"changes":155,"status":"added"}]}