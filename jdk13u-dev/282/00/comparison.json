{"files":[{"patch":"@@ -1750,0 +1750,14 @@\n+  if (C->stub_function() == NULL && BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+    st->print(\"\\n\\t\");\n+    st->print(\"ldr  rscratch1, [guard]\\n\\t\");\n+    st->print(\"dmb ishld\\n\\t\");\n+    st->print(\"ldr  rscratch2, [rthread, #thread_disarmed_offset]\\n\\t\");\n+    st->print(\"cmp  rscratch1, rscratch2\\n\\t\");\n+    st->print(\"b.eq skip\");\n+    st->print(\"\\n\\t\");\n+    st->print(\"blr #nmethod_entry_barrier_stub\\n\\t\");\n+    st->print(\"b skip\\n\\t\");\n+    st->print(\"guard: int\\n\\t\");\n+    st->print(\"\\n\\t\");\n+    st->print(\"skip:\\n\\t\");\n+  }\n@@ -1782,0 +1796,5 @@\n+  if (C->stub_function() == NULL) {\n+    BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+    bs->nmethod_entry_barrier(&_masm);\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -339,0 +340,4 @@\n+\n+  \/\/ Insert nmethod entry barrier into frame.\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->nmethod_entry_barrier(this);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,1 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n@@ -27,0 +28,1 @@\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -28,0 +30,1 @@\n+#include \"interpreter\/interp_masm.hpp\"\n@@ -30,0 +33,1 @@\n+#include \"runtime\/sharedRuntime.hpp\"\n@@ -32,0 +36,1 @@\n+\n@@ -232,0 +237,64 @@\n+\n+void BarrierSetAssembler::nmethod_entry_barrier(MacroAssembler* masm) {\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+\n+  if (bs_nm == NULL) {\n+    return;\n+  }\n+\n+  Label skip, guard;\n+  Address thread_disarmed_addr(rthread, in_bytes(bs_nm->thread_disarmed_offset()));\n+\n+  __ ldrw(rscratch1, guard);\n+\n+  \/\/ Subsequent loads of oops must occur after load of guard value.\n+  \/\/ BarrierSetNMethod::disarm sets guard with release semantics.\n+  __ membar(__ LoadLoad);\n+  __ ldrw(rscratch2, thread_disarmed_addr);\n+  __ cmpw(rscratch1, rscratch2);\n+  __ br(Assembler::EQ, skip);\n+\n+  __ mov(rscratch1, StubRoutines::aarch64::method_entry_barrier());\n+  __ blr(rscratch1);\n+  __ b(skip);\n+\n+  __ bind(guard);\n+\n+  __ emit_int32(0);   \/\/ nmethod guard value. Skipped over in common case.\n+\n+  __ bind(skip);\n+}\n+\n+void BarrierSetAssembler::c2i_entry_barrier(MacroAssembler* masm) {\n+  BarrierSetNMethod* bs = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs == NULL) {\n+    return;\n+  }\n+\n+  Label bad_call;\n+  __ cbz(rmethod, bad_call);\n+\n+  \/\/ Pointer chase to the method holder to find out if the method is concurrently unloading.\n+  Label method_live;\n+  __ load_method_holder_cld(rscratch1, rmethod);\n+\n+  \/\/ Is it a strong CLD?\n+  __ ldr(rscratch2, Address(rscratch1, ClassLoaderData::keep_alive_offset()));\n+  __ cbnz(rscratch2, method_live);\n+\n+  \/\/ Is it a weak but alive CLD?\n+  __ stp(r10, r11, Address(__ pre(sp, -2 * wordSize)));\n+  __ ldr(r10, Address(rscratch1, ClassLoaderData::holder_offset()));\n+\n+  \/\/ Uses rscratch1 & rscratch2, so we must pass new temporaries.\n+  __ resolve_weak_handle(r10, r11);\n+  __ mov(rscratch1, r10);\n+  __ ldp(r10, r11, Address(__ post(sp, 2 * wordSize)));\n+  __ cbnz(rscratch1, method_live);\n+\n+  __ bind(bad_call);\n+\n+  __ far_jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub()));\n+  __ bind(method_live);\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.cpp","additions":70,"deletions":1,"binary":false,"changes":71,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,2 @@\n+#include \"gc\/shared\/barrierSet.hpp\"\n+#include \"gc\/shared\/barrierSetNMethod.hpp\"\n@@ -75,0 +77,4 @@\n+\n+  virtual void nmethod_entry_barrier(MacroAssembler* masm);\n+  virtual void c2i_entry_barrier(MacroAssembler* masm);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetAssembler_aarch64.hpp","additions":7,"deletions":1,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,0 +26,2 @@\n+#include \"code\/codeCache.hpp\"\n+#include \"code\/nativeInst.hpp\"\n@@ -27,0 +29,5 @@\n+#include \"logging\/log.hpp\"\n+#include \"memory\/resourceArea.hpp\"\n+#include \"runtime\/sharedRuntime.hpp\"\n+#include \"runtime\/thread.hpp\"\n+#include \"utilities\/align.hpp\"\n@@ -29,0 +36,60 @@\n+class NativeNMethodBarrier: public NativeInstruction {\n+  address instruction_address() const { return addr_at(0); }\n+\n+  int *guard_addr() {\n+    return reinterpret_cast<int*>(instruction_address() + 10 * 4);\n+  }\n+\n+public:\n+  int get_value() {\n+    return OrderAccess::load_acquire(guard_addr());\n+  }\n+\n+  void set_value(int value) {\n+    OrderAccess::release_store(guard_addr(), value);\n+  }\n+\n+  void verify() const;\n+};\n+\n+\/\/ Store the instruction bitmask, bits and name for checking the barrier.\n+struct CheckInsn {\n+  uint32_t mask;\n+  uint32_t bits;\n+  const char *name;\n+};\n+\n+static const struct CheckInsn barrierInsn[] = {\n+  { 0xff000000, 0x18000000, \"ldr (literal)\" },\n+  { 0xfffff0ff, 0xd50330bf, \"dmb\" },\n+  { 0xffc00000, 0xb9400000, \"ldr\"},\n+  { 0x7f20001f, 0x6b00001f, \"cmp\"},\n+  { 0xff00001f, 0x54000000, \"b.eq\"},\n+  { 0xff800000, 0xd2800000, \"mov\"},\n+  { 0xff800000, 0xf2800000, \"movk\"},\n+  { 0xff800000, 0xf2800000, \"movk\"},\n+  { 0xfffffc1f, 0xd63f0000, \"blr\"},\n+  { 0xfc000000, 0x14000000, \"b\"}\n+};\n+\n+\/\/ The encodings must match the instructions emitted by\n+\/\/ BarrierSetAssembler::nmethod_entry_barrier. The matching ignores the specific\n+\/\/ register numbers and immediate values in the encoding.\n+void NativeNMethodBarrier::verify() const {\n+  intptr_t addr = (intptr_t) instruction_address();\n+  for(unsigned int i = 0; i < sizeof(barrierInsn)\/sizeof(struct CheckInsn); i++ ) {\n+    uint32_t inst = *((uint32_t*) addr);\n+    if ((inst & barrierInsn[i].mask) != barrierInsn[i].bits) {\n+      tty->print_cr(\"Addr: \" INTPTR_FORMAT \" Code: 0x%x\", addr, inst);\n+      fatal(\"not an %s instruction.\", barrierInsn[i].name);\n+    }\n+    addr +=4;\n+  }\n+}\n+\n+\n+\/* We're called from an nmethod when we need to deoptimize it. We do\n+   this by throwing away the nmethod's frame and jumping to the\n+   ic_miss stub. This looks like there has been an IC miss at the\n+   entry of the nmethod, so we resolve the call, which will fall back\n+   to the interpreter if the nmethod has been unloaded. *\/\n@@ -30,1 +97,47 @@\n-  ShouldNotReachHere();\n+\n+  typedef struct {\n+    intptr_t *sp; intptr_t *fp; address lr; address pc;\n+  } frame_pointers_t;\n+\n+  frame_pointers_t *new_frame = (frame_pointers_t *)(return_address_ptr - 5);\n+\n+  JavaThread *thread = (JavaThread*)Thread::current();\n+  RegisterMap reg_map(thread, false);\n+  frame frame = thread->last_frame();\n+\n+  assert(frame.is_compiled_frame() || frame.is_native_frame(), \"must be\");\n+  assert(frame.cb() == nm, \"must be\");\n+  frame = frame.sender(&reg_map);\n+\n+  LogTarget(Trace, nmethod, barrier) out;\n+  if (out.is_enabled()) {\n+    Thread* thread = Thread::current();\n+    assert(thread->is_Java_thread(), \"must be JavaThread\");\n+    JavaThread* jth = (JavaThread*) thread;\n+    ResourceMark mark;\n+    log_trace(nmethod, barrier)(\"deoptimize(nmethod: %s(%p), return_addr: %p, osr: %d, thread: %p(%s), making rsp: %p) -> %p\",\n+                                nm->method()->name_and_sig_as_C_string(),\n+                                nm, *(address *) return_address_ptr, nm->is_osr_method(), jth,\n+                                jth->get_thread_name(), frame.sp(), nm->verified_entry_point());\n+  }\n+\n+  new_frame->sp = frame.sp();\n+  new_frame->fp = frame.fp();\n+  new_frame->lr = frame.pc();\n+  new_frame->pc = SharedRuntime::get_handle_wrong_method_stub();\n+}\n+\n+\/\/ This is the offset of the entry barrier from where the frame is completed.\n+\/\/ If any code changes between the end of the verified entry where the entry\n+\/\/ barrier resides, and the completion of the frame, then\n+\/\/ NativeNMethodCmpBarrier::verify() will immediately complain when it does\n+\/\/ not find the expected native instruction at this offset, which needs updating.\n+\/\/ Note that this offset is invariant of PreserveFramePointer.\n+\n+static const int entry_barrier_offset = -4 * 11;\n+\n+static NativeNMethodBarrier* native_nmethod_barrier(nmethod* nm) {\n+  address barrier_address = nm->code_begin() + nm->frame_complete_offset() + entry_barrier_offset;\n+  NativeNMethodBarrier* barrier = reinterpret_cast<NativeNMethodBarrier*>(barrier_address);\n+  debug_only(barrier->verify());\n+  return barrier;\n@@ -34,1 +147,9 @@\n-  ShouldNotReachHere();\n+  if (!supports_entry_barrier(nm)) {\n+    return;\n+  }\n+\n+  \/\/ Disarms the nmethod guard emitted by BarrierSetAssembler::nmethod_entry_barrier.\n+  \/\/ Symmetric \"LDR; DMB ISHLD\" is in the nmethod barrier.\n+  NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);\n+\n+  barrier->set_value(disarmed_value());\n@@ -38,2 +159,6 @@\n-  ShouldNotReachHere();\n-  return false;\n+  if (!supports_entry_barrier(nm)) {\n+    return false;\n+  }\n+\n+  NativeNMethodBarrier* barrier = native_nmethod_barrier(nm);\n+  return barrier->get_value() != disarmed_value();\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/shared\/barrierSetNMethod_aarch64.cpp","additions":130,"deletions":5,"binary":false,"changes":135,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -26,3 +26,0 @@\n-#include \"runtime\/globals.hpp\"\n-#include \"runtime\/globals_extension.hpp\"\n-#include \"utilities\/debug.hpp\"\n@@ -31,3 +28,1 @@\n-  \/\/ Disable class unloading - we don't support concurrent class unloading yet.\n-  FLAG_SET_DEFAULT(ClassUnloading, false);\n-  FLAG_SET_DEFAULT(ClassUnloadingWithConcurrentMark, false);\n+    \/\/ Does nothing\n","filename":"src\/hotspot\/cpu\/aarch64\/gc\/z\/zArguments_aarch64.cpp","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -3713,0 +3713,5 @@\n+void MacroAssembler::load_method_holder_cld(Register rresult, Register rmethod) {\n+  load_method_holder(rresult, rmethod);\n+  ldr(rresult, Address(rresult, InstanceKlass::class_loader_data_offset()));\n+}\n+\n@@ -3734,0 +3739,16 @@\n+\/\/ ((WeakHandle)result).resolve();\n+void MacroAssembler::resolve_weak_handle(Register rresult, Register rtmp) {\n+  assert_different_registers(rresult, rtmp);\n+  Label resolved;\n+\n+  \/\/ A null weak handle resolves to null.\n+  cbz(rresult, resolved);\n+\n+  \/\/ Only 64 bit platforms support GCs that require a tmp register\n+  \/\/ Only IN_HEAP loads require a thread_tmp register\n+  \/\/ WeakHandle::resolve is an indirection like jweak.\n+  access_load_at(T_OBJECT, IN_NATIVE | ON_PHANTOM_OOP_REF,\n+                 rresult, Address(rresult), rtmp, \/*tmp_thread*\/noreg);\n+  bind(resolved);\n+}\n+\n@@ -4108,3 +4129,3 @@\n-\/\/ immediate instrcutions, i.e. we are not going to patch this\n-\/\/ instruction while the code is being executed by another thread.  In\n-\/\/ that case we can use move immediates rather than the constant pool.\n+\/\/ immediate instructions and nmethod entry barriers are not enabled.\n+\/\/ i.e. we are not going to patch this instruction while the code is being\n+\/\/ executed by another thread.\n@@ -4125,1 +4146,5 @@\n-  if (! immediate) {\n+\n+  \/\/ nmethod entry barrier necessitate using the constant pool. They have to be\n+  \/\/ ordered with respected to oop accesses.\n+  \/\/ Using immediate literals would necessitate ISBs.\n+  if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL || !immediate) {\n@@ -4130,0 +4155,1 @@\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":30,"deletions":4,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2014, 2015, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n@@ -161,0 +161,2 @@\n+  using Assembler::ldrw;\n+  using Assembler::strw;\n@@ -794,0 +796,1 @@\n+  void load_method_holder_cld(Register rresult, Register rmethod);\n@@ -801,0 +804,1 @@\n+  void resolve_weak_handle(Register result, Register tmp);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/barrierSetAssembler.hpp\"\n@@ -727,0 +728,3 @@\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->c2i_entry_barrier(masm);\n+\n@@ -1511,0 +1515,3 @@\n+  BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n+  bs->nmethod_entry_barrier(masm);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2014, 2019, Red Hat Inc. All rights reserved.\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2020, Red Hat Inc. All rights reserved.\n@@ -4120,0 +4120,44 @@\n+    address generate_method_entry_barrier() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"nmethod_entry_barrier\");\n+\n+    Label deoptimize_label;\n+\n+    address start = __ pc();\n+\n+    __ set_last_Java_frame(sp, rfp, lr, rscratch1);\n+\n+    __ enter();\n+    __ add(rscratch2, sp, wordSize);  \/\/ rscratch2 points to the saved lr\n+\n+    __ sub(sp, sp, 4 * wordSize);  \/\/ four words for the returned {sp, fp, lr, pc}\n+\n+    __ push_call_clobbered_registers();\n+\n+    __ mov(c_rarg0, rscratch2);\n+    __ call_VM_leaf\n+         (CAST_FROM_FN_PTR\n+          (address, BarrierSetNMethod::nmethod_stub_entry_barrier), 1);\n+\n+    __ reset_last_Java_frame(true);\n+\n+    __ mov(rscratch1, r0);\n+\n+    __ pop_call_clobbered_registers();\n+\n+    __ cbnz(rscratch1, deoptimize_label);\n+\n+    __ leave();\n+    __ ret(lr);\n+\n+    __ BIND(deoptimize_label);\n+\n+    __ ldp(\/* new sp *\/ rscratch1, rfp, Address(sp, 0 * wordSize));\n+    __ ldp(lr, \/* new pc*\/ rscratch2, Address(sp, 2 * wordSize));\n+\n+    __ mov(sp, rscratch1);\n+    __ br(rscratch2);\n+\n+    return start;\n+  }\n+\n@@ -5853,0 +5897,4 @@\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != NULL) {\n+      StubRoutines::aarch64::_method_entry_barrier = generate_method_entry_barrier();\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":50,"deletions":2,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -59,0 +59,1 @@\n+address StubRoutines::aarch64::_method_entry_barrier = NULL;\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -72,0 +72,3 @@\n+\n+  static address _method_entry_barrier;\n+\n@@ -174,0 +177,4 @@\n+  static address method_entry_barrier() {\n+    return _method_entry_barrier;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":8,"deletions":1,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2018, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2018, 2020, Oracle and\/or its affiliates. All rights reserved.\n@@ -67,0 +67,10 @@\n+\n+  \/\/ Diagnostic option to force deoptimization 1 in 3 times. It is otherwise\n+  \/\/ a very rare event.\n+  if (DeoptimizeNMethodBarriersALot) {\n+    static volatile uint32_t counter=0;\n+    if (Atomic::add(1u, &counter) % 3 == 0) {\n+      may_enter = false;\n+    }\n+  }\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/barrierSetNMethod.cpp","additions":11,"deletions":1,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2467,1 +2467,5 @@\n-          \"Use platform unstable time where supported for timestamps only\")\n+          \"Use platform unstable time where supported for timestamps only\") \\\n+                                                                            \\\n+  diagnostic(bool, DeoptimizeNMethodBarriersALot, false,                    \\\n+                \"Make nmethod barriers deoptimise a lot.\")                  \\\n+\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2883,1 +2883,6 @@\n-\n+#if defined(AARCH64)\n+      \/\/ On AArch64 with ZGC and nmethod entry barriers, we need all oops to be\n+      \/\/ in the constant pool to ensure ordering between the barrier and oops\n+      \/\/ accesses. For native_wrappers we need a constant.\n+      buffer.initialize_consts_size(8);\n+#endif\n","filename":"src\/hotspot\/share\/runtime\/sharedRuntime.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -38,0 +38,12 @@\n+\n+\/*\n+ * @test TestGCBasherDeoptWithZ\n+ * @key gc stress\n+ * @library \/\n+ * @requires vm.gc.Z\n+ * @requires vm.flavor == \"server\" & !vm.emulatedClient & !vm.graal.enabled & vm.opt.ClassUnloading != false\n+ * @summary Stress ZGC with nmethod barrier forced deoptimization enabled.\n+ * @run main\/othervm\/timeout=200 -Xlog:gc*=info,nmethod+barrier=trace -Xmx384m -server -XX:+UseZGC\n+ *  -XX:+UnlockDiagnosticVMOptions -XX:+DeoptimizeNMethodBarriersALot -XX:-Inline\n+ *   gc.stress.gcbasher.TestGCBasherWithZ 120000\n+ *\/\n","filename":"test\/hotspot\/jtreg\/gc\/stress\/gcbasher\/TestGCBasherWithZ.java","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"}]}