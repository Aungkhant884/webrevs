{"files":[{"patch":"@@ -7339,1 +7339,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -7349,0 +7349,26 @@\n+instruct loadNKlassLilliput(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  effect(TEMP_DEF dst, KILL cr);\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":27,"deletions":1,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -236,0 +237,7 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  assert(UseCompactObjectHeaders, \"Only use with compact object headers\");\n+  __ bind(_entry);\n+  Register d = _result->as_register();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(_continuation);\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1232,1 +1233,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2355,2 +2356,2 @@\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n+        __ load_nklass(tmp, src);\n+        __ load_nklass(rscratch1, dst);\n@@ -2359,2 +2360,2 @@\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n+        __ ldr(tmp, Address(src, oopDesc::klass_offset_in_bytes()));\n+        __ ldr(rscratch1, Address(dst, oopDesc::klass_offset_in_bytes()));\n@@ -2484,3 +2485,0 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp);\n-    }\n@@ -2489,8 +2487,1 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2498,7 +2489,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, tmp, rscratch1);\n@@ -2507,7 +2492,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2594,1 +2573,12 @@\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+      __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+      __ tst(result, markWord::monitor_value);\n+      __ br(Assembler::NE, *op->stub()->entry());\n+      __ bind(*op->stub()->continuation());\n+\n+      \/\/ Shift to get proper narrow Klass*.\n+      __ lsr(result, result, markWord::klass_shift);\n+    } else {\n+      __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":21,"deletions":31,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -183,7 +183,3 @@\n-  \/\/ This assumes that all prototype bits fit in an int32_t\n-  mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n-  str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n-\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    encode_klass_not_null(t1, klass);\n-    strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  if (UseCompactObjectHeaders) {\n+    ldr(t1, Address(klass, Klass::prototype_header_offset()));\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -191,1 +187,10 @@\n-    str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    \/\/ This assumes that all prototype bits fit in an int32_t\n+    mov(t1, (int32_t)(intptr_t)markWord::prototype().value());\n+    str(t1, Address(obj, oopDesc::mark_offset_in_bytes()));\n+\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      encode_klass_not_null(t1, klass);\n+      strw(t1, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    } else {\n+      str(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -196,1 +201,1 @@\n-  } else if (UseCompressedClassPointers) {\n+  } else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -215,0 +220,6 @@\n+  \/\/ Zero first 4 bytes, if start offset is not word aligned.\n+  if (!is_aligned(hdr_size_in_bytes, BytesPerWord)) {\n+    strw(zr, Address(obj, hdr_size_in_bytes));\n+    hdr_size_in_bytes += BytesPerInt;\n+  }\n+\n@@ -274,1 +285,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, int f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, int f, Register klass, Label& slow_case) {\n@@ -287,1 +298,1 @@\n-  mov(arr_size, (int32_t)header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  mov(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -296,1 +307,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, t1, t2);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, t1, t2);\n@@ -316,1 +327,5 @@\n-  assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  if (UseCompactObjectHeaders) {\n+    assert(!MacroAssembler::needs_explicit_null_check(oopDesc::mark_offset_in_bytes()), \"must add explicit null check\");\n+  } else {\n+    assert(!MacroAssembler::needs_explicit_null_check(oopDesc::klass_offset_in_bytes()), \"must add explicit null check\");\n+  }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":28,"deletions":13,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -291,1 +291,0 @@\n-\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -94,0 +94,11 @@\n+int C2LoadNKlassStub::max_size() const {\n+  return 8;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ ldr(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ b(continuation());\n+}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4317,0 +4318,24 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    return;\n+  }\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4319,1 +4344,5 @@\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(dst, src);\n+    } else {\n+      ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4357,0 +4386,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -4358,1 +4388,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4527,0 +4561,14 @@\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeMovk: return \"movk\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n@@ -4528,2 +4576,4 @@\n-  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4531,2 +4581,5 @@\n-  if (_klass_decode_mode != KlassDecodeNone) {\n-    return _klass_decode_mode;\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n@@ -4534,0 +4587,6 @@\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n@@ -4535,2 +4594,1 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+  const uint64_t base_u64 = (uint64_t) base;\n@@ -4538,2 +4596,2 @@\n-  if (CompressedKlassPointers::base() == nullptr) {\n-    return (_klass_decode_mode = KlassDecodeZero);\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n@@ -4542,7 +4600,3 @@\n-  if (operand_valid_for_logical_immediate(\n-        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n-    const uint64_t range_mask =\n-      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n-    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n-      return (_klass_decode_mode = KlassDecodeXor);\n-    }\n+  if (operand_valid_for_logical_immediate(false, base_u64) &&\n+      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n+    return KlassDecodeXor;\n@@ -4551,4 +4605,4 @@\n-  const uint64_t shifted_base =\n-    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n-  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n-            \"compressed class base bad alignment\");\n+  const uint64_t shifted_base = base_u64 >> LogKlassAlignmentInBytes;\n+  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n+    return KlassDecodeMovk;\n+  }\n@@ -4556,1 +4610,1 @@\n-  return (_klass_decode_mode = KlassDecodeMovk);\n+  return KlassDecodeNone;\n@@ -4560,0 +4614,2 @@\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4562,5 +4618,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    lsr(dst, src, LogKlassAlignmentInBytes);\n@@ -4570,6 +4622,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    lsr(dst, dst, LogKlassAlignmentInBytes);\n@@ -4579,5 +4627,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n-    } else {\n-      movw(dst, src);\n-    }\n+    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n@@ -4599,0 +4643,2 @@\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n@@ -4601,5 +4647,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    if (dst != src) mov(dst, src);\n@@ -4609,6 +4651,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    lsl(dst, src, LogKlassAlignmentInBytes);\n+    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n@@ -4621,0 +4659,3 @@\n+    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n+    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n+\n@@ -4623,5 +4664,1 @@\n-\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n-    }\n-\n+    lsl(dst, dst, LogKlassAlignmentInBytes);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":91,"deletions":54,"binary":false,"changes":145,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+ public:\n+\n@@ -96,1 +98,9 @@\n-  KlassDecodeMode klass_decode_mode();\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n@@ -852,0 +862,1 @@\n+  void load_nklass(Register dst, Register src);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":12,"deletions":1,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -3571,1 +3571,1 @@\n-    __ sub(r3, r3, sizeof(oopDesc));\n+    __ sub(r3, r3, oopDesc::base_offset_in_bytes());\n@@ -3576,1 +3576,6 @@\n-      __ add(r2, r0, sizeof(oopDesc));\n+      __ add(r2, r0, oopDesc::base_offset_in_bytes());\n+      if (!is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong)) {\n+        __ strw(zr, Address(__ post(r2, BytesPerInt)));\n+        __ sub(r3, r3, BytesPerInt);\n+        __ cbz(r3, initialize_header);\n+      }\n@@ -3586,5 +3591,8 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n+    if (UseCompactObjectHeaders) {\n+      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+      __ store_klass(r0, r4);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -974,1 +974,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -167,1 +167,1 @@\n-                                       int header_size, int element_size,\n+                                       int header_size_in_bytes, int element_size,\n@@ -170,1 +170,0 @@\n-  const int header_size_in_bytes = header_size * BytesPerWord;\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -4482,9 +4482,0 @@\n-void MacroAssembler::test_bit(Register Rd, Register Rs, uint32_t bit_pos, Register tmp) {\n-  assert(bit_pos < 64, \"invalid bit range\");\n-  if (UseZbs) {\n-    bexti(Rd, Rs, bit_pos);\n-    return;\n-  }\n-  andi(Rd, Rs, 1UL << bit_pos, tmp);\n-}\n-\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1685,1 +1685,1 @@\n-      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1638,1 +1638,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3073,0 +3073,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3264,7 +3265,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klass(src, dst, tmp, tmp2);\n@@ -3430,4 +3425,1 @@\n-\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3436,2 +3428,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3440,2 +3431,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3537,0 +3527,14 @@\n+  if (UseCompactObjectHeaders) {\n+    Register tmp = rscratch1;\n+    assert_different_registers(tmp, obj);\n+    assert_different_registers(tmp, result);\n+\n+    \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+    __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+    __ bind(*op->stub()->continuation());\n+    \/\/ Fast-path: shift and decode Klass*.\n+    __ shrq(result, markWord::klass_shift);\n+    __ decode_klass_not_null(result, tmp);\n+  } else\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":20,"deletions":16,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -166,2 +166,6 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  if (UseCompactObjectHeaders) {\n+    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+  } else {\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -169,5 +173,5 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, rscratch1);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      movptr(t1, klass);\n+      encode_klass_not_null(t1, rscratch1);\n+      movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n+    } else\n@@ -175,2 +179,3 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    {\n+      movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    }\n@@ -178,1 +183,0 @@\n-\n@@ -183,1 +187,1 @@\n-  else if (UseCompressedClassPointers) {\n+  else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -225,0 +229,1 @@\n+    int hdr_size_aligned = align_up(hdr_size_in_bytes, BytesPerWord); \/\/ klass gap is already cleared by init_header().\n@@ -227,1 +232,1 @@\n-      initialize_body(obj, index, hdr_size_in_bytes, t1_zero);\n+      initialize_body(obj, index, hdr_size_aligned, t1_zero);\n@@ -232,1 +237,1 @@\n-      for (int i = hdr_size_in_bytes; i < con_size_in_bytes; i += BytesPerWord)\n+      for (int i = hdr_size_aligned; i < con_size_in_bytes; i += BytesPerWord)\n@@ -234,1 +239,1 @@\n-    } else if (con_size_in_bytes > hdr_size_in_bytes) {\n+    } else if (con_size_in_bytes > hdr_size_aligned) {\n@@ -239,1 +244,1 @@\n-      movptr(index, (con_size_in_bytes - hdr_size_in_bytes) >> 3);\n+      movptr(index, (con_size_in_bytes - hdr_size_aligned) >> 3);\n@@ -241,1 +246,1 @@\n-      if (((con_size_in_bytes - hdr_size_in_bytes) & 4) != 0)\n+      if (((con_size_in_bytes - hdr_size_aligned) & 4) != 0)\n@@ -246,1 +251,1 @@\n-        movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (1*BytesPerWord)),\n+        movptr(Address(obj, index, Address::times_8, hdr_size_aligned - (1*BytesPerWord)),\n@@ -248,1 +253,1 @@\n-        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (2*BytesPerWord)),\n+        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_aligned - (2*BytesPerWord)),\n@@ -264,1 +269,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -277,1 +282,1 @@\n-  movptr(arr_size, header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -287,1 +292,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":26,"deletions":21,"binary":false,"changes":47,"status":"modified"},{"patch":"@@ -96,0 +96,11 @@\n+\n+int C2LoadNKlassStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(continuation());\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -666,1 +666,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -668,7 +668,0 @@\n-  \/\/ If we weren't able to swing _owner from null to the BasicLock\n-  \/\/ then take the slow path.\n-  jccb  (Assembler::notZero, NO_COUNT);\n-  \/\/ update _owner from BasicLock to thread\n-  get_thread (scrReg);                    \/\/ beware: clobbers ICCs\n-  movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);\n-  xorptr(boxReg, boxReg);                 \/\/ set icc.ZFlag = 1 to indicate success\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":1,"deletions":8,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4172,1 +4173,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -4178,0 +4179,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -4190,1 +4204,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -5122,0 +5135,22 @@\n+#ifdef _LP64\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    return;\n+  }\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5127,1 +5162,1 @@\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    load_nklass(dst, src);\n@@ -5135,0 +5170,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5143,1 +5179,40 @@\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+   movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+}\n+\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    load_nklass(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(src, dst, tmp1, tmp2);\n+    load_nklass(tmp1, src);\n+    load_nklass(tmp2, dst);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n@@ -5350,0 +5425,61 @@\n+MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n+\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeAdd:  return \"add\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  }\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n+  }\n+\n+  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n+    return KlassDecodeXor;\n+  }\n+\n+  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n+  \/\/ will use larger shifts\n+\n+  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n+  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n+  \/\/  encoding base. This does not matter in reality since the shift values we use for\n+  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n+  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n+  \/\/  either CDS or CCS.\n+  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n+    return KlassDecodeAdd;\n+  }\n+\n+  return KlassDecodeNone;\n+}\n+\n@@ -5352,1 +5488,12 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+    xorq(r, tmp);\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5355,0 +5502,2 @@\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5356,3 +5505,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5364,1 +5512,13 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    xorq(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5367,2 +5527,2 @@\n-  } else {\n-    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n@@ -5370,3 +5530,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5378,8 +5537,5 @@\n-  \/\/ Note: it will change flags\n-  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n-  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n-  \/\/ Also do not verify_oop as this is called by verify_oop.\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5387,2 +5543,11 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n+    xorq(r, tmp);\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n@@ -5390,0 +5555,4 @@\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -5395,3 +5564,1 @@\n-  \/\/ Note: it will change flags\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5401,18 +5568,28 @@\n-  if (CompressedKlassPointers::base() == nullptr &&\n-      CompressedKlassPointers::shift() == 0) {\n-    \/\/ The best case scenario is that there is no base or shift. Then it is already\n-    \/\/ a pointer that needs nothing but a register rename.\n-    movl(dst, src);\n-  } else {\n-    if (CompressedKlassPointers::base() != nullptr) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n-    } else {\n-      addq(dst, src);\n-    }\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    xorq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    addq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":222,"deletions":45,"binary":false,"changes":267,"status":"modified"},{"patch":"@@ -82,0 +82,22 @@\n+ public:\n+\n+  enum KlassDecodeMode {\n+    KlassDecodeNone,\n+    KlassDecodeZero,\n+    KlassDecodeXor,\n+    KlassDecodeAdd\n+  };\n+\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+\n+ private:\n+  static KlassDecodeMode _klass_decode_mode;\n+\n@@ -366,0 +388,3 @@\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#endif\n@@ -369,0 +394,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses t1 and t2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":33,"deletions":0,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -5317,0 +5317,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -5327,0 +5328,23 @@\n+instruct loadNKlassLilliput(rRegN dst, indOffset8 mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(TEMP_DEF dst, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ movq(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(dst, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, stub->entry());\n+    __ bind(stub->continuation());\n+    __ shrq(dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n@@ -12679,0 +12703,1 @@\n+  predicate(!UseCompactObjectHeaders);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":25,"deletions":0,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -175,0 +175,1 @@\n+\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1244,1 +1244,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = UseCompactObjectHeaders ? new LoadKlassStub(klass) : nullptr;\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -220,2 +222,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See ArchiveBuilder::make_shallow_copies: make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, KlassAlignmentInBytes) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -607,4 +611,5 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n-    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n-    \/\/ in systemDictionaryShared.cpp.\n+    \/\/ Reserve space for a pointer immediately in front of an InstanceKlass. That space will\n+    \/\/ later be used to store the RuntimeClassInfo* pointer directly in front of the archived\n+    \/\/ InstanceKlass, in order to have a quick lookup InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\/::set_for() for\n+    \/\/ details.\n@@ -616,0 +621,3 @@\n+    dest = dump_region->allocate(bytes, KlassAlignmentInBytes);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -617,1 +625,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -636,1 +643,2 @@\n-  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d (%s)\", p2i(src), p2i(dest), bytes,\n+                 MetaspaceObj::type_name(ref->msotype()));\n@@ -640,0 +648,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -720,0 +730,7 @@\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      Klass* requested_k = to_requested(k);\n+      narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+    }\n+#endif \/\/_LP64\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":25,"deletions":8,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -46,3 +47,13 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ CDS has three alignments to deal with:\n+\/\/ - SharedSpaceObjectAlignment, always 8 bytes: used for placing arbitrary structures.\n+\/\/   These may contain 64-bit members (not larger, we know that much). Therefore we\n+\/\/   need to use 64-bit alignment on both 32-bit and 64-bit platforms. We reuse metaspace\n+\/\/   minimal alignment for this, which follows the same logic.\n+\/\/ - With CompressedClassPointers=1, we need to store Klass structures with a large\n+\/\/   alignment (Lilliput specific narrow Klass pointer encoding) - KlassAlignmentInBytes.\n+\/\/ - Header data and tags are squeezed in with word alignment, which happens to be 4 bytes\n+\/\/   on 32-bit. See ReadClosure::do_xxx() and DumpRegion::append_intptr().\n+const int SharedSpaceObjectAlignment = metaspace::MetaspaceMinAlignmentBytes;\n+\n+\/\/ standard alignment should be sufficient for storing 64-bit values.\n+STATIC_ASSERT(SharedSpaceObjectAlignment >= sizeof(uint64_t));\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -79,0 +79,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -1527,0 +1528,2 @@\n+  GCForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -213,0 +214,1 @@\n+  GCForwarding::begin();\n@@ -225,0 +227,2 @@\n+  GCForwarding::end();\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -44,1 +45,1 @@\n-  if (obj->is_forwarded()) {\n+  if (GCForwarding::is_forwarded(obj)) {\n@@ -55,2 +56,2 @@\n-  assert(obj->is_forwarded(), \"Sanity!\");\n-  assert(obj->forwardee() != obj, \"Object must have a new location\");\n+  assert(GCForwarding::is_forwarded(obj), \"Sanity!\");\n+  assert(GCForwarding::forwardee(obj) != obj, \"Object must have a new location\");\n@@ -61,1 +62,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n@@ -123,1 +124,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -105,2 +106,2 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n-    assert(object->is_forwarded(), \"must be forwarded\");\n+    GCForwarding::forward_to(object, cast_to_oop(_compaction_top));\n+    assert(GCForwarding::is_forwarded(object), \"must be forwarded\");\n@@ -108,1 +109,1 @@\n-    assert(!object->is_forwarded(), \"must not be forwarded\");\n+    assert(GCForwarding::is_not_forwarded(object), \"must not be forwarded\");\n@@ -171,2 +172,2 @@\n-  obj->forward_to(cast_to_oop(dest_hr->bottom()));\n-  assert(obj->is_forwarded(), \"Object must be forwarded!\");\n+  GCForwarding::forward_to(obj, cast_to_oop(dest_hr->bottom()));\n+  assert(GCForwarding::is_forwarded(obj), \"Object must be forwarded!\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -105,1 +105,1 @@\n-  if (obj->is_forwarded()) {\n+  if (GCForwarding::is_forwarded(obj)) {\n@@ -108,1 +108,1 @@\n-    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n+    if (cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj)) < _dense_prefix_top) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -231,1 +231,1 @@\n-      forwardee = cast_to_oop(m.decode_pointer());\n+      forwardee = obj->forwardee(m);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -223,1 +223,0 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n@@ -253,1 +252,0 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n@@ -380,1 +378,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  oop const old, Klass* klass, size_t word_sz, uint age,\n@@ -384,1 +382,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -388,1 +386,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -396,0 +394,1 @@\n+                                                   Klass* klass,\n@@ -418,1 +417,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, old, klass, word_sz, age, obj_ptr, node_index);\n@@ -453,0 +452,4 @@\n+  if (old_mark.is_marked()) {\n+    \/\/ Already forwarded by somebody else, return forwardee.\n+    return old->forwardee(old_mark);\n+  }\n@@ -455,1 +458,9 @@\n-  Klass* klass = old->klass();\n+  Klass* klass;\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    klass = old_mark.safe_klass();\n+  } else\n+#endif\n+  {\n+    klass = old->klass();\n+  }\n@@ -468,1 +479,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, old, klass, word_sz, age, node_index);\n@@ -623,1 +634,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":21,"deletions":10,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -165,0 +165,1 @@\n+                               Klass* klass,\n@@ -199,1 +200,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              oop const old, Klass* klass, size_t word_sz, uint age,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -90,0 +91,2 @@\n+  GCForwarding::begin();\n+\n@@ -102,0 +105,2 @@\n+  GCForwarding::end();\n+\n@@ -161,0 +166,3 @@\n+  _young_marked_objects = 0;\n+  _old_marked_objects = 0;\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -135,0 +136,2 @@\n+  GCForwarding::initialize(_reserved, SpaceAlignment);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -406,0 +407,2 @@\n+  GCForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words());\n+\n@@ -952,1 +955,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1294,0 +1297,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2002,0 +2002,3 @@\n+#ifdef _LP64\n+              oopDesc::release_set_mark(result, ik->prototype_header());\n+#else\n@@ -2003,2 +2006,1 @@\n-              oopDesc::set_klass_gap(result, 0);\n-\n+#endif\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -69,1 +69,2 @@\n-    _store->push(ObjectSampleMarkWord(obj, obj->mark()));\n+    markWord mark = obj->mark();\n+    _store->push(ObjectSampleMarkWord(obj, mark));\n@@ -73,1 +74,6 @@\n-    obj->set_mark(markWord::prototype().set_marked());\n+#ifdef _LP64\n+    if (mark.has_displaced_mark_helper()) {\n+      mark = mark.displaced_mark_helper();\n+    }\n+#endif\n+    obj->set_mark(markWord::prototype().set_marked() LP64_ONLY(.set_narrow_klass(mark.narrow_klass())));\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/objectSampleMarker.hpp","additions":8,"deletions":2,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2376,1 +2376,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -291,1 +291,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedOops.hpp\"\n@@ -108,0 +109,9 @@\n+    out->cr();\n+    out->print_cr(\"KlassAlignmentInBytes: %d\", KlassAlignmentInBytes);\n+    out->print(\"KlassEncodingMetaspaceMax: \");\n+#ifdef _LP64\n+    \/\/ TODO: This currently doesn't compile on 32bit because of size_t overflow.\n+    print_human_readable_size(out, KlassEncodingMetaspaceMax, scale);\n+    out->cr();\n+#endif\n+    CompressedKlassPointers::print_mode(out);\n@@ -109,1 +119,1 @@\n-    out->print(\"No class space\");\n+    out->print_cr(\"No class space\");\n@@ -111,1 +121,0 @@\n-  out->cr();\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceReporter.cpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -677,0 +679,7 @@\n+  markWord prototype_header() const      {\n+    assert(UseCompactObjectHeaders, \"only use with compact object headers\");\n+    return _prototype_header;\n+  }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -29,1 +29,1 @@\n-#include \"oops\/oopsHierarchy.hpp\"\n+#include \"oops\/compressedKlass.hpp\"\n@@ -44,1 +44,1 @@\n-\/\/  unused:25 hash:31 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n+\/\/  nklass:32 hash:25 -->| unused_gap:1  age:4  unused_gap:1  lock:2 (normal object)\n@@ -106,4 +106,6 @@\n-  static const int first_unused_gap_bits          = 1;\n-  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - first_unused_gap_bits;\n-  static const int hash_bits                      = max_hash_bits > 31 ? 31 : max_hash_bits;\n-  static const int second_unused_gap_bits         = LP64_ONLY(1) NOT_LP64(0);\n+  static const int self_forwarded_bits            = 1;\n+  static const int max_hash_bits                  = BitsPerWord - age_bits - lock_bits - self_forwarded_bits;\n+  static const int hash_bits                      = max_hash_bits > 25 ? 25 : max_hash_bits;\n+#ifdef _LP64\n+  static const int klass_bits                     = 32;\n+#endif\n@@ -112,2 +114,6 @@\n-  static const int age_shift                      = lock_bits + first_unused_gap_bits;\n-  static const int hash_shift                     = age_shift + age_bits + second_unused_gap_bits;\n+  static const int self_forwarded_shift           = lock_shift + lock_bits;\n+  static const int age_shift                      = self_forwarded_shift + self_forwarded_bits;\n+  static const int hash_shift                     = age_shift + age_bits;\n+#ifdef _LP64\n+  static const int klass_shift                    = hash_shift + hash_bits;\n+#endif\n@@ -117,0 +123,2 @@\n+  static const uintptr_t self_forwarded_mask      = right_n_bits(self_forwarded_bits);\n+  static const uintptr_t self_forwarded_mask_in_place = self_forwarded_mask << self_forwarded_shift;\n@@ -122,0 +130,5 @@\n+#ifdef _LP64\n+  static const uintptr_t klass_mask               = right_n_bits(klass_bits);\n+  static const uintptr_t klass_mask_in_place      = klass_mask << klass_shift;\n+#endif\n+\n@@ -250,0 +263,9 @@\n+#ifdef _LP64\n+  inline Klass* klass() const;\n+  inline Klass* klass_or_null() const;\n+  inline Klass* safe_klass() const;\n+  inline markWord set_klass(const Klass* klass) const;\n+  inline narrowKlass narrow_klass() const;\n+  inline markWord set_narrow_klass(const narrowKlass klass) const;\n+#endif\n+\n@@ -263,0 +285,8 @@\n+\n+  inline bool self_forwarded() const {\n+    return mask_bits(value(), self_forwarded_mask_in_place) != 0;\n+  }\n+\n+  inline markWord set_self_forwarded() const {\n+    return markWord(value() | self_forwarded_mask_in_place | marked_value);\n+  }\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":38,"deletions":8,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -158,2 +160,3 @@\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n+  \/\/ Only has a klass gap when compressed class pointers are used, but\n+  \/\/ only if not using compact headers..\n+  return UseCompressedClassPointers && !UseCompactObjectHeaders;\n@@ -166,1 +169,5 @@\n-  _metadata._compressed_klass = nk;\n+  if (UseCompactObjectHeaders) {\n+    set_mark(mark().set_narrow_klass(nk));\n+  } else {\n+    _metadata._compressed_klass = nk;\n+  }\n@@ -171,7 +178,8 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return nullptr;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_metadata._klass;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":18,"deletions":10,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,1 +38,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -41,0 +42,3 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -73,0 +77,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -81,0 +89,10 @@\n+markWord oopDesc::resolve_mark() const {\n+  assert(LockingMode != LM_LEGACY, \"Not safe with legacy stack-locking\");\n+  markWord hdr = mark();\n+  if (hdr.has_monitor()) {\n+    ObjectMonitor* monitor = hdr.monitor();\n+    return monitor->header();\n+  }\n+  return hdr;\n+}\n+\n@@ -82,0 +100,7 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord header = resolve_mark();\n+    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"expect compressed klass pointers\");\n+    set_mark(markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value()));\n+  } else\n+#endif\n@@ -86,1 +111,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (CompressedKlassPointers::use_compact_object_headers()) {\n+    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass();\n+  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -88,3 +118,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -94,1 +124,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (CompressedKlassPointers::use_compact_object_headers()) {\n+    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass_or_null();\n+  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -96,3 +131,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -102,6 +137,14 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n-  }\n+#ifdef _LP64\n+  if (CompressedKlassPointers::use_compact_object_headers()) {\n+    assert(CompressedKlassPointers::use_compressed_class_pointers(), \"only with compressed class pointers\");\n+    markWord header = mark_acquire();\n+    if (header.has_monitor()) {\n+      header = header.monitor()->header();\n+    }\n+    return header.klass_or_null();\n+  } else if (CompressedKlassPointers::use_compressed_class_pointers()) {\n+     narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n+     return CompressedKlassPointers::decode(nklass);\n+  } else\n+#endif\n+  return Atomic::load_acquire(&_metadata._klass);\n@@ -111,5 +154,1 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_raw(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+  return klass();\n@@ -119,2 +158,3 @@\n-  assert(Universe::is_bootstrapping() || (k != nullptr && k->is_klass()), \"incorrect Klass\");\n-  if (UseCompressedClassPointers) {\n+  assert(Universe::is_bootstrapping() || (k != NULL && k->is_klass()), \"incorrect Klass\");\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n+  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -128,1 +168,2 @@\n-  assert(Universe::is_bootstrapping() || (k != nullptr && k->is_klass()), \"incorrect Klass\");\n+  assert(Universe::is_bootstrapping() || (k != NULL && k->is_klass()), \"incorrect Klass\");\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -130,1 +171,1 @@\n-  if (UseCompressedClassPointers) {\n+  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -139,1 +180,2 @@\n-  if (UseCompressedClassPointers) {\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n+  if (CompressedKlassPointers::use_compressed_class_pointers()) {\n@@ -270,1 +312,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -274,0 +316,16 @@\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  set_mark(m);\n+#else\n+  forward_to(oop(this));\n+#endif\n+}\n+\n@@ -276,1 +334,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -281,1 +339,1 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n@@ -285,0 +343,22 @@\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n+  }\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n+}\n+\n@@ -289,2 +369,14 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -345,1 +437,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":122,"deletions":31,"binary":false,"changes":153,"status":"modified"},{"patch":"@@ -40,3 +40,0 @@\n-\/\/ If compressed klass pointers then use narrowKlass.\n-typedef juint  narrowKlass;\n-\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -102,0 +102,10 @@\n+\n+class C2LoadNKlassStub : public C2CodeStub {\n+private:\n+  Register _dst;\n+public:\n+  C2LoadNKlassStub(Register dst) : C2CodeStub(), _dst(dst) {}\n+  Register dst() { return _dst; }\n+  int max_size() const;\n+  void emit(C2_MacroAssembler& masm);\n+};\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -323,1 +323,1 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n+    const size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n@@ -325,1 +325,1 @@\n-    const size_t aligned_hs = align_object_offset(hs);\n+    const size_t aligned_hs_bytes = align_up(hs_bytes, BytesPerLong);\n@@ -327,2 +327,2 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (aligned_hs_bytes > hs_bytes) {\n+      Copy::zero_to_bytes(obj + hs_bytes, aligned_hs_bytes - hs_bytes);\n@@ -331,0 +331,1 @@\n+    const size_t aligned_hs = aligned_hs_bytes \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -1490,0 +1490,17 @@\n+\n+  if (UseCompactObjectHeaders) {\n+    \/\/ 512 byte alignment, 22-bit values (Lilliput)\n+    LogKlassAlignmentInBytes = 9;\n+    MaxNarrowKlassPointerBits = 22;\n+  } else {\n+    \/\/ Traditional: 8 byte alignment, 32-bit values\n+    LogKlassAlignmentInBytes = 3;\n+    MaxNarrowKlassPointerBits = 32;\n+  }\n+\n+  KlassAlignmentInBytes = 1 << LogKlassAlignmentInBytes;\n+  assert(is_aligned(KlassAlignmentInBytes, BytesPerWord), \"Must be at least word-sized\");\n+  KlassAlignmentInWords = KlassAlignmentInBytes \/ BytesPerWord;\n+  NarrowKlassPointerBitMask = ((((uint64_t)1) << MaxNarrowKlassPointerBits) - 1);\n+  KlassEncodingMetaspaceMax = UCONST64(1) << (MaxNarrowKlassPointerBits + LogKlassAlignmentInBytes);\n+\n@@ -1508,1 +1525,10 @@\n-#endif \/\/ _LP64\n+\n+  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n+  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n+  \/\/ case the default is wrong.\n+  \/\/ TODO: This is placed wrong. The CompressedClassSpaceSizeFunc is done after ergo, but this\n+  \/\/ assert is during ergo.\n+  \/\/ assert(CompressedClassSpaceSize <= Metaspace::max_class_space_size(),\n+  \/\/        \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\",\n+  \/\/        CompressedClassSpaceSize, Metaspace::max_class_space_size());\n+#endif\n@@ -3106,0 +3132,16 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders && FLAG_IS_CMDLINE(UseCompressedClassPointers) && !UseCompressedClassPointers) {\n+    \/\/ If user specifies -UseCompressedClassPointers, disable compact headers with a warning.\n+    warning(\"Compact object headers require compressed class pointers. Disabling compact object headers.\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+\n+  if (UseCompactObjectHeaders && LockingMode == LM_LEGACY) {\n+    FLAG_SET_DEFAULT(LockingMode, LM_LIGHTWEIGHT);\n+  }\n+\n+  if (!UseCompactObjectHeaders) {\n+    FLAG_SET_DEFAULT(UseSharedSpaces, false);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":43,"deletions":1,"binary":false,"changes":44,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -132,0 +132,3 @@\n+  product(bool, UseCompactObjectHeaders, true, EXPERIMENTAL,                \\\n+                \"Use 64-bit object headers instead of 96-bit headers\")      \\\n+                                                                            \\\n@@ -149,0 +152,1 @@\n+const bool UseCompactObjectHeaders = false;\n@@ -1059,1 +1063,1 @@\n-  develop(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1416,1 +1420,1 @@\n-          range(1*M, 3*G)                                                   \\\n+          constraint(CompressedClassSpaceSizeConstraintFunc, AfterErgo)     \\\n@@ -1418,1 +1422,1 @@\n-  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n+  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n@@ -1979,0 +1983,6 @@\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":14,"deletions":4,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -222,0 +222,1 @@\n+  static int header_offset_in_bytes()      { return offset_of(ObjectMonitor, _header); }\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -1701,0 +1702,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+  template(HeapObjectStatistics)                  \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -381,2 +381,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base_copy,                           address)                                        \\\n+     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n@@ -2592,0 +2592,1 @@\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -599,5 +599,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -611,5 +606,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -57,0 +57,3 @@\n+    if (VM.getVM().isLP64()) {\n+      klassShift          = db.lookupLongConstant(\"markWord::klass_shift\").longValue();\n+    }\n@@ -85,0 +88,1 @@\n+  private static long klassShift;\n@@ -105,0 +109,4 @@\n+  public static long getKlassShift() {\n+    return klassShift;\n+  }\n+\n@@ -184,0 +192,6 @@\n+  public Klass getKlass() {\n+    assert(VM.getVM().isCompactObjectHeadersEnabled());\n+    assert(!hasMonitor());\n+    return (Klass)Metadata.instantiateWrapperFor(addr.getCompKlassAddressAt(0));\n+  }\n+\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":14,"deletions":0,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -76,1 +76,2 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -173,0 +173,13 @@\n+\n+Lilliput temporary:\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java 8301785 generic-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-beyond-encoding-range-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-partly-within-encoding-range-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-within-encoding-range-use-zero 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-no-low-bits-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-with-low-bits-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-1 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-2 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-3 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassSpaceSize.java 8302094 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -387,2 +387,0 @@\n- -runtime\/Metaspace\/elastic\/TestMetaspaceAllocationMT1.java \\\n- -runtime\/Metaspace\/elastic\/TestMetaspaceAllocationMT2.java \\\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -25,0 +25,5 @@\n+\/\/ Note Lilliput:\n+\/\/ Tests rely on array members starting at the same offset, otherwise vectorization does not kick in. Not true\n+\/\/ for Lilliput.\n+\/\/ For now I just enforce -CompactObjectHeaders.\n+\n@@ -81,1 +86,1 @@\n-                                   \"-XX:LoopUnrollLimit=1000\");\n+                                   \"-XX:LoopUnrollLimit=1000\", \"-XX:+UnlockExperimentalVMOptions\", \"-XX:-UseCompactObjectHeaders\");\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -62,0 +62,34 @@\n+\n+\n+\/* @test id=ccs-on-compact-headers-on\n+ * @summary Run with +UseCCS and +UseCompactObjectHeaders\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -Xlog:metaspace* -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders -XX:+UseCompressedClassPointers -XX:VerifyMetaspaceInterval=3\n+ *\/\n+\n+\/* @test id=ccs-on-compact-headers-off\n+ * @summary Run with +UseCCS and -UseCompactObjectHeaders\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -Xlog:metaspace* -XX:+UnlockExperimentalVMOptions -XX:-UseCompactObjectHeaders -XX:+UseCompressedClassPointers -XX:VerifyMetaspaceInterval=3\n+ *\/\n+\n+\/* @test id=ccs-off\n+ * @summary Run with -UseCCS\n+ * @library \/test\/lib\n+ * @modules java.base\/jdk.internal.misc\n+ *          java.xml\n+ * @requires vm.bits == 64\n+ * @requires vm.flagless\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -Xlog:metaspace* -XX:-UseCompressedClassPointers -XX:VerifyMetaspaceInterval=3\n+ *\/\n+\n+\n+\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceGtests.java","additions":34,"deletions":0,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -51,11 +51,1 @@\n- * @test id=test-64bit-noccs\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dwithout-compressed-class-space -XX:MaxMetaspaceSize=201M -Xmx100M -XX:-UseCompressedOops -XX:-UseCompressedClassPointers PrintMetaspaceDcmd\n- *\/\n-\n- \/*\n- * @test id=test-nospecified\n+ * @test id=test-64bit-compact-headers\n@@ -64,0 +54,1 @@\n+ * @requires vm.debug == true\n@@ -67,1 +58,1 @@\n- * @run main\/othervm -Dno-specified-flag -Xmx100M -XX:-UseCompressedOops -XX:-UseCompressedClassPointers PrintMetaspaceDcmd\n+ * @run main\/othervm -Dwith-compressed-class-space -XX:+UnlockExperimentalVMOptions -XX:+UseCompactObjectHeaders -XX:MaxMetaspaceSize=201M -Xmx100M -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UnlockDiagnosticVMOptions -XX:+MetaspaceGuardAllocations PrintMetaspaceDcmd\n","filename":"test\/hotspot\/jtreg\/runtime\/Metaspace\/PrintMetaspaceDcmd.java","additions":3,"deletions":12,"binary":false,"changes":15,"status":"modified"}]}