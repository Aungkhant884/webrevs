{"files":[{"patch":"@@ -1791,12 +1791,0 @@\n-  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n-  if (UseFastLocking && max_monitors > 0) {\n-    C2CheckLockStackStub* stub = new (C->comp_arena()) C2CheckLockStackStub();\n-    C->output()->add_stub(stub);\n-    __ ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n-    __ ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n-    __ add(r9, r9, max_monitors * oopSize);\n-    __ cmp(r9, r10);\n-    __ br(Assembler::GE, stub->entry());\n-    __ bind(stub->continuation());\n-  }\n-\n@@ -3854,38 +3842,1 @@\n-    if (!UseHeavyMonitors) {\n-      if (UseFastLocking) {\n-        __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count, false);\n-\n-        \/\/ Indicate success at cont.\n-        __ cmp(oop, oop);\n-        __ b(count);\n-      } else {\n-        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-        __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-        \/\/ Initialize the box. (Must happen before we update the object mark!)\n-        __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-        \/\/ Compare object markWord with an unlocked value (tmp) and if\n-        \/\/ equal exchange the stack address of our box with object markWord.\n-        \/\/ On failure disp_hdr contains the possibly locked markWord.\n-        __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-                   \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-        __ br(Assembler::EQ, cont);\n-\n-        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-        \/\/ object, will have now locked it will continue at label cont\n-\n-        \/\/ Check if the owner is self by comparing the value in the\n-        \/\/ markWord of object (disp_hdr) with the stack pointer.\n-        __ mov(rscratch1, sp);\n-        __ sub(disp_hdr, disp_hdr, rscratch1);\n-        __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-        \/\/ If condition is true we are cont and hence we can store 0 as the\n-        \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-        __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-        __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-        __ b(cont);\n-      }\n-    } else {\n+    if (LockingMode == LM_MONITOR) {\n@@ -3894,0 +3845,33 @@\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+      \/\/ Initialize the box. (Must happen before we update the object mark!)\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Compare object markWord with an unlocked value (tmp) and if\n+      \/\/ equal exchange the stack address of our box with object markWord.\n+      \/\/ On failure disp_hdr contains the possibly locked markWord.\n+      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+      __ br(Assembler::EQ, cont);\n+\n+      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+      \/\/ object, will have now locked it will continue at label cont\n+\n+      \/\/ Check if the owner is self by comparing the value in the\n+      \/\/ markWord of object (disp_hdr) with the stack pointer.\n+      __ mov(rscratch1, sp);\n+      __ sub(disp_hdr, disp_hdr, rscratch1);\n+      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+      \/\/ If condition is true we are cont and hence we can store 0 as the\n+      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+      __ b(cont);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count);\n+      __ b(count);\n@@ -3907,1 +3891,1 @@\n-    if (!UseFastLocking) {\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n@@ -3947,1 +3931,1 @@\n-    if (!UseHeavyMonitors && !UseFastLocking) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -3960,17 +3944,1 @@\n-    if (!UseHeavyMonitors) {\n-      if (UseFastLocking) {\n-        __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n-\n-        \/\/ Indicate success at cont.\n-        __ cmp(oop, oop);\n-        __ b(count);\n-      } else {\n-        \/\/ Check if it is still a light weight lock, this is is true if we\n-        \/\/ see the stack address of the basicLock in the markWord of the\n-        \/\/ object.\n-\n-        __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-                   \/*release*\/ true, \/*weak*\/ false, tmp);\n-        __ b(cont);\n-      }\n-    } else {\n+    if (LockingMode == LM_MONITOR) {\n@@ -3979,0 +3947,12 @@\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Check if it is still a light weight lock, this is is true if we\n+      \/\/ see the stack address of the basicLock in the markWord of the\n+      \/\/ object.\n+\n+      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+      __ b(cont);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+      __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n+      __ b(count);\n@@ -3988,1 +3968,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -3994,1 +3974,1 @@\n-      __ tst(tmp2, (uint64_t)(intptr_t) ANONYMOUS_OWNER);\n+      __ tst(tmp2, (uint64_t)ObjectMonitor::ANONYMOUS_OWNER);\n@@ -4368,1 +4348,12 @@\n-  predicate(n->get_int() < (int)(BoolTest::unsigned_compare));\n+  predicate(!Matcher::is_unsigned_booltest_pred(n->get_int()));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ BoolTest condition for unsigned compare\n+operand immI_cmpU_cond()\n+%{\n+  predicate(Matcher::is_unsigned_booltest_pred(n->get_int()));\n@@ -4475,0 +4466,22 @@\n+\/\/ 5 bit signed integer\n+operand immI5()\n+%{\n+  predicate(Assembler::is_simm(n->get_int(), 5));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 7 bit unsigned integer\n+operand immIU7()\n+%{\n+  predicate(Assembler::is_uimm(n->get_int(), 7));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -4617,0 +4630,22 @@\n+\/\/ 5 bit signed long integer\n+operand immL5()\n+%{\n+  predicate(Assembler::is_simm(n->get_long(), 5));\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n+\/\/ 7 bit unsigned long integer\n+operand immLU7()\n+%{\n+  predicate(Assembler::is_uimm(n->get_long(), 7));\n+  match(ConL);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":107,"deletions":72,"binary":false,"changes":179,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,1 +48,1 @@\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+  assert(SharedRuntime::polling_page_return_handler_blob() != nullptr,\n@@ -345,1 +345,1 @@\n-  if (call == NULL) {\n+  if (call == nullptr) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_CodeStubs_aarch64.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -115,1 +115,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -126,1 +126,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -136,1 +136,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -246,1 +246,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n@@ -282,1 +282,1 @@\n-        __ stop(\"locked object is NULL\");\n+        __ stop(\"locked object is null\");\n@@ -332,1 +332,1 @@\n-  if (o == NULL) {\n+  if (o == nullptr) {\n@@ -340,1 +340,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n@@ -383,1 +383,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -431,1 +431,1 @@\n-  MonitorExitStub* stub = NULL;\n+  MonitorExitStub* stub = nullptr;\n@@ -435,1 +435,1 @@\n-    if (UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n@@ -459,1 +459,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -470,1 +470,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -491,1 +491,1 @@\n-  if (info->exception_handlers() != NULL) {\n+  if (info->exception_handlers() != nullptr) {\n@@ -513,1 +513,1 @@\n-  guarantee(info != NULL, \"Shouldn't be NULL\");\n+  guarantee(info != nullptr, \"Shouldn't be null\");\n@@ -608,1 +608,1 @@\n-        const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);\n+        const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, nullptr);\n@@ -615,1 +615,1 @@\n-      const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, NULL);\n+      const2reg(src, FrameMap::rscratch1_opr, lir_patch_none, nullptr);\n@@ -779,1 +779,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -851,1 +851,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -894,1 +894,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n@@ -947,1 +947,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -1057,3 +1057,3 @@\n-  assert(op->block() == NULL || op->block()->label() == op->label(), \"wrong label\");\n-  if (op->block() != NULL)  _branch_target_blocks.append(op->block());\n-  if (op->ublock() != NULL) _branch_target_blocks.append(op->ublock());\n+  assert(op->block() == nullptr || op->block()->label() == op->label(), \"wrong label\");\n+  if (op->block() != nullptr)  _branch_target_blocks.append(op->block());\n+  if (op->ublock() != nullptr) _branch_target_blocks.append(op->ublock());\n@@ -1063,1 +1063,1 @@\n-    if (op->info() != NULL) add_debug_info_for_branch(op->info());\n+    if (op->info() != nullptr) add_debug_info_for_branch(op->info());\n@@ -1292,1 +1292,1 @@\n-    assert(method != NULL, \"Should have method\");\n+    assert(method != nullptr, \"Should have method\");\n@@ -1295,1 +1295,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -1297,1 +1297,1 @@\n-    assert(data != NULL,                \"need data for type check\");\n+    assert(data != nullptr,                \"need data for type check\");\n@@ -1379,1 +1379,1 @@\n-      __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+      __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -1432,1 +1432,1 @@\n-      assert(method != NULL, \"Should have method\");\n+      assert(method != nullptr, \"Should have method\");\n@@ -1435,1 +1435,1 @@\n-      assert(md != NULL, \"Sanity\");\n+      assert(md != nullptr, \"Sanity\");\n@@ -1437,1 +1437,1 @@\n-      assert(data != NULL,                \"need data for type check\");\n+      assert(data != nullptr,                \"need data for type check\");\n@@ -1470,1 +1470,1 @@\n-    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -1624,1 +1624,1 @@\n-    const2reg(opr1, tmp, lir_patch_none, NULL);\n+    const2reg(opr1, tmp, lir_patch_none, nullptr);\n@@ -1634,1 +1634,1 @@\n-    const2reg(opr2, tmp, lir_patch_none, NULL);\n+    const2reg(opr2, tmp, lir_patch_none, nullptr);\n@@ -1645,1 +1645,1 @@\n-  assert(info == NULL, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n+  assert(info == nullptr, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n@@ -2037,1 +2037,1 @@\n-  if (call == NULL) {\n+  if (call == nullptr) {\n@@ -2048,1 +2048,1 @@\n-  if (call == NULL) {\n+  if (call == nullptr) {\n@@ -2059,1 +2059,1 @@\n-  if (stub == NULL) {\n+  if (stub == nullptr) {\n@@ -2230,1 +2230,1 @@\n-  BasicType basic_type = default_type != NULL ? default_type->element_type()->basic_type() : T_ILLEGAL;\n+  BasicType basic_type = default_type != nullptr ? default_type->element_type()->basic_type() : T_ILLEGAL;\n@@ -2234,1 +2234,1 @@\n-  if (default_type == NULL \/\/ || basic_type == T_OBJECT\n+  if (default_type == nullptr \/\/ || basic_type == T_OBJECT\n@@ -2246,1 +2246,1 @@\n-    assert(copyfunc_addr != NULL, \"generic arraycopy stub required\");\n+    assert(copyfunc_addr != nullptr, \"generic arraycopy stub required\");\n@@ -2286,1 +2286,1 @@\n-  assert(default_type != NULL && default_type->is_array_klass() && default_type->is_loaded(), \"must be true at this point\");\n+  assert(default_type != nullptr && default_type->is_array_klass() && default_type->is_loaded(), \"must be true at this point\");\n@@ -2296,1 +2296,1 @@\n-  \/\/ test for NULL\n+  \/\/ test for null\n@@ -2381,1 +2381,1 @@\n-      __ check_klass_subtype_fast_path(src, dst, tmp, &cont, &slow, NULL);\n+      __ check_klass_subtype_fast_path(src, dst, tmp, &cont, &slow, nullptr);\n@@ -2393,1 +2393,1 @@\n-      if (copyfunc_addr != NULL) { \/\/ use stub if available\n+      if (copyfunc_addr != nullptr) { \/\/ use stub if available\n@@ -2540,2 +2540,2 @@\n-  if (UseHeavyMonitors) {\n-    if (op->info() != NULL) {\n+  if (LockingMode == LM_MONITOR) {\n+    if (op->info() != nullptr) {\n@@ -2550,1 +2550,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -2568,1 +2568,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -2598,1 +2598,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -2600,1 +2600,1 @@\n-  assert(data != NULL && data->is_CounterData(), \"need CounterData for calls\");\n+  assert(data != nullptr && data->is_CounterData(), \"need CounterData for calls\");\n@@ -2613,1 +2613,1 @@\n-    if (C1OptimizeVirtualCallProfiling && known_klass != NULL) {\n+    if (C1OptimizeVirtualCallProfiling && known_klass != nullptr) {\n@@ -2638,1 +2638,1 @@\n-        if (receiver == NULL) {\n+        if (receiver == nullptr) {\n@@ -2705,1 +2705,1 @@\n-  bool exact_klass_set = exact_klass != NULL && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n+  bool exact_klass_set = exact_klass != nullptr && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n@@ -2741,1 +2741,1 @@\n-    if (exact_klass != NULL) {\n+    if (exact_klass != nullptr) {\n@@ -2752,2 +2752,2 @@\n-      if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {\n-        if (exact_klass != NULL) {\n+      if (exact_klass == nullptr || TypeEntries::is_type_none(current_klass)) {\n+        if (exact_klass != nullptr) {\n@@ -2782,1 +2782,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -2803,1 +2803,1 @@\n-      assert(exact_klass != NULL, \"should be\");\n+      assert(exact_klass != nullptr, \"should be\");\n@@ -2832,1 +2832,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -2896,1 +2896,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":61,"deletions":61,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -66,2 +66,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr);\n@@ -86,3 +85,4 @@\n-  if (UseFastLocking) {\n-    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case, false);\n-  } else {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    fast_lock(obj, hdr, rscratch1, rscratch2, slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n+    Label done;\n@@ -97,1 +97,1 @@\n-    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/NULL);\n+    cmpxchgptr(hdr, disp_hdr, rscratch2, rscratch1, done, \/*fallthough*\/nullptr);\n@@ -116,1 +116,1 @@\n-    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n@@ -120,0 +120,2 @@\n+    \/\/ done\n+    bind(done);\n@@ -121,2 +123,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -134,1 +134,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n@@ -137,1 +137,1 @@\n-    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if the loaded hdr is null we had recursive locking\n@@ -146,1 +146,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -148,0 +148,4 @@\n+    \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+    \/\/ be encoded.\n+    tst(hdr, markWord::monitor_value);\n+    br(Assembler::NE, slow_case);\n@@ -149,1 +153,1 @@\n-  } else {\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -321,1 +325,1 @@\n-  \/\/ explicit NULL check not needed since load from [klass_offset] causes a trap\n+  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n@@ -333,1 +337,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, int max_monitors) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n@@ -340,12 +344,0 @@\n-  if (UseFastLocking && max_monitors > 0) {\n-    Label ok;\n-    ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n-    ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n-    add(r9, r9, max_monitors * oopSize);\n-    cmp(r9, r10);\n-    br(Assembler::LT, ok);\n-    assert(StubRoutines::aarch64::check_lock_stack() != NULL, \"need runtime call stub\");\n-    far_call(StubRoutines::aarch64::check_lock_stack());\n-    bind(ok);\n-  }\n-\n@@ -354,1 +346,1 @@\n-  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":20,"deletions":28,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -371,1 +371,1 @@\n-  OopMap* oop_map = NULL;\n+  OopMap* oop_map = nullptr;\n@@ -544,1 +544,1 @@\n-  assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+  assert(deopt_blob != nullptr, \"deoptimization blob must have been created\");\n@@ -618,2 +618,2 @@\n-  OopMapSet* oop_maps = NULL;\n-  OopMap* oop_map = NULL;\n+  OopMapSet* oop_maps = nullptr;\n+  OopMap* oop_map = nullptr;\n@@ -836,1 +836,1 @@\n-        __ check_klass_subtype_slow_path(r4, r0, r2, r5, NULL, &miss);\n+        __ check_klass_subtype_slow_path(r4, r0, r2, r5, nullptr, &miss);\n@@ -906,1 +906,1 @@\n-        assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+        assert(deopt_blob != nullptr, \"deoptimization blob must have been created\");\n@@ -993,1 +993,1 @@\n-        assert(deopt_blob != NULL, \"deoptimization blob must have been created\");\n+        assert(deopt_blob != nullptr, \"deoptimization blob must have been created\");\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_Runtime1_aarch64.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+  assert(SharedRuntime::polling_page_return_handler_blob() != nullptr,\n@@ -67,12 +67,4 @@\n-int C2CheckLockStackStub::max_size() const {\n-  return 20;\n-}\n-\n-void C2CheckLockStackStub::emit(C2_MacroAssembler& masm) {\n-  __ bind(entry());\n-  assert(StubRoutines::aarch64::check_lock_stack() != NULL, \"need runtime call stub\");\n-  __ far_call(StubRoutines::aarch64::check_lock_stack());\n-  __ b(continuation());\n-}\n-\n-  return 20;\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return 24;\n@@ -92,3 +84,6 @@\n-  __ ldr(t, Address(rthread, JavaThread::lock_stack_current_offset()));\n-  __ sub(t, t, oopSize);\n-  __ str(t, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  __ ldrw(t, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  __ subw(t, t, oopSize);\n+#ifdef ASSERT\n+  __ str(zr, Address(rthread, t));\n+#endif\n+  __ strw(t, Address(rthread, JavaThread::lock_stack_top_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":12,"deletions":17,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -652,1 +652,1 @@\n-  assert(last_java_pc != NULL, \"must provide a valid PC\");\n+  assert(last_java_pc != nullptr, \"must provide a valid PC\");\n@@ -690,1 +690,1 @@\n-  assert(CodeCache::find_blob(entry.target()) != NULL,\n+  assert(CodeCache::find_blob(entry.target()) != nullptr,\n@@ -709,1 +709,1 @@\n-  assert(CodeCache::find_blob(entry.target()) != NULL,\n+  assert(CodeCache::find_blob(entry.target()) != nullptr,\n@@ -867,1 +867,1 @@\n-    assert(CodeCache::find_blob(target) != NULL &&\n+    assert(CodeCache::find_blob(target) != nullptr &&\n@@ -899,1 +899,1 @@\n-        if (stub == NULL) {\n+        if (stub == nullptr) {\n@@ -901,1 +901,1 @@\n-          return NULL; \/\/ CodeCache is full\n+          return nullptr; \/\/ CodeCache is full\n@@ -931,2 +931,2 @@\n-  if (stub == NULL) {\n-    return NULL;  \/\/ CodeBuffer::expand failed\n+  if (stub == nullptr) {\n+    return nullptr;  \/\/ CodeBuffer::expand failed\n@@ -972,1 +972,1 @@\n-  mov_metadata(rmethod, (Metadata*)NULL);\n+  mov_metadata(rmethod, nullptr);\n@@ -1168,1 +1168,1 @@\n-  \/\/ for (scan = klass->itable(); scan->interface() != NULL; scan += scan_step) {\n+  \/\/ for (scan = klass->itable(); scan->interface() != nullptr; scan += scan_step) {\n@@ -1227,2 +1227,2 @@\n-  check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &L_success, &L_failure, NULL);\n-  check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &L_success, NULL);\n+  check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &L_success, &L_failure, nullptr);\n+  check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &L_success, nullptr);\n@@ -1251,4 +1251,4 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  if (L_slow_path == NULL) { L_slow_path = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_slow_path == nullptr) { L_slow_path = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -1373,3 +1373,3 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -1446,1 +1446,1 @@\n-  assert(L_fast_path != NULL || L_slow_path != NULL, \"at least one is required\");\n+  assert(L_fast_path != nullptr || L_slow_path != nullptr, \"at least one is required\");\n@@ -1450,1 +1450,1 @@\n-  if (L_fast_path == NULL) {\n+  if (L_fast_path == nullptr) {\n@@ -1452,1 +1452,1 @@\n-  } else if (L_slow_path == NULL) {\n+  } else if (L_slow_path == nullptr) {\n@@ -1479,1 +1479,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -1511,1 +1511,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -1643,1 +1643,1 @@\n-    \/\/ provoke OS NULL exception if reg = NULL by\n+    \/\/ provoke OS null exception if reg is null by\n@@ -1649,1 +1649,1 @@\n-    \/\/ will provoke OS NULL exception if reg = NULL\n+    \/\/ will provoke OS null exception if reg is null\n@@ -1967,1 +1967,1 @@\n-  if (last != NULL && nativeInstruction_at(last)->is_Membar() && prev == last) {\n+  if (last != nullptr && nativeInstruction_at(last)->is_Membar() && prev == last) {\n@@ -2452,2 +2452,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  if (!UseCompressedOops || Universe::ptr_base() == NULL) {\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  if (!UseCompressedOops || Universe::ptr_base() == nullptr) {\n@@ -2474,1 +2474,1 @@\n-  cbz(value, done);           \/\/ Use NULL as-is.\n+  cbz(value, done);           \/\/ Use null as-is.\n@@ -2505,1 +2505,1 @@\n-  cbz(value, done);           \/\/ Use NULL as-is.\n+  cbz(value, done);           \/\/ Use null as-is.\n@@ -2531,1 +2531,1 @@\n-  const char* buf = NULL;\n+  const char* buf = nullptr;\n@@ -3100,1 +3100,1 @@\n-  if (last == NULL || !nativeInstruction_at(last)->is_Imm_LdSt()) {\n+  if (last == nullptr || !nativeInstruction_at(last)->is_Imm_LdSt()) {\n@@ -4393,1 +4393,1 @@\n-    if (CompressedKlassPointers::base() == NULL) {\n+    if (CompressedKlassPointers::base() == nullptr) {\n@@ -4433,1 +4433,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -4466,1 +4466,1 @@\n-  if (CompressedOops::base() != NULL) {\n+  if (CompressedOops::base() != nullptr) {\n@@ -4488,1 +4488,1 @@\n-  if (CompressedOops::base() != NULL) {\n+  if (CompressedOops::base() != nullptr) {\n@@ -4505,1 +4505,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -4522,1 +4522,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -4528,1 +4528,1 @@\n-    if (CompressedOops::base() != NULL) {\n+    if (CompressedOops::base() != nullptr) {\n@@ -4534,1 +4534,1 @@\n-    assert (CompressedOops::base() == NULL, \"sanity\");\n+    assert (CompressedOops::base() == nullptr, \"sanity\");\n@@ -4540,1 +4540,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -4546,1 +4546,1 @@\n-    if (CompressedOops::base() != NULL) {\n+    if (CompressedOops::base() != nullptr) {\n@@ -4552,1 +4552,1 @@\n-    assert (CompressedOops::base() == NULL, \"sanity\");\n+    assert (CompressedOops::base() == nullptr, \"sanity\");\n@@ -4683,2 +4683,2 @@\n-    assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-    assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+    assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+    assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -4698,1 +4698,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -4751,1 +4751,1 @@\n-\/\/ Used for storing NULLs.\n+\/\/ Used for storing nulls.\n@@ -4757,1 +4757,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs a Recorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs a Recorder\");\n@@ -4766,1 +4766,1 @@\n-  if (obj == NULL) {\n+  if (obj == nullptr) {\n@@ -4791,1 +4791,1 @@\n-  if (obj == NULL) {\n+  if (obj == nullptr) {\n@@ -4804,1 +4804,1 @@\n-    assert(oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+    assert(oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5018,1 +5018,1 @@\n-    assert(count_pos.target() != NULL, \"count_positives stub has not been generated\");\n+    assert(count_pos.target() != nullptr, \"count_positives stub has not been generated\");\n@@ -5020,1 +5020,1 @@\n-    if (tpc1 == NULL) {\n+    if (tpc1 == nullptr) {\n@@ -5023,1 +5023,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -5029,1 +5029,1 @@\n-    assert(count_pos_long.target() != NULL, \"count_positives_long stub has not been generated\");\n+    assert(count_pos_long.target() != nullptr, \"count_positives_long stub has not been generated\");\n@@ -5031,1 +5031,1 @@\n-    if (tpc2 == NULL) {\n+    if (tpc2 == nullptr) {\n@@ -5034,1 +5034,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -5081,1 +5081,1 @@\n-    \/\/ if (a1 == null || a2 == null)\n+    \/\/ if (a1 == nullptr || a2 == nullptr)\n@@ -5212,1 +5212,1 @@\n-    assert(stub.target() != NULL, \"array_equals_long stub has not been generated\");\n+    assert(stub.target() != nullptr, \"array_equals_long stub has not been generated\");\n@@ -5214,1 +5214,1 @@\n-    if (tpc == NULL) {\n+    if (tpc == nullptr) {\n@@ -5217,1 +5217,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -5364,1 +5364,1 @@\n-  assert(zero_blocks.target() != NULL, \"zero_blocks stub has not been generated\");\n+  assert(zero_blocks.target() != nullptr, \"zero_blocks stub has not been generated\");\n@@ -5371,1 +5371,1 @@\n-    assert(zero_blocks.target() != NULL, \"zero_blocks stub has not been generated\");\n+    assert(zero_blocks.target() != nullptr, \"zero_blocks stub has not been generated\");\n@@ -5382,1 +5382,1 @@\n-      if (tpc == NULL) {\n+      if (tpc == nullptr) {\n@@ -5384,1 +5384,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -5733,1 +5733,1 @@\n-      assert(stub.target() != NULL, \"large_byte_array_inflate stub has not been generated\");\n+      assert(stub.target() != nullptr, \"large_byte_array_inflate stub has not been generated\");\n@@ -5735,1 +5735,1 @@\n-      if (tpc == NULL) {\n+      if (tpc == nullptr) {\n@@ -5738,1 +5738,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -6125,1 +6125,1 @@\n-  \/\/ See if oop is NULL if it is we need no handle\n+  \/\/ See if oop is null if it is we need no handle\n@@ -6138,1 +6138,1 @@\n-    \/\/ conditionally move a NULL\n+    \/\/ conditionally move a null\n@@ -6144,1 +6144,1 @@\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-null\n@@ -6171,1 +6171,1 @@\n-    \/\/ Store oop in handle area, may be NULL\n+    \/\/ Store oop in handle area, may be null\n@@ -6179,1 +6179,1 @@\n-    \/\/ conditionally move a NULL\n+    \/\/ conditionally move a null\n@@ -6247,3 +6247,4 @@\n-\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n-\/\/ on failure.\n-\/\/ Registers:\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with ZF set.\n+\/\/\n@@ -6252,3 +6253,3 @@\n-\/\/  - t1, t2, t3: temporary registers, will be destroyed\n-void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack) {\n-  assert(UseFastLocking, \"only used with fast-locking\");\n+\/\/  - t1, t2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n@@ -6257,7 +6258,4 @@\n-  if (rt_check_stack) {\n-    \/\/ Check if we would have space on lock-stack for the object.\n-    ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n-    ldr(t2, Address(rthread, JavaThread::lock_stack_limit_offset()));\n-    cmp(t1, t2);\n-    br(Assembler::GE, slow);\n-  }\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  cmpw(t1, (unsigned)LockStack::end_offset() - 1);\n+  br(Assembler::GT, slow);\n@@ -6275,4 +6273,4 @@\n-  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n-  str(obj, Address(t1, 0));\n-  add(t1, t1, oopSize);\n-  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  str(obj, Address(rthread, t1));\n+  addw(t1, t1, oopSize);\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n@@ -6281,0 +6279,7 @@\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with ZF set.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object\n+\/\/ - t1, t2: temporary registers\n@@ -6282,1 +6287,1 @@\n-  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n@@ -6285,2 +6290,33 @@\n-  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n-  andr(hdr, hdr, ~markWord::lock_mask_in_place);\n+#ifdef ASSERT\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+    cmpw(t1, (unsigned)LockStack::start_offset());\n+    br(Assembler::GT, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    subw(t1, t1, oopSize);\n+    ldr(t1, Address(rthread, t1));\n+    cmpoop(t1, obj);\n+    br(Assembler::EQ, tos_ok);\n+    STOP(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+    Label hdr_ok;\n+    tst(hdr, markWord::lock_mask_in_place);\n+    br(Assembler::EQ, hdr_ok);\n+    STOP(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+#endif\n@@ -6297,3 +6333,6 @@\n-  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n-  sub(t1, t1, oopSize);\n-  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  ldrw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n+  subw(t1, t1, oopSize);\n+#ifdef ASSERT\n+  str(zr, Address(rthread, t1));\n+#endif\n+  strw(t1, Address(rthread, JavaThread::lock_stack_top_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":136,"deletions":97,"binary":false,"changes":233,"status":"modified"},{"patch":"@@ -60,1 +60,1 @@\n-    Label *retaddr = NULL\n+    Label *retaddr = nullptr\n@@ -615,1 +615,1 @@\n-  \/\/ Support for NULL-checks\n+  \/\/ Support for null-checks\n@@ -617,1 +617,1 @@\n-  \/\/ Generates code that causes a NULL OS exception if the content of reg is NULL.\n+  \/\/ Generates code that causes a null OS exception if the content of reg is null.\n@@ -640,1 +640,1 @@\n-  static void pd_patch_instruction(address branch, address target, const char* file = NULL, int line = 0) {\n+  static void pd_patch_instruction(address branch, address target, const char* file = nullptr, int line = 0) {\n@@ -886,1 +886,1 @@\n-  \/\/ Used for storing NULL. All other oop constants should be\n+  \/\/ Used for storing null. All other oop constants should be\n@@ -893,1 +893,1 @@\n-  \/\/ converting a zero (like NULL) into a Register by giving\n+  \/\/ converting a zero (like null) into a Register by giving\n@@ -965,1 +965,1 @@\n-  \/\/ One of the three labels can be NULL, meaning take the fall-through.\n+  \/\/ One of the three labels can be null, meaning take the fall-through.\n@@ -998,2 +998,2 @@\n-                      Label* L_fast_path = NULL,\n-                      Label* L_slow_path = NULL);\n+                      Label* L_fast_path = nullptr,\n+                      Label* L_slow_path = nullptr);\n@@ -1202,1 +1202,1 @@\n-  \/\/ Return: the call PC or NULL if CodeCache is full.\n+  \/\/ Return: the call PC or null if CodeCache is full.\n@@ -1594,1 +1594,1 @@\n-  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack = true);\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":11,"deletions":11,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -142,2 +142,2 @@\n-\/\/ Store an oop (or NULL) at the Address described by obj.\n-\/\/ If val == noreg this means store a NULL\n+\/\/ Store an oop (or null) at the Address described by obj.\n+\/\/ If val == noreg this means store a null\n@@ -417,1 +417,1 @@\n-    __ mov(result, 0);  \/\/ NULL object reference\n+    __ mov(result, 0);  \/\/ null object reference\n@@ -1112,1 +1112,1 @@\n-  \/\/ do array store check - check for NULL value first\n+  \/\/ do array store check - check for null value first\n@@ -1140,1 +1140,1 @@\n-  \/\/ Have a NULL in r0, r3=array, r2=index.  Store NULL at ary[idx]\n+  \/\/ Have a null in r0, r3=array, r2=index.  Store null at ary[idx]\n@@ -1144,1 +1144,1 @@\n-  \/\/ Store a NULL\n+  \/\/ Store a null\n@@ -1867,1 +1867,1 @@\n-    \/\/ r0: osr nmethod (osr ok) or NULL (osr not possible)\n+    \/\/ r0: osr nmethod (osr ok) or null (osr not possible)\n@@ -2277,1 +2277,1 @@\n-    __ clinit_barrier(temp, rscratch1, NULL, &clinit_barrier_slow);\n+    __ clinit_barrier(temp, rscratch1, nullptr, &clinit_barrier_slow);\n@@ -2436,1 +2436,1 @@\n-      __ mov(c_rarg1, zr); \/\/ NULL object reference\n+      __ mov(c_rarg1, zr); \/\/ null object reference\n@@ -2441,1 +2441,1 @@\n-    \/\/ c_rarg1: object pointer or NULL\n+    \/\/ c_rarg1: object pointer or null\n@@ -2689,1 +2689,1 @@\n-    \/\/ c_rarg1: object pointer set up above (NULL if static)\n+    \/\/ c_rarg1: object pointer set up above (null if static)\n@@ -3694,1 +3694,1 @@\n-  \/\/ Collect counts on whether this test sees NULLs a lot or not.\n+  \/\/ Collect counts on whether this test sees nulls a lot or not.\n@@ -3747,1 +3747,1 @@\n-  \/\/ Collect counts on whether this test sees NULLs a lot or not.\n+  \/\/ Collect counts on whether this test sees nulls a lot or not.\n@@ -3756,2 +3756,2 @@\n-  \/\/ r0 = 0: obj == NULL or  obj is not an instanceof the specified klass\n-  \/\/ r0 = 1: obj != NULL and obj is     an instanceof the specified klass\n+  \/\/ r0 = 0: obj == nullptr or  obj is not an instanceof the specified klass\n+  \/\/ r0 = 1: obj != nullptr and obj is     an instanceof the specified klass\n@@ -3817,1 +3817,1 @@\n-  \/\/ check for NULL object\n+  \/\/ check for null object\n@@ -3829,1 +3829,1 @@\n-  __ mov(c_rarg1, zr); \/\/ points to free slot or NULL\n+  __ mov(c_rarg1, zr); \/\/ points to free slot or null\n@@ -3920,1 +3920,1 @@\n-  \/\/ check for NULL object\n+  \/\/ check for null object\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":18,"deletions":18,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -148,1 +148,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n@@ -2434,1 +2434,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/cpu\/arm\/c1_LIRAssembler_arm.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"logging\/log.hpp\"\n@@ -56,1 +57,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n@@ -201,0 +202,1 @@\n+  \/\/ save object being locked into the BasicObjectLock\n@@ -214,2 +216,2 @@\n-  \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n-  \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    log_trace(fastlock)(\"C1_MacroAssembler::lock fast\");\n@@ -217,2 +219,3 @@\n-  \/\/ Must be the first instruction here, because implicit null check relies on it\n-  ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    Register t1 = disp_hdr; \/\/ Needs saving, probably\n+    Register t2 = hdr;      \/\/ blow\n+    Register t3 = Rtemp;    \/\/ blow\n@@ -220,2 +223,2 @@\n-  tst(hdr, markWord::unlocked_value);\n-  b(fast_lock, ne);\n+    fast_lock_2(obj \/* obj *\/, t1, t2, t3, 1 \/* savemask - save t1 *\/, slow_case);\n+    \/\/ Success: fall through\n@@ -223,14 +226,1 @@\n-  \/\/ Check for recursive locking\n-  \/\/ See comments in InterpreterMacroAssembler::lock_object for\n-  \/\/ explanations on the fast recursive locking check.\n-  \/\/ -1- test low 2 bits\n-  movs(tmp2, AsmOperand(hdr, lsl, 30));\n-  \/\/ -2- test (hdr - SP) if the low two bits are 0\n-  sub(tmp2, hdr, SP, eq);\n-  movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n-  \/\/ If still 'eq' then recursive locking OK\n-  \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n-  str(tmp2, Address(disp_hdr, mark_offset));\n-  b(fast_lock_done, eq);\n-  \/\/ else need slow case\n-  b(slow_case);\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -238,0 +228,2 @@\n+    \/\/ On MP platforms the next load could return a 'stale' value if the memory location has been modified by another thread.\n+    \/\/ That would be acceptable as ether CAS or slow case path is taken in that case.\n@@ -239,3 +231,2 @@\n-  bind(fast_lock);\n-  \/\/ Save previous object header in BasicLock structure and update the header\n-  str(hdr, Address(disp_hdr, mark_offset));\n+    \/\/ Must be the first instruction here, because implicit null check relies on it\n+    ldr(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n@@ -243,1 +234,2 @@\n-  cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n+    tst(hdr, markWord::unlocked_value);\n+    b(fast_lock, ne);\n@@ -245,1 +237,24 @@\n-  bind(fast_lock_done);\n+    \/\/ Check for recursive locking\n+    \/\/ See comments in InterpreterMacroAssembler::lock_object for\n+    \/\/ explanations on the fast recursive locking check.\n+    \/\/ -1- test low 2 bits\n+    movs(tmp2, AsmOperand(hdr, lsl, 30));\n+    \/\/ -2- test (hdr - SP) if the low two bits are 0\n+    sub(tmp2, hdr, SP, eq);\n+    movs(tmp2, AsmOperand(tmp2, lsr, exact_log2(os::vm_page_size())), eq);\n+    \/\/ If still 'eq' then recursive locking OK\n+    \/\/ set to zero if recursive lock, set to non zero otherwise (see discussion in JDK-8267042)\n+    str(tmp2, Address(disp_hdr, mark_offset));\n+    b(fast_lock_done, eq);\n+    \/\/ else need slow case\n+    b(slow_case);\n+\n+\n+    bind(fast_lock);\n+    \/\/ Save previous object header in BasicLock structure and update the header\n+    str(hdr, Address(disp_hdr, mark_offset));\n+\n+    cas_for_lock_acquire(hdr, disp_hdr, obj, tmp2, slow_case);\n+\n+    bind(fast_lock_done);\n+  }\n@@ -263,4 +278,2 @@\n-  \/\/ Load displaced header and object from the lock\n-  ldr(hdr, Address(disp_hdr, mark_offset));\n-  \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n-  cbz(hdr, done);\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    log_trace(fastlock)(\"C1_MacroAssembler::unlock fast\");\n@@ -268,2 +281,1 @@\n-  \/\/ load object\n-  ldr(obj, Address(disp_hdr, obj_offset));\n+    ldr(obj, Address(disp_hdr, obj_offset));\n@@ -271,2 +283,3 @@\n-  \/\/ Restore the object header\n-  cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n+    Register t1 = disp_hdr; \/\/ Needs saving, probably\n+    Register t2 = hdr;      \/\/ blow\n+    Register t3 = Rtemp;    \/\/ blow\n@@ -274,0 +287,17 @@\n+    fast_unlock_2(obj \/* object *\/, t1, t2, t3, 1 \/* savemask (save t1) *\/,\n+                    slow_case);\n+    \/\/ Success: Fall through\n+\n+  } else if (LockingMode == LM_LEGACY) {\n+\n+    \/\/ Load displaced header and object from the lock\n+    ldr(hdr, Address(disp_hdr, mark_offset));\n+    \/\/ If hdr is null, we've got recursive locking and there's nothing more to do\n+    cbz(hdr, done);\n+\n+    \/\/ load object\n+    ldr(obj, Address(disp_hdr, obj_offset));\n+\n+    \/\/ Restore the object header\n+    cas_for_lock_release(disp_hdr, hdr, obj, tmp2, slow_case);\n+  }\n@@ -277,1 +307,0 @@\n-\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":64,"deletions":35,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -62,0 +62,1 @@\n+#define STOP(str) stop(str);\n@@ -2419,1 +2420,1 @@\n-    bgtu(in_nmethod ? sp : fp, t0, slow_path, true \/* is_far *\/);\n+    bgtu(in_nmethod ? sp : fp, t0, slow_path, \/* is_far *\/ true);\n@@ -4064,12 +4065,11 @@\n-#define FCVT_SAFE(FLOATCVT, FLOATEQ)                                                             \\\n-void MacroAssembler:: FLOATCVT##_safe(Register dst, FloatRegister src, Register tmp) {           \\\n-  Label L_Okay;                                                                                  \\\n-  fscsr(zr);                                                                                     \\\n-  FLOATCVT(dst, src);                                                                            \\\n-  frcsr(tmp);                                                                                    \\\n-  andi(tmp, tmp, 0x1E);                                                                          \\\n-  beqz(tmp, L_Okay);                                                                             \\\n-  FLOATEQ(tmp, src, src);                                                                        \\\n-  bnez(tmp, L_Okay);                                                                             \\\n-  mv(dst, zr);                                                                                   \\\n-  bind(L_Okay);                                                                                  \\\n+#define FCVT_SAFE(FLOATCVT, FLOATSIG)                                                     \\\n+void MacroAssembler::FLOATCVT##_safe(Register dst, FloatRegister src, Register tmp) {     \\\n+  Label done;                                                                             \\\n+  assert_different_registers(dst, tmp);                                                   \\\n+  fclass_##FLOATSIG(tmp, src);                                                            \\\n+  mv(dst, zr);                                                                            \\\n+  \/* check if src is NaN *\/                                                               \\\n+  andi(tmp, tmp, 0b1100000000);                                                           \\\n+  bnez(tmp, done);                                                                        \\\n+  FLOATCVT(dst, src);                                                                     \\\n+  bind(done);                                                                             \\\n@@ -4078,4 +4078,4 @@\n-FCVT_SAFE(fcvt_w_s, feq_s)\n-FCVT_SAFE(fcvt_l_s, feq_s)\n-FCVT_SAFE(fcvt_w_d, feq_d)\n-FCVT_SAFE(fcvt_l_d, feq_d)\n+FCVT_SAFE(fcvt_w_s, s);\n+FCVT_SAFE(fcvt_l_s, s);\n+FCVT_SAFE(fcvt_w_d, d);\n+FCVT_SAFE(fcvt_l_d, d);\n@@ -4482,3 +4482,4 @@\n-\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n-\/\/ on failure.\n-\/\/ Registers:\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object.\n+\/\/ Falls through upon success.\n+\/\/\n@@ -4489,1 +4490,1 @@\n-  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n@@ -4493,3 +4494,3 @@\n-  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n-  ld(tmp2, Address(xthread, JavaThread::lock_stack_limit_offset()));\n-  bge(tmp1, tmp2, slow, true);\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  mv(tmp2, (unsigned)LockStack::end_offset());\n+  bge(tmp1, tmp2, slow, \/* is_far *\/ true);\n@@ -4501,0 +4502,1 @@\n+\n@@ -4507,6 +4509,5 @@\n-  \/\/ TODO: Can we avoid re-loading the current offset? The CAS above clobbers it.\n-  \/\/ Maybe we could ensure that we have enough space on the lock stack more cleverly.\n-  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n-  sd(obj, Address(tmp1, 0));\n-  add(tmp1, tmp1, oopSize);\n-  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  add(tmp2, xthread, tmp1);\n+  sd(obj, Address(tmp2, 0));\n+  addw(tmp1, tmp1, oopSize);\n+  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n@@ -4515,0 +4516,7 @@\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure.\n+\/\/ Falls through upon success.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object\n+\/\/ - tmp1, tmp2: temporary registers\n@@ -4516,1 +4524,1 @@\n-  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n@@ -4519,3 +4527,33 @@\n-  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n-  mv(tmp1, ~markWord::lock_mask_in_place);\n-  andr(hdr, hdr, tmp1);\n+#ifdef ASSERT\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+    mv(tmp2, (unsigned)LockStack::start_offset());\n+    bgt(tmp1, tmp2, stack_ok);\n+    STOP(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    subw(tmp1, tmp1, oopSize);\n+    add(tmp1, xthread, tmp1);\n+    ld(tmp1, Address(tmp1, 0));\n+    beq(tmp1, obj, tos_ok);\n+    STOP(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+   Label hdr_ok;\n+    andi(tmp1, hdr, markWord::lock_mask_in_place);\n+    beqz(tmp1, hdr_ok);\n+    STOP(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+#endif\n@@ -4532,12 +4570,7 @@\n-  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n-  sub(tmp1, tmp1, oopSize);\n-  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n-}\n-\n-void MacroAssembler::test_bit(Register Rd, Register Rs, uint32_t bit_pos, Register tmp) {\n-  assert(bit_pos < 64, \"invalid bit range\");\n-  if (UseZbs) {\n-    bexti(Rd, Rs, bit_pos);\n-    return;\n-  }\n-  andi(Rd, Rs, 1UL << bit_pos, tmp);\n+  lwu(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n+  subw(tmp1, tmp1, oopSize);\n+#ifdef ASSERT\n+  add(tmp2, xthread, tmp1);\n+  sd(zr, Address(tmp2, 0));\n+#endif\n+  sw(tmp1, Address(xthread, JavaThread::lock_stack_top_offset()));\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":79,"deletions":46,"binary":false,"changes":125,"status":"modified"},{"patch":"@@ -1674,32 +1674,1 @@\n-    if (!UseHeavyMonitors) {\n-      if (UseFastLocking) {\n-        __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-        __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n-      } else {\n-        \/\/ Load (object->mark() | 1) into swap_reg % x10\n-        __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-        __ ori(swap_reg, t0, 1);\n-\n-        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-        \/\/ src -> dest if dest == x10 else x10 <- dest\n-        __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n-\n-        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-        \/\/  1) (mark & 3) == 0, and\n-        \/\/  2) sp <= mark < mark + os::pagesize()\n-        \/\/ These 3 tests can be done by evaluating the following\n-        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-        \/\/ assuming both stack pointer and pagesize have their\n-        \/\/ least significant 2 bits clear.\n-        \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n-\n-        __ sub(swap_reg, swap_reg, sp);\n-        __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-        \/\/ Save the test result, for recursive case, the result is zero\n-        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-        __ bnez(swap_reg, slow_path_lock);\n-      }\n-    } else {\n+    if (LockingMode == LM_MONITOR) {\n@@ -1707,0 +1676,30 @@\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ Load (object->mark() | 1) into swap_reg % x10\n+      __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ ori(swap_reg, t0, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+      \/\/ src -> dest if dest == x10 else x10 <- dest\n+      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 3) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 2 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n+\n+      __ sub(swap_reg, swap_reg, sp);\n+      __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+      __ bnez(swap_reg, slow_path_lock);\n+    } else {\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n@@ -1801,1 +1800,1 @@\n-    if (!UseHeavyMonitors && !UseFastLocking) {\n+    if (LockingMode == LM_LEGACY) {\n@@ -1816,15 +1815,12 @@\n-    if (!UseHeavyMonitors) {\n-      if (UseFastLocking) {\n-        __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-        __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n-      } else {\n-        \/\/ get address of the stack lock\n-        __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-        \/\/  get old displaced header\n-        __ ld(old_hdr, Address(x10, 0));\n-\n-        \/\/ Atomic swap old header if oop still contains the stack lock\n-        Label count;\n-        __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n-        __ bind(count);\n-      }\n+    if (LockingMode == LM_MONITOR) {\n+      __ j(slow_path_unlock);\n+    } else if (LockingMode == LM_LEGACY) {\n+      \/\/ get address of the stack lock\n+      __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+      \/\/  get old displaced header\n+      __ ld(old_hdr, Address(x10, 0));\n+\n+      \/\/ Atomic swap old header if oop still contains the stack lock\n+      Label count;\n+      __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n+      __ bind(count);\n@@ -1833,1 +1829,6 @@\n-      __ j(slow_path_unlock);\n+      assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+      __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      __ andi(t0, old_hdr, markWord::monitor_value);\n+      __ bnez(t0, slow_path_unlock);\n+      __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      __ decrement(Address(xthread, JavaThread::held_monitor_count_offset()));\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":50,"deletions":49,"binary":false,"changes":99,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n@@ -457,1 +457,1 @@\n-    if (UseHeavyMonitors) {\n+    if (LockingMode == LM_MONITOR) {\n@@ -3493,1 +3493,1 @@\n-  if (UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n@@ -3501,1 +3501,1 @@\n-    Register tmp = UseFastLocking ? op->scratch_opr()->as_register() : noreg;\n+    Register tmp = LockingMode == LM_LIGHTWEIGHT ? op->scratch_opr()->as_register() : noreg;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -65,1 +65,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -72,2 +72,2 @@\n-    fast_lock_impl(obj, hdr, thread, tmp, slow_case, LP64_ONLY(false) NOT_LP64(true));\n-  } else {\n+    fast_lock_impl(obj, hdr, thread, tmp, slow_case);\n+  } else  if (LockingMode == LM_LEGACY) {\n@@ -75,0 +75,1 @@\n+    \/\/ and mark it as unlocked\n@@ -121,1 +122,1 @@\n-  if (!UseFastLocking) {\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n@@ -134,1 +135,1 @@\n-  if (UseFastLocking) {\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -138,1 +139,1 @@\n-  } else {\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -325,1 +326,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n@@ -346,13 +347,0 @@\n-#ifdef _LP64\n-  if (UseFastLocking && max_monitors > 0) {\n-    Label ok;\n-    movptr(rax, Address(r15_thread, JavaThread::lock_stack_current_offset()));\n-    addptr(rax, max_monitors * wordSize);\n-    cmpptr(rax, Address(r15_thread, JavaThread::lock_stack_limit_offset()));\n-    jcc(Assembler::less, ok);\n-    assert(StubRoutines::x86::check_lock_stack() != nullptr, \"need runtime call stub\");\n-    call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n-    bind(ok);\n-  }\n-#endif\n-\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":8,"deletions":20,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -76,11 +76,0 @@\n-int C2CheckLockStackStub::max_size() const {\n-  return 10;\n-}\n-\n-void C2CheckLockStackStub::emit(C2_MacroAssembler& masm) {\n-  __ bind(entry());\n-  assert(StubRoutines::x86::check_lock_stack() != NULL, \"need runtime call stub\");\n-  __ call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n-  __ jmp(continuation(), false \/* maybe_short *\/);\n-}\n-\n@@ -89,1 +78,4 @@\n-  return 18;\n+  \/\/ Max size of stub has been determined by testing with 0, in which case\n+  \/\/ C2CodeStubList::emit() will throw an assertion and report the actual size that\n+  \/\/ is needed.\n+  return DEBUG_ONLY(36) NOT_DEBUG(21);\n@@ -95,0 +87,1 @@\n+  Register t = tmp();\n@@ -96,1 +89,5 @@\n-  __ subptr(Address(r15_thread, JavaThread::lock_stack_current_offset()), oopSize);\n+  __ subl(Address(r15_thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  __ movl(t, Address(r15_thread, JavaThread::lock_stack_top_offset()));\n+  __ movptr(Address(r15_thread, t), 0);\n+#endif\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":10,"deletions":13,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors) {\n+void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n@@ -130,14 +130,0 @@\n-#ifdef _LP64\n-  if (UseFastLocking && max_monitors > 0) {\n-    C2CheckLockStackStub* stub = new (Compile::current()->comp_arena()) C2CheckLockStackStub();\n-    Compile::current()->output()->add_stub(stub);\n-    assert(!is_stub, \"only methods have monitors\");\n-    Register thread = r15_thread;\n-    movptr(rax, Address(thread, JavaThread::lock_stack_current_offset()));\n-    addptr(rax, max_monitors * oopSize);\n-    cmpptr(rax, Address(thread, JavaThread::lock_stack_limit_offset()));\n-    jcc(Assembler::greaterEqual, stub->entry());\n-    bind(stub->continuation());\n-  }\n-#endif\n-\n@@ -607,1 +593,1 @@\n-    assert(!UseHeavyMonitors, \"+UseHeavyMonitors and +UseRTMForStackLocks are mutually exclusive\");\n+    assert(LockingMode != LM_MONITOR, \"LockingMode == 0 (LM_MONITOR) and +UseRTMForStackLocks are mutually exclusive\");\n@@ -616,1 +602,1 @@\n-  jccb(Assembler::notZero, IsInflated);\n+  jcc(Assembler::notZero, IsInflated);\n@@ -618,34 +604,1 @@\n-  if (!UseHeavyMonitors) {\n-    if (UseFastLocking) {\n-#ifdef _LP64\n-      fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT, false);\n-      jmp(COUNT);\n-#else\n-      \/\/ We can not emit the lock-stack-check in verified_entry() because we don't have enough\n-      \/\/ registers (for thread ptr). Therefore we have to emit the lock-stack-check in\n-      \/\/ fast_lock_impl(). However, that check can take a slow-path with ZF=1, therefore\n-      \/\/ we need to handle it specially and force ZF=0 before taking the actual slow-path.\n-      Label slow;\n-      fast_lock_impl(objReg, tmpReg, thread, scrReg, slow);\n-      jmp(COUNT);\n-      bind(slow);\n-      testptr(objReg, objReg); \/\/ ZF=0 to indicate failure\n-      jmp(NO_COUNT);\n-#endif\n-    } else {\n-      \/\/ Attempt stack-locking ...\n-      orptr (tmpReg, markWord::unlocked_value);\n-      movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-      lock();\n-      cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-      jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-      \/\/ Recursive locking.\n-      \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-      \/\/ Locked by current thread if difference with current SP is less than one page.\n-      subptr(tmpReg, rsp);\n-      \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n-      movptr(Address(boxReg, 0), tmpReg);\n-    }\n-  } else {\n+  if (LockingMode == LM_MONITOR) {\n@@ -654,0 +607,19 @@\n+  } else if (LockingMode == LM_LEGACY) {\n+    \/\/ Attempt stack-locking ...\n+    orptr (tmpReg, markWord::unlocked_value);\n+    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+    lock();\n+    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+    jcc(Assembler::equal, COUNT);           \/\/ Success\n+\n+    \/\/ Recursive locking.\n+    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+    \/\/ Locked by current thread if difference with current SP is less than one page.\n+    subptr(tmpReg, rsp);\n+    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+    movptr(Address(boxReg, 0), tmpReg);\n+  } else {\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"\");\n+    fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT);\n+    jmp(COUNT);\n@@ -709,1 +681,1 @@\n-  cmpxchgptr(r15_thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(scrReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -716,1 +688,1 @@\n-  cmpptr(r15_thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n+  cmpptr(thread, rax);                \/\/ Check if we are already the owner (recursive lock)\n@@ -732,6 +704,1 @@\n-#ifndef _LP64\n-  get_thread(tmpReg);\n-  incrementl(Address(tmpReg, JavaThread::held_monitor_count_offset()));\n-#else \/\/ _LP64\n-  incrementq(Address(r15_thread, JavaThread::held_monitor_count_offset()));\n-#endif\n+  increment(Address(thread, JavaThread::held_monitor_count_offset()));\n@@ -789,1 +756,1 @@\n-    assert(!UseHeavyMonitors, \"+UseHeavyMonitors and +UseRTMForStackLocks are mutually exclusive\");\n+    assert(LockingMode != LM_MONITOR, \"LockingMode == 0 (LM_MONITOR) and +UseRTMForStackLocks are mutually exclusive\");\n@@ -801,1 +768,1 @@\n-  if (!UseHeavyMonitors && !UseFastLocking) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -806,1 +773,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode != LM_MONITOR) {\n@@ -808,9 +775,7 @@\n-#if INCLUDE_RTM_OPT\n-    if (UseFastLocking && use_rtm) {\n-      jcc(Assembler::zero, Stacked);\n-    } else\n-#endif\n-    jccb(Assembler::zero, Stacked);\n-    if (UseFastLocking) {\n-      \/\/ If the owner is ANONYMOUS, we need to fix it.\n-      testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) (intptr_t) ANONYMOUS_OWNER);\n+    jcc(Assembler::zero, Stacked);\n+  }\n+\n+  \/\/ It's inflated.\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ If the owner is ANONYMOUS, we need to fix it -  in an outline stub.\n+    testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) ObjectMonitor::ANONYMOUS_OWNER);\n@@ -818,1 +783,2 @@\n-      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg);\n+    if (!Compile::current()->output()->in_scratch_emit_size()) {\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg, boxReg);\n@@ -822,1 +788,3 @@\n-#else\n+    } else\n+#endif\n+    {\n@@ -826,1 +794,0 @@\n-#endif\n@@ -830,1 +797,0 @@\n-  \/\/ It's inflated.\n@@ -951,1 +917,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode != LM_MONITOR) {\n@@ -953,1 +919,1 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n@@ -957,1 +923,1 @@\n-    } else {\n+    } else if (LockingMode == LM_LEGACY) {\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":44,"deletions":78,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -9851,1 +9851,9 @@\n-void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack) {\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with ZF cleared.\n+\/\/ Falls through upon success with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be locked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ thread: the thread which attempts to lock obj\n+\/\/ tmp: a temporary register\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow) {\n@@ -9856,15 +9864,5 @@\n-  if (rt_check_stack) {\n-    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n-    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n-    jcc(Assembler::greaterEqual, slow);\n-  }\n-#ifdef ASSERT\n-  else {\n-    Label ok;\n-    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n-    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n-    jcc(Assembler::less, ok);\n-    stop(\"Not enough room in lock stack; should have been checked in the method prologue\");\n-    bind(ok);\n-  }\n-#endif\n+  \/\/ Note: we subtract 1 from the end-offset so that we can do a 'greater' comparison, instead\n+  \/\/ of 'greaterEqual' below, which readily clears the ZF. This makes C2 code a little simpler and\n+  \/\/ avoids one branch.\n+  cmpl(Address(thread, JavaThread::lock_stack_top_offset()), LockStack::end_offset() - 1);\n+  jcc(Assembler::greater, slow);\n@@ -9873,1 +9871,1 @@\n-  \/\/ Clear lowest two header bits (locked state).\n+  \/\/ Clear lock_mask bits (locked state).\n@@ -9876,1 +9874,1 @@\n-  \/\/ Set lowest bit (unlocked state).\n+  \/\/ Set unlocked_value bit.\n@@ -9883,4 +9881,4 @@\n-  movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n-  movptr(Address(tmp, 0), obj);\n-  increment(tmp, oopSize);\n-  movptr(Address(thread, JavaThread::lock_stack_current_offset()), tmp);\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), obj);\n+  incrementl(tmp, oopSize);\n+  movl(Address(thread, JavaThread::lock_stack_top_offset()), tmp);\n@@ -9889,0 +9887,7 @@\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with ZF cleared.\n+\/\/ Falls through upon success, with unspecified ZF.\n+\/\/\n+\/\/ obj: the object to be unlocked\n+\/\/ hdr: the (pre-loaded) header of the object, must be rax\n+\/\/ tmp: a temporary register\n@@ -9893,1 +9898,1 @@\n-  \/\/ Mark-word must be 00 now, try to swing it back to 01 (unlocked)\n+  \/\/ Mark-word must be lock_mask now, try to swing it back to unlocked_value.\n@@ -9904,1 +9909,6 @@\n-  get_thread(rax);\n+  get_thread(thread);\n+#endif\n+  subl(Address(thread, JavaThread::lock_stack_top_offset()), oopSize);\n+#ifdef ASSERT\n+  movl(tmp, Address(thread, JavaThread::lock_stack_top_offset()));\n+  movptr(Address(thread, tmp), 0);\n@@ -9906,1 +9916,0 @@\n-  subptr(Address(thread, JavaThread::lock_stack_current_offset()), oopSize);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":34,"deletions":25,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -174,0 +174,2 @@\n+  void increment(Address dst, int value = 1)  { LP64_ONLY(incrementq(dst, value)) NOT_LP64(incrementl(dst, value)) ; }\n+  void decrement(Address dst, int value = 1)  { LP64_ONLY(decrementq(dst, value)) NOT_LP64(decrementl(dst, value)) ; }\n@@ -2043,1 +2045,1 @@\n-  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack = true);\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -925,2 +925,1 @@\n-  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL, max_monitors);\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n@@ -5402,1 +5401,1 @@\n-  predicate(UseAVX > 0 && !n->is_reduction());\n+  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n@@ -5424,1 +5423,1 @@\n-  predicate(UseAVX > 0 && n->is_reduction());\n+  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n@@ -5438,1 +5437,1 @@\n-  predicate(UseAVX > 0 && !n->is_reduction());\n+  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n@@ -5460,1 +5459,1 @@\n-  predicate(UseAVX > 0 && n->is_reduction());\n+  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n@@ -5474,1 +5473,1 @@\n-  predicate(UseAVX > 0 && !n->is_reduction());\n+  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n@@ -5496,1 +5495,1 @@\n-  predicate(UseAVX > 0 && n->is_reduction());\n+  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n@@ -5510,1 +5509,1 @@\n-  predicate(UseAVX > 0 && !n->is_reduction());\n+  predicate(UseAVX > 0 && !SuperWord::is_reduction(n));\n@@ -5532,1 +5531,1 @@\n-  predicate(UseAVX > 0 && n->is_reduction());\n+  predicate(UseAVX > 0 && SuperWord::is_reduction(n));\n@@ -12477,0 +12476,11 @@\n+instruct testI_reg_reg(rFlagsReg cr, rRegI src1, rRegI src2, immI_0 zero)\n+%{\n+  match(Set cr (CmpI (AndI src1 src2) zero));\n+\n+  format %{ \"testl   $src1, $src2\" %}\n+  ins_encode %{\n+    __ testl($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n@@ -12794,0 +12804,11 @@\n+instruct testL_reg_reg(rFlagsReg cr, rRegL src1, rRegL src2, immL0 zero)\n+%{\n+  match(Set cr (CmpL (AndL src1 src2) zero));\n+\n+  format %{ \"testq   $src1, $src2\\t# long\" %}\n+  ins_encode %{\n+    __ testq($src1$$Register, $src2$$Register);\n+  %}\n+  ins_pipe(ialu_cr_reg_imm);\n+%}\n+\n@@ -13336,1 +13357,1 @@\n-                 $scr$$Register, noreg, noreg, r15_thread, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, nullptr, nullptr, nullptr, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":32,"deletions":11,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -86,1 +86,0 @@\n-  int                _max_monitors; \/\/ Max number of active monitors, for fast-locking\n@@ -144,1 +143,0 @@\n-  int max_monitors() const                       { return _max_monitors; }\n@@ -177,1 +175,0 @@\n-  void push_monitor()                            { _max_monitors++; }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":0,"deletions":3,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -624,1 +624,1 @@\n-  CodeStub* slow_path = new MonitorExitStub(lock, !UseHeavyMonitors, monitor_no);\n+  CodeStub* slow_path = new MonitorExitStub(lock, LockingMode != LM_MONITOR, monitor_no);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -172,1 +172,0 @@\n-  _special_refs = new (mtClassShared) GrowableArray<SpecialRefInfo>(24 * K, mtClassShared);\n@@ -190,1 +189,0 @@\n-  delete _special_refs;\n@@ -429,7 +427,0 @@\n-  virtual void push_special(SpecialRef type, Ref* ref, intptr_t* p) {\n-    assert(type == _method_entry_ref, \"only special type allowed for now\");\n-    address src_obj = ref->obj();\n-    size_t field_offset = pointer_delta(p, src_obj,  sizeof(u1));\n-    _builder->add_special_ref(type, src_obj, field_offset);\n-  };\n-\n@@ -477,4 +468,0 @@\n-void ArchiveBuilder::add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset) {\n-  _special_refs->append(SpecialRefInfo(type, src_obj, field_offset));\n-}\n-\n@@ -685,15 +672,0 @@\n-void ArchiveBuilder::update_special_refs() {\n-  for (int i = 0; i < _special_refs->length(); i++) {\n-    SpecialRefInfo s = _special_refs->at(i);\n-    size_t field_offset = s.field_offset();\n-    address src_obj = s.src_obj();\n-    address dst_obj = get_buffered_addr(src_obj);\n-    intptr_t* src_p = (intptr_t*)(src_obj + field_offset);\n-    intptr_t* dst_p = (intptr_t*)(dst_obj + field_offset);\n-    assert(s.type() == MetaspaceClosure::_method_entry_ref, \"only special type allowed for now\");\n-\n-    assert(*src_p == *dst_p, \"must be a copy\");\n-    ArchivePtrMarker::mark_pointer((address*)dst_p);\n-  }\n-}\n-\n@@ -728,1 +700,0 @@\n-  update_special_refs();\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":0,"deletions":29,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -136,17 +136,0 @@\n-  class SpecialRefInfo {\n-    \/\/ We have a \"special pointer\" of the given _type at _field_offset of _src_obj.\n-    \/\/ See MetaspaceClosure::push_special().\n-    MetaspaceClosure::SpecialRef _type;\n-    address _src_obj;\n-    size_t _field_offset;\n-\n-  public:\n-    SpecialRefInfo() {}\n-    SpecialRefInfo(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset)\n-      : _type(type), _src_obj(src_obj), _field_offset(field_offset) {}\n-\n-    MetaspaceClosure::SpecialRef type() const { return _type;         }\n-    address src_obj()                   const { return _src_obj;      }\n-    size_t field_offset()               const { return _field_offset; }\n-  };\n-\n@@ -244,1 +227,0 @@\n-  GrowableArray<SpecialRefInfo>* _special_refs;\n@@ -282,1 +264,0 @@\n-  void update_special_refs();\n@@ -371,1 +352,0 @@\n-  void add_special_ref(MetaspaceClosure::SpecialRef type, address src_obj, size_t field_offset);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":0,"deletions":20,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -174,1 +174,1 @@\n-  if (res == NULL && do_expand) {\n+  if (res == nullptr && do_expand) {\n@@ -190,1 +190,1 @@\n-      \/\/ In either case allocate_free_region() will check for NULL.\n+      \/\/ In either case allocate_free_region() will check for null.\n@@ -283,1 +283,1 @@\n-  assert(first_hr != NULL, \"pre-condition\");\n+  assert(first_hr != nullptr, \"pre-condition\");\n@@ -352,1 +352,1 @@\n-  if (humongous_start == NULL) {\n+  if (humongous_start == nullptr) {\n@@ -357,1 +357,1 @@\n-    if (humongous_start != NULL) {\n+    if (humongous_start != nullptr) {\n@@ -367,2 +367,2 @@\n-  HeapWord* result = NULL;\n-  if (humongous_start != NULL) {\n+  HeapWord* result = nullptr;\n+  if (humongous_start != nullptr) {\n@@ -370,1 +370,1 @@\n-    assert(result != NULL, \"it should always return a valid result\");\n+    assert(result != nullptr, \"it should always return a valid result\");\n@@ -419,2 +419,2 @@\n-  \/\/ return NULL.\n-  HeapWord* result = NULL;\n+  \/\/ return null.\n+  HeapWord* result = nullptr;\n@@ -431,1 +431,1 @@\n-      if (result != NULL) {\n+      if (result != nullptr) {\n@@ -442,1 +442,1 @@\n-        if (result != NULL) {\n+        if (result != nullptr) {\n@@ -458,2 +458,2 @@\n-      if (result != NULL) {\n-        assert(succeeded, \"only way to get back a non-NULL result\");\n+      if (result != nullptr) {\n+        assert(succeeded, \"only way to get back a non-null result\");\n@@ -467,1 +467,1 @@\n-        \/\/ point in trying to allocate further. We'll just return NULL.\n+        \/\/ point in trying to allocate further. We'll just return null.\n@@ -470,1 +470,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -479,1 +479,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -499,1 +499,1 @@\n-    if (result != NULL) {\n+    if (result != nullptr) {\n@@ -512,1 +512,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -572,1 +572,1 @@\n-    assert(r->is_empty() && !r->is_pinned(), \"Region already in use (%u)\", r->hrm_index());\n+    assert(r->is_empty(), \"Region already in use (%u)\", r->hrm_index());\n@@ -641,1 +641,1 @@\n-  if (result == NULL) {\n+  if (result == nullptr) {\n@@ -647,1 +647,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -688,2 +688,2 @@\n-  \/\/ return NULL.\n-  HeapWord* result = NULL;\n+  \/\/ return null.\n+  HeapWord* result = nullptr;\n@@ -703,1 +703,1 @@\n-      if (result != NULL) {\n+      if (result != nullptr) {\n@@ -720,2 +720,2 @@\n-      if (result != NULL) {\n-        assert(succeeded, \"only way to get back a non-NULL result\");\n+      if (result != nullptr) {\n+        assert(succeeded, \"only way to get back a non-null result\");\n@@ -732,1 +732,1 @@\n-        \/\/ point in trying to allocate further. We'll just return NULL.\n+        \/\/ point in trying to allocate further. We'll just return null.\n@@ -735,1 +735,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -744,1 +744,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -772,1 +772,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -779,1 +779,1 @@\n-         \"the current alloc region was unexpectedly found to be non-NULL\");\n+         \"the current alloc region was unexpectedly found to be non-null\");\n@@ -785,1 +785,1 @@\n-    if (result != NULL && policy()->need_to_start_conc_mark(\"STW humongous allocation\")) {\n+    if (result != nullptr && policy()->need_to_start_conc_mark(\"STW humongous allocation\")) {\n@@ -929,1 +929,1 @@\n-  GCTraceTime(Info, gc) tm(\"Pause Full\", NULL, gc_cause(), true);\n+  GCTraceTime(Info, gc) tm(\"Pause Full\", nullptr, gc_cause(), true);\n@@ -985,1 +985,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -994,1 +994,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -1012,1 +1012,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1027,1 +1027,1 @@\n-  if (result != NULL || !*succeeded) {\n+  if (result != nullptr || !*succeeded) {\n@@ -1038,1 +1038,1 @@\n-  if (result != NULL || !*succeeded) {\n+  if (result != nullptr || !*succeeded) {\n@@ -1049,1 +1049,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -1060,1 +1060,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1066,1 +1066,1 @@\n-\/\/ allocated block, or else \"NULL\".\n+\/\/ allocated block, or else null.\n@@ -1084,1 +1084,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1105,1 +1105,1 @@\n-  if (expand_time_ms != NULL) {\n+  if (expand_time_ms != nullptr) {\n@@ -1224,5 +1224,5 @@\n-  _service_thread(NULL),\n-  _periodic_gc_task(NULL),\n-  _free_arena_memory_task(NULL),\n-  _workers(NULL),\n-  _card_table(NULL),\n+  _service_thread(nullptr),\n+  _periodic_gc_task(nullptr),\n+  _free_arena_memory_task(nullptr),\n+  _workers(nullptr),\n+  _card_table(nullptr),\n@@ -1233,1 +1233,1 @@\n-  _bot(NULL),\n+  _bot(nullptr),\n@@ -1237,1 +1237,1 @@\n-  _allocator(NULL),\n+  _allocator(nullptr),\n@@ -1239,1 +1239,1 @@\n-  _verifier(NULL),\n+  _verifier(nullptr),\n@@ -1256,1 +1256,1 @@\n-  _heap_sizing_policy(NULL),\n+  _heap_sizing_policy(nullptr),\n@@ -1258,1 +1258,1 @@\n-  _rem_set(NULL),\n+  _rem_set(nullptr),\n@@ -1261,5 +1261,5 @@\n-  _cm(NULL),\n-  _cm_thread(NULL),\n-  _cr(NULL),\n-  _task_queues(NULL),\n-  _ref_processor_stw(NULL),\n+  _cm(nullptr),\n+  _cm_thread(nullptr),\n+  _cr(nullptr),\n+  _task_queues(nullptr),\n+  _ref_processor_stw(nullptr),\n@@ -1268,1 +1268,1 @@\n-  _ref_processor_cm(NULL),\n+  _ref_processor_cm(nullptr),\n@@ -1298,1 +1298,1 @@\n-  guarantee(_task_queues != NULL, \"task_queues allocation failure.\");\n+  guarantee(_task_queues != nullptr, \"task_queues allocation failure.\");\n@@ -1334,1 +1334,1 @@\n-  if (_service_thread->osthread() == NULL) {\n+  if (_service_thread->osthread() == nullptr) {\n@@ -1403,1 +1403,1 @@\n-  if(heap_storage == NULL) {\n+  if(heap_storage == nullptr) {\n@@ -1465,1 +1465,1 @@\n-  if (_workers == NULL) {\n+  if (_workers == nullptr) {\n@@ -2099,1 +2099,1 @@\n-    if (hr_claimer == NULL || hr_claimer->claim_region(region_idx)) {\n+    if (hr_claimer == nullptr || hr_claimer->claim_region(region_idx)) {\n@@ -2260,1 +2260,1 @@\n-  if (_cm != NULL) {\n+  if (_cm != nullptr) {\n@@ -2380,2 +2380,2 @@\n-  assert(result == NULL || ret_succeeded,\n-         \"the result should be NULL if the VM did not succeed\");\n+  assert(result == nullptr || ret_succeeded,\n+         \"the result should be null if the VM did not succeed\");\n@@ -2603,1 +2603,1 @@\n-  assert(obj != NULL, \"must not be NULL\");\n+  assert(obj != nullptr, \"must not be null\");\n@@ -2614,1 +2614,1 @@\n-    if (pll_head != NULL) {\n+    if (pll_head != nullptr) {\n@@ -2628,1 +2628,1 @@\n-  return candidates != NULL && candidates->num_remaining() > 0;\n+  return candidates != nullptr && candidates->num_remaining() > 0;\n@@ -2660,1 +2660,1 @@\n-  if (free_list != NULL) {\n+  if (free_list != nullptr) {\n@@ -2683,1 +2683,1 @@\n-  assert(list != NULL, \"list can't be null\");\n+  assert(list != nullptr, \"list can't be null\");\n@@ -2883,1 +2883,1 @@\n-    if (new_alloc_region != NULL) {\n+    if (new_alloc_region != nullptr) {\n@@ -2890,1 +2890,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -2923,1 +2923,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -2938,1 +2938,1 @@\n-  if (new_alloc_region != NULL) {\n+  if (new_alloc_region != nullptr) {\n@@ -2951,1 +2951,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -2983,1 +2983,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -3048,1 +3048,1 @@\n-  guarantee(nm != NULL, \"sanity\");\n+  guarantee(nm != nullptr, \"sanity\");\n@@ -3054,1 +3054,1 @@\n-  guarantee(nm != NULL, \"sanity\");\n+  guarantee(nm != nullptr, \"sanity\");\n@@ -3081,1 +3081,1 @@\n-    if (nm != NULL) {\n+    if (nm != nullptr) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":82,"deletions":82,"binary":false,"changes":164,"status":"modified"},{"patch":"@@ -263,1 +263,3 @@\n-  } else if (hr->is_pinned()) {\n+  } else if (hr->is_humongous()) {\n+    \/\/ Humongous objects will never be moved in the \"main\" compaction phase, but\n+    \/\/ afterwards in a special phase if needed.\n@@ -330,1 +332,4 @@\n-  scope()->tracer()->report_object_count_after_gc(&_is_alive);\n+  {\n+    GCTraceTime(Debug, gc, phases) debug(\"Report Object Count\", scope()->timer());\n+    scope()->tracer()->report_object_count_after_gc(&_is_alive, _heap->workers());\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -71,1 +71,0 @@\n-  assert(!hr->is_pinned(), \"Should be no pinned region in compaction queue\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-  return _current_region != NULL;\n+  return _current_region != nullptr;\n@@ -75,1 +75,1 @@\n-  assert(next != NULL, \"Must return valid region\");\n+  assert(next != nullptr, \"Must return valid region\");\n@@ -97,1 +97,1 @@\n-  assert(_current_region != NULL, \"Must have been initialized\");\n+  assert(_current_region != nullptr, \"Must have been initialized\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -43,1 +43,1 @@\n-  \/\/ There is no need to iterate and forward objects in pinned regions ie.\n+  \/\/ There is no need to iterate and forward objects in non-movable regions ie.\n@@ -45,1 +45,1 @@\n-  if (hr->is_pinned()) {\n+  if (hr->is_humongous()) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -151,1 +151,1 @@\n-  assert(to_rem_set != NULL, \"Need per-region 'into' remsets.\");\n+  assert(to_rem_set != nullptr, \"Need per-region 'into' remsets.\");\n@@ -235,1 +235,1 @@\n-    assert(forwardee != NULL, \"forwardee should not be NULL\");\n+    assert(forwardee != nullptr, \"forwardee should not be null\");\n@@ -260,1 +260,1 @@\n-  if (obj == NULL) {\n+  if (obj == nullptr) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1OopClosures.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -69,2 +69,2 @@\n-    _closures(NULL),\n-    _plab_allocator(NULL),\n+    _closures(nullptr),\n+    _plab_allocator(nullptr),\n@@ -79,2 +79,2 @@\n-    _surviving_young_words_base(NULL),\n-    _surviving_young_words(NULL),\n+    _surviving_young_words_base(nullptr),\n+    _surviving_young_words(nullptr),\n@@ -88,1 +88,1 @@\n-    _obj_alloc_stat(NULL),\n+    _obj_alloc_stat(nullptr),\n@@ -151,1 +151,1 @@\n-  assert(task != NULL, \"invariant\");\n+  assert(task != nullptr, \"invariant\");\n@@ -159,1 +159,1 @@\n-  assert(task != NULL, \"invariant\");\n+  assert(task != nullptr, \"invariant\");\n@@ -187,1 +187,1 @@\n-  \/\/ Reference should not be NULL here as such are never pushed to the task queue.\n+  \/\/ Reference should not be null here as such are never pushed to the task queue.\n@@ -347,1 +347,1 @@\n-    if (obj_ptr != NULL) {\n+    if (obj_ptr != nullptr) {\n@@ -359,1 +359,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -398,1 +398,1 @@\n-  HeapWord* obj_ptr = NULL;\n+  HeapWord* obj_ptr = nullptr;\n@@ -406,1 +406,1 @@\n-    if (obj_ptr == NULL) {\n+    if (obj_ptr == nullptr) {\n@@ -413,1 +413,1 @@\n-  if (obj_ptr != NULL) {\n+  if (obj_ptr != nullptr) {\n@@ -477,2 +477,2 @@\n-  \/\/ normally check against NULL once and that's it.\n-  if (obj_ptr == NULL) {\n+  \/\/ normally check against null once and that's it.\n+  if (obj_ptr == nullptr) {\n@@ -480,1 +480,1 @@\n-    if (obj_ptr == NULL) {\n+    if (obj_ptr == nullptr) {\n@@ -487,1 +487,1 @@\n-  assert(obj_ptr != NULL, \"when we get here, allocation should have succeeded\");\n+  assert(obj_ptr != nullptr, \"when we get here, allocation should have succeeded\");\n@@ -508,1 +508,1 @@\n-  if (forward_ptr == NULL) {\n+  if (forward_ptr == nullptr) {\n@@ -577,1 +577,1 @@\n-  if (_states[worker_id] == NULL) {\n+  if (_states[worker_id] == nullptr) {\n@@ -635,1 +635,1 @@\n-  if (forward_ptr == NULL) {\n+  if (forward_ptr == nullptr) {\n@@ -690,1 +690,1 @@\n-  if (_obj_alloc_stat != NULL) {\n+  if (_obj_alloc_stat != nullptr) {\n@@ -697,1 +697,1 @@\n-  if (_obj_alloc_stat != NULL) {\n+  if (_obj_alloc_stat != nullptr) {\n@@ -719,1 +719,1 @@\n-    _states[i] = NULL;\n+    _states[i] = nullptr;\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":24,"deletions":24,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2014, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2014, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -189,1 +189,1 @@\n-  \/\/ Returns a non-NULL pointer if successful, and updates dest if required.\n+  \/\/ Returns a non-null pointer if successful, and updates dest if required.\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -221,1 +221,4 @@\n-  gc_tracer()->report_object_count_after_gc(&is_alive);\n+  {\n+    GCTraceTime(Debug, gc, phases) tm_m(\"Report Object Count\", gc_timer());\n+    gc_tracer()->report_object_count_after_gc(&is_alive, nullptr);\n+  }\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -438,1 +438,1 @@\n-  TraceMemoryManagerStats tmms(gen->gc_manager(), gc_cause());\n+  TraceMemoryManagerStats tmms(gen->gc_manager(), gc_cause(), heap()->is_young_gen(gen) ? \"end of minor GC\" : \"end of major GC\");\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -62,1 +62,0 @@\n-#include \"gc\/shenandoah\/shenandoahStringDedup.hpp\"\n@@ -484,2 +483,2 @@\n-  _stw_memory_manager(\"Shenandoah Pauses\", \"end of GC pause\"),\n-  _cycle_memory_manager(\"Shenandoah Cycles\", \"end of GC cycle\"),\n+  _stw_memory_manager(\"Shenandoah Pauses\"),\n+  _cycle_memory_manager(\"Shenandoah Cycles\"),\n@@ -1192,3 +1191,0 @@\n-  if (ShenandoahStringDedup::is_enabled()) {\n-    ShenandoahStringDedup::threads_do(tcl);\n-  }\n@@ -2200,1 +2196,1 @@\n-  if (ShenandoahSuspendibleWorkers || UseStringDeduplication) {\n+  if (ShenandoahSuspendibleWorkers) {\n@@ -2206,1 +2202,1 @@\n-  if (ShenandoahSuspendibleWorkers || UseStringDeduplication) {\n+  if (ShenandoahSuspendibleWorkers) {\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":4,"deletions":8,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -627,1 +627,1 @@\n-        bool call_vm = UseHeavyMonitors;\n+        bool call_vm = (LockingMode == LM_MONITOR);\n@@ -726,1 +726,1 @@\n-      bool call_vm = UseHeavyMonitors;\n+      bool call_vm = (LockingMode == LM_MONITOR);\n@@ -1656,1 +1656,1 @@\n-          bool call_vm = UseHeavyMonitors;\n+          bool call_vm = (LockingMode == LM_MONITOR);\n@@ -1692,1 +1692,1 @@\n-            bool call_vm = UseHeavyMonitors;\n+            bool call_vm = (LockingMode == LM_MONITOR);\n@@ -3194,1 +3194,1 @@\n-          } else if (UseHeavyMonitors) {\n+          } else if (LockingMode == LM_MONITOR) {\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2017, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2017, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -48,1 +48,1 @@\n-    ObjectSampleMarkWord() : _obj(NULL), _mark_word(markWord::zero()) {}\n+    ObjectSampleMarkWord() : _obj(nullptr), _mark_word(markWord::zero()) {}\n@@ -57,1 +57,1 @@\n-    assert(_store != NULL, \"invariant\");\n+    assert(_store != nullptr, \"invariant\");\n@@ -67,1 +67,1 @@\n-    assert(obj != NULL, \"invariant\");\n+    assert(obj != nullptr, \"invariant\");\n","filename":"src\/hotspot\/share\/jfr\/leakprofiler\/chains\/objectSampleMarker.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -36,1 +36,1 @@\n-#include \"interpreter\/linkResolver.hpp\"\n+#include \"interpreter\/linkResolver.hpp\"\n@@ -39,1 +39,1 @@\n-#include \"jvmci\/jvmciCompilerToVM.hpp\"\n+#include \"jvmci\/jvmciCompilerToVM.hpp\"\n@@ -47,1 +47,1 @@\n-#include \"oops\/instanceMirrorKlass.hpp\"\n+#include \"oops\/instanceMirrorKlass.hpp\"\n@@ -50,0 +50,1 @@\n+#include \"oops\/objArrayKlass.inline.hpp\"\n@@ -64,1 +65,1 @@\n-#include \"runtime\/vframe_hp.hpp\"\n+#include \"runtime\/vframe_hp.hpp\"\n@@ -104,0 +105,48 @@\n+class JavaArgumentUnboxer : public SignatureIterator {\n+ protected:\n+  JavaCallArguments*  _jca;\n+  arrayOop _args;\n+  int _index;\n+\n+  Handle next_arg(BasicType expectedType);\n+\n+ public:\n+  JavaArgumentUnboxer(Symbol* signature,\n+                      JavaCallArguments* jca,\n+                      arrayOop args,\n+                      bool is_static)\n+    : SignatureIterator(signature)\n+  {\n+    this->_return_type = T_ILLEGAL;\n+    _jca = jca;\n+    _index = 0;\n+    _args = args;\n+    if (!is_static) {\n+      _jca->push_oop(next_arg(T_OBJECT));\n+    }\n+    do_parameters_on(this);\n+    assert(_index == args->length(), \"arg count mismatch with signature\");\n+  }\n+\n+ private:\n+  friend class SignatureIterator;  \/\/ so do_parameters_on can call do_type\n+  void do_type(BasicType type) {\n+    if (is_reference_type(type)) {\n+      _jca->push_oop(next_arg(T_OBJECT));\n+      return;\n+    }\n+    Handle arg = next_arg(type);\n+    int box_offset = java_lang_boxing_object::value_offset(type);\n+    switch (type) {\n+    case T_BOOLEAN:     _jca->push_int(arg->bool_field(box_offset));    break;\n+    case T_CHAR:        _jca->push_int(arg->char_field(box_offset));    break;\n+    case T_SHORT:       _jca->push_int(arg->short_field(box_offset));   break;\n+    case T_BYTE:        _jca->push_int(arg->byte_field(box_offset));    break;\n+    case T_INT:         _jca->push_int(arg->int_field(box_offset));     break;\n+    case T_LONG:        _jca->push_long(arg->long_field(box_offset));   break;\n+    case T_FLOAT:       _jca->push_float(arg->float_field(box_offset));    break;\n+    case T_DOUBLE:      _jca->push_double(arg->double_field(box_offset));  break;\n+    default:            ShouldNotReachHere();\n+    }\n+  }\n+};\n@@ -386,3 +435,7 @@\n-  JVMCIKlassHandle klass(THREAD);\n-  jlong base_address = 0;\n-  if (base_object.is_non_null() && offset == oopDesc::klass_offset_in_bytes()) {\n+  if (base_object.is_null()) {\n+    JVMCI_THROW_MSG_NULL(NullPointerException, \"base object is null\");\n+  }\n+\n+  const char* base_desc = nullptr;\n+  JVMCIKlassHandle klass(THREAD);\n+  if (offset == oopDesc::klass_offset_in_bytes()) {\n@@ -394,1 +447,1 @@\n-      assert(false, \"What types are we actually expecting here?\");\n+      goto unexpected;\n@@ -397,11 +450,42 @@\n-    if (base_object.is_non_null()) {\n-      if (JVMCIENV->isa_HotSpotResolvedJavaMethodImpl(base_object)) {\n-        base_address = (intptr_t) JVMCIENV->asMethod(base_object);\n-      } else if (JVMCIENV->isa_HotSpotConstantPool(base_object)) {\n-        base_address = (intptr_t) JVMCIENV->asConstantPool(base_object);\n-      } else if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(base_object)) {\n-        base_address = (intptr_t) JVMCIENV->asKlass(base_object);\n-      } else if (JVMCIENV->isa_HotSpotObjectConstantImpl(base_object)) {\n-        Handle base_oop = JVMCIENV->asConstant(base_object, JVMCI_CHECK_NULL);\n-        if (base_oop->is_a(vmClasses::Class_klass())) {\n-          base_address = cast_from_oop<jlong>(base_oop());\n+    if (JVMCIENV->isa_HotSpotConstantPool(base_object)) {\n+      ConstantPool* cp = JVMCIENV->asConstantPool(base_object);\n+      if (offset == ConstantPool::pool_holder_offset_in_bytes()) {\n+        klass = cp->pool_holder();\n+      } else {\n+        base_desc = FormatBufferResource(\"[constant pool for %s]\", cp->pool_holder()->signature_name());\n+        goto unexpected;\n+      }\n+    } else if (JVMCIENV->isa_HotSpotResolvedObjectTypeImpl(base_object)) {\n+      Klass* base_klass = JVMCIENV->asKlass(base_object);\n+      if (offset == in_bytes(Klass::subklass_offset())) {\n+        klass = base_klass->subklass();\n+      } else if (offset == in_bytes(Klass::super_offset())) {\n+        klass = base_klass->super();\n+      } else if (offset == in_bytes(Klass::next_sibling_offset())) {\n+        klass = base_klass->next_sibling();\n+      } else if (offset == in_bytes(ObjArrayKlass::element_klass_offset()) && base_klass->is_objArray_klass()) {\n+        klass = ObjArrayKlass::cast(base_klass)->element_klass();\n+      } else if (offset >= in_bytes(Klass::primary_supers_offset()) &&\n+                 offset < in_bytes(Klass::primary_supers_offset()) + (int) (sizeof(Klass*) * Klass::primary_super_limit()) &&\n+                 offset % sizeof(Klass*) == 0) {\n+        \/\/ Offset is within the primary supers array\n+        int index = (int) ((offset - in_bytes(Klass::primary_supers_offset())) \/ sizeof(Klass*));\n+        klass = base_klass->primary_super_of_depth(index);\n+      } else {\n+        base_desc = FormatBufferResource(\"[%s]\", base_klass->signature_name());\n+        goto unexpected;\n+      }\n+    } else if (JVMCIENV->isa_HotSpotObjectConstantImpl(base_object)) {\n+      Handle base_oop = JVMCIENV->asConstant(base_object, JVMCI_CHECK_NULL);\n+      if (base_oop->is_a(vmClasses::Class_klass())) {\n+        if (offset == java_lang_Class::klass_offset()) {\n+          klass = java_lang_Class::as_Klass(base_oop());\n+        } else if (offset == java_lang_Class::array_klass_offset()) {\n+          klass = java_lang_Class::array_klass_acquire(base_oop());\n+        } else {\n+          base_desc = FormatBufferResource(\"[Class=%s]\", java_lang_Class::as_Klass(base_oop())->signature_name());\n+          goto unexpected;\n+        }\n+      } else {\n+        if (!base_oop.is_null()) {\n+          base_desc = FormatBufferResource(\"[%s]\", base_oop()->klass()->signature_name());\n@@ -409,0 +493,1 @@\n+        goto unexpected;\n@@ -410,3 +495,6 @@\n-      if (base_address == 0) {\n-        JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                    err_msg(\"Unexpected arguments: %s \" JLONG_FORMAT \" %s\", JVMCIENV->klass_name(base_object), offset, compressed ? \"true\" : \"false\"));\n+    } else if (JVMCIENV->isa_HotSpotMethodData(base_object)) {\n+      jlong base_address = (intptr_t) JVMCIENV->asMethodData(base_object);\n+      klass = *((Klass**) (intptr_t) (base_address + offset));\n+      if (klass == nullptr || !klass->is_loader_alive()) {\n+        \/\/ Klasses in methodData might be concurrently unloading so return null in that case.\n+        return nullptr;\n@@ -414,0 +502,2 @@\n+    } else {\n+      goto unexpected;\n@@ -415,5 +505,1 @@\n-    klass = *((Klass**) (intptr_t) (base_address + offset));\n-    JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n-                err_msg(\"Unexpected arguments: %s \" JLONG_FORMAT \" %s\",\n-                        base_object.is_non_null() ? JVMCIENV->klass_name(base_object) : \"null\",\n-                        offset, compressed ? \"true\" : \"false\"));\n+    goto unexpected;\n@@ -422,3 +508,14 @@\n-  assert (klass == nullptr || klass->is_klass(), \"invalid read\");\n-  JVMCIObject result = JVMCIENV->get_jvmci_type(klass, JVMCI_CHECK_NULL);\n-  return JVMCIENV->get_jobject(result);\n+\n+  {\n+    if (klass == nullptr) {\n+      return nullptr;\n+    }\n+    JVMCIObject result = JVMCIENV->get_jvmci_type(klass, JVMCI_CHECK_NULL);\n+    return JVMCIENV->get_jobject(result);\n+  }\n+\n+unexpected:\n+  JVMCI_THROW_MSG_NULL(IllegalArgumentException,\n+                       err_msg(\"Unexpected arguments: %s%s \" JLONG_FORMAT \" %s\",\n+                               JVMCIENV->klass_name(base_object), base_desc == nullptr ? \"\" : base_desc,\n+                               offset, compressed ? \"true\" : \"false\"));\n@@ -929,2 +1026,2 @@\n-  method->set_not_c1_compilable();\n-  method->set_not_c2_compilable();\n+  method->set_is_not_c1_compilable();\n+  method->set_is_not_c2_compilable();\n@@ -1715,0 +1812,7 @@\n+  \/\/ Java code should never directly access the extra data section\n+  JVMCI_THROW_MSG_0(IllegalArgumentException, err_msg(\"Invalid profile data position %d\", position));\n+C2V_END\n+\n+C2V_VMENTRY_0(jint, methodDataExceptionSeen, (JNIEnv* env, jobject, jlong method_data_pointer, jint bci))\n+  MethodData* mdo = (MethodData*) method_data_pointer;\n+  MutexLocker mu(mdo->extra_data_lock());\n@@ -1716,1 +1820,1 @@\n-  DataLayout* end   = mdo->extra_data_limit();\n+  DataLayout* end   = mdo->args_data_limit();\n@@ -1719,3 +1823,17 @@\n-    profile_data = data->data_in();\n-    if (mdo->dp_to_di(profile_data->dp()) == position) {\n-      return profile_data->size_in_bytes();\n+    int tag = data->tag();\n+    switch(tag) {\n+      case DataLayout::bit_data_tag: {\n+        BitData* bit_data = (BitData*) data->data_in();\n+        if (bit_data->bci() == bci) {\n+          return bit_data->exception_seen() ? 1 : 0;\n+        }\n+        break;\n+      }\n+    case DataLayout::no_tag:\n+      \/\/ There is a free slot so return false since a BitData would have been allocated to record\n+      \/\/ true if it had been seen.\n+      return 0;\n+    case DataLayout::arg_info_data_tag:\n+      \/\/ The bci wasn't found and there are no free slots to record a trap for this location, so always\n+      \/\/ return unknown.\n+      return -1;\n@@ -1724,1 +1842,2 @@\n-  JVMCI_THROW_MSG_0(IllegalArgumentException, err_msg(\"Invalid profile data position %d\", position));\n+  ShouldNotReachHere();\n+  return -1;\n@@ -3020,0 +3139,1 @@\n+  {CC \"methodDataExceptionSeen\",                      CC \"(JI)I\",                                                                           FN_PTR(methodDataExceptionSeen)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":157,"deletions":37,"binary":false,"changes":194,"status":"modified"},{"patch":"@@ -59,0 +59,18 @@\n+  static_field(CompilerToVM::Data,             SharedRuntime_polling_page_return_handler,                                            \\\n+                                                                                       address)                                      \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             nmethod_entry_barrier, address)                                                       \\\n+  static_field(CompilerToVM::Data,             thread_disarmed_guard_value_offset, int)                                              \\\n+  static_field(CompilerToVM::Data,             thread_address_bad_mask_offset, int)                                                  \\\n+  AARCH64_ONLY(static_field(CompilerToVM::Data, BarrierSetAssembler_nmethod_patching_type, int))                                     \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_oop_field_preloaded, address)                      \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_weak_oop_field_preloaded, address)                 \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_phantom_oop_field_preloaded, address)              \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_oop_field_preloaded, address)                 \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_weak_oop_field_preloaded, address)            \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_weak_load_barrier_on_phantom_oop_field_preloaded, address)         \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_load_barrier_on_oop_array, address)                                \\\n+  static_field(CompilerToVM::Data,             ZBarrierSetRuntime_clone, address)                                                    \\\n+                                                                                                                                     \\\n+  static_field(CompilerToVM::Data,             continuations_enabled, bool)                                                          \\\n@@ -132,1 +150,1 @@\n-  nonstatic_field(ConstMethod,                 _flags,                                 u2)                                           \\\n+  nonstatic_field(ConstMethod,                 _flags._flags,                          u4)                                           \\\n@@ -187,0 +205,1 @@\n+  nonstatic_field(JavaThread,                  _saved_exception_pc,                           address)                               \\\n@@ -231,1 +250,1 @@\n-  nonstatic_field(Method,                      _flags,                                        u2)                                    \\\n+  nonstatic_field(Method,                      _flags._status,                                u4)                                    \\\n@@ -418,2 +437,0 @@\n-  declare_constant(JVM_ACC_MONITOR_MATCH)                                 \\\n-  declare_constant(JVM_ACC_HAS_MONITOR_BYTECODES)                         \\\n@@ -479,0 +496,1 @@\n+  declare_constant(CodeInstaller::ENTRY_BARRIER_PATCH)                    \\\n@@ -584,5 +602,10 @@\n-  declare_constant(ConstMethod::_has_linenumber_table)                    \\\n-  declare_constant(ConstMethod::_has_localvariable_table)                 \\\n-  declare_constant(ConstMethod::_has_exception_table)                     \\\n-  declare_constant(ConstMethod::_has_method_annotations)                  \\\n-  declare_constant(ConstMethod::_has_parameter_annotations)               \\\n+  declare_constant(ConstMethodFlags::_misc_has_linenumber_table)          \\\n+  declare_constant(ConstMethodFlags::_misc_has_localvariable_table)       \\\n+  declare_constant(ConstMethodFlags::_misc_has_exception_table)           \\\n+  declare_constant(ConstMethodFlags::_misc_has_method_annotations)        \\\n+  declare_constant(ConstMethodFlags::_misc_has_parameter_annotations)     \\\n+  declare_constant(ConstMethodFlags::_misc_caller_sensitive)              \\\n+  declare_constant(ConstMethodFlags::_misc_is_hidden)                     \\\n+  declare_constant(ConstMethodFlags::_misc_intrinsic_candidate)           \\\n+  declare_constant(ConstMethodFlags::_misc_reserved_stack_access)         \\\n+  declare_constant(ConstMethodFlags::_misc_changes_current_thread)        \\\n@@ -685,7 +708,2 @@\n-  declare_constant(Method::_caller_sensitive)                             \\\n-  declare_constant(Method::_force_inline)                                 \\\n-  declare_constant(Method::_dont_inline)                                  \\\n-  declare_constant(Method::_hidden)                                       \\\n-  declare_constant(Method::_intrinsic_candidate)                          \\\n-  declare_constant(Method::_reserved_stack_access)                        \\\n-  declare_constant(Method::_changes_current_thread)                       \\\n+  declare_constant(MethodFlags::_misc_force_inline)                       \\\n+  declare_constant(MethodFlags::_misc_dont_inline)                        \\\n@@ -698,0 +716,4 @@\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::stw_instruction_and_data_patch))  \\\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::conc_instruction_and_data_patch)) \\\n+  AARCH64_ONLY(declare_constant(NMethodPatchingType::conc_data_patch))                 \\\n+                                                                          \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":38,"deletions":16,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -128,1 +128,0 @@\n-  out->print_cr(\"MetaspaceReclaimPolicy: %s\", MetaspaceReclaimPolicy);\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceReporter.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -399,0 +399,4 @@\n+#if INCLUDE_JVMCI\n+  static ByteSize subklass_offset()              { return in_ByteSize(offset_of(Klass, _subklass)); }\n+  static ByteSize next_sibling_offset()          { return in_ByteSize(offset_of(Klass, _next_sibling)); }\n+#endif\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -53,1 +53,2 @@\n-\/\/    [ptr             | 00]  locked             ptr points to real header on stack\n+\/\/    [ptr             | 00]  locked             ptr points to real header on stack (stack-locking in use)\n+\/\/    [header          | 00]  locked             locked regular object header (fast-locking in use)\n@@ -55,1 +56,1 @@\n-\/\/    [ptr             | 10]  monitor            inflated lock (header is wapped out)\n+\/\/    [ptr             | 10]  monitor            inflated lock (header is swapped out)\n@@ -57,1 +58,1 @@\n-\/\/    [0 ............ 0| 00]  inflating          inflation in progress\n+\/\/    [0 ............ 0| 00]  inflating          inflation in progress (stack-locking in use)\n@@ -172,0 +173,1 @@\n+  \/\/ Fast-locking does not use INFLATING.\n@@ -186,1 +188,2 @@\n-    return !UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n+    assert(LockingMode == LM_LEGACY, \"should only be called with legacy stack locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n@@ -194,1 +197,2 @@\n-    return UseFastLocking && ((value() & lock_mask_in_place) == locked_value);\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"should only be called with new lightweight locking\");\n+    return (value() & lock_mask_in_place) == locked_value;\n@@ -197,0 +201,1 @@\n+    \/\/ Clear the lock_mask_in_place bits to set locked_value:\n@@ -210,2 +215,2 @@\n-    return UseFastLocking ? lockbits == monitor_value   \/\/ monitor?\n-                    : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n+    return LockingMode == LM_LIGHTWEIGHT  ? lockbits == monitor_value   \/\/ monitor?\n+                                          : (lockbits & unlocked_value) == 0; \/\/ monitor | stack-locked?\n","filename":"src\/hotspot\/share\/oops\/markWord.hpp","additions":12,"deletions":7,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -124,1 +124,1 @@\n-  \/\/ at a safepoint, it must not be zero.\n+  \/\/ at a safepoint, it must not be zero, except when using the new lightweight locking.\n@@ -127,1 +127,1 @@\n-  if (ignore_mark_word || UseFastLocking) {\n+  if (ignore_mark_word) {\n@@ -133,1 +133,1 @@\n-  return !SafepointSynchronize::is_at_safepoint();\n+  return LockingMode == LM_LIGHTWEIGHT || !SafepointSynchronize::is_at_safepoint();\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -90,1 +90,1 @@\n-  assert(UseFastLocking, \"Only safe with fast-locking\");\n+  assert(LockingMode != LM_LEGACY, \"Not safe with legacy stack-locking\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -77,0 +77,5 @@\n+\/\/ Extra verification when creating and using oops.\n+\/\/ Used to catch broken oops as soon as possible.\n+using CheckOopFunctionPointer = void(*)(oopDesc*);\n+extern CheckOopFunctionPointer check_oop_function;\n+\n@@ -83,3 +88,6 @@\n-  void register_if_checking() {\n-    if (CheckUnhandledOops) register_oop();\n-  }\n+  \/\/ Extra verification of the oop\n+  void check_oop() const { if (check_oop_function != nullptr && _o != nullptr) check_oop_function(_o); }\n+\n+  void on_usage() const  { check_oop(); }\n+  void on_construction() { check_oop(); if (CheckUnhandledOops)   register_oop(); }\n+  void on_destruction()  {              if (CheckUnhandledOops) unregister_oop(); }\n@@ -88,3 +96,3 @@\n-  oop()             : _o(nullptr) { register_if_checking(); }\n-  oop(const oop& o) : _o(o._o)    { register_if_checking(); }\n-  oop(oopDesc* o)   : _o(o)       { register_if_checking(); }\n+  oop()             : _o(nullptr) { on_construction(); }\n+  oop(const oop& o) : _o(o._o)    { on_construction(); }\n+  oop(oopDesc* o)   : _o(o)       { on_construction(); }\n@@ -92,1 +100,1 @@\n-    if (CheckUnhandledOops) unregister_oop();\n+    on_destruction();\n@@ -95,3 +103,4 @@\n-  oopDesc* obj() const                 { return _o; }\n-  oopDesc* operator->() const          { return _o; }\n-  operator oopDesc* () const           { return _o; }\n+  oopDesc* obj() const                  { on_usage(); return _o; }\n+\n+  oopDesc* operator->() const           { return obj(); }\n+  operator oopDesc* () const            { return obj(); }\n@@ -99,2 +108,2 @@\n-  bool operator==(const oop& o) const  { return _o == o._o; }\n-  bool operator!=(const oop& o) const  { return _o != o._o; }\n+  bool operator==(const oop& o) const   { return obj() == o.obj(); }\n+  bool operator!=(const oop& o) const   { return obj() != o.obj(); }\n@@ -102,2 +111,2 @@\n-  bool operator==(std::nullptr_t) const     { return _o == nullptr; }\n-  bool operator!=(std::nullptr_t) const     { return _o != nullptr; }\n+  bool operator==(std::nullptr_t) const { return obj() == nullptr; }\n+  bool operator!=(std::nullptr_t) const { return obj() != nullptr; }\n@@ -105,1 +114,1 @@\n-  oop& operator=(const oop& o)         { _o = o._o; return *this; }\n+  oop& operator=(const oop& o)          { _o = o.obj(); return *this; }\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":24,"deletions":15,"binary":false,"changes":39,"status":"modified"},{"patch":"@@ -89,8 +89,0 @@\n-class C2CheckLockStackStub : public C2CodeStub {\n-public:\n-  C2CheckLockStackStub() : C2CodeStub() {}\n-\n-  int max_size() const;\n-  void emit(C2_MacroAssembler& masm);\n-};\n-\n@@ -106,1 +98,1 @@\n-  Register tmp()     { return _tmp; }\n+  Register tmp() { return _tmp; }\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":1,"deletions":9,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -113,3 +113,4 @@\n-address OptoRuntime::_notify_jvmti_object_alloc                   = nullptr;\n-address OptoRuntime::_notify_jvmti_mount                          = nullptr;\n-address OptoRuntime::_notify_jvmti_unmount                        = nullptr;\n+address OptoRuntime::_notify_jvmti_vthread_start                  = nullptr;\n+address OptoRuntime::_notify_jvmti_vthread_end                    = nullptr;\n+address OptoRuntime::_notify_jvmti_vthread_mount                  = nullptr;\n+address OptoRuntime::_notify_jvmti_vthread_unmount                = nullptr;\n@@ -157,3 +158,4 @@\n-  gen(env, _notify_jvmti_object_alloc      , notify_jvmti_object_alloc_Type, SharedRuntime::notify_jvmti_object_alloc, 0, true, false);\n-  gen(env, _notify_jvmti_mount             , notify_jvmti_Type            , SharedRuntime::notify_jvmti_mount,   0 , true, false);\n-  gen(env, _notify_jvmti_unmount           , notify_jvmti_Type            , SharedRuntime::notify_jvmti_unmount, 0 , true, false);\n+  gen(env, _notify_jvmti_vthread_start     , notify_jvmti_vthread_Type    , SharedRuntime::notify_jvmti_vthread_start, 0, true, false);\n+  gen(env, _notify_jvmti_vthread_end       , notify_jvmti_vthread_Type    , SharedRuntime::notify_jvmti_vthread_end,   0, true, false);\n+  gen(env, _notify_jvmti_vthread_mount     , notify_jvmti_vthread_Type    , SharedRuntime::notify_jvmti_vthread_mount, 0, true, false);\n+  gen(env, _notify_jvmti_vthread_unmount   , notify_jvmti_vthread_Type    , SharedRuntime::notify_jvmti_vthread_unmount, 0, true, false);\n@@ -481,1 +483,1 @@\n-const TypeFunc *OptoRuntime::notify_jvmti_object_alloc_Type() {\n+const TypeFunc *OptoRuntime::notify_jvmti_vthread_Type() {\n@@ -483,7 +485,4 @@\n-  const Type **fields = TypeTuple::fields(1);\n-  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n-\n-   \/\/ create result type (range)\n-   fields = TypeTuple::fields(1);\n-   fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ Returned oop\n+  const Type **fields = TypeTuple::fields(2);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ VirtualThread oop\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ jboolean\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+2,fields);\n@@ -491,1 +490,4 @@\n-   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+  \/\/ no result type needed\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n@@ -493,1 +495,1 @@\n-   return TypeFunc::make(domain, range);\n+  return TypeFunc::make(domain,range);\n@@ -1670,18 +1672,0 @@\n-  return TypeFunc::make(domain,range);\n-}\n-#endif\n-\n-#if INCLUDE_JVMTI\n-const TypeFunc *OptoRuntime::notify_jvmti_Type() {\n-  \/\/ create input type (domain)\n-  const Type **fields = TypeTuple::fields(3);\n-  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ VirtualThread oop\n-  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ jboolean\n-  fields[TypeFunc::Parms+2] = TypeInt::BOOL;        \/\/ jboolean\n-  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+3,fields);\n-\n-  \/\/ no result type needed\n-  fields = TypeTuple::fields(1);\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n-  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":18,"deletions":34,"binary":false,"changes":52,"status":"modified"},{"patch":"@@ -526,0 +526,2 @@\n+  { \"RefDiscoveryPolicy\",           JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n+  { \"MetaspaceReclaimPolicy\",       JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n@@ -1478,3 +1480,0 @@\n-      if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n@@ -1515,16 +1514,9 @@\n-  if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS && !UseCompressedOops) {\n-    if (UseCompressedClassPointers) {\n-      warning(\"UseCompressedClassPointers requires UseCompressedOops\");\n-    }\n-    FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-  } else {\n-    \/\/ Turn on UseCompressedClassPointers too\n-    if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-      FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-    }\n-    \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-    if (UseCompressedClassPointers) {\n-      if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-        warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-        FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-      }\n+  \/\/ Turn on UseCompressedClassPointers too\n+  if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n+    FLAG_SET_ERGO(UseCompressedClassPointers, true);\n+  }\n+  \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n+  if (UseCompressedClassPointers) {\n+    if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n+      warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n+      FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n@@ -1704,3 +1696,0 @@\n-          if (COMPRESSED_CLASS_POINTERS_DEPENDS_ON_COMPRESSED_OOPS) {\n-            FLAG_SET_ERGO(UseCompressedClassPointers, false);\n-          }\n@@ -1966,1 +1955,8 @@\n-#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64)\n+\n+#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM)\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    FLAG_SET_CMDLINE(LockingMode, LM_LEGACY);\n+    warning(\"New lightweight locking not supported on this platform\");\n+  }\n+#endif\n+\n@@ -1968,0 +1964,10 @@\n+    if (FLAG_IS_CMDLINE(LockingMode) && LockingMode != LM_MONITOR) {\n+      jio_fprintf(defaultStream::error_stream(),\n+                  \"Conflicting -XX:+UseHeavyMonitors and -XX:LockingMode=%d flags\", LockingMode);\n+      return false;\n+    }\n+    FLAG_SET_CMDLINE(LockingMode, LM_MONITOR);\n+  }\n+\n+#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64)\n+  if (LockingMode == LM_MONITOR) {\n@@ -1969,1 +1975,1 @@\n-                \"UseHeavyMonitors is not fully implemented on this architecture\");\n+                \"LockingMode == 0 (LM_MONITOR) is not fully implemented on this architecture\");\n@@ -1974,1 +1980,1 @@\n-  if (UseHeavyMonitors && UseRTMForStackLocks) {\n+  if (LockingMode == LM_MONITOR && UseRTMForStackLocks) {\n@@ -1976,1 +1982,1 @@\n-                \"-XX:+UseHeavyMonitors and -XX:+UseRTMForStackLocks are mutually exclusive\");\n+                \"LockingMode == 0 (LM_MONITOR) and -XX:+UseRTMForStackLocks are mutually exclusive\");\n@@ -1981,1 +1987,1 @@\n-  if (VerifyHeavyMonitors && !UseHeavyMonitors) {\n+  if (VerifyHeavyMonitors && LockingMode != LM_MONITOR) {\n@@ -1983,1 +1989,1 @@\n-                \"-XX:+VerifyHeavyMonitors requires -XX:+UseHeavyMonitors\");\n+                \"-XX:+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\");\n@@ -1986,1 +1992,0 @@\n-\n@@ -3076,0 +3081,5 @@\n+\n+    \/\/ String deduplication may cause CDS to iterate the strings in different order from one\n+    \/\/ run to another which resulting in non-determinstic CDS archives.\n+    \/\/ Disable UseStringDeduplication while dumping CDS archive.\n+    UseStringDeduplication = false;\n@@ -3129,2 +3139,2 @@\n-  if (UseCompactObjectHeaders && !UseFastLocking) {\n-    FLAG_SET_DEFAULT(UseFastLocking, true);\n+  if (UseCompactObjectHeaders && LockingMode == LM_LEGACY) {\n+    FLAG_SET_DEFAULT(LockingMode, LM_LIGHTWEIGHT);\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":41,"deletions":31,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -705,4 +705,0 @@\n-  product(bool, PostVirtualThreadCompatibleLifecycleEvents, true, EXPERIMENTAL, \\\n-               \"Post virtual thread ThreadStart and ThreadEnd events for \"  \\\n-               \"virtual thread unaware agents\")                             \\\n-                                                                            \\\n@@ -741,2 +737,3 @@\n-          \"off). The check is performed on GuaranteedSafepointInterval \"    \\\n-          \"or AsyncDeflationInterval.\")                                     \\\n+          \"off). The check is performed on GuaranteedSafepointInterval, \"   \\\n+          \"AsyncDeflationInterval or GuaranteedAsyncDeflationInterval, \"    \\\n+          \"whichever is lower.\")                                            \\\n@@ -1429,3 +1426,0 @@\n-  product(ccstr, MetaspaceReclaimPolicy, \"balanced\", DIAGNOSTIC,            \\\n-          \"options: balanced, aggressive\")                                  \\\n-                                                                            \\\n@@ -1995,2 +1989,6 @@\n-  product(bool, UseFastLocking, false, EXPERIMENTAL,                        \\\n-                \"Use fast-locking instead of stack-locking\")                \\\n+  product(int, LockingMode, LM_LEGACY, EXPERIMENTAL,                        \\\n+          \"Select locking mode: \"                                           \\\n+          \"0: monitors only (LM_MONITOR), \"                                 \\\n+          \"1: monitors & legacy stack-locking (LM_LEGACY, default), \"       \\\n+          \"2: monitors & new lightweight locking (LM_LIGHTWEIGHT)\")         \\\n+          range(0, 2)                                                       \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":9,"deletions":11,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -150,1 +150,1 @@\n-  \/\/   we achieve this by using the lowest two bits\n+  \/\/   we achieve this by using the lowest two bits.\n@@ -152,1 +152,1 @@\n-  \/\/   and small values encode much better\n+  \/\/   and small values encode much better.\n@@ -156,1 +156,7 @@\n-  #define ANONYMOUS_OWNER reinterpret_cast<void*>(1)\n+public:\n+  \/\/ NOTE: Typed as uintptr_t so that we can pick it up in SA, via vmStructs.\n+  static const uintptr_t ANONYMOUS_OWNER = 1;\n+\n+private:\n+  static void* anon_owner_ptr() { return reinterpret_cast<void*>(ANONYMOUS_OWNER); }\n+\n@@ -189,0 +195,1 @@\n+\n@@ -254,1 +261,1 @@\n-  intptr_t  is_entered(JavaThread* current) const;\n+  bool is_entered(JavaThread* current) const;\n@@ -276,1 +283,1 @@\n-    set_owner_from(NULL, ANONYMOUS_OWNER);\n+    set_owner_from(nullptr, anon_owner_ptr());\n@@ -280,1 +287,1 @@\n-    return owner_raw() == ANONYMOUS_OWNER;\n+    return owner_raw() == anon_owner_ptr();\n@@ -284,1 +291,1 @@\n-    set_owner_from(ANONYMOUS_OWNER, owner);\n+    set_owner_from(anon_owner_ptr(), owner);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":14,"deletions":7,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -27,0 +27,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -328,5 +329,12 @@\n-  if ((mark.is_fast_locked() && current->lock_stack().contains(oop(obj))) ||\n-      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n-    \/\/ Degenerate notify\n-    \/\/ stack-locked by caller so by definition the implied waitset is empty.\n-    return true;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if (mark.is_fast_locked() && current->lock_stack().contains(cast_to_oop(obj))) {\n+      \/\/ Degenerate notify\n+      \/\/ fast-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Degenerate notify\n+      \/\/ stack-locked by caller so by definition the implied waitset is empty.\n+      return true;\n+    }\n@@ -403,10 +411,10 @@\n-    \/\/ This Java Monitor is inflated so obj's header will never be\n-    \/\/ displaced to this thread's BasicLock. Make the displaced header\n-    \/\/ non-null so this BasicLock is not seen as recursive nor as\n-    \/\/ being locked. We do this unconditionally so that this thread's\n-    \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n-    \/\/ performance reasons, stack walkers generally first check for\n-    \/\/ stack-locking in the object's header, the second check is for\n-    \/\/ recursive stack-locking in the displaced header in the BasicLock,\n-    \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n-    if (!UseFastLocking) {\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ This Java Monitor is inflated so obj's header will never be\n+      \/\/ displaced to this thread's BasicLock. Make the displaced header\n+      \/\/ non-null so this BasicLock is not seen as recursive nor as\n+      \/\/ being locked. We do this unconditionally so that this thread's\n+      \/\/ BasicLock cannot be mis-interpreted by any stack walkers. For\n+      \/\/ performance reasons, stack walkers generally first check for\n+      \/\/ stack-locking in the object's header, the second check is for\n+      \/\/ recursive stack-locking in the displaced header in the BasicLock,\n+      \/\/ and last are the inflated Java Monitor (ObjectMonitor) checks.\n@@ -483,1 +491,1 @@\n-  return UseHeavyMonitors;\n+  return LockingMode == LM_MONITOR;\n@@ -503,1 +511,2 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n@@ -505,4 +514,3 @@\n-\n-      markWord header = obj()->mark_acquire();\n-      while (true) {\n-        if (header.is_neutral()) {\n+      if (lock_stack.can_push()) {\n+        markWord mark = obj()->mark_acquire();\n+        if (mark.is_neutral()) {\n@@ -510,4 +518,4 @@\n-          \/\/ Try to swing into 'fast-locked' state without inflating.\n-          markWord locked_header = header.set_fast_locked();\n-          markWord witness = obj()->cas_set_mark(locked_header, header);\n-          if (witness == header) {\n+          \/\/ Try to swing into 'fast-locked' state.\n+          markWord locked_mark = mark.set_fast_locked();\n+          markWord old_mark = obj()->cas_set_mark(locked_mark, mark);\n+          if (old_mark == mark) {\n@@ -518,5 +526,0 @@\n-          \/\/ Otherwise retry.\n-          header = witness;\n-        } else {\n-          \/\/ Fall-through to inflate-enter.\n-          break;\n@@ -525,1 +528,2 @@\n-    } else {\n+      \/\/ All other paths fall-through to inflate-enter.\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -536,1 +540,1 @@\n-                 current->is_lock_owned((address)mark.locker())) {\n+                 current->is_lock_owned((address) mark.locker())) {\n@@ -538,1 +542,1 @@\n-        assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n+        assert(lock != (BasicLock*) obj->mark().value(), \"don't relock with same BasicLock\");\n@@ -550,1 +554,1 @@\n-    guarantee(!obj->mark().has_locker() && !obj->mark().is_fast_locked(), \"must not be stack-locked\");\n+    guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -569,1 +573,2 @@\n-    if (UseFastLocking) {\n+    if (LockingMode == LM_LIGHTWEIGHT) {\n+      \/\/ Fast-locking does not use the 'lock' argument.\n@@ -571,7 +576,9 @@\n-        markWord unlocked_header = mark.set_unlocked();\n-        markWord witness = object->cas_set_mark(unlocked_header, mark);\n-        if (witness != mark) {\n-          \/\/ Another thread beat us, it can only have installed an anonymously locked monitor at this point.\n-          \/\/ Fetch that monitor, set owner correctly to this thread, and exit it (allowing waiting threads to enter).\n-          assert(witness.has_monitor(), \"must have monitor\");\n-          ObjectMonitor* monitor = witness.monitor();\n+        markWord unlocked_mark = mark.set_unlocked();\n+        markWord old_mark = object->cas_set_mark(unlocked_mark, mark);\n+        if (old_mark != mark) {\n+          \/\/ Another thread won the CAS, it must have inflated the monitor.\n+          \/\/ It can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and\n+          \/\/ exit it (allowing waiting threads to enter).\n+          assert(old_mark.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = old_mark.monitor();\n@@ -586,1 +593,1 @@\n-    } else {\n+    } else if (LockingMode == LM_LEGACY) {\n@@ -627,1 +634,1 @@\n-    guarantee(!object->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee((object->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -634,2 +641,2 @@\n-  if (UseFastLocking && monitor->is_owner_anonymous()) {\n-    \/\/ It must be us. Pop lock object from lock stack.\n+  if (LockingMode == LM_LIGHTWEIGHT && monitor->is_owner_anonymous()) {\n+    \/\/ It must be owned by us. Pop lock object from lock stack.\n@@ -731,4 +738,10 @@\n-  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n-      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -747,4 +760,10 @@\n-  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n-      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n-    \/\/ Not inflated so there can't be any waiters to notify.\n-    return;\n+  if (LockingMode == LM_LIGHTWEIGHT) {\n+    if ((mark.is_fast_locked() && current->lock_stack().contains(obj()))) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n+  } else if (LockingMode == LM_LEGACY) {\n+    if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ Not inflated so there can't be any waiters to notify.\n+      return;\n+    }\n@@ -776,1 +795,2 @@\n-  if (!mark.is_being_inflated() || UseFastLocking) {\n+  if (!mark.is_being_inflated() || LockingMode == LM_LIGHTWEIGHT) {\n+    \/\/ New lightweight locking does not use the markWord::INFLATING() protocol.\n@@ -891,0 +911,2 @@\n+\/\/ Can be called from non JavaThreads (e.g., VMThread) for FastHashCode\n+\/\/ calculations as part of JVM\/TI tagging.\n@@ -892,2 +914,2 @@\n-  assert(UseFastLocking, \"only call this with fast-locking enabled\");\n-  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only call this with new lightweight locking enabled\");\n+  return thread->is_Java_thread() ? JavaThread::cast(thread)->lock_stack().contains(obj) : false;\n@@ -904,2 +926,2 @@\n-      assert(UseHeavyMonitors, \"+VerifyHeavyMonitors requires +UseHeavyMonitors\");\n-      guarantee(!mark.has_locker(), \"must not be stack locked\");\n+      assert(LockingMode == LM_MONITOR, \"+VerifyHeavyMonitors requires LockingMode == 0 (LM_MONITOR)\");\n+      guarantee((obj->mark().value() & markWord::lock_mask_in_place) != markWord::locked_value, \"must not be lightweight\/stack-locked\");\n@@ -950,2 +972,2 @@\n-    } else if (mark.is_fast_locked() && is_lock_owned(current, obj)) {\n-      \/\/ This is a fast lock owned by the calling thread so use the\n+    } else if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast-lock owned by the calling thread so use the\n@@ -957,2 +979,2 @@\n-    } else if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n-      \/\/ This is a stack lock owned by the calling thread so fetch the\n+    } else if (LockingMode == LM_LEGACY && mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+      \/\/ This is a stack-lock owned by the calling thread so fetch the\n@@ -969,1 +991,1 @@\n-      \/\/ So we have to inflate the stack lock into an ObjectMonitor\n+      \/\/ So we have to inflate the stack-lock into an ObjectMonitor\n@@ -1022,2 +1044,2 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked case, header points into owner's stack\n@@ -1027,2 +1049,2 @@\n-  \/\/ Fast-locking case.\n-  if (mark.is_fast_locked()) {\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locking case, see if lock is in current's lock stack\n@@ -1032,1 +1054,1 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n+    \/\/ Inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -1048,2 +1070,3 @@\n-  \/\/ Uncontended case, header points to stack\n-  if (mark.has_locker()) {\n+  if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+    \/\/ stack-locked so header points into owner's stack.\n+    \/\/ owning_thread_from_monitor_owner() may also return null here:\n@@ -1053,1 +1076,3 @@\n-  if (mark.is_fast_locked()) {\n+  if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n+    \/\/ fast-locked so get owner from the object.\n+    \/\/ owning_thread_from_object() may also return null here:\n@@ -1057,1 +1082,1 @@\n-  \/\/ Contended case, header points to ObjectMonitor (tagged pointer)\n+    \/\/ Inflated monitor so header points to ObjectMonitor (tagged pointer).\n@@ -1063,0 +1088,1 @@\n+    \/\/ owning_thread_from_monitor() may also return null here:\n@@ -1066,0 +1092,5 @@\n+  \/\/ Unlocked case, header in place\n+  \/\/ Cannot have assertion since this object may have been\n+  \/\/ locked by another thread when reaching here.\n+  \/\/ assert(mark.is_neutral(), \"sanity check\");\n+\n@@ -1072,1 +1103,1 @@\n-\/\/ ObjectMonitors where owner is set to a stack lock address in thread.\n+\/\/ ObjectMonitors where owner is set to a stack-lock address in thread.\n@@ -1082,1 +1113,1 @@\n-      \/\/ is set to a stack lock address in the target thread.\n+      \/\/ is set to a stack-lock address in the target thread.\n@@ -1108,1 +1139,1 @@\n-    \/\/ Owner set to a stack lock address in thread should never be seen here:\n+    \/\/ Owner set to a stack-lock address in thread should never be seen here:\n@@ -1292,4 +1323,11 @@\n-    \/\/ *  Inflated     - just return\n-    \/\/ *  Stack-locked - coerce it to inflated\n-    \/\/ *  INFLATING    - busy wait for conversion to complete\n-    \/\/ *  Neutral      - aggressively inflate the object.\n+    \/\/ *  inflated     - Just return if using stack-locking.\n+    \/\/                   If using fast-locking and the ObjectMonitor owner\n+    \/\/                   is anonymous and the current thread owns the\n+    \/\/                   object lock, then we make the current thread the\n+    \/\/                   ObjectMonitor owner and remove the lock from the\n+    \/\/                   current thread's lock stack.\n+    \/\/ *  fast-locked  - Coerce it to inflated from fast-locked.\n+    \/\/ *  stack-locked - Coerce it to inflated from stack-locked.\n+    \/\/ *  INFLATING    - Busy wait for conversion from stack-locked to\n+    \/\/                   inflated.\n+    \/\/ *  neutral      - Aggressively inflate the object.\n@@ -1302,1 +1340,1 @@\n-      if (UseFastLocking && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+      if (LockingMode == LM_LIGHTWEIGHT && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n@@ -1304,2 +1342,1 @@\n-        assert(current->is_Java_thread(), \"must be Java thread\");\n-        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        JavaThread::cast(current)->lock_stack().remove(object);\n@@ -1310,12 +1347,12 @@\n-    \/\/ CASE: inflation in progress - inflating over a stack-lock.\n-    \/\/ Some other thread is converting from stack-locked to inflated.\n-    \/\/ Only that thread can complete inflation -- other threads must wait.\n-    \/\/ The INFLATING value is transient.\n-    \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n-    \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n-    \/\/ NOTE: We need to check UseFastLocking here, because with fast-locking, the header\n-    \/\/ may legitimately be zero: cleared lock-bits and all upper header bits zero.\n-    \/\/ With fast-locking, the INFLATING protocol is not used.\n-    if (mark == markWord::INFLATING() && !UseFastLocking) {\n-      read_stable_mark(object);\n-      continue;\n+    if (LockingMode != LM_LIGHTWEIGHT) {\n+      \/\/ New lightweight locking does not use INFLATING.\n+      \/\/ CASE: inflation in progress - inflating over a stack-lock.\n+      \/\/ Some other thread is converting from stack-locked to inflated.\n+      \/\/ Only that thread can complete inflation -- other threads must wait.\n+      \/\/ The INFLATING value is transient.\n+      \/\/ Currently, we spin\/yield\/park and poll the markword, waiting for inflation to finish.\n+      \/\/ We could always eliminate polling by parking the thread on some auxiliary list.\n+      if (mark == markWord::INFLATING()) {\n+        read_stable_mark(object);\n+        continue;\n+      }\n@@ -1324,2 +1361,9 @@\n-    \/\/ CASE: stack-locked\n-    \/\/ Could be stack-locked either by this thread or by some other thread.\n+    \/\/ CASE: fast-locked\n+    \/\/ Could be fast-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_\n+    \/\/ attempting to set the object's mark to the new ObjectMonitor. If\n+    \/\/ this thread owns the monitor, then we set the ObjectMonitor's\n+    \/\/ owner to this thread. Otherwise, we set the ObjectMonitor's owner\n+    \/\/ to anonymous. If we lose the race to set the object's mark to the\n+    \/\/ new ObjectMonitor, then we just delete it and loop around again.\n@@ -1327,9 +1371,1 @@\n-    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_ attempting\n-    \/\/ to install INFLATING into the mark word.  We originally installed INFLATING,\n-    \/\/ allocated the ObjectMonitor, and then finally STed the address of the\n-    \/\/ ObjectMonitor into the mark.  This was correct, but artificially lengthened\n-    \/\/ the interval in which INFLATING appeared in the mark, thus increasing\n-    \/\/ the odds of inflation contention.\n-\n-    if (mark.is_fast_locked()) {\n-      assert(UseFastLocking, \"can only happen with fast-locking\");\n+    if (LockingMode == LM_LIGHTWEIGHT && mark.is_fast_locked()) {\n@@ -1348,2 +1384,2 @@\n-      markWord witness = object->cas_set_mark(monitor_mark, mark);\n-      if (witness == mark) {\n+      markWord old_mark = object->cas_set_mark(monitor_mark, mark);\n+      if (old_mark == mark) {\n@@ -1352,2 +1388,1 @@\n-          assert(current->is_Java_thread(), \"must be: checked in is_lock_owned()\");\n-          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+          JavaThread::cast(current)->lock_stack().remove(object);\n@@ -1374,1 +1409,1 @@\n-        continue;\n+        continue;  \/\/ Interference -- just retry\n@@ -1378,2 +1413,13 @@\n-    if (mark.has_locker()) {\n-      assert(!UseFastLocking, \"can not happen with fast-locking\");\n+    \/\/ CASE: stack-locked\n+    \/\/ Could be stack-locked either by current or by some other thread.\n+    \/\/\n+    \/\/ Note that we allocate the ObjectMonitor speculatively, _before_ attempting\n+    \/\/ to install INFLATING into the mark word.  We originally installed INFLATING,\n+    \/\/ allocated the ObjectMonitor, and then finally STed the address of the\n+    \/\/ ObjectMonitor into the mark.  This was correct, but artificially lengthened\n+    \/\/ the interval in which INFLATING appeared in the mark, thus increasing\n+    \/\/ the odds of inflation contention. If we lose the race to set INFLATING,\n+    \/\/ then we just delete the ObjectMonitor and loop around again.\n+    \/\/\n+    if (LockingMode == LM_LEGACY && mark.has_locker()) {\n+      assert(LockingMode != LM_LIGHTWEIGHT, \"cannot happen with new lightweight locking\");\n@@ -1541,2 +1587,2 @@\n-\/\/ is set to a stack lock address are NOT associated with the JavaThread\n-\/\/ that holds that stack lock. All of the current consumers of\n+\/\/ is set to a stack-lock address are NOT associated with the JavaThread\n+\/\/ that holds that stack-lock. All of the current consumers of\n@@ -1544,1 +1590,1 @@\n-\/\/ those do not have the owner set to a stack lock address.\n+\/\/ those do not have the owner set to a stack-lock address.\n@@ -1561,1 +1607,1 @@\n-      \/\/ not include when owner is set to a stack lock address in thread.\n+      \/\/ not include when owner is set to a stack-lock address in thread.\n@@ -1644,1 +1690,1 @@\n-    if (current->is_Java_thread()) {\n+    if (current->is_monitor_deflation_thread()) {\n@@ -1661,0 +1707,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1670,0 +1719,4 @@\n+    } else {\n+      \/\/ This is not a monitor deflation thread.\n+      \/\/ No handshake or rendezvous is needed when we are already at safepoint.\n+      assert_at_safepoint();\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":171,"deletions":118,"binary":false,"changes":289,"status":"modified"},{"patch":"@@ -94,1 +94,0 @@\n-  template(RendezvousGCThreads)                   \\\n@@ -111,1 +110,2 @@\n-  template(JvmtiPostObjectFree)\n+  template(JvmtiPostObjectFree)                   \\\n+  template(RendezvousGCThreads)\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -51,0 +51,1 @@\n+#include \"gc\/shared\/stringdedup\/stringDedupThread.hpp\"\n@@ -301,1 +302,0 @@\n-  nonstatic_field(Method,                      _flags,                                        u2)                                    \\\n@@ -310,1 +310,1 @@\n-  nonstatic_field(ConstMethod,                 _flags,                                        u2)                                    \\\n+  nonstatic_field(ConstMethod,                 _flags._flags,                                 u4)                                    \\\n@@ -706,2 +706,2 @@\n-  nonstatic_field(LockStack,                   _current,                                      oop*)                                  \\\n-  nonstatic_field(LockStack,                   _base,                                         oop*)                                  \\\n+  nonstatic_field(LockStack,                   _top,                                          uint32_t)                              \\\n+  nonstatic_field(LockStack,                   _base[0],                                      oop)                                   \\\n@@ -1317,0 +1317,1 @@\n+        declare_type(StringDedupThread, JavaThread)                       \\\n@@ -2090,10 +2091,0 @@\n-  declare_constant(JVM_ACC_MONITOR_MATCH)                                 \\\n-  declare_constant(JVM_ACC_HAS_MONITOR_BYTECODES)                         \\\n-  declare_constant(JVM_ACC_HAS_LOOPS)                                     \\\n-  declare_constant(JVM_ACC_LOOPS_FLAG_INIT)                               \\\n-  declare_constant(JVM_ACC_QUEUED)                                        \\\n-  declare_constant(JVM_ACC_NOT_C2_OSR_COMPILABLE)                         \\\n-  declare_constant(JVM_ACC_HAS_JSRS)                                      \\\n-  declare_constant(JVM_ACC_IS_OLD)                                        \\\n-  declare_constant(JVM_ACC_IS_OBSOLETE)                                   \\\n-  declare_constant(JVM_ACC_IS_PREFIXED_NATIVE)                            \\\n@@ -2183,0 +2174,3 @@\n+  declare_constant(Method::nonvirtual_vtable_index)                       \\\n+  declare_constant(Method::extra_stack_entries_for_jsr292)                \\\n+                                                                          \\\n@@ -2187,20 +2181,10 @@\n-  declare_constant(Method::_caller_sensitive)                             \\\n-  declare_constant(Method::_force_inline)                                 \\\n-  declare_constant(Method::_dont_inline)                                  \\\n-  declare_constant(Method::_hidden)                                       \\\n-  declare_constant(Method::_changes_current_thread)                       \\\n-                                                                          \\\n-  declare_constant(Method::nonvirtual_vtable_index)                       \\\n-                                                                          \\\n-  declare_constant(Method::extra_stack_entries_for_jsr292)                \\\n-                                                                          \\\n-  declare_constant(ConstMethod::_has_linenumber_table)                    \\\n-  declare_constant(ConstMethod::_has_checked_exceptions)                  \\\n-  declare_constant(ConstMethod::_has_localvariable_table)                 \\\n-  declare_constant(ConstMethod::_has_exception_table)                     \\\n-  declare_constant(ConstMethod::_has_generic_signature)                   \\\n-  declare_constant(ConstMethod::_has_method_parameters)                   \\\n-  declare_constant(ConstMethod::_has_method_annotations)                  \\\n-  declare_constant(ConstMethod::_has_parameter_annotations)               \\\n-  declare_constant(ConstMethod::_has_default_annotations)                 \\\n-  declare_constant(ConstMethod::_has_type_annotations)                    \\\n+  declare_constant(ConstMethodFlags::_misc_has_linenumber_table)          \\\n+  declare_constant(ConstMethodFlags::_misc_has_checked_exceptions)        \\\n+  declare_constant(ConstMethodFlags::_misc_has_localvariable_table)       \\\n+  declare_constant(ConstMethodFlags::_misc_has_exception_table)           \\\n+  declare_constant(ConstMethodFlags::_misc_has_generic_signature)         \\\n+  declare_constant(ConstMethodFlags::_misc_has_method_parameters)         \\\n+  declare_constant(ConstMethodFlags::_misc_has_method_annotations)        \\\n+  declare_constant(ConstMethodFlags::_misc_has_parameter_annotations)     \\\n+  declare_constant(ConstMethodFlags::_misc_has_default_annotations)       \\\n+  declare_constant(ConstMethodFlags::_misc_has_type_annotations)          \\\n@@ -2436,0 +2420,8 @@\n+  \/**********************************************\/                        \\\n+  \/* LockingMode enum (globalDefinitions.hpp) *\/                          \\\n+  \/**********************************************\/                        \\\n+                                                                          \\\n+  declare_constant(LM_MONITOR)                                            \\\n+  declare_constant(LM_LEGACY)                                             \\\n+  declare_constant(LM_LIGHTWEIGHT)                                        \\\n+                                                                          \\\n@@ -2621,2 +2613,4 @@\n-  declare_constant(InvocationCounter::count_shift)\n-\n+  declare_constant(InvocationCounter::count_shift)                        \\\n+                                                                          \\\n+  \/* ObjectMonitor constants *\/                                           \\\n+  declare_constant(ObjectMonitor::ANONYMOUS_OWNER)                        \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":30,"deletions":36,"binary":false,"changes":66,"status":"modified"},{"patch":"@@ -1035,0 +1035,9 @@\n+enum LockingMode {\n+  \/\/ Use only heavy monitors for locking\n+  LM_MONITOR     = 0,\n+  \/\/ Legacy stack-locking, with monitors as 2nd tier\n+  LM_LEGACY      = 1,\n+  \/\/ New lightweight locking, with monitors as 2nd tier\n+  LM_LIGHTWEIGHT = 2\n+};\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -952,0 +952,8 @@\n+  STEP_IF(\"printing registered callbacks\", _verbose && _thread != nullptr);\n+    for (VMErrorCallback* callback = _thread->_vm_error_callbacks;\n+        callback != nullptr;\n+        callback = callback->_next) {\n+      callback->call(st);\n+      st->cr();\n+    }\n+\n@@ -1900,0 +1908,11 @@\n+\n+VMErrorCallbackMark::VMErrorCallbackMark(VMErrorCallback* callback)\n+  : _thread(Thread::current()) {\n+  callback->_next = _thread->_vm_error_callbacks;\n+  _thread->_vm_error_callbacks = callback;\n+}\n+\n+VMErrorCallbackMark::~VMErrorCallbackMark() {\n+  assert(_thread->_vm_error_callbacks != nullptr, \"Popped too far\");\n+  _thread->_vm_error_callbacks = _thread->_vm_error_callbacks->_next;\n+}\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":19,"deletions":0,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2001, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2001, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -109,5 +109,0 @@\n-  \/* Constants in markWord used by CMS. *\/\n-  private static long cmsShift;\n-  private static long cmsMask;\n-  private static long sizeShift;\n-\n@@ -219,1 +214,1 @@\n-  public long getSize() { return (long)(value() >> sizeShift); }\n+  public long getSize() { return (long)value(); }\n","filename":"src\/jdk.hotspot.agent\/share\/classes\/sun\/jvm\/hotspot\/oops\/Mark.java","additions":2,"deletions":7,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -153,1 +153,1 @@\n-    final int methodFlagsOffset = getFieldOffset(\"Method::_flags\", Integer.class, \"u2\");\n+    final int methodFlagsOffset = getFieldOffset(\"Method::_flags._status\", Integer.class, \"u4\");\n@@ -159,5 +159,2 @@\n-    final int methodFlagsCallerSensitive = getConstant(\"Method::_caller_sensitive\", Integer.class);\n-    final int methodFlagsForceInline = getConstant(\"Method::_force_inline\", Integer.class);\n-    final int methodFlagsIntrinsicCandidate = getConstant(\"Method::_intrinsic_candidate\", Integer.class);\n-    final int methodFlagsDontInline = getConstant(\"Method::_dont_inline\", Integer.class);\n-    final int methodFlagsReservedStackAccess = getConstant(\"Method::_reserved_stack_access\", Integer.class);\n+    final int methodFlagsForceInline = getConstant(\"MethodFlags::_misc_force_inline\", Integer.class);\n+    final int methodFlagsDontInline = getConstant(\"MethodFlags::_misc_dont_inline\", Integer.class);\n@@ -194,1 +191,1 @@\n-    final int constMethodFlagsOffset = getFieldOffset(\"ConstMethod::_flags\", Integer.class, \"u2\");\n+    final int constMethodFlagsOffset = getFieldOffset(\"ConstMethod::_flags._flags\", Integer.class, \"u4\");\n@@ -202,5 +199,8 @@\n-    final int constMethodHasLineNumberTable = getConstant(\"ConstMethod::_has_linenumber_table\", Integer.class);\n-    final int constMethodHasLocalVariableTable = getConstant(\"ConstMethod::_has_localvariable_table\", Integer.class);\n-    final int constMethodHasMethodAnnotations = getConstant(\"ConstMethod::_has_method_annotations\", Integer.class);\n-    final int constMethodHasParameterAnnotations = getConstant(\"ConstMethod::_has_parameter_annotations\", Integer.class);\n-    final int constMethodHasExceptionTable = getConstant(\"ConstMethod::_has_exception_table\", Integer.class);\n+    final int constMethodFlagsReservedStackAccess = getConstant(\"ConstMethodFlags::_misc_reserved_stack_access\", Integer.class);\n+    final int constMethodFlagsCallerSensitive = getConstant(\"ConstMethodFlags::_misc_caller_sensitive\", Integer.class);\n+    final int constMethodFlagsIntrinsicCandidate = getConstant(\"ConstMethodFlags::_misc_intrinsic_candidate\", Integer.class);\n+    final int constMethodHasLineNumberTable = getConstant(\"ConstMethodFlags::_misc_has_linenumber_table\", Integer.class);\n+    final int constMethodHasLocalVariableTable = getConstant(\"ConstMethodFlags::_misc_has_localvariable_table\", Integer.class);\n+    final int constMethodHasMethodAnnotations = getConstant(\"ConstMethodFlags::_misc_has_method_annotations\", Integer.class);\n+    final int constMethodHasParameterAnnotations = getConstant(\"ConstMethodFlags::_misc_has_parameter_annotations\", Integer.class);\n+    final int constMethodHasExceptionTable = getConstant(\"ConstMethodFlags::_misc_has_exception_table\", Integer.class);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -136,0 +136,1 @@\n+gtest\/NMTGtests.java 8306561 aix-ppc64\n@@ -150,0 +151,1 @@\n+vmTestbase\/nsk\/jvmti\/AttachOnDemand\/attach002a\/TestDescription.java 8307462 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -128,0 +128,7 @@\n+serviceability_ttf_virtual = \\\n+  serviceability\/ \\\n+  -serviceability\/jvmti\/vthread \\\n+  -serviceability\/jvmti\/thread  \\\n+  -serviceability\/jvmti\/events  \\\n+  -serviceability\/jvmti\/negative\n+\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -35,2 +35,1 @@\n- * @requires vm.compiler2.enabled\n- * @requires vm.cpu.features ~= \".*avx2.*\" | vm.cpu.features ~= \".*asimd.*\"\n+ * @requires vm.compiler2.enabled\n@@ -102,2 +101,2 @@\n-\/\/        init(goldI5, goldF5);\n-\/\/        test5(goldI5, goldI5, goldF5, goldF5);\n+        init(goldI5, goldF5);\n+        test5(goldI5, goldI5, goldF5, goldF5);\n@@ -240,23 +239,21 @@\n-\/\/ TODO uncomment after fixing JDK-8304720\n-\/\/\n-\/\/    @Run(test = \"test5\")\n-\/\/    public void runTest5() {\n-\/\/        int[] dataI = new int[RANGE];\n-\/\/        float[] dataF = new float[RANGE];\n-\/\/        init(dataI, dataF);\n-\/\/        test5(dataI, dataI, dataF, dataF);\n-\/\/        verify(\"test5\", dataI, goldI5);\n-\/\/        verify(\"test5\", dataF, goldF5);\n-\/\/    }\n-\/\/\n-\/\/    @Test\n-\/\/    static void test5(int[] dataIa, int[] dataIb, float[] dataFa, float[] dataFb) {\n-\/\/        for (int i = 0; i < RANGE; i+=2) {\n-\/\/            \/\/ same as test2, except that reordering leads to different semantics\n-\/\/            \/\/ explanation analogue to test4\n-\/\/            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, dataIa[i+0] + 1); \/\/ A\n-\/\/            dataIb[i+0] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0); \/\/ X\n-\/\/            dataIb[i+1] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4); \/\/ Y\n-\/\/            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, dataIa[i+1] + 1); \/\/ B\n-\/\/        }\n-\/\/    }\n+    @Run(test = \"test5\")\n+    public void runTest5() {\n+        int[] dataI = new int[RANGE];\n+        float[] dataF = new float[RANGE];\n+        init(dataI, dataF);\n+        test5(dataI, dataI, dataF, dataF);\n+        verify(\"test5\", dataI, goldI5);\n+        verify(\"test5\", dataF, goldF5);\n+    }\n+\n+    @Test\n+    static void test5(int[] dataIa, int[] dataIb, float[] dataFa, float[] dataFb) {\n+        for (int i = 0; i < RANGE; i+=2) {\n+            \/\/ same as test2, except that reordering leads to different semantics\n+            \/\/ explanation analogue to test4\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 0, dataIa[i+0] + 1); \/\/ A\n+            dataIb[i+0] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 0); \/\/ X\n+            dataIb[i+1] = 11 * unsafe.getInt(dataFb, unsafe.ARRAY_INT_BASE_OFFSET + 4 * i + 4); \/\/ Y\n+            unsafe.putInt(dataFa, unsafe.ARRAY_FLOAT_BASE_OFFSET + 4 * i + 4, dataIa[i+1] + 1); \/\/ B\n+        }\n+    }\n","filename":"test\/hotspot\/jtreg\/compiler\/loopopts\/superword\/TestIndependentPacksWithCyclicDependency.java","additions":24,"deletions":27,"binary":false,"changes":51,"status":"modified"},{"patch":"@@ -43,28 +43,0 @@\n-\n-\n-\n-\/* @test id=reclaim-aggressive-debug\n- * @bug 8251158\n- * @summary Run metaspace-related gtests for reclaim policy aggressive (with verifications)\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.xml\n- * @requires vm.debug\n- * @requires vm.flagless\n- * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockDiagnosticVMOptions -XX:MetaspaceReclaimPolicy=aggressive -XX:VerifyMetaspaceInterval=3\n- *\/\n-\n-\/* @test id=reclaim-aggressive-ndebug\n- * @bug 8251158\n- * @summary Run metaspace-related gtests for reclaim policy aggressive\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.xml\n- * @requires vm.debug == false\n- * @requires vm.flagless\n- * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockDiagnosticVMOptions -XX:MetaspaceReclaimPolicy=aggressive\n- *\/\n-\n-\n-\n-\n@@ -81,3 +53,0 @@\n-\n-\n-\n@@ -91,1 +60,1 @@\n- * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockDiagnosticVMOptions -XX:MetaspaceReclaimPolicy=balanced -XX:-UseCompressedClassPointers\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockDiagnosticVMOptions -XX:-UseCompressedClassPointers\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceGtests.java","additions":1,"deletions":32,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -39,10 +39,0 @@\n-\/*\n- * @test id=test-64bit-ccs-aggressivereclaim\n- * @summary Test the VM.metaspace command\n- * @requires vm.bits == \"64\"\n- * @library \/test\/lib\n- * @modules java.base\/jdk.internal.misc\n- *          java.management\n- * @run main\/othervm -Dwith-compressed-class-space -XX:MaxMetaspaceSize=201M -Xmx100M -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:+UnlockDiagnosticVMOptions -XX:MetaspaceReclaimPolicy=aggressive PrintMetaspaceDcmd\n- *\/\n-\n","filename":"test\/hotspot\/jtreg\/runtime\/Metaspace\/PrintMetaspaceDcmd.java","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"}]}