{"files":[{"patch":"@@ -1791,0 +1791,12 @@\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (C->comp_arena()) C2CheckLockStackStub();\n+    C->output()->add_stub(stub);\n+    __ ldr(r9, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    __ ldr(r10, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    __ add(r9, r9, max_monitors * oopSize);\n+    __ cmp(r9, r10);\n+    __ br(Assembler::GE, stub->entry());\n+    __ bind(stub->continuation());\n+  }\n+\n@@ -3825,1 +3837,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3843,17 +3855,2 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ orr(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n-                 \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n-      __ br(Assembler::EQ, cont);\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n+      if (UseFastLocking) {\n+        __ fast_lock(oop, disp_hdr, tmp, rscratch1, no_count, false);\n@@ -3861,9 +3858,33 @@\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ mov(rscratch1, sp);\n-      __ sub(disp_hdr, disp_hdr, rscratch1);\n-      __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n-      \/\/ If condition is true we are cont and hence we can store 0 as the\n-      \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-      __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n-      __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        \/\/ Indicate success at cont.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ orr(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(oop, tmp, box, Assembler::xword, \/*acquire*\/ true,\n+                   \/*release*\/ true, \/*weak*\/ false, disp_hdr);\n+        __ br(Assembler::EQ, cont);\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ mov(rscratch1, sp);\n+        __ sub(disp_hdr, disp_hdr, rscratch1);\n+        __ mov(tmp, (address) (~(os::vm_page_size()-1) | markWord::lock_mask_in_place));\n+        \/\/ If condition is true we are cont and hence we can store 0 as the\n+        \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+        __ ands(tmp\/*==0?*\/, disp_hdr, tmp);   \/\/ Sets flags for result\n+        __ str(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ b(cont);\n+      }\n@@ -3872,0 +3893,1 @@\n+      __ b(cont);\n@@ -3873,1 +3895,0 @@\n-    __ b(cont);\n@@ -3886,7 +3907,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n-    __ mov(tmp, (address)markWord::unused_mark().value());\n-    __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::enter.\n+      __ mov(tmp, (address)markWord::unused_mark().value());\n+      __ str(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -3907,0 +3929,1 @@\n+    __ bind(count);\n@@ -3920,1 +3943,1 @@\n-    Label no_count;\n+    Label count, no_count;\n@@ -3924,1 +3947,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -3938,3 +3961,2 @@\n-      \/\/ Check if it is still a light weight lock, this is is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        __ fast_unlock(oop, tmp, box, disp_hdr, no_count);\n@@ -3942,2 +3964,12 @@\n-      __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n-                 \/*release*\/ true, \/*weak*\/ false, tmp);\n+        \/\/ Indicate success at cont.\n+        __ cmp(oop, oop);\n+        __ b(count);\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n+\n+        __ cmpxchg(oop, box, disp_hdr, Assembler::xword, \/*acquire*\/ false,\n+                   \/*release*\/ true, \/*weak*\/ false, tmp);\n+        __ b(cont);\n+      }\n@@ -3946,0 +3978,1 @@\n+      __ b(cont);\n@@ -3947,1 +3980,0 @@\n-    __ b(cont);\n@@ -3955,0 +3987,14 @@\n+\n+    if (UseFastLocking) {\n+      \/\/ If the owner is anonymous, we need to fix it -- in an outline stub.\n+      Register tmp2 = disp_hdr;\n+      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      \/\/ We cannot use tbnz here, the target might be too far away and cannot\n+      \/\/ be encoded.\n+      __ tst(tmp2, (uint64_t)(intptr_t) ANONYMOUS_OWNER);\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmp, tmp2);\n+      Compile::current()->output()->add_stub(stub);\n+      __ br(Assembler::NE, stub->entry());\n+      __ bind(stub->continuation());\n+    }\n+\n@@ -3981,0 +4027,1 @@\n+    __ bind(count);\n@@ -7257,1 +7304,1 @@\n-  predicate(!needs_acquiring_load(n));\n+  predicate(!needs_acquiring_load(n) && !UseCompactObjectHeaders);\n@@ -7267,0 +7314,26 @@\n+instruct loadNKlassLilliput(iRegNNoSp dst, memory4 mem, rFlagsReg cr)\n+%{\n+  match(Set dst (LoadNKlass mem));\n+  effect(TEMP_DEF dst, KILL cr);\n+  predicate(!needs_acquiring_load(n) && UseCompactObjectHeaders);\n+\n+  ins_cost(4 * INSN_COST);\n+  format %{ \"ldrw  $dst, $mem\\t# compressed class ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset\");\n+    assert($mem$$index$$Register == noreg, \"expect no index\");\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ ldr(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    \/\/ NOTE: We can't use tbnz here, because the target is sometimes too far away\n+    \/\/ and cannot be encoded.\n+    __ tst(dst, markWord::monitor_value);\n+    __ br(Assembler::NE, stub->entry());\n+    __ bind(stub->continuation());\n+    __ lsr(dst, dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow);\n+%}\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":117,"deletions":44,"binary":false,"changes":161,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -245,1 +246,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -1232,1 +1233,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -2355,2 +2356,2 @@\n-        __ ldrw(tmp, src_klass_addr);\n-        __ ldrw(rscratch1, dst_klass_addr);\n+        __ load_nklass(tmp, src);\n+        __ load_nklass(rscratch1, dst);\n@@ -2359,2 +2360,2 @@\n-        __ ldr(tmp, src_klass_addr);\n-        __ ldr(rscratch1, dst_klass_addr);\n+        __ ldr(tmp, Address(src, oopDesc::klass_offset_in_bytes()));\n+        __ ldr(rscratch1, Address(dst, oopDesc::klass_offset_in_bytes()));\n@@ -2484,3 +2485,0 @@\n-    if (UseCompressedClassPointers) {\n-      __ encode_klass_not_null(tmp);\n-    }\n@@ -2489,8 +2487,1 @@\n-\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2498,7 +2489,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, src_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, src_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(src, tmp, rscratch1);\n@@ -2507,7 +2492,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ ldrw(rscratch1, dst_klass_addr);\n-        __ cmpw(tmp, rscratch1);\n-      } else {\n-        __ ldr(rscratch1, dst_klass_addr);\n-        __ cmp(tmp, rscratch1);\n-      }\n+      __ cmp_klass(dst, tmp, rscratch1);\n@@ -2594,1 +2573,12 @@\n-    __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+      __ ldr(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+      __ tst(result, markWord::monitor_value);\n+      __ br(Assembler::NE, *op->stub()->entry());\n+      __ bind(*op->stub()->continuation());\n+\n+      \/\/ Shift to get proper narrow Klass*.\n+      __ lsr(result, result, markWord::klass_shift);\n+    } else {\n+      __ ldrw(result, Address (obj, oopDesc::klass_offset_in_bytes()));\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":22,"deletions":32,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -761,50 +761,55 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    orr(swap_reg, rscratch1, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    Label fail;\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from sp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n-    \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n-    \/\/ copy\n-    mov(rscratch1, sp);\n-    sub(swap_reg, swap_reg, rscratch1);\n-    ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    str(swap_reg, Address(lock_reg, mark_offset));\n-    br(Assembler::EQ, count);\n-\n+    if (UseFastLocking) {\n+      ldr(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, rscratch1, rscratch2, slow_case);\n+      b(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      orr(swap_reg, rscratch1, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      Label fail;\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: sp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from sp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %r0 as the result of cmpxchg\n+      \/\/ NOTE2: aarch64 does not like to subtract sp from rn so take a\n+      \/\/ copy\n+      mov(rscratch1, sp);\n+      sub(swap_reg, swap_reg, rscratch1);\n+      ands(swap_reg, swap_reg, (uint64_t)(7 - (int)os::vm_page_size()));\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      str(swap_reg, Address(lock_reg, mark_offset));\n+      br(Assembler::EQ, count);\n+    }\n@@ -816,1 +821,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -853,3 +858,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %r0\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %r0\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -863,3 +870,2 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ldr(header_reg, Address(swap_reg,\n-                            BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n@@ -867,2 +873,16 @@\n-    \/\/ Test for recursion\n-    cbz(header_reg, count);\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = header_reg;\n+      ldr(tmp, Address(rthread, JavaThread::lock_stack_current_offset()));\n+      ldr(tmp, Address(tmp, -oopSize));\n+      cmpoop(tmp, obj_reg);\n+      br(Assembler::NE, slow_case);\n+\n+      ldr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, rscratch1, slow_case);\n+      b(count);\n+      bind(slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      ldr(header_reg, Address(swap_reg,\n+                              BasicLock::displaced_header_offset_in_bytes()));\n@@ -870,2 +890,2 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+      \/\/ Test for recursion\n+      cbz(header_reg, count);\n@@ -873,0 +893,3 @@\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":84,"deletions":61,"binary":false,"changes":145,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4317,0 +4318,24 @@\n+\/\/ Loads the obj's Klass* into dst.\n+\/\/ Preserves all registers (incl src, rscratch1 and rscratch2).\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expects UseCompressedClassPointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n+    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    return;\n+  }\n+\n+  Label fast;\n+\n+  \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+  ldr(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  tbz(dst, exact_log2(markWord::monitor_value), fast);\n+\n+  \/\/ Fetch displaced header\n+  ldr(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  \/\/ Fast-path: shift and decode Klass*.\n+  bind(fast);\n+  lsr(dst, dst, markWord::klass_shift);\n+}\n+\n@@ -4319,1 +4344,5 @@\n-    ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(dst, src);\n+    } else {\n+      ldrw(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4357,0 +4386,1 @@\n+  assert_different_registers(oop, trial_klass, tmp);\n@@ -4358,1 +4388,5 @@\n-    ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    if (UseCompactObjectHeaders) {\n+      load_nklass(tmp, oop);\n+    } else {\n+      ldrw(tmp, Address(oop, oopDesc::klass_offset_in_bytes()));\n+    }\n@@ -4527,0 +4561,14 @@\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeMovk: return \"movk\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n@@ -4528,2 +4576,4 @@\n-  assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n-  assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n@@ -4531,2 +4581,5 @@\n-  if (_klass_decode_mode != KlassDecodeNone) {\n-    return _klass_decode_mode;\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n@@ -4534,0 +4587,2 @@\n+  return _klass_decode_mode;\n+}\n@@ -4535,2 +4590,4 @@\n-  assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift()\n-         || 0 == CompressedKlassPointers::shift(), \"decode alg wrong\");\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4538,2 +4595,4 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n-    return (_klass_decode_mode = KlassDecodeZero);\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n@@ -4542,7 +4601,3 @@\n-  if (operand_valid_for_logical_immediate(\n-        \/*is32*\/false, (uint64_t)CompressedKlassPointers::base())) {\n-    const uint64_t range_mask =\n-      (1ULL << log2i(CompressedKlassPointers::range())) - 1;\n-    if (((uint64_t)CompressedKlassPointers::base() & range_mask) == 0) {\n-      return (_klass_decode_mode = KlassDecodeXor);\n-    }\n+  if (operand_valid_for_logical_immediate(false, base_u64) &&\n+      ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0)) {\n+    return KlassDecodeXor;\n@@ -4551,4 +4606,4 @@\n-  const uint64_t shifted_base =\n-    (uint64_t)CompressedKlassPointers::base() >> CompressedKlassPointers::shift();\n-  guarantee((shifted_base & 0xffff0000ffffffff) == 0,\n-            \"compressed class base bad alignment\");\n+  const uint64_t shifted_base = base_u64 >> CompressedKlassPointers::shift();\n+  if ((shifted_base & 0xffff0000ffffffff) == 0) {\n+    return KlassDecodeMovk;\n+  }\n@@ -4556,1 +4611,1 @@\n-  return (_klass_decode_mode = KlassDecodeMovk);\n+  return KlassDecodeNone;\n@@ -4560,0 +4615,2 @@\n+  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n@@ -4562,5 +4619,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsr(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    lsr(dst, src, LogKlassAlignmentInBytes);\n@@ -4570,6 +4623,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-      lsr(dst, dst, LogKlassAlignmentInBytes);\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n+    lsr(dst, dst, LogKlassAlignmentInBytes);\n@@ -4579,5 +4628,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      ubfx(dst, src, LogKlassAlignmentInBytes, 32);\n-    } else {\n-      movw(dst, src);\n-    }\n+    ubfx(dst, src, LogKlassAlignmentInBytes, MaxNarrowKlassPointerBits);\n@@ -4599,0 +4644,2 @@\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n@@ -4601,5 +4648,1 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-    } else {\n-      if (dst != src) mov(dst, src);\n-    }\n+    if (dst != src) mov(dst, src);\n@@ -4609,6 +4652,2 @@\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, src, LogKlassAlignmentInBytes);\n-      eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n-    } else {\n-      eor(dst, src, (uint64_t)CompressedKlassPointers::base());\n-    }\n+    lsl(dst, src, LogKlassAlignmentInBytes);\n+    eor(dst, dst, (uint64_t)CompressedKlassPointers::base());\n@@ -4621,0 +4660,3 @@\n+    \/\/ Invalid base should have been gracefully handled via klass_decode_mode() in VM initialization.\n+    assert((shifted_base & 0xffff0000ffffffff) == 0, \"incompatible base\");\n+\n@@ -4623,5 +4665,1 @@\n-\n-    if (CompressedKlassPointers::shift() != 0) {\n-      lsl(dst, dst, LogKlassAlignmentInBytes);\n-    }\n-\n+    lsl(dst, dst, LogKlassAlignmentInBytes);\n@@ -6209,0 +6247,55 @@\n+\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1, t2, t3: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  if (rt_check_stack) {\n+    \/\/ Check if we would have space on lock-stack for the object.\n+    ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+    ldr(t2, Address(rthread, JavaThread::lock_stack_limit_offset()));\n+    cmp(t1, t2);\n+    br(Assembler::GE, slow);\n+  }\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  orr(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into t2\n+  eor(t2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  cmpxchg(\/*addr*\/ obj, \/*expected*\/ hdr, \/*new*\/ t2, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t1);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  str(obj, Address(t1, 0));\n+  add(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, t1, t2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  andr(hdr, hdr, ~markWord::lock_mask_in_place);\n+\n+  \/\/ Load the new header (unlocked) into t1\n+  orr(t1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  cmpxchg(obj, hdr, t1, Assembler::xword,\n+          \/*acquire*\/ true, \/*release*\/ true, \/*weak*\/ false, t2);\n+  br(Assembler::NE, slow);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ldr(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+  sub(t1, t1, oopSize);\n+  str(t1, Address(rthread, JavaThread::lock_stack_current_offset()));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":147,"deletions":54,"binary":false,"changes":201,"status":"modified"},{"patch":"@@ -89,0 +89,2 @@\n+ public:\n+\n@@ -96,1 +98,9 @@\n-  KlassDecodeMode klass_decode_mode();\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n@@ -852,0 +862,1 @@\n+  void load_nklass(Register dst, Register src);\n@@ -1583,0 +1594,3 @@\n+  void fast_lock(Register obj, Register hdr, Register t1, Register t2, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock(Register obj, Register hdr, Register t1, Register t2, Label& slow);\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":15,"deletions":1,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1782,28 +1782,33 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg %r0\n-      __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ orr(swap_reg, rscratch1, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest iff dest == r0 else r0 <- dest\n-      __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n-\n-      \/\/ Hmm should this move to the slow path code area???\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n-\n-      __ sub(swap_reg, sp, swap_reg);\n-      __ neg(swap_reg, swap_reg);\n-      __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ str(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ br(Assembler::NE, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ldr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg %r0\n+        __ ldr(rscratch1, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ orr(swap_reg, rscratch1, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest iff dest == r0 else r0 <- dest\n+        __ cmpxchg_obj_header(r0, lock_reg, obj_reg, rscratch1, count, \/*fallthrough*\/NULL);\n+\n+        \/\/ Hmm should this move to the slow path code area???\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %r0 as the result of cmpxchg\n+\n+        __ sub(swap_reg, sp, swap_reg);\n+        __ neg(swap_reg, swap_reg);\n+        __ ands(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ str(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ br(Assembler::NE, slow_path_lock);\n+      }\n@@ -1920,1 +1925,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1936,9 +1941,14 @@\n-      \/\/ get address of the stack lock\n-      __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ldr(old_hdr, Address(r0, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ldr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, rscratch1, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(r0, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ldr(old_hdr, Address(r0, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(r0, old_hdr, obj_reg, rscratch1, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":48,"deletions":38,"binary":false,"changes":86,"status":"modified"},{"patch":"@@ -5477,0 +5477,23 @@\n+  address generate_check_lock_stack() {\n+    __ align(CodeEntryAlignment);\n+    StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+\n+    address start = __ pc();\n+\n+    __ set_last_Java_frame(sp, rfp, lr, rscratch1);\n+    __ enter();\n+    __ push_call_clobbered_registers();\n+\n+    __ mov(c_rarg0, r9);\n+    __ call_VM_leaf(CAST_FROM_FN_PTR(address, LockStack::ensure_lock_stack_size), 1);\n+\n+\n+    __ pop_call_clobbered_registers();\n+    __ leave();\n+    __ reset_last_Java_frame(true);\n+\n+    __ ret(lr);\n+\n+    return start;\n+  }\n+\n@@ -8054,0 +8077,4 @@\n+\n+    if (UseFastLocking) {\n+      StubRoutines::aarch64::_check_lock_stack = generate_check_lock_stack();\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":27,"deletions":0,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -60,0 +60,1 @@\n+address StubRoutines::aarch64::_check_lock_stack = NULL;\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -76,0 +76,2 @@\n+  static address _check_lock_stack;\n+\n@@ -186,0 +188,4 @@\n+  static address check_lock_stack() {\n+    return _check_lock_stack;\n+  }\n+\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":6,"deletions":0,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -3571,1 +3571,1 @@\n-    __ sub(r3, r3, sizeof(oopDesc));\n+    __ sub(r3, r3, oopDesc::base_offset_in_bytes());\n@@ -3576,1 +3576,6 @@\n-      __ add(r2, r0, sizeof(oopDesc));\n+      __ add(r2, r0, oopDesc::base_offset_in_bytes());\n+      if (!is_aligned(oopDesc::base_offset_in_bytes(), BytesPerLong)) {\n+        __ strw(zr, Address(__ post(r2, BytesPerInt)));\n+        __ sub(r3, r3, BytesPerInt);\n+        __ cbz(r3, initialize_header);\n+      }\n@@ -3586,5 +3591,8 @@\n-    __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n-    __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n-    __ store_klass_gap(r0, zr);  \/\/ zero klass gap for compressed oops\n-    __ store_klass(r0, r4);      \/\/ store klass last\n-\n+    if (UseCompactObjectHeaders) {\n+      __ ldr(rscratch1, Address(r4, Klass::prototype_header_offset()));\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+    } else {\n+      __ mov(rscratch1, (intptr_t)markWord::prototype().value());\n+      __ str(rscratch1, Address(r0, oopDesc::mark_offset_in_bytes()));\n+      __ store_klass(r0, r4);      \/\/ store klass last\n+    }\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":15,"deletions":7,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -301,0 +301,4 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -218,1 +218,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -75,33 +75,38 @@\n-  \/\/ and mark it as unlocked\n-  ori(hdr, hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  la(t1, Address(obj, hdr_offset));\n-  cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/nullptr);\n-  \/\/ if the object header was the same, we're done\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) sp <= hdr\n-  \/\/ 3) hdr <= sp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr -sp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  sub(hdr, hdr, sp);\n-  mv(t0, aligned_mask - (int)os::vm_page_size());\n-  andr(hdr, hdr, t0);\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (null in the displaced hdr location indicates recursive locking)\n-  sd(hdr, Address(disp_hdr, 0));\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  bnez(hdr, slow_case, \/* is_far *\/ true);\n-  \/\/ done\n-  bind(done);\n+\n+  if (UseFastLocking) {\n+    fast_lock(obj, hdr, t0, t1, slow_case);\n+  } else {\n+    \/\/ and mark it as unlocked\n+    jori(hdr, hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    la(t1, Address(obj, hdr_offset));\n+    cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/nullptr);\n+    \/\/ if the object header was the same, we're done\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) sp <= hdr\n+    \/\/ 3) hdr <= sp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr -sp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    sub(hdr, hdr, sp);\n+    mv(t0, aligned_mask - (int)os::vm_page_size());\n+    andr(hdr, hdr, t0);\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    sd(hdr, Address(disp_hdr, 0));\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    bnez(hdr, slow_case, \/* is_far *\/ true);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -118,16 +123,6 @@\n-  \/\/ load displaced header\n-  ld(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is null we had recursive locking\n-  \/\/ if we had recursive locking, we are done\n-  beqz(hdr, done);\n-  \/\/ load object\n-  ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n-  verify_oop(obj);\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  if (hdr_offset) {\n-    la(t0, Address(obj, hdr_offset));\n-    cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+  if (UseFastLocking) {\n+    \/\/ load object\n+    ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    verify_oop(obj);\n+    ld(hdr, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    fast_unlock(obj, hdr, t0, t1, slow_case);\n@@ -135,1 +130,21 @@\n-    cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    \/\/ load displaced header\n+    ld(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    \/\/ if we had recursive locking, we are done\n+    beqz(hdr, done);\n+    \/\/ load object\n+    ld(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+    verify_oop(obj);\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    if (hdr_offset) {\n+      la(t0, Address(obj, hdr_offset));\n+      cmpxchgptr(disp_hdr, hdr, t0, t1, done, &slow_case);\n+    } else {\n+      cmpxchgptr(disp_hdr, hdr, obj, t1, done, &slow_case);\n+    }\n+    \/\/ done\n+    bind(done);\n@@ -137,2 +152,0 @@\n-  \/\/ done\n-  bind(done);\n@@ -308,1 +321,1 @@\n-void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int framesize, int bang_size_in_bytes, int max_monitors) {\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":66,"deletions":53,"binary":false,"changes":119,"status":"modified"},{"patch":"@@ -812,28 +812,34 @@\n-    \/\/ Load (object->mark() | 1) into swap_reg\n-    ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    ori(swap_reg, t0, 1);\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-\n-    assert(lock_offset == 0,\n-           \"displached header must be first word in BasicObjectLock\");\n-\n-    cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n-\n-    \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-    \/\/  1) (mark & 7) == 0, and\n-    \/\/  2) sp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant 3 bits clear.\n-    \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n-    sub(swap_reg, swap_reg, sp);\n-    mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n-    andr(swap_reg, swap_reg, t0);\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    sd(swap_reg, Address(lock_reg, mark_offset));\n-    beqz(swap_reg, count);\n+    if (UseFastLocking) {\n+      ld(tmp, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock(obj_reg, tmp, t0, t1, slow_case);\n+      j(count);\n+    } else {\n+      \/\/ Load (object->mark() | 1) into swap_reg\n+      ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      ori(swap_reg, t0, 1);\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+\n+      assert(lock_offset == 0,\n+             \"displached header must be first word in BasicObjectLock\");\n+\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+\n+      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+      \/\/  1) (mark & 7) == 0, and\n+      \/\/  2) sp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - sp) & (7 - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant 3 bits clear.\n+      \/\/ NOTE: the oopMark is in swap_reg x10 as the result of cmpxchg\n+      sub(swap_reg, swap_reg, sp);\n+      mv(t0, (int64_t)(7 - (int)os::vm_page_size()));\n+      andr(swap_reg, swap_reg, t0);\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      sd(swap_reg, Address(lock_reg, mark_offset));\n+      beqz(swap_reg, count);\n+    }\n@@ -846,1 +852,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -884,3 +890,4 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into x10\n-    la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (UseFastLocking) {\n+      Label slow_case;\n+      \/\/ Load oop into obj_reg(c_rarg3)\n+      ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -888,2 +895,2 @@\n-    \/\/ Load oop into obj_reg(c_rarg3)\n-    ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Free entry\n+      sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -891,2 +898,6 @@\n-    \/\/ Free entry\n-    sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+      \/\/ Check for non-symmetric locking. This is allowed by the spec and the interpreter\n+      \/\/ must handle it.\n+      Register tmp = header_reg;\n+      ld(tmp, Address(xthread, JavaThread::lock_stack_current_offset()));\n+      ld(tmp, Address(tmp, -oopSize));\n+      bne(tmp, obj_reg, slow_case);\n@@ -894,3 +905,9 @@\n-    \/\/ Load the old header from BasicLock structure\n-    ld(header_reg, Address(swap_reg,\n-                           BasicLock::displaced_header_offset_in_bytes()));\n+      ld(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_unlock(obj_reg, header_reg, swap_reg, t0, slow_case);\n+      j(count);\n+\n+      bind(slow_case);\n+    } else {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into x10\n+      la(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n@@ -898,2 +915,2 @@\n-    \/\/ Test for recursion\n-    beqz(header_reg, count);\n+      \/\/ Load oop into obj_reg(c_rarg3)\n+      ld(obj_reg, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n@@ -901,2 +918,13 @@\n-    \/\/ Atomic swap back the old header\n-    cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+      \/\/ Free entry\n+      sd(zr, Address(lock_reg, BasicObjectLock::obj_offset_in_bytes()));\n+\n+      \/\/ Load the old header from BasicLock structure\n+      ld(header_reg, Address(swap_reg,\n+                             BasicLock::displaced_header_offset_in_bytes()));\n+\n+      \/\/ Test for recursion\n+      beqz(header_reg, count);\n+\n+      \/\/ Atomic swap back the old header\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n+    }\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":71,"deletions":43,"binary":false,"changes":114,"status":"modified"},{"patch":"@@ -4482,0 +4482,55 @@\n+\/\/ Attempt to fast-lock an object. Fall-through on success, branch to slow label\n+\/\/ on failure.\n+\/\/ Registers:\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - tmp1, tmp2: temporary registers, will be destroyed\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  ld(tmp2, Address(xthread, JavaThread::lock_stack_limit_offset()));\n+  bge(tmp1, tmp2, slow, true);\n+\n+  \/\/ Load (object->mark() | 1) into hdr\n+  ori(hdr, hdr, markWord::unlocked_value);\n+  \/\/ Clear lock-bits, into tmp2\n+  xori(tmp2, hdr, markWord::unlocked_value);\n+  \/\/ Try to swing header from unlocked to locked\n+  Label success;\n+  cmpxchgptr(hdr, tmp2, obj, tmp1, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  \/\/ TODO: Can we avoid re-loading the current offset? The CAS above clobbers it.\n+  \/\/ Maybe we could ensure that we have enough space on the lock stack more cleverly.\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  sd(obj, Address(tmp1, 0));\n+  add(tmp1, tmp1, oopSize);\n+  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow) {\n+  assert(UseFastLocking, \"only used with fast-locking\");\n+  assert_different_registers(obj, hdr, tmp1, tmp2);\n+\n+  \/\/ Load the expected old header (lock-bits cleared to indicate 'locked') into hdr\n+  mv(tmp1, ~markWord::lock_mask_in_place);\n+  andr(hdr, hdr, tmp1);\n+\n+  \/\/ Load the new header (unlocked) into tmp1\n+  ori(tmp1, hdr, markWord::unlocked_value);\n+\n+  \/\/ Try to swing header from locked to unlocked\n+  Label success;\n+  cmpxchgptr(hdr, tmp1, obj, tmp2, success, &slow);\n+  bind(success);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  ld(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+  sub(tmp1, tmp1, oopSize);\n+  sd(tmp1, Address(xthread, JavaThread::lock_stack_current_offset()));\n+}\n+\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":55,"deletions":0,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -1421,0 +1421,4 @@\n+\n+public:\n+  void fast_lock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n+  void fast_unlock(Register obj, Register hdr, Register tmp1, Register tmp2, Label& slow);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2513,30 +2513,40 @@\n-      \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n-      __ ori(tmp, disp_hdr, markWord::unlocked_value);\n-\n-      \/\/ Initialize the box. (Must happen before we update the object mark!)\n-      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-\n-      \/\/ Compare object markWord with an unlocked value (tmp) and if\n-      \/\/ equal exchange the stack address of our box with object markWord.\n-      \/\/ On failure disp_hdr contains the possibly locked markWord.\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n-                 Assembler::rl, \/*result*\/disp_hdr);\n-      __ mv(flag, zr);\n-      __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n-\n-      assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n-\n-      \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n-      \/\/ object, will have now locked it will continue at label cont\n-      \/\/ We did not see an unlocked object so try the fast recursive case.\n-\n-      \/\/ Check if the owner is self by comparing the value in the\n-      \/\/ markWord of object (disp_hdr) with the stack pointer.\n-      __ sub(disp_hdr, disp_hdr, sp);\n-      __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n-      \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n-      \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n-      \/\/ recursive lock.\n-      __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n-      __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-      __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_lock(oop, disp_hdr, tmp, t0, slow);\n+        \/\/ Indicate success at cont.\n+        __ mv(flag, zr);\n+        __ j(cont);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow-path\n+      } else {\n+        \/\/ Set tmp to be (markWord of object | UNLOCK_VALUE).\n+        __ ori(tmp, disp_hdr, markWord::unlocked_value);\n+\n+        \/\/ Initialize the box. (Must happen before we update the object mark!)\n+        __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+\n+        \/\/ Compare object markWord with an unlocked value (tmp) and if\n+        \/\/ equal exchange the stack address of our box with object markWord.\n+        \/\/ On failure disp_hdr contains the possibly locked markWord.\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/tmp, \/*new value*\/box, Assembler::int64, Assembler::aq,\n+                   Assembler::rl, \/*result*\/disp_hdr);\n+        __ mv(flag, zr);\n+        __ beq(disp_hdr, tmp, cont); \/\/ prepare zero flag and goto cont if we won the cas\n+\n+        assert(oopDesc::mark_offset_in_bytes() == 0, \"offset of _mark is not 0\");\n+\n+        \/\/ If the compare-and-exchange succeeded, then we found an unlocked\n+        \/\/ object, will have now locked it will continue at label cont\n+        \/\/ We did not see an unlocked object so try the fast recursive case.\n+\n+        \/\/ Check if the owner is self by comparing the value in the\n+        \/\/ markWord of object (disp_hdr) with the stack pointer.\n+        __ sub(disp_hdr, disp_hdr, sp);\n+        __ mv(tmp, (intptr_t) (~(os::vm_page_size()-1) | (uintptr_t)markWord::lock_mask_in_place));\n+        \/\/ If (mark & lock_mask) == 0 and mark - sp < page_size, we are stack-locking and goto cont,\n+        \/\/ hence we can store 0 as the displaced header in the box, which indicates that it is a\n+        \/\/ recursive lock.\n+        __ andr(tmp\/*==0?*\/, disp_hdr, tmp);\n+        __ sd(tmp\/*==0, perhaps*\/, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+        __ mv(flag, tmp); \/\/ we can use the value of tmp as the result here\n+      }\n@@ -2559,6 +2569,8 @@\n-    \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n-    \/\/ lock. The fast-path monitor unlock code checks for\n-    \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n-    \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n-    __ mv(tmp, (address)markWord::unused_mark().value());\n-    __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Store a non-null value into the box to avoid looking like a re-entrant\n+      \/\/ lock. The fast-path monitor unlock code checks for\n+      \/\/ markWord::monitor_value so use markWord::unused_mark which has the\n+      \/\/ relevant bit set, and also matches ObjectSynchronizer::slow_enter.\n+      __ mv(tmp, (address)markWord::unused_mark().value());\n+      __ sd(tmp, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    }\n@@ -2597,1 +2609,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2612,3 +2624,13 @@\n-      \/\/ Check if it is still a light weight lock, this is true if we\n-      \/\/ see the stack address of the basicLock in the markWord of the\n-      \/\/ object.\n+      if (UseFastLocking) {\n+        Label slow;\n+        __ fast_unlock(oop, tmp, box, disp_hdr, slow);\n+\n+        \/\/ Indicate success at cont.\n+        __ mv(flag, zr);\n+        __ j(cont);\n+        __ bind(slow);\n+        __ mv(flag, 1); \/\/ Set non-zero flag to indicate 'failure' -> take slow path\n+      } else {\n+        \/\/ Check if it is still a light weight lock, this is true if we\n+        \/\/ see the stack address of the basicLock in the markWord of the\n+        \/\/ object.\n@@ -2616,3 +2638,4 @@\n-      __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n-                 Assembler::rl, \/*result*\/tmp);\n-      __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+        __ cmpxchg(\/*memory address*\/oop, \/*expected value*\/box, \/*new value*\/disp_hdr, Assembler::int64, Assembler::relaxed,\n+                   Assembler::rl, \/*result*\/tmp);\n+        __ xorr(flag, box, tmp); \/\/ box == tmp if cas succeeds\n+      }\n@@ -2630,0 +2653,11 @@\n+\n+    if (UseFastLocking) {\n+      Label L;\n+      __ ld(disp_hdr, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ mv(t0, (unsigned char)(intptr_t)ANONYMOUS_OWNER);\n+      __ bne(disp_hdr, t0, L);\n+      __ mv(flag, 1); \/\/ Indicate failure at cont -- dive into slow-path.\n+      __ j(cont);\n+      __ bind(L);\n+    }\n+\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":77,"deletions":43,"binary":false,"changes":120,"status":"modified"},{"patch":"@@ -1675,25 +1675,30 @@\n-      \/\/ Load (object->mark() | 1) into swap_reg % x10\n-      __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ ori(swap_reg, t0, 1);\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-\n-      \/\/ src -> dest if dest == x10 else x10 <- dest\n-      __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) sp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n-\n-      __ sub(swap_reg, swap_reg, sp);\n-      __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n-      __ bnez(swap_reg, slow_path_lock);\n+      if (UseFastLocking) {\n+        __ ld(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock(obj_reg, swap_reg, tmp, t0, slow_path_lock);\n+      } else {\n+        \/\/ Load (object->mark() | 1) into swap_reg % x10\n+        __ ld(t0, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ ori(swap_reg, t0, 1);\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+\n+        \/\/ src -> dest if dest == x10 else x10 <- dest\n+        __ cmpxchg_obj_header(x10, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) sp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - sp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg % 10 as the result of cmpxchg\n+\n+        __ sub(swap_reg, swap_reg, sp);\n+        __ andi(swap_reg, swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ sd(swap_reg, Address(lock_reg, mark_word_offset));\n+        __ bnez(swap_reg, slow_path_lock);\n+      }\n@@ -1796,1 +1801,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1812,9 +1817,14 @@\n-      \/\/ get address of the stack lock\n-      __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ ld(old_hdr, Address(x10, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      Label count;\n-      __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n-      __ bind(count);\n+      if (UseFastLocking) {\n+        __ ld(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_unlock(obj_reg, old_hdr, swap_reg, t0, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ la(x10, Address(sp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ ld(old_hdr, Address(x10, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        Label count;\n+        __ cmpxchg_obj_header(x10, old_hdr, obj_reg, t0, count, &slow_path_unlock);\n+        __ bind(count);\n+      }\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":45,"deletions":35,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -256,0 +256,5 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  \/\/ Currently not needed.\n+  Unimplemented();\n+}\n+\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -115,1 +115,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n","filename":"src\/hotspot\/cpu\/s390\/c1_MacroAssembler_s390.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -38,0 +38,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -281,0 +282,11 @@\n+void LoadKlassStub::emit_code(LIR_Assembler* ce) {\n+  assert(UseCompactObjectHeaders, \"only with compact headers\");\n+  __ bind(_entry);\n+#ifdef _LP64\n+  Register d = _result->as_register();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(_continuation);\n+#else\n+  __ should_not_reach_here();\n+#endif\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -288,1 +288,1 @@\n-  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  __ build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n@@ -1638,1 +1638,1 @@\n-                      arrayOopDesc::header_size(op->type()),\n+                      arrayOopDesc::base_offset_in_bytes(op->type()),\n@@ -3073,0 +3073,1 @@\n+  Register tmp2 = UseCompactObjectHeaders ? rscratch2 : noreg;\n@@ -3264,7 +3265,1 @@\n-      if (UseCompressedClassPointers) {\n-        __ movl(tmp, src_klass_addr);\n-        __ cmpl(tmp, dst_klass_addr);\n-      } else {\n-        __ movptr(tmp, src_klass_addr);\n-        __ cmpptr(tmp, dst_klass_addr);\n-      }\n+      __ cmp_klass(src, dst, tmp, tmp2);\n@@ -3430,4 +3425,1 @@\n-\n-\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3436,2 +3428,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, src_klass_addr);\n-      else                   __ cmpptr(tmp, src_klass_addr);\n+      __ cmp_klass(tmp, src, tmp2);\n@@ -3440,2 +3431,1 @@\n-      if (UseCompressedClassPointers)          __ cmpl(tmp, dst_klass_addr);\n-      else                   __ cmpptr(tmp, dst_klass_addr);\n+      __ cmp_klass(tmp, dst, tmp2);\n@@ -3511,0 +3501,1 @@\n+    Register tmp = UseFastLocking ? op->scratch_opr()->as_register() : noreg;\n@@ -3512,1 +3503,1 @@\n-    int null_check_offset = __ lock_object(hdr, obj, lock, *op->stub()->entry());\n+    int null_check_offset = __ lock_object(hdr, obj, lock, tmp, *op->stub()->entry());\n@@ -3536,0 +3527,14 @@\n+  if (UseCompactObjectHeaders) {\n+    Register tmp = rscratch1;\n+    assert_different_registers(tmp, obj);\n+    assert_different_registers(tmp, result);\n+\n+    \/\/ Check if we can take the (common) fast path, if obj is unlocked.\n+    __ movq(result, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(result, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, *op->stub()->entry());\n+    __ bind(*op->stub()->continuation());\n+    \/\/ Fast-path: shift and decode Klass*.\n+    __ shrq(result, markWord::klass_shift);\n+    __ decode_klass_not_null(result, tmp);\n+  } else\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":23,"deletions":18,"binary":false,"changes":41,"status":"modified"},{"patch":"@@ -322,1 +322,2 @@\n-  monitor_enter(obj.result(), lock, syncTempOpr(), LIR_OprFact::illegalOpr,\n+  LIR_Opr tmp = UseFastLocking ? new_register(T_INT) : LIR_OprFact::illegalOpr;\n+  monitor_enter(obj.result(), lock, syncTempOpr(), tmp,\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -41,1 +41,1 @@\n-int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Label& slow_case) {\n+int C1_MacroAssembler::lock_object(Register hdr, Register obj, Register disp_hdr, Register tmp, Label& slow_case) {\n@@ -45,2 +45,1 @@\n-  assert(hdr != obj && hdr != disp_hdr && obj != disp_hdr, \"registers must be different\");\n-  Label done;\n+  assert_different_registers(hdr, obj, disp_hdr, tmp);\n@@ -65,33 +64,44 @@\n-  \/\/ and mark it as unlocked\n-  orptr(hdr, markWord::unlocked_value);\n-  \/\/ save unlocked object header into the displaced header location on the stack\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n-  \/\/ displaced header address in the object header - if it is not the same, get the\n-  \/\/ object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was the same, we're done\n-  jcc(Assembler::equal, done);\n-  \/\/ if the object header was not the same, it is now in the hdr register\n-  \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n-  \/\/\n-  \/\/ 1) (hdr & aligned_mask) == 0\n-  \/\/ 2) rsp <= hdr\n-  \/\/ 3) hdr <= rsp + page_size\n-  \/\/\n-  \/\/ these 3 tests can be done by evaluating the following expression:\n-  \/\/\n-  \/\/ (hdr - rsp) & (aligned_mask - page_size)\n-  \/\/\n-  \/\/ assuming both the stack pointer and page_size have their least\n-  \/\/ significant 2 bits cleared and page_size is a power of 2\n-  subptr(hdr, rsp);\n-  andptr(hdr, aligned_mask - (int)os::vm_page_size());\n-  \/\/ for recursive locking, the result is zero => save it in the displaced header\n-  \/\/ location (null in the displaced hdr location indicates recursive locking)\n-  movptr(Address(disp_hdr, 0), hdr);\n-  \/\/ otherwise we don't care about the result and handle locking via runtime call\n-  jcc(Assembler::notZero, slow_case);\n-  \/\/ done\n-  bind(done);\n+\n+  if (UseFastLocking) {\n+#ifdef _LP64\n+    const Register thread = r15_thread;\n+#else\n+    const Register thread = disp_hdr;\n+    get_thread(thread);\n+#endif\n+    fast_lock_impl(obj, hdr, thread, tmp, slow_case, LP64_ONLY(false) NOT_LP64(true));\n+  } else {\n+    Label done;\n+    orptr(hdr, markWord::unlocked_value);\n+    \/\/ save unlocked object header into the displaced header location on the stack\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ test if object header is still the same (i.e. unlocked), and if so, store the\n+    \/\/ displaced header address in the object header - if it is not the same, get the\n+    \/\/ object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(disp_hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was the same, we're done\n+    jcc(Assembler::equal, done);\n+    \/\/ if the object header was not the same, it is now in the hdr register\n+    \/\/ => test if it is a stack pointer into the same stack (recursive locking), i.e.:\n+    \/\/\n+    \/\/ 1) (hdr & aligned_mask) == 0\n+    \/\/ 2) rsp <= hdr\n+    \/\/ 3) hdr <= rsp + page_size\n+    \/\/\n+    \/\/ these 3 tests can be done by evaluating the following expression:\n+    \/\/\n+    \/\/ (hdr - rsp) & (aligned_mask - page_size)\n+    \/\/\n+    \/\/ assuming both the stack pointer and page_size have their least\n+    \/\/ significant 2 bits cleared and page_size is a power of 2\n+    subptr(hdr, rsp);\n+    andptr(hdr, aligned_mask - (int)os::vm_page_size());\n+    \/\/ for recursive locking, the result is zero => save it in the displaced header\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n+    movptr(Address(disp_hdr, 0), hdr);\n+    \/\/ otherwise we don't care about the result and handle locking via runtime call\n+    jcc(Assembler::notZero, slow_case);\n+    \/\/ done\n+    bind(done);\n+  }\n@@ -111,6 +121,9 @@\n-  \/\/ load displaced header\n-  movptr(hdr, Address(disp_hdr, 0));\n-  \/\/ if the loaded hdr is null we had recursive locking\n-  testptr(hdr, hdr);\n-  \/\/ if we had recursive locking, we are done\n-  jcc(Assembler::zero, done);\n+  if (!UseFastLocking) {\n+    \/\/ load displaced header\n+    movptr(hdr, Address(disp_hdr, 0));\n+    \/\/ if the loaded hdr is null we had recursive locking\n+    testptr(hdr, hdr);\n+    \/\/ if we had recursive locking, we are done\n+    jcc(Assembler::zero, done);\n+  }\n+\n@@ -119,11 +132,16 @@\n-\n-  \/\/ test if object header is pointing to the displaced header, and if so, restore\n-  \/\/ the displaced header in the object - if the object header is not pointing to\n-  \/\/ the displaced header, get the object header instead\n-  MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n-  cmpxchgptr(hdr, Address(obj, hdr_offset));\n-  \/\/ if the object header was not pointing to the displaced header,\n-  \/\/ we do unlocking via runtime call\n-  jcc(Assembler::notEqual, slow_case);\n-  \/\/ done\n-  bind(done);\n+  if (UseFastLocking) {\n+    movptr(disp_hdr, Address(obj, hdr_offset));\n+    andptr(disp_hdr, ~(int32_t)markWord::lock_mask_in_place);\n+    fast_unlock_impl(obj, disp_hdr, hdr, slow_case);\n+  } else {\n+    \/\/ test if object header is pointing to the displaced header, and if so, restore\n+    \/\/ the displaced header in the object - if the object header is not pointing to\n+    \/\/ the displaced header, get the object header instead\n+    MacroAssembler::lock(); \/\/ must be immediately before cmpxchg!\n+    cmpxchgptr(hdr, Address(obj, hdr_offset));\n+    \/\/ if the object header was not pointing to the displaced header,\n+    \/\/ we do unlocking via runtime call\n+    jcc(Assembler::notEqual, slow_case);\n+    \/\/ done\n+  }\n+  bind(done);\n@@ -147,2 +165,6 @@\n-  assert_different_registers(obj, klass, len);\n-  movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n+  assert_different_registers(obj, klass, len, t1, t2);\n+  if (UseCompactObjectHeaders) {\n+    movptr(t1, Address(klass, Klass::prototype_header_offset()));\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), t1);\n+  } else {\n+    movptr(Address(obj, oopDesc::mark_offset_in_bytes()), checked_cast<int32_t>(markWord::prototype().value()));\n@@ -150,5 +172,5 @@\n-  if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n-    movptr(t1, klass);\n-    encode_klass_not_null(t1, rscratch1);\n-    movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n-  } else\n+    if (UseCompressedClassPointers) { \/\/ Take care not to kill klass\n+      movptr(t1, klass);\n+      encode_klass_not_null(t1, rscratch1);\n+      movl(Address(obj, oopDesc::klass_offset_in_bytes()), t1);\n+    } else\n@@ -156,2 +178,3 @@\n-  {\n-    movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    {\n+      movptr(Address(obj, oopDesc::klass_offset_in_bytes()), klass);\n+    }\n@@ -159,1 +182,0 @@\n-\n@@ -164,1 +186,1 @@\n-  else if (UseCompressedClassPointers) {\n+  else if (UseCompressedClassPointers && !UseCompactObjectHeaders) {\n@@ -206,0 +228,1 @@\n+    int hdr_size_aligned = align_up(hdr_size_in_bytes, BytesPerWord); \/\/ klass gap is already cleared by init_header().\n@@ -208,1 +231,1 @@\n-      initialize_body(obj, index, hdr_size_in_bytes, t1_zero);\n+      initialize_body(obj, index, hdr_size_aligned, t1_zero);\n@@ -213,1 +236,1 @@\n-      for (int i = hdr_size_in_bytes; i < con_size_in_bytes; i += BytesPerWord)\n+      for (int i = hdr_size_aligned; i < con_size_in_bytes; i += BytesPerWord)\n@@ -215,1 +238,1 @@\n-    } else if (con_size_in_bytes > hdr_size_in_bytes) {\n+    } else if (con_size_in_bytes > hdr_size_aligned) {\n@@ -220,1 +243,1 @@\n-      movptr(index, (con_size_in_bytes - hdr_size_in_bytes) >> 3);\n+      movptr(index, (con_size_in_bytes - hdr_size_aligned) >> 3);\n@@ -222,1 +245,1 @@\n-      if (((con_size_in_bytes - hdr_size_in_bytes) & 4) != 0)\n+      if (((con_size_in_bytes - hdr_size_aligned) & 4) != 0)\n@@ -227,1 +250,1 @@\n-        movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (1*BytesPerWord)),\n+        movptr(Address(obj, index, Address::times_8, hdr_size_aligned - (1*BytesPerWord)),\n@@ -229,1 +252,1 @@\n-        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_in_bytes - (2*BytesPerWord)),\n+        NOT_LP64(movptr(Address(obj, index, Address::times_8, hdr_size_aligned - (2*BytesPerWord)),\n@@ -245,1 +268,1 @@\n-void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int header_size, Address::ScaleFactor f, Register klass, Label& slow_case) {\n+void C1_MacroAssembler::allocate_array(Register obj, Register len, Register t1, Register t2, int base_offset_in_bytes, Address::ScaleFactor f, Register klass, Label& slow_case) {\n@@ -258,1 +281,1 @@\n-  movptr(arr_size, header_size * BytesPerWord + MinObjAlignmentInBytesMask);\n+  movptr(arr_size, (int32_t)base_offset_in_bytes + MinObjAlignmentInBytesMask);\n@@ -268,1 +291,1 @@\n-  initialize_body(obj, arr_size, header_size * BytesPerWord, len_zero);\n+  initialize_body(obj, arr_size, base_offset_in_bytes, len_zero);\n@@ -302,1 +325,1 @@\n-void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes) {\n+void C1_MacroAssembler::build_frame(int frame_size_in_bytes, int bang_size_in_bytes, int max_monitors) {\n@@ -323,0 +346,13 @@\n+#ifdef _LP64\n+  if (UseFastLocking && max_monitors > 0) {\n+    Label ok;\n+    movptr(rax, Address(r15_thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * wordSize);\n+    cmpptr(rax, Address(r15_thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    assert(StubRoutines::x86::check_lock_stack() != nullptr, \"need runtime call stub\");\n+    call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n+    bind(ok);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":111,"deletions":75,"binary":false,"changes":186,"status":"modified"},{"patch":"@@ -53,1 +53,1 @@\n-  int lock_object  (Register swap, Register obj, Register disp_hdr, Label& slow_case);\n+  int lock_object  (Register swap, Register obj, Register disp_hdr, Register tmp, Label& slow_case);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"runtime\/objectMonitor.hpp\"\n@@ -75,0 +76,36 @@\n+int C2CheckLockStackStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2CheckLockStackStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  assert(StubRoutines::x86::check_lock_stack() != NULL, \"need runtime call stub\");\n+  __ call(RuntimeAddress(StubRoutines::x86::check_lock_stack()));\n+  __ jmp(continuation(), false \/* maybe_short *\/);\n+}\n+\n+#ifdef _LP64\n+int C2HandleAnonOMOwnerStub::max_size() const {\n+  return 18;\n+}\n+\n+void C2HandleAnonOMOwnerStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register mon = monitor();\n+  __ movptr(Address(mon, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), r15_thread);\n+  __ subptr(Address(r15_thread, JavaThread::lock_stack_current_offset()), oopSize);\n+  __ jmp(continuation());\n+}\n+\n+int C2LoadNKlassStub::max_size() const {\n+  return 10;\n+}\n+\n+void C2LoadNKlassStub::emit(C2_MacroAssembler& masm) {\n+  __ bind(entry());\n+  Register d = dst();\n+  __ movq(d, Address(d, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+  __ jmp(continuation());\n+}\n+#endif\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":37,"deletions":0,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub) {\n+void C2_MacroAssembler::verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors) {\n@@ -130,0 +130,14 @@\n+#ifdef _LP64\n+  if (UseFastLocking && max_monitors > 0) {\n+    C2CheckLockStackStub* stub = new (Compile::current()->comp_arena()) C2CheckLockStackStub();\n+    Compile::current()->output()->add_stub(stub);\n+    assert(!is_stub, \"only methods have monitors\");\n+    Register thread = r15_thread;\n+    movptr(rax, Address(thread, JavaThread::lock_stack_current_offset()));\n+    addptr(rax, max_monitors * oopSize);\n+    cmpptr(rax, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, stub->entry());\n+    bind(stub->continuation());\n+  }\n+#endif\n+\n@@ -551,1 +565,1 @@\n-                                 Register scrReg, Register cx1Reg, Register cx2Reg,\n+                                 Register scrReg, Register cx1Reg, Register cx2Reg, Register thread,\n@@ -605,14 +619,32 @@\n-    \/\/ Attempt stack-locking ...\n-    orptr (tmpReg, markWord::unlocked_value);\n-    movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n-    lock();\n-    cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n-    jcc(Assembler::equal, COUNT);           \/\/ Success\n-\n-    \/\/ Recursive locking.\n-    \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n-    \/\/ Locked by current thread if difference with current SP is less than one page.\n-    subptr(tmpReg, rsp);\n-    \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n-    andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n-    movptr(Address(boxReg, 0), tmpReg);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, NO_COUNT, false);\n+      jmp(COUNT);\n+#else\n+      \/\/ We can not emit the lock-stack-check in verified_entry() because we don't have enough\n+      \/\/ registers (for thread ptr). Therefore we have to emit the lock-stack-check in\n+      \/\/ fast_lock_impl(). However, that check can take a slow-path with ZF=1, therefore\n+      \/\/ we need to handle it specially and force ZF=0 before taking the actual slow-path.\n+      Label slow;\n+      fast_lock_impl(objReg, tmpReg, thread, scrReg, slow);\n+      jmp(COUNT);\n+      bind(slow);\n+      testptr(objReg, objReg); \/\/ ZF=0 to indicate failure\n+      jmp(NO_COUNT);\n+#endif\n+    } else {\n+      \/\/ Attempt stack-locking ...\n+      orptr (tmpReg, markWord::unlocked_value);\n+      movptr(Address(boxReg, 0), tmpReg);          \/\/ Anticipate successful CAS\n+      lock();\n+      cmpxchgptr(boxReg, Address(objReg, oopDesc::mark_offset_in_bytes()));      \/\/ Updates tmpReg\n+      jcc(Assembler::equal, COUNT);           \/\/ Success\n+\n+      \/\/ Recursive locking.\n+      \/\/ The object is stack-locked: markword contains stack pointer to BasicLock.\n+      \/\/ Locked by current thread if difference with current SP is less than one page.\n+      subptr(tmpReg, rsp);\n+      \/\/ Next instruction set ZFlag == 1 (Success) if difference is less then one page.\n+      andptr(tmpReg, (int32_t) (NOT_LP64(0xFFFFF003) LP64_ONLY(7 - (int)os::vm_page_size())) );\n+      movptr(Address(boxReg, 0), tmpReg);\n+    }\n@@ -662,1 +694,1 @@\n-  cmpxchgptr(scrReg, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n+  cmpxchgptr(thread, Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)));\n@@ -664,7 +696,0 @@\n-  \/\/ If we weren't able to swing _owner from null to the BasicLock\n-  \/\/ then take the slow path.\n-  jccb  (Assembler::notZero, NO_COUNT);\n-  \/\/ update _owner from BasicLock to thread\n-  get_thread (scrReg);                    \/\/ beware: clobbers ICCs\n-  movptr(Address(boxReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), scrReg);\n-  xorptr(boxReg, boxReg);                 \/\/ set icc.ZFlag = 1 to indicate success\n@@ -776,1 +801,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -783,1 +808,20 @@\n-    jccb   (Assembler::zero, Stacked);\n+#if INCLUDE_RTM_OPT\n+    if (UseFastLocking && use_rtm) {\n+      jcc(Assembler::zero, Stacked);\n+    } else\n+#endif\n+    jccb(Assembler::zero, Stacked);\n+    if (UseFastLocking) {\n+      \/\/ If the owner is ANONYMOUS, we need to fix it.\n+      testb(Address(tmpReg, OM_OFFSET_NO_MONITOR_VALUE_TAG(owner)), (int32_t) (intptr_t) ANONYMOUS_OWNER);\n+#ifdef _LP64\n+      C2HandleAnonOMOwnerStub* stub = new (Compile::current()->comp_arena()) C2HandleAnonOMOwnerStub(tmpReg);\n+      Compile::current()->output()->add_stub(stub);\n+      jcc(Assembler::notEqual, stub->entry());\n+      bind(stub->continuation());\n+#else\n+      \/\/ We can't easily implement this optimization on 32 bit because we don't have a thread register.\n+      \/\/ Call the slow-path instead.\n+      jcc(Assembler::notEqual, NO_COUNT);\n+#endif\n+    }\n@@ -795,1 +839,1 @@\n-    jmpb(DONE_LABEL);\n+    jmp(DONE_LABEL);\n@@ -909,3 +953,9 @@\n-    movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n-    lock();\n-    cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    if (UseFastLocking) {\n+      mov(boxReg, tmpReg);\n+      fast_unlock_impl(objReg, boxReg, tmpReg, NO_COUNT);\n+      jmp(COUNT);\n+    } else {\n+      movptr(tmpReg, Address (boxReg, 0));      \/\/ re-fetch\n+      lock();\n+      cmpxchgptr(tmpReg, Address(objReg, oopDesc::mark_offset_in_bytes())); \/\/ Uses RAX which is box\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":80,"deletions":30,"binary":false,"changes":110,"status":"modified"},{"patch":"@@ -32,1 +32,1 @@\n-  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub);\n+  void verified_entry(int framesize, int stack_bang_size, bool fp_mode_24b, bool is_stub, int max_monitors);\n@@ -39,1 +39,1 @@\n-                 Register scr, Register cx1, Register cx2,\n+                 Register scr, Register cx1, Register cx2, Register thread,\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -1226,53 +1226,65 @@\n-    \/\/ Load immediate 1 into swap_reg %rax\n-    movl(swap_reg, 1);\n-\n-    \/\/ Load (object->mark() | 1) into swap_reg %rax\n-    orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-    \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-\n-    assert(lock_offset == 0,\n-           \"displaced header must be first word in BasicObjectLock\");\n-\n-    lock();\n-    cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-    jcc(Assembler::zero, count_locking);\n-\n-    const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n-\n-    \/\/ Fast check for recursive lock.\n-    \/\/\n-    \/\/ Can apply the optimization only if this is a stack lock\n-    \/\/ allocated in this thread. For efficiency, we can focus on\n-    \/\/ recently allocated stack locks (instead of reading the stack\n-    \/\/ base and checking whether 'mark' points inside the current\n-    \/\/ thread stack):\n-    \/\/  1) (mark & zero_bits) == 0, and\n-    \/\/  2) rsp <= mark < mark + os::pagesize()\n-    \/\/\n-    \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n-    \/\/ neither apply the optimization for an inflated lock allocated\n-    \/\/ just above the thread stack (this is why condition 1 matters)\n-    \/\/ nor apply the optimization if the stack lock is inside the stack\n-    \/\/ of another thread. The latter is avoided even in case of overflow\n-    \/\/ because we have guard pages at the end of all stacks. Hence, if\n-    \/\/ we go over the stack base and hit the stack of another thread,\n-    \/\/ this should not be in a writeable area that could contain a\n-    \/\/ stack lock allocated by that thread. As a consequence, a stack\n-    \/\/ lock less than page size away from rsp is guaranteed to be\n-    \/\/ owned by the current thread.\n-    \/\/\n-    \/\/ These 3 tests can be done by evaluating the following\n-    \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n-    \/\/ assuming both stack pointer and pagesize have their\n-    \/\/ least significant bits clear.\n-    \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n-    subptr(swap_reg, rsp);\n-    andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n-\n-    \/\/ Save the test result, for recursive case, the result is zero\n-    movptr(Address(lock_reg, mark_offset), swap_reg);\n-    jcc(Assembler::notZero, slow_case);\n-\n-    bind(count_locking);\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = lock_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Load object header, prepare for CAS from unlocked to locked.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      fast_lock_impl(obj_reg, swap_reg, thread, tmp_reg, slow_case);\n+    } else {\n+      \/\/ Load immediate 1 into swap_reg %rax\n+      movl(swap_reg, 1);\n+\n+      \/\/ Load (object->mark() | 1) into swap_reg %rax\n+      orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+\n+      assert(lock_offset == 0,\n+             \"displaced header must be first word in BasicObjectLock\");\n+\n+      lock();\n+      cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      jcc(Assembler::zero, count_locking);\n+\n+      const int zero_bits = LP64_ONLY(7) NOT_LP64(3);\n+\n+      \/\/ Fast check for recursive lock.\n+      \/\/\n+      \/\/ Can apply the optimization only if this is a stack lock\n+      \/\/ allocated in this thread. For efficiency, we can focus on\n+      \/\/ recently allocated stack locks (instead of reading the stack\n+      \/\/ base and checking whether 'mark' points inside the current\n+      \/\/ thread stack):\n+      \/\/  1) (mark & zero_bits) == 0, and\n+      \/\/  2) rsp <= mark < mark + os::pagesize()\n+      \/\/\n+      \/\/ Warning: rsp + os::pagesize can overflow the stack base. We must\n+      \/\/ neither apply the optimization for an inflated lock allocated\n+      \/\/ just above the thread stack (this is why condition 1 matters)\n+      \/\/ nor apply the optimization if the stack lock is inside the stack\n+      \/\/ of another thread. The latter is avoided even in case of overflow\n+      \/\/ because we have guard pages at the end of all stacks. Hence, if\n+      \/\/ we go over the stack base and hit the stack of another thread,\n+      \/\/ this should not be in a writeable area that could contain a\n+      \/\/ stack lock allocated by that thread. As a consequence, a stack\n+      \/\/ lock less than page size away from rsp is guaranteed to be\n+      \/\/ owned by the current thread.\n+      \/\/\n+      \/\/ These 3 tests can be done by evaluating the following\n+      \/\/ expression: ((mark - rsp) & (zero_bits - os::vm_page_size())),\n+      \/\/ assuming both stack pointer and pagesize have their\n+      \/\/ least significant bits clear.\n+      \/\/ NOTE: the mark is in swap_reg %rax as the result of cmpxchg\n+      subptr(swap_reg, rsp);\n+      andptr(swap_reg, zero_bits - (int)os::vm_page_size());\n+\n+      \/\/ Save the test result, for recursive case, the result is zero\n+      movptr(Address(lock_reg, mark_offset), swap_reg);\n+      jcc(Assembler::notZero, slow_case);\n+\n+      bind(count_locking);\n+    }\n@@ -1287,1 +1299,1 @@\n-            lock_reg);\n+            UseFastLocking ? obj_reg : lock_reg);\n@@ -1321,3 +1333,5 @@\n-    \/\/ Convert from BasicObjectLock structure to object and BasicLock\n-    \/\/ structure Store the BasicLock address into %rax\n-    lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    if (!UseFastLocking) {\n+      \/\/ Convert from BasicObjectLock structure to object and BasicLock\n+      \/\/ structure Store the BasicLock address into %rax\n+      lea(swap_reg, Address(lock_reg, BasicObjectLock::lock_offset_in_bytes()));\n+    }\n@@ -1331,3 +1345,20 @@\n-    \/\/ Load the old header from BasicLock structure\n-    movptr(header_reg, Address(swap_reg,\n-                               BasicLock::displaced_header_offset_in_bytes()));\n+    if (UseFastLocking) {\n+#ifdef _LP64\n+      const Register thread = r15_thread;\n+#else\n+      const Register thread = header_reg;\n+      get_thread(thread);\n+#endif\n+      \/\/ Handle unstructured locking.\n+      Register tmp = swap_reg;\n+      movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+      cmpptr(obj_reg, Address(tmp, -oopSize));\n+      jcc(Assembler::notEqual, slow_case);\n+      \/\/ Try to swing header from locked to unlock.\n+      movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+      fast_unlock_impl(obj_reg, swap_reg, header_reg, slow_case);\n+    } else {\n+      \/\/ Load the old header from BasicLock structure\n+      movptr(header_reg, Address(swap_reg,\n+                                 BasicLock::displaced_header_offset_in_bytes()));\n@@ -1335,2 +1366,2 @@\n-    \/\/ Test for recursion\n-    testptr(header_reg, header_reg);\n+      \/\/ Test for recursion\n+      testptr(header_reg, header_reg);\n@@ -1338,2 +1369,2 @@\n-    \/\/ zero for recursive case\n-    jcc(Assembler::zero, count_locking);\n+      \/\/ zero for recursive case\n+      jcc(Assembler::zero, count_locking);\n@@ -1341,3 +1372,3 @@\n-    \/\/ Atomic swap back the old header\n-    lock();\n-    cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+      \/\/ Atomic swap back the old header\n+      lock();\n+      cmpxchgptr(header_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -1345,2 +1376,2 @@\n-    \/\/ zero for simple unlock of a stack-lock case\n-    jcc(Assembler::notZero, slow_case);\n+      \/\/ zero for simple unlock of a stack-lock case\n+      jcc(Assembler::notZero, slow_case);\n@@ -1348,1 +1379,2 @@\n-    bind(count_locking);\n+      bind(count_locking);\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":102,"deletions":70,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -4172,1 +4173,1 @@\n-  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  assert((offset_in_bytes & (BytesPerInt - 1)) == 0, \"offset must be a multiple of BytesPerInt\");\n@@ -4178,0 +4179,13 @@\n+  \/\/ Emit single 32bit store to clear leading bytes, if necessary.\n+  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n+#ifdef _LP64\n+  if (!is_aligned(offset_in_bytes, BytesPerWord)) {\n+    movl(Address(address, offset_in_bytes), temp);\n+    offset_in_bytes += BytesPerInt;\n+    decrement(length_in_bytes, BytesPerInt);\n+  }\n+  assert((offset_in_bytes & (BytesPerWord - 1)) == 0, \"offset must be a multiple of BytesPerWord\");\n+  testptr(length_in_bytes, length_in_bytes);\n+  jcc(Assembler::zero, done);\n+#endif\n+\n@@ -4190,1 +4204,0 @@\n-  xorptr(temp, temp);    \/\/ use _zero reg to clear memory (shorter code)\n@@ -5122,0 +5135,22 @@\n+#ifdef _LP64\n+void MacroAssembler::load_nklass(Register dst, Register src) {\n+  assert(UseCompressedClassPointers, \"expect compressed class pointers\");\n+\n+  if (!UseCompactObjectHeaders) {\n+    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    return;\n+  }\n+\n+  Label fast;\n+  movq(dst, Address(src, oopDesc::mark_offset_in_bytes()));\n+  testb(dst, markWord::monitor_value);\n+  jccb(Assembler::zero, fast);\n+\n+  \/\/ Fetch displaced header\n+  movq(dst, Address(dst, OM_OFFSET_NO_MONITOR_VALUE_TAG(header)));\n+\n+  bind(fast);\n+  shrq(dst, markWord::klass_shift);\n+}\n+#endif\n+\n@@ -5127,1 +5162,1 @@\n-    movl(dst, Address(src, oopDesc::klass_offset_in_bytes()));\n+    load_nklass(dst, src);\n@@ -5134,0 +5169,1 @@\n+  assert(!UseCompactObjectHeaders, \"not with compact headers\");\n@@ -5143,1 +5179,40 @@\n-    movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+   movptr(Address(dst, oopDesc::klass_offset_in_bytes()), src);\n+}\n+\n+void MacroAssembler::cmp_klass(Register klass, Register obj, Register tmp) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    load_nklass(tmp, obj);\n+    cmpl(klass, tmp);\n+  } else if (UseCompressedClassPointers) {\n+    cmpl(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    cmpptr(klass, Address(obj, oopDesc::klass_offset_in_bytes()));\n+  }\n+}\n+\n+void MacroAssembler::cmp_klass(Register src, Register dst, Register tmp1, Register tmp2) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    \/\/ NOTE: We need to deal with possible ObjectMonitor in object header.\n+    \/\/ Eventually we might be able to do simple movl & cmpl like in\n+    \/\/ the CCP path below.\n+    assert(tmp2 != noreg, \"need tmp2\");\n+    assert_different_registers(src, dst, tmp1, tmp2);\n+    load_nklass(tmp1, src);\n+    load_nklass(tmp2, dst);\n+    cmpl(tmp1, tmp2);\n+  } else if (UseCompressedClassPointers) {\n+    movl(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpl(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  } else\n+#endif\n+  {\n+    movptr(tmp1, Address(src, oopDesc::klass_offset_in_bytes()));\n+    cmpptr(tmp1, Address(dst, oopDesc::klass_offset_in_bytes()));\n+  }\n@@ -5350,0 +5425,62 @@\n+MacroAssembler::KlassDecodeMode MacroAssembler::_klass_decode_mode = KlassDecodeNone;\n+\n+\/\/ Returns a static string\n+const char* MacroAssembler::describe_klass_decode_mode(MacroAssembler::KlassDecodeMode mode) {\n+  switch (mode) {\n+  case KlassDecodeNone: return \"none\";\n+  case KlassDecodeZero: return \"zero\";\n+  case KlassDecodeXor:  return \"xor\";\n+  case KlassDecodeAdd:  return \"add\";\n+  default:\n+    ShouldNotReachHere();\n+  }\n+  return NULL;\n+}\n+\n+\/\/ Return the current narrow Klass pointer decode mode.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode() {\n+  if (_klass_decode_mode == KlassDecodeNone) {\n+    \/\/ First time initialization\n+    assert(UseCompressedClassPointers, \"not using compressed class pointers\");\n+    assert(Metaspace::initialized(), \"metaspace not initialized yet\");\n+\n+    _klass_decode_mode = klass_decode_mode_for_base(CompressedKlassPointers::base());\n+    guarantee(_klass_decode_mode != KlassDecodeNone,\n+              PTR_FORMAT \" is not a valid encoding base on aarch64\",\n+              p2i(CompressedKlassPointers::base()));\n+    log_info(metaspace)(\"klass decode mode initialized: %s\", describe_klass_decode_mode(_klass_decode_mode));\n+  }\n+  return _klass_decode_mode;\n+}\n+\n+\/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+\/\/ if base address is not valid for encoding.\n+MacroAssembler::KlassDecodeMode MacroAssembler::klass_decode_mode_for_base(address base) {\n+  assert(CompressedKlassPointers::shift() != 0, \"not lilliput?\");\n+\n+  const uint64_t base_u64 = (uint64_t) base;\n+\n+  if (base_u64 == 0) {\n+    return KlassDecodeZero;\n+  }\n+\n+  if ((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0) {\n+    return KlassDecodeXor;\n+  }\n+\n+  \/\/ Note that there is no point in optimizing for shift=3 since lilliput\n+  \/\/ will use larger shifts\n+\n+  \/\/ The add+shift mode for decode_and_move_klass_not_null() requires the base to be\n+  \/\/  shiftable-without-loss. So, this is the minimum restriction on x64 for a valid\n+  \/\/  encoding base. This does not matter in reality since the shift values we use for\n+  \/\/  Lilliput, while large, won't be larger than a page size. And the encoding base\n+  \/\/  will be quite likely page aligned since it usually falls to the beginning of\n+  \/\/  either CDS or CCS.\n+  if ((base_u64 & (KlassAlignmentInBytes - 1)) == 0) {\n+    return KlassDecodeAdd;\n+  }\n+\n+  return KlassDecodeNone;\n+}\n+\n@@ -5352,1 +5489,12 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+    xorq(r, tmp);\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5355,0 +5503,2 @@\n+    shrq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5356,3 +5506,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(r, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5364,1 +5513,13 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    mov64(dst, (int64_t)CompressedKlassPointers::base());\n+    xorq(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n@@ -5367,2 +5528,2 @@\n-  } else {\n-    movptr(dst, src);\n+    shrq(dst, CompressedKlassPointers::shift());\n+    break;\n@@ -5370,3 +5531,2 @@\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert (LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shrq(dst, LogKlassAlignmentInBytes);\n+  default:\n+    ShouldNotReachHere();\n@@ -5378,8 +5538,5 @@\n-  \/\/ Note: it will change flags\n-  assert(UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n-  \/\/ vtableStubs also counts instructions in pd_code_size_limit.\n-  \/\/ Also do not verify_oop as this is called by verify_oop.\n-  if (CompressedKlassPointers::shift() != 0) {\n-    assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-    shlq(r, LogKlassAlignmentInBytes);\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    break;\n@@ -5387,2 +5544,11 @@\n-  if (CompressedKlassPointers::base() != nullptr) {\n-    mov64(tmp, (int64_t)CompressedKlassPointers::base());\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n+    xorq(r, tmp);\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    shlq(r, CompressedKlassPointers::shift());\n+    mov64(tmp, base_u64);\n@@ -5390,0 +5556,4 @@\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -5395,3 +5565,1 @@\n-  \/\/ Note: it will change flags\n-  assert (UseCompressedClassPointers, \"should only be used for compressed headers\");\n-  \/\/ Cannot assert, unverified entry point counts instructions (see .ad file)\n+  \/\/ Note: Cannot assert, unverified entry point counts instructions (see .ad file)\n@@ -5401,18 +5569,28 @@\n-  if (CompressedKlassPointers::base() == nullptr &&\n-      CompressedKlassPointers::shift() == 0) {\n-    \/\/ The best case scenario is that there is no base or shift. Then it is already\n-    \/\/ a pointer that needs nothing but a register rename.\n-    movl(dst, src);\n-  } else {\n-    if (CompressedKlassPointers::base() != nullptr) {\n-      mov64(dst, (int64_t)CompressedKlassPointers::base());\n-    } else {\n-      xorq(dst, dst);\n-    }\n-    if (CompressedKlassPointers::shift() != 0) {\n-      assert(LogKlassAlignmentInBytes == CompressedKlassPointers::shift(), \"decode alg wrong\");\n-      assert(LogKlassAlignmentInBytes == Address::times_8, \"klass not aligned on 64bits?\");\n-      leaq(dst, Address(dst, src, Address::times_8, 0));\n-    } else {\n-      addq(dst, src);\n-    }\n+  const uint64_t base_u64 = (uint64_t)CompressedKlassPointers::base();\n+\n+  switch (klass_decode_mode()) {\n+  case KlassDecodeZero: {\n+    movq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeXor: {\n+    assert((base_u64 & (KlassEncodingMetaspaceMax - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for xor mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    xorq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  case KlassDecodeAdd: {\n+    assert((base_u64 & (KlassAlignmentInBytes - 1)) == 0,\n+           \"base \" UINT64_FORMAT_X \" invalid for add mode\", base_u64); \/\/ should have been handled at VM init.\n+    const uint64_t base_right_shifted = base_u64 >> CompressedKlassPointers::shift();\n+    mov64(dst, base_right_shifted);\n+    addq(dst, src);\n+    shlq(dst, CompressedKlassPointers::shift());\n+    break;\n+  }\n+  default:\n+    ShouldNotReachHere();\n@@ -9673,0 +9851,58 @@\n+\n+void MacroAssembler::fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, thread, tmp);\n+\n+  \/\/ First we need to check if the lock-stack has room for pushing the object reference.\n+  if (rt_check_stack) {\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::greaterEqual, slow);\n+  }\n+#ifdef ASSERT\n+  else {\n+    Label ok;\n+    movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+    cmpptr(tmp, Address(thread, JavaThread::lock_stack_limit_offset()));\n+    jcc(Assembler::less, ok);\n+    stop(\"Not enough room in lock stack; should have been checked in the method prologue\");\n+    bind(ok);\n+  }\n+#endif\n+\n+  \/\/ Now we attempt to take the fast-lock.\n+  \/\/ Clear lowest two header bits (locked state).\n+  andptr(hdr, ~(int32_t)markWord::lock_mask_in_place);\n+  movptr(tmp, hdr);\n+  \/\/ Set lowest bit (unlocked state).\n+  orptr(hdr, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+\n+  \/\/ If successful, push object to lock-stack.\n+  movptr(tmp, Address(thread, JavaThread::lock_stack_current_offset()));\n+  movptr(Address(tmp, 0), obj);\n+  increment(tmp, oopSize);\n+  movptr(Address(thread, JavaThread::lock_stack_current_offset()), tmp);\n+}\n+\n+void MacroAssembler::fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow) {\n+  assert(hdr == rax, \"header must be in rax for cmpxchg\");\n+  assert_different_registers(obj, hdr, tmp);\n+\n+  \/\/ Mark-word must be 00 now, try to swing it back to 01 (unlocked)\n+  movptr(tmp, hdr); \/\/ The expected old value\n+  orptr(tmp, markWord::unlocked_value);\n+  lock();\n+  cmpxchgptr(tmp, Address(obj, oopDesc::mark_offset_in_bytes()));\n+  jcc(Assembler::notEqual, slow);\n+  \/\/ Pop the lock object from the lock-stack.\n+#ifdef _LP64\n+  const Register thread = r15_thread;\n+#else\n+  const Register thread = rax;\n+  get_thread(rax);\n+#endif\n+  subptr(Address(thread, JavaThread::lock_stack_current_offset()), oopSize);\n+}\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":281,"deletions":45,"binary":false,"changes":326,"status":"modified"},{"patch":"@@ -82,0 +82,22 @@\n+ public:\n+\n+  enum KlassDecodeMode {\n+    KlassDecodeNone,\n+    KlassDecodeZero,\n+    KlassDecodeXor,\n+    KlassDecodeAdd\n+  };\n+\n+  \/\/ Return the current narrow Klass pointer decode mode. Initialized on first call.\n+  static KlassDecodeMode klass_decode_mode();\n+\n+  \/\/ Given an arbitrary base address, return the KlassDecodeMode that would be used. Return KlassDecodeNone\n+  \/\/ if base address is not valid for encoding.\n+  static KlassDecodeMode klass_decode_mode_for_base(address base);\n+\n+  \/\/ Returns a static string\n+  static const char* describe_klass_decode_mode(KlassDecodeMode mode);\n+\n+ private:\n+  static KlassDecodeMode _klass_decode_mode;\n+\n@@ -364,0 +386,3 @@\n+#ifdef _LP64\n+  void load_nklass(Register dst, Register src);\n+#endif\n@@ -367,0 +392,8 @@\n+  \/\/ Compares the Klass pointer of an object to a given Klass (which might be narrow,\n+  \/\/ depending on UseCompressedClassPointers).\n+  void cmp_klass(Register klass, Register dst, Register tmp);\n+\n+  \/\/ Compares the Klass pointer of two objects o1 and o2. Result is in the condition flags.\n+  \/\/ Uses t1 and t2 as temporary registers.\n+  void cmp_klass(Register src, Register dst, Register tmp1, Register tmp2);\n+\n@@ -2010,0 +2043,2 @@\n+  void fast_lock_impl(Register obj, Register hdr, Register thread, Register tmp, Label& slow, bool rt_check_stack = true);\n+  void fast_unlock_impl(Register obj, Register hdr, Register tmp, Label& slow);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":35,"deletions":0,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -1684,30 +1684,36 @@\n-      \/\/ Load immediate 1 into swap_reg %rax,\n-      __ movptr(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax,\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n-\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n-\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n-\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, thread, lock_reg, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax,\n+        __ movptr(swap_reg, 1);\n+\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax,\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = lock_reg iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n+\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax, as the result of cmpxchg\n+\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+       __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -1839,1 +1845,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -1855,12 +1861,18 @@\n-      \/\/  get old displaced header\n-      __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      \/\/ src -> dest iff dest == rax, else rax, <- dest\n-      \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n-      __ lock();\n-      __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/  get old displaced header\n+        __ movptr(rbx, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rbp, lock_slot_rbp_offset));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        \/\/ src -> dest iff dest == rax, else rax, <- dest\n+        \/\/ *obj_reg = rbx, iff *obj_reg == rax, else rax, = *(obj_reg)\n+        __ lock();\n+        __ cmpxchgptr(rbx, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":55,"deletions":43,"binary":false,"changes":98,"status":"modified"},{"patch":"@@ -2153,0 +2153,7 @@\n+      if (UseFastLocking) {\n+        \/\/ Load object header\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ fast_lock_impl(obj_reg, swap_reg, r15_thread, rscratch1, slow_path_lock);\n+      } else {\n+        \/\/ Load immediate 1 into swap_reg %rax\n+        __ movl(swap_reg, 1);\n@@ -2154,5 +2161,2 @@\n-      \/\/ Load immediate 1 into swap_reg %rax\n-      __ movl(swap_reg, 1);\n-\n-      \/\/ Load (object->mark() | 1) into swap_reg %rax\n-      __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        \/\/ Load (object->mark() | 1) into swap_reg %rax\n+        __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n@@ -2160,2 +2164,2 @@\n-      \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        \/\/ Save (object->mark() | 1) into BasicLock's displaced header\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n@@ -2163,4 +2167,4 @@\n-      \/\/ src -> dest iff dest == rax else rax <- dest\n-      __ lock();\n-      __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::equal, count_mon);\n+        \/\/ src -> dest iff dest == rax else rax <- dest\n+        __ lock();\n+        __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::equal, count_mon);\n@@ -2168,1 +2172,1 @@\n-      \/\/ Hmm should this move to the slow path code area???\n+        \/\/ Hmm should this move to the slow path code area???\n@@ -2170,8 +2174,8 @@\n-      \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n-      \/\/  1) (mark & 3) == 0, and\n-      \/\/  2) rsp <= mark < mark + os::pagesize()\n-      \/\/ These 3 tests can be done by evaluating the following\n-      \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n-      \/\/ assuming both stack pointer and pagesize have their\n-      \/\/ least significant 2 bits clear.\n-      \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n+        \/\/ Test if the oopMark is an obvious stack pointer, i.e.,\n+        \/\/  1) (mark & 3) == 0, and\n+        \/\/  2) rsp <= mark < mark + os::pagesize()\n+        \/\/ These 3 tests can be done by evaluating the following\n+        \/\/ expression: ((mark - rsp) & (3 - os::vm_page_size())),\n+        \/\/ assuming both stack pointer and pagesize have their\n+        \/\/ least significant 2 bits clear.\n+        \/\/ NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg\n@@ -2179,2 +2183,2 @@\n-      __ subptr(swap_reg, rsp);\n-      __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n+        __ subptr(swap_reg, rsp);\n+        __ andptr(swap_reg, 3 - (int)os::vm_page_size());\n@@ -2182,3 +2186,4 @@\n-      \/\/ Save the test result, for recursive case, the result is zero\n-      __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n-      __ jcc(Assembler::notEqual, slow_path_lock);\n+        \/\/ Save the test result, for recursive case, the result is zero\n+        __ movptr(Address(lock_reg, mark_word_offset), swap_reg);\n+        __ jcc(Assembler::notEqual, slow_path_lock);\n+      }\n@@ -2298,1 +2303,1 @@\n-    if (!UseHeavyMonitors) {\n+    if (!UseHeavyMonitors && !UseFastLocking) {\n@@ -2314,9 +2319,15 @@\n-      \/\/ get address of the stack lock\n-      __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n-      \/\/  get old displaced header\n-      __ movptr(old_hdr, Address(rax, 0));\n-\n-      \/\/ Atomic swap old header if oop still contains the stack lock\n-      __ lock();\n-      __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n-      __ jcc(Assembler::notEqual, slow_path_unlock);\n+      if (UseFastLocking) {\n+        __ movptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ andptr(swap_reg, ~(int32_t)markWord::lock_mask_in_place);\n+        __ fast_unlock_impl(obj_reg, swap_reg, lock_reg, slow_path_unlock);\n+      } else {\n+        \/\/ get address of the stack lock\n+        __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));\n+        \/\/  get old displaced header\n+        __ movptr(old_hdr, Address(rax, 0));\n+\n+        \/\/ Atomic swap old header if oop still contains the stack lock\n+        __ lock();\n+        __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));\n+        __ jcc(Assembler::notEqual, slow_path_unlock);\n+      }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":46,"deletions":35,"binary":false,"changes":81,"status":"modified"},{"patch":"@@ -3195,0 +3195,49 @@\n+\/\/ Call runtime to ensure lock-stack size.\n+\/\/ Arguments:\n+\/\/ - c_rarg0: the required _limit pointer\n+address StubGenerator::generate_check_lock_stack() {\n+  __ align(CodeEntryAlignment);\n+  StubCodeMark mark(this, \"StubRoutines\", \"check_lock_stack\");\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  __ enter(); \/\/ save rbp\n+\n+  __ pusha();\n+\n+  \/\/ The method may have floats as arguments, and we must spill them before calling\n+  \/\/ the VM runtime.\n+  assert(Argument::n_float_register_parameters_j == 8, \"Assumption\");\n+  const int xmm_size = wordSize * 2;\n+  const int xmm_spill_size = xmm_size * Argument::n_float_register_parameters_j;\n+  __ subptr(rsp, xmm_spill_size);\n+  __ movdqu(Address(rsp, xmm_size * 7), xmm7);\n+  __ movdqu(Address(rsp, xmm_size * 6), xmm6);\n+  __ movdqu(Address(rsp, xmm_size * 5), xmm5);\n+  __ movdqu(Address(rsp, xmm_size * 4), xmm4);\n+  __ movdqu(Address(rsp, xmm_size * 3), xmm3);\n+  __ movdqu(Address(rsp, xmm_size * 2), xmm2);\n+  __ movdqu(Address(rsp, xmm_size * 1), xmm1);\n+  __ movdqu(Address(rsp, xmm_size * 0), xmm0);\n+\n+  __ call_VM_leaf(CAST_FROM_FN_PTR(address, static_cast<void (*)(oop*)>(LockStack::ensure_lock_stack_size)), rax);\n+\n+  __ movdqu(xmm0, Address(rsp, xmm_size * 0));\n+  __ movdqu(xmm1, Address(rsp, xmm_size * 1));\n+  __ movdqu(xmm2, Address(rsp, xmm_size * 2));\n+  __ movdqu(xmm3, Address(rsp, xmm_size * 3));\n+  __ movdqu(xmm4, Address(rsp, xmm_size * 4));\n+  __ movdqu(xmm5, Address(rsp, xmm_size * 5));\n+  __ movdqu(xmm6, Address(rsp, xmm_size * 6));\n+  __ movdqu(xmm7, Address(rsp, xmm_size * 7));\n+  __ addptr(rsp, xmm_spill_size);\n+\n+  __ popa();\n+\n+  __ leave();\n+\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n@@ -3943,0 +3992,4 @@\n+\n+  if (UseFastLocking) {\n+    StubRoutines::x86::_check_lock_stack = generate_check_lock_stack();\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":53,"deletions":0,"binary":false,"changes":53,"status":"modified"},{"patch":"@@ -472,0 +472,2 @@\n+  address generate_check_lock_stack();\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -87,0 +87,1 @@\n+address StubRoutines::x86::_check_lock_stack = nullptr;\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -134,0 +134,2 @@\n+  static address _check_lock_stack;\n+\n@@ -223,0 +225,2 @@\n+  static address check_lock_stack() { return _check_lock_stack; }\n+\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -4034,1 +4034,2 @@\n-    __ decrement(rdx, sizeof(oopDesc));\n+    int header_size = align_up(oopDesc::base_offset_in_bytes(), BytesPerLong);\n+    __ decrement(rdx, header_size);\n@@ -4056,2 +4057,2 @@\n-    __ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 1*oopSize), rcx);\n-    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, sizeof(oopDesc) - 2*oopSize), rcx));\n+    __ movptr(Address(rax, rdx, Address::times_8, header_size - 1*oopSize), rcx);\n+    NOT_LP64(__ movptr(Address(rax, rdx, Address::times_8, header_size - 2*oopSize), rcx));\n@@ -4064,3 +4065,8 @@\n-    __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n-              (intptr_t)markWord::prototype().value()); \/\/ header\n-    __ pop(rcx);   \/\/ get saved klass back in the register.\n+    if (UseCompactObjectHeaders) {\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n+      __ movptr(rbx, Address(rcx, Klass::prototype_header_offset()));\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes ()), rbx);\n+    } else {\n+      __ movptr(Address(rax, oopDesc::mark_offset_in_bytes()),\n+                (intptr_t)markWord::prototype().value()); \/\/ header\n+      __ pop(rcx);   \/\/ get saved klass back in the register.\n@@ -4068,2 +4074,2 @@\n-    __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n-    __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n+      __ xorl(rsi, rsi); \/\/ use zero reg to clear memory (shorter code)\n+      __ store_klass_gap(rax, rsi);  \/\/ zero klass gap for compressed oops\n@@ -4071,1 +4077,2 @@\n-    __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+      __ store_klass(rax, rcx, rscratch1);  \/\/ klass\n+    }\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":16,"deletions":9,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -220,1 +220,1 @@\n-  const ptrdiff_t estimate = 136;\n+  const ptrdiff_t estimate = 137;\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -617,1 +617,2 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL);\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, C->in_24_bit_fp_mode(), C->stub_function() != NULL, max_monitors);\n@@ -13763,1 +13764,1 @@\n-instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2) %{\n+instruct cmpFastLockRTM(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eDXRegI scr, rRegI cx1, rRegI cx2, eRegP thread) %{\n@@ -13766,1 +13767,1 @@\n-  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, TEMP cx1, TEMP cx2, USE_KILL box, TEMP thread);\n@@ -13770,0 +13771,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13771,1 +13773,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, $thread$$Register,\n@@ -13779,1 +13781,1 @@\n-instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr) %{\n+instruct cmpFastLock(eFlagsReg cr, eRegP object, eBXRegP box, eAXRegI tmp, eRegP scr, eRegP thread) %{\n@@ -13782,1 +13784,1 @@\n-  effect(TEMP tmp, TEMP scr, USE_KILL box);\n+  effect(TEMP tmp, TEMP scr, USE_KILL box, TEMP thread);\n@@ -13786,0 +13788,1 @@\n+    __ get_thread($thread$$Register);\n@@ -13787,1 +13790,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, $thread$$Register, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":10,"deletions":7,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -925,1 +925,2 @@\n-  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL);\n+  int max_monitors = C->method() != NULL ? C->max_monitors() : 0;\n+  __ verified_entry(framesize, C->output()->need_stack_bang(bangsize)?bangsize:0, false, C->stub_function() != NULL, max_monitors);\n@@ -5317,0 +5318,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -5327,0 +5329,23 @@\n+instruct loadNKlassLilliput(rRegN dst, indOffset8 mem, rFlagsReg cr)\n+%{\n+  predicate(UseCompactObjectHeaders);\n+  match(Set dst (LoadNKlass mem));\n+  effect(TEMP_DEF dst, KILL cr);\n+  ins_cost(125); \/\/ XXX\n+  format %{ \"movl    $dst, $mem\\t# compressed klass ptr\" %}\n+  ins_encode %{\n+    assert($mem$$disp == oopDesc::klass_offset_in_bytes(), \"expect correct offset 4, but got: %d\", $mem$$disp);\n+    assert($mem$$index == 4, \"expect no index register: %d\", $mem$$index);\n+    Register dst = $dst$$Register;\n+    Register obj = $mem$$base$$Register;\n+    C2LoadNKlassStub* stub = new (Compile::current()->comp_arena()) C2LoadNKlassStub(dst);\n+    Compile::current()->output()->add_stub(stub);\n+    __ movq(dst, Address(obj, oopDesc::mark_offset_in_bytes()));\n+    __ testb(dst, markWord::monitor_value);\n+    __ jcc(Assembler::notZero, stub->entry());\n+    __ bind(stub->continuation());\n+    __ shrq(dst, markWord::klass_shift);\n+  %}\n+  ins_pipe(pipe_slow); \/\/ XXX\n+%}\n+\n@@ -12668,0 +12693,1 @@\n+  predicate(!UseCompactObjectHeaders);\n@@ -13294,1 +13320,1 @@\n-                 $scr$$Register, $cx1$$Register, $cx2$$Register,\n+                 $scr$$Register, $cx1$$Register, $cx2$$Register, r15_thread,\n@@ -13310,1 +13336,1 @@\n-                 $scr$$Register, noreg, noreg, NULL, NULL, NULL, false, false);\n+                 $scr$$Register, noreg, noreg, r15_thread, NULL, NULL, NULL, false, false);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":29,"deletions":3,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -389,0 +389,1 @@\n+    push_monitor();\n@@ -576,0 +577,1 @@\n+, _max_monitors(0)\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -2318,0 +2318,1 @@\n+  compilation()->push_monitor();\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -893,0 +893,1 @@\n+      if (opLoadKlass->_stub) do_stub(opLoadKlass->_stub);\n@@ -1073,0 +1074,3 @@\n+  if (stub()) {\n+    masm->append_code_stub(stub());\n+  }\n@@ -2043,0 +2047,3 @@\n+  if (stub()) {\n+    out->print(\"[lbl:\" INTPTR_FORMAT \"]\", p2i(stub()->entry()));\n+  }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":7,"deletions":0,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -1906,0 +1906,1 @@\n+  CodeStub* _stub;\n@@ -1907,1 +1908,1 @@\n-  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info)\n+  LIR_OpLoadKlass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub)\n@@ -1910,1 +1911,1 @@\n-    {}\n+    , _stub(stub) {}\n@@ -1913,0 +1914,1 @@\n+  CodeStub* stub()     const { return _stub; }\n@@ -2378,1 +2380,1 @@\n-  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info) { append(new LIR_OpLoadKlass(obj, result, info)); }\n+  void load_klass(LIR_Opr obj, LIR_Opr result, CodeEmitInfo* info, CodeStub* stub) { append(new LIR_OpLoadKlass(obj, result, info, stub)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -776,1 +776,1 @@\n-  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes());\n+  _masm->build_frame(initial_frame_size_in_bytes(), bang_size_in_bytes(), compilation()->max_monitors());\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1244,1 +1244,2 @@\n-  __ load_klass(obj, klass, null_check_info);\n+  CodeStub* slow_path = UseCompactObjectHeaders ? new LoadKlassStub(klass) : nullptr;\n+  __ load_klass(obj, klass, null_check_info, slow_path);\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -760,2 +760,2 @@\n-  assert(obj == lock->obj(), \"must match\");\n-  SharedRuntime::monitor_enter_helper(obj, lock->lock(), current);\n+  assert(UseFastLocking || obj == lock->obj(), \"must match\");\n+  SharedRuntime::monitor_enter_helper(obj, UseFastLocking ? NULL : lock->lock(), current);\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -43,0 +43,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -44,0 +45,1 @@\n+#include \"oops\/klass.inline.hpp\"\n@@ -222,2 +224,4 @@\n-    \/\/ See RunTimeClassInfo::get_for()\n-    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, SharedSpaceObjectAlignment);\n+    \/\/ See ArchiveBuilder::make_shallow_copies: make sure we have enough space for both maximum\n+    \/\/ Klass alignment as well as the RuntimeInfo* pointer we will embed in front of a Klass.\n+    _estimated_metaspaceobj_bytes += align_up(BytesPerWord, KlassAlignmentInBytes) +\n+        align_up(sizeof(void*), SharedSpaceObjectAlignment);\n@@ -620,4 +624,5 @@\n-    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n-    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n-    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n-    \/\/ in systemDictionaryShared.cpp.\n+    \/\/ Reserve space for a pointer immediately in front of an InstanceKlass. That space will\n+    \/\/ later be used to store the RuntimeClassInfo* pointer directly in front of the archived\n+    \/\/ InstanceKlass, in order to have a quick lookup InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\/::set_for() for\n+    \/\/ details.\n@@ -629,0 +634,3 @@\n+    dest = dump_region->allocate(bytes, KlassAlignmentInBytes);\n+  } else {\n+    dest = dump_region->allocate(bytes);\n@@ -630,1 +638,0 @@\n-  dest = dump_region->allocate(bytes);\n@@ -649,1 +656,2 @@\n-  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d\", p2i(src), p2i(dest), bytes);\n+  log_trace(cds)(\"Copy: \" PTR_FORMAT \" ==> \" PTR_FORMAT \" %d (%s)\", p2i(src), p2i(dest), bytes,\n+                 MetaspaceObj::type_name(ref->msotype()));\n@@ -653,0 +661,2 @@\n+\n+  DEBUG_ONLY(_alloc_stats.verify((int)dump_region->used(), src_info->read_only()));\n@@ -749,0 +759,7 @@\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      Klass* requested_k = to_requested(k);\n+      narrowKlass nk = CompressedKlassPointers::encode_not_null(requested_k, _requested_static_archive_bottom);\n+      k->set_prototype_header(markWord::prototype().set_narrow_klass(nk));\n+    }\n+#endif \/\/_LP64\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":25,"deletions":8,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"memory\/metaspace\/metaspaceAlignment.hpp\"\n@@ -46,3 +47,13 @@\n-\/\/ Metaspace::allocate() requires that all blocks must be aligned with KlassAlignmentInBytes.\n-\/\/ We enforce the same alignment rule in blocks allocated from the shared space.\n-const int SharedSpaceObjectAlignment = KlassAlignmentInBytes;\n+\/\/ CDS has three alignments to deal with:\n+\/\/ - SharedSpaceObjectAlignment, always 8 bytes: used for placing arbitrary structures.\n+\/\/   These may contain 64-bit members (not larger, we know that much). Therefore we\n+\/\/   need to use 64-bit alignment on both 32-bit and 64-bit platforms. We reuse metaspace\n+\/\/   minimal alignment for this, which follows the same logic.\n+\/\/ - With CompressedClassPointers=1, we need to store Klass structures with a large\n+\/\/   alignment (Lilliput specific narrow Klass pointer encoding) - KlassAlignmentInBytes.\n+\/\/ - Header data and tags are squeezed in with word alignment, which happens to be 4 bytes\n+\/\/   on 32-bit. See ReadClosure::do_xxx() and DumpRegion::append_intptr().\n+const int SharedSpaceObjectAlignment = metaspace::MetaspaceMinAlignmentBytes;\n+\n+\/\/ standard alignment should be sufficient for storing 64-bit values.\n+STATIC_ASSERT(SharedSpaceObjectAlignment >= sizeof(uint64_t));\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":14,"deletions":3,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -196,2 +196,7 @@\n-    oopDesc::set_mark(mem, markWord::prototype());\n-    oopDesc::release_set_klass(mem, k);\n+    if (UseCompactObjectHeaders) {\n+      narrowKlass nk = ArchiveBuilder::current()->get_requested_narrow_klass(k);\n+      oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+    } else {\n+      oopDesc::set_mark(mem, markWord::prototype());\n+      oopDesc::release_set_klass(mem, k);\n+    }\n@@ -263,2 +268,6 @@\n-  oopDesc::set_mark(mem, markWord::prototype());\n-  cast_to_oop(mem)->set_narrow_klass(nk);\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, markWord::prototype().set_narrow_klass(nk));\n+  } else {\n+    oopDesc::set_mark(mem, markWord::prototype());\n+    cast_to_oop(mem)->set_narrow_klass(nk);\n+  }\n@@ -433,1 +442,5 @@\n-    fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    if (UseCompactObjectHeaders) {\n+      fake_oop->set_mark(markWord::prototype().set_narrow_klass(nk).copy_set_hash(src_hash));\n+    } else {\n+      fake_oop->set_mark(markWord::prototype().copy_set_hash(src_hash));\n+    }\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":18,"deletions":5,"binary":false,"changes":23,"status":"modified"},{"patch":"@@ -209,3 +209,13 @@\n-char* DumpRegion::allocate(size_t num_bytes) {\n-  char* p = (char*)align_up(_top, (size_t)SharedSpaceObjectAlignment);\n-  char* newtop = p + align_up(num_bytes, (size_t)SharedSpaceObjectAlignment);\n+char* DumpRegion::allocate(size_t num_bytes, size_t alignment) {\n+  \/\/ We align the starting address of each allocation.\n+  char* p = (char*)align_up(_top, alignment);\n+  char* newtop = p + num_bytes;\n+  \/\/ Leave _top always SharedSpaceObjectAlignment aligned. But not more -\n+  \/\/  if we allocate with large alignments, lets not waste the gaps.\n+  \/\/ Ideally we would not need to align _top to anything here but CDS has\n+  \/\/  a number of implicit alignment assumptions. Leaving this unaligned\n+  \/\/  here will trip of at least ReadClosure (assuming word alignment) and\n+  \/\/  DumpAllocStats (will get confused about counting bytes on 32-bit\n+  \/\/  platforms if we align to anything less than SharedSpaceObjectAlignment\n+  \/\/  here).\n+  newtop = align_up(newtop, SharedSpaceObjectAlignment);\n@@ -213,1 +223,1 @@\n-  memset(p, 0, newtop - p);\n+  memset(p, 0, newtop - p); \/\/ todo: needed? debug_only?\n@@ -217,0 +227,4 @@\n+char* DumpRegion::allocate(size_t num_bytes) {\n+  return allocate(num_bytes, SharedSpaceObjectAlignment);\n+}\n+\n@@ -313,1 +327,1 @@\n-  assert(tag == old_tag, \"old tag doesn't match\");\n+  assert(tag == old_tag, \"tag doesn't match (%d, expected %d)\", old_tag, tag);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":19,"deletions":5,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -110,0 +110,12 @@\n+\n+#ifdef ASSERT\n+void DumpAllocStats::verify(int expected_byte_size, bool read_only) const {\n+  int bytes = 0;\n+  const int what = (int)(read_only ? RO : RW);\n+  for (int type = 0; type < int(_number_of_types); type ++) {\n+    bytes += _bytes[what][type];\n+  }\n+  assert(bytes == expected_byte_size, \"counter mismatch (%s: %d vs %d)\",\n+         (read_only ? \"RO\" : \"RW\"), bytes, expected_byte_size);\n+}\n+#endif \/\/ ASSERT\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"runtime\/safepoint.hpp\"\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -61,0 +61,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -1316,0 +1317,5 @@\n+#ifdef ASSERT\n+    if (UseCompressedClassPointers) {\n+      CompressedKlassPointers::verify_klass_pointer(record->_klass);\n+    }\n+#endif\n@@ -1317,1 +1323,0 @@\n-    assert(check_alignment(record->_klass), \"Address not aligned\");\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":6,"deletions":1,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -79,0 +79,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -1527,0 +1528,2 @@\n+  GCForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n+\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -213,0 +214,1 @@\n+  GCForwarding::begin();\n@@ -225,0 +227,2 @@\n+  GCForwarding::end();\n+\n@@ -347,1 +351,1 @@\n-  if (scope()->do_maximal_compaction() || !has_free_compaction_targets) {\n+  if (!UseCompactObjectHeaders && (scope()->do_maximal_compaction() || !has_free_compaction_targets)) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -44,1 +45,1 @@\n-  if (obj->is_forwarded()) {\n+  if (GCForwarding::is_forwarded(obj)) {\n@@ -55,2 +56,2 @@\n-  assert(obj->is_forwarded(), \"Sanity!\");\n-  assert(obj->forwardee() != obj, \"Object must have a new location\");\n+  assert(GCForwarding::is_forwarded(obj), \"Sanity!\");\n+  assert(GCForwarding::forwardee(obj) != obj, \"Object must have a new location\");\n@@ -61,1 +62,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n@@ -124,1 +125,1 @@\n-  HeapWord* destination = cast_from_oop<HeapWord*>(obj->forwardee());\n+  HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -105,2 +106,2 @@\n-    object->forward_to(cast_to_oop(_compaction_top));\n-    assert(object->is_forwarded(), \"must be forwarded\");\n+    GCForwarding::forward_to(object, cast_to_oop(_compaction_top));\n+    assert(GCForwarding::is_forwarded(object), \"must be forwarded\");\n@@ -108,1 +109,1 @@\n-    assert(!object->is_forwarded(), \"must not be forwarded\");\n+    assert(GCForwarding::is_not_forwarded(object), \"must not be forwarded\");\n@@ -171,2 +172,2 @@\n-  obj->forward_to(cast_to_oop(dest_hr->bottom()));\n-  assert(obj->is_forwarded(), \"Object must be forwarded!\");\n+  GCForwarding::forward_to(obj, cast_to_oop(dest_hr->bottom()));\n+  assert(GCForwarding::is_forwarded(obj), \"Object must be forwarded!\");\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":6,"deletions":5,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -68,2 +69,2 @@\n-  if (obj->is_forwarded()) {\n-    oop forwardee = obj->forwardee();\n+  if (GCForwarding::is_forwarded(obj)) {\n+    oop forwardee = GCForwarding::forwardee(obj);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":3,"deletions":2,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -105,1 +105,1 @@\n-  if (obj->is_forwarded()) {\n+  if (GCForwarding::is_forwarded(obj)) {\n@@ -108,1 +108,1 @@\n-    if (cast_from_oop<HeapWord*>(obj->forwardee()) < _dense_prefix_top) {\n+    if (cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj)) < _dense_prefix_top) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -209,1 +209,1 @@\n-    obj = cast_to_oop(m.decode_pointer());\n+    obj = obj->forwardee(m);\n@@ -223,1 +223,0 @@\n-  assert(from_obj->is_objArray(), \"must be obj array\");\n@@ -253,1 +252,0 @@\n-  assert(from_obj->is_objArray(), \"precondition\");\n@@ -380,1 +378,1 @@\n-                                                  oop const old, size_t word_sz, uint age,\n+                                                  oop const old, Klass* klass, size_t word_sz, uint age,\n@@ -384,1 +382,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_in_new_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -388,1 +386,1 @@\n-    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(old->klass(), word_sz * HeapWordSize, age,\n+    _g1h->gc_tracer_stw()->report_promotion_outside_plab_event(klass, word_sz * HeapWordSize, age,\n@@ -396,0 +394,1 @@\n+                                                   Klass* klass,\n@@ -418,1 +417,1 @@\n-      report_promotion_event(*dest_attr, old, word_sz, age, obj_ptr, node_index);\n+      report_promotion_event(*dest_attr, old, klass, word_sz, age, obj_ptr, node_index);\n@@ -453,0 +452,4 @@\n+  if (old_mark.is_marked()) {\n+    \/\/ Already forwarded by somebody else, return forwardee.\n+    return old->forwardee(old_mark);\n+  }\n@@ -455,1 +458,9 @@\n-  Klass* klass = old->klass();\n+  Klass* klass;\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    klass = old_mark.safe_klass();\n+  } else\n+#endif\n+  {\n+    klass = old->klass();\n+  }\n@@ -468,1 +479,1 @@\n-    obj_ptr = allocate_copy_slow(&dest_attr, old, word_sz, age, node_index);\n+    obj_ptr = allocate_copy_slow(&dest_attr, old, klass, word_sz, age, node_index);\n@@ -623,1 +634,1 @@\n-  oop forward_ptr = old->forward_to_atomic(old, m, memory_order_relaxed);\n+  oop forward_ptr = old->forward_to_self_atomic(m, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":21,"deletions":10,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -165,0 +165,1 @@\n+                               Klass* klass,\n@@ -199,1 +200,1 @@\n-                              oop const old, size_t word_sz, uint age,\n+                              oop const old, Klass* klass, size_t word_sz, uint age,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -850,1 +850,10 @@\n-    from()->set_next_compaction_space(to());\n+    \/\/ Ensure that compaction spaces are in address-order.\n+    if (from()->bottom() < to()->bottom()) {\n+      eden()->set_next_compaction_space(from());\n+      from()->set_next_compaction_space(to());\n+      to()->set_next_compaction_space(nullptr);\n+    } else {\n+      eden()->set_next_compaction_space(to());\n+      to()->set_next_compaction_space(from());\n+      from()->set_next_compaction_space(nullptr);\n+    }\n@@ -885,1 +894,15 @@\n-        obj->init_mark();\n+#ifdef _LP64\n+        if (UseCompactObjectHeaders) {\n+          oop forwardee = obj->forwardee();\n+          markWord header = forwardee->mark();\n+          if (header.has_displaced_mark_helper()) {\n+            header = header.displaced_mark_helper();\n+          }\n+          assert(UseCompressedClassPointers, \"assume +UseCompressedClassPointers\");\n+          narrowKlass nklass = header.narrow_klass();\n+          obj->set_mark(markWord::prototype().set_narrow_klass(nklass));\n+        } else\n+#endif\n+        {\n+          obj->init_mark();\n+        }\n@@ -909,1 +932,1 @@\n-  old->forward_to(old);\n+  old->forward_to_self();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":26,"deletions":3,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -39,0 +39,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -90,0 +91,2 @@\n+  GCForwarding::begin();\n+\n@@ -102,0 +105,2 @@\n+  GCForwarding::end();\n+\n@@ -161,0 +166,3 @@\n+  _young_marked_objects = 0;\n+  _old_marked_objects = 0;\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":8,"deletions":0,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -32,0 +32,1 @@\n+#include \"gc\/shared\/genCollectedHeap.hpp\"\n@@ -175,0 +176,2 @@\n+  ContinuationGCSupport::transform_stack_chunk(obj);\n+\n@@ -178,3 +181,13 @@\n-  obj->set_mark(markWord::prototype().set_marked());\n-\n-  ContinuationGCSupport::transform_stack_chunk(obj);\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord real_mark = mark;\n+    if (real_mark.has_displaced_mark_helper()) {\n+      real_mark = real_mark.displaced_mark_helper();\n+    }\n+    Klass* klass = real_mark.klass();\n+    obj->set_mark(klass->prototype_header().set_marked());\n+  } else\n+#endif\n+  {\n+    obj->set_mark(markWord::prototype().set_marked());\n+  }\n@@ -185,0 +198,6 @@\n+\n+  if (GenCollectedHeap::heap()->is_in_young(obj)) {\n+    _young_marked_objects++;\n+  } else {\n+    _old_marked_objects++;\n+  }\n@@ -240,0 +259,3 @@\n+size_t MarkSweep::_young_marked_objects = 0;\n+size_t MarkSweep::_old_marked_objects = 0;\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":25,"deletions":3,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -118,1 +118,4 @@\n- public:\n+  static size_t _young_marked_objects;\n+  static size_t _old_marked_objects;\n+\n+public:\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -230,4 +230,0 @@\n-  if (is_in(object->klass_raw())) {\n-    return false;\n-  }\n-\n@@ -260,2 +256,4 @@\n-  _filler_array_max_size = align_object_size(filler_array_hdr_size() +\n-                                             max_len \/ elements_per_word);\n+  int header_size_in_bytes = arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"must be aligned to int\");\n+  int header_size_in_ints = header_size_in_bytes \/ sizeof(jint);\n+  _filler_array_max_size = align_object_size((header_size_in_ints + max_len) \/ elements_per_word);\n@@ -422,1 +420,3 @@\n-  size_t max_int_size = typeArrayOopDesc::header_size(T_INT) +\n+  int header_size_in_bytes = typeArrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(header_size_in_bytes % sizeof(jint) == 0, \"header size must align to int\");\n+  size_t max_int_size = header_size_in_bytes \/ HeapWordSize +\n@@ -428,5 +428,2 @@\n-size_t CollectedHeap::filler_array_hdr_size() {\n-  return align_object_offset(arrayOopDesc::header_size(T_INT)); \/\/ align to Long\n-}\n-\n-  return align_object_size(filler_array_hdr_size()); \/\/ align to MinObjAlignment\n+  int aligned_header_size_words = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  return align_object_size(aligned_header_size_words); \/\/ align to MinObjAlignment\n@@ -437,2 +434,3 @@\n-  Copy::fill_to_words(start + filler_array_hdr_size(),\n-                      words - filler_array_hdr_size(), value);\n+  int payload_start = align_up(arrayOopDesc::base_offset_in_bytes(T_INT), HeapWordSize) \/ HeapWordSize;\n+  Copy::fill_to_words(start + payload_start,\n+                      words - payload_start, value);\n@@ -462,2 +460,3 @@\n-  const size_t payload_size = words - filler_array_hdr_size();\n-  const size_t len = payload_size * HeapWordSize \/ sizeof(jint);\n+  const size_t payload_size_bytes = words * HeapWordSize - arrayOopDesc::base_offset_in_bytes(T_INT);\n+  assert(payload_size_bytes % sizeof(jint) == 0, \"must be int aligned\");\n+  const size_t len = payload_size_bytes \/ sizeof(jint);\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":15,"deletions":16,"binary":false,"changes":31,"status":"modified"},{"patch":"@@ -166,1 +166,0 @@\n-  static inline size_t filler_array_hdr_size();\n@@ -312,1 +311,1 @@\n-  static constexpr size_t min_dummy_object_size() {\n+  static size_t min_dummy_object_size() {\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,57 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gcForwarding.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"runtime\/globals.hpp\"\n+\n+SlidingForwarding* GCForwarding::_sliding_forwarding = nullptr;\n+\n+void GCForwarding::initialize(MemRegion heap, size_t region_size_words) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(_sliding_forwarding == nullptr, \"only call this once\");\n+    _sliding_forwarding = new SlidingForwarding(heap, region_size_words);\n+  }\n+#endif\n+}\n+\n+void GCForwarding::begin() {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(_sliding_forwarding != nullptr, \"expect sliding forwarding initialized\");\n+    _sliding_forwarding->begin();\n+  }\n+#endif\n+}\n+\n+void GCForwarding::end() {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(_sliding_forwarding != nullptr, \"expect sliding forwarding initialized\");\n+    _sliding_forwarding->end();\n+  }\n+#endif\n+}\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.cpp","additions":57,"deletions":0,"binary":false,"changes":57,"status":"added"},{"patch":"@@ -0,0 +1,49 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_GCFORWARDING_HPP\n+#define SHARE_GC_SHARED_GCFORWARDING_HPP\n+\n+#include \"memory\/allStatic.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+class SlidingForwarding;\n+\n+class GCForwarding : public AllStatic {\n+private:\n+  static SlidingForwarding* _sliding_forwarding;\n+\n+public:\n+  static void initialize(MemRegion heap, size_t region_size_words_shift);\n+  static void begin();\n+  static void end();\n+\n+  static inline bool is_forwarded(oop obj);\n+  static inline bool is_not_forwarded(oop obj);\n+  static inline oop forwardee(oop obj);\n+  static inline void forward_to(oop obj, oop fwd);\n+};\n+\n+#endif \/\/ SHARE_GC_SHARED_GCFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.hpp","additions":49,"deletions":0,"binary":false,"changes":49,"status":"added"},{"patch":"@@ -0,0 +1,61 @@\n+\/*\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_GCFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_GCFORWARDING_INLINE_HPP\n+\n+#include \"gc\/shared\/gcForwarding.hpp\"\n+#include \"gc\/shared\/slidingForwarding.inline.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+\n+inline bool GCForwarding::is_forwarded(oop obj) {\n+  return obj->is_forwarded();\n+}\n+\n+inline bool GCForwarding::is_not_forwarded(oop obj) {\n+  return !obj->is_forwarded();\n+}\n+\n+inline oop GCForwarding::forwardee(oop obj) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(_sliding_forwarding != nullptr, \"expect sliding forwarding initialized\");\n+    return _sliding_forwarding->forwardee(obj);\n+  } else\n+#endif\n+    return obj->forwardee();\n+}\n+\n+inline void GCForwarding::forward_to(oop obj, oop fwd) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(_sliding_forwarding != nullptr, \"expect sliding forwarding initialized\");\n+    _sliding_forwarding->forward_to(obj, fwd);\n+    assert(forwardee(obj) == fwd, \"must be forwarded to correct forwardee\");\n+  } else\n+#endif\n+    obj->forward_to(fwd);\n+}\n+\n+#endif \/\/ SHARE_GC_SHARED_GCFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.inline.hpp","additions":61,"deletions":0,"binary":false,"changes":61,"status":"added"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -135,0 +136,2 @@\n+  GCForwarding::initialize(_reserved, SpaceAlignment);\n+\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":3,"deletions":0,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -0,0 +1,150 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/shared\/gc_globals.hpp\"\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n+\n+#ifdef _LP64\n+\n+HeapWord* const SlidingForwarding::UNUSED_BASE = reinterpret_cast<HeapWord*>(0x1);\n+\n+SlidingForwarding::SlidingForwarding(MemRegion heap, size_t region_size_words)\n+  : _heap_start(heap.start()),\n+    _num_regions(((heap.end() - heap.start()) \/ region_size_words) + 1),\n+    _region_size_words_shift(log2i_exact(region_size_words)),\n+  _target_base_table(nullptr),\n+  _fallback_table(nullptr) {\n+  assert(_region_size_words_shift <= NUM_COMPRESSED_BITS, \"regions must not be larger than maximum addressing bits allow\");\n+  size_t heap_size_words = heap.end() - heap.start();\n+  if (UseSerialGC && heap_size_words <= (1 << NUM_COMPRESSED_BITS)) {\n+    \/\/ In this case we can treat the whole heap as a single region and\n+    \/\/ make the encoding very simple.\n+    _num_regions = 1;\n+    _region_size_words_shift = log2i_exact(round_up_power_of_2(heap_size_words));\n+  }\n+}\n+\n+SlidingForwarding::~SlidingForwarding() {\n+  if (_target_base_table != nullptr) {\n+    FREE_C_HEAP_ARRAY(HeapWord*, _target_base_table);\n+  }\n+  if (_fallback_table != nullptr) {\n+    delete _fallback_table;\n+  }\n+}\n+\n+void SlidingForwarding::begin() {\n+  assert(_target_base_table == nullptr, \"Should be uninitialized\");\n+  _target_base_table = NEW_C_HEAP_ARRAY(HeapWord*, _num_regions * NUM_TARGET_REGIONS, mtGC);\n+  size_t max = _num_regions * NUM_TARGET_REGIONS;\n+  for (size_t i = 0; i < max; i++) {\n+    _target_base_table[i] = UNUSED_BASE;\n+  }\n+}\n+\n+void SlidingForwarding::end() {\n+  assert(_target_base_table != nullptr, \"Should be initialized\");\n+  FREE_C_HEAP_ARRAY(HeapWord*, _target_base_table);\n+  _target_base_table = nullptr;\n+\n+  if (_fallback_table != nullptr) {\n+    delete _fallback_table;\n+    _fallback_table = nullptr;\n+  }\n+}\n+\n+void SlidingForwarding::fallback_forward_to(HeapWord* from, HeapWord* to) {\n+  if (_fallback_table == nullptr) {\n+    _fallback_table = new FallbackTable();\n+  }\n+  _fallback_table->forward_to(from, to);\n+}\n+\n+HeapWord* SlidingForwarding::fallback_forwardee(HeapWord* from) const {\n+  if (_fallback_table == nullptr) {\n+    return nullptr;\n+  } else {\n+    return _fallback_table->forwardee(from);\n+  }\n+}\n+\n+FallbackTable::FallbackTable() {\n+  for (size_t i = 0; i < TABLE_SIZE; i++) {\n+    _table[i]._next = nullptr;\n+    _table[i]._from = nullptr;\n+    _table[i]._to   = nullptr;\n+  }\n+}\n+\n+FallbackTable::~FallbackTable() {\n+  for (size_t i = 0; i < TABLE_SIZE; i++) {\n+    FallbackTableEntry* entry = _table[i]._next;\n+    while (entry != nullptr) {\n+      FallbackTableEntry* next = entry->_next;\n+      FREE_C_HEAP_OBJ(entry);\n+      entry = next;\n+    }\n+  }\n+}\n+\n+size_t FallbackTable::home_index(HeapWord* from) {\n+  uint64_t val = reinterpret_cast<uint64_t>(from);\n+  val *= 0xbf58476d1ce4e5b9ull;\n+  val ^= val >> 56;\n+  val *= 0x94d049bb133111ebull;\n+  val = (val * 11400714819323198485llu) >> (64 - log2i_exact(TABLE_SIZE));\n+  assert(val < TABLE_SIZE, \"must fit in table: val: \" UINT64_FORMAT \", table-size: \" UINTX_FORMAT \", table-size-bits: %d\", val, TABLE_SIZE, log2i_exact(TABLE_SIZE));\n+  return static_cast<size_t>(val);\n+}\n+\n+void FallbackTable::forward_to(HeapWord* from, HeapWord* to) {\n+  size_t idx = home_index(from);\n+  if (_table[idx]._from != nullptr) {\n+    FallbackTableEntry* entry = NEW_C_HEAP_OBJ(FallbackTableEntry, mtGC);\n+    entry->_next = _table[idx]._next;\n+    entry->_from = _table[idx]._from;\n+    entry->_to = _table[idx]._to;\n+    _table[idx]._next = entry;\n+  }\n+  _table[idx]._from = from;\n+  _table[idx]._to   = to;\n+}\n+\n+HeapWord* FallbackTable::forwardee(HeapWord* from) const {\n+  size_t idx = home_index(from);\n+  const FallbackTableEntry* entry = &_table[idx];\n+  while (entry != nullptr) {\n+    if (entry->_from == from) {\n+      return entry->_to;\n+    }\n+    entry = entry->_next;\n+  }\n+  return nullptr;\n+}\n+\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.cpp","additions":150,"deletions":0,"binary":false,"changes":150,"status":"added"},{"patch":"@@ -0,0 +1,167 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n+\n+#ifdef _LP64\n+\n+#include \"memory\/allocation.hpp\"\n+#include \"memory\/memRegion.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oopsHierarchy.hpp\"\n+\n+class FallbackTable;\n+\n+\/**\n+ * SlidingForwarding is a method to store forwarding information in a compressed form into the object header,\n+ * that has been specifically designed for sliding compaction GCs.\n+ * It avoids overriding the compressed class pointer in the upper bits of the header, which would otherwise\n+ * be lost. SlidingForwarding requires only small side tables and guarantees constant-time access and modification.\n+ *\n+ * The idea is to use a pointer compression scheme very similar to the one that is used for compressed oops.\n+ * We divide the heap into number of logical regions. Each region spans maximum of 2^NUM_COMPRESSED_BITS words.\n+ * We take advantage of the fact that sliding compaction can forward objects from one region to a maximum of\n+ * two regions (including itself, but that does not really matter). We need 1 bit to indicate which region is forwarded\n+ * into. We also currently require the two lowest header bits to indicate that the object is forwarded. In addition to that,\n+ * we use 1 more bit to indicate that we should use a fallback-lookup-table instead of using the sliding encoding.\n+ *\n+ * For addressing, we need a table with N*2 entries, for N logical regions. For each region, it gives the base\n+ * address of the two target regions, or a special placeholder if not used.\n+ *\n+ * Adding a forwarding then works as follows:\n+ * Given an original address 'orig', and a 'target' address:\n+ * - Look-up first target base of region of orig. If it is already established and the region\n+ *   that 'target' is in, then use it in step 3. If not yet used, establish it to be the base of region of target\n+     address. Use that base in step 3.\n+ * - Else, if first target base is already used, check second target base. This must either be unused, or the\n+ *   base of the region of our target address. If unused, establish it to be the base of the region of our target\n+ *   address. Use that base for next step.\n+ * - Now we found a base address. Encode the target address with that base into lowest NUM_COMPRESSED_BITS bits, and shift\n+ *   that up by 4 bits. Set the 3rd bit if we used the secondary target base, otherwise leave it at 0. Set the\n+ *   lowest two bits to indicate that the object has been forwarded. Store that in the lowest 32 bits of the\n+ *   original object's header.\n+ *\n+ * Similarily, looking up the target address, given an original object address works as follows:\n+ * - Load lowest 32 from original object header. Extract target region bit and compressed address bits.\n+ * - Depending on target region bit, load base address from the target base table by looking up the corresponding entry\n+ *   for the region of the original object.\n+ * - Decode the target address by using the target base address and the compressed address bits.\n+ *\n+ * One complication is that G1 serial compaction breaks the assumption that we only forward\n+ * to two target regions. When that happens, we initialize a fallback-hashtable for storing those extra\n+ * forwardings, and set the 4th bit in the header to indicate that the forwardee is not encoded but\n+ * should be looked-up in the hashtable. G1 serial compaction is not very common -  it is the last-last-ditch\n+ * GC that is used when the JVM is scrambling to squeeze more space out of the heap, and at that\n+ * point, ultimate performance is no longer the main concern.\n+ *\/\n+class SlidingForwarding : public CHeapObj<mtGC> {\n+private:\n+  static const uintptr_t MARK_LOWER_HALF_MASK = 0xffffffff;\n+\n+  \/\/ We need the lowest two bits to indicate a forwarded object.\n+  \/\/ The 3rd bit (fallback-bit) indicates that the forwardee should be\n+  \/\/ looked-up in a fallback-table.\n+  static const int FALLBACK_SHIFT = markWord::lock_bits;\n+  static const int FALLBACK_BITS = 1;\n+  static const int FALLBACK_MASK = right_n_bits(FALLBACK_BITS) << FALLBACK_SHIFT;\n+  \/\/ The 4th bit selects the target region.\n+  static const int REGION_SHIFT = FALLBACK_SHIFT + FALLBACK_BITS;\n+  static const int REGION_BITS = 1;\n+\n+  \/\/ The compressed address bits start here.\n+  static const int COMPRESSED_BITS_SHIFT = REGION_SHIFT + REGION_BITS;\n+\n+  \/\/ How many bits we use for the compressed pointer\n+  static const int NUM_COMPRESSED_BITS = 32 - COMPRESSED_BITS_SHIFT;\n+\n+  static const size_t NUM_TARGET_REGIONS = 1 << REGION_BITS;\n+\n+  \/\/ Indicates an usused base address in the target base table. We cannot use 0, because that may already be\n+  \/\/ a valid base address in zero-based heaps. 0x1 is safe because heap base addresses must be aligned by 2^X.\n+  static HeapWord* const UNUSED_BASE;\n+\n+  HeapWord*  const _heap_start;\n+  size_t           _num_regions;\n+  size_t           _region_size_words_shift;\n+  HeapWord**       _target_base_table;\n+\n+  FallbackTable* _fallback_table;\n+\n+  inline size_t region_index_containing(HeapWord* addr) const;\n+  inline bool region_contains(HeapWord* region_base, HeapWord* addr) const;\n+\n+  inline uintptr_t encode_forwarding(HeapWord* original, HeapWord* target);\n+  inline HeapWord* decode_forwarding(HeapWord* original, uintptr_t encoded) const;\n+\n+  void fallback_forward_to(HeapWord* from, HeapWord* to);\n+  HeapWord* fallback_forwardee(HeapWord* from) const;\n+\n+public:\n+  SlidingForwarding(MemRegion heap, size_t region_size_words);\n+  ~SlidingForwarding();\n+\n+  void begin();\n+  void end();\n+\n+  inline void forward_to(oop original, oop target);\n+  inline oop forwardee(oop original) const;\n+};\n+\n+\/*\n+ * A simple hash-table that acts as fallback for the sliding forwarding.\n+ * This is used in the case of G1 serial compactio, which violates the\n+ * assumption of sliding forwarding that each object of any region is only\n+ * ever forwarded to one of two target regions. At this point, the GC is\n+ * scrambling to free up more Java heap memory, and therefore performance\n+ * is not the major concern.\n+ *\n+ * The implementation is a straightforward open hashtable.\n+ * It is a single-threaded (not thread-safe) implementation, and that\n+ * is sufficient because G1 serial compaction is single-threaded.\n+ *\/\n+class FallbackTable : public CHeapObj<mtGC>{\n+private:\n+  struct FallbackTableEntry {\n+    FallbackTableEntry* _next;\n+    HeapWord* _from;\n+    HeapWord* _to;\n+  };\n+\n+  static const size_t TABLE_SIZE = 128;\n+  FallbackTableEntry _table[TABLE_SIZE];\n+\n+  static size_t home_index(HeapWord* from);\n+\n+public:\n+  FallbackTable();\n+  ~FallbackTable();\n+\n+  void forward_to(HeapWord* from, HeapWord* to);\n+  HeapWord* forwardee(HeapWord* from) const;\n+};\n+\n+#endif \/\/ _LP64\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.hpp","additions":167,"deletions":0,"binary":false,"changes":167,"status":"added"},{"patch":"@@ -0,0 +1,118 @@\n+\/*\n+ * Copyright (c) 2021, Red Hat, Inc. All rights reserved.\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#ifndef SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+#define SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n+\n+#ifdef _LP64\n+\n+#include \"gc\/shared\/slidingForwarding.hpp\"\n+#include \"oops\/markWord.hpp\"\n+#include \"oops\/oop.inline.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+size_t SlidingForwarding::region_index_containing(HeapWord* addr) const {\n+  assert(addr >= _heap_start, \"sanity: addr: \" PTR_FORMAT \" heap base: \" PTR_FORMAT, p2i(addr), p2i(_heap_start));\n+  size_t index = ((size_t) (addr - _heap_start)) >> _region_size_words_shift;\n+  assert(index < _num_regions, \"Region index is in bounds: \" PTR_FORMAT, p2i(addr));\n+  return index;\n+}\n+\n+bool SlidingForwarding::region_contains(HeapWord* region_base, HeapWord* addr) const {\n+  return uintptr_t(addr - region_base) < (1ull << _region_size_words_shift);\n+}\n+\n+\n+uintptr_t SlidingForwarding::encode_forwarding(HeapWord* original, HeapWord* target) {\n+  size_t orig_idx = region_index_containing(original);\n+  size_t base_table_idx = orig_idx * 2;\n+  size_t target_idx = region_index_containing(target);\n+  HeapWord* encode_base;\n+  uintptr_t region_idx;\n+  for (region_idx = 0; region_idx < NUM_TARGET_REGIONS; region_idx++) {\n+    encode_base = _target_base_table[base_table_idx + region_idx];\n+    if (encode_base == UNUSED_BASE) {\n+      encode_base = _heap_start + target_idx * (1ull << _region_size_words_shift);\n+      _target_base_table[base_table_idx + region_idx] = encode_base;\n+      break;\n+    } else if (region_contains(encode_base, target)) {\n+      break;\n+    }\n+  }\n+  if (region_idx >= NUM_TARGET_REGIONS) {\n+    assert(G1GC_ONLY(UseG1GC) NOT_G1GC(false), \"Only happens with G1 serial compaction\");\n+    return 1 << FALLBACK_SHIFT | markWord::marked_value;\n+  }\n+  assert(region_idx < NUM_TARGET_REGIONS, \"need to have found an encoding base\");\n+  assert(target >= encode_base, \"target must be above encode base, target:\" PTR_FORMAT \", encoded_base: \" PTR_FORMAT \",  target_idx: \" SIZE_FORMAT \", heap start: \" PTR_FORMAT \", region_idx: \" INTPTR_FORMAT,\n+         p2i(target), p2i(encode_base), target_idx, p2i(_heap_start), region_idx);\n+  assert(region_contains(encode_base, target), \"region must contain target: original: \" PTR_FORMAT \", target: \" PTR_FORMAT \", encode_base: \" PTR_FORMAT \", region_idx: \" INTPTR_FORMAT, p2i(original), p2i(target), p2i(encode_base), region_idx);\n+  uintptr_t encoded = (((uintptr_t)(target - encode_base)) << COMPRESSED_BITS_SHIFT) |\n+                      (region_idx << REGION_SHIFT) | markWord::marked_value;\n+  assert(target == decode_forwarding(original, encoded), \"must be reversible\");\n+  return encoded;\n+}\n+\n+HeapWord* SlidingForwarding::decode_forwarding(HeapWord* original, uintptr_t encoded) const {\n+  assert((encoded & markWord::marked_value) == markWord::marked_value, \"must be marked as forwarded\");\n+  size_t orig_idx = region_index_containing(original);\n+  size_t region_idx = (encoded >> REGION_SHIFT) & right_n_bits(REGION_BITS);\n+  size_t base_table_idx = orig_idx * 2 + region_idx;\n+  HeapWord* decoded = _target_base_table[base_table_idx] + (encoded >> COMPRESSED_BITS_SHIFT);\n+  assert(decoded >= _heap_start, \"must be above heap start, encoded: \" INTPTR_FORMAT \", region_idx: \" SIZE_FORMAT \", base: \" PTR_FORMAT, encoded, region_idx, p2i(_target_base_table[base_table_idx]));\n+  return decoded;\n+}\n+\n+void SlidingForwarding::forward_to(oop original, oop target) {\n+  assert(_target_base_table != nullptr, \"call begin() before forwarding\");\n+  markWord header = original->mark();\n+  if (header.has_displaced_mark_helper()) {\n+    header = header.displaced_mark_helper();\n+  }\n+  HeapWord* from = cast_from_oop<HeapWord*>(original);\n+  HeapWord* to   = cast_from_oop<HeapWord*>(target);\n+  uintptr_t encoded = encode_forwarding(from, to);\n+  header = markWord((header.value() & ~MARK_LOWER_HALF_MASK) | encoded);\n+  original->set_mark(header);\n+  if ((encoded & FALLBACK_MASK) != 0) {\n+    fallback_forward_to(from, to);\n+    return;\n+  }\n+}\n+\n+oop SlidingForwarding::forwardee(oop original) const {\n+  assert(_target_base_table != nullptr, \"call begin() before forwarding\");\n+  markWord header = original->mark();\n+  if ((header.value() & FALLBACK_MASK) != 0) {\n+    HeapWord* from = cast_from_oop<HeapWord*>(original);\n+    HeapWord* to = fallback_forwardee(from);\n+    return cast_to_oop(to);\n+  }\n+  uintptr_t encoded = header.value() & MARK_LOWER_HALF_MASK;\n+  HeapWord* forwardee = decode_forwarding(cast_from_oop<HeapWord*>(original), encoded);\n+  return cast_to_oop(forwardee);\n+}\n+\n+#endif \/\/ _LP64\n+#endif \/\/ SHARE_GC_SHARED_SLIDINGFORWARDING_INLINE_HPP\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.inline.hpp","additions":118,"deletions":0,"binary":false,"changes":118,"status":"added"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/gcForwarding.inline.hpp\"\n@@ -272,1 +273,1 @@\n-    q->forward_to(cast_to_oop(compact_top));\n+    GCForwarding::forward_to(q, cast_to_oop(compact_top));\n@@ -278,1 +279,1 @@\n-    assert(!q->is_forwarded(), \"should not be forwarded\");\n+    assert(GCForwarding::is_not_forwarded(q), \"should not be forwarded\");\n@@ -438,1 +439,1 @@\n-    if (!cast_to_oop(cur_obj)->is_forwarded()) {\n+    if (GCForwarding::is_not_forwarded(cast_to_oop(cur_obj))) {\n@@ -449,1 +450,1 @@\n-      HeapWord* compaction_top = cast_from_oop<HeapWord*>(cast_to_oop(cur_obj)->forwardee());\n+      HeapWord* compaction_top = cast_from_oop<HeapWord*>(GCForwarding::forwardee(cast_to_oop(cur_obj)));\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"gc\/shared\/gcForwarding.hpp\"\n@@ -407,0 +408,2 @@\n+  GCForwarding::initialize(_heap_region, ShenandoahHeapRegion::region_size_words_shift());\n+\n@@ -953,1 +956,1 @@\n-    if (!p->is_forwarded()) {\n+    if (!ShenandoahForwarding::is_forwarded(p)) {\n@@ -1298,0 +1301,1 @@\n+    shenandoah_assert_not_in_cset_except(NULL, obj, cancelled_gc());\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"gc\/shenandoah\/shenandoahObjectUtils.inline.hpp\"\n@@ -287,1 +288,1 @@\n-  size_t size = p->size();\n+  size_t size = ShenandoahObjectUtils::size(p);\n@@ -325,2 +326,7 @@\n-  ContinuationGCSupport::relativize_stack_chunk(copy_val);\n-\n+  if (!copy_val->mark().is_marked()) {\n+    \/\/ If we copied a mark-word that indicates 'forwarded' state, then\n+    \/\/ another thread beat us, and this new copy will never be published.\n+    \/\/ ContinuationGCSupport would get a corrupt Klass* in that case,\n+    \/\/ so don't even attempt it.\n+    ContinuationGCSupport::relativize_stack_chunk(copy_val);\n+  }\n@@ -506,1 +512,1 @@\n-    size_t size = obj->size();\n+    size_t size = ShenandoahObjectUtils::size(obj);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -738,0 +738,9 @@\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    \/\/ This is a hack to get around the limitation of registers in x86_32. We really\n+    \/\/ send an oopDesc* instead of a BasicObjectLock*.\n+    Handle h_obj(current, oop((reinterpret_cast<oopDesc*>(elem))));\n+    assert(Universe::heap()->is_in_or_null(h_obj()),\n+           \"must be NULL or an object\");\n+    ObjectSynchronizer::enter(h_obj, NULL, current);\n+    return;\n+  }\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2002,0 +2002,3 @@\n+#ifdef _LP64\n+              oopDesc::release_set_mark(result, ik->prototype_header());\n+#else\n@@ -2003,2 +2006,1 @@\n-              oopDesc::set_klass_gap(result, 0);\n-\n+#endif\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2257,1 +2257,1 @@\n-  return arrayOopDesc::header_size(type) * HeapWordSize;\n+  return arrayOopDesc::base_offset_in_bytes(type);\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -272,1 +272,0 @@\n-  volatile_nonstatic_field(oopDesc,            _metadata._klass,                              Klass*)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -53,0 +53,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -54,0 +55,1 @@\n+#include \"utilities\/align.hpp\"\n@@ -195,1 +197,13 @@\n-  return Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  MetaWord* p = Metaspace::allocate(loader_data, word_size, MetaspaceObj::ClassType, THREAD);\n+  assert(is_aligned(p, KlassAlignmentInBytes), \"metaspace returned badly aligned memory.\");\n+  return p;\n+}\n+\n+static markWord make_prototype(Klass* kls) {\n+  markWord prototype = markWord::prototype();\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    prototype = prototype.set_klass(kls);\n+  }\n+#endif\n+  return prototype;\n@@ -203,0 +217,1 @@\n+                           _prototype_header(make_prototype(this)),\n@@ -747,0 +762,4 @@\n+     if (UseCompactObjectHeaders) {\n+       st->print(BULLET\"prototype_header: \" INTPTR_FORMAT, _prototype_header.value());\n+       st->cr();\n+     }\n@@ -770,0 +789,4 @@\n+  if (UseCompressedClassPointers) {\n+    assert(is_aligned(this, KlassAlignmentInBytes), \"misaligned Klass structure\");\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":24,"deletions":1,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -168,0 +168,2 @@\n+  markWord _prototype_header;   \/\/ Used to initialize objects' header\n+\n@@ -673,0 +675,7 @@\n+  markWord prototype_header() const      {\n+    assert(UseCompactObjectHeaders, \"only use with compact object headers\");\n+    return _prototype_header;\n+  }\n+  inline void set_prototype_header(markWord header);\n+  static ByteSize prototype_header_offset() { return in_ByteSize(offset_of(Klass, _prototype_header)); }\n+\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":9,"deletions":0,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -33,0 +33,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,0 +38,1 @@\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n@@ -125,1 +127,1 @@\n-  if (ignore_mark_word) {\n+  if (ignore_mark_word || UseFastLocking) {\n@@ -158,2 +160,3 @@\n-  \/\/ Only has a klass gap when compressed class pointers are used.\n-  return UseCompressedClassPointers;\n+  \/\/ Only has a klass gap when compressed class pointers are used, but\n+  \/\/ only if not using compact headers..\n+  return UseCompressedClassPointers && !UseCompactObjectHeaders;\n@@ -166,1 +169,5 @@\n-  _metadata._compressed_klass = nk;\n+  if (UseCompactObjectHeaders) {\n+    set_mark(mark().set_narrow_klass(nk));\n+  } else {\n+    _metadata._compressed_klass = nk;\n+  }\n@@ -171,7 +178,8 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass narrow_klass = obj->_metadata._compressed_klass;\n-    if (narrow_klass == 0) return nullptr;\n-    return (void*)CompressedKlassPointers::decode_raw(narrow_klass);\n-  } else {\n-    return obj->_metadata._klass;\n-  }\n+  \/\/ TODO: Remove method altogether and replace with calls to obj->klass() ?\n+  \/\/ OTOH, we may eventually get rid of locking in header, and then no\n+  \/\/ longer have to deal with that anymore.\n+#ifdef _LP64\n+  return obj->klass();\n+#else\n+  return obj->_metadata._klass;\n+#endif\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":19,"deletions":11,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n@@ -78,0 +79,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -81,0 +83,2 @@\n+  inline markWord resolve_mark() const;\n+\n@@ -99,1 +103,8 @@\n-  static constexpr int header_size() { return sizeof(oopDesc)\/HeapWordSize; }\n+  static int header_size() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      return sizeof(markWord) \/ HeapWordSize;\n+    } else\n+#endif\n+    return sizeof(oopDesc)\/HeapWordSize;\n+  }\n@@ -260,0 +271,1 @@\n+  inline void forward_to_self();\n@@ -267,0 +279,1 @@\n+  inline oop forward_to_self_atomic(markWord compare, atomic_memory_order order = memory_order_conservative);\n@@ -269,0 +282,1 @@\n+  inline oop forwardee(markWord header) const;\n@@ -312,1 +326,0 @@\n-  static int klass_offset_in_bytes()     { return offset_of(oopDesc, _metadata._klass); }\n@@ -315,0 +328,1 @@\n+    assert(!UseCompactObjectHeaders, \"don't use klass_offset_in_bytes() with compact headers\");\n@@ -318,0 +332,24 @@\n+  static int klass_offset_in_bytes()     {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      STATIC_ASSERT(markWord::klass_shift % 8 == 0);\n+      return mark_offset_in_bytes() + markWord::klass_shift \/ 8;\n+    } else\n+#endif\n+    return offset_of(oopDesc, _metadata._klass);\n+  }\n+\n+  static int base_offset_in_bytes() {\n+#ifdef _LP64\n+    if (UseCompactObjectHeaders) {\n+      \/\/ With compact headers, the Klass* field is not used for the Klass*\n+      \/\/ and is used for the object fields instead.\n+      assert(sizeof(markWord) == 8, \"sanity\");\n+      return sizeof(markWord);\n+    } else if (UseCompressedClassPointers) {\n+      return sizeof(markWord) + sizeof(narrowKlass);\n+    } else\n+#endif\n+    return sizeof(oopDesc);\n+  }\n+\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":40,"deletions":2,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"oops\/compressedKlass.inline.hpp\"\n@@ -37,1 +38,1 @@\n-#include \"oops\/markWord.hpp\"\n+#include \"oops\/markWord.inline.hpp\"\n@@ -41,0 +42,3 @@\n+#include \"runtime\/objectMonitor.inline.hpp\"\n+#include \"runtime\/safepoint.hpp\"\n+#include \"runtime\/synchronizer.hpp\"\n@@ -73,0 +77,4 @@\n+void oopDesc::release_set_mark(HeapWord* mem, markWord m) {\n+  Atomic::release_store((markWord*)(((char*)mem) + mark_offset_in_bytes()), m);\n+}\n+\n@@ -81,0 +89,10 @@\n+markWord oopDesc::resolve_mark() const {\n+  assert(UseFastLocking, \"Only safe with fast-locking\");\n+  markWord hdr = mark();\n+  if (hdr.has_monitor()) {\n+    ObjectMonitor* monitor = hdr.monitor();\n+    return monitor->header();\n+  }\n+  return hdr;\n+}\n+\n@@ -82,0 +100,7 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    markWord header = resolve_mark();\n+    assert(UseCompressedClassPointers, \"expect compressed klass pointers\");\n+    set_mark(markWord((header.value() & markWord::klass_mask_in_place) | markWord::prototype().value()));\n+  } else\n+#endif\n@@ -86,1 +111,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass();\n+  } else if (UseCompressedClassPointers) {\n@@ -88,3 +118,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -94,1 +124,6 @@\n-  if (UseCompressedClassPointers) {\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = resolve_mark();\n+    return header.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n@@ -96,3 +131,3 @@\n-  } else {\n-    return _metadata._klass;\n-  }\n+  } else\n+#endif\n+  return _metadata._klass;\n@@ -102,6 +137,14 @@\n-  if (UseCompressedClassPointers) {\n-    narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n-    return CompressedKlassPointers::decode(nklass);\n-  } else {\n-    return Atomic::load_acquire(&_metadata._klass);\n-  }\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders) {\n+    assert(UseCompressedClassPointers, \"only with compressed class pointers\");\n+    markWord header = mark_acquire();\n+    if (header.has_monitor()) {\n+      header = header.monitor()->header();\n+    }\n+    return header.klass_or_null();\n+  } else if (UseCompressedClassPointers) {\n+     narrowKlass nklass = Atomic::load_acquire(&_metadata._compressed_klass);\n+     return CompressedKlassPointers::decode(nklass);\n+  } else\n+#endif\n+  return Atomic::load_acquire(&_metadata._klass);\n@@ -111,5 +154,1 @@\n-  if (UseCompressedClassPointers) {\n-    return CompressedKlassPointers::decode_raw(_metadata._compressed_klass);\n-  } else {\n-    return _metadata._klass;\n-  }\n+  return klass();\n@@ -119,1 +158,2 @@\n-  assert(Universe::is_bootstrapping() || (k != nullptr && k->is_klass()), \"incorrect Klass\");\n+  assert(Universe::is_bootstrapping() || (k != NULL && k->is_klass()), \"incorrect Klass\");\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -128,1 +168,2 @@\n-  assert(Universe::is_bootstrapping() || (k != nullptr && k->is_klass()), \"incorrect Klass\");\n+  assert(Universe::is_bootstrapping() || (k != NULL && k->is_klass()), \"incorrect Klass\");\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* with compact headers\");\n@@ -139,0 +180,1 @@\n+  assert(!UseCompactObjectHeaders, \"don't set Klass* gap with compact headers\");\n@@ -269,1 +311,14 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n+  set_mark(m);\n+}\n+\n+void oopDesc::forward_to_self() {\n+#ifdef _LP64\n+  markWord m = mark();\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n@@ -272,0 +327,3 @@\n+#else\n+  forward_to(oop(this));\n+#endif\n@@ -275,1 +333,1 @@\n-  assert(m.decode_pointer() == p, \"encoding must be reversible\");\n+  assert(forwardee(m) == p, \"encoding must be reversable\");\n@@ -281,1 +339,1 @@\n-    return cast_to_oop(old_mark.decode_pointer());\n+    return forwardee(old_mark);\n@@ -285,0 +343,22 @@\n+oop oopDesc::forward_to_self_atomic(markWord compare, atomic_memory_order order) {\n+#ifdef _LP64\n+  markWord m = compare;\n+  \/\/ If mark is displaced, we need to preserve the Klass* from real header.\n+  assert(SafepointSynchronize::is_at_safepoint(), \"we can only safely fetch the displaced header at safepoint\");\n+  if (m.has_displaced_mark_helper()) {\n+    m = m.displaced_mark_helper();\n+  }\n+  m = m.set_self_forwarded();\n+  assert(forwardee(m) == cast_to_oop(this), \"encoding must be reversable\");\n+  markWord old_mark = cas_set_mark(m, compare, order);\n+  if (old_mark == compare) {\n+    return NULL;\n+  } else {\n+    assert(old_mark.is_marked(), \"must be marked here\");\n+    return forwardee(old_mark);\n+  }\n+#else\n+  return forward_to_atomic(oop(this), compare, order);\n+#endif\n+}\n+\n@@ -289,2 +369,14 @@\n-  assert(is_forwarded(), \"only decode when actually forwarded\");\n-  return cast_to_oop(mark().decode_pointer());\n+  return forwardee(mark());\n+}\n+\n+oop oopDesc::forwardee(markWord header) const {\n+  assert(header.is_marked(), \"must be forwarded\");\n+#ifdef _LP64\n+  if (header.self_forwarded()) {\n+    return cast_to_oop(this);\n+  } else\n+#endif\n+  {\n+    assert(header.is_marked(), \"only decode when actually forwarded\");\n+    return cast_to_oop(header.decode_pointer());\n+  }\n@@ -345,1 +437,0 @@\n-  assert(k == klass(), \"wrong klass\");\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":119,"deletions":28,"binary":false,"changes":147,"status":"modified"},{"patch":"@@ -1575,3 +1575,3 @@\n-  Node* mark_node = nullptr;\n-  \/\/ For now only enable fast locking for non-array types\n-  mark_node = phase->MakeConX(markWord::prototype().value());\n+  Node* klass_node = in(AllocateNode::KlassNode);\n+  Node* proto_adr = phase->transform(new AddPNode(klass_node, klass_node, phase->MakeConX(in_bytes(Klass::prototype_header_offset()))));\n+  Node* mark_node = LoadNode::make(*phase, control, mem, proto_adr, TypeRawPtr::BOTTOM, TypeX_X, TypeX_X->basic_type(), MemNode::unordered);\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1031,0 +1031,1 @@\n+  reset_max_monitors();\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -340,0 +340,1 @@\n+  uint                  _max_monitors;          \/\/ Keep track of maximum number of active monitors in this compilation\n@@ -634,0 +635,4 @@\n+  void          push_monitor() { _max_monitors++; }\n+  void          reset_max_monitors() { _max_monitors = 0; }\n+  uint          max_monitors() { return _max_monitors; }\n+\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":5,"deletions":0,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -183,0 +183,1 @@\n+  C->push_monitor();\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -1664,1 +1664,4 @@\n-  rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  if (!UseCompactObjectHeaders) {\n+    rawmem = make_store(control, rawmem, object, oopDesc::klass_offset_in_bytes(), klass_node, T_METADATA);\n+  }\n+\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -428,0 +428,1 @@\n+    C->push_monitor();\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -321,1 +321,1 @@\n-    const size_t hs = arrayOopDesc::header_size(elem_type);\n+    const size_t hs_bytes = arrayOopDesc::base_offset_in_bytes(elem_type);\n@@ -323,1 +323,1 @@\n-    const size_t aligned_hs = align_object_offset(hs);\n+    const size_t aligned_hs_bytes = align_up(hs_bytes, BytesPerLong);\n@@ -325,2 +325,2 @@\n-    if (aligned_hs > hs) {\n-      Copy::zero_to_words(obj+hs, aligned_hs-hs);\n+    if (aligned_hs_bytes > hs_bytes) {\n+      Copy::zero_to_bytes(obj + hs_bytes, aligned_hs_bytes - hs_bytes);\n@@ -329,0 +329,1 @@\n+    const size_t aligned_hs = aligned_hs_bytes \/ HeapWordSize;\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -5153,1 +5153,2 @@\n-    int header_size = objArrayOopDesc::header_size() * wordSize;\n+    BasicType basic_elem_type = elem()->basic_type();\n+    int header_size = arrayOopDesc::base_offset_in_bytes(basic_elem_type);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -1491,0 +1491,15 @@\n+\n+  if (UseCompactObjectHeaders) {\n+    \/\/ 512 byte alignment, 22-bit values (Lilliput)\n+    LogKlassAlignmentInBytes = 9;\n+    MaxNarrowKlassPointerBits = 22;\n+  } else {\n+    \/\/ Traditional: 8 byte alignment, 32-bit values\n+    LogKlassAlignmentInBytes = 3;\n+    MaxNarrowKlassPointerBits = 32;\n+  }\n+\n+  KlassAlignmentInBytes = 1 << LogKlassAlignmentInBytes;\n+  NarrowKlassPointerBitMask = ((((uint64_t)1) << MaxNarrowKlassPointerBits) - 1);\n+  KlassEncodingMetaspaceMax = UCONST64(1) << (MaxNarrowKlassPointerBits + LogKlassAlignmentInBytes);\n+\n@@ -1516,1 +1531,10 @@\n-#endif \/\/ _LP64\n+\n+  \/\/ Assert validity of compressed class space size. User arg should have been checked at this point\n+  \/\/ (see CompressedClassSpaceSizeConstraintFunc()), so no need to be nice about it, this fires in\n+  \/\/ case the default is wrong.\n+  \/\/ TODO: This is placed wrong. The CompressedClassSpaceSizeFunc is done after ergo, but this\n+  \/\/ assert is during ergo.\n+  \/\/ assert(CompressedClassSpaceSize <= Metaspace::max_class_space_size(),\n+  \/\/        \"CompressedClassSpaceSize \" SIZE_FORMAT \" too large (max: \" SIZE_FORMAT \")\",\n+  \/\/        CompressedClassSpaceSize, Metaspace::max_class_space_size());\n+#endif\n@@ -3096,0 +3120,16 @@\n+#ifdef _LP64\n+  if (UseCompactObjectHeaders && FLAG_IS_CMDLINE(UseCompressedClassPointers) && !UseCompressedClassPointers) {\n+    \/\/ If user specifies -UseCompressedClassPointers, disable compact headers with a warning.\n+    warning(\"Compact object headers require compressed class pointers. Disabling compact object headers.\");\n+    FLAG_SET_DEFAULT(UseCompactObjectHeaders, false);\n+  }\n+\n+  if (UseCompactObjectHeaders && !UseFastLocking) {\n+    FLAG_SET_DEFAULT(UseFastLocking, true);\n+  }\n+\n+  if (!UseCompactObjectHeaders) {\n+    FLAG_SET_DEFAULT(UseSharedSpaces, false);\n+  }\n+#endif\n+\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":41,"deletions":1,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -128,1 +128,1 @@\n-  product(bool, UseCompressedClassPointers, false,                          \\\n+  product(bool, UseCompressedClassPointers, true,                           \\\n@@ -132,0 +132,3 @@\n+  product(bool, UseCompactObjectHeaders, true, EXPERIMENTAL,                \\\n+                \"Use 64-bit object headers instead of 96-bit headers\")      \\\n+                                                                            \\\n@@ -149,0 +152,1 @@\n+const bool UseCompactObjectHeaders = false;\n@@ -1062,1 +1066,1 @@\n-  develop(bool, UseHeavyMonitors, false,                                    \\\n+  product(bool, UseHeavyMonitors, false, DIAGNOSTIC,                        \\\n@@ -1419,1 +1423,1 @@\n-          range(1*M, 3*G)                                                   \\\n+          constraint(CompressedClassSpaceSizeConstraintFunc, AfterErgo)     \\\n@@ -1421,1 +1425,1 @@\n-  develop(size_t, CompressedClassSpaceBaseAddress, 0,                       \\\n+  product(size_t, CompressedClassSpaceBaseAddress, 0, DIAGNOSTIC,           \\\n@@ -1984,0 +1988,9 @@\n+                                                                            \\\n+  product(bool, HeapObjectStats, false, DIAGNOSTIC,                         \\\n+             \"Enable gathering of heap object statistics\")                  \\\n+                                                                            \\\n+  product(size_t, HeapObjectStatsSamplingInterval, 500, DIAGNOSTIC,         \\\n+             \"Heap object statistics sampling interval (ms)\")               \\\n+                                                                            \\\n+  product(bool, UseFastLocking, false, EXPERIMENTAL,                        \\\n+                \"Use fast-locking instead of stack-locking\")                \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":17,"deletions":4,"binary":false,"changes":21,"status":"modified"},{"patch":"@@ -73,0 +73,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -493,2 +494,3 @@\n-  _SleepEvent(ParkEvent::Allocate(this))\n-{\n+  _SleepEvent(ParkEvent::Allocate(this)),\n+\n+  _lock_stack() {\n@@ -997,0 +999,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n@@ -1390,0 +1393,4 @@\n+\n+  if (!UseHeavyMonitors && UseFastLocking) {\n+    lock_stack().oops_do(f);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":9,"deletions":2,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -37,0 +37,1 @@\n+#include \"runtime\/lockStack.hpp\"\n@@ -1150,0 +1151,10 @@\n+private:\n+  LockStack _lock_stack;\n+\n+public:\n+  LockStack& lock_stack() { return _lock_stack; }\n+\n+  static ByteSize lock_stack_current_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::current_offset(); }\n+  static ByteSize lock_stack_limit_offset()    { return byte_offset_of(JavaThread, _lock_stack) + LockStack::limit_offset(); }\n+\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -337,1 +337,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -603,0 +603,16 @@\n+\/\/ We might access the dead object headers for parsable heap walk, make sure\n+\/\/ headers are in correct shape, e.g. monitors deflated.\n+void ObjectMonitor::maybe_deflate_dead(oop* p) {\n+  oop obj = *p;\n+  assert(obj != NULL, \"must not yet been cleared\");\n+  markWord mark = obj->mark();\n+  if (mark.has_monitor()) {\n+    ObjectMonitor* monitor = mark.monitor();\n+    if (p == monitor->_object.ptr_raw()) {\n+      assert(monitor->object_peek() == obj, \"lock object must match\");\n+      markWord dmw = monitor->header();\n+      obj->set_mark(dmw);\n+    }\n+  }\n+}\n+\n@@ -1138,1 +1154,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1353,1 +1369,1 @@\n-    if (current->is_lock_owned((address)cur)) {\n+    if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n@@ -1388,0 +1404,1 @@\n+  assert(cur != ANONYMOUS_OWNER, \"no anon owner here\");\n@@ -1391,1 +1408,1 @@\n-  if (current->is_lock_owned((address)cur)) {\n+  if (!UseFastLocking && current->is_lock_owned((address)cur)) {\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":21,"deletions":4,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -147,2 +147,10 @@\n-  \/\/ Used by async deflation as a marker in the _owner field:\n-  #define DEFLATER_MARKER reinterpret_cast<void*>(-1)\n+  \/\/ Used by async deflation as a marker in the _owner field.\n+  \/\/ Note that the choice of the two markers is peculiar:\n+  \/\/ - They need to represent values that cannot be pointers. In particular,\n+  \/\/   we achieve this by using the lowest two bits\n+  \/\/ - ANONYMOUS_OWNER should be a small value, it is used in generated code\n+  \/\/   and small values encode much better\n+  \/\/ - We test for anonymous owner by testing for the lowest bit, therefore\n+  \/\/   DEFLATER_MARKER must *not* have that bit set.\n+  #define DEFLATER_MARKER reinterpret_cast<void*>(2)\n+  #define ANONYMOUS_OWNER reinterpret_cast<void*>(1)\n@@ -207,0 +215,1 @@\n+  static int header_offset_in_bytes()      { return offset_of(ObjectMonitor, _header); }\n@@ -266,0 +275,12 @@\n+  void set_owner_anonymous() {\n+    set_owner_from(NULL, ANONYMOUS_OWNER);\n+  }\n+\n+  bool is_owner_anonymous() const {\n+    return owner_raw() == ANONYMOUS_OWNER;\n+  }\n+\n+  void set_owner_from_anonymous(Thread* owner) {\n+    set_owner_from(ANONYMOUS_OWNER, owner);\n+  }\n+\n@@ -324,0 +345,2 @@\n+  static void maybe_deflate_dead(oop* p);\n+\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":25,"deletions":2,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -42,0 +42,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"gc\/shared\/suspendibleThreadSet.hpp\"\n@@ -42,0 +43,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -326,1 +328,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(oop(obj))) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -409,1 +412,3 @@\n-    lock->set_displaced_header(markWord::unused_mark());\n+    if (!UseFastLocking) {\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -498,6 +503,37 @@\n-    markWord mark = obj->mark();\n-    if (mark.is_neutral()) {\n-      \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n-      \/\/ be visible <= the ST performed by the CAS.\n-      lock->set_displaced_header(mark);\n-      if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+    if (UseFastLocking) {\n+      LockStack& lock_stack = current->lock_stack();\n+\n+      markWord header = obj()->mark_acquire();\n+      while (true) {\n+        if (header.is_neutral()) {\n+          assert(!lock_stack.contains(obj()), \"thread must not already hold the lock\");\n+          \/\/ Try to swing into 'fast-locked' state without inflating.\n+          markWord locked_header = header.set_fast_locked();\n+          markWord witness = obj()->cas_set_mark(locked_header, header);\n+          if (witness == header) {\n+            \/\/ Successfully fast-locked, push object to lock-stack and return.\n+            lock_stack.push(obj());\n+            return;\n+          }\n+          \/\/ Otherwise retry.\n+          header = witness;\n+        } else {\n+          \/\/ Fall-through to inflate-enter.\n+          break;\n+        }\n+      }\n+    } else {\n+      markWord mark = obj->mark();\n+      if (mark.is_neutral()) {\n+        \/\/ Anticipate successful CAS -- the ST of the displaced mark must\n+        \/\/ be visible <= the ST performed by the CAS.\n+        lock->set_displaced_header(mark);\n+        if (mark == obj()->cas_set_mark(markWord::from_pointer(lock), mark)) {\n+          return;\n+        }\n+        \/\/ Fall through to inflate() ...\n+      } else if (mark.has_locker() &&\n+                 current->is_lock_owned((address)mark.locker())) {\n+        assert(lock != mark.locker(), \"must not re-lock the same lock\");\n+        assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n+        lock->set_displaced_header(markWord::from_pointer(nullptr));\n@@ -506,13 +542,6 @@\n-      \/\/ Fall through to inflate() ...\n-    } else if (mark.has_locker() &&\n-               current->is_lock_owned((address)mark.locker())) {\n-      assert(lock != mark.locker(), \"must not re-lock the same lock\");\n-      assert(lock != (BasicLock*)obj->mark().value(), \"don't relock with same BasicLock\");\n-      lock->set_displaced_header(markWord::from_pointer(nullptr));\n-      return;\n-    }\n-    \/\/ The object header will never be displaced to this lock,\n-    \/\/ so it does not matter what the value is, except that it\n-    \/\/ must be non-zero to avoid looking like a re-entrant lock,\n-    \/\/ and must not look locked either.\n-    lock->set_displaced_header(markWord::unused_mark());\n+      \/\/ The object header will never be displaced to this lock,\n+      \/\/ so it does not matter what the value is, except that it\n+      \/\/ must be non-zero to avoid looking like a re-entrant lock,\n+      \/\/ and must not look locked either.\n+      lock->set_displaced_header(markWord::unused_mark());\n+    }\n@@ -521,1 +550,1 @@\n-    guarantee(!obj->mark().has_locker(), \"must not be stack-locked\");\n+    guarantee(!obj->mark().has_locker() && !obj->mark().is_fast_locked(), \"must not be stack-locked\");\n@@ -540,25 +569,12 @@\n-\n-    markWord dhw = lock->displaced_header();\n-    if (dhw.value() == 0) {\n-      \/\/ If the displaced header is null, then this exit matches up with\n-      \/\/ a recursive enter. No real work to do here except for diagnostics.\n-#ifndef PRODUCT\n-      if (mark != markWord::INFLATING()) {\n-        \/\/ Only do diagnostics if we are not racing an inflation. Simply\n-        \/\/ exiting a recursive enter of a Java Monitor that is being\n-        \/\/ inflated is safe; see the has_monitor() comment below.\n-        assert(!mark.is_neutral(), \"invariant\");\n-        assert(!mark.has_locker() ||\n-        current->is_lock_owned((address)mark.locker()), \"invariant\");\n-        if (mark.has_monitor()) {\n-          \/\/ The BasicLock's displaced_header is marked as a recursive\n-          \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n-          \/\/ This is a special case where the Java Monitor was inflated\n-          \/\/ after this thread entered the stack-lock recursively. When a\n-          \/\/ Java Monitor is inflated, we cannot safely walk the Java\n-          \/\/ Monitor owner's stack and update the BasicLocks because a\n-          \/\/ Java Monitor can be asynchronously inflated by a thread that\n-          \/\/ does not own the Java Monitor.\n-          ObjectMonitor* m = mark.monitor();\n-          assert(m->object()->mark() == mark, \"invariant\");\n-          assert(m->is_entered(current), \"invariant\");\n+    if (UseFastLocking) {\n+      if (mark.is_fast_locked()) {\n+        markWord unlocked_header = mark.set_unlocked();\n+        markWord witness = object->cas_set_mark(unlocked_header, mark);\n+        if (witness != mark) {\n+          \/\/ Another thread beat us, it can only have installed an anonymously locked monitor at this point.\n+          \/\/ Fetch that monitor, set owner correctly to this thread, and exit it (allowing waiting threads to enter).\n+          assert(witness.has_monitor(), \"must have monitor\");\n+          ObjectMonitor* monitor = witness.monitor();\n+          assert(monitor->is_owner_anonymous(), \"must be anonymous owner\");\n+          monitor->set_owner_from_anonymous(current);\n+          monitor->exit(current);\n@@ -566,0 +582,3 @@\n+        LockStack& lock_stack = current->lock_stack();\n+        lock_stack.remove(object);\n+        return;\n@@ -567,0 +586,27 @@\n+    } else {\n+      markWord dhw = lock->displaced_header();\n+      if (dhw.value() == 0) {\n+        \/\/ If the displaced header is null, then this exit matches up with\n+        \/\/ a recursive enter. No real work to do here except for diagnostics.\n+#ifndef PRODUCT\n+        if (mark != markWord::INFLATING()) {\n+          \/\/ Only do diagnostics if we are not racing an inflation. Simply\n+          \/\/ exiting a recursive enter of a Java Monitor that is being\n+          \/\/ inflated is safe; see the has_monitor() comment below.\n+          assert(!mark.is_neutral(), \"invariant\");\n+          assert(!mark.has_locker() ||\n+                 current->is_lock_owned((address)mark.locker()), \"invariant\");\n+          if (mark.has_monitor()) {\n+            \/\/ The BasicLock's displaced_header is marked as a recursive\n+            \/\/ enter and we have an inflated Java Monitor (ObjectMonitor).\n+            \/\/ This is a special case where the Java Monitor was inflated\n+            \/\/ after this thread entered the stack-lock recursively. When a\n+            \/\/ Java Monitor is inflated, we cannot safely walk the Java\n+            \/\/ Monitor owner's stack and update the BasicLocks because a\n+            \/\/ Java Monitor can be asynchronously inflated by a thread that\n+            \/\/ does not own the Java Monitor.\n+            ObjectMonitor* m = mark.monitor();\n+            assert(m->object()->mark() == mark, \"invariant\");\n+            assert(m->is_entered(current), \"invariant\");\n+          }\n+        }\n@@ -568,8 +614,0 @@\n-      return;\n-    }\n-\n-    if (mark == markWord::from_pointer(lock)) {\n-      \/\/ If the object is stack-locked by the current thread, try to\n-      \/\/ swing the displaced header from the BasicLock back to the mark.\n-      assert(dhw.is_neutral(), \"invariant\");\n-      if (object->cas_set_mark(dhw, mark) == mark) {\n@@ -578,0 +616,9 @@\n+\n+      if (mark == markWord::from_pointer(lock)) {\n+        \/\/ If the object is stack-locked by the current thread, try to\n+        \/\/ swing the displaced header from the BasicLock back to the mark.\n+        assert(dhw.is_neutral(), \"invariant\");\n+        if (object->cas_set_mark(dhw, mark) == mark) {\n+          return;\n+        }\n+      }\n@@ -587,0 +634,7 @@\n+  if (UseFastLocking && monitor->is_owner_anonymous()) {\n+    \/\/ It must be us. Pop lock object from lock stack.\n+    LockStack& lock_stack = current->lock_stack();\n+    oop popped = lock_stack.pop();\n+    assert(popped == object, \"must be owned by this thread\");\n+    monitor->set_owner_from_anonymous(current);\n+  }\n@@ -677,1 +731,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -692,1 +747,2 @@\n-  if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n+  if ((mark.is_fast_locked() && current->lock_stack().contains(obj())) ||\n+      (mark.has_locker() && current->is_lock_owned((address)mark.locker()))) {\n@@ -720,1 +776,1 @@\n-  if (!mark.is_being_inflated()) {\n+  if (!mark.is_being_inflated() || UseFastLocking) {\n@@ -835,0 +891,5 @@\n+static bool is_lock_owned(Thread* thread, oop obj) {\n+  assert(UseFastLocking, \"only call this with fast-locking enabled\");\n+  return thread->is_Java_thread() ? reinterpret_cast<JavaThread*>(thread)->lock_stack().contains(obj) : false;\n+}\n+\n@@ -889,1 +950,8 @@\n-    } else if (current->is_lock_owned((address)mark.locker())) {\n+    } else if (mark.is_fast_locked() && is_lock_owned(current, obj)) {\n+      \/\/ This is a fast lock owned by the calling thread so use the\n+      \/\/ markWord from the object.\n+      hash = mark.hash();\n+      if (hash != 0) {                  \/\/ if it has a hash, just return it\n+        return hash;\n+      }\n+    } else if (mark.has_locker() && current->is_lock_owned((address)mark.locker())) {\n@@ -958,0 +1026,6 @@\n+\n+  \/\/ Fast-locking case.\n+  if (mark.is_fast_locked()) {\n+    return current->lock_stack().contains(h_obj());\n+  }\n+\n@@ -972,2 +1046,0 @@\n-  address owner = nullptr;\n-\n@@ -978,1 +1050,5 @@\n-    owner = (address) mark.locker();\n+    return Threads::owning_thread_from_monitor_owner(t_list, (address) mark.locker());\n+  }\n+\n+  if (mark.is_fast_locked()) {\n+    return Threads::owning_thread_from_object(t_list, h_obj());\n@@ -982,1 +1058,1 @@\n-  else if (mark.has_monitor()) {\n+  if (mark.has_monitor()) {\n@@ -987,1 +1063,1 @@\n-    owner = (address) monitor->owner();\n+    return Threads::owning_thread_from_monitor(t_list, monitor);\n@@ -990,10 +1066,0 @@\n-  if (owner != nullptr) {\n-    \/\/ owning_thread_from_monitor_owner() may also return null here\n-    return Threads::owning_thread_from_monitor_owner(t_list, owner);\n-  }\n-\n-  \/\/ Unlocked case, header in place\n-  \/\/ Cannot have assertion since this object may have been\n-  \/\/ locked by another thread when reaching here.\n-  \/\/ assert(mark.is_neutral(), \"sanity check\");\n-\n@@ -1236,0 +1302,5 @@\n+      if (UseFastLocking && inf->is_owner_anonymous() && is_lock_owned(current, object)) {\n+        inf->set_owner_from_anonymous(current);\n+        assert(current->is_Java_thread(), \"must be Java thread\");\n+        reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+      }\n@@ -1245,1 +1316,4 @@\n-    if (mark == markWord::INFLATING()) {\n+    \/\/ NOTE: We need to check UseFastLocking here, because with fast-locking, the header\n+    \/\/ may legitimately be zero: cleared lock-bits and all upper header bits zero.\n+    \/\/ With fast-locking, the INFLATING protocol is not used.\n+    if (mark == markWord::INFLATING() && !UseFastLocking) {\n@@ -1261,0 +1335,42 @@\n+    if (mark.is_fast_locked()) {\n+      assert(UseFastLocking, \"can only happen with fast-locking\");\n+      ObjectMonitor* monitor = new ObjectMonitor(object);\n+      monitor->set_header(mark.set_unlocked());\n+      bool own = is_lock_owned(current, object);\n+      if (own) {\n+        \/\/ Owned by us.\n+        monitor->set_owner_from(nullptr, current);\n+      } else {\n+        \/\/ Owned by somebody else.\n+        monitor->set_owner_anonymous();\n+      }\n+      markWord monitor_mark = markWord::encode(monitor);\n+      markWord witness = object->cas_set_mark(monitor_mark, mark);\n+      if (witness == mark) {\n+        \/\/ Success! Return inflated monitor.\n+        if (own) {\n+          assert(current->is_Java_thread(), \"must be: checked in is_lock_owned()\");\n+          reinterpret_cast<JavaThread*>(current)->lock_stack().remove(object);\n+        }\n+        \/\/ Once the ObjectMonitor is configured and object is associated\n+        \/\/ with the ObjectMonitor, it is safe to allow async deflation:\n+        _in_use_list.add(monitor);\n+\n+        \/\/ Hopefully the performance counters are allocated on distinct\n+        \/\/ cache lines to avoid false sharing on MP systems ...\n+        OM_PERFDATA_OP(Inflations, inc());\n+        if (log_is_enabled(Trace, monitorinflation)) {\n+          ResourceMark rm(current);\n+          lsh.print_cr(\"inflate(has_locker): object=\" INTPTR_FORMAT \", mark=\"\n+                       INTPTR_FORMAT \", type='%s'\", p2i(object),\n+                       object->mark().value(), object->klass()->external_name());\n+        }\n+        if (event.should_commit()) {\n+          post_monitor_inflate_event(&event, object, cause);\n+        }\n+        return monitor;\n+      } else {\n+        delete monitor;\n+        continue;\n+      }\n+    }\n@@ -1263,0 +1379,1 @@\n+      assert(!UseFastLocking, \"can not happen with fast-locking\");\n@@ -1476,0 +1593,10 @@\n+class VM_RendezvousGCThreads : public VM_Operation {\n+public:\n+  bool evaluate_at_safepoint() const override { return false; }\n+  VMOp_Type type() const override { return VMOp_RendezvousGCThreads; }\n+  void doit() override {\n+    SuspendibleThreadSet::synchronize();\n+    SuspendibleThreadSet::desynchronize();\n+  };\n+};\n+\n@@ -1529,0 +1656,3 @@\n+      \/\/ Also, we sync and desync GC threads around the handshake, so that they can\n+      \/\/ safely read the mark-word and look-through to the object-monitor, without\n+      \/\/ being afraid that the object-monitor is going away.\n@@ -1531,0 +1661,2 @@\n+      VM_RendezvousGCThreads sync_gc;\n+      VMThread::execute(&sync_gc);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":207,"deletions":75,"binary":false,"changes":282,"status":"modified"},{"patch":"@@ -527,0 +527,1 @@\n+  assert(!UseFastLocking, \"should not be called with fast-locking\");\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -72,0 +72,1 @@\n+#include \"runtime\/lockStack.inline.hpp\"\n@@ -1175,0 +1176,1 @@\n+  assert(!UseFastLocking, \"only with stack-locking\");\n@@ -1204,0 +1206,10 @@\n+JavaThread* Threads::owning_thread_from_object(ThreadsList * t_list, oop obj) {\n+  assert(UseFastLocking, \"Only with fast-locking\");\n+  for (JavaThread* q : *t_list) {\n+    if (q->lock_stack().contains(obj)) {\n+      return q;\n+    }\n+  }\n+  return nullptr;\n+}\n+\n@@ -1205,2 +1217,12 @@\n-  address owner = (address)monitor->owner();\n-  return owning_thread_from_monitor_owner(t_list, owner);\n+  if (UseFastLocking) {\n+    if (monitor->is_owner_anonymous()) {\n+      return owning_thread_from_object(t_list, monitor->object());\n+    } else {\n+      Thread* owner = reinterpret_cast<Thread*>(monitor->owner());\n+      assert(owner == nullptr || owner->is_Java_thread(), \"only JavaThreads own monitors\");\n+      return reinterpret_cast<JavaThread*>(owner);\n+    }\n+  } else {\n+    address owner = (address)monitor->owner();\n+    return owning_thread_from_monitor_owner(t_list, owner);\n+  }\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":24,"deletions":2,"binary":false,"changes":26,"status":"modified"},{"patch":"@@ -84,0 +84,1 @@\n+  template(HeapObjectStatistics)                  \\\n@@ -93,0 +94,1 @@\n+  template(RendezvousGCThreads)                   \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":2,"deletions":0,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -381,2 +381,2 @@\n-     static_field(CompressedKlassPointers,     _narrow_klass._base,                           address)                               \\\n-     static_field(CompressedKlassPointers,     _narrow_klass._shift,                          int)                                   \\\n+     static_field(CompressedKlassPointers,     _base,                           address)                                             \\\n+     static_field(CompressedKlassPointers,     _shift_copy,                          int)                                            \\\n@@ -705,0 +705,3 @@\n+  nonstatic_field(JavaThread,                  _lock_stack,                                   LockStack)                             \\\n+  nonstatic_field(LockStack,                   _current,                                      oop*)                                  \\\n+  nonstatic_field(LockStack,                   _base,                                         oop*)                                  \\\n@@ -1319,0 +1322,1 @@\n+  declare_toplevel_type(LockStack)                                        \\\n@@ -2596,0 +2600,1 @@\n+  LP64_ONLY(declare_constant(markWord::klass_shift))                      \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":7,"deletions":2,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -599,5 +599,0 @@\n-const int LogKlassAlignmentInBytes = 3;\n-const int LogKlassAlignment        = LogKlassAlignmentInBytes - LogHeapWordSize;\n-const int KlassAlignmentInBytes    = 1 << LogKlassAlignmentInBytes;\n-const int KlassAlignment           = KlassAlignmentInBytes \/ HeapWordSize;\n-\n@@ -611,5 +606,0 @@\n-\/\/ Maximal size of compressed class space. Above this limit compression is not possible.\n-\/\/ Also upper bound for placement of zero based class space. (Class space is further limited\n-\/\/ to be < 3G, see arguments.cpp.)\n-const  uint64_t KlassEncodingMetaspaceMax = (uint64_t(max_juint) + 1) << LogKlassAlignmentInBytes;\n-\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":0,"deletions":10,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/compressedKlass.hpp\"\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -76,1 +76,2 @@\n-    final int hubOffset = getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -170,0 +170,13 @@\n+\n+Lilliput temporary:\n+compiler\/c2\/irTests\/TestVectorizationNotRun.java 8301785 generic-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-beyond-encoding-range-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-partly-within-encoding-range-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-within-encoding-range-use-zero 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-no-low-bits-use-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#x64-area-far-out-with-low-bits-use-add 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-xor 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-1 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-2 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassPointerEncoding.java#aarch64-movk-3 8302094 windows-all,macosx-all\n+runtime\/CompressedOops\/CompressedClassSpaceSize.java 8302094 generic-all\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":13,"deletions":0,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -375,1 +375,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -382,1 +382,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n@@ -392,1 +392,1 @@\n-        long expected = roundUp(Platform.is64bit() ? 16 : 8, OBJ_ALIGN);\n+        long expected = roundUp(8, OBJ_ALIGN);\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"}]}