{"files":[{"patch":"@@ -1194,0 +1194,4 @@\n+\n+  \/\/ Convert BootTest condition to Assembler condition.\n+  \/\/ Replicate the logic of cmpOpOper::ccode() and cmpOpUOper::ccode().\n+  Assembler::Condition to_assembler_cond(BoolTest::mask cond);\n@@ -2386,0 +2390,4 @@\n+const int Matcher::superword_max_vector_size(const BasicType bt) {\n+  return Matcher::max_vector_size(bt);\n+}\n+\n@@ -2542,0 +2550,44 @@\n+\/\/ Convert BootTest condition to Assembler condition.\n+\/\/ Replicate the logic of cmpOpOper::ccode() and cmpOpUOper::ccode().\n+Assembler::Condition to_assembler_cond(BoolTest::mask cond) {\n+  Assembler::Condition result;\n+  switch(cond) {\n+    case BoolTest::eq:\n+      result = Assembler::EQ; break;\n+    case BoolTest::ne:\n+      result = Assembler::NE; break;\n+    case BoolTest::le:\n+      result = Assembler::LE; break;\n+    case BoolTest::ge:\n+      result = Assembler::GE; break;\n+    case BoolTest::lt:\n+      result = Assembler::LT; break;\n+    case BoolTest::gt:\n+      result = Assembler::GT; break;\n+    case BoolTest::ule:\n+      result = Assembler::LS; break;\n+    case BoolTest::uge:\n+      result = Assembler::HS; break;\n+    case BoolTest::ult:\n+      result = Assembler::LO; break;\n+    case BoolTest::ugt:\n+      result = Assembler::HI; break;\n+    case BoolTest::overflow:\n+      result = Assembler::VS; break;\n+    case BoolTest::no_overflow:\n+      result = Assembler::VC; break;\n+    default:\n+      ShouldNotReachHere();\n+      return Assembler::Condition(-1);\n+  }\n+\n+  \/\/ Check conversion\n+  if (cond & BoolTest::unsigned_compare) {\n+    assert(cmpOpUOper((BoolTest::mask)((int)cond & ~(BoolTest::unsigned_compare))).ccode() == result, \"Invalid conversion\");\n+  } else {\n+    assert(cmpOpOper(cond).ccode() == result, \"Invalid conversion\");\n+  }\n+\n+  return result;\n+}\n+\n@@ -4313,0 +4365,11 @@\n+\/\/ BoolTest condition for signed compare\n+operand immI_cmp_cond()\n+%{\n+  predicate(n->get_int() < (int)(BoolTest::unsigned_compare));\n+  match(ConI);\n+\n+  op_cost(0);\n+  format %{ %}\n+  interface(CONST_INTER);\n+%}\n+\n@@ -9013,2 +9076,1 @@\n-\/\/ This section is generated from aarch64_ad_cas.m4\n-\n+\/\/ This section is generated from cas.m4\n@@ -9020,1 +9082,0 @@\n-\n@@ -9039,1 +9100,0 @@\n-\n@@ -9058,1 +9118,0 @@\n-\n@@ -9076,1 +9135,0 @@\n-\n@@ -9094,1 +9152,0 @@\n-\n@@ -9240,1 +9297,0 @@\n-\n@@ -9260,1 +9316,0 @@\n-\n@@ -9280,1 +9335,0 @@\n-\n@@ -9300,1 +9354,0 @@\n-\n@@ -9320,1 +9373,0 @@\n-\n@@ -15071,2 +15123,1 @@\n-      __ fcvtsh($tmp$$FloatRegister, $src$$FloatRegister);\n-      __ smov($dst$$Register, $tmp$$FloatRegister, __ H, 0);\n+      __ flt_to_flt16($dst$$Register, $src$$FloatRegister, $tmp$$FloatRegister);\n@@ -15084,2 +15135,1 @@\n-      __ mov($tmp$$FloatRegister, __ H, 0, $src$$Register);\n-      __ fcvths($dst$$FloatRegister, $tmp$$FloatRegister);\n+      __ flt16_to_flt($dst$$FloatRegister, $src$$Register, $tmp$$FloatRegister);\n@@ -16599,18 +16649,0 @@\n-\/\/ counted loop end branch near Unsigned\n-instruct branchLoopEndU(cmpOpU cmp, rFlagsRegU cr, label lbl)\n-%{\n-  match(CountedLoopEnd cmp cr);\n-\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-  \/\/ short variant.\n-  \/\/ ins_short_branch(1);\n-  format %{ \"b$cmp $lbl \\t\/\/ counted loop end unsigned\" %}\n-\n-  ins_encode(aarch64_enc_br_conU(cmp, lbl));\n-\n-  ins_pipe(pipe_branch);\n-%}\n-\n-\/\/ counted loop end branch far unsigned\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":66,"deletions":34,"binary":false,"changes":100,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -1818,1 +1818,1 @@\n-void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr unused, LIR_Opr dest, LIR_Op* op) {\n+void LIR_Assembler::intrinsic_op(LIR_Code code, LIR_Opr value, LIR_Opr tmp, LIR_Opr dest, LIR_Op* op) {\n@@ -1822,0 +1822,2 @@\n+  case lir_f2hf: __ flt_to_flt16(dest->as_register(), value->as_float_reg(), tmp->as_float_reg()); break;\n+  case lir_hf2f: __ flt16_to_flt(dest->as_float_reg(), value->as_register(), tmp->as_float_reg()); break;\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -1859,0 +1859,11 @@\n+\n+void InterpreterMacroAssembler::load_resolved_indy_entry(Register cache, Register index) {\n+  \/\/ Get index out of bytecode pointer, get_cache_entry_pointer_at_bcp\n+  get_cache_index_at_bcp(index, 1, sizeof(u4));\n+  \/\/ Get address of invokedynamic array\n+  ldr(cache, Address(rcpool, in_bytes(ConstantPoolCache::invokedynamic_entries_offset())));\n+  \/\/ Scale the index to be the entry index * sizeof(ResolvedInvokeDynamicInfo)\n+  lsl(index, index, log2i_exact(sizeof(ResolvedIndyEntry)));\n+  add(cache, cache, Array<ResolvedIndyEntry>::base_offset_in_bytes());\n+  lea(cache, Address(cache, index));\n+}\n","filename":"src\/hotspot\/cpu\/aarch64\/interp_masm_aarch64.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -3972,0 +3972,58 @@\n+void MacroAssembler::kernel_crc32c_using_crypto_pmull(Register crc, Register buf,\n+        Register len, Register tmp0, Register tmp1, Register tmp2, Register tmp3) {\n+    Label CRC_by4_loop, CRC_by1_loop, CRC_less128, CRC_by128_pre, CRC_by32_loop, CRC_less32, L_exit;\n+    assert_different_registers(crc, buf, len, tmp0, tmp1, tmp2);\n+\n+    subs(tmp0, len, 384);\n+    br(Assembler::GE, CRC_by128_pre);\n+  BIND(CRC_less128);\n+    subs(len, len, 32);\n+    br(Assembler::GE, CRC_by32_loop);\n+  BIND(CRC_less32);\n+    adds(len, len, 32 - 4);\n+    br(Assembler::GE, CRC_by4_loop);\n+    adds(len, len, 4);\n+    br(Assembler::GT, CRC_by1_loop);\n+    b(L_exit);\n+\n+  BIND(CRC_by32_loop);\n+    ldp(tmp0, tmp1, Address(buf));\n+    crc32cx(crc, crc, tmp0);\n+    ldr(tmp2, Address(buf, 16));\n+    crc32cx(crc, crc, tmp1);\n+    ldr(tmp3, Address(buf, 24));\n+    crc32cx(crc, crc, tmp2);\n+    add(buf, buf, 32);\n+    subs(len, len, 32);\n+    crc32cx(crc, crc, tmp3);\n+    br(Assembler::GE, CRC_by32_loop);\n+    cmn(len, (u1)32);\n+    br(Assembler::NE, CRC_less32);\n+    b(L_exit);\n+\n+  BIND(CRC_by4_loop);\n+    ldrw(tmp0, Address(post(buf, 4)));\n+    subs(len, len, 4);\n+    crc32cw(crc, crc, tmp0);\n+    br(Assembler::GE, CRC_by4_loop);\n+    adds(len, len, 4);\n+    br(Assembler::LE, L_exit);\n+  BIND(CRC_by1_loop);\n+    ldrb(tmp0, Address(post(buf, 1)));\n+    subs(len, len, 1);\n+    crc32cb(crc, crc, tmp0);\n+    br(Assembler::GT, CRC_by1_loop);\n+    b(L_exit);\n+\n+  BIND(CRC_by128_pre);\n+    kernel_crc32_common_fold_using_crypto_pmull(crc, buf, len, tmp0, tmp1, tmp2,\n+      4*256*sizeof(juint) + 8*sizeof(juint) + 0x50);\n+    mov(crc, 0);\n+    crc32cx(crc, crc, tmp0);\n+    crc32cx(crc, crc, tmp1);\n+\n+    cbnz(len, CRC_less128);\n+\n+  BIND(L_exit);\n+}\n+\n@@ -4078,1 +4136,5 @@\n-  kernel_crc32c_using_crc32c(crc, buf, len, table0, table1, table2, table3);\n+  if (UseCryptoPmullForCRC32) {\n+    kernel_crc32c_using_crypto_pmull(crc, buf, len, table0, table1, table2, table3);\n+  } else {\n+    kernel_crc32c_using_crc32c(crc, buf, len, table0, table1, table2, table3);\n+  }\n@@ -4293,5 +4355,0 @@\n-void MacroAssembler::load_klass_check_null(Register dst, Register src) {\n-  null_check(src, oopDesc::klass_offset_in_bytes());\n-  load_klass(dst, src);\n-}\n-\n@@ -5577,1 +5634,1 @@\n-    umov(chk, vhix, D, 1);      ASCII(cmlt(vlox, T16B, vlox));\n+    umov(chk, vhix, D, 1);      ASCII(cm(LT, vlox, T16B, vlox));\n@@ -5603,1 +5660,1 @@\n-                                ASCII(cmlt(vtmp2, T16B, vlo));\n+                                ASCII(cm(LT, vtmp2, T16B, vlo));\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":65,"deletions":8,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -526,0 +526,4 @@\n+  void flt_to_flt16(Register dst, FloatRegister src, FloatRegister tmp) {\n+    fcvtsh(tmp, src);\n+    smov(dst, tmp, H, 0);\n+  }\n@@ -527,1 +531,4 @@\n-public:\n+  void flt16_to_flt(FloatRegister dst, Register src, FloatRegister tmp) {\n+    mov(tmp, H, 0, src);\n+    fcvths(dst, tmp);\n+  }\n@@ -857,1 +864,0 @@\n-  void load_klass_check_null(Register dst, Register src);\n@@ -1443,0 +1449,3 @@\n+  void kernel_crc32c_using_crypto_pmull(Register crc, Register buf,\n+        Register len, Register tmp0, Register tmp1, Register tmp2,\n+        Register tmp3);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":11,"deletions":2,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -573,1 +573,1 @@\n-      (Interpreter::code() != NULL || StubRoutines::code1() != NULL)) {\n+      (Interpreter::code() != NULL || StubRoutines::final_stubs_code() != NULL)) {\n@@ -581,1 +581,1 @@\n-    if (Interpreter::code() != NULL)\n+    if (Interpreter::code() != NULL) {\n@@ -585,1 +585,2 @@\n-    if (StubRoutines::code1() != NULL)\n+    }\n+    if (StubRoutines::initial_stubs_code() != NULL) {\n@@ -587,1 +588,2 @@\n-                  StubRoutines::code1()->code_begin(), StubRoutines::code1()->code_end(),\n+                  StubRoutines::initial_stubs_code()->code_begin(),\n+                  StubRoutines::initial_stubs_code()->code_end(),\n@@ -589,1 +591,2 @@\n-    if (StubRoutines::code2() != NULL)\n+    }\n+    if (StubRoutines::final_stubs_code() != NULL) {\n@@ -591,1 +594,2 @@\n-                  StubRoutines::code2()->code_begin(), StubRoutines::code2()->code_end(),\n+                  StubRoutines::final_stubs_code()->code_begin(),\n+                  StubRoutines::final_stubs_code()->code_end(),\n@@ -593,0 +597,1 @@\n+    }\n@@ -794,1 +799,0 @@\n-  __ flush();\n","filename":"src\/hotspot\/cpu\/aarch64\/sharedRuntime_aarch64.cpp","additions":11,"deletions":7,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -699,0 +699,73 @@\n+  \/\/ Helper object to reduce noise when telling the GC barriers how to perform loads and stores\n+  \/\/ for arraycopy stubs.\n+  class ArrayCopyBarrierSetHelper : StackObj {\n+    BarrierSetAssembler* _bs_asm;\n+    MacroAssembler* _masm;\n+    DecoratorSet _decorators;\n+    BasicType _type;\n+    Register _gct1;\n+    Register _gct2;\n+    Register _gct3;\n+    FloatRegister _gcvt1;\n+    FloatRegister _gcvt2;\n+    FloatRegister _gcvt3;\n+\n+  public:\n+    ArrayCopyBarrierSetHelper(MacroAssembler* masm,\n+                              DecoratorSet decorators,\n+                              BasicType type,\n+                              Register gct1,\n+                              Register gct2,\n+                              Register gct3,\n+                              FloatRegister gcvt1,\n+                              FloatRegister gcvt2,\n+                              FloatRegister gcvt3)\n+      : _bs_asm(BarrierSet::barrier_set()->barrier_set_assembler()),\n+        _masm(masm),\n+        _decorators(decorators),\n+        _type(type),\n+        _gct1(gct1),\n+        _gct2(gct2),\n+        _gct3(gct3),\n+        _gcvt1(gcvt1),\n+        _gcvt2(gcvt2),\n+        _gcvt3(gcvt3) {\n+    }\n+\n+    void copy_load_at_32(FloatRegister dst1, FloatRegister dst2, Address src) {\n+      _bs_asm->copy_load_at(_masm, _decorators, _type, 32,\n+                            dst1, dst2, src,\n+                            _gct1, _gct2, _gcvt1);\n+    }\n+\n+    void copy_store_at_32(Address dst, FloatRegister src1, FloatRegister src2) {\n+      _bs_asm->copy_store_at(_masm, _decorators, _type, 32,\n+                             dst, src1, src2,\n+                             _gct1, _gct2, _gct3, _gcvt1, _gcvt2, _gcvt3);\n+    }\n+\n+    void copy_load_at_16(Register dst1, Register dst2, Address src) {\n+      _bs_asm->copy_load_at(_masm, _decorators, _type, 16,\n+                            dst1, dst2, src,\n+                            _gct1);\n+    }\n+\n+    void copy_store_at_16(Address dst, Register src1, Register src2) {\n+      _bs_asm->copy_store_at(_masm, _decorators, _type, 16,\n+                             dst, src1, src2,\n+                             _gct1, _gct2, _gct3);\n+    }\n+\n+    void copy_load_at_8(Register dst, Address src) {\n+      _bs_asm->copy_load_at(_masm, _decorators, _type, 8,\n+                            dst, noreg, src,\n+                            _gct1);\n+    }\n+\n+    void copy_store_at_8(Address dst, Register src) {\n+      _bs_asm->copy_store_at(_masm, _decorators, _type, 8,\n+                             dst, src, noreg,\n+                             _gct1, _gct2, _gct3);\n+    }\n+  };\n+\n@@ -712,1 +785,1 @@\n-  void generate_copy_longs(Label &start, Register s, Register d, Register count,\n+  void generate_copy_longs(DecoratorSet decorators, BasicType type, Label &start, Register s, Register d, Register count,\n@@ -718,2 +791,5 @@\n-      t4 = r7, t5 = r10, t6 = r11, t7 = r12;\n-    const Register stride = r13;\n+      t4 = r7, t5 = r11, t6 = r12, t7 = r13;\n+    const Register stride = r14;\n+    const Register gct1 = rscratch1, gct2 = rscratch2, gct3 = r10;\n+    const FloatRegister gcvt1 = v6, gcvt2 = v7, gcvt3 = v16; \/\/ Note that v8-v15 are callee saved\n+    ArrayCopyBarrierSetHelper bs(_masm, decorators, type, gct1, gct2, gct3, gcvt1, gcvt2, gcvt3);\n@@ -721,2 +797,2 @@\n-    assert_different_registers(rscratch1, t0, t1, t2, t3, t4, t5, t6, t7);\n-    assert_different_registers(s, d, count, rscratch1);\n+    assert_different_registers(rscratch1, rscratch2, t0, t1, t2, t3, t4, t5, t6, t7);\n+    assert_different_registers(s, d, count, rscratch1, rscratch2);\n@@ -760,2 +836,2 @@\n-      __ ldpq(v0, v1, Address(s, 4 * unit));\n-      __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));\n+      bs.copy_load_at_32(v0, v1, Address(s, 4 * unit));\n+      bs.copy_load_at_32(v2, v3, Address(__ pre(s, 8 * unit)));\n@@ -763,4 +839,4 @@\n-      __ ldp(t0, t1, Address(s, 2 * unit));\n-      __ ldp(t2, t3, Address(s, 4 * unit));\n-      __ ldp(t4, t5, Address(s, 6 * unit));\n-      __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));\n+      bs.copy_load_at_16(t0, t1, Address(s, 2 * unit));\n+      bs.copy_load_at_16(t2, t3, Address(s, 4 * unit));\n+      bs.copy_load_at_16(t4, t5, Address(s, 6 * unit));\n+      bs.copy_load_at_16(t6, t7, Address(__ pre(s, 8 * unit)));\n@@ -786,4 +862,4 @@\n-      __ stpq(v0, v1, Address(d, 4 * unit));\n-      __ ldpq(v0, v1, Address(s, 4 * unit));\n-      __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));\n-      __ ldpq(v2, v3, Address(__ pre(s, 8 * unit)));\n+      bs.copy_store_at_32(Address(d, 4 * unit), v0, v1);\n+      bs.copy_load_at_32(v0, v1, Address(s, 4 * unit));\n+      bs.copy_store_at_32(Address(__ pre(d, 8 * unit)), v2, v3);\n+      bs.copy_load_at_32(v2, v3, Address(__ pre(s, 8 * unit)));\n@@ -791,8 +867,8 @@\n-      __ stp(t0, t1, Address(d, 2 * unit));\n-      __ ldp(t0, t1, Address(s, 2 * unit));\n-      __ stp(t2, t3, Address(d, 4 * unit));\n-      __ ldp(t2, t3, Address(s, 4 * unit));\n-      __ stp(t4, t5, Address(d, 6 * unit));\n-      __ ldp(t4, t5, Address(s, 6 * unit));\n-      __ stp(t6, t7, Address(__ pre(d, 8 * unit)));\n-      __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));\n+      bs.copy_store_at_16(Address(d, 2 * unit), t0, t1);\n+      bs.copy_load_at_16(t0, t1, Address(s, 2 * unit));\n+      bs.copy_store_at_16(Address(d, 4 * unit), t2, t3);\n+      bs.copy_load_at_16(t2, t3, Address(s, 4 * unit));\n+      bs.copy_store_at_16(Address(d, 6 * unit), t4, t5);\n+      bs.copy_load_at_16(t4, t5, Address(s, 6 * unit));\n+      bs.copy_store_at_16(Address(__ pre(d, 8 * unit)), t6, t7);\n+      bs.copy_load_at_16(t6, t7, Address(__ pre(s, 8 * unit)));\n@@ -807,2 +883,2 @@\n-      __ stpq(v0, v1, Address(d, 4 * unit));\n-      __ stpq(v2, v3, Address(__ pre(d, 8 * unit)));\n+      bs.copy_store_at_32(Address(d, 4 * unit), v0, v1);\n+      bs.copy_store_at_32(Address(__ pre(d, 8 * unit)), v2, v3);\n@@ -810,4 +886,4 @@\n-      __ stp(t0, t1, Address(d, 2 * unit));\n-      __ stp(t2, t3, Address(d, 4 * unit));\n-      __ stp(t4, t5, Address(d, 6 * unit));\n-      __ stp(t6, t7, Address(__ pre(d, 8 * unit)));\n+      bs.copy_store_at_16(Address(d, 2 * unit), t0, t1);\n+      bs.copy_store_at_16(Address(d, 4 * unit), t2, t3);\n+      bs.copy_store_at_16(Address(d, 6 * unit), t4, t5);\n+      bs.copy_store_at_16(Address(__ pre(d, 8 * unit)), t6, t7);\n@@ -820,2 +896,2 @@\n-        __ ldpq(v0, v1, Address(__ pre(s, 4 * unit)));\n-        __ stpq(v0, v1, Address(__ pre(d, 4 * unit)));\n+        bs.copy_load_at_32(v0, v1, Address(__ pre(s, 4 * unit)));\n+        bs.copy_store_at_32(Address(__ pre(d, 4 * unit)), v0, v1);\n@@ -823,4 +899,4 @@\n-        __ ldp(t0, t1, Address(s, 2 * unit));\n-        __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));\n-        __ stp(t0, t1, Address(d, 2 * unit));\n-        __ stp(t2, t3, Address(__ pre(d, 4 * unit)));\n+        bs.copy_load_at_16(t0, t1, Address(s, 2 * unit));\n+        bs.copy_load_at_16(t2, t3, Address(__ pre(s, 4 * unit)));\n+        bs.copy_store_at_16(Address(d, 2 * unit), t0, t1);\n+        bs.copy_store_at_16(Address(__ pre(d, 4 * unit)), t2, t3);\n@@ -836,2 +912,2 @@\n-      __ ldp(t0, t1, Address(__ adjust(s, 2 * unit, direction == copy_backwards)));\n-      __ stp(t0, t1, Address(__ adjust(d, 2 * unit, direction == copy_backwards)));\n+      bs.copy_load_at_16(t0, t1, Address(__ adjust(s, 2 * unit, direction == copy_backwards)));\n+      bs.copy_store_at_16(Address(__ adjust(d, 2 * unit, direction == copy_backwards)), t0, t1);\n@@ -896,4 +972,4 @@\n-      __ ldp(t0, t1, Address(s, 2 * unit));\n-      __ ldp(t2, t3, Address(s, 4 * unit));\n-      __ ldp(t4, t5, Address(s, 6 * unit));\n-      __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));\n+      bs.copy_load_at_16(t0, t1, Address(s, 2 * unit));\n+      bs.copy_load_at_16(t2, t3, Address(s, 4 * unit));\n+      bs.copy_load_at_16(t4, t5, Address(s, 6 * unit));\n+      bs.copy_load_at_16(t6, t7, Address(__ pre(s, 8 * unit)));\n@@ -928,9 +1004,9 @@\n-        __ str(t0, Address(d, 1 * unit));\n-        __ stp(t1, t2, Address(d, 2 * unit));\n-        __ ldp(t0, t1, Address(s, 2 * unit));\n-        __ stp(t3, t4, Address(d, 4 * unit));\n-        __ ldp(t2, t3, Address(s, 4 * unit));\n-        __ stp(t5, t6, Address(d, 6 * unit));\n-        __ ldp(t4, t5, Address(s, 6 * unit));\n-        __ str(t7, Address(__ pre(d, 8 * unit)));\n-        __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));\n+        bs.copy_store_at_8(Address(d, 1 * unit), t0);\n+        bs.copy_store_at_16(Address(d, 2 * unit), t1, t2);\n+        bs.copy_load_at_16(t0, t1, Address(s, 2 * unit));\n+        bs.copy_store_at_16(Address(d, 4 * unit), t3, t4);\n+        bs.copy_load_at_16(t2, t3, Address(s, 4 * unit));\n+        bs.copy_store_at_16(Address(d, 6 * unit), t5, t6);\n+        bs.copy_load_at_16(t4, t5, Address(s, 6 * unit));\n+        bs.copy_store_at_8(Address(__ pre(d, 8 * unit)), t7);\n+        bs.copy_load_at_16(t6, t7, Address(__ pre(s, 8 * unit)));\n@@ -951,9 +1027,9 @@\n-        __ str(t1, Address(d, 1 * unit));\n-        __ stp(t3, t0, Address(d, 3 * unit));\n-        __ ldp(t0, t1, Address(s, 2 * unit));\n-        __ stp(t5, t2, Address(d, 5 * unit));\n-        __ ldp(t2, t3, Address(s, 4 * unit));\n-        __ stp(t7, t4, Address(d, 7 * unit));\n-        __ ldp(t4, t5, Address(s, 6 * unit));\n-        __ str(t6, Address(__ pre(d, 8 * unit)));\n-        __ ldp(t6, t7, Address(__ pre(s, 8 * unit)));\n+        bs.copy_store_at_8(Address(d, 1 * unit), t1);\n+        bs.copy_store_at_16(Address(d, 3 * unit), t3, t0);\n+        bs.copy_load_at_16(t0, t1, Address(s, 2 * unit));\n+        bs.copy_store_at_16(Address(d, 5 * unit), t5, t2);\n+        bs.copy_load_at_16(t2, t3, Address(s, 4 * unit));\n+        bs.copy_store_at_16(Address(d, 7 * unit), t7, t4);\n+        bs.copy_load_at_16(t4, t5, Address(s, 6 * unit));\n+        bs.copy_store_at_8(Address(__ pre(d, 8 * unit)), t6);\n+        bs.copy_load_at_16(t6, t7, Address(__ pre(s, 8 * unit)));\n@@ -971,5 +1047,5 @@\n-        __ str(t0, Address(d, 1 * unit));\n-        __ stp(t1, t2, Address(d, 2 * unit));\n-        __ stp(t3, t4, Address(d, 4 * unit));\n-        __ stp(t5, t6, Address(d, 6 * unit));\n-        __ str(t7, Address(__ pre(d, 8 * unit)));\n+        bs.copy_store_at_8(Address(d, 1 * unit), t0);\n+        bs.copy_store_at_16(Address(d, 2 * unit), t1, t2);\n+        bs.copy_store_at_16(Address(d, 4 * unit), t3, t4);\n+        bs.copy_store_at_16(Address(d, 6 * unit), t5, t6);\n+        bs.copy_store_at_8(Address(__ pre(d, 8 * unit)), t7);\n@@ -977,5 +1053,5 @@\n-        __ str(t1, Address(d, 1 * unit));\n-        __ stp(t3, t0, Address(d, 3 * unit));\n-        __ stp(t5, t2, Address(d, 5 * unit));\n-        __ stp(t7, t4, Address(d, 7 * unit));\n-        __ str(t6, Address(__ pre(d, 8 * unit)));\n+        bs.copy_store_at_8(Address(d, 1 * unit), t1);\n+        bs.copy_store_at_16(Address(d, 3 * unit), t3, t0);\n+        bs.copy_store_at_16(Address(d, 5 * unit), t5, t2);\n+        bs.copy_store_at_16(Address(d, 7 * unit), t7, t4);\n+        bs.copy_store_at_8(Address(__ pre(d, 8 * unit)), t6);\n@@ -994,2 +1070,2 @@\n-        __ ldp(t0, t1, Address(s, 2 * unit));\n-        __ ldp(t2, t3, Address(__ pre(s, 4 * unit)));\n+        bs.copy_load_at_16(t0, t1, Address(s, 2 * unit));\n+        bs.copy_load_at_16(t2, t3, Address(__ pre(s, 4 * unit)));\n@@ -997,3 +1073,3 @@\n-          __ str(t0, Address(d, 1 * unit));\n-          __ stp(t1, t2, Address(d, 2 * unit));\n-          __ str(t3, Address(__ pre(d, 4 * unit)));\n+          bs.copy_store_at_8(Address(d, 1 * unit), t0);\n+          bs.copy_store_at_16(Address(d, 2 * unit), t1, t2);\n+          bs.copy_store_at_8(Address(__ pre(d, 4 * unit)), t3);\n@@ -1001,3 +1077,3 @@\n-          __ str(t1, Address(d, 1 * unit));\n-          __ stp(t3, t0, Address(d, 3 * unit));\n-          __ str(t2, Address(__ pre(d, 4 * unit)));\n+          bs.copy_store_at_8(Address(d, 1 * unit), t1);\n+          bs.copy_store_at_16(Address(d, 3 * unit), t3, t0);\n+          bs.copy_store_at_8(Address(__ pre(d, 4 * unit)), t2);\n@@ -1012,1 +1088,1 @@\n-        __ ldp(t0, t1, Address(__ pre(s, 2 * unit)));\n+        bs.copy_load_at_16(t0, t1, Address(__ pre(s, 2 * unit)));\n@@ -1014,2 +1090,2 @@\n-          __ str(t0, Address(d, 1 * unit));\n-          __ str(t1, Address(__ pre(d, 2 * unit)));\n+          bs.copy_store_at_8(Address(d, 1 * unit), t0);\n+          bs.copy_store_at_8(Address(__ pre(d, 2 * unit)), t1);\n@@ -1017,2 +1093,2 @@\n-          __ str(t1, Address(d, 1 * unit));\n-          __ str(t0, Address(__ pre(d, 2 * unit)));\n+          bs.copy_store_at_8(Address(d, 1 * unit), t1);\n+          bs.copy_store_at_8(Address(__ pre(d, 2 * unit)), t0);\n@@ -1041,1 +1117,1 @@\n-  void copy_memory_small(Register s, Register d, Register count, Register tmp, int step) {\n+  void copy_memory_small(DecoratorSet decorators, BasicType type, Register s, Register d, Register count, int step) {\n@@ -1045,1 +1121,0 @@\n-    int unit = wordSize * direction;\n@@ -1052,1 +1127,3 @@\n-    const Register t0 = r3, t1 = r4, t2 = r5, t3 = r6;\n+    const Register t0 = r3;\n+    const Register gct1 = rscratch1, gct2 = rscratch2, gct3 = r10;\n+    ArrayCopyBarrierSetHelper bs(_masm, decorators, type, gct1, gct2, gct3, fnoreg, fnoreg, fnoreg);\n@@ -1060,2 +1137,2 @@\n-    __ ldr(tmp, Address(__ adjust(s, unit, is_backwards)));\n-    __ str(tmp, Address(__ adjust(d, unit, is_backwards)));\n+    bs.copy_load_at_8(t0, Address(__ adjust(s, direction * wordSize, is_backwards)));\n+    bs.copy_store_at_8(Address(__ adjust(d, direction * wordSize, is_backwards)), t0);\n@@ -1066,2 +1143,2 @@\n-      __ ldrw(tmp, Address(__ adjust(s, sizeof (jint) * direction, is_backwards)));\n-      __ strw(tmp, Address(__ adjust(d, sizeof (jint) * direction, is_backwards)));\n+      __ ldrw(t0, Address(__ adjust(s, sizeof (jint) * direction, is_backwards)));\n+      __ strw(t0, Address(__ adjust(d, sizeof (jint) * direction, is_backwards)));\n@@ -1073,2 +1150,2 @@\n-      __ ldrh(tmp, Address(__ adjust(s, sizeof (jshort) * direction, is_backwards)));\n-      __ strh(tmp, Address(__ adjust(d, sizeof (jshort) * direction, is_backwards)));\n+      __ ldrh(t0, Address(__ adjust(s, sizeof (jshort) * direction, is_backwards)));\n+      __ strh(t0, Address(__ adjust(d, sizeof (jshort) * direction, is_backwards)));\n@@ -1080,2 +1157,2 @@\n-      __ ldrb(tmp, Address(__ adjust(s, sizeof (jbyte) * direction, is_backwards)));\n-      __ strb(tmp, Address(__ adjust(d, sizeof (jbyte) * direction, is_backwards)));\n+      __ ldrb(t0, Address(__ adjust(s, sizeof (jbyte) * direction, is_backwards)));\n+      __ strb(t0, Address(__ adjust(d, sizeof (jbyte) * direction, is_backwards)));\n@@ -1087,0 +1164,2 @@\n+  Label copy_obj_f, copy_obj_b;\n+  Label copy_obj_uninit_f, copy_obj_uninit_b;\n@@ -1095,2 +1174,2 @@\n-  void copy_memory(bool is_aligned, Register s, Register d,\n-                   Register count, Register tmp, int step) {\n+  void copy_memory(DecoratorSet decorators, BasicType type, bool is_aligned,\n+                   Register s, Register d, Register count, int step) {\n@@ -1105,2 +1184,2 @@\n-    const Register t2 = r5, t3 = r6, t4 = r7, t5 = r8;\n-    const Register t6 = r9, t7 = r10, t8 = r11, t9 = r12;\n+    const Register t2 = r5, t3 = r6, t4 = r7, t5 = r11;\n+    const Register t6 = r12, t7 = r13, t8 = r14, t9 = r15;\n@@ -1108,0 +1187,3 @@\n+    const Register gct1 = rscratch1, gct2 = rscratch2, gct3 = r10;\n+    const FloatRegister gcvt1 = v6, gcvt2 = v7, gcvt3 = v16; \/\/ Note that v8-v15 are callee saved\n+    ArrayCopyBarrierSetHelper bs(_masm, decorators, type, gct1, gct2, gct3, gcvt1, gcvt2, gcvt3);\n@@ -1128,4 +1210,4 @@\n-      __ ldpq(v0, v1, Address(s, 0));\n-      __ ldpq(v2, v3, Address(send, -32));\n-      __ stpq(v0, v1, Address(d, 0));\n-      __ stpq(v2, v3, Address(dend, -32));\n+      bs.copy_load_at_32(v0, v1, Address(s, 0));\n+      bs.copy_load_at_32(v2, v3, Address(send, -32));\n+      bs.copy_store_at_32(Address(d, 0), v0, v1);\n+      bs.copy_store_at_32(Address(dend, -32), v2, v3);\n@@ -1133,4 +1215,4 @@\n-      __ ldp(t0, t1, Address(s, 0));\n-      __ ldp(t2, t3, Address(s, 16));\n-      __ ldp(t4, t5, Address(send, -32));\n-      __ ldp(t6, t7, Address(send, -16));\n+      bs.copy_load_at_16(t0, t1, Address(s, 0));\n+      bs.copy_load_at_16(t2, t3, Address(s, 16));\n+      bs.copy_load_at_16(t4, t5, Address(send, -32));\n+      bs.copy_load_at_16(t6, t7, Address(send, -16));\n@@ -1138,4 +1220,4 @@\n-      __ stp(t0, t1, Address(d, 0));\n-      __ stp(t2, t3, Address(d, 16));\n-      __ stp(t4, t5, Address(dend, -32));\n-      __ stp(t6, t7, Address(dend, -16));\n+      bs.copy_store_at_16(Address(d, 0), t0, t1);\n+      bs.copy_store_at_16(Address(d, 16), t2, t3);\n+      bs.copy_store_at_16(Address(dend, -32), t4, t5);\n+      bs.copy_store_at_16(Address(dend, -16), t6, t7);\n@@ -1147,4 +1229,5 @@\n-    __ ldp(t0, t1, Address(s, 0));\n-    __ ldp(t2, t3, Address(send, -16));\n-    __ stp(t0, t1, Address(d, 0));\n-    __ stp(t2, t3, Address(dend, -16));\n+    bs.copy_load_at_16(t0, t1, Address(s, 0));\n+    bs.copy_load_at_16(t6, t7, Address(send, -16));\n+\n+    bs.copy_store_at_16(Address(d, 0), t0, t1);\n+    bs.copy_store_at_16(Address(dend, -16), t6, t7);\n@@ -1157,2 +1240,2 @@\n-      __ ldpq(v0, v1, Address(s, 0));\n-      __ ldpq(v2, v3, Address(s, 32));\n+      bs.copy_load_at_32(v0, v1, Address(s, 0));\n+      bs.copy_load_at_32(v2, v3, Address(s, 32));\n@@ -1170,1 +1253,4 @@\n-        __ ldp(t0, t1, Address(send, -16));\n+        bs.copy_load_at_16(t0, t1, Address(send, -16));\n+\n+        bs.copy_store_at_32(Address(d, 0), v0, v1);\n+        bs.copy_store_at_32(Address(d, 32), v2, v3);\n@@ -1172,3 +1258,1 @@\n-        __ stpq(v0, v1, Address(d, 0));\n-        __ stpq(v2, v3, Address(d, 32));\n-        __ stp(t0, t1, Address(dend, -16));\n+        bs.copy_store_at_16(Address(dend, -16), t0, t1);\n@@ -1179,1 +1263,4 @@\n-      __ ldpq(v4, v5, Address(send, -32));\n+      bs.copy_load_at_32(v4, v5, Address(send, -32));\n+\n+      bs.copy_store_at_32(Address(d, 0), v0, v1);\n+      bs.copy_store_at_32(Address(d, 32), v2, v3);\n@@ -1181,3 +1268,1 @@\n-      __ stpq(v0, v1, Address(d, 0));\n-      __ stpq(v2, v3, Address(d, 32));\n-      __ stpq(v4, v5, Address(dend, -32));\n+      bs.copy_store_at_32(Address(dend, -32), v4, v5);\n@@ -1185,11 +1270,11 @@\n-      __ ldp(t0, t1, Address(s, 0));\n-      __ ldp(t2, t3, Address(s, 16));\n-      __ ldp(t4, t5, Address(s, 32));\n-      __ ldp(t6, t7, Address(s, 48));\n-      __ ldp(t8, t9, Address(send, -16));\n-\n-      __ stp(t0, t1, Address(d, 0));\n-      __ stp(t2, t3, Address(d, 16));\n-      __ stp(t4, t5, Address(d, 32));\n-      __ stp(t6, t7, Address(d, 48));\n-      __ stp(t8, t9, Address(dend, -16));\n+      bs.copy_load_at_16(t0, t1, Address(s, 0));\n+      bs.copy_load_at_16(t2, t3, Address(s, 16));\n+      bs.copy_load_at_16(t4, t5, Address(s, 32));\n+      bs.copy_load_at_16(t6, t7, Address(s, 48));\n+      bs.copy_load_at_16(t8, t9, Address(send, -16));\n+\n+      bs.copy_store_at_16(Address(d, 0), t0, t1);\n+      bs.copy_store_at_16(Address(d, 16), t2, t3);\n+      bs.copy_store_at_16(Address(d, 32), t4, t5);\n+      bs.copy_store_at_16(Address(d, 48), t6, t7);\n+      bs.copy_store_at_16(Address(dend, -16), t8, t9);\n@@ -1205,4 +1290,4 @@\n-    __ ldr(t0, Address(s, 0));\n-    __ ldr(t1, Address(send, -8));\n-    __ str(t0, Address(d, 0));\n-    __ str(t1, Address(dend, -8));\n+    bs.copy_load_at_8(t0, Address(s, 0));\n+    bs.copy_load_at_8(t1, Address(send, -8));\n+    bs.copy_store_at_8(Address(d, 0), t0);\n+    bs.copy_store_at_8(Address(dend, -8), t1);\n@@ -1255,0 +1340,5 @@\n+    \/\/ Here we will materialize a count in r15, which is used by copy_memory_small\n+    \/\/ and the various generate_copy_longs stubs that we use for 2 word aligned bytes.\n+    \/\/ Up until here, we have used t9, which aliases r15, but from here on, that register\n+    \/\/ can not be used as a temp register, as it contains the count.\n+\n@@ -1260,2 +1350,2 @@\n-      __ ldr(tmp, Address(__ adjust(s, direction * wordSize, is_backwards)));\n-      __ str(tmp, Address(__ adjust(d, direction * wordSize, is_backwards)));\n+      bs.copy_load_at_8(t0, Address(__ adjust(s, direction * wordSize, is_backwards)));\n+      bs.copy_store_at_8(Address(__ adjust(d, direction * wordSize, is_backwards)), t0);\n@@ -1265,1 +1355,1 @@\n-        __ andr(rscratch2, s, 2 * wordSize - 1);\n+        __ andr(r15, s, 2 * wordSize - 1);\n@@ -1267,2 +1357,2 @@\n-        __ neg(rscratch2, s);\n-        __ andr(rscratch2, rscratch2, 2 * wordSize - 1);\n+        __ neg(r15, s);\n+        __ andr(r15, r15, 2 * wordSize - 1);\n@@ -1270,2 +1360,2 @@\n-      \/\/ rscratch2 is the byte adjustment needed to align s.\n-      __ cbz(rscratch2, aligned);\n+      \/\/ r15 is the byte adjustment needed to align s.\n+      __ cbz(r15, aligned);\n@@ -1273,2 +1363,2 @@\n-      if (shift)  __ lsr(rscratch2, rscratch2, shift);\n-      __ sub(count, count, rscratch2);\n+      if (shift)  __ lsr(r15, r15, shift);\n+      __ sub(count, count, r15);\n@@ -1286,2 +1376,2 @@\n-        __ sub(s, s, rscratch2);\n-        __ sub(d, d, rscratch2);\n+        __ sub(s, s, r15);\n+        __ sub(d, d, r15);\n@@ -1289,2 +1379,2 @@\n-        __ add(s, s, rscratch2);\n-        __ add(d, d, rscratch2);\n+        __ add(s, s, r15);\n+        __ add(d, d, r15);\n@@ -1293,1 +1383,1 @@\n-      copy_memory_small(s, d, rscratch2, rscratch1, step);\n+      copy_memory_small(decorators, type, s, d, r15, step);\n@@ -1303,5 +1393,18 @@\n-    __ lsr(rscratch2, count, exact_log2(wordSize\/granularity));\n-    if (direction == copy_forwards)\n-      __ bl(copy_f);\n-    else\n-      __ bl(copy_b);\n+    __ lsr(r15, count, exact_log2(wordSize\/granularity));\n+    if (direction == copy_forwards) {\n+      if (type != T_OBJECT) {\n+        __ bl(copy_f);\n+      } else if ((decorators & IS_DEST_UNINITIALIZED) != 0) {\n+        __ bl(copy_obj_uninit_f);\n+      } else {\n+        __ bl(copy_obj_f);\n+      }\n+    } else {\n+      if (type != T_OBJECT) {\n+        __ bl(copy_b);\n+      } else if ((decorators & IS_DEST_UNINITIALIZED) != 0) {\n+        __ bl(copy_obj_uninit_b);\n+      } else {\n+        __ bl(copy_obj_b);\n+      }\n+    }\n@@ -1310,1 +1413,1 @@\n-    copy_memory_small(s, d, count, tmp, step);\n+    copy_memory_small(decorators, type, s, d, count, step);\n@@ -1405,1 +1508,1 @@\n-      copy_memory(aligned, s, d, count, rscratch1, size);\n+      copy_memory(decorators, is_oop ? T_OBJECT : T_BYTE, aligned, s, d, count, size);\n@@ -1476,1 +1579,1 @@\n-      copy_memory(aligned, s, d, count, rscratch1, -size);\n+      copy_memory(decorators, is_oop ? T_OBJECT : T_BYTE, aligned, s, d, count, -size);\n@@ -1767,0 +1870,3 @@\n+    \/\/ Registers used as gc temps (r5, r6, r7 are save-on-call)\n+    const Register gct1 = r5, gct2 = r6, gct3 = r7;\n+\n@@ -1819,0 +1925,1 @@\n+    int element_size = UseCompressedOops ? 4 : 8;\n@@ -1844,1 +1951,3 @@\n-    __ store_heap_oop(__ post(to, UseCompressedOops ? 4 : 8), copied_oop, noreg, noreg, noreg, AS_RAW);  \/\/ store the oop\n+    bs->copy_store_at(_masm, decorators, T_OBJECT, element_size,\n+                      __ post(to, element_size), copied_oop, noreg,\n+                      gct1, gct2, gct3);\n@@ -1850,1 +1959,3 @@\n-    __ load_heap_oop(copied_oop, __ post(from, UseCompressedOops ? 4 : 8), noreg, noreg, AS_RAW); \/\/ load the oop\n+    bs->copy_load_at(_masm, decorators, T_OBJECT, element_size,\n+                     copied_oop, noreg, __ post(from, element_size),\n+                     gct1);\n@@ -2447,2 +2558,8 @@\n-    generate_copy_longs(copy_f, r0, r1, rscratch2, copy_forwards);\n-    generate_copy_longs(copy_b, r0, r1, rscratch2, copy_backwards);\n+    generate_copy_longs(IN_HEAP | IS_ARRAY, T_BYTE, copy_f, r0, r1, r15, copy_forwards);\n+    generate_copy_longs(IN_HEAP | IS_ARRAY, T_BYTE, copy_b, r0, r1, r15, copy_backwards);\n+\n+    generate_copy_longs(IN_HEAP | IS_ARRAY, T_OBJECT, copy_obj_f, r0, r1, r15, copy_forwards);\n+    generate_copy_longs(IN_HEAP | IS_ARRAY, T_OBJECT, copy_obj_b, r0, r1, r15, copy_backwards);\n+\n+    generate_copy_longs(IN_HEAP | IS_ARRAY | IS_DEST_UNINITIALIZED, T_OBJECT, copy_obj_uninit_f, r0, r1, r15, copy_forwards);\n+    generate_copy_longs(IN_HEAP | IS_ARRAY | IS_DEST_UNINITIALIZED, T_OBJECT, copy_obj_uninit_b, r0, r1, r15, copy_backwards);\n@@ -6321,4 +6438,4 @@\n-    __ cmhi(decH0, arrangement, decL0, v27);\n-    __ cmhi(decH1, arrangement, decL1, v27);\n-    __ cmhi(decH2, arrangement, decL2, v27);\n-    __ cmhi(decH3, arrangement, decL3, v27);\n+    __ cm(Assembler::HI, decH0, arrangement, decL0, v27);\n+    __ cm(Assembler::HI, decH1, arrangement, decL1, v27);\n+    __ cm(Assembler::HI, decH2, arrangement, decL2, v27);\n+    __ cm(Assembler::HI, decH3, arrangement, decL3, v27);\n@@ -7906,1 +8023,1 @@\n-  void generate_initial() {\n+  void generate_initial_stubs() {\n@@ -7932,0 +8049,6 @@\n+\n+    \/\/ Initialize table for copy memory (arraycopy) check.\n+    if (UnsafeCopyMemory::_table == nullptr) {\n+      UnsafeCopyMemory::create_table(8);\n+    }\n+\n@@ -7954,0 +8077,4 @@\n+\n+    if (UseFastLocking) {\n+      StubRoutines::aarch64::_check_lock_stack = generate_check_lock_stack();\n+    }\n@@ -7956,1 +8083,1 @@\n-  void generate_phase1() {\n+  void generate_continuation_stubs() {\n@@ -7966,1 +8093,1 @@\n-  void generate_all() {\n+  void generate_final_stubs() {\n@@ -7989,4 +8116,0 @@\n-    if (UseSVE == 0) {\n-      StubRoutines::aarch64::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n-    }\n-\n@@ -7996,2 +8119,22 @@\n-    \/\/ countPositives stub for large arrays.\n-    StubRoutines::aarch64::_count_positives = generate_count_positives(StubRoutines::aarch64::_count_positives_long);\n+    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+    if (bs_nm != NULL) {\n+      StubRoutines::aarch64::_method_entry_barrier = generate_method_entry_barrier();\n+    }\n+\n+    StubRoutines::aarch64::_spin_wait = generate_spin_wait();\n+\n+#if defined (LINUX) && !defined (__ARM_FEATURE_ATOMICS)\n+\n+    generate_atomic_entry_points();\n+\n+#endif \/\/ LINUX\n+\n+    StubRoutines::aarch64::set_completed(); \/\/ Inidicate that arraycopy and zero_blocks stubs are generated\n+  }\n+\n+  void generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n+\n+    if (UseSVE == 0) {\n+      StubRoutines::aarch64::_vector_iota_indices = generate_iota_indices(\"iota_indices\");\n+    }\n@@ -8004,0 +8147,6 @@\n+    \/\/ byte_array_inflate stub for large arrays.\n+    StubRoutines::aarch64::_large_byte_array_inflate = generate_large_byte_array_inflate();\n+\n+    \/\/ countPositives stub for large arrays.\n+    StubRoutines::aarch64::_count_positives = generate_count_positives(StubRoutines::aarch64::_count_positives_long);\n+\n@@ -8008,10 +8157,0 @@\n-    \/\/ byte_array_inflate stub for large arrays.\n-    StubRoutines::aarch64::_large_byte_array_inflate = generate_large_byte_array_inflate();\n-\n-    BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-    if (bs_nm != NULL) {\n-      StubRoutines::aarch64::_method_entry_barrier = generate_method_entry_barrier();\n-    }\n-    if (UseFastLocking) {\n-      StubRoutines::aarch64::_check_lock_stack = generate_check_lock_stack();\n-    }\n@@ -8104,10 +8243,1 @@\n-\n-    StubRoutines::aarch64::_spin_wait = generate_spin_wait();\n-\n-#if defined (LINUX) && !defined (__ARM_FEATURE_ATOMICS)\n-\n-    generate_atomic_entry_points();\n-\n-#endif \/\/ LINUX\n-\n-    StubRoutines::aarch64::set_completed();\n+#endif \/\/ COMPILER2_OR_JVMCI\n@@ -8117,8 +8247,18 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n+  StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -8128,6 +8268,2 @@\n-#define UCM_TABLE_MAX_ENTRIES 8\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  if (UnsafeCopyMemory::_table == NULL) {\n-    UnsafeCopyMemory::create_table(UCM_TABLE_MAX_ENTRIES);\n-  }\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/aarch64\/stubGenerator_aarch64.cpp","additions":347,"deletions":211,"binary":false,"changes":558,"status":"modified"},{"patch":"@@ -306,0 +306,12 @@\n+\n+    \/\/ Constants for CRC-32C crypto pmull implementation\n+    0x6992cea2UL, 0x00000000UL,\n+    0x0d3b6092UL, 0x00000000UL,\n+    0x740eef02UL, 0x00000000UL,\n+    0x9e4addf8UL, 0x00000000UL,\n+    0x1c291d04UL, 0x00000000UL,\n+    0xd82c63daUL, 0x00000001UL,\n+    0x384aa63aUL, 0x00000001UL,\n+    0xba4fc28eUL, 0x00000000UL,\n+    0xf20c0dfeUL, 0x00000000UL,\n+    0x4cd00bd6UL, 0x00000001UL,\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.cpp","additions":12,"deletions":0,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -38,2 +38,5 @@\n-  code_size1 = 19000,          \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 45000           \/\/ simply increase if too small (assembler will crash if too small)\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      = 10000,\n+  _continuation_stubs_code_size =  2000,\n+  _compiler_stubs_code_size     = 30000,\n+  _final_stubs_code_size        = 20000\n","filename":"src\/hotspot\/cpu\/aarch64\/stubRoutines_aarch64.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -733,2 +733,0 @@\n-  \/\/ check array\n-  __ null_check(array, arrayOopDesc::length_offset_in_bytes());\n@@ -1752,6 +1750,0 @@\n-  \/\/ We might be moving to a safepoint.  The thread which calls\n-  \/\/ Interpreter::notice_safepoints() will effectively flush its cache\n-  \/\/ when it makes a system call, but we need to do something to\n-  \/\/ ensure that we see the changed dispatch table.\n-  __ membar(MacroAssembler::LoadLoad);\n-\n@@ -1973,6 +1965,0 @@\n-  \/\/ We might be moving to a safepoint.  The thread which calls\n-  \/\/ Interpreter::notice_safepoints() will effectively flush its cache\n-  \/\/ when it makes a system call, but we need to do something to\n-  \/\/ ensure that we see the changed dispatch table.\n-  __ membar(MacroAssembler::LoadLoad);\n-\n@@ -2323,0 +2309,69 @@\n+\/\/ The rmethod register is input and overwritten to be the adapter method for the\n+\/\/ indy call. Link Register (lr) is set to the return address for the adapter and\n+\/\/ an appendix may be pushed to the stack. Registers r0-r3 are clobbered\n+void TemplateTable::load_invokedynamic_entry(Register method) {\n+  \/\/ setup registers\n+  const Register appendix = r0;\n+  const Register cache = r2;\n+  const Register index = r3;\n+  assert_different_registers(method, appendix, cache, index, rcpool);\n+\n+  __ save_bcp();\n+\n+  Label resolved;\n+\n+  __ load_resolved_indy_entry(cache, index);\n+  \/\/ Load-acquire the adapter method to match store-release in ResolvedIndyEntry::fill_in()\n+  __ lea(method, Address(cache, in_bytes(ResolvedIndyEntry::method_offset())));\n+  __ ldar(method, method);\n+\n+  \/\/ Compare the method to zero\n+  __ cbnz(method, resolved);\n+\n+  Bytecodes::Code code = bytecode();\n+\n+  \/\/ Call to the interpreter runtime to resolve invokedynamic\n+  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n+  __ mov(method, code); \/\/ this is essentially Bytecodes::_invokedynamic\n+  __ call_VM(noreg, entry, method);\n+  \/\/ Update registers with resolved info\n+  __ load_resolved_indy_entry(cache, index);\n+  \/\/ Load-acquire the adapter method to match store-release in ResolvedIndyEntry::fill_in()\n+  __ lea(method, Address(cache, in_bytes(ResolvedIndyEntry::method_offset())));\n+  __ ldar(method, method);\n+\n+#ifdef ASSERT\n+  __ cbnz(method, resolved);\n+  __ stop(\"Should be resolved by now\");\n+#endif \/\/ ASSERT\n+  __ bind(resolved);\n+\n+  Label L_no_push;\n+  \/\/ Check if there is an appendix\n+  __ load_unsigned_byte(index, Address(cache, in_bytes(ResolvedIndyEntry::flags_offset())));\n+  __ tbz(index, ResolvedIndyEntry::has_appendix_shift, L_no_push);\n+\n+  \/\/ Get appendix\n+  __ load_unsigned_short(index, Address(cache, in_bytes(ResolvedIndyEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter\n+  \/\/ since the parameter_size includes it.\n+  __ push(method);\n+  __ mov(method, index);\n+  __ load_resolved_reference_at_index(appendix, method);\n+  __ verify_oop(appendix);\n+  __ pop(method);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  \/\/ compute return type\n+  __ load_unsigned_byte(index, Address(cache, in_bytes(ResolvedIndyEntry::result_type_offset())));\n+  \/\/ load return address\n+  \/\/ Return address is loaded into link register(lr) and not pushed to the stack\n+  \/\/ like x86\n+  {\n+    const address table_addr = (address) Interpreter::invoke_return_entry_table_for(code);\n+    __ mov(rscratch1, table_addr);\n+    __ ldr(lr, Address(rscratch1, index, Address::lsl(3)));\n+  }\n+}\n+\n@@ -2329,1 +2384,1 @@\n-                                               bool is_invokedynamic) {\n+                                               bool is_invokedynamic \/*unused*\/) {\n@@ -2350,1 +2405,1 @@\n-  size_t index_size = (is_invokedynamic ? sizeof(u4) : sizeof(u2));\n+  size_t index_size = sizeof(u2);\n@@ -3170,1 +3225,1 @@\n-  if (is_invokedynamic || is_invokehandle) {\n+  if (is_invokehandle) {\n@@ -3239,1 +3294,1 @@\n-  __ load_klass_check_null(r0, recv);\n+  __ load_klass(r0, recv);\n@@ -3327,2 +3382,2 @@\n-  \/\/ Get receiver klass into r3 - also a null check\n-  __ load_klass_check_null(r3, r2);\n+  \/\/ Get receiver klass into r3\n+  __ load_klass(r3, r2);\n@@ -3342,1 +3397,1 @@\n-  \/\/ Get receiver klass into r3 - also a null check\n+  \/\/ Get receiver klass into r3\n@@ -3344,1 +3399,1 @@\n-  __ load_klass_check_null(r3, r2);\n+  __ load_klass(r3, r2);\n@@ -3441,1 +3496,1 @@\n-  prepare_invoke(byte_no, rmethod, r0);\n+  load_invokedynamic_entry(rmethod);\n@@ -3592,1 +3647,0 @@\n-  __ null_check(r0, arrayOopDesc::length_offset_in_bytes());\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":79,"deletions":25,"binary":false,"changes":104,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -45,1 +45,1 @@\n-    assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+    assert(SharedRuntime::polling_page_return_handler_blob() != nullptr,\n@@ -343,1 +343,1 @@\n-    AddressLiteral addrlit((address)NULL, metadata_Relocation::spec(_index));\n+    AddressLiteral addrlit((address)nullptr, metadata_Relocation::spec(_index));\n@@ -348,1 +348,1 @@\n-    AddressLiteral addrlit((address)NULL, oop_Relocation::spec(_index));\n+    AddressLiteral addrlit((address)nullptr, oop_Relocation::spec(_index));\n@@ -407,1 +407,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n","filename":"src\/hotspot\/cpu\/ppc\/c1_CodeStubs_ppc.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2012, 2023 SAP SE. All rights reserved.\n@@ -153,1 +153,1 @@\n-        __ stop(\"locked object is NULL\");\n+        __ stop(\"locked object is null\");\n@@ -174,1 +174,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -214,1 +214,1 @@\n-  MonitorExitStub* stub = NULL;\n+  MonitorExitStub* stub = nullptr;\n@@ -235,1 +235,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -247,1 +247,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -264,1 +264,1 @@\n-  if (o == NULL) {\n+  if (o == nullptr) {\n@@ -275,1 +275,1 @@\n-  int oop_index = __ oop_recorder()->allocate_oop_index(NULL);\n+  int oop_index = __ oop_recorder()->allocate_oop_index(nullptr);\n@@ -278,1 +278,1 @@\n-  AddressLiteral addrlit((address)NULL, oop_Relocation::spec(oop_index));\n+  AddressLiteral addrlit((address)nullptr, oop_Relocation::spec(oop_index));\n@@ -293,1 +293,1 @@\n-  int index = __ oop_recorder()->allocate_metadata_index(NULL);\n+  int index = __ oop_recorder()->allocate_metadata_index(nullptr);\n@@ -296,1 +296,1 @@\n-  AddressLiteral addrlit((address)NULL, metadata_Relocation::spec(index));\n+  AddressLiteral addrlit((address)nullptr, metadata_Relocation::spec(index));\n@@ -449,4 +449,4 @@\n-  assert(op->block() == NULL || op->block()->label() == op->label(), \"wrong label\");\n-  if (op->block() != NULL)  _branch_target_blocks.append(op->block());\n-  if (op->ublock() != NULL) _branch_target_blocks.append(op->ublock());\n-  assert(op->info() == NULL, \"shouldn't have CodeEmitInfo\");\n+  assert(op->block() == nullptr || op->block()->label() == op->label(), \"wrong label\");\n+  if (op->block() != nullptr)  _branch_target_blocks.append(op->block());\n+  if (op->ublock() != nullptr) _branch_target_blocks.append(op->ublock());\n+  assert(op->info() == nullptr, \"shouldn't have CodeEmitInfo\");\n@@ -462,1 +462,1 @@\n-      assert(op->ublock() != NULL, \"must have unordered successor\");\n+      assert(op->ublock() != nullptr, \"must have unordered successor\");\n@@ -639,1 +639,1 @@\n-  if (entry_point_toc_addr == NULL) {\n+  if (entry_point_toc_addr == nullptr) {\n@@ -822,4 +822,0 @@\n-          if (VerifyOops) {\n-            BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-            bs->check_oop(_masm, to_reg->as_register(), FILE_AND_LINE); \/\/ kills R0\n-          }\n@@ -856,4 +852,0 @@\n-        if (VerifyOops) {\n-          BarrierSetAssembler* bs = BarrierSet::barrier_set()->barrier_set_assembler();\n-          bs->check_oop(_masm, to_reg->as_register(), FILE_AND_LINE); \/\/ kills R0\n-        }\n@@ -925,1 +917,1 @@\n-  if (info != NULL && needs_explicit_null_check) {\n+  if (info != nullptr && needs_explicit_null_check) {\n@@ -945,1 +937,1 @@\n-      if (UseCompressedOops && !wide && c->as_jobject() != NULL) {\n+      if (UseCompressedOops && !wide && c->as_jobject() != nullptr) {\n@@ -969,1 +961,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -1022,1 +1014,1 @@\n-          if (const_addr == NULL) {\n+          if (const_addr == nullptr) {\n@@ -1041,1 +1033,1 @@\n-          if (const_addr == NULL) {\n+          if (const_addr == nullptr) {\n@@ -1134,1 +1126,1 @@\n-  if (info != NULL && needs_explicit_null_check) {\n+  if (info != nullptr && needs_explicit_null_check) {\n@@ -1142,1 +1134,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -1176,1 +1168,1 @@\n-  if (patch != NULL) {\n+  if (patch != nullptr) {\n@@ -1179,1 +1171,1 @@\n-  if (info != NULL && !needs_explicit_null_check) {\n+  if (info != nullptr && !needs_explicit_null_check) {\n@@ -1256,1 +1248,1 @@\n-  if (info != NULL && needs_explicit_null_check) {\n+  if (info != nullptr && needs_explicit_null_check) {\n@@ -1264,1 +1256,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -1307,1 +1299,1 @@\n-  if (patch != NULL) {\n+  if (patch != nullptr) {\n@@ -1311,1 +1303,1 @@\n-  if (info != NULL && !needs_explicit_null_check) {\n+  if (info != nullptr && !needs_explicit_null_check) {\n@@ -1354,1 +1346,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -1368,1 +1360,1 @@\n-  if (stub == NULL) {\n+  if (stub == nullptr) {\n@@ -1390,1 +1382,1 @@\n-  AddressLiteral ic = __ allocate_metadata_address((Metadata *)NULL);\n+  AddressLiteral ic = __ allocate_metadata_address((Metadata *)nullptr);\n@@ -1446,1 +1438,1 @@\n-            if (con == NULL) {\n+            if (con == nullptr) {\n@@ -1456,1 +1448,1 @@\n-          \/\/ We only need, for now, comparison with NULL for metadata.\n+          \/\/ We only need, for now, comparison with null for metadata.\n@@ -1460,1 +1452,1 @@\n-            if (p == NULL) {\n+            if (p == nullptr) {\n@@ -1542,1 +1534,1 @@\n-    lasm->const2reg(src, dst, lir_patch_none, NULL);\n+    lasm->const2reg(src, dst, lir_patch_none, nullptr);\n@@ -1612,1 +1604,1 @@\n-  assert(info == NULL, \"unused on this code path\");\n+  assert(info == nullptr, \"unused on this code path\");\n@@ -1840,1 +1832,1 @@\n-  BasicType basic_type = default_type != NULL ? default_type->element_type()->basic_type() : T_ILLEGAL;\n+  BasicType basic_type = default_type != nullptr ? default_type->element_type()->basic_type() : T_ILLEGAL;\n@@ -1845,1 +1837,1 @@\n-  const int frame_resize = frame::abi_reg_args_size - sizeof(frame::jit_abi); \/\/ C calls need larger frame.\n+  const int frame_resize = frame::native_abi_reg_args_size - sizeof(frame::java_abi); \/\/ C calls need larger frame.\n@@ -1851,1 +1843,1 @@\n-  if (op->expected_type() == NULL) {\n+  if (op->expected_type() == nullptr) {\n@@ -1855,1 +1847,1 @@\n-    assert(copyfunc_addr != NULL, \"generic arraycopy stub required\");\n+    assert(copyfunc_addr != nullptr, \"generic arraycopy stub required\");\n@@ -1886,1 +1878,1 @@\n-  assert(default_type != NULL && default_type->is_array_klass(), \"must be true at this point\");\n+  assert(default_type != nullptr && default_type->is_array_klass(), \"must be true at this point\");\n@@ -2009,1 +2001,1 @@\n-                                       &cont, copyfunc_addr != NULL ? &copyfunc : &slow, NULL);\n+                                       &cont, copyfunc_addr != nullptr ? &copyfunc : &slow, nullptr);\n@@ -2018,1 +2010,1 @@\n-      if (copyfunc_addr != NULL) { \/\/ Use stub if available.\n+      if (copyfunc_addr != nullptr) { \/\/ Use stub if available.\n@@ -2359,1 +2351,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -2361,1 +2353,1 @@\n-  assert(data != NULL,       \"need data for checkcast\");\n+  assert(data != nullptr,       \"need data for checkcast\");\n@@ -2395,2 +2387,2 @@\n-  ciMethodData* md = NULL;\n-  ciProfileData* data = NULL;\n+  ciMethodData* md = nullptr;\n+  ciProfileData* data = nullptr;\n@@ -2400,1 +2392,1 @@\n-    assert(method != NULL, \"Should have method\");\n+    assert(method != nullptr, \"Should have method\");\n@@ -2448,2 +2440,2 @@\n-      __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, R0, (need_slow_path ? success_target : NULL),\n-                                       failure_target, NULL, RegisterOrConstant(k->super_check_offset()));\n+      __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, R0, (need_slow_path ? success_target : nullptr),\n+                                       failure_target, nullptr, RegisterOrConstant(k->super_check_offset()));\n@@ -2523,2 +2515,2 @@\n-    ciMethodData* md = NULL;\n-    ciProfileData* data = NULL;\n+    ciMethodData* md = nullptr;\n+    ciProfileData* data = nullptr;\n@@ -2528,1 +2520,1 @@\n-      assert(method != NULL, \"Should have method\");\n+      assert(method != nullptr, \"Should have method\");\n@@ -2561,1 +2553,1 @@\n-    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, R0, success_target, &failure, NULL);\n+    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, R0, success_target, &failure, nullptr);\n@@ -2650,1 +2642,1 @@\n-                noreg, NULL, \/*check without ldarx first*\/true);\n+                noreg, nullptr, \/*check without ldarx first*\/true);\n@@ -2700,1 +2692,1 @@\n-      if (op->info() != NULL) {\n+      if (op->info() != nullptr) {\n@@ -2715,1 +2707,1 @@\n-      if (op->info() != NULL) {\n+      if (op->info() != nullptr) {\n@@ -2744,1 +2736,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -2767,1 +2759,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -2769,1 +2761,1 @@\n-  assert(data != NULL && data->is_CounterData(), \"need CounterData for calls\");\n+  assert(data != nullptr && data->is_CounterData(), \"need CounterData for calls\");\n@@ -2797,1 +2789,1 @@\n-    if (C1OptimizeVirtualCallProfiling && known_klass != NULL) {\n+    if (C1OptimizeVirtualCallProfiling && known_klass != nullptr) {\n@@ -2823,1 +2815,1 @@\n-        if (receiver == NULL) {\n+        if (receiver == nullptr) {\n@@ -2891,1 +2883,1 @@\n-    assert(info != NULL, \"sanity\");\n+    assert(info != nullptr, \"sanity\");\n@@ -2898,1 +2890,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -3083,1 +3075,1 @@\n-  bool exact_klass_set = exact_klass != NULL && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n+  bool exact_klass_set = exact_klass != nullptr && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n@@ -3122,1 +3114,1 @@\n-    if (exact_klass != NULL) {\n+    if (exact_klass != nullptr) {\n@@ -3135,1 +3127,1 @@\n-      if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {\n+      if (exact_klass == nullptr || TypeEntries::is_type_none(current_klass)) {\n@@ -3137,1 +3129,1 @@\n-        if (exact_klass != NULL) {\n+        if (exact_klass != nullptr) {\n@@ -3165,1 +3157,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -3178,1 +3170,1 @@\n-      assert(exact_klass != NULL, \"should be\");\n+      assert(exact_klass != nullptr, \"should be\");\n@@ -3203,1 +3195,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n","filename":"src\/hotspot\/cpu\/ppc\/c1_LIRAssembler_ppc.cpp","additions":74,"deletions":82,"binary":false,"changes":156,"status":"modified"},{"patch":"@@ -402,1 +402,1 @@\n-    assert(Lnull != NULL, \"must have Label for explicit check\");\n+    assert(Lnull != nullptr, \"must have Label for explicit check\");\n","filename":"src\/hotspot\/cpu\/ppc\/c1_MacroAssembler_ppc.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -3,1 +3,1 @@\n- * Copyright (c) 2012, 2022 SAP SE. All rights reserved.\n+ * Copyright (c) 2012, 2023 SAP SE. All rights reserved.\n@@ -278,1 +278,1 @@\n-  if (const_address == NULL) { return false; } \/\/ allocation failure\n+  if (const_address == nullptr) { return false; } \/\/ allocation failure\n@@ -365,1 +365,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs a Recorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs a Recorder\");\n@@ -372,1 +372,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs a Recorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs a Recorder\");\n@@ -379,1 +379,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -385,1 +385,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -470,1 +470,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -710,1 +710,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -734,1 +734,1 @@\n-    std(tmp, frame::abi_minframe_size + m * 8, R1_SP);\n+    std(tmp, frame::native_abi_minframe_size + m * 8, R1_SP);\n@@ -980,1 +980,1 @@\n-\/\/ Push a frame of size `bytes' plus abi_reg_args on top.\n+\/\/ Push a frame of size `bytes' plus native_abi_reg_args on top.\n@@ -982,1 +982,1 @@\n-  push_frame(bytes + frame::abi_reg_args_size, tmp);\n+  push_frame(bytes + frame::native_abi_reg_args_size, tmp);\n@@ -989,1 +989,1 @@\n-  push_frame(bytes + frame::abi_reg_args_size + frame::spill_nonvolatiles_size, tmp);\n+  push_frame(bytes + frame::native_abi_reg_args_size + frame::spill_nonvolatiles_size, tmp);\n@@ -1090,1 +1090,1 @@\n-        || fd == NULL   \/\/ support code-size estimation\n+        || fd == nullptr   \/\/ support code-size estimation\n@@ -1092,1 +1092,1 @@\n-        || fd->entry() == NULL) {\n+        || fd->entry() == nullptr) {\n@@ -1097,1 +1097,1 @@\n-      bool has_env = (fd != NULL && fd->env() != NULL);\n+      bool has_env = (fd != nullptr && fd->env() != nullptr);\n@@ -1154,1 +1154,1 @@\n-    assert(fd->entry() != NULL, \"function must be linked\");\n+    assert(fd->entry() != nullptr, \"function must be linked\");\n@@ -1159,1 +1159,1 @@\n-    if (fd->env() == NULL) {\n+    if (fd->env() == nullptr) {\n@@ -1171,1 +1171,1 @@\n-    if (!success) { return NULL; }\n+    if (!success) { return nullptr; }\n@@ -1310,2 +1310,2 @@\n-    if (polling_address_ptr != NULL) {\n-      *polling_address_ptr = NULL;\n+    if (polling_address_ptr != nullptr) {\n+      *polling_address_ptr = nullptr;\n@@ -1322,1 +1322,1 @@\n-  if (polling_address_ptr != NULL) {\n+  if (polling_address_ptr != nullptr) {\n@@ -1327,1 +1327,1 @@\n-  \/\/ Not on Linux, ucontext must be NULL.\n+  \/\/ Not on Linux, ucontext must be null.\n@@ -1389,1 +1389,1 @@\n-    return ra != 1 || rb_val >= 0 ? NULL         \/\/ not a stack bang\n+    return ra != 1 || rb_val >= 0 ? nullptr         \/\/ not a stack bang\n@@ -1392,1 +1392,1 @@\n-  return NULL; \/\/ not a stack bang\n+  return nullptr; \/\/ not a stack bang\n@@ -1396,1 +1396,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1705,1 +1705,1 @@\n-  Label& failed = (failed_ext != NULL) ? *failed_ext : failed_int;\n+  Label& failed = (failed_ext != nullptr) ? *failed_ext : failed_int;\n@@ -1713,1 +1713,1 @@\n-  assert(int_flag_success == noreg || failed_ext == NULL, \"cannot have both\");\n+  assert(int_flag_success == noreg || failed_ext == nullptr, \"cannot have both\");\n@@ -1820,1 +1820,1 @@\n-  \/\/ for (scan = klass->itable(); scan->interface() != NULL; scan += scan_step) {\n+  \/\/ for (scan = klass->itable(); scan->interface() != nullptr; scan += scan_step) {\n@@ -1905,3 +1905,3 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  if (L_slow_path == NULL) { L_slow_path = &L_fallthrough; label_nulls++; }\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_slow_path == nullptr) { L_slow_path = &L_fallthrough; label_nulls++; }\n@@ -1910,1 +1910,1 @@\n-         \"at most one NULL in the batch, usually\");\n+         \"at most one null in the batch, usually\");\n@@ -2024,1 +2024,1 @@\n-  if (L_success != NULL) { b(*L_success); }\n+  if (L_success != nullptr) { b(*L_success); }\n@@ -2043,1 +2043,1 @@\n-  assert(L_fast_path != NULL || L_slow_path != NULL, \"at least one is required\");\n+  assert(L_fast_path != nullptr || L_slow_path != nullptr, \"at least one is required\");\n@@ -2046,1 +2046,1 @@\n-  if (L_fast_path == NULL) {\n+  if (L_fast_path == nullptr) {\n@@ -2048,1 +2048,1 @@\n-  } else if (L_slow_path == NULL) {\n+  } else if (L_slow_path == nullptr) {\n@@ -2137,1 +2137,1 @@\n-  if (stub == NULL) { return NULL; } \/\/ CodeCache full: bail out\n+  if (stub == nullptr) { return nullptr; } \/\/ CodeCache full: bail out\n@@ -2353,1 +2353,1 @@\n-  if (method_data != NULL) {\n+  if (method_data != nullptr) {\n@@ -2374,1 +2374,1 @@\n-  if (method_data != NULL) {\n+  if (method_data != nullptr) {\n@@ -2390,1 +2390,1 @@\n-  assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+  assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -2397,1 +2397,1 @@\n-    assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -2501,1 +2501,1 @@\n-    assert(stack_rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(stack_rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -2568,1 +2568,1 @@\n-    assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -2711,1 +2711,1 @@\n-  \/\/ The object's monitor m is unlocked iff m->owner == NULL,\n+  \/\/ The object's monitor m is unlocked iff m->owner is null,\n@@ -2722,1 +2722,1 @@\n-  \/\/ Try to CAS m->owner from NULL to current thread.\n+  \/\/ Try to CAS m->owner from null to current thread.\n@@ -2915,1 +2915,1 @@\n-  \/\/ last_Java_pc will always be set to NULL. It is set here so that\n+  \/\/ last_Java_pc will always be set to null. It is set here so that\n@@ -4304,1 +4304,1 @@\n-  bool msg_present = (msg != NULL);\n+  bool msg_present = (msg != nullptr);\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":47,"deletions":47,"binary":false,"changes":94,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -204,1 +204,1 @@\n-  guarantee(osr_entry != NULL, \"NULL osr_entry!\");\n+  guarantee(osr_entry != nullptr, \"null osr_entry!\");\n@@ -254,1 +254,1 @@\n-        __ stop(\"locked object is NULL\");\n+        __ stop(\"locked object is null\");\n@@ -291,1 +291,1 @@\n-  if (o == NULL) {\n+  if (o == nullptr) {\n@@ -312,1 +312,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -359,1 +359,1 @@\n-  MonitorExitStub* stub = NULL;\n+  MonitorExitStub* stub = nullptr;\n@@ -387,1 +387,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -397,1 +397,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -430,1 +430,1 @@\n-  guarantee(info != NULL, \"Shouldn't be NULL\");\n+  guarantee(info != nullptr, \"Shouldn't be null\");\n@@ -448,1 +448,1 @@\n-  address const_addr = NULL;\n+  address const_addr = nullptr;\n@@ -485,1 +485,1 @@\n-      assert(const_addr != NULL, \"must create float constant in the constant table\");\n+      assert(const_addr != nullptr, \"must create float constant in the constant table\");\n@@ -491,1 +491,1 @@\n-      assert(const_addr != NULL, \"must create double constant in the constant table\");\n+      assert(const_addr != nullptr, \"must create double constant in the constant table\");\n@@ -506,1 +506,1 @@\n-      if (c->as_jobject() == NULL) {\n+      if (c->as_jobject() == nullptr) {\n@@ -509,1 +509,1 @@\n-        const2reg(src, FrameMap::t1_opr, lir_patch_none, NULL);\n+        const2reg(src, FrameMap::t1_opr, lir_patch_none, nullptr);\n@@ -514,1 +514,1 @@\n-      const2reg(src, FrameMap::t1_opr, lir_patch_none, NULL);\n+      const2reg(src, FrameMap::t1_opr, lir_patch_none, nullptr);\n@@ -585,1 +585,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -733,1 +733,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -803,1 +803,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -904,1 +904,1 @@\n-  move_op(opr2, result, type, lir_patch_none, NULL,\n+  move_op(opr2, result, type, lir_patch_none, nullptr,\n@@ -909,1 +909,1 @@\n-  move_op(opr1, result, type, lir_patch_none, NULL,\n+  move_op(opr1, result, type, lir_patch_none, nullptr,\n@@ -918,1 +918,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -1081,1 +1081,1 @@\n-  assert(method != NULL, \"Should have method\");\n+  assert(method != nullptr, \"Should have method\");\n@@ -1084,1 +1084,1 @@\n-  guarantee(*md != NULL, \"Sanity\");\n+  guarantee(*md != nullptr, \"Sanity\");\n@@ -1086,1 +1086,1 @@\n-  assert(*data != NULL, \"need data for type check\");\n+  assert(*data != nullptr, \"need data for type check\");\n@@ -1121,1 +1121,1 @@\n-    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -1168,2 +1168,2 @@\n-  ciMethodData* md = NULL;\n-  ciProfileData* data = NULL;\n+  ciMethodData* md = nullptr;\n+  ciProfileData* data = nullptr;\n@@ -1297,1 +1297,1 @@\n-      if (Assembler::operand_valid_for_add_immediate(right_const)) {\n+      if (Assembler::is_simm12(right_const)) {\n@@ -1312,1 +1312,1 @@\n-      if (Assembler::operand_valid_for_add_immediate(right_const)) {\n+      if (Assembler::is_simm12(right_const)) {\n@@ -1357,1 +1357,1 @@\n-  if (call == NULL) {\n+  if (call == nullptr) {\n@@ -1367,1 +1367,1 @@\n-  if (call == NULL) {\n+  if (call == nullptr) {\n@@ -1379,1 +1379,1 @@\n-  if (stub == NULL) {\n+  if (stub == nullptr) {\n@@ -1503,1 +1503,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -1512,1 +1512,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -1529,1 +1529,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -1547,1 +1547,1 @@\n-  guarantee(md != NULL, \"Sanity\");\n+  guarantee(md != nullptr, \"Sanity\");\n@@ -1549,1 +1549,1 @@\n-  assert(data != NULL && data->is_CounterData(), \"need CounterData for calls\");\n+  assert(data != nullptr && data->is_CounterData(), \"need CounterData for calls\");\n@@ -1562,1 +1562,1 @@\n-    if (C1OptimizeVirtualCallProfiling && known_klass != NULL) {\n+    if (C1OptimizeVirtualCallProfiling && known_klass != nullptr) {\n@@ -1585,1 +1585,1 @@\n-        if (receiver == NULL) {\n+        if (receiver == nullptr) {\n@@ -1621,2 +1621,2 @@\n-  if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {\n-    if (exact_klass != NULL) {\n+  if (exact_klass == nullptr || TypeEntries::is_type_none(current_klass)) {\n+    if (exact_klass != nullptr) {\n@@ -1636,1 +1636,1 @@\n-    __ andi(t0, tmp, TypeEntries::type_unknown);\n+    __ test_bit(t0, tmp, exact_log2(TypeEntries::type_unknown));\n@@ -1653,1 +1653,1 @@\n-    assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+    assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -1658,1 +1658,1 @@\n-    __ andi(t0, tmp, TypeEntries::type_unknown);\n+    __ test_bit(t0, tmp, exact_log2(TypeEntries::type_unknown));\n@@ -1679,1 +1679,1 @@\n-  assert(exact_klass != NULL, \"should be\");\n+  assert(exact_klass != nullptr, \"should be\");\n@@ -1708,1 +1708,1 @@\n-    assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+    assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -1713,1 +1713,1 @@\n-    __ andi(t0, tmp, TypeEntries::type_unknown);\n+    __ test_bit(t0, tmp, exact_log2(TypeEntries::type_unknown));\n@@ -1747,1 +1747,1 @@\n-  bool exact_klass_set = exact_klass != NULL && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n+  bool exact_klass_set = exact_klass != nullptr && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n@@ -1772,1 +1772,1 @@\n-    if (exact_klass != NULL) {\n+    if (exact_klass != nullptr) {\n@@ -1828,1 +1828,1 @@\n-    if (!is_imm_in_range(offset, 12, 0)) {\n+    if (!Assembler::is_simm12(offset)) {\n@@ -1843,1 +1843,1 @@\n-  if (cb != NULL) {\n+  if (cb != nullptr) {\n@@ -1854,1 +1854,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -1995,1 +1995,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -2005,1 +2005,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -2015,1 +2015,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -2045,1 +2045,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n@@ -2112,2 +2112,2 @@\n-  ciMethodData* md = NULL;\n-  ciProfileData* data = NULL;\n+  ciMethodData* md = nullptr;\n+  ciProfileData* data = nullptr;\n@@ -2182,1 +2182,1 @@\n-  __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+  __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -2202,1 +2202,1 @@\n-        const2reg(src, FrameMap::t0_opr, lir_patch_none, NULL);\n+        const2reg(src, FrameMap::t0_opr, lir_patch_none, nullptr);\n@@ -2205,1 +2205,1 @@\n-        const2reg(src, FrameMap::t0_long_opr, lir_patch_none, NULL);\n+        const2reg(src, FrameMap::t0_long_opr, lir_patch_none, nullptr);\n","filename":"src\/hotspot\/cpu\/riscv\/c1_LIRAssembler_riscv.cpp","additions":61,"deletions":61,"binary":false,"changes":122,"status":"modified"},{"patch":"@@ -69,1 +69,1 @@\n-    andi(t0, hdr, JVM_ACC_IS_VALUE_BASED_CLASS);\n+    test_bit(t0, hdr, exact_log2(JVM_ACC_IS_VALUE_BASED_CLASS));\n@@ -87,1 +87,1 @@\n-    cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/NULL);\n+    cmpxchgptr(hdr, disp_hdr, t1, t0, done, \/*fallthough*\/nullptr);\n@@ -106,1 +106,1 @@\n-    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n@@ -132,1 +132,1 @@\n-    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if the loaded hdr is null we had recursive locking\n@@ -314,1 +314,1 @@\n-  \/\/ explicit NULL check not needed since load from [klass_offset] causes a trap\n+  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n@@ -330,1 +330,1 @@\n-  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n@@ -414,2 +414,2 @@\n-  NULL, \/\/ lir_cond_belowEqual\n-  NULL, \/\/ lir_cond_aboveEqual\n+  nullptr, \/\/ lir_cond_belowEqual\n+  nullptr, \/\/ lir_cond_aboveEqual\n@@ -424,2 +424,2 @@\n-  NULL, \/\/ lir_cond_belowEqual\n-  NULL  \/\/ lir_cond_aboveEqual\n+  nullptr, \/\/ lir_cond_belowEqual\n+  nullptr  \/\/ lir_cond_aboveEqual\n","filename":"src\/hotspot\/cpu\/riscv\/c1_MacroAssembler_riscv.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-  assert(entry != NULL, \"Entry must have been generated by now\");\n+  assert(entry != nullptr, \"Entry must have been generated by now\");\n@@ -102,1 +102,1 @@\n-    andi(t0, t1, JavaThread::popframe_pending_bit);\n+    test_bit(t0, t1, exact_log2(JavaThread::popframe_pending_bit));\n@@ -104,1 +104,1 @@\n-    andi(t0, t1, JavaThread::popframe_processing_bit);\n+    test_bit(t0, t1, exact_log2(JavaThread::popframe_processing_bit));\n@@ -159,1 +159,1 @@\n-    beqz(t0, L);  \/\/ if [thread->jvmti_thread_state() == NULL] then exit\n+    beqz(t0, L);  \/\/ if thread->jvmti_thread_state() is null then exit\n@@ -526,1 +526,1 @@\n-    andi(t1, t1, SafepointMechanism::poll_bit());\n+    test_bit(t1, t1, exact_log2(SafepointMechanism::poll_bit()));\n@@ -623,1 +623,1 @@\n-  andi(t0, x12, JVM_ACC_SYNCHRONIZED);\n+  test_bit(t0, x12, exact_log2(JVM_ACC_SYNCHRONIZED));\n@@ -808,1 +808,1 @@\n-      andi(tmp, tmp, JVM_ACC_IS_VALUE_BASED_CLASS);\n+      test_bit(tmp, tmp, exact_log2(JVM_ACC_IS_VALUE_BASED_CLASS));\n@@ -827,1 +827,1 @@\n-      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+      cmpxchg_obj_header(swap_reg, lock_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n@@ -929,1 +929,1 @@\n-      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/NULL);\n+      cmpxchg_obj_header(swap_reg, header_reg, obj_reg, t0, count, \/*fallthrough*\/nullptr);\n@@ -961,1 +961,1 @@\n-  \/\/ Test MDO to avoid the call if it is NULL.\n+  \/\/ Test MDO to avoid the call if it is null.\n@@ -1332,1 +1332,1 @@\n-  \/\/ observed the item[start_row] is NULL.\n+  \/\/ observed the item[start_row] is null.\n@@ -1350,1 +1350,1 @@\n-\/\/   if (row[0].rec != NULL) then [\n+\/\/   if (row[0].rec != nullptr) then [\n@@ -1356,1 +1356,1 @@\n-\/\/     if (row[1].rec != NULL) then [\n+\/\/     if (row[1].rec != nullptr) then [\n@@ -1362,1 +1362,1 @@\n-\/\/       if (row[2].rec != NULL) then [\n+\/\/       if (row[2].rec != nullptr) then [\n@@ -1644,1 +1644,1 @@\n-        \" last_sp != NULL\");\n+        \" last_sp isn't null\");\n@@ -1671,1 +1671,1 @@\n-         \" last_sp != NULL\");\n+         \" last_sp isn't null\");\n@@ -1704,1 +1704,1 @@\n-  andi(t0, obj, TypeEntries::type_unknown);\n+  test_bit(t0, obj, exact_log2(TypeEntries::type_unknown));\n@@ -1947,0 +1947,12 @@\n+void InterpreterMacroAssembler::load_resolved_indy_entry(Register cache, Register index) {\n+  \/\/ Get index out of bytecode pointer, get_cache_entry_pointer_at_bcp\n+  get_cache_index_at_bcp(index, 1, sizeof(u4));\n+  \/\/ Get address of invokedynamic array\n+  ld(cache, Address(xcpool, in_bytes(ConstantPoolCache::invokedynamic_entries_offset())));\n+  \/\/ Scale the index to be the entry index * sizeof(ResolvedInvokeDynamicInfo)\n+  slli(index, index, log2i_exact(sizeof(ResolvedIndyEntry)));\n+  add(cache, cache, Array<ResolvedIndyEntry>::base_offset_in_bytes());\n+  add(cache, cache, index);\n+  la(cache, Address(cache, 0));\n+}\n+\n@@ -1960,1 +1972,1 @@\n-void InterpreterMacroAssembler::verify_access_flags(Register access_flags, uint32_t flag_bits,\n+void InterpreterMacroAssembler::verify_access_flags(Register access_flags, uint32_t flag,\n@@ -1963,1 +1975,1 @@\n-  andi(t0, access_flags, flag_bits);\n+  test_bit(t0, access_flags, exact_log2(flag));\n","filename":"src\/hotspot\/cpu\/riscv\/interp_masm_riscv.cpp","additions":31,"deletions":19,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -258,1 +258,1 @@\n-  assert(last_java_pc != NULL, \"must provide a valid PC\");\n+  assert(last_java_pc != nullptr, \"must provide a valid PC\");\n@@ -367,1 +367,1 @@\n-  assert(L_fast_path != NULL || L_slow_path != NULL, \"at least one is required\");\n+  assert(L_fast_path != nullptr || L_slow_path != nullptr, \"at least one is required\");\n@@ -371,1 +371,1 @@\n-  if (L_fast_path == NULL) {\n+  if (L_fast_path == nullptr) {\n@@ -373,1 +373,1 @@\n-  } else if (L_slow_path == NULL) {\n+  } else if (L_slow_path == nullptr) {\n@@ -400,1 +400,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -439,1 +439,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -563,1 +563,1 @@\n-  beqz(value, done);           \/\/ Use NULL as-is.\n+  beqz(value, done);           \/\/ Use null as-is.\n@@ -575,1 +575,1 @@\n-  andi(t0, value, JNIHandles::TypeTag::weak_global);\n+  test_bit(t0, value, exact_log2(JNIHandles::TypeTag::weak_global));\n@@ -596,1 +596,1 @@\n-  beqz(value, done);           \/\/ Use NULL as-is.\n+  beqz(value, done);           \/\/ Use null as-is.\n@@ -601,1 +601,1 @@\n-    andi(t0, value, JNIHandles::TypeTag::global); \/\/ Test for global tag.\n+    test_bit(t0, value, exact_log2(JNIHandles::TypeTag::global)); \/\/ Test for global tag.\n@@ -623,1 +623,1 @@\n-  const char* buf = NULL;\n+  const char* buf = nullptr;\n@@ -638,1 +638,1 @@\n-  mov_metadata(xmethod, (Metadata*)NULL);\n+  mov_metadata(xmethod, (Metadata*)nullptr);\n@@ -651,1 +651,1 @@\n-  if (retaddr != NULL) {\n+  if (retaddr != nullptr) {\n@@ -719,1 +719,1 @@\n-  if (is_offset_in_range(offset, 32)) {\n+  if (is_simm32(offset)) {\n@@ -799,1 +799,1 @@\n-  if (do_compress() && (is_imm_in_range(imm, 6, 0) && Rd != x0)) {\n+  if (do_compress() && (is_simm6(imm) && Rd != x0)) {\n@@ -836,1 +836,1 @@\n-    assert_cond(dest != NULL);                                     \\\n+    assert_cond(dest != nullptr);                                  \\\n@@ -838,1 +838,1 @@\n-    if (is_imm_in_range(distance, 20, 1)) {                        \\\n+    if (is_simm21(distance) && ((distance % 2) == 0)) {            \\\n@@ -880,1 +880,1 @@\n-    assert_cond(dest != NULL);                                                        \\\n+    assert_cond(dest != nullptr);                                                     \\\n@@ -882,1 +882,1 @@\n-    if (is_imm_in_range(distance, 20, 1)) {                                           \\\n+    if (is_simm21(distance) && ((distance % 2) == 0)) {                               \\\n@@ -1348,1 +1348,2 @@\n-  assert(is_imm_in_range(offset, 20, 1), \"offset is too large to be patched in one jal insrusction!\\n\");\n+  assert(Assembler::is_simm21(offset) && ((offset % 2) == 0),\n+         \"offset is too large to be patched in one jal instruction!\\n\");\n@@ -1357,1 +1358,2 @@\n-  assert(is_imm_in_range(offset, 12, 1), \"offset is too large to be patched in one beq\/bge\/bgeu\/blt\/bltu\/bne insrusction!\\n\");\n+  assert(Assembler::is_simm13(offset) && ((offset % 2) == 0),\n+         \"offset is too large to be patched in one beq\/bge\/bgeu\/blt\/bltu\/bne instruction!\\n\");\n@@ -1416,1 +1418,1 @@\n-  assert_cond(insn_addr != NULL);\n+  assert_cond(insn_addr != nullptr);\n@@ -1430,1 +1432,1 @@\n-  assert_cond(insn_addr != NULL);\n+  assert_cond(insn_addr != nullptr);\n@@ -1442,1 +1444,1 @@\n-  assert_cond(insn_addr != NULL);\n+  assert_cond(insn_addr != nullptr);\n@@ -1450,1 +1452,1 @@\n-  assert_cond(insn_addr != NULL);\n+  assert_cond(insn_addr != nullptr);\n@@ -1459,1 +1461,1 @@\n-  assert_cond(insn_addr != NULL);\n+  assert_cond(insn_addr != nullptr);\n@@ -1469,1 +1471,1 @@\n-  assert_cond(insn_addr != NULL);\n+  assert_cond(insn_addr != nullptr);\n@@ -1478,1 +1480,1 @@\n-  assert_cond(branch != NULL);\n+  assert_cond(branch != nullptr);\n@@ -1506,1 +1508,1 @@\n-  assert_cond(insn_addr != NULL);\n+  assert_cond(insn_addr != nullptr);\n@@ -1565,2 +1567,1 @@\n-  assert(is_unsigned_imm_in_range(imm64, 47, 0) || (imm64 == (int64_t)-1),\n-         \"bit 47 overflows in address constant\");\n+  assert((uintptr_t)imm64 < (1ull << 48), \"48-bit overflow in address constant\");\n@@ -1586,1 +1587,1 @@\n-  if (is_imm_in_range(increment, 12, 0)) {\n+  if (is_simm12(increment)) {\n@@ -1596,1 +1597,1 @@\n-  if (is_imm_in_range(increment, 12, 0)) {\n+  if (is_simm12(increment)) {\n@@ -1606,1 +1607,1 @@\n-  if (is_imm_in_range(-decrement, 12, 0)) {\n+  if (is_simm12(-decrement)) {\n@@ -1616,1 +1617,1 @@\n-  if (is_imm_in_range(-decrement, 12, 0)) {\n+  if (is_simm12(-decrement)) {\n@@ -1852,1 +1853,1 @@\n-  if (is_imm_in_range(imm, 12, 0)) {\n+  if (is_simm12(imm)) {\n@@ -1866,1 +1867,1 @@\n-    if (is_imm_in_range(src.as_constant(), 12, 0)) {\n+    if (is_simm12(src.as_constant())) {\n@@ -1881,1 +1882,1 @@\n-    if (CompressedKlassPointers::base() == NULL) {\n+    if (CompressedKlassPointers::base() == nullptr) {\n@@ -1896,1 +1897,1 @@\n-  if (obj == NULL) {\n+  if (obj == nullptr) {\n@@ -1920,1 +1921,1 @@\n-  if (obj == NULL) {\n+  if (obj == nullptr) {\n@@ -1976,1 +1977,1 @@\n-  _masm = NULL;\n+  _masm = nullptr;\n@@ -2025,1 +2026,1 @@\n-    \/\/ provoke OS NULL exception if reg = NULL by\n+    \/\/ provoke OS null exception if reg is null by\n@@ -2031,1 +2032,1 @@\n-    \/\/ will provoke OS NULL exception if reg = NULL\n+    \/\/ will provoke OS null exception if reg is null\n@@ -2051,1 +2052,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -2082,5 +2083,0 @@\n-void MacroAssembler::load_klass_check_null(Register dst, Register src, Register tmp) {\n-  null_check(src, oopDesc::klass_offset_in_bytes());\n-  load_klass(dst, src, tmp);\n-}\n-\n@@ -2113,1 +2109,1 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n+  if (CompressedKlassPointers::base() == nullptr) {\n@@ -2148,1 +2144,1 @@\n-  if (CompressedKlassPointers::base() == NULL) {\n+  if (CompressedKlassPointers::base() == nullptr) {\n@@ -2184,1 +2180,1 @@\n-  assert(Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert(Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -2191,1 +2187,1 @@\n-    if (CompressedOops::base() != NULL) {\n+    if (CompressedOops::base() != nullptr) {\n@@ -2195,1 +2191,1 @@\n-    assert(CompressedOops::base() == NULL, \"sanity\");\n+    assert(CompressedOops::base() == nullptr, \"sanity\");\n@@ -2201,1 +2197,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -2230,1 +2226,1 @@\n-\/\/ Used for storing NULLs.\n+\/\/ Used for storing nulls.\n@@ -2373,1 +2369,1 @@\n-  if (last != NULL && nativeInstruction_at(last)->is_membar() && prev == last) {\n+  if (last != nullptr && nativeInstruction_at(last)->is_membar() && prev == last) {\n@@ -2395,1 +2391,1 @@\n-  if (is_offset_in_range(byte_offset, 12)) { \/\/ 12: imm in range 2^12\n+  if (is_simm12(byte_offset)) { \/\/ 12: imm in range 2^12\n@@ -2412,2 +2408,2 @@\n-  check_klass_subtype_fast_path(sub_klass, super_klass, tmp_reg, &L_success, &L_failure, NULL);\n-  check_klass_subtype_slow_path(sub_klass, super_klass, tmp_reg, noreg, &L_success, NULL);\n+  check_klass_subtype_fast_path(sub_klass, super_klass, tmp_reg, &L_success, &L_failure, nullptr);\n+  check_klass_subtype_slow_path(sub_klass, super_klass, tmp_reg, noreg, &L_success, nullptr);\n@@ -2425,1 +2421,1 @@\n-    andi(t0, t0, SafepointMechanism::poll_bit());\n+    test_bit(t0, t0, exact_log2(SafepointMechanism::poll_bit()));\n@@ -2450,1 +2446,1 @@\n-  if (fail != NULL) {\n+  if (fail != nullptr) {\n@@ -2714,1 +2710,1 @@\n-  assert(CodeCache::find_blob(entry.target()) != NULL,\n+  assert(CodeCache::find_blob(entry.target()) != nullptr,\n@@ -2735,1 +2731,1 @@\n-  assert(CodeCache::find_blob(entry.target()) != NULL,\n+  assert(CodeCache::find_blob(entry.target()) != nullptr,\n@@ -2771,4 +2767,4 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  if (L_slow_path == NULL) { L_slow_path = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_slow_path == nullptr) { L_slow_path = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in batch\");\n@@ -2857,2 +2853,2 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n@@ -2860,1 +2856,1 @@\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -2981,1 +2977,1 @@\n-  assert(is_valid_riscv64_address(dest.target()), \"bad address\");\n+  assert((uintptr_t)dest.target() < (1ull << 48), \"bad address\");\n@@ -3063,2 +3059,2 @@\n-    assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-    assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+    assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+    assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -3077,1 +3073,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -3106,1 +3102,1 @@\n-        if (stub == NULL) {\n+        if (stub == nullptr) {\n@@ -3108,1 +3104,1 @@\n-          return NULL; \/\/ CodeCache is full\n+          return nullptr; \/\/ CodeCache is full\n@@ -3133,1 +3129,1 @@\n-  assert_cond(entry != NULL);\n+  assert_cond(entry != nullptr);\n@@ -3152,2 +3148,2 @@\n-  if (stub == NULL) {\n-    return NULL;  \/\/ CodeBuffer::expand failed\n+  if (stub == nullptr) {\n+    return nullptr;  \/\/ CodeBuffer::expand failed\n@@ -3215,1 +3211,1 @@\n-           is_offset_in_range(dst.offset(), 12)) || is_imm_in_range(value, 12, 0)),\n+           is_simm12(dst.offset())) || is_simm12(value)),\n@@ -3226,1 +3222,1 @@\n-           is_offset_in_range(dst.offset(), 12)) || is_imm_in_range(value, 12, 0)),\n+           is_simm12(dst.offset())) || is_simm12(value)),\n@@ -3237,1 +3233,1 @@\n-           is_offset_in_range(dst.offset(), 12)) || is_imm_in_range(value, 12, 0)),\n+           is_simm12(dst.offset())) || is_simm12(value)),\n@@ -3248,1 +3244,1 @@\n-           is_offset_in_range(dst.offset(), 12)) || is_imm_in_range(value, 12, 0)),\n+           is_simm12(dst.offset())) || is_simm12(value)),\n@@ -3676,1 +3672,1 @@\n-    andi(t0, t0, 0x1);\n+    test_bit(t0, t0, 0);\n@@ -3902,1 +3898,1 @@\n-    assert(zero_blocks.target() != NULL, \"zero_blocks stub has not been generated\");\n+    assert(zero_blocks.target() != nullptr, \"zero_blocks stub has not been generated\");\n@@ -3905,1 +3901,1 @@\n-      if (tpc == NULL) {\n+      if (tpc == nullptr) {\n@@ -3908,1 +3904,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -3917,1 +3913,1 @@\n-    andi(t0, cnt, i);\n+    test_bit(t0, cnt, exact_log2(i));\n@@ -3927,1 +3923,1 @@\n-    andi(t0, cnt, 1);\n+    test_bit(t0, cnt, 0);\n@@ -4326,1 +4322,1 @@\n-  assert_cond(map != NULL && receiver_offset != NULL);\n+  assert_cond(map != nullptr && receiver_offset != nullptr);\n@@ -4331,1 +4327,1 @@\n-  \/\/ See if oop is NULL if it is we need no handle\n+  \/\/ See if oop is null if it is we need no handle\n@@ -4343,1 +4339,1 @@\n-    \/\/ conditionally move a NULL\n+    \/\/ conditionally move a null\n@@ -4351,1 +4347,1 @@\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-null\n@@ -4378,1 +4374,1 @@\n-    \/\/ Store oop in handle area, may be NULL\n+    \/\/ Store oop in handle area, may be null\n@@ -4540,0 +4536,9 @@\n+\n+void MacroAssembler::test_bit(Register Rd, Register Rs, uint32_t bit_pos, Register tmp) {\n+  assert(bit_pos < 64, \"invalid bit range\");\n+  if (UseZbs) {\n+    bexti(Rd, Rs, bit_pos);\n+    return;\n+  }\n+  andi(Rd, Rs, 1UL << bit_pos, tmp);\n+}\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":100,"deletions":95,"binary":false,"changes":195,"status":"modified"},{"patch":"@@ -30,1 +30,1 @@\n-#include \"asm\/assembler.hpp\"\n+#include \"asm\/assembler.inline.hpp\"\n@@ -157,1 +157,1 @@\n-    Label*  retaddr = NULL\n+    Label*  retaddr = nullptr\n@@ -198,1 +198,0 @@\n-  void load_klass_check_null(Register dst, Register src, Register tmp = t0);\n@@ -222,1 +221,1 @@\n-  \/\/ Used for storing NULL. All other oop constants should be\n+  \/\/ Used for storing null. All other oop constants should be\n@@ -227,1 +226,1 @@\n-  \/\/ converting a zero (linked NULL) into a Register by giving\n+  \/\/ converting a zero (linked null) into a Register by giving\n@@ -232,1 +231,1 @@\n-  \/\/ Support for NULL-checks\n+  \/\/ Support for null-checks\n@@ -234,1 +233,1 @@\n-  \/\/ Generates code that causes a NULL OS exception if the content of reg is NULL.\n+  \/\/ Generates code that causes a null OS exception if the content of reg is null.\n@@ -274,1 +273,1 @@\n-      if (!is_offset_in_range(adr.offset(), 12)) {\n+      if (!is_simm12(adr.offset())) {\n@@ -295,1 +294,1 @@\n-  \/\/ One of the three labels can be NULL, meaning take the fall-through.\n+  \/\/ One of the three labels can be null, meaning take the fall-through.\n@@ -398,1 +397,1 @@\n-  static void pd_patch_instruction(address branch, address target, const char* file = NULL, int line = 0) {\n+  static void pd_patch_instruction(address branch, address target, const char* file = nullptr, int line = 0) {\n@@ -593,1 +592,1 @@\n-    assert_cond(dest != NULL);                                                                           \\\n+    assert_cond(dest != nullptr);                                                                        \\\n@@ -595,1 +594,1 @@\n-    guarantee(is_imm_in_range(offset, 12, 1), \"offset is invalid.\");                                     \\\n+    guarantee(is_simm13(offset) && ((offset % 2) == 0), \"offset is invalid.\");                           \\\n@@ -783,1 +782,1 @@\n-    assert_cond(dest != NULL);                                                                     \\\n+    assert_cond(dest != nullptr);                                                                  \\\n@@ -785,1 +784,1 @@\n-    if (is_offset_in_range(distance, 32)) {                                                        \\\n+    if (is_simm32(distance)) {                                                                     \\\n@@ -806,1 +805,1 @@\n-        if (is_offset_in_range(adr.offset(), 12)) {                                                \\\n+        if (is_simm12(adr.offset())) {                                                             \\\n@@ -840,1 +839,1 @@\n-    assert_cond(dest != NULL);                                                                     \\\n+    assert_cond(dest != nullptr);                                                                  \\\n@@ -842,1 +841,1 @@\n-    if (is_offset_in_range(distance, 32)) {                                                        \\\n+    if (is_simm32(distance)) {                                                                     \\\n@@ -864,1 +863,1 @@\n-        if (is_offset_in_range(adr.offset(), 12)) {                                                \\\n+        if (is_simm12(adr.offset())) {                                                             \\\n@@ -900,1 +899,1 @@\n-    assert_cond(dest != NULL);                                                                     \\\n+    assert_cond(dest != nullptr);                                                                  \\\n@@ -903,1 +902,1 @@\n-    if (is_offset_in_range(distance, 32)) {                                                        \\\n+    if (is_simm32(distance)) {                                                                     \\\n@@ -922,1 +921,1 @@\n-        if (is_offset_in_range(adr.offset(), 12)) {                                                \\\n+        if (is_simm12(adr.offset())) {                                                             \\\n@@ -946,1 +945,1 @@\n-    assert_cond(dest != NULL);                                                                     \\\n+    assert_cond(dest != nullptr);                                                                  \\\n@@ -948,1 +947,1 @@\n-    if (is_offset_in_range(distance, 32)) {                                                        \\\n+    if (is_simm32(distance)) {                                                                     \\\n@@ -966,1 +965,1 @@\n-        if (is_offset_in_range(adr.offset(), 12)) {                                                \\\n+        if (is_simm12(adr.offset())) {                                                             \\\n@@ -1142,1 +1141,1 @@\n-  \/\/ Return: the call PC or NULL if CodeCache is full.\n+  \/\/ Return: the call PC or null if CodeCache is full.\n@@ -1162,1 +1161,1 @@\n-  void clinit_barrier(Register klass, Register tmp, Label* L_fast_path = NULL, Label* L_slow_path = NULL);\n+  void clinit_barrier(Register klass, Register tmp, Label* L_fast_path = nullptr, Label* L_slow_path = nullptr);\n@@ -1214,0 +1213,3 @@\n+  \/\/ test single bit in Rs, result is set to Rd\n+  void test_bit(Register Rd, Register Rs, uint32_t bit_pos, Register tmp = t0);\n+\n@@ -1265,1 +1267,1 @@\n-  inline void vncvt_x_x_w(VectorRegister vd, VectorRegister vs, VectorMask vm) {\n+  inline void vncvt_x_x_w(VectorRegister vd, VectorRegister vs, VectorMask vm = unmasked) {\n@@ -1277,0 +1279,39 @@\n+  inline void vmsgt_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmslt_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmsgtu_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmsltu_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmsge_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmsle_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmsgeu_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmsleu_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmfgt_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmflt_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  inline void vmfge_vv(VectorRegister vd, VectorRegister vs2, VectorRegister vs1, VectorMask vm = unmasked) {\n+    vmfle_vv(vd, vs1, vs2, vm);\n+  }\n+\n+  \/\/ Copy mask register\n+  inline void vmmv_m(VectorRegister vd, VectorRegister vs) {\n+    vmand_mm(vd, vs, vs);\n+  }\n+\n+  \/\/ Clear mask register\n+  inline void vmclr_m(VectorRegister vd) {\n+    vmxor_mm(vd, vd, vd);\n+  }\n+\n+  \/\/ Set mask register\n+  inline void vmset_m(VectorRegister vd) {\n+    vmxnor_mm(vd, vd, vd);\n+  }\n+\n@@ -1333,1 +1374,1 @@\n-    assert_cond(dest != NULL);\n+    assert_cond(dest != nullptr);\n@@ -1362,6 +1403,0 @@\n-  \/\/ Return true if an address is within the 48-bit RISCV64 address space.\n-  bool is_valid_riscv64_address(address addr) {\n-    \/\/ sv48: must have bits 63–48 all equal to bit 47\n-    return ((uintptr_t)addr >> 47) == 0;\n-  }\n-\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.hpp","additions":69,"deletions":34,"binary":false,"changes":103,"status":"modified"},{"patch":"@@ -833,1 +833,2 @@\n-\/\/ Class for all RVV vector registers\n+\/\/ Class for RVV vector registers\n+\/\/ Note: v0, v30 and v31 are used as mask registers.\n@@ -863,3 +864,1 @@\n-    V29, V29_H, V29_J, V29_K,\n-    V30, V30_H, V30_J, V30_K,\n-    V31, V31_H, V31_J, V31_K\n+    V29, V29_H, V29_J, V29_K\n@@ -915,0 +914,17 @@\n+\n+\/\/ Class for RVV v0 mask register\n+\/\/ https:\/\/github.com\/riscv\/riscv-v-spec\/blob\/master\/v-spec.adoc#53-vector-masking\n+\/\/ The mask value used to control execution of a masked vector\n+\/\/ instruction is always supplied by vector register v0.\n+reg_class vmask_reg_v0 (\n+    V0\n+);\n+\n+\/\/ Class for RVV mask registers\n+\/\/ We need two more vmask registers to do the vector mask logical ops,\n+\/\/ so define v30, v31 as mask register too.\n+reg_class vmask_reg (\n+    V0,\n+    V30,\n+    V31\n+);\n@@ -1525,1 +1541,1 @@\n-  if (src_hi != OptoReg::Bad) {\n+  if (src_hi != OptoReg::Bad && !bottom_type()->isa_vectmask()) {\n@@ -1561,0 +1577,19 @@\n+    } else if (bottom_type()->isa_vectmask() && cbuf) {\n+      C2_MacroAssembler _masm(cbuf);\n+      int vmask_size_in_bytes = Matcher::scalable_predicate_reg_slots() * 32 \/ 8;\n+      if (src_lo_rc == rc_stack && dst_lo_rc == rc_stack) {\n+        \/\/ stack to stack\n+        __ spill_copy_vmask_stack_to_stack(src_offset, dst_offset,\n+                                           vmask_size_in_bytes);\n+      } else if (src_lo_rc == rc_vector && dst_lo_rc == rc_stack) {\n+        \/\/ vmask to stack\n+        __ spill_vmask(as_VectorRegister(Matcher::_regEncode[src_lo]), ra_->reg2offset(dst_lo));\n+      } else if (src_lo_rc == rc_stack && dst_lo_rc == rc_vector) {\n+        \/\/ stack to vmask\n+        __ unspill_vmask(as_VectorRegister(Matcher::_regEncode[dst_lo]), ra_->reg2offset(src_lo));\n+      } else if (src_lo_rc == rc_vector && dst_lo_rc == rc_vector) {\n+        \/\/ vmask to vmask\n+        __ vmv1r_v(as_VectorRegister(Matcher::_regEncode[dst_lo]), as_VectorRegister(Matcher::_regEncode[src_lo]));\n+      } else {\n+        ShouldNotReachHere();\n+      }\n@@ -1645,1 +1680,1 @@\n-    if (bottom_type()->isa_vect() != NULL) {\n+    if (bottom_type()->isa_vect() && !bottom_type()->isa_vectmask()) {\n@@ -1653,0 +1688,4 @@\n+    } else if (ideal_reg() == Op_RegVectMask) {\n+      assert(Matcher::supports_scalable_vector(), \"bad register type for spill\");\n+      int vsize = Matcher::scalable_predicate_reg_slots() * 32;\n+      st->print(\"\\t# vmask spill size = %d\", vsize);\n@@ -1699,1 +1738,1 @@\n-  if (is_imm_in_range(offset, 12, 0)) {\n+  if (Assembler::is_simm12(offset)) {\n@@ -1701,1 +1740,1 @@\n-  } else if (is_imm_in_range(offset, 32, 0)) {\n+  } else {\n@@ -1704,2 +1743,0 @@\n-  } else {\n-    ShouldNotReachHere();\n@@ -1713,1 +1750,1 @@\n-  if (is_imm_in_range(offset, 12, 0)) {\n+  if (Assembler::is_simm12(offset)) {\n@@ -1848,4 +1885,0 @@\n-\n-    case Op_ConvF2HF:\n-    case Op_ConvHF2F:\n-      return UseZfhmin;\n@@ -1872,1 +1905,53 @@\n-  return false;\n+  if (!UseRVV) {\n+    return false;\n+  }\n+  switch (opcode) {\n+    case Op_AddVB:\n+    case Op_AddVS:\n+    case Op_AddVI:\n+    case Op_AddVL:\n+    case Op_AddVF:\n+    case Op_AddVD:\n+    case Op_SubVB:\n+    case Op_SubVS:\n+    case Op_SubVI:\n+    case Op_SubVL:\n+    case Op_SubVF:\n+    case Op_SubVD:\n+    case Op_MulVB:\n+    case Op_MulVS:\n+    case Op_MulVI:\n+    case Op_MulVL:\n+    case Op_MulVF:\n+    case Op_MulVD:\n+    case Op_DivVF:\n+    case Op_DivVD:\n+    case Op_VectorLoadMask:\n+    case Op_VectorMaskCmp:\n+    case Op_AndVMask:\n+    case Op_XorVMask:\n+    case Op_OrVMask:\n+    case Op_RShiftVB:\n+    case Op_RShiftVS:\n+    case Op_RShiftVI:\n+    case Op_RShiftVL:\n+    case Op_LShiftVB:\n+    case Op_LShiftVS:\n+    case Op_LShiftVI:\n+    case Op_LShiftVL:\n+    case Op_URShiftVB:\n+    case Op_URShiftVS:\n+    case Op_URShiftVI:\n+    case Op_URShiftVL:\n+    case Op_VectorBlend:\n+      break;\n+    case Op_LoadVector:\n+      opcode = Op_LoadVectorMasked;\n+      break;\n+    case Op_StoreVector:\n+      opcode = Op_StoreVectorMasked;\n+      break;\n+    default:\n+      return false;\n+  }\n+  return match_rule_supported_vector(opcode, vlen, bt);\n@@ -1879,0 +1964,4 @@\n+const bool Matcher::vector_needs_load_shuffle(BasicType elem_bt, int vlen) {\n+  return false;\n+}\n+\n@@ -1880,1 +1969,1 @@\n-  return NULL;\n+  return &_VMASK_REG_mask;\n@@ -1884,1 +1973,1 @@\n-  return NULL;\n+  return new TypeVectMask(elemTy, length);\n@@ -1952,0 +2041,4 @@\n+const int Matcher::superword_max_vector_size(const BasicType bt) {\n+  return Matcher::max_vector_size(bt);\n+}\n+\n@@ -2411,1 +2504,1 @@\n-      __ andi(flag, flag, JVM_ACC_IS_VALUE_BASED_CLASS, tmp \/* tmp *\/);\n+      __ test_bit(flag, flag, exact_log2(JVM_ACC_IS_VALUE_BASED_CLASS), tmp \/* tmp *\/);\n@@ -2416,1 +2509,1 @@\n-    __ andi(t0, disp_hdr, markWord::monitor_value);\n+    __ test_bit(t0, disp_hdr, exact_log2(markWord::monitor_value));\n@@ -2527,1 +2620,1 @@\n-    __ andi(t0, tmp, markWord::monitor_value);\n+    __ test_bit(t0, tmp, exact_log2(markWord::monitor_value));\n@@ -2926,1 +3019,1 @@\n-  predicate(Assembler::operand_valid_for_add_immediate((int64_t)n->get_int()));\n+  predicate(Assembler::is_simm12((int64_t)n->get_int()));\n@@ -2936,1 +3029,1 @@\n-  predicate(Assembler::operand_valid_for_add_immediate(-(int64_t)n->get_int()));\n+  predicate(Assembler::is_simm12(-(int64_t)n->get_int()));\n@@ -3091,1 +3184,1 @@\n-  predicate(Assembler::operand_valid_for_add_immediate(n->get_long()));\n+  predicate(Assembler::is_simm12(n->get_long()));\n@@ -3101,1 +3194,1 @@\n-  predicate(Assembler::operand_valid_for_add_immediate(-(n->get_long())));\n+  predicate(Assembler::is_simm12(-(n->get_long())));\n@@ -3182,1 +3275,1 @@\n-  predicate(is_imm_in_range(n->get_int(), 12, 0));\n+  predicate(Assembler::is_simm12(n->get_int()));\n@@ -3191,1 +3284,1 @@\n-  predicate(is_imm_in_range(n->get_long(), 12, 0));\n+  predicate(Assembler::is_simm12(n->get_long()));\n@@ -3591,0 +3684,22 @@\n+operand vRegMask()\n+%{\n+  constraint(ALLOC_IN_RC(vmask_reg));\n+  match(RegVectMask);\n+  match(vRegMask_V0);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n+\/\/ The mask value used to control execution of a masked\n+\/\/ vector instruction is always supplied by vector register v0.\n+operand vRegMask_V0()\n+%{\n+  constraint(ALLOC_IN_RC(vmask_reg_v0));\n+  match(RegVectMask);\n+  match(vRegMask);\n+  op_cost(0);\n+  format %{ %}\n+  interface(REG_INTER);\n+%}\n+\n@@ -5286,1 +5401,1 @@\n-    if (is_imm_in_range($mem$$disp, 12, 0)) {\n+    if (Assembler::is_simm12($mem$$disp)) {\n@@ -7306,1 +7421,1 @@\n-    __ minmax_FD(as_FloatRegister($dst$$reg),\n+    __ minmax_fp(as_FloatRegister($dst$$reg),\n@@ -7322,1 +7437,1 @@\n-    __ minmax_FD(as_FloatRegister($dst$$reg),\n+    __ minmax_fp(as_FloatRegister($dst$$reg),\n@@ -7338,1 +7453,1 @@\n-    __ minmax_FD(as_FloatRegister($dst$$reg),\n+    __ minmax_fp(as_FloatRegister($dst$$reg),\n@@ -7354,1 +7469,1 @@\n-    __ minmax_FD(as_FloatRegister($dst$$reg),\n+    __ minmax_fp(as_FloatRegister($dst$$reg),\n@@ -8217,38 +8332,0 @@\n-\/\/ float <-> half float\n-\n-instruct convHF2F_reg_reg(fRegF dst, iRegINoSp src, fRegF tmp) %{\n-  predicate(UseZfhmin);\n-  match(Set dst (ConvHF2F src));\n-  effect(TEMP tmp);\n-\n-  ins_cost(XFER_COST);\n-  format %{ \"fmv.h.x $tmp, $src\\t#@convHF2F_reg_reg\\n\\t\"\n-            \"fcvt.s.h $dst, $tmp\\t#@convHF2F_reg_reg\"\n-  %}\n-\n-  ins_encode %{\n-    __ fmv_h_x($tmp$$FloatRegister, $src$$Register);\n-    __ fcvt_s_h($dst$$FloatRegister, $tmp$$FloatRegister);\n-  %}\n-\n-  ins_pipe(fp_i2f);\n-%}\n-\n-instruct convF2HF_reg_reg(iRegINoSp dst, fRegF src, fRegF tmp) %{\n-  predicate(UseZfhmin);\n-  match(Set dst (ConvF2HF src));\n-  effect(TEMP tmp);\n-\n-  ins_cost(XFER_COST);\n-  format %{ \"fcvt.h.s $tmp, $src\\t#@convF2HF_reg_reg\\n\\t\"\n-            \"fmv.x.h $dst, $tmp\\t#@convF2HF_reg_reg\"\n-  %}\n-\n-  ins_encode %{\n-    __ fcvt_h_s($tmp$$FloatRegister, $src$$FloatRegister);\n-    __ fmv_x_h($dst$$Register, $tmp$$FloatRegister);\n-  %}\n-\n-  ins_pipe(fp_f2i);\n-%}\n-\n@@ -8980,20 +9057,0 @@\n-instruct cmpU_loop(cmpOpU cmp, iRegI op1, iRegI op2, label lbl)\n-%{\n-  \/\/ Same match rule as `far_cmpU_loop'.\n-  match(CountedLoopEnd cmp (CmpU op1 op2));\n-\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-\n-  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpU_loop\" %}\n-\n-  ins_encode %{\n-    __ cmp_branch($cmp$$cmpcode | C2_MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n-                  as_Register($op2$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmp_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9059,19 +9116,0 @@\n-instruct cmpUL_loop(cmpOpU cmp, iRegL op1, iRegL op2, label lbl)\n-%{\n-  \/\/ Same match rule as `far_cmpUL_loop'.\n-  match(CountedLoopEnd cmp (CmpUL op1 op2));\n-\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpUL_loop\" %}\n-\n-  ins_encode %{\n-    __ cmp_branch($cmp$$cmpcode | C2_MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n-                  as_Register($op2$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmp_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9099,20 +9137,0 @@\n-instruct cmpP_loop(cmpOpU cmp, iRegP op1, iRegP op2, label lbl)\n-%{\n-  \/\/ Same match rule as `far_cmpP_loop'.\n-  match(CountedLoopEnd cmp (CmpP op1 op2));\n-\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-\n-  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpP_loop\" %}\n-\n-  ins_encode %{\n-    __ cmp_branch($cmp$$cmpcode | C2_MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n-                  as_Register($op2$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmp_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9140,20 +9158,0 @@\n-instruct cmpN_loop(cmpOpU cmp, iRegN op1, iRegN op2, label lbl)\n-%{\n-  \/\/ Same match rule as `far_cmpN_loop'.\n-  match(CountedLoopEnd cmp (CmpN op1 op2));\n-\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-\n-  format %{ \"b$cmp  $op1, $op2, $lbl\\t#@cmpN_loop\" %}\n-\n-  ins_encode %{\n-    __ cmp_branch($cmp$$cmpcode | C2_MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n-                  as_Register($op2$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmp_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9179,17 +9177,0 @@\n-instruct cmpF_loop(cmpOp cmp, fRegF op1, fRegF op2, label lbl)\n-%{\n-  \/\/ Same match rule as `far_cmpF_loop'.\n-  match(CountedLoopEnd cmp (CmpF op1 op2));\n-  effect(USE lbl);\n-\n-  ins_cost(XFER_COST + BRANCH_COST);\n-  format %{ \"float_b$cmp $op1, $op2, $lbl\\t#@cmpF_loop\"%}\n-\n-  ins_encode %{\n-    __ float_cmp_branch($cmp$$cmpcode, as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_class_compare);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9215,18 +9196,0 @@\n-instruct cmpD_loop(cmpOp cmp, fRegD op1, fRegD op2, label lbl)\n-%{\n-  \/\/ Same match rule as `far_cmpD_loop'.\n-  match(CountedLoopEnd cmp (CmpD op1 op2));\n-  effect(USE lbl);\n-\n-  ins_cost(XFER_COST + BRANCH_COST);\n-  format %{ \"double_b$cmp $op1, $op2, $lbl\\t#@cmpD_loop\"%}\n-\n-  ins_encode %{\n-    __ float_cmp_branch($cmp$$cmpcode | C2_MacroAssembler::double_branch_mask, as_FloatRegister($op1$$reg),\n-                        as_FloatRegister($op2$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_class_compare);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9291,20 +9254,0 @@\n-instruct cmpUEqNeLeGt_reg_imm0_loop(cmpOpUEqNeLeGt cmp, iRegI op1, immI0 zero, label lbl)\n-%{\n-  \/\/ Same match rule as `far_cmpUEqNeLeGt_reg_imm0_loop'.\n-  match(CountedLoopEnd cmp (CmpU op1 zero));\n-\n-  effect(USE op1, USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-\n-  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpUEqNeLeGt_reg_imm0_loop\" %}\n-\n-\n-  ins_encode %{\n-    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9370,19 +9313,0 @@\n-instruct cmpULEqNeLeGt_reg_imm0_loop(cmpOpUEqNeLeGt cmp, iRegL op1, immL0 zero, label lbl)\n-%{\n-  \/\/ Same match rule as `far_cmpULEqNeLeGt_reg_imm0_loop'.\n-  match(CountedLoopEnd cmp (CmpUL op1 zero));\n-\n-  effect(USE op1, USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-\n-  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpULEqNeLeGt_reg_imm0_loop\" %}\n-\n-  ins_encode %{\n-    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9406,16 +9330,0 @@\n-instruct cmpP_imm0_loop(cmpOpEqNe cmp, iRegP op1, immP0 zero, label lbl) %{\n-  \/\/ Same match rule as `far_cmpP_reg_imm0_loop'.\n-  match(CountedLoopEnd cmp (CmpP op1 zero));\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-  format %{ \"b$cmp   $op1, zr, $lbl\\t#@cmpP_imm0_loop\" %}\n-\n-  ins_encode %{\n-    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9440,17 +9348,0 @@\n-instruct cmpN_imm0_loop(cmpOpEqNe cmp, iRegN op1, immN0 zero, label lbl) %{\n-  \/\/ Same match rule as `far_cmpN_reg_imm0_loop'.\n-  match(CountedLoopEnd cmp (CmpN op1 zero));\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-\n-  format %{ \"b$cmp  $op1, zr, $lbl\\t#@cmpN_imm0_loop\" %}\n-\n-  ins_encode %{\n-    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9474,16 +9365,0 @@\n-instruct cmpP_narrowOop_imm0_loop(cmpOpEqNe cmp, iRegN op1, immP0 zero, label lbl) %{\n-  \/\/ Same match rule as `far_cmpP_narrowOop_imm0_loop'.\n-  match(CountedLoopEnd cmp (CmpP (DecodeN op1) zero));\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-  format %{ \"b$cmp   $op1, zr, $lbl\\t#@cmpP_narrowOop_imm0_loop\" %}\n-\n-  ins_encode %{\n-    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label));\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-  ins_short_branch(1);\n-%}\n-\n@@ -9553,15 +9428,0 @@\n-instruct far_cmpU_loop(cmpOpU cmp, iRegI op1, iRegI op2, label lbl) %{\n-  match(CountedLoopEnd cmp (CmpU op1 op2));\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-  format %{ \"far_b$cmp $op1, $op2, $lbl\\t#@far_cmpU_loop\" %}\n-\n-  ins_encode %{\n-    __ cmp_branch($cmp$$cmpcode | C2_MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n-                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmp_branch);\n-%}\n-\n@@ -9611,15 +9471,0 @@\n-instruct far_cmpUL_loop(cmpOpU cmp, iRegL op1, iRegL op2, label lbl) %{\n-  match(CountedLoopEnd cmp (CmpUL op1 op2));\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpUL_loop\" %}\n-\n-  ins_encode %{\n-    __ cmp_branch($cmp$$cmpcode | C2_MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n-                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmp_branch);\n-%}\n-\n@@ -9644,18 +9489,0 @@\n-instruct far_cmpP_loop(cmpOpU cmp, iRegP op1, iRegP op2, label lbl)\n-%{\n-  match(CountedLoopEnd cmp (CmpP op1 op2));\n-\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-\n-  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpP_loop\" %}\n-\n-  ins_encode %{\n-    __ cmp_branch($cmp$$cmpcode | C2_MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n-                       as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmp_branch);\n-%}\n-\n@@ -9680,18 +9507,0 @@\n-instruct far_cmpN_loop(cmpOpU cmp, iRegN op1, iRegN op2, label lbl)\n-%{\n-  match(CountedLoopEnd cmp (CmpN op1 op2));\n-\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-\n-  format %{ \"far_b$cmp  $op1, $op2, $lbl\\t#@far_cmpN_loop\" %}\n-\n-  ins_encode %{\n-    __ cmp_branch($cmp$$cmpcode | C2_MacroAssembler::unsigned_branch_mask, as_Register($op1$$reg),\n-                  as_Register($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmp_branch);\n-%}\n-\n@@ -9716,16 +9525,0 @@\n-instruct far_cmpF_loop(cmpOp cmp, fRegF op1, fRegF op2, label lbl)\n-%{\n-  match(CountedLoopEnd cmp (CmpF op1 op2));\n-  effect(USE lbl);\n-\n-  ins_cost(XFER_COST + BRANCH_COST * 2);\n-  format %{ \"far_float_b$cmp $op1, $op2, $lbl\\t#@far_cmpF_loop\"%}\n-\n-  ins_encode %{\n-    __ float_cmp_branch($cmp$$cmpcode, as_FloatRegister($op1$$reg), as_FloatRegister($op2$$reg),\n-                        *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_class_compare);\n-%}\n-\n@@ -9749,16 +9542,0 @@\n-instruct far_cmpD_loop(cmpOp cmp, fRegD op1, fRegD op2, label lbl)\n-%{\n-  match(CountedLoopEnd cmp (CmpD op1 op2));\n-  effect(USE lbl);\n-\n-  ins_cost(XFER_COST + BRANCH_COST * 2);\n-  format %{ \"far_double_b$cmp $op1, $op2, $lbl\\t#@far_cmpD_loop\"%}\n-\n-  ins_encode %{\n-    __ float_cmp_branch($cmp$$cmpcode | C2_MacroAssembler::double_branch_mask, as_FloatRegister($op1$$reg),\n-                        as_FloatRegister($op2$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_class_compare);\n-%}\n-\n@@ -9816,18 +9593,0 @@\n-instruct far_cmpUEqNeLeGt_reg_imm0_loop(cmpOpUEqNeLeGt cmp, iRegI op1, immI0 zero, label lbl)\n-%{\n-  match(CountedLoopEnd cmp (CmpU op1 zero));\n-\n-  effect(USE op1, USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-\n-  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpUEqNeLeGt_reg_imm0_loop\" %}\n-\n-\n-  ins_encode %{\n-    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-%}\n-\n@@ -9850,15 +9609,0 @@\n-instruct far_cmpULtGe_reg_imm0_loop(cmpOpULtGe cmp, iRegI op1, immI0 zero, label lbl)\n-%{\n-  match(CountedLoopEnd cmp (CmpU op1 zero));\n-\n-  effect(USE op1, USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-\n-  format %{ \"j  $lbl if $cmp == ge\\t#@far_cmpULtGe_reg_imm0_loop\" %}\n-\n-  ins_encode(riscv_enc_far_cmpULtGe_imm0_branch(cmp, op1, lbl));\n-\n-  ins_pipe(pipe_cmpz_branch);\n-%}\n-\n@@ -9916,17 +9660,0 @@\n-instruct far_cmpULEqNeLeGt_reg_imm0_loop(cmpOpUEqNeLeGt cmp, iRegL op1, immL0 zero, label lbl)\n-%{\n-  match(CountedLoopEnd cmp (CmpUL op1 zero));\n-\n-  effect(USE op1, USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-\n-  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpULEqNeLeGt_reg_imm0_loop\" %}\n-\n-  ins_encode %{\n-    __ enc_cmpUEqNeLeGt_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-%}\n-\n@@ -9949,15 +9676,0 @@\n-instruct far_cmpULLtGe_reg_imm0_loop(cmpOpULtGe cmp, iRegL op1, immL0 zero, label lbl)\n-%{\n-  match(CountedLoopEnd cmp (CmpUL op1 zero));\n-\n-  effect(USE op1, USE lbl);\n-\n-  ins_cost(BRANCH_COST);\n-\n-  format %{ \"j  $lbl if $cmp == ge\\t#@far_cmpULLtGe_reg_imm0_loop\" %}\n-\n-  ins_encode(riscv_enc_far_cmpULtGe_imm0_branch(cmp, op1, lbl));\n-\n-  ins_pipe(pipe_cmpz_branch);\n-%}\n-\n@@ -9978,14 +9690,0 @@\n-instruct far_cmpP_imm0_loop(cmpOpEqNe cmp, iRegP op1, immP0 zero, label lbl) %{\n-  match(CountedLoopEnd cmp (CmpP op1 zero));\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-  format %{ \"far_b$cmp   $op1, zr, $lbl\\t#@far_cmpP_imm0_loop\" %}\n-\n-  ins_encode %{\n-    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-%}\n-\n@@ -10007,15 +9705,0 @@\n-instruct far_cmpN_imm0_loop(cmpOpEqNe cmp, iRegN op1, immN0 zero, label lbl) %{\n-  match(CountedLoopEnd cmp (CmpN op1 zero));\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-\n-  format %{ \"far_b$cmp  $op1, zr, $lbl\\t#@far_cmpN_imm0_loop\" %}\n-\n-  ins_encode %{\n-    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-%}\n-\n@@ -10036,14 +9719,0 @@\n-instruct far_cmpP_narrowOop_imm0_loop(cmpOpEqNe cmp, iRegN op1, immP0 zero, label lbl) %{\n-  match(CountedLoopEnd cmp (CmpP (DecodeN op1) zero));\n-  effect(USE lbl);\n-\n-  ins_cost(BRANCH_COST * 2);\n-  format %{ \"far_b$cmp   $op1, zr, $lbl\\t#@far_cmpP_narrowOop_imm0_loop\" %}\n-\n-  ins_encode %{\n-    __ enc_cmpEqNe_imm0_branch($cmp$$cmpcode, as_Register($op1$$reg), *($lbl$$label), \/* is_far *\/ true);\n-  %}\n-\n-  ins_pipe(pipe_cmpz_branch);\n-%}\n-\n","filename":"src\/hotspot\/cpu\/riscv\/riscv.ad","additions":147,"deletions":478,"binary":false,"changes":625,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"runtime\/globals.hpp\"\n@@ -181,1 +182,1 @@\n-  assert_cond(oop_maps != NULL && oop_map != NULL);\n+  assert_cond(oop_maps != nullptr && oop_map != nullptr);\n@@ -661,1 +662,1 @@\n-  address c2i_no_clinit_check_entry = NULL;\n+  address c2i_no_clinit_check_entry = nullptr;\n@@ -667,1 +668,1 @@\n-      __ andi(t1, t0, JVM_ACC_STATIC);\n+      __ test_bit(t1, t0, exact_log2(JVM_ACC_STATIC));\n@@ -684,1 +685,0 @@\n-  __ flush();\n@@ -699,1 +699,1 @@\n-  assert(regs2 == NULL, \"not needed on riscv\");\n+  assert(regs2 == nullptr, \"not needed on riscv\");\n@@ -1313,1 +1313,1 @@\n-                                       (OopMapSet*)NULL);\n+                                       (OopMapSet*)nullptr);\n@@ -1316,1 +1316,1 @@\n-  assert(native_func != NULL, \"must have function\");\n+  assert(native_func != nullptr, \"must have function\");\n@@ -1320,1 +1320,1 @@\n-  assert_cond(oop_maps != NULL);\n+  assert_cond(oop_maps != nullptr);\n@@ -1334,1 +1334,1 @@\n-  BasicType* in_elem_bt = NULL;\n+  BasicType* in_elem_bt = nullptr;\n@@ -1348,1 +1348,1 @@\n-  int out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);\n+  int out_arg_slots = c_calling_convention(out_sig_bt, out_regs, nullptr, total_c_args);\n@@ -1472,2 +1472,2 @@\n-  assert_cond(bs != NULL);\n-  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/, NULL \/* guard *\/);\n+  assert_cond(bs != nullptr);\n+  bs->nmethod_entry_barrier(masm, nullptr \/* slow_path *\/, nullptr \/* continuation *\/, nullptr \/* guard *\/);\n@@ -1510,1 +1510,1 @@\n-  assert_cond(map != NULL);\n+  assert_cond(map != nullptr);\n@@ -1755,1 +1755,3 @@\n-  __ membar(MacroAssembler::AnyAny);\n+  if (!UseSystemMemoryBarrier) {\n+    __ membar(MacroAssembler::AnyAny);\n+  }\n@@ -2037,1 +2039,1 @@\n-  assert(nm != NULL, \"create native nmethod fail!\");\n+  assert(nm != nullptr, \"create native nmethod fail!\");\n@@ -2068,1 +2070,1 @@\n-  OopMap* map = NULL;\n+  OopMap* map = nullptr;\n@@ -2070,1 +2072,1 @@\n-  assert_cond(masm != NULL && oop_maps != NULL);\n+  assert_cond(masm != nullptr && oop_maps != nullptr);\n@@ -2431,1 +2433,1 @@\n-  assert(_deopt_blob != NULL, \"create deoptimization blob fail!\");\n+  assert(_deopt_blob != nullptr, \"create deoptimization blob fail!\");\n@@ -2461,1 +2463,1 @@\n-  assert_cond(masm != NULL);\n+  assert_cond(masm != nullptr);\n@@ -2508,1 +2510,1 @@\n-  assert_cond(oop_maps != NULL && map != NULL);\n+  assert_cond(oop_maps != nullptr && map != nullptr);\n@@ -2660,2 +2662,2 @@\n-  assert_cond(oop_maps != NULL);\n-  OopMap* map = NULL;\n+  assert_cond(oop_maps != nullptr);\n+  OopMap* map = nullptr;\n@@ -2666,1 +2668,1 @@\n-  assert_cond(masm != NULL);\n+  assert_cond(masm != nullptr);\n@@ -2669,1 +2671,1 @@\n-  address call_pc = NULL;\n+  address call_pc = nullptr;\n@@ -2784,1 +2786,1 @@\n-  assert(StubRoutines::forward_exception_entry() != NULL, \"must be generated before\");\n+  assert(StubRoutines::forward_exception_entry() != nullptr, \"must be generated before\");\n@@ -2791,1 +2793,1 @@\n-  assert_cond(masm != NULL);\n+  assert_cond(masm != nullptr);\n@@ -2797,2 +2799,2 @@\n-  assert_cond(oop_maps != NULL);\n-  OopMap* map = NULL;\n+  assert_cond(oop_maps != nullptr);\n+  OopMap* map = nullptr;\n@@ -2907,1 +2909,1 @@\n-  assert_cond(masm != NULL);\n+  assert_cond(masm != nullptr);\n@@ -2963,1 +2965,1 @@\n-  assert_cond(oop_maps != NULL);\n+  assert_cond(oop_maps != nullptr);\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":32,"deletions":30,"binary":false,"changes":62,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -383,1 +383,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n","filename":"src\/hotspot\/cpu\/s390\/c1_CodeStubs_s390.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,2 +2,2 @@\n- * Copyright (c) 2016, 2022, Oracle and\/or its affiliates. All rights reserved.\n- * Copyright (c) 2016, 2019 SAP SE. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023 SAP SE. All rights reserved.\n@@ -146,1 +146,1 @@\n-      __ asm_assert_mem8_isnot_zero(slot_offset + 1*BytesPerWord, OSR_buf, \"locked object is NULL\", __LINE__);\n+      __ asm_assert_mem8_isnot_zero(slot_offset + 1*BytesPerWord, OSR_buf, \"locked object is null\", __LINE__);\n@@ -161,1 +161,1 @@\n-  if (call_addr == NULL) {\n+  if (call_addr == nullptr) {\n@@ -170,1 +170,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -216,1 +216,1 @@\n-  MonitorExitStub* stub = NULL;\n+  MonitorExitStub* stub = nullptr;\n@@ -241,1 +241,1 @@\n-  __ z_lg(Z_EXC_PC, _z_abi16(return_pc), Z_SP);\n+  __ z_lg(Z_EXC_PC, _z_common_abi(return_pc), Z_SP);\n@@ -251,1 +251,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -261,1 +261,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -276,1 +276,1 @@\n-  if (o == NULL) {\n+  if (o == nullptr) {\n@@ -289,1 +289,1 @@\n-  int oop_index = __ oop_recorder()->allocate_oop_index(NULL);\n+  int oop_index = __ oop_recorder()->allocate_oop_index(nullptr);\n@@ -294,1 +294,1 @@\n-  \/\/ The NULL will be dynamically patched later so the sequence to\n+  \/\/ The null will be dynamically patched later so the sequence to\n@@ -311,1 +311,1 @@\n-  int index = __ oop_recorder()->allocate_metadata_index(NULL);\n+  int index = __ oop_recorder()->allocate_metadata_index(nullptr);\n@@ -315,1 +315,1 @@\n-  \/\/ The NULL will be dynamically patched later so the sequence to\n+  \/\/ The null will be dynamically patched later so the sequence to\n@@ -356,3 +356,3 @@\n-  assert(op->block() == NULL || op->block()->label() == op->label(), \"wrong label\");\n-  if (op->block() != NULL)  { _branch_target_blocks.append(op->block()); }\n-  if (op->ublock() != NULL) { _branch_target_blocks.append(op->ublock()); }\n+  assert(op->block() == nullptr || op->block()->label() == op->label(), \"wrong label\");\n+  if (op->block() != nullptr)  { _branch_target_blocks.append(op->block()); }\n+  if (op->ublock() != nullptr) { _branch_target_blocks.append(op->ublock()); }\n@@ -362,1 +362,1 @@\n-    if (op->info() != NULL) { add_debug_info_for_branch(op->info()); }\n+    if (op->info() != nullptr) { add_debug_info_for_branch(op->info()); }\n@@ -367,1 +367,1 @@\n-      assert(op->ublock() != NULL, \"must have unordered successor\");\n+      assert(op->ublock() != nullptr, \"must have unordered successor\");\n@@ -507,1 +507,1 @@\n-  address virtual_call_oop_addr = NULL;\n+  address virtual_call_oop_addr = nullptr;\n@@ -549,1 +549,1 @@\n-      if (c->as_jobject() == NULL) {\n+      if (c->as_jobject() == nullptr) {\n@@ -599,1 +599,1 @@\n-        if (c->as_jobject() == NULL) {\n+        if (c->as_jobject() == nullptr) {\n@@ -669,1 +669,1 @@\n-        if (c->as_jobject() == NULL) {\n+        if (c->as_jobject() == nullptr) {\n@@ -712,1 +712,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -763,1 +763,1 @@\n-      if (const_addr == NULL) {\n+      if (const_addr == nullptr) {\n@@ -781,1 +781,1 @@\n-      if (const_addr == NULL) {\n+      if (const_addr == nullptr) {\n@@ -884,1 +884,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -972,1 +972,1 @@\n-  if (patch != NULL) {\n+  if (patch != nullptr) {\n@@ -975,1 +975,1 @@\n-  if (info != NULL) add_debug_info_for_null_check(offset, info);\n+  if (info != nullptr) add_debug_info_for_null_check(offset, info);\n@@ -1077,1 +1077,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -1179,1 +1179,1 @@\n-  if (patch != NULL) {\n+  if (patch != nullptr) {\n@@ -1183,1 +1183,1 @@\n-  if (info != NULL) add_debug_info_for_null_check(offset, info);\n+  if (info != nullptr) add_debug_info_for_null_check(offset, info);\n@@ -1214,1 +1214,1 @@\n-  guarantee(info != NULL, \"Shouldn't be NULL\");\n+  guarantee(info != nullptr, \"Shouldn't be null\");\n@@ -1229,1 +1229,1 @@\n-  if (stub == NULL) {\n+  if (stub == nullptr) {\n@@ -1239,1 +1239,1 @@\n-  AddressLiteral meta = __ allocate_metadata_address(NULL);\n+  AddressLiteral meta = __ allocate_metadata_address(nullptr);\n@@ -1292,1 +1292,1 @@\n-        \/\/ We only need, for now, comparison with NULL for metadata.\n+        \/\/ We only need, for now, comparison with null for metadata.\n@@ -1295,1 +1295,1 @@\n-        if (m == NULL) {\n+        if (m == nullptr) {\n@@ -1303,1 +1303,1 @@\n-        if (o == NULL) {\n+        if (o == nullptr) {\n@@ -1314,1 +1314,1 @@\n-      if (op->info() != NULL) {\n+      if (op->info() != nullptr) {\n@@ -1452,1 +1452,1 @@\n-    const2reg(opr1, result, lir_patch_none, NULL);\n+    const2reg(opr1, result, lir_patch_none, nullptr);\n@@ -1481,1 +1481,1 @@\n-      const2reg(opr2, result, lir_patch_none, NULL);\n+      const2reg(opr2, result, lir_patch_none, nullptr);\n@@ -1491,1 +1491,1 @@\n-  assert(info == NULL, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n+  assert(info == nullptr, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n@@ -1938,1 +1938,1 @@\n-  BasicType basic_type = default_type != NULL ? default_type->element_type()->basic_type() : T_ILLEGAL;\n+  BasicType basic_type = default_type != nullptr ? default_type->element_type()->basic_type() : T_ILLEGAL;\n@@ -1942,1 +1942,1 @@\n-  if (default_type == NULL) {\n+  if (default_type == nullptr) {\n@@ -1945,1 +1945,1 @@\n-    if (copyfunc_addr == NULL) {\n+    if (copyfunc_addr == nullptr) {\n@@ -2010,1 +2010,1 @@\n-  assert(default_type != NULL && default_type->is_array_klass() && default_type->is_loaded(), \"must be true at this point\");\n+  assert(default_type != nullptr && default_type->is_array_klass() && default_type->is_loaded(), \"must be true at this point\");\n@@ -2040,1 +2040,1 @@\n-  \/\/ test for NULL\n+  \/\/ test for null\n@@ -2118,1 +2118,1 @@\n-      __ check_klass_subtype_fast_path(src_klass, dst_klass, tmp, &cont, &slow, NULL);\n+      __ check_klass_subtype_fast_path(src_klass, dst_klass, tmp, &cont, &slow, nullptr);\n@@ -2130,1 +2130,1 @@\n-      if (copyfunc_addr != NULL) { \/\/ use stub if available\n+      if (copyfunc_addr != nullptr) { \/\/ use stub if available\n@@ -2432,2 +2432,3 @@\n-  int offset_in_bytes = param_num * BytesPerWord + FrameMap::first_available_sp_in_frame;\n-  assert(offset_in_bytes < frame_map()->reserved_argument_area_size(), \"invalid offset\");\n+  int offset_in_bytes = param_num * BytesPerWord;\n+  check_reserved_argument_area(offset_in_bytes);\n+  offset_in_bytes += FrameMap::first_available_sp_in_frame;\n@@ -2439,2 +2440,3 @@\n-  int offset_in_bytes = param_num * BytesPerWord + FrameMap::first_available_sp_in_frame;\n-  assert(offset_in_bytes < frame_map()->reserved_argument_area_size(), \"invalid offset\");\n+  int offset_in_bytes = param_num * BytesPerWord;\n+  check_reserved_argument_area(offset_in_bytes);\n+  offset_in_bytes += FrameMap::first_available_sp_in_frame;\n@@ -2457,2 +2459,2 @@\n-  ciMethodData* md = NULL;\n-  ciProfileData* data = NULL;\n+  ciMethodData* md = nullptr;\n+  ciProfileData* data = nullptr;\n@@ -2462,1 +2464,1 @@\n-    assert(method != NULL, \"Should have method\");\n+    assert(method != nullptr, \"Should have method\");\n@@ -2465,1 +2467,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2467,1 +2469,1 @@\n-    assert(data != NULL,                \"need data for type check\");\n+    assert(data != nullptr,                \"need data for type check\");\n@@ -2528,2 +2530,2 @@\n-                                     (need_slow_path ? success_target : NULL),\n-                                     failure_target, NULL,\n+                                     (need_slow_path ? success_target : nullptr),\n+                                     failure_target, nullptr,\n@@ -2573,2 +2575,2 @@\n-    ciMethodData* md = NULL;\n-    ciProfileData* data = NULL;\n+    ciMethodData* md = nullptr;\n+    ciProfileData* data = nullptr;\n@@ -2580,1 +2582,1 @@\n-      assert(method != NULL, \"Should have method\");\n+      assert(method != nullptr, \"Should have method\");\n@@ -2583,1 +2585,1 @@\n-      assert(md != NULL, \"Sanity\");\n+      assert(md != nullptr, \"Sanity\");\n@@ -2585,1 +2587,1 @@\n-      assert(data != NULL,                \"need data for type check\");\n+      assert(data != nullptr,                \"need data for type check\");\n@@ -2614,1 +2616,1 @@\n-    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -2724,1 +2726,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -2732,1 +2734,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -2751,1 +2753,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -2769,1 +2771,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -2771,1 +2773,1 @@\n-  assert(data != NULL && data->is_CounterData(), \"need CounterData for calls\");\n+  assert(data != nullptr && data->is_CounterData(), \"need CounterData for calls\");\n@@ -2787,1 +2789,1 @@\n-    if (C1OptimizeVirtualCallProfiling && known_klass != NULL) {\n+    if (C1OptimizeVirtualCallProfiling && known_klass != nullptr) {\n@@ -2812,1 +2814,1 @@\n-        if (receiver == NULL) {\n+        if (receiver == nullptr) {\n@@ -2866,1 +2868,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -2963,1 +2965,1 @@\n-  bool exact_klass_set = exact_klass != NULL && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n+  bool exact_klass_set = exact_klass != nullptr && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n@@ -2985,1 +2987,1 @@\n-    __ asm_assert_ne(\"unexpected null obj\", __LINE__);\n+    __ asm_assert(Assembler::bcondNotZero, \"unexpected null obj\", __LINE__);\n@@ -2992,1 +2994,1 @@\n-    if (exact_klass != NULL) {\n+    if (exact_klass != nullptr) {\n@@ -2996,1 +2998,1 @@\n-      __ asm_assert_eq(\"exact klass and actual klass differ\", __LINE__);\n+      __ asm_assert(Assembler::bcondEqual, \"exact klass and actual klass differ\", __LINE__);\n@@ -3004,2 +3006,2 @@\n-      if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {\n-        if (exact_klass != NULL) {\n+      if (exact_klass == nullptr || TypeEntries::is_type_none(current_klass)) {\n+        if (exact_klass != nullptr) {\n@@ -3028,1 +3030,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -3041,1 +3043,1 @@\n-      assert(exact_klass != NULL, \"should be\");\n+      assert(exact_klass != nullptr, \"should be\");\n@@ -3061,1 +3063,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n","filename":"src\/hotspot\/cpu\/s390\/c1_LIRAssembler_s390.cpp","additions":85,"deletions":83,"binary":false,"changes":168,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2016, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2016, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -133,1 +133,1 @@\n-  \/\/ location (NULL in the displaced hdr location indicates recursive locking).\n+  \/\/ location (null in the displaced hdr location indicates recursive locking).\n@@ -149,1 +149,1 @@\n-  \/\/ If the loaded hdr is NULL we had recursive locking, and we are done.\n+  \/\/ If the loaded hdr is null we had recursive locking, and we are done.\n","filename":"src\/hotspot\/cpu\/s390\/c1_MacroAssembler_s390.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -891,1 +891,1 @@\n-  return (long)((cs != NULL) ? cs->start()-pc() : 0);\n+  return (long)((cs != nullptr) ? cs->start()-pc() : 0);\n@@ -1146,1 +1146,1 @@\n-  assert(addr != NULL, \"should not happen\");\n+  assert(addr != nullptr, \"should not happen\");\n@@ -1148,1 +1148,1 @@\n-  if (addr == NULL) {\n+  if (addr == nullptr) {\n@@ -1799,1 +1799,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -1806,1 +1806,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -1813,1 +1813,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -1819,1 +1819,1 @@\n-  assert(oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert(oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -2078,1 +2078,1 @@\n-    asm_assert_eq(\"[old_sp]!=[Z_SP]\", 0x211);\n+    asm_assert(bcondEqual, \"[old_sp]!=[Z_SP]\", 0x211);\n@@ -2129,1 +2129,1 @@\n-  int retPC_offset = _z_abi16(return_pc) + frame_size_in_bytes;\n+  int retPC_offset = _z_common_abi(return_pc) + frame_size_in_bytes;\n@@ -2176,1 +2176,1 @@\n-  address return_pc = NULL;\n+  address return_pc = nullptr;\n@@ -2381,1 +2381,1 @@\n-  _last_calls_return_pc = success ? pc() : NULL;\n+  _last_calls_return_pc = success ? pc() : nullptr;\n@@ -2575,1 +2575,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -2636,1 +2636,1 @@\n-  assert(ucontext != NULL, \"must have ucontext\");\n+  assert(ucontext != nullptr, \"must have ucontext\");\n@@ -2654,1 +2654,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -2782,1 +2782,1 @@\n-  \/\/ If the entry is NULL then we've reached the end of the table\n+  \/\/ If the entry is null then we've reached the end of the table\n@@ -2949,3 +2949,3 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  if (L_slow_path == NULL) { L_slow_path = &L_fallthrough; label_nulls++; }\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_slow_path == nullptr) { L_slow_path = &L_fallthrough; label_nulls++; }\n@@ -2954,1 +2954,1 @@\n-         \"at most one NULL in the batch, usually\");\n+         \"at most one null in the batch, usually\");\n@@ -3035,3 +3035,3 @@\n-  if (L_success == NULL) { L_success = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL) { L_failure = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr) { L_success = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr) { L_failure = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -3104,1 +3104,1 @@\n-                                &L_success, &failure, NULL);\n+                                &L_success, &failure, nullptr);\n@@ -3106,1 +3106,1 @@\n-                                temp1_reg, temp2_reg, &L_success, NULL);\n+                                temp1_reg, temp2_reg, &L_success, nullptr);\n@@ -3112,1 +3112,1 @@\n-  assert(L_fast_path != NULL || L_slow_path != NULL, \"at least one is required\");\n+  assert(L_fast_path != nullptr || L_slow_path != nullptr, \"at least one is required\");\n@@ -3115,1 +3115,1 @@\n-  if (L_fast_path == NULL) {\n+  if (L_fast_path == nullptr) {\n@@ -3117,1 +3117,1 @@\n-  } else if (L_slow_path == NULL) {\n+  } else if (L_slow_path == nullptr) {\n@@ -3207,1 +3207,1 @@\n-  \/\/ The object's monitor m is unlocked iff m->owner == NULL,\n+  \/\/ The object's monitor m is unlocked iff m->owner is null,\n@@ -3210,1 +3210,1 @@\n-  \/\/ Try to CAS m->owner from NULL to current thread.\n+  \/\/ Try to CAS m->owner from null to current thread.\n@@ -3310,1 +3310,1 @@\n-  \/\/ last_Java_pc will always be set to NULL. It is set here so that\n+  \/\/ last_Java_pc will always be set to null. It is set here so that\n@@ -3406,1 +3406,1 @@\n-      \/\/ Provoke OS NULL exception if reg = NULL by\n+      \/\/ Provoke OS null exception if reg is null by\n@@ -3412,1 +3412,1 @@\n-      \/\/ will provoke OS NULL exception if reg = NULL.\n+      \/\/ will provoke OS null exception if reg is null.\n@@ -3451,1 +3451,1 @@\n-  if (base != NULL) {\n+  if (base != nullptr) {\n@@ -3518,1 +3518,1 @@\n-\/\/ when (Universe::heap() != NULL). Hence, if the instructions\n+\/\/ when Universe::heap() isn't null. Hence, if the instructions\n@@ -3526,1 +3526,1 @@\n-  if (base != NULL) {\n+  if (base != nullptr) {\n@@ -3561,1 +3561,1 @@\n-  if (base != NULL) {\n+  if (base != nullptr) {\n@@ -3608,1 +3608,1 @@\n-  if (base != NULL) {\n+  if (base != nullptr) {\n@@ -3655,5 +3655,0 @@\n-void MacroAssembler::load_klass_check_null(Register klass, Register src_oop, Register tmp) {\n-  null_check(src_oop, tmp, oopDesc::klass_offset_in_bytes());\n-  load_klass(klass, src_oop);\n-}\n-\n@@ -3688,2 +3683,2 @@\n-\/\/ maybeNULL       - True if Rop1 possibly is a NULL.\n-void MacroAssembler::compare_klass_ptr(Register Rop1, int64_t disp, Register Rbase, bool maybeNULL) {\n+\/\/ maybenull       - True if Rop1 possibly is a null.\n+void MacroAssembler::compare_klass_ptr(Register Rop1, int64_t disp, Register Rbase, bool maybenull) {\n@@ -3703,1 +3698,1 @@\n-    if (base == NULL) {\n+    if (base == nullptr) {\n@@ -3718,1 +3713,1 @@\n-      if (maybeNULL) {       \/\/ NULL ptr must be preserved!\n+      if (maybenull) {       \/\/ null ptr must be preserved!\n@@ -3821,3 +3816,3 @@\n-\/\/ maybeNULL       - True if Rop1 possibly is a NULL.\n-\/\/ maybeNULLtarget - Branch target for Rop1 == NULL, if flow control shall NOT continue with compare instruction.\n-void MacroAssembler::compare_heap_oop(Register Rop1, Address mem, bool maybeNULL) {\n+\/\/ maybenull       - True if Rop1 possibly is a null.\n+\/\/ maybenulltarget - Branch target for Rop1 == nullptr, if flow control shall NOT continue with compare instruction.\n+void MacroAssembler::compare_heap_oop(Register Rop1, Address mem, bool maybenull) {\n@@ -3832,1 +3827,1 @@\n-  assert(Universe::heap() != NULL, \"java heap must be initialized to call this method\");\n+  assert(Universe::heap() != nullptr, \"java heap must be initialized to call this method\");\n@@ -3842,1 +3837,1 @@\n-  if (base == NULL) {\n+  if (base == nullptr) {\n@@ -3857,1 +3852,1 @@\n-    if (maybeNULL) {       \/\/ NULL ptr must be preserved!\n+    if (maybenull) {       \/\/ null ptr must be preserved!\n@@ -3937,1 +3932,1 @@\n-void MacroAssembler::oop_encoder(Register Rdst, Register Rsrc, bool maybeNULL,\n+void MacroAssembler::oop_encoder(Register Rdst, Register Rsrc, bool maybenull,\n@@ -3945,1 +3940,1 @@\n-  assert(Universe::heap() != NULL, \"java heap must be initialized to call this encoder\");\n+  assert(Universe::heap() != nullptr, \"java heap must be initialized to call this encoder\");\n@@ -3948,1 +3943,1 @@\n-  if (disjoint || (oop_base == NULL)) {\n+  if (disjoint || (oop_base == nullptr)) {\n@@ -3951,1 +3946,1 @@\n-      if (oop_base != NULL && !only32bitValid) {\n+      if (oop_base != nullptr && !only32bitValid) {\n@@ -3958,1 +3953,1 @@\n-      if (oop_base != NULL && !only32bitValid) {\n+      if (oop_base != nullptr && !only32bitValid) {\n@@ -3972,1 +3967,1 @@\n-  if (maybeNULL) {\n+  if (maybenull) {\n@@ -3999,1 +3994,1 @@\n-    \/\/ Check for NULL oop (must be left alone) and shift.\n+    \/\/ Check for null oop (must be left alone) and shift.\n@@ -4010,1 +4005,1 @@\n-      z_ltgr(Rdst, Rsrc);   \/\/ Move NULL to result register.\n+      z_ltgr(Rdst, Rsrc);   \/\/ Move null to result register.\n@@ -4073,1 +4068,1 @@\n-void MacroAssembler::oop_decoder(Register Rdst, Register Rsrc, bool maybeNULL, Register Rbase, int pow2_offset) {\n+void MacroAssembler::oop_decoder(Register Rdst, Register Rsrc, bool maybenull, Register Rbase, int pow2_offset) {\n@@ -4080,1 +4075,1 @@\n-  assert(Universe::heap() != NULL, \"java heap must be initialized to call this decoder\");\n+  assert(Universe::heap() != nullptr, \"java heap must be initialized to call this decoder\");\n@@ -4086,1 +4081,1 @@\n-  if (oop_base != NULL) {\n+  if (oop_base != nullptr) {\n@@ -4097,1 +4092,1 @@\n-      if (maybeNULL) {  \/\/ NULL ptr must be preserved!\n+      if (maybenull) {  \/\/ null ptr must be preserved!\n@@ -4157,1 +4152,1 @@\n-      \/\/ Scale oop and check for NULL.\n+      \/\/ Scale oop and check for null.\n@@ -4159,1 +4154,1 @@\n-      if (maybeNULL) {  \/\/ NULL ptr must be preserved!\n+      if (maybenull) {  \/\/ null ptr must be preserved!\n@@ -4437,1 +4432,1 @@\n-  if (tocPos != NULL) {\n+  if (tocPos != nullptr) {\n@@ -4441,1 +4436,1 @@\n-  \/\/ Address_constant returned NULL, so no constant entry has been created.\n+  \/\/ Address_constant returned null, so no constant entry has been created.\n@@ -4455,1 +4450,1 @@\n-  if (tocPos != NULL) {\n+  if (tocPos != nullptr) {\n@@ -4469,1 +4464,1 @@\n-  \/\/ Address_constant returned NULL, so no constant entry has been created\n+  \/\/ Address_constant returned null, so no constant entry has been created\n@@ -4479,1 +4474,1 @@\n-  assert((address)code()->consts()->start() != NULL, \"Please add CP address\");\n+  assert((address)code()->consts()->start() != nullptr, \"Please add CP address\");\n@@ -4489,1 +4484,1 @@\n-  assert((address)code()->consts()->start() != NULL, \"Please add CP address\");\n+  assert((address)code()->consts()->start() != nullptr, \"Please add CP address\");\n@@ -4503,1 +4498,1 @@\n-  address dataLoc = NULL;\n+  address dataLoc = nullptr;\n@@ -4522,1 +4517,1 @@\n-  address dataLoc = NULL;\n+  address dataLoc = nullptr;\n@@ -4527,1 +4522,1 @@\n-    assert((cb == NULL) || (nm == (nmethod*)cb), \"instruction address should be in CodeBlob\");\n+    assert((cb == nullptr) || (nm == (nmethod*)cb), \"instruction address should be in CodeBlob\");\n@@ -5335,23 +5330,2 @@\n-#ifndef PRODUCT\n-\/\/ Assert if CC indicates \"not equal\" (check_equal==true) or \"equal\" (check_equal==false).\n-void MacroAssembler::asm_assert(bool check_equal, const char *msg, int id) {\n-  Label ok;\n-  if (check_equal) {\n-    z_bre(ok);\n-  } else {\n-    z_brne(ok);\n-  }\n-  stop(msg, id);\n-  bind(ok);\n-}\n-\n-\/\/ Assert if CC indicates \"low\".\n-void MacroAssembler::asm_assert_low(const char *msg, int id) {\n-  Label ok;\n-  z_brnl(ok);\n-  stop(msg, id);\n-  bind(ok);\n-}\n-\n-\/\/ Assert if CC indicates \"high\".\n-void MacroAssembler::asm_assert_high(const char *msg, int id) {\n+void MacroAssembler::asm_assert(branch_condition cond, const char* msg, int id, bool is_static) {\n+#ifdef ASSERT\n@@ -5359,2 +5333,2 @@\n-  z_brnh(ok);\n-  stop(msg, id);\n+  z_brc(cond, ok);\n+  is_static ? stop_static(msg, id) : stop(msg, id);\n@@ -5362,0 +5336,1 @@\n+#endif \/\/ ASSERT\n@@ -5364,8 +5339,5 @@\n-\/\/ Assert if CC indicates \"not equal\" (check_equal==true) or \"equal\" (check_equal==false)\n-\/\/ generate non-relocatable code.\n-void MacroAssembler::asm_assert_static(bool check_equal, const char *msg, int id) {\n-  Label ok;\n-  if (check_equal) { z_bre(ok); }\n-  else             { z_brne(ok); }\n-  stop_static(msg, id);\n-  bind(ok);\n+\/\/ Assert if CC indicates \"not equal\" (check_equal==true) or \"equal\" (check_equal==false).\n+void MacroAssembler::asm_assert(bool check_equal, const char *msg, int id) {\n+#ifdef ASSERT\n+  asm_assert(check_equal ? bcondEqual : bcondNotEqual, msg, id);\n+#endif \/\/ ASSERT\n@@ -5376,0 +5348,1 @@\n+#ifdef ASSERT\n@@ -5386,2 +5359,3 @@\n-  if (allow_relocation) { asm_assert(check_equal, msg, id); }\n-  else                  { asm_assert_static(check_equal, msg, id); }\n+  \/\/ if relocation is not allowed then stop_static() will be called otherwise call stop()\n+  asm_assert(check_equal ? bcondEqual : bcondNotEqual, msg, id, !allow_relocation);\n+#endif \/\/ ASSERT\n@@ -5396,10 +5370,6 @@\n-  if (tmp == noreg) {\n-    tmp = expected_size;\n-  } else {\n-    if (tmp != expected_size) {\n-      z_lgr(tmp, expected_size);\n-    }\n-    z_algr(tmp, Z_SP);\n-    z_slg(tmp, 0, Z_R0, Z_SP);\n-    asm_assert_eq(msg, id);\n-  }\n+#ifdef ASSERT\n+  lgr_if_needed(tmp, expected_size);\n+  z_algr(tmp, Z_SP);\n+  z_slg(tmp, 0, Z_R0, Z_SP);\n+  asm_assert(bcondEqual, msg, id);\n+#endif \/\/ ASSERT\n@@ -5407,1 +5377,0 @@\n-#endif \/\/ !PRODUCT\n@@ -5528,2 +5497,2 @@\n-  z_illtrap(); \/\/ Illegal instruction.\n-  z_illtrap(); \/\/ Illegal instruction.\n+  z_illtrap(id); \/\/ Illegal instruction.\n+  z_illtrap(id); \/\/ Illegal instruction.\n@@ -5543,1 +5512,1 @@\n-  BLOCK_COMMENT(err_msg(\"stop_chain(%s,%s): %s {\", reentry==NULL?\"init\":\"cont\", allow_relocation?\"reloc \":\"static\", msg));\n+  BLOCK_COMMENT(err_msg(\"stop_chain(%s,%s): %s {\", reentry==nullptr?\"init\":\"cont\", allow_relocation?\"reloc \":\"static\", msg));\n@@ -5554,1 +5523,1 @@\n-  if ((reentry != NULL) && RelAddr::is_in_range_of_RelAddr16(reentry, pc())) {\n+  if ((reentry != nullptr) && RelAddr::is_in_range_of_RelAddr16(reentry, pc())) {\n@@ -5563,1 +5532,1 @@\n-      reentry = NULL;    \/\/ Prevent reentry if code relocation is allowed.\n+      reentry = nullptr;    \/\/ Prevent reentry if code relocation is allowed.\n@@ -5568,1 +5537,1 @@\n-    z_illtrap(); \/\/ Illegal instruction as emergency stop, should the above call return.\n+    z_illtrap(id); \/\/ Illegal instruction as emergency stop, should the above call return.\n@@ -5578,1 +5547,1 @@\n-  stop_chain(NULL, type, msg, id, false);\n+  stop_chain(nullptr, type, msg, id, false);\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":97,"deletions":128,"binary":false,"changes":225,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2020, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -102,1 +102,1 @@\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+  assert(SharedRuntime::polling_page_return_handler_blob() != nullptr,\n@@ -333,1 +333,1 @@\n-    Metadata* o = NULL;\n+    Metadata* o = nullptr;\n@@ -348,1 +348,1 @@\n-    jobject o = NULL;\n+    jobject o = nullptr;\n@@ -419,1 +419,1 @@\n-  address target = NULL;\n+  address target = nullptr;\n","filename":"src\/hotspot\/cpu\/x86\/c1_CodeStubs_x86.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -147,1 +147,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -158,1 +158,1 @@\n-  if (const_addr == NULL) {\n+  if (const_addr == nullptr) {\n@@ -324,1 +324,1 @@\n-        __ stop(\"locked object is NULL\");\n+        __ stop(\"locked object is null\");\n@@ -376,1 +376,1 @@\n-  jobject o = NULL;\n+  jobject o = nullptr;\n@@ -383,1 +383,1 @@\n-  Metadata* o = NULL;\n+  Metadata* o = nullptr;\n@@ -403,1 +403,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -453,1 +453,1 @@\n-  MonitorExitStub* stub = NULL;\n+  MonitorExitStub* stub = nullptr;\n@@ -486,1 +486,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -497,1 +497,1 @@\n-  if (handler_base == NULL) {\n+  if (handler_base == nullptr) {\n@@ -544,1 +544,1 @@\n-  guarantee(info != NULL, \"Shouldn't be NULL\");\n+  guarantee(info != nullptr, \"Shouldn't be null\");\n@@ -736,1 +736,1 @@\n-      if (c->as_jobject() == NULL) {\n+      if (c->as_jobject() == nullptr) {\n@@ -802,1 +802,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -950,1 +950,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -1046,1 +1046,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -1055,1 +1055,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -1083,1 +1083,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -1201,1 +1201,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -1206,1 +1206,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -1273,1 +1273,1 @@\n-        assert(info == NULL && patch == NULL, \"must be\");\n+        assert(info == nullptr && patch == nullptr, \"must be\");\n@@ -1281,1 +1281,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -1290,1 +1290,1 @@\n-        if (patch != NULL) {\n+        if (patch != nullptr) {\n@@ -1342,1 +1342,1 @@\n-  if (patch != NULL) {\n+  if (patch != nullptr) {\n@@ -1404,3 +1404,3 @@\n-  assert(op->block() == NULL || op->block()->label() == op->label(), \"wrong label\");\n-  if (op->block() != NULL)  _branch_target_blocks.append(op->block());\n-  if (op->ublock() != NULL) _branch_target_blocks.append(op->ublock());\n+  assert(op->block() == nullptr || op->block()->label() == op->label(), \"wrong label\");\n+  if (op->block() != nullptr)  _branch_target_blocks.append(op->block());\n+  if (op->ublock() != nullptr) _branch_target_blocks.append(op->ublock());\n@@ -1410,1 +1410,1 @@\n-    if (op->info() != NULL) add_debug_info_for_branch(op->info());\n+    if (op->info() != nullptr) add_debug_info_for_branch(op->info());\n@@ -1415,1 +1415,1 @@\n-      assert(op->ublock() != NULL, \"must have unordered successor\");\n+      assert(op->ublock() != nullptr, \"must have unordered successor\");\n@@ -1572,1 +1572,1 @@\n-      assert(op->stub() != NULL, \"stub required\");\n+      assert(op->stub() != nullptr, \"stub required\");\n@@ -1685,2 +1685,2 @@\n-  ciMethodData* md = NULL;\n-  ciProfileData* data = NULL;\n+  ciMethodData* md = nullptr;\n+  ciProfileData* data = nullptr;\n@@ -1690,1 +1690,1 @@\n-    assert(method != NULL, \"Should have method\");\n+    assert(method != nullptr, \"Should have method\");\n@@ -1693,1 +1693,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -1695,1 +1695,1 @@\n-    assert(data != NULL,                \"need data for type check\");\n+    assert(data != nullptr,                \"need data for type check\");\n@@ -1801,1 +1801,1 @@\n-      __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+      __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -1845,2 +1845,2 @@\n-    ciMethodData* md = NULL;\n-    ciProfileData* data = NULL;\n+    ciMethodData* md = nullptr;\n+    ciProfileData* data = nullptr;\n@@ -1850,1 +1850,1 @@\n-      assert(method != NULL, \"Should have method\");\n+      assert(method != nullptr, \"Should have method\");\n@@ -1853,1 +1853,1 @@\n-      assert(md != NULL, \"Sanity\");\n+      assert(md != nullptr, \"Sanity\");\n@@ -1855,1 +1855,1 @@\n-      assert(data != NULL,                \"need data for type check\");\n+      assert(data != nullptr,                \"need data for type check\");\n@@ -1885,1 +1885,1 @@\n-    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, NULL);\n+    __ check_klass_subtype_fast_path(klass_RInfo, k_RInfo, Rtmp1, success_target, failure_target, nullptr);\n@@ -2024,1 +2024,1 @@\n-    const2reg(opr1, result, lir_patch_none, NULL);\n+    const2reg(opr1, result, lir_patch_none, nullptr);\n@@ -2056,1 +2056,1 @@\n-      const2reg(opr2, result, lir_patch_none, NULL);\n+      const2reg(opr2, result, lir_patch_none, nullptr);\n@@ -2066,1 +2066,1 @@\n-  assert(info == NULL, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n+  assert(info == nullptr, \"should never be used, idiv\/irem and ldiv\/lrem not handled by this method\");\n@@ -2262,1 +2262,1 @@\n-        assert(const_addr != NULL, \"incorrect float\/double constant maintenance\");\n+        assert(const_addr != nullptr, \"incorrect float\/double constant maintenance\");\n@@ -2457,0 +2457,4 @@\n+  } else if (code == lir_f2hf) {\n+    __ flt_to_flt16(dest->as_register(), value->as_xmm_float_reg(), tmp->as_xmm_float_reg());\n+  } else if (code == lir_hf2f) {\n+    __ flt16_to_flt(dest->as_xmm_float_reg(), value->as_register());\n@@ -2665,1 +2669,1 @@\n-        \/\/ All we need for now is a comparison with NULL for equality.\n+        \/\/ All we need for now is a comparison with null for equality.\n@@ -2668,1 +2672,1 @@\n-        if (m == NULL) {\n+        if (m == nullptr) {\n@@ -2676,1 +2680,1 @@\n-        if (o == NULL) {\n+        if (o == nullptr) {\n@@ -2686,1 +2690,1 @@\n-      if (op->info() != NULL) {\n+      if (op->info() != nullptr) {\n@@ -2736,1 +2740,1 @@\n-      if (op->info() != NULL) {\n+      if (op->info() != nullptr) {\n@@ -2757,1 +2761,1 @@\n-      if (op->info() != NULL) {\n+      if (op->info() != nullptr) {\n@@ -2780,1 +2784,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -2886,1 +2890,1 @@\n-  if (stub == NULL) {\n+  if (stub == nullptr) {\n@@ -2896,1 +2900,1 @@\n-  __ mov_metadata(rbx, (Metadata*)NULL);\n+  __ mov_metadata(rbx, (Metadata*)nullptr);\n@@ -3073,1 +3077,1 @@\n-  BasicType basic_type = default_type != NULL ? default_type->element_type()->basic_type() : T_ILLEGAL;\n+  BasicType basic_type = default_type != nullptr ? default_type->element_type()->basic_type() : T_ILLEGAL;\n@@ -3077,1 +3081,1 @@\n-  if (default_type == NULL) {\n+  if (default_type == nullptr) {\n@@ -3096,1 +3100,1 @@\n-    assert(copyfunc_addr != NULL, \"generic arraycopy stub required\");\n+    assert(copyfunc_addr != nullptr, \"generic arraycopy stub required\");\n@@ -3169,1 +3173,1 @@\n-  assert(default_type != NULL && default_type->is_array_klass() && default_type->is_loaded(), \"must be true at this point\");\n+  assert(default_type != nullptr && default_type->is_array_klass() && default_type->is_loaded(), \"must be true at this point\");\n@@ -3199,1 +3203,1 @@\n-  \/\/ test for NULL\n+  \/\/ test for null\n@@ -3274,1 +3278,1 @@\n-      __ check_klass_subtype_fast_path(src, dst, tmp, &cont, &slow, NULL);\n+      __ check_klass_subtype_fast_path(src, dst, tmp, &cont, &slow, nullptr);\n@@ -3290,1 +3294,1 @@\n-      if (copyfunc_addr != NULL) { \/\/ use stub if available\n+      if (copyfunc_addr != nullptr) { \/\/ use stub if available\n@@ -3490,1 +3494,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -3500,1 +3504,1 @@\n-    if (op->info() != NULL) {\n+    if (op->info() != nullptr) {\n@@ -3518,1 +3522,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -3553,1 +3557,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -3555,1 +3559,1 @@\n-  assert(data != NULL && data->is_CounterData(), \"need CounterData for calls\");\n+  assert(data != nullptr && data->is_CounterData(), \"need CounterData for calls\");\n@@ -3568,1 +3572,1 @@\n-    if (C1OptimizeVirtualCallProfiling && known_klass != NULL) {\n+    if (C1OptimizeVirtualCallProfiling && known_klass != nullptr) {\n@@ -3593,1 +3597,1 @@\n-        if (receiver == NULL) {\n+        if (receiver == nullptr) {\n@@ -3630,1 +3634,1 @@\n-  bool exact_klass_set = exact_klass != NULL && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n+  bool exact_klass_set = exact_klass != nullptr && ciTypeEntries::valid_ciklass(current_klass) == exact_klass;\n@@ -3665,1 +3669,1 @@\n-    if (exact_klass != NULL) {\n+    if (exact_klass != nullptr) {\n@@ -3678,2 +3682,2 @@\n-      if (exact_klass == NULL || TypeEntries::is_type_none(current_klass)) {\n-        if (exact_klass != NULL) {\n+      if (exact_klass == nullptr || TypeEntries::is_type_none(current_klass)) {\n+        if (exact_klass != nullptr) {\n@@ -3707,1 +3711,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -3727,1 +3731,1 @@\n-      assert(exact_klass != NULL, \"should be\");\n+      assert(exact_klass != nullptr, \"should be\");\n@@ -3758,1 +3762,1 @@\n-        assert(ciTypeEntries::valid_ciklass(current_klass) != NULL &&\n+        assert(ciTypeEntries::valid_ciklass(current_klass) != nullptr &&\n@@ -3865,1 +3869,1 @@\n-  PatchingStub* patch = NULL;\n+  PatchingStub* patch = nullptr;\n@@ -3874,1 +3878,1 @@\n-  if (patch != NULL) {\n+  if (patch != nullptr) {\n@@ -3884,1 +3888,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n@@ -3894,1 +3898,1 @@\n-  if (info != NULL) {\n+  if (info != nullptr) {\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":83,"deletions":79,"binary":false,"changes":162,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -128,1 +128,1 @@\n-  if (c && c->state_before() == NULL) {\n+  if (c && c->state_before() == nullptr) {\n@@ -146,1 +146,1 @@\n-  return c->type() != T_OBJECT || c->as_jobject() == NULL;\n+  return c->type() != T_OBJECT || c->as_jobject() == nullptr;\n@@ -315,1 +315,1 @@\n-  CodeEmitInfo* info_for_exception = NULL;\n+  CodeEmitInfo* info_for_exception = nullptr;\n@@ -387,1 +387,1 @@\n-    assert(c != NULL, \"invalid constant\");\n+    assert(c != nullptr, \"invalid constant\");\n@@ -433,1 +433,1 @@\n-    address entry = NULL;\n+    address entry = nullptr;\n@@ -503,1 +503,1 @@\n-    address entry = NULL;\n+    address entry = nullptr;\n@@ -531,1 +531,1 @@\n-    arithmetic_op_long(x->op(), reg, left.result(), right.result(), NULL);\n+    arithmetic_op_long(x->op(), reg, left.result(), right.result(), nullptr);\n@@ -543,1 +543,1 @@\n-    arithmetic_op_long(x->op(), x->operand(), left.result(), right.result(), NULL);\n+    arithmetic_op_long(x->op(), x->operand(), left.result(), right.result(), nullptr);\n@@ -587,1 +587,1 @@\n-      info = NULL;\n+      info = nullptr;\n@@ -654,1 +654,1 @@\n-  if (x->is_commutative() && x->y()->as_Constant() == NULL && x->x()->use_count() > x->y()->use_count()) {\n+  if (x->is_commutative() && x->y()->as_Constant() == nullptr && x->x()->use_count() > x->y()->use_count()) {\n@@ -695,1 +695,1 @@\n-  if (x->is_commutative() && x->y()->as_Constant() == NULL && x->x()->use_count() > x->y()->use_count()) {\n+  if (x->is_commutative() && x->y()->as_Constant() == nullptr && x->x()->use_count() > x->y()->use_count()) {\n@@ -836,0 +836,4 @@\n+  if (x->id() == vmIntrinsics::_floatToFloat16) {\n+    tmp = new_register(T_FLOAT);\n+    __ move(LIR_OprFact::floatConst(-0.0), tmp);\n+  }\n@@ -845,0 +849,6 @@\n+    case vmIntrinsics::_floatToFloat16:\n+      __ f2hf(calc_input, calc_result, tmp);\n+      break;\n+    case vmIntrinsics::_float16ToFloat:\n+      __ hf2f(calc_input, calc_result, LIR_OprFact::illegalOpr);\n+      break;\n@@ -861,1 +871,1 @@\n-  CallingConvention* cc = NULL;\n+  CallingConvention* cc = nullptr;\n@@ -886,1 +896,1 @@\n-      if (StubRoutines::dexp() != NULL) {\n+      if (StubRoutines::dexp() != nullptr) {\n@@ -893,1 +903,1 @@\n-      if (StubRoutines::dlog() != NULL) {\n+      if (StubRoutines::dlog() != nullptr) {\n@@ -900,1 +910,1 @@\n-      if (StubRoutines::dlog10() != NULL) {\n+      if (StubRoutines::dlog10() != nullptr) {\n@@ -907,1 +917,1 @@\n-      if (StubRoutines::dpow() != NULL) {\n+      if (StubRoutines::dpow() != nullptr) {\n@@ -914,1 +924,1 @@\n-      if (VM_Version::supports_sse2() && StubRoutines::dsin() != NULL) {\n+      if (VM_Version::supports_sse2() && StubRoutines::dsin() != nullptr) {\n@@ -921,1 +931,1 @@\n-      if (VM_Version::supports_sse2() && StubRoutines::dcos() != NULL) {\n+      if (VM_Version::supports_sse2() && StubRoutines::dcos() != nullptr) {\n@@ -928,1 +938,1 @@\n-      if (StubRoutines::dtan() != NULL) {\n+      if (StubRoutines::dtan() != nullptr) {\n@@ -939,1 +949,1 @@\n-      if (StubRoutines::dexp() != NULL) {\n+      if (StubRoutines::dexp() != nullptr) {\n@@ -946,1 +956,1 @@\n-      if (StubRoutines::dlog() != NULL) {\n+      if (StubRoutines::dlog() != nullptr) {\n@@ -953,1 +963,1 @@\n-      if (StubRoutines::dlog10() != NULL) {\n+      if (StubRoutines::dlog10() != nullptr) {\n@@ -960,1 +970,1 @@\n-       if (StubRoutines::dpow() != NULL) {\n+       if (StubRoutines::dpow() != nullptr) {\n@@ -967,1 +977,1 @@\n-      if (StubRoutines::dsin() != NULL) {\n+      if (StubRoutines::dsin() != nullptr) {\n@@ -974,1 +984,1 @@\n-      if (StubRoutines::dcos() != NULL) {\n+      if (StubRoutines::dcos() != nullptr) {\n@@ -981,1 +991,1 @@\n-       if (StubRoutines::dtan() != NULL) {\n+       if (StubRoutines::dtan() != nullptr) {\n@@ -1253,1 +1263,1 @@\n-  ConversionStub* stub = NULL;\n+  ConversionStub* stub = nullptr;\n@@ -1329,1 +1339,1 @@\n-  CodeEmitInfo* patching_info = NULL;\n+  CodeEmitInfo* patching_info = nullptr;\n@@ -1362,1 +1372,1 @@\n-  LIRItemList* items = new LIRItemList(i, i, NULL);\n+  LIRItemList* items = new LIRItemList(i, i, nullptr);\n@@ -1369,1 +1379,1 @@\n-  CodeEmitInfo* patching_info = NULL;\n+  CodeEmitInfo* patching_info = nullptr;\n@@ -1418,1 +1428,1 @@\n-  CodeEmitInfo* patching_info = NULL;\n+  CodeEmitInfo* patching_info = nullptr;\n@@ -1433,1 +1443,1 @@\n-    assert(patching_info == NULL, \"can't patch this\");\n+    assert(patching_info == nullptr, \"can't patch this\");\n@@ -1436,1 +1446,1 @@\n-    assert(patching_info == NULL, \"can't patch this\");\n+    assert(patching_info == nullptr, \"can't patch this\");\n@@ -1458,1 +1468,1 @@\n-  CodeEmitInfo* patching_info = NULL;\n+  CodeEmitInfo* patching_info = nullptr;\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRGenerator_x86.cpp","additions":45,"deletions":35,"binary":false,"changes":80,"status":"modified"},{"patch":"@@ -101,1 +101,1 @@\n-    \/\/ location (NULL in the displaced hdr location indicates recursive locking)\n+    \/\/ location (null in the displaced hdr location indicates recursive locking)\n@@ -124,1 +124,1 @@\n-    \/\/ if the loaded hdr is NULL we had recursive locking\n+    \/\/ if the loaded hdr is null we had recursive locking\n@@ -305,1 +305,1 @@\n-  \/\/ explicit NULL check not needed since load from [klass_offset] causes a trap\n+  \/\/ explicit null check not needed since load from [klass_offset] causes a trap\n@@ -353,1 +353,1 @@\n-    assert(StubRoutines::x86::check_lock_stack() != NULL, \"need runtime call stub\");\n+    assert(StubRoutines::x86::check_lock_stack() != nullptr, \"need runtime call stub\");\n@@ -361,1 +361,1 @@\n-  bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n+  bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2019, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -122,1 +122,1 @@\n-  void null_check(Register r, Label *Lnull = NULL) { MacroAssembler::null_check(r); }\n+  void null_check(Register r, Label *Lnull = nullptr) { MacroAssembler::null_check(r); }\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2020, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2020, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -39,1 +39,1 @@\n-  assert(SharedRuntime::polling_page_return_handler_blob() != NULL,\n+  assert(SharedRuntime::polling_page_return_handler_blob() != nullptr,\n","filename":"src\/hotspot\/cpu\/x86\/c2_CodeStubs_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -147,1 +147,1 @@\n-    if (BarrierSet::barrier_set()->barrier_set_nmethod() != NULL) {\n+    if (BarrierSet::barrier_set()->barrier_set_nmethod() != nullptr) {\n@@ -164,1 +164,1 @@\n-    bs->nmethod_entry_barrier(this, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n+    bs->nmethod_entry_barrier(this, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n@@ -245,1 +245,1 @@\n-  if (method_data != NULL) {\n+  if (method_data != nullptr) {\n@@ -259,1 +259,1 @@\n-  if (method_data != NULL) {\n+  if (method_data != nullptr) {\n@@ -277,1 +277,1 @@\n-  assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+  assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -287,1 +287,1 @@\n-    assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -373,1 +373,1 @@\n-    assert(stack_rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(stack_rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -433,1 +433,1 @@\n-    assert(rtm_counters != NULL, \"should not be NULL when profiling RTM\");\n+    assert(rtm_counters != nullptr, \"should not be null when profiling RTM\");\n@@ -2236,1 +2236,1 @@\n-  int permconst[] = {1, 14};\n+  const int permconst[] = {1, 14};\n@@ -2329,0 +2329,8 @@\n+void C2_MacroAssembler::movsxl(BasicType typ, Register dst) {\n+  if (typ == T_BYTE) {\n+    movsbl(dst, dst);\n+  } else if (typ == T_SHORT) {\n+    movswl(dst, dst);\n+  }\n+}\n+\n@@ -2340,4 +2348,1 @@\n-      if (typ == T_BYTE)\n-        movsbl(dst, dst);\n-      else if (typ == T_SHORT)\n-        movswl(dst, dst);\n+      movsxl(typ, dst);\n@@ -2347,0 +2352,1 @@\n+    movsxl(typ, dst);\n@@ -3332,3 +3338,3 @@\n-  XMMRegister vcoef[] = { vcoef0, vcoef1, vcoef2, vcoef3 },\n-              vresult[] = { vresult0, vresult1, vresult2, vresult3 },\n-              vtmp[] = { vtmp0, vtmp1, vtmp2, vtmp3 };\n+  const XMMRegister vcoef[] = { vcoef0, vcoef1, vcoef2, vcoef3 },\n+                    vresult[] = { vresult0, vresult1, vresult2, vresult3 },\n+                    vtmp[] = { vtmp0, vtmp1, vtmp2, vtmp3 };\n@@ -6142,0 +6148,11 @@\n+void C2_MacroAssembler::vector_rearrange_int_float(BasicType bt, XMMRegister dst,\n+                                                   XMMRegister shuffle, XMMRegister src, int vlen_enc) {\n+  if (vlen_enc == AVX_128bit) {\n+    vpermilps(dst, src, shuffle, vlen_enc);\n+  } else if (bt == T_INT) {\n+    vpermd(dst, shuffle, src, vlen_enc);\n+  } else {\n+    assert(bt == T_FLOAT, \"\");\n+    vpermps(dst, shuffle, src, vlen_enc);\n+  }\n+}\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":33,"deletions":16,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -136,0 +136,1 @@\n+  void movsxl(BasicType typ, Register dst);\n@@ -487,0 +488,3 @@\n+  void vector_rearrange_int_float(BasicType bt, XMMRegister dst, XMMRegister shuffle,\n+                                  XMMRegister src, int vlen_enc);\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -271,1 +271,1 @@\n-         \" last_sp != NULL\");\n+         \" last_sp != null\");\n@@ -303,1 +303,1 @@\n-         \" last_sp != NULL\");\n+         \" last_sp != nullptr\");\n@@ -402,1 +402,1 @@\n-    jcc(Assembler::zero, L); \/\/ if (thread->jvmti_thread_state() == NULL) exit;\n+    jcc(Assembler::zero, L); \/\/ if (thread->jvmti_thread_state() == nullptr) exit;\n@@ -1412,1 +1412,1 @@\n-  \/\/ Test MDO to avoid the call if it is NULL.\n+  \/\/ Test MDO to avoid the call if it is null.\n@@ -1795,1 +1795,1 @@\n-  \/\/ observed the item[start_row] is NULL.\n+  \/\/ observed the item[start_row] is null.\n@@ -1811,1 +1811,1 @@\n-\/\/   if (row[0].rec != NULL) {\n+\/\/   if (row[0].rec != nullptr) {\n@@ -1814,1 +1814,1 @@\n-\/\/     if (row[1].rec != NULL) {\n+\/\/     if (row[1].rec != nullptr) {\n@@ -1817,1 +1817,1 @@\n-\/\/       if (row[2].rec != NULL) { count.incr(); goto done; } \/\/ overflow\n+\/\/       if (row[2].rec != nullptr) { count.incr(); goto done; } \/\/ overflow\n@@ -2023,1 +2023,1 @@\n-  if (where != NULL) {\n+  if (where != nullptr) {\n@@ -2100,0 +2100,14 @@\n+\n+void InterpreterMacroAssembler::load_resolved_indy_entry(Register cache, Register index) {\n+  \/\/ Get index out of bytecode pointer, get_cache_entry_pointer_at_bcp\n+  get_cache_index_at_bcp(index, 1, sizeof(u4));\n+  \/\/ Get address of invokedynamic array\n+  movptr(cache, Address(rbp, frame::interpreter_frame_cache_offset * wordSize));\n+  movptr(cache, Address(cache, in_bytes(ConstantPoolCache::invokedynamic_entries_offset())));\n+  if (is_power_of_2(sizeof(ResolvedIndyEntry))) {\n+    shll(index, log2i_exact(sizeof(ResolvedIndyEntry))); \/\/ Scale index by power of 2\n+  } else {\n+    imull(index, index, sizeof(ResolvedIndyEntry)); \/\/ Scale the index to be the entry index * sizeof(ResolvedInvokeDynamicInfo)\n+  }\n+  lea(cache, Address(cache, index, Address::times_1, Array<ResolvedIndyEntry>::base_offset_in_bytes()));\n+}\n","filename":"src\/hotspot\/cpu\/x86\/interp_masm_x86.cpp","additions":23,"deletions":9,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -46,1 +46,0 @@\n-#include \"runtime\/flags\/flagSetting.hpp\"\n@@ -72,1 +71,1 @@\n-static Assembler::Condition reverse[] = {\n+static const Assembler::Condition reverse[] = {\n@@ -407,1 +406,1 @@\n-  FlagSetting fs(Debugging, true);\n+  DebuggingContext debugging{};\n@@ -836,1 +835,1 @@\n-  FlagSetting fs(Debugging, true);\n+  DebuggingContext debugging{};\n@@ -1040,1 +1039,1 @@\n-  \/\/ See if oop is NULL if it is we need no handle\n+  \/\/ See if oop is null if it is we need no handle\n@@ -1053,1 +1052,1 @@\n-    \/\/ conditionally move a NULL\n+    \/\/ conditionally move a null\n@@ -1058,1 +1057,1 @@\n-    \/\/ on the stack for oop_handles and pass a handle if oop is non-NULL\n+    \/\/ on the stack for oop_handles and pass a handle if oop is non-null\n@@ -1081,1 +1080,1 @@\n-    \/\/ Store oop in handle area, may be NULL\n+    \/\/ Store oop in handle area, may be null\n@@ -1089,1 +1088,1 @@\n-    \/\/ conditionally move a NULL from the handle area where it was just stored\n+    \/\/ conditionally move a null from the handle area where it was just stored\n@@ -1348,1 +1347,1 @@\n-  mov_metadata(rbx, (Metadata*) NULL);  \/\/ Method is zapped till fixup time.\n+  mov_metadata(rbx, (Metadata*) nullptr);  \/\/ Method is zapped till fixup time.\n@@ -1566,1 +1565,1 @@\n-  set_last_Java_frame(java_thread, last_java_sp, rbp, NULL, rscratch1);\n+  set_last_Java_frame(java_thread, last_java_sp, rbp, nullptr, rscratch1);\n@@ -2858,1 +2857,1 @@\n-    \/\/ provoke OS NULL exception if reg = NULL by\n+    \/\/ provoke OS null exception if reg is null by\n@@ -2867,1 +2866,1 @@\n-    \/\/ will provoke OS NULL exception if reg = NULL\n+    \/\/ will provoke OS null exception if reg is null\n@@ -2878,1 +2877,1 @@\n-  const char* buf = NULL;\n+  const char* buf = nullptr;\n@@ -3109,1 +3108,1 @@\n-  if (last_java_pc != NULL) {\n+  if (last_java_pc != nullptr) {\n@@ -3891,1 +3890,1 @@\n-  jcc(Assembler::zero, done);           \/\/ Use NULL as-is.\n+  jcc(Assembler::zero, done);           \/\/ Use null as-is.\n@@ -3925,1 +3924,1 @@\n-  jcc(Assembler::zero, done);           \/\/ Use NULL as-is.\n+  jcc(Assembler::zero, done);           \/\/ Use null as-is.\n@@ -4278,1 +4277,1 @@\n-  \/\/ for (scan = klass->itable(); scan->interface() != NULL; scan += scan_step) {\n+  \/\/ for (scan = klass->itable(); scan->interface() != nullptr; scan += scan_step) {\n@@ -4336,2 +4335,2 @@\n-  check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &L_success, &L_failure, NULL);\n-  check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &L_success, NULL);\n+  check_klass_subtype_fast_path(sub_klass, super_klass, temp_reg,        &L_success, &L_failure, nullptr);\n+  check_klass_subtype_slow_path(sub_klass, super_klass, temp_reg, noreg, &L_success, nullptr);\n@@ -4360,4 +4359,4 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  if (L_slow_path == NULL) { L_slow_path = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  if (L_slow_path == nullptr) { L_slow_path = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -4459,3 +4458,3 @@\n-  if (L_success == NULL)   { L_success   = &L_fallthrough; label_nulls++; }\n-  if (L_failure == NULL)   { L_failure   = &L_fallthrough; label_nulls++; }\n-  assert(label_nulls <= 1, \"at most one NULL in the batch\");\n+  if (L_success == nullptr)   { L_success   = &L_fallthrough; label_nulls++; }\n+  if (L_failure == nullptr)   { L_failure   = &L_fallthrough; label_nulls++; }\n+  assert(label_nulls <= 1, \"at most one null in the batch\");\n@@ -4517,1 +4516,1 @@\n-    assert(!pushed_rdi, \"rdi must be left non-NULL\");\n+    assert(!pushed_rdi, \"rdi must be left non-null\");\n@@ -4538,1 +4537,1 @@\n-  assert(L_fast_path != NULL || L_slow_path != NULL, \"at least one is required\");\n+  assert(L_fast_path != nullptr || L_slow_path != nullptr, \"at least one is required\");\n@@ -4541,1 +4540,1 @@\n-  if (L_fast_path == NULL) {\n+  if (L_fast_path == nullptr) {\n@@ -4543,1 +4542,1 @@\n-  } else if (L_slow_path == NULL) {\n+  } else if (L_slow_path == nullptr) {\n@@ -4597,1 +4596,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -4667,1 +4666,1 @@\n-  const char* b = NULL;\n+  const char* b = nullptr;\n@@ -4736,1 +4735,1 @@\n-        rc = NULL; \/\/ silence compiler warnings\n+        rc = nullptr; \/\/ silence compiler warnings\n@@ -4747,1 +4746,1 @@\n-        pc = NULL; \/\/ silence compiler warnings\n+        pc = nullptr; \/\/ silence compiler warnings\n@@ -4869,1 +4868,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -5170,9 +5169,0 @@\n-void MacroAssembler::load_klass_check_null(Register dst, Register src, Register tmp) {\n-  if (UseCompactObjectHeaders) {\n-    null_check(src, oopDesc::mark_offset_in_bytes());\n-  } else {\n-    null_check(src, oopDesc::klass_offset_in_bytes());\n-  }\n-  load_klass(dst, src, tmp);\n-}\n-\n@@ -5271,1 +5261,1 @@\n-\/\/ Used for storing NULLs.\n+\/\/ Used for storing nulls.\n@@ -5287,1 +5277,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -5312,1 +5302,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -5337,1 +5327,1 @@\n-  if (CompressedOops::base() != NULL) {\n+  if (CompressedOops::base() != nullptr) {\n@@ -5361,1 +5351,1 @@\n-  if (CompressedOops::base() != NULL) {\n+  if (CompressedOops::base() != nullptr) {\n@@ -5374,1 +5364,1 @@\n-  if (CompressedOops::base() == NULL) {\n+  if (CompressedOops::base() == nullptr) {\n@@ -5392,1 +5382,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -5399,1 +5389,1 @@\n-    if (CompressedOops::base() != NULL) {\n+    if (CompressedOops::base() != nullptr) {\n@@ -5403,1 +5393,1 @@\n-    assert (CompressedOops::base() == NULL, \"sanity\");\n+    assert (CompressedOops::base() == nullptr, \"sanity\");\n@@ -5410,1 +5400,1 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n@@ -5423,1 +5413,1 @@\n-      if (CompressedOops::base() != NULL) {\n+      if (CompressedOops::base() != nullptr) {\n@@ -5428,1 +5418,1 @@\n-    assert (CompressedOops::base() == NULL, \"sanity\");\n+    assert (CompressedOops::base() == nullptr, \"sanity\");\n@@ -5612,2 +5602,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5621,2 +5611,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5630,1 +5620,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5638,1 +5628,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5646,2 +5636,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5655,2 +5645,2 @@\n-  assert (Universe::heap() != NULL, \"java heap should be initialized\");\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (Universe::heap() != nullptr, \"java heap should be initialized\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5664,1 +5654,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5672,1 +5662,1 @@\n-  assert (oop_recorder() != NULL, \"this assembler needs an OopRecorder\");\n+  assert (oop_recorder() != nullptr, \"this assembler needs an OopRecorder\");\n@@ -5680,2 +5670,2 @@\n-    if (Universe::heap() != NULL) {\n-      if (CompressedOops::base() == NULL) {\n+    if (Universe::heap() != nullptr) {\n+      if (CompressedOops::base() == nullptr) {\n@@ -9380,1 +9370,1 @@\n-  BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n+  const BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n@@ -9396,1 +9386,1 @@\n-  BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n+  const BasicType type[] = { T_BYTE, T_SHORT, T_INT, T_LONG};\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":63,"deletions":73,"binary":false,"changes":136,"status":"modified"},{"patch":"@@ -116,1 +116,1 @@\n-  \/\/ Support for NULL-checks\n+  \/\/ Support for null-checks\n@@ -118,1 +118,1 @@\n-  \/\/ Generates code that causes a NULL OS exception if the content of reg is NULL.\n+  \/\/ Generates code that causes a null OS exception if the content of reg is null.\n@@ -144,1 +144,1 @@\n-                file == NULL ? \"<NULL>\" : file, line);\n+                file == nullptr ? \"<null>\" : file, line);\n@@ -187,0 +187,5 @@\n+  void incrementl(AddressLiteral dst, Register rscratch = noreg);\n+  void incrementl(ArrayAddress   dst, Register rscratch);\n+\n+  void incrementq(AddressLiteral dst, Register rscratch = noreg);\n+\n@@ -214,2 +219,7 @@\n-  void incrementl(AddressLiteral dst, Register rscratch = noreg);\n-  void incrementl(ArrayAddress   dst, Register rscratch);\n+  void flt_to_flt16(Register dst, XMMRegister src, XMMRegister tmp) {\n+    \/\/ Use separate tmp XMM register because caller may\n+    \/\/ requires src XMM register to be unchanged (as in x86.ad).\n+    vcvtps2ph(tmp, src, 0x04, Assembler::AVX_128bit);\n+    movdl(dst, tmp);\n+    movswl(dst, dst);\n+  }\n@@ -217,1 +227,4 @@\n-  void incrementq(AddressLiteral dst, Register rscratch = noreg);\n+  void flt16_to_flt(XMMRegister dst, Register src) {\n+    movdl(dst, src);\n+    vcvtph2ps(dst, dst, Assembler::AVX_128bit);\n+  }\n@@ -377,1 +390,0 @@\n-  void load_klass_check_null(Register dst, Register src, Register tmp);\n@@ -400,1 +412,1 @@\n-  \/\/ Used for storing NULL. All other oop constants should be\n+  \/\/ Used for storing null. All other oop constants should be\n@@ -408,1 +420,1 @@\n-  \/\/ converting a zero (like NULL) into a Register by giving\n+  \/\/ converting a zero (like null) into a Register by giving\n@@ -633,1 +645,1 @@\n-  \/\/ One of the three labels can be NULL, meaning take the fall-through.\n+  \/\/ One of the three labels can be null, meaning take the fall-through.\n@@ -666,2 +678,2 @@\n-                      Label* L_fast_path = NULL,\n-                      Label* L_slow_path = NULL);\n+                      Label* L_fast_path = nullptr,\n+                      Label* L_slow_path = nullptr);\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":24,"deletions":12,"binary":false,"changes":36,"status":"modified"},{"patch":"@@ -748,1 +748,1 @@\n-      (Interpreter::code() != NULL || StubRoutines::code1() != NULL)) {\n+      (Interpreter::code() != nullptr || StubRoutines::final_stubs_code() != nullptr)) {\n@@ -755,1 +755,1 @@\n-    if (Interpreter::code() != NULL)\n+    if (Interpreter::code() != nullptr) {\n@@ -759,1 +759,2 @@\n-    if (StubRoutines::code1() != NULL)\n+    }\n+    if (StubRoutines::initial_stubs_code() != nullptr) {\n@@ -761,1 +762,2 @@\n-                  StubRoutines::code1()->code_begin(), StubRoutines::code1()->code_end(),\n+                  StubRoutines::initial_stubs_code()->code_begin(),\n+                  StubRoutines::initial_stubs_code()->code_end(),\n@@ -763,1 +765,2 @@\n-    if (StubRoutines::code2() != NULL)\n+    }\n+    if (StubRoutines::final_stubs_code() != nullptr) {\n@@ -765,1 +768,2 @@\n-                  StubRoutines::code2()->code_begin(), StubRoutines::code2()->code_end(),\n+                  StubRoutines::final_stubs_code()->code_begin(),\n+                  StubRoutines::final_stubs_code()->code_end(),\n@@ -767,0 +771,1 @@\n+    }\n@@ -971,1 +976,0 @@\n-  __ flush();\n@@ -979,1 +983,1 @@\n-  assert(regs2 == NULL, \"not needed on x86\");\n+  assert(regs2 == nullptr, \"not needed on x86\");\n@@ -1331,1 +1335,1 @@\n-                                       (OopMapSet*)NULL);\n+                                       (OopMapSet*)nullptr);\n@@ -1334,1 +1338,1 @@\n-  assert(native_func != NULL, \"must have function\");\n+  assert(native_func != nullptr, \"must have function\");\n@@ -1350,1 +1354,1 @@\n-  BasicType* in_elem_bt = NULL;\n+  BasicType* in_elem_bt = nullptr;\n@@ -1365,1 +1369,1 @@\n-  out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);\n+  out_arg_slots = c_calling_convention(out_sig_bt, out_regs, nullptr, total_c_args);\n@@ -1491,1 +1495,1 @@\n-  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n+  bs->nmethod_entry_barrier(masm, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n@@ -1777,3 +1781,5 @@\n-  __ membar(Assembler::Membar_mask_bits(\n-            Assembler::LoadLoad | Assembler::LoadStore |\n-            Assembler::StoreLoad | Assembler::StoreStore));\n+  if (!UseSystemMemoryBarrier) {\n+    __ membar(Assembler::Membar_mask_bits(\n+              Assembler::LoadLoad | Assembler::LoadStore |\n+              Assembler::StoreLoad | Assembler::StoreStore));\n+  }\n@@ -2089,1 +2095,1 @@\n-  OopMap* map = NULL;\n+  OopMap* map = nullptr;\n@@ -2218,1 +2224,1 @@\n-  __ set_last_Java_frame(rcx, noreg, noreg, NULL, noreg);\n+  __ set_last_Java_frame(rcx, noreg, noreg, nullptr, noreg);\n@@ -2366,1 +2372,1 @@\n-  __ set_last_Java_frame(rcx, noreg, rbp, NULL, noreg);\n+  __ set_last_Java_frame(rcx, noreg, rbp, nullptr, noreg);\n@@ -2463,1 +2469,1 @@\n-  __ set_last_Java_frame(rdx, noreg, noreg, NULL, noreg);\n+  __ set_last_Java_frame(rdx, noreg, noreg, nullptr, noreg);\n@@ -2575,1 +2581,1 @@\n-  __ set_last_Java_frame(rdi, noreg, rbp, NULL, noreg);\n+  __ set_last_Java_frame(rdi, noreg, rbp, nullptr, noreg);\n@@ -2615,1 +2621,1 @@\n-  assert (StubRoutines::forward_exception_entry() != NULL, \"must be generated before\");\n+  assert (StubRoutines::forward_exception_entry() != nullptr, \"must be generated before\");\n@@ -2628,1 +2634,1 @@\n-  address call_pc = NULL;\n+  address call_pc = nullptr;\n@@ -2657,1 +2663,1 @@\n-  __ set_last_Java_frame(java_thread, noreg, noreg, NULL, noreg);\n+  __ set_last_Java_frame(java_thread, noreg, noreg, nullptr, noreg);\n@@ -2770,1 +2776,1 @@\n-  assert (StubRoutines::forward_exception_entry() != NULL, \"must be generated before\");\n+  assert (StubRoutines::forward_exception_entry() != nullptr, \"must be generated before\");\n@@ -2784,1 +2790,1 @@\n-  OopMap* map = NULL;\n+  OopMap* map = nullptr;\n@@ -2796,1 +2802,1 @@\n-  __ set_last_Java_frame(thread, noreg, rbp, NULL, noreg);\n+  __ set_last_Java_frame(thread, noreg, rbp, nullptr, noreg);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_32.cpp","additions":33,"deletions":27,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -799,1 +799,1 @@\n-      (Interpreter::code() != NULL || StubRoutines::code1() != NULL)) {\n+      (Interpreter::code() != nullptr || StubRoutines::final_stubs_code() != nullptr)) {\n@@ -808,1 +808,1 @@\n-    if (Interpreter::code() != NULL)\n+    if (Interpreter::code() != nullptr) {\n@@ -810,1 +810,2 @@\n-                  Interpreter::code()->code_start(), Interpreter::code()->code_end(),\n+                  Interpreter::code()->code_start(),\n+                  Interpreter::code()->code_end(),\n@@ -812,1 +813,2 @@\n-    if (StubRoutines::code1() != NULL)\n+    }\n+    if (StubRoutines::initial_stubs_code() != nullptr) {\n@@ -814,1 +816,2 @@\n-                  StubRoutines::code1()->code_begin(), StubRoutines::code1()->code_end(),\n+                  StubRoutines::initial_stubs_code()->code_begin(),\n+                  StubRoutines::initial_stubs_code()->code_end(),\n@@ -816,1 +819,2 @@\n-    if (StubRoutines::code2() != NULL)\n+    }\n+    if (StubRoutines::final_stubs_code() != nullptr) {\n@@ -818,1 +822,2 @@\n-                  StubRoutines::code2()->code_begin(), StubRoutines::code2()->code_end(),\n+                  StubRoutines::final_stubs_code()->code_begin(),\n+                  StubRoutines::final_stubs_code()->code_end(),\n@@ -820,0 +825,1 @@\n+    }\n@@ -1017,1 +1023,1 @@\n-  address c2i_no_clinit_check_entry = NULL;\n+  address c2i_no_clinit_check_entry = nullptr;\n@@ -1044,1 +1050,0 @@\n-  __ flush();\n@@ -1052,1 +1057,1 @@\n-  assert(regs2 == NULL, \"not needed on x86\");\n+  assert(regs2 == nullptr, \"not needed on x86\");\n@@ -1765,1 +1770,1 @@\n-                                       (OopMapSet*)NULL);\n+                                       nullptr);\n@@ -1768,1 +1773,1 @@\n-  assert(native_func != NULL, \"must have function\");\n+  assert(native_func != nullptr, \"must have function\");\n@@ -1785,1 +1790,1 @@\n-  BasicType* in_elem_bt = NULL;\n+  BasicType* in_elem_bt = nullptr;\n@@ -1800,1 +1805,1 @@\n-  out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);\n+  out_arg_slots = c_calling_convention(out_sig_bt, out_regs, nullptr, total_c_args);\n@@ -1930,1 +1935,1 @@\n-  bs->nmethod_entry_barrier(masm, NULL \/* slow_path *\/, NULL \/* continuation *\/);\n+  bs->nmethod_entry_barrier(masm, nullptr \/* slow_path *\/, nullptr \/* continuation *\/);\n@@ -2539,1 +2544,1 @@\n-  OopMap* map = NULL;\n+  OopMap* map = nullptr;\n@@ -2617,1 +2622,1 @@\n-    __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);\n+    __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n@@ -2699,1 +2704,1 @@\n-  __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n@@ -2732,1 +2737,1 @@\n-  \/\/ QQQ this is useless it was NULL above\n+  \/\/ QQQ this is useless it was null above\n@@ -2918,1 +2923,1 @@\n-  __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n@@ -3076,1 +3081,1 @@\n-  assert(StubRoutines::forward_exception_entry() != NULL,\n+  assert(StubRoutines::forward_exception_entry() != nullptr,\n@@ -3088,1 +3093,1 @@\n-  address call_pc = NULL;\n+  address call_pc = nullptr;\n@@ -3112,1 +3117,1 @@\n-  __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);  \/\/ JavaFrameAnchor::capture_last_Java_pc() will get the pc from the return address, which we store next:\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);  \/\/ JavaFrameAnchor::capture_last_Java_pc() will get the pc from the return address, which we store next:\n@@ -3242,1 +3247,1 @@\n-  assert (StubRoutines::forward_exception_entry() != NULL, \"must be generated before\");\n+  assert (StubRoutines::forward_exception_entry() != nullptr, \"must be generated before\");\n@@ -3253,1 +3258,1 @@\n-  OopMap* map = NULL;\n+  OopMap* map = nullptr;\n@@ -3262,1 +3267,1 @@\n-  __ set_last_Java_frame(noreg, noreg, NULL, rscratch1);\n+  __ set_last_Java_frame(noreg, noreg, nullptr, rscratch1);\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86_64.cpp","additions":31,"deletions":26,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -462,1 +462,1 @@\n-  assert(StubRoutines::_call_stub_return_address != NULL,\n+  assert(StubRoutines::_call_stub_return_address != nullptr,\n@@ -1094,1 +1094,1 @@\n-  __ jcc(Assembler::zero, exit); \/\/ if obj is NULL it is OK\n+  __ jcc(Assembler::zero, exit); \/\/ if obj is null it is OK\n@@ -1165,1 +1165,1 @@\n-  if (nargs >= 4)\n+  if (nargs == 4) {\n@@ -1167,0 +1167,1 @@\n+  }\n@@ -1172,1 +1173,1 @@\n-  if (nargs >= 4)\n+  if (nargs == 4) {\n@@ -1174,0 +1175,1 @@\n+  }\n@@ -1195,1 +1197,1 @@\n-void StubGenerator::setup_arg_regs_using_thread() {\n+void StubGenerator::setup_arg_regs_using_thread(int nargs) {\n@@ -1197,0 +1199,1 @@\n+  assert(nargs == 3 || nargs == 4, \"else fix\");\n@@ -1198,0 +1201,3 @@\n+  if (nargs == 4) {\n+    __ mov(rax, r9);       \/\/ r9 is also saved_r15\n+  }\n@@ -1208,0 +1214,3 @@\n+  if (nargs == 4) {\n+    __ mov(rcx, rax); \/\/ c_rarg3 (via rax)\n+  }\n@@ -3561,0 +3570,49 @@\n+\/**\n+*  Arguments:\n+*\n+*  Input:\n+*    c_rarg0   - float16  jshort\n+*\n+*  Output:\n+*       xmm0   - float\n+*\/\n+address StubGenerator::generate_float16ToFloat() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"float16ToFloat\");\n+\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  \/\/ No need for RuntimeStub frame since it is called only during JIT compilation\n+\n+  \/\/ Load value into xmm0 and convert\n+  __ flt16_to_flt(xmm0, c_rarg0);\n+\n+  __ ret(0);\n+\n+  return start;\n+}\n+\n+\/**\n+*  Arguments:\n+*\n+*  Input:\n+*       xmm0   - float\n+*\n+*  Output:\n+*        rax   - float16  jshort\n+*\/\n+address StubGenerator::generate_floatToFloat16() {\n+  StubCodeMark mark(this, \"StubRoutines\", \"floatToFloat16\");\n+\n+  address start = __ pc();\n+\n+  BLOCK_COMMENT(\"Entry:\");\n+  \/\/ No need for RuntimeStub frame since it is called only during JIT compilation\n+\n+  \/\/ Convert and put result into rax\n+  __ flt_to_flt16(rax, xmm0, xmm1);\n+\n+  __ ret(0);\n+\n+  return start;\n+}\n@@ -3856,1 +3914,1 @@\n-void StubGenerator::generate_initial() {\n+void StubGenerator::generate_initial_stubs() {\n@@ -3862,0 +3920,5 @@\n+  \/\/ Initialize table for unsafe copy memeory check.\n+  if (UnsafeCopyMemory::_table == nullptr) {\n+    UnsafeCopyMemory::create_table(16);\n+  }\n+\n@@ -3911,4 +3974,0 @@\n-  if (UsePoly1305Intrinsics) {\n-    StubRoutines::_poly1305_processBlocks = generate_poly1305_processBlocks();\n-  }\n-\n@@ -3922,2 +3981,8 @@\n-  if (UseAdler32Intrinsics) {\n-     StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+  if (VM_Version::supports_float16()) {\n+    \/\/ For results consistency both intrinsics should be enabled.\n+    \/\/ vmIntrinsics checks InlineIntrinsics flag, no need to check it here.\n+    if (vmIntrinsics::is_intrinsic_available(vmIntrinsics::_float16ToFloat) &&\n+        vmIntrinsics::is_intrinsic_available(vmIntrinsics::_floatToFloat16)) {\n+      StubRoutines::_hf2f = generate_float16ToFloat();\n+      StubRoutines::_f2hf = generate_floatToFloat16();\n+    }\n@@ -3927,0 +3992,4 @@\n+\n+  if (UseFastLocking) {\n+    StubRoutines::x86::_check_lock_stack = generate_check_lock_stack();\n+  }\n@@ -3929,1 +3998,1 @@\n-void StubGenerator::generate_phase1() {\n+void StubGenerator::generate_continuation_stubs() {\n@@ -3939,2 +4008,2 @@\n-void StubGenerator::generate_all() {\n-  \/\/ Generates all stubs and initializes the entry points\n+void StubGenerator::generate_final_stubs() {\n+  \/\/ Generates the rest of stubs and initializes the entry points\n@@ -3963,1 +4032,27 @@\n-  \/\/ entry points that are platform specific\n+  \/\/ support for verify_oop (must happen after universe_init)\n+  if (VerifyOops) {\n+    StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n+  }\n+\n+  \/\/ data cache line writeback\n+  StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n+  StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n+\n+  \/\/ arraycopy stubs used by compilers\n+  generate_arraycopy_stubs();\n+\n+  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n+  if (bs_nm != nullptr) {\n+    StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n+  }\n+\n+  if (UseVectorizedMismatchIntrinsic) {\n+    StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n+  }\n+}\n+\n+void StubGenerator::generate_compiler_stubs() {\n+#if COMPILER2_OR_JVMCI\n+\n+  \/\/ Entry points that are C2 compiler specific.\n+\n@@ -3995,12 +4090,0 @@\n-  \/\/ support for verify_oop (must happen after universe_init)\n-  if (VerifyOops) {\n-    StubRoutines::_verify_oop_subroutine_entry = generate_verify_oop();\n-  }\n-\n-  \/\/ data cache line writeback\n-  StubRoutines::_data_cache_writeback = generate_data_cache_writeback();\n-  StubRoutines::_data_cache_writeback_sync = generate_data_cache_writeback_sync();\n-\n-  \/\/ arraycopy stubs used by compilers\n-  generate_arraycopy_stubs();\n-\n@@ -4013,0 +4096,8 @@\n+  if (UseAdler32Intrinsics) {\n+     StubRoutines::_updateBytesAdler32 = generate_updateBytesAdler32();\n+  }\n+\n+  if (UsePoly1305Intrinsics) {\n+    StubRoutines::_poly1305_processBlocks = generate_poly1305_processBlocks();\n+  }\n+\n@@ -4017,0 +4108,1 @@\n+\n@@ -4023,0 +4115,1 @@\n+\n@@ -4036,0 +4129,1 @@\n+\n@@ -4068,7 +4162,0 @@\n-  BarrierSetNMethod* bs_nm = BarrierSet::barrier_set()->barrier_set_nmethod();\n-  if (bs_nm != NULL) {\n-    StubRoutines::x86::_method_entry_barrier = generate_method_entry_barrier();\n-  }\n-  if (UseFastLocking) {\n-    StubRoutines::x86::_check_lock_stack = generate_check_lock_stack();\n-  }\n@@ -4099,1 +4186,1 @@\n-  void *libjsvml = NULL;\n+  void *libjsvml = nullptr;\n@@ -4105,1 +4192,1 @@\n-  if (libjsvml != NULL) {\n+  if (libjsvml != nullptr) {\n@@ -4162,0 +4249,2 @@\n+#endif \/\/ COMPILER2_OR_JVMCI\n+}\n@@ -4163,3 +4252,19 @@\n-  if (UseVectorizedMismatchIntrinsic) {\n-    StubRoutines::_vectorizedMismatch = generate_vectorizedMismatch();\n-  }\n+StubGenerator::StubGenerator(CodeBuffer* code, StubsKind kind) : StubCodeGenerator(code) {\n+    DEBUG_ONLY( _regs_in_thread = false; )\n+    switch(kind) {\n+    case Initial_stubs:\n+      generate_initial_stubs();\n+      break;\n+     case Continuation_stubs:\n+      generate_continuation_stubs();\n+      break;\n+    case Compiler_stubs:\n+      generate_compiler_stubs();\n+      break;\n+    case Final_stubs:\n+      generate_final_stubs();\n+      break;\n+    default:\n+      fatal(\"unexpected stubs kind: %d\", kind);\n+      break;\n+    };\n@@ -4168,5 +4273,2 @@\n-void StubGenerator_generate(CodeBuffer* code, int phase) {\n-  if (UnsafeCopyMemory::_table == NULL) {\n-    UnsafeCopyMemory::create_table(16);\n-  }\n-  StubGenerator g(code, phase);\n+void StubGenerator_generate(CodeBuffer* code, StubCodeGenerator::StubsKind kind) {\n+  StubGenerator g(code, kind);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.cpp","additions":147,"deletions":45,"binary":false,"changes":192,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -125,2 +125,2 @@\n-    assert(no_overlap_target != NULL, \"must be generated\");\n-    array_overlap_test(no_overlap_target, NULL, sf);\n+    assert(no_overlap_target != nullptr, \"must be generated\");\n+    array_overlap_test(no_overlap_target, nullptr, sf);\n@@ -129,1 +129,1 @@\n-    array_overlap_test(NULL, &L_no_overlap, sf);\n+    array_overlap_test(nullptr, &L_no_overlap, sf);\n@@ -143,1 +143,1 @@\n-  void setup_arg_regs_using_thread();\n+  void setup_arg_regs_using_thread(int nargs = 3);\n@@ -149,2 +149,4 @@\n-                          Register qword_count, Register to,\n-                          Label& L_copy_bytes, Label& L_copy_8_bytes);\n+                          Register qword_count, Register tmp1,\n+                          Register tmp2, Label& L_copy_bytes,\n+                          Label& L_copy_8_bytes, DecoratorSet decorators,\n+                          BasicType type);\n@@ -154,2 +156,4 @@\n-                           Register qword_count, Register to,\n-                           Label& L_copy_bytes, Label& L_copy_8_bytes);\n+                           Register qword_count, Register tmp1,\n+                           Register tmp2, Label& L_copy_bytes,\n+                           Label& L_copy_8_bytes, DecoratorSet decorators,\n+                           BasicType type);\n@@ -475,0 +479,2 @@\n+  address generate_float16ToFloat();\n+  address generate_floatToFloat16();\n@@ -549,3 +555,4 @@\n-  void generate_initial();\n-  void generate_phase1();\n-  void generate_all();\n+  void generate_initial_stubs();\n+  void generate_continuation_stubs();\n+  void generate_compiler_stubs();\n+  void generate_final_stubs();\n@@ -554,10 +561,1 @@\n-  StubGenerator(CodeBuffer* code, int phase) : StubCodeGenerator(code) {\n-    DEBUG_ONLY( _regs_in_thread = false; )\n-    if (phase == 0) {\n-      generate_initial();\n-    } else if (phase == 1) {\n-      generate_phase1(); \/\/ stubs that must be available for the interpreter\n-    } else {\n-      generate_all();\n-    }\n-  }\n+  StubGenerator(CodeBuffer* code, StubsKind kind);\n","filename":"src\/hotspot\/cpu\/x86\/stubGenerator_x86_64.hpp","additions":20,"deletions":22,"binary":false,"changes":42,"status":"modified"},{"patch":"@@ -36,28 +36,28 @@\n-address StubRoutines::x86::_verify_mxcsr_entry = NULL;\n-address StubRoutines::x86::_upper_word_mask_addr = NULL;\n-address StubRoutines::x86::_shuffle_byte_flip_mask_addr = NULL;\n-address StubRoutines::x86::_k256_adr = NULL;\n-address StubRoutines::x86::_vector_short_to_byte_mask = NULL;\n-address StubRoutines::x86::_vector_int_to_byte_mask = NULL;\n-address StubRoutines::x86::_vector_int_to_short_mask = NULL;\n-address StubRoutines::x86::_vector_all_bits_set = NULL;\n-address StubRoutines::x86::_vector_byte_shuffle_mask = NULL;\n-address StubRoutines::x86::_vector_int_mask_cmp_bits = NULL;\n-address StubRoutines::x86::_vector_short_shuffle_mask = NULL;\n-address StubRoutines::x86::_vector_int_shuffle_mask = NULL;\n-address StubRoutines::x86::_vector_long_shuffle_mask = NULL;\n-address StubRoutines::x86::_vector_float_sign_mask = NULL;\n-address StubRoutines::x86::_vector_float_sign_flip = NULL;\n-address StubRoutines::x86::_vector_double_sign_mask = NULL;\n-address StubRoutines::x86::_vector_double_sign_flip = NULL;\n-address StubRoutines::x86::_vector_byte_perm_mask = NULL;\n-address StubRoutines::x86::_vector_long_sign_mask = NULL;\n-address StubRoutines::x86::_vector_iota_indices = NULL;\n-address StubRoutines::x86::_vector_reverse_bit_lut = NULL;\n-address StubRoutines::x86::_vector_reverse_byte_perm_mask_long = NULL;\n-address StubRoutines::x86::_vector_reverse_byte_perm_mask_int = NULL;\n-address StubRoutines::x86::_vector_reverse_byte_perm_mask_short = NULL;\n-address StubRoutines::x86::_vector_popcount_lut = NULL;\n-address StubRoutines::x86::_vector_count_leading_zeros_lut = NULL;\n-address StubRoutines::x86::_vector_32_bit_mask = NULL;\n-address StubRoutines::x86::_vector_64_bit_mask = NULL;\n+address StubRoutines::x86::_verify_mxcsr_entry = nullptr;\n+address StubRoutines::x86::_upper_word_mask_addr = nullptr;\n+address StubRoutines::x86::_shuffle_byte_flip_mask_addr = nullptr;\n+address StubRoutines::x86::_k256_adr = nullptr;\n+address StubRoutines::x86::_vector_short_to_byte_mask = nullptr;\n+address StubRoutines::x86::_vector_int_to_byte_mask = nullptr;\n+address StubRoutines::x86::_vector_int_to_short_mask = nullptr;\n+address StubRoutines::x86::_vector_all_bits_set = nullptr;\n+address StubRoutines::x86::_vector_byte_shuffle_mask = nullptr;\n+address StubRoutines::x86::_vector_int_mask_cmp_bits = nullptr;\n+address StubRoutines::x86::_vector_short_shuffle_mask = nullptr;\n+address StubRoutines::x86::_vector_int_shuffle_mask = nullptr;\n+address StubRoutines::x86::_vector_long_shuffle_mask = nullptr;\n+address StubRoutines::x86::_vector_float_sign_mask = nullptr;\n+address StubRoutines::x86::_vector_float_sign_flip = nullptr;\n+address StubRoutines::x86::_vector_double_sign_mask = nullptr;\n+address StubRoutines::x86::_vector_double_sign_flip = nullptr;\n+address StubRoutines::x86::_vector_byte_perm_mask = nullptr;\n+address StubRoutines::x86::_vector_long_sign_mask = nullptr;\n+address StubRoutines::x86::_vector_iota_indices = nullptr;\n+address StubRoutines::x86::_vector_reverse_bit_lut = nullptr;\n+address StubRoutines::x86::_vector_reverse_byte_perm_mask_long = nullptr;\n+address StubRoutines::x86::_vector_reverse_byte_perm_mask_int = nullptr;\n+address StubRoutines::x86::_vector_reverse_byte_perm_mask_short = nullptr;\n+address StubRoutines::x86::_vector_popcount_lut = nullptr;\n+address StubRoutines::x86::_vector_count_leading_zeros_lut = nullptr;\n+address StubRoutines::x86::_vector_32_bit_mask = nullptr;\n+address StubRoutines::x86::_vector_64_bit_mask = nullptr;\n@@ -65,3 +65,3 @@\n-address StubRoutines::x86::_k256_W_adr = NULL;\n-address StubRoutines::x86::_k512_W_addr = NULL;\n-address StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = NULL;\n+address StubRoutines::x86::_k256_W_adr = nullptr;\n+address StubRoutines::x86::_k512_W_addr = nullptr;\n+address StubRoutines::x86::_pshuffle_byte_flip_mask_addr_sha512 = nullptr;\n@@ -69,16 +69,16 @@\n-address StubRoutines::x86::_encoding_table_base64 = NULL;\n-address StubRoutines::x86::_shuffle_base64 = NULL;\n-address StubRoutines::x86::_avx2_shuffle_base64 = NULL;\n-address StubRoutines::x86::_avx2_input_mask_base64 = NULL;\n-address StubRoutines::x86::_avx2_lut_base64 = NULL;\n-address StubRoutines::x86::_avx2_decode_tables_base64 = NULL;\n-address StubRoutines::x86::_avx2_decode_lut_tables_base64 = NULL;\n-address StubRoutines::x86::_lookup_lo_base64 = NULL;\n-address StubRoutines::x86::_lookup_hi_base64 = NULL;\n-address StubRoutines::x86::_lookup_lo_base64url = NULL;\n-address StubRoutines::x86::_lookup_hi_base64url = NULL;\n-address StubRoutines::x86::_pack_vec_base64 = NULL;\n-address StubRoutines::x86::_join_0_1_base64 = NULL;\n-address StubRoutines::x86::_join_1_2_base64 = NULL;\n-address StubRoutines::x86::_join_2_3_base64 = NULL;\n-address StubRoutines::x86::_decoding_table_base64 = NULL;\n+address StubRoutines::x86::_encoding_table_base64 = nullptr;\n+address StubRoutines::x86::_shuffle_base64 = nullptr;\n+address StubRoutines::x86::_avx2_shuffle_base64 = nullptr;\n+address StubRoutines::x86::_avx2_input_mask_base64 = nullptr;\n+address StubRoutines::x86::_avx2_lut_base64 = nullptr;\n+address StubRoutines::x86::_avx2_decode_tables_base64 = nullptr;\n+address StubRoutines::x86::_avx2_decode_lut_tables_base64 = nullptr;\n+address StubRoutines::x86::_lookup_lo_base64 = nullptr;\n+address StubRoutines::x86::_lookup_hi_base64 = nullptr;\n+address StubRoutines::x86::_lookup_lo_base64url = nullptr;\n+address StubRoutines::x86::_lookup_hi_base64url = nullptr;\n+address StubRoutines::x86::_pack_vec_base64 = nullptr;\n+address StubRoutines::x86::_join_0_1_base64 = nullptr;\n+address StubRoutines::x86::_join_1_2_base64 = nullptr;\n+address StubRoutines::x86::_join_2_3_base64 = nullptr;\n+address StubRoutines::x86::_decoding_table_base64 = nullptr;\n@@ -86,2 +86,2 @@\n-address StubRoutines::x86::_pshuffle_byte_flip_mask_addr = NULL;\n-address StubRoutines::x86::_check_lock_stack = NULL;\n+address StubRoutines::x86::_pshuffle_byte_flip_mask_addr = nullptr;\n+address StubRoutines::x86::_check_lock_stack = nullptr;\n@@ -89,1 +89,1 @@\n-uint64_t StubRoutines::x86::_crc_by128_masks[] =\n+const uint64_t StubRoutines::x86::_crc_by128_masks[] =\n@@ -128,1 +128,1 @@\n-juint StubRoutines::x86::_crc_table[] =\n+const juint StubRoutines::x86::_crc_table[] =\n@@ -185,1 +185,1 @@\n-juint StubRoutines::x86::_crc_table_avx512[] =\n+const juint StubRoutines::x86::_crc_table_avx512[] =\n@@ -202,1 +202,1 @@\n-juint StubRoutines::x86::_crc32c_table_avx512[] =\n+const juint StubRoutines::x86::_crc32c_table_avx512[] =\n@@ -219,1 +219,1 @@\n-juint StubRoutines::x86::_crc_by128_masks_avx512[] =\n+const juint StubRoutines::x86::_crc_by128_masks_avx512[] =\n@@ -226,1 +226,1 @@\n-juint StubRoutines::x86::_shuf_table_crc32_avx512[] =\n+const juint StubRoutines::x86::_shuf_table_crc32_avx512[] =\n@@ -233,1 +233,1 @@\n-jint StubRoutines::x86::_arrays_hashcode_powers_of_31[] =\n+const jint StubRoutines::x86::_arrays_hashcode_powers_of_31[] =\n@@ -374,1 +374,1 @@\n-ATTRIBUTE_ALIGNED(64) juint StubRoutines::x86::_k256[] =\n+ATTRIBUTE_ALIGNED(64) const juint StubRoutines::x86::_k256[] =\n@@ -400,1 +400,1 @@\n-ATTRIBUTE_ALIGNED(64) julong StubRoutines::x86::_k512_W[] =\n+ATTRIBUTE_ALIGNED(64) const julong StubRoutines::x86::_k512_W[] =\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.cpp","additions":58,"deletions":58,"binary":false,"changes":116,"status":"modified"},{"patch":"@@ -35,2 +35,7 @@\n-  code_size1 = 20000 LP64_ONLY(+10000),                    \/\/ simply increase if too small (assembler will crash if too small)\n-  code_size2 = 35300 LP64_ONLY(+45000) WINDOWS_ONLY(+2048) \/\/ simply increase if too small (assembler will crash if too small)\n+  \/\/ simply increase sizes if too small (assembler will crash if too small)\n+  _initial_stubs_code_size      = 20000 WINDOWS_ONLY(+1000),\n+  _continuation_stubs_code_size =  1000 LP64_ONLY(+1000),\n+  \/\/ AVX512 intrinsics add more code in 64-bit VM,\n+  \/\/ Windows have more code to save\/restore registers\n+  _compiler_stubs_code_size     = 20000 LP64_ONLY(+30000) WINDOWS_ONLY(+2000),\n+  _final_stubs_code_size        = 10000 LP64_ONLY(+20000) WINDOWS_ONLY(+2000)\n@@ -132,2 +137,2 @@\n-  static uint64_t _crc_by128_masks[];\n-  static juint    _crc_table[];\n+  static const uint64_t _crc_by128_masks[];\n+  static const juint    _crc_table[];\n@@ -135,4 +140,4 @@\n-  static juint    _crc_by128_masks_avx512[];\n-  static juint    _crc_table_avx512[];\n-  static juint    _crc32c_table_avx512[];\n-  static juint    _shuf_table_crc32_avx512[];\n+  static const juint    _crc_by128_masks_avx512[];\n+  static const juint    _crc_table_avx512[];\n+  static const juint    _crc32c_table_avx512[];\n+  static const juint    _shuf_table_crc32_avx512[];\n@@ -143,1 +148,1 @@\n-  static jint _arrays_hashcode_powers_of_31[];\n+  static const jint _arrays_hashcode_powers_of_31[];\n@@ -151,1 +156,1 @@\n-  static juint _k256[];\n+  static const juint _k256[];\n@@ -180,1 +185,1 @@\n-  static julong _k512_W[];\n+  static const julong _k512_W[];\n","filename":"src\/hotspot\/cpu\/x86\/stubRoutines_x86.hpp","additions":16,"deletions":11,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -146,2 +146,2 @@\n-\/\/ Store an oop (or NULL) at the address described by obj.\n-\/\/ If val == noreg this means store a NULL\n+\/\/ Store an oop (or null) at the address described by obj.\n+\/\/ If val == noreg this means store a null\n@@ -455,1 +455,1 @@\n-    __ xorptr(result, result);  \/\/ NULL object reference\n+    __ xorptr(result, result);  \/\/ null object reference\n@@ -753,2 +753,0 @@\n-  \/\/ check array\n-  __ null_check(array, arrayOopDesc::length_offset_in_bytes());\n@@ -1158,1 +1156,1 @@\n-  \/\/ Have a NULL in rax, rdx=array, ecx=index.  Store NULL at ary[idx]\n+  \/\/ Have a null in rax, rdx=array, ecx=index.  Store null at ary[idx]\n@@ -1162,1 +1160,1 @@\n-  \/\/ Store a NULL\n+  \/\/ Store a null\n@@ -2211,1 +2209,1 @@\n-          UseOnStackReplacement ? &backedge_counter_overflow : NULL);\n+          UseOnStackReplacement ? &backedge_counter_overflow : nullptr);\n@@ -2219,1 +2217,1 @@\n-        UseOnStackReplacement ? &backedge_counter_overflow : NULL);\n+        UseOnStackReplacement ? &backedge_counter_overflow : nullptr);\n@@ -2245,1 +2243,1 @@\n-      \/\/ rax: osr nmethod (osr ok) or NULL (osr not possible)\n+      \/\/ rax: osr nmethod (osr ok) or null (osr not possible)\n@@ -2690,1 +2688,1 @@\n-    __ clinit_barrier(klass, thread, NULL \/*L_fast_path*\/, &L_clinit_barrier_slow);\n+    __ clinit_barrier(klass, thread, nullptr \/*L_fast_path*\/, &L_clinit_barrier_slow);\n@@ -2724,0 +2722,68 @@\n+void TemplateTable::load_invokedynamic_entry(Register method) {\n+  \/\/ setup registers\n+  const Register appendix = rax;\n+  const Register cache = rcx;\n+  const Register index = rdx;\n+  assert_different_registers(method, appendix, cache, index);\n+\n+  __ save_bcp();\n+\n+  Label resolved;\n+\n+  __ load_resolved_indy_entry(cache, index);\n+  __ movptr(method, Address(cache, in_bytes(ResolvedIndyEntry::method_offset())));\n+\n+  \/\/ Compare the method to zero\n+  __ testptr(method, method);\n+  __ jcc(Assembler::notZero, resolved);\n+\n+  Bytecodes::Code code = bytecode();\n+\n+  \/\/ Call to the interpreter runtime to resolve invokedynamic\n+  address entry = CAST_FROM_FN_PTR(address, InterpreterRuntime::resolve_from_cache);\n+  __ movl(method, code); \/\/ this is essentially Bytecodes::_invokedynamic\n+  __ call_VM(noreg, entry, method);\n+  \/\/ Update registers with resolved info\n+  __ load_resolved_indy_entry(cache, index);\n+  __ movptr(method, Address(cache, in_bytes(ResolvedIndyEntry::method_offset())));\n+\n+#ifdef ASSERT\n+  __ testptr(method, method);\n+  __ jcc(Assembler::notZero, resolved);\n+  __ stop(\"Should be resolved by now\");\n+#endif \/\/ ASSERT\n+  __ bind(resolved);\n+\n+  Label L_no_push;\n+  \/\/ Check if there is an appendix\n+  __ load_unsigned_byte(index, Address(cache, in_bytes(ResolvedIndyEntry::flags_offset())));\n+  __ testl(index, (1 << ResolvedIndyEntry::has_appendix_shift));\n+  __ jcc(Assembler::zero, L_no_push);\n+\n+  \/\/ Get appendix\n+  __ load_unsigned_short(index, Address(cache, in_bytes(ResolvedIndyEntry::resolved_references_index_offset())));\n+  \/\/ Push the appendix as a trailing parameter\n+  \/\/ since the parameter_size includes it.\n+  __ load_resolved_reference_at_index(appendix, index);\n+  __ verify_oop(appendix);\n+  __ push(appendix);  \/\/ push appendix (MethodType, CallSite, etc.)\n+  __ bind(L_no_push);\n+\n+  \/\/ compute return type\n+  __ load_unsigned_byte(index, Address(cache, in_bytes(ResolvedIndyEntry::result_type_offset())));\n+  \/\/ load return address\n+  {\n+    const address table_addr = (address) Interpreter::invoke_return_entry_table_for(code);\n+    ExternalAddress table(table_addr);\n+#ifdef _LP64\n+    __ lea(rscratch1, table);\n+    __ movptr(index, Address(rscratch1, index, Address::times_ptr));\n+#else\n+    __ movptr(index, ArrayAddress(table, Address(noreg, index, Address::times_ptr)));\n+#endif \/\/ _LP64\n+  }\n+\n+  \/\/ push return address\n+  __ push(index);\n+}\n+\n@@ -2730,1 +2796,1 @@\n-                                               bool is_invokedynamic) {\n+                                               bool is_invokedynamic \/*unused*\/) {\n@@ -2746,1 +2812,1 @@\n-  size_t index_size = (is_invokedynamic ? sizeof(u4) : sizeof(u2));\n+  size_t index_size = sizeof(u2);\n@@ -2777,1 +2843,1 @@\n-      __ xorptr(rax, rax);      \/\/ NULL object reference\n+      __ xorptr(rax, rax);      \/\/ null object reference\n@@ -2783,1 +2849,1 @@\n-    \/\/ rax,:   object pointer or NULL\n+    \/\/ rax,:   object pointer or null\n@@ -3034,1 +3100,1 @@\n-    \/\/ c_rarg1: object pointer set up above (NULL if static)\n+    \/\/ c_rarg1: object pointer set up above (null if static)\n@@ -3573,1 +3639,1 @@\n-  if (is_invokedynamic || is_invokehandle) {\n+  if (is_invokehandle) {\n@@ -3664,1 +3730,1 @@\n-  __ load_klass_check_null(rax, recv, rscratch1);\n+  __ load_klass(rax, recv, rscratch1);\n@@ -3755,1 +3821,1 @@\n-  __ load_klass_check_null(rlocals, rcx, rscratch1);\n+  __ load_klass(rlocals, rcx, rscratch1);\n@@ -3777,1 +3843,1 @@\n-  __ load_klass_check_null(rdx, rcx, rscratch1);\n+  __ load_klass(rdx, rcx, rscratch1);\n@@ -3896,2 +3962,1 @@\n-  prepare_invoke(byte_no, rbx_method, rax_callsite);\n-\n+  load_invokedynamic_entry(rbx_method);\n@@ -4066,1 +4131,0 @@\n-  __ null_check(rax, arrayOopDesc::length_offset_in_bytes());\n@@ -4122,1 +4186,1 @@\n-  \/\/ Collect counts on whether this check-cast sees NULLs a lot or not.\n+  \/\/ Collect counts on whether this check-cast sees nulls a lot or not.\n@@ -4185,1 +4249,1 @@\n-  \/\/ Collect counts on whether this test sees NULLs a lot or not.\n+  \/\/ Collect counts on whether this test sees nulls a lot or not.\n@@ -4194,2 +4258,2 @@\n-  \/\/ rax = 0: obj == NULL or  obj is not an instanceof the specified klass\n-  \/\/ rax = 1: obj != NULL and obj is     an instanceof the specified klass\n+  \/\/ rax = 0: obj == nullptr or  obj is not an instanceof the specified klass\n+  \/\/ rax = 1: obj != nullptr and obj is     an instanceof the specified klass\n@@ -4257,1 +4321,1 @@\n-  \/\/ check for NULL object\n+  \/\/ check for null object\n@@ -4273,1 +4337,1 @@\n-  __ xorl(rmon, rmon); \/\/ points to free slot or NULL\n+  __ xorl(rmon, rmon); \/\/ points to free slot or null\n@@ -4354,1 +4418,1 @@\n-  \/\/ check for NULL object\n+  \/\/ check for null object\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":95,"deletions":31,"binary":false,"changes":126,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2003, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2003, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -52,3 +52,3 @@\n-  \/\/ Can be NULL if there is no free space in the code cache.\n-  if (s == NULL) {\n-    return NULL;\n+  \/\/ Can be null if there is no free space in the code cache.\n+  if (s == nullptr) {\n+    return nullptr;\n@@ -123,1 +123,1 @@\n-    __ stop(\"Vtable entry is NULL\");\n+    __ stop(\"Vtable entry is null\");\n@@ -146,3 +146,3 @@\n-  \/\/ Can be NULL if there is no free space in the code cache.\n-  if (s == NULL) {\n-    return NULL;\n+  \/\/ Can be null if there is no free space in the code cache.\n+  if (s == nullptr) {\n+    return nullptr;\n","filename":"src\/hotspot\/cpu\/x86\/vtableStubs_x86_64.cpp","additions":8,"deletions":8,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -1247,2 +1247,8 @@\n-    MacroAssembler _masm(cbuf);\n-    __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+    if (cbuf != nullptr) {\n+      MacroAssembler _masm(cbuf);\n+      __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), Address(rsp, offset));\n+#ifndef PRODUCT\n+    } else {\n+      st->print(\"KMOV    %s, [ESP + %d]\", Matcher::regName[dst_first], offset);\n+#endif\n+    }\n@@ -1256,2 +1262,8 @@\n-    MacroAssembler _masm(cbuf);\n-    __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n+    if (cbuf != nullptr) {\n+      MacroAssembler _masm(cbuf);\n+      __ kmov(Address(rsp, offset), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+    } else {\n+      st->print(\"KMOV    [ESP + %d], %s\", offset, Matcher::regName[src_first]);\n+#endif\n+    }\n@@ -1275,2 +1287,8 @@\n-    MacroAssembler _masm(cbuf);\n-    __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+    if (cbuf != nullptr) {\n+      MacroAssembler _masm(cbuf);\n+      __ kmov(as_KRegister(Matcher::_regEncode[dst_first]), as_KRegister(Matcher::_regEncode[src_first]));\n+#ifndef PRODUCT\n+    } else {\n+      st->print(\"KMOV    %s, %s\", Matcher::regName[dst_first], Matcher::regName[src_first]);\n+#endif\n+    }\n@@ -12685,29 +12703,0 @@\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpLoopEndU(cmpOpU cop, eFlagsRegU cmp, label labl) %{\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,u  $labl\\t# Loop end\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe( pipe_jcc );\n-%}\n-\n-instruct jmpLoopEndUCF(cmpOpUCF cop, eFlagsRegUCF cmp, label labl) %{\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(200);\n-  format %{ \"J$cop,u  $labl\\t# Loop end\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe( pipe_jcc );\n-%}\n-\n@@ -12870,31 +12859,0 @@\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe( pipe_jcc );\n-  ins_short_branch(1);\n-%}\n-\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpLoopEndU_short(cmpOpU cop, eFlagsRegU cmp, label labl) %{\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,us $labl\\t# Loop end\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe( pipe_jcc );\n-  ins_short_branch(1);\n-%}\n-\n-instruct jmpLoopEndUCF_short(cmpOpUCF cop, eFlagsRegUCF cmp, label labl) %{\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"J$cop,us $labl\\t# Loop end\" %}\n-  size(2);\n-  ins_encode %{\n","filename":"src\/hotspot\/cpu\/x86\/x86_32.ad","additions":24,"deletions":66,"binary":false,"changes":90,"status":"modified"},{"patch":"@@ -8490,1 +8490,1 @@\n-instruct absI_rReg(rRegI dst, rRegI src, rRegI tmp, rFlagsReg cr)\n+instruct absI_rReg(rRegI dst, rRegI src, rFlagsReg cr)\n@@ -8493,7 +8493,4 @@\n-  effect(TEMP dst, TEMP tmp, KILL cr);\n-  format %{ \"movl $tmp, $src\\n\\t\"\n-            \"sarl $tmp, 31\\n\\t\"\n-            \"movl $dst, $src\\n\\t\"\n-            \"xorl $dst, $tmp\\n\\t\"\n-            \"subl $dst, $tmp\\n\"\n-          %}\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"xorl    $dst, $dst\\t# abs int\\n\\t\"\n+            \"subl    $dst, $src\\n\\t\"\n+            \"cmovll  $dst, $src\" %}\n@@ -8501,5 +8498,3 @@\n-    __ movl($tmp$$Register, $src$$Register);\n-    __ sarl($tmp$$Register, 31);\n-    __ movl($dst$$Register, $src$$Register);\n-    __ xorl($dst$$Register, $tmp$$Register);\n-    __ subl($dst$$Register, $tmp$$Register);\n+    __ xorl($dst$$Register, $dst$$Register);\n+    __ subl($dst$$Register, $src$$Register);\n+    __ cmovl(Assembler::less, $dst$$Register, $src$$Register);\n@@ -8512,1 +8507,1 @@\n-instruct absL_rReg(rRegL dst, rRegL src, rRegL tmp, rFlagsReg cr)\n+instruct absL_rReg(rRegL dst, rRegL src, rFlagsReg cr)\n@@ -8515,7 +8510,4 @@\n-  effect(TEMP dst, TEMP tmp, KILL cr);\n-  format %{ \"movq $tmp, $src\\n\\t\"\n-            \"sarq $tmp, 63\\n\\t\"\n-            \"movq $dst, $src\\n\\t\"\n-            \"xorq $dst, $tmp\\n\\t\"\n-            \"subq $dst, $tmp\\n\"\n-          %}\n+  effect(TEMP dst, KILL cr);\n+  format %{ \"xorl    $dst, $dst\\t# abs long\\n\\t\"\n+            \"subq    $dst, $src\\n\\t\"\n+            \"cmovlq  $dst, $src\" %}\n@@ -8523,5 +8515,3 @@\n-    __ movq($tmp$$Register, $src$$Register);\n-    __ sarq($tmp$$Register, 63);\n-    __ movq($dst$$Register, $src$$Register);\n-    __ xorq($dst$$Register, $tmp$$Register);\n-    __ subq($dst$$Register, $tmp$$Register);\n+    __ xorl($dst$$Register, $dst$$Register);\n+    __ subq($dst$$Register, $src$$Register);\n+    __ cmovq(Assembler::less, $dst$$Register, $src$$Register);\n@@ -13082,29 +13072,0 @@\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpLoopEndU(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,u   $labl\\t# loop end\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n-instruct jmpLoopEndUCF(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(200);\n-  format %{ \"j$cop,u   $labl\\t# loop end\" %}\n-  size(6);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jcc((Assembler::Condition)($cop$$cmpcode), *L, false); \/\/ Always long jump\n-  %}\n-  ins_pipe(pipe_jcc);\n-%}\n-\n@@ -13283,31 +13244,0 @@\n-\/\/ Jump Direct Conditional - Label defines a relative address from Jcc+1\n-instruct jmpLoopEndU_short(cmpOpU cop, rFlagsRegU cmp, label labl) %{\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,us  $labl\\t# loop end\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe(pipe_jcc);\n-  ins_short_branch(1);\n-%}\n-\n-instruct jmpLoopEndUCF_short(cmpOpUCF cop, rFlagsRegUCF cmp, label labl) %{\n-  match(CountedLoopEnd cop cmp);\n-  effect(USE labl);\n-\n-  ins_cost(300);\n-  format %{ \"j$cop,us  $labl\\t# loop end\" %}\n-  size(2);\n-  ins_encode %{\n-    Label* L = $labl$$label;\n-    __ jccb((Assembler::Condition)($cop$$cmpcode), *L);\n-  %}\n-  ins_pipe(pipe_jcc);\n-  ins_short_branch(1);\n-%}\n-\n@@ -13831,0 +13761,7 @@\n+\/\/ These peephole rules replace mov + I pairs (where I is one of {add, inc, dec,\n+\/\/ sal}) with lea instructions. The {add, sal} rules are beneficial in\n+\/\/ processors with at least partial ALU support for lea\n+\/\/ (supports_fast_2op_lea()), whereas the {inc, dec} rules are only generally\n+\/\/ beneficial for processors with full ALU support\n+\/\/ (VM_Version::supports_fast_3op_lea()) and Intel Cascade Lake.\n+\n@@ -13849,1 +13786,2 @@\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n@@ -13857,1 +13795,2 @@\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n@@ -13889,1 +13828,2 @@\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n@@ -13897,1 +13837,2 @@\n-  peeppredicate(VM_Version::supports_fast_2op_lea());\n+  peeppredicate(VM_Version::supports_fast_3op_lea() ||\n+                VM_Version::is_intel_cascade_lake());\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":31,"deletions":90,"binary":false,"changes":121,"status":"modified"},{"patch":"@@ -289,0 +289,5 @@\n+  if (!code()->finalize_stubs()) {\n+    bailout(\"CodeCache is full\");\n+    return;\n+  }\n+\n@@ -316,3 +321,0 @@\n-\n-  \/\/ done\n-  masm()->flush();\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.cpp","additions":5,"deletions":3,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -575,1 +575,1 @@\n-    int offset = field->offset();\n+    int offset = field->offset_in_bytes();\n@@ -585,1 +585,1 @@\n-    int offset = field->offset();\n+    int offset = field->offset_in_bytes();\n@@ -626,1 +626,1 @@\n-      int offset = field->offset();\n+      int offset = field->offset_in_bytes();\n@@ -694,1 +694,1 @@\n-      int offset = field->offset();\n+      int offset = field->offset_in_bytes();\n@@ -1755,1 +1755,1 @@\n-  const int offset = !needs_patching ? field->offset() : -1;\n+  const int offset = !needs_patching ? field->offset_in_bytes() : -1;\n","filename":"src\/hotspot\/share\/c1\/c1_GraphBuilder.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -566,0 +566,2 @@\n+    case lir_f2hf:\n+    case lir_hf2f:\n@@ -1738,0 +1740,2 @@\n+     case lir_f2hf:                  s = \"f2hf\";          break;\n+     case lir_hf2f:                  s = \"hf2f\";          break;\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -961,0 +961,2 @@\n+      , lir_f2hf\n+      , lir_hf2f\n@@ -2277,0 +2279,2 @@\n+  void f2hf(LIR_Opr from, LIR_Opr to, LIR_Opr tmp)                { append(new LIR_Op2(lir_f2hf, from, tmp, to)); }\n+  void hf2f(LIR_Opr from, LIR_Opr to, LIR_Opr tmp)                { append(new LIR_Op2(lir_hf2f, from, tmp, to)); }\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":5,"deletions":1,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2000, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2000, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -729,0 +729,2 @@\n+    case lir_f2hf:\n+    case lir_hf2f:\n","filename":"src\/hotspot\/share\/c1\/c1_LIRAssembler.cpp","additions":3,"deletions":1,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2969,0 +2969,4 @@\n+  \/\/ Use java.lang.Math intrinsics code since it works for these intrinsics too.\n+  case vmIntrinsics::_floatToFloat16: \/\/ fall through\n+  case vmIntrinsics::_float16ToFloat: do_MathIntrinsic(x); break;\n+\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -1065,3 +1065,2 @@\n-        ConstantPoolCacheEntry* cpce = pool->invokedynamic_cp_cache_entry_at(index);\n-        cpce->set_dynamic_call(pool, info);\n-        appendix = Handle(current, cpce->appendix_if_resolved(pool)); \/\/ just in case somebody already resolved the entry\n+        int indy_index = pool->decode_invokedynamic_index(index);\n+        appendix = Handle(current, pool->cache()->set_dynamic_call(info, indy_index));\n","filename":"src\/hotspot\/share\/c1\/c1_Runtime1.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -166,2 +166,1 @@\n-  _total_closed_heap_region_size(0),\n-  _total_open_heap_region_size(0),\n+  _total_heap_region_size(0),\n@@ -338,1 +337,1 @@\n-    os::_exit(0);\n+    MetaspaceShared::unrecoverable_writing_error();\n@@ -388,1 +387,1 @@\n-    os::_exit(0);\n+    MetaspaceShared::unrecoverable_writing_error();\n@@ -1071,30 +1070,23 @@\n-  \/\/ open and closed archive regions\n-  static void log_heap_regions(const char* which, GrowableArray<MemRegion> *regions) {\n-    for (int i = 0; i < regions->length(); i++) {\n-      address start = address(regions->at(i).start());\n-      address end = address(regions->at(i).end());\n-      log_region(which, start, end, to_requested(start));\n-\n-      while (start < end) {\n-        size_t byte_size;\n-        oop original_oop = ArchiveHeapWriter::buffered_addr_to_source_obj(start);\n-        if (original_oop != nullptr) {\n-          ResourceMark rm;\n-          log_info(cds, map)(PTR_FORMAT \": @@ Object %s\",\n-                             p2i(to_requested(start)), original_oop->klass()->external_name());\n-          byte_size = original_oop->size() * BytesPerWord;\n-        } else if (start == ArchiveHeapWriter::buffered_heap_roots_addr()) {\n-          \/\/ HeapShared::roots() is copied specially so it doesn't exist in\n-          \/\/ HeapShared::OriginalObjectTable. See HeapShared::copy_roots().\n-          log_info(cds, map)(PTR_FORMAT \": @@ Object HeapShared::roots (ObjArray)\",\n-                             p2i(to_requested(start)));\n-          byte_size = ArchiveHeapWriter::heap_roots_word_size() * BytesPerWord;\n-        } else {\n-          \/\/ We have reached the end of the region\n-          break;\n-        }\n-        address oop_end = start + byte_size;\n-        log_data(start, oop_end, to_requested(start), \/*is_heap=*\/true);\n-        start = oop_end;\n-      }\n-      if (start < end) {\n+  static void log_heap_region(ArchiveHeapInfo* heap_info) {\n+    MemRegion r = heap_info->memregion();\n+    address start = address(r.start());\n+    address end = address(r.end());\n+    log_region(\"heap\", start, end, to_requested(start));\n+\n+    while (start < end) {\n+      size_t byte_size;\n+      oop original_oop = ArchiveHeapWriter::buffered_addr_to_source_obj(start);\n+      if (original_oop != nullptr) {\n+        ResourceMark rm;\n+        log_info(cds, map)(PTR_FORMAT \": @@ Object %s\",\n+                           p2i(to_requested(start)), original_oop->klass()->external_name());\n+        byte_size = original_oop->size() * BytesPerWord;\n+      } else if (start == ArchiveHeapWriter::buffered_heap_roots_addr()) {\n+        \/\/ HeapShared::roots() is copied specially so it doesn't exist in\n+        \/\/ HeapShared::OriginalObjectTable. See HeapShared::copy_roots().\n+        log_info(cds, map)(PTR_FORMAT \": @@ Object HeapShared::roots (ObjArray)\",\n+                           p2i(to_requested(start)));\n+        byte_size = ArchiveHeapWriter::heap_roots_word_size() * BytesPerWord;\n+      } else {\n+        \/\/ We have reached the end of the region, but have some unused space\n+        \/\/ at the end.\n@@ -1104,0 +1096,1 @@\n+        break;\n@@ -1105,0 +1098,3 @@\n+      address oop_end = start + byte_size;\n+      log_data(start, oop_end, to_requested(start), \/*is_heap=*\/true);\n+      start = oop_end;\n@@ -1107,0 +1103,1 @@\n+\n@@ -1138,2 +1135,1 @@\n-                  GrowableArray<MemRegion> *closed_heap_regions,\n-                  GrowableArray<MemRegion> *open_heap_regions,\n+                  ArchiveHeapInfo* heap_info,\n@@ -1160,5 +1156,2 @@\n-    if (closed_heap_regions != nullptr) {\n-      log_heap_regions(\"closed heap region\", closed_heap_regions);\n-    }\n-    if (open_heap_regions != nullptr) {\n-      log_heap_regions(\"open heap region\", open_heap_regions);\n+    if (heap_info->is_used()) {\n+      log_heap_region(heap_info);\n@@ -1181,5 +1174,1 @@\n-void ArchiveBuilder::write_archive(FileMapInfo* mapinfo,\n-                                   GrowableArray<MemRegion>* closed_heap_regions,\n-                                   GrowableArray<MemRegion>* open_heap_regions,\n-                                   GrowableArray<ArchiveHeapBitmapInfo>* closed_heap_bitmaps,\n-                                   GrowableArray<ArchiveHeapBitmapInfo>* open_heap_bitmaps) {\n+void ArchiveBuilder::write_archive(FileMapInfo* mapinfo, ArchiveHeapInfo* heap_info) {\n@@ -1194,1 +1183,1 @@\n-  char* bitmap = mapinfo->write_bitmap_region(ArchivePtrMarker::ptrmap(), closed_heap_bitmaps, open_heap_bitmaps,\n+  char* bitmap = mapinfo->write_bitmap_region(ArchivePtrMarker::ptrmap(), heap_info,\n@@ -1197,11 +1186,2 @@\n-  if (closed_heap_regions != nullptr) {\n-    _total_closed_heap_region_size = mapinfo->write_heap_regions(\n-                                        closed_heap_regions,\n-                                        closed_heap_bitmaps,\n-                                        MetaspaceShared::first_closed_heap_region,\n-                                        MetaspaceShared::max_num_closed_heap_regions);\n-    _total_open_heap_region_size = mapinfo->write_heap_regions(\n-                                        open_heap_regions,\n-                                        open_heap_bitmaps,\n-                                        MetaspaceShared::first_open_heap_region,\n-                                        MetaspaceShared::max_num_open_heap_regions);\n+  if (heap_info->is_used()) {\n+    _total_heap_region_size = mapinfo->write_heap_region(heap_info);\n@@ -1210,1 +1190,1 @@\n-  print_region_stats(mapinfo, closed_heap_regions, open_heap_regions);\n+  print_region_stats(mapinfo, heap_info);\n@@ -1224,1 +1204,1 @@\n-    CDSMapLogger::log(this, mapinfo, closed_heap_regions, open_heap_regions,\n+    CDSMapLogger::log(this, mapinfo, heap_info,\n@@ -1235,3 +1215,1 @@\n-void ArchiveBuilder::print_region_stats(FileMapInfo *mapinfo,\n-                                        GrowableArray<MemRegion>* closed_heap_regions,\n-                                        GrowableArray<MemRegion>* open_heap_regions) {\n+void ArchiveBuilder::print_region_stats(FileMapInfo *mapinfo, ArchiveHeapInfo* heap_info) {\n@@ -1243,2 +1221,1 @@\n-                                _total_closed_heap_region_size +\n-                                _total_open_heap_region_size;\n+                                _total_heap_region_size;\n@@ -1247,2 +1224,1 @@\n-                             _total_closed_heap_region_size +\n-                             _total_open_heap_region_size;\n+                             _total_heap_region_size;\n@@ -1256,3 +1232,2 @@\n-  if (closed_heap_regions != nullptr) {\n-    print_heap_region_stats(closed_heap_regions, \"ca\", total_reserved);\n-    print_heap_region_stats(open_heap_regions, \"oa\", total_reserved);\n+  if (heap_info->is_used()) {\n+    print_heap_region_stats(heap_info, total_reserved);\n@@ -1261,1 +1236,1 @@\n-  log_debug(cds)(\"total    : \" SIZE_FORMAT_W(9) \" [100.0%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [%5.1f%% used]\",\n+  log_debug(cds)(\"total   : \" SIZE_FORMAT_W(9) \" [100.0%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [%5.1f%% used]\",\n@@ -1266,1 +1241,1 @@\n-  log_debug(cds)(\"bm  space: \" SIZE_FORMAT_W(9) \" [ %4.1f%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [100.0%% used]\",\n+  log_debug(cds)(\"bm space: \" SIZE_FORMAT_W(9) \" [ %4.1f%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [100.0%% used]\",\n@@ -1270,10 +1245,6 @@\n-void ArchiveBuilder::print_heap_region_stats(GrowableArray<MemRegion>* regions,\n-                                             const char *name, size_t total_size) {\n-  int arr_len = regions == nullptr ? 0 : regions->length();\n-  for (int i = 0; i < arr_len; i++) {\n-      char* start = (char*)regions->at(i).start();\n-      size_t size = regions->at(i).byte_size();\n-      char* top = start + size;\n-      log_debug(cds)(\"%s%d space: \" SIZE_FORMAT_W(9) \" [ %4.1f%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [100.0%% used] at \" INTPTR_FORMAT,\n-                     name, i, size, size\/double(total_size)*100.0, size, p2i(start));\n-  }\n+void ArchiveBuilder::print_heap_region_stats(ArchiveHeapInfo *info, size_t total_size) {\n+  char* start = info->start();\n+  size_t size = info->byte_size();\n+  char* top = start + size;\n+  log_debug(cds)(\"hp space: \" SIZE_FORMAT_W(9) \" [ %4.1f%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [100.0%% used] at \" INTPTR_FORMAT,\n+                     size, size\/double(total_size)*100.0, size, p2i(start));\n@@ -1289,2 +1260,2 @@\n-  vm_exit_during_initialization(err_msg(\"Unable to allocate from '%s' region\", name),\n-                                \"Please reduce the number of shared classes.\");\n+  log_error(cds)(\"Unable to allocate from '%s' region: Please reduce the number of shared classes.\", name);\n+  MetaspaceShared::unrecoverable_writing_error();\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":55,"deletions":84,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n-struct ArchiveHeapBitmapInfo;\n+class ArchiveHeapInfo;\n@@ -248,2 +248,1 @@\n-  size_t _total_closed_heap_region_size;\n-  size_t _total_open_heap_region_size;\n+  size_t _total_heap_region_size;\n@@ -251,3 +250,1 @@\n-  void print_region_stats(FileMapInfo *map_info,\n-                          GrowableArray<MemRegion>* closed_heap_regions,\n-                          GrowableArray<MemRegion>* open_heap_regions);\n+  void print_region_stats(FileMapInfo *map_info, ArchiveHeapInfo* heap_info);\n@@ -255,2 +252,1 @@\n-  void print_heap_region_stats(GrowableArray<MemRegion>* regions,\n-                               const char *name, size_t total_size);\n+  void print_heap_region_stats(ArchiveHeapInfo* heap_info, size_t total_size);\n@@ -417,5 +413,1 @@\n-  void write_archive(FileMapInfo* mapinfo,\n-                     GrowableArray<MemRegion>* closed_heap_regions,\n-                     GrowableArray<MemRegion>* open_heap_regions,\n-                     GrowableArray<ArchiveHeapBitmapInfo>* closed_heap_oopmaps,\n-                     GrowableArray<ArchiveHeapBitmapInfo>* open_heap_oopmaps);\n+  void write_archive(FileMapInfo* mapinfo, ArchiveHeapInfo* heap_info);\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":5,"deletions":13,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -50,1 +50,0 @@\n-\n@@ -54,6 +53,2 @@\n-size_t ArchiveHeapWriter::_buffer_top;\n-size_t ArchiveHeapWriter::_open_bottom;\n-size_t ArchiveHeapWriter::_open_top;\n-size_t ArchiveHeapWriter::_closed_bottom;\n-size_t ArchiveHeapWriter::_closed_top;\n-size_t ArchiveHeapWriter::_heap_roots_bottom;\n+size_t ArchiveHeapWriter::_buffer_used;\n+size_t ArchiveHeapWriter::_heap_roots_bottom_offset;\n@@ -63,7 +58,2 @@\n-address ArchiveHeapWriter::_requested_open_region_bottom;\n-address ArchiveHeapWriter::_requested_open_region_top;\n-address ArchiveHeapWriter::_requested_closed_region_bottom;\n-address ArchiveHeapWriter::_requested_closed_region_top;\n-\n-ResourceBitMap* ArchiveHeapWriter::_closed_oopmap;\n-ResourceBitMap* ArchiveHeapWriter::_open_oopmap;\n+address ArchiveHeapWriter::_requested_bottom;\n+address ArchiveHeapWriter::_requested_top;\n@@ -83,4 +73,2 @@\n-    _requested_open_region_bottom = nullptr;\n-    _requested_open_region_top = nullptr;\n-    _requested_closed_region_bottom = nullptr;\n-    _requested_closed_region_top = nullptr;\n+    _requested_bottom = nullptr;\n+    _requested_top = nullptr;\n@@ -100,5 +88,1 @@\n-\/\/ For the time being, always support two regions (to be strictly compatible with existing G1\n-\/\/ mapping code. We might eventually use a single region (JDK-8298048).\n-                              GrowableArray<MemRegion>* closed_regions, GrowableArray<MemRegion>* open_regions,\n-                              GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n-                              GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n+                              ArchiveHeapInfo* heap_info) {\n@@ -109,2 +93,2 @@\n-  set_requested_address_for_regions(closed_regions, open_regions);\n-  relocate_embedded_oops(roots, closed_bitmaps, open_bitmaps);\n+  set_requested_address(heap_info);\n+  relocate_embedded_oops(roots, heap_info);\n@@ -136,4 +120,2 @@\n-bool ArchiveHeapWriter::is_in_requested_regions(oop o) {\n-  assert(_requested_open_region_bottom != nullptr, \"do not call before this is initialized\");\n-  assert(_requested_closed_region_bottom != nullptr, \"do not call before this is initialized\");\n-\n+bool ArchiveHeapWriter::is_in_requested_range(oop o) {\n+  assert(_requested_bottom != nullptr, \"do not call before _requested_bottom is initialized\");\n@@ -141,2 +123,1 @@\n-  return (_requested_open_region_bottom <= a && a < _requested_open_region_top) ||\n-         (_requested_closed_region_bottom <= a && a < _requested_closed_region_top);\n+  return (_requested_bottom <= a && a < _requested_top);\n@@ -146,2 +127,2 @@\n-  oop req_obj = cast_to_oop(_requested_open_region_bottom + offset);\n-  assert(is_in_requested_regions(req_obj), \"must be\");\n+  oop req_obj = cast_to_oop(_requested_bottom + offset);\n+  assert(is_in_requested_range(req_obj), \"must be\");\n@@ -171,1 +152,1 @@\n-  return _requested_open_region_bottom + buffered_address_to_offset(buffered_addr);\n+  return _requested_bottom + buffered_address_to_offset(buffered_addr);\n@@ -175,1 +156,1 @@\n-  return requested_obj_from_buffer_offset(_heap_roots_bottom);\n+  return cast_to_oop(_requested_bottom + _heap_roots_bottom_offset);\n@@ -178,1 +159,1 @@\n-address ArchiveHeapWriter::heap_region_requested_bottom(int heap_region_idx) {\n+address ArchiveHeapWriter::requested_address() {\n@@ -180,9 +161,1 @@\n-  switch (heap_region_idx) {\n-  case MetaspaceShared::first_closed_heap_region:\n-    return _requested_closed_region_bottom;\n-  case MetaspaceShared::first_open_heap_region:\n-    return _requested_open_region_bottom;\n-  default:\n-    ShouldNotReachHere();\n-    return nullptr;\n-  }\n+  return _requested_bottom;\n@@ -194,1 +167,1 @@\n-  _open_bottom = _buffer_top = 0;\n+  _buffer_used = 0;\n@@ -206,1 +179,1 @@\n-  int length = roots != nullptr ? roots->length() : 0;\n+  int length = roots->length();\n@@ -216,2 +189,2 @@\n-  size_t new_top = _buffer_top + byte_size;\n-  ensure_buffer_space(new_top);\n+  size_t new_used = _buffer_used + byte_size;\n+  ensure_buffer_space(new_used);\n@@ -219,1 +192,1 @@\n-  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_top);\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_used);\n@@ -246,1 +219,1 @@\n-  log_info(cds)(\"archived obj roots[%d] = \" SIZE_FORMAT \" bytes, klass = %p, obj = %p\", length, byte_size, k, mem);\n+  log_info(cds, heap)(\"archived obj roots[%d] = \" SIZE_FORMAT \" bytes, klass = %p, obj = %p\", length, byte_size, k, mem);\n@@ -248,2 +221,2 @@\n-  _heap_roots_bottom = _buffer_top;\n-  _buffer_top = new_top;\n+  _heap_roots_bottom_offset = _buffer_used;\n+  _buffer_used = new_used;\n@@ -253,14 +226,0 @@\n-  copy_source_objs_to_buffer_by_region(\/*copy_open_region=*\/true);\n-  copy_roots_to_buffer(roots);\n-  _open_top = _buffer_top;\n-\n-  \/\/ Align the closed region to the next G1 region\n-  _buffer_top = _closed_bottom = align_up(_buffer_top, HeapRegion::GrainBytes);\n-  copy_source_objs_to_buffer_by_region(\/*copy_open_region=*\/false);\n-  _closed_top = _buffer_top;\n-\n-  log_info(cds, heap)(\"Size of open region   = \" SIZE_FORMAT \" bytes\", _open_top   - _open_bottom);\n-  log_info(cds, heap)(\"Size of closed region = \" SIZE_FORMAT \" bytes\", _closed_top - _closed_bottom);\n-}\n-\n-void ArchiveHeapWriter::copy_source_objs_to_buffer_by_region(bool copy_open_region) {\n@@ -271,5 +230,2 @@\n-    if (info->in_open_region() == copy_open_region) {\n-      \/\/ For region-based collectors such as G1, we need to make sure that we don't have\n-      \/\/ an object that can possible span across two regions.\n-      size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n-      info->set_buffer_offset(buffer_offset);\n+    size_t buffer_offset = copy_one_source_obj_to_buffer(src_obj);\n+    info->set_buffer_offset(buffer_offset);\n@@ -277,2 +233,1 @@\n-      _buffer_offset_to_source_obj_table->put(buffer_offset, src_obj);\n-    }\n+    _buffer_offset_to_source_obj_table->put(buffer_offset, src_obj);\n@@ -280,0 +235,5 @@\n+\n+  copy_roots_to_buffer(roots);\n+\n+  log_info(cds)(\"Size of heap region = \" SIZE_FORMAT \" bytes, %d objects, %d roots\",\n+                _buffer_used, _source_objs->length() + 1, roots->length());\n@@ -306,1 +266,1 @@\n-  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_top);\n+  HeapWord* mem = offset_to_buffered_address<HeapWord*>(_buffer_used);\n@@ -325,1 +285,1 @@\n-  size_t new_top = _buffer_top + required_byte_size + min_filler_byte_size;\n+  size_t new_used = _buffer_used + required_byte_size + min_filler_byte_size;\n@@ -327,2 +287,2 @@\n-  const size_t cur_min_region_bottom = align_down(_buffer_top, MIN_GC_REGION_ALIGNMENT);\n-  const size_t next_min_region_bottom = align_down(new_top, MIN_GC_REGION_ALIGNMENT);\n+  const size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n+  const size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n@@ -338,1 +298,1 @@\n-    const size_t fill_bytes = filler_end - _buffer_top;\n+    const size_t fill_bytes = filler_end - _buffer_used;\n@@ -344,1 +304,1 @@\n-                        array_length, fill_bytes, _buffer_top);\n+                        array_length, fill_bytes, _buffer_used);\n@@ -347,1 +307,1 @@\n-    _buffer_top = filler_end;\n+    _buffer_used = filler_end;\n@@ -356,0 +316,3 @@\n+  \/\/ For region-based collectors such as G1, the archive heap may be mapped into\n+  \/\/ multiple regions. We need to make sure that we don't have an object that can possible\n+  \/\/ span across two regions.\n@@ -358,2 +321,2 @@\n-  size_t new_top = _buffer_top + byte_size;\n-  assert(new_top > _buffer_top, \"no wrap around\");\n+  size_t new_used = _buffer_used + byte_size;\n+  assert(new_used > _buffer_used, \"no wrap around\");\n@@ -361,2 +324,2 @@\n-  size_t cur_min_region_bottom = align_down(_buffer_top, MIN_GC_REGION_ALIGNMENT);\n-  size_t next_min_region_bottom = align_down(new_top, MIN_GC_REGION_ALIGNMENT);\n+  size_t cur_min_region_bottom = align_down(_buffer_used, MIN_GC_REGION_ALIGNMENT);\n+  size_t next_min_region_bottom = align_down(new_used, MIN_GC_REGION_ALIGNMENT);\n@@ -365,1 +328,1 @@\n-  ensure_buffer_space(new_top);\n+  ensure_buffer_space(new_used);\n@@ -368,2 +331,2 @@\n-  address to = offset_to_buffered_address<address>(_buffer_top);\n-  assert(is_object_aligned(_buffer_top), \"sanity\");\n+  address to = offset_to_buffered_address<address>(_buffer_used);\n+  assert(is_object_aligned(_buffer_used), \"sanity\");\n@@ -373,2 +336,2 @@\n-  size_t buffered_obj_offset = _buffer_top;\n-  _buffer_top = new_top;\n+  size_t buffered_obj_offset = _buffer_used;\n+  _buffer_used = new_used;\n@@ -379,5 +342,2 @@\n-void ArchiveHeapWriter::set_requested_address_for_regions(GrowableArray<MemRegion>* closed_regions,\n-                                                          GrowableArray<MemRegion>* open_regions) {\n-  assert(closed_regions->length() == 0, \"must be\");\n-  assert(open_regions->length() == 0, \"must be\");\n-\n+void ArchiveHeapWriter::set_requested_address(ArchiveHeapInfo* info) {\n+  assert(!info->is_used(), \"only set once\");\n@@ -388,14 +348,2 @@\n-  size_t closed_region_byte_size = _closed_top - _closed_bottom;\n-  size_t open_region_byte_size = _open_top - _open_bottom;\n-  assert(closed_region_byte_size > 0, \"must archived at least one object for closed region!\");\n-  assert(open_region_byte_size > 0, \"must archived at least one object for open region!\");\n-\n-  \/\/ The following two asserts are ensured by copy_source_objs_to_buffer_by_region().\n-  assert(is_aligned(_closed_bottom, HeapRegion::GrainBytes), \"sanity\");\n-  assert(is_aligned(_open_bottom, HeapRegion::GrainBytes), \"sanity\");\n-\n-  _requested_closed_region_bottom = align_down(heap_end - closed_region_byte_size, HeapRegion::GrainBytes);\n-  _requested_open_region_bottom = _requested_closed_region_bottom - (_closed_bottom - _open_bottom);\n-\n-  assert(is_aligned(_requested_closed_region_bottom, HeapRegion::GrainBytes), \"sanity\");\n-  assert(is_aligned(_requested_open_region_bottom, HeapRegion::GrainBytes), \"sanity\");\n+  size_t heap_region_byte_size = _buffer_used;\n+  assert(heap_region_byte_size > 0, \"must archived at least one object!\");\n@@ -403,2 +351,2 @@\n-  _requested_open_region_top = _requested_open_region_bottom + (_open_top - _open_bottom);\n-  _requested_closed_region_top = _requested_closed_region_bottom + (_closed_top - _closed_bottom);\n+  _requested_bottom = align_down(heap_end - heap_region_byte_size, HeapRegion::GrainBytes);\n+  assert(is_aligned(_requested_bottom, HeapRegion::GrainBytes), \"sanity\");\n@@ -406,1 +354,1 @@\n-  assert(_requested_open_region_top <= _requested_closed_region_bottom, \"no overlap\");\n+  _requested_top = _requested_bottom + _buffer_used;\n@@ -408,4 +356,2 @@\n-  closed_regions->append(MemRegion(offset_to_buffered_address<HeapWord*>(_closed_bottom),\n-                                   offset_to_buffered_address<HeapWord*>(_closed_top)));\n-  open_regions->append(  MemRegion(offset_to_buffered_address<HeapWord*>(_open_bottom),\n-                                   offset_to_buffered_address<HeapWord*>(_open_top)));\n+  info->set_memregion(MemRegion(offset_to_buffered_address<HeapWord*>(0),\n+                                offset_to_buffered_address<HeapWord*>(_buffer_used)));\n@@ -417,1 +363,1 @@\n-  assert(is_in_requested_regions(cast_to_oop(p)), \"must be\");\n+  assert(is_in_requested_range(cast_to_oop(p)), \"must be\");\n@@ -420,2 +366,2 @@\n-  assert(addr >= _requested_open_region_bottom, \"must be\");\n-  size_t offset = addr - _requested_open_region_bottom;\n+  assert(addr >= _requested_bottom, \"must be\");\n+  size_t offset = addr - _requested_bottom;\n@@ -433,1 +379,1 @@\n-  assert(is_in_requested_regions(request_oop), \"must be\");\n+  assert(is_in_requested_range(request_oop), \"must be\");\n@@ -457,1 +403,1 @@\n-template <typename T> void ArchiveHeapWriter::relocate_field_in_buffer(T* field_addr_in_buffer) {\n+template <typename T> void ArchiveHeapWriter::relocate_field_in_buffer(T* field_addr_in_buffer, CHeapBitMap* oopmap) {\n@@ -462,1 +408,1 @@\n-    mark_oop_pointer<T>(field_addr_in_buffer);\n+    mark_oop_pointer<T>(field_addr_in_buffer, oopmap);\n@@ -466,1 +412,1 @@\n-template <typename T> void ArchiveHeapWriter::mark_oop_pointer(T* buffered_addr) {\n+template <typename T> void ArchiveHeapWriter::mark_oop_pointer(T* buffered_addr, CHeapBitMap* oopmap) {\n@@ -468,1 +414,0 @@\n-  ResourceBitMap* oopmap;\n@@ -471,10 +416,3 @@\n-  if (request_p >= (T*)_requested_closed_region_bottom) {\n-    assert(request_p < (T*)_requested_closed_region_top, \"sanity\");\n-    oopmap = _closed_oopmap;\n-    requested_region_bottom = _requested_closed_region_bottom;\n-  } else {\n-    assert(request_p >= (T*)_requested_open_region_bottom, \"sanity\");\n-    assert(request_p <  (T*)_requested_open_region_top, \"sanity\");\n-    oopmap = _open_oopmap;\n-    requested_region_bottom = _requested_open_region_bottom;\n-  }\n+  assert(request_p >= (T*)_requested_bottom, \"sanity\");\n+  assert(request_p <  (T*)_requested_top, \"sanity\");\n+  requested_region_bottom = _requested_bottom;\n@@ -517,1 +455,1 @@\n-template <typename T> void ArchiveHeapWriter::relocate_root_at(oop requested_roots, int index) {\n+template <typename T> void ArchiveHeapWriter::relocate_root_at(oop requested_roots, int index, CHeapBitMap* oopmap) {\n@@ -519,1 +457,1 @@\n-  relocate_field_in_buffer<T>((T*)(buffered_heap_roots_addr() + offset));\n+  relocate_field_in_buffer<T>((T*)(buffered_heap_roots_addr() + offset), oopmap);\n@@ -525,0 +463,1 @@\n+  CHeapBitMap* _oopmap;\n@@ -527,2 +466,2 @@\n-  EmbeddedOopRelocator(oop src_obj, address buffered_obj) :\n-    _src_obj(src_obj), _buffered_obj(buffered_obj) {}\n+  EmbeddedOopRelocator(oop src_obj, address buffered_obj, CHeapBitMap* oopmap) :\n+    _src_obj(src_obj), _buffered_obj(buffered_obj), _oopmap(oopmap) {}\n@@ -536,1 +475,1 @@\n-    ArchiveHeapWriter::relocate_field_in_buffer<T>((T*)(_buffered_obj + field_offset));\n+    ArchiveHeapWriter::relocate_field_in_buffer<T>((T*)(_buffered_obj + field_offset), _oopmap);\n@@ -542,2 +481,1 @@\n-                                               GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n-                                               GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n+                                               ArchiveHeapInfo* heap_info) {\n@@ -545,7 +483,2 @@\n-  size_t closed_region_byte_size = _closed_top - _closed_bottom;\n-  size_t open_region_byte_size   = _open_top   - _open_bottom;\n-  ResourceBitMap closed_oopmap(closed_region_byte_size \/ oopmap_unit);\n-  ResourceBitMap open_oopmap  (open_region_byte_size   \/ oopmap_unit);\n-\n-  _closed_oopmap = &closed_oopmap;\n-  _open_oopmap = &open_oopmap;\n+  size_t heap_region_byte_size = _buffer_used;\n+  heap_info->oopmap()->resize(heap_region_byte_size   \/ oopmap_unit);\n@@ -556,3 +489,1 @@\n-\n-    EmbeddedOopRelocator relocator(src_obj, buffered_obj);\n-\n+    EmbeddedOopRelocator relocator(src_obj, buffered_obj, heap_info->oopmap());\n@@ -566,1 +497,1 @@\n-  oop requested_roots = requested_obj_from_buffer_offset(_heap_roots_bottom);\n+  oop requested_roots = requested_obj_from_buffer_offset(_heap_roots_bottom_offset);\n@@ -571,1 +502,1 @@\n-      relocate_root_at<narrowOop>(requested_roots, i);\n+      relocate_root_at<narrowOop>(requested_roots, i, heap_info->oopmap());\n@@ -573,1 +504,1 @@\n-      relocate_root_at<oop>(requested_roots, i);\n+      relocate_root_at<oop>(requested_roots, i, heap_info->oopmap());\n@@ -577,35 +508,1 @@\n-  closed_bitmaps->append(make_bitmap_info(&closed_oopmap, \/*is_open=*\/false, \/*is_oopmap=*\/true));\n-  open_bitmaps  ->append(make_bitmap_info(&open_oopmap,   \/*is_open=*\/false, \/*is_oopmap=*\/true));\n-\n-  closed_bitmaps->append(compute_ptrmap(\/*is_open=*\/false));\n-  open_bitmaps  ->append(compute_ptrmap(\/*is_open=*\/true));\n-\n-  _closed_oopmap = nullptr;\n-  _open_oopmap = nullptr;\n-}\n-\n-ArchiveHeapBitmapInfo ArchiveHeapWriter::make_bitmap_info(ResourceBitMap* bitmap, bool is_open,  bool is_oopmap) {\n-  size_t size_in_bits = bitmap->size();\n-  size_t size_in_bytes;\n-  uintptr_t* buffer;\n-\n-  if (size_in_bits > 0) {\n-    size_in_bytes = bitmap->size_in_bytes();\n-    buffer = (uintptr_t*)NEW_C_HEAP_ARRAY(char, size_in_bytes, mtInternal);\n-    bitmap->write_to(buffer, size_in_bytes);\n-  } else {\n-    size_in_bytes = 0;\n-    buffer = nullptr;\n-  }\n-\n-  log_info(cds, heap)(\"%s @ \" INTPTR_FORMAT \" (\" SIZE_FORMAT_W(6) \" bytes) for %s heap region\",\n-                      is_oopmap ? \"Oopmap\" : \"Ptrmap\",\n-                      p2i(buffer), size_in_bytes,\n-                      is_open? \"open\" : \"closed\");\n-\n-  ArchiveHeapBitmapInfo info;\n-  info._map = (address)buffer;\n-  info._size_in_bits = size_in_bits;\n-  info._size_in_bytes = size_in_bytes;\n-\n-  return info;\n+  compute_ptrmap(heap_info);\n@@ -624,1 +521,1 @@\n-ArchiveHeapBitmapInfo ArchiveHeapWriter::compute_ptrmap(bool is_open) {\n+void ArchiveHeapWriter::compute_ptrmap(ArchiveHeapInfo* heap_info) {\n@@ -626,3 +523,3 @@\n-  Metadata** bottom = (Metadata**) (is_open ? _requested_open_region_bottom: _requested_closed_region_bottom);\n-  Metadata** top = (Metadata**) (is_open ? _requested_open_region_top: _requested_closed_region_top); \/\/ exclusive\n-  ResourceBitMap ptrmap(top - bottom);\n+  Metadata** bottom = (Metadata**) _requested_bottom;\n+  Metadata** top = (Metadata**) _requested_top; \/\/ exclusive\n+  heap_info->ptrmap()->resize(top - bottom);\n@@ -630,0 +527,1 @@\n+  BitMap::idx_t max_idx = 32; \/\/ paranoid - don't make it too small\n@@ -635,22 +533,21 @@\n-    if (p->in_open_region() == is_open) {\n-      \/\/ requested_field_addr = the address of this field in the requested space\n-      oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n-      Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n-      assert(bottom <= requested_field_addr && requested_field_addr < top, \"range check\");\n-\n-      \/\/ Mark this field in the bitmap\n-      BitMap::idx_t idx = requested_field_addr - bottom;\n-      ptrmap.set_bit(idx);\n-      num_non_null_ptrs ++;\n-\n-      \/\/ Set the native pointer to the requested address of the metadata (at runtime, the metadata will have\n-      \/\/ this address if the RO\/RW regions are mapped at the default location).\n-\n-      Metadata** buffered_field_addr = requested_addr_to_buffered_addr(requested_field_addr);\n-      Metadata* native_ptr = *buffered_field_addr;\n-      assert(native_ptr != nullptr, \"sanity\");\n-\n-      address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n-      address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n-      *buffered_field_addr = (Metadata*)requested_native_ptr;\n-    }\n+    \/\/ requested_field_addr = the address of this field in the requested space\n+    oop requested_obj = requested_obj_from_buffer_offset(p->buffer_offset());\n+    Metadata** requested_field_addr = (Metadata**)(cast_from_oop<address>(requested_obj) + field_offset);\n+    assert(bottom <= requested_field_addr && requested_field_addr < top, \"range check\");\n+\n+    \/\/ Mark this field in the bitmap\n+    BitMap::idx_t idx = requested_field_addr - bottom;\n+    heap_info->ptrmap()->set_bit(idx);\n+    num_non_null_ptrs ++;\n+    max_idx = MAX2(max_idx, idx);\n+\n+    \/\/ Set the native pointer to the requested address of the metadata (at runtime, the metadata will have\n+    \/\/ this address if the RO\/RW regions are mapped at the default location).\n+\n+    Metadata** buffered_field_addr = requested_addr_to_buffered_addr(requested_field_addr);\n+    Metadata* native_ptr = *buffered_field_addr;\n+    assert(native_ptr != nullptr, \"sanity\");\n+\n+    address buffered_native_ptr = ArchiveBuilder::current()->get_buffered_addr((address)native_ptr);\n+    address requested_native_ptr = ArchiveBuilder::current()->to_requested(buffered_native_ptr);\n+    *buffered_field_addr = (Metadata*)requested_native_ptr;\n@@ -659,9 +556,3 @@\n-  log_info(cds, heap)(\"compute_ptrmap: marked %d non-null native pointers for %s heap region\",\n-                      num_non_null_ptrs, is_open ? \"open\" : \"closed\");\n-\n-  if (num_non_null_ptrs == 0) {\n-    ResourceBitMap empty;\n-    return make_bitmap_info(&empty, is_open, \/*is_oopmap=*\/ false);\n-  } else {\n-    return make_bitmap_info(&ptrmap, is_open, \/*is_oopmap=*\/ false);\n-  }\n+  heap_info->ptrmap()->resize(max_idx + 1);\n+  log_info(cds, heap)(\"calculate_ptrmap: marked %d non-null native pointers for heap region (\" SIZE_FORMAT \" bits)\",\n+                      num_non_null_ptrs, size_t(heap_info->ptrmap()->size()));\n","filename":"src\/hotspot\/share\/cds\/archiveHeapWriter.cpp","additions":115,"deletions":224,"binary":false,"changes":339,"status":"modified"},{"patch":"@@ -167,2 +167,2 @@\n-      vm_exit_during_initialization(\"Out of memory in the CDS archive\",\n-                                    \"Please reduce the number of shared classes.\");\n+      log_error(cds)(\"Out of memory in the CDS archive: Please reduce the number of shared classes.\");\n+      MetaspaceShared::unrecoverable_writing_error();\n@@ -193,2 +193,3 @@\n-    vm_exit_during_initialization(err_msg(\"Failed to expand shared space to \" SIZE_FORMAT \" bytes\",\n-                                          need_committed_size));\n+    log_error(cds)(\"Failed to expand shared space to \" SIZE_FORMAT \" bytes\",\n+                    need_committed_size);\n+    MetaspaceShared::unrecoverable_writing_error();\n@@ -242,1 +243,1 @@\n-  log_debug(cds)(\"%-3s space: \" SIZE_FORMAT_W(9) \" [ %4.1f%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [%5.1f%% used] at \" INTPTR_FORMAT,\n+  log_debug(cds)(\"%s space: \" SIZE_FORMAT_W(9) \" [ %4.1f%% of total] out of \" SIZE_FORMAT_W(9) \" bytes [%5.1f%% used] at \" INTPTR_FORMAT,\n@@ -333,1 +334,1 @@\n-    if (CompressedOops::is_null(o) || !ArchiveHeapLoader::is_fully_available()) {\n+    if (CompressedOops::is_null(o) || !ArchiveHeapLoader::is_in_use()) {\n@@ -337,1 +338,1 @@\n-      assert(ArchiveHeapLoader::is_fully_available(), \"must be\");\n+      assert(ArchiveHeapLoader::is_in_use(), \"must be\");\n@@ -342,1 +343,1 @@\n-    if (dumptime_oop == 0 || !ArchiveHeapLoader::is_fully_available()) {\n+    if (dumptime_oop == 0 || !ArchiveHeapLoader::is_in_use()) {\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":9,"deletions":8,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-  msg.debug(\"Detailed metadata info (excluding heap regions):\");\n+  msg.debug(\"Detailed metadata info (excluding heap region):\");\n","filename":"src\/hotspot\/share\/cds\/dumpAllocStats.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -86,1 +86,0 @@\n-bool HeapShared::_copying_open_region_objects = false;\n@@ -107,4 +106,1 @@\n-\/\/ Entry fields for shareable subgraphs archived in the closed archive heap\n-\/\/ region. Warning: Objects in the subgraphs should not have reference fields\n-\/\/ assigned at runtime.\n-static ArchivableStaticFieldInfo closed_archive_subgraph_entry_fields[] = {\n+static ArchivableStaticFieldInfo archive_subgraph_entry_fields[] = {\n@@ -118,4 +114,0 @@\n-  {nullptr, nullptr},\n-};\n-\/\/ Entry fields for subgraphs archived in the open archive heap region.\n-static ArchivableStaticFieldInfo open_archive_subgraph_entry_fields[] = {\n@@ -133,2 +125,2 @@\n-\/\/ Entry fields for subgraphs archived in the open archive heap region (full module graph).\n-static ArchivableStaticFieldInfo fmg_open_archive_subgraph_entry_fields[] = {\n+\/\/ full module graph\n+static ArchivableStaticFieldInfo fmg_archive_subgraph_entry_fields[] = {\n@@ -157,3 +149,2 @@\n-  return is_subgraph_root_class_of(closed_archive_subgraph_entry_fields, ik) ||\n-         is_subgraph_root_class_of(open_archive_subgraph_entry_fields, ik) ||\n-         is_subgraph_root_class_of(fmg_open_archive_subgraph_entry_fields, ik);\n+  return is_subgraph_root_class_of(archive_subgraph_entry_fields, ik) ||\n+         is_subgraph_root_class_of(fmg_archive_subgraph_entry_fields, ik);\n@@ -265,1 +256,1 @@\n-  if (ArchiveHeapLoader::is_fully_available()) {\n+  if (ArchiveHeapLoader::is_in_use()) {\n@@ -382,2 +373,0 @@\n-  init_seen_objects_table();\n-\n@@ -389,1 +378,1 @@\n-      bool success = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n+      bool success = archive_reachable_objects_from(1, _default_subgraph_info, m);\n@@ -407,2 +396,2 @@\n-      bool success = archive_reachable_objects_from(1, _default_subgraph_info, m, \/*is_closed_archive=*\/ false);\n-      guarantee(success, \"scratch mirrors should not point to any unachivable objects\");\n+      bool success = archive_reachable_objects_from(1, _default_subgraph_info, m);\n+      guarantee(success, \"scratch mirrors must point to only archivable objects\");\n@@ -420,2 +409,1 @@\n-          bool success = HeapShared::archive_reachable_objects_from(1, _default_subgraph_info, rr,\n-                                                                    \/*is_closed_archive=*\/false);\n+          bool success = HeapShared::archive_reachable_objects_from(1, _default_subgraph_info, rr);\n@@ -429,0 +417,1 @@\n+}\n@@ -430,1 +419,8 @@\n-  delete_seen_objects_table();\n+void HeapShared::archive_strings() {\n+  oop shared_strings_array = StringTable::init_shared_table(_dumped_interned_strings);\n+  bool success = archive_reachable_objects_from(1, _default_subgraph_info, shared_strings_array);\n+  \/\/ We must succeed because:\n+  \/\/ - _dumped_interned_strings do not contain any large strings.\n+  \/\/ - StringTable::init_shared_table() doesn't create any large arrays.\n+  assert(success, \"shared strings array must not point to arrays or strings that are too large to archive\");\n+  StringTable::set_shared_strings_array_index(append_root(shared_strings_array));\n@@ -461,2 +457,1 @@\n-                                oop orig_obj,\n-                                bool is_closed_archive) {\n+                                oop orig_obj) {\n@@ -491,1 +486,1 @@\n-        bool success = archive_reachable_objects_from(level, subgraph_info, oop_field, is_closed_archive);\n+        bool success = archive_reachable_objects_from(level, subgraph_info, oop_field);\n@@ -505,1 +500,1 @@\n-  if (!ArchiveHeapLoader::is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_in_use()) {\n@@ -530,4 +525,1 @@\n-void HeapShared::archive_objects(GrowableArray<MemRegion>* closed_regions,\n-                                 GrowableArray<MemRegion>* open_regions,\n-                                 GrowableArray<ArchiveHeapBitmapInfo>* closed_bitmaps,\n-                                 GrowableArray<ArchiveHeapBitmapInfo>* open_bitmaps) {\n+void HeapShared::archive_objects(ArchiveHeapInfo *heap_info) {\n@@ -547,7 +539,1 @@\n-    log_info(cds)(\"Dumping objects to closed archive heap region ...\");\n-    copy_closed_objects();\n-\n-    _copying_open_region_objects = true;\n-\n-    log_info(cds)(\"Dumping objects to open archive heap region ...\");\n-    copy_open_objects();\n+    copy_objects();\n@@ -559,2 +545,1 @@\n-  ArchiveHeapWriter::write(_pending_roots, closed_regions, open_regions, closed_bitmaps, open_bitmaps);\n-  StringTable::write_shared_table(_dumped_interned_strings);\n+  ArchiveHeapWriter::write(_pending_roots, heap_info);\n@@ -568,8 +553,6 @@\n-    if (!ArchiveHeapWriter::is_string_too_large_to_archive(s)) {\n-      bool success = archive_reachable_objects_from(1, _default_subgraph_info,\n-                                                    s, \/*is_closed_archive=*\/true);\n-      assert(success, \"must be\");\n-      \/\/ Prevent string deduplication from changing the value field to\n-      \/\/ something not in the archive.\n-      java_lang_String::set_deduplication_forbidden(s);\n-    }\n+    assert(!ArchiveHeapWriter::is_string_too_large_to_archive(s), \"large strings must have been filtered\");\n+    bool success = archive_reachable_objects_from(1, _default_subgraph_info, s);\n+    assert(success, \"must be\");\n+    \/\/ Prevent string deduplication from changing the value field to\n+    \/\/ something not in the archive.\n+    java_lang_String::set_deduplication_forbidden(s);\n@@ -582,9 +565,6 @@\n-void HeapShared::copy_closed_objects() {\n-  assert(HeapShared::can_write(), \"must be\");\n-\n-  \/\/ Archive interned string objects\n-  copy_interned_strings();\n-\n-  archive_object_subgraphs(closed_archive_subgraph_entry_fields,\n-                           true \/* is_closed_archive *\/,\n-                           false \/* is_full_module_graph *\/);\n+void HeapShared::copy_special_objects() {\n+  \/\/ Archive special objects that do not belong to any subgraphs\n+  init_seen_objects_table();\n+  archive_java_mirrors();\n+  archive_strings();\n+  delete_seen_objects_table();\n@@ -593,1 +573,1 @@\n-void HeapShared::copy_open_objects() {\n+void HeapShared::copy_objects() {\n@@ -596,1 +576,2 @@\n-  archive_java_mirrors();\n+  copy_interned_strings();\n+  copy_special_objects();\n@@ -598,2 +579,1 @@\n-  archive_object_subgraphs(open_archive_subgraph_entry_fields,\n-                           false \/* is_closed_archive *\/,\n+  archive_object_subgraphs(archive_subgraph_entry_fields,\n@@ -601,0 +581,1 @@\n+\n@@ -602,2 +583,1 @@\n-    archive_object_subgraphs(fmg_open_archive_subgraph_entry_fields,\n-                             false \/* is_closed_archive *\/,\n+    archive_object_subgraphs(fmg_archive_subgraph_entry_fields,\n@@ -637,2 +617,1 @@\n-void KlassSubGraphInfo::add_subgraph_entry_field(\n-      int static_field_offset, oop v, bool is_closed_archive) {\n+void KlassSubGraphInfo::add_subgraph_entry_field(int static_field_offset, oop v) {\n@@ -727,1 +706,1 @@\n-  os::_exit(1);\n+  MetaspaceShared::unrecoverable_writing_error();\n@@ -828,1 +807,1 @@\n-\/\/   offset, value and is_closed_archive flag are recorded in the sub-graph\n+\/\/   offset, and value are recorded in the sub-graph\n@@ -865,1 +844,1 @@\n-      assert(ArchiveHeapLoader::is_fully_available(), \"must be\");\n+      assert(ArchiveHeapLoader::is_in_use(), \"must be\");\n@@ -925,1 +904,1 @@\n-  if (!ArchiveHeapLoader::is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_in_use()) {\n@@ -928,3 +907,2 @@\n-  resolve_classes_for_subgraphs(current, closed_archive_subgraph_entry_fields);\n-  resolve_classes_for_subgraphs(current, open_archive_subgraph_entry_fields);\n-  resolve_classes_for_subgraphs(current, fmg_open_archive_subgraph_entry_fields);\n+  resolve_classes_for_subgraphs(current, archive_subgraph_entry_fields);\n+  resolve_classes_for_subgraphs(current, fmg_archive_subgraph_entry_fields);\n@@ -958,1 +936,1 @@\n-  if (!ArchiveHeapLoader::is_fully_available()) {\n+  if (!ArchiveHeapLoader::is_in_use()) {\n@@ -1107,1 +1085,0 @@\n-  bool _is_closed_archive;\n@@ -1118,1 +1095,0 @@\n-                           bool is_closed_archive,\n@@ -1122,1 +1098,1 @@\n-    _level(level), _is_closed_archive(is_closed_archive),\n+    _level(level),\n@@ -1154,1 +1130,1 @@\n-          _level + 1, _subgraph_info, obj, _is_closed_archive);\n+          _level + 1, _subgraph_info, obj);\n@@ -1170,17 +1146,1 @@\n-  return CachedOopInfo(referrer, _copying_open_region_objects);\n-}\n-\n-void HeapShared::check_closed_region_object(InstanceKlass* k) {\n-  \/\/ Check fields in the object\n-  for (JavaFieldStream fs(k); !fs.done(); fs.next()) {\n-    if (!fs.access_flags().is_static()) {\n-      BasicType ft = fs.field_descriptor().field_type();\n-      if (!fs.access_flags().is_final() && is_reference_type(ft)) {\n-        ResourceMark rm;\n-        log_warning(cds, heap)(\n-          \"Please check reference field in %s instance in closed archive heap region: %s %s\",\n-          k->external_name(), (fs.name())->as_C_string(),\n-          (fs.signature())->as_C_string());\n-      }\n-    }\n-  }\n+  return CachedOopInfo(referrer);\n@@ -1195,2 +1155,1 @@\n-                                                oop orig_obj,\n-                                                bool is_closed_archive) {\n+                                                oop orig_obj) {\n@@ -1205,1 +1164,1 @@\n-    os::_exit(1);\n+    MetaspaceShared::unrecoverable_writing_error();\n@@ -1215,1 +1174,1 @@\n-    os::_exit(1);\n+    MetaspaceShared::unrecoverable_writing_error();\n@@ -1244,1 +1203,1 @@\n-        os::_exit(1);\n+        MetaspaceShared::unrecoverable_writing_error();\n@@ -1252,2 +1211,1 @@\n-  WalkOopAndArchiveClosure walker(level, is_closed_archive, record_klasses_only,\n-                                  subgraph_info, orig_obj);\n+  WalkOopAndArchiveClosure walker(level, record_klasses_only, subgraph_info, orig_obj);\n@@ -1255,4 +1213,1 @@\n-  if (is_closed_archive && orig_k->is_instance_klass()) {\n-    check_closed_region_object(InstanceKlass::cast(orig_k));\n-  }\n-  check_enum_obj(level + 1, subgraph_info, orig_obj, is_closed_archive);\n+  check_enum_obj(level + 1, subgraph_info, orig_obj);\n@@ -1300,2 +1255,1 @@\n-                                                             const char* field_name,\n-                                                             bool is_closed_archive) {\n+                                                             const char* field_name) {\n@@ -1319,2 +1273,1 @@\n-    bool success = archive_reachable_objects_from(1, subgraph_info, f, is_closed_archive);\n-\n+    bool success = archive_reachable_objects_from(1, subgraph_info, f);\n@@ -1328,1 +1281,1 @@\n-      subgraph_info->add_subgraph_entry_field(field_offset, f, is_closed_archive);\n+      subgraph_info->add_subgraph_entry_field(field_offset, f);\n@@ -1334,1 +1287,1 @@\n-    subgraph_info->add_subgraph_entry_field(field_offset, nullptr, false);\n+    subgraph_info->add_subgraph_entry_field(field_offset, nullptr);\n@@ -1564,2 +1517,1 @@\n-  init_subgraph_entry_fields(closed_archive_subgraph_entry_fields, CHECK);\n-  init_subgraph_entry_fields(open_archive_subgraph_entry_fields, CHECK);\n+  init_subgraph_entry_fields(archive_subgraph_entry_fields, CHECK);\n@@ -1567,1 +1519,1 @@\n-    init_subgraph_entry_fields(fmg_open_archive_subgraph_entry_fields, CHECK);\n+    init_subgraph_entry_fields(fmg_archive_subgraph_entry_fields, CHECK);\n@@ -1573,2 +1525,2 @@\n-  ArchivableStaticFieldInfo* p = open_archive_subgraph_entry_fields;\n-  int num_slots = sizeof(open_archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n+  ArchivableStaticFieldInfo* p = archive_subgraph_entry_fields;\n+  int num_slots = sizeof(archive_subgraph_entry_fields) \/ sizeof(ArchivableStaticFieldInfo);\n@@ -1641,1 +1593,0 @@\n-                                          bool is_closed_archive,\n@@ -1672,2 +1623,1 @@\n-                                                  f->offset, f->field_name,\n-                                                  is_closed_archive);\n+                                                  f->offset, f->field_name);\n@@ -1678,2 +1628,1 @@\n-  log_info(cds, heap)(\"Archived subgraph records in %s archive heap region = %d\",\n-                      is_closed_archive ? \"closed\" : \"open\",\n+  log_info(cds, heap)(\"Archived subgraph records = %d\",\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":70,"deletions":121,"binary":false,"changes":191,"status":"modified"},{"patch":"@@ -442,1 +442,1 @@\n-  MutexLocker ml(current, UnregisteredClassesTable_lock);\n+  MutexLocker ml(current, UnregisteredClassesTable_lock, Mutex::_no_safepoint_check_flag);\n@@ -526,8 +526,10 @@\n-  if (_unregistered_classes_table != nullptr) {\n-    \/\/ Remove the class from _unregistered_classes_table: keep the entry but\n-    \/\/ set it to null. This ensure no classes with the same name can be\n-    \/\/ added again.\n-    MutexLocker ml(Thread::current(), UnregisteredClassesTable_lock);\n-    InstanceKlass** v = _unregistered_classes_table->get(klass->name());\n-    if (v != nullptr) {\n-      *v = nullptr;\n+  if (Arguments::is_dumping_archive() || ClassListWriter::is_enabled()) {\n+    MutexLocker ml(Thread::current(), UnregisteredClassesTable_lock, Mutex::_no_safepoint_check_flag);\n+    if (_unregistered_classes_table != nullptr) {\n+      \/\/ Remove the class from _unregistered_classes_table: keep the entry but\n+      \/\/ set it to null. This ensure no classes with the same name can be\n+      \/\/ added again.\n+      InstanceKlass** v = _unregistered_classes_table->get(klass->name());\n+      if (v != nullptr) {\n+        *v = nullptr;\n+      }\n@@ -535,0 +537,2 @@\n+  } else {\n+    assert(_unregistered_classes_table == nullptr, \"must not be used\");\n@@ -854,6 +858,4 @@\n-  {\n-    MutexLocker mu_r(THREAD, Compile_lock);\n-    \/\/ Add to class hierarchy, and do possible deoptimizations.\n-    SystemDictionary::add_to_hierarchy(loaded_lambda);\n-    \/\/ But, do not add to dictionary.\n-  }\n+  \/\/ Add to class hierarchy, and do possible deoptimizations.\n+  loaded_lambda->add_to_hierarchy(THREAD);\n+  \/\/ But, do not add to dictionary.\n+\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":17,"deletions":15,"binary":false,"changes":32,"status":"modified"},{"patch":"@@ -197,22 +197,8 @@\n-HeapWord*\n-G1CollectedHeap::humongous_obj_allocate_initialize_regions(HeapRegion* first_hr,\n-                                                           uint num_regions,\n-                                                           size_t word_size) {\n-  assert(first_hr != NULL, \"pre-condition\");\n-  assert(is_humongous(word_size), \"word_size should be humongous\");\n-  assert(num_regions * HeapRegion::GrainWords >= word_size, \"pre-condition\");\n-\n-  \/\/ Index of last region in the series.\n-  uint first = first_hr->hrm_index();\n-  uint last = first + num_regions - 1;\n-\n-  \/\/ We need to initialize the region(s) we just discovered. This is\n-  \/\/ a bit tricky given that it can happen concurrently with\n-  \/\/ refinement threads refining cards on these regions and\n-  \/\/ potentially wanting to refine the BOT as they are scanning\n-  \/\/ those cards (this can happen shortly after a cleanup; see CR\n-  \/\/ 6991377). So we have to set up the region(s) carefully and in\n-  \/\/ a specific order.\n-\n-  \/\/ The word size sum of all the regions we will allocate.\n-  size_t word_size_sum = (size_t) num_regions * HeapRegion::GrainWords;\n+void G1CollectedHeap::set_humongous_metadata(HeapRegion* first_hr,\n+                                             uint num_regions,\n+                                             size_t word_size,\n+                                             bool update_remsets) {\n+  \/\/ Calculate the new top of the humongous object.\n+  HeapWord* obj_top = first_hr->bottom() + word_size;\n+  \/\/ The word size sum of all the regions used\n+  size_t word_size_sum = num_regions * HeapRegion::GrainWords;\n@@ -221,20 +207,2 @@\n-  \/\/ The passed in hr will be the \"starts humongous\" region. The header\n-  \/\/ of the new object will be placed at the bottom of this region.\n-  HeapWord* new_obj = first_hr->bottom();\n-  \/\/ This will be the new top of the new object.\n-  HeapWord* obj_top = new_obj + word_size;\n-\n-  \/\/ First, we need to zero the header of the space that we will be\n-  \/\/ allocating. When we update top further down, some refinement\n-  \/\/ threads might try to scan the region. By zeroing the header we\n-  \/\/ ensure that any thread that will try to scan the region will\n-  \/\/ come across the zero klass word and bail out.\n-  \/\/\n-  \/\/ NOTE: It would not have been correct to have used\n-  \/\/ CollectedHeap::fill_with_object() and make the space look like\n-  \/\/ an int array. The thread that is doing the allocation will\n-  \/\/ later update the object header to a potentially different array\n-  \/\/ type and, for a very short period of time, the klass and length\n-  \/\/ fields will be inconsistent. This could cause a refinement\n-  \/\/ thread to calculate the object size incorrectly.\n-  Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);\n+  \/\/ How many words memory we \"waste\" which cannot hold a filler object.\n+  size_t words_not_fillable = 0;\n@@ -242,1 +210,1 @@\n-  \/\/ Next, pad out the unused tail of the last region with filler\n+  \/\/ Pad out the unused tail of the last region with filler\n@@ -244,4 +212,2 @@\n-  \/\/ How many words we use for filler objects.\n-  size_t word_fill_size = word_size_sum - word_size;\n-  \/\/ How many words memory we \"waste\" which cannot hold a filler object.\n-  size_t words_not_fillable = 0;\n+  \/\/ How many words can we use for filler objects.\n+  size_t words_fillable = word_size_sum - word_size;\n@@ -250,3 +216,3 @@\n-  if (word_fill_size >= min_fill_size()) {\n-    fill_with_objects(obj_top, word_fill_size);\n-  } else if (word_fill_size > 0) {\n+  if (words_fillable >= G1CollectedHeap::min_fill_size()) {\n+    G1CollectedHeap::fill_with_objects(obj_top, words_fillable);\n+  } else {\n@@ -254,2 +220,2 @@\n-    words_not_fillable = word_fill_size;\n-    word_fill_size = 0;\n+    words_not_fillable = words_fillable;\n+    words_fillable = 0;\n@@ -262,5 +228,12 @@\n-  first_hr->set_starts_humongous(obj_top, word_fill_size);\n-  _policy->remset_tracker()->update_at_allocate(first_hr);\n-  \/\/ Then, if there are any, we will set up the \"continues\n-  \/\/ humongous\" regions.\n-  HeapRegion* hr = NULL;\n+  first_hr->hr_clear(false \/* clear_space *\/);\n+  first_hr->set_starts_humongous(obj_top, words_fillable);\n+\n+  if (update_remsets) {\n+    _policy->remset_tracker()->update_at_allocate(first_hr);\n+  }\n+\n+  \/\/ Indices of first and last regions in the series.\n+  uint first = first_hr->hrm_index();\n+  uint last = first + num_regions - 1;\n+\n+  HeapRegion* hr = nullptr;\n@@ -269,0 +242,1 @@\n+    hr->hr_clear(false \/* clear_space *\/);\n@@ -270,1 +244,3 @@\n-    _policy->remset_tracker()->update_at_allocate(hr);\n+    if (update_remsets) {\n+      _policy->remset_tracker()->update_at_allocate(hr);\n+    }\n@@ -301,0 +277,1 @@\n+}\n@@ -302,1 +279,46 @@\n-  increase_used((word_size_sum - words_not_fillable) * HeapWordSize);\n+HeapWord*\n+G1CollectedHeap::humongous_obj_allocate_initialize_regions(HeapRegion* first_hr,\n+                                                           uint num_regions,\n+                                                           size_t word_size) {\n+  assert(first_hr != NULL, \"pre-condition\");\n+  assert(is_humongous(word_size), \"word_size should be humongous\");\n+  assert(num_regions * HeapRegion::GrainWords >= word_size, \"pre-condition\");\n+\n+  \/\/ Index of last region in the series.\n+  uint first = first_hr->hrm_index();\n+  uint last = first + num_regions - 1;\n+\n+  \/\/ We need to initialize the region(s) we just discovered. This is\n+  \/\/ a bit tricky given that it can happen concurrently with\n+  \/\/ refinement threads refining cards on these regions and\n+  \/\/ potentially wanting to refine the BOT as they are scanning\n+  \/\/ those cards (this can happen shortly after a cleanup; see CR\n+  \/\/ 6991377). So we have to set up the region(s) carefully and in\n+  \/\/ a specific order.\n+\n+  \/\/ The passed in hr will be the \"starts humongous\" region. The header\n+  \/\/ of the new object will be placed at the bottom of this region.\n+  HeapWord* new_obj = first_hr->bottom();\n+\n+  \/\/ First, we need to zero the header of the space that we will be\n+  \/\/ allocating. When we update top further down, some refinement\n+  \/\/ threads might try to scan the region. By zeroing the header we\n+  \/\/ ensure that any thread that will try to scan the region will\n+  \/\/ come across the zero klass word and bail out.\n+  \/\/\n+  \/\/ NOTE: It would not have been correct to have used\n+  \/\/ CollectedHeap::fill_with_object() and make the space look like\n+  \/\/ an int array. The thread that is doing the allocation will\n+  \/\/ later update the object header to a potentially different array\n+  \/\/ type and, for a very short period of time, the klass and length\n+  \/\/ fields will be inconsistent. This could cause a refinement\n+  \/\/ thread to calculate the object size incorrectly.\n+  Copy::fill_to_words(new_obj, oopDesc::header_size(), 0);\n+\n+  \/\/ Next, update the metadata for the regions.\n+  set_humongous_metadata(first_hr, num_regions, word_size, true);\n+\n+  HeapRegion* last_hr = region_at(last);\n+  size_t used = byte_size(first_hr->bottom(), last_hr->top());\n+\n+  increase_used(used);\n@@ -305,1 +327,1 @@\n-    hr = region_at(i);\n+    HeapRegion *hr = region_at(i);\n@@ -493,8 +515,18 @@\n-bool G1CollectedHeap::check_archive_addresses(MemRegion* ranges, size_t count) {\n-  assert(ranges != NULL, \"MemRegion array NULL\");\n-  assert(count != 0, \"No MemRegions provided\");\n-  MemRegion reserved = _hrm.reserved();\n-  for (size_t i = 0; i < count; i++) {\n-    if (!reserved.contains(ranges[i].start()) || !reserved.contains(ranges[i].last())) {\n-      return false;\n-    }\n+bool G1CollectedHeap::check_archive_addresses(MemRegion range) {\n+  return _hrm.reserved().contains(range);\n+}\n+\n+template <typename Func>\n+void G1CollectedHeap::iterate_regions_in_range(MemRegion range, const Func& func) {\n+  \/\/ Mark each G1 region touched by the range as old, add it to\n+  \/\/ the old set, and set top.\n+  HeapRegion* curr_region = _hrm.addr_to_region(range.start());\n+  HeapRegion* end_region = _hrm.addr_to_region(range.last());\n+\n+  while (curr_region != nullptr) {\n+    bool is_last = curr_region == end_region;\n+    HeapRegion* next_region = is_last ? nullptr : _hrm.next_region_in_heap(curr_region);\n+\n+    func(curr_region, is_last);\n+\n+    curr_region = next_region;\n@@ -502,1 +534,0 @@\n-  return true;\n@@ -505,3 +536,1 @@\n-bool G1CollectedHeap::alloc_archive_regions(MemRegion* ranges,\n-                                            size_t count,\n-                                            bool open) {\n+bool G1CollectedHeap::alloc_archive_regions(MemRegion range) {\n@@ -509,2 +538,0 @@\n-  assert(ranges != NULL, \"MemRegion array NULL\");\n-  assert(count != 0, \"No MemRegions provided\");\n@@ -514,2 +541,0 @@\n-  HeapWord* prev_last_addr = NULL;\n-  HeapRegion* prev_last_region = NULL;\n@@ -521,34 +546,6 @@\n-  \/\/ For each specified MemRegion range, allocate the corresponding G1\n-  \/\/ regions and mark them as archive regions. We expect the ranges\n-  \/\/ in ascending starting address order, without overlap.\n-  for (size_t i = 0; i < count; i++) {\n-    MemRegion curr_range = ranges[i];\n-    HeapWord* start_address = curr_range.start();\n-    size_t word_size = curr_range.word_size();\n-    HeapWord* last_address = curr_range.last();\n-    size_t commits = 0;\n-\n-    guarantee(reserved.contains(start_address) && reserved.contains(last_address),\n-              \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n-              p2i(start_address), p2i(last_address));\n-    guarantee(start_address > prev_last_addr,\n-              \"Ranges not in ascending order: \" PTR_FORMAT \" <= \" PTR_FORMAT ,\n-              p2i(start_address), p2i(prev_last_addr));\n-    prev_last_addr = last_address;\n-\n-    \/\/ Check for ranges that start in the same G1 region in which the previous\n-    \/\/ range ended, and adjust the start address so we don't try to allocate\n-    \/\/ the same region again. If the current range is entirely within that\n-    \/\/ region, skip it, just adjusting the recorded top.\n-    HeapRegion* start_region = _hrm.addr_to_region(start_address);\n-    if ((prev_last_region != NULL) && (start_region == prev_last_region)) {\n-      start_address = start_region->end();\n-      if (start_address > last_address) {\n-        increase_used(word_size * HeapWordSize);\n-        start_region->set_top(last_address + 1);\n-        continue;\n-      }\n-      start_region->set_top(start_address);\n-      curr_range = MemRegion(start_address, last_address + 1);\n-      start_region = _hrm.addr_to_region(start_address);\n-    }\n+  \/\/ For the specified MemRegion range, allocate the corresponding G1\n+  \/\/ region(s) and mark them as old region(s).\n+  HeapWord* start_address = range.start();\n+  size_t word_size = range.word_size();\n+  HeapWord* last_address = range.last();\n+  size_t commits = 0;\n@@ -556,9 +553,3 @@\n-    \/\/ Perform the actual region allocation, exiting if it fails.\n-    \/\/ Then note how much new space we have allocated.\n-    if (!_hrm.allocate_containing_regions(curr_range, &commits, workers())) {\n-      return false;\n-    }\n-    increase_used(word_size * HeapWordSize);\n-    if (commits != 0) {\n-      log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocate archive regions). Total size: \" SIZE_FORMAT \"B\",\n-                                HeapRegion::GrainWords * HeapWordSize * commits);\n+  guarantee(reserved.contains(start_address) && reserved.contains(last_address),\n+            \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n+            p2i(start_address), p2i(last_address));\n@@ -566,1 +557,9 @@\n-    }\n+  \/\/ Perform the actual region allocation, exiting if it fails.\n+  \/\/ Then note how much new space we have allocated.\n+  if (!_hrm.allocate_containing_regions(range, &commits, workers())) {\n+    return false;\n+  }\n+  increase_used(word_size * HeapWordSize);\n+  if (commits != 0) {\n+    log_debug(gc, ergo, heap)(\"Attempt heap expansion (allocate archive regions). Total size: \" SIZE_FORMAT \"B\",\n+                              HeapRegion::GrainWords * HeapWordSize * commits);\n@@ -568,28 +567,15 @@\n-    \/\/ Mark each G1 region touched by the range as archive, add it to\n-    \/\/ the old set, and set top.\n-    HeapRegion* curr_region = _hrm.addr_to_region(start_address);\n-    HeapRegion* last_region = _hrm.addr_to_region(last_address);\n-    prev_last_region = last_region;\n-\n-    while (curr_region != NULL) {\n-      assert(curr_region->is_empty() && !curr_region->is_pinned(),\n-             \"Region already in use (index %u)\", curr_region->hrm_index());\n-      if (open) {\n-        curr_region->set_open_archive();\n-      } else {\n-        curr_region->set_closed_archive();\n-      }\n-      _hr_printer.alloc(curr_region);\n-      _archive_set.add(curr_region);\n-      HeapWord* top;\n-      HeapRegion* next_region;\n-      if (curr_region != last_region) {\n-        top = curr_region->end();\n-        next_region = _hrm.next_region_in_heap(curr_region);\n-      } else {\n-        top = last_address + 1;\n-        next_region = NULL;\n-      }\n-      curr_region->set_top(top);\n-      curr_region = next_region;\n-    }\n+\n+  \/\/ Mark each G1 region touched by the range as old, add it to\n+  \/\/ the old set, and set top.\n+  auto set_region_to_old = [&] (HeapRegion* r, bool is_last) {\n+    assert(r->is_empty() && !r->is_pinned(), \"Region already in use (%u)\", r->hrm_index());\n+\n+    HeapWord* top = is_last ? last_address + 1 : r->end();\n+    r->set_top(top);\n+\n+    r->set_old();\n+    _hr_printer.alloc(r);\n+    _old_set.add(r);\n+  };\n+\n+  iterate_regions_in_range(range, set_region_to_old);\n@@ -600,1 +586,10 @@\n-void G1CollectedHeap::fill_archive_regions(MemRegion* ranges, size_t count) {\n+void G1CollectedHeap::populate_archive_regions_bot_part(MemRegion range) {\n+  assert(!is_init_completed(), \"Expect to be called at JVM init time\");\n+\n+  iterate_regions_in_range(range,\n+                           [&] (HeapRegion* r, bool is_last) {\n+                             r->update_bot();\n+                           });\n+}\n+\n+void G1CollectedHeap::dealloc_archive_regions(MemRegion range) {\n@@ -602,4 +597,2 @@\n-  assert(ranges != NULL, \"MemRegion array NULL\");\n-  assert(count != 0, \"No MemRegions provided\");\n-  HeapWord *prev_last_addr = NULL;\n-  HeapRegion* prev_last_region = NULL;\n+  size_t size_used = 0;\n+  uint shrink_count = 0;\n@@ -608,4 +601,1 @@\n-  \/\/ For each MemRegion, create filler objects, if needed, in the G1 regions\n-  \/\/ that contain the address range. The address range actually within the\n-  \/\/ MemRegion will not be modified. That is assumed to have been initialized\n-  \/\/ elsewhere, probably via an mmap of archived heap data.\n+  \/\/ Free the G1 regions that are within the specified range.\n@@ -613,33 +603,19 @@\n-  for (size_t i = 0; i < count; i++) {\n-    HeapWord* start_address = ranges[i].start();\n-    HeapWord* last_address = ranges[i].last();\n-\n-    assert(reserved.contains(start_address) && reserved.contains(last_address),\n-           \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n-           p2i(start_address), p2i(last_address));\n-    assert(start_address > prev_last_addr,\n-           \"Ranges not in ascending order: \" PTR_FORMAT \" <= \" PTR_FORMAT ,\n-           p2i(start_address), p2i(prev_last_addr));\n-\n-    HeapRegion* start_region = _hrm.addr_to_region(start_address);\n-    HeapRegion* last_region = _hrm.addr_to_region(last_address);\n-    HeapWord* bottom_address = start_region->bottom();\n-\n-    \/\/ Check for a range beginning in the same region in which the\n-    \/\/ previous one ended.\n-    if (start_region == prev_last_region) {\n-      bottom_address = prev_last_addr + 1;\n-    }\n-\n-    \/\/ Verify that the regions were all marked as archive regions by\n-    \/\/ alloc_archive_regions.\n-    HeapRegion* curr_region = start_region;\n-    while (curr_region != NULL) {\n-      guarantee(curr_region->is_archive(),\n-                \"Expected archive region at index %u\", curr_region->hrm_index());\n-      if (curr_region != last_region) {\n-        curr_region = _hrm.next_region_in_heap(curr_region);\n-      } else {\n-        curr_region = NULL;\n-      }\n-    }\n+  HeapWord* start_address = range.start();\n+  HeapWord* last_address = range.last();\n+\n+  assert(reserved.contains(start_address) && reserved.contains(last_address),\n+         \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n+         p2i(start_address), p2i(last_address));\n+  size_used += range.byte_size();\n+\n+  \/\/ Free, empty and uncommit regions with CDS archive content.\n+  auto dealloc_archive_region = [&] (HeapRegion* r, bool is_last) {\n+    guarantee(r->is_old(), \"Expected old region at index %u\", r->hrm_index());\n+    _old_set.remove(r);\n+    r->set_free();\n+    r->set_top(r->bottom());\n+    _hrm.shrink_at(r->hrm_index(), 1);\n+    shrink_count++;\n+  };\n+\n+  iterate_regions_in_range(range, dealloc_archive_region);\n@@ -647,12 +623,5 @@\n-    prev_last_addr = last_address;\n-    prev_last_region = last_region;\n-\n-    \/\/ Fill the memory below the allocated range with dummy object(s),\n-    \/\/ if the region bottom does not match the range start, or if the previous\n-    \/\/ range ended within the same G1 region, and there is a gap.\n-    assert(start_address >= bottom_address, \"bottom address should not be greater than start address\");\n-    if (start_address > bottom_address) {\n-      size_t fill_size = pointer_delta(start_address, bottom_address);\n-      G1CollectedHeap::fill_with_objects(bottom_address, fill_size);\n-      increase_used(fill_size * HeapWordSize);\n-    }\n+  if (shrink_count != 0) {\n+    log_debug(gc, ergo, heap)(\"Attempt heap shrinking (CDS archive regions). Total size: \" SIZE_FORMAT \"B\",\n+                              HeapRegion::GrainWords * HeapWordSize * shrink_count);\n+    \/\/ Explicit uncommit.\n+    uncommit_regions(shrink_count);\n@@ -660,0 +629,1 @@\n+  decrease_used(size_used);\n@@ -687,93 +657,0 @@\n-void G1CollectedHeap::populate_archive_regions_bot_part(MemRegion* ranges, size_t count) {\n-  assert(!is_init_completed(), \"Expect to be called at JVM init time\");\n-  assert(ranges != NULL, \"MemRegion array NULL\");\n-  assert(count != 0, \"No MemRegions provided\");\n-\n-  HeapWord* st = ranges[0].start();\n-  HeapWord* last = ranges[count-1].last();\n-  HeapRegion* hr_st = _hrm.addr_to_region(st);\n-  HeapRegion* hr_last = _hrm.addr_to_region(last);\n-\n-  HeapRegion* hr_curr = hr_st;\n-  while (hr_curr != NULL) {\n-    hr_curr->update_bot();\n-    if (hr_curr != hr_last) {\n-      hr_curr = _hrm.next_region_in_heap(hr_curr);\n-    } else {\n-      hr_curr = NULL;\n-    }\n-  }\n-}\n-\n-void G1CollectedHeap::dealloc_archive_regions(MemRegion* ranges, size_t count) {\n-  assert(!is_init_completed(), \"Expect to be called at JVM init time\");\n-  assert(ranges != NULL, \"MemRegion array NULL\");\n-  assert(count != 0, \"No MemRegions provided\");\n-  MemRegion reserved = _hrm.reserved();\n-  HeapWord* prev_last_addr = NULL;\n-  HeapRegion* prev_last_region = NULL;\n-  size_t size_used = 0;\n-  uint shrink_count = 0;\n-\n-  \/\/ For each Memregion, free the G1 regions that constitute it, and\n-  \/\/ notify mark-sweep that the range is no longer to be considered 'archive.'\n-  MutexLocker x(Heap_lock);\n-  for (size_t i = 0; i < count; i++) {\n-    HeapWord* start_address = ranges[i].start();\n-    HeapWord* last_address = ranges[i].last();\n-\n-    assert(reserved.contains(start_address) && reserved.contains(last_address),\n-           \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n-           p2i(start_address), p2i(last_address));\n-    assert(start_address > prev_last_addr,\n-           \"Ranges not in ascending order: \" PTR_FORMAT \" <= \" PTR_FORMAT ,\n-           p2i(start_address), p2i(prev_last_addr));\n-    size_used += ranges[i].byte_size();\n-    prev_last_addr = last_address;\n-\n-    HeapRegion* start_region = _hrm.addr_to_region(start_address);\n-    HeapRegion* last_region = _hrm.addr_to_region(last_address);\n-\n-    \/\/ Check for ranges that start in the same G1 region in which the previous\n-    \/\/ range ended, and adjust the start address so we don't try to free\n-    \/\/ the same region again. If the current range is entirely within that\n-    \/\/ region, skip it.\n-    if (start_region == prev_last_region) {\n-      start_address = start_region->end();\n-      if (start_address > last_address) {\n-        continue;\n-      }\n-      start_region = _hrm.addr_to_region(start_address);\n-    }\n-    prev_last_region = last_region;\n-\n-    \/\/ After verifying that each region was marked as an archive region by\n-    \/\/ alloc_archive_regions, set it free and empty and uncommit it.\n-    HeapRegion* curr_region = start_region;\n-    while (curr_region != NULL) {\n-      guarantee(curr_region->is_archive(),\n-                \"Expected archive region at index %u\", curr_region->hrm_index());\n-      uint curr_index = curr_region->hrm_index();\n-      _archive_set.remove(curr_region);\n-      curr_region->set_free();\n-      curr_region->set_top(curr_region->bottom());\n-      if (curr_region != last_region) {\n-        curr_region = _hrm.next_region_in_heap(curr_region);\n-      } else {\n-        curr_region = NULL;\n-      }\n-\n-      _hrm.shrink_at(curr_index, 1);\n-      shrink_count++;\n-    }\n-  }\n-\n-  if (shrink_count != 0) {\n-    log_debug(gc, ergo, heap)(\"Attempt heap shrinking (archive regions). Total size: \" SIZE_FORMAT \"B\",\n-                              HeapRegion::GrainWords * HeapWordSize * shrink_count);\n-    \/\/ Explicit uncommit.\n-    uncommit_regions(shrink_count);\n-  }\n-  decrease_used(size_used);\n-}\n-\n@@ -968,2 +845,1 @@\n-void G1CollectedHeap::verify_before_full_collection(bool explicit_gc) {\n-  assert(!GCCause::is_user_requested_gc(gc_cause()) || explicit_gc, \"invariant\");\n+void G1CollectedHeap::verify_before_full_collection() {\n@@ -974,0 +850,3 @@\n+  if (!G1HeapVerifier::should_verify(G1HeapVerifier::G1VerifyFull)) {\n+    return;\n+  }\n@@ -975,1 +854,1 @@\n-  _verifier->verify_before_gc(G1HeapVerifier::G1VerifyFull);\n+  _verifier->verify_before_gc();\n@@ -1013,0 +892,3 @@\n+  if (!G1HeapVerifier::should_verify(G1HeapVerifier::G1VerifyFull)) {\n+    return;\n+  }\n@@ -1015,1 +897,1 @@\n-  _verifier->verify_after_gc(G1HeapVerifier::G1VerifyFull);\n+  _verifier->verify_after_gc();\n@@ -1034,2 +916,1 @@\n-bool G1CollectedHeap::do_full_collection(bool explicit_gc,\n-                                         bool clear_all_soft_refs,\n+bool G1CollectedHeap::do_full_collection(bool clear_all_soft_refs,\n@@ -1049,1 +930,1 @@\n-  G1FullCollector collector(this, explicit_gc, do_clear_all_soft_refs, do_maximal_compaction, gc_mark.tracer());\n+  G1FullCollector collector(this, do_clear_all_soft_refs, do_maximal_compaction, gc_mark.tracer());\n@@ -1063,6 +944,3 @@\n-  \/\/ When clear_all_soft_refs is set we want to do a maximal compaction\n-  \/\/ not leaving any dead wood.\n-  bool do_maximal_compaction = clear_all_soft_refs;\n-  bool dummy = do_full_collection(true,                \/* explicit_gc *\/\n-                                  clear_all_soft_refs,\n-                                  do_maximal_compaction);\n+\n+  do_full_collection(clear_all_soft_refs,\n+                     false \/* do_maximal_compaction *\/);\n@@ -1074,2 +952,1 @@\n-  bool success = do_full_collection(false \/* explicit gc *\/,\n-                                    true  \/* clear_all_soft_refs *\/,\n+  bool success = do_full_collection(true  \/* clear_all_soft_refs *\/,\n@@ -1131,2 +1008,1 @@\n-    *gc_succeeded = do_full_collection(false, \/* explicit_gc *\/\n-                                       maximal_compaction \/* clear_all_soft_refs *\/ ,\n+    *gc_succeeded = do_full_collection(maximal_compaction \/* clear_all_soft_refs *\/ ,\n@@ -1233,16 +1109,7 @@\n-  if (expanded_by > 0) {\n-    size_t actual_expand_bytes = expanded_by * HeapRegion::GrainBytes;\n-    assert(actual_expand_bytes <= aligned_expand_bytes, \"post-condition\");\n-    policy()->record_new_heap_size(num_regions());\n-  } else {\n-    log_debug(gc, ergo, heap)(\"Did not expand the heap (heap expansion operation failed)\");\n-\n-    \/\/ The expansion of the virtual storage space was unsuccessful.\n-    \/\/ Let's see if it was because we ran out of swap.\n-    if (G1ExitOnExpansionFailure &&\n-        _hrm.available() >= regions_to_expand) {\n-      \/\/ We had head room...\n-      vm_exit_out_of_memory(aligned_expand_bytes, OOM_MMAP_ERROR, \"G1 heap expansion\");\n-    }\n-  }\n-  return expanded_by > 0;\n+  assert(expanded_by > 0, \"must have failed during commit.\");\n+\n+  size_t actual_expand_bytes = expanded_by * HeapRegion::GrainBytes;\n+  assert(actual_expand_bytes <= aligned_expand_bytes, \"post-condition\");\n+  policy()->record_new_heap_size(num_regions());\n+\n+  return true;\n@@ -1331,10 +1198,0 @@\n-class ArchiveRegionSetChecker : public HeapRegionSetChecker {\n-public:\n-  void check_mt_safety() {\n-    guarantee(!Universe::is_fully_initialized() || SafepointSynchronize::is_at_safepoint(),\n-              \"May only change archive regions during initialization or safepoint.\");\n-  }\n-  bool is_correct_type(HeapRegion* hr) { return hr->is_archive(); }\n-  const char* get_description() { return \"Archive Regions\"; }\n-};\n-\n@@ -1375,1 +1232,0 @@\n-  _archive_set(\"Archive Region Set\", new ArchiveRegionSetChecker()),\n@@ -1526,1 +1382,0 @@\n-  ct->initialize();\n@@ -1673,1 +1528,1 @@\n-  GCForwarding::initialize(heap_rs.region(), HeapRegion::LogOfHRGrainBytes - LogHeapWordSize);\n+  GCForwarding::initialize(heap_rs.region(), HeapRegion::GrainWords);\n@@ -2073,0 +1928,29 @@\n+bool G1CollectedHeap::try_collect_fullgc(GCCause::Cause cause,\n+                                         const G1GCCounters& counters_before) {\n+  assert_heap_not_locked();\n+\n+  while(true) {\n+    VM_G1CollectFull op(counters_before.total_collections(),\n+                        counters_before.total_full_collections(),\n+                        cause);\n+    VMThread::execute(&op);\n+\n+    \/\/ Request is trivially finished.\n+    if (!GCCause::is_explicit_full_gc(cause) || op.gc_succeeded()) {\n+      return op.gc_succeeded();\n+    }\n+\n+    {\n+      MutexLocker ml(Heap_lock);\n+      if (counters_before.total_full_collections() != total_full_collections()) {\n+        return true;\n+      }\n+    }\n+\n+    if (GCLocker::is_active_and_needs_gc()) {\n+      \/\/ If GCLocker is active, wait until clear before retrying.\n+      GCLocker::stall_until_clear();\n+    }\n+  }\n+}\n+\n@@ -2096,5 +1980,1 @@\n-    VM_G1CollectFull op(counters_before.total_collections(),\n-                        counters_before.total_full_collections(),\n-                        cause);\n-    VMThread::execute(&op);\n-    return op.gc_succeeded();\n+    return try_collect_fullgc(cause, counters_before);\n@@ -2282,4 +2162,0 @@\n-bool G1CollectedHeap::is_archived_object(oop object) const {\n-  return object != NULL && heap_region_containing(object)->is_archive();\n-}\n-\n@@ -2367,1 +2243,0 @@\n-               \"OA=open archive, CA=closed archive, \"\n@@ -2579,0 +2454,3 @@\n+  if (!G1HeapVerifier::should_verify(type)) {\n+    return;\n+  }\n@@ -2583,1 +2461,1 @@\n-  _verifier->verify_before_gc(type);\n+  _verifier->verify_before_gc();\n@@ -2592,0 +2470,3 @@\n+  if (!G1HeapVerifier::should_verify(type)) {\n+    return;\n+  }\n@@ -2593,1 +2474,1 @@\n-  _verifier->verify_after_gc(type);\n+  _verifier->verify_after_gc();\n@@ -2792,2 +2673,1 @@\n-                                               const uint archive_regions_removed,\n-  if (old_regions_removed > 0 || archive_regions_removed > 0 || humongous_regions_removed > 0) {\n+  if (old_regions_removed > 0 || humongous_regions_removed > 0) {\n@@ -2797,1 +2677,0 @@\n-    _archive_set.bulk_remove(archive_regions_removed);\n@@ -2888,3 +2767,1 @@\n-   if (hr->is_archive()) {\n-    _archive_set.remove(hr);\n-  } else if (hr->is_humongous()) {\n+   if (hr->is_humongous()) {\n@@ -2926,1 +2803,0 @@\n-  HeapRegionSet* _archive_set;\n@@ -2936,1 +2812,0 @@\n-                           HeapRegionSet* archive_set,\n@@ -2939,1 +2814,1 @@\n-    _free_list_only(free_list_only), _old_set(old_set), _archive_set(archive_set),\n+    _free_list_only(free_list_only), _old_set(old_set),\n@@ -2944,1 +2819,0 @@\n-      assert(_archive_set->is_empty(), \"pre-condition\");\n@@ -2960,2 +2834,0 @@\n-      } else if (r->is_archive()) {\n-        _archive_set->add(r);\n@@ -2964,1 +2836,1 @@\n-        \/\/ We now move all (non-humongous, non-old, non-archive) regions to old gen,\n+        \/\/ We now move all (non-humongous, non-old) regions to old gen,\n@@ -2989,1 +2861,1 @@\n-                              &_old_set, &_archive_set, &_humongous_set,\n+                              &_old_set, &_humongous_set,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":239,"deletions":367,"binary":false,"changes":606,"status":"modified"},{"patch":"@@ -44,1 +44,1 @@\n-#include \"gc\/shared\/preservedMarks.hpp\"\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -115,1 +115,0 @@\n-                                 bool explicit_gc,\n@@ -120,1 +119,1 @@\n-    _scope(heap->monitoring_support(), explicit_gc, clear_soft_refs, do_maximal_compaction, tracer),\n+    _scope(heap->monitoring_support(), clear_soft_refs, do_maximal_compaction, tracer),\n@@ -123,0 +122,1 @@\n+    _has_humongous(false),\n@@ -127,0 +127,1 @@\n+    _humongous_compaction_point(this),\n@@ -129,0 +130,1 @@\n+    _humongous_compaction_regions(8),\n@@ -159,0 +161,1 @@\n+\n@@ -183,1 +186,1 @@\n-  _heap->verify_before_full_collection(scope()->is_explicit_gc());\n+  _heap->verify_before_full_collection();\n@@ -253,0 +256,2 @@\n+\n+  _heap->print_heap_after_full_collection();\n@@ -258,2 +263,0 @@\n-  } else if (hr->is_closed_archive()) {\n-    _region_attr_table.set_skip_marking(hr->hrm_index());\n@@ -350,0 +353,6 @@\n+\n+    if (scope()->do_maximal_compaction() &&\n+        has_humongous() &&\n+        serial_compaction_point()->has_regions()) {\n+      phase2d_prepare_humongous_compaction();\n+    }\n@@ -370,1 +379,1 @@\n-  uint lowest_current = (uint)-1;\n+  uint lowest_current = UINT_MAX;\n@@ -425,0 +434,29 @@\n+void G1FullCollector::phase2d_prepare_humongous_compaction() {\n+  GCTraceTime(Debug, gc, phases) debug(\"Phase 2: Prepare humongous compaction\", scope()->timer());\n+  G1FullGCCompactionPoint* serial_cp = serial_compaction_point();\n+  assert(serial_cp->has_regions(), \"Sanity!\" );\n+\n+  uint last_serial_target = serial_cp->current_region()->hrm_index();\n+  uint region_index = last_serial_target + 1;\n+  uint max_reserved_regions = _heap->max_reserved_regions();\n+\n+  G1FullGCCompactionPoint* humongous_cp = humongous_compaction_point();\n+\n+  while (region_index < max_reserved_regions) {\n+    HeapRegion* hr = _heap->region_at_or_null(region_index);\n+\n+    if (hr == nullptr) {\n+      region_index++;\n+      continue;\n+    } else if (hr->is_starts_humongous()) {\n+      uint num_regions = humongous_cp->forward_humongous(hr);\n+      region_index += num_regions; \/\/ Skip over the continues humongous regions.\n+      continue;\n+    } else if (is_compaction_target(region_index)) {\n+      \/\/ Add the region to the humongous compaction point.\n+      humongous_cp->add(hr);\n+    }\n+    region_index++;\n+  }\n+}\n+\n@@ -443,0 +481,5 @@\n+\n+  if (!_humongous_compaction_regions.is_empty()) {\n+    assert(scope()->do_maximal_compaction(), \"Only compact humongous during maximal compaction\");\n+    task.humongous_compaction();\n+  }\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullCollector.cpp","additions":50,"deletions":7,"binary":false,"changes":57,"status":"modified"},{"patch":"@@ -46,10 +46,1 @@\n-    HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n-\n-    \/\/ copy object and reinit its mark\n-    HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n-    assert(obj_addr != destination, \"everything in this pass should be moving\");\n-    Copy::aligned_conjoint_words(obj_addr, destination, size);\n-\n-    \/\/ There is no need to transform stack chunks - marking already did that.\n-    cast_to_oop(destination)->init_mark();\n-    assert(cast_to_oop(destination)->klass() != NULL, \"should have a class\");\n+    G1FullGCCompactTask::copy_object_to_new_location(obj);\n@@ -64,0 +55,15 @@\n+void G1FullGCCompactTask::copy_object_to_new_location(oop obj) {\n+  assert(GCForwarding::is_forwarded(obj), \"Sanity!\");\n+  assert(GCForwarding::forwardee(obj) != obj, \"Object must have a new location\");\n+\n+  size_t size = obj->size();\n+  \/\/ Copy object and reinit its mark.\n+  HeapWord* obj_addr = cast_from_oop<HeapWord*>(obj);\n+  HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n+  Copy::aligned_conjoint_words(obj_addr, destination, size);\n+\n+  \/\/ There is no need to transform stack chunks - marking already did that.\n+  cast_to_oop(destination)->init_mark();\n+  assert(cast_to_oop(destination)->klass() != nullptr, \"should have a class\");\n+}\n+\n@@ -102,0 +108,46 @@\n+\n+void G1FullGCCompactTask::humongous_compaction() {\n+  GCTraceTime(Debug, gc, phases) tm(\"Phase 4: Humonguous Compaction\", collector()->scope()->timer());\n+\n+  for (HeapRegion* hr : collector()->humongous_compaction_regions()) {\n+    assert(collector()->is_compaction_target(hr->hrm_index()), \"Sanity\");\n+    compact_humongous_obj(hr);\n+  }\n+}\n+\n+void G1FullGCCompactTask::compact_humongous_obj(HeapRegion* src_hr) {\n+  assert(src_hr->is_starts_humongous(), \"Should be start region of the humongous object\");\n+\n+  oop obj = cast_to_oop(src_hr->bottom());\n+  size_t word_size = obj->size();\n+\n+  uint num_regions = (uint)G1CollectedHeap::humongous_obj_size_in_regions(word_size);\n+  HeapWord* destination = cast_from_oop<HeapWord*>(GCForwarding::forwardee(obj));\n+\n+  assert(collector()->mark_bitmap()->is_marked(obj), \"Should only compact marked objects\");\n+  collector()->mark_bitmap()->clear(obj);\n+\n+  copy_object_to_new_location(obj);\n+\n+  uint dest_start_idx = _g1h->addr_to_region(destination);\n+  \/\/ Update the metadata for the destination regions.\n+  _g1h->set_humongous_metadata(_g1h->region_at(dest_start_idx), num_regions, word_size, false);\n+\n+  \/\/ Free the source regions that do not overlap with the destination regions.\n+  uint src_start_idx = src_hr->hrm_index();\n+  free_non_overlapping_regions(src_start_idx, dest_start_idx, num_regions);\n+}\n+\n+void G1FullGCCompactTask::free_non_overlapping_regions(uint src_start_idx, uint dest_start_idx, uint num_regions) {\n+  uint dest_end_idx = dest_start_idx + num_regions -1;\n+  uint src_end_idx  = src_start_idx + num_regions - 1;\n+\n+  uint non_overlapping_start = dest_end_idx < src_start_idx ?\n+                               src_start_idx :\n+                               dest_end_idx + 1;\n+\n+  for (uint i = non_overlapping_start; i <= src_end_idx; ++i) {\n+    HeapRegion* hr = _g1h->region_at(i);\n+    _g1h->free_humongous_region(hr, nullptr);\n+  }\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactTask.cpp","additions":62,"deletions":10,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -30,0 +30,1 @@\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -134,0 +135,81 @@\n+\n+void G1FullGCCompactionPoint::add_humongous(HeapRegion* hr) {\n+  assert(hr->is_starts_humongous(), \"Sanity!\");\n+\n+  _collector->add_humongous_region(hr);\n+\n+  G1CollectedHeap* g1h = G1CollectedHeap::heap();\n+  g1h->humongous_obj_regions_iterate(hr,\n+                                     [&] (HeapRegion* r) {\n+                                       add(r);\n+                                       _collector->update_from_skip_compacting_to_compacting(r->hrm_index());\n+                                     });\n+}\n+\n+uint G1FullGCCompactionPoint::forward_humongous(HeapRegion* hr) {\n+  assert(hr->is_starts_humongous(), \"Sanity!\");\n+\n+  oop obj = cast_to_oop(hr->bottom());\n+  size_t obj_size = obj->size();\n+  uint num_regions = (uint)G1CollectedHeap::humongous_obj_size_in_regions(obj_size);\n+\n+  if (!has_regions()) {\n+    return num_regions;\n+  }\n+\n+  \/\/ Find contiguous compaction target regions for the humongous object.\n+  uint range_begin = find_contiguous_before(hr, num_regions);\n+\n+  if (range_begin == UINT_MAX) {\n+    \/\/ No contiguous compaction target regions found, so the object cannot be moved.\n+    return num_regions;\n+  }\n+\n+  \/\/ Preserve the mark for the humongous object as the region was initially not compacting.\n+  _collector->marker(0)->preserved_stack()->push_if_necessary(obj, obj->mark());\n+\n+  HeapRegion* dest_hr = _compaction_regions->at(range_begin);\n+  GCForwarding::forward_to(obj, cast_to_oop(dest_hr->bottom()));\n+  assert(GCForwarding::is_forwarded(obj), \"Object must be forwarded!\");\n+\n+  \/\/ Add the humongous object regions to the compaction point.\n+  add_humongous(hr);\n+\n+  \/\/ Remove covered regions from compaction target candidates.\n+  _compaction_regions->remove_range(range_begin, (range_begin + num_regions));\n+\n+  return num_regions;\n+}\n+\n+uint G1FullGCCompactionPoint::find_contiguous_before(HeapRegion* hr, uint num_regions) {\n+  assert(num_regions > 0, \"Sanity!\");\n+  assert(has_regions(), \"Sanity!\");\n+\n+  if (num_regions == 1) {\n+    \/\/ If only one region, return the first region.\n+    return 0;\n+  }\n+\n+  uint contiguous_region_count = 1;\n+\n+  uint range_end = 1;\n+  uint range_limit = (uint)_compaction_regions->length();\n+\n+  for (; range_end < range_limit; range_end++) {\n+    if (contiguous_region_count == num_regions) {\n+      break;\n+    }\n+    \/\/ Check if the current region and the previous region are contiguous.\n+    bool regions_are_contiguous = (_compaction_regions->at(range_end)->hrm_index() - _compaction_regions->at(range_end - 1)->hrm_index()) == 1;\n+    contiguous_region_count = regions_are_contiguous ? contiguous_region_count + 1 : 1;\n+  }\n+\n+  if (contiguous_region_count < num_regions &&\n+      hr->hrm_index() - _compaction_regions->at(range_end-1)->hrm_index() != 1) {\n+    \/\/ We reached the end but the final region is not contiguous with the target region;\n+    \/\/ no contiguous regions to move to.\n+    return UINT_MAX;\n+  }\n+  \/\/ Return the index of the first region in the range of contiguous regions.\n+  return range_end - contiguous_region_count;\n+}\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCCompactionPoint.cpp","additions":82,"deletions":0,"binary":false,"changes":82,"status":"modified"},{"patch":"@@ -82,1 +82,1 @@\n-  return _bitmap->is_marked(p) || _collector->is_skip_marking(p);\n+  return _bitmap->is_marked(p);\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCOopClosures.inline.hpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -36,7 +36,2 @@\n-template<bool is_humongous>\n-void G1DetermineCompactionQueueClosure::free_pinned_region(HeapRegion* hr) {\n-  if (is_humongous) {\n-    _g1h->free_humongous_region(hr, nullptr);\n-  } else {\n-    _g1h->free_region(hr, nullptr);\n-  }\n+void G1DetermineCompactionQueueClosure::free_empty_humongous_region(HeapRegion* hr) {\n+  _g1h->free_humongous_region(hr, nullptr);\n@@ -91,6 +86,3 @@\n-        free_pinned_region<true>(hr);\n-      }\n-    } else if (hr->is_open_archive()) {\n-      bool is_empty = _collector->live_words(hr->hrm_index()) == 0;\n-      if (is_empty) {\n-        free_pinned_region<false>(hr);\n+        free_empty_humongous_region(hr);\n+      } else {\n+        _collector->set_has_humongous();\n@@ -98,2 +90,0 @@\n-    } else if (hr->is_closed_archive()) {\n-      \/\/ nothing to do with closed archive region\n","filename":"src\/hotspot\/share\/gc\/g1\/g1FullGCPrepareTask.inline.hpp","additions":5,"deletions":15,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -61,1 +61,1 @@\n-                                           uint n_workers,\n+                                           uint num_workers,\n@@ -84,1 +84,1 @@\n-    _partial_array_stepper(n_workers),\n+    _partial_array_stepper(num_workers),\n@@ -86,1 +86,1 @@\n-    _num_optional_regions(optional_cset_length),\n+    _max_num_optional_regions(optional_cset_length),\n@@ -109,1 +109,1 @@\n-  _oops_into_optional_regions = new G1OopStarChunkedList[_num_optional_regions];\n+  _oops_into_optional_regions = new G1OopStarChunkedList[_max_num_optional_regions];\n@@ -576,1 +576,1 @@\n-  assert(worker_id < _n_workers, \"out of bounds access\");\n+  assert(worker_id < _num_workers, \"out of bounds access\");\n@@ -581,2 +581,4 @@\n-                               worker_id, _n_workers,\n-                               _young_cset_length, _optional_cset_length,\n+                               worker_id,\n+                               _num_workers,\n+                               _young_cset_length,\n+                               _optional_cset_length,\n@@ -596,1 +598,1 @@\n-  for (uint worker_id = 0; worker_id < _n_workers; ++worker_id) {\n+  for (uint worker_id = 0; worker_id < _num_workers; ++worker_id) {\n@@ -606,1 +608,1 @@\n-    size_t copied_bytes = pss->flush_stats(_surviving_young_words_total, _n_workers) * HeapWordSize;\n+    size_t copied_bytes = pss->flush_stats(_surviving_young_words_total, _num_workers) * HeapWordSize;\n@@ -613,1 +615,1 @@\n-    _states[worker_id] = NULL;\n+    _states[worker_id] = nullptr;\n@@ -619,1 +621,1 @@\n-  for (uint worker_index = 0; worker_index < _n_workers; ++worker_index) {\n+  for (uint worker_index = 0; worker_index < _num_workers; ++worker_index) {\n@@ -701,1 +703,1 @@\n-                                                 uint n_workers,\n+                                                 uint num_workers,\n@@ -708,1 +710,1 @@\n-    _states(NEW_C_HEAP_ARRAY(G1ParScanThreadState*, n_workers, mtGC)),\n+    _states(NEW_C_HEAP_ARRAY(G1ParScanThreadState*, num_workers, mtGC)),\n@@ -712,1 +714,1 @@\n-    _n_workers(n_workers),\n+    _num_workers(num_workers),\n@@ -715,2 +717,2 @@\n-  _preserved_marks_set.init(n_workers);\n-  for (uint i = 0; i < n_workers; ++i) {\n+  _preserved_marks_set.init(num_workers);\n+  for (uint i = 0; i < num_workers; ++i) {\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":18,"deletions":16,"binary":false,"changes":34,"status":"modified"},{"patch":"@@ -95,1 +95,2 @@\n-  size_t _num_optional_regions;\n+  \/\/ Maximum number of optional regions at start of gc.\n+  size_t _max_num_optional_regions;\n@@ -118,1 +119,1 @@\n-                       uint n_workers,\n+                       uint num_workers,\n@@ -240,1 +241,1 @@\n-  uint _n_workers;\n+  uint _num_workers;\n@@ -246,1 +247,1 @@\n-                          uint n_workers,\n+                          uint num_workers,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":5,"deletions":4,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -375,0 +375,2 @@\n+  _ref_processor = nullptr;\n+\n@@ -395,2 +397,6 @@\n-    eden_size = max_eden_size();\n-    survivor_size = (size - eden_size)\/2;\n+    \/\/ Need to reduce eden_size to satisfy the max constraint. The delta needs\n+    \/\/ to be 2*SpaceAlignment aligned so that both survivors are properly\n+    \/\/ aligned.\n+    uintx eden_delta = align_up(eden_size - max_eden_size(), 2*SpaceAlignment);\n+    eden_size     -= eden_delta;\n+    survivor_size += eden_delta\/2;\n@@ -618,0 +624,6 @@\n+void DefNewGeneration::ref_processor_init() {\n+  assert(_ref_processor == nullptr, \"a reference processor already exists\");\n+  assert(!_reserved.is_empty(), \"empty generation?\");\n+  _span_based_discoverer.set_span(_reserved);\n+  _ref_processor = new ReferenceProcessor(&_span_based_discoverer);    \/\/ a vanilla reference processor\n+}\n@@ -723,5 +735,0 @@\n-  _gc_timer->register_gc_start();\n-  _gc_tracer->report_gc_start(heap->gc_cause(), _gc_timer->gc_start());\n-\n-  _old_gen = heap->old_gen();\n-\n@@ -737,0 +744,5 @@\n+  _gc_timer->register_gc_start();\n+  _gc_tracer->report_gc_start(heap->gc_cause(), _gc_timer->gc_start());\n+  _ref_processor->start_discovery(clear_all_soft_refs);\n+\n+  _old_gen = heap->old_gen();\n@@ -1136,1 +1148,1 @@\n-CompactibleSpace* DefNewGeneration::first_compaction_space() const {\n+ContiguousSpace* DefNewGeneration::first_compaction_space() const {\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":20,"deletions":8,"binary":false,"changes":28,"status":"modified"},{"patch":"@@ -67,1 +67,1 @@\n-void GenMarkSweep::invoke_at_safepoint(ReferenceProcessor* rp, bool clear_all_softrefs) {\n+void GenMarkSweep::invoke_at_safepoint(bool clear_all_softrefs) {\n@@ -77,5 +77,0 @@\n-  \/\/ hook up weak ref data so it can be used during Mark-Sweep\n-  assert(ref_processor() == nullptr, \"no stomping\");\n-  assert(rp != nullptr, \"should be non-null\");\n-  set_ref_processor(rp);\n-\n@@ -122,16 +117,2 @@\n-  \/\/ If compaction completely evacuated the young generation then we\n-  \/\/ can clear the card table.  Otherwise, we must invalidate\n-  \/\/ it (consider all cards dirty).  In the future, we might consider doing\n-  \/\/ compaction within generations only, and doing card-table sliding.\n-  CardTableRS* rs = gch->rem_set();\n-  Generation* old_gen = gch->old_gen();\n-\n-  \/\/ Clear\/invalidate below make use of the \"prev_used_regions\" saved earlier.\n-  if (gch->young_gen()->used() == 0) {\n-    \/\/ We've evacuated the young generation.\n-    rs->clear_into_younger(old_gen);\n-  } else {\n-    \/\/ Invalidate the cards corresponding to the currently used\n-    \/\/ region and clear those corresponding to the evacuated region.\n-    rs->invalidate_or_clear(old_gen);\n-  }\n+  bool is_young_gen_empty = (gch->young_gen()->used() == 0);\n+  gch->rem_set()->maintain_old_to_young_invariant(gch->old_gen(), is_young_gen_empty);\n@@ -141,3 +122,0 @@\n-  \/\/ refs processing: clean slate\n-  set_ref_processor(nullptr);\n-\n@@ -193,0 +171,2 @@\n+  ref_processor()->start_discovery(clear_all_softrefs);\n+\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":5,"deletions":25,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -37,4 +37,0 @@\n-#include \"oops\/instanceClassLoaderKlass.inline.hpp\"\n-#include \"oops\/instanceKlass.inline.hpp\"\n-#include \"oops\/instanceMirrorKlass.inline.hpp\"\n-#include \"oops\/instanceRefKlass.inline.hpp\"\n@@ -45,1 +41,0 @@\n-#include \"utilities\/macros.hpp\"\n@@ -57,1 +52,0 @@\n-ReferenceProcessor*     MarkSweep::_ref_processor   = nullptr;\n@@ -61,0 +55,3 @@\n+AlwaysTrueClosure   MarkSweep::_always_true_closure;\n+ReferenceProcessor* MarkSweep::_ref_processor;\n+\n@@ -172,5 +169,0 @@\n-void MarkSweep::set_ref_processor(ReferenceProcessor* rp) {\n-  _ref_processor = rp;\n-  mark_and_push_closure.set_ref_discoverer(_ref_processor);\n-}\n-\n@@ -277,0 +269,5 @@\n+\n+  \/\/ The Full GC operates on the entire heap so all objects should be subject\n+  \/\/ to discovery, hence the _always_true_closure.\n+  MarkSweep::_ref_processor = new ReferenceProcessor(&_always_true_closure);\n+  mark_and_push_closure.set_ref_discoverer(_ref_processor);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":8,"deletions":11,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/referenceProcessor.hpp\"\n@@ -38,1 +39,0 @@\n-class ReferenceProcessor;\n@@ -107,1 +107,1 @@\n-  \/\/ Reference processing (used in ...follow_contents)\n+  static AlwaysTrueClosure               _always_true_closure;\n@@ -138,1 +138,0 @@\n-  static void set_ref_processor(ReferenceProcessor* rp);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -63,1 +63,1 @@\n-size_t CollectedHeap::_lab_alignment_reserve = ~(size_t)0;\n+size_t CollectedHeap::_lab_alignment_reserve = SIZE_MAX;\n@@ -155,0 +155,4 @@\n+bool CollectedHeap::contains_null(const oop* p) const {\n+  return *p == nullptr;\n+}\n+\n@@ -636,4 +640,0 @@\n-bool CollectedHeap::is_archived_object(oop object) const {\n-  return false;\n-}\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -316,1 +316,1 @@\n-    assert(_lab_alignment_reserve != ~(size_t)0, \"uninitialized\");\n+    assert(_lab_alignment_reserve != SIZE_MAX, \"uninitialized\");\n@@ -448,0 +448,6 @@\n+  \/\/ GCs are free to represent the bit representation for null differently in memory,\n+  \/\/ which is typically not observable when using the Access API. However, if for\n+  \/\/ some reason a context doesn't allow using the Access API, then this function\n+  \/\/ explicitly checks if the given memory location contains a null value.\n+  virtual bool contains_null(const oop* p) const;\n+\n@@ -509,3 +515,0 @@\n-  \/\/ Is the given object inside a CDS archive area?\n-  virtual bool is_archived_object(oop object) const;\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":7,"deletions":4,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -32,1 +32,2 @@\n-void GCForwarding::initialize(MemRegion heap) {\n+void GCForwarding::initialize(MemRegion heap, size_t region_size_words) {\n+#ifdef _LP64\n@@ -35,8 +36,1 @@\n-    _sliding_forwarding = new SlidingForwarding(heap);\n-  }\n-}\n-\n-void GCForwarding::initialize(MemRegion heap, size_t region_size_words_shift) {\n-  if (UseCompactObjectHeaders) {\n-    assert(_sliding_forwarding == nullptr, \"only call this once\");\n-    _sliding_forwarding = new SlidingForwarding(heap, region_size_words_shift);\n+    _sliding_forwarding = new SlidingForwarding(heap, region_size_words);\n@@ -44,0 +38,1 @@\n+#endif\n@@ -47,0 +42,1 @@\n+#ifdef _LP64\n@@ -49,1 +45,1 @@\n-    _sliding_forwarding->clear();\n+    _sliding_forwarding->begin();\n@@ -51,0 +47,1 @@\n+#endif\n@@ -54,0 +51,1 @@\n+#ifdef _LP64\n@@ -56,0 +54,1 @@\n+    _sliding_forwarding->end();\n@@ -57,0 +56,1 @@\n+#endif\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.cpp","additions":10,"deletions":10,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -39,1 +39,0 @@\n-  static void initialize(MemRegion heap);\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#ifdef _LP64\n@@ -44,1 +45,2 @@\n-  } else {\n+  } else\n+#endif\n@@ -46,1 +48,0 @@\n-  }\n@@ -50,0 +51,1 @@\n+#ifdef _LP64\n@@ -54,1 +56,2 @@\n-  } else {\n+  } else\n+#endif\n@@ -56,1 +59,0 @@\n-  }\n","filename":"src\/hotspot\/share\/gc\/shared\/gcForwarding.inline.hpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -121,0 +121,3 @@\n+  ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec->max_size());\n+  ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec->max_size());\n+\n@@ -122,1 +125,2 @@\n-  _rem_set->initialize();\n+  _rem_set->initialize(young_rs.base(), old_rs.base());\n+\n@@ -127,4 +131,0 @@\n-  ReservedSpace young_rs = heap_rs.first_part(_young_gen_spec->max_size());\n-  ReservedSpace old_rs = heap_rs.last_part(_young_gen_spec->max_size());\n-\n-  old_rs = old_rs.first_part(_old_gen_spec->max_size());\n@@ -136,1 +136,1 @@\n-  GCForwarding::initialize(_reserved);\n+  GCForwarding::initialize(_reserved, SpaceAlignment);\n@@ -195,1 +195,0 @@\n-  ref_processing_init();\n@@ -199,0 +198,2 @@\n+  def_new_gen->ref_processor_init();\n+\n@@ -208,5 +209,0 @@\n-void GenCollectedHeap::ref_processing_init() {\n-  _young_gen->ref_processor_init();\n-  _old_gen->ref_processor_init();\n-}\n-\n@@ -461,17 +457,0 @@\n-    \/\/ Note on ref discovery: For what appear to be historical reasons,\n-    \/\/ GCH enables and disabled (by enqueuing) refs discovery.\n-    \/\/ In the future this should be moved into the generation's\n-    \/\/ collect method so that ref discovery and enqueueing concerns\n-    \/\/ are local to a generation. The collect method could return\n-    \/\/ an appropriate indication in the case that notification on\n-    \/\/ the ref lock was needed. This will make the treatment of\n-    \/\/ weak refs more uniform (and indeed remove such concerns\n-    \/\/ from GCH). XXX\n-\n-    \/\/ We want to discover references, but not process them yet.\n-    \/\/ This mode is disabled in process_discovered_references if the\n-    \/\/ generation does some collection work, or in\n-    \/\/ enqueue_discovered_references if the generation returns\n-    \/\/ without doing any work.\n-    ReferenceProcessor* rp = gen->ref_processor();\n-    rp->start_discovery(clear_soft_refs);\n@@ -481,3 +460,0 @@\n-\n-    rp->disable_discovery();\n-    rp->verify_no_references_recorded();\n@@ -826,3 +802,22 @@\n-  VM_GenCollectFull op(gc_count_before, full_gc_count_before,\n-                       cause, max_generation);\n-  VMThread::execute(&op);\n+  while (true) {\n+    VM_GenCollectFull op(gc_count_before, full_gc_count_before,\n+                        cause, max_generation);\n+    VMThread::execute(&op);\n+\n+    if (!GCCause::is_explicit_full_gc(cause)) {\n+      return;\n+    }\n+\n+    {\n+      MutexLocker ml(Heap_lock);\n+      \/\/ Read the GC count while holding the Heap_lock\n+      if (full_gc_count_before != total_full_collections()) {\n+        return;\n+      }\n+    }\n+\n+    if (GCLocker::is_active_and_needs_gc()) {\n+      \/\/ If GCLocker is active, wait until clear before retrying.\n+      GCLocker::stall_until_clear();\n+    }\n+  }\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":30,"deletions":35,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -26,0 +27,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -27,0 +29,2 @@\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/powerOfTwo.hpp\"\n@@ -29,0 +33,1 @@\n+\n@@ -30,1 +35,0 @@\n-#endif\n@@ -32,6 +36,6 @@\n-SlidingForwarding::SlidingForwarding(MemRegion heap)\n-#ifdef _LP64\n-        : _heap_start(heap.start()),\n-          _num_regions(((heap.end() - heap.start()) >> NUM_COMPRESSED_BITS) + 1),\n-          _region_size_words_shift(NUM_COMPRESSED_BITS),\n-          _target_base_table(NEW_C_HEAP_ARRAY(HeapWord*, _num_regions * 2, mtGC)) {\n+SlidingForwarding::SlidingForwarding(MemRegion heap, size_t region_size_words)\n+  : _heap_start(heap.start()),\n+    _num_regions(((heap.end() - heap.start()) \/ region_size_words) + 1),\n+    _region_size_words_shift(log2i_exact(region_size_words)),\n+  _target_base_table(nullptr),\n+  _fallback_table(nullptr) {\n@@ -39,15 +43,7 @@\n-#else\n-  {\n-#endif\n-}\n-\n-SlidingForwarding::SlidingForwarding(MemRegion heap, size_t region_size_words_shift)\n-#ifdef _LP64\n-        : _heap_start(heap.start()),\n-          _num_regions(((heap.end() - heap.start()) >> region_size_words_shift) + 1),\n-          _region_size_words_shift(region_size_words_shift),\n-          _target_base_table(NEW_C_HEAP_ARRAY(HeapWord*, _num_regions * (ONE << NUM_REGION_BITS), mtGC)) {\n-  assert(region_size_words_shift <= NUM_COMPRESSED_BITS, \"regions must not be larger than maximum addressing bits allow\");\n-#else\n-  {\n-#endif\n+  size_t heap_size_words = heap.end() - heap.start();\n+  if (UseSerialGC && heap_size_words <= (1 << NUM_COMPRESSED_BITS)) {\n+    \/\/ In this case we can treat the whole heap as a single region and\n+    \/\/ make the encoding very simple.\n+    _num_regions = 1;\n+    _region_size_words_shift = log2i_exact(round_up_power_of_2(heap_size_words));\n+  }\n@@ -57,3 +53,6 @@\n-#ifdef _LP64\n-  FREE_C_HEAP_ARRAY(HeapWord*, _target_base_table);\n-#endif\n+  if (_target_base_table != nullptr) {\n+    FREE_C_HEAP_ARRAY(HeapWord*, _target_base_table);\n+  }\n+  if (_fallback_table != nullptr) {\n+    delete _fallback_table;\n+  }\n@@ -62,3 +61,4 @@\n-void SlidingForwarding::clear() {\n-#ifdef _LP64\n-  size_t max = _num_regions * (ONE << NUM_REGION_BITS);\n+void SlidingForwarding::begin() {\n+  assert(_target_base_table == nullptr, \"Should be uninitialized\");\n+  _target_base_table = NEW_C_HEAP_ARRAY(HeapWord*, _num_regions * NUM_TARGET_REGIONS, mtGC);\n+  size_t max = _num_regions * NUM_TARGET_REGIONS;\n@@ -68,1 +68,0 @@\n-#endif\n@@ -70,0 +69,82 @@\n+\n+void SlidingForwarding::end() {\n+  assert(_target_base_table != nullptr, \"Should be initialized\");\n+  FREE_C_HEAP_ARRAY(HeapWord*, _target_base_table);\n+  _target_base_table = nullptr;\n+\n+  if (_fallback_table != nullptr) {\n+    delete _fallback_table;\n+    _fallback_table = nullptr;\n+  }\n+}\n+\n+void SlidingForwarding::fallback_forward_to(HeapWord* from, HeapWord* to) {\n+  if (_fallback_table == nullptr) {\n+    _fallback_table = new FallbackTable();\n+  }\n+  _fallback_table->forward_to(from, to);\n+}\n+\n+HeapWord* SlidingForwarding::fallback_forwardee(HeapWord* from) const {\n+  if (_fallback_table == nullptr) {\n+    return nullptr;\n+  } else {\n+    return _fallback_table->forwardee(from);\n+  }\n+}\n+\n+FallbackTable::FallbackTable() {\n+  for (size_t i = 0; i < TABLE_SIZE; i++) {\n+    _table[i]._next = nullptr;\n+    _table[i]._from = nullptr;\n+    _table[i]._to   = nullptr;\n+  }\n+}\n+\n+FallbackTable::~FallbackTable() {\n+  for (size_t i = 0; i < TABLE_SIZE; i++) {\n+    FallbackTableEntry* entry = _table[i]._next;\n+    while (entry != nullptr) {\n+      FallbackTableEntry* next = entry->_next;\n+      FREE_C_HEAP_OBJ(entry);\n+      entry = next;\n+    }\n+  }\n+}\n+\n+size_t FallbackTable::home_index(HeapWord* from) {\n+  uint64_t val = reinterpret_cast<uint64_t>(from);\n+  val *= 0xbf58476d1ce4e5b9ull;\n+  val ^= val >> 56;\n+  val *= 0x94d049bb133111ebull;\n+  val = (val * 11400714819323198485llu) >> (64 - log2i_exact(TABLE_SIZE));\n+  assert(val < TABLE_SIZE, \"must fit in table: val: \" UINT64_FORMAT \", table-size: \" UINTX_FORMAT \", table-size-bits: %d\", val, TABLE_SIZE, log2i_exact(TABLE_SIZE));\n+  return static_cast<size_t>(val);\n+}\n+\n+void FallbackTable::forward_to(HeapWord* from, HeapWord* to) {\n+  size_t idx = home_index(from);\n+  if (_table[idx]._from != nullptr) {\n+    FallbackTableEntry* entry = NEW_C_HEAP_OBJ(FallbackTableEntry, mtGC);\n+    entry->_next = _table[idx]._next;\n+    entry->_from = _table[idx]._from;\n+    entry->_to = _table[idx]._to;\n+    _table[idx]._next = entry;\n+  }\n+  _table[idx]._from = from;\n+  _table[idx]._to   = to;\n+}\n+\n+HeapWord* FallbackTable::forwardee(HeapWord* from) const {\n+  size_t idx = home_index(from);\n+  const FallbackTableEntry* entry = &_table[idx];\n+  while (entry != nullptr) {\n+    if (entry->_from == from) {\n+      return entry->_to;\n+    }\n+    entry = entry->_next;\n+  }\n+  return nullptr;\n+}\n+\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.cpp","additions":110,"deletions":29,"binary":false,"changes":139,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -28,0 +29,2 @@\n+#ifdef _LP64\n+\n@@ -30,0 +33,1 @@\n+#include \"oops\/markWord.hpp\"\n@@ -32,0 +36,2 @@\n+class FallbackTable;\n+\n@@ -39,1 +45,1 @@\n- * We divide the heap into number of logical regions. Each region spans maximum of 2^NUM_BITS words.\n+ * We divide the heap into number of logical regions. Each region spans maximum of 2^NUM_COMPRESSED_BITS words.\n@@ -42,1 +48,2 @@\n- * into. We also currently require the two lowest header bits to indicate that the object is forwarded.\n+ * into. We also currently require the two lowest header bits to indicate that the object is forwarded. In addition to that,\n+ * we use 1 more bit to indicate that we should use a fallback-lookup-table instead of using the sliding encoding.\n@@ -49,2 +56,3 @@\n- * - Look-up first target base of region of orig. If not yet used,\n- *   establish it to be the base of region of target address. Use that base in step 3.\n+ * - Look-up first target base of region of orig. If it is already established and the region\n+ *   that 'target' is in, then use it in step 3. If not yet used, establish it to be the base of region of target\n+     address. Use that base in step 3.\n@@ -54,3 +62,3 @@\n- * - Now we found a base address. Encode the target address with that base into lowest NUM_BITS bits, and shift\n- *   that up by 3 bits. Set the 3rd bit if we used the secondary target base, otherwise leave it at 0. Set the\n- *   lowest two bits to indicate that the object has been forwarded. Store that in the lowest NUM_BITS+3 bits of the\n+ * - Now we found a base address. Encode the target address with that base into lowest NUM_COMPRESSED_BITS bits, and shift\n+ *   that up by 4 bits. Set the 3rd bit if we used the secondary target base, otherwise leave it at 0. Set the\n+ *   lowest two bits to indicate that the object has been forwarded. Store that in the lowest 32 bits of the\n@@ -60,1 +68,1 @@\n- * - Load lowest NUM_BITS + 3 from original object header. Extract target region bit and compressed address bits.\n+ * - Load lowest 32 from original object header. Extract target region bit and compressed address bits.\n@@ -64,0 +72,7 @@\n+ *\n+ * One complication is that G1 serial compaction breaks the assumption that we only forward\n+ * to two target regions. When that happens, we initialize a fallback-hashtable for storing those extra\n+ * forwardings, and set the 4th bit in the header to indicate that the forwardee is not encoded but\n+ * should be looked-up in the hashtable. G1 serial compaction is not very common -  it is the last-last-ditch\n+ * GC that is used when the JVM is scrambling to squeeze more space out of the heap, and at that\n+ * point, ultimate performance is no longer the main concern.\n@@ -65,1 +80,0 @@\n-\n@@ -67,1 +81,0 @@\n-#ifdef _LP64\n@@ -69,8 +82,11 @@\n-  static const int NUM_REGION_BITS = 1;\n-\n-  static const uintptr_t ONE = 1ULL;\n-\n-  static const size_t NUM_REGIONS = ONE << NUM_REGION_BITS;\n-\n-  \/\/ We need the lowest three bits to indicate a forwarded object and self-forwarding.\n-  static const int BASE_SHIFT = 3;\n+  static const uintptr_t MARK_LOWER_HALF_MASK = 0xffffffff;\n+\n+  \/\/ We need the lowest two bits to indicate a forwarded object.\n+  \/\/ The 3rd bit (fallback-bit) indicates that the forwardee should be\n+  \/\/ looked-up in a fallback-table.\n+  static const int FALLBACK_SHIFT = markWord::lock_bits;\n+  static const int FALLBACK_BITS = 1;\n+  static const int FALLBACK_MASK = right_n_bits(FALLBACK_BITS) << FALLBACK_SHIFT;\n+  \/\/ The 4th bit selects the target region.\n+  static const int REGION_SHIFT = FALLBACK_SHIFT + FALLBACK_BITS;\n+  static const int REGION_BITS = 1;\n@@ -79,1 +95,1 @@\n-  static const int COMPRESSED_BITS_SHIFT = BASE_SHIFT + NUM_REGION_BITS;\n+  static const int COMPRESSED_BITS_SHIFT = REGION_SHIFT + REGION_BITS;\n@@ -81,3 +97,4 @@\n-  \/\/ How many bits we use for the compressed pointer (we are going to need one more bit to indicate target region, and\n-  \/\/ two lowest bits to mark objects as forwarded)\n-  static const int NUM_COMPRESSED_BITS = 32 - BASE_SHIFT - NUM_REGION_BITS;\n+  \/\/ How many bits we use for the compressed pointer\n+  static const int NUM_COMPRESSED_BITS = 32 - COMPRESSED_BITS_SHIFT;\n+\n+  static const size_t NUM_TARGET_REGIONS = 1 << REGION_BITS;\n@@ -90,3 +107,5 @@\n-  size_t     const _num_regions;\n-  size_t     const _region_size_words_shift;\n-  HeapWord** const _target_base_table;\n+  size_t           _num_regions;\n+  size_t           _region_size_words_shift;\n+  HeapWord**       _target_base_table;\n+\n+  FallbackTable* _fallback_table;\n@@ -100,1 +119,2 @@\n-#endif\n+  void fallback_forward_to(HeapWord* from, HeapWord* to);\n+  HeapWord* fallback_forwardee(HeapWord* from) const;\n@@ -103,2 +123,1 @@\n-  SlidingForwarding(MemRegion heap);\n-  SlidingForwarding(MemRegion heap, size_t num_regions);\n+  SlidingForwarding(MemRegion heap, size_t region_size_words);\n@@ -107,1 +126,3 @@\n-  void clear();\n+  void begin();\n+  void end();\n+\n@@ -112,0 +133,34 @@\n+\/*\n+ * A simple hash-table that acts as fallback for the sliding forwarding.\n+ * This is used in the case of G1 serial compactio, which violates the\n+ * assumption of sliding forwarding that each object of any region is only\n+ * ever forwarded to one of two target regions. At this point, the GC is\n+ * scrambling to free up more Java heap memory, and therefore performance\n+ * is not the major concern.\n+ *\n+ * The implementation is a straightforward open hashtable.\n+ * It is a single-threaded (not thread-safe) implementation, and that\n+ * is sufficient because G1 serial compaction is single-threaded.\n+ *\/\n+class FallbackTable : public CHeapObj<mtGC>{\n+private:\n+  struct FallbackTableEntry {\n+    FallbackTableEntry* _next;\n+    HeapWord* _from;\n+    HeapWord* _to;\n+  };\n+\n+  static const size_t TABLE_SIZE = 128;\n+  FallbackTableEntry _table[TABLE_SIZE];\n+\n+  static size_t home_index(HeapWord* from);\n+\n+public:\n+  FallbackTable();\n+  ~FallbackTable();\n+\n+  void forward_to(HeapWord* from, HeapWord* to);\n+  HeapWord* forwardee(HeapWord* from) const;\n+};\n+\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.hpp","additions":84,"deletions":29,"binary":false,"changes":113,"status":"modified"},{"patch":"@@ -3,0 +3,1 @@\n+ * Copyright Amazon.com Inc. or its affiliates. All Rights Reserved.\n@@ -27,0 +28,2 @@\n+#ifdef _LP64\n+\n@@ -30,0 +33,1 @@\n+#include \"utilities\/macros.hpp\"\n@@ -31,1 +35,0 @@\n-#ifdef _LP64\n@@ -40,1 +43,1 @@\n-  return uintptr_t(addr - region_base) < (ONE << _region_size_words_shift);\n+  return uintptr_t(addr - region_base) < (1ull << _region_size_words_shift);\n@@ -50,1 +53,1 @@\n-  for (region_idx = 0; region_idx < NUM_REGIONS; region_idx++) {\n+  for (region_idx = 0; region_idx < NUM_TARGET_REGIONS; region_idx++) {\n@@ -53,1 +56,1 @@\n-      encode_base = _heap_start + target_idx * (ONE << _region_size_words_shift);\n+      encode_base = _heap_start + target_idx * (1ull << _region_size_words_shift);\n@@ -60,5 +63,3 @@\n-  if (region_idx >= NUM_REGIONS) {\n-    tty->print_cr(\"target: \" PTR_FORMAT, p2i(target));\n-    for (region_idx = 0; region_idx < NUM_REGIONS; region_idx++) {\n-      tty->print_cr(\"region_idx: \" INTPTR_FORMAT \", encode_base: \" PTR_FORMAT, region_idx, p2i(_target_base_table[base_table_idx + region_idx]));\n-    }\n+  if (region_idx >= NUM_TARGET_REGIONS) {\n+    assert(G1GC_ONLY(UseG1GC) NOT_G1GC(false), \"Only happens with G1 serial compaction\");\n+    return 1 << FALLBACK_SHIFT | markWord::marked_value;\n@@ -66,1 +67,1 @@\n-  assert(region_idx < NUM_REGIONS, \"need to have found an encoding base\");\n+  assert(region_idx < NUM_TARGET_REGIONS, \"need to have found an encoding base\");\n@@ -71,1 +72,1 @@\n-                      (region_idx << BASE_SHIFT) | markWord::marked_value;\n+                      (region_idx << REGION_SHIFT) | markWord::marked_value;\n@@ -79,1 +80,1 @@\n-  size_t region_idx = (encoded >> BASE_SHIFT) & right_n_bits(NUM_REGION_BITS);\n+  size_t region_idx = (encoded >> REGION_SHIFT) & right_n_bits(REGION_BITS);\n@@ -85,1 +86,0 @@\n-#endif\n@@ -88,1 +88,1 @@\n-#ifdef _LP64\n+  assert(_target_base_table != nullptr, \"call begin() before forwarding\");\n@@ -93,3 +93,4 @@\n-  uintptr_t encoded = encode_forwarding(cast_from_oop<HeapWord*>(original), cast_from_oop<HeapWord*>(target));\n-  assert((encoded & markWord::klass_mask_in_place) == 0, \"encoded forwardee must not overlap with Klass*: \" PTR_FORMAT, encoded);\n-  header = markWord((header.value() & markWord::klass_mask_in_place) | encoded);\n+  HeapWord* from = cast_from_oop<HeapWord*>(original);\n+  HeapWord* to   = cast_from_oop<HeapWord*>(target);\n+  uintptr_t encoded = encode_forwarding(from, to);\n+  header = markWord((header.value() & ~MARK_LOWER_HALF_MASK) | encoded);\n@@ -97,3 +98,4 @@\n-#else\n-  original->forward_to(target);\n-#endif\n+  if ((encoded & FALLBACK_MASK) != 0) {\n+    fallback_forward_to(from, to);\n+    return;\n+  }\n@@ -103,1 +105,1 @@\n-#ifdef _LP64\n+  assert(_target_base_table != nullptr, \"call begin() before forwarding\");\n@@ -105,1 +107,6 @@\n-  uintptr_t encoded = header.value() & ~markWord::klass_mask_in_place;\n+  if ((header.value() & FALLBACK_MASK) != 0) {\n+    HeapWord* from = cast_from_oop<HeapWord*>(original);\n+    HeapWord* to = fallback_forwardee(from);\n+    return cast_to_oop(to);\n+  }\n+  uintptr_t encoded = header.value() & MARK_LOWER_HALF_MASK;\n@@ -108,3 +115,0 @@\n-#else\n-  return original->forwardee();\n-#endif\n@@ -113,0 +117,1 @@\n+#endif \/\/ _LP64\n","filename":"src\/hotspot\/share\/gc\/shared\/slidingForwarding.inline.hpp","additions":30,"deletions":25,"binary":false,"changes":55,"status":"modified"},{"patch":"@@ -171,1 +171,4 @@\n-ContiguousSpace::ContiguousSpace(): CompactibleSpace(), _top(nullptr) {\n+ContiguousSpace::ContiguousSpace(): Space(),\n+  _compaction_top(nullptr),\n+  _next_compaction_space(nullptr),\n+  _top(nullptr) {\n@@ -183,1 +186,3 @@\n-  CompactibleSpace::initialize(mr, clear_space, mangle_space);\n+  Space::initialize(mr, clear_space, mangle_space);\n+  set_compaction_top(bottom());\n+  _next_compaction_space = nullptr;\n@@ -189,1 +194,2 @@\n-  CompactibleSpace::clear(mangle_space);\n+  Space::clear(mangle_space);\n+  _compaction_top = bottom();\n@@ -242,13 +248,1 @@\n-void CompactibleSpace::initialize(MemRegion mr,\n-                                  bool clear_space,\n-                                  bool mangle_space) {\n-  Space::initialize(mr, clear_space, mangle_space);\n-  set_compaction_top(bottom());\n-  _next_compaction_space = nullptr;\n-}\n-\n-void CompactibleSpace::clear(bool mangle_space) {\n-  Space::clear(mangle_space);\n-  _compaction_top = bottom();\n-}\n-HeapWord* CompactibleSpace::forward(oop q, size_t size,\n+HeapWord* ContiguousSpace::forward(oop q, size_t size,\n@@ -379,1 +373,1 @@\n-void CompactibleSpace::adjust_pointers() {\n+void ContiguousSpace::adjust_pointers() {\n@@ -416,1 +410,1 @@\n-void CompactibleSpace::compact() {\n+void ContiguousSpace::compact() {\n","filename":"src\/hotspot\/share\/gc\/shared\/space.cpp","additions":12,"deletions":18,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -1747,14 +1747,2 @@\n-  while (true) {\n-    jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);\n-    if (prev == CANCELLABLE) return true;\n-    else if (prev == CANCELLED) return false;\n-    assert(ShenandoahSuspendibleWorkers, \"should not get here when not using suspendible workers\");\n-    assert(prev == NOT_CANCELLED, \"must be NOT_CANCELLED\");\n-    Thread* thread = Thread::current();\n-    if (thread->is_Java_thread()) {\n-      \/\/ We need to provide a safepoint here, otherwise we might\n-      \/\/ spin forever if a SP is pending.\n-      ThreadBlockInVM sp(JavaThread::cast(thread));\n-      SpinPause();\n-    }\n-  }\n+  jbyte prev = _cancelled_gc.cmpxchg(CANCELLED, CANCELLABLE);\n+  return prev == CANCELLABLE;\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":2,"deletions":14,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -248,6 +248,1 @@\n-  if (! (sts_active && ShenandoahSuspendibleWorkers)) {\n-    return cancelled_gc();\n-  }\n-\n-  jbyte prev = _cancelled_gc.cmpxchg(NOT_CANCELLED, CANCELLABLE);\n-  if (prev == CANCELLABLE || prev == NOT_CANCELLED) {\n+  if (sts_active && ShenandoahSuspendibleWorkers && !cancelled_gc()) {\n@@ -257,9 +252,1 @@\n-\n-    \/\/ Back to CANCELLABLE. The thread that poked NOT_CANCELLED first gets\n-    \/\/ to restore to CANCELLABLE.\n-    if (prev == CANCELLABLE) {\n-      _cancelled_gc.set(CANCELLABLE);\n-    }\n-    return false;\n-  } else {\n-    return true;\n+  return cancelled_gc();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.inline.hpp","additions":2,"deletions":15,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -959,2 +959,1 @@\n-  ConstantPoolCacheEntry* cp_cache_entry = pool->invokedynamic_cp_cache_entry_at(index);\n-  cp_cache_entry->set_dynamic_call(pool, info);\n+  pool->cache()->set_dynamic_call(info, pool->decode_invokedynamic_index(index));\n@@ -1167,1 +1166,1 @@\n-  if ((ik->field_access_flags(index) & JVM_ACC_FIELD_ACCESS_WATCHED) == 0) return;\n+  if (!ik->field_status(index).is_access_watched()) return;\n@@ -1192,1 +1191,1 @@\n-  if ((ik->field_access_flags(index) & JVM_ACC_FIELD_MODIFICATION_WATCHED) == 0) return;\n+  if (!ik->field_status(index).is_modification_watched()) return;\n","filename":"src\/hotspot\/share\/interpreter\/interpreterRuntime.cpp","additions":3,"deletions":4,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -2252,8 +2252,3 @@\n-\n-        u4 index = Bytes::get_native_u4(pc+1);\n-        ConstantPoolCacheEntry* cache = cp->constant_pool()->invokedynamic_cp_cache_entry_at(index);\n-\n-        \/\/ We are resolved if the resolved_references array contains a non-null object (CallSite, etc.)\n-        \/\/ This kind of CP cache entry does not need to match the flags byte, because\n-        \/\/ there is a 1-1 relation between bytecode type and CP entry type.\n-        if (! cache->is_resolved((Bytecodes::Code) opcode)) {\n+        u4 index = cp->constant_pool()->decode_invokedynamic_index(Bytes::get_native_u4(pc+1)); \/\/ index is originally negative\n+        ResolvedIndyEntry* indy_info = cp->resolved_indy_entry_at(index);\n+        if (!indy_info->is_resolved()) {\n@@ -2262,1 +2257,1 @@\n-          cache = cp->constant_pool()->invokedynamic_cp_cache_entry_at(index);\n+          indy_info = cp->resolved_indy_entry_at(index); \/\/ get resolved entry\n@@ -2264,2 +2259,1 @@\n-\n-        Method* method = cache->f1_as_method();\n+        Method* method = indy_info->method();\n@@ -2268,1 +2262,1 @@\n-        if (cache->has_appendix()) {\n+        if (indy_info->has_appendix()) {\n@@ -2270,1 +2264,1 @@\n-          SET_STACK_OBJECT(cache->appendix_if_resolved(cp), 0);\n+          SET_STACK_OBJECT(cp->resolved_reference_from_indy(index), 0);\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":7,"deletions":13,"binary":false,"changes":20,"status":"modified"},{"patch":"@@ -453,6 +453,1 @@\n-  JVMCIKlassHandle handle(THREAD);\n-  {\n-    \/\/ Need Compile_lock around implementor()\n-    MutexLocker locker(Compile_lock);\n-    handle = iklass->implementor();\n-  }\n+  JVMCIKlassHandle handle(THREAD, iklass->implementor());\n@@ -579,0 +574,19 @@\n+C2V_VMENTRY_NULL(jobject, lookupJClass, (JNIEnv* env, jobject, jlong jclass_value))\n+    if (jclass_value == 0L) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"jclass must not be zero\");\n+    }\n+    jclass mirror = reinterpret_cast<jclass>(jclass_value);\n+    \/\/ Since the jclass_value is passed as a jlong, we perform additional checks to prevent the caller from accidentally\n+    \/\/ sending a value that is not a JNI handle.\n+    if (JNIHandles::handle_type(thread, mirror) == JNIInvalidRefType) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"jclass is not a valid JNI reference\");\n+    }\n+    oop obj = JNIHandles::resolve(mirror);\n+    if (!java_lang_Class::is_instance(obj)) {\n+        JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"jclass must be a reference to the Class object\");\n+    }\n+    JVMCIKlassHandle klass(THREAD, java_lang_Class::as_Klass(obj));\n+    JVMCIObject result = JVMCIENV->get_jvmci_type(klass, JVMCI_CHECK_NULL);\n+    return JVMCIENV->get_jobject(result);\n+C2V_END\n+\n@@ -636,0 +650,7 @@\n+  \/\/ Get the indy entry based on CP index\n+  int indy_index = -1;\n+  for (int i = 0; i < cp->resolved_indy_entries_length(); i++) {\n+    if (cp->resolved_indy_entry_at(i)->constant_pool_index() == index) {\n+      indy_index = i;\n+    }\n+  }\n@@ -637,1 +658,1 @@\n-  BootstrapInfo bootstrap_specifier(cp, index);\n+  BootstrapInfo bootstrap_specifier(cp, index, indy_index);\n@@ -794,2 +815,2 @@\n-  if (info.is_null() || JVMCIENV->get_length(info) != 3) {\n-    JVMCI_ERROR_NULL(\"info must not be null and have a length of 3\");\n+  if (info.is_null() || JVMCIENV->get_length(info) != 4) {\n+    JVMCI_ERROR_NULL(\"info must not be null and have a length of 4\");\n@@ -800,0 +821,1 @@\n+  JVMCIENV->put_int_at(info, 3, fd.field_flags().as_uint());\n@@ -1461,1 +1483,5 @@\n-C2V_VMENTRY(void, resolveInvokeDynamicInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+C2V_VMENTRY_0(int, resolveInvokeDynamicInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+  if (!ConstantPool::is_invokedynamic_index(index)) {\n+    JVMCI_THROW_MSG_0(IllegalStateException, err_msg(\"not an invokedynamic index %d\", index));\n+  }\n+\n@@ -1464,3 +1490,4 @@\n-  LinkResolver::resolve_invoke(callInfo, Handle(), cp, index, Bytecodes::_invokedynamic, CHECK);\n-  ConstantPoolCacheEntry* cp_cache_entry = cp->invokedynamic_cp_cache_entry_at(index);\n-  cp_cache_entry->set_dynamic_call(cp, callInfo);\n+  LinkResolver::resolve_invoke(callInfo, Handle(), cp, index, Bytecodes::_invokedynamic, CHECK_0);\n+  int indy_index = cp->decode_invokedynamic_index(index);\n+  cp->cache()->set_dynamic_call(callInfo, indy_index);\n+  return cp->resolved_indy_entry_at(indy_index)->constant_pool_index();\n@@ -1515,2 +1542,4 @@\n-  if (cp_cache_entry->is_resolved(Bytecodes::_invokedynamic)) {\n-    return Bytecodes::_invokedynamic;\n+  if (cp->is_invokedynamic_index(index)) {\n+    if (cp->resolved_indy_entry_at(cp->decode_cpcache_index(index))->is_resolved()) {\n+      return Bytecodes::_invokedynamic;\n+    }\n@@ -1947,0 +1976,19 @@\n+C2V_VMENTRY_NULL(jobjectArray, getDeclaredFieldsInfo, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass)))\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  if (klass == nullptr) {\n+    JVMCI_THROW_0(NullPointerException);\n+  }\n+  if (!klass->is_instance_klass()) {\n+    JVMCI_THROW_MSG_NULL(IllegalArgumentException, \"not an InstanceKlass\");\n+  }\n+  InstanceKlass* iklass = InstanceKlass::cast(klass);\n+  int java_fields, injected_fields;\n+  GrowableArray<FieldInfo>* fields = FieldInfoStream::create_FieldInfoArray(iklass->fieldinfo_stream(), &java_fields, &injected_fields);\n+  JVMCIObjectArray array = JVMCIENV->new_FieldInfo_array(fields->length(), JVMCIENV);\n+  for (int i = 0; i < fields->length(); i++) {\n+    JVMCIObject field_info = JVMCIENV->new_FieldInfo(fields->adr_at(i), JVMCI_CHECK_NULL);\n+    JVMCIENV->put_object_at(array, i, field_info);\n+  }\n+  return array.as_jobject();\n+C2V_END\n+\n@@ -2005,4 +2053,4 @@\n-    case T_BOOLEAN: value = obj->bool_field_acquire(displacement);  break;\n-    case T_BYTE:    value = obj->byte_field_acquire(displacement);  break;\n-    case T_SHORT:   value = obj->short_field_acquire(displacement); break;\n-    case T_CHAR:    value = obj->char_field_acquire(displacement);  break;\n+    case T_BOOLEAN: value = HeapAccess<MO_SEQ_CST>::load(obj->field_addr<jboolean>(displacement)); break;\n+    case T_BYTE:    value = HeapAccess<MO_SEQ_CST>::load(obj->field_addr<jbyte>(displacement));    break;\n+    case T_SHORT:   value = HeapAccess<MO_SEQ_CST>::load(obj->field_addr<jshort>(displacement));   break;\n+    case T_CHAR:    value = HeapAccess<MO_SEQ_CST>::load(obj->field_addr<jchar>(displacement));    break;\n@@ -2010,1 +2058,1 @@\n-    case T_INT:     value = obj->int_field_acquire(displacement);   break;\n+    case T_INT:     value = HeapAccess<MO_SEQ_CST>::load(obj->field_addr<jint>(displacement));     break;\n@@ -2012,1 +2060,1 @@\n-    case T_LONG:    value = obj->long_field_acquire(displacement);  break;\n+    case T_LONG:    value = HeapAccess<MO_SEQ_CST>::load(obj->field_addr<jlong>(displacement));    break;\n@@ -2022,1 +2070,3 @@\n-      oop value = obj->obj_field_acquire(displacement);\n+      \/\/ Perform the read including any barriers required to make the reference strongly reachable\n+      \/\/ since it will be wrapped as a JavaConstant.\n+      oop value = obj->obj_field_access<MO_SEQ_CST | ON_UNKNOWN_OOP_REF>(displacement);\n@@ -2625,3 +2675,1 @@\n-C2V_VMENTRY_NULL(jobject, asReflectionField, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), jint index))\n-  requireInHotSpot(\"asReflectionField\", JVMCI_CHECK_NULL);\n-  Klass* klass = UNPACK_PAIR(Klass, klass);\n+static InstanceKlass* check_field(Klass* klass, jint index, JVMCI_TRAPS) {\n@@ -2633,2 +2681,1 @@\n-  Array<u2>* fields = iklass->fields();\n-  if (index < 0 ||index > fields->length()) {\n+  if (index < 0 || index > iklass->total_fields_count()) {\n@@ -2638,0 +2685,7 @@\n+  return iklass;\n+}\n+\n+C2V_VMENTRY_NULL(jobject, asReflectionField, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), jint index))\n+  requireInHotSpot(\"asReflectionField\", JVMCI_CHECK_NULL);\n+  Klass* klass = UNPACK_PAIR(Klass, klass);\n+  InstanceKlass* iklass = check_field(klass, index, JVMCIENV);\n@@ -2643,0 +2697,82 @@\n+static jbyteArray get_encoded_annotation_data(InstanceKlass* holder, AnnotationArray* annotations_array, bool for_class,\n+                                              jint filter_length, jlong filter_klass_pointers,\n+                                              JavaThread* THREAD, JVMCIEnv* JVMCIENV) {\n+  \/\/ Get a ConstantPool object for annotation parsing\n+  Handle jcp = reflect_ConstantPool::create(CHECK_NULL);\n+  reflect_ConstantPool::set_cp(jcp(), holder->constants());\n+\n+  \/\/ load VMSupport\n+  Symbol* klass = vmSymbols::jdk_internal_vm_VMSupport();\n+  Klass* k = SystemDictionary::resolve_or_fail(klass, true, CHECK_NULL);\n+\n+  InstanceKlass* vm_support = InstanceKlass::cast(k);\n+  if (vm_support->should_be_initialized()) {\n+    vm_support->initialize(CHECK_NULL);\n+  }\n+\n+  typeArrayOop annotations_oop = Annotations::make_java_array(annotations_array, CHECK_NULL);\n+  typeArrayHandle annotations = typeArrayHandle(THREAD, annotations_oop);\n+\n+  InstanceKlass** filter = filter_length == 1 ?\n+      (InstanceKlass**) &filter_klass_pointers:\n+      (InstanceKlass**) filter_klass_pointers;\n+  objArrayOop filter_oop = oopFactory::new_objArray(vmClasses::Class_klass(), filter_length, CHECK_NULL);\n+  objArrayHandle filter_classes(THREAD, filter_oop);\n+  for (int i = 0; i < filter_length; i++) {\n+    filter_classes->obj_at_put(i, filter[i]->java_mirror());\n+  }\n+\n+  \/\/ invoke VMSupport.encodeAnnotations\n+  JavaValue result(T_OBJECT);\n+  JavaCallArguments args;\n+  args.push_oop(annotations);\n+  args.push_oop(Handle(THREAD, holder->java_mirror()));\n+  args.push_oop(jcp);\n+  args.push_int(for_class);\n+  args.push_oop(filter_classes);\n+  Symbol* signature = vmSymbols::encodeAnnotations_signature();\n+  JavaCalls::call_static(&result,\n+                         vm_support,\n+                         vmSymbols::encodeAnnotations_name(),\n+                         signature,\n+                         &args,\n+                         CHECK_NULL);\n+\n+  oop res = result.get_oop();\n+  if (JVMCIENV->is_hotspot()) {\n+    return (jbyteArray) JNIHandles::make_local(THREAD, res);\n+  }\n+\n+  typeArrayOop ba = typeArrayOop(res);\n+  int ba_len = ba->length();\n+  jbyte* ba_buf = NEW_RESOURCE_ARRAY_IN_THREAD_RETURN_NULL(THREAD, jbyte, ba_len);\n+  if (ba_buf == nullptr) {\n+    JVMCI_THROW_MSG_NULL(InternalError,\n+              err_msg(\"could not allocate %d bytes\", ba_len));\n+\n+  }\n+  memcpy(ba_buf, ba->byte_at_addr(0), ba_len);\n+  JVMCIPrimitiveArray ba_dest = JVMCIENV->new_byteArray(ba_len, JVMCI_CHECK_NULL);\n+  JVMCIENV->copy_bytes_from(ba_buf, ba_dest, 0, ba_len);\n+  return JVMCIENV->get_jbyteArray(ba_dest);\n+}\n+\n+C2V_VMENTRY_NULL(jbyteArray, getEncodedClassAnnotationData, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass),\n+                 jobject filter, jint filter_length, jlong filter_klass_pointers))\n+  InstanceKlass* holder = InstanceKlass::cast(UNPACK_PAIR(Klass, klass));\n+  return get_encoded_annotation_data(holder, holder->class_annotations(), true, filter_length, filter_klass_pointers, THREAD, JVMCIENV);\n+C2V_END\n+\n+C2V_VMENTRY_NULL(jbyteArray, getEncodedExecutableAnnotationData, (JNIEnv* env, jobject, ARGUMENT_PAIR(method),\n+                 jobject filter, jint filter_length, jlong filter_klass_pointers))\n+  methodHandle method(THREAD, UNPACK_PAIR(Method, method));\n+  return get_encoded_annotation_data(method->method_holder(), method->annotations(), false, filter_length, filter_klass_pointers, THREAD, JVMCIENV);\n+C2V_END\n+\n+C2V_VMENTRY_NULL(jbyteArray, getEncodedFieldAnnotationData, (JNIEnv* env, jobject, ARGUMENT_PAIR(klass), jint index,\n+                 jobject filter, jint filter_length, jlong filter_klass_pointers))\n+  InstanceKlass* holder = check_field(InstanceKlass::cast(UNPACK_PAIR(Klass, klass)), index, JVMCIENV);\n+  fieldDescriptor fd(holder, index);\n+  return get_encoded_annotation_data(holder, fd.annotations(), false, filter_length, filter_klass_pointers, THREAD, JVMCIENV);\n+C2V_END\n+\n@@ -2798,0 +2934,1 @@\n+#define FIELDINFO               \"Ljdk\/vm\/ci\/hotspot\/HotSpotResolvedObjectTypeImpl$FieldInfo;\"\n@@ -2829,0 +2966,1 @@\n+  {CC \"lookupJClass\",                                 CC \"(J)\" HS_RESOLVED_TYPE,                                                            FN_PTR(lookupJClass)},\n@@ -2844,1 +2982,1 @@\n-  {CC \"resolveInvokeDynamicInPool\",                   CC \"(\" HS_CONSTANT_POOL2 \"I)V\",                                                       FN_PTR(resolveInvokeDynamicInPool)},\n+  {CC \"resolveInvokeDynamicInPool\",                   CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(resolveInvokeDynamicInPool)},\n@@ -2895,0 +3033,1 @@\n+  {CC \"getDeclaredFieldsInfo\",                        CC \"(\" HS_KLASS2 \")[\" FIELDINFO,                                                      FN_PTR(getDeclaredFieldsInfo)},\n@@ -2920,0 +3059,3 @@\n+  {CC \"getEncodedClassAnnotationData\",                CC \"(\" HS_KLASS2 OBJECT \"IJ)[B\",                                                      FN_PTR(getEncodedClassAnnotationData)},\n+  {CC \"getEncodedExecutableAnnotationData\",           CC \"(\" HS_METHOD2 OBJECT \"IJ)[B\",                                                     FN_PTR(getEncodedExecutableAnnotationData)},\n+  {CC \"getEncodedFieldAnnotationData\",                CC \"(\" HS_KLASS2 \"I\" OBJECT \"IJ)[B\",                                                  FN_PTR(getEncodedFieldAnnotationData)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":170,"deletions":28,"binary":false,"changes":198,"status":"modified"},{"patch":"@@ -107,0 +107,1 @@\n+  nonstatic_field(Annotations,                 _class_annotations,                     AnnotationArray*)                             \\\n@@ -159,1 +160,1 @@\n-  nonstatic_field(InstanceKlass,               _fields,                                       Array<u2>*)                            \\\n+  nonstatic_field(InstanceKlass,               _fieldinfo_stream,                             Array<u1>*)                            \\\n@@ -319,0 +320,1 @@\n+  static_field(StubRoutines,                _poly1305_processBlocks,                          address)                               \\\n@@ -344,0 +346,9 @@\n+  JFR_ONLY(nonstatic_field(Thread,          _jfr_thread_local,                                JfrThreadLocal))                       \\\n+                                                                                                                                     \\\n+  static_field(java_lang_Thread,            _tid_offset,                                      int)                                   \\\n+  JFR_ONLY(static_field(java_lang_Thread,   _jfr_epoch_offset,                                int))                                  \\\n+                                                                                                                                     \\\n+  JFR_ONLY(nonstatic_field(JfrThreadLocal,  _vthread_id,                                      traceid))                              \\\n+  JFR_ONLY(nonstatic_field(JfrThreadLocal,  _vthread_epoch,                                   u2))                                   \\\n+  JFR_ONLY(nonstatic_field(JfrThreadLocal,  _vthread_excluded,                                bool))                                 \\\n+  JFR_ONLY(nonstatic_field(JfrThreadLocal,  _vthread,                                         bool))                                 \\\n@@ -395,1 +406,0 @@\n-  declare_preprocessor_constant(\"FIELDINFO_TAG_SIZE\", FIELDINFO_TAG_SIZE) \\\n@@ -413,3 +423,2 @@\n-  declare_constant(JVM_ACC_FIELD_INTERNAL)                                \\\n-  declare_constant(JVM_ACC_FIELD_STABLE)                                  \\\n-  declare_constant(JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE)                   \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_injected)                   \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_stable)                     \\\n@@ -423,1 +432,0 @@\n-  declare_preprocessor_constant(\"JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE\", JVM_ACC_FIELD_INITIALIZED_FINAL_UPDATE) \\\n@@ -637,7 +645,0 @@\n-  declare_constant(FieldInfo::access_flags_offset)                        \\\n-  declare_constant(FieldInfo::name_index_offset)                          \\\n-  declare_constant(FieldInfo::signature_index_offset)                     \\\n-  declare_constant(FieldInfo::initval_index_offset)                       \\\n-  declare_constant(FieldInfo::low_packed_offset)                          \\\n-  declare_constant(FieldInfo::high_packed_offset)                         \\\n-  declare_constant(FieldInfo::field_slots)                                \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":14,"deletions":13,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -621,1 +621,1 @@\n-    if (ArchiveHeapLoader::are_archived_mirrors_available()) {\n+    if (ArchiveHeapLoader::is_in_use()) {\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -181,0 +181,1 @@\n+    _is_shared_class                       = 1 << 0,  \/\/ shadows MetaspaceObj::is_shared\n@@ -368,0 +369,9 @@\n+  bool is_shared() const                { \/\/ shadows MetaspaceObj::is_shared)()\n+    CDS_ONLY(return (_shared_class_flags & _is_shared_class) != 0;)\n+    NOT_CDS(return false;)\n+  }\n+\n+  void set_is_shared() {\n+    CDS_ONLY(_shared_class_flags |= _is_shared_class;)\n+  }\n+\n@@ -654,8 +664,0 @@\n-  bool has_final_method() const         { return _access_flags.has_final_method(); }\n-  void set_has_final_method()           { _access_flags.set_has_final_method(); }\n-  bool has_vanilla_constructor() const  { return _access_flags.has_vanilla_constructor(); }\n-  void set_has_vanilla_constructor()    { _access_flags.set_has_vanilla_constructor(); }\n-  bool has_miranda_methods () const     { return access_flags().has_miranda_methods(); }\n-  void set_has_miranda_methods()        { _access_flags.set_has_miranda_methods(); }\n-  bool is_shared() const                { return access_flags().is_shared_class(); } \/\/ shadows MetaspaceObj::is_shared)()\n-  void set_is_shared()                  { _access_flags.set_is_shared_class(); }\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":10,"deletions":8,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -242,7 +242,0 @@\n-void oopDesc::verify_forwardee(oop forwardee) {\n-#if INCLUDE_CDS_JAVA_HEAP\n-  assert(!Universe::heap()->is_archived_object(forwardee) && !Universe::heap()->is_archived_object(this),\n-         \"forwarding archive object\");\n-#endif\n-}\n-\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":0,"deletions":7,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -271,2 +271,0 @@\n-  void verify_forwardee(oop forwardee) NOT_DEBUG_RETURN;\n-\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":0,"deletions":2,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -311,1 +311,0 @@\n-  verify_forwardee(p);\n@@ -318,1 +317,0 @@\n-  verify_forwardee(this);\n@@ -335,1 +333,0 @@\n-  verify_forwardee(p);\n@@ -348,1 +345,0 @@\n-  verify_forwardee(this);\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1997, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1997, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -66,1 +66,1 @@\n-  return remove_dead_region(phase, can_reshape) ? this : NULL;\n+  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n@@ -102,1 +102,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -172,1 +172,1 @@\n-  return remove_dead_region(phase, can_reshape) ? this : NULL;\n+  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n@@ -222,1 +222,1 @@\n-  return remove_dead_region(phase, can_reshape) ? this : NULL;\n+  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n@@ -267,1 +267,1 @@\n-  assert(method != NULL, \"must be valid call site\");\n+  assert(method != nullptr, \"must be valid call site\");\n@@ -273,1 +273,1 @@\n-  _depth  = 1 + (caller == NULL ? 0 : caller->depth());\n+  _depth  = 1 + (caller == nullptr ? 0 : caller->depth());\n@@ -282,1 +282,1 @@\n-  _method(NULL) {\n+  _method(nullptr) {\n@@ -286,1 +286,1 @@\n-  _caller = NULL;\n+  _caller = nullptr;\n@@ -315,1 +315,1 @@\n-    if (p->_method == NULL)          return true;   \/\/ bci is irrelevant\n+    if (p->_method == nullptr)       return true;   \/\/ bci is irrelevant\n@@ -321,1 +321,1 @@\n-    assert(p != NULL && q != NULL, \"depth check ensures we don't run off end\");\n+    assert(p != nullptr && q != nullptr, \"depth check ensures we don't run off end\");\n@@ -342,1 +342,1 @@\n-  for (const JVMState* jvmp = this; jvmp != NULL; jvmp = jvmp->caller()) {\n+  for (const JVMState* jvmp = this; jvmp != nullptr; jvmp = jvmp->caller()) {\n@@ -354,1 +354,1 @@\n-  if (n == NULL) { st->print(\" NULL\"); return; }\n+  if (n == nullptr) { st->print(\" null\"); return; }\n@@ -377,1 +377,1 @@\n-      st->print(\" %s%d]=#NULL\",msg,i);\n+      st->print(\" %s%d]=#null\",msg,i);\n@@ -480,1 +480,1 @@\n-      ciInstanceKlass *iklass = NULL;\n+      ciInstanceKlass *iklass = nullptr;\n@@ -508,1 +508,1 @@\n-        if (iklass != NULL) {\n+        if (iklass != nullptr) {\n@@ -518,1 +518,1 @@\n-          if (iklass != NULL) {\n+          if (iklass != nullptr) {\n@@ -532,1 +532,1 @@\n-  if (caller() != NULL) caller()->format(regalloc, n, st);\n+  if (caller() != nullptr) caller()->format(regalloc, n, st);\n@@ -537,1 +537,1 @@\n-  if (_method != NULL) {\n+  if (_method != nullptr) {\n@@ -549,2 +549,2 @@\n-        if (endcn == NULL)  endcn = strchr(name, '(');\n-        if (endcn == NULL)  endcn = name + strlen(name);\n+        if (endcn == nullptr)  endcn = strchr(name, '(');\n+        if (endcn == nullptr)  endcn = name + strlen(name);\n@@ -563,1 +563,1 @@\n-  if (caller() != NULL)  caller()->dump_spec(st);\n+  if (caller() != nullptr)  caller()->dump_spec(st);\n@@ -569,1 +569,1 @@\n-                  ((caller() == NULL) || (caller()->map() != _map));\n+                  ((caller() == nullptr) || (caller()->map() != _map));\n@@ -574,1 +574,1 @@\n-      while (ex != NULL && ex->len() > ex->req()) {\n+      while (ex != nullptr && ex->len() > ex->req()) {\n@@ -581,1 +581,1 @@\n-  if (caller() != NULL) {\n+  if (caller() != nullptr) {\n@@ -586,1 +586,1 @@\n-  if (_method == NULL) {\n+  if (_method == nullptr) {\n@@ -623,1 +623,1 @@\n-  for (JVMState* p = n; p->_caller != NULL; p = p->_caller) {\n+  for (JVMState* p = n; p->_caller != nullptr; p = p->_caller) {\n@@ -635,1 +635,1 @@\n-  for (JVMState* p = this; p != NULL; p = p->_caller) {\n+  for (JVMState* p = this; p != nullptr; p = p->_caller) {\n@@ -649,1 +649,1 @@\n-  for (JVMState* jvms = this; jvms != NULL; jvms = jvms->caller()) {\n+  for (JVMState* jvms = this; jvms != nullptr; jvms = jvms->caller()) {\n@@ -668,1 +668,1 @@\n-  while (jvms != NULL) {\n+  while (jvms != nullptr) {\n@@ -713,1 +713,1 @@\n-  if (tf() != NULL)  tf()->dump_on(st);\n+  if (tf() != nullptr)  tf()->dump_on(st);\n@@ -715,1 +715,1 @@\n-  if (jvms() != NULL)  jvms()->dump_spec(st);\n+  if (jvms() != nullptr)  jvms()->dump_spec(st);\n@@ -777,1 +777,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -790,1 +790,1 @@\n-  assert((t_oop != NULL), \"sanity\");\n+  assert((t_oop != nullptr), \"sanity\");\n@@ -793,1 +793,1 @@\n-    Node* dest = NULL;\n+    Node* dest = nullptr;\n@@ -806,1 +806,1 @@\n-    guarantee(dest != NULL, \"Call had only one ptr in, broken IR!\");\n+    guarantee(dest != nullptr, \"Call had only one ptr in, broken IR!\");\n@@ -822,1 +822,1 @@\n-      if ((proj == NULL) || (phase->type(proj)->is_instptr()->instance_klass() != boxing_klass)) {\n+      if ((proj == nullptr) || (phase->type(proj)->is_instptr()->instance_klass() != boxing_klass)) {\n@@ -826,1 +826,1 @@\n-    if (is_CallJava() && as_CallJava()->method() != NULL) {\n+    if (is_CallJava() && as_CallJava()->method() != nullptr) {\n@@ -833,2 +833,2 @@\n-      Node* proj = returns_pointer() ? proj_out_or_null(TypeFunc::Parms) : NULL;\n-      if (proj != NULL) {\n+      Node* proj = returns_pointer() ? proj_out_or_null(TypeFunc::Parms) : nullptr;\n+      if (proj != nullptr) {\n@@ -836,2 +836,2 @@\n-        if ((inst_t != NULL) && (!inst_t->klass_is_exact() ||\n-                                 (inst_t->instance_klass() == boxing_klass))) {\n+        if ((inst_t != nullptr) && (!inst_t->klass_is_exact() ||\n+                                   (inst_t->instance_klass() == boxing_klass))) {\n@@ -844,1 +844,1 @@\n-        if ((inst_t != NULL) && (!inst_t->klass_is_exact() ||\n+        if ((inst_t != nullptr) && (!inst_t->klass_is_exact() ||\n@@ -869,1 +869,1 @@\n-\/\/ or returns NULL if there is no one.\n+\/\/ or returns null if there is no one.\n@@ -871,1 +871,1 @@\n-  Node *cast = NULL;\n+  Node *cast = nullptr;\n@@ -874,2 +874,2 @@\n-  if (p == NULL)\n-    return NULL;\n+  if (p == nullptr)\n+    return nullptr;\n@@ -880,1 +880,1 @@\n-      if (cast != NULL) {\n+      if (cast != nullptr) {\n@@ -899,9 +899,9 @@\n-  projs->fallthrough_proj      = NULL;\n-  projs->fallthrough_catchproj = NULL;\n-  projs->fallthrough_ioproj    = NULL;\n-  projs->catchall_ioproj       = NULL;\n-  projs->catchall_catchproj    = NULL;\n-  projs->fallthrough_memproj   = NULL;\n-  projs->catchall_memproj      = NULL;\n-  projs->resproj               = NULL;\n-  projs->exobj                 = NULL;\n+  projs->fallthrough_proj      = nullptr;\n+  projs->fallthrough_catchproj = nullptr;\n+  projs->fallthrough_ioproj    = nullptr;\n+  projs->catchall_ioproj       = nullptr;\n+  projs->catchall_catchproj    = nullptr;\n+  projs->fallthrough_memproj   = nullptr;\n+  projs->catchall_memproj      = nullptr;\n+  projs->resproj               = nullptr;\n+  projs->exobj                 = nullptr;\n@@ -918,2 +918,2 @@\n-        if (cn != NULL && cn->is_Catch()) {\n-          ProjNode *cpn = NULL;\n+        if (cn != nullptr && cn->is_Catch()) {\n+          ProjNode *cpn = nullptr;\n@@ -941,1 +941,1 @@\n-          assert(projs->exobj == NULL, \"only one\");\n+          assert(projs->exobj == nullptr, \"only one\");\n@@ -963,1 +963,1 @@\n-  assert(projs->fallthrough_proj      != NULL, \"must be found\");\n+  assert(projs->fallthrough_proj      != nullptr, \"must be found\");\n@@ -965,4 +965,4 @@\n-  assert(!do_asserts || projs->fallthrough_catchproj != NULL, \"must be found\");\n-  assert(!do_asserts || projs->fallthrough_memproj   != NULL, \"must be found\");\n-  assert(!do_asserts || projs->fallthrough_ioproj    != NULL, \"must be found\");\n-  assert(!do_asserts || projs->catchall_catchproj    != NULL, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_catchproj != nullptr, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_memproj   != nullptr, \"must be found\");\n+  assert(!do_asserts || projs->fallthrough_ioproj    != nullptr, \"must be found\");\n+  assert(!do_asserts || projs->catchall_catchproj    != nullptr, \"must be found\");\n@@ -970,2 +970,2 @@\n-    assert(!do_asserts || projs->catchall_memproj    != NULL, \"must be found\");\n-    assert(!do_asserts || projs->catchall_ioproj     != NULL, \"must be found\");\n+    assert(!do_asserts || projs->catchall_memproj    != nullptr, \"must be found\");\n+    assert(!do_asserts || projs->catchall_ioproj     != nullptr, \"must be found\");\n@@ -979,1 +979,1 @@\n-  if (cg != NULL) {\n+  if (cg != nullptr) {\n@@ -988,1 +988,1 @@\n-  if (_name != NULL && strstr(_name, \"arraycopy\") != 0) {\n+  if (_name != nullptr && strstr(_name, \"arraycopy\") != 0) {\n@@ -1016,1 +1016,1 @@\n-    if (old_in != NULL && old_in->is_SafePointScalarObject()) {\n+    if (old_in != nullptr && old_in->is_SafePointScalarObject()) {\n@@ -1030,2 +1030,2 @@\n-  set_jvms(sfpt->jvms() != NULL ? sfpt->jvms()->clone_deep(C) : NULL);\n-  for (JVMState *jvms = this->jvms(); jvms != NULL; jvms = jvms->caller()) {\n+  set_jvms(sfpt->jvms() != nullptr ? sfpt->jvms()->clone_deep(C) : nullptr);\n+  for (JVMState *jvms = this->jvms(); jvms != nullptr; jvms = jvms->caller()) {\n@@ -1043,1 +1043,1 @@\n-  if (method() == NULL) {\n+  if (method() == nullptr) {\n@@ -1080,1 +1080,1 @@\n-  if (can_reshape && cg != NULL) {\n+  if (can_reshape && cg != nullptr) {\n@@ -1091,1 +1091,1 @@\n-        set_generator(NULL);\n+        set_generator(nullptr);\n@@ -1099,1 +1099,1 @@\n-        set_generator(NULL);\n+        set_generator(nullptr);\n@@ -1109,1 +1109,1 @@\n-  if (_name != NULL && !strcmp(_name, \"uncommon_trap\")) {\n+  if (_name != nullptr && !strcmp(_name, \"uncommon_trap\")) {\n@@ -1117,1 +1117,1 @@\n-        call->in(TypeFunc::Parms) != NULL &&\n+        call->in(TypeFunc::Parms) != nullptr &&\n@@ -1131,1 +1131,1 @@\n-  if (_name != NULL) {\n+  if (_name != nullptr) {\n@@ -1165,1 +1165,1 @@\n-  if (can_reshape && cg != NULL) {\n+  if (can_reshape && cg != nullptr) {\n@@ -1198,1 +1198,1 @@\n-      set_generator(NULL);\n+      set_generator(nullptr);\n@@ -1288,1 +1288,1 @@\n-  assert(n == NULL || n->Opcode() == Op_SafePoint, \"correct value for next_exception\");\n+  assert(n == nullptr || n->Opcode() == Op_SafePoint, \"correct value for next_exception\");\n@@ -1290,1 +1290,1 @@\n-    if (n != NULL)  add_prec(n);\n+    if (n != nullptr)  add_prec(n);\n@@ -1300,1 +1300,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -1303,1 +1303,1 @@\n-    assert(n == NULL || n->Opcode() == Op_SafePoint, \"no other uses of prec edges\");\n+    assert(n == nullptr || n->Opcode() == Op_SafePoint, \"no other uses of prec edges\");\n@@ -1312,2 +1312,2 @@\n-  assert(_jvms == NULL || ((uintptr_t)_jvms->map() & 1) || _jvms->map() == this, \"inconsistent JVMState\");\n-  return remove_dead_region(phase, can_reshape) ? this : NULL;\n+  assert(_jvms == nullptr || ((uintptr_t)_jvms->map() & 1) || _jvms->map() == this, \"inconsistent JVMState\");\n+  return remove_dead_region(phase, can_reshape) ? this : nullptr;\n@@ -1325,1 +1325,1 @@\n-    if (out_c != NULL && !out_c->is_OuterStripMinedLoopEnd()) {\n+    if (out_c != nullptr && !out_c->is_OuterStripMinedLoopEnd()) {\n@@ -1511,1 +1511,1 @@\n-  if (cached != NULL) {\n+  if (cached != nullptr) {\n@@ -1536,1 +1536,1 @@\n-  : CallNode(atype, NULL, TypeRawPtr::BOTTOM)\n+  : CallNode(atype, nullptr, TypeRawPtr::BOTTOM)\n@@ -1560,1 +1560,1 @@\n-  assert(initializer != NULL &&\n+  assert(initializer != nullptr &&\n@@ -1565,1 +1565,1 @@\n-  if (analyzer == NULL) {\n+  if (analyzer == nullptr) {\n@@ -1583,1 +1583,1 @@\n-\/\/ a CastII is appropriate, return NULL.\n+\/\/ a CastII is appropriate, return null.\n@@ -1586,1 +1586,1 @@\n-  assert(length != NULL, \"length is not null\");\n+  assert(length != nullptr, \"length is not null\");\n@@ -1591,1 +1591,1 @@\n-  if (ary_type != NULL && length_type != NULL) {\n+  if (ary_type != nullptr && length_type != nullptr) {\n@@ -1604,1 +1604,1 @@\n-      \/\/ Return NULL if new nodes are not allowed\n+      \/\/ Return null if new nodes are not allowed\n@@ -1606,1 +1606,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -1611,1 +1611,1 @@\n-      if (init != NULL) {\n+      if (init != nullptr) {\n@@ -1752,2 +1752,2 @@\n-  if (ctrl == NULL)\n-    return NULL;\n+  if (ctrl == nullptr)\n+    return nullptr;\n@@ -1758,1 +1758,1 @@\n-      if (n == NULL)\n+      if (n == nullptr)\n@@ -1781,2 +1781,2 @@\n-  ProjNode *ctrl_proj = (ctrl->is_Proj()) ? ctrl->as_Proj() : NULL;\n-  if (ctrl_proj != NULL && ctrl_proj->_con == TypeFunc::Control) {\n+  ProjNode *ctrl_proj = (ctrl->is_Proj()) ? ctrl->as_Proj() : nullptr;\n+  if (ctrl_proj != nullptr && ctrl_proj->_con == TypeFunc::Control) {\n@@ -1784,1 +1784,1 @@\n-    if (n != NULL && n->is_Unlock()) {\n+    if (n != nullptr && n->is_Unlock()) {\n@@ -1804,1 +1804,1 @@\n-  LockNode *lock_result = NULL;\n+  LockNode *lock_result = nullptr;\n@@ -1808,1 +1808,1 @@\n-    assert(ctrl != NULL, \"invalid control graph\");\n+    assert(ctrl != nullptr, \"invalid control graph\");\n@@ -1816,1 +1816,1 @@\n-      if (ctrl->req() == 3 && ctrl->in(1) != NULL && ctrl->in(2) != NULL) {\n+      if (ctrl->req() == 3 && ctrl->in(1) != nullptr && ctrl->in(2) != nullptr) {\n@@ -1855,1 +1855,1 @@\n-      Node* lock1_node = NULL;\n+      Node* lock1_node = nullptr;\n@@ -1866,1 +1866,1 @@\n-      if (lock1_node != NULL && lock1_node->is_Lock()) {\n+      if (lock1_node != nullptr && lock1_node->is_Lock()) {\n@@ -1891,1 +1891,1 @@\n-    if (in_node != NULL) {\n+    if (in_node != nullptr) {\n@@ -1946,1 +1946,1 @@\n-  \/\/ perform any generic optimizations first (returns 'this' or NULL)\n+  \/\/ perform any generic optimizations first (returns 'this' or null)\n@@ -1948,1 +1948,1 @@\n-  if (result != NULL)  return result;\n+  if (result != nullptr)  return result;\n@@ -1950,1 +1950,1 @@\n-  if (in(0) && in(0)->is_top())  return NULL;\n+  if (in(0) && in(0)->is_top())  return nullptr;\n@@ -1962,1 +1962,1 @@\n-    if (cgr != NULL && cgr->not_global_escape(obj_node())) {\n+    if (cgr != nullptr && cgr->not_global_escape(obj_node())) {\n@@ -1981,1 +1981,1 @@\n-    if (iter != NULL && !is_eliminated()) {\n+    if (iter != nullptr && !is_eliminated()) {\n@@ -2061,1 +2061,1 @@\n-  return is_nested_lock_region(NULL);\n+  return is_nested_lock_region(nullptr);\n@@ -2064,1 +2064,1 @@\n-\/\/ p is used for access to compilation log; no logging if NULL\n+\/\/ p is used for access to compilation log; no logging if null\n@@ -2077,2 +2077,2 @@\n-  LockNode* unique_lock = NULL;\n-  Node* bad_lock = NULL;\n+  LockNode* unique_lock = nullptr;\n+  Node* bad_lock = nullptr;\n@@ -2087,1 +2087,1 @@\n-    this->log_lock_optimization(c, \"eliminate_lock_INLR_2b\", (unique_lock != NULL ? unique_lock : bad_lock));\n+    this->log_lock_optimization(c, \"eliminate_lock_INLR_2b\", (unique_lock != nullptr ? unique_lock : bad_lock));\n@@ -2096,1 +2096,1 @@\n-      if (unique_lock != NULL) {\n+      if (unique_lock != nullptr) {\n@@ -2100,1 +2100,1 @@\n-      if (bad_lock != NULL) {\n+      if (bad_lock != nullptr) {\n@@ -2141,1 +2141,1 @@\n-  \/\/ perform any generic optimizations first (returns 'this' or NULL)\n+  \/\/ perform any generic optimizations first (returns 'this' or null)\n@@ -2143,1 +2143,1 @@\n-  if (result != NULL)  return result;\n+  if (result != nullptr)  return result;\n@@ -2145,1 +2145,1 @@\n-  if (in(0) && in(0)->is_top())  return NULL;\n+  if (in(0) && in(0)->is_top())  return nullptr;\n@@ -2158,1 +2158,1 @@\n-    if (cgr != NULL && cgr->not_global_escape(obj_node())) {\n+    if (cgr != nullptr && cgr->not_global_escape(obj_node())) {\n@@ -2173,1 +2173,1 @@\n-  if (C == NULL) {\n+  if (C == nullptr) {\n@@ -2177,1 +2177,1 @@\n-  if (log != NULL) {\n+  if (log != nullptr) {\n@@ -2180,2 +2180,2 @@\n-    int box_id = box != NULL ? box->_idx : -1;\n-    int obj_id = obj != NULL ? obj->_idx : -1;\n+    int box_id = box != nullptr ? box->_idx : -1;\n+    int obj_id = obj != nullptr ? obj->_idx : -1;\n@@ -2186,1 +2186,1 @@\n-          kind_as_string(), box_id, obj_id, (bad_lock != NULL ? bad_lock->_idx : -1));\n+          kind_as_string(), box_id, obj_id, (bad_lock != nullptr ? bad_lock->_idx : -1));\n@@ -2190,1 +2190,1 @@\n-    while (p != NULL) {\n+    while (p != nullptr) {\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":132,"deletions":132,"binary":false,"changes":264,"status":"modified"},{"patch":"@@ -88,1 +88,1 @@\n-  if (_mach_constant_base_node == NULL) {\n+  if (_mach_constant_base_node == nullptr) {\n@@ -156,1 +156,1 @@\n-    if (cg != NULL) {\n+    if (cg != nullptr) {\n@@ -164,1 +164,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -231,1 +231,1 @@\n-  if (xtty != NULL)  xtty->head(\"statistics type='intrinsic'\");\n+  if (xtty != nullptr)  xtty->head(\"statistics type='intrinsic'\");\n@@ -245,1 +245,1 @@\n-  if (xtty != NULL)  xtty->tail(\"statistics\");\n+  if (xtty != nullptr)  xtty->tail(\"statistics\");\n@@ -250,1 +250,1 @@\n-    if (xtty != NULL)  xtty->head(\"statistics type='opto'\");\n+    if (xtty != nullptr)  xtty->head(\"statistics type='opto'\");\n@@ -260,1 +260,1 @@\n-    if (xtty != NULL)  xtty->tail(\"statistics\");\n+    if (xtty != nullptr)  xtty->tail(\"statistics\");\n@@ -298,1 +298,1 @@\n-  useful.map( estimated_worklist_size, NULL );  \/\/ preallocate space\n+  useful.map( estimated_worklist_size, nullptr );  \/\/ preallocate space\n@@ -301,1 +301,1 @@\n-  if (root() != NULL)     { useful.push(root()); }\n+  if (root() != nullptr)  { useful.push(root()); }\n@@ -303,1 +303,1 @@\n-  if( cached_top_node() ) { useful.push(cached_top_node()); }\n+  if (cached_top_node())  { useful.push(cached_top_node()); }\n@@ -351,1 +351,1 @@\n-  assert(dead != NULL && dead->is_Call(), \"sanity\");\n+  assert(dead != nullptr && dead->is_Call(), \"sanity\");\n@@ -445,1 +445,1 @@\n-  if (_modified_nodes != NULL) {\n+  if (_modified_nodes != nullptr) {\n@@ -474,1 +474,1 @@\n-  assert(env->compiler_data() == NULL, \"compile already active?\");\n+  assert(env->compiler_data() == nullptr, \"compile already active?\");\n@@ -478,1 +478,1 @@\n-  compile->set_type_dict(NULL);\n+  compile->set_type_dict(nullptr);\n@@ -482,3 +482,3 @@\n-  compile->set_last_tf(NULL, NULL);\n-  compile->set_indexSet_arena(NULL);\n-  compile->set_indexSet_free_block_list(NULL);\n+  compile->set_last_tf(nullptr, nullptr);\n+  compile->set_indexSet_arena(nullptr);\n+  compile->set_indexSet_free_block_list(nullptr);\n@@ -495,1 +495,1 @@\n-  _compile->env()->set_compiler_data(NULL);\n+  _compile->env()->set_compiler_data(nullptr);\n@@ -558,1 +558,1 @@\n-  if (xtty != NULL) {\n+  if (xtty != nullptr) {\n@@ -573,1 +573,1 @@\n-  if (xtty != NULL) {\n+  if (xtty != nullptr) {\n@@ -594,4 +594,4 @@\n-                  _ilt(NULL),\n-                  _stub_function(NULL),\n-                  _stub_name(NULL),\n-                  _stub_entry_point(NULL),\n+                  _ilt(nullptr),\n+                  _stub_function(nullptr),\n+                  _stub_name(nullptr),\n+                  _stub_entry_point(nullptr),\n@@ -616,11 +616,11 @@\n-                  _failure_reason(NULL),\n-                  _intrinsics        (comp_arena(), 0, 0, NULL),\n-                  _macro_nodes       (comp_arena(), 8, 0, NULL),\n-                  _predicate_opaqs   (comp_arena(), 8, 0, NULL),\n-                  _skeleton_predicate_opaqs (comp_arena(), 8, 0, NULL),\n-                  _expensive_nodes   (comp_arena(), 8, 0, NULL),\n-                  _for_post_loop_igvn(comp_arena(), 8, 0, NULL),\n-                  _unstable_if_traps (comp_arena(), 8, 0, NULL),\n-                  _coarsened_locks   (comp_arena(), 8, 0, NULL),\n-                  _congraph(NULL),\n-                  NOT_PRODUCT(_igv_printer(NULL) COMMA)\n+                  _failure_reason(nullptr),\n+                  _intrinsics        (comp_arena(), 0, 0, nullptr),\n+                  _macro_nodes       (comp_arena(), 8, 0, nullptr),\n+                  _predicate_opaqs   (comp_arena(), 8, 0, nullptr),\n+                  _skeleton_predicate_opaqs (comp_arena(), 8, 0, nullptr),\n+                  _expensive_nodes   (comp_arena(), 8, 0, nullptr),\n+                  _for_post_loop_igvn(comp_arena(), 8, 0, nullptr),\n+                  _unstable_if_traps (comp_arena(), 8, 0, nullptr),\n+                  _coarsened_locks   (comp_arena(), 8, 0, nullptr),\n+                  _congraph(nullptr),\n+                  NOT_PRODUCT(_igv_printer(nullptr) COMMA)\n@@ -631,1 +631,1 @@\n-                  _mach_constant_base_node(NULL),\n+                  _mach_constant_base_node(nullptr),\n@@ -633,6 +633,6 @@\n-                  _initial_gvn(NULL),\n-                  _for_igvn(NULL),\n-                  _late_inlines(comp_arena(), 2, 0, NULL),\n-                  _string_late_inlines(comp_arena(), 2, 0, NULL),\n-                  _boxing_late_inlines(comp_arena(), 2, 0, NULL),\n-                  _vector_reboxing_late_inlines(comp_arena(), 2, 0, NULL),\n+                  _initial_gvn(nullptr),\n+                  _for_igvn(nullptr),\n+                  _late_inlines(comp_arena(), 2, 0, nullptr),\n+                  _string_late_inlines(comp_arena(), 2, 0, nullptr),\n+                  _boxing_late_inlines(comp_arena(), 2, 0, nullptr),\n+                  _vector_reboxing_late_inlines(comp_arena(), 2, 0, nullptr),\n@@ -642,1 +642,1 @@\n-                  _print_inlining_list(NULL),\n+                  _print_inlining_list(nullptr),\n@@ -644,2 +644,2 @@\n-                  _print_inlining_output(NULL),\n-                  _replay_inline_data(NULL),\n+                  _print_inlining_output(nullptr),\n+                  _replay_inline_data(nullptr),\n@@ -649,1 +649,1 @@\n-                  _output(NULL)\n+                  _output(nullptr)\n@@ -665,1 +665,1 @@\n-  TraceTime t2(NULL, &_t_methodCompilation, CITime, false);\n+  TraceTime t2(nullptr, &_t_methodCompilation, CITime, false);\n@@ -725,1 +725,1 @@\n-    CallGenerator* cg = NULL;\n+    CallGenerator* cg = nullptr;\n@@ -748,1 +748,1 @@\n-      if (cg == NULL) {\n+      if (cg == nullptr) {\n@@ -755,2 +755,6 @@\n-    if (cg == NULL) {\n-      record_method_not_compilable(\"cannot parse method\");\n+    if (cg == nullptr) {\n+      const char* reason = InlineTree::check_can_parse(method());\n+      assert(reason != nullptr, \"expect reason for parse failure\");\n+      stringStream ss;\n+      ss.print(\"cannot parse method: %s\", reason);\n+      record_method_not_compilable(ss.as_string());\n@@ -763,1 +767,1 @@\n-    if ((jvms = cg->generate(jvms)) == NULL) {\n+    if ((jvms = cg->generate(jvms)) == nullptr) {\n@@ -765,1 +769,4 @@\n-        record_method_not_compilable(\"method parse failed\");\n+        assert(failure_reason() != nullptr, \"expect reason for parse failure\");\n+        stringStream ss;\n+        ss.print(\"method parse failed: %s\", failure_reason());\n+        record_method_not_compilable(ss.as_string());\n@@ -807,1 +814,1 @@\n-  set_default_node_notes(NULL);\n+  set_default_node_notes(nullptr);\n@@ -827,1 +834,1 @@\n-    if (_log != NULL) {\n+    if (_log != nullptr) {\n@@ -852,1 +859,1 @@\n-  if (directive->DumpInlineOption && (ilt() != NULL)) {\n+  if (directive->DumpInlineOption && (ilt() != nullptr)) {\n@@ -882,1 +889,1 @@\n-    _method(NULL),\n+    _method(nullptr),\n@@ -886,1 +893,1 @@\n-    _stub_entry_point(NULL),\n+    _stub_entry_point(nullptr),\n@@ -904,3 +911,3 @@\n-    _failure_reason(NULL),\n-    _congraph(NULL),\n-    NOT_PRODUCT(_igv_printer(NULL) COMMA)\n+    _failure_reason(nullptr),\n+    _congraph(nullptr),\n+    NOT_PRODUCT(_igv_printer(nullptr) COMMA)\n@@ -911,1 +918,1 @@\n-    _mach_constant_base_node(NULL),\n+    _mach_constant_base_node(nullptr),\n@@ -913,2 +920,2 @@\n-    _initial_gvn(NULL),\n-    _for_igvn(NULL),\n+    _initial_gvn(nullptr),\n+    _for_igvn(nullptr),\n@@ -917,1 +924,1 @@\n-    _print_inlining_list(NULL),\n+    _print_inlining_list(nullptr),\n@@ -919,2 +926,2 @@\n-    _print_inlining_output(NULL),\n-    _replay_inline_data(NULL),\n+    _print_inlining_output(nullptr),\n+    _replay_inline_data(nullptr),\n@@ -924,1 +931,1 @@\n-    _output(NULL),\n+    _output(nullptr),\n@@ -931,2 +938,2 @@\n-  TraceTime t1(NULL, &_t_totalCompilation, CITime, false);\n-  TraceTime t2(NULL, &_t_stubCompilation, CITime, false);\n+  TraceTime t1(nullptr, &_t_totalCompilation, CITime, false);\n+  TraceTime t2(nullptr, &_t_stubCompilation, CITime, false);\n@@ -968,1 +975,1 @@\n-  _regalloc = NULL;\n+  _regalloc = nullptr;\n@@ -970,4 +977,4 @@\n-  _tf      = NULL;  \/\/ filled in later\n-  _top     = NULL;  \/\/ cached later\n-  _matcher = NULL;  \/\/ filled in later\n-  _cfg     = NULL;  \/\/ filled in later\n+  _tf      = nullptr;  \/\/ filled in later\n+  _top     = nullptr;  \/\/ cached later\n+  _matcher = nullptr;  \/\/ filled in later\n+  _cfg     = nullptr;  \/\/ filled in later\n@@ -977,3 +984,3 @@\n-  _node_note_array = NULL;\n-  _default_node_notes = NULL;\n-  DEBUG_ONLY( _modified_nodes = NULL; ) \/\/ Used in Optimize()\n+  _node_note_array = nullptr;\n+  _default_node_notes = nullptr;\n+  DEBUG_ONLY( _modified_nodes = nullptr; ) \/\/ Used in Optimize()\n@@ -981,1 +988,1 @@\n-  _immutable_memory = NULL; \/\/ filled in at first inquiry\n+  _immutable_memory = nullptr; \/\/ filled in at first inquiry\n@@ -986,1 +993,1 @@\n-  _type_verify = NULL;\n+  _type_verify = nullptr;\n@@ -990,2 +997,2 @@\n-  \/\/ First set TOP to NULL to give safe behavior during creation of RootNode\n-  set_cached_top_node(NULL);\n+  \/\/ First set TOP to null to give safe behavior during creation of RootNode\n+  set_cached_top_node(nullptr);\n@@ -995,1 +1002,1 @@\n-  set_recent_alloc(NULL, NULL);\n+  set_recent_alloc(nullptr, nullptr);\n@@ -1042,1 +1049,1 @@\n-  if (UseRTMLocking && has_method() && (method()->method_data_or_null() != NULL)) {\n+  if (UseRTMLocking && has_method() && (method()->method_data_or_null() != nullptr)) {\n@@ -1062,1 +1069,1 @@\n-                        (comp_arena(), 8, 0, NULL));\n+                        (comp_arena(), 8, 0, nullptr));\n@@ -1075,1 +1082,1 @@\n-  _alias_types[AliasIdxTop]->Init(AliasIdxTop, NULL);\n+  _alias_types[AliasIdxTop]->Init(AliasIdxTop, nullptr);\n@@ -1081,2 +1088,2 @@\n-  \/\/ A NULL adr_type hits in the cache right away.  Preload the right answer.\n-  probe_alias_cache(NULL)->_index = AliasIdxTop;\n+  \/\/ A null adr_type hits in the cache right away.  Preload the right answer.\n+  probe_alias_cache(nullptr)->_index = AliasIdxTop;\n@@ -1107,1 +1114,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1113,1 +1120,1 @@\n-  if (_immutable_memory != NULL) {\n+  if (_immutable_memory != nullptr) {\n@@ -1125,1 +1132,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1131,1 +1138,1 @@\n-  if (tn != NULL)  verify_top(tn);\n+  if (tn != nullptr)  verify_top(tn);\n@@ -1136,3 +1143,3 @@\n-  if (_top != NULL)     _top->setup_is_top();\n-  if (old_top != NULL)  old_top->setup_is_top();\n-  assert(_top == NULL || top()->is_top(), \"\");\n+  if (_top != nullptr)     _top->setup_is_top();\n+  if (old_top != nullptr)  old_top->setup_is_top();\n+  assert(_top == nullptr || top()->is_top(), \"\");\n@@ -1151,2 +1158,2 @@\n-  \/\/ Return if CompileLog is NULL and PrintIdealNodeCount is false.\n-  if ((_log == NULL) && (! PrintIdealNodeCount)) {\n+  \/\/ Return if CompileLog is null and PrintIdealNodeCount is false.\n+  if ((_log == nullptr) && (! PrintIdealNodeCount)) {\n@@ -1170,1 +1177,1 @@\n-    if (_log != NULL) {\n+    if (_log != nullptr) {\n@@ -1180,1 +1187,1 @@\n-          if (_log != NULL) {\n+          if (_log != nullptr) {\n@@ -1191,1 +1198,1 @@\n-        if (_log != NULL) {\n+        if (_log != nullptr) {\n@@ -1200,1 +1207,1 @@\n-    if (_log != NULL) {\n+    if (_log != nullptr) {\n@@ -1206,1 +1213,1 @@\n-  if (_modified_nodes != NULL && !_inlining_incrementally && !n->is_Con()) {\n+  if (_modified_nodes != nullptr && !_inlining_incrementally && !n->is_Con()) {\n@@ -1212,1 +1219,1 @@\n-  if (_modified_nodes != NULL) {\n+  if (_modified_nodes != nullptr) {\n@@ -1220,1 +1227,1 @@\n-  if (tn != NULL) {\n+  if (tn != nullptr) {\n@@ -1223,1 +1230,1 @@\n-    assert(tn->in(0) != NULL, \"must have live top node\");\n+    assert(tn->in(0) != nullptr, \"must have live top node\");\n@@ -1232,1 +1239,1 @@\n-  guarantee(arr != NULL, \"\");\n+  guarantee(arr != nullptr, \"\");\n@@ -1247,1 +1254,1 @@\n-  if (source == NULL || dest == NULL)  return false;\n+  if (source == nullptr || dest == nullptr)  return false;\n@@ -1254,1 +1261,1 @@\n-  if (dest != NULL && dest != source && dest->debug_orig() == NULL) {\n+  if (dest != nullptr && dest != source && dest->debug_orig() == nullptr) {\n@@ -1259,1 +1266,1 @@\n-  if (node_note_array() == NULL)\n+  if (node_note_array() == nullptr)\n@@ -1265,1 +1272,1 @@\n-  if (source_notes == NULL || source_notes->is_clear())  return false;\n+  if (source_notes == nullptr || source_notes->is_clear())  return false;\n@@ -1267,1 +1274,1 @@\n-  if (dest_notes == NULL || dest_notes->is_clear()) {\n+  if (dest_notes == nullptr || dest_notes->is_clear()) {\n@@ -1301,1 +1308,1 @@\n-  bool is_known_inst = tj->isa_oopptr() != NULL &&\n+  bool is_known_inst = tj->isa_oopptr() != nullptr &&\n@@ -1372,1 +1379,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n@@ -1376,1 +1383,1 @@\n-      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,NULL,false,offset);\n+      tj = ta = TypeAryPtr::make(ptr,ta->const_oop(),tary,nullptr,false,offset);\n@@ -1388,1 +1395,1 @@\n-    if (ptr == TypePtr::NotNull || ta->klass_is_exact() || ta->speculative() != NULL) {\n+    if (ptr == TypePtr::NotNull || ta->klass_is_exact() || ta->speculative() != nullptr) {\n@@ -1425,1 +1432,1 @@\n-    if (to->speculative() != NULL) {\n+    if (to->speculative() != nullptr) {\n@@ -1433,1 +1440,1 @@\n-        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, NULL, offset);\n+        tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()->Object_klass(), false, nullptr, offset);\n@@ -1439,1 +1446,1 @@\n-        to = NULL;\n+        to = nullptr;\n@@ -1448,1 +1455,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, NULL, offset, to->instance_id());\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, true, nullptr, offset, to->instance_id());\n@@ -1450,1 +1457,1 @@\n-          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, NULL, offset);\n+          tj = to = TypeInstPtr::make(to->ptr(), canonical_holder, false, nullptr, offset);\n@@ -1527,2 +1534,2 @@\n-  _field = NULL;\n-  _element = NULL;\n+  _field = nullptr;\n+  _element = nullptr;\n@@ -1530,2 +1537,2 @@\n-  const TypeOopPtr *atoop = (at != NULL) ? at->isa_oopptr() : NULL;\n-  if (atoop != NULL && atoop->is_known_instance()) {\n+  const TypeOopPtr *atoop = (at != nullptr) ? at->isa_oopptr() : nullptr;\n+  if (atoop != nullptr && atoop->is_known_instance()) {\n@@ -1540,1 +1547,1 @@\n-  if (element() != NULL) {\n+  if (element() != nullptr) {\n@@ -1543,1 +1550,1 @@\n-  } if (field() != NULL) {\n+  } if (field() != nullptr) {\n@@ -1564,1 +1571,1 @@\n-  if (field() != NULL && tjp) {\n+  if (field() != nullptr && tjp) {\n@@ -1618,1 +1625,1 @@\n-  if (adr_type == NULL)             return alias_type(AliasIdxTop);\n+  if (adr_type == nullptr)          return alias_type(AliasIdxTop);\n@@ -1651,1 +1658,1 @@\n-    if (no_create)  return NULL;\n+    if (no_create)  return nullptr;\n@@ -1694,1 +1701,1 @@\n-      if (tinst->const_oop() != NULL &&\n+      if (tinst->const_oop() != nullptr &&\n@@ -1704,2 +1711,2 @@\n-      assert(field == NULL ||\n-             original_field == NULL ||\n+      assert(field == nullptr ||\n+             original_field == nullptr ||\n@@ -1707,1 +1714,1 @@\n-              field->offset() == original_field->offset() &&\n+              field->offset_in_bytes() == original_field->offset_in_bytes() &&\n@@ -1710,1 +1717,1 @@\n-      if (field != NULL)  alias_type(idx)->set_field(field);\n+      if (field != nullptr)  alias_type(idx)->set_field(field);\n@@ -1721,1 +1728,1 @@\n-  if (face->_adr_type == NULL) {\n+  if (face->_adr_type == nullptr) {\n@@ -1751,1 +1758,1 @@\n-  if (adr_type == NULL)             return true;\n+  if (adr_type == nullptr)             return true;\n@@ -1754,1 +1761,1 @@\n-  return find_alias_type(adr_type, true, NULL) != NULL;\n+  return find_alias_type(adr_type, true, nullptr) != nullptr;\n@@ -1761,1 +1768,1 @@\n-  if (adr_type == NULL)                 return true;  \/\/ NULL serves as TypePtr::TOP\n+  if (adr_type == nullptr)              return true;  \/\/ null serves as TypePtr::TOP\n@@ -1779,1 +1786,1 @@\n-  if (adr_type == NULL)                 return false; \/\/ NULL serves as TypePtr::TOP\n+  if (adr_type == nullptr)              return false; \/\/ null serves as TypePtr::TOP\n@@ -2074,1 +2081,1 @@\n-        if (do_print_inlining || log() != NULL) {\n+        if (do_print_inlining || log() != nullptr) {\n@@ -2127,1 +2134,1 @@\n-  \/\/ Tracking and verification of modified nodes is disabled by setting \"_modified_nodes == NULL\"\n+  \/\/ Tracking and verification of modified nodes is disabled by setting \"_modified_nodes == nullptr\"\n@@ -2130,1 +2137,1 @@\n-  assert(_modified_nodes == NULL, \"not allowed\");\n+  assert(_modified_nodes == nullptr, \"not allowed\");\n@@ -2165,1 +2172,1 @@\n-  if (r != NULL) {\n+  if (r != nullptr) {\n@@ -2168,1 +2175,1 @@\n-      if (n != NULL && n->is_SafePoint()) {\n+      if (n != nullptr && n->is_SafePoint()) {\n@@ -2310,1 +2317,1 @@\n-      if (congraph() != NULL && macro_count() > 0) {\n+      if (congraph() != nullptr && macro_count() > 0) {\n@@ -2424,1 +2431,1 @@\n-  DEBUG_ONLY( _modified_nodes = NULL; )\n+  DEBUG_ONLY( _modified_nodes = nullptr; )\n@@ -2606,1 +2613,1 @@\n-  Node* mask = pn->is_predicated_vector() ? pn->in(pn->req()-1) : NULL;\n+  Node* mask = pn->is_predicated_vector() ? pn->in(pn->req()-1) : nullptr;\n@@ -2652,1 +2659,1 @@\n-  assert(n != NULL, \"\");\n+  assert(n != nullptr, \"\");\n@@ -2768,3 +2775,3 @@\n-  Node* parent_pred = parent_is_predicated ? n->in(n->req()-1) : NULL;\n-  Node* left_child_pred = left_child_predicated ? n->in(1)->in(n->in(1)->req()-1) : NULL;\n-  Node* right_child_pred = right_child_predicated ? n->in(1)->in(n->in(1)->req()-1) : NULL;\n+  Node* parent_pred = parent_is_predicated ? n->in(n->req()-1) : nullptr;\n+  Node* left_child_pred = left_child_predicated ? n->in(1)->in(n->in(1)->req()-1) : nullptr;\n+  Node* right_child_pred = right_child_predicated ? n->in(1)->in(n->in(1)->req()-1) : nullptr;\n@@ -2837,2 +2844,2 @@\n-    Node* mask = pn->is_predicated_vector() ? pn->in(pn->req()-1) : NULL;\n-    if (mask == NULL ||\n+    Node* mask = pn->is_predicated_vector() ? pn->in(pn->req()-1) : nullptr;\n+    if (mask == nullptr ||\n@@ -3040,1 +3047,1 @@\n-            if (mem->in(i) != NULL) {\n+            if (mem->in(i) != nullptr) {\n@@ -3093,1 +3100,1 @@\n-    assert( n->in(0) != NULL || alias_idx != Compile::AliasIdxRaw ||\n+    assert( n->in(0) != nullptr || alias_idx != Compile::AliasIdxRaw ||\n@@ -3283,2 +3290,2 @@\n-      bool is_oop   = t->isa_oopptr() != NULL;\n-      bool is_klass = t->isa_klassptr() != NULL;\n+      bool is_oop   = t->isa_oopptr() != nullptr;\n+      bool is_klass = t->isa_klassptr() != nullptr;\n@@ -3288,1 +3295,1 @@\n-        Node* nn = NULL;\n+        Node* nn = nullptr;\n@@ -3297,1 +3304,1 @@\n-          if (m!= NULL && m->Opcode() == op &&\n+          if (m!= nullptr && m->Opcode() == op &&\n@@ -3303,1 +3310,1 @@\n-        if (nn != NULL) {\n+        if (nn != nullptr) {\n@@ -3320,1 +3327,1 @@\n-                assert(out_j == NULL || !out_j->is_AddP() || out_j->in(AddPNode::Base) != addp,\n+                assert(out_j == nullptr || !out_j->is_AddP() || out_j->in(AddPNode::Base) != addp,\n@@ -3344,1 +3351,1 @@\n-    if (n->in(0) != NULL) {\n+    if (n->in(0) != nullptr) {\n@@ -3379,1 +3386,1 @@\n-        \/\/ a narrow oop directly and do implicit NULL check in address:\n+        \/\/ a narrow oop directly and do implicit null check in address:\n@@ -3385,1 +3392,1 @@\n-        \/\/ use it to do implicit NULL check in address:\n+        \/\/ use it to do implicit null check in address:\n@@ -3392,1 +3399,1 @@\n-        \/\/ to keep the information to which NULL check the new DecodeN node\n+        \/\/ to keep the information to which null check the new DecodeN node\n@@ -3423,1 +3430,1 @@\n-      Node* new_in2 = NULL;\n+      Node* new_in2 = nullptr;\n@@ -3438,1 +3445,1 @@\n-          \/\/ will generated code for implicit NULL checks for compressed oops.\n+          \/\/ will generated code for implicit null checks for compressed oops.\n@@ -3444,1 +3451,1 @@\n-          \/\/    CmpP base_reg, NULL\n+          \/\/    CmpP base_reg, nullptr\n@@ -3451,1 +3458,1 @@\n-          \/\/    CmpN narrow_oop_reg, NULL\n+          \/\/    CmpN narrow_oop_reg, nullptr\n@@ -3455,1 +3462,1 @@\n-          \/\/ and the uncommon path (== NULL) will use narrow_oop_reg directly\n+          \/\/ and the uncommon path (== nullptr) will use narrow_oop_reg directly\n@@ -3479,1 +3486,1 @@\n-      if (new_in2 != NULL) {\n+      if (new_in2 != nullptr) {\n@@ -3497,1 +3504,1 @@\n-    assert(n->in(0) == NULL || (UseCompressedOops && !Matcher::narrow_oop_use_complex_address()), \"no control\");\n+    assert(n->in(0) == nullptr || (UseCompressedOops && !Matcher::narrow_oop_use_complex_address()), \"no control\");\n@@ -3532,1 +3539,1 @@\n-        if (non_io_proj  != NULL) {\n+        if (non_io_proj  != nullptr) {\n@@ -3545,1 +3552,1 @@\n-      assert(unique_in != NULL, \"\");\n+      assert(unique_in != nullptr, \"\");\n@@ -3549,1 +3556,1 @@\n-        assert(m != NULL, \"\");\n+        assert(m != nullptr, \"\");\n@@ -3551,1 +3558,1 @@\n-          unique_in = NULL;\n+          unique_in = nullptr;\n@@ -3553,1 +3560,1 @@\n-      if (unique_in != NULL) {\n+      if (unique_in != nullptr) {\n@@ -3713,1 +3720,1 @@\n-      if (t != NULL && t->is_con()) {\n+      if (t != nullptr && t->is_con()) {\n@@ -3719,1 +3726,1 @@\n-        if (t == NULL || t->_lo < 0 || t->_hi > (int)mask) {\n+        if (t == nullptr || t->_lo < 0 || t->_hi > (int)mask) {\n@@ -3773,1 +3780,1 @@\n-          if (k == NULL) {\n+          if (k == nullptr) {\n@@ -3830,2 +3837,2 @@\n-      if (m != NULL && !frc._visited.test_set(m->_idx)) {\n-        if (m->is_SafePoint() && m->as_SafePoint()->jvms() != NULL) {\n+      if (m != nullptr && !frc._visited.test_set(m->_idx)) {\n+        if (m->is_SafePoint() && m->as_SafePoint()->jvms() != nullptr) {\n@@ -3865,1 +3872,1 @@\n-    assert(jvms != NULL, \"sanity\");\n+    assert(jvms != nullptr, \"sanity\");\n@@ -3926,0 +3933,2 @@\n+    \/\/ Do not compile method that is only a trivial infinite loop,\n+    \/\/ since the content of the loop may have been eliminated.\n@@ -3936,1 +3945,1 @@\n-    _expensive_nodes.at(i)->set_req(0, NULL);\n+    _expensive_nodes.at(i)->set_req(0, nullptr);\n@@ -3991,0 +4000,1 @@\n+\n@@ -3993,0 +4003,2 @@\n+        DEBUG_ONLY( n->dump_bfs(1, 0, \"-\"); );\n+        assert(false, \"malformed control flow\");\n@@ -4011,0 +4023,3 @@\n+        DEBUG_ONLY( n->fast_out(j)->dump(); );\n+        DEBUG_ONLY( n->dump_bfs(1, 0, \"-\"); );\n+        assert(false, \"infinite loop\");\n@@ -4029,1 +4044,1 @@\n-        if (in != NULL) {\n+        if (in != nullptr) {\n@@ -4067,1 +4082,1 @@\n-  ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this->method() : NULL;\n+  ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this->method() : nullptr;\n@@ -4090,1 +4105,1 @@\n-      int mcount = (logmd == NULL)? -1: (int)logmd->trap_count(reason);\n+      int mcount = (logmd == nullptr)? -1: (int)logmd->trap_count(reason);\n@@ -4121,1 +4136,1 @@\n-  ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this->method() : NULL;\n+  ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this->method() : nullptr;\n@@ -4225,1 +4240,1 @@\n-      if (in != NULL && !visited.member(in)) {\n+      if (in != nullptr && !visited.member(in)) {\n@@ -4228,1 +4243,1 @@\n-      if (in != NULL && !in->is_top()) {\n+      if (in != nullptr && !in->is_top()) {\n@@ -4245,1 +4260,1 @@\n-      } else if (in == NULL) {\n+      } else if (in == nullptr) {\n@@ -4304,1 +4319,1 @@\n-  if (log() != NULL) {\n+  if (log() != nullptr) {\n@@ -4307,1 +4322,1 @@\n-  if (_failure_reason == NULL) {\n+  if (_failure_reason == nullptr) {\n@@ -4315,1 +4330,1 @@\n-  _root = NULL;  \/\/ flush the graph, too\n+  _root = nullptr;  \/\/ flush the graph, too\n@@ -4326,2 +4341,2 @@\n-    C = NULL;\n-    _log = NULL;\n+    C = nullptr;\n+    _log = nullptr;\n@@ -4329,1 +4344,1 @@\n-  if (_log != NULL) {\n+  if (_log != nullptr) {\n@@ -4342,1 +4357,1 @@\n-    _log = NULL;\n+    _log = nullptr;\n@@ -4356,1 +4371,1 @@\n-  if (_log != NULL) {\n+  if (_log != nullptr) {\n@@ -4419,1 +4434,1 @@\n-  if (sizetype != NULL) index_max = sizetype->_hi - 1;\n+  if (sizetype != nullptr) index_max = sizetype->_hi - 1;\n@@ -4428,1 +4443,1 @@\n-  if (ctrl != NULL) {\n+  if (ctrl != nullptr) {\n@@ -4490,1 +4505,1 @@\n-          (print_inlining_current()->cg() != NULL ||\n+          (print_inlining_current()->cg() != nullptr ||\n@@ -4497,1 +4512,1 @@\n-      if (print_inlining_current()->cg() != NULL) {\n+      if (print_inlining_current()->cg() != nullptr) {\n@@ -4539,1 +4554,1 @@\n-    assert(_print_inlining_list != NULL, \"process_print_inlining should be called only once.\");\n+    assert(_print_inlining_list != nullptr, \"process_print_inlining should be called only once.\");\n@@ -4544,1 +4559,1 @@\n-      DEBUG_ONLY(_print_inlining_list->at_put(i, NULL));\n+      DEBUG_ONLY(_print_inlining_list->at_put(i, nullptr));\n@@ -4548,1 +4563,1 @@\n-    _print_inlining_list = NULL;\n+    _print_inlining_list = nullptr;\n@@ -4559,1 +4574,1 @@\n-  if (_print_inlining_output != NULL) {\n+  if (_print_inlining_output != nullptr) {\n@@ -4565,1 +4580,1 @@\n-  if (log() != NULL) {\n+  if (log() != nullptr) {\n@@ -4569,1 +4584,1 @@\n-    while (p != NULL) {\n+    while (p != nullptr) {\n@@ -4579,1 +4594,1 @@\n-  if (log() != NULL) {\n+  if (log() != nullptr) {\n@@ -4585,1 +4600,1 @@\n-  if (log() != NULL) {\n+  if (log() != nullptr) {\n@@ -4600,1 +4615,1 @@\n-  if (C->log() != NULL) {\n+  if (C->log() != nullptr) {\n@@ -4610,1 +4625,1 @@\n-  if (inl_tree != NULL) {\n+  if (inl_tree != nullptr) {\n@@ -4620,1 +4635,1 @@\n-  if (inl_tree == NULL) {\n+  if (inl_tree == nullptr) {\n@@ -4740,1 +4755,1 @@\n-      igvn.replace_input_of(n, 0, NULL);\n+      igvn.replace_input_of(n, 0, nullptr);\n@@ -4749,1 +4764,1 @@\n-    igvn.replace_input_of(n, 0, NULL);\n+    igvn.replace_input_of(n, 0, nullptr);\n@@ -4768,1 +4783,1 @@\n-    n->set_req(0, NULL);\n+    n->set_req(0, nullptr);\n@@ -4916,1 +4931,1 @@\n-      assert((t == NULL) || (t == t->remove_speculative()), \"no more speculative types\");\n+      assert((t == nullptr) || (t == t->remove_speculative()), \"no more speculative types\");\n@@ -5078,1 +5093,1 @@\n-  if (_method != NULL && should_print_igv(1)) {\n+  if (_method != nullptr && should_print_igv(1)) {\n@@ -5093,1 +5108,1 @@\n-  if (_method != NULL && should_print_igv(1)) {\n+  if (_method != nullptr && should_print_igv(1)) {\n@@ -5126,2 +5141,2 @@\n-IdealGraphPrinter* Compile::_debug_file_printer = NULL;\n-IdealGraphPrinter* Compile::_debug_network_printer = NULL;\n+IdealGraphPrinter* Compile::_debug_file_printer = nullptr;\n+IdealGraphPrinter* Compile::_debug_network_printer = nullptr;\n@@ -5179,1 +5194,1 @@\n-  if (_debug_file_printer == NULL) {\n+  if (_debug_file_printer == nullptr) {\n@@ -5189,1 +5204,1 @@\n-  if (_debug_network_printer == NULL) {\n+  if (_debug_network_printer == nullptr) {\n@@ -5200,1 +5215,1 @@\n-  if (type != NULL && phase->type(value)->higher_equal(type)) {\n+  if (type != nullptr && phase->type(value)->higher_equal(type)) {\n@@ -5203,1 +5218,1 @@\n-  Node* result = NULL;\n+  Node* result = nullptr;\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":241,"deletions":226,"binary":false,"changes":467,"status":"modified"},{"patch":"@@ -149,1 +149,1 @@\n-  void insert(node_idx_t key, uint64_t val)      { assert(_dict->operator[](_2p(key)) == NULL, \"key existed\"); _dict->Insert(_2p(key), (void*)val); }\n+  void insert(node_idx_t key, uint64_t val)      { assert(_dict->operator[](_2p(key)) == nullptr, \"key existed\"); _dict->Insert(_2p(key), (void*)val); }\n@@ -219,1 +219,1 @@\n-  \/\/ Variant of TraceTime(NULL, &_t_accumulator, CITime);\n+  \/\/ Variant of TraceTime(nullptr, &_t_accumulator, CITime);\n@@ -268,1 +268,1 @@\n-      assert(_element == NULL, \"\");\n+      assert(_element == nullptr, \"\");\n@@ -294,3 +294,3 @@\n-  address               _stub_function;         \/\/ VM entry for stub being compiled, or NULL\n-  const char*           _stub_name;             \/\/ Name of stub or adapter being compiled, or NULL\n-  address               _stub_entry_point;      \/\/ Compile code entry for generated stub, or NULL\n+  address               _stub_function;         \/\/ VM entry for stub being compiled, or null\n+  const char*           _stub_name;             \/\/ Name of stub or adapter being compiled, or null\n+  address               _stub_entry_point;      \/\/ Compile code entry for generated stub, or null\n@@ -380,1 +380,1 @@\n-  RootNode*             _root;                  \/\/ Unique root of compilation, or NULL after bail-out.\n+  RootNode*             _root;                  \/\/ Unique root of compilation, or null after bail-out.\n@@ -441,1 +441,1 @@\n-      : _cg(NULL), _ss(default_stream_buffer_size) {}\n+      : _cg(nullptr), _ss(default_stream_buffer_size) {}\n@@ -488,1 +488,1 @@\n-  void print_inlining(ciMethod* method, int inline_level, int bci, const char* msg = NULL) {\n+  void print_inlining(ciMethod* method, int inline_level, int bci, const char* msg = nullptr) {\n@@ -558,3 +558,3 @@\n-  bool              is_method_compilation() const { return (_method != NULL && !_method->flags().is_native()); }\n-  const TypeFunc*   tf() const                  { assert(_tf!=NULL, \"\"); return _tf; }\n-  void         init_tf(const TypeFunc* tf)      { assert(_tf==NULL, \"\"); _tf = tf; }\n+  bool              is_method_compilation() const { return (_method != nullptr && !_method->flags().is_native()); }\n+  const TypeFunc*   tf() const                  { assert(_tf!=nullptr, \"\"); return _tf; }\n+  void         init_tf(const TypeFunc* tf)      { assert(_tf==nullptr, \"\"); _tf = tf; }\n@@ -641,1 +641,1 @@\n-    return method() != NULL && method()->has_option(option);\n+    return method() != nullptr && method()->has_option(option);\n@@ -762,1 +762,1 @@\n-  bool        failing() const        { return _env->failing() || _failure_reason != NULL; }\n+  bool        failing() const        { return _env->failing() || _failure_reason != nullptr; }\n@@ -766,1 +766,1 @@\n-    return (r == _failure_reason) || (r != NULL && _failure_reason != NULL && strcmp(r, _failure_reason) == 0);\n+    return (r == _failure_reason) || (r != nullptr && _failure_reason != nullptr && strcmp(r, _failure_reason) == 0);\n@@ -828,1 +828,1 @@\n-  bool                  has_mach_constant_base_node() const { return _mach_constant_base_node != NULL; }\n+  bool                  has_mach_constant_base_node() const { return _mach_constant_base_node != nullptr; }\n@@ -872,1 +872,1 @@\n-    return (m == _last_tf_m) ? _last_tf : NULL;\n+    return (m == _last_tf_m) ? _last_tf : nullptr;\n@@ -875,1 +875,1 @@\n-    assert(m != NULL || tf == NULL, \"\");\n+    assert(m != nullptr || tf == nullptr, \"\");\n@@ -881,1 +881,1 @@\n-  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = NULL) { return find_alias_type(adr_type, false, field); }\n+  AliasType*        alias_type(const TypePtr* adr_type, ciField* field = nullptr) { return find_alias_type(adr_type, false, field); }\n@@ -897,1 +897,1 @@\n-                                   JVMState* jvms, bool allow_inline, float profile_factor, ciKlass* speculative_receiver_type = NULL,\n+                                   JVMState* jvms, bool allow_inline, float profile_factor, ciKlass* speculative_receiver_type = nullptr,\n@@ -927,1 +927,1 @@\n-                      ciMethodData* logmd = NULL);\n+                      ciMethodData* logmd = nullptr);\n@@ -1069,1 +1069,1 @@\n-  bool has_method() { return method() != NULL; }\n+  bool has_method() { return method() != nullptr; }\n@@ -1188,1 +1188,1 @@\n-                              Node* ctrl = NULL);\n+                              Node* ctrl = nullptr);\n","filename":"src\/hotspot\/share\/opto\/compile.hpp","additions":23,"deletions":23,"binary":false,"changes":46,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -89,1 +89,1 @@\n-  LockNode* lock = NULL;\n+  LockNode* lock = nullptr;\n@@ -99,2 +99,2 @@\n-          if ((unique_lock != NULL) && alock->is_Lock()) {\n-            if (lock == NULL) {\n+          if ((unique_lock != nullptr) && alock->is_Lock()) {\n+            if (lock == nullptr) {\n@@ -105,1 +105,1 @@\n-              if (bad_lock != NULL) {\n+              if (bad_lock != nullptr) {\n@@ -111,1 +111,1 @@\n-          if (bad_lock != NULL) {\n+          if (bad_lock != nullptr) {\n@@ -135,1 +135,1 @@\n-  if (unique_lock != NULL && has_one_lock) {\n+  if (unique_lock != nullptr && has_one_lock) {\n","filename":"src\/hotspot\/share\/opto\/locknode.cpp","additions":7,"deletions":7,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2005, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2005, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -78,1 +78,1 @@\n-    } else if (j >= req && uin == NULL) {\n+    } else if (j >= req && uin == nullptr) {\n@@ -86,1 +86,1 @@\n-  assert(old != NULL, \"sanity\");\n+  assert(old != nullptr, \"sanity\");\n@@ -146,3 +146,3 @@\n-  if (parm0 != NULL)  call->init_req(TypeFunc::Parms+0, parm0);\n-  if (parm1 != NULL)  call->init_req(TypeFunc::Parms+1, parm1);\n-  if (parm2 != NULL)  call->init_req(TypeFunc::Parms+2, parm2);\n+  if (parm0 != nullptr)  call->init_req(TypeFunc::Parms+0, parm0);\n+  if (parm1 != nullptr)  call->init_req(TypeFunc::Parms+1, parm1);\n+  if (parm2 != nullptr)  call->init_req(TypeFunc::Parms+2, parm2);\n@@ -193,1 +193,1 @@\n-        ArrayCopyNode* ac = NULL;\n+        ArrayCopyNode* ac = nullptr;\n@@ -195,1 +195,1 @@\n-          if (ac != NULL) {\n+          if (ac != nullptr) {\n@@ -233,1 +233,1 @@\n-        if (init != NULL) {\n+        if (init != nullptr) {\n@@ -242,1 +242,1 @@\n-      Node* adr = NULL;\n+      Node* adr = nullptr;\n@@ -255,1 +255,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -265,1 +265,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -284,1 +284,1 @@\n-  Node* res = NULL;\n+  Node* res = nullptr;\n@@ -302,2 +302,2 @@\n-      Node* adr = NULL;\n-      const TypePtr* adr_type = NULL;\n+      Node* adr = nullptr;\n+      const TypePtr* adr_type = nullptr;\n@@ -327,1 +327,1 @@\n-          return NULL;\n+          return nullptr;\n@@ -335,1 +335,1 @@\n-  if (res != NULL) {\n+  if (res != nullptr) {\n@@ -342,1 +342,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -349,1 +349,1 @@\n-\/\/ Returns the computed Phi, or NULL if it cannot compute it.\n+\/\/ Returns the computed Phi, or null if it cannot compute it.\n@@ -367,1 +367,1 @@\n-  if (new_phi != NULL)\n+  if (new_phi != nullptr)\n@@ -371,1 +371,1 @@\n-    return NULL; \/\/ Give up: phi tree too deep\n+    return nullptr; \/\/ Give up: phi tree too deep\n@@ -377,1 +377,1 @@\n-  GrowableArray <Node *> values(length, length, NULL);\n+  GrowableArray <Node *> values(length, length, nullptr);\n@@ -380,1 +380,1 @@\n-  PhiNode *phi = new PhiNode(mem->in(0), phi_type, NULL, mem->_idx, instance_id, alias_idx, offset);\n+  PhiNode *phi = new PhiNode(mem->in(0), phi_type, nullptr, mem->_idx, instance_id, alias_idx, offset);\n@@ -386,1 +386,1 @@\n-    if (in == NULL || in->is_top()) {\n+    if (in == nullptr || in->is_top()) {\n@@ -398,2 +398,2 @@\n-      if (val == NULL) {\n-        return NULL;  \/\/ can't find a value on this path\n+      if (val == nullptr) {\n+        return nullptr;  \/\/ can't find a value on this path\n@@ -415,2 +415,2 @@\n-        if (val == NULL) {\n-          return NULL;\n+        if (val == nullptr) {\n+          return nullptr;\n@@ -424,1 +424,1 @@\n-        return NULL;\n+        return nullptr;\n@@ -427,2 +427,2 @@\n-        if (res == NULL) {\n-          return NULL;\n+        if (res == nullptr) {\n+          return nullptr;\n@@ -434,1 +434,1 @@\n-        return NULL;  \/\/ unknown node on this path\n+        return nullptr;  \/\/ unknown node on this path\n@@ -466,1 +466,1 @@\n-      return NULL;  \/\/ found a loop, give up\n+      return nullptr;  \/\/ found a loop, give up\n@@ -473,1 +473,1 @@\n-      if (mem == NULL) {\n+      if (mem == nullptr) {\n@@ -482,1 +482,1 @@\n-      assert(atype != NULL, \"address type must be oopptr\");\n+      assert(atype != nullptr, \"address type must be oopptr\");\n@@ -489,1 +489,1 @@\n-      Node *unique_input = NULL;\n+      Node *unique_input = nullptr;\n@@ -493,1 +493,1 @@\n-        if (n == NULL || n == top || n == mem) {\n+        if (n == nullptr || n == top || n == mem) {\n@@ -495,1 +495,1 @@\n-        } else if (unique_input == NULL) {\n+        } else if (unique_input == nullptr) {\n@@ -502,1 +502,1 @@\n-      if (unique_input != NULL && unique_input != top) {\n+      if (unique_input != nullptr && unique_input != top) {\n@@ -514,1 +514,1 @@\n-  if (mem != NULL) {\n+  if (mem != nullptr) {\n@@ -527,1 +527,1 @@\n-      if (phi != NULL) {\n+      if (phi != nullptr) {\n@@ -549,1 +549,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -556,2 +556,2 @@\n-  NOT_PRODUCT( const char* fail_eliminate = NULL; )\n-  DEBUG_ONLY( Node* disq_node = NULL; )\n+  NOT_PRODUCT( const char* fail_eliminate = nullptr; )\n+  DEBUG_ONLY( Node* disq_node = nullptr; )\n@@ -561,2 +561,2 @@\n-  const TypeOopPtr* res_type = NULL;\n-  if (res == NULL) {\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res == nullptr) {\n@@ -569,1 +569,1 @@\n-    if (res_type == NULL) {\n+    if (res_type == nullptr) {\n@@ -581,1 +581,1 @@\n-  if (can_eliminate && res != NULL) {\n+  if (can_eliminate && res != nullptr) {\n@@ -625,1 +625,1 @@\n-        if (sfptMem == NULL || sfptMem->is_top()) {\n+        if (sfptMem == nullptr || sfptMem->is_top()) {\n@@ -627,1 +627,1 @@\n-          NOT_PRODUCT(fail_eliminate = \"NULL or TOP memory\";)\n+          NOT_PRODUCT(fail_eliminate = \"null or TOP memory\";)\n@@ -657,1 +657,1 @@\n-      if (res == NULL)\n+      if (res == nullptr)\n@@ -663,1 +663,1 @@\n-      if (res == NULL)\n+      if (res == nullptr)\n@@ -668,1 +668,1 @@\n-      if (disq_node != NULL) {\n+      if (disq_node != nullptr) {\n@@ -683,1 +683,1 @@\n-  ciInstanceKlass* iklass = NULL;\n+  ciInstanceKlass* iklass = nullptr;\n@@ -688,1 +688,1 @@\n-  const Type* field_type = NULL;\n+  const Type* field_type = nullptr;\n@@ -691,3 +691,3 @@\n-  assert(res == NULL || res->is_CheckCastPP(), \"unexpected AllocateNode result\");\n-  const TypeOopPtr* res_type = NULL;\n-  if (res != NULL) { \/\/ Could be NULL when there are no users\n+  assert(res == nullptr || res->is_CheckCastPP(), \"unexpected AllocateNode result\");\n+  const TypeOopPtr* res_type = nullptr;\n+  if (res != nullptr) { \/\/ Could be null when there are no users\n@@ -697,1 +697,1 @@\n-  if (res != NULL) {\n+  if (res != nullptr) {\n@@ -719,1 +719,1 @@\n-    assert(sfpt->jvms() != NULL, \"missed JVMS\");\n+    assert(sfpt->jvms() != nullptr, \"missed JVMS\");\n@@ -735,2 +735,2 @@\n-      ciField* field = NULL;\n-      if (iklass != NULL) {\n+      ciField* field = nullptr;\n+      if (iklass != nullptr) {\n@@ -738,1 +738,1 @@\n-        offset = field->offset();\n+        offset = field->offset_in_bytes();\n@@ -746,1 +746,1 @@\n-          } else if (field != NULL && field->is_static_constant()) {\n+          } else if (field != nullptr && field->is_static_constant()) {\n@@ -751,1 +751,1 @@\n-            assert(field_type != NULL, \"field singleton type must be consistent\");\n+            assert(field_type != nullptr, \"field singleton type must be consistent\");\n@@ -769,1 +769,1 @@\n-      if (field_val == NULL) {\n+      if (field_val == nullptr) {\n@@ -807,1 +807,1 @@\n-          if (field != NULL) {\n+          if (field != nullptr) {\n@@ -818,1 +818,1 @@\n-          if (res == NULL)\n+          if (res == nullptr)\n@@ -853,1 +853,1 @@\n-  if (ctl_proj != NULL) {\n+  if (ctl_proj != nullptr) {\n@@ -856,1 +856,1 @@\n-  if (mem_proj != NULL) {\n+  if (mem_proj != nullptr) {\n@@ -864,1 +864,1 @@\n-  if (res != NULL) {\n+  if (res != nullptr) {\n@@ -943,1 +943,1 @@\n-  if (_callprojs.resproj != NULL && _callprojs.resproj->outcnt() != 0) {\n+  if (_callprojs.resproj != nullptr && _callprojs.resproj->outcnt() != 0) {\n@@ -963,1 +963,1 @@\n-        if (ctrl_proj != NULL) {\n+        if (ctrl_proj != nullptr) {\n@@ -966,1 +966,1 @@\n-          \/\/ If the InitializeNode has no memory out, it will die, and tmp will become NULL\n+          \/\/ If the InitializeNode has no memory out, it will die, and tmp will become null\n@@ -968,1 +968,1 @@\n-          assert(tmp == NULL || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n+          assert(tmp == nullptr || tmp == _callprojs.fallthrough_catchproj, \"allocation control projection\");\n@@ -972,1 +972,1 @@\n-        if (mem_proj != NULL) {\n+        if (mem_proj != nullptr) {\n@@ -989,1 +989,1 @@\n-  if (_callprojs.fallthrough_catchproj != NULL) {\n+  if (_callprojs.fallthrough_catchproj != nullptr) {\n@@ -992,1 +992,1 @@\n-  if (_callprojs.fallthrough_memproj != NULL) {\n+  if (_callprojs.fallthrough_memproj != nullptr) {\n@@ -995,1 +995,1 @@\n-  if (_callprojs.catchall_memproj != NULL) {\n+  if (_callprojs.catchall_memproj != nullptr) {\n@@ -998,1 +998,1 @@\n-  if (_callprojs.fallthrough_ioproj != NULL) {\n+  if (_callprojs.fallthrough_ioproj != nullptr) {\n@@ -1001,1 +1001,1 @@\n-  if (_callprojs.catchall_ioproj != NULL) {\n+  if (_callprojs.catchall_ioproj != nullptr) {\n@@ -1004,1 +1004,1 @@\n-  if (_callprojs.catchall_catchproj != NULL) {\n+  if (_callprojs.catchall_catchproj != nullptr) {\n@@ -1026,1 +1026,1 @@\n-  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != NULL))) {\n+  if (!alloc->_is_scalar_replaceable && (!boxing_alloc || (res != nullptr))) {\n@@ -1038,1 +1038,1 @@\n-    assert(res == NULL, \"sanity\");\n+    assert(res == nullptr, \"sanity\");\n@@ -1052,1 +1052,1 @@\n-  if (log != NULL) {\n+  if (log != nullptr) {\n@@ -1056,1 +1056,1 @@\n-    while (p != NULL) {\n+    while (p != nullptr) {\n@@ -1079,1 +1079,1 @@\n-  if (!C->eliminate_boxing() || boxing->proj_out_or_null(TypeFunc::Parms) != NULL) {\n+  if (!C->eliminate_boxing() || boxing->proj_out_or_null(TypeFunc::Parms) != nullptr) {\n@@ -1083,1 +1083,1 @@\n-  assert(boxing->result_cast() == NULL, \"unexpected boxing node result\");\n+  assert(boxing->result_cast() == nullptr, \"unexpected boxing node result\");\n@@ -1090,1 +1090,1 @@\n-  assert(t != NULL, \"sanity\");\n+  assert(t != nullptr, \"sanity\");\n@@ -1093,1 +1093,1 @@\n-  if (log != NULL) {\n+  if (log != nullptr) {\n@@ -1097,1 +1097,1 @@\n-    while (p != NULL) {\n+    while (p != nullptr) {\n@@ -1129,1 +1129,1 @@\n-  mem = StoreNode::make(_igvn, ctl, mem, adr, NULL, value, bt, MemNode::unordered);\n+  mem = StoreNode::make(_igvn, ctl, mem, adr, nullptr, value, bt, MemNode::unordered);\n@@ -1202,1 +1202,1 @@\n-  assert(ctrl != NULL, \"must have control\");\n+  assert(ctrl != nullptr, \"must have control\");\n@@ -1207,4 +1207,4 @@\n-  Node *result_region = NULL;\n-  Node *result_phi_rawmem = NULL;\n-  Node *result_phi_rawoop = NULL;\n-  Node *result_phi_i_o = NULL;\n+  Node *result_region = nullptr;\n+  Node *result_phi_rawmem = nullptr;\n+  Node *result_phi_rawoop = nullptr;\n+  Node *result_phi_i_o = nullptr;\n@@ -1222,1 +1222,1 @@\n-    initial_slow_test = NULL;\n+    initial_slow_test = nullptr;\n@@ -1230,1 +1230,1 @@\n-    initial_slow_test = NULL;\n+    initial_slow_test = nullptr;\n@@ -1233,1 +1233,1 @@\n-  bool allocation_has_use = (alloc->result_cast() != NULL);\n+  bool allocation_has_use = (alloc->result_cast() != nullptr);\n@@ -1236,1 +1236,1 @@\n-    if (init != NULL) {\n+    if (init != nullptr) {\n@@ -1239,1 +1239,1 @@\n-    if (expand_fast_path && (initial_slow_test == NULL)) {\n+    if (expand_fast_path && (initial_slow_test == nullptr)) {\n@@ -1247,1 +1247,1 @@\n-        if (res != NULL) {\n+        if (res != nullptr) {\n@@ -1260,1 +1260,1 @@\n-  Node *slow_region = NULL;\n+  Node *slow_region = nullptr;\n@@ -1264,1 +1264,1 @@\n-  if (initial_slow_test != NULL ) {\n+  if (initial_slow_test != nullptr ) {\n@@ -1317,1 +1317,1 @@\n-      Node* needgc_ctrl = NULL;\n+      Node* needgc_ctrl = nullptr;\n@@ -1320,1 +1320,1 @@\n-      intx prefetch_lines = length != NULL ? AllocatePrefetchLines : AllocateInstancePrefetchLines;\n+      intx prefetch_lines = length != nullptr ? AllocatePrefetchLines : AllocateInstancePrefetchLines;\n@@ -1326,1 +1326,1 @@\n-      if (initial_slow_test != NULL) {\n+      if (initial_slow_test != nullptr) {\n@@ -1345,1 +1345,1 @@\n-      assert (initial_slow_test != NULL, \"sanity\");\n+      assert (initial_slow_test != nullptr, \"sanity\");\n@@ -1371,1 +1371,1 @@\n-  if (length != NULL) {\n+  if (length != nullptr) {\n@@ -1381,1 +1381,1 @@\n-  if (valid_length_test != NULL) {\n+  if (valid_length_test != nullptr) {\n@@ -1410,1 +1410,1 @@\n-  if (expand_fast_path && _callprojs.fallthrough_memproj != NULL) {\n+  if (expand_fast_path && _callprojs.fallthrough_memproj != nullptr) {\n@@ -1415,2 +1415,2 @@\n-  if (_callprojs.catchall_memproj != NULL ) {\n-    if (_callprojs.fallthrough_memproj == NULL) {\n+  if (_callprojs.catchall_memproj != nullptr ) {\n+    if (_callprojs.fallthrough_memproj == nullptr) {\n@@ -1429,1 +1429,1 @@\n-  if (_callprojs.fallthrough_ioproj != NULL) {\n+  if (_callprojs.fallthrough_ioproj != nullptr) {\n@@ -1434,2 +1434,2 @@\n-  if (_callprojs.catchall_ioproj != NULL ) {\n-    if (_callprojs.fallthrough_ioproj == NULL) {\n+  if (_callprojs.catchall_ioproj != nullptr ) {\n+    if (_callprojs.fallthrough_ioproj == nullptr) {\n@@ -1459,1 +1459,1 @@\n-  if (_callprojs.fallthrough_catchproj != NULL) {\n+  if (_callprojs.fallthrough_catchproj != nullptr) {\n@@ -1467,1 +1467,1 @@\n-  if (_callprojs.resproj == NULL) {\n+  if (_callprojs.resproj == nullptr) {\n@@ -1496,1 +1496,1 @@\n-  if (_callprojs.resproj != NULL) {\n+  if (_callprojs.resproj != nullptr) {\n@@ -1506,1 +1506,1 @@\n-  if (_callprojs.fallthrough_catchproj != NULL) {\n+  if (_callprojs.fallthrough_catchproj != nullptr) {\n@@ -1510,1 +1510,1 @@\n-  if (_callprojs.catchall_catchproj != NULL) {\n+  if (_callprojs.catchall_catchproj != nullptr) {\n@@ -1514,1 +1514,1 @@\n-  if (_callprojs.fallthrough_proj != NULL) {\n+  if (_callprojs.fallthrough_proj != nullptr) {\n@@ -1519,1 +1519,1 @@\n-  if (_callprojs.fallthrough_memproj != NULL) {\n+  if (_callprojs.fallthrough_memproj != nullptr) {\n@@ -1523,1 +1523,1 @@\n-  if (_callprojs.fallthrough_ioproj != NULL) {\n+  if (_callprojs.fallthrough_ioproj != nullptr) {\n@@ -1527,1 +1527,1 @@\n-  if (_callprojs.catchall_memproj != NULL) {\n+  if (_callprojs.catchall_memproj != nullptr) {\n@@ -1531,1 +1531,1 @@\n-  if (_callprojs.catchall_ioproj != NULL) {\n+  if (_callprojs.catchall_ioproj != nullptr) {\n@@ -1565,2 +1565,2 @@\n-    (init == NULL || !init->is_complete_with_arraycopy())) {\n-    if (init == NULL || init->req() < InitializeNode::RawStores) {\n+    (init == nullptr || !init->is_complete_with_arraycopy())) {\n+    if (init == nullptr || init->req() < InitializeNode::RawStores) {\n@@ -1609,1 +1609,1 @@\n-      if (init_ctrl != NULL) {\n+      if (init_ctrl != nullptr) {\n@@ -1612,1 +1612,1 @@\n-      if (init_mem != NULL) {\n+      if (init_mem != nullptr) {\n@@ -1671,1 +1671,1 @@\n-  if (length != NULL) {         \/\/ Arrays need length field\n+  if (length != nullptr) {         \/\/ Arrays need length field\n@@ -1685,1 +1685,1 @@\n-  if (init == NULL) {\n+  if (init == nullptr) {\n@@ -1871,1 +1871,1 @@\n-  expand_allocate_common(alloc, NULL,\n+  expand_allocate_common(alloc, nullptr,\n@@ -1873,1 +1873,1 @@\n-                         OptoRuntime::new_instance_Java(), NULL);\n+                         OptoRuntime::new_instance_Java(), nullptr);\n@@ -1883,2 +1883,2 @@\n-  if (init != NULL && init->is_complete_with_arraycopy() &&\n-      ary_klass_t && ary_klass_t->elem()->isa_klassptr() == NULL) {\n+  if (init != nullptr && init->is_complete_with_arraycopy() &&\n+      ary_klass_t && ary_klass_t->elem()->isa_klassptr() == nullptr) {\n@@ -1917,1 +1917,1 @@\n-      oldbox->as_BoxLock()->is_simple_lock_region(NULL, obj, NULL)) {\n+      oldbox->as_BoxLock()->is_simple_lock_region(nullptr, obj, nullptr)) {\n@@ -2009,1 +2009,1 @@\n-      if (alock->jvms() != NULL) {\n+      if (alock->jvms() != nullptr) {\n@@ -2037,1 +2037,1 @@\n-          if (C->log() != NULL)\n+          if (C->log() != nullptr)\n@@ -2095,1 +2095,1 @@\n-  guarantee(ctrl != NULL, \"missing control projection, cannot replace_node() with NULL\");\n+  guarantee(ctrl != nullptr, \"missing control projection, cannot replace_node() with null\");\n@@ -2101,2 +2101,2 @@\n-         _callprojs.fallthrough_proj != NULL &&\n-         _callprojs.fallthrough_memproj != NULL,\n+         _callprojs.fallthrough_proj != nullptr &&\n+         _callprojs.fallthrough_memproj != nullptr,\n@@ -2114,1 +2114,1 @@\n-    assert(membar != NULL && membar->Opcode() == Op_MemBarAcquireLock, \"\");\n+    assert(membar != nullptr && membar->Opcode() == Op_MemBarAcquireLock, \"\");\n@@ -2174,2 +2174,2 @@\n-                                  OptoRuntime::complete_monitor_locking_Java(), NULL, slow_path,\n-                                  obj, box, NULL);\n+                                  OptoRuntime::complete_monitor_locking_Java(), nullptr, slow_path,\n+                                  obj, box, nullptr);\n@@ -2183,2 +2183,2 @@\n-  assert(_callprojs.fallthrough_ioproj == NULL && _callprojs.catchall_ioproj == NULL &&\n-         _callprojs.catchall_memproj == NULL && _callprojs.catchall_catchproj == NULL, \"Unexpected projection from Lock\");\n+  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n+         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2238,2 +2238,2 @@\n-  assert(_callprojs.fallthrough_ioproj == NULL && _callprojs.catchall_ioproj == NULL &&\n-         _callprojs.catchall_memproj == NULL && _callprojs.catchall_catchproj == NULL, \"Unexpected projection from Lock\");\n+  assert(_callprojs.fallthrough_ioproj == nullptr && _callprojs.catchall_ioproj == nullptr &&\n+         _callprojs.catchall_memproj == nullptr && _callprojs.catchall_catchproj == nullptr, \"Unexpected projection from Lock\");\n@@ -2263,1 +2263,1 @@\n-  assert(check->in(SubTypeCheckNode::Control) == NULL, \"should be pinned\");\n+  assert(check->in(SubTypeCheckNode::Control) == nullptr, \"should be pinned\");\n@@ -2282,1 +2282,1 @@\n-    Node* subklass = NULL;\n+    Node* subklass = nullptr;\n@@ -2287,1 +2287,1 @@\n-      subklass = _igvn.transform(LoadKlassNode::make(_igvn, NULL, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n+      subklass = _igvn.transform(LoadKlassNode::make(_igvn, nullptr, C->immutable_memory(), k_adr, TypeInstPtr::KLASS));\n@@ -2290,1 +2290,1 @@\n-    Node* not_subtype_ctrl = Phase::gen_subtype_check(subklass, superklass, &ctrl, NULL, _igvn);\n+    Node* not_subtype_ctrl = Phase::gen_subtype_check(subklass, superklass, &ctrl, nullptr, _igvn);\n@@ -2379,0 +2379,2 @@\n+               n->Opcode() == Op_MaxL      ||\n+               n->Opcode() == Op_MinL      ||\n@@ -2435,1 +2437,1 @@\n-               ifn->proj_out(1)->is_uncommon_trap_proj(Deoptimization::Reason_rtm_state_change) != NULL, \"\");\n+               ifn->proj_out(1)->is_uncommon_trap_proj(Deoptimization::Reason_rtm_state_change) != nullptr, \"\");\n@@ -2463,0 +2465,12 @@\n+      } else if (n->Opcode() == Op_MaxL) {\n+        \/\/ Since MaxL and MinL are not implemented in the backend, we expand them to\n+        \/\/ a CMoveL construct now. At least until here, the type could be computed\n+        \/\/ precisely. CMoveL is not so smart, but we can give it at least the best\n+        \/\/ type we know abouot n now.\n+        Node* repl = MaxNode::signed_max(n->in(1), n->in(2), _igvn.type(n), _igvn);\n+        _igvn.replace_node(n, repl);\n+        success = true;\n+      } else if (n->Opcode() == Op_MinL) {\n+        Node* repl = MaxNode::signed_min(n->in(1), n->in(2), _igvn.type(n), _igvn);\n+        _igvn.replace_node(n, repl);\n+        success = true;\n@@ -2492,1 +2506,1 @@\n-    if (_igvn.type(n) == Type::TOP || (n->in(0) != NULL && n->in(0)->is_top())) {\n+    if (_igvn.type(n) == Type::TOP || (n->in(0) != nullptr && n->in(0)->is_top())) {\n@@ -2545,1 +2559,1 @@\n-    if (_igvn.type(n) == Type::TOP || (n->in(0) != NULL && n->in(0)->is_top())) {\n+    if (_igvn.type(n) == Type::TOP || (n->in(0) != nullptr && n->in(0)->is_top())) {\n@@ -2600,1 +2614,1 @@\n-  ideal_nodes.map(C->live_nodes(), NULL);\n+  ideal_nodes.map(C->live_nodes(), nullptr);\n","filename":"src\/hotspot\/share\/opto\/macro.cpp","additions":172,"deletions":158,"binary":false,"changes":330,"status":"modified"},{"patch":"@@ -81,1 +81,1 @@\n-    tty->print_cr(\"%d original NULL checks - %d elided (%2d%%); optimizer leaves %d,\",\n+    tty->print_cr(\"%d original null checks - %d elided (%2d%%); optimizer leaves %d,\",\n@@ -115,1 +115,1 @@\n-  Node *l = NULL;\n+  Node *l = nullptr;\n@@ -155,1 +155,1 @@\n-      (tp != NULL && !tp->is_loaded())) {\n+      (tp != nullptr && !tp->is_loaded())) {\n@@ -171,1 +171,1 @@\n-  if (tp != NULL && !tp->is_same_java_type_as(TypeInstPtr::BOTTOM)) {\n+  if (tp != nullptr && !tp->is_same_java_type_as(TypeInstPtr::BOTTOM)) {\n@@ -173,1 +173,1 @@\n-    Node* bad_type_ctrl = NULL;\n+    Node* bad_type_ctrl = nullptr;\n@@ -216,0 +216,1 @@\n+    assert(false, \"OSR starts with an immediate trap\");\n@@ -256,0 +257,1 @@\n+    assert(false, \"OSR in empty or breakpointed method\");\n@@ -273,1 +275,1 @@\n-    if (type->isa_oopptr() != NULL) {\n+    if (type->isa_oopptr() != nullptr) {\n@@ -282,1 +284,1 @@\n-        if (C->log() != NULL) {\n+        if (C->log() != nullptr) {\n@@ -304,1 +306,1 @@\n-      \/\/ Ptr types are mixed together with T_ADDRESS but NULL is\n+      \/\/ Ptr types are mixed together with T_ADDRESS but null is\n@@ -339,1 +341,1 @@\n-    if (type->isa_oopptr() != NULL) {\n+    if (type->isa_oopptr() != nullptr) {\n@@ -401,1 +403,1 @@\n-  _alloc_with_final = NULL;\n+  _alloc_with_final = nullptr;\n@@ -403,2 +405,2 @@\n-  _tf = NULL;\n-  _block = NULL;\n+  _tf = nullptr;\n+  _block = nullptr;\n@@ -433,0 +435,1 @@\n+    assert(false, \"type flow failed during parsing\");\n@@ -455,1 +458,1 @@\n-  if (log != NULL) {\n+  if (log != nullptr) {\n@@ -481,1 +484,1 @@\n-      if (log != NULL)\n+      if (log != nullptr)\n@@ -490,1 +493,1 @@\n-  if (log != NULL && method()->has_exception_handlers()) {\n+  if (log != nullptr && method()->has_exception_handlers()) {\n@@ -494,1 +497,1 @@\n-  assert(InlineTree::check_can_parse(method()) == NULL, \"Can not parse this method, cutout earlier\");\n+  assert(InlineTree::check_can_parse(method()) == nullptr, \"Can not parse this method, cutout earlier\");\n@@ -511,0 +514,1 @@\n+      assert(false, \"type flow analysis failed for OSR compilation\");\n@@ -560,1 +564,1 @@\n-  if (failing() || entry_map == NULL) {\n+  if (failing() || entry_map == nullptr) {\n@@ -616,2 +620,0 @@\n-  C->set_default_node_notes(caller_nn);\n-\n@@ -628,0 +630,4 @@\n+  \/\/ Only reset this now, to make sure that debug information emitted\n+  \/\/ for exiting control flow still refers to the inlined method.\n+  C->set_default_node_notes(caller_nn);\n+\n@@ -814,1 +820,1 @@\n-\/\/ unknown caller.  The method & bci will be NULL & InvocationEntryBci.\n+\/\/ unknown caller.  The method & bci will be null & InvocationEntryBci.\n@@ -823,1 +829,1 @@\n-  if (old_nn != NULL && has_method()) {\n+  if (old_nn != nullptr && has_method()) {\n@@ -849,1 +855,1 @@\n-  if (caller_nn == NULL)  return NULL;\n+  if (caller_nn == nullptr)  return nullptr;\n@@ -911,1 +917,1 @@\n-    while (pop_exception_state() != NULL) ;\n+    while (pop_exception_state() != nullptr) ;\n@@ -918,1 +924,1 @@\n-  while ((ex_map = pop_exception_state()) != NULL) {\n+  while ((ex_map = pop_exception_state()) != nullptr) {\n@@ -1046,0 +1052,10 @@\n+#ifdef ASSERT\n+      tty->print_cr(\"# Can't determine return type.\");\n+      tty->print_cr(\"# exit control\");\n+      _exits.control()->dump(2);\n+      tty->print_cr(\"# ret phi type\");\n+      _gvn.type(ret_phi)->dump();\n+      tty->print_cr(\"# ret phi\");\n+      ret_phi->dump(2);\n+#endif \/\/ ASSERT\n+      assert(false, \"Can't determine return type.\");\n@@ -1071,1 +1087,1 @@\n-    while ((ex_map = kit.pop_exception_state()) != NULL) {\n+    while ((ex_map = kit.pop_exception_state()) != nullptr) {\n@@ -1106,1 +1122,1 @@\n-    while ((ex_map = caller.pop_exception_state()) != NULL) {\n+    while ((ex_map = caller.pop_exception_state()) != nullptr) {\n@@ -1121,0 +1137,1 @@\n+    \/\/ Bailout expected, this is a very rare edge case.\n@@ -1122,1 +1139,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -1136,1 +1153,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -1140,1 +1157,1 @@\n-  assert(method() != NULL, \"parser must have a method\");\n+  assert(method() != nullptr, \"parser must have a method\");\n@@ -1143,1 +1160,1 @@\n-  JVMState* jvms = new (C) JVMState(method(), _caller->has_method() ? _caller : NULL);\n+  JVMState* jvms = new (C) JVMState(method(), _caller->has_method() ? _caller : nullptr);\n@@ -1150,1 +1167,1 @@\n-  assert(inmap != NULL, \"must have inmap\");\n+  assert(inmap != nullptr, \"must have inmap\");\n@@ -1206,1 +1223,1 @@\n-    if (receiver_type != NULL && !receiver_type->higher_equal(holder_type)) {\n+    if (receiver_type != nullptr && !receiver_type->higher_equal(holder_type)) {\n@@ -1236,1 +1253,1 @@\n-    Node *lock_obj = NULL;\n+    Node *lock_obj = nullptr;\n@@ -1284,1 +1301,1 @@\n-  _start_map = NULL;\n+  _start_map = nullptr;\n@@ -1288,1 +1305,1 @@\n-  _successors = NULL;\n+  _successors = nullptr;\n@@ -1307,1 +1324,1 @@\n-  _successors = (ns+ne == 0) ? NULL : NEW_RESOURCE_ARRAY(Block*, ns+ne);\n+  _successors = (ns+ne == 0) ? nullptr : NEW_RESOURCE_ARRAY(Block*, ns+ne);\n@@ -1350,1 +1367,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1535,1 +1552,1 @@\n-    if (log != NULL) {\n+    if (log != nullptr) {\n@@ -1570,1 +1587,1 @@\n-    if (log != NULL)\n+    if (log != nullptr)\n@@ -1585,1 +1602,1 @@\n-  if (nn == NULL)  return;\n+  if (nn == nullptr)  return;\n@@ -1594,1 +1611,1 @@\n-  if (jvms != NULL && jvms->bci() != bci) {\n+  if (jvms != nullptr && jvms->bci() != bci) {\n@@ -1606,1 +1623,1 @@\n-  if (target == NULL) { handle_missing_successor(target_bci); return; }\n+  if (target == nullptr) { handle_missing_successor(target_bci); return; }\n@@ -1616,1 +1633,1 @@\n-  if (target == NULL) { handle_missing_successor(target_bci); return; }\n+  if (target == nullptr) { handle_missing_successor(target_bci); return; }\n@@ -1633,1 +1650,1 @@\n-  if (target == NULL) { handle_missing_successor(target_bci); return; }\n+  if (target == nullptr) { handle_missing_successor(target_bci); return; }\n@@ -1694,2 +1711,2 @@\n-      \/\/ zap all inputs to NULL for debugging (done in Node(uint) constructor)\n-      \/\/ for (int j = 1; j < edges+1; j++) { r->init_req(j, NULL); }\n+      \/\/ zap all inputs to null for debugging (done in Node(uint) constructor)\n+      \/\/ for (int j = 1; j < edges+1; j++) { r->init_req(j, nullptr); }\n@@ -1755,1 +1772,1 @@\n-        phi = NULL;\n+        phi = nullptr;\n@@ -1763,1 +1780,1 @@\n-          assert(phi == NULL, \"the merge contains phis, not vice versa\");\n+          assert(phi == nullptr, \"the merge contains phis, not vice versa\");\n@@ -1767,1 +1784,1 @@\n-          if (phi == NULL) {\n+          if (phi == nullptr) {\n@@ -1789,1 +1806,1 @@\n-      if (phi != NULL) {\n+      if (phi != nullptr) {\n@@ -1835,1 +1852,1 @@\n-  assert(n != NULL, \"\");\n+  assert(n != nullptr, \"\");\n@@ -1842,2 +1859,2 @@\n-  PhiNode* base = NULL;\n-  MergeMemNode* remerge = NULL;\n+  PhiNode* base = nullptr;\n+  MergeMemNode* remerge = nullptr;\n@@ -1851,3 +1868,3 @@\n-      if (remerge == NULL) {\n-        guarantee(base != NULL, \"\");\n-        assert(base->in(0) != NULL, \"should not be xformed away\");\n+      if (remerge == nullptr) {\n+        guarantee(base != nullptr, \"\");\n+        assert(base->in(0) != nullptr, \"should not be xformed away\");\n@@ -1869,1 +1886,1 @@\n-        phi = NULL;\n+        phi = nullptr;\n@@ -1872,1 +1889,1 @@\n-    if (phi != NULL) {\n+    if (phi != nullptr) {\n@@ -1886,1 +1903,1 @@\n-  if (base != NULL && pnum == 1) {\n+  if (base != nullptr && pnum == 1) {\n@@ -1945,1 +1962,1 @@\n-  r->add_req(NULL);\n+  r->add_req(nullptr);\n@@ -1955,1 +1972,1 @@\n-          phi->add_req(NULL);\n+          phi->add_req(nullptr);\n@@ -1961,1 +1978,1 @@\n-        n->add_req(NULL);\n+        n->add_req(nullptr);\n@@ -1977,1 +1994,1 @@\n-  assert(o != NULL, \"\");\n+  assert(o != nullptr, \"\");\n@@ -1979,1 +1996,1 @@\n-  if (o == top())  return NULL; \/\/ TOP always merges into TOP\n+  if (o == top())  return nullptr; \/\/ TOP always merges into TOP\n@@ -1988,1 +2005,1 @@\n-  const Type* t = NULL;\n+  const Type* t = nullptr;\n@@ -2007,1 +2024,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -2014,1 +2031,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -2032,1 +2049,1 @@\n-  assert(o != NULL && o != top(), \"\");\n+  assert(o != nullptr && o != top(), \"\");\n@@ -2066,1 +2083,1 @@\n-  assert(receiver != NULL && receiver->bottom_type()->isa_instptr() != NULL,\n+  assert(receiver != nullptr && receiver->bottom_type()->isa_instptr() != nullptr,\n@@ -2070,1 +2087,1 @@\n-  if (tinst != NULL && tinst->is_loaded() && !tinst->klass_is_exact()) {\n+  if (tinst != nullptr && tinst->is_loaded() && !tinst->klass_is_exact()) {\n@@ -2084,1 +2101,1 @@\n-  Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), klass_addr, TypeInstPtr::KLASS));\n+  Node* klass = _gvn.transform(LoadKlassNode::make(_gvn, nullptr, immutable_memory(), klass_addr, TypeInstPtr::KLASS));\n@@ -2087,1 +2104,1 @@\n-  Node* access_flags = make_load(NULL, access_flags_addr, TypeInt::INT, T_INT, MemNode::unordered);\n+  Node* access_flags = make_load(nullptr, access_flags_addr, TypeInt::INT, T_INT, MemNode::unordered);\n@@ -2110,1 +2127,1 @@\n-                                   NULL, TypePtr::BOTTOM,\n+                                   nullptr, TypePtr::BOTTOM,\n@@ -2214,1 +2231,1 @@\n-  if (value != NULL) {\n+  if (value != nullptr) {\n@@ -2241,1 +2258,1 @@\n-  SafePointNode *sfpnt = new SafePointNode(parms, NULL);\n+  SafePointNode *sfpnt = new SafePointNode(parms, nullptr);\n@@ -2283,1 +2300,1 @@\n-    assert(C->root() != NULL, \"Expect parse is still valid\");\n+    assert(C->root() != nullptr, \"Expect parse is still valid\");\n@@ -2291,2 +2308,2 @@\n-  InlineTree* ilt = NULL;\n-  if (C->ilt() != NULL) {\n+  InlineTree* ilt = nullptr;\n+  if (C->ilt() != nullptr) {\n@@ -2357,1 +2374,1 @@\n-  if( method() != NULL ) {\n+  if( method() != nullptr ) {\n","filename":"src\/hotspot\/share\/opto\/parse1.cpp","additions":96,"deletions":79,"binary":false,"changes":175,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1998, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1998, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -96,16 +96,21 @@\n-address OptoRuntime::_new_instance_Java                           = NULL;\n-address OptoRuntime::_new_array_Java                              = NULL;\n-address OptoRuntime::_new_array_nozero_Java                       = NULL;\n-address OptoRuntime::_multianewarray2_Java                        = NULL;\n-address OptoRuntime::_multianewarray3_Java                        = NULL;\n-address OptoRuntime::_multianewarray4_Java                        = NULL;\n-address OptoRuntime::_multianewarray5_Java                        = NULL;\n-address OptoRuntime::_multianewarrayN_Java                        = NULL;\n-address OptoRuntime::_vtable_must_compile_Java                    = NULL;\n-address OptoRuntime::_complete_monitor_locking_Java               = NULL;\n-address OptoRuntime::_monitor_notify_Java                         = NULL;\n-address OptoRuntime::_monitor_notifyAll_Java                      = NULL;\n-address OptoRuntime::_rethrow_Java                                = NULL;\n-\n-address OptoRuntime::_slow_arraycopy_Java                         = NULL;\n-address OptoRuntime::_register_finalizer_Java                     = NULL;\n+address OptoRuntime::_new_instance_Java                           = nullptr;\n+address OptoRuntime::_new_array_Java                              = nullptr;\n+address OptoRuntime::_new_array_nozero_Java                       = nullptr;\n+address OptoRuntime::_multianewarray2_Java                        = nullptr;\n+address OptoRuntime::_multianewarray3_Java                        = nullptr;\n+address OptoRuntime::_multianewarray4_Java                        = nullptr;\n+address OptoRuntime::_multianewarray5_Java                        = nullptr;\n+address OptoRuntime::_multianewarrayN_Java                        = nullptr;\n+address OptoRuntime::_vtable_must_compile_Java                    = nullptr;\n+address OptoRuntime::_complete_monitor_locking_Java               = nullptr;\n+address OptoRuntime::_monitor_notify_Java                         = nullptr;\n+address OptoRuntime::_monitor_notifyAll_Java                      = nullptr;\n+address OptoRuntime::_rethrow_Java                                = nullptr;\n+\n+address OptoRuntime::_slow_arraycopy_Java                         = nullptr;\n+address OptoRuntime::_register_finalizer_Java                     = nullptr;\n+#if INCLUDE_JVMTI\n+address OptoRuntime::_notify_jvmti_object_alloc                   = nullptr;\n+address OptoRuntime::_notify_jvmti_mount                          = nullptr;\n+address OptoRuntime::_notify_jvmti_unmount                        = nullptr;\n+#endif\n@@ -133,1 +138,1 @@\n-  if (var == NULL) { return false; }\n+  if (var == nullptr) { return false; }\n@@ -151,0 +156,5 @@\n+#if INCLUDE_JVMTI\n+  gen(env, _notify_jvmti_object_alloc      , notify_jvmti_object_alloc_Type, SharedRuntime::notify_jvmti_object_alloc, 0, true, false);\n+  gen(env, _notify_jvmti_mount             , notify_jvmti_Type            , SharedRuntime::notify_jvmti_mount,   0 , true, false);\n+  gen(env, _notify_jvmti_unmount           , notify_jvmti_Type            , SharedRuntime::notify_jvmti_unmount, 0 , true, false);\n+#endif\n@@ -184,1 +194,1 @@\n-  assert(rs != NULL && rs->is_runtime_stub(), \"not a runtime stub\");\n+  assert(rs != nullptr && rs->is_runtime_stub(), \"not a runtime stub\");\n@@ -306,1 +316,1 @@\n-  if ((len > 0) && (result != NULL) &&\n+  if ((len > 0) && (result != nullptr) &&\n@@ -470,0 +480,16 @@\n+#if INCLUDE_JVMTI\n+const TypeFunc *OptoRuntime::notify_jvmti_object_alloc_Type() {\n+  \/\/ create input type (domain)\n+  const Type **fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL;\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+   \/\/ create result type (range)\n+   fields = TypeTuple::fields(1);\n+   fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ Returned oop\n+\n+   const TypeTuple *range = TypeTuple::make(TypeFunc::Parms+1, fields);\n+\n+   return TypeFunc::make(domain, range);\n+}\n+#endif\n@@ -613,1 +639,1 @@\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -618,1 +644,1 @@\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -785,1 +811,1 @@\n-    fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+    fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -827,1 +853,1 @@\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -848,1 +874,1 @@\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -1028,1 +1054,1 @@\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -1074,1 +1100,1 @@\n-  fields[TypeFunc::Parms+0] = NULL;\n+  fields[TypeFunc::Parms+0] = nullptr;\n@@ -1094,1 +1120,1 @@\n-  fields[TypeFunc::Parms+0] = NULL;\n+  fields[TypeFunc::Parms+0] = nullptr;\n@@ -1182,1 +1208,1 @@\n-  fields[TypeFunc::Parms + 0] = NULL;\n+  fields[TypeFunc::Parms + 0] = nullptr;\n@@ -1222,1 +1248,1 @@\n-    fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+    fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -1263,1 +1289,1 @@\n-  fields[TypeFunc::Parms + 0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms + 0] = nullptr; \/\/ void\n@@ -1305,1 +1331,1 @@\n-  fields[TypeFunc::Parms + 0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms + 0] = nullptr; \/\/ void\n@@ -1320,1 +1346,1 @@\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -1359,2 +1385,2 @@\n-  assert(current->exception_oop() != NULL, \"exception oop is found\");\n-  address handler_address = NULL;\n+  assert(current->exception_oop() != nullptr, \"exception oop is found\");\n+  address handler_address = nullptr;\n@@ -1393,1 +1419,1 @@\n-  assert(nm != NULL, \"No NMethod found\");\n+  assert(nm != nullptr, \"No NMethod found\");\n@@ -1434,1 +1460,1 @@\n-        force_unwind ? NULL : nm->handler_for_exception_and_pc(exception, pc);\n+        force_unwind ? nullptr : nm->handler_for_exception_and_pc(exception, pc);\n@@ -1436,1 +1462,1 @@\n-      if (handler_address == NULL) {\n+      if (handler_address == nullptr) {\n@@ -1439,1 +1465,1 @@\n-        assert (handler_address != NULL, \"must have compiled handler\");\n+        assert (handler_address != nullptr, \"must have compiled handler\");\n@@ -1485,2 +1511,2 @@\n-  nmethod* nm = NULL;\n-  address handler_address = NULL;\n+  nmethod* nm = nullptr;\n+  address handler_address = nullptr;\n@@ -1499,1 +1525,1 @@\n-  if (nm != NULL) {\n+  if (nm != nullptr) {\n@@ -1551,1 +1577,1 @@\n-  assert (exception != NULL, \"should have thrown a NULLPointerException\");\n+  assert (exception != nullptr, \"should have thrown a NullPointerException\");\n@@ -1648,0 +1674,18 @@\n+#if INCLUDE_JVMTI\n+const TypeFunc *OptoRuntime::notify_jvmti_Type() {\n+  \/\/ create input type (domain)\n+  const Type **fields = TypeTuple::fields(3);\n+  fields[TypeFunc::Parms+0] = TypeInstPtr::NOTNULL; \/\/ VirtualThread oop\n+  fields[TypeFunc::Parms+1] = TypeInt::BOOL;        \/\/ jboolean\n+  fields[TypeFunc::Parms+2] = TypeInt::BOOL;        \/\/ jboolean\n+  const TypeTuple *domain = TypeTuple::make(TypeFunc::Parms+3,fields);\n+\n+  \/\/ no result type needed\n+  fields = TypeTuple::fields(1);\n+  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms, fields);\n+\n+  return TypeFunc::make(domain,range);\n+}\n+#endif\n+\n@@ -1690,1 +1734,1 @@\n-NamedCounter * volatile OptoRuntime::_named_counters = NULL;\n+NamedCounter * volatile OptoRuntime::_named_counters = nullptr;\n@@ -1746,1 +1790,1 @@\n-    ciMethod* m = jvms->has_method() ? jvms->method() : NULL;\n+    ciMethod* m = jvms->has_method() ? jvms->method() : nullptr;\n@@ -1754,1 +1798,1 @@\n-    if (m != NULL) {\n+    if (m != nullptr) {\n@@ -1773,1 +1817,1 @@\n-    c->set_next(NULL);\n+    c->set_next(nullptr);\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":91,"deletions":47,"binary":false,"changes":138,"status":"modified"},{"patch":"@@ -50,1 +50,1 @@\n-Dict* Type::_shared_type_dict = NULL;\n+Dict* Type::_shared_type_dict = nullptr;\n@@ -123,2 +123,2 @@\n-const TypePtr::InterfaceSet* TypeAryPtr::_array_interfaces = NULL;\n-const TypePtr::InterfaceSet* TypeAryKlassPtr::_array_interfaces = NULL;\n+const TypePtr::InterfaceSet* TypeAryPtr::_array_interfaces = nullptr;\n+const TypePtr::InterfaceSet* TypeAryKlassPtr::_array_interfaces = nullptr;\n@@ -140,2 +140,2 @@\n-  if (type == NULL) {\n-    return NULL;\n+  if (type == nullptr) {\n+    return nullptr;\n@@ -165,1 +165,1 @@\n-\/\/ Otherwise or if the arrays have different dimensions, return NULL.\n+\/\/ Otherwise or if the arrays have different dimensions, return null.\n@@ -169,4 +169,4 @@\n-  if (e1) *e1 = NULL;\n-  if (e2) *e2 = NULL;\n-  const TypeAryPtr* a1tap = (a1 == NULL) ? NULL : a1->isa_aryptr();\n-  const TypeAryPtr* a2tap = (a2 == NULL) ? NULL : a2->isa_aryptr();\n+  if (e1) *e1 = nullptr;\n+  if (e2) *e2 = nullptr;\n+  const TypeAryPtr* a1tap = (a1 == nullptr) ? nullptr : a1->isa_aryptr();\n+  const TypeAryPtr* a2tap = (a2 == nullptr) ? nullptr : a2->isa_aryptr();\n@@ -174,1 +174,1 @@\n-  if (a1tap != NULL && a2tap != NULL) {\n+  if (a1tap != nullptr && a2tap != nullptr) {\n@@ -253,1 +253,1 @@\n-        const Type* con_type = NULL;\n+        const Type* con_type = nullptr;\n@@ -277,1 +277,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -280,1 +280,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -319,1 +319,1 @@\n-    return NULL; \/\/ wrong offset\n+    return nullptr; \/\/ wrong offset\n@@ -331,1 +331,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -337,1 +337,1 @@\n-  if (type != NULL && type->is_instance_klass() && off >= InstanceMirrorKlass::offset_of_static_fields()) {\n+  if (type != nullptr && type->is_instance_klass() && off >= InstanceMirrorKlass::offset_of_static_fields()) {\n@@ -344,2 +344,2 @@\n-  if (field == NULL) {\n-    return NULL; \/\/ Wrong offset\n+  if (field == nullptr) {\n+    return nullptr; \/\/ Wrong offset\n@@ -353,1 +353,1 @@\n-    return NULL; \/\/ Non-constant field\n+    return nullptr; \/\/ Non-constant field\n@@ -359,1 +359,1 @@\n-  } else if (holder != NULL) {\n+  } else if (holder != nullptr) {\n@@ -367,1 +367,1 @@\n-    return NULL; \/\/ Not a constant\n+    return nullptr; \/\/ Not a constant\n@@ -382,1 +382,1 @@\n-  if (con_type != NULL && field->is_call_site_target()) {\n+  if (con_type != nullptr && field->is_call_site_target()) {\n@@ -554,1 +554,1 @@\n-  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, NULL, OffsetBot);\n+  TypeMetadataPtr::BOTTOM = TypeMetadataPtr::make(TypePtr::BotPTR, nullptr, OffsetBot);\n@@ -577,1 +577,1 @@\n-  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), NULL \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n+  TypeAryPtr::RANGE   = TypeAryPtr::make( TypePtr::BotPTR, TypeAry::make(Type::BOTTOM,TypeInt::POS), nullptr \/* current->env()->Object_klass() *\/, false, arrayOopDesc::length_offset_in_bytes());\n@@ -579,1 +579,1 @@\n-  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+  TypeAryPtr::NARROWOOPS = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeNarrowOop::BOTTOM, TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n@@ -589,1 +589,1 @@\n-    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), NULL \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n+    TypeAryPtr::OOPS  = TypeAryPtr::make(TypePtr::BotPTR, TypeAry::make(TypeInstPtr::BOTTOM,TypeInt::POS), nullptr \/*ciArrayKlass::make(o)*\/,  false,  Type::OffsetBot);\n@@ -599,2 +599,2 @@\n-  \/\/ Nobody should ask _array_body_type[T_NARROWOOP]. Use NULL as assert.\n-  TypeAryPtr::_array_body_type[T_NARROWOOP] = NULL;\n+  \/\/ Nobody should ask _array_body_type[T_NARROWOOP]. Use null as assert.\n+  TypeAryPtr::_array_body_type[T_NARROWOOP] = nullptr;\n@@ -672,1 +672,1 @@\n-  _zero_type[T_CONFLICT]= NULL;\n+  _zero_type[T_CONFLICT]= nullptr;\n@@ -707,1 +707,1 @@\n-  current->set_type_dict(NULL);\n+  current->set_type_dict(nullptr);\n@@ -712,1 +712,1 @@\n-  assert(current->type_arena() != NULL, \"must have created type arena\");\n+  assert(current->type_arena() != nullptr, \"must have created type arena\");\n@@ -714,1 +714,1 @@\n-  if (_shared_type_dict == NULL) {\n+  if (_shared_type_dict == nullptr) {\n@@ -805,1 +805,1 @@\n-            _in1(NULL), _in2(NULL), _res(NULL) {\n+            _in1(nullptr), _in2(nullptr), _res(nullptr) {\n@@ -825,1 +825,1 @@\n-          assert(v1._res == v2._res || v1._res == NULL || v2._res == NULL, \"same inputs should lead to same result\");\n+          assert(v1._res == v2._res || v1._res == nullptr || v2._res == nullptr, \"same inputs should lead to same result\");\n@@ -875,1 +875,1 @@\n-    const VerifyMeetResultEntry meet(t1, t2, NULL);\n+    const VerifyMeetResultEntry meet(t1, t2, nullptr);\n@@ -877,1 +877,1 @@\n-    const Type* res = NULL;\n+    const Type* res = nullptr;\n@@ -904,1 +904,1 @@\n-  assert(Compile::current()->_type_verify == NULL || Compile::current()->_type_verify->empty_cache(), \"cache should have been discarded\");\n+  assert(Compile::current()->_type_verify == nullptr || Compile::current()->_type_verify->empty_cache(), \"cache should have been discarded\");\n@@ -912,1 +912,1 @@\n-    if (C->_type_verify == NULL) {\n+    if (C->_type_verify == nullptr) {\n@@ -1717,1 +1717,1 @@\n-  if (old == NULL)  return this;\n+  if (old == nullptr)  return this;\n@@ -1719,1 +1719,1 @@\n-  if (ot == NULL)  return this;\n+  if (ot == nullptr)  return this;\n@@ -1748,1 +1748,1 @@\n-  if (ft == NULL || ft->empty())\n+  if (ft == nullptr || ft->empty())\n@@ -1983,1 +1983,1 @@\n-  if (old == NULL)  return this;\n+  if (old == nullptr)  return this;\n@@ -1985,1 +1985,1 @@\n-  if (ot == NULL)  return this;\n+  if (ot == nullptr)  return this;\n@@ -2014,1 +2014,1 @@\n-  if (ft == NULL || ft->empty())\n+  if (ft == nullptr || ft->empty())\n@@ -2048,1 +2048,1 @@\n-    if (n >= x + 10000)  return NULL;\n+    if (n >= x + 10000)  return nullptr;\n@@ -2051,1 +2051,1 @@\n-    if (n <= x - 10000)  return NULL;\n+    if (n <= x - 10000)  return nullptr;\n@@ -2069,1 +2069,1 @@\n-  else if ((str = longnamenear(max_juint, \"maxuint\", buf, buf_size, n)) != NULL)\n+  else if ((str = longnamenear(max_juint, \"maxuint\", buf, buf_size, n)) != nullptr)\n@@ -2071,1 +2071,1 @@\n-  else if ((str = longnamenear(max_jint, \"maxint\", buf, buf_size, n)) != NULL)\n+  else if ((str = longnamenear(max_jint, \"maxint\", buf, buf_size, n)) != nullptr)\n@@ -2073,1 +2073,1 @@\n-  else if ((str = longnamenear(min_jint, \"minint\", buf, buf_size, n)) != NULL)\n+  else if ((str = longnamenear(min_jint, \"minint\", buf, buf_size, n)) != nullptr)\n@@ -2163,1 +2163,1 @@\n-  if (recv != NULL) {\n+  if (recv != nullptr) {\n@@ -2450,1 +2450,1 @@\n-  const TypeOopPtr*  toop = NULL;\n+  const TypeOopPtr*  toop = nullptr;\n@@ -2477,7 +2477,7 @@\n-const TypeVect *TypeVect::VECTA = NULL; \/\/ vector length agnostic\n-const TypeVect *TypeVect::VECTS = NULL; \/\/  32-bit vectors\n-const TypeVect *TypeVect::VECTD = NULL; \/\/  64-bit vectors\n-const TypeVect *TypeVect::VECTX = NULL; \/\/ 128-bit vectors\n-const TypeVect *TypeVect::VECTY = NULL; \/\/ 256-bit vectors\n-const TypeVect *TypeVect::VECTZ = NULL; \/\/ 512-bit vectors\n-const TypeVect *TypeVect::VECTMASK = NULL; \/\/ predicate\/mask vector\n+const TypeVect *TypeVect::VECTA = nullptr; \/\/ vector length agnostic\n+const TypeVect *TypeVect::VECTS = nullptr; \/\/  32-bit vectors\n+const TypeVect *TypeVect::VECTD = nullptr; \/\/  64-bit vectors\n+const TypeVect *TypeVect::VECTX = nullptr; \/\/ 128-bit vectors\n+const TypeVect *TypeVect::VECTY = nullptr; \/\/ 256-bit vectors\n+const TypeVect *TypeVect::VECTZ = nullptr; \/\/ 512-bit vectors\n+const TypeVect *TypeVect::VECTMASK = nullptr; \/\/ predicate\/mask vector\n@@ -2511,1 +2511,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -2681,1 +2681,1 @@\n-  if (res->isa_ptr() == NULL) {\n+  if (res->isa_ptr() == nullptr) {\n@@ -2686,2 +2686,2 @@\n-  if (res_ptr->speculative() != NULL) {\n-    \/\/ type->speculative() == NULL means that speculation is no better\n+  if (res_ptr->speculative() != nullptr) {\n+    \/\/ type->speculative() is null means that speculation is no better\n@@ -2692,1 +2692,1 @@\n-    \/\/ type and set speculative to NULL if it is the case.\n+    \/\/ type and set speculative to null if it is the case.\n@@ -2813,1 +2813,1 @@\n-  if (_speculative == NULL) {\n+  if (_speculative == nullptr) {\n@@ -2817,1 +2817,1 @@\n-  return make(AnyPtr, _ptr, _offset, NULL, _inline_depth);\n+  return make(AnyPtr, _ptr, _offset, nullptr, _inline_depth);\n@@ -2825,1 +2825,1 @@\n-  if (speculative() == NULL) {\n+  if (speculative() == nullptr) {\n@@ -2842,1 +2842,1 @@\n-      (spec_oopptr == NULL || !spec_oopptr->klass_is_exact())) {\n+      (spec_oopptr == nullptr || !spec_oopptr->klass_is_exact())) {\n@@ -2852,2 +2852,2 @@\n-  if (_speculative == NULL) {\n-    return NULL;\n+  if (_speculative == nullptr) {\n+    return nullptr;\n@@ -2864,2 +2864,2 @@\n-  bool this_has_spec = (_speculative != NULL);\n-  bool other_has_spec = (other->speculative() != NULL);\n+  bool this_has_spec = (_speculative != nullptr);\n+  bool other_has_spec = (other->speculative() != nullptr);\n@@ -2868,1 +2868,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -2913,1 +2913,1 @@\n-  if (_speculative == NULL || other->speculative() == NULL) {\n+  if (_speculative == nullptr || other->speculative() == nullptr) {\n@@ -2928,1 +2928,1 @@\n-  if (_speculative == NULL) {\n+  if (_speculative == nullptr) {\n@@ -2941,2 +2941,2 @@\n-  if (_speculative == NULL) {\n-    return NULL;\n+  if (_speculative == nullptr) {\n+    return nullptr;\n@@ -2948,2 +2948,2 @@\n-  if (_speculative == NULL) {\n-    return NULL;\n+  if (_speculative == nullptr) {\n+    return nullptr;\n@@ -2958,1 +2958,1 @@\n-  if (_speculative != NULL && _speculative->isa_oopptr()) {\n+  if (_speculative != nullptr && _speculative->isa_oopptr()) {\n@@ -2964,1 +2964,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -2971,1 +2971,1 @@\n-  if (_speculative != NULL) {\n+  if (_speculative != nullptr) {\n@@ -2979,1 +2979,1 @@\n-  if (_speculative != NULL) {\n+  if (_speculative != nullptr) {\n@@ -2992,1 +2992,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -3007,1 +3007,1 @@\n-  if (exact_kls == NULL) {\n+  if (exact_kls == nullptr) {\n@@ -3014,1 +3014,1 @@\n-  if (speculative_type() == NULL) {\n+  if (speculative_type() == nullptr) {\n@@ -3057,1 +3057,1 @@\n-  if (ptr_kind == ProfileAlwaysNull && speculative() != NULL && speculative()->isa_oopptr()) {\n+  if (ptr_kind == ProfileAlwaysNull && speculative() != nullptr && speculative()->isa_oopptr()) {\n@@ -3065,1 +3065,1 @@\n-  \"TopPTR\",\"AnyNull\",\"Constant\",\"NULL\",\"NotNull\",\"BotPTR\"\n+  \"TopPTR\",\"AnyNull\",\"Constant\",\"null\",\"NotNull\",\"BotPTR\"\n@@ -3070,1 +3070,1 @@\n-  if( _ptr == Null ) st->print(\"NULL\");\n+  if( _ptr == Null ) st->print(\"null\");\n@@ -3083,1 +3083,1 @@\n-  if (_speculative != NULL) {\n+  if (_speculative != nullptr) {\n@@ -3124,1 +3124,1 @@\n-  assert( ptr != Null, \"Use TypePtr for NULL\" );\n+  assert( ptr != Null, \"Use TypePtr for null\" );\n@@ -3129,1 +3129,1 @@\n-  assert( bits, \"Use TypePtr for NULL\" );\n+  assert( bits, \"Use TypePtr for null\" );\n@@ -3136,1 +3136,1 @@\n-  assert( ptr != Null, \"Use TypePtr for NULL\" );\n+  assert( ptr != Null, \"Use TypePtr for null\" );\n@@ -3226,1 +3226,1 @@\n-  return NULL;                  \/\/ Lint noise\n+  return nullptr;                  \/\/ Lint noise\n@@ -3257,1 +3257,1 @@\n-        : _list(Compile::current()->type_arena(), 0, 0, NULL),\n+        : _list(Compile::current()->type_arena(), 0, 0, nullptr),\n@@ -3262,1 +3262,1 @@\n-        : _list(Compile::current()->type_arena(), interfaces->length(), 0, NULL),\n+        : _list(Compile::current()->type_arena(), interfaces->length(), 0, nullptr),\n@@ -3449,1 +3449,1 @@\n-    _exact_klass = NULL;\n+    _exact_klass = nullptr;\n@@ -3452,1 +3452,1 @@\n-  ciKlass* res = NULL;\n+  ciKlass* res = nullptr;\n@@ -3456,1 +3456,1 @@\n-      assert(res == NULL, \"\");\n+      assert(res == nullptr, \"\");\n@@ -3504,1 +3504,1 @@\n-    } else if (klass() == NULL) {\n+    } else if (klass() == nullptr) {\n@@ -3530,2 +3530,2 @@\n-          ciField* field = NULL;\n-          if (const_oop() != NULL) {\n+          ciField* field = nullptr;\n+          if (const_oop() != nullptr) {\n@@ -3535,1 +3535,1 @@\n-          if (field != NULL) {\n+          if (field != nullptr) {\n@@ -3545,1 +3545,1 @@\n-          if (field != NULL) {\n+          if (field != nullptr) {\n@@ -3569,1 +3569,1 @@\n-  ciObject* o = NULL;\n+  ciObject* o = nullptr;\n@@ -3601,1 +3601,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -3682,1 +3682,1 @@\n-  assert(const_oop() == NULL,             \"no constants here\");\n+  assert(const_oop() == nullptr,             \"no constants here\");\n@@ -3692,1 +3692,1 @@\n-    assert((deps != NULL) == (C->method() != NULL && C->method()->code_size() > 0), \"sanity\");\n+    assert((deps != nullptr) == (C->method() != nullptr && C->method()->code_size() > 0), \"sanity\");\n@@ -3700,1 +3700,1 @@\n-          && deps != NULL && UseUniqueSubclasses) {\n+          && deps != nullptr && UseUniqueSubclasses) {\n@@ -3702,1 +3702,1 @@\n-        if (sub != NULL) {\n+        if (sub != nullptr) {\n@@ -3708,1 +3708,1 @@\n-      if (!klass_is_exact && try_for_exact && deps != NULL &&\n+      if (!klass_is_exact && try_for_exact && deps != nullptr &&\n@@ -3716,1 +3716,1 @@\n-    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, NULL, 0);\n+    return TypeInstPtr::make(TypePtr::BotPTR, klass, interfaces, klass_is_exact, nullptr, 0);\n@@ -3725,2 +3725,2 @@\n-    \/\/ slam NULLs down in the subarrays.\n-    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, NULL, xk, 0);\n+    \/\/ slam nulls down in the subarrays.\n+    const TypeAryPtr* arr = TypeAryPtr::make(TypePtr::BotPTR, arr0, nullptr, xk, 0);\n@@ -3738,1 +3738,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -3755,1 +3755,1 @@\n-      return TypeInstPtr::make(TypePtr::NotNull, klass, true, NULL, 0);\n+      return TypeInstPtr::make(TypePtr::NotNull, klass, true, nullptr, 0);\n@@ -3764,1 +3764,1 @@\n-    \/\/ slam NULLs down in the subarrays.\n+    \/\/ slam nulls down in the subarrays.\n@@ -3785,1 +3785,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -3832,1 +3832,1 @@\n-  if (one == NULL || two == NULL) {\n+  if (one == nullptr || two == nullptr) {\n@@ -3891,1 +3891,1 @@\n-  if (_speculative == NULL) {\n+  if (_speculative == nullptr) {\n@@ -3895,1 +3895,1 @@\n-  return make(_ptr, _offset, _instance_id, NULL, _inline_depth);\n+  return make(_ptr, _offset, _instance_id, nullptr, _inline_depth);\n@@ -3993,1 +3993,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -4002,3 +4002,3 @@\n-  assert(k == NULL || !k->is_loaded() || !k->is_interface(), \"no interface here\");\n-  assert(k != NULL &&\n-         (k->is_loaded() || o == NULL),\n+  assert(k == nullptr || !k->is_loaded() || !k->is_interface(), \"no interface here\");\n+  assert(k != nullptr &&\n+         (k->is_loaded() || o == nullptr),\n@@ -4019,1 +4019,1 @@\n-  \/\/ Either const_oop() is NULL or else ptr is Constant\n+  \/\/ Either const_oop() is null or else ptr is Constant\n@@ -4023,1 +4023,1 @@\n-  assert( ptr != Null, \"NULL pointers are not typed\" );\n+  assert( ptr != Null, \"null pointers are not typed\" );\n@@ -4081,1 +4081,1 @@\n-  assert((const_oop() != NULL), \"should be called only for constant object\");\n+  assert((const_oop() != nullptr), \"should be called only for constant object\");\n@@ -4096,1 +4096,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -4104,1 +4104,1 @@\n-  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : NULL, _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, klass(), _interfaces, klass_is_exact(), ptr == Constant ? const_oop() : nullptr, _offset, _instance_id, _speculative, _inline_depth);\n@@ -4153,1 +4153,1 @@\n-    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, NULL, off, instance_id, speculative, depth); }\n+    else if (loaded->ptr() == TypePtr::AnyNull)  { return make(ptr, unloaded->klass(), interfaces, false, nullptr, off, instance_id, speculative, depth); }\n@@ -4223,1 +4223,1 @@\n-                  (ptr == Constant ? const_oop() : NULL), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n@@ -4251,1 +4251,1 @@\n-                  (ptr == Constant ? const_oop() : NULL), offset, instance_id, speculative, depth);\n+                  (ptr == Constant ? const_oop() : nullptr), offset, instance_id, speculative, depth);\n@@ -4289,1 +4289,1 @@\n-    ciKlass* res_klass = NULL;\n+    ciKlass* res_klass = nullptr;\n@@ -4317,1 +4317,1 @@\n-      ciObject* o = NULL;             \/\/ Assume not constant when done\n+      ciObject* o = nullptr;             \/\/ Assume not constant when done\n@@ -4321,1 +4321,1 @@\n-        if (this_oop != NULL && tinst_oop != NULL &&\n+        if (this_oop != nullptr && tinst_oop != nullptr &&\n@@ -4394,1 +4394,1 @@\n-  const T* subtype = NULL;\n+  const T* subtype = nullptr;\n@@ -4427,1 +4427,1 @@\n-    \/\/ NotNull if they do (neither constant is NULL; that is a special case\n+    \/\/ NotNull if they do (neither constant is null; that is a special case\n@@ -4454,1 +4454,1 @@\n-  if( const_oop() == NULL )  return NULL;\n+  if( const_oop() == nullptr )  return nullptr;\n@@ -4457,1 +4457,1 @@\n-  if( klass() != ciEnv::current()->Class_klass() )  return NULL;\n+  if( klass() != ciEnv::current()->Class_klass() )  return nullptr;\n@@ -4567,1 +4567,1 @@\n-  if (_speculative == NULL) {\n+  if (_speculative == nullptr) {\n@@ -4572,1 +4572,1 @@\n-              _instance_id, NULL, _inline_depth);\n+              _instance_id, nullptr, _inline_depth);\n@@ -4643,1 +4643,1 @@\n-  if (other_elem != NULL && this_elem != NULL) {\n+  if (other_elem != nullptr && this_elem != nullptr) {\n@@ -4647,1 +4647,1 @@\n-  if (other_elem == NULL && this_elem == NULL) {\n+  if (other_elem == nullptr && this_elem == nullptr) {\n@@ -4682,1 +4682,1 @@\n-  assert(!(k == NULL && ary->_elem->isa_int()),\n+  assert(!(k == nullptr && ary->_elem->isa_int()),\n@@ -4686,1 +4686,1 @@\n-  if (k != NULL && k->is_loaded() && k->is_obj_array_klass() &&\n+  if (k != nullptr && k->is_loaded() && k->is_obj_array_klass() &&\n@@ -4688,1 +4688,1 @@\n-    k = NULL;\n+    k = nullptr;\n@@ -4690,1 +4690,1 @@\n-  return (TypeAryPtr*)(new TypeAryPtr(ptr, NULL, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n+  return (TypeAryPtr*)(new TypeAryPtr(ptr, nullptr, ary, k, xk, offset, instance_id, false, speculative, inline_depth))->hashcons();\n@@ -4697,1 +4697,1 @@\n-  assert(!(k == NULL && ary->_elem->isa_int()),\n+  assert(!(k == nullptr && ary->_elem->isa_int()),\n@@ -4700,1 +4700,1 @@\n-  if (!xk)  xk = (o != NULL) || ary->ary_must_be_exact();\n+  if (!xk)  xk = (o != nullptr) || ary->ary_must_be_exact();\n@@ -4702,1 +4702,1 @@\n-  if (k != NULL && k->is_loaded() && k->is_obj_array_klass() &&\n+  if (k != nullptr && k->is_loaded() && k->is_obj_array_klass() &&\n@@ -4704,1 +4704,1 @@\n-    k = NULL;\n+    k = nullptr;\n@@ -4712,1 +4712,1 @@\n-  return make(ptr, ptr == Constant ? const_oop() : NULL, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n+  return make(ptr, ptr == Constant ? const_oop() : nullptr, _ary, klass(), klass_is_exact(), _offset, _instance_id, _speculative, _inline_depth);\n@@ -4747,1 +4747,1 @@\n-\/\/ Return NULL if the resulting int type becomes empty.\n+\/\/ Return null if the resulting int type becomes empty.\n@@ -4779,1 +4779,1 @@\n-  assert(new_size != NULL, \"\");\n+  assert(new_size != nullptr, \"\");\n@@ -4794,1 +4794,1 @@\n-  if (stable_dimension > 1 && elem_ptr != NULL && elem_ptr->isa_aryptr()) {\n+  if (stable_dimension > 1 && elem_ptr != nullptr && elem_ptr->isa_aryptr()) {\n@@ -4809,1 +4809,1 @@\n-  if (elem_ptr != NULL && elem_ptr->isa_aryptr())\n+  if (elem_ptr != nullptr && elem_ptr->isa_aryptr())\n@@ -4818,1 +4818,1 @@\n-  if (etype == NULL)  return this;\n+  if (etype == nullptr)  return this;\n@@ -4889,1 +4889,1 @@\n-      return make(ptr, (ptr == Constant ? const_oop() : NULL),\n+      return make(ptr, (ptr == Constant ? const_oop() : nullptr),\n@@ -4919,1 +4919,1 @@\n-      return make(ptr, (ptr == Constant ? const_oop() : NULL),\n+      return make(ptr, (ptr == Constant ? const_oop() : nullptr),\n@@ -4941,1 +4941,1 @@\n-    ciKlass* res_klass = NULL;\n+    ciKlass* res_klass = nullptr;\n@@ -4948,1 +4948,1 @@\n-    ciObject* o = NULL;             \/\/ Assume not constant when done\n+    ciObject* o = nullptr;             \/\/ Assume not constant when done\n@@ -4952,1 +4952,1 @@\n-      if (this_oop != NULL && tap_oop != NULL &&\n+      if (this_oop != nullptr && tap_oop != nullptr &&\n@@ -4991,1 +4991,1 @@\n-        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, NULL,offset, instance_id, speculative, depth);\n+        return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr,offset, instance_id, speculative, depth);\n@@ -5005,1 +5005,1 @@\n-          return make(ptr, (ptr == Constant ? const_oop() : NULL),\n+          return make(ptr, (ptr == Constant ? const_oop() : nullptr),\n@@ -5018,1 +5018,1 @@\n-      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, NULL, offset, instance_id, speculative, depth);\n+      return TypeInstPtr::make(ptr, ciEnv::current()->Object_klass(), interfaces, false, nullptr, offset, instance_id, speculative, depth);\n@@ -5038,1 +5038,1 @@\n-  res_klass = NULL;\n+  res_klass = nullptr;\n@@ -5199,1 +5199,1 @@\n-  if (_speculative == NULL) {\n+  if (_speculative == nullptr) {\n@@ -5203,1 +5203,1 @@\n-  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, NULL, _inline_depth);\n+  return make(_ptr, _const_oop, _ary->remove_speculative()->is_ary(), _klass, _klass_is_exact, _offset, _instance_id, nullptr, _inline_depth);\n@@ -5240,1 +5240,1 @@\n-  if (tc != NULL) {\n+  if (tc != nullptr) {\n@@ -5373,1 +5373,1 @@\n-  if (one == NULL || two == NULL) {\n+  if (one == nullptr || two == nullptr) {\n@@ -5406,1 +5406,1 @@\n-  if (ft == NULL || ft->empty())\n+  if (ft == nullptr || ft->empty())\n@@ -5509,1 +5509,1 @@\n-    return make(ptr, NULL, offset);\n+    return make(ptr, nullptr, offset);\n@@ -5556,1 +5556,1 @@\n-  assert(m == NULL || !m->is_klass(), \"wrong type\");\n+  assert(m == nullptr || !m->is_klass(), \"wrong type\");\n@@ -5564,1 +5564,1 @@\n-  if (elem->make_oopptr() != NULL) {\n+  if (elem->make_oopptr() != nullptr) {\n@@ -5592,1 +5592,1 @@\n-  assert(klass == NULL || !klass->is_loaded() || (klass->is_instance_klass() && !klass->is_interface()) ||\n+  assert(klass == nullptr || !klass->is_loaded() || (klass->is_instance_klass() && !klass->is_interface()) ||\n@@ -5607,1 +5607,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -5789,1 +5789,1 @@\n-  assert((deps != NULL) == (C->method() != NULL && C->method()->code_size() > 0), \"sanity\");\n+  assert((deps != nullptr) == (C->method() != nullptr && C->method()->code_size() > 0), \"sanity\");\n@@ -5798,1 +5798,1 @@\n-        && deps != NULL && UseUniqueSubclasses) {\n+        && deps != nullptr && UseUniqueSubclasses) {\n@@ -5800,1 +5800,1 @@\n-      if (sub != NULL) {\n+      if (sub != nullptr) {\n@@ -5812,1 +5812,1 @@\n-  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, NULL, 0);\n+  return TypeInstPtr::make(TypePtr::BotPTR, k, interfaces, xk, nullptr, 0);\n@@ -5890,1 +5890,1 @@\n-    ciKlass* res_klass = NULL;\n+    ciKlass* res_klass = nullptr;\n@@ -6042,1 +6042,1 @@\n-  assert((deps != NULL) == (C->method() != NULL && C->method()->code_size() > 0), \"sanity\");\n+  assert((deps != nullptr) == (C->method() != nullptr && C->method()->code_size() > 0), \"sanity\");\n@@ -6048,1 +6048,1 @@\n-        deps != NULL) {\n+        deps != nullptr) {\n@@ -6050,1 +6050,1 @@\n-      if (sub != NULL) {\n+      if (sub != nullptr) {\n@@ -6076,1 +6076,1 @@\n-    return TypeAryKlassPtr::make(ptr, etype, NULL, offset);\n+    return TypeAryKlassPtr::make(ptr, etype, nullptr, offset);\n@@ -6083,1 +6083,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -6110,1 +6110,1 @@\n-  ciKlass* k_ary = NULL;\n+  ciKlass* k_ary = nullptr;\n@@ -6119,4 +6119,4 @@\n-  if ((tinst = el->isa_instptr()) != NULL) {\n-    \/\/ Leave k_ary at NULL.\n-  } else if ((tary = el->isa_aryptr()) != NULL) {\n-    \/\/ Leave k_ary at NULL.\n+  if ((tinst = el->isa_instptr()) != nullptr) {\n+    \/\/ Leave k_ary at null.\n+  } else if ((tary = el->isa_aryptr()) != nullptr) {\n+    \/\/ Leave k_ary at null.\n@@ -6127,1 +6127,1 @@\n-    \/\/ Leave k_ary at NULL.\n+    \/\/ Leave k_ary at null.\n@@ -6188,2 +6188,2 @@\n-    if (k == NULL) {\n-      return NULL;\n+    if (k == nullptr) {\n+      return nullptr;\n@@ -6252,1 +6252,1 @@\n-  const Type* el = NULL;\n+  const Type* el = nullptr;\n@@ -6255,1 +6255,1 @@\n-    k = NULL;\n+    k = nullptr;\n@@ -6338,1 +6338,1 @@\n-    ciKlass* res_klass = NULL;\n+    ciKlass* res_klass = nullptr;\n@@ -6422,1 +6422,1 @@\n-  if (this_elem != NULL && other_elem != NULL) {\n+  if (this_elem != nullptr && other_elem != nullptr) {\n@@ -6425,1 +6425,1 @@\n-  if (this_elem == NULL && other_elem == NULL) {\n+  if (this_elem == nullptr && other_elem == nullptr) {\n@@ -6454,1 +6454,1 @@\n-  if (other_elem != NULL && this_elem != NULL) {\n+  if (other_elem != nullptr && this_elem != nullptr) {\n@@ -6457,2 +6457,2 @@\n-  if (other_elem == NULL && this_elem == NULL) {\n-    assert(this_one->_klass != NULL && other->_klass != NULL, \"\");\n+  if (other_elem == nullptr && this_elem == nullptr) {\n+    assert(this_one->_klass != nullptr && other->_klass != nullptr, \"\");\n@@ -6494,1 +6494,1 @@\n-  if (other_elem != NULL && this_elem != NULL) {\n+  if (other_elem != nullptr && this_elem != nullptr) {\n@@ -6497,1 +6497,1 @@\n-  if (other_elem == NULL && this_elem == NULL) {\n+  if (other_elem == nullptr && this_elem == nullptr) {\n@@ -6517,2 +6517,2 @@\n-    if (k == NULL) {\n-      return NULL;\n+    if (k == nullptr) {\n+      return nullptr;\n@@ -6528,1 +6528,1 @@\n-  if (_klass != NULL) {\n+  if (_klass != nullptr) {\n@@ -6531,1 +6531,1 @@\n-  ciKlass* k = NULL;\n+  ciKlass* k = nullptr;\n@@ -6533,1 +6533,1 @@\n-    \/\/ leave NULL\n+    \/\/ leave null\n@@ -6600,1 +6600,1 @@\n-  if (tf != NULL)  return tf;  \/\/ The hit rate here is almost 50%.\n+  if (tf != nullptr)  return tf;  \/\/ The hit rate here is almost 50%.\n@@ -6603,1 +6603,1 @@\n-    domain = TypeTuple::make_domain(NULL, method->signature(), ignore_interfaces);\n+    domain = TypeTuple::make_domain(nullptr, method->signature(), ignore_interfaces);\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":212,"deletions":212,"binary":false,"changes":424,"status":"modified"},{"patch":"@@ -46,0 +46,1 @@\n+#include \"prims\/jvmtiAgentList.hpp\"\n@@ -67,0 +68,1 @@\n+#include \"utilities\/systemMemoryBarrier.hpp\"\n@@ -73,1 +75,3 @@\n-#define DEFAULT_JAVA_LAUNCHER  \"generic\"\n+static const char _default_java_launcher[] = \"generic\";\n+\n+#define DEFAULT_JAVA_LAUNCHER _default_java_launcher\n@@ -84,1 +88,0 @@\n-bool   Arguments::_java_compiler                = false;\n@@ -99,0 +102,1 @@\n+char*  Arguments::_default_shared_archive_path  = nullptr;\n@@ -104,3 +108,0 @@\n-AgentLibraryList Arguments::_libraryList;\n-AgentLibraryList Arguments::_agentList;\n-\n@@ -216,19 +217,0 @@\n-AgentLibrary::AgentLibrary(const char* name, const char* options,\n-               bool is_absolute_path, void* os_lib,\n-               bool instrument_lib) {\n-  _name = AllocateHeap(strlen(name)+1, mtArguments);\n-  strcpy(_name, name);\n-  if (options == nullptr) {\n-    _options = nullptr;\n-  } else {\n-    _options = AllocateHeap(strlen(options)+1, mtArguments);\n-    strcpy(_options, options);\n-  }\n-  _is_absolute_path = is_absolute_path;\n-  _os_lib = os_lib;\n-  _next = nullptr;\n-  _state = agent_invalid;\n-  _is_static_lib = false;\n-  _is_instrument_lib = instrument_lib;\n-}\n-\n@@ -325,17 +307,0 @@\n-void Arguments::add_init_library(const char* name, char* options) {\n-  _libraryList.add(new AgentLibrary(name, options, false, nullptr));\n-}\n-\n-void Arguments::add_init_agent(const char* name, char* options, bool absolute_path) {\n-  _agentList.add(new AgentLibrary(name, options, absolute_path, nullptr));\n-}\n-\n-void Arguments::add_instrument_agent(const char* name, char* options, bool absolute_path) {\n-  _agentList.add(new AgentLibrary(name, options, absolute_path, nullptr, true));\n-}\n-\n-\/\/ Late-binding agents not started via arguments\n-void Arguments::add_loaded_agent(AgentLibrary *agentLib) {\n-  _agentList.add(agentLib);\n-}\n-\n@@ -542,1 +507,0 @@\n-  { \"EnableWaitForParallelLoad\",    JDK_Version::jdk(20), JDK_Version::jdk(21), JDK_Version::jdk(22) },\n@@ -551,0 +515,1 @@\n+  { \"EnableWaitForParallelLoad\",    JDK_Version::jdk(20), JDK_Version::jdk(21), JDK_Version::jdk(22) },\n@@ -558,2 +523,3 @@\n-  { \"G1ConcRSLogCacheSize\",    JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n-  { \"G1ConcRSHotCardLimit\",   JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n+  { \"G1UsePreventiveGC\",            JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::jdk(22) },\n+  { \"G1ConcRSLogCacheSize\",         JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n+  { \"G1ConcRSHotCardLimit\",         JDK_Version::undefined(), JDK_Version::jdk(21), JDK_Version::undefined() },\n@@ -1302,2 +1268,8 @@\n-    process_java_compiler_argument(value);\n-    \/\/ Record value in Arguments, but let it get passed to Java.\n+    \/\/ we no longer support java.compiler system property, log a warning and let it get\n+    \/\/ passed to Java, like any other system property\n+    if (strlen(value) == 0 || strcasecmp(value, \"NONE\") == 0) {\n+        \/\/ for applications using NONE or empty value, log a more informative message\n+        warning(\"The java.compiler system property is obsolete and no longer supported, use -Xint\");\n+    } else {\n+        warning(\"The java.compiler system property is obsolete and no longer supported.\");\n+    }\n@@ -1408,1 +1380,0 @@\n-  set_java_compiler(false);\n@@ -1922,10 +1893,0 @@\n-\/\/ Parsing of java.compiler property\n-\n-void Arguments::process_java_compiler_argument(const char* arg) {\n-  \/\/ For backwards compatibility, Djava.compiler=NONE or \"\"\n-  \/\/ causes us to switch to -Xint mode UNLESS -Xdebug\n-  \/\/ is also specified.\n-  if (strlen(arg) == 0 || strcasecmp(arg, \"NONE\") == 0) {\n-    set_java_compiler(true);    \/\/ \"-Djava.compiler[=...]\" most recently seen.\n-  }\n-}\n@@ -1934,0 +1895,3 @@\n+  if (_sun_java_launcher != _default_java_launcher) {\n+    os::free(const_cast<char*>(_sun_java_launcher));\n+  }\n@@ -2160,0 +2124,5 @@\n+  \/\/ Disable CDS for exploded image\n+  if (!has_jimage()) {\n+    no_shared_spaces(\"CDS disabled on exploded JDK\");\n+  }\n+\n@@ -2168,0 +2137,2 @@\n+  SystemMemoryBarrier::initialize();\n+\n@@ -2393,1 +2364,3 @@\n-        add_init_library(name, options);\n+        JvmtiAgentList::add_xrun(name, options, false);\n+        FREE_C_HEAP_ARRAY(char, name);\n+        FREE_C_HEAP_ARRAY(char, options);\n@@ -2463,1 +2436,3 @@\n-        add_init_agent(name, options, is_absolute_path);\n+        JvmtiAgentList::add(name, options, is_absolute_path);\n+        os::free(name);\n+        os::free(options);\n@@ -2476,1 +2451,3 @@\n-        add_instrument_agent(\"instrument\", options, false);\n+        JvmtiAgentList::add(\"instrument\", options, false);\n+        FREE_C_HEAP_ARRAY(char, options);\n+\n@@ -3048,9 +3025,0 @@\n-  \/\/ This must be done after all arguments have been processed.\n-  \/\/ java_compiler() true means set to \"NONE\" or empty.\n-  if (java_compiler() && !xdebug_mode()) {\n-    \/\/ For backwards compatibility, we switch to interpreted mode if\n-    \/\/ -Djava.compiler=\"NONE\" or \"\" is specified AND \"-Xdebug\" was\n-    \/\/ not specified.\n-    set_mode_flags(_int);\n-  }\n-\n@@ -3453,13 +3421,14 @@\n-  char *default_archive_path;\n-  char jvm_path[JVM_MAXPATHLEN];\n-  os::jvm_path(jvm_path, sizeof(jvm_path));\n-  char *end = strrchr(jvm_path, *os::file_separator());\n-  if (end != nullptr) *end = '\\0';\n-  size_t jvm_path_len = strlen(jvm_path);\n-  size_t file_sep_len = strlen(os::file_separator());\n-  const size_t len = jvm_path_len + file_sep_len + 20;\n-  default_archive_path = NEW_C_HEAP_ARRAY(char, len, mtArguments);\n-  jio_snprintf(default_archive_path, len,\n-               LP64_ONLY(!UseCompressedOops ? \"%s%sclasses_nocoops.jsa\":) \"%s%sclasses.jsa\",\n-               jvm_path, os::file_separator());\n-  return default_archive_path;\n+  if (_default_shared_archive_path == nullptr) {\n+    char jvm_path[JVM_MAXPATHLEN];\n+    os::jvm_path(jvm_path, sizeof(jvm_path));\n+    char *end = strrchr(jvm_path, *os::file_separator());\n+    if (end != nullptr) *end = '\\0';\n+    size_t jvm_path_len = strlen(jvm_path);\n+    size_t file_sep_len = strlen(os::file_separator());\n+    const size_t len = jvm_path_len + file_sep_len + 20;\n+    _default_shared_archive_path = NEW_C_HEAP_ARRAY(char, len, mtArguments);\n+    jio_snprintf(_default_shared_archive_path, len,\n+                LP64_ONLY(!UseCompressedOops ? \"%s%sclasses_nocoops.jsa\":) \"%s%sclasses.jsa\",\n+                jvm_path, os::file_separator());\n+  }\n+  return _default_shared_archive_path;\n@@ -3517,2 +3486,1 @@\n-    char* shared_archive_path = get_default_shared_archive_path();\n-    if (os::same_files(shared_archive_path, ArchiveClassesAtExit)) {\n+    if (os::same_files(get_default_shared_archive_path(), ArchiveClassesAtExit)) {\n@@ -3520,1 +3488,1 @@\n-        \"Cannot specify the default CDS archive for -XX:ArchiveClassesAtExit\", shared_archive_path);\n+        \"Cannot specify the default CDS archive for -XX:ArchiveClassesAtExit\", get_default_shared_archive_path());\n@@ -3522,1 +3490,0 @@\n-    FREE_C_HEAP_ARRAY(char, shared_archive_path);\n@@ -3619,1 +3586,1 @@\n-      PrintAssembly || TraceDeoptimization || TraceDependencies ||\n+      PrintAssembly || TraceDeoptimization ||\n@@ -4032,4 +3999,3 @@\n-  if (TraceDependencies && VerifyDependencies) {\n-    if (!FLAG_IS_DEFAULT(TraceDependencies)) {\n-      warning(\"TraceDependencies results may be inflated by VerifyDependencies\");\n-    }\n+  bool trace_dependencies = log_is_enabled(Debug, dependencies);\n+  if (trace_dependencies && VerifyDependencies) {\n+    warning(\"dependency logging results may be inflated by VerifyDependencies\");\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":57,"deletions":91,"binary":false,"changes":148,"status":"modified"},{"patch":"@@ -364,1 +364,1 @@\n-  product(bool, UseVectorizedHashCodeIntrinsic, false, DIAGNOSTIC,           \\\n+  product(bool, UseVectorizedHashCodeIntrinsic, false, DIAGNOSTIC,          \\\n@@ -373,0 +373,3 @@\n+  product_pd(bool, DelayCompilerStubsGeneration, DIAGNOSTIC,                \\\n+          \"Use Compiler thread for compiler's stubs generation\")            \\\n+                                                                            \\\n@@ -695,4 +698,0 @@\n-  product(bool, EnableWaitForParallelLoad, false,                           \\\n-          \"(Deprecated) Enable legacy parallel classloading logic for \"     \\\n-          \"class loaders not registered as parallel capable\")               \\\n-                                                                            \\\n@@ -721,0 +720,7 @@\n+  \/* notice: the max range value here is max_jint, not max_intx  *\/         \\\n+  \/* because of overflow issue                                   *\/         \\\n+  product(intx, GuaranteedAsyncDeflationInterval, 60000, DIAGNOSTIC,        \\\n+          \"Async deflate idle monitors every so many milliseconds even \"    \\\n+          \"when MonitorUsedDeflationThreshold is NOT exceeded (0 is off).\") \\\n+          range(0, max_jint)                                                \\\n+                                                                            \\\n@@ -864,3 +870,0 @@\n-  develop(bool, TraceDependencies, false,                                   \\\n-          \"Trace dependencies\")                                             \\\n-                                                                            \\\n@@ -1201,1 +1204,1 @@\n-          \"Inline intrinsics that can be statically resolved\")              \\\n+          \"Use intrinsics in Interpreter that can be statically resolved\")  \\\n@@ -1304,2 +1307,2 @@\n-  product(bool, UseSystemMemoryBarrier, false, EXPERIMENTAL,                \\\n-          \"Try to enable system memory barrier\")                            \\\n+  product(bool, UseSystemMemoryBarrier, false,                              \\\n+          \"Try to enable system memory barrier if supported by OS\")         \\\n@@ -1892,1 +1895,1 @@\n-  product(size_t, ArrayAllocatorMallocLimit, (size_t)-1, EXPERIMENTAL,      \\\n+  product(size_t, ArrayAllocatorMallocLimit, SIZE_MAX, EXPERIMENTAL,        \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":15,"deletions":12,"binary":false,"changes":27,"status":"modified"},{"patch":"@@ -239,1 +239,1 @@\n-                            THREAD);\n+                            CHECK);\n@@ -250,1 +250,1 @@\n-                            THREAD);\n+                            CHECK);\n@@ -380,0 +380,3 @@\n+  \/\/ Don't complain if running a debugging command.\n+  if (DebuggingContext::is_enabled()) return;\n+\n@@ -695,0 +698,4 @@\n+  if (AlwaysPreTouchStacks) {\n+    pretouch_stack();\n+  }\n+\n@@ -737,1 +744,3 @@\n-  \/\/ to complete once we've done the notify_all below\n+  \/\/ to complete once we've done the notify_all below. Needs a release() to obey Java Memory Model\n+  \/\/ requirements.\n+  OrderAccess::release();\n@@ -2033,3 +2042,1 @@\n-  \/\/ We could get here with a pending exception, if so clear it now or\n-  \/\/ it will cause MetaspaceShared::link_shared_classes to\n-  \/\/ fail for dynamic dump.\n+  \/\/ We could get here with a pending exception, if so clear it now.\n@@ -2040,9 +2047,0 @@\n-#if INCLUDE_CDS\n-  \/\/ Link all classes for dynamic CDS dumping before vm exit.\n-  \/\/ Same operation is being done in JVM_BeforeHalt for handling the\n-  \/\/ case where the application calls System.exit().\n-  if (DynamicArchive::should_dump_at_vm_exit()) {\n-    DynamicArchive::prepare_for_dump_at_exit();\n-  }\n-#endif\n-\n@@ -2079,2 +2077,1 @@\n-Handle JavaThread::create_system_thread_object(const char* name,\n-                                               bool is_visible, TRAPS) {\n+Handle JavaThread::create_system_thread_object(const char* name, TRAPS) {\n@@ -2137,0 +2134,19 @@\n+void JavaThread::pretouch_stack() {\n+  \/\/ Given an established java thread stack with usable area followed by\n+  \/\/ shadow zone and reserved\/yellow\/red zone, pretouch the usable area ranging\n+  \/\/ from the current frame down to the start of the shadow zone.\n+  const address end = _stack_overflow_state.shadow_zone_safe_limit();\n+  if (is_in_full_stack(end)) {\n+    char* p1 = (char*) alloca(1);\n+    address here = (address) &p1;\n+    if (is_in_full_stack(here) && here > end) {\n+      size_t to_alloc = here - end;\n+      char* p2 = (char*) alloca(to_alloc);\n+      log_trace(os, thread)(\"Pretouching thread stack from \" PTR_FORMAT \" to \" PTR_FORMAT \".\",\n+                            p2i(p2), p2i(end));\n+      os::pretouch_memory(p2, p2 + to_alloc,\n+                          NOT_AIX(os::vm_page_size()) AIX_ONLY(4096));\n+    }\n+  }\n+}\n+\n","filename":"src\/hotspot\/share\/runtime\/javaThread.cpp","additions":33,"deletions":17,"binary":false,"changes":50,"status":"modified"},{"patch":"@@ -419,0 +419,2 @@\n+  void pretouch_stack();\n+\n@@ -815,0 +817,5 @@\n+#if INCLUDE_JVMTI\n+  static ByteSize is_in_VTMS_transition_offset()     { return byte_offset_of(JavaThread, _is_in_VTMS_transition); }\n+  static ByteSize is_in_tmp_VTMS_transition_offset() { return byte_offset_of(JavaThread, _is_in_tmp_VTMS_transition); }\n+#endif\n+\n@@ -1159,4 +1166,3 @@\n-  \/\/ VM-internal thread. The thread will have the given name, be\n-  \/\/ part of the System ThreadGroup and if is_visible is true will be\n-  \/\/ discoverable via the system ThreadGroup.\n-  static Handle create_system_thread_object(const char* name, bool is_visible, TRAPS);\n+  \/\/ VM-internal thread. The thread will have the given name and be\n+  \/\/ part of the System ThreadGroup.\n+  static Handle create_system_thread_object(const char* name, TRAPS);\n","filename":"src\/hotspot\/share\/runtime\/javaThread.hpp","additions":10,"deletions":4,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -1359,5 +1359,0 @@\n-\n-\/\/ -----------------------------------------------------------------------------\n-\/\/ Class Loader deadlock handling.\n-\/\/\n-\/\/ complete_exit\/reenter operate as a wait without waiting\n@@ -1389,14 +1384,0 @@\n-\/\/ reenter() enters a lock and sets recursion count\n-\/\/ complete_exit\/reenter operate as a wait without waiting\n-bool ObjectMonitor::reenter(intx recursions, JavaThread* current) {\n-\n-  guarantee(owner_raw() != current, \"reenter already owner\");\n-  if (!enter(current)) {\n-    return false;\n-  }\n-  \/\/ Entered the monitor.\n-  guarantee(_recursions == 0, \"reenter recursion\");\n-  _recursions = recursions;\n-  return true;\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.cpp","additions":0,"deletions":19,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -345,1 +345,0 @@\n-  bool      reenter(intx recursions, JavaThread* current);\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":0,"deletions":1,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"prims\/jvmtiAgent.hpp\"\n@@ -480,1 +481,1 @@\n-    Handle thread_oop = JavaThread::create_system_thread_object(name, true \/* visible *\/, CHECK);\n+    Handle thread_oop = JavaThread::create_system_thread_object(name, CHECK);\n@@ -538,1 +539,1 @@\n-void* os::find_agent_function(AgentLibrary *agent_lib, bool check_lib,\n+void* os::find_agent_function(JvmtiAgent *agent_lib, bool check_lib,\n@@ -565,1 +566,1 @@\n-bool os::find_builtin_agent(AgentLibrary *agent_lib, const char *syms[],\n+bool os::find_builtin_agent(JvmtiAgent* agent, const char *syms[],\n@@ -571,2 +572,2 @@\n-  assert(agent_lib != nullptr, \"sanity check\");\n-  if (agent_lib->name() == nullptr) {\n+  assert(agent != nullptr, \"sanity check\");\n+  if (agent->name() == nullptr) {\n@@ -577,1 +578,1 @@\n-  save_handle = agent_lib->os_lib();\n+  save_handle = agent->os_lib();\n@@ -579,2 +580,2 @@\n-  agent_lib->set_os_lib(proc_handle);\n-  ret = find_agent_function(agent_lib, true, syms, syms_len);\n+  agent->set_os_lib(proc_handle);\n+  ret = find_agent_function(agent, true, syms, syms_len);\n@@ -583,2 +584,2 @@\n-    agent_lib->set_valid();\n-    agent_lib->set_static_lib(true);\n+    agent->set_static_lib();\n+    agent->set_loaded();\n@@ -587,1 +588,1 @@\n-  agent_lib->set_os_lib(save_handle);\n+  agent->set_os_lib(save_handle);\n@@ -1206,0 +1207,5 @@\n+  \/\/ Still nothing? If NMT is enabled, we can ask what it thinks...\n+  if (MemTracker::print_containing_region(addr, st)) {\n+    return;\n+  }\n+\n@@ -1822,1 +1828,1 @@\n-  os::print_memory_mappings(nullptr, (size_t)-1, st);\n+  os::print_memory_mappings(nullptr, SIZE_MAX, st);\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":18,"deletions":12,"binary":false,"changes":30,"status":"modified"},{"patch":"@@ -250,2 +250,10 @@\n-static const int NINFLATIONLOCKS = 256;\n-static PlatformMutex* gInflationLocks[NINFLATIONLOCKS];\n+static constexpr size_t inflation_lock_count() {\n+  return 256;\n+}\n+\n+\/\/ Static storage for an array of PlatformMutex.\n+alignas(PlatformMutex) static uint8_t _inflation_locks[inflation_lock_count()][sizeof(PlatformMutex)];\n+\n+static inline PlatformMutex* inflation_lock(size_t index) {\n+  return reinterpret_cast<PlatformMutex*>(_inflation_locks[index]);\n+}\n@@ -254,2 +262,2 @@\n-  for (int i = 0; i < NINFLATIONLOCKS; i++) {\n-    gInflationLocks[i] = new PlatformMutex();\n+  for (size_t i = 0; i < inflation_lock_count(); i++) {\n+    ::new(static_cast<void*>(inflation_lock(i))) PlatformMutex();\n@@ -259,0 +267,3 @@\n+\n+  \/\/ Start the timer for deflations, so it does not trigger immediately.\n+  _last_async_deflation_time_ns = os::javaTimeNanos();\n@@ -287,0 +298,1 @@\n+static bool _no_progress_skip_increment = false;\n@@ -632,36 +644,0 @@\n-\/\/ -----------------------------------------------------------------------------\n-\/\/ Class Loader  support to workaround deadlocks on the class loader lock objects\n-\/\/ Also used by GC\n-\/\/ complete_exit()\/reenter() are used to wait on a nested lock\n-\/\/ i.e. to give up an outer lock completely and then re-enter\n-\/\/ Used when holding nested locks - lock acquisition order: lock1 then lock2\n-\/\/  1) complete_exit lock1 - saving recursion count\n-\/\/  2) wait on lock2\n-\/\/  3) when notified on lock2, unlock lock2\n-\/\/  4) reenter lock1 with original recursion count\n-\/\/  5) lock lock2\n-\/\/ NOTE: must use heavy weight monitor to handle complete_exit\/reenter()\n-intx ObjectSynchronizer::complete_exit(Handle obj, JavaThread* current) {\n-  \/\/ The ObjectMonitor* can't be async deflated until ownership is\n-  \/\/ dropped inside exit() and the ObjectMonitor* must be !is_busy().\n-  ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_vm_internal);\n-  intx recur_count = monitor->complete_exit(current);\n-  current->dec_held_monitor_count(recur_count + 1);\n-  return recur_count;\n-}\n-\n-\/\/ NOTE: must use heavy weight monitor to handle complete_exit\/reenter()\n-void ObjectSynchronizer::reenter(Handle obj, intx recursions, JavaThread* current) {\n-  \/\/ An async deflation can race after the inflate() call and before\n-  \/\/ reenter() -> enter() can make the ObjectMonitor busy. reenter() ->\n-  \/\/ enter() returns false if we have lost the race to async deflation\n-  \/\/ and we simply try again.\n-  while (true) {\n-    ObjectMonitor* monitor = inflate(current, obj(), inflate_cause_vm_internal);\n-    if (monitor->reenter(recursions, current)) {\n-      current->inc_held_monitor_count(recursions + 1);\n-      return;\n-    }\n-  }\n-}\n-\n@@ -835,2 +811,2 @@\n-        static_assert(is_power_of_2(NINFLATIONLOCKS), \"must be\");\n-        int ix = (cast_from_oop<intptr_t>(obj) >> 5) & (NINFLATIONLOCKS-1);\n+        static_assert(is_power_of_2(inflation_lock_count()), \"must be\");\n+        size_t ix = (cast_from_oop<intptr_t>(obj) >> 5) & (inflation_lock_count() - 1);\n@@ -838,2 +814,2 @@\n-        assert(ix >= 0 && ix < NINFLATIONLOCKS, \"invariant\");\n-        gInflationLocks[ix]->lock();\n+        assert(ix < inflation_lock_count(), \"invariant\");\n+        inflation_lock(ix)->lock();\n@@ -850,1 +826,1 @@\n-        gInflationLocks[ix]->unlock();\n+        inflation_lock(ix)->unlock();\n@@ -1177,1 +1153,8 @@\n-  return int(monitor_usage) > MonitorUsedDeflationThreshold;\n+  if (int(monitor_usage) > MonitorUsedDeflationThreshold) {\n+    log_info(monitorinflation)(\"monitors_used=\" SIZE_FORMAT \", ceiling=\" SIZE_FORMAT\n+                               \", monitor_usage=\" SIZE_FORMAT \", threshold=\" INTX_FORMAT,\n+                               monitors_used, ceiling, monitor_usage, MonitorUsedDeflationThreshold);\n+    return true;\n+  }\n+\n+  return false;\n@@ -1199,0 +1182,1 @@\n+    log_info(monitorinflation)(\"Async deflation needed: explicit request\");\n@@ -1201,0 +1185,3 @@\n+\n+  jlong time_since_last = time_since_last_async_deflation_ms();\n+\n@@ -1202,1 +1189,1 @@\n-      time_since_last_async_deflation_ms() > AsyncDeflationInterval &&\n+      time_since_last > AsyncDeflationInterval &&\n@@ -1208,0 +1195,1 @@\n+    log_info(monitorinflation)(\"Async deflation needed: monitors used are above the threshold\");\n@@ -1210,0 +1198,27 @@\n+\n+  if (GuaranteedAsyncDeflationInterval > 0 &&\n+      time_since_last > GuaranteedAsyncDeflationInterval) {\n+    \/\/ It's been longer than our specified guaranteed deflate interval.\n+    \/\/ We need to clean up the used monitors even if the threshold is\n+    \/\/ not reached, to keep the memory utilization at bay when many threads\n+    \/\/ touched many monitors.\n+    log_info(monitorinflation)(\"Async deflation needed: guaranteed interval (\" INTX_FORMAT \" ms) \"\n+                               \"is greater than time since last deflation (\" JLONG_FORMAT \" ms)\",\n+                               GuaranteedAsyncDeflationInterval, time_since_last);\n+\n+    \/\/ If this deflation has no progress, then it should not affect the no-progress\n+    \/\/ tracking, otherwise threshold heuristics would think it was triggered, experienced\n+    \/\/ no progress, and needs to backoff more aggressively. In this \"no progress\" case,\n+    \/\/ the generic code would bump the no-progress counter, and we compensate for that\n+    \/\/ by telling it to skip the update.\n+    \/\/\n+    \/\/ If this deflation has progress, then it should let non-progress tracking\n+    \/\/ know about this, otherwise the threshold heuristics would kick in, potentially\n+    \/\/ experience no-progress due to aggressive cleanup by this deflation, and think\n+    \/\/ it is still in no-progress stride. In this \"progress\" case, the generic code would\n+    \/\/ zero the counter, and we allow it to happen.\n+    _no_progress_skip_increment = true;\n+\n+    return true;\n+  }\n+\n@@ -1693,0 +1708,2 @@\n+  } else if (_no_progress_skip_increment) {\n+    _no_progress_skip_increment = false;\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":64,"deletions":47,"binary":false,"changes":111,"status":"modified"},{"patch":"@@ -61,9 +61,0 @@\n-void* Thread::allocate(size_t size, bool throw_excpt, MEMFLAGS flags) {\n-  return throw_excpt ? AllocateHeap(size, flags, CURRENT_PC)\n-                       : AllocateHeap(size, flags, CURRENT_PC, AllocFailStrategy::RETURN_NULL);\n-}\n-\n-void Thread::operator delete(void* p) {\n-  FreeHeap(p);\n-}\n-\n","filename":"src\/hotspot\/share\/runtime\/thread.cpp","additions":0,"deletions":9,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -59,0 +59,1 @@\n+#include \"prims\/jvmtiAgentList.hpp\"\n@@ -99,1 +100,0 @@\n-#include \"utilities\/systemMemoryBarrier.hpp\"\n@@ -337,4 +337,0 @@\n-  if (EagerXrunInit && Arguments::init_libraries_at_startup()) {\n-    create_vm_init_libraries();\n-  }\n-\n@@ -504,9 +500,1 @@\n-  \/\/ Convert -Xrun to -agentlib: if there is no JVM_OnLoad\n-  \/\/ Must be before create_vm_init_agents()\n-  if (Arguments::init_libraries_at_startup()) {\n-    convert_vm_init_libraries_to_agents();\n-  }\n-\n-  if (Arguments::init_agents_at_startup()) {\n-    create_vm_init_agents();\n-  }\n+  JvmtiAgentList::load_agents();\n@@ -556,8 +544,0 @@\n-  if (UseSystemMemoryBarrier) {\n-    if (!SystemMemoryBarrier::initialize()) {\n-      vm_shutdown_during_initialization(\"Failed to initialize the requested system memory barrier synchronization.\");\n-      return JNI_EINVAL;\n-    }\n-    log_debug(os)(\"Using experimental system memory barrier synchronization\");\n-  }\n-\n@@ -636,0 +616,5 @@\n+  \/\/ Launch -Xrun agents early if EagerXrunInit is set\n+  if (EagerXrunInit) {\n+    JvmtiAgentList::load_xrun_agents();\n+  }\n+\n@@ -658,0 +643,2 @@\n+  log_info(os)(\"Initialized VM with process ID %d\", os::current_process_id());\n+\n@@ -669,5 +656,3 @@\n-  \/\/ Launch -Xrun agents\n-  \/\/ Must be done in the JVMTI live phase so that for backward compatibility the JDWP\n-  \/\/ back-end can launch with -Xdebug -Xrunjdwp.\n-  if (!EagerXrunInit && Arguments::init_libraries_at_startup()) {\n-    create_vm_init_libraries();\n+  \/\/ Launch -Xrun agents if EagerXrunInit is not set.\n+  if (!EagerXrunInit) {\n+    JvmtiAgentList::load_xrun_agents();\n@@ -815,202 +800,0 @@\n-\/\/ type for the Agent_OnLoad and JVM_OnLoad entry points\n-extern \"C\" {\n-  typedef jint (JNICALL *OnLoadEntry_t)(JavaVM *, char *, void *);\n-}\n-\/\/ Find a command line agent library and return its entry point for\n-\/\/         -agentlib:  -agentpath:   -Xrun\n-\/\/ num_symbol_entries must be passed-in since only the caller knows the number of symbols in the array.\n-static OnLoadEntry_t lookup_on_load(AgentLibrary* agent,\n-                                    const char *on_load_symbols[],\n-                                    size_t num_symbol_entries) {\n-  OnLoadEntry_t on_load_entry = nullptr;\n-  void *library = nullptr;\n-\n-  if (!agent->valid()) {\n-    char buffer[JVM_MAXPATHLEN];\n-    char ebuf[1024] = \"\";\n-    const char *name = agent->name();\n-    const char *msg = \"Could not find agent library \";\n-\n-    \/\/ First check to see if agent is statically linked into executable\n-    if (os::find_builtin_agent(agent, on_load_symbols, num_symbol_entries)) {\n-      library = agent->os_lib();\n-    } else if (agent->is_absolute_path()) {\n-      library = os::dll_load(name, ebuf, sizeof ebuf);\n-      if (library == nullptr) {\n-        const char *sub_msg = \" in absolute path, with error: \";\n-        size_t len = strlen(msg) + strlen(name) + strlen(sub_msg) + strlen(ebuf) + 1;\n-        char *buf = NEW_C_HEAP_ARRAY(char, len, mtThread);\n-        jio_snprintf(buf, len, \"%s%s%s%s\", msg, name, sub_msg, ebuf);\n-        \/\/ If we can't find the agent, exit.\n-        vm_exit_during_initialization(buf, nullptr);\n-        FREE_C_HEAP_ARRAY(char, buf);\n-      }\n-    } else {\n-      \/\/ Try to load the agent from the standard dll directory\n-      if (os::dll_locate_lib(buffer, sizeof(buffer), Arguments::get_dll_dir(),\n-                             name)) {\n-        library = os::dll_load(buffer, ebuf, sizeof ebuf);\n-      }\n-      if (library == nullptr) { \/\/ Try the library path directory.\n-        if (os::dll_build_name(buffer, sizeof(buffer), name)) {\n-          library = os::dll_load(buffer, ebuf, sizeof ebuf);\n-        }\n-        if (library == nullptr) {\n-          const char *sub_msg = \" on the library path, with error: \";\n-          const char *sub_msg2 = \"\\nModule java.instrument may be missing from runtime image.\";\n-\n-          size_t len = strlen(msg) + strlen(name) + strlen(sub_msg) +\n-                       strlen(ebuf) + strlen(sub_msg2) + 1;\n-          char *buf = NEW_C_HEAP_ARRAY(char, len, mtThread);\n-          if (!agent->is_instrument_lib()) {\n-            jio_snprintf(buf, len, \"%s%s%s%s\", msg, name, sub_msg, ebuf);\n-          } else {\n-            jio_snprintf(buf, len, \"%s%s%s%s%s\", msg, name, sub_msg, ebuf, sub_msg2);\n-          }\n-          \/\/ If we can't find the agent, exit.\n-          vm_exit_during_initialization(buf, nullptr);\n-          FREE_C_HEAP_ARRAY(char, buf);\n-        }\n-      }\n-    }\n-    agent->set_os_lib(library);\n-    agent->set_valid();\n-  }\n-\n-  \/\/ Find the OnLoad function.\n-  on_load_entry =\n-    CAST_TO_FN_PTR(OnLoadEntry_t, os::find_agent_function(agent,\n-                                                          false,\n-                                                          on_load_symbols,\n-                                                          num_symbol_entries));\n-  return on_load_entry;\n-}\n-\n-\/\/ Find the JVM_OnLoad entry point\n-static OnLoadEntry_t lookup_jvm_on_load(AgentLibrary* agent) {\n-  const char *on_load_symbols[] = JVM_ONLOAD_SYMBOLS;\n-  return lookup_on_load(agent, on_load_symbols, sizeof(on_load_symbols) \/ sizeof(char*));\n-}\n-\n-\/\/ Find the Agent_OnLoad entry point\n-static OnLoadEntry_t lookup_agent_on_load(AgentLibrary* agent) {\n-  const char *on_load_symbols[] = AGENT_ONLOAD_SYMBOLS;\n-  return lookup_on_load(agent, on_load_symbols, sizeof(on_load_symbols) \/ sizeof(char*));\n-}\n-\n-\/\/ For backwards compatibility with -Xrun\n-\/\/ Convert libraries with no JVM_OnLoad, but which have Agent_OnLoad to be\n-\/\/ treated like -agentpath:\n-\/\/ Must be called before agent libraries are created\n-void Threads::convert_vm_init_libraries_to_agents() {\n-  AgentLibrary* agent;\n-  AgentLibrary* next;\n-\n-  for (agent = Arguments::libraries(); agent != nullptr; agent = next) {\n-    next = agent->next();  \/\/ cache the next agent now as this agent may get moved off this list\n-    OnLoadEntry_t on_load_entry = lookup_jvm_on_load(agent);\n-\n-    \/\/ If there is an JVM_OnLoad function it will get called later,\n-    \/\/ otherwise see if there is an Agent_OnLoad\n-    if (on_load_entry == nullptr) {\n-      on_load_entry = lookup_agent_on_load(agent);\n-      if (on_load_entry != nullptr) {\n-        \/\/ switch it to the agent list -- so that Agent_OnLoad will be called,\n-        \/\/ JVM_OnLoad won't be attempted and Agent_OnUnload will\n-        Arguments::convert_library_to_agent(agent);\n-      } else {\n-        vm_exit_during_initialization(\"Could not find JVM_OnLoad or Agent_OnLoad function in the library\", agent->name());\n-      }\n-    }\n-  }\n-}\n-\n-\/\/ Create agents for -agentlib:  -agentpath:  and converted -Xrun\n-\/\/ Invokes Agent_OnLoad\n-\/\/ Called very early -- before JavaThreads exist\n-void Threads::create_vm_init_agents() {\n-  extern struct JavaVM_ main_vm;\n-  AgentLibrary* agent;\n-\n-  JvmtiExport::enter_onload_phase();\n-\n-  for (agent = Arguments::agents(); agent != nullptr; agent = agent->next()) {\n-    \/\/ CDS dumping does not support native JVMTI agent.\n-    \/\/ CDS dumping supports Java agent if the AllowArchivingWithJavaAgent diagnostic option is specified.\n-    if (Arguments::is_dumping_archive()) {\n-      if(!agent->is_instrument_lib()) {\n-        vm_exit_during_cds_dumping(\"CDS dumping does not support native JVMTI agent, name\", agent->name());\n-      } else if (!AllowArchivingWithJavaAgent) {\n-        vm_exit_during_cds_dumping(\n-          \"Must enable AllowArchivingWithJavaAgent in order to run Java agent during CDS dumping\");\n-      }\n-    }\n-\n-    OnLoadEntry_t  on_load_entry = lookup_agent_on_load(agent);\n-\n-    if (on_load_entry != nullptr) {\n-      \/\/ Invoke the Agent_OnLoad function\n-      jint err = (*on_load_entry)(&main_vm, agent->options(), nullptr);\n-      if (err != JNI_OK) {\n-        vm_exit_during_initialization(\"agent library failed to init\", agent->name());\n-      }\n-    } else {\n-      vm_exit_during_initialization(\"Could not find Agent_OnLoad function in the agent library\", agent->name());\n-    }\n-  }\n-\n-  JvmtiExport::enter_primordial_phase();\n-}\n-\n-extern \"C\" {\n-  typedef void (JNICALL *Agent_OnUnload_t)(JavaVM *);\n-}\n-\n-void Threads::shutdown_vm_agents() {\n-  \/\/ Send any Agent_OnUnload notifications\n-  const char *on_unload_symbols[] = AGENT_ONUNLOAD_SYMBOLS;\n-  size_t num_symbol_entries = ARRAY_SIZE(on_unload_symbols);\n-  extern struct JavaVM_ main_vm;\n-  for (AgentLibrary* agent = Arguments::agents(); agent != nullptr; agent = agent->next()) {\n-\n-    \/\/ Find the Agent_OnUnload function.\n-    Agent_OnUnload_t unload_entry = CAST_TO_FN_PTR(Agent_OnUnload_t,\n-                                                   os::find_agent_function(agent,\n-                                                   false,\n-                                                   on_unload_symbols,\n-                                                   num_symbol_entries));\n-\n-    \/\/ Invoke the Agent_OnUnload function\n-    if (unload_entry != nullptr) {\n-      JavaThread* thread = JavaThread::current();\n-      ThreadToNativeFromVM ttn(thread);\n-      HandleMark hm(thread);\n-      (*unload_entry)(&main_vm);\n-    }\n-  }\n-}\n-\n-\/\/ Called for after the VM is initialized for -Xrun libraries which have not been converted to agent libraries\n-\/\/ Invokes JVM_OnLoad\n-void Threads::create_vm_init_libraries() {\n-  extern struct JavaVM_ main_vm;\n-  AgentLibrary* agent;\n-\n-  for (agent = Arguments::libraries(); agent != nullptr; agent = agent->next()) {\n-    OnLoadEntry_t on_load_entry = lookup_jvm_on_load(agent);\n-\n-    if (on_load_entry != nullptr) {\n-      \/\/ Invoke the JVM_OnLoad function\n-      JavaThread* thread = JavaThread::current();\n-      ThreadToNativeFromVM ttn(thread);\n-      HandleMark hm(thread);\n-      jint err = (*on_load_entry)(&main_vm, agent->options(), nullptr);\n-      if (err != JNI_OK) {\n-        vm_exit_during_initialization(\"-Xrun library failed to init\", agent->name());\n-      }\n-    } else {\n-      vm_exit_during_initialization(\"Could not find JVM_OnLoad function in -Xrun library\", agent->name());\n-    }\n-  }\n-}\n-\n@@ -1170,0 +953,1 @@\n+  if (version == JNI_VERSION_21) return JNI_TRUE;\n","filename":"src\/hotspot\/share\/runtime\/threads.cpp","additions":13,"deletions":229,"binary":false,"changes":242,"status":"modified"},{"patch":"@@ -81,0 +81,1 @@\n+  template(SetNotifyJvmtiEventsMode)              \\\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -69,0 +69,1 @@\n+#include \"oops\/fieldInfo.hpp\"\n@@ -223,0 +224,2 @@\n+  nonstatic_field(ConstantPoolCache,           _resolved_indy_entries,                        Array<ResolvedIndyEntry>*)             \\\n+  nonstatic_field(ResolvedIndyEntry,           _cpool_index,                                  u2)                                    \\\n@@ -228,2 +231,1 @@\n-  nonstatic_field(InstanceKlass,               _fields,                                       Array<u2>*)                            \\\n-  nonstatic_field(InstanceKlass,               _java_fields_count,                            u2)                                    \\\n+  nonstatic_field(InstanceKlass,               _fieldinfo_stream,                             Array<u1>*)                            \\\n@@ -237,1 +239,0 @@\n-  nonstatic_field(InstanceKlass,               _is_marked_dependent,                          bool)                                  \\\n@@ -476,0 +477,2 @@\n+  nonstatic_field(Array<ResolvedIndyEntry>,    _length,                                       int)                                   \\\n+  nonstatic_field(Array<ResolvedIndyEntry>,    _data[0],                                      ResolvedIndyEntry)                     \\\n@@ -1025,6 +1028,7 @@\n-  nonstatic_field(Array<int>,                  _length,                                       int)                                   \\\n-  unchecked_nonstatic_field(Array<int>,        _data,                                         sizeof(int))                           \\\n-  unchecked_nonstatic_field(Array<u1>,         _data,                                         sizeof(u1))                            \\\n-  unchecked_nonstatic_field(Array<u2>,         _data,                                         sizeof(u2))                            \\\n-  unchecked_nonstatic_field(Array<Method*>,    _data,                                         sizeof(Method*))                       \\\n-  unchecked_nonstatic_field(Array<Klass*>,     _data,                                         sizeof(Klass*))                        \\\n+  nonstatic_field(Array<int>,                         _length,                                int)                                   \\\n+  unchecked_nonstatic_field(Array<int>,               _data,                                  sizeof(int))                           \\\n+  unchecked_nonstatic_field(Array<u1>,                _data,                                  sizeof(u1))                            \\\n+  unchecked_nonstatic_field(Array<u2>,                _data,                                  sizeof(u2))                            \\\n+  unchecked_nonstatic_field(Array<Method*>,           _data,                                  sizeof(Method*))                       \\\n+  unchecked_nonstatic_field(Array<Klass*>,            _data,                                  sizeof(Klass*))                        \\\n+  unchecked_nonstatic_field(Array<ResolvedIndyEntry>, _data,                                  sizeof(ResolvedIndyEntry))             \\\n@@ -1771,0 +1775,2 @@\n+  declare_c2_type(CompressBitsVNode, VectorNode)                          \\\n+  declare_c2_type(ExpandBitsVNode, VectorNode)                            \\\n@@ -1959,0 +1965,1 @@\n+            declare_type(Array<ResolvedIndyEntry>, MetaspaceObj)          \\\n@@ -1975,0 +1982,1 @@\n+  declare_toplevel_type(ResolvedIndyEntry)                                \\\n@@ -2088,2 +2096,0 @@\n-  declare_constant(JVM_ACC_HAS_LINE_NUMBER_TABLE)                         \\\n-  declare_constant(JVM_ACC_HAS_CHECKED_EXCEPTIONS)                        \\\n@@ -2094,2 +2100,0 @@\n-  declare_constant(JVM_ACC_HAS_MIRANDA_METHODS)                           \\\n-  declare_constant(JVM_ACC_HAS_VANILLA_CONSTRUCTOR)                       \\\n@@ -2098,6 +2102,0 @@\n-  declare_constant(JVM_ACC_HAS_LOCAL_VARIABLE_TABLE)                      \\\n-  declare_constant(JVM_ACC_FIELD_ACCESS_WATCHED)                          \\\n-  declare_constant(JVM_ACC_FIELD_MODIFICATION_WATCHED)                    \\\n-  declare_constant(JVM_ACC_FIELD_INTERNAL)                                \\\n-  declare_constant(JVM_ACC_FIELD_STABLE)                                  \\\n-  declare_constant(JVM_ACC_FIELD_HAS_GENERIC_SIGNATURE)                   \\\n@@ -2235,18 +2233,0 @@\n-  \/*************************************\/                                 \\\n-  \/* FieldInfo FieldOffset enum        *\/                                 \\\n-  \/*************************************\/                                 \\\n-                                                                          \\\n-  declare_constant(FieldInfo::access_flags_offset)                        \\\n-  declare_constant(FieldInfo::name_index_offset)                          \\\n-  declare_constant(FieldInfo::signature_index_offset)                     \\\n-  declare_constant(FieldInfo::initval_index_offset)                       \\\n-  declare_constant(FieldInfo::low_packed_offset)                          \\\n-  declare_constant(FieldInfo::high_packed_offset)                         \\\n-  declare_constant(FieldInfo::field_slots)                                \\\n-                                                                          \\\n-  \/*************************************\/                                 \\\n-  \/* FieldInfo tag constants           *\/                                 \\\n-  \/*************************************\/                                 \\\n-                                                                          \\\n-  declare_preprocessor_constant(\"FIELDINFO_TAG_SIZE\", FIELDINFO_TAG_SIZE) \\\n-  declare_preprocessor_constant(\"FIELDINFO_TAG_OFFSET\", FIELDINFO_TAG_OFFSET) \\\n@@ -2322,0 +2302,11 @@\n+                                                                          \\\n+  \/******************************\/                                        \\\n+  \/* FieldFlags enum            *\/                                        \\\n+  \/******************************\/                                        \\\n+                                                                          \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_initialized)                \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_injected)                   \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_generic)                    \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_stable)                     \\\n+  declare_constant(FieldInfo::FieldFlags::_ff_contended)                  \\\n+                                                                          \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":28,"deletions":37,"binary":false,"changes":65,"status":"modified"},{"patch":"@@ -28,0 +28,1 @@\n+#include \"utilities\/attributeNoreturn.hpp\"\n@@ -1228,0 +1229,1 @@\n+\n@@ -1229,0 +1231,14 @@\n+\/\/\n+\/\/ C++14 5.8\/3: In the description of \"E1 >> E2\" it says \"If E1 has a signed type\n+\/\/ and a negative value, the resulting value is implementation-defined.\"\n+\/\/\n+\/\/ However, C++20 7.6.7\/3 further defines integral arithmetic, as part of\n+\/\/ requiring two's-complement behavior.\n+\/\/ https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2018\/p0907r3.html\n+\/\/ https:\/\/www.open-std.org\/jtc1\/sc22\/wg21\/docs\/papers\/2018\/p1236r1.html\n+\/\/ The corresponding C++20 text is \"Right-shift on signed integral types is an\n+\/\/ arithmetic right shift, which performs sign-extension.\"\n+\/\/\n+\/\/ As discussed in the two's complement proposal, all known modern C++ compilers\n+\/\/ already behave that way. And it is unlikely any would go off and do something\n+\/\/ different now, with C++20 tightening things up.\n@@ -1259,0 +1275,32 @@\n+\/\/ Taken from rom section 8-2 of Henry S. Warren, Jr., Hacker's Delight (2nd ed.) (Addison Wesley, 2013), 173-174.\n+inline uint64_t multiply_high_unsigned(const uint64_t x, const uint64_t y) {\n+  const uint64_t x1 = x >> 32u;\n+  const uint64_t x2 = x & 0xFFFFFFFF;\n+  const uint64_t y1 = y >> 32u;\n+  const uint64_t y2 = y & 0xFFFFFFFF;\n+  const uint64_t z2 = x2 * y2;\n+  const uint64_t t = x1 * y2 + (z2 >> 32u);\n+  uint64_t z1 = t & 0xFFFFFFFF;\n+  const uint64_t z0 = t >> 32u;\n+  z1 += x2 * y1;\n+\n+  return x1 * y1 + z0 + (z1 >> 32u);\n+}\n+\n+\/\/ Taken from java.lang.Math::multiplyHigh which uses the technique from section 8-2 of Henry S. Warren, Jr.,\n+\/\/ Hacker's Delight (2nd ed.) (Addison Wesley, 2013), 173-174 but adapted for signed longs.\n+inline int64_t multiply_high_signed(const int64_t x, const int64_t y) {\n+  const jlong x1 = java_shift_right((jlong)x, 32);\n+  const jlong x2 = x & 0xFFFFFFFF;\n+  const jlong y1 = java_shift_right((jlong)y, 32);\n+  const jlong y2 = y & 0xFFFFFFFF;\n+\n+  const uint64_t z2 = x2 * y2;\n+  const int64_t t = x1 * y2 + (z2 >> 32u); \/\/ Unsigned shift\n+  int64_t z1 = t & 0xFFFFFFFF;\n+  const int64_t z0 = java_shift_right((jlong)t, 32);\n+  z1 += x2 * y1;\n+\n+  return x1 * y1 + z0 + java_shift_right((jlong)z1, 32);\n+}\n+\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":48,"deletions":0,"binary":false,"changes":48,"status":"modified"},{"patch":"@@ -4,0 +4,1 @@\n+ * Copyright (c) 2023, Red Hat, Inc. and\/or its affiliates.\n@@ -35,0 +36,1 @@\n+#include \"memory\/allocation.hpp\"\n@@ -1748,3 +1750,13 @@\n-\/\/ Timeout handling: check if a timeout happened (either a single step did\n-\/\/ timeout or the whole of error reporting hit ErrorLogTimeout). Interrupt\n-\/\/ the reporting thread if that is the case.\n+\/\/ Fatal error handling is subject to several timeouts:\n+\/\/ - a global timeout (controlled via ErrorLogTimeout)\n+\/\/ - local error reporting step timeouts.\n+\/\/\n+\/\/ The latter aims to \"give the JVM a kick\" if it gets stuck in one particular place during\n+\/\/ error reporting. This prevents one error reporting step from hogging all the time allotted\n+\/\/ to error reporting under ErrorLogTimeout.\n+\/\/\n+\/\/ VMError::check_timeout() is called from the watcher thread and checks for either global\n+\/\/ or step timeout. If a timeout happened, we interrupt the reporting thread and set either\n+\/\/ _reporting_did_timeout or _step_did_timeout to signal which timeout fired. Function returns\n+\/\/ true if the *global* timeout fired, which will cause WatcherThread to shut down the JVM\n+\/\/ immediately.\n@@ -1753,0 +1765,4 @@\n+  \/\/ This function is supposed to be called from watcher thread during fatal error handling only.\n+  assert(VMError::is_error_reported(), \"Only call during error handling\");\n+  assert(Thread::current()->is_Watcher_thread(), \"Only call from watcher thread\");\n+\n@@ -1757,7 +1773,11 @@\n-  \/\/ Do not check for timeouts if we still have a message box to show to the\n-  \/\/ user or if there are OnError handlers to be run.\n-  if (ShowMessageBoxOnError\n-      || (OnError != nullptr && OnError[0] != '\\0')\n-      || Arguments::abort_hook() != nullptr) {\n-    return false;\n-  }\n+  \/\/ There are three situations where we suppress the *global* error timeout:\n+  \/\/ - if the JVM is embedded and the launcher has its abort hook installed.\n+  \/\/   That must be allowed to run.\n+  \/\/ - if the user specified one or more OnError commands to run, and these\n+  \/\/   did not yet run. These must have finished.\n+  \/\/ - if the user (typically developer) specified ShowMessageBoxOnError,\n+  \/\/   and the error box has not yet been shown\n+  const bool ignore_global_timeout =\n+      (ShowMessageBoxOnError\n+            || (OnError != nullptr && OnError[0] != '\\0')\n+            || Arguments::abort_hook() != nullptr);\n@@ -1765,10 +1785,14 @@\n-  const jlong reporting_start_time_l = get_reporting_start_time();\n-  \/\/ Timestamp is stored in nanos.\n-  if (reporting_start_time_l > 0) {\n-    const jlong end = reporting_start_time_l + (jlong)ErrorLogTimeout * TIMESTAMP_TO_SECONDS_FACTOR;\n-    if (end <= now && !_reporting_did_timeout) {\n-      \/\/ We hit ErrorLogTimeout and we haven't interrupted the reporting\n-      \/\/ thread yet.\n-      _reporting_did_timeout = true;\n-      interrupt_reporting_thread();\n-      return true; \/\/ global timeout\n+\n+  \/\/ Global timeout hit?\n+  if (!ignore_global_timeout) {\n+    const jlong reporting_start_time = get_reporting_start_time();\n+    \/\/ Timestamp is stored in nanos.\n+    if (reporting_start_time > 0) {\n+      const jlong end = reporting_start_time + (jlong)ErrorLogTimeout * TIMESTAMP_TO_SECONDS_FACTOR;\n+      if (end <= now && !_reporting_did_timeout) {\n+        \/\/ We hit ErrorLogTimeout and we haven't interrupted the reporting\n+        \/\/ thread yet.\n+        _reporting_did_timeout = true;\n+        interrupt_reporting_thread();\n+        return true; \/\/ global timeout\n+      }\n@@ -1779,2 +1803,3 @@\n-  const jlong step_start_time_l = get_step_start_time();\n-  if (step_start_time_l > 0) {\n+  \/\/ Reporting step timeout?\n+  const jlong step_start_time = get_step_start_time();\n+  if (step_start_time > 0) {\n@@ -1784,1 +1809,3 @@\n-    const jlong end = step_start_time_l + (jlong)ErrorLogTimeout * TIMESTAMP_TO_SECONDS_FACTOR \/ 4;\n+    const int max_step_timeout_secs = 5;\n+    const jlong timeout_duration = MAX2((jlong)max_step_timeout_secs, (jlong)ErrorLogTimeout * TIMESTAMP_TO_SECONDS_FACTOR \/ 4);\n+    const jlong end = step_start_time + timeout_duration;\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":50,"deletions":23,"binary":false,"changes":73,"status":"modified"},{"patch":"@@ -0,0 +1,392 @@\n+\/*\n+ * Copyright (c) 2011, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+package jdk.vm.ci.hotspot;\n+\n+import static jdk.vm.ci.hotspot.HotSpotJVMCIRuntime.runtime;\n+import static jdk.vm.ci.hotspot.UnsafeAccess.UNSAFE;\n+\n+import jdk.vm.ci.common.JVMCIError;\n+import jdk.vm.ci.services.Services;\n+import jdk.internal.misc.Unsafe;\n+\n+\/**\n+ * Used to access native configuration details.\n+ *\n+ * All non-static, public fields in this class are so that they can be compiled as constants.\n+ *\/\n+class HotSpotVMConfig extends HotSpotVMConfigAccess {\n+\n+    \/**\n+     * Gets the configuration associated with the singleton {@link HotSpotJVMCIRuntime}.\n+     *\/\n+    static HotSpotVMConfig config() {\n+        return runtime().getConfig();\n+    }\n+\n+    private final String osArch = getHostArchitectureName();\n+\n+    HotSpotVMConfig(HotSpotVMConfigStore store) {\n+        super(store);\n+\n+        int speculationLengthBits = getConstant(\"JVMCINMethodData::SPECULATION_LENGTH_BITS\", Integer.class);\n+        JVMCIError.guarantee(HotSpotSpeculationEncoding.LENGTH_BITS == speculationLengthBits, \"%d != %d\", HotSpotSpeculationEncoding.LENGTH_BITS, speculationLengthBits);\n+    }\n+\n+    \/**\n+     * Gets the host architecture name for the purpose of finding the corresponding\n+     * {@linkplain HotSpotJVMCIBackendFactory backend}.\n+     *\/\n+    String getHostArchitectureName() {\n+        String arch = Services.getSavedProperty(\"os.arch\");\n+        switch (arch) {\n+            case \"x86_64\":\n+                return \"amd64\";\n+\n+            default:\n+                return arch;\n+        }\n+    }\n+\n+    final boolean useDeferredInitBarriers = getFlag(\"ReduceInitialCardMarks\", Boolean.class);\n+\n+    final boolean useCompressedOops = getFlag(\"UseCompressedOops\", Boolean.class);\n+\n+    final int objectAlignment = getFlag(\"ObjectAlignmentInBytes\", Integer.class);\n+\n+    \/\/ TODO: Lilliput. Probably ok.\n+    final int hubOffset = 4; \/\/ getFieldOffset(\"oopDesc::_metadata._klass\", Integer.class, \"Klass*\");\n+\n+    final int subklassOffset = getFieldOffset(\"Klass::_subklass\", Integer.class, \"Klass*\");\n+    final int superOffset = getFieldOffset(\"Klass::_super\", Integer.class, \"Klass*\");\n+    final int nextSiblingOffset = getFieldOffset(\"Klass::_next_sibling\", Integer.class, \"Klass*\");\n+    final int superCheckOffsetOffset = getFieldOffset(\"Klass::_super_check_offset\", Integer.class, \"juint\");\n+    final int secondarySuperCacheOffset = getFieldOffset(\"Klass::_secondary_super_cache\", Integer.class, \"Klass*\");\n+\n+    final int classLoaderDataOffset = getFieldOffset(\"Klass::_class_loader_data\", Integer.class, \"ClassLoaderData*\");\n+\n+    \/**\n+     * The offset of the _java_mirror field (of type {@link Class}) in a Klass.\n+     *\/\n+    final int javaMirrorOffset = getFieldOffset(\"Klass::_java_mirror\", Integer.class, \"OopHandle\");\n+\n+    final int klassAccessFlagsOffset = getFieldOffset(\"Klass::_access_flags\", Integer.class, \"AccessFlags\");\n+    final int klassLayoutHelperOffset = getFieldOffset(\"Klass::_layout_helper\", Integer.class, \"jint\");\n+\n+    final int klassLayoutHelperNeutralValue = getConstant(\"Klass::_lh_neutral_value\", Integer.class);\n+    final int klassLayoutHelperInstanceSlowPathBit = getConstant(\"Klass::_lh_instance_slow_path_bit\", Integer.class);\n+\n+    final int vtableEntrySize = getFieldValue(\"CompilerToVM::Data::sizeof_vtableEntry\", Integer.class, \"int\");\n+    final int vtableEntryMethodOffset = getFieldOffset(\"vtableEntry::_method\", Integer.class, \"Method*\");\n+\n+    final int instanceKlassInitStateOffset = getFieldOffset(\"InstanceKlass::_init_state\", Integer.class, \"InstanceKlass::ClassState\");\n+    final int instanceKlassConstantsOffset = getFieldOffset(\"InstanceKlass::_constants\", Integer.class, \"ConstantPool*\");\n+    final int instanceKlassFieldInfoStreamOffset = getFieldOffset(\"InstanceKlass::_fieldinfo_stream\", Integer.class, \"Array<u1>*\");\n+    final int instanceKlassAnnotationsOffset = getFieldOffset(\"InstanceKlass::_annotations\", Integer.class, \"Annotations*\");\n+    final int instanceKlassMiscFlagsOffset = getFieldOffset(\"InstanceKlass::_misc_flags._flags\", Integer.class, \"u2\");\n+    final int klassVtableStartOffset = getFieldValue(\"CompilerToVM::Data::Klass_vtable_start_offset\", Integer.class, \"int\");\n+    final int klassVtableLengthOffset = getFieldValue(\"CompilerToVM::Data::Klass_vtable_length_offset\", Integer.class, \"int\");\n+\n+    final int instanceKlassStateLinked = getConstant(\"InstanceKlass::linked\", Integer.class);\n+    final int instanceKlassStateFullyInitialized = getConstant(\"InstanceKlass::fully_initialized\", Integer.class);\n+    final int instanceKlassStateBeingInitialized = getConstant(\"InstanceKlass::being_initialized\", Integer.class);\n+\n+    final int annotationsFieldAnnotationsOffset = getFieldOffset(\"Annotations::_fields_annotations\", Integer.class, \"Array<AnnotationArray*>*\");\n+    final int annotationsClassAnnotationsOffset = getFieldOffset(\"Annotations::_class_annotations\", Integer.class, \"AnnotationArray*\");\n+    final int fieldsAnnotationsBaseOffset = getFieldValue(\"CompilerToVM::Data::_fields_annotations_base_offset\", Integer.class, \"int\");\n+\n+    final int arrayU1LengthOffset = getFieldOffset(\"Array<int>::_length\", Integer.class, \"int\");\n+    final int arrayU1DataOffset = getFieldOffset(\"Array<u1>::_data\", Integer.class);\n+    final int arrayU2DataOffset = getFieldOffset(\"Array<u2>::_data\", Integer.class);\n+\n+    final int jvmAccHasFinalizer = getConstant(\"JVM_ACC_HAS_FINALIZER\", Integer.class);\n+    final int jvmFieldFlagInternalShift = getConstant(\"FieldInfo::FieldFlags::_ff_injected\", Integer.class);\n+    final int jvmFieldFlagStableShift = getConstant(\"FieldInfo::FieldFlags::_ff_stable\", Integer.class);\n+    final int jvmAccIsCloneableFast = getConstant(\"JVM_ACC_IS_CLONEABLE_FAST\", Integer.class);\n+\n+    \/\/ These modifiers are not public in Modifier so we get them via vmStructs.\n+    final int jvmAccSynthetic = getConstant(\"JVM_ACC_SYNTHETIC\", Integer.class);\n+    final int jvmAccAnnotation = getConstant(\"JVM_ACC_ANNOTATION\", Integer.class);\n+    final int jvmAccBridge = getConstant(\"JVM_ACC_BRIDGE\", Integer.class);\n+    final int jvmAccVarargs = getConstant(\"JVM_ACC_VARARGS\", Integer.class);\n+    final int jvmAccEnum = getConstant(\"JVM_ACC_ENUM\", Integer.class);\n+    final int jvmAccInterface = getConstant(\"JVM_ACC_INTERFACE\", Integer.class);\n+\n+    final int jvmMiscFlagsHasDefaultMethods = getConstant(\"InstanceKlassFlags::_misc_has_nonstatic_concrete_methods\", Integer.class);\n+    final int jvmMiscFlagsDeclaresDefaultMethods = getConstant(\"InstanceKlassFlags::_misc_declares_nonstatic_concrete_methods\", Integer.class);\n+\n+    \/\/ This is only valid on AMD64.\n+    final int runtimeCallStackSize = getConstant(\"frame::arg_reg_save_area_bytes\", Integer.class, osArch.equals(\"amd64\") ? null : 0);\n+\n+    private final int markWordNoHashInPlace = getConstant(\"markWord::no_hash_in_place\", Integer.class);\n+    private final int markWordNoLockInPlace = getConstant(\"markWord::no_lock_in_place\", Integer.class);\n+\n+    \/**\n+     * See {@code markWord::prototype()}.\n+     *\/\n+    long prototypeMarkWord() {\n+        return markWordNoHashInPlace | markWordNoLockInPlace;\n+    }\n+\n+    final int methodAccessFlagsOffset = getFieldOffset(\"Method::_access_flags\", Integer.class, \"AccessFlags\");\n+    final int methodConstMethodOffset = getFieldOffset(\"Method::_constMethod\", Integer.class, \"ConstMethod*\");\n+    final int methodIntrinsicIdOffset = getFieldOffset(\"Method::_intrinsic_id\", Integer.class, \"u2\");\n+    final int methodFlagsOffset = getFieldOffset(\"Method::_flags\", Integer.class, \"u2\");\n+    final int methodVtableIndexOffset = getFieldOffset(\"Method::_vtable_index\", Integer.class, \"int\");\n+\n+    final int methodDataOffset = getFieldOffset(\"Method::_method_data\", Integer.class, \"MethodData*\");\n+    final int methodCodeOffset = getFieldOffset(\"Method::_code\", Integer.class, \"CompiledMethod*\");\n+\n+    final int methodFlagsCallerSensitive = getConstant(\"Method::_caller_sensitive\", Integer.class);\n+    final int methodFlagsForceInline = getConstant(\"Method::_force_inline\", Integer.class);\n+    final int methodFlagsIntrinsicCandidate = getConstant(\"Method::_intrinsic_candidate\", Integer.class);\n+    final int methodFlagsDontInline = getConstant(\"Method::_dont_inline\", Integer.class);\n+    final int methodFlagsReservedStackAccess = getConstant(\"Method::_reserved_stack_access\", Integer.class);\n+    final int nonvirtualVtableIndex = getConstant(\"Method::nonvirtual_vtable_index\", Integer.class);\n+    final int invalidVtableIndex = getConstant(\"Method::invalid_vtable_index\", Integer.class);\n+\n+    final int methodDataSize = getFieldOffset(\"MethodData::_size\", Integer.class, \"int\");\n+    final int methodDataDataSize = getFieldOffset(\"MethodData::_data_size\", Integer.class, \"int\");\n+    final int methodDataOopDataOffset = getFieldOffset(\"MethodData::_data[0]\", Integer.class, \"intptr_t\");\n+    final int methodDataOopTrapHistoryOffset = getFieldOffset(\"MethodData::_compiler_counters._trap_hist._array[0]\", Integer.class, \"u1\");\n+    final int methodDataIRSizeOffset = getFieldOffset(\"MethodData::_jvmci_ir_size\", Integer.class, \"int\");\n+\n+    final int methodDataDecompiles = getFieldOffset(\"MethodData::_compiler_counters._nof_decompiles\", Integer.class, \"uint\");\n+    final int methodDataOverflowRecompiles = getFieldOffset(\"MethodData::_compiler_counters._nof_overflow_recompiles\", Integer.class, \"uint\");\n+    final int methodDataOverflowTraps = getFieldOffset(\"MethodData::_compiler_counters._nof_overflow_traps\", Integer.class, \"uint\");\n+\n+    final int nmethodCompLevelOffset = getFieldOffset(\"nmethod::_comp_level\", Integer.class, \"CompLevel\");\n+\n+    final int compilationLevelNone = getConstant(\"CompLevel_none\", Integer.class);\n+    final int compilationLevelSimple = getConstant(\"CompLevel_simple\", Integer.class);\n+    final int compilationLevelLimitedProfile = getConstant(\"CompLevel_limited_profile\", Integer.class);\n+    final int compilationLevelFullProfile = getConstant(\"CompLevel_full_profile\", Integer.class);\n+    final int compilationLevelFullOptimization = getConstant(\"CompLevel_full_optimization\", Integer.class);\n+\n+    final int compLevelAdjustmentNone = getConstant(\"JVMCIRuntime::none\", Integer.class);\n+    final int compLevelAdjustmentByHolder = getConstant(\"JVMCIRuntime::by_holder\", Integer.class);\n+    final int compLevelAdjustmentByFullSignature = getConstant(\"JVMCIRuntime::by_full_signature\", Integer.class);\n+\n+    final int invocationEntryBci = getConstant(\"InvocationEntryBci\", Integer.class);\n+\n+    final int extraStackEntries = getFieldValue(\"CompilerToVM::Data::Method_extra_stack_entries\", Integer.class, \"int\");\n+\n+    final int constMethodConstantsOffset = getFieldOffset(\"ConstMethod::_constants\", Integer.class, \"ConstantPool*\");\n+    final int constMethodFlagsOffset = getFieldOffset(\"ConstMethod::_flags\", Integer.class, \"u2\");\n+    final int constMethodCodeSizeOffset = getFieldOffset(\"ConstMethod::_code_size\", Integer.class, \"u2\");\n+    final int constMethodNameIndexOffset = getFieldOffset(\"ConstMethod::_name_index\", Integer.class, \"u2\");\n+    final int constMethodSignatureIndexOffset = getFieldOffset(\"ConstMethod::_signature_index\", Integer.class, \"u2\");\n+    final int constMethodMethodIdnumOffset = getFieldOffset(\"ConstMethod::_method_idnum\", Integer.class, \"u2\");\n+    final int constMethodMaxStackOffset = getFieldOffset(\"ConstMethod::_max_stack\", Integer.class, \"u2\");\n+    final int methodMaxLocalsOffset = getFieldOffset(\"ConstMethod::_max_locals\", Integer.class, \"u2\");\n+\n+    final int constMethodHasLineNumberTable = getConstant(\"ConstMethod::_has_linenumber_table\", Integer.class);\n+    final int constMethodHasLocalVariableTable = getConstant(\"ConstMethod::_has_localvariable_table\", Integer.class);\n+    final int constMethodHasMethodAnnotations = getConstant(\"ConstMethod::_has_method_annotations\", Integer.class);\n+    final int constMethodHasParameterAnnotations = getConstant(\"ConstMethod::_has_parameter_annotations\", Integer.class);\n+    final int constMethodHasExceptionTable = getConstant(\"ConstMethod::_has_exception_table\", Integer.class);\n+\n+    final int exceptionTableElementSize = getFieldValue(\"CompilerToVM::Data::sizeof_ExceptionTableElement\", Integer.class, \"int\");\n+    final int exceptionTableElementStartPcOffset = getFieldOffset(\"ExceptionTableElement::start_pc\", Integer.class, \"u2\");\n+    final int exceptionTableElementEndPcOffset = getFieldOffset(\"ExceptionTableElement::end_pc\", Integer.class, \"u2\");\n+    final int exceptionTableElementHandlerPcOffset = getFieldOffset(\"ExceptionTableElement::handler_pc\", Integer.class, \"u2\");\n+    final int exceptionTableElementCatchTypeIndexOffset = getFieldOffset(\"ExceptionTableElement::catch_type_index\", Integer.class, \"u2\");\n+\n+    final int localVariableTableElementSize = getFieldValue(\"CompilerToVM::Data::sizeof_LocalVariableTableElement\", Integer.class, \"int\");\n+    final int localVariableTableElementStartBciOffset = getFieldOffset(\"LocalVariableTableElement::start_bci\", Integer.class, \"u2\");\n+    final int localVariableTableElementLengthOffset = getFieldOffset(\"LocalVariableTableElement::length\", Integer.class, \"u2\");\n+    final int localVariableTableElementNameCpIndexOffset = getFieldOffset(\"LocalVariableTableElement::name_cp_index\", Integer.class, \"u2\");\n+    final int localVariableTableElementDescriptorCpIndexOffset = getFieldOffset(\"LocalVariableTableElement::descriptor_cp_index\", Integer.class, \"u2\");\n+    final int localVariableTableElementSlotOffset = getFieldOffset(\"LocalVariableTableElement::slot\", Integer.class, \"u2\");\n+\n+    final int constantPoolSize = getFieldValue(\"CompilerToVM::Data::sizeof_ConstantPool\", Integer.class, \"int\");\n+    final int constantPoolTagsOffset = getFieldOffset(\"ConstantPool::_tags\", Integer.class, \"Array<u1>*\");\n+    final int constantPoolHolderOffset = getFieldOffset(\"ConstantPool::_pool_holder\", Integer.class, \"InstanceKlass*\");\n+    final int constantPoolLengthOffset = getFieldOffset(\"ConstantPool::_length\", Integer.class, \"int\");\n+    final int constantPoolFlagsOffset = getFieldOffset(\"ConstantPool::_flags\", Integer.class, \"u2\");\n+\n+    final int constantPoolCpCacheIndexTag = getConstant(\"ConstantPool::CPCACHE_INDEX_TAG\", Integer.class);\n+    final int constantPoolHasDynamicConstant = getConstant(\"ConstantPool::_has_dynamic_constant\", Integer.class);\n+    final int constantPoolSourceFileNameIndexOffset = getFieldOffset(\"ConstantPool::_source_file_name_index\", Integer.class, \"u2\");\n+\n+    final int jvmConstantUtf8 = getConstant(\"JVM_CONSTANT_Utf8\", Integer.class);\n+    final int jvmConstantInteger = getConstant(\"JVM_CONSTANT_Integer\", Integer.class);\n+    final int jvmConstantLong = getConstant(\"JVM_CONSTANT_Long\", Integer.class);\n+    final int jvmConstantFloat = getConstant(\"JVM_CONSTANT_Float\", Integer.class);\n+    final int jvmConstantDouble = getConstant(\"JVM_CONSTANT_Double\", Integer.class);\n+    final int jvmConstantClass = getConstant(\"JVM_CONSTANT_Class\", Integer.class);\n+    final int jvmConstantUnresolvedClass = getConstant(\"JVM_CONSTANT_UnresolvedClass\", Integer.class);\n+    final int jvmConstantUnresolvedClassInError = getConstant(\"JVM_CONSTANT_UnresolvedClassInError\", Integer.class);\n+    final int jvmConstantString = getConstant(\"JVM_CONSTANT_String\", Integer.class);\n+    final int jvmConstantFieldref = getConstant(\"JVM_CONSTANT_Fieldref\", Integer.class);\n+    final int jvmConstantMethodref = getConstant(\"JVM_CONSTANT_Methodref\", Integer.class);\n+    final int jvmConstantInterfaceMethodref = getConstant(\"JVM_CONSTANT_InterfaceMethodref\", Integer.class);\n+    final int jvmConstantNameAndType = getConstant(\"JVM_CONSTANT_NameAndType\", Integer.class);\n+    final int jvmConstantMethodHandle = getConstant(\"JVM_CONSTANT_MethodHandle\", Integer.class);\n+    final int jvmConstantMethodHandleInError = getConstant(\"JVM_CONSTANT_MethodHandleInError\", Integer.class);\n+    final int jvmConstantMethodType = getConstant(\"JVM_CONSTANT_MethodType\", Integer.class);\n+    final int jvmConstantMethodTypeInError = getConstant(\"JVM_CONSTANT_MethodTypeInError\", Integer.class);\n+    final int jvmConstantDynamic = getConstant(\"JVM_CONSTANT_Dynamic\", Integer.class);\n+    final int jvmConstantDynamicInError = getConstant(\"JVM_CONSTANT_DynamicInError\", Integer.class);\n+    final int jvmConstantInvokeDynamic = getConstant(\"JVM_CONSTANT_InvokeDynamic\", Integer.class);\n+\n+    final int jvmConstantExternalMax = getConstant(\"JVM_CONSTANT_ExternalMax\", Integer.class);\n+    final int jvmConstantInternalMin = getConstant(\"JVM_CONSTANT_InternalMin\", Integer.class);\n+    final int jvmConstantInternalMax = getConstant(\"JVM_CONSTANT_InternalMax\", Integer.class);\n+\n+    final int heapWordSize = getConstant(\"HeapWordSize\", Integer.class);\n+\n+    final long symbolVmSymbols = getFieldAddress(\"Symbol::_vm_symbols[0]\", \"Symbol*\");\n+    final int vmSymbolsFirstSID = getConstant(\"vmSymbols::FIRST_SID\", Integer.class);\n+    final int vmSymbolsSIDLimit = getConstant(\"vmSymbols::SID_LIMIT\", Integer.class);\n+\n+    final long symbolInit = getFieldValue(\"CompilerToVM::Data::symbol_init\", Long.class);\n+    final long symbolClinit = getFieldValue(\"CompilerToVM::Data::symbol_clinit\", Long.class);\n+\n+    \/**\n+     * Returns the symbol in the {@code vmSymbols} table at position {@code index} as a\n+     * {@link String}.\n+     *\n+     * @param index position in the symbol table\n+     * @return the symbol at position id\n+     *\/\n+    String symbolAt(int index) {\n+        HotSpotJVMCIRuntime runtime = runtime();\n+        assert vmSymbolsFirstSID <= index && index < vmSymbolsSIDLimit : \"index \" + index + \" is out of bounds\";\n+        int offset = index * Unsafe.ADDRESS_SIZE;\n+        return runtime.getCompilerToVM().getSymbol(UNSAFE.getAddress(symbolVmSymbols + offset));\n+    }\n+\n+    final int universeBaseVtableSize = getFieldValue(\"CompilerToVM::Data::Universe_base_vtable_size\", Integer.class, \"int\");\n+\n+    final int baseVtableLength() {\n+        return universeBaseVtableSize \/ (vtableEntrySize \/ heapWordSize);\n+    }\n+\n+    final int klassOffset = getFieldValue(\"java_lang_Class::_klass_offset\", Integer.class, \"int\");\n+\n+    \/**\n+     * The DataLayout header size is the same as the cell size.\n+     *\/\n+    final int dataLayoutHeaderSize = getConstant(\"DataLayout::cell_size\", Integer.class);\n+    final int dataLayoutTagOffset = getFieldOffset(\"DataLayout::_header._struct._tag\", Integer.class, \"u1\");\n+    final int dataLayoutFlagsOffset = getFieldOffset(\"DataLayout::_header._struct._flags\", Integer.class, \"u1\");\n+    final int dataLayoutBCIOffset = getFieldOffset(\"DataLayout::_header._struct._bci\", Integer.class, \"u2\");\n+    final int dataLayoutCellSize = getConstant(\"DataLayout::cell_size\", Integer.class);\n+\n+    final int dataLayoutNoTag = getConstant(\"DataLayout::no_tag\", Integer.class);\n+    final int dataLayoutBitDataTag = getConstant(\"DataLayout::bit_data_tag\", Integer.class);\n+    final int dataLayoutCounterDataTag = getConstant(\"DataLayout::counter_data_tag\", Integer.class);\n+    final int dataLayoutJumpDataTag = getConstant(\"DataLayout::jump_data_tag\", Integer.class);\n+    final int dataLayoutReceiverTypeDataTag = getConstant(\"DataLayout::receiver_type_data_tag\", Integer.class);\n+    final int dataLayoutVirtualCallDataTag = getConstant(\"DataLayout::virtual_call_data_tag\", Integer.class);\n+    final int dataLayoutRetDataTag = getConstant(\"DataLayout::ret_data_tag\", Integer.class);\n+    final int dataLayoutBranchDataTag = getConstant(\"DataLayout::branch_data_tag\", Integer.class);\n+    final int dataLayoutMultiBranchDataTag = getConstant(\"DataLayout::multi_branch_data_tag\", Integer.class);\n+    final int dataLayoutArgInfoDataTag = getConstant(\"DataLayout::arg_info_data_tag\", Integer.class);\n+    final int dataLayoutCallTypeDataTag = getConstant(\"DataLayout::call_type_data_tag\", Integer.class);\n+    final int dataLayoutVirtualCallTypeDataTag = getConstant(\"DataLayout::virtual_call_type_data_tag\", Integer.class);\n+    final int dataLayoutParametersTypeDataTag = getConstant(\"DataLayout::parameters_type_data_tag\", Integer.class);\n+    final int dataLayoutSpeculativeTrapDataTag = getConstant(\"DataLayout::speculative_trap_data_tag\", Integer.class);\n+\n+    final int bciProfileWidth = getFlag(\"BciProfileWidth\", Integer.class);\n+    final int typeProfileWidth = getFlag(\"TypeProfileWidth\", Integer.class);\n+    final int methodProfileWidth = getFlag(\"MethodProfileWidth\", Integer.class, 0);\n+\n+    final int deoptReasonNone = getConstant(\"Deoptimization::Reason_none\", Integer.class);\n+    final int deoptReasonNullCheck = getConstant(\"Deoptimization::Reason_null_check\", Integer.class);\n+    final int deoptReasonRangeCheck = getConstant(\"Deoptimization::Reason_range_check\", Integer.class);\n+    final int deoptReasonClassCheck = getConstant(\"Deoptimization::Reason_class_check\", Integer.class);\n+    final int deoptReasonArrayCheck = getConstant(\"Deoptimization::Reason_array_check\", Integer.class);\n+    final int deoptReasonUnreached0 = getConstant(\"Deoptimization::Reason_unreached0\", Integer.class);\n+    final int deoptReasonTypeCheckInlining = getConstant(\"Deoptimization::Reason_type_checked_inlining\", Integer.class);\n+    final int deoptReasonOptimizedTypeCheck = getConstant(\"Deoptimization::Reason_optimized_type_check\", Integer.class);\n+    final int deoptReasonNotCompiledExceptionHandler = getConstant(\"Deoptimization::Reason_not_compiled_exception_handler\", Integer.class);\n+    final int deoptReasonUnresolved = getConstant(\"Deoptimization::Reason_unresolved\", Integer.class);\n+    final int deoptReasonJsrMismatch = getConstant(\"Deoptimization::Reason_jsr_mismatch\", Integer.class);\n+    final int deoptReasonDiv0Check = getConstant(\"Deoptimization::Reason_div0_check\", Integer.class);\n+    final int deoptReasonConstraint = getConstant(\"Deoptimization::Reason_constraint\", Integer.class);\n+    final int deoptReasonLoopLimitCheck = getConstant(\"Deoptimization::Reason_loop_limit_check\", Integer.class);\n+    final int deoptReasonAliasing = getConstant(\"Deoptimization::Reason_aliasing\", Integer.class);\n+    final int deoptReasonTransferToInterpreter = getConstant(\"Deoptimization::Reason_transfer_to_interpreter\", Integer.class);\n+    final int deoptReasonOSROffset = getConstant(\"Deoptimization::Reason_TRAP_HISTORY_LENGTH\", Integer.class);\n+\n+    final int deoptActionNone = getConstant(\"Deoptimization::Action_none\", Integer.class);\n+    final int deoptActionMaybeRecompile = getConstant(\"Deoptimization::Action_maybe_recompile\", Integer.class);\n+    final int deoptActionReinterpret = getConstant(\"Deoptimization::Action_reinterpret\", Integer.class);\n+    final int deoptActionMakeNotEntrant = getConstant(\"Deoptimization::Action_make_not_entrant\", Integer.class);\n+    final int deoptActionMakeNotCompilable = getConstant(\"Deoptimization::Action_make_not_compilable\", Integer.class);\n+\n+    final int deoptimizationActionBits = getConstant(\"Deoptimization::_action_bits\", Integer.class);\n+    final int deoptimizationReasonBits = getConstant(\"Deoptimization::_reason_bits\", Integer.class);\n+    final int deoptimizationDebugIdBits = getConstant(\"Deoptimization::_debug_id_bits\", Integer.class);\n+    final int deoptimizationActionShift = getConstant(\"Deoptimization::_action_shift\", Integer.class);\n+    final int deoptimizationReasonShift = getConstant(\"Deoptimization::_reason_shift\", Integer.class);\n+    final int deoptimizationDebugIdShift = getConstant(\"Deoptimization::_debug_id_shift\", Integer.class);\n+\n+    final int vmIntrinsicInvokeBasic = getConstant(\"vmIntrinsics::_invokeBasic\", Integer.class);\n+    final int vmIntrinsicLinkToVirtual = getConstant(\"vmIntrinsics::_linkToVirtual\", Integer.class);\n+    final int vmIntrinsicLinkToStatic = getConstant(\"vmIntrinsics::_linkToStatic\", Integer.class);\n+    final int vmIntrinsicLinkToSpecial = getConstant(\"vmIntrinsics::_linkToSpecial\", Integer.class);\n+    final int vmIntrinsicLinkToInterface = getConstant(\"vmIntrinsics::_linkToInterface\", Integer.class);\n+\n+    final int codeInstallResultOk = getConstant(\"JVMCI::ok\", Integer.class);\n+    final int codeInstallResultDependenciesFailed = getConstant(\"JVMCI::dependencies_failed\", Integer.class);\n+    final int codeInstallResultCacheFull = getConstant(\"JVMCI::cache_full\", Integer.class);\n+    final int codeInstallResultCodeTooLarge = getConstant(\"JVMCI::code_too_large\", Integer.class);\n+    final int codeInstallResultNMethodReclaimed = getConstant(\"JVMCI::nmethod_reclaimed\", Integer.class);\n+    final int codeInstallResultFirstPermanentBailout = getConstant(\"JVMCI::first_permanent_bailout\", Integer.class);\n+\n+    String getCodeInstallResultDescription(int codeInstallResult) {\n+        if (codeInstallResult == codeInstallResultOk) {\n+            return \"ok\";\n+        }\n+        if (codeInstallResult == codeInstallResultDependenciesFailed) {\n+            return \"dependencies failed\";\n+        }\n+        if (codeInstallResult == codeInstallResultCacheFull) {\n+            return \"code cache is full\";\n+        }\n+        if (codeInstallResult == codeInstallResultCodeTooLarge) {\n+            return \"code is too large\";\n+        }\n+        if (codeInstallResult == codeInstallResultNMethodReclaimed) {\n+            return \"nmethod reclaimed\";\n+        }\n+        assert false : codeInstallResult;\n+        return \"unknown\";\n+    }\n+\n+    final int bitDataExceptionSeenFlag = getConstant(\"BitData::exception_seen_flag\", Integer.class);\n+    final int bitDataNullSeenFlag = getConstant(\"BitData::null_seen_flag\", Integer.class);\n+    final int methodDataCountOffset = getConstant(\"CounterData::count_off\", Integer.class);\n+    final int jumpDataTakenOffset = getConstant(\"JumpData::taken_off_set\", Integer.class);\n+    final int jumpDataDisplacementOffset = getConstant(\"JumpData::displacement_off_set\", Integer.class);\n+    final int receiverTypeDataNonprofiledCountOffset = getConstant(\"ReceiverTypeData::nonprofiled_count_off_set\", Integer.class);\n+    final int receiverTypeDataReceiverTypeRowCellCount = getConstant(\"ReceiverTypeData::receiver_type_row_cell_count\", Integer.class);\n+    final int receiverTypeDataReceiver0Offset = getConstant(\"ReceiverTypeData::receiver0_offset\", Integer.class);\n+    final int receiverTypeDataCount0Offset = getConstant(\"ReceiverTypeData::count0_offset\", Integer.class);\n+    final int branchDataNotTakenOffset = getConstant(\"BranchData::not_taken_off_set\", Integer.class);\n+    final int arrayDataArrayLenOffset = getConstant(\"ArrayData::array_len_off_set\", Integer.class);\n+    final int arrayDataArrayStartOffset = getConstant(\"ArrayData::array_start_off_set\", Integer.class);\n+    final int multiBranchDataPerCaseCellCount = getConstant(\"MultiBranchData::per_case_cell_count\", Integer.class);\n+}\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":392,"deletions":0,"binary":false,"changes":392,"status":"added"},{"patch":"@@ -52,0 +52,1 @@\n+compiler\/loopopts\/TestUnreachableInnerLoop.java 8288981 linux-s390x\n@@ -69,0 +70,2 @@\n+compiler\/vectorapi\/VectorLogicalOpIdentityTest.java 8302459 linux-x64,windows-x64\n+\n@@ -97,2 +100,4 @@\n-runtime\/vthread\/RedefineClass.java 8297286 generic-all\n-runtime\/vthread\/TestObjectAllocationSampleEvent.java 8297286 generic-all\n+runtime\/CompressedOops\/CompressedClassPointers.java 8305765 generic-all\n+runtime\/StackGuardPages\/TestStackGuardPagesNative.java 8303612 linux-all\n+runtime\/Thread\/TestAlwaysPreTouchStacks.java 8305416 generic-all\n+runtime\/ErrorHandling\/TestDwarf.java 8305489 linux-all\n@@ -149,1 +154,0 @@\n-vmTestbase\/nsk\/jvmti\/AttachOnDemand\/attach002a\/TestDescription.java 8277812 generic-all\n@@ -166,1 +170,0 @@\n-vmTestbase\/nsk\/stress\/except\/except012.java 8297977 generic-all\n@@ -168,2 +171,0 @@\n-vmTestbase\/nsk\/jdi\/VMOutOfMemoryException\/VMOutOfMemoryException001\/VMOutOfMemoryException001.java 8303057 generic-all\n-\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":7,"deletions":6,"binary":false,"changes":13,"status":"modified"},{"patch":"@@ -281,1 +281,1 @@\n- * @run main\/othervm\/timeout=240 -Xmx8g\n+ * @run main\/othervm\/timeout=300 -Xmx8g\n","filename":"test\/jdk\/java\/lang\/instrument\/GetObjectSizeIntrinsicsTest.java","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"}]}