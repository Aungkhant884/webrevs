{"files":[{"patch":"@@ -4,1 +4,1 @@\n-version=21\n+version=22\n","filename":".jcheck\/conf","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -667,5 +667,0 @@\n-\/\/ Class for heapbase register\n-reg_class heapbase_reg(\n-    R27, R27_H\n-);\n-\n@@ -1247,2 +1242,2 @@\n-      _NO_SPECIAL_REG_mask.SUBTRACT(_HEAPBASE_REG_mask);\n-      _NO_SPECIAL_PTR_REG_mask.SUBTRACT(_HEAPBASE_REG_mask);\n+      _NO_SPECIAL_REG_mask.Remove(OptoReg::as_OptoReg(r27->as_VMReg()));\n+      _NO_SPECIAL_PTR_REG_mask.Remove(OptoReg::as_OptoReg(r27->as_VMReg()));\n@@ -1254,2 +1249,2 @@\n-      _NO_SPECIAL_REG_mask.SUBTRACT(_FP_REG_mask);\n-      _NO_SPECIAL_PTR_REG_mask.SUBTRACT(_FP_REG_mask);\n+      _NO_SPECIAL_REG_mask.Remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n+      _NO_SPECIAL_PTR_REG_mask.Remove(OptoReg::as_OptoReg(r29->as_VMReg()));\n@@ -3887,1 +3882,1 @@\n-    __ add(tmp, disp_hdr, (ObjectMonitor::owner_offset_in_bytes()-markWord::monitor_value));\n+    __ add(tmp, disp_hdr, (in_bytes(ObjectMonitor::owner_offset())-markWord::monitor_value));\n@@ -3905,1 +3900,1 @@\n-    __ increment(Address(disp_hdr, ObjectMonitor::recursions_offset_in_bytes() - markWord::monitor_value), 1);\n+    __ increment(Address(disp_hdr, in_bytes(ObjectMonitor::recursions_offset()) - markWord::monitor_value), 1);\n@@ -3971,1 +3966,1 @@\n-      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+      __ ldr(tmp2, Address(tmp, ObjectMonitor::owner_offset()));\n@@ -3981,1 +3976,1 @@\n-    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset_in_bytes()));\n+    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n@@ -3988,1 +3983,1 @@\n-    __ str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset_in_bytes()));\n+    __ str(disp_hdr, Address(tmp, ObjectMonitor::recursions_offset()));\n@@ -3993,2 +3988,2 @@\n-    __ ldr(rscratch1, Address(tmp, ObjectMonitor::EntryList_offset_in_bytes()));\n-    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::cxq_offset_in_bytes()));\n+    __ ldr(rscratch1, Address(tmp, ObjectMonitor::EntryList_offset()));\n+    __ ldr(disp_hdr, Address(tmp, ObjectMonitor::cxq_offset()));\n@@ -3999,1 +3994,1 @@\n-    __ lea(tmp, Address(tmp, ObjectMonitor::owner_offset_in_bytes()));\n+    __ lea(tmp, Address(tmp, ObjectMonitor::owner_offset()));\n@@ -5323,11 +5318,0 @@\n-\/\/ heap base register -- used for encoding immN0\n-\n-operand iRegIHeapbase()\n-%{\n-  constraint(ALLOC_IN_RC(heapbase_reg));\n-  match(RegI);\n-  op_cost(0);\n-  format %{ %}\n-  interface(REG_INTER);\n-%}\n-\n@@ -15063,36 +15047,0 @@\n-instruct convI2B(iRegINoSp dst, iRegIorL2I src, rFlagsReg cr)\n-%{\n-  match(Set dst (Conv2B src));\n-  effect(KILL cr);\n-\n-  format %{\n-    \"cmpw $src, zr\\n\\t\"\n-    \"cset $dst, ne\"\n-  %}\n-\n-  ins_encode %{\n-    __ cmpw(as_Register($src$$reg), zr);\n-    __ cset(as_Register($dst$$reg), Assembler::NE);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n-instruct convP2B(iRegINoSp dst, iRegP src, rFlagsReg cr)\n-%{\n-  match(Set dst (Conv2B src));\n-  effect(KILL cr);\n-\n-  format %{\n-    \"cmp  $src, zr\\n\\t\"\n-    \"cset $dst, ne\"\n-  %}\n-\n-  ins_encode %{\n-    __ cmp(as_Register($src$$reg), zr);\n-    __ cset(as_Register($dst$$reg), Assembler::NE);\n-  %}\n-\n-  ins_pipe(ialu_reg);\n-%}\n-\n@@ -17134,2 +17082,3 @@\n-       iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n-       iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+                          iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n+                          iRegINoSp tmp3, iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6,\n+                          vRegD_V0 vtmp0, vRegD_V1 vtmp1, rFlagsReg cr)\n@@ -17140,2 +17089,4 @@\n-         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UU)\" %}\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6,\n+         TEMP vtmp0, TEMP vtmp1, KILL cr);\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UU) \"\n+            \"# KILL $str1 $cnt1 $str2 $cnt2 $tmp1 $tmp2 $tmp3 $tmp4 $tmp5 $tmp6 V0-V1 cr\" %}\n@@ -17155,2 +17106,3 @@\n-       iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n-       iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+                          iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n+                          iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6,\n+                          vRegD_V0 vtmp0, vRegD_V1 vtmp1, rFlagsReg cr)\n@@ -17161,2 +17113,4 @@\n-         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (LL)\" %}\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6,\n+         TEMP vtmp0, TEMP vtmp1, KILL cr);\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (LL) \"\n+            \"# KILL $str1 $cnt1 $str2 $cnt2 $tmp1 $tmp2 $tmp3 $tmp4 $tmp5 $tmp6 V0-V1 cr\" %}\n@@ -17176,2 +17130,3 @@\n-       iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2, iRegINoSp tmp3,\n-       iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6, rFlagsReg cr)\n+                          iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,iRegINoSp tmp3,\n+                          iRegINoSp tmp4, iRegINoSp tmp5, iRegINoSp tmp6,\n+                          vRegD_V0 vtmp0, vRegD_V1 vtmp1, rFlagsReg cr)\n@@ -17182,2 +17137,4 @@\n-         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5, TEMP tmp6, KILL cr);\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UL)\" %}\n+         TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, TEMP tmp5,\n+         TEMP tmp6, TEMP vtmp0, TEMP vtmp1, KILL cr);\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$cnt2 -> $result (UL) \"\n+            \"# KILL $str1 cnt1 $str2 $cnt2 $tmp1 $tmp2 $tmp3 $tmp4 $tmp5 $tmp6 V0-V1 cr\" %}\n@@ -17197,2 +17154,2 @@\n-                 immI_le_4 int_cnt2, iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n-                 iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+                              immI_le_4 int_cnt2, iRegI_R0 result, iRegINoSp tmp1,\n+                              iRegINoSp tmp2, iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n@@ -17204,1 +17161,2 @@\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UU)\" %}\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UU) \"\n+            \"# KILL $str1 $cnt1 $str2 $tmp1 $tmp2 $tmp3 $tmp4 cr\" %}\n@@ -17218,2 +17176,2 @@\n-                 immI_le_4 int_cnt2, iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n-                 iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+                              immI_le_4 int_cnt2, iRegI_R0 result, iRegINoSp tmp1,\n+                              iRegINoSp tmp2, iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n@@ -17225,1 +17183,2 @@\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (LL)\" %}\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (LL) \"\n+            \"# KILL $str1 $cnt1 $str2 $tmp1 $tmp2 $tmp3 $tmp4 cr\" %}\n@@ -17239,2 +17198,2 @@\n-                 immI_1 int_cnt2, iRegI_R0 result, iRegINoSp tmp1, iRegINoSp tmp2,\n-                 iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n+                              immI_1 int_cnt2, iRegI_R0 result, iRegINoSp tmp1,\n+                              iRegINoSp tmp2, iRegINoSp tmp3, iRegINoSp tmp4, rFlagsReg cr)\n@@ -17246,1 +17205,2 @@\n-  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UL)\" %}\n+  format %{ \"String IndexOf $str1,$cnt1,$str2,$int_cnt2 -> $result (UL) \"\n+            \"# KILL $str1 $cnt1 $str2 $tmp1 $tmp2 $tmp3 $tmp4 cr\" %}\n@@ -17363,0 +17323,2 @@\n+                       vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                       vRegD_V4 vtmp4, vRegD_V5 vtmp5, vRegD_V6 vtmp6, vRegD_V7 vtmp7,\n@@ -17367,1 +17329,3 @@\n-  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3,\n+         TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP vtmp5,\n+         TEMP vtmp6, TEMP vtmp7, KILL cr);\n@@ -17369,1 +17333,1 @@\n-  format %{ \"Array Equals $ary1,ary2 -> $result    \/\/ KILL $tmp\" %}\n+  format %{ \"Array Equals $ary1,ary2 -> $result # KILL $ary1 $ary2 $tmp $tmp1 $tmp2 $tmp3 V0-V7 cr\" %}\n@@ -17384,0 +17348,2 @@\n+                       vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                       vRegD_V4 vtmp4, vRegD_V5 vtmp5, vRegD_V6 vtmp6, vRegD_V7 vtmp7,\n@@ -17388,1 +17354,3 @@\n-  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3, KILL cr);\n+  effect(KILL tmp, USE_KILL ary1, USE_KILL ary2, TEMP tmp1, TEMP tmp2, TEMP tmp3,\n+         TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP vtmp5,\n+         TEMP vtmp6, TEMP vtmp7, KILL cr);\n@@ -17390,1 +17358,1 @@\n-  format %{ \"Array Equals $ary1,ary2 -> $result    \/\/ KILL $tmp\" %}\n+  format %{ \"Array Equals $ary1,ary2 -> $result # KILL $ary1 $ary2 $tmp $tmp1 $tmp2 $tmp3 V0-V7 cr\" %}\n@@ -17420,2 +17388,2 @@\n-                         vRegD_V0 tmp1, vRegD_V1 tmp2,\n-                         vRegD_V2 tmp3, vRegD_V3 tmp4,\n+                         vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2,\n+                         vRegD_V3 vtmp3, vRegD_V4 vtmp4, vRegD_V5 vtmp5,\n@@ -17425,1 +17393,1 @@\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4,\n+  effect(TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3, TEMP vtmp4, TEMP vtmp5,\n@@ -17428,1 +17396,1 @@\n-  format %{ \"String Compress $src,$dst,$len -> $result  \/\/ KILL $src,$dst\" %}\n+  format %{ \"String Compress $src,$dst,$len -> $result # KILL $src $dst V0-V5 cr\" %}\n@@ -17431,3 +17399,3 @@\n-                           $result$$Register,\n-                           $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n-                           $tmp3$$FloatRegister, $tmp4$$FloatRegister);\n+                           $result$$Register, $vtmp0$$FloatRegister, $vtmp1$$FloatRegister,\n+                           $vtmp2$$FloatRegister, $vtmp3$$FloatRegister,\n+                           $vtmp4$$FloatRegister, $vtmp5$$FloatRegister);\n@@ -17439,2 +17407,3 @@\n-instruct string_inflate(Universe dummy, iRegP_R0 src, iRegP_R1 dst, iRegI_R2 len,\n-                        vRegD_V0 tmp1, vRegD_V1 tmp2, vRegD_V2 tmp3, iRegP_R3 tmp4, rFlagsReg cr)\n+instruct string_inflate(Universe dummy, iRegP_R0 src, iRegP_R1 dst, iRegI_R2 len, iRegP_R3 tmp,\n+                        vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                        vRegD_V4 vtmp4, vRegD_V5 vtmp5, vRegD_V6 vtmp6, rFlagsReg cr)\n@@ -17443,1 +17412,3 @@\n-  effect(TEMP tmp1, TEMP tmp2, TEMP tmp3, TEMP tmp4, USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n+  effect(TEMP vtmp0, TEMP vtmp1, TEMP vtmp2, TEMP vtmp3,\n+         TEMP vtmp4, TEMP vtmp5, TEMP vtmp6, TEMP tmp,\n+         USE_KILL src, USE_KILL dst, USE_KILL len, KILL cr);\n@@ -17445,1 +17416,1 @@\n-  format %{ \"String Inflate $src,$dst    \/\/ KILL $tmp1, $tmp2\" %}\n+  format %{ \"String Inflate $src,$dst # KILL $tmp $src $dst $len V0-V6 cr\" %}\n@@ -17448,2 +17419,2 @@\n-                                        $tmp1$$FloatRegister, $tmp2$$FloatRegister,\n-                                        $tmp3$$FloatRegister, $tmp4$$Register);\n+                                        $vtmp0$$FloatRegister, $vtmp1$$FloatRegister,\n+                                        $vtmp2$$FloatRegister, $tmp$$Register);\n@@ -17460,2 +17431,2 @@\n-                          vRegD_V0 vtmp0, vRegD_V1 vtmp1,\n-                          vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                          vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2,\n+                          vRegD_V3 vtmp3, vRegD_V4 vtmp4, vRegD_V5 vtmp5,\n@@ -17466,2 +17437,2 @@\n-  effect(USE_KILL src, USE_KILL dst, USE len,\n-         KILL vtmp0, KILL vtmp1, KILL vtmp2, KILL vtmp3, KILL cr);\n+  effect(USE_KILL src, USE_KILL dst, USE len, KILL vtmp0, KILL vtmp1,\n+         KILL vtmp2, KILL vtmp3, KILL vtmp4, KILL vtmp5, KILL cr);\n@@ -17469,1 +17440,1 @@\n-  format %{ \"Encode ISO array $src,$dst,$len -> $result\" %}\n+  format %{ \"Encode ISO array $src,$dst,$len -> $result # KILL $src $dst V0-V5 cr\" %}\n@@ -17474,1 +17445,2 @@\n-                        $vtmp2$$FloatRegister, $vtmp3$$FloatRegister);\n+                        $vtmp2$$FloatRegister, $vtmp3$$FloatRegister,\n+                        $vtmp4$$FloatRegister, $vtmp5$$FloatRegister);\n@@ -17480,2 +17452,2 @@\n-                            vRegD_V0 vtmp0, vRegD_V1 vtmp1,\n-                            vRegD_V2 vtmp2, vRegD_V3 vtmp3,\n+                            vRegD_V0 vtmp0, vRegD_V1 vtmp1, vRegD_V2 vtmp2,\n+                            vRegD_V3 vtmp3, vRegD_V4 vtmp4, vRegD_V5 vtmp5,\n@@ -17486,2 +17458,2 @@\n-  effect(USE_KILL src, USE_KILL dst, USE len,\n-         KILL vtmp0, KILL vtmp1, KILL vtmp2, KILL vtmp3, KILL cr);\n+  effect(USE_KILL src, USE_KILL dst, USE len, KILL vtmp0, KILL vtmp1,\n+         KILL vtmp2, KILL vtmp3, KILL vtmp4, KILL vtmp5, KILL cr);\n@@ -17489,1 +17461,1 @@\n-  format %{ \"Encode ASCII array $src,$dst,$len -> $result\" %}\n+  format %{ \"Encode ASCII array $src,$dst,$len -> $result # KILL $src $dst V0-V5 cr\" %}\n@@ -17494,1 +17466,2 @@\n-                        $vtmp2$$FloatRegister, $vtmp3$$FloatRegister);\n+                        $vtmp2$$FloatRegister, $vtmp3$$FloatRegister,\n+                        $vtmp4$$FloatRegister, $vtmp5$$FloatRegister);\n","filename":"src\/hotspot\/cpu\/aarch64\/aarch64.ad","additions":87,"deletions":114,"binary":false,"changes":201,"status":"modified"},{"patch":"@@ -1014,1 +1014,1 @@\n-    if (!UseZGC) {\n+    if (!(UseZGC && !ZGenerational)) {\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_LIRAssembler_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -72,1 +72,1 @@\n-  str(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+  str(obj, Address(disp_hdr, BasicObjectLock::obj_offset()));\n@@ -143,1 +143,1 @@\n-  ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+  ldr(obj, Address(disp_hdr, BasicObjectLock::obj_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/c1_MacroAssembler_aarch64.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -81,1 +81,1 @@\n-  __ str(rthread, Address(mon, ObjectMonitor::owner_offset_in_bytes()));\n+  __ str(rthread, Address(mon, ObjectMonitor::owner_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/c2_CodeStubs_aarch64.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -1147,1 +1147,1 @@\n-  int itentry_off = itableMethodEntry::method_offset_in_bytes();\n+  int itentry_off = in_bytes(itableMethodEntry::method_offset());\n@@ -1175,1 +1175,1 @@\n-  ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));\n+  ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset()));\n@@ -1183,1 +1183,1 @@\n-  if (itableOffsetEntry::interface_offset_in_bytes() != 0) {\n+  if (itableOffsetEntry::interface_offset() != 0) {\n@@ -1185,1 +1185,1 @@\n-    ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));\n+    ldr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset()));\n@@ -1196,1 +1196,1 @@\n-    ldrw(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));\n+    ldrw(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset()));\n@@ -1205,1 +1205,0 @@\n-  const int base = in_bytes(Klass::vtable_start_offset());\n@@ -1208,1 +1207,1 @@\n-  int vtable_offset_in_bytes = base + vtableEntry::method_offset_in_bytes();\n+  int64_t vtable_offset_in_bytes = in_bytes(Klass::vtable_start_offset() + vtableEntry::method_offset());\n@@ -4315,1 +4314,1 @@\n-  ldr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); \/\/ InstanceKlass*\n+  ldr(holder, Address(holder, ConstantPool::pool_holder_offset()));          \/\/ InstanceKlass*\n@@ -4380,1 +4379,1 @@\n-  ldr(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));\n+  ldr(dst, Address(dst, ConstantPool::pool_holder_offset()));\n@@ -5048,0 +5047,2 @@\n+\/\/ Clobbers: rscratch1, rscratch2, rflags\n+\/\/ May also clobber v0-v7 when (!UseSimpleArrayEquals && UseSIMDForArrayEquals)\n@@ -5597,0 +5598,1 @@\n+\/\/ Clobbers: src, dst, res, rscratch1, rscratch2, rflags\n@@ -5600,1 +5602,2 @@\n-                                      FloatRegister vtmp2, FloatRegister vtmp3)\n+                                      FloatRegister vtmp2, FloatRegister vtmp3,\n+                                      FloatRegister vtmp4, FloatRegister vtmp5)\n@@ -5619,2 +5622,2 @@\n-    FloatRegister vlo0 = v4;\n-    FloatRegister vlo1 = v5;\n+    FloatRegister vlo0 = vtmp4;\n+    FloatRegister vlo1 = vtmp5;\n@@ -5693,0 +5696,1 @@\n+\/\/ Clobbers: src, dst, len, rflags, rscratch1, v0-v6\n@@ -5801,2 +5805,3 @@\n-                                         FloatRegister tmp2, FloatRegister tmp3) {\n-  encode_iso_array(src, dst, len, res, false, tmp0, tmp1, tmp2, tmp3);\n+                                         FloatRegister tmp2, FloatRegister tmp3,\n+                                         FloatRegister tmp4, FloatRegister tmp5) {\n+  encode_iso_array(src, dst, len, res, false, tmp0, tmp1, tmp2, tmp3, tmp4, tmp5);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.cpp","additions":19,"deletions":14,"binary":false,"changes":33,"status":"modified"},{"patch":"@@ -1407,1 +1407,2 @@\n-                           FloatRegister vtmp2, FloatRegister vtmp3);\n+                           FloatRegister vtmp2, FloatRegister vtmp3,\n+                           FloatRegister vtmp4, FloatRegister vtmp5);\n@@ -1412,1 +1413,2 @@\n-                        FloatRegister vtmp2, FloatRegister vtmp3);\n+                        FloatRegister vtmp2, FloatRegister vtmp3,\n+                        FloatRegister vtmp4, FloatRegister vtmp5);\n","filename":"src\/hotspot\/cpu\/aarch64\/macroAssembler_aarch64.hpp","additions":4,"deletions":2,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -41,0 +41,1 @@\n+#include \"oops\/resolvedIndyEntry.hpp\"\n@@ -3843,1 +3844,1 @@\n-    __ ldr(rscratch1, Address(c_rarg3, BasicObjectLock::obj_offset_in_bytes()));\n+    __ ldr(rscratch1, Address(c_rarg3, BasicObjectLock::obj_offset()));\n@@ -3903,1 +3904,1 @@\n-  __ str(r0, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+  __ str(r0, Address(c_rarg1, BasicObjectLock::obj_offset()));\n@@ -3942,1 +3943,1 @@\n-    __ ldr(rscratch1, Address(c_rarg1, BasicObjectLock::obj_offset_in_bytes()));\n+    __ ldr(rscratch1, Address(c_rarg1, BasicObjectLock::obj_offset()));\n","filename":"src\/hotspot\/cpu\/aarch64\/templateTable_aarch64.cpp","additions":4,"deletions":3,"binary":false,"changes":7,"status":"modified"},{"patch":"@@ -33,1 +33,0 @@\n-#include \"logging\/log.hpp\"\n@@ -198,2 +197,2 @@\n-  assert(BasicObjectLock::lock_offset_in_bytes() == 0, \"adjust this code\");\n-  const int obj_offset = BasicObjectLock::obj_offset_in_bytes();\n+  assert(BasicObjectLock::lock_offset() == 0, \"adjust this code\");\n+  const ByteSize obj_offset = BasicObjectLock::obj_offset();\n@@ -217,1 +216,0 @@\n-    log_trace(fastlock)(\"C1_MacroAssembler::lock fast\");\n@@ -270,2 +268,2 @@\n-  assert(BasicObjectLock::lock_offset_in_bytes() == 0, \"adjust this code\");\n-  const int obj_offset = BasicObjectLock::obj_offset_in_bytes();\n+  assert(BasicObjectLock::lock_offset() == 0, \"adjust this code\");\n+  const ByteSize obj_offset = BasicObjectLock::obj_offset();\n@@ -279,1 +277,0 @@\n-    log_trace(fastlock)(\"C1_MacroAssembler::unlock fast\");\n","filename":"src\/hotspot\/cpu\/arm\/c1_MacroAssembler_arm.cpp","additions":4,"deletions":7,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -1793,1 +1793,1 @@\n-  int itentry_off = itableMethodEntry::method_offset_in_bytes();\n+  int itentry_off = in_bytes(itableMethodEntry::method_offset());\n@@ -1830,1 +1830,1 @@\n-    ld(temp2, itableOffsetEntry::interface_offset_in_bytes(), scan_temp);\n+    ld(temp2, in_bytes(itableOffsetEntry::interface_offset()), scan_temp);\n@@ -1857,1 +1857,1 @@\n-    int ito_offset = itableOffsetEntry::offset_offset_in_bytes();\n+    int ito_offset = in_bytes(itableOffsetEntry::offset_offset());\n@@ -1870,1 +1870,1 @@\n-  const int base = in_bytes(Klass::vtable_start_offset());\n+  const ByteSize base = Klass::vtable_start_offset();\n@@ -1879,1 +1879,1 @@\n-  ld(R19_method, base + vtableEntry::method_offset_in_bytes(), recv_klass);\n+  ld(R19_method, in_bytes(base + vtableEntry::method_offset()), recv_klass);\n@@ -2357,1 +2357,1 @@\n-    load_const(R0, (address)method_data + MethodData::rtm_state_offset_in_bytes(), tmpReg);\n+    load_const(R0, (address)method_data + in_bytes(MethodData::rtm_state_offset()), tmpReg);\n@@ -2377,1 +2377,1 @@\n-    load_const(R0, (address)method_data + MethodData::rtm_state_offset_in_bytes(), tmpReg);\n+    load_const(R0, (address)method_data + in_bytes(MethodData::rtm_state_offset()), tmpReg);\n@@ -2550,1 +2550,1 @@\n-  int owner_offset = ObjectMonitor::owner_offset_in_bytes() - markWord::monitor_value;\n+  int owner_offset = in_bytes(ObjectMonitor::owner_offset()) - markWord::monitor_value;\n@@ -2633,2 +2633,1 @@\n-  assert(flag != CCR0, \"bad condition register\");\n-  Label cont;\n+  assert(LockingMode != LM_LIGHTWEIGHT || flag == CCR0, \"bad condition register\");\n@@ -2653,1 +2652,1 @@\n-                      cont, object_has_monitor);\n+                      success, object_has_monitor);\n@@ -2662,1 +2661,5 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n+    \/\/ Set NE to indicate 'failure' -> take slow-path.\n+    crandc(flag, Assembler::equal, flag, Assembler::equal);\n+    b(failure);\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -2687,7 +2690,2 @@\n-  } else {\n-    \/\/ Set NE to indicate 'failure' -> take slow-path.\n-    crandc(flag, Assembler::equal, flag, Assembler::equal);\n-    b(failure);\n-  }\n-  bind(cas_failed);\n-  \/\/ We did not see an unlocked object so try the fast recursive case.\n+    bind(cas_failed);\n+    \/\/ We did not see an unlocked object so try the fast recursive case.\n@@ -2696,4 +2694,4 @@\n-  \/\/ Check if the owner is self by comparing the value in the markWord of object\n-  \/\/ (current_header) with the stack pointer.\n-  sub(current_header, current_header, R1_SP);\n-  load_const_optimized(temp, ~(os::vm_page_size()-1) | markWord::lock_mask_in_place);\n+    \/\/ Check if the owner is self by comparing the value in the markWord of object\n+    \/\/ (current_header) with the stack pointer.\n+    sub(current_header, current_header, R1_SP);\n+    load_const_optimized(temp, ~(os::vm_page_size()-1) | markWord::lock_mask_in_place);\n@@ -2701,5 +2699,4 @@\n-  and_(R0\/*==0?*\/, current_header, temp);\n-  \/\/ If condition is true we are cont and hence we can store 0 as the\n-  \/\/ displaced header in the box, which indicates that it is a recursive lock.\n-  mcrf(flag,CCR0);\n-  std(R0\/*==0, perhaps*\/, BasicLock::displaced_header_offset_in_bytes(), box);\n+    and_(R0\/*==0?*\/, current_header, temp);\n+    \/\/ If condition is true we are cont and hence we can store 0 as the\n+    \/\/ displaced header in the box, which indicates that it is a recursive lock.\n+    std(R0\/*==0, perhaps*\/, BasicLock::displaced_header_offset_in_bytes(), box);\n@@ -2707,1 +2704,10 @@\n-  b(cont);\n+    if (flag != CCR0) {\n+      mcrf(flag, CCR0);\n+    }\n+    beq(CCR0, success);\n+    b(failure);\n+  } else {\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+    fast_lock(oop, displaced_header, temp, failure);\n+    b(success);\n+  }\n@@ -2718,1 +2724,2 @@\n-                         rtm_counters, method_data, profile_rtm, cont);\n+                         rtm_counters, method_data, profile_rtm, success);\n+    bne(flag, failure);\n@@ -2723,1 +2730,1 @@\n-  addi(temp, displaced_header, ObjectMonitor::owner_offset_in_bytes()-markWord::monitor_value);\n+  addi(temp, displaced_header, in_bytes(ObjectMonitor::owner_offset()) - markWord::monitor_value);\n@@ -2732,2 +2739,4 @@\n-  \/\/ Store a non-null value into the box.\n-  std(box, BasicLock::displaced_header_offset_in_bytes(), box);\n+  if (LockingMode != LM_LIGHTWEIGHT) {\n+    \/\/ Store a non-null value into the box.\n+    std(box, BasicLock::displaced_header_offset_in_bytes(), box);\n+  }\n@@ -2742,1 +2751,1 @@\n-  ld(recursions, ObjectMonitor::recursions_offset_in_bytes()-ObjectMonitor::owner_offset_in_bytes(), temp);\n+  ld(recursions, in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), temp);\n@@ -2744,1 +2753,1 @@\n-  std(recursions, ObjectMonitor::recursions_offset_in_bytes()-ObjectMonitor::owner_offset_in_bytes(), temp);\n+  std(recursions, in_bytes(ObjectMonitor::recursions_offset() - ObjectMonitor::owner_offset()), temp);\n@@ -2750,1 +2759,0 @@\n-  bind(cont);\n@@ -2753,1 +2761,0 @@\n-  bne(flag, failure);\n@@ -2763,3 +2770,2 @@\n-  assert(flag != CCR0, \"bad condition register\");\n-  Label object_has_monitor, notRecursive;\n-  Label success, failure;\n+  assert(LockingMode != LM_LIGHTWEIGHT || flag == CCR0, \"bad condition register\");\n+  Label success, failure, object_has_monitor, notRecursive;\n@@ -2780,1 +2786,1 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode == LM_LEGACY) {\n@@ -2796,1 +2802,5 @@\n-  if (!UseHeavyMonitors) {\n+  if (LockingMode == LM_MONITOR) {\n+    \/\/ Set NE to indicate 'failure' -> take slow-path.\n+    crandc(flag, Assembler::equal, flag, Assembler::equal);\n+    b(failure);\n+  } else if (LockingMode == LM_LEGACY) {\n@@ -2812,3 +2822,3 @@\n-    \/\/ Set NE to indicate 'failure' -> take slow-path.\n-    crandc(flag, Assembler::equal, flag, Assembler::equal);\n-    b(failure);\n+    assert(LockingMode == LM_LIGHTWEIGHT, \"must be\");\n+    fast_unlock(oop, current_header, failure);\n+    b(success);\n@@ -2821,1 +2831,1 @@\n-  ld(temp,             ObjectMonitor::owner_offset_in_bytes(), current_header);\n+  ld(temp,             in_bytes(ObjectMonitor::owner_offset()), current_header);\n@@ -2823,1 +2833,1 @@\n-    \/\/ It's inflated.\n+  \/\/ It's inflated.\n@@ -2836,2 +2846,2 @@\n-  ld(displaced_header, ObjectMonitor::recursions_offset_in_bytes(), current_header);\n-\n+  \/\/ In case of LM_LIGHTWEIGHT, we may reach here with (temp & ObjectMonitor::ANONYMOUS_OWNER) != 0.\n+  \/\/ This is handled like owner thread mismatches: We take the slow path.\n@@ -2841,0 +2851,2 @@\n+  ld(displaced_header, in_bytes(ObjectMonitor::recursions_offset()), current_header);\n+\n@@ -2843,2 +2855,5 @@\n-  std(displaced_header, ObjectMonitor::recursions_offset_in_bytes(), current_header);\n-  b(success); \/\/ flag is already EQ here.\n+  std(displaced_header, in_bytes(ObjectMonitor::recursions_offset()), current_header);\n+  if (flag == CCR0) { \/\/ Otherwise, flag is already EQ, here.\n+    crorc(CCR0, Assembler::equal, CCR0, Assembler::equal); \/\/ Set CCR0 EQ\n+  }\n+  b(success);\n@@ -2847,2 +2862,2 @@\n-  ld(temp,             ObjectMonitor::EntryList_offset_in_bytes(), current_header);\n-  ld(displaced_header, ObjectMonitor::cxq_offset_in_bytes(), current_header);\n+  ld(temp,             in_bytes(ObjectMonitor::EntryList_offset()), current_header);\n+  ld(displaced_header, in_bytes(ObjectMonitor::cxq_offset()), current_header);\n@@ -2853,1 +2868,1 @@\n-  std(temp, ObjectMonitor::owner_offset_in_bytes(), current_header);\n+  std(temp, in_bytes(ObjectMonitor::owner_offset()), current_header);\n@@ -3043,1 +3058,1 @@\n-      CompressedKlassPointers::base() == 0 && src != dst) {  \/\/ Move required.\n+      (CompressedKlassPointers::base() == 0 && src != dst)) {  \/\/ Move required.\n@@ -3089,1 +3104,1 @@\n-  ld(holder, ConstantPool::pool_holder_offset_in_bytes(), holder);\n+  ld(holder, ConstantPool::pool_holder_offset(), holder);\n@@ -4414,0 +4429,1 @@\n+\/\/ Note: Must preserve CCR0 EQ (invariant).\n@@ -4422,0 +4438,1 @@\n+  crorc(CCR0, Assembler::equal, CCR0, Assembler::equal); \/\/ Restore CCR0 EQ\n@@ -4427,0 +4444,1 @@\n+\/\/ Note: Must preserve CCR0 EQ (invariant).\n@@ -4435,0 +4453,1 @@\n+  crorc(CCR0, Assembler::equal, CCR0, Assembler::equal); \/\/ Restore CCR0 EQ\n@@ -4439,0 +4458,128 @@\n+\n+\/\/ Function to flip between unlocked and locked state (fast locking).\n+\/\/ Branches to failed if the state is not as expected with CCR0 NE.\n+\/\/ Falls through upon success with CCR0 EQ.\n+\/\/ This requires fewer instructions and registers and is easier to use than the\n+\/\/ cmpxchg based implementation.\n+void MacroAssembler::atomically_flip_locked_state(bool is_unlock, Register obj, Register tmp, Label& failed, int semantics) {\n+  assert_different_registers(obj, tmp, R0);\n+  Label retry;\n+\n+  if (semantics & MemBarRel) {\n+    release();\n+  }\n+\n+  bind(retry);\n+  STATIC_ASSERT(markWord::locked_value == 0); \/\/ Or need to change this!\n+  if (!is_unlock) {\n+    ldarx(tmp, obj, MacroAssembler::cmpxchgx_hint_acquire_lock());\n+    xori(tmp, tmp, markWord::unlocked_value); \/\/ flip unlocked bit\n+    andi_(R0, tmp, markWord::lock_mask_in_place);\n+    bne(CCR0, failed); \/\/ failed if new header doesn't contain locked_value (which is 0)\n+  } else {\n+    ldarx(tmp, obj, MacroAssembler::cmpxchgx_hint_release_lock());\n+    andi_(R0, tmp, markWord::lock_mask_in_place);\n+    bne(CCR0, failed); \/\/ failed if old header doesn't contain locked_value (which is 0)\n+    ori(tmp, tmp, markWord::unlocked_value); \/\/ set unlocked bit\n+  }\n+  stdcx_(tmp, obj);\n+  bne(CCR0, retry);\n+\n+  if (semantics & MemBarFenceAfter) {\n+    fence();\n+  } else if (semantics & MemBarAcq) {\n+    isync();\n+  }\n+}\n+\n+\/\/ Implements fast-locking.\n+\/\/ Branches to slow upon failure to lock the object, with CCR0 NE.\n+\/\/ Falls through upon success with CCR0 EQ.\n+\/\/\n+\/\/  - obj: the object to be locked\n+\/\/  - hdr: the header, already loaded from obj, will be destroyed\n+\/\/  - t1: temporary register\n+void MacroAssembler::fast_lock(Register obj, Register hdr, Register t1, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr, t1);\n+\n+  \/\/ Check if we would have space on lock-stack for the object.\n+  lwz(t1, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+  cmplwi(CCR0, t1, LockStack::end_offset() - 1);\n+  bgt(CCR0, slow);\n+\n+  \/\/ Quick check: Do not reserve cache line for atomic update if not unlocked.\n+  \/\/ (Similar to contention_hint in cmpxchg solutions.)\n+  xori(R0, hdr, markWord::unlocked_value); \/\/ flip unlocked bit\n+  andi_(R0, R0, markWord::lock_mask_in_place);\n+  bne(CCR0, slow); \/\/ failed if new header doesn't contain locked_value (which is 0)\n+\n+  \/\/ Note: We're not publishing anything (like the displaced header in LM_LEGACY)\n+  \/\/ to other threads at this point. Hence, no release barrier, here.\n+  \/\/ (The obj has been written to the BasicObjectLock at obj_offset() within the own thread stack.)\n+  atomically_flip_locked_state(\/* is_unlock *\/ false, obj, hdr, slow, MacroAssembler::MemBarAcq);\n+\n+  \/\/ After successful lock, push object on lock-stack\n+  stdx(obj, t1, R16_thread);\n+  addi(t1, t1, oopSize);\n+  stw(t1, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+}\n+\n+\/\/ Implements fast-unlocking.\n+\/\/ Branches to slow upon failure, with CCR0 NE.\n+\/\/ Falls through upon success, with CCR0 EQ.\n+\/\/\n+\/\/ - obj: the object to be unlocked\n+\/\/ - hdr: the (pre-loaded) header of the object, will be destroyed\n+void MacroAssembler::fast_unlock(Register obj, Register hdr, Label& slow) {\n+  assert(LockingMode == LM_LIGHTWEIGHT, \"only used with new lightweight locking\");\n+  assert_different_registers(obj, hdr);\n+\n+#ifdef ASSERT\n+  {\n+    \/\/ Check that hdr is fast-locked.\n+    Label hdr_ok;\n+    andi_(R0, hdr, markWord::lock_mask_in_place);\n+    beq(CCR0, hdr_ok);\n+    stop(\"Header is not fast-locked\");\n+    bind(hdr_ok);\n+  }\n+  Register t1 = hdr; \/\/ Reuse in debug build.\n+  {\n+    \/\/ The following checks rely on the fact that LockStack is only ever modified by\n+    \/\/ its owning thread, even if the lock got inflated concurrently; removal of LockStack\n+    \/\/ entries after inflation will happen delayed in that case.\n+\n+    \/\/ Check for lock-stack underflow.\n+    Label stack_ok;\n+    lwz(t1, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+    cmplwi(CCR0, t1, LockStack::start_offset());\n+    bgt(CCR0, stack_ok);\n+    stop(\"Lock-stack underflow\");\n+    bind(stack_ok);\n+  }\n+  {\n+    \/\/ Check if the top of the lock-stack matches the unlocked object.\n+    Label tos_ok;\n+    addi(t1, t1, -oopSize);\n+    ldx(t1, t1, R16_thread);\n+    cmpd(CCR0, t1, obj);\n+    beq(CCR0, tos_ok);\n+    stop(\"Top of lock-stack does not match the unlocked object\");\n+    bind(tos_ok);\n+  }\n+#endif\n+\n+  \/\/ Release the lock.\n+  atomically_flip_locked_state(\/* is_unlock *\/ true, obj, hdr, slow, MacroAssembler::MemBarRel);\n+\n+  \/\/ After successful unlock, pop object from lock-stack\n+  Register t2 = hdr;\n+  lwz(t2, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+  addi(t2, t2, -oopSize);\n+#ifdef ASSERT\n+  li(R0, 0);\n+  stdx(R0, t2, R16_thread);\n+#endif\n+  stw(t2, in_bytes(JavaThread::lock_stack_top_offset()), R16_thread);\n+}\n","filename":"src\/hotspot\/cpu\/ppc\/macroAssembler_ppc.cpp","additions":203,"deletions":56,"binary":false,"changes":259,"status":"modified"},{"patch":"@@ -566,2 +566,2 @@\n-  andi(t0, value, JNIHandles::tag_mask);\n-  bnez(t0, tagged);\n+  andi(tmp1, value, JNIHandles::tag_mask);\n+  bnez(tmp1, tagged);\n@@ -576,2 +576,3 @@\n-  test_bit(t0, value, exact_log2(JNIHandles::TypeTag::weak_global));\n-  bnez(t0, weak_tagged);\n+  STATIC_ASSERT(JNIHandles::TypeTag::weak_global == 0b1);\n+  test_bit(tmp1, value, exact_log2(JNIHandles::TypeTag::weak_global));\n+  bnez(tmp1, weak_tagged);\n@@ -582,0 +583,1 @@\n+  verify_oop(value);\n@@ -601,0 +603,1 @@\n+    STATIC_ASSERT(JNIHandles::TypeTag::global == 0b10);\n@@ -602,2 +605,2 @@\n-    test_bit(t0, value, exact_log2(JNIHandles::TypeTag::global)); \/\/ Test for global tag.\n-    bnez(t0, valid_global_tag);\n+    test_bit(tmp1, value, exact_log2(JNIHandles::TypeTag::global)); \/\/ Test for global tag.\n+    bnez(tmp1, valid_global_tag);\n@@ -758,0 +761,5 @@\n+void MacroAssembler::li16u(Register Rd, uint16_t imm) {\n+  lui(Rd, (uint32_t)imm << 12);\n+  srli(Rd, Rd, 12);\n+}\n+\n@@ -1273,1 +1281,1 @@\n-    vl1re8_v(as_VectorRegister(regs[i]), stack);\n+    vl1r_v(as_VectorRegister(regs[i]), stack);\n@@ -1407,0 +1415,5 @@\n+static int patch_imm_in_li16u(address branch, uint16_t target) {\n+  Assembler::patch(branch, 31, 12, target); \/\/ patch lui only\n+  return NativeInstruction::instruction_size;\n+}\n+\n@@ -1421,1 +1434,1 @@\n-  unsigned insn = *(unsigned*)insn_addr;\n+  unsigned insn = Assembler::ld_instr(insn_addr);\n@@ -1434,1 +1447,1 @@\n-  unsigned insn = *(unsigned*)insn_addr;\n+  unsigned insn = Assembler::ld_instr(insn_addr);\n@@ -1446,2 +1459,2 @@\n-  offset = ((long)(Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12))) << 12;                                  \/\/ Auipc.\n-  offset += ((long)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20));                                         \/\/ Addi\/Jalr\/Load.\n+  offset = ((long)(Assembler::sextract(Assembler::ld_instr(insn_addr), 31, 12))) << 12;                               \/\/ Auipc.\n+  offset += ((long)Assembler::sextract(Assembler::ld_instr(insn_addr + 4), 31, 20));                                  \/\/ Addi\/Jalr\/Load.\n@@ -1454,4 +1467,4 @@\n-  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 29;    \/\/ Lui.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20)) << 17;                        \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[3], 31, 20)) << 6;                         \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[5], 31, 20));                              \/\/ Addi\/Jalr\/Load.\n+  intptr_t target_address = (((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr), 31, 12)) & 0xfffff) << 29; \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 4), 31, 20)) << 17;                 \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 12), 31, 20)) << 6;                 \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 20), 31, 20));                      \/\/ Addi\/Jalr\/Load.\n@@ -1463,5 +1476,5 @@\n-  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 44;    \/\/ Lui.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20)) << 32;                        \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[3], 31, 20)) << 20;                        \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[5], 31, 20)) << 8;                         \/\/ Addi.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[7], 31, 20));                              \/\/ Addi.\n+  intptr_t target_address = (((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr), 31, 12)) & 0xfffff) << 44; \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 4), 31, 20)) << 32;                 \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 12), 31, 20)) << 20;                \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 20), 31, 20)) << 8;                 \/\/ Addi.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 28), 31, 20));                      \/\/ Addi.\n@@ -1473,2 +1486,2 @@\n-  intptr_t target_address = (((int64_t)Assembler::sextract(((unsigned*)insn_addr)[0], 31, 12)) & 0xfffff) << 12;    \/\/ Lui.\n-  target_address += ((int64_t)Assembler::sextract(((unsigned*)insn_addr)[1], 31, 20));                              \/\/ Addiw.\n+  intptr_t target_address = (((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr), 31, 12)) & 0xfffff) << 12; \/\/ Lui.\n+  target_address += ((int64_t)Assembler::sextract(Assembler::ld_instr(insn_addr + 4), 31, 20));                       \/\/ Addiw.\n@@ -1496,0 +1509,3 @@\n+  } else if (NativeInstruction::is_li16u_at(branch)) {\n+    int64_t imm = (intptr_t)target;\n+    return patch_imm_in_li16u(branch, (uint16_t)imm);\n@@ -1499,1 +1515,1 @@\n-                  *(unsigned*)branch, p2i(branch));\n+                  Assembler::ld_instr(branch), p2i(branch));\n@@ -1629,3 +1645,1 @@\n-  \/\/ addw: The result is clipped to 32 bits, then the sign bit is extended,\n-  \/\/ and the result is stored in Rd\n-  addw(Rd, Rd, zr);\n+  sign_extend(Rd, Rd, 32);\n@@ -1636,3 +1650,1 @@\n-  \/\/ addw: The result is clipped to 32 bits, then the sign bit is extended,\n-  \/\/ and the result is stored in Rd\n-  addw(Rd, Rd, zr);\n+  sign_extend(Rd, Rd, 32);\n@@ -1643,3 +1655,1 @@\n-  \/\/ addw: The result is clipped to 32 bits, then the sign bit is extended,\n-  \/\/ and the result is stored in Rd\n-  addw(Rd, Rd, zr);\n+  sign_extend(Rd, Rd, 32);\n@@ -1693,0 +1703,87 @@\n+\/\/ granularity is 1, 2 bytes per load\n+void MacroAssembler::load_int_misaligned(Register dst, Address src, Register tmp, bool is_signed, int granularity) {\n+  if (AvoidUnalignedAccesses && (granularity != 4)) {\n+    assert_different_registers(dst, tmp, src.base());\n+    switch(granularity) {\n+      case 1:\n+        lbu(dst, src);\n+        lbu(tmp, Address(src.base(), src.offset() + 1));\n+        slli(tmp, tmp, 8);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 2));\n+        slli(tmp, tmp, 16);\n+        add(dst, dst, tmp);\n+        is_signed ? lb(tmp, Address(src.base(), src.offset() + 3)) : lbu(tmp, Address(src.base(), src.offset() + 3));\n+        slli(tmp, tmp, 24);\n+        add(dst, dst, tmp);\n+        break;\n+      case 2:\n+        lhu(dst, src);\n+        is_signed ? lh(tmp, Address(src.base(), src.offset() + 2)) : lhu(tmp, Address(src.base(), src.offset() + 2));\n+        slli(tmp, tmp, 16);\n+        add(dst, dst, tmp);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    is_signed ? lw(dst, src) : lwu(dst, src);\n+  }\n+}\n+\n+\/\/ granularity is 1, 2 or 4 bytes per load\n+void MacroAssembler::load_long_misaligned(Register dst, Address src, Register tmp, int granularity) {\n+  if (AvoidUnalignedAccesses && (granularity != 8)) {\n+    assert_different_registers(dst, tmp, src.base());\n+    switch(granularity){\n+      case 1:\n+        lbu(dst, src);\n+        lbu(tmp, Address(src.base(), src.offset() + 1));\n+        slli(tmp, tmp, 8);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 2));\n+        slli(tmp, tmp, 16);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 3));\n+        slli(tmp, tmp, 24);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 4));\n+        slli(tmp, tmp, 32);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 5));\n+        slli(tmp, tmp, 40);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 6));\n+        slli(tmp, tmp, 48);\n+        add(dst, dst, tmp);\n+        lbu(tmp, Address(src.base(), src.offset() + 7));\n+        slli(tmp, tmp, 56);\n+        add(dst, dst, tmp);\n+        break;\n+      case 2:\n+        lhu(dst, src);\n+        lhu(tmp, Address(src.base(), src.offset() + 2));\n+        slli(tmp, tmp, 16);\n+        add(dst, dst, tmp);\n+        lhu(tmp, Address(src.base(), src.offset() + 4));\n+        slli(tmp, tmp, 32);\n+        add(dst, dst, tmp);\n+        lhu(tmp, Address(src.base(), src.offset() + 6));\n+        slli(tmp, tmp, 48);\n+        add(dst, dst, tmp);\n+        break;\n+      case 4:\n+        lwu(dst, src);\n+        lwu(tmp, Address(src.base(), src.offset() + 4));\n+        slli(tmp, tmp, 32);\n+        add(dst, dst, tmp);\n+        break;\n+      default:\n+        ShouldNotReachHere();\n+    }\n+  } else {\n+    ld(dst, src);\n+  }\n+}\n+\n+\n@@ -1985,1 +2082,1 @@\n-  ld(dst, Address(dst, ConstantPool::pool_holder_offset_in_bytes()));\n+  ld(dst, Address(dst, ConstantPool::pool_holder_offset()));\n@@ -2300,1 +2397,1 @@\n-  int itentry_off = itableMethodEntry::method_offset_in_bytes();\n+  int itentry_off = in_bytes(itableMethodEntry::method_offset());\n@@ -2327,1 +2424,1 @@\n-  ld(method_result, Address(scan_tmp, itableOffsetEntry::interface_offset_in_bytes()));\n+  ld(method_result, Address(scan_tmp, itableOffsetEntry::interface_offset()));\n@@ -2335,1 +2432,1 @@\n-  ld(method_result, Address(scan_tmp, itableOffsetEntry::interface_offset_in_bytes()));\n+  ld(method_result, Address(scan_tmp, itableOffsetEntry::interface_offset()));\n@@ -2342,1 +2439,1 @@\n-    lwu(scan_tmp, Address(scan_tmp, itableOffsetEntry::offset_offset_in_bytes()));\n+    lwu(scan_tmp, Address(scan_tmp, itableOffsetEntry::offset_offset()));\n@@ -2352,1 +2449,1 @@\n-  const int base = in_bytes(Klass::vtable_start_offset());\n+  const ByteSize base = Klass::vtable_start_offset();\n@@ -2355,1 +2452,1 @@\n-  int vtable_offset_in_bytes = base + vtableEntry::method_offset_in_bytes();\n+  int vtable_offset_in_bytes = in_bytes(base + vtableEntry::method_offset());\n@@ -2429,0 +2526,4 @@\n+  assert_different_registers(addr, tmp);\n+  assert_different_registers(newv, tmp);\n+  assert_different_registers(oldv, tmp);\n+\n@@ -2615,0 +2716,3 @@\n+  assert_different_registers(addr, t0);\n+  assert_different_registers(expected, t0);\n+  assert_different_registers(new_val, t0);\n@@ -2647,0 +2751,4 @@\n+  assert_different_registers(addr, t0);\n+  assert_different_registers(expected, t0);\n+  assert_different_registers(new_val, t0);\n+\n@@ -3272,1 +3380,1 @@\n-  ld(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); \/\/ InstanceKlass*\n+  ld(holder, Address(holder, ConstantPool::pool_holder_offset()));          \/\/ InstanceKlass*\n@@ -4308,2 +4416,1 @@\n-      \/\/ 32bits extend sign\n-      addw(dst.first()->as_Register(), src.first()->as_Register(), zr);\n+      sign_extend(dst.first()->as_Register(), src.first()->as_Register(), 32);\n","filename":"src\/hotspot\/cpu\/riscv\/macroAssembler_riscv.cpp","additions":149,"deletions":42,"binary":false,"changes":191,"status":"modified"},{"patch":"@@ -1630,1 +1630,0 @@\n-    __ addw(t0, t0, zr);\n@@ -1831,1 +1830,1 @@\n-      __ andi(t0, old_hdr, markWord::monitor_value);\n+      __ test_bit(t0, old_hdr, exact_log2(markWord::monitor_value));\n@@ -1872,1 +1871,1 @@\n-  __ sd(zr, Address(x12, JNIHandleBlock::top_offset_in_bytes()));\n+  __ sd(zr, Address(x12, JNIHandleBlock::top_offset()));\n@@ -2281,1 +2280,1 @@\n-  __ lwu(xcpool, Address(x15, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));\n+  __ lwu(xcpool, Address(x15, Deoptimization::UnrollBlock::unpack_kind_offset()));\n@@ -2324,1 +2323,1 @@\n-  __ lwu(x12, Address(x15, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));\n+  __ lwu(x12, Address(x15, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset()));\n@@ -2336,1 +2335,1 @@\n-  __ lwu(x9, Address(x15, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));\n+  __ lwu(x9, Address(x15, Deoptimization::UnrollBlock::total_frame_sizes_offset()));\n@@ -2340,1 +2339,1 @@\n-  __ ld(x12, Address(x15, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+  __ ld(x12, Address(x15, Deoptimization::UnrollBlock::frame_pcs_offset()));\n@@ -2343,1 +2342,1 @@\n-  __ ld(x14, Address(x15, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));\n+  __ ld(x14, Address(x15, Deoptimization::UnrollBlock::frame_sizes_offset()));\n@@ -2346,1 +2345,1 @@\n-  __ lwu(x13, Address(x15, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));\n+  __ lwu(x13, Address(x15, Deoptimization::UnrollBlock::number_of_frames_offset()));\n@@ -2358,1 +2357,1 @@\n-                     caller_adjustment_offset_in_bytes()));\n+                     caller_adjustment_offset()));\n@@ -2482,1 +2481,1 @@\n-  __ addiw(c_rarg1, j_rarg0, 0);\n+  __ sign_extend(c_rarg1, j_rarg0, 32);\n@@ -2524,1 +2523,1 @@\n-    __ lwu(t0, Address(x14, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));\n+    __ lwu(t0, Address(x14, Deoptimization::UnrollBlock::unpack_kind_offset()));\n@@ -2544,1 +2543,1 @@\n-                      size_of_deoptimized_frame_offset_in_bytes()));\n+                      size_of_deoptimized_frame_offset()));\n@@ -2558,1 +2557,1 @@\n-                      total_frame_sizes_offset_in_bytes()));\n+                      total_frame_sizes_offset()));\n@@ -2564,1 +2563,1 @@\n-                     Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));\n+                     Deoptimization::UnrollBlock::frame_pcs_offset()));\n@@ -2569,1 +2568,1 @@\n-                     frame_sizes_offset_in_bytes()));\n+                     frame_sizes_offset()));\n@@ -2574,1 +2573,1 @@\n-                      number_of_frames_offset_in_bytes())); \/\/ (int)\n+                      number_of_frames_offset())); \/\/ (int)\n@@ -2585,1 +2584,1 @@\n-                      caller_adjustment_offset_in_bytes())); \/\/ (int)\n+                      caller_adjustment_offset())); \/\/ (int)\n","filename":"src\/hotspot\/cpu\/riscv\/sharedRuntime_riscv.cpp","additions":17,"deletions":18,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -2770,3 +2770,1 @@\n-  const int vtable_base_offset = in_bytes(Klass::vtable_start_offset());\n-\n-                     vtable_base_offset + itableOffsetEntry::interface_offset_in_bytes(),\n+                     in_bytes(Klass::vtable_start_offset() + itableOffsetEntry::interface_offset()),\n@@ -2793,2 +2791,2 @@\n-    const int vtable_offset_offset = (itableOffsetEntry::offset_offset_in_bytes() -\n-                                      itableOffsetEntry::interface_offset_in_bytes()) -\n+    const int vtable_offset_offset = in_bytes(itableOffsetEntry::offset_offset() -\n+                                              itableOffsetEntry::interface_offset()) -\n@@ -2802,1 +2800,1 @@\n-    int method_offset = itableMethodEntry::method_offset_in_bytes();\n+    int method_offset = in_bytes(itableMethodEntry::method_offset());\n@@ -2842,1 +2840,1 @@\n-                              vtableEntry::method_offset_in_bytes());\n+                              in_bytes(vtableEntry::method_offset()));\n@@ -2849,1 +2847,1 @@\n-                               base + vtableEntry::method_offset_in_bytes());\n+                               base + in_bytes(vtableEntry::method_offset()));\n@@ -3175,2 +3173,3 @@\n-  \/\/ Set mark to markWord | markWord::unlocked_value.\n-  z_oill(displacedHeader, markWord::unlocked_value);\n+  if (LockingMode != LM_MONITOR) {\n+    \/\/ Set mark to markWord | markWord::unlocked_value.\n+    z_oill(displacedHeader, markWord::unlocked_value);\n@@ -3178,1 +3177,1 @@\n-  \/\/ Load Compare Value application register.\n+    \/\/ Load Compare Value application register.\n@@ -3180,2 +3179,2 @@\n-  \/\/ Initialize the box (must happen before we update the object mark).\n-  z_stg(displacedHeader, BasicLock::displaced_header_offset_in_bytes(), box);\n+    \/\/ Initialize the box (must happen before we update the object mark).\n+    z_stg(displacedHeader, BasicLock::displaced_header_offset_in_bytes(), box);\n@@ -3183,2 +3182,2 @@\n-  \/\/ Memory Fence (in cmpxchgd)\n-  \/\/ Compare object markWord with mark and if equal exchange scratch1 with object markWord.\n+    \/\/ Memory Fence (in cmpxchgd)\n+    \/\/ Compare object markWord with mark and if equal exchange scratch1 with object markWord.\n@@ -3186,5 +3185,10 @@\n-  \/\/ If the compare-and-swap succeeded, then we found an unlocked object and we\n-  \/\/ have now locked it.\n-  z_csg(displacedHeader, box, 0, oop);\n-  assert(currentHeader==displacedHeader, \"must be same register\"); \/\/ Identified two registers from z\/Architecture.\n-  z_bre(done);\n+    \/\/ If the compare-and-swap succeeded, then we found an unlocked object and we\n+    \/\/ have now locked it.\n+    z_csg(displacedHeader, box, 0, oop);\n+    assert(currentHeader == displacedHeader, \"must be same register\"); \/\/ Identified two registers from z\/Architecture.\n+    z_bre(done);\n+  } else {\n+    \/\/ Set NE to indicate 'failure' -> take slow-path\n+    z_ltgr(oop, oop);\n+    z_bru(done);\n+  }\n@@ -3242,4 +3246,6 @@\n-  \/\/ Find the lock address and load the displaced header from the stack.\n-  \/\/ if the displaced header is zero, we have a recursive unlock.\n-  load_and_test_long(displacedHeader, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n-  z_bre(done);\n+  if (LockingMode != LM_MONITOR) {\n+    \/\/ Find the lock address and load the displaced header from the stack.\n+    \/\/ if the displaced header is zero, we have a recursive unlock.\n+    load_and_test_long(displacedHeader, Address(box, BasicLock::displaced_header_offset_in_bytes()));\n+    z_bre(done);\n+  }\n@@ -3254,6 +3260,12 @@\n-  \/\/ Check if it is still a light weight lock, this is true if we see\n-  \/\/ the stack address of the basicLock in the markWord of the object\n-  \/\/ copy box to currentHeader such that csg does not kill it.\n-  z_lgr(currentHeader, box);\n-  z_csg(currentHeader, displacedHeader, 0, oop);\n-  z_bru(done); \/\/ Csg sets CR as desired.\n+  if (LockingMode != LM_MONITOR) {\n+    \/\/ Check if it is still a light weight lock, this is true if we see\n+    \/\/ the stack address of the basicLock in the markWord of the object\n+    \/\/ copy box to currentHeader such that csg does not kill it.\n+    z_lgr(currentHeader, box);\n+    z_csg(currentHeader, displacedHeader, 0, oop);\n+    z_bru(done); \/\/ Csg sets CR as desired.\n+  } else {\n+    \/\/ Set NE to indicate 'failure' -> take slow-path\n+    z_ltgr(oop, oop);\n+    z_bru(done);\n+  }\n@@ -4201,1 +4213,1 @@\n-  mem2reg_opt(mirror, Address(mirror, ConstantPool::pool_holder_offset_in_bytes()));\n+  mem2reg_opt(mirror, Address(mirror, ConstantPool::pool_holder_offset()));\n@@ -4209,1 +4221,1 @@\n-  mem2reg_opt(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes()));\n+  mem2reg_opt(holder, Address(holder, ConstantPool::pool_holder_offset()));\n","filename":"src\/hotspot\/cpu\/s390\/macroAssembler_s390.cpp","additions":44,"deletions":32,"binary":false,"changes":76,"status":"modified"},{"patch":"@@ -1353,2 +1353,2 @@\n-    \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n-    if (!UseZGC) {\n+    if (!(UseZGC && !ZGenerational)) {\n+      \/\/ Load barrier has not yet been applied, so ZGC can't verify the oop here\n@@ -2837,1 +2837,1 @@\n-    __ set_byte_if_not_zero(dest);\n+    __ setb(Assembler::notZero, dest);\n","filename":"src\/hotspot\/cpu\/x86\/c1_LIRAssembler_x86.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -51,1 +51,1 @@\n-  movptr(Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()), obj);\n+  movptr(Address(disp_hdr, BasicObjectLock::obj_offset()), obj);\n@@ -132,1 +132,1 @@\n-  movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset_in_bytes()));\n+  movptr(obj, Address(disp_hdr, BasicObjectLock::obj_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/c1_MacroAssembler_x86.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -235,1 +235,1 @@\n-    orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), NoRTM);\n+    orl(Address(tmpReg, MethodData::rtm_state_offset()), NoRTM);\n@@ -249,1 +249,1 @@\n-    orl(Address(tmpReg, MethodData::rtm_state_offset_in_bytes()), UseRTM);\n+    orl(Address(tmpReg, MethodData::rtm_state_offset()), UseRTM);\n@@ -4289,0 +4289,50 @@\n+#ifdef _LP64\n+\n+static void convertF2I_slowpath(C2_MacroAssembler& masm, C2GeneralStub<Register, XMMRegister, address>& stub) {\n+#define __ masm.\n+  Register dst = stub.data<0>();\n+  XMMRegister src = stub.data<1>();\n+  address target = stub.data<2>();\n+  __ bind(stub.entry());\n+  __ subptr(rsp, 8);\n+  __ movdbl(Address(rsp), src);\n+  __ call(RuntimeAddress(target));\n+  __ pop(dst);\n+  __ jmp(stub.continuation());\n+#undef __\n+}\n+\n+void C2_MacroAssembler::convertF2I(BasicType dst_bt, BasicType src_bt, Register dst, XMMRegister src) {\n+  assert(dst_bt == T_INT || dst_bt == T_LONG, \"\");\n+  assert(src_bt == T_FLOAT || src_bt == T_DOUBLE, \"\");\n+\n+  address slowpath_target;\n+  if (dst_bt == T_INT) {\n+    if (src_bt == T_FLOAT) {\n+      cvttss2sil(dst, src);\n+      cmpl(dst, 0x80000000);\n+      slowpath_target = StubRoutines::x86::f2i_fixup();\n+    } else {\n+      cvttsd2sil(dst, src);\n+      cmpl(dst, 0x80000000);\n+      slowpath_target = StubRoutines::x86::d2i_fixup();\n+    }\n+  } else {\n+    if (src_bt == T_FLOAT) {\n+      cvttss2siq(dst, src);\n+      cmp64(dst, ExternalAddress(StubRoutines::x86::double_sign_flip()));\n+      slowpath_target = StubRoutines::x86::f2l_fixup();\n+    } else {\n+      cvttsd2siq(dst, src);\n+      cmp64(dst, ExternalAddress(StubRoutines::x86::double_sign_flip()));\n+      slowpath_target = StubRoutines::x86::d2l_fixup();\n+    }\n+  }\n+\n+  auto stub = C2CodeStub::make<Register, XMMRegister, address>(dst, src, slowpath_target, 23, convertF2I_slowpath);\n+  jcc(Assembler::equal, stub->entry());\n+  bind(stub->continuation());\n+}\n+\n+#endif \/\/ _LP64\n+\n","filename":"src\/hotspot\/cpu\/x86\/c2_MacroAssembler_x86.cpp","additions":52,"deletions":2,"binary":false,"changes":54,"status":"modified"},{"patch":"@@ -4260,1 +4260,1 @@\n-  int itentry_off = itableMethodEntry::method_offset_in_bytes();\n+  int itentry_off = in_bytes(itableMethodEntry::method_offset());\n@@ -4285,1 +4285,1 @@\n-    movptr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset_in_bytes()));\n+    movptr(method_result, Address(scan_temp, itableOffsetEntry::interface_offset()));\n@@ -4311,1 +4311,1 @@\n-    movl(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset_in_bytes()));\n+    movl(scan_temp, Address(scan_temp, itableOffsetEntry::offset_offset()));\n@@ -4316,0 +4316,119 @@\n+\/\/ Look up the method for a megamorphic invokeinterface call in a single pass over itable:\n+\/\/ - check recv_klass (actual object class) is a subtype of resolved_klass from CompiledICHolder\n+\/\/ - find a holder_klass (class that implements the method) vtable offset and get the method from vtable by index\n+\/\/ The target method is determined by <holder_klass, itable_index>.\n+\/\/ The receiver klass is in recv_klass.\n+\/\/ On success, the result will be in method_result, and execution falls through.\n+\/\/ On failure, execution transfers to the given label.\n+void MacroAssembler::lookup_interface_method_stub(Register recv_klass,\n+                                                  Register holder_klass,\n+                                                  Register resolved_klass,\n+                                                  Register method_result,\n+                                                  Register scan_temp,\n+                                                  Register temp_reg2,\n+                                                  Register receiver,\n+                                                  int itable_index,\n+                                                  Label& L_no_such_interface) {\n+  assert_different_registers(recv_klass, method_result, holder_klass, resolved_klass, scan_temp, temp_reg2, receiver);\n+  Register temp_itbl_klass = method_result;\n+  Register temp_reg = (temp_reg2 == noreg ? recv_klass : temp_reg2); \/\/ reuse recv_klass register on 32-bit x86 impl\n+\n+  int vtable_base = in_bytes(Klass::vtable_start_offset());\n+  int itentry_off = in_bytes(itableMethodEntry::method_offset());\n+  int scan_step = itableOffsetEntry::size() * wordSize;\n+  int vte_size = vtableEntry::size_in_bytes();\n+  int ioffset = in_bytes(itableOffsetEntry::interface_offset());\n+  int ooffset = in_bytes(itableOffsetEntry::offset_offset());\n+  Address::ScaleFactor times_vte_scale = Address::times_ptr;\n+  assert(vte_size == wordSize, \"adjust times_vte_scale\");\n+\n+  Label L_loop_scan_resolved_entry, L_resolved_found, L_holder_found;\n+\n+  \/\/ temp_itbl_klass = recv_klass.itable[0]\n+  \/\/ scan_temp = &recv_klass.itable[0] + step\n+  movl(scan_temp, Address(recv_klass, Klass::vtable_length_offset()));\n+  movptr(temp_itbl_klass, Address(recv_klass, scan_temp, times_vte_scale, vtable_base + ioffset));\n+  lea(scan_temp, Address(recv_klass, scan_temp, times_vte_scale, vtable_base + ioffset + scan_step));\n+  xorptr(temp_reg, temp_reg);\n+\n+  \/\/ Initial checks:\n+  \/\/   - if (holder_klass != resolved_klass), go to \"scan for resolved\"\n+  \/\/   - if (itable[0] == 0), no such interface\n+  \/\/   - if (itable[0] == holder_klass), shortcut to \"holder found\"\n+  cmpptr(holder_klass, resolved_klass);\n+  jccb(Assembler::notEqual, L_loop_scan_resolved_entry);\n+  testptr(temp_itbl_klass, temp_itbl_klass);\n+  jccb(Assembler::zero, L_no_such_interface);\n+  cmpptr(holder_klass, temp_itbl_klass);\n+  jccb(Assembler::equal, L_holder_found);\n+\n+  \/\/ Loop: Look for holder_klass record in itable\n+  \/\/   do {\n+  \/\/     tmp = itable[index];\n+  \/\/     index += step;\n+  \/\/     if (tmp == holder_klass) {\n+  \/\/       goto L_holder_found; \/\/ Found!\n+  \/\/     }\n+  \/\/   } while (tmp != 0);\n+  \/\/   goto L_no_such_interface \/\/ Not found.\n+  Label L_scan_holder;\n+  bind(L_scan_holder);\n+    movptr(temp_itbl_klass, Address(scan_temp, 0));\n+    addptr(scan_temp, scan_step);\n+    cmpptr(holder_klass, temp_itbl_klass);\n+    jccb(Assembler::equal, L_holder_found);\n+    testptr(temp_itbl_klass, temp_itbl_klass);\n+    jccb(Assembler::notZero, L_scan_holder);\n+\n+  jmpb(L_no_such_interface);\n+\n+  \/\/ Loop: Look for resolved_class record in itable\n+  \/\/   do {\n+  \/\/     tmp = itable[index];\n+  \/\/     index += step;\n+  \/\/     if (tmp == holder_klass) {\n+  \/\/        \/\/ Also check if we have met a holder klass\n+  \/\/        holder_tmp = itable[index-step-ioffset];\n+  \/\/     }\n+  \/\/     if (tmp == resolved_klass) {\n+  \/\/        goto L_resolved_found;  \/\/ Found!\n+  \/\/     }\n+  \/\/   } while (tmp != 0);\n+  \/\/   goto L_no_such_interface \/\/ Not found.\n+  \/\/\n+  Label L_loop_scan_resolved;\n+  bind(L_loop_scan_resolved);\n+    movptr(temp_itbl_klass, Address(scan_temp, 0));\n+    addptr(scan_temp, scan_step);\n+    bind(L_loop_scan_resolved_entry);\n+    cmpptr(holder_klass, temp_itbl_klass);\n+    cmovl(Assembler::equal, temp_reg, Address(scan_temp, ooffset - ioffset - scan_step));\n+    cmpptr(resolved_klass, temp_itbl_klass);\n+    jccb(Assembler::equal, L_resolved_found);\n+    testptr(temp_itbl_klass, temp_itbl_klass);\n+    jccb(Assembler::notZero, L_loop_scan_resolved);\n+\n+  jmpb(L_no_such_interface);\n+\n+  Label L_ready;\n+\n+  \/\/ See if we already have a holder klass. If not, go and scan for it.\n+  bind(L_resolved_found);\n+  testptr(temp_reg, temp_reg);\n+  jccb(Assembler::zero, L_scan_holder);\n+  jmpb(L_ready);\n+\n+  bind(L_holder_found);\n+  movl(temp_reg, Address(scan_temp, ooffset - ioffset - scan_step));\n+\n+  \/\/ Finally, temp_reg contains holder_klass vtable offset\n+  bind(L_ready);\n+  assert(itableMethodEntry::size() * wordSize == wordSize, \"adjust the scaling in the code below\");\n+  if (temp_reg2 == noreg) { \/\/ recv_klass register is clobbered for 32-bit x86 impl\n+    load_klass(scan_temp, receiver, noreg);\n+    movptr(method_result, Address(scan_temp, temp_reg, Address::times_1, itable_index * wordSize + itentry_off));\n+  } else {\n+    movptr(method_result, Address(recv_klass, temp_reg, Address::times_1, itable_index * wordSize + itentry_off));\n+  }\n+}\n+\n@@ -4321,1 +4440,1 @@\n-  const int base = in_bytes(Klass::vtable_start_offset());\n+  const ByteSize base = Klass::vtable_start_offset();\n@@ -4325,1 +4444,1 @@\n-                            base + vtableEntry::method_offset_in_bytes());\n+                            base + vtableEntry::method_offset());\n@@ -5132,1 +5251,1 @@\n-  movptr(holder, Address(holder, ConstantPool::pool_holder_offset_in_bytes())); \/\/ InstanceKlass*\n+  movptr(holder, Address(holder, ConstantPool::pool_holder_offset()));          \/\/ InstanceKlass*\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.cpp","additions":125,"deletions":6,"binary":false,"changes":131,"status":"modified"},{"patch":"@@ -639,0 +639,10 @@\n+  void lookup_interface_method_stub(Register recv_klass,\n+                                    Register holder_klass,\n+                                    Register resolved_klass,\n+                                    Register method_result,\n+                                    Register scan_temp,\n+                                    Register temp_reg2,\n+                                    Register receiver,\n+                                    int itable_index,\n+                                    Label& L_no_such_interface);\n+\n","filename":"src\/hotspot\/cpu\/x86\/macroAssembler_x86.hpp","additions":10,"deletions":0,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -95,0 +95,2 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if (!is_LP64 || UseAVX < 1 || !UseFMA) {\n@@ -105,0 +107,8 @@\n+  } else {\n+    assert(StubRoutines::fmod() != nullptr, \"\");\n+    jdouble (*addr)(jdouble, jdouble) = (double (*)(double, double))StubRoutines::fmod();\n+    jdouble dx = (jdouble) x;\n+    jdouble dy = (jdouble) y;\n+\n+    retval = (jfloat) (*addr)(dx, dy);\n+  }\n@@ -110,0 +120,2 @@\n+  const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);\n+  if (!is_LP64 || UseAVX < 1 || !UseFMA) {\n@@ -120,0 +132,6 @@\n+  } else {\n+    assert(StubRoutines::fmod() != nullptr, \"\");\n+    jdouble (*addr)(jdouble, jdouble) = (double (*)(double, double))StubRoutines::fmod();\n+\n+    retval = (*addr)(x, y);\n+  }\n","filename":"src\/hotspot\/cpu\/x86\/sharedRuntime_x86.cpp","additions":18,"deletions":0,"binary":false,"changes":18,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/gc_globals.hpp\"\n@@ -38,0 +39,1 @@\n+#include \"oops\/resolvedIndyEntry.hpp\"\n@@ -4350,1 +4352,1 @@\n-    __ cmpptr(Address(rtop, BasicObjectLock::obj_offset_in_bytes()), NULL_WORD);\n+    __ cmpptr(Address(rtop, BasicObjectLock::obj_offset()), NULL_WORD);\n@@ -4354,1 +4356,1 @@\n-    __ cmpptr(rax, Address(rtop, BasicObjectLock::obj_offset_in_bytes()));\n+    __ cmpptr(rax, Address(rtop, BasicObjectLock::obj_offset()));\n@@ -4403,1 +4405,1 @@\n-  __ movptr(Address(rmon, BasicObjectLock::obj_offset_in_bytes()), rax);\n+  __ movptr(Address(rmon, BasicObjectLock::obj_offset()), rax);\n@@ -4443,1 +4445,1 @@\n-    __ cmpptr(rax, Address(rtop, BasicObjectLock::obj_offset_in_bytes()));\n+    __ cmpptr(rax, Address(rtop, BasicObjectLock::obj_offset()));\n","filename":"src\/hotspot\/cpu\/x86\/templateTable_x86.cpp","additions":6,"deletions":4,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -3238,1 +3238,1 @@\n-            && is_power_of_2(n->get_int() + 1));\n+            && is_power_of_2((juint)n->get_int() + 1));\n@@ -8120,1 +8120,1 @@\n-    __ sete($res$$Register);\n+    __ setb(Assembler::equal, $res$$Register);\n@@ -8143,1 +8143,1 @@\n-    __ sete($res$$Register);\n+    __ setb(Assembler::equal, $res$$Register);\n@@ -8165,1 +8165,1 @@\n-    __ sete($res$$Register);\n+    __ setb(Assembler::equal, $res$$Register);\n@@ -8187,1 +8187,1 @@\n-    __ sete($res$$Register);\n+    __ setb(Assembler::equal, $res$$Register);\n@@ -8209,1 +8209,1 @@\n-    __ sete($res$$Register);\n+    __ setb(Assembler::equal, $res$$Register);\n@@ -8230,1 +8230,1 @@\n-    __ sete($res$$Register);\n+    __ setb(Assembler::equal, $res$$Register);\n@@ -10653,34 +10653,0 @@\n-\/\/ Convert Int to Boolean\n-instruct convI2B(rRegI dst, rRegI src, rFlagsReg cr)\n-%{\n-  match(Set dst (Conv2B src));\n-  effect(KILL cr);\n-\n-  format %{ \"testl   $src, $src\\t# ci2b\\n\\t\"\n-            \"setnz   $dst\\n\\t\"\n-            \"movzbl  $dst, $dst\" %}\n-  ins_encode %{\n-    __ testl($src$$Register, $src$$Register);\n-    __ set_byte_if_not_zero($dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n-\/\/ Convert Pointer to Boolean\n-instruct convP2B(rRegI dst, rRegP src, rFlagsReg cr)\n-%{\n-  match(Set dst (Conv2B src));\n-  effect(KILL cr);\n-\n-  format %{ \"testq   $src, $src\\t# cp2b\\n\\t\"\n-            \"setnz   $dst\\n\\t\"\n-            \"movzbl  $dst, $dst\" %}\n-  ins_encode %{\n-    __ testq($src$$Register, $src$$Register);\n-    __ set_byte_if_not_zero($dst$$Register);\n-    __ movzbl($dst$$Register, $dst$$Register);\n-  %}\n-  ins_pipe(pipe_slow); \/\/ XXX\n-%}\n-\n@@ -10699,1 +10665,1 @@\n-    __ setl($dst$$Register);\n+    __ setb(Assembler::less, $dst$$Register);\n@@ -11047,1 +11013,1 @@\n-  format %{ \"convert_f2i $dst,$src\" %}\n+  format %{ \"convert_f2i $dst, $src\" %}\n@@ -11049,1 +11015,1 @@\n-    __ convert_f2i($dst$$Register, $src$$XMMRegister);\n+    __ convertF2I(T_INT, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n@@ -11058,1 +11024,1 @@\n-  format %{ \"convert_f2l $dst,$src\"%}\n+  format %{ \"convert_f2l $dst, $src\"%}\n@@ -11060,1 +11026,1 @@\n-    __ convert_f2l($dst$$Register, $src$$XMMRegister);\n+    __ convertF2I(T_LONG, T_FLOAT, $dst$$Register, $src$$XMMRegister);\n@@ -11069,1 +11035,1 @@\n-  format %{ \"convert_d2i $dst,$src\"%}\n+  format %{ \"convert_d2i $dst, $src\"%}\n@@ -11071,1 +11037,1 @@\n-    __ convert_d2i($dst$$Register, $src$$XMMRegister);\n+    __ convertF2I(T_INT, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n@@ -11080,1 +11046,1 @@\n-  format %{ \"convert_d2l $dst,$src\"%}\n+  format %{ \"convert_d2l $dst, $src\"%}\n@@ -11082,1 +11048,1 @@\n-    __ convert_d2l($dst$$Register, $src$$XMMRegister);\n+    __ convertF2I(T_LONG, T_DOUBLE, $dst$$Register, $src$$XMMRegister);\n@@ -12856,1 +12822,1 @@\n-    __ setne($dst$$Register);\n+    __ setb(Assembler::notZero, $dst$$Register);\n@@ -12882,1 +12848,1 @@\n-    __ setne($dst$$Register);\n+    __ setb(Assembler::notZero, $dst$$Register);\n@@ -12908,1 +12874,1 @@\n-    __ setne($dst$$Register);\n+    __ setb(Assembler::notZero, $dst$$Register);\n","filename":"src\/hotspot\/cpu\/x86\/x86_64.ad","additions":19,"deletions":53,"binary":false,"changes":72,"status":"modified"},{"patch":"@@ -57,1 +57,1 @@\n-  virtual CodeEmitInfo* info() const             { return NULL; }\n+  virtual CodeEmitInfo* info() const             { return nullptr; }\n@@ -173,1 +173,1 @@\n-    assert(info != NULL, \"must have info\");\n+    assert(info != nullptr, \"must have info\");\n@@ -181,1 +181,1 @@\n-    assert(info != NULL, \"must have info\");\n+    assert(info != nullptr, \"must have info\");\n@@ -426,1 +426,1 @@\n-    , _info(NULL)\n+    , _info(nullptr)\n","filename":"src\/hotspot\/share\/c1\/c1_CodeStubs.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 1999, 2022, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 1999, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -202,1 +202,1 @@\n-    guarantee(_cfg_printer_output != NULL, \"CFG printer output not initialized\");\n+    guarantee(_cfg_printer_output != nullptr, \"CFG printer output not initialized\");\n@@ -209,1 +209,1 @@\n-  bool bailed_out() const                        { return _bailout_msg != NULL; }\n+  bool bailed_out() const                        { return _bailout_msg != nullptr; }\n","filename":"src\/hotspot\/share\/c1\/c1_Compilation.hpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -56,3 +56,3 @@\n-    if (c != NULL && !c->value()->is_loaded()) {\n-      return LIR_OprFact::metadataConst(NULL);\n-    } else if (c != NULL) {\n+    if (c != nullptr && !c->value()->is_loaded()) {\n+      return LIR_OprFact::metadataConst(nullptr);\n+    } else if (c != nullptr) {\n@@ -62,1 +62,1 @@\n-      assert (m != NULL, \"not a class or a method?\");\n+      assert (m != nullptr, \"not a class or a method?\");\n@@ -239,1 +239,1 @@\n-  : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n+  : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)nullptr)\n@@ -242,2 +242,2 @@\n-  , _ublock(NULL)\n-  , _stub(NULL) {\n+  , _ublock(nullptr)\n+  , _stub(nullptr) {\n@@ -247,1 +247,1 @@\n-  LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n+  LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)nullptr)\n@@ -249,2 +249,2 @@\n-  , _block(NULL)\n-  , _ublock(NULL)\n+  , _block(nullptr)\n+  , _ublock(nullptr)\n@@ -255,1 +255,1 @@\n-  : LIR_Op2(lir_cond_float_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)NULL)\n+  : LIR_Op2(lir_cond_float_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*)nullptr)\n@@ -259,1 +259,1 @@\n-  , _stub(NULL)\n+  , _stub(nullptr)\n@@ -264,1 +264,1 @@\n-  assert(_block != NULL, \"must have old block\");\n+  assert(_block != nullptr, \"must have old block\");\n@@ -272,1 +272,1 @@\n-  assert(_ublock != NULL, \"must have old block\");\n+  assert(_ublock != nullptr, \"must have old block\");\n@@ -294,1 +294,1 @@\n-  : LIR_Op(code, result, NULL)\n+  : LIR_Op(code, result, nullptr)\n@@ -305,1 +305,1 @@\n-  , _profiled_method(NULL)\n+  , _profiled_method(nullptr)\n@@ -310,1 +310,1 @@\n-    assert(info_for_exception != NULL, \"checkcast throws exceptions\");\n+    assert(info_for_exception != nullptr, \"checkcast throws exceptions\");\n@@ -312,1 +312,1 @@\n-    assert(info_for_exception == NULL, \"instanceof throws no exceptions\");\n+    assert(info_for_exception == nullptr, \"instanceof throws no exceptions\");\n@@ -321,1 +321,1 @@\n-  : LIR_Op(code, LIR_OprFact::illegalOpr, NULL)\n+  : LIR_Op(code, LIR_OprFact::illegalOpr, nullptr)\n@@ -324,1 +324,1 @@\n-  , _klass(NULL)\n+  , _klass(nullptr)\n@@ -329,1 +329,1 @@\n-  , _info_for_patch(NULL)\n+  , _info_for_patch(nullptr)\n@@ -331,2 +331,2 @@\n-  , _stub(NULL)\n-  , _profiled_method(NULL)\n+  , _stub(nullptr)\n+  , _profiled_method(nullptr)\n@@ -338,1 +338,1 @@\n-    assert(info_for_exception != NULL, \"store_check throws exceptions\");\n+    assert(info_for_exception != nullptr, \"store_check throws exceptions\");\n@@ -360,1 +360,1 @@\n-  : LIR_Op(lir_updatecrc32, res, NULL)\n+  : LIR_Op(lir_updatecrc32, res, nullptr)\n@@ -412,2 +412,2 @@\n-      assert(op->as_Op0() != NULL, \"must be\");\n-      assert(op->_info == NULL, \"info not used by this instruction\");\n+      assert(op->as_Op0() != nullptr, \"must be\");\n+      assert(op->_info == nullptr, \"info not used by this instruction\");\n@@ -423,2 +423,2 @@\n-      assert(op->as_Op0() != NULL, \"must be\");\n-      if (op->_info != NULL)           do_info(op->_info);\n+      assert(op->as_Op0() != nullptr, \"must be\");\n+      if (op->_info != nullptr)           do_info(op->_info);\n@@ -433,2 +433,2 @@\n-      assert(op->as_OpLabel() != NULL, \"must be\");\n-      assert(op->_info == NULL, \"info not used by this instruction\");\n+      assert(op->as_OpLabel() != nullptr, \"must be\");\n+      assert(op->_info == nullptr, \"info not used by this instruction\");\n@@ -450,1 +450,1 @@\n-      assert(op->as_Op1() != NULL, \"must be\");\n+      assert(op->as_Op1() != nullptr, \"must be\");\n@@ -462,1 +462,1 @@\n-      assert(op->as_OpReturn() != NULL, \"must be\");\n+      assert(op->as_OpReturn() != nullptr, \"must be\");\n@@ -468,1 +468,1 @@\n-      if (op_ret->stub() != NULL)      do_stub(op_ret->stub());\n+      if (op_ret->stub() != nullptr)      do_stub(op_ret->stub());\n@@ -475,1 +475,1 @@\n-      assert(op->as_Op1() != NULL, \"must be\");\n+      assert(op->as_Op1() != nullptr, \"must be\");\n@@ -478,1 +478,1 @@\n-      assert(op1->_info != NULL, \"\");  do_info(op1->_info);\n+      assert(op1->_info != nullptr, \"\");  do_info(op1->_info);\n@@ -488,1 +488,1 @@\n-      assert(op->as_OpConvert() != NULL, \"must be\");\n+      assert(op->as_OpConvert() != nullptr, \"must be\");\n@@ -491,1 +491,1 @@\n-      assert(opConvert->_info == NULL, \"must be\");\n+      assert(opConvert->_info == nullptr, \"must be\");\n@@ -503,1 +503,1 @@\n-      assert(op->as_OpBranch() != NULL, \"must be\");\n+      assert(op->as_OpBranch() != nullptr, \"must be\");\n@@ -513,1 +513,1 @@\n-      if (opBranch->_info != NULL)     do_info(opBranch->_info);\n+      if (opBranch->_info != nullptr)  do_info(opBranch->_info);\n@@ -515,1 +515,1 @@\n-      if (opBranch->_stub != NULL)     opBranch->stub()->visit(this);\n+      if (opBranch->_stub != nullptr)  opBranch->stub()->visit(this);\n@@ -524,1 +524,1 @@\n-      assert(op->as_OpAllocObj() != NULL, \"must be\");\n+      assert(op->as_OpAllocObj() != nullptr, \"must be\");\n@@ -543,1 +543,1 @@\n-      assert(op->as_OpRoundFP() != NULL, \"must be\");\n+      assert(op->as_OpRoundFP() != nullptr, \"must be\");\n@@ -546,1 +546,1 @@\n-      assert(op->_info == NULL, \"info not used by this instruction\");\n+      assert(op->_info == nullptr, \"info not used by this instruction\");\n@@ -578,1 +578,1 @@\n-      assert(op->as_Op2() != NULL, \"must be\");\n+      assert(op->as_Op2() != nullptr, \"must be\");\n@@ -603,1 +603,1 @@\n-      assert(op->as_Op4() != NULL, \"must be\");\n+      assert(op->as_Op4() != nullptr, \"must be\");\n@@ -606,1 +606,1 @@\n-      assert(op4->_info == NULL && op4->_tmp1->is_illegal() && op4->_tmp2->is_illegal() &&\n+      assert(op4->_info == nullptr && op4->_tmp1->is_illegal() && op4->_tmp2->is_illegal() &&\n@@ -626,1 +626,1 @@\n-      assert(op->as_Op2() != NULL, \"must be\");\n+      assert(op->as_Op2() != nullptr, \"must be\");\n@@ -629,1 +629,1 @@\n-      assert(op2->_info == NULL, \"not used\");\n+      assert(op2->_info == nullptr, \"not used\");\n@@ -645,1 +645,1 @@\n-      assert(op->as_Op2() != NULL, \"must be\");\n+      assert(op->as_Op2() != nullptr, \"must be\");\n@@ -659,1 +659,1 @@\n-      assert(op->as_Op1() != NULL, \"must be\");\n+      assert(op->as_Op1() != nullptr, \"must be\");\n@@ -662,1 +662,1 @@\n-      assert(op1->_info == NULL, \"no info\");\n+      assert(op1->_info == nullptr, \"no info\");\n@@ -672,1 +672,1 @@\n-      assert(op->as_Op3() != NULL, \"must be\");\n+      assert(op->as_Op3() != nullptr, \"must be\");\n@@ -691,1 +691,1 @@\n-      assert(op->as_Op3() != NULL, \"must be\");\n+      assert(op->as_Op3() != nullptr, \"must be\");\n@@ -693,1 +693,1 @@\n-      assert(op3->_info == NULL, \"no info\");\n+      assert(op3->_info == nullptr, \"no info\");\n@@ -707,1 +707,1 @@\n-      assert(opJavaCall != NULL, \"must be\");\n+      assert(opJavaCall != nullptr, \"must be\");\n@@ -734,1 +734,1 @@\n-      assert(op->as_OpRTCall() != NULL, \"must be\");\n+      assert(op->as_OpRTCall() != nullptr, \"must be\");\n@@ -755,1 +755,1 @@\n-      assert(op->as_OpArrayCopy() != NULL, \"must be\");\n+      assert(op->as_OpArrayCopy() != nullptr, \"must be\");\n@@ -776,1 +776,1 @@\n-      assert(op->as_OpUpdateCRC32() != NULL, \"must be\");\n+      assert(op->as_OpUpdateCRC32() != nullptr, \"must be\");\n@@ -782,1 +782,1 @@\n-      assert(opUp->_info == NULL, \"no info for LIR_OpUpdateCRC32\");\n+      assert(opUp->_info == nullptr, \"no info for LIR_OpUpdateCRC32\");\n@@ -791,1 +791,1 @@\n-      assert(op->as_OpLock() != NULL, \"must be\");\n+      assert(op->as_OpLock() != nullptr, \"must be\");\n@@ -813,1 +813,1 @@\n-      assert(op->as_OpDelay() != NULL, \"must be\");\n+      assert(op->as_OpDelay() != nullptr, \"must be\");\n@@ -824,1 +824,1 @@\n-      assert(op->as_OpTypeCheck() != NULL, \"must be\");\n+      assert(op->as_OpTypeCheck() != nullptr, \"must be\");\n@@ -846,1 +846,1 @@\n-      assert(op->as_OpCompareAndSwap() != NULL, \"must be\");\n+      assert(op->as_OpCompareAndSwap() != nullptr, \"must be\");\n@@ -866,1 +866,1 @@\n-      assert(op->as_OpAllocArray() != NULL, \"must be\");\n+      assert(op->as_OpAllocArray() != nullptr, \"must be\");\n@@ -889,1 +889,1 @@\n-      assert(opLoadKlass != NULL, \"must be\");\n+      assert(opLoadKlass != nullptr, \"must be\");\n@@ -901,1 +901,1 @@\n-      assert(op->as_OpProfileCall() != NULL, \"must be\");\n+      assert(op->as_OpProfileCall() != nullptr, \"must be\");\n@@ -912,1 +912,1 @@\n-      assert(op->as_OpProfileType() != NULL, \"must be\");\n+      assert(op->as_OpProfileType() != nullptr, \"must be\");\n@@ -930,1 +930,1 @@\n-  if (stub != NULL) {\n+  if (stub != nullptr) {\n@@ -936,1 +936,1 @@\n-  XHandlers* result = NULL;\n+  XHandlers* result = nullptr;\n@@ -940,1 +940,1 @@\n-    if (info_at(i)->exception_handlers() != NULL) {\n+    if (info_at(i)->exception_handlers() != nullptr) {\n@@ -948,1 +948,1 @@\n-    assert(info_at(i)->exception_handlers() == NULL ||\n+    assert(info_at(i)->exception_handlers() == nullptr ||\n@@ -954,1 +954,1 @@\n-  if (result != NULL) {\n+  if (result != nullptr) {\n@@ -979,2 +979,2 @@\n-    LIR_Op1(lir_return, opr, (CodeEmitInfo*)NULL \/* info *\/),\n-    _stub(NULL) {\n+    LIR_Op1(lir_return, opr, (CodeEmitInfo*)nullptr \/* info *\/),\n+    _stub(nullptr) {\n@@ -1032,1 +1032,1 @@\n-  if (stub() != NULL) {\n+  if (stub() != nullptr) {\n@@ -1105,1 +1105,1 @@\n-  , _file(NULL)\n+  , _file(nullptr)\n@@ -1118,2 +1118,2 @@\n-  if (f == NULL) f = strrchr(file, '\\\\');\n-  if (f == NULL) {\n+  if (f == nullptr) f = strrchr(file, '\\\\');\n+  if (f == nullptr) {\n@@ -1150,0 +1150,6 @@\n+    case lir_cas_long:\n+    case lir_cas_obj:\n+    case lir_cas_int:\n+      _cmp_opr1 = op->as_OpCompareAndSwap()->result_opr();\n+      _cmp_opr2 = LIR_OprFact::intConst(0);\n+      break;\n@@ -1151,1 +1157,1 @@\n-    case lir_zloadbarrier_test:\n+    case lir_xloadbarrier_test:\n@@ -1168,1 +1174,1 @@\n-    _operations.at_grow(n + buffer->number_of_ops() - 1, NULL);\n+    _operations.at_grow(n + buffer->number_of_ops() - 1, nullptr);\n@@ -1433,1 +1439,1 @@\n-                    NULL));\n+                    nullptr));\n@@ -1450,1 +1456,1 @@\n-  if (profiled_method != NULL) {\n+  if (profiled_method != nullptr) {\n@@ -1459,2 +1465,2 @@\n-  LIR_OpTypeCheck* c = new LIR_OpTypeCheck(lir_instanceof, result, object, klass, tmp1, tmp2, tmp3, fast_check, NULL, info_for_patch, NULL);\n-  if (profiled_method != NULL) {\n+  LIR_OpTypeCheck* c = new LIR_OpTypeCheck(lir_instanceof, result, object, klass, tmp1, tmp2, tmp3, fast_check, nullptr, info_for_patch, nullptr);\n+  if (profiled_method != nullptr) {\n@@ -1472,1 +1478,1 @@\n-  if (profiled_method != NULL) {\n+  if (profiled_method != nullptr) {\n@@ -1484,1 +1490,1 @@\n-    cmp(lir_cond_equal, opr, LIR_OprFact::oopConst(NULL));\n+    cmp(lir_cond_equal, opr, LIR_OprFact::oopConst(nullptr));\n@@ -1625,1 +1631,1 @@\n-  tty->print(\"[%d, %d] \", x->bci(), (end == NULL ? -1 : end->printable_bci()));\n+  tty->print(\"[%d, %d] \", x->bci(), (end == nullptr ? -1 : end->printable_bci()));\n@@ -1635,1 +1641,1 @@\n-  if (end != NULL && x->number_of_sux() > 0) {\n+  if (end != nullptr && x->number_of_sux() > 0) {\n@@ -1681,1 +1687,1 @@\n-  if (info() != NULL) out->print(\" [bci:%d]\", info()->stack()->bci());\n+  if (info() != nullptr) out->print(\" [bci:%d]\", info()->stack()->bci());\n@@ -1683,1 +1689,1 @@\n-  if (Verbose && _file != NULL) {\n+  if (Verbose && _file != nullptr) {\n@@ -1690,1 +1696,1 @@\n-  const char* s = NULL;\n+  const char* s = nullptr;\n@@ -1893,1 +1899,1 @@\n-  if (block() != NULL) {\n+  if (block() != nullptr) {\n@@ -1895,1 +1901,1 @@\n-  } else if (stub() != NULL) {\n+  } else if (stub() != nullptr) {\n@@ -1899,1 +1905,1 @@\n-    if (stub()->info() != NULL) out->print(\" [bci:%d]\", stub()->info()->stack()->bci());\n+    if (stub()->info() != nullptr) out->print(\" [bci:%d]\", stub()->info()->stack()->bci());\n@@ -1903,1 +1909,1 @@\n-  if (ublock() != NULL) {\n+  if (ublock() != nullptr) {\n@@ -2012,1 +2018,1 @@\n-  if (info_for_exception() != NULL) out->print(\" [bci:%d]\", info_for_exception()->stack()->bci());\n+  if (info_for_exception() != nullptr) out->print(\" [bci:%d]\", info_for_exception()->stack()->bci());\n@@ -2081,1 +2087,1 @@\n-  if  (exact_klass() == NULL) {\n+  if (exact_klass() == nullptr) {\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.cpp","additions":104,"deletions":98,"binary":false,"changes":202,"status":"modified"},{"patch":"@@ -71,2 +71,2 @@\n-  virtual LIR_Const*  as_constant()              { return NULL; }\n-  virtual LIR_Address* as_address()              { return NULL; }\n+  virtual LIR_Const*  as_constant()              { return nullptr; }\n+  virtual LIR_Address* as_address()              { return nullptr; }\n@@ -387,2 +387,2 @@\n-  bool is_constant() const     { return is_pointer() && pointer()->as_constant() != NULL; }\n-  bool is_address() const      { return is_pointer() && pointer()->as_address() != NULL; }\n+  bool is_constant() const     { return is_pointer() && pointer()->as_constant() != nullptr; }\n+  bool is_address() const      { return is_pointer() && pointer()->as_address() != nullptr; }\n@@ -1021,3 +1021,3 @@\n-  , begin_opZLoadBarrierTest\n-    , lir_zloadbarrier_test\n-  , end_opZLoadBarrierTest\n+  , begin_opXLoadBarrierTest\n+    , lir_xloadbarrier_test\n+  , end_opXLoadBarrierTest\n@@ -1088,1 +1088,1 @@\n-      _file(NULL)\n+      _file(nullptr)\n@@ -1094,1 +1094,1 @@\n-    , _info(NULL)\n+    , _info(nullptr)\n@@ -1097,1 +1097,1 @@\n-    , _source(NULL) {}\n+    , _source(nullptr) {}\n@@ -1102,1 +1102,1 @@\n-      _file(NULL)\n+      _file(nullptr)\n@@ -1111,1 +1111,1 @@\n-    , _source(NULL) {}\n+    , _source(nullptr) {}\n@@ -1144,24 +1144,24 @@\n-  virtual LIR_OpCall* as_OpCall() { return NULL; }\n-  virtual LIR_OpJavaCall* as_OpJavaCall() { return NULL; }\n-  virtual LIR_OpLabel* as_OpLabel() { return NULL; }\n-  virtual LIR_OpDelay* as_OpDelay() { return NULL; }\n-  virtual LIR_OpLock* as_OpLock() { return NULL; }\n-  virtual LIR_OpAllocArray* as_OpAllocArray() { return NULL; }\n-  virtual LIR_OpAllocObj* as_OpAllocObj() { return NULL; }\n-  virtual LIR_OpRoundFP* as_OpRoundFP() { return NULL; }\n-  virtual LIR_OpBranch* as_OpBranch() { return NULL; }\n-  virtual LIR_OpReturn* as_OpReturn() { return NULL; }\n-  virtual LIR_OpRTCall* as_OpRTCall() { return NULL; }\n-  virtual LIR_OpConvert* as_OpConvert() { return NULL; }\n-  virtual LIR_Op0* as_Op0() { return NULL; }\n-  virtual LIR_Op1* as_Op1() { return NULL; }\n-  virtual LIR_Op2* as_Op2() { return NULL; }\n-  virtual LIR_Op3* as_Op3() { return NULL; }\n-  virtual LIR_Op4* as_Op4() { return NULL; }\n-  virtual LIR_OpArrayCopy* as_OpArrayCopy() { return NULL; }\n-  virtual LIR_OpUpdateCRC32* as_OpUpdateCRC32() { return NULL; }\n-  virtual LIR_OpTypeCheck* as_OpTypeCheck() { return NULL; }\n-  virtual LIR_OpCompareAndSwap* as_OpCompareAndSwap() { return NULL; }\n-  virtual LIR_OpLoadKlass* as_OpLoadKlass() { return NULL; }\n-  virtual LIR_OpProfileCall* as_OpProfileCall() { return NULL; }\n-  virtual LIR_OpProfileType* as_OpProfileType() { return NULL; }\n+  virtual LIR_OpCall* as_OpCall() { return nullptr; }\n+  virtual LIR_OpJavaCall* as_OpJavaCall() { return nullptr; }\n+  virtual LIR_OpLabel* as_OpLabel() { return nullptr; }\n+  virtual LIR_OpDelay* as_OpDelay() { return nullptr; }\n+  virtual LIR_OpLock* as_OpLock() { return nullptr; }\n+  virtual LIR_OpAllocArray* as_OpAllocArray() { return nullptr; }\n+  virtual LIR_OpAllocObj* as_OpAllocObj() { return nullptr; }\n+  virtual LIR_OpRoundFP* as_OpRoundFP() { return nullptr; }\n+  virtual LIR_OpBranch* as_OpBranch() { return nullptr; }\n+  virtual LIR_OpReturn* as_OpReturn() { return nullptr; }\n+  virtual LIR_OpRTCall* as_OpRTCall() { return nullptr; }\n+  virtual LIR_OpConvert* as_OpConvert() { return nullptr; }\n+  virtual LIR_Op0* as_Op0() { return nullptr; }\n+  virtual LIR_Op1* as_Op1() { return nullptr; }\n+  virtual LIR_Op2* as_Op2() { return nullptr; }\n+  virtual LIR_Op3* as_Op3() { return nullptr; }\n+  virtual LIR_Op4* as_Op4() { return nullptr; }\n+  virtual LIR_OpArrayCopy* as_OpArrayCopy() { return nullptr; }\n+  virtual LIR_OpUpdateCRC32* as_OpUpdateCRC32() { return nullptr; }\n+  virtual LIR_OpTypeCheck* as_OpTypeCheck() { return nullptr; }\n+  virtual LIR_OpCompareAndSwap* as_OpCompareAndSwap() { return nullptr; }\n+  virtual LIR_OpLoadKlass* as_OpLoadKlass() { return nullptr; }\n+  virtual LIR_OpProfileCall* as_OpProfileCall() { return nullptr; }\n+  virtual LIR_OpProfileType* as_OpProfileType() { return nullptr; }\n@@ -1169,1 +1169,1 @@\n-  virtual LIR_OpAssert* as_OpAssert() { return NULL; }\n+  virtual LIR_OpAssert* as_OpAssert() { return nullptr; }\n@@ -1184,1 +1184,1 @@\n-             LIR_OprList* arguments, CodeEmitInfo* info = NULL)\n+             LIR_OprList* arguments, CodeEmitInfo* info = nullptr)\n@@ -1253,1 +1253,1 @@\n-   : LIR_Op(lir_label, LIR_OprFact::illegalOpr, NULL)\n+   : LIR_Op(lir_label, LIR_OprFact::illegalOpr, nullptr)\n@@ -1340,2 +1340,2 @@\n-   : LIR_Op(code, LIR_OprFact::illegalOpr, NULL)  { assert(is_in_range(code, begin_op0, end_op0), \"code check\"); }\n-  LIR_Op0(LIR_Code code, LIR_Opr result, CodeEmitInfo* info = NULL)\n+   : LIR_Op(code, LIR_OprFact::illegalOpr, nullptr)  { assert(is_in_range(code, begin_op0, end_op0), \"code check\"); }\n+  LIR_Op0(LIR_Code code, LIR_Opr result, CodeEmitInfo* info = nullptr)\n@@ -1370,1 +1370,1 @@\n-  LIR_Op1(LIR_Code code, LIR_Opr opr, LIR_Opr result = LIR_OprFact::illegalOpr, BasicType type = T_ILLEGAL, LIR_PatchCode patch = lir_patch_none, CodeEmitInfo* info = NULL)\n+  LIR_Op1(LIR_Code code, LIR_Opr opr, LIR_Opr result = LIR_OprFact::illegalOpr, BasicType type = T_ILLEGAL, LIR_PatchCode patch = lir_patch_none, CodeEmitInfo* info = nullptr)\n@@ -1420,1 +1420,1 @@\n-               LIR_Opr result, LIR_OprList* arguments, CodeEmitInfo* info = NULL)\n+               LIR_Opr result, LIR_OprList* arguments, CodeEmitInfo* info = nullptr)\n@@ -1581,1 +1581,1 @@\n-  virtual bool is_patching() { return _info_for_patch != NULL; }\n+  virtual bool is_patching() { return _info_for_patch != nullptr; }\n@@ -1607,1 +1607,1 @@\n-  LIR_Op2(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, CodeEmitInfo* info = NULL, BasicType type = T_ILLEGAL)\n+  LIR_Op2(LIR_Code code, LIR_Condition condition, LIR_Opr opr1, LIR_Opr opr2, CodeEmitInfo* info = nullptr, BasicType type = T_ILLEGAL)\n@@ -1623,1 +1623,1 @@\n-    : LIR_Op(code, result, NULL)\n+    : LIR_Op(code, result, nullptr)\n@@ -1639,1 +1639,1 @@\n-          CodeEmitInfo* info = NULL, BasicType type = T_ILLEGAL)\n+          CodeEmitInfo* info = nullptr, BasicType type = T_ILLEGAL)\n@@ -1656,1 +1656,1 @@\n-    : LIR_Op(code, result, NULL)\n+    : LIR_Op(code, result, nullptr)\n@@ -1707,1 +1707,1 @@\n-    : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*) NULL)\n+    : LIR_Op2(lir_branch, cond, LIR_OprFact::illegalOpr, LIR_OprFact::illegalOpr, (CodeEmitInfo*) nullptr)\n@@ -1709,3 +1709,3 @@\n-    , _block(NULL)\n-    , _ublock(NULL)\n-    , _stub(NULL) { }\n+    , _block(nullptr)\n+    , _ublock(nullptr)\n+    , _stub(nullptr) { }\n@@ -1756,1 +1756,1 @@\n-    : LIR_Op(lir_alloc_array, result, NULL)\n+    : LIR_Op(lir_alloc_array, result, nullptr)\n@@ -1790,1 +1790,1 @@\n-  LIR_Op3(LIR_Code code, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr opr3, LIR_Opr result, CodeEmitInfo* info = NULL)\n+  LIR_Op3(LIR_Code code, LIR_Opr opr1, LIR_Opr opr2, LIR_Opr opr3, LIR_Opr result, CodeEmitInfo* info = nullptr)\n@@ -1822,1 +1822,1 @@\n-    : LIR_Op(code, result, NULL)\n+    : LIR_Op(code, result, nullptr)\n@@ -1979,1 +1979,1 @@\n-    : LIR_Op(code, result, NULL)  \/\/ no result, no info\n+    : LIR_Op(code, result, nullptr)  \/\/ no result, no info\n@@ -2013,1 +2013,1 @@\n-    : LIR_Op(lir_profile_call, LIR_OprFact::illegalOpr, NULL)  \/\/ no result, no info\n+    : LIR_Op(lir_profile_call, LIR_OprFact::illegalOpr, nullptr)  \/\/ no result, no info\n@@ -2049,1 +2049,1 @@\n-  ciKlass*     _exact_klass;   \/\/ non NULL if we know the klass statically (no need to load it from _obj)\n+  ciKlass*     _exact_klass;   \/\/ non null if we know the klass statically (no need to load it from _obj)\n@@ -2052,1 +2052,1 @@\n-  bool         _no_conflict;   \/\/ true if we're profling parameters, _exact_klass is not NULL and we know\n+  bool         _no_conflict;   \/\/ true if we're profling parameters, _exact_klass is not null and we know\n@@ -2058,1 +2058,1 @@\n-    : LIR_Op(lir_profile_type, LIR_OprFact::illegalOpr, NULL)  \/\/ no result, no info\n+    : LIR_Op(lir_profile_type, LIR_OprFact::illegalOpr, nullptr)  \/\/ no result, no info\n@@ -2109,1 +2109,1 @@\n-    if (op->source() == NULL)\n+    if (op->source() == nullptr)\n@@ -2129,1 +2129,1 @@\n-    _file = NULL;\n+    _file = nullptr;\n@@ -2134,1 +2134,1 @@\n-  LIR_List(Compilation* compilation, BlockBegin* block = NULL);\n+  LIR_List(Compilation* compilation, BlockBegin* block = nullptr);\n@@ -2200,1 +2200,1 @@\n-  void leal(LIR_Opr from, LIR_Opr result_reg, LIR_PatchCode patch_code = lir_patch_none, CodeEmitInfo* info = NULL) { append(new LIR_Op1(lir_leal, from, result_reg, T_ILLEGAL, patch_code, info)); }\n+  void leal(LIR_Opr from, LIR_Opr result_reg, LIR_PatchCode patch_code = lir_patch_none, CodeEmitInfo* info = nullptr) { append(new LIR_Op1(lir_leal, from, result_reg, T_ILLEGAL, patch_code, info)); }\n@@ -2205,4 +2205,4 @@\n-  void move(LIR_Opr src, LIR_Opr dst, CodeEmitInfo* info = NULL) { append(new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, info)); }\n-  void move(LIR_Address* src, LIR_Opr dst, CodeEmitInfo* info = NULL) { append(new LIR_Op1(lir_move, LIR_OprFact::address(src), dst, src->type(), lir_patch_none, info)); }\n-  void move(LIR_Opr src, LIR_Address* dst, CodeEmitInfo* info = NULL) { append(new LIR_Op1(lir_move, src, LIR_OprFact::address(dst), dst->type(), lir_patch_none, info)); }\n-  void move_wide(LIR_Address* src, LIR_Opr dst, CodeEmitInfo* info = NULL) {\n+  void move(LIR_Opr src, LIR_Opr dst, CodeEmitInfo* info = nullptr) { append(new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, info)); }\n+  void move(LIR_Address* src, LIR_Opr dst, CodeEmitInfo* info = nullptr) { append(new LIR_Op1(lir_move, LIR_OprFact::address(src), dst, src->type(), lir_patch_none, info)); }\n+  void move(LIR_Opr src, LIR_Address* dst, CodeEmitInfo* info = nullptr) { append(new LIR_Op1(lir_move, src, LIR_OprFact::address(dst), dst->type(), lir_patch_none, info)); }\n+  void move_wide(LIR_Address* src, LIR_Opr dst, CodeEmitInfo* info = nullptr) {\n@@ -2215,1 +2215,1 @@\n-  void move_wide(LIR_Opr src, LIR_Address* dst, CodeEmitInfo* info = NULL) {\n+  void move_wide(LIR_Opr src, LIR_Address* dst, CodeEmitInfo* info = nullptr) {\n@@ -2222,1 +2222,1 @@\n-  void volatile_move(LIR_Opr src, LIR_Opr dst, BasicType type, CodeEmitInfo* info = NULL, LIR_PatchCode patch_code = lir_patch_none) { append(new LIR_Op1(lir_move, src, dst, type, patch_code, info, lir_move_volatile)); }\n+  void volatile_move(LIR_Opr src, LIR_Opr dst, BasicType type, CodeEmitInfo* info = nullptr, LIR_PatchCode patch_code = lir_patch_none) { append(new LIR_Op1(lir_move, src, dst, type, patch_code, info, lir_move_volatile)); }\n@@ -2233,1 +2233,1 @@\n-  void convert(Bytecodes::Code code, LIR_Opr left, LIR_Opr dst, ConversionStub* stub = NULL\/*, bool is_32bit = false*\/) { append(new LIR_OpConvert(code, left, dst, stub)); }\n+  void convert(Bytecodes::Code code, LIR_Opr left, LIR_Opr dst, ConversionStub* stub = nullptr\/*, bool is_32bit = false*\/) { append(new LIR_OpConvert(code, left, dst, stub)); }\n@@ -2250,1 +2250,1 @@\n-  void cmp(LIR_Condition condition, LIR_Opr left, LIR_Opr right, CodeEmitInfo* info = NULL) {\n+  void cmp(LIR_Condition condition, LIR_Opr left, LIR_Opr right, CodeEmitInfo* info = nullptr) {\n@@ -2253,1 +2253,1 @@\n-  void cmp(LIR_Condition condition, LIR_Opr left, int right, CodeEmitInfo* info = NULL) {\n+  void cmp(LIR_Condition condition, LIR_Opr left, int right, CodeEmitInfo* info = nullptr) {\n@@ -2283,1 +2283,1 @@\n-  void sub (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = NULL) { append(new LIR_Op2(lir_sub, left, right, res, info)); }\n+  void sub (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = nullptr) { append(new LIR_Op2(lir_sub, left, right, res, info)); }\n@@ -2286,1 +2286,1 @@\n-  void div (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = NULL)      { append(new LIR_Op2(lir_div, left, right, res, info)); }\n+  void div (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = nullptr)      { append(new LIR_Op2(lir_div, left, right, res, info)); }\n@@ -2288,1 +2288,1 @@\n-  void rem (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = NULL)      { append(new LIR_Op2(lir_rem, left, right, res, info)); }\n+  void rem (LIR_Opr left, LIR_Opr right, LIR_Opr res, CodeEmitInfo* info = nullptr)      { append(new LIR_Op2(lir_rem, left, right, res, info)); }\n@@ -2293,1 +2293,1 @@\n-  void load(LIR_Address* addr, LIR_Opr src, CodeEmitInfo* info = NULL, LIR_PatchCode patch_code = lir_patch_none);\n+  void load(LIR_Address* addr, LIR_Opr src, CodeEmitInfo* info = nullptr, LIR_PatchCode patch_code = lir_patch_none);\n@@ -2297,1 +2297,1 @@\n-  void store(LIR_Opr src, LIR_Address* addr, CodeEmitInfo* info = NULL, LIR_PatchCode patch_code = lir_patch_none);\n+  void store(LIR_Opr src, LIR_Address* addr, CodeEmitInfo* info = nullptr, LIR_PatchCode patch_code = lir_patch_none);\n@@ -2391,1 +2391,1 @@\n-  LIR_List*   _lir;   \/\/ the lir list where ops of this buffer should be inserted later (NULL when uninitialized)\n+  LIR_List*   _lir;   \/\/ the lir list where ops of this buffer should be inserted later (null when uninitialized)\n@@ -2409,1 +2409,1 @@\n-  LIR_InsertionBuffer() : _lir(NULL), _index_and_count(8), _ops(8) { }\n+  LIR_InsertionBuffer() : _lir(nullptr), _index_and_count(8), _ops(8) { }\n@@ -2413,1 +2413,1 @@\n-  bool initialized() const  { return _lir != NULL; }\n+  bool initialized() const  { return _lir != nullptr; }\n@@ -2415,1 +2415,1 @@\n-  void finish()             { _lir = NULL; }\n+  void finish()             { _lir = nullptr; }\n@@ -2430,1 +2430,1 @@\n-  void move(int index, LIR_Opr src, LIR_Opr dst, CodeEmitInfo* info = NULL) { append(index, new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, info)); }\n+  void move(int index, LIR_Opr src, LIR_Opr dst, CodeEmitInfo* info = nullptr) { append(index, new LIR_Op1(lir_move, src, dst, dst->type(), lir_patch_none, info)); }\n@@ -2479,1 +2479,1 @@\n-      if (address != NULL) {\n+      if (address != nullptr) {\n@@ -2507,1 +2507,1 @@\n-    assert(info != NULL, \"should not call this otherwise\");\n+    assert(info != nullptr, \"should not call this otherwise\");\n@@ -2522,1 +2522,1 @@\n-    _op = NULL;\n+    _op = nullptr;\n","filename":"src\/hotspot\/share\/c1\/c1_LIR.hpp","additions":86,"deletions":86,"binary":false,"changes":172,"status":"modified"},{"patch":"@@ -111,1 +111,1 @@\n-\/\/ Call graph: move(NULL, c) -> move(c, b) -> move(b, a)\n+\/\/ Call graph: move(null, c) -> move(c, b) -> move(b, a)\n@@ -114,1 +114,1 @@\n-\/\/ Call graph: move(NULL, a) -> move(a, b) -> move(b, a)\n+\/\/ Call graph: move(null, a) -> move(a, b) -> move(b, a)\n@@ -124,1 +124,1 @@\n-    assert(_loop == NULL, \"only one loop valid!\");\n+    assert(_loop == nullptr, \"only one loop valid!\");\n@@ -134,1 +134,1 @@\n-    } else if (src != NULL) {\n+    } else if (src != nullptr) {\n@@ -148,2 +148,2 @@\n-      _loop = NULL;\n-      move(NULL, node);\n+      _loop = nullptr;\n+      move(nullptr, node);\n@@ -169,3 +169,3 @@\n-    node = vreg_table().at_grow(vreg_num, NULL);\n-    assert(node == NULL || node->operand() == opr, \"\");\n-    if (node == NULL) {\n+    node = vreg_table().at_grow(vreg_num, nullptr);\n+    assert(node == nullptr || node->operand() == opr, \"\");\n+    if (node == nullptr) {\n@@ -207,1 +207,1 @@\n-    _gen->_instruction_for_operand.at_put_grow(opr->vreg_number(), value(), NULL);\n+    _gen->_instruction_for_operand.at_put_grow(opr->vreg_number(), value(), nullptr);\n@@ -262,1 +262,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -267,2 +267,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_IntConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_IntConstant() != nullptr, \"type check\");\n@@ -274,2 +274,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_AddressConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_AddressConstant() != nullptr, \"type check\");\n@@ -281,2 +281,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_FloatConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_FloatConstant() != nullptr, \"type check\");\n@@ -288,2 +288,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_DoubleConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_DoubleConstant() != nullptr, \"type check\");\n@@ -295,2 +295,2 @@\n-  assert(is_constant() && value() != NULL, \"\");\n-  assert(type()->as_LongConstant() != NULL, \"type check\");\n+  assert(is_constant() && value() != nullptr, \"\");\n+  assert(type()->as_LongConstant() != nullptr, \"type check\");\n@@ -313,1 +313,1 @@\n-  assert(block->lir() == NULL, \"LIR list already computed for this block\");\n+  assert(block->lir() == nullptr, \"LIR list already computed for this block\");\n@@ -354,1 +354,1 @@\n-  for (Instruction* instr = block; instr != NULL; instr = instr->next()) {\n+  for (Instruction* instr = block; instr != nullptr; instr = instr->next()) {\n@@ -358,1 +358,1 @@\n-  set_block(NULL);\n+  set_block(nullptr);\n@@ -377,1 +377,1 @@\n-         instr->as_Constant() != NULL || bailed_out(), \"invalid item set\");\n+         instr->as_Constant() != nullptr || bailed_out(), \"invalid item set\");\n@@ -385,2 +385,2 @@\n-  if ((instr->is_pinned() && instr->as_Phi() == NULL) || instr->operand()->is_valid()) {\n-    assert(instr->operand() != LIR_OprFact::illegalOpr || instr->as_Constant() != NULL, \"this root has not yet been visited\");\n+  if ((instr->is_pinned() && instr->as_Phi() == nullptr) || instr->operand()->is_valid()) {\n+    assert(instr->operand() != LIR_OprFact::illegalOpr || instr->as_Constant() != nullptr, \"this root has not yet been visited\");\n@@ -390,1 +390,1 @@\n-    \/\/ assert(instr->use_count() > 0 || instr->as_Phi() != NULL, \"leaf instruction must have a use\");\n+    \/\/ assert(instr->use_count() > 0 || instr->as_Phi() != nullptr, \"leaf instruction must have a use\");\n@@ -396,1 +396,1 @@\n-  assert(state != NULL, \"state must be defined\");\n+  assert(state != nullptr, \"state must be defined\");\n@@ -413,1 +413,1 @@\n-      if (!value->is_pinned() && value->as_Constant() == NULL && value->as_Local() == NULL) {\n+      if (!value->is_pinned() && value->as_Constant() == nullptr && value->as_Local() == nullptr) {\n@@ -440,1 +440,1 @@\n-          if (!value->is_pinned() && value->as_Constant() == NULL && value->as_Local() == NULL) {\n+          if (!value->is_pinned() && value->as_Constant() == nullptr && value->as_Local() == nullptr) {\n@@ -445,1 +445,1 @@\n-          \/\/ NULL out this local so that linear scan can assume that all non-NULL values are live.\n+          \/\/ null out this local so that linear scan can assume that all non-null values are live.\n@@ -452,1 +452,1 @@\n-  return new CodeEmitInfo(state, ignore_xhandler ? NULL : x->exception_handlers(), x->check_flag(Instruction::DeoptimizeOnException));\n+  return new CodeEmitInfo(state, ignore_xhandler ? nullptr : x->exception_handlers(), x->check_flag(Instruction::DeoptimizeOnException));\n@@ -466,2 +466,2 @@\n-    assert(info != NULL, \"info must be set if class is not loaded\");\n-    __ klass2reg_patch(NULL, r, info);\n+    assert(info != nullptr, \"info must be set if class is not loaded\");\n+    __ klass2reg_patch(nullptr, r, info);\n@@ -682,1 +682,1 @@\n-  if (type != NULL && type->is_array_klass() && type->is_loaded()) {\n+  if (type != nullptr && type->is_array_klass() && type->is_loaded()) {\n@@ -685,1 +685,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -691,2 +691,2 @@\n-  if (t == NULL) {\n-    return NULL;\n+  if (t == nullptr) {\n+    return nullptr;\n@@ -696,1 +696,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -710,1 +710,1 @@\n-  ciArrayKlass* expected_type = NULL;\n+  ciArrayKlass* expected_type = nullptr;\n@@ -716,1 +716,1 @@\n-    if (src_declared_type == NULL && (phi = src->as_Phi()) != NULL) {\n+    if (src_declared_type == nullptr && (phi = src->as_Phi()) != nullptr) {\n@@ -721,1 +721,1 @@\n-    if (dst_declared_type == NULL && (phi = dst->as_Phi()) != NULL) {\n+    if (dst_declared_type == nullptr && (phi = dst->as_Phi()) != nullptr) {\n@@ -725,1 +725,1 @@\n-    if (src_exact_type != NULL && src_exact_type == dst_exact_type) {\n+    if (src_exact_type != nullptr && src_exact_type == dst_exact_type) {\n@@ -729,1 +729,1 @@\n-    } else if (dst_exact_type != NULL && dst_exact_type->is_obj_array_klass()) {\n+    } else if (dst_exact_type != nullptr && dst_exact_type->is_obj_array_klass()) {\n@@ -731,2 +731,2 @@\n-      ciArrayKlass* src_type = NULL;\n-      if (src_exact_type != NULL && src_exact_type->is_obj_array_klass()) {\n+      ciArrayKlass* src_type = nullptr;\n+      if (src_exact_type != nullptr && src_exact_type->is_obj_array_klass()) {\n@@ -734,1 +734,1 @@\n-      } else if (src_declared_type != NULL && src_declared_type->is_obj_array_klass()) {\n+      } else if (src_declared_type != nullptr && src_declared_type->is_obj_array_klass()) {\n@@ -737,1 +737,1 @@\n-      if (src_type != NULL) {\n+      if (src_type != nullptr) {\n@@ -745,3 +745,3 @@\n-    if (expected_type == NULL) expected_type = dst_exact_type;\n-    if (expected_type == NULL) expected_type = src_declared_type;\n-    if (expected_type == NULL) expected_type = dst_declared_type;\n+    if (expected_type == nullptr) expected_type = dst_exact_type;\n+    if (expected_type == nullptr) expected_type = src_declared_type;\n+    if (expected_type == nullptr) expected_type = dst_declared_type;\n@@ -768,2 +768,2 @@\n-  if (expected_type != NULL) {\n-    Value length_limit = NULL;\n+  if (expected_type != nullptr) {\n+    Value length_limit = nullptr;\n@@ -772,1 +772,1 @@\n-    if (ifop != NULL) {\n+    if (ifop != nullptr) {\n@@ -784,1 +784,1 @@\n-    if (src_array != NULL) {\n+    if (src_array != nullptr) {\n@@ -786,1 +786,1 @@\n-      if (length_limit != NULL &&\n+      if (length_limit != nullptr &&\n@@ -794,1 +794,1 @@\n-    if (dst_array != NULL) {\n+    if (dst_array != nullptr) {\n@@ -796,1 +796,1 @@\n-      if (length_limit != NULL &&\n+      if (length_limit != nullptr &&\n@@ -814,1 +814,1 @@\n-    if (al != NULL) {\n+    if (al != nullptr) {\n@@ -843,1 +843,1 @@\n-    if (expected_type != NULL) {\n+    if (expected_type != nullptr) {\n@@ -846,2 +846,2 @@\n-      if (((arrayOopDesc::base_offset_in_bytes(t) + s_offs * element_size) % HeapWordSize == 0) &&\n-          ((arrayOopDesc::base_offset_in_bytes(t) + d_offs * element_size) % HeapWordSize == 0)) {\n+      if (((arrayOopDesc::base_offset_in_bytes(t) + (uint)s_offs * element_size) % HeapWordSize == 0) &&\n+          ((arrayOopDesc::base_offset_in_bytes(t) + (uint)d_offs * element_size) % HeapWordSize == 0)) {\n@@ -911,1 +911,1 @@\n-    assert(method != NULL, \"method should be set if branch is profiled\");\n+    assert(method != nullptr, \"method should be set if branch is profiled\");\n@@ -913,1 +913,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -915,1 +915,1 @@\n-    assert(data != NULL, \"must have profiling data\");\n+    assert(data != nullptr, \"must have profiling data\");\n@@ -966,1 +966,1 @@\n-  if (phi != NULL && cur_val != NULL && cur_val != phi && !phi->is_illegal()) {\n+  if (phi != nullptr && cur_val != nullptr && cur_val != phi && !phi->is_illegal()) {\n@@ -970,1 +970,1 @@\n-        if (op != NULL && op->type()->is_illegal()) {\n+        if (op != nullptr && op->type()->is_illegal()) {\n@@ -976,1 +976,1 @@\n-    if (cur_phi != NULL && cur_phi->is_illegal()) {\n+    if (cur_phi != nullptr && cur_phi->is_illegal()) {\n@@ -985,1 +985,1 @@\n-      assert(cur_val->as_Constant() != NULL || cur_val->as_Local() != NULL,\n+      assert(cur_val->as_Constant() != nullptr || cur_val->as_Local() != nullptr,\n@@ -1084,1 +1084,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1100,1 +1100,1 @@\n-  __ move_wide(LIR_OprFact::oopConst(NULL),\n+  __ move_wide(LIR_OprFact::oopConst(nullptr),\n@@ -1102,1 +1102,1 @@\n-  __ move_wide(LIR_OprFact::oopConst(NULL),\n+  __ move_wide(LIR_OprFact::oopConst(nullptr),\n@@ -1129,1 +1129,1 @@\n-  if (x->state_before() != NULL) {\n+  if (x->state_before() != nullptr) {\n@@ -1133,1 +1133,1 @@\n-    __ oop2reg_patch(NULL, reg, info);\n+    __ oop2reg_patch(nullptr, reg, info);\n@@ -1176,1 +1176,1 @@\n-    call_runtime(&signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), voidType, NULL);\n+    call_runtime(&signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit), voidType, nullptr);\n@@ -1203,1 +1203,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1239,1 +1239,1 @@\n-                                     NULL); \/\/ NULL CodeEmitInfo results in a leaf call\n+                                     nullptr); \/\/ null CodeEmitInfo results in a leaf call\n@@ -1258,1 +1258,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1280,1 +1280,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1298,1 +1298,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1334,1 +1334,1 @@\n-  load_klass(value.result(), klass, NULL);\n+  load_klass(value.result(), klass, nullptr);\n@@ -1477,1 +1477,1 @@\n-    if (c != NULL) {\n+    if (c != nullptr) {\n@@ -1480,1 +1480,1 @@\n-      assert(x->as_Phi() || x->as_Local() != NULL, \"only for Phi and Local\");\n+      assert(x->as_Phi() || x->as_Local() != nullptr, \"only for Phi and Local\");\n@@ -1483,1 +1483,1 @@\n-      _instruction_for_operand.at_put_grow(x->operand()->vreg_number(), x, NULL);\n+      _instruction_for_operand.at_put_grow(x->operand()->vreg_number(), x, nullptr);\n@@ -1494,1 +1494,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1502,1 +1502,1 @@\n-  return NULL;\n+  return nullptr;\n@@ -1617,1 +1617,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1619,1 +1619,1 @@\n-    assert(x->explicit_null_check() == NULL, \"can't fold null check into patching field access\");\n+    assert(x->explicit_null_check() == nullptr, \"can't fold null check into patching field access\");\n@@ -1623,1 +1623,1 @@\n-    if (nc == NULL) {\n+    if (nc == nullptr) {\n@@ -1662,1 +1662,1 @@\n-    \/\/ If the class is not loaded and the object is NULL, we need to deoptimize to throw a\n+    \/\/ If the class is not loaded and the object is null, we need to deoptimize to throw a\n@@ -1676,1 +1676,1 @@\n-                  value.result(), info != NULL ? new CodeEmitInfo(info) : NULL, info);\n+                  value.result(), info != nullptr ? new CodeEmitInfo(info) : nullptr, info);\n@@ -1682,1 +1682,1 @@\n-  bool use_length = x->length() != NULL;\n+  bool use_length = x->length() != nullptr;\n@@ -1684,1 +1684,1 @@\n-  bool needs_store_check = obj_store && (x->value()->as_Constant() == NULL ||\n+  bool needs_store_check = obj_store && (x->value()->as_Constant() == nullptr ||\n@@ -1713,1 +1713,1 @@\n-  CodeEmitInfo* null_check_info = NULL;\n+  CodeEmitInfo* null_check_info = nullptr;\n@@ -1725,1 +1725,1 @@\n-      null_check_info = NULL;\n+      null_check_info = nullptr;\n@@ -1740,1 +1740,1 @@\n-                  NULL, null_check_info);\n+                  nullptr, null_check_info);\n@@ -1826,1 +1826,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1828,1 +1828,1 @@\n-    assert(x->explicit_null_check() == NULL, \"can't fold null check into patching field access\");\n+    assert(x->explicit_null_check() == nullptr, \"can't fold null check into patching field access\");\n@@ -1832,1 +1832,1 @@\n-    if (nc == NULL) {\n+    if (nc == nullptr) {\n@@ -1858,1 +1858,1 @@\n-      __ move(LIR_OprFact::oopConst(NULL), obj);\n+      __ move(LIR_OprFact::oopConst(nullptr), obj);\n@@ -1861,1 +1861,1 @@\n-    \/\/ If the class is not loaded and the object is NULL, we need to deoptimize to throw a\n+    \/\/ If the class is not loaded and the object is null, we need to deoptimize to throw a\n@@ -1877,1 +1877,1 @@\n-                 info ? new CodeEmitInfo(info) : NULL, info);\n+                 info ? new CodeEmitInfo(info) : nullptr, info);\n@@ -1953,1 +1953,1 @@\n-  CodeEmitInfo* info = NULL;\n+  CodeEmitInfo* info = nullptr;\n@@ -1956,1 +1956,1 @@\n-    if (nc == NULL) {\n+    if (nc == nullptr) {\n@@ -1963,1 +1963,1 @@\n-      __ move(LIR_OprFact::oopConst(NULL), obj);\n+      __ move(LIR_OprFact::oopConst(nullptr), obj);\n@@ -1972,1 +1972,1 @@\n-  bool use_length = x->length() != NULL;\n+  bool use_length = x->length() != nullptr;\n@@ -1992,1 +1992,1 @@\n-  CodeEmitInfo* null_check_info = NULL;\n+  CodeEmitInfo* null_check_info = nullptr;\n@@ -1995,1 +1995,1 @@\n-    if (nc != NULL) {\n+    if (nc != nullptr) {\n@@ -2002,1 +2002,1 @@\n-      __ move(LIR_OprFact::oopConst(NULL), obj);\n+      __ move(LIR_OprFact::oopConst(nullptr), obj);\n@@ -2018,1 +2018,1 @@\n-      null_check_info = NULL;\n+      null_check_info = nullptr;\n@@ -2027,1 +2027,1 @@\n-                 NULL, null_check_info);\n+                 nullptr, null_check_info);\n@@ -2071,1 +2071,1 @@\n-    if (throw_type == NULL) {\n+    if (throw_type == nullptr) {\n@@ -2075,1 +2075,1 @@\n-    if (throw_type != NULL && throw_type->is_instance_klass()) {\n+    if (throw_type != nullptr && throw_type->is_instance_klass()) {\n@@ -2085,1 +2085,1 @@\n-  if (x->exception()->as_NewInstance() == NULL && x->exception()->as_ExceptionObject() == NULL) {\n+  if (x->exception()->as_NewInstance() == nullptr && x->exception()->as_ExceptionObject() == nullptr) {\n@@ -2249,1 +2249,1 @@\n-    int key = x->lo_key();\n+    int low = x->lo_key();\n@@ -2251,2 +2251,3 @@\n-    C1SwitchRange* range = new C1SwitchRange(key, sux);\n-    for (int i = 0; i < len; i++, key++) {\n+    C1SwitchRange* range = new C1SwitchRange(low, sux);\n+    for (int i = 0; i < len; i++) {\n+      int key = low + i;\n@@ -2323,1 +2324,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2325,1 +2326,1 @@\n-    assert(data != NULL, \"must have profiling data\");\n+    assert(data != nullptr, \"must have profiling data\");\n@@ -2381,1 +2382,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2383,1 +2384,1 @@\n-    assert(data != NULL, \"must have profiling data\");\n+    assert(data != nullptr, \"must have profiling data\");\n@@ -2449,1 +2450,1 @@\n-    assert(method != NULL, \"method should be set if branch is profiled\");\n+    assert(method != nullptr, \"method should be set if branch is profiled\");\n@@ -2451,1 +2452,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2453,1 +2454,1 @@\n-    assert(data != NULL, \"must have profiling data\");\n+    assert(data != nullptr, \"must have profiling data\");\n@@ -2498,1 +2499,1 @@\n-  ciKlass* result = NULL;\n+  ciKlass* result = nullptr;\n@@ -2507,1 +2508,1 @@\n-  ciKlass* exact_klass = NULL;\n+  ciKlass* exact_klass = nullptr;\n@@ -2513,1 +2514,1 @@\n-    if (type == NULL) {\n+    if (type == nullptr) {\n@@ -2517,2 +2518,2 @@\n-    assert(type == NULL || type->is_klass(), \"type should be class\");\n-    exact_klass = (type != NULL && type->is_loaded()) ? (ciKlass*)type : NULL;\n+    assert(type == nullptr || type->is_klass(), \"type should be class\");\n+    exact_klass = (type != nullptr && type->is_loaded()) ? (ciKlass*)type : nullptr;\n@@ -2520,1 +2521,1 @@\n-    do_update = exact_klass == NULL || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n+    do_update = exact_klass == nullptr || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n@@ -2527,1 +2528,1 @@\n-  ciKlass* exact_signature_k = NULL;\n+  ciKlass* exact_signature_k = nullptr;\n@@ -2531,1 +2532,1 @@\n-    if (exact_signature_k == NULL) {\n+    if (exact_signature_k == nullptr) {\n@@ -2539,1 +2540,1 @@\n-    \/\/ exact_klass and exact_signature_k can be both non NULL but\n+    \/\/ exact_klass and exact_signature_k can be both non null but\n@@ -2542,1 +2543,1 @@\n-    if (exact_klass == NULL && exact_signature_k != NULL && exact_klass != exact_signature_k) {\n+    if (exact_klass == nullptr && exact_signature_k != nullptr && exact_klass != exact_signature_k) {\n@@ -2547,1 +2548,1 @@\n-    if (callee_signature_k != NULL &&\n+    if (callee_signature_k != nullptr &&\n@@ -2550,1 +2551,1 @@\n-      if (improved_klass == NULL) {\n+      if (improved_klass == nullptr) {\n@@ -2553,1 +2554,1 @@\n-      if (exact_klass == NULL && improved_klass != NULL && exact_klass != improved_klass) {\n+      if (exact_klass == nullptr && improved_klass != nullptr && exact_klass != improved_klass) {\n@@ -2557,1 +2558,1 @@\n-    do_update = exact_klass == NULL || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n+    do_update = exact_klass == nullptr || ciTypeEntries::valid_ciklass(profiled_k) != exact_klass;\n@@ -2576,1 +2577,1 @@\n-                  value.result(), exact_klass, profiled_k, new_pointer_register(), not_null, exact_signature_k != NULL);\n+                  value.result(), exact_klass, profiled_k, new_pointer_register(), not_null, exact_signature_k != nullptr);\n@@ -2585,1 +2586,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -2587,1 +2588,1 @@\n-    if (md->parameters_type_data() != NULL) {\n+    if (md->parameters_type_data() != nullptr) {\n@@ -2600,1 +2601,1 @@\n-                                        profiled_k, local, mdp, false, local->declared_type()->as_klass(), NULL);\n+                                        profiled_k, local, mdp, false, local->declared_type()->as_klass(), nullptr);\n@@ -2602,1 +2603,1 @@\n-          if (exact != NULL) {\n+          if (exact != nullptr) {\n@@ -2642,1 +2643,1 @@\n-    assert(local != NULL, \"Locals for incoming arguments must have been created\");\n+    assert(local != nullptr, \"Locals for incoming arguments must have been created\");\n@@ -2648,1 +2649,1 @@\n-    _instruction_for_operand.at_put_grow(dest->vreg_number(), local, NULL);\n+    _instruction_for_operand.at_put_grow(dest->vreg_number(), local, nullptr);\n@@ -2661,1 +2662,1 @@\n-    call_runtime(&signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry), voidType, NULL);\n+    call_runtime(&signature, args, CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry), voidType, nullptr);\n@@ -2671,1 +2672,1 @@\n-      assert(receiver != NULL, \"must already exist\");\n+      assert(receiver != nullptr, \"must already exist\");\n@@ -2680,1 +2681,1 @@\n-      CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, x->check_flag(Instruction::DeoptimizeOnException));\n+      CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, SynchronizationEntryBCI), nullptr, x->check_flag(Instruction::DeoptimizeOnException));\n@@ -2683,2 +2684,2 @@\n-      \/\/ receiver is guaranteed non-NULL so don't need CodeEmitInfo\n-      __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, NULL);\n+      \/\/ receiver is guaranteed non-null so don't need CodeEmitInfo\n+      __ lock_object(syncTempOpr(), obj, lock, new_register(T_OBJECT), slow_path, nullptr);\n@@ -2690,1 +2691,1 @@\n-    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, SynchronizationEntryBCI), NULL, false);\n+    CodeEmitInfo* info = new CodeEmitInfo(scope()->start()->state()->copy(ValueStack::StateBefore, SynchronizationEntryBCI), nullptr, false);\n@@ -3036,1 +3037,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -3038,1 +3039,1 @@\n-    if (data != NULL) {\n+    if (data != nullptr) {\n@@ -3057,1 +3058,1 @@\n-        ciSignatureStream callee_signature_stream(callee_signature, has_receiver ? x->callee()->holder() : NULL);\n+        ciSignatureStream callee_signature_stream(callee_signature, has_receiver ? x->callee()->holder() : nullptr);\n@@ -3060,1 +3061,1 @@\n-        ciSignature* signature_at_call = NULL;\n+        ciSignature* signature_at_call = nullptr;\n@@ -3071,1 +3072,1 @@\n-          if (exact != NULL) {\n+          if (exact != nullptr) {\n@@ -3092,1 +3093,1 @@\n-    if (md != NULL) {\n+    if (md != nullptr) {\n@@ -3094,1 +3095,1 @@\n-      if (parameters_type_data != NULL) {\n+      if (parameters_type_data != nullptr) {\n@@ -3099,1 +3100,1 @@\n-        ciSignatureStream sig_stream(sig, has_receiver ? x->callee()->holder() : NULL);\n+        ciSignatureStream sig_stream(sig, has_receiver ? x->callee()->holder() : nullptr);\n@@ -3108,1 +3109,1 @@\n-        if (arg == NULL || !Bytecodes::has_receiver(bc)) {\n+        if (arg == nullptr || !Bytecodes::has_receiver(bc)) {\n@@ -3118,1 +3119,1 @@\n-                                        profiled_k, arg, mdp, not_null, sig_stream.next_klass(), NULL);\n+                                        profiled_k, arg, mdp, not_null, sig_stream.next_klass(), nullptr);\n@@ -3120,1 +3121,1 @@\n-          if (exact != NULL) {\n+          if (exact != nullptr) {\n@@ -3129,1 +3130,1 @@\n-                x->recv() != NULL && Bytecodes::has_receiver(bc)) {\n+                x->recv() != nullptr && Bytecodes::has_receiver(bc)) {\n@@ -3157,1 +3158,1 @@\n-  if (x->recv() != NULL || x->nb_profiled_args() > 0) {\n+  if (x->recv() != nullptr || x->nb_profiled_args() > 0) {\n@@ -3161,1 +3162,1 @@\n-  if (x->recv() != NULL) {\n+  if (x->recv() != nullptr) {\n@@ -3173,1 +3174,1 @@\n-  assert(md != NULL, \"Sanity\");\n+  assert(md != nullptr, \"Sanity\");\n@@ -3175,1 +3176,1 @@\n-  if (data != NULL) {\n+  if (data != nullptr) {\n@@ -3181,1 +3182,1 @@\n-    ciSignature* signature_at_call = NULL;\n+    ciSignature* signature_at_call = nullptr;\n@@ -3192,1 +3193,1 @@\n-    if (exact != NULL) {\n+    if (exact != nullptr) {\n@@ -3264,1 +3265,1 @@\n-    if (counters_adr == NULL) {\n+    if (counters_adr == nullptr) {\n@@ -3277,1 +3278,1 @@\n-    assert(md != NULL, \"Sanity\");\n+    assert(md != nullptr, \"Sanity\");\n@@ -3331,1 +3332,1 @@\n-  LIR_Opr result = call_runtime(signature, args, x->entry(), x->type(), NULL);\n+  LIR_Opr result = call_runtime(signature, args, x->entry(), x->type(), nullptr);\n@@ -3563,1 +3564,1 @@\n-  null_check_info = NULL;\n+  null_check_info = nullptr;\n","filename":"src\/hotspot\/share\/c1\/c1_LIRGenerator.cpp","additions":175,"deletions":174,"binary":false,"changes":349,"status":"modified"},{"patch":"@@ -73,1 +73,1 @@\n-void ArchiveBuilder::SourceObjList::append(MetaspaceClosure::Ref* enclosing_ref, SourceObjInfo* src_info) {\n+void ArchiveBuilder::SourceObjList::append(SourceObjInfo* src_info) {\n@@ -91,7 +91,2 @@\n-  \/\/ so that we can copy\/relocate it later. E.g., if we have\n-  \/\/    class Foo { intx scala; Bar* ptr; }\n-  \/\/    Foo *f = 0x100;\n-  \/\/ To mark the f->ptr pointer on 64-bit platform, this function is called with\n-  \/\/    src_info()->obj() == 0x100\n-  \/\/    ref->addr() == 0x108\n-  address src_obj = src_info->obj();\n+  \/\/ so that we can copy\/relocate it later.\n+  address src_obj = src_info->source_addr();\n@@ -181,2 +176,0 @@\n-  clean_up_src_obj_table();\n-\n@@ -243,1 +236,1 @@\n-  iterate_roots(&doit, \/*is_relocating_pointers=*\/false);\n+  iterate_roots(&doit);\n@@ -397,10 +390,4 @@\n-void ArchiveBuilder::iterate_sorted_roots(MetaspaceClosure* it, bool is_relocating_pointers) {\n-  int i;\n-\n-  if (!is_relocating_pointers) {\n-    \/\/ Don't relocate _symbol, so we can safely call decrement_refcount on the\n-    \/\/ original symbols.\n-    int num_symbols = _symbols->length();\n-    for (i = 0; i < num_symbols; i++) {\n-      it->push(_symbols->adr_at(i));\n-    }\n+void ArchiveBuilder::iterate_sorted_roots(MetaspaceClosure* it) {\n+  int num_symbols = _symbols->length();\n+  for (int i = 0; i < num_symbols; i++) {\n+    it->push(_symbols->adr_at(i));\n@@ -410,1 +397,1 @@\n-  for (i = 0; i < num_klasses; i++) {\n+  for (int i = 0; i < num_klasses; i++) {\n@@ -414,1 +401,1 @@\n-  iterate_roots(it, is_relocating_pointers);\n+  iterate_roots(it);\n@@ -424,7 +411,1 @@\n-    return _builder->gather_one_source_obj(enclosing_ref(), ref, read_only);\n-  }\n-\n-  virtual void do_pending_ref(Ref* ref) {\n-    if (ref->obj() != nullptr) {\n-      _builder->remember_embedded_pointer_in_copied_obj(enclosing_ref(), ref);\n-    }\n+    return _builder->gather_one_source_obj(ref, read_only);\n@@ -434,2 +415,1 @@\n-bool ArchiveBuilder::gather_one_source_obj(MetaspaceClosure::Ref* enclosing_ref,\n-                                           MetaspaceClosure::Ref* ref, bool read_only) {\n+bool ArchiveBuilder::gather_one_source_obj(MetaspaceClosure::Ref* ref, bool read_only) {\n@@ -440,2 +420,1 @@\n-  ref->set_keep_after_pushing();\n-  remember_embedded_pointer_in_copied_obj(enclosing_ref, ref);\n+  remember_embedded_pointer_in_enclosing_obj(ref);\n@@ -456,2 +435,1 @@\n-    ref->set_user_data((void*)p);\n-      _ro_src_objs.append(enclosing_ref, p);\n+      _ro_src_objs.append(p);\n@@ -460,1 +438,1 @@\n-      _rw_src_objs.append(enclosing_ref, p);\n+      _rw_src_objs.append(p);\n@@ -468,2 +446,2 @@\n-void ArchiveBuilder::remember_embedded_pointer_in_copied_obj(MetaspaceClosure::Ref* enclosing_ref,\n-                                                             MetaspaceClosure::Ref* ref) {\n+\/\/ Remember that we have a pointer inside ref->enclosing_obj() that points to ref->obj()\n+void ArchiveBuilder::remember_embedded_pointer_in_enclosing_obj(MetaspaceClosure::Ref* ref) {\n@@ -472,5 +450,30 @@\n-  if (enclosing_ref != nullptr) {\n-    SourceObjInfo* src_info = (SourceObjInfo*)enclosing_ref->user_data();\n-    if (src_info == nullptr) {\n-      \/\/ source objects of point_to_it\/set_to_null types are not copied\n-      \/\/ so we don't need to remember their pointers.\n+  address enclosing_obj = ref->enclosing_obj();\n+  if (enclosing_obj == nullptr) {\n+    return;\n+  }\n+\n+  \/\/ We are dealing with 3 addresses:\n+  \/\/ address o    = ref->obj(): We have found an object whose address is o.\n+  \/\/ address* mpp = ref->mpp(): The object o is pointed to by a pointer whose address is mpp.\n+  \/\/                            I.e., (*mpp == o)\n+  \/\/ enclosing_obj            : If non-null, it is the object which has a field that points to o.\n+  \/\/                            mpp is the address if that field.\n+  \/\/\n+  \/\/ Example: We have an array whose first element points to a Method:\n+  \/\/     Method* o                     = 0x0000abcd;\n+  \/\/     Array<Method*>* enclosing_obj = 0x00001000;\n+  \/\/     enclosing_obj->at_put(0, o);\n+  \/\/\n+  \/\/ We the MetaspaceClosure iterates on the very first element of this array, we have\n+  \/\/     ref->obj()           == 0x0000abcd   (the Method)\n+  \/\/     ref->mpp()           == 0x00001008   (the location of the first element in the array)\n+  \/\/     ref->enclosing_obj() == 0x00001000   (the Array that contains the Method)\n+  \/\/\n+  \/\/ We use the above information to mark the bitmap to indicate that there's a pointer on address 0x00001008.\n+  SourceObjInfo* src_info = _src_obj_table.get(enclosing_obj);\n+  if (src_info == nullptr || !src_info->should_copy()) {\n+    \/\/ source objects of point_to_it\/set_to_null types are not copied\n+    \/\/ so we don't need to remember their pointers.\n+  } else {\n+    if (src_info->read_only()) {\n+      _ro_src_objs.remember_embedded_pointer(src_info, ref);\n@@ -478,5 +481,1 @@\n-      if (src_info->read_only()) {\n-        _ro_src_objs.remember_embedded_pointer(src_info, ref);\n-      } else {\n-        _rw_src_objs.remember_embedded_pointer(src_info, ref);\n-      }\n+      _rw_src_objs.remember_embedded_pointer(src_info, ref);\n@@ -492,1 +491,1 @@\n-  iterate_sorted_roots(&doit, \/*is_relocating_pointers=*\/false);\n+  iterate_sorted_roots(&doit);\n@@ -602,2 +601,1 @@\n-  MetaspaceClosure::Ref* ref = src_info->ref();\n-  address src = ref->obj();\n+  address src = src_info->source_addr();\n@@ -610,6 +608,5 @@\n-  if (ref->msotype() == MetaspaceObj::ClassType) {\n-    \/\/ Reserve space for a pointer immediately in front of an InstanceKlass. That space will\n-    \/\/ later be used to store the RuntimeClassInfo* pointer directly in front of the archived\n-    \/\/ InstanceKlass, in order to have a quick lookup InstanceKlass* -> RunTimeClassInfo*\n-    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\/::set_for() for\n-    \/\/ details.\n+  if (src_info->msotype() == MetaspaceObj::ClassType) {\n+    \/\/ Save a pointer immediate in front of an InstanceKlass, so\n+    \/\/ we can do a quick lookup from InstanceKlass* -> RunTimeClassInfo*\n+    \/\/ without building another hashtable. See RunTimeClassInfo::get_for()\n+    \/\/ in systemDictionaryShared.cpp.\n@@ -637,1 +634,1 @@\n-  intptr_t* archived_vtable = CppVtables::get_archived_vtable(ref->msotype(), (address)dest);\n+  intptr_t* archived_vtable = CppVtables::get_archived_vtable(src_info->msotype(), (address)dest);\n@@ -644,1 +641,1 @@\n-                 MetaspaceObj::type_name(ref->msotype()));\n+                 MetaspaceObj::type_name(src_info->msotype()));\n@@ -647,1 +644,1 @@\n-  _alloc_stats.record(ref->msotype(), int(newtop - oldtop), src_info->read_only());\n+  _alloc_stats.record(src_info->msotype(), int(newtop - oldtop), src_info->read_only());\n@@ -652,0 +649,13 @@\n+\/\/ This is used by code that hand-assemble data structures, such as the LambdaProxyClassKey, that are\n+\/\/ not handled by MetaspaceClosure.\n+void ArchiveBuilder::write_pointer_in_buffer(address* ptr_location, address src_addr) {\n+  assert(is_in_buffer_space(ptr_location), \"must be\");\n+  if (src_addr == nullptr) {\n+    *ptr_location = nullptr;\n+    ArchivePtrMarker::clear_pointer(ptr_location);\n+  } else {\n+    *ptr_location = get_buffered_addr(src_addr);\n+    ArchivePtrMarker::mark_pointer(ptr_location);\n+  }\n+}\n+\n@@ -672,24 +682,0 @@\n-class RefRelocator: public MetaspaceClosure {\n-  ArchiveBuilder* _builder;\n-\n-public:\n-  RefRelocator(ArchiveBuilder* builder) : _builder(builder) {}\n-\n-  virtual bool do_ref(Ref* ref, bool read_only) {\n-    if (ref->not_null()) {\n-      ref->update(_builder->get_buffered_addr(ref->obj()));\n-      ArchivePtrMarker::mark_pointer(ref->addr());\n-    }\n-    return false; \/\/ Do not recurse.\n-  }\n-};\n-\n-void ArchiveBuilder::relocate_roots() {\n-  log_info(cds)(\"Relocating external roots ... \");\n-  ResourceMark rm;\n-  RefRelocator doit(this);\n-  iterate_sorted_roots(&doit, \/*is_relocating_pointers=*\/true);\n-  doit.finish();\n-  log_info(cds)(\"done\");\n-}\n-\n@@ -702,10 +688,0 @@\n-\/\/ We must relocate vmClasses::_klasses[] only after we have copied the\n-\/\/ java objects in during dump_java_heap_objects(): during the object copy, we operate on\n-\/\/ old objects which assert that their klass is the original klass.\n-void ArchiveBuilder::relocate_vm_classes() {\n-  log_info(cds)(\"Relocating vmClasses::_klasses[] ... \");\n-  ResourceMark rm;\n-  RefRelocator doit(this);\n-  vmClasses::metaspace_pointers_do(&doit);\n-}\n-\n@@ -728,1 +704,1 @@\n-    Klass* k = klasses()->at(i);\n+    Klass* k = get_buffered_addr(klasses()->at(i));\n@@ -818,0 +794,4 @@\n+  if (!is_in_buffer_space(p)) {\n+    \/\/ p must be a \"source\" address\n+    p = get_buffered_addr(p);\n+  }\n@@ -1140,5 +1120,0 @@\n-void ArchiveBuilder::clean_up_src_obj_table() {\n-  SrcObjTableCleaner cleaner;\n-  _src_obj_table.iterate(&cleaner);\n-}\n-\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.cpp","additions":76,"deletions":101,"binary":false,"changes":177,"status":"modified"},{"patch":"@@ -137,1 +137,0 @@\n-    MetaspaceClosure::Ref* _ref; \/\/ The object that's copied into the buffer\n@@ -144,4 +143,2 @@\n-    address _source_addr;    \/\/ The value of the source object (_ref->obj()) when this\n-                             \/\/ SourceObjInfo was created. Note that _ref->obj() may change\n-                             \/\/ later if _ref is relocated.\n-    address _buffered_addr;  \/\/ The copy of _ref->obj() insider the buffer.\n+    address _source_addr;    \/\/ The source object to be copied.\n+    address _buffered_addr;  \/\/ The copy of this object insider the buffer.\n@@ -150,1 +147,1 @@\n-      _ref(ref), _ptrmap_start(0), _ptrmap_end(0), _read_only(read_only), _follow_mode(follow_mode),\n+      _ptrmap_start(0), _ptrmap_end(0), _read_only(read_only), _follow_mode(follow_mode),\n@@ -161,1 +158,0 @@\n-    MetaspaceClosure::Ref* ref() const { return  _ref; }\n@@ -175,1 +171,6 @@\n-    address buffered_addr() const  { return _buffered_addr; }\n+    address buffered_addr() const  {\n+      if (_follow_mode != set_to_null) {\n+        assert(_buffered_addr != nullptr, \"must be initialized\");\n+      }\n+      return _buffered_addr;\n+    }\n@@ -177,3 +178,0 @@\n-\n-    \/\/ convenience accessor\n-    address obj() const { return ref()->obj(); }\n@@ -193,1 +191,1 @@\n-    void append(MetaspaceClosure::Ref* enclosing_ref, SourceObjInfo* src_info);\n+    void append(SourceObjInfo* src_info);\n@@ -201,8 +199,0 @@\n-  class SrcObjTableCleaner {\n-  public:\n-    bool do_entry(address key, const SourceObjInfo& value) {\n-      delete value.ref();\n-      return true;\n-    }\n-  };\n-\n@@ -255,1 +245,1 @@\n-  void iterate_sorted_roots(MetaspaceClosure* it, bool is_relocating_pointers);\n+  void iterate_sorted_roots(MetaspaceClosure* it);\n@@ -270,1 +260,1 @@\n-  virtual void iterate_roots(MetaspaceClosure* it, bool is_relocating_pointers) = 0;\n+  virtual void iterate_roots(MetaspaceClosure* it) = 0;\n@@ -317,0 +307,5 @@\n+  inline static u4 to_offset_u4(uintx offset) {\n+    guarantee(offset <= MAX_SHARED_DELTA, \"must be 32-bit offset \" INTPTR_FORMAT, offset);\n+    return (u4)offset;\n+  }\n+\n@@ -331,2 +326,1 @@\n-    guarantee(offset <= MAX_SHARED_DELTA, \"must be 32-bit offset \" INTPTR_FORMAT, offset);\n-    return (u4)offset;\n+    return to_offset_u4(offset);\n@@ -338,2 +332,1 @@\n-    guarantee(offset <= MAX_SHARED_DELTA, \"must be 32-bit offset \" INTPTR_FORMAT, offset);\n-    return (u4)offset;\n+    return to_offset_u4(offset);\n@@ -351,2 +344,2 @@\n-  bool gather_one_source_obj(MetaspaceClosure::Ref* enclosing_ref, MetaspaceClosure::Ref* ref, bool read_only);\n-  void remember_embedded_pointer_in_copied_obj(MetaspaceClosure::Ref* enclosing_ref, MetaspaceClosure::Ref* ref);\n+  bool gather_one_source_obj(MetaspaceClosure::Ref* ref, bool read_only);\n+  void remember_embedded_pointer_in_enclosing_obj(MetaspaceClosure::Ref* ref);\n@@ -389,2 +382,0 @@\n-  void relocate_roots();\n-  void relocate_vm_classes();\n@@ -397,0 +388,5 @@\n+  void write_pointer_in_buffer(address* ptr_location, address src_addr);\n+  template <typename T> void write_pointer_in_buffer(T* ptr_location, T src_addr) {\n+    write_pointer_in_buffer((address*)ptr_location, (address)src_addr);\n+  }\n+\n@@ -398,0 +394,4 @@\n+  template <typename T> T get_buffered_addr(T src_addr) const {\n+    return (T)get_buffered_addr((address)src_addr);\n+  }\n+\n","filename":"src\/hotspot\/share\/cds\/archiveBuilder.hpp","additions":30,"deletions":30,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -279,0 +279,15 @@\n+void WriteClosure::do_ptr(void** p) {\n+  \/\/ Write ptr into the archive; ptr can be:\n+  \/\/   (a) null                 -> written as 0\n+  \/\/   (b) a \"buffered\" address -> written as is\n+  \/\/   (c) a \"source\"   address -> convert to \"buffered\" and write\n+  \/\/ The common case is (c). E.g., when writing the vmClasses into the archive.\n+  \/\/ We have (b) only when we don't have a corresponding source object. E.g.,\n+  \/\/ the archived c++ vtable entries.\n+  address ptr = *(address*)p;\n+  if (ptr != nullptr && !ArchiveBuilder::current()->is_in_buffer_space(ptr)) {\n+    ptr = ArchiveBuilder::current()->get_buffered_addr(ptr);\n+  }\n+  _dump_region->append_intptr_t((intptr_t)ptr, true);\n+}\n+\n@@ -299,1 +314,1 @@\n-    _dump_region->append_intptr_t(*(intptr_t*)start, true);\n+    do_ptr((void**)start);\n@@ -318,0 +333,5 @@\n+void ReadClosure::do_int(int* p) {\n+  intptr_t obj = nextPtr();\n+  *p = (int)(intx(obj));\n+}\n+\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.cpp","additions":21,"deletions":1,"binary":false,"changes":22,"status":"modified"},{"patch":"@@ -190,3 +190,1 @@\n-  void do_ptr(void** p) {\n-    _dump_region->append_intptr_t((intptr_t)*p, true);\n-  }\n+  void do_ptr(void** p);\n@@ -198,0 +196,4 @@\n+  void do_int(int* p) {\n+    _dump_region->append_intptr_t((intptr_t)(*p));\n+  }\n+\n@@ -227,0 +229,1 @@\n+  void do_int(int* p);\n","filename":"src\/hotspot\/share\/cds\/archiveUtils.hpp","additions":6,"deletions":3,"binary":false,"changes":9,"status":"modified"},{"patch":"@@ -859,1 +859,1 @@\n-  soc->do_ptr((void**)&_archived_ArchiveHeapTestClass);\n+  soc->do_ptr(&_archived_ArchiveHeapTestClass);\n","filename":"src\/hotspot\/share\/cds\/heapShared.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -81,2 +81,0 @@\n-DumpTimeSharedClassTable* SystemDictionaryShared::_cloned_dumptime_table = nullptr;\n-DumpTimeLambdaProxyClassDictionary* SystemDictionaryShared::_cloned_dumptime_lambda_proxy_class_dictionary = nullptr;\n@@ -1170,13 +1168,10 @@\n-    if (len > 1) {\n-      for (int i = 0; i < len-1; i++) {\n-        InstanceKlass* ok0 = info._proxy_klasses->at(i+0); \/\/ this is original klass\n-        InstanceKlass* ok1 = info._proxy_klasses->at(i+1); \/\/ this is original klass\n-        assert(ArchiveBuilder::current()->is_in_buffer_space(ok0), \"must be\");\n-        assert(ArchiveBuilder::current()->is_in_buffer_space(ok1), \"must be\");\n-        InstanceKlass* bk0 = ok0;\n-        InstanceKlass* bk1 = ok1;\n-        assert(bk0->next_link() == 0, \"must be called after Klass::remove_unshareable_info()\");\n-        assert(bk1->next_link() == 0, \"must be called after Klass::remove_unshareable_info()\");\n-        bk0->set_next_link(bk1);\n-        bk1->set_lambda_proxy_is_available();\n-        ArchivePtrMarker::mark_pointer(bk0->next_link_addr());\n+    InstanceKlass* last_buff_k = nullptr;\n+\n+    for (int i = len - 1; i >= 0; i--) {\n+      InstanceKlass* orig_k = info._proxy_klasses->at(i);\n+      InstanceKlass* buff_k = ArchiveBuilder::current()->get_buffered_addr(orig_k);\n+      assert(ArchiveBuilder::current()->is_in_buffer_space(buff_k), \"must be\");\n+      buff_k->set_lambda_proxy_is_available();\n+      buff_k->set_next_link(last_buff_k);\n+      if (last_buff_k != nullptr) {\n+        ArchivePtrMarker::mark_pointer(buff_k->next_link_addr());\n@@ -1184,0 +1179,1 @@\n+      last_buff_k = buff_k;\n@@ -1185,1 +1181,0 @@\n-    info._proxy_klasses->at(0)->set_lambda_proxy_is_available();\n@@ -1209,0 +1204,1 @@\n+      name = ArchiveBuilder::current()->get_buffered_addr(name);\n@@ -1222,1 +1218,2 @@\n-      RunTimeClassInfo::set_for(info._klass, record);\n+      InstanceKlass* buffered_klass = ArchiveBuilder::current()->get_buffered_addr(info._klass);\n+      RunTimeClassInfo::set_for(buffered_klass, record);\n@@ -1272,1 +1269,1 @@\n-    soc->do_ptr((void**)vmClasses::klass_addr_at(id));\n+    soc->do_ptr(vmClasses::klass_addr_at(id));\n@@ -1368,0 +1365,4 @@\n+    if (record->_klass->array_klasses() != nullptr) {\n+      record->_klass->array_klasses()->cds_print_value_on(_st);\n+      _st->cr();\n+    }\n@@ -1449,81 +1450,0 @@\n-class CloneDumpTimeClassTable: public StackObj {\n-  DumpTimeSharedClassTable* _table;\n-  DumpTimeSharedClassTable* _cloned_table;\n- public:\n-  CloneDumpTimeClassTable(DumpTimeSharedClassTable* table, DumpTimeSharedClassTable* clone) :\n-                      _table(table), _cloned_table(clone) {\n-    assert(_table != nullptr, \"_dumptime_table is nullptr\");\n-    assert(_cloned_table != nullptr, \"_cloned_table is nullptr\");\n-  }\n-  void do_entry(InstanceKlass* k, DumpTimeClassInfo& info) {\n-    bool created;\n-    _cloned_table->put_if_absent(k, info, &created);\n-    assert(created, \"must be\");\n-  }\n-};\n-\n-class CloneDumpTimeLambdaProxyClassTable: StackObj {\n-  DumpTimeLambdaProxyClassDictionary* _table;\n-  DumpTimeLambdaProxyClassDictionary* _cloned_table;\n- public:\n-  CloneDumpTimeLambdaProxyClassTable(DumpTimeLambdaProxyClassDictionary* table,\n-                                     DumpTimeLambdaProxyClassDictionary* clone) :\n-                      _table(table), _cloned_table(clone) {\n-    assert(_table != nullptr, \"_dumptime_table is nullptr\");\n-    assert(_cloned_table != nullptr, \"_cloned_table is nullptr\");\n-  }\n-\n-  bool do_entry(LambdaProxyClassKey& key, DumpTimeLambdaProxyClassInfo& info) {\n-    assert_lock_strong(DumpTimeTable_lock);\n-    bool created;\n-    \/\/ make copies then store in _clone_table\n-    LambdaProxyClassKey keyCopy = key;\n-    _cloned_table->put_if_absent(keyCopy, info, &created);\n-    assert(created, \"must be\");\n-    ++ _cloned_table->_count;\n-    return true; \/\/ keep on iterating\n-  }\n-};\n-\n-\/\/ When dumping the CDS archive, the ArchiveBuilder will irrecoverably modify the\n-\/\/ _dumptime_table and _dumptime_lambda_proxy_class_dictionary (e.g., metaspace\n-\/\/ pointers are changed to use \"buffer\" addresses.)\n-\/\/\n-\/\/ We save a copy of these tables and restore them after the dumping is finished.\n-\/\/ This makes it possible to repeat the dumping operation (e.g., use\n-\/\/ \"jcmd VM.cds dynamic_dump\" multiple times on the same JVM process).\n-\/\/\n-\/\/ We use the copy constructors to clone the values in these tables. The copy constructors\n-\/\/ must make a deep copy, as internal data structures such as the contents of\n-\/\/ DumpTimeClassInfo::_loader_constraints are also modified by the ArchiveBuilder.\n-\n-void SystemDictionaryShared::clone_dumptime_tables() {\n-  Arguments::assert_is_dumping_archive();\n-  assert_lock_strong(DumpTimeTable_lock);\n-\n-  assert(_cloned_dumptime_table == nullptr, \"_cloned_dumptime_table must be cleaned\");\n-  _cloned_dumptime_table = new (mtClass) DumpTimeSharedClassTable;\n-  CloneDumpTimeClassTable copy_classes(_dumptime_table, _cloned_dumptime_table);\n-  _dumptime_table->iterate_all_live_classes(&copy_classes);\n-  _cloned_dumptime_table->update_counts();\n-\n-  assert(_cloned_dumptime_lambda_proxy_class_dictionary == nullptr,\n-         \"_cloned_dumptime_lambda_proxy_class_dictionary must be cleaned\");\n-  _cloned_dumptime_lambda_proxy_class_dictionary =\n-                                        new (mtClass) DumpTimeLambdaProxyClassDictionary;\n-  CloneDumpTimeLambdaProxyClassTable copy_proxy_classes(_dumptime_lambda_proxy_class_dictionary,\n-                                                        _cloned_dumptime_lambda_proxy_class_dictionary);\n-  _dumptime_lambda_proxy_class_dictionary->iterate(&copy_proxy_classes);\n-}\n-\n-void SystemDictionaryShared::restore_dumptime_tables() {\n-  assert_lock_strong(DumpTimeTable_lock);\n-  delete _dumptime_table;\n-  _dumptime_table = _cloned_dumptime_table;\n-  _cloned_dumptime_table = nullptr;\n-\n-  delete _dumptime_lambda_proxy_class_dictionary;\n-  _dumptime_lambda_proxy_class_dictionary = _cloned_dumptime_lambda_proxy_class_dictionary;\n-  _cloned_dumptime_lambda_proxy_class_dictionary = nullptr;\n-}\n-\n","filename":"src\/hotspot\/share\/classfile\/systemDictionaryShared.cpp","additions":19,"deletions":99,"binary":false,"changes":118,"status":"modified"},{"patch":"@@ -515,4 +515,0 @@\n-bool G1CollectedHeap::check_archive_addresses(MemRegion range) {\n-  return _hrm.reserved().contains(range);\n-}\n-\n@@ -536,1 +532,1 @@\n-bool G1CollectedHeap::alloc_archive_regions(MemRegion range) {\n+HeapWord* G1CollectedHeap::alloc_archive_region(size_t word_size, HeapWord* preferred_addr) {\n@@ -542,0 +538,6 @@\n+  if (reserved.word_size() <= word_size) {\n+    log_info(gc, heap)(\"Unable to allocate regions as archive heap is too large; size requested = \" SIZE_FORMAT\n+                       \" bytes, heap = \" SIZE_FORMAT \" bytes\", word_size, reserved.word_size());\n+    return nullptr;\n+  }\n+\n@@ -546,12 +548,4 @@\n-  \/\/ For the specified MemRegion range, allocate the corresponding G1\n-  \/\/ region(s) and mark them as old region(s).\n-  HeapWord* start_address = range.start();\n-  size_t word_size = range.word_size();\n-  HeapWord* last_address = range.last();\n-\n-  guarantee(reserved.contains(start_address) && reserved.contains(last_address),\n-            \"MemRegion outside of heap [\" PTR_FORMAT \", \" PTR_FORMAT \"]\",\n-            p2i(start_address), p2i(last_address));\n-\n-  \/\/ Perform the actual region allocation, exiting if it fails.\n-  \/\/ Then note how much new space we have allocated.\n+  \/\/ Attempt to allocate towards the end of the heap.\n+  HeapWord* start_addr = reserved.end() - align_up(word_size, HeapRegion::GrainWords);\n+  MemRegion range = MemRegion(start_addr, word_size);\n+  HeapWord* last_address = range.last();\n@@ -560,1 +554,1 @@\n-    return false;\n+    return nullptr;\n@@ -566,1 +560,0 @@\n-\n@@ -583,1 +576,1 @@\n-  return true;\n+  return start_addr;\n@@ -2627,2 +2620,2 @@\n-  G1CollectionSetCandidates* candidates = G1CollectedHeap::heap()->collection_set()->candidates();\n-  return candidates != nullptr && candidates->num_remaining() > 0;\n+  const G1CollectionSetCandidates* candidates = collection_set()->candidates();\n+  return !candidates->is_empty();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1CollectedHeap.cpp","additions":15,"deletions":22,"binary":false,"changes":37,"status":"modified"},{"patch":"@@ -62,2 +62,1 @@\n-                                           size_t young_cset_length,\n-                                           size_t optional_cset_length,\n+                                           G1CollectionSet* collection_set,\n@@ -81,1 +80,1 @@\n-    _surviving_words_length(young_cset_length + 1),\n+    _surviving_words_length(collection_set->young_region_length() + 1),\n@@ -86,1 +85,1 @@\n-    _max_num_optional_regions(optional_cset_length),\n+    _max_num_optional_regions(collection_set->optional_region_length()),\n@@ -107,1 +106,3 @@\n-  _closures = G1EvacuationRootClosures::create_root_closures(this, _g1h);\n+  _closures = G1EvacuationRootClosures::create_root_closures(_g1h,\n+                                                             this,\n+                                                             collection_set->only_contains_young_regions());\n@@ -583,2 +584,1 @@\n-                               _young_cset_length,\n-                               _optional_cset_length,\n+                               _collection_set,\n@@ -704,2 +704,1 @@\n-                                                 size_t young_cset_length,\n-                                                 size_t optional_cset_length,\n+                                                 G1CollectionSet* collection_set,\n@@ -708,0 +707,1 @@\n+    _collection_set(collection_set),\n@@ -711,3 +711,1 @@\n-    _surviving_young_words_total(NEW_C_HEAP_ARRAY(size_t, young_cset_length + 1, mtGC)),\n-    _young_cset_length(young_cset_length),\n-    _optional_cset_length(optional_cset_length),\n+    _surviving_young_words_total(NEW_C_HEAP_ARRAY(size_t, collection_set->young_region_length() + 1, mtGC)),\n@@ -721,1 +719,1 @@\n-  memset(_surviving_young_words_total, 0, (young_cset_length + 1) * sizeof(size_t));\n+  memset(_surviving_young_words_total, 0, (collection_set->young_region_length() + 1) * sizeof(size_t));\n@@ -728,1 +726,0 @@\n-  _preserved_marks_set.assert_empty();\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.cpp","additions":11,"deletions":14,"binary":false,"changes":25,"status":"modified"},{"patch":"@@ -44,0 +44,1 @@\n+class G1CollectionSet;\n@@ -120,2 +121,1 @@\n-                       size_t young_cset_length,\n-                       size_t optional_cset_length,\n+                       G1CollectionSet* collection_set,\n@@ -235,0 +235,1 @@\n+  G1CollectionSet* _collection_set;\n@@ -239,2 +240,0 @@\n-  size_t _young_cset_length;\n-  size_t _optional_cset_length;\n@@ -248,2 +247,1 @@\n-                          size_t young_cset_length,\n-                          size_t optional_cset_length,\n+                          G1CollectionSet* collection_set,\n","filename":"src\/hotspot\/share\/gc\/g1\/g1ParScanThreadState.hpp","additions":4,"deletions":6,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -833,4 +833,0 @@\n-    \/\/ A successful scavenge should restart the GC time limit count which is\n-    \/\/ for full GC's.\n-    AdaptiveSizePolicy* size_policy = heap->size_policy();\n-    size_policy->reset_gc_overhead_limit_count();\n","filename":"src\/hotspot\/share\/gc\/serial\/defNewGeneration.cpp","additions":0,"deletions":4,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -47,0 +47,1 @@\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -148,0 +149,2 @@\n+\n+  _preserved_overflow_stack_set.init(1);\n@@ -155,1 +158,1 @@\n-  _preserved_overflow_stack.clear(true);\n+  _preserved_overflow_stack_set.reclaim();\n","filename":"src\/hotspot\/share\/gc\/serial\/genMarkSweep.cpp","additions":4,"deletions":1,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -48,1 +48,1 @@\n-Stack<PreservedMark, mtGC>    MarkSweep::_preserved_overflow_stack;\n+PreservedMarksSet       MarkSweep::_preserved_overflow_stack_set(false \/* in_c_heap *\/);\n@@ -146,8 +146,0 @@\n-void PreservedMark::adjust_pointer() {\n-  MarkSweep::adjust_pointer(&_obj);\n-}\n-\n-void PreservedMark::restore() {\n-  _obj->set_mark(_mark);\n-}\n-\n@@ -165,1 +157,1 @@\n-    _preserved_overflow_stack.push(PreservedMark(obj, mark));\n+    _preserved_overflow_stack_set.get()->push_always(obj, mark);\n@@ -227,1 +219,1 @@\n-    _preserved_marks[i].adjust_pointer();\n+    PreservedMarks::adjust_preserved_mark(_preserved_marks + i);\n@@ -231,5 +223,1 @@\n-  StackIterator<PreservedMark, mtGC> iter(_preserved_overflow_stack);\n-  while (!iter.is_empty()) {\n-    PreservedMark* p = iter.next_addr();\n-    p->adjust_pointer();\n-  }\n+  _preserved_overflow_stack_set.get()->adjust_during_full_gc();\n@@ -239,1 +227,1 @@\n-  log_trace(gc)(\"Restoring \" SIZE_FORMAT \" marks\", _preserved_count + _preserved_overflow_stack.size());\n+  log_trace(gc)(\"Restoring \" SIZE_FORMAT \" marks\", _preserved_count + _preserved_overflow_stack_set.get()->size());\n@@ -243,1 +231,1 @@\n-    _preserved_marks[i].restore();\n+    _preserved_marks[i].set_mark();\n@@ -247,4 +235,1 @@\n-  while (!_preserved_overflow_stack.is_empty()) {\n-    PreservedMark p = _preserved_overflow_stack.pop();\n-    p.restore();\n-  }\n+  _preserved_overflow_stack_set.restore(nullptr);\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.cpp","additions":7,"deletions":22,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -29,0 +29,1 @@\n+#include \"gc\/shared\/preservedMarks.inline.hpp\"\n@@ -102,1 +103,1 @@\n-  static Stack<PreservedMark, mtGC>      _preserved_overflow_stack;\n+  static PreservedMarksSet               _preserved_overflow_stack_set;\n@@ -192,11 +193,0 @@\n-class PreservedMark {\n-private:\n-  oop _obj;\n-  markWord _mark;\n-\n-public:\n-  PreservedMark(oop obj, markWord mark) : _obj(obj), _mark(mark) {}\n-  void adjust_pointer();\n-  void restore();\n-};\n-\n","filename":"src\/hotspot\/share\/gc\/serial\/markSweep.hpp","additions":2,"deletions":12,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -45,0 +45,1 @@\n+#include \"memory\/metaspace.hpp\"\n@@ -230,0 +231,4 @@\n+  if (!UseCompactObjectHeaders && !Metaspace::contains(object->klass_raw())) {\n+    return false;\n+  }\n+\n@@ -400,11 +405,0 @@\n-#ifndef PRODUCT\n-void CollectedHeap::check_for_non_bad_heap_word_value(HeapWord* addr, size_t size) {\n-  if (CheckMemoryInitialization && ZapUnusedHeapArea) {\n-    \/\/ please note mismatch between size (in 32\/64 bit words), and ju_addr that always point to a 32 bit word\n-    for (juint* ju_addr = reinterpret_cast<juint*>(addr); ju_addr < reinterpret_cast<juint*>(addr + size); ++ju_addr) {\n-      assert(*ju_addr == badHeapWordVal, \"Found non badHeapWordValue in pre-allocation check\");\n-    }\n-  }\n-}\n-#endif \/\/ PRODUCT\n-\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.cpp","additions":5,"deletions":11,"binary":false,"changes":16,"status":"modified"},{"patch":"@@ -98,0 +98,1 @@\n+  friend class DisableIsGCActiveMark; \/\/ Disable current IsGCActiveMark\n@@ -182,2 +183,0 @@\n-  virtual void check_for_non_bad_heap_word_value(HeapWord* addr, size_t size)\n-    PRODUCT_RETURN;\n","filename":"src\/hotspot\/share\/gc\/shared\/collectedHeap.hpp","additions":1,"deletions":2,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -99,2 +99,1 @@\n-  _soft_ref_gen_policy(),\n-  _size_policy(nullptr),\n+  _soft_ref_policy(),\n@@ -145,11 +144,0 @@\n-void GenCollectedHeap::initialize_size_policy(size_t init_eden_size,\n-                                              size_t init_promo_size,\n-                                              size_t init_survivor_size) {\n-  const double max_gc_pause_sec = ((double) MaxGCPauseMillis) \/ 1000.0;\n-  _size_policy = new AdaptiveSizePolicy(init_eden_size,\n-                                        init_promo_size,\n-                                        init_survivor_size,\n-                                        max_gc_pause_sec,\n-                                        GCTimeRatio);\n-}\n-\n@@ -200,4 +188,0 @@\n-  initialize_size_policy(def_new_gen->eden()->capacity(),\n-                         _old_gen->capacity(),\n-                         def_new_gen->from()->capacity());\n-\n@@ -284,6 +268,1 @@\n-                                              bool is_tlab,\n-                                              bool* gc_overhead_limit_was_exceeded) {\n-  \/\/ In general gc_overhead_limit_was_exceeded should be false so\n-  \/\/ set it so here and reset it to true only if the gc time\n-  \/\/ limit is being exceeded as checked below.\n-  *gc_overhead_limit_was_exceeded = false;\n+                                              bool is_tlab) {\n@@ -371,17 +350,0 @@\n-      \/\/ Allocation has failed and a collection\n-      \/\/ has been done.  If the gc time limit was exceeded the\n-      \/\/ this time, return null so that an out-of-memory\n-      \/\/ will be thrown.  Clear gc_overhead_limit_exceeded\n-      \/\/ so that the overhead exceeded does not persist.\n-\n-      const bool limit_exceeded = size_policy()->gc_overhead_limit_exceeded();\n-      const bool softrefs_clear = soft_ref_policy()->all_soft_refs_clear();\n-\n-      if (limit_exceeded && softrefs_clear) {\n-        *gc_overhead_limit_was_exceeded = true;\n-        size_policy()->set_gc_overhead_limit_exceeded(false);\n-        if (op.result() != nullptr) {\n-          CollectedHeap::fill_with_object(op.result(), size);\n-        }\n-        return nullptr;\n-      }\n@@ -424,2 +386,1 @@\n-                           false \/* is_tlab *\/,\n-                           gc_overhead_limit_was_exceeded);\n+                           false \/* is_tlab *\/);\n@@ -941,3 +902,1 @@\n-  bool gc_overhead_limit_was_exceeded;\n-                                       true \/* is_tlab *\/,\n-                                       &gc_overhead_limit_was_exceeded);\n+                                       true \/* is_tlab *\/);\n","filename":"src\/hotspot\/share\/gc\/shared\/genCollectedHeap.cpp","additions":4,"deletions":45,"binary":false,"changes":49,"status":"modified"},{"patch":"@@ -64,1 +64,0 @@\n-  void check_for_bad_heap_word_value() const;\n@@ -86,1 +85,0 @@\n-      verify_after();\n@@ -153,16 +151,0 @@\n-void MemAllocator::Allocation::verify_after() {\n-  NOT_PRODUCT(check_for_bad_heap_word_value();)\n-}\n-\n-void MemAllocator::Allocation::check_for_bad_heap_word_value() const {\n-  MemRegion obj_range = _allocator.obj_memory_range(obj());\n-  HeapWord* addr = obj_range.start();\n-  size_t size = obj_range.word_size();\n-  if (CheckMemoryInitialization && ZapUnusedHeapArea) {\n-    for (size_t slot = 0; slot < size; slot += 1) {\n-      assert((*(intptr_t*) (addr + slot)) != ((intptr_t) badHeapWordVal),\n-             \"Found badHeapWordValue in post-allocation check\");\n-    }\n-  }\n-}\n-\n@@ -263,1 +245,0 @@\n-  NOT_PRODUCT(Universe::heap()->check_for_non_bad_heap_word_value(mem, _word_size));\n","filename":"src\/hotspot\/share\/gc\/shared\/memAllocator.cpp","additions":0,"deletions":19,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -38,1 +38,1 @@\n-    const OopAndMarkWord elem = _stack.pop();\n+    const PreservedMark elem = _stack.pop();\n@@ -44,0 +44,7 @@\n+void PreservedMarks::adjust_preserved_mark(PreservedMark* elem) {\n+  oop obj = elem->get_oop();\n+  if (GCForwarding::is_forwarded(obj)) {\n+    elem->set_oop(GCForwarding::forwardee(obj));\n+  }\n+}\n+\n@@ -45,1 +52,1 @@\n-  StackIterator<OopAndMarkWord, mtGC> iter(_stack);\n+  StackIterator<PreservedMark, mtGC> iter(_stack);\n@@ -47,6 +54,2 @@\n-    OopAndMarkWord* elem = iter.next_addr();\n-\n-    oop obj = elem->get_oop();\n-    if (GCForwarding::is_forwarded(obj)) {\n-      elem->set_oop(GCForwarding::forwardee(obj));\n-    }\n+    PreservedMark* elem = iter.next_addr();\n+    adjust_preserved_mark(elem);\n","filename":"src\/hotspot\/share\/gc\/shared\/preservedMarks.cpp","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -599,0 +599,1 @@\n+    st->cr();\n@@ -1027,4 +1028,7 @@\n-  st->print_cr(\"EU=empty-uncommitted, EC=empty-committed, R=regular, H=humongous start, HC=humongous continuation, CS=collection set, T=trash, P=pinned\");\n-  st->print_cr(\"BTE=bottom\/top\/end, U=used, T=TLAB allocs, G=GCLAB allocs, S=shared allocs, L=live data\");\n-  st->print_cr(\"R=root, CP=critical pins, TAMS=top-at-mark-start, UWM=update watermark\");\n-  st->print_cr(\"SN=alloc sequence number\");\n+  st->print_cr(\"Region state: EU=empty-uncommitted, EC=empty-committed, R=regular, H=humongous start, HP=pinned humongous start\");\n+  st->print_cr(\"              HC=humongous continuation, CS=collection set, TR=trash, P=pinned, CSP=pinned collection set\");\n+  st->print_cr(\"BTE=bottom\/top\/end, TAMS=top-at-mark-start\");\n+  st->print_cr(\"UWM=update watermark, U=used\");\n+  st->print_cr(\"T=TLAB allocs, G=GCLAB allocs\");\n+  st->print_cr(\"S=shared allocs, L=live data\");\n+  st->print_cr(\"CP=critical pins\");\n@@ -1510,1 +1514,1 @@\n-      size_t cur = Atomic::fetch_and_add(&_index, stride, memory_order_relaxed);\n+      size_t cur = Atomic::fetch_then_add(&_index, stride, memory_order_relaxed);\n@@ -2122,0 +2126,1 @@\n+  st->cr();\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahHeap.cpp","additions":10,"deletions":5,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -525,1 +525,1 @@\n-      size_t v = Atomic::fetch_and_add(&_claimed, 1u, memory_order_relaxed);\n+      size_t v = Atomic::fetch_then_add(&_claimed, 1u, memory_order_relaxed);\n","filename":"src\/hotspot\/share\/gc\/shenandoah\/shenandoahVerifier.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -0,0 +1,583 @@\n+\/*\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"classfile\/javaClasses.hpp\"\n+#include \"gc\/x\/c2\/xBarrierSetC2.hpp\"\n+#include \"gc\/x\/xBarrierSet.hpp\"\n+#include \"gc\/x\/xBarrierSetAssembler.hpp\"\n+#include \"gc\/x\/xBarrierSetRuntime.hpp\"\n+#include \"opto\/arraycopynode.hpp\"\n+#include \"opto\/addnode.hpp\"\n+#include \"opto\/block.hpp\"\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/graphKit.hpp\"\n+#include \"opto\/machnode.hpp\"\n+#include \"opto\/macro.hpp\"\n+#include \"opto\/memnode.hpp\"\n+#include \"opto\/node.hpp\"\n+#include \"opto\/output.hpp\"\n+#include \"opto\/regalloc.hpp\"\n+#include \"opto\/rootnode.hpp\"\n+#include \"opto\/runtime.hpp\"\n+#include \"opto\/type.hpp\"\n+#include \"utilities\/growableArray.hpp\"\n+#include \"utilities\/macros.hpp\"\n+\n+class XBarrierSetC2State : public ArenaObj {\n+private:\n+  GrowableArray<XLoadBarrierStubC2*>* _stubs;\n+  Node_Array                          _live;\n+\n+public:\n+  XBarrierSetC2State(Arena* arena) :\n+    _stubs(new (arena) GrowableArray<XLoadBarrierStubC2*>(arena, 8,  0, nullptr)),\n+    _live(arena) {}\n+\n+  GrowableArray<XLoadBarrierStubC2*>* stubs() {\n+    return _stubs;\n+  }\n+\n+  RegMask* live(const Node* node) {\n+    if (!node->is_Mach()) {\n+      \/\/ Don't need liveness for non-MachNodes\n+      return nullptr;\n+    }\n+\n+    const MachNode* const mach = node->as_Mach();\n+    if (mach->barrier_data() == XLoadBarrierElided) {\n+      \/\/ Don't need liveness data for nodes without barriers\n+      return nullptr;\n+    }\n+\n+    RegMask* live = (RegMask*)_live[node->_idx];\n+    if (live == nullptr) {\n+      live = new (Compile::current()->comp_arena()->AmallocWords(sizeof(RegMask))) RegMask();\n+      _live.map(node->_idx, (Node*)live);\n+    }\n+\n+    return live;\n+  }\n+};\n+\n+static XBarrierSetC2State* barrier_set_state() {\n+  return reinterpret_cast<XBarrierSetC2State*>(Compile::current()->barrier_set_state());\n+}\n+\n+XLoadBarrierStubC2* XLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n+  XLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) XLoadBarrierStubC2(node, ref_addr, ref, tmp, barrier_data);\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->stubs()->append(stub);\n+  }\n+\n+  return stub;\n+}\n+\n+XLoadBarrierStubC2::XLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) :\n+    _node(node),\n+    _ref_addr(ref_addr),\n+    _ref(ref),\n+    _tmp(tmp),\n+    _barrier_data(barrier_data),\n+    _entry(),\n+    _continuation() {\n+  assert_different_registers(ref, ref_addr.base());\n+  assert_different_registers(ref, ref_addr.index());\n+}\n+\n+Address XLoadBarrierStubC2::ref_addr() const {\n+  return _ref_addr;\n+}\n+\n+Register XLoadBarrierStubC2::ref() const {\n+  return _ref;\n+}\n+\n+Register XLoadBarrierStubC2::tmp() const {\n+  return _tmp;\n+}\n+\n+address XLoadBarrierStubC2::slow_path() const {\n+  DecoratorSet decorators = DECORATORS_NONE;\n+  if (_barrier_data & XLoadBarrierStrong) {\n+    decorators |= ON_STRONG_OOP_REF;\n+  }\n+  if (_barrier_data & XLoadBarrierWeak) {\n+    decorators |= ON_WEAK_OOP_REF;\n+  }\n+  if (_barrier_data & XLoadBarrierPhantom) {\n+    decorators |= ON_PHANTOM_OOP_REF;\n+  }\n+  if (_barrier_data & XLoadBarrierNoKeepalive) {\n+    decorators |= AS_NO_KEEPALIVE;\n+  }\n+  return XBarrierSetRuntime::load_barrier_on_oop_field_preloaded_addr(decorators);\n+}\n+\n+RegMask& XLoadBarrierStubC2::live() const {\n+  RegMask* mask = barrier_set_state()->live(_node);\n+  assert(mask != nullptr, \"must be mach-node with barrier\");\n+  return *mask;\n+}\n+\n+Label* XLoadBarrierStubC2::entry() {\n+  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n+  \/\/ However, we still need to return a label that is not bound now, but\n+  \/\/ will eventually be bound. Any label will do, as it will only act as\n+  \/\/ a placeholder, so we return the _continuation label.\n+  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n+}\n+\n+Label* XLoadBarrierStubC2::continuation() {\n+  return &_continuation;\n+}\n+\n+void* XBarrierSetC2::create_barrier_state(Arena* comp_arena) const {\n+  return new (comp_arena) XBarrierSetC2State(comp_arena);\n+}\n+\n+void XBarrierSetC2::late_barrier_analysis() const {\n+  analyze_dominating_barriers();\n+  compute_liveness_at_stubs();\n+}\n+\n+void XBarrierSetC2::emit_stubs(CodeBuffer& cb) const {\n+  MacroAssembler masm(&cb);\n+  GrowableArray<XLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+\n+  for (int i = 0; i < stubs->length(); i++) {\n+    \/\/ Make sure there is enough space in the code buffer\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == nullptr) {\n+      ciEnv::current()->record_failure(\"CodeCache is full\");\n+      return;\n+    }\n+\n+    XBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+  }\n+\n+  masm.flush();\n+}\n+\n+int XBarrierSetC2::estimate_stub_size() const {\n+  Compile* const C = Compile::current();\n+  BufferBlob* const blob = C->output()->scratch_buffer_blob();\n+  GrowableArray<XLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  int size = 0;\n+\n+  for (int i = 0; i < stubs->length(); i++) {\n+    CodeBuffer cb(blob->content_begin(), (address)C->output()->scratch_locs_memory() - blob->content_begin());\n+    MacroAssembler masm(&cb);\n+    XBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+    size += cb.insts_size();\n+  }\n+\n+  return size;\n+}\n+\n+static void set_barrier_data(C2Access& access) {\n+  if (XBarrierSet::barrier_needed(access.decorators(), access.type())) {\n+    uint8_t barrier_data = 0;\n+\n+    if (access.decorators() & ON_PHANTOM_OOP_REF) {\n+      barrier_data |= XLoadBarrierPhantom;\n+    } else if (access.decorators() & ON_WEAK_OOP_REF) {\n+      barrier_data |= XLoadBarrierWeak;\n+    } else {\n+      barrier_data |= XLoadBarrierStrong;\n+    }\n+\n+    if (access.decorators() & AS_NO_KEEPALIVE) {\n+      barrier_data |= XLoadBarrierNoKeepalive;\n+    }\n+\n+    access.set_barrier_data(barrier_data);\n+  }\n+}\n+\n+Node* XBarrierSetC2::load_at_resolved(C2Access& access, const Type* val_type) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::load_at_resolved(access, val_type);\n+}\n+\n+Node* XBarrierSetC2::atomic_cmpxchg_val_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                    Node* new_val, const Type* val_type) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::atomic_cmpxchg_val_at_resolved(access, expected_val, new_val, val_type);\n+}\n+\n+Node* XBarrierSetC2::atomic_cmpxchg_bool_at_resolved(C2AtomicParseAccess& access, Node* expected_val,\n+                                                     Node* new_val, const Type* value_type) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::atomic_cmpxchg_bool_at_resolved(access, expected_val, new_val, value_type);\n+}\n+\n+Node* XBarrierSetC2::atomic_xchg_at_resolved(C2AtomicParseAccess& access, Node* new_val, const Type* val_type) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::atomic_xchg_at_resolved(access, new_val, val_type);\n+}\n+\n+bool XBarrierSetC2::array_copy_requires_gc_barriers(bool tightly_coupled_alloc, BasicType type,\n+                                                    bool is_clone, bool is_clone_instance,\n+                                                    ArrayCopyPhase phase) const {\n+  if (phase == ArrayCopyPhase::Parsing) {\n+    return false;\n+  }\n+  if (phase == ArrayCopyPhase::Optimization) {\n+    return is_clone_instance;\n+  }\n+  \/\/ else ArrayCopyPhase::Expansion\n+  return type == T_OBJECT || type == T_ARRAY;\n+}\n+\n+\/\/ This TypeFunc assumes a 64bit system\n+static const TypeFunc* clone_type() {\n+  \/\/ Create input type (domain)\n+  const Type** domain_fields = TypeTuple::fields(4);\n+  domain_fields[TypeFunc::Parms + 0] = TypeInstPtr::NOTNULL;  \/\/ src\n+  domain_fields[TypeFunc::Parms + 1] = TypeInstPtr::NOTNULL;  \/\/ dst\n+  domain_fields[TypeFunc::Parms + 2] = TypeLong::LONG;        \/\/ size lower\n+  domain_fields[TypeFunc::Parms + 3] = Type::HALF;            \/\/ size upper\n+  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n+\n+  \/\/ Create result type (range)\n+  const Type** range_fields = TypeTuple::fields(0);\n+  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n+\n+  return TypeFunc::make(domain, range);\n+}\n+\n+#define XTOP LP64_ONLY(COMMA phase->top())\n+\n+void XBarrierSetC2::clone_at_expansion(PhaseMacroExpand* phase, ArrayCopyNode* ac) const {\n+  Node* const src = ac->in(ArrayCopyNode::Src);\n+  const TypeAryPtr* ary_ptr = src->get_ptr_type()->isa_aryptr();\n+\n+  if (ac->is_clone_array() && ary_ptr != nullptr) {\n+    BasicType bt = ary_ptr->elem()->array_element_basic_type();\n+    if (is_reference_type(bt)) {\n+      \/\/ Clone object array\n+      bt = T_OBJECT;\n+    } else {\n+      \/\/ Clone primitive array\n+      bt = T_LONG;\n+    }\n+\n+    Node* ctrl = ac->in(TypeFunc::Control);\n+    Node* mem = ac->in(TypeFunc::Memory);\n+    Node* src = ac->in(ArrayCopyNode::Src);\n+    Node* src_offset = ac->in(ArrayCopyNode::SrcPos);\n+    Node* dest = ac->in(ArrayCopyNode::Dest);\n+    Node* dest_offset = ac->in(ArrayCopyNode::DestPos);\n+    Node* length = ac->in(ArrayCopyNode::Length);\n+\n+    if (bt == T_OBJECT) {\n+      \/\/ BarrierSetC2::clone sets the offsets via BarrierSetC2::arraycopy_payload_base_offset\n+      \/\/ which 8-byte aligns them to allow for word size copies. Make sure the offsets point\n+      \/\/ to the first element in the array when cloning object arrays. Otherwise, load\n+      \/\/ barriers are applied to parts of the header. Also adjust the length accordingly.\n+      assert(src_offset == dest_offset, \"should be equal\");\n+      jlong offset = src_offset->get_long();\n+      if (offset != arrayOopDesc::base_offset_in_bytes(T_OBJECT)) {\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers or with compact object headers\");\n+        assert((arrayOopDesc::base_offset_in_bytes(T_OBJECT) - offset) == BytesPerLong, \"unexpected offset\");\n+        length = phase->transform_later(new SubLNode(length, phase->longcon(1))); \/\/ Size is in longs\n+        src_offset = phase->longcon(arrayOopDesc::base_offset_in_bytes(T_OBJECT));\n+        dest_offset = src_offset;\n+      }\n+    }\n+    Node* payload_src = phase->basic_plus_adr(src, src_offset);\n+    Node* payload_dst = phase->basic_plus_adr(dest, dest_offset);\n+\n+    const char* copyfunc_name = \"arraycopy\";\n+    address     copyfunc_addr = phase->basictype2arraycopy(bt, nullptr, nullptr, true, copyfunc_name, true);\n+\n+    const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n+    const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n+\n+    Node* call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n+    phase->transform_later(call);\n+\n+    phase->igvn().replace_node(ac, call);\n+    return;\n+  }\n+\n+  \/\/ Clone instance\n+  Node* const ctrl       = ac->in(TypeFunc::Control);\n+  Node* const mem        = ac->in(TypeFunc::Memory);\n+  Node* const dst        = ac->in(ArrayCopyNode::Dest);\n+  Node* const size       = ac->in(ArrayCopyNode::Length);\n+\n+  assert(size->bottom_type()->is_long(), \"Should be long\");\n+\n+  \/\/ The native clone we are calling here expects the instance size in words\n+  \/\/ Add header\/offset size to payload size to get instance size.\n+  Node* const base_offset = phase->longcon(arraycopy_payload_base_offset(ac->is_clone_array()) >> LogBytesPerLong);\n+  Node* const full_size = phase->transform_later(new AddLNode(size, base_offset));\n+\n+  Node* const call = phase->make_leaf_call(ctrl,\n+                                           mem,\n+                                           clone_type(),\n+                                           XBarrierSetRuntime::clone_addr(),\n+                                           \"XBarrierSetRuntime::clone\",\n+                                           TypeRawPtr::BOTTOM,\n+                                           src,\n+                                           dst,\n+                                           full_size,\n+                                           phase->top());\n+  phase->transform_later(call);\n+  phase->igvn().replace_node(ac, call);\n+}\n+\n+#undef XTOP\n+\n+\/\/ == Dominating barrier elision ==\n+\n+static bool block_has_safepoint(const Block* block, uint from, uint to) {\n+  for (uint i = from; i < to; i++) {\n+    if (block->get_node(i)->is_MachSafePoint()) {\n+      \/\/ Safepoint found\n+      return true;\n+    }\n+  }\n+\n+  \/\/ Safepoint not found\n+  return false;\n+}\n+\n+static bool block_has_safepoint(const Block* block) {\n+  return block_has_safepoint(block, 0, block->number_of_nodes());\n+}\n+\n+static uint block_index(const Block* block, const Node* node) {\n+  for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+    if (block->get_node(j) == node) {\n+      return j;\n+    }\n+  }\n+  ShouldNotReachHere();\n+  return 0;\n+}\n+\n+void XBarrierSetC2::analyze_dominating_barriers() const {\n+  ResourceMark rm;\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+  Block_List worklist;\n+  Node_List mem_ops;\n+  Node_List barrier_loads;\n+\n+  \/\/ Step 1 - Find accesses, and track them in lists\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    const Block* const block = cfg->get_block(i);\n+    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+      const Node* const node = block->get_node(j);\n+      if (!node->is_Mach()) {\n+        continue;\n+      }\n+\n+      MachNode* const mach = node->as_Mach();\n+      switch (mach->ideal_Opcode()) {\n+      case Op_LoadP:\n+        if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n+          barrier_loads.push(mach);\n+        }\n+        if ((mach->barrier_data() & (XLoadBarrierStrong | XLoadBarrierNoKeepalive)) ==\n+            XLoadBarrierStrong) {\n+          mem_ops.push(mach);\n+        }\n+        break;\n+      case Op_CompareAndExchangeP:\n+      case Op_CompareAndSwapP:\n+      case Op_GetAndSetP:\n+        if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n+          barrier_loads.push(mach);\n+        }\n+      case Op_StoreP:\n+        mem_ops.push(mach);\n+        break;\n+\n+      default:\n+        break;\n+      }\n+    }\n+  }\n+\n+  \/\/ Step 2 - Find dominating accesses for each load\n+  for (uint i = 0; i < barrier_loads.size(); i++) {\n+    MachNode* const load = barrier_loads.at(i)->as_Mach();\n+    const TypePtr* load_adr_type = nullptr;\n+    intptr_t load_offset = 0;\n+    const Node* const load_obj = load->get_base_and_disp(load_offset, load_adr_type);\n+    Block* const load_block = cfg->get_block_for_node(load);\n+    const uint load_index = block_index(load_block, load);\n+\n+    for (uint j = 0; j < mem_ops.size(); j++) {\n+      MachNode* mem = mem_ops.at(j)->as_Mach();\n+      const TypePtr* mem_adr_type = nullptr;\n+      intptr_t mem_offset = 0;\n+      const Node* mem_obj = mem->get_base_and_disp(mem_offset, mem_adr_type);\n+      Block* mem_block = cfg->get_block_for_node(mem);\n+      uint mem_index = block_index(mem_block, mem);\n+\n+      if (load_obj == NodeSentinel || mem_obj == NodeSentinel ||\n+          load_obj == nullptr || mem_obj == nullptr ||\n+          load_offset < 0 || mem_offset < 0) {\n+        continue;\n+      }\n+\n+      if (mem_obj != load_obj || mem_offset != load_offset) {\n+        \/\/ Not the same addresses, not a candidate\n+        continue;\n+      }\n+\n+      if (load_block == mem_block) {\n+        \/\/ Earlier accesses in the same block\n+        if (mem_index < load_index && !block_has_safepoint(mem_block, mem_index + 1, load_index)) {\n+          load->set_barrier_data(XLoadBarrierElided);\n+        }\n+      } else if (mem_block->dominates(load_block)) {\n+        \/\/ Dominating block? Look around for safepoints\n+        ResourceMark rm;\n+        Block_List stack;\n+        VectorSet visited;\n+        stack.push(load_block);\n+        bool safepoint_found = block_has_safepoint(load_block);\n+        while (!safepoint_found && stack.size() > 0) {\n+          Block* block = stack.pop();\n+          if (visited.test_set(block->_pre_order)) {\n+            continue;\n+          }\n+          if (block_has_safepoint(block)) {\n+            safepoint_found = true;\n+            break;\n+          }\n+          if (block == mem_block) {\n+            continue;\n+          }\n+\n+          \/\/ Push predecessor blocks\n+          for (uint p = 1; p < block->num_preds(); ++p) {\n+            Block* pred = cfg->get_block_for_node(block->pred(p));\n+            stack.push(pred);\n+          }\n+        }\n+\n+        if (!safepoint_found) {\n+          load->set_barrier_data(XLoadBarrierElided);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+\/\/ == Reduced spilling optimization ==\n+\n+void XBarrierSetC2::compute_liveness_at_stubs() const {\n+  ResourceMark rm;\n+  Compile* const C = Compile::current();\n+  Arena* const A = Thread::current()->resource_area();\n+  PhaseCFG* const cfg = C->cfg();\n+  PhaseRegAlloc* const regalloc = C->regalloc();\n+  RegMask* const live = NEW_ARENA_ARRAY(A, RegMask, cfg->number_of_blocks() * sizeof(RegMask));\n+  XBarrierSetAssembler* const bs = XBarrierSet::assembler();\n+  Block_List worklist;\n+\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    new ((void*)(live + i)) RegMask();\n+    worklist.push(cfg->get_block(i));\n+  }\n+\n+  while (worklist.size() > 0) {\n+    const Block* const block = worklist.pop();\n+    RegMask& old_live = live[block->_pre_order];\n+    RegMask new_live;\n+\n+    \/\/ Initialize to union of successors\n+    for (uint i = 0; i < block->_num_succs; i++) {\n+      const uint succ_id = block->_succs[i]->_pre_order;\n+      new_live.OR(live[succ_id]);\n+    }\n+\n+    \/\/ Walk block backwards, computing liveness\n+    for (int i = block->number_of_nodes() - 1; i >= 0; --i) {\n+      const Node* const node = block->get_node(i);\n+\n+      \/\/ Remove def bits\n+      const OptoReg::Name first = bs->refine_register(node, regalloc->get_reg_first(node));\n+      const OptoReg::Name second = bs->refine_register(node, regalloc->get_reg_second(node));\n+      if (first != OptoReg::Bad) {\n+        new_live.Remove(first);\n+      }\n+      if (second != OptoReg::Bad) {\n+        new_live.Remove(second);\n+      }\n+\n+      \/\/ Add use bits\n+      for (uint j = 1; j < node->req(); ++j) {\n+        const Node* const use = node->in(j);\n+        const OptoReg::Name first = bs->refine_register(use, regalloc->get_reg_first(use));\n+        const OptoReg::Name second = bs->refine_register(use, regalloc->get_reg_second(use));\n+        if (first != OptoReg::Bad) {\n+          new_live.Insert(first);\n+        }\n+        if (second != OptoReg::Bad) {\n+          new_live.Insert(second);\n+        }\n+      }\n+\n+      \/\/ If this node tracks liveness, update it\n+      RegMask* const regs = barrier_set_state()->live(node);\n+      if (regs != nullptr) {\n+        regs->OR(new_live);\n+      }\n+    }\n+\n+    \/\/ Now at block top, see if we have any changes\n+    new_live.SUBTRACT(old_live);\n+    if (new_live.is_NotEmpty()) {\n+      \/\/ Liveness has refined, update and propagate to prior blocks\n+      old_live.OR(new_live);\n+      for (uint i = 1; i < block->num_preds(); ++i) {\n+        Block* const pred = cfg->get_block_for_node(block->pred(i));\n+        worklist.push(pred);\n+      }\n+    }\n+  }\n+}\n+\n+#ifndef PRODUCT\n+void XBarrierSetC2::dump_barrier_data(const MachNode* mach, outputStream* st) const {\n+  if ((mach->barrier_data() & XLoadBarrierStrong) != 0) {\n+    st->print(\"strong \");\n+  }\n+  if ((mach->barrier_data() & XLoadBarrierWeak) != 0) {\n+    st->print(\"weak \");\n+  }\n+  if ((mach->barrier_data() & XLoadBarrierPhantom) != 0) {\n+    st->print(\"phantom \");\n+  }\n+  if ((mach->barrier_data() & XLoadBarrierNoKeepalive) != 0) {\n+    st->print(\"nokeepalive \");\n+  }\n+}\n+#endif \/\/ !PRODUCT\n","filename":"src\/hotspot\/share\/gc\/x\/c2\/xBarrierSetC2.cpp","additions":583,"deletions":0,"binary":false,"changes":583,"status":"added"},{"patch":"@@ -0,0 +1,106 @@\n+\/*\n+ * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n+ *\n+ * This code is free software; you can redistribute it and\/or modify it\n+ * under the terms of the GNU General Public License version 2 only, as\n+ * published by the Free Software Foundation.\n+ *\n+ * This code is distributed in the hope that it will be useful, but WITHOUT\n+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+ * version 2 for more details (a copy is included in the LICENSE file that\n+ * accompanied this code).\n+ *\n+ * You should have received a copy of the GNU General Public License version\n+ * 2 along with this work; if not, write to the Free Software Foundation,\n+ * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n+ *\n+ * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n+ * or visit www.oracle.com if you need additional information or have any\n+ * questions.\n+ *\/\n+\n+#include \"precompiled.hpp\"\n+#include \"gc\/x\/xThreadLocalData.hpp\"\n+#include \"gc\/x\/xObjArrayAllocator.hpp\"\n+#include \"gc\/x\/xUtils.inline.hpp\"\n+#include \"oops\/arrayKlass.hpp\"\n+#include \"runtime\/interfaceSupport.inline.hpp\"\n+#include \"utilities\/debug.hpp\"\n+\n+XObjArrayAllocator::XObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread) :\n+    ObjArrayAllocator(klass, word_size, length, do_zero, thread) {}\n+\n+void XObjArrayAllocator::yield_for_safepoint() const {\n+  ThreadBlockInVM tbivm(JavaThread::cast(_thread));\n+}\n+\n+oop XObjArrayAllocator::initialize(HeapWord* mem) const {\n+  \/\/ ZGC specializes the initialization by performing segmented clearing\n+  \/\/ to allow shorter time-to-safepoints.\n+\n+  if (!_do_zero) {\n+    \/\/ No need for ZGC specialization\n+    return ObjArrayAllocator::initialize(mem);\n+  }\n+\n+  \/\/ A max segment size of 64K was chosen because microbenchmarking\n+  \/\/ suggested that it offered a good trade-off between allocation\n+  \/\/ time and time-to-safepoint\n+  const size_t segment_max = XUtils::bytes_to_words(64 * K);\n+  const BasicType element_type = ArrayKlass::cast(_klass)->element_type();\n+int base_offset = arrayOopDesc::base_offset_in_bytes(element_type);\n+\n+  \/\/ Clear leading 32 bit, if necessary.\n+  if (!is_aligned(base_offset, HeapWordSize)) {\n+    assert(is_aligned(base_offset, BytesPerInt), \"array base must be 32 bit aligned\");\n+    *reinterpret_cast<jint*>(reinterpret_cast<char*>(mem) + base_offset) = 0;\n+    base_offset += BytesPerInt;\n+  }\n+  assert(is_aligned(base_offset, HeapWordSize), \"remaining array base must be 64 bit aligned\");\n+\n+  const size_t header = heap_word_size(base_offset);\n+  const size_t payload_size = _word_size - header;\n+  if (payload_size <= segment_max) {\n+    \/\/ To small to use segmented clearing\n+    return ObjArrayAllocator::initialize(mem);\n+  }\n+\n+  \/\/ Segmented clearing\n+\n+  \/\/ The array is going to be exposed before it has been completely\n+  \/\/ cleared, therefore we can't expose the header at the end of this\n+  \/\/ function. Instead explicitly initialize it according to our needs.\n+  if (UseCompactObjectHeaders) {\n+    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+  } else {\n+    arrayOopDesc::set_mark(mem, markWord::prototype());\n+    arrayOopDesc::release_set_klass(mem, _klass);\n+  }\n+  assert(_length >= 0, \"length should be non-negative\");\n+  arrayOopDesc::set_length(mem, _length);\n+\n+  \/\/ Keep the array alive across safepoints through an invisible\n+  \/\/ root. Invisible roots are not visited by the heap itarator\n+  \/\/ and the marking logic will not attempt to follow its elements.\n+  \/\/ Relocation knows how to dodge iterating over such objects.\n+  XThreadLocalData::set_invisible_root(_thread, (oop*)&mem);\n+\n+  for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n+    \/\/ Calculate segment\n+    HeapWord* const start = (HeapWord*)(mem + header + processed);\n+    const size_t remaining = payload_size - processed;\n+    const size_t segment_size = MIN2(remaining, segment_max);\n+\n+    \/\/ Clear segment\n+    Copy::zero_to_words(start, segment_size);\n+\n+    \/\/ Safepoint\n+    yield_for_safepoint();\n+  }\n+\n+  XThreadLocalData::clear_invisible_root(_thread);\n+\n+  return cast_to_oop(mem);\n+}\n","filename":"src\/hotspot\/share\/gc\/x\/xObjArrayAllocator.cpp","additions":106,"deletions":0,"binary":false,"changes":106,"status":"added"},{"patch":"@@ -25,0 +25,1 @@\n+#include \"asm\/macroAssembler.hpp\"\n@@ -47,0 +48,74 @@\n+template<typename K, typename V, size_t _table_size>\n+class ZArenaHashtable : public ResourceObj {\n+  class ZArenaHashtableEntry : public ResourceObj {\n+  public:\n+    ZArenaHashtableEntry* _next;\n+    K _key;\n+    V _value;\n+  };\n+\n+  static const size_t _table_mask = _table_size - 1;\n+\n+  Arena* _arena;\n+  ZArenaHashtableEntry* _table[_table_size];\n+\n+public:\n+  class Iterator {\n+    ZArenaHashtable* _table;\n+    ZArenaHashtableEntry* _current_entry;\n+    size_t _current_index;\n+\n+  public:\n+    Iterator(ZArenaHashtable* table)\n+      : _table(table),\n+        _current_entry(table->_table[0]),\n+        _current_index(0) {\n+      if (_current_entry == nullptr) {\n+        next();\n+      }\n+    }\n+\n+    bool has_next() { return _current_entry != nullptr; }\n+    K key()         { return _current_entry->_key; }\n+    V value()       { return _current_entry->_value; }\n+\n+    void next() {\n+      if (_current_entry != nullptr) {\n+        _current_entry = _current_entry->_next;\n+      }\n+      while (_current_entry == nullptr && ++_current_index < _table_size) {\n+        _current_entry = _table->_table[_current_index];\n+      }\n+    }\n+  };\n+\n+  ZArenaHashtable(Arena* arena)\n+    : _arena(arena),\n+      _table() {\n+    Copy::zero_to_bytes(&_table, sizeof(_table));\n+  }\n+\n+  void add(K key, V value) {\n+    ZArenaHashtableEntry* entry = new (_arena) ZArenaHashtableEntry();\n+    entry->_key = key;\n+    entry->_value = value;\n+    entry->_next = _table[key & _table_mask];\n+    _table[key & _table_mask] = entry;\n+  }\n+\n+  V* get(K key) const {\n+    for (ZArenaHashtableEntry* e = _table[key & _table_mask]; e != nullptr; e = e->_next) {\n+      if (e->_key == key) {\n+        return &(e->_value);\n+      }\n+    }\n+    return nullptr;\n+  }\n+\n+  Iterator iterator() {\n+    return Iterator(this);\n+  }\n+};\n+\n+typedef ZArenaHashtable<intptr_t, bool, 4> ZOffsetTable;\n+\n@@ -49,2 +124,4 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* _stubs;\n-  Node_Array                          _live;\n+  GrowableArray<ZBarrierStubC2*>* _stubs;\n+  Node_Array                      _live;\n+  int                             _trampoline_stubs_count;\n+  int                             _stubs_start_offset;\n@@ -53,3 +130,5 @@\n-  ZBarrierSetC2State(Arena* arena) :\n-    _stubs(new (arena) GrowableArray<ZLoadBarrierStubC2*>(arena, 8,  0, NULL)),\n-    _live(arena) {}\n+  ZBarrierSetC2State(Arena* arena)\n+    : _stubs(new (arena) GrowableArray<ZBarrierStubC2*>(arena, 8,  0, nullptr)),\n+      _live(arena),\n+      _trampoline_stubs_count(0),\n+      _stubs_start_offset(0) {}\n@@ -57,1 +136,1 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* stubs() {\n+  GrowableArray<ZBarrierStubC2*>* stubs() {\n@@ -64,1 +143,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -68,1 +147,1 @@\n-    if (mach->barrier_data() == ZLoadBarrierElided) {\n+    if (mach->barrier_data() == ZBarrierElided) {\n@@ -70,1 +149,1 @@\n-      return NULL;\n+      return nullptr;\n@@ -74,1 +153,1 @@\n-    if (live == NULL) {\n+    if (live == nullptr) {\n@@ -81,0 +160,17 @@\n+\n+  void inc_trampoline_stubs_count() {\n+    assert(_trampoline_stubs_count != INT_MAX, \"Overflow\");\n+    ++_trampoline_stubs_count;\n+  }\n+\n+  int trampoline_stubs_count() {\n+    return _trampoline_stubs_count;\n+  }\n+\n+  void set_stubs_start_offset(int offset) {\n+    _stubs_start_offset = offset;\n+  }\n+\n+  int stubs_start_offset() {\n+    return _stubs_start_offset;\n+  }\n@@ -87,2 +183,1 @@\n-ZLoadBarrierStubC2* ZLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) {\n-  ZLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZLoadBarrierStubC2(node, ref_addr, ref, tmp, barrier_data);\n+void ZBarrierStubC2::register_stub(ZBarrierStubC2* stub) {\n@@ -92,0 +187,44 @@\n+}\n+\n+void ZBarrierStubC2::inc_trampoline_stubs_count() {\n+  if (!Compile::current()->output()->in_scratch_emit_size()) {\n+    barrier_set_state()->inc_trampoline_stubs_count();\n+  }\n+}\n+\n+int ZBarrierStubC2::trampoline_stubs_count() {\n+  return barrier_set_state()->trampoline_stubs_count();\n+}\n+\n+int ZBarrierStubC2::stubs_start_offset() {\n+  return barrier_set_state()->stubs_start_offset();\n+}\n+\n+ZBarrierStubC2::ZBarrierStubC2(const MachNode* node)\n+  : _node(node),\n+    _entry(),\n+    _continuation() {}\n+\n+Register ZBarrierStubC2::result() const {\n+  return noreg;\n+}\n+\n+RegMask& ZBarrierStubC2::live() const {\n+  return *barrier_set_state()->live(_node);\n+}\n+\n+Label* ZBarrierStubC2::entry() {\n+  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n+  \/\/ However, we still need to return a label that is not bound now, but\n+  \/\/ will eventually be bound. Any eventually bound label will do, as it\n+  \/\/ will only act as a placeholder, so we return the _continuation label.\n+  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n+}\n+\n+Label* ZBarrierStubC2::continuation() {\n+  return &_continuation;\n+}\n+\n+ZLoadBarrierStubC2* ZLoadBarrierStubC2::create(const MachNode* node, Address ref_addr, Register ref) {\n+  ZLoadBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZLoadBarrierStubC2(node, ref_addr, ref);\n+  register_stub(stub);\n@@ -96,2 +235,2 @@\n-ZLoadBarrierStubC2::ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref, Register tmp, uint8_t barrier_data) :\n-    _node(node),\n+ZLoadBarrierStubC2::ZLoadBarrierStubC2(const MachNode* node, Address ref_addr, Register ref)\n+  : ZBarrierStubC2(node),\n@@ -99,5 +238,1 @@\n-    _ref(ref),\n-    _tmp(tmp),\n-    _barrier_data(barrier_data),\n-    _entry(),\n-    _continuation() {\n+    _ref(ref) {\n@@ -116,2 +251,2 @@\n-Register ZLoadBarrierStubC2::tmp() const {\n-  return _tmp;\n+Register ZLoadBarrierStubC2::result() const {\n+  return ref();\n@@ -121,0 +256,1 @@\n+  const uint8_t barrier_data = _node->barrier_data();\n@@ -122,1 +258,1 @@\n-  if (_barrier_data & ZLoadBarrierStrong) {\n+  if (barrier_data & ZBarrierStrong) {\n@@ -125,1 +261,1 @@\n-  if (_barrier_data & ZLoadBarrierWeak) {\n+  if (barrier_data & ZBarrierWeak) {\n@@ -128,1 +264,1 @@\n-  if (_barrier_data & ZLoadBarrierPhantom) {\n+  if (barrier_data & ZBarrierPhantom) {\n@@ -131,1 +267,1 @@\n-  if (_barrier_data & ZLoadBarrierNoKeepalive) {\n+  if (barrier_data & ZBarrierNoKeepalive) {\n@@ -137,4 +273,2 @@\n-RegMask& ZLoadBarrierStubC2::live() const {\n-  RegMask* mask = barrier_set_state()->live(_node);\n-  assert(mask != NULL, \"must be mach-node with barrier\");\n-  return *mask;\n+void ZLoadBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, static_cast<ZLoadBarrierStubC2*>(this));\n@@ -143,6 +277,5 @@\n-Label* ZLoadBarrierStubC2::entry() {\n-  \/\/ The _entry will never be bound when in_scratch_emit_size() is true.\n-  \/\/ However, we still need to return a label that is not bound now, but\n-  \/\/ will eventually be bound. Any label will do, as it will only act as\n-  \/\/ a placeholder, so we return the _continuation label.\n-  return Compile::current()->output()->in_scratch_emit_size() ? &_continuation : &_entry;\n+ZStoreBarrierStubC2* ZStoreBarrierStubC2::create(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic) {\n+  ZStoreBarrierStubC2* const stub = new (Compile::current()->comp_arena()) ZStoreBarrierStubC2(node, ref_addr, new_zaddress, new_zpointer, is_native, is_atomic);\n+  register_stub(stub);\n+\n+  return stub;\n@@ -151,2 +284,34 @@\n-Label* ZLoadBarrierStubC2::continuation() {\n-  return &_continuation;\n+ZStoreBarrierStubC2::ZStoreBarrierStubC2(const MachNode* node, Address ref_addr, Register new_zaddress, Register new_zpointer, bool is_native, bool is_atomic)\n+  : ZBarrierStubC2(node),\n+    _ref_addr(ref_addr),\n+    _new_zaddress(new_zaddress),\n+    _new_zpointer(new_zpointer),\n+    _is_native(is_native),\n+    _is_atomic(is_atomic) {}\n+\n+Address ZStoreBarrierStubC2::ref_addr() const {\n+  return _ref_addr;\n+}\n+\n+Register ZStoreBarrierStubC2::new_zaddress() const {\n+  return _new_zaddress;\n+}\n+\n+Register ZStoreBarrierStubC2::new_zpointer() const {\n+  return _new_zpointer;\n+}\n+\n+bool ZStoreBarrierStubC2::is_native() const {\n+  return _is_native;\n+}\n+\n+bool ZStoreBarrierStubC2::is_atomic() const {\n+  return _is_atomic;\n+}\n+\n+Register ZStoreBarrierStubC2::result() const {\n+  return noreg;\n+}\n+\n+void ZStoreBarrierStubC2::emit_code(MacroAssembler& masm) {\n+  ZBarrierSet::assembler()->generate_c2_store_barrier_stub(&masm, static_cast<ZStoreBarrierStubC2*>(this));\n@@ -160,1 +325,1 @@\n-  analyze_dominating_barriers();\n+  analyze_dominating_barriers();\n@@ -166,1 +331,2 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  GrowableArray<ZBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  barrier_set_state()->set_stubs_start_offset(masm.offset());\n@@ -170,1 +336,1 @@\n-    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == NULL) {\n+    if (cb.insts()->maybe_expand_to_ensure_remaining(PhaseOutput::MAX_inst_size) && cb.blob() == nullptr) {\n@@ -175,1 +341,1 @@\n-    ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+    stubs->at(i)->emit_code(masm);\n@@ -184,1 +350,1 @@\n-  GrowableArray<ZLoadBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n+  GrowableArray<ZBarrierStubC2*>* const stubs = barrier_set_state()->stubs();\n@@ -190,1 +356,1 @@\n-    ZBarrierSet::assembler()->generate_c2_load_barrier_stub(&masm, stubs->at(i));\n+    stubs->at(i)->emit_code(masm);\n@@ -198,2 +364,3 @@\n-  if (ZBarrierSet::barrier_needed(access.decorators(), access.type())) {\n-    uint8_t barrier_data = 0;\n+  if (!ZBarrierSet::barrier_needed(access.decorators(), access.type())) {\n+    return;\n+  }\n@@ -201,7 +368,4 @@\n-    if (access.decorators() & ON_PHANTOM_OOP_REF) {\n-      barrier_data |= ZLoadBarrierPhantom;\n-    } else if (access.decorators() & ON_WEAK_OOP_REF) {\n-      barrier_data |= ZLoadBarrierWeak;\n-    } else {\n-      barrier_data |= ZLoadBarrierStrong;\n-    }\n+  if (access.decorators() & C2_TIGHTLY_COUPLED_ALLOC) {\n+    access.set_barrier_data(ZBarrierElided);\n+    return;\n+  }\n@@ -209,3 +373,9 @@\n-    if (access.decorators() & AS_NO_KEEPALIVE) {\n-      barrier_data |= ZLoadBarrierNoKeepalive;\n-    }\n+  uint8_t barrier_data = 0;\n+\n+  if (access.decorators() & ON_PHANTOM_OOP_REF) {\n+    barrier_data |= ZBarrierPhantom;\n+  } else if (access.decorators() & ON_WEAK_OOP_REF) {\n+    barrier_data |= ZBarrierWeak;\n+  } else {\n+    barrier_data |= ZBarrierStrong;\n+  }\n@@ -213,1 +383,2 @@\n-    access.set_barrier_data(barrier_data);\n+  if (access.decorators() & IN_NATIVE) {\n+    barrier_data |= ZBarrierNative;\n@@ -215,0 +386,11 @@\n+\n+  if (access.decorators() & AS_NO_KEEPALIVE) {\n+    barrier_data |= ZBarrierNoKeepalive;\n+  }\n+\n+  access.set_barrier_data(barrier_data);\n+}\n+\n+Node* ZBarrierSetC2::store_at_resolved(C2Access& access, C2AccessValue& val) const {\n+  set_barrier_data(access);\n+  return BarrierSetC2::store_at_resolved(access, val);\n@@ -255,1 +437,1 @@\n-  const Type** domain_fields = TypeTuple::fields(4);\n+  const Type** const domain_fields = TypeTuple::fields(4);\n@@ -260,1 +442,1 @@\n-  const TypeTuple* domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n+  const TypeTuple* const domain = TypeTuple::make(TypeFunc::Parms + 4, domain_fields);\n@@ -263,2 +445,2 @@\n-  const Type** range_fields = TypeTuple::fields(0);\n-  const TypeTuple* range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n+  const Type** const range_fields = TypeTuple::fields(0);\n+  const TypeTuple* const range = TypeTuple::make(TypeFunc::Parms + 0, range_fields);\n@@ -273,1 +455,1 @@\n-  const TypeAryPtr* ary_ptr = src->get_ptr_type()->isa_aryptr();\n+  const TypeAryPtr* const ary_ptr = src->get_ptr_type()->isa_aryptr();\n@@ -275,1 +457,1 @@\n-  if (ac->is_clone_array() && ary_ptr != NULL) {\n+  if (ac->is_clone_array() && ary_ptr != nullptr) {\n@@ -285,3 +467,3 @@\n-    Node* ctrl = ac->in(TypeFunc::Control);\n-    Node* mem = ac->in(TypeFunc::Memory);\n-    Node* src = ac->in(ArrayCopyNode::Src);\n+    Node* const ctrl = ac->in(TypeFunc::Control);\n+    Node* const mem = ac->in(TypeFunc::Memory);\n+    Node* const src = ac->in(ArrayCopyNode::Src);\n@@ -289,1 +471,1 @@\n-    Node* dest = ac->in(ArrayCopyNode::Dest);\n+    Node* const dest = ac->in(ArrayCopyNode::Dest);\n@@ -299,1 +481,1 @@\n-      jlong offset = src_offset->get_long();\n+      const jlong offset = src_offset->get_long();\n@@ -301,0 +483,1 @@\n+        assert(!UseCompressedClassPointers || UseCompactObjectHeaders, \"should only happen without compressed class pointers or with compact object headers\");\n@@ -307,2 +490,2 @@\n-    Node* payload_src = phase->basic_plus_adr(src, src_offset);\n-    Node* payload_dst = phase->basic_plus_adr(dest, dest_offset);\n+    Node* const payload_src = phase->basic_plus_adr(src, src_offset);\n+    Node* const payload_dst = phase->basic_plus_adr(dest, dest_offset);\n@@ -310,2 +493,2 @@\n-    const char* copyfunc_name = \"arraycopy\";\n-    address     copyfunc_addr = phase->basictype2arraycopy(bt, NULL, NULL, true, copyfunc_name, true);\n+    const char*   copyfunc_name = \"arraycopy\";\n+    const address copyfunc_addr = phase->basictype2arraycopy(bt, nullptr, nullptr, true, copyfunc_name, true);\n@@ -313,2 +496,2 @@\n-    const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;\n-    const TypeFunc* call_type = OptoRuntime::fast_arraycopy_Type();\n+    const TypePtr* const raw_adr_type = TypeRawPtr::BOTTOM;\n+    const TypeFunc* const call_type = OptoRuntime::fast_arraycopy_Type();\n@@ -316,1 +499,1 @@\n-    Node* call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n+    Node* const call = phase->make_leaf_call(ctrl, mem, call_type, copyfunc_addr, copyfunc_name, raw_adr_type, payload_src, payload_dst, length XTOP);\n@@ -380,7 +563,19 @@\n-void ZBarrierSetC2::analyze_dominating_barriers() const {\n-  ResourceMark rm;\n-  Compile* const C = Compile::current();\n-  PhaseCFG* const cfg = C->cfg();\n-  Block_List worklist;\n-  Node_List mem_ops;\n-  Node_List barrier_loads;\n+\/\/ Look through various node aliases\n+static const Node* look_through_node(const Node* node) {\n+  while (node != nullptr) {\n+    const Node* new_node = node;\n+    if (node->is_Mach()) {\n+      const MachNode* const node_mach = node->as_Mach();\n+      if (node_mach->ideal_Opcode() == Op_CheckCastPP) {\n+        new_node = node->in(1);\n+      }\n+      if (node_mach->is_SpillCopy()) {\n+        new_node = node->in(1);\n+      }\n+    }\n+    if (new_node == node || new_node == nullptr) {\n+      break;\n+    } else {\n+      node = new_node;\n+    }\n+  }\n@@ -388,6 +583,62 @@\n-  \/\/ Step 1 - Find accesses, and track them in lists\n-  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n-    const Block* const block = cfg->get_block(i);\n-    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n-      const Node* const node = block->get_node(j);\n-      if (!node->is_Mach()) {\n+  return node;\n+}\n+\n+\/\/ Whether the given offset is undefined.\n+static bool is_undefined(intptr_t offset) {\n+  return offset == Type::OffsetTop;\n+}\n+\n+\/\/ Whether the given offset is unknown.\n+static bool is_unknown(intptr_t offset) {\n+  return offset == Type::OffsetBot;\n+}\n+\n+\/\/ Whether the given offset is concrete (defined and compile-time known).\n+static bool is_concrete(intptr_t offset) {\n+  return !is_undefined(offset) && !is_unknown(offset);\n+}\n+\n+\/\/ Compute base + offset components of the memory address accessed by mach.\n+\/\/ Return a node representing the base address, or null if the base cannot be\n+\/\/ found or the offset is undefined or a concrete negative value. If a non-null\n+\/\/ base is returned, the offset is a concrete, nonnegative value or unknown.\n+static const Node* get_base_and_offset(const MachNode* mach, intptr_t& offset) {\n+  const TypePtr* adr_type = nullptr;\n+  offset = 0;\n+  const Node* base = mach->get_base_and_disp(offset, adr_type);\n+\n+  if (base == nullptr || base == NodeSentinel) {\n+    return nullptr;\n+  }\n+\n+  if (offset == 0 && base->is_Mach() && base->as_Mach()->ideal_Opcode() == Op_AddP) {\n+    \/\/ The memory address is computed by 'base' and fed to 'mach' via an\n+    \/\/ indirect memory operand (indicated by offset == 0). The ultimate base and\n+    \/\/ offset can be fetched directly from the inputs and Ideal type of 'base'.\n+    offset = base->bottom_type()->isa_oopptr()->offset();\n+    \/\/ Even if 'base' is not an Ideal AddP node anymore, Matcher::ReduceInst()\n+    \/\/ guarantees that the base address is still available at the same slot.\n+    base = base->in(AddPNode::Base);\n+    assert(base != nullptr, \"\");\n+  }\n+\n+  if (is_undefined(offset) || (is_concrete(offset) && offset < 0)) {\n+    return nullptr;\n+  }\n+\n+  return look_through_node(base);\n+}\n+\n+\/\/ Whether a phi node corresponds to an array allocation.\n+\/\/ This test is incomplete: in some edge cases, it might return false even\n+\/\/ though the node does correspond to an array allocation.\n+static bool is_array_allocation(const Node* phi) {\n+  precond(phi->is_Phi());\n+  \/\/ Check whether phi has a successor cast (CheckCastPP) to Java array pointer,\n+  \/\/ possibly below spill copies and other cast nodes. Limit the exploration to\n+  \/\/ a single path from the phi node consisting of these node types.\n+  const Node* current = phi;\n+  while (true) {\n+    const Node* next = nullptr;\n+    for (DUIterator_Fast imax, i = current->fast_outs(imax); i < imax; i++) {\n+      if (!current->fast_out(i)->isa_Mach()) {\n@@ -396,10 +647,5 @@\n-\n-      MachNode* const mach = node->as_Mach();\n-      switch (mach->ideal_Opcode()) {\n-      case Op_LoadP:\n-        if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n-          barrier_loads.push(mach);\n-        }\n-        if ((mach->barrier_data() & (ZLoadBarrierStrong | ZLoadBarrierNoKeepalive)) ==\n-            ZLoadBarrierStrong) {\n-          mem_ops.push(mach);\n+      const MachNode* succ = current->fast_out(i)->as_Mach();\n+      if (succ->ideal_Opcode() == Op_CheckCastPP) {\n+        if (succ->get_ptr_type()->isa_aryptr()) {\n+          \/\/ Cast to Java array pointer: phi corresponds to an array allocation.\n+          return true;\n@@ -407,13 +653,5 @@\n-        break;\n-      case Op_CompareAndExchangeP:\n-      case Op_CompareAndSwapP:\n-      case Op_GetAndSetP:\n-        if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n-          barrier_loads.push(mach);\n-        }\n-      case Op_StoreP:\n-        mem_ops.push(mach);\n-        break;\n-\n-      default:\n-        break;\n+        \/\/ Other cast: record as candidate for further exploration.\n+        next = succ;\n+      } else if (succ->is_SpillCopy() && next == nullptr) {\n+        \/\/ Spill copy, and no better candidate found: record as candidate.\n+        next = succ;\n@@ -422,0 +660,7 @@\n+    if (next == nullptr) {\n+      \/\/ No evidence found that phi corresponds to an array allocation, and no\n+      \/\/ candidates available to continue exploring.\n+      return false;\n+    }\n+    \/\/ Continue exploring from the best candidate found.\n+    current = next;\n@@ -423,0 +668,2 @@\n+  ShouldNotReachHere();\n+}\n@@ -424,16 +671,25 @@\n-  \/\/ Step 2 - Find dominating accesses for each load\n-  for (uint i = 0; i < barrier_loads.size(); i++) {\n-    MachNode* const load = barrier_loads.at(i)->as_Mach();\n-    const TypePtr* load_adr_type = NULL;\n-    intptr_t load_offset = 0;\n-    const Node* const load_obj = load->get_base_and_disp(load_offset, load_adr_type);\n-    Block* const load_block = cfg->get_block_for_node(load);\n-    const uint load_index = block_index(load_block, load);\n-\n-    for (uint j = 0; j < mem_ops.size(); j++) {\n-      MachNode* mem = mem_ops.at(j)->as_Mach();\n-      const TypePtr* mem_adr_type = NULL;\n-      intptr_t mem_offset = 0;\n-      const Node* mem_obj = mem->get_base_and_disp(mem_offset, mem_adr_type);\n-      Block* mem_block = cfg->get_block_for_node(mem);\n-      uint mem_index = block_index(mem_block, mem);\n+\/\/ Match the phi node that connects a TLAB allocation fast path with its slowpath\n+static bool is_allocation(const Node* node) {\n+  if (node->req() != 3) {\n+    return false;\n+  }\n+  const Node* const fast_node = node->in(2);\n+  if (!fast_node->is_Mach()) {\n+    return false;\n+  }\n+  const MachNode* const fast_mach = fast_node->as_Mach();\n+  if (fast_mach->ideal_Opcode() != Op_LoadP) {\n+    return false;\n+  }\n+  const TypePtr* const adr_type = nullptr;\n+  intptr_t offset;\n+  const Node* const base = get_base_and_offset(fast_mach, offset);\n+  if (base == nullptr || !base->is_Mach() || !is_concrete(offset)) {\n+    return false;\n+  }\n+  const MachNode* const base_mach = base->as_Mach();\n+  if (base_mach->ideal_Opcode() != Op_ThreadLocal) {\n+    return false;\n+  }\n+  return offset == in_bytes(Thread::tlab_top_offset());\n+}\n@@ -441,5 +697,3 @@\n-      if (load_obj == NodeSentinel || mem_obj == NodeSentinel ||\n-          load_obj == NULL || mem_obj == NULL ||\n-          load_offset < 0 || mem_offset < 0) {\n-        continue;\n-      }\n+static void elide_mach_barrier(MachNode* mach) {\n+  mach->set_barrier_data(ZBarrierElided);\n+}\n@@ -447,3 +701,50 @@\n-      if (mem_obj != load_obj || mem_offset != load_offset) {\n-        \/\/ Not the same addresses, not a candidate\n-        continue;\n+void ZBarrierSetC2::analyze_dominating_barriers_impl(Node_List& accesses, Node_List& access_dominators) const {\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+\n+  for (uint i = 0; i < accesses.size(); i++) {\n+    MachNode* const access = accesses.at(i)->as_Mach();\n+    intptr_t access_offset;\n+    const Node* const access_obj = get_base_and_offset(access, access_offset);\n+    Block* const access_block = cfg->get_block_for_node(access);\n+    const uint access_index = block_index(access_block, access);\n+\n+    if (access_obj == nullptr) {\n+      \/\/ No information available\n+      continue;\n+    }\n+\n+    for (uint j = 0; j < access_dominators.size(); j++) {\n+     const  Node* const mem = access_dominators.at(j);\n+      if (mem->is_Phi()) {\n+        \/\/ Allocation node\n+        if (mem != access_obj) {\n+          continue;\n+        }\n+        if (is_unknown(access_offset) && !is_array_allocation(mem)) {\n+          \/\/ The accessed address has an unknown offset, but the allocated\n+          \/\/ object cannot be determined to be an array. Avoid eliding in this\n+          \/\/ case, to be on the safe side.\n+          continue;\n+        }\n+        assert((is_concrete(access_offset) && access_offset >= 0) || (is_unknown(access_offset) && is_array_allocation(mem)),\n+               \"candidate allocation-dominated access offsets must be either concrete and nonnegative, or unknown (for array allocations only)\");\n+      } else {\n+        \/\/ Access node\n+        const MachNode* const mem_mach = mem->as_Mach();\n+        intptr_t mem_offset;\n+        const Node* const mem_obj = get_base_and_offset(mem_mach, mem_offset);\n+\n+        if (mem_obj == nullptr ||\n+            !is_concrete(access_offset) ||\n+            !is_concrete(mem_offset)) {\n+          \/\/ No information available\n+          continue;\n+        }\n+\n+        if (mem_obj != access_obj || mem_offset != access_offset) {\n+          \/\/ Not the same addresses, not a candidate\n+          continue;\n+        }\n+        assert(is_concrete(access_offset) && access_offset >= 0,\n+               \"candidate non-allocation-dominated access offsets must be concrete and nonnegative\");\n@@ -452,1 +753,4 @@\n-      if (load_block == mem_block) {\n+      Block* mem_block = cfg->get_block_for_node(mem);\n+      const uint mem_index = block_index(mem_block, mem);\n+\n+      if (access_block == mem_block) {\n@@ -454,2 +758,2 @@\n-        if (mem_index < load_index && !block_has_safepoint(mem_block, mem_index + 1, load_index)) {\n-          load->set_barrier_data(ZLoadBarrierElided);\n+        if (mem_index < access_index && !block_has_safepoint(mem_block, mem_index + 1, access_index)) {\n+          elide_mach_barrier(access);\n@@ -457,1 +761,1 @@\n-      } else if (mem_block->dominates(load_block)) {\n+      } else if (mem_block->dominates(access_block)) {\n@@ -462,2 +766,2 @@\n-        stack.push(load_block);\n-        bool safepoint_found = block_has_safepoint(load_block);\n+        stack.push(access_block);\n+        bool safepoint_found = block_has_safepoint(access_block);\n@@ -465,1 +769,1 @@\n-          Block* block = stack.pop();\n+          const Block* const block = stack.pop();\n@@ -479,1 +783,1 @@\n-            Block* pred = cfg->get_block_for_node(block->pred(p));\n+            Block* const pred = cfg->get_block_for_node(block->pred(p));\n@@ -485,1 +789,48 @@\n-          load->set_barrier_data(ZLoadBarrierElided);\n+          elide_mach_barrier(access);\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+void ZBarrierSetC2::analyze_dominating_barriers() const {\n+  ResourceMark rm;\n+  Compile* const C = Compile::current();\n+  PhaseCFG* const cfg = C->cfg();\n+\n+  Node_List loads;\n+  Node_List load_dominators;\n+\n+  Node_List stores;\n+  Node_List store_dominators;\n+\n+  Node_List atomics;\n+  Node_List atomic_dominators;\n+\n+  \/\/ Step 1 - Find accesses and allocations, and track them in lists\n+  for (uint i = 0; i < cfg->number_of_blocks(); ++i) {\n+    const Block* const block = cfg->get_block(i);\n+    for (uint j = 0; j < block->number_of_nodes(); ++j) {\n+      Node* const node = block->get_node(j);\n+      if (node->is_Phi()) {\n+        if (is_allocation(node)) {\n+          load_dominators.push(node);\n+          store_dominators.push(node);\n+          \/\/ An allocation can't be considered to \"dominate\" an atomic operation.\n+          \/\/ For example a CAS requires the memory location to be store-good.\n+          \/\/ When you have a dominating store or atomic instruction, that is\n+          \/\/ indeed ensured to be the case. However, as for allocations, the\n+          \/\/ initialized memory location could be raw null, which isn't store-good.\n+        }\n+        continue;\n+      } else if (!node->is_Mach()) {\n+        continue;\n+      }\n+\n+      MachNode* const mach = node->as_Mach();\n+      switch (mach->ideal_Opcode()) {\n+      case Op_LoadP:\n+        if ((mach->barrier_data() & ZBarrierStrong) != 0 &&\n+            (mach->barrier_data() & ZBarrierNoKeepalive) == 0) {\n+          loads.push(mach);\n+          load_dominators.push(mach);\n@@ -487,0 +838,22 @@\n+        break;\n+      case Op_StoreP:\n+        if (mach->barrier_data() != 0) {\n+          stores.push(mach);\n+          load_dominators.push(mach);\n+          store_dominators.push(mach);\n+          atomic_dominators.push(mach);\n+        }\n+        break;\n+      case Op_CompareAndExchangeP:\n+      case Op_CompareAndSwapP:\n+      case Op_GetAndSetP:\n+        if (mach->barrier_data() != 0) {\n+          atomics.push(mach);\n+          load_dominators.push(mach);\n+          store_dominators.push(mach);\n+          atomic_dominators.push(mach);\n+        }\n+        break;\n+\n+      default:\n+        break;\n@@ -490,0 +863,5 @@\n+\n+  \/\/ Step 2 - Find dominating accesses or allocations for each access\n+  analyze_dominating_barriers_impl(loads, load_dominators);\n+  analyze_dominating_barriers_impl(stores, store_dominators);\n+  analyze_dominating_barriers_impl(atomics, atomic_dominators);\n@@ -549,1 +927,1 @@\n-      if (regs != NULL) {\n+      if (regs != nullptr) {\n@@ -567,0 +945,14 @@\n+void ZBarrierSetC2::eliminate_gc_barrier(PhaseMacroExpand* macro, Node* node) const {\n+  eliminate_gc_barrier_data(node);\n+}\n+\n+void ZBarrierSetC2::eliminate_gc_barrier_data(Node* node) const {\n+  if (node->is_LoadStore()) {\n+    LoadStoreNode* loadstore = node->as_LoadStore();\n+    loadstore->set_barrier_data(ZBarrierElided);\n+  } else if (node->is_Mem()) {\n+    MemNode* mem = node->as_Mem();\n+    mem->set_barrier_data(ZBarrierElided);\n+  }\n+}\n+\n@@ -569,1 +961,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierStrong) != 0) {\n+  if ((mach->barrier_data() & ZBarrierStrong) != 0) {\n@@ -572,1 +964,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierWeak) != 0) {\n+  if ((mach->barrier_data() & ZBarrierWeak) != 0) {\n@@ -575,1 +967,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierPhantom) != 0) {\n+  if ((mach->barrier_data() & ZBarrierPhantom) != 0) {\n@@ -578,1 +970,1 @@\n-  if ((mach->barrier_data() & ZLoadBarrierNoKeepalive) != 0) {\n+  if ((mach->barrier_data() & ZBarrierNoKeepalive) != 0) {\n@@ -581,0 +973,6 @@\n+  if ((mach->barrier_data() & ZBarrierNative) != 0) {\n+    st->print(\"native \");\n+  }\n+  if ((mach->barrier_data() & ZBarrierElided) != 0) {\n+    st->print(\"elided \");\n+  }\n","filename":"src\/hotspot\/share\/gc\/z\/c2\/zBarrierSetC2.cpp","additions":546,"deletions":148,"binary":false,"changes":694,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2019, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2019, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -32,2 +32,2 @@\n-ZObjArrayAllocator::ZObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread) :\n-    ObjArrayAllocator(klass, word_size, length, do_zero, thread) {}\n+ZObjArrayAllocator::ZObjArrayAllocator(Klass* klass, size_t word_size, int length, bool do_zero, Thread* thread)\n+  : ObjArrayAllocator(klass, word_size, length, do_zero, thread) {}\n@@ -75,0 +75,3 @@\n+\n+  \/\/ Signal to the ZIterator that this is an invisible root, by setting\n+  \/\/ the mark word to \"marked\". Reset to prototype() after the clearing.\n@@ -76,1 +79,1 @@\n-    oopDesc::release_set_mark(mem, _klass->prototype_header());\n+    oopDesc::release_set_mark(mem, _klass->prototype_header().set_marked());\n@@ -78,1 +81,1 @@\n-    arrayOopDesc::set_mark(mem, markWord::prototype());\n+    arrayOopDesc::set_mark(mem, markWord::prototype().set_marked());\n@@ -85,1 +88,1 @@\n-  \/\/ root. Invisible roots are not visited by the heap itarator\n+  \/\/ root. Invisible roots are not visited by the heap iterator\n@@ -87,14 +90,56 @@\n-  \/\/ Relocation knows how to dodge iterating over such objects.\n-  ZThreadLocalData::set_invisible_root(_thread, (oop*)&mem);\n-\n-  for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n-    \/\/ Calculate segment\n-    HeapWord* const start = (HeapWord*)(mem + header + processed);\n-    const size_t remaining = payload_size - processed;\n-    const size_t segment_size = MIN2(remaining, segment_max);\n-\n-    \/\/ Clear segment\n-    Copy::zero_to_words(start, segment_size);\n-\n-    \/\/ Safepoint\n-    yield_for_safepoint();\n+  \/\/ Relocation and remembered set code know how to dodge iterating\n+  \/\/ over such objects.\n+  ZThreadLocalData::set_invisible_root(_thread, (zaddress_unsafe*)&mem);\n+\n+  uint32_t old_seqnum_before = ZGeneration::old()->seqnum();\n+  uint32_t young_seqnum_before = ZGeneration::young()->seqnum();\n+  uintptr_t color_before = ZPointerStoreGoodMask;\n+  auto gc_safepoint_happened = [&]() {\n+    return old_seqnum_before != ZGeneration::old()->seqnum() ||\n+           young_seqnum_before != ZGeneration::young()->seqnum() ||\n+           color_before != ZPointerStoreGoodMask;\n+  };\n+\n+  bool seen_gc_safepoint = false;\n+\n+  auto initialize_memory = [&]() {\n+    for (size_t processed = 0; processed < payload_size; processed += segment_max) {\n+      \/\/ Clear segment\n+      uintptr_t* const start = (uintptr_t*)(mem + header + processed);\n+      const size_t remaining = payload_size - processed;\n+      const size_t segment = MIN2(remaining, segment_max);\n+      \/\/ Usually, the young marking code has the responsibility to color\n+      \/\/ raw nulls, before they end up in the old generation. However, the\n+      \/\/ invisible roots are hidden from the marking code, and therefore\n+      \/\/ we must color the nulls already here in the initialization. The\n+      \/\/ color we choose must be store bad for any subsequent stores, regardless\n+      \/\/ of how many GC flips later it will arrive. That's why we OR in 11\n+      \/\/ (ZPointerRememberedMask) in the remembered bits, similar to how\n+      \/\/ forgotten old oops also have 11, for the very same reason.\n+      \/\/ However, we opportunistically try to color without the 11 remembered\n+      \/\/ bits, hoping to not get interrupted in the middle of a GC safepoint.\n+      \/\/ Most of the time, we manage to do that, and can the avoid having GC\n+      \/\/ barriers trigger slow paths for this.\n+      const uintptr_t colored_null = seen_gc_safepoint ? (ZPointerStoreGoodMask | ZPointerRememberedMask)\n+                                                       : ZPointerStoreGoodMask;\n+      const uintptr_t fill_value = is_reference_type(element_type) ? colored_null : 0;\n+      ZUtils::fill(start, segment, fill_value);\n+\n+      \/\/ Safepoint\n+      yield_for_safepoint();\n+\n+      \/\/ Deal with safepoints\n+      if (!seen_gc_safepoint && gc_safepoint_happened()) {\n+        \/\/ The first time we observe a GC safepoint in the yield point,\n+        \/\/ we have to restart processing with 11 remembered bits.\n+        seen_gc_safepoint = true;\n+        return false;\n+      }\n+    }\n+    return true;\n+  };\n+\n+  if (!initialize_memory()) {\n+    \/\/ Re-color with 11 remset bits if we got intercepted by a GC safepoint\n+    const bool result = initialize_memory();\n+    assert(result, \"Array initialization should always succeed the second time\");\n@@ -105,0 +150,3 @@\n+  \/\/ Signal to the ZIterator that this is no longer an invisible root\n+  oopDesc::release_set_mark(mem, markWord::prototype());\n+\n","filename":"src\/hotspot\/share\/gc\/z\/zObjArrayAllocator.cpp","additions":68,"deletions":20,"binary":false,"changes":88,"status":"modified"},{"patch":"@@ -2,1 +2,1 @@\n- * Copyright (c) 2015, 2021, Oracle and\/or its affiliates. All rights reserved.\n+ * Copyright (c) 2015, 2023, Oracle and\/or its affiliates. All rights reserved.\n@@ -29,0 +29,1 @@\n+#include \"gc\/z\/zAllocator.inline.hpp\"\n@@ -30,0 +31,1 @@\n+#include \"gc\/z\/zCollectedHeap.hpp\"\n@@ -31,0 +33,1 @@\n+#include \"gc\/z\/zGeneration.inline.hpp\"\n@@ -32,0 +35,2 @@\n+#include \"gc\/z\/zIndexDistributor.inline.hpp\"\n+#include \"gc\/z\/zIterator.inline.hpp\"\n@@ -33,0 +38,1 @@\n+#include \"gc\/z\/zPageAge.hpp\"\n@@ -35,0 +41,2 @@\n+#include \"gc\/z\/zRootsIterator.hpp\"\n+#include \"gc\/z\/zStackWatermark.hpp\"\n@@ -37,1 +45,2 @@\n-#include \"gc\/z\/zThread.inline.hpp\"\n+#include \"gc\/z\/zUncoloredRoot.inline.hpp\"\n+#include \"gc\/z\/zVerify.hpp\"\n@@ -43,2 +52,2 @@\n-ZRelocate::ZRelocate(ZWorkers* workers) :\n-    _workers(workers) {}\n+static const ZStatCriticalPhase ZCriticalPhaseRelocationStall(\"Relocation Stall\");\n+static const ZStatSubPhase ZSubPhaseConcurrentRelocateRememberedSetFlipPromotedYoung(\"Concurrent Relocate Remset FP\", ZGenerationId::young);\n@@ -46,2 +55,1 @@\n-static uintptr_t forwarding_index(ZForwarding* forwarding, uintptr_t from_addr) {\n-  const uintptr_t from_offset = ZAddress::offset(from_addr);\n+static uintptr_t forwarding_index(ZForwarding* forwarding, zoffset from_offset) {\n@@ -51,2 +59,2 @@\n-static uintptr_t forwarding_find(ZForwarding* forwarding, uintptr_t from_addr, ZForwardingCursor* cursor) {\n-  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n+static zaddress forwarding_find(ZForwarding* forwarding, zoffset from_offset, ZForwardingCursor* cursor) {\n+  const uintptr_t from_index = forwarding_index(forwarding, from_offset);\n@@ -54,1 +62,1 @@\n-  return entry.populated() ? ZAddress::good(entry.to_offset()) : 0;\n+  return entry.populated() ? ZOffset::address(to_zoffset(entry.to_offset())) : zaddress::null;\n@@ -57,5 +65,2 @@\n-static uintptr_t forwarding_insert(ZForwarding* forwarding, uintptr_t from_addr, uintptr_t to_addr, ZForwardingCursor* cursor) {\n-  const uintptr_t from_index = forwarding_index(forwarding, from_addr);\n-  const uintptr_t to_offset = ZAddress::offset(to_addr);\n-  const uintptr_t to_offset_final = forwarding->insert(from_index, to_offset, cursor);\n-  return ZAddress::good(to_offset_final);\n+static zaddress forwarding_find(ZForwarding* forwarding, zaddress_unsafe from_addr, ZForwardingCursor* cursor) {\n+  return forwarding_find(forwarding, ZAddress::offset(from_addr), cursor);\n@@ -64,1 +69,269 @@\n-static uintptr_t relocate_object_inner(ZForwarding* forwarding, uintptr_t from_addr, ZForwardingCursor* cursor) {\n+static zaddress forwarding_find(ZForwarding* forwarding, zaddress from_addr, ZForwardingCursor* cursor) {\n+  return forwarding_find(forwarding, ZAddress::offset(from_addr), cursor);\n+}\n+\n+static zaddress forwarding_insert(ZForwarding* forwarding, zoffset from_offset, zaddress to_addr, ZForwardingCursor* cursor) {\n+  const uintptr_t from_index = forwarding_index(forwarding, from_offset);\n+  const zoffset to_offset = ZAddress::offset(to_addr);\n+  const zoffset to_offset_final = forwarding->insert(from_index, to_offset, cursor);\n+  return ZOffset::address(to_offset_final);\n+}\n+\n+static zaddress forwarding_insert(ZForwarding* forwarding, zaddress from_addr, zaddress to_addr, ZForwardingCursor* cursor) {\n+  return forwarding_insert(forwarding, ZAddress::offset(from_addr), to_addr, cursor);\n+}\n+\n+ZRelocateQueue::ZRelocateQueue()\n+  : _lock(),\n+    _queue(),\n+    _nworkers(0),\n+    _nsynchronized(0),\n+    _synchronize(false),\n+    _needs_attention(0) {}\n+\n+bool ZRelocateQueue::needs_attention() const {\n+  return Atomic::load(&_needs_attention) != 0;\n+}\n+\n+void ZRelocateQueue::inc_needs_attention() {\n+  const int needs_attention = Atomic::add(&_needs_attention, 1);\n+  assert(needs_attention == 1 || needs_attention == 2, \"Invalid state\");\n+}\n+\n+void ZRelocateQueue::dec_needs_attention() {\n+  const int needs_attention = Atomic::sub(&_needs_attention, 1);\n+  assert(needs_attention == 0 || needs_attention == 1, \"Invalid state\");\n+}\n+\n+void ZRelocateQueue::join(uint nworkers) {\n+  assert(nworkers != 0, \"Must request at least one worker\");\n+  assert(_nworkers == 0, \"Invalid state\");\n+  assert(_nsynchronized == 0, \"Invalid state\");\n+\n+  log_debug(gc, reloc)(\"Joining workers: %u\", nworkers);\n+\n+  _nworkers = nworkers;\n+}\n+\n+void ZRelocateQueue::resize_workers(uint nworkers) {\n+  assert(nworkers != 0, \"Must request at least one worker\");\n+  assert(_nworkers == 0, \"Invalid state\");\n+  assert(_nsynchronized == 0, \"Invalid state\");\n+\n+  log_debug(gc, reloc)(\"Resize workers: %u\", nworkers);\n+\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _nworkers = nworkers;\n+}\n+\n+void ZRelocateQueue::leave() {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _nworkers--;\n+\n+  assert(_nsynchronized <= _nworkers, \"_nsynchronized: %u _nworkers: %u\", _nsynchronized, _nworkers);\n+\n+  log_debug(gc, reloc)(\"Leaving workers: left: %u _synchronize: %d _nsynchronized: %u\", _nworkers, _synchronize, _nsynchronized);\n+\n+  \/\/ Prune done forwardings\n+  const bool forwardings_done = prune();\n+\n+  \/\/ Check if all workers synchronized\n+  const bool last_synchronized = _synchronize && _nworkers == _nsynchronized;\n+\n+  if (forwardings_done || last_synchronized) {\n+    _lock.notify_all();\n+  }\n+}\n+\n+void ZRelocateQueue::add_and_wait(ZForwarding* forwarding) {\n+  ZStatTimer timer(ZCriticalPhaseRelocationStall);\n+  ZLocker<ZConditionLock> locker(&_lock);\n+\n+  if (forwarding->is_done()) {\n+    return;\n+  }\n+\n+  _queue.append(forwarding);\n+  if (_queue.length() == 1) {\n+    \/\/ Queue became non-empty\n+    inc_needs_attention();\n+    _lock.notify_all();\n+  }\n+\n+  while (!forwarding->is_done()) {\n+    _lock.wait();\n+  }\n+}\n+\n+bool ZRelocateQueue::prune() {\n+  if (_queue.is_empty()) {\n+    return false;\n+  }\n+\n+  bool done = false;\n+\n+  for (int i = 0; i < _queue.length();) {\n+    const ZForwarding* const forwarding = _queue.at(i);\n+    if (forwarding->is_done()) {\n+      done = true;\n+\n+      _queue.delete_at(i);\n+    } else {\n+      i++;\n+    }\n+  }\n+\n+  if (_queue.is_empty()) {\n+    dec_needs_attention();\n+  }\n+\n+  return done;\n+}\n+\n+ZForwarding* ZRelocateQueue::prune_and_claim() {\n+  if (prune()) {\n+    _lock.notify_all();\n+  }\n+\n+  for (int i = 0; i < _queue.length(); i++) {\n+    ZForwarding* const forwarding = _queue.at(i);\n+    if (forwarding->claim()) {\n+      return forwarding;\n+    }\n+  }\n+\n+  return nullptr;\n+}\n+\n+class ZRelocateQueueSynchronizeThread {\n+private:\n+  ZRelocateQueue* const _queue;\n+\n+public:\n+  ZRelocateQueueSynchronizeThread(ZRelocateQueue* queue)\n+    : _queue(queue) {\n+    _queue->synchronize_thread();\n+  }\n+\n+  ~ZRelocateQueueSynchronizeThread() {\n+    _queue->desynchronize_thread();\n+  }\n+};\n+\n+void ZRelocateQueue::synchronize_thread() {\n+  _nsynchronized++;\n+\n+  log_debug(gc, reloc)(\"Synchronize worker _nsynchronized %u\", _nsynchronized);\n+\n+  assert(_nsynchronized <= _nworkers, \"_nsynchronized: %u _nworkers: %u\", _nsynchronized, _nworkers);\n+  if (_nsynchronized == _nworkers) {\n+    \/\/ All workers synchronized\n+    _lock.notify_all();\n+  }\n+}\n+\n+void ZRelocateQueue::desynchronize_thread() {\n+  _nsynchronized--;\n+\n+  log_debug(gc, reloc)(\"Desynchronize worker _nsynchronized %u\", _nsynchronized);\n+\n+  assert(_nsynchronized < _nworkers, \"_nsynchronized: %u _nworkers: %u\", _nsynchronized, _nworkers);\n+}\n+\n+ZForwarding* ZRelocateQueue::synchronize_poll() {\n+  \/\/ Fast path avoids locking\n+  if (!needs_attention()) {\n+    return nullptr;\n+  }\n+\n+  \/\/ Slow path to get the next forwarding and\/or synchronize\n+  ZLocker<ZConditionLock> locker(&_lock);\n+\n+  {\n+    ZForwarding* const forwarding = prune_and_claim();\n+    if (forwarding != nullptr) {\n+      \/\/ Don't become synchronized while there are elements in the queue\n+      return forwarding;\n+    }\n+  }\n+\n+  if (!_synchronize) {\n+    return nullptr;\n+  }\n+\n+  ZRelocateQueueSynchronizeThread rqst(this);\n+\n+  do {\n+    _lock.wait();\n+\n+    ZForwarding* const forwarding = prune_and_claim();\n+    if (forwarding != nullptr) {\n+      return forwarding;\n+    }\n+  } while (_synchronize);\n+\n+  return nullptr;\n+}\n+\n+void ZRelocateQueue::clear() {\n+  assert(_nworkers == 0, \"Invalid state\");\n+\n+  if (_queue.is_empty()) {\n+    return;\n+  }\n+\n+  ZArrayIterator<ZForwarding*> iter(&_queue);\n+  for (ZForwarding* forwarding; iter.next(&forwarding);) {\n+    assert(forwarding->is_done(), \"All should be done\");\n+  }\n+\n+  assert(false, \"Clear was not empty\");\n+\n+  _queue.clear();\n+  dec_needs_attention();\n+}\n+\n+void ZRelocateQueue::synchronize() {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _synchronize = true;\n+\n+  inc_needs_attention();\n+\n+  log_debug(gc, reloc)(\"Synchronize all workers 1 _nworkers: %u _nsynchronized: %u\", _nworkers, _nsynchronized);\n+\n+  while (_nworkers != _nsynchronized) {\n+    _lock.wait();\n+    log_debug(gc, reloc)(\"Synchronize all workers 2 _nworkers: %u _nsynchronized: %u\", _nworkers, _nsynchronized);\n+  }\n+}\n+\n+void ZRelocateQueue::desynchronize() {\n+  ZLocker<ZConditionLock> locker(&_lock);\n+  _synchronize = false;\n+\n+  log_debug(gc, reloc)(\"Desynchronize all workers _nworkers: %u _nsynchronized: %u\", _nworkers, _nsynchronized);\n+\n+  assert(_nsynchronized <= _nworkers, \"_nsynchronized: %u _nworkers: %u\", _nsynchronized, _nworkers);\n+\n+  dec_needs_attention();\n+\n+  _lock.notify_all();\n+}\n+\n+ZRelocate::ZRelocate(ZGeneration* generation)\n+  : _generation(generation),\n+    _queue() {}\n+\n+ZWorkers* ZRelocate::workers() const {\n+  return _generation->workers();\n+}\n+\n+void ZRelocate::start() {\n+  _queue.join(workers()->active_workers());\n+}\n+\n+void ZRelocate::add_remset(volatile zpointer* p) {\n+  ZGeneration::young()->remember(p);\n+}\n+\n+static zaddress relocate_object_inner(ZForwarding* forwarding, zaddress from_addr, ZForwardingCursor* cursor) {\n@@ -69,2 +342,6 @@\n-  const uintptr_t to_addr = ZHeap::heap()->alloc_object_for_relocation(size);\n-  if (to_addr == 0) {\n+\n+  ZAllocatorForRelocation* allocator = ZAllocator::relocation(forwarding->to_age());\n+\n+  const zaddress to_addr = allocator->alloc_object(size);\n+\n+  if (is_null(to_addr)) {\n@@ -72,1 +349,1 @@\n-    return 0;\n+    return zaddress::null;\n@@ -79,1 +356,2 @@\n-  const uintptr_t to_addr_final = forwarding_insert(forwarding, from_addr, to_addr, cursor);\n+  const zaddress to_addr_final = forwarding_insert(forwarding, from_addr, to_addr, cursor);\n+\n@@ -82,1 +360,1 @@\n-    ZHeap::heap()->undo_alloc_object_for_relocation(to_addr, size);\n+    allocator->undo_alloc_object(to_addr, size);\n@@ -88,1 +366,1 @@\n-uintptr_t ZRelocate::relocate_object(ZForwarding* forwarding, uintptr_t from_addr) const {\n+zaddress ZRelocate::relocate_object(ZForwarding* forwarding, zaddress_unsafe from_addr) {\n@@ -92,2 +370,2 @@\n-  uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n-  if (to_addr != 0) {\n+  zaddress to_addr = forwarding_find(forwarding, from_addr, &cursor);\n+  if (!is_null(to_addr)) {\n@@ -99,2 +377,3 @@\n-  if (forwarding->retain_page()) {\n-    to_addr = relocate_object_inner(forwarding, from_addr, &cursor);\n+  if (forwarding->retain_page(&_queue)) {\n+    assert(_generation->is_phase_relocate(), \"Must be\");\n+    to_addr = relocate_object_inner(forwarding, safe(from_addr), &cursor);\n@@ -103,1 +382,1 @@\n-    if (to_addr != 0) {\n+    if (!is_null(to_addr)) {\n@@ -108,8 +387,3 @@\n-    \/\/ Failed to relocate object. Wait for a worker thread to complete\n-    \/\/ relocation of this page, and then forward the object. If the GC\n-    \/\/ aborts the relocation phase before the page has been relocated,\n-    \/\/ then wait return false and we just forward the object in-place.\n-    if (!forwarding->wait_page_released()) {\n-      \/\/ Forward object in-place\n-      return forwarding_insert(forwarding, from_addr, from_addr, &cursor);\n-    }\n+    \/\/ Failed to relocate object. Signal and wait for a worker thread to\n+    \/\/ complete relocation of this page, and then forward the object.\n+    _queue.add_and_wait(forwarding);\n@@ -122,1 +396,1 @@\n-uintptr_t ZRelocate::forward_object(ZForwarding* forwarding, uintptr_t from_addr) const {\n+zaddress ZRelocate::forward_object(ZForwarding* forwarding, zaddress_unsafe from_addr) {\n@@ -124,2 +398,2 @@\n-  const uintptr_t to_addr = forwarding_find(forwarding, from_addr, &cursor);\n-  assert(to_addr != 0, \"Should be forwarded\");\n+  const zaddress to_addr = forwarding_find(forwarding, from_addr, &cursor);\n+  assert(!is_null(to_addr), \"Should be forwarded: \" PTR_FORMAT, untype(from_addr));\n@@ -129,1 +403,1 @@\n-static ZPage* alloc_page(const ZForwarding* forwarding) {\n+static ZPage* alloc_page(ZAllocatorForRelocation* allocator, ZPageType type, size_t size) {\n@@ -133,1 +407,1 @@\n-    return NULL;\n+    return nullptr;\n@@ -138,3 +412,1 @@\n-  flags.set_worker_relocation();\n-  return ZHeap::heap()->alloc_page(forwarding->type(), forwarding->size(), flags);\n-}\n+  flags.set_gc_relocation();\n@@ -142,2 +414,1 @@\n-static void free_page(ZPage* page) {\n-  ZHeap::heap()->free_page(page, true \/* reclaimed *\/);\n+  return allocator->alloc_page_for_relocation(type, size, flags);\n@@ -146,1 +417,7 @@\n-static bool should_free_target_page(ZPage* page) {\n+static void retire_target_page(ZGeneration* generation, ZPage* page) {\n+  if (generation->is_young() && page->is_old()) {\n+    generation->increase_promoted(page->used());\n+  } else {\n+    generation->increase_compacted(page->used());\n+  }\n+\n@@ -151,1 +428,3 @@\n-  return page != NULL && page->top() == page->start();\n+  if (page->used() == 0) {\n+    ZHeap::heap()->free_page(page);\n+  }\n@@ -156,1 +435,2 @@\n-  volatile size_t _in_place_count;\n+  ZGeneration* const _generation;\n+  volatile size_t    _in_place_count;\n@@ -159,1 +439,2 @@\n-  ZRelocateSmallAllocator() :\n+  ZRelocateSmallAllocator(ZGeneration* generation)\n+    : _generation(generation),\n@@ -162,3 +443,4 @@\n-  ZPage* alloc_target_page(ZForwarding* forwarding, ZPage* target) {\n-    ZPage* const page = alloc_page(forwarding);\n-    if (page == NULL) {\n+  ZPage* alloc_and_retire_target_page(ZForwarding* forwarding, ZPage* target) {\n+    ZAllocatorForRelocation* const allocator = ZAllocator::relocation(forwarding->to_age());\n+    ZPage* const page = alloc_page(allocator, forwarding->type(), forwarding->size());\n+    if (page == nullptr) {\n@@ -168,0 +450,5 @@\n+    if (target != nullptr) {\n+      \/\/ Retire the old target page\n+      retire_target_page(_generation, target);\n+    }\n+\n@@ -176,2 +463,2 @@\n-    if (should_free_target_page(page)) {\n-      free_page(page);\n+    if (page != nullptr) {\n+      retire_target_page(_generation, page);\n@@ -181,2 +468,2 @@\n-  void free_relocated_page(ZPage* page) {\n-    free_page(page);\n+  zaddress alloc_object(ZPage* page, size_t size) const {\n+    return (page != nullptr) ? page->alloc_object(size) : zaddress::null;\n@@ -185,5 +472,1 @@\n-  uintptr_t alloc_object(ZPage* page, size_t size) const {\n-    return (page != NULL) ? page->alloc_object(size) : 0;\n-  }\n-\n-  void undo_alloc_object(ZPage* page, uintptr_t addr, size_t size) const {\n+  void undo_alloc_object(ZPage* page, zaddress addr, size_t size) const {\n@@ -200,4 +483,5 @@\n-  ZConditionLock      _lock;\n-  ZPage*              _shared;\n-  bool                _in_place;\n-  volatile size_t     _in_place_count;\n+  ZGeneration* const _generation;\n+  ZConditionLock     _lock;\n+  ZPage*             _shared[ZAllocator::_relocation_allocators];\n+  bool               _in_place;\n+  volatile size_t    _in_place_count;\n@@ -206,1 +490,2 @@\n-  ZRelocateMediumAllocator() :\n+  ZRelocateMediumAllocator(ZGeneration* generation)\n+    : _generation(generation),\n@@ -208,1 +493,1 @@\n-      _shared(NULL),\n+      _shared(),\n@@ -213,2 +498,4 @@\n-    if (should_free_target_page(_shared)) {\n-      free_page(_shared);\n+    for (uint i = 0; i < ZAllocator::_relocation_allocators; ++i) {\n+      if (_shared[i] != nullptr) {\n+        retire_target_page(_generation, _shared[i]);\n+      }\n@@ -218,1 +505,9 @@\n-  ZPage* alloc_target_page(ZForwarding* forwarding, ZPage* target) {\n+  ZPage* shared(ZPageAge age) {\n+    return _shared[static_cast<uint>(age) - 1];\n+  }\n+\n+  void set_shared(ZPageAge age, ZPage* page) {\n+    _shared[static_cast<uint>(age) - 1] = page;\n+  }\n+\n+  ZPage* alloc_and_retire_target_page(ZForwarding* forwarding, ZPage* target) {\n@@ -230,3 +525,6 @@\n-    if (_shared == target) {\n-      _shared = alloc_page(forwarding);\n-      if (_shared == NULL) {\n+    const ZPageAge to_age = forwarding->to_age();\n+    if (shared(to_age) == target) {\n+      ZAllocatorForRelocation* const allocator = ZAllocator::relocation(forwarding->to_age());\n+      ZPage* const to_page = alloc_page(allocator, forwarding->type(), forwarding->size());\n+      set_shared(to_age, to_page);\n+      if (to_page == nullptr) {\n@@ -236,0 +534,5 @@\n+\n+      \/\/ This thread is responsible for retiring the shared target page\n+      if (target != nullptr) {\n+        retire_target_page(_generation, target);\n+      }\n@@ -238,1 +541,1 @@\n-    return _shared;\n+    return shared(to_age);\n@@ -242,1 +545,1 @@\n-    ZLocker<ZConditionLock> locker(&_lock);\n+    const ZPageAge age = page->age();\n@@ -244,0 +547,1 @@\n+    ZLocker<ZConditionLock> locker(&_lock);\n@@ -245,2 +549,2 @@\n-    assert(_shared == NULL, \"Invalid state\");\n-    assert(page != NULL, \"Invalid page\");\n+    assert(shared(age) == nullptr, \"Invalid state\");\n+    assert(page != nullptr, \"Invalid page\");\n@@ -248,1 +552,1 @@\n-    _shared = page;\n+    set_shared(age, page);\n@@ -258,2 +562,2 @@\n-  void free_relocated_page(ZPage* page) {\n-    free_page(page);\n+  zaddress alloc_object(ZPage* page, size_t size) const {\n+    return (page != nullptr) ? page->alloc_object_atomic(size) : zaddress::null;\n@@ -262,5 +566,1 @@\n-  uintptr_t alloc_object(ZPage* page, size_t size) const {\n-    return (page != NULL) ? page->alloc_object_atomic(size) : 0;\n-  }\n-\n-  void undo_alloc_object(ZPage* page, uintptr_t addr, size_t size) const {\n+  void undo_alloc_object(ZPage* page, zaddress addr, size_t size) const {\n@@ -276,1 +576,1 @@\n-class ZRelocateClosure : public ObjectClosure {\n+class ZRelocateWork : public StackObj {\n@@ -278,3 +578,10 @@\n-  Allocator* const _allocator;\n-  ZForwarding*     _forwarding;\n-  ZPage*           _target;\n+  Allocator* const   _allocator;\n+  ZForwarding*       _forwarding;\n+  ZPage*             _target[ZAllocator::_relocation_allocators];\n+  ZGeneration* const _generation;\n+  size_t             _other_promoted;\n+  size_t             _other_compacted;\n+\n+  ZPage* target(ZPageAge age) {\n+    return _target[static_cast<uint>(age) - 1];\n+  }\n@@ -282,1 +589,18 @@\n-  bool relocate_object(uintptr_t from_addr) const {\n+  void set_target(ZPageAge age, ZPage* page) {\n+    _target[static_cast<uint>(age) - 1] = page;\n+  }\n+\n+  size_t object_alignment() const {\n+    return (size_t)1 << _forwarding->object_alignment_shift();\n+  }\n+\n+  void increase_other_forwarded(size_t unaligned_object_size) {\n+    const size_t aligned_size = align_up(unaligned_object_size, object_alignment());\n+    if (_forwarding->is_promotion()) {\n+      _other_promoted += aligned_size;\n+    } else {\n+      _other_compacted += aligned_size;\n+    }\n+  }\n+\n+  zaddress try_relocate_object_inner(zaddress from_addr) {\n@@ -285,0 +609,3 @@\n+    const size_t size = ZUtils::object_size(from_addr);\n+    ZPage* const to_page = target(_forwarding->to_age());\n+\n@@ -286,3 +613,7 @@\n-    if (forwarding_find(_forwarding, from_addr, &cursor) != 0) {\n-      \/\/ Already relocated\n-      return true;\n+    {\n+      const zaddress to_addr = forwarding_find(_forwarding, from_addr, &cursor);\n+      if (!is_null(to_addr)) {\n+        \/\/ Already relocated\n+        increase_other_forwarded(size);\n+        return to_addr;\n+      }\n@@ -292,3 +623,2 @@\n-    const size_t size = ZUtils::object_size(from_addr);\n-    const uintptr_t to_addr = _allocator->alloc_object(_target, size);\n-    if (to_addr == 0) {\n+    const zaddress allocated_addr = _allocator->alloc_object(to_page, size);\n+    if (is_null(allocated_addr)) {\n@@ -296,1 +626,1 @@\n-      return false;\n+      return zaddress::null;\n@@ -300,3 +630,3 @@\n-    \/\/ in-place and the new object overlapps with the old object.\n-    if (_forwarding->in_place() && to_addr + size > from_addr) {\n-      ZUtils::object_copy_conjoint(from_addr, to_addr, size);\n+    \/\/ in-place and the new object overlaps with the old object.\n+    if (_forwarding->in_place_relocation() && allocated_addr + size > from_addr) {\n+      ZUtils::object_copy_conjoint(from_addr, allocated_addr, size);\n@@ -304,1 +634,1 @@\n-      ZUtils::object_copy_disjoint(from_addr, to_addr, size);\n+      ZUtils::object_copy_disjoint(from_addr, allocated_addr, size);\n@@ -308,1 +638,2 @@\n-    if (forwarding_insert(_forwarding, from_addr, to_addr, &cursor) != to_addr) {\n+    const zaddress to_addr = forwarding_insert(_forwarding, from_addr, allocated_addr, &cursor);\n+    if (to_addr != allocated_addr) {\n@@ -310,1 +641,106 @@\n-      _allocator->undo_alloc_object(_target, to_addr, size);\n+      _allocator->undo_alloc_object(to_page, to_addr, size);\n+      increase_other_forwarded(size);\n+    }\n+\n+    return to_addr;\n+  }\n+\n+  void update_remset_old_to_old(zaddress from_addr, zaddress to_addr) const {\n+    \/\/ Old-to-old relocation - move existing remset bits\n+\n+    \/\/ If this is called for an in-place relocated page, then this code has the\n+    \/\/ responsibility to clear the old remset bits. Extra care is needed because:\n+    \/\/\n+    \/\/ 1) The to-object copy can overlap with the from-object copy\n+    \/\/ 2) Remset bits of old objects need to be cleared\n+    \/\/\n+    \/\/ A watermark is used to keep track of how far the old remset bits have been removed.\n+\n+    const bool in_place = _forwarding->in_place_relocation();\n+    ZPage* const from_page = _forwarding->page();\n+    const uintptr_t from_local_offset = from_page->local_offset(from_addr);\n+\n+    \/\/ Note: even with in-place relocation, the to_page could be another page\n+    ZPage* const to_page = ZHeap::heap()->page(to_addr);\n+\n+    \/\/ Uses _relaxed version to handle that in-place relocation resets _top\n+    assert(ZHeap::heap()->is_in_page_relaxed(from_page, from_addr), \"Must be\");\n+    assert(to_page->is_in(to_addr), \"Must be\");\n+\n+\n+    \/\/ Read the size from the to-object, since the from-object\n+    \/\/ could have been overwritten during in-place relocation.\n+    const size_t size = ZUtils::object_size(to_addr);\n+\n+    \/\/ If a young generation collection started while the old generation\n+    \/\/ relocated  objects, the remember set bits were flipped from \"current\"\n+    \/\/ to \"previous\".\n+    \/\/\n+    \/\/ We need to select the correct remembered sets bitmap to ensure that the\n+    \/\/ old remset bits are found.\n+    \/\/\n+    \/\/ Note that if the young generation marking (remset scanning) finishes\n+    \/\/ before the old generation relocation has relocated this page, then the\n+    \/\/ young generation will visit this page's previous remembered set bits and\n+    \/\/ moved them over to the current bitmap.\n+    \/\/\n+    \/\/ If the young generation runs multiple cycles while the old generation is\n+    \/\/ relocating, then the first cycle will have consume the the old remset,\n+    \/\/ bits and moved associated objects to a new old page. The old relocation\n+    \/\/ could find either the the two bitmaps. So, either it will find the original\n+    \/\/ remset bits for the page, or it will find an empty bitmap for the page. It\n+    \/\/ doesn't matter for correctness, because the young generation marking has\n+    \/\/ already taken care of the bits.\n+\n+    const bool active_remset_is_current = ZGeneration::old()->active_remset_is_current();\n+\n+    \/\/ When in-place relocation is done and the old remset bits are located in\n+    \/\/ the bitmap that is going to be used for the new remset bits, then we\n+    \/\/ need to clear the old bits before the new bits are inserted.\n+    const bool iterate_current_remset = active_remset_is_current && !in_place;\n+\n+    BitMap::Iterator iter = iterate_current_remset\n+        ? from_page->remset_iterator_limited_current(from_local_offset, size)\n+        : from_page->remset_iterator_limited_previous(from_local_offset, size);\n+\n+    for (BitMap::idx_t field_bit : iter) {\n+      const uintptr_t field_local_offset = ZRememberedSet::to_offset(field_bit);\n+\n+      \/\/ Add remset entry in the to-page\n+      const uintptr_t offset = field_local_offset - from_local_offset;\n+      const zaddress to_field = to_addr + offset;\n+      log_trace(gc, reloc)(\"Remember: from: \" PTR_FORMAT \" to: \" PTR_FORMAT \" current: %d marking: %d page: \" PTR_FORMAT \" remset: \" PTR_FORMAT,\n+          untype(from_page->start() + field_local_offset), untype(to_field), active_remset_is_current, ZGeneration::young()->is_phase_mark(), p2i(to_page), p2i(to_page->remset_current()));\n+\n+      volatile zpointer* const p = (volatile zpointer*)to_field;\n+\n+      if (ZGeneration::young()->is_phase_mark()) {\n+        \/\/ Young generation remembered set scanning needs to know about this\n+        \/\/ field. It will take responsibility to add a new remember set entry if needed.\n+        _forwarding->relocated_remembered_fields_register(p);\n+      } else {\n+        to_page->remember(p);\n+        if (in_place) {\n+          assert(to_page->is_remembered(p), \"p: \" PTR_FORMAT, p2i(p));\n+        }\n+      }\n+    }\n+  }\n+\n+  static bool add_remset_if_young(volatile zpointer* p, zaddress addr) {\n+    if (ZHeap::heap()->is_young(addr)) {\n+      ZRelocate::add_remset(p);\n+      return true;\n+    }\n+\n+    return false;\n+  }\n+\n+  static void update_remset_promoted_filter_and_remap_per_field(volatile zpointer* p) {\n+    const zpointer ptr = Atomic::load(p);\n+\n+    assert(ZPointer::is_old_load_good(ptr), \"Should be at least old load good: \" PTR_FORMAT, untype(ptr));\n+\n+    if (ZPointer::is_store_good(ptr)) {\n+      \/\/ Already has a remset entry\n+      return;\n@@ -313,0 +749,74 @@\n+    if (ZPointer::is_load_good(ptr)) {\n+      if (!is_null_any(ptr)) {\n+        const zaddress addr = ZPointer::uncolor(ptr);\n+        add_remset_if_young(p, addr);\n+      }\n+      \/\/ No need to remap it is already load good\n+      return;\n+    }\n+\n+    if (is_null_any(ptr)) {\n+      \/\/ Eagerly remap to skip adding a remset entry just to get deferred remapping\n+      ZBarrier::remap_young_relocated(p, ptr);\n+      return;\n+    }\n+\n+    const zaddress_unsafe addr_unsafe = ZPointer::uncolor_unsafe(ptr);\n+    ZForwarding* const forwarding = ZGeneration::young()->forwarding(addr_unsafe);\n+\n+    if (forwarding == nullptr) {\n+      \/\/ Object isn't being relocated\n+      const zaddress addr = safe(addr_unsafe);\n+      if (!add_remset_if_young(p, addr)) {\n+        \/\/ Not young - eagerly remap to skip adding a remset entry just to get deferred remapping\n+        ZBarrier::remap_young_relocated(p, ptr);\n+      }\n+      return;\n+    }\n+\n+    const zaddress addr = forwarding->find(addr_unsafe);\n+\n+    if (!is_null(addr)) {\n+      \/\/ Object has already been relocated\n+      if (!add_remset_if_young(p, addr)) {\n+        \/\/ Not young - eagerly remap to skip adding a remset entry just to get deferred remapping\n+        ZBarrier::remap_young_relocated(p, ptr);\n+      }\n+      return;\n+    }\n+\n+    \/\/ Object has not been relocated yet\n+    \/\/ Don't want to eagerly relocate objects, so just add a remset\n+    ZRelocate::add_remset(p);\n+    return;\n+  }\n+\n+  void update_remset_promoted(zaddress to_addr) const {\n+    ZIterator::basic_oop_iterate(to_oop(to_addr), update_remset_promoted_filter_and_remap_per_field);\n+  }\n+\n+  void update_remset_for_fields(zaddress from_addr, zaddress to_addr) const {\n+    if (_forwarding->to_age() != ZPageAge::old) {\n+      \/\/ No remembered set in young pages\n+      return;\n+    }\n+\n+    \/\/ Need to deal with remset when moving objects to the old generation\n+    if (_forwarding->from_age() == ZPageAge::old) {\n+      update_remset_old_to_old(from_addr, to_addr);\n+      return;\n+    }\n+\n+    \/\/ Normal promotion\n+    update_remset_promoted(to_addr);\n+  }\n+\n+  bool try_relocate_object(zaddress from_addr) {\n+    const zaddress to_addr = try_relocate_object_inner(from_addr);\n+\n+    if (is_null(to_addr)) {\n+      return false;\n+    }\n+\n+    update_remset_for_fields(from_addr, to_addr);\n+\n@@ -316,2 +826,52 @@\n-  virtual void do_object(oop obj) {\n-    const uintptr_t addr = ZOop::to_address(obj);\n+  void start_in_place_relocation_prepare_remset(ZPage* from_page) {\n+    if (_forwarding->from_age() != ZPageAge::old) {\n+      \/\/ Only old pages have use remset bits\n+      return;\n+    }\n+\n+    if (ZGeneration::old()->active_remset_is_current()) {\n+      \/\/ We want to iterate over and clear the remset bits of the from-space page,\n+      \/\/ and insert current bits in the to-space page. However, with in-place\n+      \/\/ relocation, the from-space and to-space pages are the same. Clearing\n+      \/\/ is destructive, and is difficult to perform before or during the iteration.\n+      \/\/ However, clearing of the current bits has to be done before exposing the\n+      \/\/ to-space objects in the forwarding table.\n+      \/\/\n+      \/\/ To solve this tricky dependency problem, we start by stashing away the\n+      \/\/ current bits in the previous bits, and clearing the current bits\n+      \/\/ (implemented by swapping the bits). This way, the current bits are\n+      \/\/ cleared before copying the objects (like a normal to-space page),\n+      \/\/ and the previous bits are representing a copy of the current bits\n+      \/\/ of the from-space page, and are used for iteration.\n+      from_page->swap_remset_bitmaps();\n+    }\n+  }\n+\n+  ZPage* start_in_place_relocation(zoffset relocated_watermark) {\n+    _forwarding->in_place_relocation_claim_page();\n+    _forwarding->in_place_relocation_start(relocated_watermark);\n+\n+    ZPage* const from_page = _forwarding->page();\n+\n+    const ZPageAge to_age = _forwarding->to_age();\n+    const bool promotion = _forwarding->is_promotion();\n+\n+    \/\/ Promotions happen through a new cloned page\n+    ZPage* const to_page = promotion ? from_page->clone_limited() : from_page;\n+    to_page->reset(to_age, ZPageResetType::InPlaceRelocation);\n+\n+    \/\/ Clear remset bits for all objects that were relocated\n+    \/\/ before this page became an in-place relocated page.\n+    start_in_place_relocation_prepare_remset(from_page);\n+\n+    if (promotion) {\n+      \/\/ Register the the promotion\n+      ZGeneration::young()->in_place_relocate_promote(from_page, to_page);\n+      ZGeneration::young()->register_in_place_relocate_promoted(from_page);\n+    }\n+\n+    return to_page;\n+  }\n+\n+  void relocate_object(oop obj) {\n+    const zaddress addr = to_zaddress(obj);\n@@ -320,1 +880,1 @@\n-    while (!relocate_object(addr)) {\n+    while (!try_relocate_object(addr)) {\n@@ -324,2 +884,4 @@\n-      _target = _allocator->alloc_target_page(_forwarding, _target);\n-      if (_target != NULL) {\n+      const ZPageAge to_age = _forwarding->to_age();\n+      ZPage* to_page = _allocator->alloc_and_retire_target_page(_forwarding, target(to_age));\n+      set_target(to_age, to_page);\n+      if (to_page != nullptr) {\n@@ -329,6 +891,5 @@\n-      \/\/ Claim the page being relocated to block other threads from accessing\n-      \/\/ it, or its forwarding table, until it has been released (relocation\n-      \/\/ completed).\n-      _target = _forwarding->claim_page();\n-      _target->reset_for_in_place_relocation();\n-      _forwarding->set_in_place();\n+      \/\/ Start in-place relocation to block other threads from accessing\n+      \/\/ the page, or its forwarding table, until it has been released\n+      \/\/ (relocation completed).\n+      to_page = start_in_place_relocation(ZAddress::offset(addr));\n+      set_target(to_age, to_page);\n@@ -343,4 +904,16 @@\n-  ZRelocateClosure(Allocator* allocator) :\n-      _allocator(allocator),\n-      _forwarding(NULL),\n-      _target(NULL) {}\n+  ZRelocateWork(Allocator* allocator, ZGeneration* generation)\n+    : _allocator(allocator),\n+      _forwarding(nullptr),\n+      _target(),\n+      _generation(generation),\n+      _other_promoted(0),\n+      _other_compacted(0) {}\n+\n+  ~ZRelocateWork() {\n+    for (uint i = 0; i < ZAllocator::_relocation_allocators; ++i) {\n+      _allocator->free_target_page(_target[i]);\n+    }\n+    \/\/ Report statistics on-behalf of non-worker threads\n+    _generation->increase_promoted(_other_promoted);\n+    _generation->increase_compacted(_other_compacted);\n+  }\n@@ -348,2 +921,26 @@\n-  ~ZRelocateClosure() {\n-    _allocator->free_target_page(_target);\n+  bool active_remset_is_current() const {\n+    \/\/ Normal old-to-old relocation can treat the from-page remset as a\n+    \/\/ read-only copy, and then copy over the appropriate remset bits to the\n+    \/\/ cleared to-page's 'current' remset bitmap.\n+    \/\/\n+    \/\/ In-place relocation is more complicated. Since, the same page is both\n+    \/\/ a from-page and a to-page, we need to remove the old remset bits, and\n+    \/\/ add remset bits that corresponds to the new locations of the relocated\n+    \/\/ objects.\n+    \/\/\n+    \/\/ Depending on how long ago (in terms of number of young GC's and the\n+    \/\/ current young GC's phase), the page was allocated, the active\n+    \/\/ remembered set will be in either the 'current' or 'previous' bitmap.\n+    \/\/\n+    \/\/ If the active bits are in the 'previous' bitmap, we know that the\n+    \/\/ 'current' bitmap was cleared at some earlier point in time, and we can\n+    \/\/ simply set new bits in 'current' bitmap, and later when relocation has\n+    \/\/ read all the old remset bits, we could just clear the 'previous' remset\n+    \/\/ bitmap.\n+    \/\/\n+    \/\/ If, on the other hand, the active bits are in the 'current' bitmap, then\n+    \/\/ that bitmap will be used to both read the old remset bits, and the\n+    \/\/ destination for the remset bits that we copy when an object is copied\n+    \/\/ to it's new location within the page. We need to *carefully* remove all\n+    \/\/ all old remset bits, without clearing out the newly set bits.\n+    return ZGeneration::old()->active_remset_is_current();\n@@ -352,2 +949,10 @@\n-  void do_forwarding(ZForwarding* forwarding) {\n-    _forwarding = forwarding;\n+  void clear_remset_before_reuse(ZPage* page, bool in_place) {\n+    if (_forwarding->from_age() != ZPageAge::old) {\n+      \/\/ No remset bits\n+      return;\n+    }\n+\n+    if (in_place) {\n+      \/\/ Clear 'previous' remset bits. For in-place relocated pages, the previous\n+      \/\/ remset bits are always used, even when active_remset_is_current().\n+      page->clear_remset_previous();\n@@ -355,3 +960,0 @@\n-    \/\/ Check if we should abort\n-    if (ZAbort::should_abort()) {\n-      _forwarding->abort_page();\n@@ -361,0 +963,29 @@\n+    \/\/ Normal relocate\n+\n+    \/\/ Clear active remset bits\n+    if (active_remset_is_current()) {\n+      page->clear_remset_current();\n+    } else {\n+      page->clear_remset_previous();\n+    }\n+\n+    \/\/ Verify that inactive remset bits are all cleared\n+    if (active_remset_is_current()) {\n+      page->verify_remset_cleared_previous();\n+    } else {\n+      page->verify_remset_cleared_current();\n+    }\n+  }\n+\n+  void finish_in_place_relocation() {\n+    \/\/ We are done with the from_space copy of the page\n+    _forwarding->in_place_relocation_finish();\n+  }\n+\n+  void do_forwarding(ZForwarding* forwarding) {\n+    _forwarding = forwarding;\n+\n+    _forwarding->page()->log_msg(\" (relocate page)\");\n+\n+    ZVerify::before_relocation(_forwarding);\n+\n@@ -362,1 +993,3 @@\n-    _forwarding->object_iterate(this);\n+    _forwarding->object_iterate([&](oop obj) { relocate_object(obj); });\n+\n+    ZVerify::after_relocation(_forwarding);\n@@ -369,0 +1002,13 @@\n+    _generation->increase_freed(_forwarding->page()->size());\n+\n+    \/\/ Deal with in-place relocation\n+    const bool in_place = _forwarding->in_place_relocation();\n+    if (in_place) {\n+      finish_in_place_relocation();\n+    }\n+\n+    \/\/ Old from-space pages need to deal with remset bits\n+    if (_forwarding->from_age() == ZPageAge::old) {\n+      _forwarding->relocated_remembered_fields_after_relocate();\n+    }\n+\n@@ -372,5 +1018,13 @@\n-    if (_forwarding->in_place()) {\n-      \/\/ The relocated page has been relocated in-place and should not\n-      \/\/ be freed. Keep it as target page until it is full, and offer to\n-      \/\/ share it with other worker threads.\n-      _allocator->share_target_page(_target);\n+    if (in_place) {\n+      \/\/ Wait for all other threads to call release_page\n+      ZPage* const page = _forwarding->detach_page();\n+\n+      \/\/ Ensure that previous remset bits are cleared\n+      clear_remset_before_reuse(page, true \/* in_place *\/);\n+\n+      page->log_msg(\" (relocate page done in-place)\");\n+\n+      \/\/ Different pages when promoting\n+      ZPage* const target_page = target(_forwarding->to_age());\n+      _allocator->share_target_page(target_page);\n+\n@@ -378,1 +1032,1 @@\n-      \/\/ Detach and free relocated page\n+      \/\/ Wait for all other threads to call release_page\n@@ -380,1 +1034,10 @@\n-      _allocator->free_relocated_page(page);\n+\n+      \/\/ Ensure that all remset bits are cleared\n+      \/\/ Note: cleared after detach_page, when we know that\n+      \/\/ the young generation isn't scanning the remset.\n+      clear_remset_before_reuse(page, false \/* in_place *\/);\n+\n+      page->log_msg(\" (relocate page done normal)\");\n+\n+      \/\/ Free page\n+      ZHeap::heap()->free_page(page);\n@@ -385,1 +1048,29 @@\n-class ZRelocateTask : public ZTask {\n+class ZRelocateStoreBufferInstallBasePointersThreadClosure : public ThreadClosure {\n+public:\n+  virtual void do_thread(Thread* thread) {\n+    JavaThread* const jt = JavaThread::cast(thread);\n+    ZStoreBarrierBuffer* buffer = ZThreadLocalData::store_barrier_buffer(jt);\n+    buffer->install_base_pointers();\n+  }\n+};\n+\n+\/\/ Installs the object base pointers (object starts), for the fields written\n+\/\/ in the store buffer. The code that searches for the object start uses that\n+\/\/ liveness information stored in the pages. That information is lost when the\n+\/\/ pages have been relocated and then destroyed.\n+class ZRelocateStoreBufferInstallBasePointersTask : public ZTask {\n+private:\n+  ZJavaThreadsIterator _threads_iter;\n+\n+public:\n+  ZRelocateStoreBufferInstallBasePointersTask(ZGeneration* generation)\n+    : ZTask(\"ZRelocateStoreBufferInstallBasePointersTask\"),\n+      _threads_iter(generation->id_optional()) {}\n+\n+  virtual void work() {\n+    ZRelocateStoreBufferInstallBasePointersThreadClosure fix_store_buffer_cl;\n+    _threads_iter.apply(&fix_store_buffer_cl);\n+  }\n+};\n+\n+class ZRelocateTask : public ZRestartableTask {\n@@ -388,0 +1079,2 @@\n+  ZGeneration* const             _generation;\n+  ZRelocateQueue* const          _queue;\n@@ -391,6 +1084,2 @@\n-  static bool is_small(ZForwarding* forwarding) {\n-    return forwarding->type() == ZPageTypeSmall;\n-  }\n-\n-  ZRelocateTask(ZRelocationSet* relocation_set) :\n-      ZTask(\"ZRelocateTask\"),\n+  ZRelocateTask(ZRelocationSet* relocation_set, ZRelocateQueue* queue)\n+    : ZRestartableTask(\"ZRelocateTask\"),\n@@ -399,2 +1088,4 @@\n-      _small_allocator(),\n-      _medium_allocator() {}\n+      _generation(relocation_set->generation()),\n+      _queue(queue),\n+      _small_allocator(_generation),\n+      _medium_allocator(_generation) {}\n@@ -403,2 +1094,1 @@\n-    ZStatRelocation::set_at_relocate_end(_small_allocator.in_place_count(),\n-                                         _medium_allocator.in_place_count());\n+    _generation->stat_relocation()->at_relocate_end(_small_allocator.in_place_count(), _medium_allocator.in_place_count());\n@@ -408,2 +1098,2 @@\n-    ZRelocateClosure<ZRelocateSmallAllocator> small(&_small_allocator);\n-    ZRelocateClosure<ZRelocateMediumAllocator> medium(&_medium_allocator);\n+    ZRelocateWork<ZRelocateSmallAllocator> small(&_small_allocator, _generation);\n+    ZRelocateWork<ZRelocateMediumAllocator> medium(&_medium_allocator, _generation);\n@@ -412,2 +1102,3 @@\n-    for (ZForwarding* forwarding; _iter.next(&forwarding);) {\n-      if (is_small(forwarding)) {\n+    const auto do_forwarding = [&](ZForwarding* forwarding) {\n+      ZPage* const page = forwarding->page();\n+      if (page->is_small()) {\n@@ -418,0 +1109,109 @@\n+\n+      \/\/ Absolute last thing done while relocating a page.\n+      \/\/\n+      \/\/ We don't use the SuspendibleThreadSet when relocating pages.\n+      \/\/ Instead the ZRelocateQueue is used as a pseudo STS joiner\/leaver.\n+      \/\/\n+      \/\/ After the mark_done call a safepointing could be completed and a\n+      \/\/ new GC phase could be entered.\n+      forwarding->mark_done();\n+    };\n+\n+    const auto claim_and_do_forwarding = [&](ZForwarding* forwarding) {\n+      if (forwarding->claim()) {\n+        do_forwarding(forwarding);\n+      }\n+    };\n+\n+    const auto do_forwarding_one_from_iter = [&]() {\n+      ZForwarding* forwarding;\n+\n+      if (_iter.next(&forwarding)) {\n+        claim_and_do_forwarding(forwarding);\n+        return true;\n+      }\n+\n+      return false;\n+    };\n+\n+    for (;;) {\n+      \/\/ As long as there are requests in the relocate queue, there are threads\n+      \/\/ waiting in a VM state that does not allow them to be blocked. The\n+      \/\/ worker thread needs to finish relocate these pages, and allow the\n+      \/\/ other threads to continue and proceed to a blocking state. After that,\n+      \/\/ the worker threads are allowed to safepoint synchronize.\n+      for (ZForwarding* forwarding; (forwarding = _queue->synchronize_poll()) != nullptr;) {\n+        do_forwarding(forwarding);\n+      }\n+\n+      if (!do_forwarding_one_from_iter()) {\n+        \/\/ No more work\n+        break;\n+      }\n+\n+      if (_generation->should_worker_resize()) {\n+        break;\n+      }\n+    }\n+\n+    _queue->leave();\n+  }\n+\n+  virtual void resize_workers(uint nworkers) {\n+    _queue->resize_workers(nworkers);\n+  }\n+};\n+\n+static void remap_and_maybe_add_remset(volatile zpointer* p) {\n+  const zpointer ptr = Atomic::load(p);\n+\n+  if (ZPointer::is_store_good(ptr)) {\n+    \/\/ Already has a remset entry\n+    return;\n+  }\n+\n+  \/\/ Remset entries are used for two reasons:\n+  \/\/ 1) Young marking old-to-young pointer roots\n+  \/\/ 2) Deferred remapping of stale old-to-young pointers\n+  \/\/\n+  \/\/ This load barrier will up-front perform the remapping of (2),\n+  \/\/ and the code below only has to make sure we register up-to-date\n+  \/\/ old-to-young pointers for (1).\n+  const zaddress addr = ZBarrier::load_barrier_on_oop_field_preloaded(p, ptr);\n+\n+  if (is_null(addr)) {\n+    \/\/ No need for remset entries for null pointers\n+    return;\n+  }\n+\n+  if (ZHeap::heap()->is_old(addr)) {\n+    \/\/ No need for remset entries for pointers to old gen\n+    return;\n+  }\n+\n+  ZRelocate::add_remset(p);\n+}\n+\n+class ZRelocateAddRemsetForFlipPromoted : public ZRestartableTask {\n+private:\n+  ZStatTimerYoung                _timer;\n+  ZArrayParallelIterator<ZPage*> _iter;\n+\n+public:\n+  ZRelocateAddRemsetForFlipPromoted(ZArray<ZPage*>* pages)\n+    : ZRestartableTask(\"ZRelocateAddRemsetForFlipPromoted\"),\n+      _timer(ZSubPhaseConcurrentRelocateRememberedSetFlipPromotedYoung),\n+      _iter(pages) {}\n+\n+  virtual void work() {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+\n+    for (ZPage* page; _iter.next(&page);) {\n+      page->object_iterate([&](oop obj) {\n+        ZIterator::basic_oop_iterate_safe(obj, remap_and_maybe_add_remset);\n+      });\n+\n+      SuspendibleThreadSet::yield();\n+      if (ZGeneration::young()->should_worker_resize()) {\n+        return;\n+      }\n@@ -423,2 +1223,100 @@\n-  ZRelocateTask task(relocation_set);\n-  _workers->run(&task);\n+  {\n+    \/\/ Install the store buffer's base pointers before the\n+    \/\/ relocate task destroys the liveness information in\n+    \/\/ the relocated pages.\n+    ZRelocateStoreBufferInstallBasePointersTask buffer_task(_generation);\n+    workers()->run(&buffer_task);\n+  }\n+\n+  {\n+    ZRelocateTask relocate_task(relocation_set, &_queue);\n+    workers()->run(&relocate_task);\n+  }\n+\n+  if (relocation_set->generation()->is_young()) {\n+    ZRelocateAddRemsetForFlipPromoted task(relocation_set->flip_promoted_pages());\n+    workers()->run(&task);\n+  }\n+\n+  _queue.clear();\n+}\n+\n+ZPageAge ZRelocate::compute_to_age(ZPageAge from_age) {\n+  if (from_age == ZPageAge::old) {\n+    return ZPageAge::old;\n+  }\n+\n+  const uint age = static_cast<uint>(from_age);\n+  if (age >= ZGeneration::young()->tenuring_threshold()) {\n+    return ZPageAge::old;\n+  }\n+\n+  return static_cast<ZPageAge>(age + 1);\n+}\n+\n+class ZFlipAgePagesTask : public ZTask {\n+private:\n+  ZArrayParallelIterator<ZPage*> _iter;\n+\n+public:\n+  ZFlipAgePagesTask(const ZArray<ZPage*>* pages)\n+    : ZTask(\"ZPromotePagesTask\"),\n+      _iter(pages) {}\n+\n+  virtual void work() {\n+    SuspendibleThreadSetJoiner sts_joiner;\n+    ZArray<ZPage*> promoted_pages;\n+\n+    for (ZPage* prev_page; _iter.next(&prev_page);) {\n+      const ZPageAge from_age = prev_page->age();\n+      const ZPageAge to_age = ZRelocate::compute_to_age(from_age);\n+      assert(from_age != ZPageAge::old, \"invalid age for a young collection\");\n+\n+      \/\/ Figure out if this is proper promotion\n+      const bool promotion = to_age == ZPageAge::old;\n+\n+      if (promotion) {\n+        \/\/ Before promoting an object (and before relocate start), we must ensure that all\n+        \/\/ contained zpointers are store good. The marking code ensures that for non-null\n+        \/\/ pointers, but null pointers are ignored. This code ensures that even null pointers\n+        \/\/ are made store good, for the promoted objects.\n+        prev_page->object_iterate([&](oop obj) {\n+          ZIterator::basic_oop_iterate_safe(obj, ZBarrier::promote_barrier_on_young_oop_field);\n+        });\n+      }\n+\n+      \/\/ Logging\n+      prev_page->log_msg(promotion ? \" (flip promoted)\" : \" (flip survived)\");\n+\n+      \/\/ Setup to-space page\n+      ZPage* const new_page = promotion ? prev_page->clone_limited_promote_flipped() : prev_page;\n+      new_page->reset(to_age, ZPageResetType::FlipAging);\n+\n+      if (promotion) {\n+        ZGeneration::young()->flip_promote(prev_page, new_page);\n+        \/\/ Defer promoted page registration times the lock is taken\n+        promoted_pages.push(prev_page);\n+      }\n+\n+      SuspendibleThreadSet::yield();\n+    }\n+\n+    ZGeneration::young()->register_flip_promoted(promoted_pages);\n+  }\n+};\n+\n+void ZRelocate::flip_age_pages(const ZArray<ZPage*>* pages) {\n+  ZFlipAgePagesTask flip_age_task(pages);\n+  workers()->run(&flip_age_task);\n+}\n+\n+void ZRelocate::synchronize() {\n+  _queue.synchronize();\n+}\n+\n+void ZRelocate::desynchronize() {\n+  _queue.desynchronize();\n+}\n+\n+ZRelocateQueue* ZRelocate::queue() {\n+  return &_queue;\n","filename":"src\/hotspot\/share\/gc\/z\/zRelocate.cpp","additions":1047,"deletions":149,"binary":false,"changes":1196,"status":"modified"},{"patch":"@@ -48,0 +48,1 @@\n+#include \"oops\/resolvedIndyEntry.hpp\"\n","filename":"src\/hotspot\/share\/interpreter\/zero\/bytecodeInterpreter.cpp","additions":1,"deletions":0,"binary":false,"changes":1,"status":"modified"},{"patch":"@@ -452,1 +452,1 @@\n-      if (offset == ConstantPool::pool_holder_offset_in_bytes()) {\n+      if (offset == in_bytes(ConstantPool::pool_holder_offset())) {\n@@ -817,1 +817,1 @@\n-C2V_VMENTRY_0(jint, lookupNameAndTypeRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+C2V_VMENTRY_0(jint, lookupNameAndTypeRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jint opcode))\n@@ -819,1 +819,1 @@\n-  return cp->name_and_type_ref_index_at(index);\n+  return cp->name_and_type_ref_index_at(index, (Bytecodes::Code)opcode);\n@@ -822,1 +822,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupNameInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+C2V_VMENTRY_NULL(jobject, lookupNameInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which, jint opcode))\n@@ -824,1 +824,1 @@\n-  JVMCIObject sym = JVMCIENV->create_string(cp->name_ref_at(which), JVMCI_CHECK_NULL);\n+  JVMCIObject sym = JVMCIENV->create_string(cp->name_ref_at(which, (Bytecodes::Code)opcode), JVMCI_CHECK_NULL);\n@@ -828,1 +828,1 @@\n-C2V_VMENTRY_NULL(jobject, lookupSignatureInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which))\n+C2V_VMENTRY_NULL(jobject, lookupSignatureInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint which, jint opcode))\n@@ -830,1 +830,1 @@\n-  JVMCIObject sym = JVMCIENV->create_string(cp->signature_ref_at(which), JVMCI_CHECK_NULL);\n+  JVMCIObject sym = JVMCIENV->create_string(cp->signature_ref_at(which, (Bytecodes::Code)opcode), JVMCI_CHECK_NULL);\n@@ -834,1 +834,1 @@\n-C2V_VMENTRY_0(jint, lookupKlassRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index))\n+C2V_VMENTRY_0(jint, lookupKlassRefIndexInPool, (JNIEnv* env, jobject, ARGUMENT_PAIR(cp), jint index, jint opcode))\n@@ -836,1 +836,1 @@\n-  return cp->klass_ref_index_at(index);\n+  return cp->klass_ref_index_at(index, (Bytecodes::Code)opcode);\n@@ -909,1 +909,1 @@\n-  LinkInfo link_info(cp, index, mh, CHECK_NULL);\n+  LinkInfo link_info(cp, index, mh, code, CHECK_NULL);\n@@ -1595,2 +1595,2 @@\n-  Klass* holder = cp->klass_ref_at(index, CHECK);\n-  Symbol* name = cp->name_ref_at(index);\n+  Klass* holder = cp->klass_ref_at(index, Bytecodes::_invokehandle, CHECK);\n+  Symbol* name = cp->name_ref_at(index, Bytecodes::_invokehandle);\n@@ -1612,1 +1612,1 @@\n-    LinkInfo link_info(cp, index, CATCH);\n+    LinkInfo link_info(cp, index, Bytecodes::_invokehandle, CATCH);\n@@ -1616,1 +1616,1 @@\n-    Symbol* name_sym = cp->name_ref_at(index);\n+    Symbol* name_sym = cp->name_ref_at(index, Bytecodes::_invokehandle);\n@@ -2402,1 +2402,2 @@\n-    JVMCIEnv __peer_jvmci_env__(thread, false, __FILE__, __LINE__);\n+    bool jni_enomem_is_fatal = false;\n+    JVMCIEnv __peer_jvmci_env__(thread, false, jni_enomem_is_fatal, __FILE__, __LINE__);\n@@ -2404,0 +2405,3 @@\n+    if (peerEnv->has_jni_enomem()) {\n+      JVMCI_THROW_MSG_0(OutOfMemoryError, \"JNI_ENOMEM creating or attaching to libjvmci\");\n+    }\n@@ -2407,1 +2411,1 @@\n-      peerEnv->describe_pending_exception(true);\n+      peerEnv->describe_pending_exception(tty);\n@@ -2566,1 +2570,2 @@\n-      JVMCIEnv __peer_jvmci_env__(thread, false, __FILE__, __LINE__);\n+      bool jni_enomem_is_fatal = false;\n+      JVMCIEnv __peer_jvmci_env__(thread, false, jni_enomem_is_fatal, __FILE__, __LINE__);\n@@ -2568,0 +2573,4 @@\n+      if (peerJVMCIEnv->has_jni_enomem()) {\n+        JVMCI_THROW_MSG_0(OutOfMemoryError, \"JNI_ENOMEM creating or attaching to libjvmci\");\n+      }\n+\n@@ -2571,1 +2580,1 @@\n-        peerJVMCIEnv->describe_pending_exception(true);\n+        peerJVMCIEnv->describe_pending_exception(tty);\n@@ -2661,1 +2670,2 @@\n-  JVMCIEnv __peer_jvmci_env__(thread, !JVMCIENV->is_hotspot(), __FILE__, __LINE__);\n+  bool jni_enomem_is_fatal = false;\n+  JVMCIEnv __peer_jvmci_env__(thread, !JVMCIENV->is_hotspot(), jni_enomem_is_fatal, __FILE__, __LINE__);\n@@ -2664,0 +2674,3 @@\n+  if (peerEnv->has_jni_enomem()) {\n+      JVMCI_THROW_MSG_0(OutOfMemoryError, \"JNI_ENOMEM creating or attaching to libjvmci\");\n+  }\n@@ -3088,4 +3101,4 @@\n-  {CC \"lookupNameInPool\",                             CC \"(\" HS_CONSTANT_POOL2 \"I)\" STRING,                                                 FN_PTR(lookupNameInPool)},\n-  {CC \"lookupNameAndTypeRefIndexInPool\",              CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(lookupNameAndTypeRefIndexInPool)},\n-  {CC \"lookupSignatureInPool\",                        CC \"(\" HS_CONSTANT_POOL2 \"I)\" STRING,                                                 FN_PTR(lookupSignatureInPool)},\n-  {CC \"lookupKlassRefIndexInPool\",                    CC \"(\" HS_CONSTANT_POOL2 \"I)I\",                                                       FN_PTR(lookupKlassRefIndexInPool)},\n+  {CC \"lookupNameInPool\",                             CC \"(\" HS_CONSTANT_POOL2 \"II)\" STRING,                                                FN_PTR(lookupNameInPool)},\n+  {CC \"lookupNameAndTypeRefIndexInPool\",              CC \"(\" HS_CONSTANT_POOL2 \"II)I\",                                                      FN_PTR(lookupNameAndTypeRefIndexInPool)},\n+  {CC \"lookupSignatureInPool\",                        CC \"(\" HS_CONSTANT_POOL2 \"II)\" STRING,                                                FN_PTR(lookupSignatureInPool)},\n+  {CC \"lookupKlassRefIndexInPool\",                    CC \"(\" HS_CONSTANT_POOL2 \"II)I\",                                                      FN_PTR(lookupKlassRefIndexInPool)},\n","filename":"src\/hotspot\/share\/jvmci\/jvmciCompilerToVM.cpp","additions":36,"deletions":23,"binary":false,"changes":59,"status":"modified"},{"patch":"@@ -35,0 +35,1 @@\n+#include \"prims\/jvmtiThreadState.hpp\"\n@@ -218,0 +219,4 @@\n+  JVMTI_ONLY(nonstatic_field(JavaThread,       _is_in_VTMS_transition,                        bool))                                 \\\n+  JVMTI_ONLY(nonstatic_field(JavaThread,       _is_in_tmp_VTMS_transition,                    bool))                                 \\\n+                                                                                                                                     \\\n+  JVMTI_ONLY(static_field(JvmtiVTMSTransitionDisabler, _VTMS_notify_jvmti_events,             bool))                                 \\\n@@ -368,0 +373,1 @@\n+  static_field(java_lang_Thread,            _jvmti_is_in_VTMS_transition_offset,              int)                                   \\\n@@ -741,0 +747,1 @@\n+  declare_constant(markWord::lock_mask_in_place)                          \\\n@@ -757,0 +764,4 @@\n+  JVMTI_ONLY(declare_function(SharedRuntime::notify_jvmti_vthread_start)) \\\n+  JVMTI_ONLY(declare_function(SharedRuntime::notify_jvmti_vthread_end))   \\\n+  JVMTI_ONLY(declare_function(SharedRuntime::notify_jvmti_vthread_mount)) \\\n+  JVMTI_ONLY(declare_function(SharedRuntime::notify_jvmti_vthread_unmount)) \\\n","filename":"src\/hotspot\/share\/jvmci\/vmStructs_jvmci.cpp","additions":11,"deletions":0,"binary":false,"changes":11,"status":"modified"},{"patch":"@@ -902,1 +902,1 @@\n-  assert(loader_data != nullptr, \"Should never pass around a nullptr loader_data. \"\n+  assert(loader_data != nullptr, \"Should never pass around a null loader_data. \"\n","filename":"src\/hotspot\/share\/memory\/metaspace.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -137,1 +137,1 @@\n-  verify();\n+  SOMETIMES(verify();)\n@@ -139,1 +139,1 @@\n-    verify_allocation_guards();\n+    SOMETIMES(verify_allocation_guards();)\n@@ -164,1 +164,1 @@\n-  DEBUG_ONLY(chunk_manager()->verify();)\n+  SOMETIMES(chunk_manager()->verify();)\n@@ -425,1 +425,1 @@\n-    UL(info, \"allocation failed, returned nullptr.\");\n+    UL(info, \"allocation failed, returned null.\");\n@@ -450,1 +450,1 @@\n-  DEBUG_ONLY(verify_locked();)\n+  SOMETIMES(verify_locked();)\n","filename":"src\/hotspot\/share\/memory\/metaspace\/metaspaceArena.cpp","additions":5,"deletions":5,"binary":false,"changes":10,"status":"modified"},{"patch":"@@ -326,1 +326,1 @@\n-    DEBUG_ONLY(c->verify();)\n+    SOMETIMES(c->verify();)\n@@ -342,1 +342,1 @@\n-  DEBUG_ONLY(rca->verify_area_is_ideally_merged();)\n+  SOMETIMES(rca->verify_area_is_ideally_merged();)\n@@ -362,1 +362,1 @@\n-  DEBUG_ONLY(rca->verify_area_is_ideally_merged();)\n+  SOMETIMES(rca->verify_area_is_ideally_merged();)\n@@ -383,1 +383,1 @@\n-  DEBUG_ONLY(rca->verify_area_is_ideally_merged();)\n+  SOMETIMES(rca->verify_area_is_ideally_merged();)\n@@ -419,1 +419,1 @@\n-volatile int test_access = 0;\n+volatile uint test_access = 0;\n@@ -440,1 +440,1 @@\n-        test_access += *(int*)p;\n+        test_access += *(uint*)p;\n","filename":"src\/hotspot\/share\/memory\/metaspace\/virtualSpaceNode.cpp","additions":6,"deletions":6,"binary":false,"changes":12,"status":"modified"},{"patch":"@@ -558,1 +558,1 @@\n-    if (UseCompressedClassPointers && !UseSharedSpaces &&\n+    if (UseCompressedClassPointers && !UseSharedSpaces && !DumpSharedSpaces &&\n@@ -602,1 +602,1 @@\n-      log_trace(gc, heap, coops)(\"Trying to allocate at address nullptr heap of size \" SIZE_FORMAT_X, size + noaccess_prefix);\n+      log_trace(gc, heap, coops)(\"Trying to allocate at address null heap of size \" SIZE_FORMAT_X, size + noaccess_prefix);\n","filename":"src\/hotspot\/share\/memory\/virtualspace.cpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -281,1 +281,1 @@\n-    set_super_check_offset((address)super_check_cell - (address) this);\n+    set_super_check_offset(u4((address)super_check_cell - (address) this));\n","filename":"src\/hotspot\/share\/oops\/klass.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -40,13 +40,0 @@\n-\/\/ Klass Kinds for all subclasses of Klass\n-enum KlassKind {\n-  InstanceKlassKind,\n-  InstanceRefKlassKind,\n-  InstanceMirrorKlassKind,\n-  InstanceClassLoaderKlassKind,\n-  InstanceStackChunkKlassKind,\n-  TypeArrayKlassKind,\n-  ObjArrayKlassKind\n-};\n-\n-const uint KLASS_KIND_COUNT = ObjArrayKlassKind + 1;\n-\n@@ -82,0 +69,14 @@\n+ public:\n+  \/\/ Klass Kinds for all subclasses of Klass\n+  enum KlassKind {\n+    InstanceKlassKind,\n+    InstanceRefKlassKind,\n+    InstanceMirrorKlassKind,\n+    InstanceClassLoaderKlassKind,\n+    InstanceStackChunkKlassKind,\n+    TypeArrayKlassKind,\n+    ObjArrayKlassKind,\n+    UnknownKlassKind\n+  };\n+\n+  static const uint KLASS_KIND_COUNT = ObjArrayKlassKind + 1;\n@@ -83,0 +84,1 @@\n+\n@@ -175,1 +177,1 @@\n-  jshort _shared_class_path_index;\n+  s2 _shared_class_path_index;\n@@ -198,1 +200,1 @@\n-  Klass() : _kind(KlassKind(-1)) { assert(DumpSharedSpaces || UseSharedSpaces, \"only for cds\"); }\n+  Klass() : _kind(UnknownKlassKind) { assert(DumpSharedSpaces || UseSharedSpaces, \"only for cds\"); }\n@@ -308,1 +310,1 @@\n-  int shared_classpath_index() const   {\n+  s2 shared_classpath_index() const   {\n@@ -312,1 +314,1 @@\n-  void set_shared_classpath_index(int index) {\n+  void set_shared_classpath_index(s2 index) {\n@@ -327,1 +329,1 @@\n-    CDS_ONLY(_shared_class_flags &= ~_archived_lambda_proxy_is_available;)\n+    CDS_ONLY(_shared_class_flags &= (u2)(~_archived_lambda_proxy_is_available);)\n@@ -338,1 +340,1 @@\n-    CDS_ONLY(_shared_class_flags &= ~_has_value_based_class_annotation;)\n+    CDS_ONLY(_shared_class_flags &= (u2)(~_has_value_based_class_annotation);)\n@@ -389,10 +391,10 @@\n-  static ByteSize super_offset()                 { return in_ByteSize(offset_of(Klass, _super)); }\n-  static ByteSize super_check_offset_offset()    { return in_ByteSize(offset_of(Klass, _super_check_offset)); }\n-  static ByteSize primary_supers_offset()        { return in_ByteSize(offset_of(Klass, _primary_supers)); }\n-  static ByteSize secondary_super_cache_offset() { return in_ByteSize(offset_of(Klass, _secondary_super_cache)); }\n-  static ByteSize secondary_supers_offset()      { return in_ByteSize(offset_of(Klass, _secondary_supers)); }\n-  static ByteSize java_mirror_offset()           { return in_ByteSize(offset_of(Klass, _java_mirror)); }\n-  static ByteSize class_loader_data_offset()     { return in_ByteSize(offset_of(Klass, _class_loader_data)); }\n-  static ByteSize modifier_flags_offset()        { return in_ByteSize(offset_of(Klass, _modifier_flags)); }\n-  static ByteSize layout_helper_offset()         { return in_ByteSize(offset_of(Klass, _layout_helper)); }\n-  static ByteSize access_flags_offset()          { return in_ByteSize(offset_of(Klass, _access_flags)); }\n+  static ByteSize super_offset()                 { return byte_offset_of(Klass, _super); }\n+  static ByteSize super_check_offset_offset()    { return byte_offset_of(Klass, _super_check_offset); }\n+  static ByteSize primary_supers_offset()        { return byte_offset_of(Klass, _primary_supers); }\n+  static ByteSize secondary_super_cache_offset() { return byte_offset_of(Klass, _secondary_super_cache); }\n+  static ByteSize secondary_supers_offset()      { return byte_offset_of(Klass, _secondary_supers); }\n+  static ByteSize java_mirror_offset()           { return byte_offset_of(Klass, _java_mirror); }\n+  static ByteSize class_loader_data_offset()     { return byte_offset_of(Klass, _class_loader_data); }\n+  static ByteSize modifier_flags_offset()        { return byte_offset_of(Klass, _modifier_flags); }\n+  static ByteSize layout_helper_offset()         { return byte_offset_of(Klass, _layout_helper); }\n+  static ByteSize access_flags_offset()          { return byte_offset_of(Klass, _access_flags); }\n@@ -400,2 +402,2 @@\n-  static ByteSize subklass_offset()              { return in_ByteSize(offset_of(Klass, _subklass)); }\n-  static ByteSize next_sibling_offset()          { return in_ByteSize(offset_of(Klass, _next_sibling)); }\n+  static ByteSize subklass_offset()              { return byte_offset_of(Klass, _subklass); }\n+  static ByteSize next_sibling_offset()          { return byte_offset_of(Klass, _next_sibling); }\n","filename":"src\/hotspot\/share\/oops\/klass.hpp","additions":33,"deletions":31,"binary":false,"changes":64,"status":"modified"},{"patch":"@@ -201,1 +201,2 @@\n-void oopDesc::obj_field_put_raw(int offset, oop value)                { RawAccess<>::oop_store_at(as_oop(), offset, value); }\n+void oopDesc::obj_field_put_raw(int offset, oop value)                { assert(!(UseZGC && ZGenerational), \"Generational ZGC must use store barriers\");\n+                                                                        RawAccess<>::oop_store_at(as_oop(), offset, value); }\n","filename":"src\/hotspot\/share\/oops\/oop.cpp","additions":2,"deletions":1,"binary":false,"changes":3,"status":"modified"},{"patch":"@@ -77,0 +77,1 @@\n+  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -79,1 +80,0 @@\n-  static inline void release_set_mark(HeapWord* mem, markWord m);\n@@ -325,1 +325,1 @@\n-  static int mark_offset_in_bytes()      { return offset_of(oopDesc, _mark); }\n+  static int mark_offset_in_bytes()      { return (int)offset_of(oopDesc, _mark); }\n","filename":"src\/hotspot\/share\/oops\/oop.hpp","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -73,4 +73,0 @@\n-void oopDesc::release_set_mark(markWord m) {\n-  Atomic::release_store(&_mark, m);\n-}\n-\n@@ -81,0 +77,4 @@\n+void oopDesc::release_set_mark(markWord m) {\n+  Atomic::release_store(&_mark, m);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oop.inline.hpp","additions":4,"deletions":4,"binary":false,"changes":8,"status":"modified"},{"patch":"@@ -169,0 +169,4 @@\n+inline intptr_t p2i(narrowOop o) {\n+  return static_cast<intptr_t>(o);\n+}\n+\n","filename":"src\/hotspot\/share\/oops\/oopsHierarchy.hpp","additions":4,"deletions":0,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -29,0 +29,2 @@\n+#include \"opto\/compile.hpp\"\n+#include \"opto\/output.hpp\"\n@@ -30,0 +32,1 @@\n+#include \"utilities\/tuple.hpp\"\n@@ -34,0 +37,3 @@\n+template <class... Ts>\n+class C2GeneralStub;\n+\n@@ -39,0 +45,1 @@\n+  void add_to_stub_list();\n@@ -50,0 +57,4 @@\n+\n+  template <class... Ts>\n+  static C2GeneralStub<Ts...>* make(const Ts&... data, int max_size,\n+                                    void (*emit)(C2_MacroAssembler&, C2GeneralStub<Ts...>&));\n@@ -114,0 +125,49 @@\n+\/\/-----------------------------C2GeneralStub-----------------------------------\n+\/\/ A generalized stub that can be used to implement an arbitrary stub in a\n+\/\/ type-safe manner. An example:\n+\/\/\n+\/\/ Register dst; XMMRegister src;\n+\/\/ \/\/ The lambda defining how the code is emitted in the stub\n+\/\/ auto slowpath = [](C2_MacroAssembler& masm, C2GeneralStub<Register, XMMRegister>& stub) {\n+\/\/   \/\/ Access the saved data in a type safe manner\n+\/\/   Register dst = stub.get<0>();\n+\/\/   XMMRegister src = stub.get<1>();\n+\/\/   masm.bind(stub.entry());\n+\/\/   ...\n+\/\/   masm.jump(stub.continuation());\n+\/\/ }\n+\/\/ \/\/ Create a stub with 2 data fields being dst and src, a max size of 4 bytes\n+\/\/ \/\/ and predefined emission function\n+\/\/ auto stub = C2CodeStub::make<Register, XMMRegister>(dst, src, 4, slowpath);\n+\/\/ __ jump_conditional(stub->entry());\n+\/\/ ...\n+\/\/ __ bind(stub->continuation());\n+\/\/\n+template <class... Ts>\n+class C2GeneralStub : public C2CodeStub {\n+private:\n+  Tuple<Ts...> _data;\n+  int _max_size;\n+  void (*_emit)(C2_MacroAssembler&, C2GeneralStub&);\n+\n+  constexpr C2GeneralStub(const Ts&... data, int max_size,\n+                          void (*emit)(C2_MacroAssembler&, C2GeneralStub<Ts...>&))\n+    : _data(data...), _max_size(max_size), _emit(emit) {}\n+\n+  friend C2CodeStub;\n+public:\n+  template <std::size_t I>\n+  constexpr const auto& data() const { return _data.template get<I>(); }\n+\n+  int max_size() const { return _max_size; }\n+  void emit(C2_MacroAssembler& masm) { _emit(masm, *this); }\n+};\n+\n+template <class... Ts>\n+C2GeneralStub<Ts...>* C2CodeStub::make(const Ts&... data, int max_size,\n+                                       void (*emit)(C2_MacroAssembler&, C2GeneralStub<Ts...>&)) {\n+  auto stub = new (Compile::current()->comp_arena()) C2GeneralStub<Ts...>(data..., max_size, emit);\n+  stub->add_to_stub_list();\n+  return stub;\n+}\n+\n","filename":"src\/hotspot\/share\/opto\/c2_CodeStubs.hpp","additions":60,"deletions":0,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -789,1 +789,1 @@\n-bool CallNode::may_modify(const TypeOopPtr *t_oop, PhaseTransform *phase) {\n+bool CallNode::may_modify(const TypeOopPtr* t_oop, PhaseValues* phase) {\n@@ -1590,1 +1590,1 @@\n-Node *AllocateArrayNode::make_ideal_length(const TypeOopPtr* oop_type, PhaseTransform *phase, bool allow_new_nodes) {\n+Node *AllocateArrayNode::make_ideal_length(const TypeOopPtr* oop_type, PhaseValues* phase, bool allow_new_nodes) {\n@@ -2204,1 +2204,1 @@\n-bool CallNode::may_modify_arraycopy_helper(const TypeOopPtr* dest_t, const TypeOopPtr *t_oop, PhaseTransform *phase) {\n+bool CallNode::may_modify_arraycopy_helper(const TypeOopPtr* dest_t, const TypeOopPtr* t_oop, PhaseValues* phase) {\n","filename":"src\/hotspot\/share\/opto\/callnode.cpp","additions":3,"deletions":3,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -390,1 +390,1 @@\n-    remove_skeleton_predicate_opaq(dead);\n+    remove_template_assertion_predicate_opaq(dead);\n@@ -410,1 +410,1 @@\n-void Compile::disconnect_useless_nodes(Unique_Node_List &useful, Unique_Node_List* worklist) {\n+void Compile::disconnect_useless_nodes(Unique_Node_List& useful, Unique_Node_List& worklist) {\n@@ -433,1 +433,2 @@\n-      worklist->push(n->unique_out());\n+      assert(useful.member(n->unique_out()), \"do not push a useless node\");\n+      worklist.push(n->unique_out());\n@@ -438,2 +439,2 @@\n-  remove_useless_nodes(_predicate_opaqs,    useful); \/\/ remove useless predicate opaque nodes\n-  remove_useless_nodes(_skeleton_predicate_opaqs, useful);\n+  remove_useless_nodes(_parse_predicate_opaqs, useful); \/\/ remove useless Parse Predicate opaque nodes\n+  remove_useless_nodes(_template_assertion_predicate_opaqs, useful); \/\/ remove useless Assertion Predicate opaque nodes\n@@ -581,1 +582,0 @@\n-debug_only( int Compile::_debug_idx = 100000; )\n@@ -619,2 +619,2 @@\n-                  _predicate_opaqs   (comp_arena(), 8, 0, nullptr),\n-                  _skeleton_predicate_opaqs (comp_arena(), 8, 0, nullptr),\n+                  _parse_predicate_opaqs (comp_arena(), 8, 0, nullptr),\n+                  _template_assertion_predicate_opaqs (comp_arena(), 8, 0, nullptr),\n@@ -634,1 +634,3 @@\n-                  _for_igvn(nullptr),\n+                  _igvn_worklist(nullptr),\n+                  _types(nullptr),\n+                  _node_hash(nullptr),\n@@ -681,0 +683,1 @@\n+#endif\n@@ -685,1 +688,0 @@\n-#endif\n@@ -707,3 +709,0 @@\n-  \/\/ Node list that Iterative GVN will start with\n-  Unique_Node_List for_igvn(comp_arena());\n-  set_for_igvn(&for_igvn);\n@@ -714,1 +713,4 @@\n-  PhaseGVN gvn(node_arena(), estimated_size);\n+  _igvn_worklist = new (comp_arena()) Unique_Node_List(comp_arena());\n+  _types = new (comp_arena()) Type_Array(comp_arena());\n+  _node_hash = new (comp_arena()) NodeHash(comp_arena(), estimated_size);\n+  PhaseGVN gvn;\n@@ -804,1 +806,1 @@\n-      PhaseRemoveUseless pru(initial_gvn(), &for_igvn);\n+      PhaseRemoveUseless pru(initial_gvn(), *igvn_worklist());\n@@ -921,1 +923,3 @@\n-    _for_igvn(nullptr),\n+    _igvn_worklist(nullptr),\n+    _types(nullptr),\n+    _node_hash(nullptr),\n@@ -953,0 +957,3 @@\n+  _igvn_worklist = new (comp_arena()) Unique_Node_List(comp_arena());\n+  _types = new (comp_arena()) Type_Array(comp_arena());\n+  _node_hash = new (comp_arena()) NodeHash(comp_arena(), 255);\n@@ -954,4 +961,1 @@\n-    \/\/ The following is a dummy for the sake of GraphKit::gen_stub\n-    Unique_Node_List for_igvn(comp_arena());\n-    set_for_igvn(&for_igvn);  \/\/ not used, but some GraphKit guys push on this\n-    PhaseGVN gvn(Thread::current()->resource_area(),255);\n+    PhaseGVN gvn;\n@@ -1800,7 +1804,8 @@\n-\/\/---------------------cleanup_loop_predicates-----------------------\n-\/\/ Remove the opaque nodes that protect the predicates so that all unused\n-\/\/ checks and uncommon_traps will be eliminated from the ideal graph\n-void Compile::cleanup_loop_predicates(PhaseIterGVN &igvn) {\n-  if (predicate_count()==0) return;\n-  for (int i = predicate_count(); i > 0; i--) {\n-    Node * n = predicate_opaque1_node(i-1);\n+\/\/ Remove the opaque nodes that protect the Parse Predicates so that all unused\n+\/\/ checks and uncommon_traps will be eliminated from the ideal graph.\n+void Compile::cleanup_parse_predicates(PhaseIterGVN& igvn) const {\n+  if (parse_predicate_count() == 0) {\n+    return;\n+  }\n+  for (int i = parse_predicate_count(); i > 0; i--) {\n+    Node* n = parse_predicate_opaque1_node(i - 1);\n@@ -1810,1 +1815,1 @@\n-  assert(predicate_count()==0, \"should be clean!\");\n+  assert(parse_predicate_count() == 0, \"should be clean!\");\n@@ -1956,1 +1961,1 @@\n-    PhaseRemoveUseless pru(initial_gvn(), for_igvn());\n+    PhaseRemoveUseless pru(initial_gvn(), *igvn_worklist());\n@@ -1987,3 +1992,1 @@\n-    assert( igvn._worklist.size() == 0, \"should be done with igvn\" );\n-    for_igvn()->clear();\n-    gvn->replace_with(&igvn);\n+    igvn_worklist()->ensure_empty(); \/\/ should be done with igvn\n@@ -2052,1 +2055,1 @@\n-    PhaseRemoveUseless pru(initial_gvn(), for_igvn());\n+    PhaseRemoveUseless pru(initial_gvn(), *igvn_worklist());\n@@ -2056,1 +2059,1 @@\n-    igvn = PhaseIterGVN(initial_gvn());\n+    igvn.reset_from_gvn(initial_gvn());\n@@ -2099,2 +2102,1 @@\n-    for_igvn()->clear();\n-    initial_gvn()->replace_with(&igvn);\n+    igvn_worklist()->ensure_empty(); \/\/ should be done with igvn\n@@ -2117,1 +2119,2 @@\n-  assert( igvn._worklist.size() == 0, \"should be done with igvn\" );\n+\n+  igvn_worklist()->ensure_empty(); \/\/ should be done with igvn\n@@ -2121,2 +2124,0 @@\n-    for_igvn()->clear();\n-    initial_gvn()->replace_with(&igvn);\n@@ -2144,2 +2145,1 @@\n-    for_igvn()->clear();\n-    initial_gvn()->replace_with(&igvn);\n+    igvn_worklist()->ensure_empty(); \/\/ should be done with igvn\n@@ -2278,4 +2278,1 @@\n-    initial_gvn()->replace_with(&igvn);\n-    Unique_Node_List* old_worklist = for_igvn();\n-    old_worklist->clear();\n-    Unique_Node_List new_worklist(C->comp_arena());\n+    igvn_worklist()->ensure_empty(); \/\/ should be done with igvn\n@@ -2284,1 +2281,1 @@\n-      PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &new_worklist);\n+      PhaseRenumberLive prl(initial_gvn(), *igvn_worklist());\n@@ -2286,3 +2283,1 @@\n-    Unique_Node_List* save_for_igvn = for_igvn();\n-    set_for_igvn(&new_worklist);\n-    igvn = PhaseIterGVN(initial_gvn());\n+    igvn.reset_from_gvn(initial_gvn());\n@@ -2290,1 +2285,0 @@\n-    set_for_igvn(old_worklist); \/\/ new_worklist is dead beyond this point\n@@ -2387,1 +2381,1 @@\n-    igvn = ccp;\n+    igvn.reset_from_igvn(&ccp);\n@@ -2451,0 +2445,4 @@\n+ \/\/ We will never use the NodeHash table any more. Clear it so that final_graph_reshaping does not have\n+ \/\/ to remove hashes to unlock nodes for modifications.\n+ C->node_hash()->clear();\n+\n@@ -2851,6 +2849,1 @@\n-#ifdef ASSERT\n-      if (TraceNewVectors) {\n-        tty->print(\"new Vector node: \");\n-        macro_logic->dump();\n-      }\n-#endif\n+      VectorNode::trace_new_vector(macro_logic, \"MacroLogic\");\n","filename":"src\/hotspot\/share\/opto\/compile.cpp","additions":50,"deletions":57,"binary":false,"changes":107,"status":"modified"},{"patch":"@@ -4240,1 +4240,1 @@\n-                     vtableEntry::method_offset_in_bytes();\n+                     in_bytes(vtableEntry::method_offset());\n","filename":"src\/hotspot\/share\/opto\/library_call.cpp","additions":1,"deletions":1,"binary":false,"changes":2,"status":"modified"},{"patch":"@@ -555,1 +555,1 @@\n-Node* LoadNode::find_previous_arraycopy(PhaseTransform* phase, Node* ld_alloc, Node*& mem, bool can_see_stored_value) const {\n+Node* LoadNode::find_previous_arraycopy(PhaseValues* phase, Node* ld_alloc, Node*& mem, bool can_see_stored_value) const {\n@@ -611,1 +611,1 @@\n-ArrayCopyNode* MemNode::find_array_copy_clone(PhaseTransform* phase, Node* ld_alloc, Node* mem) const {\n+ArrayCopyNode* MemNode::find_array_copy_clone(PhaseValues* phase, Node* ld_alloc, Node* mem) const {\n@@ -655,1 +655,1 @@\n-Node* MemNode::find_previous_store(PhaseTransform* phase) {\n+Node* MemNode::find_previous_store(PhaseValues* phase) {\n@@ -1057,1 +1057,1 @@\n-Node* MemNode::can_see_stored_value(Node* st, PhaseTransform* phase) const {\n+Node* MemNode::can_see_stored_value(Node* st, PhaseValues* phase) const {\n@@ -2377,1 +2377,1 @@\n-  if (tary != nullptr && tary->elem() != Type::BOTTOM &&\n+  if (tary != nullptr &&\n@@ -2864,1 +2864,1 @@\n-bool StoreNode::value_never_loaded( PhaseTransform *phase) const {\n+bool StoreNode::value_never_loaded(PhaseValues* phase) const {\n@@ -3154,1 +3154,1 @@\n-bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseTransform* phase) {\n+bool ClearArrayNode::step_through(Node** np, uint instance_id, PhaseValues* phase) {\n@@ -3706,1 +3706,1 @@\n-intptr_t InitializeNode::get_store_offset(Node* st, PhaseTransform* phase) {\n+intptr_t InitializeNode::get_store_offset(Node* st, PhaseValues* phase) {\n@@ -3900,1 +3900,1 @@\n-                                                   PhaseTransform* phase) {\n+                                                   PhaseValues* phase) {\n@@ -3954,1 +3954,1 @@\n-                                          PhaseTransform* phase) {\n+                                          PhaseValues* phase) {\n@@ -3970,1 +3970,1 @@\n-                                       PhaseTransform* phase) {\n+                                       PhaseGVN* phase) {\n@@ -4514,1 +4514,1 @@\n-bool InitializeNode::stores_are_sane(PhaseTransform* phase) {\n+bool InitializeNode::stores_are_sane(PhaseValues* phase) {\n","filename":"src\/hotspot\/share\/opto\/memnode.cpp","additions":12,"deletions":12,"binary":false,"changes":24,"status":"modified"},{"patch":"@@ -492,1 +492,1 @@\n-  fields[TypeFunc::Parms+0] = NULL; \/\/ void\n+  fields[TypeFunc::Parms+0] = nullptr; \/\/ void\n@@ -1566,4 +1566,0 @@\n-\n-  \/\/ Enable WXWrite: the function called directly by compiled code.\n-  MACOS_AARCH64_ONLY(ThreadWXEnable wx(WXWrite, thread));\n-\n","filename":"src\/hotspot\/share\/opto\/runtime.cpp","additions":1,"deletions":5,"binary":false,"changes":6,"status":"modified"},{"patch":"@@ -416,1 +416,1 @@\n-  return t->hash();\n+  return (int)t->hash();\n@@ -773,1 +773,1 @@\n-int Type::hash(void) const {\n+uint Type::hash(void) const {\n@@ -1363,2 +1363,2 @@\n-int TypeF::hash(void) const {\n-  return *(int*)(&_f);\n+uint TypeF::hash(void) const {\n+  return *(uint*)(&_f);\n@@ -1473,2 +1473,2 @@\n-int TypeD::hash(void) const {\n-  return *(int*)(&_d);\n+uint TypeD::hash(void) const {\n+  return *(uint*)(&_d);\n@@ -1690,1 +1690,1 @@\n-            (juint)(_lo - min) >= (juint)(max - _hi)) {\n+            ((juint)_lo - min) >= ((juint)max - _hi)) {\n@@ -1767,2 +1767,2 @@\n-int TypeInt::hash(void) const {\n-  return java_add(java_add(_lo, _hi), java_add((jint)_widen, (jint)Type::Int));\n+uint TypeInt::hash(void) const {\n+  return (uint)_lo + (uint)_hi + (uint)_widen + (uint)Type::Int;\n@@ -2000,2 +2000,2 @@\n-  julong nrange = _hi - _lo;\n-  julong orange = ohi - olo;\n+  julong nrange = (julong)_hi - _lo;\n+  julong orange = (julong)ohi - olo;\n@@ -2033,2 +2033,2 @@\n-int TypeLong::hash(void) const {\n-  return (int)(_lo+_hi+_widen+(int)Type::Long);\n+uint TypeLong::hash(void) const {\n+  return (uint)_lo + (uint)_hi + (uint)_widen + (uint)Type::Long;\n@@ -2275,2 +2275,2 @@\n-int TypeTuple::hash(void) const {\n-  intptr_t sum = _cnt;\n+uint TypeTuple::hash(void) const {\n+  uintptr_t sum = _cnt;\n@@ -2278,2 +2278,2 @@\n-    sum += (intptr_t)_fields[i];     \/\/ Hash on pointers directly\n-  return sum;\n+    sum += (uintptr_t)_fields[i];     \/\/ Hash on pointers directly\n+  return (uint)sum;\n@@ -2390,2 +2390,2 @@\n-int TypeAry::hash(void) const {\n-  return (intptr_t)_elem + (intptr_t)_size + (_stable ? 43 : 0);\n+uint TypeAry::hash(void) const {\n+  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_size + (uint)(_stable ? 43 : 0);\n@@ -2578,2 +2578,2 @@\n-int TypeVect::hash(void) const {\n-  return (intptr_t)_elem + (intptr_t)_length;\n+uint TypeVect::hash(void) const {\n+  return (uint)(uintptr_t)_elem + (uint)(uintptr_t)_length;\n@@ -2804,3 +2804,2 @@\n-int TypePtr::hash(void) const {\n-  return java_add(java_add((jint)_ptr, (jint)_offset), java_add((jint)hash_speculative(), (jint)_inline_depth));\n-;\n+uint TypePtr::hash(void) const {\n+  return (uint)_ptr + (uint)_offset + (uint)hash_speculative() + (uint)_inline_depth;\n@@ -3238,2 +3237,2 @@\n-int TypeRawPtr::hash(void) const {\n-  return (intptr_t)_bits + TypePtr::hash();\n+uint TypeRawPtr::hash(void) const {\n+  return (uint)(uintptr_t)_bits + (uint)TypePtr::hash();\n@@ -3258,1 +3257,2 @@\n-          _hash_computed(0), _exact_klass_computed(0), _is_loaded_computed(0) {\n+          _hash(0), _exact_klass(nullptr) {\n+  DEBUG_ONLY(_initialized = true);\n@@ -3263,1 +3263,1 @@\n-          _hash_computed(0), _exact_klass_computed(0), _is_loaded_computed(0) {\n+          _hash(0), _exact_klass(nullptr) {\n@@ -3267,0 +3267,1 @@\n+  initialize();\n@@ -3269,0 +3270,5 @@\n+void TypePtr::InterfaceSet::initialize() {\n+  compute_hash();\n+  compute_exact_klass();\n+  DEBUG_ONLY(_initialized = true;)\n+}\n@@ -3304,3 +3310,12 @@\n-int TypePtr::InterfaceSet::hash() const {\n-  if (_hash_computed) {\n-    return _hash;\n+bool TypePtr::InterfaceSet::eq(ciInstanceKlass* k) const {\n+  assert(k->is_loaded(), \"should be loaded\");\n+  GrowableArray<ciInstanceKlass *>* interfaces = k->as_instance_klass()->transitive_interfaces();\n+  if (_list.length() != interfaces->length()) {\n+    return false;\n+  }\n+  for (int i = 0; i < interfaces->length(); i++) {\n+    bool found = false;\n+    _list.find_sorted<ciKlass*, compare>(interfaces->at(i), found);\n+    if (!found) {\n+      return false;\n+    }\n@@ -3308,2 +3323,6 @@\n-  const_cast<InterfaceSet*>(this)->compute_hash();\n-  assert(_hash_computed, \"should be computed now\");\n+  return true;\n+}\n+\n+\n+uint TypePtr::InterfaceSet::hash() const {\n+  assert(_initialized, \"must be\");\n@@ -3314,1 +3333,1 @@\n-  int hash = 0;\n+  uint hash = 0;\n@@ -3317,1 +3336,1 @@\n-    hash += (jint)k->hash();\n+    hash += k->hash();\n@@ -3319,1 +3338,0 @@\n-  _hash_computed = 1;\n@@ -3327,1 +3345,1 @@\n-void TypePtr::InterfaceSet::dump(outputStream *st) const {\n+void TypePtr::InterfaceSet::dump(outputStream* st) const {\n@@ -3347,0 +3365,1 @@\n+#ifdef ASSERT\n@@ -3348,1 +3367,0 @@\n-#ifdef DEBUG\n@@ -3355,1 +3373,1 @@\n-#endif\n+#endif\n@@ -3383,0 +3401,2 @@\n+  result.initialize();\n+#ifdef ASSERT\n@@ -3384,2 +3404,1 @@\n-#ifdef DEBUG\n-    assert(result.contains(_list.at(i)), \"missing\");\n+    assert(result._list.contains(_list.at(i)), \"missing\");\n@@ -3389,1 +3408,1 @@\n-    assert(result.contains(other._list.at(i)), \"missing\");\n+    assert(result._list.contains(other._list.at(i)), \"missing\");\n@@ -3421,0 +3440,2 @@\n+  result.initialize();\n+#ifdef ASSERT\n@@ -3422,2 +3443,1 @@\n-#ifdef DEBUG\n-    assert(!other._list.contains(_list.at(i)) || result.contains(_list.at(i)), \"missing\");\n+    assert(!other._list.contains(_list.at(i)) || result._list.contains(_list.at(i)), \"missing\");\n@@ -3427,1 +3447,1 @@\n-    assert(!_list.contains(other._list.at(i)) || result.contains(other._list.at(i)), \"missing\");\n+    assert(!_list.contains(other._list.at(i)) || result._list.contains(other._list.at(i)), \"missing\");\n@@ -3438,5 +3458,1 @@\n-  if (_exact_klass_computed) {\n-    return _exact_klass;\n-  }\n-  const_cast<InterfaceSet*>(this)->compute_exact_klass();\n-  assert(_exact_klass_computed, \"should be computed now\");\n+  assert(_initialized, \"must be\");\n@@ -3448,1 +3464,0 @@\n-    _exact_klass_computed = 1;\n@@ -3454,2 +3469,2 @@\n-    ciKlass* interface = _list.at(i);\n-    if (eq(interfaces(interface, false, true, false, trust_interfaces))) {\n+    ciInstanceKlass* interface = _list.at(i)->as_instance_klass();\n+    if (eq(interface)) {\n@@ -3457,1 +3472,1 @@\n-      res = _list.at(i);\n+      res = interface;\n@@ -3460,1 +3475,0 @@\n-  _exact_klass_computed = 1;\n@@ -3464,11 +3478,2 @@\n-bool TypePtr::InterfaceSet::is_loaded() const {\n-  if (_is_loaded_computed) {\n-    return _is_loaded;\n-  }\n-  const_cast<InterfaceSet*>(this)->compute_is_loaded();\n-  assert(_is_loaded_computed, \"should be computed now\");\n-  return _is_loaded;\n-}\n-\n-void TypePtr::InterfaceSet::compute_is_loaded() {\n-  _is_loaded_computed = 1;\n+#ifdef ASSERT\n+void TypePtr::InterfaceSet::verify_is_loaded() const {\n@@ -3477,4 +3482,1 @@\n-    if (!interface->is_loaded()) {\n-      _is_loaded = false;\n-      return;\n-    }\n+    assert(interface->is_loaded(), \"Interface not loaded\");\n@@ -3482,1 +3484,1 @@\n-  _is_loaded = true;\n+#endif\n@@ -3496,0 +3498,5 @@\n+#ifdef ASSERT\n+  if (klass() != nullptr && klass()->is_loaded()) {\n+    interfaces.verify_is_loaded();\n+  }\n+#endif\n@@ -3841,1 +3848,1 @@\n-int TypeOopPtr::hash(void) const {\n+uint TypeOopPtr::hash(void) const {\n@@ -3843,2 +3850,3 @@\n-    java_add(java_add((jint)(const_oop() ? const_oop()->hash() : 0), (jint)_klass_is_exact),\n-             java_add((jint)_instance_id, (jint)TypePtr::hash()));\n+    (uint)(const_oop() ? const_oop()->hash() : 0) +\n+    (uint)_klass_is_exact +\n+    (uint)_instance_id + TypePtr::hash();\n@@ -3988,3 +3996,1 @@\n-    ciKlass* k = _klass;\n-    const TypePtr::InterfaceSet interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n-    if (_interfaces.eq(interfaces)) {\n+    if (_interfaces.eq(_klass->as_instance_klass())) {\n@@ -4052,1 +4058,1 @@\n-      GrowableArray<ciInstanceKlass *> *k_interfaces = k->as_instance_klass()->transitive_interfaces();\n+      GrowableArray<ciInstanceKlass *>* k_interfaces = k->as_instance_klass()->transitive_interfaces();\n@@ -4396,2 +4402,0 @@\n-  InterfaceSet subtype_interfaces;\n-\n@@ -4482,3 +4486,2 @@\n-int TypeInstPtr::hash(void) const {\n-  int hash = java_add(java_add((jint)klass()->hash(), (jint)TypeOopPtr::hash()), _interfaces.hash());\n-  return hash;\n+uint TypeInstPtr::hash(void) const {\n+  return klass()->hash() + TypeOopPtr::hash() + _interfaces.hash();\n@@ -4591,5 +4594,2 @@\n-    ciKlass* k = ik;\n-    TypePtr::InterfaceSet interfaces = TypePtr::interfaces(k, true, false, false, ignore_interfaces);\n-    assert(k == ik, \"\");\n-    if (interfaces.eq(_interfaces)) {\n-      Compile *C = Compile::current();\n+    if (_interfaces.eq(ik)) {\n+      Compile* C = Compile::current();\n@@ -4836,2 +4836,2 @@\n-int TypeAryPtr::hash(void) const {\n-  return (intptr_t)_ary + TypeOopPtr::hash();\n+uint TypeAryPtr::hash(void) const {\n+  return (uint)(uintptr_t)_ary + TypeOopPtr::hash();\n@@ -5222,1 +5222,1 @@\n-int TypeNarrowPtr::hash(void) const {\n+uint TypeNarrowPtr::hash(void) const {\n@@ -5382,1 +5382,1 @@\n-int TypeMetadataPtr::hash(void) const {\n+uint TypeMetadataPtr::hash(void) const {\n@@ -5603,2 +5603,1 @@\n-    ciKlass* k = _klass;\n-    if (_interfaces.eq(TypePtr::interfaces(k, true, false, true, ignore_interfaces))) {\n+    if (_interfaces.eq(_klass->as_instance_klass())) {\n@@ -5623,2 +5622,2 @@\n-int TypeKlassPtr::hash(void) const {\n-  return java_add((jint)TypePtr::hash(), _interfaces.hash());\n+uint TypeKlassPtr::hash(void) const {\n+  return TypePtr::hash() + _interfaces.hash();\n@@ -5736,2 +5735,2 @@\n-int TypeInstKlassPtr::hash(void) const {\n-  return java_add((jint)klass()->hash(), TypeKlassPtr::hash());\n+uint TypeInstKlassPtr::hash(void) const {\n+  return klass()->hash() + TypeKlassPtr::hash();\n@@ -5801,4 +5800,1 @@\n-        ciKlass* sub_k = sub;\n-        TypePtr::InterfaceSet sub_interfaces = TypePtr::interfaces(sub_k, true, false, false, ignore_interfaces);\n-        assert(sub_k == sub, \"\");\n-        if (sub_interfaces.eq(_interfaces)) {\n+        if (_interfaces.eq(sub)) {\n@@ -6051,4 +6047,1 @@\n-        ciKlass *sub_k = sub;\n-        TypePtr::InterfaceSet sub_interfaces = TypePtr::interfaces(sub_k, true, false, false, ignore_interfaces);\n-        assert(sub_k == sub, \"\");\n-        if (sub_interfaces.eq(_interfaces)) {\n+        if (_interfaces.eq(sub)) {\n@@ -6102,2 +6095,2 @@\n-int TypeAryKlassPtr::hash(void) const {\n-  return (intptr_t)_elem + TypeKlassPtr::hash();\n+uint TypeAryKlassPtr::hash(void) const {\n+  return (uint)(uintptr_t)_elem + TypeKlassPtr::hash();\n@@ -6650,2 +6643,2 @@\n-int TypeFunc::hash(void) const {\n-  return (intptr_t)_domain + (intptr_t)_range;\n+uint TypeFunc::hash(void) const {\n+  return (uint)(uintptr_t)_domain + (uint)(uintptr_t)_range;\n","filename":"src\/hotspot\/share\/opto\/type.cpp","additions":103,"deletions":110,"binary":false,"changes":213,"status":"modified"},{"patch":"@@ -786,12 +786,14 @@\n-    ThreadsListHandle tlh;\n-    JavaThread* thr = nullptr;\n-    oop java_thread = nullptr;\n-    (void) tlh.cv_internal_thread_to_JavaThread(jthread, &thr, &java_thread);\n-    if (java_thread != nullptr) {\n-      \/\/ This is a valid oop.\n-      if (thr != nullptr) {\n-        \/\/ The JavaThread is alive.\n-        Parker* p = thr->parker();\n-        HOTSPOT_THREAD_UNPARK((uintptr_t) p);\n-        p->unpark();\n-      }\n+    oop thread_oop = JNIHandles::resolve_non_null(jthread);\n+    \/\/ Get the JavaThread* stored in the java.lang.Thread object _before_\n+    \/\/ the embedded ThreadsListHandle is constructed so we know if the\n+    \/\/ early life stage of the JavaThread* is protected. We use acquire\n+    \/\/ here to ensure that if we see a non-nullptr value, then we also\n+    \/\/ see the main ThreadsList updates from the JavaThread* being added.\n+    FastThreadsListHandle ftlh(thread_oop, java_lang_Thread::thread_acquire(thread_oop));\n+    JavaThread* thr = ftlh.protected_java_thread();\n+    if (thr != nullptr) {\n+      \/\/ The still live JavaThread* is protected by the FastThreadsListHandle\n+      \/\/ so it is safe to access.\n+      Parker* p = thr->parker();\n+      HOTSPOT_THREAD_UNPARK((uintptr_t) p);\n+      p->unpark();\n@@ -799,2 +801,1 @@\n-  } \/\/ ThreadsListHandle is destroyed here.\n-\n+  } \/\/ FastThreadsListHandle is destroyed here.\n","filename":"src\/hotspot\/share\/prims\/unsafe.cpp","additions":15,"deletions":14,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -1485,3 +1485,0 @@\n-\n-\/\/ NOTE: set_use_compressed_klass_ptrs() must be called after calling\n-\/\/ set_use_compressed_oops().\n@@ -1507,19 +1504,0 @@\n-  \/\/ On some architectures, the use of UseCompressedClassPointers implies the use of\n-  \/\/ UseCompressedOops. The reason is that the rheap_base register of said platforms\n-  \/\/ is reused to perform some optimized spilling, in order to use rheap_base as a\n-  \/\/ temp register. But by treating it as any other temp register, spilling can typically\n-  \/\/ be completely avoided instead. So it is better not to perform this trick. And by\n-  \/\/ not having that reliance, large heaps, or heaps not supporting compressed oops,\n-  \/\/ can still use compressed class pointers.\n-  \/\/ Turn on UseCompressedClassPointers too\n-  if (FLAG_IS_DEFAULT(UseCompressedClassPointers)) {\n-    FLAG_SET_ERGO(UseCompressedClassPointers, true);\n-  }\n-  \/\/ Check the CompressedClassSpaceSize to make sure we use compressed klass ptrs.\n-  if (UseCompressedClassPointers) {\n-    if (CompressedClassSpaceSize > KlassEncodingMetaspaceMax) {\n-      warning(\"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n-      FLAG_SET_DEFAULT(UseCompressedClassPointers, false);\n-    }\n-  }\n-\n@@ -1534,1 +1512,4 @@\n-#endif\n+\n+  assert(!UseCompressedClassPointers || CompressedClassSpaceSize <= KlassEncodingMetaspaceMax,\n+         \"CompressedClassSpaceSize is too large for UseCompressedClassPointers\");\n+#endif \/\/ _LP64\n@@ -1555,3 +1536,0 @@\n-\n-  \/\/ set_use_compressed_klass_ptrs() must be called after calling\n-  \/\/ set_use_compressed_oops().\n@@ -1956,1 +1934,1 @@\n-#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM)\n+#if !defined(X86) && !defined(AARCH64) && !defined(RISCV64) && !defined(ARM) && !defined(PPC64)\n@@ -1972,1 +1950,1 @@\n-#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64)\n+#if !defined(X86) && !defined(AARCH64) && !defined(PPC64) && !defined(RISCV64) && !defined(S390)\n@@ -2878,1 +2856,1 @@\n-    } else if (match_option(option, \"-XX:-EnableJVMCIProduct\")) {\n+    } else if (match_option(option, \"-XX:-EnableJVMCIProduct\") || match_option(option, \"-XX:-UseGraalJIT\")) {\n@@ -2881,1 +2859,1 @@\n-                  \"-XX:-EnableJVMCIProduct cannot come after -XX:+EnableJVMCIProduct\\n\");\n+                  \"-XX:-EnableJVMCIProduct or -XX:-UseGraalJIT cannot come after -XX:+EnableJVMCIProduct or -XX:+UseGraalJIT\\n\");\n@@ -2884,2 +2862,16 @@\n-    } else if (match_option(option, \"-XX:+EnableJVMCIProduct\")) {\n-      \/\/ Just continue, since \"-XX:+EnableJVMCIProduct\" has been specified before\n+    } else if (match_option(option, \"-XX:+EnableJVMCIProduct\") || match_option(option, \"-XX:+UseGraalJIT\")) {\n+      bool use_graal_jit = match_option(option, \"-XX:+UseGraalJIT\");\n+      if (use_graal_jit) {\n+        const char* jvmci_compiler = get_property(\"jvmci.Compiler\");\n+        if (jvmci_compiler != nullptr) {\n+          if (strncmp(jvmci_compiler, \"graal\", strlen(\"graal\")) != 0) {\n+            jio_fprintf(defaultStream::error_stream(),\n+              \"Value of jvmci.Compiler incompatible with +UseGraalJIT: %s\", jvmci_compiler);\n+            return JNI_ERR;\n+          }\n+        } else if (!add_property(\"jvmci.Compiler=graal\")) {\n+            return JNI_ENOMEM;\n+        }\n+      }\n+\n+      \/\/ Just continue, since \"-XX:+EnableJVMCIProduct\" or \"-XX:+UseGraalJIT\" has been specified before\n@@ -2892,1 +2884,1 @@\n-        if (!JVMCIGlobals::enable_jvmci_product_mode(origin)) {\n+        if (!JVMCIGlobals::enable_jvmci_product_mode(origin, use_graal_jit)) {\n@@ -2899,1 +2891,1 @@\n-      else if (!process_argument(\"EnableJVMCIProduct\", args->ignoreUnrecognized, origin)) {\n+      else if (!process_argument(use_graal_jit ? \"UseGraalJIT\" : \"EnableJVMCIProduct\", args->ignoreUnrecognized, origin)) {\n","filename":"src\/hotspot\/share\/runtime\/arguments.cpp","additions":26,"deletions":34,"binary":false,"changes":60,"status":"modified"},{"patch":"@@ -554,1 +554,1 @@\n-  product(ccstr, HeapDumpPath, nullptr, MANAGEABLE,                            \\\n+  product(ccstr, HeapDumpPath, nullptr, MANAGEABLE,                         \\\n@@ -608,1 +608,1 @@\n-  product(ccstr, PrintAssemblyOptions, nullptr, DIAGNOSTIC,                    \\\n+  product(ccstr, PrintAssemblyOptions, nullptr, DIAGNOSTIC,                 \\\n@@ -636,1 +636,1 @@\n-          \"Start debugger when an implicit OS (e.g. nullptr) \"                 \\\n+          \"Start debugger when an implicit OS (e.g. null pointer) \"         \\\n@@ -842,1 +842,1 @@\n-  product(ccstr, TraceJVMTI, nullptr,                                          \\\n+  product(ccstr, TraceJVMTI, nullptr,                                       \\\n@@ -894,4 +894,0 @@\n-                                                                            \\\n-  notproduct(bool, CheckMemoryInitialization, false,                        \\\n-          \"Check memory initialization\")                                    \\\n-                                                                            \\\n@@ -960,3 +956,0 @@\n-  product(bool, EnableThreadSMRExtraValidityChecks, true, DIAGNOSTIC,       \\\n-             \"Enable Thread SMR extra validity checks\")                     \\\n-                                                                            \\\n@@ -1039,1 +1032,1 @@\n-  product(ccstr, LogFile, nullptr, DIAGNOSTIC,                                 \\\n+  product(ccstr, LogFile, nullptr, DIAGNOSTIC,                              \\\n@@ -1043,1 +1036,1 @@\n-  product(ccstr, ErrorFile, nullptr,                                           \\\n+  product(ccstr, ErrorFile, nullptr,                                        \\\n@@ -1080,1 +1073,1 @@\n-  product(ccstr, AbortVMOnException, nullptr, DIAGNOSTIC,                      \\\n+  product(ccstr, AbortVMOnException, nullptr, DIAGNOSTIC,                   \\\n@@ -1084,1 +1077,1 @@\n-  product(ccstr, AbortVMOnExceptionMessage, nullptr, DIAGNOSTIC,               \\\n+  product(ccstr, AbortVMOnExceptionMessage, nullptr, DIAGNOSTIC,            \\\n@@ -1288,1 +1281,1 @@\n-  product(intx, MaxJavaStackTraceDepth, 1024,                               \\\n+  product(int, MaxJavaStackTraceDepth, 1024,                                \\\n@@ -1396,1 +1389,1 @@\n-  product(double, MinInlineFrequencyRatio, 0.0085, DIAGNOSTIC,               \\\n+  product(double, MinInlineFrequencyRatio, 0.0085, DIAGNOSTIC,              \\\n@@ -1741,1 +1734,1 @@\n-  product(ccstr, PerfDataSaveFile, nullptr,                                    \\\n+  product(ccstr, PerfDataSaveFile, nullptr,                                 \\\n@@ -1928,1 +1921,1 @@\n-  product(ccstr, AllocateHeapAt, nullptr,                                      \\\n+  product(ccstr, AllocateHeapAt, nullptr,                                   \\\n@@ -1963,1 +1956,1 @@\n-  JFR_ONLY(product(ccstr, FlightRecorderOptions, nullptr,                      \\\n+  JFR_ONLY(product(ccstr, FlightRecorderOptions, nullptr,                   \\\n@@ -1966,1 +1959,1 @@\n-  JFR_ONLY(product(ccstr, StartFlightRecording, nullptr,                       \\\n+  JFR_ONLY(product(ccstr, StartFlightRecording, nullptr,                    \\\n","filename":"src\/hotspot\/share\/runtime\/globals.hpp","additions":14,"deletions":21,"binary":false,"changes":35,"status":"modified"},{"patch":"@@ -220,8 +220,6 @@\n-  \/\/ TODO-FIXME: the \"offset\" routines should return a type of off_t instead of int ...\n-  \/\/ ByteSize would also be an appropriate type.\n-  static int header_offset_in_bytes()      { return offset_of(ObjectMonitor, _header); }\n-  static int owner_offset_in_bytes()       { return offset_of(ObjectMonitor, _owner); }\n-  static int recursions_offset_in_bytes()  { return offset_of(ObjectMonitor, _recursions); }\n-  static int cxq_offset_in_bytes()         { return offset_of(ObjectMonitor, _cxq); }\n-  static int succ_offset_in_bytes()        { return offset_of(ObjectMonitor, _succ); }\n-  static int EntryList_offset_in_bytes()   { return offset_of(ObjectMonitor, _EntryList); }\n+  static ByteSize header_offset()      { return byte_offset_of(ObjectMonitor, _header); }\n+  static ByteSize owner_offset()       { return byte_offset_of(ObjectMonitor, _owner); }\n+  static ByteSize recursions_offset()  { return byte_offset_of(ObjectMonitor, _recursions); }\n+  static ByteSize cxq_offset()         { return byte_offset_of(ObjectMonitor, _cxq); }\n+  static ByteSize succ_offset()        { return byte_offset_of(ObjectMonitor, _succ); }\n+  static ByteSize EntryList_offset()   { return byte_offset_of(ObjectMonitor, _EntryList); }\n@@ -241,1 +239,1 @@\n-    ((ObjectMonitor::f ## _offset_in_bytes()) - markWord::monitor_value)\n+    ((in_bytes(ObjectMonitor::f ## _offset())) - markWord::monitor_value)\n@@ -369,0 +367,1 @@\n+  void      release_object() { _object.release(_oop_storage); _object.set_null(); }\n","filename":"src\/hotspot\/share\/runtime\/objectMonitor.hpp","additions":8,"deletions":9,"binary":false,"changes":17,"status":"modified"},{"patch":"@@ -166,1 +166,1 @@\n-#if defined(_ALLBSD_SOURCE) || defined(_GNU_SOURCE)\n+#if (defined(_ALLBSD_SOURCE) || defined(_GNU_SOURCE)) && !defined(AIX)\n@@ -882,2 +882,2 @@\n-    if (have_function_name = is_function_descriptor =\n-        dll_address_to_function_name(addr2, p, buflen, &offset, demangle)) {\n+    if ((have_function_name = is_function_descriptor =\n+        dll_address_to_function_name(addr2, p, buflen, &offset, demangle))) {\n@@ -1013,0 +1013,5 @@\n+void os::print_register_info(outputStream* st, const void* context) {\n+  int continuation = 0;\n+  print_register_info(st, context, continuation);\n+}\n+\n@@ -1127,1 +1132,1 @@\n-    st->print_cr(\"0x0 is nullptr\");\n+    st->print_cr(\"0x0 is null\");\n@@ -1382,0 +1387,16 @@\n+bool os::write(int fd, const void *buf, size_t nBytes) {\n+  ssize_t res;\n+\n+  while (nBytes > 0) {\n+    res = pd_write(fd, buf, nBytes);\n+    if (res == OS_ERR) {\n+      return false;\n+    }\n+    buf = (void *)((char *)buf + nBytes);\n+    nBytes -= res;\n+  }\n+\n+  return true;\n+}\n+\n+\n","filename":"src\/hotspot\/share\/runtime\/os.cpp","additions":25,"deletions":4,"binary":false,"changes":29,"status":"modified"},{"patch":"@@ -490,1 +490,1 @@\n-#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64)\n+#if defined(X86) || defined(AARCH64) || defined(PPC64) || defined(RISCV64) || defined(S390)\n@@ -1649,0 +1649,9 @@\n+static size_t delete_monitors(GrowableArray<ObjectMonitor*>* delete_list) {\n+  size_t count = 0;\n+  for (ObjectMonitor* monitor: *delete_list) {\n+    delete monitor;\n+    count++;\n+  }\n+  return count;\n+}\n+\n@@ -1726,9 +1735,9 @@\n-    \/\/ deflated in this cycle.\n-    for (ObjectMonitor* monitor: delete_list) {\n-      delete monitor;\n-      deleted_count++;\n-\n-      if (current->is_Java_thread()) {\n-        \/\/ A JavaThread must check for a safepoint\/handshake and honor it.\n-        chk_for_block_req(JavaThread::cast(current), \"deletion\", \"deleted_count\",\n-                          deleted_count, ls, &timer);\n+    \/\/ deflated and unlinked in this cycle.\n+    if (current->is_Java_thread()) {\n+      if (ls != NULL) {\n+        timer.stop();\n+        ls->print_cr(\"before setting blocked: unlinked_count=\" SIZE_FORMAT\n+                     \", in_use_list stats: ceiling=\" SIZE_FORMAT \", count=\"\n+                     SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     unlinked_count, in_use_list_ceiling(),\n+                     _in_use_list.count(), _in_use_list.max());\n@@ -1736,0 +1745,14 @@\n+      \/\/ Mark the calling JavaThread blocked (safepoint safe) while we free\n+      \/\/ the ObjectMonitors so we don't delay safepoints whilst doing that.\n+      ThreadBlockInVM tbivm(JavaThread::cast(current));\n+      if (ls != NULL) {\n+        ls->print_cr(\"after setting blocked: in_use_list stats: ceiling=\"\n+                     SIZE_FORMAT \", count=\" SIZE_FORMAT \", max=\" SIZE_FORMAT,\n+                     in_use_list_ceiling(), _in_use_list.count(), _in_use_list.max());\n+        timer.start();\n+      }\n+      deleted_count = delete_monitors(&delete_list);\n+      \/\/ ThreadBlockInVM is destroyed here\n+    } else {\n+      \/\/ A non-JavaThread can just free the ObjectMonitors:\n+      deleted_count = delete_monitors(&delete_list);\n","filename":"src\/hotspot\/share\/runtime\/synchronizer.cpp","additions":33,"deletions":10,"binary":false,"changes":43,"status":"modified"},{"patch":"@@ -63,4 +63,13 @@\n-  template(ZMarkStart)                            \\\n-  template(ZMarkEnd)                              \\\n-  template(ZRelocateStart)                        \\\n-  template(ZVerify)                               \\\n+  template(ZMarkEndOld)                           \\\n+  template(ZMarkEndYoung)                         \\\n+  template(ZMarkFlushOperation)                   \\\n+  template(ZMarkStartYoung)                       \\\n+  template(ZMarkStartYoungAndOld)                 \\\n+  template(ZRelocateStartOld)                     \\\n+  template(ZRelocateStartYoung)                   \\\n+  template(ZRendezvousGCThreads)                  \\\n+  template(ZVerifyOld)                            \\\n+  template(XMarkStart)                            \\\n+  template(XMarkEnd)                              \\\n+  template(XRelocateStart)                        \\\n+  template(XVerify)                               \\\n@@ -169,0 +178,2 @@\n+  \/\/ Extra information about what triggered this operation.\n+  virtual const char* cause() const { return nullptr; }\n","filename":"src\/hotspot\/share\/runtime\/vmOperation.hpp","additions":15,"deletions":4,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -86,0 +86,1 @@\n+#include \"oops\/resolvedIndyEntry.hpp\"\n@@ -559,0 +560,1 @@\n+     static_field(StubRoutines,                _fmod,                                         address)                               \\\n@@ -1692,1 +1694,0 @@\n-  declare_c2_type(NodeHash, StackObj)                                     \\\n@@ -1771,2 +1772,0 @@\n-  declare_c2_type(CMoveVFNode, VectorNode)                                \\\n-  declare_c2_type(CMoveVDNode, VectorNode)                                \\\n","filename":"src\/hotspot\/share\/runtime\/vmStructs.cpp","additions":2,"deletions":3,"binary":false,"changes":5,"status":"modified"},{"patch":"@@ -1169,2 +1169,2 @@\n-inline int extract_low_short_from_int(jint x) {\n-  return x & 0xffff;\n+inline u2 extract_low_short_from_int(u4 x) {\n+  return u2(x & 0xffff);\n@@ -1173,2 +1173,2 @@\n-inline int extract_high_short_from_int(jint x) {\n-  return (x >> 16) & 0xffff;\n+inline u2 extract_high_short_from_int(u4 x) {\n+  return u2((x >> 16) & 0xffff);\n@@ -1177,1 +1177,1 @@\n-inline int build_int_from_shorts( jushort low, jushort high ) {\n+inline int build_int_from_shorts( u2 low, u2 high ) {\n@@ -1218,0 +1218,3 @@\n+inline jint  java_negate(jint  v) { return java_subtract((jint) 0, v); }\n+inline jlong java_negate(jlong v) { return java_subtract((jlong)0, v); }\n+\n@@ -1307,1 +1310,1 @@\n-  const uint64_t z2 = x2 * y2;\n+  const uint64_t z2 = (uint64_t)x2 * y2;\n","filename":"src\/hotspot\/share\/utilities\/globalDefinitions.hpp","additions":9,"deletions":6,"binary":false,"changes":15,"status":"modified"},{"patch":"@@ -55,0 +55,1 @@\n+#include \"runtime\/stackOverflow.hpp\"\n@@ -65,1 +66,1 @@\n-#include \"utilities\/vmError.hpp\"\n+#include \"utilities\/globalDefinitions.hpp\"\n@@ -67,0 +68,2 @@\n+#include \"utilities\/ostream.hpp\"\n+#include \"utilities\/vmError.hpp\"\n@@ -86,0 +89,1 @@\n+volatile bool     VMError::_step_did_succeed = false;\n@@ -98,0 +102,1 @@\n+const size_t      VMError::_reattempt_required_stack_headroom = 64 * K;\n@@ -172,0 +177,66 @@\n+static bool stack_has_headroom(size_t headroom) {\n+  const size_t stack_size = os::current_stack_size();\n+  const size_t guard_size = StackOverflow::stack_guard_zone_size();\n+  const size_t unguarded_stack_size = stack_size - guard_size;\n+\n+  if (unguarded_stack_size < headroom) {\n+    return false;\n+  }\n+\n+  const address stack_base          = os::current_stack_base();\n+  const address unguarded_stack_end = stack_base - unguarded_stack_size;\n+  const address stack_pointer       = os::current_stack_pointer();\n+\n+  return stack_pointer >= unguarded_stack_end + headroom;\n+}\n+\n+#ifdef ASSERT\n+PRAGMA_DIAG_PUSH\n+PRAGMA_INFINITE_RECURSION_IGNORED\n+void VMError::reattempt_test_hit_stack_limit(outputStream* st) {\n+  if (stack_has_headroom(_reattempt_required_stack_headroom)) {\n+    \/\/ Use all but (_reattempt_required_stack_headroom - K) unguarded stack space.\n+    const size_t stack_size     = os::current_stack_size();\n+    const size_t guard_size     = StackOverflow::stack_guard_zone_size();\n+    const address stack_base    = os::current_stack_base();\n+    const address stack_pointer = os::current_stack_pointer();\n+\n+    const size_t unguarded_stack_size = stack_size - guard_size;\n+    const address unguarded_stack_end = stack_base - unguarded_stack_size;\n+    const size_t available_headroom   = stack_pointer - unguarded_stack_end;\n+    const size_t allocation_size      = available_headroom - _reattempt_required_stack_headroom + K;\n+\n+    st->print_cr(\"Current Stack Pointer: \" PTR_FORMAT \" alloca \" SIZE_FORMAT\n+                 \" of \" SIZE_FORMAT \" bytes available unguarded stack space\",\n+                 p2i(stack_pointer), allocation_size, available_headroom);\n+\n+    \/\/ Allocate byte blob on the stack. Make pointer volatile to avoid having\n+    \/\/ the compiler removing later reads.\n+    volatile char* stack_buffer = static_cast<char*>(alloca(allocation_size));\n+    \/\/ Initialize the last byte.\n+    stack_buffer[allocation_size - 1] = '\\0';\n+    \/\/ Recursive call should hit the stack limit.\n+    reattempt_test_hit_stack_limit(st);\n+    \/\/ Perform a volatile read of the last byte to avoid having the complier\n+    \/\/ remove the allocation.\n+    static_cast<void>(stack_buffer[allocation_size - 1] == '\\0');\n+  }\n+  controlled_crash(14);\n+}\n+PRAGMA_DIAG_POP\n+#endif \/\/ ASSERT\n+\n+bool VMError::can_reattempt_step(const char* &stop_reason) {\n+  if (!stack_has_headroom(_reattempt_required_stack_headroom)) {\n+    stop_reason = \"Stack headroom limit reached\";\n+    return false;\n+  }\n+\n+  if (_step_did_timeout) {\n+    stop_reason = \"Step time limit reached\";\n+    return false;\n+  }\n+\n+  return true;\n+}\n+\n@@ -320,0 +391,21 @@\n+\/\/ Like above, but only try to figure out a short name. Return nullptr if not found.\n+static const char* find_code_name(address pc) {\n+  if (Interpreter::contains(pc)) {\n+    InterpreterCodelet* codelet = Interpreter::codelet_containing(pc);\n+    if (codelet != nullptr) {\n+      return codelet->description();\n+    }\n+  } else {\n+    StubCodeDesc* desc = StubCodeDesc::desc_for(pc);\n+    if (desc != nullptr) {\n+      return desc->name();\n+    } else {\n+      CodeBlob* cb = CodeCache::find_blob(pc);\n+      if (cb != nullptr) {\n+        return cb->name();\n+      }\n+    }\n+  }\n+  return nullptr;\n+}\n+\n@@ -428,0 +520,26 @@\n+static void print_stack_location(outputStream* st, void* context, int& continuation) {\n+  const int number_of_stack_slots = 8;\n+\n+  int i = continuation;\n+  \/\/ Update continuation with next index before fetching frame\n+  continuation = i + 1;\n+  const frame fr = os::fetch_frame_from_context(context);\n+  while (i < number_of_stack_slots) {\n+    \/\/ Update continuation with next index before printing location\n+    continuation = i + 1;\n+    \/\/ decode stack contents if possible\n+    const intptr_t *sp = fr.sp();\n+    const intptr_t *slot = sp + i;\n+    if (!is_aligned(slot, sizeof(intptr_t))) {\n+      st->print_cr(\"Misaligned sp: \" PTR_FORMAT, p2i(sp));\n+      break;\n+    } else if (os::is_readable_pointer(slot)) {\n+      st->print(\"stack at sp + %d slots: \", i);\n+      os::print_location(st, *(slot));\n+    } else {\n+      st->print_cr(\"unreadable stack slot at sp + %d\", i);\n+    }\n+    ++i;\n+  }\n+}\n+\n@@ -532,1 +650,3 @@\n-\n+  \/\/ Used by reattempt step logic\n+  static int continuation = 0;\n+  const char* stop_reattempt_reason = nullptr;\n@@ -535,0 +655,1 @@\n+    _step_did_succeed = false;                             \\\n@@ -539,1 +660,1 @@\n-# define STEP_IF(s,cond)                                   \\\n+# define STEP_IF(s, cond)                                  \\\n@@ -541,0 +662,1 @@\n+    _step_did_succeed = true;                              \\\n@@ -543,0 +665,1 @@\n+    _step_did_succeed = false;                             \\\n@@ -545,3 +668,3 @@\n-    record_step_start_time();                              \\\n-    _step_did_timeout = false;                             \\\n-    if ((cond)) {\n+    if ((cond)) {                                          \\\n+      record_step_start_time();                            \\\n+      _step_did_timeout = false;\n@@ -552,0 +675,16 @@\n+# define REATTEMPT_STEP_IF(s, cond)                        \\\n+    }                                                      \\\n+    _step_did_succeed = true;                              \\\n+  }                                                        \\\n+  if (_current_step < __LINE__ && !_step_did_succeed) {    \\\n+    _current_step = __LINE__;                              \\\n+    _current_step_info = s;                                \\\n+    const bool cond_value = (cond);                        \\\n+    if (cond_value && !can_reattempt_step(                 \\\n+                          stop_reattempt_reason)) {        \\\n+      st->print_cr(\"[stop reattempt (%s) reason: %s]\",     \\\n+                   _current_step_info,                     \\\n+                   stop_reattempt_reason);                 \\\n+    } else if (cond_value) {\n+      \/\/ [Continue Step logic]\n+\n@@ -554,0 +693,1 @@\n+    _step_did_succeed = true;                              \\\n@@ -560,1 +700,3 @@\n-  static bool print_native_stack_succeeded = false;\n+  \/\/ Native stack trace may get stuck. We try to handle the last pc if it\n+  \/\/ belongs to VM generated code.\n+  address lastpc = nullptr;\n@@ -577,0 +719,1 @@\n+#define TEST_REATTEMPT_SECONDARY_CRASH 15\n@@ -591,0 +734,33 @@\n+  \/\/ See corresponding test in test\/runtime\/ErrorHandling\/ReattemptErrorTest.java\n+  STEP_IF(\"test reattempt secondary crash\",\n+      _verbose && TestCrashInErrorHandler == TEST_REATTEMPT_SECONDARY_CRASH)\n+    st->print_cr(\"Will crash now (TestCrashInErrorHandler=%u)...\",\n+      TestCrashInErrorHandler);\n+    controlled_crash(14);\n+\n+  REATTEMPT_STEP_IF(\"test reattempt secondary crash, attempt 2\",\n+      _verbose && TestCrashInErrorHandler == TEST_REATTEMPT_SECONDARY_CRASH)\n+    st->print_cr(\"test reattempt secondary crash. attempt 2\");\n+\n+  REATTEMPT_STEP_IF(\"test reattempt secondary crash, attempt 3\",\n+      _verbose && TestCrashInErrorHandler == TEST_REATTEMPT_SECONDARY_CRASH)\n+    st->print_cr(\"test reattempt secondary crash. attempt 3\");\n+\n+  STEP_IF(\"test reattempt timeout\",\n+      _verbose && TestCrashInErrorHandler == TEST_REATTEMPT_SECONDARY_CRASH)\n+    st->print_cr(\"test reattempt timeout\");\n+    os::infinite_sleep();\n+\n+  REATTEMPT_STEP_IF(\"test reattempt timeout, attempt 2\",\n+      _verbose && TestCrashInErrorHandler == TEST_REATTEMPT_SECONDARY_CRASH)\n+    st->print_cr(\"test reattempt timeout, attempt 2\");\n+\n+  STEP_IF(\"test reattempt stack headroom\",\n+      _verbose && TestCrashInErrorHandler == TEST_REATTEMPT_SECONDARY_CRASH)\n+    st->print_cr(\"test reattempt stack headroom\");\n+    reattempt_test_hit_stack_limit(st);\n+\n+  REATTEMPT_STEP_IF(\"test reattempt stack headroom, attempt 2\",\n+      _verbose && TestCrashInErrorHandler == TEST_REATTEMPT_SECONDARY_CRASH)\n+    st->print_cr(\"test reattempt stack headroom, attempt 2\");\n+\n@@ -816,1 +992,1 @@\n-    if (os::platform_print_native_stack(st, _context, buf, sizeof(buf))) {\n+    if (os::platform_print_native_stack(st, _context, buf, sizeof(buf), lastpc)) {\n@@ -819,0 +995,7 @@\n+      \/\/ Stack walking may get stuck. Try to find the calling code.\n+      if (lastpc != nullptr) {\n+        const char* name = find_code_name(lastpc);\n+        if (name != nullptr) {\n+          st->print_cr(\"The last pc belongs to %s (printed below).\", name);\n+        }\n+      }\n@@ -826,2 +1009,1 @@\n-    print_native_stack_succeeded = true;\n-  STEP_IF(\"retry printing native stack (no source info)\", _verbose && !print_native_stack_succeeded)\n+  REATTEMPT_STEP_IF(\"retry printing native stack (no source info)\", _verbose)\n@@ -868,1 +1050,1 @@\n-    \/\/ decode register contents if possible\n+    continuation = 0;\n@@ -870,1 +1052,3 @@\n-    os::print_register_info(st, _context);\n+    st->print_cr(\"Register to memory mapping:\");\n+    st->cr();\n+    os::print_register_info(st, _context, continuation);\n@@ -873,0 +1057,5 @@\n+  REATTEMPT_STEP_IF(\"printing register info, attempt 2\",\n+      _verbose && _context != nullptr && _thread != nullptr && Universe::is_fully_initialized())\n+    ResourceMark rm(_thread);\n+    os::print_register_info(st, _context, continuation);\n+    st->cr();\n@@ -874,1 +1063,5 @@\n-  STEP(\"printing top of stack, instructions near pc\")\n+  REATTEMPT_STEP_IF(\"printing register info, attempt 3\",\n+      _verbose && _context != nullptr && _thread != nullptr && Universe::is_fully_initialized())\n+    ResourceMark rm(_thread);\n+    os::print_register_info(st, _context, continuation);\n+    st->cr();\n@@ -876,1 +1069,1 @@\n-  STEP_IF(\"printing top of stack, instructions near pc\", _verbose && _context)\n+  STEP_IF(\"printing top of stack, instructions near pc\", _verbose && _context != nullptr)\n@@ -883,13 +1076,11 @@\n-    \/\/ decode stack contents if possible\n-    frame fr = os::fetch_frame_from_context(_context);\n-    const int slots = 8;\n-    const intptr_t *start = fr.sp();\n-    const intptr_t *end = start + slots;\n-    if (is_aligned(start, sizeof(intptr_t)) && os::is_readable_range(start, end)) {\n-      st->print_cr(\"Stack slot to memory mapping:\");\n-      for (int i = 0; i < slots; ++i) {\n-        st->print(\"stack at sp + %d slots: \", i);\n-        ResourceMark rm(_thread);\n-        os::print_location(st, *(start + i));\n-      }\n-    }\n+    continuation = 0;\n+    ResourceMark rm(_thread);\n+    st->print_cr(\"Stack slot to memory mapping:\");\n+    st->cr();\n+    print_stack_location(st, _context, continuation);\n+    st->cr();\n+\n+  REATTEMPT_STEP_IF(\"inspecting top of stack, attempt 2\",\n+      _verbose && _context != nullptr && _thread != nullptr && Universe::is_fully_initialized())\n+    ResourceMark rm(_thread);\n+    print_stack_location(st, _context, continuation);\n@@ -898,1 +1089,5 @@\n-  STEP(\"printing code blobs if possible\")\n+  REATTEMPT_STEP_IF(\"inspecting top of stack, attempt 3\",\n+      _verbose && _context != nullptr && _thread != nullptr && Universe::is_fully_initialized())\n+    ResourceMark rm(_thread);\n+    print_stack_location(st, _context, continuation);\n+    st->cr();\n@@ -911,0 +1106,7 @@\n+      \/\/ Check if a pc was found by native stack trace above.\n+      if (lastpc != nullptr) {\n+        if (print_code(st, _thread, lastpc, true, printed, printed_capacity)) {\n+          printed_len++;\n+        }\n+      }\n+\n@@ -960,2 +1162,0 @@\n-  STEP(\"printing process\")\n-\n@@ -970,1 +1170,1 @@\n-  STEP_IF(\"printing all threads\", _verbose && _thread)\n+  STEP_IF(\"printing all threads\", _verbose && _thread != nullptr)\n@@ -1119,0 +1319,1 @@\n+# undef REATTEMPT_STEP_IF\n","filename":"src\/hotspot\/share\/utilities\/vmError.cpp","additions":232,"deletions":31,"binary":false,"changes":263,"status":"modified"},{"patch":"@@ -31,0 +31,1 @@\n+import jdk.internal.util.Architecture;\n@@ -46,2 +47,0 @@\n-    private final String osArch = getHostArchitectureName();\n-\n@@ -60,1 +59,1 @@\n-        String arch = Services.getSavedProperty(\"os.arch\");\n+        Architecture arch = Architecture.current();\n@@ -62,5 +61,2 @@\n-            case \"x86_64\":\n-                return \"amd64\";\n-\n-            default:\n-                return arch;\n+            case X64: return \"amd64\";\n+            default:  return arch.name().toLowerCase();\n@@ -138,1 +134,1 @@\n-    final int runtimeCallStackSize = getConstant(\"frame::arg_reg_save_area_bytes\", Integer.class, osArch.equals(\"amd64\") ? null : 0);\n+    final int runtimeCallStackSize = getConstant(\"frame::arg_reg_save_area_bytes\", Integer.class, Architecture.isX64() ? null : 0);\n","filename":"src\/jdk.internal.vm.ci\/share\/classes\/jdk\/vm\/ci\/hotspot\/HotSpotVMConfig.java","additions":5,"deletions":9,"binary":false,"changes":14,"status":"modified"},{"patch":"@@ -58,1 +58,1 @@\n-compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/locking\/TestRTMDeoptOnLowAbortRatio.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n@@ -60,1 +60,1 @@\n-compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/locking\/TestRTMLockingThreshold.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n@@ -63,2 +63,2 @@\n-compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263 generic-x64,generic-i586\n-compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263 generic-x64,generic-i586\n+compiler\/rtm\/locking\/TestUseRTMXendForLockBusy.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n+compiler\/rtm\/print\/TestPrintPreciseRTMLockingStatistics.java 8183263,8307907 generic-x64,generic-i586,aix-ppc64\n@@ -72,0 +72,4 @@\n+compiler\/c2\/irTests\/TestVectorConditionalMove.java 8306922 generic-all\n+\n+compiler\/jvmci\/TestUncaughtErrorInCompileMethod.java 8309073 generic-all\n+\n@@ -102,1 +106,0 @@\n-runtime\/Thread\/TestAlwaysPreTouchStacks.java 8305416 generic-all\n@@ -108,0 +111,1 @@\n+containers\/docker\/TestMemoryAwareness.java 8303470 linux-x64\n@@ -136,1 +140,2 @@\n-gtest\/NMTGtests.java 8306561 aix-ppc64\n+gtest\/NMTGtests.java#nmt-detail 8306561 aix-ppc64\n+gtest\/NMTGtests.java#nmt-summary 8306561 aix-ppc64\n@@ -144,2 +149,0 @@\n-vmTestbase\/nsk\/jdi\/ThreadReference\/stop\/stop001\/TestDescription.java 7034630 generic-all\n-\n","filename":"test\/hotspot\/jtreg\/ProblemList.txt","additions":11,"deletions":8,"binary":false,"changes":19,"status":"modified"},{"patch":"@@ -380,0 +380,1 @@\n+ -runtime\/ErrorHandling\/ReattemptErrorTest.java \\\n@@ -693,2 +694,1 @@\n-  vmTestbase\/gc\/ArrayJuggle\/ \\\n-  vmTestbase\/gc\/memory\/Array\/ArrayJuggle\n+  vmTestbase\/gc\/ArrayJuggle\/\n","filename":"test\/hotspot\/jtreg\/TEST.groups","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"},{"patch":"@@ -40,1 +40,1 @@\n- * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockDiagnosticVMOptions -XX:VerifyMetaspaceInterval=3\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:+UnlockDiagnosticVMOptions -XX:VerifyMetaspaceInterval=1\n@@ -50,1 +50,1 @@\n- * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:VerifyMetaspaceInterval=3 -XX:+MetaspaceGuardAllocations\n+ * @run main\/native GTestWrapper --gtest_filter=metaspace* -XX:VerifyMetaspaceInterval=1 -XX:+MetaspaceGuardAllocations\n","filename":"test\/hotspot\/jtreg\/gtest\/MetaspaceGtests.java","additions":2,"deletions":2,"binary":false,"changes":4,"status":"modified"}]}